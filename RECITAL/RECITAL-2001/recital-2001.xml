<?xml version="1.0" encoding="UTF-8"?>
<!-- Fichiers OCRisés : recital-2001-long-002, recital-2001-long-003, recital-2001-poster-002 -->
<conference>
	<edition>
		<acronyme>RECITAL'2001</acronyme>
		<titre>3e Rencontres des Étudiants Chercheurs en Informatique pour le Traitement Automatique des Langues</titre>
		<ville>Tours</ville>
		<pays>France</pays>
		<dateDebut>2001-07-02</dateDebut>
		<dateFin>>2001-07-05</dateFin>
		<presidents>
			<nom>Béatrice Bouchou</nom>
		</presidents>
		<typeArticles>
			<type id="long">Papiers longs</type>
			<type id="poster">Posters</type>
		</typeArticles>
		<statistiques>
	<!-- 		<acceptations id="long" soumissions=""></acceptations>
			<acceptations id="poster" soumissions=""></acceptations> -->
		</statistiques>
		<siteWeb>http://tln.li.univ-tours.fr/Tln_Colloques/TALN2001-RECITAL2001/</siteWeb>
		<meilleurArticle>
			<!-- <articleId></articleId> -->
		</meilleurArticle>
	</edition>
	<articles>

		<article id="recital-2001-long-001" session="">
			<auteurs>
				<auteur>
					<nom>Laurent Audibert</nom>
					<email>laurent.audibert@up.univ-aix.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">DELIC – Université de Provence 29 Avenue Robert SCHUMAN 13621 Aix-en-Provence Cedex 1</affiliation>
			</affiliations>
			<titre>LoX : outil polyvalent pour l'exploration de corpus annotés</titre>
			<type>long</type>
			<pages>405-413</pages>
			<resume>Cet article présente une application permettant d'écrire des requêtes complexes sur des corpus étiquetés et de formater librement les résultats de ces requêtes. Le formalisme des requêtes est basé sur le principe des expressions régulières bien connu de la plupart des linguistes travaillant sur des corpus écrits. Contrairement à certains logiciels, qui ne permettent que l’extraction de concordances au format relativement figé, le formatage libre du résultat des requêtes permet leur réutilisation par des programmes ultérieurs et autorise une grande diversité d'applications, s'écartant largement du cadre des simples concordanciers.</resume>
			<mots_cles>Corpus, Concordancier, TAL, Parser, Expression régulière</mots_cles>
			<title></title>
			<abstract>This paper describes a tool that enables complex queries on tagged corpora, and free formatting of the results. The formalism used is based on regular expressions, which are wellknown from most corpus linguists. As opposed to other software, the free formatting of the results enables re-use of the query results by additional tools, and proves useful for a wide range of applications well beyond that of simple concordance programs.</abstract>
			<keywords>Corpora, Concordancer, NLP, Parser, Regular expression</keywords>
		</article>
		<article id="recital-2001-long-002" session="">
			<auteurs>
				<auteur>
					<nom>Antonio Balvet</nom>
					<email>antonio.Balvet@u-paris10.fr</email>
					<email>antonio.Balvet@lcr.thomson-csf.com</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université Paris X Nanterre 200, av. de la République 92013 Nanterre</affiliation>
				<affiliation affiliationId="2">Thales Research &amp; Technologies Domaine de Corbeville, 91404 Orsay Cedex</affiliation>
			</affiliations>
			<titre>Filtrage d'information par analyse partielle Grammaires locales, dictionnaires électroniques et lexique- grammaire pour la recherche d'information</titre>
			<type>long</type>
			<pages>415-424</pages>
			<resume>Nous présentons une approche de filtrage d'information par analyse partielle, reprenant les résultats de recherches issues aussi bien de la recherche documentaire que du traitement automatique des langues. Nous précisons les contraintes liées au domaine du filtrage d'information qui militent, à nos yeux, pour une approche linguistique permettant d'obtenir des performances importantes, ainsi qu'une transparence de fonctionnement. Nous présentons quelques résultats concrets pour illustrer le potentiel de l'approche décrite.</resume>
			<mots_cles>filtrage d'information, TALN, analyse partielle, grammaires locales, lexique-grammaire</mots_cles>
			<title></title>
			<abstract>We present a partial analysis approach to the problem of information filtering, based on the results of different areas of research, ranging from information retrieval to natural language processing. We lay the emphasis on the particular constraints of the information filtering activity compatible with a linguistic and user-friendly treatment (partial analysis). We also present some results measured on an actual corpus in order to illustrate the potential of the described approach.</abstract>
			<keywords>information filtering, NLP, partial analysis, local grammars, lexicon-grammar</keywords>
		</article>
		<article id="recital-2001-long-003" session="">
			<auteurs>
				<auteur>
					<nom>Matthieu Constant</nom>
					<email>mconstant@univ-mlv.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Laboratoire d’Automatique Documentaire et Linguistique (LADL) Université de Marne-la-Vallée Batiment Copernic Champs-sur-Marne 77 457 Marne-la-Vallée</affiliation>
			</affiliations>
			<titre>Bibliothèques d’automates finis et grammaires context-free : de nouveaux traitements informatiques</titre>
			<type>long</type>
			<pages>425-434</pages>
			<resume>La quantité de documents disponibles via Internet explose. Cette situation nous incite à rechercher de nouveaux outils de localisation d’information dans des documents et, en particulier, à nous pencher sur l’algorithmique des grammaires context-free appliquée à des familles de graphes d’automates finis (strictement finis ou à cycles). Nous envisageons une nouvelle représentation et de nouveaux traitements informatiques sur ces grammaires, afin d’assurer un accès rapide aux données et un stockage peu coûteux en mémoire.</resume>
			<mots_cles>Automates finis, forme normale de Greibach, grammaires context-free, graphes</mots_cles>
			<title></title>
			<abstract>The amount of documents available over the Internet is exploding. This phenomenon requires the development of new electronic representations and tools to search information into these documents. This article deals with context-free algorithms applied to finite state graphs (strictly finite or with cycles). It shows new methods and representations to combine efficiently complexities in terms of memory space and processing time.</abstract>
			<keywords>Context-Free Grammars, Finite State Automata, Graphs, Greibach Normal Form</keywords>
		</article>
		<article id="recital-2001-long-004" session="">
			<auteurs>
				<auteur>
					<nom>Nordine Fourour</nom>
					<email>fourour@irin.univ-nantes.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Institut de Recherche en Informatique de Nantes - Université de Nantes 2, chemin de la Houssinière - BP 92208 - 44322 Nantes Cedex 3, France</affiliation>
			</affiliations>
			<titre>Identification et catégorisation automatiques des anthroponymes du Français</titre>
			<type>long</type>
			<pages>435-444</pages>
			<resume>Cet article préente un système de reconnaissance des noms propres pour le Français. Les spécifications de ce système ont été réalisées à la suite d’une étude en corpus et s’appuient sur des critères graphiques et référentiels. Les critères graphiques permettent de concevoir les traitements à mettre en place pour la délimitation des noms propres et la catégorisation repose sur les critères référentiels. Le système se base sur des règles de grammaire, exploite des lexiques spécialisés et comporte un module d’apprentissage. Les performances atteintes par le système, sur les anthroponymes, sont de 89,4% pour le rappel et 94,6% pour la précision.</resume>
			<mots_cles>Entités nommées, reconnaissance automatique, procédure incrémentielle</mots_cles>
			<title></title>
			<abstract>This paper presents a French proper name recognizer. The specifications of this system have been elaborated through corpus investigation upon graphical and semantic criteria. The graphical criteria allow to presuppose some processes to identify proper names boundaries and the semantic classification is used to categorize them. The system is grammar-rule based, uses specialized lexicons, and includes a learning processing. The system performance evaluated on the categories composing anthroponym class achieves 94.6% of precision and 89.4% of recall.</abstract>
			<keywords>Name entities, automatic recognition, incremental process</keywords>
		</article>
		<article id="recital-2001-long-005" session="">
			<auteurs>
				<auteur>
					<nom>Stéphanie Girault</nom>
					<email>sgirault@crisco.unicaen.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Laboratoire CRISCO Université de Caen 14032 Caen Cedex</affiliation>
			</affiliations>
			<titre>Pour un autre traitement de la temporalité narrative</titre>
			<type>long</type>
			<pages>445-454</pages>
			<resume>Tous les médias continus (parole, texte, musique, cinéma) ont, par définition, une structure linéaire, à partir de laquelle un processus cognitif est capable de reconstituer une organisation temporelle différente. Mais jusqu'à quel point faut-il comprendre un texte pour le segmenter en situations et les articuler entre elles ? Autrement dit : jusqu'à quel point faut-il connaître la musique pour différencier couplet et refrain ? Dans un grand nombre de cas, il est possible d'effectuer une telle segmentation automatiquement, et cela uniquement à partir d'indices morpho-syntaxiques. Notre prototype de programme identifie des situations référentielles et analyse la façon dont elles sont articulées pour reconstruire la structure temporelle d'un récit. L'objectif de cette communication n'est pas la description de ce programme, mais plutôt le point de vue du linguiste : comment détecter les discontinuités, c'est-à-dire comment décider s'il y a complétion ou rupture.</resume>
			<mots_cles>sémantique, représentation du discours, temporalité, narration</mots_cles>
			<title></title>
			<abstract>Continuous media – stories, movies, songs – all have a basic linear structure from which cognitive processes are able to retrieve some temporal organization. How much semantic computation is necessarily involved for a proper framing of events and their transition to one another ? Can this computation be approximated with the help of simple formal clues from a shallow parsing of the story stream, and how far can it go ? Our experiments with a prototype application implement a method for segmenting written stories and splicing together "referential situations" that should belong to the same time-frame. This paper does not aim to describe the implementation but rather discusses the linguistic approach to detecting discontinuity in narrative texts, based on the principles of closure and rupture in temporal consistency.</abstract>
			<keywords>semantics, discourse representation, temporality, narratives</keywords>
		</article>
		<article id="recital-2001-long-006" session="">
			<auteurs>
				<auteur>
					<nom>Laura Monceaux</nom>
					<email>Laura.Monceaux@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIMSI-CNRS, BP 133, Bat 508, 91403 Orsay Cedex</affiliation>
			</affiliations>
			<titre>Analyse sémantique dans un système de question-réponse</titre>
			<type>long</type>
			<pages>455-462</pages>
			<resume>Dans cet article, nous présentons le système QALC (Question Answering Langage Cognition) qui a participé à la tâche Question Réponse de la conférence d’évaluation TREC. Ce système a pour but d’extraire la réponse à une question d’une grande masse de documents. Afin d’améliorer les résultats de notre système, nous avons réfléchi à la nécessité de développer, dans le module d’analyse, le typage des questions mais aussi d’introduire des connaissances syntaxico-sémantiques pour une meilleure recherche de la réponse.</resume>
			<mots_cles>Analyse syntaxico-sémantique, système question–réponse</mots_cles>
			<title></title>
			<abstract>In this paper, we present the QALC system that participated to the Question Answering track of the TREC evaluation conference. This system extracts the answer to the question from a large amount of documents. In order to improve the result of our system, it is necessary to develop, in the question analysis module, the search of question type and to introduce a semantic knowledge.</abstract>
			<keywords>Semantic analysis, question–answering system</keywords>
		</article>
		<article id="recital-2001-long-007" session="">
			<auteurs>
				<auteur>
					<nom>Vincent Perlerin</nom>
					<email>perlerin@info.unicaen.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Laboratoire GREYC – Université de Caen bd Maréchal Juin – F14032 Caen Cedex</affiliation>
			</affiliations>
			<titre>La recherche documentaire : une activité langagière</titre>
			<type>long</type>
			<pages>463-472</pages>
			<resume>Un nombre important de requêtes soumises aux moteurs de recherche du W3 ne satisfont pas pleinement les attentes des utilisateurs. La liste de documents proposée en retour est souvent trop longue : son exploration représente un travail exagérément laborieux pour l’auteur de la requête. Nous proposons d’apporter une valeur ajoutée aux systèmes de recherche documentaire (RD) existants en y ajoutant un filtrage n’utilisant que des données fournies par l’utilisateur. L’objectif de notre étude est de confronter un modèle dynamique de la mémoire sémantique des individus (ou des agents) développé par notre équipe à une tâche nécessitant une compétence interprétative de la part des machines. Nous souhaitons dépasser la sémantique lexicale couramment utilisée dans ce champ d’application pour aboutir à l’utilisation d’une sémantique des textes et accroître par ce biais, à la fois la qualité des résultats et la qualité de leur présentation aux usagers.</resume>
			<mots_cles>recherche documentaire, système anthropocentré, filtrage documentaire, sémantique différentielle et componentielle, isotopie</mots_cles>
			<title></title>
			<abstract>Lot of the searches submitted to the main search engines on the www do not bring about satisfactory results for users. The list of documents proposed in return is often too long: its handling constitutes an exceedingly strenuous task for the user. We propose to improve existing document retrieval systems by adding a filter made from criteria determined by the user. The objective of our study is to contrast a dynamic model of individual’s (or agent’s) semantic memory created by our team with a task requiring an interpretative competence from the machines. By this means, we hope to go beyond the lexical semantics commonly used in this field of application and achieve the use of textual semantics.</abstract>
			<keywords>document retrieval, anthropocentred system, document filtering, differential and decomponential semantics, isotopy</keywords>
		</article>
		<article id="recital-2001-long-008" session="">
			<auteurs>
				<auteur>
					<nom>Delphine Reymond</nom>
					<email>reymond@up.univ-aix.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Equipe DELIC – Université de Provence 19, avenue Robert Schuman 13621 Aix-en-Provence - France</affiliation>
			</affiliations>
			<titre>Dictionnaires distributionnels et étiquetage lexical de corpus</titre>
			<type>long</type>
			<pages>473-482</pages>
			<resume>Ce papier présente la première partie d’un travail de thèse qui vise à construire un « dictionnaire distributionnel » à partir d’un corpus de référence. Le dictionnaire proposé est basé sur un ensemble de critères différentiels stricts qui constituent des indices exploitables par des machines pour discriminer le sens des mots en contexte. Pour l’instant, le travail a porté sur 50 000 occurrences qui ont été étiquetées de façon manuelle. Ce sous-corpus pourra servir de corpus d’amorçage pour la constitution d'un corpus étiqueté plus grand, qui pourrait servir à différents tests et travaux sur la désambiguïsation automatique.</resume>
			<mots_cles>Corpus, dictionnaire, étiquetage lexical, information distributionnelle</mots_cles>
			<title></title>
			<abstract>This paper presents the first part of a Ph. D. aimed at the construction of a “distributional dictionary” from a reference corpus. The dictionary proposed is based on a set of strict differential criteria that can be used as clues for the discrimination of word senses in context. So far, we have worked on 50,000 occurrences that have been manually tagged. This subcorpus can be used as a bootstrap corpus for the construction of a larger tagged corpus that could be used in various tests and studies on automatic disambiguation.</abstract>
			<keywords>Corpora, dictionaries, lexical tagging, distributional information</keywords>
		</article>
		<article id="recital-2001-poster-001" session="">
			<auteurs>
				<auteur>
					<nom>Francis Brunet-Manquat</nom>
					<email>Francis.Brunet-Manquat@imag.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">GETA-CLIPS-IMAG (UJF &amp; CNRS) BP 53 - 38041 Grenoble Cedex 9, France</affiliation>
			</affiliations>
			<titre>Du texte vers le sens en analyse par contraintes</titre>
			<type>poster</type>
			<pages>485-490</pages>
			<resume>Les progrès réalisés ces dernières années dans le domaine du traitement automatique des langues naturelles (TALN) ouvrent la voie à des traitements encore plus sophistiqués dans lesquels la sémantique devrait tenir une place centrale. Notre objectif, à long terme, est de réaliser un analyseur texte vers sens s'appuyant sur la théorie Sens-Texte d'Igor Mel'cuk. Cette analyse viserait une compréhension plus approfondie du texte, permettant donc d'atteindre une représentation de niveau sémantique, et une grande robustesse face à des entrées plus ou moins bien formées telles que celles issues de dialogues oraux. Mais renverser la théorie Sens-Texte passe par la définition et la mise en oeuvre de structures de données et d'algorithmes spécifiques pour la représentation et la manipulation automatique des informations linguistiques, notamment des entrées lexicales. Pour cela, nous proposons l'utilisation du paradigme de programmation par contraintes qui offre un moyen efficace d'atteindre nos objectifs.</resume>
			<mots_cles>Analyse par contraintes, théorie Sens-Texte</mots_cles>
			<title></title>
			<abstract>Recent advances in the field of natural language processing allow more sophisticated treatments in which semantics should play a key role. Our long-term goal is to produce a text towards meaning analyzer base on Igor Mel'cuk's Meaning-Text Theory (MTT). Such an analyzer should aim at a deep understanding of the input, producing a semantic representation. It should, also, be able to handled ill-formed inputs such as the ones produced by a speech recognizer. To reverse the Meaning-Text Theory we have to define and use specific data structures and algorithms to represent and handle the linguistic information, in particular the lexical entries. We investigate the use of the constraint-programming paradigm, which provides an efficient mean to reach our goals.</abstract>
			<keywords>constraint parsing, Meaning-Text theory</keywords>
		</article>
		<article id="recital-2001-poster-002" session="">
			<auteurs>
				<auteur>
					<nom>Mohamed Yassine El Amrani</nom>
					<email>elamrani@uqtr.uquebec.ca</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université du Québec à Trois-Rivières, Département de mathématiques et d’informatique C.P. 500, Trois-Rivières, Québec, Canada, G9A 5H7</affiliation>
			</affiliations>
			<titre>Outils d’assistance à la construction de Webs personnels : Utilisation des traitements des langues naturelles dans l’aide à la reformulation de requêtes</titre>
			<type>poster</type>
			<pages>491-496</pages>
			<resume>Nous présentons dans cet article le projet au sein duquel nous développons un logiciel permettant d’assister l’utilisateur lors de la formulation de sa requête de recherche sur le Web et de personnaliser des sous-ensembles du Web selon ses besoins informationnels. L’architecture du logiciel est basée sur l’intégration de plusieurs outils numériques et linguistiques de traitements des langues naturelles (TALN). Le logiciel utilise une stratégie semi-automatique où la contribution de l’utilisateur assure la concordance entre ses attentes et les résultats obtenus. Ces résultats sont stockés dans diverses bases de données permettant de conserver différents types d’informations (classes de sites/pages Web similaires, profils de l’usager, lexiques, etc.) constituant une projection locale et personnalisée du Web.</resume>
			<mots_cles>Reformulation des requêtes, recherche et extraction d'information, personnalisation</mots_cles>
			<title></title>
			<abstract>We here present a new research project in which we are developing a software that can help the user to formulate his web search query and customize (a subsets of) the WWW to her individual and subjective information needs. The software’s architecture is based on several natural language processing tools, some of them numeric and some others linguistic. The software involves a semi-automatic processing strategy in which the user’s contribution ensures that the results are useful and meaningful to her. These results are saved in various databases that capture several types of information (e.g. classes of related Web sites/pages, user profile, lexicons, etc.) that will constitute a local and personalized subset of the Web and support the user’ s information retrieval tasks.</abstract>
			<keywords>Web customization, query reformulation, information retrieval, information extraction</keywords>
		</article>
		<article id="recital-2001-poster-003" session="">
			<auteurs>
				<auteur>
					<nom>Estelle Le Roux</nom>
					<email>eleroux@ina.fr</email>
					<email>Estelle.Le.Roux@limsi.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">INA 4 avenue de l’Europe, 94366 Bry sur Marne Cedex France</affiliation>
				<affiliation affiliationId="2">LIMSI Bt 508, Université Paris-Sud, 91403 Orsay Cedex</affiliation>
			</affiliations>
			<titre>Extraction d’information de documents textuels associés à des contenus audiovisuels</titre>
			<type>poster</type>
			<pages>497-502</pages>
			<resume>L’indexation audiovisuelle, indispensable pour l’archivage et l’exploitation des documents, se révèle être un processus délicat, notamment à cause de la multiplicité de significations qui peuvent être attachées aux images. Nous proposons dans cette communication une méthode d’instanciation de ”patrons d’indexation” à partir d’un corpus d’articles de journaux écrits. Cette méthode repose sur un processus ”d’amorçage hiérachisé”, qui permet de trouver de nouveaux termes à partir de termes connus dans leur voisinage et de leurs relations taxinomiques sous forme d’ontologie.</resume>
			<mots_cles>Amorce, Extraction d’information, Ontologie, Patron d’indexation</mots_cles>
			<title></title>
			<abstract>Audiovisual indexation, essential for filing and using documents, is a difficult process notably because of the multiplicity of meanings which can be associated to the pictures. We propose a method of instanciation of ”indexation patterns” from a corpus of articles from newspapers. This method is based on a ”hierarchical bootstrapping” which can find new terms from known terms in their neighbourhood and from an ontology.</abstract>
			<keywords>Bootstrapping, Indexation patterns, Information extraction, Ontology</keywords>
		</article>
	</articles>
</conference>