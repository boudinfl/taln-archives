Récital 2001, Tours, 2-5 juillet 2001

Outils d’assistance a la construction de Webs personnels :
Utilisation des traitements des langues naturelles dans l’aide a la
reformulation de requétes

Mohamed Yassine El Amrani*

Université du Québec a Trois-Riviéres,
Département de mathématiques et d’informatique
C.P. 500, Trois-Riviéres, Québec, Canada, G9A 5H7
elarnrani@uqtr.uquebec.ca

Résumé — Abstract

Nous présentons dans cet article le projet au sein duquel nous développons un logiciel
permettant d’assister l’utilisateur lors de la formulation de sa requéte de recherche sur le Web
et de personnaliser des sous-ensembles du Web selon ses besoins informationnels.
L’architecture du logiciel est basée sur l’intégration de plusieurs outils numériques et
linguistiques de traitements des langues naturelles (TALN). Le logiciel utilise une stratégie
semi-automatique ou la contribution de l’utilisateur assure la concordance entre ses attentes et
les résultats obtenus. Ces résultats sont stockés dans diverses bases de données permettant de
conserver différents types d’informations (classes de sites/pages Web similaires, proﬁls de
l’usager, lexiques, etc.) constituant une projection locale et personnalisée du Web.

We here present a new research project in which we are developing a software that can help
the user to formulate his web search query and customize (a subsets of) the W to her
individual and subjective information needs. The software’s architecture is based on several
natural language processing tools, some of them numeric and some others linguistic. The
software involves a semi-automatic processing strategy in which the user’s contribution
ensures that the results are useful and meaningful to her. These results are saved in various
databases that capture several types of information (e.g. classes of related Web sites/pages,
user proﬁle, lexicons, etc.) that will constitute a local and personalized subset of the Web and
support the user’ s information retrieval tasks.

Mots-Clefs : Reformulation des requétes, recherche et extraction d'inforrnation,
personnalisation.

Keywords : Web customization, query reformulation, information retrieval, information
extraction.

Etudiant a la maitrise en mathe'ma11'ques et inforrnatique appliquées sous la direction conjointe des
professeurs Ismail Biskri et Sylvain Delisle.

Mohamed Yassine El Amrani

1 Introduction

Bien que le World Wide Web constitue la plus grande librairie électronique jamais construite,
nous constatons que le Web génere de grandes frustrations aupres de ses utilisateurs. Les
résultats des moteurs de recherche sont souvent trop nombreux pour étre humainement
exploitables et entachés d’un niveau de bruit inacceptable (faible précision) ou alors trop peu
nombreux dﬁ a un niveau de silence désespérant (i.e. faible taux de rappel (recall)). 11 y a une
absence d’acces a l’information basé sur le contenu des documentsl et les différents formats
d’encodages disponibles sur le Web ainsi que leur nature hétérogene compliquent
significativement la tache de recherche informationnelle automatisée. De plus, malgré
certains (faibles) espoirs de normalisation et d’auto-contr6le de la communauté du Web, une
importante difficulté persiste : Lorsqu’un utilisateur effectue une recherche sur le Web, il
essaye de trouver une concordance entre ses concepts et ceux disponibles sur le Web.
Actuellement, cette concordance est essentiellement établie a l’aide de mots-clés (terrnes
d’indexation). Cependant, si les mots-clés ne rencontrent pas les concepts de l’utilisateur et
ceux utilisés dans les documents Web pertinents, les résultats des moteurs de recherche seront
peu intéressants.

L’objectif de cet article est de présenter une alternative aux utilisateurs leur permettant de
construire un sous-ensemble du Web selon leurs besoins. Afin d’y parvenir, nous proposons
une approche semi-automatique qui utilise des outils de traitement des langues naturelles
(TLN). Ces outils permettent d’identifier les intersections entre les concepts de l’utilisateur et
ceux présents sur le Web. L’évaluation et l’interprétation des documents étant tres
personnelle, l’utilisateur doit prendre une part active dans ce processus. Notre travail peut étre
associé aux recherches liées a la personnalisation du Web ainsi qu’aux travaux utilisant les
traitements des langues naturelles (TLN). Nous résumons quelques travaux similaires dans la
section suivante.

2 Travaux similaires

Cette section ne traitera pas des travaux classiques sur la recherche documentaire tels que
Salton (1989), ni des produits commerciaux déja disponibles sur le marché (pour des
exemples de logiciels de recherche documentaire, voir
http: //www.dwinfocenter.org/docum.html). Egalement, nous n’élaborerons pas sur les
travaux traitant des agents Web. Le lecteur intéressé pourra consulter les sites << Agent—based
Information Retrieval Resources» (http://www.cs.umbc.edu/~ian/agent—ir.htm1), << UIWBC
AgentWeb >> (http://www.csee.umbc.edu/agents) ou << Web Information Retrieval & Information
Extraction» (http://www.mri.mq.edu.au/~einat/web_ir). Nous souhaitons focaliser sur les
différents aspects de la personnalisation et les approches dont l’objectif est de rendre le
logiciel adaptable aux besoins de chaque utilisateur et un aspect de la personnalisation est le
filtrage de l’information. Michel (2000) propose un systeme permettant le filtrage de données
en prenant en compte les caractéristiques personnelles des utilisateurs qui choisissent parmi
huit processus de filtrage différents. Amati et al. (1997) présentent le systeme ProFile qui
acquiert et met a jour un modele des intéréts des utilisateurs au moyen d’une interaction avec

1 Afin de simplifier la discussion, nous utilisons le terme document pour faire reference a n’importe quel

groupement des donne'es du Web qui est vu nomialement comme une unite’. Par exemple, cela peut étre une
page Web, un document textuel, une image, un fichier audio, etc.

Outils d ’assistance a la construction de Webs personnels

ceux-ci et de l’utilisation d’un algorithme d’apprentissage. Pour le ﬁltrage d’inforrnations et
la collaboration entre agents et utilisateurs, nous référons le lecteur aux travaux de Cohen &
Kudenko (1997) et de Good et al. (1999). Le systeme proposé par Craven et al. (1998) utilise
une ontologie de classes et de relations ainsi que des pages Web sélectionnées par l’utilisateur
qui servent d’exemples d’entrainements a ces classes et relations pour apprendre les
procédures permettant l’extraction de nouvelles instances a partir du Web. Jouis et al. (1998)
présentent le projet COGNIWEB qui est un modele hybride combinant des outils numériques
et linguistiques du TLN (Voir également Biskri & Delisle (1999)). C’est un outil de ﬁltrage de
documents issus du Web. Un classiﬁcateur identiﬁe les pages Web partageant un certain
nombre de termes puis une analyse sémantique est effectuée a l’aide du sous-systeme SEEK.
L’utilisateur identifie ensuite les relations sémantiques entre les termes.

3 Le systeme d’aide 51 la construction de Webs personnels

Nous décrivons maintenant les principaux éléments de notre outil d’aide a la construction de
Webs personnels. Bien que le projet soit en cours de développement, nous avons déja concu,
développé et implémenté en Visual C++ plusieurs de ses composantes. Ce projet combine
l’utilisation d’outils numériques et linguistiques du TLN. La principale justification de cette
approche hybride réside dans le fait que les outils numériques fournissent rapidement des
indices sur le theme d’un texte ou corpus alors que les outils linguistiques utilisent ces indices
aﬁn d’effectuer des traitements plus détaillés. Mais comment accéder aux sites (et pages) Web
qui nous intéressent? La solution habituelle a ce probleme consiste a effectuer une recherche
a l’aide de moteurs de recherche en spécifiant quelques mots-clés. Cette procédure simple
cache plusieurs difﬁcultés car l’utilisateur possede une connaissance partielle du domaine
dans lequel il effectue une recherche et par consequent, ne connait pas les mots-clés qui
identiﬁent le mieux l’information recherchée. De plus, plusieurs moteurs de recherche
utilisent des index construits automatiquement avec une liste de mots-clés “objectifs” ou
“standards”. Néanmoins, ces mots-clés sont plut6t subjectifs et plusieurs possedent des sens
différents selon le contexte ou le domaine dans lequel ils apparaissent. Notre obj ectif est donc
de développer un outil logiciel hvbride qui permet la construction de proiections du Web
selon les préférences de l’utilisateur. Nous décrivons ici la stratégie de traitement sous-jacente
qui est organisée en trois phases.

lm phase : Une requéte initiale est soumise a un groupe de moteurs de recherche tels que
Google, Yahoo!, AltaVista, etc. Cette requéte devrait englober l’information que l’utilisateur
espere trouver sur le Web. Par la suite, l’utilisateur aura l’opportunité de reconsidérer sa
requéte a la lumiere des résultats de recherche obtenus grace a cette requéte initiale (Figure
1). Dans le cadre de ce projet, nous considérons que chaque site Web représente un segment
textuel. Ainsi, un ensemble de sites Web forme un ensemble de segments textuels composant
ainsi un corpus ou chaque segment conserve son identité et sa source. Nous soumettons alors
ce corpus a un classiﬁcateur numérique (l\/Ieunier et al., 1997 ; Rialle et al., 1998 ; Biskri &
Delisle, 1999) permettant l’identiﬁcation des segments partageant des régularités lexicales.
Toutefois, nous demeurons ouvert a l’utilisation de classificateurs autres que ceux cités — sur
les classiﬁcateurs textuels, Voir Turenne (2000). Les classes produites par le classiﬁcateur
Vont tendre a contenir des segments de sujets similaires et Vont identiﬁer les unités lexicales
qui ont tendance a étre associées a ces sujets. Les résultats du classiﬁcateur numérique Vont
fournir une liste de termes candidats en relation avec ceux de la requéte initiale. Cela permet
de foumir une assistance dans la formulation d’une nouvelle requéte plus précise.

Mohamed Yassine El Amrani

‘ Prefi1sde1’usager | |Le‘WWW :4
IL

     
  
   

Requete initiale ‘V Liste
GWUPB [13 m'3't'3U-1'5 513 dfaclresses de
_ recherche du Web F pages web
Classiﬁcateur U i
mméﬂque is Corpus obtenus a Processus de
parhr tie segments : CDn,S1,1‘1_JJ:t1un_ :11;
(ie. 1e conienu des corpus

I

Classes de sites Web ﬁmilaires
(nu connexes) 35 classes de

termes de co- occurrence

 
    

‘ll’

Nouvelle r e quéte

 

L’usager sélectionne L’usager determine si _ Requete
de nouveaux term es a N911 lesresultais sent 0111 ﬁnale :5:
pﬂ.t‘l.:|.'t‘ des classes et "' sattsfaisants au regard " Classes
a.me'1iocre sarequéte de ses pruzupreslzuesuins ﬁnales

initciale infcsrmatciutmels

Figure 1 : Processus d’aide a la reformulation de la requéte

2"“ phase : Maintenant, lesquels de ces nouveaux termes devraient étre choisis par
l’utilisateur aﬁn de reformuler sa requéte ? Un point départ évident serait de préférer les
termes qui apparaissent dans les classes ou ﬁgurent également les mots-clés de la requéte
initiale. Ensuite, l’utilisateur peut soumettre sa nouvelle requéte aux moteurs de recherche
pour obtenir des groupes de classes de pages Web similaires ou connexes. L’utilisateur peut
alors découvrir a la fin de cette étape de nouveaux termes qui peuvent l’amener a reformuler
encore plus précisément sa requéte. Eventuellement, apres quelques iterations, l’utilisateur
obtiendra une reformulation précise de sa requéte qui l’amenera (Via les adresses Web) a
l’inforrnation recherchée ou du moins a celle qui s’en approche le plus.

3é"'° phase : Au cours de cette étape, une exploration plus détaillée des segments obtenus a
partir du classiﬁcateur numérique est effectuée. C’est une exploration qui a deux principaux
objectifs : perrnettre a l’utilisateur de construire son sous-ensemble personnel du Web
incluant un index signiﬁcatif et extraire l’information et les connaissances a partir de ces sites
Web (text mining). A la fin de cette phase, l’utilisateur sera capable de consulter directement
les pages Web ainsi que de soumettre des requétes de recherche sur son index personnel
uniquement.

A l’aide de notre systeme, l’utilisateur sera capable de construire des projections du Web.
Bien entendu, un certain coﬁt est associé au processus de construction de Webs personnels.
Comme c’est le cas des approches semi-automatiques impliquant des mécanismes
d’extraction de connaissances, ce coﬁt est relativement élevé au début mais tend a décroitre
signiﬁcativement par la suite (Barker et al., 1998).

Outils a’ ’assistance a la construction de Webs personnels

4 Implémentation

Nous présentons a present la logique de conception de l’outil d’aide a la construction de
Webs personnels. L’implémentation de cet outil est présentement en cours. Le logiciel est
concu autour du concept d’agent ou un agent est une composante logicielle capable
d’exécuter des taches particulieres de maniere autonome a partir de données provenant d’un
utilisateur humain ou d’une autre composante logicielle. L’utilisateur peut également gérer
des profils (ensemble de termes dont l’importance relative est quantifiée par un poids) qu’il
associe aux différents agents selon ses préférences. Lorsqu’un ou plusieurs profils sont
associés aux termes d’une requéte traitée par un agent, les documents obtenus doivent alors
contenir un sous-ensemble des termes des profils. De cette maniere, les termes délimitant un
champ d’intérét particulier de l’utilisateur inﬂuenceront les résultats des agents.

L’agent de recherche permet a l’utilisateur de saisir une requéte, de choisir le groupe de
moteurs de recherche qui seront sollicités et de lui associer un ou plusieurs profils et agents.
Lorsque l’utilisateur associe a l’agent de recherche un agent d’aide a la reformulation de la
requéte, celui-ci permet d’enrichir la requéte de l’utilisateur a l’aide de termes extraits de
documents issus du Web selon le processus décrit dans les phases 1 et 2 de la section 3. Les
traitements des langues naturelles utilisés par les agents sont effectués par l ’agent TLN selon
les préférences de l’utilisateur. L’utilisateur contr6le tous les traitements effectués par les
agents et évalue leurs performances. Pour évaluer le taux de satisfaction de l’utilisateur, nous
mettrons l’emphase sur les performances des différents agents ainsi que sur leur progression
dans la satisfaction des attentes de l’utilisateur vis-a-vis des résultats obtenus.

5 Conclusion

11 y a trois aspects importants qui caractérisent ce travail : Les utilisateurs ne devraient pas
s’attendre a un développement généralisé d’outils permettant une amélioration significative
de l’adaptabilité du Web aux besoins personnels ; L’idée de développement d’un Web
“obj ectif ” (i.e. non-subjectif) est problématique ; L’utilisation d’outils Web automatisés uni-
quement empéchera les utilisateurs d’atteindre plusieurs de leurs buts lors des recherches sur
le Web en écartant leur subjectivité. Tout ceci justifie l’approche utilisée dans notre travail :
fournissons aux utilisateurs du Web des outils personnalisés qui vont leur permettre de
construire leurs propres Webs personnels, subjectifs mais significatifs. L’outil d’aide a la
construction de Webs personnels est en cours de développement et il est encore t6t pour faire
un bilan global. Une dimension de l’évaluation serait de mesurer les gains que notre logiciel
apporte aux utilisateurs et de comparer les performances du logiciel avec ceux des outils de
recherche conventionnels. Plusieurs aspects de la conception actuelle du systeme pourraient
étre reconsidérés ou étendus. Par exemple, les requétes des utilisateurs sont formulées a l’aide
de mots-clés. Toutefois, nous sommes intéressés a utiliser des requétes formulées a partir
d’une expression, phrase, paragraphe ou un fichier; des marqueurs syntaxiques et
sémantiques; etc.

Références

Amati G., Crestani, F., Ubaldini, F. (1997), “A Learning System for Selective Dissemination
of Information”, Proc. Of the 15”’ International Joint Conf On Artificial Intelligence (IJCAI—
97), Nagoya, Japan, 23-29 aoﬁt 1997, 764-769.

Mohamed Yassine El Amrani

Barker K., Delisle, S., Szpakowicz, S. (1998). “Test-Driving TANKA: Evaluating a
Semi-automatic System of Text Analysis for Knowledge Acquisition”, I2th Biennal
Conference of the Canadian Society for Computational Studies of Intelligence (CAI '98),
Vancouver (B.C.), Canada, juin 18-20 1998, 60--71. Published in Lectures Notes in Artiﬁcial
Intelligence #1418, Springer.

Biskri I., Delisle, S. (1999), “Un modele hybride pour le textual data mining : un mariage de
raison entre le numérique et le linguistique”, 6eme Conférence Annuelle sur le Traitement
Automatique des Langues (TALN-99), Cargese, Corse, 12-17 juillet 1999, 55-64.

Biskri I., Delisle, S. (2000), “User-Relevant Access to Textual Information Through Flexible
Identification of Terms: A Semi-Automatic Method and Software Based on a Combination of
N-Grams and Surface Linguistic Filters”, Actes de la 6e‘me Conférence RIAO-2000
( Content-Based Multimedia Information Access), Paris (France), 12-14 avril 2000.

Cohen, W.W., Kudenko, K. (1997), “Transferring and Retraining Learned Information
Filters”, Proc. of the 14th National Conf on Artiﬁcial Intelligence (AAAI—97), Providence
(Rhode Island), USA, 27-31 juillet 1997, 583-590.

Craven, M., DiPasquo, D., Freitag, D., McCallum, A., Mitchell, T., Nigam, K., Slattery, S.
(1998), “Learning to Extract Symbolic Knowledge from the World Wide Web”, Proc. of the
15th National Conf on Artificial Intelligence (AAAI-98), Madison, Wisconsin, USA, 26-30
juillet 1998, 509-516.

Good, N., Schafer, B., Konstan, J., Borchers, A., Sarwar, B., Herlocker, J., & Riedl, J. (1999).
“Combining Collaborative Filtering With Personal Agents for Better Recommendations”, In
Proceedings of the AAAI- ’99 conference, 439-446.

Jouis C., Mustafa el-Hadi, W. Rialle, V. (1998), “COGNIWEB, Modélisation hybride
linguistique et numérique pour un outil de filtrage d’informations sur les réseaux”, Actes de la
Rencontre Internationale sur l ’Extraction, le F iltrage et le Résumé Automatique (RIFRA-98),
Sfax, Tunisie, 11-14 novembre 1998, 191-203.

Meunier, J.G., I. Biskri, G. Nault & M. Nyongwa (1997), “ALADIN et le traitement
connexionniste de l’analyse terminologique”, Actes de la Conf sur la Recherche
d ’Informations Assistée par Ordinateur (RIAO-97), Montréal (Québec), Canada, 25-27 juillet
1997, 661-664.

Michel, C. (2000), “Diagnostic Evaluation of a Personalized Filtering Information Retrieval
System: Methodology and Experimental Results”, Actes de la Conf sur la Recherche
d ’Informations Assistée par Ordinateur (RIAO-2000), Paris, France, 12-14 avril 2000, 1578-
1588.

Salton, G. (1989), Automatic Text Processing .' Ihe Transformation, Analysis and Retrieval of
Information by Computer, Addison-Wesley.

Turenne, N. (2000), Apprentissage statistique pour l ’extraction de concepts a partir de textes
(Application au ﬁltrage d’informations textuelles), these de doctorat en inforrnatique,
Université Louis-Pasteur, Strasbourg, France.

Rialle V., Meunier, J.G., Oussedik, S., Biskri, I., Nault, G. (1998), “Application de
l’algorithmique génétique a l’analyse terminologique”, Acte du Colloque international JADT-
98, Nice, France

