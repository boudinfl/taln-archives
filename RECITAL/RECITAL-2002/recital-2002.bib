@proceedings{RECITAL:2002,
  editor    = {Roussanaly, Azim},
  title     = {Actes des 4e Rencontres des Étudiants Chercheurs en Informatique pour le Traitement Automatique des Langues},
  month     = {June},
  year      = {2002},
  address   = {Nancy, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  url       = {http://www.atala.org/taln_archives/RECITAL/RECITAL-2002}
}

@inproceedings{reymond:2002:RECITAL,
  author    = {Reymond, Delphine},
  title     = {Méthodologie pour la création d’un dictionnaire distributionnel dans une perspective d’étiquetage lexical semi-automatique},
  booktitle = {Actes des 4e Rencontres des Étudiants Chercheurs en Informatique pour le Traitement Automatique des Langues},
  month     = {June},
  year      = {2002},
  address   = {Nancy, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {405--414},
  url       = {http://www.atala.org/taln_archives/RECITAL/RECITAL-2002/recital-2002-long-001},
  language  = {french},
  resume    = {Des groupes de recherche de plus en plus nombreux s'intéressent à l’étiquetage lexical ou la désambiguïsation du sens. La tendance actuelle est à l’exploitation de très grands corpus de textes qui, grâce à l'utilisation d’outils lexicographiques appropriés, peuvent fournir un ensemble de données initiales aux systèmes. A leur tour ces systèmes peuvent être utilisés pour extraire plus d'informations des corpus, qui peuvent ensuite être réinjectées dans les systèmes, dans un processus récursif. Dans cet article, nous présentons une méthodologie qui aborde la résolution de l’ambiguïté lexicale comme le résultat de l’interaction de divers indices repérables de manière semi-automatique au niveau syntaxique (valence), sémantique (collocations, classes d’objets) avec la mise en oeuvre de tests manuels.},
  abstract  = {More and more research groups are involved in sense tagging or sense disambiguation. The current trend is to use very large text corpora which, with the help of appropriate lexicographical tools, can provide initial data to the disambiguation systems. In turn, these systems can be used to extract more data from corpora, which can be fed again to the systems, in a bootstrapping process. In this paper, we tackle lexical disambiguation through the interaction of various cues which can be detected semi-automatically at the syntactic and semantic levels (valency, collocations, object classes), along with manual tests.},
  motscles  = {Désambiguïsation lexicale, dictionnaire, propriétés distributionnelles, collocations, classes d’objets},
  keywords  = {WSD, dictionary, distributional properties, collocations, classes of objects},
}

@inproceedings{audibert:2002:RECITAL,
  author    = {Audibert, Laurent},
  title     = {Etude des critères de désambiguïsation sémantique automatique : présentation et premiers résultats sur les cooccurrences},
  booktitle = {Actes des 4e Rencontres des Étudiants Chercheurs en Informatique pour le Traitement Automatique des Langues},
  month     = {June},
  year      = {2002},
  address   = {Nancy, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {415--424},
  url       = {http://www.atala.org/taln_archives/RECITAL/RECITAL-2002/recital-2002-long-002},
  language  = {french},
  resume    = {Nous présentons dans cet article les débuts d’un travail visant à rechercher et à étudier systématiquement les critères de désambiguïsation sémantique automatique. Cette étude utilise un corpus français étiqueté sémantiquement dans le cadre du projet SyntSem. Le critère ici étudié est celui des cooccurrences. Nous présentons une série de résultats sur le pouvoir désambiguïsateur des cooccurrences en fonction de leur catégorie grammaticale et de leur éloignement du mot à désambiguïser.},
  abstract  = {This paper describes the beginning of a study which aims at researching and systematically analysing criteria for automatic word sense disambiguation. This study uses a French sense tagged corpus developed in the SyntSem project. We analyse here the cooccurrence criterion and report a series of results on the disambiguative power of cooccurrences with respect to their grammatical category and distance from the word to be disambiguated.},
  motscles  = {Désambiguïsation sémantique, corpus sémantiquement étiqueté, cooccurrences},
  keywords  = {Word sense disambiguation, sense tagged corpora, cooccurrences},
}

@inproceedings{balvet:2002:RECITAL,
  author    = {Balvet, Antonio},
  title     = {LIZARD, un assistant pour le développement de ressources linguistiques à base de cascades de transducteurs},
  booktitle = {Actes des 4e Rencontres des Étudiants Chercheurs en Informatique pour le Traitement Automatique des Langues},
  month     = {June},
  year      = {2002},
  address   = {Nancy, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {425--434},
  url       = {http://www.atala.org/taln_archives/RECITAL/RECITAL-2002/recital-2002-long-003},
  language  = {french},
  resume    = {Nous présentons un outil visant à assister les développeurs de ressources linguistiques en automatisant la fouille de corpus. Cet outil, est guidé par les principes de l’analyse distributionnelle sur corpus spécialisés, étendue grâce à des ressources lexicales génériques. Nous présentons une évaluation du gain de performances dû à l’intégration de notre outil à une application de filtrage d’information et nous élargissons le champ d’application de l’assistant aux études sur corpus menées à l’aide de cascades de transducteurs à états finis.},
  abstract  = {We present a tool providing linguistic resources developers with automated corpus analysis features. Our tool implements specialized corpora distributional analysis principles, extended by the integration of external generic lexical resources. We present an evaluation of the gain in performance attributable to our tool, for a text filtering task. We also widen our tool’s scope of applications to transducer cascades-based corpus processing activities.},
  motscles  = {TALN, assistant linguistique, ressources linguistiques, agents autonomes, études sur corpus},
  keywords  = {NLP, linguistic wizard, linguistic resources, autonomous agents, corpus-based studies},
}

@inproceedings{seddah-jacquey:2002:RECITAL,
  author    = {Seddah, Djamé and Jacquey, Evelyne},
  title     = {Conceptualisation d’un système d’informations lexicales, une interface paramétrable pour le T.A.L},
  booktitle = {Actes des 4e Rencontres des Étudiants Chercheurs en Informatique pour le Traitement Automatique des Langues},
  month     = {June},
  year      = {2002},
  address   = {Nancy, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {435--444},
  url       = {http://www.atala.org/taln_archives/RECITAL/RECITAL-2002/recital-2002-long-004},
  language  = {french},
  resume    = {La nécessité de ressources lexicales normalisées et publiques est avérée dans le domaine du TAL. Cet article vise à montrer comment, sur la base d’une partie du lexique MULTEXT disponible sur le serveur ABU, il serait possible de construire une architecture permettant tout à la fois l’accès aux ressources avec des attentes différentes (lemmatiseur, parseur, extraction d’informations, prédiction, etc.) et la mise à jour par un groupe restreint de ces ressources. Cette mise à jour consistant en l’intégration et la modification, automatique ou manuelle, de données existantes. Pour ce faire, nous cherchons à prendre en compte à la fois les besoins et les données accessibles. Ce modèle est évalué conceptuellement dans un premier temps en fonction des systèmes utilisés dans notre équipe : un analyseur TAG, un constructeur de grammaires TAGs, un extracteur d’information.},
  abstract  = {Lexical ressources which would be normalized and freely accessible is a major issue in the NLP research area. This article aims to show how to built an information system which allow (1) a freely access for distinct NLP systems (tagging, parsing, information extraction, etc.) and (2) an easy update of data by a restricted team of researchers, this update being manual or computed. Starting with a a subset of the MULTEXT lexicon which is accessible from the server ABU, we aim to take into account the various needs and the variability of accessible lexical data. Our modelisation is evaluated with three existing systems of our team : EGAL (parsing), a builder of Tag grammars and VULCAIN (information extraction).},
  motscles  = {Bases de données, ressources lexicales},
  keywords  = {Data Bases, Lexical Ressources},
}

@inproceedings{bossard:2002:RECITAL,
  author    = {Bossard, Bruno},
  title     = {Problèmes posés par la reconnaissance de gestes en Langue des Signes},
  booktitle = {Actes des 4e Rencontres des Étudiants Chercheurs en Informatique pour le Traitement Automatique des Langues},
  month     = {June},
  year      = {2002},
  address   = {Nancy, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {445--454},
  url       = {http://www.atala.org/taln_archives/RECITAL/RECITAL-2002/recital-2002-long-005},
  language  = {french},
  resume    = {Le but de cet article est d’expliciter certains des problèmes rencontrés lorsque l’on cherche à concevoir un système de reconnaissance de gestes de la Langue des Signes et de proposer des solutions adaptées. Les trois aspects traités ici concernent la simultanéïté d’informations véhiculées par les gestes des mains, la synchronisation éventuelle entre les deux mains et le fait que différentes classes de signes peuvent se rencontrer dans une phrase en Langue des Signes.},
  abstract  = {The aim of this paper is to specify some of the problems raised when one wants to design a gesture recognition system dedicated to Sign Language, and to propose suited solutions. The three topics considered here concern the simultaneity of information conveyed by hand gestures, the possible synchronicity between the two hands, and the different classes of signs that may be encountered in a Sign Language sentence.},
  motscles  = {Reconnaissance de gestes, Langue des Signes},
  keywords  = {Gesture recognition, Sign Language},
}

@inproceedings{manuelian:2002:RECITAL,
  author    = {Manuélian, Hélène},
  title     = {Annotation des descriptions définies : le cas des reprises par les rôles thématiques},
  booktitle = {Actes des 4e Rencontres des Étudiants Chercheurs en Informatique pour le Traitement Automatique des Langues},
  month     = {June},
  year      = {2002},
  address   = {Nancy, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {455--466},
  url       = {http://www.atala.org/taln_archives/RECITAL/RECITAL-2002/recital-2002-long-006},
  language  = {french},
  resume    = {Nous présentons dans cet article un cas particulier de description définie où la description reprend le rôle thématique d’un argument (implicite ou explicite) d’un événement mentionné dans le contexte linguistique. Nous commençons par montrer que les schémas d’annotation proposés (MATE) et utilisés (Poesio et Vieira 2000) ne permettent pas une caractérisation uniforme ni, partant, un repérage facile de ces reprises. Nous proposons une extension du schéma MATE qui pallie cette difficulté.},
  abstract  = {We present in this paper a particular kind of definite description which mentions the thematic role of an (implicit or explicit) argument of an event mentioned in the context. We show that the annotation schemes which are either proposed (MATE) or used (Poesio and Vieira 2000) permit neither a uniform characterisation of the phenomenon nor an easy identification of these anaphors. We propose an extension of the MATE schemes which remedy this difficulty.},
  motscles  = {Génération automatique de textes, annotation, coréférence, rôles thématiques},
  keywords  = {Natural language generation, annotation, coreference, thematic role},
}

@inproceedings{henry:2002:RECITAL,
  author    = {Henry, Sandrine},
  title     = {Etude des répétitions en français parlé spontané pour les technologies de la parole},
  booktitle = {Actes des 4e Rencontres des Étudiants Chercheurs en Informatique pour le Traitement Automatique des Langues},
  month     = {June},
  year      = {2002},
  address   = {Nancy, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {467--476},
  url       = {http://www.atala.org/taln_archives/RECITAL/RECITAL-2002/recital-2002-long-007},
  language  = {french},
  resume    = {Cet article rapporte les résultats d’une étude quantitative des répétitions menée à partir d’un corpus de français parlé spontané d’un million de mots, étude réalisée dans le cadre de notre première année de thèse. L’étude linguistique pourra aider à l’amélioration des systèmes de reconnaissance de la parole et de l'étiquetage grammatical automatique de corpus oraux. Ces technologies impliquent la prise en compte et l’étude des répétitions de performance (en opposition aux répétitions de compétence, telles que nous nous sujet + complément) afin de pouvoir, par la suite, les « gommer » avant des traitements ultérieurs. Nos résultats montrent que les répétitions de performance concernent principalement les mots-outils et apparaissent à des frontières syntaxiques majeures.},
  abstract  = {This article is a report of a quantitative study of repetitions based on a corpus of a onemillion- word spontaneous spoken French, conducted during the first year of our PhD thesis. This linguistic study can contribute to the improvement of speech recognition and spoken French part-of-speech tagging. Improvement of these technologies requires taking into account and studying performance repetitions (such as complement + complement nous nous) in order to be able to "erase" them before further processing. Our results show that repetitions mainly involve function words and take place at major syntactic boundaries.},
  motscles  = {Répétitions, français parlé spontané, « disfluences », phénomènes de performance, étude quantitative, reconnaissance de la parole, étiquetage morpho-syntaxique},
  keywords  = {Repetitions, spontaneous French speech, disfluencies, performance phenomena, quantitative study, speech recognition, part-of-speech tagging},
}

@inproceedings{max:2002:RECITAL,
  author    = {Max, Aurélien},
  title     = {Normalisation de documents par analyse du contenu à l’aide d’un modèle sémantique et d’un générateur},
  booktitle = {Actes des 4e Rencontres des Étudiants Chercheurs en Informatique pour le Traitement Automatique des Langues},
  month     = {June},
  year      = {2002},
  address   = {Nancy, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {477--486},
  url       = {http://www.atala.org/taln_archives/RECITAL/RECITAL-2002/recital-2002-long-008},
  language  = {french},
  resume    = {La problématique de la normalisation de documents est introduite et illustrée par des exemples issus de notices pharmaceutiques. Un paradigme pour l’analyse du contenu des documents est proposé. Ce paradigme se base sur la spécification formelle de la sémantique des documents et utilise une notion de similarité floue entre les prédictions textuelles d’un générateur de texte et le texte du document à analyser. Une implémentation initiale du paradigme est présentée.},
  abstract  = {This paper discusses document normalization and gives examples based on a class of pharmaceutical documents. The discussion is based on a paradigm for document content analysis. This paradigm focusses on a formal specification of document semantics and uses a fuzzy matching measure between the textual predictions of a natural language generator and the input document. An initial implementation is presented.},
  motscles  = {analyse de contenu, génération, création assistée de documents, normalisation de document},
  keywords  = {content analysis, generation, document authoring, document normalization},
}

@inproceedings{pitel:2002:RECITAL,
  author    = {Pitel, Guillaume},
  title     = {Un Modèle Distribué d’Interprétation de Requêtes fondé sur la notion d’Observateur},
  booktitle = {Actes des 4e Rencontres des Étudiants Chercheurs en Informatique pour le Traitement Automatique des Langues},
  month     = {June},
  year      = {2002},
  address   = {Nancy, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {489--498},
  url       = {http://www.atala.org/taln_archives/RECITAL/RECITAL-2002/recital-2002-poster-001},
  language  = {french},
  resume    = {Nous proposons un modèle de conception d’agent conversationnel pour l’assistance d’interface. Notre but est d’obtenir un système d’interprétation de requêtes en contexte, générique pour l’indépendance vis-à-vis de la tâche, extensible pour sa capacité à intégrer des connaissances sur un nouveau domaine sans remettre en cause les connaissances antérieures et unifié dans le sens où tous les aspects du traitement de la langue naturelle, syntaxe, sémantique, ou pragmatique doivent s’exprimer dans un même formalisme. L’originalité de notre système est de permettre de représenter des connaissances d’interprétation de niveaux de granularité divers sous une même forme, réduisant la problématique de communication entre sources de connaissances qui existe dans les systèmes modulaires. Nous adoptons l’approche des micro-systèmes suivant laquelle l’interprétation de la langue se fait selon un processus non stratifié, et où absolument tous les niveaux peuvent interagir entre eux. Pour cela, nous introduisons et définissons un type d’entité que nous avons nommé observateur.},
  abstract  = {This paper describes a conversationnal agent design model for interface assistance. The model is meant to be generic, i.e. task independant extensible for the ability of integrating knowledge about a new task without revising previous knowledge, and unified because every level of Natural Language, from syntax, semantics, pragmatics and dialogue to production should be described within the same formalism. The main originality of the proposed system is its ability to represent in the same way various granularity levels of interpretation knowledge sources. This avoids communication problems between heterogeneous knowledge sources, as they occur in modular systems. Thus we adopt a micro-systemics approach, stating that the Natural Language Interpretation, rather than being made of multi-layered processes, a process where all the components levels can interact with each others. This is achieved by introducing the notion of observer.},
  motscles  = {Agents Conversationnels, Analyse Distribuée, Micro-Systèmes, Sémantique Procédurale},
  keywords  = {Conversationnal Agents, Distributed Analysis, Micro-Systems, Procedural Semantics},
}

@inproceedings{marchand:2002:RECITAL,
  author    = {Marchand, Murielle},
  title     = {Extraction et classification automatique de matériaux textuels pour la création de tests de langue},
  booktitle = {Actes des 4e Rencontres des Étudiants Chercheurs en Informatique pour le Traitement Automatique des Langues},
  month     = {June},
  year      = {2002},
  address   = {Nancy, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {499--506},
  url       = {http://www.atala.org/taln_archives/RECITAL/RECITAL-2002/recital-2002-poster-002},
  language  = {french},
  resume    = {Nous présentons l’état de développement d’un outil d’extraction et de classification automatique de phrases pour la création de tests de langue. Cet outil de TAL est conçu pour, dans un premier temps, localiser et extraire de larges corpus en ligne du matériel textuel (phrases) possédant des propriétés linguistiques bien spécifiques. Il permet, dans un deuxième temps, de classifier automatiquement ces phrases-candidates d’après le type d’erreurs qu’elles sont en mesure de contenir. Le développement de cet outil s’inscrit dans un contexte d’optimalisation du processus de production d’items pour les tests d’évaluation. Pour répondre aux exigences croissantes de production, les industries de développement de tests de compétences doivent être capable de développer rapidement de grandes quantités de tests. De plus, pour des raisons de sécurité, les items doivent être continuellement remplacés, ce qui crée un besoin d’approvisionnement constant. Ces exigences de production et révision sont, pour ces organisations, coûteuses en temps et en personnel. Les bénéfices à retirer du développement et de l’implantation d’un outil capable d’automatiser la majeure partie du processus de production de ces items sont par conséquents considérables.},
  abstract  = {We present here the state of development of an automatic sentence extractor and classificator for use in the creation of language tests. This NLP tool has been designed to, in a first instance, automatically locate and extract from designated on-line databases candidate source sentences meeting specific linguistic criteria and, in a second instance, to classify those sentences according to the specific types of errors they are capable of supporting. The development of this NLP tool is couched in the context and goals of automated item models instantiating for educational assessment. To meet increasing testing demands, assessment industries must be able to quickly produce large number of tests and regularly replace the items to prevent lapses in test security. The high number of items that must be continuously ‘retired’ and replaced creates a great need for continuous item supply. Those high production and revision demands, apart from being timeconsuming, are also costly. The development and implementation of a NLP tool capable of automating the bulk of the processing involved in instantiating the test item models is thus of considerable benefit to educational testing organisations.},
  motscles  = {Automates à états finis, analyse de corpus, extraction automatique de phrases, classification automatique de phrases, INTEX},
  keywords  = {Finite-state automaton, corpus analysis, automatic sentence extraction, automatic sentence classification, INTEX},
}

@inproceedings{perlerin:2002:RECITAL,
  author    = {Perlerin, Vincent},
  title     = {MemLabor, un environnement de création, de gestion et de manipulation de corpus de textes},
  booktitle = {Actes des 4e Rencontres des Étudiants Chercheurs en Informatique pour le Traitement Automatique des Langues},
  month     = {June},
  year      = {2002},
  address   = {Nancy, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {507--516},
  url       = {http://www.atala.org/taln_archives/RECITAL/RECITAL-2002/recital-2002-poster-003},
  language  = {french},
  resume    = {Nous présentons dans cet article un logiciel d’étude permettant la création, la gestion et la manipulation de corpus de textes. Ce logiciel appelé MemLabor se veut un outil ouvert et open-source adaptable à toutes les opérations possibles que l’on peut effectuer sur ce type de matériau. Dans une première partie, nous présenterons les principes généraux de l’outil. Dans une seconde, nous en proposerons une utilisation dans le cadre d’une acquisition supervisée de classes sémantiques.},
  abstract  = {In this article, we present a study software that allows creation, management and handling of corpora. This software called MemLabor is an open-source program, adaptable to all operations that we can carry out on this type of material. In the first part of this article, we will present the main principles of this tool. In the second part, we will suggest one of it use within the framework of a semantic classes supervised acquisition.},
  motscles  = {analyse de corpus, acquisition supervisée de terminologie, sémantique lexicale},
  keywords  = {corpus analysis, supervised terminology acquisition, lexical semantics},
}

@inproceedings{nouali:2002:RECITAL,
  author    = {Nouali, Omar},
  title     = {Classification Automatique de messages : une approche hybride},
  booktitle = {Actes des 4e Rencontres des Étudiants Chercheurs en Informatique pour le Traitement Automatique des Langues},
  month     = {June},
  year      = {2002},
  address   = {Nancy, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {517--522},
  url       = {http://www.atala.org/taln_archives/RECITAL/RECITAL-2002/recital-2002-poster-004},
  language  = {french},
  resume    = {Les systèmes actuels de filtrage de l’information sont basés d’une façon directe ou indirecte sur les techniques traditionnelles de recherche d’information (Malone, Kenneth, 1987), (Kilander, Takkinen, 1996). Notre approche consiste à séparer le processus de classification du filtrage proprement dit. Il s’agit d’effectuer un traitement reposant sur une compréhension primitive du message permettant d’effectuer des opérations de classement. Cet article décrit une solution pour classer des messages en se basant sur les propriétés linguistiques véhiculées par ces messages. Les propriétés linguistiques sont modélisées par un réseau de neurone. A l'aide d'un module d'apprentissage, le réseau est amélioré progressivement au fur et à mesure de son utilisation. Nous présentons à la fin les résultats d’une expérience d’évaluation.},
  abstract  = {The current approaches in information filtering are based directly or indirectly on the traditional methods of information retrieval (Malone, Kenneth, 1987), (Kilander, Takkinen, 1996). Our approach to email filtering is to separate classification from filtering. Once the classification process is complete, the filtering takes place. This paper presents an approach to classify e-mail, based on linguistic features model. The main feature of the model is its representation by a neural network and its learning capability. At the end, to measure the approach performances, we illustrate and discuss the results obtained by experimental evaluations.},
  motscles  = {Filtrage d’information, e-mail, classification de messages, propriétés linguistiques, réseaux de neurones, filtrage d’email},
  keywords  = {Information filtering, e-mail, information classification, linguistic proprieties, neural network, Filters, e-mail filtering},
}