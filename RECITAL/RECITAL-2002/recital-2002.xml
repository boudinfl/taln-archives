<?xml version="1.0" encoding="UTF-8"?>
<conference>
	<edition>
		<acronyme>RECITAL'2002</acronyme>
		<titre>4e Rencontres des Étudiants Chercheurs en Informatique pour le Traitement Automatique des Langues</titre>
		<ville>Nancy</ville>
		<pays>France</pays>
		<dateDebut>2002-06-24</dateDebut>
		<dateFin>2002-06-27</dateFin>
		<presidents>
			<nom>Azim Roussanaly</nom>
		</presidents>
		<typeArticles>
			<type id="long">Papiers longs</type>
			<type id="poster">Posters</type>
		</typeArticles>
		<statistiques>
			<acceptations id="long" soumissions="18">8</acceptations>
			<acceptations id="poster" soumissions="18">4</acceptations>
		</statistiques>
		<siteWeb>http://www.loria.fr/projets/JEP-TALN/TALN/</siteWeb>
		<meilleurArticle>
			<!-- <articleId></articleId> -->
		</meilleurArticle>
	</edition>
	<articles>
		<article id="recital-2002-long-001" session="Plénière">
			<auteurs>
				<auteur>
					<nom>Delphine Reymond</nom>
					<email>Delphine.Reymond@up.univ-aix.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Équipe DELIC – Université de Provence 29, Av. Robert Schuman, 13621 Aix-en-Provence</affiliation>
			</affiliations>
			<titre>Méthodologie pour la création d’un dictionnaire distributionnel dans une perspective d’étiquetage lexical semi-automatique</titre>
			<type>long</type>
			<pages>405-414</pages>
			<resume>Des groupes de recherche de plus en plus nombreux s'intéressent à l’étiquetage lexical ou la désambiguïsation du sens. La tendance actuelle est à l’exploitation de très grands corpus de textes qui, grâce à l'utilisation d’outils lexicographiques appropriés, peuvent fournir un ensemble de données initiales aux systèmes. A leur tour ces systèmes peuvent être utilisés pour extraire plus d'informations des corpus, qui peuvent ensuite être réinjectées dans les systèmes, dans un processus récursif. Dans cet article, nous présentons une méthodologie qui aborde la résolution de l’ambiguïté lexicale comme le résultat de l’interaction de divers indices repérables de manière semi-automatique au niveau syntaxique (valence), sémantique (collocations, classes d’objets) avec la mise en oeuvre de tests manuels.</resume>
			<mots_cles>Désambiguïsation lexicale, dictionnaire, propriétés distributionnelles, collocations, classes d’objets</mots_cles>
			<title></title>
			<abstract>More and more research groups are involved in sense tagging or sense disambiguation. The current trend is to use very large text corpora which, with the help of appropriate lexicographical tools, can provide initial data to the disambiguation systems. In turn, these systems can be used to extract more data from corpora, which can be fed again to the systems, in a bootstrapping process. In this paper, we tackle lexical disambiguation through the interaction of various cues which can be detected semi-automatically at the syntactic and semantic levels (valency, collocations, object classes), along with manual tests.</abstract>
			<keywords>WSD, dictionary, distributional properties, collocations, classes of objects</keywords>
		</article>
		<article id="recital-2002-long-002" session="Plénière">
			<auteurs>
				<auteur>
					<nom>Laurent Audibert</nom>
					<email>laurent.audibert@up.univ-aix.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Jeune équipe DELIC – Université de Provence 29 Avenue Robert SCHUMAN 13621 Aix-en-Provence Cedex 1</affiliation>
			</affiliations>
			<titre>Etude des critères de désambiguïsation sémantique automatique : présentation et premiers résultats sur les cooccurrences</titre>
			<type>long</type>
			<pages>415-424</pages>
			<resume>Nous présentons dans cet article les débuts d’un travail visant à rechercher et à étudier systématiquement les critères de désambiguïsation sémantique automatique. Cette étude utilise un corpus français étiqueté sémantiquement dans le cadre du projet SyntSem. Le critère ici étudié est celui des cooccurrences. Nous présentons une série de résultats sur le pouvoir désambiguïsateur des cooccurrences en fonction de leur catégorie grammaticale et de leur éloignement du mot à désambiguïser.</resume>
			<mots_cles>Désambiguïsation sémantique, corpus sémantiquement étiqueté, cooccurrences</mots_cles>
			<title></title>
			<abstract>This paper describes the beginning of a study which aims at researching and systematically analysing criteria for automatic word sense disambiguation. This study uses a French sense tagged corpus developed in the SyntSem project. We analyse here the cooccurrence criterion and report a series of results on the disambiguative power of cooccurrences with respect to their grammatical category and distance from the word to be disambiguated.</abstract>
			<keywords>Word sense disambiguation, sense tagged corpora, cooccurrences</keywords>
		</article>
		<article id="recital-2002-long-003" session="Plénière">
			<auteurs>
				<auteur>
					<nom>Antonio Balvet</nom>
					<email>antonio.balvet@u-paris10.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université Paris X Nanterre / UMR MoDyCo 200, Avenue de la République 92 001 Nanterre</affiliation>
			</affiliations>
			<titre>LIZARD, un assistant pour le développement de ressources linguistiques à base de cascades de transducteurs</titre>
			<type>long</type>
			<pages>425-434</pages>
			<resume>Nous présentons un outil visant à assister les développeurs de ressources linguistiques en automatisant la fouille de corpus. Cet outil, est guidé par les principes de l’analyse distributionnelle sur corpus spécialisés, étendue grâce à des ressources lexicales génériques. Nous présentons une évaluation du gain de performances dû à l’intégration de notre outil à une application de filtrage d’information et nous élargissons le champ d’application de l’assistant aux études sur corpus menées à l’aide de cascades de transducteurs à états finis.</resume>
			<mots_cles>TALN, assistant linguistique, ressources linguistiques, agents autonomes, études sur corpus</mots_cles>
			<title></title>
			<abstract>We present a tool providing linguistic resources developers with automated corpus analysis features. Our tool implements specialized corpora distributional analysis principles, extended by the integration of external generic lexical resources. We present an evaluation of the gain in performance attributable to our tool, for a text filtering task. We also widen our tool’s scope of applications to transducer cascades-based corpus processing activities.</abstract>
			<keywords>NLP, linguistic wizard, linguistic resources, autonomous agents, corpus-based studies</keywords>
		</article>
		<article id="recital-2002-long-004" session="Plénière">
			<auteurs>
				<auteur>
					<nom>Djamé Seddah</nom>
					<email>seddah@loria.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Evelyne Jacquey</nom>
					<email>jacquey@loria.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Laboratoire LORIA, Equipe Langue et Dialogue, Campus Scientifique, BP 239 F-54506 Vandoeuvre-lès-Nancy Cedex</affiliation>
			</affiliations>
			<titre>Conceptualisation d’un système d’informations lexicales, une interface paramétrable pour le T.A.L</titre>
			<type>long</type>
			<pages>435-444</pages>
			<resume>La nécessité de ressources lexicales normalisées et publiques est avérée dans le domaine du TAL. Cet article vise à montrer comment, sur la base d’une partie du lexique MULTEXT disponible sur le serveur ABU, il serait possible de construire une architecture permettant tout à la fois l’accès aux ressources avec des attentes différentes (lemmatiseur, parseur, extraction d’informations, prédiction, etc.) et la mise à jour par un groupe restreint de ces ressources. Cette mise à jour consistant en l’intégration et la modification, automatique ou manuelle, de données existantes. Pour ce faire, nous cherchons à prendre en compte à la fois les besoins et les données accessibles. Ce modèle est évalué conceptuellement dans un premier temps en fonction des systèmes utilisés dans notre équipe : un analyseur TAG, un constructeur de grammaires TAGs, un extracteur d’information.</resume>
			<mots_cles>Bases de données, ressources lexicales</mots_cles>
			<title></title>
			<abstract>Lexical ressources which would be normalized and freely accessible is a major issue in the NLP research area. This article aims to show how to built an information system which allow (1) a freely access for distinct NLP systems (tagging, parsing, information extraction, etc.) and (2) an easy update of data by a restricted team of researchers, this update being manual or computed. Starting with a a subset of the MULTEXT lexicon which is accessible from the server ABU, we aim to take into account the various needs and the variability of accessible lexical data. Our modelisation is evaluated with three existing systems of our team : EGAL (parsing), a builder of Tag grammars and VULCAIN (information extraction).</abstract>
			<keywords>Data Bases, Lexical Ressources</keywords>
		</article>
		<article id="recital-2002-long-005" session="Plénière">
			<auteurs>
				<auteur>
					<nom>Bruno Bossard</nom>
					<email>Bruno.Bossard@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIMSI-CNRS - Université Paris XI, Orsay Bât 508 BP133 91403 Orsay CEDEX</affiliation>
			</affiliations>
			<titre>Problèmes posés par la reconnaissance de gestes en Langue des Signes</titre>
			<type>long</type>
			<pages>445-454</pages>
			<resume>Le but de cet article est d’expliciter certains des problèmes rencontrés lorsque l’on cherche à concevoir un système de reconnaissance de gestes de la Langue des Signes et de proposer des solutions adaptées. Les trois aspects traités ici concernent la simultanéïté d’informations véhiculées par les gestes des mains, la synchronisation éventuelle entre les deux mains et le fait que différentes classes de signes peuvent se rencontrer dans une phrase en Langue des Signes.</resume>
			<mots_cles>Reconnaissance de gestes, Langue des Signes</mots_cles>
			<title></title>
			<abstract>The aim of this paper is to specify some of the problems raised when one wants to design a gesture recognition system dedicated to Sign Language, and to propose suited solutions. The three topics considered here concern the simultaneity of information conveyed by hand gestures, the possible synchronicity between the two hands, and the different classes of signs that may be encountered in a Sign Language sentence.</abstract>
			<keywords>Gesture recognition, Sign Language</keywords>
		</article>
		<article id="recital-2002-long-006" session="Plénière">
			<auteurs>
				<auteur>
					<nom>Hélène Manuélian</nom>
					<email>helene.manuelian@loria.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LORIA Campus Scientifique BP 239 54 506 Vandoeuvre Lès Nancy</affiliation>
			</affiliations>
			<titre>Annotation des descriptions définies : le cas des reprises par les rôles thématiques</titre>
			<type>long</type>
			<pages>455-466</pages>
			<resume>Nous présentons dans cet article un cas particulier de description définie où la description reprend le rôle thématique d’un argument (implicite ou explicite) d’un événement mentionné dans le contexte linguistique. Nous commençons par montrer que les schémas d’annotation proposés (MATE) et utilisés (Poesio et Vieira 2000) ne permettent pas une caractérisation uniforme ni, partant, un repérage facile de ces reprises. Nous proposons une extension du schéma MATE qui pallie cette difficulté.</resume>
			<mots_cles>Génération automatique de textes, annotation, coréférence, rôles thématiques</mots_cles>
			<title></title>
			<abstract>We present in this paper a particular kind of definite description which mentions the thematic role of an (implicit or explicit) argument of an event mentioned in the context. We show that the annotation schemes which are either proposed (MATE) or used (Poesio and Vieira 2000) permit neither a uniform characterisation of the phenomenon nor an easy identification of these anaphors. We propose an extension of the MATE schemes which remedy this difficulty.</abstract>
			<keywords>Natural language generation, annotation, coreference, thematic role</keywords>
		</article>
		<article id="recital-2002-long-007" session="Plénière">
			<auteurs>
				<auteur>
					<nom>Sandrine Henry</nom>
					<email>sandrine_henry@hotmail.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Équipe DELIC – Université de Provence 29, Av. Robert Schuman, 13621 Aix-en-Provence Cedex 1</affiliation>
			</affiliations>
			<titre>Etude des répétitions en français parlé spontané pour les technologies de la parole</titre>
			<type>long</type>
			<pages>467-476</pages>
			<resume>Cet article rapporte les résultats d’une étude quantitative des répétitions menée à partir d’un corpus de français parlé spontané d’un million de mots, étude réalisée dans le cadre de notre première année de thèse. L’étude linguistique pourra aider à l’amélioration des systèmes de reconnaissance de la parole et de l'étiquetage grammatical automatique de corpus oraux. Ces technologies impliquent la prise en compte et l’étude des répétitions de performance (en opposition aux répétitions de compétence, telles que nous nous sujet + complément) afin de pouvoir, par la suite, les « gommer » avant des traitements ultérieurs. Nos résultats montrent que les répétitions de performance concernent principalement les mots-outils et apparaissent à des frontières syntaxiques majeures.</resume>
			<mots_cles>Répétitions, français parlé spontané, « disfluences », phénomènes de performance, étude quantitative, reconnaissance de la parole, étiquetage morpho-syntaxique</mots_cles>
			<title></title>
			<abstract>This article is a report of a quantitative study of repetitions based on a corpus of a onemillion- word spontaneous spoken French, conducted during the first year of our PhD thesis. This linguistic study can contribute to the improvement of speech recognition and spoken French part-of-speech tagging. Improvement of these technologies requires taking into account and studying performance repetitions (such as complement + complement nous nous) in order to be able to "erase" them before further processing. Our results show that repetitions mainly involve function words and take place at major syntactic boundaries.</abstract>
			<keywords>Repetitions, spontaneous French speech, disfluencies, performance phenomena, quantitative study, speech recognition, part-of-speech tagging</keywords>
		</article>
		<article id="recital-2002-long-008" session="Plénière">
			<auteurs>
				<auteur>
					<nom>Aurélien Max</nom>
					<email>aurelien.max@imag.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Groupe d’Etude pour la Traduction Automatique (GETA CLIPS-IMAG) Xerox Research Centre Europe (XRCE) Grenoble, France</affiliation>
			</affiliations>
			<titre>Normalisation de documents par analyse du contenu à l’aide d’un modèle sémantique et d’un générateur</titre>
			<type>long</type>
			<pages>477-486</pages>
			<resume>La problématique de la normalisation de documents est introduite et illustrée par des exemples issus de notices pharmaceutiques. Un paradigme pour l’analyse du contenu des documents est proposé. Ce paradigme se base sur la spécification formelle de la sémantique des documents et utilise une notion de similarité floue entre les prédictions textuelles d’un générateur de texte et le texte du document à analyser. Une implémentation initiale du paradigme est présentée.</resume>
			<mots_cles>analyse de contenu, génération, création assistée de documents, normalisation de document</mots_cles>
			<title></title>
			<abstract>This paper discusses document normalization and gives examples based on a class of pharmaceutical documents. The discussion is based on a paradigm for document content analysis. This paradigm focusses on a formal specification of document semantics and uses a fuzzy matching measure between the textual predictions of a natural language generator and the input document. An initial implementation is presented.</abstract>
			<keywords>content analysis, generation, document authoring, document normalization</keywords>
		</article>
		<article id="recital-2002-poster-001" session="Posters">
			<auteurs>
				<auteur>
					<nom>Guillaume Pitel</nom>
					<email>pitel@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIMSI-CNRS F-91403 Orsay CEDEX</affiliation>
			</affiliations>
			<titre>Un Modèle Distribué d’Interprétation de Requêtes fondé sur la notion d’Observateur</titre>
			<type>poster</type>
			<pages>489-498</pages>
			<resume>Nous proposons un modèle de conception d’agent conversationnel pour l’assistance d’interface. Notre but est d’obtenir un système d’interprétation de requêtes en contexte, générique pour l’indépendance vis-à-vis de la tâche, extensible pour sa capacité à intégrer des connaissances sur un nouveau domaine sans remettre en cause les connaissances antérieures et unifié dans le sens où tous les aspects du traitement de la langue naturelle, syntaxe, sémantique, ou pragmatique doivent s’exprimer dans un même formalisme. L’originalité de notre système est de permettre de représenter des connaissances d’interprétation de niveaux de granularité divers sous une même forme, réduisant la problématique de communication entre sources de connaissances qui existe dans les systèmes modulaires. Nous adoptons l’approche des micro-systèmes suivant laquelle l’interprétation de la langue se fait selon un processus non stratifié, et où absolument tous les niveaux peuvent interagir entre eux. Pour cela, nous introduisons et définissons un type d’entité que nous avons nommé observateur.</resume>
			<mots_cles>Agents Conversationnels, Analyse Distribuée, Micro-Systèmes, Sémantique Procédurale</mots_cles>
			<title></title>
			<abstract>This paper describes a conversationnal agent design model for interface assistance. The model is meant to be generic, i.e. task independant extensible for the ability of integrating knowledge about a new task without revising previous knowledge, and unified because every level of Natural Language, from syntax, semantics, pragmatics and dialogue to production should be described within the same formalism. The main originality of the proposed system is its ability to represent in the same way various granularity levels of interpretation knowledge sources. This avoids communication problems between heterogeneous knowledge sources, as they occur in modular systems. Thus we adopt a micro-systemics approach, stating that the Natural Language Interpretation, rather than being made of multi-layered processes, a process where all the components levels can interact with each others. This is achieved by introducing the notion of observer.</abstract>
			<keywords>Conversationnal Agents, Distributed Analysis, Micro-Systems, Procedural Semantics</keywords>
		</article>
		<article id="recital-2002-poster-002" session="Posters">
			<auteurs>
				<auteur>
					<nom>Murielle Marchand</nom>
					<email>marchand@tedm.ucl.ac.be</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Centre de traitement électronique des documents - CETEDOC Université catholique de Louvain Place Blaise Pascal, 1 1348 Louvain-la-Neuve</affiliation>
			</affiliations>
			<titre>Extraction et classification automatique de matériaux textuels pour la création de tests de langue</titre>
			<type>poster</type>
			<pages>499-506</pages>
			<resume>Nous présentons l’état de développement d’un outil d’extraction et de classification automatique de phrases pour la création de tests de langue. Cet outil de TAL est conçu pour, dans un premier temps, localiser et extraire de larges corpus en ligne du matériel textuel (phrases) possédant des propriétés linguistiques bien spécifiques. Il permet, dans un deuxième temps, de classifier automatiquement ces phrases-candidates d’après le type d’erreurs qu’elles sont en mesure de contenir. Le développement de cet outil s’inscrit dans un contexte d’optimalisation du processus de production d’items pour les tests d’évaluation. Pour répondre aux exigences croissantes de production, les industries de développement de tests de compétences doivent être capable de développer rapidement de grandes quantités de tests. De plus, pour des raisons de sécurité, les items doivent être continuellement remplacés, ce qui crée un besoin d’approvisionnement constant. Ces exigences de production et révision sont, pour ces organisations, coûteuses en temps et en personnel. Les bénéfices à retirer du développement et de l’implantation d’un outil capable d’automatiser la majeure partie du processus de production de ces items sont par conséquents considérables.</resume>
			<mots_cles>Automates à états finis, analyse de corpus, extraction automatique de phrases, classification automatique de phrases, INTEX</mots_cles>
			<title></title>
			<abstract>We present here the state of development of an automatic sentence extractor and classificator for use in the creation of language tests. This NLP tool has been designed to, in a first instance, automatically locate and extract from designated on-line databases candidate source sentences meeting specific linguistic criteria and, in a second instance, to classify those sentences according to the specific types of errors they are capable of supporting. The development of this NLP tool is couched in the context and goals of automated item models instantiating for educational assessment. To meet increasing testing demands, assessment industries must be able to quickly produce large number of tests and regularly replace the items to prevent lapses in test security. The high number of items that must be continuously ‘retired’ and replaced creates a great need for continuous item supply. Those high production and revision demands, apart from being timeconsuming, are also costly. The development and implementation of a NLP tool capable of automating the bulk of the processing involved in instantiating the test item models is thus of considerable benefit to educational testing organisations.</abstract>
			<keywords>Finite-state automaton, corpus analysis, automatic sentence extraction, automatic sentence classification, INTEX</keywords>
		</article>
		<article id="recital-2002-poster-003" session="Posters">
			<auteurs>
				<auteur>
					<nom>Vincent Perlerin</nom>
					<email>perlerin@info.unicaen.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">GREYC – CRNS UMR 6072 – Université de Caen Campus II – BP 5186 – F14032 Caen Cedex</affiliation>
			</affiliations>
			<titre>MemLabor, un environnement de création, de gestion et de manipulation de corpus de textes</titre>
			<type>poster</type>
			<pages>507-516</pages>
			<resume>Nous présentons dans cet article un logiciel d’étude permettant la création, la gestion et la manipulation de corpus de textes. Ce logiciel appelé MemLabor se veut un outil ouvert et open-source adaptable à toutes les opérations possibles que l’on peut effectuer sur ce type de matériau. Dans une première partie, nous présenterons les principes généraux de l’outil. Dans une seconde, nous en proposerons une utilisation dans le cadre d’une acquisition supervisée de classes sémantiques.</resume>
			<mots_cles>analyse de corpus, acquisition supervisée de terminologie, sémantique lexicale</mots_cles>
			<title></title>
			<abstract>In this article, we present a study software that allows creation, management and handling of corpora. This software called MemLabor is an open-source program, adaptable to all operations that we can carry out on this type of material. In the first part of this article, we will present the main principles of this tool. In the second part, we will suggest one of it use within the framework of a semantic classes supervised acquisition.</abstract>
			<keywords>corpus analysis, supervised terminology acquisition, lexical semantics</keywords>
		</article>
		<article id="recital-2002-poster-004" session="Posters">
			<auteurs>
				<auteur>
					<nom>Omar Nouali</nom>
					<email>onouali@mail.cerist.dz</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Laboratoire des Logiciels de base, CE.R.I.S.T, Rue des 3 frères Aïssiou, Ben Aknoun, Alger, Algérie</affiliation>
				<affiliation affiliationId="2">LPL- Université de Provence 29, Av. Robert Schuman, F-13621 Aix-en-Provence, France</affiliation>
			</affiliations>
			<titre>Classification Automatique de messages : une approche hybride</titre>
			<type>poster</type>
			<pages>517-522</pages>
			<resume>Les systèmes actuels de filtrage de l’information sont basés d’une façon directe ou indirecte sur les techniques traditionnelles de recherche d’information (Malone, Kenneth, 1987), (Kilander, Takkinen, 1996). Notre approche consiste à séparer le processus de classification du filtrage proprement dit. Il s’agit d’effectuer un traitement reposant sur une compréhension primitive du message permettant d’effectuer des opérations de classement. Cet article décrit une solution pour classer des messages en se basant sur les propriétés linguistiques véhiculées par ces messages. Les propriétés linguistiques sont modélisées par un réseau de neurone. A l'aide d'un module d'apprentissage, le réseau est amélioré progressivement au fur et à mesure de son utilisation. Nous présentons à la fin les résultats d’une expérience d’évaluation.</resume>
			<mots_cles>Filtrage d’information, e-mail, classification de messages, propriétés linguistiques, réseaux de neurones, filtrage d’email</mots_cles>
			<title></title>
			<abstract>The current approaches in information filtering are based directly or indirectly on the traditional methods of information retrieval (Malone, Kenneth, 1987), (Kilander, Takkinen, 1996). Our approach to email filtering is to separate classification from filtering. Once the classification process is complete, the filtering takes place. This paper presents an approach to classify e-mail, based on linguistic features model. The main feature of the model is its representation by a neural network and its learning capability. At the end, to measure the approach performances, we illustrate and discuss the results obtained by experimental evaluations.</abstract>
			<keywords>Information filtering, e-mail, information classification, linguistic proprieties, neural network, Filters, e-mail filtering</keywords>
		</article>
	</articles>
</conference>