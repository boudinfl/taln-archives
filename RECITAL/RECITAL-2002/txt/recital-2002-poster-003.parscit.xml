<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Auroux</author>
</authors>
<title>Le langage, la raison et les normes,</title>
<date>1999</date>
<contexts>
<context position="13371" citStr="Auroux, 1999" startWordPosition="2009" endWordPosition="2010">ori, c&apos;est-à-dire aboutir à une sémantique légère. Nous nous plaçons dans une approche anthropocentrée où la machine se construit autour des besoins de l&apos;utilisateur (Thlivitis, 1998), et nous revendiquons une approche praxéologique de l&apos;activité langagière (sémantique tournée vers la pratique de la langue par un individu ou un groupe restreint d&apos;individus). De plus, nous désirons constuire une sémantique lexicale intra-linguistique textuellement située (à l’opposé d’une sémantique purement référentielle qui étudierait les rapports des expressions au monde - selon la différence soulignée par (Auroux, 1999 p.38). Le logiciel décrit dans cet article et l&apos;exemple d&apos;application s&apos;inscrivent donc pleinement dans ce cadre. La réalisation de MemLabor est consécutive à une étude semi-manuelle débutée en 2000 dans le cadre d’un projet visant à proposer des outils de filtrage et de réordonnancement de résultats de systèmes documentaires classiques (moteurs de recherche du Web) en fonction de ressources sémantiques fournies par l’utilisateur (Perlerin, 2001). Cette étude semi-manuelle de 1783 dépêches journalistiques (Corpus Reuter) avait montré que les pourcentages de cooccurrences en nombre de fichiers</context>
</contexts>
<marker>Auroux, 1999</marker>
<rawString>Auroux S. (1999), Le langage, la raison et les normes, Paris, PUF. Beust P. (1998) Contribution à un modèle interactionniste du sens, Thèse de Doctorat en Informatique de l’Université de Caen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Bourigault</author>
</authors>
<title>Lexter, un logiciel d&apos;extraction de terminologie. Application à l&apos;acquisition des connaissances à partir de textes. Thèse en informatique linguistique,</title>
<date>1994</date>
<booktitle>Ecole des hautes Etudes en Sciences Sociales,</booktitle>
<location>Paris.</location>
<contexts>
<context position="15897" citStr="Bourigault, 1994" startWordPosition="2387" endWordPosition="2388">t de structurer des ensembles de mots appartenant à un même thème en formant des sous-ensembles au sein desquels on peut marquer leur différence - voir le modèle Anadia (Nicolle et al., 2002). Notre but est de fournir aux utilisateurs une aide à la constitution de telles ressources. Deux tâches sont à réaliser : l’extraction des candidats termes et le classement de ces candidats aux seins de sous-ensembles. Dans la littérature, on trouve de nombreuses techniques d’acquisition de terminologie : en fonction de critères principalement morphosyntaxique (TERMINO [David et Plante, 1990], et LEXTER [Bourigault, 1994], …), en fonction de critères principalement statistiques (ANA [Enguehard, 1993], [Riloff et Shreperd, 1997],…), ou encore en fonction de marqueurs a priori (SEEK de Christophe Jouis, COATIS de Daniela Garcia). Désirant restreindre au maximum la quantité de ressources nécessaires au traitement et placer l’utilisateur au cœur du système, nous avons opté pour l’extractions des mots du domaine pour une technique statistique simple. Nous nous basons sur un calcul de type Zipf filtré à l’aide d’une liste de mots sémantiquement limités. En effet, lors du repérage des lexies mises en jeu dans les do</context>
</contexts>
<marker>Bourigault, 1994</marker>
<rawString>Bourigault D. (1994), Lexter, un logiciel d&apos;extraction de terminologie. Application à l&apos;acquisition des connaissances à partir de textes. Thèse en informatique linguistique, Ecole des hautes Etudes en Sciences Sociales, Paris.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Coursil</author>
</authors>
<title>La fonction muette du langage - Essai de linguistique générale contemporaine, Editions Ibis Rouge.</title>
<date>2000</date>
<contexts>
<context position="14209" citStr="Coursil, 2000" startWordPosition="2132" endWordPosition="2133">t visant à proposer des outils de filtrage et de réordonnancement de résultats de systèmes documentaires classiques (moteurs de recherche du Web) en fonction de ressources sémantiques fournies par l’utilisateur (Perlerin, 2001). Cette étude semi-manuelle de 1783 dépêches journalistiques (Corpus Reuter) avait montré que les pourcentages de cooccurrences en nombre de fichiers à l’intérieur d’un corpus homogène (Figure 5) pouvaient être un indice valable pour le choix de lexies appartenant à des thèmes proches en vue de construire des systèmes de classes de catégorisation selon le modèle Anadia (Coursil, 2000)(Beust, 1998). Cette observation peut être partiellement expliquée par la notion d’isotopie (récurrence syntagmatique d’un même trait sémantique) support crucial du sens dans les textes. Les lexies candidates à une même classe de catégorisation partagent en effet un certain nombre de traits génériques (Rastier, 1994) dont la redondance au sein des entités textuelles conditionne le sens. C’est la détermination du global sur le local. MemLabor, un outil de création, de gestion et de manipulation de corpus de textes 3.2 Aide pour la constitution de classes de catégorisation sémantique Nos recherc</context>
</contexts>
<marker>Coursil, 2000</marker>
<rawString>Coursil J. (2000), La fonction muette du langage - Essai de linguistique générale contemporaine, Editions Ibis Rouge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S David</author>
<author>P Plante</author>
</authors>
<title>De la nécessité d&apos;une approche morpho-syntaxique dans l&apos;analyse de textes,</title>
<date>1990</date>
<pages>2--3</pages>
<marker>David, Plante, 1990</marker>
<rawString>David S, Plante P. (1990). De la nécessité d&apos;une approche morpho-syntaxique dans l&apos;analyse de textes, ICO, 2(3):140-154.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Enguehard</author>
</authors>
<title>Acquisition de terminologie à partir de gros corpus,</title>
<date>1993</date>
<booktitle>Informatique &amp; Langue Naturelle, ILN&apos;93,</booktitle>
<pages>373--384</pages>
<location>Nantes,.</location>
<contexts>
<context position="15977" citStr="Enguehard, 1993" startWordPosition="2397" endWordPosition="2398">sous-ensembles au sein desquels on peut marquer leur différence - voir le modèle Anadia (Nicolle et al., 2002). Notre but est de fournir aux utilisateurs une aide à la constitution de telles ressources. Deux tâches sont à réaliser : l’extraction des candidats termes et le classement de ces candidats aux seins de sous-ensembles. Dans la littérature, on trouve de nombreuses techniques d’acquisition de terminologie : en fonction de critères principalement morphosyntaxique (TERMINO [David et Plante, 1990], et LEXTER [Bourigault, 1994], …), en fonction de critères principalement statistiques (ANA [Enguehard, 1993], [Riloff et Shreperd, 1997],…), ou encore en fonction de marqueurs a priori (SEEK de Christophe Jouis, COATIS de Daniela Garcia). Désirant restreindre au maximum la quantité de ressources nécessaires au traitement et placer l’utilisateur au cœur du système, nous avons opté pour l’extractions des mots du domaine pour une technique statistique simple. Nous nous basons sur un calcul de type Zipf filtré à l’aide d’une liste de mots sémantiquement limités. En effet, lors du repérage des lexies mises en jeu dans les documents d’un corpus homogène, les graphiques en histogrammes obtenus présentent </context>
</contexts>
<marker>Enguehard, 1993</marker>
<rawString>Enguehard, C (1993). Acquisition de terminologie à partir de gros corpus, Informatique &amp; Langue Naturelle, ILN&apos;93, Nantes,. 373-384.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Nicolle</author>
<author>P Beust</author>
<author>V Perlerin</author>
</authors>
<title>Un analogue de la mémoire pour un agent logiciel interactif,</title>
<date>2002</date>
<booktitle>In Cognito N°21,</booktitle>
<pages>37--66</pages>
<contexts>
<context position="15472" citStr="Nicolle et al., 2002" startWordPosition="2322" endWordPosition="2325">la cohésion textuelle. A partir d’un corpus homogène, il s’agit ici de constituer des classes de catégorisation sémantiques au sens des taxèmes de la sémantique interprétative (Rastier, 1994) : « structure paradigmatique constituée par des unités lexicales se partageant une zone commune de signification et se trouvant en opposition immédiate les unes avec les autres ». En d’autres termes et dans un cadre différentiel, il s’agit de structurer des ensembles de mots appartenant à un même thème en formant des sous-ensembles au sein desquels on peut marquer leur différence - voir le modèle Anadia (Nicolle et al., 2002). Notre but est de fournir aux utilisateurs une aide à la constitution de telles ressources. Deux tâches sont à réaliser : l’extraction des candidats termes et le classement de ces candidats aux seins de sous-ensembles. Dans la littérature, on trouve de nombreuses techniques d’acquisition de terminologie : en fonction de critères principalement morphosyntaxique (TERMINO [David et Plante, 1990], et LEXTER [Bourigault, 1994], …), en fonction de critères principalement statistiques (ANA [Enguehard, 1993], [Riloff et Shreperd, 1997],…), ou encore en fonction de marqueurs a priori (SEEK de Christop</context>
<context position="19218" citStr="Nicolle et al., 2002" startWordPosition="2887" endWordPosition="2890">bération sur le procès Microsoft en 2001 (la lexie souris est absente du corpus). Soit Li, la lexie i et P(Li,Lj) le pourcentage en nombre de fichiers au sein du corpus contenant Li où Lj apparaît5. Si les lexies L1, L2, L3 …ont été sélectionnées comme candidates à une même classe sémantique, le logiciel proposera en fonction d’une table de résultats identique à celle présentée dans la Figure 7, une liste de lexies Li classées en fonction du produit P(L1, Li)xP(L2,Li)… le facteur multiplicateur permettant de rendre compte des proportions de cooccurrence. Une première expérience (décrite dans (Nicolle et al., 2002)) sur le corpus Reuter a montré que les résultats ainsi obtenus plaçaient les mots initialement choisis pour être candidats à une même classe sémantique parmi approximativement les 15 premières places de la liste. Dans le cas du corpus utilisé pour cet article (corpus Libération – cf. Figure 7), la Figure 8 présente la liste obtenue pour les lexies déjà candidates à une même classe : entreprise et firme (le tableau de cooccurrences ayant été calculé pour les 60 premières lexies de la liste obtenue suite au calcul de type Zipf6). 5 dans la Figure 7, P(microsoft,linux) = 19,64 % et P(linux,micro</context>
</contexts>
<marker>Nicolle, Beust, Perlerin, 2002</marker>
<rawString>Nicolle A, Beust P, Perlerin V. (2002), Un analogue de la mémoire pour un agent logiciel interactif, In Cognito N°21, 37-66.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Perlerin</author>
</authors>
<title>La recherche documentaire : une activité langagière, Actes de TALNRECITAL</title>
<date>2001</date>
<pages>469--479</pages>
<contexts>
<context position="13822" citStr="Perlerin, 2001" startWordPosition="2074" endWordPosition="2075">nt située (à l’opposé d’une sémantique purement référentielle qui étudierait les rapports des expressions au monde - selon la différence soulignée par (Auroux, 1999 p.38). Le logiciel décrit dans cet article et l&apos;exemple d&apos;application s&apos;inscrivent donc pleinement dans ce cadre. La réalisation de MemLabor est consécutive à une étude semi-manuelle débutée en 2000 dans le cadre d’un projet visant à proposer des outils de filtrage et de réordonnancement de résultats de systèmes documentaires classiques (moteurs de recherche du Web) en fonction de ressources sémantiques fournies par l’utilisateur (Perlerin, 2001). Cette étude semi-manuelle de 1783 dépêches journalistiques (Corpus Reuter) avait montré que les pourcentages de cooccurrences en nombre de fichiers à l’intérieur d’un corpus homogène (Figure 5) pouvaient être un indice valable pour le choix de lexies appartenant à des thèmes proches en vue de construire des systèmes de classes de catégorisation selon le modèle Anadia (Coursil, 2000)(Beust, 1998). Cette observation peut être partiellement expliquée par la notion d’isotopie (récurrence syntagmatique d’un même trait sémantique) support crucial du sens dans les textes. Les lexies candidates à un</context>
</contexts>
<marker>Perlerin, 2001</marker>
<rawString>Perlerin V (2001), La recherche documentaire : une activité langagière, Actes de TALNRECITAL 2001, 469-479.</rawString>
</citation>
<citation valid="false">
<authors>
<author>F Rastier</author>
</authors>
<title>Eléments de théorie des genres, texte diffusé sur la liste fermée Sémantique des textes,</title>
<date>2001</date>
<booktitle>Proceedings of the Second Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>117--124</pages>
<editor>In Cardie, C. et Weischedel, R., editors,</editor>
<publisher>ACL, Somerset,</publisher>
<location>New Jersey.</location>
<note>http://www.atala.org/je/010428/Rastier/Rastier280401.html Riloff</note>
<contexts>
<context position="22608" citStr="Rastier, 2001" startWordPosition="3398" endWordPosition="3399">ie, …) est une piste intéressante de recherche pour aider à la constition de telles ressources. 4 Conclusion Dans cette article, nous avons proposé une plateforme de gestion de corpus pour le TAL. Le logiciel MemLabor s’inscrit dans une approche coopérative de la recherche en TAL en permettant l’échange de corpus ou de résultats obtenus sur ces corpus par l’intermédiaire d’une DTD XML. Le caractère open-source du programme en assure aussi sa possible d’évolution. Cette ressource logicielle exploite et rend compte de la dimension intertextuelle des documents. Des études linguistiques récentes (Rastier, 2001) montrent l’importance dans le processus d’interprétation de la place d’un document dans un ensemble saisi par le lecteur 7 des résultats sur des documents segmentés (pourcentages de cooccurrences dans les segments de documents) sont disponibles sur www.info.unicaen.fr/~perlerin 8 Association pour la recherche cognitive 9 http://users.info.unicaen.fr/~anne/HTML/atelier.htm Vincent Perlerin (ici, le corpus constitué par l’utilisateur) . MemLabor a pour objectif de participer à ce genre d’étude, (re)plaçant le document au sein d’entités plus grandes influençant son interprétation et permettant d</context>
</contexts>
<marker>Rastier, 2001</marker>
<rawString>Rastier F. (2001), Eléments de théorie des genres, texte diffusé sur la liste fermée Sémantique des textes, 2001. http://www.atala.org/je/010428/Rastier/Rastier280401.html Riloff E., Shepherd J. (1997). A corpus-based approach for building semantic lexicons. In Cardie, C. et Weischedel, R., editors, Proceedings of the Second Conference on Empirical Methods in Natural Language Processing, 117-124. ACL, Somerset, New Jersey.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Silberstein</author>
</authors>
<title>Le système INTEX, Dictionnaires électroniques et analyse automatique de textes,</title>
<date>1993</date>
<location>Paris, Masson.</location>
<marker>Silberstein, 1993</marker>
<rawString>Silberstein M. (1993), Le système INTEX, Dictionnaires électroniques et analyse automatique de textes, Paris, Masson.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Thlivitis</author>
</authors>
<title>Sémantique interprétative Intertextuelle : assitance informatique anthropocentrée à la compréhension des textes, Thèse de l&apos;Université de Rennes 1.</title>
<date>1998</date>
<contexts>
<context position="12942" citStr="Thlivitis, 1998" startWordPosition="1951" endWordPosition="1952">nsemble des fichiers du corpus – le mot souris témoin dans l’expérience n’apparaît pas au sein du corpus). 3 Acquisition terminologique supervisée 3.1 Cadre de recherche - Problématique Nos recherches en sémantique pour le TAL répondent aux exigences suivantes : nous désirons élaborer des modèles et construire des outils permettant des traitements syntagmatiques rapides et des représentations paradigmatiques non exhaustives a priori, c&apos;est-à-dire aboutir à une sémantique légère. Nous nous plaçons dans une approche anthropocentrée où la machine se construit autour des besoins de l&apos;utilisateur (Thlivitis, 1998), et nous revendiquons une approche praxéologique de l&apos;activité langagière (sémantique tournée vers la pratique de la langue par un individu ou un groupe restreint d&apos;individus). De plus, nous désirons constuire une sémantique lexicale intra-linguistique textuellement située (à l’opposé d’une sémantique purement référentielle qui étudierait les rapports des expressions au monde - selon la différence soulignée par (Auroux, 1999 p.38). Le logiciel décrit dans cet article et l&apos;exemple d&apos;application s&apos;inscrivent donc pleinement dans ce cadre. La réalisation de MemLabor est consécutive à une étude s</context>
</contexts>
<marker>Thlivitis, 1998</marker>
<rawString>Thlivitis T. (1998), Sémantique interprétative Intertextuelle : assitance informatique anthropocentrée à la compréhension des textes, Thèse de l&apos;Université de Rennes 1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G K Zipf</author>
</authors>
<title>Human Behavior and the Principle of Least Effort,</title>
<date>1949</date>
<publisher>Addison-Wesley.</publisher>
<contexts>
<context position="7625" citStr="Zipf, 1949" startWordPosition="1138" endWordPosition="1139">ire sélectionné, dans celle de droite, les fichiers candidats au corpus en cours de création. Figure 2 : Copie d’écran de MemLabor – création d’un corpus. 2.2 Travaux Dans sa version actuelle, MemLabor propose cinq types de travaux sur corpus (ou sur des sous-ensembles du corpus) : la normalisation des documents d’un corpus au format TXT (depuis XML ou HTML), un segmenteur paramétrable (tokeniseur), un calcul de type Zipf (c.f. 2.2.1), une segmentation en paragraphe et une recherche de cooccurrences d’ensembles de lexies (c.f. 2.2.2). Vincent Perlerin 2.2.1 Calcul de type Zipf Zipf a observé (Zipf, 1949) que la fréquence d&apos;utilisation des mots décroît de manière quasilinéaire et que le produit f.R , soit la fréquence d&apos;un mot multipliée par le rang de ce mot est à peu près constant (cette constante dépend du texte ou de l’ensemble de textes considéré). MemLabor permet d’effectuer un calcul de type Zipf sur l’ensemble des documents d’un corpus où sur un sous-ensemble de documents d’un corpus. Ce calcul donne lieu à la modification du fichier XML du corpus en fonction des travaux demandés et à la création d’un fichier de résultat rassemblant les lexies découvertes classées par ordre décroissant</context>
</contexts>
<marker>Zipf, 1949</marker>
<rawString>Zipf G. K. (1949), Human Behavior and the Principle of Least Effort, Addison-Wesley.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>