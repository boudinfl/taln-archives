RECITAL 2003, Batz-sur-Mer, I 1-I4juin 2003

Un systeme de segmentation du chinois basé sur des triplets

Yiping LI

LIC2M CEA Fontenay-aux-Roses — Université de Marne la Vallée
18 rue du Panorama BP 6, 92265 Fontenay aux Roses Cedex
1i@zoe.cea.fr
Date de la these future : 2005

Mots-clefs — Keywords
Tokenisation, segmentation du chinois, ngrammes, approche statistique, maximum matching

Chinese segmentation, ngrams, statistical approach, maximum matching

Résumé — Abstract

U11 des problémes rencontrés lors de l’analyse de textes en chinois est qu’il n’existe pas de
séparateur entre les mots dans cette langue. Le mot étant une unité linguistique fondamentale
en traitement automatique de la langue, il est nécessaire d'identiﬁer les mots dans un texte
chinois afm que des analyses de plus haut niveau puissent étre réalisées. Le but de cet article
est de présenter un systeme d’identiﬁcation des mots basé sur un algorithme utilisant des
triplets de catégories grarnrnaticales et des fréquences de mots. Ce systéme comprend deux
dictionnaires : l’un dédié aux mots et a leurs fréquences, 1’autre aux triplets des catégories
correspondantes. Les tests qui ont été effectués révélent que 98,5% des phrases sont
découpées correctement. Certaines erreurs sont dues a la taille limitée du dictionnaire utilisé.
Une réﬂexion sur la création de nouvelles catégories et des études proposant des régles
grarnrnaticales sont en cours de realisation afm d’augrnenter la performance du systéme.

One of the problems encountered by Chinese texts analysis is that there is no separator
between the words in this language. As a ﬁmdamental linguistic unit in automatic treatment of
the language, word is necessary to be identiﬁed in a Chinese text so that higher-level analyses
can be carried out. The goal of this work is to develop a system, identifying words, based on
an algorithm of triplets of grammatical categories and words frequencies. This system
contains two dictionaries. One is dedicated to the words and their frequencies, the other, to the
triplets of the corresponding categories. The tests carried out reveal that this system works
very well, 98.5% of the sentences are segmented correctly. Thus, a reﬂection about the
creation of new categories and the study proposing the grammatical rules are carrying out to
improve the performance of the triplets.

Y iping LI

1 Introduction

Le chinois s’écrit au moyen d’une écriture de type idéographique. 11 y a ainsi des dizaines de
milliers de caractéres en chinois. Afm de faciliter la ﬂuidité et la vitesse d’écriture, les
caracteres sont accolés, c’est-a-dire que l'écriture chinoise ne posséde pas de délimiteur e11tre
les mots O-lung, Tzeng, 1981). Ce phénoméne, qui existe aussi pour d’autres langues
asiatiques comme le japonais et le coréen, pose au Traitement Automatique des Langues
(TAL) des problemes spéciﬁques par rapport aux langues occidentales et celle de Moyen-
Orient. Les mots étant des unités linguistiques fondamentales, il est nécessaire de les
identiﬁer dans un texte aﬁn de pouvoir effectuer une analyse de plus haut niveau, comme par
exemple l’analyse syntaxique ou la désambiguisation sémantique.

Plusieurs types de phénoménes linguistiques sont £1 1’origine des ambiguités concemant le
découpage en mots du chinois. Premiérement, presque tous les caracteres peuvent constituer
un mot en soi. lls peuvent également se joindre a d’autres caractéres pour former des mots.
Deuxiémement, le chinois modeme utilise essentiellement des mots composés. Il est difﬁcile
de détenniner si un mot composé rare est un mot ou une expression. Troisiémement, les
mémes caractéres sont généralement employés pour la construction des noms communs et des
noms propres. La distinction e11tre les deux se révéle des lors difﬁcile. Quatriémement,
quelques structures morphologiques spéciales, comme la duplication et les constructions
A_non_A, AA, A_un_A, et ABAB (Xia, 2000) doivent également étre prises en compte. Par
exemple au lieu de dire "regarder", le chinois utilise tres souvent l’eXpression "regarder un
regarder", etc. Deux types d’ambigu'1'tés sont trés fréquentes : ambiguité de croisement
intérieur (si une chaine de caractéres ABC peut étre découpée en A/BC ou AB/C, ABC est
une chaine ambigué de croisement intérieur) et ambiguité de combinaison (si PQ est un mot et
P et Q peuvent aussi étre des mots indépendants. PQ est une chaine ambigué de combinaison).

2 Les approches existantes

Dans le domaine de la segmentation du chinois, i1 existe trois types principaux d’algorithmes
(Emerson, 2000) : les approches statistiques, les approches fondées sur les dictionnaires et les
approches mixtes. Les approches statistiques sont basées sur la probabilité que deux ou
plusieurs caractéres apparaissent ensemble. Elles s’appuient généralement sur des modéles a
base de HMM (modéles de Markov cachés). Ces approches présentent l’avantage d’étre peu
sensibles a l'effet des mots inconnus et des translittérations phonétiques. Toutefois, elles
dépendent d’un modele linguistique qui est lui-méme contraint par la qualité et le volume des
corpus d’apprentissage.

Les approches basées sur les dictionnaires peuvent étre divisées en approches strictement
basées sur les dictionnaires et approches combinant dictionnaire et connaissances
linguistiques. L’idée générale consiste rassembler des caractéres en mot en fonction de ceux
présents dans un dictionnaire. Les ambiguités sont levées en utilisant une heuristique générale
sur la facon de guider Pappariement : heuristique concemant la taille des mots, comme pour
le simple maximum matching (Tsai, 1996) et le complexe maximum matching (Chen, Liu,
1992); ou heuristique relative au sens dans lequel l’appan'ement se fait, comme pour le
forward maximum matching mis en oeuvre dans Chinese Segment (Peterson, 2000) et le
backward maximum matching. Les deux derniéres sont parfois utilisées conjointement aﬁn de
résoudre des ambiguités de croisement intérieur. Comme aucun dictionnaire ne peut étre

Un systeme de découpage de chinois basé sur des triplets

exhaustif, une désambiguisation a 1’aide de connaissances linguistiques peut étre utilisée en
complément. Ces connaissances, par exemple de nature grammaticale, permettent d’effectuer
certains regroupements préférentiels a partir d’une segmentation de base considérant chaque
caractére comme un mot. C’est le cas dans (Hockenmaier, Brew, 1998).

Les méthodes mixes combinent les deux types de méthodes présentés ci-dessus. Le Chinese
Morphological Analyzer (Emerson, 2000) en est un exemple.

Au vu des avantages et des inconvénients de chaque méthode, nous avons choisi le troisiéme
type de méthodes, en combinant un dictionnaire, des connaissances linguistiques et des
statistiques. L’a1gorithme de simple maximum matching implique toujours des ambiguités de
croisement intérieur et des traitements supplémentaires sont nécessaires pour améliorer la
qualité du découpage. Pour la désambiguisation, nous avons aussi besoin de connaissances
gramrnaticales. Si des ambiguités persistent encore aprés ces traitements, l’utilisation de
fréquences de mots constitue un trés bon moyen pour trouver la meilleure solution.

3 Les travaux réalisés

Aﬁn d’obtenir le dictionnaire et des informations grammaticales, nous avons utilisé le corpus
"Chinese Treebank" de l’UniVersité de Pennsylvanie. Ce corpus est constitué de 325 dépéches
en chinois. Dans chacune de ces dépéches, les mots sont d’emblée balisés avec des étiquettes
renfermant des informations sur les parties du discours, la syntaxe, les fonctions, et les
catégories vides. Afm d’évaluer la performance des dictionnaires et la méthode de
désambig11'1'sation a l’aide des triplets de catégories successives, nous avons utilisé 295
ﬁchiers pour faire un apprentissage des régles de la segmentation; les 30 ﬁchiers restants ont
servi a l’éva1uation de cet apprentissage.

L’étude de la problématique de la segmentation du chinois et des algorithmes existants a
révélé la nécessité de disposer d’un dictionnaire de bonne qualité. Nous avons créé deux
dictionnaires en nous servant des informations disponibles du corpus : un dictionnaire de mots
avec leurs catégories possibles et un dictionnaire de triplets de catégories successives en
fonction du contexte. La fréquence de chaque mot ﬁgure également dans le dictionnaire de
mots. Elle peut servir a lever certaines ambigu'1'tés. Le dictionnaire de triplets offre un
contexte pour préciser et faciliter le choix.

Avec ces deux dictionnaires, nous pouvons entamer les principales étapes de la segmentation.
Premiere étape : en travaillant au niveau de la ligne et en effectuant une comparaison par
rapport aux entrées du dictionnaire, nous avons sélectionné tous les mots du texte £1
segmenter. Ceci nous permet d’obtenir un tableau de mots présents dans le dictionnaire avec
leurs positions de début et de ﬁn dans le texte. I1 convient de remarquer que dans les
méthodes classiques de segmentation du chinois, 1e traitement est effectué ligne par ligne.
Comme il est possible de rencontrer un saut de ligne a l’intén'eur d’un mot, nous traitons
intégralement la partie entre deux ponctuations comme l’unité de traitement, sans quoi il n’est
plus possible de trouver la bonne segmentation pour des mots séparés par un saut de ligne.

Deuxiéme étape : regrouper les mots en phrases. Le tableau de tous les mots possibles est
semblable a une boite de perles. Il faut enﬁler ces perles sur une chaine selon leurs positions.
Aprés l’ana1yse de toutes les perles, une série de segmentations possibles est obtenue. Aﬁn de
faciliter les traitements ultérieurs, un numéro unique est donné a chaque segmentation.

Y iping LI

Un dictionnaire ne peut pas contenir tous les mots. En effet, lorsque des mots inconnus sont
présents dans une phrase, il est impossible d’enﬁ1er les perles sur la chaine, car il y a des
"trous" entre les mots. La segmentation ne doit pas étre interrompue pour cette raison. Tout
comme nous le faisons pour le traitement des mots dans le dictionnaire, nous placons aussi les
mots inconnus dans la boite de perles. Pendant le processus de sélection des mots (étape
deux), si aucun mot commeneant par ce caractére n’est trouvé dans le dictionnaire, nous
aj outons ce caractere dans le tableau comme un mot appartenant a la catégorie "inconnu". Des
caractéres inconnus successifs peuvent constituer un mot intégral, ils sont considérés comme
un mot a part entiére avec la catégorie "inconnue". Cette technique peut aussi servir pour
l’apprentissage de nouveaux mots.

Troisiéme étape : comparer des triplets. Nous effectuons une comparaison entre les triplets de
catégories correspondant aux mots séparés en fonction de chaque possibilité de segmentation
et les entrées du dictionnaire de triplets. En ce qui conceme le traitement des mots portant la
catégorie "inconnu" qui ne ﬁgure pas dans le dictionnaire de triplets, nous avons fait le choix
de leur affecter la catégorie "Nom Propre" puisqu'ils sont trés souvent des noms propres. Un
mot peut étre associé £1 plusieurs catégories grammaticales. Lors de 1’extraction des triplets
possibles, nous traitons plusieurs catégories d’un méme mot séparément en parcourant toutes
les combinaisons possibles des triplets. Pour chaque possibilité de segmentation, si une
combinaison de triplets correspond aux entrées du dictionnaire de triplets, la segmentation est
pertinente. Si aucune combinaison de triplets n’est cohérente avec les entrées, cette
segmentation est ﬁltrée par l’analyse au moyen des trigrarnrnes.

Les mots ayant plusieurs catégories augrnentent considérablement le nombre de combinaisons
des triplets et donc ralentissent la segmentation de maniere irnportante. En moyenne, quand le
nombre de caracteres arrive 51 35, 1e nombre de segmentations augrnente exponentiellement
jusqu’a 6000, et le nombre de combinaisons de triplets atteint presque 90 000 000.
L’a1gorithme n’est plus utilisable, le temps de traitement devenant alors excessivement long.
Il nous a done failli effectuer un ﬁltrage avant d’utiliser les triplets en choisissant la
segmentation minimisant le nombre de mots découpés. Un test sur le corpus a révélé que 99
pour cent des résultats de cette stratégie sont corrects.

Quatriéme étape: calculer et comparer la somme des fréquences de segmentation. L’utilisation
de triplets ne produit pas nécessairement un découpage unique. A l’aide du dictionnaire des
ﬁ'équences, nous avons sélectionné la segmentation ayant la fréquence la plus élevée.

4 Evaluation

Nous avons testé notre algorithrne de segmentation et en avons analysé la perfonnance.
Comme nous l’avons signalé précédemment, nous avons conservé 30 ﬁchiers pour
l’éva1uation. Nous avons extrait le texte brut comme le texte a découper et un texte o1‘1 les
mots sont balisés comme le résultat standard du découpage en supprimant des inforrnations
grarnrnaticales supplémentaires.

Notre programme de segmentation a été lancé sur le texte brut. La F-mesure est utilisée pour
évaluer le résultat (Peng, Huang, Schuurmans, Cercone, 2002). Le rappel est le nombre de
mots découpés correctement divisé par le nombre total de mots dans notre résultat. La
précision est le nombre de mots découpés correctement divisé par le nombre total de mots
dans le résultat standard. Notre programme d’évaluation révéle que 9327 mots sont découpés

Un systeme de découpage de chinois basé sur des triplets

correctement parmi 9489 mots dans le texte découpé par notre algorithme. 11 y a 9599 mots
dans le résultat standard. Les valeurs du rappel, de la précision et de la F-mesure de notre
algorithme sont respectivement de 97,2%, 98,9% et 97,7%.

Par rapport aux performances d’autres algorithmes existants, notre algorithme donne des
résultats assez satisfaisants. Le rappel, la précision et la F-mesure de l’algorithme de MMSEG
de Chih-Hao Tsai sur 1013 mots sont de 95,4%, 95,5% et 95.4% pour le simple maximum
matching, et 98,1%, 98,4% et 98,3% pour le complex maximum matching. Le résultat de
l’algorithme de Palmer (Palmer, 1997) sur le corpus XinHua a une F-mesure de 89,6%. Les F-
mesures de l’algorithme Error Driven Learning (Hockenmaier, Brew, 1998) sont de 87,9%,
87,4% et 87,1%.

5 Discussion

Aﬁn de mieux évaluer les performances de notre algorithme, nous l'avons comparé a deux
systémes actuellement disponibles (Peterson, Wu). Un découpage au moyen de ces deux
algorithmes et du notre est réalisé sur le méme corpus .

Texte 2‘: découper : iZ7'%' E|2li%%ElITl?%‘:*’Iﬁﬁ3<ﬂ” “F |l>|’%EﬁEl‘J%%lﬁ3e?<i3F?li

Segmentation correct : ii-7%“ EIZIS-ééﬂ-ﬂiii;-ﬁgﬁij-X3‘-EF E>|-4‘?Eﬁ°l5I"J'§%“° ﬁ3$'i¥Zl’z
Traduction: C’est l’évaluation des bons du Trésor la plus élevée du marché ﬁnancier

japonais pour la Banque de Chine.

Résultat d’Erik Peterson : iX'% El°2l$ﬁ'§ﬂ'Tl3i5.I":*’IEﬁ°5lﬂ”'“l“ @°’EEﬁ"l5I"J'%%'lﬁ%°i5|‘?l'(
Résultat de Zhibuiao Wu : iX-E- El 2|:-eat:-aria-mt-:<¢-:1: I3!-tﬂﬁ-E141-ﬁe“-1ﬁ%-iii‘-zit
Notre résultat : iZ°%' El 2l§'$§ﬂ'T5iZI":*’Iﬁﬁ°5lﬂ”'“F @°’EEﬁ"l5I"J'%%'lﬁ%°i5|‘?l'(

La chaine "7% El2liéz‘*%$" est ambigué. Sa segmentation peut étre 2 7%“ El Zlﬁéﬂ, 7% El '2l§$'ﬂ,
7%El-Zliﬁ-E ou bien 7%-El-21$-ﬁﬂ. La segmentation d’Erik est erronée des le début. I1
délimite directement 7%El, le reste ne peut donc pas étre correctement découpé. Par contre
notre algorithme a trouvé le bon découpage, grace au traitement intégral de la partie entre
deux ponctuations, au lien de sortir le mot rencontré le plus long immédiatement en
comparant avec le dictionnaire de mots. Toutes les possibilités de découpage sont prises en
compte. Il perrnet d’enlever un nombre important d’ambigu'1'tés.

Pendant l’évaluation, nous avons remarqué qu’un pourcentage important de mauvaises
segmentations provient d’une faiblesse du dictionnaire. Nous avons testé de nouveau
l’algorithme en rajoutant les mots manquants, et le résultat s’est nettement amélioré. Le
rappel, la précision et la F-mesure sont de 97,l7%, 98,85% et 97 ,73%.

6 Conclusion et perspectives

Dans notre algorithme de segmentation, £1 partir de toutes les segrnentations possibles
permises par le dictionnaire, basé sur une désambiguisation par triplets de catégories et sur la
fréquence des mots, nous obtenons un tres bon résultat. La notion de l’unité de traitement a
permis de prendre en compte des mots coupés par des sauts de ligne. Le traitement applique

1 Le rappel, la précision, et la F-mesure de notre algorithme sur ce corpus sont de 83,8%, 90,0% et 86,8% ;

ceux de l’algorithme d’Erik sont de 70,6%, 68,7%, 69,6% ; ceux de l’algorithme de Zhibiao sont de 68,3%,
62,6% et 65,3%. Nous voyons trés clairement que notre algorithme fonctionne mieux.

Y iping LI

aux noms inconnus a permis une segmentation complete de tous les textes. Néanmoins, la
limite de volume des dictionnaires a affecté considérablement les résultats de la segmentation.
De plus, les régles apprises £1 partir des sources de Chinese Treebank ne sont pas completes.

Vu ces problemes, nous sommes en train d’améliorer les deux dictionnaires. Nous pouvons
intégrer des mots de dictionnaires existants dans notre dictionnaire de mots. Certains mots
seront ajoutés manuellement, par exemple des unités de mesure, les chiffres, etc. Pour le
dictionnaire des triplets, nous avons besoin de perfectionner notre connaissance de la
grammaire chinoise. Avec ces connaissances approfondies, il sera possible d’écrire des régles
de triplets. Par exemple, quelles catégories ne peuvent pas étre consécutives, lesquelles
peuvent 1’étre, lesquelles sont forcément placées les unes apres les autres, etc. Cette analyse
grammaticale des phrases permet aussi de mieux en comprendre le sens. Elle constitue
également une excellente base aﬁn de procéder ultérieurement a une analyse syntaxique.

Références

Tom Emerson (2000), Segmentation of Chinese Text, Multilingual Computing & Technology,
Vol. 12 Issue 2, pp38.

Chih-Hao Tsai (1996), A Word Identiﬁcation System for Mandarin Chinese Text Based on
Two Variants of the Maximum Matching Algorithm.

Chen K. J ., & Liu S. H. (1992), Word identiﬁcation for Mandarin Chinese sentences.
Proceedings, Fifteenth International Conference on Computational Linguistics, Nantes:
COLING-92.

Hung D. L, & Tzeng O. J. L (1981), Orthographic variations and visual information
processing, Psychological Bulletin, Vol.90, pp377-414.

David Palmer (1997), A Trainable Rule-Based Algorithm for Word Segmentation
Proceedings, Acte de the 35th Annual Meeting of the Association for Computational
Linguistics, Madrid.

Fei Xia (2000), The part-of-speech tagging guidelines for the Penn Chinese T reebank (3.0),
Philadelphia, ppl4-l 5.

Fuchun Peng, Xiangji Huang, Dale Schuurmans, Nick Cercone (2002), Investigating the
relationship between word segmentation performance and retrieval performance in Chinese
IR, Acte de COLING 2002 the 19th International Conference on Computational Linguistics,
793-799.

Julia Hockenmaier, Chris Brew, (1998), Error-driven segmentation of Chinese, Acte de 12th
Paciﬁc Conference on Language and Information, 218-229.

Erik Peterson, (2000), http://wvvw.mandarintools.com

Zhibiao Wu, (1999), http://www.ldc.upenn.edu/Projects/Chinese/segmenter/ma11segment.perl
http://www.ldc.upenn.edu/Projects/Chinese/segmenter/Mandarin.fre

