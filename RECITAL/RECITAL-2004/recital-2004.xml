<?xml version="1.0" encoding="UTF-8"?>
<!--
	Fichiers problématiques ayant été OCRisés: recital-2004-poster-008
-->
<conference>
	<edition>
		<acronyme>RECITAL'2004</acronyme>
		<titre>6e Rencontres des Étudiants Chercheurs en Informatique pour le Traitement Automatique des Langues</titre>
		<ville>Fès</ville>
		<pays>Maroc</pays>
		<dateDebut>2004-04-19</dateDebut>
		<dateFin>2004-04-22</dateFin>
		<presidents>
			<president>
				<prenom>Frédéric</prenom>
				<nom>Béchet</nom>
			</president>
			<president>
				<prenom>Tristan</prenom>
				<nom>Vanrullen</nom>
			</president>
		</presidents>
		<typeArticles>
			<type id="long">Papiers longs</type>
			<type id="poster">Posters</type>
		</typeArticles>
		<siteWeb>http://aune.lpl.univ-aix.fr/jep-taln04/</siteWeb>
	</edition>
	<articles>
		<article id="recital-2004-long-001" session="Session orale A">
			<auteurs>
				<auteur>
					<prenom>Anne-Laure</prenom>
					<nom>Ligozat</nom>
					<email>annlor@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIMSI-CNRS - Université Paris Sud Orsay BP 133, 91403 Orsay</affiliation>
			</affiliations>
			<titre>Système de Question Réponse : apport de l’analyse syntaxique lors de l’extraction de la réponse</titre>
			<type>long</type>
			<pages></pages>
			<resume>Dans cet article, nous présentons le système de Question Réponse QALC, et nous nous intéressons tout particulièrement à l’extraction de la réponse. Un appariement question-réponse fondé sur les relations syntaxiques a été développé, afin d’améliorer les performances du système. Un projet de génération de réponses à partir de plusieurs documents est également discuté.</resume>
			<mots_cles>Système de Question Réponse, analyse syntaxique, fusion de documents</mots_cles>
			<title></title>
			<abstract>In this paper, we present the question answering system QALC, and we particularly focus on the answer extraction. A question and answer matching based on syntactic relations has been developed, in order to improve the results of our system. A project aiming at fusing answers from several documents is also discussed.</abstract>
			<keywords>Question answering system, syntactic analysis, document fusion</keywords>
		</article>
		<article id="recital-2004-long-002" session="Session orale A">
			<auteurs>
				<auteur>
					<prenom>Chrystel</prenom>
					<nom>Millon</nom>
					<email>Krystelkay@aol.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Equipe DELIC – Université de Provence 29, Av. Robert Schuman – 13621 Aix-en-Provence Cedex 1</affiliation>
			</affiliations>
			<titre>Acquisition de relations lexicales désambiguïsées à partir du Web</titre>
			<type>long</type>
			<pages></pages>
			<resume>Nous montrons dans cet article qu’un pré-étiquetage des usages des mots par un algorithme de désambiguïsation tel qu’HyperLex (Véronis, 2003, 2004) permet d’obtenir des relations lexicales (du type NOM-ADJECTIF, NOM de NOM, NOM-VERBE) beaucoup plus exploitables, parce qu’elles-mêmes catégorisées en fonction des usages. De plus, cette technique permet d’obtenir des relations pour des usages très peu fréquents, alors qu’une extraction indifférenciée « noie » ces relations au milieu de celles correspondant aux usages les plus fréquents. Nous avons conduit une évaluation sur un corpus de plusieurs milliers de pages Web comportant l’un des 10 mots-cibles très polysémiques choisis pour cette expérience, et nous montrons que la précision obtenue est très bonne, avec un rappel honorable, suffisant en tout cas pour de nombreuses applications. L’analyse des erreurs ouvre des perspectives d’améliorations pour la suite de notre travail de thèse.</resume>
			<mots_cles>Corpus, relations lexicales, acquisition automatique, désambiguïsation lexicale</mots_cles>
			<title></title>
			<abstract>This study shows that a pre-labeling of word uses by means of a disambiguation algorithm such as HyperLex (Véronis, 2003, 2004) allows a better extraction of lexical relations (NOUNADJECTIVE, NOUN “de” NOUN, NOUN-VERB, etc.), since these relations are categorised with respect to word use. In addition, this technique enables us to retrieve relations for very infrequent word uses, which otherwise would be buried in the residual noise corresponding to the most frequent uses. We performed an evaluation on several thousand web pages containing a target word among a set of 10 highly polysemic ones. We show that the precision obtained is very good, with a quite honourable recall, sufficient in any case for many applications. The analysis of errors opens avenues of research for the rest of our PhD work.</abstract>
			<keywords>Corpus, lexical relations, automatic acquisition, word sense disambiguation</keywords>
		</article>
		<article id="recital-2004-long-003" session="Session orale A">
			<auteurs>
				<auteur>
					<prenom>Aurélie</prenom>
					<nom>Névéol</nom>
					<email>aneveol@insa-rouen.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Laboratoire PSI – INSA et Université de Rouen BP8 - Avenue de l'Université - 76801 Saint-Etienne-du-Rouvray Cedex</affiliation>
				<affiliation affiliationId="2">Equipe CISMeF et L@STICS – Faculté de Médecine - CHU de Rouen 1, rue de Germont – 76031 Rouen</affiliation>
			</affiliations>
			<titre>Indexation automatique de ressources de santé à l’aide d’un vocabulaire contrôlé</titre>
			<type>long</type>
			<pages></pages>
			<resume>Nous présentons ici le système d’indexation automatique actuellement en cours de développement dans l’équipe CISMeF afin d’aider les documentalistes lors de l’indexation de ressources de santé. Nous détaillons l’architecture du système pour l’extraction de mots clés MeSH, et présentons les résultats d’une première évaluation. La stratégie d’indexation choisie atteint une précision comparable à celle des systèmes existants. De plus, elle permet d’extraire des paires mot clé/qualificatif, et non des termes isolés, ce qui constitue une indexation beaucoup plus fine. Les travaux en cours s’attachent à étendre la couverture des dictionnaires, et des tests à plus grande échelle sont envisagés afin de valider le système et d’évaluer sa valeur ajoutée dans le travail quotidien des documentalistes.</resume>
			<mots_cles>Indexation Automatique, Terminologie Médicale, Vocabulaire Contrôlé</mots_cles>
			<title></title>
			<abstract>This paper presents the automatic indexing system currently developed in the CISMeF team to assist human indexers. The system architecture, using the INTEX platform for MeSH term extraction is detailed. The results of a preliminary experiment indicate that the automatic indexing strategy is relevant, as it achieves a precision comparable to that of other existing operational systems. Moreover, the system presented in this paper retrieves keyword/qualifier pairs as opposed to single terms, therefore providing a significantly more precise indexing. Further development and tests will be carried out in order to improve the coverage, and validate the efficiency of the system in the librarians’ everyday work.</abstract>
			<keywords>Automatic Indexing, Medical terminology, Controlled Vocabulary</keywords>
		</article>
		<article id="recital-2004-long-004" session="Session orale A">
			<auteurs>
				<auteur>
					<prenom>Sylwia</prenom>
					<nom>Ozdowska</nom>
					<email>ozdowska@univ-tlse2.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">ERSS – Université Toulouse le Mirail Maison de la Recherche 5 allées Antonio Machado 31058 Toulouse Cedex 1</affiliation>
			</affiliations>
			<titre>Appariement bilingue de mots par propagation syntaxique à partir de corpus français/anglais alignés</titre>
			<type>long</type>
			<pages></pages>
			<resume>Nous présentons une méthode d’appariement de mots, à partir de corpus français/anglais alignés, qui s’appuie sur l’analyse syntaxique en dépendance des phrases. Tout d’abord, les mots sont appariés à un niveau global grâce au calcul des fréquences de cooccurrence dans des phrases alignées. Ces mots constituent les couples amorces qui servent de point de départ à la propagation des liens d’appariement à l’aide des différentes relations de dépendance identifiées par un analyseur syntaxique dans chacune des deux langues. Pour le moment, cette méthode dite d’appariement local traite majoritairement des cas de parallélisme, c’est-à-dire des cas où les relations syntaxiques sont identiques dans les deux langues et les mots appariés de même catégorie. Elle offre un taux de réussite de 95,4% toutes relations confondues.</resume>
			<mots_cles>appariement syntaxique de mots, corpus parallèle, traitement automatique des langues naturelles</mots_cles>
			<title></title>
			<abstract>We present a word alignment procedure based on a syntactic dependency analysis of French/English parallel corpora. First, words are associated at a global level by comparing their co-occurrences in aligned sentences with respect to their overall occurrences in order to derive a set of anchor words. The anchor words are the starting point of the propagation process of alignment links using the different syntactic relations identified by a parser for each language. This process is called the local alignment. For the moment, it is performed basically when the syntactic relations are identical in both languages and the words aligned have the same part of speech. This method achieves a precision rate of 95,4% all syntactic relations taken into account.</abstract>
			<keywords>syntactic word alignment, parallel corpora, natural language processing</keywords>
		</article>
		<article id="recital-2004-long-005" session="Session orale B">
			<auteurs>
				<auteur>
					<prenom>Marie-Laure</prenom>
					<nom>Guénot</nom>
					<email>mlg@lpl.univ-aix.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Emmanuel</prenom>
					<nom>Bellengier</nom>
					<email>bellengier@lpl.univ-aix.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Laboratoire Parole et Langage CNRS – Université de Provence 29 avenue Robert Schuman 13621 Aix-en-Provence cedex 1</affiliation>
			</affiliations>
			<titre>Quelques principes pour une grammaire multimodale non-modulaire du français</titre>
			<type>long</type>
			<pages></pages>
			<resume>Dans cet article, nous introduisons une approche de la représentation et de l’analyse des discours multimodaux, basée sur un traitement unimodulaire par contraintes. Le but de cet article est de présenter (i) un système de représentation des données et (ii) une méthode d’analyse, permettant une interaction simplifiée entre les différentes modalités de communication. L’avantage de cette méthode est qu’elle permet la prise en compte rigoureuse d’informations communicatives de natures diverses en un traitement unique, grâce à une représentation homogène des objets, de leurs relations, et de leur méthode d’analyse, selon le modèle des Grammaires de Propriétés.</resume>
			<mots_cles>Analyse multimodale, Linguistique formelle, Développement de grammaire, Grammaires de Propriétés</mots_cles>
			<title></title>
			<abstract>In this paper, we introduce an approach to multimodal discourse representation and analysis, based on a unimodular constraint-based treatment. The aim of this paper is to present (i) a data representation system, and (ii) an analysis method, which allow a simple interaction between the various modalities of communication. The advantage of this method is that it allows to rigorously take into account multi-character information within a single treatment, thanks to a homogeneous representation of the objects, of their relations, and of their analysis method, according to the Property Grammars formalism.</abstract>
			<keywords>Multimodal analysis, Formal linguistics, Grammar development, Property Grammars</keywords>
		</article>
		<article id="recital-2004-long-006" session="Session orale B">
			<auteurs>
				<auteur>
					<prenom>Christophe</prenom>
					<nom>Benzitoun</nom>
					<email>christophebenzitoun@yahoo.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Equipe DELIC – Université de Provence 29, Av. Robert Schuman, 13100 Aix-en-Provence</affiliation>
			</affiliations>
			<titre>L'annotation syntaxique de corpus oraux constitue-t-elle un problème spécifique ?</titre>
			<type>long</type>
			<pages></pages>
			<resume>Dans cet article, nous présentons une typologie des phénomènes qui posent problème pour l'annotation syntaxique de corpus oraux. Nous montrons également que ces phénomènes, même s'ils y sont d'une fréquence moindre, sont loin d'être absents à l'écrit (ils peuvent même être tout à fait significatifs dans certains corpus : e-mails, chats, SMS…), et que leur prise en compte peut améliorer l'annotation et fournir un cadre intégré pour l'oral et l'écrit.</resume>
			<mots_cles>Annotation syntaxique, corpus oraux, NFCE, annotation de référence</mots_cles>
			<title></title>
			<abstract>In this paper, we present a typology of the phenomena that create problems for the syntactic tagging of spoken corpora. We also show that these phenomena, although less frequent, are far from being absent in written language (they can even be quite significant in some corpora: e-mails, chats, SMS…). Taken them into account can improve the annotation and provide a unified analysis framework for both spoken and written data.</abstract>
			<keywords>Syntactic annotation, spoken corpora, reference annotation</keywords>
		</article>
		<article id="recital-2004-long-007" session="Session orale B">
			<auteurs>
				<auteur>
					<prenom>Olivier</prenom>
					<nom>Blanc</nom>
					<email>olivier.blanc@univ-mlv.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Anne</prenom>
					<nom>Dister</nom>
					<email>dister@tedm.ucl.ac.be</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Institut Gaspard-Monge – Université de Marne-la-Vallée 5, Bd Descartes, F- 77420 Champs-sur-Marne (France)</affiliation>
				<affiliation affiliationId="2">Cental et Centre de recherche Valibel – Université de Louvain Collège Érasme, Place Blaise Pascal, B- 1348 Louvain-la-Neuve (Belgique)</affiliation>
			</affiliations>
			<titre>Automates lexicaux avec structure de traits</titre>
			<type>long</type>
			<pages></pages>
			<resume>Nous présentons les automates lexicaux avec structure de traits, une extension du modèle des automates finis sur le mots dans lesquels les transitions sont étiquetées par des motifs qui sélectionnent un sous-ensemble des mots étiquetés en fonction de leurs traits positionnés. Nous montrons l’adéquation de ce modèle avec les ressources linguistiques dont nous disposons et nous exposons les grandes lignes de nos méthodes pour effectuer des opérations telles que la déterminisation, l’intersection ou la complémentation sur ces objets. Nous terminons en présentant une application concrète de ces méthodes pour la levée d’ambiguïtés lexicales par intersection d’automates à l’aide de contraintes locales.</resume>
			<mots_cles>automates finis, grammaire locale, dictionnaire électronique, levée d’ambiguïtés, lexique-grammaire</mots_cles>
			<title></title>
			<abstract>We present an extension to finite automata on words in which transitions are labeled with lexical masks describing a subset of their alphabet. We first show the connection between this model and our linguitic data and we present our implementation of classical automata operations on these objects. Then we show a concrete application of our methods to lexical disambiguation making use of grammatical constraints described in local grammars.</abstract>
			<keywords>finite state automata, local grammar, electronic dictionnary, disambiguation, lexicon-grammar</keywords>
		</article>
		<article id="recital-2004-long-008" session="Session orale B">
			<auteurs>
				<auteur>
					<prenom>Fabienne</prenom>
					<nom>Venant</nom>
					<email>fabienne.venant@ens.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LaTTiCe – ENS 1 rue Maurice Arnoux 92120 Montrouge</affiliation>
			</affiliations>
			<titre>Géométriser le sens</titre>
			<type>long</type>
			<pages></pages>
			<resume>Les recherches en sémantique lexicale s’appuient de plus en plus sur des ressources électroniques de grande taille (dictionnaires informatisés, corpus, ontologies) à partir desquelles on peut obtenir diverses relations sémantiques entre unités lexicales. Ces relations sont naturellement modélisées par des graphes. Bien qu'ils décrivent des phénomènes lexicaux très différents, ces graphes ont en commun des caractéristiques bien particulières. On dit qu’ils sont de type petit monde. Nous voulons mener une étude théorique mathématique et informatique de la structure de ces graphes pour le lexique. Il s’agit de les géométriser afin de faire apparaître l'organisation du lexique, qui est implicitement encodée dans leur structure. Les outils mis en place sont testés sur le graphe du dictionnaire électronique des synonymes (www.crisco.unicaen.fr). Ils constituent une extension du logiciel Visusyn développé par Ploux et Victorri (1998).</resume>
			<mots_cles>Lexique, espace sémantique, synonymie, graphes petit monde</mots_cles>
			<title></title>
			<abstract>Research in lexical semantics tends to rely on large-scale electronic language resources (machine-readable dictionaries, corpora, ontologies), from which one can get varied semantic relationships between lexical units. Such relationships are naturally modelled by graphs. Although they describe different lexical phenomena, these graphs share some very specific characteristics. They are called "small worlds". We want to carry out a theoretical mathematical and informatic study of these graphs. The point is to geometrise them in order to reveal the organisation of the lexicon that is encoded in their structure. The tools we developed are tested on the graph of the electronic dictionary of synonyms (www.crisco.unicaen.fr). They represent an extension of Visusyn, the software developed by Ploux and Victorri (1998).</abstract>
			<keywords>Lexicon, Semantic space, synonymy, graph, small world</keywords>
		</article>
		<article id="recital-2004-poster-001" session="Posters">
			<auteurs>
				<auteur>
					<prenom>Houda</prenom>
					<nom>Anoun</nom>
					<email>anoun@labri.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Equipe Signes LaBRI - Bordeaux1 - INRIA Futurs</affiliation>
			</affiliations>
			<titre>ICHARATE : Un Atelier Logique pour les Grammaires Multimodales</titre>
			<type>poster</type>
			<pages></pages>
			<resume>Cet article présente le projet de l’atelier logique ICHARATE dédié à l’étude des grammaires catégorielles multimodales. Cet atelier se présente sous la forme de bibliothèques pour l’assistant de preuves Coq.</resume>
			<mots_cles>analyse syntaxique, grammaires catégorielles, théorie des types, assistant de preuves</mots_cles>
			<title></title>
			<abstract></abstract>
			<keywords></keywords>
		</article>
		<article id="recital-2004-poster-002" session="Posters">
			<auteurs>
				<auteur>
					<prenom>Lucie</prenom>
					<nom>Barque</nom>
					<email>lbarque@linguist.jussieu.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LaTTice Université Paris 7</affiliation>
			</affiliations>
			<titre>De la lexie au vocable : la représentation formelle des liens de polysémie</titre>
			<type>poster</type>
			<pages></pages>
			<resume>Cet article s’intéresse aux définitions formalisées de la base de données BDéf et montre en quoi la structure formelle de ces définitions est à même d’offrir une représentation originale de la polysémie lexicale.</resume>
			<mots_cles>LEC, dictionnaire électronique formalisé, structure définitionnelle, TAL</mots_cles>
			<title></title>
			<abstract>This article deals with formal structure of BDéf’s definitions and shows in what way this structure is appropriate to offer an original representation of the lexical polysemy phenomenon.</abstract>
			<keywords>ECL, formalised electronical dictionnary, structure of the definition, NLP</keywords>
		</article>
		<article id="recital-2004-poster-003" session="Posters">
			<auteurs>
				<auteur>
					<prenom>Asma</prenom>
					<nom>Bouhafs</nom>
					<email>asma_bouhafs@yahoo.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Équipe Langages, Logiques, Informatique, Cognition et Communication (LaLICC) (UMR 8139) CNRS - Université de Paris Sorbonne 96, Boulevard Raspail 75006 PARIS – France</affiliation>
			</affiliations>
			<titre>Système d'extraction d'information dédié à la veille Qui est qui? Qui fait quoi? Où? Quand? Comment?</titre>
			<type>poster</type>
			<pages></pages>
			<resume>Dans cet article nous présentons un outil d’extraction d’information dédié à la veille qui répond à un certain nombre de requêtes formulées par l'utilisateur, en combinant la puissance des outils et les ressources informatiques à une analyse linguistique. Cette analyse linguistique permet le repérage des entités nommées (acteurs, lieux, temps,…) ainsi que la mise en relation des acteurs avec leur environnement dans l'espace et le temps au moyen d'indices déclencheurs, d’indices complémentaires et de règles qui les combinent, c'est le principe de l'Exploration Contextuelle. Les résultats capitalisés dans des fichiers XML, sont proposés par le biais d’une interface, soit sous forme de graphes soit sous forme de base d'informations.</resume>
			<mots_cles>Classes sémantiques, Extraction d’information, Exploration Contextuelle, Ressources, Réseau sémantique</mots_cles>
			<title></title>
			<abstract>In this article we present an information extraction tool which answers a certain number of requests formulated by the user, by combining data-processing with a linguistic analysis. This linguistic analysis allows the location of the named entities (actors, places, time...) thus the relations between actors and their environments in space and time by means of indices, indicators and rules which combine them, it is the principle of Contextual Exploration. The results capitalized in XML files are presented in an interface, either in the form of graphs or in the form of databases.</abstract>
			<keywords>Semantic classes, Information Extraction, Contextual Exploration, Resources, Semantic network</keywords>
		</article>
		<article id="recital-2004-poster-004" session="Posters">
			<auteurs>
				<auteur>
					<prenom>Atefeh</prenom>
					<nom>Farzindar</nom>
					<email>farzinda@iro.umontreal.ca</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Laboratoire RALI Département d’Informatique et recherche opérationnelle Université de Montréal, C.P. 6128, succursale Centre-ville Montréal, Québec, Canada, H3C 3J7</affiliation>
			</affiliations>
			<titre>Développement d’un système de Résumé automatique de Textes Juridiques</titre>
			<type>poster</type>
			<pages></pages>
			<resume>Nous décrivons notre méthode de production automatique du résumé de textes juridiques. C’est une nouvelle application du résumé qui permet aux juristes de consulter rapidement les idées clés d’une décision juridique pour trouver les jurisprudences pertinentes à leurs besoins. Notre approche est basée sur l’exploitation de l’architecture des documents et les structures thématiques, afin de constituer automatiquement des fiches de résumé qui augmentent la cohérence et la lisibilité du résumé. Dans cet article nous détaillons les conceptions des différentes composantes du système, appelé LetSum et le résultat d’évaluation.</resume>
			<mots_cles>Résumé automatique, fiches de résumé, segmentation thématique, textes juridiques</mots_cles>
			<title></title>
			<abstract>We describe our method for dealing with automatic summarization techniques in the legal domain. This new application of summary helps a legal expert determine the key ideas of a judgement. Our approach is based on the exploration of the document’s architecture and its thematic structures, in order to build a table style summary, which improves coherency and readability in the summary. We present the components of a system, called LetSum, built with this approach and some preliminary results of the evaluation.</abstract>
			<keywords>Automatic text summarization, summary table, topic segmentation, legals texts</keywords>
		</article>
		<article id="recital-2004-poster-005" session="Posters">
			<auteurs>
				<auteur>
					<prenom>Jens</prenom>
					<nom>Grivolla</nom>
					<email>jens.grivolla@lia.univ-avignon.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Laboratoire Informatique d’Avignon (LIA)</affiliation>
			</affiliations>
			<titre>Méthodes statistiques et apprentissage automatique pour l’évaluation de requêtes en recherche documentaire</titre>
			<type>poster</type>
			<pages></pages>
			<resume>Pour la recherche documentaire il est souvent intéressant d’avoir une bonne mesure de confiance dans les réponses trouvées par le moteur de recherche. Une bonne estimation de pertinence peut permettre de faire un choix entre plusieurs réponses (venant éventuellement de différents systèmes), d’appliquer des méthodes d’enrichissement additionnelles selon les besoins, ou encore de permettre à l’utilisateur de prendre des décisions (comme d’approfondir la recherche à travers un dialogue). Nous proposons une méthode permettant de faire une telle estimation, utilisant des connaissances extraites d’un ensemble de requˆetes connues pour en déduire des prédictions sur d’autres requˆetes posées au système de recherche documentaire.</resume>
			<mots_cles>apprentissage/décision automatique, recherche documentaire, expansion de requêtes, évaluation de difficulté</mots_cles>
			<title></title>
			<abstract>In document retrieval applications it is often interesting to have a measure of confidence in the answers found by the retrieval system. A good relevance estimation can allow us to make a choice between different answers (possibly provided by different sources), apply additional expansion techniques according to the specific needs, or enable the user to make decisions (such as to refine the search interactively). We propose a method that allows us to make such estimations, using knowledge extracted from a known query corpus to deduce predictions on new queries presented to the document retrieval system.</abstract>
			<keywords>automatic learning/decision, document retrieval, query expansion, difficulty evaluation</keywords>
		</article>
		<article id="recital-2004-poster-006" session="Posters">
			<auteurs>
				<auteur>
					<prenom>Frédérick</prenom>
					<nom>Houben</nom>
					<email>fhouben@info.unicaen.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">GREYC – UMR 6072 – Université de Caen Bureau S2 301 – campus II – BP 5186 F-14032 CAEN cedex</affiliation>
			</affiliations>
			<titre>Mot vide, mot plein ? Comment trancher localement</titre>
			<type>poster</type>
			<pages></pages>
			<resume>Nous présentons une méthode multilingue de catégorisation en mot vide / mot plein à partir de corpus brut. Cette méthode fait appel à des propriétés très générales des langues ainsi qu’à des techniques issues de la communauté de la fouille de données.</resume>
			<mots_cles>Traitements multilingues, découverte de mots vides, alternative à une stop-list, extraction de règles et de motifs fréquents</mots_cles>
			<title></title>
			<abstract>We are presenting a NLP multilingual method for function word / content word categorization using no other resource than the raw text itself. This method uses very general linguistic properties and also engineering from data mining community.</abstract>
			<keywords>Multilingual NLP, function words discovery, stop-list alternation, rules and frequent pattern mining</keywords>
		</article>
		<article id="recital-2004-poster-007" session="Posters">
			<auteurs>
				<auteur>
					<prenom>Mehand</prenom>
					<nom>Iheddadene</nom>
					<email>mehand.iheddadene@rd.francetelecom.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">France Telecom R&amp;D - DMI/GRI 2, avenue Pierre Marzin - 22307 Lannion</affiliation>
			</affiliations>
			<titre>Génération sémantico-syntaxique pour la traduction automatique basée sur une architecture interlingue</titre>
			<type>poster</type>
			<pages></pages>
			<resume>Dans cet article, nous présentons un processus de génération sémantico-syntaxique conçu et mis en oeuvre dans la réalisation d’un prototype de traduction automatique basée sur le modèle à structure intermédiaire (ou structure pivot). Dans une première partie de l’article, nous présentons l’organisation des ressources lexicales et sémantiques multilingues, ainsi que les mécanismes permettant d’exploiter ces ressources pour produire une représentation conceptuelle du sens de la phrase source. Dans une seconde partie, nous présentons la première phase de génération à partir d’une structure pivot (génération Sémantico-Syntaxique) permettant la construction d’une structure syntaxique profonde de la phrase cible à produire. Les autres phases de génération ne seront pas abordées dans cet article.</resume>
			<mots_cles>Traduction Automatique, Architecture interlingue, sémantique lexicale, Génération sémanticosyntaxique, structure syntaxique profonde</mots_cles>
			<title></title>
			<abstract>This paper aims to present a method for the semantic-syntactic generation that has been proposed to build a machine translation prototype based on the interlingua translation model. The first part of the paper introduces the multilingual lexical and semantical resources, and the way they are used to build conceptual representations of the meaning of source sentences. The following part of the paper presents the first step in generating the target text from the conceptual representation (semantic-syntactic generation) which results on a deep syntactic structure of the targeted text. The other steps of the generation process will not be discussed in this paper.</abstract>
			<keywords>Machine Translation, Interlingual architecture, lexical semantics, semantic-syntactic generation, deep syntactic structure</keywords>
		</article>
		<article id="recital-2004-poster-008" session="Posters">
			<auteurs>
				<auteur>
					<prenom>Diana</prenom>
					<nom>Jamborova-Lemay</nom>
					<email>dianalemay@aol.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">CERTAL - INALCO 73, rue Broca, 75013 Paris</affiliation>
			</affiliations>
			<titre>Reconnaissance automatique des adjectifs durs et des adverbes réguliers lors de l’analyse morphologique automatique du slovaque</titre>
			<type>poster</type>
			<pages></pages>
			<resume>L’analyse morphologique automatique du slovaque constitue la première étape d’un système d’analyse automatique du contenu des textes scientifiques et techniques slovaques. Un tel système pourrait être utilisé par des applications telles que l’indexation automatique des textes, la recherche automatique de la terminologie ou par un système de traduction. Une description des régularités de la langue par un ensemble de règles ainsi que l’utilisation de tous les éléments au niveau de la forme du mot qui rendent possible son interprétation permettent de réduire d’une manière considérable le Volume des dictionnaires. Notamment s’il s’agît d’une langue à flexion très riche, comme le slovaque. La reconnaissance automatique des adjectifs durs et des adverbes réguliers constitue la partie la plus importante de nos travaux. Les résultats que nous obtenons lors de l’analyse morphologique confirment la faisabilité et la grande fiabilité d’une analyse morphologique basée sur la reconnaissance des formes et ceci pour toutes les catégories lexicales.</resume>
			<mots_cles>Traitement automatique du slovaque, Analyse morphologique, Morphologie flexionnelle, Morphologie dérivationnelle</mots_cles>
			<title></title>
			<abstract>Automatic morphological analysis of Slovak language is the first level of an automatic analyser for Slovak scientific and technical texts. Such a system could be used for different applications: automatic text indexation, automatic research of terminology or translation systems. Rule-based descriptions of language regularities as Well as the use of all the formal level elements of Words allow reducing considerably the volume of dictionaries. Notably in case of inflectionally rich languages such as Slovak. The most important part of our research is the automatic recognition of adjectives and regular adverbs. The results obtained by our morphological analyser justify such an approach and confirm the high reliability of morphological analysis based on form-recognition for all lexical categories.</abstract>
			<keywords>Natural Language Processing of Slovak, Morphological Analysis, Inflectional Morphology, Derivational Morphology</keywords>
		</article>
		<article id="recital-2004-poster-009" session="Posters">
			<auteurs>
				<auteur>
					<prenom>Francisca</prenom>
					<nom>Luna Garcia</nom>
					<email>luna@message.lu</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Laboratoire d’Informatique et d’Intelligence Artificielle – INSA Strasbourg 24, boulevard de la Victoire 67000 Strasbourg</affiliation>
				<affiliation affiliationId="2">Université Marc Bloch 22, rue René Descartes 67084 Strasbourg Cedex</affiliation>
			</affiliations>
			<titre>Traitement informatique de l’inflexion dans le Lunaf, dictionnaire électronique du luxembourgeois</titre>
			<type>poster</type>
			<pages></pages>
			<resume>Afin de générer les formes fléchies des noms luxembourgeois dans le dictionnaire luxembourgeois, nous utilisons un code flexionnel. Ce code s’étant révélé trop contraignant pour traiter l’inflexion (alternance vocalique/Umlaut), nous présentons ici un moyen efficace pour coder ce phénomène. La pertinence de ce type de code est double. D’une part, il correspond mieux aux besoins du linguiste qui aimerait établir des classes flexionnelles naturelles sans trop de contraintes informatiques. D’autre part, il permet de réduire significativement le nombre de classes flexionnelles. Le dictionnaire électronique luxembourgeois dispose ainsi de deux codes qui peuvent se combiner entre eux pour mieux traiter les particularités morphologiques des mots luxembourgeois.</resume>
			<mots_cles>inflexion vocalique, codage, génération automatique, luxembourgeois</mots_cles>
			<title></title>
			<abstract>In order to generate the inflected forms of the Luxemburgish nouns in the Luxemburgish dictionary, we use a flexional. This code having proved to be too constraining to treat the inflection (vocalic alternation/Umlaut), we present here an effective means to encode this phenomenon. The relevance of this code is two-fold. On the one hand, it corresponds better to the needs of the linguist who would like to establish natural flexional classes without too many data-processing constraints. In addition, it reduces the number of flexional classes significantly. The Luxemburgish dictionary has thus two codes which can be combined for a better processing of the morphological characteristics of the Luxemburgish words.</abstract>
			<keywords>vowel inflection, encoding, automatic generation, Luxemburgish</keywords>
		</article>
		<article id="recital-2004-poster-010" session="Posters">
			<auteurs>
				<auteur>
					<prenom>Tuan-Dang</prenom>
					<nom>Nguyen</nom>
					<email>tnguyen@info.unicaen.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">GREYC, Université de CAEN Campus Côte de Nacre, F-14032 Caen Cedex</affiliation>
			</affiliations>
			<titre>Nouvelle méthode syntagmatique de vectorisation appliquée au self-organizing map des textes vietnamiens</titre>
			<type>poster</type>
			<pages></pages>
			<resume>Par ses caractéristiques éminentes dans la présentation des données, Self-Organizing Map (SOM) est particulièrement convenable à l’organisation des cartes. SOM se comporte d’un ensemble des vecteurs prototypes pour représenter les données d’entrée, et fait une projection, en conservant la topologie, à partir des vecteurs prototypes de n-dimensions sur une carte de 2-dimensions. Cette carte deviendra une vision qui reflète la structure des classes des données. Nous notons un problème crucial pour SOM, c’est la méthode de vectorisation des données. Dans nos études, les données se présentent sous forme des textes. Bien que le modèle général du SOM soit déjà créé, il nous faut de nouvelles recherches pour traiter des langues spécifiques, comme le vietnamien, qui sont de nature assez différente de l’anglais. Donc, nous avons appliqué la conception du syntagme pour établir un algorithme qui est capable de résoudre ce problème.</resume>
			<mots_cles>Self-Organizing Map, text mining, classification, vectorisation du texte, syntagme, évaluation visuelle</mots_cles>
			<title></title>
			<abstract></abstract>
			<keywords></keywords>
		</article>
		<article id="recital-2004-poster-011" session="Posters">
			<auteurs>
				<auteur>
					<prenom>Céline</prenom>
					<nom>Raynal</nom>
					<email>craynal@linguist.jussieu.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Lattice – CNRS UMR 8094 – Université Paris 7 2, place Jussieu – Case 7003 – 75251 Paris Cedex 05</affiliation>
			</affiliations>
			<titre>Représentation compositionnelle de la sémantique de aussi</titre>
			<type>poster</type>
			<pages></pages>
			<resume>L’objectif de notre travail est de dégager une représentation formelle compositionnelle de la contribution sémantique de aussi lorsqu’il a une valeur additive. Plusieurs problèmes de compositionnalité, liés surtout à la diversité des arguments concernés par l’adverbe, vont se poser. Nous proposons une alternative compositionnelle à la représentation proposée initialement en l-DRT.</resume>
			<mots_cles>Sémantique, l-DRT, présupposition, compositionnalité</mots_cles>
			<title></title>
			<abstract>The aim is to find a compositional formal representation to the French adverb aussi when it has an additive meaning. The variety of arguments of the adverb entails several problems of compositionality. Given the l-DRT representation is problematic, we try to propose an other one, compositional too.</abstract>
			<keywords>Semantics, l-DRT, presupposition, compositionality</keywords>
		</article>
		<article id="recital-2004-poster-012" session="Posters">
			<auteurs>
				<auteur>
					<prenom>Tahar</prenom>
					<nom>Saidane</nom>
					<email>saidane.tahar@planet.tn</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Mounir</prenom>
					<nom>Zrigui</nom>
					<email>mounir.zrigui@fsm.rnu.tn</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Mohamed</prenom>
					<nom>Ben Ahmed</nom>
					<email>Mohamed.BenAhmed@riadi.rnu.tn</email>
					<affiliationId>3</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Société Tunisienne d’Electricité et du Gaz, Centre de production de Sousse, Tunisie</affiliation>
				<affiliation affiliationId="2">Laboratoire RIADI, Unité Monastir Faculté des Sciences de Monastir, Tunisie</affiliation>
				<affiliation affiliationId="3">Laboratoire RIADI, Ecole Nationale des Sciences de l’informatique, Tunis, Tunisie</affiliation>
			</affiliations>
			<titre>La Transcription Orthographique-Phonetique De La Langue Arabe</titre>
			<type>poster</type>
			<pages></pages>
			<resume>Notre article présente les composants nécessaires à la synthèse de la parole arabe. Nous nous attarderons sur la transcription graphème phonème, étape primordiale pour l’élaboration d’un système de synthèse d’une qualité acceptable. Nous présenterons ensuite quelques-unes des règles utilisées pour la réalisation de notre système de traitement phonétique. Ces règles sont, pour notre système, stockées dans une base de données et sont parcourues plusieurs fois lors de la transcription.</resume>
			<mots_cles>Transcription graphème-phonème, langue arabe, règles de transcription</mots_cles>
			<title></title>
			<abstract>Our paper presents the components which are necessary for the arabic speech synthesis. We will dwell on the transcription graphème phoneme, a primordial stage for the development of a synthesis system with an acceptable quality. Then, we will present some of the rules used for the realization of our phonetic treatment system. These rules are, for our system, stocked in a data base and are browsed several times during the transcription.</abstract>
			<keywords>Grapheme-phoneme transcription, Arabic language, transcription rules</keywords>
		</article>
		<article id="recital-2004-poster-013" session="Posters">
			<auteurs>
				<auteur>
					<prenom>Barbara</prenom>
					<nom>Sonnenhauser</nom>
					<email>basonne@rz.uni-leipzig.de</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">University of Leipzig Beethovenstr. 15, 04107 Leipzig, Germany</affiliation>
			</affiliations>
			<titre></titre>
			<type>poster</type>
			<pages></pages>
			<resume>Les significations des expressions dans les langues naturelles sont souvent indéterminées (sous-spécifiées) et nécessitent d’être enrichies avant de devenir des propositions complètes. La sémantique générale des expressions linguistiques doit être complétée par les inférences pragmatiques, identifiées et captées d’une manière régulière et permettant ainsi un traitement opérationnel et même informatique. Cet article étudie l’indétermination de l’aspect imperfectif en russe et propose un cadre sémantique et pragmatique pour l’identification de ses différentes valeurs sémantiques à la base de règles.</resume>
			<mots_cles>Indétermination, aspect, interprétation, sémantique, pragmatique</mots_cles>
			<title>Towards a rule-guided derivation of aspectual readings in Russian</title>
			<abstract>Natural language expressions are underspecified and require enrichment to develop into full fledged propositions. Their sense-general semantics must be complemented with pragmatic inferences that have to be systematically figured out and pinned down in a principled way, so as to make them suitable inputs for NLP algorithms. This paper deals with the underspecified ipf1 aspect in Russian and introduces a semantic and pragmatic framework that might serve as the basis for a rule-guided derivation of its different readings.</abstract>
			<keywords>Underspecification, aspect, interpretation, semantics, pragmatics</keywords>
		</article>
		<article id="recital-2004-poster-014" session="Posters">
			<auteurs>
				<auteur>
					<prenom>Aree</prenom>
					<nom>Teeraparbseree</nom>
					<email>aree.teeraparbseree@imag.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">GETA, CLIPS, IMAG (UJF &amp; CNRS) 385, rue de la Bibliothèque B.P. 53 - 38041 Grenoble Cedex 9, France</affiliation>
			</affiliations>
			<titre>Un système adaptable pour l’initialisation automatique d’une base lexicale interlingue par acceptions</titre>
			<type>poster</type>
			<pages></pages>
			<resume>Cet article présente une stratégie de construction semi-automatique d’une base lexicale interlingue par acception, à partir de ressources existantes, qui utilise en synergie des techniques existantes de désambiguïsation. Les apports et limitations de chaque technique sont présentés. Notre proposition est de pouvoir composer arbitrairement des techniques, en fonction des ressources disponibles, afin d’obtenir une base interlingue de la qualité souhaitée. Jeminie, un système adaptable qui met en oeuvre cette stratégie, est introduit dans cet article.</resume>
			<mots_cles>base lexicale multilingue, construction automatique de lexies et axies, acception interlingue</mots_cles>
			<title></title>
			<abstract>This article presents a strategy for the semi-automatic building of an interlingual lexical database, based on existing resources, and using existing disambiguisation techniques in synergy. The pros and cons of each technique are given. Our proposal is to be able to compose techniques arbitrarily, according to the available resources, in order to produce an interlingual database of the desired quality. Jeminie, an adaptable system that implements this strategy, is introduced in this article.</abstract>
			<keywords>multilingual lexical database, automatic building of lexies and axies, interlingual acception</keywords>
		</article>
		<article id="recital-2004-poster-015" session="Posters">
			<auteurs>
				<auteur>
					<prenom>Tristan</prenom>
					<nom>VanRullen</nom>
					<email>tristan.vanrullen@lpl.univ-aix.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LPL-CNRS UMR 6057 - Université de Provence 29 avenue Robert Schuman - 13100 Aix-en-Provence</affiliation>
			</affiliations>
			<titre>Analyse syntaxique et granularité variable</titre>
			<type>poster</type>
			<pages></pages>
			<resume>Il est souhaitable qu’une analyse syntaxique -en traitement automatique des langues naturellessoit réalisée avec plus ou moins de précision en fonction du contexte, c’est-à-dire que sa granularité soit réglable. Afin d’atteindre cet objectif, nous présentons ici des études préliminaires permettant d’appréhender les contextes technique et scientifique qui soulèvent ce problème. Nous établissons un cadre pour les développements à réaliser. Plusieurs types de granularité sont définis. Puis nous décrivons une technique basée sur la densité de satisfaction, développée dans ce cadre avec des algorithmes basés sur un formalisme de satisfaction de contraintes (celui des Grammaires de Propriétés) ayant l’avantage de permettre l’utilisation des mêmes ressources linguistiques avec un degré de précision réglable. Enfin, nous envisageons les développements ultérieurs pour une analyse syntaxique à granularité variable.</resume>
			<mots_cles>Analyse syntaxique, granularité variable, grammaire de propriétés, shallow parsing, deep parsing, densité de satisfaction</mots_cles>
			<title></title>
			<abstract>It is gainful for a syntactic analysis - in Natural Language Processing- to be carried out with more or less accuracy depending on the context, i.e. its granularity should be adjustable. In order to reach this objective, we present here preliminary studies allowing, first of all, to understand the technical and scientific contexts which raise this problem. We establish a framework within which developments can be carried out. Several kinds of variable granularity are defined. We then describe a technic developed within this framework using satisfaction density, on algorithms based on a constraints satisfaction formalism (Property Grammars) and allowing the use of the same linguistic resources with an adjustable degree of accuracy. Lastly, we further consider developments towards a syntactic analysis with variable granularity.</abstract>
			<keywords>Parsing, variable granularity, property grammars, shallow parsing, deep parsing, satisfaction density</keywords>
		</article>
		<article id="recital-2004-poster-016" session="Posters">
			<auteurs>
				<auteur>
					<prenom>Hung</prenom>
					<nom>Vo Trung</nom>
					<email>Hung.Vo-Trung@imag.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Institut National Polytechnique de Grenoble GETA, CLIPS, IMAG - campus 385, rue de la Bibliothèque, BP 53-38041 Grenoble Cedex 9, France</affiliation>
			</affiliations>
			<titre>Réutilisation de traducteurs gratuits pour développer des systèmes multilingues</titre>
			<type>poster</type>
			<pages></pages>
			<resume>Nous présentons ici une méthode de réutilisation de systèmes de traduction automatique gratuits en ligne pour développer des applications multilingues et évaluer ces mêmes systèmes. Nous avons développé un outil de traitement et de traduction de documents hétérogènes (multilingues et multicodage). Cet outil permet d'identifier la langue et le codage du texte, de segmenter un texte hétérogène en zones homogènes, d'appeler un traducteur correspondant avec une paire de langue source et cible, et de récupérer les résultats traduits dans la langue souhaitée. Cet outil est utilisable dans plusieurs applications différentes comme la recherche multilingue, la traduction des courriers électroniques, la construction de sites web multilingues, etc.</resume>
			<mots_cles>Traduction Automatique, Traducteur Multilingue, Multilinguisme, Document Multilingue</mots_cles>
			<title></title>
			<abstract>We present here a method of the reuse of the free on line automatic translation systems to develop multilingual applications and to evaluate translators. We developed a tool for treatment and translation of the heterogeneous documents (multilingual and multicoding). This tool makes it possible to identify the language and the coding of the text, to segment a heterogeneous text in homogeneous zones, to call a translator corresponding with a fair of languages and to recover the results translated into desired language. We can use this tool in several different applications as multilingual research, the translation of the e-mail, the construction of the multilingual Web, etc.</abstract>
			<keywords>Machine Translation, Multilingual Translator, Multilingualism, Multilingual Document</keywords>
		</article>
		<article id="recital-2004-poster-017" session="Posters">
			<auteurs>
				<auteur>
					<prenom>Davy</prenom>
					<nom>Weissenbacher</nom>
					<email>dw@lipn.univ-paris13.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIPN – Université de Paris 13 99, avenue Jean-Baptiste Clément 93 430 Villetaneuse</affiliation>
			</affiliations>
			<titre>La relation de synonymie en génomique</titre>
			<type>poster</type>
			<pages></pages>
			<resume>L’accès au contenu des textes de génomique est aujourd’hui un enjeu important. Cela suppose au départ d’identifier les noms d’entités biologiques comme les gènes ou les protéines. Se pose alors la question de la variation de ces noms. Cette question revêt une importance particulière en génomique où les noms de gènes sont soumis à de nombreuses variations, notamment la synonymie. A partir d’une étude de corpus montrant que la synonymie est une relation stable et linguistiquement marquée, cet article propose une modélisation de la synonymie et une méthode d’extraction spécifiquement adaptée à cette relation. Au vu de nos premières expériences, cette méthode semble plus prometteuse que les approches génériques utilisées pour l’extraction de cette relation.</resume>
			<mots_cles>Extraction d’information, synonymie, entités nommées, génomique</mots_cles>
			<title></title>
			<abstract>The access to textual content in genomics is now recognized as an important issue. One of the first steps is the recognition of biological entity names such as gene or protein names. It has often been observed that entity names may vary in texts but this phenomenon is especially common in genomics. Besides a gene canonical name, one can find various abbreviation forms, typographic variants and synonyms. Stemming in a corpus analysis, this paper argues that synonymy in genomic texts is a stable and linguistically marked relation. This paper presents a method for extracting couples of synonymous gene or protein names. From a preliminary experiment, this method seems more promising than generic approaches that are exploited to extract synonymy relations.</abstract>
			<keywords>Information extraction, synonymy, named entities, genomics</keywords>
		</article>
		<article id="recital-2004-poster-018" session="Posters">
			<auteurs>
				<auteur>
					<prenom>Antoine</prenom>
					<nom>Widlöcher</nom>
					<email>awidloch@info.unicaen.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">GREYC - Université de Caen</affiliation>
			</affiliations>
			<titre>Analyse macro-sémantique: vers une analyse rhétorique du discours</titre>
			<type>poster</type>
			<pages></pages>
			<resume>S’inscrivant dans les domaines du TAL, de la linguistique sur corpus et de l’informatique documentaire, l’étude présentée ici opère plus précisément dans la perspective d’une analyse macrosémantique de la structuration discursive. Plus spécifiquement, nous proposons une analyse sémantique des structures rhétoriques du discours. Après avoir envisagé certaines voies ouvertes en la matière, nous définissons notre approche, et présentons les expérimentations conduites, dans le cadre du projet GeoSem, sur les structures énumératives dans le domaine géographique.</resume>
			<mots_cles>Macro-sémantique, analyse rhétorique, structure du discours, extraction d’information</mots_cles>
			<title></title>
			<abstract>Within the frameworks of automatic NLP, corpus linguistics, and automatic document processing, this study aims more precisely at a macro-semantic examination of the discursive structuring. Specifically, a semantic study of the rhetorical structures of discourse is here suggested. After considering existing approaches, we present our own methodology, and relate experimentations, as part of the GeoSem project, on enumerative structures in geographical corpus.</abstract>
			<keywords>Macro-semantics, rhetorical analysis, discourse structuring, information retrieval</keywords>
		</article>
	</articles>
</conference>
