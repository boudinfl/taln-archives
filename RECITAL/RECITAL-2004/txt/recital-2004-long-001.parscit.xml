<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S A&apos;1’t-Mokthar</author>
<author>J Chanod</author>
</authors>
<title>Robustness beyond shallowness 2 incremental deep parsing,</title>
<date>2002</date>
<journal>Journal of Natural Language Engineering,</journal>
<volume>8</volume>
<pages>3--2</pages>
<marker>A&apos;1’t-Mokthar, Chanod, 2002</marker>
<rawString>A&apos;1’t-Mokthar S., Chanod J ., Roux C. (2002), Robustness beyond shallowness 2 incremental deep parsing, Journal of Natural Language Engineering, Vol. 8, No. 3-2.</rawString>
</citation>
<citation valid="false">
<authors>
<author>C Cardie</author>
<author>V Chaudhri</author>
<author>R Gaizauskas</author>
<author>S Harabagiu</author>
<author>D Israel</author>
<author>C Jacquemin</author>
<author>C-Y Lin</author>
<author>S Maiorano</author>
<author>G Miller</author>
<author>D Moldovan</author>
<author>B Ogden</author>
<author>J Prager</author>
<author>E Riloff</author>
<author>A Singhal</author>
<author>R Shrihari</author>
<author>T Strzalkowski</author>
<author>E Voorhees</author>
<author>R Weishedel</author>
</authors>
<title>Selection et validation de la réponse dans les systemes de question-réponse, un systeme a base de contraintes sémantiques, Rapport de DESS,</title>
<date>2001</date>
<booktitle>Issues, Tasks and Program Structures to Roadmap Research in Question &amp; Answering (Q&amp;A), National Institute of Standards and Technology, http://wwwn1pir.nist.gov/projects/duc/papers/qa.Roadmap-paper_v2.doc Dalmas T.</booktitle>
<location>LIMSI-CNRS.</location>
<marker>Cardie, Chaudhri, Gaizauskas, Harabagiu, Israel, Jacquemin, Lin, Maiorano, Miller, Moldovan, Ogden, Prager, Riloff, Singhal, Shrihari, Strzalkowski, Voorhees, Weishedel, 2001</marker>
<rawString>Burger J ., Cardie C., Chaudhri V., Gaizauskas R., Harabagiu S., Israel D., Jacquemin C., Lin C.-Y., Maiorano S., Miller G., Moldovan D., Ogden B., Prager J., Riloff E., Singhal A., Shrihari R., Strzalkowski T., Voorhees E., Weishedel R. (2001), Issues, Tasks and Program Structures to Roadmap Research in Question &amp; Answering (Q&amp;A), National Institute of Standards and Technology, http://wwwn1pir.nist.gov/projects/duc/papers/qa.Roadmap-paper_v2.doc Dalmas T. (2002), Selection et validation de la réponse dans les systemes de question-réponse, un systeme a base de contraintes sémantiques, Rapport de DESS, LIMSI-CNRS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Fellbaum</author>
</authors>
<title>WordNet : An Electronic Lexical Database,</title>
<date>1998</date>
<publisher>MIT Press. Ferret</publisher>
<location>Cambridge, Massachussetts, The</location>
<contexts>
<context position="4544" citStr="Fellbaum, 1998" startWordPosition="680" endWordPosition="681">xt REtrieVal Conference, http://trec.nist.goV/. Pour une presentation de TREC 2002, Voir (Voorhees, 2002) 3Evaluation lancée par le Ministere de la Recherche, dans le cadre de l’Action Technolangue, http://www.recherche.gouv.fr/appel/2002/technolangue.ht1n 4Expression qui, dans un contexte particulier, identiﬁe de fagon unique une entité telle qu’une personne, un lieu, une organisation, une date; les noms propres correspondent a une sous—classe importante d’entités nommées. 5La réponse doit avoir une relation de type “est un” avec “stimulant” Ghttp://www.cogsci.princeton.edu/ wn/. Voir aussi (Fellbaum, 1998) Systéme de Question Réponse : apportde1’ana1yse syntaxique lors de 1’eXtraction de la réponse Quesﬁon\ CVOTPUS Traitement des &apos;M0tem. de questions recherche Catégories des . Termes Documents questions, Focus, candidats retoumes Type de la réponse * $ Traitement des documents: Ré—indexation et selection des documents (Fastr) Documents classes Reconnaissance des entités nommées Phrases étiquetées V 1 Recherche de la réponse: — reconnaissance du focus — appariement question/phrase — extraction de la réponse Sequences ordonnées de 50 caracteres Figure 1: Architecture du systeme QALC o si aucune c</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Fellbaum C. (1998), WordNet : An Electronic Lexical Database, Cambridge, Massachussetts, The MIT Press. Ferret 0., Grau B., Hurault-Plantet M., Illouz G., Jacquemin C., Monceaux L., Robba I., Vilnat A.</rawString>
</citation>
<citation valid="true">
<title>How a NLP approach beneﬁts question answering, Knowledge Organization joumal,</title>
<date>2003</date>
<journal>A Special Issue on Evaluation of HLT, Guest Editor</journal>
<booktitle>Widad Mustafa E1 Hadi,</booktitle>
<volume>2</volume>
<pages>3--4</pages>
<marker>2003</marker>
<rawString>(2003), How a NLP approach beneﬁts question answering, Knowledge Organization joumal, A Special Issue on Evaluation of HLT, Guest Editor 2 Widad Mustafa E1 Hadi, Vol. 29, 3-4.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Hovy</author>
<author>U Hermjakob</author>
<author>C-Y Lin</author>
</authors>
<date>2001</date>
<booktitle>The Use of External Knowledge in Factoid QA, TREC 10 Notebook.</booktitle>
<contexts>
<context position="7598" citStr="Hovy et al., 2001" startWordPosition="1146" endWordPosition="1149">(10.1408) type semantique the ﬁrst secretary of the French Socialist Party Lionel J ospin Patron syntaxique: What does Knight Ridder publish ‘.7 (9.671) What do GN VB Knight Ridder published 30 daily newspapers GNfocus VB GNreponse Figure 2: Exemples de chacune des strategies 2 Analyse syntaxique des phrases candidates Dans certains cas, la structure des phrases candidates peut entrainer l’eXtraction d’une mauvaise reponse par QALC. Pour la question “Who killed Lee Harvey Oswald ?”, une phrase candidate contenant la reponse est “Jack Ruby, who killed J .F. Kennedy assassin Lee Harvey Oswald” (Hovy et al., 2001). La selection de l’entite nommee la plus proche de l’objet de la question “Lee Harvey Oswald” retourne comme reponse “J .F. Kennedy” au lieu de “Jack Ruby”. L’ analyse de la question nous donne des indices sur la fonction syntaxique de la reponse attendue, qui peuvent étre utilises pour retoumer la bonne reponse : d’apres la question, l’entite nommee attendue doit étre agent du verbe “kill”; l’analyse de la phrase reponse montrerait que cet agent est bien “Jack Ruby” (Monceaux et al., 2002). Des connaissances syntaxiques sur la fonction attendue de la reponse peuvent ainsi ameliorer l’eXtract</context>
</contexts>
<marker>Hovy, Hermjakob, Lin, 2001</marker>
<rawString>Hovy E., Hermjakob U., Lin C.-Y. (2001), The Use of External Knowledge in Factoid QA, TREC 10 Notebook.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Jacquemin</author>
<author>J Klavans</author>
</authors>
<title>Expansion of multi-word terms for indexing and retrieval using morphology and syntax,</title>
<date>1997</date>
<booktitle>Actes de ACL-EACL’97,</booktitle>
<pages>24--31</pages>
<marker>Jacquemin, Klavans, 1997</marker>
<rawString>Jacquemin C., Klavans J . L., Tzoukermarm E. (1997), Expansion of multi-word terms for indexing and retrieval using morphology and syntax, Actes de ACL-EACL’97, 24-31.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Lehnert</author>
</authors>
<title>The process of question answering,</title>
<date>1978</date>
<location>Hillsdale, N.J., Lawrence Erlbaum Associates.</location>
<contexts>
<context position="10830" citStr="Lehnert, 1978" startWordPosition="1655" endWordPosition="1656">pliquer dans le second cas, et que seule la premiere réponse doit étre retoumée. D’autres inconvénients rendent par ailleurs délicate l’utilisation de patrons linguistiques : ceuxci sont doivent en effet connsidérer le plus de constructions possibles, ce qui rend leur nombre important, leur construction fastidieuse et leur couverture nécessairement incomplete. 2.3 Quels outils pour l’analyse syntaxique des phrases candidates ? QALC effectue une analyse ﬁne des questions grace a un analyseur syntaxique, qui fournit en particulier le type de la question, son objet (au sens du “focus” déﬁni par (Lehnert, 1978)), le type attendu de la réponse, entité nommée ou type sémantique général, et les relations de dépendance de la question. Cette analyse est rendue possible notamment par la forme syntaxique relativement ﬁgée des questions. En revanche, les phrases candidates sont analysées par un étiqueteur morpho-syntaxique, qui ne retourne qu’une analyse partielle de ces phrases. Aﬁn d’améliorer nos connaissances sur ces phrases, un analyseur syntaxique po11rra également étre utilisé, qui nous fournira, en plus de l’étiquetage morphologique des phrases, les fonctions syntaxiques de leurs éléments. Nous avon</context>
</contexts>
<marker>Lehnert, 1978</marker>
<rawString>Lehnert W. (1978), The process of question answering, Hillsdale, N.J., Lawrence Erlbaum Associates.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Moldovan</author>
<author>M Pasca</author>
<author>S Harabagiu</author>
<author>M Surdeanu</author>
</authors>
<title>Performance Issues and Error Analysis in an Open-Domain Question Answering System,</title>
<date>2003</date>
<journal>ACM Transactions on Information Systems,</journal>
<volume>21</volume>
<pages>133--154</pages>
<contexts>
<context position="18565" citStr="Moldovan et al., 2003" startWordPosition="2880" endWordPosition="2883">emple dans les taches “List” ou “Context” de TREC, pour lesquelles la réponse attendue est une liste (“Name 7 brand names of Belgian chocolates.”) ou les questions posées simulent un dialogue (“- How many species of spiders are there ? How many are poisonous to humans ? What percentage of spider bites in the US are fatal ?”). Si les scores des meilleurs systemes a TREC sont tres élevés (83% de bonnes réponses pour LCC a TREC 2002), les questions posées contiennent en grande majorité des questions factuelles : la distribution des questions TREC entre 1999 et 2001 est présentée dans la table 2 (Moldovan et al., 2003). Plus de 95% des questions TREC sont donc des questions factuelles ou nécessitant un raisonnement simple, ce qui est loin d’étre représentatif des besoins réels en recherche d’information. De nombreuses problématiques demeurent. Certaines font l’objet de nouvelles taches aux conférences TREC ; d’autres pourront étre abordées par d’autres évaluations comme EVALDAshttp://www.illc.uva.nl/EuroWordNet/ 9Analyseur transformationnel de surface qui reconnait les occurrences et les variantes des termes en entree (Jacquemin et al., 1997) Anne-Laure Li gozat Type Nombre(%) Factuel 985 (67.5%) Raisonneme</context>
<context position="21896" citStr="Moldovan et al., 2003" startWordPosition="3385" endWordPosition="3388">ts a partir d’informations dispersees dans plusieurs passages. Cette problematique est l’un des principaux deﬁs pour le domaine de Question Reponse dans les prochaines annees, selon le roadmap des con1°Cette information pourrait eventuellement étre recherchee dans une base de donnees de noms propres, mais ne pourra étre trouvee dans WordNet, qui ne recense qu’un nombre limite de noms propres. Systeme de Question Réponse : apportde1’ana1yse syntaxique lors de 1’eXtraction de la réponse férences TREC (Burger et al., 2001). La classiﬁcation des systemes de Question Réponse selon leurs capacités (Moldovan et al., 2003), utilisée précédemment, révele également l’importance de pouvoir traiter ce type de probleme. Cette taxonomie se présente les cinq classes de systemes suivantes : systemes capables de traiter des questions factuelles, systemes mettant en oeuvre des mécanismes de raisonnement simple, permettant par exemple de répondre a la question “How did Socrates die ?” par la phrase réponse “drinking poisoned wine”, systemes capables d’extraire la réponse a partir de plusieurs documents, les questions pouvant attendre par exemple une liste comme réponse (“Name 20 countries that produce coffee.”) ou bien un</context>
</contexts>
<marker>Moldovan, Pasca, Harabagiu, Surdeanu, 2003</marker>
<rawString>Moldovan D., Pasca M., Harabagiu S., Surdeanu M. (2003), Performance Issues and Error Analysis in an Open-Domain Question Answering System, ACM Transactions on Information Systems, Vol. 21, No. 2, 133-154.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Monceaux</author>
<author>I Robba</author>
</authors>
<title>Les analyseurs syntaxiques 2 atouts pour une analyse des questions dans un systéme de question-réponse ?, Actes de TALN2002,</title>
<date>2002</date>
<pages>195--204</pages>
<marker>Monceaux, Robba, 2002</marker>
<rawString>Monceaux L., Robba I. (2002), Les analyseurs syntaxiques 2 atouts pour une analyse des questions dans un systéme de question-réponse ?, Actes de TALN2002, 195-204.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Voorhees</author>
</authors>
<title>Overview of the TREC</title>
<date>2002</date>
<location>TREC</location>
<contexts>
<context position="4034" citStr="Voorhees, 2002" startWordPosition="616" endWordPosition="617">ns sémantiques seront utilisés (Dalmas, 2002). Par exemple pour la question “Name a stimulant.”, la réponse doit étre un hyponyme5 de “stimulant”. On Va donc rechercher une réponse qui vériﬁe cette contrainte, et qui de plus soit liée par exemple par “such as” a “stimulant”, ce qui permettra de valider une réponse comme “Stimulants such as Ritalin”. Les contraintes sémantiques sont vériﬁées dans la base de données WordNet6 ; 1Question Answering program of the Language and Cognition group at LIMSI-CNRS 2Text REtrieVal Conference, http://trec.nist.goV/. Pour une presentation de TREC 2002, Voir (Voorhees, 2002) 3Evaluation lancée par le Ministere de la Recherche, dans le cadre de l’Action Technolangue, http://www.recherche.gouv.fr/appel/2002/technolangue.ht1n 4Expression qui, dans un contexte particulier, identiﬁe de fagon unique une entité telle qu’une personne, un lieu, une organisation, une date; les noms propres correspondent a une sous—classe importante d’entités nommées. 5La réponse doit avoir une relation de type “est un” avec “stimulant” Ghttp://www.cogsci.princeton.edu/ wn/. Voir aussi (Fellbaum, 1998) Systéme de Question Réponse : apportde1’ana1yse syntaxique lors de 1’eXtraction de la rép</context>
</contexts>
<marker>Voorhees, 2002</marker>
<rawString>Voorhees E. (2002), Overview of the TREC 2002 Question Answering Task, TREC 2002.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>