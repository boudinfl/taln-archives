RECITAL 2004, Fès, 21 avril 2004
Système de Question Réponse :
apport de l’analyse syntaxique lors de l’extraction de la
réponse

Anne-Laure Ligozat
LIMSI-CNRS - Université Paris Sud Orsay
BP 133, 91403 Orsay
annlor@limsi.fr
date de soutenance prévue : 2006
Résumé - Abstract

Dans cet article, nous présentons le système de Question Réponse QALC, et nous nous intéres-
sons tout particulièrement à l’extraction de la réponse. Un appariement question-réponse fondé
sur les relations syntaxiques a été développé, afin d’améliorer les performances du système. Un
projet de génération de réponses à partir de plusieurs documents est également discuté.
In this paper, we present the question answering system QALC, and we particularly focus on
the answer extraction. A question and answer matching based on syntactic relations has been
developed, in order to improve the results of our system. A project aiming at fusing answers
from several documents is also discussed.
Mots-clefs – Keywords

Système de Question Réponse, analyse syntaxique, fusion de documents
Question answering system, syntactic analysis, document fusion
Anne-Laure Ligozat
1 Fonctionnement d’un système de Question Réponse
Développés pour répondre aux besoins croissants de recherche d’information dans des corpus
toujours plus fournis tels que le Web, les systèmes de Question Réponse correspondent à une
double problématique : permettre à l’utilisateur recherchant une information précise de poser
sa question en langage naturel, et extraire la réponse à cette question d’un grand nombre de
documents également en langage naturel.
La majorité des systèmes de question-réponse fonctionnent de la façon suivante :
la question en langage naturel est analysée, afin d’en extraire les mots-clefs qui vont per-
mettre de soumettre une requête à un moteur de recherche traditionnel, ainsi que d’autres
informations qui seront utiles par la suite ;
la requête retourne un certain nombre de documents, dont les phrases, appelées candi-
dates, sont susceptibles de contenir la réponse à la question posée. Ces phrases sont
ensuite classées en fonction de critères d’adéquation à la question ;
enfin, à partir des informations de la question, les réponses possibles sont extraites des
phrases candidates et celle qui présente la plus forte pertinence est retournée ;

Le groupe LIR (Langues, Information, Représentations) a développé un système de Question
Réponse en anglais, appelé QALC1 , qui a participé aux campagnes d’évaluation internationales
TREC2 ces dernières années, arrivant 9ème sur 34 en 2002 ; sa version française participera à
l’évaluation du programme EVALDA-Equer3 en 2004.
Le fonctionnement de QALC, à l’image des autres systèmes, peut être décomposé en plusieurs
étapes : analyse de la question, sélection de documents pertinents et reconnaissance des entités
nommées4 , et enfin extraction de la réponse (Ferret et al., 2003). L’architecture globale du
système QALC est présentée dans la figure 1.
La stratégie d’extraction de la réponse, qui nous intéresse plus particulièrement ici, dépend du
type de la question :
si la question attend une entité nommée d’un certain type, la réponse sera l’entité nommée
du type attendu la plus proche des mots de la question ;
sinon, des patrons linguistiques, sémantiques ou syntaxiques, seront appliqués aux phrases
candidates.
Si l’on a pu déterminer des contraintes sémantiques sur la réponse, des patrons séman-
tiques seront utilisés (Dalmas, 2002). Par exemple pour la question “Name a stimulant.”,
la réponse doit être un hyponyme5 de “stimulant”. On va donc rechercher une réponse
qui vérifie cette contrainte, et qui de plus soit liée par exemple par “such as” à “stimu-
lant”, ce qui permettra de valider une réponse comme “Stimulants such as Ritalin”. Les
contraintes sémantiques sont vérifiées dans la base de données WordNet 6 ;
1
Question Answering program of the Language and Cognition group at LIMSI-CNRS
2
Text REtrieval Conference, http://trec.nist.gov/. Pour une présentation de TREC 2002, voir (Voorhees, 2002)
3
Evaluation lancée par le Ministère de la Recherche, dans le cadre de l’Action Technolangue,
http://www.recherche.gouv.fr/appel/2002/technolangue.htm
4
Expression qui, dans un contexte particulier, identifie de façon unique une entité telle qu’une personne, un
lieu, une organisation, une date; les noms propres correspondent à une sous-classe importante d’entités nommées.
5
La réponse doit avoir une relation de type “est un” avec “stimulant”
6
http://www.cogsci.princeton.edu/ wn/. Voir aussi (Fellbaum, 1998)
Système de Question Réponse :
apport de l’analyse syntaxique lors de l’extraction de la réponse

Questions                                     Corpus

Traitement des                      Moteur de
questions                           recherche
Catégories des     Termes            Documents
questions, Focus,   candidats         retournés
Type de la réponse

Traitement des documents:
Ré−indexation et sélection
des documents (Fastr)
Documents classés
Reconnaissance
des entités nommées
Phrases étiquetées
Recherche de la réponse:           − reconnaissance du focus
− appariement question/phrase
− extraction de la réponse
Séquences ordonnées de 50 caractères
Figure 1: Architecture du système QALC
si aucune contrainte sémantique n’a été déterminée, des patrons syntaxiques permettront
d’apparier les phrases réponses et la question. Ces patrons sont fondés sur l’idée intuitive
que les phrases réponses correspondent à des structures syntaxiques précises, la plus sim-
ple étant la reformulation à l’affirmative de la question. Une étude manuelle de corpus a
permis de dégager des structures syntaxiques typiques pour chaque catégorie de question,
qui ont été formalisées sous forme de patrons.
La figure 2 présente des exemples de chaque stratégie. Les performances du système QALC
dépendent bien entendu du type de la question, et donc de la stratégie mise en œuvre. Le
pourcentage de bonnes réponses retournées par QALC est ainsi de 29% lorsque la question
attend une entité nommée, et de 20% pour les autres questions. Mais ces résultats dépendent
également des phrases réponses disponibles dans le corpus. En effet, selon la forme de la phrase
réponse, les stratégies de QALC seront plus ou moins adaptées.
Nous détaillerons dans la suite de cet article des problèmes qui peuvent se poser lors de l’extrac-
tion de la réponse, et montrerons qu’une analyse syntaxique des phrases candidates peut en
résoudre certains. Puis nous donnerons les principes du module que nous avons développé, qui
procède à un appariement syntaxique question-réponse, et commenterons ses possibilités et ses
limites. Enfin, nous présenterons une perspective un peu plus large d’utilisation de l’analyse
syntaxique des phrases, qui est la construction de réponses à partir de plusieurs documents.
Anne-Laure Ligozat
Entité nommée: What year was Alaska purchased ? (10.1398)
type EN = date
by 1867, when Secretary of State William H. Seward negociated

the purchase of Alaska from the Russians, ...

Patron sémantique:    Which political party is Lionel Jospin a member of ? (10.1408)
type sémantique
the first secretary of the French Socialist Party Lionel Jospin

Patron syntaxique:   What does Knight Ridder publish ? (9.671)
What do GN VB
Knight Ridder published 30 daily newspapers ...
GNfocus VB GNréponse
Figure 2: Exemples de chacune des stratégies

2 Analyse syntaxique des phrases candidates

Dans certains cas, la structure des phrases candidates peut entraîner l’extraction d’une mauvaise
réponse par QALC. Pour la question “Who killed Lee Harvey Oswald ?”, une phrase candidate
contenant la réponse est “Jack Ruby, who killed J.F. Kennedy assassin Lee Harvey Oswald”
(Hovy et al., 2001). La sélection de l’entité nommée la plus proche de l’objet de la question
“Lee Harvey Oswald” retourne comme réponse “J.F. Kennedy” au lieu de “Jack Ruby”.
L’analyse de la question nous donne des indices sur la fonction syntaxique de la réponse at-
tendue, qui peuvent être utilisés pour retourner la bonne réponse : d’après la question, l’entité
nommée attendue doit être agent du verbe “kill”; l’analyse de la phrase réponse montrerait que
cet agent est bien “Jack Ruby” (Monceaux et al., 2002). Des connaissances syntaxiques sur la
fonction attendue de la réponse peuvent ainsi améliorer l’extraction de la réponse dans QALC,
et nous nous sommes donc intéressés aux possibilités d’utilisation de ce type de connaissances.
2.1 Amélioration de la sélection des entités nommées

Dans le cas d’une question attendant une entité nommée, l’entité nommée la plus proche des
mots de la question ne correspond pas toujours à la réponse, comme le montre l’exemple précé-
dent (“Who killed Lee Harvey Oswald ?”). Des connaissances sémantiques ne pourront pas
non plus permettre de rejeter la mauvaise réponse, car d’une part les noms propres sont peu
présents dans les bases de données sémantiques, et d’autre part lorsqu’ils le sont, ils ont sou-
vent les mêmes caractéristiques sémantiques (à l’exception de leurs définitions (gloses) mais
cette information est difficile à traiter). Seule une analyse syntaxique des relations de la phrase
permettra donc de montrer que “Jack Ruby” est la bonne réponse, puisqu’il est sujet du verbe
“kill”.
2.2 Un outil plus puissant que les patrons linguistiques

Les patrons linguistiques présentent également des failles car ils ne tiennent compte que de
l’objet de la question et de connecteurs sans tenir compte de leur environnement, ce qui peut
induire en erreur. Une expression telle que “a person has a seizure disorder or epilepsy if...”
Système de Question Réponse :
apport de l’analyse syntaxique lors de l’extraction de la réponse
répond bien à la question “What is epilepsy ?”, car elle correspond au patron “GN or epilepsy”,
qui permet de repérer une définition de l’épilepsie. Mais ce même patron ne permettra pas
de rejeter la phrase “Delirium tremens or epilepsy imply serious behavioral disorders” comme
une réponse possible. Un patron sémantique ne pourra probablement pas non plus discrim-
imer les deux possibilités de réponse “seizure disorder” et “delirium tremens” car aucune des
deux n’est directement reliée à “epilepsy” dans WordNet. Une analyse syntaxique fine pourrait
en revanche, en tenant compte du pluriel du verbe “imply”, montrer que le patron syntaxique
ne peut s’appliquer dans le second cas, et que seule la première réponse doit être retournée.
D’autres inconvénients rendent par ailleurs délicate l’utilisation de patrons linguistiques : ceux-
ci sont doivent en effet connsidérer le plus de constructions possibles, ce qui rend leur nombre
important, leur construction fastidieuse et leur couverture nécessairement incomplète.
2.3 Quels outils pour l’analyse syntaxique des phrases candidates ?

QALC effectue une analyse fine des questions grâce à un analyseur syntaxique, qui fournit en
particulier le type de la question, son objet (au sens du “focus” défini par (Lehnert, 1978)),
le type attendu de la réponse, entité nommée ou type sémantique général, et les relations de
dépendance de la question. Cette analyse est rendue possible notamment par la forme syntax-
ique relativement figée des questions. En revanche, les phrases candidates sont analysées par
un étiqueteur morpho-syntaxique, qui ne retourne qu’une analyse partielle de ces phrases. Afin
d’améliorer nos connaissances sur ces phrases, un analyseur syntaxique pourra également être
utilisé, qui nous fournira, en plus de l’étiquetage morphologique des phrases, les fonctions syn-
taxiques de leurs éléments. Nous avons choisi d’utiliser pour le système QALC, l’analyseur
syntaxique pour le français XIP7 de Xerox, dont une description détaillée pourra être trouvée
dans (Aït-Mokthar et al., 2002).
2.4 Evaluation préliminaire des apports de l’analyse syntaxique

Les quelques exemples ci-dessus prouvent que l’analyse syntaxique résoudrait certaines diffi-
cultés du système actuel, mais il convient de se demander si ces exemples sont marginaux ou
non. Nous avons donc mené une étude manuelle sur les réponses de QALC à 106 questions
de l’évaluation TREC-11, choisies aléatoirement, pour tester si une analyse syntaxique sup-
posée exacte pourrait améliorer le score de QALC. Et ce de deux façons : soit en confirmant les
bonnes réponses, soit en rejetant les mauvaises. Les résultats de notre travail sont résumés dans
la table 1.

Nombre de questions   Bonnes réponses confirmées   Mauvaises réponses rejetées   Réponses justes rejetées
106                     45%                         12,5%                          5%
Table 1: Résultats de l’utilisation d’une analyse syntaxique sans erreur
Les différentes colonnes doivent être interprétées ainsi : l’étude a été menée sur 106 questions
du corpus TREC-11 et sur la phrase candidate retournée comme réponse par QALC ; une anal-
yse syntaxique aurait confirmé 45% des réponses justes que QALC avait retourné et rejeté 5%
7
Xerox Incremental Parsing
Anne-Laure Ligozat
de ces réponses. Pour les 50% restant, la possibilité de confirmer la bonne réponse dépend de la
finesse de l’analyseur syntaxique, et des reformulations prises en compte. Parmi les questions
pour lesquelles QALC avait donné une mauvaise réponse, plus de 12% auraient été rejetées par
une analyse syntaxique. En conclusion, on remarque qu’un pourcentage important des réponses
fausses aurait été rejetées avec une analyse syntaxique, et que près de la moitié des bonnes
réponses auraient été confirmées, ce qui est un point très positif pour la fiabilité du système.
Intégrer une analyse syntaxique représente donc un avantage important.
Il est à noter que les informations utilisées pour cette évaluation sont celles que donne un analy-
seur syntaxique, en supposant qu’il ne fait aucune erreur. Même s’il n’est donc pas irréaliste de
s’attendre à ces résultats, les informations fournies par un analyseur réel comprennent souvent
des erreurs, et on peut donc s’attendre à des performances moins élevées que celles prévues par
cette étude.
Cette évaluation nous a également permis de dégager les relations qui permettent, le plus sou-
vent, de rejeter une mauvaise réponse ou de confirmer une bonne. Il conviendra donc de
s’intéresser d’une part aux relations sur l’objet de la question, et d’autre part sur la fonction
syntaxique de la réponse. Partant de ces conclusions, nous avons développé un prototype
d’appariement question-réponse.
2.5 Appariement syntaxique question-réponse

L’appariement que nous avons construit consiste à déterminer les relations de dépendance de
la question, en particulier celles qui concernent l’objet de la question et le mot interrogatif,
puis à rechercher les mêmes relations dans les phrases candidates (phrase 1) ou des relations
équivalentes (phrases 2 et 3).
Le schéma suivant expose un exemple d’appariement :

Question:        Who is the evil H. R. Director in Dilbert ?

sujet    attribut
Phrase 1    Catbert is the evil H. R. Director in Dilbert.
sujet     attribut
Réponses possibles:
Phrase 2     The evil H. R. Director is Catbert.

sujet attribut

Phrase 3     Catbert, the evil H. R. Director in Dilbert...
apposition
Figure 3: Principe de l’appariement syntaxique

Sera donc privilégié comme réponse, le groupe nominal possédant la fonction syntaxique voulue,
dans une phrase où l’objet de la question a, autant que possible, les attributs attendus. Cette ap-
proche reprend en partie la stratégie des patrons syntaxiques, mais d’une part, elle peut facile-
ment améliorer les autres traitements (la recherche d’entités nommées et l’utilisation de patrons
Système de Question Réponse :
apport de l’analyse syntaxique lors de l’extraction de la réponse
sémantiques), et d’autre part, elle est plus fine que des patrons.
Un module effectuant cet appariement a été développé et sera testé sur un corpus français dès
que notre système aura été adapté à cette langue, sur un corpus de test construit par le groupe
LIR du LIMSI. Néanmoins, on peut s’attendre à ce que ce module fasse preuve d’une précision
relativement forte, mais d’un rappel faible. En effet, lorsque les relations syntaxiques de la
phrase candidate sont trop éloignées de celles qui sont attendues, le module ne renvoie pas de
réponse. Du fait des nombreuses variations contenues dans les phrases candidates, ce cas de
figure devrait se produire fréquemment.
2.6 Améliorations possibles

Afin de pallier les difficultés relevées précédemment, il est tout d’abord nécessaire de pouvoir
recourir aux stratégies actuelles si ce module échoue à fournir une réponse. Afin d’améliorer les
performances de notre module, on pourra également tenter de prendre en compte les variations
sémantiques. Plusieurs pistes peuvent être envisagées, selon le degré de variation entre les
formulations de la question et des phrases réponses.
La variation peut se situer au niveau du mot, comme dans l’exemple “- What was the name of
the dog in the Thin Man movies ? - in The Thin Man, Asta, the wire haired terrier...”. Dans
ce cas, il peut être intéressant d’utiliser un dictionnaire des synonymes, qui pourra nous fournir
une relation entre “wire haired terrier” et “dog”, ou une base de données comme WordNet ou
EuroWordNet8 , qui nous donnera une relation d’hyponymie entre ces deux termes.
Si la variation met en jeu une reformulation de la phrase, des patrons de reformulation pourront
être utilisés dans certains cas, permettant par exemple de passer d’une question du type “Who
invented...” à une phrase contenant “the inventor of...”. Un outil comme Fastr 9 , déjà utilisé
lors de la construction de la requête pour le moteur de recherche, pourrait engendrer ces refor-
mulations. Dans d’autres cas cependant, il est difficile de trouver le lien sémantique entre les
formulations de la question et de la réponse (“- What school did Emmitt Smith go to ? - Emmitt
Smith, then a freshman at Escambia High School”) et de plus, la relation entre l’objet de la
question et la réponse n’est pas toujours explicite (“- What is the chemical formula for sulphur
dioxide? - Sulphur dioxyde (SO2)”).
Enfin, l’extraction de la réponse peut nécessiter la mise en œuvre de raisonnements complémen-
taires, par exemple dans les tâches “List” ou “Context” de TREC, pour lesquelles la réponse
attendue est une liste (“Name 7 brand names of Belgian chocolates.”) ou les questions posées
simulent un dialogue (“- How many species of spiders are there ? How many are poisonous to
humans ? What percentage of spider bites in the US are fatal ?”). Si les scores des meilleurs sys-
tèmes à TREC sont très élevés (83% de bonnes réponses pour LCC à TREC 2002), les questions
posées contiennent en grande majorité des questions factuelles : la distribution des questions
TREC entre 1999 et 2001 est présentée dans la table 2 (Moldovan et al., 2003).
Plus de 95% des questions TREC sont donc des questions factuelles ou nécessitant un raison-
nement simple, ce qui est loin d’être représentatif des besoins réels en recherche d’information.
De nombreuses problématiques demeurent. Certaines font l’objet de nouvelles tâches aux con-
férences TREC ; d’autres pourront être abordées par d’autres évaluations comme EVALDA-
8
http://www.illc.uva.nl/EuroWordNet/
9
Analyseur transformationnel de surface qui reconnaît les occurrences et les variantes des termes en entrée
(Jacquemin et al., 1997)
Anne-Laure Ligozat
Type         Nombre(%)
Factuel       985 (67.5%)
Raisonnement simple 408 (27.9%)
Liste         25 (1.7%)
Contextuel      42 (2.9%)
Spéculatif       0 (0.0%)
Table 2: Distribution des questions TREC

Equer.
3 Vers des inférences en contexte large

3.1 Des réponses réparties dans plusieurs documents

Bien qu’une réponse courte issue d’un document unique soit exigée dans les conférences TREC,
une réponse plus adéquate pourrait souvent être donnée par regroupement d’informations dis-
persées dans plusieurs passages d’un texte. En effet, pour certaines questions, les éléments de
réponse apparaissent à plusieurs endroits du texte, avec des références contextuelles différentes
mais compatibles. Ainsi, pour la question “Quel coureur espagnol a gagné une étape du Tour de
France en 2002 ?”, les informations sur les résultats de l’épreuve et celles sur les nationalités des
coureurs peuvent appartenir à des passages différents du corpus. Il faudra donc pouvoir relier
ces informations qui partageront un même contexte thématique (le Tour de France) et temporel
(en 2002). Les questions de définition (“What is epilepsy ?”) pourraient également bénéficier
d’un procédé permettant de regrouper différents éléments de réponse.
Cette possibilité de trouver la réponse à partir de textes différents pourrait également être mise
à profit pour améliorer la fiabilité des réponses. Ainsi, pour la question “Quel compositeur a
donné un concert devant la tour Eiffel le 14 Juillet 1995 ?”, dont une phrase réponse est “Le
serveur du ministère de la culture vient d’ouvrir, sur Internet, un site consacré au concert donné
par Jean-Michel Jarre devant la tour Eiffel, le 14 juillet.”, certains éléments de la question ne
sont pas présents dans la phrase réponse, puisqu’il n’y est par exemple pas précisé que Jean-
Michel Jarre est compositeur. L’information manquante pourrait être cherchée dans une autre
partie du corpus, ou dans un autre corpus comme le Web, afin de pouvoir accorder un score de
confiance plus important à cette réponse10 .
3.2 La fusion d’informations : au cœur des problématiques de Question
Réponse

L’objectif est donc d’arriver à constituer des éléments de connaissance cohérents à partir d’infor-
mations dispersées dans plusieurs passages. Cette problématique est l’un des principaux défis
pour le domaine de Question Réponse dans les prochaines années, selon le roadmap des con-
10
Cette information pourrait éventuellement être recherchée dans une base de données de noms propres, mais
ne pourra être trouvée dans WordNet, qui ne recense qu’un nombre limité de noms propres.
Système de Question Réponse :
apport de l’analyse syntaxique lors de l’extraction de la réponse
férences TREC (Burger et al., 2001). La classification des systèmes de Question Réponse selon
leurs capacités (Moldovan et al., 2003), utilisée précédemment, révèle également l’importance
de pouvoir traiter ce type de problème. Cette taxonomie se présente les cinq classes de sys-
tèmes suivantes : systèmes capables de traiter des questions factuelles, systèmes mettant en œu-
vre des mécanismes de raisonnement simple, permettant par exemple de répondre à la question
“How did Socrates die ?” par la phrase réponse “drinking poisoned wine”, systèmes capables
d’extraire la réponse à partir de plusieurs documents, les questions pouvant attendre par ex-
emple une liste comme réponse (“Name 20 countries that produce coffee.”) ou bien une suite
d’instructions (“How do I assemble a bicycle ?”), systèmes interactifs, pouvant répondre à des
questions qui se suivent (“How long was the Varyag ?”; “How wide ?”), systèmes à capacité
spéculative (“Is the US out of recession ?”).
La fusion d’informations provenant de plusieurs documents serait à ranger dans la classe 3, bien
que le point de vue adopté ici soit un peu différent. En effet, le recours à plusieurs documents
n’est considéré par (Moldovan et al., 2003) que pour construire une réponse complexe, alors
que nous l’envisageons également pour déterminer ou justifier la réponse.
3.3 Les différents niveaux de raisonnement nécessaires

La mise en œuvre de la fusion d’informations peut se révéler complexe, car si dans certains cas
elle nécessite peu de raisonnements, il faudra dans d’autres cas utiliser des processus inférentiels
complexes pour mettre en relation les différentes connaissances utiles.
Ainsi, dans les exemples cités plus haut (“Quel coureur espagnol a gagné une étape du Tour de
France en 2002 ?” et “Quel compositeur a donné un concert devant la tour Eiffel le 14 Juillet
1995 ?”), les éléments de réponse sont directement présents dans le corpus. On peut en re-
vanche envisager que la construction de la réponse fasse appel à des connaissances sémantiques
plus poussées : pour la question “How many casualties were reported last week in Fredonia?”
associée aux phrases réponses “Last Monday two people were killed on the streets of Beau-
tiville, Fredonia, after a bomb exploded” et “The terrorists murdered a family with a small child
in Fredonia last Friday, near its border with Evilonia.” (Burger et al., 2001), il faudra déter-
miner que “casualties” regroupe les événements évoqués par les termes “kill” et “murder”. Les
mécanismes nécessaires peuvent également faire appel à un raisonnement de type causal : la
réponse à la question “Why there were hacker attacks on the computers at University of Cali-
fornia, Santa Barbara?” nécessite de relier les informations présentes dans les phrases réponses
“U.S. Colleges have powerful computing facilities” et “Computer hackers need speedy proces-
sors to break security passwords” et donc d’effectuer un raisonnement causal. Les ressources
à mettre en œuvre peuvent donc être de natures très diverses, selon les informations apportées
par le corpus.
Nous nous proposons d’étudier les cas où des connaissances sémantiques devraient permettre
de répondre aux questions. L’objectif est de déduire de la question les relations à rechercher
dans le corpus, et de trouver ensuite des textes les contenant, avant de relier ces connaissances.
La reconnaissance de contextes thématiques ou temporels permettra de relier les connaissances
partageant un même contexte. Nous parlerons dans ces conditions de chaîne d’inférences valide.
La construction de telles chaînes se fera à partir d’ontologies générales et de relations issues du
texte, afin de ne pas dépendre d’une base de connaissances, forcément incomplète.
Anne-Laure Ligozat
4 Conclusion
Afin de répondre aux failles de notre système de Question Réponse, nous avons développé un
module d’appariement question-réponse s’appuyant sur les relations syntaxiques données par
un analyseur. Ce module sera intégré à QALC, afin de valider notre modèle d’appariement et
d’étudier son impact sur les performances de notre système. La représentation syntaxique des
phrases élaborée dans ce cadre servira ensuite de base pour fusionner des informations lorsque
plusieurs textes sont nécessaires à l’extraction de la réponse.
Références
Aït-Mokthar S., Chanod J., Roux C. (2002), Robustness beyond shallowness : incremental deep parsing,
Journal of Natural Language Engineering, Vol. 8, No. 3-2.
Burger J., Cardie C., Chaudhri V., Gaizauskas R., Harabagiu S., Israel D., Jacquemin C., Lin C.-Y.,
Maiorano S., Miller G., Moldovan D., Ogden B., Prager J., Riloff E., Singhal A., Shrihari R., Strza-
lkowski T., Voorhees E., Weishedel R. (2001), Issues, Tasks and Program Structures to Roadmap Re-
search in Question & Answering (Q&A), National Institute of Standards and Technology, http://www-
nlpir.nist.gov/projects/duc/papers/qa.Roadmap-paper_v2.doc
Dalmas T. (2002), Sélection et validation de la réponse dans les systèmes de question-réponse, un sys-
tème à base de contraintes sémantiques, Rapport de DESS, LIMSI-CNRS.
Fellbaum C. (1998), WordNet : An Electronic Lexical Database, Cambridge, Massachussetts, The MIT
Press.
Ferret O., Grau B., Hurault-Plantet M., Illouz G., Jacquemin C., Monceaux L., Robba I., Vilnat A.
(2003), How a NLP approach benefits question answering, Knowledge Organization journal, A Special
Issue on Evaluation of HLT, Guest Editor : Widad Mustafa El Hadi, Vol. 29, 3-4.
Hovy E., Hermjakob U., Lin C.-Y. (2001), The Use of External Knowledge in Factoid QA, TREC 10
Notebook.
Jacquemin C., Klavans J. L., Tzoukermann E. (1997), Expansion of multi-word terms for indexing and
retrieval using morphology and syntax, Actes de ACL-EACL’97, 24-31.
Lehnert W. (1978), The process of question answering, Hillsdale, N.J., Lawrence Erlbaum Associates.
Moldovan D., Pa¸sca M., Harabagiu S., Surdeanu M. (2003), Performance Issues and Error Analysis in
an Open-Domain Question Answering System, ACM Transactions on Information Systems, Vol. 21, No.
2, 133-154.
Monceaux L., Robba I. (2002), Les analyseurs syntaxiques : atouts pour une analyse des questions dans
un système de question-réponse ?, Actes de TALN2002, 195-204.
Voorhees E. (2002), Overview of the TREC 2002 Question Answering Task, TREC 2002.
