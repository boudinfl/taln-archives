RECITAL 2004, Fès, 21 avril 2004
Méthodes statistiques et apprentissage automatique pour
l’évaluation de requêtes en recherche documentaire
Jens GRIVOLLA∗†
encadré par Pierre Jourlin et Renato de Mori
Laboratoire Informatique d’Avignon (LIA)
Résumé - Abstract
Pour la recherche documentaire il est souvent intéressant d’avoir une bonne mesure de confiance
dans les réponses trouvées par le moteur de recherche.
Une bonne estimation de pertinence peut permettre de faire un choix entre plusieurs réponses
(venant éventuellement de différents systèmes), d’appliquer des méthodes d’enrichissement ad-
ditionnelles selon les besoins, ou encore de permettre à l’utilisateur de prendre des décisions
(comme d’approfondir la recherche à travers un dialogue).
Nous proposons une méthode permettant de faire une telle estimation, utilisant des connais-
sances extraites d’un ensemble de requêtes connues pour en déduire des prédictions sur d’autres
requêtes posées au système de recherche documentaire.
In document retrieval applications it is often interesting to have a measure of confidence in the
answers found by the retrieval system.
A good relevance estimation can allow us to make a choice between different answers (possibly
provided by different sources), apply additional expansion techniques according to the specific
needs, or enable the user to make decisions (such as to refine the search interactively).
We propose a method that allows us to make such estimations, using knowledge extracted from
a known query corpus to deduce predictions on new queries presented to the document retrieval
system.
Mots-clefs – Keywords
apprentissage/décision automatique, recherche documentaire, expansion de requêtes, évaluation
de difficulté
automatic learning/decision, document retrieval, query expansion, difficulty evaluation
∗jens.grivolla@lia.univ-avignon.fr
†avec le soutien de Digitech S.A. et de la Région PACA
Jens Grivolla
1 Introduction
Un système de recherche documentaire doit trouver dans une base de documents (typiquement
une collection de textes) les documents répondants à une requête posée (en langage naturel) par
un utilisateur.
En général, un système de recherche documentaire répondra à une requête posée par une liste
ordonnée de documents. Le but dans la conception d’un système de recherche est de maximiser
le nombre de documents pertinents dans la liste, surtout en tête de liste, et de minimiser le
nombre de documents non-pertinents.
Dans la plupart des applications, la performance d’un système sera jugé selon la précision, c’est-
à-dire le pourcentage de documents pertinents parmi les documents trouvés par le système de
recherche. Une autre mesure importante est le rappel, le pourcentage des documents pertinents
contenus dans la collection qui ont été retournés par le système.
Nous avons utilisé la précision moyenne, qui tient compte aussi bien du rappel que de la
précision, ainsi que du classement relatif des documents dans la liste.
Beaucoup de travail est fait par un grand nombre de chercheurs pour améliorer les perfor-
mances, en particulier par des méthodes d’enrichissement de requêtes, c.à.d. des modifications
des requêtes p. ex. en rajoutant des termes additionnels (voir section 3) afin de trouver des
documents pertinent supplémentaires.
Malheureusement, ces méthodes ne sont efficaces que pour une partie des requêtes et peuvent
même dégrader les résultats pour d’autres. Pour certaines requêtes aucun système n’arrive à
fournir de réponses satisfaisantes.
De ceci se dégagent deux problématiques :
– déterminer le traitement optimal pour chaque requête
– identifier les requêtes pour lesquelles la recherche échoue afin de p. ex. poursuivre une ap-
proche interactive
2 L’environnement d’application
Toutes les expérimentations présentées dans cet article ont été effectuées sur la base du corpus
de documents et des requêtes issues des campagnes TREC (Text REtrieval Conference). Pour la
discipline appelée adhoc, la recherche non-interactive automatique à partir de requêtes écrites
en langage naturel, ces campagnes fournissent chaque année un corpus de documents (environ
500 000) et 50 requêtes, ainsi que les moyens pour évaluer automatiquement les performances
d’un système de recherche documentaire sur ces données.
Les requêtes sont disponibles sous différentes formes, pouvant être composées d’un titre
(quelques mots clés), d’un descriptif (une ou deux phrases) et d’un narratif (explication
détaillée de la thématique recherchée).
Nos méthodes ont été appliquées sur la base d’un système de recherche documentaire utilisant
le modèle probabiliste. Le système implémenté par Pierre Jourlin est relativement classique,
proche du système OKAPI, et avait préalablement servi dans le contexte du «spoken document
retrieval» (SDR) (Jones et al., 2001).
Méthodes statistiques et apprentissage automatique pour l’évaluation de requêtes
en recherche documentaire
3 L’enrichissement des requêtes
Une méthode pour enrichir des requêtes sans interaction avec l’utilisateur est le blind relevance
feedback (BRF)(Walker & de Vere, 1990).
Partant de l’hypothèse que les premiers documents dans la liste retournée par le système de
recherche ont une forte probabilité d’être pertinents, un nombre t de termes caractérisant les d
premiers documents est ajouté à la requête initiale. La requête ainsi enrichie peut permettre de
retrouver des documents pertinents qui ne contiennent pas les termes employés dans la requête
d’origine, et ainsi augmenter le rappel. Diverses études sur le BRF ont montré que la précision
est (en moyenne) également meilleure en utilisant des requêtes enrichie de cette manière.
Nous avons constaté que (quel que soit le paramétrage ou le type de requête) l’application
de BRF dégrade les résultats obtenus pour au minimum environ 30% des requêtes, malgré une
amélioration globale des performances. Pour les requêtes courtes consistant uniquement du titre
ou du descriptif, le taux d’échec peut dépasser les 50%. Il est donc intéressant d’éviter cet effet
négatif pour les requêtes concernées.
3.1 La difficulté des requêtes et l’effet de l’enrichissement
Nos travaux portant sur les deux aspects de la difficulté des requêtes posées et de l’effet qu’ont
des techniques d’enrichissement selon la requête, nous avons également analysé le lien entre la
précision moyenne obtenue (sans BRF) et le gain apporté par l’enrichissement de la requête.
 0.25
 0.2
 0.15
 0.1
 0.05
 0
-0.05
-0.1
-0.15
-0.2
-0.25
 0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9
avg. prec. w/o BRF
FIG. 1 – précision moyenne et gain par BRF : TREC 8 ad-hoc
On constate dans la figure 1 que l’expansion de requête fonctionne surtout sur des requêtes pour
lesquelles on obtient des performances moyennes sans BRF, alors que pour les requêtes qui
donnaient des résultats très bon ou très mauvais l’enrichissement a plutôt tendance à dégrader
le résultat.
On peut supposer que ceci s’explique principalement par le fait que l’enrichissement utilise des
mots extraits des premiers documents dans la liste. Si ceux-ci ne sont pas pertinents, il n’est pas
possible de trouver des termes représentatif de la thématique recherchée, l’expansion de requête
ne peut donc pas améliorer les performances. Dans le cas contraire, les documents trouvés étant
bons, une nouvelle recherche avec un classement différent des documents risque de déclasser
les documents pertinents initialement trouvés et ainsi de détériorer le résultat.
gain through BRF
Jens Grivolla
4 Notre approche
L’approche poursuivie consiste à déterminer un ensemble d’attributs calculables sur la base de
la requête même et (selon les informations disponibles) des réponses ainsi que du processus
ayant conduit à la réponse (scores internes du système de recherche documentaire, documents
dont la réponse est extraite dans les applications QA, etc.)
Chaque requête ou chaque couple requête-réponse sera ainsi représenté par un vecteur d’at-
tributs d’une dimension variant selon l’ensemble d’attributs choisi. En utilisant un corpus de
requêtes pour lesquelles la performance du système est connue, on peut ensuite appliquer des
méthodes de classification automatique afin d’aboutir à des prédictions par rapport au critère de
classification (p. ex. la difficulté de la requête en termes de précision moyenne obtenue) pour
des nouvelles requêtes non comprises dans ce corpus d’apprentissage.
Nous avons développé une liste d’attributs plus ou moins fortement corrélés avec la difficulté
de la requête (en terme de précision obtenue par le moteur de recherche). Sur la représentation
vectorielle de la requête, nous avons appliqué différentes méthodes de classification et décision
automatique (arbres de décision, SVM, etc.) afin d’obtenir des prédictions.
Les attributs choisis peuvent se diviser en deux types :
– des attributs basés purement sur la requête même
– des attributs utilisant les résultats d’une première phase de recherche
La première catégorie comprend p. ex. la longueur de la requête, des mesures de spécificité
des termes de la requête (hyponymie, IDF, ...) ou encore des mesures d’ambiguı̈té (le nombre
de sens des termes de la requête, ...) ainsi que des scores divers calculés sur la base de telles
propriétés.
Parmi les attributs du deuxième type ce trouvent en particulier des mesures de similarité ou dis-
tance entre les documents trouvés, les scores des documents d’après la formule Okapi-BM251
(Robertson et al., 1996) et des scores dérivés.
Nous avons utilisé différents classifieurs génériques sur différentes représentations vectorielles
des informations disponibles : arbres de décision, CAL5, Dipol, SVM, NaiveBayes, réseaux
de neurones, etc. Le système WEKA (Witten & Frank, 1999) permet l’utilisation d’un grand
nombre de classifieurs différents, mais nous avons également évalué d’autres implémentations.
Les algorithmes à base d’arbres de décisions présentent l’avantage de construire des règles de
classification qui peuvent être lues et interprétées. Ceci peut livrer des informations intéressantes
et exploitables sur le processus de classification. Par contre, sur le nombre limité de données dis-
ponible d’autres classifieurs (en particulier les SVM) permettent d’obtenir de meilleurs résultats
de classification.
Nous avons également utilisé un classifieur spécial pour la catégorisation de textes, les «Seman-
tic Classification Trees» (SCT) (Kuhn & De Mori, 1995), qui construit à partir d’un ensemble
de textes (dans notre cas des requêtes) un arbre basé sur des expressions régulières (extraites
automatiquement) qui servent à diviser le corpus sur les différentes branches de l’arbre.
1∑ ∑ +1)×tft,d
t cwt,d = t qtf
(K
t K×(1−b+b×Ld)+tf log
N
t,d Nt
Méthodes statistiques et apprentissage automatique pour l’évaluation de requêtes
en recherche documentaire
5 Résultats
5.1 Estimation de difficulté
Sur l’estimation de difficulté des requêtes nous avons séparé les requêtes en deux classes :
«faciles» et «difficiles», selon la précision moyenne obtenue par rapport à un seuil fixé.
Nous avons ensuite utilisé la méthode «leave one out» afin de découper le corpus de requêtes
disponibles en entraı̂nement et test. Pour chaque requête, un classifieur a ainsi été construit
sur la base du reste du corpus et appliqué sur la requête restante pour évaluer la qualité des
prédiction. La précision de classification devait ensuite être comparée au taux obtenable par
la connaissance de la distribution des classes (en faisant systématiquement la prédiction de la
classe plus grande).
Le seuil devait être fixé de manière relativement arbitraire (il dépendrait d’une application pra-
tique éventuelle), nous avons choisi de prendre pour l’instant la valeur médiane ainsi que la
moyenne pour évaluer notre système.
Actuellement, en utilisant un classifieur à base de SVM et un jeu d’environ 20 caractéristiques
(plus un grand nombre de variations) nous arrivons à un taux de classification correcte de 70%
avec la valeur médiane comme seuil (par rapport à 50% basé sur la distribution des classes).
Avec le seuil fixé à la valeur moyenne nous obtenons une précision de 73% (contre 62%).
Ces valeurs ont été obtenues sur les requêtes de TREC 8 et semblent être significativement
au-dessus des taux de référence. Il reste à vérifier si ces performances sont généralisables à
d’autres corpus, mais cela semble probable, aucune «optimisation» n’ayant été faite sur le
corpus donné. Il reste également à étudier quel taux de classification peut être obtenu avec des
seuils fixés différemment.
Utilisant un corpus très limité pour l’apprentissage du classifieur, les SVMs se sont avérés les
plus performants. Dans un contexte avec un plus grand nombre d’exemples disponibles, il est
pensable que d’autres méthodes obtiennent de meilleurs résultats, cela n’a pas encore pu être
vérifié.
Nous avons fait quelques tests avec des classifieurs capables de prédire des valeurs continues
afin d’avoir une estimation directe de la précision (du moteur de recherche) attendue, mais nous
n’avons pas obtenu de résultats satisfaisants, probablement dû à la faible quantité de données
dont nous disposons.
5.2 Décision sur l’enrichissement des requêtes
Au-delà de l’estimation de la qualité attendue du résultat, il est possible d’utiliser la classifica-
tion afin de décider de l’utilisation de méthodes d’expansion des requêtes (ou d’autres traite-
ments spécifiques).
Une première étude à permis de déterminer le potentiel de notre approche en supposant qu’on
soit capable d’une décision parfaite sur l’application d’enrichissement pour chaque requête.
En particulier, sur les requêtes de taille moyenne (titre et descriptif) de TREC 8, notre système
arrive à une moyenne (sur toutes les requêtes) des «précisions moyennes» de 24% sans ex-
pansion et 27% avec le meilleur paramétrage choisi globalement pour toutes les requêtes. Par
Jens Grivolla
contre, en choisissant les meilleurs paramètres pour chaque requête on arrive à 33%, avec un
simple choix binaire d’application de BRF sans variation des paramètres 29% sont possibles.
Il est donc possible (en principe) d’obtenir des performances considérablement supérieures aux
meilleures performances n’utilisant pas de décision pour chaque requête individuelle.
La décision sur le paramétrage précis de l’expansion à appliquer demande l’utilisation de clas-
sifieurs permettant une prédiction numérique des paramètres et suppose que l’estimation des
différents paramètres est indépendante (qu’on peut donc estimer les valeurs optimales de d et t
séparément).
Nous nous sommes pour l’instant contentés d’une décision binaire d’application ou non de
l’expansion (avec un paramétrage fixé) selon les probabilités d’une amélioration ou dégradation
des performances. Un grand choix de méthodes de classification peut être utilisé pour cela.
En appliquant les prédictions issues d’un classifieur dans la pratique nous n’obtenons actuelle-
ment pas de performances significativement meilleures que sans décision. De plus, nous avons
constaté que les résultats varient fortement selon le corpus de requêtes traité.
6 Conclusions et perspectives
Les résultats actuels pour l’estimation de difficulté sont prometteurs. L’effet de l’enrichissement
étant lié à la précision obtenue, il semble probable qu’on puisse arriver à des prédiction exploi-
tables, ce qui se traduirait directement en une meilleure performance du système utilisant ces
informations.
Le domaine «questions/réponses» semble se prêter particulièrement à notre approche, car au-
delà des attributs que nous avons utilisés pour la tâche «adhoc», une grande quantité d’infor-
mations linguistiques pourraient être exploitées.
A` plus long terme, il semble intéressant d’exploiter les informations extraites pour diriger un
dialogue avec l’utilisateur pour les requêtes qui ne peuvent pas être traitées correctement de
manière automatique.
Références
JONES K. S., JOURLIN P., JOHNSON S. & WOODLAND P. (2001). The cambridge multimedia docu-
ment retrieval (mdr) project : Summary of experiments.
KUHN R. & DE MORI R. (1995). The Application of Semantic Classification Trees to Natural Language
Understanding, volume 17, chapter 5, p. 449–460. IEEE Transactions on Pattern Analysis and Machine
Intelligence.
ROBERTSON S., WALKER S., JONES S., HANCOCK-BEAULIEU M. & GATFORD M. (1996). Okapi at
TREC-3. In Overview of the Third Text REtrieval Conference (TREC-3).
WALKER S. & DE VERE R. (1990). Improving subject retrieval in online catalogues : 2. relevance
feedback and query expansion. British Library Research Paper 72.
WITTEN I. H. & FRANK E. (1999). Data Mining : Practical machine learning tools with Java imple-
mentations. San Francisco : Morgan Kaufmann.
