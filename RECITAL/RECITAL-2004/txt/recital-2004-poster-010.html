<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
"http://www.w3.org/TR/html4/loose.dtd">
<html><head><title>Nouvelle m&#233;thode syntagmatique de vectorisation appliqu&#233;e au self-organizing map des textes vietnamiens</title>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
</head>
<body>
<div style="page-break-before:always; page-break-after:always"><div><p>RECITAL 2004, F&#232;s, 19-22 avril 2004 
</p>
<p>Nouvelle m&#233;thode syntagmatique de vectorisation 
appliqu&#233;e au self-organizing map des textes vietnamiens  
</p>
<p> 
NGUYEN Tuan-Dang 
</p>
<p>GREYC, Universit&#233; de CAEN 
Campus C&#244;te de Nacre, F-14032 Caen Cedex 
</p>
<p>tnguyen@info.unicaen.fr 
 
</p>
<p>R&#233;sum&#233; : 
 
Par ses caract&#233;ristiques &#233;minentes dans la pr&#233;sentation des donn&#233;es, Self-Organizing Map (SOM) 
est particuli&#232;rement convenable &#224; l&#8217;organisation des cartes. SOM se comporte d&#8217;un ensemble des 
vecteurs prototypes pour repr&#233;senter les donn&#233;es d&#8217;entr&#233;e, et fait une projection, en conservant la 
topologie, &#224; partir des vecteurs prototypes de n-dimensions sur une carte de 2-dimensions. Cette 
carte deviendra une vision  qui refl&#232;te la structure des classes  des donn&#233;es. Nous notons un 
probl&#232;me crucial pour SOM, c&#8217;est la m&#233;thode de vectorisation des donn&#233;es.  Dans nos &#233;tudes, les 
donn&#233;es se pr&#233;sentent sous forme des textes. Bien que le mod&#232;le g&#233;n&#233;ral du SOM soit d&#233;j&#224; cr&#233;&#233;, il 
nous faut de nouvelles recherches pour traiter des langues sp&#233;cifiques, comme le vietnamien, qui 
sont de nature assez diff&#233;rente de l&#8217;anglais. Donc, nous avons appliqu&#233; la conception du syntagme 
pour &#233;tablir un algorithme qui est capable de r&#233;soudre ce probl&#232;me. 
 
Mots-clefs :  
 
Self-Organizing Map, text mining, classification, vectorisation du texte, syntagme, &#233;valuation 
visuelle. 
 
1 Introduction 
La s&#233;lection des propri&#233;t&#233;s du texte est l&#8217;&#233;tape la plus importante dans le processus de vectorisation. 
La plupart des exp&#233;rimentations publi&#233;es dans ce domaine utilisent la m&#233;thode de d&#233;termination 
d&#8217;un vocabulaire, qui est des mots, pour repr&#233;senter le contenu du texte. N&#233;anmoins, les langues 
sont normalement diff&#233;rentes l&#8217;une de l&#8217;autre dans leur fa&#231;on et leur moyen d&#8217;exprimer les concepts. 
Nous trouvons, par exemple,  qu&#8217;une langue peut utiliser un seul mot pour exprimer une id&#233;e 
quelconque tandis que l&#8217;autre ne peut qu&#8217;exprimer la m&#234;me id&#233;e par un syntagme ou une 
proposition ou une phrase ou m&#234;me plusieurs phrases. Dans une langue monosyllabique comme le 
vietnamien, d&#8217;apr&#232;s la th&#233;orie moderne de la grammaire fonctionnelle [CAO Xuan Hao 1991, 
1998], &#224; cause de la limite du nombre des mots monosyllabes, le vietnamien est oblige d&#8217;utiliser des 
syntagmes pour exprimer des concepts que les langues indo-europ&#233;ennes expriment par un mot. Par 
exemple, pour exprimer le concept du &#171; v&#233;lo&#187;, en fran&#231;ais nous utilisons un seul mot mais en 
vietnamien nous devons utiliser un syntagme &#171; xe p &#187;. Dans cet exemple, un syntagme simple qui 
repr&#233;sente une classe des syntagmes stables, au point de vue de la composition structurelle. En effet, 
un syntagme peut se composer d&#8217;un nombre impr&#233;vu des mots. Cependant, le sen du syntagme n&#8217;est 
pas l&#8217;int&#233;gration du sens des mots composants mais le syntagme peut avoir un sens tout &#224; fait 
diff&#233;rent. L&#8217;id&#233;e de la cr&#233;ation d&#8217;un dictionnaire des syntagmes stables est impossible &#224; cause des </p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>NGUYEN Tuan-Dang 
</p>
<p>raisons suivantes: a) la combinaison r&#233;elle des mots pour cr&#233;er des syntagmes est illimit&#233;e; b) 
l&#8217;utilisation du dictionnaire pour segmenter les syntagmes dans la phrase est extr&#234;mement 
compliqu&#233;e parce qu&#8217;il n&#8217;y a pas des signes formels distinguant les noms, les verbes, les adverbes ... 
</p>
<p>Pour former une phrase, le vietnamien utilise des syntagmes fonctionnels qui sont structurellement 
dynamiques. Le sens de la phrase est pr&#233;sent&#233; par les id&#233;es composantes; chaque id&#233;e est r&#233;alis&#233;e 
par un syntagme mais non pas par des mots. Cela signifie aussi que le syntagme est r&#233;alis&#233; dans la 
phrase: il peut &#234;tre compos&#233; par un mot ou plusieurs mots ou m&#234;me une proposition; tout cela n&#8217;est 
pas important ni obligatoire. Certes, un syntagme de cette conception peut s&#8217;analyser en hi&#233;rarchie 
des syntagmes composants. Pour trouver les unit&#233;s grammaticales repr&#233;sentant des id&#233;es 
composantes de la phrase dans toute la collection des textes, nous cr&#233;ons un algorithme qui 
d&#233;termine des syntagmes et qui les analyse en syntagmes composants, jusqu&#8217;au niveau le plus bas 
que possible. Dans une phrase standard du vietnamien, il y a deux particules formelles, &#8220;TH&#204;&#8221; et 
&#8220;L&#192;&#8221;,  pour distinguer, au niveau logique le plus haut de la phrase, deux syntagmes importants: 
th&#232;me et commentaire. Le probl&#232;me difficile est que dans la plupart des phrases vietnamiennes, ces 
deux particules sont sous-entendues. Pour cette raison, l&#8217;algorithme doit utiliser des syntagmes d&#233;j&#224; 
analys&#233;s &#224; partir des phrases standard pour supposer des syntagmes existants dans les autres phrases.  
</p>
<p>2 Self-organizing map 
 
L&#8217;algorithme SOM d&#233;finit une projection non-lin&#233;aire &#224; partir d&#8217;une espace de Rn &#224; un tableau de 2-
dimensions qui contient M neurones. Les vecteurs d&#8217;entr&#233;e de n-dimensions dans l&#8217;espace d&#8217;origine 
sont not&#233;s comme ri et chaque neurone est connect&#233; &#224; un vecteur de r&#233;f&#233;rence de n-dimensions wi. 
L&#8217;algorithme concurrentiel SOM s&#8217;appuie sur la recherche des neurones les plus convenables aux 
vecteurs d&#8217;entr&#233;e, par le calcul des distances entre un vecteur avec tous les vecteurs r&#233;f&#233;rentiels des 
neurones pour trouver le neurone gagnant. L&#8217;adaptation des vecteurs r&#233;f&#233;rentiels se passera pour le 
neurone gagnant et ses voisinages. Alors, les voisinages du neurone gagnant apprennent aussi &#224; 
chaque vecteur d&#8217;entr&#233;e. Cet apprentissage local se r&#233;p&#232;te plusieurs fois et aboutit &#224; un ordre global. 
Ce dernier assure que les vecteurs proches dans l&#8217;espace d&#8217;origine de n-dimensions apparaissent aux 
neurones voisins sur une carte de 2-dimensions. Chaque &#233;tape de l&#8217;apprentissage se compose des 
&#233;tapes suivantes : 
</p>
<p>1. Choisir au hasard un vecteur d&#8217;entr&#233;e, calculer la distance entre ce vecteur et tous les vecteurs 
r&#233;f&#233;rentiels des neurones. 
</p>
<p>2. Choisir le neurone gagnant, qui a la distance la plus petite par rapport au vecteur d&#8217;entr&#233;e. La 
distance est pr&#233;d&#233;termin&#233;e. 
</p>
<p>3. Adapter les vecteurs r&#233;f&#233;rentiels au neurone gagnant j et &#224; ses neurones voisins. Le voisinage 
du neurone gagnant est d&#233;fini par la fonction Gaussienne. 
</p>
<p> 
3 Mod&#232;le de cr&#233;ation de la carte pour les textes vietnamiens 
 
3.1 Pr&#233;-traitement 
 
 M1. M&#233;thode de  seuil de la fr&#233;quence absolue: le seuil est  de 50, nous choisissons 2757 mots 
importants. 
 M2.  M&#233;thode de s&#233;lection des mots utiles du Rosengren : 2757 mots ayant l&#8217;indice KF les plus 
&#233;lev&#233;es. </p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Nouvelle m&#233;thode syntagmatique de vectorisation appliqu&#233;e au self-organizing map des textes vietnamiens 
</p>
<p> 
</p>
<p> M3. M&#233;thode de d&#233;termination des mots cl&#233;s du Guiraud : 2757 mots cl&#233;s les plus importantes 
 M4. M&#233;thode de s&#233;lection des groupes de mots : Cette m&#233;thode se base sur la notion des courts 
contextes. Un court contexte du mot m est pr&#233;sent&#233; par un mot pr&#233;c&#233;d&#233; et un mot succ&#233;d&#233; du mot m. 
Nous retenons des 5,090 groupes de mots. 
 M5. M&#233;thode de s&#233;lection des syntagmes g&#233;n&#233;raux. 
 
3.2 Algorithme de d&#233;termination des syntagmes g&#233;n&#233;raux 
 
</p>
<p>ENTR&#201;E: Ensemble des phrases de tous les textes. Ces phrases sont d&#8217;abord d&#233;compos&#233;es par 
les virgules s&#233;par&#233;es entre les propositions. 
</p>
<p>1. {}=S . 
2. Analyser toutes les phrases possibles en TH&#200;ME et COMMENTAIRE, par deux 
</p>
<p>particules &#8220;TH&#204;&#8221; et &#8220;L&#192;&#8221;. Soit R, ensemble des phrases d&#8217;entr&#233;e qui ne sont pas 
encore analys&#233;es par ces deux particules. Soit D, un ensemble des syntagmes qui sont 
des TH&#200;MES. Soit T, un ensemble des syntagmes qui sont des COMMENTAIREs. 
On appelle:  C = R + T. 
</p>
<p>3. Pour chaque syntagme, on fait: 
3a. &#201;tendre le contexte pour syntagme s&#8217;, avec s&#8217; d&#233;riv&#233; de s par l&#8217;extraction d&#8217;un dernier 
</p>
<p>mot dans la structure de s. L&#8217;extension du contexte pour s&#8217; signifie que nous 
cherchons toutes les distributions  de s&#8217; dans tous les contextes possibles. 
</p>
<p>3b. Si le volume de contextes de s&#8217; est sup&#233;rieur &#224; un seuil, qui est de 10 dans 
l&#8217;exp&#233;rience, on estime que s&#8217; est un syntagme. On s&#8217;arr&#234;te cette &#233;tape pour s actuel 
et revient &#224; la troisi&#232;me &#233;tape  pour un autre s. 
</p>
<p>3c. Revenir &#224; 3a, jusqu&#8217;au moment o&#249; s&#8217; ne peut plus &#234;tre compos&#233; au moins de deux 
mots.  
Revenir &#224; la troisi&#232;me &#233;tape pour un autre s. 
</p>
<p>4. Utiliser des syntagmes du S pour d&#233;composer d&#8217;autres phrases dans C. Cc&#8712;&#8704; , on 
d&#233;compose c en syntagme en se basant sur les syntagmes du S: 
- c  est consid&#233;r&#233; comme l&#8217;ensemble des formes syntagmatiques connues ou 
inconnues.  
- Les formes syntagmatiques inconnues dans c sont de nouvelles appliqu&#233;es &#224; 
la troisi&#232;me &#233;tape. 
</p>
<p>SORTIE: L&#8217;ensemble S des formes syntagmatiques connues. 
 
</p>
<p>3.3 Vectorisation des textes 
 
La vectorisation est un processus de l&#8217;affectation des poids aux &#233;l&#233;ments du vecteur qui repr&#233;sentent 
un texte. Chaque &#233;l&#233;ment du vecteur est une propri&#233;t&#233; du texte. Nous utilisons la fr&#233;quence des mots 
pour d&#233;terminer le poids. Nous utilisons la distance Euclidienne pour calculer la similarit&#233; des deux 
vecteurs textuels. 
 
3.4 R&#233;duction de la taille des vecteurs 
 
La m&#233;thode Random Mapping [KASKI 1998] est appliqu&#233;e pour diminuer la taille des vecteurs. La 
taille r&#233;duite est de 100 dimensions. Dans cette m&#233;thode, un vecteur n est multipli&#233; par une matrice </p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>NGUYEN Tuan-Dang 
</p>
<p>R qui dispose des valeurs initiales au hasard. La projection donne le r&#233;sultat sous forme d&#8217;un 
vecteur des dimensions diminu&#233;es : Rnx = . 
 
3.5 Construction de la carte SOM 
 
Nous pr&#233;cisons les param&#232;tres suivants: 
</p>
<p>- La carte comporte des 400 neurones, par la taille de 20 x 20.  
- Le nombre des it&#233;rations dans l&#8217;algorithme : T = 100.000. 
- Le voisinage du neurone gagnant est d&#233;termin&#233; par la fonction g&#233;om&#233;trique: 
</p>
<p>( )tttNh jj &#951;=)),(( . 
- La fonction de la proportion d&#8217;apprentissage : )/1)(()( max Tttt &#8722;=&#951;&#951; , avec max&#951; est 
</p>
<p>initialement de 50% la taille de la carte. 
 
4 Exp&#233;rimentation 
 
4.1 Donn&#233;es et m&#233;thode 
 
Nos donn&#233;es sont des 5,325 textes (full-text) &#224; partir du site du Parti Communiste du Vietnam 
(http://www.cpv.org.vn). 
 
4.2 M&#233;thode d&#8217;&#233;valuation du r&#233;sultat 
 
</p>
<p>1.  Qualit&#233; de l&#8217;algorithme SOM: nous utilisons l&#8217;&#233;valuation directe par &#171; average 
quantization error &#187; et &#171; cost function &#187; sur toute la collection des textes. 
</p>
<p>Average quantization error : &#8721;
=
</p>
<p>=
</p>
<p>&#8722;=
</p>
<p>L
</p>
<p>l
klMkdist
</p>
<p>wxLJ 1
2
</p>
<p>,...,1
min1 , L le nombre des vecteurs 
</p>
<p>d&#8217;entr&#233;e. 
Cost function : &#8721; &#8721;&#8721; &#8722;+&#8722;=
</p>
<p>k i j
jiiijck mnNhnxE
</p>
<p>22  , iN  note des vecteurs les plus proches 
</p>
<p>au vecteur r&#233;f&#233;rentiel im , &#8721; &#8712;= ik Vx kii xNn /1 , kV  voisinage Vonoroi correspondant au vecteur 
r&#233;f&#233;rentiel im . Quand nous d&#233;veloppons cette approximation, nous supposons que jihij ,,1 &#8704;= . 
</p>
<p> 
Qualit&#233; de la visualisation de la carte: La cr&#233;ation de la carte a pour but de trouver des 
</p>
<p>classes existantes dans les donn&#233;es et de les visualiser sur une carte de 2-dimensions. Pour &#233;valuer 
la convergence visuelle de l&#8217;algorithme et l&#8217;avantage de la vectorisation des textes, nous disposons 
dix groupes des textes qui sont d&#233;j&#224; class&#233;s par le r&#233;dacteur et qui composent de la collection. Bien 
que la classification humaine soit subjective, nous v&#233;rifions la convergence de chaque groupe de 
donn&#233;es homog&#232;ne par essai distinct.  Notons V  l&#8217;ensemble des unit&#233;s (neurones) qui enregistrent 
au moins un vecteur convergeant, nous d&#233;finissons les indices suivants: 
</p>
<p>Indice Ii: l&#8217;&#233;valuation de la coh&#233;rence moyenne d&#8217;une unit&#233; sur la carte. V
h
</p>
<p>I Vj
j
</p>
<p>c
&#8721;
&#8712;
</p>
<p>= , hj est le 
</p>
<p>nombre des unit&#233;s jiVi &#8800;&#8712; ,  voisines de l&#8217;unit&#233; Vj&#8712; ,  hj est &#224; 8 au maximum et hj est &#224; 0 au </p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Nouvelle m&#233;thode syntagmatique de vectorisation appliqu&#233;e au self-organizing map des textes vietnamiens 
</p>
<p> 
</p>
<p>minimum quand j est une unit&#233; isol&#233;e. La coh&#233;rence moyenne n&#8217;est pas meilleure quand il y a 
beaucoup des unit&#233;s &#233;parses ou isol&#233;es. 
</p>
<p>Indice Id: l&#8217;&#233;valuation de la densit&#233; moyenne d&#8217;une unit&#233; sur la carte. V
</p>
<p>d
I Vj
</p>
<p>j
</p>
<p>d
</p>
<p>&#8721;
&#8712;
</p>
<p>= , dj est le 
</p>
<p>nombre des vecteurs convergeant &#224; l&#8217;unit&#233;. 
Indice Icd = Ic x Id : &#233;valuation de la coh&#233;rence et de la densit&#233; moyennes des r&#233;gions sur de 
</p>
<p>la carte. 
 
4.3 Comparaison de cinq m&#233;thodes de vectorisation 
 
</p>
<p> M1 M2 M3 M4 M5 
Average 
</p>
<p>quantization error 
0.02546 0.0247 0.00029072 0.00024752 0.00017996 
</p>
<p>Cost function 534.65031 422.07605 39.3245502 17.17635 13.06308 
Tableau 1 : R&#233;sultats de cinq m&#233;thodes de vectorisation sur toute la collection 
</p>
<p> 
4.4 Comparaison de la qualit&#233; de visualisation 
 
4.4.1 Dix groupes de donn&#233;es 
 
No Nombre des textes Sujets 
1 46 La direction du Parti Communiste du Vietnam 
2 152 Collection de Ho Chi Minh, volume 1 
3 120 M&#233;moires de la prison 
4 71 Lenin 
5 63 Ho Chi Minh et les peuples vietnamiens 
6 85 Archives du Parti Communiste du Vietnam, volume 8 
7 41 Collection des informations et  des textes du Parti Communiste du 
</p>
<p>Vietnam 
8 14 Culture r&#233;volutionnaire 
9 45 Archives des t&#226;ches id&#233;ologiques 
10 399 Etudes du congr&#232;s IX 
</p>
<p>Tableau 2 : Dix groupes de donn&#233;es 
 
4.4.2 Evaluation par l&#8217;indice Icd = Ic x Id  
 
</p>
<p>Groupe de donn&#233;es M1 M2 M3 M4 M5 
1 3.38725 3.37191 4.44727 2.41928 4.45161 
2 5.34717 7.14337 8.31886 7.41942 9.88057 
3 3.91404 4.5660 8.70748 27.5 7.65306 
4 3.69093 3.26059 4.9819 6.02170 5.57438 
5 3.76460 4.10684 9.6768 2.70703 4.49732 
6 3.21898 4.88763 6.20885 3.89583 6.44315 </p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>NGUYEN Tuan-Dang 
</p>
<p>7 1.89815 2.14204 3.96 2.87284 3.96 
8 0.42857 1.35714 2.56805 0.85714 1.84722 
9 2.3625 2.53472 3.6914 1.92308 4.104 
10 10.91355 15.32225 20.52528 19.52008 22.12989 
</p>
<p>Tableau 3: Evaluation par l&#8217;indice Icd = Ic x Id  
 
4.5 Remarques 
 
L&#8217;efficacit&#233; est class&#233;e: M5, M3, M4, M2, M1. M3 se place devant  M4 car M3 se montre mieux 
que M4 dans l&#8217;&#233;valuation indirecte (par l&#8217;indice Icd), bien que M4 soit un peu plus mieux que M3 
dans l&#8217;&#233;valuation directe (par &#171; average quantization error &#187; et &#171; cost function &#187;). 
 
5 Conclusion 
 
En travaillant sur les textes vietnamiens, nous avons &#233;valu&#233; et propos&#233; de nouvelles 
m&#233;thodes de la s&#233;lection des propri&#233;t&#233;s du texte, de la vectorisation du texte et de la mesure 
qualitative de la carte. Nous constatons que la m&#233;thode universelle de la s&#233;lection des 
propri&#233;t&#233;s, en se basant sur un seuil de la fr&#233;quence absolue des mots, n&#8217;est pas efficace. La 
notion du syntagme est pratique dans le but de la pr&#233;sentation du contenu des documents 
&#233;crits en vietnamien. La contribution de notre &#233;tude sera utile de  cr&#233;er des cartes utilis&#233;es 
par les moteurs de recherche traitant sur les textes vietnamiens dans la collection. 
 
R&#233;f&#233;rences 
 
CAO Xuan Hao (1991), Le vietnamien: essai de la grammaire fonctionnelle, volume 1. Editeur des 
Sciences Sociales. 254 pages. 
 
CAO Xuan Hao (1998), Le vietnamien: quelques probl&#232;mes phon&#233;tiques, grammaticales et 
s&#233;mantiques.  Editeur de l&#8217;Education. 752 pages. 
 
NGUYEN Tuan Dang (2002), Texte mining des documents vietnamiens avec SOM, m&#233;moire du 
Master en informatique, Universit&#233; des Sciences Naturelles, H  Ch&#237; Minh ville, Vi t nam. 
 
HONKELA T., KASKI S., LAGUS K., KOHONEN T. (1997), WEBSOM self-organizing maps of 
document collections. Proceedings of WSOM'97, Workshop on Self-Organizing Maps, Espoo, 
Finland, June 4-6. 
 
IIVARINEN J., KOHONEN T., KANGAS J., KASKI S. (1994), &#8220;Visualizing the clusters on the 
self-organizing map,&#8221; in Proc. Conf. Artificial Intell. Res.Finland, C. Carlsson, T. J&#228;rvi, and T. 
Reponen, Eds. Helsinki, Finland. 
 
KASKI, S. (1998), Dimensionality reduction by random mapping: Fast similarity computation for 
clustering. Proceedings of IJCNN'98, International Joint Conference on Neural Networks. </p>

</div></div>
</body></html>