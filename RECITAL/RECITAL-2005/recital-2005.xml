<?xml version="1.0" encoding="UTF-8"?>
 <!-- Fichiers problématiques : recital-2005-court-004 -->
<conference>
	<edition>
		<acronyme>RECITAL'2005</acronyme>
		<titre>7e Rencontres des Étudiants Chercheurs en Informatique pour le Traitement Automatique des Langues</titre>
		<ville>Dourdan</ville>
		<pays>France</pays>
		<dateDebut>2005-06-06</dateDebut>
		<dateFin>2005-06-10</dateFin>
		<presidents>
			<president>
				<prenom>Nicolas</prenom>
				<nom>Hernandez</nom>
			</president>
			<president>
				<prenom>Guillaume</prenom>
				<nom>Pitel</nom>
			</president>
		</presidents>
		<typeArticles>
			<type id="long">Papiers longs</type>
			<type id="court">Papiers courts</type>
		</typeArticles>
		<siteWeb>http://www.limsi.fr/TALN05</siteWeb>
	</edition>
	<articles>
		<article id="recital-2005-long-001" session="RECITAL">
			<auteurs>
				<auteur>
					<prenom>Tonio</prenom>
					<nom>Wandmacher</nom>
					<email>tonio.wandmacher@etu.univ-tours.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Laboratoire d'Informatique, Université Francois-Rabelais de Tours</affiliation>
			</affiliations>
			<titre></titre>
			<type>long</type>
			<pages>525-534</pages>
			<resume>Au cours des dix dernières années, l'analyse de la sémantique latente (LSA) a été utilisée dans de nombreuses approches TAL avec parfois de remarquables succès. Cependant, ses capacités à exprimer des ressemblances sémantiques n’ont pas été réellement recherchées de façon systématique. C’est l’objectif de ce travail, où la LSA est appliquée à un corpus de textes de langue courante (journal allemand). Les relations lexicales entre un mot et ses termes les plus proches sont analysés pour un test de vocabulaire. Ces résultats sont alors comparés avec les résultats obtenus lors d’une analyse des collocations.</resume>
			<mots_cles>Analyse de la sémantique latente, analyse de collocations, relations lexicales, sémantique computationelle</mots_cles>
			<title>How semantic is Latent Semantic Analysis?</title>
			<abstract>In the past decade, Latent Semantic Analysis (LSA) was used in many NLP approaches with sometimes remarkable success. However, its abilities to express semantic relatedness were not yet systematically investigated. This is the aim of our work, where LSA is applied to a general text corpus (German newspaper), and for a test vocabulary, the lexical relations between a test word and its closest neighbours are analysed. These results are compared to the results from a collocation analysis.</abstract>
			<keywords>Latent Semantic Analysis, Collocation Analysis, lexical relations, computational semantics</keywords>
		</article>
		<article id="recital-2005-long-002" session="RECITAL">
			<auteurs>
				<auteur>
					<prenom>Vincent</prenom>
					<nom>Barbier</nom>
					<email>barbier@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIMSI-CNRS</affiliation>
			</affiliations>
			<titre>Quels types de connaissance sémantique pour Questions-Réponses ?</titre>
			<type>long</type>
			<pages>535-544</pages>
			<resume>Les systèmes de Questions Réponse ont besoin de connaissances sémantiques pour trouver dans les documents des termes susceptibles d’être des reformulations des termes de la question. Cependant, l’utilisation de ressources sémantiques peut apporter un bruit important et altérer la précision du système. ne fournit qu’une partie des reformulations possibles. Cet article présente un cadre d’évaluation pour les ressources sémantiques dans les systèmes de question-réponse. Il décrit la fabrication semi-automatique d’un corpus de questions et de réponses destiné à étudier les reformulations présentes entre termes de la question et termes de la réponse. Il étudie la fréquence et la fiabilité des reformulations extraites de l’ontologie WordNet.</resume>
			<mots_cles>Système de Question Réponse, ressources sémantiques, évaluation des reformulations</mots_cles>
			<title></title>
			<abstract>Question Answering systems need semantic knowledge to find in the documents terms that are reformulations of the question’s terms. However, the use of semantic resources brings an important noise and a system’s precision might get depreciated. This article presents a framework for evaluating semantic resources in question-answering systems. It describes the semi-automated construction of a corpus containing questions and answers. This corpus can be used to study the various reformulations between the terms of the questions and the terms of the answers. This article studies the frequency and reliability of reformulations given by the WordNet ontology.</abstract>
			<keywords>Question Answering system, semantic resources, evaluation of reformulations</keywords>
		</article>
		<article id="recital-2005-long-003" session="RECITAL">
			<auteurs>
				<auteur>
					<prenom>Thibault</prenom>
					<nom>Roy</nom>
					<email>troy@info.unicaen.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université de Caen / Basse-Normandie - Laboratoire GREYC</affiliation>
			</affiliations>
			<titre>Une plate-forme logicielle dédiée à la cartographie thématique de corpus</titre>
			<type>long</type>
			<pages>545-554</pages>
			<resume>Cet article présente les principes de fonctionnement et les intérêts d’une plate-forme logicielle centrée sur un utilisateur ou un groupe d’utilisateurs et dédiée à la visualisation de propriétés thématiques d’ensembles de documents électroniques. Cette plate-forme, appelée ProxiDocs, permet de dresser des représentations graphiques (des cartes) d’un ensemble de textes à partir de thèmes choisis et définis par un utilisateur ou un groupe d’utilisateurs. Ces cartes sont interactives et permettent de visualiser les proximités et les différences thématiques entre textes composant le corpus étudié. Selon le type d’analyse souhaitée par l’utilisateur, ces cartes peuvent également s’animer afin de représenter les changements thématiques d’un ensemble de textes au fil du temps.</resume>
			<mots_cles>cartographie de corpus, analyse thématique, logiciel individu-centré, analyse des données textuelles</mots_cles>
			<title></title>
			<abstract>This article presents a user-centered software dedicated to the visualization of thematic properties of sets of electronic documents. This software, called ProxiDocs, allows its users to realize thematic maps from a corpora and themes they choose and defined. These maps are interactive and reveal thematic proximities and differences between texts composing the studied corpus. According to the analysis wished by the user, maps can be animated in order to represent thematic changes of the analysed set of texts relating to the time.</abstract>
			<keywords>corpora cartography, thematic analysis, user-centered software, textual data analysis</keywords>
		</article>
		<article id="recital-2005-long-004" session="RECITAL">
			<auteurs>
				<auteur>
					<prenom>Delphine</prenom>
					<nom>Bernhard</nom>
					<email>Delphine.Bernhard@imag.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Laboratoire TIMC-IMAG Institut de l'Ingénierie et de l'Information de Santé Pavillon Le Taillefer – Faculté de Médecine F-38706 LA TRONCHE cedex</affiliation>
				<affiliation affiliationId="2"></affiliation>
				<affiliation affiliationId="3"></affiliation>
			</affiliations>
			<titre>Segmentation morphologique à partir de corpus</titre>
			<type>long</type>
			<pages>555-564</pages>
			<resume>Nous décrivons une méthode de segmentation morphologique automatique. L'algorithme utilise uniquement une liste des mots d'un corpus et tire parti des probabilités conditionnelles observées entre les sous-chaînes extraites de ce lexique. La méthode est également fondée sur l'utilisation de graphes d'alignement de segments de mots. Le résultat est un découpage de chaque mot sous la forme (préfixe*) + base + (suffixe*). Nous évaluons la pertinence des familles morphologiques découvertes par l'algorithme sur un corpus de textes médicaux français contenant des mots à la structure morphologique complexe.</resume>
			<mots_cles>Segmentation morphologique, alignement de segments de mots, corpus</mots_cles>
			<title></title>
			<abstract>We describe a method that automatically segments words into morphs. The algorithm only uses a list of words collected in a corpus. It is based on the conditional probabilities between the substrings extracted from this lexicon. The method also makes use of word segments alignment graphs. As a result, all words are segmented into a sequence of morphs which has the following pattern: (prefix*) + base + (suffix*). We evaluate the morphological families discovered by the algorithm using a corpus of French medical texts containing words whose morphological structure is complex.</abstract>
			<keywords>Morphological segmentation, word segments alignment, corpus</keywords>
		</article>
		<article id="recital-2005-long-005" session="RECITAL">
			<auteurs>
				<auteur>
					<prenom>Bruno</prenom>
					<nom>Cartoni</nom>
					<email>bruno.cartoni@eti.unige.ch</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université de Genève</affiliation>
			</affiliations>
			<titre>Traduction des règles de construction des mots pour résoudre l'incomplétude lexicale en traduction automatique - Etude de cas</titre>
			<type>long</type>
			<pages>565-574</pages>
			<resume>Cet article propose d’exploiter les similitudes constructionnelles de deux langues morphologiquement proches (le français et l’italien), pour créer des règles de construction des mots capables de déconstruire un néologisme construit de la langue source et générer de manière similaire un néologisme construit dans la langue cible. Nous commençons par présenter diverses motivations à cette méthode, puis détaillons une expérience pour laquelle plusieurs règles de transfert ont été créées et appliquées à un ensemble de néologismes construits.</resume>
			<mots_cles>Traduction automatique, morphologie constructionnelle, incomplétude lexicale</mots_cles>
			<title></title>
			<abstract>This paper presents a method which aims at exploiting constructional similarities between two morphologically-related languages (French an Italian), in order to create wordconstruction rules that can disassemble a constructed neologism and create in a similar way a constructed neologism into the target language. We present the main motivation for this method and describe an experiment for which transfer rules has been developed and applied to a group of constructed neologism.</abstract>
			<keywords>Machine Translation, constructional morphology, lexical incompleteness</keywords>
		</article>
		<article id="recital-2005-long-006" session="RECITAL">
			<auteurs>
				<auteur>
					<prenom>Ann</prenom>
					<nom>Bertels</nom>
					<email>ann.bertels@ilt.kuleuven.ac.be</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">ILT - K.U.Leuven</affiliation>
			</affiliations>
			<titre>A la découverte de la polysémie des spécificités du français technique</titre>
			<type>long</type>
			<pages>575-584</pages>
			<resume>Cet article décrit l’analyse sémantique des spécificités dans le domaine technique des machines-outils pour l’usinage des métaux. Le but de cette étude est de vérifier si et dans quelle mesure les spécificités dans ce domaine sont monosémiques ou polysémiques. Les spécificités (situées dans un continuum de spécificité) seront identifiées avec la KeyWords Method en comparant le corpus d’analyse à un corpus de référence. Elles feront ensuite l’objet d’une analyse sémantique automatisée à partir du recouvrement des cooccurrences des cooccurrences, afin d’établir le continuum de monosémie. Les travaux de recherche étant en cours, nous présenterons des résultats préliminaires de cette double analyse.</resume>
			<mots_cles>sémantique lexicale, langue spécialisée, spécificités, polysémie, cooccurrences</mots_cles>
			<title></title>
			<abstract>This article discusses a semantic analysis of pivotal terms (keywords) in the domain of machining terminology in French. Building on corpus data, the investigation attempts to find out whether, and to what extent, the keywords are polysemous. In order to identify the most typical words of the typicality continuum, the KeyWords Method will be used to compare the technical corpus with a reference corpus. The monosemy continuum will be implemented in terms of degree of overlap between the co-occurrents of the co-occurrents of the keywords. We present some preliminary results of work in progress.</abstract>
			<keywords>lexical semantics, language for specific purposes (LSP), keywords, polysemy, co-occurrences</keywords>
		</article>
		<article id="recital-2005-long-007" session="RECITAL">
			<auteurs>
				<auteur>
					<prenom>Yayoi</prenom>
					<nom>Nakamura-Delloye</nom>
					<email>yayoi@free.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université Paris VII, Laboratoire Lattice</affiliation>
			</affiliations>
			<titre>Système AlALeR - Alignement au niveau phrastique des textes parallèles français-japonais</titre>
			<type>long</type>
			<pages>585-594</pages>
			<resume>Le présent article décrit le Système AlALeR (Système d’Alignement Autonome, Léger et Robuste). Capable d’aligner au niveau phrastique un texte en français et un texte en japonais, le Système AlALeR ne recourt cependant à aucun moyen extérieur tel qu’un analyseur morphologique ou des dictionnaires, au contraire des méthodes existantes. Il est caractérisé par son analyse morphologique partielle mettant à profit des particularités du système d’écriture japonais et par la transcription des mots emprunts, à l’aide d’un transducteur.</resume>
			<mots_cles>Alignement, corpus parallèles, analyse morphologique japonaise partielle, mémoire de traduction</mots_cles>
			<title></title>
			<abstract>The present paper describes the AlALeR System, an Autonomous, Robust and Light Alignment System. Capable of aligning at the sentence level a French text and a Japanese one, the AlALeR System doesn’t use any external tool, such as morphological parsers or dictionaries, contrary to existing methods. This system is characterized by a partial morphological analysis taking advantage of some peculiarities of japanese writing system, and by the transcription of loan words with a transducer.</abstract>
			<keywords>Alignment, parallel corpora, partial japanese morphological analysis, translation memory</keywords>
		</article>
		<article id="recital-2005-long-008" session="RECITAL">
			<auteurs>
				<auteur>
					<prenom>Stéphanie</prenom>
					<nom>Léon</nom>
					<email>fanny.leon@orange.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Chrystel</prenom>
					<nom>Millon</nom>
					<email>Chrystel.Millon@up.univ-mrs.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université de Provence, Equipe DELIC</affiliation>
			</affiliations>
			<titre>Acquisition semi-automatique de relations lexicales bilingues (français-anglais) à partir du Web</titre>
			<type>long</type>
			<pages>595-604</pages>
			<resume>Cet article présente une méthode d’acquisition semi-automatique de relations lexicales bilingues (français-anglais) faisant appel à un processus de validation sur le Web. Notre approche consiste d’abord à extraire automatiquement des relations lexicales françaises. Nous générons ensuite leurs traductions potentielles grâce à un dictionnaire électronique. Ces traductions sont enfin automatiquement filtrées à partir de requêtes lancées sur le moteur de recherche Google. Notre évaluation sur 10 mots français très polysémiques montre que le Web permet de constituer ou compléter des bases de données lexicales multilingues, encore trop rares, mais dont l’utilité est pourtant primordiale pour de nombreuses applications, dont la traduction automatique.</resume>
			<mots_cles>Traduction, corpus, relations lexicales bilingues, acquisition semi-automatique, World Wide Web</mots_cles>
			<title></title>
			<abstract>This paper presents a method of semi-automatic acquisition of bilingual (French-English) lexical relations using a validation process via the Web. Our approach consists firstly of automatically extracting French lexical relations. We then generate their potential translations by means of an electronic dictionary. These translations are finally automatically filtered using queries on the Google search engine. Our evaluation on 10 very polysemous French words shows that the Web is a useful resource for building or improving multilingual lexical databases, which are urgently needed in a wide range of applications, such as machine translation.</abstract>
			<keywords>Translation, corpus, bilingual lexical relations, semi-automatic acquisition, World Wide Web</keywords>
		</article>
		<article id="recital-2005-long-009" session="RECITAL">
			<auteurs>
				<auteur>
					<prenom>Marianne</prenom>
					<nom>Santaholma</nom>
					<email>Marianne.Santaholma@eti.unige.ch</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université de Genève</affiliation>
			</affiliations>
			<titre></titre>
			<type>long</type>
			<pages>605-614</pages>
			<resume>Dans cet article nous décrivons le développement des ressources linguistiques du finnois pour un système de traduction automatique de la parole dans le domaine médical: MedSLT. Le travail inclut la construction des corpus médicaux en finnois, le développement de la grammaire finlandaise pour la génération, le développement du lexique finlandais et la définition des règles de mapping interlingue-finnois pour la traduction multilingue. Nous avons découvert que le finnois peut être introduit dans l'architecture existante de MedSLT sans trop de difficultés. En effet, malgré les différences entre l'anglais et le finnois, la grammaire finlandaise a pu être créée en adaptant manuellement la grammaire anglaise originale. Les premiers résultats de l'évaluation de la traduction anglais-finnois sont encourageants.</resume>
			<mots_cles>Grammaire d’unification, traduction automatique multilingue de la parole, interlingue, souslangage, finnois</mots_cles>
			<title>Linguistic representation of Finnish in the medical domain spoken language translation system</title>
			<abstract>This paper describes the development of Finnish linguistic resources for use in MedSLT, an Open Source medical domain speech-to-speech translation system. The paper describes the collection of medical Finnish corpora, the creation of a Finnish grammar by adapting the original English grammar, the composition of a domain specific Finnish lexicon and the definition of interlingua to Finnish mapping rules for multilingual translation. It is shown that Finnish can be effectively introduced into the existing MedSLT framework and that despite the differences between English and Finnish, the Finnish grammar can be created by manual adaptation from the original English grammar. Regarding further development, the initial evaluation results of English-Finnish speech-to-speech translation are encouraging.</abstract>
			<keywords>Domain specific unification grammar, multilingual spoken language translation, interlingua, sub-language, Finnish</keywords>
		</article>
		<article id="recital-2005-long-010" session="RECITAL">
			<auteurs>
				<auteur>
					<prenom>Achille</prenom>
					<nom>Falaise</nom>
					<email>achille.falaise@imag.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Laboratoire CLIPS-IMAG</affiliation>
			</affiliations>
			<titre>Constitution d'un corpus de français tchaté</titre>
			<type>long</type>
			<pages>615-624</pages>
			<resume>Nous présentons dans cet article un corpus de français tchaté, destiné à l'étude de la langue du tchat. Ce corpus, collecté et encodé automatiquement, est remarquable avant tout par son étendue, puisqu'il couvre un total de 4 millions de messages sur 105 canaux, hétérogènes sur les plans thématique et pragmatique. Son codage simple ne sera toutefois pas satisfaisant pour tous les usages. Il est disponible sur un site Internet, et consultable grâce à une interface web.</resume>
			<mots_cles>langue tchatée, ressources linguistiques, collecte de données</mots_cles>
			<title></title>
			<abstract>We present in this article a french chat corpus, intended for the study of chat language. This corpus, automatically collected and coded, is especially remarkable for its extent, since it covers a total of 4 million messages on 105 channels, heterogeneous from a thematic and pragmatic point of view. Its simple coding will not, however, be sufficient for all purposes. It is available on an Internet site, and viewable using a web interface.</abstract>
			<keywords>chat language, linguistic resources, resource acquisition</keywords>
		</article>
		<article id="recital-2005-long-011" session="RECITAL">
			<auteurs>
				<auteur>
					<prenom>Rémi</prenom>
					<nom>Bove</nom>
					<email>remi.bove@voila.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université de Provence, Aix-Marseille I</affiliation>
			</affiliations>
			<titre>Étude de quelques problèmes de phonétisation dans un système de synthèse de la parole à partir de SMS</titre>
			<type>long</type>
			<pages>625-634</pages>
			<resume>Cet article présente une étude dont l’objectif était d’améliorer la phonétisation d’un système de synthèse vocale de SMS en ce qui concerne trois types de problèmes : l’écriture rébus (chiffres et lettres utilisés pour leur valeur phonique), les abréviations sous forme de squelettes consonantiques et les agglutinations (déterminants ou pronoms collés graphiquement au mot qui suit). Notre approche se base sur l’analyse d’un corpus de SMS, à partir duquel nous avons extrait des listes de formes permettant de compléter les lexiques du système, et mis au point de nouvelles règles pour les grammaires internes. Les modifications effectuées apportent une amélioration substantielle du système, bien qu’il reste, évidemment, de nombreuses autres classes de problèmes à traiter.</resume>
			<mots_cles>SMS, phonétisation, synthèse de la parole</mots_cles>
			<title></title>
			<abstract>This article presents a study whose goal is to improve the grapheme-to-phoneme component of an SMS-to-speech system. The three types of problems tackled in the study are: rebus writing (digits and letters used for their phonetic value), consonant skeleton abbreviations and agglutinations (determiner or pronouns merged with the next word). Our approach is based on the analysis of an SMS corpus, from which we extracted lists of forms to enhance the system’s lexicons, and developed new grammatical rules for the internal grammars. Our modifications result in a substantial improvement of the system, although, of course, there remain many other categories of problems to address.</abstract>
			<keywords>SMS, phonetisation, speech synthesis</keywords>
		</article>
		<article id="recital-2005-court-001" session="Posters RECITAL">
			<auteurs>
				<auteur>
					<prenom>Maxime</prenom>
					<nom>Amblard</nom>
					<email>amblard@labri.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LaBRI, Université de Bordeaux 1</affiliation>
			</affiliations>
			<titre>Synchronisation syntaxe sémantique, des grammaires minimalistes catégorielles (GMC) aux Constraint Languages for Lambda Structures (CLLS)</titre>
			<type>court</type>
			<pages>637-642</pages>
			<resume>Ces travaux se basent sur l’approche computationelle et logique de Ed Stabler (?), qui donne une formalisation sous forme de grammaire du programme minimaliste de Noam Chomsky (?). La question que je veux aborder est comment, à partir d’une analyse syntaxique retrouver la forme prédicative de l’énoncé. Pour cela, il faut mettre en place une interface entre syntaxe et sémantique. C’est ce que je propose en utilisant les Grammaires Minimalistes Catégorielles (GMC) extension des GM vers le calcul de Lambeck. Ce nouveau formalisme permet une synchronisation simple avec le lambda-calcul. Parmi les questions fréquemment rencontrées dans le traitement des langues naturelles, j’interroge la performance de cette interface pour la résolution des problèmes de portée des quantificateurs. Je montre pourquoi et comment il faut utiliser un lambda-calcul plus élaboré pour obtenir les différentes lectures, en utilisant Constraint Languages for Lambda Structures -CLLS.</resume> <mots_cles>logique, grammaires minimalistes catégorielles, lambda-calcul, portée des quantificateurs, Constraint Language for Lambda Structures</mots_cles>
			<title></title>
			<abstract>This work is based on the computational and logical approach of Ed Stabler (?), which gives a formalization of the minimalist program of Noam Chomsky (?). The question I want to solve is, starting from a syntactic analysis, how to find the predicative forms of a sentence. I propose an interface between syntax and semantic by using Categorical Minimalists Grammars (CMG) extension of the MG towards the Lambeck calculus and Constraint Language for Lambda Structures (CLLS). This interface is powerful for the resolution of quantifier scope ambiguities.</abstract>
			<keywords>logic, minimalist grammars, lambda-calculus, quantifiers scope, Constraint Language for Lambda Structures</keywords>
		</article>
		<article id="recital-2005-court-002" session="Posters RECITAL">
			<auteurs>
				<auteur>
					<prenom>Siham</prenom>
					<nom>Boulaknadel</nom>
					<email>siham.boulaknadel@univ-nantes.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Fadoua</prenom>
					<nom>Ataa-Allah</nom>
					<email>fadoua_01@yahoo.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université de Nantes, LINA FRE CNRS 2729</affiliation>
				<affiliation affiliationId="2">Université Mohamed V, Faculté des Sciences de Rabat, GSCM</affiliation>
			</affiliations>
			<titre>Recherche d'information en langue arabe : influence des paramètres linguistiques et de pondération en LSA</titre>
			<type>court</type>
			<pages>643-648</pages>
			<resume>Nous nous intéressons à la recherche d’information en langue arabe en utilisant le modèle de l’analyse sémantique latente (LSA). Nous proposons dans cet article de montrer que le traitement linguistique et la pondération des unités lexicales influent sur la performance de la LSA pour quatre cas d’études : le premier avec un simple prétraitement des corpus; le deuxième en utilisant un anti-dictionnaire; le troisième avec un racineur de l’arabe ; le quatrième où nous avons combiné l’anti-dictionnaire et le racineur. Globalement les résultats de nos expérimentations montrent que les traitements linguistiques ainsi que la pondération des unités lexicales utilisés améliorent la performance de LSA.</resume>
			<mots_cles>Recherche d’information, Analyse de la sémantique latente, Langue arabe, Racinisation</mots_cles>
			<title></title>
			<abstract>We are interested in information retrieval in Arabic language by using latent semantic analysis method (LSA). We propose in this article to show that the linguistic treatment and weighting of lexemes influence the performance of LSA. Four cases are studied: the first with a simple pretreatment of the corpora; the second by using a stopword list; the third with arabic stemmer; the fourth where we combined stopword list and arabic stemmer. Broadly the results of our experiments show that the linguistic treatments as well as weighting of lexemes used improve the performance of LSA.</abstract>
			<keywords>Information retrieval, Latent semantic analyses, Arabic language, Stemming</keywords>
		</article>
		<article id="recital-2005-court-003" session="Posters RECITAL">
			<auteurs>
				<auteur>
					<prenom>Abdelhamid</prenom>
					<nom>El Jihad</nom>
					<email>eljihad@ifrance.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Abdellah</prenom>
					<nom>Yousfi</nom>
					<email>yousfi240ma@yahoo.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université Mohamed V Souissi, Institut d'études et de recherches pour l'arabisation</affiliation>
			</affiliations>
			<titre>Etiquetage morpho-syntaxique des textes arabes par modèle de Markov caché</titre>
			<type>court</type>
			<pages>649-654</pages>
			<resume>L’étiquetage des textes est un outil très important pour le traitement automatique de langage, il est utilisé dans plusieurs applications par exemple l’analyse morphologique et syntaxique des textes, l’indexation, la recherche documentaire, la voyellation pour la langue arabe, les modèles de langage probabilistes (modèles n-classes), etc. Dans cet article nous avons élaboré un système d’étiquetage morpho-syntaxique de la langue arabe en utilisant les modèles de Markov cachés, et ceci pour construire un corpus de référence étiqueté et représentant les principales difficultés grammaticales rencontrées en langue arabe générale. Pour l’estimation des paramètres de ce modèle, nous avons utilisé un corpus d’apprentissage étiqueté manuellement en utilisant un jeu de 52 étiquettes de nature morpho-syntaxique. Ensuite on procède à une amélioration du système grâce à la procédure de réestimation des paramètres de ce modèle.</resume>
			<mots_cles>Corpus, jeu d’étiquettes, Etiquetage morpho-syntaxique, texte arabe, modèle de Markov caché</mots_cles>
			<title></title>
			<abstract>The tagging of texts is a very important tool for various applications of natural language processing : morphological and syntactic analysis of texts, indexation and information retrieval, vowelling of arabic texts, probabilistic language model (n-class model). In this paper we have used the Hidden Markov Model (HMM) to tag the arabic texts. This system of tagging is used to build a large labelled arabic corpus. The experiments are carried in the set of the labelled texts and the 52 tags of morpho-syntactic nature, in order to estimate the parameters of the HMM.</abstract>
			<keywords>Corpus, the set of tags, the morpho-syntactic tagging, arabic text, Hidden Markov Model</keywords>
		</article>
		<article id="recital-2005-court-004" session="Posters RECITAL">
			<auteurs>
				<auteur>
					<prenom>Manal</prenom>
					<nom>El Zant</nom>
					<email>el.Zant@medecine.univ-mrs.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Liliane</prenom>
					<nom>Pellegrin</nom>
					<email>liliane.pellegrin@medecine.univ-mrs.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Hervé</prenom>
					<nom>Chaudet</nom>
					<email>lhcp@acm.org</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Michel</prenom>
					<nom>Roux</nom>
					<email>michel.roux@medecine.univ-mrs.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université de la Méditerranée II, Laboratoire d'informatique fondamentale</affiliation>
				<affiliation affiliationId="2">IMTSSA</affiliation>
			</affiliations>
			<titre>Identification des composants temporels pour la représentation des dépêches épidémiologiques</titre>
			<type>court</type>
			<pages>655-660</pages>
			<resume>Dans le cadre du projet EpidémIA qui vise à la construction d’un système d’aide à la décision pour assister l’utilisateur dans son activité de gestion des risques sanitaires, un travail préalable sur la compositionalité des évènements (STEEL) nous a permis d’orienter notre travail dans le domaine de la localisation d’information spatio-temporelle. Nous avons construit des graphes de transducteurs pour identifier les informations temporelles sur un corpus de 100 dépêches de la langue anglaise de ProMed. Nous avons utilisé le système d’extraction d’information INTEX pour la construction de ces transducteurs. Les résultats obtenus présentent une efficacité de ces graphes pour l’identification des données temporelles.</resume>
			<mots_cles>Analyse de textes, structure des évènements, extraction d’information, sémantique du temps</mots_cles>
			<title></title>
			<abstract>EpidémIA project aims to the construction of a computerized decision-making system to assist user in his activity of medical risk management. A preliminary work on the events compositionality (STEEL) enables us to direct our work in the field of the space-time information localization. We have created some transducers graphs to identify temporal information on a corpus of 100 SARS ProMed English reports. We used the extraction information system INTEX to construct these transducers. The results obtained present an effectiveness of these graphs to identify temporal data.</abstract>
			<keywords>Text analysis, event structure, extraction information, temporal semantics</keywords>
		</article>
		<article id="recital-2005-court-005" session="Posters RECITAL">
			<auteurs>
				<auteur>
					<prenom>Thomas</prenom>
					<nom>Heitz</nom>
					<email>heitz@lri.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LRI, Paris XI</affiliation>
			</affiliations>
			<titre>Utilisation de la Linguistique Systémique Fonctionnelle pour la détection des noms de personnes ambigus</titre>
			<type>court</type>
			<pages>661-666</pages>
			<resume>Dans cet article, nous nous proposons de construire un lexique étiqueté selon les principes de la Linguistique Systémique Fonctionnelle (LSF) et de l’appliquer à la détection des noms de personnes ambigus dans les textes. Nous ne faisons pas d’analyse complète mais testons plutôt si certaines caractéristiques de la LSF peuvent permettre de compléter les modèles linguistiques actuellement utilisés pour la détection des entités nommées. Nous souhaitons ainsi apporter une contribution à l’application du formalisme LSF dans l’analyse automatique de textes après son application déjà éprouvée à la génération de textes.</resume>
			<mots_cles>Linguistique Systémique Fonctionnelle, Détection des entités nommées, Fouille de textes</mots_cles>
			<title></title>
			<abstract>In this paper, we propose to build a tagged lexicon according to the Systemic Functional Linguistics (SFL) principles and to apply it to the recognition of ambiguous person names in texts. We do not achieve a complete analysis but rather test if some characteristics of the SFL could enable the completion of actual linguistic models used in named entity recognition. We want thus to bring a contribution to the application of the SFL model in automatic text analysis while it already proved its usefulness for text generation.</abstract>
			<keywords>Systemic Functional Linguistics, Named entity recognition, Text mining</keywords>
		</article>
		<article id="recital-2005-court-006" session="Posters RECITAL">
			<auteurs>
				<auteur>
					<prenom>Mohamed</prenom>
					<nom>Khairallah Khouja</nom>
					<email>khairallah_k@yahoo.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Mounir</prenom>
					<nom>Zrigui</nom>
					<email>mounir.zrigui@fsm.rnu.tn</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">laboratoire RIADI, Unité de Monastir</affiliation>
			</affiliations>
			<titre>Durée des consonnes géminées en parole arabe : mesures et comparaison</titre>
			<type>court</type>
			<pages>667-672</pages>
			<resume>Dans ce papier, nous présentons les résultats d’une étude expérimentale de la durée des consonnes géminées de l’arabe. Nous visons à déterminer la durée, pour une séquence VCCV, de la consonne géminée CC ainsi que de la voyelle qui la précède. Nous comparons ces valeurs à celles mesurées pour une séquence VCV. Les résultats ont prouvé que la durée de la consonne simple était sensiblement différente de celle géminée, ainsi que la durée de la voyelle précédant la consonne. A la base, ce travail est entrepris dans un but d’étudier l’utilisation des durées de phonèmes comme une source d’information pour optimiser un système de reconnaissance, donc introduire des modèles explicites de durée des phonèmes, et mettre en application ces modèles comme partie du modèle acoustique du système de reconnaissance.</resume>
			<mots_cles>Analyse acoustique, arabe standard, gémination, durée, reconnaissance de la parole</mots_cles>
			<title></title>
			<abstract>In this paper, we represent the results of an experimental study concerning the duration of geminated consonants in the Arabic language. We are seeking to determine the duration of a sequence VCCV, of the geminated consonant CC and of the vowel that precedes it. We are comparing these items with those measured for a sequence VCV. The results have shown that the duration of the simple consonant was so apparently different from that geminated and the same was true for the duration of the vowel that precedes such consonant. Basically, this work is undertaken with an aim of studying the use of the durations of phonemes like a source of information to optimize a recognition system, therefore to introduce explicit models of duration of the phonemes, and to apply these models like part of the acoustic model of the recognition system.</abstract>
			<keywords>Acoustic analyze, standard Arabic, gemination, duration, speech recognition</keywords>
		</article>
		<article id="recital-2005-court-007" session="Posters RECITAL">
			<auteurs>
				<auteur>
					<prenom>Mathieu</prenom>
					<nom>Loiseau</nom>
					<email>mathieu.loiseau@u-grenoble3.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIDILEM - Université Stendhal Grenoble 3</affiliation>
			</affiliations>
			<titre>Vers une utilisation du TAL dans la description pédagogique de textes dans l'enseignement des langues</titre>
			<type>court</type>
			<pages>673-678</pages>
			<resume>Alors que de nombreux travaux portent actuellement sur la linguistique de corpus, l'utilisation de textes authentiques en classe de langue, ou de corpus dans l'enseignement des langues (via concordanciers), quasiment aucun travail n'a été réalisé en vue de la réalisation de bases de textes à l'usage des enseignants de langue, indexées en fonction de critères relevant de la problématique de la didactique des langues. Dans le cadre de cet article, nous proposons de préciser cette notion d'indexation pédagogique, puis de présenter les principaux standards de description de ressources pédagogiques existants, avant de montrer l'inadéquation de ces standards à la description de textes dans l'optique de leur utilisation dans l'enseignement des langues. Enfin nous en aborderons les conséquences relativement à la réalisation de la base.</resume>
			<mots_cles>Corpus, ALAO, TAL, indexation pédagogique, ressources textuelles</mots_cles>
			<title></title>
			<abstract>Despite numerous works concerning corpus linguistics, the use of authentic texts in language teaching as well as corpora in language teaching (through concordancers), hardly any work deals with the creation of a teacher-friendly text base that would be indexed according to criteria relevant to the set of problems of language didactics. In this article, we will first discuss the notion of pedagogical indexation. We will then present the principal pedagogical resource description standards, before showing that those standards are inadequate to handle our problem. Finally we shall give an overview of the consequences of these inadequacies on the implementation of the text base.</abstract>
			<keywords>Corpus, CALL, NLP, pedagogical indexation, textual resources</keywords>
		</article>
		<article id="recital-2005-court-008" session="Posters RECITAL">
			<auteurs>
				<auteur>
					<prenom>Pierre-Sylvain</prenom>
					<nom>Luquet</nom>
					<email>psluquet@info.unicaen.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Laboratoire GREYC - Université de Caen</affiliation>
			</affiliations>
			<titre>Une méthode pour la classification de signal de parole sur la caractéristique de nasalisation</titre>
			<type>court</type>
			<pages>679-684</pages>
			<resume>Nous exposons ici une méthode permettant d’étudier la nature d’un signal de parole dans le temps. Plus précisément, nous nous intéressons à la caractéristique de nasalisation du signal. Ainsi nous cherchons à savoir si à un instant t le signal est nasalisé ou oralisé. Nous procédons par classification à l’aide d’un réseau de neurones type perceptron multi-couches, après une phase d’apprentissage supervisée. La classification, après segmentation du signal en fenêtres, nous permet d’associer à chaque fenêtre de signal une étiquette renseignant sur la nature du signal.</resume>
			<mots_cles>Phonologie, phonétique, classifieur, réseaux de neurones</mots_cles>
			<title></title>
			<abstract>In this paper we expose a method that allows the study of the phonetic features of a speech signal through time. More specifically, we focus on the nasal features of the signal. We try to consider the signal as [+nasal] or [-nasal] at any given time.We proceed with a classifier system based on a multilayer perceptron neural net. The classifier is trained on a hand tagged corpus. The signal is tokenized into 30ms hamming windows. The classification process lets us tag each window with information concerning the properties of its content.</abstract>
			<keywords>Phonology, phonetic, classifier, neural nets</keywords>
		</article>
		<article id="recital-2005-court-009" session="Posters RECITAL">
			<auteurs>
				<auteur>
					<prenom>Wilfried</prenom>
					<nom>Njomgue Sado</nom>
					<email>wilfried.njomgue-sado@hds.utc.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Dominique</prenom>
					<nom>Fontaine</nom>
					<email>dominique.fontaine@hds.utc.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">UMR CNRS 6599 Heudiasyc, Université Technologie de Compiègne</affiliation>
				<affiliation affiliationId="2">Suez Environnement CIRSEE Pôle Informatique Métier</affiliation>
			</affiliations>
			<titre>De la linguistique aux statistiques pour indexer des documents dans un référentiel métier</titre>
			<type>court</type>
			<pages>685-690</pages>
			<resume>Cet article présente une méthode d’indexation automatique de documents basée sur une approche linguistique et statistique. Cette dernière est une combinaison séquentielle de l’analyse linguistique du document à indexer par l’extraction des termes significatifs du document et de l’analyse statistique par la décomposition en valeurs singulières des mots composant le document. La pondération des termes tire avantage de leur contexte local, par rapport au document, global, par rapport à la base de données, et de leur position par rapport aux autres termes, les co-occurrences. Le système d’indexation présenté fait des propositions d’affectations du document à un référentiel métier dont les thèmes sont prédéfinis. Nous présentons les résultats de l’expérimentation de ce système menée sur un corpus des pôles métiers de la société Suez-Environnement.</resume>
			<mots_cles>Linguistique, indexation, recherche d’information, statistique</mots_cles>
			<title></title>
			<abstract>This article presents an automatic method of documents indexing based on a hybrid, linguistic statistical approach. The proposed approach combines a linguistic analysis of the document by the extraction of the significant terms of the document in conformity with the referential; and a statistical analysis of the same document decomposed into separed words. Innovating weighting of terms is set to take judiciously advantage of both their position with respect to other terms (co-occurrence) and their local and global context. An application was developed in order to assign referential-based topics to documents. Finally, we will present experiments results and evaluation carried out on documents of Suez-Environnement Company.</abstract>
			<keywords>Linguistics, statistics, indexing, information processing</keywords>
		</article>
		<article id="recital-2005-court-010" session="Posters RECITAL">
			<auteurs>
				<auteur>
					<prenom>Ali</prenom>
					<nom>Rachidi</nom>
					<email>rachidi.ali@caramail.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Driss</prenom>
					<nom>Mammass</nom>
					<email>driss_mammass@yahoo.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université Ibn Zohr, Laboratoire de Traitement d'Images et Systèmes d'Information (LTISI)</affiliation>
			</affiliations>
			<titre>Vers un Système d'écriture Informatique Amazighe :Méthodes et développements</titre>
			<type>court</type>
			<pages>691-696</pages>
			<resume>L'intégration des technologies de l'information et de communication (TIC) à l'apprentissage de la langue Amazighe est absolument nécessaire pour qu’elle ait droit de cité plein et entier sur le Web et dans le monde informatisé. Nous présentons quelques réflexions sur les stratégies et méthodes d'informatisation de l'amazighe qui est une langue peu dotée informatiquement. Ces réflexions visent surtout l'optimisation de l'effort d'informatisation. En effet, les méthodes proposées tiennent en compte non seulement l'alphabet proposé par l'IRCAM1 et confirmée par l'ISO (format Unicode) le 21 juin 2004 (IRCAM, 2004 a) mais aussi le contexte francophone des populations berbères.</resume>
			<mots_cles>Amazighe, Alphabet Tifinaghe, Traitement des langues naturelles</mots_cles>
			<title></title>
			<abstract>Learning the Amazighe language would require the introduction of Information and Communication technologies so that the language would have full entire freedom of a mentioning on the Web and in the computerized world. We present some reflections on strategies and methods of the amazighe computerization which is a language little dowered. These reflections aim particularly the optimisation of the effort of computerization. Indeed, proposed methods hold in account no only the proposed alphabet by the IRCAM1 and confirmed by the ISO (Unicode format) the 21 June 2004 but therefore the French speaking context of Berber populations.</abstract>
			<keywords>Amazighe, Tifinaghe Alphabet, Natural Language Processing</keywords>
		</article>
		<article id="recital-2005-court-011" session="Posters RECITAL">
			<auteurs>
				<auteur>
					<prenom>Tahar</prenom>
					<nom>Saidane</nom>
					<email>saidane.tahar@planet.tn</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Mounir</prenom>
					<nom>Zrigui</nom>
					<email>mounir.zrigui@fsm.rnu.tn</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Mohamed</prenom>
					<nom>Ben Ahmed</nom>
					<email>Mohamed.BenAhmed@riadi.rnu.tn</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">STEG</affiliation>
				<affiliation affiliationId="2">RIADI</affiliation>
			</affiliations>
			<titre>Un système de lissage linéaire pour la synthèse de la parole arabe : Discussion des résultats obtenus</titre>
			<type>court</type>
			<pages>697-702</pages>
			<resume>Notre article s'intègre dans le cadre du projet intitulé "Oréodule" : un système embarqué temps réel de reconnaissance, de traduction et de synthèse de la parole. L'objet de notre intérêt dans cet article est la présentation de notre système de synthèse hybride de la parole arabe. Nous présenterons, dans ce papier, les différents modules et les différents choix techniques de notre système de synthèse hybride par concaténation de polyphèmes. Nous détaillerons également les règles de transcription et leurs effets sur le traitement linguistique, les règles de syllabation et leurs impacts sur le coût (temps et difficulté) de réalisation du module acoustique et nous poursuivrons par l'exposé de nos choix au niveau du module de concaténation. Nous décrirons le module de lissage, un traitement acoustique, post concaténation, nécessaire à l'amélioration de la qualité de la voix synthétisée. Enfin, nous présenterons les résultats de l'étude statistique de compréhension, réalisée sur un corpus.</resume>
			<mots_cles>Synthèse de la parole arabe, Phonèmes, Diphones, Triphones, Unités acoustiques, Dictionnaire de polyphones</mots_cles>
			<title></title>
			<abstract>This research paper is within the project entitled "Oreillodule" : a real time embedded system of speech recognition, translation and synthesis. The core of our interest in this work is the presentation of the hybrid system of the Arabic speech synthesis and more precisely of the linguistic and the acoustic treatment. Indeed, we will focus on the grapheme-phoneme transcription, an integral stage for the development of this speech synthesis system with an acceptable quality. Then, we will present some of the rules used for the realization of the phonetic treatment system. These rules are stocked in a data base and browsed several times during the transcription. We will also present the module of syllabication in acoustic units of variable sizes (phoneme, diphone and triphone), as well as the corresponding polyphones dictionary. We will list the stages of the establishment of this dictionary and the difficulties faced during its development. Finally, we will present the results of the statistical survey of understanding, achieved on a corpus.</abstract>
			<keywords>Arabic speech sythesis, Phoneme, Diphones, Triphones, Acoustic units, Polyphones dictionary</keywords>
		</article>
		<article id="recital-2005-court-012" session="Posters RECITAL">
			<auteurs>
				<auteur>
					<prenom>Marina</prenom>
					<nom>Santini</nom>
					<email>Marina.Santini@itri.brighton.ac.uk</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">University of Brighton</affiliation>
			</affiliations>
			<titre></titre>
			<type>court</type>
			<pages>703-708</pages>
			<resume>Le Web a causé beaucoup de changements dans plusieurs domaines. Il a aussi influencé l'inventaire des genres textuels traditionnels. De nouveaux genres ont été créés, par exemple les blogues et foires aux questions. Il est probable que d'autres genres soient en train de se former, parce que le Web est un médium qui change constamment. Dans cet article, nous présentons une expérience qui vise à faire apparaître de façon inductive les plans textuels émergents, qui peuvent devenir un nouveau genre ou une nouvelle typologie textuelle dans peu de temps. Il s'agit de regrouper (analyse de groupement) les pages web en utilisant des traits linguistiques et de présentation. Les résultats sont encourageants et invitent à poursuivre la recherche dans ce domaine.</resume>
			<mots_cles>genre textuels sur le Web, typologie des pages Web, analyse de groupement</mots_cles>
			<title>Clustering Web Pages to Identify Emerging Textual Patterns</title>
			<abstract>The Web has triggered many adjustments in many fields. It also has had a strong impact on the genre repertoire. Novel genres have already emerged, e.g. blog and FAQs. Presumably, other new genres are still in formation, because the Web is still fluid and in constant change. In this paper we present an experiment that explores the possibility of automatically detecting the emerging textual patterns that are slowly taking shape on the Web. Emerging textual patterns can develop into novel Web genres or novel text types in the near future. The experimental set up includes a collection of unclassified web pages, two sets of features and the use of cluster analysis. Results are encouraging and deserve further investigation.</abstract>
			<keywords>Web genres, Web Text Types, Web pages, Cluster Analysis</keywords>
		</article>
		<article id="recital-2005-court-013" session="Posters RECITAL">
			<auteurs>
				<auteur>
					<prenom>Yamina</prenom>
					<nom>Tlili-Guiassa</nom>
					<email>guiyam@yahoo.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Laboratoire de recherche en Informatique-Université de Badji Mokhtar</affiliation>
			</affiliations>
			<titre>Memory-based-Learning et Base de règles pour un Etiqueteur du Texte Arabe</titre>
			<type>court</type>
			<pages>709-714</pages>
			<resume>Jusqu’a présent il n’y a pas de système automatique complet pour l’étiquetage du texte arabe. Les méthodes qu’elles soient basées sur des règles explicites ou sur des calculs statistiques, ont été développées pour pallier au problème de l’ambiguïté lexicale. Celles-ci introduisent des informations sur le contexte immédiat des mots, mais font l’impasse sur les exceptions qui échappent aux traitements. L'apparition des méthodes Memory-Based Learning(MBL) a permis l’exploitation automatique de la similarité de l’information contenue dans de grandes masses de textes et , en cas d'anomalie, permet de déduire la catégorie la plus probable dans un contexte donné, sans que le linguiste ait à formuler des règles explicites. Ce papier qui présente une approche hybride combine les méthodes à base de règles et MBL afin d’optimiser la performance de l’étiqueteur. Les résultats ainsi obtenus, présentés en section 6, sont satisfaisants et l’ objectif recherché est atteint.</resume>
			<mots_cles>Etiquetage, Memory-Based Leaning, K-NN, Base de règles, Morphosyntaxique, Langue Arabe</mots_cles>
			<title></title>
			<abstract>Since now there is no complete automatic system for tagging an Arabian text. Methods based on explicit rules or on statistical calculations, have been developed to palliate problems of lexical ambiguousness. They introduce some information on the immediate context of the words but , make the dead end on the exceptions that escape to treatments. The apparition of the Memory-Based Learning(MBL) methods, that exploit automatically the similarity of information contained in big masses of texts and permit, in case of anomaly, to deduct the likeliest category in a given context, without the linguist has to formulate explicit rules. This paper presents an hybrid approach that combines methods based on rules and MBL, thus, in order to optimize the labeller's performance. Our objective is reached and the gotten results, presented in section 6, are satisfactory.</abstract>
			<keywords>Tagging, Memory-based learning, K-NN, Based-rules, Morphosyntaxitic, Arabic language</keywords>
		</article>
		<article id="recital-2005-court-014" session="Posters RECITAL">
			<auteurs>
				<auteur>
					<prenom>Florentina</prenom>
					<nom>Vasilescu Armaselu</nom>
					<email>armaselu@sympatico.ca</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université de Montréal, Département de littérature comparée</affiliation>
			</affiliations>
			<titre>Cent mille milliards de poèmes et combien de sens? Une étude d'analyse potentielle</titre>
			<type>court</type>
			<pages>715-720</pages>
			<resume>A partir du concept de cohésion comme mesure de l’unité du texte et du modèle oulipien de la littérature par contraintes, notre étude propose une méthode d’analyse potentielle sur ordinateur dans le cas des Cent mille milliards des poèmes. En s’appuyant sur un ensemble de contraintes initiales, notre programme serait capable d’analyser tous les textes potentiels produits par la machine en utilisant ces contraintes.</resume>
			<mots_cles>Unité du discours, réseaux de cohésion, analyse thématique, littérature potentielle</mots_cles>
			<title></title>
			<abstract>Using the concept of cohesion as a measure for the unity of text and the Oulipian model of the literature by constraints, our study proposes a computational method of potential analysis for One hundred billions sonnets. Starting from a set of initial constraints, our program would be able to analyze all the potential texts produced by the machine under these constraints.</abstract>
			<keywords>Discourse unity, networks of cohesion, thematic analysis, potential literature</keywords>
		</article>
		<article id="recital-2005-court-015" session="Posters RECITAL">
			<auteurs>
				<auteur>
					<prenom>Katia</prenom>
					<nom>Zellagui</nom>
					<email>katia.zellagui@univ-fcomte.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LASELDI - Université de Franche-Comté, Besançon.</affiliation>
			</affiliations>
			<titre>Analyse informatique du roman proustien "Du coté de chez Swann"</titre>
			<type>court</type>
			<pages>721-726</pages>
			<resume>Dans le cadre du développement des environnements d’analyse linguistique, d’étiquetage de corpus et d’analyse statistique afin de traiter des corpus de grande taille, nous proposons de mettre au point des procédures nouvelles d’étiquetage morpho-syntaxique et sémantique. Nous présentons un ensemble de ressources linguistiques - dictionnaires et grammaires - dans le but d’étiqueter entièrement le roman proustien : « Du côté de chez Swann ». Notre recherche avance deux atouts majeurs : la précision des étiquettes attribuées aux formes linguistiques du texte ; et le repérage et étiquetage exhaustifs des mots composés.</resume>
			<mots_cles>Automate fini, grammaire locale, dictionnaire électronique, étiquetage morpho-syntaxique, désambiguïsation, textes littéraires</mots_cles>
			<title></title>
			<abstract>To deal with a great amount of corpus data within the framework of environmental development of linguistic analysis of corpus' tagging and statistic analysis, we propose to establish new procedures of syntactic and semantic tagging. We present some general linguistic resources, such as dictionary and grammar built-in the way to entirely tag the novel of Proust «Du côté de chez Swann». Our research leads to two main advantages: precise tagging assigned to linguistic forms of the text and identification and exhaustive tagging of compound nouns.</abstract>
			<keywords>Finite state automata, local grammar, electronic dictionary, morpho-syntactic tagging, disambiguation, literary texts</keywords>
		</article>
		<article id="recital-2005-court-016" session="Posters RECITAL">
			<auteurs>
				<auteur>
					<prenom>Anis</prenom>
					<nom>Zouaghi</nom>
					<email>Anis.Zouaghi@riadi.rnu.tn</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Mounir</prenom>
					<nom>Zrigui</nom>
					<email>Mounir.Zrigui@fsm.rnu.tn</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Mohamed</prenom>
					<nom>Ben Ahmed</nom>
					<email>Mohamed.BenAhmed@riadi.rnu.tn</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université de Mannouba - RIADI (Unité de Monastir) ENSI.</affiliation>
				<affiliation affiliationId="2">Université du centre - RIADI (Unité de Monastir) FSM</affiliation>
			</affiliations>
			<titre>Un étiqueteur sémantique des énoncés en langue arabe</titre>
			<type>court</type>
			<pages>727-732</pages>
			<resume>Notre article s’intègre dans le cadre du projet intitulé Oréodule: un système de reconnaissance, de traduction et de synthèse de la parole spontanée. L’objectif de cet article est de présenter un modèle d’étiquetage probabiliste, selon une approche componentielle et sélective. Cette approche ne considère que les éléments de l’énoncé porteurs de sens. La signification de chaque mot est représentée par un ensemble de traits sémantiques Ts. Ce modèle participe au choix des Ts candidats lors du décodage sémantique d’un énoncé.</resume>
			<mots_cles>Modèles statistiques de langage, Modèles n-classes, Décodage sémantique, Approche componentielle et sélective</mots_cles>
			<title></title>
			<abstract>The work reported here is part of a larger research project, Oréodule, aiming at developing tools for automatic speech recognition, translation, and synthesis for the Arabic language. This article focuses on a probabilistic labelling model, according to a componential and selective approach. This approach considers only the elements of the statement carrying direction. The significance of each word is represented by a whole of semantic features Ts. This model takes part in the choice of the Ts candidates at the time of the semantic decoding of a statement.</abstract>
			<keywords>Statistical models of language, Models N-classes, Semantic analyze, Componential and selective approach</keywords>
		</article>
	</articles>
</conference>
