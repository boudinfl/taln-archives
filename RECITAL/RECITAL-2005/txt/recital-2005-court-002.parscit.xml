<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>T Buckwalter</author>
</authors>
<journal>Buckwalter Arabic Morphological Analyzer Version</journal>
<volume>1</volume>
<pages>2002--49</pages>
<marker>Buckwalter, </marker>
<rawString>Buckwalter T.(2002), Buckwalter Arabic Morphological Analyzer Version 1.0, http://www.ldc.upenn.edu/Catalog/CatologEntry.jsp?catologId=LDC2002L49.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Darwish</author>
</authors>
<title>Building a Shallow Arabic Morphological Analyzer in One Day,</title>
<date>2002</date>
<booktitle>Proceedings of the workshop on Computational Approaches to Semitic Languages in the 40th Annual Meeting of the Association for Computational Linguistics (ACL-02) ,</booktitle>
<pages>47--54</pages>
<contexts>
<context position="7158" citStr="Darwish, 2002" startWordPosition="1116" endWordPosition="1117">entre les deux c’est que GfIdf augmente le poids des mots fréquents. Idf ⎜⎛ Nlog2 ⎟⎟ ⎞ ⎜ ⎝ dfi ⎠ Figure 1 : Paramètres de pondération utilisés 4 Traitements linguistiques L’arabe est une langue sémitique s’écrivant de droite à gauche elle comporte 28 consonnes et 6 voyelles standard (3 longues :َا ُو ٍي et 3 courtes : ُ/ ِ/ َ/). Le traitement automatique de l’arabe est difficile vu ses variations orthographiques et sa structure morphologique complexe. Siham Boulaknadel, Fadoua Ataa-Allah Deux approches sont utilisées dans l’analyse morphologique de l’arabe, la première que nous avons choisie (Darwish, 2002) est une analyse morphologique assouplie ou racinisation qui consiste à essayer de déceler si des suffixes ou préfixes ont été ajoutés à l’unité lexicale : par exemple pour le duel (نا) dans (ناملعم, deux professeurs), le pluriel des noms masculins (نو , ني) dans (نوملعم, des professeurs) et féminins (تا) dans (تاملسم, musulmanes) ; la forme possessive (ان, مآ, مه) dans (مهباتآ, ses livres) et les préfixes dans les articles définis (لا, لاو, لاب, لاآ, لاف). La deuxième est une lemmatisation qui consiste à réduire les formes déclinées à une représentation canonique. 5 Expérimentations Notre obj</context>
</contexts>
<marker>Darwish, 2002</marker>
<rawString>Darwish K. (2002), Building a Shallow Arabic Morphological Analyzer in One Day, Proceedings of the workshop on Computational Approaches to Semitic Languages in the 40th Annual Meeting of the Association for Computational Linguistics (ACL-02) , pp. 47-54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Deerwester</author>
<author>S T Dumais</author>
<author>G W Furnas</author>
<author>T K Landauer</author>
<author>R Hrashman</author>
</authors>
<title>Indexing by latent semantic analysis,</title>
<date>1990</date>
<journal>Journal of the</journal>
<booktitle>Vol.41,</booktitle>
<pages>391--407</pages>
<contexts>
<context position="3420" citStr="Deerwester et al., 1990" startWordPosition="495" endWordPosition="498">ion de réseaux sémantiques qui consiste à recourir à une base de connaissances linguistiques regroupant les mots sémantiquement proches et structuré selon des relations hyperonymiques et/ou synonymiques (Grefenstette, 1994), soit par l’extension de requêtes, opération par laquelle un certain nombre de termes issus de documents de la collection sont ajoutés à une requête. La troisième possibilité que nous avons choisie, consiste à se servir des relations sémantiques implicites induites par les cooccurrences entre termes dans les documents. Ainsi le modèle de l’analyse sémantique latente (LSA) (Deerwester et al., 1990) consiste à réduire le nombre de dimensions de l’espace vectoriel en s’appuyant sur le fait que les documents traitant des mêmes sujets ont des vocabulaires proches et sont donc proches dans l’espace vectoriel. Dans notre travail, nous avons sélectionné les schémas de pondération qui améliorent la performance de la méthode LSA pour le calcul de similarité, dans le cas de cinq corpus de petites tailles en langue arabe, tout en évaluant l’importance de différents paramètres linguistiques utilisés. 2 Présentation de LSA L’analyse sémantique latente (LSA) consiste à réduire le nombre de dimensions</context>
</contexts>
<marker>Deerwester, Dumais, Furnas, Landauer, Hrashman, 1990</marker>
<rawString>Deerwester S, Dumais S.T., Furnas G.W., Landauer T.K., Hrashman R. (1990), Indexing by latent semantic analysis, Journal of the american society for information science, Vol.41, pp. 391-407.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Grefenstette</author>
</authors>
<title>Explorations in automatic thesaurus discovery,</title>
<date>1994</date>
<publisher>Kluwer Academic Publishers.</publisher>
<location>New York,</location>
<contexts>
<context position="3019" citStr="Grefenstette, 1994" startWordPosition="437" endWordPosition="438"> chaque terme d’indexation constitue une dimension de l’espace vectoriel, sans considération d’éventuelles relations entre termes. En l’absence d’une connaissance approfondie de la collection de documents, la requête peut être formulé en des termes proches mais non identiques à ceux employés dans un document. Un certain nombre de chercheurs se sont intéressés à ce problème, soit par l’utilisation de réseaux sémantiques qui consiste à recourir à une base de connaissances linguistiques regroupant les mots sémantiquement proches et structuré selon des relations hyperonymiques et/ou synonymiques (Grefenstette, 1994), soit par l’extension de requêtes, opération par laquelle un certain nombre de termes issus de documents de la collection sont ajoutés à une requête. La troisième possibilité que nous avons choisie, consiste à se servir des relations sémantiques implicites induites par les cooccurrences entre termes dans les documents. Ainsi le modèle de l’analyse sémantique latente (LSA) (Deerwester et al., 1990) consiste à réduire le nombre de dimensions de l’espace vectoriel en s’appuyant sur le fait que les documents traitant des mêmes sujets ont des vocabulaires proches et sont donc proches dans l’espace</context>
</contexts>
<marker>Grefenstette, 1994</marker>
<rawString>Grefenstette G. (1994), Explorations in automatic thesaurus discovery, New York, Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
</authors>
<title>Automatic text processing the transformation analysis and retrieval of information by computer,</title>
<date>1989</date>
<publisher>Addison-Wesley.</publisher>
<location>New York,</location>
<contexts>
<context position="4982" citStr="Salton, 1989" startWordPosition="763" endWordPosition="764">taille (n x n). À partir d’un certain nombre k&lt;n, nous nous apercevons de l’existence de valeurs singulières très faibles et qui peuvent être négligées dans la matrice. De ce fait, il est démontré qu’il y a une meilleure approximation Ak de A qui est donnée par : Ak = Uk Sk V Tk Cette réduction va permettre de ne garder que les unités lexicales les plus significatives. À noter que k est déterminé de façon empirique en fonction du corpus utilisé et du degré de performance voulu. Pour évaluer la performance de la LSA on utilise les deux mesures traditionnelles de précision et de taux de rappel (Salton, 1989). Recherche d’information en langue arabe : influence des paramètres linguistiques et de pondération en LSA 3 Paramètres de pondération La pondération des unités lexicales consiste à transformer l’occurrence d’une unité lexicale dans l’unité textuelle par une combinaison de pondérations locales L(i,j), indiquant l’importance de l’unité lexicale i dans l’unité textuelle j et pondérations globales G(i), indiquant l’importance de l’unité lexicale i dans l’ensemble des unités textuelles de la collection. Avec fij la fréquence de l’unité lexicale i dans l’unité textuelle j, dfi le nombre d’unités t</context>
</contexts>
<marker>Salton, 1989</marker>
<rawString>Salton G. (1989), Automatic text processing the transformation analysis and retrieval of information by computer, New York, Addison-Wesley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
</authors>
<title>An Introduction to Modern Information Retrieval,</title>
<date>1983</date>
<location>New York, McGrawHill.</location>
<contexts>
<context position="2294" citStr="Salton, 1983" startWordPosition="336" endWordPosition="337">stemmer; the fourth where we combined stopword list and arabic stemmer. Broadly the results of our experiments show that the linguistic treatments as well as weighting of lexemes used improve the performance of LSA. Siham Boulaknadel, Fadoua Ataa-Allah 1 Introduction En recherche d’information, le problème d’accès au texte est essentiellement dû à l’écart entre les termes utilisés dans les requêtes et les documents. L’appariement entre requête et document se fait donc par l’intermédiaire de leur représentation respective. Le modèle de recherche le plus souvent utilisé est le modèle vectoriel (Salton, 1983). Un des problèmes de ce modèle réside dans l’hypothèse d’indépendance faite sur les termes d’indexation : chaque terme d’indexation constitue une dimension de l’espace vectoriel, sans considération d’éventuelles relations entre termes. En l’absence d’une connaissance approfondie de la collection de documents, la requête peut être formulé en des termes proches mais non identiques à ceux employés dans un document. Un certain nombre de chercheurs se sont intéressés à ce problème, soit par l’utilisation de réseaux sémantiques qui consiste à recourir à une base de connaissances linguistiques regro</context>
</contexts>
<marker>Salton, 1983</marker>
<rawString>Salton G. (1983), An Introduction to Modern Information Retrieval, New York, McGrawHill.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>