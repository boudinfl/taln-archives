<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
"http://www.w3.org/TR/html4/loose.dtd">
<html><head><title>Etiquetage morpho-syntaxique des textes arabes par mod&#232;le de Markov cach&#233;</title>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
</head>
<body>
<div style="page-break-before:always; page-break-after:always"><div><p>RECITAL 2005, Dourdan, 6&#8211;10 juin 2005
</p>
<p>Etiquetage morpho-syntaxique des textes arabes par mod&#232;le
de Markov cach&#233;
</p>
<p>Abdelhamid EL JIHAD (1), Abdellah YOUSFI (2)
(1),(2) Institut d&#8217;&#233;tudes et de recherches pour l&#8217;arabisation
</p>
<p>Universit&#233; Mohamed V, Rabat, Maroc
(1) eljihad@ifrance.com
</p>
<p>date de soutenance pr&#233;vue : 2007
(2) yousfi240ma@yahoo.fr
</p>
<p>date de soutenance : 19 juin 2001
</p>
<p>Mots-clefs &#8211; Keywords
</p>
<p>Corpus, jeu d&#8217;&#233;tiquettes, Etiquetage morpho-syntaxique, texte arabe, mod&#232;le de Markov cach&#233;
Corpus, the set of tags, the morpho-syntactic tagging, arabic text, Hidden Markov Model
</p>
<p>R&#233;sum&#233; - Abstract
</p>
<p>L&#8217;&#233;tiquetage des textes est un outil tr&#232;s important pour le traitement automatique de langage, il
est utilis&#233; dans plusieurs applications par exemple l&#8217;analyse morphologique et syntaxique des
textes, l&#8217;indexation, la recherche documentaire, la voyellation pour la langue arabe, les mod&#232;les
de langage probabilistes (mod&#232;les n-classes), etc.
Dans cet article nous avons &#233;labor&#233; un syst&#232;me d&#8217;&#233;tiquetage morpho-syntaxique de la langue
arabe en utilisant les mod&#232;les de Markov cach&#233;s, et ceci pour construire un corpus de r&#233;f&#233;rence
&#233;tiquet&#233; et repr&#233;sentant les principales difficult&#233;s grammaticales rencontr&#233;es en langue arabe
g&#233;n&#233;rale.
Pour l&#8217;estimation des param&#232;tres de ce mod&#232;le, nous avons utilis&#233; un corpus d&#8217;apprentissage &#233;ti-
quet&#233; manuellement en utilisant un jeu de 52 &#233;tiquettes de nature morpho-syntaxique. Ensuite
on proc&#232;de &#224; une am&#233;lioration du syst&#232;me gr&#226;ce &#224; la proc&#233;dure de r&#233;estimation des param&#232;tres
de ce mod&#232;le.
</p>
<p>The tagging of texts is a very important tool for various applications of natural language pro-
cessing : morphological and syntactic analysis of texts, indexation and information retrieval,
vowelling of arabic texts, probabilistic language model (n-class model).
In this paper we have used the Hidden Markov Model (HMM) to tag the arabic texts. This
system of tagging is used to build a large labelled arabic corpus. The experiments are carried in
the set of the labelled texts and the 52 tags of morpho-syntactic nature, in order to estimate the
parameters of the HMM.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>1 Introduction
</p>
<p>Le d&#233;veloppement des corpus &#233;lectroniques a b&#233;n&#233;fici&#233; ces derni&#232;res ann&#233;es d&#8217;un appui vigour-
eux et un soutien financier important, de la communaut&#233; du traitement automatique des langues
naturelles, qui voit l&#224; une &#233;tape indispensable pour la mise au point de syst&#232;mes de TAL ro-
bustes. Aujourd&#8217;hui de vaste corpus de textes &#233;lectroniques &#233;tiquet&#233;s sont disponibles et sont
majoritairement de langue anglaise. Ceci a permis l&#8217;essor consid&#233;rable des traitements automa-
tiques concernant cette langue; des outils d&#8217;interrogation de ces corpus ainsi que des outils
d&#8217;annotations proprement dits (&#233;tiqueteurs, analyseurs syntaxique, etc.) se r&#233;pandent. Leurs
&#233;quivalents en fran&#231;ais commence &#224; appara&#238;tre &#233;galement [Habert et al 1997].
</p>
<p>Pour la langue arabe, il n&#8217;existe pas &#224; ce jour de corpus &#233;tiquet&#233; ais&#233;ment disponible. Par
cons&#233;quent les recherches linguistiques qui ont recours &#224; des corpus &#233;tiquet&#233;s sont donc encore
rares. Motiv&#233; par ce manque, l&#8217;Institut d&#8217;Etudes et de Recherches pour l&#8217;Arabisation (IERA)
a entrepris un projet de recherche dont l&#8217;objectif est la constitution d&#8217;un corpus de r&#233;f&#233;rence
&#233;tiquet&#233; et repr&#233;sentant les principales difficult&#233;s grammaticales rencontr&#233;es en langue arabe
g&#233;n&#233;rale. La disponibilit&#233; de ce corpus &#224; l&#8217;institut, va donner le coup d&#8217;envoi aux divers travaux
de recherches linguistiques qui utilisent les corpus &#233;tiquet&#233;s. Un corpus &#233;tiquet&#233; est un corpus
dans lequel on associe &#224; des segments de textes (le plus souvent des mots) d&#8217;autres informations
de quelque nature qu&#8217;elle soit morphologique, syntaxique, s&#233;mantique, prosodique, critique, etc
[Veronis 2000][Vergne et al 1998].
</p>
<p>En particulier, dans la communaut&#233; du traitement automatique des langues naturelles, quand
on parle de corpus &#233;tiquet&#233; on fait r&#233;f&#233;rence le plus souvent &#224; un document o&#249; chaque mot pos-
s&#232;de une &#233;tiquette morpho-syntaxique et une seule.
L&#8217;&#233;tiquetage morpho-syntaxique automatique est un processus qui s&#8217;effectue g&#233;n&#233;ralement en
trois &#233;tapes [Minh et al 2003][Rajman et al 2000]: la segmentation du texte en unit&#233;s lexicales,
l&#8217;&#233;tiquetage &#224; priori, la d&#233;sambigu&#239;sation qui permet d&#8217;attribuer, pour chacun des unit&#233;s lexi-
cales et en fonction de son contexte, l&#8217;&#233;tiquette morpho-syntaxique pertinente.
La taille du jeu d&#8217;&#233;tiquettes, la taille du corpus d&#8217;apprentissage sont autant de facteur importants
pour une bonne performance du syst&#232;me d&#8217;&#233;tiquetage [Chanod 1995][Claud 1995].
En g&#233;n&#233;ral, il existe deux m&#233;thodes pour l&#8217;&#233;tiquetage morpho-syntaxique :
- M&#233;thode &#224; base de r&#232;gles [Claud 1995][Bril 1992].
- M&#233;thode probabiliste.
Dans cet article nous avons utilis&#233; la deuxi&#232;me approche.
</p>
<p>2 M&#233;thode probabiliste
</p>
<p>Le choix de l&#8217;&#233;tiquette la plus probable en un point donn&#233; se fait au regard de l&#8217;historique des
derni&#232;res &#233;tiquettes qui viennent d&#8217;&#234;tre attribu&#233;es. En g&#233;n&#233;ral cet historique se limite &#224; une ou
deux &#233;tiquettes pr&#233;c&#233;dentes. Cette m&#233;thode suppose qu&#8217;on dispose d&#8217;un corpus d&#8217;apprentissage
qui doit &#234;tre d&#8217;une taille suffisante pour permettre une estimation fiable des probabilit&#233;s [Habert
et al 1997].
Soit Ph = w1...wT une phrase constitu&#233;e des mots w1, ..., wT , E = {et1, ..., etN} un jeu
d&#8217;&#233;tiquettes.
L&#8217;&#233;tiquetage morpho-syntaxique de la phrase Ph par des &#233;tiquettes appartenant &#224;E et s&#8217;appuyant</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Etiquetage morpho-syntaxique des textes arabes par mod&#232;le de Markov cach&#233;
</p>
<p>sur l&#8217;approche probabiliste , consiste &#224; trouver l&#8217;ensemble d&#8217;&#233;tiquettes et&#8727;1...et&#8727;T associ&#233;s &#224; la
phrase Ph tel que :
</p>
<p>et&#8727;1...et&#8727;T = arg max
et1...etT
</p>
<p>Pr(w1...wT , et1...etT ) (1)
</p>
<p>Pour faciliter la r&#233;solution de ce probl&#232;me on utilise les mod&#232;les de Markov cach&#233;s d&#8217;ordre 1.
</p>
<p>3 Etiquetage morpho-syntaxique par mod&#232;le de Markov cach&#233;
d&#8217;ordre 1
</p>
<p>Un mod&#232;le de Markov cach&#233; d&#8217;ordre 1 est un double processus (Xt, Yt)t&#8805;1 avec :
&#8226;Xt est une cha&#238;ne de Markov d&#8217;ordre 1 &#224; valeur dans un ensemble d&#8217;&#233;tats finiQ = {q1, ..., qN},
Xt v&#233;rifie :
Pr(Xt+1 = qj/X1 = q1, ..., Xt = qi) = Pr(Xt+1 = qj/Xt = qi) = aij .
Pr(X1 = qi) = pii, i = 1, ..., N .
aij est la probabilit&#233; de transition entre les &#233;tats qi et qj .
pii est la probabilit&#233; que l&#8217;&#233;tats qi est un &#233;tat initial.
&#8226; Yt est un processus observable &#224; valeurs dans un ensemble mesurable Y , Yt v&#233;rifie :
Pr(Yt = yt/X1 = q1, ..., Xt = qi, Y1 = y1, ..., Yt&#8722;1 = yt&#8722;1) = Pr(Yt = yt/Xt = qi) =
bi(yt) = bit.
bit est la probabilit&#233; d&#8217;&#233;mission de l&#8217;observation yt &#224; partir de l&#8217;&#233;tat qi.
Dans la suite on supposera que le double processus :
Xt = etit repr&#233;sentant les &#233;tiquettes appartenant &#224; l&#8217;ensemble E,
Yt = wt repr&#233;sentant les mots de notre vocabulaire V = {w1, ..., wL},
est un mod&#232;le de Markov cach&#233; d&#8217;ordre 1.
Remarque :
Ce mod&#232;le est d&#233;fini enti&#232;rement par un vecteur de param&#232;tres not&#233; &#955; = (&#928;, A,B).
&#8226; &#928; = {pi1, ..., piN} l&#8217;ensemble des probabilit&#233;s initiales.
&#8226; A = (aij)1&#8804;i,j&#8804;N la matrice des probabilit&#233;s de transition entre les &#233;tiquettes.
&#8226; B = (bit)1&#8804;i&#8804;N et 1 &#8804; t &#8804; L : la matrice des probabilit&#233;s d&#8217;&#233;mission des mots &#224; partir des
&#233;tiquettes.
</p>
<p>4 Proc&#233;dure d&#8217;apprentissage (Estimation des param&#232;tres)
L&#8217;apprentissage est une op&#233;ration n&#233;cessaire pour un syst&#232;me de reconnaissance de formes
(en particulier le syst&#232;me d&#8217;&#233;tiquetage), il permet d&#8217;estimer les param&#232;tres du mod&#232;le &#955; =
(&#928;, A,B). Un apprentissage incorrect ou insuffisant diminue la performance du syst&#232;me d&#8217;&#233;tiqu-
etage. Pour pr&#233;parer le corpus d&#8217;apprentissage, on proc&#232;de par approximations successives. Un
premier corpus d&#8217;apprentissage, relativement court, permet d&#8217;&#233;tiqueter un corpus beaucoup plus
important. Celui-ci est corrig&#233;, ce qui permet de r&#233;estimer les probabilit&#233;s, il sert donc &#224; un sec-
ond apprentissage, et ainsi de suite.
En g&#233;n&#233;ral il existe trois m&#233;thodes d&#8217;estimation de ces param&#232;tres1 :
&#8226; L&#8217;estimation par maximum de vraisemblance (Maximum Likelihood Estimation), elle est r&#233;al-
is&#233;e par l&#8217;algorithme de Baum-Welch [Baum 1972] ou l&#8217;algorithme de Viterbi [Celeux 92].
</p>
<p>1Pour plus de d&#233;taille sur ces formule voir [Yousfi 2001]</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>&#8226; L&#8217;estimation par maximum a posteriori [John Arice].
&#8226; L&#8217;estimation par maximum d&#8217;information mutuel [Bahl et al 86,87][Kapadia 93].
Dans notre cas nous avons utilis&#233; l&#8217;estimation par maximum de vraisemblance car c&#8217;est la plus
utilis&#233;e et la plus facile &#224; calculer.
Alors si on prend un ensemble d&#8217;apprentissage R = {Ph1, ..., PhK}, constitu&#233; des phrases
Ph1, ..., PhK &#233;tiquet&#233;es manuellement, les formules d&#8217;estimation des param&#232;tres du mod&#232;le
&#955; = (&#928;, A,B) sont donn&#233;es par :
</p>
<p>aij =
</p>
<p>&#8721;K
n=1 le nombre de fois o&#249; la transition eti etj est dans la phrase Phn&#8721;K
</p>
<p>n=1 le nombre de fois o&#249; l&#8217;&#233;tat eti est atteint le long de la phrase Phn
</p>
<p>pii =
</p>
<p>&#8721;K
n=1 &#948;[l&#8217;&#233;tiquette eti est un &#233;tat initial dans la phrase Phn]
</p>
<p>K
</p>
<p>bit =
</p>
<p>&#8721;K
n=1 le nombre de fois o&#249; le mot wt &#224; l&#8217;&#233;tiquette eti le long de la phrase Phn&#8721;K
</p>
<p>n=1 le nombre de fois o&#249; l&#8217;&#233;tat eti est atteint le long de la phrase Phn
avec :
</p>
<p>&#948;[x] =
</p>
<p>{
1 si l&#8217;&#233;v&#233;nement x est vrai
0 sinon
</p>
<p>5 Etiquetage automatique par algorithme de Viterbi
</p>
<p>Pour un calcul plus rapide du chemin optimal2 dans la formule (1) nous avons utilis&#233; l&#8217;algorithme
de Viterbi [For 73].
On note par :
</p>
<p>&#948;t(etj) = max
eti1 ...etit
</p>
<p>Pr(w1...wt, eti1 ...etit)
</p>
<p>avec etit = etj .
Cette formule devient [Yousfi 2001]:
</p>
<p>&#948;t(etj) = max
eti
</p>
<p>&#948;t&#8722;1(eti).aij.bj(wt)
</p>
<p>On calcule cette formule pour toutes les valeurs t = 1, ..., T et j = 1, ..., N .
Enfin le chemin optimal est obtenu &#224; l&#8217;aide d&#8217;un calcul r&#233;cursif sur cette formule.
</p>
<p>6 Exp&#233;rimentation
</p>
<p>6.1 Donn&#233;es d&#8217;apprentissage
</p>
<p>Le travail exp&#233;rimental a &#233;t&#233; r&#233;alis&#233; en trois grandes &#233;tapes :
1) &#233;tape de d&#233;finition du jeu d&#8217;&#233;tiquettes et de construction de corpus d&#8217;apprentissage.
La d&#233;finition de notre propre jeu d&#8217;&#233;tiquettes morpho-syntaxique a &#233;t&#233; particuli&#232;rement d&#233;licate,
cette phase a &#233;t&#233; r&#233;alis&#233;e en collaboration avec des linguistes pour satisfaire au besoin des pro-
jets en cours de r&#233;alisation &#224; IERA. Ce jeu d&#8217;&#233;tiquettes est constitu&#233; de 52 &#233;tiquettes de nature
</p>
<p>2Nous cherchons ce chemin dans un r&#233;seau d&#8217;&#233;tiquettes. Ce r&#233;seau est construit de tel fa&#231;on &#224; ce que pour
une phrase donn&#233;e, chaque chemin de ce r&#233;seau correspond &#224; la probabilit&#233; que cette phrase &#224; les &#233;tiquettes de ce
chemin (Pr(w1...wt, eti1 ...etit)). Le chemin associ&#233; &#224; la probabilit&#233; maximale est nomm&#233; chemin optimal.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Etiquetage morpho-syntaxique des textes arabes par mod&#232;le de Markov cach&#233;
</p>
<p>morpho-syntaxique (comme par exemple ism-faail, ism-mafaoul, harf nasb,...).
Le corpus d&#8217;apprentissage est constitu&#233; d&#8217;un ensemble de phrases repr&#233;sentant les principales
r&#232;gles morphologiques et syntaxiques utilis&#233;es en langue arabe g&#233;n&#233;rale. Ce corpus a &#233;t&#233; &#233;ti-
quet&#233; manuellement par un linguiste.
2) &#233;tape d&#8217;estimation des param&#232;tres du mod&#232;le de Markov cach&#233;.
3) &#233;tape d&#8217;&#233;tiquetage automatique et r&#233;estimation des param&#232;tres du mod&#232;le de Markov cach&#233;.
Pour r&#233;aliser ces deux derni&#232;res &#233;tapes, nous avons d&#233;velopp&#233; une application en langage C,
comportant deux modules, module d&#8217;apprentissage et module d&#8217;&#233;tiquetage automatique qui
permet d&#8217;&#233;tiqueter automatiquement un corpus brut, ce dernier est corrig&#233; manuellement pour
servir &#224; une r&#233;estimation des param&#232;tres du mod&#232;le de Markov cach&#233;.
Les programmes sont &#233;valu&#233;s sur deux versions de textes voyell&#233; et non voyell&#233;.
</p>
<p>6.2 R&#233;sultats
</p>
<p>Le taux d&#8217;erreur est mesur&#233; sur deux ensembles :
Ensemble1 constitu&#233; des m&#234;mes phrases que l&#8217;ensemble d&#8217;apprentissage mais sans &#233;tiquettes,
Ensemble2 constitu&#233; de phrases (sans &#233;tiquettes) diff&#233;rentes de celles de l&#8217;ensemble d&#8217;apprenti-
ssage.
</p>
<p>Ensemble1 Ensemble2
Textes voyell&#233;s 1,76% 2%
</p>
<p>Textes non voyell&#233;s 2,5% 3%
</p>
<p>Table 1: Les taux d&#8217;erreur d&#8217;&#233;tiquetage automatique.
</p>
<p>On remarque que dans le cas des textes non voyell&#233;s le taux d&#8217;erreur augmente par rapport
aux textes voyell&#233;s, &#224; cause de l&#8217;augmentation de l&#8217;ambigu&#239;t&#233; (un mot peut prendre plusieurs
&#233;tiquettes). Pour le reste des erreurs, elles sont dues au manque de donn&#233;es d&#8217;apprentissage (il
existe des mots et des transitions entre des &#233;tiquettes qui ne sont pas repr&#233;sent&#233;es dans le corpus
d&#8217;apprentissage).
</p>
<p>7 Conclusions et perspectives
</p>
<p>En analysant les r&#233;sultats trouv&#233;s, nous avons remarqu&#233; que la majorit&#233; d&#8217;erreurs d&#8217;&#233;tiquetage
provient essentiellement du probl&#232;me de manque ou d&#8217;insuffisance de donn&#233;es d&#8217;apprentissage.
Dans notre cas il existe deux type de probl&#232;mes de manque de donn&#233;es :
&#8226; un ou plusieurs mots, appartenant &#224; la phrase &#224; &#233;tiqueter par ce syst&#232;me, n&#8217;existent pas dans
le lexique, c&#8217;est &#224; dire nous n&#8217;avons pas une estimation des probabilit&#233;s d&#8217;observation de ces
mots dans tous les &#233;tats.
&#8226; une ou plusieurs &#233;tiquettes n&#8217;ont pas de pr&#233;d&#233;cesseurs dans la phrase &#224; &#233;tiqueter automatique-
ment, c&#8217;est &#224; dire nous n&#8217;avons pas une estimation des probabilit&#233;s de transition de ces &#233;tiquettes
vers tous les autres &#233;tiquettes du syst&#232;me.
Dans la suite de notre travail, nous allons proceder &#225; deux solutions pour remedier &#224; ces deux
probl&#232;mes :</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>la premi&#232;re est d&#8217;introduire une sorte d&#8217;analyse morphologique qui s&#8217;appuit sur les formes mor-
phologiques des mots pour pouvoir identifier les &#233;tiquettes des mots inconnus.
La deuxi&#232;me est d&#8217;introduire une base de r&#232;gles syntaxiques qui d&#233;finie les transitions possibles
entre les diff&#233;rents &#233;tiquettes.
</p>
<p>R&#233;f&#233;rences
L.R. Bahl, P.F. Brown, P.V. de Souza &amp; R.L. Mercer : &quot;Maximum mutual information estimation in
</p>
<p>hidden Markov model parameters for speech recognition &quot;, Proc. ICASSP, pp. 49-52, Tokyo, 1986.
L. R. Bahl, P. F. Brown, P.V De Souza and R. L. Mercer : &quot;Estimating HMM parameters so as to
</p>
<p>maximise speech recognition accuracy &quot;, Research Report RC-13121, IBM TJ Watson Research Center,
9/10/1987.
L. Baum : &quot;An inequality and association maximization technique in statistical estimation for proba-
</p>
<p>bilistic functions of Markov processes &quot;, Inequality, vol. 3, 1972.
G. Celux, J. Clairambault :&quot;Estimation de chaines de Markov cach&#233;es: m&#233;thodes et probl&#232;mes &quot;,
</p>
<p>Journ&#233;es th&#233;matiques CNRS sur les approches markoviennes en signal et images, Septembre 1992.
Jean-Pierre Chanod and Pasi Tapanainen : &quot;Tagging French - comparing a statistical and a constraint-
based method&quot;, Proceeding of the seventh Conference of the European Chapter of the Association for
Computatinal
Linguistics (EACL.95), Dublin, Ireland. pp.149-156, 1995.
Claude De Loupy : &quot;La m&#233;thode d&#233;tiquetage d&#8217;Eric Brill&quot;. Revue T.A.L, 1995, Vol.36, nr&#780; 1-2, pp.37-46
Eric Brill : &quot;A simple rule-based part of speech tagger&quot;. Proceedings of the third Conference on Applied
Natural Language Processing, Trento, Italy. pp.152-155. Avril 1992.
Fornay D. R. : &quot;The Viterbi Algorithm &quot;, Proc. IEEE, vol. 61, n 3, mai 1973.
Beno&#238;t Habert, Adeline Nazarenko, Andr&#233; Salem : &quot;Les linguistiques de corpus&quot; , Armand colin /
</p>
<p>Masson.Paris, 1997.
John Rice : &quot;Mathematical Statistics and data
analysis &quot;, page 511-540.
S. Kapadia, V. Valtchev &amp; S.J. Young : &quot;MMI training for continuous phoneme recognition on the TIMIT
database &quot;, Proc. ICASSP, pp. II.491-494, Minneapolis, 1993.
Thi Minh Huyen Nguyen, Laurent Romary, Xuan Luong Vu : &quot;Une &#233;tude de cas pour l&#8217;&#233;tiquetage
</p>
<p>morpho-syntaxique de textes vietnamiens&quot; , 5e conf&#233;rence sur le traitement Automatique du Langage
Naturel (TALN2003), Batz-sur-Mer, 11-14 juin, 2003.
Patrick Paroubek et Martin Rajman : &quot;Etiquetage morpho-syntaxique.&quot; , Ing&#233;nierie des langues. pp.131-
150, Paris, HERMES Sciences Europe.
Jacques Vergne, Emmanuel Giguet: &quot;Regards th&#233;oriques sur le &quot;Tagging&quot; &quot; , 5e conf&#233;rence sur le
</p>
<p>traitement Automatique du Langage Naturel (TALN98), Paris, France, 10-12 juin, 1998.
Jean Veronis : &quot;Annotation automatique de corpus : panorama et &#233;tat de la technique&quot; , Ing&#233;nierie des
langues. pp.111-128. Paris, HERMES Sciences Europe.
</p>
<p>A. Yousfi : &quot;Introduction de la Vitesse d&#8217;&#233;locution dans un mod&#232;le de reconnaissance automatique de la
parole &quot; , Th&#232;se de doctorat, 19 juin 2001.</p>

</div></div>
</body></html>