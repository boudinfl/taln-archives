<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>L R Bahl</author>
<author>P F Brown</author>
<author>P V de Souza</author>
<author>R L Mercer</author>
</authors>
<title>Maximum mutual information estimation in hidden Markov model parameters for speech recognition &amp;quot;,</title>
<date>1986</date>
<booktitle>Proc. ICASSP,</booktitle>
<pages>49--52</pages>
<location>Tokyo,</location>
<marker>Bahl, Brown, de Souza, Mercer, 1986</marker>
<rawString>L.R. Bahl, P.F. Brown, P.V. de Souza &amp; R.L. Mercer : &amp;quot;Maximum mutual information estimation in hidden Markov model parameters for speech recognition &amp;quot;, Proc. ICASSP, pp. 49-52, Tokyo, 1986.</rawString>
</citation>
<citation valid="false">
<authors>
<author>L R Bahl</author>
<author>P F Brown</author>
<author>P V De Souza</author>
<author>R L</author>
</authors>
<title>Mercer : &amp;quot;Estimating HMM parameters so as to maximise speech recognition accuracy &amp;quot;,</title>
<journal>Research Report RC-13121, IBM TJ Watson Research Center,</journal>
<pages>9--10</pages>
<marker>Bahl, Brown, De Souza, L, </marker>
<rawString>L. R. Bahl, P. F. Brown, P.V De Souza and R. L. Mercer : &amp;quot;Estimating HMM parameters so as to maximise speech recognition accuracy &amp;quot;, Research Report RC-13121, IBM TJ Watson Research Center, 9/10/1987.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Baum</author>
</authors>
<title>An inequality and association maximization technique in statistical estimation for probabilistic functions of Markov processes &amp;quot;,</title>
<date>1972</date>
<journal>Inequality,</journal>
<volume>3</volume>
<contexts>
<context position="7859" citStr="Baum 1972" startWordPosition="1218" endWordPosition="1219">rentissage incorrect ou insuffisant diminue la performance du système d’étiquetage. Pour préparer le corpus d’apprentissage, on procède par approximations successives. Un premier corpus d’apprentissage, relativement court, permet d’étiqueter un corpus beaucoup plus important. Celui-ci est corrigé, ce qui permet de réestimer les probabilités, il sert donc à un second apprentissage, et ainsi de suite. En général il existe trois méthodes d’estimation de ces paramètres1 : • L’estimation par maximum de vraisemblance (Maximum Likelihood Estimation), elle est réalisée par l’algorithme de Baum-Welch [Baum 1972] ou l’algorithme de Viterbi [Celeux 92]. 1Pour plus de détaille sur ces formule voir [Yousfi 2001] • L’estimation par maximum a posteriori [John Arice]. • L’estimation par maximum d’information mutuel [Bahl et al 86,87][Kapadia 93]. Dans notre cas nous avons utilisé l’estimation par maximum de vraisemblance car c’est la plus utilisée et la plus facile à calculer. Alors si on prend un ensemble d’apprentissage R = {Ph1, ..., PhK}, constitué des phrases Ph1, ..., PhK étiquetées manuellement, les formules d’estimation des paramètres du modèle λ = (Π, A,B) s ∑∑ ont données par : K Phn aij = n=1 le</context>
</contexts>
<marker>Baum, 1972</marker>
<rawString>L. Baum : &amp;quot;An inequality and association maximization technique in statistical estimation for probabilistic functions of Markov processes &amp;quot;, Inequality, vol. 3, 1972.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Celux</author>
<author>J</author>
</authors>
<title>Clairambault :&amp;quot;Estimation de chaines de Markov cachées: méthodes et problèmes &amp;quot;, Journées thématiques CNRS sur les approches markoviennes en signal et images,</title>
<date>1992</date>
<marker>Celux, J, 1992</marker>
<rawString>G. Celux, J. Clairambault :&amp;quot;Estimation de chaines de Markov cachées: méthodes et problèmes &amp;quot;, Journées thématiques CNRS sur les approches markoviennes en signal et images, Septembre 1992.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean-Pierre Chanod</author>
<author>Pasi Tapanainen</author>
</authors>
<title>Tagging French - comparing a statistical and a constraintbased method&amp;quot;,</title>
<date></date>
<booktitle>Proceeding of the seventh Conference of the European Chapter of the Association for Computatinal Linguistics (EACL.95),</booktitle>
<pages>149--156</pages>
<location>Dublin,</location>
<marker>Chanod, Tapanainen, </marker>
<rawString>Jean-Pierre Chanod and Pasi Tapanainen : &amp;quot;Tagging French - comparing a statistical and a constraintbased method&amp;quot;, Proceeding of the seventh Conference of the European Chapter of the Association for Computatinal Linguistics (EACL.95), Dublin, Ireland. pp.149-156, 1995.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claude De</author>
</authors>
<title>Loupy : &amp;quot;La méthode détiquetage d’Eric Brill&amp;quot;. Revue T.A.L,</title>
<date>1995</date>
<booktitle>Proceedings of the third Conference on Applied Natural Language Processing,</booktitle>
<pages>152--155</pages>
<location>Trento,</location>
<marker>De, 1995</marker>
<rawString>Claude De Loupy : &amp;quot;La méthode détiquetage d’Eric Brill&amp;quot;. Revue T.A.L, 1995, Vol.36, nř 1-2, pp.37-46 Eric Brill : &amp;quot;A simple rule-based part of speech tagger&amp;quot;. Proceedings of the third Conference on Applied Natural Language Processing, Trento, Italy. pp.152-155. Avril 1992.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D R Fornay</author>
</authors>
<title>The Viterbi Algorithm &amp;quot;,</title>
<date>1973</date>
<booktitle>Proc. IEEE,</booktitle>
<volume>61</volume>
<pages>mai</pages>
<marker>Fornay, 1973</marker>
<rawString>Fornay D. R. : &amp;quot;The Viterbi Algorithm &amp;quot;, Proc. IEEE, vol. 61, n 3, mai 1973.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benoît Habert</author>
</authors>
<title>Adeline Nazarenko, André Salem : &amp;quot;Les linguistiques de corpus&amp;quot; , Armand colin / Masson.Paris,</title>
<date>1997</date>
<pages>511--540</pages>
<marker>Habert, 1997</marker>
<rawString>Benoît Habert, Adeline Nazarenko, André Salem : &amp;quot;Les linguistiques de corpus&amp;quot; , Armand colin / Masson.Paris, 1997. John Rice : &amp;quot;Mathematical Statistics and data analysis &amp;quot;, page 511-540.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kapadia</author>
<author>V Valtchev</author>
<author>S J Young</author>
</authors>
<title>MMI training for continuous phoneme recognition on the TIMIT database &amp;quot;,</title>
<date>1993</date>
<booktitle>Proc. ICASSP,</booktitle>
<pages>491--494</pages>
<location>Minneapolis,</location>
<marker>Kapadia, Valtchev, Young, 1993</marker>
<rawString>S. Kapadia, V. Valtchev &amp; S.J. Young : &amp;quot;MMI training for continuous phoneme recognition on the TIMIT database &amp;quot;, Proc. ICASSP, pp. II.491-494, Minneapolis, 1993.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thi Minh Huyen Nguyen</author>
<author>Laurent Romary</author>
</authors>
<title>Xuan Luong Vu : &amp;quot;Une étude de cas pour l’étiquetage morpho-syntaxique de textes vietnamiens&amp;quot; , 5e conférence sur le traitement Automatique du Langage Naturel</title>
<date>2003</date>
<pages>131--150</pages>
<location>Paris, HERMES Sciences Europe.</location>
<marker>Nguyen, Romary, 2003</marker>
<rawString>Thi Minh Huyen Nguyen, Laurent Romary, Xuan Luong Vu : &amp;quot;Une étude de cas pour l’étiquetage morpho-syntaxique de textes vietnamiens&amp;quot; , 5e conférence sur le traitement Automatique du Langage Naturel (TALN2003), Batz-sur-Mer, 11-14 juin, 2003. Patrick Paroubek et Martin Rajman : &amp;quot;Etiquetage morpho-syntaxique.&amp;quot; , Ingénierie des langues. pp.131-150, Paris, HERMES Sciences Europe.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacques Vergne</author>
</authors>
<title>Emmanuel Giguet: &amp;quot;Regards théoriques sur le &amp;quot;Tagging&amp;quot; &amp;quot; , 5e conférence sur le traitement Automatique du Langage Naturel (TALN98),</title>
<date>1998</date>
<pages>111--128</pages>
<location>Paris,</location>
<marker>Vergne, 1998</marker>
<rawString>Jacques Vergne, Emmanuel Giguet: &amp;quot;Regards théoriques sur le &amp;quot;Tagging&amp;quot; &amp;quot; , 5e conférence sur le traitement Automatique du Langage Naturel (TALN98), Paris, France, 10-12 juin, 1998. Jean Veronis : &amp;quot;Annotation automatique de corpus : panorama et état de la technique&amp;quot; , Ingénierie des langues. pp.111-128. Paris, HERMES Sciences Europe.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Yousfi</author>
</authors>
<title>Introduction de la Vitesse d’élocution dans un modèle de reconnaissance automatique de la parole &amp;quot; , Thèse de doctorat, 19 juin</title>
<date>2001</date>
<contexts>
<context position="7957" citStr="Yousfi 2001" startWordPosition="1234" endWordPosition="1235">r le corpus d’apprentissage, on procède par approximations successives. Un premier corpus d’apprentissage, relativement court, permet d’étiqueter un corpus beaucoup plus important. Celui-ci est corrigé, ce qui permet de réestimer les probabilités, il sert donc à un second apprentissage, et ainsi de suite. En général il existe trois méthodes d’estimation de ces paramètres1 : • L’estimation par maximum de vraisemblance (Maximum Likelihood Estimation), elle est réalisée par l’algorithme de Baum-Welch [Baum 1972] ou l’algorithme de Viterbi [Celeux 92]. 1Pour plus de détaille sur ces formule voir [Yousfi 2001] • L’estimation par maximum a posteriori [John Arice]. • L’estimation par maximum d’information mutuel [Bahl et al 86,87][Kapadia 93]. Dans notre cas nous avons utilisé l’estimation par maximum de vraisemblance car c’est la plus utilisée et la plus facile à calculer. Alors si on prend un ensemble d’apprentissage R = {Ph1, ..., PhK}, constitué des phrases Ph1, ..., PhK étiquetées manuellement, les formules d’estimation des paramètres du modèle λ = (Π, A,B) s ∑∑ ont données par : K Phn aij = n=1 le nombre de fois où la transition eti etj est dans la phrase K n=1∑le nombre de fois où l’état eti </context>
<context position="9171" citStr="Yousfi 2001" startWordPosition="1462" endWordPosition="1463"> atteint le long de la phrase PhnK pii = n=1 δ[l’étiquette eti est un état initial dans la phrase Phn] ∑ KK bit = n∑=1 le nombre de fois où le mot wt à l’étiquette eti le long de la phrase PhnK n=1 le nombre de fois où l’état eti est atteint le long de la phrase Phn avec : { 1 si l’événement x est vrai δ[x] = 0 sinon 5 Etiquetage automatique par algorithme de Viterbi Pour un calcul plus rapide du chemin optimal2 dans la formule (1) nous avons utilisé l’algorithme de Viterbi [For 73]. On note par : δt(etj) = max Pr(w1...wt, eti et 1 ...etit) i ...et1 it avec etit = etj . Cette formule devient [Yousfi 2001]: δt(etj) = max δt−1(eti).aij.bj(wt) eti On calcule cette formule pour toutes les valeurs t = 1, ..., T et j = 1, ..., N . Enfin le chemin optimal est obtenu à l’aide d’un calcul récursif sur cette formule. 6 Expérimentation 6.1 Données d’apprentissage Le travail expérimental a été réalisé en trois grandes étapes : 1) étape de définition du jeu d’étiquettes et de construction de corpus d’apprentissage. La définition de notre propre jeu d’étiquettes morpho-syntaxique a été particulièrement délicate, cette phase a été réalisée en collaboration avec des linguistes pour satisfaire au besoin des p</context>
</contexts>
<marker>Yousfi, 2001</marker>
<rawString>A. Yousfi : &amp;quot;Introduction de la Vitesse d’élocution dans un modèle de reconnaissance automatique de la parole &amp;quot; , Thèse de doctorat, 19 juin 2001.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>