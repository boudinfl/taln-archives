<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
"http://www.w3.org/TR/html4/loose.dtd">
<html><head><title>Une m&#233;thode pour la classification de signal de parole sur la caract&#233;ristique de nasalisation</title>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
</head>
<body>
<div style="page-break-before:always; page-break-after:always"><div><p>RECITAL 2005, Dourdan, 6&#8211;10 juin 2005
</p>
<p>Une m&#233;thode pour la classification de signal de parole
sur la caract&#233;ristique de nasalisation
</p>
<p>Luquet Pierre-Sylvain
GREYC - CNRS UMR 6072 - Universit&#233; de Caen
</p>
<p>Bd Mar&#233;chal Juin - F14032 Caen Cedex
psluquet@info.unicaen.fr
</p>
<p>Date de soutenance pr&#233;vue : d&#233;cembre 2005
</p>
<p>Mots-clefs &#8211; Keywords
</p>
<p>Phonologie, phon&#233;tique, classifieur, r&#233;seaux de neurones
Phonology, phonetic, classifier, neural nets
</p>
<p>R&#233;sum&#233; - Abstract
</p>
<p>Nous exposons ici une m&#233;thode permettant d&#8217;&#233;tudier la nature d&#8217;un signal de parole dans le
temps. Plus pr&#233;cis&#233;ment, nous nous int&#233;ressons &#224; la caract&#233;ristique de nasalisation du signal.
Ainsi nous cherchons &#224; savoir si &#224; un instant t le signal est nasalis&#233; ou oralis&#233;. Nous proc&#233;dons
par classification &#224; l&#8217;aide d&#8217;un r&#233;seau de neurones type perceptron multi-couches, apr&#232;s une
phase d&#8217;apprentissage supervis&#233;e. La classification, apr&#232;s segmentation du signal en fen&#234;tres,
nous permet d&#8217;associer &#224; chaque fen&#234;tre de signal une &#233;tiquette renseignant sur la nature du
signal.
</p>
<p>In this paper we expose a method that allows the study of the phonetic features of a speech
signal through time. More specifically, we focus on the nasal features of the signal. We try to
consider the signal as [+nasal] or [-nasal] at any given time. We proceed with a classifier system
based on a multilayer perceptron neural net. The classifier is trained on a hand tagged corpus.
The signal is tokenized into 30ms hamming windows. The classification process lets us tag each
window with information concerning the properties of its content.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Luquet Pierre-Sylvain
</p>
<p>1 Appuis th&#233;oriques
</p>
<p>La reconnaissance de la parole a, gr&#226;ce aux techniques Markoviennes, fait un bond qualitatif
&#233;norme ces derni&#232;res ann&#233;es. Les d&#233;codeurs acoustiques, tels que ceux d&#233;velopp&#233;s au LIMSI
(Lamel &amp; Gauvain, 1993), atteignent des taux de reconnaissance proches des 75%. Cependant,
les limitations restent nombreuses et la critique la plus largement formul&#233;e vis-&#224;-vis de ce type
de syst&#232;me est la quasi absence de connaissances sur le langage dans les mod&#232;les sous-jacents
(Plaut &amp; Kello, 1999). Les travaux actuels s&#8217;articulent autour de deux axes. Le premier s&#8217;in-
t&#233;resse &#224; l&#8217;am&#233;lioration des techniques de description du signal (Chetouani et al., 2002). Le
second est orient&#233; vers la production : acquisition de connaissances concernant les gestes arti-
culatoires des locuteurs (Vaxelaire et al., 2002), leurs influences sur le signal (Montagu, 2004),
et les processus cognitifs mis en jeu (Hawkins, 2003). Ces connaissances font l&#8217;objet de diff&#233;-
rentes &#233;tudes visant &#224; leur int&#233;gration dans les syst&#232;mes de reconnaissance automatique de la
parole (Wrench &amp; Richmond, 2000).
Nous d&#233;crivons dans ces lignes une approche sensiblement diff&#233;rente. Nous cherchons &#224; ap-
puyer une technique de d&#233;codage acoustico-phon&#233;tique sur la notion de diff&#233;rence. Saussure
affirme que &#171; dans la langue, il n&#8217;y a que des diff&#233;rences [. . .] sans termes positifs &#187; (Saus-
sure, 1986). Coursil reprend &#224; son compte cette notion dans (Coursil, 1992)1 et affirme &#224; son
tour &#171; Pour tout phon&#232;me x, il existe un phon&#232;me y tel que y = x &#224; une et une seule diff&#233;rence
cat&#233;gorique pr&#232;s &#187;. C&#8217;est &#224; partir de cette derni&#232;re affirmation qu&#8217;il construit la topique des
phon&#232;mes du fran&#231;ais contemporain2. Le but de la classification est de mettre en &#233;vidence cette
diff&#233;rence dans le signal. Notons enfin que la classification automatique d&#8217;un signal de parole
suivant un trait phon&#233;tique donn&#233; suppose que le phon&#232;me est une substance, hypoth&#232;se valid&#233;e
par la &#171; Dispersion-Focalisation Theory &#187; publi&#233;e dans (Schwartz et al., 1997).
Le trait de nasalit&#233;. On distingue dans le fran&#231;ais contemporain les phon&#232;mes nasaux des
phon&#232;mes oraux, le tableau 1 en pr&#233;sente la partition3. Du point de vue de la m&#233;canique arti-
culatoire, la nasalit&#233; est d&#233;crite comme une connexion du conduit vocal avec le conduit nasal
par le biais de l&#8217;abaissement du v&#233;lum. Les r&#233;percussions acoustiques de ce ph&#233;nom&#232;ne, sont
d&#233;crites par Jakobson dans (Jakobson, 1980) en s&#8217;appuyant sur Fant et Delattre. Pour Fant, les
consonnes nasales sont &#171; caract&#233;ris&#233;es par un spectre o&#249; F2 est faible ou bien absent &#187;4 ; Delattre
affine la description en pr&#233;cisant que pour les voyelles nasales, compar&#233;es aux orales, F1 perd
une bonne part de son intensit&#233; au profit de F2. Plus r&#233;cemment, Feng et Kotenkoff (Feng &amp;
Kotenkoff, 2004) ont men&#233; &#224; l&#8217;ICP5 des observations bas&#233;es sur une technique d&#8217;enregistrement
du locuteur en diff&#233;renciant les prises de son en provenance du conduit vocal et du conduit na-
sal. Ils ont constat&#233; que l&#8217;abaissement du v&#233;lum a deux effets distincts : pour le conduit vocal
le r&#233;tr&#233;cissement engendre le rapprochement des formants F3 et F4, et pour le conduit nasal
sa connexion entra&#238;ne un rayonnement au niveau des narines caract&#233;ris&#233; par une concentration
dans les basses fr&#233;quences et aux alentours de 3000 Hz.
</p>
<p>1Les travaux sur la phonologie de Coursil s&#8217;inscrivent dans un projet global d&#233;nomm&#233; ANADIA. On lira dans
(Mauger, 1999) l&#8217;une des extensions de ce projet.
</p>
<p>2Le format dans lequel cet article est accept&#233; ne me permet pas d&#8217;expliquer plus avant cette notion de topique.
N&#233;anmoins, je mets &#224; disposition de tout lecteur en faisant la demande une version &#233;tendue d&#233;crivant plus finement
celle-ci.
</p>
<p>3La notation employ&#233;e ici pour d&#233;signer les phon&#232;mes est le codage SAMPA (Speech As-
sessment Methods Phonetic Alphabet). Pour plus d&#8217;informations se reporter au site de l&#8217;UCL :
http://www.phon.ucl.ac.uk/home/sampa/home.htm
</p>
<p>4F1 et F2 d&#233;signent respectivement le premier et le second formants. Les formants sont des fr&#233;quences de
r&#233;sonance maximum de l&#8217;enveloppe spectrale du signal de la parole &#224; un instant donn&#233;.
</p>
<p>5Institut de la Communication Parl&#233;e - Grenoble</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Une m&#233;thode pour la classification de signal de parole
</p>
<p>nasal oral
voyelles /e&#8764;, o&#8764;, 9&#8764;, a&#8764;/ /u, y, i, E, e, O, o, 9, 2, @, a, A/
</p>
<p>consonnes /m, n, J/ /p, f, v, b, d, t, k, g, z, s, S, Z, R, l, w, H, j/
TAB. 1 &#8211; Nasal vs. oral
</p>
<p>2 Corpus
</p>
<p>2.1 Constitution
</p>
<p>Nous faisons ici l&#8217;hypoth&#232;se que le corpus pr&#233;sentant le moins de difficult&#233;s pour r&#233;aliser la
partition oralis&#233; vs. nasalis&#233; est constitu&#233; de paires minimales oral vs. nasal. De plus, nous nous
sommes concentr&#233;s sur les phon&#232;mes dont la production pouvait &#234;tre maintenue. Nous avons
retenu dans notre corpus les quatre paires oppositives suivantes : /o&#8764;/ - /o/, /e&#8764;/ - /E/, /9&#8764;/ - /9/
et /a&#8764;/ - /A/. Ces phon&#232;mes sont associ&#233;s aux mots prototypiques du tableau 2.
</p>
<p>Phon&#232;mes /o&#8764;/ /o/ /e&#8764;/ /E/ /9&#8764;/ /9/ /a&#8764;/ /A/
Prototype tronc trot bain baie un neuf pente p&#226;te
</p>
<p>TAB. 2 &#8211; Phon&#232;mes et mots prototypes
</p>
<p>Nous disposons aujourd&#8217;hui des r&#233;sultats sur 3 corpus6 de test monolocuteur (voir le tableau 3).
Le premier corpus, C1 est constitu&#233; d&#8217;une seule paire minimale (/o&#8764;/ &amp; /o/), dont la seule
variation est le trait de nasalisation (N). Les corpus C2 et C3 sont plus complexes : sur les 7
caract&#233;ristiques mises en jeu 5 varient. Pour les orales (/o/ &amp; /a/) les variations portent sur la
laxit&#233; (L), la compacit&#233; (C) et la b&#233;molisation (B) ; la hauteur (H) intervient en plus pour les
nasales (/o&#8764;/ &amp; /a&#8764;/)7.
</p>
<p>Phon&#232;mes Nb. Phon&#232;mes Nb. fen&#234;tres Maintenus Variations
C1 /o&#8764;/-/o/ 22 2250 oui N
C2 /o&#8764;, a&#8764;/-/o, a/ 20 2000 oui N, L, C, B, H
C3 /o&#8764;, a&#8764;/-/o, a/ 24 470 non N, L, C, B, H
</p>
<p>TAB. 3 &#8211; Corpus
</p>
<p>2.2 Param&#232;tres
</p>
<p>L&#8217;outil principalement utilis&#233; dans cette exp&#233;rience est le logiciel d&#8217;analyse acoustique PRAAT8.
Corpus. Nous travaillons en utilisant la technique classique de fen&#234;trage du signal. Chaque si-
gnal de parole &#224; analyser est segment&#233; en tranches de 30ms avec un d&#233;calage de 10ms. A chaque
</p>
<p>6Pour chacun de ces phon&#232;mes, il a &#233;t&#233; demand&#233; au locuteur de le prononcer dans un mot prototypique, puis
de le r&#233;p&#233;ter de fa&#231;on isol&#233;e. Cette fa&#231;on de proc&#233;der permet &#224; l&#8217;utilisateur de &#171; calibrer &#187; le phon&#232;me qu&#8217;il doit
ensuite prononcer isol&#233;ment. Nous demandons au locuteur de r&#233;p&#233;ter une dizaine de fois ce processus par couple
prototype/phon&#232;me.
</p>
<p>7Les quantit&#233;s du tableau 3 (nombre de phon&#232;mes et nombre de fen&#234;tres) donn&#233;es ici correspondent pour
chaque corpus aux versions de test. Les versions d&#8217;apprentissage sont du m&#234;me ordre de grandeur.
</p>
<p>8Pour plus d&#8217;informations voir la page web http://www.fon.hum.uva.nl/praat/</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Luquet Pierre-Sylvain
</p>
<p>fen&#234;tre est appliqu&#233;e une fonction de Hamming. Chaque tranche de signal fen&#234;tr&#233; constitue un
vecteur dont le nombre d&#8217;&#233;l&#233;ments est d&#233;pendant de la fr&#233;quence d&#8217;&#233;chantillonnage du signal
(662 &#233;chantillons par tranche pour du signal &#233;chantillonn&#233; &#224; 22kHz). Chaque vecteur est la-
bellis&#233; par sa caract&#233;ristique acoustique (&quot;nasal&quot; ou &quot;oral&quot;). Ces vecteurs sont concat&#233;n&#233;s en
matrices, qui selon le corpus servira soit &#224; l&#8217;apprentissage, soit &#224; la phase de test9.
Classifieur. Nous utilisons des r&#233;seaux de neurones type perceptron &#224; une couche cach&#233;e. L&#8217;en-
tr&#233;e du r&#233;seau comporte autant de cellules que nous avons de valeurs par vecteur de signal, soit
662 cellules. La sortie est compos&#233;e de deux cellules correspondant aux classes activables. La
couche cach&#233;e est compos&#233;e de 331 cellules. Lors des phases d&#8217;apprentissage l&#8217;&#233;valuation de
l&#8217;erreur est calcul&#233;e suivant la m&#233;thode minimum squared error.
</p>
<p>3 R&#233;sultats
</p>
<p>3.1 Variation restreinte
</p>
<p>Les r&#233;sultats du tableau 3 concernent le corpus C1 et ont &#233;t&#233; obtenus au terme d&#8217;un apprentis-
sage de 400 cycles. Les phon&#232;mes sont maintenus et la seule variation phonologique mise en
jeu est la nasalisation. Nous voyons ici que sur une quantit&#233; restreinte de corpus il est possible
de classifier le signal avec de bons r&#233;sultats. En effet nous obtenons un taux d&#8217;erreurs faible
(2, 8%), mais nous voyons surtout que le nombre de fen&#234;tres continues incorrectement classi-
fi&#233;es est tr&#232;s faible (8) en regard du nombre de fen&#234;tres par phon&#232;me (102). Le risque de mal
classifier un phon&#232;me est donc minime.
</p>
<p>fen&#234;tres 2252 fen&#234;tres par phon&#232;me 102
erreurs 63 groupes d&#8217;erreur 17
taux 2,8% erreurs par groupe 3, 71
erreurs cons&#233;cutives maximum 8
</p>
<p>TAB. 4 &#8211; R&#233;sultats C1
</p>
<p>3.2 Augmentation de la dissemblance
</p>
<p>Les r&#233;sultats donn&#233;s ici concernent le corpus C2. Le tableau 5 donne les r&#233;sultats obtenus pour
400 cycles d&#8217;apprentissage, tandis que le tableau 6 nous donne les r&#233;sultats au bout de 600
cycles. Comme pr&#233;c&#233;demment les phon&#232;mes sont maintenus mais plusieurs variations phono-
logiques sont ici mises en jeu (voire 2.1). La focalisation du classifieur sur la caract&#233;ristique de
nasalisation est donc rendue plus complexe en raison du bruit apport&#233; par les autres variations.
Cependant, les r&#233;sultats obtenus montrent qu&#8217;une classification est toujours possible. Avec 400
cycles (tableau 5), nous obtenons un taux d&#8217;erreurs qui reste faible (5, 3%). Le nombre de fe-
n&#234;tres contigu&#235;s incorrectement classifi&#233;es l&#8217;est aussi (12 fen&#234;tres mal classifi&#233;es). N&#233;anmoins,
si nous augmentons d&#8217;un tiers le nombre de cycles (tableau 6), le taux d&#8217;erreurs retombe &#224; 2, 3%.
</p>
<p>9Dans les deux cas, les valeurs des &#233;chantillons sont d&#233;cal&#233;es et mises &#224; l&#8217;&#233;chelle pour &#234;tre dans le domaine
de d&#233;finition de notre classifieur. Les valeurs d&#8217;origine varient dans l&#8217;intervalle [&#8722;1, 1]. Nous les r&#233;duisons d&#8217;un
facteur 1/2 puis les d&#233;calons de 1 pour qu&#8217;elles soient comprises dans l&#8217;intervalle d&#8217;entr&#233;e du classifieur : [0, 1].</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Une m&#233;thode pour la classification de signal de parole
</p>
<p>fen&#234;tres 1996 fen&#234;tres par phon&#232;me 100
erreurs 106 groupes d&#8217;erreur 47
taux 5,3% erreurs par groupe 2, 3
erreurs cons&#233;cutives maximum 12
</p>
<p>TAB. 5 &#8211; R&#233;sultats C2 - Apprentissage : 400 cycles
</p>
<p>fen&#234;tres 1996 fen&#234;tres par phon&#232;me 100
erreurs 47 groupes d&#8217;erreur 16
taux 2,3% erreurs par groupe 2, 9
erreurs cons&#233;cutives maximum 9
</p>
<p>TAB. 6 &#8211; R&#233;sultats C2 - Apprentissage : 600 cycles
</p>
<p>3.3 Phon&#232;mes non maintenus
</p>
<p>L&#8217;exp&#233;rience men&#233;e sur le corpus C3 est similaire &#224; l&#8217;exp&#233;rience pr&#233;c&#233;dente, mais concerne des
phon&#232;mes non maintenus. Les r&#233;sultats obtenus (tableau 7 et 8) sont nettement en retrait, mais
restent n&#233;anmoins tr&#232;s int&#233;ressants. Au terme d&#8217;un apprentissage de 300 cycles, nous observons
un taux d&#8217;erreur de 20% que nous pouvons r&#233;duire &#224; 15, 8% au terme de 600 cycles d&#8217;apprentis-
sage (soit une r&#233;duction de ce taux de 21, 5%). En revanche, le doublement du nombre de cycles
d&#8217;apprentissage n&#8217;apporte rien ici en terme de r&#233;duction du nombre d&#8217;erreurs contigu&#235;s (7 fe-
n&#234;tres mal classifi&#233;es10). N&#233;anmoins ce nombre reste acceptable, dans le cas d&#8217;une strat&#233;gie de
classification winner-takes-all dans la mesure o&#249; un phon&#232;me compte en moyenne 20 fen&#234;tres.
Notons que la taille de notre corpus d&#8217;apprentissage (412 fen&#234;tres) pose ici un probl&#232;me ; le
nombre de patrons &#233;tiquet&#233;s limite la capacit&#233; de classification. Enfin, un dernier cycle long
d&#8217;apprentissage (2000 cycles) ne nous a pas permis d&#8217;am&#233;liorer sensiblement le taux d&#8217;erreurs
et a &#233;galement confirm&#233; qu&#8217;au del&#224; de 600 cycles, la r&#233;duction de l&#8217;erreur est faible pour un co&#251;t
tr&#232;s &#233;lev&#233; ; dans notre cas le nombre de cycles a &#233;t&#233; plus que tripl&#233; pour un gain de 2 erreurs
seulement sur le corpus de test.
</p>
<p>fen&#234;tres 469 fen&#234;tres par phon&#232;me 20
erreurs 94 groupes d&#8217;erreur 37
taux 20,0% erreurs par groupe 2, 5
erreurs cons&#233;cutives maximum 7
</p>
<p>TAB. 7 &#8211; R&#233;sultats C3 - Apprentissage : 300 cycles
</p>
<p>4 Perspectives et conclusion
</p>
<p>Les r&#233;sultats pr&#233;sent&#233;s dans cet article sont prometteurs, cependant certains aspects sont &#224; ap-
profondir. D&#8217;autre types de descripteurs sont envisag&#233;s : techniques d&#8217;extraction de type MFCC
(Mel Frequency Cepstral Coefficients), LPC (Linear Predictive Coding) ou plus encore PLP
(Perceptual Linear Predictive coding). Par ailleurs, la limite en terme de fr&#233;quence d&#8217;&#233;chan-
tillonnage en de&#231;&#224; de laquelle l&#8217;apprentissage n&#8217;est plus r&#233;alisable n&#8217;est pas connue. Qu&#8217;en
</p>
<p>10Le nombre donn&#233; ici correspond au nombre maximal de fen&#234;tres contigu&#235;s mal classifi&#233;es dans un phon&#232;me.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Luquet Pierre-Sylvain
</p>
<p>fen&#234;tres 469 fen&#234;tres par phon&#232;me 20
erreurs 74 groupes d&#8217;erreur 30
taux 15,8% erreurs par groupe 2, 5
erreurs cons&#233;cutives maximum 7
</p>
<p>TAB. 8 &#8211; R&#233;sultats C3 - Apprentissage : 600 cycles
</p>
<p>est-il d&#8217;un signal de qualit&#233; t&#233;l&#233;phonique &#233;chantillonn&#233; &#224; 8kHz ?
Nous envisageons &#233;galement d&#8217;augmenter la complexit&#233; du corpus : nombre de locuteurs et
nombre de phon&#232;mes pr&#233;sents. L&#8217;augmentation du nombre de locuteurs a pour but de tester
l&#8217;ind&#233;pendance de l&#8217;apprentissage du classifieur. Pour valider notre m&#233;thode sur du signal de
parole continue, une nouvelle s&#233;rie d&#8217;exp&#233;riences est envisag&#233;e. L&#8217;augmentation du nombre de
phon&#232;mes doit permettre de multiplier les caract&#233;ristiques prises en consid&#233;ration.
En outre, nous faisons l&#8217;hypoth&#232;se que le croisement de r&#233;sultats issus de plusieurs classifieurs
(avec un apprentissage sur des cat&#233;gories phon&#233;tiques diff&#233;rentes) permettra de situer le signal
dans l&#8217;espace topique et de d&#233;terminer ainsi la classe phon&#233;tique &#224; laquelle il appartient.
</p>
<p>R&#233;f&#233;rences
CHETOUANI M., GAS B. &amp; ZARADER J. (2002). Coop&#233;ration entre codeurs neuro-pr&#233;dictifs pour
l&#8217;extraction de caract&#233;ristiques en reconnaissance de phon&#232;mes. In Reconnaissances des formes et intel-
ligence artificielle.
COURSIL J. (1992). Essai d&#8217;intelligence artificielle et de linguistique g&#233;n&#233;rale. PhD thesis, Universit&#233;
de Caen.
FENG &amp; KOTENKOFF (2004). Vers un nouveau mod&#232;le acoustique des nasales bas&#233; sur l&#8217;enregistrement
bouche - nez s&#233;par&#233;. In Journ&#233;es d&#8217;Etude sur la Parole.
HAWKINS S. (2003). Roles and representations of systematic fine phonetic detail in speech understan-
ding. Journal of Phonetics.
JAKOBSON R. (1980). La charpente phonique du langage. Paris : Editions de Minuit.
LAMEL L. &amp; GAUVAIN J. (1993). High performance speaker-independent phone recognition using
cdhmm. In European Conference on Speech Communication and Technology.
MAUGER S. (1999). L&#8217;Interpr&#233;tation des Messages &#201;nigmatiques. Essai de S&#233;mantique et de Traitement
Automatique des Langues. PhD thesis, Universit&#233; de Caen.
MONTAGU J. (2004). Les sons sous-jacents aux voyelles nasales en fran&#231;ais parisien : indices perceptifs
des changements. In Journ&#233;es d&#8217;&#201;tude sur la Parole, p. 385&#8211;388.
PLAUT D. C. &amp; KELLO C. T. (1999). The Emergence of Language, chapter The Emergence of Phono-
logy from the Interplay of Speech Comprehension and Production : A Distributed Connectionist. Law-
rence Erlbaum Assoc : Mahwah.
SAUSSURE F. (1986). Cours de linguistique g&#233;n&#233;rale. Paris : Mauro Payot.
SCHWARTZ J.-L., BO&#203; L.-J., VALL&#201;E N. &amp; ABRY C. (1997). The dispersion-focalization theory of
vowel systems. Journal of Phonetics.
VAXELAIRE B., FERBACH-HECKER V. &amp; SOCK R. (2002). La perception auditive de gestes vocaliques
anticipatoires. In Journ&#233;es d&#8217;Etude sur la Parole.
WRENCH A. A. &amp; RICHMOND K. (2000). Continuous speech recognition using articulatory data. In
International Conference on Spoken Language Processing.</p>

</div></div>
</body></html>