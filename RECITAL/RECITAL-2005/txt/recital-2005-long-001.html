<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
"http://www.w3.org/TR/html4/loose.dtd">
<html><head><title>How semantic is Latent Semantic Analysis?</title>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
</head>
<body>
<div style="page-break-before:always; page-break-after:always"><div><p>R&#201;CITAL 2005, Dourdan, 6-10 juin 2005 
</p>
<p>How semantic is Latent Semantic Analysis? 
</p>
<p>Tonio Wandmacher 
</p>
<p>Laboratoire d&#8217;Informatique &#8211; E.A. 2101 &#8211; Equipe BdTln 
Universit&#233; Fran&#231;ois Rabelais de Tours 
</p>
<p>Site de Blois &#8211; 3 place Jean Jaur&#232;s &#8211; F-41000 Blois 
tonio.wandmacher@etu.univ-tours.fr 
</p>
<p> 
</p>
<p>Mots-clefs - Keywords 
Analyse de la s&#233;mantique latente, analyse de collocations, relations lexicales, s&#233;mantique 
computationelle. 
</p>
<p>Latent Semantic Analysis, Collocation Analysis, lexical relations, computational semantics. 
</p>
<p>R&#233;sum&#233; - Abstract 
Au cours des dix derni&#232;res ann&#233;es, l'analyse de la s&#233;mantique latente (LSA) a &#233;t&#233; utilis&#233;e dans 
de nombreuses approches TAL avec parfois de remarquables succ&#232;s. Cependant, ses capacit&#233;s 
&#224; exprimer des ressemblances s&#233;mantiques n&#8217;ont pas &#233;t&#233; r&#233;ellement recherch&#233;es de fa&#231;on 
syst&#233;matique. C&#8217;est l&#8217;objectif de ce travail, o&#249; la LSA est appliqu&#233;e &#224; un corpus de textes de 
langue courante (journal allemand). Les relations lexicales entre un mot et ses termes les plus 
proches sont analys&#233;s pour un test de vocabulaire. Ces r&#233;sultats sont alors compar&#233;s avec les 
r&#233;sultats obtenus lors d&#8217;une analyse des collocations. 
</p>
<p>In the past decade, Latent Semantic Analysis (LSA) was used in many NLP approaches with 
sometimes remarkable success. However, its abilities to express semantic relatedness were not 
yet systematically investigated. This is the aim of our work, where LSA is applied to a general 
text corpus (German newspaper), and for a test vocabulary, the lexical relations between a test 
word and its closest neighbours are analysed. These results are compared to the results from a 
collocation analysis. 
</p>
<p>1 General introduction 
In its beginnings, Latent Semantic Analysis aimed at improving the vector space model in 
information retrieval. Its abilities to enhance retrieval performance were remarkable; results 
could be improved by up to 30%, compared to a standard vector space technique (Dumais, 
1995). It was further found that LSA was able to retrieve documents that did not even share a 
single word with the query but were rather semantically related. </p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Tonio Wandmacher 
</p>
<p>This finding was the headstone for many subsequent researches. It was tried to apply the LSA 
approach to other areas, such as automated evaluation of student essays (Landauer et al., 
1997) or automated summarization (Wade-Stein &amp; Kintsch, 2003). In (Landauer &amp; Dumais, 
1997), even an LSA-based theory of knowledge acquisition was presented. 
</p>
<p>In these works, many claims on the analytic power of LSA were made. It is asserted that LSA 
does not return superficial events such as co-occurrence relations, but is able to describe 
semantic similarity between two words.1 The extracted word relations are referred to as latent, 
hidden or deep2, however, none of these papers addresses the nature of this deepness. LSA is 
called &#8220;semantic&#8221;, but a thorough evaluation of its abilities to extract the semantics of a word 
or a phrase is missing.3 One work that takes a little step in this direction, was done by 
Landauer &amp; Dumais (1997). They use LSA-based similarities to solve a synonym test taken 
from the TOEFL (Test Of English as a Foreign Language) They found that the abilities of 
LSA to assign the right synonym (out of 4 test words) to the target word are comparable to 
those of human non-native speakers of English (mean LSA: 64,4%; mean humans: 64,5%). 
</p>
<p>However, this result can only be seen as a first indication for the capacity of LSA; it is neither 
a systematic assessment, nor a comparison to similar techniques. This is what we try to 
achieve in the following. Our aim is therefore not improvement, but evaluation and a better 
understanding of the method. 
</p>
<p>2 Presentation of LSA 
LSA, as presented by (Deerwester et al. 1990) and others, is based on the vector space model 
of information retrieval (Salton &amp; McGill, 1983). First, a given corpus of text is transformed 
into a term&#215;context-matrix, displaying the occurrences of each word in each context. A 
context can be only a 2-word window, a sentence, a paragraph or a full text. For LSA, a 
paragraph window is normally assumed (cf. (Dumais, 1995), (Landauer et al, 1997)).  
</p>
<p>In a second step, this matrix is weighted by one of the weighting methods used in IR (c.f. 
(Salton &amp; McGill, 1983)). For LSA, a log-entropy scheme showed the best results (Dumais, 
1990). The decisive step in the LSA process is then a Singular Value Decomposition (SVD) of 
the weighted matrix. Thereby the original matrix A is decomposed as follows: 
</p>
<p>SVD(A) = U &#0;  VT    (1) 
</p>
<p>The matrices U and V consist of the eigenvectors of the columns and rows of A. &#0;  is a 
diagonal matrix containing in descending order the singular values of A. By only keeping the k 
</p>
<p>                                                 
</p>
<p>1
 Cf. (Wade-Stein &amp; Kintsch, 2003), p. 10: &#8220;LSA does not reflect the co-occurrence among words but rather 
</p>
<p>their semantic relatedness.&#8221; 
</p>
<p>2
 Cf. (Landauer et al., 1998), p. 4: &#8220;It is important to note from the start that the similarity estimates derived by 
</p>
<p>LSA are not simple contiguity frequencies, co-occurrence counts, or correlations in usage, but depend on a 
powerful mathematical analysis that is capable of correctly inferring much deeper relations.&#8221; 
</p>
<p>3
 The &#8216;latency&#8217; of LSA was indeed assessed by Wiemer-Hastings (1999). </p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>How semantic is LSA? 
</p>
<p>strongest (k usually being around 300) singular values and remultiplying &#0; k with either U or V, 
one can construct a so-called semantic space for the terms or the contexts, respectively. Each 
term or each context then corresponds to a vector of k dimensions, whose distance to others 
can be compared by a standard vector distance measure. In most LSA approaches the cosine 
measure is used. By calculating the cosine of the angle between one term vector and all the 
others, a ranked list of next neighbours can be obtained for a given word. From the LSA point 
of view, these neighbours should be semantically related to the test word. 
</p>
<p>3 Method 
To assess the abilities of LSA to generate semantic similarity, we applied it to a large corpus 
of German newspaper text. We used a random sample of 120.000 paragraphs (app. 20 mio. 
words) of the Tageszeitung (TAZ) from 1989 to 1998, which was stoplisted for frequent words 
and lemmatized by the DMOR package (Schiller, 1995). Words having a corpus frequency of 
less than 5 were also removed. This reduced the vocabulary size from 385.344 to 63.651 
types. This was necessary, since the calculation of the SVD is heavily constrained by 
complexity matters. 
</p>
<p>After transforming the corpus into a term&#215;context matrix (having the size 63.651 &#215; 120.000), 
we applied a log-entropy weighting scheme. Using Michael Berry&#8217;s GTP package v. 3.0 for 
Linux, we calculated the SVD for the above matrix up to 400 dimensions. To find the optimal 
factor k, we conducted some preliminary tests with various dimensionalities (250 &#8211; 400). A k 
of 309 gave the best results (in terms of the percentage of meaningful relations), even though 
the results for each of the samples were very close. 
</p>
<p>For a random sample of 400 words (nouns, verbs and adjectives only), their 20 next 
neighbours (= words having the highest cosine score with the centroid, see 2.) were extracted. 
We considered a fixed number of neighbours, since the usage of a threshold distance (e.g. cos 
= 0,5) proved not to be practical (the cluster size varied strongly). 
</p>
<p>The relations between the centroid (test word) and each of the 20 neighbours were then 
manually categorized in one of eight relation classes. The classes were the following:  
</p>
<p>&#8226; Synonymy 
&#8226; Antonymy 
&#8226; Hypo-/Hypernymy 
&#8226; Co-Hyponymy 
&#8226; Mero-/Holonymy 
&#8226; Loose association 
&#8226; Morphological relation 
&#8226; Erroneous relation 
 
</p>
<p>The notion of semanticity described by this classification can be questioned. However, our 
selection of semantic relations seems to be widely accepted in lexical semantics (cf. (Cruse, 
1986)) and precise definitions exist to determine if a relation holds between two words X and 
Y (e.g. for meronymy: X IS-PART-OF Y). The same is true for the class of morphological 
</p>
<p>truly semantic relations </p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Tonio Wandmacher 
</p>
<p>relations. A derivational or inflectional relation between two terms can be recognized easily 
most of the time. 
</p>
<p>Still, we admit that our classification is neither exhaustive, nor always clear-cut. Especially 
the class of &#8220;loose association&#8221; is rather intuitive. It was assumed as a collection class for all 
term pairs that were not related by definition of a semantic or a morphological relation, but 
still were somehow connected. Typical examples for this class might be &#8216;Flugzeug&#8217; 
(&#8216;airplane&#8217;) / &#8216;landen&#8217; (&#8216;to land&#8217;) or &#8216;Katze&#8217; (&#8216;cat&#8217;) / &#8216;Milch&#8217; (&#8216;milk&#8217;).  
</p>
<p>To balance out doubtful cases, we set the size of our test sample sufficiently large (20 
neighbours for 400 words = 8000 categorized relations4) and had the classification task done 
independently by two German native speakers (including the author). 
</p>
<p>For each of the neighbours, additional information, such as its corpus frequency, context 
frequency and entropy value, was also determined. 
</p>
<p>4 Results 
</p>
<p>4.1 Quantitative Analysis 
Results were calculated for the first 5, 10, 15 and 20 neighbours, respectively. As the fractions 
for each of the semantic classes were all quite low (0-5%), only the total of semantic relations 
is displayed here: 
</p>
<p> 
</p>
<p>Figure 1: Percentages of relation classes resulting from LSA for a sample of 400 test words. 
</p>
<p>                                                 
</p>
<p>4
 For a sample size n = 400 the 95% confidence interval is maximally &#177; 4,9. </p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>How semantic is LSA? 
</p>
<p>As the number of neighbours under consideration rises, the results get generally worse. 
Considering only the first five neighbours for every test word, we find nearly a third of 
erroneous relations (32,8%). Almost half of them are loose associations, whereas the truly 
semantic relations make up only 15%. Moreover, 5% of morphologically related word pairs 
were found. 
</p>
<p>When we consider the relations for 20 neighbours, the part of erroneous relations rises to 
nearly 50%, whereas the class of associations falls to 38%. Only 10% of the relations can be 
classified as semantic and app. 2% as morphological. 
</p>
<p>Splitting up the sample into parts of speech, we get the following picture: 
</p>
<p>Relation class Nouns Adjectives Verbs 
Semantic relations: 15,1% 7,8% 3,8% 
Morphological relations: 2,4% 1,8% 2,1% 
Associations: 39,3% 30,5% 41,0% 
Erroneous relations: 43,2% 59,9% 53,1% 
</p>
<p>Table 1: Percentages of relations for the three parts of speech. 
</p>
<p>Table 1 shows a clear distinction. Nouns have far more meaningful relations (56,8%) than 
adjectives (40,1%) or verbs (46,9%). The difference becomes even clearer if only the class of 
semantic relations is considered. Opposing verbs and adjectives, another remarkable 
difference can be found: Verbs have much more (10,5%) associations than adjectives, but 
only less than half of semantic relations. 
</p>
<p>4.2 Qualitative Analysis 
If one opposes the words with the lowest and the highest fractions of meaningful relations, a 
difference in usage of the two groups can be observed: 
</p>
<p>Lowest fractions (0-5%) Highest fractions (95-100%) 
Ansehen (&#8218;image&#8217;)  nat&#252;rlich (&#8218;natural&#8217;) Mediziner (&#8218;health prof.&#8217;) singen (&#8218;to sing&#8217;) 
Aufbruch (&#8218;breakup&#8217;)  teilen (&#8218;divide&#8217;) Reporter (&#8218;reporter&#8217;) sterben (&#8218;to die&#8217;) 
Beispiel (&#8218;example&#8217;) zahlreich (&#8218;numerous&#8217;) Therapie (&#8218;therapy&#8217;) studieren (&#8218;to study&#8217;) 
Kasten (&#8218;box&#8217;)  zumuten (&#8218;to expect of&#8217;) Wohnraum (&#8218;living space&#8217;) Luftwaffe (&#8218;air force&#8217;) 
Umstand (&#8218;circumstance&#8217;)  &#252;berstehen (&#8218;to overcome&#8217;) Zuh&#246;rer (&#8218;auditor&#8217;) Malerei (&#8218;painting&#8217;) 
Unsinn (&#8218;nonsense&#8217;)  Aufl&#246;sung (&#8218;resolution&#8217;) deportieren (&#8218;to deport&#8217;) Religion (&#8218;religion&#8217;) 
aufrecht (&#8218;upright&#8217;)  R&#252;cksicht (&#8218;consideration&#8217;) gesund (&#8218;healthy&#8217;) Uniform (&#8218;uniform&#8217;) 
automatisch (&#8218;automatic)  bescheren (&#8218;to bring&#8217;) kochen (&#8218;to cook&#8217;) Wirtschaft (&#8218;economy&#8217;) 
glatt (&#8218;flat&#8217;/&#8218;smooth&#8217;)  denken (&#8218;to think&#8217;) lernen (&#8218;to learn&#8217;) lesen (&#8218;to read&#8217;) 
intensiv (&#8218;intensive&#8217;)  einfallen (&#8218;to occur&#8217;) operieren (&#8218;to do a surgery&#8217;) orthodox (&#8218;orthodox&#8217;) 
</p>
<p>Table 2: Words of the sample having the lowest and highest fractions of meaningful 
relations with the test word. 
</p>
<p>Regarding the two groups shown in table 2, it appears that the words with the worst results 
can occur in every context. Words like &#8216;Beispiel&#8217; (&#8216;example&#8217;), &#8216;Unsinn&#8217; (&#8216;nonsense&#8217;) or 
&#8216;denken&#8217; (&#8216;to think&#8217;) are not connected to a certain theme or a typical context. On the other </p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Tonio Wandmacher 
</p>
<p>hand, the words having the highest scores are rather specific. This group comprises terms such 
as &#8216;Mediziner&#8217; (&#8216;health professional&#8217;), &#8216;Malerei&#8217; (&#8216;painting&#8217;) or &#8216;kochen&#8217; (&#8216;to cook&#8217;). These 
terms have a typical context; they are bound to a particular topic. 
</p>
<p>To get a clearer picture of this kind of specificity, it seems reasonable to further analyse the 
distribution of the words in the corpus. From the research on information retrieval it is known 
that specific terms are better predictors and get therefore a higher weight (Sp&#228;rck-Jones, 
1972), (Salton &amp; McGill, 1983). The relevant values for the weighting schemes in IR are 
normally the term frequency (tf), the corpus frequency (cf) and the context (or document) 
frequency (df). A combination of these values forms the base for nearly all weighting schemes 
of the so-called tf*idf-family (s. (Salton &amp; McGill, 1983)). Could these values be good 
predictors for our purposes? 
</p>
<p>We calculated the correlation between several of these values (as well as some combinations) 
and the fraction of meaningful relations among 20 next neighbours. The results were 
disappointing. None of the pure frequencies had a significant correlation with the fraction of 
meaningful relations. While trying out several combinations of the values, we found only one 
that showed a slight correlation (Pearson-Coefficient = 0,32, significance level &lt;0,001), 
namely the quotient of cf and df. 
In addition, we calculated the correlation between the mean distance of the 20 neighbours and 
the percentage of meaningful relations for the whole test set. We observed a medium 
correlation (Pearson-Coeff. = 0,56 at a significance level of &lt;0,001). We therefore can 
conclude that medium distance and relation quality are related, although not too strongly. 
</p>
<p>5 Comparison with collocation analysis 
</p>
<p>5.1 Method 
</p>
<p>To obtain a contrastive example, we did the same experiment using collocation analysis (CA). 
We hereby used a formula presented by Quasthoff &amp; Wolff (1998), (2002). They calculate the 
collocative significance between two words A and B as follows: 
</p>
<p>!loglog),( k
n
CCk
</p>
<p>n
CCBAsig BABA +&#8901;&#8722;=  
</p>
<p>where CA (CB) is a context, in which A (B) occurs, n is the amount of all contexts and k the 
number of all contexts containing A and B.5 
</p>
<p>To ensure comparability, we used the same corpus and did the same pre-processing (i.e. 
stoplisting, removal of low frequency words etc.). We then calculated for our sample of 400 
test words the 20 words having the highest collocative significance with the test word. This 
gave us again 400 word clusters, for which every relation was categorized as above. 
                                                 
</p>
<p>5
 This measure is obviously related to the one given by Dunning (1993). Both measures appear to give rather 
</p>
<p>similar results (cf. (Quasthoff &amp; Wolff, 2002)). 
</p>
<p>(2) </p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>How semantic is LSA? 
</p>
<p>5.2 Results 
</p>
<p>Figure 2 shows the fractions of the different relation classes obtained by collocation analysis: 
</p>
<p> 
</p>
<p>Figure 2: Percentages of relation classes resulting from CA for a sample of 400 test words. 
</p>
<p>Comparing figures 1 and 2, the results seem quite close. The differences between the fractions 
for the meaningful classes do not exceed 4,5%. This is remarkable, keeping in mind that the 
collocation analysis exploits only co-occurrence relations in the text. 
</p>
<p>As a whole, the percentage of meaningful relations is a little lower for CA than for LSA. Still 
we find 8% (20 NN) to 12% (5 NN) of semantic relations, not much less regarding the LSA 
results (10%-15%). Still, all LSA scores were significantly higher than the CA scores in a 
Student&#8217;s T test (significance level: &lt;0,001). 
</p>
<p>Regarding the words and their neighbours, we get a similar picture as with LSA: terms being 
bound to a particular theme get better results than those that are context-independent. 
Moreover, the results of CA and LSA show a high correlation (Pearson-Coeff. = 0,72 at a 
significance level of &lt;0,001) for our sample. It seems therefore that LSA and CA make use of 
the same properties of a word and its context. 
</p>
<p>However, one difference can be observed with regard to ambiguous words. CA returns 
neighbours that still belong to both meanings, LSA however seems to mask out one of them. 
This behaviour can be seen in the following examples for &#8216;Bach&#8217; (Meanings: J. S. Bach, the 
composer / &#8216;creek&#8217;) and &#8216;Schlange&#8217; (Meanings: &#8216;queue&#8217; / &#8216;snake&#8217;): </p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Tonio Wandmacher 
</p>
<p> 
</p>
<p> Next neighbours for &#8216;Bach&#8217; (&#8216;creek&#8217;/J.S. Bach) Next neighbours for &#8216;Schlange&#8217; (&#8216;queue&#8217;/&#8217;snake&#8217;) 
Rank LSA results CA results LSA results CA results 
1. Musik (&#8218;music&#8217;) Sebastian (&#8218;Sebastian&#8217;) stehen (&#8218;to stand&#8217;) stehen (&#8218;to stand&#8217;) 
2. Beethoven (&#8218;Beethoven&#8217;) Johann (&#8218;John&#8217;) warten (&#8218;to wait&#8217;) warten (&#8218;to wait&#8217;) 
3. musizieren (&#8218;to make 
</p>
<p>music&#8217;) 
Musik (&#8218;music&#8217;) Schild (&#8218;sign&#8217;) Reptil (&#8218;reptile&#8217;) 
</p>
<p>4. klanglich (&#8218;sonorical&#8217;) Emanuel (&#8218;Emanuel&#8217;) Wartezeit (&#8218;waiting time&#8217;) lang (&#8218;long&#8217;) 
5. musikalisch (&#8218;musical&#8217;) Elvira (&#8218;Elvira&#8217;) Kaufhaus (&#8218;department 
</p>
<p>store&#8217;) 
bilden (&#8218;to form&#8217;) 
</p>
<p>6. Klang (&#8218;sound&#8217;) Mozart (&#8218;Mozart&#8217;) Supermarkt 
(&#8218;supermarket&#8217;) 
</p>
<p>Schalter (&#8218;counter&#8217;) 
</p>
<p>7. Gesang (&#8218;chant&#8217;) Artist (&#8218;artist&#8217;) Vesna (&#8218;name&#8217;) Kaninchen (&#8218;rabbit&#8217;) 
8. rhythmisch (&#8218;rhythmical&#8217;) runtergehen (&#8218;to flow  
</p>
<p>down&#8217;) 
lang (&#8218;long&#8217;) Buchstabe (&#8218;letter&#8217;) 
</p>
<p>9. komponieren (&#8218;to 
compose&#8217;) 
</p>
<p>verunreinigen (&#8218;to 
pollute&#8217;) 
</p>
<p>Stra&#223;enrand (&#8218;roadside&#8217;) Mensch (&#8218;human&#8217;) 
</p>
<p>10. Improvisation 
(&#8218;improvisation&#8217;) 
</p>
<p>Brahms (&#8218;Brahms&#8217;) B&#228;ckerei (&#8218;bakery&#8217;) giftig (&#8218;poisonous&#8217;) 
</p>
<p>11. Mozart (&#8218;Mozart&#8217;) Fluss (&#8218;river&#8217;) tags&#252;ber (&#8218;in the day&#8217;) auftauchen (&#8218;to emerge&#8217;) 
12. virtuos (&#8218;&#8217;virtuoso) Ton (&#8218;sound/note&#8217;) Matte (&#8218;mat&#8217;) einreihen (&#8218;to queue&#8217;) 
13. Komposition 
</p>
<p>(&#8218;composition&#8217;) 
hinunter (&#8218;down&#8217;) Stau (&#8218;holdup&#8217;) Warteschlange (&#8218;queue&#8217;) 
</p>
<p>14. Rhythmus (&#8218;rhythm&#8217;) Gew&#228;sser (&#8218;water&#8217;) Warteschlange (&#8218;queue&#8217;) lange (&#8218;long&#8217;) 
15. Saxophon (&#8218;saxophone&#8217;) Geige (&#8218;violin&#8217;) Einlass (&#8218;entry&#8217;) Australien (&#8218;Australia&#8217;) 
16. Geige (&#8218;violin&#8217;) Oboe (&#8218;oboe&#8217;) Brot (&#8218;bread&#8217;) Stau (&#8218;holdup&#8217;) 
17. Komponist (&#8218;composer&#8217;) Flussufer (&#8218;river bank&#8217;) Greenfield (&#8218;Greenfield&#8217;) Tag (&#8218;day&#8217;) 
18. akustisch (&#8218;acustical&#8217;) Auff&#252;hrung 
</p>
<p>(&#8218;performance&#8217;) 
lange (&#8218;long&#8217;) K&#228;fig (&#8218;cage&#8217;) 
</p>
<p>19. klassisch (&#8218;classical&#8217;) rauschen (&#8218;rush&#8217;) Auslage (&#8218;display&#8217;) &#246;ffnen (&#8218;to open&#8217;) 
20. Cello (&#8218;cello&#8217;) Philipp (&#8218;Philipp&#8217;) Mittelpunkt (&#8218;center&#8217;) Wartezeit (&#8218;waiting time&#8217;) 
</p>
<p>Table 3: Two examples of ambiguous words and their next neighbours. The 
neighbours belonging to the prominent meaning are in blank, the ones of the non-
prominent meaning are in black. Terms that cannot be assigned are shaded in grey. 
</p>
<p>The difference in the analyses is obvious: in both examples, LSA generates neighbours of the 
prominent meaning only, whereas the NN produced by CA are of both domains. However, 
this is only a first observation; we did not yet assess this masking-out property of LSA in a 
systematic way. It should be subject to further research. 
</p>
<p>6 Discussion 
To take up our initial question: how semantic is LSA? The conclusion that we draw from our 
results, is: not as much as its name might suggest. The fractions of truly semantic relations 
were not very high (10% at the 20-NN level), and a big part (38% at 20 NN) of the relations, 
however, could rather be described as associative. These words are conceptually related, but 
not necessarily in a narrow semantic or morphological sense. 
</p>
<p>The biggest part however, nearly half of the relations generated at the 20-NN level, are 
erroneous, i.e. there is no apparent relation between the test word and its neighbour. </p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>How semantic is LSA? 
</p>
<p>Comparing the LSA approach to other procedures exploiting co-occurrence information, one 
reason for the high percentage of error relations may be found: Techniques such as HAL 
(Lund &amp; Burgess, 1996) or the approach by Rapp (2002), (2003) use a co-occurrence window 
of a few (3-40) words only. LSA however relies on full paragraphs (average length in our 
case: 102 words). And even though a paragraph can be regarded as a semantically coherent 
unit, many of the inter-word relations in it may already be too weak. This may cause arbitrary 
relations. 
</p>
<p>Another questionable point about LSA arises from the modelling itself. The term-by-context-
matrix is extremely sparse. In our experiments, the matrix had only maximally 0,08% nonzero 
elements. This is by itself of course not harmful, but recalling that the complexity of the SVD 
process constrains the overall size of the matrix, a different modelling seems more reasonable. 
Again, the approaches of Lund and Burgess (1996) and Rapp (2003) may give the answer: A 
term-by-term-matrix6 can model the same amount of text in a smaller and less sparse format. 
Using this kind of matrix, much larger corpora can be used for the analysis; Rapp (2003) was 
able to analyse the full British National Corpus comprising more than 100 million words.  
</p>
<p>With respect to the results obtained from a collocation analysis of the same corpus, the LSA 
results do not show big differences. In general, they are significantly better, but none of the 
classes differs more than 4,5% from the CA. This is surprising, since a technique like CA 
relies on co-occurrence information only and does not make use of complex matrix 
calculations. 
</p>
<p>Still, the two analyses seem to show a different behaviour with regard to ambiguous words. 
Whereas CA finds for a given ambiguous test word neighbours from both conceptual 
domains, LSA seems to mask out one of the meanings. 
</p>
<p>We hope to have given a deeper understanding of what LSA can and cannot do. Regarding our 
results, it is not much more semantic than a simple technique like CA, and some of its 
modelling aspects, such as the optimal context size or the kind of co-occurrence matrix still 
leave space for improvement and further research. Particularly the effects of the SVD on word 
similarity should be further investigated, before LSA is used as a general tool to derive 
semantic relations from text. 
</p>
<p>References 
CRUSE, D.A. (1986), Lexical Semantics, Cambridge University Press, Cambridge. 
</p>
<p>DEERWESTER, S. C., DUMAIS, S.T., LANDAUER, T.K., FURNAS, G.W., HARSHMAN, R.A. 
(1990), &#8220;Indexing by Latent Semantic Analysis&#8221;, Journal of the American Society of 
Information Science, 41 (6), pp. 391 &#8211; 407. 
DUMAIS, S. (1990), Enhancing Performance in Latent Semantic Indexing, Technical Report 
TM-ARH-017527, Bellcore. 
                                                 
</p>
<p>6
 A term-by-term matrix for a vocabulary V is of the size |V|2 and reflects the frequency of co-occurrence of two 
</p>
<p>terms within a certain text window (e.g. &#177; 5 words). </p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Tonio Wandmacher 
</p>
<p>DUMAIS, S. T. (1995), &#8220;Latent Semantic Indexing (LSI): TREC-3 Report&#8221;, in D. Harman 
(Ed.), Proceedings of the 3rd Text REtrieval Conference (TREC-3), Vol. 500-226, pp. 219-
230, NIST Special Publication. 
</p>
<p>DUNNING, T. (1993), &#8220;Accurate Methods for the Statistics of Surprise and Coincidence&#8221;, 
Computational Linguistics 19(1), pp. 61-74. 
</p>
<p>LANDAUER, T. K. und DUMAIS, S. T. (1997), &#8220;A solution to Plato's problem: The Latent 
Semantic Analysis theory of the acquisition, induction, and representation of knowledge&#8221;, 
Psychological Review 104, pp. 211-240. 
</p>
<p>LANDAUER, T. K., LAHAM, D., REHDER, B., und SCHREINER, M. E. (1997), &#8220;How well can 
passage meaning be derived without using word order? A comparison of Latent Semantic 
Analysis and humans&#8221;, in M. G. Shafto und P. Langley (Eds.), Proceedings of the 19th annual 
meeting of the Cognitive Science Society, pp. 412-417, Erlbaum, Mawhwah, NJ. 
LANDAUER, T. K., FOLTZ, P. W., und LAHAM, D. (1998), &#8220;Introduction to Latent Semantic 
Analysis&#8221;, Discourse Processes 25, pp. 259-284. 
</p>
<p>LUND, K. and BURGESS, C. (1996), &#8220;Producing high-dimensional semantic spaces from lexical 
co-occurrence&#8221;, Behaviour Research Methods, Instruments and Computers 28(2), pp. 159-
165. 
</p>
<p>QUASTHOFF, U. (1998), &#8222;Deutscher Wortschatz im Internet&#8220;, in Proceedings des LDV-Forum 
2/98. 
</p>
<p>QUASTHOFF, U. und WOLFF, C. (2002), &#8220;The Poisson Collocation Measure and its 
Applications&#8221;, in Proceedings of the 2nd International Workshop on Computational 
Approaches to Collocations, Wien. 
</p>
<p>RAPP, R. (2002), &#8220;The Computation of Word Associations: Comparing Syntagmatic and 
Paradigmatic Approaches&#8221;, Proceedings of the COLING 02, Taipei. 
RAPP, R. (2003), &#8220;Word Sense Discovery Based on Sense Descriptor Dissimilarity&#8221;, 
Proceedings of the 9th Machine Translation Summit, New Orleans. 
SALTON, G. und MCGILL, M. (1983), Introduction to Modern Information Retrieval, 
McGraw-Hill, New York. 
</p>
<p>SCHILLER, A. (1995), DMOR: Benutzeranleitung, Technical report, IMS Stuttgart, Draft. 
</p>
<p>SP&#196;RCK-JONES, K. (1972), &#8220;A statistical interpretation of term specificity and its application 
in retrieval&#8221;, Journal of Documentation 28(1), pp. 11-20. 
WADE-STEIN, D. und KINTSCH, E. (2003), Summary Street: Interactive Computer Support for 
Writing, Technical report, University of Colorado. 
</p>
<p>WIEMER-HASTINGS, P. (1999) &#8220;How latent is Latent Semantic Analysis?&#8221;, Proceedings of the 
16th International Joint Conference on Artificial Intelligence, Stockholm, Sweden, Aug 1999, 
pp. 932-937. San Francisco: Morgan Kaufmann. </p>

</div></div>
</body></html>