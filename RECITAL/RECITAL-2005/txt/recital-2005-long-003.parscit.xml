<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J P Benzécri</author>
</authors>
<title>L’analyse des données - tome 2 : l’analyse des correspondances, Éditions Bordas. Beust P.</title>
<date>1980</date>
<contexts>
<context position="13920" citStr="Benzécri, 1980" startWordPosition="2057" endWordPosition="2059">bre total de mots du texte. Cette méthode est particulièrement intéressante lorsque la taille des textes du corpus varie significativement. Les vecteurs obtenus à l’issue de l’étape de comptage prennent place dans des espaces de dimensions égales aux nombres de thèmes définis par l’utilisateur9. Pour visualiser ces vecteurs sur des cartes en 2 ou 3 dimensions, il faut réaliser une projection de ces vecteurs. Pour cela, nous proposons plusieurs méthodes d’analyse des données dont l’Analyse en Composante Principales (ACP) (Bouroche et Saporta, 1980) et l’Analyse Factorielle des Correspondances (Benzécri, 1980). À l’issue de cette étape de projection, nous proposons aux utilisateurs des regrou9Dans l’exemple précédent, les vecteurs représentant les textes prennent place dans un espace à 4 dimensions. Thibault ROY pements automatiques des textes sur les cartes. Pour cela, nous avons intégré une méthode de catégorisation, appelée Catégorisation Hiérarchique Ascendante (Bouroche et Saporta, 1980). Afin de mettre en évidence les résultats produits par l’enchaînement de ces différents traitements, nous présentons dans la partie suivante des exemples de cartes thématiques construites par ProxiDocs à parti</context>
</contexts>
<marker>Benzécri, 1980</marker>
<rawString>Benzécri J.P. (1980), L’analyse des données - tome 2 : l’analyse des correspondances, Éditions Bordas. Beust P. (2002), Un outil de coloriage de corpus pour la représentation de thèmes, Actes des 6èmes Journées internationales de l’Analyse statistique de Données Textuelles.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bouroche J M et Saporta G</author>
</authors>
<title>L’analyse des données, Collection Que sais-je ?,</title>
<date>1980</date>
<publisher>PUF.</publisher>
<marker>G, 1980</marker>
<rawString>Bouroche J.M. et Saporta G. (1980) L’analyse des données, Collection Que sais-je ?, PUF.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Chung</author>
<author>Chen H et Numaker J F Jr</author>
</authors>
<title>Business Intelligence Explorer : A Knowledge Map Framework for Discovering Business Intelligence on the Web, Actes de la 36ème HICSS.</title>
<date>2002</date>
<marker>Chung, Jr, 2002</marker>
<rawString>Chung W., Chen H. et Numaker J.F.Jr. (2002) Business Intelligence Explorer : A Knowledge Map Framework for Discovering Business Intelligence on the Web, Actes de la 36ème HICSS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lelu A et Aubin S</author>
</authors>
<title>Vers un environnement complet de synthèse statistique de contenus textuels, Présentation au séminaire Association pour la mesure des sciences et des techniques du 13/11/2001.</title>
<date>2001</date>
<marker>S, 2001</marker>
<rawString>Lelu A. et Aubin S. (2001) Vers un environnement complet de synthèse statistique de contenus textuels, Présentation au séminaire Association pour la mesure des sciences et des techniques du 13/11/2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Mokrane</author>
<author>R Arezki</author>
<author>Dray G et Poncelet P</author>
</authors>
<title>Cartographie automatique du contenu d’un corpus de documents textuels, Actes des 7èmes Journées internationales de l’Analyse statistique de Données Textuelles,</title>
<date>2004</date>
<pages>816--823</pages>
<contexts>
<context position="5228" citStr="Mokrane et al., 2004" startWordPosition="766" endWordPosition="769">ts. Selon le type d’analyse réalisée, il est alors possible d’observer des similarités et des différences de styles, de thèmes, de mises en forme entre documents d’un même ensemble. Depuis quelques années, des outils d’analyse textuelle exploitent une technique de visualisation appelée cartographie. À la manière d’une carte routière mettant en évidence des villes et des routes les reliant, une carte d’un ensemble de données textuelles met en évidence des proximités et des liens entre entités textuelles, tels des mots, des textes, etc. Dans une tâche d’extraction d’information, les auteurs de (Mokrane et al., 2004) propose d’utiliser une technique de cartographie afin de visualiser les liens entre les principaux termes présents 1http://www.info.unicaen.fr/~troy/proxidocs 2http://www.greyc.unicaen.fr/island/logiciel Une plate-forme logicielle dédiée à la cartographie thématique de corpus dans un ensemble de dépêches d’agences de presse. Depuis 2001, les deux métamoteurs de recherche cartographiques KartOO (Chung et al., 2002) et MapStan (Spinat, 2002) sont disponibles sur le Web3. En réponse à une requête de l’utilisateur, ces deux outils retournent des cartes représentant les sites proposés en réponse à</context>
</contexts>
<marker>Mokrane, Arezki, P, 2004</marker>
<rawString>Mokrane A., Arezki R., Dray G. et Poncelet P. (2004) Cartographie automatique du contenu d’un corpus de documents textuels, Actes des 7èmes Journées internationales de l’Analyse statistique de Données Textuelles, pages 816-823.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Nicolle</author>
</authors>
<title>L’expérimentation et l’intelligence artificielle, Intellectica, numéro 22,</title>
<date>1996</date>
<booktitle>Association pour la Recherche Cognitive (ARC).</booktitle>
<pages>9--19</pages>
<contexts>
<context position="3461" citStr="Nicolle, 1996" startWordPosition="512" endWordPosition="514">thèmes entre textes. Ces cartes sont construites à partir de thèmes choisis et définis par l’utilisateur en fonction de la tâche qu’il souhaite accomplir. En ce sens, c’est un système anthropocentré tel que le définit (Thlivitis, 1998) : son exécution n’est pas guidée par des ressources propres, les traitements réalisés sont personnalisés et intégralement conditionnés par les besoins et les choix de l’utilisateur. La plate-forme ProxiDocs est open-source, développée en Java et disponible avec sa documentation sur le Web1. C’est un système développé à la façon d’un logiciel d’étude au sens de (Nicolle, 1996), c’est-à-dire qu’il est conçu dans le but de vérifier des hypothèses sur les langues en les expérimentant sur du matériau textuel attesté. ProxiDocs fait partie d’un ensemble de logiciels d’étude en constante évolution dédiée à l’analyse linguistique informatisée de corpus de documents électroniques2 développés au sein de l’équipe ISLanD du laboratoire GREYC. Dans cet article, nous présentons tout d’abord des outils utilisant des techniques de cartographie afin d’accéder aux informations contenues dans des collections de documents. Ensuite, nous abordons tout particulièrement la plate-forme P</context>
</contexts>
<marker>Nicolle, 1996</marker>
<rawString>Nicolle A. (1996), L’expérimentation et l’intelligence artificielle, Intellectica, numéro 22, pages 9-19, Association pour la Recherche Cognitive (ARC).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Nicolle</author>
<author>Beust P et Perlerin V</author>
</authors>
<title>Un analogue de la mémoire pour un agent logiciel interactif,</title>
<date>2002</date>
<booktitle>In Cognito, numéro 21,</booktitle>
<pages>37--66</pages>
<marker>Nicolle, V, 2002</marker>
<rawString>Nicolle A., Beust P. et Perlerin V. (2002), Un analogue de la mémoire pour un agent logiciel interactif, In Cognito, numéro 21, pages 37-66.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Perlerin</author>
</authors>
<title>MemLabor, un environnement de création, de gestion et de manipulation de corpus de textes,</title>
<date>2002</date>
<journal>Actes de TALN / RECITAL</journal>
<pages>507--516</pages>
<contexts>
<context position="10121" citStr="Perlerin, 2002" startWordPosition="1484" endWordPosition="1485">ots simples ou mots composés) lui étant associées du point de vue de l’utilisateur. Afin de simplifier la phase de construction des thèmes, l’utilisateur n’indique que les formes lemmatisées des lexies. La plate-forme intégre une étape de génération de formes fléchies à l’aide d’une base de données lexicales. Un utilisateur peut par exemple associer les lexies suivantes au thème de l’aviation : avion, appareil, vol, pilote, pilotage, piloter, passager, Boeing, Air France, décollage, etc. Deux logiciels sont proposés à l’utilisateur afin de l’aider à construire ses thèmes : – l’outil Memlabor (Perlerin, 2002) permettant une analyse statistique des graphies répétées d’un corpus. En exploitant le principe de cohésion lexicale, MemLabor se fonde sur l’hypothèse que plus une graphie (hors mots d’un anti-dictionnaire contenant par exemple les mots grammaticaux) est répétée dans le corpus, plus elle est susceptible de pouvoir être associée à l’un des thèmes présents dans le corpus (Perlerin, 2004, p. 141). En présentant à l’utilisateur une liste des graphies classées par ordre décroissant de fréquence d’apparition, le logiciel permet une première assistance à l’extraction de graphies intéressantes pour </context>
</contexts>
<marker>Perlerin, 2002</marker>
<rawString>Perlerin V. (2002), MemLabor, un environnement de création, de gestion et de manipulation de corpus de textes, Actes de TALN / RECITAL 2002, pages 507 à 516.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Perlerin</author>
</authors>
<title>Sémantique légère pour le document : assistance personnalisée pour l’accès au document et l’exploration de son contenu, Thèse d’informatique de l’Université de Caen.</title>
<date>2004</date>
<contexts>
<context position="10510" citStr="Perlerin, 2004" startWordPosition="1543" endWordPosition="1544">ation : avion, appareil, vol, pilote, pilotage, piloter, passager, Boeing, Air France, décollage, etc. Deux logiciels sont proposés à l’utilisateur afin de l’aider à construire ses thèmes : – l’outil Memlabor (Perlerin, 2002) permettant une analyse statistique des graphies répétées d’un corpus. En exploitant le principe de cohésion lexicale, MemLabor se fonde sur l’hypothèse que plus une graphie (hors mots d’un anti-dictionnaire contenant par exemple les mots grammaticaux) est répétée dans le corpus, plus elle est susceptible de pouvoir être associée à l’un des thèmes présents dans le corpus (Perlerin, 2004, p. 141). En présentant à l’utilisateur une liste des graphies classées par ordre décroissant de fréquence d’apparition, le logiciel permet une première assistance à l’extraction de graphies intéressantes pour l’utilisateur selon sa tâche à partir d’un corpus. – l’outil ThemeEditor (Beust, 2002) permettant de composer des graphies en lexies et de les rassembler en thèmes. Ce rassemblement est non exclusif, une lexie pouvant être associée à plusieurs thèmes. Les ressources ainsi constituées sont projetées sur le corpus initial par une annotation XML. Un principe de surlignage avec différentes </context>
<context position="24546" citStr="Perlerin, 2004" startWordPosition="3708" endWordPosition="3709">dés dans les textes de cet ensemble, d’observer des groupes de textes abordant des thèmes proches et de visualiser l’évolution des sujets abordés dans ces articles au fil du temps. Plusieurs améliorations de la plate-forme ProxiDocs sont actuellement envisagées. D’un point de vue théorique, nous souhaitons intégrer un modèle de représentation des thèmes beaucoup plus fin que celui utilisé jusqu’à présent (dépassant les simples listes de lexies). Le modèle de représentation lexicale envisagé (intitulé LUCIA) est expérimenté depuis plusieurs années au sein de notre équipe (Nicolle et al., 2002; Perlerin, 2004). L’intégration de ce modèle à notre plate-forme permettrait à l’utilisateur de préciser et de structurer les lexies relevant des thématiques de son choix en précisant, pour chacune d’elles, les significations qu’ils jugent pertinentes et appropriées à la tâche qu’il vise. Les cartes ainsi produites devraient révéler des informations plus précises sur le corpus analysé et surtout plus en rapport avec le point de vue de l’utilisateur ou du groupe d’utilisateurs destinataires des cartes. D’un point de vue applicatif, nous avons commencé le développement de deux composants : – le métamoteur de re</context>
</contexts>
<marker>Perlerin, 2004</marker>
<rawString>Perlerin V. (2004), Sémantique légère pour le document : assistance personnalisée pour l’accès au document et l’exploration de son contenu, Thèse d’informatique de l’Université de Caen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pichon R et Sébillot P</author>
</authors>
<title>Différencier les sens des mots à l’aide du thème et du contexte de leurs occurrences : une expérience, Actes de TALN</title>
<date>1999</date>
<pages>279--288</pages>
<marker>P, 1999</marker>
<rawString>Pichon R. et Sébillot P. (1999), Différencier les sens des mots à l’aide du thème et du contexte de leurs occurrences : une expérience, Actes de TALN 1999, pages 279-288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roy T et Beust P</author>
</authors>
<title>ProxiDocs, un outil de cartographie et de catégorisation thématique de corpus, Actes des 7èmes Journées internationales de l’Analyse statistique de Données Textuelles,</title>
<date>2004</date>
<pages>978--987</pages>
<marker>P, 2004</marker>
<rawString>Roy T. et Beust P. (2004), ProxiDocs, un outil de cartographie et de catégorisation thématique de corpus, Actes des 7èmes Journées internationales de l’Analyse statistique de Données Textuelles, pages 978-987.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Spinat</author>
</authors>
<title>Pourquoi intégrer des outils de cartographie au sein des systèmes d’information de l’entreprise ?, Colloque Cartographie de l’Information,</title>
<date>2002</date>
<location>Paris.</location>
<contexts>
<context position="5672" citStr="Spinat, 2002" startWordPosition="822" endWordPosition="823">ce des proximités et des liens entre entités textuelles, tels des mots, des textes, etc. Dans une tâche d’extraction d’information, les auteurs de (Mokrane et al., 2004) propose d’utiliser une technique de cartographie afin de visualiser les liens entre les principaux termes présents 1http://www.info.unicaen.fr/~troy/proxidocs 2http://www.greyc.unicaen.fr/island/logiciel Une plate-forme logicielle dédiée à la cartographie thématique de corpus dans un ensemble de dépêches d’agences de presse. Depuis 2001, les deux métamoteurs de recherche cartographiques KartOO (Chung et al., 2002) et MapStan (Spinat, 2002) sont disponibles sur le Web3. En réponse à une requête de l’utilisateur, ces deux outils retournent des cartes représentant les sites proposés en réponse à cette requête. Les sites jugés similaires par le système sont alors situés à proximité sur les cartes et il est ainsi possible de distinguer les grandes catégories d’informations proposées en réponse à la requête de l’utilisateur. Pour une tâche de parcours rapide d’un ensemble documentaire, le logiciel NeuroNav (Lelu et Aubin, 2001) de la société Diatopie4 présente sur une carte des groupes de documents. Les différents groupes déterminés </context>
</contexts>
<marker>Spinat, 2002</marker>
<rawString>Spinat E. (2002), Pourquoi intégrer des outils de cartographie au sein des systèmes d’information de l’entreprise ?, Colloque Cartographie de l’Information, Paris.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Thlivitis</author>
</authors>
<title>Sémantique interprétative intertextuelle : assistance anthropocentrée à la compré-hension des textes, Thèse d’informatique de l’Université de Rennes I.</title>
<date>1998</date>
<contexts>
<context position="3082" citStr="Thlivitis, 1998" startWordPosition="454" endWordPosition="455">e analyse importante et délicate. ProxiDocs a pour objectif d’aider ses utilisateurs dans de telles situations en leur fournissant des représentations graphiques (que nous appelons des cartes thématiques) d’un corpus de textes donné. Les cartes construites mettent en évidence la répartition des thèmes au sein des textes du corpus et révèlent des proximités et des différences de thèmes entre textes. Ces cartes sont construites à partir de thèmes choisis et définis par l’utilisateur en fonction de la tâche qu’il souhaite accomplir. En ce sens, c’est un système anthropocentré tel que le définit (Thlivitis, 1998) : son exécution n’est pas guidée par des ressources propres, les traitements réalisés sont personnalisés et intégralement conditionnés par les besoins et les choix de l’utilisateur. La plate-forme ProxiDocs est open-source, développée en Java et disponible avec sa documentation sur le Web1. C’est un système développé à la façon d’un logiciel d’étude au sens de (Nicolle, 1996), c’est-à-dire qu’il est conçu dans le but de vérifier des hypothèses sur les langues en les expérimentant sur du matériau textuel attesté. ProxiDocs fait partie d’un ensemble de logiciels d’étude en constante évolution d</context>
</contexts>
<marker>Thlivitis, 1998</marker>
<rawString>Thlivitis T. (1998), Sémantique interprétative intertextuelle : assistance anthropocentrée à la compré-hension des textes, Thèse d’informatique de l’Université de Rennes I.</rawString>
</citation>
<citation valid="true">
<title>Scalable Vector Graphics</title>
<date>2001</date>
<location>(SVG), http://www.w3.org/TR/SVG/.</location>
<marker>2001</marker>
<rawString>W3C (2001), Scalable Vector Graphics (SVG), http://www.w3.org/TR/SVG/.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>