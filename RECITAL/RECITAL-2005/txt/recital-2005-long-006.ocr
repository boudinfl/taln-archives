RECITAL 2005, Dourdan, 6-10 juin 2005

A la découverte de la polysémie des spéciﬁcités
du francais technique

Ann Bertels

ILT — K.U.Leuven
Dekenstraat 6 — B-3000 LEUVEN (Belgique)
ann.bertels@ilt.kuleuven.ac.be

Mots-clefs — Keywords
sémantique lexicale, langue spécialisée, spéciﬁcités, polysémie, cooccurrences

lexical semantics, language for speciﬁc purposes (LSP), keywords, polysemy, co-occurrences

Résumé — Abstract

Cet article décrit l’analyse sémantique des spécificités dans le domaine technique des
machines-outils pour l’usinage des métaux. Le but de cette étude est de vériﬁer si et dans
quelle mesure les spéciﬁcités dans ce domaine sont monosémiques ou polysémiques. Les
spéciﬁcités (situées dans un continuum de spéciﬁcité) seront identiﬁées avec la KeyWords
Method en comparant le corpus d’analyse a un corpus de référence. Elles feront ensuite
l’objet d’une analyse sémantique automatisée a partir du recouvrement des cooccurrences des
cooccurrences, afin d’établir le continuum de monosémie. Les travaux de recherche étant en
cours, nous présenterons des résultats préliminaires de cette double analyse.

This article discusses a semantic analysis of pivotal terms (keywords) in the domain of
machining terminology in French. Building on corpus data, the investigation attempts to ﬁnd
out whether, and to what extent, the keywords are polysemous. In order to identify the most
typical words of the typicality continuum, the KeyWords Method will be used to compare the
technical corpus with a reference corpus. The monosemy continuum will be implemented in
terms of degree of overlap between the co-occurrents of the co-occurrents of the keywords.
We present some preliminary results of work in progress.

1 Introduction et question de recherche

Cet article s’inscrit dans le cadre d’une these de doctorat sur la sémantique du vocabulaire
spéciﬁque d’un corpus de francais technique. Comme le corpus d’analyse reléve du domaine

Ann Bertels

technique des machines-outils pour l’usinage des métaux, l’analyse sémantique porte sur les
, . . , 1 , . . ,
spec1fic1tes d’une langue spec1al1see.

Dans la langue spécialisée, les besoins communicatifs requierent plus de précision, ce que la
terminologie traditionnelle déﬁnit comme l’univocité, la monoréférentialité et la monosémie
des unités terminologiques de la langue spécialisée. La terminologie traditionnelle
prescriptive et normative adopte une approche onomasiologique par domaine. Récemment, la
monosémie et l’univocité de la langue spécialisée ont été remises en question par la Théorie
Communicative de la Terminologie (Cabré, 1998, 2000), par la socioterminologie (Gaudin,
1993) et par la terminologie socio-cognitive (Temmerman, 1997). Les termes font partie
intégrante de la langue naturelle, mais véhiculent des connaissances spécialisées (Lerat,
1995). Les partisans de la terminologie descriptive rejettent la dichotomie entre la langue
générale et la langue spécialisée et adoptent une approche sémasiologique et linguistique,
basée sur l’étude de corpus de textes spécialisés (Condamines & Rebeyrolles, 1997).

Pour quantiﬁer la these monosémiste de la terminologie traditionnelle, nous nous proposons
de la reformuler en une question de recherche opérationnelle et mesurable : << Y a-t-il une
corrélation entre, d’une part, le continuum de spécificité et, d’autre part, le continuum de
monosémie (continuum de sens) ? >> L’hypothese de recherche avancée pose que,
contrairement a la these traditionnelle, les mots (les plus) spéciﬁques du corpus technique ne
sont pas nécessairement (les plus) monosémiques. L’analyse se propose donc de verifier la
polysémie des mots du corpus technique d’analyse, p.ex. le mot broche (1) << partie toumante
d’une machine-outil qui porte un outil ou une piece a usiner >> et (2) << outil servant a usiner
des pieces métalliques >>. A cet effet, ces mots sont ordonnés en fonction de leur spécificité et
situés sur une échelle de spéciﬁcité allant des mots les plus spéciﬁques aux mots les moins
spéciﬁques, mais comprenant toujours des spéciﬁcités statistiquement significatives du
corpus technique. Un deuxieme classement situe les mémes mots sur une échelle de
monosémie, a partir d’une analyse des cooccurrences de deuxieme ordre, c’est-a-dire les
cooccurrences des cooccurrences. La question de recherche principale (corrélation entre le
degré de spéciﬁcité et le degré de monosémie) sera complétée par des questions de recherche
secondaires faisant intervenir les facteurs inﬂuant sur le degré de monosémie, notamment la
fréquence et la classe lexicale. Une analyse de regression multiple permettra de verifier
l’impact des variables indépendantes (spéciﬁcité, fréquence, classe lexicale, etc.) sur le degré
de monosémie.

2 Corpus technique d’analyse et corpus de référence

Le corpus technique d’analyse est constitué de textes techniques du domaine des machines-
outils pour l’usinage des métaux et comprend environ 1.760.000 mots. Le corpus a été
étiqueté et lemmatisé par Cordial 7 Analyseur et consiste en 4 sous-corpus, datant de 1998 a
2001 : revues techniques électroniques (800.000) et fiches techniques (300.000) trouvées sur
Internet, normes ISO et directives (300.000) et quatre manuels numérisés (360.000). Les
textes se situent a différents niveaux de normalisation et de vulgarisation, s’adressant tant a
des professionnels (revues et ﬁches) qu’a des étudiants (manuels). Afin de pouvoir déterminer

1 Nous adoptons le terme << spe'cificite's » pour designer les mots les plus spe'cifiques et caractérisliques du

corpus d’analyse, indépendamment de la me’thode u11'lise’e (calcul des spe'cificite's vs. KeyWords Method).

A la découverte de la pob/sémie des Spéciﬁcités duﬁangais technique

les spéciﬁcités du corpus technique, il est complété par un corpus de référence de langue
générale. Celui-ci est constitué d’articles journalistiques du journal Le Monde (janvier —
septembre 1998). 11 a également été lemmatisé et comprend environ 15.300.000 mots.

Les ﬁchiers générés par Cordial se composent de trois colonnes, avec un mot par ligne: (1) la
forme ﬂéchie ou forme graphique, (2) le lemme ou forme canonique et (3) le code Cordial,
comparable a un POS-tag (Part-Of-Speech) indiquant la classe lexicale. Dans les fichiers
texte, nous avons corrigé quelques fautes de frappe. Les fichiers lemmatisés ont également
fait l’objet d’un nettoyage, a savoir quelques regroupements (p.ex. lemmes avec et sans point
Fig./Fig et lemmes avec et sans trait d’union) et la correction des erreurs de lemmatisation
(p.ex. machines-outils sous le lemme machine-outil). Ces opérations de nettoyage ont été
effectuées, tant pour le corpus d’analyse technique que pour le corpus de reference.

3 Approche méthodologique : spéciﬁcités et polysémie

Comme la recherche porte sur la question de savoir s’il y a une corrélation entre le continuum
de spécificité et le continuum de monosémie, la réponse et l’analyse linguistique qui en
découle requierent une approche méthodologique double. Il faut d’une part le calcul des
spécificités et d’autre part une mesure pour déterminer le degré de monosémie. Ces
spécificités se situent, non seulement au niveau des unités simples, p.ex. ﬂaisage, commande,
mais également au niveau des unités polylexicales, p.ex. commande numérique.

3.1 Spéciﬁcités

La recherche en langue spécialisée prend généralement comme point de départ l’identification
des spéciﬁcités, c’est-a-dire des mots spéciﬁques qui caractérisent le corpus spécialisé et qui
le différencient d’un corpus de langue générale. Les spéciﬁcités ne sont pas les mots les plus
fréquents de ce corpus, mais les mots les plus représentatifs. Du point de vue relatif, ces mots
ﬁgurent de facon signiﬁcative plus fréquemment dans le corpus de langue spécialisée que
dans un corpus de langue générale. Aﬁn de déterminer les spécificités, les fréquences dans le
corpus spécialisé sont comparées aux fréquences dans un corpus de référence, compte tenu de
la taille des deux corpus, ce qui revient a comparer la fréquence observée (corpus d’analyse) a
la fréquence attendue (corpus de référence). S’il y a une difference entre la fréquence
observée et la fréquence attendue, il faut vérifier si elle est statistiquement signiﬁcative. A cet
effet, deux méthodologies sont désormais disponibles : le calcul des spécificités (Lafon, 1984)
implémenté dans le logiciel Lexico32, outils de statistique textuelle, et la KeyW0rds Method
des logiciels WordSmith Tools3 et Abundantia Verborum4 (Speelman, 1997). Les deux
méthodologies aboutissent grosso modo a des résultats similaires, a savoir une liste de mots
spécifiques pourvus d’une mesure statistique indiquant le degré de spéciﬁcité. Les différences
les plus importantes résident dans la méthodologie et la statistique sous-jacentes.

Lexico3 : SYLED — CLA2T, Paris3 : http://www.cavi.ur1iv-paris3.fr/ilpga/ilpga/tal/lexicoWWW/

3 WordSn1ith Tools version 3 : http://www.lexically.net/wordsn1ith/ et http://www.oup.com

4

Abundantia Verbomm : http://wwwling.arts.kuleuven.ac.belgenlinglabundantlobtainl

Ann Bertels

Premierement, le calcul des specificites (Lafon, 1984 et Labbe & Labbe, 1991) compare une
section d’un corpus au corpus entier afin d’identifier le vocabulaire specifique de cette
section. La comparaison partie-tout permet de decider si la frequence relative d’un mot dans
la section est superieure a ce qui serait attendu, en fonction de la frequence relative dans le
corpus entier. L’analyse statistique sous-jacente au calcul des specificites utilise le test de
Fisher Exact, base sur les probabilites exactes de la distribution hypergeometrique. Fisher
Exact est generalement utilise pour un ensemble de donnees de taille modeste (n S 20). En
outre, les factorielles de la formule pour le calcul de la probabilite de la frequence observee
(Lafon, 1984) meneraient a des nombres astronomiques, si la formule etait appliquee a un
corpus de quelques milliers, voire des millions de mots. Pour remedier a ce probleme, Lafon
propose d’utiliser des logarithmes. Des lors, le resultat du calcul x est a interpreter comme
l’exposant de la base 10, d’ou resulte la probabilite 10X. Dans Lexico3, ce sont les exposants
(resultats de la formule du calcul hypergeometrique) qui figurent dans la colonne du
coefficient de specificite. Les specificites positives indiquent un suremploi dans la section
analysee, tandis que les specificites negatives signalent un sous-emploi. Le calcul des
specificites est surtout utilise par la communaute francophone (Cf. Zimina, 2004 et Drouin,
2004)

La deuxieme methodologie permettant d’identifier les mots les plus specifiques est surtout
utilisee par des utilisateurs du logiciel WordSmith—KeyWords (Berber-Sardinha, 1999). Elle
est couramment appelee KeyWords Method ou methode des mots-clefs. Les frequences dans
le corpus specialise sont comparees aux frequences dans un corpus de reference de langue
generale, compte tenu de la taille des deux corpus, ce qui permet d’identifier les mots
significativement plus frequents dans le corpus specialise. Il s’agit donc de la comparaison de
deux corpus differents, et non d’une comparaison partie-tout. La deuxieme difference entre
les deux methodologies reside dans la statistique sous-jacente. La KeyWords Method se sert
du ratio du log de vraisemblance (log likelihood ratio) (Dunning,1993). Cette statistique de
test n’est pas basee sur des probabilites exactes et par consequent, elle s’applique facilement a
des corpus plut6t volumineux. Le ratio du log de vraisemblance sera d’autant plus eleve que
le mot est plus frequent dans le corpus specialise par rapport au corpus de reference, indiquant
des lors son degre de specificite. La valeur p correspondante permet de supprimer les
specificites statistiquement non significatives (pS0.05). En plus, le tri des specificites en
fonction de la statistique de test (ratio du log de vraisemblance) permet de les classer par
ordre de specificite decroissante et par consequent, de les situer dans un continuum de
specificite.

3.2 Polysémie

Les specificites relevees dans le corpus technique d’analyse, font ensuite l’objet d’une
analyse semantique. Pour chaque specificite, on determinera le degre de monosemie, dans le
but de verifier si les mots les plus specifiques sont en effet (les plus) monosemiques et si les
mots moins specifiques ont plus tendance a étre polysemiques. Il s’agira donc d’objectiver et
de quantifier l’analyse semantique, en ayant recours aux cooccurrences. Selon Schutze
(1998), Veronis (2003) et d’autres, les cooccurrences permettent de distinguer les differents
usages et sens des mots. Audibert (2003) recourt meme aux cooccurrences comme criteres de
desambigu'1'sation semantique automatique. Dans Veronis (2003), les cooccurrences d’un mot,
a partir d’un grand corpus, sont regroupees suivant leur similarite ou dissimilarite (en fonction
de leur co-frequence) pour identifier les differents sens du mot.

A la découverte de la pob/sémie des spéciﬁcités duﬁangais technique

Afin de déterminer le degré de monosémie des spécificités, nous proposons d’aller plus loin et
d’étudier les cooccurrences de deuxieme ordre. Les cooccurrences des cooccurrences
perrnettent de trouver des synonymes d’un mot, selon Martinez (2000). Pour le mot mesures,
il trouve les cooccurrents nouvelles, prises, etc. qui cooccurrent a leur tour avec decisions.
D’apres Denhiere & Lemaire (2003), les cooccurrences de deuxieme ordre et méme d’ordre
supérieur déterminent le degré d’association de deux mots Ml et M2, méme si ces deux mots
ne figurent jamais ensemble. Si les cooccurrences Ml-M3 et M2-M3 sont suffisamment
fortes, on considere que Ml et M2 sont associés et des cooccurrents d’ordre 2. Il est
également possible d’extraire automatiquement les sens des mots a partir d’un réseau de
cooccurrences lexicales de deuxieme ordre, comme l’explique Ferret (2004). La connectivité
des cooccurrents formant un sens est plus importante que leur connectivité avec les autres
cooccurrents définissant les autres sens de ce mot, la mesure de cohésion étant l’inforrnation
mutuelle normalisée (Ferret, 2004).

Les cooccurrents de deuxieme ordre étant des criteres désambigu'1'sateurs puissants, ils seront
tres précieux lors de l’analyse sémantique des spécificités. En effet, le degré de recouvrement
des cooccurrences de deuxieme ordre sera un indice important du degré de monosémie du mot
de base. Pour étudier le caractere monosémique ou polysémique d’une unité linguistique, on
Vérifie si les contextes peuvent étre considérés comme sémantiquement homogenes ou non
(Condamines & Rebeyrolles, 1997). L’acces a la sémantique des cooccurrences pourra se
faire (automatiquement) par le biais des cooccurrences de deuxieme ordre. Le degré de
recouvrement de ces cooccurrences de deuxieme ordre indiquera a quel point les
cooccurrences de premier ordre (contextes du mot de base) sont sémantiquement homogenes.

0 Si les cooccurrents des cooccurrents (<< cc >> ou cooccurrents de deuxieme ordre) sont
formellement tres différents et se recouvrent tres peu, les différents cooccurrents
(<< c >> ou cooccurrents de premier ordre) seront sémantiquement plus diversifiés, une
structure formelle de cooccurrence différente indiquant un sens différent. Les
cooccurrents sémantiquement diversifiés appartenant a plusieurs champs
sémantiques, la spécificité aura moins de chances d’étre monosémique.

0 Et inversement, plus les cooccurrences des cooccurrences se recouvrent, plus les
cooccurrents sont sémantiquement homogenes. Le degré de ressemblance ou
similarité lexicale des cooccurrents d’un mot étant proportionnel au degré de
monosémie de ce mot, un fort recouvrement des cooccurrents de deuxieme ordre
signale un degré de monosémie plus important.

4 Premiers résultats de la recherche : spéciﬁcités et polysémie

4.1 Spéciﬁcités

La liste des spécificités du corpus technique d’analyse (l,7 million de mots) est générée avec
les logiciels Abundantia Verborum et AV Frequency List Tool. Une experimentation sur un
échantillon restreint du corpus technique sur les trois logiciels disponibles pour le calcul des
spécificités montre des résultats comparables en termes de spécificités relevées. Pour garantir
la comparaison des résultats, le corpus technique a été incorporé dans le corpus de référence
dans le logiciel Lexico3, procédant par comparaison partie-tout. Force est de constater que la

Ann Bertels

méme procédure d’incorporation pour le corpus entier (corpus technique de 1,7 million et
corpus de référence de 15,3 millions) n’aboutit pas aux résultats escomptés. Méme si la liste
de fréquence du grand corpus entier s’afﬁche, l’étape suivante du calcul des spéciﬁcités
échoue en raison de la taille trop importante du corpus.

Appliquée au corpus technique lemmatisé, la KeyW0rds Method produit une liste d’enViron
13.000 spéciﬁcités pour le corpus technique (pS0.05). A l’aide des codes Cordial, une liste de
mots grammaticaux (450) et une liste de noms propres (7200) sont générées, permettant de les
supprimer. Les opérations de ﬁltrage et de nettoyage génerent une liste de spécificités
techniques deﬁnitive de 7240 mots (lemmes), Cf. Figure 1, pour un apercu des 25 mots les
plus spécifiques. Ces 7240 mots feront l’objet de l’analyse sémantique automatisée pour
établir le degré de monosémie. La premiere colonne (Cf. Figure 1) contient les lemmes
spéciﬁques, les deuxieme et troisieme colonnes donnent la fréquence absolue dans le corpus
technique (FREQ_ABS1) et dans le corpus de référence (FREQ_ABS 2). Dans la colonne 4,
on Voit la statistique de test LLR indiquant le degré de spéciﬁcité et dans la colonne 5, le
complément de la Valeur p correspondante (1-p). Les colonnes 6 et 7 afﬁchent les fréquences
relatives (multipliées par 10000) pour les deux corpus et la demiere colonne informe sur le
type de spécificité (1 pour une spécificité positive et -1 pour une spécificité negative). 11 est a
remarquer que cette liste de spéciﬁcités contient aussi des mots de la langue générale (p.ex.
type, permettre), spéciﬁques de ce corpus technique. Ce ne sont pas des termes, mais ils sont
maintenus car nous nous proposons de comparer leur degré de monosémie a celui des termes
(dans le corpus technique) et a leur degré de monosémie dans un corpus de langue générale.

LEMME FREQ ABS1 FREQ ABS2 LLR 1-P FREQ REL1 FREQ REL2 SPEC POS
machine 12671 1052 50521,91 1 74,51 0,71 1
outil 8306 918 32037,72 1 48,84 0,62 1
usinage 6720 8 30468,41 1 39,52 0,01 1
piece 7556 2219 24407,46 1 44,43 1,50 1
mm 5490 191 23357,57 1 32,28 0,13 1
Vitesse 5283 900 19108,78 1 31,07 0,61 1
coupe 6730 4153 17063,37 1 39,58 2,80 1
broche 2893 12 13010,42 1 17,01 0,01 1
Fig 2680 0 12194,00 1 15,76 0,00 1
axe 3206 420 12079,16 1 18,85 0,28 1
copeau 2557 0 11634,18 1 15,04 0,00 1
plaquette 2407 35 10592,46 1 14,15 0,02 1
diametre 2415 95 10200,09 1 14,20 0,06 1
cornmande 2751 850 8765,71 1 16,18 0,57 1
acier 2252 277 8558,49 1 13,24 0,19 1
fraisage 1873 0 8521,34 1 11,01 0,00 1
aréte 1870 29 8213,91 1 11,00 0,02 1
precision 2263 541 7663,01 1 13,31 0,36 1
usiner 1577 11 7045,52 1 9,27 0,01 1
surface 2258 758 7037,02 1 13,28 0,51 1
type 2820 1830 6994,07 1 16,58 1,23 1
systeme 4052 5165 6915,85 1 23,83 3,48 1
fraise 1571 45 6745,88 1 9,24 0,03 1
game 1860 545 6006,35 1 10,94 0,37 1
permettre 4883 9504 5848,03 1 28,71 6,41 1

Figure 1 : Les 25 mots les plus spéciﬁques du corpus technique d’analyse

A la découverte de la pob/sémie des spéciﬁcités duﬁancais technique

4.2 Polysémie

Le degré de monosémie (ou inversement de polysémie) dépend du degré de recouvrement des
cooccurrents des cooccurrents. Aﬁn d’établir les listes des cooccurrences pertinentes a deux
niveaux et de générer une base de données qui sera interrogée pour le calcul automatique du
degré de recouvrement, nous avons recours a un algorithme de scripts Pythons.

Pour déterminer le degré de monosémie, nous proposons une formule (Cf. Figure 2), basée
sur le recouvrement des cooccurrents des cooccurrents (cc), en tenant compte (1) de la
fréquence d’un cc dans la liste des cc (= nombre de cooccurrents (c) apparaissant avec ce cc),
(2) du nombre total de c et (3) du nombre total de cc. La mesure d’association utilisée pour
déterminer les cooccurrences pertinentes est la statistique LLR (log de Vraisemblance). Nous
ne prenons en considération que les cooccurrences statistiquement signiﬁcatives (pS0.05).

Z fq cc

cc # total c - # total cc

Figure 2 : Forrnule de recouvrement des cooccurrents des cooccurrents

Dans un premier temps, nous dressons une liste des cooccurrences pertinentes a partir des
fichiers lemmatisés du corpus technique, contenant le collocatif, la base6 et leur co-fréquence.
Deux autres fichiers sont dérivés de ces informations et contiennent respectivement les bases
et les collocatifs et leurs fréquences. Toutes ces informations permettront de générer une base
de données avec des informations statistiques, a savoir la statistique de test LLR et la Valeur
p. En fait, deux listes de cooccurrences avec leur base de données correspondante sont ainsi
dressées : une premiere liste avec les spécificités comme base et leurs cooccurrents comme
collocatif et une deuxieme liste de cooccurrences avec les cooccurrents de la premiere liste
comme base et leurs cooccurrents (d’ordre 2) comme collocatif.

Les parametres modiﬁables sont le type de cooccurrent a relever (lemme ou forme ﬂéchie) et
la fenétre d’observation. Nous optons pour une fenétre de [-5,+5], 5 mots a gauche et 5 mots a
droite, parce qu’elle apporte assez d’information sémantique, sans qu’il y ait trop de bruit et
qu’elle perrnet un traitement informatique efﬁcace. Au premier niveau d’analyse de la
spécificité comme base, la base de la cooccurrence est nécessairement relevée sous forme
lemmatisée, afin de pouvoir rattacher les informations sémantiques (degré de monosémie) aux
informations de spécificité (liste de spécificités). Pour le collocatif, la forme ﬂéchie s’impose,
en raison des informations sémantiques plus riches qu’elle véhicule (Cf. différence entrepiéce
d usiner et piéce usinée). Comme ce collocatif est la base du deuxieme niveau d’analyse, la
forme ﬂéchie s’impose a ce deuxieme niveau tant pour la base que pour le collocatif.

Ces deux bases de données sont fusionnées en une grande base de données, interrogée pour
l’analyse du recouvrement des cooccurrents de deuxieme ordre. A cet effet, la fonction
Python de l’algorithme prévoit les parametres suivants : la base (spécificité a analyser), le

5

http://www.python.org/

6 la base (anglais : node) e’tant le mot e’tudie’ et le collocatif (anglais : collocate) e'tant un de ses cooccurrents

Ann Bertels

seuil de signiﬁcation pour les cooccurrents de premier niveau (p.ex. 0.95 pour p§0.05), le
seuil pour les cooccurrents de deuxieme niveau et la base de données. 11 y a plus de
recouvrement, si plus de cooccurrents (c) partagent le meme cc, ce qui signiﬁe un poids plus
lourd pour ce cc (score pres de 1). Un cc moins/pas partage indique donc peu/pas de
recouvrement (score pres de 0).

La ﬁgure 3 ci-dessous montre les premiers resultats pour le corpus technique (1.7 million) et
pour un seuil de signiﬁcation des c et cc de pS0.0001. Parrni les 25 mots les plus speciﬁques
du corpus technique entier, les 2 mots les plus spéciﬁques se caracterisent par le degre de
monosémie le moins eleve (rangs 25 et 24), indiquant peu de recouvrement des cooccurrents
d’ordre 2. Les mots en gras sont les moins monosémiques de cet echantillon, ce qui semble
contredire la these de la correlation7 entre le degré de speciﬁcite et le degré de monosemie.

LEMME FREQ_ABS1 FREQ_AB S2 LLR DEGRE DE RANG DE
MONOSEMIE MONO SEMIE

machine 12671 1052 50521,91 0,0231 25
outil 8306 918 32037,72 0,0240 24
usinage 6720 8 30468,41 0,0349 12
piece 7556 2219 24407,46 0,0310 18
mm 5490 191 23357,57 0,0534 1
Vitesse 5283 900 19108,78 0,0402 6
coupe 6730 4153 17063,37 0,0370 10
broche 2893 12 13010,42 0,0394

Fig 2680 0 12194,00 0,0483 3
axe 3206 420 12079,16 0,0340 13
copeau 2557 0 11634,18 0,0299 20
plaquette 2407 35 10592,46 0,0282 22
diametre 2415 95 10200,09 0,0444 4
commande 2751 850 8765,71 0,0317 17
acier 2252 277 8558,49 0,0282 21
fraisage 1873 0 8521,34 0,0350 11
arete 1870 29 8213,91 0,0386

precision 2263 541 7663,01 0,0491 2
usiner 1577 11 7045,52 0,0406 5
surface 2258 758 7037,02 0,0321 15
type 2820 1830 6994,07 0,0372 9
systeme 4052 5165 6915,85 0,0280 23
fraise 1571 45 6745,88 0,0319 16
gamrne 1860 545 6006,35 0,0324 14
permettre 4883 9504 5848,03 0,0303 19

Figure 3 : Degré et rang de monosemie des 25 mots les plus speciﬁques (p S 0.0001)

Pour les 100 mots les plus spéciﬁques, une premiere analyse de regression simple fait
intervenir le degre de monosemie comme Variable dépendante et le degre de speciﬁcite
comme Variable indépendante ou predictive. Elle montre qu’il y a une tres faible correlation
negative (p=0.03) et que le degre de speciﬁcite n’explique que 3.5% de la Variation du degré

7 Nous ne recourons pas a une simple mesure de correlation, en raison de l’effet d’interference attendu des

autres facteurs (frequence, classe lexicale, etc.).

A la découverte de la pob/sémie des spéciﬁcités duﬁangais technique

de monosémie. Une analyse de régression multiple, mesurant l’impact de plusieurs Variables
indépendantes (spécificité, fréquence, classe lexicale, nombre de classes lexicales et
longueur), indique comme facteurs significatifs le nombre de classes lexicales (p=0.008), la
classe lexicale (p=0.03) et la fréquence (p=0.03) pour une Variation totale expliquée de 12%
(p=0.004). Parrni les 100 mots les plus spécifiques du corpus technique, les mots les plus
polysémiques, affichant le degré de monosémie le plus bas, se caractérisent par une fréquence
absolue élevée et par leur appartenance a deux ou plusieurs classes lexicales, principalement
les classes ‘nom’ et ‘adjectif’.

5 Conclusion et perspectives

Pour étudier la sémantique des spéciﬁcités dans le domaine technique des machines-outils
pour l’usinage des métaux, nous avons eu recours a une double analyse. D’une part, la
KeyW0rds Method a permis de dresser la liste des spéciﬁcités du corpus d’analyse, ordonnées
par ordre de spécificité décroissante. D’autre part, la forrnule pour le recouvrement des
cooccurrences des cooccurrences a permis d’accéder a la sémantique des spécificités, en
évaluant le degré de monosémie. L’analyse détaillée des résultats de recherche nous
apprendra pour un nombre important de mots techniques, s’il y a une corrélation entre leur
degré de spécificité et leur degré de monosémie, en fonction d’une série de facteurs,
notamment la classe lexicale et la fréquence.

La formule pour le recouvrement des cooccurrents de deuxieme ordre et pour le degré de
monosémie sera soumise a plusieurs expérimentations en fonction de plusieurs parametres,
afin de mettre au point le calcul du degré de monosémie. Une fois recueillies pour le corpus
technique entier, les données sur le degré de monosémie permettront de situer les spécificités
dans un continuum de monosémie (continuum de sens). Des analyses statistiques dans R8
permettront ensuite de Vérifier la corrélation entre le continuum de spécificité et le continuum
de monosémie et de procéder a une analyse linguistique détaillée en fonction des Variables
(linguistiques) indépendantes.

Nous envisageons cette double analyse du degré de spécificité et du degré de monosémie pour
les collocations spéciﬁques également, étant donné que les termes se situent souvent au
niveau des unités polylexicales. Nous nous proposons aussi de procéder a une Validation
manuelle de la forrnule déterminant le degré de monosémie a l’aide de l’analyse des
collocations et cooccurrences relevées.

Références

Audibert L. (2003), Etude des criteres de désambigu'1'sation sémantique automatique : résultats
sur les cooccurrences, Actes de TALN 2003, 35-44.

Berber Sardinha A. (1999), Word sets, keywords and text contents : An investigation of text
topic on the computer, DELTA, 15-1, 141-149.

8

http://www.r-project.org/

Ann Bertels

Cabré M.T. (1998), La terminologie. Ilzéorie, méthode et applications, Ottawa, Les Presses
de l’Université.

Cabré M.T. (2000), Terminologie et linguistique: la théorie des portes, Terminologies
nouvelles, 21, 10-15.

Condamines A., Rebeyrolle J. (1997), Point de vue en langue spécialisée, Meta, XLII-1, 174-
184.

Drouin P. (2004), Spéciﬁcités lexicales et acquisition de la terminologie, Actes de JADT
2004, 345-352.

Denhiere G., Lemaire B. (2003), Modélisation des effets contextuels par l'analyse de la
sémantique latente. Actes de EPIQUE 2003, http://www.upmf-grenoble.fr/sciedu/blemaire/

epigue03 .pdf

Dunning T. (1993), Accurate methods for the statistics of surprise and coincidence,
Computational Linguistics, 19-1, 61-74.

Ferret O. (2004), Découvrir des sens de mots a partir d’un réseau de cooccurrences lexicales,
Actes de TALN 2004, http://www.lpl.uniV-aix.fr/iep-taln04/proceed/actes/taln2004-

Fez/Ferret.pdf

Gaudin F. (1993), Pour une socioterminologie. Des proble‘mes sémantiques aux pratiques
institutionnelles, Rouen, Publications de l’UniVersité de Rouen.

Labbé C., Labbé D. (2001), Que mesure la spéciﬁcité du Vocabulaire?, Lexicometrica, 3,
http://www.caVi.uniV-paris3.fr/lexicometrica/article/numero3/speciﬁcite2001.PDF

Lafon P. (1984), Dépouillements et statistiques en lexicométrie, Geneve-Paris, Slatkine-
Champion.

Lerat P. (1995), Les langues spécialisées, Paris, PUF.

Martinez W. (2000), Mise en évidence de rapports synonymiques par la méthode des
cooccurrences, Actes de JADT 2000, 78-84.

Schiitze H. (1998), Automatic Word Sense Discrimination, Computational Linguistics, 24-1,
97-123.

Speelman D. (1997), Abundantia verborum .' a computer tool for carrying out corpus-based
linguistic case studies, PhD Thesis, K.U.LeuVen.

Temmerman R. (1997), Questioning the univocity ideal. The difference between socio-
cognitive Terminology and traditional Terminology, Hermes, 18, 51-90.

Véronis J. (2003), Cartographie lexicale pour la recherche d’informations, Actes de TALN
2003, 265-274.

