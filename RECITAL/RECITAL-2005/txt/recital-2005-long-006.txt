RECITAL 2005, Dourdan, 6-I0juin 2005

Ala découverte de la polysémie des spéciﬁcités
du francais technique

Ann Bertels

ILT — K.U.Leuven
Dekenstraat 6 — B-3000 LEUVEN (Belgique)
aIm.berte1s@i1t.ku1euven.ac.be

Mots-clefs — Keywords
sémantique lexicale, langue spécialisée, spéciﬁcités, polysén1ie, cooccurrences

lexical semantics, language for speciﬁc purposes (LSP), keywords, polysemy, co-occurrences

Résumé — Abstract

Cet article décrit l’analyse sémantique des spéciﬁcités dans le domaine technique des
machines-outils pour l’usinage des métaux. Le but de cette étude est de veriﬁer si et dans
quelle mesure les spéciﬁcités dans ce domaine sont monosén1iques ou polysémiques. Les
spéciﬁcités (situées dans un continuum de spéciﬁcité) seront identiﬁées avec la KeyWords
Method en comparant le corpus d’analyse a un corpus de référence. Elles feront ensuite
l’objet d’une analyse sémantique automatisée a partir du recouvrement des cooccurrences des
cooccurrences, aﬁn d’établir le continuum de monosémie. Les travaux de recherche étant en
cours, nous présenterons des résultats prélin1inaires de cette double analyse.

This article discusses a semantic analysis of pivotal terms (keywords) in the domain of
machining terminology in French. Building on corpus data, the investigation attempts to ﬁnd
out whether, and to what extent, the keywords are polysemous. In order to identify the most
typical words of the typicality continuum, the KeyWords Method will be used to compare the
technical corpus with a reference corpus. The monosemy continuum will be implemented in
terms of degree of overlap between the co-occurrents of the co-occurrents of the keywords.
We present some preliminary results of work in progress.

1 Introduction et question de recherche

Cet article s’inscrit dans le cadre d’une these de doctorat sur la sémantique du vocabulaire
spéciﬁque d’un corpus de francais technique. Comme le corpus d’analyse releve du domaine

575

576

Ann Bertels

technique des machines-outils pour l’usinage des métaux, l’analyse sémantique porte sur les
spéciﬁcitésl d’une langue spécialisée.

Dans la langue spécialisée, les besoins communicatifs requierent plus de precision, ce que la
terminologie traditionnelle déﬁnit comme l’univocité, la monoréférentialité et la monosémie
des unites terminologiques de la langue spécialisée. La terminologie traditionnelle
prescriptive et normative adopte une approche onomasiologique par domaine. Récemment, la
monosémie et l’univocité de la langue spécialisée ont été remises en question par la Théorie
Communicative de la Terminologie (Cabré, 1998, 2000), par la socioterminologie (Gaudin,
1993) et par la terminologie socio-cognitive (Temmerman, 1997). Les termes font partie
intégrante de la langue naturelle, mais véhiculent des connaissances spécialisées (Lerat,
1995). Les partisans de la terminologie descriptive rejettent la dichotomie entre la langue
générale et la langue spécialisée et adoptent une approche sémasiologique et linguistique,
basée sur l’étude de corpus de textes specialises (Condamines & Rebeyrolles, 1997).

Pour quantiﬁer la these monosémiste de la terminologie traditionnelle, nous nous proposons
de la reformuler en une question de recherche opérationnelle et mesurable : « Y a-t-il une
correlation entre, d’une part, le continuum de spéciﬁcité et, d’autre part, le continuum de
monosémie (continuum de sens) ? » L’hypothese de recherche avancée pose que,
contrairement a la these traditionnelle, les mots (les plus) spéciﬁques du corpus technique ne
sont pas nécessairement (les plus) monosémiques. L’analyse se propose donc de veriﬁer la
polysémie des mots du corpus technique d’analyse, p.ex. le mot broche (1) « partie toumante
d’une machine-outil qui porte un outil ou une piece a usiner » et (2) « outil servant a usiner
des pieces métalliques ». A cet effet, ces mots sont ordonnés en fonction de leur spéciﬁcité et
situés sur une échelle de spéciﬁcité allant des mots les plus spéciﬁques aux mots les moins
spéciﬁques, mais comprenant toujours des spéciﬁcités statistiquement signiﬁcatives du
corpus technique. Un deuxieme classement situe les memes mots sur une échelle de
monosémie, a partir d’une analyse des cooccurrences de deuxieme ordre, c’est-a-dire les
cooccurrences des cooccurrences. La question de recherche principale (correlation entre le
degré de spéciﬁcité et le degré de monosémie) sera complétée par des questions de recherche
secondaires faisant intervenir les facteurs inﬂuant sur le degré de monosémie, notamment la
fréquence et la classe lexicale. Une analyse de regression multiple permettra de veriﬁer
l’impact des variables indépendantes (spéciﬁcité, fréquence, classe lexicale, etc.) sur le degré
de monosémie.

2 Corpus technique d’analyse et corpus de référence

Le corpus technique d’analyse est constitué de textes techniques du domaine des machines-
outils pour l’usinage des métaux et comprend environ 1.760.000 mots. Le corpus a été
étiqueté et lemmatisé par Cordial 7 Analyseur et consiste en 4 sous-corpus, datant de 1998 a
2001 : revues techniques électroniques (800.000) et ﬁches techniques (300.000) trouvées sur
Internet, normes ISO et directives (300.000) et quatre manuels numérisés (360.000). Les
textes se situent a différents niveaux de normalisation et de vulgarisation, s’adressant tant a
des professionnels (revues et ﬁches) qu’a des étudiants (manuels). Aﬁn de pouvoir determiner

Nous adoptons 1e terme << spéciﬁcités >> pour designer les mots les plus spéciﬁques et caractéristiques du
corpus d’analyse, indépendamment de la méthode utilisée (calcul des spéciﬁcités vs. KeyWords Method).

A la découverte de la polysémie des speciﬁcites dufrangais technique

les speciﬁcites du corpus technique, il est complete par un corpus de reference de langue
generale. Celui-ci est constitue d’articles journalistiques du journal Le Monde (janvier —
septembre 1998). II a egalement ete lemmatise et comprend environ 15.300.000 mots.

Les ﬁchiers generes par Cordial se composent de trois colonnes, avec un mot par ligne: (1) la
forme ﬂechie ou forme graphique, (2) le lemme ou forme canonique et (3) le code Cordial,
comparable a un POS-tag (Part-Of-Speech) indiquant la classe lexicale. Dans les ﬁchiers
texte, nous avons corrige quelques fautes de frappe. Les ﬁchiers lemmatises ont egalement
fait l’objet d’un nettoyage, a savoir quelques regroupements (p.ex. lemmes avec et sans point
Figﬂ7'ig et lemmes avec et sans trait d’union) et la correction des erreurs de lemmatisation
(p.ex. machines-outils sous le lemme machine-outil). Ces operations de nettoyage ont ete
effectuees, tant pour le corpus d’analyse technique que pour le corpus de reference.

3 Approche méthodologique : speciﬁcites et polysémie

Comme la recherche porte sur la question de savoir s’il y a une correlation entre le continuum
de speciﬁcite et le continuum de monosemie, la reponse et l’analyse linguistique qui en
decoule requierent une approche methodologique double. Il faut d’une part le calcul des
speciﬁcites et d’autre part une mesure pour determiner le degre de monosemie. Ces
speciﬁcites se situent, non seulement au niveau des unites simples, p.ex. fraisage, commande,
mais egalement au niveau des unites polylexicales, p.ex. commande numérique.

3.1 Spéciﬁcités

La recherche en langue specialisee prend generalement comme point de depart l’identiﬁcation
des speciﬁcites, c’est-a-dire des mots speciﬁques qui caracterisent le corpus specialise et qui
le differencient d’un corpus de langue generale. Les speciﬁcites ne sont pas les mots les plus
frequents de ce corpus, mais les mots les plus representatifs. Du point de vue relatif, ces mots
ﬁgurent de facon signiﬁcative plus frequemment dans le corpus de langue specialisee que
dans un corpus de langue generale. Aﬁn de determiner les speciﬁcites, les frequences dans le
corpus specialise sont comparees aux frequences dans un corpus de reference, compte tenu de
la taille des deux corpus, ce qui revient a comparer la frequence observee (corpus d’analyse) a
la frequence attendue (corpus de reference). S’il y a une difference entre la frequence
observee et la frequence attendue, il faut veriﬁer si elle est statistiquement signiﬁcative. A cet
effet, deux methodologies sont desormais disponibles : le calcul des speciﬁcites (Lafon, 1984)
implemente dans le logiciel Lexico32, outils de statistique textuelle, et la Key Words Method
des logiciels WordSmith Tools3 et Abundantia Verborum4 (Speelman, 1997). Les deux
methodologies aboutissent grosso modo a des resultats similaires, a savoir une liste de mots
speciﬁques pourvus d’une mesure statistique indiquant le degre de speciﬁcite. Les differences
les plus importantes resident dans la methodologie et la statistique sous-jacentes.

2 Lexico3 : SYLED — CLA2T, Paris3 : http://wvvw.cavi.univ-paris3.fr/ilpga/ilpga/tal/lexicoW W W/

3 WordSmith Tools version 3 : http://wvvw.1exica11y.net/words1nith/ et http://www.oup.com

4 Abundantia Verborum : http://www1ing.arts.ku1euven.ac.be/ gen1ing/abundant/obtain/

577

578

Ann Bertels

Premierement, le calcul des speciﬁcites (Lafon, 1984 et Labbe & Labbe, 1991) compare une
section d’un corpus au corpus entier aﬁn d’identiﬁer le vocabulaire speciﬁque de cette
section. La comparaison partie-tout permet de decider si la frequence relative d’un mot dans
la section est superieure a ce qui serait attendu, en fonction de la frequence relative dans le
corpus entier. L’analyse statistique sous-jacente au calcul des speciﬁcites utilise le test de
Fisher Exact, base sur les probabilites exactes de la distribution hypergeometrique. Fisher
Exact est generalement utilise pour un ensemble de donnees de taille modeste (n S 20). En
outre, les factorielles de la forrnule pour le calcul de la probabilite de la frequence observee
(Lafon, 1984) meneraient a des nombres astronon1iques, si la forrnule etait appliquee a un
corpus de quelques milliers, voire des millions de mots. Pour remedier a ce probleme, Lafon
propose d’utiliser des logarithmes. Des lors, le resultat du calcul x est a interpreter comme
l’exposant de la base 10, d’o1‘1 resulte la probabilite 10". Dans Lexico3, ce sont les exposants
(resultats de la forrnule du calcul hypergeometrique) qui ﬁgurent dans la colonne du
coefﬁcient de speciﬁcite. Les speciﬁcites positives indiquent un suremploi dans la section
analysee, tandis que les speciﬁcites negatives signalent un sous-emploi. Le calcul des
speciﬁcites est surtout utilise par la communaute ﬁancophone (Cf Zimina, 2004 et Drouin,
2004).

La deuxieme methodologie permettant d’identiﬁer les mots les plus speciﬁques est surtout
utilisee par des utilisateurs du logiciel WordSn1ith—KeyWords (Berber-Sardinha, 1999). Elle
est couramment appelee KeyWords Method ou methode des mots-clefs. Les frequences dans
le corpus specialise sont comparees aux frequences dans un corpus de reference de langue
generale, compte tenu de la taille des deux corpus, ce qui permet d’identiﬁer les mots
signiﬁcativement plus frequents dans le corpus specialise. Il s’agit donc de la comparaison de
deux corpus differents, et non d’une comparaison partie-tout. La deuxieme difference entre
les deux methodologies reside dans la statistique sous-jacente. La KeyWords Method se sert
du ratio du log de vraisemblance (log likelihood ratio) (Dunning,1993). Cette statistique de
test n’est pas basee sur des probabilites exactes et par consequent, elle s’applique facilement a
des corpus plutot volumineux. Le ratio du log de vraisemblance sera d’autant plus eleve que
le mot est plus frequent dans le corpus specialise par rapport au corpus de reference, indiquant
des lors son degre de speciﬁcite. La valeur p correspondante permet de supprimer les
speciﬁcites statistiquement non signiﬁcatives (pS0.05). En plus, le tri des speciﬁcites en
fonction de la statistique de test (ratio du log de vraisemblance) permet de les classer par
ordre de speciﬁcite decroissante et par consequent, de les situer dans un continuum de
speciﬁcite.

3.2 Polysémie

Les speciﬁcites relevees dans le corpus technique d’analyse, font ensuite l’objet d’une
analyse semantique. Pour chaque speciﬁcite, on determinera le degre de monosemie, dans le
but de veriﬁer si les mots les plus speciﬁques sont en effet (les plus) monosen1iques et si les
mots moins speciﬁques ont plus tendance a etre polysen1iques. Il s’agira donc d’objectiver et
de quantiﬁer l’analyse semantique, en ayant recours aux cooccurrences. Selon Schiitze
(1998), Veronis (2003) et d’autres, les cooccurrences permettent de distinguer les differents
usages et sens des mots. Audibert (2003) recourt meme aux cooccurrences comme criteres de
desambigu'1'sation semantique automatique. Dans Veronis (2003), les cooccurrences d’un mot,
a partir d’un grand corpus, sont regroupees suivant leur sin1ilarite ou dissimilarite (en fonction
de leur co-frequence) pour identiﬁer les differents sens du mot.

A la découverte de la polysémie des spéciﬁcités dufrangais technique

Aﬁn de determiner le degré de monosémie des spéciﬁcités, nous proposons d’aller plus loin et
d’étudier les cooccurrences de deuxieme ordre. Les cooccurrences des cooccurrences
permettent de trouver des synonymes d’un mot, selon Martinez (2000). Pour le mot mesures,
il trouve les cooccurrents nouvelles, prises, etc. qui cooccurrent a leur tour avec décisions.
D’apres Denhiere & Lemaire (2003), les cooccurrences de deuxieme ordre et meme d’ordre
supérieur déterminent le degré d’association de deux mots M1 et M2, meme si ces deux mots
ne ﬁgurent jamais ensemble. Si les cooccurrences M1-M3 et M2-M3 sont suffisamment
fortes, on considere que M1 et M2 sont associés et des cooccurrents d’ordre 2. Il est
également possible d’extraire automatiquement les sens des mots a partir d’un réseau de
cooccurrences lexicales de deuxieme ordre, comme l’explique Ferret (2004). La connectivité
des cooccurrents formant un sens est plus importante que leur connectivité avec les autres
cooccurrents déﬁnissant les autres sens de ce mot, la mesure de cohesion étant l’information
mutuelle normalisée (Ferret, 2004).

Les cooccurrents de deuxieme ordre étant des criteres désambiguisateurs puissants, ils seront
tres précieux lors de l’analyse sémantique des spéciﬁcités. En effet, le degré de recouvrement
des cooccurrences de deuxieme ordre sera un indice important du degré de monosémie du mot
de base. Pour étudier le caractere monosémique ou polysémique d’une unite linguistique, on
vériﬁe si les contextes peuvent étre considérés comme sémantiquement homogenes ou non
(Condamines & Rebeyrolles, 1997). L’acces a la sémantique des cooccurrences pourra se
faire (automatiquement) par le biais des cooccurrences de deuxieme ordre. Le degré de
recouvrement de ces cooccurrences de deuxieme ordre indiquera a quel point les
cooccurrences de premier ordre (contextes du mot de base) sont sémantiquement homogenes.

0 Si les cooccurrents des cooccurrents (« cc » ou cooccurrents de deuxieme ordre) sont
formellement tres différents et se recouvrent tres peu, les différents cooccurrents
(« c » ou cooccurrents de premier ordre) seront sémantiquement plus diversiﬁés, une
structure formelle de cooccurrence différente indiquant un sens different. Les
cooccurrents sémantiquement diversiﬁés appartenant a plusieurs champs
sémantiques, la spéciﬁcité aura moins de chances d’etre monosémique.

0 Et inversement, plus les cooccurrences des cooccurrences se recouvrent, plus les
cooccurrents sont sémantiquement homogenes. Le degré de ressemblance ou
similarité lexicale des cooccurrents d’un mot étant proportionnel au degré de
monosémie de ce mot, un fort recouvrement des cooccurrents de deuxieme ordre
signale un degré de monosémie plus important.

4 Premiers résultats de la recherche : spéciﬁcités et polysémie

4.1 Spéciﬁcités

La liste des spéciﬁcités du corpus technique d’analyse (1,7 million de mots) est générée avec
les logiciels Abundantia Verborum et AV Frequency List Tool. Une experimentation sur un
échantillon restreint du corpus technique sur les trois logiciels disponibles pour le calcul des
spéciﬁcités montre des résultats comparables en termes de spéciﬁcités relevées. Pour garantir
la comparaison des résultats, le corpus technique a été incorporé dans le corpus de reference
dans le logiciel Lexico3, procédant par comparaison partie-tout. Force est de constater que la

579

580

Ann Bertels

meme procedure d’incorporation pour le corpus entier (corpus technique de 1,7 million et
corpus de reference de 15,3 millions) n’aboutit pas aux résultats escomptés. Meme si la liste
de fréquence du grand corpus entier s’afﬁche, l’étape suivante du calcul des spéciﬁcités
échoue en raison de la taille trop importante du corpus.

Appliquée au corpus technique lemmatisé, la KeyWords Method produit une liste d’environ
13.000 spéciﬁcités pour le corpus technique (p50.05). A l’aide des codes Cordial, une liste de
mots grammaticaux (450) et une liste de noms propres (7200) sont générées, permettant de les
supprimer. Les operations de ﬁltrage et de nettoyage génerent une liste de spéciﬁcités
techniques deﬁnitive de 7240 mots (lemmes), Cf. Figure 1, pour un apercu des 25 mots les
plus spéciﬁques. Ces 7240 mots feront l’objet de l’analyse sémantique automatisée pour
établir le degré de monosémie. La premiere colonne (Cf. Figure 1) contient les lemmes
spéciﬁques, les deuxieme et troisieme colonnes donnent la fréquence absolue dans le corpus
technique (FREQ_ABS1) et dans le corpus de reference (FREQ_ABS 2). Dans la colonne 4,
on voit la statistique de test LLR indiquant le degré de spéciﬁcité et dans la colonne 5, le
complement de la valeur p correspondante (1-p). Les colonnes 6 et 7 afﬁchent les fréquences
relatives (multipliées par 10000) pour les deux corpus et la demiere colonne informe sur le
type de spéciﬁcité (1 pour une spéciﬁcité positive et -1 pour une spéciﬁcité negative). Il est a
remarquer que cette liste de spéciﬁcités contient aussi des mots de la langue générale (p.ex.
type, permettre), spéciﬁques de ce corpus technique. Ce ne sont pas des termes, mais ils sont
maintenus car nous nous proposons de comparer leur degré de monosémie a celui des termes
(dans le corpus technique) et a leur degré de monosémie dans un corpus de langue générale.
LEMME ABS1
12671 1052
18 2037 72
720 41
556 19 4407 46
90 191 357 7
283 19108 78
730 17063 7

ABS2 LLR

211

1-P REL1

4 1 71
84 62

9 2 01

43 1

REL2 SPEC POS

13
61
80
01
00

153

12 1301042
1219400

20 1207916

1163418

105 46

00
02

65 71
558 49

521
13 1
663 01
045 2
8 037 02
07
2 15 5

1571 745
1860 5
848 03

7
1 9
00
02

01

13

1

83 48
4 03
10 7
71 41

1
1
1
1
1
1
1
1
1
1
1
1
10200 09 1 06
1
1
1
1
1
1
1
1
1
1
1
1

t—It—It—It—It—It—It—It—It—I—It—It—I—It—It—It—It—It—It—It—It—It—It—It—It—I

Figure 1 : Les 25 mots les plus spéciﬁques du corpus technique d’analyse

A la découverte de la polysémie des spéczﬁcités dufrancais technique

4.2 Polysémie

Le degré de monosémie (ou inversement de polysémie) dépend du degré de recouvrement des
cooccurrents des cooccurrents. Aﬁn d’établir les listes des cooccurrences pertinentes a deux
niveaux et de générer une base de données qui sera interrogée pour le calcul automatique du
degré de recouvrement, nous avons recours a un algorithme de scripts Pythons.

Pour determiner le degré de monosémie, nous proposons une forrnule (Cf. Figure 2), basée
sur le recouvrement des cooccurrents des cooccurrents (cc), en tenant compte (1) de la
fréquence d’un cc dans la liste des cc (= nombre de cooccurrents (c) apparaissant avec ce cc),
(2) du nombre total de c et (3) du nombre total de cc. La mesure d’association utilisée pour
determiner les cooccurrences pertinentes est la statistique LLR (log de vraisemblance). Nous
ne prenons en consideration que les cooccurrences statistiquement signiﬁcatives (pS0.05).

Z fq cc

cc # total c - # total cc

Figure 2 : Forrnule de recouvrement des cooccurrents des cooccurrents

Dans un premier temps, nous dressons une liste des cooccurrences pertinentes a partir des
ﬁchiers lemmatisés du corpus technique, contenant le collocatif, la base6 et leur co-fréquence.
Deux autres ﬁchiers sont dérivés de ces informations et contiennent respectivement les bases
et les collocatifs et leurs fréquences. Toutes ces informations permettront de générer une base
de données avec des informations statistiques, a savoir la statistique de test LLR et la valeur
p. En fait, deux listes de cooccurrences avec leur base de données correspondante sont ainsi
dressées : une premiere liste avec les spéciﬁcités comme base et leurs cooccurrents comme
collocatif et une deuxieme liste de cooccurrences avec les cooccurrents de la premiere liste
comme base et leurs cooccurrents (d’ordre 2) come collocatif.

Les parametres modiﬁables sont le type de cooccurrent a relever (lemme ou forme ﬂéchie) et
la fenétre d’observation. Nous optons pour une fenétre de [-5,+5], 5 mots a gauche et 5 mots a
droite, parce qu’elle apporte assez d’information sémantique, sans qu’il y ait trop de bruit et
qu’elle permet un traitement informatique efﬁcace. Au premier niveau d’analyse de la
spéciﬁcité comme base, la base de la cooccurrence est nécessairement relevée sous forme
lemmatisée, aﬁn de pouvoir rattacher les informations sémantiques (degré de monosémie) aux
informations de spéciﬁcité (liste de spéciﬁcités). Pour le collocatif, la forme ﬂéchie s’impose,
en raison des informations sémantiques plus riches qu’elle véhicule (Cf. difference entre pie‘ce
d usiner et pie‘ce usinée). Comme ce collocatif est la base du deuxieme niveau d’analyse, la
forme ﬂéchie s’impose a ce deuxieme niveau tant pour la base que pour le collocatif.

Ces deux bases de données sont fusionnées en une grande base de données, interrogée pour
l’analyse du recouvrement des cooccurrents de deuxieme ordre. A cet effet, la fonction
Python de l’algorithme prévoit les parametres suivants : la base (spéciﬁcité a analyser), le

5 http://www.python.org/

6 la base (anglais : node) étant 1e mot étudié et le co11ocatif(ang1ais : collocate) étant un de ses cooccurrents

581

582

Ann Bertels

seuil de signiﬁcation pour les cooccurrents de premier niveau (p.ex. 0.95 pour pS0.05), le
seuil pour les cooccurrents de deuxieme niveau et la base de données. Il y a plus de
recouvrement, si plus de cooccurrents (c) partagent le meme cc, ce qui signiﬁe un poids plus
lourd pour ce cc (score pres de 1). Un cc moins/pas partagé indique donc peu/pas de
recouvrement (score pres de 0).

La ﬁgure 3 ci-dessous montre les premiers résultats pour le corpus technique (1.7 million) et
pour un seuil de signiﬁcation des c et cc de pS0.0001. Parmi les 25 mots les plus spéciﬁques
du corpus technique entier, les 2 mots les plus spéciﬁques se caractérisent par le degré de
monosémie le moins élevé (rangs 25 et 24), indiquant peu de recouvrement des cooccurrents
d’ordre 2. Les mots en gras sont les moins monosémiques de cet échantillon, ce qui semble
contredire la these de la corrélation7 entre le degré de spéciﬁcité et le degré de monosémie.

DEGRE DE RANG DE
MONOSEMIE MONOSEMIE

LEMME LLR

FREQABSI FREQ_ABS2

105 50521 1
91 32037 2
6 30468 41 1
75 221 24407 1
54 191 23357
5 19108
6 4153 17063
1 13010 4
12194
12079 1
11634 1
105
10200 0
8765 71 0 031
8558 4
8521
8213 1 0 03
7663 01 0 0491
7045 0
7037 0 0321
6994 0 03
6915
4 6745
54 6006
5848 03

12671

003

0 031
0 032
0 030

Figure 3 : Degré et rang de monosémie des 25 mots les plus spéciﬁques (p 5 0.0001)

Pour les 100 mots les plus spéciﬁques, une premiere analyse de regression simple fait
intervenir le degré de monosémie comme variable dépendante et le degré de spéciﬁcité
comme variable indépendante ou predictive. Elle montre qu’il y a une tres faible correlation
negative (p=0.03) et que le degré de spéciﬁcité n’explique que 3.5% de la variation du degré

7 Nous ne recourons pas a une simple mesure de corrélation, en raison de 1’effet d’interférence attendu des

autres facteurs (fréquence, classe lexicale, etc.).

A la découverte de la polysémie des spéczﬁcités dufrangais technique

de monosémie. Une analyse de regression multiple, mesurant l’impact de plusieurs variables
indépendantes (spéciﬁcité, fréquence, classe lexicale, nombre de classes lexicales et
longueur), indique comme facteurs signiﬁcatifs le nombre de classes lexicales (p=0.008), la
classe lexicale (p=0.03) et la fréquence (p=0.03) pour une variation totale expliquée de 12%
(p=0.004). Parmi les 100 mots les plus spéciﬁques du corpus technique, les mots les plus
polysémiques, afﬁchant le degré de monosémie le plus bas, se caractérisent par une fréquence
absolue élevée et par leur appartenance a deux ou plusieurs classes lexicales, principalement
les classes ‘nom’ et ‘adjectif’.

5 Conclusion et perspectives

Pour étudier la sémantique des spéciﬁcités dans le domaine technique des machines-outils
pour l’usinage des métaux, nous avons eu recours a une double analyse. D’une part, la
KeyWords Method a permis de dresser la liste des spéciﬁcités du corpus d’analyse, ordonnées
par ordre de spéciﬁcité décroissante. D’autre part, la formule pour le recouvrement des
cooccurrences des cooccurrences a permis d’accéder a la sémantique des spéciﬁcités, en
évaluant le degré de monosémie. L’analyse détaillée des résultats de recherche nous
apprendra pour un nombre important de mots techniques, s’il y a une correlation entre leur
degré de spéciﬁcité et leur degré de monosémie, en fonction d’une série de facteurs,
notamrnent la classe lexicale et la fréquence.

La formule pour le recouvrement des cooccurrents de deuxieme ordre et pour le degré de
monosémie sera soumise a plusieurs expérimentations en fonction de plusieurs parametres,
aﬁn de mettre au point le calcul du degré de monosémie. Une fois recueillies pour le corpus
technique entier, les données sur le degré de monosémie permettront de situer les spéciﬁcités
dans un continuum de monosémie (continuum de sens). Des analyses statistiques dans R8
permettront ensuite de veriﬁer la correlation entre le continuum de spéciﬁcité et le continuum
de monosémie et de procéder a une analyse linguistique détaillée en fonction des variables
(linguistiques) indépendantes.

Nous envisageons cette double analyse du degré de spéciﬁcité et du degré de monosémie pour
les collocations spéciﬁques également, étant donné que les termes se situent souvent au
niveau des unités polylexicales. Nous nous proposons aussi de procéder a une validation
manuelle de la formule determinant le degré de monosémie a l’aide de l’analyse des
collocations et cooccurrences relevées.

Références

Audibert L. (2003), Etude des criteres de désambiguisation sémantique automatique : résultats
sur les cooccurrences, Actes de TALN 2003, 35-44.

Berber Sardinha A. (1999), Word sets, keywords and text contents : An investigation of text
topic on the computer, DELTA, 15-1, 141-149.

8 http://wvvw.r-project.org/

583

584

Ann Bertels

Cabré M.T. (1998), La terminologie. Théorie, méthode et applications, Ottawa, Les Presses
de l’Université.

Cabré M.T. (2000), Terminologie et linguistique: la théorie des portes, Terminologies
nouvelles, 21, 10-15.

Condamines A., Rebeyrolle J. (1997), Point de vue en langue spécialisée, Meta, XLII-1, 174-
1 84.

Drouin P. (2004), Spéciﬁcités lexicales et acquisition de la terminologie, Actes de JADT
2004, 345-352.

Denhiere G., Lemaire B. (2003), Modélisation des effets contextuels par l'analyse de la
sémantique latente. Actes de EPIQUE 2003, http://www.upmf-grenoble.fr/sciedu/blemaire/

epigue03.pdf

Dunning T. (1993), Accurate methods for the statistics of surprise and coincidence,
Computational Linguistics, 19-1, 61-74.

Ferret O. (2004), Découvrir des sens de mots a partir d’un réseau de cooccurrences lexicales,
Actes de TALN 2004, http://www.lpl.univ-aix.fr/iep-taln04/proceed/actes/taln2004-

Fez/Ferret.pdf

Gaudin F. (1993), Pour une socioterminologie. Des proble‘mes sémantiques aux pratiques
institutionnelles, Rouen, Publications de l’Université de Rouen.

Labbé C., Labbé D. (2001), Que mesure la spéciﬁcité du vocabulaire?, Lexicometrica, 3,
http://www.cavi.univ-paris3 .ﬁ'/lexicometrica/article/numero3/speciﬁcite2001 .PDF

Lafon P. (1984), Dépouillements et statistiques en lexicométrie, Geneve-Paris, Slatkine-
Champion.

Lerat P. (1995), Les langues spécialisées, Paris, PUF.

Martinez W. (2000), Mise en evidence de rapports synonymiques par la méthode des
cooccurrences, Actes de JADT 2000, 78-84.

Schiitze H. (1998), Automatic Word Sense Discrimination, Computational Linguistics, 24-1,
97-123.

Speelman D. (1997), Abundantia verborum : a computer tool for carrying out corpus-based
linguistic case studies, PhD Thesis, K.U.Leuven.

Temmerman R. (1997), Questioning the univocity ideal. The difference between socio-
cognitive Terminology and traditional Terminology, Hermes, 18, 51-90.

Véronis J. (2003), Cartographie lexicale pour la recherche d’informations, Actes de TALN
2003, 265-274.

