@proceedings{RECITAL:2006,
  editor    = {Mertens, Piet and Fairon, Cédrick and Dister, Anne and Watrin, Patrick},
  title     = {Actes des 8e Rencontres des Étudiants Chercheurs en Informatique pour le Traitement Automatique des Langues},
  month     = {April},
  year      = {2006},
  address   = {Leuven, Belgique},
  publisher = {Association pour le Traitement Automatique des Langues},
  url       = {http://www.atala.org/taln_archives/RECITAL/RECITAL-2006}
}

@inproceedings{archer:2006:RECITAL,
  author    = {Archer, Vincent},
  title     = {Acquisition semi-automatique de collocations à partir de corpus monolingues et multilingues comparables},
  booktitle = {Actes des 8e Rencontres des Étudiants Chercheurs en Informatique pour le Traitement Automatique des Langues},
  month     = {April},
  year      = {2006},
  address   = {Leuven, Belgique},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {651--660},
  url       = {http://www.atala.org/taln_archives/RECITAL/RECITAL-2006/recital-2006-long-001},
  language  = {french},
  resume    = {Cet article présente une méthode d’acquisition semi-automatique de collocations. Notre extraction monolingue estime pour chaque co-occurrence sa capacité à être une collocation, d’après une mesure statistique modélisant une caractéristique essentielle (le fait qu’une collocation se produit plus souvent que par hasard), effectue ensuite un filtrage automatique (en utilisant les vecteurs conceptuels) pour ne retenir que des collocations d’un certain type sémantique, puis effectue enfin un nouveau filtrage à partir de données entrées manuellement. Notre extraction bilingue est effectuée à partir de corpus comparables, et a pour but d’extraire des collocations qui ne soient pas forcément traductions mot à mot l’une de l’autre. Notre évaluation démontre l’intérêt de mêler extraction automatique et intervention manuelle pour acquérir des collocations et ainsi permettre de compléter les bases lexicales multilingues.},
  abstract  = {This paper presents a method for the semi-automatic acquisition of collocations. Our monolingual extraction estimates the ability of each co-occurrence to be a collocation, using a statitical measure which represents an essential property (the fact that a collocation occurs more often than would be expected by chance), then makes an automatic filtering (using conceptual vectors) to keep only one semantic type of collocation, and finally makes a new filtering, using manually entered data. Our bilingual extraction uses comparable corpora, and is aiming to extract collocations which are not necessarily word-to-word translations. Our evaluation shows the interest of mixing automatic extraction and manual intervention to obtain collocations and, in this manner, to complete multilingual lexical databases.},
  motscles  = {collocations, acquisition semi-automatique, corpus comparables},
  keywords  = {collocations, semi-automatic acquisition, comparable corpora},
}

@inproceedings{cartoni:2006:RECITAL,
  author    = {Cartoni, Bruno},
  title     = {Constance et variabilité de l’incomplétude lexicale},
  booktitle = {Actes des 8e Rencontres des Étudiants Chercheurs en Informatique pour le Traitement Automatique des Langues},
  month     = {April},
  year      = {2006},
  address   = {Leuven, Belgique},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {661--669},
  url       = {http://www.atala.org/taln_archives/RECITAL/RECITAL-2006/recital-2006-long-002},
  language  = {french},
  resume    = {Cet article propose, au travers des résultats de différentes expériences sur la couverture des lexiques informatisés, de montrer que l’incomplétude lexicale est un phénomène constant dans tous les lexiques de TAL, mais que les mots inconnus eux-mêmes varient grandement selon les outils. Nous montrons également que la constance de cette incomplétude est étroitement liée à la créativité lexicale de la langue.},
  abstract  = {Through various experiments on computational lexica, we show that lexical incompleteness is a regular phenomenon across NLP lexica, but that the unknown words themselves vary strongly according to the individual lexicon. We also demonstrate that the regularity of incompleteness is closely related to lexical creativity within individual language.},
  motscles  = {lexique informatisé, incomplétude lexicale, mots inconnus, typologie},
  keywords  = {computational lexicon, lexical incompleteness, unknown words, typology},
}

@inproceedings{kervajan:2006:RECITAL,
  author    = {Kervajan, Loïc},
  title     = {Problèmes de représentation de la Langue des Signes Française en vue du traitement automatique},
  booktitle = {Actes des 8e Rencontres des Étudiants Chercheurs en Informatique pour le Traitement Automatique des Langues},
  month     = {April},
  year      = {2006},
  address   = {Leuven, Belgique},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {670--679},
  url       = {http://www.atala.org/taln_archives/RECITAL/RECITAL-2006/recital-2006-long-003},
  language  = {french},
  resume    = {Nous proposons dans cet article une description de la Langue des Signes Française dans le but de traduire des énoncés courts du français et de les faire signer par un personnage de synthèse. Cette description pose en préalable la question de la transcription des éléments d’une langue dont le signal n’est pas linéaire. Il s’agit ensuite de repérer les différentes couches linguistiques et la forme de leurs unités constitutives en vue de la répartition des tâches informatiques : la synthèse de gestes nécessite un traitement des éléments constitutifs du geste et la génération syntaxique doit pouvoir manipuler des morphèmes.},
  abstract  = {In this article, we propose a description of the French Sign Language. The aim is to translate short sentences from French and to realize them with a virtual agent. The first question is how to transcribe the elements of a language which the signal is not linear. We propose to observe the different linguistic levels and the shape of their units in order to distinguish the processing levels : gesture synthesis needs to manipulate the constitutive elements of gestures and morpheme units are required in syntactic generation.},
  motscles  = {langue des Signes Française, descritpion du geste, morphème, kinème, phonème, chérème},
  keywords  = {french Sign Language, description of gesture, morpheme, kineme, phoneme, chereme},
}

@inproceedings{kevers:2006:RECITAL,
  author    = {Kevers, Laurent},
  title     = {L’information biographique : modélisation, extraction et organisation en base de connaissances},
  booktitle = {Actes des 8e Rencontres des Étudiants Chercheurs en Informatique pour le Traitement Automatique des Langues},
  month     = {April},
  year      = {2006},
  address   = {Leuven, Belgique},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {680--689},
  url       = {http://www.atala.org/taln_archives/RECITAL/RECITAL-2006/recital-2006-long-004},
  language  = {french},
  resume    = {L’extraction et la valorisation de données biographiques contenues dans les dépêches de presse est un processus complexe. Pour l’appréhender correctement, une définition complète, précise et fonctionnelle de cette information est nécessaire. Or, la difficulté que l’on rencontre lors de l’analyse préalable de la tâche d’extraction réside dans l’absence d’une telle définition. Nous proposons ici des conventions dans le but d’en développer une. Le principal concept utilisé pour son expression est la structuration de l’information sous forme de triplets \{sujet, relation, objet\}. Le début de définition ainsi construit est exploité lors de l’étape d’extraction d’informations par transducteurs à états finis. Il permet également de suggérer une solution d’implémentation pour l’organisation des données extraites en base de connaissances.},
  abstract  = {Extraction and valorization of biographical information from news wires is a complex task. In order to handle it correctly, it is necessary to have a complete, accurate and functional definition. The preliminary analysis of the extraction task reveals the lack of such a definition. This article proposes some conventions to develop it. Information modelling as triples \{subject, relation, object\} is the main concept used at this level. This incomplete definition can be used during the information extraction step. It also allows to suggest some implementation solutions for data organisation as a knowledge base.},
  motscles  = {information biographique, modélisation, extraction d’information, transducteur à états finis, entité nommée, relation, base de connaissances},
  keywords  = {biographical information, modelling, information extraction, finite state transducers, named entities, relation, knowledge base},
}

@inproceedings{laignelet:2006:RECITAL,
  author    = {Laignelet, Marion},
  title     = {Repérage de segments d’information évolutive dans des documents de type encyclopédique},
  booktitle = {Actes des 8e Rencontres des Étudiants Chercheurs en Informatique pour le Traitement Automatique des Langues},
  month     = {April},
  year      = {2006},
  address   = {Leuven, Belgique},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {690--699},
  url       = {http://www.atala.org/taln_archives/RECITAL/RECITAL-2006/recital-2006-long-005},
  language  = {french},
  resume    = {Dans cet article, nous cherchons à caractériser linguistiquement des segments textuels définis pragmatiquement, relativement à des besoins de réédition de documents et au sein desquels l’information est susceptible d’évoluer dans le temps. Sur la base d’un corpus de textes encyclopédiques en français, nous analysons la distribution de marqueurs textuels et discursifs et leur pertinence en nous focalisant principalement sur un traitement sémantique particulier de la temporalité.},
  abstract  = {In this paper, we present a method for the automatic identification of text segments carrying information likely to evolve in the course of time. In a corpus of extracts from French encyclopediae, we analyse relevant text and discourse markers focussing on a specific approach to temporal semantics.},
  motscles  = {repérage automatique d’informations évolutives, TAL, corpus de textes encyclopédiques, marqueurs textuels et discursifs},
  keywords  = {automatic identification of evolving information, NLP, corpus of encyclopaedic texts, textual and discourse markers},
}

@inproceedings{leon:2006:RECITAL,
  author    = {Léon, Stéphanie},
  title     = {Acquisition automatique de traductions de termes complexes par comparaison de « mondes lexicaux » sur le Web},
  booktitle = {Actes des 8e Rencontres des Étudiants Chercheurs en Informatique pour le Traitement Automatique des Langues},
  month     = {April},
  year      = {2006},
  address   = {Leuven, Belgique},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {700--708},
  url       = {http://www.atala.org/taln_archives/RECITAL/RECITAL-2006/recital-2006-long-006},
  language  = {french},
  resume    = {Nous présentons une méthode de traduction automatique de termes complexes pour la construction de ressources bilingues français/anglais, basée principalement sur une comparaison entre « mondes lexicaux » (ensemble de co-occurrents), à partir du Web. Nous construisons les mondes lexicaux des termes français sur le Web. Puis, nous générons leurs traductions candidates via un dictionnaire bilingue électronique et constituons les mondes lexicaux de toutes les traductions candidates. Nous comparons enfin les mondes lexicaux français et anglais afin de valider la traduction adéquate par filtres statistiques. Notre évaluation sur 10 mots français très polysémiques montre que l’exploitation des mondes lexicaux des termes complexes sur le Web permet une acquisition automatique de traductions avec une excellente précision.},
  abstract  = {We present a method for automatic translation (French/English) of complex terms using a comparison between French and English “lexical worlds” (i.e. the set of cooccurring words) on the Web. We first generate the French lexical worlds. We then generate their potential translations by means of an electronic dictionary and extract the lexical worlds corresponding for English. Finally we compare the French and English lexical worlds in order to select the adequate translation. Our evaluation on 10 highly polysemous French words shows that a very high precision can be achieved using this technique.},
  motscles  = {termes complexes, traduction automatique, mondes lexicaux, World Wide Web},
  keywords  = {complex terms, machine translation, lexical worlds, World Wide Web},
}

@inproceedings{peirsman:2006:RECITAL,
  author    = {Peirsman, Yves},
  title     = {},
  booktitle = {Actes des 8e Rencontres des Étudiants Chercheurs en Informatique pour le Traitement Automatique des Langues},
  month     = {April},
  year      = {2006},
  address   = {Leuven, Belgique},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {709--718},
  url       = {http://www.atala.org/taln_archives/RECITAL/RECITAL-2006/recital-2006-long-007},
  language  = {french},
  note      = {Unsupervised approaches to metonymy recognition},
  resume    = {Jusqu’à présent, la question de la reconnaissance automatique de métonymies a souvent été abordée avec des approches supervisées. Toutefois, ces approches nécessitent l’annotation d’un nombre important d’occurrences d’apprentissage et, dès lors, elles empêchent le développement d’un système de reconnaissance de métonymies à grande échelle. Cet article étudie la possibilité de résoudre ce problème du goulot d’étranglement de l’acquisition des connaissances en recourant à des techniques d’apprentissages non supervisées. Bien que la technique en question, l’algorithme de Schütze (1998), soit souvent appliquée en désambiguïsation sémantique, je montrerai qu’elle s’avère trop peu solide pour le cas spécifique de la reconnaissance de métonymies. À cet effet, je propose d’étudier l’influence de quatre variables sur les performances de la technique non supervisée, à savoir le type de données, la taille de la fenêtre d’observation, l’application de la décomposition en valeurs singulières (SVD) et le type de sélection de propriétés.},
  abstract  = {To this day, the automatic recognition of metonymies has generally been addressed with supervised approaches. However, these require the annotation of a large number of training instances and hence, hinder the development of a wide-scale metonymy recognition system. This paper investigates if this knowledge acquisition bottleneck in metonymy recognition can be resolved by the application of unsupervised learning. Although the investigated technique, Schütze’s (1998) algorithm, enjoys considerable popularity in Word Sense Disambiguation, I will show that it is not yet robust enough to tackle the specific case of metonymy recognition. In particular, I will study the influence on its performance of four variables—the type of data set, the size of the context window, the application of SVD and the type of feature selection.},
  motscles  = {reconnaissance de métonymies, désambiguïsation sémantique, apprentissage par machine non supervisé},
  keywords  = {metonymy recognition, word sense disambiguation, unsupervised machine learning},
}

@inproceedings{trichetallaire:2006:RECITAL,
  author    = {Trichet-Allaire, Sarah},
  title     = {Une première approche de l’utilisation des chaînes coréférentielles pour la détection des variantes anaphoriques de termes},
  booktitle = {Actes des 8e Rencontres des Étudiants Chercheurs en Informatique pour le Traitement Automatique des Langues},
  month     = {April},
  year      = {2006},
  address   = {Leuven, Belgique},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {719--728},
  url       = {http://www.atala.org/taln_archives/RECITAL/RECITAL-2006/recital-2006-long-008},
  language  = {french},
  resume    = {Cet article traite de l’utilité à détecter une chaîne coréférentielle de termes complexes afin d’améliorer la détection de variations de ce même terme complexe. Nous implémentons pour cela un programme permettant de détecter le nombre de variantes anaphoriques d’un terme complexe ainsi que le nombre de variantes anaphoriques de termes dans un texte scientifique. Ces deux fonctionnalités sont développées avec une ancrage dans une chaîne coréférentielle et en dehors de toute chaîne coréférentielle, afin de pouvoir évaluer l’efficacité de cette méthode.},
  abstract  = {This article discusses the usefulness to detect a coreferential chain of complex terms in order to improve detection of variations of this same complex term. We implement a program for this purpose, allowing the detection of the number of anaphoric variants of a complex term, as well as the number of anaphoric variants of terms in scientific texts. These two functions are developed either with ou without the anchoring in a coreferential chain.},
  motscles  = {chaîne coréférentielle, détection automatique, variante de terme, anaphore nominale},
  keywords  = {coreferential chain, automatical detection, term variation, nominal anaphora},
}

@inproceedings{chaumartin:2006:RECITAL,
  author    = {Chaumartin, François-Régis},
  title     = {Construction automatique d’une interface syntaxe / sémantique utilisant des ressources de large couverture en langue anglaise},
  booktitle = {Actes des 8e Rencontres des Étudiants Chercheurs en Informatique pour le Traitement Automatique des Langues},
  month     = {April},
  year      = {2006},
  address   = {Leuven, Belgique},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {729--735},
  url       = {http://www.atala.org/taln_archives/RECITAL/RECITAL-2006/recital-2006-poster-001},
  language  = {french},
  resume    = {Nous décrivons ici une approche pour passer d’une représentation syntaxique (issue d’une analyse grammaticale) à une représentation sémantique (sous forme de prédicats). Nous montrons ensuite que la construction de cette interface est automatisable. Nous nous appuyons sur l’interopérabilité de plusieurs ressources couvrant des aspects d’ordre syntaxique (Link Grammar Parser), lexical (WordNet) et syntaxico-sémantique (VerbNet) de la langue anglaise. L’utilisation conjointe de ces ressources de large couverture permet d’obtenir une désambiguïsation syntaxique et lexicale au moins partielle.},
  abstract  = {We describe a way to transform a syntactic structure (generated by a syntactic parser for English) into a semantic form (in the form of predicates). We then show that the construction of such an interface can be automated. Our approach is based on the interoperability between several resources, covering syntactical (Link Grammar Parser), lexical (WordNet) and semantic (VerbNet) aspects of English. The joint use of these broad-coverage resources leads to a lexical and syntactical disambiguation (at least partially).},
  motscles  = {compréhension de texte, analyse syntaxique, désambiguïsation, interface syntaxe-sémantique, reconnaissance de schémas, logique des prédicats, unification, rôles thématiques, contraintes de sélections, VerbNet, WordNet, Link Grammar Parser},
  keywords  = {text understanding, parsing, disambiguation, syntax/semantic interface, pattern recognition, predicate calculus, unification, thematic roles, selectional restrictions, VerbNet, WordNet, Link Grammar Parser},
}

@inproceedings{filhol:2006:RECITAL,
  author    = {Filhol, Michael},
  title     = {Une approche géometrique pour la modélisation des lexiques en langues signées},
  booktitle = {Actes des 8e Rencontres des Étudiants Chercheurs en Informatique pour le Traitement Automatique des Langues},
  month     = {April},
  year      = {2006},
  address   = {Leuven, Belgique},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {736--741},
  url       = {http://www.atala.org/taln_archives/RECITAL/RECITAL-2006/recital-2006-poster-002},
  language  = {french},
  resume    = {Le contexte est celui d’une plateforme de génération automatique d’énoncés en langue signée, réalisés par un avatar 3D. Il existe quelques uns de ces systèmes aujourd’hui, par exemple le projet VisiCast (Hanke, 2002). Nous revenons ici sur les systèmes de description utilisés pour les unités gestuelles impliquées dans les énoncés, fondés sur un langage peu flexible et guère adaptatif. Nous proposons ensuite une nouvelle approche, constructiviste et géométrique, avec l’objectif de rendre la description des signes des lexiques signés plus adéquate, et par là améliorer leur intégration dans les discours générés.},
  abstract  = {In the prospect of a system for automatic sign language generation by virtual characters, we first look at the present systems (e.g. the VisiCast project), focusing on the way they describe the gestural units involved in discourse building. We show the lack of flexibility in these models, and introduce a new constructivist approach, based on space geometry, so as to make lexeme description more appropriate and context-sensitive.},
  motscles  = {langue des signes, représentation, lexique, paramètres, géométrie spatiale},
  keywords  = {sign language, representation, lexicon, parameters, space geometry},
}

@inproceedings{loukil:2006:RECITAL,
  author    = {Loukil, Noureddine},
  title     = {Une proposition de représentation normalisée des lexiques HPSG},
  booktitle = {Actes des 8e Rencontres des Étudiants Chercheurs en Informatique pour le Traitement Automatique des Langues},
  month     = {April},
  year      = {2006},
  address   = {Leuven, Belgique},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {742--747},
  url       = {http://www.atala.org/taln_archives/RECITAL/RECITAL-2006/recital-2006-poster-003},
  language  = {french},
  resume    = {L’interopérabilité entre les lexiques des grammaires d’unification passe par l’adoption d’une représentation normalisée de ces ressources. Dans ce papier, nous proposons l’utilisation de LMF pour établir la standardisation des ressources lexicales en HPSG. Nous présentons LMF d’une manière sommaire et nous détaillons son utilisation pour coder les entrées lexicales d’un lexique HPSG.},
  abstract  = {The interoperability between unification grammar lexica can be established by adopting a normalized representation for those resources. In this paper, we propose the use of the Lexical Markup Framework to establish the standardization of HPSG lexical resources. We present an overview of LMF and we detail its use to code the lexical entries of an HPSG lexicon.},
  motscles  = {lexique syntaxique, HPSG, Lexical Markup Framework, projection lexicale},
  keywords  = {syntactic lexicon, HPSG, Lexical Markup Framework, Lexical mapping},
}

@inproceedings{mesfar:2006:RECITAL,
  author    = {Mesfar, Slim},
  title     = {Analyse lexicale et morphologique de l’arabe standard utilisant la plateforme linguistique NooJ},
  booktitle = {Actes des 8e Rencontres des Étudiants Chercheurs en Informatique pour le Traitement Automatique des Langues},
  month     = {April},
  year      = {2006},
  address   = {Leuven, Belgique},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {748--754},
  url       = {http://www.atala.org/taln_archives/RECITAL/RECITAL-2006/recital-2006-poster-004},
  language  = {french},
  resume    = {Cet article décrit un système de construction du lexique et d’analyse morphologique pour l’arabe standard. Ce système profite des apports des modèles à états finis au sein de l’environnement linguistique de développement NooJ pour traiter aussi bien les textes voyellés que les textes partiellement ou non voyellés. Il se base sur une analyse morphologique faisant appel à des règles grammaticales à large couverture.},
  abstract  = {This article describes the construction of a lexicon and a morphological description for standard Arabic. This system uses finite state technology, within the linguistic developmental environment NooJ, to parse vowelled texts, as well as partially vowelled and unvowelled ones. It is based on large-coverage morphological grammars covering all grammatical rules.},
  motscles  = {TALN, NooJ, langue arabe, analyse lexicale, analyse morphologique, grammaire morphologique, agglutination, voyellation},
  keywords  = {NLP, NooJ, arabic language, lexical analysis, morphological analysis, morphological grammar, agglutination, vocalisation},
}

@inproceedings{smits:2006:RECITAL,
  author    = {Smits, Grégory},
  title     = {Contrôle dynamique multicritère des résultats d’une chaîne de TAL},
  booktitle = {Actes des 8e Rencontres des Étudiants Chercheurs en Informatique pour le Traitement Automatique des Langues},
  month     = {April},
  year      = {2006},
  address   = {Leuven, Belgique},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {755--760},
  url       = {http://www.atala.org/taln_archives/RECITAL/RECITAL-2006/recital-2006-poster-005},
  language  = {french},
  resume    = {Le traitement linguistique d’un énoncé écrit conduit le plus souvent à la prise en compte d’interprétations concurrentes, ainsi qu’à la création d’ambiguïtés artificielles. Le contrôle de ces points d’embarras est indispensable pour garantir une efficacité et une précision convenable du processus d’analyse. L’approche décrite dans ce document exploite le paradigme de l’aide multicritère à la décision dans un contexte de TALN. Elle consiste à optimiser l’apport des méthodes spécifiques de contrôle dans une chaîne de traitement.},
  abstract  = {The linguistic processing of textual utterances often leads to several concurrent interpretations that can raise artificial ambiguities. Controlling these awkward points is essential in order to obtain acceptable efficiency and precision of analysis processes. The approach described in this document makes use of the multicriteria decision aid paradigm in a context of NLP. It relies on the optimisation of the use of specific control methods in a processing chain.},
  motscles  = {hypothèses concurrentes, architecture de contrôle, aide multicritère à la décision},
  keywords  = {concurrent hypothesis, control architecture, multicriteria decision aid},
}

@inproceedings{tardif:2006:RECITAL,
  author    = {Tardif, Olivier},
  title     = {Résoudre la coréférence à l’aide d’un classifieur bayésien naïf},
  booktitle = {Actes des 8e Rencontres des Étudiants Chercheurs en Informatique pour le Traitement Automatique des Langues},
  month     = {April},
  year      = {2006},
  address   = {Leuven, Belgique},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {761--766},
  url       = {http://www.atala.org/taln_archives/RECITAL/RECITAL-2006/recital-2006-poster-006},
  language  = {french},
  resume    = {Nous présentons ici les bases d’une méthode de résolution de la coréférence entre les expressions nominales désignant des entités nommées. Nous comptons appliquer cet algorithme sur un corpus de textes journalistiques ; certains aspects de ce que l’on pourrait nommer les « facteurs de coréférence » dans ces textes nous amènent à favoriser l’utilisation de méthodes statistiques pour accomplir cette tâche. Nous décrivons l’algorithme de résolution de la coréférence mis en oeuvre, constitué d’un classifieur bayésien naïf.},
  abstract  = {In this paper we describe a coreference resolution algorithm for nominal expressions denoting named entities. The corpus used consists of French newspaper texts. In these texts, some properties of what we call “coreference factors” lead us to prefer a statistical approach for the task. We describe a coreference resolution algorithm consisting in the implementation of a naive Bayes classifier.},
  motscles  = {classifieur bayésien naïf, coréférence, entités nommées},
  keywords  = {naive Bayes classifier, coreference, named entities},
}

@inproceedings{vandecruys:2006:RECITAL,
  author    = {Van de Cruys, Tim},
  title     = {},
  booktitle = {Actes des 8e Rencontres des Étudiants Chercheurs en Informatique pour le Traitement Automatique des Langues},
  month     = {April},
  year      = {2006},
  address   = {Leuven, Belgique},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {767--772},
  url       = {http://www.atala.org/taln_archives/RECITAL/RECITAL-2006/recital-2006-poster-007},
  language  = {french},
  note      = {The Application of Singular Value Decomposition to Dutch Noun-Adjective Matrices},
  resume    = {L’apprentissage automatique de la sémantique est un sujet assez populaire dans le domaine du traitement automatique du langage. Beaucoup de recherches ont été éffectuées en comparant des contextes syntaxiques similaires. On peut, par exemple, trouver des substantifs d’un champ sémantique similaire en examinant les adjectifs avec lesquels ils sont souvent en relation. Si on opte pour cette méthode, il y a néanmoins deux problèmes qui se posent, à savoir la complexité computationnelle et l’insuffisance des données. Cet article décrit l’application d’une technique mathématique, la décomposition en valeurs singulières. Cette technique a été appliquée au domaine de Recherche d’Information avec des résultats favorables. On se demande s’il est possible de trouver, grâce à la technique, des dimensions sémantiques latentes à l’espace d’adjectifs reduit avec lesquelles on peut faire un groupement qui est aussi bon ou meilleur que le groupement original.},
  abstract  = {Automatic acquisition of semantics from text has received quite some attention in natural language processing. A lot of research has been done by looking at syntactically similar contexts. For example, semantically related nouns can be clustered by looking at the collocating adjectives. There are, however, two major problems with this approach : computational complexity and data sparseness. This paper describes the application of a mathematical technique called singular value decomposition, which has been succesfully applied in Information Retrieval to counter these problems. It is investigated whether this technique is also able to cluster nouns according to latent semantic dimensions in a reduced adjective space.},
  motscles  = {analyse sémantique, clustering sémantique, LSA},
  keywords  = {semantic analysis, semantic clustering, LSA},
}

@inproceedings{zouaghi-zrigui-benahmed:2006:RECITAL,
  author    = {Zouaghi, Anis and Zrigui, Mounir and Ben Ahmed, Mohamed},
  title     = {Calcul du sens des mots arabes ambigus},
  booktitle = {Actes des 8e Rencontres des Étudiants Chercheurs en Informatique pour le Traitement Automatique des Langues},
  month     = {April},
  year      = {2006},
  address   = {Leuven, Belgique},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/RECITAL/RECITAL-2006/recital-2006-poster-008},
  language  = {french},
  resume    = {Nous présentons dans cet article un analyseur sémantique pour la langue arabe. Cet analyseur contribue à la sélection du sens adéquat parmi l’ensemble des sens possibles que peut recevoir un mot hors contexte. Pour atteindre cet objectif, nous proposons un modèle vectoriel qui permet de lever les ambiguïtés locales au niveau de la phrase et celles relevant du domaine. Ce modèle est inspiré des modèles vectoriels très utilisés dans le domaine de la recherche documentaire.},
  abstract  = {This article describes a semantic analyzer for the Arabic language. This analyzer contributes to the selection of the adequate meaning among the set of possible meanings for a given word. To achieve this goal, we propose a vectorial model that allows lifting local ambiguities on the level of the sentence and those concerning semantic domains. This model is inspired from vector models commonly used in information retrieval.},
  motscles  = {désambiguïsation sémantique, modèle vectoriel, traitement de la parole arabe, influence sémantique},
  keywords  = {semantic disambiguation, vector models, processing of Arabic speech, pertinent context, semantic influence},
}