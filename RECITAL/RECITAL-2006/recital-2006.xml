<?xml version="1.0" encoding="UTF-8"?>
<!--
	Fichier construit à partir des fichiers pdfs de la conférence
-->
<conference>
	<edition>
		<acronyme>RECITAL'2006</acronyme>
		<titre>8e Rencontres des Étudiants Chercheurs en Informatique pour le Traitement Automatique des Langues</titre>
		<ville>Leuven</ville>
		<pays>Belgique</pays>
		<dateDebut>2006-04-10</dateDebut>
		<dateFin>2006-04-13</dateFin>
		<presidents>
			<president>
				<prenom>Piet</prenom>
				<nom>Mertens</nom>
			</president>
			<president>
				<prenom>Cédrick</prenom>
				<nom>Fairon</nom>
			</president>
			<president>
				<prenom>Anne</prenom>
				<nom>Dister</nom>
			</president>
			<president>
				<prenom>Patrick</prenom>
				<nom>Watrin</nom>
			</president>
		</presidents>
		<typeArticles>
			<type id="long">Communications orales</type>
			<type id="poster">Posters</type>
		</typeArticles>
		<siteWeb>http://cental.fltr.ucl.ac.be/~taln2006/</siteWeb>
	</edition>
	<articles>
		<article id="recital-2006-long-001" session="">
			<auteurs>
				<auteur>
					<prenom>Vincent</prenom>
					<nom>Archer</nom>
					<email>vincent.archer@imag.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">GETA-CLIPS-IMAG (UJF &amp; CNRS)</affiliation>
			</affiliations>
			<titre>Acquisition semi-automatique de collocations à partir de corpus monolingues et multilingues comparables</titre>
			<type>long</type>
			<pages>651-660</pages>
			<resume>Cet article présente une méthode d’acquisition semi-automatique de collocations. Notre extraction monolingue estime pour chaque co-occurrence sa capacité à être une collocation, d’après une mesure statistique modélisant une caractéristique essentielle (le fait qu’une collocation se produit plus souvent que par hasard), effectue ensuite un filtrage automatique (en utilisant les vecteurs conceptuels) pour ne retenir que des collocations d’un certain type sémantique, puis effectue enfin un nouveau filtrage à partir de données entrées manuellement. Notre extraction bilingue est effectuée à partir de corpus comparables, et a pour but d’extraire des collocations qui ne soient pas forcément traductions mot à mot l’une de l’autre. Notre évaluation démontre l’intérêt de mêler extraction automatique et intervention manuelle pour acquérir des collocations et ainsi permettre de compléter les bases lexicales multilingues.</resume>
			<mots_cles>collocations, acquisition semi-automatique, corpus comparables</mots_cles>
			<title></title>
			<abstract>This paper presents a method for the semi-automatic acquisition of collocations. Our monolingual extraction estimates the ability of each co-occurrence to be a collocation, using a statitical measure which represents an essential property (the fact that a collocation occurs more often than would be expected by chance), then makes an automatic filtering (using conceptual vectors) to keep only one semantic type of collocation, and finally makes a new filtering, using manually entered data. Our bilingual extraction uses comparable corpora, and is aiming to extract collocations which are not necessarily word-to-word translations. Our evaluation shows the interest of mixing automatic extraction and manual intervention to obtain collocations and, in this manner, to complete multilingual lexical databases.</abstract>
			<keywords>collocations, semi-automatic acquisition, comparable corpora</keywords>
		</article>
		<article id="recital-2006-long-002" session="">
			<auteurs>
				<auteur>
					<prenom>Bruno</prenom>
					<nom>Cartoni</nom>
					<email>bruno.cartoni@eti.unige.ch</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université de Genève – TIM/ISSCO ETI</affiliation>
			</affiliations>
			<titre>Constance et variabilité de l’incomplétude lexicale</titre>
			<type>long</type>
			<pages>661-669</pages>
			<resume>Cet article propose, au travers des résultats de différentes expériences sur la couverture des lexiques informatisés, de montrer que l’incomplétude lexicale est un phénomène constant dans tous les lexiques de TAL, mais que les mots inconnus eux-mêmes varient grandement selon les outils. Nous montrons également que la constance de cette incomplétude est étroitement liée à la créativité lexicale de la langue.</resume>
			<mots_cles>lexique informatisé, incomplétude lexicale, mots inconnus, typologie</mots_cles>
			<title></title>
			<abstract>Through various experiments on computational lexica, we show that lexical incompleteness is a regular phenomenon across NLP lexica, but that the unknown words themselves vary strongly according to the individual lexicon. We also demonstrate that the regularity of incompleteness is closely related to lexical creativity within individual language.</abstract>
			<keywords>computational lexicon, lexical incompleteness, unknown words, typology</keywords>
		</article>
		<article id="recital-2006-long-003" session="">
			<auteurs>
				<auteur>
					<prenom>Loïc</prenom>
					<nom>Kervajan</nom>
					<email>loickervajan@wanadoo.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université de Provence, DELIC</affiliation>
				<affiliation affiliationId="2">France Télécom R&amp;D/TECH/EASY/LN</affiliation>
			</affiliations>
			<titre>Problèmes de représentation de la Langue des Signes Française en vue du traitement automatique</titre>
			<type>long</type>
			<pages>670-679</pages>
			<resume>Nous proposons dans cet article une description de la Langue des Signes Française dans le but de traduire des énoncés courts du français et de les faire signer par un personnage de synthèse. Cette description pose en préalable la question de la transcription des éléments d’une langue dont le signal n’est pas linéaire. Il s’agit ensuite de repérer les différentes couches linguistiques et la forme de leurs unités constitutives en vue de la répartition des tâches informatiques : la synthèse de gestes nécessite un traitement des éléments constitutifs du geste et la génération syntaxique doit pouvoir manipuler des morphèmes.</resume>
			<mots_cles>langue des Signes Française, descritpion du geste, morphème, kinème, phonème, chérème</mots_cles>
			<title></title>
			<abstract>In this article, we propose a description of the French Sign Language. The aim is to translate short sentences from French and to realize them with a virtual agent. The first question is how to transcribe the elements of a language which the signal is not linear. We propose to observe the different linguistic levels and the shape of their units in order to distinguish the processing levels : gesture synthesis needs to manipulate the constitutive elements of gestures and morpheme units are required in syntactic generation.</abstract>
			<keywords>french Sign Language, description of gesture, morpheme, kineme, phoneme, chereme</keywords>
		</article>
		<article id="recital-2006-long-004" session="">
			<auteurs>
				<auteur>
					<prenom>Laurent</prenom>
					<nom>Kevers</nom>
					<email>laurent.kevers@uclouvain.be</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université catholique de Louvain – CENTAL</affiliation>
			</affiliations>
			<titre>L’information biographique : modélisation, extraction et organisation en base de connaissances</titre>
			<type>long</type>
			<pages>680-689</pages>
			<resume>L’extraction et la valorisation de données biographiques contenues dans les dépêches de presse est un processus complexe. Pour l’appréhender correctement, une définition complète, précise et fonctionnelle de cette information est nécessaire. Or, la difficulté que l’on rencontre lors de l’analyse préalable de la tâche d’extraction réside dans l’absence d’une telle définition. Nous proposons ici des conventions dans le but d’en développer une. Le principal concept utilisé pour son expression est la structuration de l’information sous forme de triplets {sujet, relation, objet}. Le début de définition ainsi construit est exploité lors de l’étape d’extraction d’informations par transducteurs à états finis. Il permet également de suggérer une solution d’implémentation pour l’organisation des données extraites en base de connaissances.</resume>
			<mots_cles>information biographique, modélisation, extraction d’information, transducteur à états finis, entité nommée, relation, base de connaissances</mots_cles>
			<title></title>
			<abstract>Extraction and valorization of biographical information from news wires is a complex task. In order to handle it correctly, it is necessary to have a complete, accurate and functional definition. The preliminary analysis of the extraction task reveals the lack of such a definition. This article proposes some conventions to develop it. Information modelling as triples {subject, relation, object} is the main concept used at this level. This incomplete definition can be used during the information extraction step. It also allows to suggest some implementation solutions for data organisation as a knowledge base.</abstract>
			<keywords>biographical information, modelling, information extraction, finite state transducers, named entities, relation, knowledge base</keywords>
		</article>
		<article id="recital-2006-long-005" session="">
			<auteurs>
				<auteur>
					<prenom>Marion</prenom>
					<nom>Laignelet</nom>
					<email>marion.laignelet@univ-tlse2.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université de Toulouse le Mirail, ERSS, UMR 5610</affiliation>
				<affiliation affiliationId="2">Société Initiales</affiliation>
			</affiliations>
			<titre>Repérage de segments d’information évolutive dans des documents de type encyclopédique</titre>
			<type>long</type>
			<pages>690-699</pages>
			<resume>Dans cet article, nous cherchons à caractériser linguistiquement des segments textuels définis pragmatiquement, relativement à des besoins de réédition de documents et au sein desquels l’information est susceptible d’évoluer dans le temps. Sur la base d’un corpus de textes encyclopédiques en français, nous analysons la distribution de marqueurs textuels et discursifs et leur pertinence en nous focalisant principalement sur un traitement sémantique particulier de la temporalité.</resume>
			<mots_cles>repérage automatique d’informations évolutives, TAL, corpus de textes encyclopédiques, marqueurs textuels et discursifs</mots_cles>
			<title></title>
			<abstract>In this paper, we present a method for the automatic identification of text segments carrying information likely to evolve in the course of time. In a corpus of extracts from French encyclopediae, we analyse relevant text and discourse markers focussing on a specific approach to temporal semantics.</abstract>
			<keywords>automatic identification of evolving information, NLP, corpus of encyclopaedic texts, textual and discourse markers</keywords>
		</article>
		<article id="recital-2006-long-006" session="">
			<auteurs>
				<auteur>
					<prenom>Stéphanie</prenom>
					<nom>Léon</nom>
					<email>leon.stephanie@gmail.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université de Provence – Équipe DELIC</affiliation>
			</affiliations>
			<titre>Acquisition automatique de traductions de termes complexes par comparaison de « mondes lexicaux » sur le Web</titre>
			<type>long</type>
			<pages>700-708</pages>
			<resume>Nous présentons une méthode de traduction automatique de termes complexes pour la construction de ressources bilingues français/anglais, basée principalement sur une comparaison entre « mondes lexicaux » (ensemble de co-occurrents), à partir du Web. Nous construisons les mondes lexicaux des termes français sur le Web. Puis, nous générons leurs traductions candidates via un dictionnaire bilingue électronique et constituons les mondes lexicaux de toutes les traductions candidates. Nous comparons enfin les mondes lexicaux français et anglais afin de valider la traduction adéquate par filtres statistiques. Notre évaluation sur 10 mots français très polysémiques montre que l’exploitation des mondes lexicaux des termes complexes sur le Web permet une acquisition automatique de traductions avec une excellente précision.</resume>
			<mots_cles>termes complexes, traduction automatique, mondes lexicaux, World Wide Web</mots_cles>
			<title></title>
			<abstract>We present a method for automatic translation (French/English) of complex terms using a comparison between French and English “lexical worlds” (i.e. the set of cooccurring words) on the Web. We first generate the French lexical worlds. We then generate their potential translations by means of an electronic dictionary and extract the lexical worlds corresponding for English. Finally we compare the French and English lexical worlds in order to select the adequate translation. Our evaluation on 10 highly polysemous French words shows that a very high precision can be achieved using this technique.</abstract>
			<keywords>complex terms, machine translation, lexical worlds, World Wide Web</keywords>
		</article>
		<article id="recital-2006-long-007" session="">
			<auteurs>
				<auteur>
					<prenom>Yves</prenom>
					<nom>Peirsman</nom>
					<email>yves.peirsman@arts.kuleuven.be</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">University of Leuven Quantitative Lexicology and Variational Linguistics</affiliation>
			</affiliations>
			<titre></titre>
			<type>long</type>
			<pages>709-718</pages>
			<resume>Jusqu’à présent, la question de la reconnaissance automatique de métonymies a souvent été abordée avec des approches supervisées. Toutefois, ces approches nécessitent l’annotation d’un nombre important d’occurrences d’apprentissage et, dès lors, elles empêchent le développement d’un système de reconnaissance de métonymies à grande échelle. Cet article étudie la possibilité de résoudre ce problème du goulot d’étranglement de l’acquisition des connaissances en recourant à des techniques d’apprentissages non supervisées. Bien que la technique en question, l’algorithme de Schütze (1998), soit souvent appliquée en désambiguïsation sémantique, je montrerai qu’elle s’avère trop peu solide pour le cas spécifique de la reconnaissance de métonymies. À cet effet, je propose d’étudier l’influence de quatre variables sur les performances de la technique non supervisée, à savoir le type de données, la taille de la fenêtre d’observation, l’application de la décomposition en valeurs singulières (SVD) et le type de sélection de propriétés.</resume>
			<mots_cles>reconnaissance de métonymies, désambiguïsation sémantique, apprentissage par machine non supervisé</mots_cles>
			<title>Unsupervised approaches to metonymy recognition</title>
			<abstract>To this day, the automatic recognition of metonymies has generally been addressed with supervised approaches. However, these require the annotation of a large number of training instances and hence, hinder the development of a wide-scale metonymy recognition system. This paper investigates if this knowledge acquisition bottleneck in metonymy recognition can be resolved by the application of unsupervised learning. Although the investigated technique, Schütze’s (1998) algorithm, enjoys considerable popularity in Word Sense Disambiguation, I will show that it is not yet robust enough to tackle the specific case of metonymy recognition. In particular, I will study the influence on its performance of four variables—the type of data set, the size of the context window, the application of SVD and the type of feature selection.</abstract>
			<keywords>metonymy recognition, word sense disambiguation, unsupervised machine learning</keywords>
		</article>
		<article id="recital-2006-long-008" session="">
			<auteurs>
				<auteur>
					<prenom>Sarah</prenom>
					<nom>Trichet-Allaire</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université de Nantes, LINA, CNRS LI - Université de Tours</affiliation>
			</affiliations>
			<titre>Une première approche de l’utilisation des chaînes coréférentielles pour la détection des variantes anaphoriques de termes</titre>
			<type>long</type>
			<pages>719-728</pages>
			<resume>Cet article traite de l’utilité à détecter une chaîne coréférentielle de termes complexes afin d’améliorer la détection de variations de ce même terme complexe. Nous implémentons pour cela un programme permettant de détecter le nombre de variantes anaphoriques d’un terme complexe ainsi que le nombre de variantes anaphoriques de termes dans un texte scientifique. Ces deux fonctionnalités sont développées avec une ancrage dans une chaîne coréférentielle et en dehors de toute chaîne coréférentielle, afin de pouvoir évaluer l’efficacité de cette méthode.</resume>
			<mots_cles>chaîne coréférentielle, détection automatique, variante de terme, anaphore nominale</mots_cles>
			<title></title>
			<abstract>This article discusses the usefulness to detect a coreferential chain of complex terms in order to improve detection of variations of this same complex term. We implement a program for this purpose, allowing the detection of the number of anaphoric variants of a complex term, as well as the number of anaphoric variants of terms in scientific texts. These two functions are developed either with ou without the anchoring in a coreferential chain.</abstract>
			<keywords>coreferential chain, automatical detection, term variation, nominal anaphora</keywords>
		</article>
		<article id="recital-2006-poster-001" session="">
			<auteurs>
				<auteur>
					<prenom>François-Régis</prenom>
					<nom>Chaumartin</nom>
					<email>fchaumartin@linguist.jussieu.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Lattice/Talana – Université Paris 7</affiliation>
			</affiliations>
			<titre>Construction automatique d’une interface syntaxe / sémantique utilisant des ressources de large couverture en langue anglaise</titre>
			<type>poster</type>
			<pages>729-735</pages>
			<resume>Nous décrivons ici une approche pour passer d’une représentation syntaxique (issue d’une analyse grammaticale) à une représentation sémantique (sous forme de prédicats). Nous montrons ensuite que la construction de cette interface est automatisable. Nous nous appuyons sur l’interopérabilité de plusieurs ressources couvrant des aspects d’ordre syntaxique (Link Grammar Parser), lexical (WordNet) et syntaxico-sémantique (VerbNet) de la langue anglaise. L’utilisation conjointe de ces ressources de large couverture permet d’obtenir une désambiguïsation syntaxique et lexicale au moins partielle.</resume>
			<mots_cles>compréhension de texte, analyse syntaxique, désambiguïsation, interface syntaxe-sémantique, reconnaissance de schémas, logique des prédicats, unification, rôles thématiques, contraintes de sélections, VerbNet, WordNet, Link Grammar Parser</mots_cles>
			<title></title>
			<abstract>We describe a way to transform a syntactic structure (generated by a syntactic parser for English) into a semantic form (in the form of predicates). We then show that the construction of such an interface can be automated. Our approach is based on the interoperability between several resources, covering syntactical (Link Grammar Parser), lexical (WordNet) and semantic (VerbNet) aspects of English. The joint use of these broad-coverage resources leads to a lexical and syntactical disambiguation (at least partially).</abstract>
			<keywords>text understanding, parsing, disambiguation, syntax/semantic interface, pattern recognition, predicate calculus, unification, thematic roles, selectional restrictions, VerbNet, WordNet, Link Grammar Parser</keywords>
		</article>
		<article id="recital-2006-poster-002" session="">
			<auteurs>
				<auteur>
					<prenom>Michael</prenom>
					<nom>Filhol</nom>
					<email>michael.filhol@limsi.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIMSI-CNRS</affiliation>
				<affiliation affiliationId="2">Université Paris 11</affiliation>
			</affiliations>
			<titre>Une approche géometrique pour la modélisation des lexiques en langues signées</titre>
			<type>poster</type>
			<pages>736-741</pages>
			<resume>Le contexte est celui d’une plateforme de génération automatique d’énoncés en langue signée, réalisés par un avatar 3D. Il existe quelques uns de ces systèmes aujourd’hui, par exemple le projet VisiCast (Hanke, 2002). Nous revenons ici sur les systèmes de description utilisés pour les unités gestuelles impliquées dans les énoncés, fondés sur un langage peu flexible et guère adaptatif. Nous proposons ensuite une nouvelle approche, constructiviste et géométrique, avec l’objectif de rendre la description des signes des lexiques signés plus adéquate, et par là améliorer leur intégration dans les discours générés.</resume>
			<mots_cles>langue des signes, représentation, lexique, paramètres, géométrie spatiale</mots_cles>
			<title></title>
			<abstract>In the prospect of a system for automatic sign language generation by virtual characters, we first look at the present systems (e.g. the VisiCast project), focusing on the way they describe the gestural units involved in discourse building. We show the lack of flexibility in these models, and introduce a new constructivist approach, based on space geometry, so as to make lexeme description more appropriate and context-sensitive.</abstract>
			<keywords>sign language, representation, lexicon, parameters, space geometry</keywords>
		</article>
		<article id="recital-2006-poster-003" session="">
			<auteurs>
				<auteur>
					<prenom>Noureddine</prenom>
					<nom>Loukil</nom>
					<email>noureddine.loukil@isimsf.rnu.tn</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Laboratoire de Recherche en Informatique et Multimédia</affiliation>
			</affiliations>
			<titre>Une proposition de représentation normalisée des lexiques HPSG</titre>
			<type>poster</type>
			<pages>742-747</pages>
			<resume>L’interopérabilité entre les lexiques des grammaires d’unification passe par l’adoption d’une représentation normalisée de ces ressources. Dans ce papier, nous proposons l’utilisation de LMF pour établir la standardisation des ressources lexicales en HPSG. Nous présentons LMF d’une manière sommaire et nous détaillons son utilisation pour coder les entrées lexicales d’un lexique HPSG.</resume>
			<mots_cles>lexique syntaxique, HPSG, Lexical Markup Framework, projection lexicale</mots_cles>
			<title></title>
			<abstract>The interoperability between unification grammar lexica can be established by adopting a normalized representation for those resources. In this paper, we propose the use of the Lexical Markup Framework to establish the standardization of HPSG lexical resources. We present an overview of LMF and we detail its use to code the lexical entries of an HPSG lexicon.</abstract>
			<keywords>syntactic lexicon, HPSG, Lexical Markup Framework, Lexical mapping</keywords>
		</article>
		<article id="recital-2006-poster-004" session="">
			<auteurs>
				<auteur>
					<prenom>Slim</prenom>
					<nom>Mesfar</nom>
					<email>mesfarslim@yahoo.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université de Franche-Comté – LASELDI</affiliation>
			</affiliations>
			<titre>Analyse lexicale et morphologique de l’arabe standard utilisant la plateforme linguistique NooJ</titre>
			<type>poster</type>
			<pages>748-754</pages>
			<resume>Cet article décrit un système de construction du lexique et d’analyse morphologique pour l’arabe standard. Ce système profite des apports des modèles à états finis au sein de l’environnement linguistique de développement NooJ pour traiter aussi bien les textes voyellés que les textes partiellement ou non voyellés. Il se base sur une analyse morphologique faisant appel à des règles grammaticales à large couverture.</resume>
			<mots_cles>TALN, NooJ, langue arabe, analyse lexicale, analyse morphologique, grammaire morphologique, agglutination, voyellation</mots_cles>
			<title></title>
			<abstract>This article describes the construction of a lexicon and a morphological description for standard Arabic. This system uses finite state technology, within the linguistic developmental environment NooJ, to parse vowelled texts, as well as partially vowelled and unvowelled ones. It is based on large-coverage morphological grammars covering all grammatical rules.</abstract>
			<keywords>NLP, NooJ, arabic language, lexical analysis, morphological analysis, morphological grammar, agglutination, vocalisation</keywords>
		</article>
		<article id="recital-2006-poster-005" session="">
			<auteurs>
				<auteur>
					<prenom>Grégory</prenom>
					<nom>Smits</nom>
					<email>gregory.smits@francetelecom.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">France Télécom division R&amp;D, TECH/EASY/LN</affiliation>
			</affiliations>
			<titre>Contrôle dynamique multicritère des résultats d’une chaîne de TAL</titre>
			<type>poster</type>
			<pages>755-760</pages>
			<resume>Le traitement linguistique d’un énoncé écrit conduit le plus souvent à la prise en compte d’interprétations concurrentes, ainsi qu’à la création d’ambiguïtés artificielles. Le contrôle de ces points d’embarras est indispensable pour garantir une efficacité et une précision convenable du processus d’analyse. L’approche décrite dans ce document exploite le paradigme de l’aide multicritère à la décision dans un contexte de TALN. Elle consiste à optimiser l’apport des méthodes spécifiques de contrôle dans une chaîne de traitement.</resume>
			<mots_cles>hypothèses concurrentes, architecture de contrôle, aide multicritère à la décision</mots_cles>
			<title></title>
			<abstract>The linguistic processing of textual utterances often leads to several concurrent interpretations that can raise artificial ambiguities. Controlling these awkward points is essential in order to obtain acceptable efficiency and precision of analysis processes. The approach described in this document makes use of the multicriteria decision aid paradigm in a context of NLP. It relies on the optimisation of the use of specific control methods in a processing chain.</abstract>
			<keywords>concurrent hypothesis, control architecture, multicriteria decision aid</keywords>
		</article>
		<article id="recital-2006-poster-006" session="">
			<auteurs>
				<auteur>
					<prenom>Olivier</prenom>
					<nom>Tardif</nom>
					<email>o.tardif@francetelecom.com</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">France Télécom, TECH/EASY/LN</affiliation>
				<affiliation affiliationId="2">Université de Provence, CILSH</affiliation>
			</affiliations>
			<titre>Résoudre la coréférence à l’aide d’un classifieur bayésien naïf</titre>
			<type>poster</type>
			<pages>761-766</pages>
			<resume>Nous présentons ici les bases d’une méthode de résolution de la coréférence entre les expressions nominales désignant des entités nommées. Nous comptons appliquer cet algorithme sur un corpus de textes journalistiques ; certains aspects de ce que l’on pourrait nommer les « facteurs de coréférence » dans ces textes nous amènent à favoriser l’utilisation de méthodes statistiques pour accomplir cette tâche. Nous décrivons l’algorithme de résolution de la coréférence mis en oeuvre, constitué d’un classifieur bayésien naïf.</resume>
			<mots_cles>classifieur bayésien naïf, coréférence, entités nommées</mots_cles>
			<title></title>
			<abstract>In this paper we describe a coreference resolution algorithm for nominal expressions denoting named entities. The corpus used consists of French newspaper texts. In these texts, some properties of what we call “coreference factors” lead us to prefer a statistical approach for the task. We describe a coreference resolution algorithm consisting in the implementation of a naive Bayes classifier.</abstract>
			<keywords>naive Bayes classifier, coreference, named entities</keywords>
		</article>
		<article id="recital-2006-poster-007" session="">
			<auteurs>
				<auteur>
					<prenom>Tim</prenom>
					<nom>Van de Cruys</nom>
					<email>t.van.de.cruys@rug.nl</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Rijksuniversiteit Groningen, CLCG</affiliation>
			</affiliations>
			<titre></titre>
			<type>poster</type>
			<pages>767-772</pages>
			<resume>L’apprentissage automatique de la sémantique est un sujet assez populaire dans le domaine du traitement automatique du langage. Beaucoup de recherches ont été éffectuées en comparant des contextes syntaxiques similaires. On peut, par exemple, trouver des substantifs d’un champ sémantique similaire en examinant les adjectifs avec lesquels ils sont souvent en relation. Si on opte pour cette méthode, il y a néanmoins deux problèmes qui se posent, à savoir la complexité computationnelle et l’insuffisance des données. Cet article décrit l’application d’une technique mathématique, la décomposition en valeurs singulières. Cette technique a été appliquée au domaine de Recherche d’Information avec des résultats favorables. On se demande s’il est possible de trouver, grâce à la technique, des dimensions sémantiques latentes à l’espace d’adjectifs reduit avec lesquelles on peut faire un groupement qui est aussi bon ou meilleur que le groupement original.</resume>
			<mots_cles>analyse sémantique, clustering sémantique, LSA</mots_cles>
			<title>The Application of Singular Value Decomposition to Dutch Noun-Adjective Matrices</title>
			<abstract>Automatic acquisition of semantics from text has received quite some attention in natural language processing. A lot of research has been done by looking at syntactically similar contexts. For example, semantically related nouns can be clustered by looking at the collocating adjectives. There are, however, two major problems with this approach : computational complexity and data sparseness. This paper describes the application of a mathematical technique called singular value decomposition, which has been succesfully applied in Information Retrieval to counter these problems. It is investigated whether this technique is also able to cluster nouns according to latent semantic dimensions in a reduced adjective space.</abstract>
			<keywords>semantic analysis, semantic clustering, LSA</keywords>
		</article>
		<article id="recital-2006-poster-008" session="">
			<auteurs>
				<auteur>
					<prenom>Anis</prenom>
					<nom>Zouaghi</nom>
					<email>anis.zouaghi@riadi.rnu.tn</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Mounir</prenom>
					<nom>Zrigui</nom>
					<email>mounir.zrigui@fsm.rnu.tn</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Mohamed</prenom>
					<nom>Ben Ahmed</nom>
					<email>mohamed.benahmed@riadi.rnu.tn</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Unité de Monastir – Labo RIADI</affiliation>
				<affiliation affiliationId="2">Université de la Mannouba – Labo RIADI</affiliation>
			</affiliations>
			<titre>Calcul du sens des mots arabes ambigus</titre>
			<type>poster</type>
			<pages></pages>
			<resume>Nous présentons dans cet article un analyseur sémantique pour la langue arabe. Cet analyseur contribue à la sélection du sens adéquat parmi l’ensemble des sens possibles que peut recevoir un mot hors contexte. Pour atteindre cet objectif, nous proposons un modèle vectoriel qui permet de lever les ambiguïtés locales au niveau de la phrase et celles relevant du domaine. Ce modèle est inspiré des modèles vectoriels très utilisés dans le domaine de la recherche documentaire.</resume>
			<mots_cles>désambiguïsation sémantique, modèle vectoriel, traitement de la parole arabe, influence sémantique</mots_cles>
			<title></title>
			<abstract>This article describes a semantic analyzer for the Arabic language. This analyzer contributes to the selection of the adequate meaning among the set of possible meanings for a given word. To achieve this goal, we propose a vectorial model that allows lifting local ambiguities on the level of the sentence and those concerning semantic domains. This model is inspired from vector models commonly used in information retrieval.</abstract>
			<keywords>semantic disambiguation, vector models, processing of Arabic speech, pertinent context, semantic influence</keywords>
		</article>
	</articles>
</conference>
