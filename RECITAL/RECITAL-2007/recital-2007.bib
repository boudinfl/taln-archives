@proceedings{RECITAL:2007,
  editor    = {Benamara, Farah and Ozdowska, Sylwia},
  title     = {Actes des 9e Rencontres des Étudiants Chercheurs en Informatique pour le Traitement Automatique des Langues},
  month     = {June},
  year      = {2007},
  address   = {Toulouse, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  url       = {http://www.atala.org/taln_archives/RECITAL/RECITAL-2007}
}

@inproceedings{mazuel:2007:RECITAL,
  author    = {Mazuel, Laurent},
  title     = {Utilisation des ontologies pour la modélisation logique d’une commande en langue naturel},
  booktitle = {Actes des 9e Rencontres des Étudiants Chercheurs en Informatique pour le Traitement Automatique des Langues},
  month     = {June},
  year      = {2007},
  address   = {Toulouse, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {427--436},
  url       = {http://www.atala.org/taln_archives/RECITAL/RECITAL-2007/recital-2007-long-001},
  language  = {french},
  resume    = {Dans cet article, nous nous intéressons à l’interprétation de commandes en langue naturelle pour un agent artificiel. Notre architecture repose sur une modélisation logique de la commande pour l’interprétation sémantique, qui permet de capturer la « structure fonctionnelle » de la phrase, c’est-à-dire les rôles des termes les uns par rapport aux autres. Cet article décrit une méthode d’analyse structurelle de surface qui s’appuie sur l’ontologie de l’agent pour construire cette modélisation logique. Nous définissons tout d’abord un algorithme d’ancrage des termes de la commande dans l’ontologie de l’agent puis nous montrons comment s’en servir pour l’analyse de surface. Enfin, nous expliquons brièvement comment notre modélisation peut être utilisée au moment de l’interprétation sémantique des commandes.},
  abstract  = {In this paper, we focus on natural language interaction for artificial agents. Our architecture relies on a command logical model to enhance the semantic interpretation. It allows us to catch the « functional structure » of the user sentence, i.e. each terms compared to each others. This paper describes a partial structural approach which relies on the agent ontology to build a logical form of the sentence. We first define an algorithm to anchor a word from the command in the ontology and we use it to make our partial analysis. Lastly, we explain briefly how to use our model for the semantic interpretation of the user command.},
  motscles  = {commande en langue naturelle, analyse structurelle de surface, modélisation logique, ontologies},
  keywords  = {natural language command, partial structural analysis, logical form, ontologies},
}

@inproceedings{blanchard:2007:RECITAL,
  author    = {Blanchard, Alexia},
  title     = {L’analyse morphologique des réponses d’apprenants},
  booktitle = {Actes des 9e Rencontres des Étudiants Chercheurs en Informatique pour le Traitement Automatique des Langues},
  month     = {June},
  year      = {2007},
  address   = {Toulouse, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {437--446},
  url       = {http://www.atala.org/taln_archives/RECITAL/RECITAL-2007/recital-2007-long-002},
  language  = {french},
  resume    = {Nous présentons une approche empirique de l’évaluation automatique des réponses d’apprenants au sein d’un système d’Apprentissage des Langues Assisté par Ordinateur (ALAO). Nous proposons la mise en place d’un module d’analyse d’erreurs attestées sur corpus qui s’appuie sur des techniques robustes de Traitement Automatique des Langues (TAL). Cet article montre la réalisation d’un module d’analyse de morphologie flexionnelle, en situation hors-contexte, à partir d’un modèle linguistique existant.},
  abstract  = {We present an empirical approach to the automated evaluation of learner's answers in a CALL system (Computer Assisted Language Learning). We suggest the realization of an error parsing module using NLP techniques (Natural Language Processing). The errors stem from a language learners corpus. This article describes the implementation, from an existing linguistic model, of an inflectional context-free morphology parser.},
  motscles  = {TAL, ALAO, détection d’erreurs, morphologie flexionnelle, rétroaction(s)},
  keywords  = {NLP, CALL, errors detection, inflectional morphology, feedback},
}

@inproceedings{seppala:2007:RECITAL,
  author    = {Seppälä, Selja},
  title     = {Repérage automatique de génériques dans les définitions terminographiques},
  booktitle = {Actes des 9e Rencontres des Étudiants Chercheurs en Informatique pour le Traitement Automatique des Langues},
  month     = {June},
  year      = {2007},
  address   = {Toulouse, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {447--456},
  url       = {http://www.atala.org/taln_archives/RECITAL/RECITAL-2007/recital-2007-long-003},
  language  = {french},
  resume    = {Cet article présente une procédure de repérage et de balisage de l’élément générique de la définition terminographique exploitant les caractéristiques formelles du sous-langage définitoire. La procédure, qui comporte quatre étapes, constitue l’une des sous-tâches d’un analyseur (semi-)automatique de la structure conceptuelle des définitions terminographiques, destiné à faciliter l’annotation d’un corpus en vue de l’étude de régularités dans cette structure. La tâche décrite consiste à mettre au point un système d’annotation automatique basé sur le repérage d’indices morphosyntaxiques, sans recourir à d’autres ressources linguistiques informatisées.},
  abstract  = {This article presents a procedure to locate and tag the generic elements of terminographic definitions, taking advantage of the formal characteristics of the definition sublanguage. This four step procedure is part of a larger (semi-)automatic parser of the conceptual structure of terminographic definitions, intended to ease the tagging of a corpus for studying conceptual regularities in definition structure. The method involves the development of an automatic tagging system, based on the identification of morphosyntactic boundary markers, which does not require the use of additional linguistic resources.},
  motscles  = {définition terminographique, annotation automatique, repérage de frontière, indices morphosyntaxiques, sous-langage},
  keywords  = {terminographic definition, automatic tagging, boundary location, morphosyntactic markers, sublanguage},
}

@inproceedings{chaumartin:2007:RECITAL,
  author    = {Chaumartin, François-Régis},
  title     = {Extraction de paraphrases désambiguïsées à partir d’un corpus d’articles encyclopédiques alignés automatiquement},
  booktitle = {Actes des 9e Rencontres des Étudiants Chercheurs en Informatique pour le Traitement Automatique des Langues},
  month     = {June},
  year      = {2007},
  address   = {Toulouse, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {457--466},
  url       = {http://www.atala.org/taln_archives/RECITAL/RECITAL-2007/recital-2007-long-004},
  language  = {french},
  resume    = {Nous décrivons ici comment enrichir automatiquement WordNet en y important des articles encyclopédiques. Ce processus permet de créer des nouvelles entrées, en les rattachant au bon hyperonyme. Par ailleurs, les entrées préexistantes de WordNet peuvent être enrichies de descriptions complémentaires. La répétition de ce processus sur plusieurs encyclopédies permet de constituer un corpus d’articles comparables. On peut ensuite extraire automatiquement des paraphrases à partir des couples d’articles ainsi créés. Grâce à l’application d’une mesure de similarité, utilisant la hiérarchie de verbes de WordNet, les constituants de ces paraphrases peuvent être désambiguïsés.},
  abstract  = {We describe here how to automatically import encyclopedic articles into WordNet. This process makes it possible to create new entries, attached to their appropriate hypernym. In addition, the preexisting entries of WordNet can get enriched with complementary descriptions. Reiterating this process on several encyclopedias makes it possible to constitute a corpus of comparable articles; we can then automatically extract paraphrases from the couples of articles that have been created. The paraphrases components can finally be disambiguated, by means of a similarity measure (using the verbs WordNet hierarchy).},
  motscles  = {extraction de paraphrases, fusion d’articles, mesure de similarité, distance sémantique, identification d’hyperonyme, WordNet, Wikipedia, entités nommées, analyse syntaxique, désambiguïsation lexicale, cadres de sous-catégorisation, apprentissage},
  keywords  = {paraphrases extraction, articles merging, similarity measure, semantic distance, hypernym identification, WordNet, Wikipedia, named entities, syntactic analysis, word sense disambiguation, syntactic frames, unsupervised learning},
}

@inproceedings{jousse:2007:RECITAL,
  author    = {Jousse, Anne-Laure},
  title     = {Extension de l'encodage formel des fonctions lexicales dans le cadre de la Lexicologie Explicative et Combinatoire},
  booktitle = {Actes des 9e Rencontres des Étudiants Chercheurs en Informatique pour le Traitement Automatique des Langues},
  month     = {June},
  year      = {2007},
  address   = {Toulouse, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {469--478},
  url       = {http://www.atala.org/taln_archives/RECITAL/RECITAL-2007/recital-2007-long-005},
  language  = {french},
  resume    = {Dans les ressources dictionnairiques développées à partir du cadre théorique de la Lexicologie Explicative et Combinatoire telles que le DiCo, les relations sémanticolexicales sont modélisées au moyen de fonctions lexicales. Cependant, seulement la majorité d'entre elles (dites standard) répondent véritablement à un encodage formel. Les autres (dites non standard), représentant des relations plus spécifiques à certaines unités lexicales, sont écrites sous la forme d'un encodage hétérogène et très peu formalisé. Par conséquent, certaines relations ne peuvent entrer en ligne de compte dans les traitements automatiques. Nous proposons dans cet article une méthodologie pour la normalisation des fonctions lexicales non standard afin de les rendre exploitables dans des applications telles que l'analyse et la génération de texte. Pour ce faire, nous discutons certains principes théoriques associés à ce formalisme de description et esquissons des propositions pour un traitement global et homogène de l'ensemble des relations décrites dans le DiCo.},
  abstract  = {In the lexicographical products developped within the framework of the Explicative and Combinatorial Lexicology such as the DiCo, the lexico-semantic links are modeled by means of lexical functions. However, only a part of them (called standard) happen to appear as a real formal encoding. The others (called non-standard), which represent links more specific to some lexical units, are written in a heterogeneous and barely formalized way. Therefore, some relations can't be taken into account in automatic processings. We propose, in this paper, a methodology for the normalization of non standard lexical functions in order to make them machine readable in applications such as text-analysis and generation. To complete this work, we discuss some theoretical assumptions drawn upon this formalism and sketch some propositions for a global and homogeneous processing of all the lexical links described in the DiCo.},
  motscles  = {Fonctions lexicales (non standard), modélisation des relations sémanticolexicales, DiCo},
  keywords  = {(non standard) Lexical Function, modelling of lexico-semantic links, DiCo},
}

@inproceedings{choumane:2007:RECITAL,
  author    = {Choumane, Ali},
  title     = {Traitement de désignations orales dans un contexte visuel},
  booktitle = {Actes des 9e Rencontres des Étudiants Chercheurs en Informatique pour le Traitement Automatique des Langues},
  month     = {June},
  year      = {2007},
  address   = {Toulouse, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {479--488},
  url       = {http://www.atala.org/taln_archives/RECITAL/RECITAL-2007/recital-2007-long-006},
  language  = {french},
  resume    = {Nous nous intéressons aux systèmes multimodaux qui utilisent les modes et modalités suivantes : l’oral (et le langage naturel) en entrée et en sortie, le geste en entrée et le visuel en sortie par affichage sur écran. L’usager échange avec le système par un geste et/ou un énoncé oral en langue naturelle. Dans cet échange, encodé sur les différentes modalités, se trouvent l’expression du but de l’usager et la désignation des objets (référents) nécessaires à la réalisation de ce but. Le système doit identifier de manière précise et non ambiguë les objets désignés par l’usager. Nous traitons plus spécialement dans cet article les désignations orales, sans geste, des objets dans le contexte visuel. En effet, l’ensemble du contexte multimodal, dont le mode visuel, influe sur la production de l’entrée de l’usager. Afin d’identifier une désignation produite en s’appuyant sur le contexte visuel, nous proposons un algorithme qui utilise des connaissances « classiques » linguistiques, des connaissances sur les objets manipulés, et des connaissances sur les aspects perceptifs (degré de saillance) associés à ces objets.},
  abstract  = {We are interested about multimodal systems that use the following modes and modalities : speech (and natural language) as input as well as output, gesture as input and visual as output through displaying on the screen. The user exchanges with the system by a gesture and/or an oral statement in natural language. This exchange, encoded on the different modalities, contains the goal of the user and also the designation of objects (referents) necessary to the realization of this goal. The system must identify in a precise and non-ambiguous way the objects designated by the user. In this paper, our main concern is the oral designations, without gesture, of objects in the visual context. Indeed, the whole of the multimodale context including visual mode, influences the production of the user input. In order to identify a designation based on the visual context, we propose an algorithm which uses « traditional » linguistic knowledge, knowledge about manipulated objects and perceptive aspects (degree of salience) associated to these objects.},
  motscles  = {communication homme machine multimodale, référence, saillance},
  keywords  = {multimodal human computer communication, reference, salience},
}

@inproceedings{sitbon:2007:RECITAL,
  author    = {Sitbon, Laurianne},
  title     = {Combinaison de ressources linguistiques pour l’aide à l’accès lexical : étude de faisabilité},
  booktitle = {Actes des 9e Rencontres des Étudiants Chercheurs en Informatique pour le Traitement Automatique des Langues},
  month     = {June},
  year      = {2007},
  address   = {Toulouse, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {489--498},
  url       = {http://www.atala.org/taln_archives/RECITAL/RECITAL-2007/recital-2007-long-007},
  language  = {french},
  resume    = {Cet article propose une évaluation combinée et comparative de 5 ressources (descriptive, paradigmatique et syntagmatiques) pour l’aide à l’accès lexical en situation de "mot sur le bout de la langue", en vue de la création d’un outil utilisant la combinaison de ces ressources. En situation de "mot sur le bout de la langue", l’utilisateur n’accède plus au mot qu’il veut dire ou écrire mais est capable d’en produire d’autres sémantiquement associés. L’évaluation se base sur un corpus de 20 mots "sur le bout de la langue" pour lesquels on dispose de 50 groupes de 5 associations sémantiques effectuées par des utilisateurs. Les résultats montrent que les ressources sont complémentaires et peu redondantes. De plus au moins une association proposée parmi les 5 permettrait de retrouver le mot "sur le bout de la langue" dans 79% des cas, à condition de le sélectionner parmi les 2500 mot potentiels. Enfin, les résultats montrent des disparités entre les utilisateurs, ce qui permettrait de définir des profils d’utilisateur pour une amélioration des performances.},
  abstract  = {This paper describes a joint and comparative evaluation of 5 lexical resources (descriptive, paradigmatic and syntagmatic) from a lexical access angle, with the further perspective of constructing a tool based on a combination of these resources to avoid the "tip of the tongue" (TOT) phenomenon. This phenomenon characterises a person who has difficulty in saying or writing an intended word but one who is able to produce semantically associated words. The evaluation corpus is composed of 20 TOT examples each linked to 50 users’ association sets of 5 semantically associated words. The results highlight that all the tested resources are complementary. Moreover, 79% of proposed association sets contain at least one association leading to the TOT through its relative words in at least one resource (in the worse case the TOT has to be found among 2500 words). Finally the results show variations between users which could increase performance thanks to user profiles.},
  motscles  = {réseaux sémantiques, accès lexical, profil d’utilisateur},
  keywords  = {semantic networks, lexical access, user profiling},
}

@inproceedings{acosta:2007:RECITAL,
  author    = {Acosta, Alejandro},
  title     = {Vers une nouvelle structuration de l’information extraite automatiquement},
  booktitle = {Actes des 9e Rencontres des Étudiants Chercheurs en Informatique pour le Traitement Automatique des Langues},
  month     = {June},
  year      = {2007},
  address   = {Toulouse, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {337--346},
  url       = {http://www.atala.org/taln_archives/RECITAL/RECITAL-2007/recital-2007-poster-001},
  language  = {french},
  resume    = {Les systèmes d’Extraction d’Information se contentent, le plus souvent, d’enrichir des bases de données plates avec les informations qu’ils extraient. Nous décrivons dans cet article un travail en cours sur l’utilisation de données extraites automatiquement pour la construction d’une structure de représentation plus complexe. Cette structure modélise un réseau social composé de relations entre les entités d’un corpus de biographies.},
  abstract  = {Information Extraction systems are widely used to create flat databases of templates filled with the data they extract from text. In this article we describe an ongoing research project that focuses on the use of automatically extracted data to create a more complex representation structure. This structure is a model of the social network underlying the relations that can be established between the entities of a corpus of biographies.},
  motscles  = {extraction d’information, analyse de réseaux sociaux, biographies, entités nommées, représentation de connaissances},
  keywords  = {information extraction, social network analysis, named entities, biographies, knowledge representation},
}

@inproceedings{bossard:2007:RECITAL,
  author    = {Bossard, Aurélien},
  title     = {Vers une ressource prédicative pour l’extraction d’information},
  booktitle = {Actes des 9e Rencontres des Étudiants Chercheurs en Informatique pour le Traitement Automatique des Langues},
  month     = {June},
  year      = {2007},
  address   = {Toulouse, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {347--356},
  url       = {http://www.atala.org/taln_archives/RECITAL/RECITAL-2007/recital-2007-poster-002},
  language  = {french},
  resume    = {Cet article présente une méthode pour construire, à partir d’une ressource lexicale prédicative existante, une ressource enrichie pouvant servir à une tâche d’extraction. Nous montrons les points forts et les lacunes de deux ressources existantes pour le Français : les Tables du LADL et Volem. Après avoir montré pourquoi nous avons sélectionné Volem, nous listons les données nécessaires à la tâche d’extraction d’information. Nous présentons le processus d’enrichissement de la ressource initiale et une évaluation, à travers une tâche d’extraction d’information concernant des textes de rachats d’entreprise.},
  abstract  = {In this article, we present a method aiming at building a resource for an information extraction task, from an already existing French predicative lexical resource. We point out the weaknesses and strengthnesses of two predicative resources we worked with : Les tables du LADL and Volem. We present why we select Volem as the most interesting resource for the task. Thereafter, we make a list of the needs an information extraction task implies, and how we include missing information in the resource we selected. We evaluate the resource completed by those missing informations, using it in an information extraction task.},
  motscles  = {ressource prédicative, extraction d’information, patrons lexico-syntaxiques},
  keywords  = {predicative resource, information extraction, lexico-syntactic patterns},
}

@inproceedings{bouchet:2007:RECITAL,
  author    = {Bouchet, François},
  title     = {Caractérisation d’un corpus de requêtes d’assistance},
  booktitle = {Actes des 9e Rencontres des Étudiants Chercheurs en Informatique pour le Traitement Automatique des Langues},
  month     = {June},
  year      = {2007},
  address   = {Toulouse, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {357--366},
  url       = {http://www.atala.org/taln_archives/RECITAL/RECITAL-2007/recital-2007-poster-003},
  language  = {french},
  resume    = {Afin de concevoir un agent conversationnel logiciel capable d’assister des utilisateurs novices d’applications informatiques, nous avons été amenés à constituer un corpus spécifique de requêtes d’assistance en français, et à étudier ses caractéristiques. Nous montrons ici que les requêtes d’assistance se distinguent nettement de requêtes issues d’autres corpus disponibles dans des domaines proches. Nous mettons également en évidence le fait que ce corpus n’est pas homogène, mais contient au contraire plusieurs activités conversationnelles distinctes, dont l’assistance elle-même. Ces observations nous permettent de discuter de l’opportunité de considérer l’assistance comme un registre particulier de la langue générale.},
  abstract  = {In order to conceive a conversational agent able to assist ordinary people using softwares, we have built up a specific corpus of assistance requests in french, and studied its characteristics. We show here that assistance requests can be clearly distinguished from the ones from other available corpora in related domains. We also show that this corpus isn’t homogenous, but on the contrary reflects various conversational activities, among which the assistance itself. Those observations allow us to discuss about the opportunity to consider assistance as a general language particular registre.},
  motscles  = {corpus de requêtes d’assistance, agent conversationnel, activité conversationnelle, actes de dialogue},
  keywords  = {corpus of assistance requests, conversational agent, conversational activity, speech acts},
}

@inproceedings{brixtel:2007:RECITAL,
  author    = {Brixtel, Romain},
  title     = {Extraction endogène d’une structure de document pour un alignement multilingue},
  booktitle = {Actes des 9e Rencontres des Étudiants Chercheurs en Informatique pour le Traitement Automatique des Langues},
  month     = {June},
  year      = {2007},
  address   = {Toulouse, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {367--376},
  url       = {http://www.atala.org/taln_archives/RECITAL/RECITAL-2007/recital-2007-poster-004},
  language  = {french},
  resume    = {Pour des raisons variées, diverses communautés se sont intéressées aux corpus multilingues. Parmi ces corpus, les textes parallèles sont utilisés aussi bien en terminologie, lexicographie ou comme source d’informations pour les systèmes de traduction par l’exemple. L’Union Européenne, qui a entraîné la production de document législatif dans vingtaine de langues, est une des sources de ces textes parallèles. Aussi, avec le Web comme vecteur principal de diffusion de ces textes parallèles, cet objet d’étude est passé à un nouveau statut : celui de document. Cet article décrit un système d’alignement prenant en compte un grand nombre de langues simultanément (> 2) et les caractéristiques structurelles des documents analysés.},
  abstract  = {For many reasons, the multilingual corporas have interested various communities. Among these corporas, the parallel texts are used as well in terminology, lexicography or as a source of informations for example-based translations. The European Union, which involved the production of legislative documents, generates these parrallel texts in more than twenty languages. Also, with the Web as a vector of diffusion, we can wonder if these parallel texts can be treated as documents. This article describes a alignment system taking account a great number of languages (> 2) and the structural characteristics of the analyzed documents.},
  motscles  = {alignement multilingue, corpus parrallèles, multitextes, multidocuments, extraction de structures, alignement endogène},
  keywords  = {multilingual alignment, parallel corpora, multitexts, multidocuments, extraction of structures, endogenous alignment},
}

@inproceedings{elayari:2007:RECITAL,
  author    = {El Ayari, Sarra},
  title     = {Évaluation transparente de systèmes de questions-réponses : application au focus},
  booktitle = {Actes des 9e Rencontres des Étudiants Chercheurs en Informatique pour le Traitement Automatique des Langues},
  month     = {June},
  year      = {2007},
  address   = {Toulouse, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {377--386},
  url       = {http://www.atala.org/taln_archives/RECITAL/RECITAL-2007/recital-2007-poster-005},
  language  = {french},
  resume    = {Les campagnes d’évaluation ne tiennent compte que des résultats finaux obtenus par les systèmes de recherche d’informations (RI). Nous nous situons dans une perspective d’évaluation transparente d’un système de questions-réponses, où le traitement d’une question se fait grâce à plusieurs composants séquentiels. Dans cet article, nous nous intéressons à l’étude de l’élément de la question qui porte l’information qui se trouvera dans la phrase réponse à proximité de la réponse elle-même : le focus. Nous définissons ce concept, l’appliquons au système de questions-réponses QALC, et démontrons l’utilité d’évaluations des composants afin d’augmenter la performance globale du système.},
  abstract  = {Evaluation campaigns take into account only the final results obtained by information retrieval systems. Our perspective is that of the glass box evaluation of a question-answering system. The processing of a question is accomplished by a series of components. The purpose of this article is to study the element in the sentence which holds the key information. This element is to be found again in the sentence containing the answer next to the answer itself, and is called the focus. We will begin by defining this concept. We will then applied it to the QALC question answering system. Finally we will demonstrate the pertinence of using glass box evaluations to enhance the global performance such systems.},
  motscles  = {système de questions-réponses, recherche d’information, évaluation, focus},
  keywords  = {question answering system, information retrieval, evaluation, focus},
}

@inproceedings{laignelet-pimm:2007:RECITAL,
  author    = {Laignelet, Marion and Pimm, Christophe},
  title     = {La segmentation thématique TextTiling comme indice pour le repérage de segments d'information évolutive dans un corpus de textes encyclopédiques},
  booktitle = {Actes des 9e Rencontres des Étudiants Chercheurs en Informatique pour le Traitement Automatique des Langues},
  month     = {June},
  year      = {2007},
  address   = {Toulouse, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {387--396},
  url       = {http://www.atala.org/taln_archives/RECITAL/RECITAL-2007/recital-2007-poster-006},
  language  = {french},
  resume    = {Nous faisons l'hypothèse que les bornes délimitées par la méthode statistique TextTiling peuvent servir d'indices qui, cumulées à des indices de nature linguistique, permettront de repérer automatiquement des segments d'informations évolutives. Ce travail est développé dans le cadre d'un projet industriel plus général dont le but est le repérage automatique de zones textuelles contenant de l'information potentiellement évolutive.},
  abstract  = {Our hypothesis is that the TextTiling's boundaries can be considered as clues we can use with other linguistic features to automatically detect evolving information segments. This work is developed as part of an industrial project aiming to automatically detect textual zones containing potentially evolving information.},
  motscles  = {segments d'information évolutive, segmentation, algorithme TextTiling},
  keywords  = {evolving information, segmentation, TextTiling algorithm},
}

@inproceedings{piu-bove:2007:RECITAL,
  author    = {Piu, Marie and Bove, Rémi},
  title     = {Annotation des disfluences dans les corpus oraux},
  booktitle = {Actes des 9e Rencontres des Étudiants Chercheurs en Informatique pour le Traitement Automatique des Langues},
  month     = {June},
  year      = {2007},
  address   = {Toulouse, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {397--406},
  url       = {http://www.atala.org/taln_archives/RECITAL/RECITAL-2007/recital-2007-poster-007},
  language  = {french},
  resume    = {Les disfluences (répétitions, amorces, autocorrections, constructions inachevées, etc.) inhérentes à toute production orale spontanée constituent une réelle difficulté en termes d’annotation. En effet, l’annotation de ces phénomènes se révèle difficilement automatisable dans la mesure où leur étude réclame un jugement éminemment interprétatif. Dans cet article, nous présentons une méthodologie applicable à l’annotation des disfluences (ou « phénomènes de production ») que l’on rencontre fréquemment dans les corpus oraux. Le fait de constituer un tel corpus de données annotées, permet non seulement de représenter certains aspects pertinents de l’oral (de manière à servir de base aux observations et aux comparaisons avec d’autres données) mais aussi d’améliorer in fine le traitement automatique de l’oral (notamment l’analyse syntaxique automatique).},
  abstract  = {Disfluencies (repeats, word-fragments, self-repairs, aborted constructs, etc.) inherent in any spontaneous speech production constitute a real difficulty in terms of annotation. Indeed, the annotation of these phenomena seems not easily automatizable, because their study needs an interpretative judgement. In this paper, we present a methodology for the annotation of disfluencies (also named “production phenomena”) which frequently occur in speech corpora. Constituting such data allows not only to represent some relevant aspects of speech productions (so as to be a basis for observations and comparisons with other data), but also to improve automatic speech processing (particularly for parsing).},
  motscles  = {corpus oraux, annotation, disfluences, prosodie, XML},
  keywords  = {speech corpora, annotation, disfluencies, prosody, XML},
}

@inproceedings{popescu:2007:RECITAL,
  author    = {Popescu, Vladimir},
  title     = {Architecture modulaire portable pour la génération du langage naturel en dialogue homme-machine},
  booktitle = {Actes des 9e Rencontres des Étudiants Chercheurs en Informatique pour le Traitement Automatique des Langues},
  month     = {June},
  year      = {2007},
  address   = {Toulouse, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {407--416},
  url       = {http://www.atala.org/taln_archives/RECITAL/RECITAL-2007/recital-2007-poster-008},
  language  = {french},
  resume    = {La génération du langage naturel pour le dialogue oral homme-machine pose des contraintes spécifiques, telles que la spontanéité et le caractère fragmenté des énoncés, les types des locuteurs ou les contraintes de temps de réponse de la part du système. Dans ce contexte, le problème d’une architecture rigoureusement spécifiée se pose, autant au niveau des étapes de traitement et des modules impliqués, qu’au niveau des interfaces entre ces modules. Afin de permettre une liberté quasi-totale à l’égard des démarches théoriques, une telle architecture doit être à la fois modulaire (c’est-à-dire, permettre l’indépendance des niveaux de traitement les uns des autres) et portable (c’est-à-dire, permettre l’interopérabilité avec des modules conçus selon des architectures standard en génération du langage naturel, telles que le modèle RAGS - « Reference Architecture for Generation Systems »). Ainsi, dans cet article on présente de manière concise l’architecture proposée, la comparant ensuite au modèle RAGS, pour argumenter les choix opérés en conception. Dans un second temps, la portabilité de l’architecture sera décrite à travers un exemple étendu, dont la généralité réside dans l’obtention d’un ensemble de règles permettant de plonger automatiquement les représentations des informations de notre architecture vers le format du modèle RAGS et inversement. Finalement, un ensemble de conclusions et perspectives clôturera l’article.},
  abstract  = {Natural language generation for human-computer dialogue imposes specific constraints, such as the spontaneous and fragmented character of the utterances, speaker types or constraints related to the system’s time of response. In this context, the issue of a thouroughly specified architecture stems naturally, with respect to the processing stages in the modules involved and to the interfaces between these modules as well. In order to allow for a quasi-total freedom concerning the theoretical principles driving the processing stages, such an architecture must be modular (i.e., allowing the independence of the modules of each other) and portable (i.e., allowing a certain interoperability between modules designed following this architecture and existing modules, designed following standard, reference architectures, such as the RAGS model). Thus, in this paper firstly the proposed architecture will be presented in a concise manner, comparing it then to the RAGS model and arguing for the design choices being made. Afterwards, the portability of the architecture will be described, via an extended example whose general character resides in the fact that a set of rules are obtained, that allow automatic translations between representation formats in our architecture and in the RAGS model, in both ways. Finally, a set of conclusions and pointers to further work end up the paper.},
  motscles  = {génération, dialogue, architecture modulaire, portabilité, XML},
  keywords  = {generation, dialogue, modular architecture, portability, XML},
}

@inproceedings{regnier:2007:RECITAL,
  author    = {Régnier, Alain},
  title     = {Résolution anaphorique intégrée à une analyse automatique de discours d’un corpus oral retranscrit},
  booktitle = {Actes des 9e Rencontres des Étudiants Chercheurs en Informatique pour le Traitement Automatique des Langues},
  month     = {June},
  year      = {2007},
  address   = {Toulouse, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {417--426},
  url       = {http://www.atala.org/taln_archives/RECITAL/RECITAL-2007/recital-2007-poster-009},
  language  = {french},
  resume    = {Nous présentons une résolution anaphorique intégrée à une analyse automatique de discours. Cette étude traite des anaphores pronominales et des anaphores zéro. Notre analyse est basée sur trois approches : une analyse basée sur les contraintes, une analyse fonctionnelle et une analyse dynamique. Pour évaluer la faisabilité et la fiabilité de notre approche, nous l’avons expérimentée sur un corpus de 97 histoires produites à l’oral par des enfants. Nous présentons le résultat de cette évaluation.},
  abstract  = {We present an anaphora resolution integrated in a discourse analysis. This study deals with pronoun anaphora and zero anaphora. Our analysis is based on three approaches. A constraint based rule analysis, a functional approach and a dynamic analysis. In order to evaluate the feasibility of our approach and its reliability we have experimented with a corpus of 97 speech stories produced by children. We present here the results of our evaluation experiment.},
  motscles  = {analyse de discours, résolution anaphorique, anaphore pronominales, anaphores zéro, grammaires de propriétés, grammaire fonctionnelle, analyse dynamique, discours oral},
  keywords  = {discourse analysis, anaphora resolution, pronoun anaphora, zero anaphora, property grammars, functional grammar, dynamic analysis, speech discourse},
}