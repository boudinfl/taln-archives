<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
"http://www.w3.org/TR/html4/loose.dtd">
<html><head><title>Transducteurs &#224; fen&#234;tre glissante pour l&#8217;induction lexicale</title>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
</head>
<body>
<div style="page-break-before:always; page-break-after:always"><div><p>RECITAL 2008, Avignon, 9&#8211;13 juin 2008
</p>
<p>Transducteurs &#224; fen&#234;tre glissante pour l&#8217;induction lexicale
</p>
<p>Yves Scherrer
LATL, Universit&#233; de Gen&#232;ve
</p>
<p>Rue de Candolle 5
1211 Gen&#232;ve 4, Suisse
</p>
<p>yves.scherrer@lettres.unige.ch
</p>
<p>R&#233;sum&#233;. Nous appliquons diff&#233;rents mod&#232;les de similarit&#233; graphique &#224; la t&#226;che de l&#8217;in-
duction de lexiques bilingues entre un dialecte de Suisse allemande et l&#8217;allemand standard. Nous
comparons des transducteurs stochastiques utilisant des fen&#234;tres glissantes de 1 &#224; 3 caract&#232;res,
entra&#238;n&#233;s &#224; l&#8217;aide de l&#8217;algorithme de maximisation de l&#8217;esp&#233;rance avec des corpus d&#8217;entra&#238;ne-
ment de tailles diff&#233;rentes. Si les transducteurs &#224; unigrammes donnent des r&#233;sultats satisfaisants
avec des corpus tr&#232;s petits, nous montrons que les transducteurs &#224; bigrammes les d&#233;passent &#224;
partir de 750 paires de mots d&#8217;entra&#238;nement. En g&#233;n&#233;ral, les mod&#232;les entra&#238;n&#233;s nous ont permis
d&#8217;am&#233;liorer la F-mesure de 7% &#224; 15% par rapport &#224; la distance de Levenshtein.
</p>
<p>Abstract. We apply different models of graphemic similarity to the task of bilingual
lexicon induction between a Swiss German dialect and Standard German. We compare sto-
chastic transducers using sliding windows from 1 to 3 letters, trained with the Expectation-
Maximisation algorithm on training corpora of different sizes. While the unigram transducers
provide good results with very small corpora, we show that bigram transducers outperform them
with corpora of 750 word pairs or more. Overall, the trained models show between 7% and 15%
F-measure improvement over Levenshtein distance.
</p>
<p>Mots-cl&#233;s : Induction lexicale, transducteurs stochastiques, langues apparent&#233;es.
</p>
<p>Keywords: Lexicon induction, stochastic transducers, cognate languages.
</p>
<p>1 Introduction
</p>
<p>Les ressources lexicales constituent une partie essentielle de tout syst&#232;me de traitement auto-
matique des langues. Comme la construction manuelle de telles ressources est fastidieuse et
gourmande en temps, l&#8217;induction automatique de ressources lexicales est une alternative parti-
culi&#232;rement attractive. Dans cet article, nous discuterons diff&#233;rentes approches pour l&#8217;induction
d&#8217;un dictionnaire bilingue entre un dialecte suisse allemand et la vari&#233;t&#233; standard de l&#8217;allemand.
</p>
<p>Le choix particulier de cette paire de langues a des cons&#233;quences importantes sur la m&#233;thodo-
logie. En Suisse allemande, les dialectes et la vari&#233;t&#233; standard forment une diglossie m&#233;diale :
les dialectes sont utilis&#233;s &#224; l&#8217;oral, tandis que l&#8217;allemand standard est surtout utilis&#233; &#224; l&#8217;&#233;crit. A
cause de cette distribution compl&#233;mentaire, il est difficile de trouver des corpus parall&#232;les, et
g&#233;n&#233;ralement des textes &#233;crits en dialecte. Pourtant, l&#8217;attitude positive de la population envers
le dialecte et son utilisation g&#233;n&#233;ralis&#233;e en font un candidat attractif pour le traitement automa-</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Yves Scherrer
</p>
<p>tique. Ces contraintes relatives &#224; la disponibilit&#233; des donn&#233;es placent notre recherche dans le
contexte du traitement de langues peu dot&#233;es.
</p>
<p>En m&#234;me temps, les dialectes al&#233;maniques sont &#233;troitement apparent&#233;s &#224; l&#8217;allemand standard.
Cette parent&#233; r&#233;duit la complexit&#233; des relations lexicales &#224; induire. Nos travaux s&#8217;ins&#232;rent donc
dans le courant de recherche du traitement de langues apparent&#233;es. Nous soutenons que l&#8217;avan-
tage de la parent&#233; &#233;troite est &#224; m&#234;me de lever quelques restrictions impos&#233;es par la raret&#233; des
ressources. Plus pr&#233;cis&#233;ment, nous faisons l&#8217;hypoth&#232;se que dans le cas de deux langues appa-
rent&#233;es, l&#8217;utilisation de techniques d&#8217;apprentissage automatique est possible m&#234;me si peu de
ressources existent pour l&#8217;une d&#8217;entre elles.
</p>
<p>Nous concevons un dictionnaire bilingue essentiellement comme une liste de paires de mots.1
</p>
<p>L&#8217;induction de paires de mots se fonde sur un crit&#232;re de similarit&#233;. Th&#233;oriquement, cette si-
milarit&#233; est d&#8217;ordre s&#233;mantique : deux mots sont associ&#233;s dans un dictionnaire s&#8217;ils renvoient
au m&#234;me concept. Or, il est difficile d&#8217;extraire directement les significations des mots &#224; partir
de donn&#233;es textuelles brutes. En g&#233;n&#233;ral, on recourt &#224; des crit&#232;res de similarit&#233; plus simples
et plus op&#233;rationnels, mais n&#233;anmoins corr&#233;l&#233;s avec la similarit&#233; s&#233;mantique. L&#8217;approche clas-
sique se base sur l&#8217;alignement de mots dans un corpus parall&#232;le (Brown et al., 1993) : deux mots
sont consid&#233;r&#233;s similaires s&#8217;ils ont une probabilit&#233; d&#8217;alignement suffisante. Une autre approche
(Rapp, 1999) s&#8217;affranchit de corpus parall&#232;les ; selon celle-ci, deux mots sont similaires s&#8217;ils
apparaissent dans des contextes lexicaux similaires.
</p>
<p>Les propri&#233;t&#233;s particuli&#232;res de notre paire de langues nous ont amen&#233; &#224; consid&#233;rer un autre type
de similarit&#233; : la similarit&#233; graphique. Pour des langues &#233;troitement apparent&#233;es, une grande
partie du lexique g&#233;n&#233;ral est constitu&#233; de paires lexicales apparent&#233;es (cognate word pairs) : les
mots qui ont la m&#234;me signification ont &#233;galement une forme phon&#233;tique et graphique similaire2.
</p>
<p>La similarit&#233; graphique a l&#8217;avantage de fournir de bons r&#233;sultats avec peu de ressources. S&#8217;il est
possible de l&#8217;utiliser sans donn&#233;es d&#8217;entra&#238;nement, nous montrerons que l&#8217;apprentissage auto-
matique avec un corpus de taille modeste am&#233;liore nettement les r&#233;sultats. Nous nous appuyons
sur des recherches r&#233;centes dans le domaine de la traduction entre phon&#232;mes et graph&#232;mes afin
de tenir compte du contexte des lettres lors du calcul de la similarit&#233;. Nous nous focalisons sur
les correspondances de mots simples &#224; mots simples.
</p>
<p>Apr&#232;s une discussion des travaux r&#233;cents dans ce domaine, nous pr&#233;senterons notre architecture
d&#8217;induction lexicale ainsi que les diff&#233;rents mod&#232;les impl&#233;ment&#233;s (section 3). Ensuite, nous
d&#233;crirons les donn&#233;es utilis&#233;es pour l&#8217;entra&#238;nement et l&#8217;&#233;valuation (section 4), suivi par la pr&#233;-
sentation et la discussion des r&#233;sultats obtenus (section 5).
</p>
<p>2 Travaux connexes
</p>
<p>Les mesures de similarit&#233; phon&#233;tique et graphique sont utilis&#233;es extensivement dans le cadre
du traitement de la parole, afin de transformer des s&#233;quences de phon&#232;mes en s&#233;quences de
</p>
<p>1&#201;tant donn&#233; la parent&#233; de nos langues, des informations morphologiques et syntaxiques, disponibles pour le
c&#244;t&#233; allemand standard, pourront &#234;tre projet&#233;es sur le c&#244;t&#233; dialecte sans modifications majeures.
</p>
<p>2Evidemment, l&#8217;existence de conventions orthographiques pour les langues en question et la nature de ces der-
ni&#232;res peuvent influencer les r&#233;sultats de cette m&#233;thode. Bien qu&#8217;il n&#8217;existe pas de conventions orthographiques
obligatoires pour les dialectes suisse allemands, des questions pratiques nous ont amen&#233; &#224; utiliser des donn&#233;es
&#233;crites (c&#8217;est-&#224;-dire non transcites phon&#233;tiquement). Cependant, l&#8217;orthographe utilis&#233;e est tr&#232;s proche de la pro-
nonciation.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Transducteurs &#224; fen&#234;tre glissante pour l&#8217;induction lexicale
</p>
<p>lettres et inversement. Dans ce cadre, (Ristad &amp; Yianilos, 1998) introduisent des m&#233;thodes d&#8217;ap-
prentissage automatique : ils entra&#238;nent un transducteur stochastique sans m&#233;moire (&#224; un &#233;tat)
en utilisant l&#8217;algorithme de maximisation de l&#8217;esp&#233;rance (EM). Cet algorithme it&#233;ratif permet
d&#8217;estimer les probabilit&#233;s des transitions du transducteur stochastique &#224; partir d&#8217;un corpus d&#8217;en-
tra&#238;nement contenant des paires de mots corrects.
</p>
<p>Le mod&#232;le de Ristad &amp; Yianilos a &#233;t&#233; repris pour l&#8217;induction de dictionnaires bilingues entre
langues apparent&#233;es (Mann &amp; Yarowsky, 2001). L&#8217;id&#233;e de Mann &amp; Yarowsky est d&#8217;&#233;tendre un
lexique bilingue existant &#224; une langue apparent&#233;e. Par exemple, un lexique anglais-espagnol
peut servir de base pour un lexique anglais-portugais, l&#8217;espagnol jouant le r&#244;le de pivot. Les
mesures de similarit&#233; graphique (appel&#233;s cognate models) sont utilis&#233;es pour apparier les mots
espagnols et les mots portugais. Les auteurs de cette &#233;tude font la distinction entre des mesures
statiques, qui sont assez g&#233;n&#233;riques pour &#234;tre appliqu&#233;es &#224; toute paire de langues sans entra&#238;ne-
ment pr&#233;alable, et des mesures adaptives, qui sont adapt&#233;es &#224; une paire de langues pr&#233;cise. En
particulier, un transducteur stochastique entra&#238;n&#233; &#224; l&#8217;aide de EM comme mesure adaptive ainsi
que la distance de Levenshtein comme mesure statique. La distance de Levenshtein entre deux
cha&#238;nes de caract&#232;res est d&#233;finie comme le nombre minimal d&#8217;op&#233;rations d&#8217;&#233;dition (insertion,
effacement ou substitution d&#8217;un caract&#232;re) n&#233;cessaires pour transformer une cha&#238;ne dans l&#8217;autre.
</p>
<p>La distance de Levenshtein et les m&#233;thodes bas&#233;es sur les transducteurs sans m&#233;moire ne
prennent pas en compte le contexte : un seul symbole de l&#8217;entr&#233;e est compar&#233; avec un seul sym-
bole de la sortie &#224; la fois. Cette approche s&#8217;est av&#233;r&#233;e insuffisante dans le cadre de la conversion
entre phon&#232;mes et lettres : ph doit &#234;tre converti en [f], tandis que x doit &#234;tre converti en [ks].
Une solution au premier probl&#232;me est l&#8217;utilisation d&#8217;une fen&#234;tre glissante (Jansche, 2001) :
on regarde plusieurs caract&#232;res dans l&#8217;entr&#233;e pour en g&#233;n&#233;rer un seul &#224; la sortie. Une autre
technique, plus sophistiqu&#233;e, consiste &#224; adapter l&#8217;algorithme d&#8217;apprentissage pour entra&#238;ner des
correspondances plusieurs-&#224;-plusieurs (Jiampojamarn et al., 2007). Cette technique n&#233;cessite
un pr&#233;traitement du mot source ; il faut le couper en morceaux d&#8217;une &#224; plusieurs lettres afin de
d&#233;terminer les types de correspondances &#224; utiliser. Nous avons montr&#233; dans (Scherrer, 2007)
qu&#8217;un transducteur bas&#233; sur des r&#232;gles d&#233;pendantes du contexte, impl&#233;ment&#233;es manuellement,
obtient de meilleures performances que le transducteur sans m&#233;moire entra&#238;n&#233; avec EM.
</p>
<p>Les m&#233;thodes d&#8217;induction lexicale propos&#233;es ici s&#8217;appliquent aux configurations linguistiques
dans lesquelles la majorit&#233; des paires lexicales se ressemblent graphiquement. Si cette condition
est v&#233;rifi&#233;e pour le lexique g&#233;n&#233;ral de langues apparent&#233;es, elle l&#8217;est aussi pour des domaines
lexicaux sp&#233;cifiques, ind&#233;pendamment du degr&#233; de parent&#233; des langues. Dans cette optique,
(Claveau &amp; Zweigenbaum, 2005) entra&#238;nent un transducteur (non stochastique) pour inf&#233;rer des
traductions fran&#231;aises de termes biom&#233;dicaux anglais. (Claveau, 2007) &#233;tend cette technique &#224;
d&#8217;autres paires de langues (par exemple, anglais-russe) et introduit un mod&#232;le de la langue cible
pour filtrer les candidats lexicaux propos&#233;s.
</p>
<p>3 Mod&#232;les d&#8217;induction lexicale
</p>
<p>3.1 Les deux &#233;tapes de l&#8217;induction lexicale
</p>
<p>En traduction automatique statistique, il est usuel de partager la probl&#233;matique en deux t&#226;ches
distinctes. La traduction d&#8217;une phrase doit en effet satisfaire deux conditions principales. Pre-
mi&#232;rement, son contenu doit rester fid&#232;le &#224; la phrase source, et deuxi&#232;mement, sa forme doit &#234;tre</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Yves Scherrer
</p>
<p>conforme &#224; la grammaire de la langue cible. La premi&#232;re condition est garantie par le mod&#232;le
de traduction, la deuxi&#232;me par le mod&#232;le de langue.
</p>
<p>Nous reprenons cette architecture pour l&#8217;induction du lexique. Dans une premi&#232;re &#233;tape, nous
proposons des cha&#238;nes de caract&#232;res qui restent similaires au mot source. C&#8217;est ici que les dif-
f&#233;rentes mesures de similarit&#233; graphique interviennent : elles g&#233;n&#232;rent une suite de cha&#238;nes de
caract&#232;res, ordonn&#233;es par leur taux de similarit&#233; graphique par rapport au mot source. Dans
une seconde &#233;tape, nous devons garantir que les cha&#238;nes de caract&#232;res ainsi g&#233;n&#233;r&#233;es soient
conformes &#224; la langue cible. Pour cela, nous utilisons une liste de mots de l&#8217;allemand standard
comme filtre : seules les cha&#238;nes de caract&#232;res qui sont des mots allemands sont retenues. On
peut donc consid&#233;rer ce filtre lexical comme un mod&#232;le de langue binaire. La figure 1 illustre
cette architecture &#224; l&#8217;aide d&#8217;un exemple.
</p>
<p>3.2 Distance de Levenshtein
</p>
<p>La distance de Levenshtein entre deux cha&#238;nes de caract&#232;res est d&#233;finie comme le nombre mi-
nimal d&#8217;op&#233;rations d&#8217;&#233;dition n&#233;cessaires pour transformer une cha&#238;ne dans l&#8217;autre.3 Il y a trois
types d&#8217;op&#233;rations d&#8217;&#233;dition : l&#8217;insertion d&#8217;un caract&#232;re, la substitution d&#8217;un caract&#232;re par un
autre, et l&#8217;effacement d&#8217;un caract&#232;re. La distance de Levenshtein op&#232;re sur des caract&#232;res isol&#233;s
sans prendre en compte les caract&#232;res pr&#233;c&#233;dents et suivants. Ainsi, elle peut &#234;tre impl&#233;ment&#233;e
dans un transducteur sans m&#233;moire (&#224; un &#233;tat). Par ailleurs, cette mesure de distance est sta-
tique ; elle est identique pour toutes les paires de langues. Nous l&#8217;utiliserons comme mod&#232;le de
r&#233;f&#233;rence pour nos exp&#233;riences.
</p>
<p>3.3 Transducteurs stochastiques entra&#238;n&#233;s avec EM
</p>
<p>Un transducteur impl&#233;mentant la distance de Levenshtein poss&#232;de deux classes de transitions :
les transitions d&#8217;&#233;dition avec un co&#251;t unitaire, et les transitions d&#8217;identit&#233; (le m&#234;me caract&#232;re en
entr&#233;e et en sortie) avec un co&#251;t de 0. Pour des applications linguistiques, cette classification
binaire est souvent insuffisante. Par exemple, lorsqu&#8217;on traduit des mots suisse allemands en
allemand standard, l&#8217;insertion de n ou de e est beaucoup plus fr&#233;quente que celle de m ou de i.
De m&#234;me, un a reste plus souvent identique qu&#8217;un &#252;. Afin de pouvoir pr&#233;dire de tels ph&#233;nom&#232;nes
sp&#233;cifiques, il nous faut primo un type de transducteur plus souple, permettant d&#8217;associer des
poids diff&#233;rents &#224; chaque transition, et secundo un m&#233;canisme d&#8217;apprentissage automatique
pour d&#233;terminer ces poids. Suivant (Ristad &amp; Yianilos, 1998), nous utilisons un transducteur
stochastique pour satisfaire la premi&#232;re exigence, et l&#8217;algorithme EM pour satisfaire la seconde.4
</p>
<p>Dans un transducteur stochastique, toutes les transitions repr&#233;sentent des probabilit&#233;s. La pro-
babilit&#233; de transduction d&#8217;une paire de mots donn&#233;e est la somme des probabilit&#233;s de tous les
chemins qui la g&#233;n&#232;rent. L&#8217;algorithme EM sert &#224; trouver les probabilit&#233;s de transition de sorte &#224;
</p>
<p>3Dans cet article, nous utilisons parfois le terme similarit&#233;, parfois le terme distance. Comme les valeurs de
similarit&#233; ou de distance servent seulement &#224; ordonner les candidats g&#233;n&#233;r&#233;s, le rapport exact entre ces deux notions
ne nous semble pas important : dans ce travail, il nous suffit de pouvoir comparer des listes ordonn&#233;es par similarit&#233;
d&#233;croissante avec des listes ordonn&#233;es par distance croissante.
</p>
<p>4Pour notre paire de langues, le nombre de transpositions de caract&#232;res est relativement restreint ; on peut donc
envisager de cr&#233;er un transducteur &#224; la main, sans utiliser un algorithme d&#8217;apprentissage. (Scherrer, 2007) pr&#233;sente
une telle approche.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Transducteurs &#224; fen&#234;tre glissante pour l&#8217;induction lexicale
</p>
<p>Mot d&#8217;entr&#233;e Premi&#232;re &#233;tape Deuxi&#232;me &#233;tape
G&#233;n&#233;ration de candidats Filtrage des candidats
</p>
<p>vermuetet vermuetet 29.87 vermutet 32.19
vermutet 32.19 vermutete 36.08
vermuett 32.19 vermute 36.65
vrmuetet 32.19 vermuten 37.69
vermaetet 32.68 vermutetet 39.23
vermuetit 33.41 vermottet 39.41
vermuitet 33.41 vermuteten 39.72
virmuetet 33.41 vermutest 40.57
vermuetent 33.51
vermunetet 33.51
vnermuetet 33.51
vermuetetn 33.51
nvermuetet 33.51
. . . (10000 candidats)
</p>
<p>FIG. 1 &#8211; Sortie du mod&#232;le d&#8217;induction lexicale pour le mot dialectal vermuetet &#8216;suppos&#233;&#8217;. Ce
mot doit &#234;tre associ&#233; au mot allemand standard vermutet (en gras). La colonne du milieu montre
les cha&#238;nes de caract&#232;res g&#233;n&#233;r&#233;es &#224; l&#8217;aide du mod&#232;le de similarit&#233; graphique. Les chiffres cor-
respondent &#224; des logarithmes n&#233;gatifs de probabilit&#233;s, et proviennent du transducteur stochas-
tique &#224; unigrammes (cf. section 3.3). La colonne de droite montre les candidats ayant pass&#233; la
deuxi&#232;me &#233;tape, c&#8217;est-&#224;-dire ceux qui sont effectivement des mots allemands.
</p>
<p>ce qu&#8217;elles maximisent la vraisemblance de g&#233;n&#233;rer les paires de mots vues pendant l&#8217;entra&#238;ne-
ment. Cet objectif peut &#234;tre atteint it&#233;rativement en utilisant une liste de paires de mots corrects.
Le transducteur est initialis&#233; avec des probabilit&#233;s uniformes. En traduisant les paires de mots
de la liste d&#8217;entra&#238;nement, il compte toutes les transitions utilis&#233;es dans ce processus. Ensuite,
les probabilit&#233;s des transitions sont r&#233;estim&#233;es selon la fr&#233;quence d&#8217;utilisation des transitions
compt&#233;es auparavant. Ces nouvelles probabilit&#233;s sont ensuite utilis&#233;es dans l&#8217;it&#233;ration suivante.
</p>
<p>3.4 Transducteurs &#224; fen&#234;tre glissante
</p>
<p>Le transducteur stochastique pr&#233;sent&#233; ci-dessus ne tient pas compte du contexte graphique des
caract&#232;res. La figure 1 illustre bien cette propri&#233;t&#233; : le mod&#232;le a appris que l&#8217;&#233;limination de e est
peu co&#251;teuse, mais cette &#233;limination obtient la m&#234;me probabilit&#233; dans toutes les positions. On
g&#233;n&#232;re ainsi beaucoup de candidats inutiles, car &#233;limin&#233;s dans la seconde &#233;tape. (Jansche, 2001)
a pr&#233;sent&#233; une solution &#224; ce probl&#232;me. Au lieu de fournir au transducteur un caract&#232;re &#224; la fois,
il lui fournit &#233;galement le caract&#232;re pr&#233;c&#233;dent et le caract&#232;re suivant. A partir d&#8217;un trigramme
d&#8217;entr&#233;e, le transducteur doit pr&#233;dire un seul caract&#232;re, celui du milieu. Ce transducteur poss&#232;de
donc une fen&#234;tre glissante de longueur 3. La figure 2 illustre son fonctionnement.5
</p>
<p>Afin de pouvoir conserver l&#8217;algorithme d&#8217;entra&#238;nement simple du transducteur sans m&#233;moire,
nous consid&#233;rons chaque trigramme comme un symbole primitif. N&#233;anmoins, ce choix aug-
mente consid&#233;rablement le nombre de transitions &#224; entra&#238;ner : avec un alphabet de n caract&#232;res,
</p>
<p>5Les symboles sp&#233;ciaux @ et $ sont ins&#233;r&#233;s au d&#233;but et &#224; la fin du mot afin d&#8217;obtenir un nombre suffisant de
trigrammes.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Yves Scherrer
</p>
<p>@ve ver erm rmu mue uet ete tet et$
&#8595; &#8595; &#8595; &#8595; &#8595; &#8595; &#8595; &#8595; &#8595;
v e r m u &#949; t e t
</p>
<p>FIG. 2 &#8211; Le meilleur alignement pour le mot vermuetet avec le mod&#232;le &#224; fen&#234;tre glissante de
trigrammes. L&#8217;&#233;limination du e est conditionn&#233;e par le contexte u_t.
</p>
<p>@g gu ue et t$ @h hu uu us s$
&#8595; &#8595; &#8595; &#8595; &#8595; &#8595; &#8595; &#8595; &#8595; &#8595;
g u &#949; t &#949; &#949; h a u s
</p>
<p>FIG. 3 &#8211; A gauche, le meilleur alignement pour la paire guet &#8211; gut &#8216;bon&#8217; avec un transduc-
teur &#224; bigrammes r&#233;gressif. Ce type est particuli&#232;rement bien adapt&#233; aux diphtongues dont le
deuxi&#232;me &#233;l&#233;ment est modifi&#233;, comme ue&#8594; u&#949; . A droite, le meilleur alignement pour la paire
huus &#8211; Haus &#8216;maison&#8217; avec un transducteur &#224; bigrammes progressif. Ce type est bien adapt&#233;
aux diphtongues dont le premier &#233;l&#233;ment est modifi&#233;, comme uu&#8594; au.
</p>
<p>nous obtenons n2 transitions pour le transducteur traditionnel (&#224; unigrammes), mais n4 transi-
tions pour le transducteur &#224; trigrammes. &#201;tant donn&#233; les limitations de nos donn&#233;es d&#8217;entra&#238;ne-
ment, nous avons d&#233;velopp&#233; une solution interm&#233;diaire, utilisant des fen&#234;tres glissante de lon-
gueur 2 (bigrammes). Techniquement, il existe deux variantes de transducteurs &#224; bigrammes :
une variante r&#233;gressive, g&#233;n&#233;rant un caract&#232;re en fonction des caract&#232;res courant et pr&#233;c&#233;dent,
et une variante progressive, g&#233;n&#233;rant un caract&#232;re en fonction des caract&#232;res courant et suivant.
La figure 3 en donne des exemples. Nous avons choisi de combiner les deux variantes pour nos
exp&#233;riences. Deux transducteurs sont entra&#238;n&#233;s &#224; l&#8217;aide du m&#234;me corpus, mais avec un biais ini-
tial favorisant soit les transitions de type AB&#8594; B pour la variante r&#233;gressive, soit les transitions
de type AB&#8594; A pour la variante progressive. Dans la phase d&#8217;&#233;valuation, nous utilisons l&#8217;union
des r&#233;sultats des deux transducteurs.6
</p>
<p>4 Donn&#233;es et entra&#238;nement
</p>
<p>Comme &#233;voqu&#233; dans l&#8217;introduction, il est difficile d&#8217;obtenir des donn&#233;es &#233;crites en dialecte
suisse allemand. Afin d&#8217;&#233;viter les difficult&#233;s pos&#233;es par le manque de r&#232;gles orthographiques
et par le style tr&#232;s familier de la plupart des textes, nous avons choisi un livre de litt&#233;rature en
dialecte bernois.
</p>
<p>Les diff&#233;rences linguistiques entre l&#8217;allemand standard et le dialecte bernois concernent en
grande partie les voyelles. Selon le contexte, des monophtongues allemands peuvent corres-
pondre &#224; des diphtongues bernois, ou l&#8217;inverse. Certains i allemands peuvent devenir &#252; en
bernois, et les e finaux sont soit &#233;lid&#233;s, soit transform&#233;s en i. Au niveau des consonnes, les
ph&#233;nom&#232;nes les plus fr&#233;quents sont l&#8217;effacement du n final en bernois et la vocalisation du l
pr&#233;consonantique devenant u. Nous esp&#233;rons &#233;galement capter certains ph&#233;nom&#232;nes morpho-
logiques simples comme l&#8217;alternance du suffixe diminutif (-chen en allemand standard, -li en
dialecte bernois).
</p>
<p>6L&#8217;union a donn&#233; des meilleurs r&#233;sultats que l&#8217;intersection, aussi bien au niveau de la pr&#233;cision que du rappel.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Transducteurs &#224; fen&#234;tre glissante pour l&#8217;induction lexicale
</p>
<p>De notre livre bernois, nous en avons extrait les mots sous forme d&#8217;une liste. Nous n&#8217;avons
pas fait d&#8217;analyse morphologique de ces mots ; plusieurs formes fl&#233;chies du m&#234;me lex&#232;me
peuvent donc appara&#238;tre dans la liste. Seules les variantes morpho-phonologiques (ph&#233;nom&#232;nes
de sandhi) ont &#233;t&#233; &#233;limin&#233;es. En plus, le corpus contenait quelques citations en langues &#233;tran-
g&#232;res et en allemand standard. Ces mots ont &#233;galement &#233;t&#233; exclus. Les 4731 mots restants ont
&#233;t&#233; traduits en allemand standard par l&#8217;auteur. Le contexte du mot dans le texte original aidait
&#224; r&#233;soudre les ambigu&#239;t&#233;s de traduction &#233;ventuelles. La moiti&#233; de cette liste de paires de mots a
&#233;t&#233; r&#233;serv&#233;e pour l&#8217;entra&#238;nement des mod&#232;les, l&#8217;autre moiti&#233; pour l&#8217;&#233;valuation.7
</p>
<p>Dans la partie r&#233;serv&#233;e &#224; l&#8217;entra&#238;nement, nous avons s&#233;lectionn&#233; uniquement des paires dont
la distance de Levenshtein &#233;tait inf&#233;rieure &#224; 3, afin d&#8217;&#233;viter le bruit caus&#233; par des paires non
apparent&#233;es. Par exemple, nos mod&#232;les ne permettent pas de trouver la correspondance entre le
mot bernois himug&#252;egeli &#8216;coccinelle&#8217; et le mot allemand standard Marienk&#228;ferchen.8 Lorsque
le corpus d&#8217;entra&#238;nement est relativement petit, il est crucial qu&#8217;il ne contienne aussi peu de
bruit que possible. Cet &#233;lagage a r&#233;duit la taille du corpus d&#8217;entra&#238;nement de 2365 paires &#224; 1500
paires. De plus, nous avons effectu&#233; nos exp&#233;riences avec des sous-ensembles de ce corpus
contenant 300 et 750 paires de mots. Les mod&#232;les ont &#233;t&#233; entra&#238;n&#233;s avec EM en 50 it&#233;rations.
</p>
<p>Le lexique allemand standard, utilis&#233; dans la seconde &#233;tape, comporte 202 000 formes. Les
informations morphologiques et syntaxiques pr&#233;sentes dans le lexique n&#8217;ont pas &#233;t&#233; utilis&#233;es.
</p>
<p>Le corpus de test contient 2366 paires de mots, dont 407 (17,2%) sont identiques en dialecte
et en allemand standard. 565 mots du corpus (23,9%) ne se retrouvent pas dans le lexique
allemand. M&#234;me si ces mots sont correctement induits par le mod&#232;le de similarit&#233;, ils sont
&#233;limin&#233;s en seconde &#233;tape. Il s&#8217;agit avant tout de noms compos&#233;s, dont certains ont &#233;t&#233; form&#233;s
ad hoc dans le texte litt&#233;raire. En plus, quelques mots du dialecte bernois correspondent &#224; deux
mots en allemand standard (par exemple ir &#8211; in der &#8216;dans la&#8217; ). Pour des raisons de complexit&#233;
computationnelle, nos mod&#232;les ne trouvent pas de telles correspondances.
</p>
<p>5 R&#233;sultats et discussion
</p>
<p>Ci-dessus, nous avons pr&#233;sent&#233; notre architecture d&#8217;induction lexicale &#224; deux &#233;tapes. La pre-
mi&#232;re &#233;tape prend le mot source et g&#233;n&#232;re 10 000 candidats.9 La seconde &#233;tape valide les can-
didats qui se trouvent dans le lexique de la langue cible. En g&#233;n&#233;ral, entre 0 et 20 candidats
sont ainsi valid&#233;s par mot source. Les co&#251;ts ou probabilit&#233;s associ&#233;s aux candidats lors de la
premi&#232;re &#233;tape permettent de les ordonner (cf. figure 1).
</p>
<p>Dans un premier temps, nous nous int&#233;ressons aux candidats situ&#233;s en t&#234;te de liste. La partie
gauche du tableau 1 montre combien de fois le mot allemand correct se trouve en t&#234;te de la liste
des candidats. Le rappel est calcul&#233; comme suit :
</p>
<p>rappel=
nombre de paires correctes en premi&#232;re position
nombre de paires dans le corpus d&#8217;&#233;valuation
</p>
<p>7Les dictionnaires bilingues suisse allemand &#8211; allemand standard disponibles sur Internet se limitent en g&#233;n&#233;ral
aux paires de mots non apparent&#233;es. Ils constituent donc un compl&#233;ment int&#233;ressant &#224; notre approche, mais ne
peuvent pas servir de point de d&#233;part pour l&#8217;entra&#238;nement de nos mod&#232;les.
</p>
<p>8Le seuil de 3 est arbitraire ; nous l&#8217;avons repris de (Mann &amp; Yarowsky, 2001).
9Cette valeur est arbitraire. Dans (Scherrer, 2007), nous avons g&#233;n&#233;r&#233; seulement 500 candidats. Il se trouvait
</p>
<p>alors que pour beaucoup de mots longs, aucun de ces candidats n&#8217;&#233;tait valid&#233; dans la seconde &#233;tape. Les r&#233;sultats
rapport&#233;s ici ne sont donc pas directement comparables.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Yves Scherrer
</p>
<p>T&#234;te de liste Toutes positions
N Pr&#233;cision Rappel F-mesure N Rappel
</p>
<p>Levenshtein 725 22,8 30,6 26,1 932 39,4
Unigrammes 300 840 45,8 35,5 40,0 1384 58,5
Unigrammes 750 859 44,8 36,3 40,1 1441 60,9
Unigrammes 1500 864 45,3 36,5 40,5 1446 61,1
Bigrammes 300 805 32,4 34,0 33,2 1088 46,0
Bigrammes 750 890 39,8 37,6 38,7 1239 52,4
Bigrammes 1500 930 44,4 39,3 41,7 1309 55,3
Trigrammes 1500 394 22,3 16,7 19,0 492 20,8
</p>
<p>TAB. 1 &#8211; Paires lexicales correctement induites apr&#232;s la seconde &#233;tape. La partie gauche du
tableau montre les r&#233;sultats pour les paires correctes induites en t&#234;te de liste. La partie droite
montre les r&#233;sultats pour les paires correctes induites toutes positions de liste confondues. Les
chiffres dans la premi&#232;re colonne se r&#233;f&#232;rent &#224; la taille du corpus d&#8217;entra&#238;nement. &#201;tant donn&#233;
les r&#233;sultats du mod&#232;le Trigrammes 1500, nous avons renonc&#233; &#224; indiquer les chiffres des deux
autres mod&#232;les &#224; trigrammes. N se ref&#232;re au nombre absolu de paires induites, les autres chiffres
repr&#233;sentent des pourcentages.
</p>
<p>Dans certains cas (surtout avec la distance de Levenshtein), plusieurs candidats se trouvent ex
aequo en premi&#232;re position. Il est donc utile de calculer &#233;galement la pr&#233;cision :
</p>
<p>pr&#233;cision=
nombre de paires correctes en premi&#232;re position
</p>
<p>nombre de paires en premi&#232;re position
</p>
<p>La F-mesure est calcul&#233;e de mani&#232;re standard :
</p>
<p>F =
2 &#183;pr&#233;cision &#183; rappel
pr&#233;cision+ rappel
</p>
<p>La partie gauche du tableau 1 montre que les mod&#232;les adaptifs &#224; unigrammes et &#224; bigrammes
donnent de meilleurs r&#233;sultats que la distance de Levenshtein. En revanche, le mod&#232;le des tri-
grammes fournit des r&#233;sultats d&#233;cevants : la taille du corpus d&#8217;entra&#238;nement ne suffit visible-
ment pas pour entra&#238;ner correctement le nombre &#233;lev&#233; de transitions de ce mod&#232;le. Si la taille
du corpus d&#8217;entra&#238;nement semble avoir un impact tr&#232;s l&#233;ger sur le mod&#232;le &#224; unigrammes (aug-
mentation de la F-mesure de 0,4% entre le corpus &#224; 300 paires et celui &#224; 1500 paires), l&#8217;impact
est plus prononc&#233; pour le mod&#232;le &#224; bigrammes (augmentation de la F-mesure de 8,5%). Ce
r&#233;sultat sugg&#232;re qu&#8217;un corpus plus grand pourrait encore am&#233;liorer les performances du mod&#232;le
&#224; bigrammes, permettant d&#8217;obtenir des r&#233;sultats bien meilleurs que les mod&#232;les &#224; unigrammes.
</p>
<p>Au lieu d&#8217;&#233;valuer seulement les candidats apparaissant en premi&#232;re position de la liste, il peut
&#233;galement &#234;tre int&#233;ressant de consid&#233;rer la liste enti&#232;re. Par exemple, une architecture &#233;tendue
pourrait utiliser des heuristiques (fr&#233;quence des mots, contexte syntaxique des mots, . . .) pour
r&#233;ordonner les candidats. Dans ce cas, l&#8217;ordre des candidats propos&#233; par le mod&#232;le de similarit&#233;
graphique aurait peu d&#8217;importance. On s&#8217;int&#233;resserait donc avant tout &#224; ce que le mot correct
se trouve dans la liste, peu importe sa position. La partie droite du tableau 1 montre le nombre
absolu des mots corrects apparaissant dans la liste, ainsi que le rappel.10 Ces chiffres confirment
les caract&#233;ristiques globales des mod&#232;les. En revanche, le mod&#232;le des bigrammes ne parvient
</p>
<p>10Il ne nous semble pas pertinent de calculer la pr&#233;cision pour ce cas de figure.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Transducteurs &#224; fen&#234;tre glissante pour l&#8217;induction lexicale
</p>
<p>pas &#224; &#233;galiser les r&#233;sultats du mod&#232;le &#224; unigrammes. La progression des chiffres selon la taille
du corpus d&#8217;entra&#238;nement sugg&#232;re cependant que les limites du mod&#232;le des bigrammes ne sont
pas encore atteintes.
</p>
<p>Les performances g&#233;n&#233;rales de l&#8217;induction lexicale par similarit&#233; graphique paraissent assez
faibles. Sur un corpus de 2366 paires, moins de 900 paires peuvent &#234;tre induites de mani&#232;re
fiable sans heuristiques suppl&#233;mentaires. Mais comme nous l&#8217;avons d&#233;j&#224; &#233;voqu&#233;, il est impos-
sible d&#8217;obtenir 100% de r&#233;ussite avec l&#8217;architecture choisie. D&#8217;une part, 565 paires ne peuvent
pas &#234;tre induites parce qu&#8217;elles ne sont pas pr&#233;sentes dans le lexique cible. D&#8217;autre part, cer-
taines paires de mots sont compl&#232;tement diff&#233;rentes dans les deux vari&#233;t&#233;s linguistiques, et il
est illusoire de les induire avec des mesures de similarit&#233; graphique. Si on admet, avec (Mann
&amp; Yarowsky, 2001), que les paires avec une distance de Levenshtein inf&#233;rieure &#224; 3 sont faciles
&#224; induire, on obtient une borne de 1256 paires de mots pr&#233;sents dans le lexique cible et faciles
&#224; induire. Selon les chiffres se r&#233;f&#233;rant &#224; toutes les positions de liste, les mod&#232;les &#224; bigrammes
atteignent cette borne, et les mod&#232;les &#224; unigrammes la d&#233;passent m&#234;me. De plus, on constate
que jusqu&#8217;&#224; 70% des paires faciles &#224; induire sont correctement induites en premi&#232;re position.
</p>
<p>Nous avons expliqu&#233; (Scherrer, 2007) que les &#233;tudes pr&#233;c&#233;dentes (Mann &amp; Yarowsky, 2001)
obtiennent de bien meilleurs r&#233;sultats gr&#226;ce &#224; une m&#233;thode d&#8217;&#233;valuation moins s&#233;v&#232;re. Cette
m&#233;thode consiste &#224; trouver les paires &#233;tant donn&#233; une liste de 100 mots de la langue source et la
liste (en d&#233;sordre) des 100 mots correspondants de la langue cible. Ils obtiennent autour de 67%
de r&#233;ussite sur le vocabulaire espagnol-portugais complet. Avec la m&#234;me m&#233;thode d&#8217;&#233;valuation,
nos mod&#232;les atteignent des chiffres entre 85% et 89% avec les unigrammes, et entre 71% et 81%
avec les bigrammes. Sur le vocabulaire des mots apparent&#233;s (distance de Levenshtein inf&#233;rieure
&#224; 3), Mann &amp; Yarowsky obtiennent des chiffres de 92%. Dans cette t&#226;che, les performances de
nos mod&#232;les &#224; unigrammes et &#224; bigrammes se situent entre 91% et 98%.
</p>
<p>6 Conclusion
</p>
<p>Nos exp&#233;riences ont montr&#233; que les mesures de similarit&#233; graphique peuvent faciliter l&#8217;induction
lexicale lorsque les deux langues sont &#233;troitement apparent&#233;es. En plus, nous avons montr&#233; que
l&#8217;utilisation de m&#233;thodes d&#8217;apprentissage automatique permet d&#8217;am&#233;liorer nettement les perfor-
mances par rapport &#224; des mod&#232;les g&#233;n&#233;riques comme la distance de Levenshtein. Le mod&#232;le
&#224; unigrammes fournit de bons r&#233;sultats avec des corpus d&#8217;entra&#238;nement tr&#232;s petits. Le mod&#232;le
utilisant une fen&#234;tre glissante de bigrammes permet de faire des pr&#233;dictions plus cibl&#233;es, aug-
mentant ainsi le rappel. Cependant, ce mod&#232;le n&#233;cessite des corpus d&#8217;entra&#238;nement plus grands
&#224; cause du nombre plus &#233;lev&#233; de transitions &#224; entra&#238;ner. Nos exp&#233;riences sugg&#232;rent que des cor-
pus de plus de 1500 paires de mots pourraient am&#233;liorer davantage les performances du mod&#232;le
&#224; bigrammes. Des recherches futures devraient montrer si tel est le cas. En revanche, le corpus
de 1500 paires s&#8217;est r&#233;v&#233;l&#233; clairement insuffisant pour entra&#238;ner un mod&#232;le &#224; trigrammes. Il reste
&#224; voir si un corpus plus grand permettrait &#224; ce mod&#232;le de d&#233;passer les performances du mod&#232;le
&#224; bigrammes.
</p>
<p>Nous avons constat&#233; que le lexique de la langue cible, que nous utilisons comme mod&#232;le de
langue simple, est insuffisant, car une grande partie des mots compos&#233;s ne s&#8217;y trouve pas. Pour
rem&#233;dier &#224; cette lacune, il pourrait &#234;tre avantageux de ne pas utiliser ce lexique directement,
mais plut&#244;t de mani&#232;re indirecte pour cr&#233;er un mod&#232;le de langue &#224; n-grammes de lettres, &#224;
l&#8217;instar de (Claveau, 2007).</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Yves Scherrer
</p>
<p>Les r&#233;sultats sugg&#232;rent &#233;galement qu&#8217;il peut &#234;tre int&#233;ressant d&#8217;inclure d&#8217;autres heuristiques afin
de s&#233;lectionner la bonne traduction dans la liste des candidats. En particulier, l&#8217;architecture
pr&#233;sent&#233;e ici ne tient compte ni de l&#8217;information contextuelle riche encod&#233;e dans les textes,
ni des informations morphologiques et syntaxiques contenues dans le lexique allemand utilis&#233;
dans la deuxi&#232;me &#233;tape. L&#8217;int&#233;gration de ces informations nous para&#238;t prometteuse, d&#8217;autant
plus que celles-ci sont facilement disponibles pour notre paire de langues. Les m&#233;thodes bas&#233;es
sur la similarit&#233; graphique peuvent donc &#234;tre utilis&#233;es avec profit dans la t&#226;che d&#8217;induction
lexicale pour des langues apparent&#233;es sans pour autant exiger de grandes quantit&#233;s de donn&#233;es
d&#8217;entra&#238;nement.
</p>
<p>Remerciements
</p>
<p>Nous remercions Paola Merlo pour son soutien et ses commentaires pr&#233;cieux au cours de cette
recherche. Nous aimerions aussi remercier &#201;ric Wehrli pour sa permission d&#8217;utiliser le lexique
allemand du projet Fips.
</p>
<p>R&#233;f&#233;rences
</p>
<p>BROWN P. F., PIETRA V. J. D., PIETRA S. A. D. &amp; MERCER R. L. (1993). The mathematics
of statistical machine translation : parameter estimation. Computational Linguistics, 19(2),
263&#8211;311.
</p>
<p>CLAVEAU V. (2007). Inf&#233;rence de r&#232;gles de r&#233;&#233;criture pour la traduction de termes biom&#233;di-
caux. In Actes de TALN 2007, p. 111&#8211;120, Toulouse, France.
</p>
<p>CLAVEAU V. &amp; ZWEIGENBAUM P. (2005). Traduction de termes biom&#233;dicaux par inf&#233;rence
de transducteurs. In Actes de TALN 2005, p. 253&#8211;262, Dourdan, France.
</p>
<p>JANSCHE M. (2001). Re-engineering letter-to-sound rules. In Proceedings of NAACL&#8217;01,
Pittsburgh, PA, USA.
</p>
<p>JIAMPOJAMARN S., KONDRAK G. &amp; SHERIF T. (2007). Applying many-to-many alignments
and hidden markov models to letter-to-phoneme conversion. In Proceedings of NAACL&#8217;07, p.
372&#8211;379, Rochester, NY, USA.
</p>
<p>MANN G. S. &amp; YAROWSKY D. (2001). Multipath translation lexicon induction via bridge
languages. In Proceedings of NAACL&#8217;01, Pittsburgh, PA, USA.
</p>
<p>RAPP R. (1999). Automatic identification of word translations from unrelated English and
German corpora. In Proceedings of ACL&#8217;99, p. 519&#8211;526, Maryland, USA.
</p>
<p>RISTAD E. S. &amp; YIANILOS P. N. (1998). Learning string-edit distance. IEEE Transactions
on Pattern Analysis and Machine Intelligence, 20(5), 522&#8211;532.
SCHERRER Y. (2007). Adaptive string distance measures for bilingual dialect lexicon induc-
tion. In Proceedings of ACL&#8217;07, Student Research Workshop, Prague, R&#233;publique Tch&#232;que.</p>

</div></div>
</body></html>