RECITAL 2008, Avignon, 9-13 juin 2008

Vers une nouvelle approche
de la correction grammaticale automatique

Agnes Souque
Laboratoire LIDILEM - Université Stendhal - Grenoble 3
1491 rue des Residences
38040 Grenoble Cedex 09
asouque @ gmail.com

Résumé. La correction grammaticale automatique du frangais est une fonctionnalité qui
fait cruellement défaut a la communauté des utilisateurs de logiciels libres. Dans le but de
combler cette lacune, nous avons travaillé a l’adaptation au frangais d’un outil initialement
développé pour une langue étrangere. Ce travail nous a permis de montrer que les approches
classiques du traitement automatique des langues utilisées dans le domaine ne sont pas ap-
propriées. Pour y remédier, nous proposons de faire évoluer les formalismes des correcteurs
en intégrant les principes linguistiques de la segmentation en chunks et de l’uniﬁcation. Bien
qu’efﬁcace, cette évolution n’est pas sufﬁsante pour obtenir un bon correcteur grammatical du
frangais. Nous envisageons alors une nouvelle approche de la problématique.

Abstract. Free software users community is sorely lacking French grammar checking.
With the aim of ﬁlling this gap, we have worked on the adaptation to French of a tool ori-
ginally developped for a foreign language. Thanks to this work, we could show that classic
natural language processing approaches used in grammar checking are not suitable. To remedy
it, we suggest an evolution of grammar checkers that includes linguistic principles such as chun-
king and uniﬁcation. Despite its efﬁciency, this evolution is not sufﬁcient to get a good French
grammr checker. We are then thinking of a new approach of the problem.

M0tS-CléS I correction grammaticale, syntagme, uniﬁcation.

Keywords: grammar checking, chunk, uniﬁcation.

1 Introduction

Dans le domaine de la correction grammaticale automatique, les outils existants sont générale-
ment des logiciels propriétaires aux coﬁts d’intégration élevés, qui sont peu ou pas décrits dans
la littérature, et fermés a toute amélioration exteme. Nos travaux de recherche visent done a
développer un outil de correction grammaticale libre pour le frangais, dont les ressources lan-
gagieres seront accessibles et modiﬁables, et dont le formalisme générique le rendra facilement
adaptable a d’autres langues.

Dans (Souque, 2007), nous avons étudié plusieurs correcteurs libres existant dans des langues
étrangeres. L’ adaptation au frangais de l’un d’entre eux nous a permis de mettre a la disposi-
tion de la communauté scientiﬁque, mais aussi de la communauté des utilisateurs de logiciels

Agnes Souque

libres (OpenOfﬁce.org, WEB), un formalisme, un outil informatique et une base conséquente
de regles de correction grammaticale. Cependant, nous avons mis en évidence l’inadéquation
des approches classiques du traitement automatique des langues a la problématique.

Dans la section 1 de cet article, nous commencons par expliquer dans quelle mesure le principe
de fonctionnement des correcteurs libres que nous avons étudiés nuit a leur efﬁcacité. Nous
exposons plus particulierement le probleme de la détection des erreurs qui se fonde sur des
regles sans abstraction et sur un systeme de pattem-matching rigide, dont une des conséquences
est la nécessité d’énumérer dans des regles toutes les fautes possibles.

Pour simpliﬁer la détection des erreurs, nous proposons dans la section 2 une évolution des
formalismes des correcteurs grammaticaux, qui combine deux approches syntaxiques issues
d’écoles opposées : la segmentation en chunks et l’uniﬁcation de structures de traits.

La section 3 est consacrée a la validation de l’évolution présentée dans la section précédente et
aux bénéﬁces qu’elle peut apporter. Nous avons pour cela développé un prototype de calculateur
d’uniﬁcation auquel nous avons soumis des phrases contenant des erreurs de grammaire.

2 Une correction grammaticale limitée

I1 ne nous a pas été possible d’analyser le fonctionnement des correcteurs grammaticaux pro-
priétaires, ces derniers n’étant pas documentés. Les outils que nous avons étudiés sont donc
exclusivement des outils libres, tel que An Gramadoir (AnGramadoir, WEB) de Kevin Scan-
nell, ou (LanguageTool, WEB) développé par Daniel Naber (Naber, 2003).

Ces systemes de correction grammaticale ont une structure en couche qui effectue le traite-
ment du texte en plusieurs étapes successives. Dans un premier temps, le texte est segmenté en
phrases, puis en tokens 1. L’ étape suivante consiste a étiqueter morpho-syntaxiquement chacun
de ces tokens. Lors de cette étape, les mots peuvent recevoir plusieurs étiquettes s’ils sont ambi-
gus. Un traitement supplémentaire est alors nécessaire pour les désambigu'1'ser, c’est-a-dire pour
réduire le nombre d’étiquettes qu’ils possedent, selon une méthode statistique ou fondée sur
des regles. La derniere étape consiste enﬁn a détecter les erreurs de grammaire. Cette détection
utilise le principe du pattem-matching a partir d’une base de regles de correction. Ce principe
rigide consiste a comparer des séquences de tokens du texte avec des patrons de séquences de
tokens décrits dans les regles. Une erreur est détectée lorsqu’il y a correspondance exacte entre
les deux. Le correcteur LanguageTool que nous avons adapté au francais dans (Souque, 2007),
fonctionne selon ce principe.

Dans cette section, nous présentons les principales conséquences de l’utilisation du pattern-
matching rigide dans les correcteurs grammaticaux que nous avons étudiés. Ces outils néces-
sitent un tres grand nombre de regles de correction, ils se retrouvent pris au piege d’un cercle
vicieux et se révelent incapables de détecter des erreurs, pourtant fréquentes, impliquant des
mots distants.

1Un token est généralement un signe de ponctuation ou un mot, au sens forme graphique. En effet, le mot en
linguistique peut désigner une unité sémantique constituée de plusieurs fonnes graphiques. Par exemple, "pomme
de terre" est une unité composée de 3 formes graphiques, qui correspondent a 3 tokens.

Vers une nouvelle approche de la correction grammaticale automatique

2.1 L’explosion combinatoire des régles

Pour qu’une erreur grammaticale soit détectée, elle doit donc étre décrite dans une regle. Ceci
implique que pour avoir une efﬁcacité maximale, le correcteur doit posséder une regle de cor-
rection pour chaque erreur possible.

Si nous prenons l’eXemple simple des syntagmes nominaux. Ils sont principalement constitués
de déterminants, de noms et d’adjectifs, en quantité variable, chacun se déclinant en plusieurs
genres et nombres. Les correcteurs An Gramadoir ou LanguageTool utilisent par exemple un
lexique dans lequel les déterminants, les noms et les adjectifs peuvent étre de genre masculin,
féminin ou épicene, et de nombre singulier, pluriel ou invariable.

Si nous voulons écrire une regle pour chaque erreur d’accord possible dans un syntagme no-
minal, la non abstraction des motifs dans les regles nous oblige d’une part a énumérer toutes
les positions que peut prendre chaque mot au sein du syntagme en fonction de sa catégorie, et
d’autre part a tenir compte pour chacun de toutes les combinaisons genre-nombre qu’il peut
avoir. Nous sommes confrontés ainsi rapidement a une explosion combinatoire du nombre de
regles, telle que le montre la ﬁgure 1.

600 000 C - *5

21 S58

??5

28

Nombre Lie wmbinaisons possible;

1 0 Q
2 3 4 5 E5
Nombre de mcns [DeL!NomIAdj] dans le symagme

FIG. 1 — Explosion combinatoire du nombre de regles pour un syntagme nominal

Nous retrouvons le méme probleme pour tous les types d’erreurs. Le fait de devoir décrire tous
les contextes de fautes implique qu’il faut imaginer toutes les combinaisons erronées de mots,
ce qui est impossible.

I1 s’ensuit de nombreux problemes de silence ou de bruit dans la détection des fautes : du
silence lorsqu’une erreur n’a pas été prévue et n’est pas décrite dans une regle ; du bruit lorsque
plusieurs regles, parmi les tres nombreuses existantes, détectent une meme erreur.

Par exemple, dans un énoncé tel que *"La petit aiguille indique l’ heure"
l’erreur de genre de "pet it" est détectée a deux reprises par le correcteur LanguageTool :

— une premiere regle s’applique car un déterminant féminin est suivi d’un adjectif masculin;

— une seconde regle s’applique car un adjectif masculin est suivi d’un nom féminin.

Il est généralement préférable pour l’utilisateur que le silence soit privilégié au bruit. Les fausses
détections sont en effet tres génantes, et lorsqu’elles sont trop fréquentes, elles conduisent sou-
vent l’utilisateur a ne plus utiliser le correcteur.

Agnes Souque

2.2 Le cercle vicieux

Le nombre de regles n’est pas l’unique responsable du bruit et du silence. En effet, pour que les
regles de correction puissent s’appliquer et que la détection des erreurs se fasse correctement,
selon le principe du pattem-matching, il faut que la séquence de tokens contenant l’erreur et le
patron décrit dans une regle correspondent parfaitement. Cependant, les erreurs de grammaire
ou d’orthographe dans le texte peuvent conduire a des erreurs d’étiquetage, qui a leur tour
nuisent a l’application des regles, car la correspondance entre le texte et les motifs des regles ne
peut plus se faire.

Par exemple, la regle suivante décrit un motif permettant de détecter l’oubli de la particule
"ne" de la négation : pronom sujet + verbe + "pas "

En présence d’un énoncé tel que : * " Il travail pas as sez . " la regle ne peut pas
étre appliquée. En effet, tel qu’il est orthographié, le mot "travail" porte l’étiquette nom.
La séquence ne correspond donc pas au motif décrit dans la regle et l’erreur concernant l’oubli
de " ne " ne peut pas étre détectée.

Nous nous heurtons ainsi au probleme du cercle vicieux en correction grammaticale : la bonne
détection des erreurs dépend d’un bon étiquetage, qui lui-méme dépend d’un texte sans erreurs...

Etiquetage

Q Correction

grammaticale

FIG. 2 — Cercle vicieux

2.3 La limitation au contexte immédiat

Le principe du pattem-matching a également pour conséquence de rendre la détection des er-
reurs portant sur des relations distantes tres difﬁcile, voire impossible. Prenons par exemple
l’énoncé ci-dessous :

*"Les personnes en situation de test s’autosurveille
énormément."

Le Verbe " aut o surve i lle " n’est pas accordé correctement avec le syntagme nominal sujet
"Le s personne s". Pour détecter cette erreur d’accord sur le Verbe, il faut une regle décri-
vant un motif du type : Det fém plur + Nom fém plur + 5 mots + Verbe 3e pers Sing

Une telle regle convient dans notre exemple précis, mais ne fonctionne plus si les mots qui
doivent s’accorder sont séparés d’un nombre de tokens différent de 5. Il faudrait donc créer une
regle pour chaque quantité de mots, quantité qu’il est impossible de prévoir.

D’autre part, nous pouvons évoquer a nouveau, avec cet exemple, le probleme de l’explosion
combinatoire des regles. En dehors du nombre de "mots séparateurs" a prévoir, il faut également
créer une regle pour chaque possibilité de sujet du Verbe, c’est-a-dire avec chaque combinaison
de mots pouvant constituer un syntagme nominal sujet.

Vers une nouvelle approche de la correction grammaticale automatique

2.4 Conclusion

Le principe du pattern-matching rigide, avec la non abstraction des motifs décrits dans les
regles, oblige a prévoir toutes les erreurs possibles et rend ainsi tres coﬁteux le travail de consti-
tution du nombre démesuré de regles nécessaires, travail qui doit par ailleurs étre répété pour
chaque langue.

Nous pensons qu’il est possible de simpliﬁer ce travail en utilisant des principes linguistiques
mieux adaptés, permettant d’intégrer un niveau d’abstraction dans les regles et de remplacer
ainsi les nombreuses regles spéciﬁques par quelques regles génériques simples.

3 Une amelioration possible des correcteurs

La simpliﬁcation des regles est indispensable a une correction plus efﬁcace et moins coﬁteuse.
Pour y parvenir, nous avons combiné, de maniere atypique, les deux concepts de la segmentation
en chunks et de l’uniﬁcation de structures de traits (Miller & Toris, 1990).

Dans cette partie, nous présentons donc ces deux concepts et l’intérét de les combiner pour
simpliﬁer la détection des fautes.

3.1 La segmentation en chunks

(Abney, 1991) déﬁnit les chunks de la maniere suivante :

« The typical chunk consists of a single content word surrounded by a constellation
of function words, matching a ﬁxed template »

Un chunk est donc un groupe de mots contigus, réunis autour d’une téte lexicale dont ils de-
pendent. Ces relations de dépendance font de la structure interne des chunks une structure re-
lativement ﬁgée, dans laquelle les contraintes d’accord sont assez fortes, ce qui nous intéresse
particulierement pour la correction grammaticale.

Les chunks sont principalement de type nominal ou verbal, et plus rarement adjectival ou adver-
bial. Ils sont délimités grace aux mots grammaticaux, a la ponctuation ou aux marques morpho-
logiques, et sont donc relativement faciles a déﬁnir. En général, un chunk commence par un mot
grammatical ou juste apres une ponctuation, et se terIr1ine juste avant une ponctuation ou le mot
grammatical suivant, qui marquent alors le début d’un nouveau chunk. Ainsi, la déﬁnition d’un
chunk ne se fait pas en fonction de son contenu, mais en fonction de ses marqueurs de début et
de ﬁn.

Les chunks sont également appelés syntagmes, mais ils font partie des grammaires robustes
du TAL dans lesquelles la notion de syntagme differe de celle des grammaires chomskyennes,
d’o1‘1 est issu le principe d’uniﬁcation. Dans (Chomsky, 1979), les syntagmes sont récursifs par
exemple, ce qui signiﬁe qu’ils peuvent contenir d’autres syntagmes du meme type. Ainsi " un
code élaboré et un code restreint " est considéré comme un syntagme nomi-
nal constitué de deux autres syntagmes nominaux.

Pour les grammaires robustes au contraire, l’exemple ci-dessus consiste en trois syntagmes dis-
tincts, ou chunks : " [un code élaboré] [et] [un code restreint] ". Ces
syntagmes ne sont pas récursifs, mais constituent au contraire un niveau hiérarchique entre les
mots et la phrase. Ainsi, chaque chunk est constitué d’éléments du niveau hiérarchique infe-

Agnes Souque

rieur (les mots) et constitue a son tour un élément du niveau hiérarchique supérieur (la phrase)
(Vergne & Giguet, 1998).

Lorsque nous employons le mot syntagme dans la suite de cet article, nous faisons référence
aux syntagmes non récursifs, c’est-a-dire aux chunks.

L’ exemple que nous donnons en 2.3 est segmenté en chunks de la maniere suivante :

[*Les personnes] [en situation] [de test] [s’autosurveille
énormément].

Au sein d’une phrase, les chunks entretiennent également des relations de dépendance. Un syn-
tagme dépend généralement de son prédécesseur, sauf dans le cas d’un chunk verbal. Ce dernier
dépend du chunk sujet, qui correspond en principe au premier syntagme nominal a gauche.
Ces propriétés de dépendance vont s’avérer tres utiles pour veriﬁer les accords entre différents
syntagmes.

3.2 L’uniﬁcation de structures de traits

Les étiquettes morpho-syntaxiques attribuées a chaque mot lors de l’étiquetage du texte consti-
tuent des structures de traits. Elles décrivent chaque élément des phrases en énumérant ses
caractéristiques linguistiques, sous la forme de liste de couples attribut-valeur.

L’ uniﬁcation est déﬁnie de maniere générale dans (Abeillé, 1993) :

« L’uniﬁcation de deux structures de traits A et B (notée A U B) est la structure
minimale qui est £1 lafois une extension de A et de B. Si une telle extension n’existe
pas, l’uniﬁcation «échoue» (ce qui est note’ J.) ».

Plus précisément, dans le cadre de notre travail, nous dirons que l’uniﬁcation consiste a combi-
ner les traits de deux étiquettes eta veriﬁer leur compatibilité. Deux structures de traits peuvent
s’uniﬁer si elles ont les memes valeurs pour des attributs identiques.

Par exemple, dans la ﬁgure 3, l’uniﬁcation échoue entre l’étiquette du nom " genoux " et celle
du déterminant " le " , car la valeur de l’attribut nombre n’est pas identique.

Le genoux
genre = meeeulin genre = rneeeulin _ .
nembre = eingulier U nombre = pluriel ‘ J- 9'3"“

FIG. 3 — Echec de l’uniﬁcation

3.3 La combinaison des chunks et de l’uniﬁcation

Le fait de combiner le découpage en chunks et l’uniﬁcation de structures de traits Va permettre
de faciliter la vériﬁcation grammaticale, en particulier pour les phénomenes d’accord.

Les chunks délimitent des zones de calcul en fonction de leurs frontieres, et non plus en fonction
de leur contenu, comme c’est le cas avec les patrons de séquences de tokens dans les regles. I1
n’est alors plus nécessaire de lister exhaustivement toutes les combinaisons de mots.

A l’intérieur de ces zones de calcul, tous les éléments doivent s’accorder, ce qui signiﬁe que
tous leurs traits doivent s’uniﬁer, indépendamment de leur catégorie grammaticale. Il est ainsi

Vers une nouvelle approche de la correction grammaticale automatique

possible de détecter les erreurs d’accord, meme si un mot est ambigu ou mal étiqueté.

De plus, en attribuant aux chunks les traits de leur téte lexicale, ils peuvent alors s’uniﬁer entre
eux, et des erreurs entre groupes de mots peuvent ainsi étre détectées, comme par exemple l’ac-
cord entre le syntagme nominal sujet et le syntagme verbal.

L’utilisation combinée de ces deux principes linguistiques permet donc d’introduire une abs-
traction dans les motifs des regles, dont le nombre peut étre ainsi considérablement réduit.

4 Un prototype de calculateur d’uniﬁcation

Pour valider notre hypothese selon laquelle la combinaison des chunks et de l’uniﬁcation peut
permettre de détecter des fautes a l’aide de quelques regles simpliﬁées, nous avons concu un
calculateur d’uniﬁcation. L’ outil prend en entrée des phrases préalablement étiquetées et seg-
mentées en chunks, il recherche des erreurs d’accord en calculant l’uniﬁcation a l’aide d’ex-
pressions régulieres, puis crée un ﬁchier de sortie contenant des informations sur les erreurs
détectées.

4.1 L’étiquetage de phrases test

Pour pouvoir tester notre outil, nous avions besoin de phrases étiquetées et segmentées. Nous
avons choisi le formalisme XML pour représenter ces phrases. Bien que plus lourd a traiter
par le programme, ce formalisme a plusieurs avantages. Il est facilement lisible et modiﬁable
par des linguistes, qui ne sont pas nécessairement informaticiens, c’est un formalisme ouvert
qui permet l’implantation de nombreuses fonctionnalités, et les balises qui le constituent sont
faciles a identiﬁer par un outil tel que le notre qui utilise des expressions régulieres. I1 s’agit
d’autre part du formalisme d’étiquetage utilisé dans le correcteur LanguageTool sur lequel nous
avons travaillé.

Ne disposant pas d’étiqueteur en XML ni de segmenteur en chunks compatibles avec ce que
nous voulions obtenir, nous avons réalisé le travail manuellement sur quelques phrases, extraites
d’un corpus de variations orthographiques (COVAREC, 1994) (Lucci & Millet, 1994). Nous
avons obtenu des phrases étiquetées de la maniere suivante :

*"Les personnes en situation de test s’autosurveille énormément"
<SN genre="f" nombre="p">
<D nombre="p">Les</D>
<N genre="f" nombre="p">personnes</N>
</SN>
<SP>
<P>en</P>
<N genre="f" nombre="s">situation</N>
</SP>
<SP>
<P>de</P>
<N genre="m" nombre="s">test</N>
</SP>
<SV type="ppal">
<R type="refl" pers="3" >s’</R>

Agnes Souque

<V mode="ind" temps="pres" pers="3" nombre="s">
autosurveille</V>
<A>énormément</A>
</SV>

Les balises SN, SV et SP délimitent respectivement les chunks nominaux, verbaux et preposi-
tionnels. Chacune contient a son tour un ou plusieurs elements : dans notre exemple, nous avons
les balises D (déterminant), N (nom), P (preposition), V (verbe), R (pronom) et A (adverbe), A
l’intérieur des balises, divers attributs déﬁnissent les traits morpho-syntaxiques (genre, nombre,
temps, etc.) de chaque mot.

4.2 Le fonctionnement du prototype

Notre prototype fonctionne a l’aide d’expressions régulieres, qui permettent de trouver faci-
lement des motifs dans les chaines de caracteres. Comme tout l’étiquetage se fonde sur des
balises, il est facile de déﬁnir des motifs pour repérer les différents niveaux : phrases, chunks,
éléments et attributs.

L’outil commence par parcourir le ﬁchier XML aﬁn d’isoler et de stocker dans un tableau
chaque niveau dans une phrase. Ainsi, pour l’exemple en 4.1, l’outil va commencer par iso-
ler et stocker le chunk SN, ses attributs et les 2 elements D et N avec leurs attributs, puis il va
continuer avec les chunks suivants jusqu’a la ﬁn de la phrase.

Viennent ensuite les calculs d’uniﬁcation. Ils sont d’abord effectués au sein des chunks, a partir
des données stockées précédemment. Le programme compare la valeur de chaque attribut d’un
élément avec la valeur du meme attribut dans tous les autres éléments du chunk traité. Si deux
memes attributs compares n’ ont pas une valeur identique, l’uniﬁcation échoue et renvoie "faux",
ce qui signiﬁe qu’il y a une erreur d’accord. Dans ce cas, le programme indique dans le ﬁchier
de sortie (en HTML) le chunk concerné, la phrase dans laquelle il se situe, un commentaire sur
le type d’erreur détectée et une suggestion de correction.

Une fois toutes les veriﬁcations intra-chunks effectuées, le programme procede alors aux cal-
culs d’uniﬁcation entre les chunks. Il s’agit dans ce cas généralement de detection d’erreurs
d’accord entre le sujet et le verbe. Ainsi, le verbe du chunk verbal principal doit avoir un attri-
but nombre de meme valeur que l’attribut nombre du premier chunk nominal a gauche, celui-ci
étant généralement suj et. De meme, il doit y avoir uniﬁcation entre les attributs genre et nombre
du chunk nominal sujet et ceux d’un éventuel participe passe dans le chunk verbal. Lorsque le
sujet est un pronom personnel, celui-ci étant inclus au chunk verbal, une erreur d’accord sera
détectée lors des calculs d’uniﬁcation intra-chunks.

En cas d’échec de l’uniﬁcation, le meme type d’indications que lors de la veriﬁcations intra-
chunk est retourné dans le ﬁchier de sortie.

4.3 Les résultats des tests

Nous avons testé notre prototype sur les phrases que nous avons préalablement étiquetées ma-
nuellement et nous avons obtenus des résultats plutet encourageants.

Les erreurs d’accord dans les syntagmes nominaux ont toutes été détectées, sans qu’il soit fait
usage d’une base démesurée de regles de correction. Le correcteur libre An Gramadoir (AnGra-
madoir, WEB) par exemple possede une base d’environ 450 regles, utilisant des expressions

Vers une nouvelle approche de la correction grammaticale automatique

régulieres et des macros complexes, pour la détection des erreurs d’accord dans les syntagmes
nominaux. En utilisant les propriétés des chunks et l’uniﬁcation, ces regles ne sont plus néces-
saires. Pour détecter ce type d’erreurs, il sufﬁt de calculer l’uniﬁcation entre tous les constituants
d’un chunk.

Les fautes d’accord entre les syntagmes nominaux sujet et leur verbe ou participe passé ont
également été détectées correctement. Par exemple, si nous reprenons notre énoncé :

"Les personnes en situation de test s’autosurveille énormément"
l’erreur de personne sur le verbe a été signalée. De telles erreurs sont généralement difﬁcile-
ment détectables par la plupart des correcteurs, car elles sont souvent associées a des relations
de dépendance distantes. Avec les propriétés de dépendance que les chunks entretiennent entre
eux, ce type d’erreur peut non seulement étre détecté, mais en plus détecté avec un nombre tres
réduit de regles simples.

Notre prototype ne détecte pour le moment que quelques types d’erreurs que nous venons de
mentionner (accords dans les SN, accords sujet-verbe, accords sujet-participe passé). Il permet
néanmoins de montrer que la combinaison chunks-uniﬁcation est efﬁcace pour détecter les er-
reurs d’accord. La détection se fait correctement et a moindre coﬁt car elle ne nécessite plus la
rédaction de tres nombreuses regles.

5 Conclusion et perspectives

Les correcteurs grammaticaux libres que nous connaissons ont tous a peu pres la meme struc-
ture et se fondent sur un systeme de pattem-matching, que l’absence d’abstraction des patrons
décrits dans les regles rend tres rigide. Comme nous l’avons vu dans cet article, ceci limite
de maniere importante les performances des correcteurs. Ces derniers doivent disposer d’un
nombre démesuré de regles (notamment pour les fautes d’accord), qui ne permettent cependant
pas de détecter toutes les erreurs et sont également souvent source de fausses détections.

Les approches TAL mises en oeuvres n’étant pas adaptées a la problématique, nous nous sommes
donc tournée Vers d’autres approches : la segmentation en chunks et l’uniﬁcation. Le fait de
les combiner permet d’intégrer une abstraction et ainsi de simpliﬁer les regles et d’en réduire
considérablement le nombre. Les chunks constituent en effet des zones de calcul a l’intérieur
desquelles les traits de tous les éléments doivent s’uniﬁer.

Pour vériﬁer la faisabilité d’une telle combinaison chunk-uniﬁcation, nous avons développé un
prototype de calcul d’uniﬁcation. En présence de phrases préalablement étiquetées en )ﬂV[L et
segmentées en chunks, l’outil s’est révélé parfaitement capable de détecter les erreurs d’accord,
aussi bien dans qu’entre les chunks. Cet outil nous a montré qu’il est donc possible, avec un
nombre tres réduit de regles simples, de détecter les erreurs d’accord quelle que soit la dis-
tance qui sépare les mots concernés, alors que ces détections ne sont actuellement pas toujours
possibles et nécessitent énormément de regles, parfois tres complexes.

Les bénéﬁces que nous avons mentionnés sont importants mais selon nous pas sufﬁsants pour
obtenir un outil capable de corriger efﬁcacement le francais. Il nous semble nécessaire de sortir
d’une part de l’approche énumérative des regles, d’autre part de l’approche en couches tra-
ditionnelle, ou le traitement du texte est décomposé en plusieurs étapes successives et ou la
détection des fautes est prisonniere d’un cercle vicieux (ﬁgure 2).

Nous envisageons donc une approche innovante de la correction grammaticale, que nous pour-
rions nommer "gauche-droite". Le principe est de construire simultanément l’analyse morpho-

Agnes Souque

syntaxique et la correction au fur et a mesure de la rédaction / lecture de la phrase, et de recher-
cher des incohérences grammaticales plutot que d’énumérer exhaustivement toutes les erreurs
possibles. Plus précisément, en se fondant sur le principe des latences (Tesniere, 1959) ou d’at-
tentes réciproques (Lebarbé, 2002), construire la structure syntaxique dans l’ordre de lecture des
tokens, valider les attentes réciproques (un déterminant attend un nom), constr11ire les chunks et
tester l’uniﬁcation : l’échec de l’un des trois constitue une détection d’erreur de grammaire et
donne son explication.

L’analyse de * " Les premiers linguistes on donc d’ abord écouté "
générerait une erreur sur le mot "on", le syntagme nominal qui le précede ayant une latence
droite pour un verbe ou une préposition.

"Les premiers linguistes on donc d’abord écouté"

[SN ———————————————————— ——] erreur
Un message du type "un verbe est attendu" serait alors généré pour indiquer l’erreur a l’utilisa-
teur.
Cette approche, sur laquelle nous travaillons dans le cadre de notre these, implique une recon-
sidération complete des formalismes existants (déclaration non plus des fautes, mais de ce qui
est attendu) et du traitement.

Références

ABEILLE A. (1993). Les nouvelles syntaxes. Grammaires d’uniﬁcation et analyse dufrancais.
Armand Colin.

ABNEY S. (1991). Parsing by chunks. In Principle-based parsing .' Computation and Psycho-
linguistics, p. 257-278. Kluwer Academic Publishers.

ANGRAMADOIR (WEB). http ://borel.slu.edu/gramadoir.

CHOMSKY N. (1979). Structures syntaxiques. Editions Seuil.

COVAREC (1994). Corpus de variations orthographiques. Laboratoire LIDILEM, Université
Stendhal-Grenoble 3.

LANGUAGETOOL (WEB). http ://www.languagetool.org/.

LEBARBE T. (2002). Hiérarchie inclusive des unités linguistiques en analyse syntaxique co-

opérative ; le segment, unite’ intermédiaire entre chunk et phrase dans le traitement linguistique
par systeme multi-agents. PhD thesis, Université de Caen.

LUCCI V. & MILLET A. (1994). L’orthographe de tous les jours, enquéte sur les pratiques
orthographiques desfrancais. Editions Champion.

MILLER P. & TORIS T. (1990). Formalisme pour le TALN. Hermes.

NABER D. (2003). A rule-based and grammar checker. PhD thesis, Technische Fakultat,
Universitat Beilefeld.

OPENOFFICE.0RG (WEB). http ://fr.openofﬁce.org/.

SOUQUE A. (2007). Conception et développement d’un formalisme de correction gramma-
ticale automatique - Application au francais. Mémoire de master 2 recherche sciences de
langage, Université Stendhal - Grenoble 3.

TESNIERE L. (1959). Elements de syntaxe structurale. Klincksieck.

VERGNE J. & GIGUET E. (1998). Regards théoriques sur le "tagging". Actes de la cinquieme
conférence Le Traitement Automatique des Langues Naturelles.

