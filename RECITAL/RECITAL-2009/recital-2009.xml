<?xml version="1.0" encoding="UTF-8"?>
<conference>
	<edition>
		<acronyme>RECITAL'2009</acronyme>
		<titre>11e Rencontres des Étudiants Chercheurs en Informatique pour le Traitement Automatique des Langues</titre>
		<ville>Senlis</ville>
		<pays>France</pays>
		<dateDebut>2009-06-24</dateDebut>
		<dateFin>2009-06-26</dateFin>
		<presidents>
			<nom>Thibault Mondary</nom>
			<nom>Aurélien Bossard</nom>
			<nom>Thierry Hamon</nom>
		</presidents>
		<typeArticles>
			<type id="long">Papiers longs</type>
		</typeArticles>
		<statistiques>
			<acceptations id="long" soumissions="15">12</acceptations>
		</statistiques>
		<siteWeb>http://lipn.univ-paris13.fr/taln09/index.php?conf=RECITAL</siteWeb>
		<meilleurArticle>
			<articleId>recital-2009-long-003</articleId>
		</meilleurArticle>
	</edition>
	<articles>
		<article id="recital-2009-long-001" session="Présentations Orales">
			<auteurs>
				<auteur>
					<nom>Pierre Gotab</nom>
					<email>pierre.gotab@univ-avignon.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIA / Université d’Avignon, 339 chemin des Meinajariès, 84911 Avignon</affiliation>
			</affiliations>
			<titre>Apprentissage automatique et Co-training</titre>
			<type>long</type>
			<pages></pages>
			<resume>Dans le domaine de la classification supervisée et semi-supervisée, cet article présente un contexte favorable à l’application de méthodes statistiques de classification. Il montre l’application d’une stratégie alternative dans le cas où les données d’apprentissage sont insuffisantes, mais où de nombreuses données non étiquetées sont à notre disposition : le cotraining multi-classifieurs. Les deux vues indépendantes habituelles du co-training sont remplacées par deux classifieurs basés sur des techniques de classification différentes : icsiboost sur le boosting et LIBLINEAR sur de la régression logistique.</resume>
			<mots_cles>Apprentissage automatique, classification, co-training</mots_cles>
			<title></title>
			<abstract>In the domain of supervised and semi-supervised classification, this paper describes an experimental context suitable with statistical classification. It shows an alternative method usable when learning data is unsufficient but when many unlabeled data is avaliable : the multi-classifier co-training. Two classifiers based on different classification methods replace the two independent views of the original co-training algorithm : icsiboost based on boosting and LIBLINEAR which is a logistic regression classifier.</abstract>
			<keywords>Machine learning, classification, co-training</keywords>
		</article>
		<article id="recital-2009-long-002" session="Présentations Orales">
			<auteurs>
				<auteur>
					<nom>Marianne Santaholma</nom>
					<email>Marianne.Santaholma@unige.ch</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">TIM/ISSCO/ETI – Université de Genève, 1211 Genève 4, CH</affiliation>
			</affiliations>
			<titre></titre>
			<type>long</type>
			<pages></pages>
			<resume>Nous présentons une comparaison de la performance de deux types différents de reconnaisseurs pour le japonais et l’anglais basés sur les grammaires. L’un des systèmes est dérivé à partir de règles d’une grammaire monolingue et l'autre de règles paramétrisées et multilingues. Ce dernier emploie, les mêmes règles de grammaire pour la création de modèles de langue nécessaires à la reconnaissance des langues typologiquement différentes. Nous avons effectué des expériences sur la reconnaissance dans les applications de dialogue de domaine limitée. Ces expériences montrent que les modèles de langue dérivés des règles multilingues de grammaire (1) traitent aussi bien l’un que l’autre les deux langues examinées, et (2) que leur performance est comparable à celle des reconnaisseurs dérivés de grammaires monolingues. Ceci suggère que le partage de grammaires entre langues typologiquement différentes pourrait être une solution pour rendre plus efficace le développement de systèmes de reconnaissance de la parole linguistiques.</resume>
			<mots_cles>Grammaire multilingue paramétrisé, reconnaissance de la parole</mots_cles>
			<title>Comparing Speech Recognizers Derived from Mono- and Multilingual Grammars</title>
			<abstract>This paper examines the performance of multilingual parameterized grammar rules on speech recognition. We present a performance comparison of two different types of Japanese and English grammar-based speech recognizers. One system is derived from monolingual grammar rules and the other from multilingual parameterized grammar rules. The latter one uses hence the same grammar rules for creation of the language models for these two different languages. We carried out experiments on speech recognition of limited domain dialog application. These experiments show that the language models derived from multilingual parameterized grammar rules (1) perform equally well on both tested languages, on English and Japanese, and (2) that the performance is comparable with the recognizers derived from monolingual grammars that were explicitly developed for these languages. This suggests that the sharing grammar resources between different languages could be one solution for more efficient development of rule-based speech recognizers.</abstract>
			<keywords>Parameterized multilingual grammar, speech recognition, typologically different languages</keywords>
		</article>
		<article id="recital-2009-long-003" session="Présentations Orales">
			<auteurs>
				<auteur>
					<nom>Clémentine Adam</nom>
					<email>adam@univ-tlse2.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>François Morlane-Hondère</nom>
					<email>morlanehondere@gmail.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">CLLE / Université de Toulouse &amp; CNRS</affiliation>
			</affiliations>
			<titre>Détection de la cohésion lexicale par voisinage distributionnel : application à la segmentation thématique</titre>
			<type>long</type>
			<pages></pages>
			<resume>Cette étude s’insère dans le projet VOILADIS (VOIsinage Lexical pour l’Analyse du DIScours), qui a pour objectif d’exploiter des marques de cohésion lexicale pour mettre au jour des phénomènes discursifs. Notre propos est de montrer la pertinence d’une ressource, construite par l’analyse distributionnelle automatique d’un corpus, pour repérer les liens lexicaux dans les textes. Nous désignons par voisins les mots rapprochés par l’analyse distributionnelle sur la base des contextes syntaxiques qu’ils partagent au sein du corpus. Pour évaluer la pertinence de la ressource ainsi créée, nous abordons le problème du repérage des liens lexicaux à travers une application de TAL, la segmentation thématique. Nous discutons l’importance, pour cette tâche, de la ressource lexicale mobilixsée ; puis nous présentons la base de voisins distributionnels que nous utilisons ; enfin, nous montrons qu’elle permet, dans un système de segmentation thématique inspiré de (Hearst, 1997), des performances supérieures à celles obtenues avec une ressource traditionnelle.</resume>
			<mots_cles>Cohésion lexicale, ressources lexicales, analyse distributionnelle, segmentation thématique</mots_cles>
			<title></title>
			<abstract>The present work takes place within the Voiladis project (Lexical neighborhood for discourse analysis), whose purpose is to exploit lexical cohesion markers in the study of various discursive phenomena. We want to show the relevance of a distribution-based lexical resource to locate interesting relations between lexical items in a text.We call neighbors lexical items that share a significant number of syntactic contexts in a given corpus. In order to evaluate the usefulness of such a resource, we address the task of topical segmentation of text, which generally makes use of some kind of lexical relations. We discuss here the importance of the particular resource used for the task of text segmentation. Using a system inspired by (Hearst, 1997), we show that lexical neighbors provide better results than a classical resource.</abstract>
			<keywords>Lexical cohesion, lexical resources, distributional analysis, text segmentation</keywords>
		</article>
		<article id="recital-2009-long-004" session="Présentations Orales">
			<auteurs>
				<auteur>
					<nom>Gaël Patin</nom>
					<email>gael.patin@inalco.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Er-Tim, Inalco, 75343 Paris</affiliation>
				<affiliation affiliationId="2">Arisem, Thales, 91300 Massy</affiliation>
			</affiliations>
			<titre>Extraction de lexique dans un corpus spécialisé en chinois contemporain</titre>
			<type>long</type>
			<pages></pages>
			<resume>La constitution de ressources lexicales est une tâche cruciale pour l’amélioration des performances des systèmes de recherche d’information. Cet article présente une méthode d’extraction d’unités lexicales en chinois contemporain dans un corpus spécialisé non-annoté et non-segmenté. Cette méthode se base sur une construction incrémentale de l’unité lexicale orientée par une mesure d’association. Elle se distingue des travaux précédents par une approche linguistique non-supervisée assistée par les statistiques. Les résultats de l’extraction, évalués sur un échantillon aléatoire du corpus de travail, sont honorables avec des scores de précision et de rappel respectivement de 52,6 % et 53,7 %.</resume>
			<mots_cles>corpus spécialisé, unité lexicale, lexie, extraction de lexique, chinois</mots_cles>
			<title></title>
			<abstract>Building lexical resources is a vital task in improving the efficiency of information retrieval systems. This article introduces a Chinese lexical unit extraction method for untagged specialized corpora. This method is based on an incremental process driven by an association score. This work features an unsupervised statistically aided linguistic approach. The extraction results — evaluated on a random sample of the working corpus — show decent precision and recall which amount respectively to 52.6% and 53.7%.</abstract>
			<keywords>specialized corpus, lexical unit, lexicon extraction, Chinese</keywords>
		</article>
		<article id="recital-2009-long-005" session="Présentations Orales">
			<auteurs>
				<auteur>
					<nom>Claire Mouton</nom>
					<email>Claire.Mouton@cea.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">CEA/LIST/LIC2M - BP 6 92265 Fontenay-aux-Roses Cedex</affiliation>
				<affiliation affiliationId="2">Exalead S.A. - 10 place de la Madeleine - 75008 Paris</affiliation>
			</affiliations>
			<titre>Induction de sens de mots à partir de multiples espaces sémantiques</titre>
			<type>long</type>
			<pages></pages>
			<resume>Les mots sont souvent porteurs de plusieurs sens. Pour traiter l’information correctement, un ordinateur doit être capable de décider quel sens d’un mot est employé à chacune de ses occurrences. Ce problème non parfaitement résolu a généré beaucoup de travaux sur la désambiguïsation du sens des mots (Word Sense Disambiguation) et dans la génération d’espaces sémantiques dont un des buts est de distinguer ces différents sens. Nous nous inspirons ici de deux méthodes existantes de détection automatique des différents usages et/ou sens des mots, pour les appliquer à des espaces sémantiques issus d’une analyse syntaxique effectuée sur un très grand nombre de pages web. Les adaptations et résultats présentés dans cet article se distinguent par le fait d’utiliser non plus une seule représentation mais une combinaison de multiples espaces de forte dimensionnalité. Ces multiples représentations étant en compétition entre elles, elles participent chacune par vote à l’induction des sens lors de la phase de clustering.</resume>
			<mots_cles>espace sémantique, réduction de dimensions, Locality Sensitive Hashing, induction de sens, clustering de mots, objets multi-représentés</mots_cles>
			<title></title>
			<abstract>Words can have many senses. In order to process information correctly, a computer should be able to decide which sense of a word is used in a given context. This unsolved problem has generated much research in word sense disambiguation and in the generation of semantic spaces in order to separate possible meanings. Here, we adapt two existing methods to automatically distinguish words uses and senses.We apply them to multiple semantic spaces produced by a syntactic analysis of a very large number of web pages. These adaptations and the results presented in this article differ from the original methods in that they use a combination of several high dimensional spaces instead of one single representation. Each of these competing semantic spaces takes part in a clustering phase in which they vote on sense induction.</abstract>
			<keywords>semantic space, dimensionality reduction, Locality Sensitive Hashing, Word Sense Induction, words clustering, multi-represented data</keywords>
		</article>
		<article id="recital-2009-long-006" session="Présentations Orales">
			<auteurs>
				<auteur>
					<nom>Marion Potet</nom>
					<email>Marion.Potet@imag.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Laboratoire d’informatique de Grenoble, équipe GETALP, UJF - BP 53, 38041 Grenoble Cedex 9</affiliation>
			</affiliations>
			<titre>Méta-moteur de traduction automatique : proposition d’une métrique pour le classement de traductions</titre>
			<type>long</type>
			<pages></pages>
			<resume>Compte tenu de l’essor du Web et du développement des documents multilingues, le besoin de traductions "à la volée" est devenu une évidence. Cet article présente un système qui propose, pour une phrase donnée, non pas une unique traduction, mais une liste de N hypothèses de traductions en faisant appel à plusieurs moteurs de traduction pré-existants. Neufs moteurs de traduction automatique gratuits et disponibles sur leWeb ont été sélectionnés pour soumettre un texte à traduire et réceptionner sa traduction. Les traductions obtenues sont classées selon une métrique reposant sur l’utilisation d’un modèle de langage. Les expériences conduites ont montré que ce méta-moteur de traduction se révèle plus pertinent que l’utilisation d’un seul système de traduction.</resume>
			<mots_cles>traduction automatique, web, modèle de langage, méta-moteur de traduction</mots_cles>
			<title></title>
			<abstract>Considering the Web and multilingual documents development expansion, the need of fast translation has become an evidence. This paper presents a system that proposes, for a given sentence, a list of N translation hypotheses instead of a single translation, using several machine translation systems already existing. Nine free and available (on the Internet) automatic translation engines have been chosen to submit a text to be translated and to receive its translation. The translations obtained are evaluated individually with a language model adapted and a metric elaborated by us, and in this way classified by relevance order. The experiment have pointed out that this meta-translation engine is more useful than the use of one system for translation.</abstract>
			<keywords>automatic translation, web, language model, meta-translator</keywords>
		</article>
		<article id="recital-2009-long-007" session="Présentations Orales">
			<auteurs>
				<auteur>
					<nom>Thomas François</nom>
					<email>thomas.francois@uclouvain.be</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Centre de Traitement Automatique du Langage, Université Catholique de Louvain</affiliation>
			</affiliations>
			<titre>Modèles statistiques pour l’estimation automatique de la difficulté de textes de FLE</titre>
			<type>long</type>
			<pages></pages>
			<resume>La lecture constitue l’une des tâches essentielles dans l’apprentissage d’une langue étrangère. Toutefois, la découverte d’un texte portant sur un sujet précis et qui soit adapté au niveau de chaque apprenant est consommatrice de temps et pourrait être automatisée. Des expériences montrent que, pour l’anglais, l’utilisation de classifieurs statistiques permet d’estimer automatiquement la difficulté d’un texte. Dans cet article, nous proposons une méthodologie originale comparant, pour le français langue étrangère (FLE), diverses techniques de classification (la régression logistique, le bagging et le boosting) sur deux corpus d’entraînement. Il ressort de cette analyse comparative une légère supériorité de la régression logistique multinomiale.</resume>
			<mots_cles>lisibilité, régression logistique, bagging, boosting, modèle de langue</mots_cles>
			<title></title>
			<abstract>Reading is known to be an essential task in language learning, but finding the appropriate text for every learner is far from easy. In this context, automatic procedures can support the teacher’s work. Some works on English reveal that it is possible to assess the readability of texts using statistical classifiers. In this paper, we present an original approach comparing various classification techniques, namely logistic regression, bagging and boosting on two training corpora. The results show a slight superiority for multinomial logistic regression over bagging or boosting.</abstract>
			<keywords>readability, logistic regression, bagging, boosting, language model</keywords>
		</article>
		<article id="recital-2009-long-008" session="Présentations Orales">
			<auteurs>
				<auteur>
					<nom>Florent Pompigne</nom>
					<email>florent.pompigne@loria.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">ENS Cachan / INRIA Nancy - Grand-Est</affiliation>
			</affiliations>
			<titre>Modélisation des mouvements explicites dans les ACG avec le produit dépendant</titre>
			<type>long</type>
			<pages></pages>
			<resume></resume>
			<mots_cles>syntaxe, grammaires catégorielles abstraites, types dépendant, mouvements explicites, extraction</mots_cles>
			<title></title>
			<abstract>Abstract Categorial Grammars (ACG) is a grammatical framework based on linear lambda-calculus. As in Muskens’ Lambda Grammars, an abstract term in this kind of categorial grammar can be realized in different directions, such as syntactic and semantic ones. This structure provides autonomy for these different processings. ACG’s architecture is independent from the logic used and so the type system is easily extensible in order to deal better with some linguistic phenomena. We will first introduce ACGs and the dependent product construction. This paper will then be concerned with the issue of overt grammatical movements, in particular extraction constraints in relative propositions, and how several close frameworks deal with it. Last we will show how to capture this phenomenon in extended ACG.</abstract>
			<keywords>Syntax, abstract categorial grammars, dependant product, overt movements, extraction</keywords>
		</article>
		<article id="recital-2009-long-009" session="Présentations Orales">
			<auteurs>
				<auteur>
					<nom>Vanessa Andréani</nom>
					<email>va@tkm.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">TecKnowMetrix – 4, rue Léon Béridot – ZAC Champfeuillet – 38500 Voiron – France</affiliation>
				<affiliation affiliationId="2">LIDILEM – Université Stendhal Grenoble 3 – Domaine universitaire – 1180, avenue centrale – 38400 Saint Martin d’Hères – France</affiliation>
			</affiliations>
			<titre>Normalisation des entités nommées : pour une approche mixte et orientée utilisateurs</titre>
			<type>long</type>
			<pages></pages>
			<resume>La normalisation intervient dans de nombreux champs du traitement de l'information. Elle permet d'optimiser les performances des applications, telles que la recherche ou l'extraction d'information, et de rendre plus fiable la constitution de ressources langagières. La normalisation consiste à ramener toutes les variantes d'un même terme ou d'une entité nommée à une forme standard, et permet de limiter l'impact de la variation linguistique. Notre travail porte sur la normalisation des entités nommées, pour laquelle nous avons mis en place un système complexe mêlant plusieurs approches. Nous en présentons ici une des composantes : une méthode endogène de délimitation et de validation de l’entité nommée normée, adaptée à des données multilingues. De plus, nous plaçons l'utilisateur au centre du processus de normalisation, dans l'objectif d'obtenir des données parfaitement fiables et adaptées à ses besoins.</resume>
			<mots_cles>normalisation, entités nommées, traitement de l'information, analyse de corpus, méthodes endogènes, système complexe</mots_cles>
			<title></title>
			<abstract>Normalization is involved in many fields of information processing. It improves performances for several applications, such as information retrieval or information extraction, and makes linguistic resources constitution more reliable. Normalization consists in standardizing each variant of a term or named entity into a unique form, and this way restricts the impact of term variation. Our work applies to named entity normalization, for which we implemented a complex system that mixes several approaches. We present here one of its components: an endogenous method to mark out and validate the normalized named entities. Moreover, we place the user in the center of our normalization process, in order to obtain fully reliable data that fit his needs.</abstract>
			<keywords>normalization, named entities, information processing, corpus analysis, endogenous methods, complex system</keywords>
		</article>
		<article id="recital-2009-long-010" session="Poster">
			<auteurs>
				<auteur>
					<nom>Eric Charton</nom>
					<email>eric.charton@univ-avignon.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIA / Université d’Avignon, 339 chemin des Meinajariès, 84911 Avignon</affiliation>
			</affiliations>
			<titre>Combinaison de contenus encyclopédiques multilingues pour une reconnaissance d’entités nommées en contexte</titre>
			<type>long</type>
			<pages></pages>
			<resume>Dans cet article, nous présentons une méthode de transformation de Wikipédia en ressource d’information externe pour détecter et désambiguïser des entités nommées, en milieu ouvert et sans apprentissage spécifique. Nous expliquons comment nous construisons notre système, puis nous utilisons cinq éditions linguistiques de Wikipédia afin d’enrichir son lexique. Pour finir nous réalisons une évaluation et comparons les performances du système avec et sans compléments lexicaux issus des informations inter-linguistiques, sur une tâche d’extraction d’entités nommées appliquée à un corpus d’articles journalistiques.</resume>
			<mots_cles>Etiquetage d’entités nommées, ressources sémantiques</mots_cles>
			<title></title>
			<abstract>In this paper, we present a way to use of Wikipedia as an external resource to disambiguate and detect named entities, without learning step. We explain how we build our system and why we used five linguistic editions of the Wikipedia corpus to increase the volume of potentially matching candidates. We finally experiment our system on a news corpus.</abstract>
			<keywords>Named entity labeling, semantic resources</keywords>
		</article>
		<article id="recital-2009-long-011" session="Poster">
			<auteurs>
				<auteur>
					<nom>Rami Ayadi</nom>
					<email>ayadi.rami@planet.tn</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Walid Jaoudi</nom>
					<email>walidjaouadi@yahoo.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">UTIC (monastir) – ISIMS (sfax) ,Tunisie</affiliation>
				<affiliation affiliationId="2">UTIC (Tunis) , Tunisie</affiliation>
			</affiliations>
			<titre>La distance intertextuelle pour la classification de textes en langue arabe</titre>
			<type>long</type>
			<pages></pages>
			<resume>Nos travaux de recherche s’intéressent à l’application de la théorie de la distance intertextuelle sur la langue arabe en tant qu’outil pour la classification de textes. Cette théorie traite de la classification de textes selon des critères de statistique lexicale, se basant sur la notion de connexion lexicale. Notre objectif est d’intégrer cette théorie en tant qu’outil de classification de textes en langue arabe. Ceci nécessite l’intégration d’une métrique pour la classification de textes au niveau d’une base de corpus lemmatisés étiquetés et identifiés comme étant des références d’époques, de genre, de thèmes littéraires et d’auteurs et ceci afin de permettre la classification de textes anonymes.</resume>
			<mots_cles>Distance intertextuelle, arabe, classification, lemmatisation, corpus, statistique lexicale</mots_cles>
			<title></title>
			<abstract>Our researche works are interested in the application of the intertextual distance theory on the Arabic language as a tool for the classification of texts. This theory handles the classification of texts according to criteria of lexical statistics, and it is based on the lexical connection approach. Our objective is to integrate this theory as a tool of classification of texts in Arabic language. It requires the integration of a metrics for the classification of texts using a database of lemmatized and identified corpus which can be considered as a literature reference for times, genres, literary themes and authors and this in order to permit the classification of anonymous texts.</abstract>
			<keywords>Intertextual distance, Arabic, classification, lemmatization, corpus, lexical statistics</keywords>
		</article>
		<article id="recital-2009-long-012" session="Poster">
			<auteurs>
				<auteur>
					<nom>Sara Boutouhami</nom>
					<email>boutouhami@lipn.univ-paris13.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIPN – Université Paris-Nord, 99 Avenue J.B.Clément 93430 Villetaneuse</affiliation>
			</affiliations>
			<titre>Techniques argumentatives pour aider à générer des descriptions orientées d’un événement</titre>
			<type>long</type>
			<pages></pages>
			<resume>Les moyens et les formes stratégiques permettant la génération de descriptions textuelles argumentées d’une même réalité effective sont nombreux. La plupart des définitions proposées de l’argumentation partagent l’idée qu’argumenter c’est fournir les éléments en faveur d’une conclusion donnée. Or dans notre tâche qui consiste à générer des descriptions argumentées pour des accidents de la route, nous ne disposons pas uniquement d’éléments en faveur de la conclusion souhaitée mais aussi d’éléments qui vont à l’encontre de cette dernière et dont la présence est parfois obligatoire pour la compréhension de ces descriptions. Afin de remédier à ce problème, nous proposons des techniques de génération de descriptions argumentées qui présentent au mieux les éléments indésirables à l’aide de stratégies argumentatives.</resume>
			<mots_cles>Argumentation, Insinuation, Norme coutumières, Justification</mots_cles>
			<title></title>
			<abstract>Strategic means and forms for the generation of textual descriptions of a same reality are numerous. Most definitions of the argumentation given in the literature share the idea that to argument we must provide elements in favor of a given conclusion. Unfortunately, in our task consisting in generating biased descriptions of a road crash, we do not have only elements in favor of the desired conclusion, but also elements that are against it and must nevertheless be present in the description unless we cannot be able to understand how the accident happened. To remedy this problem, we proposed a module in our system for generating biased descriptions that handles the task of presenting better the undesirable elements using argumentative techniques.</abstract>
			<keywords>Argumentation, Insinuation, Customary norms, Justification</keywords>
		</article>
	</articles>
</conference>