RECITAL 2009, Senlis, 24-26 juin 2009

Apprentissage automatique et Co-training

Pierre Gotab
LIA / Université d’Avignon, 339 chemin des Meinaj aries, 84911 Avignon
pierre. gotab @ univ-avignon.fr

Résumé. Dans le domaine de la classiﬁcation supervisée et semi-supervisée, cet article
présente un contexte favorable a l’application de méthodes statistiques de classiﬁcation. I1
montre l’application d’une stratégie alternative dans le cas ou les données d’apprentissage sont
insufﬁsantes, mais ou de nombreuses données non étiquetées sont a notre disposition : le co-
training multi-classiﬁeurs. Les deux vues indépendantes habituelles du co-training sont rempla-
cées par deux classiﬁeurs basés sur des techniques de classiﬁcation différentes : icsiboost sur le
boosting et LIBLINEAR sur de la régression logistique.

Abstract. In the domain of supervised and semi-supervised classiﬁcation, this paper des-
cribes an experimental context suitable with statistical classiﬁcation. It shows an alternative
method usable when learning data is unsufﬁcient but when many unlabeled data is avaliable :
the multi-classiﬁer co-training. Two classiﬁers based on different classiﬁcation methods replace
the two independent views of the original co-training algorithm : icsiboost based on boosting
and LIBLINEAR which is a logistic regression classiﬁer.

M0tS-CléS I Apprentissage automatique, classiﬁcation, co-training.

Keywords: Machine learning, classiﬁcation, co-training.

1 Classiﬁcation et apprentissage supervisé

Les problemes de classiﬁcation présentés ici sont relatifs a l’apprentissage automatique super-
visé. I1 s’agit de construire un modele représentatif d’un certain nombre de données organisées
en classes - ensemble que l’on appelle généralement le corpus d ’apprentissage - puis d’utiliser
ce modele aﬁn de classer de nouvelles données, c’est a dire de prédire leur classe au vu de
leurs caractéristiques (appelées parametres ou features). La construction du modele releve de
l’apprentissage automatique supervisé, l’ensemble des exemples constituant le corpus d’appren-
tissage étant annotés, c’est a dire qu’ils portent le label de leur classe donné a priori.

Le processus permettant d’obtenir ce corpus d’apprentissage est généralement d’utiliser des
annotateurs manuels, qui vont observer les exemples et, selon leur appréciation, leur attribuer
tel label ou tel autre.

Un des problemes majeurs inhérent a la classiﬁcation supervisée en Traitement Automatique de
la Langue Naturelle (TALN) est qu’obtenir un corpus d’apprentissage annoté manuellement est
assez difﬁcile, cela coute cher et n’est pas rapide. Ces corpus sont donc souvent disponibles en
quantité limitée. Or la qualité des modeles des classiﬁeurs dépend directement de la taille de
ces corpus.

Pierre Gotab

2 Application a la campagne DEFT

2.1 Presentation

DEFT (DEﬁ Fouille de Textes) est une campagne d’evaluation ayant lieu chaque annee depuis
20051. Elle porte sur la fouille de textes en langue frangaise.

Pour l’annee 2008, elle traitait de la classiﬁcation automatique de textes, et plus particu1ie-
rement de la detection du genre et du theme d’articles provenant du journal Le Monde et de
1’ encyclopedie Wikipedia.

L’interet de ce corpus est qu’il rend compte d’un probleme de classiﬁcation reel, dont les don-
nees ont ete annotees par des humains. En effet, les classes ont ete deﬁnies par Le Monde et
Wikipedia et choisies par les auteurs des articles. I1 se préte done a des experiences a la frontiere
entre apprentissage theorique et linguistique appliquee.

L’inconvenient induit est qu’il existe sans doute des erreurs d’annotation, ou, plus souvent,
de l’ambigui'te et des recouvrements entre les classes. Des articles peuvent traiter de plusieurs
themes mais ne sont ranges que dans un seul (un article de litterature scientiﬁque sera range
dans Litterature ou dans Sciences ?), et des themes peuvent etre fortement imbriques (Sport et
television par exemple). On suppose tout de meme qu’il existe un lien fort entre le contenu
d’un article et son theme principal, et c’est ce qui permettra de constr11ire des modeles pour les
classiﬁer.

Le corpus fourni pour l’apprentissage est assez important, de l’ordre d’une dizaine de milliers
d’eXemples, c’est donc un contexte tres favorable a la classiﬁcation automatique supervisee a
l’aide de methodes statistiques.

2.2 Données

Le corpus est divise en deux taches.

La premiere tache est un ensemble de 15223 articles en texte brut. Chaque article est associe a
une classe parmi : art (ART), economie (ECO), sport (SPO), television (TEL) reparties ainsi :

ART ECO SPO TEL
30.41% 8.88% 37.88% 22.82%

La partition detest, quanta elle, contient 10596 articles.

La deuxieme tache est un ensemble de 23550 articles associes aux classes : france (FRA),
international (INT), litterature (LIV), sciences (SCI), societe (SOC) reparties ainsi :

FRA INT LIV SCI SOC
14.12% 22.52% 19.43% 27.87% 16.04%

La partition de test contient 15693 articles.

lhttp ://deft.1imsi.fr/

Apprentissage automatique et Co—training

2.3 Protocole experimental

Deux classiﬁeurs sont utilises pour realiser cette tache de classiﬁcation.

Le premier est icsiboost 2 , une version open—source du classiﬁeur BoosTexter, base sur Ada-
Boost (Freund & Schapire, 1996), un algorithme de boosting. Icsiboost a l’avantage d’etre fa-
cile a mettre en oeuvre, et possede des facilites telles que la generation de N—gram pour le
texte. BoosTexter a ete speciﬁquement concu pour la classiﬁcation de textes (Schapire & Sin-
ger, 2000), icsiboost convient donc parfaitement a cette tache.

L’ algorithme AdaBoost consiste a construire une multitude d’apprenantsfaibles, qui combines
formeront l’apprencmtf0rt, qui se resume a un vote pondere des apprenants faibles. Il fonctionne
de maniere iterative. A chaque iteration, l’apprenant faible qui minimise le nombre d’erreurs de
classiﬁcation est choisi. Les exemples qu’il aura mal classes auront un poids plus fort lors de
l’iteration suivante, aﬁn que le prochain apprenant faible se concentre sur ces exemples difﬁciles
a classer. Et ainsi de suite.

Voici un exemple naif illustre, avec deux classes (positive et negative) et deux parametres (abs-
cisse et ordonnee). Les apprenants faibles sont de simples fonctions lineaires qui separent l’es—
pace en deux :

Dans notre experience sur DEFT, icsiboost est utilise en apprentissage sur le sac de mots (c’est
a dire l’ensemble des mots d’un exemple, independemment de leur nombre d’occurences), le
nombre d’iterations de boosting est ﬁxe a 1000.

zicsiboost, an opensource implementation of BoosTexter - http 2//code.google.corn/p/icsiboost

Pierre Gotab

Le second est LIBLINEAR (Lin et al. , 2007), semblable a libsvm, mais basé sur de la régression
logistique et des SVM a noyau linéaire (Vapnik, 2000). Son utilisation est plus austere mais il
est tres rapide. Il est particulierement indiqué dans la classiﬁcation de documents comme le
montre l’appendice B.2 du guide de LIBSVM (Hsu et al., 2003).

Le principe est assez proche de la régression linéaire a ceci pres qu’on utilise une fonction de la
forme p(X) = 1:':% ou oz et B sont des constantes déterminées par la méthode du maximum
de vraissemblance aﬁn de minimiser l’erreur de classiﬁcation. L’ avantage est, au contraire de
la régression linéaire, de pouvoir obtenir une probabilité (comprise dans [0..1]) associée a un
exemple donné.

Lorsque l’on a plus d’un parametre, B et X deviennent des vecteurs dont chaque dimension est
associée a un parametre.

Dans notre cas, le vecteur de liblinear pour un exemple donné est un vecteur a composantes
binaires dont chaque dimension représente un mot du lexique, et sa valeur est 1 si le mot est
dans le sac de mots, 0 sinon.

2.4 Résultats obtenus

Sur la premiere tache :

Classiﬁeur icsiboost LIBLINEAR
Classe Précision Rappel F-mesure Précision Rappel F-mesure

ART 81.7% 90.4% 85.8% 84.7% 90.2% 87.4%
ECO 84.3% 91.7% 87.9% 82.6% 96.2% 88.9%
SPO 95.2% 89.9% 92.5% 96.8% 91.6% 94.1%
TEL 83.5% 49.3% 62.0% 90.3% 48.0% 62.7%

Toutes classes confondues 85.4% 86.9%

Sur la deuxieme tache :

Classiﬁeur icsiboost LIBLINEAR
Classe Précision Rappel F-mesure Précision Rappel F-mesure

FRA 79.9% 76.1% 78.0% 84.1% 78.9% 81.4%
INT 89.2% 89.9% 89.6% 90.9% 93.2% 92.0%
LIV 92.3% 89.7% 91.0% 92.5% 93.4% 92.9%
SCI 84.6% 89.5% 87.0% 89.1% 89.8% 89.5%
SOC 67.2% 64.9% 66.1% 72.0% 71.6% 71.8%
Toutes classes confondues 83.8% 86.8%

Lors de la campagne DEFT’08 les deux meilleurs systemes ont obtenu un f-score de 87.8% sur
la premiere tache (Trinh et al., 2008) et de 87.9% sur la deuxieme (Charton et al., 2008).

En comparaison, ces résultats montrent que l’on obtient facilement des scores tres satisfaisants
avec des méthodes statistiques, et sans prétraitements, des lors que l’on a a disposition une
quantité sufﬁsante de données d’apprentissage.

Apprentissage automatique et Co-training

Il est a noter que le classiﬁeur icsiboost a été utilisé dans la fusion avec prétraitements de
l’équipe jeunes chercheurs du Laboratoire Informatique d’Avignon qui a remporté la premiere
place sur la deuxieme tache (Charton et al., 2008).

Mais lorsque nous n’avons pas acces a une quantité sufﬁsante de données d’apprentissage,
quelles sont les alternatives ?

La faible quantité de corpus d’apprentissage peut-étre contrebalancée en en augmentant la qua-
lité, c’est le but de l’active-learning (Riccardi & Hakkani-Tur, 2005). Il consiste a sélectionner
préalablement les exemples a annoter manuellement aﬁn d’avoir un corpus peu redondant qui
couvre un maximum de cas de ﬁgure en éliminant les exemples trop similaires.

Deux autres méthodes sont envisageables dans le cas o1‘1 l’on a acces a de grandes quantités de

données non annotées :

— L’ online-leaming permet a un systeme de s’améliorer alors qu’il est en production, c’est un
domaine de recherche récent et assez prospectif.

— Et le co-training (Blum & Mitchell, 1998), qui permet d’augmenter artiﬁciellement le corpus
d’apprentissage, en lui adj oignant les données non annotées, apres leur avoir attribué un label.
Il se rapproche du self-training a ceci pres qu’il met en jeu plusieurs classiﬁeurs. C’est cette
technique qui sera utilisée dans la suite de cet article.

3 Co-training

3.1 Présentation

Le co-training consiste a entrainer plusieurs classiﬁeurs, chacun basé sur une vue du corpus,
puis a les améliorer entre eux a l’aide d’une masse importante de données non annotées; les
classiﬁeurs plus a meme de classer un exemple donné jouant le role de "professeurs" pour les
autres.

L’algorithme original présenté par Blum et Mitchell (Blum & Mitchell, 1998) a subi toutes
sortes de modiﬁcations et d’adaptations a travers la littérature parue depuis :

— Utilisation d’un ensemble d’exemples commun aux classiﬁeurs (Sarkar, 2001) ou bien un
ensemble distinct par classiﬁeur (Hwa et al., 2003) (Muller et al., 2001)

— Dans le méme ordre d’idées, séparer le corpus des la premiere itération (Muller et al., 2001)
ou bien laisser les classiﬁeurs apprendre sur le meme corpus de base.

— Utiliser un pool d’exemples non annotés (comme déﬁni dans (Blum & Mitchell, 1998)) géré
de plusieurs facons :
— Aucun pool, le non annoté est traité d’un seul bloc (Guz et al., 2007)
— Un pool de taille ﬁxe réapprovisionné de facon aléatoire (Pierce & Cardie, 2001)

— Obliger les classiﬁeurs a classer un certain nombre d’exemples a chaque itération (Denis
et al., 2002)

— Respecter la distribution des classes a priori en ne retenant que les is * fc exemples les mieux
classés de la classe c de probabilité a priori fc (k est une constante empirique)

Dans notre cas, les deux classiﬁeurs commencent leur apprentissage sur le meme corpus (A),
puis se constituent un corpus personnel (G,-) qui grossira a mesure qu’il sera approvisionné par
l’autre classiﬁeur.

Pierre Gotab

Le pool d’exemples non annotes (U) est separe en autant de partitions (U,-) que l’on souhaite
d’iterations de co-training, et a chaque iteration 1', on soumet la partition U1 aux deux clas-
siﬁeurs. Pour chaque exemple, si un classiﬁeur annonce un score de conﬁance superieur a un
certain seuil, il y apposera son label et ajoutera cet exemple au corpus d’apprentissage de l’autre
classiﬁeur.

Lorsqu’il est difﬁcile de deﬁnir plusieurs vues decorrelees sur un ensemble de donnees, comme
dans le cas de l’experience precedente, il est possible d’utiliser plusieurs classiﬁeurs differents
bases sur l’ensemble des parametres P (ici, le sac de mots). Et c’est la difference de methode
de classiﬁcation qui Va introduire la complementarite necessaire au co-training.

3.2 Algorithme

Soit un ensemble de donnees d’apprentissage A, de donnees de test T et de donnees non etique-
tees U ; et un ensemble de parametres (features) P. Un exemple e E A est un couple e = (11, l)
ou 11 est un vecteur de dimension |P| representant les differentes valeurs de chaque parametre,
et l est le label (la classe) de cet exemple.

On decoupe U en N partitions de taille egale notees U1, U2, ...UN.

On entraine deux classiﬁeurs G1 et G2 sur A. Un exemple e = (11, l) classe par un classiﬁeur G1-
donne G1-(e) = (l’ , 3) ou l’ est la classe attribuee par G1 avec un score de conﬁance s E [0..1].

On deﬁnit un seuil seuili E [0..1] de conﬁance minimale pour chaque classiﬁeur G,-.

On constitue deux ensembles G1 = G2 = A.

0 Pour I1 allant de 1 a N
o Chaque exemple e = (11, l) 6 Uk est classe par G1 et G2 :
G1(e) = (l1, 31) et G2(e) = (l2, 32)
0 Si 31 > se1l1'l1, G2 := G2 U {(11, l1)}
0 Si 32 > se1l1'l2, G1 := G1 U {(11, l2)}
0 On entraine G1 sur G,-
o On teste G1 sur T

4 Co-training multi-classiﬁeurs appliqué a DEFT

4.1 Protocole experimental
Pour simuler une penurie de donnees annotees nous allons circonscrire les donnees d’appren-
tissage a seulement quelques milliers d’exemples.

Nous retirons les labels du reste des donnees d’apprentissage, qui sera alors considere comme
la partition non annotee (U).

Nous utilisons toujours icsiboost et LIBLINEAR comme classiﬁeurs.

Apprentissage automatique et Co-training

L’ apprentissage et l’évaluation sont réalisés avec la conﬁguration suivante :
— A contient quelques exemples, U contient le reste de l’apprentissage et T est la partition de
test originale

— 10 étapes de co-training (N = 10)

— 1000 itérations d’icsiboost pour l’apprentissage des modeles

— Le seuil de conﬁance d’icsiboost est ﬁxé a 0.78 et celui de LIBLINEAR a 0.98

Les seuils de conﬁance sont déterminés aﬁn d’avoir 99% des exemples annotés automatique-
ment avec la bonne étiquette. Ils sont calculés a partir de la partition de test.

Les deux classiﬁeurs servent de témoins dans l’évaluation, en comparant leurs performances
avant et apres le co-training.

4.2 Résultats obtenus

Voici deux résultats en détails, avec entre 85 et 90% du corpus d’apprentissage considéré comme
non annoté :

Résultats avant, avec 2000 articles en apprentissage, et aprés co-training en tirant partie des
13223 autres articles de la premiere tache :

Classiﬁeur icsiboost LIBLINEAR
Classe avant aprés avant aprés
ART 81.4% 82.6% 81.2% 82.3%
ECO 83.5% 84.7% 84.7% 85.9%
SPO 90.2% 90.9% 88.6% 90.4%
TEL 50.9% 53.2% 40.3% 39.3%
Toutes 81.1% 82.2% 80.5% 81.7%

On a une progression de la f-mesure de icsiboost de 1.1 points, et 1.2 points pour LIBLINEAR.

De meme sur la deuxieme tache, 3000 articles en apprentissage et 20550 articles considérés
comme non annotés :

Classiﬁeur icsiboost LIBLINEAR

Classe avant aprés avant aprés
FRA 73.7% 75.7% 76.9% 77.3%
INT 85.6% 85.5% 88.0% 88.3%
LIV 88.4% 88.3% 89.1% 89.4%
SCI 82.6% 84.1% 84.9% 85.4%
SOC 58.1% 59.6% 61.6% 61.8%
Toutes 79.4% 80.2% 81.9% 82.4%

On note un gain pour quasiment toutes les classes (a une exception pres), la f-mesure d’icsiboost
progresse de 0.8 points, et celle de LIBLINEAR de 0.5 points.

Pierre Gotab

Voici les résultats des autres expériences présentant 1’évo1ution du f-score (en ordonnée) en
fonction du nombre d’exemp1es d’apprentissage (en abscisse) :

Sur la premiere tache :

E/olution du f-soore de la téche 1 en fonction de | A|

— — icsiboost avant icsiboost aprés — — Iiblinear avant j Iiblinear aprés
f-score
0,85 , ,4
J J J a
0,83 I ’ -";’/'/’

0,81 X‘ 4”"/‘ﬂ’
/ //
0,79 /r__________. —- //
0,77 / ’

/ , / /
0,75 / ,- ’ ’
// /
0,73 4/
/
0,71 . . . . . . .|A|
250 500 750 1000 2000 3000 4000 8000

Avec 2000 articles en apprentissage par exemple, méme si on reste loin des performances obte-
nues en ayant des annotations parfaites (en prenant1’intégra1ité du corpus annoté comme dans la
premiere expérience), grace au co-training icsiboost atteint 1e score qu’on aurait eu en entrainant
les modeles sur 4000 articles, soit 1e double d’annotations manuelles.

Sur la deuxieme tache :

E/olution du f-score de la téche 2 en fonction de| A|

— — icsiboost avant
f-score
0,85

icsiboost aprés — — Iiblinear avant j Iiblinear aprés

0,8

0,75

0,7

0,65

 

0,6 I I I I I I I 
250 500 750 1000 2000 3000 4000 8000

La encore on remarque que le co-training permet d’atteindre un f-score donné en minimisant
1’ effort d’ annotation manuelle.

Apprentissage automatique et Co-training

Voici la progression du f-score obtenue grace au co-training en fonction du nombre d’exemples
d’apprentissage (c’est la différence entre les courbes précédentes) :

 

Marge de progression du f-score via Ie oo-training en fonction de | A|
Téche 1 icsiboost Téche 1 Iiblinear — —Téche 2 icsiboost — —Téche 2 Iiblinear
Af—score

0,08

0,07

0,06

0,05

0,04

0,03 ‘ \

0,02 / \ \ : \ I ’ \

§ § §
0,01 \ _ _ \ \ ‘
0 ‘ ‘ ‘
-0,01 |A|
250 500 750 1000 2000 3000 4000 8000

De maniere générale on constate que la marge de progression du co-training est inversement
proportionnelle a la qualité des modeles de départ. Le co-training semble donc particulierement
adapté lorsque le corpus d’apprentissage est insufﬁsant.

Avec 8000 exemples en apprentissage, on approche le f-score qu’on aurait avec l’ensemble
du corpus d’apprentissage. Le co-training semble alors montrer ses limites, avec de faibles
progressions des scores, voire meme une diminution de la f-mesure d’icsiboost sur la premiere
tache.

5 Conclusion

Les deux expériences présentées et leurs résultats encourageants montrent que, des lors que nous
avons acces a une quantité sufﬁsante de données non annotées, le co-training peut améliorer les
performances de classiﬁcation, et ce a moindre coﬁt.

Le "bond" que l’on observe sur la premiere tache avec 500 exemples montre que la sélection
des exemples servant de corpus de départ est primordiale. Allier active-learning et co-training
devrait donc donner de meilleurs résultats.

Les corpus a disposition étaient totalement annotés, ce qui a permis de surveiller la qualité de
l’annotation automatique de la partition non annotée (notée précédemment U). Les résultats
ameneraient a penser que le respect de la distribution a priori des classes est important. Le fait
que les courbes issues du co-training épousent vaguement celles de base semble montrer un
manque de latitude laissé aux classiﬁeurs pour choisir les exemples a annoter. I1 serait donc
intéressant de se pencher sur l’algorithme de co-training en lui meme, en confrontant plusieurs
facons de gérer le pool d’exemples non annotés, et de sélectionner les exemples a intégrer aux
modeles.

Pierre Gotab

Références

BLUM A. & MITCHELL T. (1998). Combining labeled and unlabeled data with co-training.
Proceedings of the eleventh annual conference on Computational learning theory, p. 92-100.

CHARTON E., CAMELIN N., ACUNA-AGOST R., GOTAB P., LAVALLEY R., KESSLER R. &
FERNANDEZ S. (2008). Pré-traitements classiques ou par analyse distributionnelle : applica-
tion aux méthodes de classiﬁcation automatique déployées pour DEFT08. DEF T’08.

DENIS F., GILLERON R. & TOMMASI M. (2002). Classiﬁcation de textes et co-training a par-
tir de textes positifs et non étiquetés. Actes de la Confe’rence Francophone sur l ’Apprentissage
(CAP 2002), p. 205-220.

FREUND Y. & SCHAPIRE R. E. (1996). Experiments with a new boosting algorithm. In In
Proceedings of the Thirteenth International Conference on Machine Learning, p. 148-156 :
Morgan Kaufmann.

GUZ U., CUENDET S., HAKKANI-TUR D. & TUR G. (2007). Co-training Using Prosodic
and Lexical Information for Sentence Segmentation. Interspeech, p. 2597-2600.

HSU C. W., CHANG C. C. & LIN C. J. (2003). A practical guide to support vector classiﬁ-
cation. Rapport interne, Taipei.
HWA R., OSBORNE M., SARKAR A. & STEEDMAN M. (2003). Corrected co-training for

statistical parsers. ICML-03 Workshop on the Continuum from Labeled to Unlabeled Data in
Machine Learning and Data Mining, Washington DC.

LIN C., WENG R. & KEERTHI S. (2007). Trust region Newton methods for large-scale

logistic regression. Proceedings of the 24th international conference on Machine learning, p.
561-568.

MULLER C., RAPP S. & STRUBE M. (2001). Applying Co-Training to reference resolution.

Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, p.
352-359.

PIERCE D. & CARDIE C. (2001). Limitations of co-training for natural language learning
from large datasets. Proceedings of the 2001 Conference on Empirical Methods in Natural
Language Processing (EMNLP-2001), p. 1-9.

RICCARDI G. & HAKKANI-TUR D. (2005). Active learning : theory and applications to

automatic speech recognition. IEEE Transactions on Speech and Audio Processing, 13(4),
504-51 1.

SARKAR A. (2001). Applying co-training methods to statistical parsing. North American
Chapter Of The Association For Computational Linguistics, p. 1-8.

SCHAPIRE R. & SINGER Y. (2000). BoosTexter : A Boosting-based System for Text Catego-
rization. Machine Learning, 39(2), 135-168.

T RINH A., BUFFONI D. & GALLINARI P. (2008). Classiﬁeur probabiliste avec Support Vec-
tor Machine (SVM) et Okapi. DEF T’08.

VAPNIK V. (2000). The Nature of Statistical Learning Theory. Springer.

