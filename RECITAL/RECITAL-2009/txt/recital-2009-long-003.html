<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
"http://www.w3.org/TR/html4/loose.dtd">
<html><head><title>D&#233;tection de la coh&#233;sion lexicale par voisinage distributionnel : application &#224; la segmentation th&#233;matique</title>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
</head>
<body>
<div style="page-break-before:always; page-break-after:always"><div><p>RECITAL 2009, Senlis, 24&#8211;26 juin 2009
</p>
<p>D&#233;tection de la coh&#233;sion lexicale par voisinage
distributionnel : application &#224; la segmentation th&#233;matique
</p>
<p>Cl&#233;mentine Adam Fran&#231;ois Morlane-Hond&#232;re
CLLE / Universit&#233; de Toulouse &amp; CNRS
</p>
<p>adam@univ-tlse2.fr, morlanehondere@gmail.com
</p>
<p>R&#233;sum&#233;. Cette &#233;tude s&#8217;ins&#232;re dans le projet VOILADIS (VOIsinage Lexical pour l&#8217;Ana-
lyse du DIScours), qui a pour objectif d&#8217;exploiter des marques de coh&#233;sion lexicale pour mettre
au jour des ph&#233;nom&#232;nes discursifs. Notre propos est de montrer la pertinence d&#8217;une ressource,
construite par l&#8217;analyse distributionnelle automatique d&#8217;un corpus, pour rep&#233;rer les liens lexi-
caux dans les textes. Nous d&#233;signons par voisins les mots rapproch&#233;s par l&#8217;analyse distribution-
nelle sur la base des contextes syntaxiques qu&#8217;ils partagent au sein du corpus. Pour &#233;valuer la
pertinence de la ressource ainsi cr&#233;&#233;e, nous abordons le probl&#232;me du rep&#233;rage des liens lexicaux
&#224; travers une application de TAL, la segmentation th&#233;matique. Nous discutons l&#8217;importance,
pour cette t&#226;che, de la ressource lexicale mobilixs&#233;e ; puis nous pr&#233;sentons la base de voisins
distributionnels que nous utilisons ; enfin, nous montrons qu&#8217;elle permet, dans un syst&#232;me de
segmentation th&#233;matique inspir&#233; de (Hearst, 1997), des performances sup&#233;rieures &#224; celles obte-
nues avec une ressource traditionnelle.
</p>
<p>Abstract. The present work takes place within the Voiladis project (Lexical neighborhood
for discourse analysis), whose purpose is to exploit lexical cohesion markers in the study of
various discursive phenomena. We want to show the relevance of a distribution-based lexical
resource to locate interesting relations between lexical items in a text. We call neighbors lexical
items that share a significant number of syntactic contexts in a given corpus. In order to evaluate
the usefulness of such a resource, we address the task of topical segmentation of text, which
generally makes use of some kind of lexical relations. We discuss here the importance of the
particular resource used for the task of text segmentation. Using a system inspired by (Hearst,
1997), we show that lexical neighbors provide better results than a classical resource.
</p>
<p>Mots-cl&#233;s : Coh&#233;sion lexicale, ressources lexicales, analyse distributionnelle, segmen-
tation th&#233;matique.
</p>
<p>Keywords: Lexical cohesion, lexical resources, distributional analysis, text segmenta-
tion.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Cl&#233;mentine Adam, Fran&#231;ois Morlane-Hond&#232;re
</p>
<p>1 Introduction
</p>
<p>L&#8217;&#233;tude de la structure du discours est un champ de la linguistique qui a suscit&#233; de nombreux
travaux depuis les ann&#233;es soixante, et qui b&#233;n&#233;ficie actuellement d&#8217;un regain d&#8217;int&#233;r&#234;t li&#233; aux
enjeux qu&#8217;il soul&#232;ve pour le traitement automatique des langues. L&#8217;automatisation de la mise au
jour des structures discursives pourrait en effet avoir un impact important sur toute application
de TAL n&#233;cessitant une vision des textes allant au del&#224; du simple &#171; sac de mots &#187; : fouille de
donn&#233;es textuelles, r&#233;sum&#233; automatique, navigation intra-documentaire, etc. (P&#233;ry-Woodley &amp;
Scott, 2006).
</p>
<p>L&#8217;analyse du discours repose sur l&#8217;observation selon laquelle un texte n&#8217;est pas une simple suc-
cession de phrases, mais un tout coh&#233;rent. Cette coh&#233;rence, propri&#233;t&#233; intrins&#232;que des textes, est
refl&#233;t&#233;e par les observables que sont les marques de coh&#233;sion. Le concept de coh&#233;sion englobe
tous les ph&#233;nom&#232;nes qui permettent de relier entre elles les phrases d&#8217;un texte, participant ainsi
&#224; cr&#233;er sa texture (Halliday &amp; Hasan, 1976). Les proc&#233;d&#233;s coh&#233;sifs classiquement consid&#233;r&#233;s,
dans la continuit&#233; d&#8217;Halliday &amp; Hassan, sont la r&#233;f&#233;rence, la substitution, l&#8217;ellipse, la conjonc-
tion et surtout la coh&#233;sion lexicale, qui est reconnue comme &#233;tant le principal vecteur de texture
(Hoey, 1991).
</p>
<p>Le projet VOILADIS 1 (VOIsinage Lexical pour l&#8217;Analyse du DIScours), dans lequel s&#8217;inscrit
cette &#233;tude, a pour but d&#8217;utiliser des indices lexicaux pour la mise au jour de ph&#233;nom&#232;nes
discursifs, dans une vis&#233;e d&#8217;automatisation. Ce champ est encore peu explor&#233; : la coh&#233;sion
lexicale reste peu exploit&#233;e sur le plan applicatif car elle est difficile &#224; appr&#233;hender, et donc &#224;
rep&#233;rer automatiquement. En effet, elle r&#233;side g&#233;n&#233;ralement dans des relations non classiques,
que les lexiques ne recensent pas (Morris &amp; Hirst, 2004).
</p>
<p>Dans le cadre du projet VOILADIS, la ressource mobilis&#233;e est une base de voisins distribution-
nels : l&#8217;analyse distributionnelle automatique de grands corpus permet en effet de rapprocher
des mots pr&#233;sentant des contextes d&#8217;apparition similaires, lesquels ont tendance &#224; &#234;tre li&#233;s par
une relation s&#233;mantique qui va souvent au-del&#224; des classifications traditionnelles. Cette m&#233;-
thode permet &#233;galement de disposer d&#8217;une ressource qui refl&#232;te v&#233;ritablement les relations qui
op&#232;rent sur un texte donn&#233;, dans le sens o&#249; la base distributionnelle est avant tout le fruit de
l&#8217;analyse syntaxique du corpus. L&#8217;objectif &#224; long terme du projet VOILADIS, encore en phase
exploratoire, est d&#8217;&#233;valuer l&#8217;apport d&#8217;une telle ressource &#224; diff&#233;rentes approches du discours.
</p>
<p>Dans une premi&#232;re &#233;tape, nous nous sommes int&#233;ress&#233;s &#224; une application qui a tout particuli&#232;re-
ment exploit&#233; les indices de nature lexicale : la segmentation th&#233;matique. Cette approche assez
empirique du discours vise l&#8217;identification de segments textuels, de blocs homog&#232;nes du point
de vue de leur objet. Cette t&#226;che est parmi les plus tributaires des ph&#233;nom&#232;nes de coh&#233;sion,
dans le sens o&#249; les zones th&#233;matiques ne sont d&#233;finies que par le fait qu&#8217;elles se trouvent &#234;tre
particuli&#232;rement coh&#233;sives, ce qui laisse &#224; penser que plus le rep&#233;rage des relations lexicales
sera efficace, mieux les zones seront d&#233;finies. Pour &#233;valuer la pertinence de notre base de voi-
sins distributionnels pour le rep&#233;rage des liens de coh&#233;sion lexicale, nous avons donc choisi de
mesurer son apport &#224; un syst&#232;me de segmentation automatique.
</p>
<p>Dans la suite de cet article, nous discutons de la d&#233;pendance de la segmentation th&#233;matique &#224; une
prise en compte de la coh&#233;sion lexicale, qu&#8217;elle soit basique ou plus fine. Puis nous d&#233;crivons
la base de voisins que nous avons mobilis&#233;e, et plus largement, la m&#233;thode qui a permis de
</p>
<p>1. Projet du PRES Toulouse coordonn&#233; par C&#233;cile Fabre impliquant des chercheurs des laboratoires IRIT
(&#233;quipe LiLac) et CLLE-ERSS (axes TAL et S&#8217;caladis).</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>D&#233;tection de la coh&#233;sion lexicale par voisinage distributionnel
</p>
<p>la construire, et discutons a priori de sa pertinence pour appr&#233;hender des relations lexicales
vari&#233;es. Enfin, nous relatons la d&#233;marche que nous avons suivie pour montrer l&#8217;apport de cette
ressource &#224; un syst&#232;me de segmentation th&#233;matique.
</p>
<p>2 Segmentation th&#233;matique et coh&#233;sion lexicale
</p>
<p>Le but de la segmentation th&#233;matique est d&#8217;effectuer le pavage d&#8217;un texte en segments cons&#233;-
cutifs cens&#233;s pr&#233;senter une homog&#233;n&#233;it&#233; du point de vue de leurs th&#232;mes. Cette t&#226;che peut
permettre d&#8217;am&#233;liorer les performances de diverses applications : recherche d&#8217;information &#8211;
(Callan et al., 1992), entre autres, a montr&#233; qu&#8217;un syst&#232;me de recherche d&#8217;information gagne &#224;
indexer des unit&#233;s inf&#233;rieures au document &#8211;, r&#233;sum&#233; automatique (Brunn et al., 2001), extrac-
tion d&#8217;information, etc.
</p>
<p>De nombreux algorithmes ont &#233;t&#233; d&#233;velopp&#233;s pour la segmentation th&#233;matique, que l&#8217;on peut
grosso modo regrouper en deux familles (Hernandez, 2004) : (a) ceux qui parcourent lin&#233;aire-
ment le texte selon une fen&#234;tre d&#8217;observation glissante, et proc&#232;dent donc de mani&#232;re ascendante
et (b) ceux qui calculent une matrice de similarit&#233; pour l&#8217;ensemble des unit&#233;s du texte avant de
d&#233;cider o&#249; placer les ruptures, proc&#233;dant donc de mani&#232;re descendante (Malioutov &amp; Barzilay,
2006). Les syst&#232;mes les plus connus repr&#233;sentant ces deux familles sont d&#8217;une part l&#8217;algorithme
TextTiling de (Hearst, 1997), et d&#8217;autre part l&#8217;algorithme C99 de (Choi, 2000).
</p>
<p>Malgr&#233; la vari&#233;t&#233; des approches, la diff&#233;rence entre algorithmes n&#8217;appara&#238;t pas cruciale. Ce qui
compte avant tout, ce sont les indices utilis&#233;s pour mesurer la similarit&#233; entre unit&#233;s de segmen-
tation, que ce soit au niveau local ou global. Pour mesurer la &#171; force &#187; de la coh&#233;sion lexicale
entre deux pans de texte, on se base sur le nombre de liens (&#233;ventuellement pond&#233;r&#233;s) qu&#8217;entre-
tiennent les unit&#233;s lexicales qu&#8217;ils contiennent. Ces liens peuvent &#234;tre de natures diverses.
</p>
<p>Beaucoup de syst&#232;mes de segmentation th&#233;matique se cantonnent aux liens de r&#233;p&#233;tition lexi-
cale, c&#8217;est-&#224;-dire aux r&#233;p&#233;titions de formes, de formes tronqu&#233;es ou de lemmes (Hearst, 1997;
Choi, 2000) ; une extension consiste &#224; prendre en compte les r&#233;p&#233;titions de n-grammes, en leur
attribuant un poids plus important (Beeferman et al., 1997). L&#8217;inconv&#233;nient de ces approches
est que les scores sont alors bas&#233;s sur un nombre tr&#232;s restreint d&#8217;occurrences, et que beaucoup
de liens participant &#224; la coh&#233;sion sont donc ignor&#233;s. Pour pallier ce probl&#232;me, il est n&#233;cessaire
de faire appel &#224; une ressource ext&#233;rieure. Cette solution est toujours pr&#233;sent&#233;e, &#224; notre connais-
sance, comme permettant d&#8217;am&#233;liorer les performances des syst&#232;mes, au point que les auteurs
mettent parfois plus l&#8217;accent sur la ressource utilis&#233;e que sur l&#8217;originalit&#233; de leur algorithme.
Les ressources mobilis&#233;es varient beaucoup du point de vue des relations lexicales qu&#8217;elles
permettent de d&#233;tecter.
</p>
<p>Certains utilisent une ressource g&#233;n&#233;rique, construite &#224; partir d&#8217;un dictionnaire ou d&#8217;un th&#233;sau-
rus (Kozima, 1993; Lin et al., 2004; Morris &amp; Hirst, 2004). Ainsi, les liens de synonymie, et
dans le meilleur des cas ceux relevant d&#8217;autres relations classiques telles que l&#8217;hyperonymie et
l&#8217;antonymie, peuvent &#234;tre pris en compte. D&#8217;autres s&#8217;appuient sur des ressources construites en
corpus (Choi et al., 2001; Bolshakov &amp; Gelbukh, 2001; Ferret, 2002). Les m&#233;thodes de consti-
tution de ces ressources varient, mais tournent toujours autour de l&#8217;extraction de collocations ou
de cooccurrences. Par exemple, l&#8217;analyse s&#233;mantique latente (ASL) permet d&#8217;&#233;valuer la proxi-
mit&#233; s&#233;mantique entre des couples de mots en fonction de leurs cooccurrences au sein de m&#234;mes
phrases, paragraphes ou textes sur l&#8217;ensemble d&#8217;un corpus (Choi et al., 2001).</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Cl&#233;mentine Adam, Fran&#231;ois Morlane-Hond&#232;re
</p>
<p>Les auteurs pr&#244;nant des approches bas&#233;es sur corpus font g&#233;n&#233;ralement &#233;tat de meilleures per-
formances. En effet, les liens lexicaux mis en jeu dans la coh&#233;sion lexicale vont bien au del&#224; des
relations traditionnelles. Classiquement, suivant (Halliday &amp; Hasan, 1976), on distingue entre :
&#8211; Des relations de r&#233;it&#233;ration, qui englobent la r&#233;p&#233;tition lexicale, la reprise par un synonyme
</p>
<p>ou un hyperonyme, voire d&#8217;autres relations paradigmatiques classiques telles que l&#8217;antonymie
et la m&#233;ronymie ;
</p>
<p>&#8211; Des relations dites de collocation, qui associent des mots pr&#233;sentant une tendance &#224; appa-
ra&#238;tre ensemble, mais ne relevant pas de la r&#233;it&#233;ration. Il s&#8217;agit pas n&#233;cessairement de relations
d&#8217;ordre syntagmatique, l&#8217;acception du terme &#171; collocation &#187; &#233;tant ici plus vaste. Des &#233;tudes
(Morris &amp; Hirst, 2004) ont montr&#233; que chez les lecteurs, les relations les plus pertinentes
pour le rep&#233;rage des structures discursives &#233;taient dans la plupart des cas des relations &#233;chap-
pant donc aux typologies traditionnelles. Lorsqu&#8217;il s&#8217;agit d&#8217;interpr&#233;ter un texte, les relations
comme la synonymie, l&#8217;antonymie, etc. c&#232;dent le pas &#224; des relations &#171; non classiques &#187;, moins
facilement d&#233;finissables car plus d&#233;pendantes des mots entre lesquels elles se manifestent
(chien/aboyer, abeille/miel, etc.).
</p>
<p>Ainsi, construire une ressource &#224; partir d&#8217;un corpus permet d&#8217;appr&#233;hender des relations plus
vari&#233;es, et plus pertinentes pour la structuration du discours. Nous nous inscrivons dans cette
veine. Mais nous ne nous limitons pas &#224; l&#8217;extraction de collocations ou de cooccurrences :
l&#8217;originalit&#233; de notre ressource est qu&#8217;elle ne se cantonne pas aux relations syntagmatiques.
Construite gr&#226;ce &#224; l&#8217;analyse distributionnelle d&#8217;un corpus, elle repose sur des informations lin-
guistiques plus riches, susceptibles de mettre au jour des relations d&#8217;ordre paradigmatique.
</p>
<p>3 Notre ressource : les voisins distributionnels
</p>
<p>La base de voisins que nous avons utilis&#233;e pour cette &#233;tude a &#233;t&#233; g&#233;n&#233;r&#233;e par le programme
Up&#233;ry (Bourigault, 2002) &#224; partir d&#8217;un corpus constitu&#233; de l&#8217;ensemble des articles de la ver-
sion francophone de Wikip&#233;dia, soit plus de 470 000 articles pour 194 millions de mots 2. Ces
donn&#233;es ont &#233;t&#233; pr&#233;alablement trait&#233;es par l&#8217;analyseur syntaxique Syntex (Bourigault, 2007),
qui utilise l&#8217;analyse en d&#233;pendance pour g&#233;n&#233;rer une repr&#233;sentation du texte particuli&#232;rement
propice &#224; la m&#233;thode distributionnelle.
</p>
<p>Le processus permettant d&#8217;obtenir une base de voisins distributionnels &#224; partir d&#8217;un corpus se
divise en trois &#233;tapes :
</p>
<p>1. le corpus est &#233;tiquet&#233; avec TreeTagger 3 ;
2. il est ensuite trait&#233; par Syntex, qui extrait les relations syntaxiques sous forme de triplets
</p>
<p>&lt;gouverneur, relation, d&#233;pendant&gt;. Ainsi, le programme va rep&#233;rer dans la phrase Pierre
mange un biscuit les triplets &lt;manger, suj, Pierre&gt; et &lt;manger, obj, biscuit&gt;. Quand la
relation de d&#233;pendance syntaxique se fait via une pr&#233;position, cette derni&#232;re prend la place
de la relation au sein du triplet (biscuit au chocolat se repr&#233;sente &lt;biscuit, &#224;, chocolat&gt;) ;
</p>
<p>3. afin de faciliter leur traitement par le logiciel Up&#233;ry, les triplets obtenus sont ramen&#233;s sous
la forme de couples &lt;pr&#233;dicat, argument&gt; o&#249; le pr&#233;dicat correspond au gouverneur auquel
on accole la relation, et o&#249; l&#8217;argument correspond au d&#233;pendant (&lt;biscuit, &#224;, chocolat&gt;
devient &lt;biscuit_&#224;, chocolat&gt;). Cette formalisation va permettre d&#8217;op&#233;rer un double rap-
prochement : celui des pr&#233;dicats partageant les m&#234;mes arguments, mais aussi celui des
</p>
<p>2. Il s&#8217;agit d&#8217;une version de Wikipedia datant d&#8217;avril 2007. Son traitement ainsi que la cr&#233;ation de la base de
voisins sont dus au travail de Franck Sajous (CLLE-ERSS).
</p>
<p>3. Universit&#233; de Stuttgart (www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/).</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>D&#233;tection de la coh&#233;sion lexicale par voisinage distributionnel
</p>
<p>arguments partageant les m&#234;mes pr&#233;dicats. Concr&#232;tement, dans notre base de voisins, le
pr&#233;dicat manger_obj est rapproch&#233; de se nourrir_de via les arguments pousse, bourgeon,
crustac&#233;, etc., et l&#8217;argument biscuit est rapproch&#233; de sucre via les pr&#233;dicats fabrique_de,
marque_de, production_de, etc. Lors de cette &#233;tape, le programme attribue &#224; chaque paire
de voisins un score de proximit&#233; qui indique dans quelle mesure les distributions des deux
mots rapproch&#233;s sont similaires. Ce score est obtenu par la mesure de Lin (Lin, 1998) ; il
se d&#233;roule en deux &#233;tapes. La premi&#232;re consiste &#224; calculer la quantit&#233; d&#8217;information (QI)
de chaque pr&#233;dicat et argument, c&#8217;est-&#224;-dire le rapport du nombre d&#8217;arguments/pr&#233;dicats
avec lesquels ils se combinent dans le corpus, sur la totalit&#233; des arguments/pr&#233;dicats avec
lesquels ils pourraient se combiner. Dans un second temps, il s&#8217;agit de rapprocher les ar-
guments/pr&#233;dicats entre eux en s&#8217;appuyant sur le nombre de pr&#233;dicats/arguments qu&#8217;ils
partagent et de diviser la QI de ces cooccurrents syntaxiques communs aux deux voisins
(multipli&#233;e par 2) par la somme des QI respectives de chacun des voisins.
</p>
<p>Cette m&#233;thode poss&#232;de l&#8217;avantage de permettre les rapprochements intercat&#233;goriels qui font
cruellement d&#233;faut aux ressources actuellement disponibles. Ainsi, le verbe pr&#233;dicat manger_obj
se retrouve &#233;galement associ&#233; &#224; des pr&#233;dicats nominaux comme bouillon_de, cuisson_de, re-
cette_de ou plat_de via des arguments communs comme viande, poulet, poisson, spaghetti, etc.
</p>
<p>La base ainsi obtenue compte environ quatre millions de couples, qui exhibent des relations
tr&#232;s h&#233;t&#233;rog&#232;nes : des &#233;tudes comme (Bourigault &amp; Galy, 2005) ou (Fabre &amp; Bourigault, 2006)
ont montr&#233; qu&#8217;il est difficile de d&#233;gager une typologie des relations de voisinage extraites ;
c&#8217;est la cons&#233;quence de l&#8217;application des m&#233;thodes d&#8217;analyse distributionnelle &#224; un corpus non
sp&#233;cialis&#233;, qui pr&#233;sente moins de redondance et donc des restrictions syntaxiques moins fortes.
En contrepartie, cette ressource offre la possibilit&#233; de capter un large &#233;ventail de relations de
proximit&#233; s&#233;mantique, &#224; condition d&#8217;introduire des filtres sur les scores de voisinage.
</p>
<p>4 Voisins distributionnels et segmentation th&#233;matique
</p>
<p>Le but de cette exp&#233;rience est de montrer la pertinence du voisinage distributionnel pour d&#233;tecter
les liens de coh&#233;sion lexicale, en nous appuyant sur les r&#233;sultats d&#8217;un syst&#232;me de segmentation
th&#233;matique. Nous avons &#224; cet effet impl&#233;ment&#233; un algorithme de segmentation inspir&#233; de Text-
Tiling (Hearst, 1997) et bas&#233; uniquement sur la prise en compte de liens lexicaux. Nous avons
soumis au syst&#232;me d&#233;velopp&#233; un m&#234;me corpus en sp&#233;cifiant chaque fois des liens diff&#233;rents :
uniquement des liens de r&#233;p&#233;tition lexicale dans un premier temps ; des liens de synonymie re-
p&#233;r&#233;s &#224; partir d&#8217;un dictionnaire de synonymes 4 dans un deuxi&#232;me temps ; et enfin, des liens de
voisinage distributionnel. Nous d&#233;crivons dans la suite de cette section les diff&#233;rentes &#233;tapes de
cette exp&#233;rience : constitution du corpus, projection des liens lexicaux, application de l&#8217;algo-
rithme de segmentation th&#233;matique et &#233;valuation.
</p>
<p>Corpus Le corpus que nous utilisons est constitu&#233; de 30 articles issus de l&#8217;encyclop&#233;die en
ligne Wikipedia (dans la version ayant servi &#224; construire la base de voisins distributionnels). Ces
articles traitent tous de lieux &#8211; pays (par exemple Danemark) ou villes (Salzbourg). En effet,
</p>
<p>4. Le dictionnaire Dicosyn. D&#233;velopp&#233; au CRISCO (Universit&#233; de Caen), il regroupe les synonymes pr&#233;sents
dans sept dictionnaires classiques, &#224; savoir le Bailly, le Benac, le Du Chazaud, le Guizot, le Lafaye, le Larousse
et le Robert. Il compte environ 49 000 entr&#233;es pour 396 000 relations synonymiques et est consultable en ligne &#224;
l&#8217;adresse suivante : http ://www.crisco.unicaen.fr/cgi-bin/cherches.cgi</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Cl&#233;mentine Adam, Fran&#231;ois Morlane-Hond&#232;re
</p>
<p>selon nos observations, dans cette cat&#233;gorie d&#8217;articles, les diff&#233;rentes sections correspondent
g&#233;n&#233;ralement &#224; diff&#233;rents &#171; th&#232;mes &#187; (histoire, g&#233;ographie, culture, etc.). Cela nous permet
ainsi de justifier l&#8217;utilisation des titres comme ruptures de r&#233;f&#233;rence lors de l&#8217;&#233;valuation de la
segmentation effectu&#233;e. Le corpus est divis&#233; en 1584 paragraphes (donc 1584 &#8722; 30 = 1554
ruptures possibles 5) et contient 302 titres de sections (donc 302 ruptures de r&#233;f&#233;rence).
</p>
<p>Projection des liens coh&#233;sifs sur le corpus Nous relions dans le corpus des couples de mots,
sans pond&#233;rer le lien qu&#8217;ils entretiennent. Seuls les liens allant au del&#224; de la phrase sont pris
en compte. Pour notre premi&#232;re baseline, toutes les r&#233;p&#233;titions de lemmes de noms, de verbes
et d&#8217;adjectifs sont indiqu&#233;es. Pour la seconde, toutes les paires de synonymes recens&#233;es par
notre dictionnaire sont projet&#233;es. Pour projeter les voisins, nous avons d&#251; fixer (de mani&#232;re
empirique) diff&#233;rents seuils dus au caract&#232;re pl&#233;thorique de la ressource : les couples projet&#233;s
sont ceux dont le score de Lin d&#233;passe 0.25 et pour lesquels chaque membre du couple est parmi
les 15 meilleurs voisins de l&#8217;autre membre.
</p>
<p>Nous proposons dans les figures 1, 2 et 3 des visualisations des liens obtenus avec les trois
approches d&#233;crites, pour un extrait de l&#8217;article Slovaquie. On peut constater la raret&#233; des liens
</p>
<p>FIGURE 1 &#8211; Liens de r&#233;p&#233;tition
</p>
<p>de r&#233;p&#233;tition : seulement 3 liens l&#224; o&#249; la synonymie et le voisinage permettent de d&#233;tecter
respectivement 7 et 8 liens. La r&#233;p&#233;tition est toutefois dans cet exemple la seule m&#233;thode qui
permet de tisser des liens coh&#233;sifs entre noms propres (ici, Danube). Les liens de synonymie
</p>
<p>FIGURE 2 &#8211; Liens de synonymie
</p>
<p>concernent majoritairement des adjectifs, avec grand, haut et long, tous synonymes entre eux.
Le lien de synonymie entre s&#8217;&#233;tendent et contiennent est difficilement interpr&#233;table, en tout cas
dans ce contexte : on touche ici aux limites d&#8217;une ressource constitu&#233;e in abstracto, dans le
sens o&#249; les relations lexicales qu&#8217;elle recense ne sont en aucun cas adapt&#233;es &#224; un type de texte
ou &#224; un corpus en particulier, contrairement &#224; la base de voisins qui est construite de fa&#231;on
dynamique. Dans cet exemple, les relations de voisinage permettent de lier des mots r&#233;pertori&#233;s
</p>
<p>5. Pour chacun des 30 textes de n paragraphes, on a n&#8722; 1 ruptures possibles.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>D&#233;tection de la coh&#233;sion lexicale par voisinage distributionnel
</p>
<p>FIGURE 3 &#8211; Liens de voisinage distributionnel
</p>
<p>par Dicosyn comme &#233;tant des synonymes (plaine/vall&#233;e), des co-hyponymes (nord/sud-est/sud-
ouest), mais &#233;galement des mots qui entretiennent des relations moins faciles &#224; cat&#233;goriser
comme pays/fronti&#232;re, ou fronti&#232;re/nord. Ces derni&#232;res relations sont celles qui nous sont le
plus pr&#233;cieuses, puisque leur rep&#233;rage est une des sp&#233;cificit&#233;s de notre m&#233;thode. Et m&#234;me si l&#8217;on
pourrait consid&#233;rer que le premier couple, pays/fronti&#232;re, rel&#232;ve d&#8217;une relation de m&#233;ronymie,
il est difficile de donner un nom &#224; la relation fronti&#232;re/nord. Dans la mesure o&#249; ces deux mots
font partie du m&#234;me champ s&#233;mantique (celui de la g&#233;ographie) et que leur mode de liaison
&#233;chappe &#224; toute classification, on peut consid&#233;rer qu&#8217;on a l&#224; un cas de collocation au sens de
(Halliday &amp; Hasan, 1976). Ind&#233;pendamment des performances que nous pr&#233;senterons dans la
suite de l&#8217;article, on voit d&#233;j&#224; que les voisins pr&#233;sentent un int&#233;r&#234;t &#233;vident du fait qu&#8217;ils mettent
au jour des liens qu&#8217;aucune ressource classique ne permettrait de d&#233;tecter.
</p>
<p>L&#8217;algorithme de segmentation Pour la segmentation des textes du corpus, nous avons opt&#233;
pour une approche lin&#233;aire, par fen&#234;tre glissante, &#224; la mani&#232;re de (Hearst, 1997). Cette approche
n&#8217;est pas forc&#233;ment la plus performante, mais nous l&#8217;avons pr&#233;f&#233;r&#233;e en raison de sa simplicit&#233;
d&#8217;impl&#233;mentation ; en effet, nous ne poursuivons pas ici l&#8217;efficacit&#233;, mais la comparaison entre
diff&#233;rentes ressources ; l&#8217;important pour nous &#233;tait donc avant tout d&#8217;appliquer le m&#234;me algo-
rithme pour chaque ressource, quel que soit cet algorithme.
</p>
<p>Notre unit&#233; de base est la phrase ; la fen&#234;tre d&#8217;observation que nous appliquons pour calculer les
scores de similarit&#233; est d&#8217;une taille de 6 unit&#233;s (param&#232;tre conseill&#233; par (Hearst, 1997)). Ainsi,
&#224; la fin de chaque phrase, un score bas&#233; sur les liens entretenus par deux blocs de trois phrases
est calcul&#233; (figure 4).
</p>
<p>FIGURE 4 &#8211; Repr&#233;sentation du calcul du score avec fen&#234;tre glissante
</p>
<p>Le score calcul&#233; est le suivant : S = log
(
</p>
<p>Nliens
Nliens possibles
</p>
<p>)
. Le nombre de liens Nliens est le nombre
</p>
<p>de couples de mots jug&#233;s similaires &#224; cheval entre les deux blocs de trois phrases. Le nombre de
liens possibles Nliens possibles est le produit des nombres de mots pouvant &#234;tre li&#233;s dans chaque
bloc (c&#8217;est-&#224;-dire des noms, adjectifs et verbes).
</p>
<p>La courbe des scores calcul&#233;s est ensuite liss&#233;e. Toutes les vall&#233;es sont rep&#233;r&#233;es, et leurs pro-
fondeurs calcul&#233;es. On appelle vall&#233;e un point de la courbe qui est entour&#233; par des points de</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Cl&#233;mentine Adam, Fran&#231;ois Morlane-Hond&#232;re
</p>
<p>valeurs plus &#233;lev&#233;es (c&#8217;est-&#224;-dire un minimum local). Pour d&#233;terminer la profondeur d&#8217;une val-
l&#233;e, on remonte de part et d&#8217;autre du point consid&#233;r&#233; tant que l&#8217;on rencontre des valeurs plus
&#233;lev&#233;es ; la profondeur de la vall&#233;e est calcul&#233;e en faisant la moyenne des deux diff&#233;rences cal-
cul&#233;es (&#224; gauche et &#224; droite du point). Les vall&#233;es dont la profondeur d&#233;passe l&#8217;&#233;cart-type &#224; la
moyenne sont consid&#233;r&#233;es comme correspondant aux ruptures du texte. Ces ruptures, situ&#233;es
entre deux phrases, sont ramen&#233;es &#224; la fronti&#232;re de paragraphe la plus proche, ce qui produit le
texte segment&#233; final.
</p>
<p>D&#233;cisions prises sur l&#8217;&#233;valuation &#201;valuer un syst&#232;me de segmentation th&#233;matique est d&#233;licat.
De nombreux probl&#232;mes sont soulev&#233;s, et peuvent grosso modo &#234;tre ramen&#233;s &#224; deux questions :
(a) Quelle r&#233;f&#233;rence ? (b) Quel mesure d&#8217;&#233;valuation ?
</p>
<p>(a) Pour &#233;valuer la segmentation automatique, il faut la comparer &#224; une segmentation de r&#233;f&#233;-
rence. Certains font pour cela appel &#224; des annotations manuelles, mais font g&#233;n&#233;ralement &#233;tat
d&#8217;accords inter-annotateurs tr&#232;s faibles. D&#8217;autres prennent le parti d&#8217;accoler bout &#224; bout des
s&#233;quences appartenant &#224; des textes diff&#233;rents ; les ruptures th&#233;matiques sont alors les ruptures
entre textes. Cette position pose un probl&#232;me &#233;vident de circularit&#233; : on fabrique l&#8217;objet que
l&#8217;on postule. Pour cette exp&#233;rience, nous avons d&#233;cid&#233; d&#8217;utiliser comme ruptures de r&#233;f&#233;rence
les positions des titres de sections.
</p>
<p>(b) Les scores habituels de pr&#233;cision et de rappel ne sont pas adapt&#233;s pour &#233;valuer un syst&#232;me
de segmentation th&#233;matique. En effet, ils ne permettent pas de rendre compte du fait qu&#8217;une
rupture proche de la rupture de r&#233;f&#233;rence est meilleure qu&#8217;une rupture &#233;loign&#233;e. D&#8217;autres scores
ont &#233;t&#233; propos&#233;s, dont les plus usit&#233;s sont les mesures Pk (Beeferman et al., 1999) et WindowDiff
(Pevzner &amp; Hearst, 2002). La mesure Pk consiste &#224; compter le nombre de fois o&#249; deux mots
pris au hasard &#224; une distance k sont dans le m&#234;me segment &#224; la fois dans la r&#233;f&#233;rence et dans
l&#8217;hypoth&#232;se. La mesure WindowDiff consiste &#224; calculer la diff&#233;rence du nombre de ruptures
dans une fen&#234;tre glissante. Nous donnons ici nos r&#233;sultats selon ces deux mesures.
</p>
<p>R&#233;sultats Nous reportons dans le tableau 1 les r&#233;sultats obtenus par l&#8217;algorithme de segmen-
tation appliqu&#233; au corpus d&#233;crit, selon les liens coh&#233;sifs pris en compte (r&#233;p&#233;tition, synonymie
ou voisinage distributionnel). Les scores affich&#233;s correspondent aux moyennes des scores ob-
tenus pour chaque texte. Il est &#224; noter qu&#8217;avec les mesures Pk et WindowDiff, un score moins
&#233;lev&#233; refl&#232;te de meilleures performances. Pour mettre en perspective les r&#233;sultats pr&#233;sent&#233;s,
nous rapportons &#233;galement les r&#233;sultats obtenus avec des ruptures plac&#233;es au hasard, le nombre
de ruptures de r&#233;f&#233;rences &#233;tant approximativement 6 connu.
</p>
<p>Liens pris en compte Pk WindowDiff
Hasard 0.436 0.452
</p>
<p>R&#233;p&#233;tition 0.353 0.359
Synonymie 0.349 0.358
Voisinage 0.329 0.336
</p>
<p>TABLE 1 &#8211; Performances de la segmentation th&#233;matique selon les liens pris en compte
</p>
<p>Ces r&#233;sultats sont corrects compte-tenu de la difficult&#233; de la t&#226;che : chacune des approches
permet une segmentation significativement meilleure que le hasard. Globalement, les r&#233;sultats
</p>
<p>6. Pour chaque texte, une variation de &#177;3 ruptures par rapport &#224; la r&#233;f&#233;rence est autoris&#233;e.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>D&#233;tection de la coh&#233;sion lexicale par voisinage distributionnel
</p>
<p>observ&#233;s avec les diff&#233;rents types de liens coh&#233;sifs sont assez proches. L&#8217;utilisation des liens
de voisinage semble justifi&#233;e, puisqu&#8217;elle apporte les meilleures performances dans cette ex-
p&#233;rience, alors que les synonymes ne se d&#233;marquent que tr&#232;s peu de l&#8217;approche basique par
r&#233;p&#233;titions. Il est donc confirm&#233; que les voisins permettent une d&#233;tection plus fine de la coh&#233;-
sion lexicale, du moins selon l&#8217;&#233;talon que nous avions choisi : la performance d&#8217;un syst&#232;me de
segmentation th&#233;matique.
</p>
<p>5 Conclusions et perspectives
</p>
<p>L&#8217;objectif de cette &#233;tude &#233;tait de montrer la pertinence du voisinage distributionnel pour la d&#233;-
tection de la coh&#233;sion lexicale. Nous avons &#224; cette fin impliqu&#233; les voisins recens&#233;s par notre
ressource dans un syst&#232;me de segmentation th&#233;matique. Les r&#233;sultats obtenus montrent un ap-
port significatif de la ressource mobilis&#233;e. Ainsi, nous avons vu qu&#8217;une ressource obtenue gr&#226;ce
&#224; l&#8217;analyse distributionnelle pr&#233;sente des avantages que n&#8217;ont pas les ressources traditionnelles.
Cette exp&#233;rience m&#233;riterait d&#8217;&#234;tre approfondie ; nous aimerions notamment comparer les voisins
avec une ressource plus similaire, comme par exemple avec des collocations ; il serait &#233;galement
int&#233;ressant d&#8217;&#233;tudier les possibilit&#233;s de combinaisons de ressources.
</p>
<p>Comme nous l&#8217;avons indiqu&#233; en introduction, la segmentation th&#233;matique n&#8217;est pas pour nous
une fin en soi. Si nous nous sommes ici donn&#233; pour but de rep&#233;rer des zones coh&#233;sives, c&#8217;est
avant tout pour confronter diff&#233;rentes m&#233;thodes de d&#233;tection des liens lexicaux. En effet, le
projet VOILADIS s&#8217;inscrit dans une d&#233;marche r&#233;solument ax&#233;e vers une analyse du discours
qui va au-del&#224; du simple d&#233;coupage th&#233;matique. En nous appuyant sur une ressource obtenue
gr&#226;ce &#224; l&#8217;analyse distributionnelle, nous esp&#233;rons mettre au point une m&#233;thode de d&#233;tection des
liens de coh&#233;sion lexicale assez efficace pour nous permettre de capter des particularit&#233;s dans
les fonctionnements discursifs des textes, &#224; l&#8217;instar des topic opening et topic closing que rep&#232;re
(Hoey, 1991) en se basant sur la partie du texte vers laquelle pointent les liens coh&#233;sifs.
</p>
<p>R&#233;f&#233;rences
</p>
<p>BEEFERMAN D., BERGER A. &amp; LAFFERTY J. (1997). Text segmentation using exponential
models. In Proceedings of the Second Conference on Empirical Methods in Natural Language
Processing, p. 35&#8211;46, Providence.
BEEFERMAN D., BERGER A. &amp; LAFFERTY J. (1999). Statistical models for text segmenta-
tion. Mach. Learn., 34(1-3), 177&#8211;210.
BOLSHAKOV I. A. &amp; GELBUKH A. (2001). Text segmentation into paragraphs based on local
text cohesion. In TSD &#8217;01 : Proceedings of the 4th International Conference on Text, Speech
and Dialogue, p. 158&#8211;166, Zelezna Ruda.
BOURIGAULT D. (2002). UPERY : un outil d&#8217;analyse distributionnelle &#233;tendue pour la
construction d&#8217;ontologies &#224; partir de corpus. In Actes de la 9econf&#233;rence sur le Traitement
Automatique de la Langue Naturelle, Nancy.
BOURIGAULT D. (2007). Un analyseur syntaxique op&#233;rationnel : SYNTEX. Habilitation &#224;
diriger des recherches. Universit&#233; Toulouse II &#8211; Le Mirail.
BOURIGAULT D. &amp; GALY E. (2005). Analyse distributionnelle de corpus de langue g&#233;n&#233;rale
et synonymie. In 4esJourn&#233;es de la linguistique de corpus, p. 163&#8211;174, Lorient.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Cl&#233;mentine Adam, Fran&#231;ois Morlane-Hond&#232;re
</p>
<p>BRUNN M., CHALI Y. &amp; PINCHAK C. J. (2001). Text summarization using lexical chains. In
Proceedings of the Document Understanding Conference (DUC 2001), p. 135&#8211;140, Nouvelle
Orl&#233;ans.
</p>
<p>CALLAN J. P., CROFT W. B. &amp; HARDING S. M. (1992). The inquery retrieval system. In
Proceedings of the Third International Conference on Database and Expert Systems Applica-
tions, p. 78&#8211;83.
</p>
<p>CHOI F. Y. Y. (2000). Advances in domain independent linear text segmentation. In Procee-
dings of the first conference on North American chapter of the Association for Computational
Linguistics, p. 26&#8211;33, San Francisco.
</p>
<p>CHOI F. Y. Y., WIEMER-HASTINGS P. &amp; MOORE J. (2001). Latent semantic analysis for
text segmentation. In Proceedings of the 2001 Conference on Empirical Methods in Natural
Language Processing, p. 109&#8211;117, Pittsburgh.
</p>
<p>FABRE C. &amp; BOURIGAULT D. (2006). Extraction de relations s&#233;mantiques entre noms et
verbes au-del&#224; des liens morphologiques. In Actes de la 13econf&#233;rence sur le Traitement Au-
tomatique de la Langue Naturelle, Louvain.
</p>
<p>FERRET O. (2002). Segmenter et structurer th&#233;matiquement des textes par l&#8217;utilisation
conjointe de collocations et de la r&#233;currence lexicale. In Actes de TALN 2002, p. 155&#8211;165,
Nancy.
</p>
<p>HALLIDAY M. A. K. &amp; HASAN R. (1976). Cohesion in English. Longman (Londres).
</p>
<p>HEARST M. A. (1997). Texttiling : segmenting text into multi-paragraph subtopic passages.
Computational Linguistics, 23(1), 33&#8211;64.
HERNANDEZ N. (2004). Description et d&#233;tection automatique de structures de textes. PhD
thesis, Universit&#233; Paris-Sud.
</p>
<p>HOEY M. (1991). Patterns of lexis in text. Oxford University Press (Oxford).
</p>
<p>KOZIMA H. (1993). Text segmentation based on similarity between words. In Proceedings of
the 31st annual meeting on Association for Computational Linguistics, p. 286&#8211;288, Columbus.
</p>
<p>LIN D. (1998). An information-theoretic definition of similarity. In Proceedings of the 15th
International Conference on Machine Learning, p. 296&#8211;304, Madison.
</p>
<p>LIN M., NUNAMAKER JR. J. F., CHAU M. &amp; CHEN H. (2004). Segmentation of lecture
videos based on text : a method combining multiple linguistic features. In Proceedings of the
37th Annual Hawaii International Conference on System Sciences, Hawaii.
</p>
<p>MALIOUTOV I. &amp; BARZILAY R. (2006). Minimum cut model for spoken lecture segmenta-
tion. In ACL-44 : Proceedings of the 21st International Conference on Computational Linguis-
tics and the 44th annual meeting of the Association for Computational Linguistics, p. 25&#8211;32,
Morristown, NJ, USA : Association for Computational Linguistics.
</p>
<p>MORRIS J. &amp; HIRST G. (2004). Non-classical lexical semantic relations. In Proceedings of
the HLT Workshop on Computational Lexical Semantics, p. 46&#8211;51, Boston.
</p>
<p>PEVZNER L. &amp; HEARST M. A. (2002). A critique and improvement of an evaluation metric
for text segmentation. Computational Linguistics, 28, 1&#8211;19.
P&#201;RY-WOODLEY &amp; SCOTT, Eds. (2006). Discours et Document : traitements automatiques.
Num&#233;ro th&#233;matique, volume TAL 47(2).</p>

</div></div>
</body></html>