RECITAL 2009, Senlis, 24-26 juin 2009

Detection de la cohésion lexicale par voisinage
distributionnel : application a la segmentation thématique

Clémentine Adam Francois Morlane-Hondere
CLLE / Université de Toulouse & CNRS
adam@univ-tlse2.fr, morlanehondere@ gmail.com

Résumé. Cette étude s’insere dans le projet VOILADIS (VOIsinage Lexical pour l’Ana-
lyse du DIScours), qui a pour objectif d’exploiter des marques de cohésion lexicale pour mettre
au jour des phénomenes discursifs. Notre propos est de montrer la pertinence d’une ressource,
construite par l’analyse distributionnelle automatique d’un corpus, pour repérer les liens lexi-
caux dans les textes. Nous désignons par voisins les mots rapprochés par l’analyse distribution-
nelle sur la base des contextes syntaxiques qu’ils partagent au sein du corpus. Pour évaluer la
pertinence de la ressource ainsi créée, nous abordons le probleme du repérage des liens lexicaux
a travers une application de TAL, la segmentation thématique. Nous discutons l’importance,
pour cette tache, de la ressource lexicale mobilixsée; puis nous présentons la base de voisins
distributionnels que nous utilisons; enﬁn, nous montrons qu’elle permet, dans un systeme de
segmentation thématique inspiré de (Hearst, 1997), des performances supérieures a celles obte-
nues avec une ressource traditionnelle.

Abstract. The present work takes place within the Voiladis project (Lexical neighborhood
for discourse analysis), whose purpose is to exploit lexical cohesion markers in the study of
various discursive phenomena. We want to show the relevance of a distribution-based lexical
resource to locate interesting relations between lexical items in a text. We call neighbors lexical
items that share a signiﬁcant number of syntactic contexts in a given corpus. In order to evaluate
the usefulness of such a resource, we address the task of topical segmentation of text, which
generally makes use of some kind of lexical relations. We discuss here the importance of the
particular resource used for the task of text segmentation. Using a system inspired by (Hearst,
1997), we show that lexical neighbors provide better results than a classical resource.

M0tS-CléS I Cohesion lexicale, ressources lexicales, analyse distributionnelle, segmen-
tation thématique.

Keywords: Lexical cohesion, lexical resources, distributional analysis, text segmenta-
tion.

Clementine Adam, Francois Morlane-Hondere

1 Introduction

L’ etude de la structure du discours est un champ de la linguistique qui a suscite de nombreux
travaux depuis les annees soixante, et qui beneﬁcie actuellement d’un regain d’interét lie aux
enj eux qu’il souleve pour le traitement automatique des langues. L’ automatisation de la Inise au
jour des structures discursives pourrait en effet avoir un impact important sur toute application
de TAL necessitant une vision des textes allant au dela du simple « sac de mots » : fouille de
donnees textuelles, resume automatique, navigation intra-documentaire, etc. (Pery-Woodley &
Scott, 2006).

L’ analyse du discours repose sur l’observation selon laquelle un texte n’est pas une simple suc-
cession de phrases, mais un tout coherent. Cette coherence, propriete intrinseque des textes, est
reﬂetee par les observables que sont les marques de cohesion. Le concept de cohesion englobe
tous les phenomenes qui permettent de relier entre elles les phrases d’un texte, participant ainsi
a creer sa texture (Halliday & Hasan, 1976). Les procedes cohesifs classiquement consideres,
dans la continuite d’Halliday & Hassan, sont la reference, la substitution, l’ellipse, la conjonc-
tion et surtout la cohesion lexicale, qui est reconnue comme etant le principal vecteur de texture
(Hoey, 1991).

Le projet VOILADIS1 (VOIsinage Lexical pour l’Analyse du DIScours), dans lequel s’inscrit
cette etude, a pour but d’utiliser des indices lexicaux pour la Inise au jour de phenomenes
discursifs, dans une visee d’automatisation. Ce champ est encore peu explore : la cohesion
lexicale reste peu exploitee sur le plan applicatif car elle est difﬁcile a apprehender, et done a
reperer automatiquement. En effet, elle reside generalement dans des relations non classiques,
que les lexiques ne recensent pas (Morris & Hirst, 2004).

Dans le cadre du projet VOILADIS, la ressource mobilisee est une base de voisins distribution-
nels : l’analyse distributionnelle automatique de grands corpus permet en effet de rapprocher
des mots presentant des contextes d’appa1ition similaires, lesquels ont tendance a etre lies par
une relation semantique qui Va souvent au-dela des classiﬁcations traditionnelles. Cette me-
thode permet egalement de disposer d’une ressource qui reﬂete veritablement les relations qui
operent sur un texte donne, dans le sens ou la base distributionnelle est avant tout le fruit de
l’analyse syntaxique du corpus. L’ objectif a long terme du projet VOILADIS, encore en phase
exploratoire, est d’evaluer l’apport d’une telle ressource a differentes approches du discours.

Dans une premiere etape, nous nous sommes interesses a une application qui a tout particuliere-
ment exploite les indices de nature lexicale : la segmentation thematique. Cette approche assez
empirique du discours vise l’identiﬁcation de segments textuels, de blocs homogenes du point
de vue de leur objet. Cette tache est parmi les plus tributaires des phenomenes de cohesion,
dans le sens o1‘1 les zones thematiques ne sont deﬁnies que par le fait qu’elles se trouvent etre
particulierement cohesives, ce qui laisse a penser que plus le reperage des relations lexicales
sera efﬁcace, mieux les zones seront deﬁnies. Pour evaluer la pertinence de notre base de voi-
sins distributionnels pour le reperage des liens de cohesion lexicale, nous avons donc choisi de
mesurer son apport a un systeme de segmentation automatique.

Dans la suite de cet article, nous discutons de la dependance de la segmentation thematique a une
prise en compte de la cohesion lexicale, qu’elle soit basique ou plus ﬁne. Puis nous decrivons
la base de voisins que nous avons mobilisee, et plus largement, la methode qui a permis de

1. Projet du PRES Toulouse coordonne par Cecile Fabre impliquant des chercheurs des laboratoires IRIT
(equipe LiLac) et CLLE—ERSS (axes TAL et S’ca1adis).

Détection de la cohésion lexicale par voisinage distributionnel

la construire, et discutons a priori de sa pertinence pour appréhender des relations lexicales
variées. Enﬁn, nous relatons la démarche que nous avons suivie pour montrer l’apport de cette
ressource a un systeme de segmentation thématique.

2 Segmentation thématique et cohésion lexicale

Le but de la segmentation thématique est d’effectuer le pavage d’un texte en segments conse-
cutifs censés présenter une homogénéité du point de vue de leurs themes. Cette tache peut
permettre d’améliorer les performances de diverses applications : recherche d’information —
(Callan et al., 1992), entre autres, a montré qu’un systeme de recherche d’information gagne a
indexer des unités inférieures au document —, résumé automatique (Brunn et al., 2001), extrac-
tion d’information, etc.

De nombreux algorithmes ont été développés pour la segmentation thématique, que l’on peut
grosso modo regrouper en deux familles (Hernandez, 2004) : (a) ceux qui parcourent linéaire-
ment le texte selon une fenétre d’observation glissante, et procedent donc de maniere ascendante
et (b) ceux qui calculent une matrice de similarité pour l’ensemble des unités du texte avant de
décider o1‘1 placer les ruptures, procédant donc de maniere descendante (Malioutov & Barzilay,
2006). Les systemes les plus connus représentant ces deux familles sont d’une part l’algorithme
TextTiling de (Hearst, 1997), et d’autre part l’algorithme C99 de (Choi, 2000).

Malgré la variété des approches, la différence entre algorithmes n’apparait pas cruciale. Ce qui
compte avant tout, ce sont les indices utilisés pour mesurer la similarité entre unités de segmen-
tation, que ce soit au niveau local ou global. Pour mesurer la « force » de la cohésion lexicale
entre deux pans de texte, on se base sur le nombre de liens (éventuellement pondérés) qu’entre-
tiennent les unités lexicales qu’ils contiennent. Ces liens peuvent étre de natures diverses.

Beaucoup de systemes de segmentation thématique se cantonnent aux liens de répétition lexi-
cale, c’est-a-dire aux répétitions de formes, de formes tronquées ou de lemmes (Hearst, 1997;
Choi, 2000); une extension consiste a prendre en compte les répétitions de n-grammes, en leur
attribuant un poids plus important (Beeferman et al., 1997). L’inconvénient de ces approches
est que les scores sont alors basés sur un nombre tres restreint d’occurrences, et que beaucoup
de liens participant a la cohésion sont donc ignorés. Pour pallier ce probleme, il est nécessaire
de faire appel a une ressource extérieure. Cette solution est toujours présentée, a notre connais-
sance, comme permettant d’améliorer les performances des systemes, au point que les auteurs
mettent parfois plus l’accent sur la ressource utilisée que sur l’origina1ité de leur algorithme.
Les ressources mobilisées varient beaucoup du point de Vue des relations lexicales qu’elles
permettent de détecter.

Certains utilisent une ressource générique, construite a partir d’un dictionnaire ou d’un thesau-
rus (Kozima, 1993; Lin et al., 2004; Morris & Hirst, 2004). Ainsi, les liens de synonymie, et
dans le meilleur des cas ceux relevant d’autres relations classiques telles que l’hyperonymie et
l’antonymie, peuvent étre pris en compte. D’autres s’appuient sur des ressources construites en
corpus (Choi et al., 2001; Bolshakov & Gelbukh, 2001; Ferret, 2002). Les méthodes de consti-
tution de ces ressources varient, mais tournent touj ours autour de l’extraction de collocations ou
de cooccurrences. Par exemple, l’analyse sémantique latente (ASL) permet d’évaluer la proxi-
mité sémantique entre des couples de mots en fonction de leurs cooccurrences au sein de memes
phrases, paragraphes ou textes sur l’ensemble d’un corpus (Choi et al., 2001).

Clémentine Adam, Francois Morlane-Hondere

Les auteurs pronant des approches basées sur corpus font généralement état de meilleures per-

formances. En effet, les liens lexicaux Inis en jeu dans la cohésion lexicale vont bien au dela des

relations traditionnelles. Classiquement, suivant (Halliday & Hasan, 1976), on distingue entre :

— Des relations de réitération, qui englobent la répétition lexicale, la reprise par un synonyme
ou un hyperonyme, voire d’ autres relations paradigmatiques classiques telles que l’antonyInie
et la méronymie ;

— Des relations dites de collocation, qui associent des mots présentant une tendance a appa-
raitre ensemble, mais ne relevant pas de la réitération. Il s’agit pas nécessairement de relations
d’ordre syntagmatique, l’acception du terme « collocation » étant ici plus vaste. Des études
(Morris & Hirst, 2004) ont montré que chez les lecteurs, les relations les plus pertinentes
pour le repérage des structures discursives étaient dans la plupart des cas des relations échap-
pant donc aux typologies traditionnelles. Lorsqu’il s’agit d’interpréter un texte, les relations
comme la synonymie, l’antonyInie, etc. cedent le pas a des relations « non classiques », moins
facilement déﬁnissables car plus dépendantes des mots entre lesquels elles se manifestent
(chien/aboyer, abeille/miel, etc.).

Ainsi, construire une ressource a partir d’un corpus permet d’appréhender des relations plus

variées, et plus pertinentes pour la structuration du discours. Nous nous inscrivons dans cette

veine. Mais nous ne nous limitons pas a l’extraction de collocations ou de cooccurrences :
l’originalité de notre ressource est qu’elle ne se cantonne pas aux relations syntagmatiques.

Construite grace a l’analyse distributionnelle d’un corpus, elle repose sur des informations lin-

guistiques plus riches, susceptibles de mettre au jour des relations d’ordre paradigmatique.

3 Notre ressource : les voisins distributionnels

La base de voisins que nous avons utilisée pour cette étude a été générée par le programme
Upéry (Bourigault, 2002) a partir d’un corpus constitué de l’ensemble des articles de la ver-
sion francophone de Wikipédia, soit plus de 470 000 articles pour 194 millions de mots 2. Ces
données ont été préalablement traitées par l’analyseur syntaxique Syntex (Bourigault, 2007),
qui utilise l’analyse en dépendance pour générer une représentation du texte particulierement
propice a la méthode distributionnelle.

Le processus permettant d’obtenir une base de voisins distributionnels a partir d’un corpus se
divise en trois étapes :

1. le corpus est étiqueté avec TreeTagger 3 ;

2. il est ensuite traité par Syntex, qui extrait les relations syntaxiques sous forme de triplets
<gouverneur, relation, dépendant>. Ainsi, le programme va repérer dans la phrase Pierre
mange un biscuit les triplets <manger, suj, Pierre> et <manger, obj, biscuit>. Quand la
relation de dépendance syntaxique se fait via une préposition, cette derniere prend la place
de la relation au sein du triplet (biscuit au chocolat se représente <biscuit, a, chocolat>);

3. aﬁn de faciliter leur traitement par le logiciel Upéry, les triplets obtenus sont ramenés sous
la forme de couples <prédicat, argument> ou le prédicat correspond au gouvemeur auquel
on accole la relation, et ou l’argument correspond au dépendant (<biscuit, a, chocolat>
devient <biscuit_a, chocolat>). Cette formalisation Va permettre d’opérer un double rap-
prochement : celui des prédicats partageant les memes arguments, mais aussi celui des

2. Il s’agit d’une Version de Wikipedia datant d’aVril 2007. Son traitement ainsi que la création de la base de
voisins sont dus au travail de Franck Sajous (CLLE—ERSS).
3. Université de Stuttgart (www.i1ns.uni—stuttgart.de/proj ekte/corplex/TreeTagger/).

Détection de la cohésion lexicale par voisinage distributionnel

arguments partageant les memes prédicats. Concretement, dans notre base de voisins, le
prédicat manger_0bj est rapproché de se n0urrir_de via les arguments pousse, bourgeon,
crustace’, etc., et l’argument biscuit est rapproché de sucre via les prédicats fabrique_de,
marque_de, pr0ducti0n_de, etc. Lors de cette étape, le programme attribue a chaque paire
de voisins un score de proximité qui indique dans quelle mesure les distributions des deux
mots rapprochés sont similaires. Ce score est obtenu par la mesure de Lin (Lin, 1998) ; il
se déroule en deux étapes. La premiere consiste a calculer la quantite’ d ’inf0rmati0n (QI)
de chaque prédicat et argument, c’est-a-dire le rapport du nombre d’arguments/prédicats
avec lesquels ils se combinent dans le corpus, sur la totalité des arguments/prédicats avec
lesquels ils pourraient se combiner. Dans un second temps, il s’agit de rapprocher les ar-
guments/prédicats entre eux en s’appuyant sur le nombre de prédicats/arguments qu’ils
partagent et de diviser la QI de ces cooccurrents syntaxiques communs aux deux voisins
(multipliée par 2) par la somme des QI respectives de chacun des voisins.

Cette méthode possede l’avantage de permettre les rapprochements intercatégoriels qui font
cruellement défaut aux ressources actuellement disponibles. Ainsi, le verbe prédicat manger_0bj
se retrouve également associé a des prédicats nominaux comme b0uill0n_de, cuiss0n_de, re-
cette_de ou plat_de via des arguments communs comme viande, poulet, poisson, spaghetti, etc.

La base ainsi obtenue compte environ quatre millions de couples, qui exhibent des relations
tres hétérogenes : des études comme (Bourigault & Galy, 2005) ou (Fabre & Bourigault, 2006)
ont montré qu’il est difﬁcile de dégager une typologie des relations de voisinage extraites;
c’est la conséquence de l’application des méthodes d’analyse distributionnelle a un corpus non
spécialisé, qui présente moins de redondance et donc des restrictions syntaxiques moins fortes.
En contrepartie, cette ressource offre la possibilité de capter un large éventail de relations de
proximité sémantique, a condition d’introduire des ﬁltres sur les scores de voisinage.

4 Voisins distributionnels et segmentation thématique

Le but de cette expérience est de montrer la pertinence du voisinage distributionnel pour détecter
les liens de cohésion lexicale, en nous appuyant sur les résultats d’un systeme de segmentation
thématique. Nous avons a cet effet implémenté un algorithme de segmentation inspiré de Text-
Tiling (Hearst, 1997) et basé uniquement sur la prise en compte de liens lexicaux. Nous avons
souInis au systeme développé un meme corpus en spéciﬁant chaque fois des liens différents :
uniquement des liens de répétition lexicale dans un premier temps; des liens de synonymie re-
pérés a partir d’un dictionnaire de synonymes 4 dans un deuxieme temps; et enﬁn, des liens de
voisinage distributionnel. Nous décrivons dans la suite de cette section les différentes étapes de
cette expérience : constitution du corpus, projection des liens lexicaux, application de l’algo-
rithme de segmentation thématique et évaluation.

Corpus Le corpus que nous utilisons est constitué de 30 articles issus de l’encyclopédie en
ligne Wikipedia (dans la version ayant servi a construire la base de voisins distributionnels). Ces
articles traitent tous de lieux — pays (par exemple Danemark) ou villes (Salzbourg). En effet,

4. Le dictionnaire Dicosyn. Développé au CRISCO (Université de Caen), il regroupe les synonymes présents
dans sept dictionnaires classiques, a savoir le Bailly, le Benac, le Du Chazaud, le Guizot, le Lafaye, le Larousse
et le Robert. 11 compte environ 49 000 entrées pour 396 000 relations synonymiques et est consultable en ligne a
l’adresse suivante : http ://www.crisco.unicaen.fr/cgi-bin/cherches.cgi

Clémentine Adam, Francois Morlane-Hondere

selon nos observations, dans cette catégorie d’articles, les différentes sections correspondent
généralement a différents « themes » (histoire, géographie, culture, etc.). Cela nous permet
ainsi de justiﬁer l’utilisation des titres comme ruptures de référence lors de l’évaluation de la
segmentation effectuée. Le corpus est divisé en 1584 paragraphes (donc 1584 — 30 = 1554
ruptures possibles 5) et contient 302 titres de sections (donc 302 ruptures de référence).

Projection des liens cohésifs sur le corpus Nous relions dans le corpus des couples de mots,
sans pondérer le lien qu’ils entretiennent. Seuls les liens allant au dela de la phrase sont pris
en compte. Pour notre premiere baseline, toutes les répétitions de lemmes de noms, de verbes
et d’adjectifs sont indiquées. Pour la seconde, toutes les paires de synonymes recensées par
notre dictionnaire sont projetées. Pour projeter les voisins, nous avons dﬁ ﬁxer (de maniere
empirique) différents seuils dus au caractere pléthorique de la ressource : les couples projetés
sont ceux dont le score de Lin dépasse 0.25 et pour lesquels chaque membre du couple est parmi
les 15 meilleurs voisins de l’autre membre.

Nous proposons dans les ﬁgures 1, 2 et 3 des visualisations des liens obtenus avec les trois
approches décrites, pour un extrait de l’article Slovaquie. On peut constater la rareté des liens

Le paysage slovaque est tres contrasté dans son relief . Les Carpathes ( qui commencent it Bratislava
l 5' étendenl sur la majorité de la moitié nord Pays’ . Parmi cet arc montagneux on di ' H hauls
sommets des Tatras ( Tatry l . qui so i estination t ‘ . ' r e s let contiennenl de nombreux
lacs et vallées ainsi — u:s ha‘-1t point de la Slovaquie . le Gerlachovskyl tit (2 2 655m } , et le Kr-iva
, symbole du Pays _ Les plaines se trouvenl au sud-ouest l Daﬂllbﬁ )et au sud-est . Les plus
grancles rivieres slcwaques , outre le DmuM s afﬂuenls , sont le V5111 et le Hron

. ainsi que la Morava qui forme la frontiére avec 1' Autriche .

   

FIGURE 1 — Liens de répétition

de répétition : seulement 3 liens la ou la synonymie et le voisinage permettent de détecter
respectivement 7 et 8 liens. La répétition est toutefois dans cet exemple la seule méthode qui
permet de tisser des liens cohésifs entre noms propres (ici, Danube). Les liens de synonymie

Le paysage slovaque est tres contrasté dans son relief . Les Carpathes ( qui commencent El Bratislava
) 5’ étﬂﬂdellt ' - :l - ' ' ' ' nord du as. Parmi cet arc montagneux on distin - -
sommels des Tatras { Talry 1| , qui sont une destination [res populaine 0.  I B I ‘
' ' . ‘ it {i 2 655m ] . el le Krivéi
. symbol -._ ‘ " ' u , 2- . ., : ;. 10118 du Danube let au sud—est . Les plus
grandes riweres slovaques . outre le Danube (‘ Dunaj 2) dont elles sont des afﬂuems . sont le Vzih et le Hron
. ainsi que la Morava qui forme la frontiére avec 1’ Autriche .

       
    

FIGURE 2 — Liens de synonymie

concernent majoritairement des adjectifs, avec grand, haut et long, tous synonymes entre eux.
Le lien de synonymie entre s ’e’tendent et contiennent est difﬁcilement interpretable, en tout cas
dans ce contexte : on touche ici aux limites d’une ressource constituée in abstracto, dans le
sens o1‘1 les relations lexicales qu’elle recense ne sont en aucun cas adaptées a un type de texte
ou a un corpus en particulier, contrairement a la base de voisins qui est construite de facon
dynamique. Dans cet exemple, les relations de voisinage permettent de lier des mots répertoriés

5. Pour chacun des 30 textes de 11 paragraphes, on a n — 1 ruptures possibles.

Detection de la cohesion lexicale par voisinage distributionnel

Le paysage slovaque est trés contrasté dans son relief . Les Carpathes ( qui commencent it Bratislava
) s’ étendent sur la majorité de la moilié I10l'd e Pays . Parmi cet arc montagneux on distingue les hauts

      
 
  

_ (2655m).etle Krivé,
F u Sud-0|-ii‘-St ( le long du Danube )et all Slldf-SI . Les plus
| I,’

grandes rwiéres slovaques , - . _ . unaj }dont elles sont des afﬂllellts , sont le Vtih et le I-Iron
. ainsi que la Morava qui forme la f1’0IIlié1'B avec 1' Aulriche .

lacs et W11é€S  le plus haut poi! de a ovaui - 1- A "u v
symbole du Pays Le Plames '

  
   

   
 

FIGURE 3 — Liens de voisinage distributionnel

par Dicosyn comme etant des synonymes (plaine/vallée), des co-hyponymes (n0rd/sud-est/sud-
ouest), mais egalement des mots qui entretiennent des relations moins faciles a categoriser
comme pays/frontiére, ou frontiére/nord. Ces demieres relations sont celles qui nous sont le
plus precieuses, puisque leur reperage est une des speciﬁcites de notre methode. Et meme si l’on
pourrait considerer que le premier couple, pays/frontiére, releve d’une relation de meronymie,
il est difﬁcile de donner un nom a la relation frontiére/nord. Dans la mesure o1‘1 ces deux mots
font partie du meme champ semantique (celui de la geographie) et que leur mode de liaison
echappe a toute classiﬁcation, on peut considerer qu’on a la un cas de collocation au sens de
(Halliday & Hasan, 1976). Independamment des performances que nous presenterons dans la
suite de l’article, on voit deja que les voisins presentent un interet evident du fait qu’ils mettent
au jour des liens qu’aucune ressource classique ne permettrait de detecter.

L’alg0rithme de segmentation Pour la segmentation des textes du corpus, nous avons opte
pour une approche lineaire, par fenetre glissante, a la maniere de (Hearst, 1997). Cette approche
n’est pas forcement la plus performante, mais nous l’avons preferee en raison de sa simplicite
d’implementation; en effet, nous ne poursuivons pas ici l’efﬁcacite, mais la comparaison entre
differentes ressources; l’important pour nous etait donc avant tout d’appliquer le meme algo-
rithme pour chaque ressource, quel que soit cet algorithme.

Notre unite de base est la phrase ; la fenetre d’observation que nous appliquons pour calculer les
scores de similarite est d’une taille de 6 unites (parametre conseille par (Hearst, 1997)). Ainsi,
a la ﬁn de chaque phrase, un score base sur les liens entretenus par deux blocs de trois phrases
est calcule (ﬁgure 4).

. . . | phrase k-2 | | phrasek-1 | | phrasek | | phrasek+1| | phrasek+2| | phrase k+3| . . .

I
Score(k-1)

Score(k)

Score(k+1)
FIGURE 4 — Representation du calcul du score avec fenetre glissante

Le score calcule est le suivant : S = log  Le nombre de liens Nl,-ens est le nombre
1671.8 110887. 68

de couples de mots juges similaires a cheval entre les deux blocs de trois phrases. Le nombre de

liens possibles N1,-ms possibles est le produit des nombres de mots pouvant etre lies dans chaque

bloc (c’est-a-dire des noms, adjectifs et verbes).

La courbe des scores calcules est ensuite lissee. Toutes les vallées sont reperees, et leurs pro-
fondeurs calculees. On appelle vallée un point de la courbe qui est entoure par des points de

Clémentine Adam, Francois Morlane-Hondere

valeurs plus élevées (c’est-a-dire un minimum local). Pour déterminer la profondeur d’une val-
lée, on remonte de part et d’autre du point considéré tant que l’on rencontre des valeurs plus
élevées ; la profondeur de la vallée est calculée en faisant la moyenne des deux différences cal-
culées (a gauche et a droite du point). Les vallées dont la profondeur dépasse l’écart-type a la
moyenne sont considérées comme correspondant aux ruptures du texte. Ces ruptures, situées
entre deux phrases, sont ramenées a la frontiere de paragraphe la plus proche, ce qui produit le
texte segmenté ﬁnal.

Décisions prises sur l’évaluation Evaluer un systeme de segmentation thématique est délicat.
De nombreux problemes sont soulevés, et peuvent grosso modo étre ramenés a deux questions :
(a) Quelle référence ? (b) Quel mesure d’évaluation ?

(a) Pour évaluer la segmentation automatique, il faut la comparer a une segmentation de refe-
rence. Certains font pour cela appel a des annotations manuelles, mais font généralement état
d’accords inter-annotateurs tres faibles. D’autres prennent le parti d’accoler bout a bout des
séquences appartenant a des textes différents; les ruptures thématiques sont alors les ruptures
entre textes. Cette position pose un probleme évident de circularité : on fabrique l’objet que
l’on postule. Pour cette expérience, nous avons décidé d’utiliser comme ruptures de référence
les positions des titres de sections.

(b) Les scores habituels de précision et de rappel ne sont pas adaptés pour évaluer un systeme
de segmentation thématique. En effet, ils ne permettent pas de rendre compte du fait qu’une
rupture proche de la rupture de référence est meilleure qu’une rupture éloignée. D’autres scores
ont été proposés, dont les plus usités sont les mesures Pk (Beeferman et al., 1999) et Wind0wDzﬁ°
(Pevzner & Hearst, 2002). La mesure Pk consiste a compter le nombre de fois ou deux mots
pris au hasard a une distance k sont dans le meme segment a la fois dans la référence et dans
l’hypothese. La mesure Wind0wDzﬁ‘ consiste a calculer la différence du nombre de ruptures
dans une fenétre glissante. Nous donnons ici nos résultats selon ces deux mesures.

Résultats Nous reportons dans le tableau 1 les résultats obtenus par l’algorithme de segmen-
tation appliqué au corpus décrit, selon les liens cohésifs pris en compte (répétition, synonymie
ou voisinage distributionnel). Les scores afﬁchés correspondent aux moyennes des scores ob-
tenus pour chaque texte. Il est a noter qu’avec les mesures Pk et Wind0wDi]fﬂ un score moins
élevé reﬂete de meilleures performances. Pour mettre en perspective les résultats présentés,
nous rapportons également les résultats obtenus avec des ruptures placées au hasard, le nombre
de ruptures de références étant approximativement 6 connu.

Liens pris en compte Pk WindowDiff
Hasard 0.436 0.452
Répétition 0.353 0.359
Synonymie 0.349 0.358
Voisinage 0.329 0.336

TABLE 1 — Performances de la segmentation thématique selon les liens pris en compte

Ces résultats sont corrects compte-tenu de la difﬁculté de la tache : chacune des approches
permet une segmentation signiﬁcativement meilleure que le hasard. Globalement, les résultats

6. Pour chaque texte, une Variation de :|:3 ruptures par rapport a la référence est autorisée.

Détection de la cohésion lexicale par voisinage distributionnel

observés avec les différents types de liens cohésifs sont assez proches. L’utilisation des liens
de voisinage semble justiﬁée, puisqu’elle apporte les meilleures performances dans cette ex-
périence, alors que les synonymes ne se démarquent que tres peu de l’approche basique par
répétitions. Il est donc conﬁrmé que les voisins permettent une détection plus ﬁne de la cohé-
sion lexicale, du moins selon l’étalon que nous avions choisi : la performance d’un systeme de
segmentation thématique.

5 Conclusions et perspectives

L’ objectif de cette étude était de montrer la pertinence du voisinage distributionnel pour la de-
tection de la cohésion lexicale. Nous avons a cette ﬁn impliqué les voisins recensés par notre
ressource dans un systeme de segmentation thématique. Les résultats obtenus montrent un ap-
port signiﬁcatif de la ressource mobilisée. Ainsi, nous avons vu qu’une ressource obtenue grace
a l’analyse distributionnelle présente des avantages que n’ont pas les ressources traditionnelles.
Cette expérience mériterait d’étre approfondie ; nous aimerions notamment comparer les voisins
avec une ressource plus similaire, comme par exemple avec des collocations ; il serait également
intéressant d’étudier les possibilités de combinaisons de ressources.

Comme nous l’avons indiqué en introduction, la segmentation thématique n’est pas pour nous
une ﬁn en soi. Si nous nous sommes ici donné pour but de repérer des zones cohésives, c’est
avant tout pour confronter différentes méthodes de détection des liens lexicaux. En effet, le
projet VOILADIS s’inscrit dans une démarche résolument axée vers une analyse du discours
qui Va au-dela du simple découpage thématique. En nous appuyant sur une ressource obtenue
grace a l’analyse distributionnelle, nous espérons mettre au point une méthode de détection des
liens de cohésion lexicale assez efﬁcace pour nous permettre de capter des particularités dans
les fonctionnements discursifs des textes, a l’instar des topic opening et topic closing que repere
(Hoey, 1991) en se basant sur la partie du texte vers laquelle pointent les liens cohésifs.

Références

BEEFERMAN D., BERGER A. & LAFFERTY J. (1997). Text segmentation using exponential
models. In Proceedings of the Second Conference on Empirical Methods in Natural Language
Processing, p. 35-46, Providence.

BEEFERMAN D., BERGER A. & LAFFERTY J. (1999). Statistical models for text segmenta-
tion. Mach. Leam., 34(1-3), 177-210.

BOLSHAKOV I. A. & GELBUKH A. (2001). Text segmentation into paragraphs based on local
text cohesion. In TSD ’0I .' Proceedings of the 4th International Conference on Text, Speech
and Dialogue, p. 158-166, Zelezna Ruda.

BOURIGAULT D. (2002). UPERY : un outil d’analyse distributionnelle étendue pour la
construction d’ontologies a partir de corpus. In Actes de la 9econfe’rence sur le Traitement
Automatique de la Langue Naturelle, Nancy.

BOURIGAULT D. (2007). Un analyseur syntaxique ope’rationnel .' SYNTEX. Habilitation a
diriger des recherches. Université Toulouse II - Le Mirail.

BOURIGAULT D. & GALY E. (2005). Analyse distributionnelle de corpus de langue générale
et synonymie. In 4”Joume’es de la linguistique de corpus, p. 163-174, Lorient.

Clémentine Adam, Francois Morlane-Hondere

BRUNN M., CHALI Y. & PINCHAK C. J. (2001). Text summarization using lexical chains. In
Proceedings of the Document Understanding Conference (DUC 2001), p. 135-140, Nouvelle
Orléans.

CALLAN J. P., CROFT W. B. & HARDING S. M. (1992). The inquery retrieval system. In
Proceedings of the Third International Conference on Database and Expert Systems Applica-
tions, p. 78-83.

CHOI F. Y. Y. (2000). Advances in domain independent linear text segmentation. In Procee-

dings of the ﬁrst conference on North American chapter of the Association for Computational
Linguistics, p. 26-33, San Francisco.

CHOI F. Y. Y., WIEMER-HASTINGS P. & MOORE J. (2001). Latent semantic analysis for

text segmentation. In Proceedings of the 2001 Conference on Empirical Methods in Natural
Language Processing, p. 109-117, Pittsburgh.

FABRE C. & BOURIGAULT D. (2006). Extraction de relations sémantiques entre noms et
verbes au-dela des liens morphologiques. In Actes de la I 3“confe’rence sur le Traitement Au-
tomatique de la Langue Naturelle, Louvain.

FERRET O. (2002). Segmenter et structurer thématiquement des textes par l’utilisation
conjointe de collocations et de la récurrence lexicale. In Actes de TALN 2002, p. 155-165,
Nancy.

HALLIDAY M. A. K. & HASAN R. (1976). Cohesion in English. Longman (Londres).
HEARST M. A. (1997). Texttiling : segmenting text into multi-paragraph subtopic passages.
Computational Linguistics, 23(1), 33-64.

HERNANDEZ N. (2004). Description et de’tection automatique de structures de textes. PhD
thesis, Université Paris-Sud.

HOEY M. (1991). Patterns of lexis in text. Oxford University Press (Oxford).

KOZIMA H. (1993). Text segmentation based on similarity between words. In Proceedings of
the 31 st annual meeting on Association for Computational Linguistics, p. 286-288, Columbus.

LIN D. (1998). An information-theoretic deﬁnition of similarity. In Proceedings of the I 5th
International Conference on Machine Learning, p. 296-304, Madison.

LIN M., NUNAMAKER JR. J. F., CHAU M. & CHEN H. (2004). Segmentation of lecture
videos based on text : a method combining multiple linguistic features. In Proceedings of the
37th Annual Hawaii International Conference on System Sciences, Hawaii.

MALIOUTOV I. & BARZILAY R. (2006). Minimum cut model for spoken lecture segmenta-
tion. In ACL-44 .' Proceedings of the 21 st International Conference on Computational Linguis-
tics and the 44th annual meeting of the Association for Computational Linguistics, p. 25-32,
Morristown, NJ, USA : Association for Computational Linguistics.

MORRIS J. & HIRST G. (2004). Non-classical lexical semantic relations. In Proceedings of
the HLT Workshop on Computational Lexical Semantics, p. 46-51, Boston.

PEVZNER L. & HEARST M. A. (2002). A critique and improvement of an evaluation metric
for text segmentation. Computational Linguistics, 28, 1-19.

PERY-WOODLEY & SCOTT, Eds. (2006). Discours et Document .' traitements automatiques.
Nume’ro the’matique, volume TAL 47(2).

