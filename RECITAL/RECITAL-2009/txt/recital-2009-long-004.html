<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
"http://www.w3.org/TR/html4/loose.dtd">
<html><head><title>Extraction de lexique dans un corpus sp&#233;cialis&#233; en chinois contemporain</title>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
</head>
<body>
<div style="page-break-before:always; page-break-after:always"><div><p>RECITAL 2009, Senlis, 24&#8211;26 juin 2009
</p>
<p>Extraction de lexique dans un corpus sp&#233;cialis&#233;
en chinois contemporain
</p>
<p>Ga&#235;l Patin1, 2
</p>
<p>(1) Er-Tim, Inalco, 75343 Paris
(2) Arisem, Thales, 91300 Massy
</p>
<p>gael.patin@inalco.fr
</p>
<p>R&#233;sum&#233;. La constitution de ressources lexicales est une t&#226;che cruciale pour l&#8217;am&#233;lioration
des performances des syst&#232;mes de recherche d&#8217;information. Cet article pr&#233;sente une m&#233;thode
d&#8217;extraction d&#8217;unit&#233;s lexicales en chinois contemporain dans un corpus sp&#233;cialis&#233; non-annot&#233;
et non-segment&#233;. Cette m&#233;thode se base sur une construction incr&#233;mentale de l&#8217;unit&#233; lexicale
orient&#233;e par une mesure d&#8217;association. Elle se distingue des travaux pr&#233;c&#233;dents par une approche
linguistique non-supervis&#233;e assist&#233;e par les statistiques. Les r&#233;sultats de l&#8217;extraction, &#233;valu&#233;s sur
un &#233;chantillon al&#233;atoire du corpus de travail, sont honorables avec des scores de pr&#233;cision et de
rappel respectivement de 52,6 % et 53,7 %.
</p>
<p>Abstract. Building lexical resources is a vital task in improving the efficiency of infor-
mation retrieval systems. This article introduces a Chinese lexical unit extraction method for
untagged specialized corpora. This method is based on an incremental process driven by an
association score. This work features an unsupervised statistically aided linguistic approach.
The extraction results &#8212; evaluated on a random sample of the working corpus &#8212; show decent
precision and recall which amount respectively to 52.6% and 53.7%.
</p>
<p>Mots-cl&#233;s : corpus sp&#233;cialis&#233;, unit&#233; lexicale, lexie, extraction de lexique, chinois.
</p>
<p>Keywords: specialized corpus, lexical unit, lexicon extraction, Chinese.
</p>
<p>1 Introduction
</p>
<p>Les lexiques sont des ressources indispensables aux syst&#232;mes de recherche d&#8217;information. Ils
permettent d&#8217;am&#233;liorer notablement les r&#233;sultats des proc&#233;d&#233;s automatiques d&#8217;analyse linguis-
tique &#8212; &#233;tiquetage morpho-syntaxique, interpr&#233;tation s&#233;mantique ou indexation &#8212; dans des
domaines particuliers. Or la constitution de lexiques est confront&#233;e &#224; deux types de difficul-
t&#233;s : les unes d&#8217;ordre pragmatique, telles que le co&#251;t de leur &#233;laboration ou leur r&#233;utilisabilit&#233;,
sont d&#8217;une grande importance pour la mise en &#339;uvre industrielle ; les autres d&#8217;ordre th&#233;orique,
comme la d&#233;finition de l&#8217;unit&#233; lexicale dans diff&#233;rentes langues ou la caract&#233;risation des par-
ticularit&#233;s lexicales d&#8217;un corpus sp&#233;cialis&#233;, sont primordiales pour la pertinence et la validit&#233;
des r&#233;sultats. Cette confrontation entre int&#233;r&#234;t &#233;conomique et qualitatif est une probl&#233;matique
r&#233;currente dans le milieu de l&#8217;entreprise. La recherche scientifique appliqu&#233;e doit &#234;tre &#224; m&#234;me
de proposer des solutions pour r&#233;pondre &#224; cette double exigence. Cette &#233;tude propose un &#233;l&#233;-
ment de r&#233;ponse au probl&#232;me de l&#8217;identification de lexique dans un corpus sp&#233;cialis&#233; en chinois
contemporain via un syst&#232;me de classement de lexies (unit&#233;s lexicales) candidates. Cette &#233;tude</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Ga&#235;l Patin
</p>
<p>s&#8217;int&#233;resse en particulier au cas du chinois contemporain, langue pour laquelle nous ne dispo-
sons que de peu de ressources lexicales.
</p>
<p>Dans la suite de cet article nous pr&#233;sentons en premier lieu les travaux similaires &#224; notre m&#233;thode
(section 2), puis nous introduisons une analyse linguistique de la notion de lexie en chinois
contemporain &#233;crit (section 3). Apr&#232;s une description de notre m&#233;thodologie (section 4), nous
pr&#233;sentons nos r&#233;sultats d&#8217;exp&#233;rience (section 5). Enfin, nous concluons par une discussion et
une comparaison des r&#233;sultats (section 6).
</p>
<p>2 Travaux similaires
</p>
<p>L&#8217;extraction de lexique est un probl&#232;me de premi&#232;re importance pour le traitement automa-
tique du chinois contemporain. En effet, le processus de segmentation en mots est g&#233;n&#233;ralement
consid&#233;r&#233; comme &#233;tant l&#8217;&#233;tape initiale pour le traitement de textes en chinois. Or le vocabulaire
inconnu est un facteur d&#8217;erreur significatif pour la segmentation en mots en chinois (Sproat &amp;
Emerson, 2003). De nombreuses approches s&#8217;appuient sur les r&#233;sultats d&#8217;un processus de seg-
mentation en mots (Wu &amp; Jiang, 2000; Li et al., 2004) ou d&#8217;annotations morpho-syntaxiques
(Piao et al., 2006) entra&#238;n&#233; statistiquement pour extraire du lexique inconnu. Ces techniques
obtiennent de bons r&#233;sultats mais l&#8217;application de ces proc&#233;d&#233;s sur des corpus de domaine et
genre diff&#233;rents de celui du corpus d&#8217;entra&#238;nement est peu efficace. La constitution de corpus
annot&#233;s &#233;tant tr&#232;s co&#251;teuse, nous renon&#231;ons &#224; ce type de m&#233;thode.
</p>
<p>D&#8217;autres approches s&#8217;int&#232;grent dans un syst&#232;me de segmentation en mots et visent &#224; am&#233;liorer
son efficacit&#233;. Dans leur article, Fung et Wu (1994) introduisent une m&#233;thode de s&#233;lection de
segments via une mesure d&#8217;information mutuelle mais leur m&#233;thode, test&#233;e sur un corpus sp&#233;-
cialis&#233;, souffre d&#8217;un faible score de rappel. Chang et Su (1997) pr&#233;sentent un processus it&#233;ratif
non-supervis&#233; d&#8217;extraction de lexique orient&#233; par la qualit&#233; de la segmentation obtenue avec le
lexique ainsi d&#233;couvert. Leur approche bien qu&#8217;efficace impose une restriction arbitraire de la
longueur des lexies candidates (quadrigrammes). Feng et al. (2004) introduisent une m&#233;thode
simple bas&#233;e sur une mesure de variation du contexte avec des r&#233;sultats tr&#232;s convaincants. Cer-
tains travaux utilisent des techniques d&#8217;extraction originales, comme la recherche de r&#233;p&#233;titions
caract&#233;ristiques dans la structure interne des lexies sp&#233;cialis&#233;es (Nakagawa et al., 2004) ou le
calcul des segments r&#233;p&#233;t&#233;s (Yang &amp; Li, 2002), mais leurs &#233;valuations ne font pas mention du
score de rappel de leur m&#233;thode. Enfin Sun et al. (1998) proposent un syst&#232;me de segmenta-
tion en mots sans ressources lexicales bas&#233; sur le score d&#8217;association mutuelle. Notre approche
s&#8217;inspire de cette m&#233;thode en proposant une construction incr&#233;mentale des lexies guid&#233;e par une
mesure d&#8217;association sur un corpus sp&#233;cialis&#233; non-annot&#233;.
</p>
<p>Il est cependant difficile de comparer rigoureusement nos travaux &#224; d&#8217;autres et ce pour plu-
sieurs raisons : la majeure partie des exp&#233;rimentations utilise des corpus constitu&#233;s de d&#233;p&#234;ches
d&#8217;agence de presse. Seuls (Fung &amp; Wu, 1994) utilisent un corpus sp&#233;cialis&#233; (comptes rendus
de conseil l&#233;gislatif). Par ailleurs, les crit&#232;res de s&#233;lection linguistique des lexies valides ne
sont pas d&#233;crits pr&#233;cis&#233;ment. D&#8217;autres travaux restreignent arbitrairement la longueur des lexies
recherch&#233;es, comme celle de Chang et Su (1997) et de Feng et al. (2004), ou fixent un seuil
de fr&#233;quence pour la s&#233;lection des lexies (Fung &amp; Wu, 1994). Enfin certains articles ne men-
tionnent pas de score de rappel (Yang &amp; Li, 2002; Nakagawa et al., 2004). Seules deux &#233;tudes
ont des conditions d&#8217;exp&#233;rimentation proches des n&#244;tres : la m&#233;thode de (Fung &amp; Wu, 1994) et
la m&#233;thode de (Feng et al., 2004).</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Extraction de lexique dans un corpus sp&#233;cialis&#233; en chinois contemporain
</p>
<p>3 D&#233;finition de la lexie en chinois contemporain
</p>
<p>Notre &#233;tude met l&#8217;accent sur l&#8217;importance de bien comprendre et analyser les ph&#233;nom&#232;nes lin-
guistiques recherch&#233;s pour pouvoir appliquer de mani&#232;re efficace les outils technologiques &#224;
notre disposition. Les th&#233;ories lexicologiques sont diverses et vari&#233;es, nous n&#8217;avons pas la pr&#233;-
tention d&#8217;affirmer que notre orientation est la plus juste, nous souhaitons seulement nous doter
d&#8217;un cadre th&#233;orique bien fond&#233; afin de ne pas nous contenter de simples intuitions sur la langue
pour produire un traitement automatique. Dans cet article, nous nous appuyons sur les th&#233;ories
de Mel&#8217;cuk (1993) et Polgu&#232;re (2003) appliqu&#233;es par Nguyen (2006) pour le cas du chinois
contemporain, m&#234;me si quelques approximations ou alt&#233;rations dues aux particularit&#233;s de la
langue chinoise peuvent appara&#238;tre dans notre pr&#233;sentation. Notons que les d&#233;finitions intro-
duites ne sont valides que dans le cadre de l&#8217;&#233;tude synchronique du chinois contemporain &#233;crit.
</p>
<p>La lexie (ou unit&#233; lexicale) est l&#8217;unit&#233; de description du lexique. Une lexie peut &#234;tre issue d&#8217;un
processus de formation morphologique, on parle alors de lex&#232;me, ou syntaxique, on parle alors
de locution. En chinois, certaines particularit&#233;s morpho-syntaxiques et typologiques rendent dif-
ficile la d&#233;tection automatique de lexies dans les textes. En effet, la morphologie composition-
nelle riche, souvent similaire &#224; la syntaxe, augmente combinatoirement le nombre de candidats
potentiels ; alors que le nombre r&#233;duit d&#8217;indices d&#8217;identification des lexies &#8212; d&#251; &#224; la morpholo-
gie flexionnelle pauvre, la raret&#233; des morph&#232;mes grammaticaux et l&#8217;absence de mots outils dans
la construction des lexies &#8212; limite la s&#233;lection des candidats. Enfin, &#224; l&#8217;&#233;crit, les textes sont
d&#233;pourvus d&#8217;espaces et la plupart des graphies ont individuellement un signifi&#233; lexical (m&#234;me
si elles ne sont pas n&#233;cessairement autonomes), ce qui restreint &#233;galement les indices d&#8217;auto-
nomie des signes. La lexie sp&#233;cialis&#233;e est une lexie qui acquiert un sens particulier &#224; travers
une pratique langagi&#232;re au sein d&#8217;un groupe social dans un domaine sp&#233;cifique. Il peut s&#8217;agir
de formes communes dont le sens habituel est d&#233;riv&#233; ou modifi&#233;, ou bien de lexies exclusive-
ment utilis&#233;es par les membres d&#8217;un groupe. Cette d&#233;finition rejoint les postulats &#233;nonc&#233;s par
L&#8217;homme et Polgu&#232;re (2008) dans leur description du terme .
</p>
<p>Une graphie est l&#8217;unit&#233; orthographique autonome minimale en chinois &#233;crit. La graphie cor-
respond approximativement &#224; la notion de &#171; caract&#232;re chinois &#187;. Les glyphes suivants sont des
exemples de graphies : &#30693;, &#20445;, &#38505;, &#33889;, &#33796;, &#20080;. Un morphe (not&#233; |M |) est un signe linguis-
tique au signifi&#233; &#233;l&#233;mentaire dont le signifiant est repr&#233;sentable par une suite d&#8217;une ou plusieurs
graphies. L&#8217;&#233;l&#233;mentarit&#233; du signifi&#233; exprime que le morphe ne peut &#234;tre le r&#233;sultat d&#8217;une com-
binaison d&#8217;autres morphes1. Les suites de graphies suivantes sont des morphes car associ&#233;es &#224;
un signifi&#233; et non d&#233;composables en morphes : |&#23551;| &#8216;vie&#8217;, |&#33889;&#33796;| &#8216;raisin&#8217;, |&#38463;&#21496;&#21305;&#26519;| &#8216;aspiri-
ne&#8217;, |&#20445;| &#8216;prot&#233;ger&#8217;, |&#38505;| &#8216;danger&#8217;, |&#20080;| &#8216;acheter&#8217;. En revanche, la graphie&#33796;&#8212; n&#8217;ayant aucun
signifi&#233; associ&#233; &#8212; n&#8217;est pas un morphe.
</p>
<p>Un mot-forme (not&#233; (M )) est une s&#233;quence autonome et ins&#233;cable de morphes. L&#8217;autonomie
implique que le mot-forme peut &#234;tre &#233;nonc&#233; isol&#233;ment et prendre place dans un paradigme syn-
taxique. L&#8217;ins&#233;cabilit&#233; exprime que la s&#233;paration des morphes entra&#238;ne la perte de leur relation
lexicale. Le lex&#232;me (not&#233; (( L ))) est un ensemble de mots-formes fl&#233;chis au m&#234;me signifi&#233; lexi-
cal. Un syntagme (not&#233; [ S ]) est une combinaison syntaxique de mots-formes dont les compo-
santes ont un certain degr&#233; de libert&#233;. Une locution (not&#233;e [[ L ]]) est un ensemble de syntagmes
lexicalis&#233;s fl&#233;chis au m&#234;me signifi&#233; lexical.
</p>
<p>1La d&#233;finition donn&#233;e par Polgu&#232;re parle plut&#244;t de non repr&#233;sentabilit&#233; en termes de signes, or ceci n&#8217;est valide
qu&#8217;en chinois oral. En effet, &#224; l&#8217;&#233;crit, l&#8217;analyse grammatologique des sinogrammes explique comment le sens du
sinogramme &#12296;&#26126;&#8596;&#8216;clair&#8217;&#12297; est repr&#233;sent&#233; en termes de composantes graphiques &#12296;&#26085;&#8596;&#8216;soleil&#8217;&#12297; et &#12296;&#26376;&#8596;&#8216;lune&#8217;&#12297;.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Ga&#235;l Patin
</p>
<p>Le terme lexie regroupe les deux concepts de lex&#232;me et locution, c&#8217;est l&#8217;unit&#233; d&#8217;&#233;tude principale
de la lexicologie. Les exemples du tableau (1) ci-dessous pr&#233;sentent des lex&#232;mes et locutions
avec leurs mots-formes et syntagmes associ&#233;s. Cette &#233;tude propose une m&#233;thode pour identifier
mots-formes et syntagmes lexicalis&#233;s et, par extension, identifier lex&#232;mes et locutions. Notre
m&#233;thode &#233;tant appliqu&#233; sur un corpus sp&#233;cialis&#233;, nous nous attendons &#224; extraire un ensemble de
lexies2 dont une part significative devraient &#234;tre des lexies sp&#233;cialis&#233;es.
</p>
<p>Lex&#232;me mots-formes Locution syntagmes
((
</p>
<p>aspirine
</p>
<p>&#38463;&#21496;&#21305;&#26519; )) (
aspirine
</p>
<p>|&#38463;&#21496;&#21305;&#26519;|) [[
tirer (&#224; feu)
</p>
<p>&#24320;&#26538; ]] [
tirer (&#224; feu)
</p>
<p>(&#24320;)(&#26538;)] [
tirer (&#224; feu) /accompli/
</p>
<p>(&#24320;&#20102;)(&#26538;) ]
</p>
<p>((
prendre
</p>
<p>&#25343; )) (
prendre /accompli/
</p>
<p>|&#25343;|&#20102;| ) (
prendre /duratif/
</p>
<p>|&#25343;|&#30528;| ) (
prendre /expr&#233;rience/
</p>
<p>|&#25343;|&#36807;| ) [[
&#234;tre jaloux
</p>
<p>&#21507;&#37259; ]] [
&#234;tre jaloux
</p>
<p>(&#21507;)(&#37259;)]
</p>
<p>((
assurance
</p>
<p>&#20445;&#38505; )) (
assurance
</p>
<p>|&#20445;|&#38505;|) [[
compagnie d&#8217;assurance
</p>
<p>&#20445;&#38505;&#20844;&#21496; ]] [
compagnie d&#8217;assurance
</p>
<p>(&#20445;&#38505;)(&#20844;&#21496;) ]
</p>
<p>TAB. 1 &#8211; Exemples de lexies.
</p>
<p>4 M&#233;thodologie
</p>
<p>Notre m&#233;thode s&#8217;appuie sur la description de la lexie en chinois contemporain pour extraire le
lexique inconnu d&#8217;un corpus sp&#233;cialis&#233;. Nous ne traitons pas le cas des lexies mono-morph&#232;-
miques, ni des lexies d&#233;j&#224; connues avec un statut particulier dans le corpus qui feront l&#8217;objet
d&#8217;une &#233;tude ult&#233;rieure. Notre id&#233;e est de reproduire le processus morpho-syntaxique de forma-
tion des mots-formes et syntagmes en partant de la notion de morphe sans proc&#233;der au pr&#233;alable
&#224; une segmentation en mots, puis de d&#233;tecter les mots-formes et syntagmes lexicalis&#233;s gr&#226;ce &#224;
une mesure d&#8217;association. Il s&#8217;agit concr&#232;tement de parcourir le corpus en associant incr&#233;men-
talement morphes et mot-formes, puis de s&#233;lectionner les combinaisons stables, r&#233;currentes et
ind&#233;pendantes sur l&#8217;ensemble du corpus. Ici les statistiques et ressources lexicales &#224; notre dis-
position servent &#224; d&#233;cider quels sont les &#233;l&#233;ments &#224; associer &#224; chaque &#233;tape de notre processus.
L&#8217;essence de notre m&#233;thode est le principe de construction incr&#233;mentale de la lexie, davantage
que le score d&#8217;association utilis&#233;.
</p>
<p>Dans la section pr&#233;c&#233;dente, nous avons expliqu&#233; que l&#8217;unit&#233; minimale porteuse de sens est le
morphe et que la construction lexicologique chinoise s&#8217;appuie sur l&#8217;association de composantes
lexicales. C&#8217;est pourquoi nous initialisons notre processus d&#8217;identification par une repr&#233;senta-
tion du texte en morphes (P0). Nous avons &#233;galement vu que la production des lexies en chinois
s&#8217;appuie autant sur la syntaxe que sur la morphologie. Nous posons un postulat (Pn) pour d&#233;crire
les ph&#233;nom&#232;nes de production morphologique et syntaxique, sachant que nous nous int&#233;ressons
uniquement aux syntagmes pr&#233;sents en s&#233;quence continue dans le corpus :
</p>
<p>Postulat d&#8217;initialisation (P0) : Le morphe est l&#8217;unit&#233; de construction &#233;l&#233;mentaire des lexies
en chinois contemporain.
</p>
<p>Postulat de r&#233;cursion (Pn) : Les mots-formes du chinois moderne sont une s&#233;quence de deux
morphes ou mot-formes. Les syntagmes sont une combinaison syntaxique de mots-formes.
</p>
<p>Ces deux postulats permettent de poser les bases de notre m&#233;thode en d&#233;finissant l&#8217;unit&#233; de
recherche initiale et en d&#233;crivant le processus de construction des lexies.
</p>
<p>2Dans la suite de l&#8217;article, le terme &#171; lexie &#187; sera utilis&#233; pour d&#233;signer un &#171; mot-forme ou syntagme lexicalis&#233;
repr&#233;sentant d&#8217;une lexie &#187; lorsque nous parlerons des lexies d&#233;couvertes dans le texte.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Extraction de lexique dans un corpus sp&#233;cialis&#233; en chinois contemporain
</p>
<p>4.1 Description des corpus
</p>
<p>Le corpus de r&#233;f&#233;rence contient 2 millions de graphies. Il est compos&#233; de deux corpus de chi-
nois contemporain : The Lancaster Corpus of Mandarin Chinese (McEnery &amp; Xiao, 2004) et
PFR China Daily corpus (Beijing University ICL, 2001) tous deux annot&#233;s selon la norme de
l&#8217;Universit&#233; de Beijing3. Il nous a servi &#224; recueillir toutes les informations lexicales n&#233;cessaires
&#224; notre m&#233;thode. En nous basant sur ces annotations, nous avons extrait un lexique de r&#233;f&#233;rence
d&#8217;environ 75 000 lexies, une liste des mots-vides et une liste des bigrammes syntaxiques (voir
4.2.3). Le corpus de test est un corpus de documents commerciaux de compagnies d&#8217;assurance
publi&#233;s sur leur site internet. Il est compos&#233; de dix millions de caract&#232;res. Il a &#233;t&#233; recueilli auto-
matiquement &#224; partir d&#8217;une liste d&#8217;adresses de sites de compagnies d&#8217;assurance chinoises. Les
textes sur internet contenant de nombreuses r&#233;p&#233;titions (par exemple dans les menus), tous les
paragraphes doublons ont &#233;t&#233; &#233;limin&#233;s afin de ne pas perturber les calculs statistiques.
</p>
<p>4.2 Algorithme
</p>
<p>Les lexies candidates sont construites de mani&#232;re incr&#233;mentale en associant progressivement
des couples de grains. Un grain est constitu&#233; d&#8217;une s&#233;quence de morphes pouvant repr&#233;senter
un morphe, un mot-forme, un syntagme ou une partie de syntagme. Le morphe est le niveau
de granularit&#233; initiale. Nous d&#233;duisons ensuite le niveau de granularit&#233; suivant en associant des
couples de grains. Nous utilisons une mesure d&#8217;association pour nous orienter dans le choix des
couples de grains &#224; associer et la s&#233;lection des lexies candidates. Le score d&#8217;association entre les
grains est recalcul&#233; &#224; chaque changement de granularit&#233; afin d&#8217;int&#233;grer les d&#233;ductions obtenues
lors des &#233;tapes pr&#233;c&#233;dentes. L&#8217;algorithme s&#8217;arr&#234;te lorsqu&#8217;un plafond de nombre d&#8217;associations
maximal est atteint4. L&#8217;&#233;tape finale consiste &#224; s&#233;lectionner et ordonner les lexies candidates en
fonction de leur pertinence.
</p>
<p>Algorithme d&#8217;extraction de lexies candidates
S&#233;lectionner des grains initiaux (P0)
</p>
<p>Faire
</p>
<p>| Calculer le score d&#8217;association des grains
</p>
<p>| S&#233;lectionner les grains de niveau sup&#233;rieur (Pn)
</p>
<p>Tant que le plafond d&#8217;association maximal n&#8217;est pas atteint
</p>
<p>S&#233;lection et ordonnancement des lexies candidates
</p>
<p>4.2.1 Grains initiaux : morphes
</p>
<p>Nous consid&#233;rons trois types de morphes : les morphes unigrammes, les morphes bigrammes
(&#34676;&#34678; H&#218;DI&#201; &#8216;papillon&#8217;) et les morphes translitt&#233;r&#233;s (&#24847;&#22823;&#21033; Y&#204;D&#192;L&#204; &#8216;Italie&#8217;). Un morphe uni-
gramme est une graphie &#224; laquelle est associ&#233;e un sens. Nous consid&#233;rons que toutes les graphies
sont potentiellement des morphes &#224; l&#8217;exception de celle incluses dans un morphe bigramme ou
translitt&#233;r&#233;. Un morphe translitt&#233;r&#233; est un calque phon&#233;tique d&#8217;un mot &#233;tranger utilisant la pro-
nonciation de graphies. Les graphies utilis&#233;es pour le calque phon&#233;tique sont pour la plupart
r&#233;currentes et il est possible de les d&#233;tecter avec des outils statistiques5. Un morphe bigramme
</p>
<p>3http://icl.pku.edu.cn/icl_groups/corpus/coprus-annotation.htm
4Estim&#233; &#224; 5 associations selon nos observations sur les lexies les plus grandes dans le corpus de r&#233;f&#233;rence.
5Nous avons utilis&#233; pour cela l&#8217;impl&#233;mentation CRF++ de la th&#233;orie des Conditional Random Fields.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Ga&#235;l Patin
</p>
<p>est un couple de graphies ne prenant sens que dans la mesure o&#249; elles sont associ&#233;es (&#34676; H&#218;
&#8216;&#8709;&#8217;). Il se peut que l&#8217;une des graphies du couple puisse exister en tant que morphe porteur du
m&#234;me sens (&#34678; DI&#201; &#8216;papillon&#8217;), cependant il n&#8217;est jamais autonome (&#34678;&#27891; DI&#201;YO&#780;NG &#8216;nage pa-
pillon&#8217;). Leur d&#233;tection est faite &#224; l&#8217;aide de leurs particularit&#233;s statistiques6. Lorsqu&#8217;il y a une
ambigu&#239;t&#233; entre diff&#233;rents morphes, le morphe englobant est toujours pr&#233;f&#233;r&#233;.
</p>
<p>4.2.2 Calcul du score d&#8217;association
</p>
<p>A chaque &#233;tape de l&#8217;algorithme, le score d&#8217;association de tous les couples de grains est calcul&#233;
&#224; l&#8217;aide du score Poisson-Stirling7 (Quasthoff &amp; Wolff, 2002) d&#233;fini ainsi :
</p>
<p>PS(a, b) =
k &#183; (log k &#8722; log &#955;&#8722; 1)
</p>
<p>log n
</p>
<p>o&#249; &#955; = n &#183; pa &#183; pb avec pa et pb la probabilit&#233; d&#8217;apparition respective de a et b, k le nombre
d&#8217;occurrences du couple ab et n le nombre total de couples. Ce score refl&#232;te la tendance de a et
b &#224; se retrouver plus souvent c&#244;te &#224; c&#244;te que ne le voudrait une r&#233;partition al&#233;atoire du corpus.
Plus le score est &#233;lev&#233;, plus l&#8217;association entre a et b est significative, un score nul indiquant la
neutralit&#233;. Un lien a &#233;t&#233; mis en &#233;vidence entre les mesures d&#8217;association et certains ph&#233;nom&#232;nes
linguistiques tels que les relations s&#233;mantiques ou syntaxiques (Church &amp; Hanks, 1997).
</p>
<p>4.2.3 S&#233;lection du niveau de granularit&#233; sup&#233;rieur : mots-formes ou syntagmes
</p>
<p>La s&#233;lection du niveau de granularit&#233; sup&#233;rieur consiste &#224; choisir des couples de grains &#224; asso-
cier pour identifier des lexies candidates. L&#8217;id&#233;e directrice de la s&#233;lection est que si un couple
de signes a un score d&#8217;association &#233;lev&#233; au sein d&#8217;un corpus, alors il existe une relation particu-
li&#232;re entre eux. Cette relation peut &#234;tre d&#8217;ordre lexical, syntaxique (en chinois un classificateur
est souvent associ&#233; &#224; un num&#233;ral) ou simplement refl&#233;ter des relations pr&#233;dicat-argument com-
munes dans le corpus de travail (&#21103;&#24635;&#32479;&#25253;&#36947; &#8216;le vice-pr&#233;sident a d&#233;clar&#233;&#8217;). Cependant les
relations d&#8217;ordre contextuel et syntaxique non lexicalis&#233;s ne nous int&#233;ressent pas et la faible re-
pr&#233;sentativit&#233; de certains signes peut &#233;galement induire en erreur le processus statistique. Pour
&#233;carter les associations non-souhaitables, des crit&#232;res de filtrage sont ajout&#233;s pour la s&#233;lection
des couples. Notre m&#233;thode travaille sur l&#8217;ensemble des segments du corpus. Un segment est
une suite ininterrompue de graphies du corpus situ&#233; par exemple entre des signes de ponctua-
tion, des caract&#232;res alphab&#233;tiques ou le d&#233;but et la fin de ligne.
</p>
<p>Soit Si l&#8217;ensemble des segments de sinogrammes du corpus &#224; l&#8217;it&#233;ration i. Soit le segment
sni &#8712; Si avec n le num&#233;ro de segment, compos&#233; d&#8217;une suite m1,m2 . . .m|sni | de grains telle que&#8704;mj &#8712; sni , mj &#8712; Graphie+, comme dans l&#8217;exemple (1) ci-dessous :
</p>
<p>(1) S0 = {&#183; &#183; &#183; , (&#24847;&#183;&#22806;&#183;&#36523;&#183;&#25925;&#183;&#20445;&#183;&#38505;&#183;&#37329; ), &#183; &#183; &#183; }, sn0 = (&#24847;&#183;&#22806;&#183;&#36523;&#183;&#25925;&#183;&#20445;&#183;&#38505;&#183;&#37329; )
avec m1 =&#24847; et m7 =&#37329;.
</p>
<p>Le passage d&#8217;une granularit&#233; courante &#224; une granularit&#233; sup&#233;rieure se fait en appliquant la r&#232;gle
suivante sur l&#8217;ensemble du segment.
</p>
<p>6Fort score d&#8217;association, exclusivit&#233; d&#8217;association d&#8217;un ou des composants.
7Nous avons &#233;galement test&#233; l&#8217;information mutuelle avec des r&#233;sultats &#233;quivalents modulo un gain de rappel et
</p>
<p>une perte de pr&#233;cision.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Extraction de lexique dans un corpus sp&#233;cialis&#233; en chinois contemporain
</p>
<p>&#8704; sni &#8704;mxmy &#8712; sni si dmx &#8596; myesni
alors mxmy est remplac&#233; par mxy = mx &#183;my dans sni+1
</p>
<p>o&#249; dmx &#8596; myesni signifie que &#171; mx et my sont associ&#233;s dans le contexte du segment n &#224; l&#8217;&#233;tape
i &#187;. Par exemple, si d&#24847;&#8596;&#22806;esn0 d&#36523;&#8596;&#25925;esn0 d&#20445;&#8596;&#38505;esn0 alors sn1 = (&#24847;&#22806;&#183;&#36523;&#25925;&#183;&#20445;&#38505;&#183;&#37329; ). La
proposition dmx &#8596; myesni n&#8217;est vraie que si les conditions suivantes sont remplies :
</p>
<p>1. Association interne : PS(mx,my) &gt; 0
</p>
<p>2. Libert&#233; droite : PS(mx,my) &gt; PS(my,my+1)
</p>
<p>3. Libert&#233; gauche : PS(mx,my) &gt; PS(mx&#8722;1,mx)
</p>
<p>et que les conditions de filtrage suivantes sont valides pour le couple mxmy :
</p>
<p>1. Filtre syntaxique : Le couple n&#8217;appartient pas &#224; la liste des collocations syntaxiques.
</p>
<p>2. Filtre score/fr&#233;quence : Le couple a une fr&#233;quence sup&#233;rieure &#224; 10 (garantit la repr&#233;sen-
tativit&#233; statistique) ou un score d&#8217;association sup&#233;rieur &#224; 1 (&#233;limination des cas tangents).
</p>
<p>3. Absence de mot vide : Aucun grain n&#8217;est un mot vide sauf si le couple est dans le lexique.
</p>
<p>La liste des collocations syntaxiques et des mots vides a &#233;t&#233; obtenue &#224; partir de notre corpus de
r&#233;f&#233;rence. Nous appelons collocation syntaxique tout bigramme ayant un score d&#8217;association
sup&#233;rieur &#224; z&#233;ro dans le corpus de r&#233;f&#233;rence mais n&#8217;&#233;tant pas r&#233;pertori&#233; comme lexie dans ce
corpus. La liste des mots vides est compos&#233;e de l&#8217;ensemble des particules modales, pr&#233;positions,
conjonctions de coordination rencontr&#233;es dans les textes &#233;crits du corpus de r&#233;f&#233;rence.
</p>
<p>4.2.4 S&#233;lection et ordonnancement des lexies candidates
</p>
<p>Les lexies s&#233;lectionn&#233;es sont l&#8217;ensemble des associations valides (dmx &#8596; myesni ) trouv&#233;es &#224;
chaque &#233;tape de l&#8217;algorithme qui ne se trouvent pas dans notre lexique. Des seuils de fr&#233;quence
et score d&#8217;association8 sont &#233;galement utilis&#233;s pour &#233;liminer les candidats les moins probables
en fin de traitement. Nous consid&#233;rons qu&#8217;une lexie est moins pertinente si elle est r&#233;guli&#232;rement
contenue dans une autre ; nous ajustons donc sa fr&#233;quence dans le calcul final de son score
d&#8217;association en lui soustrayant le nombre d&#8217;occurrences incluses du nombre d&#8217;occurrences
total. Les candidats sont ensuite ordonn&#233;s par ordre d&#233;croissant de score d&#8217;association ajust&#233;.
</p>
<p>4.3 Protocole d&#8217;&#233;valuation
</p>
<p>Dans la mesure o&#249; le corpus de travail est brut, sans aucune annotation et tr&#232;s volumineux (donc
co&#251;teux &#224; annoter), nous utilisons des &#233;chantillons pour &#233;valuer les r&#233;sultats. Les mesures qui
nous int&#233;ressent sont les scores de pr&#233;cision et de rappel. La pr&#233;cision est d&#233;finie comme le
pourcentage de lexies correctes dans la liste de lexies candidates extraites, et le rappel le pour-
centage de lexies inconnues d&#233;couvertes. Nous estimons le score de pr&#233;cision en s&#233;lectionnant
al&#233;atoirement un &#233;chantillon de 1 000 lexies candidates dans la liste produite par notre syst&#232;me.
L&#8217;estimation du score de rappel est obtenue en rep&#233;rant les lexies inconnues pr&#233;sentes dans une
</p>
<p>8Diff&#233;rents du filtre score/fr&#233;quence de la sous-section 4.2.3.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Ga&#235;l Patin
</p>
<p>s&#233;lection al&#233;atoire de 250 paragraphes dans le corpus de travail, dans lesquels 287 lexies in-
connues distinctes ont &#233;t&#233; identifi&#233;es. L&#8217;identification et la validation des lexies du corpus ont
&#233;t&#233; effectu&#233;es par un locuteur natif du chinois et linguiste sensibilis&#233; &#224; notre probl&#233;matique en
appliquant la d&#233;finition de la lexie donn&#233;e en section 3. Nous attendons que notre syst&#232;me pro-
duise une liste de lexies inconnues et nous souhaitons que la distribution de lexies valides soit
plus dense en haut de liste qu&#8217;en fin de liste.
</p>
<p>5 R&#233;sultats
</p>
<p>Nous avons effectu&#233; une exp&#233;rimentation (tableau 2) en s&#233;lectionnant les lexies candidates de
fr&#233;quence sup&#233;rieure &#224; 8 et de score d&#8217;association sup&#233;rieur &#224; 10 (voir 4.2.4). Notre m&#233;thode
extrait apr&#232;s cette s&#233;lection une liste de 26 388 lexies candidates parmi lesquelles on estime
&#224; 13 880 le nombre de lexies valides (extraits en tableau 3). L&#8217;ensemble de ces lexies valides
couvre 53,7 % des lexies de l&#8217;&#233;chantillon du corpus de travail. La pr&#233;cision des r&#233;sultats d&#233;croit
avec l&#8217;augmentation des centiles consid&#233;r&#233;s mais reste au-dessus de 50 %. La majorit&#233; des
erreurs (60,7 %) sont des syntagmes non-lexicalis&#233;s, c&#8217;est-&#224;-dire des combinaisons syntaxiques
de mots-formes n&#8217;ayant pas de contenu lexical particulier. Ces syntagmes peuvent cependant
contenir des lexies inconnues pr&#233;sentes dans les candidats valides ou non. Une petite partie des
erreurs (6.5 %) sont des collocations (syntagmes &#224; l&#8217;usage contraint). Les collocations ne sont
pas pertinentes pour les applications d&#8217;analyse de la langue, nous ne les avons donc pas incluses
dans les r&#233;sultats valides. Notons que 25 % des erreurs contiennent des lexies d&#233;j&#224; contenues
dans la liste des lexies d&#233;couvertes. Enfin, il est important de remarquer que 92 % des lexies non
d&#233;couvertes ont moins de 8 occurrences dans le corpus de travail. Ceci montre que les lexies
peu fr&#233;quentes repr&#233;sentent une part significative des lexies du corpus. La diminution du seuil
de fr&#233;quence des candidats d&#233;gradant de fa&#231;on significative le score de pr&#233;cision, notre m&#233;thode
n&#8217;est actuellement pas en mesure de r&#233;cup&#233;rer efficacement ce type de lexies inconnues.
</p>
<p>Pr&#233;cision / Rappel (estimation) D&#233;tail des erreurs
Centile Prec. Rapp. Nb Lexies Type % Exemple
</p>
<p>10 71,0 % 29,6 % 1 874 Collocations 6,5 %
Souscrire une assurance
</p>
<p>&#20080;&#20445;&#38505;
</p>
<p>50 60,2 % 46,0 % 7 943 Syntagmes non lexicalis&#233;s 60,7 %
le responsable participe
</p>
<p>&#36127;&#36131;&#20154;&#21442;&#21152;
</p>
<p>100 52,6 % 53,7 % 13 880 Erreurs de segmentation 32,7 %
la direction constr...
</p>
<p>&#20844;&#21496;&#24635;&#37096;&#35774;
</p>
<p>TAB. 2 &#8211; R&#233;sultats et d&#233;tails des erreurs.
</p>
<p>Lexies Rang Fr&#233;q. Lexies Rang Fr&#233;q.
institution financi&#232;re
</p>
<p>&#37329;&#34701;&#26426;&#26500; 58 536
carte d&#8217;assurance maladie
</p>
<p>&#21307;&#20445;&#21345; 9 758 15
Xincheng Assurance Vie
</p>
<p>&#20449;&#35802;&#20154;&#23551;&#20445;&#38505;&#26377;&#38480;&#20844;&#21496; 293 138
ordinateur portable
</p>
<p>&#31508;&#35760;&#26412;&#30005;&#33041; 12 615 10
PDG
</p>
<p>&#39318;&#24109;&#25191;&#34892;&#23448; 405 105
politique de soutien &#224; l&#8217;agriculture
</p>
<p>&#25903;&#20892;&#24800;&#20892; 13 062 10
assurance responsabilit&#233; d&#8217;agence de voyage
</p>
<p>&#26053;&#34892;&#31038;&#36131;&#20219;&#38505; 5 356 24
proc&#233;dure de r&#233;clamation d&#8217;indemnit&#233;s
</p>
<p>&#32034;&#36180;&#27969;&#31243; 24 603 8
</p>
<p>TAB. 3 &#8211; Echantillon de lexies valides extraites.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Extraction de lexique dans un corpus sp&#233;cialis&#233; en chinois contemporain
</p>
<p>6 Discussion et conclusion
</p>
<p>Notre exp&#233;rimentation valide le potentiel d&#8217;extraction de notre m&#233;thode qui d&#233;couvre un nombre
non n&#233;gligeable de lexies inconnues dans le corpus de travail. Nous avons &#233;galement confirm&#233;
l&#8217;utilit&#233; du crit&#232;re d&#8217;ordonnancement des candidats, la densit&#233; des lexies candidates valides
&#233;tant plus &#233;lev&#233;e en d&#233;but de liste. Pour compl&#233;ter l&#8217;&#233;valuation de notre m&#233;thode, il est n&#233;ces-
saire de la comparer &#224; d&#8217;autres travaux. Cette comparaison doit cependant se faire au regard de
l&#8217;influence des genres et usages des corpus respectifs car &#8212; comme l&#8217;indique Aussenac-Gilles
(2002) &#8212; le r&#233;sultat d&#8217;une m&#233;thode est conditionn&#233; par les caract&#233;ristiques du contexte d&#8217;appli-
cation : &#171; [Les] outils [sont] construits en r&#233;f&#233;rence &#224; des th&#233;ories sp&#233;cifiques, qui induisent des
contraintes d&#8217;utilisation et des biais dans les r&#233;sultats obtenus [...] leur utilisation [est] plus ou
moins pertinente en fonction de la nature du corpus et du produit terminologique &#224; construire &#187;.
Ainsi, la m&#233;thode de Feng et al., qui s&#8217;appuie sur la variation du contexte, est particuli&#232;rement
adapt&#233;e aux d&#233;p&#234;ches d&#8217;agences de presse (rappel 74,2 % et pr&#233;cision 81,2 %). En effet, ce
type de corpus abordant de nombreux th&#232;mes, les lexies inconnues provenant de domaines di-
vers sont nombreuses, et les contextes d&#8217;apparition plus vari&#233;s. En revanche dans les corpus
sp&#233;cialis&#233;s &#8212; o&#249; les lexies inconnues sont en partie sp&#233;cialis&#233;es &#8212; certaines formulations sont
r&#233;currentes et un syntagme non lexicalis&#233; peut appara&#238;tre tr&#232;s r&#233;guli&#232;rement, ce qui g&#233;n&#232;re du
bruit (cause de 60,7 % de nos erreurs). Cette difficult&#233; est accentu&#233;e dans le discours commercial
particuli&#232;rement r&#233;p&#233;titif. Les r&#233;sultats de la m&#233;thode de Fung et Wu sur des comptes-rendus de
conseils l&#233;gislatifs (rappel 14,0 % et pr&#233;cision 59,3 %) sont significatifs de la difficult&#233; &#224; traiter
ce type de corpus. Face &#224; ce constat, nous pensons qu&#8217;il est n&#233;cessaire de mettre en place une
r&#233;f&#233;rence de test commune via un protocole d&#8217;&#233;valuation de la t&#226;che d&#8217;extraction de lexique en
chinois. Ce protocole doit d&#233;finir clairement l&#8217;unit&#233; linguistique recherch&#233;e et mettre en place
un jeu de tests et de m&#233;triques tenant compte de la diversit&#233; des genres et usages dans les corpus.
</p>
<p>Pour conclure, notre objectif est de fournir une aide &#224; la constitution de lexique &#224; partir de cor-
pus. Dans le cadre de cette application, les r&#233;sultats produits sont corrects aux vues de la quantit&#233;
et de la qualit&#233; des lexies extraites. En rel&#226;chant la contrainte de fr&#233;quence &#224; 3, nous obtenons un
rappel avoisinant les 67,5 % et le nombre de lexies candidates triple (environ 75 000 candidats).
Cette am&#233;lioration de la couverture se fait cependant au d&#233;triment de la qualit&#233; des r&#233;sultats en
fin de liste de lexies candidates. Pour autant, gr&#226;ce &#224; l&#8217;ordonnancement des candidats, la qualit&#233;
des &#233;l&#233;ments en d&#233;but de liste n&#8217;est que peu influenc&#233;e. N&#233;anmoins la d&#233;tection des lexies &#224;
faible fr&#233;quence est importante car elle repr&#233;sente une part non n&#233;gligeable du contenu lexical
d&#8217;un corpus. Par ailleurs, l&#8217;extraction de lexies connues &#224; valeur sp&#233;ciale dans le corpus via
le calcul des sp&#233;cificit&#233;s lexicales (Drouin, 2004) doit &#234;tre &#233;galement envisag&#233;e pour compl&#233;ter
nos r&#233;sultats. Enfin, notre m&#233;thode n&#8217;organise ni ne distingue les lexies g&#233;n&#233;rales et sp&#233;cialis&#233;es.
L&#8217;int&#233;gration d&#8217;une analyse des caract&#233;ristiques endog&#232;nes des lexies permettrait une meilleure
organisation, s&#233;lection et exploitation des r&#233;sultats produits. Les futurs d&#233;veloppements de notre
m&#233;thode iront dans ces directions.
</p>
<p>Remerciements
</p>
<p>Je tiens &#224; remercier sp&#233;cialement Pierre Zweigenbaum pour ses recommandations pertinentes
et ses relectures attentives. Je remercie &#233;galement chaleureusement mes coll&#232;gues d&#8217;Arisem, en
particulier Nicolas Dessaigne et St&#233;phanie Brizard, pour leur soutien et leurs relectures.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Ga&#235;l Patin
</p>
<p>R&#233;f&#233;rences
</p>
<p>AUSSENAC-GILLES N., CONDAMINES A. &amp; SZULMAN S. (2002). Prise en compte de l&#8217;ap-
plication dans la constitution de produits terminologiques. In Actes des 2e Assises Nationales
du GDR I3, p. 289&#8211;302.
BEIJING UNIVERSITY ICL (2001). PFR china daily corpus.
CHANG J. &amp; SU K. (1997). An unsupervised iterative method for Chinese new lexicon ex-
traction. In Computational Linguistics, Computational Linguistics, p. 22&#8211;29.
CHURCH K. W. &amp; HANKS P. (1997). Word association norms, mutual information and lexi-
cography. In M. PRESS, Ed., Computational Linguistics, volume 16, p. 22&#8211;29.
DROUIN P. (2004). Sp&#233;cificit&#233;s lexicales et acquisition de la terminologie. In JADT 2004 :
7&#232;me Journ&#233;es internationales d&#8217;Analyse statistique des Donn&#233;es Textuelles, p. 345&#8211;352.
FENG H., CHEN K., DENG X. &amp; ZHENG W. (2004). Accessor variety criteria for Chinese
word extraction. In ACL, volume 30, p. 75&#8211;93, Cambridge.
FUNG P. &amp; WU D. (1994). Statistical augmentation of a Chinese machine-readable dictionary.
In WVLC-94, Second Annual Workshop on Very Large Corpus.
L&#8217;HOMME M.-C. &amp; POLGU&#200;RE A. (2008). Mettre en bons termes les dictionnaires sp&#233;cia-
lis&#233;s et les dictionnaires de langue g&#233;n&#233;rale. In Lexicologie et terminologie: histoire de mots.
Colloque en l&#8217;honneur d&#8217;Henri B&#233;joint, p. 191&#8211;206, Lyon.
LI H., HUANG C., GAO J. &amp; FAN X. (2004). The use of SVM for Chinese new word
identification. In IJCNLP 2004, First International Joint Conference, p. 723&#8211;732.
MCENERY T. &amp; XIAO R. (2004). The Lancaster corpus of Mandarin Chinese.
MEL&#8217;CUK I. (1993). Cours de morphologie g&#233;n&#233;rale introduction et premi&#232;re partie : Le mot,
volume 1. Les presses de l&#8217;universit&#233; de Montr&#233;al - CNRS Edition.
NAKAGAWA H., KOJIMA H. &amp; MAEDA A. (2004). Chinese term extraction from Web pages
based on compound word productivity. In Third SIGHAN Workshop on Chinese Language
Processing, p. 79&#8211;85, Barcelona, Spain.
NGUYEN E. V. T. (2006). Unit&#233; lexicale et morphologie en chinois mandarin vers l&#8217;&#233;labora-
tion d&#8217;un DEC du chinois. PhD thesis, Universit&#233; de Montr&#233;al.
PIAO S. S., SUN G., RAYSON P. &amp; YUAN Q. (2006). Automatic extraction of Chinese
multiword expressions with a statistical tool. In Workshop on Multi-word-expressions in a
Multilingual Context held in conjunction with the 11th EACL 2006, Trento, Italy.
POLGU&#200;RE A. (2003). Lexicologie et s&#233;mantique lexicale. Notions fondamentales. Universit&#233;
de Montr&#233;al.
QUASTHOFF U. &amp; WOLFF C. (2002). The Poisson collocation measure and its application.
In Proceedings of the 20th international conference on Computational Linguistics.
SPROAT R. &amp; EMERSON T. (2003). The first international Chinese word segmentation ba-
keoff. In Proceedings of the Second SIGHAN Workshop on Chinese Language Processing.
SUN M., SHEN D. &amp; TSOU B. K. (1998). Chinese word segmentation without lexicon and
hand-crafted training data. In Proceedings of ACL, p. 1265&#8211;1271.
WU A. &amp; JIANG Z. (2000). Statistically-enhanced new word identification in a rule-based
Chinese system. In Proceedings of the 2nd Chinese Language Processing Workshop, vo-
lume 12, p. 46&#8211;51, Hong Kong: ACL.
YANG W. &amp; LI X. (2002). Chinese keyword extraction based on max-duplicated strings of
the documents. In Proceedings of the 25th Annual International ACM, p. 439&#8211;440.</p>

</div></div>
</body></html>