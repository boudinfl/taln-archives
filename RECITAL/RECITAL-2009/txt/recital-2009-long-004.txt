RECITAL 2009, Senlis, 24-26 juin 2009

Extraction de lexique dans un corpus spécialisé
en chinois contemporain

Gael Patin1’2
(1) Er-Tim, Inalco, 75343 Paris
(2) Arisem, Thales, 91300 Massy
gael.patin@inalco.fr

Résumé. La constitution de ressources lexicales est une tache cruciale pour l’amélioration
des performances des systemes de recherche d’information. Cet article présente une méthode
d’extraction d’unités lexicales en chinois contemporain dans un corpus spécialisé non-annoté
et non-segmenté. Cette méthode se base sur une construction incrémentale de l’unité lexicale
orientée par une mesure d’association. Elle se distingue des travaux précédents par une approche
linguistique non-supervisée assistée par les statistiques. Les résultats de l’extraction, évalués sur
un échantillon aléatoire du corpus de travail, sont honorables avec des scores de précision et de
rappel respectivement de 52,6 % et 53,7 %.

Abstract. Building lexical resources is a vital task in improving the efﬁciency of infor-
mation retrieval systems. This article introduces a Chinese lexical unit extraction method for
untagged specialized corpora. This method is based on an incremental process driven by an
association score. This work features an unsupervised statistically aided linguistic approach.
The extraction results — evaluated on a random sample of the working corpus — show decent
precision and recall which amount respectively to 52.6% and 53.7%.

M0tS-CléS I corpus spécialisé, unité lexicale, lexie, extraction de lexique, chinois.

Keywords: specialized corpus, lexical unit, lexicon extraction, Chinese.

1 Introduction

Les lexiques sont des ressources indispensables aux systemes de recherche d’information. Ils
permettent d’amé1iorer notablement les résultats des procédés automatiques d’analyse linguis-
tique — étiquetage morpho-syntaxique, interprétation sémantique ou indexation — dans des
domaines particuliers. Or la constitution de lexiques est confrontée a deux types de difﬁcul-
tés : les unes d’ordre pragmatique, telles que le coﬁt de leur élaboration ou leur réutilisabilité,
sont d’une grande importance pour la Inise en oeuvre industrielle; les autres d’ordre théorique,
comme la déﬁnition de l’unité lexicale dans différentes langues ou la caractérisation des par-
ticularités lexicales d’un corpus spécialisé, sont primordiales pour la pertinence et la validité
des résultats. Cette confrontation entre intérét économique et qualitatif est une problématique
récurrente dans le milieu de l’entreprise. La recherche scientiﬁque appliquée doit étre a méme
de proposer des solutions pour répondre a cette double exigence. Cette étude propose un élé-
ment de réponse au probleme de l’identiﬁcation de lexique dans un corpus spécialisé en chinois
contemporain via un systeme de classement de lexies (unités lexicales) candidates. Cette étude

Gaél Patin

s’intéresse en particulier au cas du chinois contemporain, langue pour laquelle nous ne dispo-
sons que de peu de ressources lexicales.

Dans la suite de cet article nous présentons en premier lieu les travaux similaires a notre méthode
(section 2), puis nous introduisons une analyse linguistique de la notion de lexie en chinois
contemporain écrit (section 3). Apres une description de notre méthodologie (section 4), nous
présentons nos résultats d’expérience (section 5). Enﬁn, nous concluons par une discussion et
une comparaison des résultats (section 6).

2 Travaux similaires

L’ extraction de lexique est un probleme de premiere importance pour le traitement automa-
tique du chinois contemporain. En effet, le processus de segmentation en mots est généralement
considéré comme étant l’étape initiale pour le traitement de textes en chinois. Or le vocabulaire
inconnu est un facteur d’erreur signiﬁcatif pour la segmentation en mots en chinois (Sproat &
Emerson, 2003). De nombreuses approches s’appuient sur les résultats d’un processus de seg-
mentation en mots (Wu & Jiang, 2000; Li et al., 2004) ou d’annotations morpho-syntaxiques
(Piao et al., 2006) entrainé statistiquement pour extraire du lexique inconnu. Ces techniques
obtiennent de bons résultats mais l’application de ces procédés sur des corpus de domaine et
genre différents de celui du corpus d’entrainement est peu efﬁcace. La constitution de corpus
annotés étant tres coﬁteuse, nous renoncons a ce type de méthode.

D’autres approches s’integrent dans un systeme de segmentation en mots et visent a améliorer
son efﬁcacité. Dans leur article, Fung et Wu (1994) introduisent une méthode de sélection de
segments via une mesure d’information mutuelle mais leur méthode, testée sur un corpus spé-
cialisé, souffre d’un faible score de rappel. Chang et Su (1997) présentent un processus itératif
non-supervisé d’extraction de lexique orienté par la qualité de la segmentation obtenue avec le
lexique ainsi découvert. Leur approche bien qu’efﬁcace impose une restriction arbitraire de la
longueur des lexies candidates (quadrigrammes). Feng et al. (2004) introduisent une méthode
simple basée sur une mesure de variation du contexte avec des résultats tres convaincants. Cer-
tains travaux utilisent des techniques d’extraction originales, comme la recherche de répétitions
caractéristiques dans la structure inteme des lexies spécialisées (Nakagawa et al., 2004) ou le
calcul des segments répétés (Yang & Li, 2002), mais leurs évaluations ne font pas mention du
score de rappel de leur méthode. Enﬁn Sun et al. (1998) proposent un systeme de segmenta-
tion en mots sans ressources lexicales basé sur le score d’association mutuelle. Notre approche
s’inspire de cette méthode en proposant une construction incrémentale des lexies guidée par une
mesure d’association sur un corpus spécialisé non-annoté.

Il est cependant difﬁcile de comparer rigoureusement nos travaux a d’autres et ce pour plu-
sieurs raisons : la maj eure partie des expérimentations utilise des corpus constitués de dépéches
d’agence de presse. Seuls (Fung & Wu, 1994) utilisent un corpus spécialisé (comptes rendus
de conseil législatif). Par ailleurs, les criteres de sélection linguistique des lexies valides ne
sont pas décrits précisément. D’autres travaux restreignent arbitrairement la longueur des lexies
recherchées, comme celle de Chang et Su (1997) et de Feng et al. (2004), ou ﬁxent un seuil
de fréquence pour la sélection des lexies (Fung & Wu, 1994). Enﬁn certains articles ne men-
tionnent pas de score de rappel (Yang & Li, 2002; Nakagawa et al., 2004). Seules deux études
ont des conditions d’expérimentation proches des notres : la méthode de (Fung & Wu, 1994) et
la méthode de (Feng et al., 2004).

Extraction de lexique dans un corpus specialise en chinois contemporain

3 Déﬁnition de la lexie en chinois contemporain

Notre étude met l’accent sur l’importance de bien comprendre et analyser les phénomenes lin-
guistiques recherchés pour pouvoir appliquer de maniere efﬁcace les outils technologiques a
notre disposition. Les theories lexicologiques sont diverses et variées, nous n’avons pas la pré-
tention d’afﬁrmer que notre orientation est la plus juste, nous souhaitons seulement nous doter
d’un cadre théorique bien fondé aﬁn de ne pas nous contenter de simples intuitions sur la langue
pour produire un traitement automatique. Dans cet article, nous nous appuyons sur les theories
de Mel’cuk (1993) et Polguere (2003) appliquées par Nguyen (2006) pour le cas du chinois
contemporain, meme si quelques approximations ou alterations dues aux particularités de la
langue chinoise peuvent apparaitre dans notre presentation. Notons que les deﬁnitions intro-
duites ne sont valides que dans le cadre de l’étude synchronique du chinois contemporain écrit.

La lexie (ou unite’ lexicale) est l’unité de description du lexique. Une lexie peut etre issue d’un
processus de formation morphologique, on parle alors de lexeme, ou syntaxique, on parle alors
de location. En chinois, certaines particularités morpho-syntaxiques et typologiques rendent dif-
ﬁcile la detection automatique de lexies dans les textes. En effet, la morphologie composition-
nelle riche, souvent similaire a la syntaxe, augmente combinatoirement le nombre de candidats
potentiels ; alors que le nombre réduit d’indices d’identiﬁcation des lexies — dﬁ a la morpholo-
gie ﬂexionnelle pauvre, la rareté des morphemes grammaticaux et l’absence de mots outils dans
la construction des lexies — limite la selection des candidats. Enﬁn, a l’écrit, les textes sont
dépourvus d’espaces et la plupart des graphies ont individuellement un signiﬁé lexical (meme
si elles ne sont pas nécessairement autonomes), ce qui restreint également les indices d’auto-
nomie des signes. La lexie spe’cialise’e est une lexie qui acquiert un sens particulier a travers
une pratique langagiere au sein d’un groupe social dans un domaine spéciﬁque. Il peut s’agir
de formes communes dont le sens habituel est dérivé ou modiﬁé, ou bien de lexies exclusive-
ment utilisées par les membres d’un groupe. Cette deﬁnition rejoint les postulats énoncés par
L’ homme et Polguere (2008) dans leur description du terme .

Une graphie est l’unité orthographique autonome minimale en chinois écrit. La graphie cor-
respond approximativement a la notion de « caractere chinois ». Les glyphes suivants sont des
exemples de graphies : iv, 9%, F52, 5, 5,  Un morphe (note |M|) est un signe linguis-
tique au signiﬁé élémentaire dont le signiﬁant est representable par une suite d’une ou plusieurs
graphies. L’ élémentarité du signiﬁé exprime que le morphe ne peut etre le résultat d’une com-
binaison d’autres morphesl. Les suites de graphies suivantes sont des morphes car associées a
un signiﬁé et non décomposables en morphes :  ‘vie’, I5 5 | ‘raisin’, |F“I 5] P§7Hi| ‘aspiri-
ne’, |ﬁ’<| ‘pr0te’ger’,  ‘danger’,  ‘acheter’. En revanche, la graphie 5 — n’ayant aucun
signiﬁé associé — n’est pas un morphe.

Un mot-forme (note (M)) est une sequence autonome et insécable de morphes. L’ autonomie
implique que le mot-forme peut etre énoncé isolément et prendre place dans un paradigme syn-
taxique. L’insécabilité exprime que la separation des morphes entraine la perte de leur relation
lexicale. Le lexeme (noté (( L ))) est un ensemble de mots-formes ﬂéchis au meme signiﬁé lexi-
cal. Un syntagme (note [ S ]) est une combinaison syntaxique de mots-formes dont les compo-
santes ont un certain degré de liberté. Une location (notée [[ L H) est un ensemble de syntagmes
lexicalisés ﬂéchis au meme signiﬁé lexical.

1La deﬁnition donnee par Polguere parle plutet de non representabilite en tennes de signes, or ceci n’est Valide
qu’en chinois oral. En effet, a l’ecrit, l’analyse grammatologique des sinogrammes explique comment le sens du
sinogramme (“H <—>‘clair’) est représenté en termes de composantes graphiques ( El <—>‘soleil’) et (H <—>‘lune’).

Gael Patin

Le terme lexie regroupe les deux concepts de lexeme et locution, c’est1’unité d’étude principale
de la lexicologie. Les exemples du tableau (1) ci-dessous présentent des lexemes et locutions
avec leurs mots-formes et syntagmes associés. Cette étude propose une méthode pour identiﬁer
mots-formes et syntagmes lexicalisés et, par extension, identiﬁer lexemes et locutions. Notre
méthode étant appliqué sur un corpus spécialisé, nous nous attendons a extraire un ensemble de
lexiesz dont une part signiﬁcative devraient étre des lexies spécialisées.

Lexéme mots-formes Locution syntagmes
aspmne aspmne tirer (5 feujl tirer (5 feu) tirer (5 feu) laccompljl
((re]::Ireé] FE  )  /jjcfipff | zendre lduranfl prendre /expre’n'ence/ |[é:j;a}iEx  I:  T )  J
(6%)) (ram ) (raw )( Iéliil ) l[rw*-fjll, [(“€)(§’i)]
assurance assurance compagnre assurance compagnre assurance
((ﬁ%F&*)) (|ﬁ%|F§‘|) [[4;m~/r>éJ]] [(47m~)(»r>5J)]

TAB. 1 — Exemples de lexies.

4 Méthodologie

Notre méthode s’appuie sur la description de la lexie en chinois contemporain pour extraire 1e
lexique inconnu d’un corpus spécialisé. Nous ne traitons pas le cas des lexies mono-morphe-
miques, ni des lexies déja connues avec un statut particulier dans le corpus qui feront 1’objet
d’une étude ultérieure. Notre idée est de reproduire 1e processus morpho-syntaxique de forma-
tion des mots-formes et syntagmes en partant de la notion de morphe sans procéder au préalable
a une segmentation en mots, puis de détecter les mots-formes et syntagmes lexicalisés grace a
une mesure d’association. I1 s’agit concretement de parcourir le corpus en associant incrémen-
talement morphes et mot-formes, puis de sélectionner les combinaisons stables, récurrentes et
indépendantes sur 1’ensemb1e du corpus. Ici les statistiques et ressources lexicales a notre dis-
position servent a décider quels sont les elements a associer a chaque étape de notre processus.
L’essence de notre méthode est le principe de construction incrémentale de la lexie, davantage
que le score d’association utilisé.

Dans la section précédente, nous avons expliqué que 1’unité minimale porteuse de sens est le
morphe et que la construction lexicologique chinoise s’appuie sur1’association de composantes
lexicales. C’est pourquoi nous initialisons notre processus d’identiﬁcation par une representa-
tion du texte en morphes (P0). Nous avons également vu que la production des lexies en chinois
s’appuie autant sur la syntaxe que sur la morphologie. Nous posons un postulat (Pn) pour décrire
les phénomenes de production morphologique et syntaxique, sachant que nous nous intéressons
uniquement aux syntagmes présents en séquence continue dans le corpus :

Postulat d’initialisati0n (P0) : Le morphe est l’unité de construction e’le’mem,‘aire des lexies
en chinois contemporain.

Postulat de récursion (Pn) : Les mots-formes du chinois modeme sont une sequence de deux

morphes ou mot-formes. Les syntagmes sont une combinaison syntaxique de mots-formes.

Ces deux postulats permettent de poser les bases de notre méthode en déﬁnissant 1’unité de
recherche initiale et en décrivant 1e processus de construction des lexies.

2Dans la suite de 1’artic1e, 1e terme << lexie » sera utilisé pour désigner un << mot—forrne ou syntagme lexicalisé
représentantd’une1exie » lorsque nous parlerons des lexies découvertes dans le texte.

Extraction de lexique dans un corpus specialise’ en chinois contemporain

4.1 Description des corpus

Le corpus de référence contient 2 millions de graphies. Il est composé de deux corpus de chi-
nois contemporain : The Lancaster Corpus of Mandarin Chinese (McEnery & Xiao, 2004) et
PFR China Daily corpus (Beijing University ICL, 2001) tous deux annotés selon la norme de
l’Université de Beijing3. I1 nous a servi a recueillir toutes les informations lexicales nécessaires
a notre méthode. En nous basant sur ces annotations, nous avons extrait un lexique de référence
d’environ 75 000 lexies, une liste des mots-vides et une liste des bigrammes syntaxiques (voir
4.2.3). Le corpus de test est un corpus de documents commerciaux de compagnies d’assurance
publiés sur leur site internet. 11 est composé de dix millions de caracteres. Ila été recueilli auto-
matiquement a partir d’une liste d’adresses de sites de compagnies d’assurance chinoises. Les
textes sur internet contenant de nombreuses répétitions (par exemple dans les menus), tous les
paragraphes doublons ont été éliminés aﬁn de ne pas perturber les calculs statistiques.

4.2 Algorithme

Les lexies candidates sont construites de maniere incrémentale en associant progressivement
des couples de grains. Un grain est constitué d’une séquence de morphes pouvant représenter
un morphe, un mot-forme, un syntagme ou une partie de syntagme. Le morphe est le niveau
de granularité initiale. Nous déduisons ensuite le niveau de granularité suivant en associant des
couples de grains. Nous utilisons une mesure d’association pour nous orienter dans le choix des
couples de grains a associer et la sélection des lexies candidates. Le score d’association entre les
grains est recalculé a chaque changement de granularité aﬁn d’intégrer les déductions obtenues
lors des étapes précédentes. L’ algorithme s’arréte lorsqu’un plafond de nombre d’associations
maximal est atteint4. L’étape ﬁnale consiste a sélectionner et ordonner les lexies candidates en
fonction de leur pertinence.

Algorithme d’extraction de lexies candidates

Sélectionner des grains initiaux (Pb)

Faire

I Calculer le score d’association des grains

I Sélectionner les grains de niveau supérieur (IQ)

Tant que le plafond d’association maximal n’est pas atteint

Sélection et ordonnancement des lexies candidates

4.2.1 Grains initiaux : morphes

Nous considérons trois types de morphes : les morphes unigrammes, les morphes bigrammes
(ﬂﬂéi HUDIE ‘papillon’) et les morphes translittérés (7%; if] YiDALi ‘Italie’). Un morphe uni-
gramme est une graphie a laquelle est associée un sens. Nous considérons que toutes les graphies
sont potentiellement des morphes a l’exception de celle incluses dans un morphe bigramme ou
translittéré. Un morphe translitte’re’ est un calque phonétique d’un mot étranger utilisant la pro-
nonciation de graphies. Les graphies utilisées pour le calque phonétique sont pour la plupart
récurrentes et il est possible de les détecter avec des outils statistiques5. Un morphe bigramme

3http://icl.pku.edu.cn/icl_groups/corpus/coprus—annotation.htm
4Estimé a 5 associations selon nos observations sur les lexies les plus grandes dans le corpus de référence.
5Nous avons utilisé pour cela Pimplémentation CRF++ de la théorie des Conditional Random Fields.

Gaél Patin

est un couple de graphies ne prenant sens que dans la mesure ou elles sont associées (#3 H13
‘Q’). Il se peut que l’une des graphies du couple puisse exister en tant que morphe porteur du
méme sens (*¥ DIE ‘papillon’), cependant il n’est jamais autonome (5$i7i< DIEYGNG ‘nage pa-
pillon’). Leur détection est faite a l’aide de leurs particularités statistiques6. Lorsqu’il y a une
ambigu'1'té entre différents morphes, le morphe englobant est toujours préféré.

4.2.2 Calcul du score d’association

A chaque étape de l’algorithme, le score d’association de tous les couples de grains est calculé
a l’aide du score Poisson-Stirling7 (Quasthoff & Wolff, 2002) déﬁni ainsi :

k- log k—1og A-1
P5(“v”)=%

o1‘1 A = n - pa - pg, avec pa et pg, la probabilité d’apparition respective de a et b, k le nombre
d’occurrences du couple ab et 12 1e nombre total de couples. Ce score reﬂete la tendance de a et
b a se retrouver plus souvent cote a cote que ne le voudrait une repartition aléatoire du corpus.
Plus le score est élevé, plus l’association entre (1 et b est signiﬁcative, un score nul indiquant la
neutralité. Un lien a été mis en évidence entre les mesures d’association et certains phénomenes
linguistiques tels que les relations sémantiques ou syntaxiques (Church & Hanks, 1997).

4.2.3 Sélection du niveau de granularité supérieur : mots-formes ou syntagmes

La sélection du niveau de granularité supérieur consiste a choisir des couples de grains a asso-
cier pour identiﬁer des lexies candidates. L’idée directrice de la sélection est que si un couple
de signes a un score d’association élevé au sein d’un corpus, alors il existe une relation particu-
liere entre eux. Cette relation peut étre d’ordre lexical, syntaxique (en chinois un classiﬁcateur
est souvent associé a un numéral) ou simplement reﬂéter des relations prédicat-argument com-
munes dans le corpus de travail (é’v'J P3: 32% 33.53 ‘le vice-président a déclaré’). Cependant les
relations d’ordre contextuel et syntaxique non lexicalisés ne nous intéressent pas et la faible re-
présentativité de certains signes peut également induire en erreur le processus statistique. Pour
écarter les associations non-souhaitables, des criteres de ﬁltrage sont ajoutés pour la sélection
des couples. Notre méthode travaille sur l’ensemble des segments du corpus. Un segment est
une suite ininterrompue de graphies du corpus situé par exemple entre des signes de ponctua-
tion, des caracteres alphabétiques ou le début et la ﬁn de ligne.

Soit S, l’ensemble des segments de sinogrammes du corpus a l’itération i. Soit le segment
3;‘ E S, avec n le numéro de segment, composé d’une suite m1,m2 . . .m|s1n| de grains telle que
V mj E 37-‘ mj E Graphie+, comme dans l’eXemple (1) ci-dessous :

7, 7

(1) 30 :  .. ,( ;§'_:.5z]~.;}'.a;§;.ﬁ’;.)g.lL\.§;~  36» : ( ;§.’_:.5r]~.;y.¢§;.ﬁ';.)I§\.§~)

avec m1 =§E et m7 = 3:‘.

Le passage d’une granularité courante a une granularité supérieure se fait en appliquant la regle
suivante sur l’ensemble du segment.

5Fort score d’association, exclusivité d’association d’un ou des composants.
7Nous avons également testé l’infonnation mutuelle avec des résultats équivalents modulo un gain de rappel et
une perte de précision.

Extraction de lexique dans un corpus spécialisé en chinois contemporain

- 7.‘
V 3;‘ V mymy E 3;‘ s1 [my <—> my] 31
alors mymy est remplace par m$y = my - my dans sggl

011 [my <—> my] 3? signiﬁe que « my et my sont associés dans le contexte du segment n a l’étape
1'». Par exemple, si ]7§3<—>5'7]‘]35‘ [ﬁr <—>%'§L] 33 Wl’<<—>l‘;-‘L‘]s5L alors 3'1‘ = ( £1571‘-5}’   La

proposition [my <—> my] 3? n’est vraie que si les conditions suivantes sont remplies :

1. Association inteme: 1¥>’(m,,,my) > 0
2. Liberte’ droite : 1¥>'(m,,, my) > 1%’ (my, my+1)
3. Liberte’ gauche : 1%’ (my, my) > 1%’ (m,,_1, mm)

et que les conditions de ﬁltrage suivantes sont valides pour le couple mxmy :

1. Filtre syntaxique : Le couple n’appartient pas a la liste des collocations syntaxiques.

2. Filtre score/fréquence : Le couple a une fréquence supérieure a 10 (garantit la represen-
tativité statistique) ou un score d’association supérieur a 1 (elimination des cas tangents).

3. Absence de mot vide : Aucun grain n’est un mot vide sauf si le couple est dans le lexique.

La liste des collocations syntaxiques et des mots vides a été obtenue a partir de notre corpus de
référence. Nous appelons collocation syntaxique tout bigramme ayant un score d’association
supérieur a zero dans le corpus de référence mais n’étant pas répertorié comme lexie dans ce
corpus. La liste des mots vides est composée de l’ensemble des particules modales, prépositions,
conjonctions de coordination rencontrées dans les textes écrits du corpus de référence.

4.2.4 Sélection et ordonnancement des lexies candidates

Les lexies sélectionnées sont l’ensemble des associations valides (]m,, <—> my] 3?) trouvées a
chaque étape de l’algorithme qui ne se trouvent pas dans notre lexique. Des seuils de fréquence
et score d’association8 sont également utilisés pour éliminer les candidats les moins probables
en ﬁn de traitement. Nous considérons qu’une lexie est moins pertinente si elle est régulierement
contenue dans une autre; nous ajustons donc sa fréquence dans le calcul ﬁnal de son score
d’association en lui soustrayant le nombre d’occurrences incluses du nombre d’occurrences
total. Les candidats sont ensuite ordonnés par ordre décroissant de score d’association ajusté.

4.3 Protocole d’évaluation

Dans la mesure ou le corpus de travail est brut, sans aucune annotation et tres volumineux (donc
coﬁteux a annoter), nous utilisons des échantillons pour évaluer les résultats. Les mesures qui
nous intéressent sont les scores de précision et de rappel. La précision est déﬁnie comme le
pourcentage de lexies correctes dans la liste de lexies candidates extraites, et le rappel le pour-
centage de lexies inconnues découvertes. Nous estimons le score de précision en sélectionnant
aléatoirement un échantillon de 1 000 lexies candidates dans la liste produite par notre systeme.
L’estimation du score de rappel est obtenue en repérant les lexies inconnues présentes dans une

8Différents du ﬁltre score/fréquence de la sous-section 4.2.3.

Gaél Patin

sélection aléatoire de 250 paragraphes dans le corpus de travail, dans lesquels 287 lexies in-
connues distinctes ont été identiﬁées. L’identiﬁcation et la validation des lexies du corpus ont
été effectuées par un locuteur natif du chinois et linguiste sensibilisé a notre problématique en
appliquant la déﬁnition de la lexie donnée en section 3. Nous attendons que notre systeme pro-
duise une liste de lexies inconnues et nous souhaitons que la distribution de lexies valides soit
plus dense en haut de liste qu’en ﬁn de liste.

5 Résultats

Nous avons effectué une experimentation (tableau 2) en sélectionnant les lexies candidates de
fréquence supérieure a 8 et de score d’association supérieur a 10 (voir 4.2.4). Notre méthode
extrait apres cette sélection une liste de 26 388 lexies candidates parmi lesquelles on estime
a 13 880 le nombre de lexies valides (extraits en tableau 3). L’ ensemble de ces lexies valides
couvre 53,7 % des lexies de l’échantillon du corpus de travail. La précision des résultats décroit
avec l’augmentation des centiles considérés mais reste au-dessus de 50 %. La majorité des
erreurs (60,7 %) sont des syntagmes non-lexicalisés, c’est-a-dire des combinaisons syntaxiques
de mots-formes n’ayant pas de contenu lexical particulier. Ces syntagmes peuvent cependant
contenir des lexies inconnues présentes dans les candidats valides ou non. Une petite partie des
erreurs (6.5 %) sont des collocations (syntagmes a l’usage contraint). Les collocations ne sont
pas pertinentes pour les applications d’analyse de la langue, nous ne les avons donc pas incluses
dans les résultats valides. Notons que 25 % des erreurs contiennent des lexies déja contenues
dans la liste des lexies découvertes. Enﬁn, il est important de remarquer que 92 % des lexies non
découvertes ont moins de 8 occurrences dans le corpus de travail. Ceci montre que les lexies
peu fréquentes représentent une part signiﬁcative des lexies du corpus. La diminution du seuil
de fréquence des candidats dégradant de facon signiﬁcative le score de précision, notre méthode
n’est actuellement pas en mesure de récupérer efﬁcacement ce type de lexies inconnues.

Précision / Rappel (estimation) Détail des erreurs
Centile Prec. Rapp. Nb Lexies Type % Exemple
10 71,0 % 29,6 % 1 874 Collocations 6,5 % D §’<ﬁ’<Fé‘
1e responsable participe
50 60,2 % 46,0 % 7 943 Syntagmes non lexicalisés 60,7 % ﬁt ﬁkvéirbu

la direction oonstr...

100 52,6 % 53,7 % 13 880 Erreurs de segmentation 32,7 % 4} 5] .E’:€‘l3‘i:r’E

TAB. 2 — Résultats et détails des erreurs.

Lexies Rang Fréq. Lexies Rang Fréq.
institution hnanci re carte d’assL1rance maladie
’£‘»%i<7iIL7V~7 58 536 IEGTVF‘ 9 758 15
Xinch‘en‘g Assurance Vie or’d'in::teur portable ‘
iavitmﬁ-ﬁ'<F6~7§I*M\é1 293 138 ’%1E4'\ W12: 12 615 10
 3%?’ '7’ 405 105 3;”P:]i;q’:e;::(§t’;en§1’agﬁcl1t3m062 10
assurance responsJa_biJEé d’agence de voyage procédxurelgexréclatniition d’indemnités
ﬁﬁffairfif F5‘ 5 356 24 ‘E’ l!%in‘1.%¥_ 24 603 8

TAB. 3 — Echamfillon de lexies valides extraites.

Extraction de lexique dans un corpus spécialisé en chinois contemporain

6 Discussion et conclusion

Notre experimentation valide le potentiel d’extraction de notre methode qui decouvre un nombre
non negligeable de lexies inconnues dans le corpus de travail. Nous avons egalement conﬁrme
l’utilite du critere d’ordonnancement des candidats, la densite des lexies candidates valides
etant plus elevee en debut de liste. Pour completer l’evaluation de notre methode, il est neces-
saire de la comparer a d’autres travaux. Cette comparaison doit cependant se faire au regard de
l’inﬂuence des genres et usages des corpus respectifs car — comme l’indique Aussenac-Gilles
(2002) — le resultat d’une methode est conditionne par les caracteristiques du contexte d’app1i-
cation : « [Les] outils [sont] construits en référence £1 des théories spéciﬁques, qui induisent des
contraintes d ’utilisati0n et des biais dans les résultats obtenus [] leur utilisation [est] plus 0u
moins pertinente en fonction de la nature du corpus et du produit terminologique £1 construire ».
Ainsi, la methode de Feng et al., qui s’appuie sur la variation du contexte, est particulierement
adaptee aux depeches d’agences de presse (rappel 74,2 % et precision 81,2 %). En effet, ce
type de corpus abordant de nombreux themes, les lexies inconnues provenant de domaines di-
vers sont nombreuses, et les contextes d’apparition plus varies. En revanche dans les corpus
specialises — o1‘1 les lexies inconnues sont en partie specialisees — certaines formulations sont
recurrentes et un syntagme non lexicalise peut apparaitre tres regulierement, ce qui genere du
bruit (cause de 60,7 % de nos erreurs). Cette difﬁculte est accentuee dans le discours commercial
particulierement repetitif. Les resultats de la methode de Fung et Wu sur des comptes-rendus de
conseils legislatifs (rappel 14,0 % et precision 59,3 %) sont signiﬁcatifs de la difﬁculte a traiter
ce type de corpus. Face a ce constat, nous pensons qu’il est necessaire de mettre en place une
reference de test commune via un protocole d’evaluation de la tache d’extraction de lexique en
chinois. Ce protocole doit deﬁnir clairement l’unite linguistique recherchee et mettre en place
un jeu de tests et de metriques tenant compte de la diversite des genres et usages dans les corpus.

Pour conclure, notre objectif est de foumir une aide a la constitution de lexique a partir de cor-
pus. Dans le cadre de cette application, les resultats produits sont corrects aux vues de la quantite
et de la qualite des lexies extraites. En relachant la contrainte de frequence a 3, nous obtenons un
rappel avoisinant les 67,5 % et le nombre de lexies candidates triple (environ 75 000 candidats).
Cette amelioration de la couverture se fait cependant au detriment de la qualite des resultats en
ﬁn de liste de lexies candidates. Pour autant, grace a l’ordonnancement des candidats, la qualite
des elements en debut de liste n’est que peu inﬂuencee. Neanmoins la detection des lexies a
faible frequence est importante car elle represente une part non negligeable du contenu lexical
d’un corpus. Par ailleurs, l’extraction de lexies connues a valeur speciale dans le corpus via
le calcul des speciﬁcites lexicales (Drouin, 2004) doit étre egalement envisagee pour completer
nos resultats. Enﬁn, notre methode n’organise ni ne distingue les lexies generales et specialisees.
L’ integration d’une analyse des caracteristiques endogenes des lexies permettrait une meilleure
organisation, selection et exploitation des resultats produits. Les futurs developpements de notre
methode iront dans ces directions.

Remerciements

J e tiens a remercier specialement Pierre Zweigenbaum pour ses recommandations pertinentes
et ses relectures attentives. J e remercie egalement chaleureusement mes collegues d’Arisem, en
particulier Nicolas Dessaigne et Stephanie Brizard, pour leur soutien et leurs relectures.

Gael Patin

Références

AUSSENAC-GILLES N., CONDAMINES A. & SZULMAN S. (2002). Prise en compte de l’ap-
plication dans la constitution de produits terminologiques. In Actes des 2e Assises Nationales
du GDR I3, p. 289-302.

BEIJING UNIVERSITY ICL (2001). PFR china daily corpus.

CHANG J. & SU K. (1997). An unsupervised iterative method for Chinese new lexicon ex-
traction. In Computational Linguistics, Computational Linguistics, p. 22-29.

CHURCH K. W. & HANKS P. (1997). Word association norms, mutual information and lexi-
cography. In M. PRESS, Ed., Computational Linguistics, voluIne 16, p. 22-29.

DROUIN P. (2004). Spéciﬁcités lexicales et acquisition de la terminologie. In JADT 2004 .-
7eme Joumées intemationales d ’Analyse statistique des Donne’es Textuelles, p. 345-352.
FENG H., CHEN K., DENG X. & ZHENG W. (2004). Accessor variety criteria for Chinese
word extraction. In ACL, volume 30, p. 75-93, Cambridge.

FUNG P. & WU D. (1994). Statistical augmentation of a Chinese machine-readable dictionary.
In WVLC-94, Second Annual Workshop on Very Large Corpus.

L’HOMME M.-C. & POLGUERE A. (2008). Mettre en bons termes les dictionnaires specia-
lisés et les dictionnaires de langue générale. In Lexicologie et terminologie: histoire de mots.
Colloque en l’honneur d ’Henri Be’joint, p. 191-206, Lyon.

LI H., HUANG C., GAO J. & FAN X. (2004). The use of SVM for Chinese new word
identiﬁcation. In IJCNLP 2004, First International Joint Conference, p. 723-732.

MCENERY T. & XIAO R. (2004). The Lancaster corpus of Mandarin Chinese.

MEL’ CUK I. (1993). Cours de morphologie ge’ne’rale introduction et premiere partie .' Le mot,
volume 1. Les presses de l’université de Montréal - CNRS Edition.

NAKAGAWA H., KOJIMA H. & MAEDA A. (2004). Chinese term extraction from Web pages
based on compound word productivity. In Third SIGHAN Workshop on Chinese Language
Processing, p. 79-85, Barcelona, Spain.

NGUYEN E. V. T. (2006). Unite’ lexicale et morphologie en chinois mandarin vers l’élabora-
tion d ’un DEC du chinois. PhD thesis, Université de Montréal.

PIAO S. S., SUN G., RAYSON P. & YUAN Q. (2006). Automatic extraction of Chinese
multiword expressions with a statistical tool. In Workshop on Multi-word-expressions in a
Multilingual Context held in conjunction with the I 1th EACL 2006, Trento, Italy.

POLGUERE A. (2003). Lexicologie et sémantique lexicale. Notions fondamentales. Université
de Montréal.

QUASTHOFF U. & WOLFF C. (2002). The Poisson collocation measure and its application.
In Proceedings of the 20th international conference on Computational Linguistics.

SPROAT R. & EMERSON T. (2003). The ﬁrst international Chinese word segmentation ba-
keoff. In Proceedings of the Second SIGHAN Workshop on Chinese Language Processing.
SUN M., SHEN D. & TSOU B. K. (1998). Chinese word segmentation without lexicon and
hand-crafted training data. In Proceedings of ACL, p. 1265-1271.

WU A. & J IANG Z. (2000). Statistically-enhanced new word identiﬁcation in a rule-based
Chinese system. In Proceedings of the 2nd Chinese Language Processing Workshop, vo-
lume 12, p. 46-51, Hong Kong: ACL.

YANG W. & LI X. (2002). Chinese keyword extraction based on max-duplicated strings of
the documents. In Proceedings of the 25th Annual International ACM, p. 439-440.

