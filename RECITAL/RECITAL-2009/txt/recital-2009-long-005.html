<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
"http://www.w3.org/TR/html4/loose.dtd">
<html><head><title>Induction de sens de mots &#224; partir de multiples espaces s&#233;mantiques</title>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
</head>
<body>
<div style="page-break-before:always; page-break-after:always"><div><p>RECITAL 2009, Senlis, 24&#8211;26 juin 2009
</p>
<p>Induction de sens de mots &#224; partir de multiples espaces
s&#233;mantiques
</p>
<p>Claire Mouton1,2
(1) CEA/LIST/LIC2M - BP 6 92265 Fontenay-aux-Roses Cedex
</p>
<p>(2) Exalead S.A. - 10 place de la Madeleine - 75008 Paris
Claire.Mouton@cea.fr, Claire.Mouton@exalead.com
</p>
<p>R&#233;sum&#233;. Les mots sont souvent porteurs de plusieurs sens. Pour traiter l&#8217;information cor-
rectement, un ordinateur doit &#234;tre capable de d&#233;cider quel sens d&#8217;un mot est employ&#233; &#224; chacune
de ses occurrences. Ce probl&#232;me non parfaitement r&#233;solu a g&#233;n&#233;r&#233; beaucoup de travaux sur la
d&#233;sambigu&#239;sation du sens des mots (Word Sense Disambiguation) et dans la g&#233;n&#233;ration d&#8217;es-
paces s&#233;mantiques dont un des buts est de distinguer ces diff&#233;rents sens. Nous nous inspirons
ici de deux m&#233;thodes existantes de d&#233;tection automatique des diff&#233;rents usages et/ou sens des
mots, pour les appliquer &#224; des espaces s&#233;mantiques issus d&#8217;une analyse syntaxique effectu&#233;e
sur un tr&#232;s grand nombre de pages web. Les adaptations et r&#233;sultats pr&#233;sent&#233;s dans cet article se
distinguent par le fait d&#8217;utiliser non plus une seule repr&#233;sentation mais une combinaison de mul-
tiples espaces de forte dimensionnalit&#233;. Ces multiples repr&#233;sentations &#233;tant en comp&#233;tition entre
elles, elles participent chacune par vote &#224; l&#8217;induction des sens lors de la phase de clustering.
</p>
<p>Abstract. Words can have many senses. In order to process information correctly, a com-
puter should be able to decide which sense of a word is used in a given context. This unsolved
problem has generated much research in word sense disambiguation and in the generation of
semantic spaces in order to separate possible meanings. Here, we adapt two existing methods
to automatically distinguish words uses and senses. We apply them to multiple semantic spaces
produced by a syntactic analysis of a very large number of web pages. These adaptations and the
results presented in this article differ from the original methods in that they use a combination of
several high dimensional spaces instead of one single representation. Each of these competing
semantic spaces takes part in a clustering phase in which they vote on sense induction.
</p>
<p>Mots-cl&#233;s : espace s&#233;mantique, r&#233;duction de dimensions, Locality Sensitive Hashing,
induction de sens, clustering de mots, objets multi-repr&#233;sent&#233;s.
</p>
<p>Keywords: semantic space, dimensionality reduction, Locality Sensitive Hashing, Word
Sense Induction, words clustering, multi-represented data.
</p>
<p>1 Introduction
</p>
<p>Un m&#234;me mot peut parfois &#234;tre porteur de diff&#233;rents sens (polys&#233;mie) tandis que deux mots
distincts sont parfois porteurs du m&#234;me sens (synonymie). L&#8217;interpr&#233;tation que les humains
font &#224; l&#8217;aide du contexte discursif ou environnemental leur permet de distinguer l&#8217;emploi d&#8217;un
sens par rapport &#224; un autre ainsi que la r&#233;f&#233;rence identique &#224; un m&#234;me concept par deux mots
diff&#233;rents. Diff&#233;rentes approches permettent aux machines de mod&#233;liser cette distance entre</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Claire Mouton
</p>
<p>les sens d&#8217;un m&#234;me mot ou cette proximit&#233; s&#233;mantique entre les mots. Ces approches peuvent
&#234;tre fond&#233;es sur des ressources constitu&#233;es manuellement (ontologies, r&#233;seaux lexicaux) ou sur
des ressources automatiques comme les espaces s&#233;mantiques auxquels nous nous int&#233;ressons
dans ce travail. Nous pensons que la projection des mots dans des espaces vectoriels permet
non seulement de retrouver cette distance mais aussi de regrouper des ensembles de mots qui
pourront d&#233;finir les diff&#233;rents sens des mots polys&#233;miques. C&#8217;est l&#8217;induction de ces sens (Word
Sense Induction, WSI) qui fait l&#8217;objet de notre travail.
Le paradigme des espaces s&#233;mantiques est fond&#233; sur l&#8217;hypoth&#232;se que le sens port&#233; par les mots
d&#233;pend de leur contexte (Harris, 1985). Un contexte peut se concevoir &#224; diff&#233;rentes &#233;chelles,
on peut donc d&#233;finir diff&#233;rents types de contextes. Dans (Salton et al., 1975), les contextes du
Vector Space Model sont les documents dans lesquels apparaissent les termes du vocabulaire.
On stocke donc dans une matrice de similarit&#233; de terme g&#233;n&#233;ral fij la fr&#233;quence d&#8217;apparition
du terme i dans le document j. Les termes du vocabulaire repr&#233;sent&#233;s par les lignes peuvent
ainsi &#234;tre projet&#233;s dans un espace s&#233;mantique o&#249; les dimensions correspondent aux diff&#233;rentes
colonnes de la matrice et les coordonn&#233;es sont donn&#233;es par les valeurs de la matrice. Ainsi, plus
deux termes apparaissent fr&#233;quemment dans les m&#234;mes documents, plus ils seront proches dans
l&#8217;espace s&#233;mantique. Dans (Lund &amp; Burgess, 1996), les colonnes correspondent aux termes
les plus fr&#233;quents du vocabulaire et la matrice stocke les comptes de cooccurrences entre les
termes des lignes et ceux des colonnes sur des fen&#234;tres de taille fixe (n mots cons&#233;cutifs apr&#232;s
avoir retir&#233; les mots vides tels que les pr&#233;positions, pronoms, d&#233;terminants...). Plus deux termes
apparaissent &#224; proximit&#233; des m&#234;mes termes, plus ils seront proches dans l&#8217;espace s&#233;mantique.
Dans (Pad&#243; &amp; Lapata, 2007) et (Grefenstette, 2007), un contexte est une relation syntaxique
associ&#233;e &#224; un mot du vocabulaire. Nous donnons pour exemple le contexte sujet de manger.
Tous les mots apparaissant dans ce contexte ont une certaine similarit&#233; s&#233;mantique (entit&#233;s
capables de manger) qui sera nuanc&#233;e par l&#8217;ensemble des autres contextes.
A partir de ces espaces s&#233;mantiques o&#249; chaque terme poss&#232;de une signature (son vecteur ligne)
qui le rend plus ou moins proche d&#8217;un autre, on peut regrouper les termes entre eux en fonction
de leur similarit&#233;. Induire des sens &#224; partir de mots polys&#233;miques peut se faire &#224; partir de diff&#233;-
rentes s&#233;lections de termes. Dans (Lin, 1998), tout le vocabulaire est r&#233;parti automatiquement
(clustering) en ensembles de sens proches formant ainsi des classes de synonymes. Chaque mot
pouvant appartenir &#224; plusieurs ensembles, les mots polys&#233;miques voient ainsi leurs sens discri-
min&#233;s. L&#8217;approche de (Sch&#252;tze, 1998) consiste &#224; effectuer un clustering sur un certain nombre
d&#8217;instances du mot pour lequel on veut induire des sens, en utilisant leurs contextes d&#8217;occur-
rences dans un corpus donn&#233;. Enfin, certains comme (V&#233;ronis, 2003), (Ferret, 2004) effectuent
un clustering sur les meilleurs cooccurrents des mots &#224; distinguer en sens tandis que (Pantel &amp;
Lin, 2002) regroupent leurs plus proches voisins.
Notre travail s&#8217;inscrit dans la lign&#233;e de ceux de (Pantel &amp; Lin, 2002) car nous regroupons aussi
des plus proches voisins mais il se diff&#233;rencie par la distinction de plusieurs espaces s&#233;man-
tiques dont nous combinons les sp&#233;cificit&#233;s. En effet, jusqu&#8217;&#224; pr&#233;sent les travaux faisant usage
des espaces s&#233;mantiques fond&#233;s sur les cooccurrences syntaxiques traitaient tous les contextes
syntaxiques de la m&#234;me fa&#231;on, ind&#233;pendemment du type de relation syntaxique formant le
contexte (la seule distinction r&#233;sidait dans le fait d&#8217;utiliser une relation ou non). Notre travail
montre la variabilit&#233; des distances entre les mots selon les relations syntaxiques utilis&#233;es et ex-
plore les apports de la prise en compte sp&#233;cifique des diff&#233;rentes relations. Il reprend en outre
un algorithme de recherche rapide de plus proches voisins approximatifs d&#233;velopp&#233; par (Ra-
vichandran et al., 2005) et le modifie pour d&#8217;une part r&#233;duire sa complexit&#233; algorithmique et
d&#8217;autre part distribuer les calculs sur les noeuds d&#8217;un cluster. Nous montrons que notre version</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Induction de sens de mots &#224; partir de multiples espaces s&#233;mantiques
</p>
<p>modifi&#233;e conserve autant d&#8217;information que la version originale.
</p>
<p>La Section 2 de cet article d&#233;taille la conception des espaces s&#233;mantiques que nous utilisons.
Nous pr&#233;sentons dans la Section 3 une m&#233;thode rapide de recherche des plus proches voisins
ainsi que la m&#233;thode de r&#233;duction de dimensions associ&#233;e. La Section 4 d&#233;crit les m&#233;thodes de
clustering d&#233;velopp&#233;es pour prendre en compte les disparit&#233;s de chaque espace. Enfin la Section
5 met les r&#233;sultats en perspective et donne quelques pistes sur les travaux &#224; venir.
</p>
<p>2 Description des espaces s&#233;mantiques
</p>
<p>Les espaces s&#233;mantiques que nous utilisons pour l&#8217;induction de sens sont issus des travaux
de (Grefenstette, 2007). Au moment o&#249; nous avons effectuer nos travaux, le corpus est consti-
tu&#233; de deux millions d&#8217;urls de pages francophones sur lesquelles a &#233;t&#233; effectu&#233;e une analyse
syntaxique par le syst&#232;me d&#8217;analyse LIMA du CEA LIST (Besan&#231;on &amp; de Chalendar, 2005).
Ce syst&#232;me effectue une analyse en d&#233;pendances syntaxiques de type sujet_verbe, objet_verbe,
compl_du_nom, (...). Ces relations sont orient&#233;es, par exemple dans le syntagme traitement des
langues, langues appara&#238;t dans le contexte de traitement pour la relation compl_du_nom tandis
que traitement appara&#238;t dans le contexte de langues pour la relation compl_du_nom inverse.
</p>
<p>Le dictionnaire utilis&#233; pour la constitution des espaces s&#233;mantiques est constitu&#233; des 68000
mots les plus fr&#233;quents de la langue fran&#231;aise. On associe un espace distinct &#224; chaque relation,
enregistrant les fr&#233;quences de cooccurrences des 68000 mots avec les 68000 contextes ainsi
d&#233;finis pour cette relation. On donne ci-dessous un extrait des matrices COD_V et COMPDU-
NOM construites avec les phrases suivantes : On &#233;tudie actuellement les strat&#233;gies de traitement
de signaux afin d&#8217;obtenir les r&#233;sultats souhait&#233;s. (...) Le responsable d&#8217;un traitement de donn&#233;es doit
obtenir le consentement pr&#233;alable des personnes concern&#233;es avant toute utilisation de ces donn&#233;es. (...)
Il a obtenu un Doctorat en Traitement Automatique des Langues.
</p>
<p>COD_V COMPDUNOM
&#233;tudier obtenir strat&#233;gie traitement consentement utilisation
</p>
<p>strat&#233;gie 1 0 0 0 0 0
r&#233;sultat 0 1 0 0 0 0
consentement 0 1 0 0 0 0
doctorat 0 1 0 0 0 0
traitement 0 0 1 0 0 0
signal 0 0 0 1 0 0
donn&#233;e 0 0 0 1 0 1
personne 0 0 0 0 1 0
langue 0 0 0 1 0 0
</p>
<p>TAB. 1 &#8211; Remplissage des matrices
</p>
<p>Afin que l&#8217;information fournie par tous les mots, y compris les mots rares, puisse &#234;tre prise
en compte, nous travaillons avec des matrices d&#8217;informations mutuelles calcul&#233;es &#224; partir des
matrices de fr&#233;quence. L&#8217;information mutuelle est calcul&#233;e &#224; partir de la formule 1, o&#249; Pi est la
probabilit&#233; d&#8217;occurrence du terme d&#233;crit par la ligne i dans n&#8217;importe quel contexte de la relation
donn&#233;e, Pj est la probabilit&#233; d&#8217;occurrence du contexte d&#233;fini par la colonne j, et Pi, j est la
probabilit&#233; de cooccurrence du terme i avec le contexte j pour la relation donn&#233;e. L&#8217;information
mutuelle est positive si la probabilit&#233; de cooccurrence des termes i et j est plus grande que la
probabilit&#233; attendue si ces &#233;v&#233;nements &#233;taient ind&#233;pendants.
</p>
<p>MI = Pi,j &#8727; log(
Pi,j
</p>
<p>Pi &#8727; Pj
) (1)</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Claire Mouton
</p>
<p>La m&#234;me proc&#233;dure est effectu&#233;e pour les relations inverses, ainsi que pour les cooccurrences
dans des fen&#234;tres de taille fixe (5, 10, 20). Au final, on dispose de 71 matrices creuses et carr&#233;es
de 68000 dimensions. Nous gardons ces matrices s&#233;par&#233;es, distinguant ainsi 71 espaces s&#233;man-
tiques dans lesquels sont repr&#233;sent&#233;es les m&#234;mes donn&#233;es. On verra par la suite que ces espaces
conservent des informations diff&#233;rentes et compl&#233;mentaires.
</p>
<p>3 Plus Proches Voisins Approximatifs
</p>
<p>La taille de ces matrices n&#233;cessite une r&#233;duction du nombre de dimensions afin de travailler
sur des matrices de taille raisonnable. Les d&#233;compositions en valeurs singuli&#232;res des m&#233;thodes
d&#8217;Analyse S&#233;mantique Latente (Latent Semantic Analysis, LSA) d&#233;velopp&#233;es par (Landauer &amp;
Dumais, 1997) devenant assez lourdes sur nos matrices (complexit&#233; quadratique), nous nous
tournons vers des m&#233;thodes de r&#233;duction par Hachage Sensible &#224; la Localit&#233; (Locality Sensitive
Hashing, LSH), qui sont plus adapt&#233;es &#224; la taille de nos matrices.
</p>
<p>3.1 R&#233;duction de dimensions : Locality Sensitive Hashing
</p>
<p>(Charikar, 2002) definit une famille de fonctions LSH produisant des empreintes sur lesquelles
on peut calculer une approximation de la similarit&#233; cosinus beaucoup plus rapidement que dans
l&#8217;espace d&#8217;origine. De plus, (Ravichandran et al., 2005) montrent que ce hachage est particu-
li&#232;rement adapt&#233; pour mettre en place une m&#233;thode de recherche rapide de plus proches voisins
approximatifs. Nous reprenons ici les grandes lignes de la m&#233;thode de hachage.
</p>
<p>On tire d vecteurs unitaires&#8722;&#8594;r selon une distribution gaussienne. Ce tirage assure une r&#233;partition
&#233;quidistribu&#233;e sur l&#8217;hypersph&#232;re unitaire. Soit une famille de fonctions d&#233;finies par :
</p>
<p>h&#8722;&#8594;r (
&#8722;&#8594;u ) =
</p>
<p>{
0 if &#8722;&#8594;r .&#8722;&#8594;u &#8805; 0
</p>
<p>1 if &#8722;&#8594;r .&#8722;&#8594;u &lt; 0
(2)
</p>
<p>Soient deux vecteurs&#8722;&#8594;u et&#8722;&#8594;v , la probabilit&#233; de tirer un vecteur al&#233;atoire&#8722;&#8594;r d&#233;finissant un hyper
plan qui les s&#233;parera est &#233;gale &#224; :
</p>
<p>Pr[h&#8722;&#8594;r (
&#8722;&#8594;u ) 6= h&#8722;&#8594;r (
</p>
<p>&#8722;&#8594;v )] = &#952;(&#8722;&#8594;u ,&#8722;&#8594;v )/&#928; (3)
Sur un nombre d de vecteurs tir&#233;s al&#233;atoirement on peut mesurer cette probabilit&#233;. En effet, la
probabilit&#233; qu&#8217;un hyperplan tir&#233; al&#233;atoirement ait s&#233;par&#233; les deux vecteurs originaux u et v est
la probabilit&#233; que cet hyperplan ait donn&#233; un bit diff&#233;rent pour les deux r&#233;sultats du hachage de
u et v. La formule 4 nous donne cette probabilit&#233; :
</p>
<p>Pr[h&#8722;&#8594;r (
&#8722;&#8594;u ) 6= h&#8722;&#8594;r (
</p>
<p>&#8722;&#8594;v )] = distance_de_Hamming(&#8722;&#8594;u ,&#8722;&#8594;v )/d (4)
En combinant 3 et 4 on obtient donc l&#8217;approximation :
</p>
<p>cos(&#952;(&#8722;&#8594;u ,&#8722;&#8594;v )) &#8776; cos(distance_de_Hamming(&#8722;&#8594;u ,&#8722;&#8594;v )/d &#8727; &#928;) (5)
</p>
<p>3.2 Recherche rapide des plus proches voisins
</p>
<p>Une recherche rapide du plus proche voisin approximatif dans un espace muni d&#8217;une distance
de Hamming a &#233;t&#233; propos&#233;e par (Charikar, 2002) et reprise par (Ravichandran et al., 2005). La</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Induction de sens de mots &#224; partir de multiples espaces s&#233;mantiques
</p>
<p>m&#233;thode consiste &#224; tirer al&#233;atoirement p permutations de d &#233;l&#233;ments. Pour chaque permutation,
on permute les signatures bit &#224; bit, on proc&#232;de &#224; un tri lexicographique de tous les &#233;l&#233;ments et on
garde les B plus proches &#233;l&#233;ments des n &#233;l&#233;ments source dont l&#8217;approximation du cosinus est
inf&#233;rieur &#224; un certain seuil. Cela fonctionne car une signature permut&#233;e est une repr&#233;sentation
valide du vecteur d&#8217;origine. C&#8217;est en soi une signature par une famille de hachage.
</p>
<p>Nous remarquons que la m&#233;thode de recherche propos&#233;e par (Ravichandran et al., 2005) est
optimale lorsque l&#8217;on recherche les plus proches voisins de tous les &#233;l&#233;ments de la matrice. En
revanche elle ne permet pas de chercher les plus proches voisins d&#8217;un seul &#233;l&#233;ment sans avoir
&#224; calculer ceux de tous les &#233;l&#233;ments. Nous avons impl&#233;ment&#233; cette m&#233;thode en MPI1 afin de
lancer l&#8217;algorithme en parall&#232;le sur un cluster de machines et nous avons &#233;galement apport&#233;
deux am&#233;liorations sur l&#8217;algorithme de recherche lui-m&#234;me.
</p>
<p>Nous nous int&#233;ressons &#224; w0, terme dont on cherche les plus proches voisins. D&#8217;une part, si on
effectue un XOR de tous les &#233;l&#233;ments de la base avec le vecteur w0 alors on peut se permettre de
ne pas trier tous les &#233;l&#233;ments mais de n&#8217;extraire que les k premiers et de ne les trier eux-m&#234;mes
que par la suite. On a donc une complexit&#233; en O(p.n) &#224; la place de O(p.n.log(n)).
</p>
<p>D&#8217;autre part, le tri lexicographique sur un vecteur de bits prend en compte en moyenne deux
bits, ce qui n&#233;cessite d&#8217;avoir un tr&#232;s grand nombre p de permutations pour obtenir une bonne ap-
proximation. En effet, la probabilit&#233; qu&#8217;on ne prenne en compte que le premier bit est de 1/2, la
probabilit&#233; pour qu&#8217;on prenne en compte deux et seulement deux bits est de 1/4, la probabilit&#233;
pour qu&#8217;on prenne en compte k et seulement k bits est de (1/2)k. Le nombre moyen de bits pris
en compte est donc l&#8217;esp&#233;rance de la variable al&#233;atoire de loi de probabilit&#233; PX(i) = (1/2)i.
Cette esp&#233;rance est donn&#233;e par la s&#233;rie
</p>
<p>&#8721;n
k=1(pi.xi) =
</p>
<p>&#8721;n
k=1(k/2
</p>
<p>k) convergeant vers 2.
C&#8217;est pourquoi, pour les p permutations pour lesquelles on va extraire les B plus proches voi-
sins, on ne proc&#232;de pas &#224; une permutation bit &#224; bit mais &#224; une permutation de sous-parties de
signature pour lesquelles le tri se fera sur le nombre de bits de chaque sous-partie. Pour exemple,
si nous prenons des sous-parties de huit bits, on effectue une permutation sur ces sous-parties,
puis un tri lexicographique sur des valeurs variant de 0 &#224; 8. Le tri sur ces sous-parties conserve
la propri&#233;t&#233; de rapprocher des vecteurs pour lesquelles la distance de Hamming est faible. En
effet apr&#232;s le XOR, une sous-partie dont le compte de bits est &#233;gal &#224; 0 correspond &#224; une sous-
partie identique &#224; celle de la signature source.
Cela permettra de prendre en compte plus de bits lors du tri (la probabilit&#233; de prendre en compte
les 8 bits de la 1&#232;re sous-partie est de 1/2, les 16 bits de la premi&#232;re et deuxi&#232;me sous partie 1/4,
etc...) et donc de diminuer le nombre de permutations n&#233;cessaires &#224; la pr&#233;cision de l&#8217;algorithme.
Prenons des sous-parties dont le nombre de bits est &#233;gal &#224; &#945;&#8727;d. On obtient alors une complexit&#233;
en O(p.n&#945;.d) avec &#945;.d &lt;&lt; n pour l&#8217;extraction des b plus proches voisins d&#8217;un &#233;l&#233;ment source
et O(p.B + k. log(k)) pour le tri des k meilleurs parmi les p &#8727;B obtenus ce qui est n&#233;gligeable
car k &lt;&lt; n , soit au final une complexit&#233; en O(p1.n&#945;.d au lieu de O(p2.n.log(n)) pour la
m&#233;thode originale avec p1 &lt; p2 puisque notre m&#233;thode n&#233;cessite moins de permutations. Notre
m&#233;thode est donc plus efficace si on souhaite rechercher les plus proches voisins d&#8217;un &#233;l&#233;ment
source au coup par coup. En revanche, s&#8217;il s&#8217;agit de rechercher les plus proches voisins de tous
les &#233;l&#233;ments de la base (i.e. de calculer les valeurs de la matrice de similarit&#233; pour lesquels les
cosinus sont sup&#233;rieurs &#224; un certain seuil), on doit r&#233;p&#233;ter l&#8217;op&#233;ration n fois, et on a donc une
complexit&#233; en O(p1.n2&#945;.d) qui est sup&#233;rieure &#224; celle de l&#8217;algorithme original.
</p>
<p>1Message Passing Interface - http ://www-unix.mcs.anl.gov/mpi/</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Claire Mouton
</p>
<p>3.3 R&#233;sultats
</p>
<p>Nous avons ainsi pu calculer la liste des k plus proches voisins de mots polys&#233;miques pour
chacun des espaces syntaxiques et nous voyons par exemple dans le tableau 2 les r&#233;sultats &#224;
k=10 pour les mots barrage et vol dans diff&#233;rents espaces. Pour un mot comme barrage dont
</p>
<p>barrage COMPDUNOM barrage,infrastructure,am&#233;nagement,amont,b&#226;timent,station,canal,installation,r&#233;acteur,parcours
COD_V barrage,barri&#232;re,pont,b&#226;timent,chantier,centrale,usine,station,installation,banc
APPOS barrage,pont,canal,empierrement,d&#233;chetterie,digue,viaduc,mousqueterie,raz,&#233;cluse
APPOS.reverse barrage,digue,d&#233;versoir,pont,&#233;lectrolyseur,redresseur,lanier,route,lac,autoroute
</p>
<p>vol COMPDUNOM vol, avion, voyage, retour, course, achat, op&#233;ration, premi&#232;re, journ&#233;e, premier
COD_V vol, voyage, retour, cr&#233;ation, attaque, changement, mise, mariage, acte, fin
APPOS vol, meurtre, racket, fraude, chantage, assassinat, homicide, rapine, violence, crime
APPOS.reverse vol, meurtre, viol, prostitution, rapine, adult&#232;re, prox&#233;n&#233;tisme, idol&#226;trie, agression, luxure
</p>
<p>TAB. 2 &#8211; 10 plus proches voisins des mots barrage et vol
</p>
<p>les usages peuvent se d&#233;cliner en : construction sur un cours d&#8217;eau, infrastructure industrielle et
obstacle, les diff&#233;rents espaces retournent des listes de plus proches voisins o&#249; ces usages sont
non diff&#233;renci&#233;s. En revanche, on remarque que pour un mot comme vol les plus proches voisins
obtenus sont assez diff&#233;rents selon les espaces utilis&#233;s. Certains ram&#232;nent des plus proches
voisins orient&#233;s vers un sens pr&#233;cis. Les espaces contiennent donc des informations diff&#233;rentes
que nous supposons utiles de garder distinctes.
</p>
<p>4 Induction de sens de mots par clustering de mots multi-
repr&#233;sent&#233;s
</p>
<p>Dans notre approche, nous cherchons &#224; regrouper les plus proches voisins d&#8217;un mot source
dans des ensembles repr&#233;sentant chacun un usage. On souhaite parvenir &#224; distinguer diff&#233;rents
</p>
<p>FIG. 1 &#8211; Discrimination de sens dans l&#8217;espace compl&#233;ment_du_nom
</p>
<p>clusters comme dans l&#8217;exemple manuel de la Figure 1. Cette figure montre une projection &#224; trois
dimensions o&#249; les contextes devraient parvenir &#224; discriminer trois significations du mot barrage.
Dans cet exemple id&#233;al, l&#8217;utilisation d&#8217;un unique espace vectoriel suffit. Mais nous avons vu
plus haut que les diff&#233;rents espaces mettaient en relief diff&#233;rentes proximit&#233;s. Cette distinction
n&#8217;&#233;tant pas syst&#233;matique selon les espaces et les mots trait&#233;s, nous souhaitons prendre en compte
la sp&#233;cificit&#233; de chacun des espaces tout en permettant le regroupement inter-espaces. Pour
cela, nous nous inspirons des algorithmes de clustering Shared Nearest Neighbours adapt&#233;s et
utilis&#233;s par (Ert&#246;z et al., 2001) et (Ferret, 2004) ainsi que de l&#8217;algorithme Hyperlex d&#233;velopp&#233;
par (V&#233;ronis, 2003) et nous proposons deux m&#233;thodes de clustering par vote.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Induction de sens de mots &#224; partir de multiples espaces s&#233;mantiques
</p>
<p>Pour chacune des parties du discours pour lesquelles on souhaite induire des sens (substantif,
verbe, adjectif ), on conserve les espaces
&#8211; pertinents &#224; ces parties (e.g. lorsqu&#8217;on traite un verbe, on d&#233;laisse l&#8217;espace COMPDUNOM
</p>
<p>qui ne concerne que les substantifs et on utilise l&#8217;espace COD_V inverse et non l&#8217;espace
COD_V qui est enti&#232;rement creux pour les verbes cf. Tab 1)
</p>
<p>&#8211; et correspondant &#224; des relations entre mots pleins (substantif, verbe, adverbe, adjectif) (e.g.
on d&#233;laisse la relation reliant un d&#233;terminant &#224; son substantif).
</p>
<p>Apr&#232;s exp&#233;rimentation, nous choisissons de regrouper les mots qui apparaissent au moins deux
fois dans les 100 plus proches voisins des espaces choisis. Soit E cet ensemble.
</p>
<p>4.1 M&#233;thode inspir&#233;e de l&#8217;algorithme Shared Nearest Neighbours
</p>
<p>La m&#233;thode originale consiste &#224; extraire un certain nombre de noyaux non fix&#233; &#224; l&#8217;avance,
assigner les &#233;l&#233;ments restants &#224; ces noyaux, et enfin &#224; joindre les clusters dont les noyaux sont
trop proches et r&#233;assigner les &#233;lements appartenant &#224; des clusters jug&#233;s trop petits.
L&#8217;algorithme que nous proposons reprend la m&#233;thode de choix de noyaux de l&#8217;algorithme SNN
de (Ert&#246;z et al., 2001) pour lequel on n&#8217;a pas &#224; choisir le nombre de clusters a priori. Cependant
l&#8217;exp&#233;rimentation nous donnant de meilleurs r&#233;sultats pour un graphe de voisins directs qu&#8217;avec
le graphe des Plus Proches Voisins Partag&#233;s, nous garderons le premier. Une explication possible
concernant ce r&#233;sultat surprenant compar&#233; aux r&#233;sultats expos&#233;s dans (Ert&#246;z et al., 2001) est le
fait que nous n&#8217;utilisons qu&#8217;une petite partie des &#233;lements de l&#8217;espace d&#8217;origine, on a donc
trop peu de donn&#233;es pour exploiter ce second degr&#233; d&#8217;information. Nous conservons &#233;galement
l&#8217;id&#233;e de d&#233;finition des noyaux &#224; partir des liens forts. Le nom Shared Nearest Neighbours
n&#8217;ayant plus de raison d&#8217;&#234;tre, nous nous r&#233;f&#233;rerons d&#233;sormais &#224; cet algorithme sous le nom de
MultiNN. Dans chacun des espaces (COD_V, SUJ_V...) :
</p>
<p>1. On &#233;tablit le graphe des plus proches voisins dans lequel les noeuds sont les &#233;lements
de E et les arcs relient tous les noeuds entre eux et ont pour valeur la distance cosinus
approch&#233;e que l&#8217;on calcule entre les deux noeuds extr&#233;mit&#233;s.
</p>
<p>2. On d&#233;finit un seuil de lien fort (e.g. 0,2 de la distance maximale dans l&#8217;espace) et on
&#233;limine les arcs dont les valeurs sont inf&#233;rieures &#224; ce seuil.
</p>
<p>3. Pour chaque noeud du graphe on calcule la somme des valeurs de ses liens restants.
4. Si cette somme est plus grande qu&#8217;un certain seuil (e.g. 0.3 de la somme maximale dans
</p>
<p>l&#8217;espace), on dit que ce point est un noyau local pour cet espace.
Pour tout &#233;l&#233;ment de E, on sait le nombre d&#8217;espaces dans lesquels il est noyau local. Si ce
nombre est sup&#233;rieur &#224; un seuil (e.g. 0.8 des espaces utilis&#233;s), cet &#233;l&#233;ment est dit noyau global.
Nous constituons des clusters autour de chaque noyau global de la fa&#231;on suivante. Dans chaque
espace, on retire de E les &#233;l&#233;ments qui ont &#233;t&#233; d&#233;sign&#233;s noyaux globaux. Puis, dans chacun des
espaces et pour chaque &#233;l&#233;ment de E, on enregistre un vote pour le noyau global le plus proche
(cosinus approch&#233; le plus &#233;lev&#233;). On somme les votes sur tous les espaces et on assigne chaque
&#233;l&#233;ment &#224; son noyau le plus populaire (&#233;ventuellement &#224; plusieurs en cas d&#8217;&#233;galit&#233;).
Dans chaque espace, si deux noyaux globaux ont une valeur sup&#233;rieure &#224; un seuil donn&#233;, l&#8217;es-
pace vote pour la jonction des deux clusters associ&#233;s. Si un nombre suffisant d&#8217;espaces votent
pour cette jonction, les deux clusters sont regroup&#233;s. Les &#233;l&#233;ments des clusters consid&#233;r&#233;s trop
petits par rapport au nombre d&#8217;&#233;l&#233;ments &#224; regrouper sont r&#233;assign&#233;s un &#224; un aux gros clusters.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Claire Mouton
</p>
<p>4.2 M&#233;thode fond&#233;e sur l&#8217;algorithme HyperLex
</p>
<p>Nous adaptons une seconde m&#233;thode &#224; notre multi-repr&#233;sentation des mots. Il s&#8217;agit de l&#8217;algo-
rithme Hyperlex pr&#233;sent&#233; par (V&#233;ronis, 2003). Celui-ci est &#224; l&#8217;origine appliqu&#233; &#224; une liste de
cooccurrents mais dans notre cas nous l&#8217;appliquons &#224; une liste de plus proches voisins.
</p>
<p>L&#8217;algorithme original est le suivant. Soit E l&#8217;ensemble des mots &#224; regrouper. Pour chacun d&#8217;entre
eux, on calcule la distance au mot source. Le mot le plus proche devient noyau d&#8217;un cluster. On
attribue &#224; ce cluster tous les &#233;l&#233;ments appartenant &#224; la sph&#232;re de rayon &#981; (si leur nombre est
sup&#233;rieur &#224; un param&#232;tre k) et on retire ces &#233;l&#233;ments de E. Le mot de E le plus proche du mot
source devient noyau d&#8217;un autre cluster. On r&#233;it&#232;re jusqu&#8217;&#224; ce que E soit vide.
Avec nos multiples repr&#233;sentations, le calcul des distances des &#233;l&#233;ments de l&#8217;ensemble E par
rapport au mot source devient la moyenne des distances de tous les espaces. La sp&#233;cificit&#233; des
espaces n&#8217;est pas prise en compte &#224; ce niveau-l&#224;. En revanche, les &#233;l&#233;ments sont attribu&#233;s au
cluster uniquement si un certain nombre d&#8217;espaces valide cette attribution (vote).
</p>
<p>4.3 R&#233;sultats
</p>
<p>Les r&#233;sultats n&#8217;ont &#233;t&#233; &#233;valu&#233;s que manuellement dans cette premi&#232;re &#233;tape. Pour l&#8217;exemple du
mot barrage, nous disposons des r&#233;sultats fournis par les m&#233;thodes dont nous nous sommes
inspir&#233;es. Nous pr&#233;sentons ainsi nos r&#233;sultats pour ce mot dans le tableau 3. Nous avons privil&#233;-
gi&#233; un grand nombre d&#8217;&#233;l&#233;ments &#224; regrouper afin de rassembler le maximum de sens possibles.
Nous avons aussi opt&#233; pour un r&#233;sultat contenant un grand nombre de clusters afin d&#8217;en obtenir
le maximum possible de pr&#233;cis, quitte &#224; en obtenir certains qui pourraient encore &#234;tre regroup&#233;s.
Un classifieur destin&#233; &#224; faire de la d&#233;sambigu&#239;sation et apprenant sur ces donn&#233;es saura ne pas
classifier de mot dans l&#8217;une de ces classes si les &#233;l&#233;ments de celle-ci sont trop clairsem&#233;s et ont
des &#233;quivalents dans d&#8217;autres clusters.
La comparaison de nos clusters avec ceux des algorithmes d&#8217;origine reste difficile puisque nous
ne cherchons pas &#224; regrouper le m&#234;me type de terme (cooccurrents vs. plus proches voisins syn-
taxiques). Cependant nous pouvons voir que les sens distingu&#233;s ne sont pas toujours les m&#234;mes.
Nous retrouvons d&#8217;une part en 3.1, 3.5 et 3.10 et d&#8217;autre part en 4.4, 4.11 et 4.12 les sens de
barrage correspondant au barrage routier, frontalier ou policier, que l&#8217;on ne distingue pas bien
entre eux. Le Petit Larousse utilis&#233; dans la campagne ROMANSEVAL (Segond, 2000) ne fait
lui-m&#234;me pas la distinction et consid&#232;re simplement ceux-ci comme obstacle. En 3.4, 4.5, 4.6,
et 4.9, l&#8217;usage du barrage hydraulique est distingu&#233; en tant qu&#8217;infrastructure industrielle et en
3.7 et 4.8 en tant que construction sur un cours d&#8217;eau. Il s&#8217;agit du m&#234;me objet physique mais
l&#8217;usage est diff&#233;rent. En revanche on ne retrouve pas le sens du match de barrage pour lequel
il existe tr&#232;s peu de mots partageant les m&#234;mes contextes syntaxiques. Pour le mot vol, notre
algorithme extrait correctement les sens d&#233;lit de vol et vol a&#233;rien, ce qui n&#8217;&#233;tait pas le cas pour
l&#8217;algorithme HyperLex.
</p>
<p>Une &#233;valuation plus pertinente de ces diff&#233;rents types de clusters serait de les utiliser dans une
t&#226;che applicative (d&#233;sambigu&#239;sation de sens ou recherche d&#8217;information) et d&#8217;&#233;valuer les r&#233;sul-
tats de celle-ci. Une telle &#233;valuation automatique doit encore &#234;tre mise en place pour estimer la
meilleure approche et &#233;valuer la qualit&#233; de discrimination des clusters produits.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Induction de sens de mots &#224; partir de multiples espaces s&#233;mantiques
</p>
<p>Mot-source barrage
HyperLex 1.1 : eau, construction, ouvrage, rivi&#232;re, projet, retenue, crue
</p>
<p>(V&#233;ronis, 2003) 1.2 : routier, v&#233;hicule, camion, membre, conducteur, policier, groupement
1.3 : fronti&#232;re, Alg&#233;rie, militaire, efficacit&#233;, arm&#233;e, Suisse, poste
1.4 : match, vainqueur, victoire, rencontre, qualification, tir, football
</p>
<p>SNN 2.1 : manifestant, forces_de_l&#8217;ordre, pr&#233;fecture, agriculteur, protester, incendier, calme, pierre
(Ferret, 2004) 2.2 : conducteur, routier, v&#233;hicule, poids_lourd, camion, permis, trafic, bloquer, voiture, autoroute
</p>
<p>2.3 : fleuve, lac, rivi&#232;re, bassin, m&#232;tre_cube, crue, amont, pollution, affluent, saumon, poisson
2.4 : bless&#233;, casque_bleu, soldat, milicien, tir, milice, convoi, &#233;vacuer, croate, milicien, combattant
</p>
<p>MultiHyperLex 3.1 : conflit, barri&#232;re, liaison, fronti&#232;re, transport, environnement, division, combat, n&#233;gociation, &#233;change
3.2 : instrument, mat&#233;riau, m&#233;dicament, mat&#233;riel, technologie, &#233;quipement, dispositif
3.3 : poste, autoroute, mine, train, infrastructure, usine, distribution, logement, consommation, bateau, pression, commerce, b&#226;timent,
installation, accord, march&#233;, contr&#244;le
3.4 : quartier, tunnel, centrale, port, station, commune
3.5 : regroupement, arme, d&#233;fense, acc&#232;s, &#233;tablissement, am&#233;nagement
3.6 : r&#233;seau, immeuble, proximit&#233;, financement, secteur
3.7 : ruisseau, &#233;cluse, digue, ville, route, &#238;le, parc, pont, sol, chemin, plage, for&#234;t, canal, &#233;tang, lac, rivi&#232;re
3.8 : tir, stockage, lutte, foyer, contrainte, chantier, &#233;difice, bassin, co&#251;t, &#233;tape, d&#233;veloppement, succession, possibilit&#233;, construction,
d&#233;p&#244;t, sortie, cours, marche
3.9 : tour, h&#244;pital, habitation, obstacle, concurrence, impact, appui, exploitation, pr&#233;sent, terrain
3.10 : zone, p&#233;rim&#232;tre, ouvrage, limite, s&#233;curit&#233;, unit&#233;
</p>
<p>MultiNN 4.1 : am&#233;nagement, amont, implantation, p&#233;rim&#232;tre, proximit&#233;, &#233;difice, emplacement, habitation, stockage
4.2 : commerce, couverture, d&#233;chet, trafic, chantier, conflit, fronti&#232;re, distribution, environnement, infrastructure, logement, transport,
concurrence, mine, p&#234;che, poste
4.3 : d&#233;veloppement, avantage, conflit, division, lutte, phase, pr&#233;sent, proximit&#233;, accord, construction, contr&#244;le, cours, d&#233;part, envi-
ronnement, essai, technologie
4.4 : &#233;change, combat, conflit, liaison, ouverture, retrait
4.5 : &#233;quipement, dispositif, instrument, op&#233;ration, station, mat&#233;riel, d&#233;placement, emplacement, installation, mat&#233;riau, m&#233;-
dicament, ressource, stockage, train, usine, v&#233;hicule
4.6 : &#233;tablissement, division, fondation, r&#233;serve, station, commune, d&#233;p&#244;t, exploitation, foyer, h&#244;pital, immeuble, port, usine
4.7 : financement, couverture, entente, impact, regroupement, transfert, appui, concurrence, consommation, co&#251;t, investissement,
n&#233;gociation, revenu, soutien, succession
4.8 : for&#234;t, bloc, ruisseau, &#233;tang, plantation, lac, rivi&#232;re, bassin, &#233;difice, parc, plage, sol
4.9 : pont, chauss&#233;e, colonne, digue, tunnel, autoroute, canal, port
4.10 : possibilit&#233;, contrainte, &#233;tape, impact, limite, op&#233;ration, obstacle
4.11 : secteur, division, impact, proximit&#233;, section, zone, environnement, exploitation, march&#233;, r&#233;seau, terrain
4.12 : s&#233;curit&#233;, couverture, op&#233;ration, pr&#233;sent, protection, proximit&#233;, unit&#233;, acc&#232;s, d&#233;fense, marche, poste
4.13 : ville, arme, centre, environ, grande, proximit&#233;, tour, bateau, b&#226;timent, chemin, feu, &#238;le, port, quartier, route, village
</p>
<p>TAB. 3 &#8211; Comparatif des clusters construits &#224; partir du mot barrage
</p>
<p>5 Discussions et Perspectives
</p>
<p>Bien que nous nous soyons d&#233;gag&#233;s du choix du nombre de clusters &#224; obtenir afin de permettre
un nombre de sens diff&#233;rents selon les mots, le choix des param&#232;tres de ces deux algorithmes
reste un probl&#232;me dans le sens o&#249; nous n&#8217;avons pour l&#8217;instant pas de m&#233;thode pour apprendre
les param&#232;tres optimaux et nous restons ainsi contraints de les choisir par l&#8217;exp&#233;rimentation.
</p>
<p>Un avantage des m&#233;thodes pr&#233;sent&#233;es peut &#234;tre mis en avant : nous supposons que le fait d&#8217;utili-
ser les plus proches voisins comme ensemble d&#8217;&#233;l&#233;ments &#224; regrouper (et non les cooccurrents),
permettra d&#8217;utiliser ces voisins comme donn&#233;es d&#8217;apprentissage pour un classifieur destin&#233; &#224; la
d&#233;sambigu&#239;sation de termes ambig&#252;s apparaissant dans un nouveau texte. Cette hypoth&#232;se sera
exp&#233;riment&#233;e tr&#232;s prochainement dans la suite de ces travaux.
</p>
<p>Par ailleurs, la finalisation de ces travaux n&#233;cessite diverses &#233;tudes &#224; mener rapidement. Afin de
mettre en valeur les apports de la m&#233;thode pr&#233;sent&#233;e ici, nous souhaitons former des clusters &#224;
partir des m&#234;mes ensembles de plus proches voisins que dans cette &#233;tude mais en utilisant d&#8217;une
part l&#8217;espace de cooccurrences de fen&#234;tres seul ainsi que chacun des espaces syntaxiques seul,
et en utilisant d&#8217;autre part les matrices concat&#233;n&#233;es.
</p>
<p>Enfin, nous projetons d&#8217;utiliser ces clusters de sens pour d&#233;sambigu&#239;ser et indexer une collection</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Claire Mouton
</p>
<p>de documents et &#233;tudier l&#8217;apport de cette d&#233;sambigu&#239;sation dans la recherche d&#8217;information.
</p>
<p>R&#233;f&#233;rences
BESAN&#199;ON R. &amp; DE CHALENDAR G. (2005). L&#8217;analyseur syntaxique de lima dans la cam-
pagne d&#8217;&#233;valuation easy. In M. JARDINO, Ed., Actes de TALN 2005 (Traitement automatique
des langues naturelles), Dourdan : ATALA LIMSI.
CHARIKAR M. (2002). Similarity estimation techniques from rounding algorithms. In Pro-
ceedings of the 34th Annual ACM Symposium on Theory of Computing.
ERT&#214;Z L., STEINBACH M. &amp; KUMAR V. (2001). Finding topics in collections of documents :
A shared nearest neighbor approach. In Text Mine 01, Workshop of the 1st SIAM International
Conference on Data Mining.
FERRET O. (2004). Discovering word senses from a network of lexical cooccurrences. In
COLING &#8217;04 : Proceedings of the 20th international conference on Computational Linguistics,
p. 1326, Morristown, NJ, USA : Association for Computational Linguistics.
GREFENSTETTE G. (2007). Conquering language : Using nlp on a massive scale to build high
dimensional language models from the web. In Proceedings of the 8th CICLing Conference,
p. 35&#8211;49, Mexico.
HARRIS Z. (1985). Distributional structure. In J. J. KATZ, Ed., The Philosophy of Linguistics,
p. 26&#8211;47. New York : Oxford University Press.
LANDAUER T. K. &amp; DUMAIS S. (1997). A solution to plato&#8217;s problem : The latent semantic
analysis theory of the acquisition, induction, and representation of knowledge. Psychlogical
Review, 104(2), 211&#8211;240.
LIN D. (1998). Automatic retrieval and clustering of similar words. In Proceedings of
COLING-ACL98, p. 768&#8211;774.
LUND K. &amp; BURGESS C. (1996). Producing high-dimensional semantic spaces from lexical
co-occurrence. Behavior Research Methods, Instruments, and Computers, 28, 203&#8211;208.
PAD&#211; S. &amp; LAPATA M. (2007). Dependency-based construction of semantic space models.
Comput. Linguist., 33(2), 161&#8211;199.
PANTEL P. &amp; LIN D. (2002). Discovering word senses from text. In Proceedings of ACM
SIGKDD Conference on Knowledge Discovery and Data Mining 2002, Edmonton, Canada.
RAVICHANDRAN D., PANTEL P. &amp; HOVY E. (2005). Randomized algorithms and nlp : Using
locality sensitive hash functions for high speed noun clustering. In Proceedings of ACL, Ann
Arbour(MI).
SALTON G., WONG A. &amp; YANG C. S. (1975). A vector space model for automatic indexing.
18(11), 613&#8211;620.
SCH&#220;TZE H. (1998). Automatic word sense discrimination. Computational Linguistics, 24(1),
97&#8211;123.
SEGOND F. (2000). Framework and results for french. Computers and the Humanities, Special
Issue on SENSEVAL, 34(1).
V&#201;RONIS J. (2003). Cartographie lexicale pour la recherche d&#8217;information. In B. DAILLE,
Ed., Actes de TALN 2003 (Traitement automatique des langues naturelles), p. 265&#8211;274, Batz-
sur-mer : ATALA IRIN.</p>

</div></div>
</body></html>