RECITAL 2009, Senlis, 24-26 juin 2009

Induction de sens de mots 51 partir de multiples espaces
sémantiques

Claire Mouton”
(1) CEA/LIST/LIC2M - BP 6 92265 Fontenay-aux-Roses Cedex
(2) Exalead S.A. - 10 place de la Madeleine - 75008 Paris
Claire.Mouton@cea.fr, Claire.Mouton@exalead.com

Resume. Les mots sont souvent porteurs de plusieurs sens. Pour traiter l’information cor-
rectement, un ordinateur doit etre capable de decider quel sens d’un mot est employe a chacune
de ses occurrences. Ce probleme non parfaitement resolu a genere beaucoup de travaux sur la
desambiguisation du sens des mots (Word Sense Disambiguation) et dans la generation d’es-
paces semantiques dont un des buts est de distinguer ces differents sens. Nous nous inspirons
ici de deux methodes existantes de detection automatique des differents usages et/ou sens des
mots, pour les appliquer a des espaces semantiques issus d’une analyse syntaxique effectuee
sur un tres grand nombre de pages web. Les adaptations et resultats presentes dans cet article se
distinguent par le fait d’utiliser non plus une seule representation mais une combinaison de mul-
tiples espaces de forte dimensionnalite. Ces multiples representations etant en competition entre
elles, elles participent chacune par vote a l’induction des sens lors de la phase de clustering.

Abstract. Words can have many senses. In order to process information correctly, a com-
puter should be able to decide which sense of a word is used in a given context. This unsolved
problem has generated much research in word sense disambiguation and in the generation of
semantic spaces in order to separate possible meanings. Here, we adapt two existing methods
to automatically distinguish words uses and senses. We apply them to multiple semantic spaces
produced by a syntactic analysis of a very large number of web pages. These adaptations and the
results presented in this article differ from the original methods in that they use a combination of
several high dimensional spaces instead of one single representation. Each of these competing
semantic spaces takes part in a clustering phase in which they vote on sense induction.

M0tS-CIéS I espace semantique, reduction de dimensions, Locality Sensitive Hashing,
induction de sens, clustering de mots, objets multi-representes.

Keywords: semantic space, dimensionality reduction, Locality Sensitive Hashing, Word
Sense Induction, words clustering, multi-represented data.

1 Introduction

Un meme mot peut parfois etre porteur de differents sens (polysemie) tandis que deux mots
distincts sont parfois porteurs du meme sens (synonymie). L’interpretation que les humains
font a l’aide du contexte discursif ou environnemental leur permet de distinguer l’emploi d’un
sens par rapport a un autre ainsi que la reference identique a un meme concept par deux mots
differents. Differentes approches permettent aux machines de modeliser cette distance entre

Claire Mouton

les sens d’un meme mot ou cette proximité sémantique entre les mots. Ces approches peuvent
etre fondées sur des ressources constituées manuellement (ontologies, réseaux lexicaux) ou sur
des ressources automatiques comme les espaces sémantiques auxquels nous nous intéressons
dans ce travail. Nous pensons que la projection des mots dans des espaces vectoriels permet
non seulement de retrouver cette distance mais aussi de regrouper des ensembles de mots qui
pourront déﬁnir les différents sens des mots polysémiques. C’est l’induction de ces sens (Word
Sense Induction, WSI) qui fait l’objet de notre travail.

Le paradigme des espaces sémantiques est fondé sur l’hypothese que le sens porté par les mots
depend de leur contexte (Harris, 1985). Un contexte peut se concevoir a différentes échelles,
on peut donc déﬁnir différents types de contextes. Dans (Salton et al., 1975), les contextes du
Vector Space Model sont les documents dans lesquels apparaissent les termes du vocabulaire.
On stocke donc dans une matrice de similarité de terme général f,-j la fréquence d’apparition
du terme i dans le document j. Les termes du vocabulaire représentés par les lignes peuvent
ainsi etre projetés dans un espace sémantique ou les dimensions correspondent aux différentes
colonnes de la matrice et les coordonnées sont données par les valeurs de la matrice. Ainsi, plus
deux termes apparaissent fréquemment dans les memes documents, plus ils seront proches dans
l’espace sémantique. Dans (Lund & Burgess, 1996), les colonnes correspondent aux termes
les plus frequents du vocabulaire et la matrice stocke les comptes de cooccurrences entre les
termes des lignes et ceux des colonnes sur des fenetres de taille ﬁxe (n mots consécutifs apres
avoir retire les mots vides tels que les prépositions, pronoms, déterminants...). Plus deux termes
apparaissent a proximité des memes termes, plus ils seront proches dans l’espace sémantique.
Dans (Pado & Lapata, 2007) et (Grefenstette, 2007), un contexte est une relation syntaxique
associée a un mot du vocabulaire. Nous donnons pour exemple le contexte sujet de manger.
Tous les mots apparaissant dans ce contexte ont une certaine similarité sémantique (entite’s
capables de manger) qui sera nuancée par l’ensemble des autres contextes.

A partir de ces espaces sémantiques o1‘1 chaque terme possede une signature (son vecteur ligne)
qui le rend plus ou moins proche d’un autre, on peut regrouper les termes entre eux en fonction
de leur similarité. Induire des sens a partir de mots polysémiques peut se faire a partir de diffé-
rentes selections de termes. Dans (Lin, 1998), tout le vocabulaire est réparti automatiquement
(clustering) en ensembles de sens proches formant ainsi des classes de synonymes. Chaque mot
pouvant appartenir a plusieurs ensembles, les mots polysémiques voient ainsi leurs sens discri-
Ininés. L’ approche de (Schiitze, 1998) consiste a effectuer un clustering sur un certain nombre
d’instances du mot pour lequel on veut induire des sens, en utilisant leurs contextes d’occur-
rences dans un corpus donné. Enﬁn, certains comme (Véronis, 2003), (Ferret, 2004) effectuent
un clustering sur les meilleurs cooccurrents des mots a distinguer en sens tandis que (Pantel &
Lin, 2002) regroupent leurs plus proches voisins.

Notre travail s’inscrit dans la lignée de ceux de (Pantel & Lin, 2002) car nous regroupons aussi
des plus proches voisins mais il se différencie par la distinction de plusieurs espaces séman-
tiques dont nous combinons les spéciﬁcités. En effet, jusqu’a présent les travaux faisant usage
des espaces sémantiques fondés sur les cooccurrences syntaxiques traitaient tous les contextes
syntaxiques de la meme facon, indépendemment du type de relation syntaxique formant le
contexte (la seule distinction résidait dans le fait d’utiliser une relation ou non). Notre travail
montre la variabilité des distances entre les mots selon les relations syntaxiques utilisées et ex-
plore les apports de la prise en compte spéciﬁque des différentes relations. Il reprend en outre
un algorithme de recherche rapide de plus proches voisins approximatifs développé par (Ra-
vichandran et al., 2005) et le modiﬁe pour d’une part réduire sa complexité algorithmique et
d’autre part distribuer les calculs sur les noeuds d’un cluster. Nous montrons que notre version

Induction de sens de mots a partir de multiples espaces sémantiques

modiﬁée conserve autant d’information que la version originale.

La Section 2 de cet article détaille la conception des espaces sémantiques que nous utilisons.
Nous présentons dans la Section 3 une méthode rapide de recherche des plus proches voisins
ainsi que la méthode de réduction de dimensions associée. La Section 4 décrit les méthodes de
clustering développées pour prendre en compte les disparités de chaque espace. Enﬁn la Section
5 met les résultats en perspective et donne quelques pistes sur les travaux a venir.

2 Description des espaces sémantiques

Les espaces sémantiques que nous utilisons pour l’induction de sens sont issus des travaux
de (Grefenstette, 2007). Au moment o1‘1 nous avons effectuer nos travaux, le corpus est consti-
tué de deux millions d’urls de pages francophones sur lesquelles a été effectuée une analyse
syntaxique par le systeme d’analyse LIMA du CEA LIST (Besancon & de Chalendar, 2005).
Ce systeme effectue une analyse en dépendances syntaxiques de type sujet_verbe, 0bjet_verbe,
c0mpl_du_n0m, (...). Ces relations sont orientées, par exemple dans le syntagme traitement des
langues, langues apparait dans le contexte de traitement pour la relation c0mpl_du_n0m tandis
que traitement apparait dans le contexte de langues pour la relation c0mpl_du_n0m inverse.

Le dictionnaire utilisé pour la constitution des espaces sémantiques est constitué des 68000
mots les plus fréquents de la langue francaise. On associe un espace distinct a chaque relation,
enregistrant les fréquences de cooccurrences des 68000 mots avec les 68000 contextes ainsi
déﬁnis pour cette relation. On donne ci-dessous un extrait des matrices COD_V et COMPDU-
NOM construites avec les phrases suivantes : On étudie actuellement les stratégies de traitement
de signaux aﬁn d’obtenir les résultats souhaités. () Le responsable d’im traitement de données doit
obtenir le consentement préalable des personnes concemées avant toute utilisation de ces données. ()
II a obtenu im Doctorat en Traitement Automatique des Langues.

COD_V COMPDUNOM

étudier obtenir stratégie traitement consentement utilisation
stratégie 1 0 0 0 0 0
résultat 0 1 0 0 0 0
consentement 0 1 0 0 0 0
doctorat 0 1 0 0 0 0
traitement 0 0 1 0 0 0
signal 0 0 0 1 0 0
donnée 0 0 0 1 0 1
personne 0 0 0 0 1 0
langue 0 0 0 1 0 0

TAB. 1 — Remplissage des matrices

Aﬁn que l’information foumie par tous les mots, y compris les mots rares, puisse étre prise
en compte, nous travaillons avec des matrices d’informations mutuelles calculées a partir des
matrices de fréquence. L’ information mutuelle est calculée a partir de la formule 1, o1‘1 Pi est la
probabilité d’occurrence du terme décrit par la ligne i dans n’importe quel contexte de la relation
donnée, Pj est la probabilité d’occurrence du contexte déﬁni par la colonne j, et Pi, j est la
probabilité de cooccurrence du terme i avec le contexte j pour la relation donnée. L’ information
mutuelle est positive si la probabilité de cooccurrence des termes 1' et j est plus grande que la
probabilité attendue si ces événements étaient indépendants.

Pi-
MI=P-- ” 1
Z,]*l0g(Pi*Pj) ()

Claire Mouton

La meme procédure est effectuée pour les relations inverses, ainsi que pour les cooccurrences
dans des fenétres de taille ﬁxe (5, 10, 20). Au ﬁnal, on dispose de 71 matrices creuses et carrées
de 68000 dimensions. Nous gardons ces matrices séparées, distinguant ainsi 71 espaces seman-
tiques dans lesquels sont représentées les memes données. On verra par la suite que ces espaces
conservent des informations différentes et complémentaires.

3 Plus Proches Voisins Approximatifs

La taille de ces matrices nécessite une réduction du nombre de dimensions aﬁn de travailler
sur des matrices de taille raisonnable. Les décompositions en valeurs singulieres des méthodes
d’Analyse Sémantique Latente (Latent Semantic Analysis, LSA) développées par (Landauer &
Dumais, 1997) devenant assez lourdes sur nos matrices (complexité quadratique), nous nous
toumons vers des méthodes de réduction par Hachage Sensible a la Localité (Locality Sensitive
Hashing, LSH), qui sont plus adaptées a la taille de nos matrices.

3.1 Réduction de dimensions : Locality Sensitive Hashing

(Charikar, 2002) deﬁnit une famille de fonctions LSH produisant des empreintes sur lesquelles
on peut calculer une approximation de la similarité cosinus beaucoup plus rapidement que dans
l’espace d’origine. De plus, (Ravichandran et al., 2005) montrent que ce hachage est particu-
lierement adapté pour mettre en place une méthode de recherche rapide de plus proches Voisins
approximatifs. Nous reprenons ici les grandes lignes de la méthode de hachage.

On tire d vecteurs unitaires 7 selon une distribution gaussienne. Ce tirage assure une répartition
équidistribuée sur l’hypersphere unitaire. Soit une famille de fonctions déﬁnies par :

_, _ 0tf7’.E’20
h7(”)_ 1tf7’E’<0 (2)

Soient deux vecteurs 7 et W, la probabilité de tirer un vecteur aléatoire 7 déﬁnissant un hyper
plan qui les séparera est égale a :

PT[h7>(7) 75 h7>(7)] = 9W» 7)/H (3)

Sur un nombre d de vecteurs tirés aléatoirement on peut mesurer cette probabilité. En effet, la
probabilité qu’un hyperplan tiré aléatoirement ait séparé les deux vecteurs originaux u et 11 est
la probabilité que cet hyperplan ait donné un bit different pour les deux résultats du hachage de
a et 11. La formule 4 nous donne cette probabilité :

Pr[h7>(W) 75 h7>(7)] = distance_de_Hamming(U, 7)/d (4)
En combinant 3 et 4 on obtient donc l’approximation :

cos(0(7, 7)) m cos(distance_de_Hamming(U, 7)/d >«< H) (5)

3.2 Recherche rapide des plus proches Voisins

Une recherche rapide du plus proche voisin approximatif dans un espace muni d’une distance
de Hamming a été proposée par (Charikar, 2002) et reprise par (Ravichandran et al., 2005). La

Induction de sens de mots a partir de multiples espaces sémantiques

méthode consiste a tirer aléatoirement p permutations de d éléments. Pour chaque permutation,
on permute les signatures bit a bit, on procede a un tri lexicographique de tous les éléments et on
garde les B plus proches éléments des n elements source dont l’approximation du cosinus est
inférieur a un certain seuil. Cela fonctionne car une signature permutée est une représentation
valide du vecteur d’origine. C’est en soi une signature par une famille de hachage.

Nous remarquons que la méthode de recherche proposée par (Ravichandran et al., 2005) est
optimale lorsque l’on recherche les plus proches voisins de tous les éléments de la matrice. En
revanche elle ne permet pas de chercher les plus proches voisins d’un seul élément sans avoir
a calculer ceux de tous les éléments. Nous avons implémenté cette méthode en MPI1 aﬁn de
lancer l’algorithme en parallele sur un cluster de machines et nous avons également apporté
deux améliorations sur l’algorithme de recherche lui-meme.

Nous nous intéressons a wo, terme dont on cherche les plus proches voisins. D’une part, si on
effectue un XOR de tous les éléments de la base avec le vecteur wo alors on peut se permettre de
ne pas trier tous les éléments mais de n’extraire que les is premiers et de ne les trier eux-memes
que par la suite. On a donc une complexité en O(p.n) a la place de O(p.n.l0g(n)).

D’autre part, le tri lexicographique sur un vecteur de bits prend en compte en moyenne deux
bits, ce qui nécessite d’avoir un tres grand nombre p de permutations pour obtenir une bonne ap-
proximation. En effet, la probabilité qu’on ne prenne en compte que le premier bit est de 1/2, la
probabilité pour qu’on prenne en compte deux et seulement deux bits est de 1 / 4, la probabilité
pour qu’on prenne en compte k et seulement k bits est de (1 /2)’? Le nombre moyen de bits pris
en compte est donc l’espérance de la variable aléatoire de loi de probabilité PX  = (1 / 2)".
Cette espérance est donnée par la série Z221  = Z::1(k/2'“) convergeant vers 2.

C’est pourquoi, pour les p permutations pour lesquelles on Va extraire les B plus proches voi-
sins, on ne procede pas a une permutation bit a bit mais a une permutation de sous-parties de
signature pour lesquelles le tri se fera sur le nombre de bits de chaque sous-partie. Pour exemple,
si nous prenons des sous-parties de huit bits, on effectue une permutation sur ces sous-parties,
puis un tri lexicographique sur des valeurs variant de 0 a 8. Le tri sur ces sous-parties conserve
la propriété de rapprocher des vecteurs pour lesquelles la distance de Hamming est faible. En
effet apres le XOR, une sous-partie dont le compte de bits est égal a 0 correspond a une sous-
partie identique a celle de la signature source.

Cela permettra de prendre en compte plus de bits lors du tri (la probabilité de prendre en compte
les 8 bits de la lere sous-partie est de 1/2, les 16 bits de la premiere et deuxieme sous partie 1/4,
etc...) et donc de diIr1inuer le nombre de permutations nécessaires a la précision de l’algorithme.

Prenons des sous-parties dont le nombre de bits est égal a oz * d. On obtient alors une complexité
en O(p.na.d) avec a.d << n pour l’extraction des b plus proches voisins d’un élément source
et O(p.B + k.1og(k)) pour le tri des k meilleurs parmi les p * B obtenus ce qui est négligeable
car k << n , soit au ﬁnal une complexité en O(p1.n0z.d au lieu de O(p2.n.l0g(n)) pour la
méthode originale avec pl < pg puisque notre méthode nécessite moins de permutations. Notre
méthode est donc plus efﬁcace si on souhaite rechercher les plus proches voisins d’un élément
source au coup par coup. En revanche, s’il s’agit de rechercher les plus proches voisins de tous
les éléments de la base (i.e. de calculer les valeurs de la matrice de similarité pour lesquels les
cosinus sont supérieurs a un certain seuil), on doit répéter l’opération n fois, et on a donc une
complexité en O(p1.n20z.d) qui est supérieure a celle de l’algorithme original.

1Message Passing Interface - http ://www—unix.mcs.anl.goV/mpi/

Claire Mouton

3.3 Résultats

Nous avons ainsi pu calculer la liste des is plus proches voisins de mots polysémiques pour
chacun des espaces syntaxiques et nous voyons par exemple dans le tableau 2 les résultats a
k=10 pour les mots barrage et vol dans différents espaces. Pour un mot comme barrage dont

retour, course,
retour,
meurtre,
meurtre,

 

TAB. 2 — 10 plus proches voisins des mots barrage et vol

les usages peuvent se décliner en : construction sur un cours d ’eau, infrastructure industrielle et
obstacle, les différents espaces retournent des listes de plus proches voisins o1‘1 ces usages sont
non différenciés. En revanche, on remarque que pour un mot comme vol les plus proches voisins
obtenus sont assez différents selon les espaces utilisés. Certains ramenent des plus proches
voisins orientés vers un sens précis. Les espaces contiennent donc des informations différentes
que nous supposons utiles de garder distinctes.

4 Induction de sens de mots par clustering de mots multi-
représentés

Dans notre approche, nous cherchons a regrouper les plus proches voisins d’un mot source
dans des ensembles représentant chacun un usage. On souhaite parvenir a distinguer différents

ce11l1‘a.le P "duh"

  
  

explaimﬁnii

station "mag:
tljgue
ol.1.-zirutle f ﬁ_
mare
BARRAGE
attnqm

FIG. 1 — Discrimination de sens dans l’espace comple’ment_du_nom

clusters comme dans l’exemple manuel de la Figure 1. Cette ﬁgure montre une projection a trois
dimensions o1‘1 les contextes devraient parvenir a discriminer trois signiﬁcations du mot barrage.
Dans cet exemple idéal, l’utilisation d’un unique espace vectoriel sufﬁt. Mais nous avons vu
plus haut que les différents espaces mettaient en relief différentes proxirnités. Cette distinction
n’étant pas systématique selon les espaces et les mots traités, nous souhaitons prendre en compte
la spéciﬁcité de chacun des espaces tout en permettant le regroupement inter-espaces. Pour
cela, nous nous inspirons des algorithmes de clustering Shared Nearest Neighbours adaptés et
utilisés par (Ertoz et al., 2001) et (Ferret, 2004) ainsi que de l’algorithme Hyperlex développé
par (Véronis, 2003) et nous proposons deux méthodes de clustering par vote.

Induction de sens de mots a partir de multiples espaces sémantiques

Pour chacune des parties du discours pour lesquelles on souhaite induire des sens (substantiﬁ

verbe, adjectzf), on conserve les espaces

— pertinents a ces parties (e.g. lorsqu’on traite un verbe, on délaisse l’espace COMPDUNOM
qui ne conceme que les substantifs et on utilise l’espace COD_V inverse et non l’espace
COD_V qui est entierement creux pour les verbes cf. Tab 1)

— et correspondant a des relations entre mots pleins (substantif, verbe, adverbe, adjectif) (e.g.
on délaisse la relation reliant un déterminant a son substantif).

Apres experimentation, nous choisissons de regrouper les mots qui apparaissent au moins deux

fois dans les 100 plus proches voisins des espaces choisis. Soit E cet ensemble.

4.1 Méthode inspirée de l’algorithme Shared Nearest Neighbours

La méthode originale consiste a extraire un certain nombre de noyaux non ﬁxé a l’avance,
assigner les éléments restants a ces noyaux, et enﬁn a joindre les clusters dont les noyaux sont
trop proches et réassigner les élements appartenant a des clusters jugés trop petits.

L’ algorithme que nous proposons reprend la méthode de choix de noyaux de l’algorithme SNN
de (Ertoz et al., 2001) pour lequel on n’a pas a choisir le nombre de clusters a priori. Cependant
l’expérimentation nous donnant de meilleurs résultats pour un graphe de voisins directs qu’avec
le graphe des Plus Proches Voisins Partagés, nous garderons le premier. Une explication possible
concernant ce résultat surprenant comparé aux résultats exposés dans (Ertoz et al., 2001) est le
fait que nous n’utilisons qu’une petite partie des élements de l’espace d’origine, on a donc
trop peu de données pour exploiter ce second degré d’information. Nous conservons également
l’idée de déﬁnition des noyaux a partir des liens forts. Le nom Shared Nearest Neighbours
n’ayant plus de raison d’étre, nous nous référerons désormais a cet algorithme sous le nom de
MultiNN. Dans chacun des espaces (COD_V, SUJ_V...) :

1. On établit le graphe des plus proches voisins dans lequel les noeuds sont les élements
de E et les arcs relient tous les noeuds entre eux et ont pour valeur la distance cosinus
approchée que l’on calcule entre les deux noeuds extrémités.

2. On déﬁnit un seuil de lien fort (e.g. 0,2 de la distance maximale dans l’espace) et on
élimine les arcs dont les valeurs sont inférieures a ce seuil.

3. Pour chaque noeud du graphe on calcule la somme des valeurs de ses liens restants.

4. Si cette somme est plus grande qu’un certain seuil (e.g. 0.3 de la somme maximale dans
l’espace), on dit que ce point est un noyau local pour cet espace.

Pour tout élément de E, on sait le nombre d’espaces dans lesquels il est noyau local. Si ce
nombre est supérieur a un seuil (e. g. 0.8 des espaces utilisés), cet élément est dit noyau global.

Nous constituons des clusters autour de chaque noyau global de la facon suivante. Dans chaque
espace, on retire de E les éléments qui ont été désignés noyaux globaux. Puis, dans chacun des
espaces et pour chaque élément de E, on enregistre un vote pour le noyau global le plus proche
(cosinus approché le plus élevé). On some les votes sur tous les espaces et on assigne chaque
element a son noyau le plus populaire (éventuellement a plusieurs en cas d’égalité).

Dans chaque espace, si deux noyaux globaux ont une valeur supérieure a un seuil donné, l’es-
pace vote pour la jonction des deux clusters associés. Si un nombre sufﬁsant d’espaces votent
pour cette jonction, les deux clusters sont regroupés. Les éléments des clusters considérés trop
petits par rapport au nombre d’éléments a regrouper sont réassignés un a un aux gros clusters.

Claire Mouton

4.2 Méthode fondée sur l’algorithme HyperLex

Nous adaptons une seconde méthode a notre multi-représentation des mots. Il s’agit de l’algo-
rithme Hyperlex présenté par (Véronis, 2003). Celui-ci est a l’origine appliqué a une liste de
cooccurrents mais dans notre cas nous l’appliquons a une liste de plus proches voisins.

L’ algorithme original est le suivant. Soit E l’ ensemble des mots a regrouper. Pour chacun d’entre
eux, on calcule la distance au mot source. Le mot le plus proche devient noyau d’un cluster. On
attribue a ce cluster tous les éléments appartenant a la sphere de rayon go (si leur nombre est
supérieur a un parametre k) et on retire ces éléments de E. Le mot de E le plus proche du mot
source devient noyau d’un autre cluster. On réitere jusqu’a ce que E soit vide.

Avec nos multiples représentations, le calcul des distances des éléments de l’ensemble E par
rapport au mot source devient la moyenne des distances de tous les espaces. La spéciﬁcité des
espaces n’est pas prise en compte a ce niveau-la. En revanche, les éléments sont attribués au
cluster uniquement si un certain nombre d’espaces valide cette attribution (vote).

4.3 Résultats

Les résultats n’ont été évalués que manuellement dans cette premiere étape. Pour l’exemple du
mot barrage, nous disposons des résultats fournis par les méthodes dont nous nous sommes
inspirées. Nous présentons ainsi nos résultats pour ce mot dans le tableau 3. Nous avons privile-
gié un grand nombre d’éléments a regrouper aﬁn de rassembler le maximum de sens possibles.
Nous avons aussi opté pour un résultat contenant un grand nombre de clusters aﬁn d’en obtenir
le maximum possible de précis, quitte a en obtenir certains qui pourraient encore étre regroupés.
Un classiﬁeur destiné a faire de la désambigu'1'sation et apprenant sur ces données saura ne pas
classiﬁer de mot dans l’une de ces classes si les éléments de celle-ci sont trop clairsemés et ont
des équivalents dans d’autres clusters.

La comparaison de nos clusters avec ceux des algorithmes d’origine reste difﬁcile puisque nous
ne cherchons pas a regrouper le meme type de terme (cooccurrents vs. plus proches voisins syn-
taxiques). Cependant nous pouvons voir que les sens distingués ne sont pas toujours les memes.
Nous retrouvons d’une part en 3.1, 3.5 et 3.10 et d’autre part en 4.4, 4.11 et 4.12 les sens de
barrage correspondant au barrage routier, frontalier ou policier, que l’on ne distingue pas bien
entre eux. Le Petit Larousse utilisé dans la campagne ROMANSEVAL (Segond, 2000) ne fait
lui-méme pas la distinction et considere simplement ceux-ci comme obstacle. En 3.4, 4.5, 4.6,
et 4.9, l’usage du barrage hydraulique est distingué en tant qu’infrastmcture industrielle et en
3.7 et 4.8 en tant que construction sur un cours d ’eau. Il s’agit du meme objet physique mais
l’usage est different. En revanche on ne retrouve pas le sens du match de barrage pour lequel
il existe tres peu de mots partageant les memes contextes syntaxiques. Pour le mot vol, notre
algorithme extrait correctement les sens de’lit de vol et vol ae’rien, ce qui n’était pas le cas pour
l’algorithme HyperLeX.

Une évaluation plus pertinente de ces différents types de clusters serait de les utiliser dans une
tache applicative (désambigu'1'sation de sens ou recherche d’information) et d’évaluer les résul-
tats de celle-ci. Une telle évaluation automatique doit encore étre mise en place pour estimer la
meilleure approche et évaluer la qualité de discrimination des clusters produits.

Induction de sens de mots a partir de multiples espaces sémantiques

Mot-source
HyperLex . retenue, crue
(Véronis, 2003) '

rencontre,

(Ferret, 2004) : autoroute
' crue, amont, saumon,
. croate,
Mu1tiHyperLex 3.1 : oonﬂit, liaison, environnement, division, combat,

zposte, autoroute, commerce,
installation, accord, marché, controle

arme,
route,

sortie, cours, marche

: tour, concurrence,
: zone,

. amont,

: commerce, couverture,
concurrence, mine,
: avantage,
ronnement, essai,
' ouverture,

dicament, ressouroe, train, usine, véhicule
' commune,
couverture, entente, regroupement, concurrence,
revenu, soutien, succession
autoroute,

: secteur, zone,

arme, centre, tour,

TAB. 3 — Comparatif des clusters construits a partir du mot barrage

5 Discussions et Perspectives

Bien que nous nous soyons dégagés du choix du nombre de clusters a obtenir aﬁn de permettre
un nombre de sens différents selon les mots, le choix des parametres de ces deux algorithmes
reste un probleme dans le sens ou nous n’avons pour l’instant pas de méthode pour apprendre
les parametres optimaux et nous restons ainsi contraints de les choisir par l’eXpérimentation.

Un avantage des méthodes présentées peut étre mis en avant : nous supposons que le fait d’utili-
ser les plus proches voisins comme ensemble d’éléments a regrouper (et non les cooccurrents),
permettra d’utiliser ces voisins comme données d’apprentissage pour un classiﬁeur destiné a la
désambiguisation de termes ambigiis apparaissant dans un nouveau texte. Cette hypothese sera
expérimentée tres prochainement dans la suite de ces travaux.

Par ailleurs, la ﬁnalisation de ces travaux nécessite diverses études a mener rapidement. Aﬁn de
mettre en valeur les apports de la méthode présentée ici, nous souhaitons former des clusters a
partir des memes ensembles de plus proches voisins que dans cette étude mais en utilisant d’une
part l’espace de cooccurrences de fenétres seul ainsi que chacun des espaces syntaxiques seul,
et en utilisant d’autre part les matrices concaténées.

Enﬁn, nous projetons d’utiliser ces clusters de sens pour désambiguiser et indexer une collection

Claire Mouton

de documents et étudier l’apport de cette désambigu'1'sation dans la recherche d’information.

Références

BESANCON R. & DE CHALENDAR G. (2005). L’analyseur syntaxique de lima dans la cam-
pagne d’évaluation easy. In M. JARDINO, Ed., Actes de TALN 2005 (Traitement automatique
des langues naturelles), Dourdan : ATALA LIMSI.

CHARIKAR M. (2002). Similarity estimation techniques from rounding algorithms. In Pro-
ceedings of the 34th Annual ACM Symposium on Theory of Computing.

ERT(")Z L., STEINBACH M. & KUMAR V. (2001). Finding topics in collections of documents :
A shared nearest neighbor approach. In Text Mine 0I, Workshop of the I st SIAM International
Conference on Data Mining.

FERRET O. (2004). Discovering word senses from a network of lexical cooccurrences. In
C OLING ’04 .' Proceedings of the 20th international conference on Computational Linguistics,
p. 1326, Morristown, NJ, USA : Association for Computational Linguistics.

GREFENSTETTE G. (2007). Conquering language : Using nlp on a massive scale to build high
dimensional language models from the web. In Proceedings of the 8th CICLing Conference,
p. 35-49, Mexico.

HARRIS Z. (1985). Distributional structure. In J. J. KATZ, Ed., The Philosophy of Linguistics,
p. 26-47. New York : Oxford University Press.

LANDAUER T. K. & DUMAIS S. (1997). A solution to plato’s problem : The latent semantic
analysis theory of the acquisition, induction, and representation of knowledge. Psychlogical
Review, 104(2), 211-240.

LIN D. (1998). Automatic retrieval and clustering of similar words. In Proceedings of
COLING-ACL98, p. 768-774.

LUND K. & BURGESS C. (1996). Producing high-dimensional semantic spaces from lexical
co-occurrence. Behavior Research Methods, Instruments, and Computers, 28, 203-208.
PADO S. & LAPATA M. (2007). Dependency-based construction of semantic space models.
Comput. Linguist., 33(2), 161-199.

PANTEL P. & LIN D. (2002). Discovering word senses from text. In Proceedings of ACM
SIGKDD Conference on Knowledge Discovery and Data Mining 2002, Edmonton, Canada.

RAVICHANDRAN D., PANTEL P. & HOVY E. (2005). Randomized algorithms and nlp : Using
locality sensitive hash functions for high speed noun clustering. In Proceedings of ACL, Ann
Arbour(MI).

SALTON G., WONG A. & YANG C. S. (1975). A vector space model for automatic indexing.
18(11), 613-620.

SCHUTZE H. (1998). Automatic word sense discrimination. Computational Linguistics, 24(1),
97-123.

SEGOND F. (2000). Framework and results for french. Computers and the Humanities, Special
Issue on SENSEVAL, 34(1).

VERONIS J. (2003). Cartographie lexicale pour la recherche d’information. In B. DAILLE,
Ed., Actes de TALN 2003 (Traitement automatique des langues naturelles), p. 265-274, Batz-
sur-mer : ATALA IRIN.

