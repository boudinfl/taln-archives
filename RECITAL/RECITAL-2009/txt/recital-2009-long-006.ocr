RECITAL 2009, Senlis, 24-26 juin 2009

Méta-moteur de traduction automatique : proposition d’une
métrique pour le classement de traductions

Marion Potet
Laboratoire d’informatique de Grenoble, équipe GETALP
UJF - BP 53, 38041 Grenoble Cedex 9
Marion.Potet@imag.fr

Résumé. Compte tenu de l’essor du Web et du développement des documents multi-
lingues, le besoin de traductions "a la volée" est devenu une évidence. Cet article présente un
systeme qui propose, pour une phrase donnée, non pas une unique traduction, mais une liste de
N hypotheses de traductions en faisant appel a plusieurs moteurs de traduction pré-existants.
Neufs moteurs de traduction automatique gratuits et disponibles sur le Web ont été sélectionnés
pour soumettre un texte a traduire et réceptionner sa traduction. Les traductions obtenues sont
classées selon une métrique reposant sur l’utilisation d’un modele de langage. Les expériences
conduites ont montré que ce méta-moteur de traduction se révele plus pertinent que l’utilisation
d’un seul systeme de traduction.

Abstract. Considering the Web and multilingual documents development expansion, the
need of fast translation has become an evidence. This paper presents a system that proposes,
for a given sentence, a list of N translation hypotheses instead of a single translation, using
several machine translation systems already existing. Nine free and available (on the Internet)
automatic translation engines have been chosen to submit a text to be translated and to receive its
translation. The translations obtained are evaluated individually with a language model adapted
and a metric elaborated by us, and in this way classiﬁed by relevance order. The experiment
have pointed out that this meta-translation engine is more useful than the use of one system for
translation.

M0tS-Cl traduction automatique, web, modele de langage, méta-moteur de traduction.

Keywords: automatic translation, web, language model, meta-translator.

Marion Potet

1 Introduction

Le premiere approche utilisee pour traduire des textes etait basee sur des regles linguistiques
qui visent a formaliser toutes les connaissances necessaires a la traduction. Celle-ci necessite
beaucoup de travail de la part des linguistes pour deﬁnir le vocabulaire et la grammaire. Cette
methode a donne naissance, par exemple, au celebre systeme de traduction Systran. D’autres
methodes existent, comme les methodes empiriques et parmi elles, l’approche statistique, dont
les bases theoriques ont ete posees par (Brown et al., 1993) puis (Koehn et al., 2003), qui fait
en sorte que toute la connaissance translingue soit acquise de maniere automatique a partir de
corpus.

Plusieurs travaux se sont interesses a la comparaison des performances des systemes issus des
differentes approches. Il en ressort que, pour des performances equivalentes, les approches ex-
pertes a base de regle et les approches empiriques font des erreurs differentes lors de la tra-
duction. Dans (Dugast et al., 2008), par exemple, on remarque que les oublis de mots et la ge-
neration de mots inconnus sont des types d’erreurs speciﬁques aux systemes statistiques alors
que les erreurs dans l’ordre des mots, tout comme les erreurs de vocabulaire, sont speciﬁques
aux systemes a base de regles. D’autre part, (Rayner & Bouillon, 1995) precisent que les regles
semblent encoder les informations grammaticales alors que les statistiques encodent les infor-
mations liees au domaine. Par ailleurs, la combinaison d’un systeme base sur les regles et d’un
module statistique, par exemple au moyen de la post-edition comme dans (Simard et al., 2007)
et (Koehn et al., 2007), montre une amelioration signiﬁcative de la qualite de traduction.

Cet article propose de combiner les resultats de plusieurs moteurs de traduction automatique
issus de systemes de natures differentes. Son originalite est qu’il fait appel a differents moteurs
de traduction automatique du Web aﬁn d’obtenir plusieurs traductions qui sont ensuite classees
par ordre de pertinence a l’ aide d’une metrique basee sur la ﬂuidite de la traduction. L’ idee est de
tirer parti de cette variabilite inter-systemes en regroupant, pour une meme phrase, les resultats
obtenus des differents moteurs aﬁn de les mettre en concurrence. Le systeme utilise plusieurs
moteurs de traduction simultanement; par deﬁnition; nous pouvons lui attribuer l’appellation
de méta-moteur de traduction.

Dans un premier temps, des interfaces de traduction sont referencees, selectionnees puis testees.
Il en resulte une liste de N moteurs de traduction qui sont ensuite interroges par programme aﬁn
d’obtenir N traductions d’une phrase source initiale.

Une fois les resultats des differents moteurs de traduction obtenus, il s’agit de scorer chaque
traduction, en vue de les classer. Pour une phrase source, il en resulte une liste ordonnee d’hy-
potheses de traduction. La section 3, decrit la metrique utilisee pour classer ces traductions. La
demarche est validee par une evaluation automatique (score BLEU) et par une evaluation sub-
jective dont les resultats sont decrits dans la section suivante. Enﬁn, le systeme a ete implemente
a travers une interface graphique, accessible sur le Web, presentee dans la section 5.

2 Récupération de traductions

Le cadre applicatif est restreint au domaine des informations joumalistiques ou depeches, plus
communement appele "news", en vue de valoriser ce travail par des d’applications directes
comme la traduction automatique d’articles joumalistiques de la langue anglaise vers la langue

Méta-moteur de traduction automatique

francaise.

Une sélection manuelle des moteurs de traduction a été réalisée sur la base des caractéristiques
suivantes : disponible sur le net ; gratuit; permettre la traduction de phrases ou de textes ; traiter
la traduction de la langue anglaise vers la langue francaise ; ne pas pratiquer de "blacklistage"1.
Peuvent également étre prises en compte la performance des résultats, la rapidité de réponse et
une bonne utilisabilité. Selon ces criteres, 22 interfaces ont ainsi été répertoriées.

Lors de l’analyse et des tests des systemes de traduction présents sur le web, plusieurs inter-
faces différentes présentaient systématiquement, pour une meme phrase a traduire, des résultats
identiques. Ces interfaces ont été regroupées et neuf moteurs de traduction distincts ont été
identiﬁés. Comme plusieurs interfaces utilisent le meme outil traductif, le choix se porte sur
celles qui présentent un temps de réponse court et une bonne utilisabilité (pas d’identiﬁcation
aupres du serveur, méthode d’envoi du formulaire, etc). Par soucis d’anonymat, dans la suite
de l’article, les neuf interfaces ﬁnalement retenues seront désignées par : M TG, M T3, M TR,
MTA, MTE, MTF, MTL, MTP, MTW.

3 Classement des traductions

3.1 Description des corpus

Etant donné le cadre applicatif, les corpus doivent nécessairement comporter des données jour-
nalistiques de type "news" et ils doivent atteindre une taille sufﬁsante pour permettre des trai-
tements statistiques ﬁables. Etant donné le peu de ressources disponibles, dans le domaine spe-
ciﬁque des données joumalistiques, nous avons, pour le besoin, constitué nos propres corpus a
partir de textes écrits collectés sur le World Wide Web. Nous avons constitué, d’une part, un
corpus monolingue en francais pour le modele de langage sur lequel d’appuie la métrique ainsi
qu’un corpus bilingue francais/anglais pour tester le systeme.

L’ apprentissage du modele de langage, utilisé pour classer les traductions de chaque moteurs,
nécessite un ou des corpus représentatifs des conditions d’utilisation et de l’application envisa-
gée : des corpus monolingues francais de données contemporaines et joumalistiques et de taille
sufﬁsante pour une estimation ﬁable des probabilités.

Trois modeles de langage ont été entrainés a partir des corpus d’apprentissage ( décrits dans le
tableau 1). Ils sont ensuite combinés par interpolation linéaire. Le corpus de développement né-
cessaire a l’estimation des pondérations de chaque modele de langage, est composé de données
joumalistiques du site France242 du ler au 14 mai 2008.

Par ailleurs, l’évaluation des systemes nécessite un corpus de test, bilingue et aligné, de phrases
issues du domaine joumalistique qui vont servir de référence pour mesurer la qualité des traduc-
teurs. Pour cela, nous avons donc constitué un corpus bilingue a partir des versions anglaises
et francaises du site Web de France24. Les phrases alignées ont ensuite été sélectionnées et
extraites manuellement, directement d’apres la mise en correspondance automatique des do-

1Le nombre de requétes pennises pour une méme machine sur un moteur de traduction est suceptible d’étre
limité ou controlé par le serveur de certains moteurs de traduction
zhttp ://www.france24.com
5 Association Européenne pour les ressources linguistiques.

Marion Potet

Source Description Nombre de mots Période
France24 www.France24.com 4 M février/avril 2008
Web données du web 72 M juin 2003/avril 2008
Le Monde CDRom de ELRA5 23 M janvier/décembre 2003

TAB. 1 — Description des corpus d’apprentissage du modele de langage

cuments. Il est a noter que les textes relataient le méme événement sans étre pour autant la
traduction l’un de l’autre. Les phrases traduites prélevées ont donc dﬁ étre vériﬁées et, au be-
soin, corrigées une par une. Le résultat est un corpus de 300 phrases, bilingue et aligné de
données journalistiques récentes.

3.2 Description du modéle de langage

Avant tout traitement, les corpus d’ apprentissage ont été norrnalisés avec la boite a outils CLIPS-
Text-Toolkit-2.5 (Bigi & Le, 2008). Les modéles de langage ont été entrainés a partir des corpus
a l’aide de l’outil SRILM (Stolcke, 2002). Les modéles utilisent des trigrammes de mots, ils
ont étés lissés avec la méthode de repli de Kneser-Ney modiﬁée et un pruning a 10‘9 leur été

appliqué.

Nous avons donc créé trois modéles de langage que nous nommerons, en rappel a leur corpus
d’apprentissage respectif (vus dans la table 1), LeM0nde, Web et France24. Le modele de lan-
gage ﬁnal est le résultat de l’interpolation linéaire du modele France24 avec un coefﬁcient de
0,41, du modele Web avec un coefﬁcient de 0,42 et du modele LeM0nde avec un coefﬁcient de
0,17. Son vocabulaire est de 60 147 mots et le taux de mots inconnus est de 1,36 % dans le
corpus de test.

Nous avons choisi un apprentissage a vocabulaire fermé, un vocabulaire ouvert étant, a notre
avis, inadapté dans le cas de données journalistiques fortement dépendante de l’actualité car le
nombre de mots inconnus est tres important et la distribution des probabilités en est fortement
affectée.

3.3 Déﬁnition d’une métrique pour classer les traductions

Un modele de langage analyse une phrase et lui applique plusieurs métriques. Celles qui nous
interressent sont : le nombre de mots de la phrase, le nombre de mots de la phrase ne ﬁgurant pas
dans le modele de langage ou mots inconnus (notés OOV83) et le logarithme de la probabilité
de la phrase WK = 11111112 - - - w K, avec wk les K mots de la phrase. Cette derniére notée logprob
s’estime comme suit :

K
l0gpr0b(WK) = 2 log P(w;,|w;,_1,w;,_2)
k=2

avec P(w,,|w,,_1, w,,_2) la probabilité du trigramme w,,_2, w,,_1, wk.

3OOV= Out Of Vocabulary.

Meta-moteur de traduction automatique

En pratique, l’outil SRILM possede une commande ngram qui, a partir d’un modele de lan-
gage, applique a toute phrase les metriques precedemment citees. Cette commande appliquee a
la phrase "jefais un essai " donnera le resultat suivant :

<s> je fais un essai </s>
l sentence, 4 words, 0 OOVs, logprob= -10.1085

Dans notre application, la metrique recherchee sera utilisee pour comparer les hypotheses de
traduction issues des differents moteurs en ligne.

Pour l’attribution d’un score aux differentes traductions, il est inutile d’introduire un parametre
destine a normaliser la longueur des phrases car les phrases, etant la traduction d’une seule
et meme phrase source, ont necessairement approximativement le meme nombre de mots. Le
critere logprob, bien que fortement dependant du nombre de mots dans la phrase, semble donc
en parti approprie a notre application. Le probleme est, qu’avec le modele de langage utilise, ce
critere sur-evalue les phrases comportant un ou plusieurs mots inconnus.

Dans le cas des donnees d’actualite, le vocabulaire du corpus d’apprentissage est tres large et
le modele de langage associe une probabilite trop importante aux mots hors vocabulaire. Ainsi,
certaines suites de mots comme "J e fais un MOTINC" peuvent se voir attribuer une probabilite
d’apparition beaucoup plus forte que la phrase "J e fais un essai". Pour traiter ce probleme, une
technique consiste a assigner une probabilite que l’on deﬁnira aux mots n’apparaissant pas dans
le corpus d’apprentissage. Notre metrique de classement doit donc tenir compte du nombre de
mots inconnus dans la phrase, en leur attribuant une ponderation adequate. Il est alors necessaire
d’introduire une penalite e appliquee a chacun des mots hors vocabulaire rencontres dans les
phrases. Pour cela nous avons determine empiriquement une valeur adequate en veriﬁant qu’elle
soit inferieure au plus petit des logarithmes de probabilite attribue a un mot connu. Celle-ci a

ete ﬁxee a e = -8. Finalement, le score LPOOV attribue a une phrase traduite WK s’ec1it :
K
LP00V(WK) = Z l0gP('w;, | w;,_1w;,_2) + OOV8 X 6 (1)
k=2

o1‘1 OOVs est le nombre de mots hors vocabulaire de la phrase.

Par la suite, nous classerons les phrases par maximisation du score LPOOV de l’equation 1.

4 Expérimentations et résultats

Le méta-moteur a l’interet de presenter, pour une seule et meme phrase a traduire, plusieurs tra-
ductions provenant de differents systemes de traduction, d’une part, et de les presenter classees
selon un critere de qualite, d’autre part. C’est ce classement qui va etre juge lors des evalua-
tions. Le systeme de classement est donc evalue, avec les methodes usuelles, en considerant
uniquement la traduction qu’il classe premiere (1stBest). Il presente, en effet, un interet parti-
culier s’il apporte plus d’information que l’utilisation d’un seul des moteurs de traduction, pris
separement, en l’occurrence le meilleur. On prevoit donc que les performances de notre systeme
soient meilleures que celles du meilleur moteur de traduction utilise. Pour veriﬁer cette hypo-
these, nous procederons a une evaluation automatique que l’on conﬁrmera par une evaluation
subjective.

Marion Potet

4.1 Evaluation automatique

La qualité des traductions est estimée automatiquement par la métrique existante la plus utilisée
dans le domaine, le score BLEU (Papineni et al., 2002), complete avec le score NIST (Dod-
dington, 2002) et le score METEOR (Lavie & Agarwal., 2005). La métrique BLEU repose sur
la comparaison de la sortie du traducteur avec les traductions dites de référence. Cette métrique
mesure le recouvrement lexical de la phrase traduite avec une ou plusieurs phrases données
comme références de la traduction. Le score BLEU varie de 0 a 1 et, étant un score de pré-
cision, il est d’autant meilleur qu’il est grand. Néanmoins difﬁcile a interpréter dans l’absolu,
nous comparerons le score BLEU de notre systeme au score Bleu des moteurs de traduction en
ligne utilisés. Les scores NIST et METEOR, quant a eux, reprennent le principe du score BLEU
et l’adaptent légerement.

Le tableau 2 montre les résultats de l’évaluation des neufs moteurs de traduction utilisés. Ils sont
classés par ordre de performance, sur notre corpus de test aligné de 300 phrases. Le traducteur
M TG est en téte avec un score BLEU de 0,311. On constate également que les scores NIST et
METEOR sont corrélés avec l’évaluation BLEU.

METEOR NIST BLEU
M TG 0,165 6,68 0,311
M TR 0,145 6,20 0,258
M T3 0,141 6,07 0,252
M Tp 0,142 6,06 0,251
M TA 0,135 5,89 0,234
MTE 0,122 5,83 0,216
M TW 0,130 5,78 0,230
MTF 0,122 5,78 0,216
M TL 0,120 5,51 0,206

TAB. 2 — Scores BLEU, NIST et METEOR des 9 moteurs de traduction sur notre corpus jour-
nalistique de 300 phrases

Le tableau 3 présente les résultats de l’évaluation du classement proposé par la métrique de
l’équation 1. L’ ensemble des traductions classées premieres par notre systeme, que l’on nom-
mera par la suite 1stBest, obtient un score BLEU de 0,317. De la meme facon, on appelle
respectivement 2ndBest et 3rdBest l’ensemble des traductions classées par le systeme en
deuxieme et troisieme position.

METEOR NIST BLEU
1stBest 0,170 6,82 0,317
2ndBest 0,154 6,48 0,285
3rdBest 0,144 6,21 0,261

TAB. 3 — Scores BLEU, NIST et METEOR des trois premiers résultats de notre méta-traducteur

Le tableau 4 détaille la proportion des différents moteurs de traduction présents dans les trois
premiers résultats. Bien que M TG soit present a 80,7 % dans les trois premieres réponses don-
nées par le systeme, les autres traducteurs apparaissent également en téte des résultats du clas-

Méta-moteur de traduction automatique

sement. Les neufs moteurs de traduction sélectionnés apportent donc tous leur contribution aux
performances des trois meilleurs sélectionnés.

% MTG M TR M Tp M TA M T3 M TE M TL M Tp M TW total
1stBest 54,88 21,44 5,4 5,7 2,8 4,3 3,4 2,0 0 100
2ndBest 15,3 46,4 13,0 6,0 5,8 6,0 4,3 4,3 0,9 100
3rdBest 46,8 8,5 8,8 8,0 9,0 6,3 4,8 4,5 3,0 100
Total 39,0 25,4 9,0 5,5 5,9 5,5 4,1 3,6 1,3 100

TAB. 4 — Proportions des moteurs dans les trois premiers résultats du méta-traducteur

En sélectionnant automatiquement la meilleure phrase, au sens de la métrique, proposée par
les neuf moteurs de traduction, notre systeme présente un score BLEU de 0,317 alors que le
meilleur des moteurs de traduction utilisé (MTG) obtient un score de 0,311. Les scores NIST
et METEOR sont eux aussi nettement améliorés. Néanmoins, il est difﬁcile de savoir si ce gain
est réellement signiﬁcatif. De ce fait, il parait utile d’avoir recours a une évaluation humaine
subjective pour vériﬁer le résultat obtenu de facon automatique.

4.2 Evaluation subjective

Le but de cette évaluation subjective est de faire appel a des annotateurs pour conﬁrmer les
résultats obtenus avec l’évaluation automatique. Vériﬁer le classement complet des traductions
par le systeme serait tres complexe a mettre en oeuvre, on préferera évaluer le premier choix
établi par notre systeme comparativement au meilleur des moteurs de traduction utilisé, en
l’occurrence MTG.

On évalue donc la meilleure traduction sélectionnée par notre systeme comparativement a la
traduction que donne MTG. La consigne donnée aux participants est de choisir, parmi les deux
phrases, celle qui leur semble la meilleure, ou de n’effectuer aucun choix s’ils jugent celles-
ci équivalentes. 11 y a donc trois réponses possibles : la phrase donnée par notre systeme est
meilleure, la phrase donnée par MTG est meilleure, aucune n’est meilleure que l’autre. Il est a
noter que nous ne précisons évidemment pas la provenance des phrases.

Seize volontaires ont participé a l’expérimentation et chaque phrase a été évaluée par six annota-
teurs différents. Nous avons vériﬁé la cohérence des réponses a l’aide de l’indice de Bayes. Plus
l’erreur de Bayes est petite, plus les annotateurs sont cohérents dans leurs réponses. Nous avons
donc éliminé les phrases pour lesquelles les annotateurs étaient trop partagés, pour obtenir 78
phrases de test restantes (indice de Bayes < 0,9).

Sur ces phrases, on peut donc dire que les participants préferent dans 55 % des cas les réponses
renvoyées par notre systeme contre 33 % pour MTG.

L’ évaluation subjective conﬁrme l’évaluation automatique : il est préférable d’utiliser une tra-
duction choisie parmi plusieurs, provenant de moteurs différents, que de n’utiliser systemati-
quement que celui qui obtient le meilleur score moyen. On peut en déduire que ce concept de
méta-traducteur est une solution pour améliorer la qualité des traductions et que le classement
proposé par notre systeme est pertinent.

Marion Potet

5 Création d’une interface graphique en ligne

//

Pour mettre ce service de méta-traducteur a disposition d’utilisateurs potentiels, nous avons cre
une interface graphique qui permet d’interagir avec le systeme a partir du Web.

L’ interface permet a l’utilisateur d’entrer un texte a traduire et de choisir les moteurs de traduc-
tion a utiliser parmi les 9 disponibles ( ﬁgure 1). Une fois le texte traduit entré et les moteurs
de traduction a utiliser sélectionnés, l’utilisateur a le choix de l’afﬁchage des résultats. Ceux-ci
peuvent apparaitre classés par ordre de pertinence ou non. L’ option d’afﬁchage classé enverra la
requéte aux moteurs de traduction sélectionnés et, apres réception des réponses, les évaluera a
l’aide du modele de langage et les afﬁchera classées (ﬁgure 2). L’ option d’afﬁchage non classé
déroule le processus jusqu’a réception des requétes et afﬁche les traductions selon leur ordre
d’arrivée sur le serveur. L’interface est accessible, a partir de l’URL suivante :

http : //www — clips.imag.fr/ge0d/User/marionpotet/GUI/metatranslatonphp.

Elmisir la iangue du site I I Frangans v ok

% Métazrradubteun rl

Traductinn autumal iqua Ang|a.isIFra.n|;ais

Ca SHE propose dc traduire des phlases Ur: Wa Iangue anWaW5e war: In Iangua fmmgmse WW any-cue wire
phrase ii Iraclulra a plusleurs moteurs cle traductmn autumatluua cIW5pnniDWes sur la webel affiche leurs
résultats Pnur gala, iW vnus suffit d'E|'It|'E|'IJ|'IE phrase atraduire dans We champ pre'v'LI Ea naleﬁetet Ha
5eWev:tmW'WnEr Wes mmeurs de traductlnn qua vnus snuhamaz Interrnger En’5a\fnil'Q|LI,5,

Tex1e en anulais R traduire:

WI carry out a try.

S-éleuliun des muieuts de Iraducliun:
 F’n,,I I  WE-n|l'||iIIgn
 l:l-.‘ETfcWl'WE|il- JII  Ep -|:2

1: W_muL!amW:'  n‘\p|l|WE=L1 LaW ,‘Ll«1[_1--

.Iuut..:u£W1er. V
Options duffichnge:
..5' Classer les tradu ctions par ordre de uerlinence

" Tracluctl ans. raplrles {sans classem em}

1 Traduire 

FIG. 1 — Interface graphique : page d’accueil

6 Conclusion

Au terme de ce travail, il ressort que, une solution possible pour améliorer la qualité de la
traduction automatique, est de tirer parti de la variabilité des résultats de plusieurs systemes
de traduction. C’est l’intéret et l’originalité de l’outil traductif proposé dans cet article. Notre

Méta-moteur de traduction automatique

‘ '5}'_ ,m V‘ Fr  :1 "win I ' ’ En‘..-
> —‘ |'-‘=. 1 L|"m._. C 5: Al.-1"-!r|L£v1“IJ.;.-‘E
-TEJUTII COCHEF

Dplinns ﬂ'afliI: huge:

5' Classer Iestraducunns par nrd re ne perunente

' Traducliuns rapides (sans classernenij

Traduire

glza

A Kg Icarry uul auy.
 Traductians prnpnsées:

1 pr nI:Jeréal1'se un essax.
2 at  ‘ Je prnneuer E1 un essai.
‘ ed‘ ngu.aualrE»'EI'.1uts}~ -_?h.1n . .2l'eﬁat:Iua un assai.

:J'exécute un essai.

5 at-als. : .2l‘en'Ipurte un essai.

6frc:eTrEns|En.iE-I1 .J‘exécule un essaxe

  

NuuveHEiradu:.1iun ...

 

FIG. 2 — Interface graphique : résultat d’une traduction

méta-traducteur utilise, en effet, plusieurs moteurs de traduction existants et met a proﬁt, pour
chaque situation, ceux qui semblent répondre le Inieux. La validation des solutions proposées a
été réalisée dans la cadre d’une traduction de phrases anglaises vers la langue francaise de don-
nées de type journalistique. Pour chaque phrase donnée a traduire, les traductions obtenues sont
classées selon un critere qualitatif évalué par une métrique reposant sur un modele de langage.
Dans l’échantillon de phrases testé, il s’est révélé étre plus pertinent de proposer la meilleure
traduction, sélectionnée automatiquement par notre systeme parmi les neufs hypotheses de tra-
duction, que de proposer systématiquement un seul des moteurs de traduction utilisé. Ceci a été
conﬁrmé aussi bien lors de l’évaluation automatique que lors de tests subjectifs aupres d’utili-
sateurs potentiels. Au vu de ces résultats, l’outil a été mis a disposition sur le web a travers une
interface graphique. Il est ainsi possible de soumettre un texte a traduire, de sélectionner des
traducteurs automatiques a interroger et de Visualiser la liste des traductions proposées classées
par ordre de pertinence.

Références

BIGI B. & LE V. B. (2008). Normalisation et alignement de corpus francais et vietnamiens :
Format et logiciels. In 9es joume’es intemationales d’analyse statistique des données tex-
tuelles, France, Lyon.

BROWN P. E., PIETRA V. J. D., PIETRA S. A. D. & MERCER R. L. (1993). The mathematics
of machine translation : Parameter estimation. Computational linguistics, 19, 263-311.

DODDINGTON G. (2002). Automatic evaluation of machine translation quality using n-gram
co-occurrence statistics. Human Language Technology Conference archive Proceedings of

Marion Potet

the second international conference on Human Language Technology Research, San Diego,
California, p. 138-145.

DUGAST L., SENELLART J. & KOEHN P. (2008). Can we relearn an rbmt system? In The
Third Workshop on statistical Machine Translation, Columbus, Ohio, USA, volume 3, p. 175-
179.

KOEHN P., DUGAST L. & SEMELLARD J. (2007). Statistical post-editing on systran’s rule

based translation system. In The Second Workshop on Statistical Machine Translation, Prague,
Czech Republic, volume 23, p. 220-223.

KOEHN P., OCH F. J. & MARCU D. (2003). Statistical phrase-based translation. In Confe-
rence of the North American Chapter of the Association for Computational Linguistics on
Human Language Technology, Edmonton, Canada, volume 1, p. 48-54.

LAVIE & AGARWAL. (2005). Meteor : An automatic metric for mt evaluation with improved

correlation with human judgments. In Workshop on Statistical Machine Translation, Michigan,
USA, p. 65-72.

PAPINENI K., ROUKOS S., WARD T. & ZHU W. -J . (2002). Bleu : A method for automatic
evaluation of machine translation. In the 40th Annual Meeting on Association for Computatio-

nal Linguistics SESSION .' Machine translation and evaluation, Philadelphia, Pennsylvania,
p. 311-318.

RAYNER M. & BOUILLON P. (1995). Hybrid transfer in an english-french spoken language

translator. In Joumées intemationales .' Language engineering, Montpellier, FRANCE, vo-
lume 15, p. 153-162.

SIMARD M., GOUTTE C. & ISABELLE P. (2007). statistical phrase-based post-editing. In The

Conference of the North American Chapter of the Association for Computational Linguistics,
pages , Rochester, USA, p. 508-515.

STOLCKE A. (2002). Srilm, an extensible language modeling toolkit. In The International
Conference on Spoken Language Processing, Denver, Colorado, volume 3, p. 901-904.

