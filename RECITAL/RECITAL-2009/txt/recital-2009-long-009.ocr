RECITAL 2009, Senlis, 24-26 juin 2009

Normalisation des entités nommées : pour une approche mixte et
orientée utilisateurs

Vanessa Andréani

TecKnowMetrix — 4, rue Léon Béridot — ZAC Champfeuillet — 38500
Voiron — France

LIDILEM — Université Stendhal Grenoble 3 — Domaine universitaire — 1180,
avenue centrale — 38400 Saint Martin d’Heres — France
Va@tkm.fr

Résumé La normalisation intervient dans de nombreux champs du traitement de
l'information. Elle permet d'optiIr1iser les performances des applications, telles que la
recherche ou l'extraction d'information, et de rendre plus fiable la constitution de ressources
langagieres. La normalisation consiste a ramener toutes les variantes d'un méme terme ou
d'une entité nommée a une forme standard, et permet de limiter l'impact de la variation
linguistique. Notre travail porte sur la normalisation des entités nommées, pour laquelle nous
avons mis en place un systeme complexe mélant plusieurs approches. Nous en présentons ici
une des composantes: une méthode endogene de délimitation et de validation de l’entité
nommée normée, adaptée a des données multilingues. De plus, nous placons l'utilisateur au
centre du processus de normalisation, dans l'objectif d'obtenir des données parfaitement
fiables et adaptées a ses besoins.

Abstract Normalization is involved in many fields of information processing. It improves
performances for several applications, such as information retrieval or information extraction,
and makes linguistic resources constitution more reliable. Normalization consists in
standardizing each variant of a term or named entity into a unique form, and this way restricts
the impact of term variation. Our work applies to named entity normalization, for which we
implemented a complex system that mixes several approaches. We present here one of its
components: an endogenous method to mark out and validate the normalized named entities.
Moreover, we place the user in the center of our normalization process, in order to obtain fully
reliable data that fit his needs.

Mots-clés 2 normalisation, entités nommées, traitement de l'information, analyse de
corpus, méthodes endogenes, systeme complexe.

Keywords: normalization, named entities, information processing, corpus analysis,
endogenous methods, complex system.
Introduction

Tout comme les unités lexicales « classiques », les entités nommées (EN) sont soumises a une
grande complexité et a une variation linguistique importante. Leur normalisation est donc une

Vanessa Andréani

étape nécessaire pour un grand nombre de champs d’applications du Traitement Automatique
des Langues (TAL) pour obtenir des données fiables et de qualité. C’est notamment le cas de
l’eXtraction d’information, mais aussi de toutes les taches qui traitent massivement les EN.
Les méthodes de normalisation existantes relevent la plupart du temps d’un seul type
d’approche, ce qui les rend efﬁcaces sur un aspect précis de la normalisation, mais parfois
inopérantes sur d’autres points de difficulté. De plus, tres peu de systemes actuels placent
l’utilisateur au centre du processus, alors que ce sont souvent ses connaissances et son point
de vue qui permettent de réaliser une normalisation pertinente. Enfin, peu de systemes de ce
type sont portables d’une langue a l’autre sans une modification, parfois coﬁteuse, des
ressources utilisées pour le traitement des données.

Dans une premiere partie de cet article, nous proposons de définir les notions liées au
probleme de la normalisation, et exposons les techniques existantes. Dans un deuxieme temps,
nous présentons le systeme de normalisation complexe que nous avons mis en place, puis
démontrons l’intérét d’une méthode de normalisation basée sur différents criteres
linguistiques et intégrant l’utilisateur. Enfin, nous détaillons notre approche endogene pour la
normalisation, particulierement adaptée a des données multilingues, et discutons nos
premieres évaluations.

1 La normalisation des entités nommées

Dans le cadre de l'exploitation de données textuelles pour l’analyse de l'information, la
disparité des formes renvoyant a une meme entité nommée (EN) ou a un meme terme pose
rapidement probleme. Par conséquent, il est souvent nécessaire de procéder a une étape de
normalisation de ces données, avant de débuter l'analyse a proprement parler.

Nous définissons la normalisation comme un processus permettant de ramener plusieurs
formes de surface différentes renvoyant a un meme référent a une forme standard, et dont la
ﬁnalité est de reconnaitre, dans des données structurées ou non, toutes les réalisations
linguistiques pour une meme entité.

Les travaux présentés ici portent exclusivement sur la normalisation des EN. A la suite de
(Poibeau, 2001), nous considérons que les EN sont « l'ensemble des noms de personnes,
d'entreprises et de lieux présents dans un texte donné »1. Parmi tous ces types, nous ne nous
intéresserons qu'aux noms de personnes et d'organisations, seules entités qui apparaissent dans
les données que nous souhaitons traiter. Les EN révelent une complexité semblable a celle des
unités lexicales « classiques » (Ehrmann, Jacquet, 2006), et sont donc soumises a la variation
linguistique au méme titre que les termes « communs ». Par conséquent, pour les domaines du
TAL traitant massivement les EN, la normalisation de ces entités doit réduire l’impact de cette
variation au minimum. Ainsi, les entités nommées President John Kennedy, John F. Kennedy
et John Fitzgerald Kenedy renvoient toutes au 356 président des Etats-Unis, John Fitzgerald
Kennedy, malgré les différences graphiques qu’elles présentent, et doivent étre traitées en
conséquence. Nous nous intéressons donc a une normalisation au niveau graphique, destinée a
homogénéiser les formes de surface pour une meme entité.

Les champs concemés par cette étape de normalisation sont relativement nombreux. Pour la
recherche d’information (RI), la possibilité d’identiﬁer toutes les formes possibles d’une
entité via une étape de normalisation est un moyen d’améliorer les performances en

L'auteur inclut également dans cette catégorie << les dates, les unités monétaires, les pourcentages, etc. »

Normalisation des entités nommées .' pour une approche mixte et 0riente’e utilisateurs

augmentant significativement le taux de rappel. Une telle étape présente le meme avantage
pour l’extraction d’information (EI) et les systemes question-réponse. Pour la constitution de
ressources termino-ontologiques (RTO), c'est-a-dire des « modele[s] de connaissances
comportant un réseau conceptuel et des termes associés » (Aussenac-Gilles, 2007), une étape
préalable de normalisation permet de standardiser les EN qui formeront les instanciations des
noeuds du réseau. Cette normalisation impacte la qualité des résultats foumis par les
applications utilisant la RTO ainsi constituée. Enfin, la normalisation s’avere nécessaire pour
le traitement de données structurées : elle permet de régler les cas d’entrées dupliquées dans
les bases de données, c'est-a-dire des cas ou plusieurs entrées referent a la meme EN mais
présentent des differences graphiques qui empéchent de les traiter comme identiques
(Elmagarmid et al., 2007).

Nous pouvons classer les approches de normalisation d’EN en trois catégories : les méthodes
qui s’appuient sur des ressources externes, les approches fondées sur l’utilisation de patrons,
et les techniques adaptées au cas particulier des données structurées en bases de données.

Les approches prenant appui sur des ressources extemes

Pour la normalisation d’EN dans leur acception la plus restrictive, a savoir les noms propres
de lieux, d'organisations ou de personnes, une approche courante consiste a utiliser un
dictionnaire ou toute ressource externe suffisamment exhaustive, de maniere a ramener
chaque occurrence d'une EN a l'entrée « standard » qui lui correspond dans la ressource. Cela
permet de résoudre les difficultés liées a la variation linguistique, notamment en termes de
synonymie. (Khalid et al., 2008) regle ce probleme en utilisant Wikipédia en tant que
ressource externe : il se sert des liens de redirection du site pour la gestion de la synonymie.
En comparant les résultats obtenus par un systeme de RI avec et sans normalisation en amont,

il démontre qu’il est plus efficace lorsque les EN ont été préalablement normalisées.
Les méthodes fondées sur des patrons d ’extracti0n

Dans le cadre de systemes d’EI dans des domaines spécialisés, l'utilisation de patrons pour la
normalisation permet de régler efﬁcacement les problemes de variation linguistique. Selon les
systemes, l'acception du terme d'entite’ n0mme’e peut étre assez large et inclure d'autres
éléments que les EN courantes. Les auteurs de (Alphonse et al., 2004), dans le cadre du projet
Caderige, travaillent a l'eXtraction d’information, et plus particulierement d’interactions
géniques, a partir de textes du domaine biomédical. Dans un premier temps, ils procedent a
une normalisation des noms de genes et de protéines, en l'occurrence considérées comme des
EN. Des amorces de synonymie telles que « formerly » ou « also called », et des patrons tels
que « gene amorce gene », permettent de gérer la variation linguistique. Ils démontrent que
cette étape de normalisation « facilite l'acquisition et l'apprentissage de regles d'eXtraction en
offrant une représentation plus abstraite des phrases ». Avec le meme objectif, mais dans le
domaine ﬁnancier cette fois, (Poibeau, 2003) prone lui aussi une phase de normalisation des
EN préalable a l'extraction, et la qualifie d' « étape [...] essentielle ».

Le cas particulier des données structurées

Le probleme majeur qui se pose dans les données structurées est l'eXistence de doublons dans
lesquels la forme des EN varie, rendant impossibles les recoupements. Ces doublons sont la
conséquence d'erreurs de typographie, de l'utilisation d'abréviations, mais également de
l'utilisation de plusieurs sources d’information, pour lesquelles les conventions graphiques
peuvent varier (Jijkoun et al., 2008). Les méthodes relevent alors souvent de calculs de
similarité entre champs, de maniere a rapprocher deux ou plusieurs noms qui font référence au

Vanessa Andréani

méme objet du monde. (Elmagarmid et al., 2007) recense les méthodes utilisées pour la
détection d'entrées dupliquées dans les BD, et donc pour leur normalisation.

Les méthodes utilisant des ressources exogenes sont efﬁcaces sur du texte tout venant, e.g.
des corpus de presse. Cependant, des lors que les domaines abordés sont tres spécialisés, ou
que les EN sont des noms de petites entreprises par exemple, des ressources généralistes sont
insuffisantes. L’altemative qui vise a constituer des ressources spécialisées est extrémement
coﬁteuse, sans garantie d’exhaustivité. Une méthode de normalisation par patrons peut quant a
elle représenter une bonne alternative aux dictionnaires et autres ressources pour le traitement
de textes spécialisés. Enﬁn, les calculs de similarité ont l'avantage d'étre peu coﬁteux en
termes de mise au point, et offrent des résultats satisfaisants pour certains aspects de la
normalisation, en particulier pour la correction d’erreurs typographiques.

2 Un systéme de normalisation complexe

Notre travail intervient dans un contexte industriel, sur des données multilingues, et vise a
normaliser les noms des organisations qui déposent des brevets ou publient des articles
scientiﬁques. Les informations concemant ces publications sont stockées dans des bases de
données contenant plusieurs millions d’entrées. L’objectif de cette normalisation est de
foumir dans les tables contenant les noms d’organisations des données standardisées, et ce
quelle que soit la langue employée, de maniere a permettre des analyses de corpus fiables,
qu’il s’agisse d’analyses statistiques, comme des comptages, ou d’analyses textuelles.

Nous avons concu un systeme de normalisation des EN qui méle plusieurs des méthodes que
nous venons d’aborder, et qui implique la participation de l’utilisateur. Grace a cette approche
multiple, nous pouvons couvrir un grand nombre de cas de figure problématiques lies a la
variation linguistique. Le systeme est composé de trois principaux modules :

0 L’eXtraction et la réécriture des noms d’organisations grace a des patrons ;

0 La correction d’erreurs typographiques par des mesures de similarité entre les noms
d’organisations de la base ;

0 L’eXtraction du nom d’organisation le plus cohérent et le plus général via notre
approche endogene, lorsque les deux premieres étapes n’ont pas été sufﬁsamment
efﬁcaces.

Le fait d’associer ces types d’approche permet de traiter les trois grandes catégories de
difﬁcultés rencontrées dans notre corpus :

0 les erreurs typographiques et orthographiques courantes, comme pour Mitsubisi au lieu
de Mitsubishiz ;

0 la grande diversité des sources dont les données sont issues, ce qui entraine une grande
disparité dans la graphie et dans la description d’une méme entité. C'est par exemple le
cas pour Mitsubishi Corp et Mitsubishi ;

2 Mitsubishi est une marque enregistrée de Mitsubishi Corporation

Normalisation des entités nommées .' pour une approche mixte et orientée utilisateurs

0 pour les EN désignant des institutions publiques, il n'est pas rare de trouver a
l’intérieur d’un meme nom plusieurs sous-éléments d’une meme EN. Un cas comme :

Division of Gynecologic Oncology, Department of Obstetrics and Gynecology, University of
Kentucky Medical Center, 800 Rose Street, Whitney—Hendrickson Building, Lexington 40536,

USA. jowell@gx.net

est assez représentatif. Cela pose probleme pour dégager l’entité de niveau supérieur
tout en mettant de cote les sous-organisations, qui viendraient bruiter les données.

Nous illustrons dans le tableau qui suit les traitements effectués par chaque module :

Nom d’organisation brut

Modules

T ulane-Xavier Center for Bioenvironmental
Research, Department of
Pharmacology, T ulan University Medical
Center, LA 70112, USA.

1. Extraction et réécriture par patrons

Univ Tulan Medical Center

2. Correction d’erreurs typographiques par
mesures de similarité

Univ Tulane Medical Center

3. Extraction du nom d’orgaI1isation par méthode

Univ Tulane

endogene

Tableau 1: traitements effectués par les modules du systéme de normalisation

L’utilisateur intervient a la ﬁn des phases 2 et 3, et doit valider les suggestions de correction
du systeme3. Notons que l’utilisateur a le choix de procéder ou non a cette verification, en
fonction des contraintes liées a son activité. De plus, chacun de ses choix est gardé en
mémoire, ce qui permet au systeme d’apprendre les corrections adéquates pour un nom déja
rencontré auparavant. Ainsi, le nombre de corrections a valider par l’utilisateur diminue au fil
des norrnalisations. Cette supervision par l’utilisateur expert du domaine traité permet de

garantir la qualité et la ﬁabilité des données qui seront stockées dans les bases.
Placer cette étape endogene a la ﬁn dutraitementpermetd’éliIr1iner un maximum de bruit par

des méthodes de pattern matching peu coﬁteuses en temps de calcul, et de concentrer ensuite
la procedure endogene, plus lourde en termes de coﬁt calculatoire, sur les cas irrésolus.

3 Une approche endogéne pour la normalisation

3.1 Principe et objectifs

Un systeme endogene trouve les informations dont il a besoin dans les données qu’il doit
traiter. Ces informations permettent de résoudre des cas problématiques pour lesquels les
méthodes a base de regles et de patrons atteignent leurs limites.

3 Cette Validation se fait via une interface dédiée, dans laquelle l’utilisateur doit cocher la suggestion la plus

pertinente. Il a aussi la possibilité de rentrer manuellement un nom d’organisation, lorsque les suggestions ne
sont pas adaptées.

Vanessa Andréani

En France, les principaux travaux dans ce domaine ont été menés avec pour objectif de
réaliser des analyses syntaxiques et / ou morphologiques la ou les systemes préexistants
n’étaient pas assez efﬁcaces en raison d’une variation linguistique trop importante (Vergne,
2004 ; Bourigault, 1993 ; Bourigault et Frérot, 2006). (Frérot et al., 2003) démontre que des
approches de ce type, c'est-a-dire des « procédures non supervisées d’apprentissage sur corpus
qui [...] permettent d’eXploiter le corpus d’analyse pour acquérir les
informations nécessaires » aux traitements, sont particulierement efﬁcaces lorsque le domaine
des textes a traiter n’est pas connu a l’avance, et que des ressources constituées a priori sont
par conséquent peu adaptées. Ces méthodes se fondent essentiellement sur le principe de
productivité, c'est-a-dire sur le nombre de contextes différents avec lesquels peut apparaitre le
mot ou le terme étudié, et s’appuient de facon privilégiée sur les redondances du corpus.

Une autre approche consiste a utiliser la longueur et la fréquence des mots pour réaliser une
analyse syntaxique partielle sans ressources (Vergne, 2004). Le fait d’utiliser de tels criteres
d’analyse permet de traiter des textes multilingues sans coﬁt supplémentaire, puisqu’aucun
lexique n’est nécessaire.

Notre objectif differe de ceux des travaux évoqués ci-avant, puisque nous cherchons pour
notre part a normaliser des noms d’organisations en vue d’un traitement de l’information
stratégique. Cependant, une approche endogene fondée sur la récurrence de séquences nous
parait tout a fait appropriée au vu de nos données, et peut étre un moyen de déterminer la
structure d’un nom d’organisation. En effet, en nous fondant sur les fréquences de segments
de différentes longueurs, et non plus sur des structures syntaxiques ou des indices lexicaux,
nous évitons plusieurs écueils. Tout d'abord, cela évite d'avoir a constituer des lexiques
adaptés, ce qui serait tres coﬁteux puisque nous travaillons sur des données multilingues.
D'autre part, du fait que nous traitons exclusivement des noms propres, aucun dictionnaire ne
serait sufﬁsamment exhaustif. Enfin, une telle approche permet d'avoir un traitement
parfaitement adapté au corpus traité, quel que soit le domaine: un systeme endogene est
forcément adapté au corpus qu'il traite, puisque par essence, les données qu'il utilise pour
traiter le corpus sont issues du corpus lui-méme.

3.2 Application £1 nos données

Les données que nous normalisons permettent de mener des analyses textuelles et statistiques
et de tirer parti d’informations stratégiques. Ainsi, a partir des séquences suivantes :

I. Division of Gynecologic Oncology, Department of Obstetrics and Gynecology,
University of Kentucky Medical Center, Whitney—Hendrickson Building, Lexington 40536, USA.

2. University of Kentucky, Lexington 40536-0098, USA. runge @pop.uky.edu
3. University of Kentucky Research Institute, USA

une méthode endogene devra permettre d’eXtraire la séquence récurrente University of
Kentucky, qui représente le nom normalisé que doivent prendre ces trois noms d’organisations
bruts. De cette maniere, il sera possible d’identifier toutes les publications émanant de cette
organisation, et ce quel que soit le département ou la division.

Nous avons réduit l’utilisation des ressources exogenes au minimum : nous avons notamment
établi une liste d’une centaine d’amorces, c'est-a-dire de mots qui permettent de détecter la
présence d’une entité nommée, ainsi que son niveau hiérarchique. Par exemple, une
université, détectée par l’amorce Univ suivie ou non de plusieurs caracteres (University,

Normalisation des entités nommées .' pour une approche mixte et 0riente’e utilisateurs

Universite’, ...) est considérée comme hiérarchiquement supérieure a un centre, repéré par
l’amorce Center, Centre, Centro, etc. Nous avons en effet observé que lorsque ces deux types
d’entités nommées se trouvaient dans le meme nom d’organisation brut, le centre était
rattaché a l’université, et représentait donc une « sous-partie » de l’université en question.

Pour mettre en pratique une approche endogene adaptée a nos besoins, nous nous fondons sur
deux calculs : la fréquence des sous-séquences et la surface des sous-séquences.

La fréquence des sous-séquences

Nous définissons une séquence comme un segment d’un nom d’organisation placé entre deux
virgules. Une sous-séquence est donc une suite de n items issus de cette séquence, dans une
fenétre dont la taille s’incrémente d’un item a chaque passe.

Grace a notre liste d’amorces, nous pouvons déterminer que l’amorce de plus haut niveau en 1
est University: nous ne conserverons donc que cette séquence. En revanche, elle pose
probleme puisqu’elle comporte une deuxieme amorce, Center, de niveau inférieur. Toute la
difﬁculté est donc de délimiter le nom d’organisation le plus cohérent au sein de cette
séquence, c'est-a-dire d’eXtraire la sous-séquence la plus pertinente. C’est l’étape endogene
qui permettra de déterminer que University of Kentucky doit étre conservé comme nom
d’organisation, et non, par exemple, University of Kentucky Medical.

Nous partons de l'hypothese selon laquelle la sous-séquence la plus fréquente est le nom
d’organisation le plus cohérent. Pour parvenir a extraire ce segment, nous comptabilisons le
nombre d’occurrences de chaque sous-séquence de University of Kentucky Medical Center
comportant l’amorce de niveau supérieur, en l’occurrence University of. Le cas échéant, les
premiers mots sélectionnés sont ceux situés a gauche de l'amorce, du voisin immédiat
jusqu’au plus lointain. Cela se justiﬁe par le fait que la plupart des données problématiques a
ce stade sont des données en anglais, et que la qualification des noms dans cette langue se fait
souvent de droite a gauche. Puis les mots places a droite sont a leur tour ajoutés un par un. Les
sous-séquences testées pour notre exemple sont donc :

(1.a) University of Kentucky
(1.b) University of Kentucky Medical

Le systeme ne Va pas plus loin, puisqu’il rencontre apres Medical la seconde amorce, Center,
qui n’appartient pas au nom d’organisation le plus cohérent.

Pour chacune de ces sous-séquences, le systeme Va donc calculer leur fréquence dans
l’intégralité de notre corpus de plusieurs millions de noms d’organisations. Dans ce corpus,
nous avons relevé 71 occurrences de la suite (1.a) et 9 occurrences de la suite (1.b). De fait, en
sélectionnant la sous-séquence la plus fréquente, nous sommes bien en mesure de détecter le
nom d’organisation le plus cohérent, soit : (1.a) University of Kentucky.

Les memes calculs sont effectués sur les deux autres séquences, soit University of Kentucky et
University of Kentucky Research Institute. University of Kentucky étant toujours la séquence
la plus fréquente, nous considérons qu’il s’agit du nom d’organisation le plus cohérent.

La surface des sous-séquences

Les calculs de fréquence permettent de couvrir un grand nombre de cas semblables a ceux que
nous venons d’évoquer. Cependant, il reste des problemes non résolus. Ainsi les exemples :

Vanessa Andreani

4. New York University Medical School
5. New York University Dental Center

Ces sequences presentent un probleme supplementaire, puisque le nom de l’universite, soit
New York, est un mot compose, et que le nom d’universite York existe par ailleurs. Le calcul
de frequences de sous-sequences donne les resultats suivants :

0 York University 182 occurrences
0 New York University I 74 occurrences
0 New York University Medical 24 occurrences

0 New York University Dental 1 occurrence
En suivant notre hypothese de depart, le nom d’organisation le plus coherent serait donc York
University, avec 182 occurrences. Or, nous savons que dans les exemples que nous etudions,
le nom devrait etre New York University. Pour resoudre cette difficulte, nous avons Inis en

place un calcul de surface des sous-sequences, qui met en rapport leur frequence et leur
longueur. Le calcul de surface est le suivant :

Surface = nombre de tokens * nombre d’occurrences

Nous presentons les resultats de ces calculs sur la ﬁgure 1 :

OCCIIITCIICCS Surface = 

200 z
182
174 '

150 surface = 522

    

100

surface = 96

50

24 tokens

New York U niversitv Medical

Figure 1: Calcul de surfaces pour les sous-sequences de New York
University Medical Center

Ici, le meilleur score de surface est celui de la sequence New York University, soit le nom que
nous avions identifie manuellement comme le plus coherent. Les calculs de surface nous
permettent donc, dans un certain nombre de cas, de resoudre le probleme des mots composes.

3.3 Evaluation

Nous exposons ici les resultats d’une evaluation manuelle sur un echantillon de donnees. Par
la suite, nous menerons une evaluation plus poussee sur un plus grand nombre de donnees,
pour chaque bloc de traitement ainsi que pour l’ensemble de notre systeme.

Pour l’heure, sur un echantillon compose de 15 noms d’organisations bruts, dont la majorite
contiennent des noms d’organisations composes et donc les cas les plus difﬁciles, le calcul de
frequences permet de determiner le nom d’organisation le plus coherent dans 8 cas. Sur les
memes donnees, le calcul de surfaces extrait 10 noms d’organisations correctement. Les cas

Normalisation des entités nommées .' pour une approche mixte et oriente’e utilisateurs

mal identiﬁés quant a eux, permettent d’identiﬁer les difﬁcultés qui expliquent que le calcul
de surface ne soit pas efﬁcace. Soit le nom brut suivant :

6. Nagoya City University Medical School

Notons que nous considérons Nagoya City comme un nom composé, puisqu’il s’agit du nom
complet de l’université. Les trois sous-séquences testées sont les suivantes :

Sous-séquence Fréquence Surface
City University 151 302
Nagoya City University 48 144
Nagoya City University 15 60
Medical

Tableau 2: Fréquences et surfaces pour les sous—séquences de Nagoya City University Medical School

Le calcul de fréquence comme le calcul de surface sont inefﬁcaces sur ce nom. En effet, s’ils
permettent d’élin1iner Medical sans équivoque, ils ne permettent pas de conserver Nagoya,
puisque la fréquence et la surface de Nagoya City University sont inférieures a celles de City
University. La présence de City, un mot trop « générique » et donc trop fréquent dans notre
corpus, empéche une normalisation correcte de ce nom d’organisation.

Pour dépasser cette difﬁculté, nous envisageons de coupler ces calculs a un calcul de
fréquence des items de maniére isolée. Nous avons pu observer en corpus que lorsqu’un mot
voisin d’une amorce était le moins fréquent d’une séquence, il permettait de situer le point de
rupture entre le nom d’organisation cohérent et le « bruit». Généralement, ce mot peu
fréquent est inclus dans le nom d’organisation, et la rupture se situe juste avant ou apres lui.
Par exemple, pour le nom 6, nous avons les fréquences suivantes :

tokens Nagoya City University Medical

fréquences 756 3153 40118 14930

courbe g 

Figure 2: Courbe des fréquences des tokens pour la sous—séquence Nagoya CIty University Medical

De cette maniere, nous pouvons déterminer que Nagoya fait bien partie du nom d’organisation
le plus cohérent. Le calcul de fréquences ou de surfaces permettra d’éliminer Medical, et nous
obtenons donc en sortie du traitement Nagoya City University comme nom le plus cohérent.

Conclusion

A travers cet article, nous avons abordé le probleme complexe de la normalisation des entités
nommées, situé en amont de différentes taches de traitement de l'information. Les retombées
impliquées par une normalisation précise des entités nommées sont tres importantes lorsque
les taches de traitement de l’information doivent atteindre un degré de précision important,
comme c'est le cas pour la plupart des acteurs du domaine du traitement automatique des
langues et de l’ingénierie linguistique.

Vanessa Andréani

Une approche endogene permet de couvrir des cas non résolus par d’autres méthodes,
particulierement dans des domaines spécialisés pour lesquels il n’existe pas ou peu de
ressources, et d’autant moins lorsque le travail porte sur les entités nommées.

Le fait d’allier cette méthode a d’autres techniques, comme l’utilisation de patrons ou des
mesures de similarité, permet d’obtenir une couverture bien plus large que ce vers quoi nous
pourrions tendre avec un systeme n’utilisant qu’un seul type de processus. De cette maniere,
tous les points de difficulté peuvent étre traités. De plus, l’intervention de l’utilisateur dans la
constitution des données garantit leur qualité et leur ﬁabilité, indispensables aux traitements
ultérieurs qui prennent ces informations en entrée.

Références

ALPHONSE E., AUBIN S., BESSIERES P., BISSON G., HAMON T., LAGARRIGUE S.,
NAZARENKO A., NEDELLEC C., OULD ABDEL VETAH M., POIBEAU T., WEISSENBACHER D.
(2004). Extraction d'information appliquée au domaine biomédical - apprentissage et
traitement automatique de la langue. Actes de CIF T.

AUSSENAC-GILLES N. (2007). Projet DaFOE4App - Dossier A.0 / Document A.0.I - Etat de
l’art et e’tude des besoins pour une plateforme de construction d’ontologies. Rapport de
contrat, IRIT/RT—2007-2—FR, IRIT.

BOURIGAULT D. (1993). An endogenous Corpus Based Method for Structural Noun Phrase
Disambiguation. Actes de la Conference of the European Chapter of ACL (EACL), 81-86.

BOURIGAULT D., FREROT C. (2006). Acquisition et évaluation sur corpus de propriétés de
sous-catégorisation syntaxique. TAL 47, 141-154.

EHRMANN M., J ACQUET G. (2006). Vers une double annotation des Entités Nommées. TAL 47,
63-88.

ELMAGARMID A.K., IPEIROTIS P.G., VERYKIOS V.S. (2007). Duplicate Record Detection : A
Survey. IEEE Transactions on Knowledge and Data Engineering 19, 1-16.

JIJKOUN V., KHALID M.A., MARX M., DE RIJKE M. (2008). Named Entity Normalization in
User Generated Content. Actes de SIGIR 2008 — Workshop on Analytics for Noisy
Unstructured Text Data.

KHALID M.A., JIJKOUN V., DE RIJKE M. (2008). The Impact of Named Entity Normalization
on Information Retrieval for Question Answering. LNCS 4956.

POIBEAU T. (2003). Extraction automatique d ’information .' du texte brut au web sémantique.
Paris : Hermes.

VERGNE J . (2004). Découverte locale des mots vides dans des corpus bruts de langues
inconnues, sans aucune ressource. Actes des JADT 2004 2, 1158-1164.

