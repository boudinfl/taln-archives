RECITAL 2009, Senlis, 24-26 juin 2009

Combinaison de contenus encyclopédiques multilingues pour
une reconnaissance d’entités nommées en contexte

Eric Charton
(1) LIA / Universite d’Avignon, 339 chemin des Meinajaries, 84911 Avignon
eric.charton@univ-avignon.fr

Resume. Dans cet article, nous presentons une methode de transformation de Wikipedia
en ressource d’information exteme pour detecter et desambiguiser des entites nommees, en
milieu ouvert et sans apprentissage speciﬁque. Nous expliquons comment nous construisons
notre systeme, puis nous utilisons cinq editions linguistiques de Wikipedia aﬁn d’enrichir son
lexique. Pour ﬁnir nous realisons une evaluation et comparons les performances du systeme
avec et sans complements lexicaux issus des informations inter-linguistiques, sur une teche
d’extraction d’entites nommees appliquee a un corpus d’articles joumalistiques.

Abstract. In this paper, we present a way to use of Wikipedia as an external resource to
disambiguate and detect named entities, without learning step. We explain how we build our
system and why we used ﬁve linguistic editions of the Wikipedia corpus to increase the volume
of potentially matching candidates. We ﬁnally experiment our system on a news corpus.
M0tS-CléS I Etiquetage d’entites nommees, ressources semantiques.

Keywords: Named entity labeling, semantic resources.

1 Introduction

L’extraction d’entites nommees (EEN) consiste a localiser precisement des assemblages de
lettres ou de mots en leur attribuant une classe semantique. Ces assemblages - que nous ap-
pellerons desformes de surface - peuvent etre des acronymes, des mots ou des groupes de mots
correspondant de maniere generale a des concepts tels que des produits, des organisations, des
personnes ou encore des lieux.

Attribuer une classe semantique exacte a une expression ecrite est une teche difﬁcile. Pour
une meme entite semantique, il existe souvent plusieurs formes de surface. A titre d’exemple,
considerons les noms d’entreprises qui sont aussi bien exprimes sous forme d’acronymes que de
mots isoles ou groupes (IBM pour International Business Machine). Citons encore les formes de
surface qui, pour decrire un meme lieu, peuvent etre composees d’un ou plusieurs mots (Paris,
Paris Intra Muros), mais aussi prendre la forme de descriptions familieres ou d’expressions
(dans le cas de Paris, Paname, la Wlle Lumiere). Par ailleurs, a une forme de surface identiﬁee
peuvent aussi parfois correspondre plusieurs entites semantiques (Le paquebot Wlle d ’Alger, ou
la ville du meme nom), que seule l’analyse du contexte permet de departager.

Dans cet article, nous decrivons un systeme d’EEN dont l’originalite est de collecter un maxi-
mum de formes de surface disponibles pour une meme entite semantique en exploitant plu-
sieurs versions linguistiques de l’encyclopedie Wikipedial. Nous utilisons ensuite un algo-
rithme d’EEN reposant sur le calcul de la similarite cosinus entre le contexte textuel d’une

1. www.wikipedia.org, promu par la fondation www.wikimedia.org.

Eric Charton

forme candidate identiﬁée et les mots contenus dans l’article encyclopédique qui lui corres-
pond. Pour évaluer l’inﬂuence de l’apport de formes de surface issues d’autres langues que
celle de départ sur le processus d’EEN, nous mesurons les performances obtenues par notre
systeme avec les seules formes de surface issues du corpus francais de Wikipédia puis celles
obtenues apres introduction de formes de surface complémentaires extraites de corpus Wikipé-
dia d’autres langues. Les résultats obtenus démontrent que l’introduction de formes de surface
collectées dans plusieurs éditions linguistiques de Wikipédia peut améliorer les capacités de
détection d’un systeme d’EEN.

Cet article est organise comme suit. Dans la section 2, apres une rapide description des systemes
existants et de leurs évolutions récentes vers des propositions reposant sur des ressources lexi-
cales ou encyclopédiques, nous justiﬁons nos propres choix. Dans la section 3 nous présentons
notre lexique de formes de surface d’entités nommées (EN) et le systeme d’apprentissage concu
pour le construire. Dans la section 4 nous décrivons un systeme complet de détection et de desa-
mbigu'1'sation d’EN. Enﬁn, dans la section 5, nous présentons les résultats de nos expériences,
puis nous concluons sur nos projets futurs.

2 Systémes de détection d’entités nommées

De nombreuses propositions ont été faites pour résoudre le probleme posé par la tache d’extrac-
tion et de reconnaissance des EN (Nadeau & Sekine, 2007). A la ﬁn des années 90, les systemes
de référence (Lafferty et al., 2001; Favre et al., 2005) réalisaient la tache d’EEN en utilisant
des réseaux de connaissances et d’identiﬁcations obtenus d’apres des automates a états ﬁnis ou
des modeles stochastiques appris sur un corpus pré-étiqueté. Ces méthodes qui reposent sur un
entrainement, posent le probleme de l’exhaustivité des corpus d’apprentissage. Pour modéliser
ﬁnement les motifs et sequences a détecter, il faut disposer des corpus d’apprentissage de tres
grande taille, étiquetés et donc coﬁteux. On a donc souvent complété les systemes automatiques
par des éléments lexicaux (dictionnaires de noms propres, de villes, de lieux) aﬁn d’accroitre
leur robustesse.

Malgré cela, les meilleurs systemes hybrides (utilisant apprentissage et connaissances lexicales)
ont parfois du mal a résoudre le probleme des mots hors vocabulaires (Out Of Vocabulary
abrégés par OOV) posé par l’apparition continuelle de nouvelles entités dans le langage. Pour
répondre a ce probleme, une nouvelle génération de systemes d’EEN reposant sur des connais-
sances encyclopédiques (Bunescu & Pasca, 2006; Junichi & Kentaro, 2007) a récemment vu le
jour. La plupart de ces systemes utilisent l’encyclopédie collaborative et multilingue Wikipédia.
Cette ressource contient plus de 6 millions d’entités 2 couvrant un spectre tres large de concepts
(des objets, des individus, des lieux).

Un des avantages de cette ressource encyclopédique est que son contenu est tres rapidement Inis
a jour et augmente continuellement. Dans Wikipédia, les articles concernant des événements ou
des nouveautés sont parfois incorporés quotidiennement. Ces mises a jour sont diffusées de
maniere instantanée 3. Cette possibilité est actuellement l’une des plus performantes qui soit
donnée pour résoudre le probleme des OOV.

L’ aspect multilingue, interconnecté et encyclopédique de Wikipédia peut, lui aussi, se révéler
essentiel dans le cadre applicatif d’un systeme d’EEN. Un nom étranger ou des formes de ce

2. Décompte réalisé sur les ﬁchiers publics de Wikipédia au 16 Oct 2008 dans les 7 langues les plus utilisées a
savoir l’Anglais, l’Allemand, le Frangais, l’Espagnol, l’Italien, le Portugais, le Polonais.

3. Sous forme de dump XML réguliérement diffusés, complétés par la fonction "Modiﬁcations Récentes" du
logiciel Média Wiki.

Combinaison de contenus encyclopédiques multilingues pour une reconnaissance d’entités
nommées en contexte

nom (d’entreprise, d’individu) peuvent apparaitre dans un corpus spéciﬁque a une langue et étre
absents de celui d’une autre : or, cette connaissance interlinguale peut s’avérer tres utile dans le
cadre de l’EEN. On peut illustrer cette afﬁrmation par la forme AMD (le nom d’une entreprise
fabricant des composants électroniques) qui est la seule connue dans le corpus francophone,
alors que la forme Advanced Micro Device n’existe que dans le corpus anglophone4. Pourtant
ces deux formes de surface d’un meme concept peuvent étre rencontrés dans un texte en fran-
cais pour faire reference a la société désignée. Nous souhaitons exploiter cette possibilité de
découvrir dans un corpus linguistique une représentation sémantique utilisable dans une autre
langue pour améliorer l’étiquetage d’EN.

Le systeme que nous proposons construit une représentation lexicale et statistique d’un concept
sémantique d’apres un article encyclopédique, en utilisant plusieurs versions linguistiques de
Wikipédia. Nous intitulerons metadata cette représentation. Chaque metadata contient un graphe
des formes de surface correspondant a une entité nommée a détecter. Ce graphe contient des
formes de surface extraites des cinq éditions linguistiques principales de Wikipédia (anglais,
allemand, francais, italien, espagnol). Ces graphes sont construits d’apres des liens interlingues
(correspondance d’un terme entre plusieurs corpus linguistiques), des redirections internes (plu-
sieurs formes d’écritures d’un mot qui dirigent vers une unique page encyclopédique) et des
pages descriptives d’homonyInie. Le jeu complet de metadata en francais couvre la description
de 215.287 personnes, 172.340 lieux, 72.519 organisations et 90.917 produits 5. Ces concepts
sont représentés par un total de 901.592 formes de surface. Pour réaliser la tache de reconnais-
sance des EN et leur extraction ou labellisation, un jeu d’algorithmes utilise les metadata. Pour
la partie détection et désambigu'1'sation de notre travail, la proposition théorique que nous avons
retenue est celle de (Bunescu & Pasca, 2006). Ces auteurs utilisent une mesure de similarité
cosinus entre les mots issus d’un article de Wikipédia et le contexte d’une séquence de mots a
identiﬁer. Par ailleurs, pour que notre systeme soit en mesure d’étiqueter des EN avec un jeu
de classes réduit tel que celui rencontré lors des campagnes d’évaluation Ester 2 6, ACE (NIST,
2007) ou ConLL (Sang & Meulder, 2003), nous avons développé un systeme de classiﬁcation
des articles de Wikipédia en un arbre a 4 classes sémantiques, compatibles avec les normes
d’étiquetage d’EN, selon une proposition a base de SVM appliqués a Wikipédia proche de celle
de (Wisam & Silviu, 2008) mais légerement améliorée.

3 Construire un systéme pour apprendre les informations sur
les entités

Notre systeme de construction des metadata assure une transformation de la structure interne
de Wikipédia en une représentation sémantiques et statistique. Dans la section 3.1, nous déﬁnis-
sons la structure interne des corpus Wikipédia. Dans la section 3.2 nous décrivons de maniere
formelle une metadata : chaque enregistrement de metadata est composé du nom de l’article
encyclopédique utilisé en tant que clé, d’un graphe représentant les formes de surface poten-
tielles d’une EN, d’un label représentant une classe sémantique et d’un ensemble de mots et
leurs poids t f.idf . Nous expliquons quels éléments d’un article encyclopédique sont utilisés
pour construire une metadata. Puis nous décrivons dans la section 3.3, l’algorithme déployé
pour procéder a la transformation de Wikipédia en metadata. Nous illustrons, pour ﬁnir, ces
étapes par un exemple.

4. cc cas est Vériﬁable a cette adresse http ://www.n1gbase.org/perl/disp1ay.p1?query=AdVanced Micro De-
Vices&search=FR.
5. Comptage par le systéme NLGbAse, Voir statistiques a jour sur www.nlgbase.org.
6. Articles at paraitre, Voir http ://WWW.afcp-parole.org/ester/index.ht1n1.

Eric Charton

3.1 Structure de la ressource encyclopédique Wikipédia

       
 

W = Contenu encyclopédique fr

C = sous-corpus linguistique

 
 
     
   
 

 
   
   

en
Csouscomus linguistique Dvmrmgo
D.t = "Victor Hugo"
D’ V'H”"° D.w="Victor Hugo
D.r=depuis page redirect - tné[...]
D"'°‘°'”“"° D.i = liens interwiki
Du D.t = "\Iictor Hugo"
D t : Hugo D.w="VIctor Hugo
' born In [...]

Lien de désambi uation
D Hugo q/clan

D.t = "Hugo Cyclon"
D.w="This cyc|on[...]

FIGURE 1 — Structure des données dans Wikipédia

Dans notre application, considérons M qui représente l’encyclopédie Wikipédia et 0‘ un corpus

représentant une version linguistique. Cette version linguistique contient des articles structurés

d’ apres une DTD 7. Un ensemble d’espaces de nom 8 décrit par la DTD permet de reconnaitre les

articles (contenus dans l’espace encyclopédique) des autres documents (Modeles, Utilisateurs,

Images). Chaque article contient en ensemble de mots, en relation avec le concept encyclopé-

dique qu’il décrit.

Considérons D un article du corpus Wikipédia Cl, déﬁnit par des propriétés :

— D.t est un titre composé d’une séquence de mots.

— D.w est la liste des mots contenus dans l’article.

— Si un article est susceptible d’ambigiiité par homonymie de son titre descriptif, une page
spéciale intitulée Page d’homonymie dans la version francaise ou Disambiguation page
dans la version anglaise est créée. Nous intitulerons ces pages Dd 6 Cl. Elles contiennent
plusieurs references a des pages D E C‘ a désambiguiser.

— Des articles uniques dits Pages de redirection existent pour répertorier les noms altematifs
susceptibles de correspondre a un article de Wikipédia. Nous utilisons ces pages pour générer
les graphes de formes de surface. Nous appelons D" une page de redirection et D".R le lien
de redirection unique qu’elle contient et qui correspond au nom original D.t de l’article D
vers lequel elle redirige.

— Chaque article contenu dans une version linguistique de Wikipédia peut inclure des liens
vers les articles similaires contenus dans d’autres versions linguistiques de l’encyclopédie.
Ces liens sont intitulés des Interwiki. Nous intitulons relation interwiki D.z', le lien de D
vers son équivalent dans un corpus d’une autre langue C” de M.

7. Consulter http ://metawikimedia.org/wiki/Wikipedia.DTD.
8. Lire fr.wikipedia.org/wiki/Aide :Espace_de_noms pour des précisions sur1’espace dc nom.

Combinaison de contenus encyclopédiques multilingues pour une reconnaissance d’entités
nommées en contexte

3.2 Les metadata produites d’aprés la ressource encyclopédique

E“°‘°““‘-‘°= Entité extraite

Ambiguiié,exisianidansuneautreeniiiéE E i : FR Victor Hugo
.t = Victor Hugo

Victor Hugo

          
         

   

   

E.Vx.li

Autres formes de surfaces issues

d’autres do

E.w

   

Terme TF.|DF
Né 200
France 196
Ecrivain 122
[---i

FIGURE 2 — Structure des données d’une Metadata

Les metadata de notre application sont composées d’un jeu d’entités E dérivées de D 6 Cl.

Chaque entité E 6 Cl est déﬁnie par un jeu de propriétés (E.t, E.v, E.w, E.i, 

— E .75 est le titre de l’entité, correspondant a D.t titre unique d’une page Wikipedia.

— E.v est l’ensemble de toutes les formes de surface qui peuvent écrire E. E.v contient donc
des formes de surface synonymes. Cet ensemble est construit d’apres les pages de redirection
D’ reliées a la page D utilisée pour constr11ire E . On notera que lorsqu’une page d’homony-
mie Dd existe, contenant des liens vers la page D utilisée pour constr11ire E, le titre Dd.t de
Dd est inclus dans E.v.

— E .i est l’ensemble de relations interwiki contenues dans la propriété D.i correspondante. E .i
représente la relation entre E E C‘ et tout E’ 6 0'“, lo 73 l. Ceci signiﬁe que E.i contient
les références des entités correspondantes dans les autres corpus linguistiques que celui de
départ.

— E.w est un ensemble de mots avec leurs poids (exprimés sous la forme d’une valeur t f .idf)
associés a l’entité. Cet ensemble de propriétés est construit d’apres le texte D.w contenu dans
l’article D original de Wikipédia, utilisée pour construire E.

Il est essentiel d’associer aux entités extraites depuis Wikipédia une classe pour rendre la detec-

tion exploitable mais aussi conforme aux spéciﬁcations des campagnes d’évaluation.

— Nous ajoutons donc aux me’tadata une propriété E.k qui est un label d’étiquetage personne,
produit, lieu ou organisation en accord avec le standard ESTER 29. Une classe spéciﬁque
intitulée unknown est par ailleurs introduite pour retirer de la liste de détection des EN les
articles relatifs a des descriptions encyclopédiques inutiles pour la tache d’EEN (un théoreme
mathématique, une mode).

La méthode de classiﬁcation retenue est basée sur une combinaison de trois classiﬁeurs : SVM-

Lib, un classiﬁer bayésien na'1'f, Icsiboost (AdaBoost). Plusieurs méthodes de normalisation sont

appliquées aux textes utilisés pour construire les classes. Cette méthode est décrite en détails

dans (Charton et al. , 2008) eta été déployée lors de la campagne d’évaluation DEFT’08 (Grouin
et al., 2008) sur un corpus incluant notamment des données de Wikipedia.

9. Voir la convention d’a1motation sur http ://www.afcp-parole.org/ester/docs/Conventions_EN_ESTER2_V01.pdf.

Eric Charton

3.3 Transformation d’un article encyclopédique en metadata

Une description formelle de l’algorithme de construction d’une metadata d’apres un article
encyclopédique peut étre présentée comme suit :

Considérons que tout article Wikipedia D 6 Cd est déﬁni par une ensemble de propriétés
(D.t, D.w, D.r,D.i) telles que D.t est un titre représenté par une séquence de mots, D.w
un ensemble de mots, D.r une relation unique entre D et n’importe quel élément de Cd, D.i
est un ensemble de relations interwiki entre des éléments de D et n’importe quel élément de
Cd’ 6 M \Cd. Pour générer une table T‘ d’entités, nous explorons dans un premier temps Cd
et conservons tout élément de D qui n’est pas une page de redirection (Dd) ou de Page d ’ho-
monymie (Dd). On considere que E et D sont en relation si et seulement si E.t = D.t. Ceci
est formalisé par E —> D. Puis, pour tout E de la table T‘, nous cherchons tous les Dd 6 Cd
en relation avec E et incluons le nom alternatif Dd.t dans E.v. Nous cherchons aussi tous les
Dd 6 Cd en relation avec E et déﬁnissons E.v = Dd.t.

Ces étapes sont répétées pour les corpus de toutes les versions linguistiques de Wikipédia rete-
nues. Nous obtenons donc pour chaque corpus linguistique de Wikipedia Cd une entité El. Puis,
pour procéder a l’agrégation des formes de surface localisées dans ces différentes versions, nous
collectons les relations interwiki disponibles dans la section D.i de l’article encyclopédique et
les attribuons a la propriété  Nous agrégeons toutes les formes de surface EL” dans l’entité
E.v.

3.4 Exemple de transformation d’un article encyclopédique en metadata

La structure d’une metadata E.Wctor Hugo constr11ite d’apres l’article D.\/zctor Hugo est pre-
sentée dans la ﬁgure 2. Les informations originales D contenues dans Wikipédia ayant servi
a construire E sont illustrées dans la ﬁgure 1. Pour construire E, considérons que le titre
D.t =Wctor Hugo de la page Wikipedia est utilisé en tant que descripteur unique E.t de la
metadata Wctor Hugo. Les formes de surface sont ensuite collectées pour construire le graphe
E.v. D’abord, nous ajoutons le titre original D.t a E.v. Ce titre encyclopédique original est
toujours la forme minimale (et parfois unique) d’écriture possible pour E.v. Les pages de re-
direction de Wikipedia D’ contenant un lien D’.R vers D =Wctor Hugo sont recherchées;
chaque titre D'.t d’une page de redirection est un synonyme de Wctor Hugo et est inclus dans
E.v. Nous cherchons les Page d ’homonymie Dd qui contiennent des liens vers l’entité D. Ces
liens sont contenus a l’intérieur du corps de texte de Dd en accord avec les spéciﬁcations du for-
malisme de Wikipedia. Dans l’exemple de Wctor Hugo, la page d ’homonymie Dd est “Hugo” et
est reliée a 45 autres pages encyclopédiques qui décrivent des personnes (la page Wctor Hugo
elle-méme), des lieux, des événements (ouragans) ou des organisations (un prix littéraire). Nous
ajoutons le titre Dd.t = Hugo a E.v.

L’ agrégation ﬁnale de toutes les formes de surface est réalisée par l’eXploration des relations
interwiki. Les formes collectées sont indiquées dans la ﬁgure 2 par le lien E On peut observer
une introduction de bruit lors de la collecte des formes de surface. En effet le graphe de formes
de surface de Wctor Hugo contient 14 formes valides et une forme erronée, Adele Foucher. Ce
phénomene est lié au caractere manuel de la saisie des pages de redirection par les contributeurs
de Wikipédia. Nous discutons de son inﬂuence dans nos expériences.

Tous les termes de l’article Wctor Hugo de la version linguistique francaise de Wikipédia font
l’objet d’un calcul de poids t f.idf et sont intégrés dans E.w. On observe que les terInes de
poids le plus fort sont hautement contextuels (Ecrivain, France, etc) 1°.

10. Voir http ://www.nlgbase.org/per]/display.pl ?query=Victor Hugo&search=FR pour un descriptif complet.

Combinaison de contenus encyclopédiques multilingues pour une reconnaissance d’entités
nommées en contexte

Gao (Region) Gao Xingjian Gao (Ville)
mali 244.92 nobel 231.3 mali 263.78
bozo 235.36 litteracy 225.49 songhay 251.27
region 219.44 tien 225.22 city 212.45
kidal 218.67 chinese 224.23 river 29.81

TABLE 1 — Trois représentations différentes de metadata partageant le meme nom (Gao) et la
table des mots contextuels, accompagnés de leurs poids t f .idf , pour chacun de ces noms

4 Algorithme du systéme d’eXtraction, d’identiﬁcation et de
désambiguisation des entités nommées

Considérons une phrase S contenant une séquence de mots s; pour tout s, nous recherchons
dans les metadata extraites depuis Wikipédia une forme de surface ou plus, candidate pour
étiqueter l’EN éventuellement contenue dans s. Nous intitulons Rm l’ensemble des formes
de surface candidates. Pour détecter les EN, et éventuellement les désambigu'1'ser, le systeme
d’EEN utilise les mots contextuels contenus dans E.w avec leurs poids t f.idf aﬁn de calculer
le degré de similarité entre une forme de surface candidate et son contexte textuel dans 3.

Le systeme de détection est divisé en 2 algorithmes intitulés A1 et A2 qui exploitent deux
fonctions 2

1. La fonction Rm = fsynsets (3) recherche dans les metadata les éléments des E11 qui
correspondent a un sous ensemble de 3.

2. La fonction fs,-,,,a,s(Rm, S) calcule la similarité cosinus de S comparée a tout élément E
de Rm d’apres E.w.

Dans A1, nous considérons que si |Rm| = 1, Rm est l’unique EN candidate valable. Nous
mesurons alors la similarité cosinus entre le contexte de l’EN et les poids de mots contenus
dans E.w. Si |Rm| > 1, nous en concluons que l’algorithme a trouvé un ensemble d’entités
ambigiies 2 A2 recherche alors la meilleure entité contenue dans Rm avec les scores de similarité
cosinus. On considere que A1 ou A2 ne proposent d’entité que si le score de similarité cosinus
entre des entités candidates et le contexte d’étiquetage est supérieur a un seuil cs.

Finalement, si A1 or A2 ne proposent pas d’entité candidate pour s, nous décidons que 3 n’est
pas une entité. Conformément a cette description, notre systeme fonctionne de la maniere sui-
vante 2

_ Rm = fsynset(5)

— A1 2 Une seule proposition dans Rm (métadonnée candidate unique) > si fs,-mws > 03 D extraction

d’entité
— A2 2 Plusieurs propositions dans Rm (Ambiguité de détection entre plusieurs EN)

1. Utilise un score de similarité cosinus pour classer les propositions

(a) A2.1 2 Le meilleur du classement de fs,-mos > 03 > extraction d’EN
(b) A2.2 2 fs,-mm, < 03 pas d’EN candidate acceptable > pas d’EN

Le processus de détection débute avec fsy,,se,;(s) qui doit identiﬁer les séquences de mots 3 de
la phrase a étiqueter S identiques a des formes de surface contenues dans les metadata.

Eric Charton

[...] cinq équipe de F1 2 Toyota, [Williams], Sauber,
Red-Bull, et Minardi, sur dix engagées dans le [...]

Entité correspondante pour for Williams Score Cosinus
1 Williams (F1 Team) 0.91
2 Franck Williams 0.45
3 John Williams 0
4 Robby Williams 0

TABLE 2 — Exemple de localisation d’une entité dans son contexte, en utilisant le classement de
la mesure de similarité cosinus pour identiﬁer le terme le plus approprié.

Considérons M le corpus Wikipédia et E les entités extraites représentées par des formes de
surface contenues dans E .11 :
_ fsynset(5)
1. 8 = {E0, ..., En} ensemble d’entités extraites de M.

2. Chaque En contient un ensemble de En.v, {v0,  formes de surface utilisées pour la détec-
tion

3. s = {t0, ..., tu} ensemble de séquences de mots (n-gram a 1-grarn) t E S
4. Pour tout 5"‘ E S,

(a) Rm = 0

(b) Pour tout En E 8 , Um E E,,.v

(c) SI ( um = 5"‘ ) alors add En to Sd
5. Retourner Rm

Si Rm retourné par fsynset(8) ne contient qu’une proposition, A1 s’applique, sinon A2 est
invoqué. Ce serait le cas avec l’entité Gao (voir tableau 1), qui exprime trois concepts différents
(personne, région, ville).

Chaque E inclut un ensemble de mots avec leurs t f .idf contenus dans E.w. Considérons pour
la phrase S un groupe de mots X avec ses poids X .t f.idf . Ce groupe est intitulé contexte. Le
contexte est composé de q mots a droite et a gauche de l’entité E identique a s E S.
Considérant que la fonction de similarité cosinus c0s(E .t f .idf , X .t f .idf ) existe, nous pouvons
déterminer quelle entité candidate EC de Rm obtient le plus haut score en mesurant le cosinus
de l’angle entre les vecteurs de poids des mots correspondant a Ec.w de Rm et les X .t f .idf .
Nous obtenons alors une liste de scores indiquant la meilleure entité candidate EC dans son
contexte. Nous utilisons dans A2.1 la formule :

A

E = argmaJ:En sc0re(c0s(E,,.tf.idf, X.tf.idf))

Un exemple de liste de scores produite par A2 pour un contexte donné, est présenté dans le
tableau 2.

5 Expériences et résultats

Pour nos expériences, nous avons utilisé 100 articles de presse du quotidien Francais Le Monde
issus du corpus fourni durant la campagne d’évaluation DEFT’07. Nous avons étiqueté ces ar-
ticles de maniere semi-automatique, par une premiere application de notre systeme, suivie d’une
correction manuelle. Les 100 articles représentaient 1.996 phrases, 61.000 mots et 1.307 entités
a étiqueter. Nous avons appliqué un systeme de détection d’EN concu d’apres les algorithmes

Combinaison de contenus encyclopédiques multilingues pour une reconnaissance d’entités
nommées en contexte

décrits précédemment. Nous avons évalué les résultats de notre systeme par précision, rappel
et F-Score. Aﬁn de déterminer quelle était l’inﬂuence de l’incorporation de formes de surface
issues de corpus d’une autre langue sur l’étiquetage de documents en francais, nous avons testé
notre systeme avec les deux conﬁgurations suivantes :

1. Etiquetage d’EN en utilisant des metadata incluant uniquement des formes de surface
issues de la version linguistique francaise de Wikipédia.
2. Etiquetage d’EN en utilisant des metadata incluant des formes de surface issues de cinq
versions linguistiques de Wikipédia (francais, anglais, espagnol, italien, allemand).
Les résultats obtenus sont indiqués dans le tableau 3.

Systeme (p) (r) (F-m)
Métadata francaises 0.89 0.87 0.88
Métadata francaises

et cross-linguistiques 0.90 0.91 0.91

TABLE 3 — Evaluation et comparaison des performances avec des métadata et des formes de
surface issues du corpus linguistique francais de Wikipédia, et de celles issues de cinq corpus
linguistiques

5.1 Discussions

Le tableau 3 montre que l’introduction de ressources interlinguales dans le systeme de de-
tection d’entités augmente ses performances. On peut en déduire que si les performances de
l’algorithme de mesure par similarité cosinus restent constantes, l’ajout de nouvelles formes
de surface pour les entités, collectées depuis les versions anglaise, espagnole, italienne et alle-
mande du corpus Wikipédia a amélioré la couverture du systeme et limité les OOV. Nous notons
que le bruit introduit par la présence des formes de surface sémantiquement non correctes (voir
la section 3.3) est sans inﬂuence sur les performances du systeme d’EEN : le calcul de similarité
cosinus entre la metadata et le contexte textuel d’une EN candidate les écarte automatiquement
et efﬁcacement du processus de detection.

6 Conclusion et travaux futurs

Dans cet article nous avons présenté une nouvelle ressource sémantique qui peut étre utilisée
pour détecter les variations d’écriture et les ambigu'1'tés d’une EN dans son contexte.

L’ originalité de ce systeme est qu’il utilise des formes de surface collectées via une exploration
des relations interlinguales de corpus encyclopédiques. Nos expériences ont montré que cette
démarche pouvait améliorer les performances du systeme d’EEN et sa capacité a détecter des
variations d’écritures.

Nous envisageons maintenant de mener des expériences sur des combinaisons linguistiques en
utilisant le potentiel offert par les 253 langues disponibles dans Wikipédia. Notre idée serait de
déterminer s’il existe des combinaisons interlinguales de formes de surface plus performantes
pour la tache d’EEN que celle expérimentée ici.

La ressource mise au point, intitulée N LGbAse peut étre consultée et téléchargée librement. Le
systeme d’EEN peut étre expérimenté en ligne et sera prochainement diffusé sous une forme
libre 11. Dans une perspective plus large d’amé1ioration d’un systeme de reconnaissance exis-
tant, nous avons également prévu d’intégrer notre systeme en tant que ressource lexicale et

11. Consulter le site www.nlgbase.org.

Eric Charton

fonction complémentaire d’EEN a postériori a un systeme d’EEN stochastique. Un prototype
basé sur le programme LIA_NE 12 a été déployé lors de la campagne ESTER 2 et fera l’objet
de communication ultérieure.

Notre objectif principal lorsque nous avons entamé ce travail était de concevoir un ensemble de
metadata issu d’un contenu encyclopédique et de l’associer a des algorithmes d’étiquetage et
d’extraction d’information en vue de participer aux campagnes d’évaluation telles que KBP13.
Nous travaillons actuellement a cette évolution.

Remerciements

J e remercie vivement les relecteurs pour leurs commentaires et propositions particulierement
détaillés et utiles.

Références

BUNESCU R. & PASCA M. (2006). Using encyclopedic knowledge for named entity di-
sambiguation. In EACL Proceedings of the Third International Joint Conference on Natural
Language Processing, April 3-7, 2006, Trento, Italy : EACL.

CHARTON E., CAMELIN N., ACUNA-AGOST R., GOTAB P., KESSLER R., LAVALLEY R.
& FERNANDEZ S. (2008). Pré-traitements classiques ou par analyse distributionnelle :appli-
cation aux méthodes de classiﬁcation. In Atelier De’ﬁ Fouille de Texte, Actes de TALN 2008,
Avignon : DEFT.

FAVRE B., BECHET F. & NOCERA P. (2005). Robust named entity extraction from large
spoken archives. In Proceedings of HLT-EMNLP’05, Vancouver (Canada) : HLT-EMNLP.

GROUIN C., BERTHELIN J .—B., AYARI S. E., HURAULT-PLANTET M. & LOISEAU S.
(2008). Presentation de deft08 (deﬁ fouille de textes). In Atelier De’ﬁ Fouille de Texte, Actes
de TALN 2008, Avignon : DEFT.

J UNiCHI K. & KENTARO T. (2007). Exploiting wikipedia as external knowledge for named
entity recognition. In Joint Conference on Empirical Methods in Natural Language Processing
and Computational Natural Language Learning, p. 698-707.

LAFFERTY J ., MCCALLUM A. & PEREIRA F. (2001). Crf : Probalistic models for segmenting

and labeling sequence data. In Proceedings of the Eighteenth International Conference on
Machine Learning(ICML-2001) : IMCL.

NADEAU D. & SEKINE S. (2007). A survey of named entity recognition and classiﬁcation.
In Lingvisticae Investigationes, Vol 30, number I, September 2007.

NIST (2007). : NIST.

SANG E. F. T. K. & MEULDER F. D. (2003). Introduction to the conll-2003 shared task :
Language-independent named entity recognition. In Seventh Conference on Natural Language
Learning, May 31 and June I, 2003, HLT-NAACL 2003, Edmonton, Canada : CoNLL.

WISAM D. & SILVIU C. (2008). Augmenting wikipedia with named entity tags. In ACL
Proceedings of the Third International Joint Conference on Natural Language Processing :
ACL.

12. Disponible sur http ://lia.univ-avignon.fr/ﬁleadmin/documents/U sers/Intranet/chercheurs/bechet/download_fred.ht1n1.
13. Knowledge Base Population, Tache de TAC 2009, en cours. Voir http ://apl.jhu.edu/paulmac/kbp.ht1n1.

