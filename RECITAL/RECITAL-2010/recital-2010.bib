@proceedings{RECITAL:2010,
  editor    = {Patry, Alexandre and Langlais, Philippe and Max, Aurélien},
  title     = {Actes des 12e Rencontres des Étudiants Chercheurs en Informatique pour le Traitement Automatique des Langues},
  month     = {July},
  year      = {2010},
  address   = {Montréal, Canada},
  publisher = {Association pour le Traitement Automatique des Langues},
  url       = {http://www.atala.org/taln_archives/RECITAL/RECITAL-2010}
}

@inproceedings{laroche:2010:RECITAL,
  author    = {Laroche, Audrey},
  title     = {Attribution d’auteur au moyen de modèles de langue et de modèles stylométriques},
  booktitle = {Actes des 12e Rencontres des Étudiants Chercheurs en Informatique pour le Traitement Automatique des Langues},
  month     = {July},
  year      = {2010},
  address   = {Montréal, Canada},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/RECITAL/RECITAL-2010/recital-2010-long-001},
  language  = {french},
  resume    = {Dans une tâche consistant à trouver l’auteur (parmi 53) de chacun de 114 textes, nous analysons la performance de modèles de langue et de modèles stylométriques sous les angles du rappel et du nombre de paramètres. Le modèle de mots bigramme à lissage de Kneser-Ney modifié interpolé est le plus performant (75 % de bonnes réponses au premier rang). Parmi les modèles stylométriques, une combinaison de 7 paramètres liés aux parties du discours produit les meilleurs résultats (rappel de 25 % au premier rang). Dans les deux catégories de modèles, le rappel maximal n’est pas atteint lorsque le nombre de paramètres est le plus élevé.},
  abstract  = {In a task consisting of attributing the proper author (among 53) of each of 114 texts, we analyze the performance of language models and stylometric models from the point of view of recall and the number of parameters. The best performance is obtained with a bigram word model using interpolated modified Kneser-Ney smoothing (first-rank recall of 75 %). The best of the stylometric models, which combines 7 parameters characterizing the proportion of the different parts of speech in a text, has a firstrank recall of 25 % only. In both types of models, the maximal recall is not reached when the number of parameters is highest.},
  motscles  = {Attribution d’auteur, modèle de langue, stylométrie, n-grammes, vecteurs de traits},
  keywords  = {Authorship attribution, language model, stylometry, n-grams, feature vectors},
}

@inproceedings{lee-EtAl:2010:RECITAL,
  author    = {Lee, Hyeran and Gambette, Philippe and Maillé, Elsa and Thuillier, Constance},
  title     = {Densidées : calcul automatique de la densité des idées dans un corpus oral},
  booktitle = {Actes des 12e Rencontres des Étudiants Chercheurs en Informatique pour le Traitement Automatique des Langues},
  month     = {July},
  year      = {2010},
  address   = {Montréal, Canada},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/RECITAL/RECITAL-2010/recital-2010-long-002},
  language  = {french},
  resume    = {La densité des idées, qui correspond au ratio entre le nombre de propositions sémantiques et le nombre de mots dans un texte reflète la qualité informative des propositions langagières d’un texte. L'apparition de la maladie d'Alzheimer a été reliée à une dégradation de la densité des idées, ce qui explique l'intérêt pour un calcul automatique de cette mesure. Nous proposons une méthode basée sur un étiquetage morphosyntaxique et des règles d'ajustement, inspirée du logiciel CPIDR. Cette méthode a été validée sur un corpus de quarante entretiens oraux transcrits et obtient de meilleurs résultats pour le français que CPIDR pour l’anglais. Elle est implémentée dans le logiciel libre Densidées disponible sur http://code.google.com/p/densidees.},
  abstract  = {Idea density, which is the ratio of semantic propositions divided by the number of words in a text, reflects the informative quality of the sentences of a text. A decreasing idea density has been identified as one of the symptoms of Alzheimer’s disease, which explains the interest in an automatic calculation of idea density. We propose a method based on part-of-speech tagging followed by adjustment rules inspired from the CPIDR software. This method was validated on a corpus of 40 transcribed conversations in French and obtains better results in French than CPIDR in English. It is implemented in the free software Densidées available at http://code.google.com/p/densidees.},
  motscles  = {densité des idées, analyse prédicative, étiquetage sémantique, psycholinguistique},
  keywords  = {idea density, propositional analysis, semantic tagging, psycholinguistics},
}

@inproceedings{wu:2010:RECITAL,
  author    = {Wu, Li-Chi},
  title     = {Outils de segmentation du chinois et textométrie},
  booktitle = {Actes des 12e Rencontres des Étudiants Chercheurs en Informatique pour le Traitement Automatique des Langues},
  month     = {July},
  year      = {2010},
  address   = {Montréal, Canada},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/RECITAL/RECITAL-2010/recital-2010-long-003},
  language  = {french},
  resume    = {La segmentation en mots est une première étape possible dans le traitement automatique de la langue chinoise. Les systèmes de segmentation se sont beaucoup développés depuis le premier apparu dans les années 1980. Il n’existe cependant aucun outil standard aujourd’hui. L’objectif de ce travail est de faire une comparaison des différents outils de segmentation en s’appuyant sur une analyse statistique. Le but est de définir pour quel type de texte chacun d’eux est le plus performant. Quatre outils de segmentation et deux corpus avec des thèmes distincts ont été choisis pour cette étude. À l’aide des outils textométriques Lexico3 et mkAlign, nous avons centré notre analyse sur le nombre de syllabes du chinois. Les données quantitatives ont permis d’objectiver des différences entre les outils. Le système Hylanda s’avère performant dans la segmentation des termes spécialisés et le système Stanford est plus indiqué pour les textes généraux. L’étude de la comparaison des outils de segmentation montre le statut incontournable de l’analyse textométrique aujourd’hui, celle-ci permettant d’avoir accès rapidement à la recherche d’information.},
  abstract  = {Chinese word segmentation is the first step in Chinese natural language processing. The system of segmentation has considerably developed since the first automatic system of segmentation of the 1980’s. However, till today there are no standard tools. The aim of this paper is to compare various tools of segmentation by through statistical analysis. Our goal is to identify the kind of texts for which these segmentation tools are the most effective. This study chose four segmentation tools and two corpora, marked by distinct themes. Using two textometric toolboxes, Lexico3 and mkAlign, we focused on the number of syllables in Chinese. The quantitative data allowed us to objectify disparities between tools. The Hylanda system turns out to be effective in the segmentation of specialized terms and the Stanford system is more appropriate for general texts. The comparative study of segmenters shows the undeniable status of textometrical analysis which is able to quickly access information retrieval.},
  motscles  = {Textométrie, comparaison des segmenteurs chinois, nombre de syllabes},
  keywords  = {Textometry, comparison of Chinese segmenters, number of syllables},
}

@inproceedings{ezzat:2010:RECITAL,
  author    = {Ezzat, Mani},
  title     = {Acquisition de grammaires locales pour l’extraction de relations entre entités nommées},
  booktitle = {Actes des 12e Rencontres des Étudiants Chercheurs en Informatique pour le Traitement Automatique des Langues},
  month     = {July},
  year      = {2010},
  address   = {Montréal, Canada},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/RECITAL/RECITAL-2010/recital-2010-long-004},
  language  = {french},
  resume    = {La constitution de ressources linguistiques est une tâche cruciale pour les systèmes d’extraction d’information fondés sur une approche symbolique. Ces systèmes reposent en effet sur des grammaires utilisant des informations issues de dictionnaires électroniques ou de réseaux sémantiques afin de décrire un phénomène linguistique précis à rechercher dans les textes. La création et la révision manuelle de telles ressources sont des tâches longues et coûteuses en milieu industriel. Nous présentons ici un nouvel algorithme produisant une grammaire d’extraction de relations entre entités nommées, de manière semi-automatique à partir d’un petit ensemble de phrases représentatives. Dans un premier temps, le linguiste repère un jeu de phrases pertinentes à partir d’une analyse des cooccurrences d’entités repérées automatiquement. Cet échantillon n’a pas forcément une taille importante. Puis, un algorithme permet de produire une grammaire en généralisant progressivement les éléments lexicaux exprimant la relation entre entités. L’originalité de l’approche repose sur trois aspects : une représentation riche du document initial permettant des généralisations pertinentes, la collaboration étroite entre les aspects automatiques et l’apport du linguiste et sur la volonté de contrôler le processus en ayant toujours affaire à des données lisibles par un humain.},
  abstract  = {Building linguistics resources is a vital task for information extraction systems based on a symbolic approach : cascaded patterns use information from digital dictionaries or semantic networks to describe a precise linguistic phenomenon in texts. The manual elaboration and revision of such patterns is a long and costly process in an industrial environment. This work presents a semi-automatic method for creating patterns that detect relations between named entities in corpora. The process is made of two different phases. The result of the first phase is a collection of sentences containing the relevant relation. This collection isn’t necessairly big. During the second phase, an algorithm automatically produces the recognition grammar by generalizing the actual content of the different relevant sentences. This method is original from three different points of view : it uses a rich description of the linguistic content to allow accurate generalizations, it is based on a close collaboration between an automatic process and a linguist and, lastly, the output of the acquisition process is always readable and modifiable by the end user.},
  motscles  = {relation, entité nommée, grammaire},
  keywords  = {relation, named entity, pattern},
}

@inproceedings{bouamor:2010:RECITAL,
  author    = {Bouamor, Houda},
  title     = {Construction d’un corpus de paraphrases d’énoncés par traduction multiple multilingue},
  booktitle = {Actes des 12e Rencontres des Étudiants Chercheurs en Informatique pour le Traitement Automatique des Langues},
  month     = {July},
  year      = {2010},
  address   = {Montréal, Canada},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/RECITAL/RECITAL-2010/recital-2010-long-005},
  language  = {french},
  resume    = {Les corpus de paraphrases à large échelle sont importants dans de nombreuses applications de TAL. Dans cet article nous présentons une méthode visant à obtenir un corpus parallèle de paraphrases d’énoncés en français. Elle vise à collecter des traductions multiples proposées par des contributeurs volontaires francophones à partir de plusieurs langues européennes. Nous formulons l’hypothèse que deux traductions soumises indépendamment par deux participants conservent généralement le sens de la phrase d’origine, quelle que soit la langue à partir de laquelle la traduction est effectuée. L’analyse des résultats nous permet de discuter cette hypothèse.},
  abstract  = {Large scale paraphrase corpora are important for a variety of natural language processing applications. In this paper, we present an approach which collects multiple translations from several languages proposed by volunteers in order to obtain a parallel corpus of paraphrases in French. We hypothesize that two translations proposed independently by two volunteers usually retain the meaning of the original sentence, regardless of the language from which the translation is done. The analysis of results allows us to discuss this hypothesis.},
  motscles  = {corpus monolingue parallèle, paraphrases, traductions multiples},
  keywords  = {monolingual parallel corpora, paraphrases, multiple translations},
}

@inproceedings{bouabdallah:2010:RECITAL,
  author    = {Bouabdallah, Amaria Adila},
  title     = {Ces noms qui cachent des événements : un premier repérage},
  booktitle = {Actes des 12e Rencontres des Étudiants Chercheurs en Informatique pour le Traitement Automatique des Langues},
  month     = {July},
  year      = {2010},
  address   = {Montréal, Canada},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/RECITAL/RECITAL-2010/recital-2010-long-006},
  language  = {french},
  resume    = {La détection des informations temporelles est cruciale pour le traitement automatique des textes, qu’il s’agisse de modélisation linguistique, d’applications en compréhension du langage ou encore de tâches de recherche documentaire ou d’extraction d’informations. De nombreux travaux ont été dédiés à l’analyse temporelle des textes, et plus précisément l’annotation des expressions temporelles ou des événements sous leurs différentes formes : verbales, adjectivales ou nominales. Dans cet article, nous décrivons une méthode pour la détection des syntagmes nominaux dénotant des événements. Notre approche est basée sur l’implémentation d’un test linguistique simple proposé par les linguistes pour cette tâche. Nous avons expérimenté notre méthode sur deux corpus différents ; le premier est composé d’articles de presse et le second est beaucoup plus grand, utilisant une interface pour interroger automatiquement le moteur de recherche Yahoo. Les résultats obtenus ont montré que cette méthode se révèle plus pertinente pour un plus large corpus.},
  abstract  = {The detection of temporal information is a crucial task for automatic text processing. It is not only used in linguistics for the modelization of phenomenon and reasoning implying time entities but also in numerous applications in language comprehension, information retrieval, question-answering and information extraction. Many studies have been devoted to the temporal analysis of texts, and more precisely to the tagging of temporal entities and relations occurring in texts. Among these lasts, the various avatars of events in their multiples occurring forms has been tackled by numerous works. In this article we describe a method for the detection of noun phrases denoting events. Our approach is based on the implementation of a simple linguistic test proposed by linguists for this task. Our method is applied on two different corpora ; the first is composed of newspaper articles and the second, a much larger one, rests on an interface for automatically querying the Yahoo search engine. Primary results are encouraging and increasing the size of the learning corpora should allow for a real statistical validation of the results.},
  motscles  = {Repérage des événements nominaux, annotation temporelle},
  keywords  = {Nominal event recognition, temporal annotation},
}

@inproceedings{chardon:2010:RECITAL,
  author    = {Chardon, Baptiste},
  title     = {Catégorisation automatique d'adjectifs d'opinion à partir d'une ressource linguistique générique},
  booktitle = {Actes des 12e Rencontres des Étudiants Chercheurs en Informatique pour le Traitement Automatique des Langues},
  month     = {July},
  year      = {2010},
  address   = {Montréal, Canada},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/RECITAL/RECITAL-2010/recital-2010-long-007},
  language  = {french},
  resume    = {Cet article décrit un processus d’annotation manuelle de textes d’opinion, basé sur un schéma fin d'annotation indépendant de la langue et du corpus. Ensuite, à partir d'une partie de ce schéma, une méthode de construction automatique d'un lexique d'opinion à partir d'un analyseur syntaxique et d'une ressource linguistique est décrite. Cette méthode consiste à construire un arbre de décision basé sur les classes de concepts de la ressource utilisée. Dans un premier temps, nous avons étudié la couverture du lexique d'opinion obtenu par comparaison avec l’annotation manuelle effectuée sur un premier corpus de critiques de restaurants. La généricité de ce lexique a été mesurée en le comparant avec un second lexique, généré à partir d'un corpus de commentaires de films. Dans un second temps, nous avons évalué l'utilisabilité du lexique au travers d'une tâche extrinsèque, la reconnaissance de la polarité de commentaires d'internautes.},
  abstract  = {This paper introduces a manual annotation process of opinion texts, based on a fine-featured annotation scheme, independent of language and corpus. Then, from a part of this scheme, a method to build automatically an opinion lexicon from a syntactic analyser and a linguistic resource is described. This method consists in building a decision tree from the classes of the resource. The coverage of the lexicon has been determined by comparing it to the gold annotation of a restaurants review corpus. Its genericity was determined by comparing it to another lexicon generated from a different domain corpus (movie reviews). Eventually, the usefulness of the lexicon has been measured with an extrinsic task, the recognition of the polarity of reviews.},
  motscles  = {Analyse d'opinion, Extension de lexique, Annotation d'opinions},
  keywords  = {Opinion mining, Lexicon extension, Opinion annotation},
}

@inproceedings{hedimaaloul-keskes:2010:RECITAL,
  author    = {Hédi Maâloul, Mohamed and keskes, Iskandar},
  title     = {Résumé automatique de documents arabes basé sur la technique RST},
  booktitle = {Actes des 12e Rencontres des Étudiants Chercheurs en Informatique pour le Traitement Automatique des Langues},
  month     = {July},
  year      = {2010},
  address   = {Montréal, Canada},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/RECITAL/RECITAL-2010/recital-2010-long-008},
  language  = {french},
  resume    = {Dans cet article, nous nous intéressons au résumé automatique de textes arabes. Nous commençons par présenter une étude analytique réalisée sur un corpus de travail qui nous a permis de déduire, suite à des observations empiriques, un ensemble de relations et de frames (règles ou patrons) rhétoriques; ensuite nous présentons notre méthode de production de résumés pour les textes arabes. La méthode que nous proposons se base sur la Théorie de la Structure Rhétorique (RST) (Mann et al., 1988) et utilise des connaissances purement linguistiques. Le principe de notre proposition s’appuie sur trois piliers. Le premier pilier est le repérage des relations rhétoriques entres les différentes unités minimales du texte dont l’une possède le statut de noyau – segment de texte primordial pour la cohérence – et l’autre a le statut noyau ou satellite – segment optionnel. Le deuxième pilier est le dressage et la simplification de l’arbre RST. Le troisième pilier est la sélection des phrases noyaux formant le résumé final, qui tiennent en compte le type de relation rhétoriques choisi pour l’extrait.},
  abstract  = {In this paper, we focus on automatic summarization of Arabic texts. We start by presenting an analytical study carried out on a study corpus which enabled us to deduce, following empirical observations, a set of relations and rhetorical frames; then we present our proposed method to produce summaries for Arabic texts. This method is based bases on the Rhetorical Structure Theory (RST) technique (Mann and Al., 1988) and uses purely linguistic knowledge. The principle of the proposed method is based on three pillars. The first pillar is the location of the rhetorical relations between the various minimal units of the text of which one has the status of nucleus - text segment necessary to maintain coherence - and the other has the status of nucleus or satellite - optional segment. The second pillar is the representation and the simplification of RST-tree that is considered most descriptive. The third pillar is the selection of the nucleus sentences forming the final summary, which hold in account the type of rhetorical relations chosen.},
  motscles  = {Théorie de la Structure Rhétorique, Relations rhétoriques, Marqueurs linguistiques, Résumé automatique de textes arabes},
  keywords  = {Rhetorical Structure Theory, Rhetorical relations, Linguistic markers, Automatic summarization of Arabic texts},
}

@inproceedings{ro:2010:RECITAL,
  author    = {Ro, Hee-Jin},
  title     = {Inférences aspecto-temporelles analysées avec la Logique Combinatoire},
  booktitle = {Actes des 12e Rencontres des Étudiants Chercheurs en Informatique pour le Traitement Automatique des Langues},
  month     = {July},
  year      = {2010},
  address   = {Montréal, Canada},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/RECITAL/RECITAL-2010/recital-2010-long-009},
  language  = {french},
  resume    = {Ce travail s’inscrit dans une recherche centrée sur une approche de l’Intelligence Artificielle (IA) et de la linguistique computationnelle. Il permet d’intégrer différentes techniques formelles de la Logique Combinatoire avec des types (Curry) et sa programmation fonctionnelle (Haskell) avec une théorie énonciative du temps et de l’aspect. Nous proposons des calculs formels de valeurs aspectotemporelles (processus inaccompli présent, processus inaccompli passé, événement passé et étatrésultant présent) associées à des représentations de significations verbales sous forme de schèmes applicatifs.},
  abstract  = {This work is in line with the research centered on an approach of the Artificial Intelligence and the Computational linguistic. It allows integrating different formal technologies of the Combinatory Logic with types (Curry) and their functional programme (Haskell) with an enunciative theory of the tense and the aspect. We propose formal calculus of aspecto-temporal values (present unaccomplished process, past unaccomplished process, past event and present résultative-state) associated with representations of verbal meanings in the form of applicative schemes.},
  motscles  = {Logique Combinatoire, Référentiel énonciatif, Schème sémantico-cognitif, Grammaire Applicative et Cognitive, Haskell},
  keywords  = {Combinatory Logic, Enunciative Frame of reference, Semantic-cognitive scheme, Applicative and Cognitive Grammar, Haskell},
}

@inproceedings{seppala:2010:RECITAL,
  author    = {Seppälä, Selja},
  title     = {Automatiser la rédaction de définitions terminographiques : questions et traitements},
  booktitle = {Actes des 12e Rencontres des Étudiants Chercheurs en Informatique pour le Traitement Automatique des Langues},
  month     = {July},
  year      = {2010},
  address   = {Montréal, Canada},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/RECITAL/RECITAL-2010/recital-2010-long-010},
  language  = {french},
  resume    = {Dans cet article, nous présentons une analyse manuelle de corpus de contextes conceptuels afin (i) de voir dans quelle mesure les méthodes de TALN existantes sont en principe adéquates pour automatiser la rédaction de définitions terminographiques, et (ii) de dégager des question précises dont la résolution permettrait d’automatiser davantage la production de définitions. Le but est de contribuer à la réflexion sur les enjeux de l’automatisation de cette tâche, en procédant à une série d’analyses qui nous mènent, étape par étape, à examiner l’adéquation des méthodes d’extraction de définitions et de contextes plus larges au travail terminographique de rédaction des définitions. De ces analyses émergent des questions précises relatives à la pertinence des informations extraites et à leur sélection. Des propositions de solutions et leurs implications pour le TALN sont examinées.},
  abstract  = {A manual corpus analysis of conceptual contexts is presented in order (i) to indicatively evaluate to what extent NLP methods can in principle be used to automate the production of terminographic definitions, and (ii) to identify central questions to be answered if one wants to further automate the task. The objective is to contribute to reflection on the challenges faced by the automation of this task. Through a series of analyses, the adequacy of extraction methods for defining or knowledge-rich contexts is examined in the light of the terminographic activity of definition writing. Precise questions emerge from these analyses relating to the relevance and the selection of the extracted information. Some solutions are proposed and their implications to NLP reviewed.},
  motscles  = {Terminologie, définitions terminographiques, sélection des traits, pertinence des traits, extraction de définitions, contextes conceptuels, traitement automatique des définitions.},
  keywords  = {Terminology, terminographic definitions, feature selection, feature relevance, definition extraction, conceptual contexts, definition processing.},
}

@inproceedings{trouvilliez:2010:RECITAL,
  author    = {Trouvilliez, Benoît},
  title     = {Représentation vectorielle de textes courts d’opinions, Analyse de traitements sémantiques pour la fouille d’opinions par clustering},
  booktitle = {Actes des 12e Rencontres des Étudiants Chercheurs en Informatique pour le Traitement Automatique des Langues},
  month     = {July},
  year      = {2010},
  address   = {Montréal, Canada},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/RECITAL/RECITAL-2010/recital-2010-long-011},
  language  = {french},
  resume    = {Avec le développement d’internet et des sites d’échanges (forums, blogs, sondages en ligne, ...), l’exploitation de nouvelles sources d’informations dans le but d’en extraire des opinions sur des sujets précis (film, commerce,...) devient possible. Dans ce papier, nous présentons une approche de fouille d’opinions à partir de textes courts. Nous expliquons notamment en quoi notre choix d’utilisation de regroupements autour des idées exprimées nous a conduit à opter pour une représentation implicite telle que la représentation vectorielle. Nous voyons également les différents traitements sémantiques intégrés à notre chaîne de traitement (traitement de la négation, lemmatisation, stemmatisation, synonymie ou même polysémie des mots) et discutons leur impact sur la qualité des regroupements obtenus.},
  abstract  = {With the internet and sharing web sites developement (forums, blogs, online surveys, ...), new data source exploitation in order to extract opinions about various subjects (film, business, ...) becomes possible. In this paper, we show an opinion mining approach from short texts. We explain how our choice of using opinions clustering have conducted us to use an implicit representation like vectorial representation. We present different semantic process that we have incorporated into our process chain (negation process, lemmatisation, stemmatisation, synonymy or polysemy) and we discut their impact on the cluster quality.},
  motscles  = {représentation des textes, représentation vectorielle, traitement de textes courts, regroupements d’opinions},
  keywords  = {text representation, vectorial representation, short text processing, opinion clustering},
}