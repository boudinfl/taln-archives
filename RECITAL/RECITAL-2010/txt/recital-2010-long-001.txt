RECITAL 2010, Montréal, 19-23 ju1'11et 201 0

Attribution d’auteur au moyen de modéles de langue et de modéles
stylométriques

Audrey Laroche
OLST, Dép. de linguistique et de traduction, Université de Montreal
audrey.1aroche@umontrea1.ca

Résumé. Dans une tache consistant a trouver l’auteur (parmi 53) de chacun de 114 textes, nous
analysons la performance de modeles de langue et de modeles stylométriques sous les angles du rappel
et du nombre de parametres. Le modele de mots bigramme a lissage de Kneser-Ney modiﬁé interpolé est
le plus performant (75 % de bonnes réponses au premier rang). Parmi les modeles stylométriques, une
combinaison de 7 parametres liés aux parties du discours produit les meilleurs résultats (rappel de 25 % au
premier rang). Dans les deux catégories de modeles, le rappel maximal n’est pas atteint lorsque le nombre
de parametres est le plus élevé.

Abstract. In a task consisting of attributing the proper author (among 53) of each of 114 texts, we
analyze the performance of language models and stylometric models from the point of View of recall and
the number of parameters. The best performance is obtained with a bigram word model using interpolated
modiﬁed Kneser-Ney smoothing (ﬁrst-rank recall of 75 %). The best of the stylometric models, which
combines 7 parameters characterizing the proportion of the different parts of speech in a text, has a ﬁrst-
rank recall of 25 % only. In both types of models, the maximal recall is not reached when the number of
parameters is highest.

M0tS-CléS I Attribution d’auteur, modele de langue, stylométrie, n-graInInes, vecteurs de traits.

Keywords: Authorship attribution, language model, stylometry, n-grams, feature vectors.

1 Introduction

L’ attribution d’auteur est une tache qui intéresse les chercheurs depuis le XIX° siecle (Holmes, 1994). Elle
a permis d’identiﬁer l’auteur d’oeuvres de provenance contestée, comme les Federalist Papers (McEnery
& Oakes, 2000) ; elle a aujourd’hui des applications dans des domaines comme la détection de plagiat dans
les travaux scolaires et la linguistique légale. La plupart des techniques d’identiﬁcation d’auteur ont trait
a la stylométrie, c’est-a-dire la mesure quantitative d’indices textuels de natures diverses qui caractérisent
le style d’un auteur. Les indices stylistiques et leur quantité varient d’une étude a l’autre. Par exemple,
Schaalje et al. (1997) tentent de déterminer quels types d’indices donnent de meilleurs résultats ; selon eux,
ce sont les mots fonctionnels qui permettent de distinguer les auteurs, et non la richesse du vocabulaire (ex.
ratio types/tokens). Stamatatos et al. (1999) forment des vecteurs de 22 indices stylistiques (ex. nombre
de syntagmes nominaux par rapport au nombre total de syntagmes) qu’ils comparent statistiquement a un
article de journal pour en trouver l’auteur. Pour attribuer un auteur a des courriels, Koppel & Schler (2003)
combinent trois classes de traits : lexicaux (fréquence des mots fonctionnels), collocationels (fréquence

AUDREY LAROCHE

des bigrammes de parties du discours) et idiosyncratiques (ex. epellation, formatage). Les parametres
sont beaucoup plus nombreux dans Van Halteren (2004), dont la technique est basee sur des milliers
de traits lexicaux et syntaxiques (mots-formes, parties du discours, bigrammes, trigrammes). D’autres
etudes portent sur la performance de reseaux de neurones et d’algorithmes genetiques (McEnery & Oakes,
2000). Un autre type d’approche ne fait appel qu’a des modeles de langue : entre autres, Keselj et al.
(2003) construisent des ensembles optimaux de n-graInInes de lettres pour former des proﬁls d’auteur. Les
performances de toutes ces techniques sont variables (les meilleures rapportees allant de 50 % a 98 %) et
dependent fortement de la tache effectuee, du nombre d’auteurs candidats (Luyckx & Daelemans, 2008)
et de la taille des corpus d’entrainement, en plus des indices stylistiques ou des n-grammes selectionnes.

L’objectif de la presente etude est de comparer la performance de differentes methodes d’identiﬁcation
automatique de l’auteur d’un texte : certaines sont basees sur des modeles de langue et d’autres sur des
modeles stylometriques. Pour determiner l’auteur d’un texte, les modeles construits selon ces deux ap-
proches lui sont compares tour a tour; il s’agit en fait d’un probleme de categorisation. Les meilleurs
modeles doivent combiner un rappel eleve et un petit nombre de parametres.

Le plan de l’article est le suivant. Notre corpus est decrit a la section 2. La section 3 presente les deux
types d’approches testes dans la tache d’attribution d’auteur. Nous discutons dans la section 4 des resultats
obtenus lors des experiences. La section 5 conclut l’article en suggerant des pistes d’amelioration.

2 Corpus

Le corpus utilise dans les experiences est constitue de 167 textes de 53 auteurs differents (minimum de
2 textes par auteur; moyenne de 3,2). Ces textes, ecrits en francais (pas de traductions), proviennent de
diverses regions de la francophonie (France, Quebec, etc.) et sont anterieurs a la seconde moitie du XX°
siecle. Ils s’inscrivent dans des genres litteraires heterogenes : romans (73 textes), essais (21), poemes (14),
pieces de theatre (14), memoires (12), nouvelles (9), biographies (8), lettres (6), journaux (6) et dialogues
(4). De plus, les textes de 21 des auteurs ne sont pas du meme genre. Les textes sont generalement assez
longs : le plus court fait 2100 mots, le plus long 261 700 mots, et un texte compte en moyenne 53 200
mots.

Cent six textes de 31 auteurs sont extraits de la Bibliotheque Universelle (ABU)1 ; les 61 textes des 22
auteurs restants sont tires du Projet Gutenberg (PG)2. Pour les experiences, l’ensemble du corpus est divise
arbitrairement en sous-corpus d’entrainement et de test. Le corpus d’entrainement, qui sert a modeliser
chaque auteur, est forme d’un texte par auteur. Le corpus de test est donc constitue de 114 textes dont
l’auteur est inconnu, mais fait partie des 53 auteurs modelises.

A notre connaissance, aucun corpus francais ayant servi dans les etudes anterieures sur l’attribution d’au-
teur n’est disponible. Les corpus grec de Stamatatos et al. (1999) et chinois de Fuchun et al. (2003)
ont bien ete repris dans Keselj et al. (2003), mais ces derniers ont ete contraints, comme nous, d’as-
sembler pour l’anglais un corpus constitue de textes classiques (ex. Shakespeare, Dickens) libres de
droits. A des ﬁns de reproductibilite et de comparaison, l’ensemble du corpus que nous avons constitue
(sous ses formes originale et pretraitee) est disponible a l’adresse http : / /olst . ling.umontreal.
ca/~audrey/recital20lO/.

1
2

http://abu.cnam.fr/BIB/auteurs/
http://www.gutenberg.org/browse/languages/fr

ATTRIBUTION D’AUTEUR AU MOYEN DE MODELES DE LANGUE ET DE MODELES STYLOMETRIQUES
2.1 Prétraitement

Les textes des corpus d’entrainement et de test passent par une serie de transformations avant d’etre mo-
delises. D’abord, au moyen de scripts, les licences d’ABU et du PG sont enlevees, de meme que, pour
anonymiser les oeuvres, les 10 premieres lignes des textes d’ABU et les 20 premieres des textes du PG. Les
textes sont segmentes ﬁnement en mots a l’aide d’un script ecrit par Tanguy & Hathout (2003). Ce script
de segmentation est adapte au francais : une liste d’exceptions permet de ne pas separer les constituants
des mots complexes (au fur et 61 mesure) et des mots comprenant un signe de ponctuation (R.-de-ch.). Par
la suite, une etiquette morphosyntaxique et un lemme sont attribues a chaque mot a l’aide de TreeTagger3.
La lemmatisation de TreeTagger est desambiguisee a l’aide d’un autre script de Tanguy & Hathout (2003)
qui selectionne le lemme le plus frequent (selon un corpus de reference) dans les cas ou TreeTagger pro-
pose plusieurs lemmatisations pour un mot. Enﬁn, pour certaines des experiences, les textes segmentes
sont e galement decoupes en syntagmes avec le chunker de TreeTagger.

3 Approches

Le principe general de la tache d’attribution d’auteur consiste tout d’abord a modeliser les 53 auteurs du
corpus d’entrainement selon une technique donnee (section 3.1). Ensuite, les modeles sont compares un
a un au texte detest dont nous cherchons a determiner l’auteur (section 3.2). Cette tache est repetee pour
chacun des 114 textes du corpus detest, et la performance des differents modeles, qui fait l’objet de notre
etude, est evaluee a l’aide des metriques presentees a la section 3.3.

3.1 Phase d’entra’1‘nement

Nous avons implemente deux categories de methodes pour modeliser des auteurs a partir d’un de leurs
textes. La premiere est constituee de modeles de langue d’ordres, de lissages et d’unites distincts. La
seconde categorie regroupe des modeles stylometriques simples et complexes.

3.1.1 Acquisition des modéles de langue

Un modele de langue d’ordre n, ou modele n-gramme, dans son acception la plus courante, est un modele
statistique qui calcule la probabilite d’un mot etant donnes les n-1 mots qui le precedent (Goodman, 2001).
Un tel modele est habituellement lisse aﬁn d’accorder une probabilite non nulle a des sequences non
observees dans le corpus d’entrainement. L’utilisation de modeles de langue dans l’attribution d’auteur
est assez repandue; elle est souvent conjuguee a des indices stylistiques, comme dans Koppel & Schler
(2003) et Van Halteren (2004). Keselj et al. (2003) en font une etude plus detaillee en tentant de construire
de petits modeles de caracteres d’ordres differents. Nous voulons veriﬁer si les modeles les plus petits
sont effectivement meilleurs et etudier l’inﬂuence des proprietes d’un modele de langue, soit son type de
lissage, son ordre et son unite de base. Dans notre premiere serie d’experiences, 16 modeles de langue sont
construits pour chacun des 53 auteurs a l’aide de la boite a outils SRILM (Stolcke, 2002). Le lissage de
ces modeles est soit celui de Kneser-Ney modiﬁe interpole (KN), soit celui de repli de Witten-Bell (WB).

3http://www.ims.uni—stuttgart.de/projekte/corplex/TreeTagger/

AUDREY LAROCHE

Pour chaque type de lissage, des modeles d’ordre 2, 3, 4 et 5 sont créés. Les modeles a lissage KN sont
des modeles de mots. Pour les modeles a lissage WB, trois unités correspondant a des niveaux d’analyse
plus ou moins abstraits sont utilisées : modeles de mots, de lemmes et de parties du discours (les deux
derniers sont créés a partir du corpus étiqueté par TreeTagger). Le vocabulaire des modeles de lemmes et
de parties du discours dans le corpus étant restreint, le lissage KN (bien qu’il soit généralement le meilleur
selon Chen & Goodman (1998)) n’est pas approprié pour construire des modeles de langue basés sur ces
unités4 ; Stolcke et al. (2010) recommandent plutot d’utiliser le lissage WB dans ces cas.

3.1.2 Acquisition des modéles stylométriques

L’ approche employée dans la deuxieme série d’expériences consiste a acquérir des modeles stylométriques
pour représenter les auteurs. Les indices stylistiques étudiés formant ce type de modele sont pour la plupart
tirés de l’état de l’art de Holmes (1994) et ont trait aux proportions de parties du discours. Les modeles
stylométriques sont construits a partir des textes analysés par TreeTagger. Chaque auteur est d’abord mo-
délisé neuf fois a partir d’un trait stylistique unique parIni les suivants : 1) nombre de noms par rapport au
nombre de verbes, 2) nombre de noms par rapport au nombre total de mots dans le texte (N), 3) nombre
de verbes par rapport a N, 4) nombre d’adverbes par rapport a N, 5) nombre de mots fonctionnels5 par
rapport a N, 6) nombre de signes de ponctuation par rapport a N, 7) nombre d’adjectifs par rapport au
nombre de noms, 8) nombre d’adjectifs par rapport a N, et 9) longueur moyenne des syntagmes nominaux
(SN). Des modeles stylométriques complexes sont ensuite construits de facon vorace : selon leur perfor-
mance respective dans la tache d’attribution d’auteur, les indices stylistiques simples sont combinés de
facon incrémentale en un vecteur de deux a neuf traits pour former des modeles d’auteur complexes. No-
tons que les indices stylistiques que nous étudions sont beaucoup moins nombreux que ceux dans Koppel
& Schler (2003), Van Halteren (2004), Luyckx & Daelemans (2008), etc. Ceux-ci utilisent des techniques
d’apprentissage machine pour pondérer les indices. Comme nous avons pour objectif d’utiliser le moins
de parametres possibles, nous n’avons pas encore exploré cette voie qui donne de bons résultats dans la
littérature, bien qu’elle nécessite beaucoup de ressources.

3.2 Phase de test

Dans la phase de test, notre systeme doit identiﬁer l’auteur d’un texte du corpus de test parmi les auteurs
modélisés a l’aide des deux méthodes d’acquisition présentées a la section 3.1. Pour ce faire, il attribue un
score a chaque modele d’auteur pour indiquer a quel point celui-ci ressemble au texte d’auteur inconnu.
Les modeles sont ordonnés selon leur score; le modele en téte de liste correspond a l’auteur attribué au
texte par le systeme. La facon de calculer ce score dépend du type de modele employé.

3.2.1 Attribution £1 l’aide de modéles de langue

Le score indiquant a quel point un modele de langue modélise bien le texte d’auteur inconnu est donné par
la perplexité. La perplexité d’un modele de langue est la moyenne géométrique de la probabilité inverse

4En effet, SRILM ne parvient pas a calculer ces modéles.
5Les mots considérés comme fonctionnels sont les déterminants, les conjonctions, les pronoms et les prépositions.

ATTRIBUTION D’AUTEUR AU MOYEN DE MODELES DE LANGUE ET DE MODELES STYLOMETRIQUES

des mots du corpus detest (Goodman, 2001, p. 4) :

5C07‘eper,-plex-,:té(.’.E1....Tn) =

 

La perplexité des modeles de langue par rapport au texte de test est calculée a l’aide de la boite a outils
SRILM. Plus la perplexité est basse, plus le modele de langue réussit bien a prédire le texte. L’auteur
présumé d’un texte est par conséquent celui qui obtient la plus petite perplexité parmi tous les auteurs
modélisés.

3.2.2 Attribution £1 l’aide de modéles stylométriques

Tel que décrit a la section 3.1.2, chaque auteur est représenté a l’aide de modeles stylométriques simples
et complexes. Les modeles stylométriques simples sont constitués d’un parametre unique. Pour attribuer
l’auteur d’un texte de test, ce dernier doit étre représenté par le meme parametre que le type de modele
stylométrique a l’étude. Le score indiquant a quel point un modele d’auteur représente bien le texte d’au-
teur inconnu est égal a la différence, en valeur absolue, entre la valeur du parametre pour le modele (m) et
celle du méme parametre pour le texte (t) :

5C07'eMS simple(m> t) = lm _ ti

Le score est calculé pour tous les auteurs modélisés; le plus petit des scores obtenus indique lequel des
modeles d’auteur correspond le Inieux au texte.

Pour comparer des modeles stylométriques complexes, constitués d’un vecteur d’indices stylistiques, a un
texte d’auteur inconnu, ce texte est d’abord caractérisé par un vecteur formé des memes parametres que
les modeles. Le vecteur de chaque modele d’auteur (772) est ensuite comparé au vecteur du texte (£3 a l’aide
de la mesure du cosinus :

2221 W X ti

221:1  221:1 tg

Plus le cosinus entre deux vecteurs est élevé, plus ces vecteurs sont similaires. L’ auteur attribué au texte
est donc celui qui donne la plus grande valeur de cosinus.

5 007' ecosinus (ma £5 =

3.3 Métriques d’évaluation

Dans la phase de test, les modeles d’auteur sont ordonnés selon un score indiquant leur degré de ressem-
blance au texte dont nous cherchons a déterminer l’auteur. Pendant les expériences, cette procédure est
répétée pour chaque type de modele de langue et de modele stylométrique parmi ceux décrits a la section
3.1. La comparaison des performances des différentes approches est basée sur deux criteres : le rappel et
le nombre de parametres que renferme le modele. Le rappel au rang 11 est donné par :

nombre de bonnes réponses au rang n X 100

l =
rappe n nombre de textes de test

Ainsi, le rappel d’un modele au rang 1 correspond au pourcentage des 114 textes de test pour lesquels le
systeme trouve le bon auteur en premiere position. Plus le rappel est élevé, meilleure est la performance
du modele. Quant au nombre de parametres, il équivaut a la quantité de n-grammes distincts caractérisant

AUDREY LAROCHE

les modeles de langue ou a la taille des vecteurs d’indices stylistiques dans les modeles stylométriques
simples et complexes. I-/3tant donné qu’une quantité réduite de parametres requiert moins de ressources
(temps de calcul, mémoire), la performance d’un modele du point de vue du nombre de parametres est
meilleure si ce modele implique moins de parametres.

4 Résultats et discussion

Les deux types d’approches — modeles de langue et modeles stylométriques — ont fait l’objet d’expé-
riences dans lesquelles un auteur parmi les 53 modélisés doit étre attribué a chacun des 114 textes d’auteur
inconnu. De facon générale, les meilleurs résultats en termes de rappel sont obtenus avec des modeles
de langue. Dans les deux approches, un nombre de parametres plus élevé n’est pas directement lié a un
rappel supérieur. Les paragraphes qui suivent présentent les résultats obtenus lors des expériences d’attri-
bution d’auteur dans lesquelles le type de modélisation varie (sections 4.1 et 4.2) ; la section 4.3 montre les
résultats d’expériences sur l’inﬂuence de la taille des corpus d’entrainement et de test sur la performance.

4.1 Performance des modéles de langue

Dans une premiere série d’expériences, les auteurs sont modélisés avec des modeles de langue dont le
lissage, l’ordre et l’unité varient. Le tableau 1 permet de comparer les différents types de modeles de

TAB. 1 — Rappel au rang 1 des modeles de langue

Mots Lemmes Parties du discours
KN WB WB WB
2- gramme 74,56 0,00 5,26 54,39
3- gramme 71,93 0,00 7,02 50,00
4- gramme 72,81 0,00 9,65 47,37
5- gramme 72,81 0,00 10,53 49,12

langue. I1 montre que le choix de la méthode de lissage est tres important : le rappel au rang 1 pour les
modeles de mots a lissage de Kneser-Ney modiﬁé interpolé est supérieur a 70 %, tandis que les modeles
de mots a lissage de Witten-Bell ne donnent jamais la bonne réponse au rang 1. L’ ordre des modeles de
langue a lui aussi une inﬂuence (irréguliere) sur la performance. L’ ordre 2 donne le meilleure rappel dans
le cas des modeles de mots KN (74,56 %) et des modeles de parties du discours WB (54,39 %), mais
c’est l’ordre le plus élevé (5) qui produit le meilleur rappel parmi les modeles de lemmes WB (10,53 %);
l’ordre n’inﬂuence pas le rappel au rang 1 des modeles de mots WB, qui demeure nul. Par ailleurs, plus le
niveau d’analyse du texte est abstrait (du plus concret au plus abstrait : mot, lemme, partie du discours),
plus le rappel des modeles WB augmente. Par exemple, le rappel au rang 1 des modeles WB bigrammes
est décuplé lorsque l’unité passe du lemme (5,26 %) a la partie du discours (54,39 %). La ﬁgure 1(a) (page
suivante) montre l’importante amélioration du rappel aux rangs 1 a 10 selon l’unité du modele de langue
WB. Tout se passe comme si la variation au niveau des suites de mots entre les textes d’un meme auteur est
plus marquée que celle au niveau des suites de parties du discours. En construisant des modeles d’auteur
encore plus abstraits, des modeles syntaxiques, Baayen et al. (1996) ont en effet noté que les structures
syntaxiques constituent de meilleurs indices stylistiques que les indices portant sur le lexique. Les modeles
KN sont impossibles a construire lorsque la taille de vocabulaire est réduite (Stolcke et al., 2010), comme

ATTRIBUTION D’AUTEUR AU MOYEN DE MODELES DE LANGUE ET DE MODELES STYLOMETRIQUES

100 70
more wb Egram <o—

90 lemmas wb Zgram +
pdd wb2 ram \x— 50

80
70 50

E0
40

SD

   

Rappei
Rappal

30
40 traxts 1,2

traits 1,2,3 e«—

30 20 tra1ts1,2,3,4 —x—
tra1ts1,2,3,4,5 «a—

30 traIts1,2,3,4,5,6 +
10 tra1t51,2,3,4,5,6,7 «e—

10 tra\ts1,2,3,4,5,6,7,B 7 . 7
0 0 traIts1,2,3,4,5,Ei,7,B,§ \.e—

1 2 3 4 5 6 7 E 9 10 1 2 3 4 5 6 7 S Q 10

   

(a) Modeles de langue Witten—Bell bi— (b) Modeles stylométriques complexes
grammes

FIG. 1 — Rappel en fonction du rang

c’est le cas des modeles de lemmes et des modeles de parties du discours. S’il existait un type de lissage
plus performant que WB (comme KN) et adapté aux vocabulaires restreints, la performance dans la tache
d’attribution d’auteur serait probablement améliorée.

Le nombre de parametres, notre autre critere de performance, ne varie pas en fonction du type de lissage
(seul le poids des n-grammes varie), mais selon l’ordre et l’unité des modeles de langue. Plus l’ordre du
modele est élevé, plus le nombre de parametres (nombre de n-grammes distincts dans le texte modélisé)
est élevé. Par exemple, pour un texte de 48 500 mots (taille médiane des textes du corpus d’entrainement),
le modele bigrarnme contient 41 400 parametres, celui d’ordre 3, 45 900 parametres, d’ordre 4, 47 600
parametres et d’ordre 5, 48 200 parametres. L’ abstraction des niveaux d’analyse textuelle est inversement
proportionnelle a la quantité de parametres. A titre d’exemple, dans le texte de 48 500 mots, un modele
de mots bigrarnme contient 41 400 parametres, un modele de lemmes 24 700 parametres et un modele de
parties du discours, 300.

Comme le montre le tableau 1 ci-dessus, un nombre de parametres plus élevé n’est pas une condition
nécessaire pour obtenir un meilleur rappel. Au contraire, parrni tous les modeles WB, c’est celui qui
compte le moins de parametres (bigramme, parties du discours) qui donne le meilleur rappel au rang 1
(54,39 %). Parmi les modeles KN, c’est également celui qui contient le moins de parametres, le modele
bigrarnme, qui donne le meilleur rappel (74,56 %). Les modeles d’ordre plus élevé souffrent donc d’un
probleme de surapprentissage.

4.2 Performance des modéles stylométriques

L’attribution d’auteur a l’aide de modeles stylométriques simples donne des rappels au rang 1 (tableau
2, page suivante) de beaucoup inférieurs a ceux obtenus avec les modeles de mots a lissage KN et de
parties du discours a lissage WB. Les modeles stylométriques simples, qui comptent un seul parametre,
ont cependant un meilleur rappel au rang 1 que les modeles WB de mots et de lemmes, qui contiennent
des Inilliers de parametres.

Il est intéressant de constater que les meilleurs indices stylistiques sont ceux qui sont liés aux proportions

AUDREY LAROCHE

TAB. 2 — Rappel des modeles stylométriques simples aux rangs 1 et 10

Id Trait R1 R10 | Id Trait R1 R10
1 ||Noms|| / ||Verbes|| 9,65 42,98 6 ||Ponctuations|| / ||Mots|| 6,14 41,23
2 ||Noms||/||Mots|| 9,65 40,35 7 ||Adjectifs||/||Noms|| 5,26 36,84
3 ||Verbes||/||Mots|| 7,02 43,86 8 ||Adjectifs||/||Mots|| 3,51 40,35
4 ||AdVerbes  / ||Mots  7,02 40,35 9 Longueur moyenne SN 2,63 26,32
5 ||Mots fonctionne1s|| / ||Mots|| 6,14 47,37

des mots lexicaux (traits 1 a 4). En effet, plusieurs études stylométriques utilisent des indices liés aux mots
fonctionnels et a la ponctuation (McEnery & Oakes, 2000), mais nos résultats montrent que ces indices
sont moins performants que ceux liés aux noms, aux verbes et aux adverbes. Notons toutefois qu’au rang
10, 1e meilleur rappel est celui donné par la proportion de mots fonctionnels dans les textes (47,37 %). Les
indices liés aux proportions d’adjectifs et a la longueur moyenne des syntagmes nominaux (indices 7 a 9)
ont une performance médiocre.

Pour former les modeles stylométriques complexes, nous utilisons une approche vorace : les indices
simples sont graduellement combinés en des vecteurs de traits par ordre de leur rappel au rang 1 tel
que noté dans le tableau 2. La ﬁgure 1(b) montre le rappel aux rangs 1 a 10 de ces modeles de vecteurs de
traits stylométriques.

Comme dans le cas des modeles de langue, les modeles stylométriques complexes ayant le plus grand
nombre de parametres (équivalant a la taille du vecteur) ne sont pas ceux qui ont le meilleur rappel. En
effet, le meilleur ensemble de parametres combine les indices stylistiques 1 a 7 eta un rappel de 24,56 %
au rang 1 ; le plus gros des modeles stylométriques (9 parametres) a un rappel de 19,30 % au premier rang.
La ﬁgure 1(b) montre que l’introduction des traits 4, 6 et 7 a une plus grande inﬂuence sur le rappel au
rang 1 que celle des traits 5, 8 et 9 (ces deux derniers diminuant meme le rappel). Notre étude, dans son
état actuel, étant donné qu’elle ne fait intervenir que 9 indices, ne permet pas de conclure qu’un nombre
inférieur de parametres stylistiques donne nécessairement de meilleurs résultats. I1 pourrait au contraire
étre proﬁtable de sélectionner une grande quantité d’indices comme le font Luyckx & Daelemans (2008)
puis de les pondérer automatiquement.

4.3 L’inﬂuence de la taille du corpus

Les textes d’entrainement et de test utilisés dans les expériences décrites ci-dessus sont longs (53 200 mots
en moyenne) et leur taille varie beaucoup : cela peut inﬂuencer les résultats. Nous avons vériﬁé la perfor-
mance du meilleur modele de chaque approche — modele de mots bigramme KN et modele stylométrique
complexe a 7 parametres — sur des textes d’entrainement de tailles égales (2000, 2500 et 3000 mots), puis
sur des textes de test de 500, 750 et 1000 mots6. Lorsque la taille des textes d’entrainement est réduite par
rapport aux textes originaux, les textes de test conservent leur taille originale, et inversement.

Comme l’indique le tableau 3, plus la taille des textes d’entrainement est importante, meilleur est le rappel
au rang 1 (ce qui est conforme aux résultats de Luyckx & Daelemans (2008)). I1 n’en est pas ainsi pour
les textes de test (ceux dont on cherche a déterminer l’auteur) : le rappel maximal est atteint avec la

5Ces tailles sont détenninées en fonction de la taille des plus petits textes d’entrainement (6000 mots) et detest (2000 mots).
La tache a été effectuée deux ou trois fois pour chaque taille; les résultats rapportés correspondent a la moyenne des rappels
obtenus.

ATTRIBUTION D’AUTEUR AU MOYEN DE MODELES DE LANGUE ET DE MODELES STYLOMETRIQUES

TAB. 3 — Rappel au rang 1 des meilleurs modeles en fonction de la taille des corpus

Modéle Taille du corpus d’entrainement Taille du corpus de test
2000 mots 2500 mots 3000 mots 500 mots 750 mots 1000 mots
Modele de langue 38,89 42,11 44,30 46,49 48,25 47,81
Modele stylometrique 6,14 9,21 10,53 l 5,85 6,14 5,26

taille mediane. Ces constats sont valides pour les deux approches de modelisation. Dans tous les cas,
cependant, le rappel dans cette serie d’experiences (dans laquelle les textes sont coupes) est inferieur a
celui obtenu lorsque tous les textes ont leur taille originale. Cette difference est plus marquee lorsque
les textes d’entrainement sont raccourcis dans le cas des modeles de langue et, dans le cas des modeles
stylometriques, lorsque c’est la taille des textes de test qui est reduite.

5 Conclusion

Nous avons presente une tache d’attribution d’auteur dans laquelle un texte doit etre categorise en etant
compare a des modeles d’auteurs. Plusieurs approches de modelisation sont possibles; nous avons teste
la performance de modeles de langue et de modeles stylometriques, ces demiers etant tres repandus dans
la litterature. Nos resultats montrent que les modeles de langue atteignent de meilleurs rappels que les
modeles stylometriques, bien que des disparites de performance importantes existent entre les differents
types de modeles dans chaque approche. Le rappel maximal obtenu est de 75 % (avec un modele de mots
bigramme a lissage de Kneser-Ney modiﬁe interpole); ce resultat est difﬁcilement comparable a ceux
mentionnes dans les travaux anterieurs, ceux-ci traitant en general moins d’auteurs (entre 2 et 10) et leurs
corpus etant varies. Soulignons que Luyckx & Daelemans (2008), qui se sont penches sur l’inﬂuence
du nombre d’auteurs, obtiennent un rappel semblable au notre (76 %) pour 20 auteurs (avec une approche
basee sur l’apprentissage machine), alors que notre etude porte sur 53 auteurs. Nous constatons par ailleurs
qu’un grand nombre de parametres dans les modeles n’est pas gage de meilleurs resultats; dans le cas des
modeles stylometriques, ces resultats suggerent qu’il serait interessant de ponderer les indices stylistiques,
comme le font plusieurs auteurs. Des experiences futures porteront aussi sur des modeles de langue bases
sur les caracteres et sur les structures syntaxiques.

I1 serait interessant de tester les approches sur des corpus differents (le notre etant relativement long et
compose de genres heterogenes) aﬁn de mesurer leur capacite a accomplir des taches plus pres d’applica-
tions reelles. La matrice de confusion obtenue a partir du meilleur modele montre qu’en moyenne, 28 %
des textes d’un auteur sont attribues a un autre; 38 % de cette moyenne est dﬁ a 5 des 53 auteurs. Si le
texte d’entrainement est d’un genre different de celui des textes detest (par ex. un poeme et des romans), la
categorisation effectuee par notre systeme est plus mauvaise : 52 % des auteurs ayant mal ete attribues au
moins une fois sont representes par des genres differents au sein de notre corpus. Par ailleurs, si les textes
d’entrainement et de test portent sur le meme theme (par ex. La femme du mort, Tome I et La femme du
mort, Tome II), ce qui est le cas pour 9 des 53 auteurs (15 textes detest), le meilleur modele de langue a un
rappel de 100 %. I1 semble donc que l’homogeneite du genre et du theme soit un facteur non negligeable
pour l’attribution d’auteur; de plus amples experiences pourraient le conﬁrmer.

AUDREY LAROCHE

Remerciements

Nous remercions Philippe Langlais pour ses nombreuses et précieuses suggestions de meme que pour la
constitution d’une partie du corpus. Nous remercions aussi Patrick Drouin et les relecteurs anonymes pour
leurs commentaires.

Références

BAAYEN H., VAN HALTEREN H. & TWEEDIE F. (1996). Outside the cave of shadows : using syntactic
annotation to enhance authorship attribution. Literary and Linguistic Computing, 11, 121-132.

CHEN S. F. & GOODMAN J. (1998). An Empirical Study of Smoothing Techniques for Language Mode-
ling. Rapport interne.

FUCHUN P., SCHUURMANS D., KESELJ V. & WANG S. (2003). Automated authorship attribution with
character level language models. In Proceedings of the tenth conference of the European Chapter of the
Association for Computational Linguistics, p. 12-17, Budapest, Hongrie.

GOODMAN J. (2001). A Bit of Progress in Language Modeling. Rapport interne, Redmond, WA.
HOLMES D. I. (1994). Authorship attribution. Computers and the Humanities, 28(2), 87-106.

KESELJ V., FUCHUN P., CERCONE N. & THOMAS C. (2003). N-gram-based author proﬁles for author-
ship attribution. In Proceedings of the Conference Paciﬁc Association for Computational Linguistics,
PACLING’03, p. 255-264, Halifax, Canada : Paciﬁc Association for Computational Linguistics.
KOPPEL M. & SCHLER J. (2003). Exploiting stylistic idiosyncrasies for authorship attribution. In
Proceedings of IJCAI’03 Workshop on Computational Approaches to Style Analysis and Synthesis, p.
69-72.

LUYCKX K. & DAELEMANS W. (2008). Authorship attribution and veriﬁcation with many authors
and liIr1ited data. In Proceedings of the 22”’ International Conference on Computational Linguistics
(COLING 2008), p. 513-520, Manchester.

MCENERY T. & OAKES M. (2000). Authorship identiﬁcation and computational stylometry, In R.
DALE, H. MOISL & H. SOMERS, Eds., Handbook of Natural Language Processing, p. 545-562. Marcel
Dekker : New York.

SCHAALJE G. B ., HILTON J. L. & ARCHER J. B. (1997). Comparative power of three author-attribution
techniques for differentiating authors. Journal of Book of Mormon Studies, 6(1), 47-63.

STAMATATOS E., FAKOTAKIS N. & KOKKINAKIS G. (1999). Automatic authorship attribution. In
Proceedings of the ninth conference of the European Chapter of the Association for Computational Lin-
guistics, p. 158-164, Morristown, NJ : Association for Computational Linguistics.

STOLCKE A. (2002). SRILM - an extensible language modeling toolkit. In Proceedings of the Interna-
tional Conference on Spoken Language Processing, p. 901-904, Menlo Park, CA : SRI International.
STOLCKE A., YURET D. & MADNANI N. (2010). SRILM—FAQ. http : //www. speech.sri.
com/projects/srilm/manpages/srilm—faq.7.html.

TANGUY L. & HATHOUT N. (2003). Perl pour les linguistes. Paris : Lavoisier.

VAN HALTEREN H. (2004). Linguistic proﬁling for author recognition and veriﬁcation. In Proceedings
of the 42”’ Annual Meeting of the Association for Computational Linguistics, p. 199-206, Morristown,
NJ : Association for Computational Linguistics.

