TALN 2011, Montpellier, 27juin — l"'ju1'11et 2011

La complexité linguistique
Méthode d’analyse

Adrien Barbaresi
ICAR, ENS LYON

Résﬂmé. La complexite linguistique regroupe differents phenomenes dont il s’agit de modeliser le rapport.
Le travail en cours que je decris ici propose une reﬂexion sur les approches linguistiques et techniques de cette
notion et la mise en application d’un balayage des textes qui s’efforce de contribuer a leur enrichissement. Ce
traitement en surface effectue suivant une liste de criteres qui representent parfois des approximations de logiques
plus elaborees tente de foumir une image << raisonnable >> de la complexite.

Abstract. Linguistic complexity includes various linguistic phenomena which interaction is to be modeled.
The ongoing work described here tackles linguistic and technical approaches of this idea as well as an irnplemen—
tation of a parsing method which is part of text enrichment techniques. This chunk parsing is performed according
to a list of criteria that may consist in logical approximations of more sophisticated processes in order to provide
a < reasonable >> image of complexity..

M0tS-CléS 3 Complexite, lisibilite, allemand, analyse de surface.

Keywords: Complexity, lisibility, German, chunk parsing.

1 Enjeux

L’ analyse de la complexite se situe dans le cadre de l’assistance a la comprehension. I1 s’agit ici de determiner
la lisibilite d’un texte pour des humains ou pour des machines, c’est-a-dire d’une part le niveau de maitrise et de
pratique de la langue requis et d’autre part le modele formel et les instruments a utiliser.

Ce theme tres riche invite a penser les langues avant de les analyser a differentes echelles et selon differents modes
operatoires. Du point de vue disciplinaire, on peut le situer a la croisee de la linguistique, des etudes sur la lisibilite,
des sciences cognitives et de la theorie de l’information. Le traitement envisage aborde des notions d’informatique
et l’integration d’un marquage des textes etudies, approches qui sont en prise directe avec la reﬂexion actuelle chez
les chercheurs et les entrepreneurs sur les donnees, leur statut, leur forme et leur traitement.

Cette demarche s’inscrit en ce sens entre la reﬂexion en sciences humaines et l’exploitation technique. Elle est
egalement en rapport avec la transmission d’une langue et son << outillage >> (Auroux, 1994). En effet, au—dela
d’une tentative consistant a modeliser les processus a l’oeuvre lors du dechiffrage d’un texte, il s’agit d’equiper
une langue, de l’enrichir d’une description utile.

De fait, pour (Gibson, 1998), etudier la complexite linguistique, c’est expliquer les etapes de l’apprentissage de sa
langue matemelle par un enfant, donner des elements pour aborder les problemes syntaxiques chez les aphasiques,
et foumir des applications dans lesquelles la comprehensibilite de la langue est importante, comme les correcteurs
grammaticaux ou la generation automatique de textes.

L’ interet premier porte sur la complexite de phrases, de paragraphes ou de textes ecrits en allemand. L’ attention
portera speciﬁquement sur un standard de cette langue, considere comme une koiné, une langue commune qui
depasse des disparites regionales.

ll ne s’agit donc pas d’un travail de comparaison entre differentes langues. En revanche, la pertinence de la notion
de sous—langage po11rra etre examinee. De meme, dans un deuxieme temps, une adaptation de la demarche et des
outils a l’anglais et au francais apportera peut—etre quelques elements qui viendront enrichir la comprehension du
suj et en inﬁrmant ou conﬁrmant des hypotheses.

ADRIEN BARBARESI

2 Méthode

2.1 Diviser pour mieux appréhender

Du point de Vue linguistique, la complexite est avant tout un phénomene difﬁcile a deﬁnir (Kusters & Muysken,
2001)

Dans la lignée du concept d’<< architecture >> de la complexite (Simon, 1962), i1 s’agit de diviser pour mieux
comprendre. Tout systeme complexe est quasiment decomposable (<< nearly—decomposable >>) en sous-éléments,
qui peuvent eux—memes etre complexes. Meme si cette approche ne tient pas compte des interactions e11tre les
differents sous—systemes, elle propose néanmoins de chercher des strates élémentaires en bas de la hierarchie d’ un
systeme, postulant que celles—ci peuvent etre comprises et modelisées efﬁcacement.

Or, dans le cas qui nous intéresse, ces unites pourraient etre les mots, qui permettent d’aborder la complexite mor-
phologique ainsi que par extension les bouts de phrases (chunks), aﬁn de saisir la complexité morpho—syntaxique et
éventuellement de s’interesser a la decomposition syntaxique d’un groupe ou d’une phrase. Les phenomenes inter-
phrastiques representent un niveau supérieur et bien plus délicat a analyser automatiquement, celui du discours et
de la linguistique textuelle.

La division du texte en unites linguistiques pertinentes comme par exemple les groupes nominaux presente
également l’intérét de s’approcher des études psycho—linguistiques qui prennent en compte la notion de coﬁt de
rattachement des composants les uns aux autres. En effet, la tradition cognitiviste Voit la complexite linguistique
comme un coﬁt supplementaire de traitement pour un éventuel recepteur. La dimension syntaxique joue un grand
role a travers notamment les principes d’ integration et de distance entre constituants. Le coﬁt de rattachement des
unites linguistiques est modelisé en termes d’unites d’énergie et d’unités de memoire a tres court terme (Gibson,
1998), dans la lignée des premiers travaux dans cette discipline (Miller, 1956).

Le cadre de pensée de la grammaire generative a incontestablement contribué a faire évoluer l’idee de complexité.
Le fait qu’il s’agisse avant tout d’un modele syntaxique du langage a aussi joue un role dans l’abord de cette notion
en conférant une certaine importance a la syntaxe qui joue aujourd’hui encore un role. Cela dit, cette notion est
loin de se limiter a cette dimension. La complexite d’ un point de Vue linguistique regroupe différents phenomenes
dont il s’agit de modéliser le rapport, de meme que différentes approches a croiser.

2.2 Croiser les approches et enrichir les pratiques

(Corbin, 1980) opposait << deux facons pour un linguiste de constituer les données sur lesquelles il travaille :
l’introspection, le corpus >>, et deux methodes, la << linguistique de bureau >> et la << linguistique de terrain >>.
Cette distinction a également investi le traitement automatique des langues, ou elle semble toujours d’actualité
selon (Cori, 2008), qui oppose << TAL theorique >> et << TAL robuste >>. Elle conceme aussi bien la démarche
(processus longs et diversiﬁés d’une part, traitement plus pres de la surface d’autre part) que les objectifs (en lien
avec la constitution d’un systeme linguistique d’un cote et avec des contraintes pratiques comme l’hétérogenéite
des textes de l’autre).

On pourrait ajouter a la distinction établie la << linguistique a l’instrument >> déﬁnie par (Habert, 2005), une
discipline qui exploite des corpus tout en menant une réﬂexion sur les instruments qu’elle utilise. Reﬂexion que
(Latour, 1985) appelait déja de ses Voeux :

<< Nous oublions toujours l’importance des inscriptions, de leurs strates successives et leur ”mise
en instrument” alors que nous parlons pourtant d’étres qui ne sont Visibles qu’ainsi. >>

Dans un contexte informatique qui a Vu augmenter la Vitesse de calcul a un rythme soutenu, on a Vu s’operer
un déplacement en TAL des modeles de la competence, fondés sur des bases théoriques fortes qui precedent le
traitement automatique, Vers ceux de la performance, qui ont une Vision plus statistique du langage et donnent
plus d’i1nportance a l’apprentissage par des techniques d’intelligence artiﬁcielle, si bien que ces derniers ont
aujourd’hui souvent la preference dans la communaute scientiﬁque.

Ce changement a bel et bien contribué a ameliorer l’efﬁcacite des méthodes sur des criteres quantitatifs. Toutefois,
cette reussite ne doit pas faire oublier qu’il s’agit de << croiser les approches >> et d’aller dans le sens d’une

LA COMPLEXITE LINGUISTIQUE, M]-’3THODE D’ANALYSE

evaluation qualitative qui s’attache a

< munir les donnees attestees d’annotations ﬁnes, multiples, permettant de progresser vers les
regularites sous—jacentes >> (Habert & Zweigenbaum, 2002)

Cette demarche se veut attentive a la pertinence des textes etudies, en rapport avec le but de recherche poursuivi, a
l’existence de nombreux criteres d’annotation qui doivent pouvoir coexister, se confronter, interagir a travers leur
marquage et enﬁn a un apport proprement linguistique concemant la comprehension d’un phenomene langagier
resultant de cette analyse.

3 Travail sur corpus

Ce travail est a situer dans un contexte ou il n’y a plus de separation hermetique entre les approches fondees sur
un corpus (corpus—based) et celles partant d’un modele theorique (theory—driven), mais bien une approche hybride
fondee sur la connaissance pour evaluer, tester, generer des hypotheses (Wallis & Nelson, 2001).

Le corpus est constitue de textes écrits en allemand standard, et devra permettre une analyse djfférenciee de
la complexite. Pour ce faire, on peut imaginer de traiter des textes divers, dont un echantillon aura ete releve
manuellement voire soumis a un panel test pour permettre d’etudier la correlation des resultats obtenus a grande
echelle avec les resultats obtenus sur l’echantillon.

L’ etude sera comparative, et ce a deux niveaux differents : comparaison des resultats obtenus avec ceux d’un
corpus etalonné d’ une part, comparaison de corpus connus pour etre deux versions d’un meme texte d’autre part
(litterature simpliﬁée pour les enfants ou les apprenants par exemple, articles de joumaux sur le meme theme,
articles scientiﬁques et leur vulgarisation).

Par ailleurs, on peut imaginer d’effectuer d’autres comparaisons apportant un autre eclairage sur la notion de
complexite, comme les regularites dans l’utilisation d’une langue apprise (ou seconde). On peut determiner si
d’apres les mesures les textes écrits par des locuteurs natifs se caracterisent par la presence d’un plus grand
nombre d’indicateurs de la complexite.

Concemant les textes pris en compte, l’apport des textes du domaine public est essentiel dans une optique de
transmissibilite des corpus et des resultats. En effet, la question des droits d’auteurs est encadree beaucoup plus
strictement en Allemagne qu’en France.

Aussi l’etude d’articles de joumaux, par exemple une comparaison sur les articles traitant des memes sujets dans
un quotidien a tres grand tirage (la Bild-Zeitung) et dans un hebdomadaire (Die Zeit) est soumise a caution. S’il
est possible d’obtenir les articles en téléchargeant et nettoyant des pages web, s’il est possible de les analyser
librement, il n’est pas permis de rendre disponible le texte enrichi et etiqueté sans recourir a des techniques dites
de << masque >> (Rehm et al., 2007), par exemple en remplagant les mots étiquetés par des autres choisis au hasard
dans la catégorie correpondante, ou en melangeant les phrases du corpus au hasard. Ces techniques etent toute
notion de coherence et de cohesion au texte obtenu, limitant fortement l’interet d’un tel corpus.

Les articles de joumaux ne sont donc utilises jusqu’ici que pour un echantillonnage et une analyse < a vue >.
L’ etude sur corpus a jusqu’a present porté essentiellement sur des romans classiques, des textes philosophiques,
des romans pour enfants et des romans de gare. Le genre narratif est sur—représenté, reﬂetant la nature des oeuvres
rassemblees par le Projet Gutenberg.

Néanmoins, les articles scientiﬁques constituent une piste interessante a double titre : d’une part du point de vue
technique, parce qu’il est souvent aise d’en extraire le texte, voire de remonter automatiquement d’un article a
un autre et parce qu’ils peuvent dans la plupart des cas etre republies, d’autre part du point de vue linguistique,
puisqu’ils ne sont pas toujours ecrits par des germanophones et qu’une etude comparée pourrait souligner l’exis—
tence ou non de regularites chez les uns et les autres.

Enﬁn, le corpus est susceptible d’etre elargi a des textes en anglais et en frangais, aﬁn de valider ou d’inﬁrmer les
resultats obtenus et d’en tirer d’autres conclusions par une etude comparative.

ADRIEN BARBARESI

4 Traitement

4.1 Hypothese de recherche

En prenant connais sance d’un texte, on se demande souvent s’il est djfﬁcile a comprendre. Un rapide survol permet
d’ en savoir un peu plus sur le temps, les connaissances et les ressources necessaires a une lecture satisfaisante.

11 est possible de concevoir un programme capable d’effectuer une operation de << survol >> qui determine le degre
de complexite de donnees textuelles destinees a etre traitees automatiquement. Cet indice peut prendre en compte
plusieurs dimensions de ce phenomene et plusieurs echelles, de la phrase au texte, de la morphologie a l’analyse
du discours en passant par la syntaxe.

11 est possible d’isoler les parties qui ne poseront aucun probleme et celles qui ont besoin d’une analyse en pro-
fondeur. Plutot que d’appliquer une methode qui privilegierait la rapidite a la justesse (ou Vice-Versa) on peut
alors ponderer ces deux parametres en appliquant un traitement djfferencie, Voire meme adapte aux difﬁcultes
rencontrees.

ll s’agit ici de traiter cette question avec les moyens de la linguistique informatique et de l’integrer au sein de la
chaine d’ operations qui Va du texte brut au texte enrichi syntaxiquement et semantiquement. Mesurer la complexite
peut amener a mobiliser des ressources et des traitements complexes (comme l’analyse grammaticale automatique
par exemple).

ll importe des lors d’ experimenter l’apport effectif dans un indicateur de complexite des informations construites
a l’aide de ces ressources et traitements. Cela peut conduire a adopter des approximations de certains de ces
traitements, a partir du moment ou il s’aVere que de tels traitements moins complexes foumissent une image
<< raisonnable >> de la complexite.

4.2 Architecture de traitement

La principale contrainte du point de Vue du fonctionnement est donc de maintenir un niveau de complexite algo-
rithmique relativement faible et de rester au plus pres d’une approche lineaire, balayant les mots au ﬁl du texte.

La chaine de traitement Va du texte brut au texte enrichi selon le schema suivant :

T
Ne“30Y3ge l _ Etiquetage _[Chunk parsing] [Base de données]

(  Scripts I 7 TreeTagger [Automate ﬁni J L SQLYFC J
Km

La chaine de traitement est guidee par un script bash. Le nettoyage emploie notamment le logiciel sed, il comprend
un decoupage en tokens qui comme le reste des scripts est implemente en Perl, en raison de la polyvalence et de
l’adequation de ce langage a la manipulation de texte. L’ etiqueteur utilise est le TreeTagger de l’IMS Stuttgart
(Sch1nid, 1994), en raison de sa precision dans le cas de l’allemand.

Les mesures de complexite sont ensuite effectuees et exportees au choix sous deux formes differentes :

Export XML

    

Mesures
Scripts Perl

   
   

La decomposition du texte et la mesure se font par des cascades a etats ﬁnis (Abney, 1996). On tente par la meme
d’ eviter dans la mesure du possible une << dilution des heuristiques >> (Trouilleux, 2009), qui pourrait resulter
de l’utilisation de techniques d’ intelligence artiﬁcielle necessitant un apprentissage cible comme les machines a
Vecteurs de support (Support Vector Machines, SVM) ou les reseaux de neurones artiﬁciels qui conduisent souvent
a ce que l’on appelle des << boites noires >>.

LA COMPLEXITE LINGUISTIQUE, METHODE D’ANALYSE

L’ elaboration du programme de traitement se situe a un stade de recherche selon la classiﬁcation de (Veronis,
2000) : les travaux realises sont de nature prospective mais ne donnent pas encore lieu a des implementations
utilisables en situation d’annotation reelle.

4.3 Critéres

Cette liste n’a pas vocation a etre exhaustive, elle concerne des phenomenes dont le reperage est envisageable,
voire parfois deja mis en pratique.

l. MOTS

longueur par exemple en ﬁxant un seuil a 17 caracteres, ce qui represente en general un peu moins de 5 %
des mots rencontres dans la langue;

fréquence relative au sein du document et par rapport a une liste de mots frequents etablie a partir d’un
grand corpus (par exemple la liste des 10000 mots les plus frequents du Corpus Leipzig 1) ;

lemme savoir si le lemme est reconnu par l’etiqueteur morpho-syntaxique (le TreeTagger propose cette
option) revient a approximer un ensemble de mots connus;

2. GROUPES

taille elle peut donner une idee des djfﬁcultes de rattachement des composants, par exemple dans un groupe
nominal long ;

composition des groupes nominaux atypiques, reperables par une suite d’etiquettes, peuvent signaler une
structure plus complexe (de meme qu’une erreur de l’etiqueteur, ce qui represente egalement une
forme de complexité);

nombre une phrase comportant de nombreux groupes posera sans doute des problemes de rattachement,
par exemple si cinq groupes nominaux sont trouves;

3. PHRASES

longueur parmi les criteres les plus souvent retenus dans les etudes de lisibilite, la longueur en caracteres
ﬁgure en bonne place (a l’usage, les seuils aux alentours de 130 et de 190 caracteres semblent indiquer
que la phrase se complexiﬁe) ;

virgules en allemand, les propositions sont obligatoirement suivies de virgules, c’est la un moyen efﬁcace
pour repérer d’eventuelles subordonnees, a condition d’eviter les enumérations. (Les seuils retenus
jusque la sont de 0 come indice de simpliﬁcation et 3 come complexiﬁcation) ;

subordonnées determiner leur type de meme que leur nombre par phrase par une observation des pronoms
relatifs correlee au critere precedent peut s’averer pertinente;

verbes la forme et la rection des verbes peuvent corroborer l’examen des groupes nominaux;

attaque d’énoncé ce champ est ﬂexible en allemand, il denote parfois des phenomenes de linearisation, par
exemple une volonte de mettre en avant une partie de la phrase pour la rendre plus comprehensible.

Parmi les sources eventuelles de complexité pour lesquelles les criteres manquent citons les ellipses et la densite
conceptuelle en general, les a1nbigu'1'tes syntaxiques et semantiques et a plus large echelle les phenomenes de
coherence et de cohesion textuelle.

5 Notion de complexité

On peut dire qu’au sein d’une tradition philosophique puis philologique les debats autour de la notion de com-
plexité du langage sont anciens, notarmnent a travers la question qui consiste a savoir si certaines pensées ou idees
sont en elles-memes complexes, si elles prennent une toumure complexe lors de leur expression ou s’il existe un
lien entre les deux. Cette question se pose de maniere particuliere depuis le courant rationaliste, qui postule le
primat de l’idee pure sur la materialite de l’expression. Avec le concept de structure profonde, la complexité est

1. http ://wortschatz.uni-leipzig.de/

ADRIEN BARBARESI

meme en quelque sorte inherente a la relecture d’une certaine << linguistique cartésienne >> (Chomsky, 1966), c’est
donc tout naturellement que l’attrait pour cette notion a connu une nette recrudescence lors du développement de
la grammaire generative.

Plus tard, le mouvement de la << nouvelle lisibilité >> dans les armees 80, en butte a une conception machinale du
cerveau vu comme un mécanisme de traitement de l’information, se réapproprie l’idée et 1’ applique au champ de la
psycho—linguistique toume vers des facteurs organisationnels comme la densité des idées et des concepts (Kemper,
1983) ou la structure et la presentation d’un document (Britton et al., 1982). Ce changement de paradigme dans
les sciences cognitives a partie liée avec l’émergence de la linguistique textuelle.

Plus récemment, le langage vu comme systeme complexe adaptatif (Beckner et al., 2009) est une these qui séduit
de plus de chercheurs qui souhaitent fournir une théorie de l’interdependance des sous-systemes que sont par
exemple les traits phonétiques, morphologiques ou la syntaxe.

Toutes ces approches de la complexité sont autant de raisons de s’atteler a la Inise en valeur de la proximité des
<< traces recombinées >> (Latour, 1985).

Références

ABNEY S. (1996). Partial parsing via ﬁnite-state cascades. Natural language Engineering, 2(4), 337-344.
AUROUX S. (1994). la revolution technologique de la grammatisation. Liege : Mardaga.

BECKNER C., ELLIS N., BLYTHE R., HOLLAND J., BYBEE J., KE J., CHRISTIANSEN M., LARSEN-
FREEMAN D., CROFT W. & SCHOENEMANN T. (2009). Language Is a Complex Adaptive System : Position
Paper. language As a Complex Adaptive System, 59(1), 1-26.

BRITTON B., GLYNN S., MEYER B. & PENLAND M. (1982). Effects of text structure on use of cognitive
capacity during reading. Journal of Educational Psychology, 74(1), 51-61.

CHOMSKY N. (1966). Cartesian linguistics : A chapter in the history of rationalist thought. Harper & Row.
CORBIN P. (1980). De la production des données en linguistique introspective. In Theories linguistiques et
traditions grammaticales, p. 121-179. Villeneuve—d’Ascq : Presses Universitaires de Lille.

CORI M. (2008). Des méthodes de traitement automatique aux linguistiques fondées sur les corpus. langages,
171.

GIBSON E. (1998). Linguistic complexity : Locality of syntactic dependencies. Cognition, 68(1), 1-76.
HABERT B. (2005). Portrait de linguiste(s) a l’instrument. Texto !, 10(4).

HABERT B. & ZWEIGENBAUM P. (2002). Régler les regles. TAL, 43(3), 83-105.

KEMPER S. (1983). Measuring the inference load of a text. Journal of educational psychology, 75(3), 391-401.
KUSTERS W. & MUYSKEN P. (2001). The complexities of arguing about complexity. Linguistic Typology,
5(2/3), 182-185.

LATOUR B. (1985). Les << vues >> de l’esprit. Culture technique, 14, 4-29.

MILLER G. A. (1956). The magical number seven, plus or minus two : Some limits on our capacity for process-
ing information. The Psychological Review, 63, 81-97.

REHM G., WITT A., ZINSMEISTER H. & DELLERT J . (2007). Corpus masking : Legally bypassing licensing
restrictions for the free distribution of text collections. Digital Humanities, p. 166-170.

SCHMID H. (1994). Probabilistic Part—Of—Speech Tagging Using Decision Trees. In Proceedings of the Interna-
tional Conference on New Methods in language Processing, volume 12 : Manchester.

SIMON H. A. (1962). The Architecture of Complexity. Proceedings of the American Philosophical Society,
106(6), 467-482.

TROUILLEUX F. (2009). Un analyseur de surface non déterministe pour le francais. In I6eme Conference sur le
Traitement Automatique des langues Naturelles, Senlis.

VERONIS J . (2000). Annotation automatique de corpus : panorama et état de la technique. In J .—M. PIERREL,
Ed., Ingénierie des langues, Informatique et systemes d ’information, p. 111-129. Paris : Hermes Science.

WALLIS S. & NELSON G. (2001). Knowledge discovery in grammatically analysed corpora. Data Mining and
Knowledge Discovery, 5(4), 305-335.

