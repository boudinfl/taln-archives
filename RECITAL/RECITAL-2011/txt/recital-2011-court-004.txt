TALN 2011, Montpellier, 27 juin – 1er juillet 2011
La complexité linguistique
Méthode d’analyse
Adrien Barbaresi
ICAR, ENS LYON
Résumé. La complexité linguistique regroupe différents phénomènes dont il s’agit de modéliser le rapport.
Le travail en cours que je décris ici propose une réflexion sur les approches linguistiques et techniques de cette
notion et la mise en application d’un balayage des textes qui s’efforce de contribuer à leur enrichissement. Ce
traitement en surface effectué suivant une liste de critères qui représentent parfois des approximations de logiques
plus élaborées tente de fournir une image  raisonnable  de la complexité.
Abstract. Linguistic complexity includes various linguistic phenomena which interaction is to be modeled.
The ongoing work described here tackles linguistic and technical approaches of this idea as well as an implemen-
tation of a parsing method which is part of text enrichment techniques. This chunk parsing is performed according
to a list of criteria that may consist in logical approximations of more sophisticated processes in order to provide
a  reasonable  image of complexity..
Mots-clés : Complexité, lisibilité, allemand, analyse de surface.
Keywords: Complexity, lisibility, German, chunk parsing.
1 Enjeux
L’analyse de la complexité se situe dans le cadre de l’assistance à la compréhension. Il s’agit ici de déterminer
la lisibilité d’un texte pour des humains ou pour des machines, c’est-à-dire d’une part le niveau de maı̂trise et de
pratique de la langue requis et d’autre part le modèle formel et les instruments à utiliser.
Ce thème très riche invite à penser les langues avant de les analyser à différentes échelles et selon différents modes
opératoires. Du point de vue disciplinaire, on peut le situer à la croisée de la linguistique, des études sur la lisibilité,
des sciences cognitives et de la théorie de l’information. Le traitement envisagé aborde des notions d’informatique
et l’intégration d’un marquage des textes étudiés, approches qui sont en prise directe avec la réflexion actuelle chez
les chercheurs et les entrepreneurs sur les données, leur statut, leur forme et leur traitement.
Cette démarche s’inscrit en ce sens entre la réflexion en sciences humaines et l’exploitation technique. Elle est
également en rapport avec la transmission d’une langue et son  outillage  (Auroux, 1994). En effet, au-delà
d’une tentative consistant à modéliser les processus à l’œuvre lors du déchiffrage d’un texte, il s’agit d’équiper
une langue, de l’enrichir d’une description utile.
De fait, pour (Gibson, 1998), étudier la complexité linguistique, c’est expliquer les étapes de l’apprentissage de sa
langue maternelle par un enfant, donner des éléments pour aborder les problèmes syntaxiques chez les aphasiques,
et fournir des applications dans lesquelles la compréhensibilité de la langue est importante, comme les correcteurs
grammaticaux ou la génération automatique de textes.
L’intérêt premier porte sur la complexité de phrases, de paragraphes ou de textes écrits en allemand. L’attention
portera spécifiquement sur un standard de cette langue, considéré comme une koinè, une langue commune qui
dépasse des disparités régionales.
Il ne s’agit donc pas d’un travail de comparaison entre différentes langues. En revanche, la pertinence de la notion
de sous-langage pourra être examinée. De même, dans un deuxième temps, une adaptation de la démarche et des
outils à l’anglais et au français apportera peut-être quelques éléments qui viendront enrichir la compréhension du
sujet en infirmant ou confirmant des hypothèses.
ADRIEN BARBARESI
2 Méthode
2.1 Diviser pour mieux appréhender
Du point de vue linguistique, la complexité est avant tout un phénomène difficile à définir (Kusters & Muysken,
2001).
Dans la lignée du concept d’ architecture  de la complexité (Simon, 1962), il s’agit de diviser pour mieux
comprendre. Tout système complexe est quasiment décomposable ( nearly-decomposable ) en sous-éléments,
qui peuvent eux-mêmes être complexes. Même si cette approche ne tient pas compte des interactions entre les
différents sous-systèmes, elle propose néanmoins de chercher des strates élémentaires en bas de la hiérarchie d’un
système, postulant que celles-ci peuvent être comprises et modélisées efficacement.
Or, dans le cas qui nous intéresse, ces unités pourraient être les mots, qui permettent d’aborder la complexité mor-
phologique ainsi que par extension les bouts de phrases (chunks), afin de saisir la complexité morpho-syntaxique et
éventuellement de s’intéresser à la décomposition syntaxique d’un groupe ou d’une phrase. Les phénomènes inter-
phrastiques représentent un niveau supérieur et bien plus délicat à analyser automatiquement, celui du discours et
de la linguistique textuelle.
La division du texte en unités linguistiques pertinentes comme par exemple les groupes nominaux présente
également l’intérêt de s’approcher des études psycho-linguistiques qui prennent en compte la notion de coût de
rattachement des composants les uns aux autres. En effet, la tradition cognitiviste voit la complexité linguistique
comme un coût supplémentaire de traitement pour un éventuel récepteur. La dimension syntaxique joue un grand
rôle à travers notamment les principes d’intégration et de distance entre constituants. Le coût de rattachement des
unités linguistiques est modélisé en termes d’unités d’énergie et d’unités de mémoire à très court terme (Gibson,
1998), dans la lignée des premiers travaux dans cette discipline (Miller, 1956).
Le cadre de pensée de la grammaire générative a incontestablement contribué à faire évoluer l’idée de complexité.
Le fait qu’il s’agisse avant tout d’un modèle syntaxique du langage a aussi joué un rôle dans l’abord de cette notion
en conférant une certaine importance à la syntaxe qui joue aujourd’hui encore un rôle. Cela dit, cette notion est
loin de se limiter à cette dimension. La complexité d’un point de vue linguistique regroupe différents phénomènes
dont il s’agit de modéliser le rapport, de même que différentes approches à croiser.
2.2 Croiser les approches et enrichir les pratiques
(Corbin, 1980) opposait  deux façons pour un linguiste de constituer les données sur lesquelles il travaille :
l’introspection, le corpus , et deux méthodes, la  linguistique de bureau  et la  linguistique de terrain .
Cette distinction a également investi le traitement automatique des langues, où elle semble toujours d’actualité
selon (Cori, 2008), qui oppose  TAL théorique  et  TAL robuste . Elle concerne aussi bien la démarche
(processus longs et diversifiés d’une part, traitement plus près de la surface d’autre part) que les objectifs (en lien
avec la constitution d’un système linguistique d’un côté et avec des contraintes pratiques comme l’hétérogénéité
des textes de l’autre).
On pourrait ajouter à la distinction établie la  linguistique à l’instrument  définie par (Habert, 2005), une
discipline qui exploite des corpus tout en menant une réflexion sur les instruments qu’elle utilise. Réflexion que
(Latour, 1985) appelait déjà de ses vœux :
 Nous oublions toujours l’importance des inscriptions, de leurs strates successives et leur ”mise
en instrument” alors que nous parlons pourtant d’êtres qui ne sont visibles qu’ainsi. 
Dans un contexte informatique qui a vu augmenter la vitesse de calcul à un rythme soutenu, on a vu s’opérer
un déplacement en TAL des modèles de la compétence, fondés sur des bases théoriques fortes qui précèdent le
traitement automatique, vers ceux de la performance, qui ont une vision plus statistique du langage et donnent
plus d’importance à l’apprentissage par des techniques d’intelligence artificielle, si bien que ces derniers ont
aujourd’hui souvent la préférence dans la communauté scientifique.
Ce changement a bel et bien contribué à améliorer l’efficacité des méthodes sur des critères quantitatifs. Toutefois,
cette réussite ne doit pas faire oublier qu’il s’agit de  croiser les approches  et d’aller dans le sens d’une
LA COMPLEXITÉ LINGUISTIQUE, MÉTHODE D’ANALYSE
évaluation qualitative qui s’attache à
 munir les données attestées d’annotations fines, multiples, permettant de progresser vers les
régularités sous-jacentes  (Habert & Zweigenbaum, 2002)
Cette démarche se veut attentive à la pertinence des textes étudiés, en rapport avec le but de recherche poursuivi, à
l’existence de nombreux critères d’annotation qui doivent pouvoir coexister, se confronter, interagir à travers leur
marquage et enfin à un apport proprement linguistique concernant la compréhension d’un phénomène langagier
résultant de cette analyse.
3 Travail sur corpus
Ce travail est à situer dans un contexte où il n’y a plus de séparation hermétique entre les approches fondées sur
un corpus (corpus-based) et celles partant d’un modèle théorique (theory-driven), mais bien une approche hybride
fondée sur la connaissance pour évaluer, tester, générer des hypothèses (Wallis & Nelson, 2001).
Le corpus est constitué de textes écrits en allemand standard, et devra permettre une analyse différenciée de
la complexité. Pour ce faire, on peut imaginer de traiter des textes divers, dont un échantillon aura été relevé
manuellement voire soumis à un panel test pour permettre d’étudier la corrélation des résultats obtenus à grande
échelle avec les résultats obtenus sur l’échantillon.
L’étude sera comparative, et ce à deux niveaux différents : comparaison des résultats obtenus avec ceux d’un
corpus étalonné d’une part, comparaison de corpus connus pour être deux versions d’un même texte d’autre part
(littérature simplifiée pour les enfants ou les apprenants par exemple, articles de journaux sur le même thème,
articles scientifiques et leur vulgarisation).
Par ailleurs, on peut imaginer d’effectuer d’autres comparaisons apportant un autre éclairage sur la notion de
complexité, comme les régularités dans l’utilisation d’une langue apprise (ou seconde). On peut déterminer si
d’après les mesures les textes écrits par des locuteurs natifs se caractérisent par la présence d’un plus grand
nombre d’indicateurs de la complexité.
Concernant les textes pris en compte, l’apport des textes du domaine public est essentiel dans une optique de
transmissibilité des corpus et des résultats. En effet, la question des droits d’auteurs est encadrée beaucoup plus
strictement en Allemagne qu’en France.
Aussi l’étude d’articles de journaux, par exemple une comparaison sur les articles traitant des mêmes sujets dans
un quotidien à très grand tirage (la Bild-Zeitung) et dans un hebdomadaire (Die Zeit) est soumise à caution. S’il
est possible d’obtenir les articles en téléchargeant et nettoyant des pages web, s’il est possible de les analyser
librement, il n’est pas permis de rendre disponible le texte enrichi et étiqueté sans recourir à des techniques dites
de  masque  (Rehm et al., 2007), par exemple en remplaçant les mots étiquetés par des autres choisis au hasard
dans la catégorie correpondante, ou en mélangeant les phrases du corpus au hasard. Ces techniques ôtent toute
notion de cohérence et de cohésion au texte obtenu, limitant fortement l’intérêt d’un tel corpus.
Les articles de journaux ne sont donc utilisés jusqu’ici que pour un échantillonnage et une analyse  à vue .
L’étude sur corpus a jusqu’à présent porté essentiellement sur des romans classiques, des textes philosophiques,
des romans pour enfants et des romans de gare. Le genre narratif est sur-représenté, reflétant la nature des œuvres
rassemblées par le Projet Gutenberg.
Néanmoins, les articles scientifiques constituent une piste intéressante à double titre : d’une part du point de vue
technique, parce qu’il est souvent aisé d’en extraire le texte, voire de remonter automatiquement d’un article à
un autre et parce qu’ils peuvent dans la plupart des cas être republiés, d’autre part du point de vue linguistique,
puisqu’ils ne sont pas toujours écrits par des germanophones et qu’une étude comparée pourrait souligner l’exis-
tence ou non de régularités chez les uns et les autres.
Enfin, le corpus est susceptible d’être élargi à des textes en anglais et en français, afin de valider ou d’infirmer les
résultats obtenus et d’en tirer d’autres conclusions par une étude comparative.
ADRIEN BARBARESI
4 Traitement
4.1 Hypothèse de recherche
En prenant connaissance d’un texte, on se demande souvent s’il est difficile à comprendre. Un rapide survol permet
d’en savoir un peu plus sur le temps, les connaissances et les ressources nécessaires à une lecture satisfaisante.
Il est possible de concevoir un programme capable d’effectuer une opération de  survol  qui détermine le degré
de complexité de données textuelles destinées à être traitées automatiquement. Cet indice peut prendre en compte
plusieurs dimensions de ce phénomène et plusieurs échelles, de la phrase au texte, de la morphologie à l’analyse
du discours en passant par la syntaxe.
Il est possible d’isoler les parties qui ne poseront aucun problème et celles qui ont besoin d’une analyse en pro-
fondeur. Plutôt que d’appliquer une méthode qui privilégierait la rapidité à la justesse (ou vice-versa) on peut
alors pondérer ces deux paramètres en appliquant un traitement différencié, voire même adapté aux difficultés
rencontrées.
Il s’agit ici de traiter cette question avec les moyens de la linguistique informatique et de l’intégrer au sein de la
chaı̂ne d’opérations qui va du texte brut au texte enrichi syntaxiquement et sémantiquement. Mesurer la complexité
peut amener à mobiliser des ressources et des traitements complexes (comme l’analyse grammaticale automatique
par exemple).
Il importe dès lors d’expérimenter l’apport effectif dans un indicateur de complexité des informations construites
à l’aide de ces ressources et traitements. Cela peut conduire à adopter des approximations de certains de ces
traitements, à partir du moment où il s’avère que de tels traitements moins complexes fournissent une image
 raisonnable  de la complexité.
4.2 Architecture de traitement
La principale contrainte du point de vue du fonctionnement est donc de maintenir un niveau de complexité algo-
rithmique relativement faible et de rester au plus près d’une approche linéaire, balayant les mots au fil du texte.
La chaı̂ne de traitement va du texte brut au texte enrichi selon le schéma suivant :
Nettoyage Étiquetage Chunk parsing Base de données
Texte brut
Scripts TreeTagger Automate fini SQLite
La chaı̂ne de traitement est guidée par un script bash. Le nettoyage emploie notamment le logiciel sed, il comprend
un découpage en tokens qui comme le reste des scripts est implémenté en Perl, en raison de la polyvalence et de
l’adéquation de ce langage à la manipulation de texte. L’étiqueteur utilisé est le TreeTagger de l’IMS Stuttgart
(Schmid, 1994), en raison de sa précision dans le cas de l’allemand.
Les mesures de complexité sont ensuite effectuées et exportées au choix sous deux formes différentes :
Mesures Base de données
Texte enrichi Scripts Perl
Export XML
La décomposition du texte et la mesure se font par des cascades à états finis (Abney, 1996). On tente par là même
d’éviter dans la mesure du possible une  dilution des heuristiques  (Trouilleux, 2009), qui pourrait résulter
de l’utilisation de techniques d’intelligence artificielle nécessitant un apprentissage ciblé comme les machines à
vecteurs de support (Support Vector Machines, SVM) ou les réseaux de neurones artificiels qui conduisent souvent
à ce que l’on appelle des  boı̂tes noires .
LA COMPLEXITÉ LINGUISTIQUE, MÉTHODE D’ANALYSE
L’élaboration du programme de traitement se situe à un stade de recherche selon la classification de (Véronis,
2000) : les travaux réalisés sont de nature prospective mais ne donnent pas encore lieu à des implémentations
utilisables en situation d’annotation réelle.
4.3 Critères
Cette liste n’a pas vocation à être exhaustive, elle concerne des phénomènes dont le repérage est envisageable,
voire parfois déjà mis en pratique.
1. MOTS
longueur par exemple en fixant un seuil à 17 caractères, ce qui représente en général un peu moins de 5 %
des mots rencontrés dans la langue ;
fréquence relative au sein du document et par rapport à une liste de mots fréquents établie à partir d’un
grand corpus (par exemple la liste des 10000 mots les plus fréquents du Corpus Leipzig 1) ;
lemme savoir si le lemme est reconnu par l’étiqueteur morpho-syntaxique (le TreeTagger propose cette
option) revient à approximer un ensemble de mots connus ;
2. GROUPES
taille elle peut donner une idée des difficultés de rattachement des composants, par exemple dans un groupe
nominal long ;
composition des groupes nominaux atypiques, repérables par une suite d’étiquettes, peuvent signaler une
structure plus complexe (de même qu’une erreur de l’étiqueteur, ce qui représente également une
forme de complexité) ;
nombre une phrase comportant de nombreux groupes posera sans doute des problèmes de rattachement,
par exemple si cinq groupes nominaux sont trouvés ;
3. PHRASES
longueur parmi les critères les plus souvent retenus dans les études de lisibilité, la longueur en caractères
figure en bonne place (à l’usage, les seuils aux alentours de 130 et de 190 caractères semblent indiquer
que la phrase se complexifie) ;
virgules en allemand, les propositions sont obligatoirement suivies de virgules, c’est là un moyen efficace
pour repérer d’éventuelles subordonnées, à condition d’éviter les énumérations. (Les seuils retenus
jusque là sont de 0 comme indice de simplification et 3 comme complexification) ;
subordonnées déterminer leur type de même que leur nombre par phrase par une observation des pronoms
relatifs corrélée au critère précedent peut s’avérer pertinente ;
verbes la forme et la rection des verbes peuvent corroborer l’examen des groupes nominaux ;
attaque d’énoncé ce champ est flexible en allemand, il dénote parfois des phénomènes de linéarisation, par
exemple une volonté de mettre en avant une partie de la phrase pour la rendre plus compréhensible.
Parmi les sources éventuelles de complexité pour lesquelles les critères manquent citons les ellipses et la densité
conceptuelle en général, les ambiguı̈tés syntaxiques et sémantiques et à plus large échelle les phénomènes de
cohérence et de cohésion textuelle.
5 Notion de complexité
On peut dire qu’au sein d’une tradition philosophique puis philologique les débats autour de la notion de com-
plexité du langage sont anciens, notamment à travers la question qui consiste à savoir si certaines pensées ou idées
sont en elles-mêmes complexes, si elles prennent une tournure complexe lors de leur expression ou s’il existe un
lien entre les deux. Cette question se pose de manière particulière depuis le courant rationaliste, qui postule le
primat de l’idée pure sur la matérialité de l’expression. Avec le concept de structure profonde, la complexité est
1. http ://wortschatz.uni-leipzig.de/
ADRIEN BARBARESI
même en quelque sorte inhérente à la relecture d’une certaine  linguistique cartésienne  (Chomsky, 1966), c’est
donc tout naturellement que l’attrait pour cette notion a connu une nette recrudescence lors du développement de
la grammaire générative.
Plus tard, le mouvement de la  nouvelle lisibilité  dans les années 80, en butte à une conception machinale du
cerveau vu comme un mécanisme de traitement de l’information, se réapproprie l’idée et l’applique au champ de la
psycho-linguistique tourné vers des facteurs organisationnels comme la densité des idées et des concepts (Kemper,
1983) ou la structure et la présentation d’un document (Britton et al., 1982). Ce changement de paradigme dans
les sciences cognitives a partie liée avec l’émergence de la linguistique textuelle.
Plus récemment, le langage vu comme système complexe adaptatif (Beckner et al., 2009) est une thèse qui séduit
de plus de chercheurs qui souhaitent fournir une théorie de l’interdépendance des sous-systèmes que sont par
exemple les traits phonétiques, morphologiques ou la syntaxe.
Toutes ces approches de la complexité sont autant de raisons de s’atteler à la mise en valeur de la proximité des
 traces recombinées  (Latour, 1985).
Références
ABNEY S. (1996). Partial parsing via finite-state cascades. Natural Language Engineering, 2(4), 337–344.
AUROUX S. (1994). La révolution technologique de la grammatisation. Liège : Mardaga.
BECKNER C., ELLIS N., BLYTHE R., HOLLAND J., BYBEE J., KE J., CHRISTIANSEN M., LARSEN-
FREEMAN D., CROFT W. & SCHOENEMANN T. (2009). Language Is a Complex Adaptive System : Position
Paper. Language As a Complex Adaptive System, 59(1), 1–26.
BRITTON B., GLYNN S., MEYER B. & PENLAND M. (1982). Effects of text structure on use of cognitive
capacity during reading. Journal of Educational Psychology, 74(1), 51–61.
CHOMSKY N. (1966). Cartesian linguistics : A chapter in the history of rationalist thought. Harper & Row.
CORBIN P. (1980). De la production des données en linguistique introspective. In Théories linguistiques et
traditions grammaticales, p. 121–179. Villeneuve-d’Ascq : Presses Universitaires de Lille.
CORI M. (2008). Des méthodes de traitement automatique aux linguistiques fondées sur les corpus. Langages,
171.
GIBSON E. (1998). Linguistic complexity : Locality of syntactic dependencies. Cognition, 68(1), 1–76.
HABERT B. (2005). Portrait de linguiste(s) à l’instrument. Texto !, 10(4).
HABERT B. & ZWEIGENBAUM P. (2002). Régler les règles. TAL, 43(3), 83–105.
KEMPER S. (1983). Measuring the inference load of a text. Journal of educational psychology, 75(3), 391–401.
KUSTERS W. & MUYSKEN P. (2001). The complexities of arguing about complexity. Linguistic Typology,
5(2/3), 182–185.
LATOUR B. (1985). Les  vues  de l’esprit. Culture technique, 14, 4–29.
MILLER G. A. (1956). The magical number seven, plus or minus two : Some limits on our capacity for process-
ing information. The Psychological Review, 63, 81–97.
REHM G., WITT A., ZINSMEISTER H. & DELLERT J. (2007). Corpus masking : Legally bypassing licensing
restrictions for the free distribution of text collections. Digital Humanities, p. 166–170.
SCHMID H. (1994). Probabilistic Part-Of-Speech Tagging Using Decision Trees. In Proceedings of the Interna-
tional Conference on New Methods in Language Processing, volume 12 : Manchester.
SIMON H. A. (1962). The Architecture of Complexity. Proceedings of the American Philosophical Society,
106(6), 467–482.
TROUILLEUX F. (2009). Un analyseur de surface non déterministe pour le français. In 16ème Conférence sur le
Traitement Automatique des Langues Naturelles, Senlis.
VÉRONIS J. (2000). Annotation automatique de corpus : panorama et état de la technique. In J.-M. PIERREL,
Ed., Ingénierie des langues, Informatique et systèmes d’information, p. 111–129. Paris : Hermès Science.
WALLIS S. & NELSON G. (2001). Knowledge discovery in grammatically analysed corpora. Data Mining and
Knowledge Discovery, 5(4), 305–335.
