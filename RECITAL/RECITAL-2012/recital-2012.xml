<?xml version="1.0" encoding="UTF-8"?>
<conference>
	<edition>
		<acronyme>RECITAL'2012</acronyme>
		<titre>14e Rencontres des Étudiants Chercheurs en Informatique pour le Traitement Automatique des Langues</titre>
		<ville>Grenoble</ville>
		<pays>France</pays>
		<dateDebut>2012-06-04</dateDebut>
		<dateFin>2012-06-08</dateFin>
		<presidents>
			<nom>Didier Schwab</nom>
			<nom>Jorge-Mauricio Molina-Mejia</nom>
		</presidents>
		<typeArticles>
			<type id="long">Papiers longs</type>
		</typeArticles>
		<statistiques>
			<acceptations id="long" soumissions="42">28</acceptations>
		</statistiques>
		<siteWeb>http://www.jeptaln2012.org/</siteWeb>
		<meilleurArticle>
			<articleId>recital-2012-long-001</articleId>
		</meilleurArticle>
	</edition>
	<articles>
		<article id="recital-2012-long-001" session="Orale 1">
			<auteurs>
					<auteur>
						<nom>Pierre Magistry</nom>
						<email></email>
						<affiliationId>1</affiliationId>
					</auteur>
				</auteurs>
				<affiliations>
					<affiliation affiliationId="1">Alpage, INRIA Paris-Rocquencourt &amp; Université Paris Diderot</affiliation>
				</affiliations>
			<titre>Segmentation non supervisée : le cas du mandarin</titre>
			<type>long</type>
			<pages>1-13</pages>
			<resume>Dans cet article, nous présentons un système de segmentation non supervisée que nous évaluons sur des données en mandarin. Notre travail s’inspire de l’hypothèse de Harris (1955) et suit Kempe (1999) et Tanaka-Ishii (2005) en se basant sur la reformulation de l’hypothèse en termes de variation de l’entropie de branchement. Celle-ci se révèle être un bon indicateur des frontières des unités linguistiques. Nous améliorons le système de (Jin et Tanaka-Ishii, 2006) en ajoutant une étape de normalisation qui nous permet de reformuler la façon dont sont prises les décisions de segmentation en ayant recours à la programmation dynamique. Ceci nous permet de supprimer la plupart des seuils de leur modèle tout en obtenant de meilleurs résultats, qui se placent au niveau de l’état de l’art (Wang et al., 2011) avec un système plus simple que ces derniers. Nous présentons une évaluation des résultats sur plusieurs corpus diffusés pour le Chinese Word Segmentation bake-off II (Emerson, 2005) et détaillons la borne supérieure que l’on peut espérer atteindre avec une méthode non-supervisée. Pour cela nous utilisons ZPAR en apprentissage croisé (Zhang et Clark, 2010) comme suggéré dans (Huang et Zhao, 2007; Zhao et Kit, 2008)</resume>
			<mots_cles>Apprentissage non-supervisé, segmentation, chinois, mandarin</mots_cles>
			<title>Unsupervized Word Segmentation</title>
			<abstract>In this paper, we present an unsupervised segmentation system tested on Mandarine Chinese. Following Harris’s Hypothesis in Kempe (1999) and Tanaka-Ishii (2005) reformulation, we base our work on the Variation of Branching Entropy. We improve on (Jin et Tanaka-Ishii, 2006) by adding normalization and Viterbi-decoding. This enables us to remove most of the thresholds and parameters from their model and to reach near state-of-the-art results (Wang et al., 2011) with a simpler system. We provide evaluation on different corpora available from the Segmentation bake-off II (Emerson, 2005) and define a more precise topline for the task using cross-trained supervised system available off-the-shelf (Zhang et Clark, 2010; Zhao et Kit, 2008; Huang et Zhao, 2007)</abstract>
			<keywords>Unsupervized machine learning, segmentation, Mandarin Chinese</keywords>
		</article>
		<article id="recital-2012-long-002" session="Orale 1">
			<auteurs>
				<auteur>
					<nom>Matthias Tauveron</nom>
					<email>matthias.tauveron@etu.unistra.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Fonctionnements Discursifs et Traduction, UR LiLPa, Université de Strasbourg</affiliation>
			</affiliations>
			<titre>Incrémentation lexicale dans les textes : une auto-organisation</titre>
			<type>long</type>
			<pages>15-28</pages>
			<resume>Nous proposons une étude dynamique du lexique, en décrivant la manière dont il s’organise progressivement du début à la fin d’un texte. Pour ce faire, nous nous focalisons sur la co-occurrence généralisée, en formant un graphe qui représente tous les lemmes du texte et synthétise leurs relations mutuelles de co-occurrence. L’étude d’un corpus de 40 textes montre que ces relations évoluent d’une manière auto-organisée : la forme - et l’identité - du graphe de co-occurrence restent stables après une phase d’organisation terminée avant la 1ère moitié du texte. Ensuite, il n’évolue plus : les nouveaux mots et les nouvelles relations de co-occurrence s’inscrivent peu à peu dans le réseau, sans modifier la forme d’ensemble de la structure. La relation de co-occurrence généralisée dans un texte apparaît donc comme la construction rapide d’un système, qui est ensuite assez souple pour canaliser un flux d’information sans changer d’identité.</resume>
			<mots_cles>Texte, lexique, co-occurrence généralisée, auto-organisation</mots_cles>
			<title>Lexical Incrementation within Texts: a Self-Organization</title>
			<abstract>We propose here a dynamic study of lexicon: we describe how it is organized progressively from the beginning to the end of a given text. We focus on the “generalized co-occurrence”, forming a graph that represents all the lemmas of the text and their mutual co-occurrence relations. The study of a corpus of 40 texts shows that these relations have a self-organized evolution: the shape and the identity of the graph of cooccurrence become stable after a period of organization finished before the first half of the text. Then they no longer change: new words and new co-occurrence relations gradually take place in the network without changing its overall shape. We show that the evolution of the “generalized co-occurrence” is the quick construction of a system, which is then flexible enough to channel the flow of information without changing its identity.</abstract>
			<keywords>Text, lexicon, generalized co-occurrence, self-organization</keywords>
		</article>
		<article id="recital-2012-long-003" session="Orale 1">
			<auteurs>
				<auteur>
					<nom>Alexander Panchenko</nom>
					<email>alexander.panchenko@student.uclouvain.be</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Center for Natural Language Processing (CENTAL), Université catholique de Louvain, College Erasme, 1 place Blaise Pascal, B-1348 Louvain-la-Neuve (Belgium</affiliation>
			</affiliations>
			<titre>Etude des mesures de similarité hétérogènes pour l’extraction de relations sémantiques</titre>
			<type>long</type>
			<pages>29-42</pages>
			<resume>L’article évalue un éventail de mesures de similarité qui ont pour but de prédire les scores de similarité sémantique et les relations sémantiques qui s’établissent entre deux termes, et étudie les moyens de combiner ces mesures. Nous présentons une analyse comparative à grande échelle de 34 mesures basées sur des réseaux sémantiques, le Web, des corpus, ainsi que des définitions. L’article met en évidence les forces et les faiblesses de chaque approche en contexte de l’extraction de relations. Enfin, deux techniques de combinaison de mesures sont décrites et testées. Les résultats montrent que les mesures combinées sont plus performantes que toutes les mesures simples et aboutissent à une corrélation de 0,887 et une Precision(20) de 0,979.</resume>
			<mots_cles>Similarité sémantique, Relations sémantiques, Similarité distributionnelle</mots_cles>
			<title>A Study of Heterogeneous Similarity Measures for Semantic Relation Extraction</title>
			<abstract>This paper evaluates a wide range of heterogeneous semantic similarity measures on the task of predicting semantic similarity scores and the task of predicting semantic relations that hold between two terms, and investigates ways to combine these measures. We present a large-scale benchmarking of 34 knowledge-, web-, corpus-, and definition-based similarity measures. The strengths and weaknesses of each approach regarding relation extraction are discussed. Finally, we describe and test two techniques for measure combination. These combined measures outperform all single measures, achieving a correlation of 0.887 and Precision(20) of 0.979.</abstract>
			<keywords>Semantic Similarity, Semantic Relations, Distributional Similarity</keywords>
		</article>
		<article id="recital-2012-long-004" session="Orale 2">
			<auteurs>
				<auteur>
					<nom>Luong Ngoc Quang</nom>
					<email>Ngoc-Quang.Luong@imag.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Laboratoire LIG, GETALP, Grenoble, France</affiliation>
			</affiliations>
			<titre>Intégration de paramètres lexicaux, syntaxiques et issus du système de traduction automatique pour améliorer l’estimation des mesures de confiance au niveau des mots</titre>
			<type>long</type>
			<pages>43-56</pages>
			<resume>L’estimation des mesures de confiance (MC) au niveau des mots consiste à prédire leur exactitude dans la phrase cible générée par un système de traduction automatique. Ceci permet d’estimer la fiabilité d'une sortie de traduction et de filtrer les segments trop mal traduits pour une post-édition. Nous étudions l’impact sur le calcul des MC de différents paramètres : lexicaux, syntaxiques et issus du système de traduction. Nous présentons la méthode permettant de labelliser automatiquement nos corpus (mot correct ou incorrect), puis le classifieur à base de champs aléatoires conditionnels utilisé pour intégrer les différents paramètres et proposer une classification appropriée des mots. Nous avons effectué des expériences préliminaires, avec l’ensemble des paramètres, où nous mesurons la précision, le rappel et la F-mesure. Finalement nous comparons les résultats avec notre système de référence. Nous obtenons de bons résultats pour la classification des mots considérés comme corrects (F-mesure : 86.7%), et encourageants pour ceux estimés comme mal traduits (F-mesure : 36,8%).</resume>
			<mots_cles>Système de traduction automatique, mesure de confiance, estimation de la confiance, champs aléatoires conditionnels</mots_cles>
			<title>Integrating Lexical, Syntactic and System-based Features to Improve Word Confidence Estimation in SMT</title>
			<abstract>Confidence Estimation at word level is the task of predicting the correct and incorrect words in the target sentence generated by a MT system. It helps to conclude the reliability of a given translation as well as to filter out sentences that are not good enough for post-editing. This paper investigates various types of features to circumvent this issue, including lexical, syntactic and system-based features. A method to set training label for each word in the hypothesis is also presented. A classifier based on conditional random fields (CRF) is employed to integrate features and determine the word’s appropriate label. We conducted our preliminary experiment with all features, tracked precision, recall and F-score and we compared with our baseline system. Experimental results of the full combination of all features yield the very encouraging precision, recall and F-score for Good label (F-score: 86.7%), and acceptable scores for Bad label (F-score: 36.8%).</abstract>
			<keywords>Machine translation, confidence measure, confidence estimation, conditional random fields</keywords>
		</article>
		<article id="recital-2012-long-005" session="Orale 2">
			<auteurs>
				<auteur>
					<nom>Gabriel Bernier-Colborne</nom>
					<email>gabriel.bernier-colborne@umontreal.ca</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Observatoire de linguistique Sens-Texte, Université de Montréal</affiliation>
			</affiliations>
			<titre>Application d’un algorithme de traduction statistique à la normalisation de textos</titre>
			<type>long</type>
			<pages>71-79</pages>
			<resume>Ce travail porte sur l’application d’une technique de traduction statistique au problème de la normalisation de textos. La méthode est basée sur l’algorithme de recherche vorace décrit dans (Langlais et al., 2007). Une première normalisation est générée, puis nous appliquons itérativement une fonction qui génère des nouvelles hypothèses à partir de la normalisation courante, et maximisons une fonction de score. Cette méthode fournit une réduction du taux d’erreurs moyen par phrase de 33 % sur le corpus de test, et une augmentation du score BLEU de plus de 30 %. Nous mettons l’accent sur les fonctions qui génèrent la normalisation initiale et sur les opérations permettant de générer des nouvelles hypothèses.</resume>
			<mots_cles>Traduction statistique, normalisation de textos, algorithme de recherche vorace, modèle de langue</mots_cles>
			<title>Applying a Statistical Machine Translation Algorithm to SMS Text Message Normalization</title>
			<abstract>We report on the application of a statistical machine translation algorithm to the problem of SMS text message normalization. The technique is based on a greedy search algorithm described in (Langlais et al., 2007). A first normalization is generated, then a function that generates new hypotheses is applied iteratively to a current best guess, while maximizing a scoring function. This method leads to a drop in word error rate of 33% on a held-out test set, and a BLEU score gain of over 30%. We focus on the methods of generating the initial normalization and the operations that allow us to generate new hypotheses.</abstract>
			<keywords>Machine translation, SMS, text message, normalization, greedy search algorithm, language model</keywords>
		</article>
		<article id="recital-2012-long-006" session="Orale 2">
			<auteurs>
				<auteur>
					<nom>Marion Baranes</nom>
					<email>marion.baranes@viavoo.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Alpage, INRIA Paris-Rocquencourt &amp; Université Paris Diderot, 175 rue du Chevaleret, 75013 Paris</affiliation>
				<affiliation affiliationId="2">viavoo, 69 rue Danjou, 92100 Boulogne Billancourt</affiliation>
			</affiliations>
			<titre>Vers la correction automatique de textes bruités: Architecture générale et détermination de la langue d’un mot inconnu</titre>
			<type>long</type>
			<pages>95-108</pages>
			<resume>Dans ce papier, nous introduisons le problème que pose la correction orthographique sur des corpus de qualité très dégradée tels que les messages publiés sur les forums, les sites d’avis ou les réseaux sociaux. Nous proposons une première architecture de correction qui a pour objectif d’éviter au maximum la sur-correction. Nous présentons, par ailleurs l’implémentation et les résultats d’un des modules de ce système qui a pour but de détecter si un mot inconnu, dans une phrase de langue connue, est un mot qui appartient à cette langue ou non.</resume>
			<mots_cles>Correction automatique, détection de langue, données produite par l’utilisateur</mots_cles>
			<title>Towards Automatic Spell-Checking of Noisy Texts : General Architecture and Language Identification for Unknown Words</title>
			<abstract>This paper deals with the problem of spell checking on degraded-quality corpora such as blogs, review sites and social networks. We propose a first architecture of correction which aims at reducing overcorrection, and we describe its implementation. We also report and discuss the results obtained thanks to the module that detects whether an unknown word from a sentence in a known language belongs to this language or not.</abstract>
			<keywords>Spelling correction, language identification, User-Generated Content</keywords>
		</article>
		<article id="recital-2012-long-007" session="Orale 2">
			<auteurs>
				<auteur>
					<nom>Carlos Ramisch</nom>
					<email>Carlos.Ramisch@imag.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIG-GETALP, Grenoble, France</affiliation>
				<affiliation affiliationId="2">INF-UFRGS, Porto Alegre, Brésil</affiliation>
			</affiliations>
			<titre>Une plate-forme générique et ouverte pour l’acquisition des expressions polylexicales</titre>
			<type>long</type>
			<pages>137-149</pages>
			<resume>Cet article présente et évalue une plate-forme ouverte et flexible pour l’acquisition automatique d’expressions polylexicales (EPL) à partir des corpus monolingues. Nous commençons par une motivation pratique suivie d’une discussion théorique sur le comportement et les défis posés par les EPL dans les applications de TAL. Ensuite, nous décrivons les modules de notre plate-forme, leur enchaînement et les choix d’implémentation. L’évaluation de la plate-forme a été effectuée à travers une applications : la lexicographie assistée par ordinateur. Cette dernière peut bénéficier de l’acquisition d’EPL puisque les expressions acquises automatiquement à partir des corpus peuvent à la fois accélérer la création et améliorer la qualité et la couverture des ressources lexicales. Les résultats prometteurs encouragent une recherche plus approfondie sur la manière optimale d’intégrer le traitement des EPL dans de nombreuses applications de TAL, notamment dans les systèmes traduction automatique.</resume>
			<mots_cles>Expressions polylexicales, extraction lexicale, lexique, mesures d’association, corpus, lexicographie</mots_cles>
			<title>An Open and Generic Framework for the Acquisition of Multiword Expressions</title>
			<abstract>In this paper, we present and evaluate an open and flexible methodological framework for the automatic acquisition of multiword expressions (MWEs) from monolingual textual corpora. We start with a pratical motivation followed by a theoretical discussion of the behaviour and of the challenges that MWEs pose for NLP applications. Afterwards, we describe the modules of our framework, the overall pipeline and the design choices of the tool implementing the framework. The evaluation of the framework was performed extrinsically based on an application : computerassisted lexicography. This application can benefit from MWE acquisition because the expressions acquired automatically from corpora can both speed up the creation and improve the quality and the coverage of the lexical resources. The promising results of previous and ongoing experiments encourage further investigation about the optimal way to integrate MWE treatment into NLP applications, and particularly into machine translation systems.</abstract>
			<keywords>Multiword expression, lexical extraction, lexicon, association measures, corpus, lexicography</keywords>
		</article>
		<article id="recital-2012-long-008" session="Orale 3">
			<auteurs>
				<auteur>
					<nom>Aurélie Merlo</nom>
					<email>aurelie.merlo@etu.univ-lille3.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">STL UMR 8163, rue du Barreau BP 60149 59653 Villeneuve d’Ascq Cedex</affiliation>
			</affiliations>
			<titre>Système de prédiction de néologismes formels : le cas des N suffixés par -IER dénotant des artefacts</titre>
			<type>long</type>
			<pages>57-70</pages>
			<resume>Nous présentons ici un système de prédiction de néologismes formels avec pour exemple la génération automatique de néologismes nominaux suffixés par -IER dénotant des artefacts (saladier, brassière, thonier). L’objectif de cet article est double. Il s’agira (i) de mettre en évidence les contraintes de la suffixation par -IER afin de les implémenter dans un système de génération morphologique puis (ii) de montrer qu’il est possible de prédire les néologismes formels. Ce système de prédiction permettrait ainsi de compléter automatiquement les lexiques pour le Traitement Automatique des Langues (TAL).</resume>
			<mots_cles>morphologie constructionnelle, néologie, génération morphologique, incomplétude lexicale</mots_cles>
			<title>Prediction Device of Formal Neologisms : the Case of -IER Suffixed Nouns Denoting Artifacts</title>
			<abstract>We’ll introduce here a device that can predict neologisms using an example the automatical generation of nominal neologisms suffixed by -IER denoting artifacts (saladier, brassière, thonier). The aim of this article is double. We will first address the - IER suffixation constraints in order to take them into account on the implementation of our morphological generator. Second, we will describe our method to predict formal neologisms. Such a method will permit to automatically enrich NLP lexicons.</abstract>
			<keywords>constructional morphology, neology, morphological generation, lexical incompleteness</keywords>
		</article>
		<article id="recital-2012-long-009" session="Orale 3">
			<auteurs>
				<auteur>
					<nom>Boris Karlov</nom>
					<email>bnkarlov@gmail.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Ophélie Lacroix</nom>
					<email>ophelie.lacroix@univ-nantes.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université de Tver, 33, rue Zheliabov, 170000, Tver, Russie</affiliation>
				<affiliation affiliationId="2">LINA, 2, rue de la Houssinière 44322 Nantes Cedex 3</affiliation>
			</affiliations>
			<titre>Prémices d’une analyse syntaxique par transition pour des structures de dépendance non-projectives</titre>
			<type>long</type>
			<pages>81-94</pages>
			<resume>L’article présente une extension de l’analyseur traditionnel en dépendances par transitions adapté aux dépendances discontinues et les premiers résultats de son entraînement sur un corpus de structures de dépendances de phrases en français. Les résultats des premières expérimentations vont servir de base pour le choix des traits des configurations de calcul bien adaptés aux dépendances discontinues pour améliorer l’apprentissage des dépendances tête.</resume>
			<mots_cles>analyse syntaxique par transitions, structure de dépendance non-projective, grammaire catégorielle de dépendance</mots_cles>
			<title>Beginnings of a Transition-Based Parsing for Non-Projectives Dependency Structures</title>
			<abstract>This paper presents an extension of the traditional transition-based dependency parser adapted to discontinuous dependencies and the first results of its training on a dependency tree corpus of French. The first experimental results will be useful for the choice of parsing configuration features well adapted to discontinuous dependencies in order to ameliorate learning of head dependencies.</abstract>
			<keywords>transition-based parsing, non-projective dependency structure, dependency categorial grammar</keywords>
		</article>
		<article id="recital-2012-long-010" session="Orale 3">
			<auteurs>
				<auteur>
					<nom>Julie Belião</nom>
					<email>julie@beliao.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LPP - Université Paris Sorbonne Nouvelle (ILPGA) - CNRS - UMR 7018</affiliation>
				<affiliation affiliationId="2">MoDyCo - Université Paris Ouest Nanterre La Défense - CNRS - UMR 7114</affiliation>
			</affiliations>
			<titre>Création d’un multi-arbre à partir d’un texte balisé : l’exemple de l’annotation d’un corpus d’oral spontané</titre>
			<type>long</type>
			<pages>109-122</pages>
			<resume>Dans cette étude, nous nous intéressons au problème de l’analyse d’un corpus annoté de l’oral. Le système d’annotation considéré est celui introduit par l’équipe des syntacticiens du projet Rhapsodie. La principale problématique qui sous-tend un tel projet est que la base écrite sur laquelle on travaille est en réalité une transcription de l’oral, balisée par les annotateurs de manière à délimiter un ensemble de structures arborescentes. Un tel système introduit plusieurs structures, en particulier macro et micro-syntaxiques. Du fait de leur étroite imbrication, il s’est avéré difficile de les analyser de façon indépendante et donc de travailler sur l’aspect macro-syntaxique indépendamment de l’aspect micro-syntaxique. Cependant, peu d’études jusqu’à présent considèrent ces problèmes conjointement et de manière automatisée. Dans ce travail, nous présentons nos efforts en vue de produire un outil de parsing capable de rendre compte à la fois de l’information micro et macro-syntaxique du texte annoté. Pour ce faire, nous proposons une représentation partant de la notion de multi-arbre et nous montrons comment une telle structure peut être générée à partir de l’annotation et utilisée à des fins d’analyse.</resume>
			<mots_cles>Arbres syntaxiques, unité illocutoire, unités rectionnelles, micro-syntaxe, macrosyntaxe, entassement</mots_cles>
			<title>Creating a Multi-Tree from a Tagged Text : Annotating Spoken French</title>
			<abstract>This study focuses on automatic analysis of annotated transcribed speech. The annotation system considered has been recently introduced to address the several limitations of classical syntactic annotations when faced to natural speech transcriptions. It introduces many different components such as embedding, piles, kernels, pre-kernels, discursive markers etc.. All those components are tightly coupled in a complex tree structure and can hardly be considered separately because of their close intrication. Hence, a joint analysis is required but no analysis tool to handle them all together was available yet. In this study, we introduce such an automatic parser of annotated transcriptions of speech and present the corresponding framework based on multi-trees. This framework permits to jointly handle separate aspects of speech such as macro and micro syntactic levels, which are traditionnaly considered separately. Several applications are proposed, including analysis of the transcribed speech by classical parsers designed for written language.</abstract>
			<keywords>Syntactic trees, illocutionary unit, microsyntax, macrosyntax, piles</keywords>
		</article>
		<article id="recital-2012-long-011" session="Orale 3">
			<auteurs>
				<auteur>
					<nom>Noémi Boubel</nom>
					<email>noemi.boubel@uclouvain.be</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">UCLouvain, Cental, Place Blaise Pascal, 1, B-1348 Louvain-la-Neuve, Belgique</affiliation>
			</affiliations>
			<titre>Construction automatique d’un lexique de modifieurs de polarité</titre>
			<type>long</type>
			<pages>123-136</pages>
			<resume>La recherche présentée 1 s’inscrit dans le domaine de la fouille d’opinion, domaine qui consiste principalement à déterminer la polarité d’un texte ou d’une phrase. Dans cette optique, le contexte autour d’un mot polarisé joue un rôle essentiel, car il peut modifier la polarité initiale de ce terme. Nous avons choisi d’approfondir cette question et de détecter précisément ces modifieurs de polarité. Une étude exploratoire, décrite dans des travaux antérieurs, nous a permis d’extraire automatiquement des adverbes qui jouent un rôle sur la polarité des adjectifs auxquels ils sont associés et de préciser leur impact. Nous avons ensuite amélioré le système d’extraction afin de construire automatiquement un lexique de structures lexico-syntaxiques modifiantes associées au type d’impact qu’elles ont sur un terme polarisé. Nous présentons ici le fonctionnement du système actuel ainsi que l’évaluation du lexique obtenu.</resume>
			<mots_cles>fouille d’opinion, modifieurs de valence affective, modifieurs de polarité</mots_cles>
			<title>Automatic Construction of a Contextual Valence Shifters Lexicon</title>
			<abstract>The research presented in this paper takes place in the field of Opinion Mining, which is mainly devoted to assigning a positive or negative label to a text or a sentence. The context of a highly polarized word plays an essential role, as it can modify its original polarity. The present work addresses this issue and focuses on the detection of polarity shifters. In a previous study, we have automatically extracted adverbs impacting the polarity of the adjectives they are associated to and qualified their influence. The extraction system has then been improved to automatically build a lexicon of contextual valence shifters. This lexicon contains lexico-syntactic patterns combined with the type of influence they have on the valence of the polarized item. The purpose of this paper is to show how the current system works and to present the evaluation of the created lexicon.</abstract>
			<keywords>opinion mining, contextual valence shifters</keywords>
		</article>
		<article id="recital-2012-long-012" session="Poster 1">
			<auteurs>
				<auteur>
					<nom>Ahmed Hamdi</nom>
					<email>ahmed.hamdi@lif.univ-mrs.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Aix Marseille Université, LIF-CNRS, Marseille</affiliation>
			</affiliations>
			<titre>Apport de la diacritisation dans l’analyse morphosyntaxique de l’arabe</titre>
			<type>long</type>
			<pages>247-254</pages>
			<resume>Ce travail s’inscrit dans le cadre de l’analyse morphologique et syntaxique automatique de la langue arabe. Nous nous intéressons au traitement de la diacritisation et à son apport pour l’analyse morphologique. En effet, la plupart des analyseurs morphologiques et des étiqueteurs morphosyntaxiques existants ignorent les diacritiques présents dans le texte à analyser et commettent des erreurs qui pourraient être évitées. Dans cet article, nous proposons une méthode qui prend en considération les diacritiques lors de l’analyse, et nous montrons que cette prise en compte permet de diminuer considérablement le taux d’erreur de l’analyse morphologique selon le taux de diacritiques du texte traité.</resume>
			<mots_cles>diacritisation, traitement automatique, analyse morphosyntaxique, langue arabe</mots_cles>
			<title>Apport of Diacritization in Arabic Morpho-Syntactic Analysis</title>
			<abstract>This work is concerned with the automatic morphological and syntactical analysis of the Arabic language. It focuses on diacritization and on its contribution to morphological analysis. Most of existing morphological analyzers and syntactical taggers do not take diacritics into account; as a consequence, they make mistakes that could have been avoided. In this paper, we propose a method which process diacritics. We show that doing so reduces considerably the morphological error rate, depending on the diacritics rate in the input text.</abstract>
			<keywords>diacritization, computer processing, morpho-syntaxic analysis, Arabic language</keywords>
		</article>
		<article id="recital-2012-long-013" session="Poster 1">
			<auteurs>
				<auteur>
					<nom>Alexandre Baudrillart</nom>
					<email>alexandre.baudrillart@u-grenoble3.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université Stendhal Grenoble 3, BP 25, 38040 Grenoble Cedex 9, France</affiliation>
				<affiliation affiliationId="2">Université de Lyon, CNRS INSA-Lyon, UMR5205 F-69621, France</affiliation>
			</affiliations>
			<titre>Extraction d’indicateurs de construction collective de connaissances dans la formation en ligne</titre>
			<type>long</type>
			<pages>337-350</pages>
			<resume>Dans le cadre d’apprentissages humains assistés par des environnements informatiques, les techniques de TAL ne sont que rarement employées ou restreintes à des tâches ou des domaines spécifiques comme l’ALAO (Apprentissage de la Langue Assisté par Ordinateur) où elles sont omniprésentes mais ne concernent que certaines dimensions du TAL. Nous cherchons à explorer les possibilités ou les performances des techniques voire des méthodes de TAL pour des systèmes moins spécifiques dès lors qu’une dimension de réseau et de collectivité est présente. Plus particulièrement, notre objectif est d’obtenir des indicateurs sur la construction collective de connaissances, et ses modalités. Ce papier présente la problématique de notre thèse, son contexte, nos motivations ainsi que nos premières réflexions.</resume>
			<mots_cles>TAL, EIAH, formation en ligne, socio-constructivisme, acquisition des connaissances, apprentissage collaboratif en ligne</mots_cles>
			<title>Collaborative Knowledge Building Indicators Extraction in Distance Learning</title>
			<abstract>Natural Language Processing techniques are still not very much used within the field of Technology Enhanced Learning. They are restricted to specific tasks or domains such as CALL (standing for Computer Assisted Language Learning) in which they are ubiquitous but do not match every linguistic aspect they could process. We are seeking to explore possibilities or performances of thoses techniques for less specific systems including a network or community aspect. More precisely, our goal is to get indicators about collective knowledge building and its modalities. This paper presents the problem and the background of our thesis problem, as well as our motivation and our first reflections.</abstract>
			<keywords>NLP, TEL, distance learning, socio-constructivism, knowledge aquisition, collaboration, CSCL</keywords>
		</article>
		<article id="recital-2012-long-014" session="Poster 1">
			<auteurs>
				<auteur>
					<nom>Andon Tchechmedjiev</nom>
					<email>andon.tchechmedjiev@imag.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIG-GETALP, Laboratoire d’Informatique de Grenoble-Groupe d’Étude pour la Traduction Automatique/Traitement Automatisé des Langues et de la Parole, Université de Grenoble</affiliation>
			</affiliations>
			<titre>État de l’art : mesures de similarité sémantique locales et algorithmes globaux pour la désambiguïsation lexicale à base de connaissances</titre>
			<type>long</type>
			<pages>295-308</pages>
			<resume>Dans cet article, nous présentons les principales méthodes non supervisées à base de connaissances pour la désambiguïsation lexicale. Elles sont composées d’une part de mesures de similarité sémantique locales qui donnent une valeur de proximité entre deux sens de mots et, d’autre part, d’algorithmes globaux qui utilisent les mesures de similarité sémantique locales pour trouver les sens appropriés des mots selon le contexte à l’échelle de la phrase ou du texte.</resume>
			<mots_cles>désambiguïsation lexicale non-supervisée, mesures de similarité sémantique à base de connaissances, algorithmes globaux de propagation de mesures locales</mots_cles>
			<title>State of the art : Local Semantic Similarity Measures and Global Algorithmes for Knowledge-based Word Sense Disambiguation</title>
			<abstract>We present the main methods for unsupervised knowledge-based word sense disambiguation. On the one hand, at the local level, we present semantic similarity measures, which attempt to quantify the semantic proximity between two word senses. On the other hand, at the global level, we present algorithms which use local semantic similarity measures to assign the appropriate senses to words depending on their context, at the scale of a text or of a corpus.</abstract>
			<keywords>unsupervised word sense disambiguation, knowledge-based semantic similarity measures, global algorithms for the propagation of local measures</keywords>
		</article>
		<article id="recital-2012-long-015" session="Poster 1">
			<auteurs>
				<auteur>
					<nom>Arnaud Kirsch</nom>
					<email>arnaud.kirsch@student.uclouvain.be</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">UCL - CenTAL - Place Blaise Pascal 1, Louvain-la-Neuve, 1348</affiliation>
			</affiliations>
			<titre>Compression textuelle sur la base de règles issues d'un corpus de sms</titre>
			<type>long</type>
			<pages>309-322</pages>
			<resume>La présente recherche cherche à réduire la taille de messages textuels sur la base de techniques de compression observées, pour la plupart, dans un corpus de sms. Ce papier explique la méthodologie suivie pour établir des règles de contraction. Il présente ensuite les 33 règles retenues, et illustre les quatre niveaux de compression proposés par deux exemples concrets, produits automatiquement par un premier prototype. Le but de cette recherche n'est donc pas de produire de "l'écrit-sms", mais d'élaborer un procédé de compression capable de produire des textes courts et compréhensibles à partir de n'importe quelle source textuelle en français. Le terme "d'essentialisation" est proposé pour désigner cette approche de réduction textuelle.</resume>
			<mots_cles>résumé automatique, compression de texte, sms, lisibilité, essentialisation</mots_cles>
			<title>Textual Compression Based on Rules Arising from a Corpus of Text Messages</title>
			<abstract>The present research seeks to reduce the size of text messages on the basis of compression techniques observed mostly in a corpus of sms. This paper explains the methodology followed to establish compression rules. It then presents the 33 considered rules, and illustrates the four suggested levels of compression with two practical examples, automatically generated by a first prototype. This research’s main purpose is not to produce "sms-language", but consists in designing a textual compression process able to generate short and understandable texts from any textual source in French. The term of "essentialization" is proposed to describe this approach of textual reduction.</abstract>
			<keywords>summarization, text compression, text messaging, readability, essentialization</keywords>
		</article>
		<article id="recital-2012-long-016" session="Poster 1">
			<auteurs>
				<auteur>
					<nom>Aurélie Joseph</nom>
					<email>joseph.aurelie@gmail.com</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LDI, 99 avenue Jean-Baptiste Clément, 93 Villetaneuse</affiliation>
				<affiliation affiliationId="2">ITESFOT, Parc d’Andron, Le Séquoia, 30470 Aimargues</affiliation>
			</affiliations>
			<titre>Pour un étiquetage automatique des séquences verbales figées : état de l’art et approche transformationnelle</titre>
			<type>long</type>
			<pages>255-266</pages>
			<resume>Cet article présente une approche permettant de reconnaitre automatiquement dans un texte des séquences verbales figées (casser sa pipe, briser la glace, prendre en compte) à partir d’une ressource. Cette ressource décrit chaque séquence en termes de possibilités et de restrictions transformationnelles. En effet, les séquences figées ne le sont pas complètement et nécessitent une description exhaustive afin de ne pas extraire seulement les formes canoniques. Dans un premier temps nous aborderons les approches traditionnelles permettant d’extraire des séquences phraséologiques. Par la suite, nous expliquerons comment est constituée notre ressource et comment celle-ci est utilisée pour un traitement automatique.</resume>
			<mots_cles>séquences verbales figées, reconnaissance automatique, étiquetage, transformations linguistiques, ressources électroniques</mots_cles>
			<title>For an Automatic Fixed Verbal Sequence Tagging: State of the Art and Transformational Approach</title>
			<abstract>This article presents a resource-based method aiming at automatically recognizing fixed verbal sequences in French (i.e casser sa pipe, briser la glace, prendre en compte) inside a text. This resource describes each sequence from the view-point of transformational possibilities and restrictions. Fixed sequences are not totally fixed and an exhaustive description is necessary to not only extract canonical forms. We will first describe some transformational approaches that are able to extract phraseological sequences. The building of the resource will be then addressed followed by our approach to automatically recognize fixed sequences in corpora.</abstract>
			<keywords>fixed verbal sequences, automatic recognition, tagging, linguistical transformations, electronic resources</keywords>
		</article>
		<article id="recital-2012-long-017" session="Poster 1">
			<auteurs>
				<auteur>
					<nom>Céline Battaïa</nom>
					<email>celine.battaia@u-grenoble3.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Groupe de Recherche sur les Enjeux de la Communication (GRESEC), Université Stendhal, Laboratoire Gresec, Grenoble 3</affiliation>
			</affiliations>
			<titre>L’analyse de l’émotion dans les forums de santé</titre>
			<type>long</type>
			<pages>267-280</pages>
			<resume>Les travaux sur l’émotion dans les forums sont nombreux en Linguistique et Psychologie. L’objectif de cette contribution est de proposer une analyse de l’émotion dans les forums de santé selon l’angle des Sciences de l’Information et de la Communication mais également selon une approche interdisciplinaire. Il s’agira ici, d’étudier l’émotion comme un critère de pertinence lorsque des personnes malades effectuent des recherches dans les forums. Ce papier introduit la méthodologie utilisée en traitement automatique de la langue afin de répondre à cette interrogation. Ainsi, le travail présenté abordera l’exploitation d’un corpus de messages de forums, la catégorisation semi-supervisée et l’utilisation du logiciel NooJ pour traiter de manière automatique les données.</resume>
			<mots_cles>émotion, forum de santé, traitement automatique de la langue, désambiguïsation lexicale</mots_cles>
			<title>Analysis of Emotion in Health Fora</title>
			<abstract>Studies about emotion in fora are numerous in Linguistics and Psychology. This contribution approaches this subject from an Information and Communication Sciences point of view, and studies emotion as a criteron of pertinence for patients in a health forum. This paper introduces the empirical step of automatic language processing in order to answer this question, and uses data processing on the corpus of forum messages, semi-supervised categorisation of messages and use of software NooJ for Natural Language Processing.</abstract>
			<keywords>emotion, health forum, automatic language processing, lexical disambiguation</keywords>
		</article>
		<article id="recital-2012-long-018" session="Poster 1">
			<auteurs>
				<auteur>
					<nom>Driss Sadoun</nom>
					<email>driss.sadoun@limsi.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIMSI/CNRS, B.P. 133 91403 Orsay Cedex, France</affiliation>
				<affiliation affiliationId="2">Université Paris-Sud, 91400 Orsay, France</affiliation>
			</affiliations>
			<titre>Peuplement d’une ontologie modélisant le comportement d’un environnement intelligent guidé par l’extraction d’instances de relations</titre>
			<type>long</type>
			<pages>281-294</pages>
			<resume>Nous présentons une approche de peuplement d’ontologie dont le but est de modéliser le comportement de composants logiciels afin de faciliter le passage de descriptions d’exigences en langue naturelle à des spécifications formelles. L’ontologie que nous cherchons à peupler a été conçue à partir des connaissances du domaine de la domotique et est initialisée à partir d’une description de la configuration physique d’un environnement intelligent. Notre méthode est guidée par l’extraction d’instances de relations permettant par là-même d’extraire les instances de concepts liés par ces relations. Nous construisons des règles d’extraction à partir d’éléments issus de l’analyse syntaxique de descriptions de besoins utilisateurs et de ressources terminologiques associées aux concepts et relations de l’ontologie. Notre approche de peuplement se distingue par sa finalité qui n’est pas d’extraire toutes les instances décrivant un domaine mais d’extraire des instances pouvant participer sans conflit à un des multiples fonctionnements décrit par des utilisateurs.</resume>
			<mots_cles>extraction de relations, peuplement d’ontologie, représentation des connaissances</mots_cles>
			<title>Population of an Ontology Modeling the Behavior of an Intelligent Environment Guided by Instance Relation Extractions</title>
			<abstract>We present an approach for ontology population, which aims at modeling the behavior of software components, for enabling a transition from natural language requirements to formal specifications. The ontology was designed based on the knowledge of the domotic domain and is initialized from a description of a physical configuration of an intelligent environment. Our method focuses on extracting relation instances which allows the extraction of concept instances linked by these relations. We built extraction rules using elements coming from syntactic analysis of user need descriptions, semantic and terminological resources linked to the knowledge contained in the ontology. Our approach for ontology population, distinguishes itself by its purpose, which is not to extract all instances describing a domain but to extract instances that can participate without any conflict to one of the mutiple operation decribed by users.</abstract>
			<keywords>relation extraction, ontology population, knowledge representation</keywords>
		</article>
		<article id="recital-2012-long-019" session="Poster 1">
			<auteurs>
				<auteur>
					<nom>Franck Dernoncourt</nom>
					<email>franck.dernoncourt@lip6.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIP6, 4 place Jussieu, 75005 Paris</affiliation>
			</affiliations>
			<titre>De l'utilisation du dialogue naturel pour masquer les QCM au sein des jeux sérieux</titre>
			<type>long</type>
			<pages>323-336</pages>
			<resume>Une des principales faiblesses des jeux sérieux à l'heure actuelle est qu'ils incorporent très souvent des questionnaires à choix multiple (QCM). Or, aucune étude n'a démontré que les QCM sont capables d'évaluer précisément le niveau de compréhension des apprenants. Au contraire, certaines études ont montré expérimentalement que permettre à l'apprenant d'entrer une phrase libre dans le programme au lieu de simplement cocher une réponse dans un QCM rend possible une évaluation beaucoup plus fine des compétences de l'apprenant. Nous proposons donc de concevoir un agent conversationnel capable de comprendre des énoncés en langage naturel dans un cadre sémantique restreint, cadre correspondant au domaine de compétence testé chez l'apprenant. Cette fonctionnalité est destinée à permettre un dialogue naturel avec l'apprenant, en particulier dans le cadre des jeux sérieux. Une telle interaction en langage naturel a pour but de masquer les QCM sous-jacents. Cet article présente notre approche.</resume>
			<mots_cles>Agent conversationnel éducatif, intelligence artificielle, jeu sérieux, questionnaire à choix multiple, système d'évaluation de réponses libres</mots_cles>
			<title>Of the Use of Natural Dialogue to Hide MCQs in Serious Games</title>
			<abstract>A major weakness of serious games at the moment is that they often incorporate multiple choice questionnaires (MCQs). However, no study has demonstrated that MCQs can accurately assess the level of understanding of a learner. On the contrary, some studies have experimentally shown that allowing the learner to input a free-text answer in the program instead of just selecting one answer in an MCQ allows a much finer evaluation of the learner's skills. We therefore propose to design a conversational agent that can understand statements in natural language within a narrow semantic context corresponding to the area of competence on which we assess the learner. This feature is intended to allow a natural dialogue with the learner, especially in the context of serious games. Such interaction in natural language aims to hide the underlying MCQs. This paper presents our approach.</abstract>
			<keywords>Educational conversational agent, artificial intelligence, serious game, multiple-choice questionnaire, automatic assessment of free-text answer</keywords>
		</article>
		<article id="recital-2012-long-020" session="Poster 2">
			<auteurs>
				<auteur>
					<nom>Jihene Jmal</nom>
					<email>fer.jmal_jihene@hotmail.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LARODEC, ISG, Université de Tunis, 2000, Le Bardo, Tunisie</affiliation>
			</affiliations>
			<titre>ResTS : Système de Résumé Automatique des Textes d’Opinions basé sur Twitter et SentiWordNet</titre>
			<type>long</type>
			<pages>233-246</pages>
			<resume>Comme le E-commerce est devenu de plus en plus populaire, le nombre de commentaires des internautes est en croissance constante. Les opinions sur le Web affectent nos choix et nos décisions. Il s’avère alors indispensable de traiter une quantité importante de critiques des clients afin de présenter à l’utilisateur l’information dont il a besoin dans la forme la plus appropriée. Dans cet article, nous présentons ResTS, un nouveau système de résumé automatique de textes d’opinions basé sur les caractéristiques des produits. Notre approche vise à transformer les critiques des utilisateurs en des scores qui mesurent le degré de satisfaction des clients pour un produit donné et pour chacune de ses caractéristiques. Ces scores sont compris entre 0 et 1 et peuvent être utilisés pour la prise de décision. Nous avons étudié les opinions véhiculées par les noms, les adjectifs, les verbes et les adverbes, contrairement aux recherches précédentes qui utilisent essentiellement les adjectifs. Les résultats expérimentaux préliminaires montrent que notre méthode est comparable aux méthodes classiques de résumé automatique basées sur les caractéristiques des produits.</resume>
			<mots_cles>Fouille d’opinion, Classification, Intensité de l’Opinion, Résumé de texte d’opinion, Popularité</mots_cles>
			<title>System of Customer Review Summarization using Twitter and SentiWordNet</title>
			<abstract>As E-commerce is becoming more and more popular, the number of customer reviews raises rapidly. Opinions on the Web affect our choices and decisions. Thus, it is more efficient to automatically process a mixture of reviews and prepare to the customer the required information in an appropriate form. In this paper, we present ResTS, a new system of feature-based opinion summarization. Our approach aims to turn the customer reviews into scores that measure the customer satisfaction for a given product and its features. These scores are between 0 and 1 and can be used for decision making and then help users in their choices. We investigated opinions extracted from nouns, adjectives, verbs and adverbs contrary to previous research which use only adjectives. Experimental results show that our method performs comparably to classic feature-based summarization methods.</abstract>
			<keywords>Opinion mining, Sentiment Classification, Opinion Strength, Feature-based Opinion Summarization, Feature Buzz Summary</keywords>
		</article>
		<article id="recital-2012-long-021" session="Poster 2">
			<auteurs>
				<auteur>
					<nom>Mathieu-Henri Falco</nom>
					<email>falco@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIMSI-CNRS, Université Paris-Sud, 91403 Orsay, France</affiliation>
			</affiliations>
			<titre>Typologie des questions à réponses multiples pour un système de question-réponse</titre>
			<type>long</type>
			<pages>191-204</pages>
			<resume>L’évaluation des systèmes de question-réponse lors des campagnes repose généralement sur la validité d’une réponse individuelle supportée par un passage (question factuelle) ou d’un groupe de réponses toutes contenues dans un même passage (questions listes). Ce cadre évaluatif empêche donc de fournir un ensemble de plusieurs réponses individuelles et ne permet également pas de fournir des réponses provenant de documents différents. Ce recoupement inter-documents peut être necessaire pour construire une réponse composée de plusieurs éléments afin d’être le plus complet possible. De plus une grande majorité de questions formulées au singulier et semblant n’attendre qu’une seule réponse se trouve être des questions possédant plusieurs réponses correctes. Nous présentons ici une typologie des questions à réponses multiples ainsi qu’un aperçu sur les problèmes posés à un système de question-réponse par ce type de question.</resume>
			<mots_cles>question-réponse, questions à réponses multiples, question liste</mots_cles>
			<title>Typology of Multiple Answer Questions for a Question-answering System</title>
			<abstract>The evaluation campaigns of question-answering systems are generally based on the validity of an individual answer supported by a passage (for a factual question) or a group of answers coming all from a same supporting passage (for a list question). This framework does not allow the possibily to answer with a set of answers, nor with answers gathered from several documents. This cross-checking can be needed for building an answer composed of several elements in order to be as accurate as possible. Besides a large majority of questions with a singular form seems to be answered with a single answer whereas they can be satisfied with many. We present here a typology of questions with multiple answers and an overview of problems encountered by a question-answering system with this kind of questions.</abstract>
			<keywords>question-answering, multiple answer questions, list question</keywords>
		</article>
		<article id="recital-2012-long-022" session="Poster 2">
			<auteurs>
				<auteur>
					<nom>Mohamed Hatmi</nom>
					<email>mohamed.hatmi@univ-nantes.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LINA, UMR 6241,Université de Nantes</affiliation>
			</affiliations>
			<titre>Adaptation d’un système de reconnaissance d’entités nommées pour le français à l’anglais à moindre coût</titre>
			<type>long</type>
			<pages>151-161</pages>
			<resume>La portabilité entre les langues des systèmes de reconnaissance d’entités nommées est coûteuse en termes de temps et de connaissances linguistiques requises. L’adaptation des systèmes symboliques souffrent du coût de développement de nouveaux lexiques et de la mise à jour des règles contextuelles. D’un autre côté, l’adaptation des systèmes statistiques se heurtent au problème du coût de préparation d’un nouveau corpus d’apprentissage. Cet article étudie l’intérêt et le coût associé pour porter un système existant de reconnaissance d’entités nommées pour du texte bien formé vers une autre langue. Nous présentons une méthode peu coûteuse pour porter un système symbolique dédié au français vers l’anglais. Pour ce faire, nous avons d’une part traduit automatiquement l’ensemble des lexiques de mots déclencheurs au moyen d’un dictionnaire bilingue. D’autre part, nous avons manuellement modifié quelques règles de manière à respecter la syntaxe de la langue anglaise. Les résultats expérimentaux sont comparés à ceux obtenus avec un système de référence développé pour l’anglais.</resume>
			<mots_cles>Reconnaissance d’entités nommées, approche symbolique, portabilité entre les langues</mots_cles>
			<title>Adapting a French Named Entity Recognition System to English with Minimal Costs</title>
			<abstract>Cross-language portability of Named Entity Recognition systems requires linguistic expertise and needs human effort. Adapting symbolic systems suffers from the cost of developing new lexicons and updating grammar rules. Porting statistical systems on the other hand faces the problem of the high cost of annotation of new training corpus. This paper examines the cost of adapting a rule-based Named Entity Recognition system designed for well-formed text to another language. We present a low-cost method to adapt a French rule-based Named Entity Recognition system to English. We first solve the problem of lexicon adaptation to English by simply translating the French lexical resources. We then get to the task of grammar adaptation by slightly modifying the grammar rules. Experimental results are compared to a state-of-the-art English system.</abstract>
			<keywords>Named entity recognition, symbolic approache, cross-language portability</keywords>
		</article>
		<article id="recital-2012-long-023" session="Poster 2">
			<auteurs>
				<auteur>
					<nom>Monia Ben Mlouka</nom>
					<email>mlouka@irit.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">IRIT -TCI , UMR5505, 31000 Toulouse</affiliation>
			</affiliations>
			<titre>Analyse automatique de discours en langue des signes : Représentation et traitement de l’espace de signation</titre>
			<type>long</type>
			<pages>219-232</pages>
			<resume>En langue des signes, l’espace est utilisé pour localiser et faire référence à certaines entités dont l’emplacement est important pour la compréhension du sens. Dans cet article, nous proposons une représentation informatique de l’espace de signation et les fonctions de création et d’accès associées, afin d’analyser les gestes manuels et non manuels qui contribuent à la localisation et au référencement des signes et de matérialiser leur effet. Nous proposons une approche bi-directionnelle qui se base sur l’analyse de données de capture de mouvement de discours en langue des signes dans le but de caractériser les événements de localisation et de référencement.</resume>
			<mots_cles>Langue des signes, Espace de signation, gestes de pointage, capture de mouvement, suivi du regard</mots_cles>
			<title>Automatic Analysis of Discourse in Sign Language : Signing Space Representation and Processing</title>
			<abstract>In sign language, signing space is used to locate and refer to entities whose locations are important for understanding the meaning. In this paper, we propose a computer-based representation of the signing space and their associated functions. It aims to analyze manual and non-manual gestures, that contribute to locating and referencing signs, and to make real their effect. For that, we propose an approach based on the analysis of motion capture data of entities’ assignment and activation events in the signing space.</abstract>
			<keywords>Sign language, Signing space, pointing gestures, motion capture, gaze tracker</keywords>
		</article>
		<article id="recital-2012-long-024" session="Poster 2">
			<auteurs>
				<auteur>
					<nom>Morgane Marchand</nom>
					<email>morgane.marchand@cea.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">CEA, LIST, Laboratoire Vision et Ingénierie des Contenus, Centre Nano-Innov Saclay, 91191 Gif-sur-Yvette Cedex</affiliation>
				<affiliation affiliationId="2">LIMSI-CNRS, Univ. Paris-Sud, 91403 Orsay Cedex</affiliation>
			</affiliations>
			<titre>État de l’art : l’influence du domaine sur la classification de l’opinion, Dis-moi de quoi tu parles, je te dirai ce que tu penses</titre>
			<type>long</type>
			<pages>177-190</pages>
			<resume>L’intérêt pour la fouille d’opinion s’est développé en même temps que se sont répandus les blogs, forums et autres plate-formes où les internautes peuvent librement exprimer leur opinion. La très grande quantité de données disponibles oblige à avoir recours à des traitements automatiques de fouille d’opinion. Cependant, la manière dont les gens expriment leur avis change selon ce dont ils parlent. Les distributions des mots utilisés sont différentes d’un domaine à l’autre. Aussi, il est très difficile d’obtenir un classifieur d’opinion fonctionnant sur tous les domaines. De plus, on ne peut appliquer sans adaptation sur un domaine cible un classifieur entraîné sur un domaine source différent. L’objet de cet article est de recenser les moyens de résoudre ce problème difficile.</resume>
			<mots_cles>État de l’art, Fouille d’opinion, Multi-domaines, Cross-domaines</mots_cles>
			<title>State of the Art : Influence of Domain on Opinion Classification</title>
			<abstract>The interest in opinion mining has grown concurrently with blogs, forums, and others platforms where the internauts can freely write about their opinion on every topic. As the amounts of available data are increasingly huge, the use of automatic methods for opinion mining becomes imperative. However, sentiment is expressed differently in different domains : words distributions can indeed differ significantly. An effective global opinion classifier is therefore hard to develop. Moreover, a classifier trained on a source domain can’t be used without adaptation on a target domain. This article aims to describe the state-of-the-art methods used to solve this difficult task.</abstract>
			<keywords>State of the art, Opinion mining, Multi-domain, Cross-domain</keywords>
		</article>
		<article id="recital-2012-long-025" session="Poster 2">
			<auteurs>
				<auteur>
					<nom>Mounira Manser</nom>
					<email>manser.mounira@gmail.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIM&amp;BIO (EA3969), Université Paris 13, 93017 Bobigny Cedex, France</affiliation>
			</affiliations>
			<titre>État de l’art sur l’acquisition de relations sémantiques entre termes : contextualisation des relations de synonymie</titre>
			<type>long</type>
			<pages>163-175</pages>
			<resume>L’accès au contenu des textes de spécialité est une tâche difficile à réaliser. Cela nécessite la définition de méthodes automatiques ou semi-automatiques pour identifier des relations sémantiques entre les termes que contiennent ces textes. Nous distinguons les approches de TAL permettant d’acquérir ces relations suivant deux types d’information : la structure interne des termes ou le contexte de ces termes en corpus. Afin d’améliorer la qualité des relations acquises et faciliter leur réutilisation en corpus, nous nous intéressons à la prise en compte du contexte dans une méthode d’acquisition de relations de synonymie basée sur l’utilisation de la structure interne des termes. Nous présentons les résultats d’une expérience préliminaire tenant compte de l’usage des termes dans un corpus biomédical en anglais. Nous donnons quelques pistes de travail pour définir des contraintes sémantiques sur les relations de synonymie acquises.</resume>
			<mots_cles>Acquisition de relations, Synonymie, Relations sémantiques, Terminologie, Domaine Biomédical, Corpus de spécialité</mots_cles>
			<title>State of the Art on the Acquisition of Semantic Relations between Terms : Contextualisation of the Synonymy Relations</title>
			<abstract>Accessing to the context of specialised texts is a crucial but difficult task. It requires automatic or semi-automatic methods dedicated to the identification of semantic relations between terms appearing in the texts. NLP approaches for acquiring semantic relations between terms can be distinguished according to the type of information : the internal structure of the terms and the term context. In order to improve the quality of the acquired synonymy relations and their reusability in other corpora, we aim at taking into account the context into an approach based on the internal structure of the terms. We present the results of a preliminary experiment taking into account the use of the terms in a English biomedical corpora. This experiment will be helpful to add semantic constraints to the already acquired synonymy relations.</abstract>
			<keywords>Relation Acquisition, Synonymy, Semantic Relations, Terminology, Biomedical Domain, Specialised corpora</keywords>
		</article>
		<article id="recital-2012-long-026" session="Poster 2">
			<auteurs>
				<auteur>
					<nom>Noémie-Fleur Sandillon-Rezer</nom>
					<email>nfsr@labri.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">CNRS, Esplanade des Arts et Métiers, 33402 Talence</affiliation>
				<affiliation affiliationId="2">LaBRI, 351 Cours de la Libération, 33405 Talence</affiliation>
			</affiliations>
			<titre>Extraction de PCFG et analyse de phrases pré-typées</titre>
			<type>long</type>
			<pages>205-218</pages>
			<resume>Cet article explique la chaîne de traitement suivie pour extraire une grammaire PCFG à partir du corpus de Paris VII. Dans un premier temps cela nécessite de transformer les arbres syntaxiques du corpus en arbres de dérivation d’une grammaire AB, ce que nous effectuons en utilisant un transducteur d’arbres généralisé ; il faut ensuite extraire de ces arbres une PCFG. Le transducteur d’arbres généralisé est une variation des transducteurs d’arbres classiques et c’est l’extraction de la grammaire à partir des arbres de dérivation qui donnera l’aspect probabiliste à la grammaire. La PCFG extraite est utilisée via l’algorithme CYK pour l’analyse de phrases.</resume>
			<mots_cles>Extraction de grammaire, grammaire de Lambek, PCFG, transducteur d’arbre, algorithme CYK</mots_cles>
			<title>PCFG Extraction and Pre-typed Sentences Analysis</title>
			<abstract>This article explains the way we extract a PCFG from the Paris VII treebank. Firslty, we need to transform the syntactic trees of the corpus into derivation trees. The transformation is done with a generalized tree transducer, a variation of the usual top-down tree transducers, and gives as result some derivation trees for an AB grammar. Secondely, we have to extract a PCFG from the derivation trees. For this, we assume that the derivation trees are representative of the grammar. The extracted grammar is used, via the CYK algorithm, for sentence analysis.</abstract>
			<keywords>Grammar Extraction, Lambek grammar, PCFG, tree transducer, CYK Algorithm</keywords>
		</article>
	</articles>
</conference>