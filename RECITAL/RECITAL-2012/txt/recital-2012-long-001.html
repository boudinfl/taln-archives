<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
"http://www.w3.org/TR/html4/loose.dtd">
<html><head><title>Segmentation non supervis&#233;e : le cas du mandarin</title>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
</head>
<body>
<div style="page-break-before:always; page-break-after:always"><div><p>Actes de la conf&#233;rence conjointe JEP-TALN-RECITAL 2012, volume 3: RECITAL, pages 1&#8211;13,
Grenoble, 4 au 8 juin 2012. c&#169;2012 ATALA &amp; AFCP
</p>
<p>Segmentation non supervis&#233;e :
le cas du mandarin
</p>
<p>Pierre Magistry
Alpage, INRIA Paris&#8211;Rocquencourt &amp; Universit&#233; Paris Diderot
</p>
<p>R&#201;SUM&#201;
Dans cet article, nous pr&#233;sentons un syst&#232;me de segmentation non supervis&#233;e que nous &#233;valuons
sur des donn&#233;es en mandarin. Notre travail s&#8217;inspire de l&#8217;hypoth&#232;se de Harris (1955) et suit
Kempe (1999) et Tanaka-Ishii (2005) en se basant sur la reformulation de l&#8217;hypoth&#232;se en termes
de variation de l&#8217;entropie de branchement. Celle-ci se r&#233;v&#232;le &#234;tre un bon indicateur des fronti&#232;res
des unit&#233;s linguistiques. Nous am&#233;liorons le syst&#232;me de (Jin et Tanaka-Ishii, 2006) en ajoutant
une &#233;tape de normalisation qui nous permet de reformuler la fa&#231;on dont sont prises les d&#233;cisions
de segmentation en ayant recours &#224; la programmation dynamique. Ceci nous permet de supprimer
la plupart des seuils de leur mod&#232;le tout en obtenant de meilleurs r&#233;sultats, qui se placent au
niveau de l&#8217;&#233;tat de l&#8217;art (Wang et al., 2011) avec un syst&#232;me plus simple que ces derniers. Nous
pr&#233;sentons une &#233;valuation des r&#233;sultats sur plusieurs corpus diffus&#233;s pour le Chinese Word
Segmentation bake-off II (Emerson, 2005) et d&#233;taillons la borne sup&#233;rieure que l&#8217;on peut esp&#233;rer
atteindre avec une m&#233;thode non-supervis&#233;e. Pour cela nous utilisons ZPAR en apprentissage
crois&#233; (Zhang et Clark, 2010) comme sugg&#233;r&#233; dans (Huang et Zhao, 2007; Zhao et Kit, 2008)
</p>
<p>ABSTRACT
Unsupervized Word Segmentation
</p>
<p>In this paper, we present an unsupervised segmentation system tested on Mandarine Chinese.
Following Harris&#8217;s Hypothesis in Kempe (1999) and Tanaka-Ishii (2005) reformulation, we base
our work on the Variation of Branching Entropy. We improve on (Jin et Tanaka-Ishii, 2006) by
adding normalization and Viterbi-decoding. This enables us to remove most of the thresholds and
parameters from their model and to reach near state-of-the-art results (Wang et al., 2011) with
a simpler system. We provide evaluation on different corpora available from the Segmentation
bake-off II (Emerson, 2005) and define a more precise topline for the task using cross-trained
supervised system available off-the-shelf (Zhang et Clark, 2010; Zhao et Kit, 2008; Huang et
Zhao, 2007)
</p>
<p>MOTS-CL&#201;S : Apprentissage non-supervis&#233;, segmentation, chinois, mandarin.
</p>
<p>KEYWORDS: Unsupervized machine learning, segmentation, Mandarin Chinese.
</p>
<p>1</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>1 Introduction
</p>
<p>Pour la plupart des langues utilisant l&#8217;alphabet latin, un d&#233;coupage sur les espaces est une bonne
approximation d&#8217;une segmentation en unit&#233;s lexicales. L&#8217;&#233;criture chinoise en revanche ne d&#233;limite
pas ces unit&#233;s par la typographie. Seules les marques de ponctuation indiquent une partie des
fronti&#232;res entre les unit&#233;s lexicales qui peuvent &#234;tre form&#233;es d&#8217;un ou plusieurs caract&#232;res chinois.
L&#8217;&#233;tape de tokenisation, pr&#233;alable &#224; beaucoup de syst&#232;mes d&#8217;analyse automatique est de ce fait
plus d&#233;licate. Pour les langues sans caract&#232;re d&#8217;espacement ou &#233;quivalent, on parle d&#8217;&#233;tape de
segmentation en mot.
</p>
<p>De nombreux syst&#232;mes de segmentation par apprentissage supervis&#233; ont &#233;t&#233; propos&#233;s mais ils
requi&#232;rent des corpus segment&#233;s manuellement. Ceux-ci sont souvent sp&#233;cifiques &#224; un genre, un
domaine ou une vari&#233;t&#233; de mandarin et en l&#8217;absence d&#8217;un consensus sur la d&#233;finition de ce qu&#8217;est
un &#171; mot &#187;, ils suivent des guides d&#8217;annotations qui divergent.
</p>
<p>Les syst&#232;mes supervis&#233;s atteignent aujourd&#8217;hui des r&#233;sultats satisfaisants lorsque le corpus
appropri&#233; pour l&#8217;entra&#238;nement est disponible. Cependant, si l&#8217;on veut faire face &#224; une plus
grande diversit&#233; en genres et en domaines ou r&#233;pondre &#224; des questions plus th&#233;orique sur la
caract&#233;risation formelle des unit&#233;s de langue, s&#8217;int&#233;resser aux approches non-supervis&#233;es nous
semble n&#233;cessaire.
</p>
<p>De plus, il est important de souligner que le syst&#232;me que nous pr&#233;sentons ici n&#8217;est pas sp&#233;ci-
fique au mandarin, ni &#224; la segmentation de s&#233;quence de caract&#232;res chinois en &#171; mots &#187;. Des
exp&#233;rimentations sur d&#8217;autres langues et &#224; partir d&#8217;autres unit&#233;s initiales (lettre, phon&#232;me, mot
orthographique) sont en cours et donnent des r&#233;sultats prometteurs. En l&#8217;&#233;tat actuel de l&#8217;avan-
cement de nos travaux, nous ne pouvons fournir d&#8217;&#233;valuation compl&#232;te que pour le mandarin.
Nous limitons donc notre pr&#233;sentation &#224; cette langue.
</p>
<p>Cette article est organis&#233; ainsi : apr&#232;s une courte pr&#233;sentation de l&#8217;&#233;tat de l&#8217;art &#224; la section 2,
nous d&#233;taillons les m&#233;thodes et les difficult&#233;s d&#8217;&#233;valuation des syst&#232;mes de segmentation non-
supervis&#233;s &#224; la section 3. Les sections 4 &#224; 5 pr&#233;sentent le fonctionnement notre syst&#232;me et la
section 6 les r&#233;sultats obtenus.
</p>
<p>2 &#201;tat de l&#8217;art
</p>
<p>Les syst&#232;mes de segmentation non supervis&#233;s reposent g&#233;n&#233;ralement sur un des trois types de
mesures suivantes ou une combinaison des trois : le niveau de coh&#233;sion des unit&#233;s obtenues (par
exemple en utilisant l&#8217;information mutuelle, comme dans (Sproat et Shih, 1990)) ; le degr&#233; de
s&#233;paration des unit&#233;s obtenues (par exemple la diversit&#233; des contextes, (Feng et al., 2004)) ou la
probabilit&#233; d&#8217;une segmentation &#233;tant donn&#233;e une cha&#238;ne (Goldwater et al., 2006; Mochihashi
et al., 2009).
</p>
<p>Dans un article publi&#233; r&#233;cemment, Wang et al. (2011) pr&#233;sentent une m&#233;thode baptisee &#171; Eva-
luation, Selection, Adjustment. &#187; (ESA). Cette m&#233;thode combine coh&#233;sion et s&#233;paration en une
mesure &#224; maximiser sur l&#8217;ensemble d&#8217;une s&#233;quence. Ils utilisent ensuite les r&#233;sultats de leur
syst&#232;me pour en modifier les param&#232;tres (essentiellement les comptes de n-grammes) et r&#233;it&#233;rer
le processus 10 &#224; 30 fois. Ils obtiennent ainsi les meilleurs r&#233;sultats actuels en segmentation
non-supervis&#233;e du mandarin.
</p>
<p>2</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Les principaux inconv&#233;nients de l&#8217;approche ESA sont d&#8217;une part le fait qu&#8217;il faille it&#233;rer le processus
sur le corpus environ 10 fois avant d&#8217;atteindre des niveaux de performance satisfaisants, et d&#8217;autre
part la n&#233;cessit&#233; d&#8217;avoir &#224; fixer le param&#232;tre de couplage entre mesure de coh&#233;sion et mesure
de s&#233;paration. Empiriquement, on constate une corr&#233;lation entre ce param&#232;tre et la taille du
corpus, mais cette corr&#233;lation d&#233;pend de la fa&#231;on dont sont trait&#233;s les caract&#232;res latins et les
chiffres arabes au cours des pr&#233;traitements. De plus, calculer cette corr&#233;lation et choisir la valeur
optimale du param&#232;tre en question (ce que les auteurs appellent le proper exponent) n&#233;cessite un
corpus segment&#233; &#224; la main, ce qui contredit le caract&#232;re non-supervis&#233; de l&#8217;approche. Toutefois,
si parmi les diff&#233;rents types de pr&#233;traitements pour lesquels ESA a &#233;t&#233; &#233;valu&#233; on se r&#233;f&#232;re aux
configurations qui se rapprochent des n&#244;tres, les r&#233;sultats de Wang et al. (2011) avec leur
approche ESA se situent tous aux alentours de 0,80 de f-mesure sur les mots.
</p>
<p>L&#8217;approche plus ancienne de Jin et Tanaka-Ishii (2006) ne repose que sur une mesure de
s&#233;paration, elle-m&#234;me directement insipir&#233;e par une hypoth&#232;se linguistique formul&#233;e par Harris
(1955). Reformul&#233;e au moyen de la notion d&#8217;entropie de branchement (Branching Entropy, BE)
par Tanaka-Ishii (2005) en suivant les travaux de Kempe (1999), cette hypoth&#232;se peut s&#8217;&#233;noncer
comme suit : si les s&#233;quences de graph&#232;mes, phon&#232;mes, ou autres produites par l&#8217;homme
&#233;taient al&#233;atoires, on s&#8217;attendrait &#224; ce que l&#8217;entropie de branchement d&#8217;une s&#233;quence (estim&#233;e
&#224; partir de n-grammes en corpus) d&#233;croisse lorsque la longueur de la s&#233;quence cro&#238;t. Ainsi, la
variation de l&#8217;entropie de branchement (Variation of the Branching Entropy, VBE) devrait &#234;tre
syst&#233;matiquement n&#233;gative. Lorsque l&#8217;on observe au contraire une VBE positive, l&#8217;hypoth&#232;se de
Harris conduit &#224; conclure que l&#8217;on se situe &#224; une fronti&#232;re d&#8217;unit&#233;s linguistiques. C&#8217;est sur la base
de cette hypoth&#232;se que Jin et Tanaka-Ishii (2006) proposent un syst&#232;me qui segmente d&#232;s que
la BE cro&#238;t (c&#8217;est-&#224;-dire que la VBE est positive) ou lorsqu&#8217;elle atteint un certain maximum. Les
auteurs fixent la longueur maximale des s&#233;quences calcul&#233;es &#224; 6 et lisent le corpus de gauche &#224;
droite et de droite &#224; gauche. &#192; chaque intervalle entre deux caract&#232;res, ils peuvent donc observer
jusqu&#8217;&#224; 12 valeurs desquelles ils conservent le maximum.
</p>
<p>Le principal inconv&#233;nient de l&#8217;approche de Jin et Tanaka-Ishii (2006) est que les d&#233;cisions de
segmentation sont prises tr&#232;s localement 1 et ne d&#233;pendent pas des segmentations voisines.
De plus, ce syst&#232;me repose lui aussi sur des param&#232;tres, et notamment le seuil sur la VBE au
dessus duquel le syst&#232;me d&#233;cide de segmenter (dans leur syst&#232;me, il y a segmentation d&#232;s lors
que VBE&#8805; 0). En th&#233;orie, on pourrait d&#233;cider de segmenter d&#232;s lors que la BE ne d&#233;cro&#238;t pas
suffisamment, ou &#224; l&#8217;inverse ne segmenter que si la VBE est non seulement positive mais m&#234;me
au dessus d&#8217;un certain seuil non nul. &#192; cet &#233;gard, placer le seuil &#224; la valeur 0 peut &#234;tre consid&#233;r&#233;
comme une valeur par d&#233;faut, mais reste un param&#232;tre adaptable. Enfin, Jin et Tanaka-Ishii ne
prennent pas en compte le fait que la VBE pour un n-gramme n&#8217;est pas forc&#233;ment comparable a
priori avec la VBE pour un m-gramme d&#232;s lors que n 6= m : une normalisation est ici n&#233;cessaire,
comme le sugg&#232;rent notamment Cohen et al. (2002).
</p>
<p>Faute de place, nous ne d&#233;crirons pas ici d&#8217;autres syst&#232;mes que ceux de Wang et al. (2011) et de
Jin et Tanaka-Ishii (2006). Un &#233;tat de l&#8217;art plus exhaustif peut &#234;tre trouv&#233; dans les articles de
(Zhao et Kit, 2008) et de (Wang et al., 2011).
</p>
<p>Dans cet article, nous montrons que l&#8217;on peut corriger les inconv&#233;nients du mod&#232;le de Jin et
Tanaka-Ishii (2006) et atteindre des niveaux de performance comparables &#224; ceux de l&#8217;&#233;tat de
l&#8217;art, c&#8217;est-&#224;-dire de Wang et al. (2011), le tout avec un syst&#232;me plus simple.
</p>
<p>1. Dans sa th&#232;se, Jin utilise l&#8217;auto-apprentissage et le paradigme de la minimum description length (MDL) pour pallier
&#224; ce probl&#232;me.
</p>
<p>3</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>mots caract&#232;res
Corpus en tout diff&#233;rents en tout diff&#233;rents
</p>
<p>Academia Sinica (AS) 5 449 698 141 340 8 368 050 6 117
City University of Hong Kong (CITYU) 1 455 629 69085 2 403 355 4 923
</p>
<p>Peking University (PKU) 1 109 947 55303 1 826 448 4 698
Microsoft Research (MSR) 2 368 391 88119 4 050 469 5 167
</p>
<p>TABLE 1 &#8211; Taille des corpus utilis&#233;s
</p>
<p>3 Probl&#232;mes d&#8217;&#233;valuation
</p>
<p>Dans cet article, afin de pouvoir nous comparer au syst&#232;me de Wang et al. (2011), nous nous
&#233;valuons sur les corpus du second Bakeoff international de segmentation du chinois (Second
International Chinese Word Segmentation Bakeoff, Emerson, 2005). Ces corpus couvrent 4 guides
de segmentation diff&#233;rents, d&#233;velopp&#233;s au sein de 4 institutions distinctes : l&#8217;Academia Sinica
(AS), la City University de Hong-Kong (CITYU), l&#8217;universit&#233; de P&#233;kin (PKU) et Microsoft Research
(MSR).
</p>
<p>Des informations sur la taille des corpus sont donn&#233;es au tableau 1. Dans le cadre du Bakeoff, Les
d&#233;tails du contenu n&#8217;&#233;taient pas connus des participants. Mais on peut noter que le corpus de
PKU est constitu&#233; d&#8217;extraits du Quotidien du Peuple, journal de P&#233;kin. Le corpus de CITYU est
extrait du LIVAC (T&#8217;sou et al., 1997), aussi constitu&#233; de textes de presse, mais d&#8217;origines plus
vari&#233;es. Le projet du LIVAC cherchant &#224; rendre compte des variantes g&#233;ographique du mandarin,
il inclut des articles provenant de P&#233;kin, Hong-Kong, Singapour ou Ta&#239;wan. Le corpus de l&#8217;AS
est un corpus &#233;quilibr&#233;, qui rend essentiellement compte de la variante du mandarin utilis&#233;e &#224;
Ta&#239;wan. Aucune description du contenu du corpus de MSR n&#8217;est disponible &#224; notre connaissance.
</p>
<p>L&#8217;&#233;valuation de syst&#232;mes non-supervis&#233;s est une probl&#233;matique en soi. Un consensus sur une
d&#233;finition pr&#233;cise de la notion de mot restant difficile &#224; atteindre, diff&#233;rents guides d&#8217;annotation
pour la segmentation en mots ont &#233;t&#233; propos&#233;s et appliqu&#233;s &#224; divers corpus. L&#8217;&#233;valuation de
syst&#232;mes de segmentation supervis&#233;s peut &#234;tre r&#233;alis&#233;e sur n&#8217;importe quel corpus, ind&#233;pendam-
ment du guide d&#8217;annotation sous-jacent, pour peu que les donn&#233;es d&#8217;entra&#238;nement et les donn&#233;es
d&#8217;&#233;valuation soient coh&#233;rentes. Cependant, pour les syst&#232;mes non-supervis&#233;s, il n&#8217;y a aucune
raison d&#8217;obtenir des r&#233;sultats plus proches de l&#8217;un des guides existants que d&#8217;un autre, plut&#244;t que
des r&#233;sultats se situant quelque part entre les diff&#233;rents guides. Huang et Zhao (2007) propose
d&#8217;utiliser l&#8217;entra&#238;nement et l&#8217;&#233;valuation crois&#233;s de syst&#232;mes de segmentation supervis&#233;s pour avoir
un ordre d&#8217;id&#233;e sur le taux de d&#233;saccord entre guides d&#8217;annotation. L&#8217;id&#233;e est donc d&#8217;entra&#238;ner puis
d&#8217;&#233;valuer un syst&#232;me supervis&#233; sur deux corpus respectant deux guides d&#8217;annotation distincts,
et d&#8217;en tirer une approximation de leur d&#233;saccord. C&#8217;est &#233;galement un moyen d&#8217;estimer une
borne sup&#233;rieur de ce que l&#8217;on est en droit d&#8217;attendre de la part d&#8217;un syst&#232;me non-supervis&#233;, qui
n&#8217;a pas de raison d&#8217;&#234;tre plus proche d&#8217;un guide d&#8217;annotation que ne le sont les autres guides
existants (Zhao et Kit, 2008). Nous avons reproduit ce type de mesures sur nos 4 corpus au
moyen du syst&#232;me supervis&#233; ZPAR (Zhang et Clark, 2010), et nous avons trouv&#233; une coh&#233;rence
moyenne similaire &#224; celle obtenue par Huang et Zhao (2007), de l&#8217;ordre de seulement 0,84
(f-mesure), qui sera donc notre topline. Par ailleurs, il est g&#233;n&#233;ralement admis que segmenter
chaque caract&#232;re individuellement est une baseline raisonnable, puisque pr&#232;s de la moiti&#233; des
mots-formes dans un corpus segment&#233; &#224; la main sont des unigrammes. Une telle baseline obtient
</p>
<p>4</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>un f-score d&#8217;environ 0, 35.
</p>
<p>Ces &#233;valuations globales peuvent &#234;tre raffin&#233;es en d&#233;composant les r&#233;sultats en fonction de
la longueur des mots. Les mots de longueurs diff&#233;rentes ont en effet des distributions tr&#232;s
dissemblables. Les &#233;valuations par longueur donnent les r&#233;sultats suivants : sur les unigrammes,
les f-scores se situent entre 0,81 et 0,90, similaires aux r&#233;sultats globaux. Les r&#233;sultats pour
les bigrammes sont l&#233;g&#232;rement meilleurs (0,85&#8211;0,92), mais bien plus bas sur les trigrammes,
descendant entre 0, 59 et 0, 79. Or, dans un texte en mandarin, la majorit&#233; des occurrences sont
des mots unigrammes ou bigrammes, mais le lexique est principalement compos&#233; de bigrammes
et de trigrammes. Ceci vient du fait que les unigrammes sont souvent des mots grammaticaux &#224;
haute fr&#233;quence, alors que les trigrammes sont souvent le r&#233;sultat d&#8217;affixations plus ou moins
productives. Pour cette raison, les r&#233;sultats uniquement calcul&#233;s sur les occurrences ne p&#226;tissent
pas &#233;norm&#233;ment de mauvaises performances sur les trigrammes, m&#234;me si une proportion
significative du lexique est ainsi mal trait&#233;e.
</p>
<p>Une autre difficult&#233; concernant l&#8217;&#233;valuation et la comparaison entre syst&#232;mes non-supervis&#233;s est
de prendre en compte de fa&#231;on &#233;quitable les pr&#233;traitements et les connaissances a priori qui sont
fournies aux syst&#232;mes. Par exemple, Wang et al. (2011) utilise diff&#233;rents niveaux de pr&#233;traitement
(qu&#8217;ils appellent settings et que nous appelerons &#171; configurations &#187;). Dans les configurations 1
et 2, Wang et al. (2011) essayent de ne pas se reposer sur la ponctuation et l&#8217;encodage des
caract&#232;res (notamment la distinction entre caract&#232;res chinois et latins). Cependant, ils optimisent
ind&#233;pendamment leur param&#232;tre pour chaque configuration. Nous consid&#233;rons donc que leur
syst&#232;me prend en compte le niveau de pr&#233;traitement qui est effectu&#233; sur les caract&#232;res latins
et les chiffres romains, et sait donc &#224; quoi s&#8217;attendre en la mati&#232;re. Dans leur configuration 3,
les auteurs ajoutent la connaissance de la ponctuation en tant que fronti&#232;res de mots, et leur
configuration 4 ajoute &#224; cela un pr&#233;traitement des caract&#232;res latins et des chiffres arabes, ce qui
conduit &#224; des r&#233;sultats plus significatifs, moins questionnables et plus convaincants.
</p>
<p>Nous sommes plus int&#233;ress&#233;s par une r&#233;duction du travail humain que par le d&#233;ploiement &#224; tout
prix d&#8217;un syst&#232;me strictement non-supervis&#233;. Nous ne pensons donc pas utile de nous emp&#234;cher
de proc&#233;der &#224; quelques pr&#233;traitements simples, tels que ceux discut&#233;s ci-dessus : d&#233;tection des
ponctuations, des caract&#232;res latins et des chiffres arabes 2. C&#8217;est la raison pour laquelle nos
exp&#233;riences correspondent aux configurations 3 et 4 de Wang et al. (2011), et c&#8217;est &#224; elles que
nous nous comparons, en appliquant notre syst&#232;me aux m&#234;mes corpus.
</p>
<p>4 Variation de l&#8217;entropie de branchement
</p>
<p>4.1 Formulation
</p>
<p>Notre syst&#232;me repose sur l&#8217;hypoth&#232;se de Harris (1955) et sa reformulation par Kempe (1999) et
Tanaka-Ishii (2005). D&#233;finissons &#224; pr&#233;sent les notions sous-jacentes &#224; notre syst&#232;me.
</p>
<p>Soit un n-gramme x0..n = x0..1 x1..2 . . . xn&#8722;1..n dont le contexte droit &#967;&#8594; contient tous les caract&#232;res
observ&#233;s &#224; sa droite dans le corpus. Nous d&#233;finissons son entropie de branchement droite (Right
</p>
<p>2. De simples expressions r&#233;guli&#232;res peuvent aussi &#234;tre envisag&#233;es pour traiter les cas non-ambigus de nombres et de
dates utilisant les sinogrammes
</p>
<p>5</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Branching Entropy, RBE) comme suit :
</p>
<p>h&#8594;(x0..n) = H(&#967;&#8594; | x0..n)
= &#8722;&#8721;
</p>
<p>x&#8712;&#967;&#8594;
P(x | x0..n) log P(x | x0..n).
</p>
<p>L&#8217;entropie de branchement gauche (Left Branching Entropy, LBE) est d&#233;finie de fa&#231;on sym&#233;trique :
si l&#8217;on note &#967;&#8592; le contexte gauche de x0..n, sa LBE est d&#233;finie par :
</p>
<p>h&#8592;(x0..n) = H(&#967;&#8592; | x0..n).
</p>
<p>La RBE h&#8594;(x0..n) peut &#234;tre consid&#233;r&#233;e comme l&#8217;entropie de branchement (BE) de x0..n au cours
d&#8217;un parcours de gauche &#224; droite, alors que la LBE est la BE de x0..n au cours d&#8217;un parcours de
droite &#224; gauche.
</p>
<p>&#192; partir, d&#8217;une part, de h&#8594;(x0..n) et h&#8594;(x0..n&#8722;1), et d&#8217;autre part de h&#8592;(x0..n) et h&#8592;(x1..n), nous
d&#233;finissons la variation de l&#8217;entropie de branchement (Variation of Branching Entropy, VBE) dans
les deux directions comme suit :
</p>
<p>&#948;h&#8594;(x0..n) = h&#8594;(x0..n)&#8722; h&#8594;(x0..n&#8722;1)
&#948;h&#8592;(x0..n) = h&#8592;(x0..n)&#8722; h&#8592;(x1..n).
</p>
<p>4.2 Observations interm&#233;diaires
</p>
<p>Apr&#232;s avoir reproduit les exp&#233;riences de Jin et Tanaka-Ishii (2006), nous avons effectu&#233; une s&#233;rie
d&#8217;observations des valeurs prises par l&#8217;entropie de branchement et ses variations. Certaines de
ces observations ont motiv&#233; les modifications apport&#233;es au mod&#232;le que nous pr&#233;senterons &#224; la
section suivante. Dans cette section, nous pr&#233;sentons les plus pertinentes de ces observations.
Pour des questions de place disponible et pour &#233;viter la redondance, les graphiques de cette
section sont produits &#224; partir du corpus de PKU uniquement.
</p>
<p>4.2.1 Confirmation de l&#8217;hypoth&#232;se de Harris
</p>
<p>Dans un premier temps, nous allons confirmer l&#8217;hypoth&#232;se de Harris sur nos donn&#233;es. Pour cela
nous nous limitons &#224; l&#8217;observation de la fronti&#232;re droite des bigrammes de notre corpus (le choix
des bigrammes &#233;tant motiv&#233; par leur repr&#233;sentativit&#233; tant en nombre d&#8217;occurences en corpus
qu&#8217;en nombre d&#8217;entr&#233;es dans le lexique). Cette valeur est donc calcul&#233;e pour chaque bigramme
observ&#233; au moins deux fois dans le corpus. On affiche ensuite l&#8217;ensemble de ces valeurs sous
forme d&#8217;une courbe de densit&#233; qui donne ainsi la r&#233;partition des valeurs prises par la variation
d&#8217;entropie (c&#8217;est &#224; dire les &#8217;&#948;h&#8594;(x0..2)). On distingue ensuite de l&#8217;ensemble de tous les bigrammes
ceux qui sont consid&#233;r&#233;s comme des mots par l&#8217;annotation manuelle de ceux qui ne le sont
pas. Le r&#233;sultat est pr&#233;sent&#233; figure 4.1.1. On observe que les mots valides (qui forment une tr&#232;s
petite proportion de l&#8217;ensemble des bigrammes observ&#233;s) se d&#233;marquent bien par une variation
d&#8217;entropie plus grande &#224; leur fronti&#232;re droite. Cependant, on observe aussi une zone relativement
importante de confusion, qui confirme la n&#233;cessit&#233; de chercher la segmentation optimale d&#8217;une
phrase, et non simplement les fronti&#232;res de fa&#231;on ind&#233;pendantes les unes des autres.
</p>
<p>6</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>FIGURE 1 &#8211; Distribution des valeurs prises par la VBE &#224; droite des bigrammes
</p>
<p>On peut ensuite g&#233;n&#233;raliser ce mode d&#8217;observation. En se demandant notamment si les valeurs de
VBE &#224; l&#8217;int&#233;rieur d&#8217;un mot sont aussi discriminantes que les valeurs qui en marquent les fronti&#232;res.
Pour diff&#233;rentes tailles de n-grammes mais en fixant n on va effectuer la m&#234;me mesure et la m&#234;me
distinction entre mots et non-mots que pr&#233;c&#233;demment mais cette fois-ci en prenant en compte
les deux fronti&#232;res gauche et droite ainsi que les valeurs observ&#233;es &#224; chaque inter-caract&#232;re &#224;
l&#8217;int&#233;rieur du n-gramme et dans chacun des deux sens de lecture possibles. Pour avoir une vue
d&#8217;ensemble, on affiche ces r&#233;sultats deux &#224; deux sous la forme de courbes de niveaux. Les r&#233;sultats
pour les trigrammes sont pr&#233;sent&#233;s figure 2. On observe que des diff&#233;rentes valeurs observ&#233;es
les plus discriminantes sont sans conteste &#171; gauche 1 &#187; et &#171; droite 3 &#187;, c&#8217;est &#224; dire les entropies
aux fronti&#232;res. Il appara&#238;t vraisemblable que la structure interne des unit&#233;s morphologiquement
complexes affectent la VBE, ce qui rend les valeurs internes plus difficiles &#224; utiliser en pratique,
contrairement &#224; l&#8217;hypoth&#232;se suivie initialement dans (Magistry et Sagot, 2011).
</p>
<p>4.2.2 Limites de la formulation par entropie
</p>
<p>En pr&#233;sence de donn&#233;es al&#233;atoires, on s&#8217;attend &#224; ce que l&#8217;entropie de branchement diminue &#224;
mesure que la longueur de la cha&#238;ne consid&#233;r&#233;e grandit. L&#8217;hypoth&#232;se de Harris nous fait dire que
pour une cha&#238;ne donn&#233;e en langue naturelle, la variation de l&#8217;entropie de branchement lorsque
l&#8217;on atteint une fronti&#232;re sera anormalement &#233;lev&#233;e. Par ailleurs, on observe bien que pour des
cha&#238;nes de m&#234;me longueur, la variation de l&#8217;entropie de branchement aux fronti&#232;res permet, au
moins en partie, de distinguer les mots des non-mots. Toutefois, rien ne permet d&#8217;affirmer que
cette distinction reste observable si l&#8217;on s&#8217;int&#233;resse &#224; des cha&#238;nes de longueurs diff&#233;rentes.
</p>
<p>Nous avons donc cherch&#233; &#224; observer les valeurs prises par la VBE aux fronti&#232;res des n-grammes
pour diff&#233;rentes valeurs de n. La figure 4.1.2 pr&#233;sente ces valeurs pour les uni- bi- et trigrammes.
Elle montre qu&#8217;une normalisation ou qu&#8217;un recentrage de ces valeurs est n&#233;cessaire pour les
rendre comparables.
</p>
<p>7</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>FIGURE 2 &#8211; Courbes de niveaux repr&#233;sentant la distribution des variations de l&#8217;entropie de
branchement internes et aux fronti&#232;res des trigrammes. La partie inf&#233;rieur gauche du graphique
(en noir) correspond &#224; toutes les occurences de trigramme confondues, tandis que la partie
sup&#233;rieur droite distingue les mots (en bleu) des non-mots (en rouge). les dimensions indiquent
qu&#8217;on s&#8217;int&#233;resse &#224; la variation d&#8217;entropie &#224; gauche ou &#224; droite d&#8217;un des trois caract&#232;res qui
forment le trigramme, toujours en partant de l&#8217;extr&#233;mit&#233; oppos&#233;e du trigramme.
</p>
<p>8</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>FIGURE 3 &#8211; VBE observ&#233;e &#224; droite des mots de diff&#233;rentes longueurs
</p>
<p>4.2.3 Cons&#233;quences
</p>
<p>Nous venons d&#8217;observer d&#8217;une part que l&#8217;information que l&#8217;on calcule aux fronti&#232;res des mots est
de loin la plus pertinente pour distinguer les unit&#233;s lexicales recherch&#233;es, et d&#8217;autre part que si
l&#8217;on cherche &#224; comparer ces valeurs pour des unit&#233;s de longueurs diff&#233;rentes, une normalisation
est n&#233;cessaire. Ces observations motivent et permettent des modifications importantes du mod&#232;le
de segmentation : les valeurs pertinentes sont moins nombreuses et ind&#233;pendantes du contexte.
Elles sont donc pr&#233;calculables pour l&#8217;ensemble des n-grammes observ&#233;s. De plus il nous faut les
normaliser et chercher, pour une s&#233;quence de caract&#232;re donn&#233;e, &#224; trouver la segmentation qui
maximise ces mesures.
</p>
<p>Ces modifications sont int&#233;gr&#233;es &#224; notre algorithme de d&#233;codage pr&#233;sent&#233; &#224; la section suivante.
</p>
<p>5 Algorithme de segmentation propos&#233;
</p>
<p>Les VBE ne sont pas directement comparables pour des cha&#238;nes de longeurs diff&#233;rentes, et doivent
&#234;tre normalis&#233;es. Nous recentrons les VBE, pour chaque longueur de cha&#238;ne, autour de 0. Pour
cela, nous retranchons simplement &#224; la VBE d&#8217;une cha&#238;ne de longeur k la moyenne des VBE de
toutes cha&#238;nes de m&#234;me longueur. Nous notons &#948;&#771;h&#8594;(x) et &#948;&#771;h&#8592;(x) les VBE normalis&#233;es. Pour
simplifier, nous ne donnons que la d&#233;finition de &#948;&#771;h&#8594;(x), celle de &#948;&#771;h&#8592;(x) en &#233;tant sym&#233;trique :
pour chaque longueur k et chaque k-gramme x tel que len(x) = k, &#948;&#771;h&#8594;(x) = &#948;h&#8594;(x)&#8722;&#181;&#8594;,k,
o&#249; &#181;&#8594;,k est la moyenne des valeurs de &#948;h&#8594;(y) de tous les k-grammes y .
Il est important de noter que nous utilisons et normalisons la VBE et non l&#8217;entropie de bran-
chement elle m&#234;me. En effet, utiliser la BE contredirait l&#8217;hypoth&#232;se de Harris, puisque l&#8217;on ne
</p>
<p>9</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>s&#8217;attendrait plus &#224; ce que l&#8217;on ait h&#771;(x0..n)&lt; h&#771;(x0..n&#8722;1) aux endroits qui ne sont pas des fronti&#232;res
de mots. De nombreux travaux utilisent pourtant la BE, normalis&#233;e ou non, et non la VBE, et
obtiennent des r&#233;sultats inf&#233;rieurs &#224; l&#8217;&#233;tat de l&#8217;art (Cohen et al., 2002).
</p>
<p>Si nous ne basons nos d&#233;cisions de segmentations que sur la VBE aux fronti&#232;res de mots, chercher
la meilleure segmentation d&#8217;une phrase revient &#224; chercher en celle-ci les mots pr&#233;sentant les
&#171; meilleures fronti&#232;res &#187;. Cette qualit&#233; des fronti&#232;res rejoint intuitivement (et empiriquement
dans une certainement mesure, voir Magistry et Sagot (2011)) la notion d&#8217;autonomie syntaxique
des unit&#233;s qui composent la phrase. En termes de VBE, on peut d&#233;finir la mesure d&#8217;autonomie
d&#8217;un n-gramme comme a(x) = &#948;&#771;&#8592;h(x) + &#948;&#771;h&#8594;(x).
On peut alors dire que plus l&#8217;autonomie a(x) d&#8217;un n-gram x est grande, plus x est susceptible
d&#8217;&#234;tre un mot.
</p>
<p>Avec cette mesure d&#8217;autonomie, on peut reformuler le probl&#232;me de la segmentation d&#8217;une phrase
comme la recherche du d&#233;coupage qui maximise l&#8217;autonomie des mots qu&#8217;il d&#233;limite. Pour une
s&#233;quence de caract&#232;res s, si on note Seg(s) l&#8217;ensemble de toutes les segmentations possibles, on
cherche :
</p>
<p>arg max
W&#8712;Seg(s)
</p>
<p>&#8721;
wi&#8712;W
</p>
<p>a(wi)&#215; len(wi)
</p>
<p>O&#249; W est une segmentation d&#233;limitant les mots w0w1 . . .wm et len(wi) est la longueur d&#8217;un mot
wi , utilis&#233;e ici pour rendre comparables des segmentations aboutissant &#224; des nombres de mots
diff&#233;rents. Multiplier la mesure d&#8217;autonomie par la longueur du mot revient &#224; attribuer un score
aux caract&#232;res, qui contrairement aux mots sont en nombre constant entre les segmentations
possibles d&#8217;une m&#234;me cha&#238;ne.
</p>
<p>Cette segmentation optimale en terme de VBEs est calculable simplement par programmation
dynamique.
</p>
<p>5.1 D&#233;codage par programmation dynamique
</p>
<p>Notre mesure d&#8217;autonomie d&#8217;un n-gramme donn&#233; est calcul&#233;e &#224; partir de tous ses contextes
observ&#233;s en corpus. Mais une fois calcul&#233;e, elle ne d&#233;pend pas d&#8217;un contexte particulier. Elle ne
d&#233;pend notamment pas du contexte observ&#233; sp&#233;cifiquement au sein d&#8217;une cha&#238;ne en cours de
segmentation.
</p>
<p>Pour une cha&#238;ne donn&#233;e u0..k de longueur k, il y a 2
k&#8722;1 segmentations possibles. Mais l&#8217;on peut
</p>
<p>remarquer que si l&#8217;on conna&#238;t la meilleure segmentation pour celle-ci et pour ses pr&#233;fixes u0..n,
n&#8804; k, consid&#233;rer un caract&#232;re suppl&#233;mentaire et segmenter la cha&#238;ne u0..k+1 ne n&#233;cessite que de
consid&#233;rer les appartenances possibles du caract&#232;re suppl&#233;mentaire (le k+ 1i&#232;me). &#201;tant donn&#233;
que nos mots sont contraints &#224; &#234;tre des s&#233;quences continues (ins&#233;cables) de caract&#232;res, il nous
suffit donc de consid&#233;rer les cas suivants :
</p>
<p>1. l&#8217;ajout du k+1i&#232;me caract&#232;re comme un mot de longueur 1 &#224; fin de la meilleure segmentation
de u0..k
</p>
<p>2. pour chaque prefixe u0..n de u0..k (avec 0&lt; n&lt; k), le cas o&#249; le k+1i&#232;me caract&#232;re est int&#233;gr&#233;
&#224; un mot unique de longueur k&#8722; n qui vient s&#8217;ajouter &#224; la fin de la meilleur segmentation
de u0..n
</p>
<p>10</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>3. toute la cha&#238;ne u0..k+1 est un mot unique de longueur k+ 1.
les cas 1 et 3 ci-dessus peuvent &#234;tre vus comme les bornes du second cas, qui est le cas g&#233;n&#233;ral
si on prend 0 &#8804; n &#8804; k. Ils sont explicit&#233;s ici pour plus de clart&#233;. Ce constat nous permet de
reformuler la meilleure segmentation de u0..k+1 &#224; partir des meilleures segmentations de ses
pr&#233;fixes comme suit :
</p>
<p>arg max
W&#8712;Seg(u0..k+1)
</p>
<p>= argmax
V&#8712;&#8899;n&#8804;k&#8899;S&#8712;Seg(u0..n) S&#8746;{un..k+1}
</p>
<p>&#8721;
wi&#8712;V
</p>
<p>a(wi)&#215; len(wi)
</p>
<p>Cette reformulation permet une programmation dynamique qui garde en m&#233;moire les meilleures
segmentations des Seg(u0..n) et qui nous am&#232;ne &#224; ne consid&#233;rer que
</p>
<p>&#8721;k
n=2 n segmentations au
</p>
<p>lieu des 2k&#8722;1 th&#233;oriquement possibles. Cette m&#233;thode am&#232;ne un surco&#251;t n&#233;gligeable pour k &lt; 5
et devient de plus en plus int&#233;ressante &#224; mesure que k grandit &#224; partir de k &#8805; 5, ce qui est le plus
souvent le cas.
</p>
<p>6 Resultats et discussion
</p>
<p>Nous avons &#233;valu&#233; notre syst&#232;me sur les quatre corpus du Bakeoff 2 et dans les configurations 2
et 3 telles que d&#233;crites &#224; la Section 3. Nous comparons notre syst&#232;me (nVBE) aux r&#233;sultats de
Wang et al. (2011) ainsi qu&#8217;&#224; notre propre impl&#233;mentation de la strat&#233;gie &#171; couper si une BE
est croissante &#187;. avec des variations de BE calcul&#233;es dans les deux sens de lecture et pour toutes
les longueurs de n-grammes, 1 &#8804; n &#8804; 6. (&#224; chaque position entre deux caract&#232;res, au plus 12
variations sont calcul&#233;es, on segmente si au moins l&#8217;une d&#8217;entre elles est positive). Les r&#233;sultats
sont donn&#233;s Table 2. Les r&#233;sultats filtr&#233;s par longueur de mot se trouvent Table 3.
</p>
<p>Comme nous pouvons le voir, notre syst&#232;me est nettement meilleur que la strat&#233;gie de coupure
sur accroissement de BE et obtient des scores comparables &#224; ceux de ESA sans n&#233;cessiter de
nombreuses it&#233;rations ni recourir &#224; un param&#232;tre.
</p>
<p>Cela montre qu&#8217;on peut atteindre un bon niveau de segmentation en se basant uniquement
sur une mesure de s&#233;paration. Lorsque celle-ci est maximis&#233;e pour une s&#233;quence donn&#233;e, il est
raisonnable de penser qu&#8217;il existe une corr&#233;lation avec une &#233;ventuelle mesure de coh&#233;sion. Il
n&#8217;est ainsi plus n&#233;cessaire d&#8217;avoir &#224; trouver comment combiner les deux mesures.
</p>
<p>On peut noter par ailleurs que l&#8217;&#233;volution de nos r&#233;sultats en fonction de la longueur des mots
semble en accord avec la coh&#233;rence des guides d&#8217;annotation.
</p>
<p>Nous ne pouvons fournir ici une analyse qualitative d&#233;taill&#233;e des r&#233;sultats. Signalons tout de
m&#234;me que les erreurs observ&#233;es nous semblent de m&#234;me nature que nos observations ant&#233;rieurs
(Magistry et Sagot (2011)) et que celles pr&#233;sentes de la th&#232;se de Jin. De nombreuses erreurs
sont aussi li&#233;es aux dates et nombres &#233;crits en chinois. Elles pourraient &#234;tre &#233;cart&#233;es lors du
pr&#233;traitement. D&#8217;autres erreurs concernent des morph&#232;mes grammaticaux (&#171; mots vides &#187;)de
haute fr&#233;quence et des affixes particuli&#232;rement productifs. Ces erreurs sont susceptibles de
questionner les linguistes. Elle pourraient &#234;tre corrig&#233;es en post-traitement par l&#8217;introduction de
connaissances linguistiques.
</p>
<p>Contrairement aux mots &#171; pleins &#187;, ces mots vides ou morph&#232;mes grammaticaux forment des
classes ferm&#233;es. De ce fait, introduire la connaissance linguistique n&#233;cessaire &#224; leur bon traitement
</p>
<p>11</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>System AS CITYU PKU MSR
</p>
<p>Setting 3
ESA bas 0.729 0.795 0.781 0.768
ESA haut 0.782 0.816 0.795 0.802
</p>
<p>nVBE 0.758 0.775 0.781 0.798
Setting 4
</p>
<p>VBE&gt; 0 0.63 0.640 0.703 0.713
ESA bas 0.732 0.809 0.784 0.784
ESA haut 0.786 0.829 0.800 0.818
</p>
<p>nVBE 0.766 0.767 0.800 0.813
</p>
<p>TABLE 2 &#8211; &#201;valuation sur les donn&#233;es du Bakeoff 2, suivant les configurations d&#233;finies dans
Wang et al. (2011). &#171; Bas &#187; et &#171; haut &#187; indiquent l&#8217;&#233;tendue des r&#233;sultats obtenus par ESA pour
diff&#233;rentes valeurs du param&#232;tre du mod&#232;le. VBE&gt; 0 segmente d&#232;s qu&#8217;une BE est croissante.
nVBE correspond &#224; la maximisation de la variation d&#8217;entropie de branchement normalis&#233;e aux
fronti&#232;res.
</p>
<p>corpus global unigrammes bigrammes trigrammes
</p>
<p>AS 0.766 0.741 0.828 0.494
CITYU 0.767 0.739 0.834 0.555
PKU 0.800 0.789 0.855 0.451
MSR 0.813 0.823 0.856 0.482
</p>
<p>TABLE 3 &#8211; Resultats par longueur de mots (nVBE, configuration 4)
</p>
<p>dans un syst&#232;me de segmentation ne n&#233;cessite qu&#8217;une quantit&#233; de travail limit&#233;e. Recourir &#224;
un syst&#232;me d&#8217;apprentissage supervis&#233; ou symbolique pour traiter les classes de mots ferm&#233;es et
d&#233;l&#233;guer la gestion des classes ouvertes &#224; un syst&#232;me non-supervis&#233; nous semble &#234;tre une voie
prometteuse et linguistiquement pertinente.
</p>
<p>Remarquons enfin que notre syst&#232;me obtient de bien meilleurs r&#233;sultats sur les corpus de MSR et
de PKU. Le corpus PKU &#233;tant le plus petit et AS le plus grand, la taille du corpus d&#8217;entra&#238;nement
ne semble donc pas jouer &#224; elle seule un r&#244;le primordial pour expliquer les diff&#233;rences. En
revanche, PKU est le corpus le plus homog&#232;ne, il contient des articles qui sont tous issus du m&#234;me
journal. Le corpus AS au contraire est &#233;quilibr&#233; et pr&#233;sente une forte h&#233;t&#233;rog&#233;n&#233;it&#233; des contenus.
Le corpus CITYU est presque aussi petit que PKU mais contient des articles issus de journaux
repr&#233;sentatifs de diff&#233;rentes vari&#233;t&#233;s de mandarin, on peut donc s&#8217;attendre &#224; ce que son contenu
pr&#233;sente de grandes variations. Il semblerait donc que l&#8217;homog&#233;n&#233;it&#233; des donn&#233;es d&#8217;entra&#238;nement
soit aussi importante sinon plus que la quantit&#233; des donn&#233;es utilis&#233;es pour le bon fonctionnement
du syst&#232;me pr&#233;sent&#233; ici. Cette observation devra &#234;tre v&#233;rifi&#233;e dans de prochains travaux. Si elle
se confirmait, une &#233;tape de classification automatique des donn&#233;es d&#8217;entra&#238;nement pourrait &#234;tre
un pr&#233;traitement essentiel.
</p>
<p>12</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>R&#233;f&#233;rences
</p>
<p>COHEN, P., HEERINGA, B. et ADAMS, N. (2002). An unsupervised algorithm for segmenting
categorical timeseries into episodes. Pattern Detection and Discovery, pages 117&#8211;133.
</p>
<p>EMERSON, T. (2005). The second international chinese word segmentation bakeoff. In Proceedings
of the Fourth SIGHAN Workshop on Chinese Language Processing, volume 133.
</p>
<p>FENG, H., CHEN, K., DENG, X. et ZHENG, W. (2004). Accessor variety criteria for chinese word
extraction. Computational Linguistics, 30(1):75&#8211;93.
</p>
<p>GOLDWATER, S., GRIFFITHS, T. et JOHNSON, M. (2006). Contextual dependencies in unsupervised
word segmentation. In Proceedings of the 21st International Conference on Computational
Linguistics and the 44th annual meeting of the Association for Computational Linguistics, pages
673&#8211;680.
</p>
<p>HARRIS, Z. S. (1955). From phoneme to morpheme. Language, 31(2):190&#8211;222.
</p>
<p>HUANG, C. et ZHAO, H. (2007). &#20013;&#25991;&#20998;&#35789;&#21313;&#24180;&#22238;&#39038;(chinese word segmentation : A decade
review). Journal of Chinese Information Processing, 21(3):8&#8211;20.
</p>
<p>JIN, Z. et TANAKA-ISHII, K. (2006). Unsupervised segmentation of chinese text by use of branching
entropy. In Proceedings of the COLING/ACL on Main conference poster sessions, pages 428&#8211;435.
</p>
<p>KEMPE, A. (1999). Experiments in unsupervised entropy-based corpus segmentation. In Workshop
of EACL in Computational Natural Language Learning, page 713.
</p>
<p>MAGISTRY, P. et SAGOT, B. (2011). Segmentation et induction de lexique non-supervis&#233;es du
mandarin. In Actes de TALN 2011, Montpellier, pages 333&#8211;344.
</p>
<p>MOCHIHASHI, D., YAMADA, T. et UEDA, N. (2009). Bayesian unsupervised word segmentation with
nested Pitman-Yor language modeling. In Proceedings of the Joint Conference of the 47th Annual
Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of
the AFNLP : Volume 1-Volume 1, pages 100&#8211;108.
</p>
<p>SPROAT, R. W. et SHIH, C. (1990). A statistical method for finding word boundaries in chinese
text. Computer Processing of Chinese and Oriental Languages, 4(4):336&#8211;351.
</p>
<p>TANAKA-ISHII, K. (2005). Entropy as an indicator of context boundaries : An experiment using a
web search engine. In International Joint Conference on Natural Language Processing (IJCNLP-
2005), pages 93&#8211;105.
</p>
<p>T&#8217;SOU, B., LIN, H., LIU, G., CHAN, T., HU, J., CHEW, C. et TSE, J. (1997). A synchronous
chinese language corpus from different speech communities : Construction and applications.
Computational Linguistics and Chinese Language Processing, 2(1):91&#8211;104.
</p>
<p>WANG, H., ZHU, J., TANG, S. et FAN, X. (2011). A new unsupervised approach to word segmenta-
tion. Computational Linguistics, 37(3):421&#8211;454.
</p>
<p>ZHANG, Y. et CLARK, S. (2010). A fast decoder for joint word segmentation and POS-tagging
using a single discriminative model. In Proceedings of the 2010 Conference on Empirical Methods
in Natural Language Processing, pages 843&#8211;852.
</p>
<p>ZHAO, H. et KIT, C. (2008). An empirical comparison of goodness measures for unsupervised
chinese word segmentation with a unified framework. In The Third International Joint Conference
on Natural Language Processing (IJCNLP-2008), Hyderabad, India.
</p>
<p>13</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div>
</div></div>
</body></html>