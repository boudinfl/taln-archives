Segmentation non supervisée :
le cas du mandarin

Pierre Magistry
Alpage, INRIA Paris—Rocquencourt 8: Université Paris Diderot

RESUME
Dans cet article, nous présentons un systéme de segmentation non supervisée que nous évaluons
sur des données en mandarin. Notre travail s’inspire de l’hypothese de Harris (1955) et suit
Kempe (1999) et Tanaka-Ishii (2005) en se basant sur la reformulation de l’hypo1hése en termes
de variation de l’entropie de branchement. Celle-ci se révéle étre un bon indicateur des frontiéres
des unités linguistiques. Nous améliorons le systéme de (Jin et Tanaka-Ishii, 2006) en ajoutant
une étape de normalisation qui nous permet de reformuler la fagon dont sont prises les décisions
de segmentation en ayant recours 5 la programmation dynamique. Ceci nous permet de supprimer
la plupart des seuils de leur modele tout en obtenant de meilleurs résultats, qui se placent au
niveau de l’état de l’art (Wang et al., 2011) avec un systéme plus simple que ces derniers. Nous
présentons une évaluation des résultats sur plusieurs corpus diffusés pour le Chinese Word
Segmentation bake-ojfll (Emerson, 2005) et détaillons la borne supérieure que l’on peut espérer
atteindre avec une méthode non-supervisée. Pour cela nous utilisons ZPAR en apprentissage
croisé (Zhang et Clark, 2010) comme suggéré dans (Huang et Zhao, 2007; Zhao et Kit, 2008)

AB STRACT
Unsupervized Word Segmentation

In this paper, we present an unsupervised segmentation system tested on Mandarine Chinese.
Following Harris’s Hypothesis in Kempe (1999) and Tanaka-Ishii (2005) reformulation, we base
our work on the Variation of Branching Entropy. We improve on (Jin et Tanaka-Ishii, 2006) by
adding normalization and Viterbi-decoding. This enables us to remove most of the thresholds and
parameters from their model and to reach near state-of-the-art results (Wang et al., 2011) with
a simpler system. We provide evaluation on different corpora available from the Segmentation
bake-off II (Emerson, 2005) and deﬁne a more precise topline for the task using cross-trained
supervised system available off-the-shelf (Zhang et Clark, 2010; Zhao et Kit, 2008; Huang et
Zhao, 2007)

MOTS-CLES : Apprentissage non-supervisé, segmentation, chinois, mandarin.

KEYWORDS: Unsupervized machine learning, segmentation, Mandarin Chinese.

Actes de la conference conjointe JEP-TALN-RECITAL 2012, volume 3: RECITAL, pages 1-13,
Grenoble, 4 an 8 juin 2012. ©2012 ATAI.A 8: AFCP

1 Introduction

Pour la plupart des langues utilisant l’alphabet latin, un découpage sur les espaces est une bonne
approximation d’une segmentation en unités lexicales. L’écriture chinoise en revanche ne délimite
pas ces unités par la typographie. Seules les marques de ponctuation indiquent une partie des
frontiéres entre les unités lexicales qui peuvent étre formées d’un ou plusieurs caractéres chinois.
L’étape de tokenisation, préalable ‘a beaucoup de systémes d’analyse automatique est de ce fait
plus délicate. Pour les langues sans caractére d’espacement ou équivalent, on parle d’étape de
segmentation en mot.

De nombreux systémes de segmentation par apprentissage supervisé ont été proposés mais ils
requiérent des corpus segmentés manuellement. Ceux-ci sont souvent spéciﬁques a un genre, un
domaine ou une variété de mandarin et en l’absence d’un consensus sur la déﬁnition de ce qu’est
un « mot », ils suivent des guides d’annotations qui divergent.

Les systémes supervisés atteignent aujourd’hui des résultats satisfaisants lorsque le corpus
approprié pour l’entrainement est disponible. Cependant, si l’on veut faire face ‘a une plus
grande diversité en genres et en domaines ou répondre a des questions plus théorique sur la
caractérisation formelle des unités de langue, s’intéresser aux approches non-supervisées nous
semble nécessaire.

De plus, il est important de souligner que le systéme que nous présentons ici n’est pas spéci-
fique au mandarin, ni ‘a la segmentation de séquence de caractéres chinois en « mots ». Des
expérimentations sur d’autres langues et 21 partir d’autres unités initiales (lettre, phoneme, mot
orthographique) sont en cours et donnent des résultats prometteurs. En l’état actuel de l’avan-
cement de nos travaux, nous ne pouvons fournir d’évaluation complete que pour le mandarin.
Nous limitons donc notre présentation 5 cette langue.

Cette article est organisé ainsi : aprés une courte présentation de l’état de l’art a la section 2,
nous détaillons les méthodes et les difficultés d’évaluation des systémes de segmentation non-
supervisés ‘a la section 3. Les sections 4 a 5 présentent le fonctionnement notre systéme et la
section 6 les résultats obtenus.

2 Etat de l’art

Les systémes de segmentation non supervisés reposent généralement sur un des trois types de
mesures suivantes ou une combinaison des trois : le niveau de cohésion des unités obtenues (par
exemple en utilisant l’information mutuelle, comme dans (Sproat et Shi.h, 1990)) ; le degré de
séparation des unités obtenues (par exemple la diversité des contextes, (Feng et al., 2004)) ou la
probabilité d’une segmentation étant donnée une chaine (Goldwater et al., 2006; Mochihashi
et al., 2009).

Dans un article publié récemment, Wang et aL (2011) présentent une méthode baptisee << Eva-
luation, Selection, Adjustment. >> (ESA). Cette méthode combine cohésion et séparation en une
mesure a maximiser sur l’ensemble d’une séquence. Ils utilisent ensuite les résultats de leur
systeme pour en modiﬁer les parametres (essentiellement les comptes de n-grammes) et réitérer
le processus 10 a 30 fois. Ils obtiennent ainsi les meilleurs résultats actuels en segmentation
non-supervisée du mandarin.

3. toute la chaine uo"k+1 est un mot unique de longueur k + 1.
les cas 1 et 3 ci-dessus peuvent étre vus comme les bornes du second cas, qui est le cas général
si on prend 0 S n S k. Ils sont explicités ici pour plus de clarté. Ce constat nous permet de
reformuler la meilleure segmentation de uo__k+1 ‘a partir des meilleures segmentations de ses
préﬁxes comme suit :

argmax = argmax Z a(wi) X len(wi)
WeSeS("o..k+1) VeunskUseSe8{"o"n)SU{u,,__k+1} wiev

Cette reformulation permet une programmation dynamique qui garde en mémoire les meilleures
segmentations des Seg(uo_.,l) et qui nous améne ‘a ne considérer que 25:2 n segmentations au
lieu des 2"‘1 théoriquement possibles. Cette méthode améne un surcoﬁt négligeable pour k < 5
et devient de plus en plus intéressante a mesure que k grandit a partir de k 2 5, ce qui est le plus
souvent le cas.

6 Resultats et discussion

Nous avons évalué notre systéme sur les quatre corpus du Bakeojf 2 et dans les conﬁgurations 2
et 3 telles que décrites ‘a la Section 3. Nous comparons notre systéme (nVBE) aux résultats de
Wang et al. (201 1) ainsi qu’a notre propre implémentation de la stratégie << couper si une BE
est croissante ». avec des variations de BE calculées dans les deux sens de lecture et pour toutes
les longueurs de n-grammes, 1 S n S 6. (a chaque position entre deux caractéres, au plus 12
variations sont calculées, on segmente si au moins l’une d’entre elles est positive). Les résultats
sont donnés Table 2. les résultats ﬁltrés par longueur de mot se trouvent Table 3.

Come nous pouvons le voir, notre systéme est nettement meilleur que la stratégie de coupure
sur accroissement de BE et obtient des scores comparables ‘a ceux de ESA sans nécessiter de
nombreuses itérations ni recourir a un paramétre.

Cela montre qu’on peut atteindre un bon niveau de segmentation en se basant uniquement
sur une mesure de séparation. 1.orsque celle-ci est maximisée pour une séquence donnée, il est
raisonnable de penser qu’il existe une corrélation avec une éventuelle mesure de cohésion. Il
n’est ainsi plus nécessaire d’avoir a trouver comment combiner les deux mesures.

On peut noter par ailleurs que l’évoluu'on de nos résultats en fonction de la longueur des mots
semble en accord avec la cohérence des guides d’annotation.

Nous ne pouvons fournir ici une analyse qualitative détaillée des résultats. Signalons tout de
méme que les erreurs observées nous semblent de méme nature que nos observations antérieurs
(Magistry et Sagot (2011)) et que celles présentes de la these de Jin. De nombreuses erreurs
sont aussi liées aux dates et nombres écrits en chinois. Elles pourraient étre écartées lors du
prétraitement. D’autres erreurs concement des morphémes grammaticaux (<< mots vides >>)de
haute fréquence et des affixes parﬁculiérement productifs. Ces erreurs sont susceptibles de
questionner les linguistes. Elle pourraient étre corrigées en post-traitement par l’introduction de
connaissances linguistiques.

Contrairement aux mots << pleins », ces mots vides ou morphémes grammaticaux forment des
classes fermées. De ce fait, introduire la connaissance linguistique nécessaire a leur bon traitement

11

System | AS | CI'I'YU | PKU | MSR
Setting 3

ESA bas 0.729 0.795 0.781 0.768
ESA haut 0.782 0.816 0.795 0.802
nVBE 0.758 0.775 0.781 0.798
Setting 4
VBE> 0 0.63 0.640 0.703 0.713
ESA bas 0.732 0.809 0.784 0.784
ESA haut 0.786 0.829 0.800 0.818
nV'BE 0.766 0.767 0.800 0.813

TABLE 2 — Evaluation sur les données du Bakeoff 2, suivant les conﬁgurations déﬁnies dans
Wang et al. (2011). « Bas » et « haut » indiquent l’étendue des résultats obtenus par ESA pour
différentes valeurs du parametre du modele. VBE> 0 segmente des qu’une BE est croissante.
nVBE correspond ‘a la maximisation de la variation d’entropie de branchement normalisée aux
frontiéres.

corpus global unigrammes bigrammes trigrammes
AS 0.766 0.741 0.828 0.494
CI'I'YU 0.767 0.739 0.834 0.555
PKU 0.800 0.789 0.855 0.45 1
MSR 0.813 0.823 0.856 0.482

TABLE 3 — Resultats par longueur de mots (nVBE, conﬁguration 4)

dans un systeme de segmentation ne nécessite qu’une quantité de travail limitée. Recourir a
un systéme d’apprentissage supervisé ou symbolique pour traiter les classes de mots fermées et
déléguer la gestion des classes ouvertes ‘a un systeme non-supervisé nous semble étre une voie
prometteuse et linguistiquement pertinente.

Remarquons enﬁn que notre systéme obtient de bien meilleurs résultats sur les corpus de MSR et
de PKU. Le corpus PKU étant le plus petit et AS le plus grand, la taille du corpus d’entrainement
ne semble donc pas jouer ‘a elle seule un role primordial pour expliquer les différences. En
revanche, PKU est le corpus le plus homogéne, il contient des articles qui sont tous issus du méme

journal. Le corpus AS au contraire est équilibré et présente une forte hétérogénéité des contenus.

Le corpus CITYU est presque aussi petit que PKU mais contient des articles issus de journaux
représentatifs de différentes variétés de mandarin, on peut donc s’attendre a ce que son contenu
présente de grandes variations. Il semblerait donc que l’homogénéité des données d’entrainement
soit aussi importante sinon plus que la quantité des données utilisées pour le bon fonctionnement
du systéme présenté ici. Cette observation devra étre vériﬁée dans de prochains travaux. Si elle
se conﬁrmait, une étape de classiﬁcation automatique des données d’entrainement pourrait étre
un prétraitement essentiel.

12

Références

CoHEN, R, HEERINGA, B. et ADAMS, N. (2002). An unsupervised algorithm for segmenting
categorical timeseries into episodes. Pattern Detection and Discovery, pages 117-133.

EMERSoN, T. (2005). The second international chinese word segmentation bakeoff. In Proceedings
of the Fourth SIGHAN Workshop on Chinese Language Processing, volume 133.

FENG, H., CHEN, K., DENG, X. et ZHENG, W. (2004). Accessor variety criteria for chinese word
extraction. Computational Linguistics, 30(1) :75—93.

GOLDWATER, S., GRIFFITHS, T. et JoHNsoN, M. (2006). Contextual dependencies in unsupervised
word segmentation. In Proceedings of the 21st International Conference on Computational
Linguistics and the 44th annual meeting of the Association for Computational Linguistics, pages
673-680.

HARRIS, Z. S. (1955). From phoneme to morpheme. Language, 31(2):190—222.

HUANG, C. et ZHAo, H. (2007). "F'3‘C5}W‘l‘5l3|Ellm(chinese word segmentation : A decade
review). Journal of Chinese Information Processing, 21(3) :8-20.

JIN, Z. et TANAI<A-ISHII, K. (2006). Unsupervised segmentation of chinese text by use of branching
entropy. In Proceedings of the COLING/ACL on Main conference poster sessions, pages 4284135.

KEMPE, A. (1999). Experiments in unsupervised entropy-based corpus segmentation. In Workshop
of EACL in Computational Natural Language Learning, page 713.

MAGISTRY, P. et SAGOT, B. (2011). Segmentation et induction de lexique non-supervisées du
mandarin. In Actes de TALN 201 1, Montpellier, pages 333-344.

MocHIHAsHI, D., YAMADA, T. et UEDA, N. (2009). Bayesian unsupervised word segmentation with
nested Pitman-Yor language modeling. In Proceedings of the Joint Conference of the 47th Annual
Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of
the AFNLP: Volume 1-Volume 1, pages 100-108.

SPROAT, R. W. et SHIH, C. (1990). A statistical method for ﬁnding word boundaries in chinese
text. Computer Processing of Chinese and Oriental Languages, 4(4) :336—351.

TANAKA-IsHII, K. (2005). Entropy as an indicator of context boundaries : An experiment using a
web search engine. In International Joint Conference on Natural Language Processing (IJCNLP-
2005), pages 93-105.

T’SoU, B., LIN, H., LIU, G., CHAN, T., HU, J., CHEw, C. et TSE, J. (1997). A synchronous
chinese language corpus from different speech communities : Construction and applications.
Computational Linguistics and Chinese Language Processing, 2(1):91—104.

WANG, H., ZHU, J., TANG, S. et FAN, X. (2011). A new unsupervised approach to word segmenta-
tion. Computational Linguistics, 37(3) :421Jr54.

ZHANG, Y. et CLARK, S. (2010). A fast decoder for joint word segmentation and POS-tagging
using a single discriminative model. In Proceedings of the 2010 Conference on Empirical Methods
in Natural Language Processing, pages 843-852.

ZHAo, H. et KIT, C. (2008). An empirical comparison of goodness measures for unsupervised
chinese word segmentation with a uniﬁed framework. In The Third International Joint Conference
on Natural Language Processing (IJCNI.P-2008), Hyderabad, India.

13

Les principaux inconvénients de l’approche ESA sont d’une part le fait qu’il faille itérer le processus
sur le corpus environ 10 fois avant d’atteindre des niveaux de performance satisfaisants, et d’autre
part la nécessité d’avoir ‘a ﬁxer le paramétre de couplage entre mesure de cohésion et mesure
de séparation. Empiriquement, on constate une corrélation entre ce paramétre et la taille du
corpus, mais cette corrélation dépend de la facon dont sont traités les caractéres latins et les
chiffres arabes au cours des prétraitements. De plus, calculer cette corrélation et choisir la valeur
optimale du paramétre en question (ce que les auteurs appellent le proper exponent) nécessite un
corpus segmenté 21 la main, ce qui contredit le caractére non-supervisé de l’approche. Toutefois,
si parmi les différents types de prétraitements pour lesquels ESA a été évalué on se réfere aux
conﬁgurations qui se rapprochent des nétres, les résultats de Wang et al. (2011) avec leur
approche ESA se situent tous aux alentours de 0,80 de f-mesure sur les mots.

L’approche plus ancienne de Jin et Tanaka-Ishii (2006) ne repose que sur une mesure de
séparation, elle-méme directement insipirée par une hypothése linguistique formulée par Harris
(1955). Reformulée au moyen de la notion d’entropie de branchement (Branching Entropy, BE)
par Tana.ka-Ishii (2005) en suivant les travaux de Kempe (1999), cette hypothése peut s’énoncer
comme suit : si les séquences de graphémes, phonemes, ou autres produites par l’homme
étaient aléatoires, on s’attendrait a ce que l’entropie de branchement d’une séquence (estimée
a par1ir de n-grammes en corpus) décroisse lorsque la longueur de la séquence croit. Ainsi, la
variation de l’entropie de branchement (Variation of the Branching Entropy, VBE) devrait étre
systématiquement négative. Lorsque l’on observe au contraire une VBE positive, l’hypothése de
Harris conduit a conclure que l’on se situe a une frontiére d’unités linguistiques. C’est sur la base
de cette hypothése que Jin et Tanaka-Ishii (2006) proposent un systéme qui segmente des que
la BE croit (c’est-a-dire que la VBE est positive) ou lorsqu’elle atteint un certain maximum. Les
auteurs ﬁxent la longueur maximale des séquences calculées 21 6 et lisent le corpus de gauche a
droite et de droite a gauche. A chaque intervaJle entre deux caractéres, ils peuvent donc observer
jusqu’a 12 valeurs desquelles ils conservent le maximum.

Le principal inconvénient de l’approche de Jin et Tanaka-Ishii (2006) est que les décisions de
segmentation sont prises trés localementl et ne dépendent pas des segmentations voisines.
De plus, ce systéme repose lui aussi sur des paramétres, et notamment le seuil sur la VBE au
dessus duquel le systéme décide de segmenter (dans leur systéme, il y a segmentation des lors
que VBEZ 0). En théorie, on pourrait décider de segmenter des lors que la BE ne décroit pas
sufﬁsamment, ou 21 l’inverse ne segmenter que si la VBE est non seulement positive mais méme
au dessus d’un certain seuil non r1ul. A cet égard, placer le seuil a la valeur 0 peut étre considéré
comme une valeur par défaut, mais reste un parametre adaptable. Enﬁn, Jin et Tanaka-Ishii ne
prennent pas en compte le fait que la VBE pour un n-gramme n’est pas forcément comparable a
priori avec la VBE pour un m-gramme des lors que n aé m : une normalisation est ici nécessaire,
comme le suggérent notamment Cohen et aL (2002).

Faute de place, nous ne décrirons pas ici d’autres systémes que ceux de Wang et aL (2011) et de
Jin et Tanaka-Ishii (2006). Un état de l’art plus exhaustif peut étre trouvé dans les articles de
(Zhao et Kit, 2008) et de (Wang et al., 2011).

Dans cet article, nous montrons que l’on peut corriger les inconvénients du modéle de Jin et
Tanaka-Ishii (2006) et atteindre des niveaux de performance comparables a ceux de l’état de
l’art, c’est-a-dire de Wang et aL (2011), le tout avec un systéme plus simple.

1. Dans sa these, Jin utilise 1’auto-apprentissage et le paradigme de la minimum description length (MDL) pour pallier
21 ce problerne.

mots
en tout en tout
5 449 141 340 8 050

1455 085 2403 355
1 109 947 55 303 1 448
2 391 88 119 4050

 

TABLE 1 — Taille des corpus utilisés

3 Problémes d’évaluation

Dans cet article, aﬁn de pouvoir nous comparer au systeme de Wang et al. (201 1), nous nous
évaluons sur les corpus du second Bakeoff international de segmentation du chinois (Second
International Chinese Word Segmentation Bakeojf, Emerson, 2005). Ces corpus couvrent 4 guides
de segmentation différents, développés au sein de 4 institutions distinctes : l’Academia Sinica
(AS), la City University de Hong-Kong (CITYU), l’um'versité de Pékin (PKU) et Microsoft Research
(MSR).

Des informations sur la taille des corpus sont données au tableau 1. Dans le cadre du Bakeoﬁ, Les
détails du contenu n’étaient pas connus des participants. Mais on peut noter que le corpus de
PKU est constitué d’extraits du Quotidien du Peuple, journal de Pékin. Le corpus de CITYU est
extrait du LIVAC (T’sou et al., 1997), aussi constitué de textes de presse, mais d’origines plus
variées. Le projet du LIVAC cherchant a rendre compte des variantes géographique du mandarin,
il inclut des articles provenant de Pékin, Hong-Kong, Singapour ou Taiwan. Le corpus de l’AS
est un corpus équilibré, qui rend essentiellement compte de la variante du mandarin utilisée ‘a
Taiwan. Aucune description du contenu du corpus de MSR n’est disponible a notre connaissance.

L’évaluation de systemes non-supervisés est une problématique en soi. Un consensus sur une
déﬁnition précise de la notion de mot restant difﬁcile a atteindre, différents guides d’annotation
pour la segmentation en mots ont été proposés et appliqués ‘a divers corpus. L’évaluation de
systémes de segmentation supervisés peut étre réalisée sur n’importe quel corpus, indépendam-
ment du guide d’annotation sous-jacent, pour peu que les données d’entrainement et les données
d’évaluation soient cohérentes. Cependant, pour les systemes non-supervisés, il n’y a aucune
raison d’obtenir des résultats plus proches de l’un des guides existants que d’un autre, plutét que
des résultats se situant quelque part entre les différents guides. Huang et Zhao (2007) propose
d’utiliser l’entrainement et l’évaluation croisés de systémes de segmentation supervisés pour avoir
un ordre d’idée sur le taux de désaccord entre guides d’annotation. L’idée est donc d’entrainer puis
d’évaluer un systeme supervisé sur deux corpus respectant deux guides d’annotation distincts,
et d’en tirer une approximation de leur désaccord. C’est également un moyen d’estimer une
borne supérieur de ce que l’on est en droit d’attendre de la part d’un systéme non-supervisé, qui
n’a pas de raison d’étre plus proche d’un guide d’annotation que ne le sont les autres guides
existants (Zhao et Kit, 2008). Nous avons reproduit ce type de mesures sur nos 4 corpus au
moyen du systéme supervisé ZPAR (Zhang et Clark, 2010), et nous avons trouvé une cohérence
moyenne similaire ‘a celle obtenue par Huang et Zhao (2007), de l’ordre de seulement 0, 84
(f-mesure), qui sera donc notre topline. Par ailleurs, il est généralement admis que segmenter
chaque caractere individuellement est une baseline raisonnable, puisque pres de la moitié des
mots-formes dans un corpus segmenté a la main sont des unigrammes. Une telle baseline obtient

un f-score d’environ 0, 35.

Ces évaluations globales peuvent étre rafﬁnées en décomposant les résultats en fonction de
la longueur des mots. Les mots de longueurs différentes ont en effet des distributions tres
dissemblables. Les évaluations par longueur donnent les résultats suivants : sur les unigrammes,
les f-scores se situent entre 0,81 et 0,90, similaires aux résultats globaux. Les résultats pour
les bigrammes sont légerement meilleurs (0, 85-0, 92), mais bien plus bas sur les trigrammes,
descendant entre 0, 59 et 0, 79. Or, dans un texte en mandarin, la majorité des occurrences sont
des mots unigrammes ou bigrammes, mais le lexique est principalement composé de bigrammes
et de trigrammes. Ceci vient du fait que les unigrammes sont souvent des mots grammaticaux a
haute fréquence, alors que les trigrammes sont souvent le résultat d’afﬁxations plus ou moins
productives. Pour cette raison, les résultats uniquement calculés sur les occurrences ne patissent
pas énormément de mauvaises performances sur les trigrammes, méme si une proportion
signiﬁcative du lexique est ainsi mal traitée.

Une autre difﬁculté concernant l’évaluau'on et la comparaison entre systemes non-supervisés est
de prendre en compte de fagon équitable les prétraitements et les connaissances a priori qui sont
fournies aux systémes. Par exemple, Wang et al. (2011) utilise différents niveaux de prétraitement
(qu’ils appellent settings et que nous appelerons << conﬁgurations >>). Dans les conﬁgurations 1
et 2, Wang et al. (2011) essayent de ne pas se reposer sur la ponctuation et l’encodage des
caractéres (notamment la distinction entre caractéres chinois et latins). Cependant, ils optimisent
indépendamment leur parametre pour chaque conﬁguration. Nous considérons donc que leur
systeme prend en compte le niveau de prétraitement qui est effectué sur les caracteres latins
et les chiffres romains, et sait donc ‘a quoi s’attendre en la matiere. Dans leur conﬁguration 3,
les auteurs ajoutent la connaissance de la ponctuation en tant que frontieres de mots, et leur
conﬁguration 4 ajoute a cela un prétraitement des caracteres latins et des chiffres arabes, ce qui
conduit a des résultats plus signiﬁcatifs, moins questionnables et plus convaincants.

Nous sommes plus intéressés par une réduction du travail humain que par le déploiement a tout
prix d’un systéme strictement non-supervisé. Nous ne pensons donc pas utile de nous empécher
de procéder ‘a quelques prétraitements simples, tels que ceux discutés ci-dessus : détection des
ponctuations, des caracteres latins et des chiffres arabes 2. C’est la raison pour laquelle nos
expériences correspondent aux conﬁgurations 3 et 4 de Wang et al. (2011), et c’est ‘a elles que
nous nous comparons, en appliquant notre systéme aux mémes corpus.

4 Variation de l’entropie de branchement

4. 1 Formulation
Notre systeme repose sur l’hypothese de Harris (1955) et sa reformulation par Kempe (1999) et
Tanaka-Ishii (2005). Déﬁnissons a présent les notions sous-jacentes a notre systéme.

Soit un n-gramme xoun = xoul x1__2 . . .x,,_1_.,, dont le contexte droit 14 contient tous les caractéres
observés ‘a sa droite dans le corpus. Nous déﬁnissons son entropie de branchement droite (Right

2. De simples expressions réguliéres peuvent aussi étre euvisagées pour traiter les cas non—ambigus de nombres et de
dates utilisant les sinogrammes

Branching Entropy, RBE) comme suit :

ha(x0..n) = H(J(~|Xo..n)
—EP(x I xo..n)108P(X I xo...).

xeza

L’entropie de branchement gauche (Left Branching Entropy, LBE) est déﬁnie de facon symétrique :
si1’on note x._ 1e contexte gauche de xo_.,,, sa LBE est déﬁnie par :

h<—(x0..n) = H(J(«_ I x0..n)'

La RBE hq(xo_.,,) peut étre considérée comme l’entropie de branchement (BE) de xo__,, au cours
d’un parcours de gauche ‘a droite, alors que la LBE est la BE de xoun au cours d’un parcours de
droite a gauche.

A partir, d’une part, de h4(xo__,,) et h4(xo__,,_1), et d’autre part de h,_(xo__,,) et h,_(x1__,,), nous
déﬁnissons la variation de l’entropie de branchement (Variation of Branching Entropy, VBE) dans
les deux directions comme suit :

5ha(x0..n) = h—»(x0..n)_h—»(x0..n—1)
5h<—(x0..n) = h<—(x0..n)—h¢—(x1..n)-

4.2 Observations intermédiaires
Aprés avoir reproduit les expériences de Jin et Tanaka-Ishii (2006), nous avons effectué une série

d’observations des valeurs prises par l’entropie de branchement et ses variations. Certaines de
ces observations ont motivé les modiﬁcations apportées au modéle que nous présenterons ‘a la

section suivante. Dans cette section, nous présentons les plus pertinentes de ces observations.

Pour des questions de place disponible et pour éviter 1a redondance, les graphiques de cette
section sont produits a partir du corpus de PKU uniquement.

4.2.1 Conﬁrmation de1’hypothése de Harris

Dans un premier temps, nous allons conﬁrmer1’hypothése de Harris sur nos données. Pour cela
nous nous limitons a1’observation de la frontiére droite des bigrammes de notre corpus (le choix
des bigrammes étant motivé par leur représentativité tant en nombre d’occurences en corpus
qu’en nombre d’entrées dans le lexique). Cette valeur est donc calculée pour chaque bigramme
observé au moins deux fois dans le corpus. On affiche ensuite 1’ensemb1e de ces valeurs sous
forme d’une courbe de densité qui donne ainsi 1a répar1ition des valeurs prises par la variation
d’entropie (c’est a dire les ’5hq(xo__2)). On distingue ensuite de 1’ensemb1e de tous les bigrammes
ceux qui sont considérés comme des mots par 1’annotation manuelle de ceux qui ne 1e sont
pas. Le résultat est présenté ﬁgure 4.1.1. On observe que les mots valides (qui forment une trés
petite proportion de 1’ensemb1e des bigrammes observés) se démarquent bien par une variation
d’entropie plus grande a leur frontiére droite. Cependant, on observe aussi une zone relativement
importante de confusion, qui conﬁrme la nécessité de chercher la segmentation optimale d’une
phrase, et non simplement 1es frontiéres de facon indépendantes 1es unes des autres.

020

0.15

densxlé
010

0.05

 

000

3h(xl
FIGURE 1 — Distribution des valeurs prises par la VBE a droite des bigrammes

On peut ensuite généraliser ce mode d’observation. En se demandant notamment si les valeurs de
VBE a l’intérieur d’un mot sont aussi discriminantes que les valeurs qui en marquent les frontieres.
Pour différentes tailles de n-grammes mais en ﬁxant n on va effectuer la méme mesure et la méme
distinction entre mots et non-mots que précédemment mais cette fois-ci en prenant en compte
les deux frontieres gauche et droite ainsi que les valeurs observées a chaque inter-caractere a
l’intérieur du n-gramme et dans chacun des deux sens de lecture possibles. Pour avoir une vue
d’ensemble, on afﬁche ces résultats deux a deux sous la forme de courbes de niveaux. Les résultats
pour les trigrammes sont présentés ﬁgure 2. On observe que des différentes valeurs observées
les plus discriminantes sont sans conteste « gauche 1 » et « droite 3 », c’est a dire les entropies
aux frontieres. I1 apparait vraisemblable que la structure interne des unités morphologiquement
complexes affectent la VBE, ce qui rend les valeurs internes plus difﬁciles a utiliser en pratique,
contrairement a l’hypothese suivie initialement dans (Magistry et Sagot, 2011).

4.2.2 Limites de la formulation par entropie

En présence de données aléatoires, on s’attend a ce que l’entropie de branchement diminue a
mesure que la longueur de la chaine considérée grandit. L’hypothese de Harris nous fait dire que
pour une chaine donnée en langue naturelle, la variation de l’entropie de branchement lorsque
l’on atteint une frontiere sera anormalement élevée. Par ailleurs, on observe bien que pour des
chaines de méme longueur, la variation de l’entropie de branchement aux frontieres permet, au
moins en partie, de distinguer les mots des non-mots. Toutefois, rien ne permet d’afﬁrmer que
cette distinction reste observable si l’on s’intéresse a des chaines de longueurs différentes.

Nous avons donc cherché a observer les valeurs prises par la VBE aux frontieres des n-grammes
pour différentes valeurs de n. La ﬁgure 4.1.2 présente ces valeurs pour les uni- bi- et trigrammes.
Elle montre qu’une normalisation ou qu’un recentrage de ces valeurs est nécessaire pour les
rendre comparables.

-10-50 5 -10 -o -2 -so 5
nu: nunun

@ wgﬁ gauche 3

gs?” 1%
2;‘E® 6%

droite 1

@~‘7 %
® ;§3

 


 

@”“*'E@

In. In...
-505 -10-6-2

  droite3
..o ' L L

FIGURE 2 — Courbes de niveaux représentant la distribution des variations de 1’entropie de
branchement internes et aux frontieres des trigrammes. La partie inférieur gauche du graphique
(en noir) correspond £1 toutes les occurences de trigramme confondues, tandis que la partie
supérieur droite distingue les mots (en bleu) des non—mots (en rouge). les dimensions indiquent
qu’on s’intéresse £1 la variation d’entropie £1 gauche ou £1 droite d’un des trois caracteres qui
forment le trigramme, toujours en partant de Yextrémité opposée du trigramme.

G30

— 1-gvamme
\ w — zgramme
l . ll ‘‘ — Jrgvamme
. ‘ 1 I — Agvamme

D25

D20

den.-me
Cl ID {I I5

UEIE

Clﬂﬂ

 

FIGURE 3 — VBE observée a droite des mots de différentes longueurs

4.2.3 Conséquences

Nous venons d’observer d’une part que l’information que l’on calcule aux frontieres des mots est
de loin la plus pertinente pour distinguer les unités lexicales recherchées, et d’autre part que si
l’on cherche a comparer ces valeurs pour des unités de longueurs différentes, une normalisation
est nécessaire. Ces observations motivent et permettent des modiﬁcations importantes du modele
de segmentation : les valeurs pertinentes sont moins nombreuses et indépendantes du contexte.
Elles sont donc précalculables pour l’ensemble des n-grammes observés. De plus il nous faut les
normaliser et chercher, pour une sequence de caractere donnée, a trouver la segmentation qui
maximise ces mesures.

Ces modiﬁcations sont intégrées a notre algorithme de décodage présenté a la section suivante.

5 Algorithme de segmentation proposé

Les VBE ne sont pas directement comparables pour des chaines de longeurs différentes, et doivent
étre normalisées. Nous recentrons les VBE, pour chaque longueur de chaine, autour de 0. Pour
cela, nous retranchons simplement a la VBE d’une chaine de longeur k la moyenne des VBE de
toutes chaines de méme longueur. Nous notons Shﬂ (x) et Shh (x) les VBE normalisées. Pour
simpliﬁer, nous ne donnons que la déﬁnition de 5hﬁ(x), celle de 5hR(x) en étant symétrique :
pour chaque longueur k et chaque k-gramme x tel que len(x) = k, 5hﬁ(x) = 5hﬁ(x) — uik,
ou uik est la moyenne des valeurs de 5hﬁ(y) de tous les k-grammes y.

11 est important de noter que nous utilisons et normalisons la VBE et non l’entropie de bran-
chement elle méme. En effet, utiliser la BE contredirait l’hypothese de Harris, puisque l’on ne

s’attendrait plus a ce que l’on ait 71(xo__n) < 71(xo__n_1) aux endroits qui ne sont pas des frontiéres
de mots. De nombreux travaux utilisent pourtant la BE, normalisée ou non, et non la VBE, et
obtiennent des résultats inférieurs a l’état de l’art (Cohen et al., 2002).

Si nous ne basons nos décisions de segmentations que sur la VBE aux frontiéres de mots, chercher
la meilleure segmentation d’une phrase revient a chercher en celle-ci les mots présentant les
<< meilleures frontiéres >>. Cette qualité des frontiéres rejoint intuitivement (et empiriquement
dans une certainement mesure, voir Magistry et Sagot (2011)) la notion d’autonomie syntaxique
des unités qui composent la phrase. En termes de VBE, on peut définir la mesure d’autonomie
d’un n-gramme comme a(x) = 5._h(x) + 5h4(x).

On peut alors dire que plus l’autonomie a(x) d’un n-gram x est grande, plus x est susceptible
d’étre un mot.

Avec cette mesure d’autonomie, on peut reformuler le probléme de la segmentation d’une phrase
comme la recherche du découpage qui maximise l’autonomie des mots qu’il délimite. Pour une
séquence de caractéres s, si on note Seg(s) l’ensemble de toutes les segmentations possibles, on
cherche :

argmax Z a(wi) X len(wi)
W6Ses(s) w,-ew

on W est une segmentation délimitant les mots wowl ...w,,, et len(wi) est la longueur d’un mot
wi, utilisée ici pour rendre comparables des segmentations aboutissant ‘a des nombres de mots
différents. Multiplier la mesure d’autonomie par la longueur du mot revient a attribuer un score
aux caractéres, qui contrairement aux mots sont en nombre constant entre les segmentations
possibles d’une méme chaine.

Cette segmentation optimale en terme de VBEs est calculable simplement par programmation
dynamique.

5.1 Décodage par programmation dynamique

Notre mesure d’autonomie d’un n-gramme donné est calculée ‘a partir de tous ses contextes
observés en corpus. Mais une fois calculée, elle ne dépend pas d’un contexte particulier. Elle ne
dépend notamment pas du contexte observé spécifiquement au sein d’une chaine en cours de
segmentation.

Pour une chaine donnée uouk de longueur k, il y a 2"‘1 segmentations possibles. Mais l’on peut
remarquer que si l’on connait la meilleure segmentation pour celle-ci et pour ses préfixes uoun,
n S k, considérer un caractére supplémentaire et segmenter la chaine uo"k+1 ne nécessite que de
considérer les appartenances possibles du caractére supplémentaire (le k + 115"“). Etant donné
que nos mots sont contraints ‘a étre des séquences continues (insécables) de caractéres, il nous
sufﬁt donc de considérer les cas suivants :
1. l’ajout du k+ liéme caractére comme un mot de longueur 1 a ﬁn de la meilleure segmentation
de Uo..k
2. pour chaque preﬁxe uoun de uo__k (avec 0 < n < k), le cas o1‘1le k+ 1i“‘—“'° caractére est intégré
a un mot unique de longueur k — n qui vient s’ajouter a la ﬁn de la meilleur segmentation
de u0..n

10

