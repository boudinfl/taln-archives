<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
"http://www.w3.org/TR/html4/loose.dtd">
<html><head><title>&#201;tat de l&#8217;art : mesures de similarit&#233; s&#233;mantique locales et algorithmes globaux pour la d&#233;sambigu&#239;sation lexicale &#224; base de connaissances</title>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
</head>
<body>
<div style="page-break-before:always; page-break-after:always"><div><p>Actes de la conf&#233;rence conjointe JEP-TALN-RECITAL 2012, volume 3: RECITAL, pages 295&#8211;308,
Grenoble, 4 au 8 juin 2012. c&#169;2012 ATALA &amp; AFCP
</p>
<p>&#201;tat de l&#8217;art : mesures de similarit&#233; s&#233;mantique locales et
algorithmes globaux pour la d&#233;sambigu&#239;sation lexicale &#224; base
</p>
<p>de connaissances
</p>
<p>Andon Tchechmedjiev
LIG-GETALP
</p>
<p>Laboratoire d&#8217;Informatique de Grenoble-Groupe d&#8217;&#201;tude pour la Traduction Automatique/Traitement
Automatis&#233; des Langues et de la Parole
</p>
<p>Universit&#233; de Grenoble
andon.tchechmedjiev@imag.fr
</p>
<p>R&#201;SUM&#201;
Dans cet article, nous pr&#233;sentons les principales m&#233;thodes non supervis&#233;es &#224; base de connais-
sances pour la d&#233;sambigu&#239;sation lexicale. Elles sont compos&#233;es d&#8217;une part de mesures de similarit&#233;
s&#233;mantique locales qui donnent une valeur de proximit&#233; entre deux sens de mots et, d&#8217;autre part,
d&#8217;algorithmes globaux qui utilisent les mesures de similarit&#233; s&#233;mantique locales pour trouver les
sens appropri&#233;s des mots selon le contexte &#224; l&#8217;&#233;chelle de la phrase ou du texte.
</p>
<p>ABSTRACT
State of the art : Local Semantic Similarity Measures and Global Algorithmes for
Knowledge-based Word Sense Disambiguation
</p>
<p>We present the main methods for unsupervised knowledge-based word sense disambiguation.
On the one hand, at the local level, we present semantic similarity measures, which attempt to
quantify the semantic proximity between two word senses. On the other hand, at the global level,
we present algorithms which use local semantic similarity measures to assign the appropriate
senses to words depending on their context, at the scale of a text or of a corpus.
</p>
<p>MOTS-CL&#201;S : d&#233;sambigu&#239;sation lexicale non-supervis&#233;e, mesures de similarit&#233; s&#233;mantique &#224;
base de connaissances, algorithmes globaux de propagation de mesures locales.
</p>
<p>KEYWORDS: unsupervised word sense disambiguation, knowledge-based semantic similarity
measures, global algorithms for the propagation of local measures.
</p>
<p>1 Introduction
</p>
<p>Les ambigu&#239;t&#233;s font partie int&#233;grante des langues naturelles, mais les humains ont la capacit&#233;,
dans la plupart des cas et en s&#8217;aidant du contexte, &#224; d&#233;sambigu&#239;ser sans trop d&#8217;efforts. Cependant,
pour le traitement automatique des langues naturelles, cette ambigu&#239;t&#233; pose probl&#232;me, et il
est fondamental de trouver des m&#233;thodes pour affecter aux mots les sens corrects vis &#224; vis du
contexte.
</p>
<p>Il existe diff&#233;rentes approches pour r&#233;soudre ce probl&#232;me. Elle se divisent en deux cat&#233;gories
principales : d&#8217;une part les approches supervis&#233;es, n&#233;cessitant des corpus d&#8217;entra&#238;nement &#233;tiquet&#233;s
manuellement et, d&#8217;autre part, des approches non-supervis&#233;es (Navigli, 2009).
</p>
<p>295</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Le probl&#232;me avec les algorithmes supervis&#233;s est le fait qu&#8217;obtenir de grandes quantit&#233;s de texte
annot&#233; en sens est tr&#232;s couteux en temps et en argent, et que l&#8217;on se heurte au goulot d&#8217;acquisition
de donn&#233;es (Wagner, 2008). De plus, la qualit&#233; de la d&#233;sambiguisation de ces approches est
restreinte par les exemples utilis&#233;s pour l&#8217;entrainement.
</p>
<p>C&#8217;est pourquoi les m&#233;thodes non supervis&#233;es sont int&#233;ressantes. Elles n&#8217;utilisent pas de corpus
annot&#233;s. Il existe l&#224; aussi des distinctions : d&#8217;une part les approches non supervis&#233;es classiques
(clustering) qui exploitent les donn&#233;es non annot&#233;es ; et d&#8217;autre part les approches &#224; base de
savoirs qui utilisent des connaissances issues de ressources lexicales.
</p>
<p>Nous nous int&#233;ressons ici &#224; ces derni&#232;res. Il y a diff&#233;rents aspects &#224; consid&#233;rer dans le cadre
des approches non-supervis&#233;es &#224; base de connaissances : d&#8217;abord la question essentielle des
ressources lexicales qu&#8217;il est possible d&#8217;utiliser, ensuite la question de comment exploiter la, ou
les ressources lexicales pour d&#233;sambigu&#239;ser.
</p>
<p>Ce dernier aspect se pr&#233;sente sous deux dimensions : la dimension locale o&#249; l&#8217;on cherche &#224;
d&#233;terminer la proximit&#233; entre les sens possibles des diff&#233;rents mots et, la dimension globale o&#249;
l&#8217;on cherche &#224; affecter les bons sens aux mots &#224; l&#8217;&#233;chelle d&#8217;un texte. Il existe &#224; la fois des m&#233;thodes
qui propagent les mesures locales en les utilisant pour &#233;valuer les combinaisons de sens, mais
aussi des m&#233;thodes purement globales qui exploitent directement la structure linguistique de
l&#8217;ensemble du texte sans s&#8217;int&#233;resser aux sens individuellement.
</p>
<p>Nous pr&#233;senterons, les principales ressources lexicales (Section 2), les principales mesures de
similarit&#233; s&#233;mantique (Section 3) puis une description de quelques algorithmes globaux qui
exploitent ces mesures (Section 4). Nous terminerons par des consid&#233;rations sur l&#8217;&#233;valuation et
la comparaison de ces algorithmes (Section 5).
</p>
<p>Pour un &#233;tat de l&#8217;art complet et plus d&#233;taill&#233;, le lecteur se r&#233;f&#233;rera &#224; (Ide et Veronis, 1998) et plus
r&#233;cemment (Navigli, 2009).
</p>
<p>2 Ressources lexicales
</p>
<p>Une caract&#233;ristique des approches &#224; base de connaissances est qu&#8217;elles utilisent des ressources
lexicales. Un premier type de ressource qui peut &#234;tre exploit&#233;e est l&#8217;inventaire de sens, c&#8217;est-&#224;-
dire une ressource qui, &#224; chaque mot, lie une liste de sens possibles comme par exemple, un
dictionnaire (par exemple (Collins, 1998)). D&#8217;autre part, des ressources telles que les th&#233;saurus
(par exemple (Roget, 1989)) peuvent &#234;tre utiles pour &#233;tablir des liens entre les sens des diff&#233;rents
mots.
</p>
<p>Par ailleurs, des ressources lexicales telles que WordNet (Miller, 1995) sont structur&#233;es et jouent
le r&#244;le d&#8217;inventaires de sens et de dictionnaires, mais donnent &#233;galement acc&#232;s &#224; une hi&#233;rarchie
de sens (en quelque sorte un th&#233;saurus structur&#233;).
</p>
<p>La majorit&#233; des mesures de similarit&#233; que nous allons pr&#233;senter se basent sur Wordnet 1.
</p>
<p>WordNet est structur&#233; autour de la notion de synsets, c&#8217;est-&#224;-dire en quelque sorte un ensemble
de synonymes qui forment un concept. Un synset repr&#233;sente un sens de mot. Les synsets sont
reli&#233;s entre eux par des relations, soit lexicales (antonymie par exemple) ou taxonomiques
(hyperonymie, m&#233;ronymie, etc).
</p>
<p>1. Il est possible de les utiliser sur d&#8217;autres ressources &#233;galement.
</p>
<p>296</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>3 Mesures de similarit&#233; S&#233;mantique &#224; base de connaissances
</p>
<p>Parmi les mesures de similarit&#233; s&#233;mantique on retrouve trois types principaux que nous allons
maintenant d&#233;crire. Il faut noter que les mesures de de similarit&#233; g&#233;om&#233;triques ne sont pas &#224;
base de connaissances et ne seront pas pr&#233;sent&#233;es.
</p>
<p>3.1 &#192; base de traits
</p>
<p>3.1.1 Similarit&#233; de Tsversky
</p>
<p>Avant d&#8217;&#234;tre abord&#233;e en TALN, la notion de similarit&#233; s&#233;mantique a &#233;t&#233; trait&#233;e dans le domaine
de la psychologie cognitive. Un travail souvent cit&#233; est (Tversky, 1977) qui propose une nouvelle
approche bas&#233;e sur le recouvrement ou non de traits entre deux objets. Plus pr&#233;cis&#233;ment, est
consid&#233;r&#233; comme concept ou signification rattach&#233;e &#224; un objet toute propri&#233;t&#233; dudit objet. La
similarit&#233; entre deux objets est exprim&#233;e comme le nombre pond&#233;r&#233; de propri&#233;t&#233;s en commun,
auxquelles on retire le nombre pond&#233;r&#233; de propri&#233;t&#233;s sp&#233;cifiques &#224; chaque objet. Il propose donc
un mod&#232;le de similarit&#233; non sym&#233;trique, que l&#8217;on appelle &#171;mod&#232;le de contraste&#187; (Figure 1).
</p>
<p>FIGURE 1 &#8211; Le mod&#232;le de contraste entre deux concepts
</p>
<p>Plus formellement, si l&#8217;on reprend la notation de (Pirr&#243; et Euzenat, 2010) o&#249; &#936;(c) est l&#8217;en-
semble des traits se rapportant &#224; un sens s, alors la similarit&#233; de Tsversky peut s&#8217;exprimer par :
simt vr(s1, s2) = &#952; F(&#936;(s1)&#8745;&#936;(s2))&#8722;&#945;F(&#936;(s1)\&#936;(s2))&#8722;&#946;F(&#936;(s2)\&#936;(s1)) o&#249; F est une fonction
qui associe une pertinence aux traits, et o&#249; &#952; , &#945; et &#946; sont des facteurs qui marquent respective-
ment l&#8217;importance relative de la similarit&#233; entre les sens, des dissimilarit&#233;s entre s1 et s2 et des
dissimilarit&#233;s entre s2 et s1, et o&#249; \ est l&#8217;op&#233;rateur de diff&#233;rence ensembliste.
Il est &#233;galement possible d&#8217;exprimer cette mesure en tant que rapport, afin d&#8217;avoir une valeur de
similarit&#233; normalis&#233;e (avec &#952; = 1) :,
</p>
<p>simt v t r(c1, c2) =
F(&#936;(c1)&#8745;&#936;(c2))
</p>
<p>F(&#936;(c1)&#8745;&#936;(c2)) +&#945;F(&#936;(c1) \&#936;(c2)) + &#946;F(&#936;(c2) \&#936;(c1))
Comme r&#233;capitul&#233; dans (Pirr&#243; et Euzenat, 2010), diff&#233;rentes valeurs de &#945; et de &#946; m&#232;nent &#224;
diff&#233;rents types de similarit&#233;. Si &#945;= &#946; = 0, on ne s&#8217;int&#233;resse qu&#8217;aux points communs entre les
deux sens. Si &#945; &gt; &#946; ou &#945; &lt; &#946; alors on s&#8217;int&#233;resse assym&#233;triquement &#224; la similarit&#233; de s1 avec s2
ou vice versa. Si &#945; = &#946; 6= 0 on s&#8217;int&#233;resse &#224; la similarit&#233; mutuelle entre s1 et s2. Quand &#945; = &#946; = 1
la mesure de Tversky est &#233;quivalente &#224; la similarit&#233; de Tanimoto (Rogers et Tanimoto, 1960).
Dans le cas o&#249; &#945;= &#946; = 0.5 alors elle est &#233;quivalente au coefficient de Dice (Dice, 1945).
</p>
<p>297</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>3.1.2 Similarit&#233; de Lesk
</p>
<p>(Lesk, 1986) a propos&#233; un algorithme de d&#233;sambigu&#239;sation lexicale tr&#232;s simple, qui consid&#232;re la
similarit&#233; entre deux sens comme le nombre de mots en commun dans leurs d&#233;finitions. Dans
la version originale, on ne prend pas en compte l&#8217;ordre des mots dans les d&#233;finitions (sac de
mots). Dans ce cadre l&#224;, il appara&#238;t que cette m&#233;thode puisse &#234;tre ramen&#233;e &#224; un cas particulier
de la similarit&#233; de Tsversky (en tant que rapport ou non), en consid&#233;rant que les concepts sont
des sens de mots, que les traits sont des mots de la d&#233;finition des sens, avec &#945;= &#946; = 0, et avec
&#936;(s) = D(d) qui retournant un ensemble contenant les mots de la d&#233;finition d&#8217;un sens de mot s.
Quant &#224; la fonction F on la choisit comme la fonction cardinalit&#233; d&#8217;ensemble. On obtient ainsi :
simlesk(s1, s2) =| D(s1)&#8745; D(s2) |
L&#8217;avantage de cette mesure de similarit&#233; est qu&#8217;elle est extr&#234;mement simple &#224; calculer, et ne
requiert qu&#8217;un dictionnaire. Dans le contexte de l&#8217;algorithme de Lesk original, la similarit&#233; &#233;tait
calcul&#233;e de mani&#232;re exhaustive entre tous les sens de tous les mots du contexte, il existe une
variante (Navigli, 2009) utilis&#233;e sur une fen&#234;tre de contexte autour du mot auquel appartient
le sens. Elle correspond au recouvrement entre la d&#233;finition du sens et entre un sac de mot
contenant tous les mots des d&#233;finitions des mots du contexte : Leskvar =| contex te(w)&#8745;D(swn) |.
Comme le met en avant (Navigli, 2009), un probl&#232;me important de la mesure de Lesk est qu&#8217;elle
est tr&#232;s sensible aux mots pr&#233;sents dans la d&#233;finition, et si certains mots importants manquent
dans les d&#233;finitions utilis&#233;es, les r&#233;sultats obtenus seront de qualit&#233; moindre. De plus si les
d&#233;finitions sont trop concises (comme c&#8217;est souvent le cas) il est difficile d&#8217;obtenir des distinctions
de similarit&#233; fines.
</p>
<p>Cependant, un certain nombre d&#8217;am&#233;liorations de la mesure de Lesk ont &#233;t&#233; propos&#233;es.
</p>
<p>3.1.3 Extensions de la mesure de Lesk
</p>
<p>Tout d&#8217;abord, (Wilks et Stevenson, 1998) proposent de pond&#233;rer chaque mot de la d&#233;finition par
la longueur de celle-ci afin de donner la m&#234;me importance &#224; toutes les d&#233;finitions, au lieu de
syst&#233;matiquement privil&#233;gier les d&#233;finitions longues.
</p>
<p>Plus r&#233;cemment (Banerjee et Pedersen, 2002) ont propos&#233; la mesure de &quot;Lesk &#233;tendu&quot;, qui
am&#233;liore Lesk de deux fa&#231;ons. La premi&#232;re est l&#8217;incorporation des d&#233;finitions des sens reli&#233;s par
des relations taxonomiques WordNet dans la d&#233;finition d&#8217;un sens donn&#233;. La deuxi&#232;me est une
nouvelle mani&#232;re de calculer le recouvrement entre les mots des d&#233;finitions.
</p>
<p>Pour calculer le recouvrement entre deux sens, ils proposent de consid&#233;rer le recouvrement entre
les d&#233;finitions des deux sens mais aussi des d&#233;finitions issues de diff&#233;rentes relations : hyper-
onymes (has-kind), hyponymes (kind-of), meronymes (part-of), holonymes (has-part),
troponymes mais aussi par les relations attribute, similar-to, also-see.
</p>
<p>Afin de garantir que la mesure soit sym&#233;trique, ils proposent de prendre les combinaisons deux &#224;
deux des relations consid&#233;r&#233;es et de ne conserver une paire de relations (R1, R2) que si la paire
inverse (R2, R1) est pr&#233;sente. On obtient ainsi un ensemble RELPAIRS. De plus, le recouvrement
entre deux d&#233;finitions A et B se calcule comme la somme des carr&#233;s des longueurs de toutes les
sous-chaines de mots de A dans B, ce que l&#8217;on exprime avec l&#8217;op&#233;rateur &#229;. Nous avons ainsi :
Lesketendu(s1, s2) =
</p>
<p>&#8721;
&#8704;(R1,R2)&#8712;RELPAIRS2 (| D(R1(s1))&#229; D(R2(s2)) |)
</p>
<p>Le calcul du recouvrement est bas&#233; sur le principe relev&#233; par la loi de Zipf (Zipf, 1949), qui met
en &#233;vidence une relation quadratique entre la longueur d&#8217;une phrase et sa fr&#233;quence d&#8217;occurrence
dans un corpus. De ce fait, n mots qui apparaissent ensemble portent plus d&#8217;informations que si
ils &#233;taient s&#233;par&#233;s.
</p>
<p>298</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>3.2 &#192; base de distance taxonomique
</p>
<p>Le principe des mesures &#224; base de distance taxonomique est de compter le nombre d&#8217;arcs qui
s&#233;parent deux sens dans une taxinomie.
</p>
<p>La Figure 2 (Wu et Palmer, 1994) repr&#233;sente la relation de deux sens quelconques S1 et S2 dans
une taxinomie par rapport &#224; leur sens commun le plus sp&#233;cifique S3 et par rapport &#224; la racine de
la taxinomie ; cette figure servira &#224; exprimer de mani&#232;re homog&#232;ne les formules des diff&#233;rentes
mesures de similarit&#233;.
</p>
<p>FIGURE 2 &#8211; Deux sens et leur sens commun le plus sp&#233;cifique dans une taxinomie
</p>
<p>La mesure de Rada (Rada et al., 1989) est la premi&#232;re &#224; utiliser la distance entre les n&#339;uds
correspondant aux deux sens sur les liens d&#8217;hyponymie et hyperonymie :
</p>
<p>SimRada(s1, s2) = d(s1, s2) = N1 + N2
</p>
<p>Les termes se trouvant plus profond&#233;ment dans la taxonomie &#233;tant toujours plus proches que les
termes plus g&#233;n&#233;raux, (Wu et Palmer, 1994) proposent de prendre en compte la distance entre
l&#8217;anc&#234;tre commun le plus sp&#233;cifique et la racine pour y rem&#233;dier.
</p>
<p>SimWuP =
2 &#183; N3
</p>
<p>N1 + N2 + 2 &#183; N3
(Leacock et Chodorow, 1998) se basent &#233;galement sur la mesure de Rada, mais au lieu de
normaliser par la profondeur relative de la taxinomie par rapport aux sens, ils choisissent une
normalisation par rapport &#224; la profondeur totale de la taxinomie D et normalisent avec un
logarithme :
</p>
<p>SimLCH =&#8722;log(N1 + N22 &#183; D )
(Hirst et St-Onge, 1998) adaptent le concept de chaines lexicales d&#233;velopp&#233;es par (Morris et Hirst,
1991) comme mesure de similarit&#233; s&#233;mantique en utilisant la structure de WordNet. Cette mesure
se base sur l&#8217;id&#233;e de (Halliday et Hasan, 1976) que dans un texte, des mots ont une forte proba-
bilit&#233; de r&#233;f&#233;rer &#224; des mots ant&#233;rieurs ou &#224; d&#8217;autres concepts reli&#233;s, et que l&#8217;enchainement de ces
mots forment des chaines coh&#233;sives. Par exemple, (Navigli, 2009) Rome-&gt;ville-&gt;habitant
et manger-&gt;plat-&gt;l&#233;gume-&gt;aubergine, sont des chaines lexicales.
</p>
<p>&#192; chaque relation est associ&#233;e une direction horizontale, ascendante ou descendante, qui marque
respectivement une relation forte, tr&#232;s forte et moyennement forte (par exemple l&#8217;hyperonymie
</p>
<p>299</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>est une relation ascendante, l&#8217;holonymie une relation descendante, et l&#8217;antonymie une relation
horizontale). Les changements de direction constituent un &#233;l&#233;ment de dissimilarit&#233; et la proximit&#233;
dans la taxinomie un &#233;l&#233;ment de similarit&#233;. Un changement de direction est d&#233;fini comme le
passage d&#8217;un &#233;l&#233;ment A de la taxinomie &#224; un &#233;l&#233;ment B par une relation d&#8217;un autre type que celle
qui a permis d&#8217;arriver sur A. Notons que plus la distance entre les sens est grande, plus il y aura
de changements de direction potentiels.
</p>
<p>M&#234;me si l&#8217;algorithme propos&#233; est un algorithme global, il est possible d&#8217;utiliser la fonction
d&#8217;&#233;valuation des chaines lexicales en tant que mesure de similarit&#233;.
</p>
<p>Soient C et k deux constantes et soit la fonction virages(s1, s2) qui retourne le nombre de
changements de direction entre les sens s1 et s2, alors la mesure de similarit&#233; s&#8217;exprime :
</p>
<p>SimHso = C &#8722; (N1 + N2)&#8722; k &#183; virages(s1, s2)
Il existe &#233;galement d&#8217;autres mesures exploitant la structure taxonomique, mais en pond&#233;rant les
arcs avec des valeurs de contenu informationnel, notion que nous allons maintenant d&#233;finir.
</p>
<p>3.3 &#192; base de contenu informationnel
</p>
<p>L&#8217;approche de (Resnik, 1995) est bas&#233;e sur la d&#233;termination du contenu informationnel du
concept commun le plus sp&#233;cifique &#224; deux sens. Dans la figure 2, le concept commun le plus
sp&#233;cifique &#224; S1 et S2 est S3 (not&#233;e lso(S1, S2) = S3 pour lowest superordinate). Quant &#224; la quantit&#233;
d&#8217;information, elle est calcul&#233;e &#224; partir de la probabilit&#233; p(S3) : IC(s) =&#8722;log(P(S3))
Les probabilit&#233;s d&#8217;occurrence de chaque concept de la taxinomie sont calcul&#233;es &#224; partir d&#8217;un
corpus non-annot&#233; par estimation du maximum de vraisemblance. Ainsi la mesure similarit&#233; de
Resnik s&#8217;exprime :
</p>
<p>SimRes = IC(lso(S1, S2)) = IC(S3)
</p>
<p>(Jiang et Conrath, 1997) partent du constat qu&#8217;utiliser seulement l&#8217;anc&#234;tre commun n&#8217;offre pas
une granularit&#233; assez fine et proposent de prendre en compte la quantit&#233; d&#8217;information port&#233;e
par les deux sens. Cette mesure s&#8217;exprime :
</p>
<p>SimJCN = IC(S1) + IC(S2) + 2 &#183; IC(lso(S1, S2))
</p>
<p>(Lin 1998) propose &#233;galement un mesure de similarit&#233; tr&#232;s proche, qui revient essentiellement &#224;
une reformulation sous forme de rapport de la formule de Jiang and Conrath :
</p>
<p>SimLin =
2IC(lso(S1, S2))
IC(S1) + IC(S2)
</p>
<p>Plus r&#233;cemment, (Seco et al., 2004) ont argument&#233; qu&#8217;il est possible d&#8217;extraire directement
de WordNet les valeurs de contenu informationnel sans avoir &#224; passer par un corpus. La taxi-
nomie de WordNet &#233;tant structur&#233;e &#224; partir de principes psycholinguistiques, on peut faire
l&#8217;hypoth&#232;se que cette structure (liens d&#8217;hyperonymie et d&#8217;hyponymie) est repr&#233;sentative du
contenu informationnel ; c&#8217;est-&#224;-dire que les concepts qui ont beaucoup d&#8217;hyponymes portent
une quantit&#233; d&#8217;information moins importante que les concepts feuilles. Si l&#8217;on note hypo(s)
une fonction qui retourne le nombre d&#8217;hyponymes d&#8217;un concept s et maxwn le nombre total
</p>
<p>300</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>de concepts dans la taxinomie, alors on peut exprimer le contenu informationnel intrins&#232;que :
i IC(s) = 1&#8722; log(hypo(s)+1)
</p>
<p>log(maxwn)
</p>
<p>On peut substituer i IC &#224; IC dans toutes les mesures pr&#233;c&#233;dentes pour supprimer le besoin
d&#8217;apprentissage non supervis&#233;. Cependant, l&#8217;i IC est limit&#233;e car, elle n&#8217;exploite qu&#8217;un seul type de
relations, alors que d&#8217;autres pourraient &#234;tre int&#233;ressantes. C&#8217;est pourquoi (Pirr&#243; et Euzenat, 2010)
proposent une mesure d&#8217;i IC &#233;tendue qui va prendre en compte les autres relations pr&#233;sentes
(meronymie par exemple). Ils l&#8217;expriment comme : eIC(s) = &#950;i IC(s) +&#951;EIC(s)
</p>
<p>o&#249; &#950; et &#951; sont des constantes et o&#249; EIC(s) est la somme des i IC moyennes des concepts reli&#233;s
&#224; s par les autres relations. Si l&#8217;on note Rels(s) l&#8217;ensemble des relations possibles pour s, et si
pour une relation sr &#8712; Rels(s), sr(s) est l&#8217;ensemble des concepts reli&#233;s &#224; s par sr , alors on obtient :
EIC(s) =
</p>
<p>&#8721;
sr&#8712;Rels(s)
</p>
<p>&#8721;
c&#8712;sr (s) i IC(c)
|sr (s)|
</p>
<p>Il est &#233;galement possible de substituer eIC &#224; IC .
</p>
<p>Plus r&#233;cemment ont commenc&#233; &#224; appara&#238;tre des mesures hybrides qui, soit essayent de combiner
diff&#233;rents types de mesures de similarit&#233;, soit essayent d&#8217;exploiter la structure de plusieurs
ontologies (cross-ontology similarity).
</p>
<p>3.4 Mesures hybrides
</p>
<p>Ici, nous nous focalisons sur l&#8217;aspect combinaisons de mesures plut&#244;t que sur les approches &#224;
ontologies crois&#233;es.
</p>
<p>(Li et al 2003) proposent une mesure qui combine &#224; la fois la distance taxonomique (l = N1+N2),
la profondeur du concept commun le plus sp&#233;cifique dans la taxinomie (h = N3) ainsi que
la densit&#233; s&#233;mantique locale(d = IC(lso(s1, s2))) , cette derni&#232;re &#233;tant exprim&#233;e en terme de
contenu informationnel. Leur mesure est exprim&#233;e par : SimLi(s1, s2) = f ( f1(l), f2(h), f3(d))
o&#249; f1, f2 and f3 sont les fonctions de transfert non-lin&#233;aires respectives pour chaque type
d&#8217;information.
</p>
<p>Le but des fonctions de transfert est de normaliser dans l&#8217;intervalle [0; 1] les mesures pour qu&#8217;elles
puissent &#234;tre combin&#233;es. f1(l) = e&#8722;&#945;l o&#249; &#945; est une constante. f2(h) = (e&#946;h&#8722; e&#8722;&#946;h)&#247; (e&#946;h+ e&#8722;&#946;h)
o&#249; &#946; &gt; 0 est un facteur de lissage. f3(d) = (e&#955;d &#8722; e&#8722;&#955;d)&#247; (e&#955;d + e&#8722;&#955;d) avec &#955; &gt; 0. Quant &#224; la
fonction f , elle constitue n&#8217;importe quelle combinaison de ces trois mesures, et est &#224; choisir selon
les applications et la nature des sources d&#8217;information disponibles.
</p>
<p>FaITH, une autre mesure locale qui combine des aspects de diff&#233;rents types, est propos&#233;e par (Pirr&#243;
et Euzenat, 2010) sous la forme de l&#8217;extension des mesures &#224; base de contenu informationnel en
reprenant le mod&#232;le de contraste de Tsversky :
</p>
<p>SimFaI T H =
IC(lso(s1, s2))
</p>
<p>IC(s1) + IC(s2)&#8722; IC(lso(s1, s2))
Ici la fonction F est remplac&#233;e par IC , les traits communs aux concepts sont repr&#233;sent&#233;s par
le contenu informationnel du concept commun le plus sp&#233;cifique, et les traits sp&#233;cifiques &#224; un
concept sont repr&#233;sent&#233;s par la diff&#233;rence entre le contenu informationnel de ce concept auquel
on soustrait le contenu informationnel du concept commun le plus sp&#233;cifique.
</p>
<p>301</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>4 Algorithmes globaux de d&#233;sambigu&#239;sation lexicale
</p>
<p>Maintenant que nous avons pass&#233;s en revue les principales de mesures de similarit&#233; s&#233;mantique,
nous allons pr&#233;senter diff&#233;rents algorithmes qui les utilisent comme heuristiques pour &#233;valuer
des combinaisons de sens.
</p>
<p>4.1 Approche exhaustive
</p>
<p>L&#8217;approche originellement adopt&#233;e par (Lesk, 1986) pour d&#233;sambigu&#239;ser un texte en entier, est
d&#8217;&#233;valuer toutes les combinaisons possibles de sens et de choisir la combinaison qui maximise le
score du texte &#8211; exprim&#233; comme la somme des scores des sens choisis par rapport aux autres
mots du texte.
</p>
<p>En d&#8217;autres termes, si le sens s&#233;lectionn&#233; d&#8217;un mot w dans une combinaison C est Sw et
un texte T une liste ordonn&#233;e de mots, alors le score de la combinaison est score(C) =&#8721;
</p>
<p>wi&#8712;T
&#8721;
</p>
<p>w j&#8712;T sim(Swi , Sw j ) et il y a en tout
&#8719;
</p>
<p>w&#8712;T Nw combinaisons &#224; &#233;valuer, avec Nw le nombre
de sens de w (Gelbukh et al., 2005), c&#8217;est-&#224;-dire un nombre exponentiel de combinaisons.
</p>
<p>Par exemple pour une phrase de 10 mots avec 10 sens en moyenne par mot il y aurait 1010
2
=
</p>
<p>10100 combinaisons.
</p>
<p>Pour diminuer le temps de calcul on peut utiliser une fen&#234;tre autour du mot afin de r&#233;duire le
temps d&#8217;&#233;valuation d&#8217;une combinaison au prix d&#8217;une perte de coh&#233;rence globale de la d&#233;sambi-
gu&#239;sation.
</p>
<p>Une autre approche est d&#8217;utiliser des meta-heuristiques d&#8217;optimisation combinatoire pour obtenir
des solutions de qualit&#233; convenable d&#8217;une mani&#232;re qui soit traitable calculatoirement.
</p>
<p>4.2 Recuit simul&#233;
</p>
<p>Le recuit simul&#233; est une m&#233;thode d&#8217;optimisation stochastique classique, et f&#251;t appliqu&#233; &#224; la
d&#233;sambigu&#239;sation lexicale par (Cowie et al., 1992).
</p>
<p>Le principe est de faire des changements al&#233;atoires dans la configuration 2 it&#233;rativement de
l&#8217;espace de recherche puis d&#8217;&#233;valuer si le changement est b&#233;n&#233;fique. Dans le cas &#233;ch&#233;ant, il est
conserv&#233;, sinon, il y a une probabilit&#233; de le conserver quand m&#234;me.
</p>
<p>Cette &#233;valuation se fait en utilisant une m&#233;trique heuristique, et dans le cas de la d&#233;sambigu&#239;sation,
on utilise les mesures de similarit&#233; pour jouer ce r&#244;le. Le score d&#8217;une configuration se calcule de
la m&#234;me mani&#232;re que pour l&#8217;&#233;valuation d&#8217;une combinaison dans le cas de l&#8217;approche exhaustive.
</p>
<p>Quant &#224; la probabilit&#233; de conserver une configuration inf&#233;rieure, elle se calcule par rapport &#224; la
diff&#233;rence des scores entre la configuration modifi&#233;e( C &#8242;) et la configuration avant modification
(C) avec&#8710;s = score(C &#8242;)&#8722;score(C) et un param&#232;tre de temp&#233;rature T : P(conservation) = e &#8722;&#8710;sT .
Dans le cas d&#8217;une descente par gradients o&#249; l&#8217;on ne garde que les meilleures configurations, on
est confront&#233; &#224; un probl&#232;me de convergence sur des maxima locaux. L&#8217;objectif de l&#8217;acceptation
possible des configurations inf&#233;rieures pour le recuit simul&#233; est de leur &#233;chapper. Cependant cela
pourrait mener &#224; une non convergence du syst&#232;me, c&#8217;est pourquoi la diminution g&#233;om&#233;trique
de la temp&#233;rature T permet progressivement de se ramener &#224; une descente de gradient, dont la
convergence est garantie.
</p>
<p>2. Une configuration est repr&#233;sent&#233;e par un vecteur d&#8217;entiers correspondant aux num&#233;ros de sens s&#233;lectionn&#233;s pour
chaque mot dans l&#8217;ordre d&#8217;apparition des mots dans le texte.
</p>
<p>302</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>4.3 Algorithme g&#233;n&#233;tique
</p>
<p>Les algorithmes g&#233;n&#233;tiques sont inspir&#233;s de l&#8217;&#233;volution g&#233;n&#233;tique des esp&#232;ces et sont utilis&#233;s pour
trouver des solutions &#224; des probl&#232;mes d&#8217;optimisation combinatoire.
</p>
<p>(Gelbukh et al., 2003) l&#8217;ont appliqu&#233; &#224; la d&#233;sambigu&#239;sation lexicale. La repr&#233;sentation de la
configuration utilis&#233;e est la m&#234;me que pour le recuit simul&#233;, cependant, on consid&#232;re une
population de &#955; configurations (chromosomes). Chaque indice du vecteur d&#8217;une configuration
est consid&#233;r&#233; comme un all&#232;le, et les all&#232;les possibles pour un indice sont les diff&#233;rents sens du
mot en question.
</p>
<p>Le d&#233;roulement de l&#8217;algorithme est inspir&#233; des cycles reproductifs et de s&#233;lection naturelle des
esp&#232;ces. La qualit&#233; d&#8217;un individu est estim&#233;e avec une fonction de score heuristique, ici la m&#234;me
que pour le recuit simul&#233;.
</p>
<p>&#192; chaque cycle, les scores de tous les individus sont calcul&#233;s, et un nombre pair d&#8217;individus sont
</p>
<p>s&#233;lectionn&#233;s de mani&#232;re probabiliste pour &#234;tre crois&#233;s : &#8704;&#955;i &#8712; &#955;, p(crois&#955;i ) = C r &#8727;
&#16;
</p>
<p>score(&#955;i)
scoremax
</p>
<p>&#17;
,
</p>
<p>o&#249; scoremax est le meilleur score dans la population et o&#249; C r est un rapport de s&#233;lection.
</p>
<p>Le croisement s&#8217;effectue par une permutation autour d&#8217;un ou plusieurs points de pivots choisis au
hasard dans la configuration, habituellement un ou deux. Les individus non retenus pour le croi-
sement sont dupliqu&#233;s. On obtient ainsi une nouvelle population (qui remplace l&#8217;ancienne). Sur
chaque individu, on applique avec une probabilit&#233; p(M), Mn changements al&#233;atoires uniformes.
Le score de la nouvelle population est calcul&#233; apr&#232;s la phase de mutation, puis un nouveau cycle
commence.
</p>
<p>Parmi les strat&#233;gies de convergence, on trouve un nombre fixe de cycles o&#249; encore une sta-
bilisation de la distribution des scores de la population pendant plusieurs cycles successifs.
</p>
<p>4.4 Chaines lexicales
</p>
<p>Comme d&#233;crit dans la Section 3.2 (Hirst et St-Onge, 1998) est un algorithme global qui se
base sur la construction de chaines lexicales afin d&#8217;&#233;valuer les combinaisons en int&#233;grant des
connaissances linguistiques pour r&#233;duire l&#8217;espace de recherche.
</p>
<p>Ils placent tout d&#8217;abord des restrictions sur les enchainements de types de liens possibles : il est
impossible d&#8217;avoir plus d&#8217;un changement de direction ; un lien ascendant est terminal, sauf si il
est suivi d&#8217;un lien horizontal faisant le lien avec un lien descendant.
</p>
<p>La chaine lexicale globale est construite dans l&#8217;ordre des mots du le texte. Lorsqu&#8217;un mot est ins&#233;r&#233;
(pr&#233;sent dans le texte, ou transitivement par une relation), un certain nombre de ses synsets lui
sont reli&#233;s : si c&#8217;est le premier mot de la chaine ou si le mot provient d&#8217;une relation tr&#232;s forte alors
on garde tous les synsets ; quand il provient d&#8217;un lien fort, alors on inclut seulement les synsets
qui lui sont reli&#233;s par des liens forts ; et quand le mot provient d&#8217;une relation moyennement forte
alors on ne consid&#232;re que les synsets avec le meilleur score (avec leur mesure locale).
</p>
<p>Tous les synsets qui ne participent pas &#224; une relation selon les crit&#232;res pr&#233;c&#233;dents sont supprim&#233;s.
De plus, au fur et &#224; mesure que des mots sont ajout&#233;s &#224; la chaine, et si elle devient ill&#233;gale, tous
ses synsets sont supprim&#233;s.
</p>
<p>Par ailleurs quand on ins&#232;re un mot, on cherche d&#8217;abord parmi les relations par ordre d&#233;croissant
de force et dans un contexte de plus en plus petit (respectivement, toutes les phrases, sept
</p>
<p>303</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>phrases, trois phrases) si une chaine existe d&#233;j&#224;, auquel cas le mot y est ajout&#233; ; dans le cas
contraire une nouvelle chaine est cr&#233;&#233;e.
</p>
<p>Le probl&#232;me de cette m&#233;thode est qu&#8217;elle est peu pr&#233;cise &#224; cause de sa nature gloutonne (Navigli,
2009) et diff&#233;rentes am&#233;liorations ont &#233;t&#233; propos&#233;es. On peut citer (Barzilay et Elhadad, 1997)
qui gardent toutes les interpr&#233;tations possibles, ce qui augmente la pr&#233;cision au d&#233;triment des
performances. (Silber et McCoy, 2000) proposent un algorithme de construction de chaines
lexicales lin&#233;aire, qui permet de r&#233;soudre le probl&#232;me de performance tout en conservant la
qualit&#233; accrue.
</p>
<p>4.5 Algorithme &#224; base d&#8217;exploration pseudo al&#233;atoire de graphes
</p>
<p>D&#8217;autres algorithmes sont ceux qui se basent sur le principe d&#8217;une marche al&#233;atoire dans un
graphe, ce qui inclut &#224; la fois des algorithmes de type PageRank, mais aussi des m&#233;ta-heuristiques
&#224; colonies de fourmis.
</p>
<p>4.5.1 Algorithme &#224; base de PageRank
</p>
<p>(Mihalcea et al. 2004) ont appliqu&#233; l&#8217;algorithme de PageRank (Brin et Page, 1998) pour la
d&#233;sambigu&#239;sation lexicale. Le principe est d&#8217;assigner des poids aux arcs d&#8217;un graphe r&#233;cursivement
en exploitant les informations globalement disponibles.
</p>
<p>(Mihalcea et al. 2004) utilisent WordNet et ses relations pour construire un graphe dirig&#233; 3 ou
non repr&#233;sentant les diff&#233;rentes combinaisons de sens &#224; partir du texte &#224; d&#233;sambigu&#239;ser. Pour
chaque mot, l&#8217;ensemble des synsets qui lui sont li&#233;s constituent les n&#339;uds du graphe, alors que
les arcs sont les relations issues de Wordnet (ou d&#8217;une combinaison de relations) entre les synsets
des mots du texte. &#192; noter que les synsets du m&#234;me mot ne sont jamais reli&#233;s entre eux.
</p>
<p>Une fois le graphe construit, un poids est associ&#233; &#224; chaque arc. Le choix des poids initiaux peut
se faire de deux mani&#232;res : initialis&#233;s &#224; 1, ou par une mesure de similarit&#233; s&#233;mantique.
</p>
<p>S&#8217;en suit la marche al&#233;atoire du PageRank. Le marcheur choisit l&#8217;arc &#224; emprunter de mani&#232;re
al&#233;atoire suivant la distribution des valeurs de PageRank des n&#339;uds reli&#233;s au n&#339;ud courant. &#192;
chaque passage sur un n&#339;ud, le score est mis &#224; jour avec : S(Ni) = (1&#8722;d)+d&#8727;&#8721; j&#8712;In(Ni) S(Ni)|Out(Ni)| , o&#249;
d est un facteur de lissage, Ni un n&#339;ud du graphe, In(Ni) l&#8217;ensemble des n&#339;uds pr&#233;d&#233;cesseurs de
Ni et Out(Ni) l&#8217;ensemble des n&#339;uds successeurs. Lorsque le syst&#232;me a converg&#233; sur la distribution
stationnaire, le sens de chaque mot correspondant au n&#339;ud avec le meilleur score PageRank est
s&#233;lectionn&#233; le sens.
</p>
<p>4.5.2 Algorithme &#224; colonies de fourmis
</p>
<p>Les algorithmes &#224; colonies de fourmis sont des algorithmes multi-agents r&#233;actifs qui cherchent
&#224; imiter le fonctionnement d&#8217;une colonie de fourmis. Cette approche fut initialement propos&#233;e
par (Dorigo, 1992), et part de la constatation faite par (Deneubourg et al., 1983) que les
fourmis, lorsque plusieurs chemins sont possibles pour atteindre de la nourriture, convergent
syst&#233;matiquement vers le chemin le plus court. Lors de leur passage, les fourmis d&#233;posent une
substance chimique (ph&#233;romone) pour alerter les autres fourmis de la pr&#233;sence de nourriture ;
</p>
<p>3. Sachant que les relations dans Wordnet son sym&#233;triques, on retiens arbitrairement soit la relation, soit son inverse
</p>
<p>304</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>cette substance s&#8217;&#233;vapore si elle n&#8217;est pas renforc&#233;e par le passage d&#8217;autres fourmis. C&#8217;est cette
communication indirecte au travers de l&#8217;environnement (stigmergie) qu&#8217;&#233;merge une convergence
optimale du syst&#232;me. L&#8217;utilisation d&#8217;un tel algorithme pour la d&#233;sambigu&#239;sation lexicale a &#233;t&#233;
propos&#233;e par (Guinand et Lafourcade, 2010) en utilisant un mod&#232;le de similarit&#233; &#224; base de
vecteurs pour r&#233;gir le d&#233;placement des fourmis. Plus r&#233;cemment (Schwab et al., 2011) ont
propos&#233; de remplacer ce mod&#232;le par l&#8217;utilisation d&#8217;une approche bas&#233;e sur la mesure de Lesk
&#233;tendu.
</p>
<p>FIGURE 3 &#8211; La structure de l&#8217;environnement de l&#8217;algorithme &#224; colonies de fourmis
</p>
<p>Le graphe, contrairement &#224; celui de (Mihalcea et al., 2004), ne lie pas les sens entre eux, mais
reprends une structure d&#8217;arbre qui suit celle du texte 4.
</p>
<p>Nous pouvons voir sur la Figure 3 un exemple de graphe pour une phrase simple. La racine est
un n&#339;ud correspondant au texte. Les n&#339;uds fils sont des n&#339;uds correspondant aux phrases ;
leurs n&#339;uds fils correspondant aux mots les feuilles des mots correspondent aux sens. Ces n&#339;uds
produiront les fourmis (fourmili&#232;res). Au d&#233;part il n&#8217;y a aucune connexion entre les n&#339;uds &#8220;sens&#8221;
des diff&#233;rents mots, ce sont les fourmis qui vont cr&#233;er des &#8220;ponts&#8221; entre eux. Chaque n&#339;ud qui
n&#8217;est pas une fourmili&#232;re poss&#232;de un vecteur de sens qui lui est attach&#233; (vecteur contenant des
identifiants de sens WordNet).
</p>
<p>Les n&#339;uds fourmili&#232;res poss&#232;dent une quantit&#233; d&#8217;&#233;nergie qu&#8217;elles peuvent utiliser pour produire
des fourmis &#224; chaque it&#233;ration de l&#8217;algorithme.
</p>
<p>Les fourmis, partent &#224; l&#8217;exploration du graphe de mani&#232;re pseudo al&#233;atoire. La probabilit&#233;
de prendre un chemin d&#233;pend de la quantit&#233; d&#8217;&#233;nergie sur le n&#339;ud, de la concentration de
ph&#233;romone, et du score entre le n&#339;ud (son vecteur de sens) o&#249; elle se trouve et sa fourmili&#232;re
d&#8217;origine 5.
</p>
<p>Quand une fourmi arrive sur un n&#339;ud, elle pr&#233;l&#232;ve une quantit&#233; d&#8217;&#233;nergie et a une probabilit&#233;
d&#233;pendant de la quantit&#233; d&#8217;&#233;nergie qu&#8217;elle porte de passer en mode retour pour rapporter l&#8217;&#233;nergie
&#224; sa fourmili&#232;re.
</p>
<p>Lorsqu&#8217;une fourmi passe sur un n&#339;ud non fourmili&#232;re elle va d&#233;poser le sens correspondant &#224; sa
fourmili&#232;re dans le vecteur de sens du n&#339;ud ainsi qu&#8217;une quantit&#233; de ph&#233;romone. La ph&#233;romone
s&#8217;&#233;vapore en partie &#224; chaque it&#233;ration.
</p>
<p>Si une fourmi arrive sur le n&#339;ud d&#8217;un autre sens que le sien, il y a une probabilit&#233; (d&#233;pendant du
score avec son sens d&#8217;origine) qu&#8217;elle construise un &#8220;pont&#8221; vers sa fourmili&#232;re afin d&#8217;y revenir
directement. Lorsque la fourmi passe par un pont elle d&#233;pose &#233;galement des ph&#233;romones, ce qui
pourra inciter d&#8217;autres fourmis &#224; la suivre.
</p>
<p>4. On conserve ainsi la proximit&#233; et l&#8217;ordre des mots par exemple
5. &#224; l&#8217;aide de mesures de similarit&#233; locales.
</p>
<p>305</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Lorsque de nombreux ponts ont &#233;t&#233; construits, certains ponts vont se renforcer et d&#8217;autres
s&#8217;&#233;vaporer (lorsqu&#8217;il n&#8217;y aura plus de ph&#233;romone) ; cela va mener &#224; une monopolisation des
ressources au niveau des fourmili&#232;res avec les ponts les plus fr&#233;quent&#233;s. Les ponts correspondant
ainsi &#224; des chemins interpr&#233;tatifs parmi les combinaisons de sens possibles. &#192; la fin de la
simulation les sens qui correspondent aux fourmili&#232;res avec le plus d&#8217;&#233;nergie sont choisis.
</p>
<p>5 Crit&#232;res de choix des algorithmes
</p>
<p>Le choix de la mesure de similarit&#233; &#224; utiliser d&#233;pends d&#8217;une part des contraintes sur les ressources
lexicales disponibles mais aussi du contexte applicatif : certaines seront plus adapt&#233;es car relevant
mieux de certains aspects plut&#244;t que d&#8217;autres de la similarit&#233; s&#233;mantique r&#233;elle. (Budanitsky et
Hirst, 2006) proposent une comparaison empirique de 5 mesures par rapport au jugement humain
de mani&#232;re tr&#232;s d&#233;taill&#233;e, ce qui peut constituer un &#233;l&#233;ment de choix utile. Plus r&#233;cemment (Pirr&#243;
et Euzenat, 2010), entreprennent &#233;galement de comparer leur mesure &#224; la plupart des mesures
classiques.
</p>
<p>Quant aux algorithmes globaux, il y a deux aspects &#224; consid&#233;rer, d&#8217;une part l&#8217;&#233;valuation de
la qualit&#233; des solutions, et d&#8217;autre part le temps d&#8217;ex&#233;cution de l&#8217;algorithme. Les taches de
d&#233;sambigu&#239;sation des campagnes d&#8217;&#233;valuation telles que Semeval (anciennement SenseEval), ne
sont ax&#233;es que sur l&#8217;&#233;valuation de la qualit&#233; par rapport &#224; une d&#233;sambigu&#239;sation de r&#233;f&#233;rence du
corpus faite manuellement. Elles fournissent cependant un premier &#233;l&#233;ment de comparaison.
</p>
<p>D&#8217;une part on peut discuter de la valeur d&#8217;une telle &#233;valuation de la qualit&#233; dans un syst&#232;me
appliqu&#233; &#224; un probl&#232;me r&#233;el, et d&#8217;autre part de la vitesse d&#8217;ex&#233;cution qui est un facteur tr&#232;s
important pour des applications telles que la traduction automatique, surtout si il s&#8217;agit de
traduction de parole &#224; parole (o&#249; il y a un besoin de traitement en temps r&#233;el).
</p>
<p>(Schwab et al., 2012) ont entrepris de comparer le recuit simul&#233;, l&#8217;algorithme g&#233;n&#233;tique ainsi que
l&#8217;algorithme &#224; colonie de fourmis &#224; la fois en termes de qualit&#233; (Semeval 2007 &#8211; T&#226;che 7), mais
&#233;galement en terme de convergence et de vitesse d&#8217;ex&#233;cution en utilisant comme mesure locale
Lesk &#233;tendu. Ils concluent que les trois algorithmes avec la m&#234;me mesure de similarit&#233; locale
offrent des r&#233;sultats en terme de qualit&#233; comparables ; c&#8217;est cependant l&#8217;algorithme &#224; colonie de
fourmis qui s&#8217;av&#232;re le plus rapide (environ 10 fois plus que le recuit simul&#233; et 100 fois plus que
l&#8217;algorithme g&#233;n&#233;tique).
</p>
<p>6 Conclusions et perspectives de recherche
</p>
<p>Nous avons pass&#233;s en revue les principales m&#233;thodes de d&#233;sambigu&#239;sation lexicale bas&#233;es sur
des connaissances, que ce soit les mesures au niveau local ou les algorithmes au niveau global.
D&#8217;un point de vue local, les mesures de similarit&#233; s&#233;mantique sont bien entendu tr&#232;s utiles pour
les syst&#232;mes de TALN, mais elles jouent &#233;galement un r&#244;le de plus en plus important pour des
applications au web s&#233;mantique, et &#233;galement pour la construction automatis&#233;e de ressources
lexicales. La recherche se focalise principalement d&#8217;une part sur l&#8217;hybridation et la combinai-
son des mesures de similarit&#233;, mais &#233;galement sur la combinaison de sources d&#8217;information
(multilingues ou non), ou encore des ontologies crois&#233;es.
</p>
<p>Du point de vue global une possibilit&#233; d&#8217;am&#233;lioration se situe au niveau du temps de convergence
et des performances en g&#233;n&#233;ral. Nous nous int&#233;ressons surtout aux combinaisons de mesures de
</p>
<p>306</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>similarit&#233;. Ces combinaisons peuvent se faire en utilisant des mesures diff&#233;rentes pour essayer
de capter diff&#233;rents aspects utiles &#224; la d&#233;sambigu&#239;sation. Par exemple, on peut imaginer utiliser
une mesure par cat&#233;gorie lexicale, utiliser diff&#233;rentes mesures exploitant diff&#233;rentes sources
d&#8217;information ou adopter une strat&#233;gie de vote, ou enfin, au niveau de l&#8217;algorithme &#224; colonies
de fourmis, utiliser diff&#233;rentes castes de fourmis, chacune utilisant une mesure de similarit&#233;
diff&#233;rente pour ses d&#233;placements.
</p>
<p>R&#233;f&#233;rences
</p>
<p>BANERJEE, S. et PEDERSEN, T. (2002). An adapted lesk algorithm for word sense disambiguation
using wordnet. In CICLing &#8217;02, pages 136&#8211;145, London, UK.
</p>
<p>BARZILAY, R. et ELHADAD, M. (1997). Using lexical chains for text summarization. In In
Proceedings of the ACL Workshop on Intelligent Scalable Text Summarization, pages 10&#8211;17.
</p>
<p>BRIN, S. et PAGE, L. (1998). The anatomy of a large-scale hypertextual web search engine. In
WWW7, pages 107&#8211;117, Amsterdam.
</p>
<p>BUDANITSKY, A. et HIRST, G. (2006). Evaluating wordnet-based measures of lexical semantic
relatedness. Computational Linguistics, 32(1):13&#8211;47.
</p>
<p>COLLINS (1998). Cobuild English Dictionary. Harper Collins Publishers.
</p>
<p>COWIE, J., GUTHRIE, J. et GUTHRIE, L. (1992). Lexical disambiguation using simulated annealing.
In COLING &#8217;92, pages 359&#8211;365, Stroudsburg, PA, USA. ACL.
</p>
<p>DENEUBOURG, J. L., PASTEELS, J. M. et VERHAEGE, J. C. (1983). Probabilistic behaviour in ants : a
strategy of errors ? Journal of Theoretical Biology, 105:259&#8211;271.
</p>
<p>DICE, L. R. (1945). Measures of the amount of ecologic association between species. Ecology,
26(3):297&#8211;302.
</p>
<p>DORIGO, M. (1992). Optimization, Learning and Natural Algorithms. Th&#232;se de doctorat, Politec-
nico di Milano, Italie.
</p>
<p>GELBUKH, A., SIDOROV, G. et HAN, S.-Y. (2003). Evolutionary approach to natural language
word sense disambiguation through global coherence optimization. WSEAS Transactions on
Communications, 1(2):11&#8211;19.
</p>
<p>GELBUKH, A., SIDOROV, G. et HAN, S.-Y. (2005). On some optimization heuristics for lesk-like
wsd algorithms. In NLDB&#8217;05, pages 402&#8211;405, Berlin, Heidelberg.
</p>
<p>GUINAND, F. et LAFOURCADE, M. (2010). Artificial ants for Natural Language Processing, chapitre 20,
pages 455&#8211;492. Artificial Ants. From Collective Intelligence to Real-life Optimization and Beyond.
Monmarch&#233;, N. and Guinand, F. and P. Siarry.
</p>
<p>HALLIDAY, M. A. et HASAN, R. (1976). Cohesion in English. Longman Group Ltd, London, U.K.
</p>
<p>HIRST, G. et ST-ONGE, D. D. (1998). Lexical chains as representations of context for the detection
and correction of malapropisms. WordNet : An electronic Lexical Database. C. Fellbaum., pages
305&#8211;332. Ed. MIT Press.
</p>
<p>IDE, N. et VERONIS, J. (1998). Word sense disambiguation : The state of the art. Computational
Linguistics, 24:1&#8211;40.
</p>
<p>JIANG, J. J. et CONRATH, D. W. (1997). Semantic Similarity Based on Corpus Statistics and Lexical
Taxonomy. In ROCLING X.
</p>
<p>307</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>LEACOCK, C. et CHODOROW, M. (1998). Combining local context and wordnet similarity for
word sense identification. WordNet : An Electronic Lexical Database. C. Fellbaum. Ed. MIT Press.
Cambridge. MA.
</p>
<p>LESK, M. (1986). Automatic sense disambiguation using machine readable dictionaries : how to
tell a pine cone from an ice cream cone. In SIGDOC &#8217;86, pages 24&#8211;26, New York, NY, USA. ACM.
</p>
<p>MIHALCEA, R., TARAU, P. et FIGA, E. (2004). Pagerank on semantic networks, with application to
word sense disambiguation. In COLING &#8217;04, Stroudsburg, PA, USA. ACL.
</p>
<p>MILLER, G. A. (1995). Wordnet : a lexical database for english. Commun. ACM, 38(11):39&#8211;41.
</p>
<p>MORRIS, J. et HIRST, G. (1991). Lexical cohesion computed by thesaural relations as an indicator
of the structure of text. Comput. Linguist., 17(1):21&#8211;48.
</p>
<p>NAVIGLI, R. (2009). Word sense disambiguation : A survey. ACM Comput. Surv., 41(2):10 :1&#8211;
10 :69.
</p>
<p>PIRR&#211;, G. et EUZENAT, J. (2010). A feature and information theoretic framework for semantic
similarity and relatedness. In ISWC 2010, volume 6496 de Lecture Notes in Computer Science,
pages 615&#8211;630.
</p>
<p>RADA, R., MILI, H., BICKNELL, E. et BLETTNER, M. (1989). Development and application of a
metric on semantic nets. IEEE Transactions on Systems, Man, and Cybernetics, 19(1):17&#8211;30.
</p>
<p>RESNIK, P. (1995). Using information content to evaluate semantic similarity in a taxonomy. In
IJCAI&#8217;95, pages 448&#8211;453, San Francisco, CA, USA.
</p>
<p>ROGERS, D. et TANIMOTO, T. (1960). A computer program for classifying plants. Science,
132(3434):1115&#8211;1118.
</p>
<p>ROGET (1989). New Roget&#8217;s Thesaurus. P.S.I.
</p>
<p>SCHWAB, D., GOULIAN, J. et GUILLAUME, N. (2011). D&#233;sambigu&#239;sation lexicale par propagation
de mesures s&#233;mantiques locales par algorithmes &#224; colonies de fourmis. In TALN, Montpellier
(France).
</p>
<p>SCHWAB, D., GOULIAN, J. et TCHECHMEDJIEV, A. (2012). Comparaison th&#233;orique et pratique
d&#8217;algorithmes d&#8217;optimisation globaux pour la d&#233;sambigu&#239;sation lexicale non supervis&#233;e. Traite-
ment Automatique des Langues, 1(53):37 pages. Soumis &#224; la revue Traitement Automatique des
Langues.
</p>
<p>SECO, N., VEALE, T. et HAYES, J. (2004). An intrinsic information content metric for semantic
similarity in wordnet. In Proceedings of ECAI&#8217;2004, pages 1089&#8211;1090, Valencia, Spain.
</p>
<p>SILBER, H. G. et MCCOY, K. F. (2000). Efficient text summarization using lexical chains. In IUI
&#8217;00, pages 252&#8211;255, New York, NY, USA. ACM.
</p>
<p>TVERSKY, A. (1977). Features of similarity. Psychological Review, 84(4):327&#8211;352.
</p>
<p>WAGNER, C. (2008). Breaking the knowledge acquisition bottleneck through conversational
knowledge management. Information Resources Management Journal, 19(1):70&#8211;83.
</p>
<p>WILKS, Y. et STEVENSON, M. (1998). Word sense disambiguation using optimised combinations
of knowledge sources. In COLING &#8217;98, pages 1398&#8211;1402, Stroudsburg, PA, USA. ACL.
</p>
<p>WU, Z. et PALMER, M. (1994). Verbs semantics and lexical selection. In Proceedings of the 32nd
annual meeting on ACL, volume 2 de ACL &#8217;94, pages 133&#8211;138, Stroudsburg, PA, USA. Association
for Computational Linguistics.
</p>
<p>ZIPF, G. K. (1949). Human Behavior and the Principle of Least Effort. Addison-Wesley (Reading
MA).
</p>
<p>308</p>

</div></div>
</body></html>