Etat de l’art : mesures de similarité sémantique locales et
algorithmes globaux pour la désambigu'1'sation lexicale a base
de connaissances

Andon Tchechmedjiev
LIG—GETALP
Laboratoire d’Informatique de Grenoble—Groupe d’f‘.tude pour la Traduction Automatique/'I‘raitement
Automatisé des Langues et de la Parole
Université de Grenoble
a.ndon . tche chmedj ievimag . fr

RESUME
Dans cet article, nous présentons les principales méthodes non supervisées ‘a base de connais-
sances pour la désambiguisation lexicale. Elles sont composées d’une part de mesures de similarité
sémantique locales qui donnent une valeur de proximité entre deux sens de mots et, d’autre part,
d’algorithmes globaux qui utilisent les mesures de similarité sémantique locales pour trouver les
sens appropriés des mots selon le contexte 5 l’échelle de la phrase ou du texte.

ABSTRACT
State of the art : Local Semantic Similarity Measures and Global Algorithmes for
Knowledge-based Word Sense Disambiguation

We present the main methods for unsupervised knowledge-based word sense disambiguation.
On the one hand, at the local level, we present semantic similarity measures, which attempt to
quantify the semantic proximity between two word senses. On the other hand, at the global level,
we present algorithms which use local semantic similarity measures to assign the appropriate
senses to words depending on their context, at the scale of a text or of a corpus.

MOTS-CLES : désambigu'1'sation lexicale non-supervisée, mesures de similarité sémantique a
base de connaissances, algorithmes globaux de propagation de mesures locales.

KEYWORDS: unsupervised word sense disambiguation, knowledge-based semantic similarity
measures, global algorithms for the propagation of local measures.

1 Introduction

Les ambiguités font partie intégrante des langues naturelles, mais les humains ont la capacité,
dans la plupart des cas et en s’aidant du contexte, a désambiguiser sans trop d’efforts. Cependant,
pour le traitement automatique des langues naturelles, cette ambiguité pose probléme, et il
est fondamental de trouver des méthodes pour affecter aux mots les sens corrects vis ‘a vis du
contexte.

Il existe différentes approches pour résoudre ce probleme. Elle se divisent en deux catégories
principales : d’une part les approches supervisées, nécessitant des corpus d’entrainement étiquetés
manuellement et, d’autre part, des approches non-supervisées (Navigli, 2009).

Actes de la con_fe'rence conjointe JEP-TALN-RECITAL 2012, volume 3: RECITAL, pages 295-308,
Grenoble, 4 an 8 juin 2012. ©2012 ATAI.A 8: AFCP

295

Le probleme avec les algorithmes supervisés est le fait qu’obtenir de grandes quantités de texte
annoté en sens est trés couteux en temps et en argent, et que l’on se heurte au goulot d’acquisition
de données (Wagner, 2008). De plus, la qualité de la désambiguisation de ces approches est
restreinte par les exemples utilisés pour l’entrainement.

C’est pourquoi les méthodes non supervisées sont intéressantes. Elles n’utilisent pas de corpus
annotés. I1 existe l‘a aussi des distinctions : d’une part les approches non supervisées classiques
(clustering) qui exploitent les données non annotées; et d’autre part les approches ‘a base de
savoirs qui utilisent des connaissances issues de ressources lexicales.

Nous nous intéressons ici ‘a ces dernieres. Il y a différents aspects ‘a considérer dans le cadre
des approches non-supervisées ‘a base de connaissances : d’abord la question essentielle des
ressources lexicales qu’il est possible d’ut1'liser, ensuite la question de comment exploiter la, ou
les ressources lexicales pour désambiguiser.

Ce dernier aspect se présente sous deux dimensions : la dimension locale o1‘1 l’on cherche ‘a
déterminer la proximité entre les sens possibles des différents mots et, la dimension globale o1‘1
l’on cherche a affecter les bons sens aux mots a l’échelle d’un texte. Il existe a la fois des méthodes
qui propagent les mesures locales en les utilisant pour évaluer les combinaisons de sens, mais
aussi des méthodes purement globales qui exploitent directement la structure linguistique de
l’ensemble du texte sans s’intéresser aux sens individuellement.

Nous présenterons, les principales ressources lexicales (Section 2), les principales mesures de
similarité sémantique (Section 3) puis une description de quelques algorithmes globaux qui
exploitent ces mesures (Section 4). Nous terminerons par des considérations sur l’évaluation et
la comparaison de ces algorithmes (Section 5).

Pour un état de l’art complet et plus détaillé, le lecteur se référera a (Ide et Veronis, 1998) et plus
récemment (Navigli, 2009).

2 Ressources lexicales

Une caractéristique des approches ‘a base de connaissances est qu’elles utilisent des ressources
lexicales. Un premier type de ressource qui peut étre exploitée est l’inventaire de sens, c’est-‘a-
dire une ressource qui, a chaque mot, lie une liste de sens possibles comme par exemple, un
dictionnaire (par exemple (Collins, 1998)). D’autre part, des ressources telles que les thésaurus
(par exemple (Roget, 1989)) peuvent étre utiles pour établir des liens entre les sens des différents
mots.

Par aﬂleurs, des ressources lexicales telles que WordNet (Miller, 1995) sont structurées et jouent
le role d’inventaires de sens et de dictionnaires, mais donnent également accés a une hiérarchie
de sens (en quelque sorte un thésaurus suucturé).

La majorité des mesures de similarité que nous allons présenter se basent sur Wordnet 1.

WordNet est structuré autour de la notion de synsets, c’est-‘a-dire en quelque sorte un ensemble
de synonymes qui forment un concept. Un synset représente un sens de mot. les synsets sont
reliés entre eux par des relations, soit lexicales (antonymie par exemple) ou taxonomiques
(hyperonymie, méronymie, etc).

1. Il est possible de les utiliser sur d’autres ressources également.

296

3 Mesures de similarité Sémantique a base de connaissances

Parmi les mesures de similarité sémantique on retrouve trois types principaux que nous allons
maintenant décrire. I1 faut noter que les mesures de de similarité géométriques ne sont pas a
base de connaissances et ne seront pas présentées.

3.1 A base de traits
3.1.1 Similarité de Tsversky

Avant d’étre abordée en TALN, la notion de similarité sémantique a été traitée dans le domaine
de la psychologie cognitive. Un travail souvent cité est (Tversky, 1977) qui propose une nouvelle
approche basée sur le recouvrement ou non de traits er1tre deux objets. Plus précisément, est
considéré comme concept ou signiﬁcation rattachée a un objet toute propriété dudit objet. La
similarité entre deux objets est exprimée comme le nombre pondéré de propriétés en commun,
auxquelles on retire le nombre pondéré de propriétés spéciﬁques a chaque objet. Il propose donc
un modéle de similarité non symétrique, que l’on appelle «modéle de contraste» (Figure 1).

 

FIGURE 1 — Le modéle de contraste entre deux concepts

Plus formellement, si l’on reprend la notation de (Pirré et Euzenat, 2010) oil \II(c) est l’en-
semble des traits se rapportant a un sens $, alors la similarité de Tsversky peut s’exprin1er par :
$im1V,($1,$2) = 9F(\II($1) F1 \II($2)) — aF(\II($1) \ \II($2)) — [5F(\II($2) \ \II($1)) ou F est une fonction
qui associe une pertinence aux traits, et oil 9, a et [5 sont des facteurs qui marquent respective-
ment l’in1portance relative de la similarité entre les sens, des dissimilarités entre $1 et $2 et des
dissimilarités entre $2 et $1, et ou \ est 1’opérateur de différence ensembliste.

Il est également possible d’exprimer cette mesure en tant que rapport, aﬁn d’avoir une valeur de
similarité normalisée (avec 9 = 1) :,

F(‘1’(C1) 0 ‘I/(C2))
F01/(C1) 0 ‘1'(C2))+ aF(\1/(C1)\ \1’(C2))+ /5F(\1/(C2)\\1/(C1))

simtVtr(C1! C2) :

Comme récapitulé dans (Pirré et Euzenat, 2010), différentes valeurs de a et de [5 menent a
différents types de similarité. Si a = [5 = 0, on ne s’intéresse qu’aux points communs er1tre les
deux sens. Si a > [5 ou a < [5 alors on s’intéresse assyrnétriquement a la similarité de $1 avec $2
ou vice versa. Si a = [5 ;£ 0 on s’intéresse a la similarité mutuelle entre $1 et $2. Quand a = [5 = 1
la mesure de Tversky est équivalente a la similarité de Tanimoto (Rogers et Tanimoto, 1960).
Dans le cas oil a = [5 = 0.5 alors elle est équivalente au coefficient de Dice (Dice, 1945).

297

3.1.2 Simjlarité de Lesk

(Lesk, 1986) a proposé un algorithme de désambiguisaﬁon lexicale trés simple, qui considére la
similarité entre deux sens comme le nombre de mots en commun dans leurs déﬁnitions. Dans
la version originale, on ne prend pas en compte l’ordre des mots dans les déﬁnitions (sac de
mots). Dans ce cadre la, il apparait que cette méthode puisse étre ramenée ‘a un cas particulier
de la similarité de Tsversky (en tant que rapport ou non), en considérant que les concepts sont
des sens de mots, que les traits sont des mots de la déﬁnition des sens, avec a = /3 = 0, et avec
\II(s) = D(d) qui retournant un ensemble contenant les mots de la déﬁniﬁon d’un sens de mot s.
Quant ‘a la fonction F on la choisit comme la fonction cardinalité d’ensemble. On obtient ainsi :
simlesk(51:52) =| D(-51) n D(-92) l

L’avantage de cette mesure de similarité est qu’elle est extrémement simple ‘a calculer, et ne
requiert qu’un dictionnaire. Dans le contexte de l’algorithme de Lesk original, la similarité était
calculée de maniere exhaustive entre tous les sens de tous les mots du contexte, il existe une
variante (Navigli, 2009) utilisée sur une fenétre de contexte autour du mot auquel appartient
le sens. Elle correspond au recouvrement entre la définition du sens et entre un sac de mot
contenant tous les mots des déﬁnitions des mots du contexte : Lesk”, =| contexte(w)n D(sWn) I.
Comme le met en avant (Navigli, 2009), un probléme important de la mesure de Lesk est qu’elle
est tres sensible aux mots présents dans la définition, et si certains mots importants manquent
dans les déﬁnitions utilisées, les résultats obtenus seront de qualité moindre. De plus si les
déﬁnitions sont trop concises (comme c’est souvent le cas) il est difﬁcile d’obtenir des distinctions
de similarité ﬁnes.

Cependant, un certain nombre d’améliorations de la mesure de Lesk ont été proposées.

3.1.3 Extensions de la mesure de Lesk

Tout d’abord, (Wi]ks et Stevenson, 1998) proposent de pondérer chaque mot de la déﬁnition par
la longueur de celle-ci afin de donner la méme importance a toutes les déﬁnitions, au lieu de
systématiquement privﬂégier les déﬁnitions longues.

Plus récemment (Banerjee et Pedersen, 2002) ont proposé la mesure de "Iesk étendu", qui
améliore Lesk de deux fagons. La premiére est l’incorporation des déﬁnitions des sens reliés par
des relations taxonomiques WordNet dans la définition d’un sens donné. La deuxieme est une
nouvelle maniére de calculer le recouvrement entre les mots des déﬁnitions.

Pour calculer le recouvrement entre deux sens, ils proposent de considérer le recouvrement entre
les déﬁnitions des deux sens mais aussi des déﬁnitions issues de différentes relations : hyper-
onymes (has—kind), hyponymes (kind—of), meronymes (part—of), holonymes (has—part),
troponymes mais aussi par les relations attribute, similar—to, also—see.

Aﬁn de garantir que la mesure soit symétrique, ils proposent de prendre les combinaisons deux a
deux des relations considérées et de ne conserver une paire de relations (R1,Rz) que si la paire
inverse (Rz,R1) est présente. On obtient ainsi un ensemble RELPAIRS. De plus, le recouvrement
entre deux déﬁnitions A et B se calcule comme la somme des carrés des longueurs de toutes les
sous-chaines de mots de A dans B, ce que l’on exprime avec l’opérateur rm. Nous avons ainsi :
Lesketendu(s1:s2) = Zv(R1,R2)eRELpA[R51  D(R1(s1)) rm D(R2(s2)) 

Le calcul du recouvrement est basé sur le principe relevé par la loi de Zipf (Zipf, 1949), qui met
en évidence une relation quadratique entre la longueur d’une phrase et sa fréquence d’occurrence
dans un corpus. De ce fait, n mots qui apparaissent ensemble portent plus d’informations que si
ils étaient séparés.

298

3.2 A base de distance taxonomique

Le principe des mesures a base de distance taxonomique est de compter le nombre d’arcs qui
séparent deux sens dans une taxinomie.

La Figure 2 (Wu et Palmer, 1994) représente la relation de deux sens quelconques S1 et S2 dans
une taxinomie par rapport a leur sens commun le plus spéciﬁque S3 et par rapport a la racine de
la taxinomie ; cette ﬁgure servira a exprimer de maniére homogéne les formules des différentes
mesures de similarité.

Racine
N3 1
N1 53 N2
fS1 52‘

FIGURE 2 — Deux sens et leur sens commun le plus spéciﬁque dans une taxinomie

La mesure de Rada (Rada et al., 1989) est la premiere a utiliser la distance entre les noeuds
correspondant aux deux sens sur les liens d’hyponymie et hyperonymie :

Sim-Rada(-91:52) = dl-91,52) = N1 +N2

Les termes se trouvant plus profondément dans la taxonomie étant toujours plus proches que les
termes plus généraux, (Wu et Palmer, 1994) proposent de prendre en compte la distance entre
l’ancétre commun le plus spéciﬁque et la racine pour y remédier.

S_ 2 ~N3
lm : T
W” N1 +N2 +2~N3
(Leacock et Chodorow, 1998) se basent également sur la mesure de Rada, mais au lieu de
normaliser par la profondeur relative de la taxinomie par rapport aux sens, ils choisissent une
normalisation par rapport a la profondeur totale de la taxinomie D et normalisent avec un
logarithme :
N1 + N2
Sim = -10 T
LCH gl 2 _ D )

(Hirst et St-Onge, 1998) adaptent le concept de chaines lexicales développées par (Morris et Hirst,
1991) comme mesure de similarité sémantique en utilisant la structure de WordNet. Cette mesure
se base sur l’idée de (Halliday et Hasan, 1976) que dans un texte, des mots ont une forte proba-
bilité de référer a des mots antérieurs ou a d’autres concepts reliés, et que Fenchainement de ces
mots forment des chaines cohésives. Par exemple, (Navigli, 2009) Rome—>Ville—>habitant
et manger—>p1at—>1égume—>aubergine, sont des chaines lexicales.

A chaque relation est associée une direction horizontale, ascendante ou descendante, qui marque
respectivement une relation forte, trés forte et moyennement forte (par exemple l’hyperonymie

299

est une relation ascendante, l’holonymie une relation descendante, et l’antonymie une relation
horizontale). Les changements de direction constituent un élément de dissimilarité et la proximité
dans la taxinomie un élément de similarité. Un changement de direction est déﬁni comme le
passage d’un élément A de la taxinomie a un élément B par une relation d’un autre type que celle
qui a permis d’arriver sur A. Notons que plus la distance entre les sens est grande, plus il y aura
de changements de direction potentiels.

Méme si l’algorithme proposé est un algorithme global, il est possible d’utiliser la fonction
d’évaluation des chaines lexicales en tant que mesure de similarité.

Soient C et k deux constantes et soit la fonction virages(s1,s2) qui retoume le nombre de
changements de direction entre les sens s1 et sz, alors la mesure de similarité s’exprime :

SimHs,, = C — (N1 +N2) — k - virages(s1,s2)

Il existe également d’autres mesures exploitant la structure taxonomique, mais en pondérant les
arcs avec des valeurs de contenu informationnel, notion que nous allons maintenant déﬁnir.

3.3 A base de contenu informationnel

L’approche de (Resni.k, 1995) est basée sur la détermination du contenu informationnel du
concept commun le plus spéciﬁque ‘a deux sens. Dans la ﬁgure 2, le concept commun le plus
spéciﬁque a S1 et S2 est S3 (notée lso(S1,S2) = S3 pour lowest superordinate). Quant a la quantité
d’information, elle est calculée a par11'r de la probabilité p(S3) : I C(s) = —log(P(S3))

Les probabilités d’occurrence de chaque concept de la taxinomie sont calculées ‘a partir d’un
corpus non-annoté par estimation du maximum de vraisemblance. Ainsi la mesure similarité de
Resnik s’exprime :

Simm = IC(lso(S1,S2)) = IC(S3)

(Jiang et Conrath, 1997) partent du constat qu’ut1'liser seulement l’ancétre commun n’offre pas
une granularité assez ﬁne et proposent de prendre en compte la quantité d’information portée
par les deux sens. Cette mesure s’exprime :

SimJCN = IC(S1) + IC(Sz) + 2 - IC(lso(S1,S2))

(Lin 1998) propose également un mesure de similarité trés proche, qui revient essentiellement a
une reformulation sous forme de rapport de la formule de Jiang and Conrath :

Sim _ _ 2IC(lso(S1,S2))
“" “ 1c(s1) +1c(s2)

Plus récemment, (Seco et al., 2004) ont argumenté qu’il est possible d’extraire directement
de WordNet les valeurs de contenu informationnel sans avoir a passer par un corpus. Ia taxi-
nomie de WordNet étant structurée a partir de principes psycholinguistiques, on peut faire
l’hypothese que cette structure (liens d’hyperonymie et d’hyponymie) est représentative du
contenu informationnel; c’est-a-dire que les concepts qui ont beaucoup d’hyponymes portent
une quantité d’information moins importante que les concepts feuilles. Si l’on note hypo(s)
une fonction qui retourne le nombre d’hyponymes d’un concept s et maxwn le nombre total

300

de concepts dans la taxinomie, alors on peut exprimer le contenu informationnel intrinséque :
- _ _ lag(hy1w(s)+1)

lIC(s) _ 1 lag(maxw..)

On peut substituer iI C ‘a I C dans toutes les mesures précédentes pour supprimer le besoin
d’apprentissage non supervisé. Cependant, 1’iIC est limitée car, elle n’exploite qu’un seul type de
relations, alors que d’autres pourraient étre intéressantes. C’est pourquoi (Pirro et Euzenat, 2010)
proposent une mesure d’iI C étendue qui va prendre en compte les autres relations présentes
(meronymie par exemple). Ils l’expriment comme : eIC(s) = §iIC(s) + nEIC(s)

oil 5' et 1) sont des constantes et o1‘1 EIC(s) est la somme des iI C moyennes des concepts reliés
‘a s par les autres relations. Si l’on note Rels(s) l’ensemble des relations possibles pour s, et si
pour une relation s, e Rels(s), s,(s) est l’ensemble des concepts reliés a s par s,, alors on obtient :

_ Zcesyrs) “Cm
EIC(s) _ Zs,Ekls(s)  
Il est également possible de substituer eI C 21 I C.

Plus récemment ont commencé a apparaitre des mesures hybrides qui, soit essayent de combiner
différents types de mesures de similarité, soit essayent d’exploiter la structure de plusieurs
ontologies (cross-ontology similarity).

3.4 Mesures hybrides

Ici, nous nous focalisons sur l’aspect combinaisons de mesures plutét que sur les approches ‘a
ontologies croisées.

(Li et al 2003) proposent une mesure qui combine a la fois 1a distance taxonomique (l = N1 +N2),
la profondeur du concept commun le plus spéciﬁque dans la taxinomie (h = N3) ainsi que
la densité sémantique locale(d = I C(lso(s1,s2))) , Cette derniére étant exprimée en terme de
contenu informationnel. Leur mesure est exprimée par : SimL,-(s1,s2) = f ( f1(l), fz(h), f3(d))
o1‘1 f1, f2 and f3 sont les fonctions de transfert non-linéaires respectives pour chaque type
d’information.

Le but des fonctions de transfert est de normaliser dans l’intervaJle [0; 1] les mesures pour qu’elles
puissent étre combinées. f1(l) = e‘“l ou a est une constante. fz(h) = (e’"‘ — e“"‘) + (e’"‘ + e‘”")
oil /3 > 0 est un facteur de lissage. f3(d) = (em — e‘M) + (em + e‘’”) avec A > 0. Quant ‘a la
fonction f, elle constitue n’importe quelle combinaison de ces trois mesures, et est a choisir selon
les applications et la nature des sources d’informau'on disponibles.

FaI'IH, une autre mesure locale qui combine des aspects de différents types, est proposée par (Pirro
et Euzenat, 2010) sous la forme de 1’extension des mesures a base de contenu informationnel en
reprenant le modéle de contraste de Tsversky:

S. I C(lso(s1,sz))
1m =  
“NH IC(s1) +IC(sz) —IC(lso(s1,s2))
Ici la fonction F est remplacée par I C, les traits communs aux concepts sont représentés par
le contenu informationnel du concept commun le plus spéciﬁque, et les traits spéciﬁques ‘a un

concept sont représentés par la différence entre le contenu informationnel de ce concept auquel
on soustrait le contenu informationnel du concept commun le plus spéciﬁque.

301

4 Algorithmes globaux de désambiguisation lexicale

Maintenant que nous avons passés en revue les principales de mesures de similarité sémantique,
nous allons présenter différents algorithmes qui les utilisent comme heuristiques pour évaluer
des combinaisons de sens.

4. 1 Approche exhaustive

L’approche originellement adoptée par CLesk, 1986) pour désambiguiser un texte en entier, est
d’évaluer toutes les combinaisons possibles de sens et de choisir la combinaison qui maximise le
score du texte — exprimé comme la somme des scores des sens choisis par rapport aux autres
mots du texte.

En d’autres termes, si le sens sélectionné d’un mot w dans une combinaison C est SW et
un texte T une liste ordonnée de mots, alors le score de la combinaison est score(C) =
ZMET Ewe, sim(SWl_,SWj) et il y a en tout ]_[WeT NW combinaisons 5 évaluer, avec NW le nombre
de sens de w (Gelbukh et al., 2005), c’est-‘a-dire un nombre exponenﬁel de combinaisons.

Par exemple pour une phrase de 10 mots avec 10 sens en rnoyenne par mot il y aurait 101°2 =
101°° combinaisons.

Pour diminuer le temps de calcul on peut utiliser une fenétre autour du mot aﬁn de réduire le
temps d’évaluation d’une combinaison au prix d’une perte de cohérence globale de la désambi-
guisaﬁon.

Une autre approche est d’uti1iser des meta-heuristiques d’optimisation combinatoire pour obtenir
des solutions de qualité convenable d’une maniére qui soit traitable calculatoirement.

4.2 Recuit simulé

Le recuit simulé est une méthode d’optimisation stochastique classique, et fﬁt appliqué ‘a la
désambiguisation lexicale par (Cowie et al., 1992).

Le principe est de faire des changements aléatoires dans la conﬁgurationz itérativement de
l’espace de recherche puis d’évaluer si le changement est bénéﬁque. Dans le cas échéant, il est
conservé, sinon, il y a une probabilité de le conserver quand méme.

Cette évaluation se fait en utilisant une métrique heuristique, et dans le cas de la désambiguisation,
on utilise les mesures de similarité pour jouer ce role. Le score d’une conﬁguration se calcule de
la méme maniére que pour l’évaluau'on d’une combinaison dans le cas de l’approche exhaustive.

Quant 21 la probabilité de conserver une conﬁguration inférieure, elle se calcule par rapport 5 la
différence des scores entre la conﬁguration modiﬁée( C’) et la conﬁguration avant modiﬁcation
(C) avec As = score(C’)—score(C) et un paramétre de température T : P(conservation) = e%.
Dans le cas d’une descente par gradients o1‘1l’on ne garde que les meilleures conﬁgurations, on
est confronté 21 un probléme de convergence sur des maxima locaux. L’objectif de l’acceptation
possible des conﬁgurations inférieures pour le recuit simulé est de leur échapper. Cependant cela
pourrait mener 21 une non convergence du systéme, c’est pourquoi la diminution géométrique
de la température T permet progressivement de se ramener 5 une descente de gradient, dont la
convergence est garantie.

2. Une conﬁguration est représentée par un vecteur d’entiers correspondant aux numéros de sens sélectionnés pour
chaque mot dans1’ordre d’apparition des mots dans le texte.

302

4.3 Algorithme génétique

Les algorithmes génétiques sont inspirés de l’évolution génétique des espéces et sont utilisés pour
trouver des solutions a des problémes d’optimisation combinatoire.

(Gelbukh et aL, 2003) l’ont appliqué a la désambiguisation lexicale. La représentation de la
conﬁguration utilisée est la méme que pour le recuit simulé, cependant, on considere une
population de A conﬁgurations (chromosomes). Chaque indice du vecteur d’une conﬁguration
est considéré comme un allele, et les alleles possibles pour un indice sont les différents sens du
mot en question.

Le déroulement de l’algorithme est inspiré des cycles reproductifs et de sélection naturelle des
espéces. La qualite’ d’un individu est estimée avec une fonction de score heuristique, ici la méme
que pour le recuit simulé.

A chaque cycle, les scores de tous les individus sont calculés, et un nombre pair d’individus sont
scare(l,-)
scare,,,,,,,
o1‘1 scoremax est le meilleur score dans la population et o1‘1 Cr est un rapport de sélection.

sélectionnés de maniere probabiliste pour étre croisés : VA, 6 A, p(crois,1x_) = Cr * (

Le croisement s’effectue par une permutation autour d’un ou plusieurs points de pivots choisis au
hasard dans la conﬁguration, habituellement un ou deux. Les individus non retenus pour le croi-
sement sont dupliqués. On obtient ainsi une nouvelle population (qui remplace l’ancienne). Sur
chaque individu, on applique avec une probabilité p(M), Mn changements aléatoires uniformes.
Le score de la nouvelle population est calculé aprés 1a phase de mutation, puis un nouveau cycle
commence.

Parmi les stratégies de convergence, on trouve un nombre ﬁxe de cycles o1‘1 encore une sta-
bilisation de la distribution des scores de la population pendant plusieurs cycles successifs.

4.4 Chaines lexicales

Comme décrit dans la Section 3.2 (Hirst et St-Onge, 1998) est un algorithme global qui se
base sur la construction de chaines lexicales aﬁn d’évaluer les combinaisons en intégrant des
connaissances linguistiques pour réduire l’espace de recherche.

Ils placent tout d’abord des restrictions sur les enchainements de types de liens possibles : il est
impossible d’avoir plus d’un changement de direction; un lien ascendant est terminal, sauf si il
est suivi d’un lien horizontal faisant le lien avec un lien descendant.

La chaine lexicale globale est construite dans l’ordre des mots du le texte. Lorsqu’un mot est inséré
(présent dans le texte, ou transitivement par une relation), un certain nombre de ses synsets lui
sont reliés : si c’est le premier mot de la chaine ou si le mot provient d’une relation trés forte alors
on garde tous les synsets ; quand il provient d’un lien fort, alors on inclut seulement les synsets
qui lui sont reliés par des liens forts; et quand le mot provient d’une relation moyennement forte
alors on ne considére que les synsets avec le meilleur score (avec leur mesure locale).

Tous les synsets qui ne participent pas a une relation selon les critéres précédents sont supprimés.
De plus, au fur et a mesure que des mots sont ajoutés a la chaine, et si elle devient illégale, tous
ses synsets sont supprimés.

Par ailleurs quand on insére un mot, on cherche d’abord parmi les relations par ordre décroissant
de force et dans un contexte de plus en plus petit (respectivement, toutes les phrases, sept

303

phrases, trois phrases) si une chaine existe déja, auquel cas le mot y est ajouté; dans le cas
contraire une nouvelle chaine est créée.

Le probléme de cette méthode est qu’elle est peu précise a cause de sa nature gloutonne (Navigli,
2009) et différentes améliorations ont été proposées. On peut citer (Barzilay et Elhadad, 1997)
qui gardent toutes les interprétations possibles, ce qui augrnente la précision au détriment des
performances. (Silber et McCoy, 2000) proposent un algorithme de construction de chaines
lexicales linéaire, qui permet de résoudre le probleme de performance tout en conservant la
qualité accrue.

4.5 Algorithme :31 base d’exploration pseudo aléatoire de graphes

D’autres algorithmes sont ceux qui se basent sur le principe d’une marche aléatoire dans un
graphe, ce qui inclut a la fois des algorithmes de type PageRank, mais aussi des méta-heuristiques
a colonies de fourmis.

4.5.1 Algorithme a base de PageRank

(Mi.halcea et al. 2004) ont appliqué l’algorithme de PageRank (Brin et Page, 1998) pour la
désambiguisation lexicale. Le principe est d’assigner des poids aux arcs d’un graphe récursivement
en exploitant les informations globalement disponibles.

(Mihalcea et al. 2004) utilisent WordNet et ses relations pour construire un graphe dirigé 3 ou
non représentant les différentes combinaisons de sens ‘a partir du texte ‘a désambiguiser. Pour
chaque mot, l’ensemble des synsets qui lui sont liés constituent les noeuds du graphe, alors que
les arcs sont les relations issues de Wordnet (ou d’une combinaison de relations) entre les synsets
des mots du texte. A noter que les synsets du méme mot ne sont jamais reliés entre eux.

Une fois le graphe construit, un poids est associé a chaque arc. Le choix des poids initiaux peut
se faire de deux maniéres : initialisés a 1, ou par une mesure de similarité sémantique.

S’en suit la marche aléatoire du PageRank. Le marcheur choisit l’arc ‘a emprunter de maniere
aléatoire suivant la distribution des valeurs de PageRank des noeuds reliés au noeud courant. A
chaque passage sur un noeud, le score est mis ajour avec : S(N,-) = (1—d)+d*Zjem(Ni)  , ou
d est un facteur de lissage, Ni un noeud du graphe, I n(N,-) l’ensemble des noeuds prédécesseurs de
Ni et Out(N,-) l’ensemble des noeuds successeurs. Lorsque le systéme a convergé sur la distribution
stationnaire, le sens de chaque mot correspondant au noeud avec le meilleur score PageRank est
sélectionné le sens.

4.5.2 Algorithme 51 colonies de fourmis

Les algorithmes ‘a colonies de fourmis sont des algorithmes multi-agents réactifs qui cherchent
‘a imiter le fonctionnement d’une colonie de fourmis. Cette approche fut initialement proposée
par (Dorigo, 1992), et part de la constatation faite par (Deneubourg et al., 1983) que les
fourmis, lorsque plusieurs chemins sont possibles pour atteindre de la nourriture, convergent
systématiquement vers le chemin le plus court. Lors de leur passage, les fourmis déposent une
substance chimique (phéromone) pour alerter les autres fourmis de la présence de nourriture;

3. Sachant que les relations dans Wordnet son syméttiques, on retiens arbitrairement soit la relation, soit son inverse

304

cette substance s’evapore si elle n’est pas renforcee par le passage d’autres fourmis. C’est cette
communication indirecte au travers de l’environnement (stigmergie) qu’emerge une convergence
optimale du systeme. L’utilisation d’un tel algorithme pour la desambiguisation lexicale a ete
proposee par (Guinand et Lafourcade, 2010) en utilisant un modele de similarite a base de
vecteurs pour regir le deplacement des fourmis. Plus recemment (Schwab et al., 2011) ont
propose de remplacer ce modele par l’utilisation d’une approche basee sur la mesure de Lesk
etendu.

 

Johnis hur\t_‘~~-__,/” He fell down.

FIGURE 3 — La structure de l’environnement de l’algorithme a colonies de fourmis

Le graphe, contrairement a celui de (Mihalcea et al., 2004), ne lie pas les sens entre eux, mais
reprends une structure d’arbre qui suit celle du texte4.

Nous pouvons voir sur la Figure 3 un exemple de graphe pour une phrase simple. La racine est
un noeud correspondant au texte. Les noeuds ﬁls sont des noeuds correspondant aux phrases ;
leurs noeuds ﬁls correspondant aux mots les feuilles des mots correspondent aux sens. Ces noeuds
produiront les fourmis (fourmilieres). Au depart il n’y a aucune connexion entre les noeuds “sens”
des differents mots, ce sont les fourmis qui vont creer des “ponts” entre eux. Chaque noeud qui
n’est pas une fourmiliere possede un vecteur de sens qui lui est attache (vecteur contenant des
identiﬁants de sens WordNet).

Les noeuds fourmilieres possedent une quantite d’energie qu’elles peuvent utiliser pour produire
des fourmis a chaque iteration de l’algorithme.

Les fourmis, partent a l’exploration du graphe de maniere pseudo aleatoire. La probabilite
de prendre un chemin depend de la quantite d’energie sur le noeud, de la concentration de
pheromone, et du score entre le noeud (son vecteur de sens) ou elle se trouve et sa fourmiliere
d’origine 5.

Quand une fourmi arrive sur un noeud, elle preleve une quantite d’energie et a une probabilite
dependant de la quantité d’energie qu’elle porte de passer en mode retour pour rapporter l’energie
a sa fourmiliere.

Lorsqu’une fourmi passe sur un noeud non fourmiliere elle va déposer 1e sens correspondant a sa
fourmiliere dans le vecteur de sens du noeud ainsi qu’une quantité de pheromone. La pheromone
s’evapore en partie a chaque iteration.

Si une fourmi arrive sur le noeud d’un autre sens que le sien, il y a une probabilite (dependant du
score avec son sens d’origine) qu’elle construise un “pont” vers sa fourmiliere aﬁn d’y revenir
directement. Lorsque la fourmi passe par un pont elle depose egalement des pheromones, ce qui
pourra inciter d’autres fourmis a la suivre.

4. On conserve ainsi la proximite et l’ordre des mots par exemple
S. 51 l’aide de mesures de similarite locales.

305

Lorsque de nombreux ponts ont été construits, certains ponts vont se renforcer et d’autres
s’évaporer (lorsqu’il n’y aura plus de phéromone) ; cela va mener ‘a une monopolisation des
ressources au niveau des fourmiliéres avec les ponts les plus fréquentés. Les ponts correspondant
ainsi ‘a des chemins interprétatifs parmi les combinaisons de sens possibles. A la ﬁn de la
simulation les sens qui correspondent aux fourmiliéres avec le plus d’énergie sont choisis.

5 Critéres de choix des algorithmes

Le choix de la mesure de similarité a utiliser dépends d’une part des contraintes sur les ressources
lexicales disponibles mais aussi du contexte applicatif : certaines seront plus adaptées car relevant
mieux de certains aspects plutot que d’autres de la similarité sémantique réelle. (Budanitsky et
Hirst, 2006) proposent une comparaison empirique de 5 mesures par rapport au jugement humain
de maniére trés détaillée, ce qui peut constituer un élément de choix utile. Plus récemment (Pirré
et Euzenat, 2010), entreprennent également de comparer leur mesure a la plupart des mesures
classiques.

Quant aux algorithmes globaux, il y a deux aspects a considérer, d’une part l’évaluation de
la qualité des solutions, et d’autre part le temps d’exécution de l’algorithme. Les taches de
désambiguisation des campagnes d’évaluation telles que Semeval (anciennement SenseEval), ne
sont axées que sur l’évaluation de la qualité par rapport a une désambiguisation de référence du
corpus faite manuellement. Elles fournissent cependant un premier élément de comparaison.

D’une part on peut discuter de la valeur d’une telle évaluation de la qualité dans un systéme
appliqué ‘a un probléme réel, et d’autIe part de la vitesse d’exécution qui est un facteur trés
important pour des applications telles que la traduction automatique, surtout si il s’agit de
traduction de parole a parole (o1‘1 il y a un besoin de traitement en temps réel).

(Schwab et al., 2012) ont entrepris de comparer le recuit simulé, l’algorithme génétique ainsi que
l’algorithme a colonie de fourmis a la fois en termes de qualité (Semeval 2007 — Téiche 7), mais
également en terme de convergence et de vitesse d’exécution en utilisant comme mesure locale
Lesk étendu. Ils concluent que les trois algorithmes avec la méme mesure de similarité locale
offrent des résultats en terme de qualité comparables ; c’est cependant l’algorithme a colonie de
fourmis qui s’avére le plus rapide (environ 10 fois plus que le recuit simulé et 100 fois plus que
l’algorithme génétique).

6 Conclusions et perspectives de recherche

Nous avons passés en revue les principales méthodes de désambiguisation lexicale basées sur
des connaissances, que ce soit les mesures au niveau local ou les algorithmes au niveau global.
D’un point de vue local, les mesures de similarité sémantique sont bien entendu trés uﬁles pour
les systémes de TALN, mais elles jouent également un role de plus en plus important pour des
applications au web sémantique, et également pour la construction automatisée de ressources
lexicales. La recherche se focalise principalement d’une part sur l’hybridation et la combinai-
son des mesures de similarité, mais également sur la combinaison de sources d’information
(multilingues ou non), ou encore des ontologies croisées.

Du point de vue global une possibilité d’amélioration se situe au niveau du temps de convergence
et des performances en général. Nous nous intéressons surtout aux combinaisons de mesures de

306

similarité. Ces combinaisons peuvent se faire en utilisant des mesures différentes pour essayer
de capter différents aspects utiles a la désambiguisation. Par exemple, on peut imaginer utiliser
une mesure par catégorie lexicale, utiliser différentes mesures exploitant différentes sources
d’information ou adopter une stratégie de vote, ou enﬁn, au niveau de l’algorithme a colonies
de fourrnis, utiliser différentes castes de fourrnis, chacune utilisant une mesure de similarité
différente pour ses déplacements.

Références

BANERJEE, S. et PEDERSEN, T. (2002). An adapted lesk algorithm for word sense disambiguation
using wordnet. In CICLing ’02, pages 136-145, London, UK.

BARZILAY, R. et ELHADAD, M. (1997). Using lexical chains for text summarization. In In
Proceedings of the ACL Workshop on Intelligent Scalable Text Summarization, pages 10-17.
BRIN, S. et PAGE, L. (1998). The anatomy of a large-scale hypertextual web search engine. In
WWW7, pages 107-117, Amsterdam.

BUDANITSKY, A. et HIRST, G. (2006). Evaluating wordnet-based measures of lexical semantic
relatedness. Computational Linguistics, 32(1): 1347.

COLLINS (1998). Cobuild English Dictionary. Harper Collins Publishers.

COWIE, J., GUTHRIE, J. et GUTHRIE, L. (1992). Lexical disambiguation using simulated annealing.
In COLING ’92, pages 359-365, Stroudsburg, PA, USA. ACL.

DENEUBOURG, J. L., PAsrEELs, J. M. et VERHAEGE, J. C. (1983). Probabilistic behaviour in ants : a
strategy of errors ? Journal of Theoretical Biology, 105:259-271.

DICE, L. R. (1945). Measures of the amount of ecologic association between species. Ecology,
26(3) :297-302.

DORIGO, M. (1992). Optimization, Learning and Natural Algorithms. These de doctorat, Politec-
nico di Milano, Italie.

GELBUKH, A., SIDOROV, G. et HAN, S.-Y. (2003). Evolutionary approach to natural language
word sense disambiguation through global coherence optimization. WSEAS Transactions on
Communications, 1 (2) : 1 1-19.

GELBUKH, A., S1DoRov, G. et HAN, S.-Y. (2005). On some optimization heuristics for lesk-like
wsd algorithms. In NLDB’05, pages 4024105, Berlin, Heidelberg.

GUINAND, E et LAFOURCADE, M. (2010). Artiﬁcial ants forNatural Language Processing, chapitre 20,
pages 455-492. Artiﬁcial Ants. From Collective Intelligence to Real-life Optimization and Beyond.
Monmarché, N. and Guinand, F. and P. Siarry.

HALLIDAY, M. A. et HAsAN, R. (1976). Cohesion in English. I.ongman Group Ltd, London, U.K.
Hmsr, G. et Sr-ONGE, D. D. (1998). Lexical chains as representations of context for the detection
and correction of malapropisms. WordNet : An electronic Lexical Database. C. Fellbaum., pages
305-332. Ed. MI'I' Press.

IDE, N. et VERONIS, J. (1998). Word sense disambiguation : The state of the art. Computational
Linguistics, 24:140.

JIANG, J. J. et CONRATH, D. W. (1997). Semantic Similarity Based on Corpus Statistics and Lexical
Taxonomy. In ROCLING X.

307

LEACOCK, C. et CHoDoRow, M. (1998). Combining local context and wordnet similarity for
word sense identiﬁcation. WordNet : An Electronic Lexical Database. C. Fellbaum. Ed. MIT Press.
Cambridge. MA.

LEs1<, M. (1986). Automatic sense disambiguation using machine readable dictionaries : how to
tell a pine cone from an ice cream cone. In SIGDOC ’86, pages 24-26, New York, NY, USA. ACM.
MIHALCEA, R., TARAU, P. et FIGA, E. (2004). Pagerank on semantic networks, with application to
word sense disambiguation. In COLING ’04, Stroudsburg, PA, USA. ACL.

MILLER, G. A. (1995). Wordnet : a lexical database for english. Commun. ACM, 38(11):39-41.
MoRR1s, J. et HIRST, G. (1991). Lexical cohesion computed by thesaural relations as an indicator
of the structure of text. Comput. Linguist., 17(1) :21Jr8.

NAVIGLI, R. (2009). Word sense disambiguation : A survey. ACM Cornput. Sum, 41 (2):10 :1-
10 :69.

PIRRO, G. et EUZENAT, J. (2010). A feature and information theoretic framework for semantic
similarity and relatedness. In ISWC 201 0, volume 6496 de Lecture Notes in Computer Science,
pages 615-630.

RADA, R., MILI, H., BICKNELL, E. et BLETTNER, M. (1989). Development and application of a
metric on semantic nets. IEEE Transactions on Systems, Man, and Cybernetics, 19(1):17—30.
REsN1K, P. (1995). Using information content to evaluate semantic similarity in a taxonomy. In
IJCAI’95, pages 448-453, San Francisco, CA, USA.

ROGERS, D. et TANIMOTO, T. (1960). A computer program for classifying plants. Science,
132(3434):1115—1118.

ROGET (1989). New Roget’s Thesaurus. P.S.I.

SCHWAB, D., GOULIAN, J. et GUILLAUME, N. (2011). Désambiguisation lexicale par propagation
de mesures sémantiques locales par algorithmes ‘a colonies de fourrnis. In TALN, Montpellier
(France).

SCHWAB, D., GoUL1AN, J. et TCHECHMEDJIEV, A. (2012). Comparaison théorique et pratique
d’algorithmes d’optimisation globaux pour la désambiguisation lexicale non supervisée. Traite-
ment Automatique des Langues, 1(53) :37 pages. Soumis a la revue Traitement Automatique des
Iangues.

SECO, N., VEALE, T. et HAYES, J. (2004). An intrinsic information content metric for semantic
similarity in wordnet. In Proceedings of ECAI’2004, pages 1089-1090, Valencia, Spain.

SILBER, H. G. et MCCOY, K. F‘. (2000). Efficient text summarization using lexical chains. In IUI
’00, pages 252-255, New York, NY, USA. ACM.

TVERSKY, A. (1977). Features of similarity. Psychological Review, 84(4):327—352.

WAGNER, C. (2008). Breaking the knowledge acquisition bottleneck through conversational
knowledge management. Information Resources Management Journal, 19(1) :70—83.

WILKS, Y. et STEVENSON, M. (1998). Word sense disambiguation using optimised combinations
of knowledge sources. In COLING ’98, pages 1398-1402, Stroudsburg, PA, USA. ACL.

WU, Z. et PALMER, M. (1994). Verbs semantics and lexical selection. In Proceedings of the 32nd
annual meeting on ACL, volume 2 de ACL ’94, pages 133-138, Stroudsburg, PA, USA. Association
for Computational Linguistics.

ZIPF, G. K. (1949). Human Behavior and the Principle of Least Effort. Addison-Wesley (Reading
MA).

308

