De l'utilisation du dialogue naturelppur masquer les
QCM au se1n des Jeux serleux

Franck Dernoncourtl
(1) LIP6, 4 place Jussieu, 75005 Paris
franck.dernoncourt@lip6.fr

RESUME

Une des principales faiblesses des jeux sérieux 21 l‘heure actuelle est qu‘ils incorporent
tres souvent des questionnaires 21 choix multiple (QCM). Or, aucune étude n‘a démontré
que les QCM sont capables d‘évaluer précisément le niveau de compréhension des
apprenants. Au contraire, certaines études ont montré expérimentalement que permettre
21 l‘apprenant d‘entrer une phrase libre dans le programme au lieu de simplement cocher
une réponse dans un QCM rend possible une évaluation beaucoup plus ﬁne des
compétences de l‘apprenant. Nous proposons donc de concevoir un agent
conversationnel capable de comprendre des énoncés en langage naturel dans un cadre
sémantique restreint, cadre correspondant au domaine de compétence testé chez
Fapprenant. Cette fonctionnalité est destinée 21 permettre un dialogue naturel avec
l‘apprenant, en particulier dans le cadre des jeux sérieux. Une telle interaction en
langage naturel a pour but de masquer les QCM sous-jacents. Cet article présente notre
approche.

ABSTRACT
Of the Use of Natural Dialogue to Hide MCQs in Serious Games

A major weakness of serious games at the moment is that they often incorporate
multiple choice questionnaires (MCQs). However, no study has demonstrated that MCQs
can accurately assess the level of understanding of a learner. On the contrary, some
studies have experimentally shown that allowing the learner to input a free-text answer
in the program instead of just selecting one answer in an MCQ allows a much ﬁner
evaluation of the learner's skills. We therefore propose to design a conversational agent
that can understand statements in natural language within a narrow semantic context
corresponding to the area of competence on which we assess the learner. This feature is
intended to allow a natural dialogue with the learner, especially in the context of
serious games. Such interaction in natural language aims to hide the underlying MCQs.
This paper presents our approach.

MOTS-CLES : Agent conversationnel éducatif, intelligence artiﬁcielle, jeu sérieux,
questionnaire 21 choix multiple, systeme d‘évaluation de réponses libres.

KEYWORDS: Educational conversational agent, artiﬁcial intelligence, serious game,
multiple-choice questionnaire, automatic assessment of free-text answer.

Acres de la conférence conjointe JEP-TALN-RECITAL 2012, volume 3: RECITAL, pages 323-336,
Grenoble, 4 an 8 juin 2012. ©2012 ATAIA 8: AFCP

323

1 Introduction

Nous déﬁnirons dans cette premiere partie les concepts clés de l‘article, nommément le
contexte des jeux sérieux ainsi que les agents conversationnels qui constitue la solution
que nous explorons pour répondre a la problématique de masquage des QCM.

1 .1 Les jeux sérieux

Les jeux sérieux correspondent a une approche de l‘apprentissage qui utilise des moyens
ludiques. L‘apprentissage peut se situer aussi bien dans le cadre d‘une formation que
dans un contexte de sensibilisation ou de communication (Thomas, 2004). Le marché des
jeux sérieux présente une croissance exponentielle : atteignant déja 1 milliard de dollars
en 2004 (Sawyer, 2004), les spécialistes l‘estimaient a environ 10 milliards de dollars en
2010.

Dialoguer avec un agent virtuel contribue a maintenir l‘attention et la motivation du
joueur dans un jeu sérieux. Actuellement, ce dialogue, que ce soit dans les jeux sérieux
ou dans les jeux video de type récit (storytelling) ainsi que dans la plupart des
environnements informatiques pour l’apprentissage humain, est constitué de QCM : le
joueur interagit donc avec le jeu avec des QCM, qui font ofﬁce de dialogue.

Le dialogue est donc tres contraint, réduisant ainsi l’apprentissage du joueur qui peut se
contenter de cliquer sur une des possibilités sans véritablement réﬂéchir. Nous pensons
que des systemes de dialogue davantage ﬂexibles peuvent constituer une réponse
pertinente a ce probleme.

1.2 Les agents conversationnels

Un dialogue est une activité verbale qui fait intervenir au moins deux interlocuteurs
servant a accomplir une tache ou simplement échanger des mots dans une situation de
communication donnée. Il constitue une suite coordonnée d’actions (langagieres et
non-langagieres) (Vernant, 1992).

L’idée d‘une interaction homme-machine se basant sur le fonctionnement du langage
naturel n’est pas nouvelle : elle a vu le jour dans les années 1950 avec le test de Turing.
Néanmoins, cette problématique, aux niveaux conceptuel et pratique, demeure toujours
d’actualité. Il existe, par exemple, des competitions annuelles comme le Loebner Prize
(Loebner, 2003) ou le Chatterbox Challenge visant a réussir un test de Turing en imitant
l’interaction verbale humaine, mais aucun programme n’est parvenu a ce jour a atteindre
le niveau d’un humain (Floridi et al., 2009).

Aﬁn de déﬁnir des criteres d’efﬁcacité des agents conversationnels, nous allons prendre
en compte les quatre criteres suivants pré-conditionnant l‘élaboration d‘un systeme
de dialogue intelligent et proposés par (Rastier, 2001) :

1. apprentissage : integration au moins temporaire d‘informations issues des propos
de l‘utilisateur ;

2. questionnement : demande de précisions de la part du systeme ;

3. rectiﬁcation: suggestion de rectiﬁcations a la question posée, lorsque

324

nécessaire ;

4. explicitationz explicitation par le systeme d‘une réponse qu‘il a apportée
précédemment.

Les agents conversationnels se divisent en deux classes principales :

° les agents conversationnels non orientés tache destinés a converser avec
l‘utilisateur sur n‘importe quel sujet avec une relation souvent amicale, tel
ALICE (Wallace, 2009) ;

° les agents conversationnels orientés tache, lesquels ont un but qui leur est
assigné dans leur conception.

Les agents conversationnels orientés tache sont eux-memes classés usuellement en deux
catégories :

° les agents conversationnels orientés service, par exemple fournir un service de
conseil sur un site Internet, telle l’assistante virtuelle Sarah de PayPal1 ;

° les agents conversationnels éducatifs, dont le but est d‘aider l‘utilisateur a
apprendre.

Notre travail se concentre sur les agents conversationnels éducatifs (tutor bots).

2 Etat de l'art

Apres avoir posé les deﬁnitions de base dans la partie précédente, nous exposerons ici
brievement l‘état de l‘art sur l‘architecture des agents conversationnels ainsi que sur les
systemes d‘évaluation des réponses libres plus en details.

2.1 Architecture d'un agent conversalionnel

La ﬁgure 1 montre un exemple d‘architecture d'un agent conversationnel. L‘utilisateur
entre une phrase que l’agent conversationnel convertit en un langage abstrait, ici
Artiﬁcial Intelligence Markup Language (AIML) : cette traduction permet d‘analyser le
contenu de la phrase et de faire des requétes via un moteur de recherche dans une base
de connaissances. La réponse est générée via un langage abstrait, ici également AIML,
qu‘il faut traduire en langage naturel avant de la présenter a l‘utilisateur.

Néanmoins, cette architecture est rudimentaire et tres rigide. Il faut par exemple
souvent mettre a jour la base de connaissances pour y inclure des connaissances sur
l‘utilisateur, notamment dans le cadre d‘une activité de tutorat qui nécessite le suivi des
acquis de l‘utilisateur ainsi que de sa motivation. Un certain nombre d’agents
conversationnels éducatifs ont déja été concus et implémentés, comme (Zhang et al.,
2009), (De Pietro et al., 2005), (Core et al., 2006), (Pilato et al., 2008) ou encore (Fonte
et al., 2009).

Diverses architectures ont été élaborées, voici les éléments communs a la plupart d’entre

1 https://www.paypal~virtualchat.com/

325

elles :
° une base de connaissances inhérente au domaine, objet de l’application ;
° un gestionnaire de répliques ;

° des structures de stockage des échanges sous forme d’arborescences surtout dans
les agents conversationnels éducatifs concus dans le cadre d‘un jeu Video.

Input de |'uti|isateur

iv

TutorBot
AIML
, . Moteur de H Base de
Theme Mom _> recherche connaissances
Template

Sortie en Iangage naturel

FIGURE 1 — Exemple d‘architecture d‘un agent conversatjonnel (TutorBot)
Source : (De Pietro et al., 2005).

Bien que sa simplicité d‘utjlisation ainsi que la performance relatjvement bonne des
agents conversationnels l’utilisant le rendent attrayant, AIML est un langage tres limité
qui peut se résumer a un simple ﬁltrage par motif, les motifs des inputs (phrases de
l‘utjlisateur) et des outputs (réponses de l‘agent conversatjonnel) étant déﬁnis en grande
partie par extension et a priori.

2.2 Systémes d'évaluation des réponses libres
En parallele de la recherche sur les agents conversationnels, beaucoup de travaux se
sont penchés sur l‘évaluation des réponses libres, c‘est-a-dire en langage naturel,

données par des apprenants. Ces travaux sont motives par les résultats expérimentaux
montrant les limites des QCM, en tant qu‘outjl d'évaluation de la connaissance des

326

apprenants (Whittjngton et Hunt, 1999), ainsi que sa complémentarité avec les réponses
libres (Anbar, 1991). Par connaissance, nous entendons ici et dans le reste de l‘article
non seulement la capacité a retranscrire des informations précédemment apprises, mais
également la capacité a opérer des raisonnements de base montrant la compréhension
du sujet.

Par exemple, (Anbar, 1991) a montré que les étudiants qui excellent lors des examens
oraux auront tendance a avoir des performances médiocres dans les QCM. Inversement,
les résultats aux QCM ne permettent pas de prédire les performances de l‘apprenant
dans le cadre d‘un dialogue en langage naturel.

Nonobstant ces limitations bien connues des QCM, ces derniers représentent toujours
l‘outil le plus utjlisé pour les évaluations des apprenants. Ce paradoxe s‘explique
simplement par le coﬁt beaucoup plus élevé des méthodes alternatives : s‘il est trivial de
corriger automatiquement les QCM, il n‘en va pas de meme des autres méthodes,
lesquelles nécessitent, étant données les techniques actuelles, des interventions humaines
longues, donc coﬁteuses.

L‘évaluatjon automatique des réponses libres a toutefois également ses détracteurs, qui
soulignent que le fait qu‘évaluer un essai est une tache par nature complexe et
subjective. Cependant, cette subjectivité ayant pour conséquence une variation de notes
non négligeable parmi les correcteurs humains, le systeme d‘évaluatjon automatique
pourra au moins étre consistant dans sa subjectivité.

Les premieres recherches sur l‘évaluatjon automatique apparurent il y a une
cinquantaine d‘années. Un des projets remarquables fut le Project Essay Grade, dirigé
par Ellis Batten Page a l‘université Duke (Page, 1968). Ses travaux se sont basés sur
l‘utjlisatjon des caractéristjques stylistjques de la réponse de l‘apprenant, tels la taille des
mots et le nombre de prépositions, pour prédire la note du correcteur humain. Dans ses
dernieres expériences (Page, 1995), ce systeme semble prédire la note du correcteur
humain plus précisément que ne le fait un second correcteur humain.

A la ﬁn des années 1980, une nouvelle technique a été développée en vue de mieux
saisir les concepts sous-jacents a un texte : l‘analyse sémantjque latente (LSA)
(Deerwester et al., 1988 ; Deerwester et al., 1990). Cette technique fut dans un premier
temps utjlisée dans le cadre de recherche de l‘informatjon ; elle ne fut que plus tard
appliquée a l‘évaluatjon des réponses libres. La LSA serait aisée a réaliser si un mot ne
correspondait qu‘a un seul concept, et inversement. Néanmoins, dans les langages
naturels, un mot peut avoir différentes signiﬁcatjons : un mot peut subséquemment faire
reference a différents concepts, faisant ainsi apparaitre une ambigu'1'té a l‘échelle du mot
La LSA utilise le contexte dans lequel le mot est utjlisé aﬁn de lever l‘ambigu'1'té,
autrement dit de comprendre a quel concept le mot fait référence dans le contexte
donné. Par exemple, le mot vol désigne tres certainement le concept de soustraire
frauduleusement le bien d‘autrui si le mot est utjlisé a proximité des mots bulin et
dérober. Par contre, si mot vol est proche des mots de ciel et oiseau, vol désigne alors
probablement un moyen de locomotion aérienne. La ﬁgure 2 illustre l‘objectif de la ISA.

La ISA ne prend pas en compte l‘ordre des mots, ni a fortiori les relations syntaxiques
ou logiques. En outre, elle peut s‘avérer assez coﬁteuse d‘un point de vue
computatjonnel. Malgré cela, des expériences ont montré que les scores de qualité

327

globale a un essai donnés par des experts sont moins précis que le score résultant d‘une
ISA (Landauer, 1998). Ce résultat surprenant est néanmoins a relatjviser au vu des
limitations de la ISA précédemment mentionnées et dépend évidemment des conditions
de l‘eXpérience.

Une approche totalement différente de la ISA a été adoptée par l‘Educatjonal Testing
Service (ETS). ETS est la plus grande organisation privée a but non lucratif de mesure et
d‘évaluation éducative au monde. Faisant passer plus de 20 millions d‘eXamens
annuellement (TOEFL, GRE, GMAT, etc.), ETS peut ainsi avoir acces a des corpus
considérables. Depuis plus d‘une vingtaine d‘années, son département R&D travaille sur
des solutions permettant de noter automatiquement les réponses des candidats. Apres
avoir essayé d‘utiliser la LSA aﬁn de classiﬁer les réponses (Burstein et al., 1996), ETS a
décidé de s‘en éloigner pour développer la technologie c-rater (Leacock et al., 2003), C
pour contenu, qui se focalise sur les réponses de petite taille, allant de quelques a une
centaine de mots. C-rater se base sur un pré-traitement de la réponse suivant
l‘architecture présentée a la ﬁgure 3. Ce pré-traitement permet de faire apparaitre dans
la réponse diverses caractéristiques linguistjques, tels les POS tags, les lemmes de
chaque mot ou la présence de négatjon. Ces caractéristjques linguistjques sont ensuite
utjlisées pour comparer la réponse du candidat avec une réponse modele a l‘aide d‘un
algorithme de détectjon des concepts nommé Goldmap. Dans un premier temps, Goldmap
était basé sur un ensemble de regles de ﬁltrage par motif déterminées de fagon binaire.
Bien que cela permettait de comprendre aisément les décisions, la binarité des regles
induisait un manque important de ﬂexibilité. Aﬁn de faire face a ce probleme, Goldmap
adopte a present une approche probabilistjque en se basant sur le principe d‘entropie
maximale pour la détectjon des concepts et en intégrant une dizaine de regles ad hoc.
Ies résultats obtenus semblent prometteurs selon leurs auteurs (Leacock et al., 2003).
Cependant, a notre connaissance il n‘eXiste pas a ce jour de test de performance
standardisé pour comparer les différents systemes d‘évaluation automatjque : il est donc
difﬁcile de comparer efﬁcacement les différents systemes.

Mot #1 e) Concept #1

Mot #2 Concept #2
Mot #3 Concept #3
Mot #n Concept #m

FIGURE 2 — Objectjf de la ISA : trouver les concepts auxquels les mots sont associés.

Outre la LSA et c-rater, il est intéressant de noter que beaucoup d‘artjcles soulignent les
apports potentiels de la traduction automatjque vers l‘évaluation de réponses libres. Un
des meilleurs exemples est la méthode BLEU (Papineni et al., 2001). Concue
originellement pour évaluer et classer les systemes de traduction automatjque, la

328

méthode BLEU a été appliquée avec succes a l‘éValuation de réponses libres. La méthode
repose sur la comparaison entre le texte candidat et un ensemble de textes modeles.
Appliquée a la traduction, le texte candidat correspond a la sortie du systeme de
traduction automatjque, et les textes modeles correspondent a des traductjons réalisées
par des experts humains. La note donnée par BLEU au texte candidat se base sur le
nombre de N-grammes communs entre le texte candidat et les textes modeles, ce qui
s‘aVere étre une mesure efﬁcace malgré sa simplicité, mais est toutefois tres sensible au
choix d‘écriture dans les textes modeles. Lorsque BLEU est appliqué a l‘éValuatjon de
réponses libres, le texte candidat correspond alors a la réponse de l‘apprenant, et les
textes modeles correspondent a des réponses types données par les professeurs.
Néanmoins, BLEU présente des limitations importantes, comme par exemple la mauvaise
gestjon des négatjons : une phrase niant un fait A aurait par exemple presque le méme
score qu‘une phrase afﬁrmant A.

Au-dela de la méthode BLEU, il est intéressant de remarquer que le domaine de la
traduction ainsi que de l‘éValuation cherche le meme idéal : trouver un formalisme dans
lequel les faits pourraient étre exprimés indépendamment de langage.

C-Rater Engine

The body could raise tempratuer

  
    

Item 8.Mode| fpemng Ce Tokenizer
orrector_/l

The bod#i:ou|d'raie-

L

temperature

ulti-sources U’
I of correctly
_ spelled words

PCl§V&AParser Np Vp

1

Tagged lexicon - Extractor Raise : v (could : m_d.
and annmated The body: (np.eubJ}] the : det, body : n])
Temperature: {np.obi) [temperature : n])

parsed data Pmnoun

RESOIVET The body — an animal's body
Np: [An : det, animal : n. '9: PS. body : n]

U
«-
a-r
Z
3
0 U1
EL
<
03
2
"U

 

Morphology
Lexicon

Morphology
Analyzer

Model including \
‘/ Ontology
Annotated Matching ’ N

Malchinﬂ A'El°Ti“'"'|'| Increase temperature
p corpora lGoldmap) Have a fever

FIGURE 3 — Architecture de c-rater. Source : Sukkarieh et al., 2009.

   

(animal : n. '5 : PS) : PS body

L L

   

L

329

3 Approche

Nous avons vu dans la partje précédente que beaucoup de travaux se sont penchés sur
les systemes d‘évaluation de réponses libres. Dans cette partje, nous mettrons en exergue
les partjcularités de notre approche, en partjculier les particularités afférentes a
l‘évaluatjon de la réponse libre a l‘aune du QCM sous-jacent ainsi qu‘a l‘environnement
de jeu sérieux.

3.1 Particularités des QCM

Nos travaux ont pour but de donner une note a la réponse de l‘apprenant. Dans notre
approche, nous nous distinguons des systemes d‘évaluation classiques de réponses libres
de par deux points majeurs :

° La réponse de l‘apprenant n‘est pas notée par rapport a des réponses modeles,
mais est reliée a un QCM sous-jacent ;

° Une interaction est possible avec l‘apprenant, car le systeme a la forme d‘un
agent conversationnel.

Ainsi, les recherches se sont penchées sur l‘évaluatjon de réponses libres mais a notre
connaissance aucune n‘a cherché a évaluer une réponse libre a l‘aune d‘un QCM sous-
jacent Nous allons donc élaborer des variantes aux techniques habituelles (LSA, BLEU
et c-rater) aﬁn de les adapter a l‘utjlisation de QCM.

L‘intérét d‘apparenter la réponse de l‘utilisateur a un QCM est multiple. D‘une part, de
nombreux tests d‘évaluation se présentent actuellement sous forme de QCM : nous
pourrions ainsi nous baser directement sur les tests existants. D‘autre part, la littérature
sur la génératjon automatique de QCM a partjr d‘ontologie est riche (Papasalouros et al.,
2008) : nous pourrons donc a terme avoir un systeme complet d‘évaluation directement
a partjr des ontologies, voire des supports de cours. Le QCM permet de faire la jonctjon
entre la base de connaissances que constjtue le cours et les tests donnés a l‘apprenant

Dans un QCM, l‘apprenant choisit une ou plusieurs réponses. Outre les choix corrects, il
existe également un certain nombre de choix incorrects. Ces choix incorrects permettent
de détecter la présence d‘erreur chez l‘apprenant de fagon active, c‘est-a-dire en vériﬁant
directement si la réponse ne contjent pas le choix incorrect Cette détection active des
erreurs est absente de la plupart des systemes d‘évaluation de réponses libres car ces
derniers ne reposent que sur la comparaison avec des phrases modeles. Nous pouvons
par conséquent identiﬁer ces erreurs alors que les systemes classiques ont tendance a les
ignorer.

Le fait que le systeme soit sous la forme d‘un agent conversationnel nous permet
naturellement de faire face plus aisément aux situations ou la réponse de l‘apprenant ne
réussit pas a étre directement évaluée par le systeme : via l‘agent conversationnel, une
nouvelle question pourra étre posée a l‘apprenant aﬁn de l‘inviter a reformuler ou
préciser sa réponse. Cette interaction avec l‘agent conversationnel peut étre comparée
aux tests oraux avec un examinateur humain et permet donc d‘éviter les inconvénients

émanant des examens écrits classiques qui sont par nature statjques.

330

3.2 Insertion dans un environnement ludique et sérieux

La simulation d‘un dialogue naturel avec le joueur dans un jeu vidéo date d‘une trentaine
d‘années. Le jeu d‘aVenture King's Quest I: Quest for the Crown développé par Sierra On-
Line et publié en 1984 figure parmi les pionniers dans le genre. Ce n‘est que récemment
que la modalité conversationnelle a été utilisée a des ﬁns pédagogiques, notablement
dans le jeu Facade (Mateas et al., 2005), que nous allons trés briévement présenter dans
le paragraphe suivant.

Dans Facade, le joueur est invité a un diner ou se déroule un conﬂit marital : l‘objectif du
joueur est de réconcilier le couple. Pour cela, le joueur entre des phrases de maniére
écrite, et les deux membres du couples répondent oralement. La ﬁgure 4 montre une
capture d‘écran dans laquelle le joueur demande a la femme Grace si elle se sent
énervée Vis-a-vis de son mari Trip. En interagissant ainsi avec le couple, le joueur
apprend a mieux comprendre les relations de couple.

 

FIGURE 4 — Capture d‘écran de jeu Facade. Le joueur interagit avec le couple.

Néanmoins, jusqu‘a présent, ce genre de systéme de dialogue repose essentiellement sur
le repérage de mots clés en fonction desquels le scénario du jeu s‘adapte et ne fait pas
appel a un QCM sous-jacent. Aﬁn de nous focaliser sur les aspects agent conversationnel
et QCM, nous intégrons notre systéme au sein de la plate-forme Learning Adventurez
(Carron, 2010).

Learning Adventure est un environnement ouvert en 3D, en ligne et multijoueur ou
l‘apprenant doit réaliser des quétes en réalisant diverses activités qui le font interagir
avec l‘environnement et les autres joueurs. L‘accent est mis sur le caractére immersif du
jeu, a l‘instar des MMORPG populaires actuels. L‘interaction avec les autres joueurs,
autrement dit avec les autres apprenants, est une dimension importante du jeu car elle
contribue grandement a la motivation du joueur : le QCM devient pas un jeu solitaire,
mais un jeu social, ou entrent alors les mécanismes classiques de motivation par les
pairs (Dickey, 2007) (Kim et al.,2009).

2 http://learning~adventure.eu

331

Outre la motivation résultant de cette collaboration et compétition entre les apprenants,
cette dimension multijoueur peut également donner l‘occasion pour un tuteur humain
d‘intervenir dans le jeu. Une telle intervention peut avoir plusieurs objectifs : aider les
apprenants dans les taches réputées difﬁciles, renforcer les relations éleves-professeur en
partageant un moment ludique, etc.

La modalité en ligne du jeu présente quant a elle de nombreux autres intéréts, en
particulier s‘assurer que le contenu pédagogique est a jour, suivre aisément l‘avancement
des différents apprenants et faciliter le déploiement de nouveaux contenus.

La ﬁgure 5 illustre un QCM qui apparait dans le cadre du jeu. La ﬁgure 6 présente
l‘éditeur de scénarii, qui permet notamment d‘ajouter et modiﬁer aisément des QCM
sans avoir aucune compétence informatique particuliere. Notre systeme a pour objectif a
terme de rendre le QCM invisible et d‘utiliser l‘éditeur de scénarii pour permettre a
l‘enseignant d‘inclure les QCM ainsi que les autres éléments du scénario pédagogique.

D : dans mon travail quotidie
1 H9 nu

    

FIGURE 5 — Capture d‘écran de la plate-forme Learning Adventure (Carron, 2010)

A l‘instar de c-rater d‘ETS, nous opérons un pré-traitement de la phrase de l‘apprenant,
comme le montre la ﬁgure 3: apres une étape liminaire de correction de surface
(orthographe, ponctuation, etc.), la réponse est analysée lexicalement, puis étiquetée
grammaticalement et parsée, avant de passer a l‘étape de désambigu'1'sation référentielle
et d‘analyse morphologique. Comme ces processus sont supposés étre connus chez le
lecteur, nous ne les détaillerons pas ici. Sur le plan technique, ces étapes (hormis la
premiere qui utilise le correcteur Hunspell) se basent essentiellement sur la librairie
open-source et gratuite NLTK (Bird et al. 2009) qui offre de nombreuses fonctionnalités
de traitement des langages.

Une fois la réponse pré-traitée, chaque mot de la réponse est décrit par un ensemble

332

d‘attributs. Nous pouvons alors la comparer avec les différentes réponses du QCM. Les
données extraites lors de la phase de pré-traitement contiennent de nombreuses
informations essentielles pour la comparaison, telle la présence de negation.

Aﬁn d‘accroitre la robustesse du systeme, nous envisageons de regrouper les résultats
issus de la comparaison se basant sur la réponse pré-traitée avec les algorithmes
statistiques ISA et BLEU. Cette comparaison multiple ainsi que les résultats obtenus
seront exposes en détail dans un futur article, le but du present article étant de présenter
globalement les objectifs, le contexte du probleme ainsi que la structure générale de
notre systeme.

Les agents conversationnels classiques n‘utilisent que tres peu les outils usuels du TALN,
comme le montre cet exemple suivant issu du site ofﬁciel de Cleverbot.com, qui est
pourtant un des agents conversationnels les plus réputés :

Human: Hi.

Cleverbot: Are you a boy?
Human: Yes I am a 135
Cleverbot: Are you a girl?

Dans cet exemple, nous voyons que la faute de frappe de l‘utilisateur qui a écrit << boys »
au lieu de << boy » a manifestement empéché l‘agent conversationnel Cleverbot de
comprendre la phrase et de proposer une suite cohérente au dialogue.

 ScenariiEditor - =1 I
Fichier Afﬁchage ‘.7
u I‘ 1 ’ Q
‘P’ ' - 1 ‘Exercises J Ressources j Activités
C 29 . IJ Questions
(3 28 . . I" QCM é réponses multiples v
‘ 3 27 '3 ‘ 'Un diagramme de Gantt est :
F3 26 O
C'- 25 It
‘3 24 CA
'3 23 I5
022 On
E 2 21 O
F3 20 O
C 19 O
<:;« 13 it
C: 17 .
=7? ‘I6 0
0
II
0
O
T?

-0 lnutiie dans u...
0 Utilisé en pro...
9 Une représen..
0 Une représen..

memo

C315

<1 14

C 13

C 12
Ajouter i

- ‘I Ii

etirer ' 3 Ajouter _ Retirer Ajouter , Retirer

FIGURE 6 — L‘éditeur de scénarii pour Learning Adventure

333

En restreignant le champ sémantjque et en précisant son objectif, nous pouvons ainsi
intégrer les techniques usuelles du TALN dans notre agent conversationnel aﬁn de
rendre transparents les QCM vis-a-vis de l‘apprenant.

Enﬁn, comme le montre (D’Mello et al., 2010), l‘apprentjssage par agent conversationnel
est amélioré lorsque la modalité est orale et non écrite. Par conséquent, nous utjlisons
Dragon NaturallySpeaking 11, qui est le leader de la reconnaissance vocale et édité par
la société Nuance, ainsi que le logiciel AT&T Natural Voices® Text-to-Speech pour
transmettre les réponses de l‘agent conversationnel sous forme orale. A noter que ces
deux logiciels ne sont pas libres.

4 Conclusions et perspectives

Cet article a présenté une approche nouvelle pour évaluer les apprenants en se basant
sur des QCM masqués par un agent conversationnel au sein d‘un jeu sérieux. La nature
interactive du dialogue peut apporter au systeme d‘évaluation une dimension nouvelle,
permettant notamment de faire des demandes de clariﬁcation (Purver et al., 2003).

Une des difﬁcultés dans la recherche de systemes d‘évaluation de réponses libres est
l‘absence de benchmarks, absence que certains expliquent par des raisons de propriété
intellectuelle (Sukkarieh et Blackmore, 2009). Quelles qu‘en soient les raisons, cette
lacune est génante pour la recherche dans le domaine.

Depuis quelques mois, trois initiatives majeures MITX, Coursera et Udacity ont été
lancées ; leur objectjf est de fournir aux internautes des cours en ligne gratuits, qui ont
déja attjré plus de 100 000 étudiants. Tous trois reposent en grande partje (en plus des
tests de programmatjon dans lesquels le code de l‘apprenant est évalué sur un jeu de
tests) sur des QCM pour évaluer les apprenants, a défaut de systemes plus efﬁcaces. Or,
ces QCM sont critiques comme étant une des limites de ce genre de cours en ligne dont
l‘évaluatjon est entjerement automatique afin de pouvoir garantjr la gratuité vis-a-vis
d‘un nombre important d‘apprenants. La demande de masquage des QCM est donc tres
importante et contjnuera de s‘accentuer par le nombre croissant de cours en ligne.

Au-dela des contextes d‘apprentjssage, un tel systeme pourrait également étre utilisé
dans d‘autres domaines comme l‘aide personnalisée, a l‘instar de celle fournie par les
centres d‘appel qui est en général tres scriptée, c‘est-a-dire suivant des scénarii tres peu
ﬂexibles, correspondant a un enchainement de QCM.

5 Remerciements

Je souhaite partjculierement remercier mon directeur de these Jean-Marc Labat pour ses
précieux conseils, indispensables a la réalisation de ce projet, ainsi que la DGA pour son
soutjen ﬁnancier. Je souhaite également remercier Thibault Carron pour ses nombreuses
idées ainsi que son aide sur Learning Adventure dont il est un des initjateurs.

334

6 Références

ALHADEFF, E. (2008). Reconciling Serious Games Market Size Different Estimates. In
Futurlab Business & Games Magazine - Numéro du 9 avril 2008.

BIRD, S., KLEIN, E. et LOPER, E. (2009). Natural Language Processing with Python. O’Reilly
Media.

BURSTEIN, J., KAPLAN, R., WOLFF, S. et LU, C. (1996). Using Lexical Semantic Techniques
to Classify Free-Responses. In Proceedings of SIGLEX 1996 Workshop, Annual Meeting of
the Association of Computational Linguistics, University of California, Santa Cruz.

CARRON T., MARTY JC. et TALBOT S. (2010). Interactive Widgets for Regulation in
Learning Games. The 10”’ IEEE Conference on Advanced Learning Technologies, Sousse,
Tunisia.

CORE, M., TRAUM, D., LANE, H. C., SWARTOUT, W., GRATCH, J., LENT, M. V. et MARSELLA.
S. (2006). Teaching negotiation skills through practice and reﬂection with virtual
humans. Simulation 82(11):685—701, 2006.

D‘MELLO, S., GRAESSER, A. et KING, B. (2010). Toward Spoken Human-Computer Tutorial
Dialogues. Human-Computer Interaction, (4):289--323.

DE PIETRO, 0., M. DE ROSE et G. FRONTERA. (2005). Automatic Update of AIML
Knowledge Base in E-Learning Environment. In Proceedings of Computers and Advanced
Technology in Education., Oranjestad, Aruba, August (2005): 29-31.

DEERWESTER, S., DUMAIS, S., FURNAS, G., LANDAUER, T., HARSHMAN, R., LOCHBAUM K. et
STREETER, L. (1988). Brevet (US Patent 4,839,853).

DEERWESTER, S., DUMAIS, S., FURNAS, G., LANDAUER, T. et HARSHMAN, R., Indexing by
Latent Semantic Analysis. In Journal of the Society for Information Science, vol. 41, no 6,
1990, p. 391-407.

DICKEY, M. D. (2007). Game design and learning: A conjectural analysis of how massively
multiple online role-playing games (MMORPGs) foster intrinsic motivation. Educational
Technology Research and Development, 55(3), 253-273.

FLORIDI, L., TADDEO, M. et TURILLI, M. (2009). Turing’s Imitation Game: Still an
Impossible Challenge for All Machines and Some Judges—An Evaluation of the 2008
Loebner Contest Minds and Machines. Springer.

LANDAUER, T.K., LAHAM, D., REHDER, B. et SCHREINER, M.E. (1997). How Well can Passage
Meaning be Derived Without Using Word Order? A Comparison of Latent Semantic
Analysis and Humans, in Proceedings of the 19th Annual Conference of the Cognitive Science
Society.

LEACOCK, C. et CHODOROW, M. (2003). C-rater: Automated Scoring of Short-Answer
Questions. Computers and Humanities. pp. 389-40.

LOEBNER, H. (2003). Home Page of the Loebner Prize - The First Turing Test.
http://www.loebner.net/Prizef/loebner-prize.html [consultée le 03/03/2012].

KIM, B., PARK, H. et BAEK, Y. (2009). Not just fun, but serious strategies: Using meta-

335

cognitive strategies in game based learning. Computers & Education, 52(4), 800-810.
doi: 10. 1016/j.compedu.2008. 1 2.004.

MATEAS, M. et STERN, A. (2005). Structuring Content in the Facade Interactive Drama
Architecture. AIIDE.

PAGE, E.B. (1968). The Use of the Computer in Analyzing Student Essays. International
Review ofEducation, 14, 210-224.

PAGE, E.B. (1995). The Computer Moves into Essay Grading: Updating the Ancient
Test, Phi Delta Kappan, 76(Mar), 561-565.

PAPASALOUROS, A., KOTIS, K. et KANARIS, K. (2008). Automatic generation of multiple-
choice questionsfrom domain ontologies. IADIS e—Learning, Amsterdam.

PAPINENI, K., ROUKOS, S., WARD T. et ZHU, W. (2001). BLEU: a method for automatic
evaluation of machine translation. In Proceedings of the 40th annual meeting on Association
for Computational Linguistics. 3 1 1—318.

PEREZ, D., ALFONSECA, E. et RODRIGUEZ, P. (2004). Application of the BLEU method for
evaluating free-text answers in an e-learning environment In Proceedings of the Language
Resources and Evaluation Conference (LREC).

PILATO, G., ROBERTO P. et RICCARDO R. (2008). A kst-based system for student tutoring.
Applied Artificial Intelligence 22, no. 4: 283-308.

PURVER, M., GINZBURG, J. et HEALEY, P. (2003). On the means for clariﬁcation in dialogue.
Current and new directions in discourse and dialogue. Springer. 235—255.

RASTIER, F. (2001). Sémantique et recherches cognitives, PUF (2e éd).

SUKKARIEH, J. Z. et BLACKMORE, J. (2009). c-rater: Automatic content scoring for short-
constructed responses. Florida Artificial Intelligence Research Society (FLAIRS) Conference,
Sanibel, FL.

SAWYER, B. (2004). Serious Games Market Size. Serious Games initiative Forum 01-04-
2004.

THOMAS, P., éditeurs (2010). Actes de RJC EIAH 2010 (Rencontres Jeunes Chercheurs en
Environnements Inforrnatiques pour l'Apprentissage Humain), Lyon. ATIEF.

VERNANT, D. (1992). Modele projectif et structure actionnelle du dialogue informatif. In
Du dialogue, Recherches sur la philosophie du langage, Vrin éd., Paris, n°14, p. 295-314.

WALLACE. , S. (2009). Parsing the Turing Test, The Anatomy of A.L.I.C.E. Springer.

WHITTINGTON, D. et HUNT, H. (1999). Approaches to the computerized assessment of free
text responses. In Danson, M. (Ed.), Proceedings of the Sixth International Computer Assisted
Assessment Conference, Loughborough, UK.

ZHANG, H. 1..., Z. SHEN, X. TAO, C. MIAO et B. Li. (2009). Emotional agent in serious game
(DINO). In Proceedings of The 8”’ International Conference on Autonomous Agents and Multi-
agent Systems-Volume 2, 1385-1386.

336

