<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
"http://www.w3.org/TR/html4/loose.dtd">
<html><head><title>Typologie des questions &#224; r&#233;ponses multiples pour un syst&#232;me de question-r&#233;ponse</title>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
</head>
<body>
<div style="page-break-before:always; page-break-after:always"><div><p>Actes de la conf&#233;rence conjointe JEP-TALN-RECITAL 2012, volume 3: RECITAL, pages 191&#8211;204,
Grenoble, 4 au 8 juin 2012. c&#169;2012 ATALA &amp; AFCP
</p>
<p>Typologie des questions &#224; r&#233;ponses multiples pour un syst&#232;me
de question-r&#233;ponse&#8727;
</p>
<p>Mathieu-Henri Falco
LIMSI-CNRS, Universit&#233; Paris-Sud, 91403 Orsay, France
falco@limsi.fr
</p>
<p>R&#201;SUM&#201;
L&#8217;&#233;valuation des syst&#232;mes de question-r&#233;ponse lors des campagnes repose g&#233;n&#233;ralement sur
la validit&#233; d&#8217;une r&#233;ponse individuelle support&#233;e par un passage (question factuelle) ou d&#8217;un
groupe de r&#233;ponses toutes contenues dans un m&#234;me passage (questions listes). Ce cadre &#233;valuatif
emp&#234;che donc de fournir un ensemble de plusieurs r&#233;ponses individuelles et ne permet &#233;galement
pas de fournir des r&#233;ponses provenant de documents diff&#233;rents. Ce recoupement inter-documents
peut &#234;tre necessaire pour construire une r&#233;ponse compos&#233;e de plusieurs &#233;l&#233;ments afin d&#8217;&#234;tre
le plus complet possible. De plus une grande majorit&#233; de questions formul&#233;es au singulier
et semblant n&#8217;attendre qu&#8217;une seule r&#233;ponse se trouve &#234;tre des questions poss&#233;dant plusieurs
r&#233;ponses correctes. Nous pr&#233;sentons ici une typologie des questions &#224; r&#233;ponses multiples ainsi
qu&#8217;un aper&#231;u sur les probl&#232;mes pos&#233;s &#224; un syst&#232;me de question-r&#233;ponse par ce type de question.
</p>
<p>ABSTRACT
Typology of Multiple Answer Questions for a Question-answering System
</p>
<p>The evaluation campaigns of question-answering systems are generally based on the validity
of an individual answer supported by a passage (for a factual question) or a group of answers
coming all from a same supporting passage (for a list question). This framework does not allow
the possibily to answer with a set of answers, nor with answers gathered from several documents.
This cross-checking can be needed for building an answer composed of several elements in order
to be as accurate as possible. Besides a large majority of questions with a singular form seems
to be answered with a single answer whereas they can be satisfied with many. We present here
a typology of questions with multiple answers and an overview of problems encountered by a
question-answering system with this kind of questions.
</p>
<p>MOTS-CL&#201;S : question-r&#233;ponse, questions &#224; r&#233;ponses multiples, question liste.
</p>
<p>KEYWORDS: question-answering, multiple answer questions, list question.
</p>
<p>&#8727;Ces travaux ont &#233;t&#233; partiellement financ&#233;s par OSEO dans le cadre du programme QUAERO.
</p>
<p>191</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>1 Introduction
</p>
<p>Les syst&#232;mes de question-r&#233;ponse (SQR) ont pour but de fournir une r&#233;ponse pr&#233;cise &#224; une
question formul&#233;e en langue naturelle par un utilisateur : ils peuvent travailler &#224; partir de bases
de donn&#233;es et/ou de collections de documents ; nous nous int&#233;ressons ici uniquement &#224; ceux
interrogeant un corpus de documents. Ces SQR combinent plusieurs domaines dont notamment
la recherche d&#8217;information et le TAL &#224; travers l&#8217;extraction d&#8217;informations. En effet, l&#224; o&#249; des
moteurs de recherche renvoient des r&#233;f&#233;rences de documents (avec &#233;ventuellement un extrait)
suite &#224; une requ&#234;te sous forme de mots-cl&#233;s, les SQR travaillent &#224; partir d&#8217;une question en
langue dont tous les mots ne sont pas forc&#233;ment pertinents pour la recherche d&#8217;information,
s&#233;lectionnent un ensemble de documents de la collection puis extraient la r&#233;ponse pr&#233;cise de ces
documents afin de la pr&#233;senter &#224; l&#8217;utilisateur (&#233;ventuellement accompagn&#233;e de l&#8217;extrait contenant
cette r&#233;ponse) .
</p>
<p>Les SQR existants utilisent des approches vari&#233;es, pouvant s&#8217;appliquer sur la totalit&#233; du syst&#232;me
ou seulement certaines parties. Par exemple, des syst&#232;mes utilisent une repr&#233;sentation de la
question et des documents logique (Moldovan et al., 2007) ou discursive (Bos et al., 2007a). La
syntaxe peut &#234;tre utilis&#233;e au niveau de l&#8217;extraction de la r&#233;ponse : par exemple pour la fusion
d&#8217;information multidocuments &#224; l&#8217;aide de d&#233;pendances syntaxiques (Moriceau et Tannier, 2010),
(Katz et Lin, 2003). Des heuristiques de distance (Fangtao et al., 2008) ou un apprentissage
automatique (Grappy, 2011) peuvent &#234;tre utilis&#233;s pour la validation d&#8217;un candidat-r&#233;ponse. Au
final, les SQR se trouvent souvent brid&#233;s par le processus d&#8217;&#233;valuation actuelle des campagnes
&#224; savoir fournir, pour chaque question, plusieurs r&#233;ponses (g&#233;n&#233;ralement de trois &#224; cinq) sous
la forme d&#8217;une paire r&#233;f&#233;rence du document/r&#233;ponse pr&#233;cise &#233;ventuellement accompagn&#233;e d
&#8217;un extrait du document d&#8217;o&#249; la r&#233;ponse a &#233;t&#233; extraite (passage justificatif) comme pour Quaero
2010 (Quintard et al., 2010) ou TREC-2007 (Dang et al., 2007) (toutes les &#233;ditions et guidelines
des campagnes CLEF1 et TREC2 sont en ligne).
</p>
<p>Nous avons choisi de nous int&#233;resser plus particuli&#232;rement aux questions que nous appellerons
&#8221;questions &#224; r&#233;ponses multiples&#8221; comme par exemple les questions Quels sont les sept astres du
syst&#232;me solaire visibles &#224; l&#8217;oeil nu ? (le Soleil, la Lune, Mercure, V&#233;nus, Mars, Jupiter, Saturne)
ou Quand le Paris Saint-Germain a-t-il &#233;t&#233; sacr&#233; champion de France de football ? (1986 et 1994
pour l&#8217;&#233;quipe professionnelle homme). Ces questions ne sont que peu ou pas &#233;valu&#233;es lors des
campagnes d&#8217;&#233;valuation des SQR. Pourtant, elles pr&#233;sentent un int&#233;r&#234;t tant au niveau de l&#8217;analyse
de la question que de l&#8217;extraction des r&#233;ponses et de leur pr&#233;sentation &#224; l&#8217;utilisateur. En effet, un
syst&#232;me doit &#234;tre capable dans le meilleur des cas d&#8217;extraire une liste de r&#233;ponses bien form&#233;e
d&#8217;un document mais, le plus souvent, le syst&#232;me doit reconstituer une liste &#224; partir d&#8217;&#233;l&#233;ments
provenant de documents diff&#233;rents. Nous avons choisi de nous int&#233;resser aux SQR qui interrogent
le Web car cela nous permet de travailler en domaine ouvert et, &#233;tant donn&#233; le nombre important
de documents, le travail de recoupement des r&#233;ponses multi-documents s&#8217;av&#232;re indispensable.
</p>
<p>Dans cet article, nous commen&#231;ons donc par d&#233;finir le terme question &#224; r&#233;ponses multiples
(question-ARM) et pr&#233;sentons un &#233;tat de l&#8217;art concernant le traitement de ce type de questions
par les SQR ainsi que les &#233;l&#232;ments structuraux sources de r&#233;ponses de type liste. Dans la section
3, nous pr&#233;sentons les observations constat&#233;es sur les donn&#233;es de deux campagnes d&#8217;&#233;valuation
proposant des questions-ARM. Les sections 4 et 5 pr&#233;senteront respectivement notre corpus
</p>
<p>1
http://nlp.uned.es/clef-qa/
</p>
<p>2
http://trec.nist.gov/data/qamain.html
</p>
<p>192</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>d&#8217;&#233;tude et les typologies que nous avons d&#233;finies. Enfin, la section 6 pr&#233;sentera les perspectives
envisag&#233;es.
</p>
<p>2 Contexte et &#233;tat de l&#8217;art
</p>
<p>Une question &#224; r&#233;ponses multiples est une question pour laquelle plusieurs r&#233;ponses peuvent
&#234;tre correctes. La forme la plus &#233;vidente de r&#233;ponse &#224; ce type de question est bien s&#251;r la liste, par
exemple :
question : Quels sont les sept astres du syst&#232;me solaire visibles &#224; l&#8217;oeil nu ?
r&#233;ponse : le Soleil, la Lune, Mercure, V&#233;nus, Mars, Jupiter, Saturne
passage : Les astres visibles &#224; l&#8217;oeil nu, le Soleil, la Lune, Mercure, V&#233;nus, Mars,
Jupiter et Saturne tiennent leur nom du monde romain.
</p>
<p>Les &#233;l&#233;ments composant la liste de r&#233;ponses peuvent bien s&#251;r &#234;tre d&#233;j&#224; sous la forme d&#8217;une liste
dans un document mais ils peuvent aussi &#234;tre r&#233;partis dans un document ou m&#234;me dans plusieurs
documents, par exemple :
question : Quand le Paris Saint-Germain a-t-il &#233;t&#233; sacr&#233; champion de France de football ?
r&#233;ponse : 1986 et 1994
document 1 : Le PSG champion de France 1986, vu par son entra&#238;neur, G&#233;rard
Houiller.
document 2 : Les positions resteront les m&#234;mes durant tout le reste de la saison : le
PSG sera sacr&#233; champion de France 1994.
</p>
<p>Nous pr&#233;sentons ici comment ces questions sont abord&#233;es par les SQR ainsi que les &#233;l&#233;ments
structuraux qui permettent d&#8217;identifier les r&#233;ponses dans les textes.
</p>
<p>2.1 L&#8217;&#233;valuation des SQR
</p>
<p>L&#8217;&#233;valuation des SQR peut se faire au niveau de la satisfaction utilisateur (point de vue applicatif
et qualitatif) ou par l&#8217;interm&#233;diaire d&#8217;une m&#233;trique (point de vue comparatif car quantitatif).
Les campagnes d&#8217;&#233;valuations de SQR ont pour but de jauger les performances des diff&#233;rentes
approches et proposent pour cela un nombre de questions significatif pour les cat&#233;gories les
plus fr&#233;quentes : factuelles, bool&#233;ennes, d&#233;finition, complexes, liste et nil (questions n&#8217;ayant pas
de r&#233;ponses dans la collection de documents). Les syst&#232;mes doivent fournir plusieurs r&#233;ponses
pour chaque question (de trois &#224; cinq g&#233;n&#233;ralement) et sont le plus souvent &#233;valu&#233;s gr&#226;ce &#224; la
m&#233;trique du MRR (Mean Reciprocal Rank) qui favorise ainsi les SQR fournissant une r&#233;ponse
correcte dans les premiers rangs.
</p>
<p>Il est tr&#232;s difficile de garantir qu&#8217;une unique r&#233;ponse correcte puisse &#234;tre obtenue &#224; partir de la
collection de documents disponible pour l&#8217;&#233;valuation, ce qui serait peu int&#233;ressant d&#8217;ailleurs, et
une &#233;valuation humaine des r&#233;ponses doit parfois avoir lieu pour juger la r&#233;ponse ainsi que le
passage justificatif.
</p>
<p>Dans les campagnes traitant des question-listes (questions de type liste), pour indiquer qu&#8217;on
</p>
<p>193</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>n&#8217;attend pas une r&#233;ponse unique, une marque de pluriel est toujours pr&#233;sente mais le nombre
de r&#233;ponses attendues n&#8217;est pas toujours mentionn&#233; dans la question comme dans les exemples
suivants : Quelles sont les 4 localisations possibles des neuroblastomes ? (EQueR, Quaero 2008,
2009, TREC 2001, 2002) ou Quels sont les secteurs qui recrutent ? (Quaero 2010, TREC 2003 &#224;
2007). La m&#233;trique utilis&#233;e pour &#233;valuer les r&#233;ponses &#224; ce type de questions est alors dans le
premier cas la pr&#233;cision moyenne (nombre de r&#233;ponses correctes/nombre de r&#233;ponses attendues)
et dans le second la F-mesure (en consid&#233;rant l&#8217;ensemble des r&#233;ponses jug&#233;es correctes par les
&#233;valuateurs.
</p>
<p>Par l&#8217;utilisation du MRR, les campagnes analysant un triplet question/r&#233;ponse/passage obligent
donc les SQR &#224; faire un choix d&#8217;au plus N r&#233;ponses par question. Une r&#233;ponse issue d&#8217;un
recoupement d&#8217;informations entre plusieurs documents est donc difficile &#224; justifier dans le cadre
d&#8217;une campagne d&#8217;&#233;valuation. De plus, la r&#233;ponse et le passage doivent obligatoirement &#234;tre du
texte issu d&#8217;un document de la collection alors qu&#8217;il peut &#234;tre parfois plus pertinent de renvoyer
un &#233;l&#233;ment structural (un tableau par exemple). Ces &#233;l&#233;ments structuraux sont tr&#232;s pr&#233;sents
dans les documents Web mais, de toutes les campagnes &#233;voqu&#233;es jusqu&#8217;&#224; pr&#233;sent, seule Quaero
utilise une collection de documents Web et impose un format de r&#233;ponse assez identique &#224; celui
des autres campagnes (Quintard et al., 2010).
</p>
<p>2.2 Le traitement des questions &#224; r&#233;ponses multiples par les SQR
</p>
<p>Les questions-listes sont un cas particulier des questions-ARM : elles attendent comme r&#233;ponse
une liste d&#8217;items provenant d&#8217;une m&#234;me entit&#233; (phrase ou document). Parmi les SQR ayant
particip&#233; aux campagnes proposant des questions dont la r&#233;ponse est de type liste, plusieurs
ont adapt&#233; leur traitement de questions factuelles aux listes. Cette adaptation consiste &#224; utiliser
la liste ordonn&#233;e de leurs r&#233;ponses trouv&#233;es dans la collection en renvoyant directement un
top-N de leurs r&#233;ponses, le nombre N pouvant &#234;tre fixe (par exemple 5 pour (Chu-carroll et al.,
2004) et 20 pour (Wu et al., 2003)) ou d&#233;pendant d&#8217;un nombre de r&#233;ponses attendues pr&#233;sent
dans la question (Bos et al., 2007b) ou d&#8217;un seuil d&#233;termin&#233; par le SQR selon son syst&#232;me
d&#8217;ordonnancement (Kaisser et Becker, 2004) (Schlaefer et al., 2007).
</p>
<p>Les SQR ayant d&#233;velopp&#233; un traitement sp&#233;cifique pour les listes ont notamment utilis&#233; la
d&#233;tection de doublon pour &#233;viter la redondance de candidat-r&#233;ponse (Katz et al., 2006) et
certains utilisent en plus la r&#233;conciliation de r&#233;f&#233;rence &#224; l&#8217;aide de ressources ext&#233;rieures (Schlaefer
et al., 2007), (Dan I. Moldovan and et Bowden, 2007). &#192; travers l&#8217;expansion de requ&#234;te, la co-
occurrence des candidats-r&#233;ponses, au niveau de la phrase ou du document, sert notamment de
crit&#232;re de validation (Razmara et Kosseim, 2008) (Wang et al., 2008) (Figueroa et Neumann,
2008).
</p>
<p>La plupart de ces SQR utilisent donc des ressources ext&#233;rieures comme des bases de donn&#233;es
ou le web, or nous ne souhaitons pas en d&#233;pendre et seulement utiliser une collection de
documents. De plus l&#8217;aspect multi-document n&#8217;est vu g&#233;n&#233;ralement qu&#8217;en phase de validation par
la co-occurrence.
</p>
<p>194</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>2.3 Les &#233;l&#233;ments structuraux
</p>
<p>Nous nous sommes int&#233;ress&#233;s aux &#233;l&#233;ments structuraux que sont l&#8217;objet tableau et l&#8217;objet liste car
nous nous attendons &#224; ce qu&#8217;ils soient une source de r&#233;ponses &#224; des questions-ARM (nous ne nous
int&#233;ressons qu&#8217;aux donn&#233;es textuelles et avons donc laiss&#233; de c&#244;t&#233; les images, figures, animations
flash, etc.). Nous consid&#233;rons ici le terme d&#8217;objet tableau comme une structure &#224; au moins deux
lignes et deux colonnes, et l&#8217;objet liste comme une constitution de plusieurs entit&#233;s dispos&#233;es
horizontalement (&#233;num&#233;ration dans une phrase) ou verticalement (amorce se terminant par le
symbole deux-points et un item par ligne par exemple).
</p>
<p>Les listes ont &#233;t&#233; l&#8217;objet d&#8217;&#233;tudes approfondies du point de vue discursif afin de mieux cerner la
structure d&#8217;un document (Ho-Dac, 2007). Les travaux de (P&#233;ry-Woodley, 2000), (Luc, 2001),
(Bras et al., 2008), (Laignelet, 2009), (Ho-Dac et al., 2010) ont beaucoup trait&#233; de cette
question et ont ainsi d&#233;fini l&#8217;objet r&#233;pondant au terme de &quot;structure &#233;num&#233;rative&quot; comme
&#233;tant compos&#233; d&#8217;une amorce (phrase introductrice), d&#8217;une &#233;num&#233;ration compos&#233;e d&#8217;items (en-
tit&#233; co-&#233;num&#233;r&#233;e caract&#233;ris&#233;e par diverses marques typographiques, dispositionnelles, lexico-
syntaxiques). Plusieurs travaux applicatifs se sont int&#233;ress&#233;s aux listes dans le cadre du peuple-
ment d&#8217;ontologie (Laignelet et al., 2011), les entit&#233;s nomm&#233;es (Jacquemin et Bush, 2000) et de
l&#8217;analyse syntaxique. En effet, l&#8217;objet liste est par nature difficile &#224; analyser syntaxiquement au
sens o&#249; il peut utiliser la verticalit&#233;, une autre ponctuation (le point-virgule entre les items), une
typographie assez libre (choix des puces) et cr&#233;er des liens syntaxiques entre les items, l&#8217;amorce
et la conclusion. Xerox a creus&#233; dans cette direction &#224; travers les travaux de (A&#239;t-Mokhtar et al.,
2003) et (Gala, 2003).
</p>
<p>Les tableaux ont &#233;t&#233; trait&#233;s du point de vue HTML avec pour but de typer les cases, soit &#224;
des fins de visualisations ergonomiques, soit pour de l&#8217;extraction d&#8217;information. Deux types
d&#8217;approches dominent : &#224; bases de r&#232;gles (Gatterbauer et al., 2007), (Tajima et Ohnishi, 2008) et
par apprentissage automatique sur un corpus annot&#233; manuellement (Wang et Hu, 2002).
</p>
<p>3 Premi&#232;res observations sur des corpus de campagnes d&#8217;&#233;-
valuation proposant des questions-ARM
</p>
<p>Nous nous sommes int&#233;ress&#233;s dans un premier temps aux campagnes pour le fran&#231;ais EQueR-
Evalda 2004 (Ayache et al., 2006) et QUAERO 2008 (Quintard et al., 2010) qui comportaient des
questions-listes et pour lesquelles nous avions acc&#232;s aux collections (voir tableau 1). La campagne
d&#8217;&#233;valuation EQueR a propos&#233; deux t&#226;ches : une g&#233;n&#233;rique sur une collection h&#233;t&#233;rog&#232;ne de
textes journalistiques (d&#233;sign&#233;e ici par Eq-Jour) et une sp&#233;cifique sur une collection de textes
m&#233;dicaux (d&#233;sign&#233;e par Eq-M&#233;d).
</p>
<p>3.1 Collecte des donn&#233;es
</p>
<p>Dans un premier temps, nous sommes partis des questions typ&#233;es par les &#233;valuateurs des
campagnes et avons &#233;tudi&#233; les questions-listes. Ces questions portaient toutes une marque de
pluriel sous forme de nombre de r&#233;ponses attendu et se formulaient syntaxiquement sous quatre
patrons (voir tableau 2).
</p>
<p>195</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Eq-M&#233;d Eq-Jour Quaero
Domaine m&#233;dical presse, politique ouvert
Format des documents texte texte HTML
Nombre de documents 5 623 557 300 499 736
Taille de la collection 0,135 Go 1,5 Go 5 Go
Nombre de questions-listes 25 30 29
</p>
<p>TAB. 1 &#8211; Caract&#233;ristiques des corpus EQueR et Quaero
</p>
<p>Eq-M&#233;d Eq-Jour Quaero
Citez X 13 5 14
Quels sont les X ? 12 22 15
Qui sont les X ? 0 2 0
Comment se pr&#233;nommaient les X ? 0 1 0
Nombre de questions-listes 25 30 29
</p>
<p>TAB. 2 &#8211; Nombre de questions-listes par forme syntaxique (X est le nombre de r&#233;ponses attendu).
</p>
<p>Nous avons ensuite effectu&#233; une premi&#232;re &#233;tude des documents contenant des r&#233;ponses correctes,
documents fournis par les organisateurs des campagnes sous forme d&#8217;un fichier de r&#233;f&#233;rence.
Nous consid&#233;rons ici qu&#8217;une r&#233;ponse est correcte si elle r&#233;pond &#224; la question (validation humaine),
m&#234;me s&#8217;il existe des r&#233;ponses correctes plus pertinentes (au sens de plus r&#233;centes par exemple,
ou bien satisfaisant plus l&#8217;utilisateur dans un cadre applicatif), et m&#234;me si la question attendait
plusieurs r&#233;ponses de fa&#231;on explicite (nombre d&#233;termin&#233; dans la question). Nous utiliserons le
terme r&#233;ponse-liste pour d&#233;signer le groupe de r&#233;ponses correctes &#224; une question en attendant un
nombre d&#233;termin&#233;.
</p>
<p>Nous avons utilis&#233; le moteur de recherche Lucene3 pour rechercher les documents contenant au
moins une r&#233;ponse aux questions-listes puisque les r&#233;ponses de fichier de r&#233;f&#233;rence n&#8217;&#233;taient pas
forc&#233;ment exhaustives. Les requ&#234;tes ont &#233;t&#233; formul&#233;es soit &#224; partir des termes de la question jug&#233;s
importants, soit &#224; partir des r&#233;ponses des fichiers r&#233;f&#233;rences. Nous avons &#233;tudi&#233; manuellement
jusqu&#8217;&#224; 50 extraits de documents par question puis les documents entiers si les snippets &#233;taient
pertinents. Les requ&#234;tes ont ensuite &#233;t&#233; reformul&#233;es &#224; l&#8217;aide de synonymes pour les termes de
la question et des r&#233;ponses nouvellement trouv&#233;es afin d&#8217;augmenter le nombre de passages-
r&#233;ponses. Nous avons arr&#234;t&#233; la collection quand nous n&#8217;observions plus de nouvelle r&#233;ponse ou
de nouveaux ph&#233;nom&#232;nes.
</p>
<p>3.2 &#201;tude des couples questions-ARM/r&#233;ponses
</p>
<p>L&#8217;&#233;tude des r&#233;ponses aux questions-listes a r&#233;v&#233;l&#233; un nombre important de passages-r&#233;ponses
pour les trois collections (voir tableau 3). Les r&#233;sultats montrent que la forme pr&#233;f&#233;rentielle
d&#8217;une r&#233;ponse &#224; une question-liste dans ces campagnes est majoritairement la phrase comme par
exemple :
</p>
<p>3
http://lucene.apache.org/core/
</p>
<p>196</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Eq-M&#233;d Eq-Jour Quaero
Nombre de questions-listes 25 30 29
Nombre de passages-r&#233;ponses 56 112 122
Nombre moyen de passages-r&#233;ponses 2,33 3,73 4,21
par question
Passage sous forme de phrase 30 (51,85 %) 73 (69,67 %) 85 (70,25 %)
Passage sous forme de paragraphe 11 (20,37 %) 37 (33,04 %) 19 (15,57 %)
Passage sous forme de liste 15 (27,78 %) 2 (1,79 %) 17 (13,93 %)
Passage sous forme de tableau 0 (0 %) 0 (0 %) 1 (0,82 %)
</p>
<p>TAB. 3 &#8211; Forme du passage-r&#233;ponse.
</p>
<p>Eq-M&#233;d Eq-Jour Quaero
Nombre de questions-listes 24 30 29
Nombre de questions avec plusieurs 8 (33,33 %) 4 (13,33 %) 12 (41,38 %)
passages-r&#233;ponses dans un m&#234;me document
Nombre de questions o&#249; un document 12 (50 %) 18 (60 %) 22 (75,86 %)
contient une r&#233;ponse-liste
Nombre de questions o&#249; la r&#233;ponse-liste est 6 (25 %) 0 (0 %) 0 (0 %)
obligatoirement inter-document
Nombre de passages-r&#233;ponses 56 112 122
</p>
<p>TAB. 4 &#8211; R&#233;partition des r&#233;ponses dans les documents.
</p>
<p>question : Quels sont les 7 astres du syst&#232;me solaire visibles &#224; l&#8217;oeil nu ?
passage-r&#233;ponse : Les astres visibles &#224; l&#8217;oeil nu, le Soleil, la Lune, Mercure, V&#233;nus, Mars, Jupiter et
Saturne tiennent leur nom du monde romain.
</p>
<p>Cette r&#233;partition centr&#233;e sur un bloc de texte continu (phrase, paragraphe, liste) contenant toutes
les r&#233;ponses est due aux choix des organisateurs de la campagne. Les questions-listes g&#233;n&#233;r&#233;es
pour Quaero l&#8217;&#233;taient notamment sur la base d&#8217;un document devant contenir tous les &#233;l&#233;ments
permettant d&#8217;y r&#233;pondre.
</p>
<p>Nous avons ensuite &#233;tudi&#233; la r&#233;partition des passages-r&#233;ponses dans les documents (voir
tableau 4). Il en a r&#233;sult&#233; une confirmation d&#8217;une redondance des r&#233;ponses inter-documents
et &#233;galement intra-document. La redondance inter-documents &#233;tait totale au sens o&#249; chaque
document contenant une r&#233;ponse correcte contenait aussi la r&#233;ponse-liste : seul un quart des
questions de Eq-M&#233;d n&#233;cessitait au moins deux documents de Eq-M&#233;d pour pouvoir composer
l&#8217;ensemble des r&#233;ponses attendues (il n&#8217;existait pas de document unique contenant le nombre de
r&#233;ponse attendu pour 25 % de ces questions de Eq-M&#233;d).
</p>
<p>Devant le peu de questions soulevant une n&#233;cessit&#233; de traitement inter-document dans ces
collections, nous avons donc d&#233;cid&#233; de constituer un autre corpus d&#8217;&#233;tude pour les questions-
ARM.
</p>
<p>197</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>4 Corpus d&#8217;&#233;tude pour les questions-ARM
</p>
<p>Afin d&#8217;&#233;tudier en d&#233;tail la forme des r&#233;ponses dans le but de mieux les extraire automatiquement,
nous avons constitu&#233; un corpus d&#8217;&#233;tude pour les questions-ARM en g&#233;n&#233;rant d&#8217;abord des questions-
ARM puis en r&#233;cup&#233;rant des documents permettant d&#8217;y r&#233;pondre.
</p>
<p>4.1 Constitution et caract&#233;ristiques du corpus
</p>
<p>Nous avons d&#8217;abord repris sept questions listes existantes dans EQueR et Quaero en supprimant le
nombre de r&#233;ponses attendu (Qui sont les huit personnages de &quot;Disney Princess&quot; ?) ou en changeant
des termes (Quels pays &#233;taient candidats &#224; l&#8217;organisation de la coupe du monde 2018 ? au lieu de
2006). Nous avons ensuite imagin&#233; des questions propices aux r&#233;ponses multiples : par exemple,
Qui a incarn&#233; Batman ?, Quand la France a-t-elle perdu son triple-A ?. En utilisant trois moteurs
de recherche sur Internet (Exalead4, Bing5 et Google6), nous avons collect&#233; des documents
contenant au moins une r&#233;ponse correcte. Ainsi un document peut ne contenir qu&#8217;un seul pays
candidat &#224; l&#8217;organisation de la coupe du monde 2018 ou qu&#8217;un seul nom d&#8217;acteur ayant incarn&#233;
Batman et non pas forc&#233;ment la r&#233;ponse-liste. Si ce document contenait une ou des r&#233;ponses
dans un tableau ou une liste, il &#233;tait ajout&#233; au corpus au m&#234;me titre que les autres documents.
</p>
<p>Le corpus d&#8217;&#233;tude se compose actuellement de cent questions ayant &#233;t&#233; g&#233;n&#233;r&#233;es manuellement
sur des th&#233;matiques vari&#233;es (sport, sant&#233;, politique, culture, &#233;conomie, informatique) et sous
plusieurs types. Les informations sont pr&#233;sent&#233;es de la fa&#231;on suivante : type de la question
(nombre de questions dans le corpus) (nombre de questions necessitant un traitement inter-
document pour r&#233;pondre pertinemment) : exemple.
&#8211; factuelle (61) (11) : Quand la France a-t-elle perdue son triple-A ? ;
&#8211; liste (17) (2) : Quels pays &#233;taient candidats &#224; l&#8217;organisation de la coupe du monde 2018 ? ;
&#8211; complexe (10) (3) : Comment a &#233;volu&#233; la croissance fran&#231;aise en 2011 ? ;
&#8211; bool&#233;enne (8) (3) : Pluton est-elle une plan&#232;te ? ;
&#8211; d&#233;finition (4) (0) : Qu&#8217;est-ce que la croissance ?.
Pour ces questions, nous avons r&#233;cup&#233;r&#233; 232 fichiers au format HTML, chacun d&#8217;entre eux
contenant donc au moins une r&#233;ponse correcte. Au total, seules 19 questions n&#233;cessitent obli-
gatoirement un traitement inter-document pour composer la r&#233;ponse. Cette basse proportion
s&#8217;explique notamment par le fait que quelques pages tr&#232;s pertinentes (Wikip&#233;dia notamment)
contenaient effectivement toutes les r&#233;ponses. Nous avons d&#233;cid&#233; de les garder car les r&#233;ponses
&#233;taient r&#233;parties sur l&#8217;ensemble du document (souvent de tr&#232;s grande taille). De plus leur
identification nous servira de baseline pour mesurer les r&#233;sultats sans traitement inter-document.
</p>
<p>4.2 Observations
</p>
<p>L&#8217;observation des passages-r&#233;ponses (voir tableau 5) a d&#8217;abord montr&#233; des probl&#232;mes r&#233;currents
des SQR pour lesquels il existe d&#233;j&#224; une base de travaux s&#8217;y int&#233;ressant, &#224; savoir la r&#233;solution
d&#8217;anaphore, la r&#233;conciliation de r&#233;f&#233;rence, le type m&#233;taphorique de la formulation de r&#233;ponse
</p>
<p>4
http://www.exalead.fr/search/
</p>
<p>5
http://www.bing.com/
</p>
<p>6
http://www.google.fr
</p>
<p>198</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>(Le triple A, c&#8217;est une ligne Maginot.), le besoin de contexte, les faux candidats (en France nous
avons le quintuple A (amicale des amateurs d&#8217;andouillettes authentique)), pour ne citer qu&#8217;eux.
</p>
<p>Le recensement pr&#233;cis a montr&#233; plusieurs ph&#233;nom&#232;nes dont les plus &#233;mergents sont :
&#8211; les r&#233;ponses se trouvant dans des tableaux de donn&#233;es ce qui confirme le besoin de savoir les
</p>
<p>analyser ;
&#8211; la pr&#233;sence d&#8217;informations incertaines (par exemple, des rumeurs ou avec l&#8217;usage du condition-
</p>
<p>nel) ;
&#8211; les r&#233;ponses sont r&#233;parties dans des chronologies narratives (document retra&#231;ant
</p>
<p>chronologiquement un th&#232;me) ;
&#8211; le recoupement d&#8217;informations r&#233;parties dans plusieurs documents.
</p>
<p>Nombre OCC PH&#201;NOM&#200;NE
82 crit&#232;re variant
72 formulation de la r&#233;ponse
53 ancre r&#233;f&#233;rentielle &#224; chercher
46 faux candidats
21 rumeur
20 chronologie narrative
18 tableau de donn&#233;es
17 terminologie dans la question
13 indice d&#8217;expansion de requ&#234;te
12 beaucoup de candidats-r&#233;ponses du type attendu
</p>
<p>dans un court passage textuel
12 besoin de contexte
11 type m&#233;taphorique de la r&#233;ponse
10 terminologie dans la r&#233;ponse
</p>
<p>TAB. 5 &#8211; Occurrences des ph&#233;nom&#232;nes (non mutuellement exclusifs) recens&#233;s les plus fr&#233;quents.
</p>
<p>Nous pr&#233;sentons ici les 3 ph&#233;nom&#232;nes auxquels nous avons choisi de nous int&#233;resser par la suite
car ce sont les plus fr&#233;quents dans notre corpus.
</p>
<p>Le ph&#233;nom&#232;ne le plus fr&#233;quent est la variation des r&#233;ponses selon certains crit&#232;res : un crit&#232;re
de pr&#233;cision (que nous appellerons crit&#232;re variant) d&#8217;un &#233;l&#233;ment permet de cr&#233;er plusieurs
r&#233;ponses correctes. Ici, la note souveraine de la France d&#233;pend de l&#8217;agence de notation :
</p>
<p>question : Quelle est la note de la France sur les march&#233;s financiers ?
passage-r&#233;ponse 1 : L&#8217;agence de notation am&#233;ricaine Egan-Jones a abaiss&#233; aujourd&#8217;hui
la note attribu&#233;e &#224; la dette de la France &#224; &quot;A&quot;, cinq crans en dessous du &quot;triple A&quot; des trois
grandes du secteur, Standard and Poor&#8217;s, Moody&#8217;s et Fitch.(&#8212;). La France avait perdu son
&quot;AAA&quot; chez cette agence en juillet.
passage-r&#233;ponse 2 : &quot;Moody&#8217;s a maintenu le triple A de la France, la meilleure note
possible&quot;, annon&#231;ait le matin une d&#233;p&#234;che AFP, aussit&#244;t reprise par une partie de la presse
fran&#231;aise.
passage-r&#233;ponse 3 : Peu apr&#232;s 16 heures, ce vendredi, une source gouvernementale a
indiqu&#233; que l&#8217;agence de notation financi&#232;re Standard &amp; Poor&#8217;s avait bel et bien d&#233;cid&#233; de
d&#233;grader la France en lui retirant sa note d&#8217;excellence triple A.
</p>
<p>199</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Le probl&#232;me de la formulation de la r&#233;ponse est un aussi probl&#232;me habituel des SQR : la
r&#233;ponse, par la synonymie ou la paraphrase, peut prendre plusieurs formes :
</p>
<p>question : Qui a incarn&#233; Batman ?
passage-r&#233;ponse 1 : Apr&#232;s avoir us&#233; Michael Keaton, Val Kilmer et George Clooney
dans le r&#244;le de Batman, les sp&#233;culations sur le prochain vengeur masqu&#233; de Gotham City se pour-
suivent.
passage-r&#233;ponse 2 : Le r&#233;alisateur chinois Zhang Yimou a choisi pour son prochain film l&#8217;acteur
britannique Christian Bale, qui a incarn&#233; Batman, pour jouer le r&#244;le d&#8217;un pr&#234;tre h&#233;ro&#239;que durant
le sac de Nankin par les troupes japonaises en 1937.
</p>
<p>L&#8217;ancre r&#233;f&#233;rentielle est le ph&#233;nom&#232;ne n&#233;cessitant un besoin de rattachement &#224; une date pr&#233;cise.
En effet, le temps est un crit&#232;re variant et les r&#233;ponses correctes ne le sont parfois que par
rapport &#224; un moment temporel pr&#233;cis. Par exemple dans les trois passages-r&#233;ponses suivants, les
r&#233;ponses n&#233;cessitent de trouver la date absolue &#224; partir des indices temporels relatifs (en gras)
pour pouvoir &#234;tre valid&#233;es :
</p>
<p>question : Quand la France a-t-elle perdu son triple-A ?
passage-r&#233;ponse 1 : L&#8217;agence de notation am&#233;ricaine Egan-Jones a abaiss&#233; aujourd&#8217;hui la note
attribu&#233;e &#224; la dette de la France &#224; &quot;A&quot;, cinq crans en dessous du &quot;triple A&quot; des trois grandes du secteur,
Standard and Poor&#8217;s, Moody&#8217;s et Fitch.(&#8212;).La France avait perdu son &quot;AAA&quot; chez cette agence en
juillet.
passage-r&#233;ponse 2 : &quot;Moody&#8217;s a maintenu le triple A de la France, la meilleure note possible&quot;,
annon&#231;ait le matin une d&#233;p&#234;che AFP, aussit&#244;t reprise par une partie de la presse fran&#231;aise.
passage-r&#233;ponse 3 : Peu apr&#232;s 16 heures, ce vendredi, une source gouvernementale a indiqu&#233;
que l&#8217;agence de notation financi&#232;re Standard &amp; Poor&#8217;s avait bel et bien d&#233;cid&#233; de d&#233;grader la France
en lui retirant sa note d&#8217;excellence triple A.
</p>
<p>5 Exp&#233;rimentation dans un cadre classique
</p>
<p>Nous avons soumis au SQR FIDJI (Moriceau et Tannier, 2010) les cent questions de notre corpus
afin d&#8217;analyser son comportement pr&#233;vu pour une campagne d&#8217;&#233;valuation classique. L&#8217;&#233;tude des
r&#233;sultats nous a permis dans un premier temps de mieux cat&#233;goriser les questions-ARM afin d&#8217;en
dresser une typologie et dans un deuxi&#232;me temps de mieux cibler les difficult&#233;s &#224; r&#233;soudre pour
pouvoir y r&#233;pondre dans le futur.
</p>
<p>5.1 Typologie des questions-ARM
</p>
<p>L&#8217;&#233;tude avait r&#233;vel&#233; que 47 des 61 questions factuelles se r&#233;v&#233;laient &#234;tre potentiellement des
questions-ARM. Nous avons donc &#233;tudi&#233; les ph&#233;nom&#232;nes composant ces questions-ARM en plus
des questions-listes afin d&#8217;&#234;tre en mesure de les typer lors de l&#8217;analyse des questions (figure 1). La
marque du pluriel sur le focus de la question indique explicitement une question-ARM tandis que
certains indices (notion temporelle, granularit&#233; du pronom qui et des adverbes interrogatif o&#249; et
quand) peuvent potentiellement indiquer une question-ARM mais seules les r&#233;ponses permettront
au final de trancher. Le crit&#232;re variant le plus fr&#233;quent (53,55 %) est le crit&#232;re temporel mais il
peut &#234;tre plus g&#233;n&#233;ral : les questions &#233;tant souvent courtes, le sens prototypique des concepts est
fr&#233;quemment utilis&#233;. Ainsi, parmi les exemples suivants de questions illustrant les ph&#233;nom&#232;nes de
</p>
<p>200</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>la typologie en figure 1, la question (6) pour un fran&#231;ais passionn&#233; de football fait commun&#233;ment
r&#233;f&#233;rence &#224; la ligue des champions de football masculine et europ&#233;enne alors qu&#8217;aucun de ces
deux termes n&#8217;est pr&#233;sent :
</p>
<p>&#8211; (identifiant en figure 1) Pourcentage sur les 100 questions du corpus : Question explication sur
les r&#233;ponses&quot;
</p>
<p>&#8211; (1), 10 % : Quels minist&#232;res a occup&#233; Alliot-Marie ? &quot;La D&#233;fense, l&#8217;Int&#233;rieur...&quot; ;
&#8211; (2) 7 % : Quels sont les pays de l&#8217;UE ? &quot;France, Finlande, Allemagne...&quot; (27 pays en 2012) ;
&#8211; (3) 1 % : Quelles sont les neuf plan&#232;tes du syst&#232;me solaire ? &quot;Mercure, V&#233;nus, Terre...&quot; ;
&#8211; (4) 33 % : O&#249;/Quand/&#192; qui Sarkozy a-t-il pr&#233;sent&#233; ses voeux 2012 ? &quot;&#192; Lille le 12 janvier aux
</p>
<p>fonctionnaire, &#192; Lyon le 19 janvier au monde &#233;conomique...&quot; ;
&#8211; (5) 11 % : Qui sont les Disney Princess ? &quot;Tiana a &#233;t&#233; ajout&#233;e en 2009 &#224; la collection, Raiponce
</p>
<p>en 2010&quot; ;
&#8211; (6) 95,74 % : Qui a gagn&#233; la ligue des champions en 2011 ? &quot;Barcelone en UEFA homme, Lyon
</p>
<p>en UEFA femme, Esp&#233;rance de Tunis en CAF homme&quot;
&#8211; (7) 4,26 % : Quelle superbe victoire a remport&#233; la France en 1998 ? &quot;1-0 contre la Finlande le 5
</p>
<p>juin&quot;, &quot;3-0 en France contre le Br&#233;sil le 12 juillet...&quot; (onze victoires en tout en 1998) ;
&#8211; (8) 10 % : Quand d&#233;marra le troisi&#232;me gouvernement Fillon ? &quot;le 13/11/10&quot; (annonce par
</p>
<p>Fillon), &quot;le 16/11/10&quot; (publication au Journal officiel) ;
&#8211; (9) 4 % : Quel jour Nicolas Sarkozy est-il devenu pr&#233;sident de la R&#233;publique ? &quot;&#201;lu le 6 mai 2007,
</p>
<p>investi le 16 mai&quot; ;
&#8211; (10) 1 % : Quand fut f&#234;t&#233; le bicentenaire de la r&#233;volution fran&#231;aise ? 1789 + 200 = 1989 ;
&#8211; (11) 11 % : Quels JO se sont d&#233;roul&#233;s il y a 16 ans ? ;
</p>
<p>FIG. 1 &#8211; Typologie des questions-ARM. Les chiffres correspondent aux exemples pr&#233;c&#233;dents.
</p>
<p>201</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>5.2 Approche classique avec FIDJI
</p>
<p>Le SQR FIDJI permet de recouper les informations entre documents en se basant sur la syntaxe
(Moriceau et Tannier, 2010) mais n&#8217;a pas encore de dispositif fonctionnel concernant les questions-
ARM. Un traitement des question-liste existe cependant en recherchant dans un m&#234;me document
un groupe d&#8217;&#233;l&#233;ments cons&#233;cutifs. Les r&#233;sultats actuels vont nous servir de premier &#233;tat des lieux
pour impl&#233;menter le traitement des questions-ARM. Ainsi nous avons rencontr&#233; les ph&#233;nom&#232;nes
suivants :
&#8211; (A) FIDJI choisit de ne renvoyer qu&#8217;une seule r&#233;ponse &#224; fort score de confiance plut&#244;t que
</p>
<p>plusieurs &#224; faibles scores (m&#234;me si la question est une question-liste) ;
&#8211; (B) FIDJI d&#233;tecte la r&#233;ponse-liste dans un document mais ne l&#8217;extrait pas correctement ;
&#8211; (C) FIDJI renvoie deux r&#233;ponses-listes correctes sans les fusionner ;
&#8211; (D) FIDJI renvoie une r&#233;ponse correcte (&quot;2005&quot;) mais il existait une r&#233;ponse correcte plus
</p>
<p>pertinente dans le passage (&quot;octobre 2003&quot;) : Quand est sorti l&#8217;Ibook G4 ? Avec les nouveaux
iBook G4 2005, Apple introduit Bluetooth2 de s&#233;rie (+ERD) (...). Le tableau ci-dessous retrace
toute l&#8217;histoire de l&#8217;iBook G4 de sa sortie en octobre 2003 &#224; nos jours.
</p>
<p>Nous voyons donc des pistes concr&#232;tes d&#8217;am&#233;liorations puisque (A) est d&#251; &#224; un manque dans
l&#8217;analyse des questions, (B) &#224; une extraction &#224; am&#233;liorer, (C) &#224; un manque de recoupement entre
les documents et (D) &#224; une granularit&#233; de la pertinence de la r&#233;ponse &#224; renvoyer.
</p>
<p>Le ph&#233;nom&#232;ne du crit&#232;re variant est bien pr&#233;sent dans les r&#233;sultats et nous montre l&#8217;int&#233;r&#234;t &#224;
d&#233;passer le cadre de la r&#233;ponse unique &#224; extraire d&#8217;un passage-candidat.
</p>
<p>6 Conclusion et perspectives
</p>
<p>En nous int&#233;ressant aux modes d&#8217;&#233;valuation des SQR lors des campagnes pour le fran&#231;ais, nous
avons constat&#233; un bridage n&#233;cessaire sur la pr&#233;sentation finale des r&#233;ponses et relativement peu
d&#8217;inter-documentalit&#233; pour les questions-listes. Apr&#232;s avoir constitu&#233; une collection de questions-
ARM et de documents permettant d&#8217;y r&#233;pondre, l&#8217;exp&#233;rimentation avec un SQR rod&#233; a confirm&#233;
la n&#233;cessit&#233; de mettre en place un traitement inter-document pour &#234;tre en mesure de r&#233;pondre le
plus pertinemment possible &#224; une question-ARM.
</p>
<p>Nous allons donc impl&#233;menter un module de traitement des questions-ARM afin de d&#233;passer
le cadre habituel d&#8217;&#233;valuation des SQR et se diriger vers un cadre utilisateur. En &#233;largissant
nos sources d&#8217;informations (HTML, &#233;l&#233;ments structuraux comme les tableaux), nous esp&#233;rons
b&#233;n&#233;ficier de plus d&#8217;informations pertinentes r&#233;parties dans des documents diff&#233;rents.
</p>
<p>Un autre aspect int&#233;ressant est la pr&#233;sentation des r&#233;ponses &#224; l&#8217;utilisateur. Nous pensons proposer
&#224; l&#8217;utilisateur des r&#233;ponses regroup&#233;es selon des crit&#232;res variants s&#8217;ils existent, notamment &#224; l&#8217;aide
d&#8217;&#233;l&#233;ments structuraux (tableau par exemple). De plus, il serait int&#233;ressant d&#8217;ajouter aux r&#233;ponses
textuelles des donn&#233;es multimedia (URLs, images, etc.) qui permettront de justifier les r&#233;ponses.
L&#8217;&#233;valuation des choix de regroupement serait alors faite du point de vue de l&#8217;utilisateur.
</p>
<p>Plusieus approches applicatives se sont int&#233;ress&#233;es &#224; la pr&#233;sentation des r&#233;ponses &#224; l&#8217;utilisateur.
Par exemple, le SQR WolframQA7 utilise &#233;galement les images, les tableaux et les chronologies
pour pr&#233;senter plusieurs r&#233;ponses &#224; l&#8217;utilisateur. On retrouve les tableaux dans Google squared
</p>
<p>7
http://www.wolframalpha.com
</p>
<p>202</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>(Crow, 2010) et des chronologies dans Google News et ChronoZoom8 ainsi que dans les travaux de
(Llorens et al., 2011) qui s&#8217;int&#233;resse &#224; l&#8217;annotation temporelle de textes &#224; des fins de visualisations
ergonomiques pour l&#8217;utilisateur.
</p>
<p>R&#233;f&#233;rences
</p>
<p>AYACHE, C., GRAU, B. et VILNAT, A. (2006). Equer : the french evaluation campaign of question-
answering systems. In Proceedings of The Fifth International Conference on Language Resources
and Evaluation (LREC 2006), Genoa, Italy.
</p>
<p>A&#207;T-MOKHTAR, S., LUX, V. et BANIK, E. (2003). Linguistic parsing of lists in structured documents.
In Proceedings of the EACL Workshop on Language Technology and the Semantic Web (3rd Workshop
on NLP and XML, NLPXML-2003), Budapest, Hungary.
</p>
<p>BOS, J., GUZZETTI, E. et CURRAN, J. R. (2007a). The pronto qa system at trec 2007 : Harvesting
hyponyms, using nominalisation patterns, and computing answer cardinality. In TREC-16.
</p>
<p>BOS, J., GUZZETTI, E. et CURRAN, J. R. (2007b). The pronto qa system at trec 2007 : Harvesting
hyponyms, using nominalisation patterns, and computing answer cardinality. In TREC-16.
</p>
<p>BRAS, M., PR&#201;VOT, L. et VERGEZ-COURET, M. (2008). Quelle(s) relation(s) de discours pour les
structures &#233;num&#233;ratives ? CMLF (Congr&#232;s mondial de linguistique fran&#231;aise).
</p>
<p>CHU-CARROLL, J., CZUBA, K., PRAGER, J. et BLAIR-GOLDENSOHN, S. (2004). Ibm&#8217;s piquant ii in
trec2004. In TREC-13.
</p>
<p>CROW, D. (2010). Google squared : Web scale, open domain information extraction and
presentation. In ECIR.
</p>
<p>DAN I. MOLDOVAN AND, C. C. et BOWDEN, M. (2007). Lymba&#8217;s poweranswer 4 in trec 2007. In
TREC-16.
</p>
<p>DANG, H. T., KELLY, D. et LIN, J. (2007). Overview of the trec 2007 question answering track. In
TREC-16.
</p>
<p>FANGTAO, L., XIAN, Z. et XIAOYAN, Z. (2008). Answer validation by information distance calcu-
lation. In Coling 2008 : Proceedings of the 2nd workshop on Information Retrieval for Question
Answering, IRQA &#8217;08, pages 42&#8211;49, Stroudsburg, PA, USA. Association for Computational Lin-
guistics.
</p>
<p>FIGUEROA, A. et NEUMANN, G. (2008). Finding distinct answers in web snippets. In In the 4th
International Conference on Web Information Systems and Technologies, pages 26&#8211;33. INSTICC
Press.
</p>
<p>GALA, N. (2003). Un mod&#232;le d&#8217;analyseur syntaxique robuste fond&#233; sur la modularit&#233; et la lexicali-
sation de ses grammaires. Th&#232;se de doctorat, Universit&#233; Paris-Sud.
</p>
<p>GATTERBAUER, W., BOHUNSKY, P., HERZOG, M., KR&#220;PL, B. et POLLAK, B. (2007). Towards domain-
independent information extraction from web tables. In Proceedings of the 16th international
conference on World Wide Web, WWW &#8217;07, pages 71&#8211;80. ACM.
</p>
<p>GRAPPY, A. (2011). Validation de r&#233;ponse dans un syst&#232;me de question-r&#233;ponse. Th&#232;se de doctorat.
</p>
<p>HO-DAC, L.-M. (2007). La position initiale dans l&#8217;organisation du discours : une exploration en
corpus. Th&#232;se de doctorat, Universit&#233; Toulouse le Mirail.
</p>
<p>8
http://research.microsoft.com/en-us/projects/chronozoom/
</p>
<p>203</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>HO-DAC, L.-M., P&#201;RY-WOODLEY, M.-P. et TANGUY, L. (2010). Anatomie des structures &#233;num&#233;ratives.
</p>
<p>JACQUEMIN, C. et BUSH, C. (2000). Fouille du web pour la collecte d&#8217;entit&#233;s nomm&#233;es. In TALN.
</p>
<p>KAISSER, M. et BECKER, T. (2004). Question answering by searching large corpora with linguistic
methods. In TREC-13.
</p>
<p>KATZ, B. et LIN, J. (2003). Selectively using relations to improve precision in question answering.
In EACL-2003 workshop on natural language processing for question answering.
</p>
<p>KATZ, B., MARTON, G., FELSHIN, S., LORETO, D., LU, B., MORA, F., &#214;zlem UZUNER, MCGRAW-HERDEG,
M., CHEUNG, N., RADUL, A., SHEN, Y. K., LUO, Y. et ZACCAK, G. (2006). Question answering
experiments and resources. In TREC-15.
</p>
<p>LAIGNELET, M. (2009). Analyse discursive pour le rep&#233;rage automatique de segments obsolescents
dans les documents encyclop&#233;diques. Th&#232;se de doctorat.
</p>
<p>LAIGNELET, M., KAMEL, M. et AUSSENAC-GILLES, N. (2011). Enrichir la notion de patron par la
prise en compte de la structure textuelle - application &#224; la construction d&#8217;ontologie. In TALN.
</p>
<p>LLORENS, H., SAQUETE, E., NAVARRO, B. et GAIZAUSKAS, R. (2011). Time-surfer : time-based
graphical access to document content. In ECIR&#8217;11 : Proceedings of the 33rd European conference
on Advances in information retrieval, pages 767&#8211;771, Berlin, Heidelberg. Springer-Verlag.
</p>
<p>LUC, C. (2001). Une typologie des &#233;num&#233;rations bas&#233;e sur les structures rh&#233;toriques et architec-
turales du texte. In TALN.
</p>
<p>MOLDOVAN, D. I., CLARK, C. et BOWDEN, M. (2007). Lymba&#8217;s poweranswer 4 in trec 2007. In
TREC-16.
</p>
<p>MORICEAU, V. et TANNIER, X. (2010). Fidji : Using syntax for validating answers in multiple
documents. Information Retrieval, Special Issue on Focused Information Retrieval, (10791).
</p>
<p>P&#201;RY-WOODLEY, M.-P. (2000). Une pragmatique &#224; fleur de texte : approche en corpus de
l&#8217;organisation textuelle. HDR.
</p>
<p>QUINTARD, L., GALIBERT, O., ADDA, G., GRAU, B., LAURENT, D., MORICEAU, V., ROSSET, S., TANNIER,
X. et VILNAT, A. (2010). Question answering on web data : the qa evaluation in qu&#230;ro.
In Proceedings of the Seventh conference on International Language Resources and Evaluation
(LREC&#8217;10), Valletta, Malta.
</p>
<p>RAZMARA, M. et KOSSEIM, L. (2008). Answering list questions using co-occurrence and clustering.
In LREC. European Language Resources Association.
</p>
<p>SCHLAEFER, N., KO, J., BETTERIDGE, J., SAUTTER, G., PATHAK, M. et NYBERG, E. (2007). Semantic
extensions of the ephyra qa system for trec 2007. In TREC-16.
</p>
<p>TAJIMA, K. et OHNISHI, K. (2008). Browsing large html tables on small screens. In UIST, pages
259&#8211;268.
</p>
<p>WANG, R. C., SCHLAEFER, N., COHEN, W. W. et NYBERG, E. (2008). Automatic set expansion for
list question answering. In EMNLP.
</p>
<p>WANG, Y. et HU, J. (2002). A machine learning based approach for table detection on the web.
In Proceedings of the 11th international conference on World Wide Web, pages 242&#8211;250. ACM.
</p>
<p>WU, M., ZHENG, X., DUAN, M., LIU, T. et STRZALKOWSKI, T. (2003). Questioning answering by
pattern matching, web-proofing, semantic form proofing. In TREC-12, pages 578&#8211;585.
</p>
<p>204</p>

</div></div>
</body></html>