Typologie des questions 51 réponses multiples pour un systéme
de question-réponse*

Mathieu—Henri Falco
LIMSI—CNRS, Université Paris—Sud, 91403 Orsay, France
falcoﬁlimsi . fr

RESUME
L’évaluation des systémes de question-réponse lors des campagnes repose généralement sur
la validité d’une réponse individuelle supportée par un passage (question factuelle) ou d’un
groupe de réponses toutes contenues dans un méme passage (questions listes). Ce cadre évaluatif
empéche donc de fournir un ensemble de plusieurs réponses individuelles et ne permet également
pas de fournir des réponses provenant de documents différents. Ce recoupement inter-documents
peut étre necessaire pour construire une réponse composée de plusieurs éléments afin d’étre
le plus complet possible. De plus une grande majorité de questions formulées au singulier
et semblant n’attendre qu’une seule réponse se trouve étre des questions possédant plusieurs
réponses correctes. Nous présentons ici une typologie des questions ‘a réponses multiples ainsi
qu’un apergu sur les problémes posés £1 un systéme de question-réponse par ce type de question.

AB STRACT
Typology of Multiple Answer Questions for a Question-answering System

The evaluation campaigns of question-answering systems are generally based on the validity
of an individual answer supported by a passage (for a factual question) or a group of answers
coming all from a same supporting passage (for a list question). This framework does not allow
the possibily to answer with a set of answers, nor with answers gathered from several documents.
This cross-checking can be needed for building an answer composed of several elements in order
to be as accurate as possible. Besides a large majority of questions with a singular form seems
to be answered with a single answer whereas they can be satisﬁed with many. We present here
a typology of questions with multiple answers and an overview of problems encountered by a
question-answering system with this kind of questions.

MOTS-CLES : question-réponse, questions 51 réponses multiples, question liste.

KEYWORDS: question-answering, multiple answer questions, list question.

{les ttavaux ont été partiellernent ﬁnancés par OSEO dans le cadre du programme QUAERO.

Actes de la con_fe'rence conjointe JEP-TALN-RECITAL 2012, volume 3: RECITAL, pages 191-204,
Grenoble, 4 an 8 juin 2012. ©2012 ATAI.A 8: AFCP

191

1 Introduction

Les systemes de question-réponse (SQR) ont pour but de fournir une réponse précise a une
question formulée en langue naturelle par un utilisateur : ils peuvent travailler a partir de bases
de données et/ou de collections de documents; nous nous intéressons ici uniquement ‘a ceux
interrogeant un corpus de documents. Ces SQR combinent plusieurs domaines dont notamment
la recherche d’information et le TAL ‘a travers l’extraction d’informau'ons. En effet, la ou des
moteurs de recherche renvoient des références de documents (avec éventuellement un extrait)
suite ‘a une requéte sous forme de mots-clés, les SQR travaillent ‘a partir d’une question en
langue dont tous les mots ne sont pas forcément peninents pour la recherche d’information,
sélectionnent un ensemble de documents de la collection puis extraient la réponse précise de ces
documents aﬁn de la présenter a l’uti1isateur (éventuellement accompagnée de l’extrait contenant
cette réponse) .

Les SQR existants utilisent des approches variées, pouvant s’appliquer sur la totalité du systéme
ou seulement certaines parties. Par exemple, des systemes utilisent une représentation de la
question et des documents logique (Moldovan et al., 2007) ou discursive (Bos et al., 2007a). La
syntaxe peut étre utilisée au niveau de l’extraction de la réponse : par exemple pour la fusion
d’information multidocuments a l’aide de dépendances syntaxiques (Moriceau et Tannier, 2010),
(Katz et Lin, 2003). Des heuristiques de distance (Fangtao et al., 2008) cu un apprentissage
automaﬁque (Grappy, 201 1) peuvent étre utilisés pour la validation d’un candidat-réponse. Au
ﬁnal, les SQR se trouvent souvent bridés par le processus d’évaluation actuelle des campagnes
‘a savoir foumir, pour chaque question, plusieurs réponses (généralement de trois ‘a cinq) sous
la forme d’une paire référence du document/réponse précise éventuellement accompagnée d
’un extrait du document d’o1‘1 la réponse a été ex1Iaite (passage justiﬁcatif) comme pour Quaero
2010 (Quintard et aL, 2010) ou TREC-2007 (Dang et aL, 2007) (toutes les éditions et guidelines
des campagnes CLEF1 et TREC2 sont en ligne).

Nous avons choisi de nous intéresser plus particulierement aux questions que nous appellerons
”questions ‘a réponses multiples” comme par exemple les questions Quels sont les sept astres du
sysbéme solaire visibles c‘1 l’oeil nu ? (le Soleil, la Lune, Mercure, Vénus, Mars, Jupiter, Satume)
ou Quand le Paris Saint-Germain a-t-il été sacre’ champion de France de football ? (1986 et 1994
pour l’équipe professionnelle homme). Ces questions ne sont que peu ou pas évaluées lors des
campagnes d’évaluation des SQR. Pourtant, elles présentent un intérét tant au niveau de l’analyse
de la question que de l’extraction des réponses et de leur présentation al’ut1'lisateur. En effet, un
systeme doit étre capable dans le meilleur des cas d’extraire une liste de réponses bien formée
d’un document mais, le plus souvent, le systeme doit reconstituer une liste ‘a parﬁr d’éléments
provenant de documents différents. Nous avons choisi de nous intéresser aux SQR qui interrogent
le Web car cela nous permet de travailler en domaine ouvert et, étant donné le nombre important
de documents, le travail de recoupement des réponses multi-documents s’avére indispensable.

Dans cet arﬁcle, nous commencons donc par définir le terme question a réponses multiples
(question-ARM) et présentons un état de l’art concemant le traitement de ce type de questions
par les SQR ainsi que les élements structuraux sources de réponses de type liste. Dans la section
3, nous présentons les observations constatées sur les données de deux campagnes d’évaluation
proposant des questions-ARM. Les sections 4 et 5 présenteront respectivement notre corpus

lhttp : //nlp .u.ned . es/clef—qa/
zhttp : //trec .nis1: .gov/data/qa.main.html

192

d’étude et les typologies que nous avons déﬁnies. Enﬁn, la section 6 présentera les perspectives
envisagées.

2 Contexte et état de l’art

Une question ‘a réponses multiples est une question pour laquelle plusieurs réponses peuvent
étre correctes. La forme la plus évidente de réponse a ce type de question est bien sﬁr la liste, par
exemple 2
question 2 Quels sont les Sept astres du systéme solaire visibles £1 l’0eil nu ?
réponse 2 1e Soleil, la Lune, Mercure, Vénus, Mars, Jupiter, Saturne
passage 2 Les astres visibles a l’oei1 nu, le Soleil, la Lune, Mercure, Vénus, Mars,
Jupiter et Saturne tiennent leur nom du monde romain.

Les éléments composant la liste de réponses peuvent bien sﬁr étre déja sous la forme d’une liste
dans un document mais ils peuvent aussi étre répartis dans un document ou méme dans plusieurs
documents, par exemple 2
question 2 Quand le Paris Saint-Germain a-t-il été sacré champion de France de football ?
réponse 2 1986 et 1994
document 1 2 Le PSG champion de France 1986, vu par son entraineur, Gérard
Houiller.
document 2 2 Les positions resteront les mémes durant tout le reste de la saison 2 le
PSG sera sacré champion de France 1994.

Nous présentons ici comment ces questions sont abordées par les SQR ainsi que les éléments
structuraux qui permettent d’identiﬁer les réponses dans les textes.

2.1 L’évaluation des SQR

L’évaluation des SQR peut se faire au niveau de la satisfaction utilisateur (point de vue applicatif
et qualitatif) ou par l’intermédiaire d’une métrique (point de vue comparatif car quantitatif).
Les campagnes d’évaluations de SQR ont pour but de jauger les performances des différentes
approches et proposent pour cela un nombre de questions signiﬁcatif pour les catégories les
plus fréquentes : factuelles, booléennes, définition, complexes, liste et nil (questions n’ayant pas
de réponses dans la collection de documents). les systemes doivent fournir plusieurs réponses
pour chaque question (de trois a cinq généralement) et sont le plus souvent évalués grace ‘a la
métrique du MRR (Mean Reciprocal Rank) qui favorise ainsi les SQR fournissant une réponse
correcte dans les premiers rangs.

Il est tres difﬁcile de garantir qu’une unique réponse correcte puisse étre obtenue a partir de la
collection de documents disponible pour l’évaluation, ce qui serait peu intéressant d’ailleurs, et
une évaluation humaine des réponses doit parfois avoir lieu pour juger la réponse ainsi que le
passage justiﬁcatif.

Dans les campagnes traitant des question-listes (questions de type liste), pour indiquer qu’on

193

n’attend pas une réponse unique, une marque de pluriel est toujours présente mais le nombre
de réponses attendues n’est pas toujours mentionné dans la question comme dans les exemples
suivants : Quelles sont les 4 localisations possibles des neuroblastomes ? (EQueR, Quaero 2008,
2009, TREC 2001, 2002) ou Quels sont les secteurs qui recrutent ? (Quaero 2010, TREC 2003 ‘a
2007). La métrique utilisée pour évaluer les réponses ‘a ce type de questions est alors dans le
premier cas la précision moyenne (nombre de réponses correctes/nombre de réponses attendues)
et dans le second la F-mesure (en considérant l’ensemble des réponses jugées correctes par les
évaluateurs.

Par l’uu'lisation du MRR, les campagnes analysant un triplet question / réponse/ passage obligent
donc les SQR ‘a faire un choix d’au plus N réponses par question. Une réponse issue d’un
recoupement d’informations entre plusieurs documents est donc difﬁcile a justiﬁer dans le cadre
d’une campagne d’évaluation. De plus, la réponse et le passage doivent obligatoirement étre du
texte issu d’un document de la collection alors qu’il peut étre parfois plus pertinent de renvoyer
un élément structural (un tableau par exemple). Ces éléments structuraux sont tres présents
dans les documents Web mais, de toutes les campagnes évoquées jusqu’a présent, seule Quaero
utilise une collection de documents Web et impose un format de réponse assez identique a celui
des autres campagnes (Quintard et al., 2010).

2.2 Le traitement des questions :31 réponses multiples par les SQR

Les questions-listes sont un cas particulier des questions-ARM : elles attendent comme réponse
une liste d’items provenant d’une méme entité (phrase ou document). Parmi les SQR ayant
participé aux campagnes proposant des questions dont la réponse est de type liste, plusieurs
ont adapté leur traitement de questions factuelles aux listes. Cette adaptation consiste a utiliser
la liste ordonnée de leurs réponses trouvées dans la collection en renvoyant directement un
top-N de leurs réponses, le nombre N pouvant étre ﬁxe (par exemple 5 pour (Chu-carroll et aL,
2004) et 20 pour (Wu et al., 2003)) ou dépendant d’un nombre de réponses attendues présent
dans la question (Bos et al., 2007b) ou d’un seuil déterminé par le SQR selon son systeme
d’ordonnancement (Kaisser et Becker, 2004) (Schlaefer et al., 2007).

Les SQR ayant développé un traitement spéciﬁque pour les listes ont notamment utilisé la
détection de doublon pour éviter la redondance de candidat-réponse (Katz et al., 2006) et
certains utilisent en plus la réconciliation de référence a l’aide de ressources extérieures (Schlaefer
et al., 2007), (Dan I. Moldovan and et Bowden, 2007). A travers l’expansion de requéte, la co-
occurrence des candidats-réponses, au niveau de la phrase ou du document, sert notamment de
critere de validation (Razmara et Kosseim, 2008) (Wang et al., 2008) (Figueroa et Neumann,
2008).

La plupart de ces SQR utilisent donc des ressources extérieures comme des bases de données
ou le web, or nous ne souhaitons pas en dépendre et seulement utiliser une collection de
documents. De plus l’aspect multi-document n’est vu généralement qu’en phase de validation par
la co-occurrence.

194

2.3 Les éléments structuraux

Nous nous sommes intéressés aux éléments structuraux que sont l’objet tableau et l’objet liste car
nous nous attendons a ce qu’i1s soient une source de réponses a des questions-ARM (nous ne nous
intéressons qu’aux données textuelles et avons donc laissé de c6té les images, ﬁgures, animations
ﬂash, etc.). Nous considérons ici le terme d’objet tableau comme une structure a au moins deux
lignes et deux colonnes, et l’objet liste comme une constitution de plusieurs entités disposées
horizontalement (énumération dans une phrase) ou verticalement (amorce se terminant par le
symbole deux-points et un item par ligne par exemple).

Les listes ont été l’objet d’études approfondies du point de vue discursif aﬁn de mieux cerner la
structure d’un document (Ho-Dac, 2007). Les travaux de (Péry-Woodley, 2000), (Luc, 2001),
(Bras et al., 2008), (Laignelet, 2009), (Ho-Dac et al., 2010) ont beaucoup traité de cette
question et ont ainsi déﬁni l’objet répondant au terme de "structure énumérative" comme
étant composé d’une amorce (phrase introductrice), d’une énumération composée d’items (en-
tité co-énumérée caractérisée par diverses marques typographiques, dispositionnelles, lexico-
syntaxiques). Plusieurs travaux applicatifs se sont intéressés aux listes dans le cadre du peuple-
ment d’ontologie (Laignelet et aL, 2011), les entités nommées (Jacquemin et Bush, 2000) et de
l’analyse syntaxique. En effet, l’objet liste est par nature difﬁcile ‘a analyser syntaxiquement au
sens ou il peut utiliser la verticalité, une autre ponctuation (le point-virgule entre les items), une
typographie assez libre (choix des puces) et créer des liens syntaxiques entre les items, l’amorce
et la conclusion. Xerox a creusé dans cette direction a travers les travaux de (A'it-Mokhtar et al.,
2003) et (Gala, 2003).

Les tableaux ont été traités du point de vue HTML avec pour but de typer les cases, soit ‘a
des fins de visualisations ergonomiques, soit pour de l’extracu'on d’information. Deux types
d’approches dominent : a bases de régles (Gatterbauer et al., 2007), (Tajima et Ohnishi, 2008) et
par apprentissage automaﬁque sur un corpus annoté manuellement (Wang et Hu, 2002).

3 Premiéres observations sur des corpus de campagnes d’é-
valuation proposant des questions-ARM

Nous nous sommes intéressés dans un premier temps aux campagnes pour le francais EQueR-
Evalda 2004 (Ayache et al., 2006) et QUAERO 2008 (Quintard et al., 2010) qui comportaient des
questions-listes et pour lesquelles nous avions accés aux collections (voir tableau 1). La campagne
d’évaluation EQueR a proposé deux taches : une générique sur une collection hétérogene de
textes journalistiques (désignée ici par Eq-Jour) et une spécifique sur une collection de textes
médicaux (désignée par Eq-Me’d).

3.1 Collecte des données
Dans un premier temps, nous sommes partis des questions typées par les évaluateurs des
campagnes et avons étudié les questions-listes. Ces questions portaient toutes une marque de

pluriel sous forme de nombre de réponses attendu et se formulaient syntaxiquement sous quatre
patrons (voir tableau 2).

195

Jour Quaero
presse, ouvert
Format texte texte HTML

5 623 557 300 499 736
0,135 Go 1,5 Go 5 Go
25 30 29

 

TAB. 1 — Caractéristiques des corpus EQueR et Quaero

Eq-Jour Quaero
X 13 5 14
sont X ? 12 22 15

sont X ? 0 2 0
Comment se . 0 1 0
25 30 29

 

TAB. 2 — Nombre de questions-listes par forme syntaxique (X est le nombre de réponses attendu).

Nous avons ensuite effectué une premiere étude des documents contenant des réponses correctes,
documents fournis par les organisateurs des campagnes sous forme d’un ﬁchier de référence.
Nous considérons ici qu’une réponse est correcte si elle répond a la question (validation humaine),
méme s’il existe des réponses correctes plus pertinentes (au sens de plus récentes par exemple,
ou bien satisfaisant plus l’utilisateur dans un cadre applicatif), et méme si la question attendait
plusieurs réponses de fagon explicite (nombre déterminé dans la question). Nous utiliserons le
terme réponse-Ziste pour désigner le groupe de réponses correctes a une question en attendant un
nombre déterminé.

Nous avons utilisé le moteur de recherche Lucenea pour rechercher les documents contenant au
moins une réponse aux questions-listes puisque les réponses de ﬁchier de référence n’étaient pas
forcément exhaustives. Les requétes ont été formulées soit a partir des termes de la question jugés
importants, soit a partir des réponses des ﬁchiers références. Nous avons étudié manuellement
jusqu’a 50 extraits de documents par question puis les documents entiers si les snippets étaient
pertinents. Les requétes ont ensuite été reformulées ‘a l’aide de synonymes pour les termes de
la question et des réponses nouvellement trouvées afin d’augmenter le nombre de passages-
réponses. Nous avons arrété la collection quand nous n’observions plus de nouvelle réponse ou
de nouveaux phénoménes.

3.2 Etude des couples questions-ARM/réponses

L’étude des réponses aux questions-listes a révélé un nombre important de passages-réponses
pour les trois collections (voir tableau 3). Les résultats montrent que la forme préférentielle
d’une réponse a une question-liste dans ces campagnes est majoritairement la phrase comme par
exemple :

3h1:1:p : //lucene . apache . org/core/

196

Eq-Méd Eq-Jour Quaero

Nombre de questions-listes 25 30 29
Nombre de passages-réponses 56 112 122
Nombre moyen de passages-réponses 2,33 3,73 4,21

par question

Passage sous forme de phrase 30 (51,85 %) 73 (69,67 %) 85 (70,25 %)
Passage sous forme de paragraphe 11 (20,37 %) 37 (33,04 %) 19 (15,57 %)
Passage sous forme de liste 15 (27,78 %) 2 (1,79 %) 17 (13,93 %)
Passage sous forme de tableau 0 (0 %) 0 (0 %) 1 (0,82 %)

TAB. 3 — Forme du passage-réponse.

Eq-Méd Eq-Jour Quaero
Nombre de questions-listes 24 30 29
Nombre de questions avec plusieurs 8 (33,33 %) 4 (13,33 %) 12 (41,38 %)
passages-réponses dans un méme document
Nombre de questions o1‘1 un document 12 (50 %) 18 (60 %) 22 (75,86 %)
contient une réponse-liste
Nombre de questions ou la réponse-liste est 6 (25 %) 0 (0 %) 0 (0 %)
obligatoirement inter-document
Nombre de passages-réponses 56 112 122

TAB. 4 — Répartition des réponses dans les documents.

question : Quels sont les 7 astres du systéme solaire visibles 61 l’oeil nu ?
passage-réponse : Les astres visibles 61 l’oeil nu, le Soleil, la Lune, Mercure, Vénus, Mars, Jupiter et
Saturne tiennent leur nom du monde romain.

Cette répartition centrée sur un bloc de texte continu (phrase, paragraphe, liste) contenant toutes
les réponses est due aux choix des organisateurs de la campagne. Les questions-listes générées
pour Quaero l’étaient notamment sur la base d’un document devant contenir tous les éléments
permettant d’y répondre.

Nous avons ensuite étudié la répartition des passages-réponses dans les documents (voir
tableau 4). Il en a résulté une conﬁrmation d’une redondance des réponses inter-documents
et également intra-document. La redondance inter-documents était totale au sens o1‘1 chaque
document contenant une réponse correcte contenait aussi la réponse-liste : seul un quart des
questions de Eq-Méd nécessitait au moins deux documents de Eq-Méd pour pouvoir composer
l’ensemble des réponses attendues (il n’existait pas de document unique contenant le nombre de
réponse attendu pour 25 % de ces questions de Eq-Méd).

Devant le peu de questions soulevant une nécessité de traitement inter-document dans ces
collections, nous avons donc décidé de constituer un autre corpus d’étude pour les questions-
ARM.

197

4 Corpus d’étude pour les questions-ARM

Aﬁn d’étudier en détail la forme des réponses dans le but de mieux les extraire automatiquement,
nous avons constitué un corpus d’étude pour les questions-ARM en générant d’abord des questions-
ARM puis en récupérant des documents permettant d’y répondre.

4.1 Constitution et caractéristiques du corpus

Nous avons d’abord repris sept questions listes existantes dans EQueR et Quaero en supprimant le
nombre de réponses attendu (Qui sont les heeitpersonnages de “Disney Princess“ ?) ou en changeant
des termes (Quels pays e’taient candidats a l’organisat1'on de la coupe du monde 2018 ? au lieu de
2006). Nous avons ensuite imaginé des questions propices aux réponses multiples : par exemple,
Qui a incarné Batman ?, Quand la France a-t-elle perdu son triple-A ?. En utilisant trois moteurs
de recherche sur Internet (Exalead4, Bing5 et Google5), nous avons collecté des documents
contenant au moins une réponse correcte. Ainsi un document peut ne contenir qu’un seul pays
candidat a l’organisation de la coupe du monde 2018 cu qu’un seul nom d’acteur ayant incarné
Batman et non pas forcément la réponse-liste. Si ce document contenait une ou des réponses
dans un tableau ou une liste, il était ajouté au corpus au méme titre que les autres documents.

Le corpus d’étude se compose actuellement de cent questions ayant été générées manuellement
sur des thématiques variées (sport, santé, politique, culture, économie, informatique) et sous
plusieurs types. Les informations sont présentées de la facon suivante : type de la question
(nombre de questions dans le corpus) (nombre de questions necessitant un traitement inter-
document pour répondre pertinemment) : exemple.

— factuelle (61) (11) : Quand la France a-t-elle perdue son triple-A ?;

— liste (17) (2) : Quels pays e’taient candidats a l’organisation de la coupe du monde 2018 ?;

— complexe (10) (3) : Comment a e’volue’ la croissance francaise en 2011 ?;

— booléenne (8) (3) : Pluton est-elle une planéte ? ;

— déﬁnition (4) (0) : Qu’est-oe que la croissance ?.

Pour ces questions, nous avons récupéré 232 fichiers au format HTML, chacun d’entre eux
contenant donc au moins une réponse correcte. Au total, seules 19 questions nécessitent obli-
gatoirement un traitement inter-document pour composer la réponse. Cette basse proportion
s’explique notamment par le fait que quelques pages trés pertinentes (Wi.kipédia notamment)
contenaient effecﬁvement toutes les réponses. Nous avons décidé de les garder car les réponses
étaient réparties sur l’ensemble du document (souvent de trés grande taille). De plus leur
identiﬁcation nous servira de baseline pour mesurer les résultats sans traitement inter-document.

4.2 Observations

L’observation des passages-réponses (voir tableau 5) a d’abord montré des problémes récurrents

des SQR pour lesquels il existe déja une base de travaux s’y intéressant, ‘a savoir la résolution

d’ana hore, la réconciliation de référence, le e méta hori ue de la formulation de ré onse
P

4h1:1:p : //mm . exalead. fr/search/
5h1:1:p://mm.bing.com/
5h1:1:p://mm .goog1e .fr

198

(Le triple A, c’est une ligne Maginot.), le besoin de contexte, les faux candidats (en France nous
avons le quintuple A (amicale des amateurs d’andouillettes authentique)), pour ne citer qu’eux.

Le recensement précis a montré plusieurs phénoménes dont les plus émergents sont :
— les réponses se trouvant dans des tableaux de données ce qui conﬁrme le besoin de savoir les
analyser;

— la présence d’informations incertaines (par exemple, des rumeurs ou avec l’usage du condition-
nel) ;

les réponses sont réparties dans des chronologies narratives (document retragant
chronologiquement un theme) ;

le recoupement d’informations réparties dans plusieurs documents.

Nombre OCC
82 variant
72
53
46
21
20
18

17
13

12 type
dans un court passage textuel
12 contexte
11 e
10

 

TAB. 5 — Occurrences des phénoménes (non mutuellement exclusifs) recensés les plus fréquents.

Nous présentons ici les 3 phénomenes auxquels nous avons choisi de nous intéresser par la suite
car ce sont les plus fréquents dans notre corpus.

Le phénomene le plus fréquent est la variation des réponses selon certains criteres : un critere
de précision (que nous appellerons critére variant) d’un élément permet de créer plusieurs
réponses correctes. Ici, la note souveraine de la France dépend de l’agence de notation :

question : Quelle est la note de la France sur les marches ﬁnanciers ?

passage-réponse 1 : L’agence de notation américaine Egan-Jones a abaisse’ aujourd’hui
la note attribuée a la dette de la France a "A“, cinq crans en dessous du “triple A“ des trois
grandes du secteur, Standard and Poor’s, Moody’s et Fitch.(—). La France avait perdu son
"AAA" chez cette agence en juillet.

passage-réponse 2 2 "Moody’s a maintenu le triple A de la France, la meilleure note
possible", annoncait le matin une dépéche AFB aussitot reprise par une partie de la presse
jrancaise.

passage-réponse 3 2 Peu apres 16 heures, ce vendredi, une source gouvernementale a
indiqué que l’agence de notation financiere Standard & Poor’s avait bel et bien décidé de
dégrader la France en lui retirant sa note d’excellence triple A.

199

Le probleme de la formulation de la réponse est un aussi probleme habituel des SQR : la
réponse, par la synonymie ou la paraphrase, peut prendre plusieurs formes :

question : Qui a incarne' Batman ?

passage-réponse 1 : Apres avoir use’ Michael Keaton, Val Kilmer et George Clooney
dans le réle de Batman les speculations sur le prochain vengeur masque’ de Gotham City se pour-
suivent.

passage-réponse 2 : Le re'alisateur chinois Zhang Yimou a choisi pour son prochain film l’acteur
britannique Christian Bale, qui a incarne' Batman, pourjouer le role d’un prétre he'roi'que durant
le sac de Nankin par les troup .

 

L’ancre référentielle est le phénoméne nécessitant un besoin de rattachement a une date précise.
En effet, le temps est un critere variant et les réponses correctes ne le sont parfois que par
rapport a un moment temporel précis. Par exemple dans les trois passages-réponses suivants, les
réponses nécessitent de trouver la date absolue a partir des indices temporels relatifs (en gras)
pour pouvoir étre validées :

question : Quand la France a-t-elle perdu son triple-A ?

passage-réponse 1 : Eagence de notation ame'ricaine Egan-Jones a abaisse' aujourd’hui la note
attribuée a la dette de la France a "A", cinq crans en des5o e A" des trois grandes du secteur;
Standard and Poors, Moodys et Fitch.(—).La France avait perdu son "AAA" chez cette agence en
juillet.

passage-réponse 2 : "Moodys a maintenu le triple A de la France, la meilleure note possible",
annoncait le matin z  AFB aussitét reprise par une partie de la presse francaise.
passage-réponse 3 : Peu apres 16 heures, ce vendredi, une source gouvernementale a indique’
que l’agence de notation financiere Standard & Poors avait bel et bien de'cide' de dégrader la France
en lui retirant sa note d’excellence triple A.

5 Expérimentation dans un cadre classique

Nous avons soumis au SQR FIDJI (Moriceau et Tannier, 2010) les cent questions de notre corpus
aﬁn d’analyser son comportement prévu pour une campagne d’évaluation classique. L’étude des
résultats nous a permis dans un premier temps de mieux catégoriser les questions-ARM aﬁn d’en
dresser une typologie et dans un deuxiéme temps de mieux cibler les difﬁcultés a résoudre pour
pouvoir y répondre dans le futur.

5.1 Typologie des questions-ARM

L’étude avait révelé que 47 des 61 questions factuelles se révélaient étre potentiellement des
questions-ARM. Nous avons donc étudié les phénoménes composant ces questions-ARM en plus
des questions-listes aﬁn d’étre en mesure de les typer lors de l’analyse des questions (ﬁgure 1). La
marque du pluriel sur le focus de la question indique explicitement une question-ARM tandis que
certains indices (notion temporelle, granularité du pronom qui et des adverbes interrogatif oi‘: et
quand) peuvent potentiellement indiquer une question-ARM mais seules les réponses permettront
au ﬁnal de trancher. Le critére variant le plus fréquent (53,55 %) est le critére temporel mais il
peut étre plus général : les questions étant souvent courtes, le sens prototypique des concepts est
fréquemment utilisé. Ainsi, parmi les exemples suivants de questions illustrant les phénoménes de

200

la typologie en ﬁgure 1, la question (6) pour un francais passionné de football fait communément
référence a la ligue des champions de football masculine et européenne alors qu’aucun de ces
deux termes n’est présent :

— (identiﬁant en ﬁgure 1) Pourcentage sur les 100 questions du corpus : Question explication sur
les réponses“

(1), 10 % : Quels ministéres a occupe’ Alliot-Marie ? “La Défense, l’Intérieur...“;

(2) 7 % : Quels sont les pays de l’UE ? “France, Finlande, Alleniagne...“ (27 pays en 2012) ;

— (3) 1 % : Quelles sont les neufplanétes du systéme solaire ? “Mercure, Vénus, Terre...“;

(4) 33 % : Oil/Quand/A qui Sarkozy a-t-il présente’ ses voeux 2012 ? “A Lille le 12 janvier aux
fonctionnaire, A Lyon le 19 janvier au monde éconon1ique...“;

— (5) 11 % 2 Qui sont les Disney Princess ? “Tiana a été ajoutée en 2009 a la collection, Raiponce
en 2010“ ;

(6) 95,74 % 2 Qui a gagné la ligue des champions en 2011 ? “Barcelone en UEFA homme, Lyon
en UEFA femme, Espérance de Tunis en CAF honinie“

— (7) 4,26 % : Quelle superbe Victoire a remporté la France en 1998 ? "1-0 contre la Finlande le 5

juin“, "3-O en France contre le Brésil le 12 juillet...“ (onze victoires en tout en 1998) ;
— (8) 10 % : Quand démarra le troisieme gouvernement Fillon ? "le 13/11/10" (annonce par
Fillon), “le 16/11/10“ (publication au Journal ofﬁciel) ;
— (9) 4 % : Queljour Nicolas Sarkozy est-il devenu president de la République ? "I-/Elu le 6 mai 2007,
investi le 16 mai“ ;
— (10) 1 % : Quandﬁltféte’ le bicentenaire de la révolution ﬁangzaise ? 1789 + 200 = 1989;
— (11) 11 % : Quels JO se sont déroulés ily a 16 ans ?;

     

quesI1'oll—ARM + relalioll esl—lln

p0IEll|1El.lE
(la réponse tranche le slam! de la qllesﬁolls)
plllriel (plus ou moins) cacllé (4)
. , . , . \ , 4 I ll
Ilolrqllallilﬁe quanuﬁe (3) autre cntere reponse lempmelle ”p‘?‘“.°,“°" ““p‘’" E
mals llee all temps (5)
raI1'onel(6) emoI1'onnel(7) granlllarilé ouvene (ll) granlllarilé fennée (9)
(les classes ne som pas mumllemem exclusives)
a calclller (10)

FIG. 1 — Typologie des questions-ARM. Les chiffres correspondent aux exemples précédents.

I») relation pent étre—un

 
   

  

   
    
 

     
   
 

       
       

201

5.2 Approche classique avec FIDJI

Le SQR FIDJ I permet de recouper les informations entre documents en se basant sur la syntaxe

(Moriceau et Tannier, 2010) mais n’a pas encore de dispositif fonctionnel concernant les questions-

ARM. Un traitement des question-liste existe cependant en recherchant dans un méme document

un groupe d’éléments consécutifs. Les résultats actuels vont nous servir de premier état des lieux

pour implémenter le traitement des questions-ARM. Ainsi nous avons rencontré les phénoménes
suivants :

— (A) FIDJI choisit de ne renvoyer qu’une seule réponse ‘a fort score de confiance plut6t que
plusieurs a faibles scores (méme si la question est une question-liste) ;

— (B) FIDJI détecte la réponse-liste dans un document mais ne l’extrait pas correctement;

— (C) FIDJI renvoie deux réponses-listes correctes sans les fusionner;

— (D) FIDJI renvoie une réponse correcte ("2005") mais il existait une réponse correcte plus
per11'nente dans le passage ("octobre 2003") : Quand est sorI1'1’Ibook G4 ? Avec les nouveaux
iBook G4 2005, Apple introduit Bluetooth2 de se’rie (+ERD) (...). Le tableau ci-dessous retrace
toute l’histoire de l’iBook G4 de sa sortie en octobre 2003 5: nos jours.

Nous voyons donc des pistes concrétes d’améliorations puisque (A) est dﬁ ‘a un manque dans

l’analyse des questions, (B) a une extraction a améliorer, (C) a un manque de recoupement entre

les documents et (D) a une granularité de la pertinence de la réponse a renvoyer.

Le phénoméne du critére variant est bien présent dans les résultats et nous montre l’intérét a
dépasser le cadre de la réponse unique a extraire d’un passage-candidat.

6 Conclusion et perspectives

En nous intéressant aux modes d’évaluation des SQR lors des campagnes pour le francais, nous
avons constaté un bridage nécessaire sur la présentation ﬁnale des réponses et relativement peu
d’inter-documentalité pour les questions-listes. Aprés avoir constitué une collection de questions-
ARM et de documents permettant d’y répondre, l’expérimentation avec un SQR rodé a conﬁrmé
la nécessité de mettre en place un traitement inter-document pour étre en mesure de répondre le
plus per11'nemment possible a une question-ARM.

Nous allons donc implémenter un module de traitement des questions-ARM afin de dépasser
le cadre habituel d’évaluation des SQR et se diriger vers un cadre utilisateur. En élargissant
nos sources d’informations (HTML, éléments structuraux comme les tableaux), nous espérons
bénéﬁcier de plus d’informations per11'nentes réparﬁes dans des documents différents.

Un autre aspect intéressant est la présentation des réponses a l’utilisateur. Nous pensons proposer
a l’utilisateur des réponses regroupées selon des critéres variants s’ils existent, notamment a l’aide
d’éléments structuraux (tableau par exemple). De plus, il serait intéressant d’ajouter aux réponses
textuelles des données multimedia (URLs, images, etc.) qui permettront de justiﬁer les réponses.
L’évaluau'on des choix de regroupement serait alors faite du point de vue de l’utilisateur.

Plusieus approches applicatives se sont intéressées a la présentation des réponses a l’utilisateur.
Par exemple, le SQR WolframQA7 utilise également les images, les tableaux et les chronologies
pour présenter plusieurs réponses al’ut1'lisateur. On retrouve les tableaux dans Google squared

7h1:1:p : //mm . wolframalpha . com

202

(Crow, 2010) et des chronologies dans Google News et Chronozoome ainsi que dans les travaux de
(Llorens et aL, 2011) qui s’intéresse a l’annotation temporelle de textes a des ﬁns de visualisations
ergonomiques pour l’utilisateur.

Références

AYACHE, C., GRAU, B. et VILNAT, A. (2006). Equer : the french evaluation campaign of question-
answering systems. In Proceedings of The Fifth International Conference on Language Resources
and Evaluation (LREC 2006), Genoa, Italy.

Air-MOKHTAR, S., LUX, V et BANIK, E. (2003). Linguistic parsing of lists in structured documents.
In Proceedings of the EACL Workshop on Language Technology and the Semantic Web (3rd Workshop
on MP and XML, NLPXML-2003), Budapest, Hungary.

Bos, J., GUZZETTI, E. et CURRAN, J. R. (2007a). The pronto qa system at trec 2007 : Harvesting
hyponyms, using nominalisation patterns, and computing answer cardinality. In 'I'REC-16.
Bos, J., GUZZETTI, E. et CURRAN, J. R. (2007b). The pronto qa system at trec 2007 : Harvesting
hyponyms, using nominalisation patterns, and computing answer cardinality. In 'I'REC-16.
BRAS, M., PREVOT, L. et VERGEZ-COURET, M. (2008). Quelle(s) relation(s) de discours pour les
structures énumératives ? CN[L.F (Congres mondial de linguistique francaise).

CHU-CARROLL, J., CZUBA, K., PRAGER, J. et BLAIR-GOLDENSOHN, S. (2004). Ibm’s piquant ii in
trec2004. In 'IREC-13.

CROW, D. (2010). Google squared : Web scale, open domain information extraction and
presentation. In ECIR.

DAN I. MOLDOVAN AND, C. C. et BOWDEN, M. (2007). Lymba’s poweranswer 4 in trec 2007. In
'IREC-1 6.

DANG, H. T., KELLY, D. et LIN, J. (2007). Overview of the trec 2007 question answering track. In
T'REC-1 6.

FANGTAO, L., XIAN, Z. et XIAOYAN, Z. (2008). Answer validation by information distance calcu-
lation. In Coling 2008 : Proceedings of the 2nd workshop on Information Retrieval for Question
Answering, IRQA ’08, pages 42-49, Stroudsburg, PA, USA. Association for Computational lin-
guistics.

FIGUEROA, A. et NEUMANN, G. (2008). Finding distinct answers in web snippets. In In the 4th
International Conference on Web Information Systems and Technologies, pages 26-33. INSTICC
Press.

GALA, N. (2003). Un modele d’analyseur syntaxique robuste fonde’ sur la modularite’ et la lexicali-
sation de ses grammaires. These de doctorat, Université Paris-Sud.

GATTERBAUER, W., BOHUNSKY, P, HERZOG, M., KRi'JPL, B. et POLLAK, B. (2007). Towards domain-
independent information extraction from web tables. In Proceedings of the 16th international
conference on World Wide Web, WWW ’07, pages 71-80. ACM.

GRAPPY, A. (2011). Validation de re’ponse dans un systeme de question-re’ponse. These de doctorat.
Ho-DAc, L.-M. (2007). La position initiale dans l’organisation du discours : une exploration en
corpus. These de doctorat, Université Toulouse le Mirail.

81:11:12 ://resea:rch.microsoft.com/en-us/ ro'ects/cl:L'ronozoom/
P P J

203

Ho-DAc, L.-M., PERY-WOODLEY, M.-13! et TANGUY, L. (2010). Anatomie des structures énumératives.
JACQUEMIN, C. et BUSH, C. (2000). Fouille du web pour la collecte d’entités nommées. In TALN.

KAISSER, M. et BECKER, T. (2004). Question answering by searching large corpora with linguistic
methods. In 'IREC-13.

KATZ, B. et LIN, J. (2003). Selectively using relations to improve precision in question answering.
In EACL-2003 workshop on natural language processing for question answering.

KATZ, B., MARTON, G., FELSHIN, S., LORETO, D., LU, B., MoRA, F., Qzlem UZUNER, MCGRAW-HERDEG,
M., CHEUNG, N., RADUL, A., SHEN, Y. K., LUo, Y. et ZACCAK, G. (2006). Question answering
experiments and resources. In 'I'REC-15.

LAIGNELET, M. (2009). Analyse discursive pour le repe’rage automatique de segments obsolescents
dans les documents encyclopédiques. These de doctorat.

LAIGNELET, M., KAMEL, M. et AUSSENAC-GILLES, N. (2011). Enrichir la notion de patron par la
prise en compte de la stiucture textuelle - application a la construction d’ontologie. In TALN.

LLORENS, H., SAQUETE, E., NAVARRO, B. et GA1zAUsKAs, R. (2011). Time-surfer : time-based
graphical access to document content. In ECIR’1 1 : Proceedings of the 33rd European conference
on Advances in information retrieval, pages 767-771, Berlin, Heidelberg. Springer-Verlag.

LUc, C. (2001). Une typologie des énumérations basée sur les structures rhétoriques et architec-
turales du texte. In TALN.

MoLDovAN, D. I., CLARK, C. et BOWDEN, M. (2007). Lymba’s poweranswer 4 in trec 2007. In
'I'REC-16.

MORICEAU, V et TANNIER, X. (2010). Fidji : Using syntax for validating answers in multiple
documents. Information Retrieval, Special Issue on Focused Information Retrieval, (10791).
PERY-WOODLEY, M.-F! (2000). Une pragmatique a ﬂeur de texte : approche en corpus de
l’organisation textuelle. HDR.

QUINTARD, L., GALIBERT, 0., ADDA, G., GRAU, B., LAURENT, D., MORICEAU, V, ROSSET, S., TANNIER,
X. et VILNAT, A. (2010). Question answering on web data : the qa evaluation in quaaro.
In Proceedings of the Seventh conference on International Language Resources and Evaluation
(LREC’10), Valletta, Malta.

RAZMARA, M. et KOSSEIM, L. (2008). Answering list questions using co-occurrence and clustering.
In LREC. European Language Resources Association.

SCHLAEFER, N., K0, J., BETTERIDGE, J., SAUTTER, G., PATHAK, M. et NYBERG, E. (2007). Semantic
extensions of the ephyra qa system for tree 2007. In 'IREC-16.

TAJIMA, K. et OHNISHI, K. (2008). Browsing large html tables on small screens. In UIST, pages
259-268.

WANG, R. C., SCHLAEFER, N., COHEN, W. W. et NYBERG, E. (2008). Automatic set expansion for
list question answering. In EMNLP.

WANG, Y. et HU, J. (2002). A machine learning based approach for table detection on the web.
In Proceedings of the 1 1th international conference on World Wide Web, pages 242-250. ACM.

WU, M., ZHENG, X., DUAN, M., LIU, T. et STRZALKOWSKI, T. (2003). Questioning answering by
pattern matching, web-prooﬁng, semantic form prooﬁng. In 'I'REC-12, pages 578-585.

204

