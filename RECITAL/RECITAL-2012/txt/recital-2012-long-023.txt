Analyse automatique de discours en langue des signes :
Représentation et traitement de l’espace de signation

Monia Ben Mlouka
IRIT -TCI , UMR5505, 31000 Toulouse
mlouka@irit.fr

RÉSUMÉ
En langue des signes, l’espace est utilisé pour localiser et faire référence à certaines entités dont
l’emplacement est important pour la compréhension du sens. Dans cet article, nous proposons
une représentation informatique de l’espace de signation et les fonctions de création et d’accès
associées, afin d’analyser les gestes manuels et non manuels qui contribuent à la localisation
et au référencement des signes et de matérialiser leur effet. Nous proposons une approche
bi-directionnelle qui se base sur l’analyse de données de capture de mouvement de discours en
langue des signes dans le but de caractériser les événements de localisation et de référencement.
ABSTRACT
Automatic Analysis of Discourse in Sign Language : Signing Space Representation and
Processing
In sign language, signing space is used to locate and refer to entities whose locations are important
for understanding the meaning. In this paper, we propose a computer-based representation of
the signing space and their associated functions. It aims to analyze manual and non-manual
gestures, that contribute to locating and referencing signs, and to make real their effect. For that,
we propose an approach based on the analysis of motion capture data of entities’ assignment and
activation events in the signing space.
MOTS-CLÉS : Langue des signes, Espace de signation, gestes de pointage, capture de mouvement,
suivi du regard.
KEYWORDS: Sign language, Signing space, pointing gestures, motion capture, gaze tracker.
219
1     Introduction

L’étude de l’aspect gestuel dans les langues naturelles fait l’objet de plusieurs travaux. Plusieurs
études ont porté sur l’analyse des gestes manuels et non manuels en situation de dialogue.
L’une d’entre elles a apporté une classification fonctionnelle des gestes manuels et non manuels
(Cosnier, 1997). Celle de (Montredon, 2001) établit une relation entre les caractéristiques spatio-
temporelles des gestes manuels et leurs rôles dans l’énoncé. En langue des signes, le canal étant
visuo-gestuel, l’analyse d’énoncés est en premier lieu une analyse d’un signal visuel. Elle peut
être entièrement réalisée à ce seul niveau ou être complétée par des analyses du geste 3D si l’on
sait reconstruire cette information à partir de données visuelles.
Nous proposons dans cette étude une représentation informatique de l’espace de signation comme
étant un élément important pour la compréhension d’un discours dans la langue des signes. Pour
cela, nous introduirons, en premier lieu, l’espace de signation et les gestes qui contribuent à la
localisation de signes. Dans un second lieu, nous décrirons le corpus sur lequel se base notre
analyse. Par la suite, nous présenterons notre approche d’analyse géométrique 3D suivie de
quelques résultats.
2     L’espace de signation

L’espace de signation est défini comme étant l’espace qui entoure le signeur et qui est atteignable
par ses deux mains. L’espace de signation sert à localiser les entités ou notions associées à certains
signes, éventuellement à spécifier leurs propriétés de forme et de taille et à établir des relations
spatiales entre les entités (Cuxac, 2000).
2.1    Evénements liés à l’espace de signation

Notre représentation informatique de l’espace de signation étant un graphe d’entités spatialisées
dans un espace 3D. Elle dispose des fonctions classiques de création, de modification, de sup-
pression et d’accès. Chacune de ces fonctions est déclenchée par un événement survenu dans
l’espace de signation. L’image (1a) est un exemple d’un signe [TABLE] qui occupe une zone de
l’espace. L’image (1b) illustre une association spatiale d’une action [S’ASSOIR]. L’image (1c) est
un exemple d’un pointage manuel vers une zone particulière de l’entité [TABLE]. La zone est
spécifiée par la direction de la main dominante.
2.2    Aspect multilinéaire

L’étude de (Fusellier-Souza, 2004) s’est intéressée aux gestes manuels et non manuels qui contri-
buent aux changements d’états de l’espace de signation. Une étude similaire, celle de (Thompson
et al., 2006) a porté sur la relation entre le regard et la réalisation d’actions spatialisées. Les deux
études soulignent l’aspect multilinéaire dans la réalisation de gestes. Dans cette étude nous nous
focaliserons sur les gestes de création et de référencement d’entités dans l’espace de signation.
Comme nous l’avons cité précédemment, les gestes sont manuels et non manuels que ce soit pour
les événements de création ou de référencement. Les gestes manuels de création d’entités dans
220
FIGURE 1 – a : Un objet spatialisé [TABLE], b : Une action spatialisée [S’ASSOIR], c : Un pointage
vers une partie de la portion de l’espace occupé par le signe[TABLE]
l’espace de signation représentent les signes effectués à un emplacement spécifique ou les signes
localisés sur le corps puis situés dans l’espace de signation par un pointage :
– L’orientation du regard et l’inclinaison de la tête permettent d’associer une zone de l’espace de
signation au signe en cours de réalisation.
– Les gestes manuels peuvent associer une forme à l’entité et préciser la taille qu’elle occupe
dans l’espace.
Les images (1a), (1b) et (1c) représentent des exemples de réalisation de l’aspect de multi-
linéarité :
– Dans (1a) et (1c), on observe que le regard du signeur est orienté vers la même zone d’espace
occupée par l’entité.
– On observe également une posture particulière dans (1b) qui se manifeste par une inclinaison
de la tête vers l’emplacement de la main qui effectue le signe [S’ASSOIR].
– Dans (1c), on ne peut pas déterminer si le regard fixe cet endroit de l’espace car la tête du
signeur est baissée mais on peut le déduire grâce à la position de la tête légèrement inclinée
vers le bas.
Par ces exemples, nous avons illustré l’importance de l’aspect multilinéaire de gestes qui contri-
buent aux fonctions de création, de référencement de l’entité [S’ASSEOIR] : fixation du regard,
gestes manuels et mouvements de la tête. Dans la littérature, peu de travaux ont porté sur la
représentation informatique de l’espace de signation. Nous citons l’étude de (Lenseigne, 2005)
qui a porté sur la représentation informatique de la structure de l’espace de signation dans un
discours en langue des signes française. L’aspect gestuel a été pris en compte dans l’étude de
(Braffort, 1996) qui consistait à modéliser les gestes dans les verbes directionnels et déictiques à
partir de données fournies par un gant numérique. (Lu et Huenerfauth, 2011) a développé une
technique qui, à partir de données de capture de mouvement, permet de modéliser les gestes dont
la réalisation est influencée par la localisation spatiale des entités dans l’espace de signation.Dans
cette dernière étude, (Lu et Huenerfauth, 2011) s’est intéressé aux gestes exprimant des verbes.
Notre travail s’inscrit dans le même cadre et s’intéresse à l’aspect multilinéaire (combinaisons de
gestes manuels et non manuels) et se base à la fois sur des données tri-dimensionnelles de capture
de mouvement et sur des données de suivi du regard synchronisées avec les enregistrements
vidéos.
221
3     Acquisition du Corpus

Le corpus sur lequel nous avons appliqué notre approche d’analyse a été enregistré dans le
cadre du partenariat franco-québécois (Marqspat) 1 . Les sessions de capture ont été réalisées
avec un système de capture de mouvement (VICON) 2 , une caméra vidéo pour filmer le cadre
complet de la scène et un système de capture du regard (FaceLab) 3 . Lors de l’enregistrement,
le signeur commence et termine sa production par un "clap" manuel qui permettra d’effectuer
ultérieurement une synchronisation de la vidéo et des données de la capture de mouvement et
commence à répondre à des questions sous forme de vidéos projetées. Les questions concernent
des détails à propos de scènes enregistrées préalablement. Les données de capture de mouvement
et celles de suivi du regard feront l’objet d’une analyse automatique (Ben Mlouka et al., 2010)
dont on détaillera les étapes dans la section suivante.
4     Annotations et représentations informatiques

Nous avons adopté une méthode composée de plusieurs étapes :
4.1     Grille d’annotation

L’annotation 4 du corpus a été réalisée et vérifiée par plusieurs annotateurs québécois de compé-
tences variées en Langue des Signes Française, Québécoise et Américaine. La grille d’annotation
se compose de :
Une annotation en gloses Les annotateurs ont transcrit les signes effectués par les deux mains
et qui peuvent avoir ou non une association spatiale spécifique. La capture d’écran de la grille
d’annotation (2) montre un exemple de valeurs attribuées aux pistes : la main droite (MD) :
[S’ASSEOIR], la piste (MG) transcrit les signes effectués par la main gauche, la piste (2M) inclut
ceux à deux mains.
Une annotation de gestes et de signes en liaison avec l’espace Les gestes manuels et non
manuels transcrits sont nécessairement associés à une zone de l’espace de signation. A chaque
geste ou signe spatialisé, on associe une étiquette (ex. x, y, z, etc.) pour étiqueter la zone à
laquelle est associé ce geste ou ce signe. Dans l’exemple (2), la piste (MC) mentionne le nom de
la zone associée au signe [S’ASSEOIR], cela veut dire qu’une zone est occupée par une entité "y"
dont le signifiant est l’action [S’ASSEOIR]. Le reste des pistes visibles inclut les noms d’entités
spatiales vers lesquelles un composant corporel fait référence :

1. Lien vers le site web du projet : http ://www.irit.fr/marqspat/index.html
2. Il s’agit d’un système de capture composé de 8 caméras infrarouges qui enregistrent les positions 3D de marqueurs
réfléchissants posés sur les membres du signeur
3. Un système non invasif composé de deux caméras et d’un émetteur infra-rouge. Il fournit sous forme de données
3D, l’orientation du regard. Une caméra de scène qui permet de synchroniser les données vidéos et données 3D
4. Ces annotations manuelles ont été effectuées par l’équipe "Groupe de recherche sur la langue des signes québécoise
et le bilinguisme sourd" qui pilote le projet (Marqspat)
222
FIGURE 2 – Un exemple de valeurs d’annotation
1. La piste (MR), représente les entités ciblées par les pointages manuels

2. La piste (TR) celle des entités référencées par la tête.

3. La piste (RM) celle de la direction du regard où on nomme l’entité sur laquelle se focalise
le regard.
Dans l’image (3a), la zone de l’espace occupée par le signe [S’ASSOIR] est étiquetée x. L’image
(3b) illustre un exemple où la main dominante effectue le signe [FILLE], le regard et la tête se
dirigent vers une même cible (x)
La lecture transversale de la grille d’annotation est un moyen qui permet de grouper les mêmes
étiquettes d’entités (x dans les deux exemples précédents). Une interprétation simple de cette
lecture transversale : Le signeur associe une zone de l’espace à une entité (x) grâce à l’orientation
du regard et de la tête et associe le référent [FILLE] à la zone (x) grâce au signe effectué près de
la tête.
Cette mise en correspondance entre les gestes et leurs interprétations sera généralisée grâce
à l’extraction et la synchronisation automatique des annotations manuelles avec les données
de capture de mouvement qui leurs correspondent dans le but de mettre en place des modèles
géométriques propres aux gestes qui contribuent à la création ou au référencement d’une même
entité.
223
FIGURE 3 – Une lecture transversale d’annotation d’un événement de référencement : a- Locali-
sation manuelle et non manuelle (regard) du signe [S’ASSOIR], b- Localisation non manuelle
(tête et regard) du signe [FILLE] car celui-ci se réalise par la main dominante à un emplacement
spécifique (près de la tête)
4.2   Représentation géométrique

Comme nous l’avons mentionné dans 4.1, les articulateurs représentées sont la main dominante,
la tête et le regard.
Mesure de l’enveloppe de la main La main dominante est représentée par une sphère de
centre milieu des bases de l’index et celui de l’auriculaire, de rayon la longueur du majeur(voir
figure 4).
!                    FIGURE 4 – Représentation géométrique de la main
224
La cible du regard La cible du regard est représentée par un point dont les coordonnées sont
fournies 5 par le système de suivi du regard. Il fournit la position 3D de la cible du regard à un
instant donné, exprimée dans le même repère que celui des positions des marqueurs de capture
de mouvement 6 . Nous avons noté que le taux de points de vergence reconstruits par (FaceLab)
est relativement faible par rapport aux données enregistrées. Ceci est dû au fait que à plusieurs
moments, les directions du regard calculées ne sont pas convergentes et par conséquent ne
permettent pas de calculer les positions des points de vergence.
TOPH
TOPH

RHEA

RHEA                        LHEA
FORH                                     LHEA
FORH
Orientation de la tête

Orientation de la tête
FIGURE 5 – Représentation géométrique de l’orientation de la tête (à gauche : vue de face, à
droite : vue de dessus)
Mesure de l’orientation de la tête L’orientation de la tête est mesurée comme étant la normale
à la droite passant par les marqueurs "RHEA" et "LHEA" et passant par le marqueur "FORH" (voir
figure 5). Le vecteur n est le vecteur normal au plan formé par les marqueurs : RHEA, LHEA et
TOPH. La figure (5) indique la position de ces marqueurs en vue de dessus. L’équation du plan
étant :
P : a.x + b. y + c.z + d = 0                                          (1)

Le vecteur normal est le résultat du produit vectoriel suivant :

n = AB ∧ AC                                             (2)

Tels que : A, B et C représentent la position géométriques des marqueurs RHEA, TOPH et LHEA
respectivement. Dans la suite nous allons caractériser l’aspect multi-composant entre les différents
modèles géométriques.

5. (FaceLab) fournit une liste de mesures sur : la position des globes oculaires, les pupilles, le degré de fermeture
des yeux, l’angle d’orientation du regard, etc. Dans ce travail, nous nous sommes intéressés aux positions de points de
vergence seulement
6. Nous avons fusionné les données fournies par le système de capture de mouvement et celles fournies par (FaceLab)
dans un même repère
225
4.3     Mesures et relations

On se propose dans cette phase de prendre en compte le sens comme cela a été détaillé dans 4.1
et d’extraire les positions géométriques correspondantes de chaque composant corporel. Par la
suite nous caractériserons la convergence des composants corporels comme étant l’intersection
simultanée ou différée des représentations géométriques de l’orientation de la tête, la position
de la main dominante et la cible du regard. La notion d’intersection géométrique inclut deux
différents composants corporels, on parle ainsi d’une relation entre composants. On qualifie
l’intersection différée de deux positions géométriques d’un même composant de relation intra-
composant.
4.3.1   Relations entre composants

Mesures de l’intersection orientation tête et main dominante On se propose de mesurer la
distance d entre la droite portant l’orientation de la tête D et le centre de la main S :

|| n ∧ S M D ||
d=                                                       (3)
|| n ||

Tels que M est un point appartenant à la droite D. Cette même formule s’applique également
pour la mesure de l’intersection entre la cible fixée ou pointée et l’orientation de la tête.
Mesures de l’intersection cible du regard et main dominante On se propose de mesurer la
distance d entre le point qui représente la cible fixée par le regard et P et le centre de la main S :

d=     (x C − x p )2 + ( yC − y p )2 + (zC − z p )2                    (4)

Tels que C est le centre de la main dominante et p et le point cible du regard. Cette formule
s’applique également pour la mesure de l’intersection entre la cible fixée et espace référencé par
la main dominante.
4.3.2   Relations intra-composant

Mesures de la convergence des espaces occupées par une seule entité On se propose de
mesurer la distance d entre deux sphères représentatives de la position de la main dominante à
deux instants distincts :

d=     (x C1 − x C2 )2 + ( yC1 − yC2 )2 + (zC1 − zC2 )2                   (5)

Tels que C1 et C2 sont les centres des sphères qui représentent la main dominante à deux instants
différents. Cette formule s’applique également pour la mesure de convergence entre espace
occupé et espace référencé par la main dominante.
Mesure de la variation de l’orientation de la tête Dans le but de mesurer la variation de cette
orientation au cours d’un événement de référencement, nous nous proposons de mesurer l’angle
226
formé par deux vecteurs porteurs des droites D1 et D2 d’orientation de la tête correspondante à
deux instants distincts.
L’angle θ est mesuré selon cette formule :

n1 .n2
cos(θ )                                                  (6)
|| n1 || . || n2 ||

Tels que n1 et n2 sont les vecteurs directeurs de D1 et D2 respectivement dont les coefficients
sont calculés selon la formule énoncée en (4.2).
4.4   Premiers résultats

4.4.1 Objectifs de l’analyse

On se propose d’apporter des éléments de réponses par rapport à l’état de l’espace de signation.
En particulier, on veut déterminer si l’espace de signation, tel qu’il est perçu par le signeur,
subit des transformations géométriques (translation et/ou rotation) au cours des événements
de référencement. Pour cela, on étudiera la relation entre l’emplacement de l’entité qui occupe
une partie de l’espace de signation et la position de la main dominante du signeur quand celui-ci
pointe vers cette même entité.
Pointages manuel vers une même entité Comme cela a été détaillé dans 4.3.2, nous avons
mesuré la distance qui sépare deux positions de la main dominante lors de la réalisation d’un
signe spatialisé et lors d’un pointage vers cette même entité 6.
FIGURE 6 – a- Création d’une entité - X, b, c et d- Pointages vers X
Figure 6b            Figure 6c   Figure 6d
Moyenne de la distance (mm)        685, 6               611, 5      569, 7
Ecart-type de la distance (mm)     3, 2                 1, 6        25, 2

TABLE 1 – Mesure de la distance qui sépare deux positions de la main dominante lors de la
réalisation d’un signe spatialisé et lors d’un pointage vers cette même entité
227
Les valeurs moyennes des distances du tableau 1 sont significatives car elles vérifient la règle
68-95-99.7 de la loi normale 7

µ − 3 ∗ σ < 99.7% ∗ N < µ + 3 ∗ si gma                                          (7)

µ − 2 ∗ σ < 95% ∗ N < µ + 2 ∗ si g ma                                          (8)
µ − σ < 68% ∗ N < µ + si g ma                                              (9)
Tels que : µ est la valeur moyenne et σ est l’écart-type de l’ensemble des valeurs de distance N
De ce fait, on déduit que les distance entre positions de la main dominante illustrées dans les
images (6b, c et d) et celle de l’image (6a) peuvent être générées par une même loi de distribution
normale de moyenne : 622, 3 et d’écart-type :58, 7 (en mm). Cela veut dire que dans ces trois
exemples (Création-Référencement), les distances qui séparent deux positions différentes de la
main dominante (la première en phase de création et la deuxième en phase de référencement)
ne sont pas exactement les mêmes mais varient autour d’une même moyenne.
Pointages manuels vers deux entités différentes                   La même formule 4.3.2 a été appliquée sur
les séquences 7.
FIGURE 7 – a- Création d’une entité - X, b- Pointage vers X, c- Création d’une entité - Y, d- Pointage
vers Y
TABLE 2 – Mesure de la distance qui sépare deux positions de la main dominante lors de la
réalisation d’un signe spatialisé et lors d’un pointage vers une même entité

Figure 7b       Figure 7d
Moyenne de la distance (mm)                 685, 6          194, 3
Ecart-type de la distance (mm)               3, 2            10, 6
Les valeurs moyennes des distances du tableau 2 ne vérifient pas la règle 68-95-99.7.
Bien que la variation des distances n’est pas importante car l’écart type est de l’ordre de (6.9)mm,
les distances mesurées ne vérifient pas une distribution normale.
7. Loi normale : 68% de la population se trouve entre µ − σ et µ + σ, 95% de la population se trouve entre µ − 2 ∗ σ
et µ + 2 ∗ σ,99.7% de la population se trouve entre µ − 3 ∗ σ et µ + 3 ∗ σ
228
Pointages Par la tête    La même formule 4.3.1 a été appliquée sur les séquences de la figure
(8).
FIGURE 8 – a- Localisation d’une entité - X, b- création d’une entité X, c- création et pointage non
manuel vers X
TABLE 3 – Mesure de la distance entre la droite portant l’orientation de la tête et la position de la
main dominante

Figure 8a    Figure 8b     Figure 8c
Moyenne de la distance (mm)           168, 1       221, 6        106, 1
Ecart-type de la distance (mm)         8, 2         2, 7          2,1
– La distance moyenne 168, 1 mm est la distance qui sépare la droite portant l’orientation de la
tête (voir 8c) et la position de la main dominante illustrée dans (8a).
– La distance moyenne 221, 6 mm est la distance qui sépare la droite portant l’orientation de la
tête (voir 8c) et la position de la main dominante illustrée dans (8b).
– La distance moyenne 106, 1 mm est la distance qui sépare la droite portant l’orientation de la
tête et la position de la main dominante illustrées dans (8c).
Dans le paragraphe 4.4.1, nous avons mesuré la distance qui sépare la position de la main domi-
nante à deux instants différents, lorsque la main réalise un signe spatialié et lorsqu’elle le pointe.
Dans ce paragraphe, nous avons appliqué la même méthode en remplaçant le référencement
manuel par le référencement réalisé par la tête (comme l’illustre la figure la figure 9c). Nous
avons calculé non pas la distance entre deux positions de la main mais la distance entre une
position de la main et la droite qui porte l’orientation de la tête. D’après les mesures du tableau
3, n’appartiennent pas à une même loi de distribution normale. Bien qu’il s’agisse de la même
entité, les mesures de distances (Tête-main) varient différemment pour chaque cas.
Pointages par le regard     La même formule 4.3.1 a été appliquée sur les séquences 10.
– La distance moyenne 545, 3 mm est la distance qui sépare la position de la cible du regard et
la position de la main dominante dans les figures (10a) et (10b) .
229
FIGURE 9 – Posture de la tête et position de la main dominante qui réalisent la localisation d’une
même entité
Position de la cible
Position de la cible                    du regard
Position de la main                    du regard
dominante
reconstruite par le
système de suivi
du regard
FIGURE 10 – a- Localisation d’une entité (Y) par le regard et par la main dominante , b- Localisation
de la même entité par le regard
TABLE 4 – Mesure de la distance entre la position de la cible du regard et la position de la main
dominante

Moyenne des distances (mm)          545, 3
Ecart-type des distance (mm)         9, 1
230
4.5     Retour sur résultats

Pointages manuels D’après les mesures de distances réalisées sur une session de capture, les
trois pointages manuels qui pointent vers une même zone spatiale (6b, c et d) présentent un
même comportement spatial par rapport à la position de l’entité créée dans l’espace de signation
(6 a). Nous en déduisons que la position spatiale de la zone occupée par l’entité [S’ASSOIR]
perçue par le signeur est conservée au cours des trois pointages manuels. Les séquences de
pointages manuels illustrés dans (7 b et d) font référence à deux entités différentes (X) et (Y)
respectivement. Les mêmes mesures de distance indiquent une évolution différente de la distance.
Ceci est en relation avec l’entité pointée non pas avec la notion de pointage en tant que notion
linguistique qui ne dépend pas de la cible vers laquelle pointe le signeur.
Pointages non manuel Les mesures du tableau (3) montrent que les distances entre la droite
portant l’orientation de la tête et la position de la main dominante illustrées dans (8a) et (8b)
ne sont pas similaires. Nous en déduisons que l’orientation de la tête est étroitement liée à
la position de la main dominante courante pour le signe [S’ASSOIR]. En d’autres termes, la
position qu’occupe l’entité [S’ASSOIR] dans l’espace telle qu’elle est perçue par le signeur n’est
pas conservée lors des deux différents pointages par la tête. Les mesures du tableau (4) montrent
que la distance entre la mire (cible du regard) et la position de la main dominante garde une
valeur quasi constante ce qui signifie que la position de l’entité [S’ASSOIR] perçue par le signeur
est la même lors des deux pointages distincts par le regard.
5     Perspectives

L’approche que nous avons présentée concerne l’analyse de gestes manuels et non manuels
liés à la localisation d’entités dans l’espace de signation. Cette approche pourrait apporter des
éléments de réponses par rapport aux propriétés spatiales des entités qui occupent l’espace de
signation au moment du discours. Les premières interprétations 4.5 des mesures effectuées sur
une base de données de capture de mouvement révèlent que la zone occupée par une entité
telle qu’elle est perçue chez le signeur ne change pas au cours d’un discours continu. Nous avons
abouti à cette conclusion grâce aux positions relatives de la main dominante, de la cible du
regard et de l’orientation de la tête. Cela veut dire que l’espace de signation dans sa globalité ne
subit pas de changement (translation ou rotation) au cours des séquences d’enregistrements sur
lesquelles nous avons effectué nos mesures. Ceci écarte l’hypothèse d’un éventuel changement
de position de l’espace de signation au cours d’un discours continu et apporte des précisions
pour sa représentation informatique. Toutefois, il serait intéressant d’analyser l’évolution de l’état
de l’espace de signation lors des pseudo-transferts de rôle 8 où l’hypothèse de changement de
positions de l’espace de signation est fortement appuyée.
8. ou semi-transfert personnel : Un court moment où le signeur émet une action (verbe) et devient le personnage qui
fait l’action à travers sa posture et son expression faciale(Cuxac, 2000)
231
6    Conclusion

L’approche proposée se base sur l’interprétation linguistique d’un discours en langue des signes
et exploite les données tridimensionnelles fournies afin d’extraire des comportements répétitifs
des gestes liés à l’espace de signation. Cette analyse prend en compte la multi-linéarité des gestes
effectués à la fois par la main dominante et la tête. Le regard contribue par des fixations vers
des emplacements spécifiques de l’espace de signation. Cependant, nous nous sommes focalisés
seulement sur l’analyse de deux fonctions linguistiques, celles de création et de référencement
d’entités dans l’espace de signation.
L’analyse que nous avons menée visait à apporter des précisions sur l’évolution de la structure
de l’espace de signation. En particulier, nous avons pu déduire que le signeur perçoit les zones
occupées par des entités comme étant des zones fixes. Par conséquent, l’espace de signation reste
figé au cours de pointages manuels et de pointages par le regard.
Remerciements

Le corpus 3D a été réalisé dans Le cadre du projet (Maqspat) qui porte sur le thème du marquage
spatial dans les langues des signes française, américaine et québécoise. Le projet est soutenu par
le CRSH, dans le cadre d’un partenariat stratégique soutenu par le CFQCU. Le corpus d’illustration
(1a et c) a été réalisé en interne par Juliette Dalle, assistante ingénieur de l’équipe Traitement et
compréhension de d’Image de l’Institut de Recherche en Informatique de Toulouse (IRIT).
Références
BEN MLOUKA, M., ALBARET-LEFEBVRE, F., DALLE, J. et DALLE, P. (2010). Annotation automatique
d’une vidéo en lsf à partir de données de capture de mouvement. In TALS, Montréal, Canada.
BRAFFORT, A. (1996). Reconnaissance et compréhension de gestes, application à la langue des
signes. Thèse de doctorat, Université de Paris XI.
COSNIER, J. (1997). Sémiotique des gestes communicatifs. Nouveaux actes sémiotiques, 52:7–28.
CUXAC, C. (2000). Faits de Langues - La langue des signes française (LSF) - Les voies de l’iconicité.
Faits Des Langues : Ophrys, Paris.
FUSELLIER-SOUZA, I. (2004). Sémiogenèse des langues des signes : étude de langues des signes
primaires (LSP) pratiquées par des sourds brésiliens. Thèse de doctorat, Université Paris 8.
LENSEIGNE, B. (2005). Modélisation de l’espace discursif pour l’analyse de la langue des signes.
In TALN, Dourdan.
LU, P. et HUENERFAUTH, M. (2011). Synthesizing American Sign Language Spatially Inflected
Verbs from Motion-Capture Data. In SLTAT, Dundee, UK.
MONTREDON, J. (2001). De la gestualité co-verbale, dimensions cognitives et symboliques. In
PRESSES UNIV. LIMOGES, ., éditeur : Sémio, pages 15–18.
THOMPSON, R., EMMOREY, K. et KLUENDER, R. (2006). The Relationship between Eye Gaze and
Verb Agreement in American Sign Language : An Eye-tracking Study. Natural Language &
Linguistic Theory, 24(2):571–604.
232
