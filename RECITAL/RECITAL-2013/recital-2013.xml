<?xml version="1.0" encoding="UTF-8"?>
<conference>
	<edition>
		<acronyme>RECITAL'2013</acronyme>
		<titre>15e Rencontres des Étudiants Chercheurs en Informatique pour le Traitement Automatique des Langues</titre>
		<ville>Sables d'Olonne</ville>
		<pays>France</pays>
		<dateDebut>2013-06-17</dateDebut>
		<dateFin>2012-06-21</dateFin>
		<presidents>
			<nom>Florian Boudin</nom>
			<nom>Loïc Barrault</nom>
		</presidents>
		<typeArticles>
			<type id="long">Articles longs</type>
		</typeArticles>
		<statistiques>
			<acceptations id="long" soumissions="25">18</acceptations>
		</statistiques>
		<siteWeb>http://www.taln2013.org/</siteWeb>
		<meilleurArticle>
			<articleId></articleId>
		</meilleurArticle>
	</edition>
	<articles>
		<article id="recital-2013-long-001" session="Orale">
			<auteurs>
				<auteur>
					<nom>Dhouha Bouamor</nom>
					<email>dhouha.bouamor@cea.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
					<affiliationId>3</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">CEA-LIST, LVIC, F91191 Gif sur Yvette Cedex, France</affiliation>
				<affiliation affiliationId="2">LIMSI-CNRS, F-91403 Orsay, France</affiliation>
				<affiliation affiliationId="3">Univ. Paris Sud, Orsay, France</affiliation>
			</affiliations>
			<titre>Acquisition de lexique bilingue d’expressions polylexicales: Une application à la traduction automatique statistique</titre>
			<type>long</type>
			<pages>1-13</pages>
			<resume>Cet article décrit une méthode permettant d’acquérir un lexique bilingue d’expressions polylexicales (EPLS) à partir d’un corpus parallèle français-anglais. Nous identifions dans un premier temps les EPLS dans chaque partie du corpus parallèle. Ensuite, nous proposons un algorithme d’alignement assurant la mise en correspondance bilingue d’EPLS. Pour mesurer l’apport du lexique construit, une évaluation basée sur la tâche de Traduction Automatique Statistique (TAS) est menée. Nous étudions les performances de trois stratégies dynamiques et d’une stratégie statique pour intégrer le lexique bilingue d’expressions polylexicales dans un système de TAS. Les expériences menées dans ce cadre montrent que ces unités améliorent significativement la qualité de traduction.</resume>
			<mots_cles>Expression polylexicale, alignement bilingue, traduction automatique statistique</mots_cles>
			<title>Mining a Bilingual Lexicon of MultiWord Expressions : A Statistical Machine Translation Evaluation Perspective</title>
			<abstract>This paper describes a method aiming to construct a bilingual lexicon of MultiWord Expressions (MWES) from a French-English parallel corpus. We first extract monolingual MWES from each part of the parallel corpus. The second step consists in acquiring bilingual correspondences of MWEs. In order to assess the quality of the mined lexicon, a Statistical Machine Translation (SMT) task-based evaluation is conducted. We investigate the performance of three dynamic strategies and of one static strategy to integrate the mined bilingual MWES lexicon in a SMT system. Experimental results show that such a lexicon significantly improves the quality of translation.</abstract>
			<keywords>MultiWord expression, bilingual alignment, statistical machine translation</keywords>
		</article>
		<article id="recital-2013-long-002" session="Poster">
			<auteurs>
				<auteur>
					<nom>Guiyao Ke</nom>
					<email>guiyao.ke@univ-ubs.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">IRISA, UMR 6074</affiliation>
				<affiliation affiliationId="2">Université de Bretagne Sud, 56000 Vannes</affiliation>
			</affiliations>
			<titre>Quelques variations sur les mesures de comparabilité quantitatives et évaluations sur des corpus comparables Français-Anglais synthétiques</titre>
			<type>long</type>
			<pages>15-27</pages>
			<resume>Dans la suite des travaux de (Li et Gaussier, 2010) nous abordons dans cet article l'analyse d'une famille de mesures quantitatives de comparabilité pour la construction ou l'évaluation des corpus comparables. Après avoir rappelé la définition de la mesure de comparabilité proposée par (Li et Gaussier, 2010), nous développons quelques variantes de cette mesure basées principalement sur la prise en compte des fréquences d'occurrences des entrées lexicales et du nombre de leurs traductions. Nous comparons leurs avantages et inconvénients respectifs dans le cadre d'expérimentations basées sur la dégradation progressive du corpus parallèle Europarl par remplacement de blocs selon la méthodologie suivie par (Li et Gaussier, 2010). L'impact sur ces mesures des taux de couverture des dictionnaires bilingues vis-à-vis des blocs considérés est également examiné.</resume>
			<mots_cles>Corpus comparables, Mesures de comparabilité, Évaluation</mots_cles>
			<title>Some variations on quantitative comparability measures and evaluations on synthetic French-English comparable corpora</title>
			<abstract>Following the pioneering work by (Li et Gaussier, 2010) we address in this paper the analysis of a family of quantitative measures of comparability dedicated to the construction or evaluation of comparable corpora. After recalling the definition of the comparability measure proposed by (Li et Gaussier, 2010), we develop some variants of this measure based primarily on the consideration of the occurrence frequency of lexical entries and the number of their translations. We compare the respective advantages and disadvantages of these variants in the context of an experiments based on the progressive degradation of the Europarl parallel corpus, by replacing blocks according to the methodology followed by (Li et Gaussier, 2010). The impact of the coverage of bilingual dictionaries on these measures is also discussed.</abstract>
			<keywords>Comparable corpora, Comparability measures, Evaluation</keywords>
		</article>
		<article id="recital-2013-long-003" session="Orale">
			<auteurs>
				<auteur>
					<nom>Noémie-Fleur Sandillon-Rezer</nom>
					<email>nfsr@labri.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">CNRS, Esplanade des Arts et Métiers, 33402 Talence</affiliation>
				<affiliation affiliationId="2">LaBRI, 351 Cours de la Libération, 33405 Talence</affiliation>
			</affiliations>
			<titre>Inférence grammaticale guidée par clustering</titre>
			<type>long</type>
			<pages>28-41</pages>
			<resume>Dans cet article, nous nous focalisons sur la manière d’utiliser du clustering hiérarchique pour apprendre une grammaire AB à partir d’arbres de dérivation partiels. Nous décrirons brièvement les grammaires AB ainsi que les arbres de dérivation dont nous nous servons comme entrée pour l’algorithme, puis la manière dont nous extrayons les informations des corpus arborés pour l’étape de clustering. L’algorithme d’unification, dont le pivot est le cluster, sera décrit et les résultats analysés en détails.</resume>
			<mots_cles>grammaires catégorielles, clustering hiérarchique, inférence grammaticale</mots_cles>
			<title>Clustering for categorial grammar induction</title>
			<abstract>In this article, we describe the way we use hierarchical clustering to learn an AB grammar from partial derivation trees. We describe AB grammars and the derivation trees we use as input for the clustering, then the way we extract information from Treebanks for the clustering. The unification algorithm, based on the information extracted from our cluster, will be explained and the results discussed.</abstract>
			<keywords>categorial grammars, hierarchical clustering, grammatical inference</keywords>
		</article>
		<article id="recital-2013-long-004" session="Poster">
			<auteurs>
				<auteur>
					<nom>Aurélie Joseph</nom>
					<email>joseph.aurelie@gmail.com</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LDI, 99 avenue Jean-Baptise Clément F-93430 Villetaneuse</affiliation>
				<affiliation affiliationId="2">ITESOFT, Parc d’Andron, le Séquoia, 30470 Aimargues</affiliation>
			</affiliations>
			<titre>Améliorer l’extraction et la description d’expressions polylexicales grâce aux règles transformationnelles</titre>
			<type>long</type>
			<pages>42-55</pages>
			<resume>Cet article présente une méthodologie permettant d’extraire et de décrire des locutions verbales vis-à-vis de leur comportement transformationnel. Plusieurs objectifs sont ciblés : 1) extraire automatiquement les expressions phraséologiques et en particulier les expressions figées, 2) décrire linguistiquement le comportement des phraséologismes 3) comparer les méthodes statistiques et notre approche et enfin 4) montrer l’importance de ces expressions dans un outil de classification de textes.</resume>
			<mots_cles>expressions polylexicales, expressions figées, locution verbale, extraction, transformation, classification de textes</mots_cles>
			<title>Enhance Multiword Expressions Extraction and Description with Transformational Rules</title>
			<abstract>This paper presents a methodology to extract and describe verbal multiword expressions using their transformational behavior. Several objectives are targeted: 1) automatically extracting MWE and especially frozen expression, 2) describing linguistically their MWE behavior, 3) comparing statistical methods and our approach, and finally 4) showing the importance of MWE in a text classification tool.</abstract>
			<keywords>multiword expression, verbal phrase, extraction, transformation, text classification</keywords>
		</article>
		<article id="recital-2013-long-005" session="Poster">
			<auteurs>
				<auteur>
					<nom>Manuela Yapomo</nom>
					<email>yapomodomkem@etu.unistra.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LiLPa (Linguistique, Langues, Parole), EA 1339</affiliation>
				<affiliation affiliationId="2">ICube - Laboratoire des sciences de l’Ingénieur, de l’Informatique et de l’Imagerie, UMR 7357 Université de Strasbourg</affiliation>
			</affiliations>
			<titre>Construction de corpus multilingues : état de l’art</titre>
			<type>long</type>
			<pages>56-68</pages>
			<resume>Les corpus multilingues sont extensivement exploités dans plusieurs branches du traitement automatique des langues. Cet article présente une vue d’ensemble des travaux en construction automatique de ces corpus. Nous traitons ce sujet en donnant premièrement un aperçu de différentes perceptions de la comparabilité. Nous examinons ensuite les principales approches de calcul de similarité, de construction et d’évaluation développées dans le domaine. Nous observons que Le calcul de la similarité textuelle se fait généralement sur la base de statistiques de corpus, de la structure de ressources ontologiques ou de la combinaison de ces deux approches. Dans un cadre multilingue avec l’utilisation d’un dictionnaire multilingue ou d’un traducteur automatique, de nombreux problèmes apparaissent. L’exploitation d’une ressource ontologique multilingue semble être une solution. En classification, la problématique de l’ajout de documents à la base initiale sans affecter la qualité des clusters demeure ouverte.</resume>
			<mots_cles>corpus multilingues, comparabilité, similarité textuelle translingue, classification</mots_cles>
			<title>Multilingual document clustering : state of the art</title>
			<abstract>Multilingual corpora are extensively exploited in several branches of natural language processing. This paper presents an overview of works in the automatic construction of such corpora. We address this topic by first providing an overview of different perceptions of comparability. We then examine the main approaches to similarity computation, construction and evaluation developed in the field. We notice that the measurement of the textual similarity is usually based on corpus statistics or the structure of ontological resources or on a combination of these two approaches. In a multilingual framework, with the use of a multilingual dictionary or a machine translator, many problems arise. The exploitation of a multilingual ontological ressource seems to be a worthy option. In clustering, the problem of adding documents to the initial base without affecting the quality of clusters remains open.</abstract>
			<keywords>multilingual corpora, comparability, crosslingual textual similarity, classification</keywords>
		</article>
		<article id="recital-2013-long-006" session="Poster">
			<auteurs>
				<auteur>
					<nom>Dhaou Ghoul</nom>
					<email>Dhaou.Ghoul@gmail.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">STIH, 1, rue Victor Cousin 75005 Paris</affiliation>
			</affiliations>
			<titre>Développement de ressources pour l’entrainement et l’utilisation de l’étiqueteur morphosyntaxique TreeTagger sur l’arabe</titre>
			<type>long</type>
			<pages>69-82</pages>
			<resume>Dans cet article, nous présentons les étapes du développement de ressources pour l’entraînement et l’utilisation d’un nouvel outil de l’étiquetage morphosyntaxique de la langue arabe. Nous avons mis en oeuvre un système basé sur l'étiqueteur stochastique TreeTagger, réputé pour son efficacité et la généricité de son architecture. Pour ce faire, nous avons commencé par la constitution de notre corpus de travail. Celui-ci nous a d'abord servi à réaliser l'étape de segmentation lexicale. Dans un second temps, ce corpus a permis d'effectuer l'entrainement de TreeTagger, grâce à un premier étiquetage réalisé avec l'étiqueteur ASVM 1.0, suivi d'une phase de correction manuelle. Nous détaillons ainsi les prétraitements requis, et les différentes étapes de la phase d'apprentissage avec cet outil. Nous terminons par une évaluation sommaire des résultats, à la fois qualitative et quantitative. Cette évaluation, bien que réalisée sur un corpus de test de taille modeste, montre que nos premiers résultats sont encourageants.</resume>
			<mots_cles>TALN, langue arabe, corpus d'apprentissage, étiquetage morphosyntaxique, segmentation de l'arabe, arbre de décision, lexique, jeux d’étiquette, TreeTagger, ASVM 1.0</mots_cles>
			<title>Development of resources for training and the use of the tagger TreeTagger on Arabic</title>
			<abstract>In this paper, we present the steps of the development of resources for training and the use of a new tool for the part-of-speech tagging of Arabic. We implemented a tagging system based on TreeTagger, a generic stochastic tagging tool, very popular for its efficiency. First of all, we began by gathering a working corpus, large enough to ensure a general linguistic coverage. This corpus has been used to implement the tokenization process, as well as to train TreeTagger. We first present our method of tokenization, then we describe all the steps of the preprocessing and training process, using ASVM 1.0 to yield a raw POS tagging that was subsequently manually corrected. Finally, we implemented a straightforward evaluation of the outputs, both in a quantitative and qualitative way, on a small test corpus. Though restricted, this evaluation showed really encouraging results.</abstract>
			<keywords>NLP, Arabic language, training corpus, POS tagging, tokenization, decision tree, lexicon, tagsets, TreeTagger, ASVM 1.0</keywords>
		</article>
		<article id="recital-2013-long-007" session="Poster">
			<auteurs>
				<auteur>
					<nom>Amel Ziani</nom>
					<email>Z_amel1911@live.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Nabiha Azizi</nom>
					<email>yamina.tlili@univ-annaba.org</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Yamina Tlili-Guiassa</nom>
					<email>nabiha.azizi@univ-annaba.org</email>
					<affiliationId>3</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Département d’informatique, Université Badji Mokhtar Annaba (Algérie)</affiliation>
				<affiliation affiliationId="2">Lri laboratory : laboratoire de recherche en informatique</affiliation>
				<affiliation affiliationId="3">Labged laboratory : Laboratoire de gestion électronique des documents</affiliation>
			</affiliations>
			<titre>Détection de polarité d’opinions dans les forums en langue arabe par fusion de plusieurs SVM</titre>
			<type>long</type>
			<pages>83-95</pages>
			<resume>Cet article décrit notre contribution sur la détection de polarité d’opinions en langue arabe par apprentissage supervisé. En effet le système proposé comprend trois phases: le prétraitement du corpus, l’extraction des caractéristiques et la classification. Pour la deuxième phase, nous utilisons vingt caractéristiques dont les principales sont l’émotivité, la réflexivité, l’adressage et la polarité. La phase de classification représente dans notre travail la combinaison des plusieurs classifieurs SVMs (Machine à Vecteur de Support) pour résoudre le problème multi classes. Nous avons donc analysés les deux stratégies de SVM multi classes qui sont : « un contre tous » et « un contre un » afin de comparer les résultats et améliorer la performance du système global.</resume>
			<mots_cles>Fouille d’opinions, apprentissage supervisé, Machine à Vecteur de Support (SVM), combinaison des classifieurs</mots_cles>
			<title>Polarity Opinion Detection in Arabic Forums by Fusing Multiple SVMs</title>
			<abstract>This article describes our contribution on the polarity’s detection of opinions in Arabian language by supervised training. Indeed the proposed system consists of three phases: the pretreatment of the corpus, the extraction of the features and the classification. For the second phase, we use twenty features of which the main are emotionalism, the reflexivity, the adressage and the polarity. The phase of classification represents in our work the combination of the several SVMs (Support Vector Machine),to solve the multi class problem. We analyzed the two strategies of the SVMs multi class that are: "one against all" and "one against one" in order to compare the results and to improve the performance of the global system.</abstract>
			<keywords>Opinion Mining, supervised training, Support Vector Machine (SVM), classifiers combination</keywords>
		</article>
		<article id="recital-2013-long-008" session="Poster">
			<auteurs>
				<auteur>
					<nom>Adrien Bougouin</nom>
					<email>adrien.bougouin@univ-nantes.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LINA - UMR CNRS 6241, Université de Nantes, France</affiliation>
			</affiliations>
			<titre>État de l’art des méthodes d’extraction automatique de termes-clés</titre>
			<type>long</type>
			<pages>96-109</pages>
			<resume>Cet article présente les principales méthodes d’extraction automatique de termes-clés. La tâche d’extraction automatique de termes-clés consiste à analyser un document pour en extraire les expressions (phrasèmes) les plus représentatives de celui-ci. Les méthodes d’extraction automatique de termes-clés sont réparties en deux catégories : les méthodes supervisées et les méthodes non supervisées. Les méthodes supervisées réduisent la tâche d’extraction de termes-clés à une tâche de classification binaire (tous les phrasèmes sont classés parmi les termesclés ou les non termes-clés). Cette classification est possible grâce à une phase préliminaire d’apprentissage, phase qui n’est pas requise par les méthodes non-supervisées. Ces dernières utilisent des caractéristiques (traits) extraites du document analysé (et parfois d’une collection de documents de références) pour vérifier des propriétés permettant d’identifier ses termes-clés.</resume>
			<mots_cles>extraction de termes-clés, méthodes supervisées, méthodes non-supervisées, état de l’art</mots_cles>
			<title>State of the Art of Automatic Keyphrase Extraction Methods</title>
			<abstract>This article presents the state of the art of the automatic keyphrase extraction methods. The aim of the automatic keyphrase extraction task is to extract the most representative terms of a document. Automatic keyphrase extraction methods can be divided into two categories : supervised methods and unsupervised methods. For supervised methods, the task is reduced to a binary classification where terms are classified as keyphrases or non keyphrases. This classification requires a learning step which is not required by unsupervised methods. The unsupervised methods use features extracted from the analysed document (sometimes a document collection) to check properties which allow keyphrase identification.</abstract>
			<keywords>keyphrase extraction, supervised methods, unsupervised methods, state of the art</keywords>
		</article>
		<article id="recital-2013-long-009" session="Poster">
			<auteurs>
				<auteur>
					<nom>Ophélie Lacroix</nom>
					<email>ophelie.lacroix@univ-nantes.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LINA - Université de Nantes, 2 Rue de la Houssinière, 44322 Nantes Cedex 3</affiliation>
			</affiliations>
			<titre>Influence de l’étiquetage syntaxique des têtes sur l’analyse en dépendances discontinues du français</titre>
			<type>long</type>
			<pages>110-123</pages>
			<resume>Dans cet article nous souhaitons mettre en évidence l’utilité d’un étiquetage syntaxique appliqué en amont d’une analyse syntaxique en dépendances. Les règles de la grammaire catégorielle de dépendances du français utilisées pour l’analyse gèrent les dépendances discontinues et les relations syntaxiques à longue distance. Une telle méthode d’analyse génère un nombre conséquent de structures de dépendances et emploie un temps d’analyse trop important. Nous voulons alors montrer qu’une méthode locale d’étiquetage peut diminuer l’ampleur de ces difficultés et par la suite aider à résoudre le problème global de désambiguïsation d’analyse en dépendances. Nous adaptons alors une méthode d’étiquetage aux catégories de la grammaire catégorielle de dépendance. Nous obtenons ainsi une pré-sélection des têtes des dépendances permettant de réduire l’ambiguïté de l’analyse et de voir que les résultats locaux d’une telle méthode permettent de trouver des relations distantes de dépendances.</resume>
			<mots_cles>Analyse syntaxique en dépendances discontinues, Étiquetage syntaxique</mots_cles>
			<title>On the Effect of Head Tagging on Parsing Discontinuous Dependencies in French</title>
			<abstract>In this paper we want to show the strong impact of syntactic tagging on syntactic dependency parsing. The rules of categorial dependency grammar used to parse French deal with discontinuous dependencies and long distance syntactic relations. Such parsing method produces a substantial number of dependency structures and takes too much parsing time. We want to show that a local tagging method can reduce these problems and help to solve the global problem of dependency parsing disambiguation. Then we adapt a tagging method to types of the categorial dependency grammar. We obtain a dependency-head pre-selection allowing to reduce parsing ambiguity and to see that we can find distant relation of dependencies through local results of such method.</abstract>
			<keywords>Discontinuous Dependency Parsing, Syntactic Tagging</keywords>
		</article>
		<article id="recital-2013-long-010" session="Poster">
			<auteurs>
				<auteur>
					<nom>Houda Saadane</nom>
					<email>houda.saadane@e.u-grenoble3.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIDILEM, Université Stendhal – Grenoble III, 1180, avenue central, F-38400 Saint Martin d’Hères</affiliation>
			</affiliations>
			<titre>Une approche linguistique pour l'extraction des connaissances dans un texte arabe</titre>
			<type>long</type>
			<pages>124-137</pages>
			<resume>Nous présentons dans cet article un système d'extraction de connaissances en arabe, fondé sur une analyse morphosyntaxique profonde. Ce système reconnaît les mots simples, les expressions idiomatiques, les mots composés et les entités nommées. L'analyse identifie aussi les relations syntaxiques de dépendance et traite les formes passives et actives. L’extraction des connaissances est propre à l’application et utilise des règles d’extraction sémantiques qui s'appuient sur le résultat de l'analyse morphosyntaxique. A ce niveau, le type de certaines entités nommées peut être révisé. L'extraction se base, dans nos expérimentations, sur une ontologie dans le domaine de la sécurité. Le RDF (Resource Description Framework) produit est ensuite traité pour regrouper les informations qui concernent un même événement ou une même entité nommée. Les informations ainsi extraites peuvent alors aider à appréhender les informations contenues dans un ensemble de textes, alimenter une base de connaissances, ou bien servir à  des outils de veille.</resume>
			<mots_cles>Analyse linguistique, fouille de textes, arabe, entités nommées, extraction d’informations, règles d’extraction, ontologie</mots_cles>
			<title>A linguistic approach for knowledge extraction from an Arabic text</title>
			<abstract>We present in this paper a knowledge extraction system for Arabic. The information extraction is based on a deep morphosyntactic analysis. It also recognizes single words, idiomatic expressions, compounds and named entities. The analysis also identifies dependency relations, verb tenses and passive/active forms. Information extraction is application-independent and uses extraction rules that rely on the result of the morphosyntactic analysis. At this level, some named entity categories can be reconsidered. This extraction is based in our experimentations on the security ontology. The Resource Description Framework (RDF) obtained is then processed to gather information concerning a single event or named entity. The information extracted can help to understand the information contained in a set of texts, to infer knowledge into a knowledge base, or be used for monitoring tools.</abstract>
			<keywords>Linguistic analysis, Text Mining, Arabic, named entities, information extraction, extraction rules, ontology</keywords>
		</article>
		<article id="recital-2013-long-011" session="Poster">
			<auteurs>
				<auteur>
					<nom>Sylvain Hatier</nom>
					<email>sylvain.hatier@u-grenoble3.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIDILEM, BP 25, 38040 Grenoble Cedex 09</affiliation>
			</affiliations>
			<titre>Extraction des mots simples du lexique scientifique transdisciplinaire dans les écrits de sciences humaines : une première expérimentation</titre>
			<type>long</type>
			<pages>138-149</pages>
			<resume>Nous présentons dans cet article les premiers résultats de nos travaux sur l'extraction de mots simples appartenant au lexique scientifique transdisciplinaire sur un corpus analysé morpho-syntaxiquement composé d'articles de recherche en sciences humaines et sociales. La ressource générée sera utilisée lors de l'indexation automatique de textes comme filtre d'exclusion afin d'isoler ce lexique de la terminologie. Nous comparons plusieurs méthodes d'extraction et montrons qu'un premier lexique de mots simples peut être dégagé et que la prise en compte des unités polylexicales ainsi que de la distribution seront nécessaires par la suite afin d'extraire l'ensemble de la phraséologie transdisciplinaire.</resume>
			<mots_cles>corpus, écrits scientifiques, lexique, phraséologie</mots_cles>
			<title>Extraction of academic lexicon's simple words in humanities writings</title>
			<abstract>This paper presents a first extraction of academic lexicon's simple words in french academic writings in the fields of humanities and social sciences through a corpus study of research articles using morpho-syntactic analysis. This academic lexicon resource will be used for automatic indexing as a stoplist in order to exclude this lexicon from the terminology. We try various extraction methods and show that a first simple words lexicon can be generated but that multiwords expressions and words distribution should be taken into consideration to extract academic phraseology.</abstract>
			<keywords>corpus, scientific writings, lexicon, phraseology</keywords>
		</article>
		<article id="recital-2013-long-012" session="Orale">
			<auteurs>
				<auteur>
					<nom>Marie Dubremetz</nom>
					<email>marie.dubremetz@lingfil.uu.se</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Uppsala universitet, Institutionen för lingvistik och filologi - Box 635 - 751 26 Uppsala, Suède</affiliation>
				<affiliation affiliationId="2">Université Paris Ouest La Défense - 200, Avenue de la République - 92001 Nanterre, France</affiliation>
			</affiliations>
			<titre>Vers une identification automatique du chiasme de mots</titre>
			<type>long</type>
			<pages>150-163</pages>
			<resume>Cette recherche porte sur le chiasme de mots : figure de style jouant sur la réversion (ex. « Bonnet blanc, blanc bonnet »). Elle place le chiasme dans la problématique de sa reconnaissance automatique : qu’est-ce qui le définit et comment un ordinateur peut le trouver ? Nous apportons une description formelle du phénomène. Puis nous procédons à la constitution d’une liste d’exemples contextualisés qui nous sert au test des hypothèses. Nous montrons ainsi que l’ajout de contraintes formelles (contrôle de la ponctuation et omission des mots vides) pénalise très peu le rappel et augmente significativement la précision de la détection. Nous montrons aussi que la lemmatisation occasionne peu d’erreurs pour le travail d’extraction mais qu’il n’en est pas de même pour la racinisation. Enfin nous mettons en évidence que l’utilisation d’un thésaurus apporte quelques résultats pertinents.</resume>
			<mots_cles>chiasme, rhétorique, antimétabole, figure de style</mots_cles>
			<title>Towards an automatic identification of chiasmus of words</title>
			<abstract>This article summarises the study of the rhetorical figure “chiasmus” (e.g : “Quitters never win and winners never quit.”). We address the problem of its computational identification. How can a computer identify this automatically ? For this purpose this article will provide a formal description of the phenomenon. First, we put together an annotated text for testing our hypothesis. At the end we demonstrate that the use of stopword lists and the identification of the punctuation improve the precision of the results with very little impact on the recall. We discover also that using lemmatization improves the results but stemming doesn’t. Finally we see that a French thesaurus provided us with good results on the most elaborate form of chiasmus.</abstract>
			<keywords>chiasmus, rhetoric, antimetabole, stylistic device</keywords>
		</article>
		<article id="recital-2013-long-013" session="Orale">
			<auteurs>
				<auteur>
					<nom>Maxime Lefrançois</nom>
					<email>maxime.lefrancois@inria.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">WIMMICS, Inria, 2004, route des Lucioles, BP 93, 06902 Sophia Antipolis Cedex</affiliation>
			</affiliations>
			<titre>Représentation des connaissances du DEC: Concepts fondamentaux du formalisme des Graphes d’Unités</titre>
			<type>long</type>
			<pages>164-177</pages>
			<resume>Dans cet article nous nous intéressons au choix d’un formalisme de représentation des connaissances qui nous permette de représenter, manipuler, interroger et raisonner sur des connaissances linguistiques du Dictionnaire Explicatif et Combinatoire (DEC) de la Théorie Sens-Texte. Nous montrons que ni les formalismes du web sémantique ni le formalisme des Graphes conceptuels n’est adapté pour cela, et justifions l’introduction d’un nouveau formalisme dit des Graphes d’Unités. Nous introduisons la hiérarchie des Types d’Unités au coeur du formalisme, et présentons les Graphes d’Unités ainsi que la manière dont on peut les utiliser pour représenter certains aspects du DEC.</resume>
			<mots_cles>Représentation de Connaissances Linguistiques, Théorie Sens-Texte, Graphes d’Unités, Dictionnaire Explicatif et Combinatoire</mots_cles>
			<title>ECD Knowledge Representation : Fundamental Concepts of the Unit Graphs Framework</title>
			<abstract>In this paper we are interested in the choice of a knowledge representation formalism that enables the representation, manipulation, query, and reasoning over linguistic knowledge of the Explanatory and Combinatorial Dictionary (ECD) of the Meaning-Text Theory. We show that neither the semantic web formalisms nor the Conceptual Graphs Formalism suit our needs, and justify the introduction of a new formalism denoted Unit Graphs. We introduce the core of this formalism which is the Unit Types hierarchy, and present Unit Graphs and how one may use them to represent aspects of the ECD.</abstract>
			<keywords>Linguistic Knowledge Representation, Meaning-Text Theory, Unit Graphs, Explanatory and Combinatorial Dictionary</keywords>
		</article>
		<article id="recital-2013-long-014" session="Poster">
			<auteurs>
				<auteur>
					<nom>Corentin Ribeyre</nom>
					<email>corentin.ribeyre@inria.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université Paris 7 Diderot, 75013 PARIS</affiliation>
				<affiliation affiliationId="2">INRIA Paris-Rocquencourt, Rocquencourt BP 105 78153 LE CHESNAY</affiliation>
			</affiliations>
			<titre>Vers un système générique de réécriture de graphes pour l’enrichissement de structures syntaxiques</titre>
			<type>long</type>
			<pages>178-191</pages>
			<resume>Ce travail présente une nouvelle approche pour injecter des dépendances profondes (sujet des verbes à contrôle, partage du sujet en cas d’ellipses, ...) dans un corpus arboré présentant un schéma d’annotation surfacique et projectif. Nous nous appuyons sur un système de réécriture de graphes utilisant des techniques de programmation par contraintes pour produire des règles génériques qui s’appliquent aux phrases du corpus. Par ailleurs, nous testons la généricité des règles en utilisant des sorties de trois analyseurs syntaxiques différents, afin d’évaluer la dégradation exacte de l’application des règles sur des analyses syntaxiques prédites.</resume>
			<mots_cles>réécriture de graphes, évaluation de shéma d’annotations, parsing, analyse en syntaxe profonde</mots_cles>
			<title>Towards a generic graph rewriting system to enrich syntactic structures</title>
			<abstract>This work aims to present a new approach for injecting deep dependencies (subject of control verbs, subject sharing in case of ellipsis, ...) into a surfacic and projective treebank. We use a graph rewriting system with constraint programming techniques for producing generic rules which can be easily applied to a treebank. Moreover, we are testing the genericity of our rules by using output of three different parsers to evaluate how the rules behave on predicted parse trees.</abstract>
			<keywords>graph rewriting system, annotation schemes evaluation, deep syntax parsing</keywords>
		</article>
		<article id="recital-2013-long-015" session="Poster">
			<auteurs>
				<auteur>
					<nom>Mohammad Nasiruddin</nom>
					<email>mohammad.nasiruddin@imag.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Laboratoire d’Informatique de Grenoble-Groupe d’Étude pour la Traduction Automatique/Traitement Automatisé des Langues et de la Parole, Univ. Grenoble Alpes</affiliation>
			</affiliations>
			<titre>État de l'art de l'induction de sens: une voie vers la désambiguïsation lexicale pour les langues peu dotées</titre>
			<type>long</type>
			<pages>192-205</pages>
			<resume>La désambiguïsation lexicale, le processus qui consiste à automatiquement identifier le ou les sens possible d'un mot polysémique dans un contexte donné, est une tâche fondamentale pour le Traitement Automatique des Langues (TAL). Le développement et l'amélioration des techniques de désambiguïsation lexicale ouvrent de nombreuses perspectives prometteuses pour le TAL. En effet, cela pourrait conduire à un changement paradigmatique en permettant de réaliser un premier pas vers la compréhension des langues naturelles. En raison du manque de ressources langagières, il est parfois difficile d'appliquer des techniques de désambiguïsation à des langues peu dotées. C'est pourquoi, nous nous intéressons ici, à enquêter sur comment avoir un début de recherche sur la désambiguïsation lexicale pour les langues peu dotées, en particulier en exploitant des techniques d'induction des sens de mots, ainsi que quelques suggestions de pistes intéressantes à explorer.</resume>
			<mots_cles>désambiguïsation lexicale, induction de sens, langues peu dotées, ressources langagières</mots_cles>
			<title>A State of the Art of Word Sense Induction: A Way Towards Word Sense Disambiguation for Under-Resourced Languages</title>
			<abstract>Word Sense Disambiguation (WSD), the process of automatically identifying the meaning of a polysemous word in a sentence, is a fundamental task in Natural Language Processing (NLP). Progress in this approach to WSD opens up many promising developments in the field of NLP and its applications. Indeed, improvement over current performance levels could allow us to take a first step towards natural language understanding. Due to the lack of lexical resources it is sometimes difficult to perform WSD for under-resourced languages. This paper is an investigation on how to initiate research in WSD for under-resourced languages by applying Word Sense Induction (WSI) and suggests some interesting topics to focus on.</abstract>
			<keywords>Word Sense Disambiguation, Word Sense Induction, under-resourced languages, lexical resources</keywords>
		</article>
		<article id="recital-2013-long-016" session="Poster">
			<auteurs>
				<auteur>
					<nom>Rahma Boujelbane</nom>
					<email>Rahma.boujelbane@gmail.com</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">ANLP_MIRACL, Sfax, Tunisie</affiliation>
				<affiliation affiliationId="2">LIF, UMR7279, 13288, Marseille, France</affiliation>
			</affiliations>
			<titre>Génération des corpus en dialecte tunisien pour la modélisation de langage d'un système de reconnaissance</titre>
			<type>long</type>
			<pages>206-216</pages>
			<resume>Ces derniers temps, vu la situation préoccupante du monde arabe, les dialectes arabes et notamment le dialecte tunisien est devenu de plus en plus utilisé dans les interviews, les journaux télévisés et les émissions de débats. Cependant, cette situation présente des conséquences négatives importantes pour le Traitement Automatique du Langage Naturel (TALN): depuis que les dialectes parlés ne sont pas officiellement écrits et n’ont pas d’orthographe standard, il est très coûteux d'obtenir des corpus adéquats à utiliser pour des outils de TALN. Par conséquent, il n’existe pas des corpus parallèles entre l’Arabe Standard Moderne(ASM) et le Dialecte Tunisien (DT). Dans ce travail, nous proposons une méthode pour la création d’un lexique bilingue ASM–DT et un processus pour la génération automatique de corpus dialectaux. Ces ressources vont servir à la construction d’un modèle de langage pour les journaux télévisés tunisiens, afin de l’intégrer dans un Système de Reconnaissance Automatique de Parole (SRAP).</resume>
			<mots_cles>Dialecte Tunisien, lexique ASM-DT, TDT: Tunisian Dialect Translator</mots_cles>
			<title>Generation of tunisian dialect corpora for adapting language models</title>
			<abstract>Lately, given the serious situation in the Arab world, the Arab dialects such as Tunisian dialect became increasingly used and represented in the interviews, news and debate programs. However, this situation presents negative consequences for Natural Language Processing (NLP): Since dialects are not officially written and have no orthographic standard, it is very costly to obtain adequate corpora to train NLP tools. Therefore, it does not even exist parallel corpora between Standard Arabic (MSA) and Tunisian Dialect(TD). In this work, we propose a method for the creation of a bilingual lexicon MSA-TD and an automatic process for generating dialectal corpora. These resources will be used to build a language model for Tunisian news, in order to integrate it into an Automatic Speech Recognition (ASR).</abstract>
			<keywords>Tunisian Dialect, MSA-TD lexicon, TDT: Tunisian Dialect Translator</keywords>
		</article>
		<article id="recital-2013-long-017" session="Orale">
			<auteurs>
				<auteur>
					<nom>Simon Leva</nom>
					<email>sleva@univ-tlse2.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Nicolas Faessel</nom>
					<email>nicolas.faessel@irit.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">CLLE–ERSS : CNRS et Université de Toulouse (UMR 5263), 5 allées Antonio Machado, 31058 Toulouse Cedex 9</affiliation>
				<affiliation affiliationId="2">IRIT : CNRS et Université de Toulouse (UMR 5505), 118 route de Narbonne, 31062 Toulouse Cedex 9</affiliation>
			</affiliations>
			<titre>Détection automatique des sessions de recherche par similarité des résultats provenant d’une collection de documents externe</titre>
			<type>long</type>
			<pages>217-230</pages>
			<resume>Les utilisateurs d’un système de recherche d’information mettent en oeuvre des comportements de recherche complexes tels que la reformulation de requête et la recherche multitâche afin de satisfaire leurs besoins d’information. Ces comportements de recherche peuvent être observés à travers des journaux de requêtes, et constituent des indices permettant une meilleure compréhension des besoins des utilisateurs. Dans cette perspective, il est nécessaire de regrouper au sein d’une même session de recherche les requêtes reliées à un même besoin d’information. Nous proposons une méthode de détection automatique des sessions exploitant la collection de documents WIKIPÉDIA, basée sur la similarité des résultats renvoyés par l’interrogation de cette collection afin d’évaluer la similarité entre les requêtes. Cette méthode obtient de meilleures performances que les approches temporelle et lexicale traditionnellement employées pour la détection de sessions séquentielles, et peut être appliquée à la détection de sessions imbriquées. Ces expérimentations ont été réalisées sur des données provenant du portail OpenEdition.</resume>
			<mots_cles>Recherche d’information, détection automatique de sessions de recherche, analyse de journal de requêtes</mots_cles>
			<title>Automatic search session detection exploiting results similarity from an external document collection</title>
			<abstract>Search engines users apply complex search behaviours such as query reformulation and multitasking search to satisfy their information needs. These search behaviours may be observed through query logs, and constitute clues allowing a better understanding of users’ needs. In this perspective, it is decisive to group queries related to the same information need into a unique search session. We propose an automatic session detection method exploiting the WIKIPEDIA documents collection, based on the similarity between the results returned for each query pair to estimate the similarity between queries. This method shows better performance than both temporal and lexical approaches traditionally used for successive session detection, and can be applied as well to multitasking search session detection. These experiments were conducted on a dataset originating from the OpenEdition Web portal.</abstract>
			<keywords>Information retrieval, automatic search session detection, query log analysis</keywords>
		</article>
		<article id="recital-2013-long-018" session="Orale">
			<auteurs>
				<auteur>
					<nom>Zhen Wang</nom>
					<email>zhen.wang@geolsemantics.com</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">GEOLSemantics, 32, rue Brancion, 75015, Paris</affiliation>
				<affiliation affiliationId="2">INALCO, ERTIM, 2 rue de Lille, 75343, Paris</affiliation>
			</affiliations>
			<titre>Une approche mixte morpho-syntaxique et statistique pour la reconnaissance d'entités nommées en langue chinoise</titre>
			<type>long</type>
			<pages>231-243</pages>
			<resume>Cet article présente une approche mixte, morpho-syntaxique et statistique, pour la reconnaissance d'entités nommées en langue chinoise dans un système d'extraction automatique d'information. Le processus se divise principalement en trois étapes : la première génère des noms propres potentiels à l'aide de règles morphologiques ; la deuxième utilise un modèle de langue afin de sélectionner le meilleur résultat ; la troisième effectue la reconnaissance d'entités nommées grâce à une analyse syntaxique locale. Cette dernière permet une reconnaissance automatique d'entités nommées plus pertinente et plus complète.</resume>
			<mots_cles>Reconnaissance de noms propres, Reconnaissance d'entités nommées, Traitement automatique du chinois, Extraction d'information, Analyse syntaxique</mots_cles>
			<title>A Mixed Morpho-Syntactic and Statistical Approach to Chinese Named Entity Recognition</title>
			<abstract>This paper presents a morpho-syntactic and statistical approach for Chinese named entity recognition which is a part of an automatic system for information extraction. The process is divided into three steps : first, the generation of possible proper nouns is based on morphological rules; second a language model is used to select the best result, and last, a local syntactic parsing performs the named entity recognition. Syntactic parsing makes named entity recognition more relevant and more complete.</abstract>
			<keywords>Proper noun recogition, Named entity recognition (NER), Chinese Natural Language Processing, Information extraction, Syntactic parsing</keywords>
		</article>
	</articles>
</conference>