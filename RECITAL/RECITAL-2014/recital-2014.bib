@proceedings{RECITAL:2014,
  editor    = {Gala, Núria and Peshkov, Klim},
  title     = {Actes des 16e Rencontres des Étudiants Chercheurs en Informatique pour le Traitement Automatique des Langues},
  month     = {July},
  year      = {2014},
  address   = {Marseille, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  url       = {http://www.atala.org/taln_archives/RECITAL/RECITAL-2014}
}

@inproceedings{michalon:2014:RECITAL,
  author    = {Michalon, Olivier},
  title     = {Modélisation probabiliste de l’interface syntaxe sémantique à l’aide de grammaires hors contexte probabilistes Expériences avec FrameNet},
  booktitle = {Actes des 16e Rencontres des Étudiants Chercheurs en Informatique pour le Traitement Automatique des Langues},
  month     = {July},
  year      = {2014},
  address   = {Marseille, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {1--12},
  url       = {http://www.atala.org/taln_archives/RECITAL/RECITAL-2014/recital-2014-long-001},
  language  = {french},
  note      = {Probabilistic modeling of the syntax semantic interface using probabilistic context free grammars, Experiments with FrameNet},
  resume    = {Cet article présente une méthode générative de prédiction de la structure sémantique en cadres d’une phrase à partir de sa structure syntaxique et décrit les grammaires utilisées ainsi que leurs performances. Ce système permet de prédire, pour un mot dans le contexte syntaxique d’une phrase, le cadre le plus probable. Le système génératif permet d’attribuer à ce mot un cadre et à l’ensemble de chemins des rôles sémantiques. Bien que les résultats ne soient pas encore satisfaisants, cet analyseur permet de regrouper les tâches d’analyse sémantique (sélection du cadre, sélection des actants, attribution des rôles), contrairement aux travaux précédemment publiés. De plus, il offre une nouvelle approche de l’analyse sémantique en cadres, dans la mesure où elle repose plus sur la structure syntaxique que sur les mots de la phrase.},
  abstract  = {This paper presents a generative method for predicting the frame semantic structure of a sentence from its syntactic structure and describes the grammars used with their performances. This system allows to predict, for a word in the syntactic context of a sentence, the most probable frame. The generative system allows to give a frame to a word and semantic roles to a set of pathes. Although results are not yet satisfying, this parser allows to group semantic parsing tasks (frame selection, role fillers selection, role assignment) unlike previously published works. In addition, it offers a new approach to parse semantic frames insofar as it is based more on syntactic structure rather than words of the sentence.},
  motscles  = {Analyse sémantique automatique, interface syntaxe sémantique, FrameNet},
  keywords  = {Automatic Semantic Parsing, syntax semantic parsing, FrameNet},
}

@inproceedings{djemaa:2014:RECITAL,
  author    = {Djemaa, Marianne},
  title     = {Traitement FrameNet des constructions à attribut de l’objet},
  booktitle = {Actes des 16e Rencontres des Étudiants Chercheurs en Informatique pour le Traitement Automatique des Langues},
  month     = {July},
  year      = {2014},
  address   = {Marseille, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {13--24},
  url       = {http://www.atala.org/taln_archives/RECITAL/RECITAL-2014/recital-2014-long-002},
  language  = {french},
  note      = {Addressing object predicative complements in a French FrameNet},
  resume    = {Dans le cadre du projet ASFALDA, qui comporte une phase d’annotation sémantique d’un FrameNet français, nous cherchons à fournir un traitement linguistiquement motivé des constructions à attribut de l’objet, un exemple typique de divergence syntaxe-sémantique. Pour ce faire, nous commençons par dresser un panorama des propriétés syntaxiques et sémantiques des constructions à attribut de l’objet. Nous étudions ensuite le traitement FrameNet des verbes anglais typiques de cette construction, avant de nous positionner pour un traitement homogénéisé dans le cas du FrameNet français.},
  abstract  = {Within the ASFALDA project, which includes the production of a French FrameNet, we try to provide a linguistically motivated treatment for a typical example of syntax-semantics mismatch : object complement construction. In order to do so, we first give an overview of syntactic and semantic properties of object complement constructions. Next, we study the way FrameNet deals with English verbs taking part in those constructions, and finally take a stance for a homogenized treatment of the construction within the French FrameNet.},
  motscles  = {FrameNet, français, construction à attribut de l’objet, divergence syntaxe-sémantique},
  keywords  = {FrameNet, French, object complement construction, syntax-semantics mismatch},
}

@inproceedings{latour:2014:RECITAL,
  author    = {Latour, Marilyne},
  title     = {Expressions différenciées des besoins informationnels en Langue Naturelle : construction de profils utilisateurs en fonction des tâches de recherche d’informations},
  booktitle = {Actes des 16e Rencontres des Étudiants Chercheurs en Informatique pour le Traitement Automatique des Langues},
  month     = {July},
  year      = {2014},
  address   = {Marseille, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {25--36},
  url       = {http://www.atala.org/taln_archives/RECITAL/RECITAL-2014/recital-2014-long-003},
  language  = {french},
  note      = {Distinctions in the Formulation of Information Needs in Natural Language; Construction of a Task-Based User Profile Database},
  resume    = {Devant des collections massives et hétérogènes de données, les systèmes de RI doivent désormais pouvoir appréhender des comportements d’utilisateurs aussi variés qu’imprévisibles. L’objectif de notre approche est d’évaluer la façon dont un utilisateur verbalise un besoin informationnel à travers un énoncé de type « expression libre » ; appelé langage naturel (LN). Pour cela, nous nous situons dans un contexte applicatif, à savoir des demandes de remboursement des utilisateurs d’un moteur de recherche dédié à des études économiques en français. Nous avons recueilli via ce moteur, les demandes en LN sur 5 années consécutives totalisant un corpus de 1398 demandes. Nous avons alors comparé l’expression en tant que tel du besoin informationnel en fonction de la tâche de recherche d’informations (RI) de l’utilisateur.},
  abstract  = {With the massive and heterogeneous web document collections, IR system must analyze the behaviors of users which are unpredictable and varied. The approach described in this paper provides a description of the verbalizations of the information need in natural language. For this, we used data collected (i.e. users’ complaints in natural language) through a search engine dedicated to economic reports in French over 5 consecutive years totaling a corpus of 1398 natural language requests. Then, we compared the expression as such of the information need according to the IR task.},
  motscles  = {Recherche informations, Besoin informationnel, Expression et interprétation des besoins, Formulation question, Langage naturel, comportement utilisateur, tâches de recherche d’informations},
  keywords  = {Information Retrieval, Information Need, Query formulation and Query Expression, Query Formulation, Natural Language, User Behavior, IR task},
}

@inproceedings{wandjitchami:2014:RECITAL,
  author    = {Wandji Tchami, Ornella},
  title     = {Les modèles de description du verbe dans les travaux de Linguistique, Terminologie et TAL},
  booktitle = {Actes des 16e Rencontres des Étudiants Chercheurs en Informatique pour le Traitement Automatique des Langues},
  month     = {July},
  year      = {2014},
  address   = {Marseille, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {37--48},
  url       = {http://www.atala.org/taln_archives/RECITAL/RECITAL-2014/recital-2014-long-004},
  language  = {french},
  note      = {The descriptive approaches of the verb in Linguistics, Terminology and NLP},
  resume    = {Dans le cadre de notre projet de recherche, qui a pour but l’implémentation d’un outil de simplification des emplois spécialisés de verbes dans des corpus médicaux à partir de l’analyse syntaxico-sémantique de ces verbes en contexte, nous proposons une analyse de quelques approches et travaux qui ont pour objet principal la description du verbe dans les trois domaines de recherche à l’interface desquels se situe notre projet : linguistique, TAL et terminologie. Nous décrivons plus particulièrement les travaux qui peuvent avoir une incidence sur notre étude. Cet état de l’art nous permet de mieux connaître le cadre théorique dans lequel s’intègre notre projet de recherche et d’avoir les repères et références susceptibles de contribuer à sa réalisation.},
  abstract  = {As part of our research project, which aims to implement a text simplification tool for the specialized usages of verbs in medical corpora using the syntactic and semantic analysis of these verbs in context, we propose an overview of some approaches and work whose main research object is the description of verbs, within the three research areas which interface our study is : linguistics, terminology and NLP. We pay a particular attention to studies that can have an impact on our work. This state of the art allows us to better understand the theoretical framework related to our research project. Moreover, it allows us to have benchmarks and references that might be usefull for the realization of our project.},
  motscles  = {Verbe terminologique ou spécialisé, sémantique des cadres, sémantique lexicale, structure argumentale, étiquetage en rôles sémantiques},
  keywords  = {Specialized verb, Frame Semantics, lexical semantics, argumental structure, Semantic Role Labeling},
}

@inproceedings{tafforeau:2014:RECITAL,
  author    = {Tafforeau, Jérémie},
  title     = {Réseau de neurones profond pour l’étiquetage morpho-syntaxique},
  booktitle = {Actes des 16e Rencontres des Étudiants Chercheurs en Informatique pour le Traitement Automatique des Langues},
  month     = {July},
  year      = {2014},
  address   = {Marseille, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {49--58},
  url       = {http://www.atala.org/taln_archives/RECITAL/RECITAL-2014/recital-2014-long-005},
  language  = {french},
  note      = {Deep Neural Network applied to Part-of-Speech Tagging},
  resume    = {L’analyse syntaxique et sémantique de langages non-canoniques est principalement limitée par le manque de corpus annotés. Il est donc primordial de mettre au point des systèmes robustes capables d’allier références canoniques et non-canoniques. Les méthodes exploitant la théorie des réseaux de neurones profonds ont prouvé leur efficacité dans des domaines tels que l’imagerie ou les traitements acoustiques. Nous proposons une architecture de réseau de neurones appliquée au traitement automatique des langages naturels, et plus particulièrement à l’étiquetage morpho-syntaxique. De plus, plutôt que d’extraire des représentations empiriques d’une phrase pour les injecter dans un algorithme de classification, nous nous inspirons de récents travaux portant sur l’extraction automatique de représentations vectorielles des mots à partir de corpus non-annotés. Nous souhaitons ainsi tirer profit des propriétés de linéarité et de compositionnalité de tels plongements afin d’améliorer les performances de notre système.},
  abstract  = {Syntactic and semantic parsing of non-canonical languages is mainly restricted by the lack of labelled data sets. It is thus essential to develop strong systems capable of combining canonical and non-canonical text corpora. Deep Learning methods proved their efficiency in domains such as imaging or acoustic process.We propose neural network architecture applied to natural languages processing. Furthermore, instead of extracting from the sentence a rich set of hand-crafted features wich are the fed to a standard classification algorithm, we drew our inspiration from recent papers about the automatic extraction of word embeddings from large unlabelled data sets. On such embeddings, we expect to benefit from linearity and compositionality properties to improve our system performances.},
  motscles  = {TALN, Étiquetage morpho-syntaxique, Apprentissage Automatique, Réseau de Neurones Profond, Plongements},
  keywords  = {NLP, Part-of-Speech Tagging, Machine Learning, Deep Neural Network, Embeddings},
}

@inproceedings{korenchuk:2014:RECITAL,
  author    = {Korenchuk, Yuliya},
  title     = {Extraction terminologique : vers la minimisation de ressources},
  booktitle = {Actes des 16e Rencontres des Étudiants Chercheurs en Informatique pour le Traitement Automatique des Langues},
  month     = {July},
  year      = {2014},
  address   = {Marseille, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {59--70},
  url       = {http://www.atala.org/taln_archives/RECITAL/RECITAL-2014/recital-2014-long-006},
  language  = {french},
  note      = {Terminology Extraction: Towards Resource Minimization},
  resume    = {Cet article présente une méthode ayant pour objectif de minimiser l’apport extérieur nécessaire à la tâche d’extraction terminologique (ET) et de rendre cette tâche moins dépendante de la langue. Pour cela, la méthode prévoit des ressources morphologiques et morphosyntaxiques simplifiées construites directement à partir d’un corpus lemmatisé. Ces ressources endogènes servent à la création d’un système de filtres qui affinent les calculs statistiques et à la génération de patrons pour l’identification de candidats termes polylexicaux. La méthode a été testée sur deux corpus comparables en chimie et en télécommunication, en français et en anglais. La précision observée sur les 100 premiers candidats termes monolexicaux fluctue entre 71% et 87% pour le français et entre 44 % et 69 % en anglais ; celle des candidats termes polylexicaux s’élève à 69-78 % en français et 69-85 % en anglais en fonction du domaine.},
  abstract  = {The article presents the method which aims to minimize the use of external resources for the terminology extraction task and to make this task less langage dependent. For that purpose, the method builds simplified morphological and morphosyntactic resources directly from a lemmatized corpus. These endogenous resources are used both in filters, which refine the statistical calculations, and in patterns for polylexical terms identification. The method was tested on two comparable corpora in chemistry and in telecommunication in French and in English. The precision observed on the first 100 monolexical terms fluctuates between 71% and 87% for French and between 44% and 69% in English ; for polylexical terms the precision was 69-78% in French and 69-85% in English depending on the domain.},
  motscles  = {extraction terminologique, ressources endogènes, apprentissage automatique},
  keywords  = {terminology extraction, endogenous resources, machine learning},
}

@inproceedings{hadjadj:2014:RECITAL,
  author    = {Hadjadj, Mohamed},
  title     = {Une description des structures de la durée en Langue des Signes Française à partir d’une grammaire formelle},
  booktitle = {Actes des 16e Rencontres des Étudiants Chercheurs en Informatique pour le Traitement Automatique des Langues},
  month     = {July},
  year      = {2014},
  address   = {Marseille, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {71--80},
  url       = {http://www.atala.org/taln_archives/RECITAL/RECITAL-2014/recital-2014-long-007},
  language  = {french},
  note      = {Description of structures of time (in French sign 
language) based on a formal grammar},
  resume    = {Dans cet article, nous abordons la problématique du fonctionnement de la temporalité en langue des signes française (LSF). Nous allons étudier plus particulièrement quelques structures portant sur la durée. Nous présenterons dans un premier temps les descriptions existantes du système aspecto-temporel de la LSF et les difficultés que nous trouvons pour modéliser ces travaux. Le but de cet article est de proposer une grammaire formelle qui prenne en compte le fonctionnement de la LSF et qui puisse faire l’objet d’un traitement de modélisation. Notre démarche consiste à étudier un corpus LSF pour établir des liens de fonction à forme afin d’obtenir des règles de grammaire qu’on peut générer dans un projet de synthèse à l’aide d’un signeur avatar.},
  abstract  = {Temporality constitutes a major issue in filed of modeling french signed language (LSF). In fact, it is very difficult to model actual discriptions of the aspect-temporal systems of LSF. In this paper we present the bases of a novel formal grammar that permits the modeling of the LSF. This paper presents a study to construct this grammar. We analysed a French SL corpus to create formel rool between the signed gesture and its signification. Our objective is to obtain rules of grammar that can generate a synthesis project using a signer avatar.},
  motscles  = {grammaire, LSF, temporalité, modélisation LSF},
  keywords  = {grammar, LSF, temporality, modeling LSF},
}

@inproceedings{letard:2014:RECITAL,
  author    = {Letard, Vincent},
  title     = {Interaction homme-machine en domaine large à l’aide du langage naturel : une amorce par mise en correspondance},
  booktitle = {Actes des 16e Rencontres des Étudiants Chercheurs en Informatique pour le Traitement Automatique des Langues},
  month     = {July},
  year      = {2014},
  address   = {Marseille, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {81--91},
  url       = {http://www.atala.org/taln_archives/RECITAL/RECITAL-2014/recital-2014-long-008},
  language  = {french},
  note      = {Natural Human-Machine Interaction for Manipulating Formal Language: Bootstrapping with Mapping},
  resume    = {Cet article présente le problème de l’association entre énoncés en langage naturel exprimant des instructions opérationnelles et leurs expressions équivalentes et langage formel. Nous l’appliquons au cas du français et du langage R. Développer un assistant opérationnel apprenant, qui constitue notre objectif à long terme, requiert des moyens pour l’entraîner et l’évaluer, c’est-à-dire un système initial capable d’interagir avec l’utilisateur. Après avoir introduit la ligne directrice de ce travail, nous proposons un modèle pour représenter le problème et discutons de l’adéquation des méthodes par mise en correspondance, ou mapping, à notre tâche. Pour finir, nous montrons que, malgré des scores modestes, une approche simple semble suffisante pour amorcer un tel système interactif apprenant.},
  abstract  = {We consider the problem of mapping natural language written utterances expressing operational instructions to formal language expressions, applied to French and the R programming language. Designing a learning operational assistant, which is our long term goal, requires the means to train and evaluate it, that is, a baseline system able to interact with the user. After presenting the guidelines of our work, we propose a model to represent the problem and discuss the fit of direct mapping methods to our task. Finally, we show that, while not resulting in excellent scores, a simple approach seems to be sufficient to bootstrap an interactive learning system.},
  motscles  = {assistants interactifs, apprentissage artificiel, systèmes de question-réponse},
  keywords  = {interactive assistants, machine learning, question answering systems},
}

@inproceedings{ollinger:2014:RECITAL,
  author    = {Ollinger, Sandrine},
  title     = {Regroupement de structures de dérivations lexicales par raisonnement analogique},
  booktitle = {Actes des 16e Rencontres des Étudiants Chercheurs en Informatique pour le Traitement Automatique des Langues},
  month     = {July},
  year      = {2014},
  address   = {Marseille, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {92--103},
  url       = {http://www.atala.org/taln_archives/RECITAL/RECITAL-2014/recital-2014-long-009},
  language  = {french},
  note      = {Merging structures of lexical derivations by analogical reasoning},
  resume    = {Cet article propose une méthode de regroupement de structures de dérivations lexicales par raisonnement analogique. Nous présentons les caractéristiques générales d’un graphe lexical issu du Réseau Lexical du Français, dont nous exploitons par la suite les composantes faiblement connexes. Ces composantes sont regroupées en trois étapes : par isomorphisme, par similarité de relations, puis par similarité d’attributs. Les résultats du dernier regroupement sont analysés en détail.},
  abstract  = {This paper presents a method for merging structures of lexical derivations by analogical reasoning. Following the presentation of general features of a lexical graph from the French Lexical Network, we focus on the weak connected components of this graph. This components are grouped together in three steps : by isomorphism, by relational similarity and finally by attributional similarity. The results of the last merging are analyzed in detail.},
  motscles  = {graphe lexical, composantes connexes, analogie, raisonnement analogique, dérivation lexicale},
  keywords  = {lexical graph, analogy, connected components, analogical reasoning, lexical derivation},
}

@inproceedings{trione:2014:RECITAL,
  author    = {Trione, Jérémy},
  title     = {Méthodes par extraction pour le résumé automatique de conversations parlées provenant de centres d’appels},
  booktitle = {Actes des 16e Rencontres des Étudiants Chercheurs en Informatique pour le Traitement Automatique des Langues},
  month     = {July},
  year      = {2014},
  address   = {Marseille, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {104--111},
  url       = {http://www.atala.org/taln_archives/RECITAL/RECITAL-2014/recital-2014-long-010},
  language  = {french},
  note      = {Extraction methods for automatic summarization of spoken conversations from call centers},
  resume    = {Dans ce papier nous traitons des résumés automatiques de conversations parlées spontanées. Pour cela nous utilisons des conversations provenant de cas réels d’appels téléphoniques de centre d’appels issues du corpus DECODA. Nous testons des méthodes extractives classiques utilisées en résumé de texte (MMR) ainsi que des méthodes basées sur des heuristiques du dialogue dans le cadre des centres d’appels. Il s’agit de la sélection du tour de parole le plus long dans le premier quart de la conversation, dans l’ensemble de la conversation et dans le dernier quart de la conversation. L’ensemble est évalué avec la métrique ROUGE. Les résultats obtenus soulignent les limites de ces approches « classiques » et confirment la nécessité d’envisager des méthodes abstractives intégrant des informations de structures sur les conversations. En effet, ces premiers résultats montrent que les méthodes heuristiques basées sur la structure produisent des résultats comparables, voir meilleurs que des méthodes telles que MMR.},
  abstract  = {In this paper we speak about automatic spoken conversation summaries. We use conversation from some real cases call from a call center extracted from the DECODA corpus. We test some extractive summary methods used in text summary (MMR) and some dialogue heuristics methods. It’s mainly to select the longest speaker turn in different part of the dialogue, the first quarter, the whole dialogue, and the last quarter of the dialogue. All the results are evaluated thanks the ROUGE software. The results show the limits of these classical approaches and suggest that we need some abstractive methods including some structural features of the conversation. In fact, these results show that the structural heuristics based methods are even or better than the classic method like MMR.},
  motscles  = {Résumé de conversations parlées, résumé par extraction, ROUGE, corpus DECODA, MMR},
  keywords  = {spoken conversation summarization, extractive summary, ROUGE, DECODA corpus, MMR},
}

@inproceedings{hmida:2014:RECITAL,
  author    = {Hmida, Firas},
  title     = {Identification de Contextes Riches en Connaissances en corpus comparable},
  booktitle = {Actes des 16e Rencontres des Étudiants Chercheurs en Informatique pour le Traitement Automatique des Langues},
  month     = {July},
  year      = {2014},
  address   = {Marseille, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {112--123},
  url       = {http://www.atala.org/taln_archives/RECITAL/RECITAL-2014/recital-2014-long-011},
  language  = {french},
  note      = {Knowledge-Rich Contexts Extraction from Comparable Corpora},
  resume    = {Dans les études s’intéressant à la traduction assistée par ordinateur (TAO), l’un des objectifs consiste à repérer les passages qui focalisent l’attention des traducteurs humains. Ces passages sont considérés comme étant des contextes « riches en connaissances », car ils aident à la traduction. Certains contextes textuels ne donnent qu’une simple attestation d’un terme recherché, même s’ils le renferment. Ils ne fournissent pas d’informations qui permettent de saisir sa signification. D’autres, en revanche, contiennent des fragments de définitions, mentionnent une variante terminologique ou utilisent d’autres notions facilitant la compréhension du terme. Ce travail s’intéresse aux « contextes définitoires » qui sont utiles à l’acquisition de connaissances à partir de textes, en particulier dans la perspective de traduction terminologique assistée par ordinateur à partir de corpus comparables. En effet, l’association d’un exemple à un terme permet d’en appréhender le sens exact. Nous proposons, tout d’abord, trois hypothèses définissant la notion d’exemple définitoire. Ensuite nous évaluons sa validité grâce une méthode s’appuyant sur les Contextes Riches en Connaissances (CRC) ainsi que les relations hiérarchiques reliant les termes entre eux.},
  abstract  = {Some contexts provide only a simple explanation of a given term even if they contain it. However, others contain fragments of definitions, mention a terminological variant or use other concepts to make it easy the term understanding. In this work we focus on « definitory contexts » that would be valuable to a human for knowledge acquisition from texts, mainly in order to assist in terminological translation from comparable corpora. Indeed, provide the term with an example, makes it possible to understand its exact meaning. First, we specify three hypothesis defining the concept of a defnitory example. Then we evaluate its validity through a method based on the knowledge-Rich Contexts (KRCs) and hierarchical relationships between terms.},
  motscles  = {Contextes Riches en Connaissances, CRC, identification de définitions, identification d’exemples, énoncé définitoire, terminologie, traduction terminologique},
  keywords  = {Knowledge-Rich Contexts, KRCs, mining definitions, minging examples, terminology, terminological translation},
}

@inproceedings{bensalembahloul-elkarwi:2014:RECITAL,
  author    = {Bensalem Bahloul, Raja and Elkarwi, Marwa},
  title     = {Induction d’une grammaire de propriétés à granularité variable à partir du treebank arabe ATB},
  booktitle = {Actes des 16e Rencontres des Étudiants Chercheurs en Informatique pour le Traitement Automatique des Langues},
  month     = {July},
  year      = {2014},
  address   = {Marseille, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {124--135},
  url       = {http://www.atala.org/taln_archives/RECITAL/RECITAL-2014/recital-2014-long-012},
  language  = {french},
  note      = {Induction of a variable granularity property grammar from the Arabic Treebank ATB},
  resume    = {Dans cet article, nous présentons une démarche pour l’induction d’une grammaire de propriétés (GP) arabe en utilisant le treebank ATB. Cette démarche se base sur deux principales étapes : (1) l’induction d’une grammaire hors contexte et (2) l’induction d’une GP par la génération automatique des relations qui peuvent exister entre les unités grammaticales décrites dans la CFG. Le produit obtenu constitue une ressource ouvrant de nouvelles perspectives pour la description et le traitement de la langue arabe.},
  abstract  = {This paper presents an approach for building an Arabic property grammar using the treebank ATB. This approach consists in two main steps: (1) inducing a context-free grammar from a treebank and (2) inducing a property grammar. So, we acquire first a context-free grammar (CFG) from the source treebank and then, we induce the property grammar by generating automatically existing relations between grammatical units described in the CFG. The result is a new resource for Arabic, opening the way to new tools and descriptions.},
  motscles  = {Treebanks, langue arabe, grammaire hors-contexte, grammaires de propriétés},
  keywords  = {Treebanks, Arabic language, context-free grammar, property grammars},
}