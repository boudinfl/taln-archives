<?xml version="1.0" encoding="UTF-8"?>

<conference>
	<edition>
		<acronyme>RECITAL'2015</acronyme>
		<titre>17e Rencontres des Étudiants Chercheurs en Informatique pour le Traitement Automatique des Langues</titre>
		<ville>Caen</ville>
		<pays>France</pays>
		<dateDebut>2015-06-22</dateDebut>
		<dateFin>2015-06-25</dateFin>
		<presidents>
			<president>
				<prenom>Charlotte</prenom>
				<nom>Lecluze</nom>
			</president>
			<president>
				<prenom>Jose G</prenom>
				<nom>Moreno</nom>
			</president>
		</presidents>
		<editeurs>
			<editeur>
				<prenom>Jose G</prenom>
				<nom>Moreno</nom>
			</editeur>
		</editeurs>
		<typeArticles>
			<type id="long">Papiers longs</type>
		</typeArticles>
		<siteWeb>http://taln2015.greyc.fr</siteWeb>
	</edition>
	<articles>
		<article id="recital-2015-long-001" session="Compréhension et paraphrase">
			<auteurs>
				<auteur>
					<prenom>Benoît</prenom>
					<nom>Couderc</nom>
					<email>benoit.couderc@etu.univ-amu.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Jérémy</prenom>
					<nom>Ferrero</nom>
					<email>jeremy.ferrero@imag.fr</email>
					<affiliationId>2</affiliationId>
					<affiliationId>3</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Aix Marseille Université, Marseille, France</affiliation>
				<affiliation affiliationId="2">Compilatio, 276 rue du Mont Blanc, 74540 Saint-Félix, France</affiliation>
				<affiliation affiliationId="3">LIG-GETALP, Université Grenoble Alpes, France</affiliation>
			</affiliations>
			<titre>fr2sql : Interrogation de bases de données en français</titre>
			<type>long</type>
			<pages>1-12</pages>
			<resume>Les bases de données sont de plus en plus courantes et prennent de plus en plus d'ampleur au sein des applications et sites Web actuels. Elles sont souvent amenées à être utilisées par des personnes n'ayant pas une grande compétence en la matière et ne connaissant pas rigoureusement leur structure. C'est pour cette raison que des traducteurs du langage naturel aux requêtes SQL sont développés. Malheureusement, la plupart de ces traducteurs se cantonnent à une seule base du fait de la spéciﬁcité de l'architecture de celle-ci. Dans cet article, nous proposons une méthode visant à pouvoir interroger n'importe quelle base de données à partir de questions en français. Nous évaluons notre application sur deux bases à la structure différente et nous montrons également qu'elle supporte plus d'opérations que la plupart des autres traducteurs.</resume>
			<mots_cles>Base de données (BdD), Requêtes SQL, Traducteur français SQL, Interface Homme-BdD</mots_cles>
			<title>fr2sql : database query in French</title>
			<abstract>Databases are increasingly common and are becoming increasingly important in actual applications and Web sites. They often used by people who do not have great competence in this domain and who do not know exactly their structure. This is why translators from natural language to SQL queries are developed. Unfortunately, most of these translators is conﬁned to a single database due to the speciﬁcity of the base architecture. In this paper, we propose a method to query any database from french. We evaluate our application on two different databases and we also show that it supports more operations than most other translators.</abstract>
			<keywords>Databases (DB), Structured Query Language (SQL), French to SQL translator, Natural language interfaces to databases (NLIDB)</keywords>
		</article>
		<article id="recital-2015-long-002" session="Désambiguïsation">
			<auteurs>
				<auteur>
					<prenom>Mokhtar Boumedyen</prenom>
					<nom>Billami</nom>
					<email>mokhtar.billami@lif.univ-mrs.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Aix-Marseille Université, LIF UMR 7279, 163 avenue de Luminy, 13288 Marseille Cedex 9</affiliation>
			</affiliations>
			<titre>Désambiguïsation lexicale à base de connaissances par sélection distributionnelle et traits sémantiques</titre>
			<type>long</type>
			<pages>13-24</pages>
			<resume>La désambiguïsation lexicale permet d'améliorer de nombreuses applications en traitement automatique des langues (TAL) comme la recherche d'information, l'extraction d'information, la traduction automatique, ou la simplification lexicale de textes. Schématiquement, il s'agit de choisir quel est le sens le plus approprié pour chaque mot d'un texte. Une des approches classiques consiste à estimer la similarité sémantique qui existe entre les sens de deux mots puis de l'étendre à l'ensemble des mots du texte. La méthode la plus directe donne un score de similarité à toutes les paires de sens de mots puis choisit la chaîne de sens qui retourne le meilleur score (on imagine la complexité exponentielle liée à cette approche exhaustive). Dans cet article, nous proposons d'utiliser une méta-heuristique d'optimisation combinatoire qui consiste à choisir les voisins les plus proches par sélection distributionnelle autour du mot à désambiguïser. Le test et l'évaluation de notre méthode portent sur un corpus écrit en langue française en se servant du réseau sémantique BabelNet. Le taux d'exactitude obtenu est de 78% sur l'ensemble des noms et des verbes choisis pour l'évaluation.</resume>
			<mots_cles>désambiguïsation lexicale non supervisée, mesure de similarité distributionnelle, mesures de similarité sémantique</mots_cles>
			<title>A Knowledge-Based Approach to Word Sense Disambiguation by distributional selection and semantic features</title>
			<abstract>Word sense disambiguation improves many Natural Language Processing (NLP) applications such as Information Retrieval, Information Extraction, Machine Translation, or Lexical Simplification. Roughly speaking, the aim is to choose for each word in a text its best sense. One of the most popular method estimates local semantic similarity relatedness between two word senses and then extends it to all words from text. The most direct method computes a rough score for every pair of word senses and chooses the lexical chain that has the best score (we can imagine the exponential complexity that returns this comprehensive approach). In this paper, we propose to use a combinatorial optimization metaheuristic for choosing the nearest neighbors obtained by distributional selection around the word to disambiguate. The test and the evaluation of our method concern a corpus written in French by means of the semantic network BabelNet. The obtained accuracy rate is 78 % on all names and verbs chosen for the evaluation.</abstract>
			<keywords>unsupervised word sense disambiguation, distributional similarity measure, semantic similarity measures</keywords>
		</article>
		<article id="recital-2015-long-003" session="Opinions et sentiments">
			<auteurs>
				<auteur>
					<prenom>Caroline</prenom>
					<nom>Langlet</nom>
					<email>caroline.langlet@telecom-paristech.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Institut Mines-Télécom ; Télécom ParisTech ; CNRS LTCI, Paris</affiliation>
			</affiliations>
			<titre>Vers un modèle de détection des affects, appréciations et jugements dans le cadre d'interactions humain-agent</titre>
			<type>long</type>
			<pages>25-37</pages>
			<resume>Cet article aborde la question de la détection des expressions d'attitude – i.e affect, d'appréciation et de jugement (Martin &amp; White, 2005) – dans le contenu verbal de l'utilisateur au cours d'interactions en face-à-face avec un agent conversationnel animé. Il propose un positionnement en termes de modèles et de méthodes pour le développement d'un système de détection adapté aux buts communicationnels de l'agent et à une parole conversationnelle. Après une description du modèle théorique de référence choisi, l'article propose un modèle d'annotation des attitudes dédié l'exploration de ce phénomène dans un corpus d'interaction humain-agent. Il présente ensuite une première version de notre système. Cette première version se concentre sur la détection des expressions d'attitudes pouvant référer à ce qu'aime ou n'aime pas l'utilisateur. Le système est conçu selon une approche symbolique fondée sur un ensemble de règles sémantiques et de représentations logico-sémantiques des énoncés.</resume>
			<mots_cles>Analyse de sentiments, interaction humain-agent, agents conversationnels animés, dialogue homme-machine</mots_cles>
			<title>Toward a detection model of affects, appreciations, jugements within human-agent interactions</title>
			<abstract>This article concerns the detection of attitudes – i.e affects, appreciations and jugements (Martin &amp; White, 2005) – in the user's verbal content during a face-to-face interaction with an embodied conversational agent. It tackles the issue of the adaptation to the ECA's communicational goals and to the conversational speech. After a description of our theoretical model, it introduces an annotation model dedicated to the study of attitudes in a human-agent interaction corpus. Then, it describes a ﬁrst version of our detection system, focusing on the attitude which can refer to a user's like or dislike. The system is rule-based and embeds logic and semantic representations of the sentences.</abstract>
			<keywords>Sentiment analysis, human-agent interaction, embodied conversational agents, dialogue homme-machine</keywords>
		</article>
		<article id="recital-2015-long-004" session="Posters">
			<auteurs>
				<auteur>
					<prenom>Maâli</prenom>
					<nom>Mnasri</nom>
					<email>maali.mnasri@cea.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">CEA,LIST, Laboratoire Vision et Ingénierie des Contenus, Gif-sur-Yvette, F-91191 France</affiliation>
				<affiliation affiliationId="2">Univ. Paris Sud, Orsay, France</affiliation>
			</affiliations>
			<titre>Résumé Automatique Multi-Document Dynamique : État de l'Art</titre>
			<type>long</type>
			<pages>38-49</pages>
			<resume>Les travaux menés dans le cadre du résumé automatique de texte ont montré des résultats à la fois très encourageants mais qui sont toujours à améliorer. La problématique du résumé automatique ne cesse d'évoluer avec les nouveaux champs d'application qui s'imposent, ce qui augmente les contraintes liées à cette tâche. Nous nous inté- ressons au résumé extractif multi-document dynamique. Pour cela, nous examinons les différentes approches existantes en mettant l'accent sur les travaux les plus récents. Nous montrons ensuite que la performance des systèmes de résumé multi-document et dynamique est encore modeste. Trois contraintes supplémentaires sont ajoutées : la redondance inter-document, la redondance à travers le temps et la grande taille des données à traiter. Nous essayons de déceler les insufﬁsances des systèmes existants aﬁn de bien déﬁnir notre problématique et guider ainsi nos prochains travaux.</resume>
			<mots_cles>Résumé multi-document, résumé dynamique, redondance, évaluation</mots_cles>
			<title>Automatic multi-document update summarization : State of the Art</title>
			<abstract>The ﬁeld of automatic text summarization is characterized both by some interesting achievements and a lot of issues to address, especially with the introduction of new tasks brought by applications. In this article, we focus more particularly on the multi-document update summarization task and review the existing work about it with a special emphasis on recent work. We show that the results for this task are still low because of the necessity to take into account three important constraints : information redundancy through documents and time and the size of data. We analyze the strengths and weaknesses of existing systems according to these constraints to propose subsequently new solutions.</abstract> 
			<keywords>Multi-document summarization, update summarization, redundancy, evaluation</keywords>
		</article>
		<article id="recital-2015-long-005" session="Posters">
			<auteurs>
				<auteur>
					<prenom>Hugo</prenom>
					<nom>Mougard</nom>
					<email>hugo.mougard@univ-nantes.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université de Nantes</affiliation>
			</affiliations>
			<titre>Alignement multimodal de ressources éducatives et scientiﬁques</titre>
			<type>long</type>
			<pages>50-59</pages>
			<resume>Cet article présente certaines questions de recherche liées au projet COC O 1 . L'ambition de ce projet est de valoriser les ressources éducatives et académiques en exploitant au mieux les différents médias disponibles (vidéos de cours ou de présentations d'articles, manuels éducatifs, articles scientiﬁques, présentations, etc). Dans un premier temps, nous décrirons le problème d'utilisation jointe de ressources multimédias éducatives ou scientiﬁques pour ensuite introduire l'état de l'art dans les domaines concernés. Cela nous permettra de présenter quelques questions de recherche sur lesquelles porteront des études ultérieures. Enﬁn nous ﬁnirons en introduisant trois prototypes développés pour analyser ces questions.</resume>
			<mots_cles>alignement, e-research, e-learning, multimodalité</mots_cles>
			<title>Multimodal Alignment of Educative and Scientiﬁc Resources</title>
			<abstract>This article presents some questions linked to the COC O 1 project. The ambition of this project is to better exploit educa- tive and academic resources by leveraging the different available supports (courses and conference talks videos, scientiﬁc articles, textbooks, slides, etc). We will ﬁrst describe the problem of joint educative or scientiﬁc multimedia resources use. We will then give an overview of the related state of the art that will allow us to introduce a few research questions that will be the subject of further studies. Finally we will introduce three research prototypes that are helpful to start to investigate those research questions.</abstract>
			<keywords>alignment, e-research, e-learning, multimodality</keywords>
		</article>
		<article id="recital-2015-long-006" session="Posters">
			<auteurs>
				<auteur>
					<prenom>Souﬁan</prenom>
					<nom>Salim</nom>
					<email>souﬁan.salim@univ-nantes.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LINA UMR 6241 - Université de Nantes, 2 rue de la houssinière, 44322 Nantes Cedex 03</affiliation>
			</affiliations>
			<titre>État de l'art : analyse des conversations écrites en ligne porteuses de demandes d'assistance en termes d'actes de dialogue</titre>
			<type>long</type>
			<pages>60-71</pages>
			<resume>Le développement du Web 2.0 et le processus de création et de consommation massive de contenus générés par les utilisateurs qu'elle a enclenché a permis le développement de nouveaux types d'interactions chez les internautes. En particulier, nous nous intéressons au développement du support en ligne et des plate-formes d'entraide. En effet, les archives de conversations en ligne porteuses de demandes d'assistance représentent une ressource inestimable, mais peu exploitée. L'exploitation de cette ressource permettrait non seulement d'améliorer les systèmes liés à la résolution collaborative des problèmes, mais également de perfectionner les canaux de support proposés par les entreprises opérant sur le web. Pour ce faire, il est cependant nécessaire de déﬁnir un cadre formel pour l'analyse discursive de ce type de conversations. Cet article a pour objectif de présenter l'état de la recherche en analyse des conversations écrites en ligne, sous différents médiums, et de montrer dans quelle mesure les différentes méthodes exposées dans la littérature peuvent être appliquées à des conversations fonctionnelles inscrites dans le cadre de la résolution collaborative des problèmes utilisateurs.</resume>
			<mots_cles>Analyse discursive, Conversation, Résolution de problèmes, Schéma d'annotation, Acte de dialogue</mots_cles>
			<title>State of the Art : analysis of problem-oriented online written conversations in terms of dialog acts</title>
			<abstract>The advent of Web 2.0 and the massive creation and consumption of user generated content it triggered allowed for new kinds of interactions among web users. Indeed, freely available archives of problem-oriented online conversations represent an invaluable resource, however it remains largely unexploited. Exploiting this resource would not only allow for the improvement of systems related to collaborative problem resolution, but also to reﬁne the customer support channels that web-based companies put at their users' disposal. However, in order to achieve this, it is ﬁrst necessary to deﬁne a formal framework for the ﬁne-grained analysis of online written conversations bearing requests for assistance. This paper presents an overview of the state of the art for discourse analysis in online written conversations, and discusses the applicability of the different methods for the analysis of conversations set within the scope of collaborative problem-solving.</abstract>
			<keywords>Discourse analysis, Conversation, Problem solving, Annotation scheme, Dialog act</keywords>
		</article>
	</articles>
</conference>
