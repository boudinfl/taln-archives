<?xml version="1.0" encoding="UTF-8"?>
<!-- Fichier construit à partir des programmes fournis par Pierre. -->
<conference>
	<edition>
		<acronyme>TALN'1998</acronyme>
		<titre>5ème conférence sur le Traitement Automatique des Langues Naturelles</titre>
		<ville>Paris</ville>
		<pays>France</pays>
		<dateDebut>1998-06-10</dateDebut>
		<dateFin>1998-06-12</dateFin>
		<presidents>
			<nom>Pierre Zweigenbaum</nom>
		</presidents>
		<typeArticles>
			<type id="invite">Conférences invitées</type>
			<type id="long">Communications Orales</type>
			<type id="poster">Affiches</type>
			<type id="démonstration">Démonstrations</type>
			<type id="projet">Projets</type>
		</typeArticles>
		<statistiques>
			<!-- <acceptations id="long" soumissions=""></acceptations>
			<acceptations id="affiche" soumissions=""></acceptations> -->
		</statistiques>
		<siteWeb>http://www.biomath.jussieu.fr/taln1998/</siteWeb>
		<meilleurArticle>
			<articleId></articleId>
		</meilleurArticle>
	</edition>
	<articles>
		<article id="taln-1998-invite-001" session="">
			<auteurs>
				<auteur>
					<nom>Évelyne Tzoukermann</nom>
					<email>evelyne@bell-labs.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Bell Labs Innovations / Lucent Technologies / États-Unis</affiliation>
			</affiliations>
			<titre>Recherches en TALN et applications industrielles</titre>
			<type>invite</type>
			<pages></pages>
			<resume>Après un bref aperçu des activités de TALN au sein des Laboratoires Bell, je présenterai certains de mes travaux et leurs applications. En particulier, je décrirai les travaux en phonologie, morphologie et syntaxe partielle, les principes théoriques sur lesquels ils sont basés, et les systèmes multiples dans lesquels ils sont utilisés, en particulier dans l'identification des parties du discours grâu;ce à un étiqueteur, dans un système de génération morpho-syntaxique appliqué à l'informatique documentaire, et également dans la phase d'analyse textuelle précédant la synthèse de la parole. En synthèse de la parole, la phase d'analyse textuelle est constituée de plusieurs étapes que je décrirai en détails : a) la conversion de graphèmes à phonèmes, comportant analyse morphologique, modèles de language, et règles de réécriture, b) la durée des phonèmes, et c) les contours prosodiques.</resume>
			<mots_cles></mots_cles>
			<title></title>
			<abstract></abstract>
			<keywords></keywords>
		</article>
		<article id="taln-1998-invite-002" session="">
			<auteurs>
				<auteur>
					<nom>Naomi Sager</nom>
					<email>sager@cs.nyu.edu</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">New York University</affiliation>
			</affiliations>
			<titre>Traitement automatique de la langue médicale (rencontre avec un(e) Collègue)</titre>
			<type>invite</type>
			<pages></pages>
			<resume>Cette session cherche à favoriser la discussion avec un(e) collègue, en consacrant une large part du temps aux questions et à la discussion avec la salle. Elle se veut plus informelle et plus interactive qu'une conférence invitée. Naomi Sager présentera ses travaux sur le traitement automatique de la langue médicale. Si les conditions techniques le permettent, une démonstration est également prévue.</resume>
			<mots_cles></mots_cles>
			<title></title>
			<abstract></abstract>
			<keywords></keywords>
		</article>
		<article id="taln-1998-long-001" session="">
			<auteurs>
				<auteur>
					<nom>Pascal Amsili</nom>
					<email>amsili@linguist.jussieu.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Nabil Hathout</nom>
					<email>hathout@inalf.cnrs.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">TALANA, UFRL Université Paris 7 2, pl. Jussieu, case 7003 F-75251 Paris Cedex 05</affiliation>
				<affiliation affiliationId="2">CNRS – INaLF Château du Montet Rue du Doyen Roubault F-54500 Vandoeuvre-lès-Nancy</affiliation>
			</affiliations>
			<titre>Systèmes de types pour la (lambda-)DRT ascendante</titre>
			<type>long</type>
			<pages></pages>
			<resume>Le terme de lambda-DRT désigne un ensemble de méthodes permettant de construire des représentations sémantiques (DRS) à partir d’arbres syntaxiques. La mise en oeuvre de telles méthodes nécessite l’élaboration de systèmes de types dont le détail est rarement présenté. C’est à la description d’un tel système que cet article est consacré.</resume>
			<mots_cles></mots_cles>
			<title></title>
			<abstract></abstract>
			<keywords></keywords>
		</article>
		<article id="taln-1998-long-002" session="">
			<auteurs>
				<auteur>
					<nom>Philippe Blache</nom>
					<email>pb@lpl.univ-aix.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LPL CNRS</affiliation>
			</affiliations>
			<titre>Une stratégie de contrôle pour l'analyse syntaxique</titre>
			<type>long</type>
			<pages></pages>
			<resume>Les techniques de pré-analyse ne permettent généralement pas de fournir des informations extrêmement précises. Or, un contrôle efficace de l'analyse syntaxique, en particulier pour traiter la question de l'ambiguïté, doit nécessairement faire appel à des contraintes suffisamment évoluées, par exemple pour contrôler efficacement le retardement d'évaluation. Nous proposons dans cet article une méthode construisant à partir d'une phrase des contraintes syntaxiques de haut niveau sans faire appel à la grammaire ni, par voie de conséquence, à des mécanismes d'analyse proprement dits. Elle est donc efficace et générale puisqu'indépendante des formalismes syntaxiques.</resume>
			<mots_cles></mots_cles>
			<title></title>
			<abstract></abstract>
			<keywords></keywords>
		</article>
		<article id="taln-1998-long-003" session="">
			<auteurs>
				<auteur>
					<nom>Marie-Hélène Candito</nom>
					<email>marie-helene.candito@linguist.jussieu.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Sylvain Kahane</nom>
					<email>sk@ccr.jussieu.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">TALANA, Université Paris 7, case 70032, place Jussieu 75251 Paris cedex 05</affiliation>
			</affiliations>
			<titre>Une grammaire TAG vue comme une grammaire Sens-Texte précompilée</titre>
			<type>long</type>
			<pages>102-111</pages>
			<resume>Dans cet article, nous comparons deux modèles linguistiques utilisés en TAL, les grammaires d'arbres adjoints [= TAG] et le Théorie Sens-Texte [= TST]. Nous montrons que ces deux modèles présentent des similitudes notables, et que les représentations les plus abstraites qu'ils donnent d'une phrase — la représentation sémantique en TST et l'arbre de dérivation en TAG — sont équivalentes. De ce rapprochement découle d'une part que l'on peut s'inspirer de la procédure de dérivation TAG pour opérer la correspondance Sens-Texte, et d'autre part que l'on peut concevoir une grammaire TAG comme le résultat de la précompilation d'une grammaire Sens-Texte.</resume>
			<mots_cles></mots_cles>
			<title></title>
			<abstract></abstract>
			<keywords></keywords>
		</article>
		<article id="taln-1998-long-004" session="">
			<auteurs>
				<auteur>
					<nom>Jean-Cédric Chappelier</nom>
					<email>chaps@lia.di.epfl.ch</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Martin Rajman</nom>
					<email>rajman@lia.di.epfl.ch</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">EPFL – DI-LIA, Écublens, CH-1015 Lausanne, Suisse</affiliation>
			</affiliations>
			<titre>Extraction stochastique d'arbres d'analyse pour le modèle DOP</titre>
			<type>long</type>
			<pages>52-61</pages>
			<resume>Dans le cadre des approches à base de grammaires faiblement sensibles au contexte, cette contribution passe en revue le problème de l'extraction de l'arbre d'analyse le plus probable dans le modèle du Data-Oriented Parsing (DOP). Une démonstration formelle de l'utilisabilité des méthodes de Monte-Carlo est donnée, puis une technique d'échantillonnage contrôlée est développée permettant de garantir (avec un certain seuil de confiance fixé a priori) que l'arbre d'analyse sélectionné est bien l'arbre d'analyse le plus probable au sens de DOP. plus probable au sens de DOP.</resume>
			<mots_cles></mots_cles>
			<title></title>
			<abstract></abstract>
			<keywords></keywords>
		</article>
		<article id="taln-1998-long-005" session="">
			<auteurs>
				<auteur>
					<nom>Patrice Lopez</nom>
					<email>Patrice.Lopez@loria.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1"></affiliation>
				<affiliation affiliationId="2"></affiliation>
				<affiliation affiliationId="3"></affiliation>
			</affiliations>
			<titre>Analyse guidée par connexité de TAG lexicalisées</titre>
			<type>long</type>
			<pages></pages>
			<resume>Cet article présente un nouveau type d'algorithme pour l'analyse de TAG lexicalisés, basé sur la prise en compte de la connexité d'ancres afin de faire tomber un maximum d'hypothèses au plus tôt lors de l'analyse. Cet algorithme fonctionne de manière bidirectionnelle, sans prévision et est destiné à être appliqué au langage naturel et à des grammaires de taille importante. En dépit d'une complexité théorique au pire des cas en O(NG 2 n 7 ), il permet en moyenne une efficacité intéressante par rapport aux algorithmes fonctionnant au pire en n 6 (avec ou sans prédiction) et une grande souplesse.</resume>
			<mots_cles></mots_cles>
			<title></title>
			<abstract>This paper presents a new sort of algorithm for parsing lexicalized TAG. It proceeds conside-ring connected anchors in order to eliminate hypotheses as soon as possible during the analysis. This algorithm proceeds without predictions in a bidirectional fashion. It has been designed to be applied to natural language and grammars of important size. In spite of a theoretical worst case time complexity in O(NG 2 n 7 ), it permits in practice an interesting efficiency on average case comparing to algorithms which work in n 6 complexity (with or without predictions) and an important flexibility.</abstract>
			<keywords></keywords>
		</article>
		<article id="taln-1998-long-006" session="">
			<auteurs>
				<auteur>
					<nom>Ruslan Mitkov</nom>
					<email>R.Mitkov@wlv.ac.uk</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Lamia Belguith</nom>
					<email>belguith.lamia@planet.tn</email>
					<affiliationId></affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1"></affiliation>
				<affiliation affiliationId="2"></affiliation>
				<affiliation affiliationId="3"></affiliation>
			</affiliations>
			<titre></titre>
			<type>long</type>
			<pages></pages>
			<resume></resume>
			<mots_cles></mots_cles>
			<title>Pronoun resolution made simple: a robust, knowledge-poor approach in action</title>
			<abstract>Most traditional approaches to anaphora resolution rely heavily on linguistic and domain knowledge. One of the disadvantages of developing a knowledge-based system, however, is that it is a very labour-intensive and time-consuming task. This paper presents a robust, knowledge-poor approach to resolving pronouns in technical manuals. This approach is a modification of the practical approach (reported in Mitkov 1998a) and operates on texts pre-processed by a part-of-speech tagger. Input is checked against agreement and tested for a number of antecedent indicators. Candidates are assigned scores by each indicator and the candidate with the highest aggregate score is returned as the antecedent. The approach can be regarded as multilingual 1 : the English version gave good results when used directly for Arabic and minimum modification produced an Arabic version with a still better perform-ance. Preliminary evaluation reports high success rates in the range of and over 90%.</abstract>
			<keywords></keywords>
		</article>
		<article id="taln-1998-long-007" session="">
			<auteurs>
				<auteur>
					<nom>Philippe Muller</nom>
					<email>muller@irit.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Laure Sarda</nom>
					<email>lsarda@cict.fr</email>
					<affiliationId></affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1"></affiliation>
				<affiliation affiliationId="2"></affiliation>
				<affiliation affiliationId="3"></affiliation>
			</affiliations>
			<titre>Représentation de la sémantique des verbes de déplacements transitifs du français</titre>
			<type>long</type>
			<pages></pages>
			<resume>Nous presentons ici un travail de semantique lexicale sur une classe de verbes de deplacements du français. Apres avoir etabli une classification de ces verbes, nous proposons une representation de leur semantique dans un modele logique de l'espace-temps de sens commun.</resume>
			<mots_cles></mots_cles>
			<title></title>
			<abstract></abstract>
			<keywords></keywords>
		</article>
		<article id="taln-1998-long-008" session="">
			<auteurs>
				<auteur>
					<nom>Alain Polguère</nom>
					<email>polguera@ERE.UMontreal.CA</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1"></affiliation>
				<affiliation affiliationId="2"></affiliation>
				<affiliation affiliationId="3"></affiliation>
			</affiliations>
			<titre>Une modélisation Sens-Texte de la lexicalisation en génération de texte</titre>
			<type>long</type>
			<pages></pages>
			<resume>Cet article propose un modèle général de la lexicalisation en génération de texte, basé sur une classification systématique des différents types de choix devant être opérés dans le processus de lexicalisation et structurant ces choix de façon logique. L'hypothèse sous-jacente à notre approche est que la modélisation de la lexicalisation doit être parfaitement intégrée à un système global de modélisation des connaissances linguistiques „ grammaticales et lexicales. En conséquence, un tel modèle doit être structuré selon les principes d'une théorie linguistique homogène. L'approche théorique adoptée ici est la théorie linguistique Sens-Texte. Dans une première section, nous allons brièvement présenter le problème de la lexicalisation et son importance en génération de texte. La Section 2 introduit plusieurs concepts propres à la lexicalisation, qui nous semblent nécessaires pour modéliser celle-ci dans un cadre Sens-Texte. La Section 3 décrit l'organisation procédurale de notre modèle. Finalement, la Conclusion présente l'utilisation que nous comptons faire de ce modèle dans le cadre d'un projet de générateur Sens-Texte en cours de réalisation.</resume>
			<mots_cles></mots_cles>
			<title></title>
			<abstract></abstract>
			<keywords></keywords>
		</article>
		<article id="taln-1998-long-009" session="">
			<auteurs>
				<auteur>
					<nom>David Roussel</nom>
					<email>roussel@thomson-lcr.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Didier Pernel</nom>
					<email>pernel@thomson-lcr.fr</email>
					<affiliationId></affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1"></affiliation>
				<affiliation affiliationId="2"></affiliation>
				<affiliation affiliationId="3"></affiliation>
			</affiliations>
			<titre>Intégration de prédictions linguistiques dans un système de reconnaissance de la parole : une expérience utilisant une grammaire d'arbres lexicalisée</titre>
			<type>long</type>
			<pages></pages>
			<resume>Nous présentons une expérience destinée à améliorer les performances d'un système de reconnaissance de la parole en prédisant les actes de dialogue et en réduisant l'espace de recherche. Le système est piloté par le biais de règles de réécritures pondérées, dérivées d'une grammaire de type LTAG (Lexicalized Tree Adjoining Grammar). La variante des LTAGs considérée rend possible la conversion de sous-ensembles de la grammaire lexicalisée, calculés pour chaque acte de dialogue, en sous-grammaires hors contexte. Leur probabilité est alors redéfinie au fur et à mesure du dialogue. La ou les sous-grammaires dont l'énoncé reconnu est issu indiquent en même temps le ou les actes de dialogue pertinents pour traiter l'énoncé.</resume>
			<mots_cles></mots_cles>
			<title></title>
			<abstract></abstract>
			<keywords></keywords>
		</article>
		<article id="taln-1998-long-010" session="">
			<auteurs>
				<auteur>
					<nom>Jacques Vergne</nom>
					<email>Jacques.Vergne@info.unicaen.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Emmanuel Giguet</nom>
					<email>Emmanuel.Giguet@info.unicaen.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">GREYC - CNRS UPRESA 6072 - Université de Caen F-14032 Caen cedex France</affiliation>
			</affiliations>
			<titre>Regards théoriques sur le ``tagging''</titre>
			<type>long</type>
			<pages></pages>
			<resume>Dans cet article, nous proposons de prendre du recul par rapport à l'aspect opératoire du tagging, et nous tentons de montrer que le tagging ouvre la voie au renouveau de l'analyse syntaxique en la fondant sur l'explicitation des processus: processus de déduction locale dans les syntagmes non récursifs, et processus de mise en relation des syntagmes non récursifs, étendant ainsi à l'analyse syntaxique les propriétés calculatoires du tagging.</resume>
			<mots_cles></mots_cles>
			<title></title>
			<abstract></abstract>
			<keywords></keywords>
		</article>
		<article id="taln-1998-long-011" session="">
			<auteurs>
				<auteur>
					<nom>Jawad Berri</nom>
					<email>berri@ifi.unizh.ch</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Diego Mollá Aliod</nom>
					<email>molla@ifi.unizh.ch</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Michael Hess</nom>
					<email>hess@ifi.unizh.ch</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Département d'Informatique (IFI), Équipe de Linguistique Informatique Université de Zurich Winterthurerstrasse 190, CH-8057 Zurich Suisse</affiliation>
			</affiliations>
			<titre>Extraction automatique de réponses : implémentation du système ExtrAns</titre>
			<type>long</type>
			<pages></pages>
			<resume>Nous décrivons dans cet article un système d'extraction automatique de réponses. L'extraction automatique de réponses (EAR) a pour but de trouver les passages d'un document qui répondent directement à une question posée par un utilisateur. L'EAR est plus ambitieuse que la recherche d'informations et l'extraction d'informations dans le sens que les résultats de la recherche sont des phrases et non pas des documents en entier, et dans le sens que les questions peuvent être formulées de façon libre. Elle est par contre moins ambitieuse que les systèmes questions-réponses car les réponses ne sont pas générées à partir d'une base de connaissance, mais repérées dans les textes des documents. La version actuelle d'ExtrAns permet d'analyser la documentation en ligne (en Anglais) du système Unix (les "man pages"), et de construire une représentation sémantique (sous forme d'expressions logiques) des phrases. Un programme de démonstration de théorèmes trouve ensuite les passages pertinents qui sont mis en évidence dans leur contexte.</resume>
			<mots_cles></mots_cles>
			<title></title>
			<abstract></abstract>
			<keywords></keywords>
		</article>
		<article id="taln-1998-long-012" session="">
			<auteurs>
				<auteur>
					<nom>Narjes Boufaden</nom>
					<email>boufaden@ift.ulaval.ca</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Sylvain Delisle</nom>
					<email>Sylvain_Delisle@uqtr.uquebec.ca</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Bernard Moulin</nom>
					<email>moulin@ift.ulaval.ca</email>
					<affiliationId>1</affiliationId>
					<affiliationId>3</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Département d’informatique, Université Laval, Québec, Canada</affiliation>
				<affiliation affiliationId="2">Département de mathématiques et d’informatique, Université du Québec à Trois-Rivières, Québec, Canada</affiliation>
				<affiliation affiliationId="3">Centre de recherche en géomatique de l’Université Laval</affiliation>
			</affiliations>
			<titre>Analyse syntaxique robuste des dialogues retranscrits : peut-on vraiment traiter l'oral à partir de l'écrit ?</titre>
			<type>long</type>
			<pages></pages>
			<resume>L'analyseur syntaxique robuste que nous décrivons dans cet article s'insère dans le cadre des travaux relatifs au traitement du langage oral. Nous montrons à partir d'une étude menée sur des dialogues transcrits qu'il est avantageux de traiter le langage oral avec un analyseur syntaxique robuste faisant appel à un analyseur syntaxique conçu pour l'écrit. Pour ce faire, nous avons réalisé un système qui se base sur une architecture à deux couches : le noyau et la périphérie, l'une interagissant avec l'autre via un superviseur. La première couche, le noyau, est dédiée à l'analyse des constituants d'énoncés respectant la grammaire standard : c'est un analyseur syntaxique de l'écrit. La deuxième couche, la périphérie, se charge de “corriger” les constituants de l'énoncé oral ayant subis des distorsions. Cette couche intervient lorsque le noyau ne parvient plus à progresser dans son analyse. L'ordre d'intervention de ces deux couches dans le processus d'analyse est déterminé par l'occurrence de marques de surface qui signalent la présence d'une distorsion ou d'une construction particulière (interrogative, relative, etc.). Le système ainsi conçu nous a permis de traiter les bruits et différents types de répétitions caractéristiques de l'oral. La première version de l'analyseur nous a fourni des résultats encourageants qui nous permettent de confirmer l'interdépendance entre le traitement du langage oral et le traitement du langage écrit. Ces résultats nous invitent aussi à reconsidérer le rôle et l'utilité, pour le traitement de l'oral, des ressources d'informatique linguistique développées pour l'écrit.</resume>
			<mots_cles></mots_cles>
			<title></title>
			<abstract></abstract>
			<keywords></keywords>
		</article>
		<article id="taln-1998-long-013" session="">
			<auteurs>
				<auteur>
					<nom>Jean Caelen</nom>
					<email>Jean.Caelen@imag.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Ludovic Imberdis</nom>
					<email>Ludovic.Imberdis@imag.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Laboratoire CLIPS - IMAG Domaine universitaire, BP 53 38041 Grenoble cedex 9</affiliation>
			</affiliations>
			<titre>Génération d'actes illocutoires pour le dialogue</titre>
			<type>long</type>
			<pages></pages>
			<resume>Les sorties parlées d'un système de dialogue homme-machine sont souvent d'une qualité médiocre pour deux raisons : le texte généré par le générateur ne tient souvent pas compte de l'usager et la synthèse de la parole à partir de ce texte, donne souvent une prosodie de lecture. L'article décrit les principes d'introduction d'une composante pragmatique en amont du système de génération afin de tenir compte des degrés de force et des buts illocutoires. En effet, les études en génération portent généralement sur les réalisations linguistiques du contenu propositionnel de la réponse (c'est-à-dire le sujet du message) mais très rarement sur la constitution de l'acte énonciatif qui est généralement abandonnée au moteur de dialogue de l'application. Nous développons cet aspect au cœur même de la génération afin de fournir un outil performant et surtout pour prendre en compte les aspects pragmatiques de manière précoce dans le dialogue. La conclusion est que notre système, grâce à son module de traitement pragmatique, a la capacité de générer des énoncés plus naturels. La génération est basée non plus sur le contenu informatif seul, mais aussi sur une situation pragmatique, exprimée dans un cadre bien défini. Notre approche montre l'intérêt de prendre en compte les paramètres illocutoires dans le cadre du dialogue.</resume>
			<mots_cles></mots_cles>
			<title></title>
			<abstract></abstract>
			<keywords></keywords>
		</article>
		<article id="taln-1998-long-014" session="">
			<auteurs>
				<auteur>
					<nom>Hervé Déjean</nom>
					<email>dejean@info.unicaen.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1"></affiliation>
				<affiliation affiliationId="2"></affiliation>
				<affiliation affiliationId="3"></affiliation>
			</affiliations>
			<titre>Inférence automatique de contextes distributionnels</titre>
			<type>long</type>
			<pages></pages>
			<resume>Nous allons présenter, dans cet article une méthodologie permettant d'une part la définition des catégories distributionnelles des mots d'une langue et d'autre part un algorithme réalisant cette catégorisation. Ce travail ne se base que sur des critères formels. Aucune connaissance lexicale ou sémantique n'est requise. Ces critères formels ayant l'avantage d'être applicables à un grand nombre de langues, la méthode est donc multilingue. Le critère de catégorisation repose sur deux notions structurelles : le syntagme simple et la proposition. La découverte de ces deux structures permet la mise à jour de classes particulières qui vont correspondre à nos catégories distributionnelles.</resume>
			<mots_cles></mots_cles>
			<title></title>
			<abstract></abstract>
			<keywords></keywords>
		</article>
		<article id="taln-1998-long-015" session="">
			<auteurs>
				<auteur>
					<nom>Thierry Etchegoyhen</nom>
					<email>etchegoyhen@latl.unige.ch</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Eric Wehrli</nom>
					<email>wehrli@latl.unige.ch</email>
					<affiliationId></affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1"></affiliation>
				<affiliation affiliationId="2"></affiliation>
				<affiliation affiliationId="3"></affiliation>
			</affiliations>
			<titre>Traduction automatique et structures d'interface</titre>
			<type>long</type>
			<pages></pages>
			<resume>Le choix d'un niveau de représentation adéquat pour l'expression des correspondances entre énoncés de langues différentes est au coeur de la problématique en traduction automatique. Dans cet article, nous présentons un système de traduction automatique basé sur un concept hybride d'interlangue (pour les structures) et de transfert (pour les items lexicaux). Le recours à des structures d'interface suffisamment abstraites permet de faire l'économie du transfert structural. Ces structures spécifient toutefois les unités lexicales des langues particulières, évitant ainsi le problème classique que pose la décomposition lexicale. Une démonstration du système est envisagée.</resume>
			<mots_cles></mots_cles>
			<title></title>
			<abstract></abstract>
			<keywords></keywords>
		</article>
		<article id="taln-1998-long-016" session="">
			<auteurs>
				<auteur>
					<nom>Olivier Ferret</nom>
					<email>ferret@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1"></affiliation>
				<affiliation affiliationId="2"></affiliation>
				<affiliation affiliationId="3"></affiliation>
			</affiliations>
			<titre>Une segmentation thématique fondée sur la cohésion lexicale</titre>
			<type>long</type>
			<pages></pages>
			<resume>L'analyse thématique s'avère très utile, voire indispensable, dans nombre d'applications en traitement automatique des langues. Des méthodes permettent déjà de segmenter sur une large échelle des textes en blocs thématiquement cohérents. Elles s'appuient pour l'essentiel sur la distribution des mots. Certaines tentent également d'exploiter la notion de cohésion lexicale. Cette utilisation présuppose toutefois l'existence d'une source de connaissances rendant compte de cette cohésion entre mots. Nous proposons de constituer celle-ci de manière automatique au travers d'un réseau de cooccurrences lexicales élaboré à partir d'un vaste ensemble de textes. Nous proposons également une méthode spécifique capable d'exploiter ce réseau afin de segmenter des textes. Cette méthode repose sur l'évaluation de la cohésion thématique en tout point d'un texte au moyen d'une fenêtre glissante. Cette cohésion est calculée directement à partir de la cohésion mutuelle des mots présents dans la fenêtre. La segmentation proprement dite est réalisée automatiquement par une analyse de cette courbe de cohésion. On obtient ainsi un ensemble de segments dotés chacun d'une évaluation de sa cohésion interne. Une évaluation de cette méthode est présentée pour une tâche de segmentation de textes contigus ainsi que de façon plus indirecte pour une tâche d'extraction de domaines sémantiques.</resume>
			<mots_cles></mots_cles>
			<title></title>
			<abstract></abstract>
			<keywords></keywords>
		</article>
		<article id="taln-1998-long-017" session="">
			<auteurs>
				<auteur>
					<nom>Serge Fleury</nom>
					<email>fleury@ens-fcl.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">UMR 9952 - CNRS, Ecole Normale Supérieure de Fontenay-St Cloud</affiliation>
			</affiliations>
			<titre>Gaspar, un dispositif de TALN basé sur la programmation à prototypes</titre>
			<type>long</type>
			<pages></pages>
			<resume>Nous présentons ici le dispositif GASPAR qui construit des représentations des mots sous la forme d'objets informatiques appelés des prototypes ; GASPAR associe à ces objets les comportements syntaxiques et sémantiques des mots en prenant appui sur des informations extraites à partir d'un corpus. GASPAR a pour première tâche de construire progressivement une représentation informatique des mots, sans présumer de leurs descriptions linguistiques ; il doit ensuite reclasser les mots représentés et mettre au jour, de manière inductive, les classes de mots du sous-langage étudié. Nous montrons comment la programmation à prototypes permet de représenter des mots dynamiquement par apprentissage et par affinements successifs. Elle permet ensuite d'amorcer un début de classement de ces mots sur la base de leurs contraintes syntaxico-sémantiques en construisant des hiérarchies locales de comportements partagés.</resume>
			<mots_cles></mots_cles>
			<title></title>
			<abstract></abstract>
			<keywords></keywords>
		</article>
		<article id="taln-1998-long-018" session="">
			<auteurs>
				<auteur>
					<nom>Lidia Fraczak</nom>
					<email>lidia@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Guy Lapalme</nom>
					<email>lapalme@iro.umontreal.ca</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Michael Zock</nom>
					<email>zock@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIMSI-CNRS, BP 133, F-91403 Orsay Cedex, France</affiliation>
				<affiliation affiliationId="2">DIRO, Université de Montréal, CP 6128, Succ. Centre-Ville, Montréal Québec H3C 3J7 Canada</affiliation>
			</affiliations>
			<titre>Variation du contenu et de la forme dans la génération de descriptions d'itinéraires en métro</titre>
			<type>long</type>
			<pages></pages>
			<resume>Une des tâches humaines les plus courantes est la navigation dans l'environnement. Pour se rendre d'un endroit à un autre, les gens ont souvent recours à des descriptions d'itinéraires. Dans cet article, nous présentons notre générateur de descriptions d'itinéraires en métro, fondé sur un modèle cognitif de la production de descriptions d'itinéraires et sur une analyse de corpus. Nous montrons, en particulier, comment les facteurs pragmatiques d'importance informationnelle et les choix stylistiques font varier les descriptions d'un même itinéraire par rapport à leur contenu conceptuel et par rapport à leur forme textuelle.</resume>
			<mots_cles></mots_cles>
			<title></title>
			<abstract></abstract>
			<keywords></keywords>
		</article>
		<article id="taln-1998-long-019" session="">
			<auteurs>
				<auteur>
					<nom>Brigitte Le Pévédic</nom>
					<email>Brigitte.Lepevedic@irin.univ-nantes.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1"></affiliation>
				<affiliation affiliationId="2"></affiliation>
				<affiliation affiliationId="3"></affiliation>
			</affiliations>
			<titre>Le niveau syntaxique dans le système HandiAS</titre>
			<type>long</type>
			<pages></pages>
			<resume>Cet article décrit une partie d'unsystème adaptatif d'aide à la composition écrite de phrases. Le système HandiAS est dédié aux personnes handicapées physiques afin de faciliter leur communication en économisant la saisie des dernières lettres de chaque mot de leur énoncé. Ce système se base sur une méthode de prédiction morphosyntaxique évolutive, à partir du contexte gauche, utilisant une technique hybride symbolique et statistique.</resume>
			<mots_cles></mots_cles>
			<title></title>
			<abstract></abstract>
			<keywords></keywords>
		</article>
		<article id="taln-1998-long-020" session="">
			<auteurs>
				<auteur>
					<nom>Emmanuel Morin</nom>
					<email>Emmanuel.Morin@irin.univ-nantes.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Institut de Recherche en Informatique de Nantes (IRIN) 2 rue de la houssinière, BP 92208 44322 Nantes Cedex 3, FRANCE</affiliation>
			</affiliations>
			<titre>Prométhée : un outil d'aide à l'acquisition de relations sémantiques entre termes</titre>
			<type>long</type>
			<pages></pages>
			<resume>Les outils d'aide à la construction de terminologie à partir de corpus ont connu un essor important ces dernières années. D'un autre côté, les outils d'aide à l'acquisition de relations sémantiques entre termes sont peu nombreux. Face à ce problème, nous avons développé le système Prométhée qui acquière incrémentalement un ensemble de patrons lexico-syntaxiques caractéristiques d'une relation sémantique. Ainsi, pour la relation d'hyponymie, Prométhée a extrait un ensemble de patrons qui servent à améliorer la couverture d'un thesaurus ou d'une base de connaissances.</resume>
			<mots_cles></mots_cles>
			<title></title>
			<abstract></abstract>
			<keywords></keywords>
		</article>
		<article id="taln-1998-long-021" session="">
			<auteurs>
				<auteur>
					<nom>Yannick Toussaint</nom>
					<email>Yannick.Toussaint@loria.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Fiammetta Namer</nom>
					<email></email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Béatrice Daille</nom>
					<email></email>
					<affiliationId>3</affiliationId>
				</auteur>
				<auteur>
					<nom>Christian Jacquemin</nom>
					<email></email>
					<affiliationId>4</affiliationId>
				</auteur>
				<auteur>
					<nom>Jean Royauté</nom>
					<email></email>
					<affiliationId>5</affiliationId>
				</auteur>
				<auteur>
					<nom>Nabil Hathout</nom>
					<email></email>
					<affiliationId>6</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LORIA-INRIA UMR 7503, Nancy</affiliation>
				<affiliation affiliationId="2">Université de Nancy II</affiliation>
				<affiliation affiliationId="3">IRIN, Nantes</affiliation>
				<affiliation affiliationId="4">LIMSI, Orsay</affiliation>
				<affiliation affiliationId="5">INIST-CNRS, Nancy</affiliation>
				<affiliation affiliationId="6">INaLF-CNRS, Nancy</affiliation>
			</affiliations>
			<titre>Une approche linguistique et statistique pour l'analyse de l'information en corpus</titre>
			<type>long</type>
			<pages></pages>
			<resume>Cet article présente une chaîne de traitement automatique réalisée dans le cadre du projet ILIAD (Informatique Linguistique et Infométrie pour l'Analyse de grands fonds Docu-mentaires) du GIS Sciences de la Cognition. Cette chaˆine est dédiée à l'analyse de l'infor-mation à partir de corpus de textes de très grand volume, en franc¸ais. Elle est expérimentée sur un corpus de 2,5 Mb et a conduit à la création de 50 classes de termes. Ces classes sont construites sur la base de la cooccurrence des termes et représentent des connaissances du do-maine. Les différentes étapes de la chaˆine associent des méthodes linguistiques informatiques et des méthodes statistiques : pré-traitement des textes, étiquetage, morphologie, terminologie et analyse des documents. Pour chacune d'entre elles, nous présentons les méthodes, les outils ainsi que leur evaluation.</resume>
			<mots_cles></mots_cles>
			<title></title>
			<abstract></abstract>
			<keywords></keywords>
		</article>
		<article id="taln-1998-poster-001" session="">
			<auteurs>
				<auteur>
					<nom>Pierre-André Buvet</nom>
					<email>pab@lli-hp.univ-paris13.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Xavier Blanco</nom>
					<email></email>
					<affiliationId></affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1"></affiliation>
				<affiliation affiliationId="2"></affiliation>
				<affiliation affiliationId="3"></affiliation>
			</affiliations>
			<titre>Perspectives pour la traduction automatique des déterminants</titre>
			<type>poster</type>
			<pages></pages>
			<resume>Nous exposons ici la possibilité de traduire automatiquement, entre deux langues proches, des contraintes sur leurs déterminants en faisant appel, d'une part, à la notion de classe d'objets et, d'autre part, à des outils informatiques, les automates finis et les transducteurs. Nous traitons, en partie, les cas de la détermination des noms de maladies et de sports en français et en espagnol.</resume>
			<mots_cles></mots_cles>
			<title></title>
			<abstract></abstract>
			<keywords></keywords>
		</article>
		<article id="taln-1998-poster-002" session="">
			<auteurs>
				<auteur>
					<nom>Bilel Gargouri</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Mohamed Jmaiel</nom>
					<email></email>
					<affiliationId></affiliationId>
				</auteur>
				<auteur>
					<nom>Abdelmajid Ben Hamadou</nom>
					<email>Abdelmajid.Benhamadou@fsegs.rnu.tn</email>
					<affiliationId></affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1"></affiliation>
				<affiliation affiliationId="2"></affiliation>
				<affiliation affiliationId="3"></affiliation>
			</affiliations>
			<titre>Intérêts des méthodes formelles en génie linguistique</titre>
			<type>poster</type>
			<pages></pages>
			<resume></resume>
			<mots_cles></mots_cles>
			<title>Lingware Engineering and formal Methods Needs</title>
			<abstract>This paper investigates the needs of formal methods for specifying and developing applications related to the Natural Language Processing (NLP) domain. First, we show the main advantages and facilities provided by formal methods in the development of general software. Then, we illustrate the principal results obtained from an experiment carried out a real application. Thereafter, we deduce the contributions of formal methods in the context of natural language processing. Finally, we propose methodological criteria allowing the choice of the appropriate formal method.</abstract>
			<keywords></keywords>
		</article>
		<article id="taln-1998-poster-003" session="">
			<auteurs>
				<auteur>
					<nom>Fabrice Issac</nom>
					<email>fabrice.issac@lipn.univ-paris13.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1"></affiliation>
				<affiliation affiliationId="2"></affiliation>
				<affiliation affiliationId="3"></affiliation>
			</affiliations>
			<titre>Une famille d'algorithmes de désambiguïsation pour des formalismes lexicalisés</titre>
			<type>poster</type>
			<pages></pages>
			<resume>Nous montrons dans cet article que la désambiguïsation peut etre vue, dans le cadre des formalismes lexicalisés, comme un problème de flux de données dans un graphe orienté. Nous d´efinissons une famille d'algorithmes de désambiguïsation, l'algorithme le plus efficace n'étant autre qu'un reconnaisseur. Nous présentons ici, en utilisant le formalisme des grammaires d'arbres adjoints lexicalisés (LTAG), deux algorithmes simples de complexité O(G 2 )etO(G 3 =n) (où G est la taille de la grammaire et n le nombre de mots de la phrase). Finalement nous pré-sentons un reconnaisseur de type montant dont la complexité estenO(G 2 n 6 ).</resume>
			<mots_cles></mots_cles>
			<title></title>
			<abstract></abstract>
			<keywords></keywords>
		</article>
		<article id="taln-1998-poster-004" session="">
			<auteurs>
				<auteur>
					<nom>Daniel Kayser</nom>
					<email>Daniel.Kayser@lipn.univ-paris13.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1"></affiliation>
				<affiliation affiliationId="2"></affiliation>
				<affiliation affiliationId="3"></affiliation>
			</affiliations>
			<titre>Extraction sémantique sur un corpus d'annonces légales</titre>
			<type>poster</type>
			<pages></pages>
			<resume>Nous décrivons un procédé visant à extraire de textes simples une structure sémantique facile à interroger. Le corpus est constitué d'objets sociaux déclarés dans les avis de constitution de sociétés. La méthode consiste à accompagner la convergence des textes du corpus vers un "axiome" au moyen de règles ad-hoc, par des actions de construction de structures de traits. Le fractionnement en étapes successives garantit l'efficacité du traitement, mais empêche la prise en compte des influences réciproques du texte et du co-texte. Le procédé n'est donc applicable que dans des limites assez étroites. L'article présente et discute les résultats obtenus.</resume>
			<mots_cles></mots_cles>
			<title></title>
			<abstract></abstract>
			<keywords></keywords>
		</article>
		<article id="taln-1998-poster-005" session="">
			<auteurs>
				<auteur>
					<nom>Mathieu Mangeot-Lerebours</nom>
					<email>Mathieu.Mangeot@imag.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">GETA-CLIPS - IMAG Campus BP 53 38041 Grenoble cedex 9</affiliation>
			</affiliations>
			<titre>Conception, implementation et indexation de BaLeM, une base lexicale multilingue</titre>
			<type>poster</type>
			<pages></pages>
			<resume>Dans le domaine de l'ingénierie linguistique et de la connaissance, le problème des ressources lexicales et linguistiques s'est toujours posé. Néanmoins, l'avancée des techniques du Traitement Automatique des Langues Naturelles (TALN) l'a rendu plus sensible. Il nous faut maintenant pouvoir répondre à des besoins importants en terme de quantité, de qualité et de complexité. La complexité et la diversité des informations requises augmente avec les exigences des outils de TALN ainsi qu'avec le développement de nouvelles applications (humaines ou machinales). Si la récupération (semi)automatique d'information lexicale est une piste, elle ne pourra remplacer la création manuelle de dictionnaires. Nous nous sommes donc intéressés à la construction d'outils pour lexicographes et lexicologues. pour répondre aux besoins de nos systèmes de traduction et à la demande du projet Universal Networking Language (UNL), nous avons décidé d'informatiser la construction d'une base lexicale multilingue. Dans ce but, nous avons fusionné des dictionnaires existants. A partir de ces données, nous générons automatiquement des fichiers qui sont envoyés aux lexicographes. Ceux-ci complètent et corrigent les données sur leur plate-forme avec des outils très simples. Les fichiers sont ensuite réintégrés dans la base lexicale. La dernière étape est la génération de dictionnaires nécessaires à nos systèmes de traduction.</resume>
			<mots_cles></mots_cles>
			<title></title>
			<abstract></abstract>
			<keywords></keywords>
		</article>
		<article id="taln-1998-poster-006" session="">
			<auteurs>
				<auteur>
					<nom>Yann Mathet</nom>
					<email>mathet@info.unicaen.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">GREYC Université de Caen Esplanade de la Paix 14032 Caen cedex</affiliation>
			</affiliations>
			<titre>Un formalisme pour le déplacement</titre>
			<type>poster</type>
			<pages></pages>
			<resume>Le traitement de la sémantique de la spatialité nécessite de manipuler des entités qu'il n'est pas aisé de définir. On parle souvent de lieux, mais on les assimile trop à un sens hors contexte des mots désignant des objets ; on évoque les notions de routine et de trajectoire, mais elles ne sont pas réellement intégrées au calcul du sens. Nous discutons dans cet article de la façon d'intégrer ces référents spatiaux à un traitement automatique, et nous focalisons en particulier sur la richesse que recèlent les chemins et les trajectoires dans l'expression du déplacement lorsqu'un formalisme leur donne existence.</resume>
			<mots_cles></mots_cles>
			<title></title>
			<abstract></abstract>
			<keywords></keywords>
		</article>
		<article id="taln-1998-poster-007" session="">
			<auteurs>
				<auteur>
					<nom>Michael Zock</nom>
					<email>zock@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1"></affiliation>
				<affiliation affiliationId="2"></affiliation>
				<affiliation affiliationId="3"></affiliation>
			</affiliations>
			<titre>La rédaction: conflit entre les idées et leur organisation</titre>
			<type>poster</type>
			<pages></pages>
			<resume>De nombreux générateurs de texte ont vu le jour depuis le milieu des années 80. La question est de savoir ce qu'ils apportent, et dans quelle mesure ils simulent certains aspects fondamentaux de la rédaction, comme celui d'interaction entre la génération de données (idées, messages) et leur structuration (induction du plan). Nous nous intéressons ici surtout à ce dernier aspect, en analysant LE système de référence en la matière, le système TEXT (Mc Keown, 1982, 1985). Plusieurs hypothèses sont à la base de notre réflexion: (i) -l'information à transmettre n'est pas structurée et représentée de la même manière dans la mémoire et dans le texte. Au niveau de la mémoire elle existe de façon brute; (ii) il y a généralement des interactions entre les données (messages, idées) et leur organisation (plan), l'un pouvant influer sur l'autre (haut-bas vs bas-haut). Autrement dit, il n'y a pas de priorité d'ordre; (iii) la rédaction implique des actes de réflexion, c'est-à-dire, des raisonne-ments: on ne peut vraiment écrire sans "raisonner". De toutes les opérations de raisonnement nous ne traiterons qu'une seule: l'inclusion. Celle-ci étant un principe fondamental d'organisation de données.</resume>
			<mots_cles></mots_cles>
			<title></title>
			<abstract></abstract>
			<keywords></keywords>
		</article>
		<article id="taln-1998-demo-001" session="">
			<auteurs>
				<auteur>
					<nom>Philippe Boula de Mareüil</nom>
					<email>mareuil@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1"></affiliation>
				<affiliation affiliationId="2"></affiliation>
				<affiliation affiliationId="3"></affiliation>
			</affiliations>
			<titre>Traitements linguistiques pour la synthèse de la parole à partir du texte</titre>
			<type>démonstration</type>
			<pages></pages>
			<resume>Cet article est consacré aux traitements linguistiques qui ont été implémentés dans le système de synthèse de la parole à partir du texte du LIMSI-CNRS. Ils se divisent en deux volets : conversion graphème-phonème et analyse syntaxique, essentiellement pour la génération automatique de la prosodie. Nous nous sommes efforcé de procéder par intension : conversion graphème-phonème par règles plutôt qu'à base de lexique d'exceptions (un critère de règle plus générale a été mis en évidence), et étiquetage en parties du discours non lexicaliste. L'approche structurelle a également été préférée aux modèles probabilistes. Une grammaire en tronçons a été développée, qui segmente la phrase en séquences non récursives. Celles-ci, définies comme des ensembles de catégories possibles, permettent de définir des frontières prosodiques potentielles (mineures, majeures ou majeures intermédiaires). Ces traitements linguistiques, en cours de commercialisation, ont fait l'objet de nombreux tests. Le module de transcription graphème-phonème, notamment, a obtenu le score le plus élevé des 8 convertisseurs ayant participé à une campagne d'évaluation de l'AUPELF-UREF.</resume>
			<mots_cles></mots_cles>
			<title></title>
			<abstract></abstract>
			<keywords></keywords>
		</article>
		<article id="taln-1998-demo-002" session="">
			<auteurs>
				<auteur>
					<nom>Sylviane Cardey</nom>
					<email>Sylviane.Cardey@univ-fcomte.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Zahra El Harouchy</nom>
					<email></email>
					<affiliationId></affiliationId>
				</auteur>
				<auteur>
					<nom>Peter Greenfield</nom>
					<email></email>
					<affiliationId></affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1"></affiliation>
				<affiliation affiliationId="2"></affiliation>
				<affiliation affiliationId="3"></affiliation>
			</affiliations>
			<titre>Dictionnaires en intention pour le Traitement Automatique des Langues</titre>
			<type>démonstration</type>
			<pages></pages>
			<resume></resume>
			<mots_cles></mots_cles>
			<title></title>
			<abstract></abstract>
			<keywords></keywords>
		</article>
		<article id="taln-1998-demo-003" session="">
			<auteurs>
				<auteur>
					<nom>David Faure</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Claire Nédellec</nom>
					<email>Claire.Nedellec@lri.fr</email>
					<affiliationId></affiliationId>
				</auteur>
				<auteur>
					<nom>Céline Rouveirol</nom>
					<email></email>
					<affiliationId></affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1"></affiliation>
				<affiliation affiliationId="2"></affiliation>
				<affiliation affiliationId="3"></affiliation>
			</affiliations>
			<titre>Apprentissage de cadres de sous-catégorisation et de restriction de sélection à partir de textes</titre>
			<type>démonstration</type>
			<pages></pages>
			<resume>Nous décrivons dans cet article le système d'apprentissage ASIUM qui permet d'acquérir des schémas actantiels de verbes et des ontologies à partir de l'analyse syntaxique de textes techniques. ASIUM est basé sur une méthode de classification conceptuelle originale qui exploite les régularités syntaxiques (les occurrences des mots de tête des compléments apparaissant avec le même verbe et la même préposition) pour apprendre les concepts de l'ontologie à partir d'un corpus. Les schémas actantiels de verbes sont appris en parallèle, de telle sorte que chaque nouveau concept appris permet de remplir un trait sémantique d'un schéma de verbe. Les premières expérimentations faites sur des textes de recettes de cuisine ont donné des résultats très prometteurs.</resume>
			<mots_cles></mots_cles>
			<title></title>
			<abstract></abstract>
			<keywords></keywords>
		</article>
		<article id="taln-1998-demo-004" session="">
			<auteurs>
				<auteur>
					<nom>Andrei Popescu-Belis</nom>
					<email>popescu@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Isabelle Robba</nom>
					<email>robba@limsi.fr</email>
					<affiliationId></affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1"></affiliation>
				<affiliation affiliationId="2"></affiliation>
				<affiliation affiliationId="3"></affiliation>
			</affiliations>
			<titre>Un atelier de traitement de la référence</titre>
			<type>démonstration</type>
			<pages></pages>
			<resume>Un ensemble de modules pour le traitement de la référence dans des récits a été réalisé. Le module principal, le résolveur de références, a pour tâche de grouper en un même ensemble, appelé " représentation mentale " toutes les expressions référentielles du récit qui se rapportent à un même référent. Les performances du résolveur ont été examinées sur deux textes de 2600 et 28600 mots ce qui représente 714 et 3812 expressions référentielles.</resume>
			<mots_cles></mots_cles>
			<title></title>
			<abstract></abstract>
			<keywords></keywords>
		</article>
		<article id="taln-1998-demo-005" session="">
			<auteurs>
				<auteur>
					<nom>Yves Simon</nom>
					<email>Yves.Simon@iut-nantes.univ-nantes.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Jean-François Hüe</nom>
					<email></email>
					<affiliationId></affiliationId>
				</auteur>
				<auteur>
					<nom>Chantal Enguehard</nom>
					<email></email>
					<affiliationId></affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1"></affiliation>
				<affiliation affiliationId="2"></affiliation>
				<affiliation affiliationId="3"></affiliation>
			</affiliations>
			<titre>Les métaphores : les générer pour les connaître</titre>
			<type>démonstration</type>
			<pages></pages>
			<resume>L'objet privilégié des études sur la métaphore et la métonymie est la recherche d'une représentation satisfaisante du sens dans les phrases où apparaissent ces deux figures. En particulier les recherches sur les métaphores portent essentiellement sur la nature de la relation entre la "source" et la "cible" d'une métaphore, de manière à pouvoir en proposer une interprétation en contexte, sans avoir recours à un enregistrement préalable dans le lexique de tous les sens d'un terme. D'après la classification proposée par D. Fass [91], les approches habituelles utilisées en reconnaissance de métaphores mettent respectivement l'accent sur les aspects analogiques, nouveaux et anormaux entre la source et la cible, sans traiter cependant les problèmes de détection des concepts métaphoriques mis en jeu. J. H. Martin [91] propose une méthode fondée sur celle de G. Lakoff [80] dans laquelle le sens métaphorique "conventionnel" y est systématiquement opposé à la notion de sens "littéral", lequel est reproduit à la suite d'un traitement sémantique où un concept est associé à une entrée lexicale. Martin acquiert de "nouvelles" métaphores en prolongeant, et en combinant des métaphores déjà connues. Il remplace alors une profusion de sens pour un terme du lexique par une profusion de concepts métaphoriques. Dans ces conditions, la solution retenue donne une couverture lexicale hétérogène. Nous proposons ici de montrer un mécanisme permettant de pallier l'hétérogénéité de la couverture lexicale de différents champs sémantiques. Ainsi, par exemple, dans le domaine de l'acquisition de connaissances concernant le concept "livre", les différentes formes de cette acquisition, et notamment les indicateurs du degré d'appétence, n'ont que peu de formes dédiées. Pour pallier ce manque, le recours à des constructions métaphoriques faisant usage du vocabulaire issu d'autres domaines pourra être utilisé avec profit. L'usage des éléments du lexique dédiés au domaine de l'alimentation donnera des expressions telles que : "c'est un livre très indigeste", "ce livre est un vrai régal". De la même manière, des domaines d'activités sont apparus sans qu'une terminologie suffisante n'ait accompagné leur développement. Le domaine de la bourse en est un bon exemple, dans lequel les diverses activités et éléments de description font usage de terminologies issues de divers domaines que l'on reconnaîtra dans des expressions comme : "la bourse plonge, coule", "le cours de la bourse explose". La métaphore réalise ainsi un transfert lexical d'un domaine à un autre en s'appuyant sur la structuration lexicale d'un domaine (fondée sur des relations sémantiques entre éléments du lexique) pour plaquer sur l'autre des éléments du lexique du premier, absents dans le second. Les relations permettent alors de fonder et par-là même d'attribuer, une nouvelle signification au terme transféré. Si ce mécanisme et la correspondance mise en jeu sont implicites dans un énoncé tel que "le livre rassasie l'esprit" dans lequel c'est le rôle (ou le cas) occupé par l'élément (ici "livre" qui occupe le cas "agent" de "rassasier") qui induit sur cet élément un effet métaphorique lui attribuant un correspondant dans l'autre domaine ("livre" en position d'agent de rassasier joue le rôle d'aliment), ils en deviennent explicites dans l'énoncé "le livre est un aliment pour l'esprit" où l'expression de la relation "est un" indique explicitement comment construire la signification dérivée (le livre nourrit l'esprit comme l'aliment nourrit le corps). Les métaphores envisagées dans le système COMET (COnstruction de METaphores) se construisent à partir de la structuration du lexique pour lequel une taxonomie est construite. De par cette organisation, où l'on a un ensemble de domaines structurés en une hiérarchie de classes, une hiérarchisation des éléments du lexique est réalisée, en allant du général au particulier. Nous présenterons la représentation employée pour saisir les contraintes posées par cette approche des métaphores, à travers leur génération. Nous parlerons d'abord des conditions de fond pour représenter des métaphores individuelles. Nous montrerons ensuite que notre connaissance des métaphores dans le langage ne peut pas être appréhendée de manière exhaustive, par simple énumération de chacune d'entre elles. L'examen des métaphores dans le langage montre certaines régularités. Pour percevoir ces régularités, des ensembles d'exemples seront analysés. Pour chaque cas appréhendé : nous donnerons un ensemble d'exemples illustratifs ; une analyse du phénomène en termes de représentation sera effectuée ; enfin, certaines suggestions seront faites, afin de donner une représentation qui pourrait suffisamment rendre compte des régularités rencontrées.</resume>
			<mots_cles></mots_cles>
			<title></title>
			<abstract></abstract>
			<keywords></keywords>
		</article>
		<article id="taln-1998-projet-001" session="">
			<auteurs>
				<auteur>
					<nom>Bernard Normier</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">ERLI</affiliation>
			</affiliations>
			<titre>Rôle des projets nationaux et européens dans le développement d'une offre produits en ingénierie linguistique</titre>
			<type>projet</type>
			<pages></pages>
			<resume></resume>
			<mots_cles></mots_cles>
			<title></title>
			<abstract></abstract>
			<keywords></keywords>
		</article>
		<article id="taln-1998-projet-002" session="">
			<auteurs>
				<auteur>
					<nom>Sylvie Régnier-Prost</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Aérospatiale</affiliation>
			</affiliations>
			<titre>Le projet européen DiET : aide à l'évaluation d'applications de TALN</titre>
			<type>projet</type>
			<pages></pages>
			<resume></resume>
			<mots_cles></mots_cles>
			<title></title>
			<abstract></abstract>
			<keywords></keywords>
		</article>
		<article id="taln-1998-projet-003" session="">
			<auteurs>
				<auteur>
					<nom>Célestin Sedogbo</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Thomson-CSF LCR</affiliation>
			</affiliations>
			<titre>Le projet national CORAIL : filtrage de l'information sur Internet</titre>
			<type>projet</type>
			<pages></pages>
			<resume></resume>
			<mots_cles></mots_cles>
			<title></title>
			<abstract></abstract>
			<keywords></keywords>
		</article>
	</articles>
</conference>