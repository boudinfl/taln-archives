&RQIpUHQFH 7$/1 
 &DUJqVH  MXLOOHW 
/DQJXDJH ,QGHSHQGHQW $XWRPDWLF $FTXLVLWLRQ RI 5LJLG
0XOWLZRUG 8QLWV IURP 8QUHVWULFWHG 7H[W &RUSRUD
Ga√´l Dias1, Sylvie Guillor√©2 and Jos√© Gabriel Pereira Lopes1
1
Universidade Nova de Lisboa, FCT/DI
Quinta da Torre, 2825-114, Monte da Caparica, Portugal
{ddg,gpl}@di.fct.unl.pt
2
Universit√© d'Orl√©ans, Laboratoire d'Informatique Fondamentale d'Orl√©ans
BP 6102 - 45061, Orl√©ans C√©dex 2, France
sylvie.guillore@lifo.univ-orleans.fr
$EVWUDFW
Multiword units are groups of words that occur together more often than expected by
chance in sub-languages. 3UpVLGHQW GH OD 5pSXEOLTXH, &RXSH GX PRQGH and 7UDLWp GH
0DDVWULFKW are multiword units. Unfortunately, most of the machine-readable dictionaries
contain clearly insufficient information about multiword unitsi. Therefore, their automatic
extraction from corpora is an important issue not only for natural language processing but also
for applications on Information Retrieval, Information Extraction and Machine Translation. In
this paper, we propose a new extraction system based on a new association measure, the
Mutual Expectation, and a new acquisition process based on an algorithm of local maxima,
the LocalMax algorithm.
 ,QWURGXFWLRQ

The acquisition of multiword units (MWUs) has long been a significant problem in natural
language processing, being relegated to the borders of lexicographic treatment. For the past
fifteen years, there has been a renewal in phraseology due to full access to large-scale text
corpora in machine-readable formats that allowed testing assumptions made about syntactical
regularities and flexibility constraints. From a statistical point of view, multiword units are
groups of words that occur together more often than expected by chance. Compound nouns
(3UpVLGHQW GH OD 5pSXEOLTXH), compound verbs (PHWWUH HQ RHXYUH), adverbial locutions (GqV
TXH SRVVLEOH), prepositional locutions (HQ PDWLqUH GH), conjunctive locutions (DLQVL TXH) and
frozen forms (-DFTXHV 'HORUV) share the properties of MWUs. In this paper, we present and
access a system exclusively based on statistics for massively extracting, from raw text,
contiguous MWUs (i.e. uninterrupted sequences of words) and non-contiguous rigid MWUs
(i.e. sequences of words interrupted by one or more gaps that are filled in by a small number
of interchangeable words). In order to extract MWUs, a new association measure based on the
concept of normalized expectation, the Mutual Expectation (ME) is conjugated with a new
multiword unit acquisition process based on a algorithm of local maxima, the LocalMax
algorithm [Silva et al.99]. The proposed approach copes with two major problems evidenced
by all previous works in the literature: the definition of unsatisfactory association measures

i
Two exceptions are the BBI Combinatory Dictionary of English [Benson86] and the DELAC and DELACS
[Silberztein90].

Ga√´l Dias et al.

and the ad hoc establishment of global association measure thresholds used to select MWUs
among word groups. In order to evaluate the quality of the results obtained, a comparison with
four other association measures proposed in the literature (the association ratio [Church90],
the Dice coefficient [Smadja96], the Œ¶2 [Gale91] and the Log-likelihood ratio [Dunning93]) is
performed once those measures are normalized in order to accommodate the MWU length
factor.
In the first two sections of this paper, we propose the core of the system by respectively
introducing the Mutual Expectation measure and the LocalMax algorithm. In the third section,
we discuss the comparative results obtained by applying to a French corpus of political
debates, the LocalMax algorithm with the four association measures listed above.
 7KH 0XWXDO ([SHFWDWLRQ 0HDVXUH

The transformation of the input text corpus into contingency tables, by counting contiguous
and non-contiguous Q-grams, allows the definition of mathematical models (or association
measures) to describe the degree of cohesiveness that stands between words. However, the
association measures presented so far in the literature (cf. [Church90], [Gale91], [Dunning93],
[Smadja93] and [Smadja96]) are unsatisfactory as they only evaluate the degree of
cohesiveness between two discrete random variables and do not generalize for the case of Q
variables. Moreover, many of these association measures rely too much on the marginal
probabilities thus misevaluating the attraction between words. In this section, we introduce the
Mutual Expectation measure based on the normalized expectation and the fair point of
expectation methodology that allows the generalization of the association measures for the
case of Q words.
We define the normalized expectation (NE) existing between Q words as the average
expectation of one word occurring in a given position knowing the presence of the other Q-1
words also constrained by their positions. The underlying concept is based on the conditional
probability defined in (1) where S
; [ ,< \
is the joint discrete density function between
the two random variables ;, < and S
< \
is the marginal discrete density function of the
variable <.
S ( ; = [, < = \ )                    ()
S( ; = [ | < = \ ) =
S(< = \ )

Let's take the Q-gram [w1 p12 w2 p13 w3 ... p1i wi ... p1n wn] where p1i, for i=2,...,n, denotes
the signed distanceii that separates word wi from word w1. In order to suit the definition of the
NE, an Q-gram can be considered as the composition of Q VXE-(Q)-gram obtained from the Q-
gram by extracting one word at a time from it. This can be thought as giving rise to the
occurrence of any of the following Q events where the underline denotes the missing word
from the Q-gram:

[ _____ w2 p23 w3 ... p2i wi ... p2n wn],                               word w1 missing,
[w1 _____ p13 w3 ... p1i wi ... p1n wn],                                word w2 missing, ...
[w1 p12 w2 p13 w3...p1(i-1) w(i-1) _____ p1(i+1) w(i+1)...p1n wn],      word wi missing, ...
[w1 p12 w2 p13 w3 ... p1i wi ... p1(n-1) w(n-1) _____ ],                word wn missing.
ii
The sign ‚Äú+‚Äù ("-") is used to represent words on the right (left) of w1

Automatic Acquisition of Rigid Multiword Units

In order to take into account all these events in just one measure, it is necessary to calculate
an average conditional probability. This is realized by the fair point of expectation (FPE)
which is defined in Equation (4), where p([w2 ... p2i wi ... p2n wn]), for i=3,...,n, is the
probability of the occurrence of the (Q)-gram [w2 ... p2i wi ... p2n wn] and
Ô£´Ô£Æ          ^  ^
Ô£π Ô£∂ iii
SÔ£¨Ô£¨ Ô£Ø w1 ... p1i wi ... p1n wn Ô£∫ Ô£∑Ô£∑      is the probability of the occurrence of one (Q)-gram containing
Ô£≠ Ô£∞                          Ô£ªÔ£∏
necessarily the first word w1 of the Q-gram.
1Ô£´                                                    Ô£´                                   Ô£π Ô£∂Ô£∂

‚àë SÔ£¨Ô£¨Ô£≠ Ô£ÆÔ£ØÔ£∞w ... p
Q
)3( ([w1 p12 w2... p1i wi ... p1n wn ]) =                Ô£¨ S([w2 ... p2i wi ... p2n wn ]) +
^    ^
wi ... p1n wn Ô£∫ Ô£∑Ô£∑ Ô£∑Ô£∑
Q Ô£¨Ô£≠
1        1i
L   =2                                           Ô£ª Ô£∏Ô£∏

So, the NE of a generic Q-gram is defined as being a "fair" conditional probability using the
fair point of expectation and is defined in (5), where p([w1... p1i wi ... p1n wn]) is the
probability of occurrence of the Q-gram [w1... p1i wi ... p1n wn] and FPE([w1... p1i wi ... p1n
wn]) its normalized expectation.
S([w1 ...p1i wi ... p1n wn])

1(([w1 ...p1i wi ... p1n wn ]) =
)3(([w1 ...p1i wi ... p1n wn ])

However, [Daille95] shows that one effective criterion for multiword unit identification is
simple frequency. From this assumption, we deduce that between two Q-grams with the same
normalized expectation, the more frequent Q-gram is more likely to be a multiword unit. So,
the Mutual Expectation between Q words is defined in (6) based on the normalized
expectation, NE([w1 ... p1i wi ... p1n wn]), and the simple frequency of the particular Q-gram [w1
... p1i wi ... p1n wn] , f([w1 ... p1i wi ... p1n wn]).

0(([Z1S1 Z  S1 Z ])
v   v    ¬Å√É   ¬Å     I ([Z1S1 Z  S1 Z ]) √ó 1(([Z1S1 Z  S1 Z ])
v    v     ¬Å√É   ¬Å                     v       v       ¬Å√É   ¬Å                            ()
 7KH /RFDO0D[ $OJRULWKP

Most of the approaches proposed for the extraction of multiword units are based on
association measure thresholds (cf. [Church90], [Daille95] and [Smadja96]). This is defined
by the underlying concept that there exits a limit association measure that allows to decide
whether an Q-gram is a multiword unit or not. But, these thresholds can only be justified
experimentally and so are prone to error. Moreover, they may vary with the type, the size and
the language of the document and vary obviously with the association measure. The
LocalMax algorithm [Silva et al.99] proposes a more robust, flexible and fine tuned approach.

The LocalMax algorithm elects the multiword units from the set of all the valued Q-grams
based on two assumptions: all the association measuresiv show that the more cohesive is a
group of words, the higher its score will be, and multiword units are highly associated
localized groups of words. As a consequence, an Q-gram (let‚Äôs name it 1) is a multiword
lexical unit if its association measure value, YDO
1
, is a local maximum. Let‚Äôs define the set of
the association measure values of all the (Q
JUDP contained in the Q-gram 1, by ‚Ñ¶n-1 and
the set of the association measure values of all the (Q
JUDP containing the Q-gram 1, by
‚Ñ¶n+1. The LocalMax algorithm is defined as follows:
iii
The "^" corresponds to a convention frequently used in Algebra that consists in writing a "^" on the top of the
omitted term of a given succession indexed from 1 to n.
iv The conditional entropy measure is one of the exceptions.

Ga√´l Dias et al.

‚àÄ[ ‚àà‚Ñ¶n-1 , ‚àÄ\ ‚àà‚Ñ¶n+1 , if [ ‚â§ YDO
1
and YDO
1
> \ then YDO
1
is a local maximum

One important property of the LocalMax algorithm is that it can be tested with any association
measure as they all share the first assumption explained above. In our system, an Q-gram is a
MWU if and only if its ME value is a local maximum.
 (YDOXDWLRQ RI WKH 5HVXOWV

In this section, we present the results obtained by applying the LocalMax algorithm and the
Mutual Expectation to a French corpus of political debates taken from the European
Parliament debates collection, that contains approximately 300000 words. The results are
compared with the ones obtained by applying to the same corpus the LocalMax algorithm
with the normalized association ratiov, the normalized Dice coefficientvi, the normalized Œ¶2 vii
and the normalized Log-likelihood ratioviii.
Contiguous and non-contiguous rigid multiword units have been extracted. In the case of
the extracted non-contiguous rigid multiword units, we analyze the results obtained for units
containing exactly one gap leaving for further study the analysis of all the units containing
two or more gaps. Indeed, the relevance of such units is difficult to judge and a case by case
analysis is needed. However, the reader may retain the basic idea that the more gaps there
exists in a multiword unit the less this unit is meaningful and the more it is likely to be an
incorrect multiword unit. The first results (See Table 1) show that 72.43% of the extracted
units are contiguous and only 27.57% are non-contiguous. From this result one can
acknowledge that the non-contiguous rigid multiword units are less expressive in this sub-
language than are the contiguous rigid multiword units. Nevertheless, their average frequency
is very similar to the one of the extracted contiguous multiword units showing that they do not
embody exceptions and that they reveal interesting phenomena of the sub-language. The
extracted contiguous multiword units can be classified into four types. 61% are noun phrases
and most of them are representative of the domain such as 3DUOHPHQW (XURSpHQ, )RQGV VRFLDO
HXURSpHQ or eWDWV PHPEUHV. 18% embody verbal multiword units such as IDLUH OH SRLQW PHWWUH
HQ RHXYUH, GUHVVHU OD OLVWH or LO \ D. 17% are prepositional/adverbial locutions such as HQ
PDWLqUH GH HQ UDLVRQ GH or HQ FRQIRUPLWp DYHF. Finally, 4% are prepositional structures such
as GH OD, VXU OH or GDQV OHV. The extracted non-contiguous rigid multiword units can be
classified into two main types. 78% are meaningful multiword units where the gap is a
generalization of the multiword unit meaning. For instance, the concept of WUDQVSRUW GH BBBB
GDQJHUHXVHV will be specified by filling in the gap with possible occurrences such as
VXEVWDQFHV or PDWLqUHV. 22% are multiword units that embody a syntactical relation. The
coordination structure is evidenced by units such as GH BBBBB HW GH and Q¬û BBBBB HW Q¬û, and the
negation structure by the -gram QH BBBBB SDV. The latter are much more frequent than the
former as they characterize patterns of the language and not only of the sub-language. In a
second stage, we measured the precision of the results based on two assumptions: multiword
v The normalized association ratio is the result of the application of the fair point of expectation methodology to
the association ratio introduced by [Church90].
vi The normalized Dice coefficient is the result of the application of the fair point of expectation methodology to
the Dice coefficient used by [Smadja96].
vii The normalized Œ¶2 is the result of the application of the fair point of expectation methodology to the Œ¶2 used
by [Gale91].
viii The normalized Log-likelihood ratio is the result of the application of the fair point of expectation
methodology to the Log-likelihood ratio used by [Dunning93].

Automatic Acquisition of Rigid Multiword Units

units are valid units if they are grammatically appropriate units (i.e. compound nouns/names,
compound verbs, compound prepositions/adverbs/conjunctions and frozen forms) or if they
are meaningful units even though they are not grammaticalix. In these conditions, the system
shows a precision of 86,59%. Unfortunately, we do not present the "classical" recall rate in
this experiment due to the lack of a reference corpus where all multiword units are identified.
However, we present the extraction rate, which is the percentage of well-extracted multiword
units in relation with the size of the corpus. It was evaluated at 1,58% (See Table1) referring
to the corpus with 300000 words.
7DEOH : Comparative results between five association measures

0XWXDO          1RUPDOL]HG         1RUPDOL]HG       1RUPDOL]HG        1RUPDOL]HG
([SHFWDWLRQ       $VVRFLDWLRQ           'LFH             Œ¶                 /RJ
5DWLR            &RHIILFLHQW                         OLNHOLKRRG
RI &0:8[                72.43             48.14             62.49             58.99            60.47
RI 1&0:8[L               27.57             51.86             37.51             41.01            39.53
$YHUDJH IUHT &0:8              7.11              2.24              9.11              9.79             5.39
$YHUDJH IUHT 1&0:8              7.12              2.14              8.78              3.64             4.23
$YHUDJH OHQJWK RI 0:8             3.32              3.33              2.06              2.85             2.36
3UHFLVLRQ              86.59             54.34             47.01             68.34            41.81
([WUDFWLRQ UDWH             1.58              0,84              1.56              0.95             3.05

The results obtained by applying the LocalMax algorithm to the other association
measures, compared with the ones obtained for the ME, show that the Mutual Expectation
measure evidences significant improvements in terms of precision and correctness of the
elected MWUs. The normalized association ratio makes rare word groups look more similar
than they really arexii and as a consequence the average frequency of the extracted multiword
units falls to 2.24 for the case of the CMWUs raising a weak extraction rate. Besides, almost
no -grams are extracted thus over-evaluating the average length of the units to 3.33 words.
The normalized Dice coefficient gives good results in terms of coverage but shows one of the
worst precision rate of all. Indeed, the Dice coefficient tends to elect exclusively -grams
showing an average length around 2.06 words. Moreover, the average-frequency rows of
Table 1 highlight the fact that the normalized Dice coefficient tends to elect preferably
frequent MWUs. The normalized Œ¶2 shows one of the best precision rate of all, but its
extraction rate is weak comparing to most of the other measures. Satisfyingly, it tends to elect
a more variegate set of MWUs than most of the other measures as it is evidenced by an
average length of 2.85. Finally, the normalized Log-likelihood ratio reveals the worst
precision rate of all measures in contrast with its extraction rate that evidences the best result.
But, similarly to the normalized Dice coefficient, it tends to elect exclusively -grams
showing an average length around 2.36 words. Besides, the precision of the elected Q-grams,
for Q higher than 2, is weak causing the low precision rate result. But, the measures presented
by the four other authors all raise the typical problem of high frequency words as they highly
depend on the marginal probabilities. Indeed, they underestimate the degree of cohesiveness
between words when the marginal probability of one variable (i.e. one word) is high.
ix This choice can easily be argued as a precision measure should be calculated in relation with a particular task.
For instance, one may calculate the precision of the extracted multiword units for machine translation purposes,
for information retrieval purposes or for lexicographic purposes.
x CMWU stands for Contiguous MultiWord Unit.
xi NCMWU stands for Non-Contiguous MultiWord Unit.
xii This confirms the results obtained by [Daille98].

Ga√´l Dias et al.
 &RQFOXVLRQ

We proposed in this paper a language independent statistically-based system to
automatically extract contiguous and non-contiguous rigid multiword units from unrestricted
text corpora. The method introduces a new association measure, the Mutual Expectation, and
a new multiword unit acquisition process, the LocalMax algorithm [Silva et al.99]. The
experiments realized on a 300000-words corpus of the legal domain evaluated the precision of
the system at 86,59%. We compared the Mutual Expectation measure with four other
normalized association measures, the association ratio [Church90], the Dice coefficient
[Smadja96], the Œ¶2 [Gale91] and the Log-likelihood ratio [Dunning93] by running the system
in the four cases. The comparative results showed that the Mutual Expectation gives higher
precision, overcomes the problem of highly frequent words raised by the four other measures
and satisfyingly tends to elect longer multiword units. Finally, the system ensures total
portability. It is applicable to various languages as it uses plain text corpora and requires only
the general information appearing in it [Dias99].
5HIHUHQFHV

Benson M. (1986), 7KH %%, &RPELQDWRU\ 'LFWLRQDU\ RI (QJOLVK
D *XLGH WR :RUG
&RPELQDWLRQV, Amsterdam and Philadelphia, John Benjamins
Church K. et al. (1990), Word Association Norms, Mutual Information, and Lexicography,
&RPSXWDWLRQDO /LQJXLVWLFV, Vol. 16(1), pp.23-29
Daille B. (1995), Study and Implementation of Combined Techniques for Automatic
Extraction of Terminology, 7KH EDODQFLQJ DFW FRPELQLQJ V\PEROLF DQG VWDWLVWLFDO DSSURDFKHV
WR ODQJXDJH, MIT Press
Daille B. et al. (1998), An evaluation of Statistical scores for word association 7ELOLVL
6\PSRVLXP RQ /RJLF /DQJXDJH DQG &RPSXWDWLRQ (CSLI)
Dias G. et al. (1999), Mutual Expectation and LocalMax Algorithm for Multiword Lexical
Unit Extraction, 3DSHU VXEPLWWHG DW (3,$
Dunning T. (1993), Accurate Methods for the Statistics of Surprise and Coincidence, $&/,
Vol. 19-1
Gale W. (1991), Concordances for Parallel Texts, Proceedings of Seventh Annual Conference
of the UW Center for the New OED and Text Research, Using Corpora, Oxford
Silberztein M. (1990), Le Dictionnaire Electronique des Mots Compos√©s, /DQJXH )UDQoDLVH,
Vol. 87, pp.79-83
Silva, J.F. and Lopes, J.G.P. (1999), A Local Maxima Method and a Fair Dispersion
Normalization for Extracting multiword units, ,Q 3URFHHGLQJV RI WKH WK 0HHWLQJ RQ
0DWKHPDWLFV RI /DQJXDJH
02/
 Orlando, Florida July 23-25, 1999
Smadja F. (1993), Retrieving Collocations From Text: XTRACT, &RPSXWDWLRQDO /LQJXLVWLFV,
Vol. 19 (1)
Smadja F. (1996), Translating Collocations for Bilingual Lexicons: A Statistical Approach,
$VVRFLDWLRQ IRU &RPSXWDWLRQDO /LQJXLVWLFV, Vol. 22-1
