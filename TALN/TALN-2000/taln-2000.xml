<?xml version="1.0" encoding="UTF-8"?>
<!-- 
	Meta-données construites à partir du programme http://liawww.epfl.ch/taln2000/ sur web archives.
	Recherche des articles un à un sur Google et modification des méta-données
	Mots clés récupérés de http://sites.univ-provence.fr/veronis/Atala/TALN/alpha.html
	Problèmes de titre, d'auteurs manquants, etc.
	Fichiers pdfs récupérés à la pêche sur internet.
-->
<conference>
	<edition>
		<acronyme>TALN'2000</acronyme>
		<titre>7ème conférence sur le Traitement Automatique des Langues Naturelles</titre>
		<ville>Lausanne</ville>
		<pays>Suisse</pays>
		<dateDebut>2000-10-16</dateDebut>
		<dateFin>2000-10-18</dateFin>
		<presidents>
			<nom>Eric Wehrli</nom>
		</presidents>
		<typeArticles>
			<type id="invite">Conférences invitées</type>
			<type id="long">Papiers longs</type>
			<type id="tutoriel">Tutoriels</type>
			<type id="poster">Posters</type>
			<type id="démonstration">Démonstrations</type>
		</typeArticles>
		<statistiques>
			<!-- <acceptations id="long" soumissions=""></acceptations>
			<acceptations id="court" soumissions=""></acceptations> -->
		</statistiques>
		<siteWeb>http://liawww.epfl.ch/taln2000/</siteWeb>
		<meilleurArticle>
			<!-- <articleId></articleId> -->
		</meilleurArticle>
	</edition>
	<articles>
		<article id="taln-2000-invite-001" session="">
			<auteurs>
				<auteur>
					<nom>Udo Hahn</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Universität Freiburg</affiliation>
			</affiliations>
			<titre></titre>
			<type>invite</type>
			<pages></pages>
			<resume></resume>
			<mots_cles></mots_cles>
			<title>Emerging Content Management Technologies</title>
			<abstract>Recent advances in language engineering have led to diverse techniques by which content items in written documents can be tracked and managed. Some of these techniques elaborate on fairly standard information retrieval methodologies (tf-idf, vector space model, etc.), though different applications are envisaged (e.g., filtering, notification or recommender systems). Current efforts also target on systems which provide automatic summarization based on the extraction of sentences (or phrases). Finally, information extraction is concerned with methodologies to extract relevant data from textual sources. In this talk, some of the core techniques for content tracking and management are identified, evaluation results are presented, and open challenges are discussed. </abstract>
			<keywords></keywords>
		</article>
		<article id="taln-2000-invite-002" session="">
			<auteurs>
				<auteur>
					<nom>Marc El-Bèze</nom>
					<email>marc.elbeze@lia.univ-avignon.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Laboratoire Informatique d'Avignon </affiliation>
			</affiliations>
			<titre>Entre corpus annoté et lexique sémantique, quelles options pour le TALN ?</titre>
			<type>invite</type>
			<pages></pages>
			<resume>Il est communément admis que la tâche de désambiguïsation sémantique n'est pas une fin en soi. Pour tenter d'apporter un début de solution à ce problème reconnu comme très difficile, de nombreux systèmes ont été développés. Pour la plupart, ces systèmes sont destinés à être les composants de systèmes plus complexes (moteurs de recherche d'information, de dialogue personne-machine, ou d'aide à la traduction). Néanmoins, ils sont testés en tant que tels dans le cadre de campagnes d'évaluation, comme par exemple Senseval ou Romanseval. La seconde édition de ces campagnes est d'ores et déjà planifiée. De fait, on est en droit de se demander - sans pour autant vouloir chercher à enrayer le mouvement -, si la désambiguïsation sémantique a un sens, et si oui lequel. Il ne faut pas voir dans ce questionnement un jeu de mots gratuit, mais bien la nécessité de soumettre à l'examen une pratique dans laquelle s'engagent de plus en plus de chercheurs, qu'ils soient linguistes ou informaticiens. Si l'on s'en tient au protocole suivi lors de la première campagne d'évaluation de Senseval, on peut dégager de ses caractéristiques un certain nombre d'observations qui peuvent alimenter la réflexion. Une quarantaine de mots appartenant à l'une ou l'autre de trois catégories grammaticales avait été retenue : les noms, les verbes et les adjectifs. Pour chacun de ces mots était fournie une liste d'étiquettes sémantiques et pour couvrir l'ensemble de ces sens, en moyenne, une centaine d'exemples étiquetés ainsi qu'une définition pour chaque étiquette. Pour chaque mot, enfin une centaine d'exemples de tests devaient être étiquetés par les différents systèmes en lice. Pour un mot donné, les étiquettes pouvaient entretenir des relations de type hiérarchique, ce qui permettait d'évaluer les systèmes à trois niveaux  de granularité : fin, grossier, et intermédiaire. Une remarque préalable concerne le corpus d'apprentissage disponible pour chacun des mots. Pour un mot donné, seul le mot en question était étiqueté. Pour les mots du contexte aucune étiquette sémantique n'était proposée. Les annotations sémantiques posées par des juges humains sur chacun des exemples relatifs à un mot particulier, avait fait l'objet d'un arbitrage, et quand cela  s'avérait impossible plusieurs étiquettes sémantiques avaient été maintenues. Enfin, détail qui peut avoir son importance : les étiquettes sémantiques utilisées pour annoter le corpus d'apprentissage étaient plus fines que celles qui étaient employés pour le niveau le plus fin d'évaluation. Notre propos n'est pas ici de décrire ifficultés à mettre en relation des définitions et des emplois de mots en contexte .Une des significations d'un mot employé dans un contexte particulier peut se trouver absente de la ressource pour plusieurs raisons. Les lacunes des dictionnaires ont  suffisamment été pointées du doigt à diverses reprises, pour qu'il soit nécessaire d'en rajouter sur le sujet. Par essence, une ressource finie ne peut couvrir toutes les productions résultant des capacités créatives qui s'exercent sur les langages naturels. Certains usages langagiers correspondent à des nuances fines dont il est difficile de rendre compte dans un lexique où par contre figurent souvent des acceptions qui n'ont plus cours. Par ailleurs, il n'y a pas de découpage unique d'un mot en unités de sens. Il suffit pour s'en convaincre de comparer les choix faits par différents dictionnaires. Mais, le problème est plus complexe que cela. En analysant le fonctionnement des métaphores, on peut expliquer comment certaines figures de style permettent de rajouter un sens (le plus souvent figuré) à un mot tout en maintenant en partie son sens premier. Ces évidences expliquent en grande partie la complexité de la relation entre étiquetage et choix d'étiquettes sémantiques. Les méthodes numériques ont leur mot à dire pour tenter de trouver une voie entre lexique et corpus annoté. Toute approche qui entre dans cette catégorie peut non seulement permettre de choisir une étiquette parmi plusieurs, mais aussi servir à classer toutes les étiquettes candidates soit par calcul de distances ou de vraisemblances. Si la méthode retenue est de ce type, le vecteur final associé à un exemple peut être vu comme un moyen de localiser un emploi particulier dans l'espace déterminé par la base que forment les étiquettes sémantiques. Par le biais d'une analyse en composantes principales ou d'une analyse discriminante, des axes orthogonaux peuvent être dégagés un à un, axes correspondant à un compromis entre le jeu d'étiquettes initial et les exemples présents dans le corpus annoté. Même si le processus n'a pas tendance à converger, il ne serait peut-être pas inutile de de le voir comme une étape parmi d'autres d'une procédure itérative appliquée s'il le faut sur des données mouvantes afin de reproduire les aspects dynamiques de toute langue vivante. Si l'on accepte l'idée que Numérique et Métrique ont un rôle à jouer dans le domaine de la Sémantique, il est possible de voir le problème de la désambiguïsation sémantique comme formant un tout avec celui du choix des étiquettes. La question ne serait plus comment choisir entre tel ou tel sens pour un emploi donné, mais dans quelle région se situe cet emploi, sachant que la somme des usages aura tendance à modifier l'espace lui-même, dès qu'il sera patent qu'il aura été pour une raison ou pour autre, sous ou sur dimensionné.</resume>
			<mots_cles></mots_cles>
			<title></title>
			<abstract></abstract>
			<keywords></keywords>
		</article>
		<article id="taln-2000-long-001" session="">
			<auteurs>
				<auteur>
					<nom>Laurence Danlos</nom>
					<email></email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Bertrand Gaiffe</nom>
					<email></email>
					<affiliationId>2</affiliationId>
					<affiliationId>3</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LATTICE ET LORIA-INRIA</affiliation>
				<affiliation affiliationId="2">LANGUE ET DIALOGUE (INRIA Lorraine - LORIA)</affiliation>
				<affiliation affiliationId="3">CNRS</affiliation>
			</affiliations>
			<titre>Coréférence événementielle et relations de discours</titre>
			<type>long</type>
			<pages></pages>
			<resume>La coréférence événementielle est un phénomène largement ignoré tant dans les travaux sur la coréférence que dans ceux sur l'ordre temporel dans le discours. Pourtant, la coréférence événementielle est la clef de voûte sur laquelle reposent au moins quatre types de discours. Les descriptions et analyses linguistiques de ces discours permettront de mettre en avant des phénomènes linguistiques inhabituels (e.g. coréférence entre éléments quantifiés existentiellement). Les relations de discours qui sont en jeu seront ensuite examinées. Cette étude nous amènera à introduire et définir de nouvelles relations de discours qui seront discutées dans le cadre de la SDRT.</resume>
			<mots_cles>coréférence événementielle, relation de discours, relation de coréférence</mots_cles>
			<title></title>
			<abstract></abstract>
			<keywords></keywords>
		</article>
		<article id="taln-2000-long-002" session="">
			<auteurs>
				<auteur>
					<nom>Holger Schauer</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1"></affiliation>
				<affiliation affiliationId="2"></affiliation>
				<affiliation affiliationId="3"></affiliation>
			</affiliations>
			<titre></titre>
			<type>long</type>
			<pages></pages>
			<resume></resume>
			<mots_cles></mots_cles>
			<title>Referential Structure and Coherence Structure</title>
			<abstract></abstract>
			<keywords></keywords>
		</article>
		<article id="taln-2000-long-003" session="">
			<auteurs>
				<auteur>
					<nom>Jill Burstein</nom>
					<email>jburstein@ets.org</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Daniel Marcu</nom>
					<email>marcu@isi.edu</email>
					<affiliationId></affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">ETS Technologies Rosedale Road, MS 11R Princeton, NJ 08541 USA</affiliation>
				<affiliation affiliationId="2">Information Sciences Institute/ University of Southern California 4676 AdmiraltyWay, Suite 1001 Marina Del Rey, CA 90292 USA</affiliation>
			</affiliations>
			<titre></titre>
			<type>long</type>
			<pages></pages>
			<resume></resume>
			<mots_cles></mots_cles>
			<title>Toward Using Text Summarization for Essay-Based Feedback</title>
			<abstract>We empirically study the impact of using automatically generated summaries in the context of electronic essay rating. Our results indicate that 40% and 60% discourse-based essay summaries improve the performance of the topical analysis module of e-rater. E-rater is a system that electronically scores GMAT essays. We envision using automatically generated essay summaries for instructional feedback, as a supplement to the e-rater score.</abstract>
			<keywords></keywords>
		</article>
		<article id="taln-2000-long-004" session="">
			<auteurs>
				<auteur>
					<nom>Gees C. Stein</nom>
					<email>steing@crd.ge.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Amit Bagga</nom>
					<email>bagga@crd.ge.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>G. Bowden Wise</nom>
					<email>wiseg@crd.ge.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">General Electric, Corporate R&amp;D, One Research Circle, Niskayuna NY 12309, USA</affiliation>
			</affiliations>
			<titre></titre>
			<type>long</type>
			<pages></pages>
			<resume></resume>
			<mots_cles></mots_cles>
			<title>Multi-Document Summarization: Methodologies and Evaluations</title>
			<abstract>This paper describes a system for the summarization of multiple documents. The system produces multi-document summaries using clustering techniques to identify common themes across the set of documents. For each theme, the system identifies representative passages that are included in the final summary. We also describe a methodology for evaluation of our system which is based upon a question answering task. Results of our evaluation are also presented.</abstract>
			<keywords></keywords>
		</article>
		<article id="taln-2000-long-005" session="">
			<auteurs>
				<auteur>
					<nom>Mare Koit</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Hadur Oim</nom>
					<email></email>
					<affiliationId></affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1"></affiliation>
				<affiliation affiliationId="2"></affiliation>
				<affiliation affiliationId="3"></affiliation>
			</affiliations>
			<titre></titre>
			<type>long</type>
			<pages></pages>
			<resume></resume>
			<mots_cles></mots_cles>
			<title>Reasoning in Interaction: A Model of Dialogue</title>
			<abstract></abstract>
			<keywords></keywords>
		</article>
		<article id="taln-2000-long-006" session="">
			<auteurs>
				<auteur>
					<nom>Mohamed-Zakaria Kurdi</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1"></affiliation>
				<affiliation affiliationId="2"></affiliation>
				<affiliation affiliationId="3"></affiliation>
			</affiliations>
			<titre>La grammaire sémantique d'unification d'arbres: un formalisme pour l'analyse des dialogues oraux spontanés</titre>
			<type>long</type>
			<pages></pages>
			<resume>Cet article porte sur la grammaire sémantique d'unification d'arbres (STUO). 11 s'agit d'un formalisme que nous proposons comme une alternative aux approches simplificatrices menées dans le contexte du traitement automatique de la parole ainsi qu'aux approches à base de grammaires classiques qui sont généralement non adaptées au traitement de l'oral. La motivation essentielle de ce formalisme est la combinaison de la robustesse et la simplicité des grammaires sémantiques à la profondeur des grammaires classiques. Les propriétés essentielles de ce formalisme sont : une interaction directe entre la syntaxe et la sémantique, un système de traits économique et une simplicité tant de la mise en oeuvre de la grammaire que pour sa modification. La STUG a été implémentée au sein du système OASIS qui est un système d'analyse partielle de la parole spontanée. Les résultats de l'évaluation ont montré la bonne couverture de notre grammaire tant au niveau des arbres analysés qu'au niveau lexical ainsi que l'efficacité de cette grammaire pour la desambiguïsation et pour l'évitement des erreurs dans l'entrée.</resume>
			<mots_cles>grammaire, grammaire sémantique, unification d'arbres, parole, parole spontanée, ambiguïté contextuelle</mots_cles>
			<title></title>
			<abstract></abstract>
			<keywords></keywords>
		</article>
		<article id="taln-2000-long-007" session="">
			<auteurs>
				<auteur>
					<nom>Cristian Ciressan</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Martin Rajman</nom>
					<email></email>
					<affiliationId></affiliationId>
				</auteur>
				<auteur>
					<nom>Eduardo Sanchez</nom>
					<email></email>
					<affiliationId></affiliationId>
				</auteur>
				<auteur>
					<nom>Jean-Cédric Chappelier</nom>
					<email></email>
					<affiliationId></affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1"></affiliation>
				<affiliation affiliationId="2"></affiliation>
				<affiliation affiliationId="3"></affiliation>
			</affiliations>
			<titre></titre>
			<type>long</type>
			<pages></pages>
			<resume></resume>
			<mots_cles></mots_cles>
			<title>Towards NLP co-processing: An FPGA implementation of a context-free parser</title>
			<abstract></abstract>
			<keywords></keywords>
		</article>
		<article id="taln-2000-long-008" session="">
			<auteurs>
				<auteur>
					<nom>André Kempe</nom>
					<email>andre.kempe@xrce.xerox.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Xerox Research Centre Europe – Grenoble Laboratory 6 chemin de Maupertuis – 38240 Meylan – France</affiliation>
			</affiliations>
			<titre>Reduction of Intermediate Alphabets in Finite-State Transducer Cascades</titre>
			<type>long</type>
			<pages></pages>
			<resume></resume>
			<mots_cles></mots_cles>
			<title></title>
			<abstract>This article describes an algorithm for reducing the intermediate alphabets in cascades of finite-state transducers (FSTs). Although the method modifies the component FSTs, there is no change in the overall relation described by the whole cascade. No additional information or special algorithm, that could decelerate the processing of input, is required at runtime. Two examples from Natural Language Processing are used to illustrate the effect of the algorithm on the sizes of the FSTs and their alphabets. With some FSTs the number of arcs and symbols shrank considerably.</abstract>
			<keywords></keywords>
		</article>
		<article id="taln-2000-long-009" session="">
			<auteurs>
				<auteur>
					<nom>Gabriel Illouz</nom>
					<email>gabrieli@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Limsi -CNRS BP133 91403 Orsay Cedex France</affiliation>
			</affiliations>
			<titre>Vers un apprentissage en TALN dépendant du type de Texte</titre>
			<type>long</type>
			<pages></pages>
			<resume>Dans cet article, nous présentons la problématique de l’hétérogénéité des données textuelles et la possibilité d’utiliser cette dernière pour améliorer les traitements automatiques du langage naturel. Cette hypothèse a été abordée dans (Biber, 1993) et a donné lieu à une première vérification empirique dans (Sekine, 1998). Cette vérification a pour limite de ne s’adapter qu’à des textes dont le type est explicitement marqué. Dans le cadre de textes tout venant, nous proposons une méthode pour induire des types de textes, apprendre des traitements spécifiques à ces types puis, de façon itérative, en améliorer les performances.</resume>
			<mots_cles>annotation morpho-syntaxique, type de texte, linguistique de corpus, apprentissage, classification</mots_cles>
			<title></title>
			<abstract></abstract>
			<keywords></keywords>
		</article>
		<article id="taln-2000-long-010" session="">
			<auteurs>
				<auteur>
					<nom>Emmanuel Morin</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1"></affiliation>
				<affiliation affiliationId="2"></affiliation>
				<affiliation affiliationId="3"></affiliation>
			</affiliations>
			<titre>Complémentarité des approches supervisées et non supervisées pour l'acquisition de relations entre termes</titre>
			<type>long</type>
			<pages></pages>
			<resume>Cet article a pour objectif de préciser la complémentarité des approches supervisées et non supervisées utilisées en structuration terminologique pour extraire des relations entre termes. Cette étude est réalisée sur un exemple concret où nous cherchons à faire ressortir les avantages et les inconvénients de chaque approche. Au terme de cette analyse, nous proposons un cadre pour les employer de façon synergique.</resume>
			<mots_cles>analyse distributionnelle, terminologie, relation conceptuelle entre termes, extraction, extraction de couple</mots_cles>
			<title></title>
			<abstract></abstract>
			<keywords></keywords>
		</article>
		<article id="taln-2000-long-011" session="">
			<auteurs>
				<auteur>
					<nom>Thierry Poibeau</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1"></affiliation>
				<affiliation affiliationId="2"></affiliation>
				<affiliation affiliationId="3"></affiliation>
			</affiliations>
			<titre>De l'acquisition de classes lexicales à l'induction semi-automatique de grammaires locales</titre>
			<type>long</type>
			<pages></pages>
			<resume>Cette étude vise à automatiser partiellement l'acquisition de ressources pour un système d'extraction fondé sur la boîte à outils INTEX. Les processus d'apprentissage mis en oeuvre sont symboliques, supervisés et fortement interactifs afin de n'apprendre que ce qui est utile pour la tâche. Nous examinons d'abord la notion d'automate patron, permettant l'acquisition d'éléments apparaissant dans des contextes similaires, nous proposons ensuite plusieurs mécanismes de généralisation avant d'envisager l'induction semi-automatique de grammaires locales.</resume>
			<mots_cles>grammaire, grammaire locale, acquisition de classes, classes lexicales, corpus, automate, automate patron, induction de grammaire</mots_cles>
			<title></title>
			<abstract></abstract>
			<keywords></keywords>
		</article>
		<article id="taln-2000-long-012" session="">
			<auteurs>
				<auteur>
					<nom>Mariana Damova</nom>
					<email>damova@cs.concordia.ca</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Sabine Bergler</nom>
					<email>bergler@cs.concordia.ca</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Computer Science Department, Concordia University, 1455 de Maisonnauve blvd. W, Montreal (Quebec), Canada, H3G 1M8</affiliation>
			</affiliations>
			<titre></titre>
			<type>long</type>
			<pages></pages>
			<resume></resume>
			<mots_cles></mots_cles>
			<title>The Aspectual Type BEGIN</title>
			<abstract>This paper deals with the notion of aspect as it is understood in the eventuality structure based formal approaches to aspect. These approaches typically link aspect to the interpretation of the philosophical and ontological notion of event, seen as a conceptual entity with rigid edges: beginning, protraction and end, and analyse and study extensively the end part of events ((Vendler, 1967),(Moens &amp; Steedman, 1988), (Smith, 1991), (Pustejovsky, 1991), (Krifka, 1989), (Partee, 1984), (Hinrichs, 1986), etc.). The beginning, a semantic counterpart of the culmination on the other hand, has not been discussed so much at large. We analyse various language means that convey beginning and argue for the need of a mechanism to provide a uniform interpretation for them. We define the aspectual type BEGIN, and develop its semantic representation along the general lines of accounts of temporal reference of Discourse Representation Theory ((Kamp, 1979); (Kamp &amp; Reyle, 1993)). We extend the DRT analysis of tense and aspect in postulating a three layered formal representation for aspect. The aspectual type BEGIN introduces a DRS aspectual operator, instead of a temporal discourse referent. We embed its explicit event structure into the operator’s definition, by adopting Pustejovsky’s formalisation (Pustejovsky, 1995). We show that the proposed approach represents the aspectual type BEGIN correctly across categories, that is, it works on all relevant levels: lexical semantics, grammatical devices, secondary predication, discourse, and it covers the semantics of BEGIN in a uniform way.</abstract>
			<keywords></keywords>
		</article>
		<article id="taln-2000-long-013" session="">
			<auteurs>
				<auteur>
					<nom>Sandiway Fong</nom>
					<email>sandiway@research.nj.nec.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Christiane Fellbaum</nom>
					<email>fellbaum@research.nj.nec.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>David Lebeaux</nom>
					<email>lebeaux@research.nj.nec.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">NEC Research Institute, Princeton NJ</affiliation>
			</affiliations>
			<titre></titre>
			<type>long</type>
			<pages></pages>
			<resume></resume>
			<mots_cles></mots_cles>
			<title>Semantic Templates and Transitivity Alternations in a Computational Lexicon</title>
			<abstract>A systematic and principled account of verb subcategorization is important for largescale lexicon construction. A given verb may have several subcategorization frames in which its arguments appear. Starting from a lexical-semantic description of event structure, we describe a mechanism for generating subcategorization properties for a large variety of verb classes. In this paper, we motivate this mechanism by proposing a distinction between two kinds of resultative as well as the unaccusative and middle constructions.</abstract>
			<keywords></keywords>
		</article>
		<article id="taln-2000-long-014" session="">
			<auteurs>
				<auteur>
					<nom>Claude Laï</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1"></affiliation>
				<affiliation affiliationId="2"></affiliation>
				<affiliation affiliationId="3"></affiliation>
			</affiliations>
			<titre>Propagation de traits conceptuels au moyen des métastructures Prolog</titre>
			<type>long</type>
			<pages></pages>
			<resume>Après avoir effectué une description des métastructures Prolog, nous montrons leur utilité dans le domaine du Traitement Automatique du Langage Naturel, et plus précisément dans la propagation de traits conceptuels complexes comme l'appartenance des individus à des domaines pouvant faire intervenir des unions de produits cartésiens d'ensembles.</resume>
			<mots_cles>métastructure Prolog, langage, langage naturel, programmation, programmation par contrainte, produit cartésien</mots_cles>
			<title></title>
			<abstract></abstract>
			<keywords></keywords>
		</article>
		<article id="taln-2000-long-015" session="">
			<auteurs>
				<auteur>
					<nom>Jacques Savoy</nom>
					<email>Jacques.Savoy@unine.ch</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Yves Rasolofo</nom>
					<email>Yves.Rasolofo@unine.ch</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Institut interfacultaire d'informatique, Pierre-à-Mazel 7, 2000 Neuchâtel (Suisse)</affiliation>
			</affiliations>
			<titre>Recherche d'informations dans un environnement distribué</titre>
			<type>long</type>
			<pages></pages>
			<resume>Le Web ou les bibliothèques numériques offrent la possibilité d'interroger de nombreux serveurs d'information (collections ou moteurs de recherche) soulevant l'épineux problème de la sélection des meilleures sources de documents et de la fusion des résultats provenant de différents serveurs interrogés. Dans cet article, nous présentons un nouvelle approche pour la sélection des collections basée sur les arbres de décision. De plus, nous avons évalué différentes stratégies de fusion et de sélection permettant une meilleure vue d'ensemble des différentes solutions.</resume>
			<mots_cles>recherche d'information, modèle vectoriel, arbre, arbre de décision, moteur de recherche, indexation</mots_cles>
			<title></title>
			<abstract>The Web and digital libraries offer the possibility to send natural language queries to various information servers (corpora or search engines) raising the difficult problem of selecting the best document sources and merging the results provided by different servers. In this paper, a new approach for collections selection based on decision trees is described. Moreover, different merging and selection procedures have been evaluated leading to an overview of the suggested approaches.</abstract>
			<keywords></keywords>
		</article>
		<article id="taln-2000-long-016" session="">
			<auteurs>
				<auteur>
					<nom>Sébastien L'haire</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Juri Mengon</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Christopher Laenzlinger</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Laboratoire d’Analyse et de Technologie du Langage Université de Genève 2, rue de Candolle, 1211 Genève 4</affiliation>
			</affiliations>
			<titre>Outils génériques et transfert hybride pour la traduction automatique sur Internet</titre>
			<type>long</type>
			<pages></pages>
			<resume>Dans cet article, nous décrivons un système de traduction automatique pour l’allemand, le français, l’italien et l’anglais. Nous utilisons la technique classique analyse-transfert-génération. Les phrases d’entrée sont analysées par un analyseur générique multilingue basé sur la théorie ((Principes &amp; Paramètres)) de la grammaire générative chomskienne. Le mécanisme de transfert agit sur des représentations hybrides qui combinent des éléments lexicaux avec de l’information sémantique abstraite. Enfin, un générateur inspiré de la même théorie linguistique engendre des phrases de sortie correctes. Nous décrivons également brièvement les différentes interfaces envisagées sur Internet.</resume>
			<mots_cles>traduction, traduction automatique, transfert lexico-structural, sémantique, éléments lexicaux</mots_cles>
			<title></title>
			<abstract></abstract>
			<keywords></keywords>
		</article>
		<article id="taln-2000-long-017" session="">
			<auteurs>
				<auteur>
					<nom>Didier Maillat</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1"></affiliation>
				<affiliation affiliationId="2"></affiliation>
				<affiliation affiliationId="3"></affiliation>
			</affiliations>
			<titre></titre>
			<type>long</type>
			<pages></pages>
			<resume></resume>
			<mots_cles></mots_cles>
			<title>Computing the Spatial Frame of Reference: How to Interpret Directional Prepositions</title>
			<abstract></abstract>
			<keywords></keywords>
		</article>
		<article id="taln-2000-long-018" session="">
			<auteurs>
				<auteur>
					<nom>Yann Mathet</nom>
					<email>mathet@info.unicaen.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">GREYC, Université de Caen - Campus II, 14032 Caen Cedex</affiliation>
			</affiliations>
			<titre>Le paradigme monodimensionnel dans l'expression de l'espace et du déplacement</titre>
			<type>long</type>
			<pages></pages>
			<resume>La sémantique de certains verbes (doubler, distancer, suivre) et de certaines prépositions ou adverbes (devant, derrière) peut poser problème dès lors qu'elle est considérée comme purement spatiale, c'est-à-dire en des termes " classiques " comme la topologie, le repérage ou la distance. Nous proposons dans cet article une description plus générale de ces items lexicaux basée sur la notion d'axe abstrait, rendant compte de leur sens dans différents domaines, ainsi que les différents mécanismes permettant de les plonger dans le domaine qui concerne notre recherche, le spatio-temporel. Ces mécanismes sont intégrés dans un modèle informatique de génération automatique de prédicats verbaux afin d'éprouver leur pertinence.</resume>
			<mots_cles>paradigme monodimensionnel, prédication monodimensionnelle, prédicat, prédicat verbal, sémantique</mots_cles>
			<title></title>
			<abstract></abstract>
			<keywords></keywords>
		</article>
		<article id="taln-2000-long-019" session="">
			<auteurs>
				<auteur>
					<nom>Philippe Blache</nom>
					<email>pb@lpl.univ-aix.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LPL-CNRS, Université de Provence 29 Avenue Robert Schuman 13621 Aix-en-Provence, France</affiliation>
			</affiliations>
			<titre>Le rôle des contraintes dans les théories linguistiques et leur intérêt pour l'analyse automatique</titre>
			<type>long</type>
			<pages></pages>
			<resume>Tous les formalismes linguistiques font usage de la notion de contrainte qui, dans son sens le plus large, indique une propriété devant être satisfaite. Les contraintes sont extrêmement utiles à la fois pour représenter l’information linguistique, mais également pour en contrôler le processus d’analyse. Cependant, l’usage qui est fait des contraintes peut être très différent d’une approche à l’autre : dans certains cas, il s’agit simplement d’un mécanisme d’appoint, dans d’autres, les contraintes sont au coeur de la théorie. Il existe cependant un certain nombre de restrictions à leur utilisation, en particulier pour ce qui concerne leur implantation. Plus précisément, s’il semble naturel (au moins dans certains paradigmes) de considérer l’analyse syntaxique comme un problème de satisfaction de contraintes, on constate cependant qu’il est extrêmement difficile de réaliser concrètement une telle implantation. Ce constat est en fait révélateur d’un problème dépassant le simple cadre de l’implémentation : nous montrons dans cet article qu’une approche totalement basée sur les contraintes (permettant donc de concevoir l’analyse comme un problème de satisfaction) est incompatible avec une interprétation générative classique accordant un statut particulier à la relation de dominance. Nous proposons ici un cadre permettant à la fois de tirer parti des avantages des grammaires syntagmatiques tout en s’affranchissant des problèmes liés aux approches génératives pour ce qui concerne l’usage des contraintes en tant qu’unique composant grammatical. Nous présentons ici cette approche, les Grammaires de Propriétés, ainsi que leur implémentation.</resume>
			<mots_cles>contrainte contextuelle, grammaire, grammaire de propriétés, relation de dépendance, grammaire de dépendance</mots_cles>
			<title></title>
			<abstract></abstract>
			<keywords></keywords>
		</article>
		<article id="taln-2000-long-020" session="">
			<auteurs>
				<auteur>
					<nom>Thierry Etchegoyhen</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1"></affiliation>
				<affiliation affiliationId="2"></affiliation>
				<affiliation affiliationId="3"></affiliation>
			</affiliations>
			<titre>Analyse Syntaxique Monotone par Décisions Différées</titre>
			<type>long</type>
			<pages></pages>
			<resume>Dans cet article nous présentons une approche à l'analyse syntaxique automatique où la levée d'ambiguïtés est différée jusqu'à l'apparition d'éléments de la chaîne d'entrée permettant de procéder à une analyse correcte, la désambiguisation étant alors effectuée en cascade. L'analyseur a pour caractéristiques une croissance monotone de l'information syntaxique au fil de l'analyse, la garantie de ne pas échouer sur des phrases grammaticales telles les phrases-labyrinthe, et une faible complexité computationnelle. Le système présenté cumule ainsi les avantages d'une approche déterministe (efficacité et optimisation des calculs) et ceux d'une approche non-déterministe (adéquation empirique).</resume>
			<mots_cles>analyse syntaxique, analyse syntaxique monotone, désambiguïsation, désambiguïsation syntaxique, levée d'ambiguïté structurelle</mots_cles>
			<title></title>
			<abstract></abstract>
			<keywords></keywords>
		</article>
		<article id="taln-2000-long-021" session="">
			<auteurs>
				<auteur>
					<nom>Sylvain Kahane</nom>
					<email>sk@ccr.jussieu.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">TALaNa/LaTTiCe (Univ. Paris 7) et Univ. Paris 10 - Nanterre</affiliation>
			</affiliations>
			<titre>Des grammaires pour définir une correspondance: un point de vue mathématique et épistémologique</titre>
			<type>long</type>
			<pages></pages>
			<resume>Dans cet article nous introduisons la notion de grammaire transductive, c'est-à-dire une grammaire formelle définissant une correspondance entre deux familles de structures. L'accent sera mis sur le module syntaxique de la théorie Sens-Texte et sur une famille élémentaire de grammaires de dépendance transductives. Nous nous intéresserons à la comparaison avec les grammaires génératives, ce qui nous amènera à discuter de l'interprétation des modèles génératifs actuels.</resume>
			<mots_cles>grammaire, grammaire transductive, grammaire générative, grammaire formelle, grammaire de dépendance, lexie</mots_cles>
			<title></title>
			<abstract></abstract>
			<keywords></keywords>
		</article>
		<article id="taln-2000-long-022" session="">
			<auteurs>
				<auteur>
					<nom>Farid Cerbah</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1"></affiliation>
				<affiliation affiliationId="2"></affiliation>
				<affiliation affiliationId="3"></affiliation>
			</affiliations>
			<titre>Une étude comparative de méthodes de catégorisation sémantique de termes techniques</titre>
			<type>long</type>
			<pages></pages>
			<resume>L'acquisition et la mise à jour de ressources terminologiques sont des tâches difficiles, en particulier lorsque ces ressources contiennent des informations d'ordre sémantique. Cette article traite de la catégorisation sémantique de termes techniques. Le but de ce processus est d'assigner des domaines sémantiques à de nouveaux termes. Nous proposons deux approches qui reposent sur des sources d'informations différentes. L'approche exogène exploite des informations contextuelles extraites de corpus. L'approche endogène repose sur une analyse lexicale de termes déjà catégorisés. Nous décrivons les deux approches mises en oeuvre ainsi que les expérimentations menées sur des jeux de test significatifs. Les résultats obtenus montrent que la catégorisation de termes peut constituer une aide conséquente dans les processus d'acquisition de ressources terminologiques.</resume>
			<mots_cles>acquisition de termes techniques, terminologie, analyse lexicale, corpus</mots_cles>
			<title></title>
			<abstract></abstract>
			<keywords></keywords>
		</article>
		<article id="taln-2000-long-023" session="">
			<auteurs>
				<auteur>
					<nom>Gaël De Chalendar</nom>
					<email>Gael.de.Chalendar@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Brigitte Grau</nom>
					<email>Brigitte.Grau@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIMSI/CNRS, BP 133, 91 403 Orsay Cedex, France</affiliation>
			</affiliations>
			<titre>SVETLAN' ou Comment Classer les Mots en fonction de leur Contexte</titre>
			<type>long</type>
			<pages></pages>
			<resume>L’utilisation de connaissances sémantiques dans les applications de TAL améliore leurs performances. Cependant, bien que des lexiques étendus aient été développés, il y a peu de ressources non dédiées à des domaines spécialisés et contenant des informations sémantiques pour les mots. Dans le but de construire une telle base, nous avons conçu le système SVETLAN’, capable d’apprendre des catégories de noms à partir de textes, quel que soit leur domaine. Dans le but d’éviter de créer des classes générales regroupant tous les sens des mots, les classes sont apprises en fonction de l’usage des mots en contexte.</resume>
			<mots_cles>sémantique, langue, langue générale, segment textuel, module d'apprentissage</mots_cles>
			<title></title>
			<abstract></abstract>
			<keywords></keywords>
		</article>
		<article id="taln-2000-long-024" session="">
			<auteurs>
				<auteur>
					<nom>Patrick Paroubek</nom>
					<email>pap@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Martin Rajman</nom>
					<email>rajman@lia.di.epfl.ch</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIMSI - CNRS, Batiment 508 Universite Paris XI, 91403 Orsay Cedex</affiliation>
				<affiliation affiliationId="2">Laboratoire d’Intelligence Artificielle, Département Informatique Ecole Polytechnique Fédérale de Lausanne, CH-1015 Lausanne-Ecublens, Switzerland</affiliation>
			</affiliations>
			<titre>MULTITAG, une ressource linguistique produit du paradigme d'évaluation</titre>
			<type>long</type>
			<pages></pages>
			<resume>Dans cet article, nous montrons comment le paradigme d'évaluation peut servir pour produire de façon plus économique des ressources linguistiques validées de grande qualité. Tous d'abord nous présentons le paradigme d'évaluation et rappelons les points essentiels de son histoire pour le traitement automatique des langues, depuis les premières applications dans le cadre des campagnes d'évaluation américaines organisées par le NIST et le DARPA jusqu'aux derniers efforts européens en la matière. Nous présentons ensuite le principe qui permet de produire à coût réduit des ressources linguistiques validées et de grande qualité à partir des données qui sont produites lorsque l'on applique le paradigme d'évaluation. Ce principe trouve ses origines dans les expériences (Recognizer Output Voting Error Recognition) qui ont été effectuées pendant les campagnes d'évaluation américaine pour la reconnaissance automatique de la parole. Il consiste à combiner les données produites par les systèmes à l'aide d'une simple stratégie de vote pour diminuer le nombre d'erreurs. Nous faisons alors un lien avec les stratégies d'apprentissages automatiques fondées sur la combinaison de systèmes de même nature. Notre propos est illustré par la description de la production du corpus MULTITAG (projet du programme Ingénierie des Langues des département SPI et SHS du CNRS) à partir des données qui avaient été annotées lors de la campagne d'évaluation GRACE, correspondant à un corpus d'environ 1 million de mots annotés avec un jeu d'étiquettes morpho-syntaxiques de grain très fin dérivé de celui qui a été défini dans les projets EAGLES et MULTEXT. Nous présentons le corpus MULTITAG et la procédure qui a été suivie pour sa production et sa validation. Nous concluons en présentant le gain obtenu par rapport à une méthode classique de validation de marquage morho-syntaxique.</resume>
			<mots_cles>paradigme d'évaluation, campagne d'évaluation, système d'annotation, corpus</mots_cles>
			<title></title>
			<abstract></abstract>
			<keywords></keywords>
		</article>
		<article id="taln-2000-long-025" session="">
			<auteurs>
				<auteur>
					<nom>Chafik Aloulou</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Lamia Hadrich Belguith</nom>
					<email></email>
					<affiliationId></affiliationId>
				</auteur>
				<auteur>
					<nom>Abdelmajid Ben Hamadou</nom>
					<email></email>
					<affiliationId></affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1"></affiliation>
				<affiliation affiliationId="2"></affiliation>
				<affiliation affiliationId="3"></affiliation>
			</affiliations>
			<titre>Vers un système d'analyse syntaxique robuste pour l'Arabe</titre>
			<type>long</type>
			<pages></pages>
			<resume>Le degré de profondeur et de finesse de l'analyse syntaxique d'un texte écrit dépend énormément de l'objectif de l'analyse (analyse globale, analyse partielle, analyse détaillée, etc.) ainsi que du type d'application nécessitant cette analyse. Dans cet article, nous présentons une approche originale d'analyse syntaxique robuste appliquée à l'arabe et basée sur l'architecture multiagent. Comme première application de notre approche, notre système sera couplé avec un système de reconnaissance de l'écriture arabe dans le but d'effectuer, d'une part, la validation linguistique des mots reconnus par l'OCR (Optical Character Recognition) et d'autre part la détection et la correction des erreurs d'ordre lexicales, morphologiques, syntaxiques (cas des erreurs d'accord) et qui sont dues à la non ou au mal reconnaissance de certains mots par l'OCR. Le couplage de notre système avec le système de reconnaissance de l'écriture arabe entre dans le cadre d'un projet de coopération avec l'équipe Perception, Système et Information (PSI) de l'université de Rouen.</resume>
			<mots_cles>analyse syntaxique, analyse syntaxique robuste, langue, langue arabe, corpus, système de reconnaissance, système de reconnaissance de l'écriture arabe</mots_cles>
			<title></title>
			<abstract></abstract>
			<keywords></keywords>
		</article>
		<article id="taln-2000-long-026" session="">
			<auteurs>
				<auteur>
					<nom>Frédéric Béchet</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Alexis Nasr</nom>
					<email>2</email>
					<affiliationId></affiliationId>
				</auteur>
				<auteur>
					<nom>Franck Genet</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIA - Université d’Avignon - BP1228 - Avignon Cedex 9</affiliation>
				<affiliation affiliationId="2">LIM - Université Aix-Marseille 2 - 163, avenue de Luminy - 13288 Marseille Cedex 9</affiliation>
			</affiliations>
			<titre>Enrichissement automatique de lexique de noms propres à partir de corpus</titre>
			<type>long</type>
			<pages></pages>
			<resume>Cet article présente une méthode d’étiquetage sémantique de noms propres fondé sur la technique des arbres de décision. Ces derniers permettent de modéliser les éléments saillants dans les contextes d’occurrence de noms propres d’une classe donnée. Les arbres de décision sont construits automatiquement sur un corpus d’apprentissage étiqueté, ils sont ensuite utilisés pour étiqueter des noms propres apparaissant dans un corpus de test. Les résultats de l’étiquetage du corpus de test est utilisé pour enrichir un lexique de noms propres. Ce dernier peut être utilisé à son tour pour réestimer les paramètres d’un étiqueteur stochastique. Nous nous intéressons en particulier au cas où le corpus de test a été glané sur le Web.</resume>
			<mots_cles>expression régulière, entrée lexicale, étiquetage, arbre, arbre de décision, corpus, corpus de test</mots_cles>
			<title></title>
			<abstract></abstract>
			<keywords></keywords>
		</article>
		<article id="taln-2000-long-027" session="">
			<auteurs>
				<auteur>
					<nom>Rodolfo Delmonte</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1"></affiliation>
				<affiliation affiliationId="2"></affiliation>
				<affiliation affiliationId="3"></affiliation>
			</affiliations>
			<titre></titre>
			<type>long</type>
			<pages></pages>
			<resume></resume>
			<mots_cles></mots_cles>
			<title>Parsing with Getarun</title>
			<abstract></abstract>
			<keywords></keywords>
		</article>
		<article id="taln-2000-long-028" session="">
			<auteurs>
				<auteur>
					<nom>Olivier Kraif</nom>
					<email>okraif@mageos.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LILLA, Université de Nice Sophia Antipolis, 98 Bd. E. Herriot BP 369 06007 Nice Cedex</affiliation>
			</affiliations>
			<titre>Extraction automatique de correspondances lexicales: évaluation d'indices et d'algorithmes</titre>
			<type>long</type>
			<pages></pages>
			<resume>Les bi-textes sont des corpus bilingues parallèles, généralement segmentés et alignés au niveau des phrases. Une des applications les plus directes de ces corpus consiste à en extraire automatiquement des correspondances lexicales, fournissant une information utile aux traducteurs, aux lexicographes comme aux terminologues. Comme pour l’alignement, des méthodes statistiques ont donné de bons résultats dans ce domaine. Nous pensons qu’une exploitation judicieuse d’indices statistiques adaptés et d’algorithmes de conception simple permet d’obtenir des correspondances fiables. Après avoir présenté les indices classiques, auxquels nous essayons d’apporter des améliorations, nous proposons dans cette article une étude empirique destinée à en montrer les potentialités.</resume>
			<mots_cles>extraction, extraction automatique de correspondances lexicales, alignement, alignement lexical, lexicographie, relation de traduction</mots_cles>
			<title></title>
			<abstract></abstract>
			<keywords></keywords>
		</article>
		<article id="taln-2000-long-029" session="">
			<auteurs>
				<auteur>
					<nom>Anne Vandeventer</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1"></affiliation>
				<affiliation affiliationId="2"></affiliation>
				<affiliation affiliationId="3"></affiliation>
			</affiliations>
			<titre>Diagnostic d'erreurs grammaticales par relâchement de contraintes dans le cadre de l'ELAO</titre>
			<type>long</type>
			<pages></pages>
			<resume></resume>
			<mots_cles></mots_cles>
			<title></title>
			<abstract></abstract>
			<keywords></keywords>
		</article>
		<article id="taln-2000-long-030" session="">
			<auteurs>
				<auteur>
					<nom>Christian Jacquemin</nom>
					<email>jacquemin@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Caroline Bush</nom>
					<email>caroline@limsi.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">CNRS-LIMSI, BP 133, F-91403 ORSAY Cedex, FRANCE</affiliation>
				<affiliation affiliationId="2">UMIST, Dept of Language Engineering, PO Box 88, Manchester M60 1QD, UK</affiliation>
			</affiliations>
			<titre>Fouille du Web pour la collecte d'Entités Nommées</titre>
			<type>long</type>
			<pages></pages>
			<resume>Cette étude porte sur l’acquisition des Entités Nommées (EN) à partir du Web. L’application présentée se compose d’un moissonneur de pages et de trois analyseurs surfaciques dédiés à des structures spécifiques. Deux évaluations sont proposées : une évaluation de la productivité des moteurs en fonction des types d’EN et une mesure de la précision.</resume>
			<mots_cles>entité nommée, expression régulière, acquisition lexicale, marqueur, marqueur discursif, moteur de recherche</mots_cles>
			<title></title>
			<abstract></abstract>
			<keywords></keywords>
		</article>
		<article id="taln-2000-long-031" session="">
			<auteurs>
				<auteur>
					<nom>Julie Carson-Berndsen</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Michael Walsh</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Department of Computer Science, University College Dublin, Ireland </affiliation>
			</affiliations>
			<titre></titre>
			<type>long</type>
			<pages></pages>
			<resume></resume>
			<mots_cles></mots_cles>
			<title>Generic Techniques for Multilingual Speech Technology Applications</title>
			<abstract>This paper is concerned with generic techniques for representing and evaluating phonological information in multilingual speech technology applications. A computational linguistic model of phonological interpretation is enhanced by a framework for constructing and evaluating phonotactic automata and by a generic lexicon model. The techniques make way for the extension of current speech technology to languages which have received little attention thus far.</abstract>
			<keywords></keywords>
		</article>
		<article id="taln-2000-long-032" session="">
			<auteurs>
				<auteur>
					<nom>Roberto Basili</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Alessandro Moschitti</nom>
					<email></email>
					<affiliationId></affiliationId>
				</auteur>
				<auteur>
					<nom>Maria Teresa Pazienza</nom>
					<email></email>
					<affiliationId></affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1"></affiliation>
				<affiliation affiliationId="2"></affiliation>
				<affiliation affiliationId="3"></affiliation>
			</affiliations>
			<titre></titre>
			<type>long</type>
			<pages></pages>
			<resume></resume>
			<mots_cles></mots_cles>
			<title>Modeling Terminological Information in Text Classification</title>
			<abstract></abstract>
			<keywords></keywords>
		</article>
		<article id="taln-2000-long-033" session="">
			<auteurs>
				<auteur>
					<nom>Mohamed-Zakaria Kurdi</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1"></affiliation>
				<affiliation affiliationId="2"></affiliation>
				<affiliation affiliationId="3"></affiliation>
			</affiliations>
			<titre>Une approche intégrée pour la normalisation des extragrammaticalités de la parole spontanée</titre>
			<type>long</type>
			<pages></pages>
			<resume>Dans cet article, nous présentons une nouvelle approche pour la normalisation des extragrammaticalités de la parole. La particularité de cène approche est l'intégration de différentes sources de connaissances de haut niveau, en particulier le lexique, la syntaxe et la sémantique. Ainsi, le traitement des extragrammaticalités se déroule suivant deux étapes : dans la première, le système normalise les Extragrammaticalités Lexicales (Eis) (hésitations, amalgames, etc.) et dans la deuxième, le système détecte et corrige les Extragrammaticalités Supra Lexicales (ESLs). Ce traitement est base sur des modèles de ESLs (règles et pattems) qui considèrent à la fois les informations syntaxiques et les informations structurales dans la détection et la correction des extragrammaticalités. De même, le système a été doté de patterns de contrôle ainsi que de grammaires sémantiques afin de réduire au maximum la surgénérativité. Les résultats de l'évaluation ont montré l'efficacité de notre approche à détecter et à corriger les extragrammaticalités tout en évitant les cas de surgénérativité.</resume>
			<mots_cles>parole, parole spontanée, extragrammaticalité lexicale, corpus, corpus d'apprentissage, information, information structurale</mots_cles>
			<title></title>
			<abstract></abstract>
			<keywords></keywords>
		</article>
		<article id="taln-2000-long-034" session="">
			<auteurs>
				<auteur>
					<nom>Terry Copeck</nom>
					<email>terry@site.uottawa.ca</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Ken Barker</nom>
					<email>kbarker@cs.utexas.edu</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Sylvain Delisle</nom>
					<email>Sylvain_Delisle@uqtr.uquebec.ca</email>
					<affiliationId>3</affiliationId>
				</auteur>
				<auteur>
					<nom>Stan Szpakowicz</nom>
					<email>szpak@site.uottawa.ca</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">School of Information Technology and Engineering University of Ottawa Ottawa, Ontario, Canada, K1N 6N5</affiliation>
				<affiliation affiliationId="2">Department of Computer Sciences University of Texas at Austin Austin, Texas, USA, 78712</affiliation>
				<affiliation affiliationId="3">Département de mathématiques et d’informatique Université du Québec à Trois-Rivières Trois-Rivières, Québec, Canada, G9A 5H7</affiliation>
			</affiliations>
			<titre></titre>
			<type>long</type>
			<pages></pages>
			<resume></resume>
			<mots_cles></mots_cles>
			<title>Automating the Measurement of Linguistic Features to Help Classify Texts as Technical</title>
			<abstract>Text classification plays a central role in software systems which perform automatic information classification and retrieval. Occurrences of linguistic feature values must be counted by any mechanism that classifies or characterizes natural language text by topic, style, genre or, in our case, by the degree to which a text is technical. We discuss the methodology and key details of the feature value extraction process, paying attention to fast and reliable implementation. Our results are mixed but support continued investigation— while a significant level of automation has been achieved, the successfully extracted feature counts do not always correlate with technicality as strongly as anticipated.</abstract>
			<keywords></keywords>
		</article>
		<article id="taln-2000-long-035" session="">
			<auteurs>
				<auteur>
					<nom>Carole Tiberius</nom>
					<email>Carole.Tiberius@itri.brighton.ac.uk</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Roger Evans</nom>
					<email>Roger.Evans@itri.brighton.ac.uk</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Information Technology Research Institute University of Brighton Brighton, UK</affiliation>
			</affiliations>
			<titre></titre>
			<type>long</type>
			<pages></pages>
			<resume></resume>
			<mots_cles></mots_cles>
			<title>Phonological feature based multilingual lexical description</title>
			<abstract>This paper presents a framework for compactly describing word forms in terms of phonological features. Using a highly modular default-inheritance based approach, the framework supports the description of lexical generalisations traditionally modelled as morphology and phonology in a single phonology-based representation. This representation is more uniform and more detailed than previous approaches of this kind, allowing us to capture generalisations within a language and between related language elegantly and flexibly. The framework is illustrated with examples taken from English, German, Dutch and Danish.</abstract>
			<keywords></keywords>
		</article>
		<article id="taln-2000-long-036" session="">
			<auteurs>
				<auteur>
					<nom>Olivier Ferret</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Brigitte Grau</nom>
					<email></email>
					<affiliationId></affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1"></affiliation>
				<affiliation affiliationId="2"></affiliation>
				<affiliation affiliationId="3"></affiliation>
			</affiliations>
			<titre>Une analyse thématique fondée sur un principe d'amorçage</titre>
			<type>long</type>
			<pages></pages>
			<resume>L'analyse thématique est une étape importante pour de nombreuses applications en traitement automatique des langues, telles que le résumé ou l'extraction d'information par exemple. Elle ne peut être réalisée avec une bonne précision qu'en exploitant une source de connaissances structurées sur les thèmes, laquelle est difficile à constituer à une large échelle. Dans cet article, nous proposons de résoudre ce problème par un principe d'amorçage : une première analyse thématique, fondée sur l'utilisation d'une source de connaissances faiblement structurée mais relativement aisée à construire, un réseau de collocations, permet d'apprendre des représentations explicites de thèmes, appelées signatures thématiques. Ces dernières sont ensuite utilisées pour mettre en oeuvre une seconde analyse thématique, plus précise et plus fiable.</resume>
			<mots_cles>analyse thématique, cohésion lexicale, focalisation, réseau de collocations</mots_cles>
			<title></title>
			<abstract></abstract>
			<keywords></keywords>
		</article>
		<article id="taln-2000-tutoriel-001" session="">
			<auteurs>
				<auteur>
					<nom>Piet Mertens</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">CCL, KU Leuven</affiliation>
			</affiliations>
			<titre>Intonation et syntaxe: traitement automatique pour la synthèse de la parole</titre>
			<type>tutoriel</type>
			<pages></pages>
			<resume>Le tutoriel se propose de décrire comment, en synthèse à partir du texte, on exploite les informations syntaxiques pour générer une intonation "naturelle" (entendez: grammaticale et variée). Un premier volet présentera l'intonation du français sous plusieurs aspects: ses formes auditive (mélodie, accentuation, pauses, rythme) et acoustique, l'analyse interactive par (re)synthèse, avec des éléments de perception tonale. Quelques exercices de discrimination, de transcription et de production, et l'illustration d'outils d'analyse permettront de concrétiser ces notions. On proposera enfin un modèle abstrait de l'intonation (niveaux de hauteur, accents, tons, unités) qui débouche sur une esquisse de ses fonctions communicatives et pragmatiques. Le deuxième volet établit le rapport avec la syntaxe: Est-il possible d'aller de la syntaxe à l'intonation ? Quelles informations sont requises: structure de constituants, rapports de dépendance, autres propriétés ? Qu'est-ce que nous apprennent les cas de non-congruence ? Qu'en est-il de l'autonomie de l'intonation (ou sa priorité) ? Le dernier volet est consacré au TALN pour la synthèse à partir du texte, plus particulièrement au système Mingus pour la génération de l'intonation. Celui-ci comporte deux blocs majeurs: d'une part la génération d'une représentation symbolique de l'intonation à partir de l'arborescence syntaxique et de la phonétisation; d'autre part, le modèle mélodique et le modèle de durée, qui effectuent la conversion de cette notation symbolique en valeurs acoustiques, nécessaires pour le synthétiseur (MBROLA, de la Faculté Polytechnique de Mons). Grâce au caractère paramétrique du module mélodique, il est possible de contrôler certains aspects émotifs. L'entrée à Mingus consiste soit de la sortie de l'analyseur syntaxique FIPS (LATL, Université de Genève), soit d'une analyse syntaxique superficielle propre. Le choix de la représentation de la structure syntaxique (et sa conversion éventuelle) dépend des contraintes explicitées dans le deuxième volet.</resume>
			<mots_cles></mots_cles>
			<title>Intonation and syntax: natural language processing for speech synthesis</title>
			<abstract>This tutorial describes how, in text-to-speech synthesis, syntactic information is used to generate natural intonation contours (i.e. which are grammatically correct and varied). A first part presents several aspect of French intonation: its auditory form (pitch, stress, pause, rhythm), its acoustic form, interactive analysis of intonation by (re)synthesis, as well as some notions of tonal perception. Some exercices in discrimination, transcription and production of pitch variations, and the illustration of analysis tools will help to provide a good understanding of these notions. Finally we briefly present an abstract model of French intonation (pitch levels, stress types, tones, units) and sketch its communicative and pragmatic functions. The second part studies the syntax-intonation interface: is it possible to predict intonation from syntactic structure ? What information is required: phrase structure, dependency relations, other properties ? What can be learned from cases of non-agreement ? Is intonation autonomous or anterior to syntax ? The last part deals with NLP for text-to-speech synthesis, more specifically in the Mingus system for intonation generation. There a two major blocks: first the generation of a symbolic representation of intonation, on the basis of the parse tree and grapheme-to-phoneme conversion; second the pitch model and the duration model which convert the symbolic representation into acoustic parameter values needed by the synthesizer (MBROLA, from Faculté Polytechnique de Mons). The parametric design of the pitch model allows for some control of emotional aspects. The input to Mingus is either the output of the full-blown syntactic parser FIPS (LATL, University of Geneva), or of Mingus' home-made shallow parser. The choice of syntactic representation (or its necessary conversion) stems from the constraints mentioned in the second part. </abstract>
			<keywords></keywords>
		</article>
		<article id="taln-2000-tutoriel-002" session="">
			<auteurs>
				<auteur>
					<nom>Afzal Ballim</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Dan Cristea</nom>
					<email></email>
					<affiliationId></affiliationId>
				</auteur>
				<auteur>
					<nom>Florian Seydoux</nom>
					<email></email>
					<affiliationId></affiliationId>
				</auteur>
				<auteur>
					<nom>Sebastian Moeller</nom>
					<email></email>
					<affiliationId></affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1"></affiliation>
				<affiliation affiliationId="2"></affiliation>
				<affiliation affiliationId="3"></affiliation>
			</affiliations>
			<titre></titre>
			<type>tutoriel</type>
			<pages></pages>
			<resume></resume>
			<mots_cles></mots_cles>
			<title>Practical Dialogue Management</title>
			<abstract>Dialogue modelling is a difficult but necessary task for intelligent human/machine comunication. This tutorial will look at the problems involved, and will introduce the basic concepts and methods in the development of dialogue management systems. The current state of the art in this field will be presented, and particular attention will be paid to semantic and pragmatic models of dialogue based on the intentions of participants. A hands-on session will be organized to present rapid dialogue prototyping environment.</abstract>
			<keywords></keywords>
		</article>
		<article id="taln-2000-poster-001" session="">
			<auteurs>
				<auteur>
					<nom>Hervé Blanchon</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Christian Boitet</nom>
					<email></email>
					<affiliationId></affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1"></affiliation>
				<affiliation affiliationId="2"></affiliation>
				<affiliation affiliationId="3"></affiliation>
			</affiliations>
			<titre>Traduction de la parole pour le français: une première étage et quelques perspectives</titre>
			<type>poster</type>
			<pages></pages>
			<resume></resume>
			<mots_cles></mots_cles>
			<title></title>
			<abstract></abstract>
			<keywords></keywords>
		</article>
		<article id="taln-2000-poster-002" session="">
			<auteurs>
				<auteur>
					<nom>Armelle Brun</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Kamel Smaïli</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Jean-Paul Haton</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">INRIA – CNRS : UMR7503 – Université Henri Poincaré - Nancy I – Université Nancy II – Institut National Polytechnique de Lorraine (INPL)</affiliation>
			</affiliations>
			<titre></titre>
			<type>poster</type>
			<pages></pages>
			<resume></resume>
			<mots_cles>anguage models, topic detection, tfidf, speech recognition, modeles de langage, détection de thèmes, reconnaissance de la parole</mots_cles>
			<title>Topic Identification Challenge Based on Short Word History</title>
			<abstract>This paper presents several methods for topic detection on newspaper articles based on either a general vocabulary or a set of topic vocabularies. Our topic detection methods will be applied to speech recognition framework. The originality and the difficulty of our work lies in the fact that both training and test corpora contain few words (less than 200 words for test corpora). Test corpora are very small because our objective is to identify topic and adapt the language model, after uttering only few words. Experiments show that beyond 60 words, topic detection methods are not reliable. On and after 80 words, topic detection rate reaches 82% for the two first hypotheses, which is promising due to the conditions of our experimentation.</abstract>
			<keywords></keywords>
		</article>
		<article id="taln-2000-poster-003" session="">
			<auteurs>
				<auteur>
					<nom>José Castano</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1"></affiliation>
				<affiliation affiliationId="2"></affiliation>
				<affiliation affiliationId="3"></affiliation>
			</affiliations>
			<titre></titre>
			<type>poster</type>
			<pages></pages>
			<resume></resume>
			<mots_cles></mots_cles>
			<title>Some considerations about parsing and the Minimalist program</title>
			<abstract></abstract>
			<keywords></keywords>
		</article>
		<article id="taln-2000-poster-004" session="">
			<auteurs>
				<auteur>
					<nom>Karim Chibout</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Anne Vilnat</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1"></affiliation>
				<affiliation affiliationId="2"></affiliation>
				<affiliation affiliationId="3"></affiliation>
			</affiliations>
			<titre>SCALP: un Système de Compréhension Automatique du Lexique Polysémique d'inspiration linguistique</titre>
			<type>poster</type>
			<pages></pages>
			<resume></resume>
			<mots_cles></mots_cles>
			<title></title>
			<abstract></abstract>
			<keywords></keywords>
		</article>
		<article id="taln-2000-poster-005" session="">
			<auteurs>
				<auteur>
					<nom>Bertrand Gaiffe</nom>
					<email>Bertrand.Gaiffe@loria.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Anne Reboul</nom>
					<email></email>
					<affiliationId></affiliationId>
				</auteur>
				<auteur>
					<nom>Jean-Marie Pierrel</nom>
					<email></email>
					<affiliationId></affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1"></affiliation>
				<affiliation affiliationId="2"></affiliation>
				<affiliation affiliationId="3"></affiliation>
			</affiliations>
			<titre>Le traitement des déictiques dans le DOHM</titre>
			<type>poster</type>
			<pages></pages>
			<resume>Dans ce papier, nous proposons une analyse logique des déictiques. Notre travail s'effectue dans le cadre du dialogue de commande homme-machine. Dans un premier temps, nous présentons ce cadre et justifions la nécessité de représenter le contenu propositionnel des énoncé. Dans un second temps, nous illustrons la différence entre forme logique et contenu propositionnel ; cette différence est particulièrement importante dans le cas du traitement des déictiques. Nous montrons ensuite qu'au delà de l'intuition qui fait dire que " je " réfère à celui qui parle, il est nécessaire pour le traitement des déictiques de faire la différence entre l'acte locutionnaire (dans lequel le référent des déictiques doit être recherché) et l'acte illocutionnaire sous le champ duquel on ne peut voir apparaître que le référent des déictiques et pas leur sens.</resume>
			<mots_cles></mots_cles>
			<title></title>
			<abstract></abstract>
			<keywords>man-machine dialogue, logical form, speech acts, dohm, déictiques, forme logique, actes de langage</keywords>
		</article>
		<article id="taln-2000-poster-006" session="">
			<auteurs>
				<auteur>
					<nom>Françoise Gayral</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Daniel Kayser</nom>
					<email></email>
					<affiliationId></affiliationId>
				</auteur>
				<auteur>
					<nom>François Levy</nom>
					<email></email>
					<affiliationId></affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1"></affiliation>
				<affiliation affiliationId="2"></affiliation>
				<affiliation affiliationId="3"></affiliation>
			</affiliations>
			<titre>Analyse sémantique des pluriels : une première approche</titre>
			<type>poster</type>
			<pages></pages>
			<resume></resume>
			<mots_cles></mots_cles>
			<title></title>
			<abstract></abstract>
			<keywords></keywords>
		</article>
		<article id="taln-2000-poster-007" session="">
			<auteurs>
				<auteur>
					<nom>Jean-Philippe Goldman</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Eric Wehrli</nom>
					<email></email>
					<affiliationId></affiliationId>
				</auteur>
				<auteur>
					<nom>Arnaud Gaudinat</nom>
					<email></email>
					<affiliationId></affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1"></affiliation>
				<affiliation affiliationId="2"></affiliation>
				<affiliation affiliationId="3"></affiliation>
			</affiliations>
			<titre>Utilisation de l'analyse syntaxique pour la synthèse de la parole, l'enseignement des langues et l'étiquetage grammatical</titre>
			<type>poster</type>
			<pages></pages>
			<resume></resume>
			<mots_cles></mots_cles>
			<title></title>
			<abstract></abstract>
			<keywords></keywords>
		</article>
		<article id="taln-2000-poster-008" session="">
			<auteurs>
				<auteur>
					<nom>Andreas Herzig</nom>
					<email>herzig@irit.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Dominique Longin</nom>
					<email>longin@irit.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Institut de Recherche en Informatique de Toulouse (IRIT) (CNRS – Université Paul Sabatier – INPT) 118 Route de Narbonne, F-31062 Toulouse Cedex 4</affiliation>
			</affiliations>
			<titre></titre>
			<type>poster</type>
			<pages></pages>
			<resume></resume>
			<mots_cles></mots_cles>
			<title>A topic-based framework for rational interaction</title>
			<abstract>Cooperative dialogue is one of the most important challenges of computer science. The background of this work are the dialogue systems developed by France Telecom R&amp; D as instantiations of its ARTIMIS (Sadek, 1999; Sadek et al., 1997; Sadek et al., 1996) generic rational agent technology. Such systems allow to manage real-time cooperative dialogues in natural language. Our framework is what we call an “intentional approach” of dialogue (Cohen &amp; Levesque, 1990a; Sadek, 1991; Sadek, 1992; Rao &amp; Georgeff, 1992). This approach is based on theories of Intentionality (Searle, 1983; Bratman, 1987). Within these theories, an agent is represented by its “mental state”, which is a set of informations. This set contains the different mental attitudes about the world the agent has: beliefs, goals, intentions... These theories are at the base of what is called “BDI-architectures” (for belief, desire and intention) in the literature. Intentional approaches are defined within twofaced formal theories : rational balance and rational interaction. The first theory describes, through properties of mental attitudes and action, the relationships that must be maintained as true (the relationships between the different mental attitudes of an agent firstly, and between these mental attitudes, plans and actions secondly). The second theory characterizes the inter-agent relationship within a multiagent environment (communication, cooperation, ...). Agents built on these twofaced theories are called rational agents. In this paper, we focus on the belief change process, viz. the ability to take into account the dynamics of the world.</abstract>
			<keywords></keywords>
		</article>
		<article id="taln-2000-poster-009" session="">
			<auteurs>
				<auteur>
					<nom>Evelyne Jacquey</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Bertrand Gaiffe</nom>
					<email></email>
					<affiliationId></affiliationId>
				</auteur>
				<auteur>
					<nom>Jean-Marie Pierrel</nom>
					<email></email>
					<affiliationId></affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1"></affiliation>
				<affiliation affiliationId="2"></affiliation>
				<affiliation affiliationId="3"></affiliation>
			</affiliations>
			<titre>Un traitement sémantique de la polysémie lexicale dans le domaine des interfaces homme-machine de dialogues de commandes</titre>
			<type>poster</type>
			<pages></pages>
			<resume>L'objet de cet article est de proposer un traitement lexical des noms polysemiques qui admettent des copredications avec ou sans reprises pronominales. Ces copredications peuvent combiner des predicats vehiculant des restrictions selectionnelles differentes et incompatibles a premiere vue. Ces donnees semblent donc remettre en cause l'hypothese qu'une copredication, par le biais d'une coordination ou d'une reprise pronominale, ne modifie pas le type de l'element qui subit la copredication. Parmi un certain nombre de travaux existants dans ce domaine (Godard and Jayez 96) et (Pustejovsky 94,95) entre autre, (Asher et Pustejovsky 2000) proposent un traitement qui verifie l'hypothese d'un typage constant des arguments par leur predicat sans pour autant interdire ces phenomenes de copredication. Nous montrerons que ces travaux constituent une alternative a l'hypothese de (Godard and Jayez 96). De plus, dans la perspective de l'elaboration d'une interface de dialogue de commandes pour guider les taches accomplies par une application preexistante, nous montrerons que le traitement propose par (Asher and Pustejovsky 2000) fournit des representations semantiques adaptables a deux types possibles de modeles conceptuels.</resume>
			<mots_cles></mots_cles>
			<title></title>
			<abstract></abstract>
			<keywords>lexical semantics, formal semantics, polysemy, typing, sémantique lexicale, sémantique formelle, polysémie, typage</keywords>
		</article>
		<article id="taln-2000-poster-010" session="">
			<auteurs>
				<auteur>
					<nom>Thomas Lebardé</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1"></affiliation>
				<affiliation affiliationId="2"></affiliation>
				<affiliation affiliationId="3"></affiliation>
			</affiliations>
			<titre>Analyse Syntaxique par Segments Logiques</titre>
			<type>poster</type>
			<pages>447­-453</pages>
			<resume></resume>
			<mots_cles></mots_cles>
			<title></title>
			<abstract></abstract>
			<keywords></keywords>
		</article>
		<article id="taln-2000-poster-011" session="">
			<auteurs>
				<auteur>
					<nom>Jacques Menezo</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1"></affiliation>
				<affiliation affiliationId="2"></affiliation>
				<affiliation affiliationId="3"></affiliation>
			</affiliations>
			<titre>CELINE, un système multi-agents de détection-correction d'erreurs, adaptatif et semi-automatique</titre>
			<type>poster</type>
			<pages></pages>
			<resume></resume>
			<mots_cles></mots_cles>
			<title></title>
			<abstract></abstract>
			<keywords></keywords>
		</article>
		<article id="taln-2000-poster-012" session="">
			<auteurs>
				<auteur>
					<nom>Victoria Panchuk</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1"></affiliation>
				<affiliation affiliationId="2"></affiliation>
				<affiliation affiliationId="3"></affiliation>
			</affiliations>
			<titre></titre>
			<type>poster</type>
			<pages></pages>
			<resume></resume>
			<mots_cles></mots_cles>
			<title>Computer Lexicography in Ukraine: an Overview</title>
			<abstract></abstract>
			<keywords></keywords>
		</article>
		<article id="taln-2000-poster-013" session="">
			<auteurs>
				<auteur>
					<nom>Patrick Ruch</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Arnaud Gaudinat</nom>
					<email></email>
					<affiliationId></affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1"></affiliation>
				<affiliation affiliationId="2"></affiliation>
				<affiliation affiliationId="3"></affiliation>
			</affiliations>
			<titre>De l'ambiguïté lexicale selon le domaine</titre>
			<type>poster</type>
			<pages></pages>
			<resume></resume>
			<mots_cles></mots_cles>
			<title></title>
			<abstract></abstract>
			<keywords></keywords>
		</article>
		<article id="taln-2000-poster-014" session="">
			<auteurs>
				<auteur>
					<nom>Tassadit Amghar</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Bernard Levrat</nom>
					<email></email>
					<affiliationId></affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1"></affiliation>
				<affiliation affiliationId="2"></affiliation>
				<affiliation affiliationId="3"></affiliation>
			</affiliations>
			<titre>Modification et métonymie dans les Graphes Conceptuels</titre>
			<type>poster</type>
			<pages></pages>
			<resume></resume>
			<mots_cles></mots_cles>
			<title></title>
			<abstract></abstract>
			<keywords></keywords>
		</article>
		<article id="taln-2000-demo-001" session="">
			<auteurs>
				<auteur>
					<nom>Lionel Clement</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1"></affiliation>
				<affiliation affiliationId="2"></affiliation>
				<affiliation affiliationId="3"></affiliation>
			</affiliations>
			<titre>Une plateforme de développement de Grammaires Lexicales Fonctionnelles</titre>
			<type>démonstration</type>
			<pages></pages>
			<resume></resume>
			<mots_cles></mots_cles>
			<title></title>
			<abstract></abstract>
			<keywords></keywords>
		</article>
		<article id="taln-2000-demo-002" session="">
			<auteurs>
				<auteur>
					<nom>Jacques Menezo</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Marie-Hélène Stefanini</nom>
					<email></email>
					<affiliationId></affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1"></affiliation>
				<affiliation affiliationId="2"></affiliation>
				<affiliation affiliationId="3"></affiliation>
			</affiliations>
			<titre>BRUTAL, une plate-forme générique pour le TAL</titre>
			<type>démonstration</type>
			<pages>269-272</pages>
			<resume></resume>
			<mots_cles></mots_cles>
			<title></title>
			<abstract></abstract>
			<keywords></keywords>
		</article>
	</articles>
</conference>