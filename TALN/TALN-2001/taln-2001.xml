<?xml version="1.0" encoding="UTF-8"?>
<!-- Fichiers ocrisés : taln-2001-invite-002, taln-2001-long-003, taln-2001-long-007, taln-2001-long-010, taln-2001-long-012, taln-2001-long-016, taln-2001-long-025, taln-2001-long-026, taln-2001-long-027, taln-2001-poster-002, taln-2001-poster-003, taln-2001-poster-005, taln-2001-poster-008, taln-2001-poster-009, taln-2001-poster-012, taln-2001-tutoriel-001, taln-2001-tutoriel-002, taln-2001-tutoriel-005, taln-2001-tutoriel-006 -->
<conference>
	<edition>
		<acronyme>TALN'2001</acronyme>
		<titre>8ème conférence sur le Traitement Automatique des Langues Naturelles</titre>
		<ville>Tours</ville>
		<pays>France</pays>
		<dateDebut>2001-07-02</dateDebut>
		<dateFin>>2001-07-05</dateFin>
		<presidents>
			<nom>Denis Maurel</nom>
		</presidents>
		<typeArticles>
			<type id="invite">Conférences invitées</type>
			<type id="long">Papiers longs</type>
			<type id="poster">Posters</type>
			<type id="tutoriel">Turoriels</type>
		</typeArticles>
		<statistiques>
<!-- 			<acceptations id="long" soumissions=""></acceptations>
			<acceptations id="poster" soumissions=""></acceptations> -->
		</statistiques>
		<siteWeb>http://tln.li.univ-tours.fr/Tln_Colloques/TALN2001-RECITAL2001/</siteWeb>
		<meilleurArticle>
			<!-- <articleId></articleId> -->
		</meilleurArticle>
	</edition>
	<articles>
		<article id="taln-2001-invite-001" session="">
			<auteurs>
				<auteur>
					<nom>Mehryar Mohri</nom>
					<email>mohri@research.att.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">AT&amp;T Labs - Research 180 Park Avenue Florham Park, NJ 07932-0971, USA</affiliation>
			</affiliations>
			<titre></titre>
			<type>invite</type>
			<pages>5-14</pages>
			<resume>Les automates et transducteurs pondérés sont utilisés dans un éventail d’applications allant de la reconnaissance et synthèse automatiques de la langue à la biologie informatique. Ils fournissent un cadre commun pour la représentation des composants d’un système complexe, ce qui rend possible l’application d’algorithmes d’optimisation généraux tels que la déterminisation, l’élimination des mots vides, et la minimisation des transducteurs pondérés. Nous donnerons un bref aperçu des progrès récents dans le traitement de la langue à l’aide d’automates et transducteurs pondérés, y compris une vue d’ensemble de la reconnaissance de la parole avec des transducteurs pondérés et des résultats algorithmiques récents dans ce domaine. Nous présenterons également de nouveaux résultats liés à l’approximation des grammaires context-free pondérées et à la reconnaissance à l’aide d’automates pondérés.</resume>
			<mots_cles></mots_cles>
			<title>Language Processing with Weighted Transducers</title>
			<abstract>Weighted automata and transducers are used in a variety of applications ranging from automatic speech recognition and synthesis to computational biology. They give a unifying framework for the representation of the components of complex systems. This provides opportunities for the application of general optimization algorithms such as determinization, epsilon-removal and minimization of weighted transducers. We give a brief survey of recent advances in language processing with weighted automata and transducers, including an overview of speech recognition with weighted transducers and recent algorithmic results in that field. We also present new results related to the approximation of weighted context-free grammars and language recognition with weighted automata.</abstract>
			<keywords>automatic speech recognition, weighted finite-state transducers, weighted automata, context-free grammars, regular approximation of CFGs, rational power series</keywords>
		</article>
		<article id="taln-2001-invite-002" session="">
			<auteurs>
				<auteur>
					<nom>Jacques Vergne</nom>
					<email>Jacques.Vergne@info.unicaen.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">GREYC - Université de Caen BP 5186 - 14032 Caen cedex</affiliation>
			</affiliations>
			<titre>Analyse syntaxique automatique de langues du combinatoire au calculatoire</titre>
			<type>invite</type>
			<pages>15-29</pages>
			<resume>Nous proposons de montrer comment l'analyse syntaxique automatique est aujourd'hui à un tournant de son évolution, en mettant l'accent sur l'évolution des modèles d'analyse syntaxique : de l'analyse de langages de programmation (compilation) à l'analyse de langues, et, dans le cadre de l'analyse de langues, de l'analyse combinatoire à l'analyse calculatoire, en passant par le tagging et le chunking (synthèse en section 4). On marquera d'abord le poids historique des grammaires formelles, comme outil de modélisation des langues et des langages formels (section 1), et comment la compilation a été transposée en traduction automatique par Bernard Vauquois. On analysera ensuite pourquoi il n'a pas été possible d'obtenir en analyse de langue un fonctionnement analogue à la compilation, et pourquoi la complexité linéaire de la compilation n'a pas pu être transposée en analyse syntaxique (section 2). Les codes analysés étant fondamentalement différents, et le tagging ayant montré la voie, nous en avons pris acte en abandonnant la compilation transposée : plus de dictionnaire exhaustif en entrée, plus de grammaire formelle pour modéliser les structures linguistiques (section 3). Nous montrerons comment, dans nos analyseurs, nous avons implémenté une solution calculatoire, de complexité linéaire (section 5). Nous conclurons (section 6) en pointant quelques évolutions des tâches de l'analyse syntaxique.</resume>
			<mots_cles></mots_cles>
			<title></title>
			<abstract>In this paper, we intend to show how automatic parsing is today at a change of direction. We will stress the evolution of parsing models : from programming language parsing (compilation) to natural language parsing, and, within the frame of natural language parsing, from combinatory parsing to calculatory parsing, while going through tagging and chunking (synthesis in section 4). First we stress the historical weight of formal grammars, as a modelling tool for natural languages and formal languages (section 1), and how compilation has been transposed into machine translation by Bernard Vauquois. Then we analyse why it was not possible to get a natural language parsing which works the same way as compilation, and why the linear complexity of compilation could not be transposed into NL parsing (section 2). Since parsed languages are radically different, and since tagging showed the right way, we decided to abandon the transposed compilation : no more exhaustive dictionary as input, no more formal grammar to model linguistic structures (section 3). We will show how, in our parsers, we implemented a calculatory solution, of linear complexity (section 5). We conclude (section 6), stressing some trends of parsing about its tasks.</abstract>
			<keywords></keywords>
		</article>
		<article id="taln-2001-long-001" session="">
			<auteurs>
				<auteur>
					<nom>Anne Abeillé</nom>
					<email>abeille@linguist.jussieu.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Lionel Clément</nom>
					<email>clement@linguist.jussieu.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Alexandra Kinyon</nom>
					<email>kinyon@linguist.jussieu.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>François Toussenel</nom>
					<email>ftoussen@linguist.jussieu.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">UFRL, Université Paris 7</affiliation>
				<affiliation affiliationId="2">IRCS - Upenn</affiliation>
			</affiliations>
			<titre>Un corpus français arboré : quelques interrogations</titre>
			<type>long</type>
			<pages>33-42</pages>
			<resume>Dans cet article nous présentons les premiers résultats de l’exploitation d’un Corpus français arboré (Abeillé et al., 2001). Le corpus comprend 1 million de mots entièrement annotés et validé pour les parties du discours, la morphologie, les mots composés et les lemmes, et partiellement annotés pour les constituants syntaxiques. Il comprend des extraits de journaux parus entre 1989 et 1993 et écrits par divers auteurs, et couvre différents thèmes (économie, littérature, politique, etc.). Après avoir expliqué comment ce corpus a été construit, et comment l’exploiter à l’aide d’un outil de recherche spécifique, nous exposerons quelques résultats linguistiques concernant les fréquences et les préférences lexicales et syntaxiques. Nous expliquerons pourquoi nous pensons que certains de ces résultats sont pertinents en linguistique théorique et en psycholinguistique.</resume>
			<mots_cles>Corpus arboré, corpus journalistique, français, syntaxe</mots_cles>
			<title></title>
			<abstract>This paper presents the first linguistic results exploiting a new treebank for French (Abeillé et al., 2001). The corpus comprises 1 million words fully annotated and disambiguated for parts of speech, inflectional morphology, compounds and lemmas, and partially annotated with syntactic constituents. It is made of extracts from newspapers ranging from 1989 to 1993 and written by different authors, and covers a variety of subjects (economy, literature, politics, etc.). After explaining how this corpus was built, and how it can be used with a specific search tool, we present some linguistic results obtained when searching the corpus for lexical or syntactic frequencies and preferences, and explain why we think some of these results are relevant both for theoretical linguistics and psycholinguistics.</abstract>
			<keywords>Treebank, French, Newspaper corpora, syntax</keywords>
		</article>
		<article id="taln-2001-long-002" session="">
			<auteurs>
				<auteur>
					<nom>Tassadit Amghar</nom>
					<email>Tassadit.Amghar@univ-angers.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Delphine Battistelli</nom>
					<email>Delphine.Battistelli@paris4.sorbonne.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Thierry Charnois</nom>
					<email>Thierry.Charnois@lipn.univ-paris13.fr</email>
					<affiliationId>3</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LERIA, Université d’Angers, 2 bd Lavoisier, 49045 Angers Cedex 01</affiliation>
				<affiliation affiliationId="2">Equipe LaLic, Université Paris-Sorbonne, 96 bd Raspail, 75006 Paris</affiliation>
				<affiliation affiliationId="3">LIPN, Université Paris 13, Av. J.-B. Clément, 93430 Villetaneuse</affiliation>
			</affiliations>
			<titre>Représenter le temps en langue dans le formalisme des graphes conceptuels une approche basée sur les schèmes sémantico-cognitifs</titre>
			<type>long</type>
			<pages>43-52</pages>
			<resume>L’objectif de notre travail est de construire une représentation sémantique d’un corpus de textes français au sein des graphes conceptuels simples. Notre conceptualisation est fondée sur les Schèmes Sémantico-Cognitifs et la théorie aspecto-temporelle introduits par J. P. Desclés. Un texte est représenté par deux structures. La première modélise la représention semanticocognitive des propositions du texte, et la seconde le diagramme temporel exprimant les contraintes temporelles entre les différentes situations décrites dans le texte. La prise en compte de ces deux structures et des liens qu’elles entretiennent nous a amenés à modifier le modèle des graphes conceptuels simples et à envisager les modes d’interaction entre temps, aspect (grammatical) et significations des lexèmes verbaux.</resume>
			<mots_cles>Temps linguistique, Valeurs Aspectuelles, Schèmes Sémantico-Cognitifs, Graphes Conceptuels</mots_cles>
			<title></title>
			<abstract>We propose here a system which deals with time, aspect and verbal meanings in natural language processing within Simple Conceptual Graphs on the basis of the Semantico-Cognitive Schemes and the aspecto-temporal theory both introduced by J-P. Desclés. Our work bears on French texts. A text is represented by two different structures both represented within the SCG model. The first one models the semantico-cognitive representation while the second one is the temporal diagram representing the temporal constraints between the situations described in the text. Linking these structures leads us to slightly extend the original SCG model.</abstract>
			<keywords>Linguistic time, Aspectual values, Semantico-Cognitive Schemes, Conceptual graphs</keywords>
		</article>
		<article id="taln-2001-long-003" session="">
			<auteurs>
				<auteur>
					<nom>Nicolas Auclerc</nom>
					<email>nicolas.auclerc@slt.atr.co.jp</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Yves Lepage</nom>
					<email>yves.lepage@slt.atr.co.jp</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1"></affiliation>
				<affiliation affiliationId="2"></affiliation>
				<affiliation affiliationId="3"></affiliation>
			</affiliations>
			<titre>Aides à l'analyse pour la construction de banque d’arbres : étude de l'effort</titre>
			<type>long</type>
			<pages>53-62</pages>
			<resume>La construction de banque d’arbres est une entreprise lourde qui prend du temps. Pour faciliter cette construction, nous voyons la construction de banques d’arbres comme une série d’opérations d’édition et de recherche. Le but de cet article est d’estimer l’effort, en nombre d’opérations d’éditions, nécessaire pour ajouter une nouvelle phrase dans la banque d’arbres. Nous avons proposé un outil, Boardedit, qui inclut un éditeur d’arbres et des aides a l’analyse. Comme l’effort nécessaire dépend bien sûr de la qualité des réponses fournies par les aides a l’analyse, il peut être vue comme une mesure de la qualité de ces aides. L’éditeur d’arbres restant indispensable a notre outil pendant l’eXpérience, les aides a l’analyse seront donc toujours associées a l’éditeur d’arbres. Dans l’eXpérience proposée, nous augmentons une banque d’arbres de 5 000 phrases par l 553 nouvelles phrases. La réduction obtenue est supérieure auX 4/5 de l’effort.</resume>
			<mots_cles>Banque d’arbres, analogie, filtrage tolérant, éditeur d’arbres, mesure de l’effort</mots_cles>
			<title></title>
			<abstract>The construction of a treebank is a Very cumbersome and time-comsuming process. To speed up this process, we see the process of building a treebank as a sequence of edition and search operations. Our purpose is to assess the effort, measured by operations (cliks and keystrokes), needed to augment a treebank. We have proposed a tool, Boardedit, which incorporates a tree editor and parsing aids. However, the effort needed will depend on the quality of the answer of the parsing aids, this can be seen as a measure of the quality of the parsing aids. Our tree editor is essential for our tools during this experiment, parsing aids will be always used with the tree editor. In the experiment, we augment a tree-bank of 5 000 sentences with 1 553 new sentences. We show that the reduction in the number of operations is more than 4/5 of the effort.</abstract>
			<keywords>Treebank, analogy, approximate matching, tree editor, tree-banking assessment</keywords>
		</article>
		<article id="taln-2001-long-004" session="">
			<auteurs>
				<auteur>
					<nom>François Barthélemy</nom>
					<email>barthe@cnam.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Pierre Boullier</nom>
					<email></email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Philippe Deschamp</nom>
					<email></email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Linda Kaouane</nom>
					<email></email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Éric Villemonte De La Clergerie</nom>
					<email>Eric.De_La_Clergerie@inria.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">CEDRIC - CNAM, 92 Rue St Martin - FR-75141 Paris Cedex 03</affiliation>
				<affiliation affiliationId="2">ATOLL - INRIA, Domaine de Voluceau - BP 105 - 78153 Le Chesnay Cedex</affiliation>
			</affiliations>
			<titre>Atelier ATOLL pour les grammaires d’arbres adjoints</titre>
			<type>long</type>
			<pages>63-72</pages>
			<resume>Cet article présente l’environnement de travail que nous développons au sein de l’équipe ATOLL pour les grammaires d’arbres adjoints. Cet environnement comprend plusieurs outils et ressources fondés sur l’emploi du langage de balisage XML. Ce langage facilite la mise en forme et l’échange de ressources linguistiques.</resume>
			<mots_cles>TAG, XML, Ressources linguistiques</mots_cles>
			<title></title>
			<abstract>This paper presents the ATOLL workbench for Tree Adjoining Grammars. This workbench provides several tools and resources based on the use of the markup language XML which eases the construction and the exchange of linguistic resources.</abstract>
			<keywords></keywords>
		</article>
		<article id="taln-2001-long-005" session="">
			<auteurs>
				<auteur>
					<nom>Slim Ben Hazez</nom>
					<email>Slim.Ben-Hazez@paris4.sorbonne.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Jean-Pierre Desclés</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Jean-Luc Minel</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">CAMS-LaLIC, UMR 8557 du CNRS, EHESS, Université de Paris-Sorbonne 96 boulevard Raspail 75 006 Paris – France</affiliation>
			</affiliations>
			<titre>Modèle d’exploration contextuelle pour l’analyse sémantique de textes</titre>
			<type>long</type>
			<pages>73-82</pages>
			<resume>Nous présentons dans cet article un modèle d’exploration contextuelle et une plate-forme logicielle qui permet d’accéder au contenu sémantique des textes et d’en extraire des séquences particulièrement pertinentes. L'objectif est de développer et d’exploiter des ressources linguistiques pour identifier dans les textes, indépendamment des domaines traités, certaines des relations organisatrices des connaissances ainsi que les organisations discursives mises en places par l'auteur. L'analyse sémantique du texte est guidée par le repérage d'indices linguistiques déclencheurs dont l'emploi est représentatif des notions étudiées.</resume>
			<mots_cles></mots_cles>
			<title></title>
			<abstract>In this paper, we present a model of contextual exploration and a workstation dedicated to semantic filtering and relevant sentence extracting. The purpose is to develop and to exploit linguistics resources in order to identify in texts, independently of processed domains, some specific relations which organize knowledge and author discourse. Semantic analysis is driven by the identification of linguistic indicators which are relevant clues for the studied notions.</abstract>
			<keywords></keywords>
		</article>
		<article id="taln-2001-long-006" session="">
			<auteurs>
				<auteur>
					<nom>Romaric Besançon</nom>
					<email>Romaric.Besancon@epfl.ch</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Antoine Rozenknop</nom>
					<email>Antoine.Rozenknop@epfl.ch</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Jean-Cédric Chappelier</nom>
					<email>Jean-Cedric.Chappelier@epfl.ch</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Martin Rajman</nom>
					<email>Martin.Rajman@epfl.ch</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Laboratoire d’Intelligence Artificielle, Département Informatique École Polytechnique Fédérale de Lausanne</affiliation>
			</affiliations>
			<titre>Intégration probabiliste de sens dans la représentation de textes</titre>
			<type>long</type>
			<pages>83-91</pages>
			<resume>Le sujet du présent article est l’intégration des sens portés par les mots en contexte dans une représentation vectorielle de textes, au moyen d’un modèle probabiliste. La représentation vectorielle considérée est le modèle DSIR, qui étend le modèle vectoriel (VS) standard en tenant compte à la fois des occurrences et des co-occurrences de mots dans les documents. L’intégration des sens dans cette représentation se fait à l’aide d’un modèle de Champ de Markov avec variables cachées, en utilisant une information sémantique dérivée de relations de synonymie extraites d’un dictionnaire de synonymes.</resume>
			<mots_cles>Désambiguïsation, Sémantique Distributionnelle, Représentation Vectorielle, Recherche Documentaire, Champs de Markov, algorithme EM</mots_cles>
			<title></title>
			<abstract>The present contribution focuses on the integration of word senses in a vector representation of texts, using a probabilistic model. The vector representation under consideration is the DSIR model, that extends the standard Vector Space (VS) model by taking into account both occurrences and co-occurrences of words. The integration of word senses into the co-occurrence model is done using a Markov Random Field model with hidden variables, using semantic information derived from synonymy relations extracted from a synonym dictionary.</abstract>
			<keywords>Word Sense Disambiguation, Distributional Semantics, Vector Space Representation, Information Retrieval, Markov Random Fields, EM algorithm</keywords>
		</article>
		<article id="taln-2001-long-007" session="">
			<auteurs>
				<auteur>
					<nom>Ismaïl Biskri</nom>
					<email>ismail_biskri@uqtr.uquebec.ca</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Sylvain Delisle</nom>
					<email>sylvain_delisle@uqtr.uquebec.ca</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université du Québec a Trois Rivières Département de mathématiques et d’informatique C.P. 500, Trois-Rivières, Québec, Canada, G9A 5H7</affiliation>
			</affiliations>
			<titre>Les n-grams de caractères pour l'aide à l’extraction de connaissances dans des bases de données textuelles multilingues</titre>
			<type>long</type>
			<pages>93-102</pages>
			<resume>Une véritable classification numérique multilingue est impossible si on considère seulement le mot comme unité d’information privilégiée. En traitant les mots comme jetons, la tokenisation s’avère relativement simple pour le français et l’anglais, mais très difficile pour des langues comme l’allemand ou l’arabe. D’autre part, la lemmatisation utilisée comme moyen de normalisation et de réduction du lexique constitue un écueil non moins négligeable. La notion de n-grams, qui depuis une décennie donne de bons résultats dans Pidentification de la langue ou dans l’analyse de l’oral, est, par les recherches récentes, devenue un axe privilégié dans l’acquisition et l’extraction des connaissances dans les textes. Dans cet article, nous présenterons un outil de classification numérique basé sur le concept de n-grams de caractères. Nous évaluons aussi les résultats de cet outil que nous comparons à des résultats obtenus au moyen d’une classification fondée sur des mots.</resume>
			<mots_cles>classification numérique de textes, n-grams, multilinguisme</mots_cles>
			<title></title>
			<abstract>Real multilingual numerical classification is impossible if only words are treated as the privileged unit of information. Although it makes tokenisation (in which words are considered as tokens) relatively easy in English or French, it makes it much more difficult for other languages such as German or Arabic. Moreover, lemmatisation, typically used to normalise and reduce the size of the lexicon, poses another challenge. The notion of n-grams which, for the last ten years, seems to have produced good results both in language identification and speech analysis, has recently become a privileged research axis in several areas of knowledge acquisition and extraction from text. In this paper, we present a text classification tool based on n-grams of characters and evaluate its results and compare them with those obtained from a different classification tool based solely on the processing of words.</abstract>
			<keywords></keywords>
		</article>
		<article id="taln-2001-long-008" session="">
			<auteurs>
				<auteur>
					<nom>Philippe Blache</nom>
					<email>pb@lpl.univ-aix.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LPL-CNRS, Université de Provence 29 Avenue Robert Schuman 13621 Aix-en-Provence, France</affiliation>
			</affiliations>
			<titre>Dépendances à distance dans les grammaires de propriétés : l’exemple des disloquées</titre>
			<type>long</type>
			<pages>103-112</pages>
			<resume>Cet article propose une description des dépendances à distances s’appuyant sur une approche totalement déclarative, les grammaires de propriétés, décrivant l’information linguistique sous la forme de contraintes. L’approche décrite ici consiste à introduire de façon dynamique en cours d’analyse de nouvelles contraintes, appelées propriétés distantes. Cette notion est illustrée par la description du phénomène des disloquées en français.</resume>
			<mots_cles></mots_cles>
			<title></title>
			<abstract></abstract>
			<keywords></keywords>
		</article>
		<article id="taln-2001-long-009" session="">
			<auteurs>
				<auteur>
					<nom>Béatrice Bouchou</nom>
					<email>bouchou@univ-tours.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Julien Lerat</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Denis Maurel</nom>
					<email>maurel@univ-tours.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LI, Université François Rabelais E3i, 64 avenue Jean Portalis, 37200 Tours</affiliation>
				<affiliation affiliationId="2"></affiliation>
				<affiliation affiliationId="3"></affiliation>
			</affiliations>
			<titre>L'interrogation de bases de données comme application des classes d'objets</titre>
			<type>long</type>
			<pages>113-122</pages>
			<resume>En travaillant sur l'interrogation de bases de données en langue naturelle, nous sommes amenés à exploiter les propositions du Laboratoire de Linguistique Informatique (LLI) en matière de représentation de la langue : les classes d'objets. Un outil d'interrogation définit une application du langage vers le modèle de l'information stockée. Ici les classes d'objets et leurs prédicats appropriés modélisent le langage source, tandis que le modèle relationnel sert pour les données interrogées. Nous présentons d'abord ce contexte d'application, puis comment nous utilisons les classes d'objets et prédicats appropriés dans ce cadre.</resume>
			<mots_cles>interrogation de BD en langage naturel, modèle relationnel, classes d'objets</mots_cles>
			<title></title>
			<abstract>We investigate how to use natural language to query a database from both the linguistic and database points of view (but without AI considerations). In order to achieve this goal, we need a natural language model which we can map on to a relational database model. We have chosen to use the word classification called « classes d'objets » as proposed by the Laboratoire de Linguistique Informatique (LLI). We present here the first results of this work.</abstract>
			<keywords>natural language database query, relational model, « classes d'objets »</keywords>
		</article>
		<article id="taln-2001-long-010" session="">
			<auteurs>
				<auteur>
					<nom>Estelle Campione</nom>
					<email>Estelle.Campione@up.univ-aix.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Jean Véronis</nom>
					<email>Jean.Veronis@up.univ-mrs.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Equipe DELIC - Université de Provence 29, Av. Robert Schuman, 13100 Aix-en-Provence</affiliation>
			</affiliations>
			<titre>Etiquetage prosodique semi-automatique des corpus oraux</titre>
			<type>long</type>
			<pages>123-132</pages>
			<resume>La transcription manuelle de la prosodie est une tâche extrêmement coûteuse en temps, qui requiert des annotateurs très spécialisés, et qui est sujette à de multiples erreurs et une grande part de subjectivité. Une automatisation complète n’est pas envisageable dans l’état actuel de la technologie, mais nous présentons dans cette communication des outils et une méthodologie qui permettent une réduction substantielle du temps d’intervention manuelle, et améliorent l’objectivité et la cohérence du résultat. De plus, les étapes manuelles nécessaires ne demandent pas une expertise phonétique poussée et peuvent être menées à bien par des étudiants et des "linguistes de corpus".</resume>
			<mots_cles>corpus, prosodie, étiquetage</mots_cles>
			<title></title>
			<abstract>The manual transcription of prosody is an extremely time-consuming activity, which requires highly specialised experts, and is prone to errors and subjectivity. Full automation is not achievable in the current state of the technology, but we present in this paper a technique that automates critical steps in the process, which results in a substantial annotation time reduction, and improves the objectivity and coherence of the annotation. In addition, the necessary human phases do not require a highly specific training in phonetics, and can be achieved by syntax students and corpus workers.</abstract>
			<keywords>corpus, prosody, tagging</keywords>
		</article>
		<article id="taln-2001-long-011" session="">
			<auteurs>
				<auteur>
					<nom>Jean-Cédric Chappelier</nom>
					<email>Jean-Cedric.Chappelier@epfl.ch</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Martin Rajman</nom>
					<email>Martin.Rajman@epfl.ch</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">EPFL DI-LIA, IN (Écublens) CH-1015 Lausanne, Switzerland</affiliation>
			</affiliations>
			<titre>Grammaire à substitution d’arbre de complexité polynomiale : un cadre efficace pour DOP</titre>
			<type>long</type>
			<pages>133-142</pages>
			<resume>Trouver l’arbre d’analyse le plus probable dans le cadre du modèle DOP (Data-Oriented Parsing) — une version probabiliste de grammaire à substitution d’arbres développée par R. Bod (1992) — est connu pour être un problème NP-difficile dans le cas le plus général (Sima’an, 1996a). Cependant, si l’on introduit des restrictions a priori sur le choix des arbres élémentaires, on peut obtenir des instances particulières de DOP pour lesquelles la recherche de l’arbre d’analyse le plus probable peut être effectuée en un temps polynomial (par rapport à la taille de la phrase à analyser). La présente contribution se propose d’étudier une telle instance polynomiale de DOP, fondée sur le principe de sélection miminale-maximale et d’en évaluer les performances sur deux corpus différents.</resume>
			<mots_cles></mots_cles>
			<title></title>
			<abstract>Finding the most probable parse tree in the framework of Data-Oriented Parsing (DOP), a Stochastic Tree Substitution Parsing scheme developed by R. Bod (1992), has proven to be NP-hard in the most general case (Sima’an, 1996a). However, introducing some a priori restrictions on the choice of the elementary trees leads to interesting DOP instances with polynomial time-complexity. The purpose of this paper is to present such an instance, based on the minimal-maximal selection principle, and to evaluate its performances on two different corpora.</abstract>
			<keywords></keywords>
		</article>
		<article id="taln-2001-long-012" session="">
			<auteurs>
				<auteur>
					<nom>Choy-Kim Chuah</nom>
					<email>kimc@cs.usm.my</email>
					<email>choy.kim.chuah@umontreal.ca</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">School of Computer Science Universiti Sains Malaysia, 11800 Penang, Malaysia</affiliation>
				<affiliation affiliationId="2">Département de linguistique et de traduction Université de Montréal Montréal (Québec) H3C 3J7, Canada</affiliation>
			</affiliations>
			<titre></titre>
			<type>long</type>
			<pages>143-152</pages>
			<resume>La plupart du temps, les études qui portent sur l’aggrégation des phrases en génération de texte, se focalisent sur l’utilisation des connecteurs pour relier les phrases courtes et inventées. Mais, les connecteurs limitent le nombre des unités qu'il est possible de combiner à la fois. Comment condenser l’information en peu d'unités, sans utiliser trop de connecteurs ‘.7 Cette étude porte sur des documents ayant trait à la biologie et discute de l'agrégation des phrases par les auteurs quand ils résument. Cet article présente aussi quelques préalables et difficultés pour un système de résumé automatique. Beaucoup de phrases sont aggrégées sans signe explicite, ni connecteur, ni ponctuation.</resume>
			<mots_cles></mots_cles>
			<title>Aggregation by Conflation of Quasi-Synonymous Units in Author Abstracting</title>
			<abstract>In text generation, studies on aggregation often focus on the use of connectives to combine short made-up sentences. But connectives restrict the number of units that may be combined at any one time. So, how does information get condensed into fewer units without excessive use of connectives? From a comparison of document and abstract, this reconnaissance study reports on some preferred patterns in aggregation when authors write abstracts for journal articles on biology. The paper also discusses some prerequisites and difficulties anticipated for abstracting systems. More sentences were aggregated without than with the use of an explicit sign, such as a connective or a (semi-)colon.</abstract>
			<keywords>abstracting, sentence, aggregation, synonymy, conflation</keywords>
		</article>
		<article id="taln-2001-long-013" session="">
			<auteurs>
				<auteur>
					<nom>Olivier Ferret</nom>
					<email>ferret@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Brigitte Grau</nom>
					<email>bg@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Martine Hurault-Plantet</nom>
					<email>mhp@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Gabriel Illouz</nom>
					<email>gabrieli@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Christian Jacquemin</nom>
					<email>jacquemin@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIMSI-CNRS, BP 133, 91403 Orsay cedex</affiliation>
			</affiliations>
			<titre>Utilisation des entités nommées et des variantes terminologiques dans un système de question-réponse</titre>
			<type>long</type>
			<pages>153-162</pages>
			<resume>Nous présentons dans cet article le système QALC qui a participé à la tâche Question Answering de la conférence d’évaluation TREC. Ce système repose sur un ensemble de modules de Traitement Automatique des Langues (TAL) intervenant essentiellement en aval d’un moteur de recherche opérant sur un vaste ensemble de documents : typage des questions, reconnaissance des entités nommées, extraction et reconnaissance de termes, simples et complexes, et de leurs variantes. Ces traitements permettent soit de mieux sélectionner ces documents, soit de décider quelles sont les phrases susceptibles de contenir la réponse à une question.</resume>
			<mots_cles>Système de question-réponse, entité nommée, variante terminologique, recherche d'information</mots_cles>
			<title></title>
			<abstract>We developed a system, QALC, that participated to the Question Answering track of the TREC evaluation conference. QALC exploits an analysis of documents, selected by a search engine, based on the search for multi-words terms and their variations both to select a minimal number of documents to be processed and to give indices for comparing question and sentence representations. This comparison also takes advantage of a question analysis module and a recognition of numeric and named entities in the documents.</abstract>
			<keywords></keywords>
		</article>
		<article id="taln-2001-long-014" session="">
			<auteurs>
				<auteur>
					<nom>Olivier Ferret</nom>
					<email>olivier.ferret@cea.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>4</affiliationId>
				</auteur>
				<auteur>
					<nom>Brigitte Grau</nom>
					<email>Brigitte.Grau@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Jean-Luc Minel</nom>
					<email>minel@msh-paris.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Sylvie Porhiel</nom>
					<email>sylvieporhiel@hotmail.com</email>
					<affiliationId>1</affiliationId>
					<affiliationId>3</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIMSI – CNRS BP 13391403 Orsay Cedex</affiliation>
				<affiliation affiliationId="2">CAMS, équipe LaLIC – CNRS, EHESS, Université Paris-Sorbonne 96 Boulevard Raspail 75 006 Paris</affiliation>
				<affiliation affiliationId="3">LATTICE – CNRS ENS, 1 rue Maurice Arnoux – 92120 Montrouge</affiliation>
				<affiliation affiliationId="4">CEA DTI/SITI 91191 Gif-sur-Yvette Cedex</affiliation>
			</affiliations>
			<titre>Repérage de structures thématiques dans des textes</titre>
			<type>long</type>
			<pages>163-172</pages>
			<resume>Afin d’améliorer les performances des systèmes de résumé automatique ou de filtrage sémantique concernant la prise en charge de la cohérence thématique, nous proposons un modèle faisant collaborer une méthode d'analyse statistique qui identifie les ruptures thématiques avec un système d'analyse linguistique qui identifie les cadres de discours.</resume>
			<mots_cles>Cadre thématique, cohérence thématique, exploration contextuelle</mots_cles>
			<title></title>
			<abstract>To improve the results of automatic summarization or semantic filtering systems concerning thematic coherence, we propose a model which combines a statistic analysis system identifying thematic breaks and a linguistic analysis system identifying discourse frames.</abstract>
			<keywords></keywords>
		</article>
		<article id="taln-2001-long-015" session="">
			<auteurs>
				<auteur>
					<nom>Cécile Fougeron</nom>
					<email>Cecile.Fougeron@pse.unige.ch</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Jean-Philippe Goldman</nom>
					<email>Jean-Philippe.Goldman@lettres.unige.ch</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Alicia Dart</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Laurence Guélat</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Clémentine Jeager</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Laboratoire de Psycholinguistique Expérimentale - Université de Geneve 40 boulevard du Pont d’Arve, 1211 Geneve 4 - Suisse</affiliation>
				<affiliation affiliationId="2">Laboratoire d’Analyse et de Technologie du Langage - Université de Geneve 2 rue de Candolle, 1211 Geneve 4 - Suisse</affiliation>
			</affiliations>
			<titre>Influence de facteurs stylistiques, syntaxiques et lexicaux sur la réalisation de la liaison en français</titre>
			<type>long</type>
			<pages>173-182</pages>
			<resume>Les nombreuses recherches portant sur le phénomène de la liaison en français ont pu mettre en évidence l’influence de divers paramètres linguistiques et para-linguistiques sur la réalisation des liaisons. Notre contribution vise à déterminer la contribution relative de certains de ces facteurs en tirant parti d’une méthodologie robuste ainsi que d’outils de traitement automatique du langage. A partir d’un corpus de 5h de parole produit par 10 locuteurs, nous étudions les effets du style de parole (lecture oralisée/parole spontanée), du débit de parole (lecture normale/rapide), ainsi que la contribution de facteurs syntaxiques et lexicaux (longueur et fréquence lexicale) sur la réalisation de la liaison. Les résultats montrent que si plusieurs facteurs étudiés prédisent certaines liaisons, ces facteurs sont souvent interdépendants et ne permettent pas de modéliser avec exactitude la réalisation des liaisons.</resume>
			<mots_cles>liaison, lecture, spontané, débit de parole, longueur, fréquence lexicale</mots_cles>
			<title></title>
			<abstract>Various studies on liaison phenomena in French have shown the influence of several linguistics as well as para-linguistics factors on liaison realization. In this study we aim at determining the relative contribution of certain of these factors by using a robust methodology and tools used in automatic language processing. In a 5 hours speech corpus, produced by 10 speakers, we study the effect of speech style (oral reading/spontaneous speech), speech rate, as well as the contribution of syntactic and lexical (word length and frequency) factors on liaison realization. Results show that even if several factors can contribute to predict some liaisons, these factors are often interdependant and do not allow a sufficient prediction of liaison realization.</abstract>
			<keywords></keywords>
		</article>
		<article id="taln-2001-long-016" session="">
			<auteurs>
				<auteur>
					<nom>Nathalie Friburger</nom>
					<email>friburger@univ-tours .fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Denis Maurel</nom>
					<email>maurel@univ-tours .fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Laboratoire d'Informatique de Tours 64 avenue Jean Portalis, 37000 Tours</affiliation>
			</affiliations>
			<titre>Elaboration d'une cascade de transducteurs pour l'extraction des noms de personnes dans les textes</titre>
			<type>long</type>
			<pages>183-192</pages>
			<resume>Cet article décrit une cascade de transducteurs pour l'extraction de noms propres dans des textes. Après une phase de pré-traitement (découpage du texte en phrases, étiquetage à l'aide de dictionnaires), une série de transducteurs sont appliqués les uns après les autres sur le texte et permettent de repérer, dans les contextes gauches et droits des éléments "déclencheurs" qui signalent la présence d'un nom de personne. Une évaluation sur un corpus journalistique (journal Le Monde) fait apparaître un taux de précision de 98,7% pour un taux de rappel de 91,9%.</resume>
			<mots_cles>Transducteur, noms propres, extraction de motifs</mots_cles>
			<title></title>
			<abstract>This article describes a fmite-state cascade for proper nouns extraction in texts. After a preprocessing (division of the text in sentences, tagging with dictionaries, etc.), a series of finite state transducers cascade is applied one after the other to the text and locate left and right contexts which indicate presence of a person name. An evaluation on a journalistic corpus (Le Monde) gives a rate of precision of 98,7% for a rate of recall of 91,9%.</abstract>
			<keywords>Transducer, proper nouns, pattern extraction</keywords>
		</article>
		<article id="taln-2001-long-017" session="">
			<auteurs>
				<auteur>
					<nom>Jean-Gabriel Ganascia</nom>
					<email>Jean-Gabriel.Ganascia@lip6.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIP6 - Université Pierre et Marie Curie 8, rue du Capitaine Scott, 75015 Paris France</affiliation>
			</affiliations>
			<titre>Extraction automatique de motifs syntaxiques</titre>
			<type>long</type>
			<pages>193-202</pages>
			<resume>Cet article présente un nouvel algorithme de détection de motifs syntaxiques récurrents dans les textes écrits en langage naturel. Il décrit d’abord l’algorithme d’extraction fondé sur un modèle d’édition généralisé à des arbres stratifiés ordonnés (ASO). Il décrit ensuite les expérimentations qui valident l’approche préconisée sur des textes de la littérature française classique des XVIIIe et XIXe siècle. Une sous-partie est consacrée à l’évaluation empirique de la complexité algorithmique. La dernière sous-partie donnera quelques exemples de motifs récurrents typiques d’un auteur du XVIIIe siècle, Madame de Lafayette.</resume>
			<mots_cles>Extraction de motifs, arbres stratifiés ordonnés, distances d'édition, séquences</mots_cles>
			<title></title>
			<abstract>This paper presents a new algorithm designed to detect recurrent syntactical patterns in natural language texts. It first describes the pattern extraction algorithm which is based on an edit model generalized to Stratified Ordered Trees (SOT). Then it focuses on experiments with french classical literature of the 18th and 19th century. One section is dedicated to the efficiency evaluation. The last provides some examples of such recurrent patterns that are typical of an 18th century author, Madame de Lafayette.</abstract>
			<keywords>Pattern extraction, stratified ordered trees, edit distance, sequences</keywords>
		</article>
		<article id="taln-2001-long-018" session="">
			<auteurs>
				<auteur>
					<nom>Jérôme Goulian</nom>
					<email>jerome.goulian@univ-ubs.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Jean-Yves Antoine</nom>
					<email>jean-yves.antoine@univ-ubs.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">VALORIA, EA 2593 - Université de Bretagne Sud Site de Tohannic, rue Yves Mainguy, 56000 Vannes, France</affiliation>
			</affiliations>
			<titre>Compréhension Automatique de la Parole combinant syntaxe locale et sémantique globale pour une CHM portant sur des tâches relativement complexes</titre>
			<type>long</type>
			<pages>203-212</pages>
			<resume>Nous présentons dans cet article un système de Compréhension Automatique de la Parole (CAP) tentant de concilier les contraintes antinomiques de robustesse et d’analyse détaillée de la parole spontanée. Dans une première partie, nous montrons l’importance de la mise en oeuvre d’une CAP fine dans l’optique d’une Communication Homme-Machine (CHM) sur des tâches moyennement complexes. Nous présentons ensuite l’architecture de notre système qui repose sur une analyse en deux étapes : une première étape d’analyse syntaxique de surface (Shallow Parsing) générique suivie d’une seconde étape d’analyse sémantico-pragmatique – dépendante du domaine d’application – de la structure profonde de l’´enoncé complet.</resume>
			<mots_cles>Communication Homme-Machine, Compréhension Automatique de la Parole, robustesse, analyse syntaxique partielle, grammaires de dépendances</mots_cles>
			<title></title>
			<abstract>This paper presents a spoken french understanding system which aims at providing a detailed linguistic analysis as well as preserving the robustness of standard methods. The first part focusses on the importance of a richer analysis for applications that are not dedicated to a very restricted task. We then present our two-level architecture system : a robust independant-domain shallow parsing step followed by a task based analysis of the whole structure of the utterance.</abstract>
			<keywords>Man Machine Dialogue, speech understanding, robustness, shallow parsing, dependency grammars</keywords>
		</article>
		<article id="taln-2001-long-019" session="">
			<auteurs>
				<auteur>
					<nom>Thierry Hamon</nom>
					<email>Thierry.Hamon@lipn.univ-paris13.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Adeline Nazarenko</nom>
					<email>Adeline.Nazarenko@lipn.univ-paris13.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIPN – UPRES-A 7030 Université Paris-Nord &amp; CNRS avenue J.B. Clément 93430 Villetaneuse, FRANCE</affiliation>
			</affiliations>
			<titre>Exploitation de l’expertise humaine dans un processus de constitution de terminologie</titre>
			<type>long</type>
			<pages>213-222</pages>
			<resume>Le processus de construction de terminologie ne peut être entièrement automatisé. Les méthodes et des outils de la terminologie computationnelle permettent de prendre en charge une partie de la tâche, mais l’expertise humaine garde une place prépondérant. Le défi pour les outils terminologiques est de dégrossir les tâches qui sont soit trop longues soit trop complexes pour l’utilisateur tout en permettant à ce dernier d’intégrer ses propres connaissances spécialisées et en lui laissant le contrôle sur la terminologie à construire. Nous montrons ici comment le rôle de cette expertise est pris en compte dans SynoTerm, l’outil d’acquisition de relation de synonymie entre termes que nous avons d´eveloppé.</resume>
			<mots_cles>Structuration de terminologie, aide à la validation, synonymie</mots_cles>
			<title></title>
			<abstract>The terminology building process cannot be fully automatized. Various terminological tools have been designed but human expert knoxledge is still required. This paper shows how SynoTerm, our system for the discovery of synonymy relations between terms, deals with human knowledge. The challenge for terminological tools is to complete the time-consuming tasks under human control and to combine the algorithmic results with human specific information.</abstract>
			<keywords></keywords>
		</article>
		<article id="taln-2001-long-020" session="">
			<auteurs>
				<auteur>
					<nom>Nabil Hathout</nom>
					<email>Nabil.Hathout@univ-tlse2.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">ERSS – CNRS &amp; Université de Toulouse Le Mirail Maison de la Recherche. F-31058 Toulouse cedex 1. France</affiliation>
			</affiliations>
			<titre>Analogies morpho-synonymiques. Une méthode d’acquisition automatique de liens morphologiques à partir d’un dictionnaire de synonymes</titre>
			<type>long</type>
			<pages>223-232</pages>
			<resume>Cet article présente une méthode de construction automatique de liens morphologiques à partir d’un dictionnaire de synonymes. Une analyse de ces liens met en lumière certains aspects de la structure morphologique du lexique dont on peut tirer partie pour identifier les variations allomorphiques des suffixations extraites.</resume>
			<mots_cles>morphologie dérivationnelle, analogie, structure du lexique</mots_cles>
			<title></title>
			<abstract>We present a method to extract morphological links from a synonym dictionary without supervision. The analysis of these links brings somes aspects of the morphological structure of the lexicon to light. We have then taken advantage of this structure to identify allomorphic variations of the extracted suffixations.</abstract>
			<keywords>derivational morphology, analogy, structure of the lexicon</keywords>
		</article>
		<article id="taln-2001-long-021" session="">
			<auteurs>
				<auteur>
					<nom>Mathieu Lafourcade</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Violaine Prince</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIRMM : Dép. ARC, grp. TAL, Université Montpellier 2 161, rue Ada, 35392 Montpellier Cedex 5, France</affiliation>
			</affiliations>
			<titre>Synonymies et vecteurs conceptuels</titre>
			<type>long</type>
			<pages>233-242</pages>
			<resume>La synonymie est une relation importante en TAL mais qui reste problématique. La distinction entre synonymie relative et synonymie subjective permet de contourner certaines difficultés. Dans le cadre des vecteurs conceptuels, il est alors possible de définir formellement des fonctions de test de synonymie et d’en expérimenter l’usage.</resume>
			<mots_cles>Synonymie relative, synonymie subjective, approches statistiques, distances thématiques</mots_cles>
			<title></title>
			<abstract>Synonymy is a pivot relation in NLP but remains problematic. Putting forward, the distinction between relative and subjective synonymy, allows us to circunvent some difficulties. In the framework of conceptual vectors, it is then possible to formalize test functions for synonymy and to experiment their use.</abstract>
			<keywords>Relative synonymy, subjective synonymy, statistical approaches, thematic distances</keywords>
		</article>
		<article id="taln-2001-long-022" session="">
			<auteurs>
				<auteur>
					<nom>Philippe Langlais</nom>
					<email>Felipe@IRO.UMontreal.CA</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Michel Simard</nom>
					<email>Simardm@IRO.UMontreal.CA</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">RALI/DIRO Universié de Montréal</affiliation>
			</affiliations>
			<titre>Récupération de segments sous-phrastiques dans une mémoire de traduction</titre>
			<type>long</type>
			<pages>243-252</pages>
			<resume>L’utilité des outils d’aide à la traduction reposant sur les mémoires de traduction est souvent limitée par la nature des segments que celles-ci mettent en correspondance, le plus souvent des phrases entières. Cet article examine le potentiel d’un type de système qui serait en mesure de récupérer la traduction de séquences de mots de longueur arbitraire.</resume>
			<mots_cles>mémoire de traduction sous-phrastique, traduction assistée par ordinateur, traduction automatique à base d’exemples</mots_cles>
			<title></title>
			<abstract>The usefullness of translation support tools based on translation memories is often limited by the nature of the text segments that they connect, generally whole sentences. This article examines the potential of a type of system that would be able to recuperate the translation of arbitrary sequences of words.</abstract>
			<keywords></keywords>
		</article>
		<article id="taln-2001-long-023" session="">
			<auteurs>
				<auteur>
					<nom>Thomas Lebarbé</nom>
					<email>lebarbe@info.unicaen.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">GREYC - Université de Caen 14032 Caen Cedex</affiliation>
			</affiliations>
			<titre>Vers une plate-forme multi-agents pour l’exploration et le traitement linguistiques</titre>
			<type>long</type>
			<pages>253-262</pages>
			<resume>Dans cet article, nous proposons une plate-forme multi-agents pour l’expérimentation et le traitement linguistique. Après une description du modèle d’agent APA, nous présentons l’état actuel de nos travaux: une implémentation en système multi-agents de l’analyse syntaxique selon le paradigme des grammaires de dépendances en chunk. Nous montrons ensuite d’autres possibilités d’implémentation selon d’autres paradigmes syntaxiques mais aussi au delà de la simple syntaxe.</resume>
			<mots_cles>système multi-agents, syntaxe, analyse syntaxique, environnement</mots_cles>
			<title></title>
			<abstract>In this article, we present un multi-agent plateform for natural language experimentation and processing. After describing the APA agent model, we present the advancement in our work: an agent-based implementation of a syntactic parser according to the chunk dependency paradigm. We then show other potential implementations according to other syntactic paradigms but also further than mere syntax.</abstract>
			<keywords></keywords>
		</article>
		<article id="taln-2001-long-024" session="">
			<auteurs>
				<auteur>
					<nom>Christophe Luc</nom>
					<email>luc@irit.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Institut de Recherche en Informatique de Toulouse - Université Paul Sabatier 118 route de Narbonne 31062 Toulouse Cedex</affiliation>
			</affiliations>
			<titre>Une typologie des énumérations basée sur les structures rhétoriques et architecturales du texte</titre>
			<type>long</type>
			<pages>263-272</pages>
			<resume>Cet article concerne la caractérisation et la représentation de la structure interne des énumérations. Pour ce faire, nous utilisons deux modèles de texte : d’une part la Théorie des Structures Rhétoriques (RST) qui fournit un cadre d’interprétation pour la structure discursive des textes et d’autre part le modèle de représentation de l’architecture textuelle qui est principalement dédié à l’étude et à la représentation des structures visuelles des textes. Après une brève présentation des modèles, nous nous concentrons sur l’étude de l’objet “énumérations”. Nous exhibons et commentons trois exemples d’énumérations spécifiques que nous appelons des énumérations non-parallèles. Nous analysons la structure de ces énumérations et proposons un principe de composition des modèles de référence pour représenter ces énumérations. Enfin, nous présentons une classification des énumérations s’appuyant sur les caractéristiques de ces modèles.</resume>
			<mots_cles>structures textuelles, énumérations, représentation, classification, modèles de texte</mots_cles>
			<title></title>
			<abstract>This paper is related to the study and the representation of the internal structure of enumerations. We use two types of text structure: on one hand, the Rhetorical Structure Theory (RST) which provides a framework for interpreting and representing the discursive structures of texts and on the other hand, the model of text architecture which is is mainly dedicated to the study and representation of visuo-spatial structures of texts. After a brief presentation of the theories and models, we focus the paper on the study of enumerations. We exhibit three significant examples of particular enumerations which are called non-parallel enumerations. We analyse these examples and their characteristics and we propose a general principle for the representation of these structures with these two reference models. Then, we present a classification of the different types of enumeration in the light of these models.</abstract>
			<keywords>Text structures, enumerations, representation, classification, text models</keywords>
		</article>
		<article id="taln-2001-long-025" session="">
			<auteurs>
				<auteur>
					<nom>François Maniez</nom>
					<email>maniezf@univ-lyon2.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Centre de Recherche en Terminologie et en Traduction - Université Lumière Lyon 2. 86, rue Pasteur 69007 LYON</affiliation>
				<affiliation affiliationId="2"></affiliation>
				<affiliation affiliationId="3"></affiliation>
			</affiliations>
			<titre>Désambiguïsation syntaxique des groupes nominaux en anglais médical : étude des structures adjectivales à partir d'un corpus bilingue</titre>
			<type>long</type>
			<pages>273-282</pages>
			<resume>L'ambiguïté syntaxique constitue un problème particulièrement délicat à résoudre pour les analyseurs morphosyntaxiques des logiciels d'aide à la traduction, en particulier dans le cas des longs groupes nominaux typiques des langues de spécialité. En utilisant un corpus bilingue d'articles médicaux anglais traduits vers le français, nous examinons divers moyens de résoudre l'ambiguïté du rattachement de l'adjectif à l'un des deux noms qui le suivent dans les tournures anglaises de forme adjectif-nom-nom.</resume>
			<mots_cles>adjectif, ambiguïté syntaxique, anglais médical, corpus bilingue, découpage, groupe nominal, traduction, traduction automatique</mots_cles>
			<title></title>
			<abstract>Syntactic ambiguity is a particularly difficult problem to solve for the morpho-syntactic analysis programs of machine translation software, especially in the case of the long noun phrases that are typical of technical or scientific writing. Using a bilingual corpus of medical research articles translated into French, we examine various ways in which disambiguation can be achieved in English adj ective-noun-noun structures, where the adjective may modify either of the following nouns.</abstract>
			<keywords>adjective, bilingual corpus, machine translation, medical English, noun clause, syntactic ambiguity, syntactic structure, translation</keywords>
		</article>
		<article id="taln-2001-long-026" session="">
			<auteurs>
				<auteur>
					<nom>Archibald Michiels</nom>
					<email>amichiels@ulg.ac.be</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Département de langue et linguistique anglaises - Université de Liège, 3, Place Cockerill, B-4000 Liège, Belgique</affiliation>
			</affiliations>
			<titre>DEFI, un outil d’aide à la compréhension</titre>
			<type>long</type>
			<pages>283-292</pages>
			<resume></resume>
			<mots_cles>dictionnairique, aide à la compréhension, lecture active, sélection d’acceptions</mots_cles>
			<title></title>
			<abstract>DEFI acts as a filter on a bilingual dictionary (a merge of the Oxford/Hachette (OH) and Robert/Collins (RC) English-French and French-English bilinguals) to provide the user with the most likely translation(s) of the item he has requested help about. The tasks involved are the following: recognition of general language multi-word units (mwu's) stored in the bilingual dictionaries. This task also includes the presentation to the user of relevant dictionary examples, because the concept of mwu is extended here to cover examples as selected and/or edited by lexicographers for both multi-word units and single-word lexical items, restriction of the range of translations, such restriction to be based on properties of the source text, i.e. the textual environment of the item the user has asked to get the translation of. In the best of cases, the translation that ranks highest according to the DEFI matcher is the one that is most appropriate to the context.</abstract>
			<keywords>machine-readable dictionaries, reading aids, translation aids, word sense assignment</keywords>
		</article>
		<article id="taln-2001-long-027" session="">
			<auteurs>
				<auteur>
					<nom>Thierry Poibeau</nom>
					<email>Thierry.Poibeau@lcr.thomson-csf.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Thales Recherche et Technologie Domaine de Corbeville 91404 Orsay</affiliation>
				<affiliation affiliationId="2">Laboratoire d’lnformatique de Paris-Nord avenue J.-B. Clément 93340 Villetaneuse</affiliation>
			</affiliations>
			<titre>Extraction d’information dans les bases de données textuelles en génomique au moyen de transducteurs à nombre fini d’états</titre>
			<type>long</type>
			<pages>293-302</pages>
			<resume>Cet article décrit un système d’extraction d’information sur les interactions entre gènes à partir de grandes bases de données textuelles. Le système est fondé sur une analyse au moyen de transducteurs à nombre fini d’états. L’article montre comment une partie des ressources (verbes d’interaction) peut être acquise de manière semi-automatique. Une évaluation détaillée du système est fournie.</resume>
			<mots_cles>extraction d’information, génomique, transducteurs linguistiques</mots_cles>
			<title></title>
			<abstract>This papers describes a system extracting information about interactions between genes or proteins, from large textual databases. The system is based on a set of linguistic finite-state transducers. The paper shows how a part of the resources (namely the set of verbs expressing the notion of interaction) can be acquired semi-automatically from the corpus. A detailed evaluation is provided.</abstract>
			<keywords></keywords>
		</article>
		<article id="taln-2001-long-028" session="">
			<auteurs>
				<auteur>
					<nom>Amalia Todirascu</nom>
					<email>amalia@infoiasi.ro</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>François Rousselot</nom>
					<email>rousse@liia.u-strasbg.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Faculty of Computer Science, University ”Al.I.Cuza” of Iasi, 16, Berthelot Str., Iasi 6600, Romania</affiliation>
				<affiliation affiliationId="2">LIIA, ENSAIS, 24, Bd. de la Victoire, 67084 Strasbourg Cedex, France</affiliation>
			</affiliations>
			<titre></titre>
			<type>long</type>
			<pages>303-312</pages>
			<resume></resume>
			<mots_cles></mots_cles>
			<title>Ontologies for Information Retrieval</title>
			<abstract>The paper presents a system for querying (in natural language) a set of text documents from a limited domain. The domain knowledge, represented in description logics (DL), is used for filtering the documents returned as answer and it is extended dynamically (when new concepts are identified in the texts), as result of DL inference mechanisms. The conceptual hierarchy is built semi-automatically from the texts. Concept instances are identified using shallow natural language parsing techniques.</abstract>
			<keywords>dynamically modified ontologies, description logics, NLP for IR systems</keywords>
		</article>
		<article id="taln-2001-long-029" session="">
			<auteurs>
				<auteur>
					<nom>Nikolai Vazov</nom>
					<email>c1144@er.uqam.ca</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Groupe sur l’asymétrie des langues naturelles, UQAM</affiliation>
			</affiliations>
			<titre></titre>
			<type>long</type>
			<pages>313-322</pages>
			<resume>Cet article présente un système pour l’identification automatique des expressions temporelles dans des textes français. La procédure d’identification repose sur une stratégie d’exploration contextuelle qui met en oeuvre deux techniques complémentaires: recherche des patrons (expressions régulières) et chart parsing qui est déclenché en fonction des patrons repérés.</resume>
			<mots_cles>chart parsing, exploration contextuelle</mots_cles>
			<title>A System for Extraction of Temporal Expressions from French Texts</title>
			<abstract>We present a system for extraction of temporal expressions from French texts. The identification of the temporal expressions is based on a context-scanning strategy (CSS) which is carried out by two complementary techniques: search for regular expressios and left-to-right and right-to-left local chartparsing. A number of semantic and distant-dependency constraints have been integrated to the chartparsing procedure in order to improve the precision of the system.</abstract>
			<keywords>chart parsing, context-scanning</keywords>
		</article>
		<article id="taln-2001-poster-001" session="">
			<auteurs>
				<auteur>
					<nom>Frédéric Béchet</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Yannick Estève</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Renato De Mori</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIA - Université d’Avignon - BP1228 - Avignon Cedex 9</affiliation>
			</affiliations>
			<titre>Modèles de langage hiérarchiques pour les applications de dialogue en parole spontanée</titre>
			<type>poster</type>
			<pages>325-330</pages>
			<resume>Le cadre de cette étude concerne les systèmes de dialogue via le téléphone entre un serveur de données et un utilisateur. Nous nous intéresserons au cas de dialogues non contraints où l’utilisateur à toute liberté pour formuler ses requêtes. Généralement, le module de Reconnaissance Automatique de la Parole (RAP) de tels serveurs utilise un seul Modèle de Langage (ML) de type bigramme ou trigramme pour modéliser l’ensemble des interventions possibles de l’utilisateur. Ces ML sont appris sur des corpus de phrases retranscrites à partir de sessions entre le serveur et plusieurs utilisateurs. Nous proposons dans cette étude une méthode de segmentation de corpus d’apprentissage de dialogue utilisant une stratégie mixte basée à la fois sur des connaissances explicites mais aussi sur l’optimisation d’un critère statistique. Nous montrons qu’un gain en terme de perplexité et de taux d’erreurs/mot peut être constaté en utilisant un ensemble de sous modèles de langage issus de la segmentation plutôt qu’un modèle unique appris sur l’ensemble du corpus.</resume>
			<mots_cles>Reconnaissance Automatique de la Parole, Modèles de Langage statistique, Serveurs de Dialogue, Arbre de Décision</mots_cles>
			<title></title>
			<abstract>Within the framework of Human-Computer dialogue in spontaneous speech, we propose in this paper a method which automatically builds, from a training corpus, a set of Language Models (LMs) organized as a binary tree. Each LM correspond to a specific dialogue state, where the general LM is attached to the root node and the more specialized ones are represented by the leaves. Such LMs can be used to automatically adapt the decoding process to the dialog situation performed. We propose a two-pass decoding strategy, which implements this idea by dynamically selecting a set of LMs according to the dialog situation detected.</abstract>
			<keywords></keywords>
		</article>
		<article id="taln-2001-poster-002" session="">
			<auteurs>
				<auteur>
					<nom>Pascale Bernard</nom>
					<email>contact@inalf.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Charles Bernet</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Jacques Dendien</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Jean-Marie Pierrel</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Gilles Souvay</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Zina Tucsnak</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">ATILF "Analyses et Traitements Informatiques du Lexique Français" UMR CNRS Université Nancy 2 44, avenue de la Libération, BP 30687 54063 Nancy cedex</affiliation>
			</affiliations>
			<titre>Ressources linguistiques informatisées de l’ATILF</titre>
			<type>poster</type>
			<pages>331-336</pages>
			<resume>Cette contribution présente les ressources linguistiques informatisées du laboratoire ATILF (Analyses et Traitements Informatiques du Lexique Français) disponibles sur la toile et sert de support aux démonstrations prévues dans le cadre de TALN 2001. L’ATILF est la nouvelle U1\/[R créée en association entre le CNRS et l’Université Nancy 2 qui, depuis le 2 janvier 2001, a succédé à la composante nancéienne de l’INaLF. Ces importantes ressources sur la langue française regroupent un ensemble de plus de 3500 textes réunis dans Frantext et divers dictionnaires, lexiques et autres bases de données. Ces ressources exploitent les fonctionnalités du logiciel Stella, qui correspond à un véritable moteur de recherche dédié aux bases textuelles s’appuyant sur une nouvelle théorie des objets textuels. La politique du laboratoire consiste à ouvrir très largement ses ressources en particulier au monde de la recherche et de l’enseignement.</resume>
			<mots_cles>Ressources linguistiques, Bases de données textuelles, Dictionnaires, Web</mots_cles>
			<title></title>
			<abstract>This paper presents the computerized linguistic resources of the Research Laboratory ATILF (Analyses et Traitements Informatiques du Lexique Francais) available via the Web, and will serve as a helping document for demonstrations planned within the framework of TALN 2001. The Research Laboratory ATEF is the new IHVIR (Unité Mixte de Recherche) created in association between the CNRS and the University of Nancy 2 since January 2nd, and succeeds to the local component of the INaLF situated in Nancy. This considerable amount of resources concerning French language consists in a set of more than 3500 literary works grouped together in Frantext, plus a number of dictionaries, lexis and other databases. These web available resources are operated and run through the potentialities and powerful capacities of a software called Stella, a search engine specially dedicated to textual databases and relying on a new theory of textual objects. The general policy of our laboratory is to welcome and give the research and teaching world the widest access to all our resources.</abstract>
			<keywords></keywords>
		</article>
		<article id="taln-2001-poster-003" session="">
			<auteurs>
				<auteur>
					<nom>Choy-Kim Chuah</nom>
					<email>kimc@cs.usm.my</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">School of Computer Science Universiti Sains Malaysia, 11800 Penang, Malaysia</affiliation>
			</affiliations>
			<titre></titre>
			<type>poster</type>
			<pages>337-342</pages>
			<resume>Les résumés constitués de phrases extraites d'un texte contiennent souvent des mots inutiles, il est possible de les éliminer ou d'en réduire le nombre. Par un étude comparative des phrases du documents et des phrases correspondantes dans le résumé, cet article présente un inventaire partiel des unités qui sont souvent éliminées ou réduites en nombre. Les metadiscours, les textes entre parenthèses, les unités redondantes (emphases, répétitions), les appositions, modifieurs et relatives ne sont pas a place dans un résumé.</resume>
			<mots_cles></mots_cles>
			<title>Just What May be Deleted or Compressed in Abstracting?</title>
			<abstract>Abstracts constituted from extracted sentences contain unneeded information that may be deleted, or compressed into simpler units. By comparing full text sentences used in abstracting with correspond-ing sentences in abstract, the study found such units to include metadiscourse phrases, parenthetical texts, redundant units inserted for emphasis, or are repetitions. Apposed texts and units such as modifiers and relative clauses which provide details and precision in the full text, but are out of place in an abstract, are also deleted.</abstract>
			<keywords>abstracting, deletion, compression, metadiscourse, redundancy</keywords>
		</article>
		<article id="taln-2001-poster-004" session="">
			<auteurs>
				<auteur>
					<nom>Isabelle Debourges</nom>
					<email>debourge@lifo.univ-orleans.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Sylvie Guilloré-Billot</nom>
					<email>guillore@lifo.univ-orleans.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Christel Vrain</nom>
					<email>cv@lifo.univ-orleans.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Laboratoire d’Informatique Fondamentale d’Orléans Bâtiment IIIA Rue Léonard de Vinci – B.P. 6759 F-45067 Orléans Cedex 2</affiliation>
			</affiliations>
			<titre>Cartographie de Textes: Une aide à l’utilisateur dans le cadre de la découverte de nouveaux domaines</titre>
			<type>poster</type>
			<pages>343-348</pages>
			<resume>Nous présentons les avancées d’un projet dans un thème que nous qualifions de Cartographie de Textes qui permet à l’utilisateur novice d’explorer un nouveau domaine par navigation au sein d’un corpus homogène grâce à des cartes conceptuelles interactives. Une carte est composée de concepts pertinents relativement à la requête initiale et à son évolution, au sein du corpus; des relations extraites du corpus les lient aux mots de la requête. Des techniques d’apprentissage automatique sont combinées avec des heuristiques statistiques de Traitement Automatique des Langues pour la mise en évidence de collocations afin de construire les cartes.</resume>
			<mots_cles>Cartographie de Textes, Recherche d’Information, Extraction d’Information, Apprentissage Automatique</mots_cles>
			<title></title>
			<abstract>We present an ongoing research project on the new field of Text Mapping that allows a novice user to explore a new domain by navigation through an homogeneous corpus thanks to interactive conceptual maps. A map is composed of concepts (the nodes) depending on the user’s request and its evolution, and semantic/lexical relations (the links). Machine Learning techniques are combined with Natural Language Processing methodologies to build the maps.</abstract>
			<keywords>Text Mapping, Information Retrieval, Information Extraction, Machine Learning</keywords>
		</article>
		<article id="taln-2001-poster-005" session="">
			<auteurs>
				<auteur>
					<nom>Sébastien Gérard</nom>
					<email>gerard@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Jean Paul Sansonnet</nom>
					<email>jps@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">CNRS Université de Paris Bâtiment 508 91403 Orsay, France</affiliation>
			</affiliations>
			<titre>Un Modèle Cognitif pour la Résolution de la Référence dans le Dialogue Homme-Machine</titre>
			<type>poster</type>
			<pages>349-354</pages>
			<resume>Dans cette étude, nous proposons un modèle pour la résolution de la référence dans le cadre du dialogue homme machine. Partant de considérations psychologiques sur la nécessité d'un partage du système inférenciel pour permettre la communication, nous définissons un alisme basé sur des règles de production associées à des coûts cognitifs. Au travers d'exemples, nous montrons comment ce formalisme peut être utilisé comme cadre pour intégrer le traitement de différents phénomènes liés à la référence, et comment cette tégration peut conduire à des interfaces en langue naturelle plus efficaces.</resume>
			<mots_cles>Dialogue Homme-Machine, traitement de la référence</mots_cles>
			<title></title>
			<abstract>ln this study, we propose a model for reference resolution in the context of human computer interaction. Starting from psychological consi erations on the necessity of a common system of inferential strategies in order to allow communication, we define a forma ism based on production rules associated with cognitive costs. Through examples, we will show how this malism can be used as a framework to integrate the processing of various reference nomena, and how this integration can lead to more efficient interactive systems.</abstract>
			<keywords></keywords>
		</article>
		<article id="taln-2001-poster-006" session="">
			<auteurs>
				<auteur>
					<nom>Leila Kosseim</nom>
					<email>kosseim@iro.umontreal.ca</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Guy Lapalme</nom>
					<email>lapalme@iro.umontreal.ca</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">RALI, DIRO, Université de Montréal</affiliation>
			</affiliations>
			<titre>Critères de sélection d’une approche pour le suivi automatique du courriel</titre>
			<type>poster</type>
			<pages>355-360</pages>
			<resume>Cet article discute de différentes approches pour faire le suivi automatique du courrier-électronique. Nous présentons tout d’abord les méthodes de traitement automatique de la langue (TAL) les plus utilisées pour cette tâche, puis un ensemble de critères influençant le choix d’une approche. Ces critères ont été développés grâce à une étude de cas sur un corpus fourni par Bell Canada Entreprises. Avec notre corpus, il est apparu que si aucune méthode n’est complètement satisfaisante par elle-même, une approche combinée semble beaucoup plus prometteuse.</resume>
			<mots_cles>Réponse au courriel, Analyse de texte, Analyse de corpus</mots_cles>
			<title></title>
			<abstract>This paper discusses the strengths and weaknesses of different NLP techniques applied to the automatic follow-up of e-mail. First, we present the most widely-used methods, then we present a set of criteria for selecting the method to be used in particular circumstances. These criteria have been developed through a case study of a corpus provided by Bell Canada Enterprises. With our corpus, it appears that although no individual method is completely satisfactory, an approach combining several techniques is more appropriate.</abstract>
			<keywords></keywords>
		</article>
		<article id="taln-2001-poster-007" session="">
			<auteurs>
				<auteur>
					<nom>Leila Kosseim</nom>
					<email>kosseim@iro.umontreal.ca</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Thierry Poibeau</nom>
					<email>Thierry.Poibeau@thalesgroup.com</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">RALI, Université de Montréal</affiliation>
				<affiliation affiliationId="2">Thales/TRT et LIPN, Institut Galilée, Université Paris-Nord</affiliation>
			</affiliations>
			<titre>Extraction de noms propres à partir de textes variés: problématique et enjeux</titre>
			<type>poster</type>
			<pages>361-366</pages>
			<resume>Cet article porte sur l’identification de noms propres à partir de textes écrits. Les stratégies à base de règles développées pour des textes de type journalistique se révèlent généralement insuffisantes pour des corpus composés de textes ne répondant pas à des critères rédactionnels stricts. Après une brève revue des travaux effectués sur des corpus de textes de nature journalistique, nous présentons la problématique de l’analyse de textes variés en nous basant sur deux corpus composés de courriers électroniques et de transcriptions manuelles de conversations téléphoniques. Une fois les sources d’erreurs présentées, nous décrivons l’approche utilisée pour adapter un système d’extraction de noms propres développé pour des textes journalistiques à l’analyse de messages électroniques.</resume>
			<mots_cles>Extraction d’information, Entités nommées</mots_cles>
			<title></title>
			<abstract>This paper discusses the influence of the corpus on the automatic identification of proper names in texts. Rule-based techniques developed for the newswire genre are generally not sufficient to deal with larger corpora containing texts that do not follow strict writing constraints. After a brief review of the research performed on news texts, we present some of the problems involved in the analysis of informal texts by using two different corpora (the first one composed of electronic mails, the second one of hand-transcribed telephone conversations). Once the sources of errors have been presented, we then describe an approach to adapt a proper name extraction system developed for newspaper texts to the analysis of e-mail messages.</abstract>
			<keywords></keywords>
		</article>
		<article id="taln-2001-poster-008" session="">
			<auteurs>
				<auteur>
					<nom>Yves Lepage</nom>
					<email>yves.lepage©slt.atr.co.jp</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">ATR - Laboratoires télécommunication langue parlée Hikaridai 2-2-2, Seika-tyô, Sôraku-gun, 619-0288 Kyôto, Japon</affiliation>
			</affiliations>
			<titre>Défense et illustration de l'analogie</titre>
			<type>poster</type>
			<pages>367-372</pages>
			<resume>L'argumentation générativiste contre l’analogie tenait en trois points: l’hypothèse de l’inné, celle du hors-contexte et la surproduction. Des résultats théoriques et expérimen- taux reposant sur une formulation calculatoire nouvelle de l’analogie contribuent de façon constructive a la réfutation de ces points.</resume>
			<mots_cles>Formalisation de l’analogie, langages de mots analogiques, mise en correspondance d’espaces analogiques</mots_cles>
			<title></title>
			<abstract>The generativists had three arguments against analogy: the innate hypothesis and the conteXt-free hypothesis, and overgeneration. Theoretical and experimental results, based on a recently obtained cornputational expression of analogy, contribute to the refutation of these three points in a constructive Way.</abstract>
			<keywords>Formalisation of analogy, languages of analogical strings, mapping of analogical spaces</keywords>
		</article>
		<article id="taln-2001-poster-009" session="">
			<auteurs>
				<auteur>
					<nom>Florence Le Priol</nom>
					<email>Florence.Le-Priol@paris4.sorbonne.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LaLIC (Langage Logique Informatique Cognition) Université Paris-Sorbonne - 96 bd Raspail - 75006 Paris</affiliation>
			</affiliations>
			<titre>Identification, interprétation et représentation de relations sémantiques entre concepts</titre>
			<type>poster</type>
			<pages>373-378</pages>
			<resume>SEEK-JAVA est un système permettant Pidentification, l’interprétation et la représentation de connaissances à partir de textes. Il attribue une étiquette aux relations et identifie automatiquement les concepts arguments des relations. Les résultats, capitalisés dans une base de données, sont proposés, par le biais d’une interface, soit sous forme de graphes soit sous forme de tables. Ce système, intégré dans la plate-forme FilText, s’appuie sur la méthode d’ exploration contextuelle.</resume>
			<mots_cles>Identification, interprétation, représentation des connaissances, exploration contextuelle, graphes, base de données</mots_cles>
			<title></title>
			<abstract>SEEK-JAVA is a identification, interpretation and representation system of knowledge extracted from texts. It gives label to the relation between concepts and automatically identify arguments of the relations. The results, capitalized in a data base, are proposed by an interface, either in the form of graphs or in the form of tables. This system, integrated in the FilText platform, is based on contextual exploration method.</abstract>
			<keywords></keywords>
		</article>
		<article id="taln-2001-poster-010" session="">
			<auteurs>
				<auteur>
					<nom>Sophie Rosset</nom>
					<email>rosset@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Lori Lamel</nom>
					<email>lamel@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIMSI-CNRS, 91403 Orsay Cédex</affiliation>
			</affiliations>
			<titre>Gestionnaire de dialogue pour un système d’informations à reconnaissance vocale</titre>
			<type>poster</type>
			<pages>379-384</pages>
			<resume>Dans cet article, nous présentons un gestionnaire de dialogue pour un système de demande d’informations à reconnaissance vocale. Le gestionnaire de dialogue dispose de différentes sources de connaissance, des connaissances statiques et des connaissances dynamiques. Ces connaissances sont gérées et utilisées par le gestionnaire de dialogue via des stratégies. Elles sont mises en oeuvre et organisées en fonction des objectifs concernant le système de dialogue et en fonction des choix ergonomiques que nous avons retenus. Le gestionnaire de dialogue utilise un modèle de dialogue fondé sur la détermination de phases et un modèle de la tâche dynamique. Il augmente les possibilités d’adaptation de la stratégie en fonction des historiques et de l’état du dialogue. Ce gestionnaire de dialogue, implémenté et évalué lors de la dernière campagne d’évaluation du projet LE-3 ARISE, a permi une amélioration du taux de succès de dialogue (de 53% à 85%).</resume>
			<mots_cles>dialogue oral homme-machine, gestionnaire de dialogue, modèle de dialogue, modèle de la tâche</mots_cles>
			<title></title>
			<abstract>In this paper we describe the dialog manager of a spoken language dialog system for information retrieval. The dialog manager maintains both static and dynamic knowledge sources, which are used according to the dialog strategies. These strategies have been developed so as to obtain the overall system objectives while taking into consideration the desired ergonomic choices. Dialog management is based on a dialog model which divides the interaction into phases and a dynamic model of the task. This allows the dialog strategy to be adapted as a function of the history and the dialog state. The dialog manager was evaluated in the context of the LE3-ARISE project, where in the last user tests the dialog success improved from 53% to 85%.</abstract>
			<keywords>spoken language dialog, dialog management, dialog model, task model</keywords>
		</article>
		<article id="taln-2001-poster-011" session="">
			<auteurs>
				<auteur>
					<nom>Antoine Rozenknop</nom>
					<email>Antoine.Rozenknop@epfl.ch</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Marius Silaghi</nom>
					<email>Marius.Silaghi@epfl.ch</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">EPFL (DI-LIA) CH-1015 Lausanne (Suisse)</affiliation>
			</affiliations>
			<titre>Algorithme de décodage de treillis selon le critère du coût moyen pour la reconnaissance de la parole</titre>
			<type>poster</type>
			<pages>385-390</pages>
			<resume>Les modèles de langage stochastiques utilisés pour la reconnaissance de la parole continue, ainsi que dans certains systèmes de traitement automatique de la langue, favorisent pour la plupart l’interprétation d’un signal par les phrases les plus courtes possibles, celles-ci étant par construction bien souvent affectées des coûts les plus bas. Cet article expose un algorithme permettant de répondre à ce problème en remplaçant le coût habituel affecté par le modèle de langage par sa moyenne sur la longueur de la phrase considérée. Cet algorithme est très général et peut être adapté aisément à de nombreux modèles de langage, y compris sur des tâches d’analyse syntaxique.</resume>
			<mots_cles></mots_cles>
			<title></title>
			<abstract>Stochastic language models used for continous speech recognition, and also in some Automated Language Processing systems, often favor the shortest interpretation of a signal, which are affected with the lowest costs by construction. To cope with this problem, this article presents an algorithm that allows the computation of the sequence with the lowest mean cost, in a very systematical way. This algorithm can easily be adapted to several kinds of language models, and to other tasks, such as syntactic analysis.</abstract>
			<keywords>Continuous speech recognition, Stochastic language models, Mean score</keywords>
		</article>
		<article id="taln-2001-poster-012" session="">
			<auteurs>
				<auteur>
					<nom>Grigori Sidorov</nom>
					<email>sidorov@cic.ipn.mx</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Alexander Gelbukh</nom>
					<email>gelbukh@cic.ipn.mx</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Natural Language Laboratory, Center for Computing Research, National Polytechnic Institute AV. Juan de Dios Batiz, s/n, esq. Mendizabal, Zacatenco, C.P. 07738, Mexico D.F., Mexico.</affiliation>
			</affiliations>
			<titre></titre>
			<type>poster</type>
			<pages>391-396</pages>
			<resume>Nous appliquons la désambiguïsation du sens des mot aux définitions d'un dictionnaire explicatif espagnol. Pour calculer le grand nombre de sens de mot en se basant sur le contexte (qui, dans notre cas, est la définition du dictionnaire), nous employons une modification de l'algorithme de Lesk. L'algorithme originel compare les mots pour savoir si ils appartiennent à un même lexème ou non; notre modification consiste en une comparaison floue employant un grand dictionnaire de synonyme et un système de morphologie dérivationnelle simple. L'application de la désambiguïsation aux définitions de dictionnaire (par contraste avec des textes habituels) permet quelques simplifications de l'algorithme (par exemple, nous ne nous soucions pas de la taille de la fenêtre de contexte).</resume>
			<mots_cles>désambiguisalion du sens des mol, usage des ressources linguistiques, algorithme de Lesk, comparaison floue</mots_cles>
			<title>Word Sense Disambiguation in a Spanish Explanatory Dictionary</title>
			<abstract>We apply word sense disambiguation to the definitions in a Spanish explanatory dictionary. To calculate the scores of word senses basing on the context (which in our case is the dictionary definition), we use a modification of Lesk’s algorithm. The algorithm relies on a comparison between two words. In the original Lesk’s algorithm, the comparison is trivial: two words are either the same lexeme or not; our modification consists in fuzzy (weighted) comparison using a large synonym dictionary and a simple derivational morphology system. Application of disambiguation to dictionary definitions (in contrast to usual texts) allows for some simplifications of the algorithm, e.g., we do not have to care of context window size.</abstract>
			<keywords>word sense disambiguation, usage of linguistic resources, Lesk’s algorithm, fuzzy comparison</keywords>
		</article>
		<article id="taln-2001-poster-013" session="">
			<auteurs>
				<auteur>
					<nom>Pierre Zweigenbaum</nom>
					<email>pz@biomath.jussieu.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Natalia Grabar</nom>
					<email>ngr@biomath.jussieu.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Stefan Darmoni</nom>
					<email>stefan.darmoni@chu-rouen.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">DIAM —SIM/DSI, Assistance Publique – Hôpitaux de Paris &amp; Département de Biomathématiques, Université Paris 6</affiliation>
				<affiliation affiliationId="2">Département Informatique et Réseaux, CHU de Rouen &amp; Laboratoire Perception, Information et Systèmes, INSA, Rouen</affiliation>
			</affiliations>
			<titre>L’apport de connaissances morphologiques pour la projection de requêtes sur une terminologie normalisée</titre>
			<type>poster</type>
			<pages>397-402</pages>
			<resume>L’apport de connaissances linguistiques à la recherche d’information reste un sujet de débat. Nous examinons ici l’influence de connaissances morphologiques (flexion, dérivation) sur les résultats d’une tâche spécifique de recherche d’information dans un domaine spécialisé. Cette influence est étudiée à l’aide d’une liste de requêtes réelles recueillies sur un serveur opérationnel ne disposant pas de connaissances linguistiques. Nous observons que pour cette tâche, flexion et dérivation apportent un gain modéré mais réel.</resume>
			<mots_cles>Morphologie, recherche d’information, variantes de termes, médecine, terminologie, Doc’CISMeF, MeSH</mots_cles>
			<title></title>
			<abstract></abstract>
			<keywords></keywords>
		</article>
		<article id="taln-2001-tutoriel-001" session="">
			<auteurs>
				<auteur>
					<nom>Béatrice Daille</nom>
					<email>daille@irin.univ-nantes.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">IRIN, Université de Nantes</affiliation>
			</affiliations>
			<titre>Extraction de collocations à partir de textes</titre>
			<type>tutoriel</type>
			<pages>3-8</pages>
			<resume>Les collocations sont intéressantes dans de nombreuses applications du TALN comme la l'analyse ou la génération de textes ou encore la lexicographie monolingue ou bilingue. Les premières tentatives d'extraction automatique de collocations à partir de textes ou de dictionnaires ont vu le jour dans les années 1970. Il s'agissait principalement de méthodes à base de statistiques lexicales. Aujourd'hui, les méthodes d'identification automatique font toujours appel à des statistiques mais qu'elles combinent avec des analyses linguistiques. Nous examinons quelques méthodes d'identification des collocations en corpus en soulignant pour chaque méthode les propriétés linguistiques des collocations qui ont été prises en compte.</resume>
			<mots_cles>collocations, statistiques lexicales, extraction automatique</mots_cles>
			<title></title>
			<abstract>Collocations are interesting for several NLP applications such as language generation or analysis and monolingual or bilingual lexicography. The first approaches to finding collocations appeared in 1970's and were statistically based. Today, the methods adopted for the identification of collocations still include statistics but also linguistic processing. We introduce a few approaches to finding collocations in corpora. For each method, we precise the linguistic characteristic of collocation which as been taken into account.</abstract>
			<keywords>collocations, automatic identification, lexical statistics</keywords>
		</article>
		<article id="taln-2001-tutoriel-002" session="">
			<auteurs>
				<auteur>
					<nom>Geoffrey Williams</nom>
					<email>GeoffreyWilliams@univ-ubs.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">CRELLIC - Université de Bretagne Sud Lorient</affiliation>
			</affiliations>
			<titre>Sur les caractéristiques de la collocation</titre>
			<type>tutoriel</type>
			<pages>9-16</pages>
			<resume>Le terme "collocation "a été introduit dans les années '30 par J. R. F irth, membre-fondateur de l'école contextualiste britannique, pour caractériser certains phénomènes linguistiques de cooccurrence. Ce phénomène est maintenant accepté comme central dans la compétence linguistique des locuteurs natifs et de grande importance pour l enseignement, la traduction, la lexicographie, et dorénavant, le TALN. Malheureusement, le concept est difiicile a formaliser et ne peut être étudié que par rapport a des exemples prototypiques. Quatre caractéristiques sont analysées, leur nature habituelle, lexicalement transparente, arbitraire et syntactiquement bien formée. Les avantages et inconvénients de chaque critère sont discutés.</resume>
			<mots_cles></mots_cles>
			<title></title>
			<abstract>The term collocation was first introduced in the thirties by J.R. Firth, founder of the British Contextualist school of thought in linguistics, to characterise certain forms of lexical co- occurrence. Gradually this phenomenon has been seen as a central element in native speaker competence and of great importance in teaching, translation,lexicography, and, now, Natural Language Processing. Unfortunately, the concept is diflicult to tie down and can only be studied through reference to prototypical examples. Four characteristics of collocation are discussed their habitual, arbitrary, lexically transparent and grammatically well-formed nature. The advantages and drawbacks of each are considered.</abstract>
			<keywords></keywords>
		</article>
		<article id="taln-2001-tutoriel-003" session="">
			<auteurs>
				<auteur>
					<nom>Sylvain Kahane</nom>
					<email>sk@ccr.jussieu.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Lattice, Universit Paris 7, UFRL Case 7003, 2, place Jussieu, 75251 Paris cedex 5</affiliation>
			</affiliations>
			<titre>Grammaires de dpendance formelles et thorie Sens-Texte</titre>
			<type>tutoriel</type>
			<pages>17-76</pages>
			<resume>On appelle grammaire de dpendance toute grammaire formelle qui manipule comme reprsentations syntaxiques des structures de dpendance. Le but de ce cours est de prsenter  la fois les grammaires de dpendance (formalismes et algorithmes de synthse et dÕanalyse) et la thorie Sens-Texte, une thorie linguistique riche et pourtant mconnue, dans laquelle la dpendance joue un rle crucial et qui sert de base thorique  plusieurs grammaires de dpendance.</resume>
			<mots_cles></mots_cles>
			<title></title>
			<abstract>We call dependency grammar every grammar which handles dependency structures as syntactic representations. The aim of this course is to present both dependency grammars (formalisms, analysis and synthesis algorithms) and the Meaning-Text theory, a rich but nevertheless unrecognized linguistic theory, in which dependency plays a crucial role and which serves as theoretical basis of several dependency grammars.</abstract>
			<keywords></keywords>
		</article>
		<article id="taln-2001-tutoriel-004" session="">
			<auteurs>
				<auteur>
					<nom>Carlos Martín-Vide</nom>
					<email>cmv@correu.urv.es</email>
					<email>cmv@nil.fut.es</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Research Group on Mathematical Linguistics Rovira i Virgili University Pl. Imperial Tàrraco, 1 43005 Tarragona, Spain</affiliation>
			</affiliations>
			<titre>Formal Languages for Linguists: Classical and Nonclassical Models</titre>
			<type>tutoriel</type>
			<pages>77-127</pages>
			<resume></resume>
			<mots_cles></mots_cles>
			<title></title>
			<abstract>The basics of classical formal language theory are introduced, as well as a wide coverage is given of some new nonstandard devices motivated in molecular biology, which are challenging traditional conceptions, are making the theory revived and could have some linguistic relevance. Only definitions and a few results are presented, without including any proof. The chapter can be profitably read without any special previous mathematical background. A long list of references completes the chapter, which intends to give a flavour of the field and to encourage young researchers to go deeper into it.</abstract>
			<keywords></keywords>
		</article>
		<article id="taln-2001-tutoriel-005" session="">
			<auteurs>
				<auteur>
					<nom>Claude De Loupy</nom>
					<email>loupy@sinequa.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Sinequa 51 rue Ledru-Rollin Ivry-sur-Seine</affiliation>
			</affiliations>
			<titre>L’apport de connaissances linguistiques en recherche documentaire</titre>
			<type>tutoriel</type>
			<pages>129-144</pages>
			<resume>L’utilisation de connaissances et de traitements linguistiques évolués en recherche documentaire ne fait pas l’unanimité dans le milieu scientifique. En effet, de nombreuses expériences semblent montrer que les résultats obtenus ne sont pas améliorés, voire sont parfois dégradés, lorsque de telles connaissances sont utilisées dans un système de RD. Dans ce tutoriel, nous montrons que les environnements d’évaluation ne sont pas adaptés aux besoins réels d’un utilisateur car celui-ci recherche presque toujours une information. Il veut donc retrouver des documents pertinents le plus rapidement possible car ce n’est pas là le but de sa recherche. Le temps global de la recherche est donc fondamentalement important. Néanmoins, le cadre d’évaluation TREC nous permet de montrer que l’utilisation de connaissances linguistiques permet d’augmenter la précision des premiers documents renvoyés, ce qui est très important pour diminuer le temps de recherche.</resume>
			<mots_cles></mots_cles>
			<title></title>
			<abstract>The use of linguistic knowledge and natural language processing (NLP) systems is generally not considered to be the best way to perform document retrieval. Indeed numerous experiments seem to show that NLP does not improve the performances and, even can decrease them. In this tutorial we show the environments used for evaluation do not fit the real world needs. Almost every time, the user search for an information. He wants relevant documents as soon as possible because they are not his goal. So, time is really important in evaluation of document retrieval systems. Nevertheless, using TREC, we show NLP can lead to a better precision for the first documents retrieved and this is really important in order to improve the speed of search process.</abstract>
			<keywords></keywords>
		</article>
		<article id="taln-2001-tutoriel-006" session="">
			<auteurs>
				<auteur>
					<nom>Max Silberztein</nom>
					<email>msl@us.ibm.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Thierry Poibeau</nom>
					<email>Thierry.Poibeau@thalesgroup.com</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Antonio Balvet</nom>
					<email>Antonio.Balvet@thalesgroup.com</email>
					<affiliationId>3</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">IBM et LADL</affiliation>
				<affiliation affiliationId="2">Thales TRT et LIPN</affiliation>
				<affiliation affiliationId="3">Thales TRT et Modyco</affiliation>
			</affiliations>
			<titre>Intex et ses applications informatiques</titre>
			<type>tutoriel</type>
			<pages>145-174</pages>
			<resume>Intex est un environnement de développement utilisé pour construire, tester et accumuler rapidement des motifs morpho-syntaxiques qui apparaissent dans des textes écrits en langue naturelle. Un survol du système est présenté dans [Silberztein, 1999] , le manuel d'instruction est disponible [Silberztein 2000]. Chaque description élémentaire est représentée par une grammaire locale, qui est habituellement entrée en machine grâce à l'éditeur de graphe d'Intex. Une caractéristique importante d'Intex est que chaque grammaire locale peut être facilement réemployée dans d'autres grammaires locales. Typiquement, les développeurs construisent des graphes élémentaires qui sont équivalents à des transducteurs à états finis, et réemploient ces graphes dans d'autres graphes de plus en plus complexes. Une seconde caractéristique d'Intex est que les objets traités (grammaires, dictionnaires et textes) sont représentés de façon interne par des transducteurs à états finis. En conséquence, toutes les fonctionnalités du système se ramènent à un nombre limité d'opérations sur des transducteurs. Par exemple, appliquer une grammaire à un texte revient à construire l'union des transducteurs élémentaires, la déterminiser, puis à calculer l'intersection du résultat avec le transducteur du texte. Cette architecture permet d'utiliser des algorithmes efficaces (par ex. lorsqu'on applique un transducteur déterministe à un texte préalablement indexé), et donne à Intex la puissance d'une machine de Turing (grâce à la possibilité d'appliquer des transducteurs en cascade). Dans ce tutoriel, nous montrerons comment utiliser un outil linguistique tel qu'Intex dans des environnements informatiques. Nous nous appuierons sur des applications de filtrage et d'extraction d'information, réalisées notamment au centre de recherche de Thales. Les applications suivantes seront détaillées, tant sur le plan linguistique qu'informatique filtrage d'information a partir d'un flux AFP [Meunier et al. l999] extraction de tables d'interaction entre gènes à partir de bases de données textuelles en génomique. [Poibeau 2001] Le tutoriel montrera comment Intex peut être employé comme moteur de filtrage d'un flux de dépêches de type AFP dans un cadre industriel. Il détaillera également les fonctionnalités de transformations des textes (transduction) permettant de passer rapidement de structures linguistiques variées à des formes normalisées permettant de remplir une base de données. Sur le plan informatique, on détaillera l'appel aux routines Intex, les paramétrages possibles (découpage en phrases, choix des dictionnaires...), et on survolera les nouvelles possibilités d'intégration (Intex API).</resume>
			<mots_cles></mots_cles>
			<title></title>
			<abstract></abstract>
			<keywords></keywords>
		</article>
	</articles>
</conference>