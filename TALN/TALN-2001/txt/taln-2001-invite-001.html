<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
"http://www.w3.org/TR/html4/loose.dtd">
<html><head><title>Language Processing with Weighted Transducers</title>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
</head>
<body>
<div style="page-break-before:always; page-break-after:always"><div><p>TALN 2001, Tours, 2-5 juillet 2001
</p>
<p>Language Processing with Weighted Transducers
</p>
<p>Mehryar Mohri
AT&amp;T Labs - Research
</p>
<p>180 Park Avenue
Florham Park, NJ 07932-0971, USA
mohri@research.att.com
</p>
<p>Re&#769;sume&#769; - Abstract
</p>
<p>Les automates et transducteurs ponde&#769;re&#769;s sont utilise&#769;s dans un e&#769;ventail d&#8217;applications allant de
la reconnaissance et synthe&#768;se automatiques de la langue a&#768; la biologie informatique. Ils four-
nissent un cadre commun pour la repre&#769;sentation des composants d&#8217;un syste&#768;me complexe, ce qui
rend possible l&#8217;application d&#8217;algorithmes d&#8217;optimisation ge&#769;ne&#769;raux tels que la de&#769;terminisation,
l&#8217;e&#769;limination des mots vides, et la minimisation des transducteurs ponde&#769;re&#769;s.
</p>
<p>Nous donnerons un bref aperc&#807;u des progre&#768;s re&#769;cents dans le traitement de la langue a&#768; l&#8217;aide
d&#8217;automates et transducteurs ponde&#769;re&#769;s, y compris une vue d&#8217;ensemble de la reconnaissance
de la parole avec des transducteurs ponde&#769;re&#769;s et des re&#769;sultats algorithmiques re&#769;cents dans ce
domaine. Nous pre&#769;senterons e&#769;galement de nouveaux re&#769;sultats lie&#769;s a&#768; l&#8217;approximation des gram-
maires context-free ponde&#769;re&#769;es et a&#768; la reconnaissance a&#768; l&#8217;aide d&#8217;automates ponde&#769;re&#769;s.
</p>
<p>Weighted automata and transducers are used in a variety of applications ranging from automatic
speech recognition and synthesis to computational biology. They give a unifying framework for
the representation of the components of complex systems. This provides opportunities for the
application of general optimization algorithms such as determinization, &#0; -removal and mini-
mization of weighted transducers.
</p>
<p>We give a brief survey of recent advances in language processing with weighted automata and
transducers, including an overview of speech recognition with weighted transducers and recent
algorithmic results in that field. We also present new results related to the approximation of
weighted context-free grammars and language recognition with weighted automata.
</p>
<p>Keywords: automatic speech recognition, weighted finite-state transducers, weighted automata,
context-free grammars, regular approximation of CFGs, rational power series.
</p>
<p>1 Introduction
</p>
<p>It is a common observation that massive quantities of digitized data are widely available for
various information sources such as text, speech, biological sequences, images, and handwritten</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Mehryar Mohri
</p>
<p>characters or patterns. But much needs to be done to fully exploit these resources since they
are all highly variable or noisy sources of information. Natural language texts are extremely
ambiguous, speech and handwritten texts highly variable and hard to detect in presence of
noise, biological sequences often altered.
</p>
<p>To cope with this variability, sophisticated machine learning techniques have been used to de-
sign statistical models for these information sources (Vapnik1995). The theory of weighted
finite-state transducers combining the classical automata theory and statistical models provides
a general framework for processing such sources of information.
</p>
<p>Weighted transducers are in fact used in a variety of applications ranging from automatic
speech recognition and synthesis to computational biology (Baldi and Brunak1998; Culik II
and Kari1997; Mohri1997).
They give a common representation for the components of complex language processing sys-
tems. This provides opportunities for the application of general optimization algorithms such
as determinization, &#0; -removal, and minimization of weighted transducers.
</p>
<p>In what follows, we give a brief survey of some recent advances in language processing with
weighted automata and transducers, including an overview of speech recognition with weighted
transducers and recent algorithmic results in that field. We also present new results related to
the approximation of weighted context-free grammars and language recognition with weighted
automata.
</p>
<p>2 Speech recognition with weighted transducers
</p>
<p>2.1 Preliminaries
</p>
<p>A weighted finite-state transducer &#1;&#3;&#2;&#5;&#4; &#6;&#8;&#7; &#9;&#10;&#7; &#11;&#12;&#7; &#13;&#14;&#7; &#15;&#16;&#7; &#17;&#18;&#7; &#19;&#20;&#7; &#21;&#23;&#22; over a weight set &#24; (Bers-
tel1979; Eilenberg1974) is a generalization of the classical definition of a finite automaton and
is given by a finite set of states &#11; , an input alphabet &#6; , an output alphabet &#9; , a finite set of
transitions &#13;&#26;&#25;ff&#11;flfiffi&#4; &#6; &#31;&quot;! &#0; # &#22;$fiffi&#4; &#9; &#31;%! &#0; # &#22;&amp;fi &#24; fi&quot;&#11; , an initial state '$( &#11; , a set of final states
&#17;fl&#25;ff&#11;
</p>
<p>, an initial weight &#19; and a final weight function &#21; . Figure 1 gives an example of weighted
transducer.
</p>
<p>For numerical stability, the weights used in speech recognition systems are often log proba-
bilities, thus &#24; &#2;*)fl&#31;+!-, # . The weight of a path is obtained by adding the weights of its
constituent transitions. We denote by .0/ 132 the original and by 4$/ 1&#20;2 the destination state of a
path 1 . The weight associated by &#1; to a pair of strings &#4; 53&#7; 67&#22; , 5 ( &#6;&#18;8 , 6 ( &#9;&#18;8 , is then given by:
</p>
<p>/ /
</p>
<p>&#1;
</p>
<p>2 2
</p>
<p>&#4; 53&#7; 67&#22;&amp;&#2;&#3;9
</p>
<p>:&#20;; &lt; = &gt; ?
</p>
<p>&#19;&#14;@ A
</p>
<p>/ 1&#20;2
</p>
<p>@ &#21;&#16;&#4;
</p>
<p>4$/ 132
</p>
<p>&#22;
</p>
<p>where the sum runs over B &#4; 53&#7; 67&#22; , the set of paths in &#1; with input label 5 and output label 6 and
where C is defined by:D
</p>
<p>E
</p>
<p>&#7; F
</p>
<p>(
</p>
<p>)G&#31; !-,
</p>
<p>#
</p>
<p>&#7;
</p>
<p>E
</p>
<p>C
</p>
<p>F&#8;&#2;IH J KML7&#4; N 5
</p>
<p>.
</p>
<p>&#4; H
</p>
<p>E
</p>
<p>&#22;0O+N 5
</p>
<p>.
</p>
<p>&#4; H&#10;F &#22; &#22;
</p>
<p>When a Viterbi approximation is assumed, C is replaced by P&#14;Q R in these definitions.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Language Processing with Weighted Transducers
</p>
<p>0
</p>
<p>1
</p>
<p>a:e/1
</p>
<p>b:e/2
</p>
<p>c:e/5
</p>
<p>2
</p>
<p>a: &#949;/3
</p>
<p>b: &#949;/4
</p>
<p>c: &#949;/7
</p>
<p>d:e/8
</p>
<p>e:d/9
</p>
<p>3/0
</p>
<p>f: &#949;/9
</p>
<p>e: &#949;/8
</p>
<p>e: &#949;/10
</p>
<p>e:e/11
</p>
<p>f:e/12
</p>
<p>f:e/13
</p>
<p>Figure 1: Example of a weighted transducer. The initial state is represented by a bold circle,
final states by double circles. Inside each circle, the first number indicates the state number, the
second, at final states only, the value of the final weight function &#0; at that state. Each transition
is labeled with &#1;&#3;&#2;&#5;&#4;&#7;&#6; &#8; where &#1; is the input label, &#4; the output label and &#8; its weight. The
symbol &#9; denotes the empty string.
</p>
<p>0 1good/11.11
</p>
<p>3
data/10.89
</p>
<p>2
day/13.97
</p>
<p>4
&#949;/3.537
</p>
<p>data/6.603
&#949;/2.374
</p>
<p>5/0
</p>
<p>&#949;/3.961
</p>
<p>&#949;/1.882
&#949;/2.306
</p>
<p>&#949;/1.102
</p>
<p>&#949;/1.913
good/9.394
</p>
<p>data/9.268
</p>
<p>day/11.51
</p>
<p>&#949;/3.173
</p>
<p>Figure 2: Toy bigram model represented by a weighted automaton.
</p>
<p>2.2 Speech recognition components
</p>
<p>Weighted finite-state transducers provide a common and natural representation for all the com-
ponents used in the search stage of speech recognition systems: HMM models, context-depen-
dency, pronunciation dictionaries, and language models (Mohri et al.1998). We briefly illustrate
this in the following.
</p>
<p>Most statistical grammars used in speech recognition can be represented by weighted automata.
Figure 2 shows a toy grammar corresponding to an &#10; -gram model restricted to a few words.
In particular, &#10; -gram models can be represented very naturally by weighted automata. The
construction is based on associating one state to each possible &#11; &#10;&#13;&#12;&#15;&#14;&#17;&#16; -gram sequence. For
example, in the bigram model of figure 2, state 1 corresponds to the word &#8220;good&#8221;, state 2 to the
word &#8220;day&#8221; and state 3 to the word &#8220;data&#8221;. More general weighted grammars such as weighted
context-free grammars can also be approximated with weighted regular grammars and thus be
represented by weighted automata as shown in the next section.
</p>
<p>Pronunciation dictionaries can also be compactly represented by weighted transducers mapping</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Mehryar Mohri
</p>
<p>0 1d: e/0 2ey: e/0.92
ae: e/0.51
</p>
<p>3dx: e/0.22
t: e/1.61
</p>
<p>4/0ax:data/0
</p>
<p>Figure 3: Sample pronunciation dictionary transducer encoding four different pronunciations
of the word &#8220;data&#8221;.
</p>
<p>e,e
</p>
<p>d,dd/e_d:d d,t
</p>
<p>d/e_t:d
</p>
<p>t,d
</p>
<p>t/e_t:t
</p>
<p>t,tt/e_t:t
</p>
<p>d,e
</p>
<p>d/e_e:d
</p>
<p>t,e
</p>
<p>t/e_e:t
</p>
<p>d/d_d:d
</p>
<p>d/d_t:d
</p>
<p>d/d_e:d
</p>
<p>t/d_d:t
t/d_t:t
</p>
<p>t/d_e:t
</p>
<p>d/t_d:d
d/t_t:d
</p>
<p>d/t_e:d
t/t_d:t
</p>
<p>t/t_t:t
</p>
<p>t/t_e:t
</p>
<p>Figure 4: A triphonic context-dependent model for the phones
&#0;
</p>
<p>and &#1; represented by a finite-
state transducer. The symbol &#2;&#4;&#3; &#5; &#6; represents the context-dependent phone &#2; with the left
context &#5; and right context &#6; .
</p>
<p>phonemic transcriptions to word sequences. Figure 5 illustrates this in the particular case of
the pronunciation of the word &#8220;data&#8221; in English corresponding to the four cases of flapped d or
t. The weights can be used to encode the probability of each pronunciation typically based on
data collected from a large number of speakers.
</p>
<p>Figure 4 illustrates the construction of a simple triphonic context-dependency transducer &#7;
mapping context-dependent phones to phones with only two phones
</p>
<p>&#0;
</p>
<p>and &#1; (Pereira and Ri-
ley1997). Each state &#8; &#2;&#4;&#9; &#5;&#11;&#10; encodes the most recent pair of phones read. &#12; represents the
start or end of a phone sequence. More general context-dependent models corresponding to
weighted rewrite rules can also be compiled into weighted finite-state transducers (Mohri and
Sproat1996).
Figure 5 shows a three-state HMM transducer mapping sequences of distribution indices to
context-dependent phones. Such models can clearly be represented by weighted transducers.
The global HMM transducer for speech recognition is obtained by taking the closure of the
union of all HMMs used in acoustic modeling.
</p>
<p>2.3 New algorithmic results
</p>
<p>Weighted transducers map input sequences to output sequences with some weights. They can
be composed like other mappings to create more complex mappings. The composition of two</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Language Processing with Weighted Transducers
</p>
<p>0
</p>
<p>d1:e
</p>
<p>1d1:p
</p>
<p>d2:e
</p>
<p>2d2:e
</p>
<p>d3:e
</p>
<p>3d3:e
</p>
<p>Figure 5: Three-state HMM transducer mapping sequences of distribution indices to context-
dependent phones.
</p>
<p>H C L G
distributions context&#8722;dependent
</p>
<p>phones
phones words word
</p>
<p>sequences
</p>
<p>Figure 6: Recognition cascade.
</p>
<p>transducers &#0;&#2;&#1; and &#0;&#4;&#3; is defined by:
&#5;&#7;&#6;&#4;&#8; &#9;&#11;&#10; &#10;
</p>
<p>&#0;&#2;&#1;&#2;&#12;&#13;&#0;&#4;&#3; &#14; &#14; &#15;
</p>
<p>&#6;&#4;&#8; &#9;&#17;&#16;&#19;&#18;&#21;&#20;&#7;&#22;&#23;&#10; &#10;
</p>
<p>&#0;&#4;&#1; &#14; &#14; &#15;
</p>
<p>&#6;&#4;&#8; &#24;&#25;&#16;ff&#26;fi&#10; &#10;
</p>
<p>&#0;fl&#3; &#14; &#14; &#15;
</p>
<p>&#24;&#17;&#8; &#9;&#17;&#16;
</p>
<p>There exists a natural and efficient composition algorithm for combining weighted transducers
(Mohri, Pereira, and Riley1996). The algorithm is an extension to the weighted case of the
classical composition algorithm for unweighted transducers (Berstel1979). It is based on a
filter represented by a transducer that eliminates the redundancy of ffi -paths.
</p>
<p>The composition of the components just presented: &#31; , the transducer mapping sequences of
distribution indices to context-dependent phone,  the context-dependent model, ! the lexicon
or pronunciation dictionary, and &quot; the grammar, gives a mapping from sequences of distribution
names to word sequences:
</p>
<p>&#31;
</p>
<p>&#12;
</p>
<p> 
</p>
<p>&#12;
</p>
<p>!
</p>
<p>&#12;
</p>
<p>&quot;
</p>
<p>Figure 6 illustrates that recognition cascade. Recent work in very large-vocabulary speech
recognition has shown that it is in fact possible to build off-line the result of that cascade
of compositions, even for very large tasks, using general optimization algorithms such as ffi -
removal (Mohri2000a), determinization (Mohri1997) and minimization of weighted finite-state
transducers (Mohri2000b).
The result is thus a single transducer that integrates all the speech recognition components,
directly mapping from HMM states to words (Mohri and Riley1999). Experiments with a
463,331-word vocabulary North American Business News (NAB) Task show that this also leads
to a substantial improvement of the recognition speed (Mohri and Riley1999). The size of the
integrated context-dependent networks constructed can be further dramatically reduced using a
factoring algorithm. With that construction, the integrated NAB recognition transducer contains
only about #%$ &amp; times as many transitions as the language model &quot; it is constructed from (Mohri
and Riley1999).
The weights of the integrated recognition transducer can be distributed in many equivalent ways
along its paths. For speech recognition, the weight distribution is crucial since pruning is typ-
ically based on the combined weight from the acoustic, duration, pronunciation, and language
model components accumulated so far along an explored path: the integrated recognition trans-</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Mehryar Mohri
</p>
<p>0 1a/1.428
</p>
<p>b/0.5
</p>
<p>2
</p>
<p>b/0.3
</p>
<p>a/0.5
</p>
<p>3/0
c/0.1
</p>
<p>a/0.6
</p>
<p>(a)
</p>
<p>0 1a/0
</p>
<p>b/0.5
</p>
<p>2
</p>
<p>b/2.328
</p>
<p>a/2.528
</p>
<p>3/0
c/1.528
</p>
<p>a/0
</p>
<p>(b)
</p>
<p>Figure 7: Weight pushing algorithm in the log semiring. The resulting automaton (b) is equiv-
alent to (a) and is stochastic: at each state, the probability weights of outgoing transitions sum
to 1.
</p>
<p>ducer is searched with a simple Viterbi decoder combined with a beam pruning to find the best
path and to output the transcription corresponding to the input speech utterance.
</p>
<p>It is possible to modify the weights so that the sum of the probabilities for all transitions leaving
a state is &#0; . This makes the transducer stochastic and results in an equivalent transducer whose
weight distribution is more suitable for pruning and speech recognition and leads to substantial
improvements in the recognition speed as demonstrated in several tasks. As an example, with
this technique, we can obtain a &#1;&#2;&#1; &#3;&#5;&#4; speed-up at &#6;&#7;&#6;&#5;&#4; word accuracy in rescoring NAB word
lattices with more accurate 2nd-pass models.
</p>
<p>Figures 7 (a)-(b) illustrate the application of the algorithm in the case of a simple weighted
automaton. There exists a generalization of the algorithm of Floyd-Warshall that can be used to
effectively compute the equivalent stochastic transducer (Mohri1998). However, that algorithm
has a quadratic time complexity and a cubic space complexity which makes its application to
the large transducers used in speech recognition impossible in practice &#8211; such transducers may
have several million transitions. A new algorithm devised recently has been shown to be prac-
tical for the computation of the resulting stochastic transducer even for such large transducers
(Mohri1998). The algorithm has been shown to be practical for transducers of more than 5M
transitions such as a factored integrated recognition transducer used for the 463,331-word vo-
cabulary NAB task. It is also applied to word lattices to speed-up rescoring with more accurate
2nd-pass models.
</p>
<p>3 Regular approximation of context-free grammars
</p>
<p>General context-free grammars are computationally too demanding for real-time applications
such as speech recognition. The grammars used in those applications often represent regular
languages either by construction or as as a result of a regular approximation of a more general
context-free grammar (Pereira and Wright1997; Grimley Evans1997; Johnson1998).</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Language Processing with Weighted Transducers
</p>
<p>&#0;&#2;&#1;&#4;&#3;&#6;&#5;
</p>
<p>&#0;&#2;&#1;&#4;&#7; &#8;
</p>
<p>&#8;&#9;&#1;&#4;&#3;
</p>
<p>&#8;&#9;&#1;&#4;&#3;&#10;&#0;
</p>
<p>&#8;&#9;&#1;&#4;&#7; &#8;&#11;&#8;
</p>
<p>&#5;&#9;&#1;&#4;&#7;
</p>
<p>&#5;&#9;&#1;&#4;&#7; &#0;
</p>
<p>&#5;&#9;&#1;&#4;&#3;&#6;&#5;&#12;&#5;
</p>
<p>&#0;&#13;&#1;&#14;&#3;&#15;&#5;
</p>
<p>&#5;&#12;&#16;&#17;&#1;&#18;&#0;&#19;&#16;
</p>
<p>&#0;&#13;&#1;&#14;&#7; &#8;
</p>
<p>&#8;&#11;&#16;&#20;&#1;&#18;&#0;&#19;&#16;
</p>
<p>&#8;&#21;&#1;&#14;&#3;&#15;&#8;&#11;&#16;
</p>
<p>&#8;&#11;&#16;&#20;&#1;&#14;&#22;
</p>
<p>&#8;&#21;&#1;&#14;&#3;&#6;&#0;
</p>
<p>&#0;
</p>
<p>&#16;
</p>
<p>&#1;&#14;&#22;
</p>
<p>&#0;&#19;&#16;&#23;&#1;&#14;&#8;&#24;&#16;
</p>
<p>&#8;&#13;&#1;&#14;&#7; &#8;
</p>
<p>&#8;&#24;&#16;&#20;&#1;&#14;&#8;
</p>
<p>&#8;&#24;&#16;&#20;&#1;&#14;&#8;&#24;&#16;
</p>
<p>&#5;&#13;&#1;&#14;&#7; &#5;&#12;&#16;
</p>
<p>&#5;&#25;&#16;&#17;&#1;&#14;&#22;
</p>
<p>&#5;&#13;&#1;&#14;&#7; &#0;
</p>
<p>&#0;
</p>
<p>&#16;
</p>
<p>&#1;&#14;&#5;
</p>
<p>&#16;
</p>
<p>&#5;&#21;&#1;&#4;&#3;&#6;&#5;
</p>
<p>&#5;&#25;&#16;&#26;&#1;&#4;&#5;
</p>
<p>&#5;&#25;&#16;&#26;&#1;&#4;&#5;&#12;&#16;
</p>
<p>(a) (b)
Figure 8: Regular approximation by transformation. (a) Context-free grammar ffflfi . (b) Gram-
mar ff&#25;ffi obtained from ff
</p>
<p>fi
</p>
<p>by transformation..
</p>
<p>0
</p>
<p>1
a
</p>
<p>2
</p>
<p>b
</p>
<p>a
</p>
<p>3
</p>
<p>b
</p>
<p>b
a
</p>
<p>a
</p>
<p>b
</p>
<p>Figure 9: Finite automaton realizing the approximated grammar ff ffi shown in figure 8 (b), using
the compilation algorithm presented in (Mohri and Pereira1998).
</p>
<p>The effect of such approximations are often complex and it is difficult for the grammar writer
to modify the resulting grammar or to adapt it to a specific application. Furthermore, many of
these approximations do not scale. They blow up for grammars of several hundred or thousand
rules (Nederhof2000).
A new approximation algorithm has been devised more recently that applies to any context-free
grammar and that guarantees that the result can be compiled into a finite automaton (Mohri
and Nederhof2001). The resulting grammar contains at most one new nonterminal for any
nonterminal symbol of the input grammar, and new rules are formed out of rules from the
input grammar by means of a straightforward decomposition. The result thus remains readable
and if necessary modifiable. The algorithm also extends to the case of weighted context-free
grammars.
</p>
<p>An approximate grammar ff&#12;ffi is obtained from ff
fi
</p>
<p>by introducing at most one new non-terminal
symbol
</p>
<p>&#8;&#24;&#16;
</p>
<p>for each non-terminal
&#8;
</p>
<p>and by introducing the rule
&#8;&#11;&#16;&#31;&#1; &#22;
</p>
<p>. Each rule of the
form
&#8;!&#1;#&quot;&#19;$&#24;&#5;
</p>
<p>fi
</p>
<p>&quot;
</p>
<p>fi
</p>
<p>&#5;
</p>
<p>ffi
</p>
<p>&quot;
</p>
<p>ffi&amp;% % %
</p>
<p>&#5;&#11;'(&quot;)'
</p>
<p>where *,+&#26;- and where
&#5;
</p>
<p>fi . / / / .
</p>
<p>&#5;&#11;'
</p>
<p>are mutually
dependent non-terminals is split into the following set of rules:
</p>
<p>&#8;&#23;&#1;!&quot;0$&#25;&#5;
</p>
<p>fi ,
</p>
<p>&#5;&#25;&#16;
</p>
<p>fi
</p>
<p>&#1;!&quot;
</p>
<p>fi
</p>
<p>&#5;
</p>
<p>ffi
</p>
<p>&#5;&#25;&#16;
</p>
<p>ffi
</p>
<p>&#1;1&quot;
</p>
<p>ffi
</p>
<p>&#5;32
</p>
<p>/ / /
</p>
<p>&#5;&#12;&#16;
</p>
<p>'54
</p>
<p>fi
</p>
<p>&#1;,&quot;)'54
</p>
<p>fi
</p>
<p>&#5;&#11;'6&#5;&#25;&#16;
</p>
<p>'
</p>
<p>&#1;1&quot;)'6&#8;&#11;&#16;
</p>
<p>.
</p>
<p>Figures 8 (a)-(b) illustrate this approximation. The resulting grammar ff&#31;ffi is strongly regular
and can be compiled efficiently into the finite automaton of figure 9 (Mohri and Pereira1998).
This approximation algorithm, as well as three other variants, have been fully implemented
and incorporated in the GRM library (Mohri2001). We used that approximation algorithm and
implementation to approximate a weighted grammar of about 798&#6;. -9-&#15;- rules used for translation
at AT&amp;T. The transformed grammar had about :&#15;;&#6;. -&#15;-9- rules. The whole approximation process
including the creation of a finite automaton accepting that grammar took about one minute using</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Mehryar Mohri
</p>
<p>0
</p>
<p>(/1
</p>
<p>1/0
</p>
<p>a/0
</p>
<p>b/0
</p>
<p>e/0 2
</p>
<p>(/-1
</p>
<p>+/0
</p>
<p>(/1
</p>
<p>)/-1
*/0
</p>
<p>a/0
</p>
<p>b/0
</p>
<p>e/0
</p>
<p>(/-1
</p>
<p>3/0)/1
</p>
<p>(/-1
a/0
</p>
<p>b/0
</p>
<p>e/0
</p>
<p>+/0
</p>
<p>(/-1
</p>
<p>a/0
</p>
<p>b/0
</p>
<p>e/0
</p>
<p>)/1
*/0
</p>
<p>Figure 10: A &#0; -state weighted automaton over the tropical semiring recognizing regular expres-
sions over the alphabet &#1; &#2;&#4;&#3; &#5; &#6; . The symbol &#7; corresponds to the empty string used in regular
expressions. For simplicity, the empty set regular expression &#8; has been omitted.
</p>
<p>an SGI Origin 2000.
</p>
<p>4 Context-free recognition with weighted automata
</p>
<p>Weighted automata can be used to recognize more complex languages than just regular lan-
guages (Cortes and Mohri2000). The definition of recognition with weighted automata is a
natural generalization of that of recognition with unweighted automata. Let &#9; be a subset of the
weight set &#10; over which the automaton has been defined. A string &#11; is said to be &#9; -accepted by
the automaton &#12; when the sum 1 of the weights of all paths in &#12; labeled with &#11; is an element of
&#9; . &#9; is often chosen to be a singleton which makes it possible to test in constant time if a weight
is in &#9; .
</p>
<p>An example of a non-regular language that can be recognized by a weighted automaton is the
language of regular expressions. The description of that language in formal language theory
courses is often confusing since it is more powerful (it is context-free) than the set of objects
it is meant to describe (regular languages). This forces the introduction of the more general
concepts of context-free grammars and parsing to give a full description of the conceptually
simpler regular expressions.
</p>
<p>Figure 10 shows a simple automaton that &#13; -recognizes the language of regular expressions over
the alphabet &#1; a,b &#6; . The semiring considered here is &#14; &#15;&#17;&#16;&#18;&#1;&#20;&#19;&#21;&#6;&#22;&#3; &#23;&#25;&#24; &#26;&#4;&#3; fffi&#3; &#19;&#21;&#3; &#13;ffifl , the tropical
semiring. Thus, a string &#11; is accepted by that automaton iff the minimum weight of a path
labeled with &#11; is &#13; . This membership test can be performed in linear time.
</p>
<p>1The sum here corresponds to the first operation of the semiring &#31; .</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Language Processing with Weighted Transducers
</p>
<p>5 Conclusion
</p>
<p>We gave a brief survey of recent algorithmic and theoretical results related to the use of weighted
finite-state transducers in language processing. Weighted transducers provide compact repre-
sentations for the components or models of language processing systems. Efficient algorithms
such as composition can be used to combine these models. General optimization algorithms
help reducing their size or increasing their efficiency of use. The theoretical foundation for
weighted finite-state transducers, the theory of rational power series, combines the theory of
probabilistic modeling and classical automata theory.
</p>
<p>Acknowledgements
</p>
<p>The material presented in this paper is in large parts the result of collaboration with Corinna
Cortes, Mark-Jan Nederhof, Fernando Pereira, and Michael Riley.
</p>
<p>References
</p>
<p>Baldi, Pierre and Soren Brunak. 1998. Bioinformatics: The Machine Learning Approach
(Adaptive Computation and Machine Learning). MIT Press.
Berstel, Jean. 1979. Transductions and Context-Free Languages. Teubner Studienbucher:
Stuttgart.
</p>
<p>Cortes, Corinna and Mehryar Mohri. 2000. Context-Free Recognition with Weighted Au-
tomata. Grammars, 3(2-3).
Culik II, Karel and Jarkko Kari. 1997. Digital images and formal languages. In Grzegorz
Rozenberg and Arto Salomaa, editors, Handbook of Formal Languages. Springer, pages 599&#8211;
616.
</p>
<p>Eilenberg, Samuel. 1974. Automata, Languages and Machines, volume A. Academic Press.
</p>
<p>Grimley Evans, E. 1997. Approximating context-free grammars with a finite-state calculus.
In 35th Annual Meeting of the ACL, pages 452&#8211;459.
Johnson, M. 1998. Finite-state approximation of constraint-based grammars using left-corner
grammar transforms. In 36th Annual Meeting of the ACL and 17th International Conference
on Computational Linguistics, volume 1, pages 619&#8211;623.
</p>
<p>Mohri, Mehryar. 1997. Finite-State Transducers in Language and Speech Processing. Com-
putational Linguistics, 23:2.
</p>
<p>Mohri, Mehryar. 1998. General Algebraic Frameworks and Algorithms for Shortest-Distance
Problems. Technical Memorandum 981210-10TM, AT&amp;T Labs - Research, 62 pages.
</p>
<p>Mohri, Mehryar. 2000a. Generic Epsilon-Removal Algorithm for Weighted Automata. In
Proceedings of the Fifth International Conference on Implementation and Application of Au-
tomata (CIAA&#8217;2000), London, Ontario, Canada, July.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Mehryar Mohri
</p>
<p>Mohri, Mehryar. 2000b. Minimization algorithms for sequential transducers. Theoretical
Computer Science, 234:177&#8211;201, March.
</p>
<p>Mohri, Mehryar. 2001. Weighted Grammar Tools: the GRM Library. In Jean claude Junqua
and Gertjan van Noord, editors, Robustness in Language and Speech Technology. Kluwer
Academic Publishers, The Netherlands, pages 165&#8211;186.
</p>
<p>Mohri, Mehryar and Mark-Jan Nederhof. 2001. Regular Approximation of Context-Free
Grammars through Transformation. In Jean claude Junqua and Gertjan van Noord, editors,
Robustness in Language and Speech Technology. Kluwer Academic Publishers, The Nether-
lands, pages 153&#8211;163.
</p>
<p>Mohri, Mehryar and Fernando C. N. Pereira. 1998. Dynamic Compilation of Weighted
Context-Free Grammars. In &#0;&#2;&#1; th Meeting of the Association for Computational Linguistics
(ACL &#8217;98), Proceedings of the Conference, Montre&#769;al, Que&#769;bec, Canada. ACL.
Mohri, Mehryar, Fernando C. N. Pereira, and Michael Riley. 1996. Weighted Automata
in Text and Speech Processing. In Proceedings of the 12th biennial European Conference
on Artificial Intelligence (ECAI-96), Workshop on Extended finite state models of language,
Budapest, Hungary. ECAI.
</p>
<p>Mohri, Mehryar and Michael Riley. 1999. Integrated Context-Dependent Networks in Very
Large Vocabulary Speech Recognition. In Proceedings of the 6th European Conference on
Speech Communication and Technology (Eurospeech &#8217;99), Budapest, Hungary.
Mohri, Mehryar, Michael Riley, Don Hindle, Andrej Ljolje, and Fernando C. N. Pereira. 1998.
Full Expansion of Context-Dependent Networks in Large Vocabulary Speech Recognition. In
Proceedings of the International Conference on Acoustics, Speech, and Signal Processing
(ICASSP &#8217;98), Seattle, Washington.
Mohri, Mehryar and Richard Sproat. 1996. An Efficient Compiler for Weighted Rewrite Rules.
In
</p>
<p>&#0;&#2;&#3;
</p>
<p>th Meeting of the Association for Computational Linguistics (ACL &#8217;96), Proceedings of
the Conference, Santa Cruz, California. ACL.
Nederhof, M.-J. 2000. Practical experiments with regular approximation of context-free lan-
guages. Computational Linguistics, 26(1).
Pereira, F.C.N. and R.N. Wright. 1997. Finite-state approximation of phrase-structure gram-
mars. In E. Roche and Y. Schabes, editors, Finite-State Language Processing. MIT Press,
pages 149&#8211;173.
</p>
<p>Pereira, Fernando C. N. and Michael Riley, 1997. Finite State Language Processing, chapter
Weighted Rational Transductions and their Application to Human Language Processing. The
MIT Press.
</p>
<p>Vapnik, Vladimir. 1995. The Nature of Statistical Learning Theory. Springer-Verlag: Berlin-
New York.</p>

</div></div>
</body></html>