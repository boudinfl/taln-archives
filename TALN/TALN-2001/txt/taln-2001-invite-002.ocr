TALN 2001, Tours, 2-5 juillet 2001

Analyse syntaxique automatique de langues :
du combinatoire au calculatoirel

Jacques Vergne

GREYC - Université de Caen
BP 5186 - 14032 Caen cedex
Jacques.Vergne@info.unicaen.fr

Résumé — Abstract

Nous proposons de montrer comment l'analyse syntaxique automatique est aujourd'hui a un
tournant de son évolution, en mettant l'accent sur l'éVolution des modeles d'analyse
syntaxique : de l'analyse de langages de programmation (compilation) a l'analyse de langues,
et, dans le cadre de l'analyse de langues, de l'analyse combinatoire a l'analyse calculatoire, en
passant par le tagging et le chunking (synthese en section 4). On marquera d'abord le poids
historique des grammaires formelles, comme outil de modélisation des langues et des
langages formels (section 1), et comment la compilation a été transposée en traduction
automatique par Bernard Vauquois. On analysera ensuite pourquoi il n'a pas été possible
d'obtenir en analyse de langue un fonctionnement analogue a la compilation, et pourquoi la
complexité linéaire de la compilation n'a pas pu étre transposée en analyse syntaxique
(section 2). Les codes analysés étant fondamentalement différents, et le tagging ayant montré
la Voie, nous en avons pris acte en abandonnant la compilation transposée : plus de
dictionnaire exhaustif en entrée, plus de grammaire formelle pour modéliser les structures
linguistiques (section 3). Nous montrerons comment, dans nos analyseurs, nous avons
implémenté une solution calculatoire, de complexité linéaire (section 5). Nous conclurons
(section 6) en pointant quelques évolutions des taches de l'analyse syntaxique.

In this paper, we intend to show how automatic parsing is today at a change of direction. We
will stress the evolution of parsing models : from programming language parsing
(compilation) to natural language parsing, and, within the frame of natural language parsing,
from combinatory parsing to calculatory parsing, while going through tagging and chunking
(synthesis in section 4). First we stress the historical weight of formal grammars, as a
modelling tool for natural languages and formal languages (section 1), and how compilation
has been transposed into machine translation by Bernard Vauquois. Then we analyse why it
was not possible to get a natural language parsing which works the same way as compilation,
and why the linear complexity of compilation could not be transposed into NL parsing

1 Cet article consiste en une synthese du contenu de ma conference invite'e a TALN'2001. La presentation est

disponible sur 2 http: //www. info . uni caen . fr/~ jvergne/TALN200l_JV. ppt

Jacques Vergne

(section 2). Since parsed languages are radically different, and since tagging showed the right
way, we decided to abandon the transposed compilation : no more exhaustive dictionary as
input, no more formal grammar to model linguistic structures (section 3). We will show how,
in our parsers, we implemented a calculatory solution, of linear complexity (section 5). We
conclude (section 6), stressing some trends of parsing about its tasks.

     
   
    
     

Introduction / \

analyse
calculatoire

Nous proposons une étude des modeles
d'analyse syntaxique: origines, evolu-
tion, analogies, ruptures, et continuités.
Les criteres sont : le modele du code
analysé, la place de ce modele dans
l'analyseur, le processus d'analyse et sa
complexité.

tagging
chunking

analyse
combinatoire

analyse syntaxique
de langages
de nfogfammation analyse syntaxique de langues

Figure 1 : Plan général

1 Le poids historique des grammaires formelles

Les grammaires formelles permettent de modéliser des objets différents dans les trois
domaines, linguistique, informatique et TAL, ce qui entraine une certaine confusion sur leur
place en TAL :

1.1 Les grammaires formelles, outil de modélisation de la syntaxe des
langues

Noam Chomsky a eu une formation initiale de mathématicien, puis il est entré en linguistique,
dans la filiation de Harris et Bloomfield, mais en rupture avec eux sur l'objet étudié : Harris et
Bloomfield étudiaient le matériau attesté, alors que Chomsky a estimé impossible d'induire
une théorie a partir d'un matériau attesté nécessairement fini, ce qui l'a conduit a définir son
obj et comme étant la "compétence" du locuteur natif, concept qu'il a lui-méme élaboré
(Chomsky, 1965).

En 1957, il publie Syntactic Structures, un ouvrage de linguiste (Chomsky, 1957).

Pour les linguistes chomskiens, les grammaires formelles servent a modéliser la compétence
du locuteur natif, en génération (considérée comme la déduction de la démarche hypothético-
déductive). Mais, pour la communauté des Traitements Automatiques des Langues, elles
servent a modéliser la syntaxe des langues, comme matériau attesté, en analyse.

Analyse syntaxique automatique de langues .' du combinatoire au calculatoire

La divergence entre les deux courants s'est produite Vers 1971, avec la Théorie Standard
Etendue, présente dans les formalismes syntaxiques classiques en TAL (Abeillé, 1993).

1.2 Les grammaires formelles, outil de modélisation de la syntaxe des
langages de programmation

Certains concepts élaborés par Chomsky des 1957 trouvent une application inattendue en
1960, avec la création d'ALGOL 60, premier langage de programmation dont la syntaxe est
définie par une grammaire formelle (context free), ce qui guide la conception du compilateur.

ALGOL est le premier Langage de programmation Orienté "ALGorithmique" : il n'y a plus de
goto obligatoire dans les structures de contr6le, mais les structures alternative (if . . . then
. else) et répétitive (for . . . step . . . until . . . do et for . . . while . . . do).

ALGOL 60 est aussi le premier langage a structure de bloc récursif, définie par la grammaire
formelle suivante :

programme —> instruction_complexe *
instruction complexe —> instruction_simple | bloc
bloc —> instruction_complexe *

ALGOL 60 aura eu une filiation proliﬁque : Pascal, C, Ada, CH, et Java.

1.3 Triple présence des grammaires formelles en TAL

Les Traitements Automatiques des Langues sont a l'intersection de la linguistique et de
l'informatique et héritent d'une partie de leurs concepts et de leur culture. Les grammaires
formelles se retrouvent donc dans trois modeles différents dans les trois disciplines :

0 en linguistique, le modele de la "compétence" du locuteur natif, en génération

0 en inforrnatique, le modele de la syntaxe des langages de programmation

0 en TAL, le modele de la syntaxe des langues (matériau attesté), en
analyse.

Cette triple présence conduit a une situation confuse des grammaires formelles en TAL.

2 De l'analyse de langages de programmation £1 l'analyse de
langues

Bernard Vauquois fut le messager qui a transposé les grammaires formelles de l'informatique
Vers la Traduction Automatique :
2.1 Bernard Vauquois, membre du groupe ALGOL

Sept pays ont participé a la conception d'ALGOL : Allemagne, Danemark, Etats-Unis,
France, Grande-Bretagne, Pays-Bas, et Suisse. Les conférences se sont échelonnées des 1958

Jacques Vergne

: Zurich (1958), Mayenne (1958), Copenhague (1959), Paris (juin 1959, janvier 1960).
Quatorze délégués y participerent : Backus, Bauer, Green, Katz, Mc Carthy, Naur, Perlis,
Rutishauser, Salmelson, Turanski, Vauquois, Wegstein, Van Wijngaarden, Woodger.

2.2 Bernard Vauquois, directeur du CETA en 1961

Bernard Vauquois fut d'abord astronome et mathématicien, puis enseignant en informatique,
en théorie des langages formels a l'uniVersité de Grenoble. En 1961, alors qu'il Vient de
participer a la création d'ALGOL, il prend la direction du CETA (Centre d'Btudes pour la
Traduction Automatique) a Grenoble. Il fonde alors la Traduction Automatique (TA) de 26“
génération sur les principes explicites suivants :

0 utiliser la théorie des langages formels
0 fonder la Traduction Automatique sur le modele de la compilation (Boitet, 1989).

2.3 Transposition explicite de la compilation en TA par Bernard Vauquois

Nous pouvons concevoir la programmation comme la traduction par un humain d'instructions
pensées et exprimées en une langue, en instructions exprimées en langage machine destinées
a étre exécutées par un processeur; cette opération de traduction consiste en deux étapes : une
étape de traduction humaine (appelée analyse-programmation) qui conduit a l'écriture des
instructions en un langage de programmation, suivie d'une étape de traduction automatique,
appelée compilation. La compilation est une traduction automatique entre deux langages
formels, donc clos et totalement connus, ce qui permet justement d'automatiser la traduction.

La traduction automatique de langue a langue, deux codes ouverts, connus partiellement, est
pour ces raisons de nature radicalement différente de la compilation.

Or, Bernard Vauquois transpose explicitement la compilation dans la traduction automatique
de langues. Voici ce qu'écrit Christian Boitet, page 3 de l'introduction des Analectes,
"L'apport scientiﬁque de Bernard Vauquois" (Boitet, 1989) :

Au tout debut de la TA, on ﬁt l'analogie entre traduction et décodage (Warren Weaver,
1949), pour se rendre assez vite compte de son manque de pertinence. Il revient sans doute au
CETA, a l 'initiative de B. Vauquois, d'avoir introduit l 'analogie beaucoup plus féconde entre
TA et compilation. [...], on cherche seulement a transformer la forme en préservant le sens,
de meme qu’un compilateur transforme un programme en un programme equivalent, [..]

Ainsi un systeme de TA est-il vu comme une sorte de "compilateur de langue naturelle ".

2.4 Dela compilation a l'analyse de langues

La transposition de la compilation reste aujourd'hui valide dans l'analyse de langues dans son
mode classique (et combinatoire). On notera en particulier que l'on a en entrée le méme
modele du code analysé, mais que les processus sont différents :

0 méme modele du code analysé : ressources lexicales (dictionnaire) et syntaxiques
(grammaire formelle) exhaustives placées en entrée

Analyse syntaxique automatique de langues .' du combinatoire au calculatoire

0 mais processus différents :

critéres compilation analyse de langues
processus répétitif / token combinatoire
déterministe non déterministe
com lexité en tem s théori ue: ol nomiale théori ue : ex onentielle
P P 9 P Y 9 P _
pratique : linéaire pratique : polynomiale

Tableau 2 : De la compilation a l'analyse de langue : comparaison des processus
Les processus sont différents car les codes analysés sont différents : langage de
programmation Versus langue.
2.5 Quelle différence entre les deux codes analysés ?

La question qui se pose est alors : quelle est la difference entre les codes analysés qui
provoque le caractere combinatoire de l'analyse de langues ?

critéres langages de programmation langues
dictionnaire énuméré, fermé et ﬁgé ouvert et évolutif
combien d'étiquettes par 1 étiquette unique plusieurs étiquettes
token ?

Tableau 3 : Comparaison entre langages de programmation et langues

Dans une langue, un token a plusieurs étiquettes possibles : c'est la cause principale du
caractere combinatoire de l'analyse de langue (dans son mode classique)2.

3 Analyse de langues : du combinatoire au calculatoire

Dans cette section, nous rappelons par un exemple ces deux modes de résolution d'un
probleme, et nous caractérisons plus précisément comment poser et résoudre un probleme de
maniere combinatoire. Puis nous montrons comment construire une résolution calculatoire.
Nous passons ensuite au cas particulier de l'analyse syntaxique de langue : la poser et la
résoudre de maniere combinatoire, puis calculatoire. Puis nous montrons le role déterminant
du tagging, comme passerelle vers l'analyse de langue calculatoire. Enfin nous présentons les
grandes lignes de l'analyse de langue calculatoire.

2 Les autres causes de combinatoire sont a rechercher dans les autres "plusieurs" du processus d'ar1alyse :

principalement, plusieurs régles sont applicables au meme moment.

Jacques Vergne

3.1 Un exemple des deux modes de résolution
0 Voici un probléme :
unﬁls ale quart de l'age de son pere, et leur diﬂérence a"age est de 30 ans
f = p / 4 p - f = 30
0 Voici sa résolution combinatoire :

avec 0<f<lOO, 0<p<lO0 (en supposant une solution entiére) :
pour chacun des 10 000 couples (f p),
tester les 2 contraintes;
le nombre de solutions est inconnu a priori.

0 Et Voici sa résolution calculatoire :

résoudre le systéme des 2 équations a 2 inconnues, ce qui implique une solution unique :

f= p/4 et p-f=30 => 4f-f= 3f: 30 => f=l0 => p=30+f=40

3.2 Poser et résoudre un probleme : du combinatoire au calculatoire

Le terme "combinatoire" dénote une maniére de oser et résoudre un robléme mais non
7
pas le probléme lu1-méme.

Voici comment on pose un probléme de maniére combinatoire : les attributs d'un ensemble
d'unités ont plusieurs Valeurs possibles, on a des contraintes sur les Valeurs des attributs, et on
Veut trouver les Valeurs des attributs qui satisfont les contraintes. Ceci revient a poser le
probléme comme un probléme de satisfaction de contraintes (CSP).

Dans un processus de résolution combinatoire, toutes les combinaisons de Valeurs sont
constituées incrémentalement, dans un parcours d'arbre en profondeur d'abord (en analyse
syntaxique combinatoire, la profondeur correspond au nombre de mots, et l'arité d'un noeud
correspond au nombre d'étiquettes possibles d'un mot). Chaque combinaison partielle ou
complete satisfait ou non les contraintes. Si les contraintes ne sont pas satisfaites, il y a un
retour en arriére au dernier noeud satisfaisant les contraintes, et un essai d'une nouvelle Valeur
possible. La complexité théorique en temps est exponentielle selon le nombre de mots.

Dans une résolution combinatoire, on a trouvé comment vérifier la satisfaction des
contraintes sur une combinaison déja constituéez la combinaison est en entrée de la
validation par les contraintes. Le résultat est déja présent, énuméré, explicité, mais caché et
dispersé dans les ressources, et il faut l'en extraire.

   

      
     

unités combinai S0l’1S combinai sons

51 traiter de Valeurs

Validées

  
   

 

Valeurs possibles des attributs contraintes

Analyse syntaxique automatique de langues .' du combinatoire au calculatoire

Figure 4 : Résolution combinatoire d'un probleme

Pour construire une résolution calculatoire, i1 faut avoir une meilleure connaissance des
propriétés des unités a traiter et mieux exploiter cette connaissance, et trouver plus de
contraintes et la maniere de les utiliser pour calculer directement et définitivement la Valeur
d'un attribut : la "combinaison" est alors en sortie du calcul par les contraintes :

       
  

unités a traiter "combinaison"

   

calculée
contraintes

Figure 5 : Résolution calculatoire d'un probleme (calcul direct)

On peut aussi dire que, dans une résolution calculatoire, i1 faut trouver des opérations (en
petit nombre, placées en ressources et exécutées par 1'a1gorithme) portant sur des opérandes
(les unités a traiter placées en entrée). Ce qui était "contraintes" est alors Vu comme des
opérations sur les unités a traiter :

      
  
  

Opérandes résultats

 

calculés

   

opérations
Figure 6 : Résolution calculatoire d'un probleme : opérations sur des opérandes

3.3 Poser et résoudre l'analyse syntaxique de maniére combinatoire

Le probleme de l'analyse syntaxique automatique est traditionnellement posé et résolu de
maniere combinatoire.

Le probleme est ainsi posé :

0 les mots d'une phrase ont plusieurs catégories possibles,

0 toutes les Valeurs possibles des attributs des mots d'une phrase sont explicitées
"exhaustiVement" dans le dictionnaire (toutes les graphies, catégories, genres,
nombres, personnes,  des mots),

0 les contraintes sur les catégories sont les structures possibles des phrases et des
syntagmes, explicitées "exhaustivement" dans la gmmmaireformelle,

0 et, pour les mots de la phrase entrante, on Veut trouver les catégories qui satisfont les
contraintes (= "désambigu'1'sation").

Résolution : a partir des mots de la phrase entrante et du dictionnaire, un processus
combinatoire constitue des combinaisons des catégories possibles des premiers mots. Chaque
combinaison partielle est validée de maniere booléenne sur la grammaticalité du debut de la
phrase, selon la grammaire placée en entrée. Une phrase est analysée entierement si on trouve
une combinaison sur la phrase, ce qui correspond a la grammaticalité de la phrase entiere. Le

Jacques Vergne

nombre de solutions est inconnu a priori : 0, 1 ou beaucoup, sans criteres de choix entre

solutions.

3.4 Analyse combinatoire —> tagging —> analyse calculatoire

Le tableau suivant montre :

0 la rupture entre analyse combinatoire et tagging,

0 et la continuité entre tagging et analyse calculatoire :

critéres analyse de langues tagging, chunking analyse de langues
combinatoire calculatoire
structures explicitées sous forme pas de structure pas de structure
attendues d'une grammaire formelle attendue attendue
ressources exhaustives partielles régles :
syntaxiques (grammaire formelle) (régles contextuelles) condition => action
ressources exhaustives (dictionnaire) exhaustives mots grammaticaux
lexicales (ou partielles) seulement
processus arborescent, répétitif par token, répétitif par token,
combinatoire, déterministe déterministe
non déterministe
complexité en théorique : exponentielle théorique : linéaire, théorique : linéaire,
temps pratique : polynomiale pralique : linéaire pratique : linéaire
code analysé langue langue langue

Tableau 7 : De l'analyse de langues combinatoire a l'analyse de langues calculatoire, en
passant par le tagging et le chunking.

Le tagging amorce une évolution fondamentale en analyse linguistique :

0 l'abandon du modele des structures en entrée,

0 l'abandon des grammaires formelles pour modéliser les structures,
0 et l'utilisation explicite du contexte

permettent de construire un processus de complexité linéaire.

Le tagging dans sa Version classique utilise, comme l'analyse combinatoire, des ressources
lexicales supposées exhaustives, si l'on considere qu'il s'agit de réduire les listes exhaustives
d'étiquettes possibles extraites du dictionnaire, par l'application des régles contextuelles. Mais
il est aussi possible de tagger avec des ressources partielles : les mots grammaticaux (avec
une seule étiquette par défaut) qui marquent le début d'un chunk (Abney, 1991) et qui le
typent nominal ou Verbal, type qui contraint les mots suivants du chunk (Vergne, 1998b).

Analyse syntaxique automatique de langues .' du combinatoire au calculatoire

Le chunking fonctionne exactement comme le tagging, et simultanement au tagging, les deux
fonctions etant simplifiees quand on les aborde ensemble (voir ci-dessous en 5.1).

Tagging et chunking ouvrent et montrent la voie vers l'analyse de langue calculatoire : les
fonctions de segmentation et d'identiﬁcation des segments sont realisees; il suffit d'y ajouter
un processus de mise en relation des segments qui soit aussi implementable par des regles
conditions => actions3 (voir ci-dessous en 5.2).

3.5 Poser et résoudre l'analyse syntaxique de maniére calculatoire

L'ouverture de la langue est prise en compte explicitement : une langue n'est pas
caracterisable exhaustivement, contrairement a un langage de programmation; les ressources
lexicales sont minimales : mots grammaticaux, morphemes de fin de mots; il n'y a pas de
grammaire forrnelle, c'est-a-dire pas d'inVentaire exhaustif des structures attendues, et pas de
test de grammaticalite.

Le modele linguistique est dissocie du modele de traitement informatique, et n'est pas
implémenté tel quel dans sa totalite (dictionnaire, grammaire), mais il est partiellement et
implicitement utilisé dans la redaction des regles de calcul.

Le processus de calcul est execute par un moteur a base de regles, et consiste a passer des
régles "conditions => actions" une fois sur chaque unite : caracteres, tokens, syntagmes,
propositions, phrases, (paragraphes, ..., textes entiers). Les conditions portent sur les attributs
d'unites et sur les relations entre unites (contigu'1'tes, constituances et dependances). Les
actions consistent a affecter des Valeurs aux attributs, etablir des relations, et generer les
unites du niveau superieur. Le traitement est en ﬂux a debit constant, sans decouper a priori
une unite a traiter dans sa totalite telle que la phrase. Un element du ﬂux est traite
completement, une fois pour toutes, en passe unique, avant de passer a l'element suivant.

Les regles explicitent dans un méme formalisme :

0 les ressources lexicales et morphologiques partielles,
0 les proprietes contextuelles (utilisation explicite du contexte),
0 et le processus de mise en relation (voir ci-dessous en 5.2).

Le lexique et les structures syntaxiques du texte analyse sont calcules et produits en sortie. Le
processus d'analyse est de complexite lineaire.

On a ainsi une nouvelle maniere de modeliser les proprietes linguistiques des langues dans les
traitements informatiques, qui consiste a ne transposer dans le modele informatique qu'un tout
petit nombre de proprietes generales du modele linguistique, seulement celles qui servent aux

3 voir aussi la presentation et les references du tutoriel du Coling 2000 "Trends in Robust Parsing" sur

http: //www. info . unicaen . fr/~jvergne/tutorialColing2000 . html (Vergne, 2000).

Jacques Vergne

opérations des calculs. Le modele linguistique n'est plus implémenté dans sa totalité. Il est
ainsi dissocié du modele informatique4.

4 Comparaison entre modéles d'analyse syntaxique

Le tableau suivant propose une synthese comparative entre les quatre modeles d'analyse et

met en évidence :

0 les ruptures et les continuités entre modeles d'analyse syntaxique,

0 le retour a la complexité linéaire du processus d'analyse,

0 et le fait que le modele du code analysé n'est plus implémenté dans sa totalité dans le
modele informatique sous forme de ressources exhaustives placées en entrée. Seules
quelques propriétés générales du code analysé servent a définir les opérations du

calcul.
critéres compilation analyse de tagging, analyse de
langues chunking langues
combinatoire calculatoire
structures grammaire grammaire pas de structure pas de structure
attendues formelle formelle attendue attendue
ressources exhaustives exhaustives partielles régles :
syntaxiques (grammaire (grammaire (régles condition
formelle) formelle) contextuelles) => action
ressources exhaustives exhaustives exhaustives ou mots
lexicales (primitives) (dictionnaire) partielles grammaticaux
seulement
processus répétitif / token, arborescent, répétitif / token, répétitif / token,
déterministe combinatoire, calculatoire, calculatoire,
non déterministe déterministe déterministe
complexité en théorique : théorique : théorique : théorique :
temps polynomiale exponentielle linéaire, linéaire,
pratique : pratique : pratique : pratique :
linéaire polynomiale linéaire linéaire
code analysé langage formel langue langue langue

Tableau 8 : Comparaison entre les quatre modeles d'analyse syntaxique

4

Le theme de la confusion des modeles linguistique et informatique sera de'veloppe' par Thomas Lebarbe' dans
sa these de doctorat.

Analyse syntaxique automatique de langues .' du combinatoire au calculatoire

5 Quelques caractéristiques de nos analyseurs

Nous proposons de mettre l'accent sur quelques principes fondamentaux de nos analyseurs : le
calcul forrne-position, le processus de mise en relation des unités, et les méthodes pour
s'abstraire du niveau d'unité traitée, et de la langue traitée.

"Nos analyseurs" s'entend ainsi :

"l'analyseur 98" (concu et réalisé en solitaire durant et depuis ma these soutenue en 1989 :
"Analyse morpho-syntaxique automatique sans dictionnaire"), est non combinatoire
depuis 1993; c'est un analyseur de phrases du francais; il s'est placé premier a l'action
GRACE (d'octobre 1995 a novembre 1998), et a été transposé dans la synthese Vocale
KALI (V annier, 1999), commercialisée depuis 1999 par Electrel (ﬁnancement FEDER);

"l'analyseur du GREYC" a été concu (a partir du premier) et réalisé en 1999-2000, en
collaboration avec Emmanuel Giguet (Giguet, 1998) et Nadine Lucas (membres du
Groupe Syntaxe et Ingénierie Multilingue du GREYC), dans le cadre d'un transfert de
technologie vers la société DATOPS, (ﬁnancement MENRT, contrat n° 98.K.6411), et il
est actuellement en service dans les traitements de l'entreprise; c'est un analyseur
linguistique générique : les regles actuellement développées permettent l'analyse morpho-
syntaxique de phrases du francais et de l'anglais et la détection des citations (Lucas,
2000b). 11 a été développé en Java; le moteur compilé et les regles tiennent sur moins de
500 K0.

5.1 Calculer it partir des formes et des positions

On abandonne la correspondance formes - étiquettes possibles a priori hors contexte
habituellement explicitée dans le dictionnaire exhaustif. Les formes connues au début du
calcul sont les mots grammaticaux qui marquent le début d'un chunk et qui le typent nominal
ou Verbal, type qui contraint les mots suivants du chunk (V ergne, 1998b) :

Je le bois dans le bois. (exemple explicatif)

0 début et typage de chunk : J e => [ chunk Verbal dans => [ chunk nominal

[Je le bois ] [dans le bois ]

chunk Verbal chunk nominal
0 contraintes du type de chunk sur l'étiquette des mots :

[Je le

pronom

bois ] [dans le

Verbe

bois ] .

déterminant nom

Le calcul porte ainsi sur les formes et les positions, et produit la segmentation et l'étiquetage
des segments.

Jacques Vergne

5.2 Un processus de complexité linéaire pour relier les unités

Soit le ﬂux des unités (chunks par exemple) traitées au fur et a mesure de leur arrivée par le
moteur. Les conditions des regles portent sur l'unité courante et sur toute unité qui lui est
reliée (relations de contigu'1'té, constituance et dépendance). Soit une unité Virtuelle invocable
a tout moment dans les conditions des regles, et servant d'intermédiaire dans la mise en
relation par un processus en deux temps :

0 au temps 1, a l'arriVée de l'unité i, une regle 1 pose que cette unité attend une
éventuelle unité j en la reliant a l'unité Virtuelle par un lien typé;

0 au temps 2, a l'arriVée de l'unité j, une regle 2 examine si une unité i attend une unité j
en consultant les liens typés de l'unité Virtuelle a une unité i, puis supprime le lien
entre l'unité i, relie l'unité i a l'unité j par un lien du méme type, et enﬁn supprime tous
les liens a l'unité Virtuelle de toutes les unités situées entre les unités i et j (ces attentes
sont résolues et supprimées)5.

Les attentes non résolues a la ﬁn d'unités supérieures (phrases par exemple) sont supprimées
par une regle 3. Ce processus est implémenté dans les regles interprétées par le moteur, et
ainsi est aussi de complexité linéaire. Il est indépendant des unités arrivées entre les deux
unités reliées.

Du point de vue linguistique, ce processus modélise le déroulement d'une saturation de
Valence.

5.3 Comment s'abstraire de l'unité traitée ?

Les opérations sont identiques quel que soit le niveau hiérarchique des unités traitées : par
exemple, les débuts de propositions sont marqués de maniere analogue aux débuts de chunks
(Lucas, 2000a). Les deux hiérarchies des unités sont déclaratives : on a deux hiérarchies de
constituants non récursifs en miroir :

0 hiérarchie des unités physiques : document, paragraphes, mots

0 hiérarchie des unités calculées : mots, tokens, chunks, propositions, phrases

5.4 Comment s'abstraire de la langue ?

Certaines propriétés du ﬂux entrant sont indépendantes de sa langue :

0 le ﬂux entrant est touj ours unidimensionnel

0 ce sont toujours deux humains qui communiquent (c'est un processus) : méme
appareil Vocal, méme systeme cognitif, méme recherche du moindre effort
(optimisation).

Le chunk est un constituant fondé sur l'oral (un groupe accentuel), contraint par l'appareil
Vocal (Abney, 1991). Nous faisons l'hypothese que c'est un concept indépendant de la langue,

5 On pourra se reporter a l'anima11'on de la presentation pour faciliter la comprehension du processus.

Analyse syntaxique automatique de langues .' du combinatoire au calculatoire

hypothese corroborée par les travaux de Hervé Déjean (Déjean, 1998) sur des corpus de
langues tres Variées.

Dans l'analyse automatique, comment s'abstraire de la langue du ﬂux entrant ? Un premier
paquet de regles affecte des Valeurs a des attributs de certains mots a partir de leur graphie
(une unique Valeur par défaut, Valeur initiale des calculs). Les paquets suivants provoquent
des calculs sur les attributs, calculs indépendants de la langue du ﬂux.

Dans l'analyseur du GREYC, analyseur syntaxique de l'anglais et du francais, les opérations
communes anglais - francais sont les suivantes : segmentation en propositions, mise en
relation des chunks dans les propositions, segmentation en phrases. Les regles sont communes
pour les deux langues et sont mises au point sur corpus anglais - francais.

6 Evolution des téiches de l'analyse syntaxique

Le domaine opératoire des analyseurs combinatoires correspond aux applications qui traitent
des phrases courtes (requétes des pages jaunes par exemple), soit ou une complexité non
linéaire est supportable (la TA en traitement par lot par exemple). Le domaine opératoire des
analyseurs calculatoires concerne des applications ou le traitement a debit constant est requis :
les traitements en temps réel comme la synthese Vocale, les traitements en ﬂux a haut débit,
comme l'indexation de dépéches sur internet pour la recherche d'informations.

On peut s'interroger sur la prégnance des traditions scolaires : les mots dans la phrase, les
parties du discours. Quel est le rapport avec la tache ? D'autres grains (document entier,
paragraphe, chunk), d'autres catégorisations (chunk nominal ou Verbal par exemple) sont
possibles.

Pourquoi imiter l'humain? Par exemple, dans l'éValuation du tagging, la référence est
l'imitation de l'humain étiqueteur; dans quel but ?

Au sujet des modeles linguistiques, il n'est pas nécessaire de les implémenter. Les études
linguistiques sont faites en amont, mais elles ne sont inscrites que partiellement et
implicitement dans les systemes.

La tache conditionne les traitements, et l'analyse syntaxique n'est pas toujours un premier
traitement obligatoire. On cherchera plut6t a concevoir des outils légers et rapides
(complexité linéaire obligatoire), éventuellement interactifs, utilisant des ressources
restreintes, ce qui permet des traitements peu dépendants de la Variété des langues.

La frontiere syntaxe - sémantique s'efface progressivement dans les traitements : on fait des
traitements sur les formes, en sélectionnant des documents, en coloriant des passages par un
calcul portant sur quelques marques et leurs positions relatives entre elles et dans les
segments (début, milieu, fin de segment), et c'est ensuite a l'humain utilisateur d'interpréter les
résultats.

Pour les applications de recherche d'informations, les fonctions des termes deviennent
prioritaires par rapport aux étiquettes de catégorie qui ne sont que des intermédiaires de calcul
partiellement résolus. Les intéréts des utilisateurs évoluent Vers les entités nommées, les
prises de parole, les themes ou la tonalité des documents. On s'oriente Vers l'analyse de

Jacques Vergne

documents entiers : le grain-mot devient trop fin et secondaire. Dans un ﬂux massif de
documents, un document n'est plus lui-méme qu'un grain minuscule.

Conclusion

L'analyse de langue combinatoire est issue de la transposition de la compilation, mais seul le
modele du code analysé est transposé, et, du fait que les mots d'une langue ont plusieurs
étiquettes possibles dans le dictionnaire, la faible complexité de la compilation n'est pas au
rendez-Vous : le probleme est posé et résolu comme un probleme de satisfaction de
contraintes, d'ou sa complexité théorique exponentielle. L'analyse de langue combinatoire
utilise comme ressources toutes les formes possibles des mots, des syntagmes et des phrases
sous la forme d'un dictionnaire et d'une grammaire formelle.

L'abandon de ces ressources exhaustives, associé a des calculs a partir de formes partielles et
de leurs positions, a permis de rendre l'analyse de langue calculatoire. Ces calculs n'utilisent
que quelques propriétés du modele linguistique, qui est ainsi dissocié du modele
informatique.

La rupture entre les deux paradigmes combinatoire et calculatoire s'est produite avec l'arriVée
du tagging. Est-ce une rupture de paradigme a la Kuhn (Kuhn, 1983) ?

Références
Abeillé A. (1993), Les nouvelles syntaxes, Paris, Colin.

Boitet Ch. (1989), Bernard Vauquois et la TAO —Analectes, édité par Christian Boitet, GETA,
Grenoble.

Abney S. (1991), Parsing by Chunks, In : Robert Berwick, Steven Abney and Carol Tenny

(eds.), Principle-Based Parsing, Dordrecht, Kluwer Academic Publishers, téléchargeable sur :
http: //www . sfs . nphil . uni—tuebingen . de/~abney/Abney_90e . ps . gz

Chomsky N. (1969), Structures syntaxiques, Paris, Point Seuil, trad. fr. de : (1957), Syntactic
structures, La Haye, Mouton & Co.

Chomsky N. (1971), Aspects de la théorie syntaxique, Paris, Seuil, trad. fr. par J.-C. Milner de
: (1965), Aspects of the Theory of Syntax, Cambridge, MIT Press.

Déj ean H. (1998), Concepts et algorithmes pour la découverte des structures formelles des
langues, these de doctorat de l'UniVersité de Caen.

Giguet E. (1998), Méthode pour l'anab/se automatique de structures formelle sur documents
multilingues, these de doctorat de l'UniVersité de Caen.

Kuhn Th. (1983), La structure des revolutions scientiﬁques, Paris, Flammarion, trad. fr. de :
(1962-1970), The Structure of Scientiﬁc Revolutions, Chicago, The University of Chicago
Press.

Analyse syntaxique automatique de langues .' du combinatoire au calculatoire

Lucas N. (2000a), Le changement d'éche11e dans 1’ana1yse logique, 9é”’es Rencontres
interdisciplinaires sur les systemes complexes naturels et artiﬁciels .' Representations
graphiques dans les syste‘mes complexes naturels et artiﬁciels, Rochebrune.

Lucas N. (2000b), Le role de la citation dans la structuration des articles de presse, Actes du
premier colloque d ’étudesjaponaises de l 'Université Marc Bloch, Strasbourg.

Vannier G. (1999), Etude des contributions des structures textuelles et syntaxiques pour la
prosodie .' application a un systeme de synthese vocale a partir du texte, these de doctorat de
l'Université de Caen.

Vergne J. (1989), Analyse morpho—syntaxique automatique sans dictionnaire, these de
doctorat de l'Université Paris 6.

Vergne J. (1998a), Entre arbre de dépendance et ordre linéaire, les deux processus de
transformation : linéarisation, puis reconstruction de 1'arbre, Cahiers de Grammaire, n° 23,
pp. 95-136, ERSS, Toulouse.

Vergne J., Giguet E. (1998b), Regards Théoriques sur le «Tagging», Actes de Traitement
Automatique des Langues Naturelles (TALN’98), pp. 22-31, Paris.

Vergne J. (1999), Etude et modélisation de la syntaxe des langues a l'aide de l 'ordinateur,
Analyse syntaxique automatique non combinatoire, Habilitation a Diriger des Recherches,
Université de Caen.

Vergne J. (2000), Trends in Robust Parsing, tutoriel du Coling 2000, téléchargeable sur
http: //www. info . unicaen . fr/~jvergne/tutorialColing2000 . html

