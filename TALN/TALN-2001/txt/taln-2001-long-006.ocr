TALN 2001, Tours, 2-5jui11et 2001

Intégration probabiliste de sens dans la représentation
de textes

Romaric Besangon, Antoine Rozenknop,
J ean-Cédric Chappelier et Martin Rajman
Laboratoire d’Intelligence Artiﬁcielle, Département Informatique

Ecole Polytechnique Fédérale de Lausanne
e—mai1: {Romaric.Besancon, Antoine.Rozenknop, J ean—Cedric.Chappelier, MaItin.Rajman}@epﬂ.ch

Résumé - Abstract

Le sujet du présent article est l’intégration des sens portés par les mots en contexte dans une
représentation vectorielle de textes, au moyen d’un modele probabiliste. La représentation
vectorielle considérée est le modele DSIR, qui étend le modele vectoriel (VS) standard en
tenant compte a la fois des occurrences et des co-occurrences de mots dans les documents.
L’intégration des sens dans cette représentation se fait a l’aide d’un modele de Champ de
Markov avec variables cachées, en utilisant une information sémantique dérivée de relations
de synonymie extraites d’un dictionnaire de synonymes.

Mots-clés: Désambiguisation, Sémantique Distributionnelle, Représentation Vectorielle, Re-
cherche Documentaire, Champs de Markov, algorithme EM.

The present contribution focuses on the integration of word senses in a vector representation of
texts, using a probabilistic model. The vector representation under consideration is the DSIR
model, that extends the standard Vector Space (VS) model by taking into account both occur-
rences and co-occurrences of words. The integration of word senses into the co-occurrence
model is done using a Markov Random Field model with hidden variables, using semantic in-
formation derived from synonymy relations extracted from a synonym dictionary.

Keywords: Word Sense Disambiguation, Distributional Semantics, Vector Space Representa-
tion, Information Retrieval, Markov Random Fields, EM algorithm.

1 Introduction

Les systemes pour le traitement de bases d’information textuelle de grande taille reposent
généralement sur la notion de similarité entre textes. Par exemple, les systemes de Recherche
Documentaire (RD) cherchent a calculer les similarités entre une requéte et une collection de
documents. Les techniques de classiﬁcation non supervisées reposent également sur une mesure
de similarité qui permet de construire des classes en regroupant des documents similaires.

Ces systemes de similarités textuelles utilisent en général un modele vectoriel pour la représen-
tation des documents (Salton and Buckley, 1988; Rajman et al., 2000) pour dériver une mesure

R. Besancon, A. Rozenknop, J .-C. Chappelier, M. Rajman

de similarite (au sens mathematique) dans l’espace vectoriel considere (par exemple, le cosi-
nus de l’angle entre les vecteurs representant les documents). Les dimensions de cet espace
vectoriel sont en general associees a des unites linguistiques speciﬁques, qui peuvent par ex-
emple etre des mots, des stems ou des lemmes. Ces unites linguistiques sont appelees termes
d ’indexation. L’ idee presentee dans cet article est d’ameliorer la qualite des representations des
textes en precisant le sens des unites linguistiques polysemiques. Dans ce but, une phase de
desambiguisation semantique est integree dans le processus de representation. L’information
semantique requise pour cette etape est extraite des relations de synonymie contenues dans un
dictionnaire de synonymes.

Dans la section 2, nous presentons rapidement le modele DSIR (Distributional Semantics for
Information Retrieval — Recherche Documentaire a base de Semantique Distributionnelle) pour
la representation de textes. Dans la section 3, nous presentons une methode pour deriver une
notion de concepts a partir d’un ensemble de relations de synonymie. La section 4 traite de
l’integration de ces concepts dans le modele probabiliste, et de l’utilisation d’un algorithme
de type EM pour estimer les parametres de ce modele. Finalement, dans la section 5, nous
presentons les premiers resultats pour la validation de cette approche, appliquee a la Recherche
documentaire.

2 Le modéle DSIR de représentation de textes

Les modeles vectoriels standards representent les documents par un vecteur, appele proﬁl lexi-
cal, dont chaque composante represente l’importance dans le document du terme d’indexation
associe a cette dimension. La notion d’importance (i. e. la composante du vecteur) se traduit
habituellement par une valeur qui depend de la frequence du terme dans le document et, even-
tuellement, d’informations globales comme la frequence en documents (Salton and Buckley,
1988; Singhal, 1997). L’idee du modele DSIR, fonde sur la notion de Sémantique Distri-
butionnelle, est d’integrer plus d’information semantique dans la representation au moyen de
l’utilisation de co-occurrences de termes (Rungsawang and Rajman, 1995; Rajman et al., 2000).

L’ idee d’origine de la semantique distributionnelle est que le mot prend son sens en contexte et
que le sens d’un mot est donc relie a l’ensemble des contextes dans lequel il apparait (Rajman
and Bonnet, 1992). Dans l’approche DSIR, les contextes sont pris en compte au moyen des
frequences de co-occurrence. Une unite linguistique Uj est representee par son proﬁl de co-
occurrence (cﬂ, . . . ,cjM), o1‘1 M est la taille de l’ensemble des termes d’indexation et Cji est
la frequence de co-occurrence entre Uj et le terme d’indexation ti au sein d’une unite textuelle
predeﬁnie (fenetre de taille ﬁxe, phrase, paragraphe, etc). Les documents sont alors representes
par la moyenne ponderee des proﬁls de co-occurrence des mots qu’ils contiennent, i. e. d =

(d1, . . . ,dM), Oil
di = Z fj Cjz‘
uj EU
U etant l’ensemble de toutes les unites linguistiques, et f j la frequence de Uj dans le document d.

Comme montre dans (Besancon et al., 1999; Rajman et al., 2000), le modele DSIR a egalement
une interpretation probabiliste.

La facon de calculer les frequences de co-occurrence depend des relations de co-occurrence
considerees. L’ approche la plus simple est de calculer toutes les co-occurrences entre toutes les
unites linguistiques dans une phrase ou une fenetre de taille donnee sur un corpus de reference.

Intégration probabiliste de sens dans la représentation de textes

Une approche plus ﬁne passe par l’utilisation d’une information syntaxique supplémentaire
permettant de produire des groupes syntaxiques (associés chacun a une unité linguistique par-
ticuliere considérée comme téte du groupe) : on ne considére alors que les co-occurrences a
l’intérieur d’un groupe syntaxique ou entre tétes de différents groupes (Besancon et al., 1999),
ce qui permet d’éviter de tenir compte de co-occurrences entre unités linguistiques non syn-
taxiquement liées. Dans tous les cas, la phrase est transformée en un graphe de co-occurrence
dans lequel les noeuds sont associés aux unités linguistiques considérées et les arcs représentent
les relations de co-occurrence.

Prenons par exemple la phrase : "Chat échaude’ craint l’eau froide. ". Apres un prétraitement
permettant de ne garder que les lemmes des mots pleinsl, l’information de co-occurrence peut
étre calculée et les graphes de co-occurrences, correspondant a la prise en compte ou non d’un
ﬁltrage syntaxique, sont présentés dans la Figure 1.

chat — échaudé chat — échaudé
craindre — eau —fr0id craindre — eau —fr0id
V
chat échaudé craindre eau froid ( *chat échaudé) ( *craindre) ( *eau froid)
(3) (b)

Figure 1: Exemples de graphes de co-occurrences , (a) sans ﬁltrage syntaxique, (b) avec ﬁltrage
syntaxique (les étoiles indiquent les tétes des groupes syntaxiques).

L’ objectif de l’intégration des sens dans le modele de représentation des textes est de proposer
un espace de représentation fondé sur les sens, c’est-a-dire un espace vectoriel dont chaque
dimension est associée a un sens et non plus a un mot (terme d’indexation). Dans le cadre du
modele DSIR, cette intégration passe par le calcul des co-occurrences entre sens au lieu des co-
occurrences entre mots, permettant de construire des représentations sémantiques plus riches
dans les proﬁls de co-occurrence.

Les sens qui sont associés aux dimensions de l’espace sont dérivés d’un ensemble de relations
de synonymie extraites d’un dictionnaire de synonymes. La section suivante présente une me-
thode de construction de ces sens a partir des données brutes (et ambigues) d’un dictionnaire de
synonymes.

3 Désambiguisation d’un ensemble de relations de synonymie

Un dictionnaire de synonymes peut étre considéré comme un ensemble de mots U (entrées du
dictionnairez) associés chacun a un ensemble de listes de synonymes. Par exemple, quelques
sens du mot froid et les listes de synonymes correspondantes sont (selon le dictionnaire des
synonymes Hachette) :

lpar exemple, on peut décider de ne garder que les lemmes des noms, des Verbes et des adjectifs.
zcet ensemble correspond ici a l’ensemble d’unités linguistiques considéré dans la section 2.

R. Besancon, A. Rozenknop, J .-C. Chappelier, M. Rajman

Sens 1 => calme, de marbre, ﬂegmatique, impossible, imperturbable, indiﬁérent, insensi-
ble, marmoréen;

Sens 2 => dédaigneux, distant, ﬁer, hautain, refrigerant, renfermé, reserve, sec;

Sens 3 => austere, glace’, inexpressif, monotone, nu, plat, sec, severe, simple, teme;

Considérons alors la relation est-synonyme-de3. Comme, dans les dictionnaires standards, la
transitivité n’est en général pas garantie pour cette relation, on construit tout d’abord la ferme-
ture transitive de la relation est-synonyme-de, puis on extrait les sous-graphes correspondant a
des composantes connexes. Par construction, une telle composante connexe contient alors un
ensemble de sens recoupant une meme notion. Cet ensemble de sens connexes sera appelé un
concept.

Cependant, de facon générale, les mots apparaissant dans la liste des synonymes fournie par un
dictionnaire sont souvent eux-meme polysémiques et doivent étre désambiguisés pour permettre
la construction des concepts : dans l’exemple précédent, le mot sec est synonyme de deux sens
différents de froid. On suppose donc que ce mot a lui-meme au moins deux sens différents et
des approches heuristiques doivent étre envisagées pour associer un sens unique a chacun des
mots au sein des listes de synonymes.

Notons wZ.1,. . . ,wZ7” les ni sens d’un mot wi, et syn(wf) C U l’ensemble de synonymes du
mot wi dans son sens j. L’heuristique utilisée ici est d’associer a chaque mot w, E syn(w§€)
le sens wf pour lequel l’intersection entre la liste des synonymes représentant wf et la liste des
synonymes représentant wig est maximale. Plus précisément, le sens j* associé a w, est déﬁni
par :

j* = arggnax \syn<w:> n {w%. u syn<w%.> \ wf}

A l’aide d’une telle heuristique, il est alors possible d’associer a chaque mot un ensemble de
concepts possibles (les différents sens du mot) et a chaque concept un ensemble de mots (les
réalisations du concept).

Le probleme principal de l’intégration des sens dans la représentation des textes est alors de
choisir les bons concepts a associer aux mots d’un corpus d’apprentissage, pour le calcul des
co-occurrences entre concepts.

4 Integration des sens dans le modele DSIR

L’ approche choisie pour la désambiguisation sémantique des mots est de suivre l’intuition qui
est a la base de la Sémantique Distributionnelle : on fait l’hypothese que la séquence de con-
cepts a associer a une séquence de mots est celle qui maximise la probabilité d’affecter des
concepts aux noeuds du graphe de co-occurrence dérivé de la séquence de mots. En d’autres
termes, pour une conﬁguration de mots w associée aux noeuds d’un graphe de co-occurrence g,
la conﬁguration de concepts associée 0* est telle que

0* = argmaXp(0|w, 9) = argmaxmwlc, 9) p(0|9)
C C

3on suppose que cette relation est symétrique, ce qui correspond a l’intuition.

Intégration probabiliste de sens dans la représentation de textes

On notera, pour tout graphe g, N9 l’ensemble de ses noeuds, 14.g l’ensemble de ses arcs, et
wl, . . . ,wlN-9' (resp. cl, . . . ,c'N-9') une conﬁguration de mots (resp. de concepts) associée aux

noeuds du graphe.

Pour calculer p(w|c, g) p(c| g), on fait les hypotheses suivantes :

0 H1: le conditionnement impose a l’association d’un mot a un noeud est limité au seul
concept associé a ce noeud, ce qui se traduit par : p(w |c, g) =  p(w‘|c‘);

o H2: le conditionnement de l’association d’un concept a un noeud du graphe est limité
aux concepts associés aux noeuds voisins : p(CZ|(Cj)jENg) = p(c‘|(c3),~Ey2.), o1‘1 V, est le
voisinage du noeud 71 dans le graphe g.

L’ hypotyhese (H2) de conditionnement limité au voisinage, induit une structure de Champ de
Markov (Markov Random Field — MRF) pour le modele probabiliste. D’apres le théoreme de
Hammerlsey (Besag, 1974), la distribution de probabilité p(c|g) est alors une distribution de
Gibbs, c’est-a-dire une distribution de la forme :

p(c|9) = Zioexpzvm

*yEI‘

ou Z0 est un facteur de normalisation (Z 0 = 26 exp 761. V7 (c)]), F est un ensemble de cliques
dans g et V7 (0) est une fonction, appelée potentiel, qui associe une valeur réelle a chacune de ces
cliques. Comme, dans notre cas, on ne regarde que des relations de co-occurrence, les cliques
considérées sont d’ordre au plus 2, et on peut donc réécrire p(c| g) comme:

p<clg>=Zi0exp §;v<ci>+ Z V(c‘}c’")

iezv, (i,j)eAg

La probabilité a maximiser pour obtenir la conﬁguration de concepts optimale 0* est donc :

p<wlc,g> mom) = gm 2 v;<wici> + Z W‘) + Z v<cic">

iezv, iEN_,, (i,j)eAg

ou Ve wi, cl est le otentiel associé a la robabilité d’éIr1ission p wi cl .
P P

Cette distribution de probabilité correspond a un modele paramétrique, dont l’ensemble des
parametres est 0 = {(1/(CZ'))c2.EC,(V(Ci,Cj))ci7cjEc,(1/;g('U}i,Cj))wiEU,cjEC}s ou C est l’ensemble
des concepts. Pour trouver des valeurs pertinentes pour ces parametres, on utilise l’approche
standard qui consiste a choisir les valeurs qui maximisent la log-vraisemblance du corpus
d’apprentissage C (les graphes de co-occurrence g étant donnés par une analyse syntaxique
préalable des phrases de 0).

On utilise dans ce but un algorithme dérivé des algorithmes Estimation-Maximisation (EM)
(Dempster et al., 1977) et Improved Iterative Scaling (IIS) (Lafferty, 1996; Pietra et al., 1997),
dont l’objectif est de déterminer l’ensemble de parametres 0 qui maximise l’espérance de la
log-vraisemblance Lg(C) = Em 9) EC log pg(w| g).

R. Besancon, A. Rozenknop, J .-C. Chappelier, M. Rajman

La structure générale de cette approche itérative est la suivante : étant donné un ensemble
de parametres 0, on cherche l’ensemble de parametre 0’ qui maximise l’espérance de la log-
vraisemblance sachant les parametres 0. Cela revient a maximiser la fonction Q(0, 0’) suivante :

we’): 2 Z Pa(0|wa9)10g 

(w,g)EC cECg(w) paw’ 0'9)

o1‘1 C_,,(w) est l’ensemble des conﬁgurations de concepts possibles sur le graphe g, sachant les
mots associés aux noeuds du graphe.

Dans le cas d’une distribution de Gibbs, la log-vraisemblance est difﬁcile a calculer, en raison
du caractere global de la constante de normalisation Z, trop coﬁteuse a évaluer. L’ approche
généralement utilisée (en particulier dans le domaine du traitement d’image) pour résoudre
ce probleme est de remplacer la vraisemblance par une pseudo-vraisemblance (Besag, 1974;
Chalmond, 1989), déﬁnie comme le produit des probabilités conditionnelles :

PLa(0|9) = H P(0i|(07.)jeV.-)

iENg
Il a été montré que l’estimateur ainsi obtenu est aussi consistant que l’estimateur du maximum

de vraisemblance (Gidas, 1988).

Pour ce qui est de la distribution jointe des variables observées et cachées, la pseudo-vrai-
semblance est déﬁnie par (Chalmond, 1989) :

PLg(’LU, c|g) = pg(w|c)PLg(c|g)
et on cherche alors a maximiser la fonction Q(0, 0’) suivante :

me’): 2 Z pa<clw,g>1og  <1)

(w,g)EC cECg(w)

Comme il n’est en général pas possible de maximiser directement cette expression, l’approche
usuelle est d’utiliser une fonction auxiliaire A(0, 0’) qui constitue une borne inférieure pour la
fonction Q(0, 0’)4.

En effectuant les calculs a partir de la déﬁnition (1), on peut séparer la fonction Q en deux
fonctions que l’on po11rra maximiser séparément :

Q(07 0,) =  0,) + QC'(07 0,)

QE(0, 0') = Z(w,g)ec Zcec_,(w)Pa(0|7~U; g> log P’ (‘"1"’)

pg(w|c
avec
PLg« (cfg
QC(07 9') = Z(w,g)ec ZCECg(w)p0(Ciw7g) 10% ﬁwlg)

4maxi1m'ser A(t9, 0’ ), si ce maximum est strictement positif, assure que les paramétres 0’ déﬁnissent un meilleur

modéle que 0.

Integration probabiliste de sens dans la representation de textes

Et on derive ainsi les fonctions auxiliaires suivantes (en explicitant les valeurs de pg(w|c) et
PLg(c|g)) :

AE(0,0’)=: Z pg(c|w,g)Z

(w,g) cECg(w) iENg

AC(0,0’)=: Z pg(c|w,g)Z
(

w,g) cECg(w) iENg

 

Al/e(w”., cl) — Z pg(w|ci) exp Al/e(w, 

weci

Ax/(oi) + Z Ax/(oi, 07)
16172’

 

Z

_ Z pg(Ck|(Cj)jEyi))\i  exp  Al/(Ck,  + €Xp  

ckEC ‘ jev.

o1‘1 les AV sont les differences de potentiel entre les modeles 0’ et 0, et A, est le nombre de
voisins du noeudi augmente de 1 (A, =  + 1).

Les derivees $%ﬂ ne dependent alors que des Al/e(w, c). On peut donc trouver les para-

metres Ve(w, 0) en calculant iterativement les valeurs Al/e(w, c) qui maximisent A E(0, 0’ ) (en
utilisant des methodes comme l’algorithme de Newton pour resoudre les equations %% =
0) et remplacer Ve(w, 0) par Ve(w, c) + Al/e(w, c) jusqu’a convergence. On fait de meme pour

les parametres AV(c) et AV(c,~, Cj).

Une fois les parametres optimaux obtenus, on peut, pour chacune des phrases du corpus, cal-
culer la conﬁguration de concepts la plus probable qui lui est associee. On obtient donc un
corpus compose de graphes de co-occurrences dont les noeuds sont associes a des concepts, a
partir duquel on peut calculer les co-occurrences necessaires a la representation des textes dans
l’espace des concepts.

5 Evaluation

L’ approche presentee dans cet article a ete evaluee pour une tache de Recherche Documentaire,
en utilisant un corpus d’articles du journal La Monde, provenant de la campagne d’evaluation
AMARYLLIS. Cette evaluation n’est pas directement centree sur les resultats obtenus pour la
desambigui'sation semantique (c’est-a-dire l’affectation des concepts les plus probables a une
phrase), mais plutot sur l’impact de l’utilisation des concepts produits pour une application du
type Recherche Documentaire. Pour cette tache, les documents de la base documentaire et
les requétes sont representes dans l’espace vectoriel des concepts, a l’aide d’un modele DSIR
utilisant les co-occurrences entre concepts au lieu des co-occurrences entre mots.

Le travail de desambigui'sation d’une liste de synonymes derivee d’un dictionnaire de syn-
onymes a ete Inise en oeuvre separement et un ensemble de concepts en ont ete derivees (Pﬁster,
2000).

Pour la tache de Recherche Documentaire, un lexique de 7073 mots (lemmes) est utilise, qui
correspond a un total de 2936 concepts (le nombre moyen de mots par concepts est 2.7). Le
corpus de reference est compose de 9574 documents et de 4 requétes. Les resultats, en termes
de precision/rappel, presentes en Figure 2 montrent une amelioration des performances lors de
l’utilisation de la representation en concepts des documents et des requétes (courbe "c0ncepts")
par rapport a la representation en mots (courbe "m0ts"). Un test supplementaire a ete effec-
tue en associant le meme concept a toutes les occurrences d’un mot donne, par exemple, en

R. Besancon, A. Rozenknop, J .-C. Chappelier, M. Rajman

concepts *9-
_ mots —+——-
conc--pts simples —B——

0.8

0.6

Precision

0.4 ‘X;

0.2

 

Rappel

Figure 2: Resultats de l’integration des sens pour une tache de Recherche Documentaire sur le
corpus "Le Monde".

prenant toujours le premier de la liste des concepts possibles du mot. Les resultats de ce test
(representes par la courbe "concepts simples") montrent que cette representation n’ameliore pas
les performances par rapport a la representation directe en mots, ce qui semble conﬁrmer que
l’amelioration des performances est effectivement due a la pertinence de l’association des con-
cepts aux mots par la methode presentee. D’autres experiences de validation sont neanmoins
necessaires pour fournir plus de details sur les caracteristiques de la methode qui expliquent
cette amelioration.

6 Conclusion

Nous presentons dans cet article un modele de representation a base de Champ de Markov per-
mettant d’integrer les sens des mots dans un modele probabiliste de representation de textes. Les
champs de Markov semblent fournir un bon cadre pour la representation de ce type d’informa-
tion de voisinage non-oriente, et peuvent egalement étre envisages pour la modelisation directe
des co-occurrences de mots dans la representation de documents. Les premiers resultats pour
l’evaluation de cette representation pour une tache de recherche documentaire montrent une
amelioration des performances, mais une evaluation plus poussee devrait etre Inise en oeuvre.
D’autre part, la disponibilite d’un modele de co-occurrence de concepts permet egalement de
mettre en oeuvre des techniques de desambiguisation semantique, et ce modele devrait aussi étre
evalue precisement pour cette seule tache, par une methode adaptee, et pourrait etre compare a
d’autres techniques de desambigu'1'sation.

Integration probabiliste de sens dans la representation de textes

Références

Besag, J. (1974). Spatial interaction and the statistical analysis of lattice systems. Journal of Royal
Statistics Society, 36:192-236.

Besancon, R., Rajman, M., and Chappelier, J .—C. (1999). Textual similarities based on a distributional
approach. In International Workshop on Similarity Search (IWOSS99), Florence, Italy.

Chalmond, B. (1989). An iterative gibbsian technique for reconstruction of m—ary images. Pattern
Recognition, 22(6):747-761.

Dempster, A., Laird, N., and Rubin, D. (1977). Maximum likelihood from incomplete data via the EM
algorithm. Jounal of Royal Statistics Society, 39:185-197.

Gidas, B. (1988). Consistency of maximum likelihood and pseudolikelihood estimators for gibbs distri-
butions. In Fleming, W. and Lions, P., editors, Stochastic Differential System, Stochastic Control Theory
and Applications, pages 129-145. Springer, New York.

Lafferty, J . (1996). Gibbs—markov models. Computing Science and Statistics, 27:370-377.
Pﬁster, J .—P. (2000). Désambiguisation d’un dictionnaire de synonymes. Technical report, EPFL.

Pietra, S., Pietra, V., and Lafferty, J . (1997). Inducing features of random ﬁelds. IEEE Transactions on
Pattern Analysis and Machine Intelligence, 19(4):380-393.

Rajman, M., Besancon, R., and Chappelier, J .—C. (2000). Le modele DSIR : Une approche a base de
sémantique distributionnelle pour la recherche doc11mentaire. Traitement Automatique des Langues,
41(2).

Rajman, M. and Bonnet, A. (1992). Corpora—base linguistics: new tools for natural language processing.
In I st Annual Conference of the Association for Global Strategic Information, Bad Kreuznach, Germany.

Rungsawang, A. and Rajman, M. (1995). Textual information retrieval based on the concept of distri-
butional semantics. In proc. of JADT’95 (3rd International Conference on Statistical Analysis of Textual
Data), Rome.

Salton, G. and Buckley, C. (1988). Term weighting approaches in automatic text retrieval. Information
Processing and Management, 24:513-523.

Singhal, A. (1997). Term Weighting Revisited. PhD thesis, Department of Computer Science, Cornell
University.

