<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Besag</author>
</authors>
<title>Spatial interaction and the statistical analysis of lattice systems.</title>
<date>1974</date>
<journal>Journal of Royal Statistics Society,</journal>
<pages>36--192</pages>
<contexts>
<context position="12625" citStr="Besag, 1974" startWordPosition="1942" endWordPosition="1943">les hypotheses suivantes : 0 H1: le conditionnement impose a l’association d’un mot a un noeud est limité au seul concept associé a ce noeud, ce qui se traduit par : p(w |c, g) = p(w‘|c‘); o H2: le conditionnement de l’association d’un concept a un noeud du graphe est limité aux concepts associés aux noeuds voisins : p(CZ|(Cj)jENg) = p(c‘|(c3),~Ey2.), o1‘1 V, est le voisinage du noeud 71 dans le graphe g. L’ hypotyhese (H2) de conditionnement limité au voisinage, induit une structure de Champ de Markov (Markov Random Field — MRF) pour le modele probabiliste. D’apres le théoreme de Hammerlsey (Besag, 1974), la distribution de probabilité p(c|g) est alors une distribution de Gibbs, c’est-a-dire une distribution de la forme : p(c|9) = Zioexpzvm *yEI‘ ou Z0 est un facteur de normalisation (Z 0 = 26 exp 761. V7 (c)]), F est un ensemble de cliques dans g et V7 (0) est une fonction, appelée potentiel, qui associe une valeur réelle a chacune de ces cliques. Comme, dans notre cas, on ne regarde que des relations de co-occurrence, les cliques considérées sont d’ordre au plus 2, et on peut donc réécrire p(c |g) comme: p&lt;clg&gt;=Zi0exp §;v&lt;ci&gt;+ Z V(c‘}c’&amp;quot;) iezv, (i,j)eAg La probabilité a maximiser pour obten</context>
<context position="15138" citStr="Besag, 1974" startWordPosition="2339" endWordPosition="2340"> 0. Cela revient a maximiser la fonction Q(0, 0’) suivante : we’): 2 Z Pa(0|wa9)10g (w,g)EC cECg(w) paw’ 0&apos;9) o1‘1 C_,,(w) est l’ensemble des conﬁgurations de concepts possibles sur le graphe g, sachant les mots associés aux noeuds du graphe. Dans le cas d’une distribution de Gibbs, la log-vraisemblance est difﬁcile a calculer, en raison du caractere global de la constante de normalisation Z, trop coﬁteuse a évaluer. L’ approche généralement utilisée (en particulier dans le domaine du traitement d’image) pour résoudre ce probleme est de remplacer la vraisemblance par une pseudo-vraisemblance (Besag, 1974; Chalmond, 1989), déﬁnie comme le produit des probabilités conditionnelles : PLa(0|9) = H P(0i|(07.)jeV.-) iENg Il a été montré que l’estimateur ainsi obtenu est aussi consistant que l’estimateur du maximum de vraisemblance (Gidas, 1988). Pour ce qui est de la distribution jointe des variables observées et cachées, la pseudo-vraisemblance est déﬁnie par (Chalmond, 1989) : PLg(’LU, c|g) = pg(w|c)PLg(c|g) et on cherche alors a maximiser la fonction Q(0, 0’) suivante : me’): 2 Z pa&lt;clw,g&gt;1og &lt;1) (w,g)EC cECg(w) Comme il n’est en général pas possible de maximiser directement cette expression, l’a</context>
</contexts>
<marker>Besag, 1974</marker>
<rawString>Besag, J. (1974). Spatial interaction and the statistical analysis of lattice systems. Journal of Royal Statistics Society, 36:192-236.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Besancon</author>
<author>M Rajman</author>
<author>J —C Chappelier</author>
</authors>
<title>Textual similarities based on a distributional approach.</title>
<date>1999</date>
<booktitle>In International Workshop on Similarity Search (IWOSS99),</booktitle>
<location>Florence, Italy.</location>
<contexts>
<context position="5707" citStr="Besancon et al., 1999" startWordPosition="840" endWordPosition="843">te linguistique Uj est representee par son proﬁl de cooccurrence (cﬂ, . . . ,cjM), o1‘1 M est la taille de l’ensemble des termes d’indexation et Cji est la frequence de co-occurrence entre Uj et le terme d’indexation ti au sein d’une unite textuelle predeﬁnie (fenetre de taille ﬁxe, phrase, paragraphe, etc). Les documents sont alors representes par la moyenne ponderee des proﬁls de co-occurrence des mots qu’ils contiennent, i. e. d = (d1, . . . ,dM), Oil di = Z fj Cjz‘ uj EU U etant l’ensemble de toutes les unites linguistiques, et f j la frequence de Uj dans le document d. Comme montre dans (Besancon et al., 1999; Rajman et al., 2000), le modele DSIR a egalement une interpretation probabiliste. La facon de calculer les frequences de co-occurrence depend des relations de co-occurrence considerees. L’ approche la plus simple est de calculer toutes les co-occurrences entre toutes les unites linguistiques dans une phrase ou une fenetre de taille donnee sur un corpus de reference. Intégration probabiliste de sens dans la représentation de textes Une approche plus ﬁne passe par l’utilisation d’une information syntaxique supplémentaire permettant de produire des groupes syntaxiques (associés chacun a une uni</context>
</contexts>
<marker>Besancon, Rajman, Chappelier, 1999</marker>
<rawString>Besancon, R., Rajman, M., and Chappelier, J .—C. (1999). Textual similarities based on a distributional approach. In International Workshop on Similarity Search (IWOSS99), Florence, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Chalmond</author>
</authors>
<title>An iterative gibbsian technique for reconstruction of m—ary images.</title>
<date>1989</date>
<journal>Pattern Recognition,</journal>
<pages>22--6</pages>
<contexts>
<context position="15155" citStr="Chalmond, 1989" startWordPosition="2341" endWordPosition="2342">ent a maximiser la fonction Q(0, 0’) suivante : we’): 2 Z Pa(0|wa9)10g (w,g)EC cECg(w) paw’ 0&apos;9) o1‘1 C_,,(w) est l’ensemble des conﬁgurations de concepts possibles sur le graphe g, sachant les mots associés aux noeuds du graphe. Dans le cas d’une distribution de Gibbs, la log-vraisemblance est difﬁcile a calculer, en raison du caractere global de la constante de normalisation Z, trop coﬁteuse a évaluer. L’ approche généralement utilisée (en particulier dans le domaine du traitement d’image) pour résoudre ce probleme est de remplacer la vraisemblance par une pseudo-vraisemblance (Besag, 1974; Chalmond, 1989), déﬁnie comme le produit des probabilités conditionnelles : PLa(0|9) = H P(0i|(07.)jeV.-) iENg Il a été montré que l’estimateur ainsi obtenu est aussi consistant que l’estimateur du maximum de vraisemblance (Gidas, 1988). Pour ce qui est de la distribution jointe des variables observées et cachées, la pseudo-vraisemblance est déﬁnie par (Chalmond, 1989) : PLg(’LU, c|g) = pg(w|c)PLg(c|g) et on cherche alors a maximiser la fonction Q(0, 0’) suivante : me’): 2 Z pa&lt;clw,g&gt;1og &lt;1) (w,g)EC cECg(w) Comme il n’est en général pas possible de maximiser directement cette expression, l’approche usuelle e</context>
</contexts>
<marker>Chalmond, 1989</marker>
<rawString>Chalmond, B. (1989). An iterative gibbsian technique for reconstruction of m—ary images. Pattern Recognition, 22(6):747-761.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Dempster</author>
<author>N Laird</author>
<author>D Rubin</author>
</authors>
<title>Maximum likelihood from incomplete data via the EM algorithm.</title>
<date>1977</date>
<journal>Jounal of Royal Statistics Society,</journal>
<pages>39--185</pages>
<contexts>
<context position="14037" citStr="Dempster et al., 1977" startWordPosition="2166" endWordPosition="2169">n p wi cl . P P Cette distribution de probabilité correspond a un modele paramétrique, dont l’ensemble des parametres est 0 = {(1/(CZ&apos;))c2.EC,(V(Ci,Cj))ci7cjEc,(1/;g(&apos;U}i,Cj))wiEU,cjEC}s ou C est l’ensemble des concepts. Pour trouver des valeurs pertinentes pour ces parametres, on utilise l’approche standard qui consiste a choisir les valeurs qui maximisent la log-vraisemblance du corpus d’apprentissage C (les graphes de co-occurrence g étant donnés par une analyse syntaxique préalable des phrases de 0). On utilise dans ce but un algorithme dérivé des algorithmes Estimation-Maximisation (EM) (Dempster et al., 1977) et Improved Iterative Scaling (IIS) (Lafferty, 1996; Pietra et al., 1997), dont l’objectif est de déterminer l’ensemble de parametres 0 qui maximise l’espérance de la log-vraisemblance Lg(C) = Em 9) EC log pg(w |g). R. Besancon, A. Rozenknop, J .-C. Chappelier, M. Rajman La structure générale de cette approche itérative est la suivante : étant donné un ensemble de parametres 0, on cherche l’ensemble de parametre 0’ qui maximise l’espérance de la logvraisemblance sachant les parametres 0. Cela revient a maximiser la fonction Q(0, 0’) suivante : we’): 2 Z Pa(0|wa9)10g (w,g)EC cECg(w) paw’ 0&apos;9) </context>
</contexts>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>Dempster, A., Laird, N., and Rubin, D. (1977). Maximum likelihood from incomplete data via the EM algorithm. Jounal of Royal Statistics Society, 39:185-197.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Gidas</author>
</authors>
<title>Consistency of maximum likelihood and pseudolikelihood estimators for gibbs distributions.</title>
<date>1988</date>
<booktitle>Stochastic Differential System, Stochastic Control Theory and Applications,</booktitle>
<pages>129--145</pages>
<editor>In Fleming, W. and Lions, P., editors,</editor>
<publisher>Springer,</publisher>
<location>New York.</location>
<contexts>
<context position="15376" citStr="Gidas, 1988" startWordPosition="2373" endWordPosition="2374">aphe. Dans le cas d’une distribution de Gibbs, la log-vraisemblance est difﬁcile a calculer, en raison du caractere global de la constante de normalisation Z, trop coﬁteuse a évaluer. L’ approche généralement utilisée (en particulier dans le domaine du traitement d’image) pour résoudre ce probleme est de remplacer la vraisemblance par une pseudo-vraisemblance (Besag, 1974; Chalmond, 1989), déﬁnie comme le produit des probabilités conditionnelles : PLa(0|9) = H P(0i|(07.)jeV.-) iENg Il a été montré que l’estimateur ainsi obtenu est aussi consistant que l’estimateur du maximum de vraisemblance (Gidas, 1988). Pour ce qui est de la distribution jointe des variables observées et cachées, la pseudo-vraisemblance est déﬁnie par (Chalmond, 1989) : PLg(’LU, c|g) = pg(w|c)PLg(c|g) et on cherche alors a maximiser la fonction Q(0, 0’) suivante : me’): 2 Z pa&lt;clw,g&gt;1og &lt;1) (w,g)EC cECg(w) Comme il n’est en général pas possible de maximiser directement cette expression, l’approche usuelle est d’utiliser une fonction auxiliaire A(0, 0’) qui constitue une borne inférieure pour la fonction Q(0, 0’)4. En effectuant les calculs a partir de la déﬁnition (1), on peut séparer la fonction Q en deux fonctions que l’o</context>
</contexts>
<marker>Gidas, 1988</marker>
<rawString>Gidas, B. (1988). Consistency of maximum likelihood and pseudolikelihood estimators for gibbs distributions. In Fleming, W. and Lions, P., editors, Stochastic Differential System, Stochastic Control Theory and Applications, pages 129-145. Springer, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lafferty</author>
</authors>
<date>1996</date>
<booktitle>Gibbs—markov models. Computing Science and Statistics,</booktitle>
<pages>27--370</pages>
<contexts>
<context position="14089" citStr="Lafferty, 1996" startWordPosition="2175" endWordPosition="2176"> a un modele paramétrique, dont l’ensemble des parametres est 0 = {(1/(CZ&apos;))c2.EC,(V(Ci,Cj))ci7cjEc,(1/;g(&apos;U}i,Cj))wiEU,cjEC}s ou C est l’ensemble des concepts. Pour trouver des valeurs pertinentes pour ces parametres, on utilise l’approche standard qui consiste a choisir les valeurs qui maximisent la log-vraisemblance du corpus d’apprentissage C (les graphes de co-occurrence g étant donnés par une analyse syntaxique préalable des phrases de 0). On utilise dans ce but un algorithme dérivé des algorithmes Estimation-Maximisation (EM) (Dempster et al., 1977) et Improved Iterative Scaling (IIS) (Lafferty, 1996; Pietra et al., 1997), dont l’objectif est de déterminer l’ensemble de parametres 0 qui maximise l’espérance de la log-vraisemblance Lg(C) = Em 9) EC log pg(w |g). R. Besancon, A. Rozenknop, J .-C. Chappelier, M. Rajman La structure générale de cette approche itérative est la suivante : étant donné un ensemble de parametres 0, on cherche l’ensemble de parametre 0’ qui maximise l’espérance de la logvraisemblance sachant les parametres 0. Cela revient a maximiser la fonction Q(0, 0’) suivante : we’): 2 Z Pa(0|wa9)10g (w,g)EC cECg(w) paw’ 0&apos;9) o1‘1 C_,,(w) est l’ensemble des conﬁgurations de con</context>
</contexts>
<marker>Lafferty, 1996</marker>
<rawString>Lafferty, J . (1996). Gibbs—markov models. Computing Science and Statistics, 27:370-377.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J —P Pﬁster</author>
</authors>
<title>Désambiguisation d’un dictionnaire de synonymes.</title>
<date>2000</date>
<tech>Technical report, EPFL.</tech>
<contexts>
<context position="18559" citStr="Pﬁster, 2000" startWordPosition="2887" endWordPosition="2888">t-a-dire l’affectation des concepts les plus probables a une phrase), mais plutot sur l’impact de l’utilisation des concepts produits pour une application du type Recherche Documentaire. Pour cette tache, les documents de la base documentaire et les requétes sont representes dans l’espace vectoriel des concepts, a l’aide d’un modele DSIR utilisant les co-occurrences entre concepts au lieu des co-occurrences entre mots. Le travail de desambigui&apos;sation d’une liste de synonymes derivee d’un dictionnaire de synonymes a ete Inise en oeuvre separement et un ensemble de concepts en ont ete derivees (Pﬁster, 2000). Pour la tache de Recherche Documentaire, un lexique de 7073 mots (lemmes) est utilise, qui correspond a un total de 2936 concepts (le nombre moyen de mots par concepts est 2.7). Le corpus de reference est compose de 9574 documents et de 4 requétes. Les resultats, en termes de precision/rappel, presentes en Figure 2 montrent une amelioration des performances lors de l’utilisation de la representation en concepts des documents et des requétes (courbe &amp;quot;c0ncepts&amp;quot;) par rapport a la representation en mots (courbe &amp;quot;m0ts&amp;quot;). Un test supplementaire a ete effectue en associant le meme concept a toutes </context>
</contexts>
<marker>Pﬁster, 2000</marker>
<rawString>Pﬁster, J .—P. (2000). Désambiguisation d’un dictionnaire de synonymes. Technical report, EPFL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Pietra</author>
<author>V Pietra</author>
<author>J Lafferty</author>
</authors>
<title>Inducing features of random ﬁelds.</title>
<date>1997</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<pages>19--4</pages>
<contexts>
<context position="14111" citStr="Pietra et al., 1997" startWordPosition="2177" endWordPosition="2180">amétrique, dont l’ensemble des parametres est 0 = {(1/(CZ&apos;))c2.EC,(V(Ci,Cj))ci7cjEc,(1/;g(&apos;U}i,Cj))wiEU,cjEC}s ou C est l’ensemble des concepts. Pour trouver des valeurs pertinentes pour ces parametres, on utilise l’approche standard qui consiste a choisir les valeurs qui maximisent la log-vraisemblance du corpus d’apprentissage C (les graphes de co-occurrence g étant donnés par une analyse syntaxique préalable des phrases de 0). On utilise dans ce but un algorithme dérivé des algorithmes Estimation-Maximisation (EM) (Dempster et al., 1977) et Improved Iterative Scaling (IIS) (Lafferty, 1996; Pietra et al., 1997), dont l’objectif est de déterminer l’ensemble de parametres 0 qui maximise l’espérance de la log-vraisemblance Lg(C) = Em 9) EC log pg(w |g). R. Besancon, A. Rozenknop, J .-C. Chappelier, M. Rajman La structure générale de cette approche itérative est la suivante : étant donné un ensemble de parametres 0, on cherche l’ensemble de parametre 0’ qui maximise l’espérance de la logvraisemblance sachant les parametres 0. Cela revient a maximiser la fonction Q(0, 0’) suivante : we’): 2 Z Pa(0|wa9)10g (w,g)EC cECg(w) paw’ 0&apos;9) o1‘1 C_,,(w) est l’ensemble des conﬁgurations de concepts possibles sur le</context>
</contexts>
<marker>Pietra, Pietra, Lafferty, 1997</marker>
<rawString>Pietra, S., Pietra, V., and Lafferty, J . (1997). Inducing features of random ﬁelds. IEEE Transactions on Pattern Analysis and Machine Intelligence, 19(4):380-393.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Rajman</author>
<author>R Besancon</author>
<author>J —C Chappelier</author>
</authors>
<title>Le modele DSIR : Une approche a base de sémantique distributionnelle pour la recherche doc11mentaire. Traitement Automatique des Langues,</title>
<date>2000</date>
<volume>41</volume>
<issue>2</issue>
<contexts>
<context position="2474" citStr="Rajman et al., 2000" startWordPosition="335" endWordPosition="338">temes pour le traitement de bases d’information textuelle de grande taille reposent généralement sur la notion de similarité entre textes. Par exemple, les systemes de Recherche Documentaire (RD) cherchent a calculer les similarités entre une requéte et une collection de documents. Les techniques de classiﬁcation non supervisées reposent également sur une mesure de similarité qui permet de construire des classes en regroupant des documents similaires. Ces systemes de similarités textuelles utilisent en général un modele vectoriel pour la représentation des documents (Salton and Buckley, 1988; Rajman et al., 2000) pour dériver une mesure R. Besancon, A. Rozenknop, J .-C. Chappelier, M. Rajman de similarite (au sens mathematique) dans l’espace vectoriel considere (par exemple, le cosinus de l’angle entre les vecteurs representant les documents). Les dimensions de cet espace vectoriel sont en general associees a des unites linguistiques speciﬁques, qui peuvent par exemple etre des mots, des stems ou des lemmes. Ces unites linguistiques sont appelees termes d ’indexation. L’ idee presentee dans cet article est d’ameliorer la qualite des representations des textes en precisant le sens des unites linguistiq</context>
<context position="4767" citStr="Rajman et al., 2000" startWordPosition="674" endWordPosition="677">e composante represente l’importance dans le document du terme d’indexation associe a cette dimension. La notion d’importance (i. e. la composante du vecteur) se traduit habituellement par une valeur qui depend de la frequence du terme dans le document et, eventuellement, d’informations globales comme la frequence en documents (Salton and Buckley, 1988; Singhal, 1997). L’idee du modele DSIR, fonde sur la notion de Sémantique Distributionnelle, est d’integrer plus d’information semantique dans la representation au moyen de l’utilisation de co-occurrences de termes (Rungsawang and Rajman, 1995; Rajman et al., 2000). L’ idee d’origine de la semantique distributionnelle est que le mot prend son sens en contexte et que le sens d’un mot est donc relie a l’ensemble des contextes dans lequel il apparait (Rajman and Bonnet, 1992). Dans l’approche DSIR, les contextes sont pris en compte au moyen des frequences de co-occurrence. Une unite linguistique Uj est representee par son proﬁl de cooccurrence (cﬂ, . . . ,cjM), o1‘1 M est la taille de l’ensemble des termes d’indexation et Cji est la frequence de co-occurrence entre Uj et le terme d’indexation ti au sein d’une unite textuelle predeﬁnie (fenetre de taille ﬁx</context>
</contexts>
<marker>Rajman, Besancon, Chappelier, 2000</marker>
<rawString>Rajman, M., Besancon, R., and Chappelier, J .—C. (2000). Le modele DSIR : Une approche a base de sémantique distributionnelle pour la recherche doc11mentaire. Traitement Automatique des Langues, 41(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Rajman</author>
<author>A Bonnet</author>
</authors>
<title>Corpora—base linguistics: new tools for natural language processing.</title>
<date>1992</date>
<booktitle>In I st Annual Conference of the Association for Global Strategic Information,</booktitle>
<location>Bad Kreuznach, Germany.</location>
<contexts>
<context position="4979" citStr="Rajman and Bonnet, 1992" startWordPosition="711" endWordPosition="714">nd de la frequence du terme dans le document et, eventuellement, d’informations globales comme la frequence en documents (Salton and Buckley, 1988; Singhal, 1997). L’idee du modele DSIR, fonde sur la notion de Sémantique Distributionnelle, est d’integrer plus d’information semantique dans la representation au moyen de l’utilisation de co-occurrences de termes (Rungsawang and Rajman, 1995; Rajman et al., 2000). L’ idee d’origine de la semantique distributionnelle est que le mot prend son sens en contexte et que le sens d’un mot est donc relie a l’ensemble des contextes dans lequel il apparait (Rajman and Bonnet, 1992). Dans l’approche DSIR, les contextes sont pris en compte au moyen des frequences de co-occurrence. Une unite linguistique Uj est representee par son proﬁl de cooccurrence (cﬂ, . . . ,cjM), o1‘1 M est la taille de l’ensemble des termes d’indexation et Cji est la frequence de co-occurrence entre Uj et le terme d’indexation ti au sein d’une unite textuelle predeﬁnie (fenetre de taille ﬁxe, phrase, paragraphe, etc). Les documents sont alors representes par la moyenne ponderee des proﬁls de co-occurrence des mots qu’ils contiennent, i. e. d = (d1, . . . ,dM), Oil di = Z fj Cjz‘ uj EU U etant l’ens</context>
</contexts>
<marker>Rajman, Bonnet, 1992</marker>
<rawString>Rajman, M. and Bonnet, A. (1992). Corpora—base linguistics: new tools for natural language processing. In I st Annual Conference of the Association for Global Strategic Information, Bad Kreuznach, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Rungsawang</author>
<author>M Rajman</author>
</authors>
<title>Textual information retrieval based on the concept of distributional semantics.</title>
<date>1995</date>
<booktitle>In proc. of JADT’95 (3rd International Conference on Statistical Analysis of Textual Data),</booktitle>
<location>Rome.</location>
<contexts>
<context position="4745" citStr="Rungsawang and Rajman, 1995" startWordPosition="670" endWordPosition="673">ele proﬁl lexical, dont chaque composante represente l’importance dans le document du terme d’indexation associe a cette dimension. La notion d’importance (i. e. la composante du vecteur) se traduit habituellement par une valeur qui depend de la frequence du terme dans le document et, eventuellement, d’informations globales comme la frequence en documents (Salton and Buckley, 1988; Singhal, 1997). L’idee du modele DSIR, fonde sur la notion de Sémantique Distributionnelle, est d’integrer plus d’information semantique dans la representation au moyen de l’utilisation de co-occurrences de termes (Rungsawang and Rajman, 1995; Rajman et al., 2000). L’ idee d’origine de la semantique distributionnelle est que le mot prend son sens en contexte et que le sens d’un mot est donc relie a l’ensemble des contextes dans lequel il apparait (Rajman and Bonnet, 1992). Dans l’approche DSIR, les contextes sont pris en compte au moyen des frequences de co-occurrence. Une unite linguistique Uj est representee par son proﬁl de cooccurrence (cﬂ, . . . ,cjM), o1‘1 M est la taille de l’ensemble des termes d’indexation et Cji est la frequence de co-occurrence entre Uj et le terme d’indexation ti au sein d’une unite textuelle predeﬁnie</context>
</contexts>
<marker>Rungsawang, Rajman, 1995</marker>
<rawString>Rungsawang, A. and Rajman, M. (1995). Textual information retrieval based on the concept of distributional semantics. In proc. of JADT’95 (3rd International Conference on Statistical Analysis of Textual Data), Rome.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>C Buckley</author>
</authors>
<title>Term weighting approaches in automatic text retrieval.</title>
<date>1988</date>
<booktitle>Information Processing and Management,</booktitle>
<pages>24--513</pages>
<contexts>
<context position="2452" citStr="Salton and Buckley, 1988" startWordPosition="331" endWordPosition="334">hm. 1 Introduction Les systemes pour le traitement de bases d’information textuelle de grande taille reposent généralement sur la notion de similarité entre textes. Par exemple, les systemes de Recherche Documentaire (RD) cherchent a calculer les similarités entre une requéte et une collection de documents. Les techniques de classiﬁcation non supervisées reposent également sur une mesure de similarité qui permet de construire des classes en regroupant des documents similaires. Ces systemes de similarités textuelles utilisent en général un modele vectoriel pour la représentation des documents (Salton and Buckley, 1988; Rajman et al., 2000) pour dériver une mesure R. Besancon, A. Rozenknop, J .-C. Chappelier, M. Rajman de similarite (au sens mathematique) dans l’espace vectoriel considere (par exemple, le cosinus de l’angle entre les vecteurs representant les documents). Les dimensions de cet espace vectoriel sont en general associees a des unites linguistiques speciﬁques, qui peuvent par exemple etre des mots, des stems ou des lemmes. Ces unites linguistiques sont appelees termes d ’indexation. L’ idee presentee dans cet article est d’ameliorer la qualite des representations des textes en precisant le sens</context>
<context position="4501" citStr="Salton and Buckley, 1988" startWordPosition="636" endWordPosition="639"> nous presentons les premiers resultats pour la validation de cette approche, appliquee a la Recherche documentaire. 2 Le modéle DSIR de représentation de textes Les modeles vectoriels standards representent les documents par un vecteur, appele proﬁl lexical, dont chaque composante represente l’importance dans le document du terme d’indexation associe a cette dimension. La notion d’importance (i. e. la composante du vecteur) se traduit habituellement par une valeur qui depend de la frequence du terme dans le document et, eventuellement, d’informations globales comme la frequence en documents (Salton and Buckley, 1988; Singhal, 1997). L’idee du modele DSIR, fonde sur la notion de Sémantique Distributionnelle, est d’integrer plus d’information semantique dans la representation au moyen de l’utilisation de co-occurrences de termes (Rungsawang and Rajman, 1995; Rajman et al., 2000). L’ idee d’origine de la semantique distributionnelle est que le mot prend son sens en contexte et que le sens d’un mot est donc relie a l’ensemble des contextes dans lequel il apparait (Rajman and Bonnet, 1992). Dans l’approche DSIR, les contextes sont pris en compte au moyen des frequences de co-occurrence. Une unite linguistique</context>
</contexts>
<marker>Salton, Buckley, 1988</marker>
<rawString>Salton, G. and Buckley, C. (1988). Term weighting approaches in automatic text retrieval. Information Processing and Management, 24:513-523.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Singhal</author>
</authors>
<title>Term Weighting Revisited.</title>
<date>1997</date>
<tech>PhD thesis,</tech>
<institution>Department of Computer Science, Cornell University.</institution>
<contexts>
<context position="4517" citStr="Singhal, 1997" startWordPosition="640" endWordPosition="641">ers resultats pour la validation de cette approche, appliquee a la Recherche documentaire. 2 Le modéle DSIR de représentation de textes Les modeles vectoriels standards representent les documents par un vecteur, appele proﬁl lexical, dont chaque composante represente l’importance dans le document du terme d’indexation associe a cette dimension. La notion d’importance (i. e. la composante du vecteur) se traduit habituellement par une valeur qui depend de la frequence du terme dans le document et, eventuellement, d’informations globales comme la frequence en documents (Salton and Buckley, 1988; Singhal, 1997). L’idee du modele DSIR, fonde sur la notion de Sémantique Distributionnelle, est d’integrer plus d’information semantique dans la representation au moyen de l’utilisation de co-occurrences de termes (Rungsawang and Rajman, 1995; Rajman et al., 2000). L’ idee d’origine de la semantique distributionnelle est que le mot prend son sens en contexte et que le sens d’un mot est donc relie a l’ensemble des contextes dans lequel il apparait (Rajman and Bonnet, 1992). Dans l’approche DSIR, les contextes sont pris en compte au moyen des frequences de co-occurrence. Une unite linguistique Uj est represen</context>
</contexts>
<marker>Singhal, 1997</marker>
<rawString>Singhal, A. (1997). Term Weighting Revisited. PhD thesis, Department of Computer Science, Cornell University.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>