<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
"http://www.w3.org/TR/html4/loose.dtd">
<html><head><title>Grammaire &#224; substitution d&#8217;arbre de complexit&#233; polynomiale : un cadre efficace pour DOP</title>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
</head>
<body>
<div style="page-break-before:always; page-break-after:always"><div><p>TALN 2001, Tours, 2-5 juillet 2001
</p>
<p>Grammaire &#224; substitution d&#8217;arbre de complexit&#233;
polynomiale : un cadre efficace pour DOP
</p>
<p>Jean-C&#233;dric Chappelier et Martin Rajman
EPFL
</p>
<p>DI-LIA, IN (&#201;cublens)
CH-1015 Lausanne, Switzerland
</p>
<p>{Jean-Cedric.Chappelier,Martin.Rajman}@epfl.ch
</p>
<p>R&#233;sum&#233; - Abstract
</p>
<p>Trouver l&#8217;arbre d&#8217;analyse le plus probable dans le cadre du mod&#232;le DOP (Data-Oriented Pars-
ing) &#8212; une version probabiliste de grammaire &#224; substitution d&#8217;arbres d&#233;velopp&#233;e par R. Bod
(1992) &#8212; est connu pour &#234;tre un probl&#232;me NP-difficile dans le cas le plus g&#233;n&#233;ral (Sima&#8217;an,
1996a). Cependant, si l&#8217;on introduit des restrictions a priori sur le choix des arbres &#233;l&#233;men-
taires, on peut obtenir des instances particuli&#232;res de DOP pour lesquelles la recherche de l&#8217;arbre
d&#8217;analyse le plus probable peut &#234;tre effectu&#233;e en un temps polynomial (par rapport &#224; la taille
de la phrase &#224; analyser). La pr&#233;sente contribution se propose d&#8217;&#233;tudier une telle instance poly-
nomiale de DOP, fond&#233;e sur le principe de s&#233;lection miminale-maximale et d&#8217;en &#233;valuer les
performances sur deux corpus diff&#233;rents.
</p>
<p>Finding the most probable parse tree in the framework of Data-Oriented Parsing (DOP), a
Stochastic Tree Substitution Parsing scheme developed by R. Bod (1992), has proven to be
NP-hard in the most general case (Sima&#8217;an, 1996a). However, introducing some a priori re-
strictions on the choice of the elementary trees leads to interesting DOP instances with poly-
nomial time-complexity. The purpose of this paper is to present such an instance, based on the
minimal-maximal selection principle, and to evaluate its performances on two different corpora.
</p>
<p>1 Motivations
</p>
<p>Introduite par R. Scha (Scha, 1990) puis d&#233;velopp&#233;e par R. Bod (Bod, 1992; Bod, 1998),
l&#8217;approche Data-Oriented Parsing (DOP) pour l&#8217;analyse syntaxique probabiliste a depuis &#233;t&#233;
largement &#233;tudi&#233;e par diverses &#233;quipes de recherche. La principale limitation de cette approche
reste cependant le caract&#232;re NP-difficile du probl&#232;me consistant &#224; trouver l&#8217;arbre d&#8217;analyse le
plus probable (MPP, pour most probable parse) (Sima&#8217;an, 1996a). Diff&#233;rentes solutions ap-
proch&#233;es (heuristiques) ont &#233;t&#233; propos&#233;es (Bod, 1992; Goodman, 1996; Chappelier &amp; Rajman,
1998) mais une autre direction de recherche prometteuse consiste &#224; explorer diff&#233;rentes re-
strictions a priori du jeu d&#8217;arbres &#233;l&#233;mentaires utilis&#233; par la grammaire DOP, restrictions pour
lesquelles trouver l&#8217;arbre d&#8217;analyse le plus probable n&#8217;est plus un probl&#232;me NP-difficile. L&#8217;objet
du pr&#233;sent article est de pr&#233;senter et d&#8217;&#233;valuer un exemple de telle restriction.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>J.-C. Chappelier et M. Rajman
</p>
<p>Nous commencerons tout d&#8217;abord par une br&#232;ve introduction au Data-Oriented Parsing et d&#233;fini-
rons quelques notations. Puis nous pr&#233;senterons dans la seconde partie un principe de s&#233;lection
d&#8217;arbres &#233;l&#233;mentaires &#8212; le principe de s&#233;lection minimale-maximale &#8211; permettant d&#8217;obtenir
une restriction de DOP ayant une complexit&#233; polynomiale (pour le probl&#232;me du MPP). Nous
terminerons enfin par la pr&#233;sentation de plusieurs exp&#233;riences sur deux corpus diff&#233;rents.
</p>
<p>2 Data-Oriented Parsing
</p>
<p>2.1 Le mod&#232;le DOP
</p>
<p>DOP est un mod&#232;le d&#8217;analyse syntaxique probabiliste &#224; base de grammaires &#224; substitution
d&#8217;arbres (STSG pour Stochastic Tree-Substitution Grammars). Une STSG est une grammaire
pour laquelle les r&#232;gles sont des arbres &#233;l&#233;mentaires1 que l&#8217;on peut combiner &#224; l&#8217;aide de l&#8217;op&#233;-
rateur de substitution2 pour obtenir les d&#233;rivations des arbres d&#8217;analyse.
</p>
<p>Dans sa d&#233;finition la plus g&#233;n&#233;rale, le mod&#232;le DOP utilise comme arbres &#233;l&#233;mentaires l&#8217;ensemble
de tous les sous-arbres des arbres d&#8217;analyse contenus dans un corpus annot&#233; (treebank) disponible
pour l&#8217;entra&#238;nement du mod&#232;le. Chaque arbre &#233;l&#233;mentaire &#0; est associ&#233; &#224; une probabilit&#233; &#233;l&#233;-
mentaire &#1;&#3;&#2; &#0; &#4; qui est proportionnelle au nombre d&#8217;occurrences de &#0; dans le corpus d&#8217;entra&#238;nement.
</p>
<p>La probabilit&#233; &#1;&#3;&#2; &#5; &#4; d&#8217;une d&#233;rivation &#5; est alors d&#233;finie par3 &#1;&#3;&#2; &#5; &#4;&#7;&#6;&#9;&#8;
&#10; &#11; &#12;
</p>
<p>&#1;&#3;&#2;
</p>
<p>&#0; &#4;
</p>
<p>et la DOP-probabilit&#233;
</p>
<p>d&#8217;un arbre d&#8217;analyse &#13; est donn&#233;e par :
&#14;
</p>
<p>DOP &#2; &#13;
&#4;&#15;&#6;&#17;&#16;
</p>
<p>&#12; &#18;&#7;&#19;
</p>
<p>&#1;&#3;&#2; &#5;
</p>
<p>&#4;&#7;&#6;&#17;&#16;
</p>
<p>&#12; &#18;&#7;&#19;
</p>
<p>&#8;
</p>
<p>&#10; &#11; &#12;
</p>
<p>&#1;&#3;&#2;
</p>
<p>&#0; &#4;
</p>
<p>o&#249; &#8220; &#5;&#21;&#20;&#22;&#13; &#8221; signifie &#8220;pour toute d&#233;rivation &#5; produisant l&#8217;arbre d&#8217;analyse &#13; &#8221;4.
</p>
<p>2.2 Analyse syntaxique la plus probable
</p>
<p>L&#8217;analyse syntaxique d&#8217;une phrase s&#8217;effectue g&#233;n&#233;ralement en deux &#233;tapes distinctes :
l&#8217;analyse proprement dite, dont le but est de produire une repr&#233;sentation compacte de l&#8217;ensemble
</p>
<p>des arbres d&#8217;analyse possibles pour la phrase analys&#233;e ;
l&#8217;exploitation des r&#233;sultats &#224; partir de la structure compacte pr&#233;c&#233;demment construite. Cette
</p>
<p>&#233;tape peut par exemple consister &#224; pr&#233;senter l&#8217;ensemble de la for&#234;t d&#8217;analyse, &#224; extraire
l&#8217;arbre d&#8217;analyse le plus probable, &#224; extraire la d&#233;rivation la plus probable5, etc...
</p>
<p>Dans le cas des STSGs, l&#8217;&#233;tape d&#8217;analyse peut, comme dans le cas des grammaires hors-
contexte, &#234;tre r&#233;alis&#233;e en un temps polynomial (cubique) par rapport &#224; la taille de la phrase
&#224; analyser. Par contre, l&#8217;extraction de l&#8217;arbre d&#8217;analyse le plus probable (MPP) constitue, dans
le cas g&#233;n&#233;ral, un probl&#232;me NP-difficile (Sima&#8217;an, 1996a) et ne peut donc pas s&#8217;effectuer en
</p>
<p>1comme pour les TAG (Tree Adjoining Grammars)
2mais pas de l&#8217;op&#233;rateur d&#8217;adjonction, contrairement aux TAG
3 &#23;&#25;&#24;ff&#26; repr&#233;sente le fait que le sous-arbre &#23; participe &#224; la d&#233;rivation &#26; .
4Un m&#234;me arbre d&#8217;analyse peut en effet poss&#233;der plusieurs d&#233;rivations diff&#233;rentes (et ce malgr&#233; la convention
</p>
<p>de r&#233;&#233;criture de la feuille non-terminale la plus &#224; gauche en premier).
5Dans le cas de DOP, et &#224; la diff&#233;rence des grammaires hors-contexte probabilistes classiques, l&#8217;arbre d&#8217;analyse
</p>
<p>le plus probable et l&#8217;arbre associ&#233; &#224; la d&#233;rivation la plus probable ne sont pas n&#233;cessairement identiques.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Grammaire &#224; substitution d&#8217;arbre de complexit&#233; polynomiale
</p>
<p>g&#233;n&#233;ral de fa&#231;on efficace (polynomiale). Notons toutefois que la d&#233;rivation la plus probable
peut par contre &#234;tre trouv&#233;e en un temps cubique en utilisant les algorithmes usuels d&#233;velopp&#233;s
pour les grammaires hors-contexte probabilistes.
</p>
<p>Pour contourner le caract&#232;re NP-difficile de la recherche du MPP dans le mod&#232;le DOP, diverses
heuristiques correspondant &#224; des solutions approch&#233;es ont &#233;t&#233; propos&#233;es : Monte-Carlo Sam-
pling (Bod, 1992), General Recall (Goodman, 1998), &#233;chantillonage contr&#244;l&#233; (Chappelier &amp;
Rajman, 1998; Chappelier &amp; Rajman, 2000).
Une autre approche possible, d&#233;velopp&#233;e dans cette contribution, consiste &#224; explorer les restric-
tions du mod&#232;le DOP pour lesquelles la recherche de l&#8217;arbre d&#8217;analyse le plus probable peut
se faire de fa&#231;on exacte en un temps polynomial (cubique) par rapport &#224; la taille de la phrase &#224;
analyser. L&#8217;id&#233;e g&#233;n&#233;rale d&#8217;une telle approche consiste &#224; limiter a priori l&#8217;ensemble des arbres
&#233;l&#233;mentaires constituant la grammaire du mod&#232;le DOP de telle sorte que l&#8217;on puisse exhiber
une grammaire hors-contexte probabiliste qui soit &#233;quivalente &#224; la STSG utilis&#233;e, c.-&#224;-d. telle
que pour tout arbre d&#8217;analyse de DOP-probabilit&#233; maximale dans le mod&#232;le DOP, il existe au
moins une d&#233;rivation dans la grammaire hors-contexte &#233;quivalente dont la probabilit&#233;
</p>
<p>1. est &#233;gale &#224; la DOP-probabilit&#233; de cet arbre d&#8217;analyse ;
2. est maximale (parmi les probabilit&#233;s de toutes les d&#233;rivations produites pour la phrase
</p>
<p>analys&#233;e par la grammaire hors-contexte &#233;quivalente).
Une STSG pour laquelle une grammaire hors-contexte probabiliste &#233;quivalente peut &#234;tre con-
struite sera appel&#233;e &#171; grammaire probabiliste &#224; substitution d&#8217;arbres polynomiale &#187; (PSTSG)6.
Notons que, dans ce cas, la complexit&#233; algorithmique li&#233;e &#224; la sommation des probabilit&#233;s sur
les diverses d&#233;rivations d&#8217;un m&#234;me arbre d&#8217;analyse peut &#234;tre efficacement factoris&#233;e7.
</p>
<p>Tout l&#8217;enjeu de l&#8217;approche propos&#233;e ici est donc de trouver diverses restrictions du mod&#232;le DOP
correspondant &#224; des PSTSG. Un exemple trivial d&#8217;une telle restriction est donn&#233; par le mod&#232;le
DOP o&#249; les arbres &#233;l&#233;mentaires sont limit&#233;s aux sous-arbres de profondeur 1. Ce mod&#232;le DOP
est strictement &#233;quivalent &#224; une grammaire hors-contexte probabiliste (SCFG) et ne pr&#233;sente
donc aucun apport par rapport aux SCFG usuelles8.
</p>
<p>Dans la suite de cet article, nous nous proposons d&#8217;examiner une autre PSTSG, moins triviale
et linguistiquement plus int&#233;ressante. Elle correspond &#224; la restriction du mod&#232;le DOP dans
laquelle les arbres &#233;l&#233;mentaires sont limit&#233;s aux sous-arbres de profondeur 1 et aux sous-arbres
totalement ancr&#233;s, i.e. les sous-arbres dont les feuilles sont toutes des terminaux de la gram-
maire (i.e. des mots). Ce choix pour les arbres &#233;l&#233;mentaires sera appel&#233; principe de s&#233;lection
minimale-maximale 9 et la restriction du mod&#232;le DOP correspondante sera appel&#233;e sa restriction
minimale-maximale.
</p>
<p>Nous allons maintenant montrer qu&#8217;une telle STSG est effectivement polynomiale, c.-&#224;-d. qu&#8217;il
existe une grammaire hors-contexte qui lui soit &#233;quivalente au sens d&#233;fini ci-dessus.
</p>
<p>6pour Polynomial Stochastic Tree Substitution Grammar. &#171; polynomiale &#187; fait ici r&#233;f&#233;rence &#224; la complexit&#233; du
probl&#232;me MPP pour une telle grammaire.
</p>
<p>7C&#8217;est pr&#233;cis&#233;ment cette sommation n&#233;cessaire qui est la source du caract&#232;re NP-difficile de la recherche du
MPP dans le cas g&#233;n&#233;ral.
</p>
<p>8En particulier la probabilisation des analyses est strictement la m&#234;me.
9Ce principe de s&#233;lection a &#233;t&#233; introduit pour la premi&#232;re fois par Jacques Han dans son travail doctoral non
</p>
<p>encore publi&#233;.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>J.-C. Chappelier et M. Rajman
</p>
<p>3 Restriction minimale-maximale de DOP
</p>
<p>3.1 D&#233;finition de la grammaire hors-contexte &#233;quivalente
</p>
<p>Pour toute grammaire DOP
&#0;
</p>
<p>DOP, soit
&#0;
</p>
<p>equiv la grammaire hors-contexte contenant toutes les
r&#232;gles racine-feuilles10 associ&#233;es &#224; tous les arbres &#233;l&#233;mentaires de
</p>
<p>&#0;
</p>
<p>DOP.
11 D&#8217;un point de vue
</p>
<p>purement structurel (i.e. sans les probabilit&#233;s) &#0; DOP est &#233;quivalente &#224; &#0; equiv.12 Cette propri&#233;t&#233; est
toujours vraie, m&#234;me dans le cas g&#233;n&#233;ral de DOP, mais dans le cas particulier des restrictions
minimales-maximales, cette &#233;quivalence peut &#234;tre &#233;tendue aux probabilit&#233;s. Plus pr&#233;cis&#233;ment,
il devient alors possible de probabiliser
</p>
<p>&#0;
</p>
<p>equiv de sorte que trouver la d&#233;rivation la plus probable
au sens de
</p>
<p>&#0;
</p>
<p>equiv soit &#233;quivalent &#224; trouver l&#8217;analyse la plus probable au sens de
&#0;
</p>
<p>DOP.
</p>
<p>La solution permettant d&#8217;obtenir une telle &#233;quivalence consiste &#224; associer &#224; chaque r&#232;gle de
&#0;
</p>
<p>equiv un coefficient stochastique correspondant &#224; la DOP-probabilit&#233; de l&#8217;arbre &#233;l&#233;mentaire
correspondant. Remarquez bien la diff&#233;rence avec la d&#233;marche usuelle qui consiste &#224; associer &#224;
la r&#232;gle la probabilit&#233; &#233;l&#233;mentaire &#1; de l&#8217;arbre &#233;l&#233;mentaire et non pas sa DOP-probabilit&#233;.
</p>
<p>Nous montrerons dans la section 3.3 qu&#8217;une telle probabilisation de
&#0;
</p>
<p>equiv est facile &#224; r&#233;aliser
lors de la construction de
</p>
<p>&#0;
</p>
<p>DOP &#224; partir du corpus d&#8217;entra&#238;nement. Mais montrons tout d&#8217;abord
qu&#8217;avec une telle probabilisation, trouver la d&#233;rivation la plus probable avec
</p>
<p>&#0;
</p>
<p>equiv est effective-
ment &#233;quivalent &#224; trouver l&#8217;analyse la plus probable au sens de
</p>
<p>&#0;
</p>
<p>DOP.
</p>
<p>3.2 D&#233;monstration de l&#8217;&#233;quivalence probabiliste
</p>
<p>L&#8217;&#233;quivalence &#224; &#233;tablir repose sur la propri&#233;t&#233; g&#233;n&#233;rale suivante : la probabilit&#233; d&#8217;une d&#233;rivation
dans
</p>
<p>&#0;
</p>
<p>equiv est toujours inf&#233;rieure ou &#233;gale &#224; la DOP-probabilit&#233; de l&#8217;arbre d&#8217;analyse de &#0; DOP
&#233;quivalent. La d&#233;monstration de cette propri&#233;t&#233;, trop longue pour &#234;tre donn&#233;e ici, peut &#234;tre
trouv&#233;e dans (Chappelier &amp; Rajman, 2001). &#192; l&#8217;aide de cette propri&#233;t&#233;, pour montrer qu&#8217;il est
&#233;quivalent de trouver la d&#233;rivation la plus probable au sens de
</p>
<p>&#0;
</p>
<p>equiv et l&#8217;arbre d&#8217;analyse le plus
probable au sens de
</p>
<p>&#0;
</p>
<p>DOP, il est donc suffisant de montrer que, pour chaque arbre d&#8217;analyse &#2;
au sens de
</p>
<p>&#0;
</p>
<p>DOP, il existe au moins une d&#233;rivation &#233;quivalente dans
&#0;
</p>
<p>equiv dont la probabilit&#233; est
&#233;gale &#224; la DOP-probabilit&#233; de &#2; .
</p>
<p>Cette derni&#232;re propri&#233;t&#233; peut se d&#233;montrer par r&#233;currence sur la profondeur de &#2; :
1) La propri&#233;t&#233; est trivialement vraie pour tout arbre de profondeur 1, puisque dans ce cas il n&#8217;y
a qu&#8217;une d&#233;rivation possible de l&#8217;arbre d&#8217;analyse et la DOP-probabilit&#233; de cet arbre est alors,
par construction, &#233;gale &#224; la probabilit&#233; de cette d&#233;rivation.
2) Supposons maintenant que pour tout arbre d&#8217;analyse (au sens de &#0; DOP) &#2; de profondeur au
plus &#3; , il existe dans
</p>
<p>&#0;
</p>
<p>equiv une d&#233;rivation &#4; &#233;quivalente &#224; &#2; dont la probabilit&#233; est la DOP-
probabilit&#233; de &#2; . Nous devons alors &#233;tablir que cette proposition est &#233;galement vraie pour tout
arbre d&#8217;analyse de profondeur &#3;&#6;&#5;&#8;&#7; .
</p>
<p>10Pour tout arbre &#9; , la r&#232;gle racine-feuilles associ&#233;e &#10;&#12;&#11; &#9;&#14;&#13; est la r&#232;gle hors-contexte dont la partie gauche est la
racine de &#9; et la partie droite la s&#233;quence des feuilles de &#9; .
</p>
<p>11Notez qu&#8217;une r&#232;gle hors-contexte diff&#233;rente est cr&#233;&#233;e pour chacun des arbres &#233;l&#233;mentaires. Les r&#232;gles asso-
ci&#233;es &#224; des arbres &#233;l&#233;mentaires diff&#233;rents mais ayant m&#234;me racine et m&#234;mes feuilles sont diff&#233;renci&#233;es par leur
indices. Il y a donc bijection entre les r&#232;gles de &#15; equiv et les arbres &#233;l&#233;mentaires de &#15; DOP.
</p>
<p>12Il y a en effet bijection entre les d&#233;rivations de ces deux grammaire. La reconstruction de l&#8217;arbre d&#8217;analyse
complet &#224; partir d&#8217;une d&#233;rivation dans &#15; equiv peut &#234;tre effectu&#233;e de fa&#231;on triviale &#224; l&#8217;aide des indices des r&#232;gles de
&#15; equiv qui r&#233;f&#233;rencent &#233;galement les arbres &#233;l&#233;mentaires de &#15; DOP.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Grammaire &#224; substitution d&#8217;arbre de complexit&#233; polynomiale
</p>
<p>X1 X2
</p>
<p>T2T1
</p>
<p>A
</p>
<p>...
</p>
<p>...
</p>
<p>X
</p>
<p>T
</p>
<p>q
</p>
<p>q
</p>
<p>t1
T
</p>
<p>Figure 1: D&#233;rivation avec une grammaire suivant le principe de restriction minimale-maximale
de DOP : toute d&#233;rivation d&#8217;un arbre d&#8217;analyse donn&#233; ( &#0; ici), qui n&#8217;est pas lui-m&#234;me un arbre
&#233;l&#233;mentaire, commence par un arbre &#233;l&#233;mentaire de profondeur 1 ( &#1; &#2; ici), lequel est partag&#233; par
toutes les d&#233;rivations de l&#8217;arbre d&#8217;analyse consid&#233;r&#233;.
</p>
<p>Soit maintenant &#0;&#4;&#3;&#4;&#5;&#7;&#6;&#4;&#8;
&#2;
</p>
<p>un arbre d&#8217;analyse (de &#9; DOP) de profondeur &#10;&#12;&#11;&#14;&#13; de la cha&#238;ne &#6;&#15;&#8;&#2; . Si
&#0;
</p>
<p>est lui-m&#234;me un arbre &#233;l&#233;mentaire de la grammaire &#9; DOP, alors la r&#232;gle hors-contexte racine-
feuilles &#16;&#18;&#17; &#0;&#20;&#19; qui lui est associ&#233;e est une r&#232;gle de &#9; equiv dont la probabilit&#233; est par construction
la DOP-probabilit&#233; de &#0; . Et puisque &#16;&#18;&#17; &#0;&#20;&#19; est une d&#233;rivation de &#6; &#8;
</p>
<p>&#2;
</p>
<p>, il existe au moins une
d&#233;rivation repr&#233;sentant &#0; dans &#9; equiv (et dont la probabilit&#233; est la DOP-probabilit&#233; de &#0; ).
Si au contraire &#0; n&#8217;est pas un arbre &#233;l&#233;mentaire de &#9; DOP, alors
</p>
<p>&#0;
</p>
<p>poss&#232;de au moins une d&#233;riva-
tion. Notons-la &#21;&#23;&#22;&#4;&#1; &#2;&#25;&#24;&#20;&#26; &#26; &#26; &#24;ff&#1; fi ( fl&#14;ffi&#31;&#13; ).
Il est alors important de remarquer qu&#8217;en raison du choix des arbres &#233;l&#233;mentaires impos&#233; par
la s&#233;lection minimale-maximale, &#1; &#2; est n&#233;cessairement un arbre de profondeur 1. Il est de plus
partag&#233; par toutes les d&#233;rivations de &#0; . La DOP-probabilit&#233; de &#0; est donc :
</p>
<p> 
</p>
<p>DOP &#17;
&#0;&#20;&#19;
</p>
<p>&#22;&quot;!
</p>
<p># $ %ff&amp;('
</p>
<p>) *
</p>
<p># $ +
</p>
<p>&#17; &#1;
</p>
<p>&#19;
</p>
<p>&#22;
</p>
<p>+
</p>
<p>&#17; &#1; &#2;
</p>
<p>&#19;
</p>
<p>!
</p>
<p># , %ff&amp; , - - -
</p>
<p>!
</p>
<p># . %ff&amp; .
</p>
<p>/
</p>
<p>'
</p>
<p>0 1
</p>
<p>&#2;
</p>
<p>+
</p>
<p>&#17; &#21;
</p>
<p>0
</p>
<p>&#19;
</p>
<p>&#22;
</p>
<p>+
</p>
<p>&#17; &#1; &#2;
</p>
<p>&#19;
</p>
<p>-
</p>
<p>/
</p>
<p>'
</p>
<p>0 1
</p>
<p>&#2;
</p>
<p>23
</p>
<p>!
</p>
<p># 4 %ff&amp; 4 +
</p>
<p>&#17; &#21;
</p>
<p>0
</p>
<p>&#19; 56
</p>
<p>&#22;
</p>
<p>+
</p>
<p>&#17; &#1; &#2;
</p>
<p>&#19;
</p>
<p>-
</p>
<p>/
</p>
<p>'
</p>
<p>0 1
</p>
<p>&#2;
</p>
<p> 
</p>
<p>DOP &#17;
&#0;
</p>
<p>0
</p>
<p>&#19;
</p>
<p>en notant
&#0;
</p>
<p>&#2; , ...,
</p>
<p>&#0;
</p>
<p>/ les sous-arbres totalement ancr&#233;s de &#0; qui sont fils de &#1; &#2; dans &#21; (cf fig 1).
Or, puisque &#0; &#2; , ..., &#0; / sont de profondeur au plus &#10; , il existe par hypoth&#232;se de r&#233;currence
pour chaque &#0; 0 une d&#233;rivation &#21; equiv &#17;
</p>
<p>&#0;
</p>
<p>0
</p>
<p>&#19; dans &#9; equiv dont la probabilit&#233; (dans &#9; equiv) est &#233;gale &#224;
 
</p>
<p>DOP &#17;
&#0;
</p>
<p>0
</p>
<p>&#19;
</p>
<p>.
</p>
<p>La d&#233;rivation &#17; &#1; &#2; 7 &#21; equiv &#17;
&#0;
</p>
<p>&#2;
</p>
<p>&#19;
</p>
<p>7 &#26; &#26; &#26; 7 &#21; equiv &#17;
&#0;
</p>
<p>/
</p>
<p>&#19; &#19;
</p>
<p>est une d&#233;rivation de &#0; au sens de &#9; equiv dont la prob-
abilit&#233; est13
</p>
<p>+
</p>
<p>&#17; &#1; &#2;
</p>
<p>&#19;
</p>
<p>-
</p>
<p> 
</p>
<p>DOP &#17;
&#0;
</p>
<p>&#2;
</p>
<p>&#19;
</p>
<p>-
</p>
<p>&#26; &#26; &#26;
</p>
<p>-
</p>
<p> 
</p>
<p>DOP &#17;
&#0;
</p>
<p>/
</p>
<p>&#19;
</p>
<p>, c.-&#224;-d. la DOP-probabilit&#233; de &#0; .
</p>
<p>Il existe donc au moins une d&#233;rivation de &#0; au sens de &#9; equiv dont la probabilit&#233; est &#233;gale &#224; la
DOP-probabilit&#233; de &#0; . 8
</p>
<p>Rechercher la d&#233;rivation la plus probable au sens de &#9; equiv est donc bien &#233;quivalent &#224; rechercher
l&#8217;arbre d&#8217;analyse le plus probable au sens de &#9; DOP.
</p>
<p>3.3 Construction pratique de la grammaire hors-contexte &#233;quivalente
</p>
<p>Il nous faut maintenant expliquer comment la grammaire hors-contexte &#233;quivalente &#9; equiv peut
&#234;tre construite &#224; partir du corpus d&#8217;entra&#238;nement utilis&#233; pour le mod&#232;le DOP consid&#233;r&#233;.
</p>
<p>13par construction (d&#233;rivation hors-contexte)</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>J.-C. Chappelier et M. Rajman
</p>
<p>Les arbres de profondeur 1 sont tout d&#8217;abord extraits, de fa&#231;on identique &#224; la constitution d&#8217;une
grammaire hors-contexte &#224; partir d&#8217;un corpus annot&#233;. Puis, pour chaque n&#339;ud de chaque arbre
du corpus, le sous-arbre totalement ancr&#233; correspondant est extrait et la r&#232;gle racine-feuilles
associ&#233;e est ajout&#233;e aux r&#232;gles de &#0; equiv (en regroupant les occurrences multiples correspondant
au m&#234;me arbre &#233;l&#233;mentaire mais en diff&#233;renciant, par leurs indices, les r&#232;gles racine-feuilles
identiques provenant d&#8217;arbres &#233;l&#233;mentaires diff&#233;rents).
La DOP-probabilit&#233; de chaque sous-arbre totalement ancr&#233; est ensuite calcul&#233;e par ordre de
profondeur d&#8217;arbre croissante. En effet, si la DOP-probabilit&#233; de tout arbre &#233;l&#233;mentaire de
profondeur &#1; a d&#233;j&#224; &#233;t&#233; calcul&#233;e, alors la DOP-probabilit&#233; de tout arbre &#233;l&#233;mentaire &#2; de pro-
fondeur &#1;&#4;&#3;&#6;&#5; peut facilement &#234;tre calcul&#233;e par la formule suivante :
</p>
<p>&#7;
</p>
<p>DOP &#8; &#2;&#10;&#9;&#12;&#11;&#14;&#13; &#8; &#2;&#10;&#9;&#15;&#3;&#16;&#13; &#8; &#17; &#18; &#9;&#20;&#19;
</p>
<p>&#21;
</p>
<p>&#22;
</p>
<p>&#23; &#24;
</p>
<p>&#18;
</p>
<p>&#7;
</p>
<p>DOP &#8; &#2;
&#23;
</p>
<p>&#9;
</p>
<p>4 Un exemple
</p>
<p>Consid&#233;rons maintenant un exemple jouet illustrant l&#8217;approche pr&#233;c&#233;demment d&#233;taill&#233;e et mon-
trant en quoi notre approche diff&#232;re du mod&#232;le DOP g&#233;n&#233;ral.
</p>
<p>Consid&#233;rons le corpus d&#8217;entra&#238;nement trivial contenant 2 arbres suivants :
</p>
<p>&#17; &#18;
</p>
<p>&#11;
</p>
<p>P
</p>
<p>N
Pierre
</p>
<p>V
mange
</p>
<p>GNP
Prep
avec
</p>
<p>N
Pierrette
</p>
<p>&#17; &#25;
</p>
<p>&#11;
</p>
<p>P
</p>
<p>N
Pierre
</p>
<p>GV
</p>
<p>V
mange
</p>
<p>GNP
Prep
avec
</p>
<p>N
Pierrette
</p>
<p>Pour construire la STSG
&#0;
</p>
<p>DOP suivant le principe de s&#233;lection minimale-maximale, les sous-
arbres de profondeur 1 ainsi que tous les sous-arbres totalement ancr&#233;s doivent &#234;tre extraits ;
ce qui donne les 12 arbres suivants : 8 arbres de profondeur 1,
</p>
<p>&#17; &#18;
</p>
<p>et
&#17; &#25;
</p>
<p>eux-m&#234;mes, et les deux
sous-arbre totalement ancr&#233;s suivants :
</p>
<p>GNP
Prep
avec
</p>
<p>N
Pierrette
</p>
<p>GV
</p>
<p>V
mange
</p>
<p>GNP
Prep
avec
</p>
<p>N
Pierrette
</p>
<p>Remarquons qu&#8217;un arbre comme par exemple P
N
</p>
<p>Pierre
V
</p>
<p>mange
GNP
</p>
<p>n&#8217;appartient pas &#224;
&#0;
</p>
<p>DOP, alors que cela aurait &#233;t&#233; le cas dans la version non restreinte de DOP.
</p>
<p>Comme
&#0;
</p>
<p>DOP contient 12 arbres &#233;l&#233;mentaires, la grammaire hors-contexte &#233;quivalente
&#0;
</p>
<p>equiv
contient 12 r&#232;gles, lesquelles sont donn&#233;es dans la table 1. Remarquez bien que la r&#232;gle
P -&gt; Pierre mange avec Pierrette appara&#238;t deux fois dans la grammaire. Ceci est d&#251;
au fait que chacune de ces deux r&#232;gles correspond &#224; un arbre &#233;l&#233;mentaire de
</p>
<p>&#0;
</p>
<p>DOP diff&#233;rent.
</p>
<p>Notez aussi que &#26;fiff &#7;
&#8; fl&#31;ffi! 
</p>
<p>&#9; n&#8217;est pas &#233;gale &#224; &#5; dans le cas g&#233;n&#233;ral, c.-&#224;-d. que
&#0;
</p>
<p>equiv n&#8217;est</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Grammaire &#224; substitution d&#8217;arbre de complexit&#233; polynomiale
</p>
<p>r&#232;gle &#0; DOP &#1;
&#2; &#3; : P -&gt; N V GNP 0.25 0.25
&#2; &#4; : P -&gt; N GV 0.25 0.25
&#2; &#5; : P -&gt; Pierre mange avec Pierrette 0.344 0.25
&#2; &#6; : P -&gt; Pierre mange avec Pierrette 0.359 0.25
&#2; &#7; : N -&gt; Pierre 0.5 0.5
&#2; &#8; : N -&gt; Pierrette 0.5 0.5
&#2; &#9; : V -&gt; mange 1.0 1.0
&#2; &#10; : Prep -&gt; avec 1.0 1.0
&#2; &#11; : GNP -&gt; Prep N 0.5 0.5
&#2; &#3; &#12; : GNP -&gt; avec Pierrette 0.75 0.5
&#2; &#3; &#3; : GV -&gt; V GNP 0.5 0.5
&#2; &#3; &#4; : GV -&gt; mange avec Pierrette 0.875 0.5
</p>
<p>Table 1: Les 12 r&#232;gles de la grammaire hors-contexte &#233;quivalente. Le coefficient stochastique
est donn&#233; par &#0; DOP. &#1; correspond &#224; la probabilit&#233; &#233;l&#233;mentaire de l&#8217;arbre &#233;l&#233;mentaire dans la
grammaire DOP d&#8217;origine.
</p>
<p>pas une grammaire probabiliste &#171; propre14 &#187;. &#13; equiv doit simplement &#234;tre vue comme un moyen
efficace d&#8217;impl&#233;menter la recherche de l&#8217;arbre d&#8217;analyse le plus probable dans le mod&#232;le DOP
(restreint) et non pas comme une grammaire stochastique en tant que telle. En particulier, le
mod&#232;le probabiliste sous-jacent reste bien celui associ&#233; au mod&#232;le DOP et non pas celui d&#8217;une
grammaire hors-contexte probabiliste (propre).
Si l&#8217;on consid&#232;re maintenant la phrase &#14; =&#8220;Pierrette mange avec Pierrette&#8221;, sa d&#233;rivation la plus
probable dans &#13; equiv est &#15;&#17;&#16; &#2; &#4;&#19;&#18;&#20;&#2; &#8;&#21;&#18;&#20;&#2; &#3; &#4; de probabilit&#233; &#1;&#23;&#22; &#15;&#25;&#24;&#20;&#16;ff&#26;&#25;fi fl&#31;ffi! &#26;&#25;fi ffi&quot; &#26;&#25;fi #%$ ffi&#17;&#16;ff&#26;&#25;fi &amp; &#26;&#31;' . Cette
d&#233;rivation correspond dans &#13; DOP &#224; la d&#233;rivation suivante :
</p>
<p>P
N V
</p>
<p>&#18;
</p>
<p>N
Pierrette
</p>
<p>&#18;
</p>
<p>GV
</p>
<p>V
mange
</p>
<p>GNP
P
</p>
<p>avec
</p>
<p>N
Pierrette
</p>
<p>et l&#8217;arbre d&#8217;analyse le plus probable (pour &#14; dans &#13; DOP) est donc :
</p>
<p>P
</p>
<p>N
Pierrette
</p>
<p>GV
</p>
<p>V
mange
</p>
<p>GNP
P
</p>
<p>avec
</p>
<p>N
Pierrette
</p>
<p>de DOP-probabilit&#233; &#233;gale &#224; &#26;&#25;fi &amp; &#26;&#31;' .
</p>
<p>14Traduction libre du terme proper introduit dans (Booth &amp; Thompson, 1973)</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>J.-C. Chappelier et M. Rajman
</p>
<p>corpus
nombre de nombre nombre nombre nombre longueur nb moyen
</p>
<p>phrases de r&#232;gles de non- de de moyenne de r&#232;gles
annot&#233;es hors-contexte terminaux terminaux PoS tags de phrase par phrase
</p>
<p>ATIS 1&#8217;381 1&#8217;027 40 1&#8217;167 38 12.5 23.3
Susanne 6&#8217;728 20&#8217;302 767 17&#8217;863 130 20.4 36.0
Susanne
r&#233;duit 4&#8217;000 8&#8217;882 469 10&#8217;284 122 12.9 23.8
</p>
<p>Table 2: Diff&#233;rentes caract&#233;ristiques des 2 corpus utilis&#233;s pour les exp&#233;riences.
</p>
<p>5 Exp&#233;riences
</p>
<p>Pour &#233;valuer de fa&#231;on exp&#233;rimentale les performances du mod&#232;le DOP restreint par le principe
de s&#233;lection minimale-maximale, nous avons utilis&#233; deux corpus diff&#233;rents : le corpus ATIS
(Hemphill et al., 1990) et le corpus Susanne3 (Sampson, 1994). Pour des raisons de taille de la
grammaire compl&#232;te du mod&#232;le restreint nous avons cependant d&#251; utiliser une version r&#233;duite
du corpus Susanne3 : parmi les 6803 phrases d&#8217;origine, nous n&#8217;avons finalement utilis&#233; que
4000 d&#8217;entre elles (ce corpus est appel&#233; &#171; Susanne r&#233;duit &#187; ci-apr&#232;s). Les caract&#233;ristiques des
ces corpus sont donn&#233;es en table 2.
</p>
<p>Contrairement &#224; la plupart des exp&#233;riences faites sur DOP jusqu&#8217;&#224; pr&#233;sent (Bod, 1998; Sima&#8217;an,
1996b; Goodman, 1996), nous n&#8217;avons pas mis les corpus sous forme normale de Chomsky (ar-
bres binaires), mais sommes plut&#244;t rest&#233;s le plus proche possible des donn&#233;es d&#8217;origine15. Dans
le m&#234;me ordre d&#8217;id&#233;e, nous n&#8217;avons pas tronqu&#233; les arbres au niveau des &#233;tiquettes morpho-
syntaxiques mais avons travaill&#233; sur les phrases elles-m&#234;mes (i.e. au niveaux des mots).
La m&#233;thodologie d&#8217;&#233;valuation utilis&#233;e pour la production de ces r&#233;sultats a &#233;t&#233; la m&#234;me pour
toutes les exp&#233;riences :
</p>
<p>&#0; partitionner (de fa&#231;on al&#233;atoire) le corpus en un corpus d&#8217;entra&#238;nement (90%) et un corpus
de test (les 10% restants) ;
</p>
<p>&#0; extraire la grammaire et les probabilit&#233;s &#224; partir du corpus d&#8217;entra&#238;nement. Le lexique est
par contre toujours extrait du corpus complet : nous n&#8217;avons pas cherch&#233; ici &#224; &#233;tudier le
comportement du mod&#232;le sur les mots inconnus.
</p>
<p>&#0; &#233;valuer les performances sur le corpus de test ; la mesure utilis&#233;e est le nombre d&#8217;arbres
d&#8217;analyse complets corrects obtenus.
</p>
<p>Pour chacun des deux corpus ATIS et &#171; Susanne r&#233;duit &#187; d&#233;crits pr&#233;c&#233;demment, les r&#233;sultats
fournis sont des moyennes sur au moins 10 partionnements al&#233;atoires ind&#233;pendants.
</p>
<p>Pour disposer d&#8217;une r&#233;f&#233;rence, nous avons &#233;galement extrait et &#233;valu&#233; la grammaire hors-
contexte probabiliste standard. Il ne nous a par contre pas encore &#233;t&#233; possible d&#8217;effectuer des
exp&#233;riences sur le mod&#232;le DOP complet en raison du nombre gigantesque d&#8217;arbres &#233;l&#233;men-
taires g&#233;n&#233;r&#233;s dans les conditions exp&#233;rimentales choisies (arbres &#1; -aires et prise en compte des
mots).
Les r&#233;sultats obtenus sont r&#233;sum&#233;s dans la table 3. Pour chaque mod&#232;le, &#171; % parsed &#187; repr&#233;sente
la couverture du mod&#232;le, c.-&#224;-d. le pourcentage de phrases du corpus de test qui ont obtenu au
moins une analyse16, &#171; % correct on parsed &#187; indique sa pr&#233;cision, c.-&#224;-d. le pourcentage de
</p>
<p>15Seules les productions vides (&#171; traces &#187;) et quelques erreurs manifestes ont &#233;t&#233; supprim&#233;es.
16Notez bien que ce nombre est par construction toujours le m&#234;me pour le mod&#232;le hors-contexte et pour le</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Grammaire &#224; substitution d&#8217;arbre de complexit&#233; polynomiale
</p>
<p>Susanne r&#233;duit ATIS
% % correct % correct % % correct % correct
</p>
<p>parsed on parsed overall parsed on parsed overall
test hors-contexte 45.5 23.0 10.5 99.6 25.4 25.3
</p>
<p>(10%) min-max DOP 45.5 24.4 11.1 99.6 21.0 20.9
entr.
</p>
<p>hors-contexte 100 61.2 61.2 100 33.8 33.8
min-max DOP 100 87.9 87.9 100 76.1 76.1
</p>
<p>Table 3: R&#233;sultats exp&#233;rimentaux obtenus dans les conditions de test (s&#233;paration al&#233;atoire du
corpus en 90%&#8211;10%) et sur le corpus d&#8217;entra&#238;nement complet (100%).
</p>
<p>phrases analys&#233;es pour lesquelles l&#8217;analyse la plus probable est correcte, et &#171; % correct overall &#187;
indique la performance globale du mod&#232;le, c.-&#224;-d. le pourcentage de phrases (parmi toutes celles
du corpus de test) pour lesquelles l&#8217;analyse la plus probable est correcte.
Les conclusions g&#233;n&#233;rales que nous pouvons tirer de ces r&#233;sultats sont les suivantes :
</p>
<p>1. La restriction minimale-maximale du mod&#232;le DOP fournit de meilleurs r&#233;sultats que les
grammaires hors-contexte probabilistes standard sur le corpus Susanne r&#233;duit. La dif-
f&#233;rence en performance globale observ&#233;e est effectivement statistiquement significative &#224;
un niveau de confiance de 95%.
</p>
<p>2. La performance et la pr&#233;cision sur le corpus ATIS corpus sont assez mauvaises pour les
deux mod&#232;les : cette mauvaise performance est li&#233;e &#224; la nature trop bruit&#233;e de ce corpus17.
</p>
<p>3. La faible couverture obtenue sur le corpus Susanne r&#233;duit est li&#233;e au tr&#232;s fort taux (77 %)
de r&#232;gles hors-contexte n&#8217;apparaissant qu&#8217;une fois dans tout le corpus (hapax).
</p>
<p>6 Conclusion
</p>
<p>Cette contribution pr&#233;sente une nouvelle approche du Data-Oriented Parsing : sa restriction
&#224; des grammaires &#224; substitution d&#8217;arbres polynomiales (PSTSG), c.-&#224;-d. &#224; des STSG pour
lesquelles la recherche de l&#8217;arbre d&#8217;analyse le plus probable peut s&#8217;effectuer de fa&#231;on exacte
en un temps polynomial (par opposition au caract&#232;re NP-difficile de cette recherche dans le cas
g&#233;n&#233;ral).
Le cas particulier de la restriction des arbres &#233;l&#233;mentaires aux seuls arbres de profondeur 1 et
aux arbres totalement ancr&#233;s (principe de s&#233;lection minimale-maximale) a &#233;t&#233; analys&#233;. On a
pu dans ce cas exhiber une grammaire hors-contexte probabiliste &#233;quivalente au mod&#232;le DOP
consid&#233;r&#233;, c.-&#224;-d. une grammaire hors-contexte pour laquelle la recherche de la d&#233;rivation la
plus probable est &#233;quivalente &#224; la recherche de l&#8217;analyse la plus probable au sein du mod&#232;le DOP
consid&#233;r&#233;. Cette construction est rendue possible gr&#226;ce au fait qu&#8217;avec la restriction appliqu&#233;e,
il existe toujours au moins une d&#233;rivation pouvant porter la totalit&#233; de la DOP-probabilit&#233; de
l&#8217;arbre d&#8217;analyse correspondant.
</p>
<p>Le mod&#232;le pr&#233;sent&#233; constitue donc un compromis int&#233;ressant entre le mod&#232;le DOP g&#233;n&#233;ral et les
grammaires hors-contexte : il est aussi &#171; simple &#187; &#224; analyser qu&#8217;une grammaire hors-contexte
</p>
<p>mod&#232;le DOP
17Ce qui a d&#233;j&#224; &#233;t&#233; &#233;voqu&#233; dans la litt&#233;rature : voir par exemple (Goodman, 1998) p. 179.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>J.-C. Chappelier et M. Rajman
</p>
<p>(complexit&#233; cubique) tout en permettant une probabilisation plus riche que celle des mod&#232;les
hors-contexte usuels.
</p>
<p>Une question encore ouverte sur laquelle nous travaillons actuellement est de savoir s&#8217;il existe
d&#8217;autres PSTSG, c.-&#224;-d. d&#8217;autres restrictions de DOP linguistiquement int&#233;ressantes, associ&#233;es
&#224; des m&#233;canismes de s&#233;lection des arbres &#233;l&#233;mentaires diff&#233;rents de celui pr&#233;sent&#233; ici, mais
poss&#233;dant les m&#234;mes propri&#233;t&#233;s d&#8217;efficacit&#233; (analyse polynomiale) et de facilit&#233; de construction.
Une comparaison avec d&#8217;autres extensions des SCFGs, comme par exemple les &#171; Stochastic
Lexicalized CFG &#187; (Schabes &amp; Waters, 1993), est &#233;galement envisag&#233;e.
</p>
<p>R&#233;f&#233;rences
BOD R. (1992). Applying Monte Carlo techniques to Data Oriented Parsing. In Proceedings Computa-
tional Linguistics in the Netherlands, Tilburg (The Netherlands).
BOD R. (1998). Beyond Grammar, An Experience-Based Theory of Language. Number 88 in CSLI
Lecture Notes. Standford (CA): CSLI Publications.
BOOTH T. L. &amp; THOMPSON R. A. (1973). Applying probability measures to abstract languages. IEEE
Transactions on Computers, C-22(5), 442&#8211;450.
CHAPPELIER J.-C. &amp; RAJMAN M. (1998). Extraction stochastique d&#8217;arbres d&#8217;analyse pour le mod&#232;le
DOP. In Proc. of 5&#232;me conf&#233;rence sur le Traitement Automatique du Langage Naturel (TALN98), p.
52&#8211;61, Paris (France).
CHAPPELIER J.-C. &amp; RAJMAN M. (2000). Monte-Carlo sampling for NP-hard maximization problems
in the framework of weighted parsing. In D. CHRISTODOULAKIS, Ed., Natural Language Processing &#8211;
NLP 2000, number 1835 in Lecture Notes in Artificial Intelligence, p. 106&#8211;117. Springer.
CHAPPELIER J.-C. &amp; RAJMAN M. (2001). Polynomial Tree Substitution Grammars: an efficient frame-
work for Data-Oriented Parsing. Rapport interne 01/tocome, D&#233;partement Informatique, EPFL, Lau-
sanne (Switzerland).
GOODMAN J. (1996). Efficient algorithms for parsing the DOP model. In Proc. of the Conf. on Empiri-
cal Methods in Natural Language Processing, p. 143&#8211;152.
GOODMAN J. (1998). Parsing Inside-Out. PhD thesis, Harvard University. cmp-lg/9805007.
HEMPHILL C. T., GODFREY J. J. &amp; DODDINGTON G. R. (1990). The ATIS spoken language systems
pilot corpus. In M. KAUFMANN, Ed., DARPA Speech and Natural Language Workshop.
SAMPSON G. (1994). The Susanne corpus, release 3. In School of Cognitive &amp; Computing Sciences,
Brighton (England): University of Sussex Falmer.
SCHA R. (1990). Language theory and language technology: competence and performance. In DE KORT
&amp; LEERDAM, Eds., Computertoepassingen in de Neerlandistiek. Almere (The Netherlands): LVVN-
jaarboek. in Dutch.
SCHABES Y. &amp; WATERS R. C. (1993). Stochastic Lexicalized Context-Free Grammars. Rapport interne
93&#8211;12, Mitshbishi Electric Research Labs.
SIMA&#8217;AN K. (1996a). Computational complexity of probabilistic disambiguation by means of tree gram-
mars. In Proceedings of COLING&#8217;96, Copenhagen (Denmark). cmp-lg/9606019.
SIMA&#8217;AN K. (1996b). Efficient disambiguation by means of stochastic tree substitution grammars. In
R. MITKOV &amp; N. NICOLOV, Eds., Recent Advances in NLP, volume 136 of Current Issues in Linguistic
Theory. Amsterdam (The Netherlands): Benjamins.</p>

</div></div>
</body></html>