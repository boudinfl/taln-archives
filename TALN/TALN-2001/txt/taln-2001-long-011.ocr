TALN 2001, Tours, 2-5 juillet 2001

Grammaire a substitution d’arbre de complexité
polynomiale : un cadre efﬁcace pour DOP

J ean-Cedric Chappelier et Martin Raj man
EPFL
DI-LIA, IN (Ecublens)
CH-1015 Lausanne, Switzerland

{Jean—Cedric.Chappelier,Martin.Rajman}@epfl.ch

Résumé - Abstract

Trouver l’arbre d’analyse le plus probable dans le cadre du modele DOP (Data-Oriented Pars-
ing) — une version probabiliste de grammaire a substitution d’arbres développée par R. Bod
(1992) — est connu pour étre un probleme NP-difﬁcile dans le cas le plus général (Sima’an,
1996a). Cependant, si l’on introduit des restrictions a priori sur le choix des arbres élémen-
taires, on peut obtenir des instances particulieres de DOP pour lesquelles la recherche de l’arbre
d’analyse le plus probable peut étre effectuée en un temps polynomial (par rapport a la taille
de la phrase a analyser). La présente contribution se propose d’étudier une telle instance poly-
nomiale de DOP, fondée sur le principe de sélection Iniminale-maximale et d’en évaluer les
performances sur deux corpus différents.

Finding the most probable parse tree in the framework of Data-Oriented Parsing (DOP), a
Stochastic Tree Substitution Parsing scheme developed by R. Bod (1992), has proven to be
NP-hard in the most general case (Sima’an, 1996a). However, introducing some a priori re-
strictions on the choice of the elementary trees leads to interesting DOP instances with poly-
nomial time-complexity. The purpose of this paper is to present such an instance, based on the
minimal-maximal selection principle, and to evaluate its performances on two different corpora.

1 Motivations

Introduite par R. Scha (Scha, 1990) puis développée par R. Bod (Bod, 1992; Bod, 1998),
l’approche Data-Oriented Parsing (DOP) pour l’ana1yse syntaxique probabiliste a depuis été
largement étudiée par diverses équipes de recherche. La principale limitation de cette approche
reste cependant le caractere NP-difﬁcile du probleme consistant a trouver l’arbre d’analyse le
plus probable (MPP, pour most probable parse) (Sima’an, 1996a). Différentes solutions ap-
prochées (heuristiques) ont été proposées (Bod, 1992; Goodman, 1996; Chappelier & Rajman,
1998) mais une autre direction de recherche prometteuse consiste a explorer différentes re-
strictions a priori du jeu d’arbres élémentaires utilisé par la grammaire DOP, restrictions pour
lesquelles trouver 1’ arbre d’ analyse le plus probable 11’ est plus un probleme NP-difﬁcile. L’ obj et
du présent article est de présenter et d’évaluer un exemple de telle restriction.

J .-C. Chappelier et M. Rajman

Nous commencerons tout d’ abord par une breve introduction au Data-Oriented Parsing et déﬁni-
rons quelques notations. Puis nous présenterons dans la seconde partie un principe de sélection
d’arbres élémentaires — le principe de sélection minimale-maximale — permettant d’obtenir
une restriction de DOP ayant une complexité polynomiale (pour le probleme du MPP). Nous
terminerons enﬁn par la présentation de plusieurs expériences sur deux corpus différents.

2 Data-Oriented Parsing

2.1 Le modéle DOP

DOP est un modele d’analyse syntaxique probabiliste a base de grammaires a substitution
d’arbres (STSG pour Stochastic Tree-Substitution Grammars). Une STSG est une grammaire
pour laquelle les regles sont des arbres élémentairesl que l’on peut combiner a l’aide de l’opé-
rateur de substitutionz pour obtenir les dérivations des arbres d’analyse.

Dans sa déﬁnition la plus générale, le modele DOP utilise comme arbres élémentaires l’ ensemble
de tous les sous-arbres des arbres d’analyse contenus dans un corpus annoté (treebank) disponible
pour l’entrainement du modele. Chaque arbre élémentaire t est associé a une probabilité élé-

mentaire p(t) qui est proportionnelle au nombre d’ occurrences de t dans le corpus d’ entrainement.

La probabilité p(d) d’une dérivation d est alors déﬁnie par3 p(d) = H p(t) et la DOP-probabilité
2: d
d’un arbre d’analyse T est donnée par : E

Pno1=(T) = 2 PW) = Z llP(t)

d=>T d=>T t€d

ou “d :> T” signiﬁe “pour toute dérivation d produisant l’arbre d’analyse T”4.

2.2 Analyse syntaxique la plus probable

L’ analyse syntaxique d’une phrase s’effectue généralement en deux étapes distinctes :

l’analyse proprement dite, dont le but est de produire une representation compacte de l’ ensemble
des arbres d’analyse possibles pour la phrase analysée ;

l’exploitation des résultats a partir de la structure compacte précédemment construite. Cette
étape peut par exemple consister a presenter l’ensemble de la forét d’analyse, a extraire
l’arbre d’analyse le plus probable, a extraire la dérivation la plus probable5, etc...

Dans le cas des STSGs, l’étape d’analyse peut, comme dans le cas des grammaires hors-

contexte, étre réalisée en un temps polynomial (cubique) par rapport a la taille de la phrase

a analyser. Par contre, l’extraction de l’arbre d’analyse le plus probable (MPP) constitue, dans

le cas général, un probleme NP-difﬁcile (Sima’an, 1996a) et ne peut donc pas s’effectuer en

lcomme pour les TAG (Tree Adjoining Grammars)

zmais {E de l’opérateur d’adj onction, contrairement aux TAG

3t E d représente le fait que le sous—arbret participe a la derivation d .

4Un meme arbre d’analyse peut en effet posséder plusieurs derivations différentes (et ce malgré la convention
de réécriture de la feuille non—terminale la plus a gauche en premier).

5Dans le cas de DOP, et a la difference des grammaires hors-contexte probabilistes classiques, l’arbre d’analyse
le plus probable et l’arbre associé a la derivation la plus probable ne sont pas nécessairement identiques.

Grammaire a substitution d’arbre de complexité polynomiale

general de facon efﬁcace (polynomiale). Notons toutefois que la derivation la plus probable
peut par contre etre trouvée en un temps cubique en utilisant les algorithmes usuels développés
pour les grammaires hors-contexte probabilistes.

Pour contoumer le caractere NP-difﬁcile de la recherche du MPP dans le modele DOP, diverses
heuristiques correspondant a des solutions approchées ont ete proposees : Monte-Carlo Sam-
pling (Bod, 1992), General Recall (Goodman, 1998), échantillonage controle (Chappelier &
Rajman, 1998; Chappelier & Rajman, 2000).

Une autre approche possible, développée dans cette contribution, consiste a explorer les restric-
tions du modele DOP pour lesquelles la recherche de l’arbre d’analyse le plus probable peut
se faire de facon exacte en un temps polynomial (cubique) par rapport a la taille de la phrase a
analyser. L’ idée génerale d’une telle approche consiste a limiter a priori l’ensemble des arbres
élémentaires constituant la grammaire du modele DOP de telle sorte que l’on puisse exhiber
une grammaire hors-contexte probabiliste qui soit équivalente a la STSG utilisée, c.-a-d. telle
que pour tout arbre d’analyse de DOP-probabilité maximale dans le modele DOP, il existe au
moins une derivation dans la grammaire hors-contexte équivalente dont la probabilité

1. est égale a la DOP-probabilité de cet arbre d’analyse ;

2. est maximale (parmi les probabilites de toutes les derivations produites pour la phrase

analysee par la grammaire hors-contexte équivalente).

Une STSG pour laquelle une grammaire hors-contexte probabiliste équivalente peut etre con-
struite sera appelée « grammaire probabiliste a substitution d ’arbres polynomiale » (PSTSG)6.

Notons que, dans ce cas, la complexité algorithmique liee a la sommation des probabilites sur
les diverses derivations d’un meme arbre d’analyse peut etre efﬁcacement factorisee7.

Tout l’enjeu de l’approche proposée ici est donc de trouver diverses restrictions du modele DOP
correspondant a des PSTSG. Un exemple trivial d’une telle restriction est donne par le modele
DOP o1‘1 les arbres élémentaires sont limités aux sous-arbres de profondeur 1. Ce modele DOP
est strictement equivalent a une grammaire hors-contexte probabiliste (SCFG) et ne présente
donc aucun apport par rapport aux SCFG usuelles8.

Dans la suite de cet article, nous nous proposons d’exaIniner une autre PSTSG, moins triviale
et linguistiquement plus intéressante. Elle correspond a la restriction du modele DOP dans
laquelle les arbres élémentaires sont limités aux sous-arbres de profondeur 1 et aux sous-arbres
totalement ancrés, i.e. les sous-arbres dont les feuilles sont toutes des terminaux de la gram-
maire (i.e. des mots). Ce choix pour les arbres élémentaires sera appele principe de sélection
minimale-maximale 9 et la restriction du modele DOP correspondante sera appelée sa restriction
minimale-maximale.

Nous allons maintenant montrer qu’une telle STSG est effectivement polynomiale, c.-a-d. qu’il
existe une grammaire hors-contexte qui lui soit équivalente au sens déﬁni ci-dessus.

Gpour Polynomial Stochastic Tree Substitution Grammar. << polynomiale » fait ici reference a la complexite du
probleme MPP pour une telle grammaire.

7C’est precisement cette sommation necessaire qui est la source du caractere NP—difﬁci1e de la recherche du
MPP dans le cas general.

8En particulier la probabilisation des analyses est strictement la meme.

9Ce principe de selection a ete introduit pour la premiere fois par Jacques Han dans son travail doctoral non
encore publie.

J .-C. Chappelier et M. Rajman

3 Restriction minimale-maximale de DOP

3.1 Déﬁnition de la grammaire hors-contexte équivalente

Pour toute grammaire DOP QDOP, soit Qequiv la grammaire hors-contexte contenant toutes les
regles racine-feuilles“) associees a tous les arbres elementaires de Qmp.“ D’un point de vue
purement structurel (i.e. sans les probabilites) Q D013 est equivalente a Qequiwlz Cette propriete est
toujours vraie, meme dans le cas general de DOP, mais dans le cas particulier des restrictions
minimales-maximales, cette equivalence peut étre etendue aux probabilites. Plus precisement,
il devient alors possible de probabiliser Qequiv de sorte que trouver la derivation la plus probable
au sens de Qequiv soit equivalent a trouver l’analyse la plus probable au sens de Qpop.

La solution permettant d’obtenir une telle equivalence consiste a associer a chaque regle de
Qequiv un coefﬁcient stochastique correspondant a la DOP-probabilite de l’arbre elementaire
correspondant. Remarquez bien la difference avec la demarche usuelle qui consiste a associer a
la regle la probabilite elementaire p de l’arbre elementaire et non pas sa DOP-probabilite.

Nous montrerons dans la section 3.3 qu’une telle probabilisation de Qequiv est facile a realiser
lors de la construction de Qpop a partir du corpus d’entrainement. Mais montrons tout d’abord
qu’avec une telle probabilisation, trouver la derivation la plus probable avec Qequiv est effective-
ment equivalent a trouver l’analyse la plus probable au sens de Qpop.

3.2 Demonstration de l’équivalence probabiliste

L’ equivalence a etablir repose sur la propriete generale suivante : la probabilite d’une derivation
dans Qequiv est toujours inferieure ou egale a la DOP-probabilite de l’arbre d’analyse de Qpop
equivalent. La demonstration de cette propriete, trop longue pour etre donnee ici, peut etre
trouvee dans (Chappelier & Rajman, 2001). A l’aide de cette propriete, pour montrer qu’il est
equivalent de trouver la derivation la plus probable au sens de Qequiv et l’arbre d’analyse le plus
probable au sens de Qpop, il est donc sufﬁsant de montrer que, pour chaque arbre d’analyse T
au sens de Qpop, il existe au moins une derivation equivalente dans Qequiv dont la probabilite est
égale 61 la DOP-probabilite de T.

Cette derniere propriete peut se demontrer par recurrence sur la profondeur de T :

1) La propriete est trivialement vraie pour tout arbre de profondeur 1, puisque dans ce cas il n’y
a qu’une derivation possible de l’arbre d’analyse et la DOP-probabilite de cet arbre est alors,
par construction, egale a la probabilite de cette derivation.

2) Supposons maintenant que pour tout arbre d’analyse (au sens de Qpop) T de profondeur au
plus n, il existe dans Qequiv une derivation d equivalente a T dont la probabilite est la DOP-
probabilite de T. Nous devons alors etablir que cette proposition est egalement vraie pour tout
arbre d’analyse de profondeur n + 1.

1°Pour tout arbre T, la regle racine—feuilles associee R(T) est la regle hors-contexte dont la partie gauche est la
racine de T et la partie droite la sequence des feuilles de T.

“Notez qu’une regle hors-contexte differente est creee pour chacun des arbres elementaires. Les regles asso-
ciées a des arbres elementaires differents mais ayant meme racine et memes feuilles sont differenciees par leur
indices. 11 y a done bijection entre les regles de Qequiv et les arbres elementaires de QDOP.

1211 y a en effet bij ection entre les derivations de ces deux grammaire. La reconstruction de l’arbre d’analyse
complet a partir d’une derivation dans Qequiv peut étre effectuee de fagon triviale a1’aide des indices des regles de
Qequiv qui referencent egalement les arbres elementaires de QDOP.

Grammaire a substitution d’arbre de complexité polynomiale

t1

MAR
T X1 X2  Xq
Figure 1: Dérivation avec une grammaire suivant le principe de restriction minimale-maximale
de DOP : toute dérivation d’un arbre d’analyse donné (T ici), qui n’est pas lui-méme un arbre

élémentaire, commence par un arbre élémentaire de profondeur 1 (t1 ici), lequel est partagé par
toutes les dérivations de l’arbre d’analyse considéré.

Soit maintenant T =>* Wf’ un arbre d’analyse (de Qpop) de profondeur n +1 de la chaine Wf’ . Si
T est lui-méme un arbre élémentaire de la grammaire Qpop, alors la regle hors-contexte racine-
feuilles R(T) qui lui est associée est une regle de Qequiv dont la probabilité est par construction
la DOP-probabilité de T. Et puisque R(T) est une dérivation de Wf’, il existe au moins une
dérivation représentant T dans Qequiv (et dont la probabilité est la DOP-probabilité de T).

Si au contraire T n’est pas un arbre élémentaire de Qpop, alors T possede au moins une dériva-
tion. Notons-la d = t1 0  o tk (k > 1).

Il est alors important de remarquer qu’en raison du choix des arbres élémentaires imposé par
la sélection minimale-maximale, t1 est nécessairement un arbre de profondeur 1. Il est de plus
partagé par toutes les dérivations de T. La DOP-probabilité de T est donc :

P.m<T> = Z Hp<t>:p<t1> Z  Z ﬁp<d.>

d’=>T ted’ d1=>T1 dq=>Tq 2'21
‘1 ‘J
: p(t1)‘H ( Z  : p(t1)‘H PDoP(T2')
i=1 d,‘=>T,‘ i=1
en notant T1, ..., Tq les sous-arbres totalement ancrés de T qui sont ﬁls de t1 dans d (cf ﬁg 1).

Or, puisque T1, ..., Tq sont de profondeur au plus n, il existe par hypothese de récurrence
pour chaque T.» une dérivation dequiV(T.») dans Qequiv dont la probabilité (dans Qequiv) est égale a
PDoP(T2')-

La dérivation (t1, dequiV(T1), ..., dequiV(Tq)) est une dérivation de T au sens de Qequiv dont la prob-
abilité est13 p(t1) - PDoP(T1) -  - PDoP(Tq), c.-a-d. la DOP-probabilité de T.

11 existe donc au moins une dérivation de T au sens de Qequiv dont la probabilité est égale a la
DOP-probabilité de T. D

Rechercher la dérivation la plus probable au sens de Qequiv est donc bien equivalent a rechercher
l’arbre d’analyse le plus probable au sens de Qpop.

3.3 Construction pratique de la grammaire hors-contexte équivalente

I1 nous faut maintenant expliquer comment la grammaire hors-contexte équivalente Qequiv peut
étre construite a partir du corpus d’entrainement utilisé pour le modele DOP considéré.

“par construction (dérivation hors-contexte)

J .-C. Chappelier et M. Rajman

Les arbres de profondeur 1 sont tout d’abord extraits, de facon identique a la constitution d’une
grammaire hors-contexte a partir d’un corpus annoté. Puis, pour chaque noeud de chaque arbre
du corpus, le sous-arbre totalement ancré correspondant est extrait et la regle racine-feuilles
associée est ajoutée aux regles de Qequiv (en regroupant les occurrences multiples correspondant
au méme arbre élémentaire mais en différenciant, par leurs indices, les regles racine-feuilles
identiques provenant d’arbres élémentaires différents).

La DOP-probabilité de chaque sous-arbre totalement ancré est ensuite calculée par ordre de
profondeur d’arbre croissante. En effet, si la DOP-probabilité de tout arbre élémentaire de

profondeur n a deja eté calculée, alors la DOP-probabilité de tout arbre élémentaire T de pro-
fondeur n + 1 peut facilement étre calculée par la formule suivante :

‘J

PDoP(T) : MT) ‘l’ PU1) ‘ H PDoP(T2')

i=1

4 Un exemple

Considérons maintenant un exemple jouet illustrant 1’ approche précédemment détaillée et mon-
trant en quoi notre approche differe du modele DOP général.

Considérons le corpus d’entrainement trivial contenant 2 arbres suivants :

P P
\ \
17] Y

Prep N / GNP

/ \
Pierre mange avlec Pierrette N V Prep N
I

I
Pierre mange avec Pierrette

Pour construire la STSG QDOP suivant le principe de sélection minimale-maximale, les sous-
arbres de profondeur 1 ainsi que tous les sous-arbres totalement ancrés doivent étre extraits ;
ce qui donne les 12 arbres suivants : 8 arbres de profondeur 1, t1 et t2 eux-mémes, et les deux
sous-arbre totalement ancrés suivants :

GV

/GNP\ \ GNP
Prep lTI / \
avlec Pierrette V Prlep ITI
mange avec Pierrette
Remarquons qu’un arbre comme par exemple
/ P\\
N V GNP

Pierre maiige

n’appartient pas a Qpop, alors que cela aurait été le cas dans la version non restreinte de DOP.
Comme Qpop contient 12 arbres élémentaires, la grammaire hors-contexte équivalente Qequiv
contient 12 regles, lesquelles sont données dans la table 1. Remarquez bien que la regle

P —> Pierre mange avec Pierrette apparait deux fois dans la grammaire. Ceci est dﬁ
au fait que chacune de ces deux regles correspond a un arbre élémentaire de Qpop different.

Notez aussi que Za P(X —> oz) n’est pas égale a l dans le cas général, c.-a-d. que Qequiv n’est

Grammaire a substitution d’arbre de complexité polynomiale

regle PDOP p
r1: P —> N V GNP 0.25 0.25
r2: P —> N GV 0.25 0.25
T32 P —> Pierre mange avec Pierrette 0.344 0.25
T42 P —> Pierre mange avec Pierrette 0.359 0.25
T5: N —> Pierre 0.5 0.5
T62 N —> Pierrette 0.5 0.5
T7: V —> mange 1.0 1.0
T3: Prep —> avec 1.0 1.0
r9: GNP —> Prep N 0.5 0.5
T103 GNP —> avec Pierrette 0.75 0.5
7111: GV —> V GNP 0.5 0.5
T123 GV —> mange avec Pierrette 0.875 0.5

Table 1: Les 12 regles de la grammaire hors-contexte équivalente. Le coefﬁcient stochastique
est donné par PDOP. p correspond a la probabilité élémentaire de l’arbre élémentaire dans la
grammaire DOP d’origine.

pas une grammaire probabiliste « pr0pre14 ». Qequiv doit simplement étre vue comme un moyen
efﬁcace d’implémenter la recherche de l’arbre d’analyse le plus probable dans le modele DOP
(restreint) et non pas comme une grammaire stochastique en tant que telle. En particulier, le
modele probabiliste sous-jacent reste bien celui associé au modele DOP et non pas celui d’une
grammaire hors-contexte probabiliste (propre).

Si l’on considere maintenant la phrase s=“Pierrette mange avec Pierrette”, sa dérivation la plus
probable dans Qequiv est d : r2 0 r6 0 7112 de probabilité p(d) : 0.25 - 0.5 - 0.875 : 0.109. Cette
dérivation correspond dans Qpop a la dérivation suivante :

GV
\

GNP

P N P/\

/ \ o . ' 0 V N
N V Pierrette

I . '
mange avec Pierrette

et l’arbre d’analyse le plus probable (pour 3 dans Qpop) est donc :

P

. I | . I
Pierrette mange avec Pierrette

de DOP-probabilité égale a 0.109.

14Traduction1ibre du tenne proper introduit dans (Booth & Thompson, 1973)

J .-C. Chappelier et M. Rajman

Table 2: Differentes caracteristiques des 2 corpus utilises pour les experiences.

5 Experiences

Pour evaluer de facon experimentale les performances du modele DOP restreint par le principe
de selection minimale-maximale, nous avons utilise deux corpus differents : le corpus ATIS
(Hemphill et al., 1990) et le corpus Susanne3 (Sampson, 1994). Pour des raisons de taille de la
grammaire complete du modele restreint nous avons cependant dﬁ utiliser une version reduite
du corpus Susanne3 : parmi les 6803 phrases d’origine, nous n’avons ﬁnalement utilise que
4000 d’entre elles (ce corpus est appele « Susanne reduit » ci-apres). Les caracteristiques des
ces corpus sont donnees en table 2.

Contrairement a la plupart des experiences faites sur DOP jusqu’a present (Bod, 1998; Sima’an,
1996b; Goodman, 1996), nous n’avons pas Inis les corpus sous forme normale de Chomsky (ar-
bres binaires), mais sommes plutot restes le plus proche possible des donnees d’origine15. Dans
le meme ordre d’idee, nous n’avons pas tronque les arbres au niveau des etiquettes morpho-
syntaxiques mais avons travaille sur les phrases elles-memes (i.e. au niveaux des mots).

La methodologie d’evaluation utilisee pour la production de ces resultats a ete la meme pour
toutes les experiences :

0 partitionner (de facon aleatoire) le corpus en un corpus d’entrainement (90%) et un corpus
de test (les 10% restants) ;

0 extraire la grammaire et les probabilites a partir du corpus d’entrainement. Le lexique est
par contre toujours extrait du corpus complet : nous n’avons pas cherche ici a etudier le
comportement du modele sur les mots inconnus.

o evaluer les performances sur le corpus detest ; la mesure utilisee est le nombre d’arbres
d’analyse complets corrects obtenus.

Pour chacun des deux corpus AT IS et « Susanne reduit » decrits precedemment, les resultats
fournis sont des moyennes sur au moins 10 partionnements aleatoires independants.

Pour disposer d’une reference, nous avons egalement extrait et evalue la grammaire hors-
contexte probabiliste standard. 11 ne nous a par contre pas encore ete possible d’effectuer des
experiences sur le modele DOP complet en raison du nombre gigantesque d’arbres elemen-
taires generes dans les conditions experimentales choisies (arbres n-aires et prise en compte des
mots).

Les resultats obtenus sont resumes dans la table 3. Pour chaque modele, « % parsed » represente
la couverture du modele, c.-a-d. le pourcentage de phrases du corpus de test qui ont obtenu au
moins une analyse16, « % correct on parsed » indique sa precision, c.-a-d. le pourcentage de

15Seules les productions Vides (<< traces ») et quelques erreurs manifestes ont ete supprimees.
15Notez bien que ce nombre est par construction toujours le meme pour le modele hors-contexte et pour le

nombre de nombre nombre nombre nombre longueur nb moyen

corpus phrases de regles de non- de de moyenne de regles

annotees hors-contexte terminaux terminaux PoS tags de phrase par phrase

ATIS 1’381 1’027 40 1’ 167 38 12.5 23.3

Susanne 6’728 20’302 767 17’863 130 20.4 36.0

S,“Sa.m‘° 4’ooo 8’882 469 10’284 122 12.9 23.8
redu1t

Grammaire a substitution d’arbre de complexité polynomiale

Susanne réduit AT IS
% % correct % correct % % correct % correct
parsed on parsed overall parsed on parsed overall

test hors-contexte 45.5 23.0 10.5 99.6 25.4 25.3
(10%) min-max DOP 45.5 24.4 11.1 99.6 21.0 20.9
hors-contexte 100 61.2 61.2 100 33.8 33.8

"'““' min-max DOP 100 87.9 87.9 100 76.1 76.1

Table 3: Résultats expérimentaux obtenus dans les conditions de test (séparation aléatoire du
corpus en 90%—10%) et sur le corpus d’entrainement complet (100%).

phrases analysées pour lesquelles l’analyse la plus probable est correcte, et « % correct overall »
indique la performance globale du modele, c.-a-d. le pourcentage de phrases (parmi toutes celles
du corpus detest) pour lesquelles l’analyse la plus probable est correcte.

Les conclusions générales que nous pouvons tirer de ces résultats sont les suivantes :

1. La restriction minimale-maximale du modele DOP fournit de meilleurs résultats que les
grammaires hors-contexte probabilistes standard sur le corpus Susanne réduit. La dif-
férence en performance globale observée est effectivement statistiquement signiﬁcative a
un niveau de conﬁance de 95%.

2. La performance et la précision sur le corpus ATIS corpus sont assez mauvaises pour les
deux modeles : cette mauvaise performance est liée a la nature trop bruitée de ce corpus”.

3. La faible couverture obtenue sur le corpus Susanne réduit est liée au tres fort taux (77 %)
de regles hors-contexte n’apparaissant qu’une fois dans tout le corpus (hapax).

6 Conclusion

Cette contribution présente une nouvelle approche du Data-Oriented Parsing : sa restriction
a des grammaires a substitution d’arbres polynomiales (PSTSG), c.-a-d. a des STSG pour
lesquelles la recherche de l’arbre d’analyse le plus probable peut s’effectuer de facon exacte
en un temps polynomial (par opposition au caractere NP-difﬁcile de cette recherche dans le cas
général).

Le cas particulier de la restriction des arbres élémentaires aux seuls arbres de profondeur 1 et
aux arbres totalement ancrés (principe de sélection minimale-maximale) a été analysé. On a
pu dans ce cas exhiber une grammaire hors-contexte probabiliste équivalente au modele DOP
considéré, c.-a-d. une grammaire hors-contexte pour laquelle la recherche de la dérivation la
plus probable est équivalente a la recherche de 1’ analyse la plus probable au sein du modele DOP
considéré. Cette construction est rendue possible grace au fait qu’avec la restriction appliquée,
il existe toujours au moins une dérivation pouvant porter la totalité de la DOP-probabilité de
1’ arbre d’ analyse correspondant.

Le modele présenté constitue donc un compromis intéressant entre le modele DOP général et les
grammaires hors-contexte : il est aussi « simple » a analyser qu’une grammaire hors-contexte

modéle DOP
“Ce qui a déja été évoqué dans la littérature : Voir par exemple (Goodman, 1998) p. 179.

J .—C. Chappelier et M. Rajman

(complexité cubique) tout en permettant une probabilisation plus riche que celle des modeles
hors-contexte usuels.

Une question encore ouverte sur laquelle nous travaillons actuellement est de savoir s’il existe
d’autres PSTSG, c.-a-d. d’autres restrictions de DOP linguistiquement intéressantes, associées
a des mécanismes de sélection des arbres élémentaires différents de celui présenté ici, mais
possédant les memes propriétés d’efﬁcacité (analyse polynomiale) et de facilité de construction.

Une comparaison avec d’autres extensions des SCFGs, comme par exemple les « Stochastic
Lexicalized CFG » (Schabes & Waters, 1993), est également envisagée.

Références

BOD R. (1992). Applying Monte Carlo techniques to Data Oriented Parsing. In Proceedings Computa-
tional Linguistics in the Netherlands, Tilburg (The Netherlands).

BOD R. (1998). Beyond Grammar, An Experience-Based Theory of Language. Number 88 in CSLI
Lecture Notes. Standford (CA): CSLI Publications.

BOOTH T. L. & THOMPSON R. A. (1973). Applying probability measures to abstract languages. IEEE
Transactions on Computers, C-22(5), 442-450.

CHAPPELIER J .—C. & RAJMAN M. (1998). Extraction stochastique d’arbres d’analyse pour le modele
DOP. In Proc. of 5eme confe’rence sur le Traitement Automatique du Langage Naturel (TALN98), p.
52-61, Paris (France).

CHAPPELIER J .—C. & RAJMAN M. (2000). Monte—Carlo sampling for NP—hard maximization problems
in the framework of weighted parsing. In D. CHRISTODOULAKIS, Ed., Natural Language Processing —
NLP 2000, number 1835 in Lecture Notes in Artiﬁcial Intelligence, p. 106-117. Springer.

CHAPPELIER J .—C. & RAJMAN M. (2001). Polynomial Tree Substitution Grammars: an eﬂicient frame-
work for Data-Oriented Parsing. Rapport interne 01/tocome, Département Informatique, EPFL, Lau-
sarme (Switzerland).

GOODMAN J . (1996). Efﬁcient algorithms for parsing the DOP model. In Proc. of the Conf on Empiri-
cal Methods in Natural Language Processing, p. 143-152.

GOODMAN J . (1998). Parsing Inside-Out. PhD thesis, Harvard University. cmp—lg/9805007.

HEMPHILL C. T., GODFREY J . J . & DODDINGTON G. R. (1990). The ATIS spoken language systems
pilot corpus. In M. KAUFMANN, Ed., DARPA Speech and Natural Language Workshop.

SAMPSON G. (1994). The Susarme corpus, release 3. In School of Cognitive & Computing Sciences,
Brighton (England): University of Sussex Falmer.

SCHA R. (1990). Language theory and language technology: competence and performance. In DE KORT
& LEERDAM, Eds., Computertoepassingen in de Neerlandistiek. Almere (The Netherlands): LVVN—
jaarboek. in Dutch.

SCHABES Y. & WATERS R. C. (1993). Stochastic Lexicalized Context-Free Grammars. Rapport interne
93-12, Mitshbishi Electric Research Labs.

SIMA’ AN K. (1996a). Computational complexity of probabilistic disambiguation by means of tree gram-
mars. In Proceedings of COLING’96, Copenhagen (Demnark). cmp—lg/9606019.

SIMA’ AN K. (1996b). Efﬁcient disambiguation by means of stochastic tree substitution grammars. In
R. MITKOV & N. NICOLOV, Eds., Recent Advances in NLP, volume 136 of Current Issues in Linguistic
Theory. Amsterdam (The Netherlands): Benj amins.

