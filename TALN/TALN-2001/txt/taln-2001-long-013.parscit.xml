<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>CELEX</author>
</authors>
<title>http://www.ldc.upenn.edu/readme_files/celex.readme.html, UPenns, Eds., Actes Consortium for Lexical Resources,</title>
<date>1998</date>
<publisher>MIT Press.</publisher>
<location>New</location>
<contexts>
<context position="9414" citStr="CELEX 1998" startWordPosition="1410" endWordPosition="1411">ioration prévue consiste à utiliser WordNet pour compléter les catégories sémantiques. 3.2 Reconnaissance des entités nommées Les entités nommées sont marquées dans les documents par une balise dont le type correspond aux types de réponses présentés à la Figure 2. Les types retenus sont reconnus par des règles grâce à l’exploitation conjointe de deux sources d’information : • des lexiques généraux permettant de trouver des traits syntaxiques et sémantiques associés aux mots simples en complément de traits lexicaux, et • des dictionnaires d&apos;entités nommées. Les ressources utilisées sont CELEX (CELEX 1998), un lexique de 160 595 mots fléchis auxquels sont associés leur lemme et leur catégorie syntaxique, une liste de 8 070 prénoms (6 763 provenant de l&apos;archive de CLR (CLR 1998)) et une liste de 211 587 noms de familles, provenant aussi de CLR, une liste de 22 095 entreprises provenant du &amp;quot;Wall Street Research Network&amp;quot; et 649 noms d&apos;organisations obtenus à partir d&apos;une acquisition lexicale sur Internet (Jacquemin 2000), deux listes dédiées aux noms de lieux : l&apos;une de 7 813 villes et l&apos;autre de 1 144 pays issus de CLR, plus des listes constituées manuellement sur les unités physiques et monétair</context>
<context position="12858" citStr="CELEX 1998" startWordPosition="1955" endWordPosition="1956">ments pertinents, c&apos;est à dire ceux qui contiennent les réponses correctes, pour plus de 95% des questions. Une indexation automatique de ces documents en fonction des termes de la question est ensuite faite par FASTR, un analyseur transformationnel de surface pour la reconnaissance de variantes terminologiques. Les termes extraits de la question sont transformés en règles de grammaire et les mots simples qui les composent sont stockés dans un lexique munis de liens morphologiques et sémantiques. La famille morphologique d&apos;un mot simple m est l&apos;ensemble M(m) des mots simples de la base CELEX (CELEX 1998) qui ont la même racine que m . Par exemple, la famille morphologique du nom maker (fabricant) se compose des noms maker, make (marque) et remake (remake), et des verbes to make (faire) et to remake (refaire). La famille sémantique d&apos;un mot simple m est l&apos;union S(m) des synsets de WordNet1.6 (Fellbaum 1998) auxquels ce mot m appartient. Un synset est l&apos;ensemble des mots qui partagent un lien de synonymie sur une de leurs entrées sémantiques. Par exemple, la famille sémantique de maker se compose de trois noms : maker, manufacturer (fabricant), shaper (façonneur) et la famille sémantique de car</context>
</contexts>
<marker>CELEX, 1998</marker>
<rawString>CELEX, 1998, http://www.ldc.upenn.edu/readme_files/celex.readme.html, UPenns, Eds., Actes Consortium for Lexical Resources, (1998) CLR, 1998, http://crl.nmsu.edu/cgi-bin/Tools/CLR/clrcat#D3, NMSUs, Eds., Actes Consortium for Lexical Resources, New Mexico (1998) Fellbaum C., (1998) WordNet: An Electronic Lexical Database, Cambridge, MA, MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Ferret</author>
<author>B Grau</author>
<author>M Hurault-Plantet</author>
<author>G Illouz</author>
<author>C Jacquemin</author>
</authors>
<date>2000</date>
<booktitle>QALC — the Question-Answering system of LIMSI-CNRS, pre-proceedings of TREC9,</booktitle>
<pages>NIST.</pages>
<contexts>
<context position="12031" citStr="Ferret et al. 2000" startWordPosition="1824" endWordPosition="1827">vants sont extraits : US helicopter pilot (pilote d&apos;hélicoptère américain), helicopter pilot (pilote d&apos;hélicoptère), pilot (pilote) et shoot (abattre). 1 JJ : adjectif, NN : nom, NP : nom propre, VBG et VBD : verbe au gérondif ou au participe passé, ? : indiquant au plus une occurrence de l’un des éléments entre parenthèses. Entités nommées et variantes dans un système de question-réponse 4.2 Reconnaissance et marquage des variantes par FASTR Pour chaque question, nous ne retenons que les 200 premiers documents renvoyés par le moteur de recherche2. D&apos;après les tests3 que nous avons effectués (Ferret et al. 2000), c&apos;est le nombre minimum de documents qui permet de conserver le maximum de documents contenant la réponse. Les performances des moteurs de recherche que nous avons utilisé sont très bonnes : ils conservent les documents pertinents, c&apos;est à dire ceux qui contiennent les réponses correctes, pour plus de 95% des questions. Une indexation automatique de ces documents en fonction des termes de la question est ensuite faite par FASTR, un analyseur transformationnel de surface pour la reconnaissance de variantes terminologiques. Les termes extraits de la question sont transformés en règles de gramm</context>
</contexts>
<marker>Ferret, Grau, Hurault-Plantet, Illouz, Jacquemin, 2000</marker>
<rawString>Ferret O., Grau B., Hurault-Plantet M., Illouz G., Jacquemin C. (2000), QALC — the Question-Answering system of LIMSI-CNRS, pre-proceedings of TREC9, NIST.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Grishman</author>
<author>B Sundheim</author>
</authors>
<date>1995</date>
<booktitle>Design of the MUC-6 evaluation, Actes de MUC-6, NISTs, Eds.,</booktitle>
<publisher>Morgan Kauffmann Publisher,</publisher>
<location>Columbia, MD.</location>
<marker>Grishman, Sundheim, 1995</marker>
<rawString>Grishman R., Sundheim B., (1995), Design of the MUC-6 evaluation, Actes de MUC-6, NISTs, Eds., Morgan Kauffmann Publisher, Columbia, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Harabagiu</author>
<author>M Pasca</author>
<author>J Maiorano</author>
</authors>
<date>2000</date>
<booktitle>Experiments with Open-Domain Textual Question Answering, Actes de Coling&apos;2000,</booktitle>
<location>Saarbrucken, Germany.</location>
<contexts>
<context position="24790" citStr="Harabagiu et al, 2000" startWordPosition="3958" endWordPosition="3961"> à ce que serait la forme affirmative de la question posée. 7 Résultats et discussion Dans le cadre de TREC9, les résultats de QALC ont été évalués dans trois conditions différentes, les variations de l’une à l’autre concernant le moteur de recherche utilisé et la taille de la réponse (250 ou 50 caractères). Le meilleur de ces tests a obtenu un score de 0,407 avec 375 réponses trouvées sur 682, pour des réponses sur 250 caractères. Le calcul de ce score tient compte du rang de classement (de 1 à 5) de la réponse trouvée. Ce score nous a placé en 6ième position sur 28 participants. Le premier (Harabagiu et al, 2000) a obtenu un score de 0,760, le deuxième (Kwok et al, 2000) un score de 0,464, et les trois suivants respectivement des scores de 0,460, 0,457 (Ittycheriah et al, 2000) et 0,425. La conférence d’évaluation TREC offre une référence intéressante pour mesurer l’efficacité des méthodes utilisées par les différents systèmes de question-réponse. L’architecture de base généralement adoptée par les systèmes participants est conforme à celle de notre système QALC, avec éventuellement quelques variantes. 5 Nous ne retenons ici que les expressions les plus longues des différents termes Entités nommées et</context>
<context position="26223" citStr="Harabagiu et al, 2000" startWordPosition="4177" endWordPosition="4180">éponse courte (50 caractères). Parmi les questions posées à TREC, certaines attendent en réponse une entité nommée telle qu’une date, un nom de personne ou un nom d’organisation par exemple. Dans ce cas, le typage de la réponse est simple même si son exploitation nécessite un bon système de reconnaissance des entités nommées et si le nombre de catégories de types peut être augmenté de manière à raffiner le typage. En revanche, lorsque la réponse attendue est constituée d’un nom commun ou d’une phrase, son typage, plus complexe, est rarement réalisé. Certains systèmes, comme le système FALCON (Harabagiu et al, 2000), utilisent les hiérarchies des classes de mots dans WordNet pour typer les réponses. Pour sa part, le système développé par (Ittycheriah et al, 2000) se fonde sur un modèle de l’entropie maximum pour la classification des types de réponse. Sur les 682 questions de TREC9, 57,5% ont été analysées par QALC comme étant des questions à entité nommée, les autres n’ont pas été typées. Parmi les réponses correctes de notre meilleur test, 62,7% répondent à des questions à entité nommée. Par ailleurs, le test que nous avons fait pour des réponses plus courtes donne 84% de réponses possédant une entité </context>
<context position="27906" citStr="Harabagiu et al, 2000" startWordPosition="4447" endWordPosition="4450">ctionnent le ou les paragraphes pertinents de chaque document retrouvé. Comme dans QALC, la recherche des meilleures réponses est fondée sur un appariement entre question et réponse reposant sur la comparaison des mots des questions avec ceux des phrases sélectionnées et tenant compte du type attendu de la réponse et des entités nommées. Les critères retenus pour effectuer l’appariement peuvent varier d’un système à l’autre. Kwok et al (Kwok et al, 2000) par exemple utilisent entre autres un dictionnaire de synonymes qu’ils ont extrait manuellement de WordNet. Le système FALCON, quant à lui, (Harabagiu et al, 2000) utilise une approche sémantique pour réaliser cet appariement : une unification est recherchée entre la représentation sémantique de la question et les représentations sémantiques des paragraphes sélectionnés. C’est également le seul système à effectuer une justification de ses réponses. 8 Conclusion Un système de question-réponse doit trouver la réponse à une question précise, dans un temps suffisamment court pour être compatible avec une éventuelle utilisation interactive. Cette réponse étant recherchée dans une grande masse de documents, il est tentant d’appliquer des méthodes essentiellem</context>
</contexts>
<marker>Harabagiu, Pasca, Maiorano, 2000</marker>
<rawString>Harabagiu S., Pasca M., Maiorano J., (2000), Experiments with Open-Domain Textual Question Answering, Actes de Coling&apos;2000, Saarbrucken, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ittycheriah</author>
<author>M Franz</author>
<author>W-J Zhu</author>
<author>A Ratnaparkhi</author>
</authors>
<title>IBM&apos;s statistical Question Answering System, , Actes préliminaires de TREC9,</title>
<date>2000</date>
<pages>60--65</pages>
<publisher>Eds,</publisher>
<location>Gaithersburg, MD, NIST</location>
<contexts>
<context position="24958" citStr="Ittycheriah et al, 2000" startWordPosition="3988" endWordPosition="3991">ons différentes, les variations de l’une à l’autre concernant le moteur de recherche utilisé et la taille de la réponse (250 ou 50 caractères). Le meilleur de ces tests a obtenu un score de 0,407 avec 375 réponses trouvées sur 682, pour des réponses sur 250 caractères. Le calcul de ce score tient compte du rang de classement (de 1 à 5) de la réponse trouvée. Ce score nous a placé en 6ième position sur 28 participants. Le premier (Harabagiu et al, 2000) a obtenu un score de 0,760, le deuxième (Kwok et al, 2000) un score de 0,464, et les trois suivants respectivement des scores de 0,460, 0,457 (Ittycheriah et al, 2000) et 0,425. La conférence d’évaluation TREC offre une référence intéressante pour mesurer l’efficacité des méthodes utilisées par les différents systèmes de question-réponse. L’architecture de base généralement adoptée par les systèmes participants est conforme à celle de notre système QALC, avec éventuellement quelques variantes. 5 Nous ne retenons ici que les expressions les plus longues des différents termes Entités nommées et variantes dans un système de question-réponse Le typage de la réponse attendue est évidemment une fonctionnalité indispensable à un système de question-réponse, partic</context>
<context position="26373" citStr="Ittycheriah et al, 2000" startWordPosition="4201" endWordPosition="4204">onne ou un nom d’organisation par exemple. Dans ce cas, le typage de la réponse est simple même si son exploitation nécessite un bon système de reconnaissance des entités nommées et si le nombre de catégories de types peut être augmenté de manière à raffiner le typage. En revanche, lorsque la réponse attendue est constituée d’un nom commun ou d’une phrase, son typage, plus complexe, est rarement réalisé. Certains systèmes, comme le système FALCON (Harabagiu et al, 2000), utilisent les hiérarchies des classes de mots dans WordNet pour typer les réponses. Pour sa part, le système développé par (Ittycheriah et al, 2000) se fonde sur un modèle de l’entropie maximum pour la classification des types de réponse. Sur les 682 questions de TREC9, 57,5% ont été analysées par QALC comme étant des questions à entité nommée, les autres n’ont pas été typées. Parmi les réponses correctes de notre meilleur test, 62,7% répondent à des questions à entité nommée. Par ailleurs, le test que nous avons fait pour des réponses plus courtes donne 84% de réponses possédant une entité nommée parmi les bonnes réponses. Typer la réponse permet donc de mieux cibler la portion de phrase qui peut la contenir. Tous les systèmes ayant part</context>
</contexts>
<marker>Ittycheriah, Franz, Zhu, Ratnaparkhi, 2000</marker>
<rawString>Ittycheriah A., Franz M., Zhu W-J., Ratnaparkhi A., (2000), IBM&apos;s statistical Question Answering System, , Actes préliminaires de TREC9, Gaithersburg, MD, NIST Eds, 60-65.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Jacquemin</author>
</authors>
<title>Syntagmatic and paradigmatic representations of term variation,</title>
<date>1999</date>
<booktitle>Actes de ACL&apos;99,</booktitle>
<pages>341--348</pages>
<contexts>
<context position="5757" citStr="Jacquemin, 1999" startWordPosition="855" endWordPosition="856">ment Question/Phrase Séquence ordonnée de 250 et 50 caractères Figure 1. Architecture du système QALC L&apos;analyse des questions est réalisée par un analyseur partiel dédié qui attribue aux questions des catégories correspondant aux types d&apos;entités nommées pouvant répondre à la question. La sélection d’un sous-ensemble de documents pertinents repose sur la reconnaissance des termes de la question ou de leurs variantes dans les documents sélectionnés par un moteur de Entités nommées et variantes dans un système de question-réponse recherche classique. Cette reconnaissance est effectuée par FASTR (Jacquemin, 1999) sur la base des termes extraits de la question. Cette sélection revêt toute son importance lorsque le système applique les processus ultérieurs, à savoir la reconnaissance des entités nommées telles que les personnes, organisations, lieux et valeurs numériques, et la comparaison entre phrase et question, processus fortement consommateurs de temps de traitement. Le dernier module, qui propose un ensemble limité de réponses à chaque question, met en œuvre un calcul de similarité entre une question, représentée par un vecteur contenant ses mots pleins lemmatisés, ses termes et le type attendu de</context>
<context position="15788" citStr="Jacquemin 1999" startWordPosition="2419" endWordPosition="2420">Illouz, C. Jacquemin variante reconnue et un identificateur de variation v (une métarègle). Par exemple, l&apos;index suivant : LA092690-0038 t(131,1) making many automobiles NtoVSemArg signifie que l&apos;occurrence making many automobiles du document n ° LA092690-0038 est reconnue comme une variante du terme 1 (car maker) de la question q=131 au moyen de la variation NtoVSemArg donnée en section 4.2. Chaque document sélectionné pour une question reçoit un poids. La fonction de pondération Wq(d) (voir la formule (1)) repose sur une mesure de qualité des différentes familles de variations décrite dans (Jacquemin 1999) : le poids (w(v)) des occurrences de termes sans variation est 3, celui des variantes morphologiques et morpho-syntaxiques est 2 et celui des variantes sémantiques et morpho-sémantico-syntaxiques est 1. Les noms propres représentent des indices importants. Chaque terme t(q, i) reçoit un poids P(t(q, i)) entre 0 et 1 correspondant à sa proportion de noms propres. Par exemple, President Cleveland&apos;s wife (la femme du président Cleveland) a un poids de 2/3=0,67 selon ce critère. Enfin, le dernier facteur de fiabilité est le nombre de mots du terme, représenté par la quantité |t(q, i) |dans la for</context>
</contexts>
<marker>Jacquemin, 1999</marker>
<rawString>Jacquemin C., (1999), Syntagmatic and paradigmatic representations of term variation, Actes de ACL&apos;99, 341-348.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Jacquemin</author>
<author>C Bush</author>
</authors>
<title>Fouille du Web pour la collecte d&apos;entités nommées, Actes de TALN</title>
<date>2000</date>
<pages>187--196</pages>
<location>Lausanne</location>
<marker>Jacquemin, Bush, 2000</marker>
<rawString>Jacquemin C., Bush C., Fouille du Web pour la collecte d&apos;entités nommées, Actes de TALN 2000, Lausanne (2000), 187-196.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Justeson</author>
<author>S Katz</author>
</authors>
<title>Technical terminology: some linguistic properties and an algorithm for identification in texte,</title>
<date>1995</date>
<journal>Natural Language Engineering ,</journal>
<volume>1</volume>
<pages>9--27</pages>
<marker>Justeson, Katz, 1995</marker>
<rawString>Justeson J., Katz S., (1995), Technical terminology: some linguistic properties and an algorithm for identification in texte, Natural Language Engineering , Vol 1, pp. 9-27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K L Kwok</author>
<author>L Grunfeld</author>
<author>N Dinstl</author>
<author>M Chan</author>
</authors>
<title>TREC9 Cross Language, Web and Question-Answering Track experiments using PIRCS, Actes préliminaires de TREC9,</title>
<date>2000</date>
<pages>26--35</pages>
<publisher>Eds.,</publisher>
<location>Gaithersburg, MD, NIST</location>
<contexts>
<context position="24849" citStr="Kwok et al, 2000" startWordPosition="3970" endWordPosition="3973">sultats et discussion Dans le cadre de TREC9, les résultats de QALC ont été évalués dans trois conditions différentes, les variations de l’une à l’autre concernant le moteur de recherche utilisé et la taille de la réponse (250 ou 50 caractères). Le meilleur de ces tests a obtenu un score de 0,407 avec 375 réponses trouvées sur 682, pour des réponses sur 250 caractères. Le calcul de ce score tient compte du rang de classement (de 1 à 5) de la réponse trouvée. Ce score nous a placé en 6ième position sur 28 participants. Le premier (Harabagiu et al, 2000) a obtenu un score de 0,760, le deuxième (Kwok et al, 2000) un score de 0,464, et les trois suivants respectivement des scores de 0,460, 0,457 (Ittycheriah et al, 2000) et 0,425. La conférence d’évaluation TREC offre une référence intéressante pour mesurer l’efficacité des méthodes utilisées par les différents systèmes de question-réponse. L’architecture de base généralement adoptée par les systèmes participants est conforme à celle de notre système QALC, avec éventuellement quelques variantes. 5 Nous ne retenons ici que les expressions les plus longues des différents termes Entités nommées et variantes dans un système de question-réponse Le typage de</context>
<context position="27742" citStr="Kwok et al, 2000" startWordPosition="4422" endWordPosition="4425">s mise à disposition par le NIST. Dans le système QALC, nous conservons dans son intégralité chaque document retrouvé par le moteur. Mais d’autres sytèmes sélectionnent le ou les paragraphes pertinents de chaque document retrouvé. Comme dans QALC, la recherche des meilleures réponses est fondée sur un appariement entre question et réponse reposant sur la comparaison des mots des questions avec ceux des phrases sélectionnées et tenant compte du type attendu de la réponse et des entités nommées. Les critères retenus pour effectuer l’appariement peuvent varier d’un système à l’autre. Kwok et al (Kwok et al, 2000) par exemple utilisent entre autres un dictionnaire de synonymes qu’ils ont extrait manuellement de WordNet. Le système FALCON, quant à lui, (Harabagiu et al, 2000) utilise une approche sémantique pour réaliser cet appariement : une unification est recherchée entre la représentation sémantique de la question et les représentations sémantiques des paragraphes sélectionnés. C’est également le seul système à effectuer une justification de ses réponses. 8 Conclusion Un système de question-réponse doit trouver la réponse à une question précise, dans un temps suffisamment court pour être compatible </context>
</contexts>
<marker>Kwok, Grunfeld, Dinstl, Chan, 2000</marker>
<rawString>Kwok K.L., Grunfeld L., Dinstl N., Chan M., (2000), TREC9 Cross Language, Web and Question-Answering Track experiments using PIRCS, Actes préliminaires de TREC9, Gaithersburg, MD, NIST Eds., 26-35.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Lehnert</author>
</authors>
<title>Human and computational question answering,</title>
<date>1977</date>
<journal>Cognitive Science ,</journal>
<volume>1</volume>
<pages>47--63</pages>
<contexts>
<context position="2688" citStr="Lehnert (1977)" startWordPosition="394" endWordPosition="395">entes, soit en proposant la réponse, s’il s&apos;agit d&apos;une question d’ordre factuel, ou un résumé si la requête est d&apos;ordre thématique. Dans un système de question-réponse, la recherche de documents pertinents est complétée par la sélection de courts extraits de texte contenant la réponse à la question posée. Cette sélection est opérée par un ensemble de modules de TAL, à la fois de nature syntaxique et sémantique, devant posséder une grande couverture linguistique et s’appliquer indépendamment du domaine abordé. La problématique &amp;quot;questionréponse&amp;quot; a été introduite dès la fin des années 70 lorsque Lehnert (1977) a jeté les bases d’un O. Ferret, B. Grau, M. Hurault-Plantet, G. Illouz, C. Jacquemin système de question-réponse avec le système QUALM. Plus récemment, Molla et al (2000) ont proposé EXTRANS, un système de question-réponse sur le manuel Unix. Même dans ce domaine limité, les auteurs associent une approche fondée sur une analyse syntaxique et sémantique des questions et du manuel à une approche fondée sur des mots-clés de manière à rendre leur système plus robuste. Dans cet article, nous présentons notre système de question-réponse QALC, conçu pour traiter des questions factuelles ou encyclop</context>
</contexts>
<marker>Lehnert, 1977</marker>
<rawString>Lehnert W., (1977), Human and computational question answering, Cognitive Science , 1, p. 47-63.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C de Loupy</author>
<author>P Bellot</author>
<author>M El-Bèze</author>
<author>P-F Marteau</author>
</authors>
<date>1998</date>
<booktitle>Query Expansion and Classification of Retrieved Documents, TREC7 ,</booktitle>
<pages>382--389</pages>
<marker>de Loupy, Bellot, El-Bèze, Marteau, 1998</marker>
<rawString>de Loupy C., Bellot P., El-Bèze M., Marteau P.-F. (1998). Query Expansion and Classification of Retrieved Documents, TREC7 , p.382-389.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A D Mollà</author>
</authors>
<title>EXTRANS, An Answer Extraction System, Traitement automatique des langues</title>
<date>2000</date>
<journal></journal>
<volume>41</volume>
<pages>495--522</pages>
<marker>Mollà, 2000</marker>
<rawString>Mollà A. D. et al., (2000), EXTRANS, An Answer Extraction System, Traitement automatique des langues , 41, p. 495-522.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Schmid</author>
</authors>
<title>Improvements in Part-of-Speech Tagging with an Application To German, Natural Language Processing Using Very Large Corpora,</title>
<date>1999</date>
<publisher>Kluwer Academic Publisher.</publisher>
<location>Dordrecht,</location>
<contexts>
<context position="10660" citStr="Schmid 1999" startWordPosition="1607" endWordPosition="1608">s La reconnaissance et le marquage dans les documents des termes caractérisant la question et de leurs variantes servent dans un premier temps à réaliser une post-sélection des documents après celle effectuée par le moteur de recherche. Elle sert dans un second temps à donner des indices supplémentaires au module d’appariement question/réponse. 4.1 Extraction des termes L’extraction automatique des termes à partir des questions utilise une technique simple de filtrage par des patrons de catégories syntaxiques. Les questions sont d’abord segmentées, étiquetées et lemmatisées par le TreeTagger (Schmid 1999). Des patrons de catégories syntaxiques sont ensuite utilisés pour extraire des termes des questions. Ces patrons ne diffèrent de ceux définis par Justeson et Katz (Justeson, Katz 1995) que par le fait que nous ne prenons pas en compte les syntagmes prépositionnels postposés. Les patrons utilisés sont synthétisés par l’expression régulière suivante1 : (((((JJ |NN |NP |VBG)) ? (JJ |NN |NP |VBG) (NP |NN))) |(VBD) |(NN) |(NP) |(CD)) La chaîne la plus longue est acquise en premier et les sous-chaînes ne peuvent être extraites que si elles ne commencent pas par le même mot que la surchaîne. Par exe</context>
<context position="20291" citStr="Schmid 1999" startWordPosition="3198" endWordPosition="3199">r enfin les Na phrases les plus similaires (Na est égal à 5 pour la tâche Question Answering de TREC). Pour effectuer cette comparaison, QALC construit une même représentation pour la question et pour la phrase candidate à la réponse. Cette représentation consiste en un vecteur contenant trois types d’éléments : des mots pleins, des termes, et des entités nommées. Chacun de ces éléments est pondéré en fonction de son importance par rapport aux autres. Les mots pleins sont essentiellement les adjectifs, les verbes et noms, sous leur forme lemmatisée, telle qu’elle est donnée par le TreeTagger (Schmid 1999). Les mots de la phrase qui ne sont pas présents dans la question reçoivent un poids nul. Les autres se voient attribuer un poids, de type tf.idf, en rapport avec leur fréquence dans un corpus de référence. Les termes proviennent de l’extracteur de termes décrit au paragraphe 4.1. Les termes de la question sont affectés d’un poids fixe. Pour la phrase, les termes considérés sont les variantes des termes de la question reconnues par FASTR dont le poids est celui qui permet de pondérer les documents et qui exprime la distance entre la variante et le terme correspondant (voir paragraphe 5). Les e</context>
</contexts>
<marker>Schmid, 1999</marker>
<rawString>Schmid H., (1999), Improvements in Part-of-Speech Tagging with an Application To German, Natural Language Processing Using Very Large Corpora, Dordrecht, S. Armstrong, K. W. Chuch, P. Isabelle, E. Tzoukermann, D. Yarowski, Eds., Kluwer Academic Publisher.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>