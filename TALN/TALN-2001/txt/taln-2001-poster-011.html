<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
"http://www.w3.org/TR/html4/loose.dtd">
<html><head><title>Algorithme de d&#233;codage de treillis selon le crit&#232;re du co&#251;t moyen pour la reconnaissance de la parole</title>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
</head>
<body>
<div style="page-break-before:always; page-break-after:always"><div><p>TALN 2001, Tours, 2-5 juillet 2001
</p>
<p>Algorithme de de&#769;codage de treillis selon le crite&#768;re du cou&#770;t
moyen pour la reconnaissance de la parole
</p>
<p>Antoine Rozenknop (1) et Marius Silaghi (2)
EPFL (DI-LIA) CH-1015 Lausanne (Suisse)
</p>
<p>(1) Antoine.Rozenknop@epfl.ch, (2) Marius.Silaghi@epfl.ch
</p>
<p>Re&#769;sume&#769; - Abstract
</p>
<p>Les mode&#768;les de langage stochastiques utilise&#769;s pour la reconnaissance de la parole continue,
ainsi que dans certains syste&#768;mes de traitement automatique de la langue, favorisent pour la
plupart l&#8217;interpre&#769;tation d&#8217;un signal par les phrases les plus courtes possibles, celles-ci e&#769;tant par
construction bien souvent affecte&#769;es des cou&#770;ts les plus bas. Cet article expose un algorithme
permettant de re&#769;pondre a&#768; ce proble&#768;me en remplac&#807;ant le cou&#770;t habituel affecte&#769; par le mode&#768;le de
langage par sa moyenne sur la longueur de la phrase conside&#769;re&#769;e. Cet algorithme est tre&#768;s ge&#769;ne&#769;ral
et peut e&#770;tre adapte&#769; aise&#769;ment a&#768; de nombreux mode&#768;les de langage, y compris sur des ta&#770;ches
d&#8217;analyse syntaxique.
</p>
<p>Stochastic language models used for continous speech recognition, and also in some Automated
Language Processing systems, often favor the shortest interpretation of a signal, which are
affected with the lowest costs by construction. To cope with this problem, this article presents
an algorithm that allows the computation of the sequence with the lowest mean cost, in a very
systematical way. This algorithm can easily be adapted to several kinds of language models,
and to other tasks, such as syntactic analysis.
</p>
<p>Mots-clefs/Keywords : Continuous speech recognition, Stochastic language models, Mean score.
</p>
<p>1 Introduction
</p>
<p>La reconnaissance de la parole continue a&#768; partir d&#8217;un signal acoustique est un proble&#768;me d&#8217;une
grande complexite&#769; du fait de la taille de l&#8217;espace de recherche des solutions. Afin de la rendre
envisageable, il faut impe&#769;rativement restreindre cet espace, par exemple en utilisant des mode&#768;les
de langage. Cependant, me&#770;me ainsi, le nombre de solutions correspondant a&#768; une re&#769;alisation
acoustique proche du signal observe&#769; reste toujours tre&#768;s grand; une probabilisation a priori
de l&#8217;espace de recherche est donc ne&#769;cessaire (Murveit, Moore, 1990). Des exemples de tels
mode&#768;les de langage probabilistes sont accessibles et bien e&#769;tudie&#769;s, parfois dans un cadre de
Traitement Automatique du Langage, mais pre&#769;sentent certains de&#769;fauts s&#8217;ils sont utilise&#769;s sans
adaptation pre&#769;alable au proble&#768;me de la reconnaissance de la parole. En particulier, les plus
utilise&#769;s d&#8217;entre eux, qui reposent sur une mode&#769;lisation parame&#769;trique de processus stochastiques</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>A. Rozenknop, M. Silaghi
</p>
<p>(N-grams, grammaires stochastiques) affectent par construction des probabilite&#769;s moindres aux
phrases les plus longues, ce qui peut induire un fort biais lorsque le nombre de mots prononce&#769;s
n&#8217;est pas connu a&#768; l&#8217;avance.
</p>
<p>Une pre&#769;sentation succinte de l&#8217;utilisation de mode&#768;les de langages value&#769;s en reconnaissance de
la parole est donne&#769;e dans la section 2. Nous y exposons aussi une ide&#769;e tout-a&#768;-fait classique :
elle consiste a&#768; utiliser en reconnaissance non pas le cou&#770;t d&#8217;une phrase telle qu&#8217;un mode&#768;le de
langage stochastique le de&#769;finit (c&#8217;est-a&#768;-dire comme l&#8217;oppose&#769; du logarithme de sa probabilite&#769;),
mais la moyenne de ce cou&#770;t sur le nombre de mots de la phrase. Nous pre&#769;sentons ensuite dans la
section 3 un algorithme original de de&#769;codage ite&#769;ratif, qui permet de de&#769;terminer les solutions de
plus faible cou&#770;t moyen, en s&#8217;appuyant sur les algorithmes existants de recherche de la solution
de plus faible cou&#770;t global. L&#8217;expose&#769; de l&#8217;algorithme est suivi de sa preuve, ainsi que d&#8217;un petit
exemple de de&#769;roulement, ou&#768; l&#8217;on utilise une grammaire stochastique pour de&#769;coder un treillis de
mots.
</p>
<p>La mention de certains mode&#768;les de langage dans cet article a pour but de faire sentir l&#8217;inte&#769;re&#770;t
qu&#8217;il y a a&#768; pouvoir extraire une phrase de cou&#770;t moyen minimum. Il ne faut cependant en aucun
cas y voir une limitation pour l&#8217;algorithme pre&#769;sente&#769;, dont la grande force est justement son
caracte&#768;re tre&#768;s ge&#769;ne&#769;ral.
</p>
<p>2 Mode&#768;les de langage value&#769;s pour la reconnaissance de la
parole
</p>
<p>Nombre de syste&#768;mes de reconnaissance de la parole s&#8217;appuient sur des mode&#768;les de langage
probabilistes pour se&#769;lectionner une se&#769;quence de mots parmi les diffe&#769;rentes interpre&#769;tations possib-
</p>
<p>les d&#8217;un signal. Chaque interpre&#769;tation &#0; rec&#807;oit alors un &#8221;cou&#770;t&#8221; acoustique &#1;&#3;&#2;&#5;&#4; &#0;&#7;&#6; calcule&#769;
par un module acoustique, ainsi qu&#8217;un &#8221;cou&#770;t&#8221; linguistique &#1;&#9;&#8; &#4; &#0;&#7;&#6; calcule&#769; a&#768; l&#8217;aide du mode&#768;le
de langage probabiliste conside&#769;re&#769;; l&#8217;interpre&#769;tation se&#769;lectionne&#769;e est celle ayant le cou&#770;t total
&#1;&#10;&#4;
</p>
<p>&#0;&#7;&#6;&#12;&#11;
</p>
<p>&#1;&#13;&#2;&#5;&#4;
</p>
<p>&#0;&#7;&#6;&#15;&#14;
</p>
<p>&#1;
</p>
<p>&#8;
</p>
<p>&#4;
</p>
<p>&#0;&#7;&#6;
</p>
<p>minimal.
</p>
<p>Or, les mode&#768;les de langage les plus utilise&#769;s sont des mode&#768;les ge&#769;ne&#769;ratifs stochastiques, qui
produisent des se&#769;quences de mots par une succession d&#8217;e&#769;tapes ale&#769;atoires. Les mode&#768;les de N-
grams et les grammaires stochastiques hors-context (SCFG) en sont de bons exemples. L&#8217;inte&#769;re&#770;t
de tels mode&#768;les est double : d&#8217;une part, leurs parame&#768;tres sont facilement obtensibles a&#768; partir de
bases d&#8217;exemples; d&#8217;autre part, l&#8217;existence d&#8217;algorithmes efficaces autorisent leur exploitation
effective pour le de&#769;codage de signaux de parole (Mteer, Jelinek, 1993). Mais ils pre&#769;sentent
aussi un inconve&#769;nient majeur : le cou&#770;t d&#8217;une se&#769;quence, calcule&#769; comme l&#8217;oppose&#769; du logarithme
de sa probabilite&#769; d&#8217;e&#770;tre produite par le mode&#768;le, cro&#305;&#770;t rapidement avec le nombre d&#8217;e&#769;tapes du
processus de production, donc avec le nombre de mots qui la composent. Ainsi ces mode&#768;les
conside&#768;rent-ils que les hypothe&#768;ses les plus courtes sont toujours les meilleures !
Pour pallier ce proble&#768;me, on cherche en ge&#769;ne&#769;ral a&#768; minimiser &#1;&#10;&#4; &#0;&#7;&#6;&#12;&#16;&#18;&#17;&#13;&#19; &#0;&#20;&#19; pluto&#770;t que &#1;&#10;&#4; &#0;&#7;&#6;
</p>
<p>( &#19; &#0;&#20;&#19; est le nombre de mots de &#0; , et &#17; est une constante empiriquement de&#769;termine&#769;e).
Une autre ide&#769;e naturelle est de chercher a&#768; minimiser le cou&#770;t moyen par mot &#21;&#1;&#10;&#4; &#0;&#7;&#6;&#22;&#11; &#1;&#23;&#4; &#0;&#7;&#6; &#24;&#25;&#19; &#0;&#20;&#19; .
C&#8217;est ce que permet de re&#769;aliser l&#8217;algorithme pre&#769;sente&#769; dans la suite. Comme il utilise ite&#769;rativement
l&#8217;algorithme qui calcule &#26;fiff fl&#31;ffi&#10; !#&quot;&#18;&#1;&#23;&#4;
</p>
<p>&#0;&#7;&#6;$&#16;%&#17;&#22;&#19; &#0;&#20;&#19;
</p>
<p>, il ne requiert aucun espace me&#769;moire supple&#769;-
mentaire; le nombre d&#8217;ite&#769;rations ne&#769;cessaires a&#768; l&#8217;obtention du re&#769;sultat est le seul surcou&#770;t algo-</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Algorithme de de&#769;codage de treillis selon le crite&#768;re du cou&#770;t moyen
</p>
<p>rithmique, et peut e&#770;tre majore&#769; par &#0;&#2;&#1;&#4;&#3;&#6;&#5; &#7;&#8;&#5; &#9;&#10;&#5; &#7;&#10;&#11;&#4;&#5; .
</p>
<p>3 Algorithme de de&#769;codage ite&#769;ratif
</p>
<p>3.1 Ingre&#769;dients
</p>
<p>On dispose des e&#769;le&#769;ments suivants :
</p>
<p>&#8211; un ensemble &#12; de phrases appartenant a&#768; un langage &#13; , chaque phrase &#7; e&#769;tant constitue&#769;e
de &#5; &#7;&#8;&#5; mots,
</p>
<p>&#8211; une fonction de cou&#770;t &#14; qui a&#768; chaque e&#769;le&#769;ment &#7; de &#13; associe un re&#769;el &#14;&#16;&#15; &#7;&#10;&#17; ,
</p>
<p>&#8211; un algorithme &#18;&#19;&#15; &#12;&#2;&#20; &#14;&#6;&#20; &#21; &#17; qui permet d&#8217;extraire :
&#22;&#24;&#23; &#25;
</p>
<p>&#0;&#2;&#26; ffflfi&#31;ffi  
</p>
<p>&#14;&#16;&#15;
</p>
<p>&#7;&#10;&#17;!&#9;
</p>
<p>&#21;
</p>
<p>&#5; &#7;&#8;&#5; pour n&#8217;importe quel re&#769;el &#21; .
</p>
<p>3.2 Re&#769;alisation
</p>
<p>L&#8217;algorithme &quot;#&#15; &#18;&#16;&#20; &#12;&#2;&#20; &#14; &#17; suivant permet de trouver une solution &#7;&#10;&#11; de
&#22;&#24;&#23; &#25;
</p>
<p>&#0;&#2;&#26; ffflfi&#6;ffi  %$
</p>
<p>&#14;&#2;&#15;
</p>
<p>&#7;&#10;&#17;
</p>
<p>, ou&#768;
$
</p>
<p>&#14;&#10;&amp;%&#14;&#16;&#15;
</p>
<p>&#7;&#10;&#17; '(&#5; &#7;&#8;&#5; :
</p>
<p>1. Initialisation : on pose $&#14;&#16;&#15; &#7;*)(+ &#17; &amp;%, . 1 2
</p>
<p>2. Ite&#769;rations : on calcule &#7;.- &amp;
&#22;&#24;&#23; &#25;
</p>
<p>&#0;&#2;&#26; ff
</p>
<p>fi&#31;ffi  
</p>
<p>&#14;&#16;&#15;
</p>
<p>&#7;&#10;&#17;&#4;&#9;&#8;$
</p>
<p>&#14;&#16;&#15;
</p>
<p>&#7;.-
</p>
<p>)/+
</p>
<p>&#17;10 &#5; &#7;&#8;&#5; en utilisant l&#8217;algorithme
&#18;&#19;&#15; &#12;&#2;&#20; &#14;&#6;&#20;
</p>
<p>$
</p>
<p>&#14;&#2;&#15;
</p>
<p>&#7;.-
</p>
<p>)(+
</p>
<p>&#17; &#17; 3
.
</p>
<p>3. Crite&#768;re d&#8217;arre&#770;t : on cesse les ite&#769;rations lorsque &#5; &#7;*- &#5; &amp; &#5; &#7;.- )(+ &#5; . &#7;.- est alors une solution
du proble&#768;me.
</p>
<p>3.3 Preuve
</p>
<p>The&#769;ore&#768;me 1 L&#8217;algorithme &quot;#&#15; &#18;&#16;&#20; &#12;&#2;&#20; &#14; &#17; converge vers une solution &#7;2&#11; &amp;
&#22;&#24;&#23; &#25;
</p>
<p>&#0;&#2;&#26; ffflfi&#6;ffi  %$
</p>
<p>&#14;&#2;&#15;
</p>
<p>&#7;&#10;&#17;
</p>
<p>en un nombre d&#8217;ite&#769;rations infe&#769;rieur a&#768; &#5; &#7;*+ &#5; &#9;%&#5; &#7;%&#11;&#4;&#5; .
</p>
<p>Lemme 1 Le cou&#770;t moyen $&#14;&#16;&#15; &#7; - &#17; de&#769;cro&#305;&#770;t strictement avec 3 , pour 3 supe&#769;rieur a&#768; 0 et tant que
$
</p>
<p>&#14;4&#15;
</p>
<p>&#7;%&#11; &#17; n&#8217;a pas e&#769;te&#769; atteint :
</p>
<p>5
</p>
<p>376%,28
</p>
<p>$
</p>
<p>&#14;&#2;&#15;
</p>
<p>&#7;.- &#17;79
</p>
<p>$
</p>
<p>&#14;&#16;&#15;
</p>
<p>&#7;
</p>
<p>&#11;
</p>
<p>&#17;7:
</p>
<p>$
</p>
<p>&#14;&#16;&#15;
</p>
<p>&#7;.- &#17;;9
</p>
<p>$
</p>
<p>&#14;&#16;&#15;
</p>
<p>&#7;.- &lt;
</p>
<p>+
</p>
<p>&#17; =
</p>
<p>1. &gt;&#2;?fl@ n&#8217;existant pas, ceci n&#8217;est qu&#8217;une convention d&#8217;e&#769;criture.
2. La valeur initiale de A
</p>
<p>BDC
</p>
<p>&gt;&#2;?fl@ E est sans importance pour la correction de l&#8217;algorithme. Le choix du F est
arbitraire, et de fait, si l&#8217;on a une estimation a priori de la valeur de l&#8217;optimum A
</p>
<p>B;C
</p>
<p>&gt;HG E , le choix de cette estimation
pour A
</p>
<p>BDC
</p>
<p>&gt;&#16;?fl@ E acce&#769;le&#769;rera la convergence de l&#8217;algorithme par rapport au choix de la valeur F .
3. I est l&#8217;indice de l&#8217;ite&#769;ration en cours, et vaut F pour la premie&#768;re ite&#769;ration.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>A. Rozenknop, M. Silaghi
</p>
<p>De&#769;monstration du lemme 1 :
Notons
</p>
<p>&#0;&#2;&#1;
</p>
<p>&#3; &#4; &#5;&#7;&#6;&#9;&#8;
</p>
<p>&#0;
</p>
<p>&#4; &#5;&#7;&#6;&#11;&#10;&#13;&#12;
</p>
<p>&#0;
</p>
<p>&#4; &#5;
</p>
<p>&#3;
</p>
<p>&#6; &#14; &#5;&#15;&#14;
</p>
<p>.
</p>
<p>Par de&#769;finition de &#12;
&#0;
</p>
<p>, on remarque imme&#769;diatement que :
&#0;&#16;&#1;
</p>
<p>&#3;
</p>
<p>&#4; &#5;&#7;&#6;&#9;&#8;&#15;&#14; &#5;&#15;&#14; &#4;
</p>
<p>&#12;
</p>
<p>&#0;
</p>
<p>&#4; &#5;&#7;&#6;&#11;&#10;
</p>
<p>&#12;
</p>
<p>&#0;
</p>
<p>&#4; &#5;
</p>
<p>&#3;
</p>
<p>&#6; &#6;
</p>
<p>.
</p>
<p>L&#8217;algorithme &#17; &#4; &#18;&#20;&#19;
&#0;
</p>
<p>&#19;
</p>
<p>&#12;
</p>
<p>&#0;
</p>
<p>&#4; &#5;
</p>
<p>&#3;
</p>
<p>&#6; &#6; trouve une solution &#5; &#3; &#21;&#23;&#22; qui minimise
&#0;&#2;&#1;
</p>
<p>&#3;
</p>
<p>&#4; &#5;&#7;&#6; , d&#8217;ou&#768; :
</p>
<p>&#5;
</p>
<p>&#3; &#21;&#23;&#22;
</p>
<p>&#8;
</p>
<p>&#17;
</p>
<p>&#4; &#18;&#20;&#19;
</p>
<p>&#0;
</p>
<p>&#19;&#9;&#12;
</p>
<p>&#0;
</p>
<p>&#4; &#5;
</p>
<p>&#3;
</p>
<p>&#6; &#6;&#13;&#24;&#25;&#5;
</p>
<p>&#3; &#21;&#26;&#22;
</p>
<p>&#8;fiffffifl &#31;! &#20;&quot; #
</p>
<p>$&#16;% &amp;
</p>
<p>&#0;
</p>
<p>&#1;
</p>
<p>&#3; &#4; &#5;&#7;&#6;
</p>
<p>&#24;
</p>
<p>&#0;
</p>
<p>&#1;
</p>
<p>&#3; &#4; &#5;
</p>
<p>&#3; &#21;&#23;&#22;
</p>
<p>&#6;('
</p>
<p>&#0;
</p>
<p>&#1;
</p>
<p>&#3; &#4; &#5;fi) &#6;
</p>
<p>&#24;*&#14; &#5;
</p>
<p>&#3; &#21;&#26;&#22;
</p>
<p>&#14; &#4;
</p>
<p>&#12;
</p>
<p>&#0;
</p>
<p>&#4; &#5;
</p>
<p>&#3; &#21;&#23;&#22;
</p>
<p>&#6;&#11;&#10;
</p>
<p>&#12;
</p>
<p>&#0;
</p>
<p>&#4; &#5;
</p>
<p>&#3;
</p>
<p>&#6; &#6;('&#15;&#14; &#5;
</p>
<p>)
</p>
<p>&#14; &#4;
</p>
<p>&#12;
</p>
<p>&#0;
</p>
<p>&#4; &#5;
</p>
<p>)
</p>
<p>&#6;&#11;&#10;
</p>
<p>&#12;
</p>
<p>&#0;
</p>
<p>&#4; &#5;
</p>
<p>&#3;
</p>
<p>&#6; &#6;
</p>
<p>&#24;+&#12;
</p>
<p>&#0;
</p>
<p>&#4; &#5;
</p>
<p>&#3; &#21;&#23;&#22;
</p>
<p>&#6;(',&#12;
</p>
<p>&#0;
</p>
<p>&#4; &#5;
</p>
<p>&#3;
</p>
<p>&#6;.-
</p>
<p>&#14; &#5;
</p>
<p>)
</p>
<p>&#14;
</p>
<p>&#14; &#5;
</p>
<p>&#3; &#21;&#26;&#22;
</p>
<p>&#14;
</p>
<p>&#4;&#23;&#12;
</p>
<p>&#0;
</p>
<p>&#4; &#5;fi) &#6;&#11;&#10;&#13;&#12;
</p>
<p>&#0;
</p>
<p>&#4; &#5;
</p>
<p>&#3;
</p>
<p>&#6; &#6;
</p>
<p>&#24;+&#12;
</p>
<p>&#0;
</p>
<p>&#4; &#5;
</p>
<p>&#3; &#21;&#23;&#22;
</p>
<p>&#6;(/0&#12;
</p>
<p>&#0;
</p>
<p>&#4; &#5;
</p>
<p>&#3;
</p>
<p>&#6;
</p>
<p>La dernie&#768;re ligne de la de&#769;monstration vient de l&#8217;hypothe&#768;se &#12;
&#0;
</p>
<p>&#4; &#5;
</p>
<p>&#3;
</p>
<p>&#6;213&#12;
</p>
<p>&#0;
</p>
<p>&#4; &#5;
</p>
<p>)
</p>
<p>&#6; , et de la stricte
positivite&#769; de &#14; &#5;&#15;&#14; pour tout &#5; appartement a&#768; &#18; .
</p>
<p>Lemme 2 La taille &#14; &#5; &#3; &#14; des solutions successives de&#769;cro&#305;&#770;t strictement pour 4 supe&#769;rieur a&#768; 5 et
tant que &#12;
</p>
<p>&#0;
</p>
<p>&#4; &#5;
</p>
<p>)
</p>
<p>&#6; n&#8217;a pas e&#769;te&#769; atteint :
</p>
<p>6
</p>
<p>4&#9;7&#7;5&#7;8
</p>
<p>&#12;
</p>
<p>&#0;
</p>
<p>&#4; &#5;
</p>
<p>&#3;
</p>
<p>&#6;(1
</p>
<p>&#12;
</p>
<p>&#0;
</p>
<p>&#4; &#5;
</p>
<p>)
</p>
<p>&#6;&#9;&#24;9&#14; &#5;
</p>
<p>&#3; &#21;&#26;&#22;
</p>
<p>&#14;:/;&#14; &#5;
</p>
<p>&#3;
</p>
<p>&#14; &lt;
</p>
<p>De&#769;monstration du lemme 2 :
Pour 4=7&gt;5 ,
</p>
<p>&#5;
</p>
<p>&#3;
</p>
<p>&#8;
</p>
<p>&#17;
</p>
<p>&#4; &#18;&#20;&#19;
</p>
<p>&#0;
</p>
<p>&#19;
</p>
<p>&#12;
</p>
<p>&#0;
</p>
<p>&#4; &#5;
</p>
<p>&#3; ?@&#22;
</p>
<p>&#6; &#6;A&#24;B&#5;
</p>
<p>&#3;
</p>
<p>&#8;fiffffifl &#31;! &#20;&quot; #
</p>
<p>$&#2;% &amp;
</p>
<p>&#0;
</p>
<p>&#1;
</p>
<p>&#3; ?&#26;&#22;
</p>
<p>&#4; &#5;&#7;&#6;
</p>
<p>&#24;
</p>
<p>&#0;
</p>
<p>&#1;
</p>
<p>&#3; ?@&#22; &#4; &#5;
</p>
<p>&#3; &#21;&#26;&#22;
</p>
<p>&#6;
</p>
<p>7
</p>
<p>&#0;
</p>
<p>&#1;
</p>
<p>&#3; ?@&#22; &#4; &#5;
</p>
<p>&#3;
</p>
<p>&#6;
</p>
<p>&#24;C&#14; &#5;
</p>
<p>&#3; &#21;&#26;&#22;
</p>
<p>&#14; &#4;
</p>
<p>&#12;
</p>
<p>&#0;
</p>
<p>&#4; &#5;
</p>
<p>&#3; &#21;&#23;&#22;
</p>
<p>&#6;&#11;&#10;
</p>
<p>&#12;
</p>
<p>&#0;
</p>
<p>&#4; &#5;
</p>
<p>&#3; ?@&#22;
</p>
<p>&#6; &#6;
</p>
<p>7
</p>
<p>&#14; &#5;
</p>
<p>&#3;
</p>
<p>&#14; &#4;
</p>
<p>&#12;
</p>
<p>&#0;
</p>
<p>&#4; &#5;
</p>
<p>&#3;
</p>
<p>&#6;&#11;&#10;
</p>
<p>&#12;
</p>
<p>&#0;
</p>
<p>&#4; &#5;
</p>
<p>&#3; ?@&#22;
</p>
<p>&#6; &#6;
</p>
<p>Or d&#8217;apre&#768;s le lemme 1, &#12;
&#0;
</p>
<p>&#4; &#5;
</p>
<p>&#3;
</p>
<p>&#6;&#20;1D&#12;
</p>
<p>&#0;
</p>
<p>&#4; &#5;
</p>
<p>&#3; &#21;&#23;&#22;
</p>
<p>&#6; , ce qui permet de minorer le second membre de
l&#8217;ine&#769;galite&#769; pre&#769;ce&#769;dente, et d&#8217;obtenir par transitivite&#769; :
</p>
<p>&#14; &#5;
</p>
<p>&#3; &#21;&#26;&#22;
</p>
<p>&#14; &#4;
</p>
<p>&#12;
</p>
<p>&#0;
</p>
<p>&#4; &#5;
</p>
<p>&#3; &#21;&#23;&#22;
</p>
<p>&#6;&#11;&#10;
</p>
<p>&#12;
</p>
<p>&#0;
</p>
<p>&#4; &#5;
</p>
<p>&#3; ?@&#22;
</p>
<p>&#6; &#6;
</p>
<p>7
</p>
<p>&#14; &#5;
</p>
<p>&#3;
</p>
<p>&#14; &#4;
</p>
<p>&#12;
</p>
<p>&#0;
</p>
<p>&#4; &#5;
</p>
<p>&#3; &#21;&#23;&#22;
</p>
<p>&#6;&#11;&#10;
</p>
<p>&#12;
</p>
<p>&#0;
</p>
<p>&#4; &#5;
</p>
<p>&#3; ?@&#22;
</p>
<p>&#6; &#6;
</p>
<p>Toujours d&#8217;apre&#768;s le lemme 1, &#12;&#0; &#4; &#5; &#3; ?@&#22; &#6;ffi1 &#12;&#0; &#4; &#5; &#3; &#6; (car 4 &#10; 527&gt;E ), donc &#12;&#0; &#4; &#5; &#3; ?&#26;&#22; &#6;ffi1 &#12;&#0; &#4; &#5; &#3; &#21;&#23;&#22; &#6; .
On peut alors simplifier les deux membres de l&#8217;ine&#769;galite&#769; pre&#769;ce&#769;dente, en la renversant, ce qui
donne finalement :
</p>
<p>&#14; &#5;
</p>
<p>&#3; &#21;&#26;&#22;
</p>
<p>&#14;!/;&#14; &#5;
</p>
<p>&#3;
</p>
<p>&#14;
</p>
<p>De&#769;monstration du the&#769;ore&#768;me 1 :
Le lemme 2 montre que la taille des solutions &#5; &#3; de&#769;cro&#305;&#770;t strictement pour 4F7G5 tant que
&#12;
</p>
<p>&#0;
</p>
<p>&#4; &#5;
</p>
<p>&#3;
</p>
<p>&#6;ffi1H&#12;
</p>
<p>&#0;
</p>
<p>&#4; &#5;
</p>
<p>)
</p>
<p>&#6;
</p>
<p>. Comme cette taille est un entier strictement positif, elle cesse force&#769;ment de
de&#769;cro&#305;&#770;tre pour un certain 4 &#8; 4 I , ce qui implique que &#12;
</p>
<p>&#0;
</p>
<p>&#4; &#5;
</p>
<p>&#3; J
</p>
<p>&#6;K&#8;L&#12;
</p>
<p>&#0;
</p>
<p>&#4; &#5;
</p>
<p>)
</p>
<p>&#6;
</p>
<p>. L&#8217;algorithme atteint
donc la solution du proble&#768;me en un nombre fini d&#8217;ite&#769;rations, et comme &#14; &#5; &#3; &#14; de&#769;cro&#305;&#770;t strictement
de 4 &#8; 5 a&#768; 4 &#8; 4 I &#10; 5 , le nombre d&#8217;ite&#769;rations 4 I ve&#769;rifie : 4 I '&#15;&#14; &#5; &#22; &#14; &#10;&#7;&#14; &#5; &#3; J &#14;!&#8;&#15;&#14; &#5; &#22; &#14; &#10;&#7;&#14; &#5; ) &#14; .</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Algorithme de de&#769;codage de treillis selon le crite&#768;re du cou&#770;t moyen
</p>
<p>3.4 Exemple de de&#769;roulement
</p>
<p>Afin d&#8217;illustrer l&#8217;algorithme ite&#769;ratif, nous pre&#769;sentons dans cette partie un petit exemple de
de&#769;codage d&#8217;un treillis de mots a&#768; l&#8217;aide d&#8217;une grammaire stochastique. Les re&#768;gles de la grammaire
apparaissent dans la figure 1. Le treillis a&#768; de&#769;coder contient trois interpre&#769;tations possibles :
(1) Ce&#769;lime&#768;ne, (2) Ce&#769;line me&#768;ne et (3) C&#8217;est l&#8217;hymen. `A chacune correspond un arbre
syntaxique, repre&#769;sente&#769; dans la figure 2, avec son cou&#770;t associe&#769;. On rappelle que le cou&#770;t d&#8217;un
arbre est la somme des cou&#770;ts des re&#768;gles qui le constituent, et que le cou&#770;t moyen est cette me&#770;me
somme divise&#769;e par le nombre de feuilles de l&#8217;arbre.
</p>
<p>Re&#768;gle
&#0; &#1;&#3;&#2; &#0;&#5;&#4; &#6;&#3;&#2; &#0;&#5;&#4;
</p>
<p>Re&#768;gle
&#0; &#1;&#3;&#2; &#0;&#5;&#4; &#6;&#7;&#2; &#0;&#5;&#4;
</p>
<p>&#0;&#5;&#8; &#8;
</p>
<p>S &#9; NP &#10; &#11; &#12; &#13; &#14; &#15; &#16; &#16;
&#0;&#5;&#17; &#18;
</p>
<p>V &#9; me&#768;ne &#10; &#11; &#19; &#13; &#14; &#12; &#13; &#10;
&#0;&#20;&#17; &#8;
</p>
<p>S &#9; NP V &#10; &#11; &#12; &#13; &#14; &#15; &#16; &#16;
&#0;&#5;&#18; &#18;
</p>
<p>V &#9; est &#10; &#11; &#19; &#13; &#14; &#12; &#13; &#10;
&#0;&#20;&#18; &#8;
</p>
<p>S &#9; P V A N &#10; &#11; &#12; &#13; &#14; &#15; &#16; &#16;
&#0;&#5;&#18; &#21;
</p>
<p>N &#9; hymen &#10; &#11; &#10; &#22; &#10; &#14; &#10; &#16; &#23;
&#0;&#5;&#8; &#17;
</p>
<p>NP &#9; Ce&#769;lime&#768;ne &#10; &#11; &#19; &#13; &#14; &#12; &#13; &#10;
&#0;&#5;&#21;
</p>
<p>N &#9; bateau &#10; &#15; &#11; &#10; &#22; &#13; &#14; &#13; &#12; &#13;
&#0;&#20;&#17; &#17;
</p>
<p>NP &#9; Ce&#769;line &#10; &#11; &#19; &#13; &#14; &#12; &#13; &#10;
&#0;&#20;&#24;
</p>
<p>A &#9; un &#10; &#11; &#19; &#13; &#14; &#12; &#13; &#10;
&#0;&#20;&#18; &#17;
</p>
<p>P &#9; c&#8217; &#10; &#13;
&#0;&#5;&#18; &#24;
</p>
<p>A &#9; l&#8217; &#10; &#11; &#19; &#13; &#14; &#12; &#13; &#10;
</p>
<p>FIG. 1 &#8211; Re&#768;gles de la grammaire avec leurs probabilite&#769;s et leurs cou&#770;ts. Le cou&#770;t d&#8217;une re&#768;gle vaut
&#25;ff&#26; fiffifl&#5;&#31;! &quot;$#
</p>
<p>.
</p>
<p>% &amp;&#3;' &amp;&#7;( &amp;&#7;)
</p>
<p>Interpre&#769;tation Ce&#769;lime&#768;ne Ce&#769;line me&#768;ne C&#8217;est l&#8217;hymen
Re&#768;gles *
</p>
<p>' '
</p>
<p>, *
</p>
<p>' (
</p>
<p>*
</p>
<p>( '
</p>
<p>, *
</p>
<p>( (
</p>
<p>, *
</p>
<p>( )
</p>
<p>*
</p>
<p>) '
</p>
<p>, *
</p>
<p>) (
</p>
<p>, *
</p>
<p>) )
</p>
<p>, *
</p>
<p>) +
</p>
<p>, *
</p>
<p>) ,
</p>
<p>Probabilite&#769; - . / - . - 0 - . - 1 2
Cou&#770;t 243 5 5 1 - 3 2 5 6 0ffi3 0 7 7
Cou&#770;t moyen 243 5 5 1 2ffi3 7 842 2ffi3 7 / 8
</p>
<p>FIG. 2 &#8211; Cou&#770;ts des diffe&#769;rentes interpre&#769;tations possibles.
</p>
<p>Les e&#769;tapes de l&#8217;algorithme sont de&#769;taille&#769;es dans la figure 3, chaque ligne y repre&#769;sentant une
ite&#769;ration, avec : (1) l&#8217;indice de l&#8217;ite&#769;ration, (2) le cou&#770;t moyen de l&#8217;interpre&#769;tation extraite lors de
l&#8217;ite&#769;ration pre&#769;ce&#769;dente, (3) le crite&#768;re a&#768; minimiser, (4,5,6) les valeurs du crite&#768;re pour les diffe&#769;rentes
interpre&#769;tations possibles, (7) l&#8217;interpre&#769;tation qui minimise le crite&#768;re et (8) son cou&#770;t moyen.
L&#8217;algorithme d&#8217;analyse syntaxique utilise&#769; (Chappelier et al., 1999; Chappelier,Rajman,1998)
peut extraire la solution qui minimise le crite&#768;re 9;:&lt;  =&gt;# en utilisant pour les re&#768;gles terminales
&quot;@? ( ACBCDffiE4FffiG FffiFffiG F4HIG HIFffiG HffiHIG HffiJIG HffiKIG JIG KIL ) les cou&#770;ts 9  &quot;M?I#&#20;&#25;ON9  = &lt; P ' # a&#768; la place de 9  &quot;M?I# .
On peut remarquer que, comme pre&#769;vu par les lemmes 1 et 2, le cou&#770;t moyen des solutions
successives diminue a&#768; partir de Q&#5;RTS , et que leur taille diminue a&#768; partir de Q&#20;RUE .
</p>
<p>4 Conclusion
</p>
<p>Dans cet article, nous avons de&#769;crit une classe de mode&#768;les de langage value&#769;s, utilise&#769;s pour
la reconnaissance de la parole et permettant de de&#769;coder un signal en extrayant la phrase de
cou&#770;t minimal. Ces mode&#768;les ayant souvent la proprie&#769;te&#769; de faire de&#769;pendre le cou&#770;t d&#8217;une phrase
de sa longueur, on en a de&#769;rive&#769; un V V meta-algorithme W W , aussi ge&#769;ne&#769;ral que possible, qui extrait</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>A. Rozenknop, M. Silaghi
</p>
<p>Ite&#769;ration
&#0;
</p>
<p>&#1;&#3;&#2; &#4;&#6;&#5; &#7;&#9;&#8; &#10; &#1;&#12;&#11;
</p>
<p>&#5;
</p>
<p>&#2; &#4;&#13;&#10;
</p>
<p>Valeurs de
&#1;&#12;&#11;
</p>
<p>&#5;
</p>
<p>&#2; &#4;&#13;&#10; &#4;&#6;&#5;
</p>
<p>&#0;
</p>
<p>&#1;&#3;&#2; &#4;&#6;&#5; &#10;
</p>
<p>&#14;
</p>
<p>&#8;
</p>
<p>&#14;&#16;&#15; &#14;&#16;&#17;
</p>
<p>&#18;&#20;&#19;&#22;&#21; &#21;
</p>
<p>&#1;&#3;&#2; &#4;&#13;&#10;
</p>
<p>&#21;&#9;&#23; &#24;&#25;&#24;&#25;&#26; fffi&#23; &#21;&#25;&#26; flffi&#23; fl &#31;
</p>
<p>&#14;
</p>
<p>&#8;
</p>
<p>&#21;ffi&#23; &#24;fi&#24; &#26;
</p>
<p>&#18;&#20;&#19; ff &#21;ffi&#23; &#24;fi&#24; &#26;
</p>
<p>&#1;&#3;&#2; &#4;&#13;&#10;&#20;!
</p>
<p>&#21;ffi&#23; &#24;fi&#24; &#26;#&quot;
</p>
<p>&#4;
</p>
<p>&quot; &#21;
</p>
<p>!
</p>
<p>&#21;ffi&#23; $ffi&#24;&#25;&#24;
</p>
<p>!
</p>
<p>&#21;ffi&#23; &#26;ffi%&#25;fl
</p>
<p>&#14;
</p>
<p>&#17;
</p>
<p>&#21;ffi&#23; %&#25;&#31;fi%
</p>
<p>&#18;&#20;&#19;&#13;fl &#21;ffi&#23; %&#25;&#31;fi%
</p>
<p>&#1;&#3;&#2; &#4;&#13;&#10;&#20;!
</p>
<p>&#21;ffi&#23; %&#25;&#31;fi%&amp;&quot;
</p>
<p>&#4;
</p>
<p>&quot; &#21;&#9;&#23; flfiff '
</p>
<p>!
</p>
<p>&#21;ffi&#23; &#21;ffi%fiff &#21;
</p>
<p>&#14;
</p>
<p>&#15;
</p>
<p>&#21;ffi&#23; %&#25;$&#25;&#21;
</p>
<p>&#18;&#20;&#19;&#22;' &#21;ffi&#23; %&#25;$&#25;&#21;
</p>
<p>&#1;&#3;&#2; &#4;&#13;&#10;&#20;!
</p>
<p>&#21;ffi&#23; %&#25;$&#25;&#21;#&quot;
</p>
<p>&#4;
</p>
<p>&quot; &#21;&#9;&#23; fl 'fi&#26; &#21; &#21;ffi&#23; ff
</p>
<p>&#14;&#16;&#15;
</p>
<p>&#21;ffi&#23; %&#25;$&#25;&#21;
</p>
<p>FIG. 3 &#8211; De&#769;roulement de l&#8217;algorithme ite&#769;ratif
</p>
<p>du signal une phrase de cou&#770;t moyen minimal, et qui repose sur l&#8217;ite&#769;ration d&#8217;un algorithme
spe&#769;cifique au mode&#768;le de langage conside&#769;re&#769;. Sur une premie&#768;re expe&#769;rience, on a constate&#769; que
le nombre d&#8217;ite&#769;rations ne&#769;cessaire avant la convergence reste tre&#768;s faible, me&#770;me par rapport a&#768; sa
valeur maximale the&#769;orique, ce qui rend ce ( ( meta-algorithme ) ) inte&#769;ressant du point de vue de son
efficacite&#769;. En revanche, la pertinence de l&#8217;utilisation du cou&#770;t moyen comme crite&#768;re d&#8217;extraction
doit encore e&#770;tre e&#769;value&#769;e pour d&#8217;autres mode&#768;les de langage stochastiques, et en fonction de
l&#8217;application conside&#769;re&#769;e.
</p>
<p>Re&#769;fe&#769;rences
</p>
<p>F.Itakura A.Ogawa, K.Takeda. &#8221;balancing acoustic and linguistic probabilities&#8221;. IEEE, pages 181&#8211;184,
1998.
C. Chelba. Exploiting Syntactic Structure for Natural Language Modeling. PhD thesis, John Hopkins
University, Baltimore, Maryland, 2000.
</p>
<p>J.-M. Boite H. Bourlard, B. D&#8217;Hoore. Optimizing recognition and rejection performance in wordspotting
systems. In ICASSP&#8217;94, volume I, pages 373&#8211;376, 1994.
H.Murveit and R.Moore. &#8221;integrating natural language constraints into hmm-based speech recognition&#8221;.
In ICASSP&#8217;90, pages 573&#8211;576, 1990.
C.-H. Lee E.R. Goodman J.G. Wilpon, L.R. Rabiner. Application of hidden markov models of keywords
in unconstrained speech. In ICASSP&#8217;89, pages 254&#8211;257, 1989.
M. Eske&#769;nazi L.F. Lamel, J.-L. Gauvain. BREF, a large vocabulary spoken corpus for french. In
Eurospeech&#8217;91, pages 505&#8211;508, 1991.
B.Juang L.Rabiner. Fundamentals of Speech Recognition. Prentice-Hall, 1993.
M.Meteer and J.R.Rohlicek. &#8221;statistical language modeling combining n-gram and context-free
grammars&#8221;. In Proc.of ICASSP&#8217;93, volume 2, pages 37&#8211;40, 1993.
M.C. Silaghi and H. Bourlard. &#8221;A new keyword spotting approach based on iterative dynamic
programming.&#8221; In ICASSP&#8217;2000, Istanbul, 2000.
J.-C.Chappelier, M.Rajman, R.Aragu&#776;e&#769;s, A.Rozenknop. &#8221;Lattice Parsing for Speech Recognition&#8221; In
TALN&#8217;99, pages 95&#8211;104, 1999.
J.-C.Chappelier, M.Rajman. &#8221;A generalized CYK algorithm for parsing stochastic CFG&#8221; In TAPD&#8217;98,
pages 133&#8211;137, 1998.</p>

</div></div>
</body></html>