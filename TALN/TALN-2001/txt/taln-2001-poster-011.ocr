TALN 2001, Tours, 2-5 juillet 2001

Algorithme de décodage de treillis selon le critere du coiit
moyen pour la reconnaissance de la parole

Antoine Rozenknop (1) et Marius Silaghi (2)
EPFL (DI-LIA) CH-1015 Lausanne (Suisse)
(1) Antoine.Rozenknop@epﬂ.ch, (2) Marius.Silaghi@epﬂ.ch

Résumé - Abstract

Les modeles de langage stochastiques utilisés pour la reconnaissance de la parole continue,
ainsi que dans certains systemes de traitement automatique de la langue, favorisent pour la
plupart l’interprétation d’un signal par les phrases les plus courtes possibles, celles-ci étant par
construction bien souvent affectées des coﬁts les plus bas. Cet article expose un algorithme
permettant de répondre an ce probleme en remplagant le coﬁt habituel affecté par le modele de
langage par sa moyenne sur la longueur de la phrase considérée. Cet algorithme est tres général
et peut étre adapté aisément a de nombreux modeles de langage, y compris sur des taches
d’analyse syntaxique.

Stochastic language models used for continous speech recognition, and also in some Automated
Language Processing systems, often favor the shortest interpretation of a signal, which are
affected with the lowest costs by construction. To cope with this problem, this article presents
an algorithm that allows the computation of the sequence with the lowest mean cost, in a very
systematical way. This algorithm can easily be adapted to several kinds of language models,
and to other tasks, such as syntactic analysis.

Mots-clefs/Keywords : Continuous speech recognition, Stochastic language models, Mean score.

1 Introduction

La reconnaissance de la parole continue a partir d’un signal acoustique est un probleme d’une
grande complexité du fait de la taille de l’espace de recherche des solutions. Aﬁn de la rendre
envisageable, il faut impérativement restreindre cet espace, par exemple en utilisant des modeles
de langage. Cependant, méme ainsi, le nombre de solutions correspondant a une realisation
acoustique proche du signal observé reste toujours tres grand; une probabilisation a priori
de l’espace de recherche est donc nécessaire (Murveit, Moore, 1990). Des exemples de tels
modeles de langage probabilistes sont accessibles et bien étudiés, parfois dans un cadre de
Traitement Automatique du Langage, mais présentent certains défauts s’ils sont utilisés sans
adaptation préalable au probleme de la reconnaissance de la parole. En particulier, les plus
utilisés d’entre eux, qui reposent sur une modélisation paramétrique de processus stochastiques

A. Rozenknop, M. Silaghi

(N-grams, grammaires stochastiques) affectent par construction des probabilités moindres aux
phrases les plus longues, ce qui peut induire un fort biais lorsque le nombre de mots prononcés
n’est pas connu a l’avance.

Une présentation succinte de l’utilisation de modeles de langages valués en reconnaissance de
la parole est donnée dans la section 2. Nous y exposons aussi une idée tout-a-fait classique:
elle consiste a utiliser en reconnaissance non pas le coﬁt d’une phrase telle qu’un modele de
langage stochastique le déﬁnit (c’est-a-dire comme l’opposé du logarithme de sa probabilité),
mais la moyenne de ce coﬁt sur le nombre de mots de la phrase. Nous présentons ensuite dans la
section 3 un algorithme original de décodage itératif, qui permet de déterminer les solutions de
plus faible coﬁt moyen, en s’appuyant sur les algorithmes existants de recherche de la solution
de plus faible coﬁt global. L’ exposé de l’algorithme est suivi de sa preuve, ainsi que d’un petit
exemple de déroulement, ou l’on utilise une grammaire stochastique pour décoder un treillis de
mots.

La mention de certains modeles de langage dans cet article a pour but de faire sentir l’intérét
qu’il y a a pouvoir extraire une phrase de coﬁt moyen minimum. Il ne faut cependant en aucun
cas y voir une limitation pour l’algorithme présenté, dont la grande force est justement son
caractere tres general.

2 Modeles de langage valués pour la reconnaissance de la
parole

Nombre de systemes de reconnaissance de la parole s’appuient sur des modeles de langage
probabilistes pour sélectionner une séquence de mots parmi les différentes interprétations possib-

les d’un signal. Chaque interprétation M recoit alors un ”coﬁt” acoustique C,,(M) calculé
par un module acoustique, ainsi qu’un ”coﬁt” linguistique C;(M) calculé a l’aide du modele
de langage probabiliste considéré; l’interprétation sélectionnée est celle ayant le coﬁt total
C(M) = C,,(M) + Cl(M) minimal.

Or, les modeles de langage les plus utilisés sont des modeles génératifs stochastiques, qui
produisent des séquences de mots par une succession d’étapes aléatoires. Les modeles de N-
grams et les grammaires stochastiques hors-context (SCFG) en sont de bons exemples. L’ intérét
de tels modeles est double: d’une part, leurs parametres sont facilement obtensibles a partir de
bases d’exemples; d’autre part, l’existence d’algorithmes efﬁcaces autorisent leur exploitation
effective pour le décodage de signaux de parole (Mteer, Jelinek, 1993). Mais ils présentent
aussi un inconvénient majeur: le coﬁt d’une séquence, calculé comme l’opposé du logarithme
de sa probabilité d’étre produite par le modele, croit rapidement avec le nombre d’étapes du
processus de production, donc avec le nombre de mots qui la composent. Ainsi ces modeles
considerent-ils que les hypotheses les plus courtes sont toujours les meilleures !

Pour pallier ce probleme, on cherche en general a minimiser C (M) — B |M | plutot que C (M)
(|M | est le nombre de mots de M, et B est une constante empiriquement déterminée).

Une autre idée naturelle est de chercher a minimiser le c0 :22,‘ moyen par mot C‘ (M) = C (M) / | M |.

C’est ce que permet de réaliser l’algorithme présenté dans la suite. Comme il utilise itérativement
l’algorithme qui calcule ArgminM C (M) — ﬂ M |, il ne requiert aucun espace mémoire supple-
mentaire; le nombre d’itérations nécessaires a l’obtention du résultat est le seul surcoﬁt algo-

Algorithme de décodage de treillis selon le critére du C0111‘ moyen

rithmique, et peut étre majoré par max |M | — |M° 

3 Algorithme de décodage itératif

3.1 Ingrédients

On dispose des éléments suivants:

— un ensemble E de phrases appartenant a un langage L, chaque phrase M étant constituée
de |M | mots,

— une fonction de coﬁt C qui a chaque élément M de L associe un réel C (M),

— un algorithme A(E, C, ﬂ) qui permet d’eXtraire :
ArgminMEE C(M) — ﬁ|M| pour n’importe quel réel B.

3.2 Réalisation

1:’ algorithme I(.A, E, C) suivant permet de trouver une solution M 0 de ArgminMEE C‘ (M) , on
C = C(M)/ IM I 3

1. Initialisation: on pose C‘ (M_1) = 0.12

2. Itérations_: on calcule Mi = ArgminMEE C (M) — C‘ (MZ~_1) - |M | en utilisant l’algorithme
A(E7 C7  3'

3. Critere d’arrét: on cesse les itérations lorsque  = |MZ~_1|. Mi est alors une solution
du probleme.
3.3 Preuve

Théoréme 1 L’alg0rithme I(.A, E, C) converge vers une solution M 0 = ArgminMEE C‘ (M)
en un nombre d ’itérati0ns inférieur £1 M1| — |M° 

Lemme 1 Le com moyen  décroft strictement avec i, pour 2' supérieur £1 0 et tam‘ que
C (M 0) n’a pas été atteint .'

Vi 2 0 [C‘(MZ~) > C*(M°) => C‘(MZ~) > C‘(MZ-+1)]

1. M _1 n’existant pas, ceci n’est qu’une convention d’écriture.

2. La Valeur initiale de C’ (M _1) est sans importance pour la correction de l’a1gorithme. Le choix du 0 est
arbitraire, et de fait, si You a une estimation a priori de la Valeur de l’optimum C’ (M 0), le choix de cette estimation
pour C’ (M _1) accelérera la convergence de l’a1gorithme par rapport au choix de la Valeur 0.

3. i est l’indice de l’itération en cours, et Vaut 0 pour la premiere iteration.

A. Rozenknop, M. Silaghi

Démonstration du lemme 1 :

Notons  = C(]\/[) — 

Par déﬁnition de C‘, on remarque immédiatement que: C; (M) = |M | (C (M) — C‘
L’algorithme A(E, C, C‘ trouve une solution MZ~+1 qui minimise C; (M), d’ou:

Mz'+1 = v4(E» 0; C(Mz')) Mz'+1 = Argmin CKM)

C‘£(Mz~+1) :Eg‘£(M°)
|Mz-+1|(C‘(Mz~+1)- 5‘(M)) S |M°|(C‘(M°) - C‘(Mz~))
I °|

llllllll

C‘(MZ~+1) g C*(MZ-) +
=> C*(MZ~+1) < C*(MZ~)

La derniere ligne de la demonstration vient de l’hypothese C‘ > C‘ (M 0), et de la stricte
positivité de |M | pour tout M appartement 2‘: E.

Lemme 2 La taille  des solutions successives décroft strictement pour 2' supérieur £1 1 et
tant que C (M 0) n’a pas éte’ atteint .'

vi 2 1 [C(Mi) > cm“) a» {Mm < {Mn}

Démonstration du lemme 2:
Pouri 2 1 ,

Mi =  C,  =>  = Argmin 

=> Ci—1(M:\J/:1E)EZ Ci—1(Mi)
=> |Mi+1|(O(Mi+1)— O(Mz‘—1)) Z |Mz"|(O(Mz‘) — C(Mz'—1))

Or d’apres le lemme 1, C‘ > C‘ (MZ11), ce qui permet de Ininorer le second membre de
l’inégalité précédente, et d’obtenir par transitivité :

|Mz+1|(5’(Mz+1)- C’(Mz'—1)) Z |Mz|(C_‘(Mz+1)- C_’(Mz—1))

Toujours d’apres le lemme 1, C‘(MZ~_1) >  (cari — 1 2 0), donc C‘(MZ~_1) > C‘(MZ~+1).
On peut alors simpliﬁer les deux membres de l’inégalité précédente, en la renversant, ce qui
donne ﬁnalement:

|Mz'+1| < |Mi|

Démonstration du théoreme 1 :

Le lemme 2 montre que la taille des solutions M i décroit strictement pour 71 Z 1 tant que
C‘ > C‘ (M 0). Comme cette taille est un entier strictement positif, elle cesse forcément de
décroitre pour un certain 71 = 71 f, ce qui implique que C‘ f) = C‘ (M 0). L’ algorithme atteint
donc la solution du probleme en un nombre ﬁni d’itérations, et comme |M décroit strictement
de 71 = 1 hi =71]: — 1, le nombre d’itérations if vériﬁe: if 3 |M1| —  = |M1| — |M°|.

Algorithme de décodage de treillis selon le critére du C0111‘ moyen

3.4 Exemple de déroulement

Aﬁn d’illustrer l’algorithme itératif, nous présentons dans cette partie un petit exemple de
décodage d’un treillis de mots a l’aide d’une grammaire stochastique. Les regles de la grammaire
apparaissent dans la ﬁgure 1. Le treillis a decoder contient trois interprétations possibles:
(1) Cé1im‘ene, (2) Celine m‘ene et (3) C’ est 1’ hymen. A chacune correspond un arbre
syntaxique, représenté dans la ﬁgure 2, avec son coﬁt associé. On rappelle que le coﬁt d’un
arbre est la somme des coﬁts des regles qui le constituent, et que le coﬁt moyen est cette meme
somme divisée par le nombre de feuilles de l’arbre.

Régle R P(R) C (R) Régle R P(R) C(R)
R11 S —> NP  0,  R23 V —> 1115116 1/2 0, 
R21 S —> NP V 1/3 0,477 R33 V —> est 1/2 0,301

R31    R35 N—>hymen  1,176
R12 NP —> Céliméne 1/2 0,301 R5 N —> bateau 14/15 0,030
R22 NP —> Céline 1/2 0,301 R4 A —> un 1/2 0,301
R32 P—> C, 1 0 R34 A—> 1, 1/2 

FIG. 1 — Régles de la grammaire avec leurs probabilités et leurs coﬁts. Le com,‘ d ’une régle vaut
— log P

M A1 A2 A3

Interpretation Célimene Céline mene C’ est 1 ’ hymen
Régles R11, R12 R21, R22, R23 R31, R32, R33, R34, R35
Probabilité 1 / 6 1/12 1/180

Coﬁt 0, 778 1, 079 2, 255

Coﬁt moyen 0,778 0,540 0,564

FIG. 2 — Colfnfs des dzﬁérentes interprétations possibles.

Les étapes de l’algorithme sont détaillées dans la ﬁgure 3, chaque ligne y représentant une
itération, avec: (1) l’indice de l’itération, (2) le coﬁt moyen de l’interprétation extraite lors de
l’itération précédente, (3) le critere a minimiser, (4,5,6) les valeurs du critere pour les différentes
interprétations possibles, (7) l’interprétation qui minimise le critere et (8) son coﬁt moyen.

L’algorithme d’analyse syntaxique utilisé (Chappelier et al., 1999; Chappelier,Rajman,1998)
peut extraire la solution qui minimise le critere C,’ (M) en utilisant pour les regles terIr1inales
R0, (oz 6 «[12, 22, 23, 32, 33, 34, 35, 4, 5}) les coﬁts C(RO,) — C(M,~_1) a la place de C(RO,).
On peut remarquer que, comme prévu par les lemmes 1 et 2, le coﬁt moyen des solutions
successives diminue a partir de 71 = 0, et que leur taille diminue a partir de 71 = 1.

4 Conclusion

Dans cet article, nous avons décrit une classe de modeles de langage valués, utilisés pour
la reconnaissance de la parole et permettant de décoder un signal en extrayant la phrase de
coﬁt minimal. Ces modeles ayant souvent la propriété de faire dépendre le coﬁt d’une phrase
de sa longueur, on en a dérivé un <<meta-algorithme>>, aussi général que possible, qui extrait

A. Rozenknop, M. Silaghi

Itération C(M,~_1)  Valeurs de  M, 
A1 A2 A3
i=0 0 C(M) 0,778 1,08 2,26 A1 0,778
i=1 0,778 C(M)-0,778|M| 0 —0,477 -0,852 A3 0,565
i=2 0,565 C(M)-0,565|M| 0,213 —0,051 0 A2 0,540
4:3 0,540 C(M)-0,540|M| 0,238 0 0,1 A2 0,540

FIG. 3 - Déroulement de l’alg0rithme itératif

du signal une phrase de coﬁt moyen minimal, et qui repose sur l’itération d’un algorithme
spéciﬁque au modele de langage considéré. Sur une premiere experience, on a constaté que
le nombre d’itérations nécessaire avant la convergence reste tres faible, meme par rapport a sa
valeur maximale théorique, ce qui rend ce <<meta-algorithme» intéressant du point de vue de son
efﬁcacité. En revanche, la pertinence de l’utilisation du coﬁt moyen comme critere d’extraction
doit encore étre évaluée pour d’autres modeles de langage stochastiques, et en fonction de
1’ application considérée.

Références

F.Itakura A.Ogawa, K.Takeda. ”balancing acoustic and linguistic probabilities”. IEEE, pages 181-184,
1998.

C. Chelba. Exploiting Syntactic Structure for Natural Language Modeling. PhD thesis, John Hopkins
University, Baltimore, Maryland, 2000.

J .—M. Boite H. Bourlard, B. D’Hoore. Optimizing recognition and rejection performance in wordspotting
systems. In ICASSP’94, volume I, pages 373-376, 1994.

H.Murveit and R.Moore. ”integrating natural language constraints into hmm—based speech recognition”.
In ICASSP’90, pages 573-576, 1990.

C.—H. Lee E.R. Goodman J .G. Wilpon, L.R. Rabiner. Application of hidden markov models of keywords
in unconstrained speech. In ICASSP’89, pages 254-257, 1989.

M. Eskénazi L.F. Lamel, J .—L. Gauvain. BREF, a large vocabulary spoken corpus for french. In
Eurospeech’9I, pages 505-508, 1991.

B.Juang L.Rabiner. Fundamentals of Speech Recognition. Prentice—Hall, 1993.

M.Meteer and J .R.Rohlicek. ”statistical language modeling combining n—graIn and context—free
grarmnars”. In Proc.ofICASSP’93, volume 2, pages 37-40, 1993.

M.C. Silaghi and H. Bourlard. ”A new keyword spotting approach based on iterative dynamic
programming.” In ICASSP’2000, Istanbul, 2000.

J .—C.Chappelier, M.Rajman, R.Arag1'iés, A.Rozenk11op. ”Lattice Parsing for Speech Recognition” In
TALN’99, pages 95-104, 1999.

J .—C.Chappelier, M.Rajman. ”A generalized CYK algorithm for parsing stochastic CFG” In TAPD’98,
pages 133-137, 1998.

