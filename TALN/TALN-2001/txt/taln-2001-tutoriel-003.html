<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
"http://www.w3.org/TR/html4/loose.dtd">
<html><head><title>Grammaires de d&#142;pendance formelles et th&#142;orie Sens-Texte</title>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
</head>
<body>
<div style="page-break-before:always; page-break-after:always"><div><p>TALN 2001, Tours, 2-5 juillet 2001
</p>
<p>Grammaires de d&#142;pendance formelles et th&#142;orie Sens-Texte
</p>
<p>Sylvain Kahane
</p>
<p>Lattice, Universit&#142; Paris 7, UFRL
Case 7003, 2, place Jussieu, 75251 Paris cedex 5
</p>
<p>sk@ccr.jussieu.fr
</p>
<p>R&#142;sum&#142; &#8212; Abstract
On appelle grammaire de d&#142;pendance toute grammaire formelle qui manipule comme
repr&#142;sentations syntaxiques des structures de d&#142;pendance. Le but de ce cours est de pr&#142;senter &#136;
la fois les grammaires de d&#142;pendance (formalismes et  algorithmes de synth&#143;se et d&#213;analyse) et
la th&#142;orie Sens-Texte, une th&#142;orie linguistique riche et pourtant m&#142;connue, dans laquelle la
d&#142;pendance joue un r&#153;le crucial et qui sert de base th&#142;orique &#136; plusieurs grammaires de
d&#142;pendance.
</p>
<p>We call dependency grammar every grammar which handles dependency structures as syntactic
representations. The aim of this course is to present both dependency grammars (formalisms,
analysis and synthesis algorithms) and the Meaning-Text theory, a rich but nevertheless
unrecognized linguistic theory, in which dependency  plays a crucial role and which serves as
theoretical basis of several dependency grammars.
</p>
<p>1 Introduction
</p>
<p>La repr&#142;sentation syntaxique d&#8217;une phrase par un arbre de d&#142;pendance est certainement plus
ancienne que la repr&#142;sentation par un arbre syntagmatique. L&#8217;usage des d&#142;pendances remonte &#136;
l&#8217;antiquit&#142;. Les grammairiens arabes du 8i&#143;me si&#143;cle, comme Sibawaih, distinguaient d&#142;j&#136;
gouverneur et gouvern&#142; en syntaxe et utilisait cette distinction pour formuler des r&#143;gles d&#8217;ordre
des mots ou de rection (Owens 1988:79-81). On trouve des repr&#142;sentations de structures de
d&#142;pendance dans des grammaires du 19&#142;me si&#143;cle (Weber 1992:13). La premi&#143;re th&#142;orie
linguistique bas&#142;e sur la d&#142;pendance est incontestablement celle de Tesni&#143;re (1934, 1959), sans
minimiser des travaux pr&#142;curseurs, comme les repr&#142;sentations &#210;mol&#142;culaires&#211; de Jespersen
(1924) ou la syntaxe du russe de Pe&#228;kovskij (1934). Peu apr&#143;s, Hays (1960, 1964) d&#142;veloppait
la premi&#143;re grammaire de d&#142;pendance, tandis que Gaifman (1965) &#142;tablissait les liens entre les
grammaires de d&#142;pendance de Hays, les grammaires cat&#142;gorielles de Bar-Hillel et les
grammaires de r&#142;&#142;criture de Chomsky. A l&#8217;exception de la grammaire de Robinson (1970), les
grammaires de d&#142;pendance se sont ensuite surtout d&#142;velopp&#142;es en Europe, notamment autour de
Sgall et Haji&#139;ov&#135; &#136; Prague (Sgall et al. 1986) et de Mel&#8217;&#139;uk &#136; Moscou (Mel&#8217;&#139;uk 1974, 1988a),
ainsi qu&#8217;en Allemagne (cf., par ex., la classique grammaire de l&#8217;allemand de Engel 1992) et au
Royaume Uni autour de Anderson (1971) et Hudson (1990), la France restant curieusement &#136;
l&#8217;&#142;cart.
</p>
<p>La repr&#142;sentation syntaxique d&#8217;une phrase par une structure syntagmatique, quant &#136; elle, ne
s&#8217;est d&#142;velopp&#142;e qu&#8217;&#136; partir de Bloomfield (1933) et des travaux des distributionnalistes.
L&#8217;engouement formidable pour les grammaires g&#142;n&#142;ratives-transformationnelles de Chomsky
(1957, 1965) dans les ann&#142;es 60-70 a retard&#142; l&#8217;essor des grammaires de d&#142;pendance. Pourtant,</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Sylvain Kahane
</p>
<p>depuis la fin des ann&#142;es 70 et l&#8217;av&#143;nement de la Syntaxe X-barre, la plupart des mod&#143;les
linguistiques issus de la mouvance chomskienne (GB/PP/MP, LFG, G/HPSG) ont introduit
l&#8217;usage de la d&#142;pendance syntaxique sous des formes plus ou moins cach&#142;es (fonctions
syntaxiques, constituants avec t&#144;te, cadre de sous-cat&#142;gorisation, c-commande). De leur c&#153;t&#142;,
les grammaires compl&#143;tement lexicalis&#142;es comme les Grammaires Cat&#142;gorielles ou TAG (Joshi
1987), en d&#142;rivant une phrase par combinaison de structures &#142;l&#142;mentaires associ&#142;es aux mots de
la phrase, construisent, par un effet de bord, des structures de d&#142;pendances.
</p>
<p>Le retour de la d&#142;pendance au premier plan, au cours des ann&#142;es 80, est d&#158; &#136; deux facteurs
principaux : le retour en gr&#137;ce du lexique d&#8217;une part et de la s&#142;mantique d&#8217;autre part. Pour le
lexique, grammaires de d&#142;pendance, en mettant la lexie au centre de la structure syntaxique,
permettent d&#8217;exprimer simplement les relations lexicales comme la valence et le r&#142;gime (ou sous-
cat&#142;gorisation). Pour la s&#142;mantique, les structures de d&#142;pendance, en permettant de dissocier
l&#8217;ordre des mots et la structure syntaxique proprement dite, se rapprochent davantage d&#8217;une
repr&#142;sentation s&#142;mantique que ne le fait une structure syntagmatique. Mieux encore, les relations
s&#142;mantiques pr&#142;dicat-argument, parfois appel&#142;es d&#142;pendances s&#142;mantiques, bien que devant &#144;tre
distingu&#142;es des d&#142;pendances syntaxiques, co&#149;ncident en partie avec celles-ci (cf. Mel&#8217;&#139;uk
1988b, Kahane &amp; Mel&#8217;&#139;uk 1999).
</p>
<p>Enfin, les grammaires de d&#142;pendance prouvent &#136; l&#8217;heure actuelle leur bonne ad&#142;quation au
traitement automatique des langues. Citons deux syst&#143;mes d&#8217;envergure d&#142;velopp&#142;s en France :
le g&#142;n&#142;rateur de texte d&#142;velopp&#142; &#136; LexiQuest par Coch (1996) et int&#142;gr&#142; au syst&#143;me MultiM&#142;t&#142;o
(Coch 1998) de g&#142;n&#142;ration de bulletins m&#142;t&#142;o multilingues et l&#8217;analyseur en flux de Vergne
(2000) qui a remport&#142; l&#8217;action Grace, portant sur l&#8217;&#142;valuation des &#142;tiqueteurs pour le fran&#141;ais.
Pour d&#8217;autres travaux, on pourra &#142;galement se reporter aux actes du dernier atelier sur le
traitement automatique par des grammaires bas&#142;es sur la d&#142;pendance (Kahane &amp; Polgu&#143;re
1998), au num&#142;ro sp&#142;cial de la revue TAL sur les grammaires de d&#142;pendance (Kahane 2000c) et
au portail officiel des grammaires de d&#142;pendance (http://ufal.mff.cuni.cz/dg.html).
Comme annonc&#142; dans le titre, cet expos&#142; est consacr&#142; aux grammaires de d&#142;pendance en g&#142;n&#142;ral
et &#136; la th&#142;orie Sens-Texte en particulier. Dans la Section 2, nous tenterons de caract&#142;riser les
notions de base de d&#142;pendance syntaxique et de fonction syntaxique et nous pr&#142;senterons les
premi&#143;res grammaires de d&#142;pendance. La Section 3 sera consacr&#142;e &#136; la th&#142;orie Sens-Texte
[TST], probablement la plus achev&#142;e des th&#142;ories linguistiques bas&#142;es sur la d&#142;pendance. Dans
la Section 4, nous ferons le lien entre la TST, dont les r&#143;gles servent &#136; mettre en correspondance
des structures, et les grammaires g&#142;n&#142;ratives, dont les r&#143;gles servent &#136; g&#142;n&#142;rer des structures, et
nous proposerons une grammaire de d&#142;pendance bas&#142;e sur les principes th&#142;oriques de la TST,
mais utilisant un formalisme d&#8217;unification. Dans la Section 5, nous nous pencherons sur les
techniques de base pour l&#8217;analyse avec une grammaire de d&#142;pendance.
</p>
<p>Je souhaite insister sur le fait que cet expos&#142; n&#8217;est pas un survol impartial du domaine des
grammaires de d&#142;pendance, loin de l&#136;. Il s&#8217;agit clairement d&#8217;un point de vue personnel sur la
question, enrichi, je l&#8217;esp&#143;re, de mes nombreuses discussions avec Igor Mel&#8217;&#139;uk sur les
fondements de la th&#142;orie Sens-Texte et sur le r&#153;le de la d&#142;pendance en linguistique. J&#8217;en profite
pour le remercier chaleureusement pour ses nombreuses remarques sur la premi&#143;re version de ce
texte.
</p>
<p>2 Arbres et grammaires de d&#142;pendance
Dans la Section 2.1, nous tenterons de caract&#142;riser la notion de d&#142;pendance syntaxique. A
travers divers exemples, nous exposerons les points qui font l&#213;unanimit&#142; entre les diff&#142;rentes
th&#142;ories et ceux qui posent probl&#143;me (l&#8217;auxiliaire, la conjonction de subordination, le
d&#142;terminant, le pronom relatif, la coordination, &#201;). Nous nous int&#142;resserons aux diff&#142;rentes
fa&#141;ons d&#8217;encoder la d&#142;pendance et en particulier &#136; l&#8217;&#142;quivalence entre arbres de d&#142;pendance et
arbres syntagmatiques avec t&#144;tes. Dans la Section 2.2, nous nous int&#142;resserons &#136; la notion de</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Grammaires de d&#142;pendance formelles et th&#142;orie Sens-Texte
</p>
<p>fonction syntaxique qui est indissociable de celle de d&#142;pendance syntaxique. Enfin, dans la
Section 2.3, nous pr&#142;senterons les premi&#143;res grammaires de d&#142;pendance (Hays 1964, Gaifman
1965) et leur lien avec les grammaires cat&#142;gorielles et les grammaires de r&#142;&#142;criture hors-
contexte.
</p>
<p>2.1 Caract&#142;risation de la notion de d&#142;pendance syntaxique
La quasi-totalit&#142; des th&#142;ories linguistiques s&#8217;accordent sur le fait que, au-del&#136; de la question du
sens, les mots d&#8217;une phrase ob&#142;issent &#136; un syst&#143;me d&#8217;organisation relativement rigide, qu&#8217;on
appellera la structure syntaxique de la phrase. Il existe deux grands paradigmes pour repr&#142;senter
cette structure : soit d&#142;crire la fa&#141;on dont les mots peuvent &#144;tre group&#142;s en des paquets de plus
en plus gros (ce qui a donn&#142; les structures syntagmatiques), soit expliquer la fa&#141;on dont les
mots, par leur pr&#142;sence, d&#142;pendent les uns des autres (ce qui a donn&#142; les structures de
d&#142;pendance). Comme on le verra, les deux paradigmes s&#8217;opposent davantage sur la fa&#141;on de
pr&#142;senter l&#8217;organisation syntaxique que sur la nature m&#144;me de cette organisation.1
</p>
<p>Consid&#142;rer qu&#8217;un arbre de d&#142;pendance syntaxique peut rendre compte des propri&#142;t&#142;s
syntaxiques d&#8217;une phrase, c&#8217;est consid&#142;rer que dans une phrase, la pr&#142;sence de chaque mot (sa
nature et sa position) est l&#142;gitim&#142;e par la pr&#142;sence d&#8217;un autre mot (son gouverneur syntaxique),
&#136; l&#8217;exception d&#8217;un mot, le mot principal associ&#142; au sommet de l&#8217;arbre syntaxique. La
d&#142;pendance syntaxique est donc une d&#142;pendance entre mots (Figure 1 &#136; gauche).
</p>
<p>Figure 1 : Arbre de d&#142;pendance et arbre &#136; la Gladkij
pour Le petit gar&#141;on parle &#136; Marie
</p>
<p>Cette pr&#142;sentation de la structure syntaxique de la phrase est souvent mal accept&#142;e de ceux qui
voient plut&#153;t des relations entre des mots et des groupes de mots que des relations entre des
mots seulement. Pr&#142;cisons ce point. Quand un mot x l&#142;gitime la pr&#142;sence d&#8217;un mot y (c&#8217;est-&#136;-
dire quand x gouverne y), en fait, par transitivit&#142;, x l&#142;gitime &#142;galement la pr&#142;sence des mots
l&#142;gitim&#142;s par y et des mots l&#142;gitim&#142;s par ceux-ci. En cons&#142;quence, x l&#142;gitime non seulement la
pr&#142;sence de y, mais la pr&#142;sence d&#8217;un groupe de mot, qu&#8217;on appelle la projection de y. On peut
donc pr&#142;senter la structure de d&#142;pendance non pas comme des d&#142;pendances entre mots, mais
comme des d&#142;pendances entre des mots et des groupes de mots (&#136; l&#8217;int&#142;rieur desquels il y a &#136;
nouveau des d&#142;pendances entre mots et groupes de mots. Cette structure de d&#142;pendance entre
des mots et des groupes peut &#144;tre repr&#142;sent&#142;e par une structure que nous appellerons un &#210;arbre&#211;
&#136; la Gladkij (Figure 1 &#136; droite) (Gladkij 1968, Kahane 1997). A l&#8217;int&#142;rieur de chaque groupe
</p>
<p>                                                
</p>
<p>1
 Je parle ici des repr&#142;sentations elles-m&#144;mes. Il existe bien s&#158;r des oppositions plus fondamentales qui ont
</p>
<p>conduit les uns o&#157; les autres &#136; d&#142;velopper telle ou telle mani&#143;re de pr&#142;senter les choses. En particulier, comme
je l&#8217;ai d&#142;j&#136; dit, la grammaire syntagmatique est n&#142;e d&#8217;une vision purement orient&#142; vers l&#8217;analyse (&#136; partir du
texte), le distributionnalisme, et d&#8217;un rejet presque absolu des diff&#142;rences lexicales (Gross 1975) et des
questions de s&#142;mantique.
</p>
<p>parle
</p>
<p>&#136;
</p>
<p>petit Marie
</p>
<p>gar&#141;on
</p>
<p>le
</p>
<p>parle
</p>
<p>&#136;
</p>
<p>petit Marie
</p>
<p>gar&#141;on
</p>
<p>le</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Sylvain Kahane
</p>
<p>ainsi consid&#142;r&#142;, il y a un mot qui n&#8217;appartient &#136; aucun sous-groupe et qu&#8217;on appelle la t&#144;te2.  On
peut aussi repr&#142;senter l&#8217;arbre &#136; la Gladkij par une structure syntagmatique avec t&#144;te lexicale,
c&#8217;est-&#136;-dire une structure syntagmatique traditionnelle3 o&#157; chaque constituant poss&#143;de un sous-
constituant t&#144;te qui est un mot (voir Figure 2 o&#157; la structure syntagmatique est repr&#142;sent&#142;e, &#136;
gauche, par un ench&#137;ssement de groupe et, &#136; droite, par un arbre non &#142;tiquet&#142; formellement
&#142;quivalent ; dans les deux cas, le sous-constituant t&#144;te est indiqu&#142; par l&#8217;&#142;tiquette T). Le fait de
consid&#142;rer pour chaque constituant une t&#144;te n&#8217;est pas nouveau (cf. par ex. Pittman 1948). Ceci
est devenu monnaie courante depuis la Syntaxe X-barre (Jackendoff 1977).
</p>
<p>Figure 2 : Arbres syntagmatiques avec t&#144;tes
pour Le petit gar&#141;on parle &#136; Marie
</p>
<p>Les structures syntagmatiques avec t&#144;te et les arbres de d&#142;pendance (entre mots) sont
formellement &#142;quivalents (Gladkij 1966, Robinson 1970). On a vu comment on passe d&#8217;un
arbre de d&#142;pendance &#136; une structure syntagmatique avec t&#144;te en introduisant les groupes obtenus
par transitivation de la relation de d&#142;pendance. Inversement, on passe d&#8217;un arbre syntagmatique
avec t&#144;te lexicale &#136; un arbre de d&#142;pendance en ne repr&#142;sentant plus les groupes et en reliant le
gouverneur d&#8217;un groupe directement avec la t&#144;te de ce groupe. 4
</p>
<p>Apr&#143;s nous &#144;tre attach&#142; aux diff&#142;rentes fa&#141;ons de repr&#142;senter formellement la d&#142;pendance, nous
allons aborder la question de la caract&#142;risation th&#142;orique de la d&#142;pendance. Tesni&#143;re lui-m&#144;me ne
caract&#142;rise pas clairement la d&#142;pendance.  Mel&#8217;&#139;uk 1988a propose, &#136; la suite de Garde 1977,
une tentative de caract&#142;risation directement en terme de d&#142;pendance entre mots. Du fait de
l&#8217;&#142;quivalence entre arbres de d&#142;pendance et structure syntagmatique avec t&#144;te, il est &#142;galement
possible de caract&#142;riser la d&#142;pendance en caract&#142;risant le constituant, puis la t&#144;te d&#8217;un
constituant. Nous ne nous attarderons pas sur la fa&#141;on d&#8217;identifier les constituants, mais sur la
fa&#141;on d&#8217;identifier la t&#144;te d&#8217;un constituant. Concernant les diff&#142;rentes d&#142;finitions possibles de la
t&#144;te et les cas litigieux, citons tout particuli&#143;rement le travail de Zwicky 1985. Nous adopterons
ici la d&#142;finition suivante (Mel&#8217;&#139;uk 1988a) :
</p>
<p>                                                
</p>
<p>2 Nous distinguons clairement les termes t&#144;te et gouverneur. Le gouverneur x d&#8217;un mot y (ou d&#8217;un goupe G) est
le mot qui l&#142;gitimise la pr&#142;sence de y (ou de G). Il n&#8217;appartient pas &#136; G. Par contre, la t&#144;te y du groupe G est
un mot de G qui l&#142;gitimise, par transitivit&#142;, la pr&#142;sence des autres mots de G.
</p>
<p>3
 L&#8217;arbre de d&#142;pendance n&#8217;encode pas l&#8217;ordre lin&#142;aire des mots. Bien que ce ne soit pas l&#8217;usage, on parle donc ici
</p>
<p>d&#8217;un arbre syntagmatique non ordonn&#142;.
</p>
<p>4
 On peut &#142;galement obtenir un arbre de d&#142;pendance &#136; partir d&#8217;une structure syntagmatique avec t&#144;te o&#157; on
</p>
<p>autorise le sous-constituant t&#144;te &#136; &#144;tre un groupe de mots. En d&#8217;autres termes, on autorise en fait un mot &#136; &#144;tre
la t&#144;te lexicale de plusieurs constituants. Dans ce cas, lors du passage &#136; un arbre de d&#142;pendance, on &#142;crase les
diff&#142;rents constituants qui poss&#143;dent la m&#144;me t&#144;te et la structure de d&#142;pendance est donc structurellement plus
pauvre, bien qu&#8217;on puisse r&#142;cup&#142;rer cette information (l&#8217;appartenance &#136; une projection de la t&#144;te et pas &#136; une
autre) autrement, par exemple dans l&#8217;&#142;tiquetage des d&#142;pendances, en consid&#142;rant diff&#142;rents types de relations
syntaxiques comme cela est l&#8217;usage en grammaire de d&#142;pendance.
</p>
<p>parle
&#136;
</p>
<p>petit Marie
</p>
<p>gar&#141;on
</p>
<p>le
</p>
<p>T
</p>
<p>TT parle
</p>
<p>&#136;petit Mariegar&#141;on le
T
</p>
<p>T
</p>
<p>T</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Grammaires de d&#142;pendance formelles et th&#142;orie Sens-Texte
</p>
<p>La t&#144;te syntaxique d&#8217;un constituant est l&#8217;&#142;l&#142;ment qui d&#142;termine la valence passive de ce
constituant, c&#8217;est-&#136;-dire l&#8217;&#142;l&#142;ment qui contr&#153;le la distribution de ce constituant.
</p>
<p>Nous allons illustrer cette d&#142;finition par des exemples. Commence par la t&#144;te de la phrase. A
l&#8217;int&#142;rieur de la proposition, tout le monde s&#8217;accorde &#136; consid&#142;rer que le verbe fini est la t&#144;te, car
c&#8217;est bien la pr&#142;sence du verbe fini qui fait qu&#8217;il s&#8217;agit d&#8217;une proposition. Signalons n&#142;anmoins
deux difficult&#142;s :
</p>
<p>1) Lorsque le verbe est compos&#142; (comme dans Pierre a donn&#142; un livre &#136; Marie), on peut
s&#8217;interroger sur qui de l&#8217;auxiliaire ou du participe est la t&#144;te de la proposition. Certains
consid&#143;rent que l&#8217;auxiliaire d&#142;pend du participe. Je serais plut&#153;t enclin &#136; pr&#142;f&#142;rer consid&#142;rer,
&#136; la suite de Tesni&#143;re ou de Mel&#8217;&#139;uk, que l&#8217;auxiliaire est la t&#144;te. En effet, c&#8217;est l&#8217;auxiliaire qui
porte le mode (Il faudrait que Pierre ait/*a donn&#142; &#201; ), qui h&#142;rite de marques grammaticales
(Pierre pense avoir donn&#142; &#201; ; Pierre a-t-il donn&#142;&#201;?), qui porte la n&#142;gation (Pierre n&#8217;a
pas donn&#142;&#201;), qui peut rester seul (Pierre a-t-il donn&#142; &#201;? Oui, il a.), &#201;
</p>
<p>2) Dans des langues comme le fran&#141;ais ou l&#8217;anglais o&#157; la pr&#142;sence du sujet est obligatoire, des
linguistes ont &#142;t&#142; amen&#142;s &#136; consid&#142;rer que la pr&#142;sence du sujet n&#8217;&#142;tait pas l&#142;gitim&#142; par le
verbe, mais par un principe sup&#142;rieur. Dans la grammaire g&#142;n&#142;rative, on consid&#143;re &#136; l&#8217;heure
actuelle que le sujet, &#136; la diff&#142;rence des compl&#142;ments n&#8217;est pas gouvern&#142; par le verbe en tant
que tel, mais par le morph&#143;me grammatical  exprimant le temps. Dans la mesure o&#157; ce
morph&#143;me appartient &#136; la forme verbale et o&#157; en grammaire de d&#142;pendance on ne consid&#143;re
que les d&#142;pendances entre mots, les deux approches sont compatibles. Elles le sont encore
en consid&#142;rant que lorsque le verbe est compos&#142;, le sujet d&#142;pend de l&#8217;auxiliaire qui est aussi
le porteur de la flexion temporelle.
</p>
<p>Plus g&#142;n&#142;ralement, tout le monde s&#8217;accorde sur le sens de la relation de d&#142;pendance lorsqu&#8217;il
existe une relation de subordination, c&#8217;est-&#136;-dire lorsqu&#8217;il existe une relation actancielle (entre
une t&#144;te et son actant) ou une relation modificative (entre une t&#144;te et un modifieur) (m&#144;me si la
fronti&#143;re entre actant et modifieur est parfois difficile &#136; saisir).5 Pose probl&#143;me la coordination et
les relations avec des &#142;l&#142;ments jouant un r&#153;le grammatical, notamment les compl&#142;menteurs, les
d&#142;terminants et les auxiliaires. Avant de parler de la coordination, nous allons aborder la
question des &#142;l&#142;ments grammaticaux en &#142;voquant la th&#142;orie de la translation de Tesni&#143;re (1959).
Si l&#8217;&#711;uvre de Tesni&#143;re est bien connue pour ce qui concerne la d&#142;pendance, on a souvent oubli&#142;
sa th&#142;orie de la translation qu&#8217;il consid&#142;rait probablement comme sa d&#142;couverte principale (bien
qu&#8217;on puisse estimer que l&#8217;id&#142;e est d&#142;j&#136; l&#136; dans la th&#142;orie des rangs de Jespersen 1924). Selon
Tesni&#143;re, il existe 4 parties du discours majeures (verbe, nom, adjectif, adverbe) avec des
relations prototypiques entre ces parties du discours : les actants du verbe sont des noms et ses
modifieurs des adverbes, les d&#142;pendants du nom sont des adjectifs et les d&#142;pendants de
l&#8217;adjectif et de l&#8217;adverbe sont des adverbes. N&#142;anmoins, un &#142;l&#142;ment de partie du discours X
peut venir occuper une position normalement r&#142;serv&#142;e &#136; un &#142;l&#142;ment de partie du discours Y,
mais dans ce cas, l&#8217;&#142;l&#142;ment doit &#144;tre translat&#142; de la partie du discours X &#136; la partie du discours Y
par un &#142;l&#142;ment morphologique ou analytique appel&#142; un translatif de X en Y. Comme il y a 4
parties du discours majeures, il y aura 16 types de translatifs (y compris des translatifs de X en
X qui ne change pas la partie du discours). Par exemple un verbe peut &#144;tre l&#8217;actant d&#8217;un autre
verbe (c&#8217;est-&#136;-dire occuper une position nominale), mais il devra &#144;tre &#136; l&#8217;infinitif ou &#144;tre
accompagn&#142; de la conjonction de subordination que (Pierre veut la parole ; Pierre veut parler
; Pierre veut que Marie parle). L&#8217;infinitif et la conjonction de subordination que sont donc des
translatifs de verbe en nom. De m&#144;me, les participes pass&#142; et pr&#142;sent, qui permettent &#136; un verbe
de modifier un nom (le livre rouge ; le livre vol&#142; par Pierre ; la personne volant le livre), sont
                                                
</p>
<p>5
 La distinction entre actants et modifieurs est consid&#142;r&#142;e par Tesni&#143;re (1959), &#136; qui l&#8217;on doit le terme d&#8217;actant (et
</p>
<p>de circonstant pour les modifieurs). Nous reviendrons dans la Section 3.2 sur cette distinction qui joue un
grand r&#153;le dans la th&#142;orie Sens-Texte.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Sylvain Kahane
</p>
<p>des translatifs de verbe en adjectif, la copule &#142;tant &#136; son tour un translatif d&#8217;adjectif en verbe (le
livre est rouge ; le livre est vol&#142; par Pierre). Les pr&#142;positions quant &#136; elles seront cat&#142;goris&#142;es
comme des translatifs de nom en adjectif ou en adverbe (le livre rouge ; le livre de Pierre ;
Pierre boit maladroitement ; Pierre boit avec maladresse).
Les cas de translation suscitent g&#142;n&#142;ralement des discussions quant au choix de la t&#144;te : le
translatif, lorsqu&#8217;il est analytique doit-il &#144;tre trait&#142; comme le gouverneur du translat&#142; ou comme
un d&#142;pendant ? Si l&#8217;on s&#8217;en tient &#136; notre d&#142;finition de la t&#144;te, le translatif doit &#144;tre clairement
consid&#142;r&#142; comme le gouverneur, car c&#8217;est bien lui qui contr&#153;le la valence passive, son r&#153;le &#142;tant
justement de permettre au translat&#142; d&#8217;occuper des positions auxquelles il ne pourrait acc&#142;der
sans &#144;tre translat&#142;. N&#142;anmoins, certains, comme Pollard &amp; Sag (1994:44), consid&#143;re que la
conjonction de subordination que doit &#144;tre trait&#142;e comme un marqueur, le verbe restant la t&#144;te de
la compl&#142;tive, arguant du fait que la distribution de la compl&#142;tive d&#142;pend &#142;galement du mode qui
est port&#142; par le verbe (Il faut que Pierre parte/*part ; Marie pense que Pierre part/*parte). En
fait, cela revient &#136; traiter les deux &#142;l&#142;ments, le translatif et le translat&#142;, plus ou moins comme des
co-t&#144;tes, puisque les traits t&#144;te et marqueur sont tous les deux des traits de t&#144;te (c&#8217;est-&#136;-dire des
traits dont les valeurs montent sur la structure r&#142;sultante de leur combinaison). Tesni&#143;re lui-
m&#144;me h&#142;site &#136; traiter le translatif comme le gouverneur du translat&#142; et pr&#142;f&#143;re parler de nucl&#142;us
translatif : il repr&#142;sente alors le translatif et le translat&#142; comme un groupe (dessin&#142;
horizontalement) et d&#142;pendant ensemble de leur gouverneur. En plus, du fait que le translat&#142;
contr&#153;le aussi quelque peu la distribution du groupe translatif-translat&#142; (par exemple, certaines
positions n&#8217;acceptent que des verbes infinitifs, c&#8217;est-&#136;-dire des verbes translat&#142;s en nom, mais
pas de noms : Pierre peut partir ; *Pierre peut le d&#142;part), Tesni&#143;re argue du fait que les
translatifs ont tendance &#136; &#144;tre analytique au d&#142;part et &#136; se morphologiser par la suite (c&#8217;est-&#136;-dire
&#136; devenir des morph&#143;mes flexionnels sur le mot qu&#8217;il translate)6 et que le lien entre le translatif
et le translat&#142; est particuli&#143;rement &#142;troit.
</p>
<p>La coordination est un autre cas qui pose probl&#143;me. Si l&#8217;on s&#8217;en tient &#136; consid&#142;rer que la
structure syntaxique doit &#144;tre un arbre de d&#142;pendance et que tout groupe doit avoir une t&#144;te, le
meilleur candidat est sans conteste le premier conjoint (Mel&#8217;&#139;uk 1988a). Certains proposent
&#142;galement de prendre la conjonction de coordination comme t&#144;te du groupe coordonn&#142;, mais il
s&#8217;agit alors d&#8217;un choix davantage guid&#142; par la s&#142;mantique, la conjonction de coordination
agissant comme op&#142;rateur s&#142;mantique prenant les conjoints comme arguments.7 Mais on peut
aussi consid&#142;rer comme le font la plupart des grammaires syntagmatiques avec t&#144;te (Jackendoff
1977, Pollard &amp; Sag 1994) que les conjoints sont des co-t&#144;tes. C&#8217;est &#142;galement ce que propose
Tesni&#143;re, bien que sa solution reste tr&#143;s informelle. Cf. Kahane 1997 pour voir comme la notion
d&#8217;arbre de d&#142;pendance peut-&#144;tre &#142;tendue pour prendre en compte cette hypoth&#143;se sans renoncer
au traitement par un arbre de d&#142;pendance dans les autres cas. Parmi les autres cas qui posent
probl&#143;me citons le cas du d&#142;terminant (Zwicky 1985 ; Abney 1987 o&#157; il est d&#142;fendu que le
groupe nominal a le d&#142;terminant pour t&#144;te) et du pronom relatif (Tesni&#143;re 1959:561, Kahane &amp;
Mel&#8217;&#139;uk 1999, Kahane 2000a).
Si comme on l&#8217;a vu, le recours &#136; un arbre de d&#142;pendance peut dans certains cas ne pas &#144;tre
enti&#143;rement satisfaisant, je voudrais insister sur le fait que m&#144;me dans ces cas-l&#136;, l&#8217;arbre de
d&#142;pendance reste un moyen d&#8217;encodage suffisant. Il est possible que des moyens d&#8217;encodage de
                                                
</p>
<p>6
 Tesni&#143;re distingue les translat&#142;s des d&#142;riv&#142;s : le translat&#142;, m&#144;me lorsque la translation est morphologique,
</p>
<p>continue &#136; se comporter vis-&#136;-vis de ses d&#142;pendants comme un &#142;l&#142;ment de sa partie du discours initiale : par
exemple, le verbe &#136; l&#8217;infinitif, c&#8217;est-&#136;-dire le verbe translat&#142; en nom, continue &#136; se comporter comme un verbe
vis-&#136;-vis de ses d&#142;pendants, &#136; la diff&#142;rence du d&#142;riv&#142; (voler un livre  est r&#142;pr&#142;hensible ; le vol de l ivre  est
r&#142;pr&#142;hensible).
</p>
<p>7
 De la m&#144;me fa&#141;on, un adjectif agit comme un pr&#142;dicat s&#142;mantique qui prend le nom qu&#8217;il modifie comme
</p>
<p>argument (le livre rouge : rouge(livre)) sans qu&#8217;on souhaite pour autant consid&#142;rer l&#8217;adjectif comme le
gouverneur syntaxique du nom.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Grammaires de d&#142;pendance formelles et th&#142;orie Sens-Texte
</p>
<p>l&#8217;organisation syntaxique plus puissants permettent des analyses plus &#142;l&#142;gantes, mais l&#8217;arbre de
d&#142;pendance conserve l&#8217;avantage de la simplicit&#142;. En plus, l&#8217;arbre de d&#142;pendance n&#8217;a pas, &#136; la
diff&#142;rence du r&#153;le donn&#142; &#136; l&#8217;arbre syntagmatique en grammaire g&#142;n&#142;rative, comme objectif
d&#8217;encoder toutes les informations pertinentes sur une phrase. Dans la plupart des grammaires de
d&#142;pendance et en particulier dans la th&#142;orie Sens-Texte, l&#8217;arbre de d&#142;pendance est avant tout une
repr&#142;sentation interm&#142;diaire entre la repr&#142;sentation s&#142;mantique et la repr&#142;sentation
morphologique (l&#136; o&#157; les mots sont form&#142;s et ordonn&#142;s). L&#8217;arbre de d&#142;pendance doit donc
contenir suffisamment d&#8217;information pour exprimer la relation avec la repr&#142;sentation
s&#142;mantique, notamment les possibilit&#142;s de redistribution ou de pronominalisation. De l&#8217;autre
cot&#142;, il doit &#142;galement contenir suffisamment d&#8217;informations pour exprimer les relations avec la
repr&#142;sentation morphologique, c&#8217;est-&#136;-dire les diff&#142;rentes possibilit&#142;s d&#8217;ordre, d&#8217;accord ou
d&#8217;assignation de cas. Ni plus, ni moins.
</p>
<p>2.2 Fonctions syntaxiques
Un arbre de d&#142;pendance ne suffit pas &#136; encoder l&#8217;organisation syntaxique des phrases sans un
&#142;tiquetage des d&#142;pendances par des fonctions syntaxiques. Les fonctions syntaxiques permettent
de distinguer les diff&#142;rents d&#142;pendants d&#8217;un m&#144;me mot, mais aussi de rapprocher deux
d&#142;pendants de deux mots diff&#142;rents qui pr&#142;sentent des comportements similaires vis-&#136;-vis de
diff&#142;rentes propri&#142;t&#142;s syntaxiques : placement, pronominalisation, r&#142;gime, accord,
redistribution, cooccurrence, &#201; La notion de fonction syntaxique a &#142;t&#142; &#142;labor&#142;e et utilis&#142;e
ind&#142;pendamment des grammaires de d&#142;pendance (cf. , par exemple, Jespersen 1924), m&#144;me si
la th&#142;orie de Tesni&#143;re a certainement marqu&#142; une &#142;tape fondamentale dans la compr&#142;hension de
cette notion. Dans le courant g&#142;n&#142;rativiste, on a &#142;vit&#142; le recours explicite &#136; un &#142;tiquetage
fonctionnel en tentant d&#8217;encoder les diff&#142;rences de comportement d&#8217;un d&#142;pendant d&#8217;un mot par
des diff&#142;rences de position dans l&#8217;arbre syntagmatique (par exemple le sujet est le GN sous S ou
Infl&#8217;, alors que l&#8217;objet direct est le GN sous GV). N&#142;anmoins, de nombreuses th&#142;ories issues
de la grammaire syntagmatique (notamment LFG et HPSG) ont r&#142;introduit explicitement la
notion de fonction syntaxique, notamment &#136; la suite des travaux de Comrie &amp; Keenan 1987 sur
la hi&#142;rarchie fonctionnelle.8 (Cf. Abeill&#142; 1996-97 pour un survol des diff&#142;rents arguments pour
l&#8217;usage des fonctions syntaxiques en grammaire syntagmatique.)
L&#8217;une des principales difficult&#142;s pour d&#142;cider combien de fonctions syntaxiques il est n&#142;cessaire
de consid&#142;rer est qu&#8217;on peut toujours attribuer une propri&#142;t&#142; particuli&#143;re &#136; la cat&#142;gorie du
d&#142;pendant ou du gouverneur (comme le font les grammaires syntagmatiques) plut&#153;t qu&#8217;&#136;
l&#8217;&#142;tiquette de la relation de d&#142;pendance entre eux. Quitte &#136; multiplier les cat&#142;gories syntaxiques,
il est formellement possible de limiter l&#8217;&#142;tiquetage des relations &#136; un simple num&#142;rotage (il faut
quand m&#144;me garder un minimum pour distinguer entre eux les diff&#142;rents compl&#142;ments du
verbe). Il semble donc difficile d&#8217;&#142;tablir des crit&#143;res exacts pour d&#142;cider si deux d&#142;pendances
doivent ou non correspondre &#136; la m&#144;me fonction et il est n&#142;cessaire de prendre en compte
l&#8217;&#142;conomie g&#142;n&#142;rale du syst&#143;me en cherchant &#136; limiter &#136; la fois le nombre de cat&#142;gories
syntaxiques et le nombre de fonctions syntaxiques et &#136; chercher la plus grande simplicit&#142; dans
les r&#143;gles grammaticales. On attribuera donc &#136; la cat&#142;gorie syntaxique les propri&#142;t&#142;s intrins&#143;ques
d&#8217;une lexie (c&#8217;est-&#136;-dire qui ne d&#142;pendent pas de la position syntaxique) et &#136; la fonction les
propri&#142;t&#142;s intrins&#143;ques d&#8217;une position syntaxique (c&#8217;est-&#136;-dire qui ne d&#142;pendent pas de la lexie
qui l&#8217;occupe). Autrement dit, on attribuera la m&#144;me cat&#142;gorie &#136; des lexies qui pr&#142;sentent un
comportement similaire dans toutes les positions syntaxiques et la m&#144;me fonction &#136; des
positions syntaxiques qui pr&#142;sentent des comportements similaires avec toutes les lexies.
</p>
<p>Pour caract&#142;riser l&#8217;ensemble des diff&#142;rentes fonctions syntaxiques, nous avons besoin de
crit&#143;res pour d&#142;cider 1) si deux d&#142;pendants d&#8217;un m&#144;me mot (dans deux phrases diff&#142;rentes)
                                                
</p>
<p>8
 Toute grammaire syntagmatique qui fait usage des fonctions syntaxiques d&#142;finit un arbre &#136; la Gladkij (chaque
</p>
<p>syntagme d&#142;pend d&#8217;un mot) et devient de fait une grammaire de d&#142;pendance.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Sylvain Kahane
</p>
<p>remplissent la m&#144;me fonction et 2) si deux d&#142;pendants de deux mots diff&#142;rents remplissent la
m&#144;me fonction.
</p>
<p>Pour le premier cas consid&#142;rons le paradigme suivant : Pierre lit le livre / Pierre le lit / le livre
que Pierre lit. On admet g&#142;n&#142;ralement que les syntagmes un livre, le et que9 sont des
r&#142;alisations du deuxi&#143;me argument s&#142;mantique du pr&#142;dicat lire ; plus pr&#142;cis&#142;ment, le et que sont
des formes pronominales de cet argument. De plus, ces syntagmes s&#8217;excluent mutuellement (*ce
que Pierre lit le livre ; *ce que Pierre le lit ; seul Pierre le lit le livre est possible, mais avec une
prosodie sur le livre tr&#143;s diff&#142;rente de Pierre lit le livre, qui laisse &#136; penser que le livre ne remplit
pas alors la m&#144;me fonction). Dans ce cas, on consid&#143;re que ces &#142;l&#142;ments remplissent tous la
m&#144;me fonction (&#136; savoir la fonction d&#8217;objet direct).
Pourtant, les compl&#142;ments un livre, le et que ne se positionnent pas de la m&#144;me fa&#141;on et les
pronoms, &#136; la diff&#142;rence des groupes nominaux, distinguent les cas (il/le/lui ; qui/que). Peut-
&#144;tre, peut-on distinguer fonction et relation syntaxique et dire que le clitique le remplit la
fonction d&#8217;objet direct, mais d&#142;pend de son gouverneur par une relation sp&#142;cifique (comme
objet-clitique) qui impose un placement particulier ainsi que l&#8217;assignation d&#8217;un cas.10 Dans ce
cas, l&#8217;arbre de d&#142;pendance sera &#142;tiquet&#142; par des relations syntaxiques et deux &#142;l&#142;ments
remplissant des fonctions syntaxiques similaires pourront d&#142;pendre de leur gouverneur par des
relations syntaxiques diff&#142;rentes. N&#142;anmoins, il ne semble pas n&#142;cessaire de leur attribuer des
relations syntaxiques diff&#142;rentes, car le comme que appartiennent &#136; des classes ferm&#142;es de mots
outils pour lesquels on peut donner facilement des r&#143;gles d&#8217;ordre sp&#142;cifique. Mais on peut
comprendre que certains pr&#142;f&#143;rent introduire des relations syntaxiques sp&#142;cifiques pour ces
&#142;l&#142;ments plut&#153;t que de devoir invoquer des propri&#142;t&#142;s cat&#142;gorielles de l&#8217;&#142;l&#142;ment d&#142;pendant dans
la r&#143;gle de placement de l&#8217;objet direct.
Consid&#142;rons un deuxi&#143;me paradigme : Pierre veut un bonbon / Pierre veut manger / Pierre
veut qu&#8217;on lui donne un bonbon. Encore une fois, ces diff&#142;rents compl&#142;ments r&#142;alisent
tous le deuxi&#143;me argument s&#142;mantique du verbe vouloir, s&#8217;excluent mutuellement et se
pronominalisent de la m&#144;me fa&#141;on (Pierre le veut ; Que veut Pierre ?), ce qui nous inciterait &#136;
leur attribuer la m&#144;me fonction syntaxique. N&#142;anmoins, la construction avec verbe infinitif (veut
manger) n&#142;cessite des sp&#142;cifications suppl&#142;mentaires, &#136; savoir qu&#8217;il s&#8217;agit d&#8217;une construction &#136;
verbe contr&#153;le, ou equi-construction, o&#157; le sujet du verbe vouloir co&#149;ncide avec le &#210;sujet&#211; de
l&#8217;infinitif. Ceci peut suffire &#136; vouloir introduire une relation particuli&#143;re, bien qu&#8217;il existe
d&#8217;autres fa&#141;ons d&#8217;encoder cette propri&#142;t&#142; (par exemple, en consid&#142;rant directement une relation
particuli&#143;re entre le verbe infinitif et le sujet  de vouloir).
Notons qu&#8217;il existe un autre crit&#143;re souvent invoqu&#142; pour d&#142;cider si deux d&#142;pendants d&#8217;un
m&#144;me mot remplissent la m&#144;me fonction : la coordination (Sag et al. 1985, Hudson 1988). On
peut d&#142;cider par exemple que deux syntagmes peuvent &#144;tre coordonn&#142;s seulement s&#8217;ils
remplissent la m&#144;me fonction (condition &#136; laquelle s&#8217;ajouteront d&#8217;autres conditions, notamment
sur l&#8217;identit&#142; cat&#142;gorielle).11 Dans notre dernier exemple, le fait que la coordination soit possible
(Pierre veut un bonbon et manger) nous incitera encore davantage &#136; utiliser la m&#144;me
fonction.
</p>
<p>                                                
</p>
<p>9
 On suppose ici que que est trait&#142; comme un d&#142;pendant du verbe, ce qui n&#8217;est pas n&#142;cessairement justifi&#142;
</p>
<p>(Kahane 2000b).
10
</p>
<p> Pour assurer la mont&#142;e du clitique dans, par exemple, Pierre le fait lire &#136; Marie, on peut m&#144;me consid&#142;rer que
le clitique d&#142;pend de faire, alors qu&#8217;il remplit une fonction vis-&#136;-vis de lire.
</p>
<p>11
 Par exemple, la coordination des adjectifs &#142;pith&#143;tes ob&#142;it &#136; des conditions complexes et l&#8217;it&#142;ration de la
</p>
<p>relation d&#8217;&#142;pith&#143;te est souvent pr&#142;f&#142;rable &#136; la coordination : des plats fran&#141;ais exquis, ?*des plats fran&#141;ais et
exquis, des plats fran&#141;ais et n&#142;anmoins exquis.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Grammaires de d&#142;pendance formelles et th&#142;orie Sens-Texte
</p>
<p>Consid&#142;rons maintenant le deuxi&#143;me cas : comment d&#142;cider si des d&#142;pendants de deux mots
diff&#142;rents doivent recevoir la m&#144;me fonction. On consid&#143;re que les d&#142;pendants de deux mots
diff&#142;rents remplissent la m&#144;me fonction si et seulement si ils acceptent les m&#144;mes
redistributions, les m&#144;mes pronominalisations et les m&#144;mes lin&#142;arisations (Iordanskaja &amp;
Mel&#8217;&#139;uk 2000).
Consid&#142;rons un premier exemple : Pierre compte sur Marie / Pierre pose le livre sur la
table&#730;/ le livre est sur la table. Les d&#142;pendants sur Marie et sur la table remplissent-ils la
m&#144;me fonction ? Ces d&#142;pendants se distinguent nettement par leurs possibilit&#142;s de
pronominalisation : seul le deuxi&#143;me accepte la cliticisation en y (*Pierre y compte ; Pierre y
pose le livre ; le livre y est) et les interrogatives et les relatives en o&#157; (*O&#157; Pierre compte-t-il ? ;
O&#157; Pierre pose-t-il le livre ? ; O&#157; le livre est-il ?). On distinguera donc deux fonctions
syntaxiques diff&#142;rentes, compl&#142;ment oblique pour compter et compl&#142;ment locatif pour poser et
&#144;tre (qui n&#8217;est pas le m&#144;me &#144;tre que la copule).
Deuxi&#143;me exemple : Pierre compte sur Marie / Pierre est aid&#142; par Marie. Les d&#142;pendants sur
Marie et par Marie remplissent-ils la m&#144;me fonction ? Aucune redistribution de ces d&#142;pendants
n&#8217;est possible. On peut objecter que Pierre est aid&#142; par Marie est le r&#142;sultat de la passivation de
Marie aide Pierre, mais la passivation est en quelque sorte orient&#142;e et Marie aide Pierre n&#8217;est pas
le r&#142;sultat d&#8217;une redistribution de Pierre est aid&#142; par Marie. Les possibilit&#142;s de pronominalisation
sont les m&#144;mes : pas de cliticisation, m&#144;me pronominalisation pour les interrogatives et les
relatives. On pourrait objecter que sur N accepte la pronominalisation en dessus, mais celle-ci
est tr&#143;s r&#142;guli&#143;re et doit &#144;tre imput&#142;e &#136; la pr&#142;position sur (de m&#144;me qu&#8217;on aura dessous pour
sous ou dedans pour dans) plut&#153;t qu&#8217;&#136; la fonction de sur N . Les possibilit&#142;s de placement sont
&#142;galement les m&#144;mes. En cons&#142;quence on peut attribuer &#136; ces deux relations la m&#144;me &#142;tiquette,
par exemple compl&#142;ment oblique. Cela n&#8217;emp&#144;che pas de dire que par Marie dans Pierre est aid&#142;
par Marie est un compl&#142;ment d&#8217;agent ; cela ne signifie pas que ce groupe remplit la fonction
syntaxique de compl&#142;ment d&#8217;agent qui n&#8217;a pas de raison d&#8217;exister en tant que telle, mais
simplement que ce groupe est le r&#142;sultat d&#8217;une r&#142;alisation particuli&#143;re de l&#8217;&#210;agent&#211; suite &#136; une
redistribution.
</p>
<p>Un dernier exemple : Pierre mange un bonbon / Pierre veut un bonbon. Les deux
d&#142;pendants un  bonbon remplissent-ils la m&#144;me fonction ? Les deux d&#142;pendants partagent les
m&#144;mes propri&#142;t&#142;s &#136; une exception pr&#143;s, la passivation (le bonbon est mang&#142; par Pierre ; ?*le
bonbon est voulu par Pierre). Deux solutions sont alors possibles : 1) consid&#142;rer qu&#8217;il s&#8217;agit de
la m&#144;me fonction dans les deux cas (objet direct) et faire assumer la diff&#142;rence &#136; la cat&#142;gorie du
verbe qui gouverne cette position ou 2) consid&#142;rer qu&#8217;il s&#8217;agit de deux fonctions diff&#142;rentes.
Etant donn&#142;e la grande similitude comportement, la premi&#143;re solution est plus &#142;conomique.
</p>
<p>En conclusion, comme on l&#8217;a vu, le choix d&#8217;un ensemble de fonctions syntaxiques est
directement li&#142; &#136; la fa&#141;on dont seront &#142;crites les r&#143;gles de pronominalisation, lin&#142;arisation,
redistribution ou coordination.
</p>
<p>2.3 Premi&#143;res grammaires de d&#142;pendance
Dans cette section, nous allons pr&#142;senter les premi&#143;res grammaires de d&#142;pendance (Hays 1960,
Gaifman 1965), qui ont pour particularit&#142; de ne traiter que des structures projectives.
Rappelons que l&#213;un des points remarquables de la th&#142;orie de Tesni&#143;re est d&#213;avoir dissoci&#142; la
repr&#142;sentation syntaxique de l&#213;ordre lin&#142;aire des mots&#730;: les arbres de d&#142;pendance de Tesni&#143;re ne
sont pas ordonn&#142;s. L&#213;objet de la syntaxe est alors d&#213;exprimer le lien entre l&#213;ordre des mots et
leurs relations de d&#142;pendance.
</p>
<p>L&#213;une des principales propri&#142;t&#142;s de compatibilit&#142; entre un arbre de d&#142;pendance et un ordre
lin&#142;aire est la projectivit&#142; (Lecerf 1961, Iordanskaja 1963, Gladkij 1966). Un arbre de
d&#142;pendance assorti d&#8217;un ordre lin&#142;aire sur les n&#711;uds est dit projectif si et seulement si, en</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Sylvain Kahane
</p>
<p>pla&#141;ant les n&#711;uds sur une ligne droite et tous les arcs dans le m&#144;me demi-plan, on peut assurer
que 1) deux arcs ne se coupent jamais et que 2) aucun arc ne couvre la racine de l&#213;arbre (Figure
4)12.
</p>
<p>Figure 3 : Les cas de non projectivit&#142;
</p>
<p>La projectivit&#142; est &#142;quivalente au fait que la projection de tout n&#711;ud x de l&#8217;arbre (c&#213;est-&#136;-dire
l&#8217;ensemble des n&#711;uds domin&#142;s par x, x compris) forme un segment continu de la phrase
(Lecerf 1961, Gladkij 1966). Autrement dit, la projectivit&#142; dans le cadre des grammaires de
d&#142;pendance correspond &#136; la continuit&#142; des constituants dans le cadre des grammaires
syntagmatiques. La litt&#142;rature sur les structures de d&#142;pendance non projectives est d&#213;ailleurs
toute aussi abondante que la litt&#142;rature sur les constituants discontinus (toutes proportions
gard&#142;es). Nous y reviendrons &#136; la fin de cette section.
La projectivit&#142; pr&#142;sente un int&#142;r&#144;t imm&#142;diat : il suffit, pour ordonner un arbre projectif, de
sp&#142;cifier la position de chaque n&#711;ud par rapport &#136; son gouverneur, ainsi que vis-&#136;-vis de ses
fr&#143;res (Figure 4). Nous allons voir comment cette propri&#142;t&#142; est exploit&#142;e par les premi&#143;res
grammaires de d&#142;pendance.
</p>
<p>Figure 4 : Un exemple d&#213;arbre de d&#142;pendance projectif
</p>
<p>La premi&#143;re grammaire de d&#142;pendance formelle est due &#136; Hays (1960). Une grammaire de Hays
est constitu&#142;e d&#8217;un vocabulaire V, d&#8217;une ensemble de cat&#142;gories lexicales C, d&#8217;un lexique
associant &#136; chaque &#142;l&#142;ment du vocabulaire une cat&#142;gorie et d&#8217;un ensemble de r&#143;gles de la forme
X(Y1Y2&#201;Yk*Yk+1&#201;Yn) o&#157; X et les Yi sont des cat&#142;gories lexicales. La r&#143;gle
X(Y1Y2&#201;Yk*Yk+1&#201;Yn) indique qu&#8217;un n&#711;ud de cat&#142;gorie X peut poss&#142;der n d&#142;pendants de
cat&#142;gories respectives Y1, Y2, &#201;, Yn plac&#142;s dans l&#8217;ordre lin&#142;aire Y1Y2&#201;Yk*Yk+1&#201;Yn (o&#157; *
indique la place de X par rapport &#136; ses d&#142;pendants). Une r&#143;gle de la forme X(*) indique qu&#8217;un
n&#711;ud de cat&#142;gorie X peut &#144;tre une feuille de l&#8217;arbre de d&#142;pendance. Une telle grammaire permet
de g&#142;n&#142;rer des arbres de d&#142;pendance projectifs dont les n&#711;uds sont &#142;tiquet&#142;s par un mot de V et
sa cat&#142;gorie syntaxique dans C ou, ce qui revient au m&#144;me, &#136; g&#142;n&#142;rer des suites de mots de V o&#157;
chaque mot correspond &#136; un n&#711;ud d&#8217;un arbre de d&#142;pendance &#142;tiquet&#142; par une cat&#142;gorie
syntaxique dans C. Comme on le voit les grammaires de Hays n&#8217;ont pas recours aux fonctions
syntaxiques et elles g&#142;n&#143;rent simultan&#142;ment des arbres de d&#142;pendances et des suites de mots.
Comme l&#8217;a remarqu&#142; Gaifman (1965), les grammaires de Hays peuvent &#144;tre simul&#142;es par des
grammaires cat&#142;gorielles &#136; la Ajdukiewicz-Bar-Hillel (Ajdukiewicz 1935 ; Bar-Hillel 1953), la
r&#143;gle X(Y1Y2&#201;Yk*Yk+1&#201;Yn) correspondant simplement &#136; la cat&#142;gorie complexe
Yk&#201;Y1\X/Yn&#201;Yk+1 (l&#8217;inversion dans l&#8217;ordre des cat&#142;gories est due au fait que la cat&#142;gorie la
plus &#136; l&#8217;ext&#142;rieur sera la premi&#143;re a &#144;tre r&#142;duite et donnera donc le d&#142;pendant le plus proche de
X). Si les grammaires cat&#142;gorielles &#136; la Ajdukiewicz-Bar-Hillel ne sont pas consid&#142;r&#142;es comme
                                                
</p>
<p>12
 Suivant Hudson 2000, nous repr&#142;senterons la racine de l&#213;arbre avec une d&#142;pendance gouverneur verticale
</p>
<p>(potentiellement infinie). La condition (2) se ram&#143;ne alors &#136; un cas particulier de la condition (1).
</p>
<p>le    petit gar&#141;on parle   &#136;     Zo&#142;
</p>
<p>&#220;
</p>
<p>&#221;&#220;
</p>
<p>&#221;
</p>
<p>&#221;
</p>
<p>(1) (2)
</p>
<p>* *</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Grammaires de d&#142;pendance formelles et th&#142;orie Sens-Texte
</p>
<p>les premi&#143;res grammaires de d&#142;pendance, c&#8217;est que les auteurs n&#8217;ont jamais mis leur formalisme
en relation avec la construction d&#8217;arbres de d&#142;pendance (ni d&#8217;arbres syntagmatiques d&#8217;ailleurs).
De plus, une cat&#142;gorie complexe comme la cat&#142;gorie N/N donn&#142;e &#136; un adjectif ant&#142;pos&#142; ne
s&#8217;interpr&#143;te pas par &#210; un adjectif est un N dont d&#142;pend un N &#136; droite&#211;, mais comme &#210;un adjectif
est un mot qui combin&#142; &#136; un N &#136; sa droite donne un syntagme de m&#144;me nature&#211; (voir n&#142;anmoins
Lecomte 1992 pour une interpr&#142;tation des grammaires cat&#142;gorielles en termes de graphes et
R&#142;tor&#142; 1996 pour le lien entre grammaire logique et r&#142;seaux de preuve, eux-m&#144;mes
interpr&#142;tables en termes de graphes de d&#142;pendance). Gaifman (1965) a &#142;galement not&#142; que les
grammaires de Hays sont trivialement simulables par des grammaires de r&#142;&#142;criture hors-
contextes o&#157; la r&#143;gle X(Y1Y2&#201;Yk*Yk+1&#201;Yn) correspond &#136; un famille de r&#143;gle de r&#142;&#142;criture
X&#8594;Y1Y2&#201;YkaYk+1&#201;Yn pour tout mot a de cat&#142;gorie X. Les grammaires de Hays, les
grammaires d&#8217;Ajdukiewicz-Bar-Hillel et leurs &#142;quivalents en grammaire de r&#142;&#142;criture se
distinguent par la fa&#141;on dont le vocabulaire pointe sur les r&#143;gles syntaxiques.
</p>
<p>L&#8217;article de Gaifman (1965) contient &#142;galement deux r&#142;sultats remarquables&#730;: l&#8217;&#142;quivalence
faible entre les grammaires hors-contexte et les grammaires de d&#142;pendance de Hays13 et un
th&#142;or&#143;me d&#8217;&#142;quivalence forte entre une large classe de grammaires hors-contextes et les
grammaires de d&#142;pendance de Hays (cf. &#142;galement Dikovsky &amp; Modina 2000).
D&#8217;un point de vue linguistique, les grammaires de Hays pr&#142;sentent plusieurs faiblesses : elles ne
s&#142;parent pas les r&#143;gles de bonne formation des arbres de d&#142;pendance des r&#143;gles de lin&#142;arisation,
c&#8217;est-&#136;-dire des r&#143;gles de mise en correspondance d&#8217;un arbre de d&#142;pendance et d&#8217;un ordre
lin&#142;aire. De plus, concernant la bonne formation des arbres de d&#142;pendance, elles ne distinguent
pas la sous-cat&#142;gorisation et la modification. Ceci peut &#144;tre r&#142;solu tr&#143;s simplement en divisant
une r&#143;gle de la forme X(Y1Y2&#201;Yk*Yk+1&#201;Yn) en trois familles de r&#143;gles : une r&#143;gle indiquant
quels sont les cat&#142;gories des s de X, des r&#143;gles indiquant quels sont les cat&#142;gories des
modifieurs potentiels de X et une ou des r&#143;gles indiquant comment les d&#142;pendants de X se
placent les uns par rapport aux autres. On aura alors avantage &#136; &#142;tiqueter les d&#142;pendances par
des fonctions et &#136; mentionner les fonctions plut&#153;t que les cat&#142;gories dans les r&#143;gles de
lin&#142;arisation. Nous verrons dans la suite comment ces diff&#142;rentes r&#143;gles se pr&#142;sentent dans le
cadre de la th&#142;orie Sens-Texte.
</p>
<p>Enfin, les grammaires de Hays ne pr&#142;voient pas le traitement de structures non projectives. Pour
traiter les cas non projectifs, diff&#142;rentes extensions sont possibles : on peut introduire des traits
Slash dans les cat&#142;gories comme cela est fait en GPSG et HPSG (Pollard &amp; Sag 1994 ; cf.
Lombardo &amp; Lesmo 2000 pour une adaptation du proc&#142;d&#142; aux grammaires de d&#142;pendance),
proposer des r&#143;gles sp&#142;cifiques qui permettent de d&#142;placer des &#142;l&#142;ments dans l&#8217;arbre de
d&#142;pendance pour se ramener &#136; un arbre projectif (Hudson 2000, Kahane et al. 1998) ou utiliser
une structure plus complexe o&#157; est v&#142;rifi&#142; un &#142;quivalent de la projectivit&#142; (Kahane 2000a).
D&#8217;autres m&#142;thodes consistent &#136; ne pas mettre en relation l&#8217;arbre de d&#142;pendance directement en
relation avec l&#8217;ordre lin&#142;aire, mais &#136; utiliser une structure syntagmatique interm&#142;diaire comme
cela est fait en LFG (Bresnan 1982, Bresnan et al. 1982 ; cf. Gerdes &amp; Kahane 2001 ou
Duchier &amp; Debusman 2001 pour des m&#142;thodes &#142;quivalentes dans le cadre des grammaires de
d&#142;pendance).
</p>
<p>                                                
</p>
<p>13
 Plus pr&#142;cis&#142;ment, Gaifman 1965 montre que toute grammaire hors contexte est simulable par une grammaire
</p>
<p>dont les r&#143;gles sont de la forme X&#8594;aY1Y2, X&#8594;aY1 et X&#8594;a, ce qui est un th&#142;or&#143;me bien connu sous le nom
de th&#142;or&#143;me de mise en forme normale de Greibach, th&#142;or&#143;me attribu&#142; &#136; Greibach (1965) par qui ce r&#142;sultat a
&#142;t&#142; d&#142;montr&#142; ind&#142;pendamment.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Sylvain Kahane
</p>
<p>3 Pr&#142;sentation de la th&#142;orie Sens-Texte
La th&#142;orie Sens-Texte [TST] est n&#142;e il y a 35 ans des premiers travaux en traduction
automatique en URSS (&#235;olkovskij &amp; Mel&#8217;&#139;uk 1965, 1967) et s&#213;est depuis d&#142;velopp&#142;e autour
d&#213;Igor Mel&#8217;&#139;uk (Mel&#8217;&#139;uk 1974, 1988a, 1997). Cf. &#142;galement, pour d&#8217;autres pr&#142;sentations,
Mili&#141;evi&#141; 2001 ou Weiss 1999. La TST est int&#142;ressante &#136; &#142;tudier dans le cadre d&#213;une
pr&#142;sentation des grammaires de d&#142;pendance, non seulement parce qu&#213;il s&#213;agit d&#213;une des th&#142;ories
majeures utilisant des arbres de d&#142;pendance comme repr&#142;sentations syntaxiques, mais parce que
les postulats m&#144;me de la th&#142;orie conduisent naturellement &#136; consid&#142;rer une telle structure, o&#157; le
mot joue un r&#153;le central et o&#157; la structure syntaxique doit rendre compte des relations entre les
mots. L&#213;approche de la TST se distingue des grammaires syntagmatiques &#136; plus d&#213;un titre&#730;:
1) en privil&#142;giant la s&#142;mantique sur la syntaxe ;
2) en privil&#142;giant le sens de la synth&#143;se sur celui de l&#213;analyse pour la description ;
3) en donnant une grande importance au lexique (avec notamment la consid&#142;ration de la notion
</p>
<p>de fonction lexicale qui permet de d&#142;crire les relations lexicales d&#142;rivationnelles et
collocationnelles) ;
</p>
<p>4) en pr&#142;f&#142;rant une repr&#142;sentation syntaxique bas&#142;e sur un arbre de d&#142;pendance plut&#153;t qu&#213;un
arbre syntagmatique (ce qui est, en quelque sorte, une cons&#142;quence naturelle des points
pr&#142;c&#142;dents).
</p>
<p>Dans cette section, nous pr&#142;senterons les postulats de base de la TST (Section 3.1), les
diff&#142;rentes repr&#142;sentations d&#8217;une phrase consid&#142;r&#142;es par la TST (Section 3.2) et les diff&#142;rentes
r&#143;gles d&#8217;un mod&#143;le Sens-Texte (Section 3.3). Dans la Section 4, nous pr&#142;senterons une
grammaire d&#8217;unification bas&#142;e sur la TST.
</p>
<p>3.1 Les postulats de base de la th&#142;orie Sens-Texte
La th&#142;orie Sens-Texte [TST] repose sur les trois postulats suivants.
Postulat 1. Une langue est (consid&#142;r&#142;e comme) une correspondance multivoque14 entre des
sens et des textes15.
</p>
<p>Postulat 2. Une correspondance Sens-Texte est d&#142;crite par un syst&#143;me formel simulant
l&#213;activit&#142; linguistique d&#213;un sujet parlant.
Postulat 3. La correspondance Sens-Texte  est modulaire et pr&#142;sente au moins deux niveaux
de repr&#142;sentation interm&#142;diaires&#730;: le niveau syntaxique (structure des phrases) et le niveau
morphologique (structure des mots).
Commentaires sur les postulats.
</p>
<p>1) Le premier postulat de la TST signifie que la description d&#8217;une langue naturelle L consiste en
la description de la correspondance entre l&#8217;ensemble des sens de L et l&#8217;ensemble des textes de L.
On peut comparer ce point de vue &#136; celui de Chomsky 1957, dont l&#8217;influence a &#142;t&#142; primordiale :
la description d&#8217;une langue L consiste en un syst&#143;me formel d&#142;rivant l&#8217;ensemble des phrases
(acceptables) de L. Pendant longtemps, ce point de vue a eu une interpr&#142;tation plut&#153;t restrictive,
                                                
</p>
<p>14
 Plusieurs sens peuvent correspondre au m&#144;me texte (homonymie) et plusieurs textes peuvent correspondre au
</p>
<p>m&#144;me sens (synonymie).
15
</p>
<p> Texte renvoie &#136; n&#8217;importe quel segment de parole, de n&#8217;importe quelle longueur, et son pourrait &#144;tre un
meilleur terme.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Grammaires de d&#142;pendance formelles et th&#142;orie Sens-Texte
</p>
<p>une phrase &#142;tant comprise comme une suite de caract&#143;res16 &#209; c&#8217;est-&#136;-dire un texte dans la
terminologie de la TST &#209; ou au mieux comme une structure syntagmatique. N&#142;anmoins, le
postulat de Chomsky est formellement &#142;quivalent au premier postulat de la TST d&#143;s qu&#8217;on
entend par phrase un signe au sens saussurien avec un signifi&#142; (le texte) et un signifiant (le
sens). D&#8217;un point de vue math&#142;matique, il est en effet &#142;quivalent de d&#142;finir une correspondance
entre l&#8217;ensemble des sens et l&#8217;ensemble des textes ou de d&#142;finir l&#8217;ensemble des couples form&#142;s
d&#8217;un sens et d&#8217;un texte en correspondance, un tel couple repr&#142;sentant une phrase17 (Kahane
2000b, 2001).
2) Le deuxi&#143;me postulat met l&#8217;accent sur le fait qu&#8217;une langue naturelle doit &#144;tre d&#142;crite comme
une correspondance. Un locuteur parle. Un mod&#143;le Sens-Texte (= le mod&#143;le d&#8217;une langue
donn&#142;e dans le cadre de la TST) doit mod&#142;liser l&#8217;activit&#142; d&#8217;un locuteur, c&#8217;est-&#136;-dire mod&#142;liser
comment un locuteur transforme ce qu&#8217;il veut dire (un sens) en ce qu&#8217;il dit (un texte). C&#8217;est
l&#8217;une des principales particularit&#142;s de la TST de dire qu&#8217;une langue doit &#144;tre d&#142;crite comme une
correspondance (Sens-Texte) et, qui plus est, que la direction du sens au texte doit &#144;tre
privil&#142;gi&#142;e sur la direction du texte au sens.
</p>
<p>3) Le troisi&#143;me postulat de la TST appelle plusieurs commentaires. La plupart des th&#142;ories
linguistiques consid&#143;rent des niveaux de repr&#142;sentation syntaxique et morphologique. La
particularit&#142; de la TST est de consid&#142;rer que ces niveaux sont des niveaux interm&#142;diaires entre le
niveau s&#142;mantique (le sens) et le niveau phonologique (le texte). En cons&#142;quence, la
correspondance entre les sens et les textes sera enti&#143;rement modulaire : une correspondance
entre les niveaux s&#142;mantique et syntaxique, une correspondance entre les niveaux syntaxique et
morphologique et une correspondance entre les niveaux morphologique et phonologique. (En
fait, la TST consid&#143;re non pas deux, mais cinq niveaux interm&#142;diaires, ce qui ne change rien &#136;
notre discussion.)
Le r&#142;sultat est que le module syntaxique, qui assure la correspondance entre les niveaux
syntaxique et morphologique, ne fait qu&#8217;associer des repr&#142;sentations syntaxiques avec des
repr&#142;sentations morphologiques. Il n&#8217;a pas pour objet, comme cela l&#8217;est pour une grammaire
g&#142;n&#142;rative, de donner une caract&#142;risation compl&#143;te des repr&#142;sentations qu&#8217;il manipule. Dans le
sens de la synth&#143;se, le module syntaxique prend en entr&#142;e des repr&#142;sentations syntaxiques qui
ont &#142;t&#142; synth&#142;tis&#142;es par le module s&#142;mantique &#136; partir de repr&#142;sentations s&#142;mantiques bien
form&#142;es et qui repr&#142;sentent des sens r&#142;els. En cons&#142;quence, une repr&#142;sentation syntaxique est
caract&#142;ris&#142;e par l&#8217;ensemble des modules, par le fait qu&#8217;elle est un interm&#142;diaire possible entre
une repr&#142;sentation s&#142;mantique bien form&#142;e et une repr&#142;sentation phonologique correspondante.
En conclusion, la TST ne donne aucune primaut&#142; &#136; la syntaxe et la TST n&#8217;a pas pour objectif de
donner une caract&#142;risation explicite des repr&#142;sentations syntaxiques bien form&#142;es.
</p>
<p>Je pense que, maintenant, 35 ans apr&#143;s leur premi&#143;re formulation, les postulats de la TST,
m&#144;me s&#8217;ils peuvent appara&#148;tre avec des formulations diff&#142;rentes, sont plus ou moins accept&#142;s
par l&#8217;ensemble de la communaut&#142; scientifique. Par exemple, j&#8217;aimerais citer les toutes premi&#143;res
phrase d&#8217;une monographie consacr&#142;e au Programme Minimaliste, la plus r&#142;cente des th&#142;ories
chomskienne (Brody 1997) : &#210;It is a truism that grammar relates sound and meaning. Theories
that account for this relationship with reasonable success postulate representational levels
corresponding to sound and meaning and assume that the relationship is mediated through
complex representations that are composed of smaller units.&#211; Le principal point qui semble ne
                                                
</p>
<p>16
 Le meilleur exemple de cette interpr&#142;tation restrictive du postulat de Chomsky est la d&#142;finition du terme
</p>
<p>langage formel comme une suite de caract&#143;res. Un langage formel, pris dans ce sens, ne peut jamais mod&#142;liser
l&#8217;essence d&#8217;une langue naturelle. En aucun cas, le fait de conna&#148;tre l&#8217;ensemble des suites de caract&#143;res
acceptables d&#8217;une langue ne peut &#144;tre consid&#142;r&#142; comme la connaissance d&#8217;une langue ; il faut &#142;videmment &#144;tre
capable d&#8217;associer ces suites &#136; leur sens.
</p>
<p>17
 Nous laissons de c&#153;t&#142; le fait que la description d&#8217;un langage ne se r&#142;duit pas &#136; la description de phrases isol&#142;es.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Sylvain Kahane
</p>
<p>pas &#144;tre pris en consid&#142;ration par la plupart des descriptions formelles contemporaines des
langues naturelles est le fait qu&#8217;une langue, si elle repr&#142;sente une correspondance entre des sens
et des textes, doit &#144;tre d&#142;crite par des r&#143;gles de correspondance.
</p>
<p>3.2 Niveaux de repr&#142;sentation
La TST s&#142;pare clairement les diff&#142;rents niveaux de repr&#142;sentation. Les repr&#142;sentations des
diff&#142;rents niveaux ont des organisations structurelles diff&#142;rentes : les repr&#142;sentations
s&#142;mantiques sont des graphes (de relations pr&#142;dicat-argument), les repr&#142;sentations syntaxiques
sont des arbres de d&#142;pendance (non ordonn&#142;s) et les repr&#142;sentations morphologiques sont des
suites. Dans l&#8217;approche Sens-Texte, tout ce qui peut &#144;tre diff&#142;renci&#142; doit &#144;tre diff&#142;renci&#142;. Et des
objets avec des organisations diff&#142;rentes doivent &#144;tre repr&#142;sent&#142;s avec des moyens diff&#142;rents.
De plus, la TST donne une grande importance &#136; la g&#142;om&#142;trie des repr&#142;sentations. Le fait que les
humains communiquent par la voix entra&#148;ne que les productions linguistiques sont
irr&#142;m&#142;diablement lin&#142;aires (m&#144;me si &#136; la suite des phon&#143;mes se superpose la prosodie et si des
gestes peuvent accompagner la parole). Par contre, tout laisse &#136; penser que, dans notre cerveau
tridimensionnel, le sens poss&#143;de une structure multidimensionnelle. Le passage du sens au texte
comprendrait alors, du point de vue de l&#8217;organisation structurelle, deux &#142;tapes essentielles : la
hi&#142;rarchisation, c&#8217;est-&#136;-dire le passage d&#8217;un sens multidimensionnel &#136; une structure syntaxique
hi&#142;rarchique (= bidimensionnelle), et la lin&#142;arisation, c&#8217;est-&#136;-dire le passage de cette structure
hi&#142;rarchique &#136; une structure lin&#142;aire (= unidimensionnelle).
</p>
<p>3.2.1  Repr&#142;sentation s&#142;mantique
</p>
<p>Le sens est d&#142;fini, dans le cadre de la TST, comme un invariant de paraphrase, c&#213;est-&#136;-dire
comme ce qui est commun &#136; toutes les phrases qui ont le m&#144;me sens. Ceci fait automatiquement
de la TST est un mod&#143;le de la paraphrase (Mel&#8217;&#139;uk 1988b) et, par cons&#142;quent, un outil adapt&#142; &#136;
la traduction automatique (les deux sont intimement li&#142;es, la paraphrase &#142;tant de la traduction
intralangue).
Le c&#711;ur de la repr&#142;sentation18 s&#142;mantique est un graphe dont les n&#711;uds sont &#142;tiquet&#142;s par des
s&#142;mant&#143;mes. Une repr&#142;sentation s&#142;mantique est un objet purement linguistique sp&#142;cifique &#136; une
langue. Un s&#142;mant&#143;me lexical d&#8217;une langue L est le sens d&#8217;une lexie19 de L dont le signifiant
                                                
</p>
<p>18
 Le terme de repr&#142;sentation, utilis&#142; par Mel&#8217;&#139;uk lui-m&#144;me, est en fait un peu contradictoire avec le point de vue
</p>
<p>de la TST. En un sens, la repr&#142;sentation s&#142;mantique ne repr&#142;sente pas le sens d&#8217;un texte, mais c&#8217;est plut&#153;t les
textes qui expriment des repr&#142;sentations s&#142;mantiques. Mel&#8217;&#139;uk (2001:15) dit d&#8217;ailleurs &#136; ce propos : &#210;During
the process of sentence construction (= synthesis), lexical and syntactic choices carried out by the Speaker
very often lead to the modification of the starting meaning, i.e. of the initial semantic representation, making
it more precise and specific: the lexical units bring with them additional nuances of meaning that have not
been present in the initial semantic representation. The MTT tries to model this phenomenon; as a result,
quite often the following situation obtains:  Suppose that the synthesis starts with the representation &#212;&#963;&#213; and
produces sentences &#212;S1&#213;, &#212;S2&#213;, &#201;, &#212;Sn&#213;; the sentences having as their common source the semantic
representation &#212;&#963;&#213; are considered to be synonymous. Now if we analyze these sentences semantically, the
semantic &#212;S1&#213;, &#212;S2&#213;, &#201;, &#212;Sn&#213; obtained from this process may well be different from each other and from the
initial semantic representation &#212;&#963;&#213; ! [...] The initial semantic representation is taken to be rather
approximate&#209;it need not necessarily fully specify the meaning of the sentences that can be obtained from it.
The meaning can become more precise&#209;or less precise&#209;in the course of its lexicalization and
syntacticization.&#211;
</p>
<p>19
 Un vocable est ensemble de lexies correspondant aux diff&#142;rentes acceptions d&#8217;un m&#144;me mot. En toute rigueur,
</p>
<p>le nom d&#8217;une lexie doit &#144;tre accompagn&#142;, comme dans le dictionnaire, d&#8217;un num&#142;ro qui la distingue des autres
lexies du vocable.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Grammaires de d&#142;pendance formelles et th&#142;orie Sens-Texte
</p>
<p>peut-&#144;tre un mot ou une configuration de mots formant une locution. Par exemple, &#212;cheval&#213;,
&#212;pomme de terre&#213;, &#212;prendre le taureau par les cornes&#213; sont des s&#142;mant&#143;mes du fran&#141;ais. Des
lexies de parties du discours diff&#142;rentes peuvent avoir le m&#144;me s&#142;mant&#143;me ; ainsi, &#212;partir&#213; =
&#212;d&#142;part&#213; (&#212;j&#8217;attends ton d&#142;part&#213; = &#212;j&#8217;attends que tu partes&#213;) ou &#212;durer&#213; = &#212;pendant&#213; (&#212;Ta sieste a
dur&#142; 2 heures&#213; = &#212;Tu as fais la sieste pendant 2 heures&#213;)20. Il existe aussi des s&#142;mant&#143;mes
grammaticaux correspondant au sens des morph&#143;mes flexionnels (ou de configurations
contenant des morph&#143;mes flexionnels, comme le pass&#142; compos&#142;) : par exemple, &#212;singulier&#213;,
&#212;d&#142;fini&#213;, &#212;pr&#142;sent&#213; ou &#212;pass&#142; compos&#142;&#213; sont des s&#142;mant&#143;mes grammaticaux.21
</p>
<p>Un s&#142;mant&#143;me agit comme un pr&#142;dicat et est li&#142; &#136; ses arguments par des arcs pointant sur eux.
Les diff&#142;rents arcs &#142;mergeant d&#8217;un s&#142;mant&#143;me sont num&#142;rot&#142;s de 1 &#136; n, en suivant l&#8217;ordre
d&#8217;oblicit&#142; croissant des arguments. Un arc repr&#142;sente une relation pr&#142;dicat-argument et est
appel&#142;e une d&#142;pendance s&#142;mantique. Les d&#142;pendances s&#142;mantiques doivent &#144;tre distingu&#142;es des
d&#142;pendances syntaxiques. Comme l&#8217;a not&#142; Tesni&#143;re lui-m&#144;me (1959:42), dans la plupart des
cas, quand un mot B d&#142;pend syntaxiquement d&#8217;un mot A, il y a une d&#142;pendance s&#142;mantique
entre &#212;A&#213; et &#212;B&#213;. Mais ce que n&#8217;avait pas vu Tesni&#143;re (et qui est probablement une d&#142;couverte
attribuable &#136; &#235;olkovskij &amp; Mel&#8217;&#139;uk 1965), c&#8217;est que la d&#142;pendance s&#142;mantique peut &#144;tre orient&#142;e
de &#212;A&#213; et &#212;B&#213; comme de &#212;B&#213; vers &#212;A&#213;. Par exemple, dans une petite rivi&#143;re, petite d&#142;pend
syntaxiquement de rivi&#143;re, mais, parce que la petitesse est une propri&#142;t&#142; de la rivi&#143;re, &#212;rivi&#143;re&#213;
est un argument du pr&#142;dicat &#212;petit&#213;. Par contre, dans la rivi&#143;re coule, rivi&#143;re d&#142;pend
syntaxiquement de coule et, parce que l&#8217;&#142;coulement est une propri&#142;t&#142; de la rivi&#143;re, &#212;rivi&#143;re&#213; est
un argument du pr&#142;dicat &#212;couler&#213;. Quand les d&#142;pendances s&#142;mantique et syntaxique sont dans la
m&#144;me direction, on dit que B est un actant de A (rivi&#143;re est un actant de coule dans la rivi&#143;re
coule), tandis que, quand les d&#142;pendances s&#142;mantique et syntaxique sont dans la direction
oppos&#142;e, on dit que B est un modifieur de A (petite est un modifieur de rivi&#143;re dans une petite
rivi&#143;re). Il existe aussi des cas o&#157; d&#142;pendances s&#142;mantique et syntaxique ne se correspondent
pas, comme dans les ph&#142;nom&#143;nes de mont&#142;e (dans Pierre semble malade, Pierre d&#142;pend
syntaxiquement de semble, mais &#212;sembler&#213; est un pr&#142;dicat unaire qui prend seulement &#212;malade&#213;
comme argument) ou de tough-movement (dans un livre facile &#136; lire, facile d&#142;pend
syntaxiquement de livre, mais &#212;livre&#213; est un argument de &#212;lire&#213; et pas de &#212;facile&#213;) ; voir &#142;galement
le cas des relatives et des interrogatives indirectes (Kahane &amp; Mel&#8217;&#139;uk 1999).
La valence s&#142;mantique d&#8217;un s&#142;mant&#143;me, c&#8217;est-&#136;-dire l&#8217;ensemble de ses arguments s&#142;mantiques,
est d&#142;termin&#142;e par sa d&#142;finition lexicographique. Ainsi &#212;blessureI.2&#213; est une pr&#142;dicat ternaire
(Mel&#8217;&#139;uk et al. 1999 ; d&#142;finition r&#142;vis&#142;e) : &#212;blessureI.2 de X &#136; Y par Z&#213; = &#212;l&#142;sion &#136; la partie Y du
corps de X qui est caus&#142;e par Z et qui peut causer une ouverture de la peau de Y, un saignement
de Y, une douleur de X &#136; Y ou la mort de X&#213; (saX blessure par balleZ &#136; la jambeY).
La repr&#142;sentation s&#142;mantique comprend, en plus du graphe s&#142;mantique, trois autres structures
qui s&#8217;y superposent : la structure communicative, la structure r&#142;f&#142;rentielle (qui relie des portions
du graphe aux r&#142;f&#142;rents qu&#8217;elles d&#142;notent) et la structure rh&#142;torique (qui indiquent les intentions
stylistiques du locuteur, c&#8217;est-&#136;-dire si celui-ci veut &#144;tre neutre, ironique, rel&#137;ch&#142;, humoristique,
&#201;). La Figure 5 pr&#142;sente une repr&#142;sentation s&#142;mantique simplifi&#142;e (limit&#142;e au graphe
s&#142;mantique et &#136; la th&#142;maticit&#142;) pour la phrase (1) :
(1) Zo&#142; essaye de parler &#136; la belle dame
</p>
<p>                                                
</p>
<p>20
 Les deux phrases peuvent appara&#148;tre non synonymes en raison de la structure communicative (voir plus loin) :
</p>
<p>par exemple, si la premi&#143;re peut facilement avoir pour th&#143;me &#212;la dur&#142;e de ta sieste&#213; (= &#212;ta sieste a dur&#142;&#213;), cela
para&#148;t plus difficile pour la deuxi&#143;me qui aura plut&#153;t pour th&#143;me &#212;toi&#213; ou &#212;ta sieste&#213; (= &#212;tu as fait la sieste&#213;).
</p>
<p>21
 Comme pour les lexies, les diff&#142;rentes acceptions d&#8217;un morph&#143;me flexionnel devraient &#144;tre distingu&#142;es par des
</p>
<p>num&#142;raux. A noter que Mel&#8217;&#139;uk ne consid&#143;re pas de s&#142;mant&#143;mes grammaticaux et utilisent des paraphrases
lexicales : &#212;plus d&#8217;un&#213;, &#212;avant maintenant&#213;, &#201;</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Sylvain Kahane
</p>
<p>La m&#144;me repr&#142;sentation s&#142;mantique vaut pour des paraphrases de (1) comme Zo&#142; cherche &#136; dire
un mot &#136; la jolie femme.
</p>
<p>Figure 5 : La repr&#142;sentation s&#142;mantique de (1)
</p>
<p>La structure communicative sp&#142;cifie la fa&#141;on dont le locuteur veut pr&#142;senter l&#8217;information qu&#8217;il
communique (de quoi il parle, ce qu&#8217;il veut dire, ce qu&#8217;il veut souligner, ce qu&#8217;il pr&#142;sente
comme information commune avec son interlocuteur, &#201;). La structure communicative est
encod&#142;e en marquant certaines zones du graphe s&#142;mantique par des marques communicatives.
Dans chacune de ces zones, on indique (par un soulignement) le s&#142;mant&#143;me qui r&#142;sume le
contenu s&#142;mantique de cette zone (Polgu&#143;re 1990). Mel&#8217;&#139;uk 2001 propose huit cat&#142;gories
communicatives : th&#142;maticit&#142; (th&#143;me-rh&#143;me-sp&#142;cifieurs), donn&#142;-nouveau, focalisation,
perspective (arri&#143;re-plan), emphatisation, pr&#142;supposition, unitarit&#142; (unitaire-articul&#142;) et
locutionalit&#142; (signal&#142;-perform&#142;-communiqu&#142;). Nous allons montrer comment des changements
dans la th&#142;matisation et la focalisation du graphe s&#142;mantique de la Figure 5 donnent d&#8217;autres
phrases. Tout message doit n&#142;cessairement communiquer quelque chose (le rh&#143;me) &#136; propos de
quelque chose (le th&#143;me) ou &#142;ventuellement de rien. La phrase (1) peut &#144;tre glos&#142;e par &#212;&#136; propos
de Zo&#142; (th&#143;me), je veux dire qu&#8217;elle essaye de parler &#136; une belle dame (rh&#143;me)&#213;. La partition
th&#143;me-rh&#143;me s&#8217;identifie en voyant quelle est la question sous-jacente au message communiqu&#142;
(ici &#212;que fait Zo&#142; ?&#213;). Un &#142;l&#142;ment focalis&#142; quant &#136; lui est une partie du sens que le locuteur
pr&#142;sente comme &#142;tant localement pro&#142;minente pour lui &#8212; ou, en d&#8217;autres termes, comme &#142;tant le
si&#143;ge (angl. focus) de son attention (Mel&#8217;&#139;uk 2001). Voici quelques phrases ayant le m&#144;me
graphe s&#142;mantique que (1) avec des structures communicatives diff&#142;rentes (et pour lesquelles les
m&#144;mes choix lexicaux ont &#142;t&#142; faits) :
(3) a. La belle dame, Zo&#142; essaye de lui parler. (&#212;belle dame&#213; th&#143;me focalis&#142;)
</p>
<p>b. C&#8217;est &#136; la belle dame que Zo&#142; essaye de parler. (&#212;belle dame&#213; rh&#143;me focalis&#142;)
c. Zo&#142;, c&#8217;est &#136; la belle dame qu&#8217;elle essaye de parler. (focalisation de &#212;Zo&#142;&#213; en plus)
d. Ce que Zo&#142; essaye de faire, c&#8217;est de parler &#136; la belle dame. (&#212;Zo&#142; essaye&#213; th&#143;me foc.)
</p>
<p>On trouvera de nombreux exemples dans Mel&#8217;&#139;uk 2001. Le r&#153;le de la structure communicative
dans la production des relatives est &#142;tudi&#142; dans Kahane &amp; Mel&#8217;&#139;uk 1999. Notons encore que la
structure communicative est souvent consid&#142;r&#142;e, &#136; la diff&#142;rence de la structure pr&#142;dicat-
argument, comme tr&#143;s vague et difficile &#136; cerner pr&#142;cis&#142;ment. Nous pensons que cette vision est
compl&#143;tement fausse et due en partie au fait que les linguistes &#142;tudient g&#142;n&#142;ralement cette
question du point de vue de l&#8217;analyse, en cherchant &#136; d&#142;terminer la structure communicative de
textes. Evidemment, si l&#8217;on consid&#143;re un &#142;nonc&#142; isol&#142; tel que Marie a dit que Pierre est parti, il
est impossible de d&#142;terminer sa partition th&#143;me-rh&#143;me. Cette phrase peut &#210;r&#142;pondre&#211;, avec certes
des prosodies diff&#142;rentes, &#136; des questions aussi diverses que Que fait Marie ?, Qu&#8217;a dit Marie ?,
Qu&#8217;a dit Marie de Pierre ?, Que fait Pierre ?, Qui a dit que Pierre est parti ?, Qui est parti&#730;?, &#201;
(par exemple, lorsqu&#8217;elle r&#142;pond &#136; la question Que fait Pierre ?, &#212;Pierre&#213; est le th&#143;me, &#212;partir&#213; est
le rh&#143;me, &#212;Marie m&#8217;a dit&#213; un sp&#142;cifieur qui sp&#142;cifie sous quelles conditions je peux dire que
Pierre est parti). Par contre si on se place du point de vue de la synth&#143;se, il est &#142;vident que le
locuteur sait de quoi il veut parler et ce qu&#8217;il veut dire &#136; ce propos. La partition th&#143;me-rh&#143;me est
donc parfaitement &#142;tablie (notons d&#8217;ailleurs qu&#8217;elle s&#8217;&#142;tablit au niveau s&#142;mantique). D&#158; au
</p>
<p>1 2
</p>
<p>&#212;essayer&#213;
</p>
<p>&#212;Zo&#142;&#213;
2
</p>
<p>&#212;parler&#213;
</p>
<p>&#212;dame&#213; &#212;beau&#213;1
</p>
<p>1
</p>
<p>&#212;pr&#142;sent&#213;
</p>
<p>&#212;d&#142;fini&#213;
</p>
<p>1
</p>
<p>1 1
</p>
<p>&#212;singulier&#213;
</p>
<p>T
</p>
<p>R</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Grammaires de d&#142;pendance formelles et th&#142;orie Sens-Texte
</p>
<p>pouvoir paraphrastique de la langue, il reste au locuteur de nombreuses possibilit&#142;s
d&#8217;&#142;nonciation conditionn&#142;es aussi par les autres choix communicatifs, notamment la
focalisation. Des &#142;nonc&#142;s tels que Marie a dit que Pierre est parti, peu conditionn&#142;s par la
structure communicative, seront possibles avec de nombreux choix communicatifs, mais
d&#8217;autres choix syntaxiques ne seront possibles qu&#8217;avec des choix  communicatifs pr&#142;cis :
D&#8217;apr&#143;s Marie, Pierre est parti (&#212;Marie a dit&#213;sp&#142;cifieur), C&#8217;est Marie qui a dit que Pierre est parti
(&#212;Marie&#213; rh&#143;me focalis&#142;), etc.
Une repr&#142;sentation s&#142;mantique peut &#144;tre encod&#142;e dans un style inspir&#142; de la logique. La
traduction d&#8217;un graphe s&#142;mantique en une formule logique n&#142;cessite d&#8217;introduire une variable
pour chaque n&#711;ud du graphe (&#136; l&#8217;exception des n&#711;uds &#142;tiquet&#142;s par un s&#142;mant&#143;me
grammatical). Cette variable repr&#142;sente le n&#711;ud et est utilis&#142;e comme argument par tout
s&#142;mant&#143;me pointant sur le n&#711;ud. En introduisant des variables x, y, p, e et e&#8217; pour les
s&#142;mant&#143;mes lexicaux  &#212;Zo&#142;&#213;, &#212;dame&#213;, &#212;beau&#213;, &#212;essayer&#213; et &#212;parler&#213;, on peut encoder le graphe de
la Figure 5 par la formule (2).
(2) THEME(x)
</p>
<p>x  : &#212;Zo&#142;&#213;
RHEME(e)
e : &#212;essayer&#213;(x,e&#213;)
e&#8217; : &#212;parler&#213;(x,y)
y : &#212;dame&#213;
p : &#212;beau&#213;(y)
&#212;pr&#142;sent&#213;(e)
&#212;singulier&#213;(y)
&#212;d&#142;fini&#213;(y)
</p>
<p>La structure th&#143;me-rh&#143;me est encod&#142;e par la partition des s&#142;mant&#143;mes en deux groupes et par les
&#210;pr&#142;dicats&#211; THEME et RHEME pointant sur les n&#711;uds dominants de ces deux zones. Si l&#8217;on
omet la structure th&#143;me-rh&#143;me, l&#8217;ordre des pr&#142;dicats n&#8217;est pas pertinent et la formule s&#8217;apparente
&#136; une formule conjonctive du calcul des pr&#142;dicats (cf. par exemple les repr&#142;sentations
s&#142;mantiques de la DRT ; Kamp 1981, Kamp &amp; Reyle 1993). La variable repr&#142;sentant un n&#711;ud
peut d&#8217;ailleurs &#144;tre attribu&#142;e au s&#142;mant&#143;me (on parle de r&#142;ification) : ainsi &#136; la place des notations
y : &#212;dame&#213; ou e&#730;:&#730;&#212;essayer&#213;(x,e&#213;), on peut utiliser les notations &#212;dame&#213;(y) ou &#212;essayer&#213;(e,x,e&#213;),
plus habituelles en logique.
</p>
<p>Malgr&#142; leur similitude formelle, les repr&#142;sentations s&#142;mantiques de la TST doivent &#144;tre
distingu&#142;es des repr&#142;sentations s&#142;mantiques des s&#142;mantiques issues de la logique fr&#142;g&#142;enne,
comme la DRT. En TST, la repr&#142;sentation s&#142;mantique ne repr&#142;sente pas l&#8217;&#142;tat du monde que
d&#142;note un sens, mais le sens lui-m&#144;me. En particulier, les variables que nous avons introduites
lors de la r&#142;ification ne renvoient pas, comme c&#8217;est le cas dans la logique fr&#142;g&#142;enne, &#136; des objets
du monde. Les variables renvoient ici uniquement aux s&#142;mant&#143;mes, c&#8217;est-&#136;-dire aux signifi&#142;s
des mots. Donnons un exemple : dans le sens de une grosse fourmi, le s&#142;mant&#143;me &#212;gros&#213; est un
pr&#142;dicat unaire dont l&#8217;argument est le s&#142;mant&#143;me &#212;fourmi&#213; et en aucun cas le r&#142;f&#142;rent de fourmi.
D&#8217;ailleurs, quand on parle d&#8217;une grosse fourmi, on ne veut pas dire que le r&#142;f&#142;rent de fourmi est
gros en soi (d&#8217;ailleurs rien n&#8217;est gros en soi), mais qu&#8217;il est gros en tant que fourmi. La chose
est peut-&#144;tre encore plus &#142;vidente quand on parle d&#8217;un gros fumeur. Ici non plus, on ne veut pas
dire que le r&#142;f&#142;rent de fumeur est gros, mais que quelque chose dans le sens &#212;fumeur&#213; est gros.
En effet, si un &#212;fumeur&#213; est une &#212;personne qui fume (r&#142;guli&#143;rement)&#213;, un &#212;gros fumeur&#213; est une
&#212;personne qui fume (r&#142;guli&#143;rement) en grosse quantit&#142;&#213;. D&#8217;autre part, le s&#142;mant&#143;me &#212;gros&#213;
pourra lui-m&#144;me &#144;tre l&#8217;argument d&#8217;un autre s&#142;mant&#143;me comme dans une tr&#143;s grosse fourmi ou
une fourmi plus grosse que mon pouce, ce qui n&#142;cessite d&#8217;introduire une variable pour &#212;gros&#213;
lors de la r&#142;ification, sans qu&#8217;on veuille pour autant consid&#142;rer que gros poss&#143;de un r&#142;f&#142;rent de
discours.
</p>
<p>En TST, le sens est d&#142;fini comme ce qui est commun &#136; tous les &#142;nonc&#142;s qui ont le m&#144;me sens.
La d&#142;finition n&#8217;est pas circulaire, &#212;avoir le m&#144;me sens&#213; &#142;tant d&#142;fini pr&#142;alablement au &#212;sens&#213; : il
est plus facile de demander &#136; un locuteur si deux &#142;nonc&#142;s ont le m&#144;me sens (sont synonymes)</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Sylvain Kahane
</p>
<p>que de lui demander quel est le sens d&#8217;un &#142;nonc&#142; (ce qui d&#8217;ailleurs le conduira essentiellement &#136;
proposer des &#142;nonc&#142;s qui ont le m&#144;me sens). La TST est donc un mod&#143;le de la paraphrase. Le
sens est un objet purement linguistique. La description du monde est rel&#142;gu&#142;e &#136; un niveau de
repr&#142;sentation plus profond, extralinguistique. Remarquons tout de m&#144;me que la r&#142;f&#142;rence au
monde ext&#142;rieur n&#8217;est pas exclue de la repr&#142;sentation s&#142;mantique de la TST et fait l&#8217;objet d&#8217;une
structure particuli&#143;re, la structure r&#142;f&#142;rentielle, superpos&#142;e au graphe s&#142;mantique et indiquant
quelle zone du graphe correspond &#136; un r&#142;f&#142;rent de discours.
</p>
<p>Notons enfin, pour terminer sur les diff&#142;rences entre les repr&#142;sentations s&#142;mantiques de la TST
et les formules logiques, que tous les s&#142;mant&#143;mes sont formalis&#142;s par des pr&#142;dicats (les noms
s&#142;mantiques comme &#212;Zo&#142;&#213; ou &#212;dame&#213; &#142;tant  des cas particuliers de pr&#142;dicats &#136; z&#142;ro argument),
m&#144;me des sens comme &#212;quel que soit&#213;, &#212;quelqu&#8217;un&#213;, &#212;et&#213; ou &#212;non&#213;, qui sont habituellement
formalis&#142;s en logique par des objets d&#8217;une autre nature, quantifieurs ou connecteurs.22
</p>
<p>3.2.2   Repr&#142;sentation syntaxique profonde
Le niveau syntaxique profond est un niveau interm&#142;diaire entre le niveau s&#142;mantique et le niveau
syntaxique de surface, o&#157; le graphe a &#142;t&#142; hi&#142;rarchis&#142; et les s&#142;mant&#143;mes lexicalis&#142;s, mais o&#157; ne
figure pas encore &#136; proprement parl&#142; les mots. Le c&#711;ur de la repr&#142;sentation syntaxique profonde
est un arbre de d&#142;pendance (non ordonn&#142;) dont les n&#711;uds sont &#142;tiquet&#142;s par des lexies
profondes accompagn&#142;es chacune d&#8217;une liste de gramm&#143;mes profonds. Les lexies profondes
sont des lexies pleines correspondant &#136; des mots ou &#136; des locutions. Les lexies vides, comme
les pr&#142;positions r&#142;gies, n&#8217;apparaissent qu&#8217;au niveau syntaxique de surface. De m&#144;me, les
gramm&#143;mes profonds sont des morph&#143;mes grammaticaux pleins ; les gramm&#143;mes vides dus &#136;
l&#8217;accord ou &#136; la rection, comme le cas, apparaissent plus tard. Les cat&#142;gories grammaticales
profondes du verbe sont le mode, le temps et la voix. Les lexies sont &#142;crites en majuscules et les
gramm&#143;mes plac&#142;s en indice : LEXIEgramm&#143;me. Les branches de l&#8217;arbre sont &#142;tiquet&#142;es avec un
petit ensemble de relations syntaxiques profondes : les actants sont simplement num&#142;rot&#142;s par
oblicit&#142; croissante (I, II, III, &#201;), les modifieurs reli&#142;s &#136; leur gouverneur par la relation ATTR
(angl. attributive) et deux autres relations sont consid&#142;r&#142;es, COORD pour les groupes
coordonn&#142;s (Marie, Jean et Pierre : MARIE &#8722;COORD&#8594; JEAN &#8722;COORD&#8594; ET &#8722;II&#8594; PIERRE)
et APPEND pour les parenth&#142;tiques, les interjections, les interpellations, etc. (Naturellement,
il n&#8217;a rien fait ; O&#157; vas-tu, Zo&#142; ?). Kahane &amp; Mel&#8217;&#139;uk 1999 introduisent une autre relation pour
les modifieurs qualitatifs (non restrictifs) et Kahane 1998 propose l&#8217;introduction d&#8217;une relation
sp&#142;cifique pour un actant r&#142;trograd&#142; (tel que le compl&#142;ment d&#8217;agent).
Nous proposons Figure 6 la repr&#142;sentation syntaxique profonde de (1) ; Le trait hachur&#142;
repr&#142;sente une relation de cor&#142;f&#142;rence entre les deux occurrences de ZO&#131; r&#142;sultant de la coupure
du graphe s&#142;mantique au niveau du s&#142;mant&#143;me &#212;Zo&#142;&#213;.  L&#8217;une des deux occurrences sera effac&#142;e
en surface par la r&#143;gle de pronominalisation de l&#8217;actant I d&#8217;un verbe &#136; l&#8217;infinitif. Le gramm&#143;me
infinitif sera introduit par le r&#142;gime de ESSAYER au niveau syntaxique de surface. La structure
                                                
</p>
<p>22
 La diff&#142;renciation formelle des quantifieurs est certainement n&#142;cessaire pour la d&#142;duction logique, mais ne l&#8217;est
</p>
<p>pas forc&#142;ment pour la paraphrase et la traduction. La port&#142;e des quantifieurs n&#8217;est pas clairement encod&#142;e dans
les repr&#142;sentations s&#142;mantiques standard. En un sens, il n&#8217;est pas s&#158;r que la port&#142;e des quantificateurs doive
r&#142;ellement &#144;tre encod&#142;e dans la repr&#142;sentation s&#142;mantique de Tous les hommes cherchent un chat et il est
curieux de voir fleurir des travaux qui montrent comment sous-sp&#142;cifier les repr&#142;sentations s&#142;mantiques dans
des formalismes qui obligent &#136; indiquer la port&#142;e des quantifieurs. Mel&#8217;&#139;uk 2001 &#142;met l&#8217;hypoth&#143;se que les
effets de port&#142;e des quantificateurs r&#142;sultent de la structure communicative. Polgu&#143;re 1992 propose d&#8217;encoder
les quantifieurs comme des s&#142;mant&#143;mes biactanciels dont le deuxi&#143;me argument, repr&#142;sentant la port&#142;e, pointe
sur une zone du graphe, ce qui pourrait &#144;tre reli&#142; &#136; l&#8217;hypoth&#143;se pr&#142;c&#142;dente. Dymetman &amp; Coperman 1996
proposent une solution &#136; l&#8217;encodage de la port&#142;e des quantifieurs avec une repr&#142;sentation interm&#142;diaire entre
graphe s&#142;mantique et formule logique.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Grammaires de d&#142;pendance formelles et th&#142;orie Sens-Texte
</p>
<p>communicative syntaxique profonde qui reprend la structure communicative s&#142;mantique n&#8217;est
pas repr&#142;sent&#142;e ici.
</p>
<p>Figure 6 : Repr&#142;sentation syntaxique profonde de (1)
</p>
<p>Notons encore l&#8217;une des sp&#142;cificit&#142;s de l&#8217;approche Sens-Texte : le concept de fonction lexicale
(&#235;olkovskij &amp; Mel&#8217;&#139;uk 1965, Mel&#8217;&#139;uk et al. 1995, Wanner 1996, Kahane &amp; Polgu&#143;re 2001).
Certains sens, comme l&#8217;intensification, le commencement, la causation, la r&#142;alisation, etc.,
tendent &#136; s&#8217;exprimer de mani&#143;re collocationnelle, c&#8217;est-&#136;-dire que leur expression n&#8217;est pas
d&#142;termin&#142;e librement, mais d&#142;pend fortement de l&#8217;expression d&#8217;un de leurs arguments
s&#142;mantiques. Les fonctions lexicales sont des &#210;lexies&#211; dont le signifiant n&#8217;est pas un mot pr&#142;cis,
mais varie en fonction de l&#8217;expression d&#8217;un argument. Par exemple, le sens &#212;intense&#213; pourra
s&#8217;exprimer avec amoureux par follement, avec heureux par comme un pape, avec improbable
par hautement, avec bless&#142; par gravement, etc. De m&#144;me, le sens &#212;commencer&#213; pour s&#8217;exprimer
avec incendie par se d&#142;clarer, avec jour ou vent par se lever, avec orage par &#142;clater, etc. Les
fonctions lexicales correspondantes seront not&#142;es Magn et Incep. Ces fonctions seront
utilis&#142;es pour &#142;tiqueter un n&#711;ud de l&#8217;arbre syntaxique profond correspondant &#136; un sens
&#212;intense&#213; ou &#212;commencer&#213;. Les valeurs seront introduites seulement dans l&#8217;arbre syntaxique de
surface (Mel&#8217;&#139;uk 1988, Polgu&#143;re 1998).
</p>
<p>3.2.3  Repr&#142;sentation syntaxique de surface
Le c&#711;ur de la repr&#142;sentation syntaxique de surface d&#213;une phrase est un arbre de d&#142;pendance
(non ordonn&#142;) &#136; la fa&#141;on des arbres de d&#142;pendance de Tesni&#143;re 1959. Les n&#711;uds de l&#8217;arbre sont
&#142;tiquet&#142;s par des lexies de surface accompagn&#142;es chacune d&#213;une liste de gramm&#143;mes de surface.
Chaque lexie de surface correspond &#136; un mot de la phrase. Ces lexies peuvent correspondre
directement &#136; une lexie profonde, ou bien correspondre &#136; l&#8217;un des mots d&#8217;une locution, ou &#144;tre
la valeur d&#8217;une fonction lexicale, ou bien &#144;tre une lexie vide introduite par un r&#142;gime (comme les
pr&#142;positions DE et  &#807; ici), ou encore &#144;tre une partie de l&#8217;expression d&#8217;un gramm&#143;me profond
(comme un auxiliaire de temps ou l&#8217;article LE ici). Les gramm&#143;mes de surface correspondent
directement &#136; un gramm&#143;me profond, sauf pour les gramm&#143;mes profonds qui ont une
expression analytique comme les temps compos&#142;s, les voix ou la d&#142;termination. Les
gramm&#143;mes de surface d&#213;accord ou de r&#142;gime (comme les cas) ne sont introduits qu&#8217;au niveau
morphologique profond. Les branches de l&#8217;arbre syntaxique de surface sont &#142;tiquet&#142;es par des
fonctions syntaxiques ou relations syntaxiques de surface (cf. Mel&#8217;&#139;uk 1974 pour le russe,
Mel&#8217;&#139;uk &amp; Pertsov 1987 pour l&#8217;anglais et Iordanskaja &amp; Mel&#8217;&#139;uk 2000 pour le fran&#141;ais; voir
aussi Section 2.2).
</p>
<p>I II
</p>
<p>ESSAYER
</p>
<p>ATTR
</p>
<p>ZO&#131;
PARLER
</p>
<p>DAME
</p>
<p>ind,pr&#142;sent,actif
</p>
<p>sg,d&#142;f
I II
</p>
<p>BEAU
</p>
<p>ZO&#131;
</p>
<p>actif</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Sylvain Kahane
</p>
<p>Figure 7 : Repr&#142;sentation syntaxique de surface de (1)
</p>
<p>3.2.4  Repr&#142;sentation morphologique profonde
</p>
<p>Le c&#711;ur de la repr&#142;sentation morphologique profonde d&#8217;une phrase est la suite des
repr&#142;sentations morphologiques des mots de la phrase, c&#8217;est-&#136;-dire une cha&#148;ne morphologique.
La repr&#142;sentation morphologique d&#8217;un mot est une lexie de surface accompagn&#142;e d&#8217;une liste de
gramm&#143;mes de surface. A noter que, en fran&#141;ais, l&#8217;adjectif s&#8217;accorde en genre et nombre avec le
nom et le verbe en personne et nombre avec son sujet. Le nom poss&#143;de une marque de genre qui
est indiqu&#142;e dans son entr&#142;e lexicale, mais ne porte pas de gramm&#143;me de genre comme les
adjectifs, car il n&#8217;est pas fl&#142;chi par le genre.23
</p>
<p>La cha&#148;ne morphologique de (1) est :
(2) ZO&#131; ESSAYERind,pr&#142;sent,3,sg DE PARLERinf &#184;  LEf&#142;m,sg BEAUf&#142;m,sg  DAMEsg
La repr&#142;sentation morphologique d&#8217;une phrase comprend, en plus de la cha&#148;ne morphologique,
une structure prosodique. La structure prosodique au niveau morphologique est essentiellement
un regroupement des mots en groupes prosodiques, agr&#142;ment&#142; de marques prosodiques
calcul&#142;es en fonction des marques communicatives des portions de l&#8217;arbre syntaxique de surface
auxquelles correspondent ces groupes. La v&#142;ritable structure prosodique sera calcul&#142;e au niveau
phonologique &#136; partir de cette structure prosodique de niveau morphologique et des propri&#142;t&#142;s
phonologiques des mots (qui ne sont pas encore prises en compte au niveau morphologique, les
phon&#143;mes n&#8217;&#142;tant pas consid&#142;r&#142;s). Dans Gerdes &amp; Kahane 2001 (inspir&#142;s par des discussions
avec Igor Mel&#8217;&#139;uk), nous proposons de construire au niveau morphologique une structure
syntagmatique qui, contrairement &#136; l&#8217;usage qu&#8217;en font les grammaires bas&#142;es sur la syntaxe X-
barre, n&#8217;encode pas la repr&#142;sentation syntaxique de la phrase, mais plut&#153;t sa structure
prosodique de niveau morphologique (cf. aussi Section 6).
</p>
<p>3.3 Mod&#143;le Sens-Texte standard
On appelle mod&#143;le Sens-Texte [MST] d&#8217;une langue le mod&#143;le de cette langue dans le cadre de la
TST. Le terme mod&#143;le est pr&#142;f&#142;r&#142; au terme plus couru de grammaire (formelle), car le terme
grammaire masque le fait qu&#8217;une composante essentielle d&#8217;un mod&#143;le d&#8217;une langue, &#136; c&#153;t&#142; de la
grammaire proprement dite est le lexique.
                                                
</p>
<p>23
 Il n&#8217;est probablement pas judicieux de consid&#142;rer que les noms poss&#143;dent un trait de personne. On peut penser
</p>
<p>que seuls les pronoms poss&#143;dent un tel trait et que le verbe prend la 3&#143;me personne par d&#142;faut, comme il le
fait aussi avec les sujets verbaux ou phrastiques (que tu viennes est une bonne surprise).
</p>
<p>suj inf
ESSAYER
</p>
<p>DE
pr&#142;pZO&#131;
</p>
<p>iobj
</p>
<p>d&#142;t mod
</p>
<p>PARLER
</p>
<p>DAME
</p>
<p>BEAULE
</p>
<p>ind,pr&#142;sent
</p>
<p>inf
</p>
<p>sg
</p>
<p>&#184;
</p>
<p>pr&#142;p</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Grammaires de d&#142;pendance formelles et th&#142;orie Sens-Texte
</p>
<p>Dans la Section 3.3.1, nous pr&#142;senterons le lexique d&#8217;un MST, puis, dans la Section 3.3.2,
l&#8217;architecture g&#142;n&#142;rale d&#8217;un module de correspondance. Enfin, dans les Sections 3.3.2 &#136; 0, nous
pr&#142;senterons les modules s&#142;mantique, syntaxique profond et syntaxique de surface d&#8217;un MST.
</p>
<p>3.3.1  Le lexique d&#8217;un mod&#143;le Sens-Texte
</p>
<p>La TST donne une tr&#143;s grande importance au lexique. Le lexique d&#8217;un MST est appel&#142; un
Dictionnaire Explicatif et Combinatoire [DEC] . Un premier DEC pour le russe a &#142;t&#142; propos&#142; par
Mel&#8217;&#139;uk &amp; &#235;olkovsky 1984. Un DEC du fran&#141;ais est maintenant en d&#142;veloppement depuis 20
ans &#136; l&#213;universit&#142; de Montr&#142;al (Mel&#8217;&#139;uk et al. 1984, 1988, 1992, 1999). Les entr&#142;es du DEC
sont les lexies profondes.24 En plus de la description des valences s&#142;mantique et syntaxique, qui
est indissociable des approches bas&#142;es sur la d&#142;pendance, le DEC se caract&#142;rise par le grand
soin donn&#142; &#136; la d&#142;finition s&#142;mantique des lexies (bas&#142;e sur la paraphrase) et par l&#213;utilisation des
fonctions lexicales dans la description des liens d&#142;rivationnels et collocationnels entre lexies
(Mel&#8217;&#139;uk et al. 1995, Kahane &amp; Polgu&#143;re 2001).
Nous allons pr&#142;senter et commenter l&#8217;entr&#142;e (r&#142;vis&#142;e et simplifi&#142;e) de la lexie BLESSUREI.2
(Mel&#8217;&#139;uk et al. 1999). Chaque article est divis&#142; en trois zones :
- la zone s&#142;mantique donne la d&#142;finition lexicographique de la lexie ;
</p>
<p>- la zone syntaxique donne le tableau de r&#142;gime (ou cadre de sous-cat&#142;gorisation de la lexie,
c&#8217;est-&#136;-dire la correspondance entre les actants s&#142;mantiques (X, Y, &#201;), les actants
syntaxiques profonds (I, II, &#201;) et les leur expression de surface (&#136; N, par N, &#201;); le
tableau de r&#142;gime est suivi par des conditions particuli&#143;re (l&#8217;expression 1 de la colonne 3
n&#8217;est possible que si N est une arme blanche) et des exemples de r&#142;alisations et de
combinaisons des diff&#142;rents actants ;
</p>
<p>- la zone de cooccurrence lexicale donne les valeurs des fonctions lexicales pour la lexie,
c&#8217;est-&#136;-dire les collocations form&#142;es avec cette lexie (une blessure cuisante, se faire une
blessure, la blessure s&#8217;infecte, &#201;) et les d&#142;rivations s&#142;mantiques de la lexie (bless&#142;, plaie,
se blesser, &#201;).
</p>
<p>Exemple : article de dictionnaire de BLESSUREI.2
</p>
<p>D&#142;finition lexicographique
&#212;blessureI.2 de X &#136; Y par Z&#213; = &#212;l&#142;sion &#136; la partie Y du corps de X qui est caus&#142;e par Z et qui
peut causer (I) une ouverture de la peau de Y, (II) un saignement de Y, (III) une douleur de X &#136;
Y ou (IV) la mort de X&#213;25
</p>
<p>                                                
</p>
<p>24
 La description s&#142;par&#142;e des lexies de surface pourrait &#144;tre &#142;galement utile. Pour l&#8217;instant, celles-ci sont d&#142;crites
</p>
<p>grossi&#143;rement lorsqu&#8217;elle apparaissent dans la description des lexies profondes, comme &#142;l&#142;ment d&#8217;une locution,
comme valeur d&#8217;une fonction lexicale ou comme &#142;l&#142;ment r&#142;gi introduit dans le tableau de r&#142;gime.
</p>
<p>25
 La d&#142;finition lexicographique est bas&#142;e sur la paraphrase (la d&#142;finition de L doit est substituable &#136; L) et la
</p>
<p>cooccurrence lexicale : les composantes (I) &#136; (IV) sont conditionn&#142;es par les diff&#142;rentes valeurs des Fact-Real
de BLESSUREI.2. Cette portion de la d&#142;finition indique les &#210;objectifs&#211; inh&#142;rents de L (une blessure peut &#144;tre
profonde (I), saigner (II), faire souffrir (III) ou &#144;tre fatale (IV)). Voir, pour comparaison, les valeurs du trait
telic dans les descriptions lexicales du Lexique G&#142;n&#142;ratif de Pustejovsky 1995.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Sylvain Kahane
</p>
<p>R&#142;gime
</p>
<p>X  = 1 Y = 2 Z = 3
</p>
<p>1. de N
2. Apos
</p>
<p>1. &#136; N 1. &#136; N
2. par N
</p>
<p>Contrainte sur 3.1 : N d&#142;signe une arme blanche
Contrainte sur 3.2 : N = balle, &#201;
</p>
<p>Exemples
1 : la blessure de Jean/du soldat/du cheval ; sa blessure
2 : une blessure &#136; l&#8217;&#142;paule/au c&#711;ur/&#136; l&#8217;abdomen ; des blessures au corps
3 : une blessure &#136; l&#8217;arme blanche/au couteau ; une blessure par balle
1 + 2 : les blessures de l&#8217;enfant aux bras ; sa blessure au poignet droit
1 + 2 +3 : sa blessure par balle &#136; la jambe
</p>
<p>Fonctions lexicales
Syn&#8834; : l&#142;sion
Syn&#8835; : coupure, &#142;corchure; &#142;gratignure; morsure; br&#158;lure; ecchymose;
</p>
<p>d&#142;chirure; fracture; entorse
Syn&#8745; : plaie; bobo &#210;fam&#211;
personne-S1 : bless&#142;
A1/2 : // bless&#142;
A1/2+Magn : couvert, cribl&#142; [de ~s]
Magn : grave, majeure, s&#142;rieuse
AntiMagn : l&#142;g&#143;re, mineure, superficielle // &#142;gratignure
AntiBon : mauvaise, vilaine
IncepMinusBon : s&#8217;aggraver; s&#8217;enflammer, s&#8217;envenimer, s&#8217;infecter
Oper1 : avoir [ART ~]; porter [ART ~]; souffrir [de ART ~]
FinOper1 : se remettre, se r&#142;tablir [de ART ~]
Caus1Oper1 : se faire [ART ~]
LiquOper1 : gu&#142;rir [N de ART ~]
FinFunc0 : se cicatriser, (se) gu&#142;rir, se refermer
essayer de LiquFunc0 : soigner, traiter [ART ~]; bander, panser [ART ~]
CausFunc1 : faire [ART ~ &#136; N]; infliger [ART ~ &#136; N] // blesser [N] [avec N=Z]
Caus1Func1 : se faire [ART ~]; se blesser [avec N=Z]
Real1 : (II) souffrir [de ART ~];
</p>
<p>  (IV) succomber [&#136; ART ~], mourir [de ART ~]
AntiReal1 : (IV) r&#142;chapper [de ART ~]
Fact0 : (I) s&#8217;ouvrir, se rouvrir; (II) saigner
Fact1 : (IV) emporter, tuer [N]
Able1Fact1 (&#8773; Magn) : (I) ouverte &lt; profonde &lt; b&#142;ante (III) cuisante, douloureuse;
</p>
<p>  (IV) fatale, mortelle, qui ne pardonne pas
AntiAble1Fact1 (&#8773; AntiMagn) : b&#142;nigne &#210;sp&#142;c&#211;, sans cons&#142;quence
</p>
<p>Nous ne pouvons expliquer ici les sens des diff&#142;rentes fonctions lexicales (cf. Mel&#8217;&#139;uk et al.
1995). Chaque fonction lexicale simple (Magn, Oper1, &#201;) correspond &#136; une r&#143;gle s&#142;mantique
particuli&#143;re (voir Section 3.3.3) et les fonctions lexicales complexes (IncepOper1, Able1Fact1,
&#201;) correspondent &#136; des op&#142;rations naturelles sur les fonctions lexicales simples (Kahane &amp;
Polgu&#143;re 2001). Les fonctions lexicales jouent un grand r&#153;le dans les choix lexicaux (Mel&#8217;&#139;uk
1988a, Polgu&#143;re 1998), ainsi que dans la paraphrase et la traduction (Mel&#8217;&#139;uk 1988b).</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Grammaires de d&#142;pendance formelles et th&#142;orie Sens-Texte
</p>
<p>3.3.2  Les modules de correspondance d&#8217;un mod&#143;le Sens-Texte
</p>
<p>La grammaire d&#8217;un mod&#143;le Sens-Texte est divis&#142;e en modules. Chaque module assure la
correspondance entre deux niveaux adjacents : le module s&#142;mantique assure la correspondance
entre le niveau s&#142;mantique et le niveau syntaxique profond, le module syntaxique profond la
correspondance entre le niveau syntaxique profond et le niveau syntaxique de surface, le module
syntaxique de surface la correspondance entre le niveau syntaxique de surface et le niveau
morphologique profond, etc.
</p>
<p>Les r&#143;gles de grammaire d&#213;un mod&#143;le Sens-Texte sont toutes des r&#143;gles de correspondance entre
deux niveaux adjacents, c&#213;est-&#136;-dire des r&#143;gles qui associent un fragment d&#213;une structure d&#213;un
niveau donn&#142; avec un fragment d&#213;une structure d&#213;un niveau adjacent. Les r&#143;gles se pr&#142;sentent
toutes sous la forme  A &#8660; B | C  o&#157; A et B sont des fragments de structure de deux niveaux
adjacents et C est un ensemble de conditions. La r&#143;gle doit &#144;tre lue &#210;si les conditions C sont
v&#142;rifi&#142;es, A peut &#144;tre traduit par B&#211; dans le sens de la synth&#143;se et &#210;si les conditions C sont
v&#142;rifi&#142;es, B peut &#144;tre traduit par A&#211; dans le sens de l&#8217;analyse. En fait, ce n&#8217;est pas l&#8217;ensemble
des configurations A et B qui sont traduites l&#8217;une dans l&#8217;autre : les configurations contiennent
aussi des &#142;l&#142;ments qui indiquent comment la r&#143;gle va s&#8217;articuler avec d&#8217;autres r&#143;gles, comment
la configuration produite par la r&#143;gle va s&#8217;attacher aux configurations produites par les autres
r&#143;gles (voir Section 3.3.3). Suivant Kahane &amp; Mel&#8217;&#139;uk 1999, nous s&#142;parons les r&#143;gles en
r&#143;gles nodales et sagittales : les r&#143;gles nodales sont les r&#143;gles o&#157; la portion de A manipul&#142; par la
r&#143;gle est un n&#711;ud, tandis que les r&#143;gles sagittales (lat. sagitta) sont les r&#143;gles o&#157; la portion de A
manipul&#142;e par la r&#143;gle est une fl&#143;che (une d&#142;pendance s&#142;mantique,  syntaxique ou, pour le
niveau morphologique, une relation d&#8217;ordre).
Nous allons maintenant pr&#142;senter les trois premiers modules d&#8217;un MST.
</p>
<p>3.3.3  Le module s&#142;mantique d&#8217;un mod&#143;le Sens-Texte
</p>
<p>Le module s&#142;mantique r&#142;alise la correspondance entre le niveau s&#142;mantique et le niveau
syntaxique profond. Le module s&#142;mantique assure deux op&#142;rations fondamentales : la
lexicalisation et la hi&#142;rarchisation ou arborisation du graphe s&#142;mantique.
</p>
<p>La hi&#142;rarchisation est assur&#142;e par les r&#143;gles sagittales. Parmi les r&#143;gles sagittales s&#142;mantiques,
on distingue les r&#143;gles positives et n&#142;gatives. Une r&#143;gle positive transforme une d&#142;pendance
s&#142;mantique en une d&#142;pendance syntaxique de m&#144;me direction, tandis qu&#8217;une r&#143;gle n&#142;gative
inverse la direction. L&#8217;arborisation consiste &#136; choisir une entr&#142;e dans le graphe qui donnera la
racine de l&#8217;arbre, puis &#136; parcourir le graphe &#136; partir de ce n&#711;ud d&#8217;entr&#142;e. Les d&#142;pendances
s&#142;mantiques parcourues positivement (du pr&#142;dicat vers l&#8217;argument) seront traduites par des
r&#143;gles positives, tandis que les d&#142;pendances parcourues n&#142;gativement seront traduites par des
r&#143;gles n&#142;gatives. Le choix du n&#711;ud d&#8217;entr&#142;e, ainsi que celui des n&#711;uds o&#157; seront coup&#142;s les
cycles du graphe, est guid&#142; par la structure communicative. Nous ne d&#142;velopperons pas ce point
ici (cf. Polgu&#143;re 1990, Kahane &amp; Mel&#8217;&#139;uk 1999). Notons simplement que le n&#711;ud d&#8217;entr&#142;e est
par d&#142;faut le n&#711;ud dominant du rh&#143;me lorsque celui-ci peut &#144;tre lexicalis&#142; par un verbe (ou une
tournure &#142;quivalente de type verbe support-nom pr&#142;dicatif ou verbe copule-adjectif) et qu&#8217;il
prend le n&#711;ud dominant du th&#143;me comme argument s&#142;mantique.
</p>
<p>Une r&#143;gle sagittale s&#142;mantique positive traduit une d&#142;pendance s&#142;mantique en une d&#142;pendance
syntaxique profonde actancielle, tandis qu&#8217;une r&#143;gle n&#142;gative traduit une d&#142;pendance s&#142;mantique
en une d&#142;pendance syntaxique profonde ATTR, COORD ou APPEND (Figure 8). Chaque
d&#142;pendance s&#142;mantique est attach&#142;e a deux n&#711;uds s&#142;mantiques &#212;X&#213; et &#212;Y&#213; dont les
correspondants syntaxiques profonds sont X et Y ; ces &#142;tiquettes permettent de s&#8217;articuler la
r&#143;gle d&#142;crite ici avec les r&#143;gles nodales qui traduisent &#212;X&#213; en X et &#212;Y&#213; en Y. La grosse fl&#143;che que
nous indiquons dans la partie gauche de la r&#143;gle indique le sens de parcours et doit &#144;tre
compatible avec la structure communicative (cf. Mel&#8217;&#139;uk &amp; Kahane 1999).</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Sylvain Kahane
</p>
<p>Figure 8 : Trois r&#143;gles s&#142;mantiques sagittales
</p>
<p>Toutes les r&#143;gles que nous pr&#142;sentons dans la Figure 8 sont locales, c&#8217;est-&#136;-dire que la
d&#142;pendance syntaxique profonde qui traduit la d&#142;pendance s&#142;mantique consid&#142;r&#142;e doit &#144;tre
attach&#142;e &#136; la traduction n&#711;uds X et Y des n&#711;uds &#212;X&#213; et &#212;Y&#213; auxquels est attach&#142; la d&#142;pendance
s&#142;mantique. Il existe pourtant des disparit&#142;s entre les structures s&#142;mantiques et syntaxiques
profondes n&#142;cessitant des r&#143;gles non locales (cf. Kahane &amp; Mel&#8217;&#139;uk 1999 pour des r&#143;gles non
locales pour le traitement des phrases &#136; extraction).
Les r&#143;gles s&#142;mantiques nodales associent un s&#142;mant&#143;me &#136; une lexicalisation de ce s&#142;mant&#143;me.
La plupart des s&#142;mant&#143;mes sont lexicalis&#142;s par une lexie profonde. Certains s&#142;mant&#143;mes comme
&#212;intense&#213; vont &#144;tre lexicalis&#142; par une fonction lexicale (ici Magn), dont la valeur sera recherch&#142;e
par le module syntaxique profond dans l&#8217;entr&#142;e lexicale de l&#8217;argument concern&#142;.  Enfin, des
r&#143;gles s&#142;mantiques particuli&#143;res assurent la r&#142;alisation des s&#142;mant&#143;mes grammaticaux par des
gramm&#143;mes de surface.
</p>
<p>Terminons notre pr&#142;sentation du module s&#142;mantique en montrant comment on passe de la
repr&#142;sentation s&#142;mantique de (1) (Figure 5) &#136; sa repr&#142;sentation syntaxique profonde (Figure 6).
On commence par choisir le n&#711;ud d&#8217;entr&#142;e de la repr&#142;sentation s&#142;mantique. Le s&#142;mant&#143;me
&#212;essayer&#213; est choisi car il est le n&#711;ud dominant du rh&#143;me, qu&#8217;il peut &#144;tre lexicalis&#142; par un verbe
et qu&#8217;il prend le n&#711;ud dominant du th&#143;me comme argument. Ce n&#711;ud est lexicalis&#142; par
ESSAYER. Ensuite, on parcourt le graphe &#136; partir de ce n&#711;ud. Le cycle form&#142; par &#212;essayer&#213;,
&#212;parler&#213; et &#212;Zo&#142;&#213; sera coup&#142; au niveau de  &#212;Zo&#142;&#213; afin d&#8217;assurer la connexit&#142; du rh&#143;me. Le sens de
parcours de toutes les d&#142;pendances s&#142;mantique est maintenant d&#142;cid&#142;. Les d&#142;pendances
s&#142;mantiques parcourues positivement donneront des d&#142;pendances syntaxiques profondes
actancielles. Seule la d&#142;pendance entre &#212;beau&#213; et &#212;dame&#213;, parcourue n&#142;gativement, donnera une
d&#142;pendance ATTR. Comme &#212;dame&#213; sera lexicalis&#142; par le nom DAME, &#212;beau&#213; devra &#144;tre
lexicalis&#142; par un adjectif. Nous ne pr&#142;sentons pas les r&#143;gles grammaticales.
</p>
<p>3.3.4  Le module syntaxique profond d&#8217;un mod&#143;le Sens-Texte
Le module syntaxique profond r&#142;alise la correspondance entre le niveau syntaxique profond et le
niveau syntaxique de surface. Le module syntaxique profond doit assurer l&#8217;introduction de
toutes les lexies de surface de la phrase (lesquelles correspondent un &#136; un aux mots de la phrase,
&#136; l&#8217;exception de cas de r&#142;duction comme de le en du).
</p>
<p>&#8660;
</p>
<p>&#212;X&#213;
</p>
<p>&#212;Y&#213;
</p>
<p>I
</p>
<p>X
</p>
<p>Y
</p>
<p>1
X est un N
ou X est un V
pas au passif
</p>
<p>Pierre   part   ; le d&#142;part   de Pierre Y X X Y
</p>
<p>&#8660;
</p>
<p>&#212;X&#213;
</p>
<p>&#212;Y&#213;
</p>
<p>II
</p>
<p>X
</p>
<p>Y
</p>
<p>1
</p>
<p>le livre vol&#142;   par Pierre   YX
</p>
<p>&#8660;
</p>
<p>&#212;X&#213;
</p>
<p>&#212;Y&#213;
</p>
<p>ATTR
</p>
<p>X
</p>
<p>Y
</p>
<p>1
(X est un N
et Y est un Adj) ou 
(X n&#8217;est pas un N 
et Y est un Adv)
</p>
<p>un gros   tas   ; tr&#143;s   gros   ; partir   viteYX YX XY</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Grammaires de d&#142;pendance formelles et th&#142;orie Sens-Texte
</p>
<p>Les r&#143;gles syntaxiques profondes sagittales traduisent une d&#142;pendance syntaxique profonde en
fonction de la nature des &#142;l&#142;ments qu&#8217;elle relient et de leurs tableaux de r&#142;gime. En particulier,
ces r&#143;gles introduisent les pr&#142;positions r&#142;gies (Figure 9).
Les r&#143;gles syntaxiques profondes nodales traduisent une lexie profonde. La plupart de ces
r&#143;gles sont contr&#153;l&#142;e par le lexique, comme l&#8217;expansion d&#8217;une locution, ou l&#8217;introduction de la
valeur d&#8217;une fonction lexicale. Les r&#143;gles syntaxiques profondes nodales comprennent
&#142;galement les r&#143;gles de pronominalisation : dans une cha&#148;ne de r&#142;f&#142;rence (c&#8217;est-&#136;-dire une cha&#148;ne
de lexies profonde qui correspondent &#136; un m&#144;me n&#711;ud s&#142;mantique), il faut remplacer sous des
conditions pr&#142;cises certaines lexies par des pronoms. Ces r&#143;gles n&#8217;ont pas fait l&#8217;objet d&#8217;une
&#142;tude s&#142;rieuse pour l&#8217;instant.
</p>
<p>Le module syntaxique profond contient &#142;galement des r&#143;gles grammaticales qui assurent la
traduction des gramm&#143;mes profonds, notamment ceux qui comme la d&#142;termination, la voix ou le
temps peuvent s&#8217;exprimer par des expressions analytiques comprenant des mots. L&#136; encore ces
r&#143;gles n&#8217;ont pas fait l&#8217;objet d&#8217;une &#142;tude s&#142;rieuse en TST. Nous en proposerons Section 4.2.1
dans le formalisme GUST.
</p>
<p>Figure 9 : Cinq r&#143;gles syntaxiques profondes sagittales
</p>
<p>Montrons comment on passe de la repr&#142;sentation syntaxique de (1) (Figure 6) &#136; sa
repr&#142;sentation syntaxique de surface (Figure 7). Comme le verbe ESSAYER est fini, l&#8217;actant I
de ESSAYER devient sujet. La r&#143;gle sagittale qui traduit l&#8217;actant II de ESSAYER doit, en
fonction du tableau de r&#142;gime de ESSAYER, introduire une relation syntaxique infinitive, la
pr&#142;position DE et le gramm&#143;me infinitif sur PARLER. L&#8217;actant I de PARLER est effac&#142; par la
r&#143;gle de &#210;pronominalisation&#211; de l&#8217;actant I d&#8217;un verbe &#136; l&#8217;infinitif. L&#8217;actant II de PARLER est
</p>
<p>&#8660;
X est un V 
fini
</p>
<p>Pierre   part   Y X
</p>
<p>I
</p>
<p>X
</p>
<p>Y
</p>
<p>suj
</p>
<p>X
</p>
<p>Y
</p>
<p>&#8660;
X est un N et 
Y est un 
pronom
</p>
<p>son    d&#142;part Y X
</p>
<p>I
</p>
<p>X
</p>
<p>Y
</p>
<p>d&#142;t
</p>
<p>X
</p>
<p>Y
</p>
<p>&#8660; X est un N et Y est un N 
quantitatif
</p>
<p>un verre   de vin   ; trois m&#143;tres   de tissuY X XY
</p>
<p>ATTR
</p>
<p>X
</p>
<p>Y
</p>
<p>cnom
</p>
<p>X
</p>
<p>Y
</p>
<p>pr&#142;p
DE
</p>
<p>&#8660; X est un N et Y est un N
</p>
<p>le d&#142;part    de PierreX Y
</p>
<p>I
</p>
<p>X
</p>
<p>Y
</p>
<p>cnom
</p>
<p>Y
</p>
<p>X
</p>
<p>pr&#142;p
DE
</p>
<p>&#8660;
X est un N et 
Y est un Adj
</p>
<p>le gros   tas   Y X
</p>
<p>ATTR
</p>
<p>X
</p>
<p>Y
</p>
<p>mod
</p>
<p>X
</p>
<p>Y</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Sylvain Kahane
</p>
<p>traduit, en fonction du tableau de r&#142;gime de PARLER, par la relation d&#8217;objet indirect (iobj) et la
pr&#142;position &#184;. Comme BEAU est un adjectif, la relation ATTR donne la relation syntaxique
modifieur. Enfin, comme DAME n&#8217;a pas de d&#142;terminant, le gramm&#143;me d&#142;fini sur DAME
donne le d&#142;terminant LE, reli&#142; &#136; DAME par une relation d&#142;terminative.
</p>
<p>3.3.5  Le module syntaxique de surface d&#8217;un mod&#143;le Sens-Texte
Le module syntaxique de surface r&#142;alise la correspondance entre le niveau syntaxique de surface
et le niveau morphologique profond. Le module syntaxique de surface assure la lin&#142;arisation,
l&#8217;accord et le r&#142;gime (Figure 10) (cf. Mel&#8217;&#139;uk &amp; Pertsov 1987 pour un fragment cons&#142;quent du
module syntaxique de l&#8217;anglais).
Les r&#143;gles de lin&#142;arisation indiquent comment un &#142;l&#142;ment se place par rapport &#136; son gouverneur
(X &lt; Y ou Y &lt; X). Mais, elles doivent aussi indiquer comment les diff&#142;rents d&#142;pendants d&#8217;un
m&#144;me n&#711;ud se placent les uns par rapport aux autres. Plusieurs techniques sont possibles : on
peut par exemple indiquer dans la r&#143;gle de lin&#142;arisation d&#8217;un d&#142;pendant quels sont les autres
d&#142;pendants qui peuvent se placer entre lui et son gouverneur (Mel&#8217;&#139;uk &amp; Pertsov 1987, Nasr
1996). Nous pr&#142;f&#142;rons encoder le placement des co-d&#142;pendants par une marque de position
indiquant la &#210;distance&#211; d&#8217;un d&#142;pendant donn&#142; au gouverneur (Mel&#8217;&#139;uk 1967, Courtin &amp;
Genthial 1998, Kahane 2000a). Comme m&#142;taphore, on peut voir les d&#142;pendances comme des
&#142;lastiques auxquels sont accroch&#142;s les mots avec un poids &#142;gal &#136; la valeur du trait position : plus
le poids est grand (en valeur absolu), plus le mot est loin de son gouverneur. On peut &#142;galement
voir les marques de position comme des adresses de positions pr&#142;cises ouvertes par le
gouverneur. Par exemple, un verbe fini ouvre 7 positions devant lui pour les clitiques (il &lt; ne &lt;
me &lt; le &lt; lui &lt; en &lt; y). Nous donnons Figure 10 les r&#143;gles de lin&#142;arisation du sujet : un sujet
non pronominal peut se placer devant le verbe &#136; la position -10 ou apr&#143;s le verbe &#136; la position
+10 sous certaines conditions, tandis qu&#8217;un sujet pronominal se cliticise et occupe la position -7
devant le verbe. De m&#144;me, un objet direct pronominal se cliticise et occupe la position -5 ou -4
selon sa personne.
</p>
<p>Figure 10 : Trois r&#143;gles syntaxiques de surface sagittales
</p>
<p>Notons que le placement des co-d&#142;pendants d&#142;pend &#142;galement de la taille du syntagme domin&#142;
par le d&#142;pendant (les gros syntagmes ont tendance &#136; &#144;tre plus &#142;loign&#142;s) et de la structure
communicative (les syntagmes les plus saillants communicativement ont tendance a &#144;tre plus
&#142;loign&#142;s); la marque de position devrait donc &#144;tre une fonction d&#142;pendant de la relation
syntaxique, de la taille du syntagme et de la saillance communicative.
</p>
<p>Pierre   part   Y X
</p>
<p>&#8660;suj
X
</p>
<p>Y
</p>
<p>Y  &lt;  X
-10      0
</p>
<p>Y n&#8217;est pas 
un pronom
</p>
<p>je   pars   Y X
</p>
<p>&#8660;suj
X
</p>
<p>Y
</p>
<p>Y  &lt;  X
-7       0
</p>
<p>Y est un 
pronom
</p>
<p>le livre que lit   Pierre  X Y
</p>
<p>&#8660;suj
X
</p>
<p>Y
</p>
<p>X  &lt;  Y
0    +10
</p>
<p>Y n&#8217;est pas un pronom et X 
est le verbe principal d&#8217;une 
proposition o&#157; un &#142;l&#142;ment a 
&#142;t&#142; extrait et o&#157; il n&#8217;y a pas 
d&#8217;objet direct non pronominal</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Grammaires de d&#142;pendance formelles et th&#142;orie Sens-Texte
</p>
<p>Pour le traitement des constructions non projectives, des r&#143;gles non locales sont n&#142;cessaires,
puisque l&#8217;&#142;l&#142;ment ne se place plus par rapport &#136; son gouverneur, mais par rapport &#136; un anc&#144;tre
plus &#142;loign&#142;.26
</p>
<p>Montrons comment on passe de la repr&#142;sentation syntaxique de surface de (1) (Figure 7) &#136; sa
repr&#142;sentation morphologique profonde (2). La racine ESSAYER de l&#8217;arbre syntaxique est
plac&#142;e en premier. Le sujet ZO&#131; est plac&#142; &#136; sa gauche et la pr&#142;position DE, qui est la t&#144;te de son
compl&#142;ment infinitif, &#136; sa droite, conform&#142;ment aux r&#143;gles de lin&#142;arisation des relations sujet et
infinitive. Le d&#142;pendant PARLER de la pr&#142;position DE est plac&#142; &#136; sa droite, puis la pr&#142;position
&#184;, qui est la t&#144;te de l&#8217;objet indirect de PARLER, &#136; sa droite, puis le nom DAME qui d&#142;pend de
&#184; &#136; sa droite. L&#8217;article LE et l&#8217;adjectif BEAU seront plac&#142;s &#136; gauche de DAME. En raison de la
projectivit&#142;, ils devront se placer entre DAME et son gouverneur &#184;. Enfin conform&#142;ment aux
marques de position des r&#143;gles de placement du d&#142;terminant et du modifieur, l&#8217;article LE sera
plac&#142; &#136; gauche de l&#8217;adjectif BEAU.
Nous terminons ici notre pr&#142;sentation de la TST standard. On trouvera une description des
r&#143;gles morphologiques dans Mel&#8217;&#139;uk 1993-2001.
</p>
<p>4 Une grammaire Sens-Texte bas&#142;e sur l&#8217;unification
Afin de proposer une version compl&#143;tement formalis&#142;e de la TST et d&#213;&#142;tablir le lien entre
l&#213;approche Sens-Texte et d&#213;autres approches, nous allons montrer comment les r&#143;gles de
correspondance d&#8217;un mod&#143;le Sens-Texte peuvent &#144;tre interpr&#142;t&#142;es comme des r&#143;gles g&#142;n&#142;ratives
bas&#142;e sur l&#213;unification, c&#8217;est-&#136;-dire comment un mod&#143;le Sens-Texte standard peut &#144;tre simul&#142;
par une grammaire qui g&#142;n&#143;re des portions de structures et les combine par unification. Le
formalisme que nous pr&#142;sentons sera appel&#142; GUST (Grammaire d&#8217;Unification Sens-Texte).
Nous ferons le lien entre GUST et d&#8217;autres formalismes bien connus comme HPSG et TAG,
dont il s&#8217;inspire d&#8217;ailleurs largement.
</p>
<p>4.1 Grammaires transductives et grammaires g&#142;n&#142;ratives
Les r&#143;gles de la TST sont des r&#143;gles qui mettent en correspondance deux fragments de deux
structures de deux niveaux de repr&#142;sentation adjacents (par exemple un fragment de structure
syntaxique de surface avec un fragment de cha&#148;ne morphologique profonde, c&#8217;est-&#136;-dire un
fragment d&#8217;arbre de d&#142;pendance avec un fragment d&#8217;ordre lin&#142;aire). Etant donn&#142;s deux
ensembles S et S&#8217;  de structures (graphes, arbres, suites, &#201;), nous appellerons grammaire
transductive entre S et S&#8217; une grammaire G qui met en correspondance des &#142;l&#142;ments de S et de
S&#8217; par un ensemble fini de r&#143;gles de correspondance qui mettent en correspondance un fragment
d&#8217;une structure de S avec un fragment d&#8217;une structure de S&#8217; (Kahane 2000b). Tous les modules
de la TST sont des grammaires transductives. Un mod&#143;le Sens-Texte, le mod&#143;le d&#8217;une langue
donn&#142;e, est encore une grammaire transductive obtenue par composition des diff&#142;rents modules
du mod&#143;le.27
</p>
<p>                                                
</p>
<p>26
 La r&#143;gle d&#8217;inversion du sujet que nous proposons Figure 10 devrait &#144;tre en fait une r&#143;gle non locale : le sujet
</p>
<p>invers&#142; ne se place pas par rapport &#136; son gouverneur, mais rapport au nucl&#142;us verbal qui contr&#153;le l&#8217;extraction.
Cf. Kahane 2000a pour une formalisation.
</p>
<p>27
 La compos&#142;e de deux grammaires transductives ne donnent pas trivialement une grammaire transductive. En
</p>
<p>effet, si G est une grammaire transductive entre S  et S&#8217; et G&#8217; une grammaire transductive entre S&#8217; et S&quot;, on
peut construire une grammaire transductive G&#959;G&#8217; entre S  et S&quot;, mais cette grammaire n&#8217;est pas obtenue en
composant simplement les r&#142;gles de G avec les r&#143;gles de G&#8217;. La difficult&#142; vient du fait que les fragments de
structure de S&#8217; consid&#142;r&#142;s par G ne sont pas forc&#142;ment les m&#144;mes que ceux consid&#142;r&#142;s par G&#8217;. Ainsi le module
syntaxique profond de la TST consid&#143;re comme fragments des portions importantes d&#8217;arbre syntaxique de</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Sylvain Kahane
</p>
<p>Remarquons qu&#8217;une grammaire transductive G entre S et S&#8217;  d&#142;finit davantage qu&#8217;une
correspondance entre S et S&#8217;. En effet, pour chaque couple (s,s&#8217;) de structures appartenant &#136; S
et S&#8217; et mises en correspondance par G (c&#8217;est-&#136;-dire par des r&#143;gles de correspondance qui vont
associer des fragments de s avec des fragments de s&#8217;), G d&#142;finit aussi des partitions de s et s &#8217;
(les fragments consid&#142;r&#142;s par les r&#143;gles) et une fonction &#981;(s,s&#8217;) entre ces partitions. Nous
appellerons cela une supercorrespondance (Kahane 2000b). Par exemple, le module syntaxique
de surface ne fait pas que mettre en correspondance des arbres de d&#142;pendance et des cha&#148;nes
morphologiques : pour chaque arbre et cha&#148;ne en correspondance, il met en correspondance les
n&#711;uds de l&#8217;arbre avec les &#142;l&#142;ments de la cha&#148;ne (par l&#8217;interm&#142;diaire des r&#143;gles nodales) (Figure
11).
La supercorrespondance entre S et S&#8217;  d&#142;finie par une grammaire transductive est
math&#142;matiquement &#142;quivalente &#136; l&#8217;ensemble des triplets (s,s&#8217;, &#981;(s,s&#8217;)) o&#157; s et s&#8217; sont des &#142;l&#142;ments
de S et S&#8217; mis en correspondance et &#981;(s,s&#8217;) est la fonction associant les partitions de s et s&#8217; d&#142;finies
par la mise en correspondance de s et s&#8217;. Un triplet de la forme (s,s&#8217;, &#981;(s,s&#8217;)) est en fait une
structure produit au sens math&#142;matique du terme, c&#8217;est &#136; dire une structure complexe obtenue
par l&#8217;enchev&#144;trement de deux structures (l&#8217;enchev&#144;trement est du au fait que, en un sens, les
deux structures sont d&#142;finies sur le m&#144;me ensemble &#8212; l&#8217;ensemble des fragments mis en
correspondance). Pour prendre l&#8217;exemple du module syntaxique de surface d&#8217;un MST, si s est
un arbre de d&#142;pendance, s&#8217; une suite et &#981;(s,s&#8217;) une correspondance entre les n&#711;uds de s et les
&#142;l&#142;ments de s&#8217;, le triplet (s,s&#8217;, &#981;(s,s&#8217;)) n&#8217;est autre qu&#8217;un arbre ordonn&#142; (Figure 11).
</p>
<p>Figure 11 : Equivalence entre un arbre et une suite en correspondance et un arbre ordonn&#142;
</p>
<p>Une grammaire transductive entre S et S&#8217;  peut  &#144;tre simul&#142;e par une grammaire g&#142;n&#142;rative qui
g&#142;n&#143;re l&#8217;ensemble des triplets (s,s&#8217;, &#981;(s,s&#8217;)) d&#142;crit par G. Les r&#143;gles de correspondance sont alors
vues comme des r&#143;gles g&#142;n&#142;rant des fragments de structure produit. Nous allons d&#142;velopper
cette id&#142;e dans la suite.
</p>
<p>Inversement, les grammaires g&#142;n&#142;ratives qui g&#142;n&#143;rent des structures produits peuvent vues
comme des grammaires transductives. Par exemple, les grammaires de Gaifman-Hays, qui
g&#142;n&#143;rent  des arbres de d&#142;pendance ordonn&#142;s, peuvent &#144;tre vue comme des grammaires mettant
en correspondance des arbres de d&#142;pendance non ordonn&#142;s avec des suites, c&#8217;est-&#136;-dire comme
une impl&#142;mentation d&#8217;un module syntaxique de surface de la TST. Notons quand m&#144;me que les
                                                                                                                                                        
</p>
<p>surface qui correspondent dans la structure syntaxique profonde &#136; un seul n&#711;ud (par exemple pour la r&#143;gle
d&#8217;expansion d&#8217;une locution), alors que le module syntaxique de surface n&#8217;en consid&#143;re pratiquement pas (et pas
les m&#144;mes).
</p>
<p>suj inf
essaye
</p>
<p>de
pr&#142;p
</p>
<p>Zo&#142;
</p>
<p>iobj
</p>
<p>d&#142;t mod
</p>
<p>parler
</p>
<p>bellela
</p>
<p>&#136;
</p>
<p>pr&#142;p
</p>
<p>&#221;
</p>
<p>inf pr&#142;p iobj
&#220;
</p>
<p>suj
</p>
<p>Zo&#142; essaye  de  parler     &#136;       la    belle dame
</p>
<p>pr&#142;p
</p>
<p>&#220;
</p>
<p>d&#142;t
mod&#221; &#221;
</p>
<p>&#221;
</p>
<p>Zo&#142; essaye  de  parler     &#136;       la    belle dame
</p>
<p>&#8801;
</p>
<p>dame
</p>
<p>&#220;</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Grammaires de d&#142;pendance formelles et th&#142;orie Sens-Texte
</p>
<p>grammaires de Gaifman-Hays cherchent aussi &#136; assurer la bonne formation des arbres de
d&#142;pendance, alors que, dans le cadre de la TST, celle-ci r&#142;sulte de l&#8217;interaction des diff&#142;rents
modules.
</p>
<p>4.2 Grammaire d&#8217;Unification Sens-Texte
Nous allons maintenant montrer comment un mod&#143;le Sens-Texte peut &#144;tre simul&#142; par une
grammaire g&#142;n&#142;rative bas&#142;e sur l&#8217;unification, que nous appelons GUST (Grammaire
d&#8217;Unification Sens-Texte).28 Le formalisme de GUST s&#8217;inspire de la grammaire de Nasr (1995,
1996; Kahane 2000a), elle-m&#144;me inspir&#142;e des grammaires TAG lexicalis&#142;es (Schabes 1990,
Abeill&#142; 1991, XTAG 1995, Candito 1999)..29
</p>
<p>Nous ne consid&#142;rons que 3 des 7 niveaux de repr&#142;sentations de la TST : le niveau s&#142;mantique,
le niveau syntaxique (de surface) et le niveau morphologique (profond). Le 4&#143;me et dernier
niveau consid&#142;r&#142; sera le texte lui-m&#144;me, c&#8217;est-&#136;-dire la s&#142;quence des caract&#143;res de la phrase.
Nous aurons ainsi trois modules (modules s&#142;mantique, syntaxique et morphologique). Nous
allons les pr&#142;senter maintenant, puis nous &#142;tudierons leurs diff&#142;rentes combinaisons, ce qui
nous permettra de faire le lien avec les grammaires compl&#143;tement lexicalis&#142;es comme TAG.
</p>
<p>4.2.1  Module s&#142;mantique de GUST
</p>
<p>Le module s&#142;mantique de GUST assure directement la correspondance entre le niveau
s&#142;mantique et le niveau syntaxique de surface, sans consid&#142;rer un niveau syntaxique profond
interm&#142;diaire. Nous consid&#142;rons deux types de r&#143;gles : des r&#143;gles s&#142;mantiques lexicales, qui
manipulent la configuration s&#142;mantique form&#142;e d&#8217;un s&#142;mant&#143;me lexical et de ses arguments
(plus exactement des d&#142;pendances s&#142;mantiques vers ses arguments), et des r&#143;gles s&#142;mantiques
grammaticales, qui manipulent un s&#142;mant&#143;me grammatical (et la d&#142;pendance vers son
argument). Ces deux types de r&#143;gles suffisent &#136; assurer la correspondance entre un graphe
s&#142;mantique et un arbre syntaxique de surface, puisque n&#8217;importe quel graphe s&#142;mantique peut
&#144;tre partitionn&#142; en un ensemble de configurations prises en entr&#142;e par nos r&#143;gles s&#142;mantiques
lexicales et grammaticales.
</p>
<p>On voit Figure 12 la r&#143;gle qui donne la r&#142;alisation syntaxique de surface de la configuration
s&#142;mantique compos&#142;e du pr&#142;dicat &#212;parler&#213; et de ces deux premiers arguments. Dans la TST
standard, cette information se trouve dans l&#8217;entr&#142;e de dictionnaire de PARLER. En un sens, le
dictionnaire ne dit pas comment obtenir la correspondance entre ces deux configurations, et
l&#8217;information de la Figure 12 est en fait le r&#142;sultat de la composition de plusieurs r&#143;gles nodales
et sagittales s&#142;mantiques et syntaxiques profondes d&#142;clench&#142;es sous le contr&#153;le de l&#8217;entr&#142;e de
dictionnaire de PARLER. Dans la r&#143;gle de la Figure 12, il est &#142;galement indiqu&#142; que PARLER
est un verbe et que ce verbe doit recevoir des gramm&#143;mes profonds de mode, temps et voix. Les
fl&#143;ches (&#8594;) qui pr&#142;c&#143;dent ces gramm&#143;mes indiquent que ceux-ci ne sont pas encore exprim&#142;s.
Ils le seront par des r&#143;gles s&#142;mantiques grammaticales qui seront obligatoirement d&#142;clench&#142;es
(en exigeant que les fl&#143;ches aient disparues dans une repr&#142;sentation syntaxique bien form&#142;e).
</p>
<p>                                                
</p>
<p>28
 GUST n&#8217;est pas exactement une autre pr&#142;sentation de la TST. Certains choix th&#142;oriques peuvent &#144;tre diff&#142;rents
</p>
<p>et nous pensons que ce formalisme permet de r&#142;soudre certaines questions dont le traitement classique en TST
n&#8217;est pas tr&#143;s clair, notamment tout ce qui concerne l&#8217;interaction entre les diff&#142;rentes r&#143;gles d&#8217;un m&#144;me module
ou de deux modules adjacents.
</p>
<p>29
 Voir &#142;galement Hellwig 1986 pour une proposition ant&#142;rieure de grammaire de d&#142;pendance bas&#142;e sur
</p>
<p>l&#8217;unification, appel&#142;e DUG (Dependency Unification Grammar).</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Sylvain Kahane
</p>
<p>Figure 12 : Une r&#143;gle de correspondance TST (s&#142;mantico-syntaxique profonde)
</p>
<p>Nous proposons Figure 13 la r&#143;gle de GUST qui simule la r&#143;gle TST de la Figure 12. Au lieu
de mettre en correspondance un fragment de structure s&#142;mantique avec un fragment de structure
syntaxique, cette r&#143;gle propose un fragment de structure produit s&#142;mantique-syntaxique,
exprimant &#136; la fois la relation entre le s&#142;mant&#143;me &#212;parler&#213; et la lexie PARLER et les relations
entre les arguments 1 et 2 de &#212;parler&#213; et les sujet et objet indirect (iobj) de PARLER. Dans la
r&#143;gle de la Figure 13, l&#8217;arbre syntaxique est repr&#142;sent&#142; explicitement (ce qui donne une certaine
primaut&#142; &#136; la syntaxe), alors que le graphe s&#142;mantique est encod&#142; dans l&#8217;&#142;tiquetage des n&#711;uds
par l&#8217;interm&#142;diaire des traits s&#142;m, arg1 et arg2. Chaque n&#711;ud poss&#143;de un trait s&#142;m dont la
valeur est un s&#142;mant&#143;me, le signifi&#142; de la lexie &#142;tiquetant ce n&#711;ud. Lorsque ce s&#142;mant&#143;me a des
arguments, ceux-ci sont les valeurs des traits arg1, arg2, etc., valeurs qui sont partag&#142;es avec
les traits s&#142;m des n&#711;uds syntaxiques qui r&#142;alisent ces arguments. Le partage d&#8217;une valeur par
plusieurs traits est indiqu&#142; par une variable, la valeur elle-m&#144;me n&#8217;&#142;tant indiqu&#142;e qu&#8217;une fois.30
Notons encore que les mots vides portent un trait &#180;s&#142;m qui bloquera l&#8217;unification avec une
&#142;tiquette portant le trait s&#142;m.
</p>
<p>Figure 13 : Une r&#143;gle s&#142;mantique lexicale GUST
</p>
<p>Les r&#143;gles lexicales se combinent par unification. Nous pr&#142;sentons Figure 14 la d&#142;rivation de la
phrase Le petit chat dort ici par combinaison des r&#143;gles lexicales associ&#142;es aux lexies de cette
phrase (il s&#8217;agit en fait de r&#143;gles lexicales sur lesquelles ont d&#142;j&#136; &#142;t&#142; appliqu&#142;es les r&#143;gles
grammaticales, comme on le verra plus loin). Deux r&#143;gles se combinent par fusion de deux
n&#711;uds et unification des &#142;tiquettes correspondantes. Comme nous le verrons dans la suite,
plusieurs n&#711;uds, ainsi que des d&#142;pendances, peuvent fusionner lors de la combinaison de deux
r&#143;gles. Le r&#142;sultat d&#8217;une d&#142;rivation est bien form&#142; si cette d&#142;rivation met bien en correspondance
</p>
<p>                                                
</p>
<p>30
 Le fait de faire partager une m&#144;me valeur &#136; plusieurs traits est une technique bien connue dans les formalismes
</p>
<p>bas&#142;s sur l&#8217;unification. Voir l&#8217;usage intensif qu&#8217;en fait par exemple le formalisme HPSG (Pollard &amp; Sag
1994).
</p>
<p>suj iobj
PARLER
</p>
<p>X
&#184;
</p>
<p>pr&#142;p
</p>
<p>Y
</p>
<p>&#8660;1 2
&#212;parler&#213; 
</p>
<p>&#212;X&#213; &#212;Y&#213; (N)
</p>
<p>(N)
</p>
<p>(Pr&#142;p)
</p>
<p>(V)&#8594;m,&#8594;t,&#8594;v
</p>
<p>s&#142;mantique syntaxique (de surface)
</p>
<p>suj iobj
</p>
<p>PARLER
(V)&#8594;m,&#8594;t,&#8594;v
s&#142;m: &#212;parler&#213;
arg1: x
arg2: y
</p>
<p>&#184;
</p>
<p>(Pr&#142;p)
&#180;s&#142;m
</p>
<p>pr&#142;p
</p>
<p>(N)
s&#142;m: y
</p>
<p>(N)
s&#142;m: x</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Grammaires de d&#142;pendance formelles et th&#142;orie Sens-Texte
</p>
<p>un graphe s&#142;mantique connexe avec un arbre de d&#142;pendance, c&#8217;est-&#136;-dire si le r&#142;sultat est un
arbre dont tous les traits s&#142;m sont instanci&#142;s (certains traits ont pour valeur une variable qui
indique en fait l&#8217;adresse de la valeur d&#8217;un autre trait qui est instanci&#142;).
</p>
<p>Figure 14 : D&#142;rivation de Le petit chat dort ici
</p>
<p>Avant de revenir sur les r&#143;gles lexicales, nous allons pr&#142;senter les r&#143;gles s&#142;mantiques
grammaticales. Les gramm&#143;mes profonds sont calcul&#142;s &#136; partir de la repr&#142;sentation s&#142;mantique :
certains yapparaissent explicitement comme des s&#142;mant&#143;mes grammaticaux, d&#8217;autres seront
calcul&#142;s &#136; partir de la structure communicative s&#142;mantique (comme la voix qui d&#142;pend en partie
de la partition th&#143;me-rh&#143;me) et d&#8217;autres encore sont impos&#142;s par la rection (comme le mode
infinitif). Il n&#8217;est pas ais&#142; de traiter la combinaison entre une r&#143;gle lexicale et une r&#143;gle
grammaticale par unification, car un gramm&#143;me ne fait pas qu&#8217;ajouter de l&#8217;information : il peut
aussi entra&#148;ner une modification importante du comportement de la lexie qu&#8217;il sp&#142;cifie. C&#8217;est le
cas, par exemple, d&#8217;un gramm&#143;me de voix passive qui entra&#148;ne une redistribution des fonctions
des actants syntaxiques de la lexie (l&#8217;objet devient sujet et le sujet devient un compl&#142;ment
d&#8217;agent).
Dans un premier temps, nous allons traiter les r&#143;gles grammaticales comme des op&#142;rateurs qui
associent &#136; une r&#143;gle lexicale une nouvelle r&#143;gle lexicale (o&#157; un gramm&#143;me profond
suppl&#142;mentaire est exprim&#142;). Nous pr&#142;sentons Figure 15 les r&#143;gles pour le pr&#142;sent, le pass&#142;
compos&#142; et la voix passive. Le pass&#142; compos&#142; d&#8217;un verbe X  est exprim&#142; par l&#8217;auxiliaire
AVOIR31 au pr&#142;sent et le verbe X au participe pass&#142;. L&#8217;auxiliaire AVOIR est l&#8217;auxiliaire par
d&#142;faut : si X  poss&#143;de un trait aux indiquant un autre auxiliaire (par exemple &#230;TRE), la valeur
@aux de ce trait sera utilis&#142;e &#136; la place de AVOIR. 32 Le s&#142;mant&#143;me &#212;pass&#142; compos&#142;&#213; appara&#148;t
dans l&#8217;&#142;tiquette de X (son argument est la valeur du trait s&#142;m de X), mais le gramm&#143;me profond
pass&#142; compos&#142; n&#8217;appara&#148;t pas en tant que tel. Seuls apparaissent les gramm&#143;mes de surface
tels que pr&#142;sent ou p-pass&#142; (participe pass&#142;). Notez &#142;galement le positionnement des traits
pour le mode sur l&#8217;auxiliaire et de la voix sur le verbe X.
</p>
<p>                                                
</p>
<p>31
 Nous passons sous silence la question de la s&#142;mantique de l&#8217;auxiliaire. Nous devons assurer que les modifieurs
</p>
<p>de la forme verbale compos&#142;e qui d&#142;pendent s&#142;mantiquement de l&#8217;auxiliaire prennent bien le signifi&#142; du verbe
comme argument s&#142;mantique.
</p>
<p>32
 La notation a//b utilis&#142;e dans les r&#143;gles comme valeur d&#8217;un trait signifie : la valeur est a, si la valeur b ne peut
</p>
<p>&#144;tre trouv&#142;e, et b sinon.
</p>
<p>(N)
s&#142;m: x
</p>
<p>mod
</p>
<p>PETIT
(Adj)
s&#142;m: &#212;petit&#213;
arg1: x
</p>
<p>DORMIR
(V)ind,pr&#142;sent
t: &#212;pr&#142;sent&#213;
s&#142;m: &#212;dormir&#213;
arg1: x
</p>
<p>(N)
s&#142;m: x
</p>
<p>sujet
</p>
<p>d&#142;t
</p>
<p>LE
(D&#142;t)
&#180;s&#142;m
</p>
<p>(N)
s&#142;m: x
</p>
<p>d: &#212;d&#142;fini&#213;
CHAT
(N,masc)sg,&#8594;d
s&#142;m: &#212;chat&#213;
</p>
<p>(V)
s&#142;m: x
</p>
<p>adv
</p>
<p>ICI
(Adv)
s&#142;m: &#212;ici&#213;
arg1: x
</p>
<p>suj
</p>
<p>d&#142;t mod
</p>
<p>adv
</p>
<p>LE
(D&#142;t)
&#180;s&#142;m
</p>
<p>PETIT
(Adj)
s&#142;m: &#212;petit&#213;
arg1: x
</p>
<p>CHAT
(N,masc)sg
d: &#212;d&#142;fini&#213;
s&#142;m: x &#212;chat&#213;
</p>
<p>DORMIR
(V)ind,pr&#142;sent
t: &#212;pr&#142;sent&#213;
s&#142;m: e &#212;dormir&#213;
arg1:  x
</p>
<p>ICI
(Adv)
s&#142;m: &#212;ici&#213;
arg1: e 
</p>
<p>&#8658;</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Sylvain Kahane
</p>
<p>Figure 15 : R&#143;gles grammaticales GUST pour le pr&#142;sent, le pass&#142; compos&#142;
et la voix passive (version op&#142;rateur)
</p>
<p>Nous proposons Figure 16 une autre version de la r&#143;gle grammaticale pour le pass&#142;
compos&#142;. Cette r&#143;gle se combine par unification avec la r&#143;gle lexicale d&#8217;un verbe. La mont&#142;e
du sujet Y de X  sur l&#8217;auxiliaire est assur&#142;e par la fl&#143;che &#142;tiquet&#142;e sujet de X  &#136; Y . Cette fl&#143;che,
que nous appelons une quasi-d&#142;pendance, va fusionner avec la d&#142;pendance sujet de X  (dans sa
r&#143;gle lexicale) et tuer cette d&#142;pendance. Nous proposons &#142;galement une r&#143;gle grammaticale pour
le d&#142;fini lorsqu&#8217;il est exprim&#142; par l&#8217;article LE. La d&#142;termination (d&#142;fini, ind&#142;fini, partitif)
est un gramm&#143;me profond qui a une expression purement analytique et ne donne donc pas de
gramm&#143;me de surface.33
</p>
<p>                                                
</p>
<p>33
 Ce comportement marginal de la d&#142;termination peut pousser certains &#136; ne pas traiter la d&#142;termination comme
</p>
<p>une cat&#142;gorie flexionnelle et &#136; pr&#142;f&#142;rer traiter les lexies LE ou UN comme des lexies pleines exprimant les
sens &#212;d&#142;fini&#213; et &#212;ind&#142;fini&#213;. Nous pr&#142;f&#142;rons notre solution. Cette solution, par l&#8217;obligation d&#8217;exprimer la
d&#142;termination, r&#143;gle aussi le probl&#143;me de la pr&#142;sence obligatoire du d&#142;terminant.
</p>
<p>suj &#8658;
</p>
<p>Y
</p>
<p>suj aux
</p>
<p>Y
</p>
<p>AVOIR//@aux
(V)&#8594;m,pr&#142;sent
</p>
<p>X
(V)p-pass&#142;,&#8594;v
t: &#212;pass&#142; compos&#142;&#213;
</p>
<p>suj aux
</p>
<p>(obl)
PAR//@pr&#142;p-passif
&#180;s&#142;mpr&#142;p
</p>
<p>suj dobj
</p>
<p>Y Z Y
</p>
<p>Z
</p>
<p>&#230;TRE
(V)&#8594;m,&#8594;t
</p>
<p>X
(V)p-pass&#142;&#8658;
</p>
<p>X
(V)&#8594;m,&#8594;t,&#8594;v
</p>
<p>X
(V)&#8594;m,pr&#142;sent,&#8594;v
t: &#212;pr&#142;sent&#213;&#8658;
</p>
<p>pr&#142;sent
</p>
<p>pass&#142; compos&#142;
</p>
<p>passif
</p>
<p>X
(V)&#8594;m,&#8594;t,&#8594;v
</p>
<p>X
(V)&#8594;m,&#8594;t,&#8594;v</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Grammaires de d&#142;pendance formelles et th&#142;orie Sens-Texte
</p>
<p>Figure 16 : R&#143;gles grammaticales GUST pour le pass&#142; compos&#142; et le d&#142;fini
(version unification)
</p>
<p>Nous allons maintenant montrer comment sont trait&#142;s quelques ph&#142;nom&#143;nes linguistiques en
proposant d&#8217;autres r&#143;gles s&#142;mantiques lexicales. Nous donnons Figure 17 la r&#143;gle pour la
locution LA MOUTARDE MONTER AU NEZ. La r&#143;gle s&#142;mantique pour une locution fait
correspondre un s&#142;mant&#143;me &#136; une configuration de lexies de surface. D&#158; au fait que seul la
racine de cette configuration accepte des modifications (cf. (4)), seule la racine de l&#8217;arbre aura
un trait s&#142;m (instanci&#142; par le signifi&#142; de locution), tandis que les autres n&#711;uds auront un trait
&#180;s&#142;m qui bloquera toute modification (puisqu&#8217;un modifieur est un pr&#142;dicat qui prend son
gouverneur comme argument et exige donc que celui-ci ait un trait s&#142;m (cf. les r&#143;gles pour
PETIT et ICI de la Figure 14). Un verbe avalent (sans argument) comme PLEUVOIR aura une
r&#143;gle similaire &#136; celle d&#8217;une locution, avec un n&#711;ud &#180;s&#142;m pour le sujet vide.34
</p>
<p>(4) a. La moutarde me monte s&#142;rieusement au nez
b. *La moutarde forte me monte au nez
</p>
<p>Figure 17 : R&#143;gles lexicales pour une locution et pour un verbe avalent
</p>
<p>Le contraste entre verbe &#136; contr&#153;le (comme ESSAYER) et verbes &#136; mont&#142;e (comme
COMMENCER) est traditionnellement encod&#142; dans les grammaires syntagmatiques dans la
structure syntaxique. Dans notre approche, les deux types de verbes ont exactement la m&#144;me
repr&#142;sentation syntaxique : le verbe gouverne un sujet et un infinitif qui partage avec le verbe le
m&#144;me sujet (nous reviendrons sur la relation sujet de l&#8217;infinitif). Le contraste vient de la
                                                
</p>
<p>34
 Le traitement est diff&#142;rent en TST o&#157; l&#8217;introduction d&#8217;un sujet vide r&#142;sulte d&#8217;une r&#143;gle grammaticale syntaxique
</p>
<p>profonde. D&#8217;ailleurs, notre traitement n&#8217;est pas enti&#143;rement satisfaisant. Il serait probablement pr&#142;f&#142;rable de
traiter le sujet de PLEUVOIR comme un &#142;l&#142;ment grammatical et non comme une portion de locution,
puisque celui-ci n&#8217;appara&#148;t pas dans certaines constructions comme Dieu fait pleuvoir.
</p>
<p>suj aux
suj
</p>
<p>Y
</p>
<p>AVOIR//@aux
</p>
<p>(V)&#8594;m,pr&#142;sent
</p>
<p>X
</p>
<p>(V)p-pass&#142;,&#8594;v
</p>
<p>: &#212;pass&#142; compos&#142;&#213;
</p>
<p>d&#142;t
</p>
<p>LE
</p>
<p>(D&#142;t)
</p>
<p>&#180;s m
</p>
<p>N)
</p>
<p>s&#142;m: x
</p>
<p>d: &#212;d&#142;fini&#213;
</p>
<p>MONTER
</p>
<p>(V)&#8594;m,&#8594;t,&#8594;v
</p>
<p>s&#142; : &#212;la moutarde monter au nez&#213;
</p>
<p>arg1: x
</p>
<p>&#184;, (Pr&#142;p), &#180;s&#142;m
</p>
<p>pr&#142;p
</p>
<p>NEZ
</p>
<p>(N masc)sg
</p>
<p>&#180;s&#142;
</p>
<p>MOUTARDE
</p>
<p>(N,f&#142;m sg
</p>
<p>&#180;s&#142;m
</p>
<p>d t
</p>
<p>d t
</p>
<p>LE
</p>
<p>D&#142;t)
</p>
<p>&#180;s m
</p>
<p>LE, (D&#142;t), &#180;s&#142;m
</p>
<p>(N,pro)
</p>
<p>m: x
</p>
<p>suj
</p>
<p>iobj
</p>
<p>PLEUVOIR
</p>
<p>(V)&#8594; ,&#8594;t,&#8594;v
</p>
<p>s&#142;m: &#212;pleuvoir&#213;
</p>
<p>IL
</p>
<p>(N)c
</p>
<p>s
</p>
<p>suj</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Sylvain Kahane
</p>
<p>repr&#142;sentation s&#142;mantique : un verbe &#136; contr&#153;le prend son sujet comme argument s&#142;mantique,
mais pas un verbe &#136; mont&#142;e (Figure 18). Dans les deux cas, l&#8217;infinitif contr&#153;le le sujet de son
gouverneur et il faut un moyen d&#8217;assurer cela. Pour cela, nous consid&#142;rons qu&#8217;un infinitif
poss&#143;de une sorte de d&#142;pendance sujet ; ce lien s&#8217;apparente &#136; une d&#142;pendance, mais n&#8217;en est pas
une, car il ne compte pas dans la structure d&#8217;arbre et il n&#8217;est pas pris en compte dans la
lin&#142;arisation (cf. Hudson 2000 pour une proposition similaire). Un tel lien sera appel&#142; une
quasi-d&#142;pendance. De m&#144;me, on aura dans la r&#143;gle d&#8217;un verbe &#136; contr&#153;le ou &#136; mont&#142;e une quasi-
d&#142;pendance sujet pour l&#8217;infinitif avec laquelle la quasi d&#142;pendance de la r&#143;gle de l&#8217;infinitif devra
s&#8217;unifier. La quasi-d&#142;pendance est donc juste un moyen assez simple d&#8217;assurer le contr&#153;le  du
sujet du verbe &#136; contr&#153;le ou &#136; mont&#142;e par l&#8217;infinitif. Notons que ce contr&#153;le est bien syntaxique :
il s&#8217;agit du sujet du verbe infinitif et pas d&#8217;un argument s&#142;mantique pr&#142;cis. En effet, il peut
s&#8217;agir d&#8217;un sujet s&#142;mantique vide (5a), d&#8217;un sujet qui fait partie d&#8217;une locution (5b) et, lorsque
ce sujet est plein, il peut s&#8217;agir aussi bien du premier argument (5c) que du second argument
(5d). En cons&#142;quence, les infinitifs doivent avoir un sujet dans leur repr&#142;sentation syntaxique,
mais ce sujet sera une quasi-d&#142;pendance (afin d&#8217;&#142;viter qu&#8217;un verbe infinitif ait un vrai sujet). La
r&#143;gle s&#142;mantique du gramm&#143;me infinitif devra assurer que la relation sujet devienne bien une
quasi-d&#142;pendance .
</p>
<p> (5) a. Il commence &#136; pleuvoir.
b. La moutarde commence &#136; lui monter au nez.
c. Le bruit commence &#136; g&#142;ner le gar&#141;on.
d. Le gar&#141;on commence &#136; &#144;tre gen&#142; par le bruit.
</p>
<p>Figure 18 : R&#143;gles lexicales pour un verbe &#136; contr&#153;le et un verbe &#136; mont&#142;e
</p>
<p>Remarquons que le fait qu&#8217;un verbe &#136; contr&#153;le prenne son sujet comme argument s&#142;mantique
suffit &#136; &#142;viter que ce verbe ait un sujet vide (ce qui est un contraste bien connu entre verbes &#136;
contr&#153;le et verbes &#136; mont&#142;e) :
(6) a. *Il essaye de pleuvoir.
</p>
<p>b. *La moutarde essaye de lui monter au nez.
</p>
<p>Les verbes copules, c&#8217;est-&#136;-dire les verbes prenant un attribut, seront trait&#142;s de fa&#141;on similaire
aux verbes &#136; mont&#142;e : dans la r&#143;gle s&#142;mantique lexicale d&#8217;un verbe copule, on aura une quasi-
d&#142;pendance modifieur indiquant le lien entre le d&#142;pendant du verbe copule &#210;modifi&#142;&#211; par
l&#8217;adjectif attribut et l&#8217;adjectif lui-m&#144;me (Figure 19). Cette quasi-d&#142;pendance permet &#136; la fois &#136;
l&#8217;adjectif de r&#142;cup&#142;rer son argument s&#142;mantique et d&#8217;assurer l&#8217;accord de l&#8217;adjectif avec le nom
&#210;modifi&#142;&#211; par la r&#143;gle d&#8217;accord ordinaire de l&#8217;adjectif avec le nom qu&#8217;il modifie (et sans qu&#8217;il
soit n&#142;cessaire de faire circuler de l&#8217;information au travers du verbe copule). Enfin, cette
solution permet d&#8217;utiliser la m&#144;me r&#143;gle lexicale pour l&#8217;adjectif qu&#8217;il soit &#142;pith&#143;te (7a) ou qu&#8217;il
contr&#153;le le sujet (7b) ou l&#8217;objet (7c) (voir Figure 19 la r&#143;gle pour l&#8217;adjectif PETIT).
</p>
<p>suj inf
</p>
<p>COMMENCER
(V)&#8594;m,&#8594;t,&#8594;v
s&#142;m: &#212;commencer&#213;
arg1: x
</p>
<p>&#184;
</p>
<p>(Pr&#142;p)
&#180;s&#142;m
</p>
<p>pr&#142;p
</p>
<p>(V)inf
s&#142;m: x
</p>
<p>suj
           
          
sujinf
ESSAYER
(V)&#8594;m,&#8594;t,&#8594;v
s&#142;m: &#212;essayer&#213;
arg1: x
arg2: y
DE
(Pr&#142;p)
s&#142;m
pr&#142;p
(V)inf
s&#142;m: y
(N)
s&#142;m: x
suj</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Grammaires de d&#142;pendance formelles et th&#142;orie Sens-Texte
</p>
<p> (7) a. un petit livre
b. ce livre est petit
c. Pierre trouve ce livre petit
</p>
<p>Figure 19 : R&#143;gles lexicales pour les verbes copules et les adjectifs
</p>
<p>Le ph&#142;nom&#143;ne dit du tough-movement peut &#144;tre d&#142;crit de la m&#144;me fa&#141;on. Quand un adjectif tel
que FACILE gouverne un verbe, le nom que modifie l&#8217;adjectif n&#8217;est pas son argument
s&#142;mantique mais un argument s&#142;mantique du verbe (8a). De plus, le nom doit remplir le r&#153;le
d&#8217;objet direct du verbe. Par cons&#142;quent, la r&#143;gle s&#142;mantique de FACILE contient une quasi-
d&#142;pendance objet direct entre le verbe gouvern&#142; et le nom modifi&#142; (Figure 19). Ainsi seul un
verbe pourvu d&#8217;un objet direct peut se combiner avec FACILE et l&#8217;&#210;extraction&#211; de l&#8217;objet direct
du verbe sera assur&#142;e par l&#8217;unification de la d&#142;pendance objet direct du verbe avec la quasi-
d&#142;pendance de m&#144;me r&#153;le de la r&#143;gle de FACILE. On peut remarquer que la r&#143;gle de FACILE
peut aussi se combiner avec un verbe copule (8b,c).
</p>
<p> (8) a. un livre facile &#136; lire
b. ce livre est facile &#136; lire
c. Pierre trouve ce livre facile &#136; lire
</p>
<p>Nous arr&#144;terons l&#136; la pr&#142;sentation du module s&#142;mantique de GUST. Comme on l&#8217;a vu, l&#8217;un des
objectifs de GUST est d&#8217;&#142;viter la multiplication des r&#143;gles associ&#142; &#136; une lexie. Sur le fragment
de grammaire propos&#142;, on a pu couvrir avec une seule r&#143;gle lexicale par lexie un grand nombre
de constructions diverses. De m&#144;me, par la combinaison avec les r&#143;gles grammaticales, on
construit les r&#143;gles des diff&#142;rentes formes d&#8217;un verbe &#136; partir d&#8217;une seule r&#143;gle lexicale.35
</p>
<p>4.2.2  Module syntaxique de GUST
</p>
<p>Le module syntaxique de GUST correspond au module syntaxique de surface de la TST : il
assure la correspondance entre le niveau syntaxique de surface et le niveau morphologique
profond. Le module syntaxique poss&#143;de trois types de r&#143;gles : des r&#143;gles d&#8217;accord, des r&#143;gles
de r&#143;gime et des r&#143;gles de lin&#142;arisation. On voit Figure 20 la r&#143;gle d&#8217;accord du verbe avec son
sujet et la r&#143;gle de rection des pronoms sujet (qui re&#141;oivent le nominatif). La r&#143;gle d&#8217;accord du
sujet indique que le sujet s&#8217;accorde en nombre et en personne avec son sujet (9a). Lorsque le
                                                
</p>
<p>35
 Nous n&#8217;avons pas discut&#142; des diff&#142;rentes sous-cat&#142;gorisations d&#8217;une m&#144;me lexie (par exemple, demander N &#136; N,
</p>
<p>que Vsubj, &#136; Vinf, &#201;). Tel que nous avons pr&#142;sent&#142; le formalisme, nous devrions introduire une r&#143;gle pour
chaque sous-cat&#142;gorisation. Pour des questions d&#8217;efficacit&#142; de l&#8217;analyse automatique ou de pertinence cognitive
(cf. Section 5.3), nous pensons pr&#142;f&#142;rable, tant que cela est possible, de rassembler ces diff&#142;rentes sous-
cat&#142;gorisations dans une m&#144;me r&#143;gle. Nous devrons alors de introduire des disjonctions et consid&#142;rer des
d&#142;pendances optionnelles.
</p>
<p>suj pr&#142;d
</p>
<p>&#230;TRE
(V)&#8594;m,&#8594;t,&#8594;v
</p>
<p>(Adj)mod           
          
</p>
<p>suj
</p>
<p>TROUVER
(V)&#8594;m,&#8594;t,&#8594;v
s&#142;m: &#212;trouver&#213;
arg1: x
arg2: y
</p>
<p>(Adj)
s&#142;m: y
</p>
<p>mod
</p>
<p>(N)
s&#142;m: x
</p>
<p>         
</p>
<p>dobj pr&#142;d
</p>
<p>(N)
s&#142;m: x
</p>
<p>od
</p>
<p>PETIT
Adj)
</p>
<p> &#212;petit&#213;
arg1
</p>
<p>(N)
mod
</p>
<p>FACILE
( dj)
s&#142;m: &#212;facile&#213;
arg1: xinf
</p>
<p>pr&#142;p
&#184;, (Pr&#142;p), &#180;s&#142;m
(V)inf
s&#142;m: x
</p>
<p>dobj</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Sylvain Kahane
</p>
<p>sujet ne poss&#143;de pas de trait de personne, la valeur par d&#142;faut est 3 (9b,c) et lorsqu&#8217;il ne porte
pas de gramm&#143;me de nombre la valeur par d&#142;faut est le singulier (sg) (9c).
</p>
<p>(9) a. Nous viendrons.
b. Pierre viendra.
c. Que tu viennesest impossible.
</p>
<p>Figure 20 : R&#143;gles syntaxiques d&#8217;accord et de rection
</p>
<p>Les r&#143;gles de lin&#142;arisation de GUST simulent les r&#143;gles de lin&#142;arisation de la TST (pr&#142;sent&#142;es
dans la Figure 7 de la Section 3.2.3). Nous reprenons Figure 21 la r&#143;gle de placement par
d&#142;faut d&#8217;un sujet non pronominal. Une r&#143;gle de ce type met en correspondance la d&#142;pendance
entre deux n&#711;uds syntaxiques avec une relation d&#8217;ordre (agr&#142;ment&#142;e d&#8217;un trait de position) entre
les n&#711;uds morphologiques correspondants. Pour pr&#142;parer le passage &#136; GUST, nous avons
d&#142;plac&#142; les conditions d&#8217;application de la r&#143;gle.
</p>
<p>Figure 21 : Une r&#143;gle de lin&#142;arisation TST
</p>
<p>Nous proposons Figure 22 la r&#143;gle de GUST qui simule la r&#143;gle TST de la Figure 21. Au lieu
de mettre en correspondance un fragment de structure syntaxique avec un fragment de structure
morphologique, cette r&#143;gle propose un fragment de structure produit syntaxique-
morphologique, c&#8217;est-&#136;-dire un morceau d&#8217;arbre ordonn&#142;.
</p>
<p>Figure 22 : Une r&#143;gle de lin&#142;arisation GUST
</p>
<p>Les r&#143;gles comme celles de la Figure 22 se combinent par unification. On impose que le r&#142;sultat
soit un arbre ordonn&#142; projectif (Kahane 2000b, 2001). Nous ne traiterons pas ici la question de
la lin&#142;arisation des arbres non projectifs (voir, par exemple, Br&#154;ker 1998, Lombardo &amp; Lesmo
1998, Kahane et al. 1998, Hudson 2000, Gerdes &amp; Kahane 2001).
</p>
<p>4.2.3  Module morphologique GUST
</p>
<p>Nous terminons notre pr&#142;sentation des modules de GUST par le module morphologique qui
assure la correspondance entre le niveau morphologique (profond) et le niveau textuel, c&#8217;est-&#136;-
dire la cha&#148;ne de caract&#143;res qui forme le texte d&#8217;une phrase. Les r&#143;gles TST permettent de mettre
</p>
<p>suj
</p>
<p>(V)3//p,sg//n
</p>
<p>(N,p)n
suj
</p>
<p>(V)
</p>
<p>(N,pro)nom
</p>
<p>&#8660;suj
X(V)
</p>
<p>Y
</p>
<p>Y  &lt;  X
-10 0
</p>
<p>syntaxique morphologique
</p>
<p>(&#180;pro)
</p>
<p>&#220;
</p>
<p>suj, pos: -10
</p>
<p>(&#180;pro) (V)</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Grammaires de d&#142;pendance formelles et th&#142;orie Sens-Texte
</p>
<p>en correspondance la repr&#142;sentation morphologique profonde d&#8217;un mot, une lexie de surface
accompagn&#142;e d&#8217;une liste de gramm&#143;mes de surface, avec une cha&#148;ne de caract&#143;res. Nous
pr&#142;sentons Figure 23 une r&#143;gle de ce type dans le style TST 36.
</p>
<p>Figure 23 : Une r&#143;gle morphologique TST
</p>
<p>Cette r&#143;gle est simul&#142;e en GUST par une r&#143;gle qui pr&#142;sente ces deux informations dans une
m&#144;me structure (Figure 24).
</p>
<p>Figure 24  : Une r&#143;gle morphologique GUST
</p>
<p>Contrairement &#136; la TST, GUST n&#8217;utilise pas de dictionnaire s&#142;par&#142; : par exemple, le tableau de
r&#142;gime est compl&#143;tement encod&#142; dans les r&#143;gles s&#142;mantiques. De m&#144;me, la partie du discours et
tous les traits pertinents (genre des noms, personne des pronoms, comportements particuliers,
&#201;) devront &#144;tre introduit par la r&#143;gle morphologique.
</p>
<p>4.3 Combinaison des modules
Nous allons maintenant montrer comment les r&#143;gles des diff&#142;rents modules se combinent pour
d&#142;river une phrase, c&#8217;est-&#136;-dire pour mettre en correspondance une repr&#142;sentation s&#142;mantique
avec un texte. L&#8217;avantage de GUST, sur un mod&#143;le Sens-Texte standard, est que, comme pour
tous les formalismes bas&#142;s sur l&#8217;unification, il est tr&#143;s facile de combiner n&#8217;importe quelles
r&#143;gles ensemble. En particulier, comme nous allons le voir, une grammaire GUST peut garder
une forme modulaire, comme la TST, ou &#144;tre compl&#143;tement lexicalis&#142;e, comme TAG (avec des
avantages sur cette derni&#143;re, notamment le fait qu&#8217;on peut &#142;viter l&#8217;explosion du nombre de
structures &#142;l&#142;mentaires associ&#142;es &#136; chaque entr&#142;e lexicale.
</p>
<p>4.3.1  D&#142;rivation d&#8217;une phrase
</p>
<p>Nous pr&#142;sentons Figure 25 l&#8217;ensemble des r&#143;gles n&#142;cessaires &#136; la d&#142;rivation de la phrase (10) :
(10) Nous essayons de manger la soupe.
Ces r&#143;gles permettent de mettre en correspondance la repr&#142;sentation s&#142;mantique de (10) avec le
texte de (10). Il y a plusieurs fa&#141;ons d&#8217;utiliser ces r&#143;gles, dans le sens de l&#8217;analyse comme de la
synth&#143;se. Nous allons regarder le sens de l&#8217;analyse. Il s&#8217;agit de construire une repr&#142;sentation
s&#142;mantique &#136; partir du texte. On peut distinguer deux strat&#142;gies principales : la strat&#142;gie
horizontale et la strat&#142;gie verticale. La m&#142;taphore horizontal/vertical s&#8217;entend par rapport au
</p>
<p>                                                
</p>
<p>36
 Dans la TST standard, une telle r&#143;gle est en fait le r&#142;sultat de la composition d&#8217;un grand nombre de r&#143;gles
</p>
<p>morphologiques et phonologiques. Si nous voulons &#144;tre capable de traiter des mots inconnus, nous devrons
avoir des r&#143;gles de ce type.
</p>
<p>&#8660;MANGER (V)ind,pr&#142;sent,1,pl mangeons
</p>
<p>morphologique (profond) textuel
</p>
<p>MANGER
(V)ind,pr&#142;sent,1,pl
graph: mangeons</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Sylvain Kahane
</p>
<p>d&#142;coupage de l&#8217;ensemble des r&#143;gles de la Figure 25, selon qu&#8217;il est fait en tranches horizontales
ou verticales.
</p>
<p>Figure 25 : D&#142;rivation de Nous essayons de manger la soupe
</p>
<p>4.3.2  Strat&#142;gie horizontale
</p>
<p>La strat&#142;gie horizontale consiste &#136; d&#142;clencher les r&#143;gles module apr&#143;s module.
</p>
<p>1) Le module morphologique permet de passer du texte proprement dit (la cha&#148;ne de caract&#143;re)
&#136; la repr&#142;sentation morphologique, c&#8217;est-&#136;-dire une suite de lexies accompagn&#142;es d&#8217;une liste
de gramm&#143;mes. La r&#143;gle introduit &#142;galement la partie du discours et tous les traits pertinents
pour la suite. Le module morphologique r&#142;alise ce qu&#8217;on appelle traditionnellement la
lemmatisation, l&#8217;&#142;tiquetage morphologique ou le tagging. A noter que le module n&#8217;a pas le
pouvoir, comme le font ce qu&#8217;on nomme g&#142;n&#142;ralement des taggeurs, de filtrer certaines
s&#142;quences de lexies (ou de cat&#142;gories lexicales) qui ne peuvent appara&#148;tre dans la langue. De
tels filtres sont en fait la projection d&#8217;informations contenues dans le module syntaxique, et
nous pensons qu&#8217;il est pr&#142;f&#142;rable d&#8217;utiliser le module syntaxique lui-m&#144;me pour cette t&#137;che.
Notre &#142;tiqueteur ne fait donc que proposer pour chaque mot toute les lemmatisations
possibles sans tenir compte des &#142;tiquettes attribu&#142;es aux lexies voisines.
</p>
<p>2) Le module syntaxique permet de passer de la repr&#142;sentation morphologique &#136; la
repr&#142;sentation syntaxique. Il propose pour chaque couple de lexies, en fonction de leurs
positions relatives, une liste (&#142;ventuellement vide) de d&#142;pendances susceptibles de les lier.
Nous verrons Section 5 diff&#142;rentes proc&#142;dures pour produire des arbres syntaxiques. Le
module syntaxique r&#142;alise ce qu&#8217;on appelle traditionnellement le shallow parsing ou analyse
</p>
<p>suj inf
</p>
<p>ESSAYER
(V)ind,pr&#142;sent
s&#142;m: &#212;essayer&#213;
t: &#212;pr&#142;sent&#213;
arg1: x
arg2: y
</p>
<p>DE
(Pr&#142;p)
&#180;s&#142;m
</p>
<p>pr&#142;p
</p>
<p>(V)inf
s&#142;m: y
</p>
<p>(N)
s&#142;m: x
</p>
<p>suj
NOUS
(N,pro,1)pl
s&#142;m: &#212;nous&#213;
d&#142;t
LE
(D&#142;t)
(N)d&#142;f
s&#142;m: x
d: &#212;d&#142;fini&#213;
12
&#212;essayer&#213;
&#212;nous&#213;
&#212;manger&#213;
1
&#212;soupe&#213;
inf, pos: +10
(V)(Pr&#142;p)
d&#142;t
(N,g)n
(D&#142;t)g,n
suj
(V)3//p,sg//n
(N,p)n
pr&#142;p, pos: +5
(Pr&#142;p)(N/V)
&#220;
d&#142;t, pos: -5
(D&#142;t)(N)
&#220;
suj, pos: -7
(N,pro)(V)
&#221;
&#221;
NOUS
(N,pro,1)pl,cl,nom
graph: nous
ESSAYER
(V)ind,pr&#142;sent,1,pl
graph: essayons
DE
(Pr&#142;p)
graph: de
MANGER
(V)inf
graph: manger
LE
(D&#142;t)f&#142;m,sg
graph: la
dobj, pos: +10
(V)(N)
&#221;
SOUPE
(N,f&#142;m)sg
graph: soupe
suj
dobj
MANGER
(V)inf
s&#142;m: &#212;manger&#213;
arg1: x
arg2: y
(N)
s&#142;m: x
(N)
s&#142;m: y
niveau syntaxique
niveaus&#142;mantique
niveau morphologique
niveau textuel
2
suj
(V)
(N,pro)nom
SOUPE
(N,f&#142;m)sg,&#8594;d
s&#142;m: &#212;soupe&#213;</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Grammaires de d&#142;pendance formelles et th&#142;orie Sens-Texte
</p>
<p>superficielle. A noter que le module syntaxique n&#8217;a pas le pouvoir de contr&#153;ler la sous-
cat&#142;gorisation des lexies, ni m&#144;me de v&#142;rifier qu&#8217;un verbe a bien un et un seul sujet. Ceci
sera contr&#153;l&#142; par le module s&#142;mantique. Comme pr&#142;c&#142;demment, il est possible de projeter
une partie du module s&#142;mantique sur le module syntaxique pour assurer ces points, bien que
nous pensions qu&#8217;il est pr&#142;f&#142;rable d&#8217;utiliser le module s&#142;mantique lui-m&#144;me.37
</p>
<p>3) Le module s&#142;mantique permet de passer de la repr&#142;sentation syntaxique &#136; la repr&#142;sentation
s&#142;mantique. Chaque d&#142;pendance syntaxique doit &#144;tre associ&#142;e &#136; une configuration mise en
correspondance avec une configuration s&#142;mantique de relations pr&#142;dicat-argument entre
s&#142;mant&#143;mes pour &#144;tre valid&#142;e. Comme on l&#8217;a dit, notre repr&#142;sentation s&#142;mantique est une
repr&#142;sentation du sens purement linguistique et n&#8217;a pas l&#8217;ambition d&#8217;&#144;tre une repr&#142;sentation
de l&#8217;&#142;tat du monde d&#142;not&#142;e par la phrase. Pour cette raison, une grande partie de ce que
r&#142;alise notre module s&#142;mantique est consid&#142;r&#142;e par beaucoup comme une &#142;tape de l&#8217;analyse
syntaxique et correspond &#136; ce qu&#8217;on appelle g&#142;n&#142;ralement l&#8217;analyse profonde, ou deep
analysis.
</p>
<p>La strat&#142;gie horizontale est la strat&#142;gie retenue par la plupart des approches modulaires. Le
principal inconv&#142;nient de la strat&#142;gie horizontale est le fait que la d&#142;sambigu&#149;sation (quand elle
est possible) n&#8217;intervient qu&#8217;au niveau s&#142;mantique et qu&#8217;il faudra manipuler aux niveaux
morphologique et syntaxique un tr&#143;s grand nombre d&#8217;analyses concurrentes.
</p>
<p>4.3.3  Strat&#142;gie verticale et lexicalisation compl&#143;te
</p>
<p>La strat&#142;gie verticale consiste &#136; d&#142;clencher les r&#143;gles mot apr&#143;s mot.
</p>
<p>Prenons l&#8217;exemple (10). Lorsqu&#8217;on analyse le mot nous, le module morphologique propose
(parmi d&#8217;autres propositions) d&#8217;&#142;tiqueter nous comme une forme nominative du pronom
NOUS. Mais, on peut alors, par la r&#143;gle syntaxique de rection, en d&#142;duire qu&#8217;il s&#8217;agit d&#8217;un
clitique sujet, puis par la r&#143;gle de lin&#142;arisation des pronoms sujet et par la r&#143;gle d&#8217;accord pr&#142;dire
la position du verbe et en partie sa forme. La r&#143;gle s&#142;mantique associ&#142;e &#136; la lexie NOUS peut
&#144;tre &#142;galement d&#142;clench&#142;e. Par la seule analyse du mot nous, on peut donc d&#142;clencher 5 r&#143;gles et
d&#142;buter l&#8217;analyse syntaxique et s&#142;mantique. La m&#144;me chose lorsqu&#8217;on analyse le mot suivant
essayons. Le module morphologique proposer d&#8217;&#142;tiqueter essayons comme une forme du verbe
ESSAYER. Cette forme remplit les conditions impos&#142;es par nous &#136; son gouverneur syntaxique
et la r&#143;gle peut donc &#144;tre imm&#142;diatement combin&#142;e avec les r&#143;gles pr&#142;c&#142;dentes. Une des r&#143;gles
s&#142;mantiques associ&#142;es &#136; ESSAYER peut &#144;tre d&#142;clench&#142;e. Si l&#8217;on d&#142;clenche la r&#143;gle o&#157; le
deuxi&#143;me argument est r&#142;alis&#142; par DE Vinf, on d&#142;clenchera les r&#143;gles syntaxiques de
lin&#142;arisation associ&#142;es &#136; une telle construction. Et ainsi de suite. La Figure 26 montre les
diff&#142;rents paquets de r&#143;gles d&#142;clench&#142;s par les diff&#142;rents mots de la phrase.
</p>
<p>                                                
</p>
<p>37
 Pour que l&#8217;analyseur soit robuste, la grammaire doit proposer un traitement par d&#142;faut des mots inconnus. Par
</p>
<p>exemple, le module s&#142;mantique doit proposer, parmi ses diff&#142;rentes r&#143;gles par d&#142;faut, une r&#143;gle pour une
forme verbale qui indiquera que la forme en question doit avoir un sujet et qu&#8217;elle aura au plus un objet direct,
un objet indirect et deux compl&#142;ments obliques. Ce sont les projections de ces r&#143;gles s&#142;mantiques par d&#142;faut
qui sont les r&#143;gles filtres g&#142;n&#142;ralement utilis&#142;es par les modules syntaxiques.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Sylvain Kahane
</p>
<p>Figure 26 : Regroupement des r&#143;gles dans l&#8217;analyse verticale
</p>
<p>Une strat&#142;gie d&#8217;analyse verticale semble beaucoup plus s&#142;duisante qu&#8217;une strat&#142;gie d&#8217;analyse
horizontale si l&#8217;on se place du point de vue cognitif, c&#8217;est-&#136;-dire du point de vue de la
mod&#142;lisation du processus d&#8217;analyse linguistique par un locuteur. M&#144;me du point de vue du
traitement informatique, une analyse verticale pourrait s&#8217;av&#142;rer plus efficace qu&#8217;une analyse
horizontale. Il existe d&#8217;ailleurs une variante de l&#8217;analyse verticale qui consiste &#136; pr&#142;compiler les
paquets de r&#143;gles d&#142;clench&#142;s par chaque mot. On obtient ainsi une grammaire dite compl&#143;tement
lexicalis&#142;e (fully lexicalized grammar). La grammaire ainsi obtenue s&#8217;apparente &#136; la grammaire
propos&#142;e par Nasr 1995, 1996, elle-m&#144;me inspir&#142;e des TAG (cf. &#142;galement Kahane 2000a pour
un traitement des extractions). Le passage &#136; des r&#143;gles compl&#143;tement lexicalis&#142;es se fait
simplement par combinaison d&#8217;un paquet de r&#143;gles modulaires (Figure 27). Il s&#8217;agit de la
combinaison ordinaire des r&#143;gles de la grammaire (bas&#142;e sur l&#8217;unification) ; la seule diff&#142;rence
est que la combinaison des r&#143;gles ne se fait pas au moment de l&#8217;analyse, mais dans une phase
pr&#142;alable de pr&#142;compilation (Candito 1996, Candito &amp; Kahane 1998).
Le passage d&#8217;une grammaire modulaire &#136; une grammaire compl&#143;tement lexicalis&#142;e am&#143;ne
plusieurs commentaires.
</p>
<p>1) L&#8217;analyse verticale avec la grammaire modulaire revient en fait &#136; utiliser une grammaire
compl&#143;tement lexicalis&#142;e sans l&#8217;avoir lexicalis&#142;e au pr&#142;alable, mais en construisant  les
r&#143;gles lexicalis&#142;es &#136; la demande (on line) au moment de l&#8217;analyse. Quels sont alors les
avantages ou les inconv&#142;nients de la grammaire compl&#143;tement lexicalis&#142;e ? La
pr&#142;compilation consomme de l&#8217;espace, puisqu&#8217;il faut m&#142;moriser toutes les r&#143;gles
lexicalis&#142;es, lesquelles sont extr&#144;mement redondantes entre elles. Par contre, le temps
d&#8217;analyse, si l&#8217;acc&#143;s aux r&#143;gles compil&#142;es est bien g&#142;r&#142;, devrait &#144;tre am&#142;lior&#142; par le fait
qu&#8217;une partie des combinaisons de r&#143;gles est d&#142;j&#136; faite. L&#8217;alternative entre grammaire
modulaire et grammaire compl&#143;tement lexicalis&#142;e peut aussi &#144;tre consid&#142;r&#142;e du point de vue
cognitif : sous-quelle forme la grammaire est-elle cod&#142;e dans notre cerveau ? Y-a-t-il des
</p>
<p>suj inf
</p>
<p>ESSAYER
(V)ind,pr&#142;sent
s&#142;m: &#212;essayer&#213;
t: &#212;pr&#142;sent&#213;
arg1: x
arg2: y
</p>
<p>DE
(Pr&#142;p)
&#180;s&#142;m
</p>
<p>pr&#142;p
</p>
<p>(V)inf
s&#142;m: y
</p>
<p>(N)
s&#142;m: x
</p>
<p>suj
NOUS
(N,pro,1)pl
s&#142;m: &#212;nous&#213;
d&#142;t
LE
(D&#142;t)
(N)d&#142;f
s&#142;m: x
d: &#212;d&#142;fini&#213;SOUPE
(N,f&#142;m)sg,&#8594;d
s&#142;m: &#212;soupe&#213;
inf, pos: +10
(V)(Pr&#142;p)
d&#142;t
(N,g)n
(D&#142;t)g,n
suj
(V)3//p,sg//n
(N,p)n
pr&#142;p, pos: +5
(Pr&#142;p)(N/V)
&#220;
d&#142;t, pos: -5
(D&#142;t)(N)
&#220;
suj, pos: -7
(N,pro)(V)
&#221;
&#221;
NOUS
(N,pro,1)pl,cl,nom
graph: nous
ESSAYER
(V)ind,pr&#142;sent,1,pl
graph: essayons
DE
(Pr&#142;p)
graph: de
MANGER
(V)inf
graph: manger
LE
(D&#142;t)f&#142;m,sg
graph: la
dobj, pos: +10
(V)(N)
&#221;
SOUPE
(N,f&#142;m)sg
graph: soupe
suj
dobj
MANGER
(V)inf
s&#142;m: &#212;manger&#213;
arg1: x
arg2: y
(N)
s&#142;m: x
(N)
s&#142;m: y
niveau syntaxique
niveau morphologique
niveau textuel
suj
(V)
(N,pro)nom
niveau s&#142;mantique</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Grammaires de d&#142;pendance formelles et th&#142;orie Sens-Texte
</p>
<p>constructions linguistiques plus fr&#142;quentes que d&#8217;autres qui sont d&#142;j&#136; &#210;lexicalis&#142;es&#211; ? La
grammaire s&#8217;acquiert-elle sous forme modulaire ou &#210;lexicalis&#142;e&#211; ?
</p>
<p>2) Les deux analyses, avec ou sans pr&#142;compilation, posent les m&#144;mes probl&#143;mes th&#142;oriques, &#136;
savoir quels sont les paquets de r&#143;gles qui doivent &#144;tre associ&#142;s &#136; une lexie donn&#142;e ou, de
mani&#143;re &#142;quivalente, mais en se pla&#141;ant du point de vue des r&#143;gles plut&#153;t que des lexies, &#136;
quelle lexie doit &#144;tre associ&#142;e une r&#143;gle donn&#142;e. Prenons un exemple : &#136; quelle lexie,
gouverneur ou d&#142;pendant, doit &#144;tre associ&#142;e une r&#143;gle de lin&#142;arisation? Consid&#142;rons le cas
de l&#8217;objet direct en fran&#141;ais. Les r&#143;gles sont les suivantes : un nom objet direct se place
derri&#143;re le verbe, un pronom clitique se place devant le verbe (&#136; une place bien pr&#142;cise par
rapport aux autres clitiques) et un pronoms relatif ou interrogatif se place &#136; l&#8217;avant de la
proposition. Il serait peu &#142;conomique d&#8217;indiquer pour chaque nom, dans la r&#143;gle lexicalis&#142;e
qui lui correspond, comment il se place quand il est objet direct, sujet ou autre chose encore.
Il est donc pr&#142;f&#142;rable d&#8217;attacher la r&#143;gle de lin&#142;arisation de l&#8217;objet direct aux verbes qui en
poss&#143;de un. Par contre, le pronom clitique objet direct a une forme bien particuli&#143;re et un
placement bien particulier. Il semble plus &#142;conomique d&#8217;attacher &#136; ce seul mot, le, le pronom
clitique objet direct, les r&#143;gles qui lui sont sp&#142;cifiques. De m&#144;me, les pronoms relatifs ou
interrogatifs ont un placement particulier qui ne d&#142;pend pas r&#142;ellement de leur fonction. Il
semble donc aussi plus &#142;conomique que la r&#143;gle de placement de ces &#142;l&#142;ments leur soit
attach&#142;e. La solution retenue est donc de panacher l&#8217;information sur le placement de l&#8217;objet
direct entre les verbes transitifs pour les &#142;l&#142;ments canoniques (les noms) et les &#142;l&#142;ments non
canoniques eux-m&#144;mes pour ce qui les concerne (voir Figure 27). Cette fa&#141;on de faire
permet d&#8217;&#142;viter la multiplication des r&#143;gles lexicalis&#142;es associ&#142;es &#136; un m&#144;me verbe, comme
cela est le cas par exemple en TAG o&#157; le formalisme ne permet pas d&#8217;encoder le placement
des arguments d&#8217;une lexie ailleurs que dans la r&#143;gle (appel&#142;e structure &#142;l&#142;mentaire en TAG)
de cette lexie. Reste une difficult&#142; : il faut &#142;viter, lors de la combinaison d&#8217;un verbe x avec
un &#142;l&#142;ment en position non canonique y, que rentre en conflit la r&#143;gle de lin&#142;arisation des
&#142;l&#142;ments en position canonique attach&#142;e &#136; x  avec la r&#143;gle de lin&#142;arisation sp&#142;cifique attach&#142;e
&#136; l&#8217;&#142;l&#142;ment  y (voir Kahane 2000a pour une solution bas&#142;e sur l&#8217;unification consistant &#136;
&#210;tuer&#211; la r&#143;gle de positionnement attach&#142;e &#136; la d&#142;pendance objet du verbe en l&#8217;unifiant avec
un leurre, une quasi-d&#142;pendance objet plac&#142;e dans la structure associ&#142;e &#136; l&#8217;&#142;l&#142;ment y).
</p>
<p>3) En dehors des questions computationnelles, les grammaires compl&#143;tement lexicalis&#142;es ont
un autre int&#142;r&#144;t : il est tr&#143;s facile d&#8217;&#142;crire un premier fragment de grammaire et de l&#8217;&#142;tendre &#136;
chaque nouvelle construction rencontr&#142;e. N&#142;anmoins, de cette fa&#141;on, on contr&#153;le
difficilement la consistance globale de la grammaire et certaines constructions obtenues
seulement par combinaison de ph&#142;nom&#143;nes divers peuvent &#144;tre facilement oubli&#142;es (par ex.,
la forme passive d&#8217;un verbe o&#157; un compl&#142;ment est relativis&#142; et un autre cliticis&#142;). Pour cette
raison, d&#143;s qu&#8217;on souhaite d&#142;velopper et maintenir une grammaire &#136; large couverture, il est
n&#142;cessaire de contr&#153;ler la grammaire compl&#143;tement lexicalis&#142;e par une grammaire modulaire
&#136; partir de laquelle on la g&#142;n&#143;re. Dans le cadre des TAG, il a &#142;t&#142; d&#142;velopp&#142; des formalismes
modulaires &#136; partir desquels on peut g&#142;n&#142;rer la grammaire TAG (Vijay-Shanker 1992,
Candito 1996, 1999), ainsi que des proc&#142;dures pour g&#142;n&#142;rer la grammaire TAG &#136; partir
d&#8217;une grammaire modulaire existante, comme HPSG (Kasper et al. 1995). Notre approche
pr&#142;sente un avantage par le fait que nous proposons un formalisme qui permet d&#8217;&#142;crire &#136; la
fois une grammaire modulaire et une grammaire compl&#143;tement lexicalis&#142;e. On peut ainsi
envisager de lexicaliser une partie de la grammaire seulement et de maintenir une grammaire
non lexicalis&#142;e pour les constructions marginales.
</p>
<p>J&#8217;aimerais insister, pour terminer cette section sur l&#8217;analyse verticale, sur le fait qu&#8217;une
grammaire modulaire ne s&#8217;utilise pas n&#142;cessairement module par module. Quand on parle d&#8217;une
architecture modulaire pour un syst&#143;me de TAL, on pense g&#142;n&#142;ralement, &#136; tord, &#136; une
succession de modules agissant les uns apr&#143;s les autres. D&#8217;autre part, si j&#8217;ai mis l&#8217;accent sur le
lien entre l&#8217;analyse verticale et les grammaires lexicalis&#142;es, c&#8217;est parce que ce lien existe et que
les grammaires lexicalis&#142;es connaissent &#136; l&#8217;heure actuelle un certain succ&#143;s en TAL. Mais je ne
voudrais pas que ceci masque le fait que l&#8217;analyse verticale est possible sans pr&#142;compilation et
qu&#8217;il s&#8217;agit, &#136; mon avis, de la meilleure solution.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Sylvain Kahane
</p>
<p>Enfin, tout ce que nous venons de montrer pour l&#8217;analyse est aussi valable pour la synth&#143;se. La
aussi, on peut envisager des strat&#142;gies horizontales ou verticales et il est possible d&#8217;utiliser la
grammaire sous forme modulaire ou de la pr&#142;compiler en une grammaire lexicalis&#142;e (cf. Danlos
1998, Candito &amp; Kahane 1998 pour l&#8217;usage d&#8217;une grammaire compl&#143;tement lexicalis&#142;e en
synth&#143;se).
</p>
<p>Figure 27 :  Lexicalisation compl&#143;te  de essayons
</p>
<p>Pour clore cette section sur les diff&#142;rentes strat&#142;gies dans la combinaison des r&#143;gles, notons que
les strat&#142;gies verticales et horizontales repr&#142;sentent les deux cas extr&#144;mes et que des strat&#142;gies
interm&#142;diaires peuvent &#144;tre envisag&#142;es.  Par exemple, on peut envisager une strat&#142;gie verticale
o&#157; les r&#143;gles ne sont pas regroup&#142;es mots par mots, mais chunks par chunks.
</p>
<p>5 Analyse en grammaire de d&#142;pendance
Apr&#143;s nos pr&#142;sentations des grammaires TST et GUST et de l&#8217;articulation des modules, nous
allons nous concentrer sur le module qui pose les plus grandes difficult&#142;s en analyse, le module
syntaxique de la grammaire, c&#8217;est-&#136;-dire le module qui assure la correspondance entre une
cha&#148;ne de mots et un arbre de d&#142;pendance. Nous consid&#142;rerons les r&#143;gles de syntaxiques
pr&#142;sent&#142;es dans les Sections 0 et 4.2.2 (sous forme transductive, puis g&#142;n&#142;rative) : une telle
r&#143;gle associe une d&#142;pendance entre deux mots &#136; une relation d&#8217;ordre entre les deux m&#144;mes mots.
Nous allons pr&#142;senter trois techniques d&#8217;analyse : l&#8217;analyse par contrainte et l&#8217;analyse CKY, qui
sont des strat&#142;gies d&#8217;analyse horizontale (les r&#143;gles sont d&#142;clench&#142;es module  apr&#143;s module), et
l&#8217;analyse incr&#142;mentale avec un analyseur &#136; pile, qui est une strat&#142;gie verticale (les r&#143;gles sont
d&#142;clench&#142;es mot apr&#143;s mot).
Nous illustrerons nos diff&#142;rentes techniques d&#8217;analyse sur l&#8217;exemple suivant :
</p>
<p>(11) Le boucher sale la tranche
Cette phrase bien connue poss&#143;de deux interpr&#142;tations : &#212;le boucher est sale et il tranche quelque
chose&#213; ou &#212;le boucher met du sel sur la tranche&#213;.
</p>
<p>suj inf
</p>
<p>ESSAYER
(V)ind,pr&#142;sent
s&#142;m: &#212;essayer&#213;
t: &#212;pr&#142;sent&#213;
arg1: x
arg2: y
</p>
<p>DE
(Pr&#142;p)
&#180;s&#142;m
</p>
<p>pr&#142;p
</p>
<p>(V)inf
s&#142;m: y
</p>
<p>(N)
s&#142;m: x
</p>
<p>suj
inf, pos: +10
(V)(Pr&#142;p)
suj
(N,p)n
pr&#142;p, pos: +5
(Pr&#142;p)(N/V)
&#220;
suj, pos: -10
(N)(V)
&#221;&#221;
ESSAYER
(V)ind,pr&#142;sent,1,pl
graph: essayons
DE
(Pr&#142;p)
graph: de
inf
pos: +10
pr&#142;p
pos: +5
suj
pos: -10
ESSAYER
graph: essayons
(V)ind,pr&#142;sent,1,pl
s&#142;m: &#212;essayer&#213;
t: &#212;pr&#142;sent&#213;
arg1: x
arg2: y
DE
graph: de
(Pr&#142;p)
s&#142;m
(V)inf
s&#142;m: y
(N,1)pl
s&#142;m: x
suj
&#8801;
&#220;
&#220;
&#221;&#221;
(V)3//p,sg//n</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Grammaires de d&#142;pendance formelles et th&#142;orie Sens-Texte
</p>
<p>5.1 Analyse par contraintes
Le principe de l&#8217;analyse par contraintes est de consid&#142;rer toutes les structures imaginables et de
filtrer &#136; l&#8217;aide des r&#143;gles les structures bien form&#142;es qui peuvent correspondre &#136; la phrase. Plut&#153;t
que tester l&#8217;une apr&#143;s l&#8217;autre toutes les structures imaginables (ce qui serait trop long), on
construit en fait une structure tr&#143;s g&#142;n&#142;rale que l&#8217;on contraint par les r&#143;gles et par les propri&#142;t&#142;s
de bonne formation (par exemple le fait que l&#8217;on veuille un arbre projectif).
On commence donc par envisager pour chaque couple de mots de la phrase toutes les
d&#142;pendances imaginables : on obtient ainsi un graphe de d&#142;pendance complet (Figure 28 de
gauche). Ensuite, on applique les r&#143;gles de lin&#142;arisation pour filtrer les d&#142;pendances qui sont
valid&#142;es par une r&#143;gle de lin&#142;arisation (Figure 28 de droite)38. Rappelons qu&#8217;une r&#143;gle de
lin&#142;arisation dit, vu du point de vue de l&#8217;analyse, que si deux mots sont de telle et telle
cat&#142;gories et s&#8217;ils sont dans tel ordre, alors une d&#142;pendance avec telle fonction syntaxique peut
les relier : par exemple, si un N suit un V, alors le N peut &#144;tre l&#8217;objet du V.
</p>
<p>Figure 28 :  Graphes de (11) avant filtrage et apr&#143;s filtrage par les r&#143;gles de lin&#142;arisation
</p>
<p>La derni&#143;re &#142;tape consiste &#136; extraire des arbres projectifs du graphe ainsi obtenu (Figure 29).
(Nous ne d&#142;taillons pas cette &#142;tape ; coir les sections suivantes pour cela).
</p>
<p>Figure 29 :  Graphes de (11) apr&#143;s filtrage complet
</p>
<p>L&#8217;analyse par contraintes est particuli&#143;rement adapt&#142;e aux grammaires de d&#142;pendances par le fait
que, contrairement aux grammaires syntagmatiques, il est facile de consid&#142;rer une structure qui
contient en elle toutes les structures acceptables apr&#143;s filtrage. L&#8217;analyse par contraintes dans les
</p>
<p>                                                
</p>
<p>38
 Pour simplifier la pr&#142;sentation, nous utilisons des cat&#142;gories lexicales tr&#143;s grossi&#143;res. Par exemple, la
</p>
<p>cat&#142;gorie Cl vaut pour tous les clitiques et comprend donc les (N,pro)nom et (N,pro)acc.
</p>
<p>le boucher sale    la  tranche
</p>
<p>&#221; &#221; &#221; &#221;
</p>
<p>&#221; &#221; &#221;
</p>
<p>&#221;
</p>
<p>&#221;
</p>
<p>&#221;
</p>
<p>&#220; &#220;
</p>
<p>&#220; &#220;
</p>
<p>&#220; &#220; &#220;
</p>
<p>&#220;
</p>
<p>&#220;
</p>
<p>&#220;
</p>
<p>le boucher sale    la  tranche
&#221;
</p>
<p>&#221;
</p>
<p>&#221;
</p>
<p>&#220;
</p>
<p>&#220;
</p>
<p>&#220; &#220;
</p>
<p>&#220;
</p>
<p>&#220;
</p>
<p>D/Cl   N/V    A/V  D/Cl/N   N/V
suj
</p>
<p>mod
</p>
<p>obj
obj
</p>
<p>suj
</p>
<p>&#221;
</p>
<p>obj
&#221;
</p>
<p>obj
</p>
<p>&#220;d&#142;tobj
&#220;d&#142;tobj mod
</p>
<p>modobjd&#142;t
objd&#142;t
</p>
<p>le boucher sale    la  tranche
&#221;
</p>
<p>&#220;
</p>
<p>D        N         A        Cl        V
</p>
<p>mod
</p>
<p>suj
</p>
<p>&#220;obj
&#220;d&#142;t
</p>
<p>le boucher sale    la  tranche
</p>
<p>&#221;
</p>
<p>&#220;
</p>
<p>D        N         V        D         N
suj
</p>
<p>obj
</p>
<p>&#220;d&#142;t
&#220;d&#142;t</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Sylvain Kahane
</p>
<p>grammaires de d&#142;pendance a &#142;t&#142; introduite par Maruyama (1990a, 1990b) et d&#142;velopp&#142;e, par
exemple, par Duchier (1999 ; Duchier &amp; Debusman 2001) ou par Blache (1998, 2001)39.
Le m&#144;me genre de techniques peuvent &#144;tre appliqu&#142;es avec des r&#143;gles pond&#142;r&#142;es suivant leur
probabilit&#142; d&#8217;apparition dans une situation donn&#142;e. Chaque r&#143;gle poss&#143;de un poids compris
entre 0 et 1&#730;; plus le poids est proche de 0 plus la r&#143;gle est contraignante. Apr&#143;s avoir construit
le graphe de toutes les d&#142;pendances imaginables, on va utiliser les r&#143;gles de lin&#142;arisation pour
adresser &#136; chaque d&#142;pendance un poids : soit le poids de la r&#143;gle si une r&#143;gle s&#8217;applique, soit le
poids 0.1 si aucune r&#143;gle ne s&#8217;applique (on &#142;vite les poids 0 qui &#142;craseraient d&#142;finitivement le
score final). On pourra alors extraire du graphe l&#8217;arbre projectif qui donne le meilleur score (le
score d&#8217;un arbre est le produit des scores des d&#142;pendances) (Menzel &amp; Schr&#154;der 1998 ;
Schr&#154;der et al. 2000). On pourra m&#144;me accepter des entorses &#136; la d&#142;pendance en pond&#142;rant
&#142;galement les r&#143;gles qui assurent la projectivit&#142;. Plus g&#142;n&#142;ralement, pour des m&#142;thodes
probabilistes en grammaire de d&#142;pendance, voir Eisner 1996, Collins 1997.
</p>
<p>5.2 Analyse CKY
L&#8217;analyse CKY a &#142;t&#142; d&#142;velopp&#142;e ind&#142;pendamment par Cocke, Kasami et Younger pour les
grammaires de r&#142;&#142;criture hors-contextes (Kasami 1963, Younger 1967, Floyd &amp; Biegel 1995).
L&#8217;analyse CKY est une analyse montante : il s&#8217;agit d&#8217;identifier des segments analysables de la
phrase de d&#142;part en allant des plus petits aux plus grands : si le plus grand segment analysable
est la phrase compl&#143;te, la phrase est donc analysable. L&#8217;algorithme fonctionne en temps 0(n3) o&#157;
n est le nombre de mots de la phrase. Avec une grammaire syntagmatique hors-contexte, on
m&#142;morise pour chaque segment analys&#142; sa cat&#142;gorie syntagmatique. L&#8217;algorithme peut &#144;tre
adapt&#142; trivialement aux grammaires de d&#142;pendance : dans ce cas, on m&#142;morisera la cat&#142;gorie
lexicale de la t&#144;te du segment.
</p>
<p>Consid&#142;rons une phrase de longueur n. Pour chaque segment analys&#142; allant du i-i&#143;me mot au j-
i&#143;me mot (compris), on m&#142;morise la cat&#142;gorie X de la t&#144;te du segment sous la forme d&#8217;un triplet
[i,j,X]. Avec le module morphologique, on commence par analyser tous les mots de la phrase,
c&#8217;est-&#136;-dire tous lessegments de longueur 1. Pour la phrase (11), on obtient :
[1,1,D], [1,1,Cl], [2,2,N], [2,2,V], &#201;, [5,5,N], [5,5,V]
On essaye ensuite d&#8217;obtenir des segments de longueur 2 en utilisant les r&#143;gles de lin&#142;arisation.
Par exemple, pour la phrase (11), [1,1,D] + [2,2,N] = [1,2,N], car un &#142;l&#142;ment  de cat&#142;gorie D &#136;
la gauche d&#8217;un &#142;l&#142;ment de cat&#142;gorie N peut d&#142;pendre de celui-ci par un d&#142;pendance d&#142;t. On
obtient donc, pour la phrase (11) :
[1,2,N], [1,2,V], [2,3,N], [2,3,V], &#201;, [4,5,N], [4,5,V]
Et ainsi de suite : les segments [i,j,X] et [j+1,k,Y] peuvent &#144;tre combin&#142;s pour donner le
segment  [i,k,X] (resp. [i,k,Y]) s&#8217;il existe une r&#143;gle de lin&#142;arisation indiquant  qu&#8217;un &#142;l&#142;ment de
cat&#142;gorie X pr&#142;c&#142;dant un &#142;l&#142;ment de cat&#142;gorie Y peut gouverner celui-ci (resp. peut d&#142;pendre de
celui-ci). On construit ainsi tous les segments de longueur 2, 3, etc., jusqu&#8217;&#136; n. Par exemple,
pour construire les segments de longueur k (qui sont tous de la forme [i,i+k-1,Z]), on va
consid&#142;rer tous les couples de segments d&#142;j&#136; obtenus de la forme ([i,j,X],[j+1,i+k-1,Y]) et
chercher &#136; les combiner par les r&#143;gles de lin&#142;arisation. Ceci demande k(n-k)C2R op&#142;rations40 o&#157;
                                                
</p>
<p>39
 Blache 1998 consid&#143;re au d&#142;part une grammaire syntagmatique avec t&#144;te &#136; partir de laquelle il construit ensuite
</p>
<p>un graphe de d&#142;pendances.
</p>
<p>40
 Pour k donn&#142;, on a n-k valeurs pour i, k valeurs pour j, C valeurs pour X et Y et R fa&#141;ons de combiner les
</p>
<p>segments &#136; tester.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Grammaires de d&#142;pendance formelles et th&#142;orie Sens-Texte
</p>
<p>C est le nombre de cat&#142;gories lexicales et R le nombre de r&#143;gles de lin&#142;arisation. En sommant
sur k, on obtient un r&#142;sultat en O(n3C2R).
Nous avons pr&#142;sent&#142; l&#8217;algorithme de base. Tel quel cet algorithme v&#142;rifie que la phrase peut &#144;tre
associ&#142;e &#136; un arbre de d&#142;pendance projectif, mais il ne construit pas un tel arbre. Pour construire
des arbres associ&#142;s, le plus simple est de redescendre le calcul en partant des segments
maximaux et de construire les arbres &#136; partir de la racine. Pour l&#8217;exemple (11), le segment final
[1,5,V] peut &#144;tre obtenu de trois fa&#141;ons : en combinant [1,3,N] et [4,5,V] par la r&#143;gle de
placement du sujet, en combinant [1,3,V] et [4,5,N] par la r&#143;gle de placement de l&#8217;objet ou en
combinant [1,2,N] et [3,5,V] par la r&#143;gle de placement du sujet. Comme les deux derni&#143;res
correspondent au m&#144;me arbre, on obtient en continuant les deux arbres de la Figure 29. On peut
&#142;viter de refaire les calculs en descendant l&#8217;arbre en conservant davantage d&#8217;informations lors du
premier calcul (en indiquant pour chaque segment sa d&#142;composition et la r&#143;gle qui permet de
l&#8217;obtenir), mais cela est en fait plus co&#158;teux. Quoi qu&#8217;il en soit, il faut noter que le nombre
d&#8217;arbres correspondant &#136; une phrase de longueur n est dans le pire des cas une fonction
exponentielle de  n et que, par cons&#142;quent, un algorithme qui construirait tous les arbres de peut
pas &#144;tre polynomial (sauf &#136; repr&#142;senter la for&#144;t d&#8217;arbres sous forme compacte).
Cet algorithme peut &#144;tre enrichi de diff&#142;rentes fa&#141;ons.
</p>
<p>1) Nous n&#8217;avons pas encore pris en compte le placement respectif des diff&#142;rents d&#142;pendants
d&#8217;un m&#144;me n&#711;ud. Celui-ci est encod&#142; dans nos r&#143;gles de lin&#142;arisation par les traits de
position sur les d&#142;pendances. On peut tr&#143;s facilement tenir compte des positions en gardant
en m&#142;moire la derni&#143;re position utilis&#142;e pour chacune des deux directions : au lieu de
segments [i,j,X], on manipulera des segments [i,j,X,p,q], o&#157; p est la derni&#143;re position
utilis&#142;e pour un d&#142;pendant gauche de la t&#144;te du segment et q est la derni&#143;re position utilis&#142;e
pour un d&#142;pendant droit (p et q &#142;tant &#142;gaux &#136; 0 si aucune r&#143;gle n&#8217;a &#142;t&#142; utilis&#142;). Un tel
segment ne pourra pas &#144;tre combin&#142; &#136; un segment d&#142;pendant que par une r&#143;gle dont le trait
de position n&#8217;est pas compris entre p et q (le nouveau segment d&#142;pendant doit &#144;tre
positionn&#142; loin que les pr&#142;c&#142;dents). Pour l&#8217;exemple (11), si on combine les segments
[4,4,Cl,0,0] et [5,5,V,0,0] avec la r&#143;gle qui relie un clitique objet dans la position -4 au
verbe, on obtiendra le segment [4,5,V,-4,0]. Ce segment ne pourra pas &#144;tre combin&#142; avec
un clitique qui exige une position entre -4 et 0, mais pourra &#144;tre combin&#142; avec un &#142;l&#142;ment
qui accepte une position inf&#142;rieure &#136; -4.
</p>
<p>2) Nous n&#8217;avons pas encore pris en compte les r&#143;gles de sous-cat&#142;gorisation, qui font partie
des r&#143;gles s&#142;mantiques de notre grammaire. On peut consid&#142;rer cette information en
indiquant dans la description d&#8217;un segment, en plus de la cat&#142;gorie lexicale de la t&#144;te X, la
liste des &#142;l&#142;ments sous-cat&#142;goris&#142;s par X qui ne sont pas dans le segment. Si on reprend
l&#8217;exemple (11), le segment form&#142; du seul mot tranche, lorsque ce dernier est analys&#142; comme
une forme du verbe TRANCHER, recevra une liste de sous-cat&#142;gorisation avec sujet et objet
direct : [5,5,V,{suj,dobj}]. Lorsque ce segment sera combin&#142; avec le segment form&#142; du mot
la reconnu comme clitique accusatif, on obtiendra le segment [4,5,V,{suj}], par application
de la r&#143;gle de lin&#142;arisation du clitique objet direct. Lorsque, ce nouveau segment sera
combin&#142; avec le segment le boucher sale reconnu comme groupe nominal (et donc d&#142;crit
comme [1,3,N,&#8709;]), on obtiendra, par application de la r&#143;gle de lin&#142;arisation du sujet, le
segment [1,5,V, &#8709;]. A noter qu&#8217;on impose que, lors de la combinaison de deux segments,
le segment d&#142;pendant soit satur&#142;, c&#8217;est-&#136;-dire que sa liste de sous-cat&#142;gorisation soit vide.
Quant &#136; la liste de sous-cat&#142;gorisation du segment t&#144;te, elle est priv&#142;e de l&#8217;&#142;l&#142;ment
correspondant &#136; la fonction du segment d&#142;pendant.
</p>
<p>Dans cet exemple, nous avons r&#142;duit l&#8217;information de niveau s&#142;mantique prise en compte au
minimum. Si nous prenons en compte l&#8217;ensemble de l&#8217;information contenue dans la r&#143;gle
s&#142;mantique de la t&#144;te, notamment la description des d&#142;pendances s&#142;mantiques, nos
descriptions de segments vont alors s&#8217;apparenter fortement aux descriptions de syntagmes
en HPSG (Pollard &amp; Sag 1994). Le mode de combinaison des segments que nous venons
de d&#142;crire s&#8217;apparente lui-m&#144;me au sch&#142;ma de combinaison t&#144;te-actant d&#8217;HPSG (head-</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Sylvain Kahane
</p>
<p>daughter schema) : lorsque deux syntagmes X et Y se combinent pour en former un
nouveau syntagme et que l&#8217;un des segments, par exemple Y, est reconnu comme un actant
de la t&#144;te de l&#8217;autre, la description du nouveau segment est &#142;gale &#136; la description du segment
t&#144;te X o&#157; l&#8217;&#142;l&#142;ment Y &#136; &#142;t&#142; retir&#142; de la liste des actants de X. La principale diff&#142;rence entre
notre approche et HPSG est que la combinaison de deux segments doit &#144;tre valid&#142;e par une
r&#143;gle syntaxique s&#142;par&#142;e. En conclusion, en restant &#136; un niveau de description grossier, on
peut voir HPSG comme une version proc&#142;durale orient&#142;e vers l&#8217;analyse CKY d&#8217;une
grammaire de d&#142;pendance.
</p>
<p>3) Nous n&#8217;avons pas non plus pris en compte l&#8217;analyse des structures non projectives. Cela est
possible. La modification de l&#8217;algorithme d&#142;pend de la fa&#141;on dont sont &#142;crites les r&#143;gles qui
assurent le placement des &#142;l&#142;ments qui ne sont pas dans la projection de leur gouverneur.
Kahane et al. 1998 propose des r&#143;gles de &#210;lifting&#211;, permettant de remonter un &#142;l&#142;ment sur
un anc&#144;tre de son gouverneur  et de le positionner par rapport &#136; ce dernier(cf. &#142;galement
Br&#154;ker 2000 pour une analyse comment&#142;e de cette solution). Dans la description d&#8217;un
segment, on indiquera donc en plus de la cat&#142;gorie lexicale de la t&#144;te et de sa liste de sous-
cat&#142;gorisation, la liste des &#142;l&#142;ments lift&#142;s. Encore une fois, cette solution s&#8217;apparente
fortement aux descriptions de syntagmes en HPSG o&#157; appara&#148;t un trait Slash (non-local) : le
trait Slash contient pr&#142;cis&#142;ment la liste des &#142;l&#142;ments &#210;lift&#142;s&#211;, c&#8217;est-&#136;-dire des &#142;l&#142;ments qui ne
se placent pas dans la projection de leur t&#144;te, mais dans celle d&#8217;un anc&#144;tre de leur t&#144;te.
Avec les r&#143;gles de &#210;lifting&#211;, on peut encore obtenir un algorithme polynomial, mais il faut
pour cela borner le nombre d&#8217;&#142;l&#142;ments &#210;lift&#142;s&#211; dans un segment (sinon le nombre de
segments que l&#8217;on peut consid&#142;rer cro&#148;t exponentiellement avec n)..41
</p>
<p>Remarquons que l&#8217;algorithme CKY est strictement montant et qu&#8217;une fois qu&#8217;un &#142;l&#142;ment a &#142;t&#142;
combin&#142; avec son gouverneur il n&#8217;est plus possible de le combiner avec un de ses d&#142;pendants.
(puisque le segment n&#8217;est repr&#142;sent&#142; que par sa t&#144;te). Par exemple, si l&#8217;on analyse la phrase Le
gar&#141;on que j&#8217;ai rencontr&#142; la semaine derni&#143;re est &#142;tudiant, il n&#8217;est pas possible de combiner le
gar&#141;on avec la t&#144;te de la relative (que ou ai suivant les analyses) tant que la relative dans son
entier n&#8217;a pas &#142;t&#142; analys&#142;e. La seule fa&#141;on d&#8217;analyser le sujet de cette phrase est de combiner la
et derni&#143;re &#136; semaine, puis la derni&#143;re semaine &#136; rencontr&#142;, puis le tout &#136; ai, et ainsi de suite
jusqu&#8217;&#136; la combinaison de la relative compl&#143;te avec gar&#141;on. Autrement dit, l&#8217;algorithme CKY,
s&#8217;il a l&#8217;avantage d&#8217;&#144;tre simple, ne peut en aucune fa&#141;on &#144;tre rendu incr&#142;mental.
</p>
<p>Il existe un autre algorithme classique pour les grammaires hors-contextes, l&#8217;algorithme
d&#8217;Earley (Earley 1970, Floyd &amp; Biegel 1995), qui peut &#144;tre aussi adapt&#142; aux grammaires de
d&#142;pendance (Lombardo 1996). A l&#8217;inverse de l&#8217;algorithme CKY, l&#8217;algorithme d&#8217;Earley est un
algorithme descendant. L&#8217;algorithme d&#8217;Earley  fonctionne &#142;galement en temps 0(n3) (o&#157; n est le
nombre de mots de la phrase) et m&#144;me en temps 0(n2) pour les grammaires non ambigu&#145;.
N&#142;anmoins l&#8217;algorithme d&#8217;Earley n&#8217;est pas pr&#142;cis&#142;ment adapt&#142; &#136; la langue naturelle qui est
hautement ambigu&#145;. En particulier, cet algorithme, s&#8217;il appara&#148;t comme plut&#153;t incr&#142;mental,
oblige en fait &#136; construire l&#8217;arbre &#136; partir de la racine et &#136; anticiper, d&#143;s la lecture du premier mot,
sur la cha&#148;ne compl&#143;te de ces anc&#144;tres dans l&#8217;arbre.42
</p>
<p>                                                
</p>
<p>41
 Il est probable qu&#8217;en l&#8217;absence d&#8217;une borne sur le nombre d&#8217;&#142;l&#142;ments lift&#142;s, le probl&#143;me de la reconnaissance
</p>
<p>par une telle grammaire soit NP-complet (cf. Neuhaus &amp; Br&#154;ker 1997 pour un r&#142;sultat de ce type).
42
</p>
<p> Pour cette raison, l&#8217;algorithme d&#8217;Earley n&#8217;est pas applicable pour des grammaires d&#142;crivant des constructions
r&#142;cursives (un V peut subordonner un V qui subordonne un V qui &#201;) sans introduire une limitation sur la
profondeur de la r&#142;cursion. Ceci est un vrai probl&#143;me si on veut traiter de la langue naturelle. Par exemple, en
fran&#141;ais, anglais, allemand, etc., le degr&#142; d&#8217;ench&#137;ssement d&#8217;un groupe topicalis&#142; en d&#142;but de phrase est
potentiellement infini.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Grammaires de d&#142;pendance formelles et th&#142;orie Sens-Texte
</p>
<p>5.3 Analyse incr&#142;mentale
On appelle analyse incr&#142;mentale une analyse qui se d&#142;veloppe au fur et &#136; mesure de la lecture.
</p>
<p>Les diff&#142;rents algorithmes d&#8217;analyse incr&#142;mentale diff&#143;rent sur le traitement de l&#8217;ambigu&#149;t&#142;.
Lorsque deux r&#143;gles concurrentes sont applicables deux strat&#142;gies sont possibles : 1) choisir une
des deux r&#143;gles et en cas d&#8217;&#142;chec revenir en arri&#143;re (back-track) et essayer la deuxi&#143;me r&#143;gle ou
2) mener en parall&#143;le les deux analyses. Nous appellerons analyse incr&#142;mentale stricte une
analyse incr&#142;mentale qui ne permet pas de retour en arri&#143;re.
</p>
<p>5.3.1  Analyse incr&#142;mentale et cognition
</p>
<p>L&#8217;analyse incr&#142;mentale est la plus s&#142;duisante des techniques d&#8217;analyse du point de vue cognitif.
Il est clair que les humains analysent un texte au fur et &#136; mesure qu&#8217;il en prenne connaissance et
qu&#8217;il peuvent parfaitement faire l&#8217;analyse d&#8217;un d&#142;but de phrase et m&#144;me en proposer des
continuations.
</p>
<p>Des exp&#142;riences de psycholinguistique montrent par ailleurs que dans certains cas d&#8217;ambigu&#149;t&#142;
majeure, les sujets humains font des retours en arri&#143;re (O&#8217;Regan &amp; Pynte 1992). Les exemples
de ce type sont appel&#142;s des garden paths. Par exemple, lors de la lecture de (12b), on observe
(par l&#8217;analyse du mouvement des yeux) au moment de la lev&#142;e d&#8217;ambigu&#149;t&#142;, lorsque est est
consid&#142;r&#142;, une saccade r&#142;gressive sur reconduit.
</p>
<p>(12) a. L&#8217;espion russe reconduit &#136; la fronti&#143;re un espion international.
b. L&#8217;espion russe reconduit &#136; la fronti&#143;re est un espion international.
</p>
<p>Dans la suite, nous allons donc nous int&#142;resser &#136; des algorithmes d&#8217;analyse incr&#142;mentale non
stricte, lesquels correspondent davantage au fonctionnement humain. Nous reviendrons dans la
Section 5.3.3 sur la question de savoir quelles sont les situations o&#157; doit &#144;tre fait un choix,
conduisant &#142;ventuellement &#136; un &#142;chec, et quelles sont les situations o&#157; il faut &#142;viter de traiter
s&#142;par&#142;ment deux options.
</p>
<p>Du point de vue computationnel, il est permis de penser que dans la mesure o&#157; un syst&#143;me de
TAL cherche &#136; obtenir les m&#144;mes r&#142;sultats qu&#8217;un humain, la meilleure technique consiste &#136;
chercher &#136; simuler au maximum la fa&#141;on dont proc&#143;de un humain.
</p>
<p>5.3.2  Analyseurs &#136; pile
</p>
<p>Laissons de c&#153;t&#142; la question de l&#8217;ambigu&#149;t&#142; pour le moment. On peut associer un analyseur &#136;
pile &#136; un module syntaxique avec des r&#143;gles de lin&#142;arisation comme celles que nous avons
pr&#142;sent&#142;es dans les Sections 3.3.5 et 4.2.2 (qui indiquent quels sont les couples de mots qui
peuvent &#144;tre reli&#142;s entre eux). La technique consiste &#136; charger dans la pile les mots au fur et &#136;
mesure de la lecture et &#136; relier les mots par des op&#142;rations au sommet de la pile (Kornai &amp; Tuza
1992, Kahane 2000b). Nous allons montrer sur un exemple comment fonctionne pr&#142;cis&#142;ment
l&#8217;analyseur &#136; pile associ&#142; &#136; notre module syntaxique. Nous commenterons nos r&#143;gles sur
l&#8217;exemple de la Figure 30.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Sylvain Kahane
</p>
<p>Figure 30 : Analyse de (11)
</p>
<p>L&#8217;analyseur effectue la lecture de la phrase de gauche &#136; droite. Au d&#142;part, la pile est vide (&#142;tape
0 : initialisation). Nous avons quatre types de r&#143;gles de transition.
1) Transition d&#8217;empilement (&#142;tapes 1, 2, 4, 6 7). A chaque fois qu&#8217;un mot nouveau est lu,
</p>
<p>une r&#143;gle morphologique est d&#142;clench&#142;e et la cat&#142;gorie du mot analys&#142; est stock&#142;e dans la
pile. Trois autres param&#143;tres compl&#143;tent la cat&#142;gorie : le deuxi&#143;me param&#143;tre indique si le
n&#711;ud est d&#142;j&#136; gouvern&#142; ou non (- pour non gouvern&#142;, + pour gouvern&#142;), le troisi&#143;me
param&#143;tre donne le valeur du trait de position de la derni&#143;re r&#143;gle de lin&#142;arisation avec un
</p>
<p>&#221;
</p>
<p>D        N         A        Cl        V
</p>
<p>mod
&#220;
</p>
<p>suj
&#220;
</p>
<p>obj
&#220;
</p>
<p>d&#142;t
</p>
<p>&#221;
</p>
<p>D        N         A        Cl        V
</p>
<p>mod
&#220;
</p>
<p>obj
&#220;
</p>
<p>d&#142;t
</p>
<p>&#221;
</p>
<p>D        N         A        Cl        V
</p>
<p>mod
&#220;
</p>
<p>obj
&#220;
</p>
<p>d&#142;t
</p>
<p>&#221;
</p>
<p>D        N         A        Cl        V
</p>
<p>mod
&#220;
</p>
<p>d&#142;t
</p>
<p>&#221;
</p>
<p>D        N         A        Cl
</p>
<p>mod
&#220;
</p>
<p>d&#142;t
</p>
<p>&#221;
</p>
<p>D        N         A 
</p>
<p>mod
&#220;
</p>
<p>d&#142;t
</p>
<p>D        N         A   
</p>
<p>&#220;
</p>
<p>d&#142;t
</p>
<p>D        N   
</p>
<p>&#220;
</p>
<p>d&#142;t
</p>
<p>D        N       
</p>
<p>D   
</p>
<p>V    -  -10    0
</p>
<p>&#220;
</p>
<p>suj, pos: -10
</p>
<p>(N) (V)
</p>
<p>V&#9;   -   -4     0
N    -   -10 +5
</p>
<p>V&#9;   -   -4     0
A    +    0     0
N    -   -10 +5
</p>
<p>V&#9;   -     0     0
Cl   -     0     0
A    +    0     0
N    -   -10 +5
</p>
<p>Cl   -     0     0
A    +    0     0
N    -   -10 +5
</p>
<p>A    +    0     0
N    -   -10 +5
</p>
<p>N&#9;   -  -10    0
</p>
<p>N&#9;   -     0    0
D    -     0    0
</p>
<p>D&#9;   -     0    0
</p>
<p>A    -     0    0
N    -  -10    0
</p>
<p>&#220;
</p>
<p>obj, pos: -4
</p>
<p>(Cl) (V)
</p>
<p>mod, pos: +5
</p>
<p>(N) (A)
&#221;
</p>
<p>&#220;
</p>
<p>d&#142;t, pos: -10
</p>
<p>(D) (N)
</p>
<p>le  &#8660;  LE
&#9;&#9;&#9;&#9;&#9;&#9;&#9;&#9;&#9;&#9; (D)masc,sg
</p>
<p>boucher  &#8660;  BOUCHER
                     (N,masc)sg
</p>
<p>sale  &#8660;  SALE
              (Adj)masc,sg
</p>
<p>la  &#8660;  LUI
           (Cl)acc,f&#142;m,sg
</p>
<p>tranche  &#8660;  TRANCHER
                    (V)ind,pr&#142;s,3,sg
</p>
<p>d&#142;pilement
</p>
<p>INPUT                                        PILE                R&#216;GLES
</p>
<p>initialisation
</p>
<p>le boucher sale    la  tranche
</p>
<p>0
</p>
<p>1
</p>
<p>2
</p>
<p>3
</p>
<p>4
</p>
<p>5
</p>
<p>6
</p>
<p>7
</p>
<p>8
</p>
<p>9
</p>
<p>10</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Grammaires de d&#142;pendance formelles et th&#142;orie Sens-Texte
</p>
<p>position n&#142;gative utilis&#142;e et le quatri&#143;me param&#143;tre donne la valeur du trait de position de la
derni&#143;re r&#143;gle de lin&#142;arisation avec une position positive utilis&#142;e. Lors du stockage, ces
param&#143;tres ont respectivement la valeur -, 0 et 0.
</p>
<p>2) Transition de liaison &#136; un d&#142;pendant &#136; gauche (&#142;tapes 3, 8, 10). Une telle transition
correspond &#136; une r&#143;gle de lin&#142;arisation dont le d&#142;pendant est &#136; gauche. Les deux n&#711;uds que
nous allons lier se trouvent dans les deux cases sup&#142;rieures de la pile. Appelons les x et y, x
&#142;tant le dernier n&#711;ud lu et se trouvant sur le dessus de la pile. La r&#143;gle peut s&#8217;appliquer si x
et y poss&#143;dent les cat&#142;gories requises par la r&#143;gle de lin&#142;arisation. Le n&#711;ud y ne doit pas &#144;tre
d&#142;j&#136; gouvern&#142; (valeur - du deuxi&#143;me param&#143;tre). Enfin, la valeur du trait de position de la
r&#143;gle doit &#144;tre sup&#142;rieure en valeur absolue &#136; celle de la derni&#143;re r&#143;gle avec une position
n&#142;gative utilis&#142;e pour relier x &#136; un d&#142;pendant &#136; sa gauche (voir le troisi&#143;me param&#143;tre). Ainsi
&#136; l&#8217;&#142;tape 8, le verbe tranche (= x) est li&#142; avec le clitique la (= y) par un r&#143;gle de position -4.
Apr&#143;s application de la r&#143;gle, le troisi&#143;me param&#143;tre de tranche prend la valeur -4. A l&#8217;&#142;tape
10, le verbe tranche est li&#142; &#136; son sujet par une r&#143;gle de position -10, ce qui est possible car  
-10 est sup&#142;rieur en valeur absolue &#136; -4. Lors de l&#8217;application de la r&#143;gle, la valeur -10 est
consign&#142;e &#136; la place de -4 dans la case de tranche. En raison de la projectivit&#142;, le n&#711;ud y est
retir&#142; de la pile. En effet, ce n&#711;ud ne peut avoir de d&#142;pendants &#136; la droite de x, sans
enfreindre la projectivit&#142;.
</p>
<p>3) Transition de liaison &#136; un gouverneur &#136; gauche (&#142;tape 5). Une telle transition
correspond &#136; une r&#143;gle de lin&#142;arisation dont le gouverneur est &#136; gauche. Comme
pr&#142;c&#142;demment, les deux n&#711;uds que nous allons lier se trouvent dans les deux cases
sup&#142;rieures de la pile. Appelons les x et y, x &#142;tant le dernier n&#711;ud lu et se trouvant sur le
dessus de la pile. La r&#143;gle peut s&#8217;appliquer si x et y poss&#143;dent les cat&#142;gories requises par la
r&#143;gle de lin&#142;arisation. Le n&#711;ud x ne doit pas &#144;tre d&#142;j&#136; gouvern&#142; (valeur - du param&#143;tre
correspondant). Apr&#143;s la transition, les n&#711;uds x et y sont tous les deux maintenus dans la
pile, puisqu&#8217;ils peuvent avoir tous deux des d&#142;pendants &#136; droite de x. Comme le n&#711;ud x est
maintenant gouvern&#142;, la valeur du param&#143;tre passe de - &#136; + . Enfin, la valeur du trait de
position de la r&#143;gle doit &#144;tre sup&#142;rieure en valeur absolue &#136; celle de la derni&#143;re r&#143;gle de
lin&#142;arisation avec une position positive utilis&#142;e pour relier y &#136; un d&#142;pendant &#136; sa gauche (voir
le quatri&#143;me param&#143;tre). A l&#8217;&#142;tape 5, le nom boucher (= y) n&#8217;a pas encore eu de d&#142;pendant &#136;
droite. Son quatri&#143;me param&#143;tre est donc &#142;gal &#136; 0. Apr&#143;s application de la r&#143;gle, ce
param&#143;tre aura la valeur +5.
</p>
<p>4) Transition de d&#142;pilement (&#142;tape 9). A tout moment, il est possible de retirer de la pile
un n&#711;ud qui est d&#142;j&#136; gouvern&#142;. Pour pouvoir lier tranche &#136; son sujet boucher, on est ainsi
oblig&#142; de d&#142;piler l&#8217;adjectif sale (qui de toute fa&#141;on, en raison de la projectivit&#142; ne pourra plus
avoir de d&#142;pendant  au -del&#136; de tranche).
</p>
<p>Une phrase est reconnue si &#136; la fin de la lecture, la pile contient un unique n&#711;ud non gouvern&#142;,
qui est en fait la racine de l&#8217;arbre de d&#142;pendance. Notons que nous assurons bien que le graphe
construit est un arbre et que cet arbre est projectif.
L&#213;analyseur en flux de Vergne (2000) utilise une m&#142;thode similaire &#136; l&#8217;analyseur que nous
venons de pr&#142;senter, si ce n&#213;est qu&#213;il effectue un s&#142;quen&#141;age (chunking) pr&#142;alable de la phrase
et charge, au lieu des mots, les blocs (chunks) ainsi obtenus dans la pile (construisant ainsi un
arbre de d&#142;pendance sur les blocs).
Comme pour l&#8217;analyseur CKY, cet analyseur peut &#144;tre enrichi en prenant en compte des r&#143;gles
de plus haut niveau, notamment des r&#143;gles de sous-cat&#142;gorisation. On chargera alors dans la pile
non seulement les caract&#142;ristiques morphologiques d&#8217;un mot (sa cat&#142;gorie lexicale), mais aussi
ses caract&#142;ristiques s&#142;mantiques. Voir Nasr 1995, 1996, Kahane 2000a, Lombardo 1992 pour
des analyseurs de ce type (lesquels s&#8217;apparentent &#142;galement aux grammaires cat&#142;gorielles de
Ajdukiewicz-Bar-Hillel). Une variante de ces analyseurs consiste &#136; charger dans la pile non pas
des mots, mais les liens potentiels qu&#213;un mot peut avoir avec les mots qui le suivent. Cette
m&#142;thode repose sur une description compl&#143;te des valences possibles d&#8217;un mot. Le plus abouti</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Sylvain Kahane
</p>
<p>des analyseurs de ce type est la Link Grammar de Sleator &amp; Temperley 1993. Dans ce
formalisme lexicalis&#142;, chaque r&#143;gle d&#142;crit l&#8217;ensemble des liens que peut avoir un mot donn&#142;,
c&#8217;est-&#136;-dire les actants, mais aussi les modifieurs et les conjoints &#142;ventuels, ainsi que le
gouverneur.
</p>
<p>Ces m&#142;thodes peuvent &#144;tre complexifi&#142;es pour traiter des arbres non projectifs : on peut, par
exemple, garder en m&#142;moire dans la pile dans la case d&#8217;un mot donn&#142; des informations sur
certains de ses d&#142;pendants et autoriser les dits d&#142;pendant &#136; cr&#142;er des liens lorsque la case de leur
gouverneur est consid&#142;r&#142;e. Nous ne d&#142;velopperons pas cette question, par ailleurs fort
int&#142;ressante, dans cette pr&#142;sentation (voir Nasr 1995, 1996, Kahane 2000a pour des traitements
de ce type).
</p>
<p>5.3.3  Traitement des ambigu&#149;t&#142;s
</p>
<p>Comment traiter les cas d&#8217;ambigu&#149;t&#142; avec un analyseur incr&#142;mental ? La premi&#143;re technique
consiste &#136; faire des choix. A chaque fois que plusieurs r&#143;gles concurrentes se pr&#142;sentent, il
faudra choisir une r&#143;gle. On peut d&#142;velopper des heuristiques, pour &#136; chaque fois qu&#8217;un choix
se pr&#142;sente, faire le &#210;meilleur&#211; choix. En cas d&#8217;&#142;chec, on peut effectuer un retour en arri&#143;re au
dernier point o&#157; un choix a &#142;t&#142; fait et essayer le choix suivant. Si on autorise les retours en
arri&#143;re sans m&#142;canismes additionnels, on obtient un algorithme en temps exponentiel dans le
pire des cas. En particulier, le pire des cas sera atteint &#136; chaque fois qu&#8217;on aura affaire &#136; une
phrase agrammaticale (= pour laquelle notre grammaire ne peut fournir d&#8217;analyse). Pour obtenir
un temps de traitement raisonnable, deux techniques sont possibles. La premi&#143;re consiste
simplement &#136; limiter les retours en arri&#143;re : on peut par exemple se fier enti&#143;rement aux
heuristiques qui nous aident &#136; faire le meilleur choix et interdire tout retour en arri&#143;re. On obtient
alors un traitement en temps lin&#142;aire. C&#8217;est ce que fait l&#8217;analyseur en flux de Vergne 2000. Voir
&#142;galement Arnola 1998 pour un analyseur d&#142;terministe bas&#142; sur les d&#142;pendances. La deuxi&#143;me
technique, appeler m&#142;mo&#149;sation,  consiste, lors d&#8217;un retour en arri&#143;re, &#136; conserver en m&#142;moire
les analyses d&#142;j&#136; faites pour ne pas avoir &#136; les refaire. Par exemple, si l&#8217;on consid&#143;re l&#8217;exemple
(11b) de garden-path, il ne sera pas n&#142;cessaire apr&#143;s le retour en arri&#143;re et le deuxi&#143;me choix
pour reconduit de refaire l&#8217;analyse de &#136; la fronti&#143;re (ce qui peut devenir vraiment int&#142;ressant si
fronti&#143;re gouverne en plus une relative) La m&#142;mo&#149;sation, utilis&#142;e par Sleator &amp; Temperley 1993
pour les Link Grammars, permet d&#8217;assurer une complexit&#142; en 0(n3).
La deuxi&#143;me technique de traitement des ambigu&#149;t&#142;s consiste &#136; ne pas faire de choix et &#136; mener
en parall&#143;le les diff&#142;rentes analyses. Par exemple, Nasr 1995, 1996 utilise une technique
adapt&#142;e de Tomita 1988 consistant &#136; dupliquer la pile ; on peut ensuite factoriser un certain
nombre d&#8217;op&#142;rations effectu&#142;es plusieurs fois dans plusieurs piles en factorisant les piles au sein
d&#8217;une pile &#136; structure de graphe et garantir un temps de traitement polynomial.
</p>
<p>J&#8217;aimerais faire quelques commentaires sur la question du choix en me pla&#141;ant d&#8217;un point de vue
linguistique et cognitif. Certaines grammaires, notamment des grammaires compl&#143;tement
lexicalis&#142;es comme TAG, consid&#143;rent des r&#143;gles diff&#142;rentes pour chacune des sous-
cat&#142;gorisations d&#8217;un verbe (par exemple, parler &#136; Marie, parler de Jean, parler de Jean &#136; Marie
correspondront &#136; trois r&#143;gles diff&#142;rentes pour parler). De telles grammaires obligent &#136; faire des
choix &#136; tout va et ne font pas la diff&#142;rence entre des choix non pertinents (comme les diff&#142;rentes
sous-cat&#142;gorisation de parler qui devraient &#144;tre trait&#142;es en parall&#143;le) et des choix pertinents
(comme les deux reconduit que l&#8217;on trouve en (11a) et (11b), qui diff&#143;rent fortement puisque le
premier gouverne le nom qui le pr&#142;c&#143;de, tandis que le deuxi&#143;me en d&#142;pend). Je pense que la
grammaire doit &#144;tre &#142;crite de telle fa&#141;on que seuls les choix r&#142;els (les choix qui pourront
conduire un locuteur &#136; un retour en arri&#143;re s&#8217;il n&#8217;a pas fait le bon choix du premier coup)
correspondent &#136; des r&#143;gles s&#142;par&#142;es. Par exemple, les diff&#142;rentes sous-cat&#142;gorisations possibles
d&#8217;un m&#144;me verbe devront &#144;tre rassembl&#142;es en une m&#144;me r&#143;gle. Il serait souhaitable d&#8217;&#142;valuer
pr&#142;cis&#142;ment les situations qui provoquent des retours en arri&#143;re chez un locuteur afin de d&#142;cider
quand deux constructions doivent &#144;tre trait&#142;es par une m&#144;me r&#143;gle et quand deux situations
doivent correspondre &#136; deux r&#143;gles bien s&#142;par&#142;es entre lesquelles le locuteur doit faire un choix.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Grammaires de d&#142;pendance formelles et th&#142;orie Sens-Texte
</p>
<p>On pourra noter toute l&#8217;attention que nous avons port&#142;e &#136; cette question dans l&#8217;&#142;criture des
r&#143;gles de GUST (Section 4.2).
</p>
<p>5.3.4  Limitation du flux
On peut observer sur les arbres de d&#142;pendances ordonn&#142;s des phrases d&#8217;une langue certaines
limitations. Ainsi, bien qu&#8217;en l&#8217;absence de larges corpus &#142;tiquet&#142;s par des d&#142;pendances nous ne
puissions &#144;tre absolument affirmatif, il appara&#148;t que le flux des d&#142;pendances est g&#142;n&#142;ralement
born&#142; par 6 ou 7 (voir Yngve 1960, Tuza &amp; Kornai 1992 ou Murata et al. 2001 pour des
hypoth&#143;ses de cette nature). Nous appelons flux des d&#142;pendances en une position donn&#142;e (entre
deux mots d&#8217;une phrase) le nombre de d&#142;pendances qui relient un mot &#136; gauche de cette position
&#136; un mot &#136; droite (Figure 31).
</p>
<p>Figure 31 : Le flux des d&#142;pendances pour (11)
</p>
<p>On peut penser que cette borne sur le flux correspond &#136; une limitation li&#142;e &#136; la m&#142;moire
imm&#142;diate: un locuteur ne peut g&#142;rer simultan&#142;ment plus de 7 d&#142;pendances (voir la fameuse
&#142;tude de Miller 1956 sur le fait que les humains ont au plus 7 &#177; 2 &#142;l&#142;ments dans leur m&#142;moire &#136;
court terme).
Si l&#8217;on borne le flux des d&#142;pendances, on peut alors borner la taille de la pile dans l&#8217;analyseur &#136;
pile que nous avons pr&#142;sent&#142;. Comme le langage de la pile est fini, le nombre de contenus
possible de la pile est alors fini (bien que tr&#143;s gros). L&#8217;analyseur &#136; pile, si l&#8217;on ne s&#8217;int&#142;resse
plus aux arbres de d&#142;pendance qu&#8217;il produit, est alors &#142;quivalent &#136; un automate &#136; nombre fini
d&#8217;&#142;tats (un &#142;tat de l&#8217;automate est un contenu de la pile). Cet automate est donc &#142;quivalent &#136; un
automate d&#142;terministe, ce qui nous donne un reconnaisseur en temps lin&#142;aire (l&#8217;automate ne
fournit plus d&#8217;analyse, mais peut seulement reconna&#148;tre les phrases qui ont une analyse). Cet
automate peut &#144;tre particuli&#143;rement utile pour filtrer les phrases agrammaticales, qui sont les
phrases les plus co&#158;teuses pour l&#8217;analyseur incr&#142;mental (puisque n&#8217;importe quel choix conduit &#136;
une situation d&#8217;&#142;chec et &#136; un retour en arri&#143;re). N&#142;anmoins, pour que cet automate ne soit pas
trop gros, il faudra certainement limiter le nombre de symboles de pile (le nombre d&#8217;&#142;tat de
l&#8217;automate avant d&#142;terminisation est major&#142; par Zk o&#157; Z est le nombre de symboles de pile et k le
nombre maximum de n&#711;uds autoris&#142;s dans la pile).
On peut &#142;galement esp&#142;rer optimiser l&#8217;analyseur incr&#142;mental par une &#142;tude statistique des
contenus possibles de la pile pour des analyses correctes. Ceci permettrait dans une situation
donn&#142;e de choisir entre des r&#143;gles afin d&#8217;obtenir le contenu de pile le plus probable et d&#8217;&#142;viter au
maximum les &#142;checs et les retours en arri&#143;re.
</p>
<p>6 Conclusion
Comme nous l&#8217;avons dit au d&#142;but de cet expos&#142;, la d&#142;pendance est maintenant une notion
utilis&#142;e par toutes les th&#142;ories linguistiques, bien qu&#8217;elle soit souvent cach&#142;e sous diverses
formes (fonctions syntaxiques, constituants avec t&#144;te, &#201;). Nous esp&#142;rons avoir convaincu le
lecteur de l&#8217;int&#142;r&#144;t qu&#8217;il y a &#136; mettre en avant la d&#142;pendance et &#136; &#142;crire des r&#143;gles qui manipulent
explicitement des d&#142;pendances.
</p>
<p>le boucher sale    la  tranche
</p>
<p>&#221;
</p>
<p>0         1          2        1          2         0
</p>
<p>mod
suj
</p>
<p>&#220;
</p>
<p>obj
&#220;
</p>
<p>d&#142;t</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Sylvain Kahane
</p>
<p>A travers l&#8217;&#142;tude des d&#142;pendances, nous avons souhait&#142; mettre l&#8217;accent sur la th&#142;orie Sens-
Texte. La TST est l&#8217;une des th&#142;ories qui s&#142;pare le plus clairement les notions s&#142;mantiques,
syntaxiques et morphologiques, en distinguant, en particulier, les d&#142;pendances s&#142;mantiques et
syntaxiques, les  lexies profondes et de surface, les gramm&#143;mes profonds et de surface ou en
s&#142;parant clairement les r&#143;gles de sous-cat&#142;gorisation, d&#8217;ordre des mots, d&#8217;accord et de rection.
D&#8217;autre part, la TST, en privil&#142;giant la synth&#143;se sur l&#8217;analyse, met bien en &#142;vidence l&#213;avantage
des grammaires de d&#142;pendance sur les grammaires syntagmatiques. En effet, la synth&#143;se d&#142;bute
avec une repr&#142;sentation s&#142;mantique, &#136; un moment o&#157; l&#8217;ordre des mots n&#8217;est pas encore fix&#142;.
Lorsqu&#8217;on veut d&#142;crire la synth&#143;se d&#8217;une phrase en passant d&#8217;une repr&#142;sentation s&#142;mantique &#136;
une repr&#142;sentation o&#157; les mots sont ordonn&#142;s, on voit tout l&#213;avantage qu&#213;il y a &#136; avoir un moyen
de repr&#142;senter la structure syntaxique sans avoir encore encoder l&#213;ordre des mots ou m&#144;me le
regroupement des mots en constituants de surface. La synth&#143;se met &#142;galement l&#8217;accent sur
l&#8217;importance des choix lexicaux et du lexique. En particulier, un notion comme celle de fonction
lexicale prend toute son importance lorsqu&#8217;il faut faire les bons choix lexicaux (et ne pas dire
follement improbable ou hautement amoureux &#136; la place de hautement improbable ou de
follement amoureux).
Nous avons &#142;galement pr&#142;sent&#142; une grammaire d&#8217;unification bas&#142;e sur la TST, la Grammaire
d&#8217;Unification Sens-Texte (GUST). Au del&#136; de son int&#142;r&#144;t propre, ce formalisme permet de
rattacher plus facilement la TST &#136; d&#8217;autres formalismes contemporains, comme HPSG, LFG,
les Grammaires Cat&#142;gorielles ou TAG. GUST h&#142;rite de la TST une claire s&#142;paration des
informations s&#142;mantiques, syntaxiques et morphologiques et  la modularit&#142; qui en r&#142;sulte. Nous
avons pu, au travers de GUST, montrer comment les r&#143;gles de diff&#142;rents modules pouvaient
&#144;tre combin&#142;es, permettant, par exemple, d&#8217;&#142;crire une grammaire compl&#143;tement lexicalis&#142;e.
D&#8217;autre part, contrairement aux grammaires compl&#143;tement lexicalis&#142;es &#142;crites dans d&#8217;autres
formalismes (comme TAG), GUST permet de porter une grande attention &#136; la fa&#141;on dont les
r&#143;gles de la grammaire modulaire doivent &#144;tre r&#142;parties entre les diff&#142;rentes lexies pour &#142;viter
une explosion du nombre de r&#143;gles de la grammaire lexicalis&#142;e.
</p>
<p>Nous avons termin&#142; notre expos&#142; par une pr&#142;sentation th&#142;orique des principales techniques
d&#8217;analyse. Il faut noter que les grammaires de d&#142;pendances, &#136; la diff&#142;rence des grammaires
syntagmatiques, n&#8217;ont pas encore fait l&#8217;objet de travaux math&#142;matiques ou d&#8217;informatique
th&#142;orique d&#8217;envergure. Il n&#8217;existe pas pour les grammaires de d&#142;pendance de formalisme de
r&#142;f&#142;rence (voir Kahane 2000b pour une proposition), comme le sont les grammaires de
r&#142;&#142;criture hors-contextes de Chomsky (1957) pour la grammaire syntagmatique. De m&#144;me, tous
les compilateurs de langages de programmation sont bas&#142;s sur des techniques d&#142;velopp&#142;es pour
les grammaires hors-contextes. Nous esp&#142;rons avoir montr&#142; que les m&#144;mes techniques (comme
l&#8217;algorithme CKY) se pr&#144;taient au traitement des grammaires de d&#142;pendance et qu&#8217;en plus, les
grammaires de d&#142;pendance permettaient des techniques propres, comme l&#8217;analyse incr&#142;mentale
avec un analyseur &#136; pile dont les cases de la pile contiennent les descriptions des mots de la
phrase. D&#8217;autre part, nous avons pu faire le lien entre GUST et HPSG, montrant comment les
grammaires syntagmatiques se pr&#142;sentent en fait comme des versions proc&#142;durales des
grammaires de d&#142;pendance orient&#142;es vers l&#8217;analyse (et plus pr&#142;cis&#142;ment l&#8217;analyse CKY, qui
n&#8217;est pas, du point de vue cognitif et m&#144;me computationnel,  le plus int&#142;ressant des algorithmes
d&#8217;analyse de la langue )43.
Nous souhaiterions clore cet expos&#142;, en &#142;voquant ce que nous aurions aim&#142; pr&#142;sent&#142; et que
nous n&#8217;avons pu pr&#142;senter faute d&#8217;une maturit&#142; suffisante des notions concern&#142;es et d&#8217;un
d&#142;veloppement suffisant des travaux sur ces questions. Dans la Section 3.2.1, nous avons
montr&#142; le r&#153;le primordial que joue  la structure communicative dans la repr&#142;sentation s&#142;mantique
d&#8217;une phrase, mais nous n&#8217;avons pas pu montrer comment la structure communicative
                                                
</p>
<p>43
 Quand on sait que les fondements de la grammaire syntagmatique reposent sur le distributionnalisme, c&#8217;est-&#136;-
</p>
<p>dire sur une description des langues par la distribution des segments de textes, il n&#8217;est pas &#142;tonnant que la
grammaire syntagmatique ait un lien &#142;troit avec un algorithme de type CKY.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Grammaires de d&#142;pendance formelles et th&#142;orie Sens-Texte
</p>
<p>intervenait dans les diff&#142;rentes r&#143;gles des diff&#142;rents niveaux. La structure communicative joue
un r&#153;le essentiel dans la hi&#142;rarchisation du graphe s&#142;mantique (notamment le choix de la t&#144;te
syntaxique de la phrase) et dans la lin&#142;arisation. Dans les langues &#136; ordre des mots relativement
libre comme le russe ou l&#8217;allemand, la structure communicative (notamment la partition th&#143;me-
rh&#143;me et la focalisation) contr&#153;le fortement l&#8217;ordre des mots et la prosodie. Dans des langues &#136;
l&#8217;ordre moins libre, comme le fran&#141;ais ou l&#8217;anglais, la structure communicative se r&#142;alise par des
constructions particuli&#143;res, comme le clivage, le pseudo-clivage ou la dislocation en fran&#141;ais.
D&#8217;autre part, nous n&#8217;avons pas abord&#142; la question des constituants morphologiques : les mots,
lorsqu&#8217;ils sont lin&#142;aris&#142;s, s&#8217;assemblent pour former des groupes qui sont plac&#142;s les uns par
rapport aux autres. Ces constituants morphologiques sont mis en &#142;vidence, entre autres, par la
prosodie. La notion de constituant morphologique doit &#144;tre distingu&#142;e de la notion de constituant
syntaxique, laquelle n&#8217;est pas directement  consid&#142;r&#142;e en grammaire de d&#142;pendance.44  Les
constituants morphologiques forment une hi&#142;rarchie comparable aux constituants syntaxiques,
mais il ne servent pas &#136; repr&#142;senter la structure syntaxique d&#8217;une phrase, laquelle est
repr&#142;sent&#142;e, dans notre cadre th&#142;orique, par un arbre de d&#142;pendance. Parmi les constituants
morphologiques, il faut en particulier distinguer les blocs (ou chunks) &#136; l&#8217;int&#142;rieur desquels
l&#8217;ordre des mots est tr&#143;s rigide et qui n&#8217;accepte pas de coupures prosodiques, comme les
s&#142;quences d&#142;terminant-adjectifs-nom ou clitiques-verbe du fran&#141;ais (Mel&#8217;&#139;uk 1967, Abney
1991, Vergne 2000). Le r&#153;le jou&#142; par de tels blocs (que ne consid&#143;rent d&#8217;ailleurs pas les
grammaires syntagmatiques) n&#8217;est plus &#136; faire en TAL, que ce soit pour l&#8217;analyse syntaxique ou
la synth&#143;se de la prosodie (Mertens 1997, Vergne 2000). La structure communicative joue un
grand r&#153;le, &#136; cot&#142; de la structure de d&#142;pendance,  dans la formation des constituants. Kahane &amp;
Gerdes 2001 propose, &#136; partir de l&#8217;&#142;tude de l&#8217;ordre des mots en allemand, un formalisme qui
permet d&#8217;associer &#136; un arbre de d&#142;pendance une hi&#142;rarchie de constituants morphologiques, qui
n&#8217;est pas le reflet imm&#142;diat de l&#8217;arbre de d&#142;pendance (et qui ne correspond donc pas non plus &#136;
une structure de constituants syntaxiques). Un m&#144;me arbre de d&#142;pendance correspond &#136; de
nombreux ordres des mots et un m&#144;me ordre des mots peut recevoir diff&#142;rentes structures de
constituants morphologiques correspondant &#136; diff&#142;rentes prosodies, mettant en &#142;vidence
diff&#142;rentes structures communicatives. Ce travail doit maintenant &#144;tre poursuivi pour montrer
comment une structure communicative permet de choisir une structure de constituants
morphologiques plut&#153;t qu&#8217;une autre.
</p>
<p>R&#142;f&#142;rences
Abeill&#142; Anne, 1991, Une grammaire lexicalis&#142;e d&#8217;Arbres Adjoints pour le fran&#141;ais, Th&#143;se de
doctorat, Universit&#142; Paris 7, Paris.
</p>
<p>Abeill&#142; Anne, 1996-97, &#210;Fonction objet ou position objet &#211; (1&#143;re et 2nde parties), Gr&#142; des
langues, 11, 8-29 ; 12, 8-33.
</p>
<p>Abney Steven, 1987, The English Noun Phrase in its Sentential Aspect, PhD thesis, MIT,
Cambridge.
</p>
<p>Abney Steven, 1991, &#210;Parsing by chunks&#211;, in R. Berwick, S. Abney and C. Tenny (eds.),
Principle-Based Parsing, Kluwer.
</p>
<p>Abney Steven, 1992, &#210;Prosodic structure, performance structure and phrase structure&#211;,
Proceedingsof Speech and Natural Language Workshop, Morgan Kaufmann, San Mateo, CA.
                                                
</p>
<p>44
 Comme nous l&#8217;avons montr&#142; dans la Section 2.1, les constituants syntaxiques consid&#142;r&#142;s par les grammaires
</p>
<p>syntagmatiques peuvent &#144;tre r&#142;cup&#142;r&#142;s &#136; partir de l&#8217;arbre de d&#142;pendance : ce sont les projections des sous-arbres
constitu&#142;s d&#8217;un n&#711;ud et de tout ou partie de ses d&#142;pendants. Par exemple, un constituant S ou Infl&#8217; est la
projection d&#8217;un verbe, tandis qu&#8217;un constituant GV est la projection d&#8217;un verbe sans son sujet.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Sylvain Kahane
</p>
<p>Anderson John, 1971, The Grammar of Case: Towards a Localist Theory, Cambridge
University Press, Cambridge.
</p>
<p>Ajdukiewicz Kasimir, 1935, &#210;Die syntaktische Konnexit&#138;t&#211;, Studia Philosophica, 1, 1-27.
Apresjan J., Boguslavskij I., Iomdin L., Lazurskij A., Sannikov V., Tsinman L., 1992,
&#210;ETAP-2: The linguistics of a machine-translation system&#211;, Meta, 37:1, 97-112.
</p>
<p>Arnola Harri, 1998, &#210;On parsing binary dependency structures deterministically in linear time&#211;,
Processing of Dependency-based Grammars, COLING/ACL&#8217;98 Workshop, 68-77.
Bar-Hillel Yehoshua, 1953, &#210;A quasi-arithmetical notation for syntactic description&#211;, Language,
29.1, 47-58.
</p>
<p>Bar-Hillel Yehoshua, Gaifman Ha&#149;m, Shamir E., 1960, &#210;On categorial and phrase-structure
grammars&#211;, Bull. Res. Counc. of Isra&#145;l, 9F, 1-16.
Blache Philippe, 1998, &#210;Parsing ambiguous structures using controlled disjunctions and unary
quasi-trees&#211;, COLING/ACL&#8217;98, Montr&#142;al, 124-30.
</p>
<p>Blache Philippe, 2001, Les grammaires de propri&#142;t&#142;s : des contraintes pour le traitement
automatique des langues naturelles, Herm&#143;s, 224p.
</p>
<p>Blanche-Benveniste Claire, 1975, Recherche en vue d&#8217;une th&#142;orie de la grammaire fran&#141;aise.
Essai d&#8217;application &#136; la syntaxe des pronoms, Champion, Paris.
</p>
<p>Bloomfield Leonard, 1933, Language, New York.
</p>
<p>Boyer Michel, Lapalme Guy, 1985, &#210;Generating paraphrases from Meaning-Text semantic
networks&#211;, Computational Intelligence, 1, 103-117.
</p>
<p>Bresnan Joan (ed), 1982, The Mental Representation of Grammatical Relations, MIT Press,
Cambridge.
</p>
<p>Bresnan Joan, Kaplan Ronald, Peters Stanley, Zaenen Annie, 1982, &#210;Cross-serial dependencies
in Dutch&#211; Linguistic Inquiry, 13:4, 613-635.
</p>
<p>Brody Michael, 1997, Lexico-Logical Form: A Radically Minimalist Theory, MIT Press,
Cambridge.
</p>
<p>Br&#154;ker Norbert, 2000, &#210;Unordered and non-projective dependency grammars&#211;, T.A.L., 41:1,
245-272.
</p>
<p>Candito Marie-H&#142;l&#143;ne, 1996, &#210;A principle-based hierarchical representation of LTAG&#211;,
COLING&#213;96, Copenhagen.
</p>
<p>Candito Marie-H&#142;l&#143;ne, 1999, Organisation modulaire et param&#142;trable de grammaires
&#142;lectroniques lexicalis&#142;es. Application au fran&#141;ais et &#136; l&#8217;italien, Th&#143;se de doctorat, Universit&#142;
Paris 7, Paris.
</p>
<p>Candito Marie-H&#142;l&#143;ne, Kahane Sylvain, 1998, &#210;Une grammaire TAG vue comme une
grammaire Sens-Texte pr&#142;compil&#142;e&#211;, TALN&#8217;98, Paris, 40-49.
</p>
<p>Chomsky Noam, 1957, Syntactic Structure, MIT Press, Cambridge.
</p>
<p>Chomsky Noam, 1965, Aspects of the Theory of Syntax, MIT Press, Cambridge.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Grammaires de d&#142;pendance formelles et th&#142;orie Sens-Texte
</p>
<p>Coch Jos&#142;, 1996, &#210;Overview of AlethGen&#211;, Proc. 8th Int. Workshop on Natural Language
Generation (INLG&#8217;96), Vol. 2, Herstmonceux, 25-28.
Coch Jos&#142;, 1998, &#210;Interactive generation and knowledge administration in MultiMeteo&#211;, Proc.
9th Int. Workshop on Natural Language Generation (INLG&#8217;98), Niagara-on-the-Lake, 300-
303.
</p>
<p>Courtin Jacques, Genthial Damien, &#210;Parsing with dependency relations and robust parsing&#211;,
Workshop on Dependency-based Grammars, COLING/ACL&#213;98, Montr&#142;al, 25-28.
</p>
<p>Danlos Laurence, 1998, &#210;G-TAG : un formalisme lexicalis&#142; pour la g&#142;n&#142;rationde textes inspir&#142;
de TAG&#211;, T.A.L., 39:2, 7-34.
</p>
<p>Dikovsky Alexander, Modina Larissa, 2000, &#210;Dependencies on the other side of the Curtain&#211;,
T.A.L., 41:1, 79-111.
</p>
<p>Duchier Denys, 1999, &#210;Axiomatizing dependency parsing using set constraints&#211;, Proc. 6th
Meeting of the Mathematics of Language (MOL 6), Orlando, 115-126.
Duchier Denys, Ralph Debusmann, 2001, &#210;Topological dependency trees: A constraint-based
account of linear precedence&#211;, ACL 2001, Toulouse.
</p>
<p>Dymetman Marc, Copperman Max, 1996, &#210;Extended dependency structures and their formal
interpretation&#211;, COLING&#8217;96, Copenhague, 255-61.
</p>
<p>Earley J., 1970, &#210;An efficient context-free parsing algorithm&#211;, Communications of the ACM,
13:2, 94-102.
</p>
<p>Eisner Jason M., 1996, &#210;Three new probabilistic models for dependency parsing: An
exploration&#211;, COLING&#8217;96, Copenhague.
</p>
<p>Engel Ulrich, 1992, Deutsche Grammatik.
</p>
<p>Floyd Robert, Biegel Richard, 1995, Le langage des machines : une introduction &#136; la
calculabilit&#142; et aux langages formels, International Thomson Publishing, Paris.
Gaifman Ha&#149;m, 1965, &#210;Dependency systems and phrase-structure systems&#211;, Information and
Control, 18, 304-337&#730;; Rand Corporation, 1961, RM-2315.
</p>
<p>Garde Paul, &#210;Ordre lin&#142;aire et d&#142;pendance syntaxique : contribution &#136; une typologie&#211;, Bull. Soc.
Ling. Paris, 72:1, 1-26.
</p>
<p>Gerdes Kim, Kahane Sylvain, 2001, &#210;Word order in German: A formal dependency grammar
using a topological hierarchy&#211;, ACL 2001, Toulouse.
</p>
<p>Gladkij Aleksej V., 1966, Leckii po matematiceskoj linguistike dlja studentov NGU,
Novosibirsk (French transl: Le&#141;ons de linguistique math&#142;matique, fasc. 1, 1970, Dunod).
Gladkij Aleksej V., 1968, &#210;On describing  the syntactic structure of a sentence&#211; (en russe avec
r&#142;sum&#142; en anglais), Computational Linguistics, 7, Budapest, 21-44.
Gross Maurice 1975, M&#142;thodes en syntaxe, Hermann, Paris.
</p>
<p>Hays David, 1960, &#210;Grouping and dependency theories&#211;, Technical report RM-2646, Rand
Corporation.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Sylvain Kahane
</p>
<p>Hays David, 1964, &#210;Dependency theory: A formalism and some observations&#211;, Language,
40:4, 511-525.
</p>
<p>Hellwig Peter, 1986, &#210;Dependency Unification Grammar (DUG)&#211;, COLING&#8217;86, 195-98.
Hudson Richard, 1988, &#210;Coordination and grammatical relations&#211;, Journal of Linguistics, 24,
303-342.
</p>
<p>Hudson Richard, 1990, English Word Grammar, Oxford ,Blackwell.
</p>
<p>Iordanskaja Lidija, 1963, &#210;O nekotoryx svojstvax pravil&#8217;noj sintaksi&#139;eskoj struktury (na
materiale russkogo jazyka)&#211; [On some Properties of Correct Syntactic Structure (on the Basis of
Russian)], Voprosy Jazykoznanija, 4, 102-12.
Iordanskaja L., Kim M., Kittredge R. I., Lavoie B., Polgu&#143;re A., 1992, &#210;Generation of
extended bilingual statistical reports&#211;, COLING&#8217;92, Nantes, 1019-23.
</p>
<p>Iordanskaja L., Mel&#8217;&#139;uk I., 2000, &#210;The notion of surface-syntactic relation revisited (Valence-
controlled surface-syntactic relations in French)&#211;, in L.L. Iomdin, L.P. Krysin (ed), Slovo v
tekste i v slovare [Les mots dans le texte et dans le dictionnaire], Jazuki Russkoj Kul&#8217;tury,
Moscou, 391-433.
</p>
<p>Jackendoff Ray, X-bar Syntax. A Study of Phrase Structure, MIT Press, Cambridge.
Jespersen Otto, 1924, Philosophy of Grammar, Londres.
Joshi Aravind, 1987, &#210;Introduction to Tree Adjoining Grammar&#211;, in Manaster Ramer (ed), The
Mathematics of Language, Benjamins, Amsterdam, 87-114.
Kahane Sylvain, 1996, &#210;If HPSG were a dependency grammar ...&#211;, TALN&#213;96, Marseille, 45-
49.
</p>
<p>Kahane Sylvain, 1997, &#210;Bubble trees and syntactic representations&#211;, in Becker T., Krieger U.
(eds), Proc. 5th Meeting of the Mathematics of Language (MOL5), DFKI, Saarbr&#159;cken, 70-76.
Kahane Sylvain, 1998, &#210;Le calcul des voix grammaticales&#211;, Bull. Soc. Ling. de Paris, 93:1,
325-48.
</p>
<p>Kahane Sylvain, 2000a, &#210;Extractions dans une grammaire de d&#142;pendance lexicalis&#142;e &#136; bulles&#211;,
T.A.L., 41:1, 211-243.
</p>
<p>Kahane Sylvain, 2000b, &#210;Des grammaires formelles pour d&#142;finir une correspondance&#211;, TALN
2000, Lausanne, 197-206.
</p>
<p>Kahane Sylvain (ed), 2000c, Grammaires de d&#142;pendance, T.A.L., 41:1, Herm&#143;s.
Kahane Sylvain, 2001, &#210;What is a natural language and how to describe it? Meaning-Text
approaches in contrast with generative approaches&#211;, Computational Linguistics and Intelligent
Text Processing, Springer, 1-17.
</p>
<p>Kahane Sylvain, Mel&#8217;&#139;uk Igor, 1999, &#210;La synth&#143;se s&#142;mantique ou la correspondance entre
graphes s&#142;mantiques et arbres syntaxiques. Le cas des phrases &#136; extraction en fran&#141;ais
contemporain&#211;, T.A.L., 40:2, 25-85.
</p>
<p>Kahane Sylvain, Nasr Alexis, Rambow Owen, 1998, &#210;Pseudo-projectivity: A polynomially
parsable non-projective dependency grammar&#211;, ACL/COLING&#8217;98, Montr&#142;al, 646-52.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Grammaires de d&#142;pendance formelles et th&#142;orie Sens-Texte
</p>
<p>Kahane Sylvain, Polgu&#143;re Alain (eds), 1998, Workshop on Dependency-Based Grammars,
ACL/COLING&#8217;98, Montr&#142;al.
</p>
<p>Kahane Sylvain, Polgu&#143;re Igor, 2001, &#210;Formal foundation of lexical functions&#211;, in B. Daille,
G. Williams (eds), Workshop on Collocation, ACL 2001, Toulouse.
Kamp Hans, 1981, &#210;Ev&#143;nements, repr&#142;sentations discursives et r&#142;f&#142;rence temporelle&#211;,
Langages, 64, 34-64.
</p>
<p>Kamp Hans, Reyle Uwe, 1993, From Discourse to Logic, Kluwer, Dordrecht.
</p>
<p>Kasami T., 1963, &#210;An efficient recognition and syntax analysis algorithm for context-free
languages,&quot; AFCRL-65-758, Air Force Cambridge Research Laboratory, Bedford, MA.
</p>
<p>Kasper R., Kiefer B., Netter K., Vijay-Shanker K., 1995, &#210;Compilation of HPSG to TAG&#211;,
ACL&#213;95.
</p>
<p>Keenan Edward, Comrie Bernard, 1977, &#210;Noun phrase accessibility and universal grammar&#211;,
Linguistic Inquiry, 8, 63-100.
</p>
<p>Kittredge Richard, Polgu&#143;re Alain, 1991, &#210;dependency grammars for bilingual text generation:
Inside FoG&#8217;s stratificational models&#211;, Proc. Int. Conf. on Current Issues in Computational
Linguistics, Penang, 318-30.
</p>
<p>Kornai Andre&#135;s, Tuza Zsolt, 1992, &#210;narrowness, pathwidth, and their application in natural
language processing&#211;, Disc. Appl. Math, 36, 87-92.
</p>
<p>Lavoie Benoit, Rambow Owen, 1997, &#210;RealPro: A fast, portable sentence realizer&#211;, Proc. 5th
Conf. On Applied Natural Language Processing (ANLP&#8217;97), Washington, 265-68.
Lecerf Yves, 1961, &#210;Une repr&#142;sentation alg&#142;brique de la structure des phrases dans diverses
langues natuelles&#211;, C. R. Acad. Sc. Paris, 252, 232-34.
</p>
<p>Lecomte Alain, 1992, &#210;Connection grammars: A graph-oriented interpretation&#211;, in Lecomte A.
(ed), Word Order in Categorial Grammar, Adosa, Clermont-Ferrand, 129-48.
Lombardo Vincenzo, 1992, &#210;Incremental dependency parsing&quot;, ACL&#8217;92, 291-93.
</p>
<p>Lombardo Vincenzo, 1996, &#210;An Earley-style parser for dependency grammars&#211;, COLING&#8217;96,
Copenhague.
</p>
<p>Lombardo Vincenzo, Leonardo Lesmo, 1998, &#210;Formal aspects and parsing issues of
dependency theory&#211;, COLING/ACL&#213;98, Montr&#142;al, 787-93.
</p>
<p>Lombardo Vincenzo, Lesmo&#730;Leonardo, 2000, &#210;A formal theory of dependency syntax with
empty units&#211;, T.A.L., 41:1, 179-210.
</p>
<p>Maruyama Hiroshi, 1990a, Constraint Dependency Grammar, Technical Report RT0044, IBM,
Tokyo.
</p>
<p>Maruyama Hiroshi, 1990b, &#210;structural disambiguisation with constraint propagation&#211;, ACL&#8217;90,
Pittsburgh, 31-38.
</p>
<p>Mel&#8217;&#139;uk Igor, 1967, &#210;Ordre des mots en synth&#143;se automatique des textes russes&#211;, T.A.
Informations, 8:2, 65-84.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Sylvain Kahane
</p>
<p>Mel&#8217;&#139;uk Igor, 1974, Opyt teorii linguisti&#139;eskix modelej &#210;Smysl Tekst&#211;. Semantika, Sintaksis
[Esquisse d&#8217;une th&#142;orie des mod&#143;les linguistiques &#210;Sens-Texte&#211;. S&#142;mantique, Syntaxe],
Moscou, Nauka, 314p.
</p>
<p>Mel&#8217;&#139;uk Igor, 1988a, Dependency Syntax: Theory and Practice, State Univ. of New York
Press, Albany.
</p>
<p>Mel&#8217;&#139;uk Igor, 1988b, &#210;Paraphrase et lexique: La Th&#142;orie Sens-Texteet le Dictionnaire explicatif
et combinatoire&#211;, in Mel&#8217;&#139;uk et al. 1988, 9-58.
</p>
<p>Mel&#8217;&#139;uk Igor, 1993-2001, Cours de morphologie g&#142;n&#142;rale, Vol. 1-5, Presses de l&#8217;Univ. de
Montr&#142;al / CNRS.
</p>
<p>Mel&#8217;&#139;uk Igor, 1997, Vers une Linguistique Sens-Texte, Le&#141;on inaugurale au Coll&#143;ge de France,
Coll&#143;ge de France, Paris, 78p.
</p>
<p>Mel&#8217;&#139;uk Igor, 2001, Communicative Organization in Natural Language (The Semantic-
Communicative Structure of Sentences), Benjamins, Amsterdam.
Mel&#8217;&#139;uk Igor, Clas Andr&#142;, Polgu&#143;re Alain, 1995, Introduction &#136; la lexicologie explicative et
combinatoire, Duculot, Paris.
</p>
<p>Mel&#8217;&#139;uk Igor, Pertsov Nikolaj, 1987, Surface Syntax of English. A Formal Model within the
Meaning-Text Framework, Benjamins, Amsterdam.
Mel&#8217;&#139;uk Igor, &#235;olkovsky Alexandr, 1984, Explanatory Combinatorial Dictionary of Modern
Russian, Wiener Slawistischer Almanach, Vienne.
</p>
<p>Mel&#8217;&#139;uk Igor et al., 1984, 1988, 1992, 1999, Dictionnnaire explicatif et combinatoire du
fran&#141;ais contemporain, Vol. 1, 2, 3, 4, Presses de l&#8217;Univ. de Montr&#142;al, Montr&#142;al.
Menzel Wolfgang, Schr&#154;der Ingo, 1998, &#212;Decision Procedures for Dependency Parsing Using
Graded Constraints&#211;, Workshop on Processing of Dependency-Based Grammars,
COLING/ACL&#8217;98, Montr&#142;al, 78-87.
</p>
<p>Mertens Piet, 1997, &#210;De la cha&#148;ne lin&#142;aire &#136; la s&#142;quence de tons&#211;, T.A.L., 38:1, 27-52.
</p>
<p>Mili&#141;evi&#141; Jasmina, 2001, &#210;A short guide to the Meaning-Text linguistic theory&#211;, in A. Gelbukh
(ed), Proc. of CICLing 2000, &#136; para&#148;tre chez Springer.
Miller George A., 1956, &#210;The magical number seven, plus or minus two: Some limits on our
capacity for processing information&#211;, The Psychological Review, 63, 81-97.
</p>
<p>Murata Masaki, Uchimoto Kiyotaka, Ma Qing, Isahara Hitoshi, 2001, &#210;Magical number seven
plus or minus two: Syntactic structure recognition in Japanese and English sentences&#211;, in A.
Gelbukh (ed), Computational Linguistics and Intelligent Text Processing, Springer, 43-52.
Nasr Alexis, 1995, &#210;A formalism and a parser for lexicalised dependency grammars&#211;, 4th Int.
Workshop on Parsing Tecnologies, State Univ. of NY Press.
</p>
<p>Nasr Alexis, 1996, Un mod&#143;le de reformulation automatique fond&#142; sur la Th&#142;orie Sens-Texte &#8212;
Application aux langues contr&#153;l&#142;es, Th&#143;se de doctorat, Universit&#142; Paris 7, Paris.
</p>
<p>Neuhaus Peter, Br&#154;ker Norbert, 1997, &#210;The complexity of recognition of linguistically adequate
dependency grammars&#211;, ACL/EACL&#8217;97, Madrid, 337-43.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Grammaires de d&#142;pendance formelles et th&#142;orie Sens-Texte
</p>
<p>O&#8217;Regan Kevin, Pynte Jo&#145;l, 1992, &#210;Regard et lecture&#211;, Sciences cognitives, Courrier du CNRS
n&#176; 79, CNRS, Paris, p. 16.
</p>
<p>Owens Jonathan, 1988, The Foundations of Grammar : An Introduction to Mediaeval Arabic
Grammatical Theory, Benjamins, Amsterdam.
Pe&#225;kovskij Aleksandr, 1934, Russkij sintaksis v nau&#139;nom osve&#228;&#139;nii [Syntaxe russe : une
approche scientifique], Moscou, U&#139;pedgiz.
Polgu&#143;re Alain, 1990, Structuration et mise en jeu proc&#142;durale d&#8217;un mod&#143;le linguistique
d&#142;claratif dans un cadre de g&#142;n&#142;ration de texte, Th&#143;se de doctorat, Universit&#142; de Montr&#142;al.
Polgu&#143;re Alain, 1992, &#210;Remarques sur les r&#142;seaux s&#142;mantiques Sens-texte&#213;, in A. Clas (ed), Le
mot, les mots, les bons mots, Presses de l&#8217;Univ. de Montr&#142;al, Montr&#142;al.
</p>
<p>Polgu&#143;re Alain, 1998, &#210;Pour un mod&#143;le stratifi&#142; de la lexicalisation en g&#142;n&#142;ration de texte&#211;,
T.A.L., 39:2, 57-76.
</p>
<p>Pollard Carl, Sag Ivan, 1994, Head-driven Phrase Structure Grammar, Stanford CSLI.
</p>
<p>Pustejovsky James, 1995, The Generative Lexicon, MIT Press, Cambridge.
Robinson Jane, 1970, &#210;Dependency structures and transformational rules&#211;, Language, 46, 259-
85.
</p>
<p>Sag I., Gazdar G., Wasow T., Wisler S., 1985, &#210;Coordination and how to distinguish
categories&#211;, Natural Language and Linguistic Theory, 3:2, 117-171.
</p>
<p>Schabes Yves, 1990, Mathematical and Computational Aspects of Lexicalized Grammars, PhD
thesis, University of Pennsylvania, Philadelphie.
</p>
<p>Schr&#154;der Ingo, Menzel Wolfgang, Foth Kilian, Schulz Michael, 2000, &#210;Modeling dependency
grammar with restricted constraints&#211;, T.A.L., 41:1, 113-44.
</p>
<p>Schubert Klaus, 1987, Metataxis: Contrastive Dependency Syntax for Machine Translation,
Foris, Dordrecht.
</p>
<p>Sgall Petr, Hajicov&#135; Eva, Panenov&#135; Jarmila, 1986, The Meaning of the Sentence in Its Semantic
and Pragmatic Aspects, Reidel, Dordrecht.
</p>
<p>Sleator Daniel, Temperley Davy, 1993, &#210;Parsing English with a Link Grammar&quot;, Third Int.
Workshop on Parsing Technologies ; Carnegie Mellon Univ. Comp. Sc. Techn. Report CMU-
CS-91-196, 1991.
</p>
<p>Tesni&#143;re Lucien, 1934, &#210;Comment construire une syntaxe&#211;, Bulletin de la Facult&#142; des Lettres de
Strasbourg, 7, 12&#143;me ann&#142;e, 219-229.
</p>
<p>Tesni&#143;re Lucien, 1959, &#131;l&#142;ments de syntaxe structurale, Kincksieck, Paris.
</p>
<p>Tomita Masaru, 1988, &#210;Graph structured stack and natural language parsing&#211;, ACL&#8217;88,
Buffalo.
</p>
<p>Vergne Jacques, 2000, &#131;tude et mod&#142;lisation de la syntaxe des langues &#136; l&#8217;aide de l&#8217;ordinateur -
Analyse syntaxique automatique non combinatoire, Th&#143;se d&#8217;HDR, Universit&#142; de Caen.
</p>
<p>Vijay-Shanker K., Yves Schabes, 1992, &#210;Structure sharing in Lexicalized TAG&#211;, COLING&#213;92.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Sylvain Kahane
</p>
<p>Wanner Leo (ed), 1996, Lexical Functions in Lexicography and Natural Language Processing,
Benjamins, Amsterdam.
Weiss Daniel, 1999, &#210;Sowjetische Sprachmodelle und ihre Weiter f&#159;rhung&#211;, Handbuch des
sprachwissenschaftlich Russistik und ihrer Grenzdisziplinen, Harrassowitz, 973-09.
XTAG Research Group, 1995, &#210;A Lexicalized Tree Adjoining Grammar for English&#211;, technical
Report IRCS 95-03, University of Pennsylvania (version mise &#136; jour sur le web).
Yngve&#730;Victor H., 1960, &#210;A model and an hypothesis for language structure&#211;, The American
Philosophical Society, 104:5, 444-66.
</p>
<p>Yngve&#730;Victor H., 1961, &#210;The Depth Hypothesis&#211;, Proceedings of Symposia in Applied
Mathematics, Vol. 12: Structure of Language and its Mathematical Aspects, American
Mathematical Society, Providence, 130-138.
</p>
<p>Younger D.H., 1967, Recognition of context-free languages in time n3&#211;, Information and
Control, 10:2, 189-208.
</p>
<p>&#235;olkovskij Aleksandr, Mel&#8217;&#139;uk Igor, 1965, &#210;O vozmo&#236;nom metode i instrumentax
semanti&#139;eskogo sinteza&#211; [Sur une m&#142;thode possible et des outils pour la synth&#143;se s&#142;mantique
(de textes)], Nau&#139;no-texni&#139;eskaja informacija [Scientific and Technological Information], 6, 23-
28.
</p>
<p>&#235;olkovskij Aleksandr, Mel&#8217;&#139;uk Igor, 1967, &#210;O semanti&#139;eskom sinteze&#211; [Sur la synth&#143;se
s&#142;mantique (de textes)], Problemy Kybernetiki [Probl&#143;mes de Cybern&#142;tique], 19, 177-238.
[trad. fran&#141;. : 1970, T.A. Information, 2, 1-85.]
Zwicky Arnold, 1985, &#210;Heads&#211;, Journal of Linguistics, 21, 1-29.</p>

</div></div>
</body></html>