TALN 2001, Tours, 2-5 juillet 2001

Grammaires de dépendance formelles et théorie Sens-Texte

Sylvain Kahane

Lattice, Université Paris 7, UFRL
Case 7003, 2, place Jussieu, 75251 Paris cedex 5
sk@ccr.jussieu.fr

Résumé — Abstract

On appelle grammaire de dépendance toute grammaire formelle qui manipule comme
représentations syntaxiques des structures de dépendance. Le but de ce cours est de présenter a
la fois les grammaires de dépendance (formalismes et algorithmes de synthese et d’analyse) et
la théorie Sens-Texte, une théorie linguistique riche et pourtant méconnue, dans laquelle la

dépendance joue un role crucial et qui sert de base théorique a plusieurs grammaires de
dépendance.

We call dependency grammar every grammar which handles dependency structures as syntactic
representations. The aim of this course is to present both dependency grammars (formalisms,
analysis and synthesis algorithms) and the MeaI1ing-Text theory, a rich but nevertheless
unrecognized linguistic theory, in which dependency plays a crucial role and which serves as
theoretical basis of several dependency grammars.

1 Introduction

La représentation syntaxique d'une phrase par un arbre de dépendance est certainement plus
ancienne que la représentation par un arbre syntagmatique. L'usage des dépendances remonte a
l'antiquité. Les grammaitiens arabes du 8ieme siecle, comme Sibawaih, distinguaient déja
gouvemeur et gouvemé en syntaxe et utilisait cette distinction pour formuler des regles d'ordre
des mots ou de rection (Owens 1988279-81). On trouve des représentations de structures de
dépendance dans des grammaires du 19éme siecle (Weber 1992213). La premiere théorie
linguistique basée sur la dépendance est incontestablement celle de Tesniere (1934, 1959), sans
minimiser des travaux précurseurs, comme les représentations “moléculaires” de Jespersen
(1924) ou la syntaxe du russe de Peskovskij (1934). Peu apres, Hays (1960, 1964) développait
la premiere grammaire de dépendance, tandis que Gaifman (1965) établissait les liens entre les
graInInaires de dépendance de Hays, les grammaires catégorielles de Bar-I-Iillel et les
grammaires de réécriture de Chomsky. A l'exception de la grammaire de Robinson (1970), les
grammaires de dépendance se sont ensuite surtout développées en Europe, notamment autour de
Sgall et Hajiéova 51 Prague (Sgall et al. 1986) et de Mel'éuk a Moscou (Mel'éuk 1974, 1988a),
ainsi qu'en Allemagne (cf., par ex., la classique grammaire de l'allemand de Engel 1992) et au
Royaume Uni autour de Anderson (1971) et Hudson (1990), la France restant curieusement a
l'écart.

La représentation syntaxique d'une phrase par une structure syntagmatique, quant a elle, ne
s'est développée qu'a partir de Bloomﬁeld (1933) et des travaux des distributionnalistes.
L'engouement formidable pour les grammaires génératives-transformationnelles de Chomsky
(1957, 1965) dans les années 60-70 a retardé l'essor des grammaires de dépendance. Pourtant,

Sylvain Kahane

depuis la ﬁn des annees 70 et l'avenement de la Syntaxe X-barre, la plupart des modeles
linguistiques issus de la mouvance chomskienne (GB/PP/MP, LFG, Gﬂ-IPSG) ont introduit
l'usage de la dependance syntaxique sous des formes plus ou moins cachees (fonctions
syntaxiques, constituants avec tete, cadre de sous-categorisation, c-commande). De leur cote,
les grammaires completement lexicalisees comme les GraInInaires Categorielles ou TAG (Joshi
1987), en derivant une phrase par combinaison de structures elementaires associees aux mots de
la phrase, construisent, par un effet de bord, des structures de dependances.

Le retour de la dependance au premier plan, au cours des annees 80, est dﬁ a deux facteurs
principaux : le retour en grace du lexique d'une part et de la semantique d'autre part. Pour le
lexique, grammaires de dependance, en mettant la lexie au centre de la structure syntaxique,
permettent d'exprimer simplement les relations lexicales comme la valence et le regime (ou sous-
categorisation). Pour la semantique, les structures de dependance, en permettant de dissocier
l'ordre des mots et la structure syntaxique proprement dite, se rapprochent davantage d'une
representation semantique que ne le fait une structure syntagmatique. Mieux encore, les relations
semantiques predicat-argument, parfois appelees dependances semantiques, bien que devant etre
distinguees des dependances syntaxiques, co'1'ncident en partie avec celles-ci (cf. Mel'cuk
1988b, Kahane & Mel'éuk 1999).

Enﬁn, les grammaires de dependance prouvent a l'heure actuelle leur bonne adequation au
traitement automatique des langues. Citons deux systemes d'envergure developpes en France :
le generateur de texte developpe a LexiQuest par Coch (1996) et integre au systeme MultiMeteo
(Coch 1998) de generation de bulletins meteo multilingues et l'analyseur en ﬂux de Vergne
(2000) qui a remporte l'action Grace, portant sur l'evaluation des etiqueteurs pour le francais.
Pour d'autres travaux, on po11rra egalement se reporter aux actes du dernier atelier sur le
traitement automatique par des graInInaires basees sur la dependance (Kahane & Polguere
1998), au numero special de la revue TAL sur les grammaires de dependance (Kahane 2000c) et
au portail ofﬁciel des graInInaires de dependance (http://ufal.mff.cuni.cz/dg.htInl).

Comme annonce dans le titre, cet expose est consacre aux grammaires de dependance en general
et a la theorie Sens-Texte en particulier. Dans la Section 2, nous tenterons de caracteriser les
notions de base de dependance syntaxique et de fonction syntaxique et nous presenterons les
premieres grammaires de dependance. La Section 3 sera consacree a la theorie Sens-Texte
[TST], probablement la plus achevee des theories linguistiques basees sur la dependance. Dans
la Section 4, nous ferons le lien entre la TST, dont les regles servent a mettre en correspondance
des structures, et les grammaires generatives, dont les regles servent a generer des structures, et
nous proposerons une grammaire de dependance basee sur les principes theoriques de la TST,
mais utilisant un formalisme d'unification. Dans la Section 5, nous nous pencherons sur les
techniques de base pour l'analyse avec une grammaire de dependance.

Je souhaite insister sur le fait que cet expose n'est pas un survol impartial du domaine des
graInInaires de dependance, loin de la. 11 s'agit clairement d'un point de vue personnel sur la
question, enrichi, je l'espere, de mes nombreuses discussions avec Igor Mel'éuk sur les
fondements de la theorie Sens-Texte et sur le role de la dependance en linguistique. J 'en proﬁte
pour le remercier chaleureusement pour ses nombreuses remarques sur la premiere version de ce
texte.

2 Arbres et grammaires de dépendance

Dans la Section 2.1, nous tenterons de caracteriser la notion de dependance syntaxique. A
travers divers exemples, nous exposerons les points qui font l’unaI1iInite entre les differentes
theories et ceux qui posent probleme (l'auxiliaire, la conjonction de subordination, le
determinant, le pronom relatif, la coordination, ...). Nous nous interesserons aux differentes
facons d'encoder la dependance et en particulier a l'equivalence entre arbres de dependance et
arbres syntagmatiques avec tétes. Dans la Section 2.2, nous nous interesserons a la notion de

Grammaires de dépendance formelles et théorie Sens- Texte

fonction syntaxique qui est indissociable de celle de dépendance syntaxique. Enfin, dans la
Section 2.3, nous présenterons les premieres grammaires de dépendance (Hays 1964, Gaifman
1965) et leur lien avec les grammaires catégorielles et les grammaires de réécriture hors-
contexte.

2.1 Caractérisation de la notion de dépendance syntaxique

La quasi-totalité des théories linguistiques s'accordent sur le fait que, au-dela de la question du
sens, les mots d'une phrase obéissent a un systeme d'organisation relativement rigide, qu'on
appellera la structure syntaxique de la phrase. Il existe deux grands paradigmes pour représenter
cette structure : soit décrire la facon dont les mots peuvent étre groupés en des paquets de plus
en plus gros (ce qui a donné les structures syntagmatiques), soit expliquer la facon dont les
mots, par leur présence, dépendent les uns des autres (ce qui a donné les structures de
dépendance). Come on le verra, les deux paradigmes s'opposent davantage sur la facon de
présenter l'organisation syntaxique que sur la nature meme de cette organisation.1

Considérer qu'un arbre de dépendance syntaxique peut rendre compte des propriétés
syntaxiques d'une phrase, c'est considérer que dans une phrase, la présence de chaque mot (sa
nature et sa position) est légitimée par la présence d'un autre mot (son gouvemeur syntaxique),
a l'exception d'un mot, le mot principal associé au sommet de l'arbre syntaxique. La
dépendance syntaxique est donc une dépendance entre mots (Figure 1 a gauche).

parle
O
gargon \ $1

O. O I
le petit Marie

 

Figure 1 : Arbre de dépendance et arbre a la Gladkij
pour Le petit garcon parle 61 Marie

Cette presentation de la structure syntaxique de la phrase est souvent mal acceptée de ceux qui
voient plutot des relations entre des mots et des groupes de mots que des relations entre des
mots seulement. Précisons ce point. Quand un mot x légitime la présence d'un mot y (c'est-a-
dire quand x gouverne y), en fait, par transitivité, x légitime également la présence des mots
légitimés par y et des mots légitimés par ceux-ci. En conséquence, x légitime non seulement la
présence de y, mais la présence d'un groupe de mot, qu'on appelle la projection de y. On peut
donc présenter la structure de dépendance non pas comme des dépendances entre mots, mais
comme des dépendances entre des mots et des groupes de mots (a l'intérieur desquels il y a a
nouveau des dépendances entre mots et groupes de mots. Cette structure de dépendance entre
des mots et des groupes peut étre représentée par une structure que nous appellerons un “mbre”
61 la Gladkij (Figure 1 a droite) (Gladkij 1968, Kahane 1997). A l'intérieur de chaque groupe

1 Je parle ici des representations elles—memes. Il existe bien sur des oppositions plus fondamentales qui ont

conduit les uns o1‘1 les autres a développer telle ou telle maniere de presenter les choses. En particulier, comme
je 1'ai déja dit, la grammaire syntagmatique est nee d'une Vision purement orienté Vers 1'ana1yse (a partir du
texte), le distributionnalisme, et d'un rejet presque absolu des differences lexicales (Gross 1975) et des
questions de sémantique.

Sylvain Kahane

ainsi considere, il y a un mot qui n'appartient a aucun sous-groupe et qu'on appelle la tétez. On
peut aussi representer l'arbre a la Gladkij par une structure syntagmatique avec téte lexicale,
c'est-a-dire une structure syntagmatique traditionnelle3 o1‘1 chaque constituant possede un sous-
constituant tete qui est un mot (voir Figure 2 ou la structure syntagmatique est representee, a
gauche, par un enchassement de groupe et, a droite, par un arbre non etiquete formellement
equivalent ; dans les deux cas, le sous-constituant tete est indique par l'etiquette T). Le fait de
considerer pour chaque constituant une tete n'est pas nouveau (cf. par ex. Pittman 1948). Ceci
est devenu monnaie courante depuis la Syntaxe X-barre (J ackendoff 1977).

 

Figure 2 : Arbres syntagmatiques avec tétes
pour Le petit gargon parle 61 Marie

Les structures syntagmatiques avec tete et les arbres de dependance (entre mots) sont
formellement equivalents (Gladkij 1966, Robinson 1970). On a vu comment on passe d'un
arbre de dependance a une structure syntagmatique avec téte en introduisant les groupes obtenus
par transitivation de la relation de dependance. Inversement, on passe d'un arbre syntagmatique
avec tete lexicale a un arbre de dependance en ne representant plus les groupes et en reliant le
gouverneur d'un groupe directement avec la téte de ce groupe. 4

Apres nous etre attache aux differentes facons de representer formellement la dependance, nous
allons aborder la question de la caracterisation theorique de la dependance. Tesniere lui-meme ne
caracterise pas clairement la dependance. Mel'éuk 1988a propose, a la suite de Garde 1977,
une tentative de caracterisation directement en terme de dependance er1tre mots. Du fait de
l'equivalence er1tre arbres de dependance et structure syntagmatique avec tete, il est egalement
possible de caracteriser la dependance en caracterisant le constituant, puis la tete d'un
constituant. Nous ne nous attarderons pas sur la facon d'identiﬁer les constituants, mais sur la
facon d'identifier la téte d'un constituant. Concemant les differentes deﬁnitions possibles de la
tete et les cas litigieux, citons tout particulierement le travail de Zwicky 1985. Nous adopterons
ici la deﬁnition suivante (Mel'éuk 1988a) :

2 Nous distinguons clairement les termes téte et gouverneur. Le gouverneur x d'un mot y (ou d'un goupe G) est

le mot qui legitimise la presence de y (ou de G). Il n'appartient pas a G. Par contre, la téte y du groupe G est
un mot de G qui legitimise, par transitivite, la presence des autres mots de G.

L'arbre de dependance n'encode pas l'ordre lineaire des mots. Bien que ce ne soit pas l'usage, on parle donc ici
d'un arbre syntagmatique non ordonne.

On peut egalement obtenir un arbre de dependance a partir d'une structure syntagmatique avec tete 01) on
autorise le sous-constituant tete a etre un groupe de mots. En d'autres termes, on autorise en fait un mot a etre
la tete lexicale de plusieurs constituants. Dans ce cas, lors du passage a un arbre de dependance, on ecrase les
djfferents constituants qui possedent la meme tete et la structure de dependance est donc structurellement plus
pauvre, bien qu'on puisse recuperer cette information (l'appartenance a une projection de la tete et pas a une
autre) autrement, par exemple dans l'etiquetage des dependances, en considerant differents types de relations
syntaxiques comme cela est l'usage en grammaire de dependance.

Grammaires de dependance formelles et theorie Sens- Texte

La tete syntaxiqne d'un constituant est l'element qui determine la valence passive de ce
constituant, c'est-a-dire l'element qui controle la distribution de ce constituant.

Nous allons illustrer cette deﬁnition par des exemples. Commence par la tete de la phrase. A
l'interieur de la proposition, tout le monde s'accorde a considerer que le verbe fini est la tete, car
c'est bien la presence du verbe ﬁni qui fait qu'il s'agit d'une proposition. Signalons neanmoins
deux difﬁcultes :

1) Lorsque le verbe est compose (comme dans Pierre a donné nn livre a Marie), on peut
s'interroger sur qui de l'auXiliaire ou du participe est la tete de la proposition. Certains
considerent que l'auxiliaire depend du participe. J e serais plutot enclin a preferer considerer,
a la suite de Tesniere ou de Mel'éuk, que l'auXiliaire est la tete. En effet, c'est l'auXiliaire qui
porte le mode (Il fandrait qne Pierre ai2Y*a donne’  ), qui herite de marques grammaticales
(Pierre pense avoir donne’  ; Pierre a-t-il donne’... ?), qui porte la negation (Pierre n’a
pas donne’...), qui peut rester seul (Pierre a-t-il donne’  ? Oni, il a.), 

2) Dans des langues comme le francais ou l'anglais ou la presence du sujet est obligatoire, des
linguistes ont ete amenes a considerer que la presence du sujet n'etait pas legitime par le
verbe, mais par un principe superieur. Dans la grammaire generative, on considere a l'heure
actuelle que le sujet, a la difference des complements n'est pas gouveme par le verbe en tant
que tel, mais par le morpheme grammatical exprimant le temps. Dans la mesure ou ce
morpheme appartient a la forme verbale et o1‘1 en grammaire de dependance on ne considere
que les dependances entre mots, les deux approches sont compatibles. Elles le sont encore
en considerant que lorsque le verbe est compose, le sujet depend de l'auxiliaire qui est aussi
le porteur de la ﬂexion temporelle.

Plus generalement, tout le monde s'accorde sur le sens de la relation de dependance lorsqu'il
existe une relation de subordination, c'est-a-dire lorsqu'il existe une relation actancielle (entre
une tete et son actant) ou une relation modiﬁcative (entre une tete et un modifienr) (meme si la
frontiere entre actant et modiﬁeur est parfois difﬁcile a saisir).5 Pose probleme la coordination et
les relations avec des elements jouant un role grammatical, notamment les complementeurs, les
determinants et les auxiliaires. Avant de parler de la coordination, nous allons aborder la
question des elements grammaticaux en evoquant la theorie de la translation de Tesniere (1959).

Si l'oeuvre de Tesniere est bien connue pour ce qui concerne la dependance, on a souvent oublie
sa theorie de la translation qu'il considerait probablement comme sa decouverte principale (bien
qu'on puisse estimer que l'idee est deja la dans la theorie des rangs de Jespersen 1924). Selon
Tesniere, il existe 4 parties du discours majeures (verbe, nom, adjectif, adverbe) avec des
relations prototypiques entre ces parties du discours : les actants du verbe sont des noms et ses
modiﬁeurs des adverbes, les dependants du nom sont des adjectifs et les dependants de
l'adjectif et de l'adverbe sont des adverbes. Neanmoins, un element de partie du discours X
peut venir occuper une position normalement reservee a un element de partie du discours Y,
mais dans ce cas, l'element doit etre translate de la partie du discours X a la partie du discours Y
par un element morphologique ou analytique appele un translatif de X en Y. Comme il y a 4
parties du discours majeures, il y aura 16 types de translatifs (y compris des translatifs de X en
X qui ne change pas la partie du discours). Par exemple un verbe peut etre l'actant d'un autre
verbe (c'est-a-dire occuper une position nominale), mais il devra etre a l'infinitif ou etre
accompagne de la conjonction de subordination qne (Pierre vent la parole ; Pierre vent parler
; Pierre vent que Marie parle). L'inﬁnitif et la conjonction de subordination qne sont donc des
translatifs de verbe en nom. De meme, les participes passe et present, qui permettent a un verbe
de modiﬁer un nom (le livre rouge ; le livre vole’ par Pierre ; la personne volant le livre), sont

5 La distinction entre actants et modiﬁeurs est consideree par Tesniére (1959), a qui l'on doit le terme d'actant (et

de circonstant pour les modiﬁeurs). Nous reviendrons dans la Section 3.2 sur cette distinction qui joue un
grand role dans la theorie Sens—Texte.

Sylvain Kahane

des translatifs de verbe en adjectif, la copule etant a son tour un translatif d'adjectif en verbe (le
livre est rouge ; le livre est vole par Pierre). Les prepositions quant a elles seront categorisees
comme des translatifs de nom en adjectif ou en adverbe (le livre rouge ; le livre de Pierre ;
Pierre boit maladroitement ; Pierre boit avec maladresse).

Les cas de translation suscitent generalement des discussions quant au choix de la tete : le
translatif, lorsqu'il est analytique doit-il etre traite comme le gouvemeur du translate ou comme
un dependant ? Si l'on s'en tient a notre deﬁnition de la tete, le translatif doit etre clairement
considere comme le gouvemeur, car c'est bien lui qui controle la valence passive, son role etant
justement de permettre au translate d'occuper des positions auxquelles il ne pourrait acceder
sans etre translate. Neanmoins, certains, comme Pollard & Sag (1994244), considere que la
conjonction de subordination que doit etre traitee comme un marqueur, le verbe restant la tete de
la completive, arguant du fait que la distribution de la completive depend egalement du mode qui
est porte par le verbe (Ilfaut que Pierre parte/*part ; Marie pense que Pierre part/*parte). En
fait, cela revient a traiter les deux elements, le translatif et le translate, plus ou moins comme des
co-tetes, puisque les traits tete et marqueur sont tous les deux des traits de tete (c'est-a-dire des
traits dont les valeurs montent sur la structure resultante de leur combinaison). Tesniere lui-
meme hesite a traiter le translatif comme le gouverneur du translate et prefere parler de nucleus
translatif : il represente alors le translatif et le translate comme un groupe (dessine
horizontalement) et dependant ensemble de leur gouverneur. En plus, du fait que le translate
controle aussi quelque peu la distribution du groupe translatif-translate (par exemple, certaines
positions n'acceptent que des verbes inﬁnitifs, c'est-a-dire des verbes translates en nom, mais
pas de noms : Pierre peut partir ; *Pierre peut le depart), Tesniere argue du fait que les
translatifs ont tendance a etre analytique au depart eta se morphologiser par la suite (c'est-a-dire
a devenir des morphemes ﬂexionnels sur le mot qu'il translate)“ et que le lien entre le translatif
et le translate est particulierement etroit.

La coordination est un autre cas qui pose probleme. Si l'on s'en tient a considerer que la
structure syntaxique doit etre un arbre de dependance et que tout groupe doit avoir une tete, le
meilleur candidat est sans conteste le premier conjoint (Mel'éuk 1988a). Certains proposent
egalement de prendre la conjonction de coordination comme tete du groupe coordonne, mais il
s'agit alors d'un choix davantage guide par la semantique, la conjonction de coordination
agissant comme operateur semantique prenant les conjoints comme arguments.7 Mais on peut
aussi considerer comme le font la plupart des grammaires syntagmatiques avec tete (J ackendoff
1977, Pollard & Sag 1994) que les conjoints sont des co-tetes. C'est egalement ce que propose
Tesniere, bien que sa solution reste tres informelle. Cf. Kahane 1997 pour voir comme la notion
d'arbre de dependance peut-étre etendue pour prendre en compte cette hypothese sans renoncer
au traitement par un arbre de dependance dans les autres cas. Parmi les autres cas qui posent
probleme citons le cas du determinant (Zwicky 1985 ; Abney 1987 011 il est defendu que le
groupe nominal ale determinant pour tete) et du pronom relatif (Tesniere 19592561, Kahane &
Mel'éuk 1999, Kahane 2000a).

Si come on l'a vu, le recours a un arbre de dependance peut dans certains cas ne pas etre
entierement satisfaisant, je voudrais insister sur le fait que meme dans ces cas-la, l'arbre de
dependance reste un moyen d'encodage sufﬁsant. Il est possible que des moyens d'encodage de

Tesniere distingue les translates des derives : le translate, meme lorsque la translation est morphologique,
continue a se comporter vis—a—vis de ses dependants comme un element de sa partie du discours initiale : par
exemple, le verbe a l'infinitif, c'est—a-dire le verbe translate en nom, continue a se comporter comme un verbe
vis-a-vis de ses dependants, a la difference du derive (voler un livre est reprehensible ; le vol de livre est
reprehensible).

De la meme fagon, un adjectif agit comme un predicat semantique qui prend le nom qu'il modifie comme
argument (le livre rouge : rouge(livre)) sans qu'on souhaite pour autant considerer l'adjectif comme le
gouvemeur syntaxique du nom.

Grammaires de dépendance formelles et théorie Sens- Texte

l'organisation syntaxique plus puissants permettent des analyses plus elegantes, mais l'arbre de
dependance conserve l'avantage de la simplicite. En plus, l'arbre de dependance n'a pas, a la
difference du role donne a l'arbre syntagmatique en grammaire generative, comme objectif
d'encoder toutes les informations pertinentes sur une phrase. Dans la plupart des grammaires de
dependance et en particulier dans la theorie Sens-Texte, l'arbre de dependance est avant tout une
representation intermediaire entre la representation semantique et la representation
morphologique (la o1‘1 les mots sont formes et ordonnes). L'arbre de dependance doit donc
contenir suffisamment d'information pour exprimer la relation avec la representation
semantique, notamment les possibilites de redistribution ou de pronominalisation. De l'autre
cote, il doit egalement contenir sufﬁsamment d'informations pour exprimer les relations avec la
representation morphologique, c'est-a-dire les differentes possibilites d'ordre, d'accord ou
d'assignation de cas. Ni plus, ni moins.

2.2 Fonctions syntaxiques

Un arbre de dependance ne sufﬁt pas a encoder l'organisation syntaxique des phrases sans un
etiquetage des dependances par des fonctions syntaxiques. Les fonctions syntaxiques permettent
de distinguer les differents dependants d'un meme mot, mais aussi de rapprocher deux
dependants de deux mots differents qui presentent des comportements similaires vis-a-vis de
differentes proprietes syntaxiques : placement, pronominalisation, regime, accord,
redistribution, cooccurrence,  La notion de fonction syntaxique a ete elaboree et utilisee
independamment des grammaires de dependance (cf. , par exemple, Jespersen 1924), meme si
la theorie de Tesniere a certainement marque une etape fondamentale dans la comprehension de
cette notion. Dans le courant generativiste, on a evite le recours explicite a un etiquetage
fonctionnel en tentant d'encoder les differences de comportement d'un dependant d'un mot par
des differences de position dans l'arbre syntagmatique (par exemple le sujet est le GN sous S ou
Inﬂ', alors que l'objet direct est le GN sous GV). Neanmoins, de nombreuses theories issues
de la grammaire syntagmatique (notamment LFG et HPSG) ont reintroduit explicitement la
notion de fonction syntaxirgue, notamment a la suite des travaux de Comrie & Keenan 1987 sur
la hierarchie fonctionnelle. (Cf. Abeille 1996-97 pour un survol des differents arguments pour
l'usage des fonctions syntaxiques en grammaire syntagmatique.)

L'une des principales difficultes pour decider combien de fonctions syntaxiques il est necessaire
de considerer est qu'on peut toujours attribuer une propriete particuliere a la categorie du
dependant ou du gouvemeur (comme le font les grammaires syntagmatiques) plutot qu'a
l'etiquette de la relation de dependance entre eux. Quitte a multiplier les categories syntaxiques,
il est formellement possible de limiter l'etiquetage des relations a un simple numerotage (il faut
quand meme garder un minimum pour distinguer entre eux les differents complements du
verbe). Il semble donc difficile d'etablir des criteres exacts pour decider si deux dependances
doivent ou non correspondre a la meme fonction et il est necessaire de prendre en compte
l'economie generale du systeme en cherchant a limiter a la fois le nombre de categories
syntaxiques et le nombre de fonctions syntaxiques et a chercher la plus grande simplicite dans
les regles grammaticales. On attribuera donc a la categorie syntaxique les proprietes intrinseques
d'une lexie (c'est-a-dire qui ne dependent pas de la position syntaxique) et a la fonction les
proprietes intrinseques d'une position syntaxique (c'est-a-dire qui ne dependent pas de la lexie
qui l'occupe). Autrement dit, on attribuera la meme categorie a des lexies qui presentent un
comportement similaire dans toutes les positions syntaxiques et la meme fonction a des
positions syntaxiques qui presentent des comportements similaires avec toutes les lexies.

Pour caracteriser l'ensemble des differentes fonctions syntaxiques, nous avons besoin de
criteres pour decider I) si deux dependants d'un meme mot (dans deux phrases differentes)

8 Toute grammaire syntagmatique qui fait usage des fonctions syntaxiques deﬁnit un arbre a la Gladkij (chaque

syntagme depend d'un mot) et devient de fait une grammaire de dependance.

Sylvain Kahane

remplissent la meme fonction et 2) si deux dependants de deux mots differents remplissent la
meme fonction.

Pour le premier cas considerons le paradigme suivant : Pierre lit le livre / Pierre le litl le livre
que Pierre lit. On admet generalement que les syntagmes un livre, le et que9 sont des
realisations du deuxieme argument semantique du predicat lire ; plus precisement, le et que sont
des formes pronominales de cet argument. De plus, ces syntagmes s'excluent mutuellement (*ce
que Pierre lit le livre ; *ce que Pierre le lit ; seul Pierre le lit le livre est possible, mais avec une
prosodie sur le livre tres differente de Pierre lit le livre, qui laisse a penser que le livre ne remplit
pas alors la meme fonction). Dans ce cas, on considere que ces elements remplissent tous la
meme fonction (a savoir la fonction d'0bjet direct).

Pourtant, les complements un livre, le et que ne se positionnent pas de la meme facon et les
pronoms, a la difference des groupes nominaux, distinguent les cas (il/le/lui ; qui/que). Peut-
etre, peut-on distinguer fonction et relation syntaxique et dire que le clitique le remplit la
fonction d'objet direct, mais depend de son gouvemeur par une relation speciﬁque (comme
objet-clitique) qui impose un placement particulier ainsi que l'assignation d'un cas.” Dans ce
cas, l'arbre de dependance sera etiquete par des relations syntaxiques et deux elements
remplissant des fonctions syntaxiques similaires pourront dependre de leur gouvemeur par des
relations syntaxiques differentes. Neanmoins, il ne semble pas necessaire de leur attribuer des
relations syntaxiques differentes, car le comme que appartiennent a des classes fermees de mots
outils pour lesquels on peut donner facilement des regles d'ordre specifique. Mais on peut
comprendre que certains preferent introduire des relations syntaxiques speciﬁques pour ces
elements plutet que de devoir invoquer des proprietes categorielles de l'element dependant dans
la regle de placement de l'objet direct.

Considerons un deuxieme paradigme : Pierre vent un bonbon / Pierre vent manger / Pierre
vent qu’on lui donne un bonbon. Encore une fois, ces differents complements realisent
tous le deuxieme argument semantique du verbe vouloir, s'excluent mutuellement et se
pronominalisent de la meme facon (Pierre le vent ; Que vent Pierre .7), ce qui nous inciterait a
leur attribuer la meme fonction syntaxique. Neanmoins, la construction avec verbe inﬁnitif (vent
manger) necessite des specifications supplementaires, a savoir qu'il s'agit d'une construction a
verbe contrele, ou equi-construction, ou le sujet du verbe vouloir co'1'ncide avec le “sujet” de
l'infinitif. Ceci peut sufﬁre a vouloir introduire une relation particuliere, bien qu'il existe
d'autres facons d'encoder cette propriete (par exemple, en considerant directement une relation
particuliere entre le verbe inﬁnitif et le sujet de vouloir).

Notons qu'il existe un autre critere souvent invoque pour decider si deux dependants d'un
meme mot remplissent la meme fonction : la coordination (Sag et al. 1985, Hudson 1988). On
peut decider par exemple que deux syntagmes peuvent etre coordonnes seulement s'ils
remplissent la meme fonction (condition a laquelle s'ajouteront d'autres conditions, notamment
sur l'identite categorielle).“ Dans notre dernier exemple, le fait que la coordination soit possible
(Pierre vent un bonbon et manger) nous incitera encore davantage a utiliser la meme
fonction.

On suppose ici que que est traite comme un dependant du Verbe, ce qui n'est pas necessairement justifie
(Kahane 2000b).

Pour assurer la montee du clitique dans, par exemple, Pierre le fait lire a Marie, on peut meme considerer que
le clitique depend defaire, alors qu'il remplit une fonction Vis—a—Vis de lire.

Par exemple, la coordination des adjectifs epithetes obeit a des conditions complexes et l'iteration de la
relation d'epithete est souvent preferable a la coordination : des plats frangais exquis, ?*des plats frangais et
exquis, des plats frangais et néanmoins exquis.

Grammaires de dependance formelles et theorie Sens- Texte

Considerons maintenant le deuxieme cas : comment decider si des dependants de deux mots
differents doivent recevoir la meme fonction. On considere que les dependants de deux mots
differents remplissent la meme fonction si et seulement si ils acceptent les memes
redistributions, les memes pronominalisations et les memes linearisations (Iordanskaja &
Mel'euk 2000).

Considerons un premier exemple : Pierre compte sur Marie / Pierre pose le livre sur la
table / le livre est sur la table. Les dependants si;r Marie et sur la table remplissent-ils la
meme fonction ? Ces dependants se distinguent nettement par leurs possibilites de
pronominalisation : seul le deuxieme accepte la cliticisation en y (*Pierre y compte ; Pierre y
pose le livre ; le livre y est) et les interrogatives et les relatives en oi‘; (*0i‘; Pierre compte-t-il ? ;
Oi‘; Pierre pose-t-il le livre ? ; Oi‘; le livre est-il .7). On distinguera donc deux fonctions
syntaxiques differentes, complement oblique pour compter et complement locatif pour poser et
etre (qui n'est pas le meme etre que la copule).

Deuxieme exemple : Pierre compte sur Marie /Pierre est aide par Marie. Les dependants si;r
Marie et par Marie remplissent-ils la meme fonction ? Aucune redistribution de ces dependants
n'est possible. On peut objecter que Pierre est aide par Marie est le resultat de la passivation de
Marie aide Pierre, mais la passivation est en quelque sorte orientee et Marie aide Pierre n'est pas
le resultat d'une redistribution de Pierre est aide par Marie. Les possibilites de pronominalisation
sont les memes : pas de cliticisation, meme pronominalisation pour les interrogatives et les
relatives. On pourrait objecter que si;r N accepte la pronominalisation en dessi;s, mais celle-ci
est tres reguliere et doit etre imputee a la preposition si;r (de meme qu'on aura dessoi;s pour
soi;s ou dedans pour dans) plutot qu'a la fonction de si;r N. Les possibilites de placement sont
egalement les memes. En consequence on peut attribuer a ces deux relations la meme etiquette,
par exemple complement oblique. Cela n'empeche pas de dire que par Marie dans Pierre est aide
par Marie est un complement d'agent ; cela ne signiﬁe pas que ce groupe remplit la fonction
syntaxique de complement d'agent qui n'a pas de raison d'eXister en tant que telle, mais
simplement que ce groupe est le resultat d'une realisation particuliere de l"‘agent” suite a une
redistribution.

Un demier exemple : Pierre mange un bonbon / Pierre vei;t un bonbon. Les deux
dependants i;n bonbon remplissent-ils la meme fonction ? Les deux dependants partagent les
memes proprietes a une exception pres, la passivation (le bonbon est mange par Pierre ; ?*le
bonbon est voi;li; par Pierre). Deux solutions sont alors possibles : 1) considerer qu'il s'agit de
la meme fonction dans les deux cas (objet direct) et faire assumer la difference a la categorie du
verbe qui gouveme cette position ou 2) considerer qu'il s'agit de deux fonctions differentes.
Etant donnee la grande similitude comportement, la premiere solution est plus econoIr1ique.

En conclusion, come on l'a vu, le choix d'un ensemble de fonctions syntaxiques est
directement lie a la facon dont seront ecrites les regles de pronominalisation, linearisation,
redistribution ou coordination.

2.3 Premieres grammaires de dépendance

Dans cette section, nous allons presenter les premieres grammaires de dependance (Hays 1960,
Gaifman 1965), qui ont pour particularite de ne traiter que des structures projectives.

Rappelons que l’un des points remarquables de la theorie de Tesniere est d’avoir dissocie la
representation syntaxique de l’ordre lineaire des mots : les arbres de dependance de Tesniere ne
sont pas ordonnes. L’objet de la syntaxe est alors d’exprimer le lien entre l’ordre des mots et
leurs relations de dependance.

L’une des principales proprietes de compatibilite entre un arbre de dependance et un ordre
lineaire est la projectivite (Lecerf 1961, Iordanskaja 1963, Gladkij 1966). Un arbre de
dependance assorti d'un ordre lineaire sur les noeuds est dit projectif si et seulement si, en

Sylvain Kahane

placant les noeuds sur une ligne droite et tous les arcs dans le méme demi-plan, on peut assurer
que 1) deux arcs ne se coupent jamais et que 2) aucun arc ne couvre la racine de l’arbre (Figure
4) .

(1) (2)

Figure 3 : Les cas de non projectivité

La projectivité est équivalente au fait que la projection de tout noeud x de l'arbre (c’est-a-dire
l'ensemble des noeuds dominés par x, x compris) forme un segment continu de la phrase
(Lecerf 1961, Gladkij 1966). Autrement dit, la projectivité dans le cadre des grammaires de
dépendance correspond a la continuité des constituants dans le cadre des grammaires
syntagmatiques. La littérature sur les structures de dépendance non projectives est d’ailleurs
toute aussi abondante que la littérature sur les constituants discontinus (toutes proportions
gardées). Nous y reviendrons a la fin de cette section.

La projectivité présente un intérét immédiat : il suffit, pour ordonner un arbre projectif, de
spéciﬁer la position de chaque noeud par rapport a son gouverneur, ainsi que vis-a-vis de ses
freres (Figure 4). Nous allons voir comment cette propriété est exploitée par les premieres

grammaires de dépendance.
O 

le petit garcon parle a Zoé
Figure 4 : Un exemple d’arbre de dépendance projectif

La premiere grammaire de dépendance formelle est due a Hays (1960). Une grammaire de Hays
est constituée d'un vocabulaire V, d'une ensemble de catégories lexicales C, d'un lexique
associant a chaque élément du vocabulaire une catégorie et d'un ensemble de regles de la forme
X(Y1Y2...Yk*Yk+1...Yn) 011 X et les Yi sont des catégories lexicales. La regle
X(Y1Y2...Yk*Yk+1...Yn) indique qu'un noeud de catégorie X peut posséder n dépendants de
catégories respectives Y1, Y2, ..., Yn placés dans l'ordre linéaire Y1Y2...Yk*Yk+1...Yn (o1‘1 *
indique la place de X par rapport a ses dépendants). Une regle de la forme X(*) indique qu'un
noeud de catégorie X peut étre une feuille de l'arbre de dépendance. Une telle grammaire permet
de générer des arbres de dépendance projectifs dont les noeuds sont étiquetés par un mot de V et
sa catégorie syntaxique dans C ou, ce qui revient au méme, a générer des suites de mots de V 011
chaque mot correspond a un noeud d'un arbre de dépendance étiqueté par une catégorie
syntaxique dans C. Come on le voit les grammaires de Hays n'ont pas recours aux fonctions
syntaxiques et elles génerent simultanément des arbres de dépendances et des suites de mots.
Comme l'a remarqué Gaifman (1965), les grammaires de Hays peuvent étre simulées par des
grammaires catégorielles a la Ajdukiewicz-Bar-Hillel (Ajdukiewicz 1935 ; Bar-Hillel 1953), la
regle X(Y1Y2. . .Yk*Yk+1. . .Yn) correspondant simplement a la catégorie complexe
Yk...Y1\X/Yn...Yk+1 (l'inversion dans l'ordre des catégories est due au fait que la catégorie la
plus a l'extérieur sera la premiere a étre réduite et donnera donc le dépendant le plus proche de
X). Si les grammaires catégorielles a la Ajdukiewicz-Bar-Hillel ne sont pas considérées comme

12 Suivant Hudson 2000, nous représenterons la racine de 1’arbre avec une dépendance gouvemeur Verticale
(potentiellement inﬁnie). La condition (2) se raméne alors a un cas particulier de la condition (1).

Grammaires de dépendance formelles et théorie Sens- Texte

les premieres grammaires de dependance, c'est que les auteurs n'ont jamais mis leur formalisme
en relation avec la construction d'arbres de dependance (ni d'arbres syntagmatiques d'ailleurs).
De plus, une categorie complexe comme la categorie N/N donnee a un adjectif antepose ne
s'interprete pas par “ un adjectif est un N dont depend un N a droite”, mais comme “un adjectif
est un mot qui combine a un N a sa droite donne un syntagme de meme nature” (voir neanmoins
Lecomte 1992 pour une interpretation des grammaires categorielles en termes de graphes et
Retore 1996 pour le lien entre grammaire logique et reseaux de preuve, eux-memes
interpretables en termes de graphes de dependance). Gaifman (1965) a egalement note que les
grammaires de Hays sont trivialement simulables par des grammaires de reecriture hors-
contextes ou la regle X(Y1Y2...Yk*Yk+1...Yn) correspond a un famille de regle de reecriture
X—>Y1Y2...YkaYk+1...Yn pour tout mot a de categorie X. Les grammaires de Hays, les
grammaires d'Ajdukiewicz-Bar-I-Iillel et leurs equivalents en grammaire de reecriture se
distinguent par la facon dont le vocabulaire pointe sur les regles syntaxiques.

L'article de Gaifman (1965) contient egalement deux resultats remarquables: l'equivalence
faible entre les grammaires hors-contexte et les grammaires de dependance de Hays” et un
theoreme d'equivalence forte entre une large classe de grammaires hors-contextes et les
grammaires de dependance de Hays (cf. egalement Dikovsky & Modina 2000).

D'un point de vue linguistique, les grammaires de Hays presentent plusieurs faiblesses : elles ne
separent pas les regles de bonne formation des arbres de dependance des regles de linearisation,
c'est-a-dire des regles de Inise en correspondance d'un arbre de dependance et d'un ordre
lineaire. De plus, concemant la bonne formation des arbres de dependance, elles ne distinguent
pas la sous-categorisation et la modiﬁcation. Ceci peut étre resolu tres simplement en divisant
une regle de la forme X(Y1Y2...Yk*Yk+1...Yn) en trois familles de regles : une regle indiquant
quels sont les categories des s de X, des regles indiquant quels sont les categories des
modiﬁeurs potentiels de X et une ou des regles indiquant comment les dependants de X se
placent les uns par rapport aux autres. On aura alors avantage a etiqueter les dependances par
des fonctions et a mentionner les fonctions plutot que les categories dans les regles de
linearisation. Nous verrons dans la suite comment ces differentes regles se presentent dans le
cadre de la theorie Sens-Texte.

Enﬁn, les grammaires de Hays ne prevoient pas le traitement de structures non proj ectives. Pour
traiter les cas non projectifs, differentes extensions sont possibles : on peut introduire des traits
Slash dans les categories comme cela est fait en GPSG et HPSG (Pollard & Sag 1994 ; cf.
Lombardo & Lesmo 2000 pour une adaptation du procede aux grammaires de dependance),
proposer des regles speciﬁques qui permettent de deplacer des elements dans l'arbre de
dependance pour se ramener a un arbre projectif (Hudson 2000, Kahane et al. 1998) ou utiliser
une structure plus complexe o1‘1 est veriﬁe un equivalent de la projectivite (Kahane 2000a).
D'autres methodes consistent a ne pas mettre en relation l'arbre de dependance directement en
relation avec l'ordre lineaire, mais a utiliser une structure syntagmatique intermediaire comme
cela est fait en LFG (Bresnan 1982, Bresnan et al. 1982 ; cf. Gerdes & Kahane 2001 ou
Duchier & Debusman 2001 pour des methodes equivalentes dans le cadre des grammaires de
dependance).

13 Plus precisement, Gaifman 1965 montre que toute grammaire hors contexte est simulable par une grammaire
dont les régles sont de la forme X—>aY1Y2, X—>aY1 et X—>a, ce qui est un theoréme bien connu sous le nom
de theoréme de mise en forme nonnale de Greibach, theoréme attribue a Greibach (1965) par qui ce resultat a
ete demontre independamment.

Sylvain Kahane

3 Présentation de la théorie Sens-Texte

La théorie Sens-Texte ITST] est née il y a 35 ans des premiers travaux en traduction
automatique en URSS (Zolkovskij & Mel'éuk 1965, 1967) et s’est depuis développée autour
d’Igor Mel'éuk (Mel'éuk 1974, 1988a, 1997). Cf. également, pour d'autres presentations,
Miliéevié 2001 ou Weiss 1999. La TST est intéressante a étudier dans le cadre d’une
présentation des grammaires de dépendance, non seulement parce qu’il s’agit d’une des théories
majeures utilisant des arbres de dépendance comme représentations syntaxiques, mais parce que
les postulats meme de la théorie conduisent naturellement a considérer une telle structure, ou le
mot joue un role central et ou la structure syntaxique doit rendre compte des relations entre les
mots. L’approche de la TST se distingue des grammaires syntagmatiques a plus d’un titre :

1) en privilégiant la sémantique sur la syntaxe ;
2) en privilégiant le sens de la synthese sur celui de l’analyse pour la description ;

3) en donnant une grande importance au lexique (avec notamment la considération de la notion
de fonction lexicale qui permet de décrire les relations lexicales dérivationnelles et
collocationnelles) ;

4) en préférant une représentation syntaxique basée sur un arbre de dépendance plutot qu’un
arbre syntagmatique (ce qui est, en quelque sorte, une conséquence naturelle des points
précédents).

Dans cette section, nous présenterons les postulats de base de la TST (Section 3.1), les
différentes représentations d'une phrase considérées par la TST (Section 3.2) et les différentes
regles d'un modele Sens-Texte (Section 3.3). Dans la Section 4, nous présenterons une
grammaire d'uniﬁcation basée sur la TST.

3.1 Les postulats de base de la théorie Sens-Texte
La théorie Sens-Texte [TST] repose sur les trois postulats suivants.

Postulat 1. Une langue est (considérée comme) une correspondance multivoque” entre des
sens et des textes”.

Postulat 2. Une correspondance Sens-Texte est décrite par un systeme formel simulant
l’activité linguistique d’un sujet parlant.

Postulat 3. La correspondance Sens-Texte est modulaire et présente au moins deux niveaux
de représentation intermédiaires: le niveau syntaxique (structure des phrases) et le niveau
morphologique (structure des mots).

Commentaires sur les postulats.

1) Le premier postulat de la TST signiﬁe que la description d'une langue naturelle L consiste en
la description de la correspondance entre l'ensemble des sens de L et l'ensemble des textes de L.
On peut comparer ce point de vue a celui de Chomsky 1957, dont l'inﬂuence a été primordiale :
la description d'une langue L consiste en un systeme formel dérivant l'ensemble des phrases
(acceptables) de L. Pendant longtemps, ce point de vue a eu une interpretation plutot restrictive,

14 Plusieurs sens peuvent correspondre au meme texte (homonymie) et plusieurs textes peuvent correspondre au

meme sens (synonymie).

Texte renvoie a n'importe quel segment de parole, de n'importe quelle longueur, et son pourrait etre un
meilleur tenne.

Grammaires de dépendance formelles et théorie Sens- Texte

une phrase etant comprise comme une suite de caracteres” — c'est-a-dire un texte dans la
terminologie de la TST — ou au Inieux comme une structure syntagmatique. Neanmoins, le
postulat de Chomsky est formellement equivalent au premier postulat de la TST des qu'on
entend par phrase un signe au sens saussurien avec un signiﬁe (le texte) et un signiﬁant (le
sens). D'un point de vue mathematique, il est en effet equivalent de deﬁnir une correspondance
entre l'ensemble des sens et l'ensemble des textes ou de deﬁnir l'ensemble des couples formes
d'un sens et d'un texte en correspondance, un tel couple representant une phrase” (Kahane
2000b, 2001).

2) Le deuxieme postulat met l'accent sur le fait qu'une langue naturelle doit etre decrite comme
une correspondance. Un locuteur parle. Un modele Sens-Texte (= le modele d'une langue
donnee dans le cadre de la TST) doit modeliser l'activite d'un locuteur, c'est-a-dire modeliser
comment un locuteur transforme ce qu'il veut dire (un sens) en ce qu'il dit (un texte). C'est
l'une des principales particularites de la TST de dire qu'une langue doit étre decrite comme une
correspondance (Sens-Texte) et, qui plus est, que la direction du sens au texte doit etre
privilegiee sur la direction du texte au sens.

3) Le troisieme postulat de la TST appelle plusieurs commentaires. La plupart des theories
linguistiques considerent des niveaux de representation syntaxique et morphologique. La
particularite de la TST est de considerer que ces niveaux sont des niveaux intennédiaires entre le
niveau semantique (le sens) et le niveau phonologique (le texte). En consequence, la
correspondance entre les sens et les textes sera entierement modulaire : une correspondance
entre les niveaux semantique et syntaxique, une correspondance entre les niveaux syntaxique et
morphologique et une correspondance entre les niveaux morphologique et phonologique. (En
fait, la TST considere non pas deux, mais cinq niveaux intermediaires, ce qui ne change rien a
notre discussion.)

Le resultat est que le module syntaxique, qui assure la correspondance entre les niveaux
syntaxique et morphologique, ne fait qu'associer des representations syntaxiques avec des
representations morphologiques. Il n'a pas pour objet, comme cela l'est pour une grammaire
generative, de donner une caracterisation complete des representations qu'il manipule. Dans le
sens de la synthese, le module syntaxique prend en entree des representations syntaxiques qui
ont ete synthetisees par le module semantique a partir de representations semantiques bien
formees et qui representent des sens reels. En consequence, une representation syntaxique est
caracterisee par l'ensemble des modules, par le fait qu'elle est un intermediaire possible entre
une representation semantique bien formee et une representation phonologique correspondante.
En conclusion, la TST ne donne aucune primaute a la syntaxe et la TST n'a pas pour objectif de
donner une caracterisation explicite des representations syntaxiques bien formees.

Je pense que, maintenant, 35 ans apres leur premiere formulation, les postulats de la TST,
meme s'ils peuvent apparaitre avec des formulations differentes, sont plus ou moins acceptes
par l'ensemble de la communaute scientiﬁque. Par exemple, j'aimerais citer les toutes premieres
phrase d'une monographie consacree au Programme Minimaliste, la plus recente des theories
chomskienne (Brody 1997) : “It is a truism that grammar relates sound and meaning. Theories
that account for this relationship with reasonable success postulate representational levels
corresponding to sound and meaning and assume that the relationship is mediated through
complex representations that are composed of smaller units.” Le principal point qui semble ne

Le meilleur exemple de cette interpretation restrictive du postulat de Chomsky est la deﬁnition du terme
langage formel comme une suite de caracteres. Un langage formel, pris dans ce sens, ne peut jamais modéliser
l'essence d'une langue naturelle. En aucun cas, le fait de connaitre l'ensemble des suites de caracteres
acceptables d'une langue ne peut etre considere comme la connaissance d'une langue ; il faut evidemment etre
capable d'associer ces suites a leur sens.

Nous laissons de cote le fait que la description d'un langage ne se reduit pas a la description de phrases isolees.

Sylvain Kahane

pas etre pris en consideration par la plupart des descriptions formelles contemporaines des
langues naturelles est le fait qu'une langue, si elle represente une correspondance entre des sens
et des textes, doit etre decrite par des regles de correspondance.

3.2 Niveaux de représentation

La TST separe clairement les differents niveaux de representation. Les representations des
differents niveaux ont des organisations structurelles differentes : les representations
semantiques sont des graphes (de relations predicat-argument), les representations syntaxiques
sont des arbres de dependance (non ordonnes) et les representations morphologiques sont des
suites. Dans l'approche Sens-Texte, tout ce qui peut etre differencie doit etre differencie. Et des
objets avec des organisations differentes doivent etre representes avec des moyens differents.
De plus, la TST donne une grande importance a la geometrie des representations. Le fait que les
humains communiquent par la voix entraine que les productions linguistiques sont
irremediablement lineaires (meme si a la suite des phonemes se superpose la prosodie et si des
gestes peuvent accompagner la parole). Par contre, tout laisse a penser que, dans notre cerveau
tridimensionnel, le sens possede une structure multidimensionnelle. Le passage du sens au texte
comprendrait alors, du point de vue de l'organisation structurelle, deux etapes essentielles : la
hierarchisation, c'est-a-dire le passage d'un sens multidimensionnel a une structure syntaxique
hierarchique (= bidimensionnelle), et la linearisation, c'est-a-dire le passage de cette structure
hierarchique a une structure lineaire (= unidimensionnelle).

3.2.1 Représentation sémantique

Le sens est defini, dans le cadre de la TST, comme un invariant de paraphrase, c’est-a-dire
comme ce qui est commun a toutes les phrases qui ont le meme sens. Ceci fait automatiquement
de la TST est un modele de la paraphrase (Mel'éuk 1988b) et, par consequent, un outil adapte a
la traduction automatique (les deux sont intimement liees, la paraphrase etant de la traduction
intralangue).

Le coeur de la repre’sentati0n“’ sémantique est un graphe dont les noeuds sont etiquetes par des
sémamrémes. Une representation semantique est un objet purement lin istique speciﬁque a une
langue. Un sémantéme lexical d'une langue L est le sens d'une lexie 9 de L dont le signiﬁant

18 Le terme de représentation, utilise par Mel'éuk lui-meme, est en fait un peu contradictoire avec le point de Vue
de la TST. En un sens, la representation sémantique ne represente pas le sens d'un texte, mais c'est plutot les
textes qui expriment des representations semantiques. Mel'éuk (2001115) dit d'ailleurs :21 Ce propos : “During
the process of sentence construction (= synthesis), lexical and syntactic choices carried out by the Speaker
Very often lead to the modification of the starting meaning, i.e. of the initial semantic representation, making
it more precise and speciﬁc: the lexical units bring with them additional nuances of meaning that have not
been present in the initial semantic representation. The MTT tries to model this phenomenon; as a result,
quite often the following situation obtains: Suppose that the synthesis starts with the representation ‘o" and

produces sentences ‘S1’, ‘S2’, ..., ‘Sn’; the sentences having as their common source the semantic
representation ‘o" are considered to be synonymous. Now if we analyze these sentences semantically, the
semantic ‘S1’, ‘S2’, ..., ‘Sn’ obtained from this process may well be different from each other and from the

initial semantic representation ‘o" ! [...] The initial semantic representation is taken to be rather
approxi1nate—it need not necessarily fully specify the meaning of the sentences that can be obtained from it.
The meaning can become more precise—or less precise—in the course of its lexicalization and
syntacticization.”

Un Vocable est ensemble de lexies correspondant aux différentes acceptions d'un meme mot. En toute rigueur,
le nom d'une lexie doit etre accompagne, comme dans le dictionnaire, d'un numero qui la distingue des autres
lexies du Vocable.

Grammaires de dépendance formelles et théorie Sens- Texte

peut-etre un mot ou une conﬁguration de mots formant une locution. Par exemple, ‘cheval’,
‘pomme de terre’, ‘prendre le taureau par les comes’ sont des sémantemes du francais. Des
lexies de parties du discours différentes peuvent avoir le meme sémanteme ; ainsi, ‘partir’ =
‘depart’ (‘j'attends ton depart’ = ‘j'attends que tu partes’) ou ‘durer’ = ‘pendant’ (‘Ta sieste a
duré 2 heures’ = ‘Tu as fais la sieste pendant 2 heures’)2°. Il existe aussi des sémantemes
grammaticawc correspondant au sens des morphemes ﬂexionnels (ou de configurations
contenant des morphemes ﬂexionnels, comme le passé compose) : par exemple, ‘singulier’,
‘de’fini’, ‘present’ ou ‘passe’ compose’ sont des sémantemes grammaticaux.

Un sémanteme agit comme un prédicat et est lié a ses arguments par des arcs pointant sur eux.
Les différents arcs émergeant d'un sémanteme sont numérotés de 1 a n, en suivant l'ordre
d'oblicité croissant des arguments. Un arc représente une relation prédicat-argument et est
appelée une dépendance sémantique. Les dépendances sémantiques doivent etre distinguées des
dépendances syntaxiques. Comme l'a note Tesniere lui-meme (1959242), dans la plupart des
cas, quand un mot B dépend syntaxiquement d'un mot A, il y a une dépendance sémantique
entre ‘A’ et ‘B’. Mais ce que n'avait pas vu Tesniere (et qui est probablement une découverte
attribuable a Zolkovskij & Mel'euk 1965), c'est que la dépendance sémantique peut etre orientée
de ‘A’ et ‘B’ comme de ‘B’ vers ‘A’. Par exemple, dans une petite riviere, petite dépend
syntaxiquement de riviere, mais, parce que la petitesse est une propriété de la riviere, ‘riviere’
est un argument du prédicat ‘petit’. Par contre, dans la riviere coule, riviere dépend
syntaxiquement de coule et, parce que l'écoulement est une propriété de la riviere, ‘riviere’ est
un argument du prédicat ‘couler’. Quand les dépendances sémantique et syntaxique sont dans la
meme direction, on dit que B est un actant de A (riviere est un actant de coule dans la riviere
coule), tandis que, quand les dépendances sémantique et syntaxique sont dans la direction
opposée, on dit que B est un modifieur de A (petite est un modiﬁeur de riviere dans une petite
riviere). Il existe aussi des cas o1‘1 dépendances sémantique et syntaxique ne se correspondent
pas, comme dans les phénomenes de montée (dans Pierre semble malade, Pierre dépend
syntaxiquement de semble, mais ‘sembler’ est un prédicat unaire qui prend seulement ‘malade’
comme argument) ou de tough-movement (dans un livre facile a lire, facile dépend
syntaxiquement de livre, mais ‘livre’ est un argument de ‘lire’ et pas de ‘facile’) ; voir également
le cas des relatives et des interrogatives indirectes (Kahane & Mel'euk 1999).

La valence sémantique d'un sémanteme, c'est-a-dire l'ensemble de ses arguments sémantiques,
est déterminée par sa deﬁnition lexicographique. Ainsi ‘b1essureI.2’ est une prédicat temaire
(Mel'euk et al. 1999 ; deﬁnition révisée) : ‘b1essureI.2 de X a Y par Z’ = ‘lesion a la partie Y du
corps de X qui est causée par Z et qui peut causer une ouverture de la peau de Y, un saignement
de Y, une douleur de X a Y ou la mort de X’ (sax blessure par ballez a la jambeY).

La representation sémantique comprend, en plus du graphe sémantique, trois autres structures
qui s'y superposent : la structure communicative, la structure référentielle (qui relie des portions
du graphe aux referents qu'elles dénotent) et la structure rhétorique (qui indiquent les intentions
stylistiques du locuteur, c'est-a-dire si celui-ci veut etre neutre, ironique, relaché, humoristique,
...). La Figure 5 présente une representation sémantique simpliﬁée (limitée au graphe
sémantique et a la thématicité) pour la phrase (1) :

(1) Zoe’ essaye de parler a la belle dame

2° Les deux phrases peuvent apparaitre non synonymes en raison de la structure communicative (voir plus loin) :

par exemple, si la premiere peut facilement avoir pour theme ‘la durée de ta sieste’ (= ‘ta sieste a dure’), cela

parait plus difficile pour la deuxieme qui aura plutot pour theme ‘toi’ ou ‘ta sieste’ (= ‘tu as fait la sieste’).
21 Comme pour les lexies, les differentes acceptions d'un morpheme ﬂexionnel devraient etre distinguées par des
numéraux. A noter que Mel'cuk ne considere pas de sémantemes grammaticaux et utilisent des paraphrases
lexicales : ‘plus d'un’, ‘avant maintenant’, 

Sylvain Kahane

La meme representation semantique vaut pour des paraphrases de (1) come Zoe cherche £1 dire
un mot 61 la jolie femme.

 
      
   

  

‘essa er’ ,
O\4_1_Q ‘present’

2 S 9
_\O parler

Figure 5 : La representation semantique de (1)

La structure communicative speciﬁe la facon dont le locuteur veut presenter l'information qu'il
communique (de quoi il parle, ce qu'il veut dire, ce qu'il veut souligner, ce qu'il presente
comme information commune avec son interlocuteur, ...). La structure communicative est
encodee en marquant certaines zones du graphe semantique par des marques communicatives.
Dans chacune de ces zones, on indique (par un soulignement) le semanteme qui resume le
contenu semantique de cette zone (Polguere 1990). Mel'éuk 2001 propose huit categories
communicatives : thematicite (theme-rheme-speciﬁeurs), donne-nouveau, focalisation,
perspective (arriere-plan), emphatisation, presupposition, unitarite (unitaire-articule) et
locutionalite (signale-performe-communique). Nous allons montrer comment des changements
dans la thematisation et la focalisation du graphe semantique de la Figure 5 donnent d'autres
phrases. Tout message doit necessairement communiquer quelque chose (le rheme) a propos de
quelque chose (le theme) ou eventuellement de rien. La phrase (1) peut etre glosee par ‘a propos
de Zoe (theme), je veux dire qu'elle essaye de parler a une belle dame (rheme)’. La partition
theme-rheme s'identifie en voyant quelle est la question sous-jacente au message communique
(ici ‘que fait Zoe ?’). Un element focalise’ quant a lui est une partie du sens que le locuteur
presente comme etant localement proeminente pour lui — ou, en d'autres termes, comme etant le
siege (angl. focus) de son attention (Mel'éuk 2001). Voici quelques phrases ayant le meme
graphe semantique que (1) avec des structures communicatives differentes (et pour lesquelles les
memes choix lexicaux ont ete faits) :

(3) a. La belle dame, Zoe essaye de lui parler. (‘belle dame’ theme focalise)
b. C ’est 61 la belle dame que Zoe essaye de parler. (‘belle dame’ rheme focalise)
c. Zoe, c’est 61 la belle dame qu'elle essaye de parler. (focalisation de ‘Zoe’ en plus)
d. Ce que Zoe essaye defaire, c’est de parler 61 la belle dame. (‘Zoe essaye’ theme foc.)

On trouvera de nombreux exemples dans Mel'éuk 2001. Le role de la structure communicative
dans la production des relatives est etudie dans Kahane & Mel'éuk 1999. Notons encore que la
structure communicative est souvent consideree, a la difference de la structure predicat-
argurnent, comme tres vague et difficile a cerner precisement. Nous pensons que cette vision est
completement fausse et due en partie au fait que les linguistes etudient generalement cette
question du point de vue de l'analyse, en cherchant a determiner la structure communicative de
textes. Evidemment, si l'on considere un enonce isole tel que Marie a dit que Pierre est parti, il
est impossible de determiner sa partition theme-rheme. Cette phrase peut “repondre”, avec certes
des prosodies differentes, a des questions aussi diverses que Que fait Marie ?, Qu’a dit Marie ?,
Qu’a dit Marie de Pierre ?, Que fait Pierre ?, Qui a dit que Pierre est parti ?, Qui est parti ?, 
(par exemple, lorsqu'elle repond a la question Que fait Pierre ?, ‘Pierre’ est le theme, ‘partir’ est
le rheme, ‘Marie m'a dit’ un speciﬁeur qui speciﬁe sous quelles conditions je peux dire que
Pierre est parti). Par contre si on se place du point de vue de la synthese, il est evident que le
locuteur sait de quoi il veut parler et ce qu'il veut dire a ce propos. La partition theme-rheme est
donc parfaitement etablie (notons d'ailleurs qu'elle s'etablit au niveau semantique). Dﬁ au

Grammaires de dépendance formelles et théorie Sens- Texte

pouvoir paraphrastique de la langue, il reste au locuteur de nombreuses possibilites
d'enonciation conditionnees aussi par les autres choix communicatifs, notamment la
focalisation. Des enonces tels que Marie a dit que Pierre est parti, peu conditionnes par la
structure communicative, seront possibles avec de nombreux choix communicatifs, mais
d'autres choix syntaxiques ne seront possibles qu'avec des choix communicatifs precis :
D’apres Marie, Pierre est parti (‘Marie a dit’speciﬁeur), C ’est Marie qui a dit que Pierre est parii
(‘Marie’ rheme focalise), etc.

Une representation semantique peut etre encodee dans un style inspire de la logique. La
traduction d'un graphe semantique en une formule logique necessite d'introduire une variable
pour chaque noeud du graphe (a l'eXception des noeuds etiquetes par un semanteme
grammatical). Cette variable represente le noeud et est utilisee comme argument par tout
semanteme pointant sur le noeud. En introduisant des variables x, y, p, e et e’ pour les
semantemes lexicaux ‘Zoe’, ‘dame’, ‘beau’, ‘essayer’ et ‘parler’, on peut encoder le graphe de
la Figure 5 par la formule (2).

(2) THEME(x) RHEME(e)
x : ‘Zoe’ e :‘essayer’(x,e’)
e’ : ‘parler’(x,y)
y : ‘dame,’
p : beau (y)
‘present’ (e)
‘singulier’(y)
‘de’fini’(y)

La structure theme-rheme est encodee par la partition des semantemes en deux groupes et par les
“predicats” THEME et RHEME pointant sur les noeuds dominants de ces deux zones. Si l'on
omet la structure theme-rheme, l'ordre des predicats n'est pas pertinent et la formule s'apparente
a une formule conjonctive du calcul des predicats (cf. par exemple les representations
semantiques de la DRT ; Kamp 1981, Kamp & Reyle 1993). La variable representant un noeud
peut d'ailleurs etre attribuee au semanteme (on parle de reiﬁcation) : ainsi a la place des notations
y : ‘dame’ ou e : ‘essayer’(x,e’), on peut utiliser les notations ‘dame’(y) ou ‘essayer’(e,x,e’),
plus habituelles en logique.

Malgre leur similitude formelle, les representations semantiques de la TST doivent etre
distinguees des representations semantiques des semantiques issues de la logique fregeenne,
comme la DRT. En TST, la representation semantique ne represente pas l'etat du monde que
denote un sens, mais le sens lui-meme. En particulier, les variables que nous avons introduites
lors de la reiﬁcation ne renvoient pas, comme c'est le cas dans la logique fregeenne, a des objets
du monde. Les variables renvoient ici uniquement aux semantemes, c'est-a-dire aux signiﬁes
des mots. Donnons un exemple : dans le sens de une grossefourmi, le semanteme ‘gros’ est un
predicat unaire dont l'argument est le semanteme ‘fourmi’ et en aucun cas le referent de fourmi.
D'ailleurs, quand on parle d'une grosse fourmi, on ne veut pas dire que le referent de fourmi est
gros en soi (d'ailleurs rien n'est gros en soi), mais qu'il est gros en tant que fourmi. La chose
est peut-etre encore plus evidente quand on parle d'un grosﬁrmeur. Ici non plus, on ne veut pas
dire que le referent deﬁrmeur est gros, mais que quelque chose dans le sens ‘fumeur’ est gros.
En effet, si un ‘fumeur’ est une ‘personne qui fuIne (regulierement)’, un ‘gros fumeur’ est une
‘personne qui fuIne (regulierement) en grosse quantite’. D'autre part, le semanteme ‘gros’
po11rra lui-meme etre l'argument d'un autre semanteme comme dans une trés grosse fourmi ou
une fourmi plus grosse que mon pouce, ce qui necessite d'introduire une variable pour ‘gros’
lors de la reification, sans qu'on veuille pour autant considerer que gros possede un referent de
discours.

En TST, le sens est deﬁni comme ce qui est commun a tous les enonces qui ont le meme sens.
La deﬁnition n'est pas circulaire, ‘avoir le meme sens’ etant deﬁni prealablement au ‘sens’ : il
est plus facile de demander a un locuteur si deux enonces ont le meme sens (sont synonymes)

Sylvain Kahane

que de lui demander quel est le sens d'un enonce (ce qui d'ailleurs le conduira essentiellement a
proposer des enonces qui ont le meme sens). La TST est donc un modele de la paraphrase. Le
sens est un objet purement linguistique. La description du monde est releguee a un niveau de
representation plus profond, extralinguistique. Remarquons tout de meme que la reference au
monde exterieur n'est pas exclue de la representation semantique de la TST et fait l'objet d'une
structure particuliere, la structure re’fe’rentielle, superposee au graphe semantique et indiquant
quelle zone du graphe correspond a un referent de discours.

Notons enfin, pour terminer sur les differences entre les representations semantiques de la TST
et les formules logiques, que tous les semantemes sont formalises par des predicats (les noms
semantiques comme ‘Zoe’ ou ‘dame’ etant des cas particuliers de predicats a zero argument),
meme des sens comme ‘quel que soit’, ‘quelqu'un’, ‘et’ ou ‘non’, qui sont habituellement
formalises en logique par des objets d'une autre nature, quantiﬁeurs ou connecteurs.”

3.2.2 Représentation syntaxique profonde

Le niveau syntaxique profond est un niveau intermediaire entre le niveau semantique et le niveau
syntaxique de surface, ou le graphe a ete hierarchise et les semantemes lexicalises, mais o1‘1 ne
ﬁgure pas encore a proprement parle les mots. Le coeur de la representation syntaxique profonde
est un arbre de dependance (non ordonne) dont les noeuds sont etiquetes par des lexies
profondes accompagnees chacune d'une liste de grammemes profonds. Les lexies profondes
sont des lexies pleines correspondant a des mots ou a des locutions. Les lexies vides, comme
les prepositions regies, n'apparaissent qu'au niveau syntaxique de surface. De meme, les
grammemes profonds sont des morphemes grammaticaux pleins ; les grammemes vides dus a
l'accord ou a la rection, comme le cas, apparaissent plus tard. Les categories grammaticales
profondes du verbe sont le mode, le temps et la voix. Les lexies sont ecrites en majuscules et les
grammemes places en indice : LEXIEgramméme. Les branches de l'arbre sont etiquetees avec un
petit ensemble de relations syntaxiques profondes : les actants sont simplement numerotes par
oblicite croissante (I, II, III, ...), les modifieurs relies a leur gouverneur par la relation A'I'IR
(angl. attributive) et deux autres relations sont considerees, COORD pour les groupes
coordonnes (Marie, Jean et Pierre : MARIE —CO0RD—> JEAN —CO0RD—> ET —II—> PIERRE)
et APPEND pour les parenthetiques, les interjections, les interpellations, etc. (Naturellement,
il n’a rienfait ; 012 vas-ta, Zoe’ .9). Kahane & Mel'éuk 1999 introduisent une autre relation pour
les modiﬁeurs qualitatifs (non restrictifs) et Kahane 1998 propose l'introduction d'une relation
speciﬁque pour un actant retrograde (tel que le complement d'agent).

Nous proposons Figure 6 la representation syntaxique profonde de/ (1) ; Le trait hachure
represente une relation de coreference entre les deux occurrences de ZOE resultant de la coupure
du graphe semantique au niveau du semanteme ‘Zoe’. L'une des deux occurrences sera effacee
en surface par la regle de pronominalisation de l'actant I d'un verbe a l'infinitif. Le grammeme
inﬁnitif sera introduit par le regime de ESSAYER au niveau syntaxique de surface. La structure

22 La djfferenciation formelle des quantiﬁeurs est certainement necessaire pour la deduction logique, mais ne l'est
pas forcement pour la paraphrase et la traduction. La portee des quantifieurs n'est pas clairement encodee dans
les representations sémantiques standard. En un sens, il n'est pas sﬁr que la portee des quantiﬁcateurs doive
reellement etre encodee dans la representation semantique de Tous les hommes cherchent un chat et il est
curieux de Voir ﬂeurir des travaux qui montrent comment sous—specifier les representations sémantiques dans
des fonnalismes qui obligent a indiquer la portee des quantifieurs. Mel'éuk 2001 emet 1'hypothese que les
effets de portee des quantificateurs resultent de la structure communicative. Polguere 1992 propose d'encoder
les quantiﬁeurs comme des semantemes biactanciels dont le deuxieme argument, representant la portee, pointe
sur une zone du graphe, ce qui pourrait etre relie a 1'hypothese precedente. Dymetman & Copennan 1996
proposent une solution a l'encodage de la portee des quantiﬁeurs avec une representation intermédiaire entre
graphe semantique et formule logique.

Grammaires de dépendance formelles et théorie Sens- Texte

communicative syntaxique profonde qui reprend la structure communicative sémantique n'est
pas représentée ici.

ESSAYERind présent actif
/Ox ’ ’

i 0/1 H/\O\PARLER
ZOE . I H
‘K ‘('3 DAME
ZOE ATTR
J:
BEAU

Figure 6 : Representation syntaxique profonde de (1)

sg,déf

1\{otons encore 1'une des spéciﬁcités de 1'approche Sens-Texte : 1e concept de fonction lexicale
(Zo1kovskij & Me1'éuk 1965, Me1'éuk et al. 1995, Wanner 1996, Kahane & Polguere 2001).
Certains sens, comme 1'intensiﬁcation, 1e commencement, la causation, la réalisation, etc.,
tendent a s'exprimer de maniere collocationnelle, c'est-a-dire que leur expression n'est pas
déterminée librement, mais dépend fortement de 1'expression d'un de leurs arguments
sémantiques. Les fonctions lexicales sont des “lexies” dont le signifiant n'est pas un mot précis,
mais varie en fonction de 1'expression d'un argument. Par exemple, le sens ‘intense’ po11rra
s'exprimer avec amoureux par follement, avec heureux par comme un pape, avec improbable
par hautement, avec blesse’ par gravement, etc. De meme, 1e sens ‘commencer’ pour s'exprimer
avec incendie par se de’clarer, avec jour ou vent par se lever, avec orage par e’clater, etc. Les
fonctions lexicales correspondantes seront notées Magn et Incep. Ces fonctions seront
utilisées pour étiqueter un noeud de 1'arbre syntaxique profond correspondant a un sens
‘intense’ ou ‘commencer’. Les valeurs seront introduites seulement dans 1'arbre syntaxique de
surface (Me1'cuk 1988, Polguere 1998).

3.2.3 Représentation syntaxique de surface

Le coeur de la repre’sentation syntaxique de surface d’une phrase est un arbre de dépendance
(non ordonné) a la facon des arbres de dépendance de Tesniere 1959. Les noeuds de 1'arbre sont
étiquetés par des lexies de surface accompagnées chacune d’une liste de grammemes de surface.
Chaque lexie de surface correspond a un mot de la phrase. Ces lexies peuvent correspondre
directement a une lexie profonde, ou bien correspondre a 1'un des mots d'une locution, ou étre
la valeur d'une fonction lexicale, ou bien étre une lexie vide introduite par un régime (comme les
prépositions DE et A ici), ou encore étre une partie de 1'expression d'un grammeme profond
(comme un auxiliaire de temps ou 1'artic1e LE ici). Les grammemes de surface correspondent
directement a un grammeme profond, sauf pour les grammemes profonds qui ont une
expression analytique comme les temps composés, les voix ou la détermination. Les
grammemes de surface d’accord ou de régime (comme les cas) ne sont introduits qu'au niveau
morphologique profond. Les branches de 1'arbre syntaxique de surface sont étiquetées par des
fonctions syntaxiques ou relations syntaxiques de surface (cf. Me1'éuk 1974 pour le russe,
Me1'éuk & Pertsov 1987 pour 1'ang1ais et Iordanskaja & Me1'éuk 2000 pour le francais; voir
aussi Section 2.2).

Sylvain Kahane

ESSAYERind présent
/O\ ’
suj inf

<5 ‘C? DE

ZOE prép

¢ PARLERM
iobj
<,'> A
prép
/3>\DAME sg
dét mod
LE 0’ ‘O BEAU

Figure 7 : Représentation syntaxique de surface de (1)

3.2.4 Représentation morphologique profonde

Le coeur de la représentation morphologique profonde d'une phrase est la suite des
représentations morphologiques des mots de la phrase, c'est-a-dire une chafne morphologique.
La représentation morphologique d'un mot est une lexie de surface accompagnée d'une liste de
grammemes de surface. A noter que, en francais, l'adjectif s'accorde en genre et nombre avec le
nom et le verbe en personne et nombre avec son sujet. Le nom possede une marque de genre qui
est indiquée dans son entrée lexicale, mais ne porte pas de grammeme de genre comme les
adjectifs, car il n'est pas ﬂéchi par le genre.”

La chaine morphologique de (1) est :
(2) z0E’ ESSAYERmd,p,ésem,3,sg DE PARLER,-nf/l LEfe’m,sg BEA Ufémg DAMEsg

La représentation morphologique d'une phrase comprend, en plus de la chaine morphologique,
une structure prosodique. La structure prosodique au niveau morphologique est essentiellement
un regroupement des mots en groupes prosodiques, agrémenté de marques prosodiques
calculées en fonction des marques communicatives des portions de l'arbre syntaxique de surface
auxquelles correspondent ces groupes. La véritable structure prosodique sera calculée au niveau
phonologique a partir de cette structure prosodique de niveau morphologique et des propriétés
phonologiques des mots (qui ne sont pas encore prises en compte au niveau morphologique, les
phonemes n'étant pas considérés). Dans Gerdes & Kahane 2001 (inspirés par des discussions
avec Igor Mel'éuk), nous proposons de construire au niveau morphologique une structure
syntagmatique qui, contrairement a l'usage qu'en font les grammaires basées sur la syntaxe X-
barre, n'encode pas la représentation syntaxique de la phrase, mais plutot sa structure
prosodique de niveau morphologique (cf. aussi Section 6).

3.3 Modéle Sens-Texte standard

On appelle modéle Sens-Texte [MST] d'une langue le modele de cette langue dans le cadre de la
TST. Le terme modéle est préféré au terme plus couru de grammaire (formelle), car le terme
grammaire masque le fait qu'une composante essentielle d'un modele d'une langue, a coté de la
grammaire proprement dite est le lexique.

23 Il n'est probablement pas judicieux de considérer que les noms possédent un trait de personne. On peut penser
que seuls les pronoms possédent un tel trait et que le Verbe prend la 3éme personne par défaut, comme il le
fait aussi avec les sujets Verbaux ou phrastiques (que tu viermes est une borme surprise).

Grammaires de dépendance formelles et théorie Sens- Texte

Dans la Section 3.3.1, nous presenterons le lexique d'un MST, puis, dans la Section 3.3.2,
l'architecture generale d'un module de correspondance. Enfin, dans les Sections 3.3.2 a 0, nous
presenterons les modules semantique, syntaxique profond et syntaxique de surface d'un MST.

3.3.1 Le lexique d'un modele Sens-Texte

La TST donne une tres grande importance au lexique. Le lexique d'un MST est appele un
DictionnaireExplicanf et Combinatoire [DEC] . Un premier DEC pour le russe a ete propose par
Mel'éuk & Zolkovsky 1984. Un DEC du francais est maintenant en developpement depuis 20
ans a l’universite de Montreal (Mel'éuk et al. 1984, 1988, 1992, 1999). Les entrees du DEC
sont les lexies profondes.24 En plus de la description des valences semantique et syntaxique, qui
est indissociable des approches basees sur la dependance, le DEC se caracterise par le grand
soin donne a la definition semantique des lexies (basee sur la paraphrase) et par l’utilisation des
fonctions lexicales dans la description des liens derivationnels et collocationnels entre lexies
(Mel'éuk et al. 1995, Kahane & Polguere 2001).

Nous allons presenter et commenter l'entree (revisee et simpliﬁee) de la lexie BLESSUREI.2
(Mel'éuk et al. 1999). Chaque article est divise en trois zones :

- la zone sémantique donne la deﬁnition lexicographique de la lexie ;

- la zone syntaxique donne le tableau de regime (ou cadre de sous-categorisation de la lexie,
c'est-a-dire la correspondance entre les actants semantiques (X, Y, ...), les actants
syntaxiques profonds (I, II, ...) et les leur expression de surface (61 N, par N, ...); le
tableau de regime est suivi par des conditions particuliere (l'eXpression 1 de la colonne 3
n'est possible que si N est une arme blanche) et des exemples de realisations et de
combinaisons des differents actants ;

- la zone de cooccurrence lexicale donne les valeurs des fonctions lexicales pour la lexie,
c'est-a-dire les collocations formees avec cette lexie (une blessure cuisante, se faire une
blessure, la blessure s’infecte, ...) et les derivations semantiques de la lexie (blessé, plaie,
se blesser, ...).

Exemple : article de dictionnaire de BLESSUREI.2

Définition lexicographique
‘blessureI.2 de X a Y par Z’ = ‘lesion a la partie Y du corps de X qui est causee par Z et qui

peut causer (I) une ouverture de la peau de Y, (II) un saignement de Y, (III) une douleur de X a
Y ou (IV) la mort de X’25

24 La description separee des lexies de surface pourrait etre egalement utile. Pour l'instant, cel1es—ci sont decrites
grossierement1orsqu'el1e apparaissent dans la description des lexies profondes, comme element d'une locution,
comme Valeur d'une fonction lexicale ou comme element regi introduit dans le tableau de regime.

La deﬁnition lexicographique est basee sur la paraphrase (la deﬁnition de L doit est substituable a L) et la
cooccurrence lexicale : les composantes (I) :21 (IV) sont conditionnees par les differentes valeurs des Fact—Rea1
de BLESSUREI.2. Cette portion de la definition indique les “objectifs” inherents de L (une blessure peut étre
profonde (I), saigner (II), faire souﬂrir (III) on étre fatale (IV)). Voir, pour comparaison, les valeurs du trait
telic dans les descriptions lexicales du Lexique Generatif de Pustejovsky 1995.

Sylvain Kahane

Régime
X = 1 Y = 2 Z = 3

1. de N 1. 61 N 1. 61 N

2' Apos 2. par N
Contrainte sur 3.1 : N désigne une arme blanche
Contrainte sur 3.2 : N = balle, 
Exemples
1 : la blessure de Jean/du soldat/du cheval ; sa blessure
2 : une blessure £1 l’e’paule/au caeur/£1 l’abd0men ; des blessures au corps
3 : une blessure £1 l’arme blanche/au couteau ; une blessure par balle
1 + 2 : les blessures de l’enfant aux bras ; sa blessure au poignet droit

1 + 2 +3 : sa blessure par balle £1 lajambe

Fonctions lexicales

Sync
Syn:

Synn
personne-S1
A1/2

A1,2+Magn
Magn
AntiMagn
AntiBon
IncepMinusBon
Operl

FinOper1
Caus1Oper1
LiquOper1
FinFunc0
essayer de LiquFunc0
CausFunc1
Caus1Func1
Reall

AntiReal1

Facto

Factl

Able1Fact1 ("=' Magn)

: lésion
: coupure, écorchure; égratignure; morsure; brﬁlure; ecchymose;

déchirure; fracture; entorse

: plaie; bobo “fam”

: blessé

: // blessé

: couvert, criblé [de ~s]

: grave, majeure, sérieuse

: légere, Inineure, superﬁcielle // égratignure

: mauvaise, vilaine

: s'aggraver; s'enﬂaInmer, s'envenimer, s'infecter
: avoir [ART ~]; porter [ART ~]; souffrir [de ART ~]
: se remettre, se rétablir [de ART ~]

: se faire [ART ~]

: guérir [N de ART ~]

: se cicatriser, (se) guérir, se refermer

: soigner, traiter [ART ~]; bander, panser [ART ~]

: faire [ART ~ a N]; inﬂiger [ART ~ a N] // blesser [N] [avec N=Z]
: se faire [ART ~]; se blesser [avec N=Z]

: (II) souffrir [de ART ~];

(IV) succomber [a ART ~], mourir [de ART ~]

: (IV) réchapper [de ART ~]

: (I) s'ouvrir, se rouvrir; (II) saigner

: (IV) emporter, tuer [N]

: (I) ouverte < profonde < béante (III) cuisante, douloureuse;

(IV) fatale, mortelle, qui ne pardonne pas

AntiAble1Fact1 ("=' AntiMagn) : bénigne “spec”, sans conséquence

Nous ne pouvons expliquer ici les sens des différentes fonctions lexicales (cf. Mel'éuk et al.
1995). Chaque fonction lexicale simple (Magn, Operl, ...) correspond a une regle sémantique
particuliere (voir Section 3.3.3) et les fonctions lexicales complexes (IncepOper1, Able1Fact1,
...) correspondent a des opérations naturelles sur les fonctions lexicales simples (Kahane &
Polguere 2001). Les fonctions lexicales jouent un grand role dans les choix lexicaux (Mel'éuk
1988a, Polguere 1998), ainsi que dans la paraphrase et la traduction (Mel'éuk 1988b).

Grammaires de dépendance formelles et théorie Sens- Texte

3.3.2 Les modules de correspondance d’un modéle Sens-Texte

La grammaire d’un modele Sens-Texte est divisée en modules. Chaque module assure la
correspondance entre deux niveaux adjacents : le module sémantique assure la correspondance
entre le niveau sémantique et le niveau syntaxique profond, le module syntaxique profond la
correspondance entre le niveau syntaxique profond et le niveau syntaxique de surface, le module
syntaxique de surface la correspondance entre le niveau syntaxique de surface et le niveau
morphologique profond, etc.

Les regles de grammaire d’un modele Sens-Texte sont toutes des regles de correspondance entre
deux niveaux adjacents, c’est-a-dire des regles qui associent un fragment d’une structure d’un
niveau donné avec un fragment d’une structure d’un niveau adjacent. Les regles se présentent
toutes sous la forme A <=> B I C ou A et B sont des fragments de structure de deux niveaux
adjacents et C est un ensemble de conditions. La regle doit étre lue “si les conditions C sont
vérifiées, A peut étre traduit par B” dans le sens de la synthese et “si les conditions C sont
vérifiées, B peut étre traduit par A” dans le sens de l'analyse. En fait, ce n'est pas l'ensemble
des conﬁgurations A et B qui sont traduites l'une dans l'autre : les conﬁgurations contiennent
aussi des éléments qui indiquent comment la regle va s'articuler avec d'autres regles, comment
la configuration produite par la regle va s'attacher aux configurations produites par les autres
regles (voir Section 3.3.3). Suivant Kahane & Mel'éuk 1999, nous séparons les regles en
regles nodales et sagittales : les regles nodales sont les regles ou la portion de A manipulé par la
regle est un noeud, tandis que les regles sagittales (lat. sagitta) sont les regles ou la portion de A
manipulée par la regle est une ﬂeche (une dépendance sémantique, syntaxique ou, pour le
niveau morphologique, une relation d'ordre).

Nous allons maintenant présenter les trois premiers modules d’un MST.

3.3.3 Le module sémantique d’un modéle Sens-Texte

Le module sémantique realise la correspondance entre le niveau sémantique et le niveau
syntaxique profond. Le module sémantique assure deux opérations fondamentales : la
lexicalisation et la hiérarchisation ou arborisation du graphe sémantique.

La hiérarchisation est assurée par les regles sagittales. Parmi les regles sagittales sémantiques,
on distingue les regles positives et négatives. Une regle positive transforme une dépendance
sémantique en une dépendance syntaxique de meme direction, tandis qu'une regle negative
inverse la direction. L'arborisation consiste a choisir une entrée dans le graphe qui donnera la
racine de l'arbre, puis a parcourir le graphe a partir de ce noeud d'entrée. Les dépendances
sémantiques parcourues positivement (du prédicat vers l'argument) seront traduites par des
regles positives, tandis que les dépendances parcourues négativement seront traduites par des
regles négatives. Le choix du noeud d'entrée, ainsi que celui des noeuds ou seront coupés les
cycles du graphe, est guidé par la structure communicative. Nous ne développerons pas ce point
ici (cf. Polguere 1990, Kahane & Mel'éuk 1999). Notons simplement que le noeud d'entrée est
par défaut le noeud dominant du rheme lorsque celui-ci peut étre lexicalisé par un verbe (ou une
toumure équivalente de type verbe support-nom prédicatif ou verbe copule-adjectif) et qu'il
prend le noeud dominant du theme comme argument sémantique.

Une regle sagittale sémantique positive traduit une dépendance sémantique en une dépendance
syntaxique profonde actancielle, tandis qu'une regle négative traduit une dépendance sémantique
en une dépendance syntaxique profonde ATTR, COORD ou APPEND (Figure 8). Chaque
dépendance sémantique est attachée a deux noeuds sémantiques ‘X’ et ‘Y’ dont les
correspondants syntaxiques profonds sont X et Y ; ces étiquettes permettent de s'articuler la
regle décrite ici avec les regles nodales qui traduisent ‘X’ en X et ‘Y’ en Y. La grosse ﬂeche que
nous indiquons dans la partie gauche de la regle indique le sens de parcours et doit étre
compatible avec la structure communicative (cf. Mel'éuk & Kahane 1999).

Sylvain Kahane

4X3 X 4X3 X
‘P <|3 X est un N 9 Cl?
ll 4:} I ouXestunV ‘ 1 4:; 11
g CI) pas au passif 8 J)
4Y3 Y ‘Y, Y
PierreY partx; le départx de PierreY le livre voléx par PierreY

I (X est un N

1 ' et Y est un Adj) ou

t 5 <=> ATJDTR (X n'est pas un N
et Y est un Adv)

un grosX tasY ; tresx grosY ; partirY vitex

Figure 8 : Trois regles sémantiques sagittales

Toutes les regles que nous présentons dans la Figure 8 sont locales, c'est-a-dire que la
dépendance syntaxique profonde qui traduit la dépendance sémantique considérée doit étre
attachée a la traduction noeuds X et Y des noeuds ‘X’ et ‘Y’ auxquels est attaché la dépendance
sémantique. I1 existe pourtant des disparités entre les structures sémantiques et syntaxiques
profondes nécessitant des regles non locales (cf. Kahane & Mel'éuk 1999 pour des regles non
locales pour le traitement des phrases a extraction).

Les regles sémantiques nodales associent un sémanteme a une lexicalisation de ce sémanteme.
La plupart des sémantemes sont lexicalisés par une lexie profonde. Certains sémantemes comme
‘intense’ vont étre lexicalisé par une fonction lexicale (ici Magn), dont la valeur sera recherchée
par le module syntaxique profond dans l'entrée lexicale de l'argument concemé. Enfin, des
regles sémantiques particulieres assurent la réalisation des sémantemes grammaticaux par des
grammemes de surface.

Terminons notre présentation du module sémantique en montrant comment on passe de la
représentation sémantique de (1) (Figure 5) a sa représentation syntaxique profonde (Figure 6).
On commence par choisir le noeud d'entrée de la représentation sémantique. Le sémanteme
‘essayer’ est choisi car il est le noeud dominant du rheme, qu'il peut étre lexicalisé par un verbe
et qu'il prend le noeud dominant du theme comme argument. Ce noeud est lexicalisé par
ESSAYER. Ensuite, on parcourt le graphe a partir de ce noeud. Le cycle formé par ‘essayer’,
‘parler’ et ‘Zoé’ sera coupé au niveau de ‘Zoé’ aﬁn d'assurer la connexité du rheme. Le sens de
parcours de toutes les dépendances sémantique est maintenant décidé. Les dépendances
sémantiques parcourues positivement donneront des dépendances syntaxiques profondes
actancielles. Seule la dépendance entre ‘beau’ et ‘dame’, parcourue négativement, donnera une
dépendance ATTR. Comme ‘dame’ sera lexicalisé par le nom DAME, ‘beau’ devra étre
lexicalisé par un adjectif. Nous ne présentons pas les regles grammaticales.

3.3.4 Le module syntaxique profond d’un modéle Sens-Texte

Le module syntaxique profond réalise la correspondance entre le niveau syntaxique profond et le
niveau syntaxique de surface. Le module syntaxique profond doit assurer l'introduction de
toutes les lexies de surface de la phrase (lesquelles correspondent un a un aux mots de la phrase,
a l'exception de cas de réduction comme de le en du).

Grammaires de dépendance formelles et théorie Sens- Texte

Les regles syntaxiques profondes sagittales traduisent une dépendance syntaxique profonde en
fonction de la nature des éléments qu'elle relient et de leurs tableaux de régime. En particulier,
ces regles introduisent les prépositions régies (Figure 9).

Les regles syntaxiques profondes nodales traduisent une lexie profonde. La plupart de ces
regles sont controlée par le lexique, comme l'expansion d'une locution, ou l'introduction de la
valeur d'une fonction lexicale. Les regles syntaxiques profondes nodales comprennent
également les regles de pronominalisation : dans une chaine de référence (c'est-a-dire une chaine
de lexies profonde qui correspondent a un méme noeud sémantique), il faut remplacer sous des
conditions précises certaines lexies par des pronoms. Ces regles n'ont pas fait l'objet d'une
étude sérieuse pour l'instant.

Le module syntaxique profond contient également des regles grammaticales qui assurent la
traduction des grammemes profonds, notamment ceux qui comme la détermination, la voix ou le
temps peuvent s'exprimer par des expressions analytiques comprenant des mots. La encore ces
regles n'ont pas fait l'objet d'une étude sérieuse en TST. Nous en proposerons Section 4.2.1
dans le formalisme GUST.

X X X X

‘,3 ‘,3 XestunV (,3 (,3 XestunNet

I 4:} suj ﬁni I 4:; dér Y est un

(I (I I I pronom

Y Y Y Y

PierreY partx sonY départx
X X X X
0 O O O X est un N et
I I X I I _
I : cnom Y   g et ATTR : mod Y est 1111 Ad]
(I +DE J; J»
Y prép Y Y
cl) le grosY tasX
Y
le départx de PierreY X Y
o o
' ' X est un N et
ATER <:> Cngm Y est un N
IDE quantitatif
Y prép
X
un verreY de vinX ; trois metresYde tissux

Figure 9 : Cinq regles syntaxiques profondes sagittales

Montrons comment on passe de la représentation syntaxique de (1) (Figure 6) a sa
représentation syntaxique de surface (Figure 7). Come le verbe ESSAYER est ﬁni, l'actant I
de ESSAYER devient sujet. La regle sagittale qui traduit l'actant II de ESSAYER doit, en
fonction du tableau de régime de ESSAYER, introduire une relation syntaxique inﬁnitive, la
préposition DE et le grammeme infinitif sur PARLER. L'actant I de PARLER est effacé par la
regle de “pronominalisation” de l'actant I d'un verbe a l'infinitif. L'actant II de PARLER est

Sylvain Kahane

traduit, en fonction du tableau de regime de PARLER, par la relation d'obj et indirect (iobj) et la
preposition A. Comme BEAU est un adjectif, la relation A'I'I‘R donne la relation syntaxique
modzfieur. Enﬁn, comme DAME n'a pas de determinant, le grammeme défini sur DAME
donne le determinant LE, relie a DAME par une relation détenninative.

3.3.5 Le module syntaxique de surface d’un modele Sens-Texte

Le module syntaxique de surface realise la correspondance entre le niveau syntaxique de surface
et le niveau morphologique profond. Le module syntaxique de surface assure la linearisation,
l'accord et le regime (Figure 10) (cf. Mel'éuk & Pertsov 1987 pour un fragment consequent du
module syntaxique de l'anglais).

Les regles de linearisation indiquent comment un element se place par rapport a son gouverneur
(X < Y ou Y < X). Mais, elles doivent aussi indiquer comment les differents dependants d’un
meme noeud se placent les uns par rapport aux autres. Plusieurs techniques sont possibles : on
peut par exemple indiquer dans la regle de linearisation d’un dependant quels sont les autres
dependants qui peuvent se placer entre lui et son gouverneur (Mel'éuk & Pertsov 1987, Nasr
1996). Nous preferons encoder le placement des co-dependants par une marque de position
indiquant la “distance” d’un dependant donne au gouverneur (Mel'éuk 1967, Courtin &
Genthial 1998, Kahane 2000a). Comme metaphore, on peut voir les dependances comme des
elastiques auxquels sont accroches les mots avec un poids egal a la valeur du trait position : plus
le poids est grand (en valeur absolu), plus le mot est loin de son gouverneur. On peut egalement
voir les marques de position comme des adresses de positions precises ouvertes par le
gouverneur. Par exemple, un verbe fini ouvre 7 positions devant lui pour les clitiques (il < ne <
me < le < lui < en < y). Nous donnons Figure 10 les regles de linearisation du sujet : un sujet
non pronominal peut se placer devant le verbe a la position -10 ou apres le verbe a la position
+10 sous certaines conditions, tandis qu'un sujet pronominal se cliticise et occupe la position -7
devant le verbe. De meme, un objet direct pronominal se cliticise et occupe la position -5 ou -4
selon sa personne.

X X
0 o
' . Y ' t - . Y est un
sw <=> Y < X unnpizngfg su; <=> Y < X pronom
6 -10 0 <5 -7 0
Y Y
PierreY partx jeY parsx

Y n'est pas un pronom et X
est le verbe principal d'une
<=> X < Y proposition o1‘1 un element a
0 +10 ete extraitet o1‘1iln'y a pas
d'obj et direct non pronominal

*-<0-3-o ><

le livre que litx PierreY

Figure 10 : Trois regles syntaxiques de surface sagittales

Notons que le placement des co-dependants depend egalement de la taille du syntagme domine
par le dependant (les gros syntagmes ont tendance a etre plus eloignes) et de la structure
communicative (les syntagmes les plus saillants communicativement ont tendance a etre plus
eloignes); la marque de position devrait donc etre une fonction dependant de la relation

syntaxique, de la taille du syntagme et de la saillance communicative.

Grammaires de dépendance formelles et théorie Sens- Texte

Pour le traitement des constructions non projectives, des regles non locales sont necessaires,
puisque l'element ne se place plus par rapport a son gouverneur, mais par rapport a un ancétre
plus eloigne.“

Montrons comment on passe de la representation syntaxique de surface de (1) (Figure 7) a sa
representation morphologique profonde (2). La racine ESSAYER de l'arbre syntaxique est
placee en premier. Le sujet ZOE est place a sa gauche et la preposition DE, qui est la tete de son
complement inﬁnitif, a sa droite, conformement aux regles de linearisation des relations sujet et
ipﬁnitive. Le dependant PARLER de la preposition DE est place a sa droite, puis la preposition
A, qui est la téte de l'0bjet indirect de PARLER, a sa droite, puis le nom DAME qui depend de
A a sa droite. L'article LE et l'adjectif BEAU seront places a gauche de DAME. En raison de la
projectivite, ils devront se placer entre DAME et son gouvemeur A. Enﬁn conformement aux
marques de position des regles de placement du détenninant et du modzfieur, l'article LE sera
place a gauche de l'adjectif BEAU.

Nous terminons ici notre presentation de la TST standard. On trouvera une description des
regles morphologiques dans Mel'éuk 1993-2001.

4 Une grammaire Sens-Texte basée sur l'unification

Afm de proposer une version completement formalisee de la TST et d’etablir le lien entre
l’approche Sens-Texte et d’autres approches, nous allons montrer comment les regles de
correspondance d'un modele Sens-Texte peuvent etre interpretees comme des regles generatives
basee sur l’uniﬁcation, c'est-a-dire comment un modele Sens-Texte standard peut étre simule
par une grammaire qui genere des portions de structures et les combine par uniﬁcation. Le
formalisme que nous presentons sera appele GUST (Grammaire d'Unification Sens-Texte).
Nous ferons le lien entre GUST et d’autres formalismes bien connus comme HPSG et TAG,
dont il s'inspire d'ailleurs largement.

4.1 Grammaires transductives et grammaires génératives

Les regles de la TST sont des regles qui mettent en correspondance deux fragments de deux
structures de deux niveaux de representation adjacents (par exemple un fragment de structure
syntaxique de surface avec un fragment de chaine morphologique profonde, c'est-a-dire un
fragment d'arbre de dependance avec un fragment d'ordre lineaire). Etant donnes deux
ensembles S et S’ de structures (graphes, arbres, suites, ...), nous appellerons grammaire
transductive entre S et S’ une grammaire G qui met en correspondance des elements de S et de
S’ par un ensemble fini de régles de correspondance qui mettent en correspondance un fragment
d'une structure de S avec un fragment d'une structure de S’ (Kahane 2000b). Tous les modules
de la TST sont des grammaires transductives. Un modele Sens-Texte, le modele d'une langue
donnee, est encore une grammaire transductive obtenue par composition des differents modules
du modele.”

26 La regle d'inVersion du sujet que nous proposons Figure 10 devrait etre en fait une regle non locale : le sujet
inverse ne se place pas par rapport a son gouvemeur, mais rapport au nucleus Verbal qui contr6le l'extraction.
Cf. Kahane 2000a pour une fonnalisation.

27 La composee de deux grammaires transductives ne donnent pas trivialement une grammaire transductive. En

effet, si G est une grammaire transductive e11tre S et S’ et G’ une grammaire transductive e11tre S’ et S”, on

peut construire une grammaire transductive GoG' e11tre S et S”, mais cette grammaire n'est pas obtenue en
composant simplement les regles de G avec les regles de G‘. La difficulte Vient du fait que les fragments ch
structure de S’ consideres par G ne sont pas forcement les memes que ceux consideres par G‘. Ainsi le module
syntaxique profond de la TST considere comme fragments des portions importantes d'arbre syntaxique ch

Sylvain Kahane

Remarquons qu'une grammaire transductive G entre S et S’ déﬁnit davantage qu'une
correspondance entre S et S’. En effet, pour chaque couple (s,s') de structures appartenant a S
et S’ et Inises en correspondance par G (c'est-a-dire par des regles de correspondance qui vont
associer des fragments de s avec des fragments de s’), G déﬁnit aussi des partitions de s et s'
(les fragments considérés par les regles) et une fonction (p(s,s.) entre ces partitions. Nous
appellerons cela une supercorrespondance (Kahane 2000b). Par exemple, le module syntaxique
de surface ne fait pas que mettre en correspondance des arbres de dépendance et des chaines
morphologiques : pour chaque arbre et chaine en correspondance, il met en correspondance les
noeuds de l'arbre avec les éléments de la chaine (par l'intermédiaire des regles nodales) (Figure
1 1).

La supercorrespondance entre S et S’ déﬁnie par une grammaire transductive est
mathématiquement équivalente a l'ensemble des triplets (s,s', (pm) o1‘1 s et s' sont des éléments
de S et S’ mis en correspondance et (p(s,s.) est la fonction associant les partitions de s et s' déﬁnies
par la mise en correspondance de s et s'. Un triplet de la forme (s,s', (p(s,s.)) est en fait une
structure produit au sens mathématique du terme, c'est a dire une structure complexe obtenue
par l'enchevétrement de deux structures (l'enchevétrement est du au fait que, en un sens, les
deux structures sont déﬁnies sur le meme ensemble — l'ensemble des fragments mis en
correspondance). Pour prendre l'exemple du module syntaxique de surface d'un MST, si s est
un arbre de dépendance, s' une suite et (pm) une correspondance entre les noeuds de s et les
éléments de s', le triplet (s,s', (p(s,s.)) n'est autre qu'un arbre ordonné (Figure 11).

essaye
/Ox
suj inf
Z066 ‘('3 (16 I
prép prep
Epparler S j 1- prép i bj dét
iobj mod
C‘) Q E o O o o o 0 o
pr'ép Zoé essaye de parler a la belle dame
<5 dame
/ \
dét mod
<5 ‘o
la bemx
O O O O O O O
Zoé essaye de parler a la belle dame

Figure 11 : Equivalence entre un arbre et une suite en correspondance et un arbre ordonné

Une grammaire transductive entre S et S’ peut étre simulée par une grammaire générative qui
génere l'ensemble des triplets (s,s', (p(s,s.)) décrit par G. Les regles de correspondance sont alors
vues comme des regles générant des fragments de structure produit. Nous allons développer
cette idée dans la suite.

Inversement, les grammaires génératives qui génerent des structures produits peuvent vues
comme des grammaires transductives. Par exemple, les grammaires de Gaifman-Hays, qui
génerent des arbres de dépendance ordonnés, peuvent étre vue comme des grammaires mettant
en correspondance des arbres de dépendance non ordonnés avec des suites, c'est-a-dire comme
une implementation d'un module syntaxique de surface de la TST. Notons quand meme que les

surface qui correspondent dans la structure syntaxique profonde a un seul noeud (par exemple pour la regle
d'expansion d'une locution), alors que le module syntaxique de surface n'en considere pratiquement pas (et pas
les memes).

Grammaires de dépendance formelles et théorie Sens- Texte

grammaires de Gaifman-Hays cherchent aussi a assurer la bonne formation des arbres de
dépendance, alors que, dans le cadre de la TST, celle-ci résulte de l'interaction des différents
modules.

4.2 Grammaire d'Unification Sens-Texte

Nous allons maintenant montrer comment un modele Sens-Texte peut étre simulé par une
grammaire générative basée sur l'unification, que nous appelons GUST (Grammaire
d ’Umficati0n Sens-Texte).28 Le formalisme de GUST s'inspire de la grammaire de Nasr (1995,
1996; Kahane 2000a), elle-méme inspirée des grammaires TAG lexicalisées (Schabes 1990,
Abeillé 1991, XTAG 1995, Candito 1999)..”

Nous ne considérons que 3 des 7 niveaux de representations de la TST : le niveau sémantique,
le niveau syntaxique (de surface) et le niveau morphologique (profond). Le 4eme et derr1ier
niveau considéré sera le texte lui-méme, c'est-a-dire la séquence des caracteres de la phrase.
Nous aurons ainsi trois modules (modules sémantique, syntaxique et morphologique). Nous
allons les présenter maintenant, puis nous étudierons leurs différentes combinaisons, ce qui
nous permettra de faire le lien avec les grammaires completement lexicalisées comme TAG.

4.2.1 Module sémantique de GUST

Le module sémantique de GUST assure directement la correspondance er1tre le niveau
sémantique et le niveau syntaxique de surface, sans considérer un niveau syntaxique profond
intermédiaire. Nous considérons deux types de regles : des régles sémantiques lexicales, qui
mar1ipulent la conﬁguration sémantique formée d'un sémanteme lexical et de ses arguments
(plus exactement des dépendances sémantiques vers ses arguments), et des régles sémantiques
grammaticales, qui mar1ipulent un sémanteme grammatical (et la dépendance vers son
argument). Ces deux types de regles sufﬁsent a assurer la correspondance er1tre un graphe
sémantique et un arbre syntaxique de surface, puisque n'importe quel graphe sémantique peut
étre partitionné en un ensemble de conﬁgurations prises en entrée par nos regles sémantiques
lexicales et grammaticales.

On voit Figure 12 la regle qui donne la réalisation syntaxique de surface de la conﬁguration
sémantique composée du prédicat ‘parler’ et de ces deux premiers arguments. Dans la TST
standard, cette information se trouve dans l'entrée de dictionnaire de PARLER. En un sens, le
dictionnaire ne dit pas comment obtenir la correspondance er1tre ces deux configurations, et
l'information de la Figure 12 est en fait le résultat de la composition de plusieurs regles nodales
et sagittales sémantiques et syntaxiques profondes déclenchées sous le controle de l'entrée de
dictionnaire de PARLER. Dans la regle de la Figure 12, il est également indiqué que PARLER
est un verbe et que ce verbe doit recevoir des grammemes profonds de mode, temps et voix. Les
ﬁeches (—>) qui precedent ces grammemes indiquent que ceux-ci ne sont pas encore exprimés.
Ils le seront par des regles sémantiques grammaticales qui seront obligatoirement déclenchées
(en exigeant que les ﬁeches aier1t disparues dans une representation syntaxique bien formée).

28 GUST n'est pas exactement une autre presentation de la TST. Certains choix théoriques peuvent etre différents

et nous pensons que ce formalisme permet de résoudre certaines questions dont le traitement classique en TST
n'est pas tres clair, notamment tout ce qui conceme 1'interaction entre les différentes regles d'un meme module
ou de deux modules adjacents.
29 Voir également Hellwig 1986 pour une proposition antérieure de grammaire de dépendance basée sur
1'unification, appelée DUG (Dependency Unification Grammar).

Sylvain Kahane

‘parler’ PARLER (V)—>m,—>t,—>V
,O\ /O\
1 2‘ : dsuj iobj A
0 ‘Q (Prép)
‘X3 ‘Y9  
Y
(N)
sémantique syntaxique (de surface)

Figure 12 : Une regle de correspondance TST (sémantico-syntaxique profonde)

Nous proposons Figure 13 la regle de GUST qui simule la regle TST de la Figure 12. Au lieu
de mettre en correspondance un fragment de structure sémantique avec un fragment de structure
syntaxique, cette regle propose un fragment de structure produit sémantique-syntaxique,
exprimant a la fois la relation entre le sémanteme ‘parler’ et la lexie PARLER et les relations
entre les arguments 1 et 2 de ‘parler’ et les sujet et objet indirect (iobﬂ de PARLER. Dans la
regle de la Figure 13, l'arbre syntaxique est représenté explicitement (ce qui donne une certaine
primauté a la syntaxe), alors que le graphe sémantique est encodé dans l'étiquetage des noeuds
par l'intermédiaire des traits sém, argl et arg2. Chaque noeud possede un trait sém dont la
valeur est un sémanteme, le signiﬁé de la lexie étiquetant ce noeud. Lorsque ce sémanteme a des
arguments, ceux-ci sont les valeurs des traits arg1, arg2, etc., valeurs qui sont partagées avec
les traits sém des noeuds syntaxiques qui réalisent ces arguments. Le partage d'une valeur par
plusieurs traits est indiqué par une variable, la valeur elle-méme n'étant indiquée qu'une fois.3°
Notons encore que les mots vides portent un trait -Isém qui bloquera l'unification avec une
étiquette portant le trait sém.

PARLER

(V)+m,+t,+V
sém: ‘parler’

argl: x
arg2: y p
/O\ A
dsuj iobj; 
I ‘ISGIII
(N) pfép
sém: x o
(N)
sém: y

Figure 13 : Une regle sémantique lexicale GUST

Les regles lexicales se combinent par uniﬁcation. Nous présentons Figure 14 la dérivation de la
phrase Le petit chat dort ici par combinaison des regles lexicales associées aux lexies de cette
phrase (il s'agit en fait de regles lexicales sur lesquelles ont déja été appliquées les regles
graInInaticales, come on le verra plus loin). Deux regles se combinent par fusion de deux
noeuds et uniﬁcation des étiquettes correspondantes. Comme nous le verrons dans la suite,
plusieurs noeuds, ainsi que des dépendances, peuvent fusionner lors de la combinaison de deux
regles. Le résultat d'une dérivation est bien formé si cette derivation met bien en correspondance

3° Le fait de faire partager une méme valeur a plusieurs traits est une technique bien connue dans les fonnalismes
basés sur l'unification. Voir 1'usage intensif qu'en fait par exemple le fonnalisme HPSG (Pollard & Sag
1994).

Grammaires de dépendance formelles et théorie Sens- Texte

un graphe sémantique connexe avec un arbre de dépendance, c'est-a-dire si le résultat est un
arbre dont tous les traits sém sont instanciés (certains traits ont pour valeur une variable qui
indique en fait l'adresse de la valeur d'un autre trait qui est instancié).

DORMIR DORMIR
(vﬁndsprésem (V)ind,présent
ti ‘Présema t: ‘présent’
sém: ‘dormira (V) CHAT sém: e ‘dormir’
(N) arglz X sém: )5 N argl: x
sémzac .5 (N) CHAT ('3 9 E1:  /O\ ICI
d. deﬁm sem. x (N,maSC)Sg7_)d S]/[Jet adv sém: X ‘Chat, /SI/I] adv (Adv)
('3 ’ (,3 sém: ‘chat’ 6) ('3 ’/ \ sém; ‘jcj’
dét mod 0 (N) [C1 def "10? argl: e
‘J’ 6 Sémzx (Adv). . LE 6 PETI9l"
LE PETIT sém: ‘1c1’ (Dét) (Ad)
(Dét) (Adj) arglr X , , J 4 ft,
-Isem sem: e1
«sém sémz. ‘petit’ arglz E
argl. x

Figure 14 : Derivation de Le petit chat dort ici

Avant de revenir sur les regles lexicales, nous allons présenter les regles sémantiques
grammaticales. Les grammemes profonds sont calculés a partir de la représentation sémantique :
certains yapparaissent explicitement comme des sémantemes grammaticaux, d'autres seront
calculés a partir de la structure communicative sémantique (comme la voix qui dépend en partie
de la partition theme-rheme) et d'autres encore sont imposés par la rection (comme le mode
infinitif). Il n'est pas aisé de traiter la combinaison entre une regle lexicale et une regle
grammaticale par uniﬁcation, car un grammeme ne fait pas qu'ajouter de l'information : il peut
aussi entrainer une modiﬁcation importante du comportement de la lexie qu'il spécifie. C'est le
cas, par exemple, d'un grammeme de voix passive qui entraine une redistribution des fonctions
des actants syntaxiques de la lexie (l'0bjet devient sujet et le sujet devient un complément
d'agent).

Dans un premier temps, nous allons traiter les regles grammaticales comme des opérateurs qui
associent a une regle lexicale une nouvelle regle lexicale (o1‘1 un grammeme profond
supplémentaire est exprimé). Nous présentons Figure 15 les regles pour le présent, le passé
composé et la voix passive. Le passé composé d'un verbe X est exprimé par l'auxiliaire
AVOIR“ au présent et le verbe X au participe passé. L'auxiliaire AVOIR est l'auxiliaire par
défaut : si X possede un trait aux indiquant un autre auxiliaire (par exemple ETRE), la valeur
@aux de ce trait sera utilisée a la place de AVOIR. 32 Le sémanteme ‘passé composé’ apparait
dans l'étiquette de X (son argument est la valeur du trait sém de X), mais le grammeme profond
passé composé n'apparait pas en tant que tel. Seuls apparaissent les grammemes de surface
tels que présent ou p-passé (participe passé). Notez également le positionnement des traits
pour le mode sur l'auXiliaire et de la voix sur le verbe X.

Nous passons sous silence la question de la sémantique de l'auxiliaire. Nous devons assurer que les modiﬁeurs
de la forme verbale composée qui dépendent sémantiquement de l'auxiliaire prennent bien le signifié du verbe
comme argument sémantique.

32 La notation a//b utilisée dans les régles comme valeur d'un trait signifie : la valeur est a, si la valeur b ne peut
étre trouvée, et b sinon.

Sylvain Kahane

X X
(V)—>m,+t,—>V présent (V)—>m,présent,—>v
0 => t: ‘présent’
o

X AVOIR//@aux
(V)am,—>t,av , , (V)+m,présent

O passe compose O

' . 2 ./ ‘

su] su] 01/Us X
I o
(1; Y (V)p-passe,+v
t: ‘passé composé’

X ETRE
(V)+H1,—>t,~>V . (V)—>m,—>t

/O\ passif /o\ X

dsbtj dobzo :) O/SM] audgq (V)p_paSSé
Y Z Y (obl)
PAR// @ prép—passif
prép ‘ISGIII
Z

Figure 15 : Regles grammaticales GUST pour le présent, 1e passé composé
et la voix passive (version opérateur)

Nous proposons Figure 16 une autre version de la regle grammaticale pour le passé
composé. Cette regle se combine par uniﬁcation avec la regle lexicale d'un verbe. La montée
du sujet Y de X sur 1'auXi1iaire est assurée par la ﬂeche étiquetée sujet de X a Y. Cette ﬂeche,
que nous appelons une quasi-dépendance, Va fusionner avec la dépendance sujet de X (dans sa
regle lexicale) et tuer cette dépendance. Nous proposons également une regle grammaticale pour
le défini 1orsqu'il est exprimé par 1'artic1e LE. La détermination (défini, indéfini, partitif)
est un grammeme profond qui a une expression purement analytique et ne donne donc pas de
grammeme de surface.”

33 Ce comportement marginal de la déterrnination peut pousser certains a ne pas traiter la détennination comme
une catégorie ﬂexionnelle et a préférer traiter les lexies LE ou UN comme des lexies pleines exprimant les
sens ‘déﬁni’ et ‘indéﬁni’. Nous préférons notre solution. Cette solution, par 1'ob1igation d'exprimer la
déterrnjnation, régle aussi 1e probléme de la présence obligatoire du déterrninant.

Grammaires de dépendance formelles et théorie Sens- Texte

/O\
suj aux
C5‘  0

Figure 16 : Regles grammaticales GUST pour le passé composé et le défini
(version uniﬁcation)

Nous allons maintenant montrer comment sont traités quelques phénomenes linguistiques en
proposant d'autres regles sémantiques lexicales. Nous donnons Figure 17 la regle pour la
locution LA MOUTARDE MONTER AU NEZ. La regle sémantique pour une locution fait
correspondre un sémanteme a une conﬁguration de lexies de surface. Dﬁ au fait que seul la
racine de cette configuration accepte des modiﬁcations (cf. (4)), seule la racine de 1'arbre aura
un trait sém (instancié par le signiﬁé de locution), tandis que les autres noeuds auront un trait
-Isém qui bloquera toute modiﬁcation (puisqu'un modiﬁeur est un prédicat qui prend son
gouvemeur comme argument et exige donc que celui-ci ait un trait sém (cf. les regles pour
PET IT etICIde1a Figure 14). Un verbe avalent (sans argument) comme PLEUVOIR aura une
regle similaire a celle d'une locution, avec un noeud -sém pour le sujet vide.34

(4) a. La moutarde me monte sérieusement au nez
b. *La moutarde forte me momfe au nez

Figure 17 : Regles lexicales pour une locution et pour un verbe avalent

Le contraste entre verbe a controle (comme ESSAYER) et verbes a montée (comme
COMMENCER) est traditionnellement encodé dans les grammaires syntagmatiques dans la
structure syntaxique. Dans notre approche, les deux types de verbes ont exactement la méme
représentation syntaxique : le verbe gouverne un sujet et un inﬁnitif qui partage avec le verbe 1e
méme sujet (nous reviendrons sur la relation sujet de 1'inﬁnitif). Le contraste vient de la

34 Le traitement est différent en TST o1‘1 1'introduction d'un sujet Vide résulte d'une régle grammaticale syntaxique
profonde. D'ai11eurs, notre traitement n'est pas entiérement satisfaisant. I1 serait probablement préférable ck:
traiter le sujet dc PLEUVOIR comme un élément grammatical et non comme une portion dc locution,
puisque ce1ui—ci n'apparait pas dans certaines constructions comme Dieufait pleuvoir.

Sylvain Kahane

représentation sémantique : un verbe a controle prend son sujet comme argument sémantique,
mais pas un verbe a montée (Figure 18). Dans les deux cas, l'infinitif controle le sujet de son
gouvemeur et il faut un moyen d'assurer cela. Pour cela, nous considérons qu'un inﬁnitif
possede une sorte de dépendance sujet ; ce lien s'apparente a une dépendance, mais n'en est pas
une, car il ne compte pas dans la structure d'arbre et il n'est pas pris en compte dans la
linéarisation (cf. Hudson 2000 pour une proposition similaire). Un tel lien sera appelé une
quasi-dépendance. De meme, on aura dans la regle d'un verbe a controle ou a montée une quasi-
dépendance sujet pour l'inﬁnitif avec laquelle la quasi dépendance de la regle de l'infinitif devra
s'unifier. La quasi-dépendance est donc juste un moyen assez simple d'assurer le controle du
sujet du verbe a controle ou a montée par l'infinitif. Notons que ce controle est bien syntaxique :
il s'agit du sujet du verbe inﬁnitif et pas d'un argument sémantique précis. En effet, il peut
s'agir d'un sujet sémantique vide (5a), d'un sujet qui fait partie d'une locution (Sb) et, lorsque
ce sujet est plein, il peut s'agir aussi bien du premier argument (Sc) que du second argument
(Sd). En conséquence, les inﬁnitifs doivent avoir un sujet dans leur représentation syntaxique,
mais ce sujet sera une quasi-dépendance (aﬁn d'éviter qu'un verbe inﬁnitif ait un vrai sujet). La
regle sémantique du grammeme infinitif devra assurer que la relation sujet devienne bien une
quasi-dépendance .

(5) a. ll commence £1 pleuvoir.
b. La moutarde commence £1 lui monter au nez.
c. Le bruit commence £1 géner le gargon.
(1. Le gargon commence £1 étre gene’ par le bruit.

COMMENCER
(V) —>m,—>t,—>V
sém: ‘commencer’

argl: x
/O\ 
Sui inf‘ (Prép)
Qfusém
. Mir),
0
(V)inf
sém: x

Figure 18 : Regles lexicales pour un verbe a controle et un verbe a montée

Remarquons que le fait qu'un verbe a controle prenne son sujet comme argument sémantique
suffit a éviter que ce verbe ait un sujet vide (ce qui est un contraste bien connu entre verbes a
controle et verbes a montée) :

(6) a. *Il essaye de pleuvoir.
b. *La moutarde essaye de lui monter au nez.

Les verbes copules, c'est-a-dire les verbes prenant un attribut, seront traités de facon similaire
aux verbes a montée : dans la regle sémantique lexicale d'un verbe copule, on aura une quasi-
dépendance modzﬁeur indiquant le lien entre le dépendant du verbe copule “modiﬁe” par
l'adjectif attribut et l'adjectif lui-meme (Figure 19). Cette quasi-dépendance permet a la fois a
l'adjectif de récupérer son argument sémantique et d'assurer l'accord de l'adjectif avec le nom
“modiﬁe” par la regle d'accord ordinaire de l'adjectif avec le nom qu'il modiﬁe (et sans qu'il
soit nécessaire de faire circuler de l'information au travers du verbe copule). Enﬁn, cette
solution permet d'utiliser la meme regle lexicale pour l'adjectif qu'il soit épithete (7a) ou qu'il
controle le sujet (7b) ou l'objet (7c) (voir Figure 19 la regle pour l'adjectif PET IT ).

Grammaires de dépendance formelles et theorie Sens- Texte

(7) a. un petit livre
b. ce livre est petit
c. Pierre trouve ce livre petit

TROUVER
ETRE (V/’)ﬁItI1,—>t,—,>V
SCIIII I'Ol1VCI'
(V)—>II1é—>t,—>V argé, X
/ \ '
suj préd arg ‘:0
L [\O .
m0 (Adj) O/sw Cl)

Figure 19 : Regles lexicales pour les verbes copules et les adjectifs

Le phenomene dit du tough-movement peut étre decrit de la meme facon. Quand un adjectif tel
que FACILE gouverne un verbe, le nom que modiﬁe l'adjectif n'est pas son argument
semantique mais un argument semantique du verbe (8a). De plus, le nom doit remplir le role
d'objet direct du verbe. Par consequent, la regle semantique de FACILE contient une quasi-
dependance objet direct entre le verbe gouverne et le nom modiﬁe (Figure 19). Ainsi seul un
verbe pourvu d'un objet direct peut se combiner avec FACILE et l"‘extraction” de l'objet direct
du verbe sera assuree par l'unification de la dependance objet direct du verbe avec la quasi-
dependance de meme role de la regle de FACILE. On peut remarquer que la regle de FACILE
peut aussi se combiner avec un verbe copule (8b,c).

(8) a. un livre facile £1 lire
b. ce livre est facile £1 lire
c. Pierre trouve ce livre facile £1 lire

Nous arréterons la la presentation du module semantique de GUST. Come on l'a vu, l'un des
objectifs de GUST est d'eviter la multiplication des regles associe a une lexie. Sur le fragment
de grammaire propose, on a pu couvrir avec une seule regle lexicale par lexie un grand nombre
de constructions diverses. De meme, par la combinaison avec les regles grammaticales, on
construit les regles des differentes formes d'un verbe a partir d'une seule regle lexicale.”

4.2.2 Module syntaxique de GUST

Le module syntaxique de GUST correspond au module syntaxique de surface de la TST : il
assure la correspondance entre le niveau syntaxique de surface et le niveau morphologique
profond. Le module syntaxique possede trois types de regles : des regles d'accord, des regles
de regime et des regles de linearisation. On voit Figure 20 la regle d'accord du verbe avec son
sujet et la regle de rection des pronoms sujet (qui recoivent le nominatif). La regle d'accord du
sujet indique que le sujet s'accorde en nombre et en personne avec son sujet (9a). Lorsque le

35 Nous n'aVons pas discute des différentes sous—categorisations d'une meme lexie (par exemple, demander N a N,
que Vsubj, ti Vinf, ...). Tel que nous avons presente le fonnalisme, nous devrions introduire une regle pour
chaque sous-categorisation. Pour des questions d'efﬁcacite de l'analyse automatique ou de pertinence cognitive
(cf. Section 5.3), nous pensons preferable, tant que cela est possible, de rassembler ces différentes sous-
categorisations dans une meme regle. Nous devrons alors de introduire des disjonctions et considerer des
dependances optionnelles.

Sylvain Kahane

sujet ne possede pas de trait de personne, la valeur par défaut est 3 (9b,c) et lorsqu'il ne porte
pas de grammeme de nombre la valeur par défaut est le singulier (sg) (9c).

(9) a. Nous viendrons.
b. Pierre viendra.
c. Que tu viennesest impossible.

(V)3//p,sg//n (V)
‘.3 ‘P
suj suj
<5 <5
(N,p)n (N,pro)nom

Figure 20 : Regles syntaxiques d'accord et de rection

Les regles de linéarisation de GUST simulent les regles de linéarisation de la TST (présentées
dans la Figure 7 de la Section 3.2.3). Nous reprenons Figure 21 la regle de placement par
défaut d'un sujet non pronominal. Une regle de ce type met en correspondance la dépendance
entre deux noeuds syntaxiques avec une relation d'ordre (agrémentée d'un trait de position) entre
les noeuds morphologiques correspondants. Pour préparer le passage a GUST, nous avons
déplacé les conditions d'application de la regle.

f_f(V)
SW" <=> Y < X
g -10 0
(-'pro)
syntaxique morphologique

Figure 21 : Une regle de linéarisation TST

Nous proposons Figure 22 la regle de GUST qui simule la regle TST de la Figure 21. Au lieu
de mettre en correspondance un fragment de structure syntaxique avec un fragment de structure
morphologique, cette regle propose un fragment de structure produit syntaXique-
morphologique, c'est-a-dire un morceau d'arbre ordonné.

suj, pos: -10

Q

("P1‘0) (V)
Figure 22 : Une regle de linéarisation GUST

Les regles comme celles de la Figure 22 se combinent par uniﬁcation. On impose que le résultat
soit un arbre ordonné projectif (Kahane 2000b, 2001). Nous ne traiterons pas ici la question de
la linéarisation des arbres non projectifs (voir, par exemple, Broker 1998, Lombardo & Lesmo
1998, Kahane et al. 1998, Hudson 2000, Gerdes & Kahane 2001).

4.2.3 Module morphologique GUST

Nous terminons notre presentation des modules de GUST par le module morphologique qui
assure la correspondance entre le niveau morphologique (profond) et le niveau textuel, c'est-a-
dire la chaine de caracteres qui forme le texte d'une phrase. Les regles TST permettent de mettre

Grammaires de dépendance formelles et théorie Sens- Texte

en correspondance la représentation morphologique profonde d'un mot, une lexie de surface
accompagnée d'une liste de grammemes de surface, avec une chaine de caracteres. Nous
présentons Figure 23 une regle de ce type dans le style TST 35.

MANGER (Win d,PréSent,1,p1 {:) mangeons

morphologique (profond) textllel
Figure 23 : Une regle morphologique TST

Cette regle est simulée en GUST par une regle qui présente ces deux informations dans une
meme structure (Figure 24).

MANGER
(V)ind,présent, 1 ,pl
graph: mangeons

Figure 24 : Une regle morphologique GUST

Contrairement a la TST, GUST n'utilise pas de dictionnaire séparé : par exemple, le tableau de
régime est completement encodé dans les regles sémantiques. De meme, la partie du discours et
tous les traits pertinents (genre des noms, personne des pronoms, comportements particuliers,
...) devront étre introduit par la regle morphologique.

4.3 Combinaison des modules

Nous allons maintenant montrer comment les regles des différents modules se combinent pour
dériver une phrase, c'est-a-dire pour mettre en correspondance une représentation sémantique
avec un texte. L'avantage de GUST, sur un modele Sens-Texte standard, est que, comme pour
tous les formalismes basés sur l'unification, il est tres facile de combiner n'importe quelles
regles ensemble. En particulier, comme nous allons le voir, une grammaire GUST peut garder
une forme modulaire, comme la TST, ou étre completement lexicalisée, comme TAG (avec des
avantages sur cette demiere, notamment le fait qu'on peut éviter l'explosion du nombre de
structures élémentaires associées a chaque entrée lexicale.

4.3.1 Dérivation d'une phrase

Nous présentons Figure 25 l'ensemble des regles nécessaires a la dérivation de la phrase (10) :
(10) Nous essayons de manger la soupe.

Ces regles permettent de mettre en correspondance la représentation sémantique de (10) avec le
texte de (10). 11 y a plusieurs facons d'utiliser ces regles, dans le sens de l'analyse comme de la
synthese. Nous allons regarder le sens de l'analyse. Il s'agit de construire une représentation

sémantique a partir du texte. On peut distinguer deux stratégies principales : la stratégie
horizontale et la stratégie verticale. La métaphore horizontal/vertical s'entend par rapport au

36 Dans la TST standard, une telle régle est en fait le résultat de la composition d'un grand nombre de régles
morphologiques et phonologiques. Si nous Voulons étre capable de traiter des mots inconnus, nous devrons
avoir des régles de ce type.

Sylvain Kahane

découpage de l'ensemble des regles de la Figure 25, selon qu'il est fait en tranches horizontales
ou verticales.

ES S AYER
(V)ind,présent
sém: ‘essayer’
t: ‘present’
arg l: x

arg2: y
_/O\ _ DE
(‘SI/l_] mf\ 
V , -sém
(N) pfép

 ,. U
(V)inf
sém: y

Figure 25 : Dérivation de Nous essayons de manger la soupe

4.3.2 Stratégie horizontale

La strate’gie horizontale consiste a déclencher les regles module apres module.

1)

2)

Le module morphologique permet de passer du texte proprement dit (la chaine de caractere)
a la représentation morphologique, c'est-a-dire une suite de lexies accompagnées d'une liste
de grammemes. La regle introduit également la partie du discours et tous les traits pertinents
pour la suite. Le module morphologique réalise ce qu'on appelle traditionnellement la
lemmatisation, l'e’tiquetage morphologique ou le tagging. A noter que le module n'a pas le
pouvoir, comme le font ce qu'on nomme généralement des taggeurs, de ﬁltrer certaines
séquences de lexies (ou de catégories lexicales) qui ne peuvent apparaitre dans la langue. De
tels ﬁltres sont en fait la projection d'informations contenues dans le module syntaxique, et
nous pensons qu'il est préférable d'utiliser le module syntaxique lui-meme pour cette tache.
Notre étiqueteur ne fait donc que proposer pour chaque mot toute les lemmatisations
possibles sans tenir compte des étiquettes attribuées aux lexies voisines.

Le module syntaxique permet de passer de la représentation morphologique a la
représentation syntaxique. Il propose pour chaque couple de lexies, en fonction de leurs
positions relatives, une liste (éventuellement vide) de dépendances susceptibles de les lier.
Nous verrons Section 5 différentes procédures pour produire des arbres syntaxiques. Le
module syntaxique réalise ce qu'on appelle traditionnellement le shallow parsing ou analyse

Grammaires de dépendance formelles et théorie Sens- Texte

superficielle. A noter que le module syntaxique n'a pas le pouvoir de controler la sous-
catégorisation des lexies, ni meme de veriﬁer qu'un verbe a bien un et un seul sujet. Ceci
sera controlé par le module sémantique. Comme précédemment, il est possible de projeter
une partie du module sémantique sur le module syntaxique pour assurer ces points, bien que
nous pensions qu'il est préférable d'utiliser le module sémantique lui-meme. 7

3) Le module sémantique permet de passer de la représentation syntaxique a la representation
sémantique. Chaque dépendance syntaxique doit étre associée a une conﬁguration Inise en
correspondance avec une conﬁguration sémantique de relations prédicat-argument entre
sémantemes pour étre validée. Come on l'a dit, notre représentation sémantique est une
représentation du sens purement linguistique et n'a pas l'ambition d'étre une representation
de l'état du monde dénotée par la phrase. Pour cette raison, une grande partie de ce que
réalise notre module sémantique est considérée par beaucoup comme une étape de l'analyse
syntaxique et correspond a ce qu'on appelle généralement l'analyse profonde, ou deep
analysis.

La stratégie horizontale est la stratégie retenue par la plupart des approches modulaires. Le
principal inconvénient de la stratégie horizontale est le fait que la désaInbigui'sation (quand elle
est possible) n'intervient qu'au niveau sémantique et qu'il faudra manipuler aux niveaux
morphologique et syntaxique un tres grand nombre d'analyses concurrentes.

4. 3. 3 Stratégie verticale et lexicalisation complete
La stratégie verticale consiste a déclencher les regles mot apres mot.

Prenons l'exemple (10). Lorsqu'on analyse le mot nous, le module morphologique propose
(parmi d'autres propositions) d'étiqueter nous comme une forme nominative du pronom
NOUS. Mais, on peut alors, par la regle syntaxique de rection, en déduire qu'il s'agit d'un
clitique sujet, puis par la regle de linéarisation des pronoms sujet et par la regle d'accord prédire
la position du verbe et en partie sa forme. La regle sémantique associée a la lexie NOUS peut
étre également déclenchée. Par la seule analyse du mot nous, on peut donc déclencher 5 regles et
débuter l'analyse syntaxique et sémantique. La meme chose lorsqu'on analyse le mot suivant
essayons. Le module morphologique proposer d'étiqueter essayons comme une forme du verbe
ESSAYER. Cette forme remplit les conditions imposées par nous a son gouverneur syntaxique
et la regle peut donc étre immédiatement combinée avec les regles précédentes. Une des regles
sémantiques associées a ESSAYER peut étre déclenchée. Si l'on déclenche la regle ou le
deuxieme argument est réalisé par DE Vinf, on déclenchera les regles syntaxiques de
linéarisation associées a une telle construction. Et ainsi de suite. La Figure 26 montre les
différents paquets de regles déclenchés par les différents mots de la phrase.

37 Pour que l'ana1yseur soit robuste, la grammaire doit proposer un traitement par défaut des mots inconnus. Par
exemple, le module sémantique doit proposer, parmi ses différentes regles par défaut, une regle pour une
forme Verbale qui indiquera que la forme en question doit avoir un suj et et qu'elle aura au plus un objet direct,
un objet indirect et deux complements obliques. Ce sont les projections de ces regles sémantiques par défaut
qui sont les regles filtres généralement utilisées par les modules syntaxiques.

Sylvain Kahane

ES SAYER
(V)ind,présent
sém: ‘essayer’
t: ‘présent’
argl: x
arg2: y
/Ox. DE
SI/l_] lI”lf\ 
O,‘ (.3 -Isém

(N) prep
sém: x ('3
(V)inf
sém: y

Figure 26 : Regroupement des regles dans l'analyse verticale

Une stratégie d'analyse verticale semble beaucoup plus séduisante qu'une stratégie d'analyse
horizontale si l'on se place du point de Vue cognitif, c'est-a-dire du point de vue de la
modélisation du processus d'analyse linguistique par un locuteur. Meme du point de vue du
traitement informatique, une analyse verticale pourrait s'avérer plus efﬁcace qu'une analyse
horizontale. I1 existe d'ailleurs une variante de l'analyse verticale qui consiste a précompiler les
paquets de regles déclenchés par chaque mot. On obtient ainsi une grammaire dite complétement
lexicalisée (fully lexicalized grammar). La grammaire ainsi obtenue s'apparente a la grammaire
proposée par Nasr 1995, 1996, elle-méme inspirée des TAG (cf. également Kahane 2000a pour
un traitement des extractions). Le passage a des regles completement lexicalisées se fait
simplement par combinaison d'un paquet de regles modulaires (Figure 27). I1 s'agit de la
combinaison ordinaire des regles de la grammaire (basée sur l'unification) ; la seule difference
est que la combinaison des regles ne se fait pas au moment de l'analyse, mais dans une phase
préalable de précompilation (Candito 1996, Candito & Kahane 1998).

Le passage d'une grammaire modulaire a une grammaire completement lexicalisée amene
plusieurs commentaires.

1) L'analyse verticale avec la grammaire modulaire revient en fait a utiliser une grammaire
completement lexicalisée sans l'avoir lexicalisée au préalable, mais en construisant les
regles lexicalisées a la demande (on line) au moment de l'analyse. Quels sont alors les
avantages ou les inconvénients de la grammaire completement lexicalisée ? La
précompilation consomme de l'espace, puisqu'il faut mémoriser toutes les regles
lexicalisées, lesquelles sont extrémement redondantes entre elles. Par contre, le temps
d'analyse, si l'acces aux regles compilées est bien géré, devrait étre amélioré par le fait
qu'une partie des combinaisons de regles est déja faite. L'alternative entre grammaire
modulaire et grammaire completement lexicalisée peut aussi étre considérée du point de vue
cognitif : sous-quelle forme la grammaire est-elle codée dans notre cerveau ? Y-a-t-il des

Grammaires de dépendance formelles et théorie Sens- Texte

2)

3)

constructions linguistiques plus fréquentes que d'autres qui sont déja “lexicalisées” ? La
grammaire s'acquiert-elle sous forme modulaire ou “lexicalisée” ?

Les deux analyses, avec ou sans précompilation, posent les memes problemes théoriques, a
savoir quels sont les paquets de regles qui doivent étre associés a une lexie donnée ou, de
maniere équivalente, mais en se placant du point de Vue des regles plutot que des lexies, a
quelle lexie doit étre associée une regle donnée. Prenons un exemple : a quelle lexie,
gouvemeur ou dépendant, doit étre associée une regle de linéarisation? Considérons le cas
de l'objet direct en francais. Les regles sont les suivantes : un nom objet direct se place
derriere le verbe, un pronom clitique se place devant le verbe (a une place bien précise par
rapport aux autres clitiques) et un pronoms relatif ou interrogatif se place a l'avant de la
proposition. Il serait peu économique d'indiquer pour chaque nom, dans la regle lexicalisée
qui lui correspond, comment il se place quand il est objet direct, sujet ou autre chose encore.
Il est donc préférable d'attacher la regle de linéarisation de l'objet direct aux verbes qui en
possede un. Par contre, le pronom clitique objet direct a une forme bien particuliere et un
placement bien particulier. Il semble plus économique d'attacher a ce seul mot, le, le pronom
clitique objet direct, les regles qui lui sont spécifiques. De meme, les pronoms relatifs ou
interrogatifs ont un placement particulier qui ne dépend pas réellement de leur fonction. Il
semble donc aussi plus économique que la regle de placement de ces éléments leur soit
attachée. La solution retenue est donc de panacher l'information sur le placement de l'objet
direct entre les verbes transitifs pour les éléments canoniques (les noms) et les éléments non
canoniques eux-memes pour ce qui les conceme (voir Figure 27). Cette facon de faire
permet d'éviter la multiplication des regles lexicalisées associées a un meme verbe, comme
cela est le cas par exemple en TAG ou le formalisme ne permet pas d'encoder le placement
des arguments d'une lexie ailleurs que dans la regle (appelée structure élémentaire en TAG)
de cette lexie. Reste une difﬁculté : il faut éviter, lors de la combinaison d'un verbe x avec
un élément en position non canonique y, que rentre en conﬂit la regle de linéarisation des
éléments en position canonique attachée a x avec la regle de linéarisation spéciﬁque attachée
a l'élément y (voir Kahane 2000a pour une solution basée sur l'uniﬁcation consistant a
“tuer” la regle de positionnement attachée a la dépendance objet du verbe en l'unifiant avec
un leurre, une quasi-dépendance objet placée dans la structure associée a l'élément y).

En dehors des questions computationnelles, les grammaires completement lexicalisées ont
un autre intérét : il est tres facile d'écrire un premier fragment de grammaire et de l'étendre a
chaque nouvelle construction rencontrée. Néanmoins, de cette facon, on controle
difﬁcilement la consistance globale de la grammaire et certaines constructions obtenues
seulement par combinaison de phénomenes divers peuvent étre facilement oubliées (par ex. ,
la forme passive d'un verbe o1‘1 un complément est relativisé et un autre cliticisé). Pour cette
raison, des qu'on souhaite développer et maintenir une grammaire a large couverture, il est
nécessaire de controler la grammaire completement lexicalisée par une grammaire modulaire
a partir de laquelle on la génere. Dans le cadre des TAG, il a été développé des formalismes
modulaires a partir desquels on peut générer la grammaire TAG (Vijay-Shanker 1992,
Candito 1996, 1999), ainsi que des procédures pour générer la grammaire TAG a partir
d'une grammaire modulaire existante, comme HPSG (Kasper et al. 1995). Notre approche
présente un avantage par le fait que nous proposons un formalisme qui permet d'écrire a la
fois une grammaire modulaire et une grammaire completement lexicalisée. On peut ainsi
envisager de lexicaliser une partie de la grammaire seulement et de maintenir une grammaire
non lexicalisée pour les constructions marginales.

J 'aimerais insister, pour terminer cette section sur l'analyse verticale, sur le fait qu'une
grammaire modulaire ne s'utilise pas nécessairement module par module. Quand on parle d'une
architecture modulaire pour un systeme de TAL, on pense généralement, a tord, a une
succession de modules agissant les uns apres les autres. D'autre part, si j'ai mis l'accent sur le
lien entre l'analyse verticale et les grammaires lexicalisées, c'est parce que ce lien existe et que
les grammaires lexicalisées connaissent a l'heure actuelle un certain succes en TAL. Mais je ne
voudrais pas que ceci masque le fait que l'analyse verticale est possible sans précompilation et
qu'il s'agit, a mon avis, de la meilleure solution.

Sylvain Kahane

Enfin, tout ce que nous venons de montrer pour l'analyse est aussi valable pour la synthese. La
aussi, on peut envisager des strategies horizontales ou verticales et il est possible d'utiliser la
grammaire sous forme modulaire ou de la precompiler en une grammaire lexicalisee (cf. Danlos
1998, Candito & Kahane 1998 pour l'usage d'une grammaire completement lexicalisee en
synthese).

ESSAYER
(V)ind,present
sem: ‘essayer’
t: ‘présent’
argl: x
arg2: y
/Ox DE
suj inf (prep)
C5‘ ‘<_3 -sem
(N) prép
sémzx
(V)inf
sem: y

Figure 27 : Lexicalisation complete de essayons

Pour clore cette section sur les differentes strategies dans la combinaison des regles, notons que
les strategies verticales et horizontales representent les deux cas extremes et que des strategies
intermediaires peuvent étre envisagees. Par exemple, on peut envisager une strategie verticale
ou les regles ne sont pas regroupees mots par mots, mais chunks par chunks.

5 Analyse en grammaire de dépendance

Apres nos presentations des grammaires TST et GUST et de l'articulation des modules, nous
allons nous concentrer sur le module qui pose les plus grandes difﬁcultes en analyse, le module
syntaxique de la grammaire, c'est-a-dire le module qui assure la correspondance entre une
chaine de mots et un arbre de dependance. Nous considererons les regles de syntaxiques
presentees dans les Sections 0 et 4.2.2 (sous forme transductive, puis generative) : une telle
regle associe une dependance entre deux mots a une relation d'ordre entre les deux memes mots.
Nous allons presenter trois techniques d'analyse : l'analyse par contrainte et l'analyse CKY, qui
sont des strategies d'analyse horizontale (les regles sont declenchees module apres module), et
l'analyse incrementale avec un analyseur a pile, qui est une strategie verticale (les regles sont
declenchees mot apres mot).

Nous illustrerons nos differentes techniques d'analyse sur l'eXemple suivant :
(11) Le boucher sale la tranche

Cette phrase bien connue possede deux interpretations : ‘le boucher est sale et il tranche quelque
chose’ ou ‘le boucher met du sel sur la tranche’.

Grammaires de dépendance formelles et théorie Sens- Texte

5.1 Analyse par contraintes

Le principe de l'analyse par contraintes est de considérer toutes les structures imaginables et de
ﬁltrer a l'aide des regles les structures bien formées qui peuvent correspondre a la phrase. Plutot
que tester l'une apres l'autre toutes les structures imaginables (ce qui serait trop long), on
construit en fait une structure tres générale que l'on contraint par les regles et par les propriétés
de bonne formation (par exemple le fait que l'on veuille un arbre proj ectii).

On commence donc par envisager pour chaque couple de mots de la phrase toutes les
dépendances imaginables : on obtient ainsi un graphe de dépendance complet (Figure 28 de
gauche). Ensuite, on applique les regles de linéarisation pour ﬁltrer les dépendances qui sont
validées par une regle de linéarisation (Figure 28 de droite)38. Rappelons qu'une regle de
linéarisation dit, vu du point de vue de l'analyse, que si deux mots sont de telle et telle
catégories et s'ils sont dans tel ordre, alors une dépendance avec telle fonction syntaxique peut
les relier : par exemple, si un N suit un V, alors le N peut étre l'objet du V.

 

O O O O 0
le boucher sale la tranche
D/Cl N/V A/V D/Cl/N N/V

'1

le boucher sale la tranche

 

Figure 28 : Graphes de (11) avant filtrage et apres ﬁltrage par les regles de linéarisation

La demiere étape consiste a extraire des arbres projectifs du graphe ainsi obtenu (Figure 29).
(Nous ne détaillons pas cette étape ; coir les sections suivantes pour cela).

m /93“

o o o o o o o
lseboulclher sile la tranche le boucher sale la tranche

Cl V D N V D N
Figure 29 : Graphes de (11) apres filtrage complet

L'analyse par contraintes est particulierement adaptée aux grammaires de dépendances par le fait
que, contrairement aux grammaires syntagmatiques, il est facile de considérer une structure qui
contient en elle toutes les structures acceptables apres ﬁltrage. L'analyse par contraintes dans les

38 Pour simplifier la présentation, nous utilisons des catégories lexicales trés grossiéres. Par exemple, la
catégorie Cl Vaut pour tous les clitiques et comprend donc les (N ,pro)nom et (N,pro)acc.

Sylvain Kahane

graInInaires de dépendance a été introduite par Maruyama (1990a, 1990b) et développée, par
exemple, par Duchier (1999 ; Duchier & Debusman 2001) ou par Blache (1998, 2001 .

Le méme genre de techniques peuvent étre appliquées avec des regles pondérées suivant leur
probabilité d'apparition dans une situation donnée. Chaque regle possede un poids compris
entre 0 et 1 ; plus le poids est proche de 0 plus la regle est contraignante. Apres avoir construit
le graphe de toutes les dépendances imaginables, on Va utiliser les regles de linéarisation pour
adresser a chaque dépendance un poids : soit le poids de la regle si une regle s'applique, soit le
poids 0.1 si aucune regle ne s'applique (on évite les poids 0 qui écraseraient déﬁnitivement le
score final). On po11rra alors extraire du graphe l'arbre projectif qui donne le meilleur score (le
score d'un arbre est le produit des scores des dépendances) (Menzel & Schroder 1998 ;
Schroder et al. 2000). On po11rra méme accepter des entorses a la dépendance en pondérant
également les regles qui assurent la projectivité. Plus généralement, pour des méthodes
probabilistes en graInInaire de dépendance, voir Eisner 1996, Collins 1997.

5.2 Analyse CKY

L'analyse CKY a été développée indépendamment par Cocke, Kasami et Younger pour les
grammaires de réécriture hors-contextes (Kasami 1963, Younger 1967, Floyd & Biegel 1995).
L'analyse CKY est une analyse montante : il s'agit d'identifier des segments analysables de la
phrase de départ en allant des plus petits aux plus grands : si le plus grand segment analysable
est la phrase complete, la phrase est donc analysable. L'algorithme fonctionne en temps 0(n3) ou
11 est le nombre de mots de la phrase. Avec une grammaire syntagmatique hors-contexte, on
mémorise pour chaque segment analysé sa catégorie syntagmatique. L'algorithme peut étre
adapté ttivialement aux graInInaires de dépendance : dans ce cas, on mémorisera la catégorie
lexicale de la téte du segment.

Considérons une phrase de longueur n. Pour chaque segment analysé allant du i-ieme mot au j-
ieme mot (compris), on mémorise la catégorie X de la téte du segment sous la forme d'un triplet
[i,j,X]. Avec le module morphologique, on commence par analyser tous les mots de la phrase,
c'est-a-dire tous lessegments de longueur 1. Pour la phrase (11), on obtient :

|:1’19D]9 |:1’19C1]9 |:2’29N]9 |:2’29V]9 "'9 |:5’59N]9 

On essaye ensuite d'obtenir des segments de longueur 2 en utilisant les regles de linéarisation.
Par exemple, pour la phrase (11), [1,1,D] + [2,2,N] = [1,2,N], car un élément de catégorie D a
la gauche d'un élément de catégorie N peut dépendre de celui-ci par un dépendance dét. On
obtient donc, pour la phrase (11) :

[1,2,N], [1,2,V], [2,3,N], [2,3,V],  [4,5,N], [4,5,V]

Et ainsi de suite : les segments [i,j,X] et [j+1,k,Y] peuvent étre combinés pour donner le
segment [i,k,X] (resp. [i,k,Y]) s'il existe une regle de linéarisation indiquant qu'un élément de
catégorie X précédant un élément de catégorie Y peut gouvemer celui-ci (resp. peut dépendre de
celui-ci). On construit ainsi tous les segments de longueur 2, 3, etc., jusqu'a n. Par exemple,
pour construire les segments de longueur k (qui sont tous de la forme [i,i+k-1,Z]), on Va
considérer tous les couples de segments déja obtenus de la forme ([i,j,X],[j+1,i+k-1,Y] et
chercher ales combiner par les regles de linéarisation. Ceci demande k(n-k)C2R opérations4 ou

39 Blache 1998 considere au depart une grammaire syntagmatique avec tete a partir de laquelle i1 construit ensuite

un graphe de dépendances.

Pour k donne, on a n—k Valeurs pour i, k Valeurs pour j, C Valeurs pour X et Y et R fagons de combiner les
segments a tester.

Grammaires de dépendance formelles et théorie Sens- Texte

C est le nombre de catégories lexicales et R le nombre de regles de linéarisation. En sommant
sur k, on obtient un résultat en O(n3C2R).

Nous avons présenté l'algorithme de base. Tel quel cet algorithme vérifie que la phrase peut étre
associée a un arbre de dépendance projectif, mais il ne construit pas un tel arbre. Pour construire
des arbres associés, le plus simple est de redescendre le calcul en partant des segments
maximaux et de construire les arbres a partir de la racine. Pour l'exemple (11), 1e segment ﬁnal
[1,5,V] peut étre obtenu de trois facons : en combinant [1,3,N] et [4,5,V] par la regle de
placement du sujet, en combinant [1,3,V] et [4,5,N] par la regle de placement de l'objet ou en
combinant [1,2,N] et [3,5,V] par la regle de placement du sujet. Comme les deux dernieres
correspondent au méme arbre, on obtient en continuant les deux arbres de la Figure 29. On peut
éviter de refaire les calculs en descendant l'arbre en conservant davantage d'informations lors du
premier calcul (en indiquant pour chaque segment sa décomposition et la regle qui permet de
l'obtenir), mais cela est en fait plus coﬁteux. Quoi qu'il en soit, il faut noter que le nombre
d'arbres correspondant a une phrase de longueur 11 est dans le pire des cas une fonction
exponentielle de 11 et que, par conséquent, un algorithme qui construirait tous les arbres de peut
pas étre polynomial (sauf a représenter la forét d'arbres sous forme compacte).

Cet algorithme peut étre enrichi de différentes facons.

1) Nous n'avons pas encore pris en compte le placement respectif des différents dépendants
d'un meme noeud. Celui-ci est encodé dans nos regles de linéarisation par les traits de
position sur les dépendances. On peut tres facilement tenir compte des positions en gardant
en mémoire la derniere position utilisée pour chacune des deux directions : au lieu de
segments [i,j,X], on maI1ipulera des segments [i,j,X,p,q], ou p est la derniere position
utilisée pour un dépendant gauche de la téte du segment et (1 est la derniere position utilisée
pour un dépendant droit (p et (1 étant égaux a 0 si aucune regle n'a été utilisé). Un tel
segment ne po11rra pas étre combine a un segment dépendant que par une regle dont le trait
de position n'est pas compris entre p et q (le nouveau segment dépendant doit étre
positionné loin que les précédents). Pour l'exemple (11), si on combine les segments
[4,4,Cl,0,0] et [5,5,V,0,0] avec la regle qui relie un clitique objet dans la position -4 au
verbe, on obtiendra le segment [4,5,V,-4,0]. Ce segment ne po11rra pas étre combiné avec
un clitique qui exige une position entre -4 et 0, mais po11rra étre combiné avec un élément
qui accepte une position inférieure a -4.

2) Nous n'avons pas encore pris en compte les regles de sous-catégorisation, qui font partie
des regles sémantiques de notre grammaire. On peut considérer cette information en
indiquant dans la description d'un segment, en plus de la catégorie lexicale de la téte X, la
liste des éléments sous-catégorisés par X qui ne sont pas dans le segment. Si on reprend
l'exemple (11), 1e segment formé du seul mot tranche, lorsque ce dernier est analysé comme
une forme du verbe TRANCHER, recevra une liste de sous-catégorisation avec sujet et objet
direct : [5,5,V,{suj,d0bj}]. Lorsque ce segment sera combiné avec le segment formé du mot
la reconnu comme clitique accusatif, on obtiendra le segment [4,5,V,{suj}], par application
de la regle de linéarisation du clitique objet direct. Lorsque, ce nouveau segment sera
combiné avec le segment le boucher sale reconnu comme groupe nominal (et donc décrit
comme [1,3,N,®]), on obtiendra, par application de la regle de linéarisation du sujet, le
segment [1,5,V, Q]. A noter qu'on impose que, lors de la combinaison de deux segments,
le segment dépendant soit saturé, c'est-a-dire que sa liste de sous-catégorisation soit vide.
Quant a la liste de sous-catégorisation du segment téte, elle est privée de l'élément
correspondant a la fonction du segment dépendant.

Dans cet exemple, nous avons réduit l'information de niveau sémantique prise en compte au
minimum. Si nous prenons en compte l'ensemble de l'information contenue dans la regle
sémantique de la téte, notamment la description des dépendances sémantiques, nos
descriptions de segments vont alors s'apparenter fortement aux descriptions de syntagmes
en HPSG (Pollard & Sag 1994). Le mode de combinaison des segments que nous venons
de décrire s'apparente lui-méme au schéma de combinaison téte-actant d'HPSG (head-

Sylvain Kahane

daughter schema) : lorsque deux syntagmes X et Y se combinent pour en former un
nouveau syntagme et que l'un des segments, par exemple Y, est reconnu comme un actant
de la téte de l'autre, la description du nouveau segment est égale a la description du segment
téte X o1‘1l'élément Y a été retire de la liste des actants de X. La principale difference entre
notre approche et HPSG est que la combinaison de deux segments doit étre validée par une
regle syntaxique séparée. En conclusion, en restant a un niveau de description grossier, on
peut voir HPSG comme une version procédurale orientée vers l'analyse CKY d'une
grammaire de dépendance.

3) Nous n'avons pas non plus pris en compte l'analyse des structures non projectives. Cela est
possible. La modiﬁcation de l'algorithme dépend de la facon dont sont écrites les regles qui
assurent le placement des éléments qui ne sont pas dans la projection de leur gouverneur.
Kahane et al. 1998 propose des regles de “lifting”, permettant de remonter un élément sur
un ancétre de son gouvemeur et de le positionner par rapport a ce demier(cf. également
Broker 2000 pour une analyse commentée de cette solution). Dans la description d'un
segment, on indiquera donc en plus de la catégorie lexicale de la téte et de sa liste de sous-
catégorisation, la liste des éléments liftés. Encore une fois, cette solution s'apparente
fortement aux descriptions de syntagmes en HPSG ou apparait un trait Slash (non-local) : le
trait Slash contient précisément la liste des éléments “liftés”, c'est-a-dire des éléments qui ne
se placent pas dans la projection de leur téte, mais dans celle d'un ancétre de leur téte.

Avec les regles de “lifting”, on peut encore obtenir un algorithme polynomial, mais il faut
pour cela borner le nombre d'éléments “liftés” dans un segment (sinon le nombre de
segments que l'on peut considérer croit exponentiellement avec 11)..“

Remarquons que l'algorithme CKY est strictement montant et qu'une fois qu'un element a été
combiné avec son gouvemeur il n'est plus possible de le combiner avec un de ses dépendants.
(puisque le segment n'est représenté que par sa téte). Par exemple, si l'on analyse la phrase Le
gargon que j ’ai rencontre’ la semaine demiére est étudiant, il n'est pas possible de combiner le
gargon avec la téte de la relative (que ou ai suivant les analyses) tant que la relative dans son
entier n'a pas été analysée. La seule facon d'analyser le sujet de cette phrase est de combiner la
et demiére a semaine, puis la demiére semaine a rencontre’, puis le tout a at", et ainsi de suite
jusqu'a la combinaison de la relative complete avec gargon. Autrement dit, l'algorithme CKY,
s'il a l'avantage d'étre simple, ne peut en aucune facon étre rendu incrémental.

I1 existe un autre algorithme classique pour les grammaires hors-contextes, l'algorithme
d'Earley (Earley 1970, Floyd & Biegel 1995), qui peut étre aussi adapté aux graIr1Inaires de
dépendance (Lombardo 1996). A l'inverse de l'algorithme CKY, l'algorithme d'Earley est un
algorithme descendant. L'algorithme d'Earley fonctionne également en temps 0(n3) (ou 11 est le
nombre de mots de la phrase) et meme en temps 0(n2) pour les grammaires non ambigue.
Néanmoins l'algorithme d'Earley n'est pas précisément adapté a la langue naturelle qui est
hautement ambigue. En particulier, cet algorithme, s'il apparait comme plutot incrémental,
oblige en fait a constr11ire l'arbre a partir de la racine eta anticiper, des la lecture du premier mot,
sur la chaine complete de ces ancétres dans l'arbre.”

41 Il est probable qu'en l'absence d'une borne sur le nombre d'éléments liftés, le probleme de la reconnaissance

par une telle grammaire soit NP—complet (cf. Neuhaus & Broker 1997 pour un résultat de ce type).
42 Pour cette raison, l'algorith1ne d'Earley n'est pas applicable pour des grammaires décrivant des constructions
récursives (un V peut subordonner un V qui subordonne un V qui ...) sans introduire une limitation sur la
profondeur de la recursion. Ceci est un vrai probleme si on veut traiter de la langue naturelle. Par exemple, en
frangais, anglais, allemand, etc., le degré d'enchassement d'un groupe topicalisé en debut de phrase est
potentiellement infini.

Grammaires de dépendance formelles et théorie Sens- Texte

5.3 Analyse incrémentale
On appelle analyse incrémentale une analyse qui se développe au fur eta mesure de la lecture.

Les différents algorithmes d'analyse incrémentale different sur le traitement de l'ambigu'1'té.
Lorsque deux regles concurrentes sont applicables deux stratégies sont possibles : 1) choisir une
des deux regles et en cas d'échec revenir en arriere (back-track) et essayer la deuxieme regle ou
2) mener en parallele les deux analyses. Nous appellerons analyse incrémentale stricte une
analyse incrémentale qui ne permet pas de retour en arriere.

5.3.1 Analyse incrémentale et cognition

L'analyse incrémentale est la plus séduisante des techniques d'analyse du point de vue cognitif.
Il est clair que les humains analysent un texte au fur eta mesure qu'il en prenne connaissance et
qu'il peuvent parfaitement faire l'analyse d'un début de phrase et méme en proposer des
continuations.

Des expériences de psycholinguistique montrent par ailleurs que dans certains cas d'ambigui'té
majeure, les sujets humains font des retours en arriere (O'Regan & Pynte 1992). Les exemples
de ce type sont appelés des garden paths. Par exemple, lors de la lecture de (12b), on observe
(par l'analyse du mouvement des yeux) au moment de la levée d'ambigui'té, lorsque est est
considéré, une saccade régressive sur reconduit.

(12) a. L’espi0n msse reconduit a la frontiere un espion international.
b. L’espi0n msse reconduit a la frontiere est un espion international.

Dans la suite, nous allons donc nous intéresser a des algorithmes d'analyse incrémentale non
stricte, lesquels correspondent davantage au fonctionnement humain. Nous reviendrons dans la
Section 5.3.3 sur la question de savoir quelles sont les situations o1‘1 doit étre fait un choix,
conduisant éventuellement a un échec, et quelles sont les situations o1‘1 il faut éviter de traiter
séparément deux options.

Du point de vue computationnel, il est permis de penser que dans la mesure o1‘1 un systeme de
TAL cherche a obtenir les mémes résultats qu'un humain, la meilleure technique consiste a
chercher a simuler au maximum la facon dont procede un humain.

5.3.2 Analyseurs a pile

Laissons de coté la question de l'ambigu'1'té pour le moment. On peut associer un analyseur a
pile a un module syntaxique avec des regles de linéarisation comme celles que nous avons
présentées dans les Sections 3.3.5 et 4.2.2 (qui indiquent quels sont les couples de mots qui
peuvent étre reliés entre eux). La technique consiste a charger dans la pile les mots au fur et a
mesure de la lecture eta relier les mots par des opérations au sommet de la pile (Kornai & Tuza
1992, Kahane 2000b). Nous allons montrer sur un exemple comment fonctionne précisément
l'analyseur a pile associé a notre module syntaxique. Nous commenterons nos regles sur
l'eXemple de la Figure 30.

INPUT
le boucher sale
0 O O O
1 0 o o
D
2 Q . O
D N
3 .<‘”""u O
D N
4 ﬁg} O
D N A

la tranche
O O
o o
o o
o o
O O
O O
0 0
Cl
0 0
Cl V

Cl V
Cl V

PILE

23>

- -10 +5

._

+00

- -10 +5

+00

- -10 +5

--40

+00

ZD>< ZD>Q< ZD>O 23>

- -10 +5

--40

Z<

- -10 +5

V

--10 0

Sylvain Kahane

REGLES

initialisation

le <=> LE
(D)masc,sg

boucher <3 BOUCHER
(N,masc)sg
dét, pos: -10

Q

(D) (N)

sale <3 SALE
(Adj)masc,sg

mod, pos: +5

/’\

o—o
(N) (A)

la <:.« LUI
(C1)acc,fém,sg

tranche <3 TRANCHER
(V)ind,prés,3,sg

obj, pos: -4

Q

(C1) (V)

dépilement

suj, pos: -10

Q

(N) (V)

Figure 30 : Analyse de (11)

L'analyseur effectue la lecture de la phrase de gauche a droite. Au départ, la pile est vide (étape
0 : initialisation). Nous avons quatre types de regles de transition.

1) Transition d'empilement (étapes 1, 2, 4, 6 7). A chaque fois qu'un mot nouveau est lu,
une regle morphologique est déclenchée et la catégorie du mot analysé est stockée dans la
pile. Trois autres parametres completent la catégorie : le deuxieme parametre indique si le
noeud est déja gouverné ou non (- pour non gouverné, + pour gouverné), le troisieme
parametre donne le valeur du trait de position de la derniere regle de linéarisation avec un

Grammaires de dépendance formelles et théorie Sens- Texte

position négative utilisée et le quatrieme parametre donne la valeur du trait de position de la
derniere regle de linéarisation avec une position positive utilisée. Lors du stockage, ces
parametres ont respectivement la valeur -, 0 et 0.

2) Transition de liaison a un dépendant a gauche (étapes 3, 8, 10). Une telle transition
correspond a une regle de linéarisation dont le dépendant est a gauche. Les deux noeuds que
nous allons lier se trouvent dans les deux cases supérieures de la pile. Appelons les x et y, x
étant le dernier noeud lu et se trouvant sur le dessus de la pile. La regle peut s'appliquer si x
et y possedent les catégories requises par la regle de linéarisation. Le noeud y ne doit pas étre
déja gouverné (valeur - du deuxieme parametre). Enﬁn, la valeur du trait de position de la
regle doit étre supérieure en valeur absolue a celle de la demiere regle avec une position
négative utilisée pour relier x a un dépendant a sa gauche (voir le troisieme parametre). Ainsi
a l'étape 8, le verbe tranche (= x) est lié avec le clitique la (= y) par un regle de position -4.
Apres application de la regle, le troisieme parametre de tranche prend la valeur -4. A l'étape
10, 1e verbe tranche est he a son sujet par une regle de position -10, ce qui est possible car
-10 est supérieur en valeur absolue a -4. Lors de l'application de la regle, la valeur -10 est
consignée a la place de -4 dans la case de tranche. En raison de la projectivité, le noeud y est
retiré de la pile. En effet, ce noeud ne peut avoir de dépendants a la droite de x, sans
enfreindre la proj ectivité.

3) Transition de liaison a un gouverneur a gauche (étape 5). Une telle transition
correspond a une regle de linéarisation dont le gouverneur est a gauche. Comme
précédemment, les deux noeuds que nous allons lier se trouvent dans les deux cases
supérieures de la pile. Appelons les x et y, x étant le dernier noeud lu et se trouvant sur le
dessus de la pile. La regle peut s'appliquer si x et y possedent les catégories requises par la
regle de linéarisation. Le noeud x ne doit pas étre déja gouverné (valeur - du parametre
correspondant). Apres la transition, les noeuds x et y sont tous les deux maintenus dans la
pile, puisqu'ils peuvent avoir tous deux des dépendants a droite de x. Comme le noeud X est
maintenant gouverné, la valeur du parametre passe de - a +. Enfin, la valeur du trait de
position de la regle doit étre supérieure en valeur absolue a celle de la derniere regle de
linéarisation avec une position positive utilisée pour relier y a un dépendant a sa gauche (voir
le quatrieme parametre). A l'étape 5, le nom boucher (= y) n'a pas encore eu de dépendant a
droite. Son quatrieme parametre est donc égal a 0. Apres application de la regle, ce
parametre aura la valeur +5.

4) Transition de dépilement (étape 9). A tout moment, il est possible de retirer de la pile
un noeud qui est déja gouverné. Pour pouvoir lier tranche a son sujet boucher, on est ainsi
obligé de dépiler l'adjectif sale (qui de toute facon, en raison de la projectivité ne po11rra plus
avoir de dépendant au -dela de tranche).

Une phrase est reconnue si a la ﬁn de la lecture, la pile contient un unique noeud non gouverné,
qui est en fait la racine de l'arbre de dépendance. Notons que nous assurons bien que le graphe
construit est un arbre et que cet arbre est projectif.

L’analyseur en ﬂux de Vergne (2000) utilise une méthode similaire a l'analyseur que nous
venons de présenter, si ce n’est qu’il effectue un séquencage (chunking) préalable de la phrase
et charge, au lieu des mots, les blocs (chunks) ainsi obtenus dans la pile (construisant ainsi un
arbre de dépendance sur les blocs).

Comme pour l'analyseur CKY, cet analyseur peut étre enrichi en prenant en compte des regles
de plus haut niveau, notamment des regles de sous-catégorisation. On chargera alors dans la pile
non seulement les caractéristiques morphologiques d'un mot (sa catégorie lexicale), mais aussi
ses caractéristiques sémantiques. Voir Nasr 1995, 1996, Kahane 2000a, Lombardo 1992 pour
des analyseurs de ce type (lesquels s'apparentent également aux grammaires catégorielles de
Ajdukiewicz-Bar-Hillel). Une variante de ces analyseurs consiste a charger dans la pile non pas
des mots, mais les liens potentiels qu’un mot peut avoir avec les mots qui le suivent. Cette
méthode repose sur une description complete des valences possibles d'un mot. Le plus abouti

Sylvain Kahane

des analyseurs de ce type est la Link Grammar de Sleator & Temperley 1993. Dans ce
formalisme lexicalise, chaque regle decrit l'ensemble des liens que peut avoir un mot donne,
c'est-a-dire les actants, mais aussi les modiﬁeurs et les conjoints eventuels, ainsi que le
gouverneur.

Ces methodes peuvent etre complexifiees pour traiter des arbres non projectifs : on peut, par
exemple, garder en memoire dans la pile dans la case d'un mot donne des informations sur
certains de ses dependants et autoriser les dits dependant a creer des liens lorsque la case de leur
gouvemeur est consideree. Nous ne developperons pas cette question, par ailleurs fort
interessante, dans cette presentation (voir Nasr 1995, 1996, Kahane 2000a pour des traitements
de ce type).

5.3.3 Traitement des ambiguftés

Comment traiter les cas d'ambigu'1'te avec un analyseur incremental ? La premiere technique
consiste a faire des choix. A chaque fois que plusieurs regles concurrentes se presentent, il
faudra choisir une regle. On peut developper des heuristiques, pour a chaque fois qu'un choix
se presente, faire le “meilleur” choix. En cas d'echec, on peut effectuer un retour en arriere au
dernier point ou un choix a ete fait et essayer le choix suivant. Si on autorise les retours en
arriere sans mecanismes additionnels, on obtient un algorithme en temps exponentiel dans le
pire des cas. En particulier, le pire des cas sera atteint a chaque fois qu'on aura affaire a une
phrase agrammaticale (= pour laquelle notre grammaire ne peut fournir d'analyse). Pour obtenir
un temps de traitement raisonnable, deux techniques sont possibles. La premiere consiste
simplement a limiter les retours en arriere : on peut par exemple se ﬁer entierement aux
heuristiques qui nous aident a faire le meilleur choix et interdire tout retour en arriere. On obtient
alors un traitement en temps lineaire. C'est ce que fait l'analyseur en ﬂux de Vergne 2000. Voir
egalement Amola 1998 pour un analyseur deterministe base sur les dependances. La deuxieme
technique, appeler memo'1'sation, consiste, lors d'un retour en arriere, a conserver en memoire
les analyses deja faites pour ne pas avoir a les refaire. Par exemple, si l'on considere l'exemple
(11b) de garden-path, il ne sera pas necessaire apres le retour en arriere et le deuxieme choix
pour reconduit de refaire l'analyse de 61 la fromriére (ce qui peut devenir vraiment interessant si
frontiére gouveme en plus une relative) La memo'1'sation, utilisee par Sleator & Temperley 1993
pour les Link Grammars, permet d'assurer une complexite en 0(n ).

La deuxieme technique de traitement des ambigu'1'tes consiste a ne pas faire de choix et a mener
en parallele les differentes analyses. Par exemple, Nasr 1995, 1996 utilise une technique
adaptee de ToIr1ita 1988 consistant a dupliquer la pile ; on peut ensuite factoriser un certain
nombre d'operations effectuees plusieurs fois dans plusieurs piles en factorisant les piles au sein
d'une pile a structure de graphe et garantir un temps de traitement polynomial.

J 'aimerais faire quelques commentaires sur la question du choix en me placant d'un point de vue
linguistique et cognitif. Certaines grammaires, notamment des grammaires completement
lexicalisees comme TAG, considerent des regles differentes pour chacune des sous-
categorisations d'un verbe (par exemple, parler 61 Marie, parler de Jean, parler de Jean 61 Marie
correspondront a trois regles differentes pour parler). De telles grammaires obligent a faire des
choix a tout Va et ne font pas la difference entre des choix non pertinents (comme les differentes
sous-categorisation de parler qui devraient etre traitees en parallele) et des choix pertinents
(comme les deux reconduit que l'on trouve en (11a) et (11b), qui different fortement puisque le
premier gouveme le nom qui le precede, tandis que le deuxieme en depend). Je pense que la
grammaire doit etre ecrite de telle facon que seuls les choix reels (les choix qui pourront
conduire un locuteur a un retour en arriere s'il n'a pas fait le bon choix du premier coup)
correspondent a des regles separees. Par exemple, les differentes sous-categorisations possibles
d'un meme verbe devront etre rassemblees en une meme regle. I1 serait souhaitable d'evaluer
precisement les situations qui provoquent des retours en arriere chez un locuteur aﬁn de decider
quand deux constructions doivent etre traitees par une meme regle et quand deux situations
doivent correspondre a deux regles bien separees entre lesquelles le locuteur doit faire un choix.

Grammaires de dépendance formelles et théorie Sens- Texte

On po11rra noter toute l'attention que nous avons portée a cette question dans l'écriture des
regles de GUST (Section 4.2).

5.3.4 Limitation du ﬂux

On peut observer sur les arbres de dépendances ordonnés des phrases d'une langue certaines
limitations. Ainsi, bien qu'en l'absence de larges corpus étiquetés par des dépendances nous ne
puissions étre absolument affirmatif, il apparait que le ﬂux des dépendances est généralement
borné par 6 ou 7 (voir Yngve 1960, Tuza & Komai 1992 ou Murata et al. 2001 pour des
hypotheses de cette nature). Nous appelons flux des dépendances en une position donnée (entre
deux mots d'une phrase) le nombre de dépendances qui relient un mot a gauche de cette position
a un mot a droite (Figure 31).

 

Figure 31 : Le ﬂux des dépendances pour (11)

On peut penser que cette borne sur le ﬂux correspond a une limitation liée a la mémoire
immédiate: un locuteur ne peut gérer simultanément plus de 7 dépendances (voir la fameuse
étude de Miller 1956 sur le fait que les humains ont au plus 7 i 2 elements dans leur mémoire a
court terme).

Si l'on borne le ﬂux des dépendances, on peut alors borner la taille de la pile dans l'analyseur a
pile que nous avons présenté. Comme le langage de la pile est fini, le nombre de contenus
possible de la pile est alors ﬁni (bien que tres gros). L'analyseur a pile, si l'on ne s'intéresse
plus aux arbres de dépendance qu'il produit, est alors equivalent a un automate a nombre ﬁni
d'états (un état de l'automate est un contenu de la pile). Cet automate est donc equivalent a un
automate déterministe, ce qui nous donne un reconnaisseur en temps linéaire (l'automate ne
fournit plus d'analyse, mais peut seulement reconnaitre les phrases qui ont une analyse). Cet
automate peut étre particulierement 11tile pour ﬁltrer les phrases agrammaticales, qui sont les
phrases les plus coﬁteuses pour l'analyseur incrémental (puisque n'importe quel choix conduit a
une situation d'échec et a un retour en arriere). Néanmoins, pour que cet automate ne soit pas
trop gros, il faudra certainement limiter le nombre de symboles de pile (le nombre d'état de
l'automate avant déterminisation est majoré par Z" ou Z est le nombre de symboles de pile et k le
nombre maximum de noeuds autorisés dans la pile).

On peut également espérer optimiser l'analyseur incrémental par une étude statistique des
contenus possibles de la pile pour des analyses correctes. Ceci permettrait dans une situation
donnée de choisir entre des regles aﬁn d'obtenir le contenu de pile le plus probable et d'éviter au
maximum les échecs et les retours en arriere.

6 Conclusion

Comme nous l'avons dit au début de cet exposé, la dépendance est maintenant une notion
utilisée par toutes les théories linguistiques, bien qu'elle soit souvent cachée sous diverses
formes (fonctions syntaxiques, constituants avec téte, ...). Nous espérons avoir convaincu le
lecteur de l'intérét qu'il y a a mettre en avant la dépendance et a écrire des regles qui maI1ipulent
explicitement des dépendances.

Sylvain Kahane

A travers l'etude des dependances, nous avons souhaite mettre l'accent sur la theorie Sens-
Texte. La TST est l'une des theories qui separe le plus clairement les notions semantiques,
syntaxiques et morphologiques, en distinguant, en particulier, les dependances semantiques et
syntaxiques, les lexies profondes et de surface, les grammemes profonds et de surface ou en
separant clairement les regles de sous-categorisation, d'ordre des mots, d'accord et de rection.
D'autre part, la TST, en privilegiant la synthese sur l'analyse, met bien en evidence l’avantage
des grammaires de dependance sur les grammaires syntagmatiques. En effet, la synthese debute
avec une representation semantique, a un moment ou l'ordre des mots n'est pas encore fixe.
Lorsqu'on veut decrire la synthese d'une phrase en passant d'une representation semantique a
une representation ou les mots sont ordonnes, on voit tout l’avantage qu’il y a a avoir un moyen
de representer la structure syntaxique sans avoir encore encoder l’ordre des mots ou meme le
regroupement des mots en constituants de surface. La synthese met egalement l'accent sur
l'importance des choix lexicaux et du lexique. En particulier, un notion comme celle de fonction
lexicale prend toute son importance lorsqu'il faut faire les bons choix lexicaux (et ne pas dire
follement improbable ou hautement amoureux a la place de hautement improbable ou de
follement amoureux).

Nous avons egalement presente une grammaire d'unification basee sur la TST, la Grammaire
d'Unification Sens-Texte (GUST). Au dela de son interet propre, ce formalisme permet de
rattacher plus facilement la TST a d'autres formalismes contemporains, comme HPSG, LFG,
les Grammaires Categorielles ou TAG. GUST herite de la TST une claire separation des
informations semantiques, syntaxiques et morphologiques et la modularite qui en resulte. Nous
avons pu, au travers de GUST, montrer comment les regles de differents modules pouvaient
etre combinees, permettant, par exemple, d'ecrire une grammaire completement lexicalisee.
D'autre part, contrairement aux grammaires completement lexicalisees ecrites dans d'autres
formalismes (comme TAG), GUST permet de porter une grande attention a la facon dont les
regles de la grammaire modulaire doivent etre reparties entre les differentes lexies pour eviter
une explosion du nombre de regles de la grammaire lexicalisee.

Nous avons termine notre expose par une presentation theorique des principales techniques
d'analyse. Il faut noter que les grammaires de dependances, a la difference des grammaires
syntagmatiques, n'ont pas encore fait l'objet de travaux mathematiques ou d'informatique
theorique d'envergure. Il n'existe pas pour les grammaires de dependance de formalisme de
reference (voir Kahane 2000b pour une proposition), comme le sont les grammaires de
reecriture hors-contextes de Chomsky (1957) pour la grammaire syntagmatique. De meme, tous
les compilateurs de langages de programmation sont bases sur des techniques developpees pour
les grammaires hors-contextes. Nous esperons avoir montre que les memes techniques (comme
l'algorithme CKY) se pretaient au traitement des grammaires de dependance et qu'en plus, les
grammaires de dependance permettaient des techniques propres, comme l'analyse incrementale
avec un analyseur a pile dont les cases de la pile contiennent les descriptions des mots de la
phrase. D'autre part, nous avons pu faire le lien entre GUST et HPSG, montrant comment les
grammaires syntagmatiques se presentent en fait comme des versions procedurales des
grammaires de dependance orientees vers l'analyse (et plus precisement l'analyse CKY, qui
n'est pas, du point de vue cognitif et meme computationnel, le plus interessant des algorithmes
d'analyse de la langue )43.

Nous souhaiterions clore cet expose, en evoquant ce que nous aurions aime presente et que
nous n'avons pu presenter faute d'une maturite suffisante des notions concemees et d'un
developpement suffisant des travaux sur ces questions. Dans la Section 3.2.1, nous avons
montre le rele primordial que joue la structure communicative dans la representation semantique
d'une phrase, mais nous n'avons pas pu montrer comment la structure communicative

43 Quand on sait que les fondements de la grammaire syntagmatique reposent sur le distributionnalisme, c'est—a-
dire sur une description des langues par la distribution des segments de textes, il n'est pas etonnant que la
grammaire syntagmatique ait un lien etroit avec un algorithme de type CKY.

Grammaires de dépendance formelles et théorie Sens- Texte

intervenait dans les différentes regles des différents niveaux. La structure communicative joue
un role essentiel dans la hiérarchisation du graphe sémantique (notamment le choix de la téte
syntaxique de la phrase) et dans la linéarisation. Dans les langues a ordre des mots relativement
libre comme le russe ou l'allemand, la structure communicative (notamment la partition theme-
rheme et la focalisation) controle fortement l'ordre des mots et la prosodie. Dans des langues a
l'ordre moins libre, comme le francais ou l'anglais, la structure communicative se réalise par des
constructions particulieres, comme le clivage, le pseudo-clivage ou la dislocation en francais.
D'autre part, nous n'avons pas abordé la question des constituants morphologiques : les mots,
lorsqu'ils sont linéarisés, s'assemblent pour former des groupes qui sont placés les uns par
rapport aux autres. Ces constituants morphologiques sont Inis en évidence, entre autres, par la
prosodie. La notion de constituant morphologique doit étre distinguée de la notion de constituant
syntaxique, laquelle n'est pas directement considérée en grammaire de dépendance.“ Les
constituants morphologiques forment une hiérarchie comparable aux constituants syntaxiques,
mais il ne servent pas a représenter la structure syntaxique d'une phrase, laquelle est
représentée, dans notre cadre théorique, par un arbre de dépendance. Parmi les constituants
morphologiques, il faut en particulier distinguer les blocs (ou chunks) a l'intérieur desquels
l'ordre des mots est tres rigide et qui n'accepte pas de coupures prosodiques, comme les
séquences determinant-adjectifs-nom ou clitiques-verbe du francais (Mel'éuk 1967, Abney
1991, Vergne 2000). Le role joué par de tels blocs (que ne considerent d'ailleurs pas les
grammaires syntagmatiques) n'est plus a faire en TAL, que ce soit pour l'analyse syntaxique ou
la synthese de la prosodie (Mertens 1997, Vergne 2000). La structure communicative joue un
grand role, a coté de la structure de dépendance, dans la formation des constituants. Kahane &
Gerdes 2001 propose, a partir de l'étude de l'ordre des mots en allemand, un formalisme qui
permet d'associer a un arbre de dépendance une hiérarchie de constituants morphologiques, qui
n'est pas le reﬂet immédiat de l'arbre de dépendance (et qui ne correspond donc pas non plus a
une structure de constituants syntaxiques). Un meme arbre de dépendance correspond a de
nombreux ordres des mots et un meme ordre des mots peut recevoir différentes structures de
constituants morphologiques correspondant a différentes prosodies, mettant en évidence
différentes structures communicatives. Ce travail doit maintenant étre poursuivi pour montrer
comment une structure communicative permet de choisir une structure de constituants
morphologiques plutot qu'une autre.

Références

Abeillé Anne, 1991, Une grammaire lexicalisée d’Arbres Adjoints pour le frangais, These de
doctorat, Université Paris 7, Paris.

Abeillé Anne, 1996-97, “Fonction objet ou position objet ” (1% et 2"“ parties), Gre’ des
langues, 11, 8-29 ; 12, 8-33.

Abney Steven, 1987, The English Noun Phrase in its Sentential Aspect, PhD thesis, MIT,
Cambridge.

Abney Steven, 1991, “Parsing by chunks”, in R. Berwick, S. Abney and C. Tenny (eds.),
Principle-Based Parsing, Kluwer.

Abney Steven, 1992, “Prosodic structure, performance structure and phrase structure”,
Proceedingsof Speech and Natural Language Workshop, Morgan Kaufmann, San Mateo, CA.

"4 Comme nous l'avons montré dans la Section 2.1, les constituants syntaxiques considérés par les grammaires
syntagmatiques peuvent etre récupérés a partir de l'arbre de dépendance : ce sont les projections des sous—arbres
constitués d'un noeud et de tout ou partie de ses dependants. Par exemple, un constituant S ou Inﬂ' est la
projection d'un verbe, tandis qu'un constituant GV est la projection d'un verbe sans son sujet.

Sylvain Kahane

Anderson John, 1971, The Grammar of Case.‘ Towards a Localist Theory, Cambridge
University Press, Cambridge.

Ajdukiewicz Kasimir, 1935, “Die syntaktische Konnexitat”, Studia Philosophica, 1, 1-27.

Apresjan J., Boguslavskij I., Iomdin L., Lazurskij A., Sannikov V., Tsinman L., 1992,
“ET AP-2: The linguistics of a machine-translation system”, Meta, 37:], 97-112.

Amola Harri, 1998, “On parsing binary dependency structures deterministically in linear time”,
Processing of Dependency-based Grammars, C OLING/ACL’98 Workshop, 68-77.

Bar-I-Iillel Yehoshua, 1953, “A quasi-arithmetical notation for syntactic description”, Language,
29.], 47-58.

Bar-I-Iillel Yehoshua, Gaifman Ha'1'm, Shamir E., 1960, “On categorial and phrase-structure
grammars”, Bull. Res. Counc. of Israel, 9F, 1-16.

Blache Philippe, 1998, “Parsing ambiguous structures using controlled disjunctions and unary
quasi-trees”, COLING/ACL’98, Montréal, 124-30.

Blache Philippe, 2001, Les grammaires de proprie’te’s .' des contraintes pour le traitement
automatique des langues naturelles, Hermes, 224p.

Blanche-Benveniste Claire, 1975, Recherche en vue d’une the’orie de la grammaire francaise.
Essai d’application a la syntaxe des pronoms, Champion, Paris.

Bloomﬁeld Leonard, 1933, Language, New York.

Boyer Michel, Lapalme Guy, 1985, “Generating paraphrases from MeaI1ing-Text semantic
networks”, Computational Intelligence, 1, 103-117.

Bresnan Joan (ed), 1982, The Mental Representation of Grammatical Relations, MIT Press,
Cambridge.

Bresnan Joan, Kaplan Ronald, Peters Stanley, Zaenen Annie, 1982, “Cross-serial dependencies
in Dutch” Linguistic Inquiry, 13:4, 613-635.

Brody Michael, 1997, Lexico-Logical Form.‘ A Radically Minimalist Theory, MIT Press,
Cambridge.

Broker Norbert, 2000, “Unordered and non-projective dependency grammars”, T.A.L., 41:1,
245-272.

Candito Marie-Helene, 1996, “A principle-based hierarchical representation of LTAG”,
COLING’96, Copenhagen.

Candito Marie-Helene, 1999, Organisation modulaire et paramétrable de grammaires
e’lectroniques lexicalisées. Application au francais et a l ’italien, These de doctorat, Université
Paris 7, Paris.

Candito Marie-Helene, Kahane Sylvain, 1998, “Une grammaire TAG vue comme une
grammaire Sens-Texte précompilée”, TALN '98, Paris, 40-49.

Chomsky Noam, 1957, Syntactic Structure, MIT Press, Cambridge.

Chomsky Noam, 1965, Aspects of the Theory of Syntax, MIT Press, Cambridge.

Grammaires de dépendance formelles et théorie Sens- Texte

Coch Jose, 1996, “Overview of AlethGen”, Proc. 8th Int. Workshop on Natural Language
Generation (INLG’96), Vol. 2, Herstmonceux, 25-28.

Coch Jose, 1998, “Interactive generation and knowledge administration in MultiMeteo”, Proc.
9th Int. Workshop on Natural Language Generation (INLG’98), Niagara-on-the-Lake, 300-
303.

Courtin Jacques, Genthial Damien, “Parsing with dependency relations and robust parsing”,
Workshop on Dependency-based Grammars, C OLING/ACL’98, Montréal, 25-28.

Danlos Laurence, 1998, “G-TAG : un formalisme lexicalisé pour la générationde textes inspiré
de TAG”, T.A.L., 39:2, 7-34.

Dikovsky Alexander, Modina Larissa, 2000, “Dependencies on the other side of the Curtain”,
T.A.L., 41:], 79-111.

Duchier Denys, 1999, “Axiomatizing dependency parsing using set constraints”, Proc. 6th
Meeting of the Mathematics of Language ( M 0L 6), Orlando, 115-126.

Duchier Denys, Ralph Debusmann, 2001, “Topological dependency trees: A constraint-based
account of linear precedence”, ACL 2001, Toulouse.

Dymetman Marc, Copperman Max, 1996, “Extended dependency structures and their formal
interpretation”, COLING '96, Copenhague, 255-61.

Earley J ., 1970, “An efﬁcient context-free parsing algorithm”, Communications of the ACM,
13.-2, 94-102.

Eisner Jason M., 1996, “Three new probabilistic models for dependency parsing: An
exploration”, COLING '96, Copenhague.

Engel Ulrich, 1992, Deutsche Grammatik.

Floyd Robert, Biegel Richard, 1995, Le langage des machines .' une introduction a la
calculabilite’ et aux langagesformels, International Thomson Publishing, Paris.

Gaifman Ha'1'm, 1965, “Dependency systems and phrase-structure systems”, Information and
Control, 18, 304-337 ; Rand Corporation, 1961, RM-2315.

Garde Paul, “Ordre linéaire et dépendance syntaxique : contribution a une typologie”, Bull. Soc.
Ling. Paris, 72:], 1-26.

Gerdes Kim, Kahane Sylvain, 2001, “Word order in German: A formal dependency grammar
using a topological hierarchy”, ACL 2001, Toulouse.

Gladkij Aleksej V., 1966, Leckii po matematiceskoj linguistike dlja studentov NG U,
Novosibirsk (French transl: Lecons de linguistique mathe’matique, fasc. 1, 1970, Dunod).

Gladkij Aleksej V., 1968, “On describing the syntactic structure of a sentence” (en russe avec
résumé en anglais), Computational Linguistics, 7, Budapest, 21-44.

Gross Maurice 1975, Me’thodes en syntaxe, Hermann, Paris.

Hays David, 1960, “Grouping and dependency theories”, Technical report RM-2646, Rand
Corporation.

Sylvain Kahane

Hays David, 1964, “Dependency theory: A formalism and some observations”, Language,
40:4, 511-525.

Hellwig Peter, 1986, “Dependency Unification Grammar (DUG)”, COLING’86, 195-98.

Hudson Richard, 1988, “Coordination and grammatical relations”, Journal of Linguistics, 24,
303-342.

Hudson Richard, 1990, English Word Grammar, Oxford ,Blackwell.

Iordanskaja Lidija, 1963, “O nekotoryx svojstvax pravil'noj sintaksiéeskoj struktury (na
materiale russkogo jazyka)” [On some Properties of Correct Syntactic Structure (on the Basis of
Russian)], Voprosy Jazykoznanija, 4, 102-12.

Iordanskaja L., Kim M., Kittredge R. I., Lavoie B., Polguere A., 1992, “Generation of
extended bilingual statistical reports”, COLING’92, Nantes, 1019-23.

Iordanskaja L., Mel'éuk I., 2000, “The notion of surface-syntactic relation revisited (Valence-
controlled surface-syntactic relations in French)”, in L.L. Iomdin, L.P. Krysin (ed), Slovo v
tekste i v slovare [Les mots dans le texte et dans le dictionnaire], Jazuki Russkoj Kul'tury,
Moscou, 391-433.

J ackendoff Ray, X-bar Syntax. A Study of Phrase Structure, MIT Press, Cambridge.
J espersen Otto, 1924, Philosophy of Grammar, Londres.

J oshi Aravind, 1987, “Introduction to Tree Adjoining Grammar”, in Manaster Ramer (ed), The
Mathematics of Language, Benjamins, Amsterdam, 87-114.

Kahane Sylvain, 1996, “If HPSG were a dependency grammar ...”, TALN’96, Marseille, 45-
49.

Kahane Sylvain, 1997, “Bubble trees and syntactic representations”, in Becker T., Krieger U.
(eds), Proc. 5th Meeting of the Mathematics of Language (M OL5 ), DFKI, Saarbriicken, 70-76.

Kahane Sylvain, 1998, “Le calcul des voix grammaticales”, Bull. Soc. Ling. de Paris, 93:],
325-48.

Kahane Sylvain, 2000a, “Extractions dans une grammaire de dépendance lexicalisée a bulles”,
T.A.L., 41:1, 211-243.

Kahane Sylvain, 2000b, “Des grammaires formelles pour déﬁnir une correspondance”, TALN
2000, Lausanne, 197-206.

Kahane Sylvain (ed), 2000c, Grammaires de dépendance, T.A.L., 41 :1, Hermes.

Kahane Sylvain, 2001, “What is a natural language and how to describe it? Meaning-Text
approaches in contrast with generative approaches”, Computational Linguistics and Intelligent
Text Processing, Springer, 1-17.

Kahane Sylvain, Mel'éuk Igor, 1999, “La synthese sémantique ou la correspondance entre
graphes sémantiques et arbres syntaxiques. Le cas des phrases a extraction en francais
contemporain”, T.A.L., 40:2, 25-85.

Kahane Sylvain, Nasr Alexis, Rambow Owen, 1998, “Pseudo-projectivity: A polynomially
parsable non-projective dependency grammar”, ACI/COLING '98, Montreal, 646-52.

Grammaires de dépendance formelles et théorie Sens- Texte

Kahane Sylvain, Polguere Alain (eds), 1998, Workshop on Dependency-Based Grammars,
ACI/COLING '98, Montreal.

Kahane Sylvain, Polguere Igor, 2001, “Formal foundation of lexical functions”, in B. Daille,
G. Williams (eds), Workshop on Collocation, ACL 2001, Toulouse.

Kamp Hans, 1981, “Evenements, représentations discursives et référence temporelle”,
Langages, 64, 34-64.

Kamp Hans, Reyle Uwe, 1993, From Discourse to Logic, Kluwer, Dordrecht.

Kasami T., 1963, “An efﬁcient recognition and syntax analysis algorithm for context-free
languages," AFCRL-65-758, Air Force Cambridge Research Laboratory, Bedford, MA.

Kasper R., Kiefer B., Netter K., Vijay-Shanker K., 1995, “Compilation of HPSG to TAG”,
ACL’95.

Keenan Edward, Comrie Bernard, 1977, “Noun phrase accessibility and universal grammar”,
Linguistic Inquiry, 8, 63-100.

Kittredge Richard, Polguere Alain, 1991, “dependency grammars for bilingual text generation:
Inside FoG's stratiﬁcational models”, Proc. Int. Conf on Current Issues in Computational
Linguistics, Penang, 318-30.

Kornai Andreas, Tuza Zsolt, 1992, “narrowness, pathwidth, and their application in natural
language processing”, Disc. Appl. Math, 36, 87-92.

Lavoie Benoit, Rambow Owen, 1997, “RealPro: A fast, portable sentence realizer”, Proc. 5th
Conf On Applied Natural Language Processing (ANLP '97), Washington, 265-68.

Lecerf Yves, 1961, “Une représentation algébrique de la structure des phrases dans diverses
langues natuelles”, C. R. Acad. Sc. Paris, 252, 232-34.

Lecomte Alain, 1992, “Connection grammars: A graph-oriented interpretation”, in Lecomte A.
(ed), Word Order in Categorial Grammar, Adosa, Clermont-Ferrand, 129-48.

Lombardo Vincenzo, 1992, “Incremental dependency parsing", ACL’92, 291-93.

Lombardo Vincenzo, 1996, “An Earley-style parser for dependency grammars”, COLING '96,
Copenhague.

Lombardo Vincenzo, Leonardo Lesmo, 1998, “Formal aspects and parsing issues of
dependency theory”, COLING/ACL’98, Montréal, 787-93.

Lombardo Vincenzo, Lesmo Leonardo, 2000, “A formal theory of dependency syntax with
empty units”, T.A.L., 41:1, 179-210.

Maruyama Hiroshi, 1990a, Constraint Dependency Grammar, Technical Report RT 0044, IBM,
Tokyo.

Maruyama Hiroshi, 1990b, “structural disambiguisation with constraint propagation”, ACL '90,
Pittsburgh, 31-38.

Mel'éuk Igor, 1967, “Ordre des mots en synthese automatique des textes russes”, T.A.
Informations, 8:2, 65-84.

Sylvain Kahane

Mel'éuk Igor, 1974, Opyt teorii linguistideskix modelej “Smysl Tekst”. Semantika, Sintaksis
[Esquisse d'une théorie des modeles linguistiques “Sens-Texte”. Sémantique, Syntaxe],
Moscou, Nauka, 314p.

Mel'éuk Igor, 1988a, Dependency Syntax.‘ Theory and Practice, State Univ. of New York
Press, Albany.

Mel'éuk Igor, 1988b, “Paraphrase et lexique: La Théorie Sens-Texteet le Dictionnaire explicatif
et combinatoire”, in Mel'éuk et al. 1988, 9-58.

Mel'éuk Igor, 1993-2001, Cours de morphologie ge’ne’rale, Vol. 1-5, Presses de l'Univ. de
Montréal / CNRS.

Mel'éuk Igor, 1997, Vers une Linguistique Sens-Texte, Lecon inaugurale au College de France,
College de France, Paris, 78p.

Mel'éuk Igor, 2001, Communicative Organization in Natural Language (The Semantic-
Communicative Structure of Sentences), Benjamins, Amsterdam.

Mel'éuk Igor, Clas André, Polguere Alain, 1995, Introduction a la lexicologie explicative et
combinatoire, Duculot, Paris.

Mel'éuk Igor, Pertsov Nikolaj, 1987, Surface Syntax of English. A Fomtal Model within the
Meaning-Text Framework, Benjamins, Amsterdam.

Mel'éuk Igor, Zolkovsky Alexandr, 1984, Explanatory Combinatorial Dictionary of Modern
Russian, Wiener Slawistischer Almanach, Vienne.

Mel'éuk Igor et al., 1984, 1988, 1992, 1999, Dictionnnaire explicatif et combinatoire du
francais contemporain, Vol. 1, 2, 3, 4, Presses de l'Univ. de Montréal, Montréal.

Menzel Wolfgang, Schroder Ingo, 1998, ‘Decision Procedures for Dependency Parsing Using
Graded Constraints”, Workshop on Processing of Dependency-Based Grammars,
COLING/ACL’98, Montréal, 78-87.

Mertens Piet, 1997, “De la chaine linéaire a la sequence de tons”, T.A.L., 38:], 27-52.

Miliéevié J asmina, 2001, “A short guide to the Meaning-Text linguistic theory”, in A. Gelbukh
(ed), Proc. of CICLing 2000, a paraitre chez Springer.

Miller George A., 1956, “The magical number seven, plus or minus two: Some limits on our
capacity for processing information”, The Psychological Review, 63, 81-97.

Murata Masaki, Uchimoto Kiyotaka, Ma Qing, Isahara Hitoshi, 2001, “Magical number seven
plus or minus two: Syntactic structure recognition in Japanese and English sentences”, in A.
Gelbukh (ed), Computational Linguistics and Intelligent Text Processing, Springer, 43-52.

Nasr Alexis, 1995, “A formalism and a parser for lexicalised dependency grammars”, 4th Int.
Workshop on Parsing Tecnologies, State Univ. of NY Press.

Nasr Alexis, 1996, Un modele de reformulation automatiquefondé sur la The’orie Sens-Texte —
Application aux langues controle’es, These de doctorat, Université Paris 7, Paris.

Neuhaus Peter, Broker Norbert, 1997, “The complexity of recognition of linguistically adequate
dependency grammars”, ACI/EACL’97, Madrid, 337-43.

Grammaires de dépendance formelles et théorie Sens- Texte

O'Regan Kevin, Pynte Joel, 1992, “Regard et lecture”, Sciences cognitives, Courrier du CNR S
n° 79, CNRS, Paris, p. 16.

Owens Jonathan, 1988, The Foundations of Grammar .' An Introduction to Mediaeval Arabic
Grammatical Theory, Benjamins, Amsterdam.

PeSkovskij Aleksandr, 1934, Russkij sintaksis v nauc'nom osves'c'nii [Syntaxe russe : une
approche scientiﬁque], Moscou, Uépedgiz.

Polguere Alain, 1990, Structuration et mise en jeu proce’durale d ’un modele linguistique
de’claratifdans un cadre de ge’ne’ration de texte, These de doctorat, Université de Montréal.

Polguere Alain, 1992, “Remarques sur les réseaux sémantiques Sens-texte’, in A. Clas (ed), Le
mot, les mots, les bons mots, Presses de l'Univ. de Montréal, Montréal.

Polguere Alain, 1998, “Pour un modele stratiﬁé de la lexicalisation en génération de texte”,
T.A.L., 39:2, 57-76.

Pollard Carl, Sag Ivan, 1994, Head-driven Phrase Structure Grammar, Stanford CSLI.
Pustejovsky James, 1995, The Generative Lexicon, MIT Press, Cambridge.

Robinson Jane, 1970, “Dependency structures and transformational rules”, Language, 46, 259-
85.

Sag I., Gazdar G., Wasow T., Wisler S., 1985, “Coordination and how to distinguish
categories”, Natural Language and Linguistic Theory, 3:2, 117-171.

Schabes Yves, 1990, Mathematical and Computational Aspects of Lexicalized Grammars, PhD
thesis, University of Pennsylvania, Philadelphie.

Schroder Ingo, Menzel Wolfgang, Foth Kilian, Schulz Michael, 2000, “Modeling dependency
grammar with restricted constraints”, T.A.L., 41 :1 , 113-44.

Schubert Klaus, 1987, Metataxis: Contrastive Dependency Syntax for Machine Translation,
Foris, Dordrecht.

Sgall Petr, Hajicova Eva, Panenova J armila, 1986, The Meaning of the Sentence in Its Semantic
and Pragmatic Aspects, Reidel, Dordrecht.

Sleator Daniel, Temperley Davy, 1993, “Parsing English with a Link Grammar", Third Int.
Workshop on Parsing Technologies ; Carnegie Mellon Univ. Comp. Sc. Techn. Report CMU-
CS-91-196, 1991.

Tesniere Lucien, 1934, “Comment construire une syntaxe”, Bulletin de la Faculte’ des Lettres de
Strasbourg, 7, 12”” année, 219-229.

T esniere Lucien, 1959, Elements de syntaxe structurale, Kincksieck, Paris.

Tomita Masaru, 1988, “Graph structured stack and natural language parsing”, ACL '88,
Buffalo.

Vergne Jacques, 2000, Etude et modélisation de la syntaxe des langues a l ’aide de l ’ordinateur -
Analyse syntaxique automatique non combinatoire, These d'HDR, Université de Caen.

Vijay-Shanker K., Yves Schabes, 1992, “Structure sharing in Lexicalized TAG”, COLING’92.

Sylvain Kahane

Wanner Leo (ed), 1996, Lexical Functions in Lexicography and Natural Language Processing,
Benjamins, Amsterdam.

Weiss Daniel, 1999, “Sowjetische Sprachmodelle und ihre Weiter fiirhung”, Handbuch des
sprachwissenschaftlich Russistik und ihrer Grenzdisziplinen, Harrassowitz, 973-09.

XTAG Research Group, 1995, “A Lexicalized Tree Adjoining Grammar for English”, technical
Report IRCS 95-03, University of Pennsylvania (version Inise a jour sur le web).

Yngve Victor H., 1960, “A model and an hypothesis for language structure”, The American
Philosophical Society, I 04:5, 444-66.

Yngve Victor H., 1961, “The Depth Hypothesis”, Proceedings of Symposia in Applied
Mathematics, Vol. I2.‘ Structure of Language and its Mathematical Aspects, American
Mathematical Society, Providence, 130-138.

Younger D.H., 1967, Recognition of context-free languages in time n3”, Information and
Control, I0.'2, 189-208.

Zolkovskij Aleksandr, Mel'éuk Igor, 1965, “O vozmoinom metode i instrumentax
semantiéeskogo sinteza” [Sur une méthode possible et des outils pour la synthese sémantique
(de textes)], Naucvno-texnicveskaja informacija [Scientiﬁc and Technological Information], 6, 23-
28.

Zolkovskij Aleksandr, Mel'éuk Igor, 1967, “O semantiéeskom sinteze” [Sur la synthese
sémantique (de textes)], Problemy Kybernetiki [Problemes de Cybemétique], I9, 177-238.
[trad. franc. : 1970, TA. Information, 2, 1-85.]

Zwicky Arnold, 1985, “Heads”, Journal of Linguistics, 21, 1-29.

