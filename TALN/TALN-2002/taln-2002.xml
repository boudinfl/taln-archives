<?xml version="1.0" encoding="UTF-8"?>
<conference>
	<edition>
		<acronyme>TALN'2002</acronyme>
		<titre>9ème conférence sur le Traitement Automatique des Langues Naturelles</titre>
		<ville>Nancy</ville>
		<pays>France</pays>
		<dateDebut>2002-06-24</dateDebut>
		<dateFin>2002-06-27</dateFin>
		<presidents>
			<nom>Jean-Marie Pierrel</nom>
		</presidents>
		<typeArticles>
			<type id="long">Papiers longs</type>
			<type id="poster">Posters</type>
			<type id="tutoriel">Tutoriels</type>
			<type id="invite">Conférences invitées</type>
		</typeArticles>
		<statistiques>
			<acceptations id="long" soumissions="62">28</acceptations>
			<acceptations id="poster" soumissions="62">13</acceptations>
		</statistiques>
		<siteWeb>http://www.loria.fr/projets/JEP-TALN/TALN/</siteWeb>
		<meilleurArticle>
			<!-- <articleId></articleId> -->
		</meilleurArticle>
	</edition>
	<articles>
		<article id="taln-2002-invite-001" session="Conférence invitée inaugurale">
			<auteurs>
				<auteur>
					<nom>Amr Helmy Ibrahim</nom>
					<email>amr.ibrahim1@libertysurf.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université de Franche-Comté</affiliation>
			</affiliations>
			<titre>Maurice Gross : une refondation de la linguistique au crible de l'analyse automatique</titre>
			<type>invite</type>
			<pages>5-30</pages>
			<resume>Qu'il s'adresse à un Prix Nobel ou à un étudiant de première année Maurice Gross ne craignait jamais d'être trop élémentaire. C'était à chaque fois comme si, entreprenant d'écrire un livre de mathématiques il ne pouvait rien démontrer avant d'avoir reconstruit les données les plus primitives du calcul et du raisonnement qui l'accompagne. Et il arrivait souvent que ceux qui l'écoutaient ou le lisaient pour la première fois, manquant par leur impatience le détail qui faisait que ses évidences n'avaient rien d'évident, s'imaginent qu'il les prenait pour des imbéciles. Parce qu'il avait l'expression littéraire et philosophique, la langue du style, la forme de l'émotion, dans les tripes – il pouvait citer sans discontinuer des poètes français ou anglais du XVIe siècle à nos jours et discuter longuement des formulations exactes d'un René Descartes ou d'un Charles Sanders Pierce, deux de ses deux philosophes préférés - il n'a jamais fait recette auprès des littéraires, des psycho-socios, des sémio-machins, des politiques et des pouvoirs académiques chez qui le raccourci, la connotation, le clin d'oeil, dont tout le monde a oublié sur quelles complicités exactes ils se fondent, tiennent lieu de découverte quand ce n'est pas de pensée. La complexité qui l'intéressait était d'une tout autre nature et autrement plus complexe. Elle avait pour horizon la phrase simple. Même pas l'énoncé, juste la phrase. Et simple c'est-à-dire constituée d'une seule proposition. Contrairement à ceux qui voyaient dans les processus de récursivité propositionnelle – relatives notamment -- une source de complexité et de créativité, il y voyait un mécanisme très banal1. La vraie complexité, celle qu'aucune machine construite à ce jour ne contrôle vraiment, il l'a exposée avec une simplicité désarmante en un peu moins de deux pages au début de Méthodes en syntaxe (1975: 17-19) dans le chapitre intitulé La créativité du langage. Elle porte sur les combinaisons possibles ou impossibles au sein d'une structure de neuf constituants formant une phrase simple. Mais ces possibilités "limitées à 1050 cas" et qui peuvent donc "être considérées comme intuitivement infinies" sans qu'il soit nécessaire "de faire appel à des mécanismes infinis pour rendre compte de leur richesse" ne sont qu'un horizon virtuel.</resume>
			<mots_cles></mots_cles>
			<title></title>
			<abstract></abstract>
			<keywords></keywords>
		</article>
		<article id="taln-2002-long-001" session="session plénière de TALN">
			<auteurs>
				<auteur>
					<nom>Mathieu Delichère</nom>
					<email>mathieu@amoweba.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Daniel Memmi</nom>
					<email>memmi@imag.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Amoweba 1 ave Berthollet, 74000 Annecy (France)</affiliation>
				<affiliation affiliationId="2">Leibniz-Imag 46 ave Félix Viallet, 38000 Grenoble (France)</affiliation>
			</affiliations>
			<titre>Analyse Factorielle Neuronale pour Documents Textuels</titre>
			<type>long</type>
			<pages>33-42</pages>
			<resume>En recherche documentaire, on représente souvent les documents textuels par des vecteurs lexicaux de grande dimension qui sont redondants et coûteux. Il est utile de réduire la dimension des ces représentations pour des raisons à la fois techniques et sémantiques. Cependant les techniques classiques d'analyse factorielle comme l'ACP ne permettent pas de traiter des vecteurs de très grande dimension. Nous avons alors utilisé une méthode adaptative neuronale (GHA) qui s'est révélée efficace pour calculer un nombre réduit de nouvelles dimensions représentatives des données. L'approche nous a permis de classer un corpus réel de pages Web avec de bons résultats.</resume>
			<mots_cles>Recherche documentaire, modèle vectoriel, réduction de dimension, analyse factorielle, ACP, GHA, réseaux de neurones</mots_cles>
			<title></title>
			<abstract>For document retrieval purposes, documents are often represented by high-dimensional lexical vectors, which are costly and redundant. Reducing vector dimensionality is then useful for both technical and semantic reasons. Classical data analysis methods such as PCA cannot unfortunately process vectors of very high dimension. We have used instead an adaptive neural network technique, the Generalized Hebbian Algorithm (GHA), which makes it possible to reduce high-dimension spaces. This approach allowed us to cluster areal end-user corpus of Web pages with very significant results.</abstract>
			<keywords>Information retrieval, vector-space model, dimensionality reduction, data analysis, PCA, GHA, neural networks</keywords>
		</article>
		<article id="taln-2002-long-002" session="session plénière de TALN">
			<auteurs>
				<auteur>
					<nom>Philippe Langlais</nom>
					<email>felipe@iro.umontreal.ca</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">RALI/DIRO - Université de Montréal C.P. 6128, succursale Centre-ville Montréal (Québec) Canada, H3C 3J7</affiliation>
			</affiliations>
			<titre>Ressources terminologiques et traduction probabiliste: premiers pas positifs vers un système adaptatif</titre>
			<type>long</type>
			<pages>43-52</pages>
			<resume>Cette dernière décennie a été le témoin d’importantes avancées dans le domaine de la traduction statistique (TS). Aucune évaluation fine n’a cependant été proposée pour mesurer l’adéquation de l’approche statistique dans un contexte applicatif réel. Dans cette étude, nous étudions le comportement d’un engin de traduction probabiliste lorsqu’il traduit un texte de nature très éloignée de celle du corpus utilisé lors de l’entraînement. Nous quantifions en particulier la baisse de performance du système et développons l’idée que l’intégration de ressources terminologiques dans le processus est une solution naturelle et salutaire à la traduction. Nous décrivons cette intégration et évaluons son potentiel.</resume>
			<mots_cles>Traduction statistique, adapatabilité, terminologie</mots_cles>
			<title></title>
			<abstract>The past decade witnessed exciting work in the field of Statistical Machine Translation (SMT). However, accurate evaluation of its potential in a real-life context is still a questionable issue. In this study, we investigate the behavior of a SMT engine faced with a corpus far different from the one it has been trained on. We show that terminological databases are obvious ressources that should be used to boost the performance of a statistical engine. We propose and evaluate a way of integrating terminology into a SMT engine which yields a significant reduction in word error rate.</abstract>
			<keywords>Statistical machine translation, adaptativity, terminology</keywords>
		</article>
		<article id="taln-2002-long-003" session="session plénière de TALN">
			<auteurs>
				<auteur>
					<nom>Pierre Zweigenbaum</nom>
					<email>pz@biomath.jussieu.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Natalia Grabar</nom>
					<email>ngr@biomath.jussieu.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">DIAM —STIM/DSI, Assistance Publique – Hôpitaux de Paris &amp; Département de Biomathématiques, Université Paris 6</affiliation>
			</affiliations>
			<titre>Accentuation de mots inconnus : application au thesaurus biomédical MeSH</titre>
			<type>long</type>
			<pages>53-62</pages>
			<resume>Certaines ressources textuelles ou terminologiques sont écrites sans signes diacritiques, ce qui freine leur utilisation pour le traitement automatique des langues. Dans un domaine spécialisé comme la médecine, il est fréquent que les mots rencontrés ne se trouvent pas dans les lexiques électroniques disponibles. Se pose alors la question de l’accentuation de mots inconnus : c’est le sujet de ce travail. Nous proposons deux méthodes d’accentuation de mots inconnus fondées sur un apprentissage par observation des contextes d’occurrence des lettres à accentuer dans un ensemble de mots d’entraînement, l’une adaptée de l’étiquetage morphosyntaxique, l’autre adaptée d’une méthode d’apprentissage de règles morphologiques. Nous présentons des résultats expérimentaux pour la lettre e sur un thesaurus biomédical en français : le MeSH. Ces méthodes obtiennent une précision de 86 à 96 % (+-4 %) pour un rappel allant de 72 à 86 %.</resume>
			<mots_cles>Réaccentuation, mots inconnus, étiquetage, langue de spécialité, médecine</mots_cles>
			<title></title>
			<abstract>Some textual or terminological resources are written without diacritic marks, which hinders their use for natural language processing. Moreover, in a specialized domain such as medicine, all words are not always found in the available lexicons. The issue of accenting unknown words then arises. This is the subject of the present work. We propose two accentuation methods which both rely on a learning process, based on the observation of the contexts of occurrence of the accentuable letters in a training corpus. One is adapted from a part-of-speech tagging method, the other from a method for learning morphological rules. We present experimental results for letter e on a French biomedical thesaurus: the MeSH. These methods obtain a precision which ranges from 86 to 96% (+-4 %) and a recall from 72 to 86%.</abstract>
			<keywords>Reaccenting, unknown words, tagging, specialized language, medicine</keywords>
		</article>
		<article id="taln-2002-long-004" session="Syntaxe">
			<auteurs>
				<auteur>
					<nom>Jacques Vergne</nom>
					<email>Jacques.Vergne@info.unicaen.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">GREYC - Université de Caen BP 5186 - 14032 Caen cedex</affiliation>
			</affiliations>
			<titre>Une méthode pour l'analyse descendante et calculatoire de corpus multilingues : application au calcul des relations sujet-verbe</titre>
			<type>long</type>
			<pages>63-74</pages>
			<resume>Nous présentons une méthode d'analyse descendante et calculatoire. La démarche d'analyse est descendante du document à la proposition, en passant par la phrase. Le prototype présenté prend en entrée des documents en anglais, français, italien, espagnol, ou allemand. Il segmente les phrases en propositions, et calcule les relations sujet-verbe dans les propositions. Il est calculatoire, car il exécute un petit nombre d'opérations sur les données. Il utilise très peu de ressources (environ 200 mots et locutions par langue), et le traitement de la phrase fait environ 60 Ko de Perl, ressources lexicales comprises. La méthode présentée se situe dans le cadre d'une recherche plus générale du Groupe Syntaxe et Ingénierie Multilingue du GREYC sur l'exploration de solutions minimales et multilingues, ajustées à une tâche donnée, exploitant peu de propriétés linguistiques profondes, la généricité allant de pair avec l'efficacité.</resume>
			<mots_cles>analyse syntaxique, analyse descendante, analyse calculatoire, corpus multilingues</mots_cles>
			<title></title>
			<abstract>We present a method for top-down and calculatory parsing. The prototype we present is top-down from the document to the clause, through the sentence. Its inputs are documents in English, French, Italian, Spanish, or German. It tokenises sentences into clauses, and computes subject-verb links inside clauses. It is calculatory, as it executes few operations on data. It uses very few resources (about 200 words or locutions per natural language), and the sentence processing size is about 60 Kb Perl, including lexical resources. This method takes place in the frame of more general researches of the "Groupe Syntaxe et Ingénierie Multilingue du GREYC" into exploring minimal and multilingual solutions, close fitted to a given task, exploiting few deep linguistic properties, presuming that genericity implies efficiency.</abstract>
			<keywords>top-down parsing, calculatory parsing, multilingual corpora</keywords>
		</article>
		<article id="taln-2002-long-005" session="Syntaxe">
			<auteurs>
				<auteur>
					<nom>Didier Bourigault</nom>
					<email>didier.bourigault@univ-tlse2.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Equipe de Recherche en Syntaxe et Sémantique CNRS – Université Toulouse le Mirail Maison de la Recherche 5, allées Antonio Machado 31058 Toulouse Cedex 1</affiliation>
			</affiliations>
			<titre>UPERY : un outil d'analyse distributionnelle étendue pour la construction d’ontologies à partir de corpus</titre>
			<type>long</type>
			<pages>75-84</pages>
			<resume>Nous présentons un module mettant en oeuvre une méthode d'analyse distributionnelle dite "étendue". L'analyseur syntaxique de corpus SYNTEX effectue l'analyse en dépendance de chacune des phrases du corpus, puis construit un réseau de mots et syntagmes, dans lequel chaque syntagme est relié à sa tête et à ses expansions. A partir de ce réseau, le module d'analyse distributionnelle UPERY construit pour chaque terme du réseau l'ensemble de ses contextes syntaxiques. Les termes et les contextes syntaxiques peuvent être simples ou complexes. Le module rapproche ensuite les termes, ainsi que les contextes syntaxiques, sur la base de mesures de proximité distributionnelle. L'ensemble de ces résultats est utilisé comme aide à la construction d'ontologie à partir de corpus spécialisés.</resume>
			<mots_cles>analyse syntaxique automatique, analyse distributionnelle, corpus, ontologie, terminologie</mots_cles>
			<title></title>
			<abstract>We present a software that implements a method of "extended" distributional analysis. The corpus syntactic analyser SYNTEX yields a dependency syntactic analyse of each sentence of the corpus. It builds a network of words and phrases in which each phrase is connected to its head and its expansion. The distributional analysis module UPERY relies on this network to associate to each term in the network a set of syntactic contexts. Syntactic contexts as well as terms may be simple or complex. The UPERY module calculates distributional proximities between terms as well as between contexts. The results are used for the building of ontological resources from specialized corpora.</abstract>
			<keywords>parsing, distributional analysis, corpus, ontology, terminology</keywords>
		</article>
		<article id="taln-2002-long-006" session="Syntaxe">
			<auteurs>
				<auteur>
					<nom>Éric Villemonte De La Clergerie</nom>
					<email>Eric.De_La_Clergerie@inria.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">ATOLL - INRIA Domaine de Voluceau Rocquencourt, B.P. 105,78153 Le Chesnay (France)</affiliation>
			</affiliations>
			<titre>Construire des analyseurs avec DyALog</titre>
			<type>long</type>
			<pages>85-94</pages>
			<resume>Cet article survole les fonctionnalités offertes par le système DyALog pour construire des analyseurs syntaxiques tabulaires. Offrant la richesse d’un environnement de programmation en logique, DyALog facilite l’écriture de grammaires, couvre plusieurs formalismes et permet le paramétrage de stratégies d’analyse.</resume>
			<mots_cles>Analyse Syntaxique, Tabulation, DCG, TAG, RCG, BMG, TFS</mots_cles>
			<title></title>
			<abstract>This paper is a survey of the functionalities provided by system DyALog to build tabular parsers. Providing the expressiveness of logic programming, DyALog eases the development of grammars, covers several linguistic formalisms, and allows the parametrization of parsing strategies.</abstract>
			<keywords>Parsing, Tabulation, DCG, TAG, RCG, BMG, TFS</keywords>
		</article>
		<article id="taln-2002-long-007" session="Syntaxe">
			<auteurs>
				<auteur>
					<nom>Antoine Rozenknop</nom>
					<email>antoine.rozenknop@epfl.ch</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">I&amp;C-IIF-LIA - EPFL CH-1015 Lausanne, Suisse</affiliation>
			</affiliations>
			<titre>Une grammaire hors-contexte valuée pour l’analyse syntaxique</titre>
			<type>long</type>
			<pages>95-104</pages>
			<resume>Les grammaires hors-contexte stochastiques sont exploitées par des algorithmes particulièrement efficaces dans des tâches de reconnaissance de la parole et d’analyse syntaxique. Cet article propose une autre probabilisation de ces grammaires, dont les propriétés mathématiques semblent intuitivement plus adaptées à ces tâches que celles des SCFG (Stochastique CFG), sans nécessiter d’algorithme d’analyse spécifique. L’utilisation de ce modèle en analyse sur du texte provenant du corpus Susanne peut réduire de 33% le nombre d’analyses erronées, en comparaison avec une SCFG entraînée dans les mêmes conditions.</resume>
			<mots_cles>Syntaxe, linguistique mathématique, apprentissage statistique, SCFG, Gibbs</mots_cles>
			<title></title>
			<abstract>Weighted Context-Free Grammars can be used for speech recognition or syntactic analysis thanks to especially efficient algorithms. In this paper, we propose an instanciation of such a grammar, whose mathematical properties are intuitively more suitable for those tasks than SCFG’s (Stochastic CFG), without requiring specific analysis algorithms. Results on Susanne text show that up to 33% of analysis errors made by a SCFG can be avoided with this model.</abstract>
			<keywords>statistical learning, Syntax, Context-free grammars</keywords>
		</article>
		<article id="taln-2002-long-008" session="Extraction d'information, similarité textuelle">
			<auteurs>
				<auteur>
					<nom>Fabrice Even</nom>
					<email>even@irin.univ-nantes.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Chantal Enguehard</nom>
					<email>enguehard@irin.univ-nantes.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Institut de Recherche en Informatique de Nantes Université de Nantes Faculté des Sciences et des Techniques 2 rue de la Houssinière, BP 92208 44322 NANTES cedex 3, France</affiliation>
			</affiliations>
			<titre>Extraction d’informations à partir de corpus dégradés</titre>
			<type>long</type>
			<pages>105-115</pages>
			<resume>Nous présentons une méthode automatique d’extraction d’information à partir d’un corpus mono-domaine de mauvaise qualité, sur lequel il est impossible d’appliquer les méthodes classiques de traitement de la langue naturelle. Cette approche se fonde sur la construction d’une ontologie semi-formelle (modélisant les informations contenues dans le corpus et les relations entre elles). Notre méthode se déroule en trois phases : 1) la normalisation du corpus, 2) la construction de l’ontologie, et 3) sa formalisation sous la forme d’une grammaire. L’extraction d’information à proprement parler exploite un étiquetage utilisant les règles définies par la grammaire. Nous illustrons notre démarche d’une application sur un corpus bancaire.</resume>
			<mots_cles>Extraction d’information, modélisation, construction d’ontologie, corpus dégradés</mots_cles>
			<title></title>
			<abstract>We present an information extraction automatic method from poor quality specific-domain corpus (with which it is impossible to apply classical natural language methods). This approach is based on building a semi-formal ontology in order to modelise information present in the corpus and their relation. Our method happens in three stage : 1) corpus normalisation, 2) ontology building and 3) model formalisation in grammar. The information extraction itself is made by a tagging process using grammar rules. We illustrate our approach by an application working on a bank corpus.</abstract>
			<keywords>Information extraction, modelling, building ontology, poor quality corpus</keywords>
		</article>
		<article id="taln-2002-long-009" session="Extraction d'information, similarité textuelle">
			<auteurs>
				<auteur>
					<nom>Brigitte Bigi</nom>
					<email>bigi@loria.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Kamel Smaïli</nom>
					<email>smaili@loria.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LORIA - Université Henri Poincaré Campus Scientifique, BP 239, 54506 Vandoeuvre-lès-Nancy</affiliation>
			</affiliations>
			<titre>Identification thématique hiérarchique : Application aux forums de discussions</titre>
			<type>long</type>
			<pages>116-124</pages>
			<resume>Les modèles statistiques du langage ont pour but de donner une représentation statistique de la langue mais souffrent de nombreuses imperfections. Des travaux récents ont montré que ces modèles peuvent être améliorés s’ils peuvent bénéficier de la connaissance du thème traité, afin de s’y adapter. Le thème du document est alors obtenu par un mécanisme d’identification thématique, mais les thèmes ainsi traités sont souvent de granularité différente, c’est pourquoi il nous semble opportun qu’ils soient organisés dans une hiérarchie. Cette structuration des thèmes implique la mise en place de techniques spécifiques d’identification thématique. Cet article propose un modèle statistique à base d’unigrammes pour identifier automatiquement le thème d’un document parmi une arborescence prédéfinie de thèmes possibles. Nous présentons également un critère qui permet au modèle de donner un degré de fiabilité à la décision prise. L’ensemble des expérimentations a été réalisé sur des données extraites du groupe ’fr’ des forums de discussion.</resume>
			<mots_cles>Identification thématique, modèles de langage, unigrammes</mots_cles>
			<title></title>
			<abstract>Statistical language modeling attempts to capture the regularities of natural language. The most accurate natural language processing systems still suffer from several shortcomings due to the complexity of natural language and from the weakness of the current language models. It is commonly conjectured that they should benefit from topic adaptation. The topic of the document is then obtained by a topic identification mechanism, but topics thus treated are often of different granularity. This is the reason why it seems appropriate to organize them in a hierarchy. This topic organization implies a development of specific techniques for topic identification. This paper proposes a statistical model based on unigrams to automatically identify the topic of a document among a tree structure of possible topics. We also present a criterion which reflects the degree of reliability of the decision. Experiments were carried out on data extracted from the French newsgroup ’fr’.</abstract>
			<keywords>Topic identification, language modeling, unigrams</keywords>
		</article>
		<article id="taln-2002-long-010" session="Extraction d'information, similarité textuelle">
			<auteurs>
				<auteur>
					<nom>Didier Schwab</nom>
					<email>schwab@lirmm.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Mathieu Lafourcade</nom>
					<email>lafourca@lirmm.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Violaine Prince</nom>
					<email>prince@lirmm.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIRMM Laboratoire d’informatique, de Robotique et de Microélectronique de Montpellier MONTPELLIER - FRANCE</affiliation>
			</affiliations>
			<titre>Vers l’apprentissage automatique, pour et par les vecteurs conceptuels, de fonctions lexicales. L’exemple de l’antonymie</titre>
			<type>long</type>
			<pages>125-134</pages>
			<resume>Dans le cadre de recherches sur le sens en traitement automatique du langage, nous nous concentrons sur la représentation de l’aspect thématique des segments textuels à l’aide de vecteurs conceptuels. Les vecteurs conceptuels sont automatiquement appris à partir de définitions issues de dictionnaires à usage humain (Schwab, 2001). Un noyau de termes manuellement indexés est nécessaire pour l’amorçage de cette analyse. Lorsque l’item défini s’y prête, ces définitions sont complétées par des termes en relation avec lui. Ces relations sont des fonctions lexicales (Mel’cuk and al, 95) comme l’hyponymie, l’hyperonymie, la synonymie ou l’antonymie. Cet article propose d’améliorer la fonction d’antonymie naïve exposée dans (Schwab, 2001) et (Schwab and al, 2002) grâce à ces informations. La fonction s’auto-modifie, par révision de listes, en fonction des relations d’antonymie avérées entre deux items. Nous exposons la méthode utilisée, quelques résultats puis nous concluons sur les perspectives ouvertes.</resume>
			<mots_cles>représentation thématique, vecteurs conceptuels, antonymie, apprentissage automatique, fonctions lexicales</mots_cles>
			<title></title>
			<abstract>In the framework of research in the field of meaning representation, we focus our attention on thematic aspects of textual segments represented with conceptual vectors. Conceptual vectors are automatically learned and refined by analyzing human usage dictionary definitions (Schwab, 2001). A kernel of terms manually indexed is needed for bootstrapping this analysis. When possible, these definitions are completed with related terms. The considered relations are typically instances of lexical functions (Mel’cuk and al, 95) as hyponymy, hyperonymy, synonymy and antonymy. This paper is an experimented proposal to take advantage of these information to enhance the naive antonymy function as proposed in (Schwab, 2001) and (Schwab and al, 2002) . The function can self-adjust, by modifications of antonym lists as extracted or induced from lexical data. We expose the overall method behind this process, some experimental results and conclude on some open perspectives.</abstract>
			<keywords>thematic representation, conceptuals vectors, antonymy, potential antonymy mesure, automatic learning, lexical functions</keywords>
		</article>
		<article id="taln-2002-long-011" session="Extraction d'information, similarité textuelle">
			<auteurs>
				<auteur>
					<nom>Romaric Besançon</nom>
					<email>Romaric.Besancon@epfl.ch</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Martin Rajman</nom>
					<email>Martin.Rajman@epfl.ch</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Laboratoire d’Intelligence Artificielle Faculté Informatique et Communications Ecole Polytechnique Fédérale de Lausanne, (IN) Ecublens 1015 Lausanne</affiliation>
			</affiliations>
			<titre>Filtrages syntaxiques de co-occurrences pour la représentation vectorielle de documents</titre>
			<type>long</type>
			<pages>135-144</pages>
			<resume>L’intégration de co-occurrences dans les modèles de représentation vectorielle de documents s’est avérée une source d’amélioration de la pertinence des mesures de similarités textuelles calculées dans le cadre de ces modèles (Rajman et al., 2000; Besançon, 2001). Dans cette optique, la définition des contextes pris en compte pour les co-occurrences est cruciale, par son influence sur les performances des modèles à base de co-occurrences. Dans cet article, nous proposons d’étudier deux méthodes de filtrage des co-occurrences fondées sur l’utilisation d’informations syntaxiques supplémentaires. Nous présentons également une évaluation de ces méthodes dans le cadre de la tâche de la recherche documentaire.</resume>
			<mots_cles>Similarités textuelles, repréesentation vectorielle de textes, sémantique distributionnelle, contexte de co-occurrence</mots_cles>
			<title></title>
			<abstract>The integration of co-occurrence information in the vector-space representation models for texts has proven to improve the relevance of textual similarities (Rajman et al., 2000; Besanc¸on, 2001). In this framework, the definition of what is the context considered for the co-occurrences is an important issue. In this paper, we provide the study of two methods for the filtering of the co-occurrences, both using additional syntactic information. We also present an evaluation of these methods in the framework of information retrieval.</abstract>
			<keywords>Textual similarities, vector space representation, distributional semantics, co-occurrence context</keywords>
		</article>
		<article id="taln-2002-long-012" session="Modèles de langage, détection de thèmes">
			<auteurs>
				<auteur>
					<nom>Armelle Brun</nom>
					<email>brun@loria.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Kamel Smaïli</nom>
					<email>smaili@loria.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Jean-Paul Haton</nom>
					<email>jph@loria.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LORIA BP 239 54506 Vandoeuvre-Lès-Nancy, France</affiliation>
			</affiliations>
			<titre>WSIM : une méthode de détection de thème fondée sur la similarité entre mots</titre>
			<type>long</type>
			<pages>145-154</pages>
			<resume>L’adaptation des modèles de langage dans les systèmes de reconnaissance de la parole est un des enjeux importants de ces dernières années. Elle permet de poursuivre la reconnaissance en utilisant le modèle de langage adéquat : celui correspondant au thème identifié. Dans cet article nous proposons une méthode originale de détection de thème fondée sur des vocabulaires caractéristiques de thèmes et sur la similarité entre mots et thèmes. Cette méthode dépasse la méthode classique (TFIDF) de 14%, ce qui représente un gain important en terme d’identification. Nous montrons également l’intérêt de choisir un vocabulaire adéquat. Notre méthode de détermination des vocabulaires atteint des performances 3 fois supérieures à celles obtenues avec des vocabulaires construits sur la fréquence des mots.</resume>
			<mots_cles>Reconnaissance de la parole, modélisation statistique du langage, détection de thème, information mutuelle, similarité</mots_cles>
			<title></title>
			<abstract>Speech recognition systems benefit from statistical language model adaptation, which is currently one of the most important challenge. This adaptation may go through the use of a particular language model : the one of the topic identified. In this article, a new and original topic identification method is presented, it is based on the similarity between words and topics. The performance of this method overcomes the one of reference, the TFIDF. The increase is 14% of topic identification. The importance of the choice of topic vocabularies is also put forward. A judicious way to create them, instead of choosing the most probable words, makes performance triple.</abstract>
			<keywords>Automatic speech recognition, statistical language modeling, topic detection, mutual information, similarity</keywords>
		</article>
		<article id="taln-2002-long-013" session="Modèles de langage, détection de thèmes">
			<auteurs>
				<auteur>
					<nom>Olivier Ferret</nom>
					<email>olivier.ferret@cea.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">CEA – LIST 92265 Fontenay-aux-Roses Cedex</affiliation>
			</affiliations>
			<titre>Segmenter et structurer thématiquement des textes par l’utilisation conjointe de collocations et de la récurrence lexicale</titre>
			<type>long</type>
			<pages>155-165</pages>
			<resume>Nous exposons dans cet article une méthode réalisant de façon intégrée deux tâches de l’analyse thématique : la segmentation et la détection de liens thématiques. Cette méthode exploite conjointement la récurrence des mots dans les textes et les liens issus d’un réseau de collocations afin de compenser les faiblesses respectives des deux approches. Nous présentons son évaluation concernant la segmentation sur un corpus en français et un corpus en anglais et nous proposons une mesure d’évaluation spécifiquement adaptée à ce type de systèmes.</resume>
			<mots_cles>Analyse du discours, analyse thématique, segmentation, détection de liens thématiques</mots_cles>
			<title></title>
			<abstract>We present in this paper a method for achieving in an integrated way two tasks of topic analy-sis: segmentation and link detection. This method combines the lexical recurrence in texts and the relations from a collocation network to compensate for the respective weaknesses of the two approaches. We report its evaluation for segmentation on a corpus in French and another in English and we propose an evaluation measure that specifically suits that kind of systems.</abstract>
			<keywords>Discourse analysis, topic analysis, topic segmentation, link detection</keywords>
		</article>
		<article id="taln-2002-long-014" session="Prosodie, parole spontanée">
			<auteurs>
				<auteur>
					<nom>Jeanne Villaneau</nom>
					<email>Jeanne.Villaneau@univ-ubs.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Jean-Yves Antoine</nom>
					<email>Jean-Yves.Antoine@univ-ubs.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Olivier Ridoux</nom>
					<email>Olivier.Ridoux@irisa.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">VALORIA —Université de Bretagne Sud 4, rue Jean Zay, 56100 LORIENT</affiliation>
				<affiliation affiliationId="2">IRISA / IFSIC — Université de Rennes 1 Campus universitaire de Beaulieu, 35042 RENNES cedex</affiliation>
			</affiliations>
			<titre>LOGUS : un système formel de compréhension du français parlé spontané-présentation et évaluation</titre>
			<type>long</type>
			<pages>166-174</pages>
			<resume>Le système de compréhension présenté dans cet article propose une approche logique et lexicalisée associant syntaxe et sémantique pour une analyse non sélective et hors-cadres sémantiques prédéterminés. L’analyse se déroule suivant deux grandes étapes ; un chunking est suivi d’une mise en relation des chunks qui aboutit à la construction de la représentation sémantique finale : formule logique ou graphe conceptuel. Nous montrons comment le formalisme a dû évoluer pour accroître l’importance de la syntaxe et améliorer la généricité des règles. Malgré l’utilisation d’une connaissance pragmatico-sémantique liée à l’application, la spécificité du système est circonscrite au choix des mots du lexique et à la définition de cette connaissance. Les résultats d’une campagne d’évaluation ont mis en évidence une bonne tolérance aux inattendus et aux phénomènes complexes, prouvant ainsi la validité de l’approche.</resume>
			<mots_cles>langue parlée spontanée, compréhension automatique, méthodes formelles</mots_cles>
			<title></title>
			<abstract>We present in this paper a speech understanding system with a lexicalized logical approach combining syntax and pragmatic knowledge, but without selective methods or predefined semantic frames. The analysis is split into two phases: a chunking phase is followed with a second phase in which different chunks are combined in order to obtain the semantic representation of the sentence: logical formula or conceptual graph. We show how we have changed the formalism to increase the part of the syntax and to obtain generic rules. Despite the use of a semantic knowledge linked to the application, the specificity of the system is limited to the lexicon and to the definition of this knowledge. The results of an evaluation campaign have showed a good tolerance with the spontaneous spoken utterances and with the complex phenomena, and so, they have showed the value of this approach.</abstract>
			<keywords>spontaneous spoken language, automated understanding, formal methods</keywords>
		</article>
		<article id="taln-2002-long-015" session="Prosodie, parole spontanée">
			<auteurs>
				<auteur>
					<nom>Estelle Campione</nom>
					<email>Estelle.Campione@up.univ-aix.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Jean Véronis</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Equipe DELIC, Université de Provence 29, Avenue Robert Schuman 13621 Aix-en-Provence Cedex 1</affiliation>
			</affiliations>
			<titre>Etude des relations entre pauses et ponctuations pour la synthèse de la parole à partir de texte</titre>
			<type>long</type>
			<pages>175-184</pages>
			<resume>Nous présentons dans cette communication la première étude à grande échelle de la relation entre pauses et ponctuations, à l’aide de l’analyse de plusieurs milliers de pauses dans un corpus comportant près de 5 heures de parole lue en cinq langues, faisant intervenir 50 locuteurs des deux sexes. Nos résultats remettent en cause l’idée reçue de rapports bi-univoques entre pauses et ponctuations. Nous mettons en évidence une proportion importante de pauses hors ponctuation, qui délimitent des constituants, mais aussi un pourcentage élevé de ponctuations faibles réalisées sans pauses. Nous notons également une très grande variabilité inter-locuteur, ainsi que des différences importantes entre langues. Enfin, nous montrons que la durée des pauses est liée au sexe des locuteurs.</resume>
			<mots_cles>Synthèse de la parole à partir de textes, pauses, ponctuation</mots_cles>
			<title></title>
			<abstract>We present in this paper the first large-scale study of pause-punctuation relationships, based on the analysis of several thousand pauses in a corpus consisting of nearly 5 hours of read speech involving 50 male and female readers in five languages. Our results call into question the generally accepted idea of a one-to-one relationship between pauses and punctuation. We observe a large proportion of pauses outside punctuations (which mark phrases), but also a high percentage of weak punctuations with no pause. We also note a very high interspeaker variability, as well as important differences among languages.</abstract>
			<keywords>Text-to-speech synthesis, pauses, punctuation</keywords>
		</article>
		<article id="taln-2002-long-016" session="session plénière TALN">
			<auteurs>
				<auteur>
					<nom>Thierry Selva</nom>
					<email>thierry.selva@ilt.kuleuven.ac.be</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">ILT – K.U. Leuven Dekenstraat 6 3000 Leuven, Belgique</affiliation>
			</affiliations>
			<titre>Génération automatique d’exercices contextuels de vocabulaire</titre>
			<type>long</type>
			<pages>185-194</pages>
			<resume>Cet article explore l’utilisation de ressources lexicales et textuelles ainsi que d’outils issus du TAL dans le domaine de l’apprentissage des langues assisté par ordinateur (ALAO). Il aborde le problème de la génération automatique ou semi-automatique d’exercices contextuels de vocabulaire à partir d’un corpus de textes et de données lexicales au moyen d’un étiqueteur et d’un parseur. Sont étudiées les caractéristiques et les limites de ces exercices.</resume>
			<mots_cles>Exercices contextuels, lexique, corpus, ALAO</mots_cles>
			<title></title>
			<abstract>This paper examines the use of lexical and textual resources and NLP tools in Computer-Assisted Language Learning (CALL). It tackles the issue of automatic or semi-automatic generation of contextual vocabulary exercises from a corpus of texts and lexical data by means of a tagger and a parser. It studies the characteristics and limits of these exercises.</abstract>
			<keywords>Contextual exercises, lexicon, corpus, CALL</keywords>
		</article>
		<article id="taln-2002-long-017" session="session plénière TALN">
			<auteurs>
				<auteur>
					<nom>Laura Monceaux</nom>
					<email>Laura.Monceaux@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Isabelle Robba</nom>
					<email>Isabelle.Robba@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIMSI / CNRS – Université Paris XI Bat 508, 91403 Orsay Cedex</affiliation>
			</affiliations>
			<titre>Les analyseurs syntaxiques : atouts pour une analyse des questions dans un système de question-réponse ?</titre>
			<type>long</type>
			<pages>195-204</pages>
			<resume>Cet article montre que pour une application telle qu’un système de question – réponse, une analyse par mots clés de la question est insuffisante et qu’une analyse plus détaillée passant par une analyse syntaxique permet de fournir des caractéristiques permettant une meilleure recherche de la réponse.</resume>
			<mots_cles>Analyse syntaxique de questions, connaissances sémantiques, détection du focus</mots_cles>
			<title></title>
			<abstract>In this paper, we show that in a question-answer application, a light parsing using only keywords is not sufficient. A syntactico-semantic parsing of the question must be used and must be completed with some others characteristics, which will a best search for the good answer.</abstract>
			<keywords>Syntactico-semantic parsing of questions, semantic knowledge, focus detection</keywords>
		</article>
		<article id="taln-2002-long-018" session="session plénière TALN">
			<auteurs>
				<auteur>
					<nom>Philippe Blache</nom>
					<email>blache@lpl.univ-aix.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Albert Di Cristo</nom>
					<email>dicristo@lpl.univ-aix.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LPL-CNRS, Université de Provence 29, Avenue Robert Schuman 13621 Aix-en-Provence, France</affiliation>
			</affiliations>
			<titre>Variabilité et dépendances des composants linguistiques</titre>
			<type>long</type>
			<pages>205-214</pages>
			<resume>Nous présentons dans cet article un cadre d’explication des relations entre les différents composants de l’analyse linguistique (prosodie, syntaxe, sémantique, etc.). Nous proposons un principe spécifiant un équilibre pour un objet linguistique donné entre ces différents composants sous la forme d’un poids (précisant l’aspect marqué de l’objet décrit) défini pour chacun d’entre eux et d’un seuil (correspondant à la somme de ces poids) à atteindre. Une telle approche permet d’expliquer certains phénomènes de variabilité : le choix d’une “tournure” à l’intérieur d’un des composants peut varier à condition que son poids n’empêche pas d’atteindre le seuil spécifié. Ce type d’information, outre son intérêt purement linguistique, constitue le premier élément de réponse pour l’introduction de la variabilité dans des applications comme les systèmes de génération ou de synthèse de la parole.</resume>
			<mots_cles></mots_cles>
			<title></title>
			<abstract></abstract>
			<keywords></keywords>
		</article>
		<article id="taln-2002-long-019" session="session plénière TALN">
			<auteurs>
				<auteur>
					<nom>Cécile Fabre</nom>
					<email>cfabre@univ-tlse2.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Cécile Frérot</nom>
					<email>cessfrerot@aol.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">ERSS, UMR 5610, Université Toulouse-Le Mirail</affiliation>
			</affiliations>
			<titre>Groupes prépositionnels arguments ou circonstants : vers un repérage automatique en corpus</titre>
			<type>long</type>
			<pages>215-224</pages>
			<resume>Dans cette étude, menée dans le cadre de la réalisation d’un analyseur syntaxique de corpus spécialisés, nous nous intéressons à la question des arguments et circonstants et à leur repérage automatique en corpus. Nous proposons une mesure simple pour distinguer automatiquement, au sein des groupes prépositionnels rattachés au verbe, des types de compléments différents. Nous réalisons cette distinction sur corpus, en mettant en oeuvre une stratégie endogène, et en utilisant deux mesures de productivité : la productivité du recteur verbal vis à vis de la préposition évalue le degré de cohésion entre le verbe et son groupe prépositionnel (GP), tandis que la productivité du régi vis à vis de la préposition permet d'évaluer le degré de cohésion interne du GP. Cet article présente ces deux mesures, commente les données obtenues, et détermine dans quelle mesure cette partition recouvre la distinction traditionnelle entre arguments et circonstants.</resume>
			<mots_cles>analyse syntaxique automatique, analyse endogène, productivité, argument, circonstant</mots_cles>
			<title></title>
			<abstract>This paper addresses the issue of the automatic distinction between arguments and adjuncts within the framework of a corpus-based parser. We propose a simple measure aimed at distinguishing automatically different types of complements among prepositional phrases (PP) attached to the verb. This corpus-based approach relies on an endogeneous technique as well as two measures of productivity. The productivity of a governor and its preposition determines the degree of cohesion between a verb and its PP. Conversely, the productivity of a governee assesses the degree of autonomy of a PP. After presenting both measures, we comment on the output data and determine whether this division characterises the traditional distinction between arguments and adjuncts.</abstract>
			<keywords>automatic parsing, endogeneous analysis, productivity, argument, adjunct</keywords>
		</article>
		<article id="taln-2002-long-020" session="session plénière TALN">
			<auteurs>
				<auteur>
					<nom>Claude De Loupy</nom>
					<email>loupy@sinequa.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Sinequa 51-54, rue Ledru-Rollin 94200 Ivry-sur-Seine France</affiliation>
			</affiliations>
			<titre>Évaluation des taux de synonymie et de polysémie dans un texte</titre>
			<type>long</type>
			<pages>225-234</pages>
			<resume>La polysémie et la synonymie sont deux aspects fondamentaux de la langue. Nous présentons ici une évaluation de l’importance de ces deux phénomènes à l’aide de statistiques basées sur le lexique WordNet et sur le SemCor. Ainsi, on a un taux de polysémie théorique de 5 sens par mot dans le SemCor. Mais si on regarde les occurrences réelles, moins de 50 % des sens possibles sont utilisés. De même, s’il y a, en moyenne, 2,7 mots possibles pour désigner un concept qui apparaît dans le corpus, plus de la moitié d’entre eux ne sont jamais utilisés. Ces résultats relativisent l’utilité de telles ressources sémantiques pour le traitement de la langue.</resume>
			<mots_cles>taux de polysémie, taux de synonymie, lexiques sémantiques, WordNet, SemCor</mots_cles>
			<title></title>
			<abstract>Polysemy and synonymy are two basic problems for natural language processing. In this paper, an evaluation of the importance of these phenomena is presented. It is based on the semantic lexicon WordNet and its associated corpus SemCor. Thus, when there are, in average, 5 possible senses for each word in the corpus, only half of them are really used. Similarly, if 2,7 words can be used to designate a concept, more than half of them are never used. These results tend to put the usefulness of such a resource in perspective.</abstract>
			<keywords>polysemy rate, synonymy rate, semantic lexicon, WordNet, SemCor</keywords>
		</article>
		<article id="taln-2002-long-021" session="Mots et morphologie">
			<auteurs>
				<auteur>
					<nom>Fiammetta Namer</nom>
					<email>namer@univ-nancy2.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">ATILF – Université Nancy 2 CLSH – BP3397 – 54015 Nancy Cedex</affiliation>
			</affiliations>
			<titre>Acquisition automatique de sens à partir d'opérations morphologiques en français : études de cas</titre>
			<type>long</type>
			<pages>235-244</pages>
			<resume>Cet article propose une méthode de codage automatique de traits lexicaux sémantiques en français. Cette approche exploite les relations fixées par l'instruction sémantique d'un opérateur de construction morphologique entre la base et le mot construit. En cela, la réflexion s'inspire des travaux de Marc Light (Light 1996) tout en exploitant le fonctionnement d’un système d'analyse morphologique existant : l’analyseur DériF. A ce jour, l'analyse de 12 types morphologiques conduit à l'étiquetage d’environ 10 % d'un lexique composé de 99000 lemmes. L’article s'achève par la description de deux techniques utilisées pour valider les traits sémantiques.</resume>
			<mots_cles>Morphologie dérivationnelle, affixation et conversion, acquisition de traits sémantiques</mots_cles>
			<title></title>
			<abstract>This paper presents an approach which aims at automatically tagging a French lexicon with semantic features. This approach makes use of correspondences which are fixed by the semantic instruction of morphological operators, between the base and the derived word. It is partly inspired by Light (1996) work, and makes use of an existing morphological parser : the DeriF system. So far, 12 morphological types have been analysed, enabling the semantic tagging of circa 10 % of a lexicon made up of 99000 lemmas. The paper ends up with the description of two techniques that have been applied to validate the semantic features.</abstract>
			<keywords>Derivational Morphology, affixation and conversion, Semantic feature acquisition</keywords>
		</article>
		<article id="taln-2002-long-022" session="Mots et morphologie">
			<auteurs>
				<auteur>
					<nom>Ludovic Tanguy</nom>
					<email>tanguy@univ-tlse2.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Nabil Hathout</nom>
					<email>hathout@univ-tlse2.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">ERSS – CNRS &amp; Université de Toulouse Le Mirail</affiliation>
			</affiliations>
			<titre>Webaffix : un outil d’acquisition morphologique dérivationnelle à partir duWeb</titre>
			<type>long</type>
			<pages>245-254</pages>
			<resume>L’article présente Webaffix, un outil d’acquisition de couples de lexèmes morphologiquement apparentés à partir du Web. La méthode utilisé est inductive et indépendante des langues particulières. Webaffix (1) utilise un moteur de recherche pour collecter des formes candidates qui contiennent un suffixe graphémique donné, (2) prédit les bases potentielles de ces candidats et (3) recherche sur le Web des cooccurrences des candidats et de leurs bases prédites. L’outil a été utilisé pour enrichir Verbaction, un lexique de liens entre verbes et noms d’action ou d’événement correspondants. L’article inclut une évaluation des liens morphologiques acquis.</resume>
			<mots_cles>Morphologie dérivationnelle, ressource lexicale, Web comme corpus, analogie</mots_cles>
			<title></title>
			<abstract>This paper presents Webaffix, a tool for finding pairs of morphologically related words on the Web. The method used is inductive and language-independent. Using theWWWas a corpus, the Webaffix tool detects the occurrences of new derived lexemes based on a given graphemic suffix, proposes a base lexeme, and then performs a compatibility test on the word pairs produced, using the Web again, but as a source of cooccurrences. The resulting pairs of words are used to enrich the Verbaction lexical database, which contains French verbs and their related nominals. The results are described and evaluated.</abstract>
			<keywords>Derivational morphology, lexical resource, Web as corpus, analogy</keywords>
		</article>
		<article id="taln-2002-long-023" session="Mots et morphologie">
			<auteurs>
				<auteur>
					<nom>Thierry Poibeau</nom>
					<email>thierry.poibeau@thalesgroup.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Dominique Dutoit</nom>
					<email>memodata@wanadoo.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Sophie Bizouard</nom>
					<email>sophie.bizouard@club-internet.fr</email>
					<affiliationId>3</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Thales et LIPN Domaine de Corbeville, 91404 Orsay, France</affiliation>
				<affiliation affiliationId="2">Memodata et CRISCO 17, rue Dumont d’Urville, 14 000 Caen, France</affiliation>
				<affiliation affiliationId="3">Crim/Inalco 2, rue de Lille, 75007 Paris, France</affiliation>
			</affiliations>
			<titre>Évaluer l’acquisition semi-automatique de classes sémantiques</titre>
			<type>long</type>
			<pages>255-264</pages>
			<resume>Cet article vise à évaluer deux approches différentes pour la constitution de classes sémantiques. Une approche endogène (acquisition à partir d’un corpus) est contrastée avec une approche exogène (à travers un réseau sémantique riche). L’article présente une évaluation fine de ces deux techniques.</resume>
			<mots_cles>Classes sémantiques, Évaluation, Ressources, Réseau sémantique</mots_cles>
			<title></title>
			<abstract>This paper evaluates two different approaches for the elaboration of semantic classes. An endogeneous approach (corpus-based learning) is contrasted with a exogeneous one (the use of a large semantic network). The two techniques are finally evaluated.</abstract>
			<keywords>Semantic classes, Evaluation, Resources, Semantic network</keywords>
		</article>
		<article id="taln-2002-long-024" session="Mots et morphologie">
			<auteurs>
				<auteur>
					<nom>Nordine Fourour</nom>
					<email>fourour@irin.univ-nantes.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Institut de Recherche en Informatique de Nantes - Université de Nantes 2, chemin de la Houssinière - BP 92208 - 44322 Nantes Cedex 3, France</affiliation>
			</affiliations>
			<titre>Nemesis, un système de reconnaissance incrémentielle des entités nommées pour le français</titre>
			<type>long</type>
			<pages>265-274</pages>
			<resume>Cet article présente une étude des conflits engendrés par la reconnaissance des entités nommées (EN) pour le français, ainsi que quelques indices pour les résoudre. Cette reconnaissance est réalisée par le système Nemesis, dont les spécifications ont été élaborées conséquemment à une étude en corpus. Nemesis se base sur des règles de grammaire, exploite des lexiques spécialisés et comporte un module d’apprentissage. Les performances atteintes par Nemesis, sur les anthroponymes et les toponymes, sont de 90% pour le rappel et 95% pour la précision.</resume>
			<mots_cles>Entités nommées, reconnaissance incrémentielle, apprentissage, surcomposition référentielle</mots_cles>
			<title></title>
			<abstract>This paper presents an investigation of the conflicts generated by the recognition of the French proper names (PN), and some indications to solve them. This recognition is carried out by the Nemesis system, whose specifications have been elaborated through corpus investigation. Nemesis is grammar-rule based, uses specialised lexicons, and includes a learning module. The system performance, evaluated on the categories composing the anthroponym and the toponym classes, achieves 95% precision and 90% recall.</abstract>
			<keywords>Name Entities, incremental recognition, learning processing, referential composition</keywords>
		</article>
		<article id="taln-2002-long-025" session="Traduction, grammaire formelle et recherche d'information">
			<auteurs>
				<auteur>
					<nom>Christian Boitet</nom>
					<email>Christian.Boitet@imag.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Wang-Ju Tsai</nom>
					<email>Wang-Ju.Tsai@imag.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">GETA, CLIPS, IMAG 385 rue de la Bibliothèque, BP 53 38041 Grenoble cedex 9, France</affiliation>
			</affiliations>
			<titre>La coédition langue↔UNL pour partager la révision entre les langues d'un document multilingue : un concept unificateur</titre>
			<type>long</type>
			<pages>275-286</pages>
			<resume>La coédition d'un texte en langue naturelle et de sa représentation dans une forme interlingue semble le moyen le meilleur et le plus simple de partager la révision du texte vers plusieurs langues. Pour diverses raisons, les graphes UNL sont les meilleurs candidats dans ce contexte. Nous développons un prototype où, dans le scénario avec partage le plus simple, des utilisateurs "naïfs" interagissent directement avec le texte dans leur langue (L0), et indirectement avec le graphe associé pour corriger les erreurs. Le graphe modifié est ensuite envoyé au déconvertisseur UNL-L0 et le résultat est affiché. S'il est satisfaisant, les erreurs étaient probablement dues au graphe et non au déconvertisseur, et le graphe est envoyé aux déconvertisseurs vers d'autres langues. Les versions dans certaines autres langues connues de l'utilisateur peuvent être affichées, de sorte que le partage de l'amélioration soit visible et encourageant. Comme les nouvelles versions sont ajoutées dans le document multilingue original avec des balises et des attributs appropriés, rien n'est jamais perdu, et le travail coopératif sur un même document est rendu possible. Du côté interne, des liaisons sont établies entre des éléments du texte et du graphe en utilisant des ressources largement disponibles comme un dictionnaire L0-anglais, ou mieux L0-UNL, un analyseur morphosyntaxique de L0, et une transformation canonique de graphe UNL à arbre. On peut établir une "meilleure" correspondance entre "l'arbre-UNL+L0" et la "structure MS-L0", une treille, en utilisant le dictionnaire et en cherchant à aligner l'arbre et une trajectoire avec aussi peu que possible de croisements de liaisons. Un but central de cette recherche est de fusionner les approches de la TA par pivot, de la TA interactive, et de la génération multilingue de texte.</resume>
			<mots_cles>Partage de révision, représentation interlingue, coédition de texte et de graphe UNL, communication multilingue</mots_cles>
			<title></title>
			<abstract>Coedition of a natural language text and its representation in some interlingual form seems the best and simplest way to share text revision across languages. For various reasons, UNL graphs are the best candidates in this context. We are developing a prototype where, in the simplest sharing scenario, naive users interact directly with the text in their language (L0), and indirectly with the associated graph, to correct errors. The modified graph is then sent to the UNL-L0 deconverter and the result shown. If it is satisfactory, the errors were probably due to the graph, not to the deconverter, and the graph is sent to deconverters in other languages. Versions in some other languages known by the user may be displayed, so that improvement sharing is visible and encouraging. As new versions are added with appropriate tags and attributes in the original multilingual document, nothing is ever lost, and cooperative working on a document is rendered feasible. On the internal side, liaisons are established between elements of the text and the graph by using broadly available resources such as a L0-English or better a L0-UNL dictionary, a morphosyntactic parser of L0, and a canonical graph2tree transformation. Establishing a "best" correspondence between the "UNL-tree+L0" and the "MS-L0 structure", a lattice, may be done using the dictionary and trying to align the tree and the selected trajectory with as few crossing liaisons as possible. A central goal of this research is to merge approaches from pivot MT, interactive MT, and multilingual text authoring.</abstract>
			<keywords>Revision sharing, interlingual representation, coedition of text &amp; UNL graph, multilingual communication</keywords>
		</article>
		<article id="taln-2002-long-026" session="Traduction, grammaire formelle et recherche d'information">
			<auteurs>
				<auteur>
					<nom>Jessie Pinkham</nom>
					<email>jessiep@microsoft.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Martine Smets</nom>
					<email>martines@microsoft.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Microsoft Research One Microsoft Way Redmond, WA 98052, USA</affiliation>
			</affiliations>
			<titre>Traduction automatique ancrée dans l’analyse linguistique</titre>
			<type>long</type>
			<pages>287-296</pages>
			<resume>Nous présentons dans cet article le système de traduction français-anglais MSR-MT développé à Microsoft dans le groupe de recherche sur le traitement du language (NLP). Ce système est basé sur des analyseurs sophistiqués qui produisent des formes logiques, dans la langue source et la langue cible. Ces formes logiques sont alignées pour produire la base de données du transfert, qui contient les correspondances entre langue source et langue cible, utilisées lors de la traduction. Nous présentons différents stages du développement de notre système, commencé en novembre 2000. Nous montrons que les performances d’octobre 2001 de notre système sont meilleures que celles du système commercial Systran, pour le domaine technique, et décrivons le travail linguistique qui nous a permis d’arriver à cette performance. Nous présentons enfin les résultats préliminaires sur un corpus plus général, les débats parlementaires du corpus du Hansard. Quoique nos résultats ne soient pas aussi concluants que pour le domaine technique, nous sommes convaincues que la résolution des problèmes d’analyse que nous avons identifiés nous permettra d’améliorer notre performance.</resume>
			<mots_cles>Traduction automatique, français-anglais, base d’exemples</mots_cles>
			<title></title>
			<abstract>In this paper, we present an overview of the MSR-MT translation system for French-English, developed at Microsoft Research in the NLP group. Our system is based on rule-based analysis which produces logical form representations of the source language and the target language. These are aligned to produce mappings then stored in an example-base. We examine the development of the system since November 2000, and show that by October 2001, we had exceeded the quality of the commercial system Systran in the technical domain. We describe the linguistic work that allowed our system to improve. We present preliminary results on the Hansard parliamentary corpus. While these results are less impressive currently, we are convinced that changes to linguistic analysis will allow the system performance to improve.</abstract>
			<keywords>Automatic translation, French-English, example-base</keywords>
		</article>
		<article id="taln-2002-long-027" session="Traduction, grammaire formelle et recherche d'information">
			<auteurs>
				<auteur>
					<nom>Guy Perrier</nom>
					<email>perrier@loria.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LORIA, CNRS BP 239 54506 Vandoeuvre-lès-Nancy Cedex France</affiliation>
			</affiliations>
			<titre>Descriptions d’arbres avec polarités : les Grammaires d’Interaction</titre>
			<type>long</type>
			<pages>297-306</pages>
			<resume>Nous présentons un nouveau formalisme linguistique, les Grammaires d’Interaction, dont les objets syntaxiques de base sont des descriptions d’arbres, c’est-à-dire des formules logiques spécifiant partiellement des arbres syntaxiques. Dans ce contexte, l’analyse syntaxique se traduit par la construction de modèles de descriptions sous la forme d’arbres syntaxiques complètement spécifiés. L’opération de composition syntaxique qui permet cette construction pas à pas est contrôlée par un système de traits polarisés agissant comme des charges électrostatiques.</resume>
			<mots_cles>Grammaire formelle, Grammaire Catégorielle, Description d’arbres</mots_cles>
			<title></title>
			<abstract>We present a new linguistic formalism, Interaction Grammars. The basic syntactic objects are tree descriptions, that is logical formulas which specify syntactic trees partially. In this context, parsing consists in building models of descriptions which are completely specified syntactic trees. The operation of syntactic composition which allows this construction to go step by step is controlled by a system of polarized features acting as electrostatic charges.</abstract>
			<keywords>Formal Grammar, Categorial Grammar, Tree description</keywords>
		</article>
		<article id="taln-2002-long-028" session="Traduction, grammaire formelle et recherche d'information">
			<auteurs>
				<auteur>
					<nom>Olivier Ferret</nom>
					<email>ferret@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Brigitte Grau</nom>
					<email>bg@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Martine Hurault-Plantet</nom>
					<email>mhp@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Gabriel Illouz</nom>
					<email>gabrieli@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Laura Monceaux</nom>
					<email>monceaux@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Isabelle Robba</nom>
					<email>robba@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Anne Vilnat</nom>
					<email>anne@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIMSI-CNRS BP 133, 91403 Orsay</affiliation>
			</affiliations>
			<titre>Recherche de la réponse fondée sur la reconnaissance du focus de la question</titre>
			<type>long</type>
			<pages>307-316</pages>
			<resume>Le système de question-réponse QALC utilise les documents sélectionnés par un moteur de recherche pour la question posée, les sépare en phrases afin de comparer chaque phrase avec la question, puis localise la réponse soit en détectant l'entité nommée recherchée, soit en appliquant des patrons syntaxiques d'extraction de la réponse, sortes de schémas figés de réponse pour un type donné de question. Les patrons d'extraction que nous avons définis se fondent sur la notion de focus, qui est l'élément important de la question, celui qui devra se trouver dans la phrase réponse. Dans cet article, nous décrirons comment nous déterminons le focus dans la question, puis comment nous l'utilisons dans l'appariement question-phrase et pour la localisation de la réponse dans les phrases les plus pertinentes retenues.</resume>
			<mots_cles>Recherche d’information, système de question-réponse, focus, patron d’extraction</mots_cles>
			<title></title>
			<abstract>The QALC question answering system we are developing uses a search engine to select documents responding to the question, matches each sentence of the selected documents with the question, then extracts the answer from the more relevant sentences, either by locating the expected named entity, or by applying extraction patterns. Patterns are based on the focus, which is the main concept in the question and is expected to be present in an answer sentence. In this paper, we will describe the way we determine the focus of the question, and the way that we use it in the question-answer pairing and answer location processes.</abstract>
			<keywords>Information retrieval, question answering system, focus, extraction pattern</keywords>
		</article>
		<article id="taln-2002-poster-001" session="Posters TALN">
			<auteurs>
				<auteur>
					<nom>Jean-Yves Antoine</nom>
					<email>Jean-Yves.Antoine@univ-ubs.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Sabine Letellier-Zarshenas</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Pascale Nicolas</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Igor Schadle</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Jean Caelen</nom>
					<email></email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">VALORIA , Université de Bretagne Sud, rue Yves Mainguy, F-56 000, France</affiliation>
				<affiliation affiliationId="2">CLIPS-IMAG , Université Joseph Fourier, BP 53, F-38041 Grenoble Cedex 9, France</affiliation>
			</affiliations>
			<titre>Corpus OTG et ECOLE_MASSY : vers la constitution d’une collection de corpus francophones de dialogue oral diffusés librement</titre>
			<type>poster</type>
			<pages>319-324</pages>
			<resume>Cet article présente deux corpus francophones de dialogue oral (OTG et ECOLE_MASSY) mis librement à la disposition de la communauté scientifique. Ces deux corpus constituent la première livraison du projet Parole Publique initié par le laboratoire VALORIA. Ce projet vise la constitution d’une collection de corpus de dialogue oral enrichis par annotation morpho-syntaxique. Ces corpus de dialogue finalisé sont essentiellement destinés à une utilisation en communication homme-machine.</resume>
			<mots_cles>ressources linguistiques francophones, dialogue oral, communication homme-machine</mots_cles>
			<title></title>
			<abstract>This paper presents two corpora (OTG et ECOLE_MASSY) of French spoken dialogue which are the first delivery of the Parole_Publique (in English : Public Speech) project held by the VALORIA laboratory. This project aims at the achievement of a collection of spoken dialogue corpora that is freely distributed on the WWW. It is primarily intended for researches on man-machine communication.</abstract>
			<keywords>French speaking linguistic ressources, spoken dialogue, man-machine communication</keywords>
		</article>
		<article id="taln-2002-poster-002" session="Posters TALN">
			<auteurs>
				<auteur>
					<nom>Laurence Kister</nom>
					<email>Laurence.Kister@univ-nancy2.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">ATILF - CNRS UMR 7118 IUT Nancy-Charlemagne - Université Nancy 2 - Département Info-Com 2ter, boulevard Charlemagne - 54000 Nancy</affiliation>
			</affiliations>
			<titre>Relatifs et référents inclus dans un SN : des paramètres pour présélectionner la saisie</titre>
			<type>poster</type>
			<pages>325-330</pages>
			<resume>Notre objectif est de repérer l’existence de régularités pour prévoir l’attachement du relatif et de son référent introduit par un SN de la forme dét. N1 de (dét.) N2 en vue d’un traitement automatique. Pour évaluer les préférences, nous avons entrepris une analyse sur corpus. L’examen des occurrences examinées laisse entrevoir des variations en fonction de paramètres d’ordre fonctionnel, syntaxique et sémantiques : déterminants, traits sémantiques, saillance, relation établie par l’utilisation de de, position grammaticale du SN qui introduit le référent dans le discours…</resume>
			<mots_cles>Anaphore, SN complexes, référence, préposition de, relatif qui</mots_cles>
			<title></title>
			<abstract></abstract>
			<keywords></keywords>
		</article>
		<article id="taln-2002-poster-003" session="Posters TALN">
			<auteurs>
				<auteur>
					<nom>Yves Bestgen</nom>
					<email>yves.bestgen@psp.ucl.ac.be</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Anne-Françoise Cabiaux</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Fonds national de la recherche scientifique Université catholique de Louvain Place du Cardinal Mercier, 10 B1347 Louvain-la-Neuve Belgique</affiliation>
			</affiliations>
			<titre>L'analyse sémantique latente et l'identification des métaphores</titre>
			<type>poster</type>
			<pages>331-337</pages>
			<resume>Après avoir présenté le modèle computationnel de l'interprétation de métaphores proposé par Kintsch (2000), nous rapportons une étude préliminaire qui évalue son efficacité dans le traitement de métaphores littéraires et la possibilité de l'employer pour leur identification.</resume>
			<mots_cles>Métaphore, Analyse sémantique latente, interprétation et détection, textes littéraires</mots_cles>
			<title></title>
			<abstract>Having introduced Kintsch's computational model of metaphor comprehension (2000), we report a preliminary study aiming at determining its efficiency in modeling the processing of literary metaphors and its ability to detect them.</abstract>
			<keywords>Metaphor, Latent semantic analysis, interpretation and detection, literary texts</keywords>
		</article>
		<article id="taln-2002-poster-004" session="Posters TALN">
			<auteurs>
				<auteur>
					<nom>Michel Généreux</nom>
					<email>michel@oefai.at</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Austrian Research Institute for Artificial Intelligence Schottengasse 3, A-1030, Vienna, Austria</affiliation>
			</affiliations>
			<titre></titre>
			<type>poster</type>
			<pages>339-344</pages>
			<resume></resume>
			<mots_cles>Analyseur sémantique, Corpus, Langue naturelle</mots_cles>
			<title>An Example-Based Semantic Parser for Natural Language</title>
			<abstract>This paper presents a method for guiding semantic parsers based on a statistical model. The parser is example driven, that is, it learns how to interpret a new utterance by looking at some examples. It is mainly predicated on the idea that similarities exist between contexts in which individual parsing actions take place. Those similarities are then used to compute the degree of certainty of a particular parse. The treatment of word order and the disambiguation of meanings can therefore be learned.</abstract>
			<keywords>Semantic parser, Corpus, Natural language</keywords>
		</article>
		<article id="taln-2002-poster-005" session="Posters TALN">
			<auteurs>
				<auteur>
					<nom>François Maniez</nom>
					<email>fmaniez@wanadoo.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Centre de Recherche en Terminologie et en Traduction Université Lumière Lyon 2</affiliation>
			</affiliations>
			<titre>Distinguer les termes des collocations : étude sur corpus du patron &lt;Adjectif – Nom&gt; en anglais médical</titre>
			<type>poster</type>
			<pages>345-350</pages>
			<resume>Un bon nombre des applications de traitement automatique des langues qui ont pour domaine les langues de spécialité sont des outils d’extraction terminologique. Elles se concentrent donc naturellement sur l’identification des groupes nominaux et des groupes prépositionnels ou prémodificateurs qui leur sont associés. En nous fondant sur un corpus composé d’articles de recherche médicale de langue anglaise, nous proposons un modèle d’extraction phraséologique semi-automatisée. Afin de distinguer, dans le cas des expressions de patron syntaxique &lt;Adjectif – Nom&gt;, les termes de la langue médicale des simples collocations, nous nous sommes livré au repérage des adjectifs entrant en cooccurrence avec les adverbes. Cette méthode, qui permet l’élimination de la plupart des adjectifs relationnels, s’avère efficace en termes de précision. L’amélioration de son rappel nécessite toutefois l’utilisation de corpus de grande taille ayant subi un étiquetage morpho-syntaxique préalable.</resume>
			<mots_cles>Termes, collocations, adjectifs, noms, corpus, anglais de spécialité</mots_cles>
			<title></title>
			<abstract>Many of the Natural Language Processing applications that deal with sublanguages are terminological extraction tools. They consequently tend to focus on the identification of noun and prepositional clauses and their modifiers. Using a corpus of English medical research articles, we suggest a semi-automatic phraseological extraction system. In order to separate terms from collocations within the category that fits the &lt;Adjective – Noun&gt; pattern, we experiment with the approach that consists in extracting adjectives that co-occur with adverbs. This method, which makes it possible to eliminate most relative adjectives, proves to provide good precision. However, the use of a larger POS-tagged corpus will be necessary in order to improve the method’s recall.</abstract>
			<keywords>Terms, collocations, adjectives, nouns, corpus, English for Specific Purposes</keywords>
		</article>
		<article id="taln-2002-poster-006" session="Posters TALN">
			<auteurs>
				<auteur>
					<nom>Nicolas Barrier</nom>
					<email>nbarrier@linguist.jussieu.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Laboratoire LLF - Université Paris VII UFR de Linguistique, case 7003 2, place Jussieu 75251 Paris Cedex 05 - FRANCE</affiliation>
			</affiliations>
			<titre>Une MétaGrammaire pour les adjectifs du français</titre>
			<type>poster</type>
			<pages>351-357</pages>
			<resume>Initialement développée au sein de l’université Paris VII depuis maintenant près de quinze ans, la grammaire FTAG, une implémentation du modèle des grammaires d’arbres adjoints pour le français, a connu ses dernières années, diverses évolutions majeures. (Candito, 1996) a ainsi réalisé l’intégration d’un modèle de représentation compact et hiérarchique d’informations redondantes que peut contenir une grammaire, au sein d’un système déjà existant. Ce modèle, que nous appelons MétaGammaire (MG) nous a permis, en pratique, de générer semi-automatiquement des arbres élémentaires, et par là même, d’augmenter de façon considérable les différents phénomènes syntaxiques couverts par notre grammaire. Un soin tout particulier a donc été apporté pour traiter les prédicats verbaux, en laissant cependant (partiellement) de côté le prédicat adjectival. Nous présentons donc ici une nouvelle implémentation de ce prédicat dans le cadre d’une extension de la grammaire FTAG existante.</resume>
			<mots_cles>Grammaires d’arbres adjoints, MétaGrammaire, Développement, Constructions adjectivales</mots_cles>
			<title></title>
			<abstract>We present here a new implementation of the adjectival predicate for the FTAG grammar, a french implementation of the Tree Adjoining Grammar model. FTAG has known over the years many improvements. (Candito, 1996) hence integrated an additional layer of syntactic representation within the system. The layer, we called Meta- Grammar (MG) let us improve the syntactic coverage of our grammar by generating semi-automatically dozens of new elementary trees.</abstract>
			<keywords>Tree-Adjoining Grammars, MetaGrammar, Development, French Adjectival constructions</keywords>
		</article>
		<article id="taln-2002-poster-007" session="Posters TALN">
			<auteurs>
				<auteur>
					<nom>Jean-Cédric Chappelier</nom>
					<email>Jean-Cedric.Chappelier@epfl.ch</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Martin Rajman</nom>
					<email>Martin.Rajman@epfl.ch</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Antoine Rozenknop</nom>
					<email>Antoine.Rozenknop@epfl.ch</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">EPFL I&amp;C–IIF–LIA, IN (Ecublens) CH-1015 Lausanne, Switzerland</affiliation>
			</affiliations>
			<titre></titre>
			<type>poster</type>
			<pages>359-364</pages>
			<resume>Le but de ce papier est de caractériser (au moins partiellement) les Grammaires à Substitution d’Arbres Polynômiales (pSTSG), instances particulières de STSG pour lesquelles la recherche de l’analyse la plus probable peut être effectuée en un temps polynômial. Nous donnons tout d’abord diverses conditions suffisantes, utilisables en pratique, qui garantissent qu’une STSG est polynômiale. Une telle condition suffisante, fondée sur la notion de "tête de syntagme", est ensuite présentée et évaluée.</resume>
			<mots_cles>Analyse Syntaxique Probabiliste, Grammaires à Substitution d’Arbres Polynomiales, Grammaires Hors-Contexte Probabilistes, Data-Oriented Parsing</mots_cles>
			<title>Polynomial Tree Substitution Grammars: Characterization and New Examples</title>
			<abstract>Polynomial Tree Substitution Grammars, a subclass of STSGs for which finding the most probable parse is no longer NP-hard but polynomial, are defined and characterized in terms of general properties on the elementary trees in the grammar. Various sufficient and easy to compute properties for a STSG to be polynomial are presented. The min-max selection principle is shown to be one such sufficient property. In addition, another, new, instance of a sufficient property, based on lexical heads, is presented. The performances of both models are evaluated on several corpora.</abstract>
			<keywords>Stochastic Parsing, Polynomial Tree Substitution Grammars, Stochastic Context-Free Grammars, Data-Oriented Parsing</keywords>
		</article>
		<article id="taln-2002-poster-008" session="Posters TALN">
			<auteurs>
				<auteur>
					<nom>Elisabeth Godbert</nom>
					<email>godbert@lim.univ-mrs.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Laboratoire d'Informatique Fondamentale de Marseille (LIF) Université de la Méditerranée et CNRS 163 Avenue de Luminy - case 901 13288 Marseille Cedex 9 - France</affiliation>
			</affiliations>
			<titre>Reformuler des expressions multimodales</titre>
			<type>poster</type>
			<pages>365-370</pages>
			<resume>Le domaine des "Interfaces Utilisateur Intelligentes" a vu ces dernières années la réalisation de systèmes complexes mettant en oeuvre une interaction multimodale dans laquelle les différentes techniques de communication (textes, gestes, parole, sélection graphique) sont coordonnées en entrée et/ou en sortie. Nous nous intéressons ici aux systèmes qui prennent en entrée des expressions multimodales et en produisent une reformulation en une expression unimodale sémantiquement équivalente. Nous proposons une modélisation du processus de traduction d'expressions multimodales en expressions unimodales, et nous décrivons la mise en oeuvre d'un processus de ce type dans un logiciel d'aide à l'apprentissage du langage.</resume>
			<mots_cles>Langage multimodal, coordination des modes, expressions sémantiquement équivalentes</mots_cles>
			<title></title>
			<abstract>These last years, "Intelligent User Interfaces" have been developped, in which input and/or output multimodality facilitates human-machine communication (written language, speech, graphic selection, gestures). This paper is concerned with systems in which multimodal input is translated into unimodal expressions semantically equivalent. A model for such a process is proposed. Then, an example is described, with an educational software enabling multimodal communication and implementing this process.</abstract>
			<keywords>Multimodal language, coordination of modalities, semantic equivalence</keywords>
		</article>
		<article id="taln-2002-poster-009" session="Posters TALN">
			<auteurs>
				<auteur>
					<nom>Yannick Fouquet</nom>
					<email>Yannick.Fouquet@imag.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Laboratoire CLIPS/IMAG, équipe GEOD 385 rue de la Bibliothèque, BP53 – 38041 Grenoble Cedex 9, France</affiliation>
			</affiliations>
			<titre>Un modèle de dialogue par les attentes du locuteur</titre>
			<type>poster</type>
			<pages>371-377</pages>
			<resume>Dans cet article, nous aborderons la notion d’attentes, vue du côté du locuteur, afin d’améliorer la modélisation du dialogue. Nous présenterons notre définition des attentes ainsi que notre notation, fondée sur une approche pragmatique du dialogue. Nous comparerons deux approches, l’une (uniquement stochastique) fondée sur la prédiction d’actes de parole, l’autre mettant en jeu les attentes du locuteur et leur gestion.</resume>
			<mots_cles>Dialogue, attentes, pragmatique, analyse, statistique, aspects cognitifs</mots_cles>
			<title></title>
			<abstract>In this paper, we will discuss about a generic dialog model. We will demonstrate the importance of a expectations based approach which consider them from user’s side. We will present our definition of expectations and their notation based on a pragmatic approach of dialog. Thus, we will compare this approach (expectations and their management), and another stochastic approach which simply consist of act prediction.</abstract>
			<keywords>Dialog, expectations, pragmatics, analysis, statistics, cognitive aspects</keywords>
		</article>
		<article id="taln-2002-poster-010" session="Posters TALN">
			<auteurs>
				<auteur>
					<nom>Narjès Boufaden</nom>
					<email>boufaden@iro.umontreal.ca</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Guy Lapalme</nom>
					<email>lapalme@iro.umontreal.ca</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Yoshua Bengio</nom>
					<email>bengioy@iro.umontreal.ca</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Département d’informatique et Recherche Opérationnelle Université de Montréal, Québec Canada</affiliation>
			</affiliations>
			<titre>Segmentation en thèmes de conversations téléphoniques : traitement en amont pour l’extraction d’information</titre>
			<type>poster</type>
			<pages>377-382</pages>
			<resume>Nous présentons une approche de découpage thématique que nous utiliserons pour faciliter l’extraction d’information à partir de conversations téléphoniques transcrites. Nous expérimentons avec un modèle de Markov caché utilisant des informations de différents niveaux linguistiques, des marques d’extra-grammaticalités et les entités nommées comme source additionnelle d’information. Nous comparons le modèle obtenu avec notre modèle de base utilisant uniquement les marques linguistiques et les extra-grammaticalités. Les résultats montrent l’efficacité de l’approche utilisant les entités nommées.</resume>
			<mots_cles>segmentation en thèmes, analyse des conversations, extraction d’information</mots_cles>
			<title></title>
			<abstract>We study the problem of topic segmentation as a means to facilitate information extraction from manually transcribed convesrations.We experiment with a first orderHMMusing a combination of linguistic-level cues and named entities. We compare the results of our linguistic-levels cues based model with the named entities based model. Results show the effectiveness of named entities as an additional source of information for topic segmentation.</abstract>
			<keywords>topic segmentation, conversation analysis, information extraction</keywords>
		</article>
		<article id="taln-2002-poster-011" session="Posters TALN">
			<auteurs>
				<auteur>
					<nom>Laurent Roussarie</nom>
					<email>roussarie@linguist.jussieu.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Pascal Amsili</nom>
					<email>amsili@linguist.jussieu.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Talana / Lattice (CNRS UMR 8094) – Université Paris 7 UFRL – Case 7003 ; 2, place Jussieu ; 75251 Paris Cedex 05</affiliation>
			</affiliations>
			<titre>Discours et compositionnalité</titre>
			<type>poster</type>
			<pages>383-388</pages>
			<resume>Partant du principe que certaines phrases peuvent réaliser plusieurs actes de langage, i.e., dans une interface sémantique–pragmatique, plusieurs constituants de discours séparés, nous proposons, dans le cadre de la SDRT, un algorithme de construction de représentations sémantiques qui prend en compte tous les aspects discursifs dès que possible et de façon compositionnelle.</resume>
			<mots_cles>Analyse du discours, compositionnalité, lambda-calcul, SDRT, actes de langage</mots_cles>
			<title></title>
			<abstract>Assuming that some sentences realise several speech acts, and that, in a semantic–pragmatic interface, they yield separate discourse constituents, we propose an algorithm, in the SDRT framework, which takes into account all discursive aspects as soon as possible and compositionaly.</abstract>
			<keywords>Discourse analysis, compositionality, lambda-calculus, SDRT, speech acts</keywords>
		</article>
		<article id="taln-2002-poster-012" session="Posters TALN">
			<auteurs>
				<auteur>
					<nom>Jérôme Goulian</nom>
					<email>jerome.goulian@univ-ubs.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Jean-Yves Antoine</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Franck Poirier</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">VALORIA , EA2593 – Université de Bretagne Sud Site de Tohannic, rue Yves Mainguy, 56000 Vannes</affiliation>
			</affiliations>
			<titre>Compréhension Automatique de la Parole et TAL : une approche syntaxico-sémantique pour le traitement des inattendus structuraux du français parlé</titre>
			<type>poster</type>
			<pages>389-394</pages>
			<resume>Dans cet article, nous présentons un système de Compréhension Automatique de la Parole dont l’un des objectifs est de permettre un traitement fiable et robuste des inattendus structuraux du français parlé (hésitations, répétitions et corrections). L’analyse d’un énoncé s’effectue en deux étapes : une première étape générique d’analyse syntaxique de surface suivie d’une seconde étape d’analyse sémantico-pragmatique, dépendante du domaine d’application et reposant sur un formalisme lexicalisé : les grammaires de liens. Les résultats de l’évaluation de ce système lors de la campagne d’évaluation du Groupe de Travail Compréhension Robuste du GDR I3 du CNRS nous permettent de discuter de l’intérêt et des limitations de l’approche adoptée.</resume>
			<mots_cles>communication orale homme-machine, compréhension automatique de la parole, répétitions, corrections, analyse syntaxique partielle, grammaires de liens</mots_cles>
			<title></title>
			<abstract>This paper discusses the issue of how a speech understanding system can be made robust against spontaneous speech phenomena (hesitations and repairs). We present a spoken French understanding system. It implements speech understanding in a two-stage process. The first stage achieves a finite-state shallow parsing that consists in segmenting the recognized sentence into basic units (spoken-adapted chunks). The second one, a Link Grammar parser, looks for interchunks dependencies in order to build a rich representation of the semantic structure of the utterance. These dependencies are mainly investigated at a pragmatic level through the consideration of a task concept hierarchy. Discussion about the approach adopted is based on the results of the system’s assessment in an evaluation campaign held by the CNRS.</abstract>
			<keywords>spoken man-machine dialog, speech understanding, repairs, shallow parsing, link grammars</keywords>
		</article>
		<article id="taln-2002-poster-013" session="Posters TALN">
			<auteurs>
				<auteur>
					<nom>Cédrick Fairon</nom>
					<email>fairon@tedm.ucl.ac.be</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>David M. Williamson</nom>
					<email>dmwilliamson@ets.org</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Centre de traitement électronique des documents (CETEDOC) University of Louvain (UCL) 1348 Louvain-la-Neuve, Belgium</affiliation>
				<affiliation affiliationId="2">Educational Testing Service (ETS) Rosedale Road, Princeton, NJ - USA</affiliation>
			</affiliations>
			<titre></titre>
			<type>poster</type>
			<pages>395-401</pages>
			<resume></resume>
			<mots_cles>Génération automatique de texte, Tests informatisés, INTEX, Transducteurs à états finis</mots_cles>
			<title>Automatic Item Text Generation in Educational Assessment</title>
			<abstract>We present an automatic text generation system (ATG) developed for the generation of natural language text for automatically produced test items. This ATG has been developed to work with an automatic item generation system for analytical reasoning items for use in tests with high-stakes outcomes (such as college admissions decisions). As such, the development and implementation of this ATG is couched in the context and goals of automated item generation for educational assessment.</abstract>
			<keywords>NLG, ATG, CAT, INTEX, Finite State Transducers</keywords>
		</article>
		<article id="taln-2002-tutoriel-001" session="Tutoriels">
			<auteurs>
				<auteur>
					<nom>Pascale Bernard</nom>
					<email>contact@inalf.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Jacques Dendien</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Josette Lecomte</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Jean-Marie Pierrel</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">ATILF-CNRS (Analyse et Traitement Informatique de la Langue Française) 44 Avenue de la Libération BP30687 F-54063 Nancy-Cedex</affiliation>
				<affiliation affiliationId="2"></affiliation>
				<affiliation affiliationId="3"></affiliation>
			</affiliations>
			<titre>Un ensemble de ressources informatisées et intégrées pour l’étude du français : FRANTEXT, TLFi, Dictionnaires de l’Académie et logiciel Stella, présentation et apprentissage de leurs exploitations</titre>
			<type>tutoriel</type>
			<pages>3-36</pages>
			<resume>Nous proposons de présenter quelques-unes des ressources linguistiques informatisées que le laboratoire ATILF propose sur la toile et leurs diversités d’exploitation potentielle. Ces importantes ressources sur la langue française regroupent un ensemble de divers dictionnaires et lexiques, et de bases de données dont les plus importants sont le TLFi (Trésor de la Langue Française informatisé) et Frantext (plus de 3500 textes, dont la plupart catégorisés). Elles exploitent, pour la plupart, les fonctionnalités du logiciel Stella, qui correspond à un véritable moteur de recherche dédié aux bases textuelles s’appuyant sur une nouvelle théorie des objets textuels. Tous les spécialistes de traitement automatique de la langue ainsi que tous les linguistes, syntacticiens aussi bien que sémanticiens, stylisticiens et autres peuvent exploiter avec bonheur les possibilités offertes par Stella sur le TLFi et autres ressources offertes par l’ATILF. Ces recherches peuvent s’articuler autour des axes suivants : études en vue de repérer des cooccurrences et collocations, extraction de sous-lexiques, études morphologiques, études de syntaxe locale, études de sémantique, études de stylistique, etc. Nous proposons de démystifier le maniement des requêtes sur le TLFi, FRANTEXT et nos autres ressources à l’aide du logiciel Stella, et d’expliquer et de montrer comment interroger au mieux ces ressources et utiliser l’hyper-navigation mise en place entre ces ressources pour en tirer les meilleurs bénéfices.</resume>
			<mots_cles>Ressources linguistiques, corpus, dictionnaires, lexiques, Frantext, TLFi, Stella</mots_cles>
			<title></title>
			<abstract></abstract>
			<keywords></keywords>
		</article>
		<article id="taln-2002-tutoriel-002" session="Tutoriels">
			<auteurs>
				<auteur>
					<nom>Alain Polguère</nom>
					<email>alain.polguere@umontreal.ca</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">OLST — Département de linguistique et de traduction Université de Montréal C. P. 6128, Succ. Centre-Ville Montréal (Québec) H3C 3J7 Canada</affiliation>
			</affiliations>
			<titre>Modélisation des liens lexicaux au moyen des fonctions lexicales</titre>
			<type>tutoriel</type>
			<pages>37-60</pages>
			<resume>Ce tutoriel est une introduction à la modélisation lexicographique des liens lexicaux au moyen des fonctions lexicales de la théorie Sens-Texte. Il s’agit donc d’examiner un sous-ensemble des tâches effectuées en lexicographie formelle basée sur la lexicologie explicative et combinatoire. Plutôt que de viser l’introduction de toutes les fonctions lexicales identifiées par la théorie Sens- Texte, je vais m’attacher à introduire la notion de fonction lexicale de façon méthodique, en présentant d’abord les notions linguistiques plus générales sur lesquelles elle s’appuie (lexie, prédicat, actant, dérivation sémantique, collocation, etc.). Ce document vise essentiellement à récapituler les définitions des notions linguistiques qui vont être vues dans le tutoriel de façon pratique, par le biais d’exercices à caractère lexicographique.</resume>
			<mots_cles>Fonction lexicale, lexicologie explicative et combinatoire, théorie Sens-Texte, lexicographie formelle</mots_cles>
			<title></title>
			<abstract>This tutorial is an introduction to the modeling of lexical relations by means of Meaning-Text lexical functions. In other words, it presents a specific aspect of formal lexicography as practiced within the framework of explanatory combinatorial lexicology. Only a subset of Meaning- Text lexical functions will be introduced. My goal is to methodically presents the notion of lexical function itself, based on the introduction of more general linguistic notions it presupposes (lexie, actant, semantic derivation, collocation, etc.). The present document summarizes definitions of notions that will be applied in lexicographic exercises during the tutorial.</abstract>
			<keywords>Lexical function, explanatory combinatorial lexicology, Meaning-Text theory, formal lexicography</keywords>
		</article>
		<article id="taln-2002-tutoriel-003" session="Tutoriels">
			<auteurs>
				<auteur>
					<nom>Pius Ten Hacken</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Universität Basel &amp; Anke Lüdeling Universität Osnabrück</affiliation>
			</affiliations>
			<titre></titre>
			<type>tutoriel</type>
			<pages>61-87</pages>
			<resume></resume>
			<mots_cles></mots_cles>
			<title>Word Formation in Computational Linguistics</title>
			<abstract></abstract>
			<keywords></keywords>
		</article>
		<article id="taln-2002-tutoriel-004" session="Tutoriels">
			<auteurs>
				<auteur>
					<nom>Antonio Balvet</nom>
					<email>antonio.balvet@u-paris10.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Olivier Grisvard</nom>
					<email>olivier.grisvard@thalesgroup.com</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Pascal Bisson</nom>
					<email>pascal.bisson@thalesgroup.com</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">UMR MoDyCo, Université Paris X Nanterre 200, av. de le République 92001 Nanterre</affiliation>
				<affiliation affiliationId="2">Thales RT, DAS-HIT Domaine de Corbeville, 91404 Orsay</affiliation>
			</affiliations>
			<titre>Tutoriel : Open Agent Architecture Développement d’applications de TALN distribuées, multiagents et multiplates-formes</titre>
			<type>tutoriel</type>
			<pages>89-108</pages>
			<resume>Nous présenterons tout d’abord la philosophie « Agents » en général, afin d’en montrer les avantages pour le domaine du TALN, qui se caractérise par une hétérogénéité avérée des systèmes existants (multiplicité des langages de programmation), ainsi qu’une forte demande en ressources (mémoire notamment). Nous ferons ensuite une présentation des principales plate-formes orientées agents, puis nous examinerons de plus près la plate-forme développée au Standford Research Institute (SRI) : OAA (licence libre). Nous clôturerons le tutoriel sur des exemples commentés d’applications industrielles utilisant OAA, permettant de donner toutes les clés nécessaires au développement d’applications distribuées (intra/internet), multiagents et multiplates-formes (plusieurs langages de programmation/systèmes d’exploitation).</resume>
			<mots_cles></mots_cles>
			<title></title>
			<abstract>We will first present the “Agent” philosophy in general, in order to put forward its usefulness for resources-intensive (memory) NLP systems, in a heterogeneous context. We will also present an overview of the main agent-oriented platforms, with a focus on the OAA platform, developed at the Stanford Research Institute. Finally, we will present a detailed example of a real-scale multiagent NLP system based on OAA, with an emphasis on development details which will provide the audience with all the required information to develop distributed (intra/internet), multiagent and multiplaforms systems (different programming languages/operating systems).</abstract>
			<keywords></keywords>
		</article>
	</articles>
</conference>