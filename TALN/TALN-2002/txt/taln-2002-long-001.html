<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
"http://www.w3.org/TR/html4/loose.dtd">
<html><head><title>Analyse Factorielle Neuronale pour Documents Textuels</title>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
</head>
<body>
<div style="page-break-before:always; page-break-after:always"><div><p>TALN 2002, Nancy, 24-27juin 2002 
</p>
<p> 
 
 
</p>
<p>Analyse Factorielle Neuronale pour Documents Textuels 
 
 
</p>
<p>Mathieu Delich&#232;re (1) &amp; Daniel Memmi (2) 
 
</p>
<p>(1) Amoweba 
1 ave Berthollet, 74000 Annecy (France) 
</p>
<p>mathieu@amoweba.com 
(2) Leibniz-Imag 
</p>
<p>46 ave F&#233;lix Viallet, 38000 Grenoble (France) 
memmi@imag.fr 
</p>
<p> 
 
 
 
</p>
<p>R&#233;sum&#233; - Abstract 
 
En recherche documentaire, on repr&#233;sente souvent les documents textuels par des vecteurs 
lexicaux de grande dimension qui sont redondants et co&#251;teux. Il est utile de r&#233;duire la 
dimension des ces repr&#233;sentations pour des raisons &#224; la fois techniques et s&#233;mantiques. 
Cependant les techniques classiques d'analyse factorielle comme l'ACP ne permettent pas de 
traiter des vecteurs de tr&#232;s grande dimension. Nous avons alors utilis&#233; une m&#233;thode adaptative 
neuronale (GHA) qui s'est r&#233;v&#233;l&#233;e efficace pour calculer un nombre r&#233;duit de nouvelles 
dimensions repr&#233;sentatives des donn&#233;es. L'approche nous a permis de classer un corpus r&#233;el 
de pages Web avec de bons r&#233;sultats. 
 
For document retrieval purposes, documents are often represented by high-dimensional 
lexical vectors, which are costly and redundant. Reducing vector dimensionality is then useful 
for both technical and semantic reasons. Classical data analysis methods such as PCA cannot 
unfortunately process vectors of very high dimension. We have used instead an adaptive 
neural network technique, the Generalized Hebbian Algorithm (GHA), which makes it 
possible to reduce high-dimension spaces. This approach allowed us to cluster areal end-user 
corpus of Web pages with very significant results. 
 
Mots-cl&#233;s - Keywords 
 
Recherche documentaire, mod&#232;le vectoriel, r&#233;duction de dimension, analyse factorielle, ACP, 
GHA, r&#233;seaux de neurones. 
Information retrieval, vector-space model, dimensionality reduction, data analysis, PCA, 
GHA, neural networks. 
 
 
 </p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Mathieu Delich&#232;re, Daniel Memmi 
</p>
<p>1  Introduction 
 
La croissance acc&#233;l&#233;r&#233;e d'Internet et du Web donne une importance grandissante au traitement 
num&#233;rique de documents textuels. Pour effectuer les diverses t&#226;ches de classification, 
recherche et filtrage de documents, il faut d'abord repr&#233;senter les textes de mani&#232;re &#224; la fois 
&#233;conomique et significative. On sait que le mod&#232;le vectoriel est probablement l'approche la 
plus courante : on repr&#233;sente un texte par un vecteur num&#233;rique obtenu en comptant les 
&#233;l&#233;ments lexicaux les plus pertinents (Salton &amp; McGill 1983)(Manning &amp; Sch&#252;tze 1999). 
Ces vecteurs sont fournis par des pr&#233;traitements simples. On commence g&#233;n&#233;ralement par 
&#233;liminer les mots grammaticaux (articles, pr&#233;positions, etc.) et par r&#233;duire les variantes 
morphologiques &#224; une forme commune (souvent appel&#233;e terme). Puis on compte les 
occurrences des termes les plus importants de mani&#232;re &#224; repr&#233;senter chaque document par un 
vecteur dans l'espace des termes. Un corpus de documents donne donc lieu &#224; une matrice 
document-terme (Fig. 1). Cette repr&#233;sentation assez simpliste permet ensuite d'appliquer les 
op&#233;rations vectorielles usuelles avec des r&#233;sultats s&#233;mantiquement pertinents dans l'ensemble. 
 
 
   terme  1 terme  2 terme 3 ... terme n 
 
 document 1 x11  x12  x13  ... x1n 
 document 2 x21  x22  x23  ... x2n 
 ...  ...  ...  ...  ... ... 
 document m xm1  xm2  xm3  ... xmn 
 
 
</p>
<p>Fig. 1 : Matrice document-terme (xij = fr&#233;quence). 
 
 
Cette approche produit malheureusement des vecteurs lexicaux de tr&#232;s grande dimension (&#224; 
grand nombre de traits). Il est fr&#233;quent d'aboutir &#224; un lexique de trois &#224; cinq mille termes, et 
donc &#224; des vecteurs de m&#234;me dimension. De tels vecteurs sont co&#251;teux &#224; stocker et &#224; traiter. 
Ils sont pourtant tr&#232;s creux, car contenant g&#233;n&#233;ralement plus de 90% de valeurs nulles. 
Comme les termes sont aussi fortement corr&#233;l&#233;s entre eux dans un corpus donn&#233;, on a affaire 
&#224; une repr&#233;sentation tout &#224; fait redondante. 
De plus ces vecteurs redondants sont trop gros, inutilement d&#233;taill&#233;s et donc peu lisibles pour 
un utilisateur humain qui voudrait s'en servir pour &#233;valuer rapidement le contenu d'un 
document et chercher &#224; voir les relations entre divers documents. En particulier les th&#232;mes 
caract&#233;ristiques d'un corpus ne ressortent pas de mani&#232;re &#233;vidente dans une telle 
repr&#233;sentation brute. 
Enfin les corr&#233;lations entre traits lexicaux r&#233;v&#232;lent des synonymies entre termes, qui ont des 
cons&#233;quences n&#233;gatives pour l'indexation et la recherche. On sait en effet que des documents 
voisins s&#233;mantiquement peuvent tr&#232;s bien ne pas contenir les m&#234;mes termes. D&#233;tecter les 
relations entre termes permettra d'am&#233;liorer la recherche de documents. 
Pour des raisons d'efficacit&#233; comme de lisibilit&#233; de la repr&#233;sentation, il serait donc utile de 
trouver une repr&#233;sentation plus compacte des documents. L'id&#233;e fondamentale est de trouver 
la dimension intrins&#232;que du domaine, c'est-&#224;-dire la dimension minimale permettant de 
repr&#233;senter les donn&#233;es sans perte d'information. 
Par ailleurs nos intuitions spatiales habituelles se r&#233;v&#232;lent fausses et trompeuses en grande 
dimension. En particulier on se heurte au ph&#233;nom&#232;ne de &quot;l'espace vide&quot; : le volume de 
l'espace total cro&#238;t tr&#232;s vite avec la dimension, ce qui fait que les donn&#233;es risquent de se </p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Analyse Factorielle Neuronale pour Documents Textuels 
</p>
<p>retrouver isol&#233;es dans cet espace. Il faudrait donc beaucoup plus de donn&#233;es pour bien 
repr&#233;senter un domaine. 
Pour toutes ces raisons, il est important d'essayer de r&#233;duire la dimension des donn&#233;es avant 
tout traitement ult&#233;rieur. La pr&#233;sence de corr&#233;lations multiples incite &#224; chercher de nouvelles 
dimensions plut&#244;t qu'une s&#233;lection parmi les traits originels. Il existe pour cela toute une 
gamme de m&#233;thodes permettant de calculer un nombre r&#233;duit de dimensions pour un 
ensemble de donn&#233;es. On les regroupe sous le nom g&#233;n&#233;ral d'analyse factorielle, mais la 
m&#233;thode la plus connue est l'Analyse en Composantes Principales (ACP). 
Malheureusement les r&#233;alisations courantes de l'ACP ne permettent pas en pratique de traiter 
des vecteurs de tr&#232;s grande dimension, ce qui est justement le cas dans le domaine des 
documents. Nous avons alors choisi d'utiliser une version neuronale de l'ACP, appel&#233;e 
Algorithme Hebbien G&#233;n&#233;ralis&#233;, permettant de traiter de tels vecteurs avec de bons r&#233;sultats. 
 
 
2  L'Analyse en Composantes Principales 
 
C'est la m&#233;thode d'analyse factorielle la plus utilis&#233;e (Bouroche &amp; Saporta 1980)(Jolliffe 
1986)(Lebart et al. 2000). L'ACP consiste &#224; calculer un nombre r&#233;duit de nouvelles 
dimensions, qui sont des combinaisons lin&#233;aires des dimensions originelles des donn&#233;es 
(c'est-&#224;-dire des traits descriptifs). Ces nouvelles dimensions sont non corr&#233;l&#233;es et expriment 
le maximum de variance des donn&#233;es (en partant de donn&#233;es centr&#233;es sur la moyenne). Les 
nouveaux axes sont les vecteurs propres, ordonn&#233;s par valeurs propres d&#233;croissantes, de la 
matrice de covariance des donn&#233;es. 
Autrement dit ce sont les principaux axes de dispersion du nuage de donn&#233;es, en ordre 
d'importance d&#233;croissante ; les valeurs propres correspondantes indiquent la part de variance 
exprim&#233;e par chaque axe. Les premiers axes rendent donc g&#233;n&#233;ralement compte de la plus 
grande partie de la variance. Les composantes principales sont les nouvelles valeurs des 
donn&#233;es sur chaque axe ainsi obtenu. 
Cette m&#233;thode peut jouer un double r&#244;le de compression des donn&#233;es et d'outil d'exploration 
dans des domaines fortement multidimensionnels. En effet les axes principaux ainsi calcul&#233;s 
permettent &#224; la fois une r&#233;duction des donn&#233;es et une interpr&#233;tation plus facile du domaine 
trait&#233;, car les nouvelles dimensions sont souvent tr&#232;s significatives. Cela peut notamment se 
r&#233;v&#233;ler int&#233;ressant pour l'analyse de donn&#233;es en langage naturel (Lebart &amp; Salem 1994). 
Le probl&#232;me est que l'ACP demande le calcul pr&#233;alable de la matrice carr&#233;e de covariance des 
donn&#233;es, qui est de taille n2 pour des vecteurs de dimension n. Cette matrice est d&#233;j&#224; co&#251;teuse 
&#224; calculer, et sa taille et son traitement deviennent prohibitifs en grande dimension. Ainsi des 
donn&#233;es de dimension 1000 donneraient lieu &#224; une matrice de un million d'&#233;l&#233;ments ! 
L'ACP est donc difficile ou impossible &#224; utiliser sur des vecteurs de documents textuels, dont 
on a vu qu'ils pouvaient comporter des milliers de traits. Diverses approches ont alors &#233;t&#233; 
propos&#233;es pour r&#233;duire la dimension des repr&#233;sentations vectorielles de textes. 
On peut citer notamment la m&#233;thode appel&#233;e Latent Semantic Indexing (LSI) ou Latent 
Semantic Analysis (LSA), qui n'utilise pas la matrice de covariance, mais extrait de nouveaux 
axes directement de la matrice document-terme (Deerwester et al. 1990). L'approche consiste 
&#224; effectuer une d&#233;composition en valeurs singuli&#232;res de la matrice des donn&#233;es pour trouver 
les nouveaux axes (voir Lebart et al. 2000). Cette technique peut &#234;tre consid&#233;r&#233;e comme une 
g&#233;n&#233;ralisation de l'extraction des vecteurs propres caract&#233;ristiques de l'ACP, g&#233;n&#233;ralisation 
permettant de traiter des matrices rectangulaires et des donn&#233;es non centr&#233;es. 
Comme le nombre de documents est souvent plus petit que le nombre de termes, cette matrice 
rectangulaire est moins encombrante et moins co&#251;teuse que la matrice de covariance. La 
m&#233;thode LSI semble donner de bons r&#233;sultats en pratique pour l'indexation et la recherche, </p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Mathieu Delich&#232;re, Daniel Memmi 
</p>
<p>mais contrairement &#224; l'ACP, la signification th&#233;orique des nouveaux axes est loin d'&#234;tre claire 
et ceux-ci sont peu intuitifs. D'autre part les co&#251;ts de calcul demeurent importants, car on 
manipule encore des matrices de grande taille. 
Nous avons donc d&#233;cid&#233; de rester dans le cadre de l'ACP et d'utiliser plut&#244;t une m&#233;thode 
neuronale it&#233;rative permettant de ne consid&#233;rer qu'un vecteur de donn&#233;es &#224; la fois sans jamais 
calculer la matrice de covariance. 
 
 
3  L'Algorithme Hebbien G&#233;n&#233;ralis&#233; 
 
Rappelons d'abord qu'un r&#233;seau neuronal (ou connexionniste) se compose de neurones 
formels connect&#233;s entre eux, par analogie avec la neurobiologie (Hertz et al. 1991)(H&#233;rault &amp; 
Jutten 1994). Un tel syst&#232;me consiste donc en un r&#233;seau de petits automates simples 
interconnect&#233;s, et c'est le fonctionnement de l'ensemble qui permet d'accomplir les t&#226;ches 
d&#233;sir&#233;es (souvent classification ou diagnostic). Chaque neurone formel calcule la somme 
pond&#233;r&#233;e de ses entr&#233;es et transmet son &#233;tat interne aux neurones auxquels il est connect&#233;. 
Certains neurones serviront d'entr&#233;e, et d'autres de sortie, mais le traitement est distribu&#233; sur 
l'ensemble du r&#233;seau. 
De plus chaque connexion entre neurones est affect&#233;e d'un poids modulant la transmission de 
l'activit&#233;. Ces poids sont ajust&#233;s graduellement par des proc&#233;dures d'apprentissage &#224; partir 
d'une pr&#233;sentation it&#233;rative des donn&#233;es. Cela permet d'adapter le syst&#232;me en fonction des 
entr&#233;es de mani&#232;re &#224; r&#233;soudre le probl&#232;me pos&#233;. Il existe diverses m&#233;thodes, mais 
l'apprentissage non supervis&#233; de type hebbien que nous allons utiliser ne n&#233;cessite que les 
donn&#233;es d'entr&#233;e, sans autre information. 
L'avantage des r&#233;seaux connexionnistes est de r&#233;partir un traitement complexe sur un 
ensemble de petits automates et d'ajuster automatiquement les param&#232;tres (les poids) du 
syst&#232;me de mani&#232;re it&#233;rative. Dans le cas qui nous int&#233;resse, on pourra donc ne consid&#233;rer 
qu'un vecteur d'entr&#233;e &#224; la fois, mais au prix d'un temps d'apprentissage plus ou moins long. 
On cherche ici non pas &#224; effectuer une classification, mais &#224; d&#233;tecter les corr&#233;lations entre les 
donn&#233;es. Le syst&#232;me va apprendre indirectement les corr&#233;lations entre traits d'un m&#234;me 
vecteur d'entr&#233;e et d'un vecteur &#224; l'autre en cherchant &#224; maximiser la sortie du r&#233;seau. 
Autrement dit, il va extraire les composantes principales. 
L'Algorithme Hebbien G&#233;n&#233;ralis&#233; est ainsi une variante neuronale de l'ACP &#233;labor&#233;e par 
Sanger sous le nom de Generalized Hebbian Algorithm (GHA). Mais c'est Oja qui proposa le 
premier une r&#232;gle d'apprentissage (Oja 1982) permettant &#224; un simple neurone lin&#233;aire 
d'extraire la premi&#232;re composante principale de donn&#233;es centr&#233;es : 
 
 w(t+1) = w(t) + &#951;(t) [y(t)x(t) &#8722; y2(t)w(t)] 
 
En notant les vecteurs en gras selon la convention usuelle, x est le vecteur d'entr&#233;e, w le 
vecteur de poids, y = w.x la sortie, et &#951; le pas d'apprentissage (typiquement choisi entre 0,1 et 
0,01). 
Le terme y(t)x(t) est de type hebbien, maximisant la variance de sortie, et y2(t)w(t) est un 
facteur de normalisation, con&#231;u pour assurer ||w|| = 1. 
 
Cette proc&#233;dure d'apprentissage a pour effet d'adapter les poids aux corr&#233;lations entre entr&#233;es 
et sortie, et du m&#234;me coup aux corr&#233;lations entre les donn&#233;es. L'apprentissage a donc pour 
effet d'amener it&#233;rativement le vecteur de poids &#224; repr&#233;senter le premier vecteur propre de la 
covariance des donn&#233;es. La sortie du neurone, projection du vecteur d'entr&#233;e sur le vecteur de </p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Analyse Factorielle Neuronale pour Documents Textuels 
</p>
<p>poids normalis&#233;, donne la nouvelle valeur de l'entr&#233;e le long de cet axe, c'est-&#224;-dire la 
premi&#232;re composante principale. 
 
 
</p>
<p>  xj
</p>
<p>yi
</p>
<p>wij
</p>
<p> 
 
   Fig. 2 : Sch&#233;ma d'un r&#233;seau GHA. 
 
 
Sanger a ensuite g&#233;n&#233;ralis&#233; cette r&#232;gle d'apprentissage &#224; un r&#233;seau &#224; une couche de neurones 
lin&#233;aires, de mani&#232;re &#224; pouvoir calculer les composantes principales successives (Sanger 
1989). Chaque neurone re&#231;oit le m&#234;me vecteur d'entr&#233;e et chacun extrait une composante 
principale en ordre d&#233;croissant de variance (Fig. 2). La r&#232;gle d'apprentissage est maintenant la 
suivante, en notation indic&#233;e : 
 
 wij(t+1) = wij(t) + &#951;(t) [yi(t)x'j(t) &#8722; yi2(t)wij(t)] 
 
chaque neurone successif utilisant une entr&#233;e modifi&#233;e x'j(t) calcul&#233;e en soustrayant de 
l'entr&#233;e r&#233;elle l'effet des neurones pr&#233;c&#233;dents : 
 
</p>
<p> x'j(t) = xj(t) &#8722; &#931;k&lt;i wkj(t)yk(t) 
 
L'apprentissage pour GHA peut aussi s'exprimer de mani&#232;re plus concise : 
 
</p>
<p> wij(t+1) = wij(t) + &#951;(t) yi(t) [xj(t) &#8722; &#931;k&#8804;i wkj(t)yk(t)] 
 
On d&#233;montre que cette r&#232;gle g&#233;n&#233;ralis&#233;e fait converger les poids vers les vecteurs propres de 
la covariance, par ordre d&#233;croissant des valeurs propres. L'algorithme ne demande qu'un 
vecteur de donn&#233;e &#224; la fois et &#233;vite donc le calcul de la matrice de covariance. On peut si on le 
souhaite le r&#233;aliser par des op&#233;rations purement locales en utilisant des connexions entre les 
neurones de sortie. 
Le r&#233;seau peut fonctionner en parall&#232;le (on ajuste alors  tous les poids en m&#234;me temps) mais il 
est plus simple et plus efficace de faire l'apprentissage de chaque neurone successivement. 
C'est la m&#233;thode que nous avons retenue. 
Il existe aussi des variantes de cet algorithme, notamment le syst&#232;me APEX utilisant des 
connexions lat&#233;rales pour exprimer l'influence d'un neurone sur les suivants (Diamantaras &amp; 
Kung 1996)(Fiori 2000). Mais ces variantes sont plus complexes et am&#233;liorent peu les 
performances de l'algorithme de base, que nous avons retenu ici. </p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Mathieu Delich&#232;re, Daniel Memmi 
</p>
<p> 
 
4  Traitement des documents 
 
Nous avons utilis&#233; un corpus de 90 pages Web environ, recueillies en vrac &#224; partir des signets 
favoris (bookmarks) d'un utilisateur r&#233;el. Chaque page Web contenait une page ou deux de 
texte ordinaire en fran&#231;ais, et l'ensemble touchait &#224; une dizaine de th&#232;mes diff&#233;rents (les 
centres d'int&#233;r&#234;t de l'utilisateur). Notre but &#233;tait d'effectuer une cat&#233;gorisation non supervis&#233;e 
(clustering) de ces pages pour &#233;tablir un profil de l'utilisateur dans le contexte du 
d&#233;veloppement d'un moteur de recherche collaboratif, le syst&#232;me Human Links. 
Nos choix ont &#233;t&#233; faits dans le cadre de ce syst&#232;me, et pour plus de d&#233;tails sur la r&#233;alisation du 
travail, voir (Delich&#232;re 2000). Pour plus d'information sur le syst&#232;me Human Links d&#233;velopp&#233; 
par la start-up Amoweba pour acc&#233;der &#224; une expertise distribu&#233;e sur Internet, on peut 
consulter le site Web : 
 
http://www.amoweba.com 
 
4.1 Pr&#233;traitements 
 
La vectorisation de chaque page Web a &#233;t&#233; faite selon une proc&#233;dure classique de mani&#232;re &#224; 
ne retenir que les mots ou termes les plus significatifs (Salton &amp; McGill 1983). On a d'abord 
enlev&#233; les mots grammaticaux (par simple consultation d'une liste), puis tronqu&#233; les mots 
lexicaux &#224; leur 5 premi&#232;res lettres pour r&#233;duire les variantes morphologique &#224; une forme 
unique (technique simpliste mais en g&#233;n&#233;ral efficace). On a compt&#233; ensuite les occurrences 
dans chaque document des termes ainsi retenus, et attribu&#233; &#224; chaque terme une valeur 
pond&#233;r&#233;e selon une variante simplifi&#233;e de la mesure TFIDF pour favoriser les termes les plus 
discriminants entre documents : 
 
fr&#233;quence dans le document / fr&#233;quence totale dans le corpus 
 
On a gard&#233; les termes d&#233;passant un seuil minimal de fr&#233;quence et seulement s'ils figuraient 
dans au moins dans 10 &#224; 20% des documents (pour assurer un niveau minimal de pertinence 
dans le corpus). Mais nous n'avons pas utilis&#233; de seuil maximal de fr&#233;quence, qui ne semblait 
pas utile pour un petit corpus assez h&#233;t&#233;rog&#232;ne. On a ainsi obtenu un vecteur num&#233;rique par 
document. 
Ces pr&#233;traitements simples et rapides, ne faisant appel qu'&#224; tr&#232;s peu de connaissances 
linguistiques, nous ont donn&#233; un espace de repr&#233;sentation de 600 traits lexicaux environ. C'est 
en fait assez bas pour ce domaine (o&#249; on atteint ais&#233;ment des dimensions de 3000 &#224; 5000 pour 
des corpus plus gros) mais cela reste une dimension &#233;lev&#233;e pour des traitements ult&#233;rieurs ou 
une analyse factorielle. 
 
4.2 R&#233;duction de dimension 
 
Les vecteurs repr&#233;sentant les documents ont alors &#233;t&#233; donn&#233;s &#224; un r&#233;seau GHA de 600 entr&#233;es 
et 20 neurones de sortie. Le nombre de sortie a &#233;t&#233; choisi empiriquement, en remarquant que 
des neurones suppl&#233;mentaires ne donnaient plus que des sorties tr&#232;s faibles. Pour acc&#233;l&#233;rer 
l'apprentissage, nous avons utilis&#233; une version s&#233;quentielle de GHA, en adaptant les poids de 
chaque neurone successivement et non en parall&#232;le. 
Nous avons exp&#233;riment&#233; avec diff&#233;rentes valeurs du pas d'apprentissage (de 0,1 &#224; 0,01) mais 
en le gardant fixe durant un apprentissage. Nous avons syst&#233;matiquement arr&#234;t&#233; </p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Analyse Factorielle Neuronale pour Documents Textuels 
</p>
<p>l'apprentissage apr&#232;s 3 mn de calcul (sur une station de travail) car l'algorithme devait servir 
en ligne dans un syst&#232;me de recherche interactif. 
Le r&#233;seau GHA a donc r&#233;duit &#224; 20 les 600 dimensions d'origine, projetant les documents dans 
un espace beaucoup plus compact. Ces dimensions se sont r&#233;v&#233;l&#233;es tr&#232;s significatives, ce qui 
nous a amen&#233;s &#224; les qualifier de concepts. En effet ces nouvelles dimensions repr&#233;sentent 
essentiellement les corr&#233;lations entre termes dans un corpus donn&#233;, r&#233;v&#233;lant de la sorte les 
th&#232;mes principaux du corpus presque aussi bien qu'une classification des documents. 
Voici les axes fournis par les neurones du r&#233;seau, correspondant donc aux composantes 
principales de plus forte variance. En repr&#233;sentant chaque axe par les termes contribuant le 
plus &#224; la nouvelle dimension, on reconna&#238;t sans mal des &#233;v&#233;nements r&#233;cents et les centres 
d'int&#233;r&#234;t divers de l'utilisateur des signets de pages : 
 
 
Axe 1 :  isra&#233;liens palestiniens barak sharon ehud 
Axe 2 :  neurone couche entr&#233;e poids matrice 
Axe 3 :  kasskooye manager newsletter incubation business 
Axe 4 :  bov&#233; jos&#233; alegre br&#233;sil porto 
Axe 5 :  kasskooye manager incubation business strategy 
Axe 6 :  hoax virus hoaxbuster hybris p&#233;tition 
Axe 7 :  hockey tennis s&#233;lectionner football 
Axe 8 :  kasskooye bov&#233; newsletter manager jos&#233; 
Axe 9 :  zdnet napster peer fichiers freenet 
Axe 10 :  algorithme individus g&#233;n&#233;tique g&#233;nome mutation 
Axe 11 :  hoax p&#233;tition hoaxbuster poulet algorithme 
Axe 12 :  hoax p&#233;tition hoaxbuster rallye hockey 
Axe 13 :  hybris virus infection code matrice 
Axe 14 :  artificielle robotique individus infection algorithme 
Axe 15 :  peer serveur joueurs client club 
Axe 16 :  lire mondialisation alegre porto janvier 
Axe 17 :  joueurs artificielle club robotique capitaine 
Axe 18 :  championnat zdnet mercato club pr&#233;sident 
Axe 19 :  rallye volley tennis s&#233;lectionner g&#233;nome 
Axe 20 :  zdnet capitaine poulet lire neurone 
 
 
On voit qu'un terme peut tr&#232;s bien contribuer &#224; des axes diff&#233;rents, repr&#233;sentant les divers 
faisceaux de corr&#233;lation auxquels ce mot appartient dans le corpus. On remarque aussi que les 
premiers axes sont th&#233;matiquement tr&#232;s coh&#233;rents, alors que les suivants deviennent plus 
difficiles &#224; interpr&#233;ter (axes 12, 19 ou 20 par exemple). Il vaudrait peut-&#234;tre mieux ne pas 
retenir les derniers axes. 
4.3 Classification 
 
Nous avons ensuite utilis&#233; les vecteurs dans le nouvel espace (donn&#233;s par les valeurs de sortie 
du r&#233;seau) pour effectuer une classification non supervis&#233;e des documents (Bouroche &amp; 
Saporta 1980)(Anderberg 1973). Nous avons essay&#233; plusieurs variantes de l'algorithme bien 
connu des centres mobiles (k-means) en employant une distance euclidienne (pour ne pas 
avoir &#224; normaliser les vecteurs). Mais nous aurions pu aussi bien utiliser des r&#233;seaux 
neuronaux comp&#233;titifs pour rester dans un cadre connexionniste (Memmi &amp; Meunier 2000). 
Les classes ainsi obtenues &#233;taient tout &#224; fait significatives, montrant clairement les th&#232;mes 
principaux du corpus de pages. Les classes correspondaient bien &#224; la dizaine de centres </p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Mathieu Delich&#232;re, Daniel Memmi 
</p>
<p>d'int&#233;r&#234;t qui avaient motiv&#233; la collection des signets de l'utilisateur humain : des articles 
d'acualit&#233; et de sport, la nouvelle &#233;conomie, les r&#233;seaux de neurones, les rumeurs et virus 
informatiques, le peer to peer, les algorithmes g&#233;n&#233;tiques... 
Plus pr&#233;cis&#233;ment, en demandant &#224; l'algorithme des centres mobiles de retrouver 10 classes, 
c'est-&#224;-dire le nombre de centres d'int&#233;r&#234;t de l'utilisateur, l'algorithme retrouve bien en g&#233;n&#233;ral 
(en faisant des tirages r&#233;p&#233;t&#233;s avec des initialisations al&#233;atoires vari&#233;es) la classification 
d'origine des documents par l'utilisateur. Il faut avouer cependant que le probl&#232;me de 
classement &#233;tait assez facile car cet ensemble de documents est particuli&#232;rement bien 
structur&#233; d'un point de vue th&#233;matique, ce qui voit d&#233;j&#224; dans les r&#233;sultats de l'analyse 
factorielle. 
Surtout cette classification apr&#232;s r&#233;duction s'est r&#233;v&#233;l&#233;e plus rapide et plus pertinente que dans 
l'espace d'origine. De fait, il aurait &#233;t&#233; impossible de classer correctement les vecteurs 
d'origine dans le temps limit&#233; impos&#233; par la t&#226;che interactive, puisque la classification devait 
se faire &#224; la demande et en ligne. 
On peut cependant noter ici le probl&#232;me classique d'&#233;valuation de la classification non 
supervis&#233;e. Contrairement &#224; la classification supervis&#233;e, il n'y malheureusement pas de 
crit&#232;res g&#233;n&#233;raux objectifs de qualit&#233; des classes obtenues, et il faut en fin de compte faire 
appel au jugement humain dans le contexte d'une t&#226;che particuli&#232;re. Mais le corpus choisi se 
pr&#234;tait bien &#224; une telle &#233;valuation s&#233;mantique, puisque les centres d'int&#233;r&#234;ts de l'utilisateur 
&#233;taient coh&#233;rents et assez bien &#233;quilibr&#233;s. 
En bref, la r&#233;duction de dimension permet non seulement une am&#233;lioration de l'efficacit&#233; 
informatique, mais elle donne aussi des r&#233;sultats int&#233;ressants d'un point de vue s&#233;mantique. 
Cela se comprend ais&#233;ment si on observe que les nouvelles dimensions correspondent aux 
corr&#233;lations les plus importantes dans les donn&#233;es, et donc &#224; des traits significatifs du corpus 
trait&#233;. L'ACP est ainsi un remarquable outil d'analyse th&#233;matique d'un corpus de documents. 
 
 
5  Discussion technique 
 
L'algorithme GHA a &#233;t&#233; g&#233;n&#233;ralement appliqu&#233; au traitement d'images, notamment pour une 
compression des images avec perte minimale d'information (codage optimal). A notre 
connaissance, c'est la premi&#232;re fois qu'on l'emploie pour le traitement de documents textuels. 
Nous avons pu v&#233;rifier que cette technique permet bien de traiter les vecteurs de grande 
dimension typiques des vecteurs de documents, alors que le traitement d'image se fait souvent 
avec des vecteurs beaucoup plus petits (de 64 valeurs par exemple en d&#233;coupant l'image en 
blocs analys&#233;s l'un apr&#232;s l'autre). 
Certes, la dimension des vecteurs que nous avons utilis&#233;s est encore relativement faible (600 
environ) compar&#233;e aux vecteurs de plusieurs milliers d'&#233;l&#233;ments souvent employ&#233;s en 
repr&#233;sentation de documents. Mais GHA a aussi &#233;t&#233; employ&#233; en traitement d'images pour des 
vecteurs de grande dimension (plusieurs milliers de traits), et nous sommes convaincus que de 
telles dimensions ne posent pas de probl&#232;mes majeurs. En effet le co&#251;t d'apprentissage et de 
fonctionnement est en premi&#232;re analyse simplement proportionnel &#224; la dimension du vecteur 
d'entr&#233;e. 
D'autre part, pour plus de clart&#233; th&#233;orique, nous avons fait une description de l'ACP standard 
portant sur des donn&#233;es centr&#233;es (de moyenne nulle). Mais diverses variantes sont possibles, 
et nous avons utilis&#233; en fait dans ce travail des donn&#233;es non centr&#233;es afin d'acc&#233;l&#233;rer les 
calculs. Dans ce cas, on ne peut plus parler de variance en toute rigueur, et les nouveaux axes 
passent par l'origine et non par le centre de gravit&#233; des donn&#233;es. Mais l'algorithme neuronal 
effectue toujours une r&#233;duction de dimension, et on a vu que les nouvelles dimensions 
obtenues &#233;taient tout &#224; fait significatives. </p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Analyse Factorielle Neuronale pour Documents Textuels 
</p>
<p>L'Algorithme Hebbien G&#233;n&#233;ralis&#233; permet donc bien d'extraire les composantes principales 
des vecteurs de tr&#232;s grande dimension caract&#233;ristiques des documents textuels, de mani&#232;re &#224; 
r&#233;duire la dimension de la repr&#233;sentation. 
De plus GHA ne calcule que les premi&#232;res composantes principales (les plus importantes), ce 
qui peut repr&#233;senter une &#233;conomie de calcul importante. La nature adaptative de l'algorithme 
permet aussi de se contenter d'une estimation approch&#233;e des r&#233;sultats (que l'on pourra affiner 
plus tard au besoin), alors que les logiciels standard d'ACP calculent tous les vecteurs propres 
avec la pr&#233;cision maximale. 
Cependant GHA pr&#233;sente aussi certains inconv&#233;nients. Comme souvent avec les techniques 
neuronales, le pas d'apprentissage doit &#234;tre estim&#233; empiriquement, et les temps 
d'apprentissage peuvent &#234;tre longs si on cherche une bonne pr&#233;cision, surtout pour les 
composantes suivantes de moindre variance. Et cet algorithme fait que les erreurs de calcul 
s'accumulent d'un neurone au suivant, ce qui diminue in&#233;vitablement la pr&#233;cision des 
composantes successives. Il est donc important en pratique de se limiter aux premi&#232;res 
composantes principales. 
Il faut rappeler que l'algorithme ne donne pas directement la part de variance correspondant &#224; 
chaque composante principale. On peut alors calculer facilement la variance de sortie de 
chaque neurone sur un &#233;chantillon du corpus de mani&#232;re &#224; trouver le nombre de composantes 
r&#233;ellement utiles (on arr&#234;tera l'apprentissage de nouveaux neurones lorsque leur variance 
devient insuffisante). 
En somme, GHA ne se justifie que lorsqu'on peut se contenter des premi&#232;res composantes 
principales. Mais c'est souvent le cas, puisqu'elles repr&#233;sentent la plus grosse part de 
l'information contenue dans les donn&#233;es. 
Enfin, tout comme l'ACP, GHA est une m&#233;thode purement lin&#233;aire qui ne peut capturer que 
des corr&#233;lations lin&#233;aires entre les donn&#233;es (cela revient en fait &#224; n'utiliser que la covariance). 
C'est probablement suffisant pour traiter des documents, mais si on veut d&#233;passer cette 
limitation, il faudrait envisager d'autres m&#233;thodes comme l'Analyse en Composantes 
Ind&#233;pendantes (voir H&#233;rault &amp; Jutten 1994) ou les Cartes de Kohonen (Ritter &amp; Kohonen 
1989)(Kohonen 1998). 
 
 
6  Conclusion 
 
Les vecteurs de grande dimension caract&#233;ristiques de la repr&#233;sentation des documents textuels 
sont tr&#232;s redondants et inutilement co&#251;teux. Pour repr&#233;senter l'information pertinente de 
mani&#232;re plus compacte, nous avons utilis&#233; une technique neuronale, l'Algorithme Hebbien 
G&#233;n&#233;ralis&#233; (GHA), qui permet d'extraire automatiquement les premi&#232;res composantes 
principales correspondant &#224; la majeure partie de l'information contenue dans les donn&#233;es. 
Cela permet une r&#233;duction de dimension &#224; la fois techniquement &#233;conomique et significative 
pour un utilisateur humain. 
L'algorithme calcule de nouvelles dimensions sans employer la matrice de covariance, ce qui 
permet d'envisager des vecteurs de tr&#232;s grande dimension qu'une ACP classique ne pourrait 
pas traiter. Nous avons trouv&#233; que cette technique connexionniste donnait de bons r&#233;sultats 
pour une t&#226;che de classification dans le cadre d'un syst&#232;me interactif r&#233;el. L'approche m&#233;rite 
donc d'&#234;tre consid&#233;r&#233;e pour r&#233;duire la dimension des donn&#233;es textuelles avant les traitements 
ult&#233;rieurs. 
Cette r&#233;alisation neuronale de l'ACP pr&#233;sente aussi l'int&#233;r&#234;t de faire &#233;merger automatiquement 
les th&#232;mes principaux d'un corpus de documents. C'est l&#224; un outil int&#233;ressant, efficace et 
relativement facile &#224; mettre en oeuvre pour l'analyse th&#233;matique d'un corpus de textes. Nous 
pensons qu'il y l&#224; mati&#232;re &#224; des travaux ult&#233;rieurs. </p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Mathieu Delich&#232;re, Daniel Memmi 
</p>
<p> 
 
R&#233;f&#233;rences 
 
Anderberg M.R. (1973) Cluster Analysis for Applications, Academic Press. 
Bouroche J.M. &amp; Saporta G. (1980) L'Analyse des Donn&#233;es, Que sais-je, PUF. 
Deerwester S., Dumais S.T., Furnas G.W., Landauer T.K. &amp; Hashman R. (1990) Indexing by 
latent semantic analysis, Journal of the American Society for Information Science 41 (6), p. 
391-407. 
</p>
<p>Delich&#232;re M. (2001) Etat de l'art et impl&#233;mentation d'algorithmes de recherche et de 
classification automatique de documents sur Internet - Int&#233;gration &#224; l'outil Human Links, 
rapport de stage de fin d'&#233;tude, EPITA, Paris. 
</p>
<p>Diamantaras K.I. &amp; Kung S.Y. (1996) Principal Component Neural Networks: Theory and 
Applications, John Wiley &amp; Sons. 
</p>
<p>Fiori S. (2000) An experimental comparison of three PCA neural networks, Neural 
Processing Letters 11 (3), p. 209-218. 
</p>
<p>Hertz J., Krogh A. &amp; Palmer R.G. (1991) Introduction to the Theory of Neural Computation, 
Addison Wesley. 
</p>
<p>H&#233;rault J. &amp; Jutten C. (1994) R&#233;seaux Neuronaux et Traitement du Signal, Herm&#232;s. 
Jolliffe I.T. (1986) Principal Component Analysis, Springer Verlag. 
Kohonen T. (1998) Self-organization of very large document collections: state of the art, 
Proc. of ICANN'98, London. 
</p>
<p>Lebart L., Morineau A. &amp; Piron M. (2000) Statistique Exploratoire Multidimensionnelle, 
Dunod. 
</p>
<p>Lebart L. &amp; Salem A. (1994) Statistique Textuelle, Dunod. 
Manning C.D. &amp; Sch&#252;tze H. (1999) Foundations of Statistical Natural Language Processing, 
MIT Press. 
</p>
<p>Memmi D. &amp; Meunier J.G. (2000) Using competitive networks for text mining, Proc. of 
Neural Computation 2000, Berlin. 
</p>
<p>Oja E. (1982) A simplified neuron model as a principal component analyzer, Journal of 
Mathematics and Biology 15, p. 267-273. 
</p>
<p>Ritter H. &amp; Kohonen T. (1989) Self-organizing semantic maps, Biological Cybernetics 61 
(4), p. 241-254. 
</p>
<p>Salton G. &amp; McGill M. (1983) Introduction to Modern Information Retrieval, McGraw-Hill. 
Sanger T.D. (1989) Optimal unsupervised learning in a single-layer linear feedforward neural 
network, Neural Networks 2 (6), p. 459-473. 
</p>
<p> </p>

</div></div>
</body></html>