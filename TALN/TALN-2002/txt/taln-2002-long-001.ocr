TALN 2002, Nancy, 24-2 7juin 2002

Analyse Factorielle Neuronale pour Documents Textuels

Mathieu Delichere (1) & Daniel Memmi (2)

(1) Amoweba
1 ave Berthollet, 74000 Annecy (France)
mathieu@amoweba.com
(2) Leibniz-Imag
46 ave Felix Viallet, 38000 Grenoble (France)
memmi@imag.fr

Résumé - Abstract

En recherche documentaire, on représente souvent les documents textuels par des vecteurs
lexicaux de grande dimension qui sont redondants et coﬁteux. Il est utile de réduire la
dimension des ces representations pour des raisons a la fois techniques et sémantiques.
Cependant les techniques classiques d'analyse factorielle comme l'ACP ne permettent pas de
traiter des vecteurs de tres grande dimension. Nous avons alors utilisé une méthode adaptative
neuronale (GHA) qui s'est révélée eff1cace pour calculer un nombre réduit de nouvelles
dimensions representatives des données. L'approche nous a permis de classer un corpus réel
de pages Web avec de bons résultats.

For document retrieval purposes, documents are often represented by high-dimensional
lexical vectors, which are costly and redundant. Reducing vector dimensionality is then useful
for both technical and semantic reasons. Classical data analysis methods such as PCA cannot
unfortunately process vectors of very high dimension. We have used instead an adaptive
neural network technique, the Generalized Hebbian Algorithm (GHA), which makes it
possible to reduce high-dimension spaces. This approach allowed us to cluster areal end-user
corpus of Web pages with very significant results.

Mots-clés - Keywords

Recherche documentaire, modele vectoriel, réduction de dimension, analyse factorielle, ACP,
GHA, réseaux de neurones.

Information retrieval, vector-space model, dimensionality reduction, data analysis, PCA,
GHA, neural networks.

Mathieu Delichére, Daniel Memmi

1 Introduction

La croissance acceleree d'Intemet et du Web donne une importance grandissante au traitement
numerique de documents textuels. Pour effectuer les diverses taches de classiﬁcation,
recherche et filtrage de documents, il faut d'abord representer les textes de maniere a la fois
economique et significative. On sait que le modele Vectoriel est probablement l'approche la
plus courante: on represente un texte par un Vecteur numerique obtenu en comptant les
elements lexicaux les plus pertinents (Salton & McGill l983)(Manning & Schiitze 1999).

Ces Vecteurs sont fournis par des pretraitements simples. On commence generalement par
eliminer les mots grammaticaux (articles, prepositions, etc.) et par reduire les Variantes
morphologiques a une forme commune (souvent appelee terme). Puis on compte les
occurrences des termes les plus importants de maniere a representer chaque document par un
Vecteur dans l'espace des termes. Un corpus de documents donne donc lieu a une matrice
document-terme (Fig. 1). Cette representation assez simpliste permet ensuite d'appliquer les
operations Vectorielles usuelles avec des resultats semantiquement pertinents dans l'ensemble.

terme I terme 2 terme 3  terme n
document] x1] x1; x13  xjn
document 2 x21 x22 x23  xgn
document m xm1 xmg xm3  xmn

Fig. 1 : Matrice document-terme (xij = frequence).

Cette approche produit malheureusement des Vecteurs lexicaux de tres grande dimension (a
grand nombre de traits). Il est frequent d'aboutir a un lexique de trois a cinq mille termes, et
done a des Vecteurs de meme dimension. De tels Vecteurs sont coﬁteux a stocker et a traiter.
Ils sont pourtant tres creux, car contenant generalement plus de 90% de Valeurs nulles.
Comme les termes sont aussi fortement correles entre eux dans un corpus donne, on a affaire
a une representation tout a fait redondante.

De plus ces Vecteurs redondants sont trop gros, inutilement detailles et donc peu lisibles pour
un utilisateur humain qui Voudrait s'en servir pour evaluer rapidement le contenu d'un
document et chercher a Voir les relations entre divers documents. En particulier les themes
caracteristiques d'un corpus ne ressortent pas de maniere evidente dans une telle
representation brute.

Enfin les correlations entre traits lexicaux revelent des synonymies entre termes, qui ont des
consequences negatives pour l'indexation et la recherche. On sait en effet que des documents
Voisins semantiquement peuvent tres bien ne pas contenir les memes termes. Detecter les
relations entre termes permettra d'ameliorer la recherche de documents.

Pour des raisons d'efficacite comme de lisibilite de la representation, il serait donc utile de
trouver une representation plus compacte des documents. L'idee fondamentale est de trouver
la dimension intrinseque du domaine, c'est-a-dire la dimension minimale permettant de
representer les donnees sans perte d'information.

Par ailleurs nos intuitions spatiales habituelles se revelent fausses et trompeuses en grande
dimension. En particulier on se heurte au phenomene de "l'espace Vide" : le Volume de
l'espace total croit tres Vite avec la dimension, ce qui fait que les donnees risquent de se

Analyse F actorielle Neuronale pour Documents T extuels

retrouver isolées dans cet espace. Il faudrait donc beaucoup plus de données pour bien
représenter un domaine.

Pour toutes ces raisons, il est important d'essayer de réduire la dimension des données avant
tout traitement ultérieur. La présence de corrélations multiples incite a chercher de nouvelles
dimensions plut6t qu'une sélection parmi les traits originels. Il existe pour cela toute une
gamme de méthodes permettant de calculer un nombre réduit de dimensions pour un
ensemble de données. On les regroupe sous le nom général d'analyse factorielle, mais la
méthode la plus connue est l'Analyse en Composantes Principales (ACP).

Malheureusement les réalisations courantes de l'ACP ne permettent pas en pratique de traiter
des vecteurs de tres grande dimension, ce qui est justement le cas dans le domaine des
documents. Nous avons alors choisi d'utiliser une version neuronale de l'ACP, appelée
Algorithme Hebbien Généralisé, permettant de traiter de tels vecteurs avec de bons résultats.

2 L'Analyse en Composantes Principales

C'est la méthode d'analyse factorielle la plus utilisée (Bouroche & Saporta l980)(Jolliffe
l986)(Lebart et al. 2000). L'ACP consiste a calculer un nombre réduit de nouvelles
dimensions, qui sont des combinaisons linéaires des dimensions originelles des données
(c'est-a-dire des traits descriptifs). Ces nouvelles dimensions sont non corrélées et expriment
le maximum de variance des données (en partant de données centrées sur la moyenne). Les
nouveaux axes sont les vecteurs propres, ordonnés par valeurs propres décroissantes, de la
matrice de covariance des données.

Autrement dit ce sont les principaux axes de dispersion du nuage de données, en ordre
d'importance décroissante ; les valeurs propres correspondantes indiquent la part de variance
exprimée par chaque axe. Les premiers axes rendent donc généralement compte de la plus
grande partie de la variance. Les composantes principales sont les nouvelles valeurs des
données sur chaque axe ainsi obtenu.

Cette méthode peut jouer un double role de compression des données et d'outil d'exploration
dans des domaines fortement multidimensionnels. En effet les axes principaux ainsi calculés
permettent a la fois une réduction des données et une interprétation plus facile du domaine
traité, car les nouvelles dimensions sont souvent tres significatives. Cela peut notamment se
révéler intéressant pour l'analyse de données en langage naturel (Lebart & Salem 1994).

Le probleme est que l'ACP demande le calcul préalable de la matrice carrée de covariance des
données, qui est de taille n2 pour des vecteurs de dimension n. Cette matrice est déja coﬁteuse
a calculer, et sa taille et son traitement deviennent prohibitifs en grande dimension. Ainsi des
données de dimension 1000 donneraient lieu a une matrice de un million d'éléments !

L'ACP est donc difﬁcile ou impossible a utiliser sur des vecteurs de documents textuels, dont
on a vu qu'ils pouvaient comporter des milliers de traits. Diverses approches ont alors été
proposées pour réduire la dimension des représentations vectorielles de textes.

On peut citer notamment la méthode appelée Latent Semantic Indexing (LS1) ou Latent
Semantic Analysis (LSA), qui n'utilise pas la matrice de covariance, mais extrait de nouveaux
axes directement de la matrice document-terme (Deerwester et al. 1990). L'approche consiste
a effectuer une décomposition en valeurs singulieres de la matrice des données pour trouver
les nouveaux axes (voir Lebart et al. 2000). Cette technique peut étre considérée comme une
généralisation de l'extraction des vecteurs propres caractéristiques de l'ACP, généralisation
permettant de traiter des matrices rectangulaires et des données non centrées.

Comme le nombre de documents est souvent plus petit que le nombre de termes, cette matrice
rectangulaire est moins encombrante et moins coﬁteuse que la matrice de covariance. La
méthode LS1 semble donner de bons résultats en pratique pour l'indexation et la recherche,

Mathieu Delichére, Daniel Memmi

mais contrairement a l'ACP, la signiﬁcation théorique des nouveaux axes est loin d'étre claire
et ceux-ci sont peu intuitifs. D'autre part les coﬁts de calcul demeurent importants, car on
manipule encore des matrices de grande taille.

Nous avons donc décidé de rester dans le cadre de l'ACP et d'utiliser plut6t une méthode
neuronale itérative permettant de ne considérer qu'un vecteur de données a la fois sans jamais
calculer la matrice de covariance.

3 L'Algorithme Hebbien Généralisé

Rappelons d'abord qu'un réseau neuronal (ou connexionniste) se compose de neurones
formels connectés entre eux, par analogie avec la neurobiologie (Hertz et al. l99l)(Hérault &
Jutten 1994). Un tel systeme consiste donc en un réseau de petits automates simples
interconnectés, et c'est le fonctionnement de l'ensemble qui permet d'accomplir les taches
désirées (souvent classiﬁcation ou diagnostic). Chaque neurone formel calcule la somme
pondérée de ses entrées et transmet son état interne aux neurones auxquels il est connecté.
Certains neurones serviront d'entrée, et d'autres de sortie, mais le traitement est distribué sur
l'ensemble du réseau.

De plus chaque connexion entre neurones est affectée d'un poids modulant la transmission de
l'activité. Ces poids sont ajustés graduellement par des procédures d'apprentissage a partir
d'une présentation itérative des données. Cela permet d'adapter le systeme en fonction des
entrées de maniere a résoudre le probleme posé. Il existe diverses méthodes, mais
l'apprentissage non supervisé de type hebbien que nous allons utiliser ne nécessite que les
données d'entrée, sans autre information.

L'avantage des réseaux connexionnistes est de répartir un traitement complexe sur un
ensemble de petits automates et d'ajuster automatiquement les parametres (les poids) du
systeme de maniere itérative. Dans le cas qui nous intéresse, on pourra donc ne considérer
qu'un vecteur d'entrée a la fois, mais au prix d'un temps d'apprentissage plus ou moins long.
On cherche ici non pas a effectuer une classification, mais a détecter les corrélations er1tre les
données. Le systeme Va apprendre indirectement les corrélations entre traits d'un méme
vecteur d'entrée et d'un vecteur a l'autre en cherchant a maximiser la sortie du réseau.
Autrement dit, il Va extraire les composantes principales.

L'Algorithme Hebbien Généralisé est ainsi une variante neuronale de l'ACP élaborée par
Sanger sous le nom de Generalized Hebbian Algorithm (GHA). Mais c'est Oja qui proposa le
premier une regle d'apprentissage (Oja 1982) permettant a un simple neurone linéaire
d'extraire la premiere composante principale de données centrées :

W(t+1) = W(t) + T1(t) [y(t)x(t) — Y2(‘E)W(‘E)]

En notant les vecteurs en gras selon la convention usuelle, x est le vecteur d'entrée, w le
vecteur de poids, y = w.x la sortie, et 1'] le pas d'apprentissage (typiquement choisi entre 0,1 et
0,01).

Le terme y(t)x(t) est de type hebbien, maximisant la variance de sortie, et y2(t)w(t) est un
facteur de normalisation, concu pour assurer ||w|| = 1.

Cette procédure d'apprentissage a pour effet d'adapter les poids aux corrélations entre entrées
et sortie, et du méme coup aux corrélations entre les données. L'apprentissage a donc pour
effet d'amener itérativement le vecteur de poids a représenter le premier vecteur propre de la
covariance des données. La sortie du neurone, projection du vecteur d'entrée sur le vecteur de

Analyse F actorielle Neuronale pour Documents T extuels

poids normalisé, donne la nouvelle Valeur de l'entrée le long de cet axe, c'est-a-dire la
premiere composante principale.

6 —>O—>O
 ’/7

Xi

Fig. 2 : Schéma d'un réseau GHA.

Sanger a ensuite généralisé cette regle d'apprentissage a un réseau a une couche de neurones
linéaires, de maniere a pouvoir calculer les composantes principales successives (Sanger
1989). Chaque neurone recoit le méme Vecteur d'entrée et chacun extrait une composante
principale en ordre décroissant de Variance (Fig. 2). La regle d'apprentissage est maintenant la
suivante, en notation indicée :

Wij(t+1) = Wij(t) + T1(t) [yi(t)X'j(t) - yi2(t)Wij(t)]

chaque neurone successif utilisant une entrée modiﬁée x'j(t) calculée en soustrayant de
l'entrée réelle l'effet des neurones précédents :

x5-ct) = x.-ct) ~ Em Wkj(t)yk(t)

L'apprentissage pour GHA peut aussi s'exprimer de maniere plus concise :

Wij(t+1) = wg-(t) + n(t) Yi(t) [xj(t) — Zksi Wkj(t)Yk(t)]

On démontre que cette regle généralisée fait converger les poids Vers les Vecteurs propres de
la covariance, par ordre décroissant des Valeurs propres. L'algorithme ne demande qu'un
Vecteur de donnée a la fois et évite donc le calcul de la matrice de covariance. On peut si on le
souhaite le réaliser par des opérations purement locales en utilisant des connexions entre les
neurones de sortie.

Le réseau peut fonctionner en parallele (on ajuste alors tous les poids en méme temps) mais il
est plus simple et plus efficace de faire l'apprentissage de chaque neurone successivement.
C'est la méthode que nous avons retenue.

Il existe aussi des Variantes de cet algorithme, notamment le systeme APEX utilisant des
connexions latérales pour exprimer l'inﬂuence d'un neurone sur les suivants (Diamantaras &
Kung l996)(Fiori 2000). Mais ces Variantes sont plus complexes et améliorent peu les
performances de l'algorithme de base, que nous avons retenu ici.

Mathieu Delichére, Daniel Memmi

4 Traitement des documents

Nous avons utilise un corpus de 90 pages Web environ, recueillies en vrac a partir des signets
favoris (bookmarks) d'un utilisateur reel. Chaque page Web contenait une page ou deux de
texte ordinaire en francais, et l'ensemble touchait a une dizaine de themes differents (les
centres d'interet de l'utilisateur). Notre but etait d'effectuer une categorisation non supervisee
(clustering) de ces pages pour etablir un profil de l'utilisateur dans le contexte du
developpement d'un moteur de recherche collaboratif, le systeme Human Links.

Nos choix ont ete faits dans le cadre de ce systeme, et pour plus de details sur la realisation du
travail, voir (Delichere 2000). Pour plus d'information sur le systeme Human Links developpe
par la start-up Amoweba pour acceder a une expertise distribuee sur Internet, on peut
consulter le site Web :

http://www.amoweba.com
4.1 Prétraitements

La Vectorisation de chaque page Web a ete faite selon une procedure classique de maniere a
ne retenir que les mots ou termes les plus signiﬁcatifs (Salton & McGill 1983). On a d'abord
enleve les mots grammaticaux (par simple consultation d'une liste), puis tronque les mots
lexicaux a leur 5 premieres lettres pour reduire les variantes morphologique a une forme
unique (technique simpliste mais en general efﬁcace). On a compte ensuite les occurrences
dans chaque document des termes ainsi retenus, et attribue a chaque terme une valeur
ponderee selon une variante simpliﬁee de la mesure TFIDF pour favoriser les termes les plus
discriminants entre documents :

frequence dans le document / frequence totale dans le corpus

On a garde les termes depassant un seuil minimal de frequence et seulement s'ils ﬁguraient
dans au moins dans 10 a 20% des documents (pour assurer un niveau minimal de pertinence
dans le corpus). Mais nous n'avons pas utilise de seuil maximal de frequence, qui ne semblait
pas utile pour un petit corpus assez heterogene. On a ainsi obtenu un vecteur numerique par
document.

Ces pretraitements simples et rapides, ne faisant appel qu'a tres peu de connaissances
linguistiques, nous ont donne un espace de representation de 600 traits lexicaux environ. C'est
en fait assez bas pour ce domaine (ou on atteint aisement des dimensions de 3000 a 5000 pour
des corpus plus gros) mais cela reste une dimension elevee pour des traitements ulterieurs ou
une analyse factorielle.

4.2 Reduction de dimension

Les vecteurs representant les documents ont alors ete donnes a un reseau GHA de 600 entrees
et 20 neurones de sortie. Le nombre de sortie a ete choisi empiriquement, en remarquant que
des neurones supplementaires ne donnaient plus que des sorties tres faibles. Pour accelerer
l'apprentissage, nous avons utilise une version sequentielle de GHA, en adaptant les poids de
chaque neurone successivement et non en parallele.

Nous avons experimente avec differentes valeurs du pas d'apprentissage (de 0,1 a 0,01) mais
en le gardant fixe durant un apprentissage. Nous avons systematiquement arrete

Analyse F actorielle Neuronale pour Documents T extuels

l'apprentissage apres 3 mn de calcul (sur une station de travail) car l'algorithme devait servir
en ligne dans un systeme de recherche interactif.

Le réseau GHA a donc réduit a 20 les 600 dimensions d'origine, projetant les documents dans
un espace beaucoup plus compact. Ces dimensions se sont révélées tres significatives, ce qui
nous a amenés a les qualifier de concepts. En effet ces nouvelles dimensions représentent
essentiellement les corrélations entre termes dans un corpus donné, révélant de la sorte les
themes principaux du corpus presque aussi bien qu'une classification des documents.

Voici les axes fournis par les neurones du réseau, correspondant donc aux composantes
principales de plus forte variance. En représentant chaque axe par les termes contribuant le
plus a la nouvelle dimension, on reconnait sans mal des événements récents et les centres
d'intérét divers de l'utilisateur des signets de pages :

Axe 1 : israéliens palestiniens barak sharon ehud

Axe 2 : neurone couche entrée poids matrice

Axe 3 : kasskooye manager newsletter incubation business
Axe 4 : bové josé alegre brésil porto

Axe 5 : kasskooye manager incubation business strategy
Axe 6 : hoax virus hoaxbuster hybris pétition

Axe 7 : hockey tennis sélectionner football

Axe 8 : kasskooye bové newsletter manager josé

Axe 9 : zdnet napster peer fichiers freenet

Axe 10 : algorithme individus génétique génome mutation
Axe 11 : hoax pétition hoaxbuster poulet algorithme

Axe 12 : hoax pétition hoaxbuster rallye hockey

Axe 13 : hybris virus infection code matrice

Axe 14 : artificielle robotique individus infection algorithme
Axe 15 : peer serveur joueurs client club

Axe 16 : lire mondialisation alegre porto janvier

Axe 17 : joueurs artificielle club robotique capitaine

Axe 18 : championnat zdnet mercato club président

Axe 19 : rallye volley tennis sélectionner génome

Axe 20 : zdnet capitaine poulet lire neurone

On voit qu'un terme peut tres bien contribuer a des axes différents, représentant les divers
faisceaux de corrélation auxquels ce mot appartient dans le corpus. On remarque aussi que les
premiers axes sont thématiquement tres cohérents, alors que les suivants deviennent plus
difficiles a interpréter (axes 12, 19 ou 20 par exemple). Il vaudrait peut-étre mieux ne pas
retenir les derniers axes.

4.3 Classiﬁcation

Nous avons ensuite utilisé les vecteurs dans le nouvel espace (donnés par les valeurs de sortie
du réseau) pour effectuer une classification non supervisée des documents (Bouroche &
Saporta 1980)(Anderberg 1973). Nous avons essayé plusieurs variantes de l'algorithme bien
connu des centres mobiles (k-means) en employant une distance euclidienne (pour ne pas
avoir a normaliser les vecteurs). Mais nous aurions pu aussi bien utiliser des réseaux
neuronaux compétitifs pour rester dans un cadre connexionniste (1\/Iemmi & Meunier 2000).

Les classes ainsi obtenues étaient tout a fait significatives, montrant clairement les themes
principaux du corpus de pages. Les classes correspondaient bien a la dizaine de centres

Mathieu Delichére, Daniel Memmi

d'intérét qui avaient motivé la collection des signets de l'utilisateur humain: des articles
d'acualité et de sport, la nouvelle économie, les réseaux de neurones, les rumeurs et virus
informatiques, le peer to peer, les algorithmes génétiques...

Plus précisément, en demandant a l'algorithme des centres mobiles de retrouver 10 classes,
c'est-a-dire le nombre de centres d'intérét de l'utilisateur, l'algorithme retrouve bien en général
(en faisant des tirages répétés avec des initialisations aléatoires variées) la classiﬁcation
d'origine des documents par l'utilisateur. Il faut avouer cependant que le probleme de
classement était assez facile car cet ensemble de documents est particulierement bien
structuré d'un point de vue thématique, ce qui voit déja dans les résultats de l'analyse
factorielle.

Surtout cette classification apres réduction s'est révélée plus rapide et plus pertinente que dans
l'espace d'origine. De fait, il aurait été impossible de classer correctement les vecteurs
d'origine dans le temps limité imposé par la tache interactive, puisque la classiﬁcation devait
se faire a la demande et en ligne.

On peut cependant noter ici le probleme classique d'évaluation de la classiﬁcation non
supervisée. Contrairement a la classiﬁcation supervisée, il n'y malheureusement pas de
criteres généraux objectifs de qualité des classes obtenues, et il faut en fin de compte faire
appel au jugement humain dans le contexte d'une tache particuliere. Mais le corpus choisi se
prétait bien a une telle évaluation sémantique, puisque les centres d'intéréts de l'utilisateur
étaient cohérents et assez bien équilibrés.

En bref, la réduction de dimension permet non seulement une amélioration de l'efficacité
informatique, mais elle donne aussi des résultats intéressants d'un point de vue sémantique.
Cela se comprend aisément si on observe que les nouvelles dimensions correspondent aux
corrélations les plus importantes dans les données, et donc a des traits significatifs du corpus
traité. L'ACP est ainsi un remarquable outil d'analyse thématique d'un corpus de documents.

5 Discussion technique

L'algorithme GHA a été généralement appliqué au traitement d'images, notamment pour une
compression des images avec perte minimale d'information (codage optimal). A notre
connaissance, c'est la premiere fois qu'on l'emploie pour le traitement de documents textuels.
Nous avons pu vérifier que cette technique permet bien de traiter les vecteurs de grande
dimension typiques des vecteurs de documents, alors que le traitement d'image se fait souvent
avec des vecteurs beaucoup plus petits (de 64 valeurs par exemple en découpant l'image en
blocs analysés l'un apres l'autre).

Certes, la dimension des vecteurs que nous avons utilisés est encore relativement faible (600
environ) comparée aux vecteurs de plusieurs milliers d'éléments souvent employés en
représentation de documents. Mais GHA a aussi été employé en traitement d'images pour des
vecteurs de grande dimension (plusieurs milliers de traits), et nous sommes convaincus que de
telles dimensions ne posent pas de problemes majeurs. En effet le coﬁt d'apprentissage et de
fonctionnement est en premiere analyse simplement proportionnel a la dimension du vecteur
d'entrée.

D'autre part, pour plus de clarté théorique, nous avons fait une description de l'ACP standard
portant sur des données centrées (de moyenne nulle). Mais diverses variantes sont possibles,
et nous avons utilisé en fait dans ce travail des données non centrées afin d'accélérer les
calculs. Dans ce cas, on ne peut plus parler de variance en toute rigueur, et les nouveaux axes
passent par l'origine et non par le centre de gravité des données. Mais l'algorithme neuronal
effectue toujours une réduction de dimension, et on a vu que les nouvelles dimensions
obtenues étaient tout a fait significatives.

Analyse F actorielle Neuronale pour Documents T extuels

L'Algorithme Hebbien Généralisé permet donc bien d'extraire les composantes principales
des vecteurs de tres grande dimension caractéristiques des documents textuels, de maniere a
réduire la dimension de la représentation.

De plus GHA ne calcule que les premieres composantes principales (les plus importantes), ce
qui peut représenter une économie de calcul importante. La nature adaptative de l'algorithme
permet aussi de se contenter d'une estimation approchée des résultats (que l'on pourra afﬁner
plus tard au besoin), alors que les logiciels standard d'ACP calculent tous les vecteurs propres
avec la précision maximale.

Cependant GHA présente aussi certains inconvénients. Comme souvent avec les techniques
neuronales, le pas d'apprentissage doit étre estimé empiriquement, et les temps
d'apprentissage peuvent étre longs si on cherche une bonne précision, surtout pour les
composantes suivantes de moindre variance. Et cet algorithme fait que les erreurs de calcul
s'accumulent d'un neurone au suivant, ce qui diminue inévitablement la précision des
composantes successives. Il est donc important en pratique de se limiter aux premieres
composantes principales.

Il faut rappeler que l'algorithme ne donne pas directement la part de variance correspondant a
chaque composante principale. On peut alors calculer facilement la variance de sortie de
chaque neurone sur un échantillon du corpus de maniere a trouver le nombre de composantes
réellement utiles (on arrétera l'apprentissage de nouveaux neurones lorsque leur variance
devient insufﬁsante).

En somme, GHA ne se justiﬁe que lorsqu'on peut se contenter des premieres composantes
principales. Mais c'est souvent le cas, puisqu'elles représentent la plus grosse part de
l'information contenue dans les données.

Enﬁn, tout comme l'ACP, GHA est une méthode purement linéaire qui ne peut capturer que
des corrélations linéaires entre les données (cela revient en fait a n'utiliser que la covariance).
C'est probablement sufﬁsant pour traiter des documents, mais si on veut dépasser cette
limitation, il faudrait envisager d'autres méthodes comme l'Analyse en Composantes
Indépendantes (voir Hérault & Jutten 1994) ou les Cartes de Kohonen (Ritter & Kohonen
l989)(Kohonen 1998).

6 Conclusion

Les vecteurs de grande dimension caractéristiques de la représentation des documents textuels
sont tres redondants et inutilement coﬁteux. Pour représenter l'information pertinente de
maniere plus compacte, nous avons utilisé une technique neuronale, l'Algorithme Hebbien
Généralisé (GHA), qui permet d'extraire automatiquement les premieres composantes
principales correspondant a la majeure partie de l'information contenue dans les données.
Cela permet une réduction de dimension a la fois techniquement économique et signiﬁcative
pour un utilisateur humain.

L'algorithme calcule de nouvelles dimensions sans employer la matrice de covariance, ce qui
permet d'envisager des vecteurs de tres grande dimension qu'une ACP classique ne pourrait
pas traiter. Nous avons trouvé que cette technique connexionniste donnait de bons résultats
pour une tache de classiﬁcation dans le cadre d'un systeme interactif réel. L'approche mérite
donc d'étre considérée pour réduire la dimension des données textuelles avant les traitements
ultérieurs.

Cette réalisation neuronale de l'ACP présente aussi l'intérét de faire émerger automatiquement
les themes principaux d'un corpus de documents. C'est la un outil intéressant, efﬁcace et
relativement facile a mettre en oeuvre pour l'analyse thématique d'un corpus de textes. Nous
pensons qu'il y la matiere a des travaux ultérieurs.

Mathieu Deliche‘re, Daniel Memmi

Références

Anderberg M.R. (1973) Cluster Anab/sis for Applications, Academic Press.

Bouroche J.M. & Saporta G. (1980) L'Anab/se des Données, Que sais-je, PUF.

Deerwester S., Dumais S.T., Furnas G.W., Landauer T.K. & Hashman R. (1990) Indexing by
latent semantic analysis, Journal of the American Society for Information Science 41 (6), p.
391-407.

Delichere M. (2001) Etat de l'art et implémentation d'algorithmes de recherche et de
classiﬁcation automatique de documents sur Internet - Intégration a l'outil Human Links,
rapport de stage de ﬁn d'étude, EPITA, Paris.

Diamantaras K.I. & Kung S.Y. (1996) Principal Component Neural Networks: Ilzeory and
Applications, John Wiley & Sons.

Fiori S. (2000) An experimental comparison of three PCA neural networks, Neural
ProcessingLetters 11 (3), p. 209-218.

Hertz J., Krogh A. & Palmer R.G. (1991) Introduction to the Ilzeory of Neural Computation,
Addison Wesley.

Hérault J. & Jutten C. (1994) Réseaux Neuronaux et T raitement du Signal, Hermes.

Jolliffe I.T. (1986) Principal Component Analysis, Springer Verlag.

Kohonen T. (1998) Self-organization of Very large document collections: state of the art,
Proc. of ICANN '98, London.

Lebart L., Morineau A. & Piron M. (2000) Statistique Exploratoire Multidimensionnelle,
Dunod.

Lebart L. & Salem A. (1994) Statistique T extuelle, Dunod.

Manning C.D. & Schutze H. (1999) Foundations of Statistical Natural Language Processing,
MIT Press.

Memmi D. & Meunier J.G. (2000) Using competitive networks for text mining, Proc. of
Neural Computation 2000, Berlin.

Oja E. (1982) A simplified neuron model as a principal component analyzer, Journal of
Mathematics and Biology 15, p. 267-273.

Ritter H. & Kohonen T. (1989) Self-organizing semantic maps, Biological Cybernetics 61
(4), p. 241-254.

Salton G. & McGill M. (1983) Introduction to Modern Information Retrieval, McGraw-Hill.

Sanger T.D. (1989) Optimal unsupervised learning in a single-layer linear feedforward neural
network, Neural Networks 2 (6), p. 459-473.

