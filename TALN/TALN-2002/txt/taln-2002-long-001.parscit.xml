<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<title>pensons qu&apos;il y là matière à des travaux ultérieurs. Mathieu Delichère, Daniel Memmi Références Anderberg M.R.</title>
<date>1973</date>
<publisher>Academic Press.</publisher>
<marker>1973</marker>
<rawString>pensons qu&apos;il y là matière à des travaux ultérieurs. Mathieu Delichère, Daniel Memmi Références Anderberg M.R. (1973) Cluster Analysis for Applications, Academic Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J M Bouroche</author>
<author>G Saporta</author>
</authors>
<title>L&apos;Analyse des Données, Que sais-je,</title>
<date>1980</date>
<publisher>PUF.</publisher>
<contexts>
<context position="6205" citStr="Bouroche &amp; Saporta 1980" startWordPosition="914" endWordPosition="917">nées. On les regroupe sous le nom général d&apos;analyse factorielle, mais la méthode la plus connue est l&apos;Analyse en Composantes Principales (ACP). Malheureusement les réalisations courantes de l&apos;ACP ne permettent pas en pratique de traiter des vecteurs de très grande dimension, ce qui est justement le cas dans le domaine des documents. Nous avons alors choisi d&apos;utiliser une version neuronale de l&apos;ACP, appelée Algorithme Hebbien Généralisé, permettant de traiter de tels vecteurs avec de bons résultats. 2 L&apos;Analyse en Composantes Principales C&apos;est la méthode d&apos;analyse factorielle la plus utilisée (Bouroche &amp; Saporta 1980)(Jolliffe 1986)(Lebart et al. 2000). L&apos;ACP consiste à calculer un nombre réduit de nouvelles dimensions, qui sont des combinaisons linéaires des dimensions originelles des données (c&apos;est-à-dire des traits descriptifs). Ces nouvelles dimensions sont non corrélées et expriment le maximum de variance des données (en partant de données centrées sur la moyenne). Les nouveaux axes sont les vecteurs propres, ordonnés par valeurs propres décroissantes, de la matrice de covariance des données. Autrement dit ce sont les principaux axes de dispersion du nuage de données, en ordre d&apos;importance décroissant</context>
<context position="20086" citStr="Bouroche &amp; Saporta 1980" startWordPosition="3051" endWordPosition="3054"> lire neurone On voit qu&apos;un terme peut très bien contribuer à des axes différents, représentant les divers faisceaux de corrélation auxquels ce mot appartient dans le corpus. On remarque aussi que les premiers axes sont thématiquement très cohérents, alors que les suivants deviennent plus difficiles à interpréter (axes 12, 19 ou 20 par exemple). Il vaudrait peut-être mieux ne pas retenir les derniers axes. 4.3 Classification Nous avons ensuite utilisé les vecteurs dans le nouvel espace (donnés par les valeurs de sortie du réseau) pour effectuer une classification non supervisée des documents (Bouroche &amp; Saporta 1980)(Anderberg 1973). Nous avons essayé plusieurs variantes de l&apos;algorithme bien connu des centres mobiles (k-means) en employant une distance euclidienne (pour ne pas avoir à normaliser les vecteurs). Mais nous aurions pu aussi bien utiliser des réseaux neuronaux compétitifs pour rester dans un cadre connexionniste (Memmi &amp; Meunier 2000). Les classes ainsi obtenues étaient tout à fait significatives, montrant clairement les thèmes principaux du corpus de pages. Les classes correspondaient bien à la dizaine de centres Mathieu Delichère, Daniel Memmi d&apos;intérêt qui avaient motivé la collection des s</context>
</contexts>
<marker>Bouroche, Saporta, 1980</marker>
<rawString>Bouroche J.M. &amp; Saporta G. (1980) L&apos;Analyse des Données, Que sais-je, PUF.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Deerwester</author>
<author>S T Dumais</author>
<author>G W Furnas</author>
<author>T K Landauer</author>
<author>R Hashman</author>
</authors>
<title>Indexing by latent semantic analysis,</title>
<date>1990</date>
<journal>Journal of the American Society for Information Science</journal>
<volume>41</volume>
<issue>6</issue>
<pages>391--407</pages>
<contexts>
<context position="8431" citStr="Deerwester et al. 1990" startWordPosition="1249" endWordPosition="1252">de dimension. Ainsi des données de dimension 1000 donneraient lieu à une matrice de un million d&apos;éléments ! L&apos;ACP est donc difficile ou impossible à utiliser sur des vecteurs de documents textuels, dont on a vu qu&apos;ils pouvaient comporter des milliers de traits. Diverses approches ont alors été proposées pour réduire la dimension des représentations vectorielles de textes. On peut citer notamment la méthode appelée Latent Semantic Indexing (LSI) ou Latent Semantic Analysis (LSA), qui n&apos;utilise pas la matrice de covariance, mais extrait de nouveaux axes directement de la matrice document-terme (Deerwester et al. 1990). L&apos;approche consiste à effectuer une décomposition en valeurs singulières de la matrice des données pour trouver les nouveaux axes (voir Lebart et al. 2000). Cette technique peut être considérée comme une généralisation de l&apos;extraction des vecteurs propres caractéristiques de l&apos;ACP, généralisation permettant de traiter des matrices rectangulaires et des données non centrées. Comme le nombre de documents est souvent plus petit que le nombre de termes, cette matrice rectangulaire est moins encombrante et moins coûteuse que la matrice de covariance. La méthode LSI semble donner de bons résultats</context>
</contexts>
<marker>Deerwester, Dumais, Furnas, Landauer, Hashman, 1990</marker>
<rawString>Deerwester S., Dumais S.T., Furnas G.W., Landauer T.K. &amp; Hashman R. (1990) Indexing by latent semantic analysis, Journal of the American Society for Information Science 41 (6), p. 391-407.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Delichère</author>
</authors>
<title>Etat de l&apos;art et implémentation d&apos;algorithmes de recherche et de classification automatique de documents sur Internet - Intégration à l&apos;outil Human Links, rapport de stage de fin d&apos;étude, EPITA,</title>
<date>2001</date>
<location>Paris.</location>
<marker>Delichère, 2001</marker>
<rawString>Delichère M. (2001) Etat de l&apos;art et implémentation d&apos;algorithmes de recherche et de classification automatique de documents sur Internet - Intégration à l&apos;outil Human Links, rapport de stage de fin d&apos;étude, EPITA, Paris.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K I Diamantaras</author>
<author>S Y Kung</author>
</authors>
<title>Principal Component Neural Networks: Theory and Applications,</title>
<date>1996</date>
<publisher>John Wiley &amp; Sons.</publisher>
<contexts>
<context position="14319" citStr="Diamantaras &amp; Kung 1996" startWordPosition="2146" endWordPosition="2149">de donnée à la fois et évite donc le calcul de la matrice de covariance. On peut si on le souhaite le réaliser par des opérations purement locales en utilisant des connexions entre les neurones de sortie. Le réseau peut fonctionner en parallèle (on ajuste alors tous les poids en même temps) mais il est plus simple et plus efficace de faire l&apos;apprentissage de chaque neurone successivement. C&apos;est la méthode que nous avons retenue. Il existe aussi des variantes de cet algorithme, notamment le système APEX utilisant des connexions latérales pour exprimer l&apos;influence d&apos;un neurone sur les suivants (Diamantaras &amp; Kung 1996)(Fiori 2000). Mais ces variantes sont plus complexes et améliorent peu les performances de l&apos;algorithme de base, que nous avons retenu ici. Mathieu Delichère, Daniel Memmi 4 Traitement des documents Nous avons utilisé un corpus de 90 pages Web environ, recueillies en vrac à partir des signets favoris (bookmarks) d&apos;un utilisateur réel. Chaque page Web contenait une page ou deux de texte ordinaire en français, et l&apos;ensemble touchait à une dizaine de thèmes différents (les centres d&apos;intérêt de l&apos;utilisateur). Notre but était d&apos;effectuer une catégorisation non supervisée (clustering) de ces pages </context>
</contexts>
<marker>Diamantaras, Kung, 1996</marker>
<rawString>Diamantaras K.I. &amp; Kung S.Y. (1996) Principal Component Neural Networks: Theory and Applications, John Wiley &amp; Sons.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Fiori</author>
</authors>
<title>An experimental comparison of three PCA neural networks,</title>
<date>2000</date>
<journal>Neural Processing Letters</journal>
<volume>11</volume>
<issue>3</issue>
<pages>209--218</pages>
<contexts>
<context position="14331" citStr="Fiori 2000" startWordPosition="2149" endWordPosition="2150">ite donc le calcul de la matrice de covariance. On peut si on le souhaite le réaliser par des opérations purement locales en utilisant des connexions entre les neurones de sortie. Le réseau peut fonctionner en parallèle (on ajuste alors tous les poids en même temps) mais il est plus simple et plus efficace de faire l&apos;apprentissage de chaque neurone successivement. C&apos;est la méthode que nous avons retenue. Il existe aussi des variantes de cet algorithme, notamment le système APEX utilisant des connexions latérales pour exprimer l&apos;influence d&apos;un neurone sur les suivants (Diamantaras &amp; Kung 1996)(Fiori 2000). Mais ces variantes sont plus complexes et améliorent peu les performances de l&apos;algorithme de base, que nous avons retenu ici. Mathieu Delichère, Daniel Memmi 4 Traitement des documents Nous avons utilisé un corpus de 90 pages Web environ, recueillies en vrac à partir des signets favoris (bookmarks) d&apos;un utilisateur réel. Chaque page Web contenait une page ou deux de texte ordinaire en français, et l&apos;ensemble touchait à une dizaine de thèmes différents (les centres d&apos;intérêt de l&apos;utilisateur). Notre but était d&apos;effectuer une catégorisation non supervisée (clustering) de ces pages pour établir</context>
</contexts>
<marker>Fiori, 2000</marker>
<rawString>Fiori S. (2000) An experimental comparison of three PCA neural networks, Neural Processing Letters 11 (3), p. 209-218.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hertz</author>
<author>A Krogh</author>
<author>R G Palmer</author>
</authors>
<title>Introduction to the Theory of Neural Computation,</title>
<date>1991</date>
<publisher>Addison Wesley.</publisher>
<contexts>
<context position="9766" citStr="Hertz et al. 1991" startWordPosition="1448" endWordPosition="1451">ication théorique des nouveaux axes est loin d&apos;être claire et ceux-ci sont peu intuitifs. D&apos;autre part les coûts de calcul demeurent importants, car on manipule encore des matrices de grande taille. Nous avons donc décidé de rester dans le cadre de l&apos;ACP et d&apos;utiliser plutôt une méthode neuronale itérative permettant de ne considérer qu&apos;un vecteur de données à la fois sans jamais calculer la matrice de covariance. 3 L&apos;Algorithme Hebbien Généralisé Rappelons d&apos;abord qu&apos;un réseau neuronal (ou connexionniste) se compose de neurones formels connectés entre eux, par analogie avec la neurobiologie (Hertz et al. 1991)(Hérault &amp; Jutten 1994). Un tel système consiste donc en un réseau de petits automates simples interconnectés, et c&apos;est le fonctionnement de l&apos;ensemble qui permet d&apos;accomplir les tâches désirées (souvent classification ou diagnostic). Chaque neurone formel calcule la somme pondérée de ses entrées et transmet son état interne aux neurones auxquels il est connecté. Certains neurones serviront d&apos;entrée, et d&apos;autres de sortie, mais le traitement est distribué sur l&apos;ensemble du réseau. De plus chaque connexion entre neurones est affectée d&apos;un poids modulant la transmission de l&apos;activité. Ces poids </context>
</contexts>
<marker>Hertz, Krogh, Palmer, 1991</marker>
<rawString>Hertz J., Krogh A. &amp; Palmer R.G. (1991) Introduction to the Theory of Neural Computation, Addison Wesley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hérault</author>
<author>C Jutten</author>
</authors>
<title>Réseaux Neuronaux et Traitement du Signal,</title>
<date>1994</date>
<location>Hermès.</location>
<contexts>
<context position="9789" citStr="Hérault &amp; Jutten 1994" startWordPosition="1451" endWordPosition="1454">es nouveaux axes est loin d&apos;être claire et ceux-ci sont peu intuitifs. D&apos;autre part les coûts de calcul demeurent importants, car on manipule encore des matrices de grande taille. Nous avons donc décidé de rester dans le cadre de l&apos;ACP et d&apos;utiliser plutôt une méthode neuronale itérative permettant de ne considérer qu&apos;un vecteur de données à la fois sans jamais calculer la matrice de covariance. 3 L&apos;Algorithme Hebbien Généralisé Rappelons d&apos;abord qu&apos;un réseau neuronal (ou connexionniste) se compose de neurones formels connectés entre eux, par analogie avec la neurobiologie (Hertz et al. 1991)(Hérault &amp; Jutten 1994). Un tel système consiste donc en un réseau de petits automates simples interconnectés, et c&apos;est le fonctionnement de l&apos;ensemble qui permet d&apos;accomplir les tâches désirées (souvent classification ou diagnostic). Chaque neurone formel calcule la somme pondérée de ses entrées et transmet son état interne aux neurones auxquels il est connecté. Certains neurones serviront d&apos;entrée, et d&apos;autres de sortie, mais le traitement est distribué sur l&apos;ensemble du réseau. De plus chaque connexion entre neurones est affectée d&apos;un poids modulant la transmission de l&apos;activité. Ces poids sont ajustés graduellem</context>
<context position="26715" citStr="Hérault &amp; Jutten 1994" startWordPosition="4020" endWordPosition="4023">e). En somme, GHA ne se justifie que lorsqu&apos;on peut se contenter des premières composantes principales. Mais c&apos;est souvent le cas, puisqu&apos;elles représentent la plus grosse part de l&apos;information contenue dans les données. Enfin, tout comme l&apos;ACP, GHA est une méthode purement linéaire qui ne peut capturer que des corrélations linéaires entre les données (cela revient en fait à n&apos;utiliser que la covariance). C&apos;est probablement suffisant pour traiter des documents, mais si on veut dépasser cette limitation, il faudrait envisager d&apos;autres méthodes comme l&apos;Analyse en Composantes Indépendantes (voir Hérault &amp; Jutten 1994) ou les Cartes de Kohonen (Ritter &amp; Kohonen 1989)(Kohonen 1998). 6 Conclusion Les vecteurs de grande dimension caractéristiques de la représentation des documents textuels sont très redondants et inutilement coûteux. Pour représenter l&apos;information pertinente de manière plus compacte, nous avons utilisé une technique neuronale, l&apos;Algorithme Hebbien Généralisé (GHA), qui permet d&apos;extraire automatiquement les premières composantes principales correspondant à la majeure partie de l&apos;information contenue dans les données. Cela permet une réduction de dimension à la fois techniquement économique et s</context>
</contexts>
<marker>Hérault, Jutten, 1994</marker>
<rawString>Hérault J. &amp; Jutten C. (1994) Réseaux Neuronaux et Traitement du Signal, Hermès.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I T Jolliffe</author>
</authors>
<title>Principal Component Analysis,</title>
<date>1986</date>
<publisher>Springer Verlag.</publisher>
<contexts>
<context position="6220" citStr="Jolliffe 1986" startWordPosition="917" endWordPosition="918">s le nom général d&apos;analyse factorielle, mais la méthode la plus connue est l&apos;Analyse en Composantes Principales (ACP). Malheureusement les réalisations courantes de l&apos;ACP ne permettent pas en pratique de traiter des vecteurs de très grande dimension, ce qui est justement le cas dans le domaine des documents. Nous avons alors choisi d&apos;utiliser une version neuronale de l&apos;ACP, appelée Algorithme Hebbien Généralisé, permettant de traiter de tels vecteurs avec de bons résultats. 2 L&apos;Analyse en Composantes Principales C&apos;est la méthode d&apos;analyse factorielle la plus utilisée (Bouroche &amp; Saporta 1980)(Jolliffe 1986)(Lebart et al. 2000). L&apos;ACP consiste à calculer un nombre réduit de nouvelles dimensions, qui sont des combinaisons linéaires des dimensions originelles des données (c&apos;est-à-dire des traits descriptifs). Ces nouvelles dimensions sont non corrélées et expriment le maximum de variance des données (en partant de données centrées sur la moyenne). Les nouveaux axes sont les vecteurs propres, ordonnés par valeurs propres décroissantes, de la matrice de covariance des données. Autrement dit ce sont les principaux axes de dispersion du nuage de données, en ordre d&apos;importance décroissante ; les valeurs</context>
</contexts>
<marker>Jolliffe, 1986</marker>
<rawString>Jolliffe I.T. (1986) Principal Component Analysis, Springer Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Kohonen</author>
</authors>
<title>Self-organization of very large document collections: state of the art,</title>
<date>1998</date>
<booktitle>Proc. of ICANN&apos;98,</booktitle>
<location>London.</location>
<contexts>
<context position="26778" citStr="Kohonen 1998" startWordPosition="4032" endWordPosition="4033">mières composantes principales. Mais c&apos;est souvent le cas, puisqu&apos;elles représentent la plus grosse part de l&apos;information contenue dans les données. Enfin, tout comme l&apos;ACP, GHA est une méthode purement linéaire qui ne peut capturer que des corrélations linéaires entre les données (cela revient en fait à n&apos;utiliser que la covariance). C&apos;est probablement suffisant pour traiter des documents, mais si on veut dépasser cette limitation, il faudrait envisager d&apos;autres méthodes comme l&apos;Analyse en Composantes Indépendantes (voir Hérault &amp; Jutten 1994) ou les Cartes de Kohonen (Ritter &amp; Kohonen 1989)(Kohonen 1998). 6 Conclusion Les vecteurs de grande dimension caractéristiques de la représentation des documents textuels sont très redondants et inutilement coûteux. Pour représenter l&apos;information pertinente de manière plus compacte, nous avons utilisé une technique neuronale, l&apos;Algorithme Hebbien Généralisé (GHA), qui permet d&apos;extraire automatiquement les premières composantes principales correspondant à la majeure partie de l&apos;information contenue dans les données. Cela permet une réduction de dimension à la fois techniquement économique et significative pour un utilisateur humain. L&apos;algorithme calcule d</context>
</contexts>
<marker>Kohonen, 1998</marker>
<rawString>Kohonen T. (1998) Self-organization of very large document collections: state of the art, Proc. of ICANN&apos;98, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Lebart</author>
<author>A Morineau</author>
<author>M Piron</author>
</authors>
<title>Statistique Exploratoire Multidimensionnelle,</title>
<date>2000</date>
<location>Dunod.</location>
<contexts>
<context position="6240" citStr="Lebart et al. 2000" startWordPosition="918" endWordPosition="921">l d&apos;analyse factorielle, mais la méthode la plus connue est l&apos;Analyse en Composantes Principales (ACP). Malheureusement les réalisations courantes de l&apos;ACP ne permettent pas en pratique de traiter des vecteurs de très grande dimension, ce qui est justement le cas dans le domaine des documents. Nous avons alors choisi d&apos;utiliser une version neuronale de l&apos;ACP, appelée Algorithme Hebbien Généralisé, permettant de traiter de tels vecteurs avec de bons résultats. 2 L&apos;Analyse en Composantes Principales C&apos;est la méthode d&apos;analyse factorielle la plus utilisée (Bouroche &amp; Saporta 1980)(Jolliffe 1986)(Lebart et al. 2000). L&apos;ACP consiste à calculer un nombre réduit de nouvelles dimensions, qui sont des combinaisons linéaires des dimensions originelles des données (c&apos;est-à-dire des traits descriptifs). Ces nouvelles dimensions sont non corrélées et expriment le maximum de variance des données (en partant de données centrées sur la moyenne). Les nouveaux axes sont les vecteurs propres, ordonnés par valeurs propres décroissantes, de la matrice de covariance des données. Autrement dit ce sont les principaux axes de dispersion du nuage de données, en ordre d&apos;importance décroissante ; les valeurs propres corresponda</context>
<context position="8588" citStr="Lebart et al. 2000" startWordPosition="1273" endWordPosition="1276"> des vecteurs de documents textuels, dont on a vu qu&apos;ils pouvaient comporter des milliers de traits. Diverses approches ont alors été proposées pour réduire la dimension des représentations vectorielles de textes. On peut citer notamment la méthode appelée Latent Semantic Indexing (LSI) ou Latent Semantic Analysis (LSA), qui n&apos;utilise pas la matrice de covariance, mais extrait de nouveaux axes directement de la matrice document-terme (Deerwester et al. 1990). L&apos;approche consiste à effectuer une décomposition en valeurs singulières de la matrice des données pour trouver les nouveaux axes (voir Lebart et al. 2000). Cette technique peut être considérée comme une généralisation de l&apos;extraction des vecteurs propres caractéristiques de l&apos;ACP, généralisation permettant de traiter des matrices rectangulaires et des données non centrées. Comme le nombre de documents est souvent plus petit que le nombre de termes, cette matrice rectangulaire est moins encombrante et moins coûteuse que la matrice de covariance. La méthode LSI semble donner de bons résultats en pratique pour l&apos;indexation et la recherche, Mathieu Delichère, Daniel Memmi mais contrairement à l&apos;ACP, la signification théorique des nouveaux axes est </context>
</contexts>
<marker>Lebart, Morineau, Piron, 2000</marker>
<rawString>Lebart L., Morineau A. &amp; Piron M. (2000) Statistique Exploratoire Multidimensionnelle, Dunod.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Lebart</author>
<author>A Salem</author>
</authors>
<title>Statistique Textuelle,</title>
<date>1994</date>
<location>Dunod.</location>
<contexts>
<context position="7544" citStr="Lebart &amp; Salem 1994" startWordPosition="1110" endWordPosition="1113">t donc généralement compte de la plus grande partie de la variance. Les composantes principales sont les nouvelles valeurs des données sur chaque axe ainsi obtenu. Cette méthode peut jouer un double rôle de compression des données et d&apos;outil d&apos;exploration dans des domaines fortement multidimensionnels. En effet les axes principaux ainsi calculés permettent à la fois une réduction des données et une interprétation plus facile du domaine traité, car les nouvelles dimensions sont souvent très significatives. Cela peut notamment se révéler intéressant pour l&apos;analyse de données en langage naturel (Lebart &amp; Salem 1994). Le problème est que l&apos;ACP demande le calcul préalable de la matrice carrée de covariance des données, qui est de taille n2 pour des vecteurs de dimension n. Cette matrice est déjà coûteuse à calculer, et sa taille et son traitement deviennent prohibitifs en grande dimension. Ainsi des données de dimension 1000 donneraient lieu à une matrice de un million d&apos;éléments ! L&apos;ACP est donc difficile ou impossible à utiliser sur des vecteurs de documents textuels, dont on a vu qu&apos;ils pouvaient comporter des milliers de traits. Diverses approches ont alors été proposées pour réduire la dimension des r</context>
</contexts>
<marker>Lebart, Salem, 1994</marker>
<rawString>Lebart L. &amp; Salem A. (1994) Statistique Textuelle, Dunod.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C D Manning</author>
<author>H Schütze</author>
</authors>
<title>Foundations of Statistical Natural Language Processing,</title>
<date>1999</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="2382" citStr="Manning &amp; Schütze 1999" startWordPosition="332" endWordPosition="335">analysis, PCA, GHA, neural networks. Mathieu Delichère, Daniel Memmi 1 Introduction La croissance accélérée d&apos;Internet et du Web donne une importance grandissante au traitement numérique de documents textuels. Pour effectuer les diverses tâches de classification, recherche et filtrage de documents, il faut d&apos;abord représenter les textes de manière à la fois économique et significative. On sait que le modèle vectoriel est probablement l&apos;approche la plus courante : on représente un texte par un vecteur numérique obtenu en comptant les éléments lexicaux les plus pertinents (Salton &amp; McGill 1983)(Manning &amp; Schütze 1999). Ces vecteurs sont fournis par des prétraitements simples. On commence généralement par éliminer les mots grammaticaux (articles, prépositions, etc.) et par réduire les variantes morphologiques à une forme commune (souvent appelée terme). Puis on compte les occurrences des termes les plus importants de manière à représenter chaque document par un vecteur dans l&apos;espace des termes. Un corpus de documents donne donc lieu à une matrice document-terme (Fig. 1). Cette représentation assez simpliste permet ensuite d&apos;appliquer les opérations vectorielles usuelles avec des résultats sémantiquement per</context>
</contexts>
<marker>Manning, Schütze, 1999</marker>
<rawString>Manning C.D. &amp; Schütze H. (1999) Foundations of Statistical Natural Language Processing, MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Memmi</author>
<author>J G Meunier</author>
</authors>
<title>Using competitive networks for text mining,</title>
<date>2000</date>
<booktitle>Proc. of Neural Computation 2000,</booktitle>
<location>Berlin.</location>
<contexts>
<context position="20422" citStr="Memmi &amp; Meunier 2000" startWordPosition="3099" endWordPosition="3102">xemple). Il vaudrait peut-être mieux ne pas retenir les derniers axes. 4.3 Classification Nous avons ensuite utilisé les vecteurs dans le nouvel espace (donnés par les valeurs de sortie du réseau) pour effectuer une classification non supervisée des documents (Bouroche &amp; Saporta 1980)(Anderberg 1973). Nous avons essayé plusieurs variantes de l&apos;algorithme bien connu des centres mobiles (k-means) en employant une distance euclidienne (pour ne pas avoir à normaliser les vecteurs). Mais nous aurions pu aussi bien utiliser des réseaux neuronaux compétitifs pour rester dans un cadre connexionniste (Memmi &amp; Meunier 2000). Les classes ainsi obtenues étaient tout à fait significatives, montrant clairement les thèmes principaux du corpus de pages. Les classes correspondaient bien à la dizaine de centres Mathieu Delichère, Daniel Memmi d&apos;intérêt qui avaient motivé la collection des signets de l&apos;utilisateur humain : des articles d&apos;acualité et de sport, la nouvelle économie, les réseaux de neurones, les rumeurs et virus informatiques, le peer to peer, les algorithmes génétiques... Plus précisément, en demandant à l&apos;algorithme des centres mobiles de retrouver 10 classes, c&apos;est-à-dire le nombre de centres d&apos;intérêt d</context>
</contexts>
<marker>Memmi, Meunier, 2000</marker>
<rawString>Memmi D. &amp; Meunier J.G. (2000) Using competitive networks for text mining, Proc. of Neural Computation 2000, Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Oja</author>
</authors>
<title>A simplified neuron model as a principal component analyzer,</title>
<date>1982</date>
<journal>Journal of Mathematics and Biology</journal>
<volume>15</volume>
<pages>267--273</pages>
<contexts>
<context position="11655" citStr="Oja 1982" startWordPosition="1730" endWordPosition="1731"> au prix d&apos;un temps d&apos;apprentissage plus ou moins long. On cherche ici non pas à effectuer une classification, mais à détecter les corrélations entre les données. Le système va apprendre indirectement les corrélations entre traits d&apos;un même vecteur d&apos;entrée et d&apos;un vecteur à l&apos;autre en cherchant à maximiser la sortie du réseau. Autrement dit, il va extraire les composantes principales. L&apos;Algorithme Hebbien Généralisé est ainsi une variante neuronale de l&apos;ACP élaborée par Sanger sous le nom de Generalized Hebbian Algorithm (GHA). Mais c&apos;est Oja qui proposa le premier une règle d&apos;apprentissage (Oja 1982) permettant à un simple neurone linéaire d&apos;extraire la première composante principale de données centrées : w(t+1) = w(t) + η(t) [y(t)x(t) − y2(t)w(t)] En notant les vecteurs en gras selon la convention usuelle, x est le vecteur d&apos;entrée, w le vecteur de poids, y = w.x la sortie, et η le pas d&apos;apprentissage (typiquement choisi entre 0,1 et 0,01). Le terme y(t)x(t) est de type hebbien, maximisant la variance de sortie, et y2(t)w(t) est un facteur de normalisation, conçu pour assurer ||w ||= 1. Cette procédure d&apos;apprentissage a pour effet d&apos;adapter les poids aux corrélations entre entrées et sor</context>
</contexts>
<marker>Oja, 1982</marker>
<rawString>Oja E. (1982) A simplified neuron model as a principal component analyzer, Journal of Mathematics and Biology 15, p. 267-273.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Ritter</author>
<author>T Kohonen</author>
</authors>
<title>Self-organizing semantic maps,</title>
<date>1989</date>
<journal>Biological Cybernetics</journal>
<volume>61</volume>
<issue>4</issue>
<pages>241--254</pages>
<contexts>
<context position="26764" citStr="Ritter &amp; Kohonen 1989" startWordPosition="4029" endWordPosition="4032">ut se contenter des premières composantes principales. Mais c&apos;est souvent le cas, puisqu&apos;elles représentent la plus grosse part de l&apos;information contenue dans les données. Enfin, tout comme l&apos;ACP, GHA est une méthode purement linéaire qui ne peut capturer que des corrélations linéaires entre les données (cela revient en fait à n&apos;utiliser que la covariance). C&apos;est probablement suffisant pour traiter des documents, mais si on veut dépasser cette limitation, il faudrait envisager d&apos;autres méthodes comme l&apos;Analyse en Composantes Indépendantes (voir Hérault &amp; Jutten 1994) ou les Cartes de Kohonen (Ritter &amp; Kohonen 1989)(Kohonen 1998). 6 Conclusion Les vecteurs de grande dimension caractéristiques de la représentation des documents textuels sont très redondants et inutilement coûteux. Pour représenter l&apos;information pertinente de manière plus compacte, nous avons utilisé une technique neuronale, l&apos;Algorithme Hebbien Généralisé (GHA), qui permet d&apos;extraire automatiquement les premières composantes principales correspondant à la majeure partie de l&apos;information contenue dans les données. Cela permet une réduction de dimension à la fois techniquement économique et significative pour un utilisateur humain. L&apos;algori</context>
</contexts>
<marker>Ritter, Kohonen, 1989</marker>
<rawString>Ritter H. &amp; Kohonen T. (1989) Self-organizing semantic maps, Biological Cybernetics 61 (4), p. 241-254.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>M McGill</author>
</authors>
<title>Introduction to Modern Information Retrieval,</title>
<date>1983</date>
<publisher>McGraw-Hill.</publisher>
<contexts>
<context position="2358" citStr="Salton &amp; McGill 1983" startWordPosition="329" endWordPosition="332">ality reduction, data analysis, PCA, GHA, neural networks. Mathieu Delichère, Daniel Memmi 1 Introduction La croissance accélérée d&apos;Internet et du Web donne une importance grandissante au traitement numérique de documents textuels. Pour effectuer les diverses tâches de classification, recherche et filtrage de documents, il faut d&apos;abord représenter les textes de manière à la fois économique et significative. On sait que le modèle vectoriel est probablement l&apos;approche la plus courante : on représente un texte par un vecteur numérique obtenu en comptant les éléments lexicaux les plus pertinents (Salton &amp; McGill 1983)(Manning &amp; Schütze 1999). Ces vecteurs sont fournis par des prétraitements simples. On commence généralement par éliminer les mots grammaticaux (articles, prépositions, etc.) et par réduire les variantes morphologiques à une forme commune (souvent appelée terme). Puis on compte les occurrences des termes les plus importants de manière à représenter chaque document par un vecteur dans l&apos;espace des termes. Un corpus de documents donne donc lieu à une matrice document-terme (Fig. 1). Cette représentation assez simpliste permet ensuite d&apos;appliquer les opérations vectorielles usuelles avec des résu</context>
<context position="15570" citStr="Salton &amp; McGill 1983" startWordPosition="2341" endWordPosition="2344">ilisateur dans le contexte du développement d&apos;un moteur de recherche collaboratif, le système Human Links. Nos choix ont été faits dans le cadre de ce système, et pour plus de détails sur la réalisation du travail, voir (Delichère 2000). Pour plus d&apos;information sur le système Human Links développé par la start-up Amoweba pour accéder à une expertise distribuée sur Internet, on peut consulter le site Web : http://www.amoweba.com 4.1 Prétraitements La vectorisation de chaque page Web a été faite selon une procédure classique de manière à ne retenir que les mots ou termes les plus significatifs (Salton &amp; McGill 1983). On a d&apos;abord enlevé les mots grammaticaux (par simple consultation d&apos;une liste), puis tronqué les mots lexicaux à leur 5 premières lettres pour réduire les variantes morphologique à une forme unique (technique simpliste mais en général efficace). On a compté ensuite les occurrences dans chaque document des termes ainsi retenus, et attribué à chaque terme une valeur pondérée selon une variante simplifiée de la mesure TFIDF pour favoriser les termes les plus discriminants entre documents : fréquence dans le document / fréquence totale dans le corpus On a gardé les termes dépassant un seuil min</context>
</contexts>
<marker>Salton, McGill, 1983</marker>
<rawString>Salton G. &amp; McGill M. (1983) Introduction to Modern Information Retrieval, McGraw-Hill.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T D Sanger</author>
</authors>
<title>Optimal unsupervised learning in a single-layer linear feedforward neural network,</title>
<date>1989</date>
<journal>Neural Networks</journal>
<volume>2</volume>
<issue>6</issue>
<pages>459--473</pages>
<contexts>
<context position="12936" citStr="Sanger 1989" startWordPosition="1933" endWordPosition="1934">age a donc pour effet d&apos;amener itérativement le vecteur de poids à représenter le premier vecteur propre de la covariance des données. La sortie du neurone, projection du vecteur d&apos;entrée sur le vecteur de Analyse Factorielle Neuronale pour Documents Textuels poids normalisé, donne la nouvelle valeur de l&apos;entrée le long de cet axe, c&apos;est-à-dire la première composante principale. yi wij xj Fig. 2 : Schéma d&apos;un réseau GHA. Sanger a ensuite généralisé cette règle d&apos;apprentissage à un réseau à une couche de neurones linéaires, de manière à pouvoir calculer les composantes principales successives (Sanger 1989). Chaque neurone reçoit le même vecteur d&apos;entrée et chacun extrait une composante principale en ordre décroissant de variance (Fig. 2). La règle d&apos;apprentissage est maintenant la suivante, en notation indicée : wij(t+1) = wij(t) + η(t) [yi(t)x&apos;j(t) − yi2(t)wij(t)] chaque neurone successif utilisant une entrée modifiée x&apos;j(t) calculée en soustrayant de l&apos;entrée réelle l&apos;effet des neurones précédents : x&apos;j(t) = xj(t) − Σk&lt;i wkj(t)yk(t) L&apos;apprentissage pour GHA peut aussi s&apos;exprimer de manière plus concise : wij(t+1) = wij(t) + η(t) yi(t) [xj(t) − Σk≤i wkj(t)yk(t)] On démontre que cette règle gén</context>
</contexts>
<marker>Sanger, 1989</marker>
<rawString>Sanger T.D. (1989) Optimal unsupervised learning in a single-layer linear feedforward neural network, Neural Networks 2 (6), p. 459-473.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>