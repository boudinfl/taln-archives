<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
"http://www.w3.org/TR/html4/loose.dtd">
<html><head><title>Une grammaire hors-contexte valu&#233;e pour l&#8217;analyse syntaxique</title>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
</head>
<body>
<div style="page-break-before:always; page-break-after:always"><div><p>TALN 2002, Nancy, 24&#8211;27 juin 2002
</p>
<p>Une grammaire hors-contexte valu&#233;e pour l&#8217;analyse
syntaxique
</p>
<p>A. Rozenknop
I&amp;C-IIF-LIA - EPFL
</p>
<p>CH-1015 Lausanne, Suisse
antoine.rozenknop@epfl.ch
</p>
<p>Mots-clefs &#8211; Keywords
</p>
<p>Syntaxe,linguistique math&#233;matique,apprentissage statistique,SCFG,Gibbs,
statistical learning,Syntax,Context-free grammars
</p>
<p>R&#233;sum&#233; - Abstract
</p>
<p>Les grammaires hors-contexte stochastiques sont exploit&#233;es par des algorithmes particuli&#232;re-
ment efficaces dans des t&#226;ches de reconnaissance de la parole et d&#8217;analyse syntaxique. Cet
article propose une autre probabilisation de ces grammaires, dont les propri&#233;t&#233;s math&#233;matiques
semblent intuitivement plus adapt&#233;es &#224; ces t&#226;ches que celles des SCFG (Stochastique CFG),
sans n&#233;cessiter d&#8217;algorithme d&#8217;analyse sp&#233;cifique. L&#8217;utilisation de ce mod&#232;le en analyse sur
du texte provenant du corpus Susanne peut r&#233;duire de &#0;&#0;&#1; le nombre d&#8217;analyses erron&#233;es, en
comparaison avec une SCFG entra&#238;n&#233;e dans les m&#234;mes conditions.
</p>
<p>Weighted Context-Free Grammars can be used for speech recognition or syntactic analysis
thanks to especially efficient algorithms. In this paper, we propose an instanciation of such
a grammar, whose mathematical properties are intuitively more suitable for those tasks than
SCFG&#8217;s (Stochastic CFG), without requiring specific analysis algorithms. Results on Susanne
text show that up to &#0;&#0;&#1; of analysis errors made by a SCFG can be avoided with this model.
</p>
<p>1 Motivations
</p>
<p>Bien qu&#8217;aujourd&#8217;hui d&#233;pass&#233;es quant &#224; leur pouvoir de description des langues naturelles, les
grammaires hors-contexte stochastiques (SCFG) restent des mod&#232;les int&#233;ressants pour l&#8217;analyse
syntaxique et la reconnaissance de la parole (Chappelier et al., 1999), du fait qu&#8217;elles se pr&#234;-
tent &#224; des algorithmes particuli&#232;rement efficaces remplissant ces t&#226;ches (Chappelier &amp; Rajman,
1998). Elles peuvent aussi &#234;tre choisies comme des repr&#233;sentations interm&#233;diaires calculatoires
de grammaires plus riches, comme les grammaires &#224; substitution d&#8217;arbres polyn&#244;miales (Chap-
</p>
<p>95</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>A.Rozenknop
</p>
<p>(&#0;) VP
</p>
<p>V
</p>
<p>NP
</p>
<p>NP
</p>
<p>Det N
</p>
<p>PP
</p>
<p>P
</p>
<p>NP
</p>
<p>Det N
</p>
<p>(&#1;) VP
</p>
<p>V
</p>
<p>NP
</p>
<p>Det N
</p>
<p>PP
</p>
<p>P
</p>
<p>NP
</p>
<p>Det N
</p>
<p>Figure 1: Corpus d&#8217;apprentissage &#0;&#2;
&#0;
</p>
<p>. Ce corpus contient l&#8217;arbre &#2;&#0;&#3; avec une fr&#233;quence relative
&#3; , et &#1; avec une fr&#233;quence relative &#4;&#0; &#3; .
</p>
<p>pelier &amp; Rajman, 2001). Cependant, certaines de leurs propri&#233;t&#233;s math&#233;matiques r&#233;duisent la
qualit&#233; des r&#233;sultats que l&#8217;on peut escompter de leur utilisation (Rozenknop &amp; Silaghi, 2001).
Cet article propose une nouvelle pond&#233;ration des grammaires hors-contexte, qui est essentielle-
ment une variante non-g&#233;n&#233;rative des SCFG. Not&#233; GCFG (pour &quot;Gibbsian CFG&quot;), ce mod&#232;le
associe &#224; chaque r&#232;gle hors-contexte un &quot;potentiel&quot; r&#233;el, &#224; la place des probabilit&#233;s d&#8217;une SCFG,
et s&#8217;appuie sur un crit&#232;re d&#8217;apprentissage &quot;orient&#233; analyse&quot;. L&#8217;int&#233;r&#234;t de ce mod&#232;le est qu&#8217;il fait
preuve d&#8217;un meilleur comportement en analyse tout en pr&#233;servant l&#8217;efficacit&#233; algorithmique des
SCFG. La suite de cet article montre un exemple de comportement non-intuitif des SCFG dans
la section 2, d&#233;crit le mod&#232;le GCFG et un algorithme d&#8217;apprentissage de ses param&#232;tres dans la
section 3, compare les comportements des SCFG et des GCFG dans la section 4.1 et donne des
conclusions dans la section 5.
</p>
<p>2 Exemple de comportement ind&#233;sirable des SCFG
</p>
<p>Cet exemple est tir&#233; d&#8217;une &#233;tude de M. Johnson (Johnson, 1998), et illustre un comportement
des SCFG qui peut sembler paradoxal. Supposons que l&#8217;on dispose d&#8217;un corpus &#0;&#2;
</p>
<p>&#0;
</p>
<p>constitu&#233; de
deux arbres &#2;&#0;&#3; et &#2;&#1;&#3; (cf fig. 1), l&#8217;arbre &#0; apparaissant avec une fr&#233;quence relative1 &#3; . On
entra&#238;ne une SCFG sur ce corpus selon la m&#233;thode habituelle, qui consiste &#224; affecter aux r&#232;gles
une probabilit&#233; proportionnelle &#224; leur fr&#233;quence en corpus.
</p>
<p>Les probabilit&#233;s ( &#1;&#4;
&#0;
</p>
<p>) des r&#232;gles apprises &#224; partir de ce corpus sont les suivantes : &#1;&#4;
&#0;
</p>
<p>&#2;VP &#1;
V NP&#3; &#5; &#3; , &#1;&#4;
</p>
<p>&#0;
</p>
<p>&#2;VP &#1; V NP PP&#3; &#5; &#4; &#0; &#3; , &#1;&#4;
&#0;
</p>
<p>&#2;NP &#1; Det N&#3; &#5; &#6;&#5;&#2;&#6; &#7; &#3;&#3; et &#1;&#4;
&#0;
</p>
<p>&#2;NP &#1;
NP PP&#3; &#5; &#3;&#5;&#2;&#6; &#7; &#3;&#3;. Lors d&#8217;une analyse syntaxique de la &quot;phrase&quot; V Det N P Det N,
les structures (&#0;) et (&#1;) se voient donc affect&#233;es des probabilit&#233;s &#2;&#4;
</p>
<p>&#0;
</p>
<p>&#2;&#0;&#3; &#5; &#8;&#3;
</p>
<p>&#1;
</p>
<p>&#5;&#2;&#6; &#7; &#3;&#3;
</p>
<p>&#2; et
&#2;
</p>
<p>&#4;
</p>
<p>&#0;
</p>
<p>&#2;&#1;&#3; &#5; &#8;&#2;&#4; &#0; &#3;&#3;&#5;&#2;&#6; &#7; &#3;&#3;
</p>
<p>&#1;
</p>
<p>. La fr&#233;quence relative estim&#233;e &#1;&#3;
&#0;
</p>
<p>de &#2;&#0;&#3;, pour la phrase V Det
N P Det N, est alors : &#1;&#3;
</p>
<p>&#0;
</p>
<p>&#5;
</p>
<p>&#1;
</p>
<p>&#4;
</p>
<p>&#0;
</p>
<p>&#2;&#0;&#3;&#5;&#2;
</p>
<p>&#1;
</p>
<p>&#4;
</p>
<p>&#0;
</p>
<p>&#2;&#0;&#3; &#7;
</p>
<p>&#1;
</p>
<p>&#4;
</p>
<p>&#0;
</p>
<p>&#2;&#1;&#3;&#3; &#5; &#3;
</p>
<p>&#1;
</p>
<p>&#5;&#2;&#6; &#0; &#3;&#3;. Id&#233;alement, cette
fr&#233;quence relative estim&#233;e &#1;&#3;
</p>
<p>&#0;
</p>
<p>devrait &#234;tre proche de sa fr&#233;quence relative r&#233;elle &#3; dans le corpus
d&#8217;apprentissage. La relation entre &#3; et &#1;&#3;
</p>
<p>&#0;
</p>
<p>est trac&#233;e sur la figure 2. On voit que les deux
fonctions peuvent diff&#233;rer substantiellement. Par exemple, si &#3; &#5; &#9;&#6; &#10;&#11;, &#1;&#3;
</p>
<p>&#0;
</p>
<p>&#5; &#9;&#6; &#8;&#11;, i.e. m&#234;me
si &#2;&#0;&#3; appara&#238;t trois fois plus souvent que &#2;&#1;&#3; dans le corpus d&#8217;apprentissage, la phrase sera
analys&#233;e par &#2;&#1;&#3; !
</p>
<p>M. Johnson soup&#231;onne que ce comportement est d&#251; &#224; la non-syst&#233;maticit&#233; de la construction
1i.e. son nombre d&#8217;occurences divis&#233; par la taille du corpus.
</p>
<p>96</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Une grammaire hors-contexte valu&#233;e pour l&#8217;analyse syntaxique
</p>
<p>0
</p>
<p>0.2
</p>
<p>0.4
</p>
<p>0.6
</p>
<p>0.8
</p>
<p>1
</p>
<p>0 0.2 0.4 0.6 0.8 1
</p>
<p>fre
qu
</p>
<p>en
ce
</p>
<p>s 
re
</p>
<p>la
tiv
</p>
<p>es
 e
</p>
<p>st
im
</p>
<p>ee
s
</p>
<p>frequence relative observee en corpus
</p>
<p>f1
f2
f3
</p>
<p>f
</p>
<p>Figure 2: Fr&#233;quences relatives estim&#233;es de l&#8217;attachement de PP &#224; NP, trac&#233;es en fonction de la
fr&#233;quence relative &#3; de cet attachement observ&#233;e dans les divers corpus d&#8217;apprentissage &#233;tudi&#233;s.
</p>
<p>des structures dans le corpus d&#8217;apprentissage : dans &#2;&#0;&#3;, (NP &#2;&#0; Det N PP) est sous forme
adjonctive de Chomsky, alors que (VP &#2;&#0; V NP PP) est sous forme plate dans &#2;&#1;&#3;. Pour
tester cette hypoth&#232;se, les calculs peuvent &#234;tre recommenc&#233;s en modifiant le corpus, soit en
aplanissant (NP &#2;&#0; Det N PP) dans &#2;&#0;&#3;, soit en rempla&#231;ant (VP &#1; V NP PP) par les deux
r&#232;gles (VP &#1; VP PP) et (VP &#1; V NP) dans &#2;&#1;&#3;. La premi&#232;re solution m&#232;ne &#224; une fr&#233;quence
relative estim&#233;e du premier arbre valant : &#1;&#3;
</p>
<p>&#1;
</p>
<p>&#5; &#2;&#3;
</p>
<p>&#1;
</p>
<p>&#0; &#6;&#3;&#3;&#5;&#2;&#6;&#3;
</p>
<p>&#1;
</p>
<p>&#0; &#3; &#0; &#6;&#3;, et la deuxi&#232;me &#224; :
&#1;
</p>
<p>&#3;
</p>
<p>&#2;
</p>
<p>&#5; &#3;
</p>
<p>&#1;
</p>
<p>&#5;&#2;&#6;&#0; &#0;&#3; &#7; &#6;&#3;
</p>
<p>&#1;
</p>
<p>&#3;. Ces deux fr&#233;quences estim&#233;es sont plus proches de &#3; que &#1;&#3;
&#0;
</p>
<p>, comme
</p>
<p>l&#8217;illustre la figure 2, mais restent inf&#233;rieures : dans tous les cas, si la fr&#233;quence relative observ&#233;e
de l&#8217;attachement de PP &#224; NP (arbre &#2;&#0;&#3;) vaut &#9;&#6; &#12;, l&#8217;analyse syntaxique de V Det N P Det
N attachera PP &#224; VP, malgr&#233; sa plus faible fr&#233;quence dans le corpus (&#9;&#6; &#8;).
Le mod&#232;le GCFG, pr&#233;sent&#233; dans la suite de cet article, peut &#234;tre &#233;valu&#233; sur les trois corpus
pr&#233;c&#233;demment d&#233;finis. Dans chaque cas, la fr&#233;quence relative attribu&#233;e &#224; &#2;&#0;&#3; est &#233;gale &#224; sa
fr&#233;quence relative observ&#233;e. Les trois courbes se confondent et sont not&#233;es f sur la figure 2.
</p>
<p>3 Grammaire Hors-Contexte avec potentiels de Gibbs (GCFG)
</p>
<p>Le mod&#232;le expos&#233; ici s&#8217;inspire directement du mod&#232;le SCFG. La grammaire est compos&#233;e d&#8217;un
ensemble de &#7;
</p>
<p>&#0;
</p>
<p>r&#232;gles de r&#233;criture &#8; &#1; &#9;
&#0;
</p>
<p>&#10;&#10;&#10;&#9;
</p>
<p>&#1;&#0;&#1;
</p>
<p>, de &#7;
&#1;
</p>
<p>symboles terminaux et non-terminaux,
les symboles terminaux n&#8217;apparaissant que dans les parties droites des r&#232;gles. De plus, &#224; chaque
r&#232;gle &#11;
</p>
<p>&#2;
</p>
<p>est associ&#233; un potentiel &#12;
&#2;
</p>
<p>.
</p>
<p>2
</p>
<p>2Dans une grammaire SCFG, chaque r&#232;gle est associ&#233;e &#224; une probabilit&#233;, les probabilit&#233;s des r&#232;gles de m&#234;me
partie gauche devant alors sommer &#224; 1 (contrainte stochastique).
</p>
<p>97</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>A.Rozenknop
</p>
<p>3.1 Potentiel d&#8217;un arbre, et probabilit&#233; conditionnelle
</p>
<p>Contrairement &#224; une SCFG, ce mod&#232;le n&#8217;est pas g&#233;n&#233;rateur. On ne cherche donc pas &#224; d&#233;finir
la probabilit&#233; de production d&#8217;un arbre avec ce mod&#232;le. En revanche, on d&#233;finit le potentiel
d&#8217;un arbre d&#8217;analyse &#13; comme la somme des potentiels des r&#232;gles qui le constituent. Il s&#8217;agit
donc du produit scalaire &#12; &#3; &#3;&#2;&#13;&#3; de deux vecteurs de taille &#7;
</p>
<p>&#0;
</p>
<p>, la &#14;&#3; composante de &#12; &#233;tant le
potentiel &#12;
</p>
<p>&#2;
</p>
<p>de la r&#232;gle &#11;
&#2;
</p>
<p>, et la &#14;&#3; composante de &#3;&#2;&#13;&#3; &#233;tant le nombre d&#8217;occurence(s) &#3;
&#2;
</p>
<p>de la
r&#232;gle &#11;
</p>
<p>&#2;
</p>
<p>dans l&#8217;arbre &#13;.
</p>
<p>On d&#233;finit de plus la probabilit&#233; d&#8217;un arbre d&#8217;analyse &#13; conditionnellement &#224; ses feuilles &#15; &#5;
&#2;&#15;
</p>
<p>&#0;
</p>
<p>&#10;&#10;&#10;&#15;
</p>
<p>&#4;
</p>
<p>&#3; (i.e. &#15; repr&#233;sente la phrase analys&#233;e) par la formule :
</p>
<p>&#16;
</p>
<p>&#5;
</p>
<p>&#2;&#13;&#4;&#15;&#3; &#5;
</p>
<p>&#17;
</p>
<p>&#5;&#2;&#6;&#3;&#7;&#4;
</p>
<p>&#3;
</p>
<p>&#8;&#3;
</p>
<p>&#0;
</p>
<p>&#9;
</p>
<p>&#17;
</p>
<p>&#5;&#2;&#6;&#3;&#8;&#4;
</p>
<p>o&#249;
&#3;
</p>
<p>&#8;&#3;
</p>
<p>&#0;
</p>
<p>&#9;
</p>
<p>est une somme sur tous les arbres &#18; de la grammaire qui ont pour feuilles &#15;.
</p>
<p>3.2 Analyse syntaxique &#224; l&#8217;aide d&#8217;une GCFG
</p>
<p>L&#8217;analyse syntaxique d&#8217;une phrase &#15; consiste &#224; d&#233;terminer l&#8217;arbre &#13; de plus fort potentiel parmi
les arbres d&#8217;analyse possibles. Au vu de la formule pr&#233;c&#233;dente, cela correspond &#224; trouver l&#8217;arbre
de probabilit&#233; maximale, conditionnellement &#224; la phrase analys&#233;e :
</p>
<p>&#13; &#5; &#14;&#15;&#16;&#17;&#18;&#19;
</p>
<p>&#7;&#3;
</p>
<p>&#0;
</p>
<p>&#9;
</p>
<p>&#12; &#3; &#3;&#2;&#13;&#3; &#5; &#14;&#15;&#16;&#17;&#18;&#19;
</p>
<p>&#7;&#3;
</p>
<p>&#0;
</p>
<p>&#9;
</p>
<p>&#16;
</p>
<p>&#5;
</p>
<p>&#2;&#13;&#4;&#15;&#3; &#5; &#14;&#15;&#16;&#17;&#18;&#19;
</p>
<p>&#7;&#3;
</p>
<p>&#0;
</p>
<p>&#9;
</p>
<p>&#4;
</p>
<p>&#0;
</p>
<p>&#0;
</p>
<p>&#4;&#7;
</p>
<p>&#17;
</p>
<p>&#5;
</p>
<p>&#0;
</p>
<p>&#6;
</p>
<p>&#0;
</p>
<p>&#3;&#7;&#4;
</p>
<p>La derni&#232;re expression montre &#224; quel point ce mod&#232;le est proche d&#8217;un mod&#232;le SCFG pour
effectuer l&#8217;analyse syntaxique. En effet, avec une SCFG, la solution est donn&#233;e par : &#13; &#5;
&#14;&#15;&#16;&#17;&#18;&#19;
</p>
<p>&#7;&#3;
</p>
<p>&#0;
</p>
<p>&#9;
</p>
<p>&#16;&#2;&#13;&#3; &#5; &#14;&#15;&#16;&#17;&#18;&#19;
</p>
<p>&#7;&#3;
</p>
<p>&#0;
</p>
<p>&#9;
</p>
<p>&#5;
</p>
<p>&#0;
</p>
<p>&#0;
</p>
<p>&#4;&#7;
</p>
<p>&#16;&#2;&#11;
</p>
<p>&#2;
</p>
<p>&#3;
</p>
<p>&#6;
</p>
<p>&#0;
</p>
<p>&#3;&#7;&#4;
</p>
<p>L&#8217;utilisation d&#8217;un mod&#232;le GCFG ne n&#233;cessite donc pas le d&#233;veloppement d&#8217;un algorithme
d&#8217;analyse sp&#233;cifique : on peut r&#233;utiliser tel quel tout algorithme d&#8217;analyse de SCFG, &#224; con-
dition que celui-ci ne requi&#232;re pas les conditions &#16;&#2;&#11;
</p>
<p>&#2;
</p>
<p>&#3; &#5; &#4; ni
&#3;
</p>
<p>&#0;
</p>
<p>&#0;
</p>
<p>&#5; &#4;
</p>
<p>3
. En particulier, les
</p>
<p>algorithmes de type &#0;&#6; (Chelba, 2000), qui n&#233;cessitent un score d&#233;croissant avec le nombre
d&#8217;&#233;tapes de l&#8217;analyse, doivent &#234;tre &#233;vit&#233;s. Un algorithme tabulaire ascendant classique (Chap-
pelier &amp; Rajman, 1998) a &#233;t&#233; utilis&#233; pour les tests d&#8217;analyse, en rempla&#231;ant simplement les
probabilit&#233;s des r&#232;gles &#16;&#2;&#11;
</p>
<p>&#2;
</p>
<p>&#3; par &#17;&#5;&#0; .
</p>
<p>3.3 Apprentissage des param&#232;tres
</p>
<p>3.3.1 Principe
</p>
<p>&#201;tant donn&#233; un corpus d&#8217;apprentissage, constitu&#233; de phrases &#19; et d&#8217;arbres d&#8217;analyse &#8; de ces
phrases, on cherche &#224; calculer les param&#232;tres du mod&#232;le &#12; de fa&#231;on &#224; maximiser la probabilit&#233;
des arbres conditionnellement aux phrases :
</p>
<p>&#13;
</p>
<p>&#12; &#5; &#14;&#15;&#16;&#17;&#18;&#19;
</p>
<p>&#5;
</p>
<p>&#16;
</p>
<p>&#5;
</p>
<p>&#2;&#8;&#4;&#19; &#3; &#5; &#14;&#15;&#16;&#17;&#18;&#19;
</p>
<p>&#5;
</p>
<p>&#6;
</p>
<p>&#7;&#4;&#10;
</p>
<p>&#20;&#21; &#16;
</p>
<p>&#5;
</p>
<p>&#2;&#13;&#4;&#15;&#2;&#13;&#3;&#3; &#5; &#14;&#15;&#16;&#17;&#18;&#19;
</p>
<p>&#5;
</p>
<p>&#7;&#2;&#12;&#3;
</p>
<p>3somme sur les r&#232;gles de m&#234;me partie gauche.
</p>
<p>98</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Une grammaire hors-contexte valu&#233;e pour l&#8217;analyse syntaxique
</p>
<p>(en notant &#15;&#2;&#13;&#3; la phrase analys&#233;e par l&#8217;arbre &#13;, i.e. les feuilles de &#13;.) &#7;&#2;&#12;&#3; est la &quot;vraisem-
blance conditionnelle&quot; du corpus; l&#8217;apprentissage du mod&#232;le consiste en la maximisation de ce
crit&#232;re. On note ici l&#8217;une des diff&#233;rences fondamentales de ce mod&#232;le avec une SCFG, pour
laquelle on cherche en g&#233;n&#233;ral &#224; maximiser la probabilit&#233; du corpus &#16;
</p>
<p>&#5;
</p>
<p>&#2;&#8;&#3;, ce qui se r&#233;soud
facilement en affectant &#224; chaque r&#232;gle une probabilit&#233; proportionnelle &#224; sa fr&#233;quence dans le
corpus.
</p>
<p>3.3.2 Improved Iterative Scaling : th&#233;orie
</p>
<p>La m&#233;thode utilis&#233;e ici pour le calcul des param&#232;tres s&#8217;inspire directement de la m&#233;thode IIS
(Improved Iterative Scaling) expos&#233;e dans (Berger, ; Pietra et al., 1997; Lafferty, 1996) pour la
probabilisation de champs de Markov. Elle est d&#233;crite dans la suite de cette section, pour le cas
particulier du mod&#232;le GCFG.
</p>
<p>Plut&#244;t que de chercher &#224; maximiser directement le crit&#232;re &#7;&#2;&#12;&#3;, ce qui se r&#233;v&#232;le trop ardu, la
m&#233;thode am&#233;liore it&#233;rativement le mod&#232;le, &#224; partir d&#8217;un mod&#232;le initial &#12;
</p>
<p>&#5;
</p>
<p>. Une it&#233;ration consiste
&#224; passer d&#8217;un mod&#232;le &#12; &#224; un mod&#232;le &#12;&#5;, en tentant de maximiser sur &#12;&#5; le crit&#232;re &#22;&#7;
</p>
<p>&#5;
</p>
<p>&#2;&#12;
</p>
<p>&#5;
</p>
<p>&#3; &#5;
</p>
<p>&#7;&#2;&#12;
</p>
<p>&#5;
</p>
<p>&#3;&#0;&#7;&#2;&#12;&#3; &#5;
</p>
<p>&#3;
</p>
<p>&#7;&#4;&#10;
</p>
<p>&#20;&#21;
</p>
<p>&#11;
</p>
<p>&#1;
</p>
<p>&#1;
</p>
<p>&#3;&#7;&#1;&#9;&#3;&#7;&#4;&#4;
</p>
<p>&#11;
</p>
<p>&#1;
</p>
<p>&#3;&#7;&#1;&#9;&#3;&#7;&#4;&#4;
</p>
<p>.
</p>
<p>Posons &#20;
&#5;&#12;&#9;
</p>
<p>&#5;
</p>
<p>&#3;
</p>
<p>&#7;&#3;
</p>
<p>&#0;
</p>
<p>&#9;
</p>
<p>&#17;
</p>
<p>&#5;&#2;&#6;&#3;&#7;&#4; la constante de normalisation associ&#233;e &#224; la phrase &#15; dans le
mod&#232;le &#12;. On peut alors &#233;crire &#16;
</p>
<p>&#5;
</p>
<p>&#2;&#13;&#4;&#15;&#3; &#5; &#2;&#20;
</p>
<p>&#5;&#12;&#9;
</p>
<p>&#3;
</p>
<p>&#6;&#0;
</p>
<p>&#17;
</p>
<p>&#5;&#2;&#6;&#3;&#7;&#4;
</p>
<p>, et :
</p>
<p>&#22;&#7;
</p>
<p>&#5;
</p>
<p>&#2;&#12;
</p>
<p>&#5;
</p>
<p>&#3; &#5;
</p>
<p>&#6;
</p>
<p>&#7;&#4;&#10;
</p>
<p>&#2;&#12;
</p>
<p>&#5;
</p>
<p>&#0; &#12;&#3; &#3; &#3;&#2;&#13;&#3;&#0;
</p>
<p>&#6;
</p>
<p>&#7;&#4;&#10;
</p>
<p>&#20;&#21;
</p>
<p>&#20;
</p>
<p>&#5;
</p>
<p>&#1;
</p>
<p>&#12;&#9;&#3;&#7;&#4;
</p>
<p>&#20;
</p>
<p>&#5;&#12;&#9;&#3;&#7;&#4;
</p>
<p>En majorant le logarithme par la formule &#20;&#21;&#21; &#5; &#21;&#0; &#4;, on peut minorer &#22;&#7;
&#5;
</p>
<p>&#2;&#12;
</p>
<p>&#5;
</p>
<p>&#3; par :
</p>
<p>&#22;&#7;
</p>
<p>&#5;
</p>
<p>&#2;&#12;
</p>
<p>&#5;
</p>
<p>&#3; &#8;
</p>
<p>&#6;
</p>
<p>&#7;&#4;&#10;
</p>
<p>&#22;&#12; &#3; &#3;&#2;&#13;&#3;&#0;
</p>
<p>&#6;
</p>
<p>&#7;&#4;&#10;
</p>
<p>&#20;
</p>
<p>&#5;
</p>
<p>&#1;
</p>
<p>&#12;&#9;&#3;&#7;&#4;
</p>
<p>&#20;
</p>
<p>&#5;&#12;&#9;&#3;&#7;&#4;
</p>
<p>&#7; &#4;&#8;&#4; (1)
</p>
<p>o&#249; &#22;&#12; &#5; &#12;&#5; &#0; &#12;, et &#4;&#8;&#4; &#5;nombre d&#8217;arbres de la base d&#8217;apprentissage. De plus,
</p>
<p>&#20;
</p>
<p>&#5;
</p>
<p>&#1;
</p>
<p>&#12;&#9;
</p>
<p>&#20;
</p>
<p>&#5;&#12;&#9;
</p>
<p>&#5;
</p>
<p>&#3;
</p>
<p>&#8;&#3;
</p>
<p>&#0;
</p>
<p>&#9;
</p>
<p>&#17;
</p>
<p>&#5;
</p>
<p>&#1;
</p>
<p>&#2;&#6;&#3;&#8;&#4;
</p>
<p>&#20;
</p>
<p>&#5;&#12;&#9;
</p>
<p>&#5;
</p>
<p>&#6;
</p>
<p>&#8;&#3;
</p>
<p>&#0;
</p>
<p>&#9;
</p>
<p>&#17;
</p>
<p>&#5;&#2;&#6;&#3;&#8;&#4;
</p>
<p>&#20;
</p>
<p>&#5;&#12;&#9;
</p>
<p>&#17;
</p>
<p>&#5;
</p>
<p>&#1;
</p>
<p>&#2;&#6;&#3;&#8;&#4;
</p>
<p>&#17;
</p>
<p>&#5;&#2;&#6;&#3;&#8;&#4;
</p>
<p>&#5;
</p>
<p>&#6;
</p>
<p>&#8;&#3;
</p>
<p>&#0;
</p>
<p>&#9;
</p>
<p>&#16;
</p>
<p>&#5;
</p>
<p>&#2;&#18;&#4;&#15;&#3;&#17;
</p>
<p>&#6;&#5;&#2;&#6;&#3;&#8;&#4; (2)
</p>
<p>On note &#3;&#7;&#2;&#18;&#3; &#5;
&#3;
</p>
<p>&#2;
</p>
<p>&#3;
</p>
<p>&#2;
</p>
<p>&#2;&#18;&#3; le nombre de r&#232;gles utilis&#233;es dans un arbre &#18;. On a alors : &#22;&#12; &#3;
&#3;&#2;&#18;&#3; &#5;
</p>
<p>&#3;
</p>
<p>&#2;
</p>
<p>&#6;
</p>
<p>&#0;
</p>
<p>&#3;&#8;&#4;
</p>
<p>&#6;
</p>
<p>&#0;
</p>
<p>&#3;&#8;&#4;
</p>
<p>&#2;&#3;
</p>
<p>&#7;
</p>
<p>&#2;&#18;&#3;&#22;&#12;
</p>
<p>&#2;
</p>
<p>&#3;, et, du fait de la convexit&#233; de l&#8217;exponentielle :
</p>
<p>&#17;
</p>
<p>&#6;&#5;&#2;&#6;&#3;&#8;&#4;
</p>
<p>&#5;
</p>
<p>&#6;
</p>
<p>&#2;
</p>
<p>&#3;
</p>
<p>&#2;
</p>
<p>&#2;&#18;&#3;
</p>
<p>&#3;
</p>
<p>&#7;
</p>
<p>&#2;&#18;&#3;
</p>
<p>&#17;
</p>
<p>&#6;
</p>
<p>&#0;
</p>
<p>&#3;&#8;&#4;&#6;&#5;
</p>
<p>&#0;
</p>
<p>&#2;
</p>
<p>&#20;
</p>
<p>&#5;
</p>
<p>&#1;
</p>
<p>&#12;&#9;
</p>
<p>&#20;
</p>
<p>&#5;&#12;&#9;
</p>
<p>&#5;
</p>
<p>&#6;
</p>
<p>&#8;&#3;
</p>
<p>&#0;
</p>
<p>&#9;
</p>
<p>&#16;
</p>
<p>&#5;
</p>
<p>&#2;&#18;&#4;&#15;&#3;
</p>
<p>&#6;
</p>
<p>&#2;
</p>
<p>&#3;
</p>
<p>&#2;
</p>
<p>&#2;&#18;&#3;
</p>
<p>&#3;
</p>
<p>&#7;
</p>
<p>&#2;&#18;&#3;
</p>
<p>&#17;
</p>
<p>&#6;
</p>
<p>&#0;
</p>
<p>&#3;&#8;&#4;&#6;&#5;
</p>
<p>&#0;
</p>
<p>En injectant ce r&#233;sultat dans l&#8217;in&#233;quation (1), il vient :
</p>
<p>&#22;&#7;
</p>
<p>&#5;
</p>
<p>&#2;&#12;
</p>
<p>&#5;
</p>
<p>&#3; &#8;
</p>
<p>&#6;
</p>
<p>&#7;&#4;&#10;
</p>
<p>&#22;&#12; &#3; &#3;&#2;&#13;&#3;&#0;
</p>
<p>&#6;
</p>
<p>&#7;&#4;&#10;
</p>
<p>&#6;
</p>
<p>&#8;&#7;&#9;&#3;&#7;&#4;
</p>
<p>&#16;
</p>
<p>&#5;
</p>
<p>&#2;&#18;&#4;&#15;&#2;&#13;&#3;&#3;
</p>
<p>&#6;
</p>
<p>&#2;
</p>
<p>&#3;
</p>
<p>&#2;
</p>
<p>&#2;&#18;&#3;
</p>
<p>&#3;
</p>
<p>&#7;
</p>
<p>&#2;&#18;&#3;
</p>
<p>&#17;
</p>
<p>&#6;&#5;
</p>
<p>&#0;
</p>
<p>&#6;
</p>
<p>&#0;
</p>
<p>&#3;&#8;&#4;
</p>
<p>&#7; &#4;&#8;&#4; &#5; &#9;
</p>
<p>&#5;
</p>
<p>&#2;&#12;
</p>
<p>&#5;
</p>
<p>&#3;
</p>
<p>On obtient ainsi le crit&#232;re &#9;
&#5;
</p>
<p>&#2;&#12;
</p>
<p>&#5;
</p>
<p>&#3;, dont le maximum en &#12;&#5; est forc&#233;ment positif, car &#9;
&#5;
</p>
<p>&#2;&#12;&#3; &#5; &#9;,
</p>
<p>et qui est toujours inf&#233;rieur &#224; &#7;
&#5;
</p>
<p>&#2;&#12;
</p>
<p>&#5;
</p>
<p>&#3;. De plus, les gradients au point &#12; de &#7; et &#9;
&#5;
</p>
<p>sont &#233;gaux,
</p>
<p>99</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>A.Rozenknop
</p>
<p>ce qui assure que si le maximum de &#9;
&#5;
</p>
<p>se trouve en &#12;, alors &#12; est un maximum local de &#7; (et
assure du m&#234;me coup que l&#8217;algorithme IIS (v.section 3.3.4) converge vers un maximum local
de &#7;).
Pour aller au plus vite vers le maximum de&#7;, on va donc chercher &#224; maximiser &#9;
</p>
<p>&#5;
</p>
<p>&#2;&#12;
</p>
<p>&#5;
</p>
<p>&#3; &#224; chaque
it&#233;ration de l&#8217;algorithme. On peut pour cela annuler ses d&#233;riv&#233;es partielles en &#22;&#12;
</p>
<p>&#2;
</p>
<p>, en r&#233;solvant
pour chaque r&#232;gle &#11;
</p>
<p>&#2;
</p>
<p>de la grammaire l&#8217;&#233;quation suivante :
</p>
<p>&#9; &#5; &#0;
</p>
<p>&#6;
</p>
<p>&#7;&#4;&#10;
</p>
<p>&#3;
</p>
<p>&#2;
</p>
<p>&#2;&#13;&#3; &#7;
</p>
<p>&#6;
</p>
<p>&#7;&#4;&#10;
</p>
<p>&#6;
</p>
<p>&#8;&#7;&#9;&#3;&#7;&#4;
</p>
<p>&#16;
</p>
<p>&#5;
</p>
<p>&#2;&#18;&#4;&#15;&#2;&#13;&#3;&#3;&#3;
</p>
<p>&#2;
</p>
<p>&#2;&#18;&#3;&#2;&#17;
</p>
<p>&#6;&#5;
</p>
<p>&#0;
</p>
<p>&#3;
</p>
<p>&#6;
</p>
<p>&#0;
</p>
<p>&#3;&#8;&#4; (3)
</p>
<p>Il s&#8217;agit d&#8217;un polyn&#244;me en &#21; &#5; &#17;&#6;&#5;&#0; , de coefficients tous positifs sauf celui de degr&#233; z&#233;ro. Ce
polyn&#244;me est donc facilement annul&#233;, par exemple par la m&#233;thode de Newton.
</p>
<p>3.3.3 Algorithme Inside-Outside
</p>
<p>Le premier terme du polyn&#244;me&#0;
&#3;
</p>
<p>&#7;&#4;&#10;
</p>
<p>&#3;
</p>
<p>&#2;
</p>
<p>&#2;&#13;&#3; est trivialement obtenu : il repr&#233;sente la fr&#233;quence
de la r&#232;gle &#11;
</p>
<p>&#2;
</p>
<p>dans le corpus arbor&#233;.
</p>
<p>Reste le probl&#232;me du calcul des coefficients de degr&#233;s sup&#233;rieurs : le terme
&#3;
</p>
<p>&#8;&#3;
</p>
<p>&#0;
</p>
<p>&#9;
</p>
<p>&#10;&#10;&#10; implique
une sommation sur toutes les analyses de la phrase &#15;. C&#8217;est l&#8217;une des difficult&#233;s majeures
de ce mod&#232;le, le nombre d&#8217;analyses pouvant cro&#238;tre exponentiellement avec la longueur de la
phrase. Cette &#233;tape de calcul n&#233;cessite dans certains mod&#232;les de Markov-Gibbs d&#8217;employer une
m&#233;thode approch&#233;e par &#233;chantillonnage (Pietra et al., 1997). Ici, une factorisation du calcul peut
&#234;tre effectu&#233;e &#224; l&#8217;aide d&#8217;un algorithme Inside-Outside (Charniak, 1993), comme le montrent les
r&#233;critures suivantes.
</p>
<p>On r&#233;crit la troisi&#232;me somme de (3) par :
</p>
<p>&#10;
</p>
<p>&#9;&#12;&#5;
</p>
<p>&#2;&#21;&#3; &#5;
</p>
<p>&#6;
</p>
<p>&#8;&#7;&#9;
</p>
<p>&#16;
</p>
<p>&#5;
</p>
<p>&#2;&#18;&#4;&#15;&#3;&#3;
</p>
<p>&#2;
</p>
<p>&#2;&#18;&#3;&#21;
</p>
<p>&#6;
</p>
<p>&#0;
</p>
<p>&#3;&#8;&#4;
</p>
<p>&#5; &#20;
</p>
<p>&#6;&#0;
</p>
<p>&#5;&#12;&#9;
</p>
<p>&#6;
</p>
<p>&#8;&#7;&#9;
</p>
<p>&#17;
</p>
<p>&#5;&#2;&#6;&#3;&#8;&#4;
</p>
<p>&#3;
</p>
<p>&#2;
</p>
<p>&#2;&#18;&#3;&#21;
</p>
<p>&#6;
</p>
<p>&#0;
</p>
<p>&#3;&#8;&#4;
</p>
<p>Notons &#22;&#2;&#18;&#6; &#23;&#23;&#6; &#11;
&#2;
</p>
<p>&#6; &#24;&#24;&#3; le fait que, dans l&#8217;arbre &#18;, la r&#232;gle &#11;
&#2;
</p>
<p>domine &#15;
&#13;
</p>
<p>&#10;&#10;&#10;&#15;
</p>
<p>&#14;
</p>
<p>(&#22;&#2;&#18;&#6; &#23;&#23;&#6; &#11;
&#2;
</p>
<p>&#6; &#24;&#24;&#3; &#5; &#4;)
ou non (&#22;&#2;&#18;&#6; &#23;&#23;&#6; &#11;
</p>
<p>&#2;
</p>
<p>&#6; &#24;&#24;&#3; &#5; &#9;). Avec cette notation, on a :
</p>
<p>&#10;
</p>
<p>&#9;&#12;&#5;
</p>
<p>&#2;&#21;&#3; &#5; &#2;&#20;
</p>
<p>&#5;&#12;&#9;
</p>
<p>&#3;
</p>
<p>&#6;&#0;
</p>
<p>&#6;
</p>
<p>&#0;&#8;&#13;&#8;&#14;&#8;&#1;&#9;&#1;
</p>
<p>&#6;
</p>
<p>&#8;&#3;
</p>
<p>&#0;
</p>
<p>&#9;
</p>
<p>&#17;
</p>
<p>&#5;&#2;&#6;&#3;&#8;&#4;
</p>
<p>&#21;
</p>
<p>&#6;
</p>
<p>&#0;
</p>
<p>&#3;&#8;&#4;
</p>
<p>&#22;&#2;&#18;&#6; &#23;&#23;&#6; &#11;
</p>
<p>&#2;
</p>
<p>&#6; &#24;&#24;&#3;
</p>
<p>&#5; &#2;&#20;
</p>
<p>&#5;&#12;&#9;
</p>
<p>&#3;
</p>
<p>&#6;&#0;
</p>
<p>&#6;
</p>
<p>&#0;&#8;&#13;&#8;&#14;&#8;&#1;&#9;&#1;
</p>
<p>&#6;
</p>
<p>&#8;&#3;
</p>
<p>&#0;
</p>
<p>&#9;
</p>
<p>&#25; &#2;&#18;&#3;&#22;&#2;&#18;&#6; &#23;&#23;&#6; &#11;
</p>
<p>&#2;
</p>
<p>&#6; &#24;&#24;&#3;
</p>
<p>o&#249; &#25; &#2;&#18;&#3; &#5; &#17;&#5;&#2;&#6;&#3;&#8;&#4;&#21;&#6;&#0;&#3;&#8;&#4; &#5;
&#5;
</p>
<p>&#0;
</p>
<p>&#0;
</p>
<p>&#4;&#8;
</p>
<p>&#2;&#3;
</p>
<p>&#2;
</p>
<p>&#21;&#3;
</p>
<p>&#6;
</p>
<p>&#0;
</p>
<p>&#3;&#8;&#4; : V(y) est le produit des polyn&#244;mes &#4;
&#2;
</p>
<p>&#2;&#21;&#3; &#5; &#2;&#12;
</p>
<p>&#2;
</p>
<p>&#21;&#3;
</p>
<p>associ&#233;s aux r&#232;gles &#11;
&#2;
</p>
<p>qui constituent &#18;.
</p>
<p>D&#8217;apr&#232;s (Goodman, 1998) (pp.26-57)&#3;
&#8;&#3;
</p>
<p>&#0;
</p>
<p>&#9;
</p>
<p>&#25; &#2;&#18;&#3;&#22;&#2;&#18;&#6; &#23;&#23;&#6; &#11;
</p>
<p>&#2;
</p>
<p>&#6; &#24;&#24;&#3; peut &#234;tre calcul&#233; pour tout &#11;
&#2;
</p>
<p>,
</p>
<p>&#23; et &#24; &#224; l&#8217;aide d&#8217;un algorithme Inside-Outside, dans le cas o&#249; la grammaire est sans cycle.
</p>
<p>Preuve : Les conditions d&#233;finies par J. Goodman sont bien r&#233;unies :
&#11; &#26; &#27;&#28;
</p>
<p>&#8;&#9;
</p>
<p>&#5;
</p>
<p>&#23;&#8;&#24;&#6;&#7;&#6; &#6;&#6; &#9;&#6; &#4; &#29;
</p>
<p>4 est un semi-anneau commutatif.
4l&#8217;ensemble des polyn&#244;mes de coefficients positifs, sur lequel sont d&#233;finies les op&#233;rations d&#8217;addition et de
</p>
<p>multiplication habituelles, et leurs &#233;l&#233;ments neutres respectifs 0 et 1. Les valeurs associ&#233;es aux r&#232;gles et aux arbres
dans l&#8217;algorithme Inside-Outside appartiennent &#224; cet ensemble.
</p>
<p>100</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Une grammaire hors-contexte valu&#233;e pour l&#8217;analyse syntaxique
</p>
<p>&#11; La valeur &#25; &#2;&#18;&#3; associ&#233;e &#224; un arbre est le produit des polyn&#244;mes &#4;
&#2;
</p>
<p>&#2;&#21;&#3; associ&#233;s aux r&#232;gles
de &#18;.
</p>
<p>&#11; Une d&#233;rivation dans une table de l&#8217;algorithme est associ&#233;e uniquement &#224; un arbre d&#8217;analyse
(et r&#233;ciproquement), et les r&#232;gles de grammaire apparaissant dans la d&#233;rivation et l&#8217;arbre
correspondant sont les m&#234;mes.
</p>
<p>L&#8217;algorithme n&#8217;est pas d&#233;taill&#233; ici pour des raisons de place. Son principe est le suivant :
</p>
<p>&#11; Pour chaque triplet &#2;&#23;&#6; &#11;
&#2;
</p>
<p>&#6; &#24;&#3;, calculer &#14;&#30;&#31;&#14; &#17;
&#0;
</p>
<p>&#23;&#23;&#6; &#11;
</p>
<p>&#2;
</p>
<p>&#6; &#24;&#24;, qui est la somme des &#25; &#2;&#18;&#3; des
arbres &#18; qui ont pour premi&#232;re r&#232;gle &#11;
</p>
<p>&#2;
</p>
<p>et qui dominent les mots &#15;
&#13;
</p>
<p>&#10;&#10;&#10;&#15;
</p>
<p>&#14;
</p>
<p>.
</p>
<p>&#11; Pour chaque triplet &#2;&#23;&#6; &#0;&#6; &#24;&#3;, calculer !&quot;#&#31;&#14; &#17;&#23;&#23;&#6; &#0;&#6; &#24;&#24;, qui est la somme des &#25; &#2;&#18;&#3; des
arbres &#18; de feuilles &#15;
</p>
<p>&#0;
</p>
<p>&#10;&#10;&#10;&#15;
</p>
<p>&#13;&#6;&#0;
</p>
<p>&#0;&#15;
</p>
<p>&#14;&#8;&#0;
</p>
<p>&#10;&#10;&#10;&#15;
</p>
<p>&#4;
</p>
<p>.
</p>
<p>&#11; En notant $&#2;&#11;
&#2;
</p>
<p>&#3; le symbole de partie gauche de &#11;
&#2;
</p>
<p>, le r&#233;sultat est obtenu par :
&#3;
</p>
<p>&#8;&#3;
</p>
<p>&#0;
</p>
<p>&#9;
</p>
<p>&#25; &#2;&#18;&#3;&#22;&#2;&#18;&#6; &#23;&#23;&#6; &#11;
</p>
<p>&#2;
</p>
<p>&#6; &#24;&#24;&#3; &#5; &#14;&#30;&#31;&#14; &#17;
</p>
<p>&#0;
</p>
<p>&#23;&#23;&#6; &#11;
</p>
<p>&#2;
</p>
<p>&#6; &#24;&#24; &#6; !&quot;#&#31;&#14; &#17;&#23;&#23;&#6; $&#2;&#11;
</p>
<p>&#2;
</p>
<p>&#3;&#6; &#24;&#24;
</p>
<p>3.3.4 Improved Iterative Scaling : algorithme
</p>
<p>Finalement, l&#8217;apprentissage se r&#233;sume par l&#8217;algorithme suivant :
</p>
<p>D&#233;finir un mod&#232;le initial &#0;&#0;, par exemple en mettant tous les param&#232;tres &#224; 0.
R&#233;p&#233;ter :
</p>
<p>&#0;&#0; &#0;
</p>
<p>&#0;
</p>
<p>.
</p>
<p>/* passage de &#0; &#224; &#0;&#0; */
Mise &#224; z&#233;ro des polyn&#244;mes &#1;&#0;&#0;&#2;&#1; associ&#233;s aux r&#232;gles &#3;.
Pour chaque exemple &#4; de la base d&#8217;apprentissage :
</p>
<p>Analyser &#5;&#0;&#4;&#1; par l&#8217;algorithme Inside-Outside.
Calculer &#6;
</p>
<p>&#1;&#2;&#3;&#0;&#4;&#1;
</p>
<p>comme la somme des coefficients du poly-
n&#244;me
</p>
<p>&#0;
</p>
<p>&#0;
</p>
<p>&#7;&#8;&#9;&#7;&#10;&#11;
</p>
<p>&#0;
</p>
<p>&#2;&#3;&#12; &#3;&#12; &#8;&#4;.
</p>
<p>Pour chaque &#233;l&#233;ment &#2;&#13;&#12; &#3;&#12; &#14;&#4; apparaissant dans la table CYK
&#1;
</p>
<p>&#0;
</p>
<p>&#0;&#2;&#1; &#5;&#6; &#1;
</p>
<p>&#0;
</p>
<p>&#0;&#2;&#1; &#7; &#0;&#6;
</p>
<p>&#1;&#2;&#3;&#0;&#4;&#1;
</p>
<p>&#1;
</p>
<p>&#1;&#2;
</p>
<p>&#0;
</p>
<p>&#5;&#2;
</p>
<p>&#0;
</p>
<p>&#3;
</p>
<p>&#15; &#0;&#16;&#1;&#17;&#0;&#16;&#12; &#2;&#13;&#12; &#3;
</p>
<p>&#6;
</p>
<p>&#12; &#14;&#4;&#1;
</p>
<p>(&#6; &#1;&#0;&#0;&#2;&#1; &#7; &#0;&#6;
&#1;&#2;&#3;&#0;&#4;&#1;
</p>
<p>&#1;
</p>
<p>&#1;&#2;
</p>
<p>&#7;&#8;&#9;&#7;&#10;&#11;
</p>
<p>&#0;
</p>
<p>&#2;&#13;&#12; &#3;&#12; &#14;&#4; &#1; &#18;&#19;&#20;&#9;&#7;&#10;&#11;&#2;&#13;&#12;&#21;&#0;&#3;&#1;&#12; &#14;&#4;)
Pour chaque r&#232;gle &#3;
</p>
<p>&#6;
</p>
<p>de la grammaire :
R&#233;soudre : &#1;&#0;&#0;&#0;&#2;&#1; &#6; &#2;&#9;&#19;&#22;
</p>
<p>&#4;&#3;&#7;
</p>
<p>&#23;
</p>
<p>&#6;
</p>
<p>&#0;&#4;&#1;
</p>
<p>Calculer le param&#232;tre du mod&#232;le &#0;&#0; par : &#0;&#0;
&#6;
</p>
<p>&#6; &#0;
</p>
<p>&#6;
</p>
<p>&#7; &#8;&#9;&#10;&#2;
</p>
<p>jusqu&#8217;&#224; la convergence du crit&#232;re&#3;&#0;&#0;&#1;.
D&#233;tails d&#8217;optimisation :
</p>
<p>&#11; le crit&#232;re &#7;&#2;&#12;&#3; est facile &#224; obtenir &#224; la fin d&#8217;une passe, du fait que les constantes de
normalisation &#20;
</p>
<p>&#5;&#12;&#9;&#3;&#7;&#4;
</p>
<p>sont calcul&#233;es lors de cette passe. C&#8217;est pourquoi le test d&#8217;arr&#234;t se
base sur le mod&#232;le pr&#233;c&#233;dent &#12; plut&#244;t que sur le nouveau mod&#232;le &#12;&#5;, quitte &#224; faire une
it&#233;ration inutile : cela &#233;vite le recalcul des constantes de normalisation, qui demanderait
une nouvelle analyse de tous les exemples de la base &#224; chaque passe.
</p>
<p>&#11; l&#8217;algorithme Inside-Outside commence par une analyse syntaxique de la phrase examin&#233;e,
puis calcule les valeurs des polyn&#244;mes &#224; annuler. Pour acc&#233;l&#233;rer l&#8217;apprentissage, les
tables des analyses syntaxiques peuvent &#234;tre sauvegard&#233;es dans une premi&#232;re &#233;tape, ce
qui permet de calculer directement les polyn&#244;mes lors des it&#233;rations.
</p>
<p>101</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>A.Rozenknop
</p>
<p>4 Exp&#233;riences
</p>
<p>4.1 R&#233;sultats
</p>
<p>Le mod&#232;le GCFG a &#233;t&#233; test&#233; sur un corpus d&#233;riv&#233; du corpus SUSANNE#3 (Sampson, 1994),
comptant &#8;&#6;&#25;&#6; arbres d&#8217;analyses, &#4;&#25;&#6;&#9; non-terminaux dont &#0;&#25;&#11; pr&#233;terminaux, &#4;&#4;&#25;&#0;&#11; terminaux,
&#4;&#10;&#12;&#12;&#25; r&#232;gles grammaticales. Certaines r&#232;gles unaires ont &#233;t&#233; manuellement retir&#233;es du corpus
original de fa&#231;on &#224; ce que la grammaire obtenue soit sans boucle.
</p>
<p>Le premier test consiste en l&#8217;apprentissage du mod&#232;le &#224; partir du corpus complet, puis en
l&#8217;analyse syntaxique des phrases du corpus, et enfin en la comparaison des arbres ainsi obtenus
avec les arbres du corpus. Les r&#233;sultats sont regroup&#233;s sous le label %&#17;&#31;# &#5; &#0;&#16;&#16;&#11;&#17;&#30;#&#14;&#31;&#31;&amp;'&#17;
de la figure 3. La colonne Tx(Ana) y repr&#233;sente le taux des phrases recevant au moins une
analyse, Tx(Jus) le taux des phrases correctement analys&#233;es parmi celles recevant au moins
une analyse. Les taux de pr&#233;cision et de rappel (colonnes Pr&#233; et Rap) sont obtenus en con-
sid&#233;rant la s&#233;quence des arbres &#0;&#2; produits par l&#8217;analyseur comme un ensemble (&#2;&#0;&#2; &#3; de triplets
&#26; &#7;&#6; '&#6;  &#29;, o&#249; &#7; est un non-terminal et ' et  sont les positions dans le corpus du premier
et dernier mot de la cha&#238;ne analys&#233;e par &#7; . En comparaison avec une s&#233;quence d&#8217;arbres de
r&#233;f&#233;rence &#0;&#2; &#5;, la pr&#233;cision et le rappel sont calcul&#233;s comme :
</p>
<p>%&#13;&#2;&#4;&#11;&#17;&#3;&#2;&#0;&#2;&#3; &#5;
</p>
<p>&#4;(&#2;&#0;&#2;&#3; &#12; (&#2;&#0;&#2;
</p>
<p>&#5;
</p>
<p>&#3;&#4;
</p>
<p>&#4;(&#2;&#0;&#2;&#3;&#4;
</p>
<p>&#6; %&#13;&#2;&#28;&amp;&#16;&#3;&#2;&#0;&#2;&#3; &#5;
</p>
<p>&#4;(&#2;&#0;&#2;&#3; &#12; (&#2;&#0;&#2;
</p>
<p>&#5;
</p>
<p>&#3;&#4;
</p>
<p>&#4;(&#2;&#0;&#2;
</p>
<p>&#5;
</p>
<p>&#3;&#4;
</p>
<p>Le second test est identique au premier, sinon que l&#8217;apprentissage s&#8217;effectue sur 9 dixi&#232;mes du
corpus, tir&#233;s al&#233;atoirement, et le test sur le dixieme restant. Les r&#233;sultats pr&#233;sent&#233;s sont des
moyennes des r&#233;sultats obtenus avec 10 tirages al&#233;atoires initiaux. La pr&#233;cision P et le rappel R
sont calcul&#233;s sur les seuls arbres recevant au moins une analyse.
</p>
<p>Le mod&#232;le SCFG a &#233;t&#233; test&#233; sur les m&#234;mes bases, et l&#8217;on indique la diminution du taux d&#8217;erreur
que la GCFG offre en comparaison avec la SCFG. Cette valeur est calcul&#233;e par : &#4;&#0; &#0;&#6;&#15;&#7;&#9;&#16;&#17;&#18;&#16;&#10;
</p>
<p>&#0;&#6;&#15;&#7;&#9;&#19;&#17;&#18;&#16;&#10;
</p>
<p>.
</p>
<p>Enfin, le corpus a &#233;t&#233; simplifi&#233; pour une deuxi&#232;me s&#233;rie de tests, en r&#233;duisant les labels des
non-terminaux &#224; leur premi&#232;re lettre, et en supprimant les r&#232;gles unaires, de fa&#231;on &#224; obtenir
une grammaire nettement plus ambigu&#235; sur laquelle l&#8217;utilisation de statistiques semble plus
pertinente, les r&#232;gles apparaissant alors plus souvent dans le corpus d&#8217;apprentissage.
</p>
<p>%&#17;&#31;# &#5; &#0;&#16;&#16;&#11;&#17;&#30;#&#14;&#31;&#31;&amp;'&#17; %&#17;&#31;# &#13;&#5; &#0;&#16;&#16;&#11;&#17;&#30;#&#14;&#31;&#31;&amp;'&#17;
</p>
<p>Mod&#232;le Tx(Ana) Tx(Jus) P R Tx(Ana) Tx(Jus) P R
Corpus Susanne
SCFG &#4; &#9;&#6; &#10;&#26;&#6; &#9;&#6; &#25;&#26;&#25; &#9;&#6; &#25;&#26;&#26; &#9;&#6; &#4;&#0;&#11; &#9;&#6; &#8;&#10;&#6; &#9;&#6; &#25;&#4;&#4; &#9;&#6; &#25;&#6;&#10;
GCFG &#4; &#9;&#6; &#26;&#8;&#11; &#9;&#6; &#25;&#25;&#8; &#9;&#6; &#25;&#25;&#0; &#9;&#6; &#4;&#0;&#11; &#9;&#6; &#8;&#12;&#6; &#9;&#6; &#25;&#9;&#26; &#9;&#6; &#25;&#6;&#8;
Diminution du Tx d&#8217;erreur &#6;&#25;&#1; &#8;&#0;&#1; &#8;&#6;&#1; &#0;&#6;&#1; &#0;&#0;&#1; &#0;&#8;&#1;
Corpus simplifi&#233;
Mod&#232;le Tx(Ana) Tx(Jus) P R Tx(Ana) Tx(Jus) P R
SCFG &#4; &#9;&#10;&#11;&#12;&#26; &#9;&#10;&#25;&#12;&#8; &#9;&#10;&#25;&#12;&#6; &#4; &#9;&#10;&#26;&#12;&#9; &#9;&#10;&#25;&#26;&#0; &#9;&#10;&#25;&#26;&#6;
GCFG &#4; &#9;&#10;&#12;&#9;&#11; &#9;&#10;&#25;&#12;&#26; &#9;&#10;&#25;&#12;&#12; &#4; &#9;&#10;&#26;&#12;&#12; &#9;&#10;&#25;&#26;&#8; &#9;&#10;&#25;&#26;&#0;
Diminution du Tx d&#8217;erreur &#26;&#6; &#12;&#1; &#4;&#4;&#6; &#8;&#1; &#4;&#9;&#6; &#0;&#1; &#8;&#6; &#4;&#1; &#8;&#6; &#11;&#1; &#11;&#6; &#12;&#1;
</p>
<p>Figure 3: R&#233;sultats compar&#233;s des mod&#232;les SCFG et GCFG, sur une t&#226;che d&#8217;analyse syntaxique.
</p>
<p>102</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Une grammaire hors-contexte valu&#233;e pour l&#8217;analyse syntaxique
</p>
<p>Avec les optimisations mentionn&#233;es &#224; la section pr&#233;c&#233;dente, et en limitant &#224; 2005 le nombre
d&#8217;it&#233;rations de IIS, un apprentissage dure environ deux heures sur une station Sun Ultra 10.
</p>
<p>4.2 Discussion
</p>
<p>L&#8217;exemple de la section 2 montre qu&#8217;une SCFG dont les param&#232;tres sont appris depuis un cor-
pus arbor&#233; peut se comporter en analyse de fa&#231;on inattendue, affectant des probabilit&#233;s plus
grandes &#224; des formes rencontr&#233;es moins souvent dans le corpus. Sur le m&#234;me exemple, une
GCFG se comporte en revanche conform&#233;ment &#224; l&#8217;intuition, ce qui peut &#234;tre montr&#233; en calcu-
lant les points fixes de l&#8217;algorithme d&#8217;apprentissage. Cette diff&#233;rence est principalement d&#251;e
aux crit&#232;res d&#8217;apprentissage des mod&#232;les : les SCFG &#233;tant usuellement consid&#233;r&#233;es comme des
grammaires g&#233;n&#233;ratives, l&#8217;apprentissage de leurs param&#232;tres se fait en maximisant la probabil-
it&#233; d&#8217;engendrer un corpus par un processus stochastique produisant un arbre depuis sa racine.
Le crit&#232;re d&#8217;apprentissage est donc6 &#16;
</p>
<p>&#5;
</p>
<p>&#2;&#8;&#3;, maximis&#233; en affectant aux r&#232;gles des probabilit&#233;s
proportionnelles &#224; leur fr&#233;quence en corpus. Un tel mod&#232;le fait apparemment l&#8217;hypoth&#232;se que
le langage est engendr&#233; par un processus grammatical.
</p>
<p>Les GCFG en revanche sont pens&#233;es comme des mod&#232;les pour l&#8217;analyse, d&#8217;o&#249; leur crit&#232;re
d&#8217;apprentissage &#16;
</p>
<p>&#5;
</p>
<p>&#2;&#8;&#4;&#19; &#3;, qui correspond inuitivement &#224; la probabilit&#233; d&#8217;engendrer un corpus
d&#8217;arbres &#224; partir de leurs feuilles. Si possible, ce crit&#232;re sera maximis&#233; lorsque les probabil-
it&#233;s affect&#233;es aux arbres de m&#234;mes feuilles seront proportionnelles &#224; leur fr&#233;quence en corpus.
Comme il y a suffisamment de param&#232;tres dans l&#8217;exemple pour atteindre ce r&#233;sultat, cela ex-
plique l&#8217;ad&#233;quation des fr&#233;quences relatives observ&#233;es et attribu&#233;es par le mod&#232;le.
</p>
<p>Les performances d&#8217;une GCFG sont bonnes en auto-apprentissage (colonne %&#17;&#31;# &#5; &#0;&#16;&#16;&#11;&#17;&#30;#&#14;&#31;-
&#31;&amp;'&#17; de la figure 3) : avec le m&#234;me nombre de param&#232;tres qu&#8217;une SCFG apprise dans les m&#234;mes
conditions, elle r&#233;duit d&#8217;un tiers le taux de phrases mal analys&#233;es, et de moiti&#233; le manque en
pr&#233;cision et en rappel (au niveau des labels); elle &quot;colle&quot; indiscutablement mieux aux donn&#233;es,
ce qui montre que le crit&#232;re d&#8217;apprentissage est adapt&#233; &#224; l&#8217;analyse syntaxique.
</p>
<p>En g&#233;n&#233;ralisation (colonne %&#17;&#31;# &#13;&#5; &#0;&#16;&#16;&#11;&#17;&#30;#&#14;&#31;&#31;&amp;'&#17;), les GCFG ne semblent utiles que lorsque
les r&#232;gles qui apparaissent dans le corpus de test sont souvent repr&#233;sent&#233;es dans le corpus
d&#8217;apprentissage. Dans le cas contraire, elles sont &#233;quivalentes aux SCFG. Cela montre peut-
&#234;tre les limites d&#8217;une approche statistique des grammaires hors-contexte : pour apprendre un
mod&#232;le pertinent, il faut suffisamment d&#8217;exemples pour chacun de ses param&#232;tres, ce qui n&#8217;est
pas le cas avec les corpus existants; ainsi avec le corpus Susanne#3, en g&#233;n&#233;ralisation, on peut se
demander si l&#8217;utilisation de la SCFG est plus pertinente qu&#8217;un tirage al&#233;atoire parmi les analyses
obtenues avec une CFG non probabiliste.
</p>
<p>5 Conclusion
</p>
<p>Cette contribution pr&#233;sente une m&#233;thode de valuation des grammaires hors-contexte qui diff&#232;re
de celle des SCFG par son crit&#232;re d&#8217;apprentissage, mieux adapt&#233; &#224; la t&#226;che d&#8217;analyse syntax-
ique, et par l&#8217;absence de contraintes stochastiques (&#16;
</p>
<p>&#2;
</p>
<p>&#5; &#4;, et
&#3;
</p>
<p>&#16; &#5; &#4;) sur ses param&#232;tres.
La forme des grammaires obtenues &#233;tant sensiblement la m&#234;me que celle des SCFG, on peut
</p>
<p>5crit&#232;re d&#8217;arr&#234;t utilis&#233; pour nos exp&#233;riences
6v.section 3 pour les notations
</p>
<p>103</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>A.Rozenknop
</p>
<p>utiliser des algorithmes standard en analyse syntaxique. Nous avons pr&#233;sent&#233; un algorithme
d&#8217;apprentissage des param&#232;tres, qui factorise suffisament les calculs pour s&#8217;ex&#233;cuter en un
temps raisonnable.
</p>
<p>Les &#233;tudes exp&#233;rimentales montrent qu&#8217;en analyse, les param&#232;tres obtenus collent mieux aux
donn&#233;es d&#8217;apprentissage et &#224; l&#8217;intuition que ceux d&#8217;une SCFG. C&#8217;est aussi le cas en g&#233;n&#233;rali-
sation, &#224; condition que le corpus et la grammaire soient &quot;adapt&#233;s&quot; &#224; un traitement statistique.
La difficult&#233; de trouver de tels corpus met en &#233;vidence les limites d&#8217;une probabilisation des
grammaires hors-contexte.
</p>
<p>D&#8217;autres applications du principe expos&#233; sont envisag&#233;es, comme son adaptation aux gram-
maires stochastiques polyn&#244;miales &#224; substitutions d&#8217;arbres (Chappelier &amp; Rajman, 2001), qui
contiennent plus d&#8217;informations linguistiques que les SCFG, mais dont l&#8217;apprentissage des
param&#232;tres est bas&#233; sur une m&#233;thode empirique, sans qu&#8217;un crit&#232;re th&#233;orique ne soit utilis&#233;.
On envisage &#233;galement l&#8217;adaptation des GCFG au probl&#232;me de reconnaissance de la parole, par
une modification de leur crit&#232;re d&#8217;apprentissage, en maximisant la probabilit&#233; d&#8217;un corpus ar-
bor&#233; conditionnellement &#224; la sortie d&#8217;un module de d&#233;codage acoustique (treillis d&#8217;hypoth&#232;ses),
et non plus conditionnellement &#224; la phrase analys&#233;e.
</p>
<p>R&#233;f&#233;rences
BERGER A. Convexity, maximum likelihood and all that.
CHAPPELIER J.-C. &amp; RAJMAN M. (1998). A generalized CYK algorithm for parsing stochastic CFG.
In TAPD&#8217;98 Workshop, p. 133&#8211;137, Paris (France).
CHAPPELIER J.-C. &amp; RAJMAN M. (2001). Grammaire &#224; substitution d&#8217;arbre de complexit&#233; polynomi-
ale : un cadre efficace pour dop. In TALN&#8217;2001, volume 1, p. 133&#8211;142.
CHAPPELIER J.-C., RAJMAN M., ARAG&#220;&#201;S R. &amp; ROZENKNOP A. (1999). Lattice parsing for speech
recognition. In Proc. of 6&#232;me conf&#233;rence sur le Traitement Automatique du Langage Naturel (TALN99),
p. 95&#8211;104, Carg&#232;se (France).
CHARNIAK E. (1993). Statistical Language Learning. Cambridge, Massachusetts: MIT Press.
CHELBA C. (2000). Exploiting Syntactic Structure for Natural Language Modeling. PhD thesis, John
Hopkins University, Baltimore, Maryland.
GOODMAN J. T. (1998). Parsing Inside-Out. PhD thesis, Harvard University, Cambridge, Mas-
sachusetts.
JOHNSON M. (1998). PCFG Models of Linguistic Tree Representations. Computational Linguistics,
24(4), 613&#8211;632.
LAFFERTY J. (1996). Gibbs-Markov models. In Computing Science and Statistics, volume 27, p. 370&#8211;
377.
PIETRA S. D., PIETRA V. J. D. &amp; LAFFERTY J. D. (1997). Inducing features of random fields. IEEE
Transactions on Pattern Analysis and Machine Intelligence, 19(4), 380&#8211;393.
ROZENKNOP A. &amp; SILAGHI M.-C. (2001). Algorithme de d&#233;codage de treillis selon le crit&#232;re du
co&#251;t moyen pour la reconnaissance de la parole. In Actes de la 8&#232;me conf&#233;rence sur le Traitement
Automatique des Langues Naturelles (TALN&#8217;2001), number 1, p. 391&#8211;396, Tours: Association pour le
Traitement Automatique des Langues.
SAMPSON G. (1994). The Susanne corpus, release 3. In School of Cognitive &amp; Computing Sciences,
Brighton (England): University of Sussex Falmer.
</p>
<p>104</p>

</div></div>
</body></html>