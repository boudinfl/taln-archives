IALIV ZUUZ, Nancy, 24-2’/jum ZUUZ

Une grammaire hors-contexte valuée pour l’analyse
syntaxique

A. Rozenknop
I&C-IIF-LIA - EPFL
CH-1015 Lausanne, Suisse
antoine.rozenknop@epﬂ.ch

Mots-clefs — Keywords

SyntaXe,linguistique mathématique,apprentissage statistique,SCFG,Gibbs,
statistical learning,Syntax,Context-free grammars

Résumé - Abstract

Les grammaires hors-contexte stochastiques sont exploitées par des algorithmes particuliere-
ment efﬁcaces dans des taches de reconnaissance de la parole et d’analyse syntaxique. Cet
article propose une autre probabilisation de ces grammaires, dont les propriétés mathématiques
semblent intuitivement plus adaptées a ces taches que celles des SCFG (Stochastique CFG),
sans nécessiter d’algorithme d’analyse spéciﬁque. L’utilisation de ce modele en analyse sur
du texte provenant du corpus Susanne peut réduire de 33% 1e nombre d’analyses erronées, en
comparaison avec une SCFG entrainée dans les mémes conditions.

Weighted Context-Free Grammars can be used for speech recognition or syntactic analysis
thanks to especially efﬁcient algorithms. In this paper, we propose an instanciation of such
a grammar, whose mathematical properties are intuitively more suitable for those tasks than
SCFG’s (Stochastic CFG), without requiring speciﬁc analysis algorithms. Results on Susanne
text show that up to 33% of analysis errors made by a SCFG can be avoided with this model.

1 Motivations

Bien qu’aujourd’hui dépassées quant a leur pouvoir de description des langues naturelles, les
grammaires hors-contexte stochastiques (SCFG) restent des modeles intéressants pour l’ analyse
syntaxique et la reconnaissance de la parole (Chappelier et al., 1999), du fait qu’elles se pre-
tent a des algorithmes particulierement efﬁcaces remplissant ces taches (Chappelier & Rajman,
1998). Elles peuvent aussi étre choisies comme des représentations intermédiaires calculatoires
de grammaires plus riches, comme les grammaires a substitution d’arbres polynomiales (Chap-

95

l'.l.l\l/(,CIl«I\¢I Ll/1/

(A) VP (3)

>NP\ N1ZP\PP
/\ / :11; /\N P/ /M:

D D
VDetNPDetN V et “N

Figure 1: Corpus d’apprentissage E. Ce corpus contient l’arbre (A) avec une fréquence relative
f, et B avec une fréquence relative 1 — f.

pelier & Rajman, 2001). Cependant, certaines de leurs propriétés mathématiques réduisent la
qualité des résultats que l’on peut escompter de leur utilisation (Rozenknop & Silaghi, 2001).

Cet article propose une nouvelle pondération des grammaires hors-contexte, qui est essentielle-
ment une variante non-générative des SCFG. Noté GCFG (pour "Gibbsian CFG"), ce modele
associe a chaque regle hors-contexte un "potentiel" réel, a la place des probabilités d’une SCFG,
et s’appuie sur un critere d’apprentissage "orienté analyse". L’ intérét de ce modele est qu’il fait
preuve d’un meilleur comportement en analyse tout en préservant l’efﬁcacité algorithmique des
SCFG. La suite de cet article montre un exemple de comportement non-intuitif des SCFG dans
la section 2, décrit le modele GCFG et un algorithme d’apprentissage de ses parametres dans la
section 3, compare les comportements des SCFG et des GCFG dans la section 4.1 et donne des
conclusions dans la section 5.

2 Exemple de comportement indésirable des SCFG

Cet exemple est tiré d’une étude de M. Johnson (Johnson, 1998), et illustre un comportement
des SCFG qui peut sembler paradoxal. Supposons que l’on dispose d’un corpus $1 constitué de
deux arbres (A) et (B) (cf ﬁg. 1), l’arbre A apparaissant avec une fréquence relativel f. On
entraine une SCFG sur ce corpus selon la méthode habituelle, qui consiste a affecter aux regles
une probabilité proportionnelle a leur fréquence en corpus.

Les probabilités (B1) des regles apprises a partir de ce corpus sont les suivantes : B1 (VP —>
VNP) = f, B1(VP —> V NP PP): 1 — f, B1(NP —> Det N) = 2/(2 + f) et B1(NP —>
NP PP) = f/(2 + f). Lors d’une analyse syntaxique de la "phrase" V Det N P Det N,
les structures (A) et (B) se voient donc affectées des probabilités  = 4 f2 / (2 + f )3 et
B:(B) = 4(1 — f)/(2 + f)2. La fréquence relative estimée fl de (A), pour la phrase V Det
N P Det N, est alors : 1/”; = B1(A)/(B1(A) + B1(B)) = f2/(2 — f). Idéalement, cette
fréquence relative estimée 1/”; devrait étre proche de sa fréquence relative réelle f dans le corpus
d’apprentissage. La relation entre f et fl est tracée sur la ﬁgure 2. C/)\n voit que les deux
fonctions peuvent différer substantiellement. Par exemple, si f = 0,75, fl = 0,45, i.e. meme
si (A) apparait trois fois plus souvent que (B) dans le corpus d’apprentissage, la phrase sera
analysée par (B) !

M. Johnson soupconne que ce comportement est dﬁ a la non-systématicité de la construction

1i.e. son nombre d’occurences divisé par la taille du corpus.

96

L/ILC 6111/IILIILIJLIC Ill/Is)'bl/Il«l/C./l«l/C Vllrl/I/ICC 1/1/I/DI L Ilrlldlrl/)’L)C s))’Il«l/Ila/ldrlil/l«C

   

U’ 1
OJ
OJ
-E 0.8 -
U:
‘D  
g /....x’
- 2 0 . 6 ' -.f’.»-'’
E /.,/‘A’ ‘/52
GJ  ..... 
W 0.4 -  _________ —— 
8 f.(/, _____ ,. ,.
C  .... ”
g 0'2 ' 
0- ,;5:’/'‘
9 M’ _____ ..
‘-|— 0 I’ I I I
0 0.2 0.4 0.6 0.8 1

frequence relative observee en corpus

Figure 2: Fréquences relatives estimées de l’attachement de PP a NP, tracées en fonction de la
frequence relative f de cet attachement observée dans les divers corpus d’apprentissage étudiés.

des structures dans le corpus d’apprentissage : dans (A), (NP =>* Det N PP) est sous forme
adjonctive de Chomsky, alors que (VP =>* V NP PP) est sous forme plate dans  Pour
tester cette hypothese, les calculs peuvent étre recommencés en modiﬁant le corpus, soit en
aplanissant (NP =>* Det N PP) dans (A), soit en remplacant (VP —> V NP PP) par les deux
regles (VP —> VP PP) et (VP —> V NP) dans  La premiere solution mene a une frequence
relative estimée du premier arbre valant : 1/‘; = (f2 — 2f (2f2 — f — 2), et la deuxieme a :
_]/E3 = f 2 / (2 — 3f + 2 f 2). Ces deux fréquences estimées sont plus proches de f que E, comme
l’illustre la ﬁgure 2, mais restent inférieures : dans tous les cas, si la fréquence relative observée
de l’attachement de PP a NP (arbre (A)) vaut 0, 6, l’analyse syntaxique de V Det N P Det
N attachera PP a VP, malgré sa plus faible frequence dans le corpus (0,4).

Le modele GCFG, présenté dans la suite de cet article, peut étre évalué sur les trois corpus
précédemment déﬁnis. Dans chaque cas, la fréquence relative attribuée a (A) est égale a sa
frequence relative observée. Les trois courbes se confondent et sont notées f sur la ﬁgure 2.

3 Grammaire Hors-Contexte avec potentiels de Gibbs (GCFG)

Le modele exposé ici s’inspire directement du modele SCFG. La graIr1maire est composée d’un
ensemble de N, regles de récriture X —> Y1...Y|,|, de N3 symboles terminaux et non-terminaux,
les symboles terminaux n’apparaissant que dans les parties droites des regles. De plus, a chaque
regle 1", est associé un potentiel )\,~. 2

2Dans une grammaire SCFG, chaque régle est associée a une probabilité, les probabilités des régles dc méme
panic gauche devant alors sommer a 1 (contrainte stochastique).

97

l'.l.l\l/(,CIl«I\¢I Ll/1/

3.1 Potentiel d’un arbre, et probabilité conditionnelle

Contrairement a une SCFG, ce modele n’est pas générateur. On ne cherche donc pas a déﬁnir
la probabilité de production d’un arbre avec ce modele. En revanche, on déﬁnit le potentiel
d’un arbre d’analyse :1: come la somme des potentiels des regles qui le constituent. I1 s’agit
donc du produit scalaire A - f de deux vecteurs de taille N,., la 715 composante de A étant le
potentiel Ai de la regle Ti, et la 2'8 composante de f étant le nombre d’occurence(s) fi de la
regle n dans l’arbre :3.

On déﬁnit de plus la probabilité d’un arbre d’analyse :3 conditionnellement a ses feuilles w =
(w1...wn) (i.e. w représente la phrase analysée) par la formule :

1’*<~"l“’> = 
y=>*w

o1‘1 Zy:>,.w est une somme sur tous les arbres y de la grammaire qui ont pour feuilles w.

3.2 Analyse syntaxique 2‘1l’aide d’une GCFG

L’ analyse syntaxique d’une phrase 11) consiste a déterrniner l’arbre E de plus fort potentiel parmi
les arbres d’analyse possibles. Au vu de la formule précédente, cela correspond a trouver l’arbre
de probabilité maximale, conditionnellement a la phrase analysée :

:73 = ArgmaX)\ - f(:v) = ArgrnaXp;\(J:|w) = Argmax H e)"'f"(‘")
w=>*w w=>*w :c=>*w
‘M651:
La derniere expression montre a quel point ce modele est proche d’un modele SCFG pour
effectuer l’analyse syntaxique. En effet, avec une SCFG, la solution est donnée par : :2 =

Argmaxxjtw p(:E) = Argmaxwjtw HUB” p(7",~)f*'(””)

L’uti1isation d’un modele GCFG ne nécessite donc pas le développement d’un algorithme
d’analyse spéciﬁque : on peut réutiliser tel quel tout algorithme d’analyse de SCFG, a con-
dition que celui-ci ne requiere pas les conditions p(7"Z~) 3 1 ni Z” = 13. En particulier, les
algorithrnes de type A* (Chelba, 2000), qui nécessitent un score décroissant avec le nombre
d’étapes de l’analyse, doivent étre évités. Un algorithme tabulaire ascendant classique (Chap-
pelier & Rajman, 1998) a été utilisé pour les tests d’analyse, en remplacant simplement les
probabilités des regles p(r,~) par e"2'.

3.3 Apprentissage des paramétres
3.3.1 Principe

I-/3tant donné un corpus d’apprentissage, constitué de phrases W et d’arbres d’analyse X de ces
phrases, on cherche a calculer les parametres du modele A de facon a maximiser la probabilité
des arbres conditionnellement aux phrases :

X = Argma.Xp;\(X|W) = Argmax ZlI1p)\($‘|U)($‘)) = Argmax /l()\)
A A

:c€X

3somme sur les régles de méme partie gauche.

98

L/ILC 6111/IILIILIJLIC Ill/Is)'bl/Il«l/C./l«l/C Vllrl/I/ICC 1/1/I/DI L Ilrlldlrl/)’L)C s))’Il«l/Ila/ldrlil/l«C

(en notant w(:v) la phrase analysée par l’arbre 51:, i.e. les feuilles de 30.) .A(A) est la "vraisem-
blance conditionnelle" du corpus; l’apprentissage du modele consiste en la maximisation de ce
critere. On note ici l’une des differences fondamentales de ce modele avec une SCFG, pour
laquelle on cherche en general a maximiser la probabilité du corpus p ,\(X ), ce qui se résoud
facilement en affectant a chaque regle une probabilité proportionnelle a sa fréquence dans le
corpus.

3.3.2 Improved Iterative Scaling : théorie

La méthode utilisée ici pour le calcul des parametres s’inspire directement de la méthode IIS
(Improved Iterative Scaling) exposée dans (Berger, ; Pietra et al., 1997; Lafferty, 1996) pour la
probabilisation de champs de Markov. Elle est décrite dans la suite de cette section, pour le cas
particulier du modele GCFG.

Plutot que de chercher a maximiser directement le critere .A(A), ce qui se révele trop ardu, la
méthode améliore itérativement le modele, a partir d’un modele initial A0. Une itération consiste
a passer d’un modele A a un modele A’, en tentant de maximiser sur A’ le critere AA,\(A’) =

A(A’) — A(A) = gm In 

pA(w|w(w))

Posons Z M, = Zxikw e"'f(’”) la constante de normalisation associée a la phrase 11) dans le
modele A. On peut alors écrire p;\(:z:|w) = (Z;\,w)‘1e"'f(’”), et :

AA)<A') = Z<A' — A) - ma) — 

wEX :c€X

En majorant le logarithme par la formule ln oz 3 oz — 1, on peut minorer A/l,\(A’) par :
Z ’ w .1}
A/l)(A’) 2 ZAA-f(:v) — ZZ*’—"+ |X| (1)
wEX wEX )"w(w)
ou AA = A’ — A, et |X | =nombre d’arbres de la base d’apprentissage. De plus,

A-f(y) A’-f(y)
 = Z z>A(yIw)e“"‘”’ <2)

y=>*w

ZNM _ ::y:>*w EA’-f(y) _ Z
ZA,w — ZA,w —

y=>*w

On note f #(y) =  f,~(y) le nombre de regles utilisées dans un arbre y. On a alors : AA -
f =   f #(y)AA,~), et, du fait de la convexité de l’eXponentielle :

. fz'(Z/) # . ZA’ w  # -
AA f( ) f ( )A)\2 A f ( MA)
6 y ‘ Z #(:t/)6 y :> ZA,w S 2 p”(y’w’:f#(y)e ’’

*0:

i y=>*w i

En injectant ce résultat dans l’inéquation (1), il vient :

AA)(A’) 2 2 AA - f(rv) — Z Z p)(ylw(rv)) Z Ji::%i/’)e““‘#‘-‘I’ + |Xl = B)(A')
)

:c€X :c€X y—;w(_z i

On obtient ainsi le critere B,\(A’), dont le maximum en A’ est forcément positif, car B;\(A) = 0,
et qui est toujours inférieur a .»4;\(A’ De plus, les gradients au point A de A et B ,\ sont égaux,

99

l'.l.l\l/(,CIl«I\¢I Ll/1/

ce qui assure que si le maximum de BA se trouve en A, alors A est un maximum local de A (et
assure du meme coup que l’algorithme IIS (v.section 3.3.4) converge vers un maximum local

de A).

Pour aller au plus Vite vers le maximum de A, on Va donc chercher a maximiser B)‘ (X ) a chaque
itération de l’algorithme. On peut pour cela annuler ses dérivées partielles en A)\i, en résolvant
pour chaque regle Ti de la grammaire l’équation suivante :

0 = - Zfz-<w> + Z Z m<ylw<x>>ri<y><eW#<y> <3)

I/REX I/REX y—>w(:c)

I1 s’agit d’un polynome en oz = eA’\i, de coefﬁcients tous positifs sauf celui de degré zéro. Ce
polynome est donc facilement annulé, par exemple par la méthode de Newton.

3.3.3 Algorithme Inside-Outside

Le premier terme du polynome — 296 EX fi  est trivialement obtenu : il représente la fréquence
de la regle Ti dans le corpus arboré.

Reste le probleme du calcul des coefﬁcients de degrés supérieurs : le terme Zyﬁiw  implique
une sommation sur toutes les analyses de la phrase 11). C’est l’une des difﬁcultés majeures
de ce modele, le nombre d’analyses pouvant croitre exponentiellement avec la longueur de la
phrase. Cette étape de calcul nécessite dans certains modeles de Markov-Gibbs d’employer une
méthode approchée par échantillonnage (Pietra et al. , 1997). Ici, une factorisation du calcul peut
étre effectuée a l’aide d’un algorithme Inside-Outside (Charniak, 1993), comme le montrent les
récritures suivantes.

On récrit la troisieme somme de (3) par :

3w,A(Oi) = Zp)\(y|w)fl.(y)Oif#(y) =  Z 6),-f(y)fl.(y)Oif#(y)

y—>w y—>w

Notons C(y, [j, Ti,  le fait que, dans l’arbre y, la regle Ti domine wjuluwk (C(y, [_7', Ti,  = 1)
ou non (C (y, [_7', Ti,  = 0). Avec cette notation, on a :

Sw,)\(a) = <zi,..,>* Z Ze*"‘”a’#‘y>0<y, [mi-,k1>

1SjSkS|w|y:>*“’

= (ZA,w)_1 Z Zv<y>0<y,[.m,k1>

1315/€S|w| ?F>*“’
ou V(y) = e"'f(y)ozf#(y) = ]_[n€y(fioz)fi(?’) : V(y) est le produit des polynomes Pi(a) = (Aioz)
associés aux regles Ti qui constituent y.

D’apres (Goodman, 1998) (pp.26-57) Zyﬁiw V(y)C(y, [j, Ti,  peut étre calculé pour tout Ti,
j et k a l’aide d’un algorithme Inside-Outside, dans le cas ou la grammaire est sans cycle.

Preuve : Les conditions déﬁnies par J. Goodman sont bien réunies :
o < 1R(J{°°[X], +, >n<, 0, 1 >4 est un semi-anneau commutatif.

41’ensemble des polynémes de coefﬁcients positifs, sur lequel sont déﬁnies les operations d’addition et de
multiplication habituelles, et leurs elements neutres respectifs 0 et 1. Les Valeurs associées aux regles et aux arbres
dans 1’a1gorith1ne Inside-Outside appartiennent a cet ensemble.

100

L/ILC 6111/IILIILIJLIC Ill/Is)'bl/Il«l/C./l«l/C Vllrl/I/ICC 1/1/I/DI L Ilrlldlrl/)’L)C s))’Il«l/Ila/ldrlil/l«C

0 La valeur V(y) associée a un arbre est le produit des polynomes Pi (oz) associés aux regles
de y.

o Une dérivation dans une table de l’ algorithme est associée uniquement a un arbre d’ analyse
(et réciproquement), et les regles de grammaire apparaissant dans la dérivation et l’arbre
correspondant sont les memes.

L’ algorithme n’est pas détaillé ici pour des raisons de place. Son principe est le suivant :

0 Pour chaque triplet (j, ri, k), calculer z'ns7lde,[j, n, is], qui est la somme des V(y) des
arbres y qui ont pour premiere regle ri et qui dominent les mots w j...wk.

0 Pour chaque triplet (j, A, Is), calculer outs7§de[j, A, k], qui est la somme des V(y) des
arbres y de feuilles w1...wj_1Awk+1...wn.

0 En notant G  le symbole de partie gauche de m, le résultat est obtenu par :

Z:y:>*w V(y)0(y, [J3 1», /6]) = insiderlj, ma /6] * Outsidelj, Gm), kl

3.3.4 Improved Iterative Scaling : algorithme

Finalement, l’apprentissage se résume par l’algorithme suivant :

Déﬁnir un modele initial A’, par exemple en mettant tous les parametres a 0.
Répéter :
A <— A’.
/* passage de A a A’ */
Mise a zero des polynomes S ’(oz) associés aux regles 7".
Pour chaque exemple :1: de la base d’apprentissage :
Analyser w(x) par l’a1gorith1ne Inside—Outside.
Calculer Z ,\,w($) comme la somme des coefﬁcients du poly-
nome Zr 2'ns2'deT[1, 7", 
Pour chaque element Li, 7", k] apparaissant dans la table CYK
SW1) == S’(0<) + (ZA,w(:c))—1 ::y=>*w V(1/)C'(1/,[J',nJ<7])
(= S’(a) + (Z;\7w(,c))‘1insideT[j,r, I<:] * outsideﬁ, G(r), /<:])
Pour chaque regle m de la grammaire :
Résoudre : S " (a) = —summEX
Calculer le parametre du modele A’ par : Ag = A1’ + log a
jusqu’2‘1 la convergence du critere .A(A).

Détails d’optimisation :

0 le critere A(A) est facile a obtenir a la ﬁn d’une passe, du fait que les constantes de
normalisation Z ;\,w($) sont calculées lors de cette passe. C’est pourquoi le test d’arrét se
base sur le modele précédent A plutot que sur le nouveau modele A’, quitte a faire une
itération inutile : cela évite le recalcul des constantes de normalisation, qui demanderait
une nouvelle analyse de tous les exemples de la base a chaque passe.

o l’algorithme Inside-Outside commence par une analyse syntaxique de la phrase examinée,
puis calcule les valeurs des polynomes a annuler. Pour accélérer l’apprentissage, les
tables des analyses syntaxiques peuvent étre sauvegardées dans une premiere étape, ce
qui permet de calculer directement les polynomes lors des itérations.

101

l'.l.l\l/(,CIl«I\¢I Ll/1/

4 Expériences

4.1 Résultats

Le modele GCFG a été testé sur un corpus dérivé du corpus SUSANNE#3 (Sampson, 1994),
comptant 4292 arbres d’analyses, 1920 non-terminaux dont 395 préterminaux, 11935 terminaux,
17669 regles graInmaticales. Certaines regles unaires ont été manuellement retirées du corpus
original de facon a ce que la grammaire obtenue soit sans boucle.

Le premier test consiste en l’apprentissage du modele a partir du corpus complet, puis en
l’analyse syntaxique des phrases du corpus, et enﬁn en la comparaison des arbres ainsi obtenus
avec les arbres du corpus. Les résultats sont regroupés sous le label Test = Apprentissage
de la ﬁgure 3. La colonne Tx (Ana) y représente le taux des phrases recevant au moins une
analyse, Tx (J us) le taux des phrases correctement analysées parmi celles recevant au moins
une analyse. Les taux de précision et de rappel (colonnes Pré et Rap) sont obtenus en con-
sidérant la séquence des arbres ? produits par l’analyseur comme un ensemble E(?) de triplets
< N, g, d >, o1‘1 N est un non-terminal et g et d sont les positions dans le corpus du premier
et dernier mot de la chaine analysée par N. En comparaison avec une séquence d’arbres de
référence ?" , la précision et le rappel sont calculés comme :
_ |E(7) F7 E(?’)|

T1:(Pre) —  , T:t(Rap)(7") =

IE6’) F7 E(?’)|
|E(?’)|

Le second test est identique au premier, sinon que l’apprentissage s’effectue sur 9 dixiemes du
corpus, tirés aléatoirement, et le test sur le dixieme restant. Les résultats présentés sont des
moyennes des résultats obtenus avec 10 tirages aléatoires initiaux. La précision P et le rappel R
sont calculés sur les seuls arbres recevant au moins une analyse.

Le modele SCFG a été testé sur les memes bases, et l’on indique la diminution du taux d ’erreur

que la GCFG offre en comparaison avec la SCFG. Cette valeur est calculée par : 1 — 

Enﬁn, le corpus a été simpliﬁé pour une deuxieme série de tests, en réduisant les labels des
non-terminaux a leur premiere lettre, et en supprimant les regles unaires, de facon a obtenir
une grammaire nettement plus ambigue sur laquelle l’utilisation de statistiques semble plus
pertinente, les regles apparaissant alors plus souvent dans le corpus d’apprentissage.

Test = Apprentissage Test 75 Apprentissage

Modele TX(Ana) | TX(Jus) | P | R TX(Ana) | TX(Jus) | P | R
Corpus Susanne

SCFG 1 0, 782 0, 989 0, 988 0,135 0, 472 0, 911 0, 927
GCFG 1 0, 845 0, 994 0, 993 0,135 0, 462 0, 908 0, 924
Diminution du TX d’erreur 29% 43% 42% —2% —3% —4%
Corpus simpliﬁé

Modele TX(Ana) TX(Jus) P R TX(Ana) TX(Jus) P R
SCFG 1 0.568 0.964 0.962 1 0.860 0.983 0.982
GCFG 1 0.605 0.968 0.966 1 0.866 0.984 0.983
Diminution du TX d’erreur 8, 6% 11, 4% 10, 3% 4, 1% 4, 5% 5, 6%

Figure 3: Résultats comparés des modeles SCFG et GCFG, sur une tache d’analyse syntaxique.

102

L/ILC 6111/IILIILIJLIC Ill/Is)'bl/Il«l/C./l«l/C Vllrl/I/ICC 1/1/I/DI L Ilrlldlrl/)’L)C s))’Il«l/Ila/ldrlil/l«C

Avec les optimisations mentionnées a la section précédente, et en limitant a 2005 le nombre
d’itérations de IIS, un apprentissage dure environ deux heures sur une station Sun Ultra 10.

4.2 Discussion

L’ exemple de la section 2 montre qu’une SCFG dont les parametres sont appris depuis un cor-
pus arboré peut se comporter en analyse de facon inattendue, affectant des probabilités plus
grandes a des formes rencontrées moins souvent dans le corpus. Sur le meme exemple, une
GCFG se comporte en revanche conformément a l’intuition, ce qui peut etre montre en calcu-
lant les points ﬁxes de l’algorithme d’apprentissage. Cette difference est principalement dﬁe
aux criteres d’apprentissage des modeles : les SCFG étant usuellement considérées comme des
grammaires génératives, l’apprentissage de leurs parametres se fait en maximisant la probabil-
ité d’engendrer un corpus par un processus stochastique produisant un arbre depuis sa racine.
Le critere d’apprentissage est donc6 p;\(X), maximise en affectant aux régles des probabilités
proportionnelles a leur fréquence en corpus. Un tel modele fait apparemment l’hypothese que
le langage est engendré par un processus grammatical.

Les GCFG en revanche sont pensées comme des modeles pour l’analyse, d’o1‘1 leur critere
d’apprentissage p ,\ (X |W), qui correspond inuitivement a la probabilité d’engendrer un corpus
d’arbres a partir de leurs feuilles. Si possible, ce critere sera maximise lorsque les probabil-
ités affectées aux arbres de memes feuilles seront proportionnelles a leur fréquence en corpus.
Comme il y a sufﬁsamment de parametres dans l’exemple pour atteindre ce résultat, cela ex-
plique l’adéquation des fréquences relatives observées et attribuées par le modele.

Les performances d’une GCFG sont bonnes en auto-apprentissage (colonne Test = Apprent7Is-
sage de la ﬁgure 3) : avec le meme nombre de parametres qu’une SCFG apprise dans les memes
conditions, elle réduit d’un tiers le taux de phrases mal analysées, et de moitié le manque en
precision et en rappel (au niveau des labels); elle "colle" indiscutablement mieux aux données,
ce qui montre que le critere d’apprentissage est adapté a l’analyse syntaxique.

En generalisation (colonne Test gé Apprentissage), les GCFG ne semblent utiles que lorsque
les regles qui apparaissent dans le corpus de test sont souvent représentées dans le corpus
d’apprentissage. Dans le cas contraire, elles sont équivalentes aux SCFG. Cela montre peut-
etre les limites d’une approche statistique des grammaires hors-contexte : pour apprendre un
modele pertinent, il faut sufﬁsamment d’exemples pour chacun de ses parametres, ce qui n’est
pas le cas avec les corpus existants; ainsi avec le corpus Susanne#3, en generalisation, on peut se
demander si l’utilisation de la SCFG est plus pertinente qu’un tirage aléatoire parmi les analyses
obtenues avec une CFG non probabiliste.

5 Conclusion

Cette contribution présente une méthode de valuation des grammaires hors-contexte qui differe
de celle des SCFG par son critere d’apprentissage, mieux adapté a la teche d’analyse syntax-
ique, et par l’absence de contraintes stochastiques (p, 3 1, et 2 p = 1) sur ses parametres.
La forme des grammaires obtenues étant sensiblement la meme que celle des SCFG, on peut

Scritere d’arret utilise pour nos experiences
5V.section 3 pour les notations

103

l'.l.l\l/(,CIl«I\¢I Ll/1/

utiliser des algorithmes standard en analyse syntaxique. Nous avons présenté un algorithme
d’apprentissage des parametres, qui factorise sufﬁsament les calculs pour s’eXécuter en un
temps raisonnable.

Les études expérimentales montrent qu’en analyse, les parametres obtenus collent mieux aux
données d’apprentissage et a l’intuition que ceux d’une SCFG. C’est aussi le cas en generali-
sation, a condition que le corpus et la grammaire soient "adaptés" a un traitement statistique.
La difﬁculté de trouver de tels corpus met en évidence les limites d’une probabilisation des
grammaires hors-contexte.

D’autres applications du principe exposé sont envisagées, comme son adaptation aux gram-
maires stochastiques polynomiales a substitutions d’arbres (Chappelier & Rajman, 2001), qui
contiennent plus d’informations linguistiques que les SCFG, mais dont l’apprentissage des
parametres est basé sur une méthode empirique, sans qu’un critere théorique ne soit utilisé.
On envisage également l’adaptation des GCFG au probleme de reconnaissance de la parole, par
une modiﬁcation de leur critere d’apprentissage, en maximisant la probabilité d’un corpus ar-
boré conditionnellement a la sortie d’un module de décodage acoustique (treillis d’hypotheses),
et non plus conditionnellement a la phrase analysée.

Références

BERGER A. Convexity, maximum likelihood and all that.

CHAPPELIER J .-C. & RAJMAN M. (1998). A generalized CYK algorithm for parsing stochastic CFG.
In TAPD’98 Workshop, p. 133-137, Paris (France).

CHAPPELIER J .-C. & RAJMAN M. (2001). Grammaire a substitution d’arbre de complexité polynomi-
ale 2 un cadre efﬁcace pour dop. In TALN’200I, volume 1, p. 133-142.

CHAPPELIER J .-C., RAJMAN M., ARAGUES R. & ROZENKNOP A. (1999). Lattice parsing for speech
recognition. In Proc. of 6eme conférence sur le Traitement Automatique du Langage Naturel (TALN99),
p. 95-104, Cargése (France).

CHARNIAK E. (1993). Statistical Language Learning. Cambridge, Massachusetts: MIT Press.

CHELBA C. (2000). Exploiting Syntactic Structure for Natural Language Modeling. PhD thesis, John
Hopkins University, Baltimore, Maryland.

GOODMAN J . T. (1998). Parsing Inside-Out. PhD thesis, Harvard University, Cambridge, Mas-
sachusetts.

JOHNSON M. (1998). PCFG Models of Linguistic Tree Representations. Computational Linguistics,
24(4), 613-632.

LAFFERTY J . (1996). Gibbs-Markov models. In Computing Science and Statistics, volume 27, p. 370-
377.

PIETRA S. D., PIETRA V. J . D. & LAFFERTY J . D. (1997). Inducing features of random ﬁelds. IEEE
Transactions on Pattern Analysis and Machine Intelligence, 19(4), 380-393.

ROZENKNOP A. & SILAGHI M.-C. (2001). Algorithme de décodage de treillis selon le critére du
coﬁt moyen pour la reconnaissance de la parole. In Actes de la éfme confe’rence sur le Traitement
Automatique des Langues Naturelles (TALN’200I), number 1, p. 391-396, Tours: Association pour le
Traitement Automatique des Langues.

SAMPSON G. (1994). The Susanne corpus, release 3. In School of Cognitive & Computing Sciences,
Brighton (England): University of Sussex Falmer.

104

