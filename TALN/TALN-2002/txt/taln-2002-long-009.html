<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
"http://www.w3.org/TR/html4/loose.dtd">
<html><head><title>Identification th&#233;matique hi&#233;rarchique : Application aux forums de discussions</title>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
</head>
<body>
<div style="page-break-before:always; page-break-after:always"><div><p>TALN 2002, Nancy, 24&#8211;27 juin 2002
</p>
<p>Identification th&#233;matique hi&#233;rarchique :
Application aux forums de discussions
</p>
<p>Brigitte Bigi, Kamel Sma&#239;li
LORIA - Universit&#233; Henri Poincar&#233;
</p>
<p>Campus Scientifique, BP 239, 54506 Vand&#339;uvre-l&#232;s-Nancy
{bigi, smaili}@loria.fr
</p>
<p>Mots-clefs &#8211; Keywords
</p>
<p>Identification th&#233;matique, mod&#232;les de langage, unigrammes
Topic identification, language modeling, unigrams
</p>
<p>R&#233;sum&#233; - Abstract
</p>
<p>Les mod&#232;les statistiques du langage ont pour but de donner une repr&#233;sentation statistique de
la langue mais souffrent de nombreuses imperfections. Des travaux r&#233;cents ont montr&#233; que
ces mod&#232;les peuvent &#234;tre am&#233;lior&#233;s s&#8217;ils peuvent b&#233;n&#233;ficier de la connaissance du th&#232;me trait&#233;,
afin de s&#8217;y adapter. Le th&#232;me du document est alors obtenu par un m&#233;canisme d&#8217;identification
th&#233;matique, mais les th&#232;mes ainsi trait&#233;s sont souvent de granularit&#233; diff&#233;rente, c&#8217;est pourquoi
il nous semble opportun qu&#8217;ils soient organis&#233;s dans une hi&#233;rarchie. Cette structuration des
th&#232;mes implique la mise en place de techniques sp&#233;cifiques d&#8217;identification th&#233;matique. Cet
article propose un mod&#232;le statistique &#224; base d&#8217;unigrammes pour identifier automatiquement le
th&#232;me d&#8217;un document parmi une arborescence pr&#233;d&#233;finie de th&#232;mes possibles. Nous pr&#233;sen-
tons &#233;galement un crit&#232;re qui permet au mod&#232;le de donner un degr&#233; de fiabilit&#233; &#224; la d&#233;cision
prise. L&#8217;ensemble des exp&#233;rimentations a &#233;t&#233; r&#233;alis&#233; sur des donn&#233;es extraites du groupe &#8217;fr&#8217;
des forums de discussion.
</p>
<p>Statistical language modeling attempts to capture the regularities of natural language. The most
accurate natural language processing systems still suffer from several shortcomings due to the
complexity of natural language and from the weakness of the current language models. It is
commonly conjectured that they should benefit from topic adaptation. The topic of the doc-
ument is then obtained by a topic identification mechanism, but topics thus treated are often
of different granularity. This is the reason why it seems appropriate to organize them in a
hierarchy. This topic organization implies a development of specific techniques for topic iden-
tification. This paper proposes a statistical model based on unigrams to automatically identify
the topic of a document among a tree structure of possible topics. We also present a criterion
which reflects the degree of reliability of the decision. Experiments were carried out on data
extracted from the French newsgroup &#8217;fr&#8217;.
</p>
<p>115</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Brigitte Bigi, Kamel Sma&#239;li
</p>
<p>1 Introduction
</p>
<p>La mod&#233;lisation statistique du langage est une partie cruciale d&#8217;une grande vari&#233;t&#233; d&#8217;applications
relatives aux technologies du langage, telles que la reconnaissance automatique de la parole
(RAP) ou la recherche documentaire. Le but des mod&#232;les de langage est de capturer les r&#233;gu-
larit&#233;s du langage naturel en estimant les fr&#233;quences des mots ou des expressions dans un his-
torique. Ces syst&#232;mes souffrent toujours d&#8217;imperfections dues &#224; la complexit&#233; du langage na-
turel et &#224; la faiblesse des mod&#232;les de langage actuels. Des travaux r&#233;cents montrent que les
mod&#232;les de langage doivent tirer b&#233;n&#233;fice de la connaissance du th&#232;me trait&#233; afin de s&#8217;y adapter.
C&#8217;est pourquoi, cet article s&#8217;int&#233;resse au probl&#232;me de l&#8217;identification th&#233;matique, afin que le
th&#232;me des documents trait&#233;s soit d&#233;termin&#233; automatiquement.
</p>
<p>L&#8217;identification th&#233;matique a ainsi pour but d&#8217;assigner un label th&#233;matique &#224; un texte parmi
un ensemble de labels possibles. Cet article pr&#233;sente une approche d&#8217;identification th&#233;matique
dans laquelle on exploite les relations s&#233;mantiques entre th&#232;mes, par le biais d&#8217;une arborescence.
Ainsi, par exemple, il peut s&#8217;av&#233;rer int&#233;ressant de sp&#233;cifier que football, escrime, natation ou
encore plong&#233;e sont des sous-th&#232;mes de sport. Un grand nombre de mots sont tr&#232;s probables
de fa&#231;on relativement commune &#224; chacune de ces sous-cat&#233;gories (comme tournoi, rencontre,
exploit, arbitre...), ce qui permettra de favoriser les th&#232;mes issus de sport, tandis que certains
mots sont sp&#233;cifiques &#224; l&#8217;une ou l&#8217;autre des sous-cat&#233;gories (but, &#233;p&#233;e, maillot, tuba).
Le fait de savoir que les th&#232;mes sont tous relatifs au sport permet &#224; la fois d&#8217;&#233;liminer un cer-
tains nombre de cat&#233;gories &quot;concurrentes&quot; pour l&#8217;identification du th&#232;me mais surtout d&#8217;&#233;viter
quelques ambigu&#239;t&#233;s sur les homonymes (le tuba de plong&#233;e, ou le tuba instrument de musique).
Nous pensons que les relations s&#233;mantiques &#233;tablies par la hi&#233;rarchie peuvent devenir un atout
pr&#233;cieux afin d&#8217;obtenir une identification th&#233;matique plus fiable. Par ailleurs, lors de la phase
d&#8217;adaptation, les relations &#233;tablies entre les mod&#232;les de langages th&#233;matiques de l&#8217;arborescence
permettent de compl&#233;ter certains mod&#232;les trop pauvres, comme cela est fait dans (Seymore &amp;
Rosenfeld, 1997).
Dans cet article, nous proposons l&#8217;utilisation de mod&#232;les de langage de type unigrammes,
car ces derniers ont d&#233;j&#224; montr&#233; leur potentiel &#224; discriminer les th&#232;mes dans des applications
d&#8217;identification th&#233;matiques (Li &amp; Yamamishi, 1997; Bigi et al., 2000). Nous proposons des
unigrammes th&#233;matiques hi&#233;rarchiques qui ont pour particularit&#233; le fait que les relations entre
fr&#232;res sont favoris&#233;es dans le mod&#232;le, par l&#8217;attribution d&#8217;un vocabulaire commun. Par ailleurs,
ce mod&#232;le utilisera son pouvoir discriminant afin d&#8217;auto-&#233;valuer sa d&#233;cision th&#233;matique, en four-
nissant un degr&#233; de fiabilit&#233; sur le th&#232;me qu&#8217;il choisit. Le mod&#232;le a &#233;t&#233; valid&#233; sur des donn&#233;es
des forums de discussion fran&#231;ais. Dans une premi&#232;re section, cet article pr&#233;sente le probl&#232;me
de l&#8217;identification th&#233;matique et la mani&#232;re dont il a &#233;t&#233; d&#233;crit dans la litt&#233;rature. Cette premi&#232;re
partie abordre &#233;galement la notion de hi&#233;rarchisation des th&#232;mes. Dans la deuxi&#232;me section,
on pr&#233;sente le mod&#232;le d&#8217;identification th&#233;matique utilis&#233; : un mod&#232;le unigramme th&#233;matique
hi&#233;rarchique. Enfin, la derni&#232;re section donne les performances concluantes que nous avons
obtenues avec notre approche.
</p>
<p>2 Positionnement du probl&#232;me
</p>
<p>Les travaux r&#233;cents (Kneser &amp; Peters, 1997; Martin et al., 1997; Mahajan et al., 1999; Khu-
danpur &amp; Wu, 1999; Bigi et al., 2000) ont montr&#233; que l&#8217;adaptation des mod&#232;les de langage au
</p>
<p>116</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Identification th&#233;matique hi&#233;rarchique : Application aux forums de discussions
</p>
<p>th&#232;me du discours permettent une am&#233;lioration significative de la perplexit&#233;1 (Jelinek, 1990).
Souvent, ce sont des techniques issues du domaine de la recherche documentaire qui sont util-
is&#233;es. Le probl&#232;me que nous soulevons dans ces travaux concerne le fait que le th&#232;me est obtenu
apr&#232;s une analyse de la totalit&#233; du document, ce qui n&#8217;est &#233;videmment pas envisageable pour
une adaptation th&#233;matique dynamique dans un syst&#232;me de RAP. Nos travaux se focalisent donc
sur des m&#233;thodes statistiques qui auront pour motivation de d&#233;terminer au mieux le th&#232;me d&#8217;un
document avec le minimum d&#8217;information possible. Dans ce cas pr&#233;cis, nos travaux pr&#233;c&#233;dents
nous ont permis d&#8217;observer que les classifieurs bay&#233;siens de type unigrammes obtiennent les
meilleures performances d&#8217;identification th&#233;matique, par rapport &#224; des m&#233;thodes classiques.
</p>
<p>2.1 Repr&#233;sentation des th&#232;mes dans une hi&#233;rarchie
</p>
<p>Dans le cas de l&#8217;identification th&#233;matique hi&#233;rarchique, le but est d&#8217;exploiter la repr&#233;sentation
arborescente des th&#232;mes. La litt&#233;rature dans le domaine de la hi&#233;rarchisation des th&#232;mes reste
assez pauvre. L&#8217;article principal concernant la reconnaisance automatique de la parole est celui
de (Seymore &amp; Rosenfeld, 1997). Dans un premier temps, ils construisent une arborescence
de clusters par un syst&#232;me simple &#224; base de mots-cl&#233;s, puis ils apprennent le mod&#232;le qui corre-
spond &#224; chaque cluster. Pour d&#233;terminer le &quot;th&#232;me&quot; du texte, ils utilisent le classifieur TFIDF
(Salton, 1991). Dans le domaine de la RAP, on trouve aussi l&#8217;article (Galescu &amp; Allen, 2000)
o&#249; les auteurs proposent un un &quot;mod&#232;le de langage statistique hybride hi&#233;rarchique&quot;. Dans les
deux cas, la combinaison du mod&#232;le classique avec le mod&#232;le issu de la hi&#233;rarchie am&#233;liore
significativement la perplexit&#233;.
</p>
<p>On retrouve des hi&#233;rarchies de th&#232;mes &#233;galement dans le domaine de la recherche documentaire
sur l&#8217;internet, o&#249; les documents sont souvent hi&#233;rarchis&#233;s (Yahoo par exemple). (McCallum
et al., 1998) prouvent que les classifieurs bay&#233;siens donnent de meilleures classifications si les
donn&#233;es sont hi&#233;rarchis&#233;es en th&#232;mes. Ce r&#233;sultat s&#8217;appuie sur une exp&#233;riementation avec des
donn&#233;es de certaines sous-cat&#233;gories du web et des newsgroups.
</p>
<p>Les travaux pr&#233;sent&#233;s dans ce document concernent le probl&#232;me de l&#8217;identification th&#233;matique
hi&#233;rarchique, dont le but est de d&#233;terminer automatiquement le th&#232;me d&#8217;un texte parmi un en-
semble hi&#233;rarchis&#233; de th&#232;mes. Ces travaux s&#8217;inscrivent dans le cadre d&#8217;une am&#233;lioration des
mod&#232;les de langage pour la RAP (figure 1). L&#8217;objectif est de r&#233;soudre les probl&#232;mes de dif-
f&#233;rences de granularit&#233; entre les th&#232;mes et de profiter des liens s&#233;mantiques entre les th&#232;mes
pour l&#8217;adaptation des mod&#232;les de langage. Malheureusement, les corpus traditionnellement
utilis&#233;s en RAP ne sont pas hi&#233;rarchis&#233;s. Cette &#233;tude portera sur le groupe &quot;fr&quot; de UseNet.
</p>
<p>1En g&#233;n&#233;ral, les mod&#232;les de langage pour la reconnaissance de la parole sont &#233;valu&#233;s en fonction de leur
impact sur la pr&#233;cision de la reconnaissance. N&#233;anmoins, ils peuvent &#234;tre &#233;valu&#233;s s&#233;par&#233;ment si l&#8217;on consid&#232;re, par
exemple, leur capacit&#233; de pr&#233;diction des mots d&#8217;un texte. La mesure la plus utilis&#233;e est la perplexit&#233;. La perplexit&#233;
d&#8217;un mod&#232;le de langage d&#233;riv&#233;e d&#8217;un corpus est d&#233;finie comme suit :
</p>
<p>&#0;&#0; &#0; &#1;
</p>
<p>&#0;&#1; &#0;&#2;
</p>
<p>&#0;
</p>
<p>&#0;
</p>
<p>&#1;
</p>
<p>&#0; &#0; &#2;&#1;
</p>
<p>&#3;
</p>
<p>&#2;
</p>
<p>&#3;
</p>
<p>&#0;
</p>
<p>&#0;
</p>
<p>&#0; (1)
</p>
<p>o&#249; &#2;&#0; &#2;&#1; &#3;
&#2;
</p>
<p>&#3; est le logarithme de la probabilit&#233; de la s&#233;quence de &#3; mots &#1; &#3;
&#2;
</p>
<p>attribu&#233;e par un mod&#232;le de langage
bigramme. Pour l&#8217;&#233;valuation d&#8217;un mod&#232;le de langage, on estime les probabilit&#233;s du mod&#232;le avec un ensemble
d&#8217;apprentissage et on &#233;value la perplexit&#233; avec ce mod&#232;le sur un corpus de texte enti&#232;rement diff&#233;rent du corpus
d&#8217;apprentissage.
</p>
<p>117</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Brigitte Bigi, Kamel Sma&#239;li
</p>
<p>di
d5
</p>
<p>d1
</p>
<p>d2
</p>
<p>dn
d8 clustering
</p>
<p>root
</p>
<p>c1 c2 ck
</p>
<p>root
</p>
<p>ml1 ml2 mlk
</p>
<p>Pprotocole FTP. Protocole d&#8217;application
</p>
<p>il est utilis&#233; pour transf&#233;rer des fichiers.
</p>
<p>faisant partie de la pile de protocole TCP/IP ;
</p>
<p>(File Transfer Protocol)
</p>
<p>Le protocole FTP est d&#233;fini dans la RFC 959.
</p>
<p>Deux modes de transferts sont possibles :
</p>
<p>&#8722; binaire : transfert de tous les bits du
fichier sans aucune modification, 
</p>
<p>caract&#232;res si besoin (ASCII ou EBCDIC) 
&#8722; caract&#232;re : transformation du codage des
</p>
<p>Nouveau Document
</p>
<p>Hi&#233;rarchie de clusters de documentsEnsemble des documents
des corpus d&#8217;apprentissage
</p>
<p>Hi&#233;rarchie de mod&#232;les de langage th&#233;matiques
</p>
<p>ML
</p>
<p>Mod&#232;le de langage g&#233;n&#233;ral
</p>
<p>Quel est le th&#232;me du document ?
</p>
<p>Interpolation
</p>
<p>Figure 1: Processus d&#8217;int&#233;gration d&#8217;une hi&#233;rarchie de th&#232;mes pour la RAP
</p>
<p>2.2 Application : sp&#233;cificit&#233; des forums de discussion
</p>
<p>Les &quot;News&quot; sont des forums de discussion f&#233;d&#233;r&#233;s par th&#232;me, o&#249;, pendant une dur&#233;e de temps
donn&#233;e, tous les courriers envoy&#233;s sont conserv&#233;s. Chaque forum est appel&#233; en anglais news-
group, chaque article d&#8217;un newsgroup est appel&#233; une News. Les groupes sont subdivis&#233;s en
sous-groupes, etc ce qui cr&#233;e une arborescence dans les groupes. Dans le cadre de l&#8217;identification
th&#233;matique hi&#233;rarchique, les news constituent une plate-forme int&#233;ressante. En effet, on peut
consid&#233;rer que les donn&#233;es des news se rapprochent des donn&#233;es de parole selon plusieurs as-
pects. En premier lieu, on peut citer le niveau de langue, qui est souvent familier. Par ailleurs,
les messages sont souvent tr&#232;s courts, comprennent de nombreuses abr&#233;viations, des fautes
d&#8217;orthographes et de grammaire, ainsi que des fichiers attach&#233;s de toutes natures, autant de
difficult&#233;s qui peuvent &#234;tre assimil&#233;es aux fautes que commet un syst&#232;me de RAP.
</p>
<p>3 Mod&#232;les unigrammes pour la classification hi&#233;rarchique
</p>
<p>3.1 Mod&#232;le unigramme th&#233;matique classique
</p>
<p>On note &#0;&#0;
&#0;
</p>
<p>&#0; &#0;&#1;
</p>
<p>&#0;
</p>
<p>&#2; &#1;
</p>
<p>&#1;
</p>
<p>&#2; &#1; &#1; &#1; &#2; &#1;
</p>
<p>&#0;
</p>
<p>&#2; le message dont le th&#232;me est &#224; d&#233;terminer, parmi les &#3;
th&#232;mes pr&#233;d&#233;finis. Le but de ce mod&#232;le est de d&#233;terminer &#4; &#1;&#5;
</p>
<p>&#1;
</p>
<p>&#3; &#0;
</p>
<p>&#0;
</p>
<p>&#0;
</p>
<p>&#2;, c&#8217;est-&#224;-dire la probabil-
it&#233; &#224; attribuer &#224; chaque th&#232;me en fonction du message. Selon la r&#232;gle de Bayes, on &#233;value cette
probabilit&#233; telle que :
</p>
<p>&#4; &#1;&#5;
</p>
<p>&#1;
</p>
<p>&#3; &#0;
</p>
<p>&#0;
</p>
<p>&#0;
</p>
<p>&#2; &#0;
</p>
<p>&#4; &#1;&#5;
</p>
<p>&#1;
</p>
<p>&#2;&#6;&#4; &#1;&#0;
</p>
<p>&#0;
</p>
<p>&#0;
</p>
<p>&#3; &#5;
</p>
<p>&#1;
</p>
<p>&#2;
</p>
<p>&#4; &#1;&#0;
</p>
<p>&#0;
</p>
<p>&#0;
</p>
<p>&#2;
</p>
<p>(2)
</p>
<p>o&#249; &#4; &#1;&#5;
&#1;
</p>
<p>&#2; est la probabilit&#233; a priori du th&#232;me &#5;
&#1;
</p>
<p>, et &#4; &#1;&#0;&#0;
&#0;
</p>
<p>&#3; &#5;
</p>
<p>&#1;
</p>
<p>&#2; repr&#233;sente la probabilit&#233; de la
s&#233;quence de mots &#0;&#0;
</p>
<p>&#0;
</p>
<p>, &#233;tant donn&#233; un th&#232;me &#5;
&#1;
</p>
<p>. Celle-ci s&#8217;estime comme suit :
</p>
<p>&#4; &#1;&#0;
</p>
<p>&#0;
</p>
<p>&#0;
</p>
<p>&#3; &#5;
</p>
<p>&#1;
</p>
<p>&#2; &#0;
</p>
<p>&#0;
</p>
<p>&#2;&#2;&#0;
</p>
<p>&#4; &#1;&#1;
</p>
<p>&#2;
</p>
<p>&#3; &#5;
</p>
<p>&#1;
</p>
<p>&#2;
</p>
<p>118</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Identification th&#233;matique hi&#233;rarchique : Application aux forums de discussions
</p>
<p>Le probl&#232;me est donc d&#8217;obtenir &#4; &#1;&#1; &#3; &#5;
&#1;
</p>
<p>&#2;, la probabilit&#233; d&#8217;un mot &#1; dans un th&#232;me &#5;
&#1;
</p>
<p>, pour
chaque mot &#1; du vocabulaire &#7;
</p>
<p>&#1;
</p>
<p>de &#5;
&#1;
</p>
<p>:
</p>
<p>&#4; &#1;&#1; &#3; &#5;
</p>
<p>&#1;
</p>
<p>&#2; &#0;
</p>
<p>&#1;
</p>
<p>&#8;
</p>
<p>&#1;
</p>
<p>&#9;&#1;&#1; &#3; &#5;
</p>
<p>&#1;
</p>
<p>&#2; &#10;&#11; &#1;&#1; &#4; &#7;
</p>
<p>&#1;
</p>
<p>&#2;
</p>
<p>&#12; &#10;&#11;&#13;&#14;&#13;
</p>
<p>(3)
</p>
<p>o&#249;&#2;
&#3;
</p>
<p>&#0;
</p>
<p>&#0;&#4;
</p>
<p>&#1;
</p>
<p>&#8;
</p>
<p>&#1;
</p>
<p>&#9;&#1;&#1; &#3; &#5;
</p>
<p>&#1;
</p>
<p>&#2;&#3;&#12; &#0; &#4;, &#9;&#1;&#1; &#3; &#5;
&#1;
</p>
<p>&#2; est la fr&#233;quence du mot &#1; dans &#5;
&#1;
</p>
<p>, apprise sur un cor-
pus d&#233;di&#233; au th&#232;me &#5;
</p>
<p>&#1;
</p>
<p>, et, &#8;
&#1;
</p>
<p>est un c&#339;fficient de normalisation qui assure que la distribution des
probabilit&#233;s &#4; &#1;&#1; &#3; &#5;
</p>
<p>&#1;
</p>
<p>&#2; somme &#224; 1. Comme il n&#8217;y a pas de mot inconnu lors de l&#8217;apprentissage,
la valeur de &#12; sera donc attribu&#233;e &#224; tous les mots n&#8217;appartenant pas au vocabulaire de &#5;
</p>
<p>&#1;
</p>
<p>lors de
la phase de test. Elle repr&#233;sente la probabilit&#233; du mot inconnu, not&#233; UNK.
</p>
<p>3.2 Mod&#232;les unigrammes th&#233;matiques hi&#233;rarchiques
</p>
<p>La hi&#233;rarchie des th&#232;mes est pr&#233;sent&#233;e sous la forme d&#8217;un arbre (figure 2). A chaque niveau
&#15;, on d&#233;finit des groupes de n&#339;uds fr&#232;res, not&#233;s &#16; (les fr&#232;res, &#233;tant, par d&#233;finition, des n&#339;uds
de m&#234;me p&#232;re). Notons &#233;galement &#5;
</p>
<p>&#1;&#5;&#6;
</p>
<p>, le &#17;-&#232;me th&#232;me, du &#16;-&#232;me groupe de fr&#232;res, du &#15;-&#232;me
niveau, ainsi que &#5;
</p>
<p>&#5;&#6;
</p>
<p>un groupe de fr&#232;res. On notera &#233;galement &#7;
&#1;&#5;&#6;
</p>
<p>le vocabulaire du th&#232;me
&#5;
</p>
<p>&#1;&#5;&#6;
</p>
<p>, d&#233;duit du corpus d&#8217;apprentissage de ce th&#232;me.
</p>
<p>&#0;&#0;
</p>
<p>&#0;&#0;
</p>
<p>&#0;&#0;
</p>
<p>&#1;&#1;
</p>
<p>&#1;&#1;
</p>
<p>&#1;&#1;
</p>
<p>&#0;&#0;
</p>
<p>&#0;&#0;
</p>
<p>&#0;&#0;
</p>
<p>&#1;&#1;
</p>
<p>&#1;&#1;
</p>
<p>&#1;&#1;
</p>
<p>&#0;
</p>
<p>&#0;
</p>
<p>&#0;
</p>
<p>&#1;
</p>
<p>&#1;
</p>
<p>&#1;
</p>
<p>&#0;
</p>
<p>&#0;
</p>
<p>&#0;
</p>
<p>&#1;
</p>
<p>&#1;
</p>
<p>&#1;
</p>
<p>&#0;&#0;
</p>
<p>&#0;&#0;
</p>
<p>&#0;&#0;
</p>
<p>&#1;&#1;
</p>
<p>&#1;&#1;
</p>
<p>&#1;&#1;
</p>
<p>&#0;&#0;
</p>
<p>&#0;&#0;
</p>
<p>&#0;&#0;
</p>
<p>&#1;&#1;
</p>
<p>&#1;&#1;
</p>
<p>&#1;&#1;
</p>
<p>&#0;&#0;&#0;&#0;&#0; &#0;
</p>
<p>&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;
</p>
<p>&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;
</p>
<p>&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;
</p>
<p>&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;
</p>
<p>&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;
</p>
<p>&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;
</p>
<p>&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;
</p>
<p>&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;
</p>
<p>&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;
</p>
<p>&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;
</p>
<p>&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;
</p>
<p>&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;
</p>
<p>&#1;&#1;&#1;&#1;&#1; &#1;
</p>
<p>&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;
</p>
<p>&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;
</p>
<p>&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;
</p>
<p>&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;
</p>
<p>&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;
</p>
<p>&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;
</p>
<p>&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;
</p>
<p>&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;
</p>
<p>&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;
</p>
<p>&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;
</p>
<p>&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;
</p>
<p>&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;
</p>
<p>&#0;&#0; &#0;&#0;&#0;&#0;&#0;
</p>
<p>&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;
</p>
<p>&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;
</p>
<p>&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;
</p>
<p>&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;
</p>
<p>&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;
</p>
<p>&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;
</p>
<p>&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;
</p>
<p>&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;
</p>
<p>&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;
</p>
<p>&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;
</p>
<p>&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;
</p>
<p>&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;
</p>
<p>&#1;&#1; &#1;&#1;&#1;&#1;&#1;
</p>
<p>&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;
</p>
<p>&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;
</p>
<p>&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;
</p>
<p>&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;
</p>
<p>&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;
</p>
<p>&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;
</p>
<p>&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;
</p>
<p>&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;
</p>
<p>&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;
</p>
<p>&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;
</p>
<p>&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;
</p>
<p>&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;
</p>
<p>&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;
</p>
<p>&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;
</p>
<p>&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;
</p>
<p>&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;
</p>
<p>&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;
</p>
<p>&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;
</p>
<p>&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;
</p>
<p>&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;
</p>
<p>&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;
</p>
<p>&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;
</p>
<p>&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;
</p>
<p>&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;
</p>
<p>&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;
</p>
<p>&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;
</p>
<p>&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;
</p>
<p>&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;
</p>
<p>&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;
</p>
<p>&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;
</p>
<p>&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;
</p>
<p>&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;
</p>
<p>&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;
</p>
<p>&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;
</p>
<p>&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;
</p>
<p>&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;
</p>
<p>&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;
</p>
<p>&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;
</p>
<p>&#0;
</p>
<p>&#0;
</p>
<p>&#0;
</p>
<p>&#1;
</p>
<p>&#1;
</p>
<p>&#1;
</p>
<p>&#0;&#0;
</p>
<p>&#0;&#0;
</p>
<p>&#0;&#0;
</p>
<p>&#1;&#1;
</p>
<p>&#1;&#1;
</p>
<p>&#1;&#1;
</p>
<p>&#0;
</p>
<p>&#0;
</p>
<p>&#0;
</p>
<p>&#1;
</p>
<p>&#1;
</p>
<p>&#1;
</p>
<p>&#0;
</p>
<p>&#0;
</p>
<p>&#1;
</p>
<p>&#1;
</p>
<p>&#0;&#0;&#0;
</p>
<p>&#0;&#0;&#0;&#0;
</p>
<p>&#0;&#0;&#0;&#0;
</p>
<p>&#0;&#0;&#0;&#0;
</p>
<p>&#0;&#0;&#0;&#0;
</p>
<p>&#0;&#0;&#0;&#0;
</p>
<p>&#0;&#0;&#0;&#0;
</p>
<p>&#0;&#0;&#0;&#0;
</p>
<p>&#0;&#0;&#0;&#0;
</p>
<p>&#0;&#0;&#0;&#0;
</p>
<p>&#0;&#0;&#0;&#0;
</p>
<p>&#0;&#0;&#0;&#0;
</p>
<p>&#0;&#0;&#0;&#0;
</p>
<p>&#1;&#1;&#1;
</p>
<p>&#1;&#1;&#1;&#1;
</p>
<p>&#1;&#1;&#1;&#1;
</p>
<p>&#1;&#1;&#1;&#1;
</p>
<p>&#1;&#1;&#1;&#1;
</p>
<p>&#1;&#1;&#1;&#1;
</p>
<p>&#1;&#1;&#1;&#1;
</p>
<p>&#1;&#1;&#1;&#1;
</p>
<p>&#1;&#1;&#1;&#1;
</p>
<p>&#1;&#1;&#1;&#1;
</p>
<p>&#1;&#1;&#1;&#1;
</p>
<p>&#1;&#1;&#1;&#1;
</p>
<p>&#1;&#1;&#1;&#1;
</p>
<p>&#0;&#0;
</p>
<p>&#0;&#0;&#0;
</p>
<p>&#0;&#0;&#0;
</p>
<p>&#0;&#0;&#0;
</p>
<p>&#0;&#0;&#0;
</p>
<p>&#0;&#0;&#0;
</p>
<p>&#0;&#0;&#0;
</p>
<p>&#0;&#0;&#0;
</p>
<p>&#0;&#0;&#0;
</p>
<p>&#0;&#0;&#0;
</p>
<p>&#0;&#0;&#0;
</p>
<p>&#0;&#0;&#0;
</p>
<p>&#0;&#0;&#0;
</p>
<p>&#0;&#0;&#0;
</p>
<p>&#1;&#1;
</p>
<p>&#1;&#1;&#1;
</p>
<p>&#1;&#1;&#1;
</p>
<p>&#1;&#1;&#1;
</p>
<p>&#1;&#1;&#1;
</p>
<p>&#1;&#1;&#1;
</p>
<p>&#1;&#1;&#1;
</p>
<p>&#1;&#1;&#1;
</p>
<p>&#1;&#1;&#1;
</p>
<p>&#1;&#1;&#1;
</p>
<p>&#1;&#1;&#1;
</p>
<p>&#1;&#1;&#1;
</p>
<p>&#1;&#1;&#1;
</p>
<p>&#1;&#1;&#1;
</p>
<p>&#0;&#0;&#0; &#0;&#0;
</p>
<p>&#0;&#0;&#0;&#0;&#0;&#0;&#0;
</p>
<p>&#0;&#0;&#0;&#0;&#0;&#0;&#0;
</p>
<p>&#0;&#0;&#0;&#0;&#0;&#0;&#0;
</p>
<p>&#0;&#0;&#0;&#0;&#0;&#0;&#0;
</p>
<p>&#0;&#0;&#0;&#0;&#0;&#0;&#0;
</p>
<p>&#0;&#0;&#0;&#0;&#0;&#0;&#0;
</p>
<p>&#0;&#0;&#0;&#0;&#0;&#0;&#0;
</p>
<p>&#0;&#0;&#0;&#0;&#0;&#0;&#0;
</p>
<p>&#0;&#0;&#0;&#0;&#0;&#0;&#0;
</p>
<p>&#0;&#0;&#0;&#0;&#0;&#0;&#0;
</p>
<p>&#0;&#0;&#0;&#0;&#0;&#0;&#0;
</p>
<p>&#0;&#0;&#0;&#0;&#0;&#0;&#0;
</p>
<p>&#0;&#0;&#0;&#0;&#0;&#0;&#0;
</p>
<p>&#1;&#1;&#1; &#1;&#1;
</p>
<p>&#1;&#1;&#1;&#1;&#1;&#1;&#1;
</p>
<p>&#1;&#1;&#1;&#1;&#1;&#1;&#1;
</p>
<p>&#1;&#1;&#1;&#1;&#1;&#1;&#1;
</p>
<p>&#1;&#1;&#1;&#1;&#1;&#1;&#1;
</p>
<p>&#1;&#1;&#1;&#1;&#1;&#1;&#1;
</p>
<p>&#1;&#1;&#1;&#1;&#1;&#1;&#1;
</p>
<p>&#1;&#1;&#1;&#1;&#1;&#1;&#1;
</p>
<p>&#1;&#1;&#1;&#1;&#1;&#1;&#1;
</p>
<p>&#1;&#1;&#1;&#1;&#1;&#1;&#1;
</p>
<p>&#1;&#1;&#1;&#1;&#1;&#1;&#1;
</p>
<p>&#1;&#1;&#1;&#1;&#1;&#1;&#1;
</p>
<p>&#1;&#1;&#1;&#1;&#1;&#1;&#1;
</p>
<p>&#1;&#1;&#1;&#1;&#1;&#1;&#1;
</p>
<p>&#0;&#0;&#0;&#0;&#1;&#1;&#1;&#1;divers newusersnewgroups .... questions automatique
</p>
<p>discussion  incognito  info   outils      publications
</p>
<p>cogni
</p>
<p>&#0;&#0;
</p>
<p>&#0;&#0;
</p>
<p>&#1;&#1;
</p>
<p>&#1;&#1;
</p>
<p>&#0;&#0;
</p>
<p>&#0;&#0;
</p>
<p>&#1;&#1;
</p>
<p>&#1;&#1;
</p>
<p>&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;
</p>
<p>&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;
</p>
<p>&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;
</p>
<p>&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;
</p>
<p>&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;
</p>
<p>&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;
</p>
<p>&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;
</p>
<p>&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;
</p>
<p>&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;
</p>
<p>&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;
</p>
<p>&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;
</p>
<p>&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;
</p>
<p>&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;
</p>
<p>&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;
</p>
<p>&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;
</p>
<p>&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;
</p>
<p>&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;
</p>
<p>&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;
</p>
<p>&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;
</p>
<p>&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;
</p>
<p>&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;
</p>
<p>&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;
</p>
<p>&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;
</p>
<p>&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;&#0;
</p>
<p>&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;
</p>
<p>&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;
</p>
<p>&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;
</p>
<p>&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;
</p>
<p>&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;
</p>
<p>&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;
</p>
<p>&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;
</p>
<p>&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;&#1;
</p>
<p>&#0;&#0;&#0;&#0;&#0;&#1;&#1;&#1;&#1;&#1; &#0;&#0;&#0;&#0;&#0;&#1;&#1;&#1;&#1;&#1;
</p>
<p>k=0
</p>
<p>k=2
</p>
<p>k=3
</p>
<p>k=1
</p>
<p>fr
</p>
<p>annonce bienvenue sci test
</p>
<p>Figure 2: Exemple d&#8217;une hi&#233;rarchie de th&#232;mes, groupe &#8217;fr&#8217; de UseNet
</p>
<p>3.2.1 Distribution de probabilit&#233;s des n&#339;uds
</p>
<p>Pour rendre compte des relations s&#233;mantiques fraternelles, nous proposons que chaque th&#232;me
d&#8217;un groupe de fr&#232;res donn&#233; soit repr&#233;sent&#233; par le m&#234;me vocabulaire, issu de l&#8217;union des vo-
cabulaires de chacun des th&#232;mes fr&#232;res. Ce vocabulaire est not&#233; &#7;
</p>
<p>&#5;&#6;
</p>
<p>. Ceci implique que certains
mots n&#8217;auront pas &#233;t&#233; observ&#233;s durant l&#8217;apprentissage et par cons&#233;quent &#9;&#1;&#1; &#3; &#5;
</p>
<p>&#1;
</p>
<p>&#2; &#0; &#5;. Le
mod&#232;le classique des unigrammes de l&#8217;&#233;quation (3) est donc modifi&#233; en introduisant un deux-
i&#232;me niveau de repli (back-off), tel que :
</p>
<p>&#4; &#1;&#1; &#3; &#5;
</p>
<p>&#1;&#5;&#6;
</p>
<p>&#2; &#0;
</p>
<p>&#3;
</p>
<p>&#4;
</p>
<p>&#5;
</p>
<p>&#4;
</p>
<p>&#6;
</p>
<p>&#8;
</p>
<p>&#1;&#5;&#6;
</p>
<p>&#9;&#1;&#1; &#3; &#5;
</p>
<p>&#1;&#5;&#6;
</p>
<p>&#2; &#10;&#11; &#1;&#1; &#4; &#7;
</p>
<p>&#1;&#5;&#6;
</p>
<p>&#2;
</p>
<p>&#18;
</p>
<p>&#5;&#6;
</p>
<p>&#10;&#11;&#13;&#14;&#13; &#10;&#11; &#1;&#1; &#4; &#7;
</p>
<p>&#5;&#6;
</p>
<p>&#2;
</p>
<p>&#12; &#10;&#11;&#13;&#14;&#13;
</p>
<p>(4)
</p>
<p>o&#249; &#8;
&#1;&#5;&#6;
</p>
<p>est un c&#339;fficient de normalisation. &#18;
&#5;&#6;
</p>
<p>repr&#233;sente le premier niveau de repli, qui prend
en compte tous les mots du vocabulaire ayant une fr&#233;quence nulle. &#12; est la probabilit&#233; du mot
</p>
<p>119</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Brigitte Bigi, Kamel Sma&#239;li
</p>
<p>inconnu, et correspond au deuxi&#232;me niveau de repli. L&#8217;estimation de &#12;, &#18;
&#5;&#6;
</p>
<p>et &#8;
&#1;&#5;&#6;
</p>
<p>reste un point
important du mod&#232;le, il est donc important d&#8217;en clarifier les diff&#233;rents aspects.
</p>
<p>Le mod&#232;le doit respecter la contrainte suivante : &#2;
&#3;
</p>
<p>&#0;
</p>
<p>&#0;&#4;
</p>
<p>&#0;
</p>
<p>&#2;&#3;
</p>
<p>&#4; &#1;&#1;
</p>
<p>&#7;
</p>
<p>&#3; &#5;
</p>
<p>&#1;&#5;&#6;
</p>
<p>&#2; &#0; &#4;, o&#249; &#0;&#7; &#1;
&#5;&#6;
</p>
<p>&#0; &#7;
</p>
<p>&#5;&#6;
</p>
<p>&#3;
</p>
<p>&#19;&#20;&#21;&#2;. Ce qui implique :
&#7;
</p>
<p>&#3;
</p>
<p>&#0;
</p>
<p>&#0;&#4;
</p>
<p>&#1;&#2;&#3;
</p>
<p>&#8;
</p>
<p>&#1;&#5;&#6;
</p>
<p>&#9;&#1;&#1;
</p>
<p>&#7;
</p>
<p>&#3; &#5;
</p>
<p>&#1;&#5;&#6;
</p>
<p>&#2; &#3;
</p>
<p>&#7;
</p>
<p>&#3;
</p>
<p>&#0;
</p>
<p>&#0;&#4;
</p>
<p>&#2;&#3;
</p>
<p>&#3;&#3;
</p>
<p>&#0;
</p>
<p>&#8;&#0;&#4;
</p>
<p>&#1;&#2;&#3;
</p>
<p>&#18;
</p>
<p>&#5;&#6;
</p>
<p>&#3; &#12; &#0; &#4;
</p>
<p>Comme il n&#8217;y a pas de mots inconnus lors de la phase d&#8217;apprentissage, &#2;
&#3;
</p>
<p>&#0;
</p>
<p>&#0;&#4;
</p>
<p>&#1;&#2;&#3;
</p>
<p>&#9;&#1;&#1;
</p>
<p>&#7;
</p>
<p>&#3; &#5;
</p>
<p>&#1;&#5;&#6;
</p>
<p>&#2; &#0;
</p>
<p>&#4;, et donc, par cons&#233;quent :
</p>
<p>&#8;
</p>
<p>&#1;&#5;&#6;
</p>
<p>&#0; &#4;&#5; &#12;&#5;
</p>
<p>&#7;
</p>
<p>&#3;
</p>
<p>&#0;
</p>
<p>&#0;&#4;
</p>
<p>&#2;&#3;
</p>
<p>&#3;&#3;
</p>
<p>&#0;
</p>
<p>&#8;&#0;&#4;
</p>
<p>&#1;&#2;&#3;
</p>
<p>&#18;
</p>
<p>&#5;&#6;
</p>
<p>(5)
</p>
<p>En pratique, on attribuera &#224; &#18;
&#5;&#6;
</p>
<p>la valeur d&#8217;un ratio par rapport &#224; la plus petite des probabilit&#233;s
observ&#233;es parmi les fr&#232;res. Ceci signifie que pour chacun des mots qui n&#8217;appartiennent pas &#224;
un th&#232;me donn&#233; mais &#224; un des th&#232;mes fr&#232;res, une distribution de probabilit&#233; uniforme sur les
th&#232;mes fr&#232;res sera attribu&#233;e. La valeur &#12;, quant &#224; elle, devra prendre une probabilit&#233; tr&#232;s petite
(plus petite que le plus petit des &#18;
</p>
<p>&#5;&#6;
</p>
<p>). On peut donc r&#233;sumer la particularit&#233; de ce mod&#232;le en
&#233;non&#231;ant que la valeur &#12; est la m&#234;me pour tous les mots inconnus de l&#8217;arbre, alors que &#18;
</p>
<p>&#5;&#6;
</p>
<p>d&#233;pend du groupe de fr&#232;res.
</p>
<p>3.2.2 Attribution d&#8217;un th&#232;me au nouveau document
</p>
<p>La probabilit&#233; du th&#232;me &#5;
&#1;&#5;&#6;
</p>
<p>&#233;tant donn&#233; le document &#0;&#0;
&#0;
</p>
<p>est evalu&#233;e de la m&#234;me mani&#232;re que
dans un unigramme classique, o&#249; les th&#232;mes sont remplac&#233;s par les n&#339;uds de l&#8217;arbre :
</p>
<p>&#4; &#1;&#5;
</p>
<p>&#1;&#5;&#6;
</p>
<p>&#3; &#0;
</p>
<p>&#0;
</p>
<p>&#0;
</p>
<p>&#2; &#0;
</p>
<p>&#4; &#1;&#5;
</p>
<p>&#1;&#5;&#6;
</p>
<p>&#2;&#6;&#4; &#1;&#0;
</p>
<p>&#0;
</p>
<p>&#0;
</p>
<p>&#3; &#5;
</p>
<p>&#1;&#5;&#6;
</p>
<p>&#2;
</p>
<p>&#4; &#1;&#0;
</p>
<p>&#0;
</p>
<p>&#0;
</p>
<p>&#2;
</p>
<p>(6)
</p>
<p>o&#249; &#4; &#1;&#5;
&#1;&#5;&#6;
</p>
<p>&#2; est la probabilit&#233; a priori du th&#232;me &#5;
&#1;&#5;&#6;
</p>
<p>.
</p>
<p>De m&#234;me &#4; &#1;&#0;&#0;
&#0;
</p>
<p>&#3; &#5;
</p>
<p>&#1;&#5;&#6;
</p>
<p>&#2; &#0;
</p>
<p>&#8;
</p>
<p>&#0;
</p>
<p>&#2;&#2;&#0;
</p>
<p>&#4; &#1;&#1;
</p>
<p>&#2;
</p>
<p>&#3; &#5;
</p>
<p>&#1;&#5;&#6;
</p>
<p>&#2; est la probabilit&#233; de la s&#233;quence de mots
&#0;
</p>
<p>&#0;
</p>
<p>&#0;
</p>
<p>&#0; &#1;
</p>
<p>&#0;
</p>
<p>&#2; &#6; &#6; &#6; &#2; &#1;
</p>
<p>&#2;
</p>
<p>, &#233;valu&#233;e comme le produit des probabilit&#233;s de chaque mot dans le th&#232;me
&#5;
</p>
<p>&#1;&#5;&#6;
</p>
<p>. On obtient donc une distribution des probabilit&#233;s th&#233;matiques de l&#8217;arbre, &#233;tant donn&#233; un
message observ&#233;.
</p>
<p>3.2.3 Auto-&#233;valuation du mod&#232;le
</p>
<p>Notre objectif est de proposer le th&#232;me qui correspond au mieux au message ennonc&#233;. Un
apport suppl&#233;mentaire qui peut s&#8217;av&#233;rer un atout int&#233;ressant, est de faire en sorte que le mod&#232;le
associe un degr&#233; de fiabilit&#233; &#224; la d&#233;cision th&#233;matique qu&#8217;il prend. Nous proposons que le mod&#232;le
associe l&#8217;un des crit&#232;res suivants :
</p>
<p>1. la d&#233;cision est certaine,
</p>
<p>2. la d&#233;cision est m&#233;diane,
</p>
<p>3. la d&#233;cision est incertaine.
</p>
<p>120</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Identification th&#233;matique hi&#233;rarchique : Application aux forums de discussions
</p>
<p>Cette auto-&#233;valuation du mod&#232;le est effectu&#233;e avec le pouvoir discriminant relatif &#224; sa d&#233;cision.
Celui-ci est &#233;valu&#233; par l&#8217;&#233;cart entre la probabilit&#233; attribu&#233;e au meilleur th&#232;me et la probabilit&#233;
attribu&#233;e au deuxi&#232;me meilleur :
</p>
<p>&#6; &#0; &#4;
</p>
<p>&#5;
</p>
<p>&#1;&#5;
</p>
<p>&#1;&#5;&#6;
</p>
<p>&#3; &#0;
</p>
<p>&#0;
</p>
<p>&#0;
</p>
<p>&#2;&#5; &#4;
</p>
<p>&#1;&#5;
</p>
<p>&#1;&#5;
</p>
<p>&#1;&#5;&#6;
</p>
<p>&#3; &#0;
</p>
<p>&#0;
</p>
<p>&#0;
</p>
<p>&#2;
</p>
<p>Cet &#233;cart est ensuite compar&#233; &#224; deux seuils &#198;
&#0;
</p>
<p>et &#198;
&#1;
</p>
<p>fix&#233;s empiriquement afin de d&#233;terminer la
fiabilit&#233;, selon l&#8217;algorithme suivant :
</p>
<p>si (&#6; &gt; &#198;
&#0;
</p>
<p>)
alors
</p>
<p>d&#233;cision certaine
sinon
</p>
<p>si (&#6; &gt; &#198;
&#1;
</p>
<p>)
alors
</p>
<p>d&#233;cision m&#233;diane
sinon
</p>
<p>d&#233;cision incertaine
finsi
</p>
<p>finsi
</p>
<p>4 Exp&#233;rimentations
</p>
<p>4.1 Les donn&#233;es
</p>
<p>Nous avons choisi, pour cette &#233;tude, de nous restreindre aux groupes de langue fran&#231;aise, plac&#233;s
dans une hi&#233;rarchie dont la racine est &quot;fr.&quot;. Nos donn&#233;es couvrent une p&#233;riode de plusieurs
mois chevauchant les ann&#233;es 2000 et 2001. Ce corpus repr&#233;sente un total de 2 Go, avec plus
d&#8217;1 million d&#8217;articles. Il est compos&#233; de 365 newsgroups, dont 307 feuilles dans lesquelles on
cherchera &#224; poster l&#8217;article. Le probl&#232;me principal relatif &#224; ces donn&#233;es concerne le fait que les
articles post&#233;s dans les news sont entach&#233;s d&#8217;erreurs. Cette contrainte implique l&#8217;application
d&#8217;un ensemble de pr&#233;-traitements afin d&#8217;extraire, autant que possible, les donn&#233;es int&#233;ressantes
de chacun des articles, c&#8217;est-&#224;-dire, le corps du texte sans les &quot;bruits&quot; associ&#233;s. Parmi ces pr&#233;-
traitements, il y a notamment une phase de segmentation en mots r&#233;alis&#233;e en comparaison &#224; un
lexique, et certains mots sont regroup&#233;s dans des classes selon deux possibilit&#233;s :
</p>
<p>&#6; en r&#233;f&#233;rence &#224; un lexique sp&#233;cialis&#233;2, pour les noms personnels, les ponctuations, les
villes, les pays et les &quot;smileys&quot; ;
</p>
<p>&#6; en utilisant des informations syntaxiques dans le cas des adresses &#233;lectroniques, des
adresses internet, des heures, des prix et des nombres (obtenus &#224; partir d&#8217;un ensemble
de r&#232;gles).
</p>
<p>2Les lexiques ont &#233;t&#233; constitu&#233;s semi-manuellement &#224; partir de donn&#233;es collect&#233;es sur internet.
</p>
<p>121</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Brigitte Bigi, Kamel Sma&#239;li
</p>
<p>4.2 R&#233;sultats
</p>
<p>Etant donn&#233; un article, notre but est de proposer le (ou les) forum de discussion qui semble le
plus ad&#233;quat, parmi l&#8217;ensemble des feuilles de l&#8217;arbre. Afin d&#8217;&#233;valuer notre mod&#232;le, nous avons
compar&#233; le (ou les) n&#339;ud ainsi propos&#233; &#224; ceux dans lesquels les articles du corpus de test ont
&#233;t&#233; envoy&#233;s. Les r&#233;sultats sont donc donn&#233;s sous forme de rappel et pr&#233;cision tels que :
</p>
<p>rappel : Ratio entre le nombre de th&#232;mes d&#233;tect&#233;s correctement et le nombre de th&#232;mes &#224;
d&#233;tecter ;
</p>
<p>pr&#233;cision : Ratio entre le nombre de th&#232;mes d&#233;tect&#233;s correctement et le nombre de th&#232;mes
d&#233;tect&#233;s.
</p>
<p>4.2.1 Premi&#232;re exp&#233;rimentation
</p>
<p>Le tableau 3 pr&#233;sente les r&#233;sultats obtenus dans ce cadre d&#8217;identification th&#233;matique hi&#233;rar-
chique. Dans ce tableau, on peut observer que le mod&#232;le unigramme hi&#233;rarchique augmente
les performances de 2 % de rappel et de pr&#233;cision par rapport &#224; un unigramme classique. Ce
r&#233;sultat fait mention uniquement de la prise en compte des relations s&#233;mantiques entre fr&#232;res :
l&#8217;augmentation l&#233;g&#232;re des r&#233;sultats nous permet de v&#233;rifier positivement que nous sommes sur la
bonne voie. En particulier, on peut observer les performances obtenues par deux sous-groupes
relatifs &#224; linux (niveau &#15; &#0; &#7;), et deux autres relatifs &#224; la biologie (niveau &#15; &#0; &#8;). On ob-
serve que les groupes linux obtiennent des taux d&#8217;identification th&#233;matique tr&#232;s int&#233;ressants,
alors que les groupes de biologie sont mal identifi&#233;s. Ces diff&#233;rences importantes de perfor-
mances peuvent s&#8217;expliquer en observant la taille de leur vocabulaire &#7;
</p>
<p>&#1;&#5;&#6;
</p>
<p>mis en rapport avec
le nombre de documents du corpus d&#8217;apprentissage. Ainsi, on peut observer qu&#8217;avec des corpus
d&#8217;apprentissage de tailles proches, les deux th&#232;mes se trouvent tr&#232;s diff&#233;remment repr&#233;sen-
t&#233;s d&#8217;un point de vue de leur vocabulaire, ceci &#233;tant d&#251; &#224; leur niveau de sp&#233;cialisation. Par
cons&#233;quent, il semble &#233;vident que si linux est tr&#232;s bien reconnu, c&#8217;est parce qu&#8217;il dispose de
suffisamment de donn&#233;es pour &#234;tre statistiquement significatif, contrairement &#224; biologie.
</p>
<p>Newsgroup Rappel Pr&#233;cision Nb news Nb news &#7;
&#1;&#5;&#6;
</p>
<p>apprentissage test
Unigramme classique, sur : 0,33 0,37 +1 Million 59157 425248
- tout le corpus de test
Unigramme hi&#233;rarchique, sur :
- tout le corpus de test 0,35 0,39 +1 Million 59157 425248
- fr.comp.os.linux.annonces 0,71 0,71 124 7 7954
- fr.comp.os.linux.configuration 0,94 0,97 16111 931 4536
- fr.bio.pharmacie 0,05 0,06 684 35 10363
- fr.bio.medecine 0,34 0,46 14316 490 44970
</p>
<p>Figure 3: Performances d&#8217;identification th&#233;matique sur quelques th&#232;mes
</p>
<p>122</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Identification th&#233;matique hi&#233;rarchique : Application aux forums de discussions
</p>
<p>4.2.2 Seconde exp&#233;rimentation
</p>
<p>Dans cette seconde exp&#233;rimentation, nous introduisons de nouvelles notions relatives &#224; la valeur
de rappel :
</p>
<p>&#6; &quot;rappel exact&quot; signifie que le th&#232;me du mod&#232;le doit &#234;tre rigoureusement celui de la solu-
tion, celui-ci correspond au rappel de l&#8217;exp&#233;rience pr&#233;c&#233;dente ;
</p>
<p>&#6; &quot;rappel voisin&quot; signifie que le th&#232;me du mod&#232;le peut &#234;tre soit exact, soit le fr&#232;re, soit le
p&#232;re, soit le fils de la solution ;
</p>
<p>&#6; &quot;rappel branche&quot; signifie que le th&#232;me du mod&#232;le est dans la m&#234;me branche que le th&#232;me
solution (c-&#224;-d les niveaux &#15; &#0; &#4; &#233;gaux).
</p>
<p>Rappel Rappel Rappel Pr&#233;cision Nombre de
exact voisins branche documents test
</p>
<p>Tout le corpus 0,35 0,45 0,60 0,39 59 157 (soit 100 %)
Classe incertaine 0,17 0,27 0,44 0,19 30 970 (soit 52 %)
Classe m&#233;diane 0,52 0,62 0,75 0,57 22 328 (soit 38 %)
Classe certaine 0,71 0,79 0,87 0,77 5 859 (soit 10 %)
</p>
<p>Figure 4: Performances d&#8217;identification th&#233;matique de l&#8217;unigramme hi&#233;rarchique incluant
l&#8217;auto-&#233;valuation
</p>
<p>Les r&#233;sultats sont pr&#233;sent&#233;s &#224; la premi&#232;re ligne du tableau 4. Avec un rappel de 0,60 sur la
branche solution, on remarque que le mod&#232;le, m&#234;me s&#8217;il ne trouve pas le &quot;bon&quot; n&#339;ud, trouve
dans la majorit&#233; des cas le domaine th&#233;matique abord&#233;.
Les lignes suivantes (tableau 4) d&#233;composent ce r&#233;sultat, lorsque le mod&#232;le propose un degr&#233; de
fiabilit&#233; associ&#233; &#224; sa d&#233;cision th&#233;matique. Ces r&#233;sultats sont int&#233;ressants, car on voit bien que
lorsque le pouvoir discriminant du mod&#232;le est important, la solution propos&#233;e par le mod&#232;le est
souvent correcte avec un rappel exact &#233;gal &#224; 0,71. Avec un rappel sur la branche de 0,87, on
voit &#233;galement que dans ces cas, m&#234;me lorque l&#8217;on ne pr&#233;dit pas le th&#232;me exact, on est quand
m&#234;me capable d&#8217;apporter une solution avoisinante, ou tout au moins d&#8217;indiquer la branche &#224;
suivre avec un risque d&#8217;erreur tr&#232;s faible.
</p>
<p>5 Conclusion
</p>
<p>Dans cet article, nous avons pr&#233;sent&#233; un mod&#232;le unigramme th&#233;matique hi&#233;rarchique pour
l&#8217;identification th&#233;matique hi&#233;rarchique. Ce mod&#232;le offre des performances l&#233;g&#232;rement sup&#233;-
rieures &#224; celles obtenues avec un unigramme classique, d&#251; &#251;au fait que les relations entre fr&#232;res
sont prises en compte &#224; travers une union de leur vocabulaire, et &#224; travers l&#8217;insertion d&#8217;un facteur
de repli &#224; deux niveaux. Nous avons &#233;galement montr&#233; que m&#234;me si les performances sur le
n&#339;ud exact ne sont pas &#233;lev&#233;es, elles augmentent nettement lorque l&#8217;on se compare &#224; la branche
choisie. Concernant l&#8217;ensemble de ces r&#233;sultats, il est important de rappeler que le th&#232;me choisi
par le mod&#232;le est compar&#233; avec celui dans lequel l&#8217;article a &#233;t&#233; post&#233; par l&#8217;exp&#233;diteur. Ce crit&#232;re
</p>
<p>123</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Brigitte Bigi, Kamel Sma&#239;li
</p>
<p>n&#8217;est pas fiable, car il existe de nombreux groupes de news, et les utilisateurs n&#8217;ont pas tou-
jours connaissance de leur existence, ou de leur contenu r&#233;el. Ceci implique que la d&#233;cision
relative au groupe dans lequel son article doit &#234;tre exp&#233;di&#233; n&#8217;est pas fiable, mais ce crit&#232;re de
comparaison est le seul dont nous disposons. Nous avons &#233;galement pu observer que le pou-
voir discriminant du mod&#232;le est un crit&#232;re suffisamment pertinent pour permettre au mod&#232;le de
s&#8217;auto-&#233;valuer et ainsi, de donner non seulement le th&#232;me du nouveau document, mais aussi, d&#8217;y
associer un degr&#233; de confiance. Ainsi, dans pr&#232;s de la moiti&#233; des articles, on trouve la branche
avec un rappel de plus de 0,75, et le n&#339;ud exact avec un rappel de plus de 0,52.
</p>
<p>Ces r&#233;sultats sont encourageants. Ils laissent entrevoir, entre-autres, la possibilit&#233; de leur utilisa-
tion dans un syst&#232;me de reconnaissance automatique de la parole. Dans ce cas, l&#8217;auto-&#233;valuation
du mod&#232;le est un facteur important qui permettra de n&#8217;introduire le mod&#232;le th&#233;matique que
dans les cas o&#249; le th&#232;me est obtenu avec confiance. Diff&#233;rentes voies de recherche restent &#224;
explorer pour am&#233;liorer encore ces travaux. Notamment, ils pourraient &#234;tre mis en place sur
une arborescence cr&#233;&#233;e automatiquement. Dans ce cas, la m&#233;thode d&#8217;identification th&#233;matique
int&#232;grera certains des param&#232;tres qui ont permis de constituer l&#8217;arbre, afin que les m&#233;thodes de
classification et d&#8217;identification soient relativement homog&#232;nes.
</p>
<p>R&#233;f&#233;rences
BIGI B., DE MORI R., EL-B&#200;ZE M. &amp; SPRIET T. (2000). A fuzzy decision strategy for topic identi-
fication and dynamic selection of language models. Special Issue on Fuzzy Logic in Signal Processing,
Signal Processing Journal, 80(6), 1085&#8211;1097.
GALESCU L. &amp; ALLEN J. (2000). Hierarchical statistical language models: experiments on in-domain
adaptation. In Proceedings of the 6th International Conference on Spoken Language Processing (IC-
SLP&#8217;2000), p. 16&#8211;20, Beijing, China.
JELINEK F. (1990). Self-organized language modeling for speech recognition. Readings in Speech
Recognition, A. Waibel and K-F. Lee editors, p. 450&#8211;506.
KHUDANPUR S. P. &amp; WU J. (1999). A maximum entropy language model integrating n-gram and topic
dependencies for conversational speech recognition. In IEEE International Conference on Acoustics,
Speech and Signal Processing, volume I, p. 2192, Phoenix, Arizona.
KNESER R. &amp; PETERS J. (1997). Semantic clustering for adaptive language modeling. In IEEE Inter-
national Conference on Acoustics, Speech and Signal Processing, p. 779&#8211;783, Munich, Germany.
LI H. &amp; YAMAMISHI K. (1997). Documentation classification using a finite mixture model. In Confer-
ence of the Association for Computational Linguistics, p. 39&#8211;47, Madrid, Spain.
MAHAJAN M., BEEFERMAN D. &amp; HUANG X. D. (1999). Improved topic-dependent language model-
ing using information retrieval techniques. In IEEE International Conference on Acoustics, Speech and
Signal Processing, volume I, p. 2391, Phoenix, Arizona.
MARTIN S. C., LIERMANN J. &amp; NEY H. (1997). Adaptive topic-dependent language modeling us-
ing word-based varigrams. In Proceeding of the European Conference On Speech Communication and
Technology.
MCCALLUM A., ROSENFELD R., MITCHELL T. &amp; NG A. (1998). Improving text classification by
shrinkage in a hierarchy of classes. In International Conference on Machine Learning.
SALTON G. (1991). Developments in automatic text retrieval. Science, 253, 974&#8211;980.
SEYMORE K. &amp; ROSENFELD R. (1997). Using story topics for language model adaptation. In Proceed-
ing of the European Conference On Speech Communication and Technology.
</p>
<p>124</p>

</div></div>
</body></html>