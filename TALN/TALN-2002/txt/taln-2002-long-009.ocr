IALIV ZUUZ, Nancy, 24-2’/jum ZUUZ

Identiﬁcation thématique hiérarchique :
Application aux forums de discussions

Brigitte Bigi, Kamel Sma'1'li
LORIA - Universite Henri Poincare
Campus Scientiﬁque, BP 239, 54506 Vandoeuvre-les-Nancy
{bigi, smaili} @loria.fr

Mots-clefs — Keywords

Identiﬁcation thematique, modeles de langage, unigrammes
Topic identiﬁcation, language modeling, unigrams

Résumé - Abstract

Les modeles statistiques du langage ont pour but de donner une representation statistique de
la langue mais souffrent de nombreuses imperfections. Des travaux recents ont montre que
ces modeles peuvent étre ameliores s’ils peuvent beneﬁcier de la connaissance du theme traite,
aﬁn de s’y adapter. Le theme du document est alors obtenu par un mecanisme d’identiﬁcation
thematique, mais les themes ainsi traites sont souvent de granularite differente, c’est pourquoi
il nous semble opportun qu’ils soient organises dans une hierarchie. Cette structuration des
themes implique la Inise en place de techniques speciﬁques d’identiﬁcation thematique. Cet
article propose un modele statistique a base d’unigraInmes pour identiﬁer automatiquement le
theme d’un document parmi une arborescence predeﬁnie de themes possibles. Nous presen-
tons egalement un critere qui permet au modele de donner un degre de ﬁabilite a la decision
prise. L’ ensemble des experimentations a ete realise sur des donnees extraites du groupe ’fr’
des forums de discussion.

Statistical language modeling attempts to capture the regularities of natural language. The most
accurate natural language processing systems still suffer from several shortcomings due to the
complexity of natural language and from the weakness of the current language models. It is
commonly conjectured that they should beneﬁt from topic adaptation. The topic of the doc-
ument is then obtained by a topic identiﬁcation mechanism, but topics thus treated are often
of different granularity. This is the reason why it seems appropriate to organize them in a
hierarchy. This topic organization implies a development of speciﬁc techniques for topic iden-
tiﬁcation. This paper proposes a statistical model based on unigrams to automatically identify
the topic of a document among a tree structure of possible topics. We also present a criterion
which reﬂects the degree of reliability of the decision. Experiments were carried out on data
extracted from the French newsgroup ’fr’.

115

IJI L614/I/C IJLSL, lxll/IILCL Lllllvllrlrl/Ir

1 Introduction

La modelisation statistique du langage est une partie cruciale d’une grande variete d’ applications
relatives aux technologies du langage, telles que la reconnaissance automatique de la parole
(RAP) ou la recherche documentaire. Le but des modeles de langage est de capturer les regu-
larites du langage naturel en estimant les frequences des mots ou des expressions dans un his-
torique. Ces systemes souffrent toujours d’imperfections dues a la complexite du langage na-
turel et a la faiblesse des modeles de langage actuels. Des travaux recents montrent que les
modeles de langage doivent tirer beneﬁce de la connaissance du theme traite aﬁn de s’y adapter.
C’est pourquoi, cet article s’interesse au probleme de l’identiﬁcation thematique, aﬁn que le
theme des documents traites soit determine automatiquement.

L’identiﬁcation thematique a ainsi pour but d’assigner un label thematique a un texte parmi
un ensemble de labels possibles. Cet article presente une approche d’identiﬁcation thematique
dans laquelle on exploite les relations semantiques entre themes, par le biais d’une arborescence.
Ainsi, par exemple, il peut s’averer interessant de speciﬁer que football, escrime, natation ou
encore plongée sont des sous-themes de sport. Un grand nombre de mots sont tres probables
de fagon relativement commune a chacune de ces sous-categories (comme toumoi, rencontre,
exploit, arbitre...), ce qui permettra de favoriser les themes issus de sport, tandis que certains
mots sont speciﬁques a l’une ou l’autre des sous-categories (but, épée, maillot, tuba).

Le fait de savoir que les themes sont tous relatifs au sport permet a la fois d’eliminer un cer-
tains nombre de categories "concurrentes" pour l’identiﬁcation du theme mais surtout d’eviter
quelques ambigu'1'tes sur les homonymes (le tuba de plongee, ou le tuba instrument de musique).
Nous pensons que les relations semantiques etablies par la hierarchie peuvent devenir un atout
precieux aﬁn d’obtenir une identiﬁcation thematique plus ﬁable. Par ailleurs, lors de la phase
d’adaptation, les relations etablies entre les modeles de langages thematiques de l’arborescence
permettent de completer certains modeles trop pauvres, comme cela est fait dans (Seymore &
Rosenfeld, 1997).

Dans cet article, nous proposons l’utilisation de modeles de langage de type unigrammes,
car ces derniers ont deja montre leur potentiel a discriminer les themes dans des applications
d’identiﬁcation thematiques (Li & Yamamishi, 1997; Bigi et al., 2000). Nous proposons des
unigrammes thematiques hierarchiques qui ont pour particularite le fait que les relations entre
freres sont favorisees dans le modele, par l’attribution d’un vocabulaire commun. Par ailleurs,
ce modele utilisera son pouvoir discriminant aﬁn d’ auto-evaluer sa decision thematique, en four-
nissant un degre de ﬁabilite sur le theme qu’il choisit. Le modele a ete valide sur des donnees
des forums de discussion frangais. Dans une premiere section, cet article presente le probleme
de l’identiﬁcation thematique et la maniere dont il a ete decrit dans la litterature. Cette premiere
partie abordre egalement la notion de hierarchisation des themes. Dans la deuxieme section,
on presente le modele d’identiﬁcation thematique utilise : un modele unigramme thematique
hierarchique. Enﬁn, la demiere section donne les performances concluantes que nous avons
obtenues avec notre approche.

2 Positionnement du probléme

Les travaux recents (Kneser & Peters, 1997; Martin et al., 1997; Mahajan et al., 1999; Khu-
danpur & Wu, 1999; Bigi et al., 2000) ont montre que l’adaptation des modeles de langage au

116

III/CID!/ldblzllrl/Ll/Il« I/ILCIILIAJ/lrlil/l«C ILLCILOI bIl«lrl1l/l«C . 1-1111]!/lrbllrl/Ll/Il« Ilrl/In/l«Jl/I I/Lilla) (JD Ilrldbl/IDOL!/IEO

theme du discours permettent une amelioration signiﬁcative de la perplexitel (Jelinek, 1990).
Souvent, ce sont des techniques issues du domaine de la recherche documentaire qui sont util-
isees. Le probleme que nous soulevons dans ces travaux concerne le fait que le theme est obtenu
apres une analyse de la totalite du document, ce qui n’est evidemment pas envisageable pour
une adaptation thematique dynamique dans un systeme de RAP. Nos travaux se focalisent donc
sur des methodes statistiques qui auront pour motivation de determiner au mieux le theme d’un
document avec le minimum d’information possible. Dans ce cas precis, nos travaux precedents
nous ont perIr1is d’observer que les classiﬁeurs bayesiens de type unigrammes obtiennent les
meilleures performances d’identiﬁcation thematique, par rapport a des methodes classiques.

2.1 Représentation des themes dans une hiérarchie

Dans le cas de l’identiﬁcation thematique hierarchique, le but est d’exploiter la representation
arborescente des themes. La litterature dans le domaine de la hierarchisation des themes reste
assez pauvre. L’ article principal concernant la reconnaisance automatique de la parole est celui
de (Seymore & Rosenfeld, 1997). Dans un premier temps, ils construisent une arborescence
de clusters par un systeme simple a base de mots-cles, puis ils apprennent le modele qui corre-
spond a chaque cluster. Pour determiner le "theme" du texte, ils utilisent le classiﬁeur T FIDF
(Salton, 1991). Dans le domaine de la RAP, on trouve aussi l’article (Galescu & Allen, 2000)
o1‘1 les auteurs proposent un un "modele de langage statistique hybride hierarchique". Dans les
deux cas, la combinaison du modele classique avec le modele issu de la hierarchie ameliore
signiﬁcativement la perplexite.

On retrouve des hierarchies de themes egalement dans le domaine de la recherche documentaire
sur l’intemet, ou les documents sont souvent hierarchises (Yahoo par exemple). (McCallum
et al., 1998) prouvent que les classiﬁeurs bayesiens donnent de meilleures classiﬁcations si les
donnees sont hierarchisees en themes. Ce resultat s’appuie sur une experiementation avec des
donnees de certaines sous-categories du web et des newsgroups.

Les travaux presentes dans ce document concernent le probleme de l’identiﬁcation thematique
hierarchique, dont le but est de deterIr1iner automatiquement le theme d’un texte parIr1i un en-
semble hierarchise de themes. Ces travaux s’inscrivent dans le cadre d’une amelioration des
modeles de langage pour la RAP (ﬁgure 1). L’ objectif est de resoudre les problemes de dif-
ferences de granularite entre les themes et de proﬁter des liens semantiques entre les themes
pour l’adaptation des modeles de langage. Malheureusement, les corpus traditionnellement
utilises en RAP ne sont pas hierarchises. Cette etude portera sur le groupe "fr" de UseNet.

1En general, les modeles de langage pour la reconnaissance de la parole sont evalues en fonction de leur
impact sur la precision de la reconnaissance. Neanmoins, ils peuvent etre evalues separement si l’on considere, par
exemple, leur capacite de prediction des mots d’un texte. La mesure la plus utilisee est la perplexite’. La perplexite
d’un modele de langage derivee d’un corpus est deﬁnie comme suit :

PP = 2LP<W1"> = P(W,")-% (1)

o1‘1 LP(W1") est le logarithme de la probabilite de la sequence de n mots W1" attribuee par un modele de langage
bigramme. Pour l’eVa1uation d’un modele de langage, on estime les probabilites du modele avec un ensemble
d’apprentissage et on evalue la perplexite avec ce modele sur un corpus de texte entierement different du corpus
d’apprentissage.

117

IJI L614/I/C IJLSL, lxll/IILCL Lllllvllrlrl/Ir

  

Ensemble des documents
des oorpus u apprentissage

®

M°d§|5 as gangage géné,a| Hiérarchie de ‘ de Iangage hématiques

 

Interpolation

§ Quel est Ie theme du document?
Nowsau Document

 

Figure 1: Processus d’integration d’une hierarchie de themes pour la RAP

2.2 Application : spéciﬁcité des forums de discussion

Les "News" sont des forums de discussion federes par theme, ou, pendant une duree de temps
donnee, tous les courriers envoyes sont conserves. Chaque forum est appele en anglais news-
group, chaque article d’un newsgroup est appele une News. Les groupes sont subdivises en
sous-groupes, etc ce qui cree une arborescence dans les groupes. Dans le cadre de l’identiﬁcation
thematique hierarchique, les news constituent une plate-forme interessante. En effet, on peut
considerer que les donnees des news se rapprochent des donnees de parole selon plusieurs as-
pects. En premier lieu, on peut citer le niveau de langue, qui est souvent familier. Par ailleurs,
les messages sont souvent tres courts, comprennent de nombreuses abreviations, des fautes
d’orthographes et de grammaire, ainsi que des ﬁchiers attaches de toutes natures, autant de
difﬁcultes qui peuvent etre assimilees aux fautes que commet un systeme de RAP.

3 Modeles unigrammes pour la classiﬁcation hiérarchique

3.1 Modele unigramme thématique classique

On note W1” = {w1, U12, - - - , wN} le message dont le theme est a determiner, parmi les J
themes predeﬁnis. Le but de ce modele est de determiner P(T,~ | W1N ), c’est-a-dire la probabil-
ite a attribuer a chaque theme en fonction du message. Selon la regle de Bayes, on evalue cette
probabilite telle que :

P(Tj)-P(W1N I T3")

P(W1N)

ou  est la probabilite a priori du theme Tj, et P(W1N |  represente la probabilite de la
sequence de mots W1 , etant donne un theme Tj. Celle-ci s’estime comme suit :

P(T.~ 1 W1”) = (2)

PM I 22> = fl Pm 1 T.)

71.21

118

III/CID!/ldblzllrl/Ll/Il« I/ILCIILIAJ/lrlil/l«C ILLCILOI bIl«lrl1l/l«C . 1-1111]!/lrbllrl/Ll/Il« Ilrl/In/l«Jl/I I/Lilla) (JD Ilrldbl/IDOL!/IEO

Le probleme est donc d’obtenir P(w | Tj), la probabilité d’un mot w dans un theme Tj, pour
chaque mot w du vocabulaire Vj de Tj :

 3i(wEVJ') (3)

sinon

o1‘1 Zwiev}. 73' f (w |  +5 = 1, f (w |  est la fréquence du mot w dans Tj, apprise sur un cor-
pus dédié au theme Tj, et, 73' est un coefﬁcient de normalisation qui assure que la distribution des
probabilités P(w |  some a 1. Come il n’y a pas de mot inconnu lors de l’apprentissage,
la valeur de 5 sera donc attribuée a tous les mots n’appartenant pas au vocabulaire de Tj lors de
la phase de test. Elle représente la probabilité du mot inconnu, noté UNK.

3.2 Modéles unigrammes thématiques hiérarchiques

La hiérarchie des themes est présentée sous la forme d’un arbre (ﬁgure 2). A chaque niveau
Is, on déﬁnit des groupes de noeuds freres, notés b (les freres, étant, par deﬁnition, des noeuds
de meme pere). Notons également Tjbk, le j -eme theme, du b-eme groupe de freres, du k-eme
niveau, ainsi que Tbk un groupe de freres. On notera également I/‘W, le vocabulaire du theme
Tjbk, déduit du corpus d’apprentissage de ce theme.

Ir
0
O  .
annonce blenvenue -—-- SCI -—-—- test

 %\ ﬂ

0 0 0 ' ' 0
divers ,, ,  ' ' oo ni ---—

k=0

k=2

3 n c
kg discussion inoogniio inio ouiils publications

Figure 2: Exemple d’une hiérarchie de themes, groupe ’fr’ de UseNet

3.2.1 Distribution de probabilités des nteuds

Pour rendre compte des relations sémantiques fraternelles, nous proposons que chaque theme
d’un groupe de freres donné soit représente par le meme vocabulaire, issu de l’union des vo-
cabulaires de chacun des themes freres. Ce vocabulaire est noté Vbk. Ceci implique que certains
mots n’auront pas été observés durant l’apprentissage et par conséquent f (w | Tj) = 0. Le
modele classique des unigrammes de l’équation (3) est donc modiﬁé en introduisant un deux-
ieme niveau de repli (back-off), tel que :

’Yjwcf(’w | Tjbk) 8i (U1 E Vjbk)
P(w | T]-bk) = wbk sinon 32' (w E Vbk) (4)
5 sinon

o1‘1 *y,~;,;, est un coefﬁcient de normalisation. wbk représente le premier niveau de repli, qui prend
en compte tous les mots du vocabulaire ayant une fréquence nulle. 5 est la probabilité du mot

119

IJI L614/I/C IJLSL, lxll/IILCL Lllllvllrlrl/Ir

inconnu, et correspond au deuxieme niveau de repli. L’estimation de 5, wbk et 7/jbk reste un point
important du modele, il est donc important d’en clariﬁer les différents aspects.

Le modele doit respecter la contrainte suivante : Zwievbrk P(w,~ | Tjbk) = 1, ou {V,,’k = Vbk +
UNK Ce qui implique :

Z 7jbkf('Wz' l  + Z wbk + 5 = 1

w2'EV}'bk wz'€V'bk;wz'¢V}'bk

Comme il n’y a pas de mots inconnus lors de la phase d’apprentissage, Zwiewbk f (w, | Tjbk) =
1, et donc, par conséquent :

’Yjbk = 1 — 5 — Z wbk (5)

W2’ El/izk ;wi¢V}'bk

En pratique, on attribuera a wbk la valeur d’un ratio par rapport a la plus petite des probabilités
observées parmi les freres. Ceci signiﬁe que pour chacun des mots qui n’appartiennent pas a
un theme donné mais a un des themes freres, une distribution de probabilité uniforme sur les
themes freres sera attribuée. La valeur 5, quanta elle, devra prendre une probabilité tres petite
(plus petite que le plus petit des wbk). On peut donc résumer la particularité de ce modele en
énoncant que la valeur 5 est la meme pour tous les mots inconnus de l’arbre, alors que wbk
dépend du groupe de freres.

3.2.2 Attribution d’un théme au nouveau document

La probabilité du theme Tm, étant donné le document WIN est evaluée de la méme maniere que
dans un unigramme classique, o1‘1 les themes sont remplacés par les noeuds de l’arbre :

P(Tjbk)-P(W1N | Table)

(6)

ou P(Tj,,k) est la probabilité a priori du theme Tjbk.

De meme P(W1N | Tjbk) = N P(w,, | T]-bk) est la probabilité de la séquence de mots

n=1
W1N = wl, . . . ,w,,, évaluée comme le produit des probabilités de chaque mot dans le theme
Tjbk. On obtient donc une distribution des probabilités thématiques de l’arbre, étant donné un
message observé.

3.2.3 Auto-évaluation du modéle

Notre objectif est de proposer le theme qui correspond au mieux au message ennoncé. Un
apport supplémentaire qui peut s’avérer un atout intéressant, est de faire en sorte que le modele
associe un degré de ﬁabilité a la décision thématique qu’il prend. Nous proposons que le modele
associe l’un des criteres suivants :

1. la décision est certaine,
2. la décision est médiane,

3. la décision est incertaine.

120

III/CID!/ldblzllrl/Ll/Il« I/ILCIILIAJ/lrlil/l«C ILLCILOI bIl«lrl1l/l«C . 1-1111]!/lrbllrl/Ll/Il« Ilrl/In/l«Jl/I I/Lilla) (JD Ilrldbl/IDOL!/IEO

Cette auto-evaluation du modele est effectuee avec le pouvoir discriminant relatif a sa decision.
Celui-ci est evalue par l’ecart entre la probabilite attribuee au meilleur theme et la probabilite
attribuee au deuxieme meilleur :

A = Pb(TjbIc | W1N) — P2b(TjbIc | W1N)

Cet ecart est ensuite compare a deux seuils 61 et 62 ﬁxes empiriquement aﬁn de determiner la
ﬁabilite, selon l’algorithme suivant :

Si  > 61)
alors
decision certaine
sinon
Si  > (32)
alors
decision mediane
sinon
decision incertaine
ﬁnsi
ﬁnsi

4 Expérimentations

4.1 Les données

Nous avons choisi, pour cette etude, de nous restreindre aux groupes de langue francaise, places
dans une hierarchie dont la racine est "fr.". Nos donnees couvrent une periode de plusieurs
mois chevauchant les annees 2000 et 2001. Ce corpus represente un total de 2 Go, avec plus
d’1 million d’articles. Il est compose de 365 newsgroups, dont 307 feuilles dans lesquelles on
cherchera a poster l’article. Le probleme principal relatif a ces donnees conceme le fait que les
articles postes dans les news sont entaches d’erreurs. Cette contrainte implique l’application
d’un ensemble de pre-traitements aﬁn d’extraire, autant que possible, les donnees interessantes
de chacun des articles, c’est-a-dire, le corps du texte sans les "bruits" associes. Parmi ces pre-
traitements, il y a notamment une phase de segmentation en mots realisee en comparaison a un
lexique, et certains mots sont regroupes dans des classes selon deux possibilites :

0 en reference a un lexique specialisez, pour les noms personnels, les ponctuations, les
villes, les pays et les "smileys" ;

0 en utilisant des informations syntaxiques dans le cas des adresses electroniques, des
adresses internet, des heures, des prix et des nombres (obtenus a partir d’un ensemble
de regles).

2Les lexiques ont ete constitues semi—manuellement a partir de données collectees sur internet.

121

IJI L614/I/C IJLSL, lxll/IILCL Lllllvllrlrl/Ir

4.2 Résultats

Etant donné un article, notre but est de proposer le (ou les) forum de discussion qui semble le
plus adéquat, parmi l’ensemble des feuilles de l’arbre. Aﬁn d’évaluer notre modele, nous avons
comparé le (ou les) noeud ainsi propose a ceux dans lesquels les articles du corpus de test ont
été envoyés. Les résultats sont donc donnés sous forme de rappel et précision tels que :

rappel : Ratio entre le nombre de themes détectés correctement et le nombre de themes a
détecter ;

précision : Ratio entre le nombre de themes détectés correctement et le nombre de themes
détectés.

4.2.1 Premiére expérimentation

Le tableau 3 présente les résultats obtenus dans ce cadre d’identiﬁcation thématique hiérar-
chique. Dans ce tableau, on peut observer que le modele unigraInme hiérarchique augmente
les performances de 2 % de rappel et de précision par rapport a un unigramme classique. Ce
résultat fait mention uniquement de la prise en compte des relations sémantiques entre freres :
l’ augmentation légere des résultats nous permet de vériﬁer positivement que nous sommes sur la
bonne voie. En particulier, on peut observer les performances obtenues par deux sous-groupes
relatifs a linux (niveau is = 4), et deux autres relatifs a la biologie (niveau k = 2). On ob-
serve que les groupes linux obtiennent des taux d’identiﬁcation thématique tres intéressants,
alors que les groupes de biologie sont mal identiﬁés. Ces différences importantes de perfor-
mances peuvent s’eXpliquer en observant la taille de leur vocabulaire 1/jbk mis en rapport avec
le nombre de documents du corpus d’apprentissage. Ainsi, on peut observer qu’avec des corpus
d’apprentissage de tailles proches, les deux themes se trouvent tres différemment represen-
tés d’un point de vue de leur vocabulaire, ceci étant dﬁ a leur niveau de spécialisation. Par
conséquent, il semble évident que si linux est tres bien reconnu, c’est parce qu’il dispose de
sufﬁsamment de données pour étre statistiquement signiﬁcatif, contrairement a biologie.

Newsgroup Rappel Précision Nb news Nb news 1/jbk
apprentissage test
UnigraInme classique, sur : 0,33 0,37 +1 Million 59157 425248

- tout le corpus detest

Unigramme hiérarchique, sur :

- tout le corpus detest 0,35 0,39 +1 Million 59157 425248
- fr.comp.os.1inux.annonces 0,71 0,71 124 7 7954
- fr.comp.os.1inux.conﬁguration 0,94 0,97 16111 931 4536
- fr.bio.pharmacie 0,05 0,06 684 35 10363
- fr.bio.medecine 0,34 0,46 14316 490 44970

Figure 3: Performances d’identiﬁcation thématique sur quelques themes

122

III/CID!/ldblzllrl/Ll/Il« I/ILCIILIAJ/lrlil/l«C ILLCILOI bIl«lrl1l/l«C . 1-1111]!/lrbllrl/Ll/Il« Ilrl/In/l«Jl/I I/Lilla) (JD Ilrldbl/IDOL!/IEO

4.2.2 Seconde expérimentation

Dans cette seconde experimentation, nous introduisons de nouvelles notions relatives a la valeur
de rappel :

o "rappel exact" signiﬁe que le theme du modele doit etre rigoureusement celui de la solu-
tion, celui-ci correspond au rappel de l’expérience précédente ;

o "rappel voisin" signiﬁe que le theme du modele peut etre soit exact, soit le frere, soit le
pere, soit le ﬁls de la solution ;

o "rappel branche" signiﬁe que le theme du modele est dans la meme branche que le theme
solution (c-a-d les niveaux is = 1 égaux).

Rappel Rappel Rappel Precision Nombre de
exact voisins branche documents test
| Tout le corpus | 0,35 | 0,45 | 0,60  0,39  59 157 (soit 100 %) |
Classe incertaine 0,17 0,27 0,44 0,19 30 970 (soit 52 %)
Classe médiane 0,52 0,62 0,75 0,57 22 328 (soit 38 %)
Classe certaine 0,71 0,79 0,87 0,77 5 859 (soit 10 %)

Figure 4: Performances d’identiﬁcation thématique de l’unigramme hiérarchique incluant
l’ auto-évaluation

Les résultats sont présentés a la premiere ligne du tableau 4. Avec un rappel de 0,60 sur la
branche solution, on remarque que le modele, meme s’il ne trouve pas le "bon" noeud, trouve
dans la maj orité des cas le domaine thématique abordé.

Les lignes suivantes (tableau 4) décomposent ce résultat, lorsque le modele propose un degré de
ﬁabilité associé a sa decision thématique. Ces résultats sont intéressants, car on voit bien que
lorsque le pouvoir discriminant du modele est important, la solution proposée par le modele est
souvent correcte avec un rappel exact égal a 0,71. Avec un rappel sur la branche de 0,87, on
voit également que dans ces cas, meme lorque l’on ne prédit pas le theme exact, on est quand
meme capable d’apporter une solution avoisinante, ou tout au moins d’indiquer la branche a
suivre avec un risque d’erreur tres faible.

5 Conclusion

Dans cet article, nous avons présenté un modele unigramme thématique hiérarchique pour
l’identiﬁcation thématique hiérarchique. Ce modele offre des performances légerement supe-
rieures a celles obtenues avec un unigramme classique, dﬁ ﬁau fait que les relations entre freres
sont prises en compte a travers une union de leur vocabulaire, et a travers l’insertion d’un facteur
de repli a deux niveaux. Nous avons également montré que meme si les performances sur le
noeud exact ne sont pas élevées, elles augmentent nettement lorque l’on se compare a la branche
choisie. Concemant l’ensemble de ces résultats, il est important de rappeler que le theme choisi
par le modele est compare avec celui dans lequel l’article a été posté par l’expéditeur. Ce critere

123

IJI L614/I/C IJLSL, lxll/IILCL Lllllvllrlrl/Ir

n’est pas ﬁable, car il existe de nombreux groupes de news, et les utilisateurs n’ont pas tou-
jours connaissance de leur existence, ou de leur contenu réel. Ceci implique que la décision
relative au groupe dans lequel son article doit étre expédié n’est pas ﬁable, mais ce critere de
comparaison est le seul dont nous disposons. Nous avons également pu observer que le pou-
voir discriminant du modele est un critere sufﬁsamment pertinent pour permettre au modele de
s’auto-évaluer et ainsi, de donner non seulement le theme du nouveau document, mais aussi, d’y
associer un degré de conﬁance. Ainsi, dans pres de la moitié des articles, on trouve la branche
avec un rappel de plus de 0,75, et le noeud exact avec un rappel de plus de 0,52.

Ces résultats sont encourageants. Ils laissent entrevoir, entre-autres, la possibilité de leur utilisa-
tion dans un systeme de reconnaissance automatique de la parole. Dans ce cas, l’auto-évaluation
du modele est un facteur important qui permettra de n’introduire le modele thématique que
dans les cas ou le theme est obtenu avec conﬁance. Différentes voies de recherche restent a
explorer pour améliorer encore ces travaux. Notamment, ils pourraient étre mis en place sur
une arborescence créée automatiquement. Dans ce cas, la méthode d’identiﬁcation thématique
integrera certains des parametres qui ont permis de constituer l’arbre, aﬁn que les méthodes de
classiﬁcation et d’identiﬁcation soient relativement homogenes.

Références

BIGI B., DE MORI R., EL-BEZE M. & SPRIET T. (2000). A fuzzy decision strategy for topic identi-
ﬁcation and dynamic selection of language models. Special Issue on Fuzzy Logic in Signal Processing,
Signal Processing Journal, 80(6), 1085-1097.

GALESCU L. & ALLEN J. (2000). Hierarchical statistical language models: experiments on in-domain
adaptation. In Proceedings of the 6th International Conference on Spoken Language Processing (IC-
SLP’2000), p. 16-20, Beijing, China.

JELINEK F. (1990). Self-organized language modeling for speech recognition. Readings in Speech
Recognition, A. Waibel and K-F Lee editors, p. 450-506.

KHUDANPUR S. P. & WU J . (1999). A maximum entropy language model integrating n-gram and topic
dependencies for conversational speech recognition. In IEEE International Conference on Acoustics,
Speech and Signal Processing, volume I, p. 2192, Phoenix, Arizona.

KNESER R. & PETERS J . (1997). Semantic clustering for adaptive language modeling. In IEEE Inter-
national Conference on Acoustics, Speech and Signal Processing, p. 779-783, Munich, Germany.

LI H. & YAMAMISHI K. (1997). Documentation classiﬁcation using a ﬁnite mixture model. In Confer-
ence of the Association for Computational Linguistics, p. 39-47, Madrid, Spain.

MAHAJAN M., BEEFERMAN D. & HUANG X. D. (1999). Improved topic-dependent language model-
ing using information retrieval techniques. In IEEE International Conference on Acoustics, Speech and
Signal Processing, volume I, p. 2391, Phoenix, Arizona.

MARTIN S. C., LIERMANN J . & NEY H. (1997). Adaptive topic-dependent language modeling us-
ing word-based varigrams. In Proceeding of the European Conference On Speech Communication and
Technology.

MCCALLUM A., ROSENFELD R., MITCHELL T. & NG A. (1998). Improving text classiﬁcation by
shrinkage in a hierarchy of classes. In International Conference on Machine Learning.

SALTON G. (1991). Developments in automatic text retrieval. Science, 253, 974-980.

SEYMORE K. & ROSENFELD R. (1997). Using story topics for language model adaptation. In Proceed-
ing of the European Conference On Speech Communication and Technology.

124

