TALN 2002, Nancy, 24‚Äì27 juin 2002
Identification th√©matique hi√©rarchique :
Application aux forums de discussions
Brigitte Bigi, Kamel Sma√Øli
LORIA - Universit√© Henri Poincar√©
Campus Scientifique, BP 239, 54506 Vand≈ìuvre-l√®s-Nancy
{bigi, smaili}@loria.fr
Mots-clefs ‚Äì Keywords
Identification th√©matique, mod√®les de langage, unigrammes
Topic identification, language modeling, unigrams
R√©sum√© - Abstract
Les mod√®les statistiques du langage ont pour but de donner une repr√©sentation statistique de
la langue mais souffrent de nombreuses imperfections. Des travaux r√©cents ont montr√© que
ces mod√®les peuvent √™tre am√©lior√©s s‚Äôils peuvent b√©n√©ficier de la connaissance du th√®me trait√©,
afin de s‚Äôy adapter. Le th√®me du document est alors obtenu par un m√©canisme d‚Äôidentification
th√©matique, mais les th√®mes ainsi trait√©s sont souvent de granularit√© diff√©rente, c‚Äôest pourquoi
il nous semble opportun qu‚Äôils soient organis√©s dans une hi√©rarchie. Cette structuration des
th√®mes implique la mise en place de techniques sp√©cifiques d‚Äôidentification th√©matique. Cet
article propose un mod√®le statistique √† base d‚Äôunigrammes pour identifier automatiquement le
th√®me d‚Äôun document parmi une arborescence pr√©d√©finie de th√®mes possibles. Nous pr√©sen-
tons √©galement un crit√®re qui permet au mod√®le de donner un degr√© de fiabilit√© √† la d√©cision
prise. L‚Äôensemble des exp√©rimentations a √©t√© r√©alis√© sur des donn√©es extraites du groupe ‚Äôfr‚Äô
des forums de discussion.
Statistical language modeling attempts to capture the regularities of natural language. The most
accurate natural language processing systems still suffer from several shortcomings due to the
complexity of natural language and from the weakness of the current language models. It is
commonly conjectured that they should benefit from topic adaptation. The topic of the doc-
ument is then obtained by a topic identification mechanism, but topics thus treated are often
of different granularity. This is the reason why it seems appropriate to organize them in a
hierarchy. This topic organization implies a development of specific techniques for topic iden-
tification. This paper proposes a statistical model based on unigrams to automatically identify
the topic of a document among a tree structure of possible topics. We also present a criterion
which reflects the degree of reliability of the decision. Experiments were carried out on data
extracted from the French newsgroup ‚Äôfr‚Äô.
115
Brigitte Bigi, Kamel Sma√Øli
1 Introduction
La mod√©lisation statistique du langage est une partie cruciale d‚Äôune grande vari√©t√© d‚Äôapplications
relatives aux technologies du langage, telles que la reconnaissance automatique de la parole
(RAP) ou la recherche documentaire. Le but des mod√®les de langage est de capturer les r√©gu-
larit√©s du langage naturel en estimant les fr√©quences des mots ou des expressions dans un his-
torique. Ces syst√®mes souffrent toujours d‚Äôimperfections dues √† la complexit√© du langage na-
turel et √† la faiblesse des mod√®les de langage actuels. Des travaux r√©cents montrent que les
mod√®les de langage doivent tirer b√©n√©fice de la connaissance du th√®me trait√© afin de s‚Äôy adapter.
C‚Äôest pourquoi, cet article s‚Äôint√©resse au probl√®me de l‚Äôidentification th√©matique, afin que le
th√®me des documents trait√©s soit d√©termin√© automatiquement.
L‚Äôidentification th√©matique a ainsi pour but d‚Äôassigner un label th√©matique √† un texte parmi
un ensemble de labels possibles. Cet article pr√©sente une approche d‚Äôidentification th√©matique
dans laquelle on exploite les relations s√©mantiques entre th√®mes, par le biais d‚Äôune arborescence.
Ainsi, par exemple, il peut s‚Äôav√©rer int√©ressant de sp√©cifier que football, escrime, natation ou
encore plong√©e sont des sous-th√®mes de sport. Un grand nombre de mots sont tr√®s probables
de fa√ßon relativement commune √† chacune de ces sous-cat√©gories (comme tournoi, rencontre,
exploit, arbitre...), ce qui permettra de favoriser les th√®mes issus de sport, tandis que certains
mots sont sp√©cifiques √† l‚Äôune ou l‚Äôautre des sous-cat√©gories (but, √©p√©e, maillot, tuba).
Le fait de savoir que les th√®mes sont tous relatifs au sport permet √† la fois d‚Äô√©liminer un cer-
tains nombre de cat√©gories "concurrentes" pour l‚Äôidentification du th√®me mais surtout d‚Äô√©viter
quelques ambigu√Øt√©s sur les homonymes (le tuba de plong√©e, ou le tuba instrument de musique).
Nous pensons que les relations s√©mantiques √©tablies par la hi√©rarchie peuvent devenir un atout
pr√©cieux afin d‚Äôobtenir une identification th√©matique plus fiable. Par ailleurs, lors de la phase
d‚Äôadaptation, les relations √©tablies entre les mod√®les de langages th√©matiques de l‚Äôarborescence
permettent de compl√©ter certains mod√®les trop pauvres, comme cela est fait dans (Seymore &
Rosenfeld, 1997).
Dans cet article, nous proposons l‚Äôutilisation de mod√®les de langage de type unigrammes,
car ces derniers ont d√©j√† montr√© leur potentiel √† discriminer les th√®mes dans des applications
d‚Äôidentification th√©matiques (Li & Yamamishi, 1997; Bigi et al., 2000). Nous proposons des
unigrammes th√©matiques hi√©rarchiques qui ont pour particularit√© le fait que les relations entre
fr√®res sont favoris√©es dans le mod√®le, par l‚Äôattribution d‚Äôun vocabulaire commun. Par ailleurs,
ce mod√®le utilisera son pouvoir discriminant afin d‚Äôauto-√©valuer sa d√©cision th√©matique, en four-
nissant un degr√© de fiabilit√© sur le th√®me qu‚Äôil choisit. Le mod√®le a √©t√© valid√© sur des donn√©es
des forums de discussion fran√ßais. Dans une premi√®re section, cet article pr√©sente le probl√®me
de l‚Äôidentification th√©matique et la mani√®re dont il a √©t√© d√©crit dans la litt√©rature. Cette premi√®re
partie abordre √©galement la notion de hi√©rarchisation des th√®mes. Dans la deuxi√®me section,
on pr√©sente le mod√®le d‚Äôidentification th√©matique utilis√© : un mod√®le unigramme th√©matique
hi√©rarchique. Enfin, la derni√®re section donne les performances concluantes que nous avons
obtenues avec notre approche.
2 Positionnement du probl√®me
Les travaux r√©cents (Kneser & Peters, 1997; Martin et al., 1997; Mahajan et al., 1999; Khu-
danpur & Wu, 1999; Bigi et al., 2000) ont montr√© que l‚Äôadaptation des mod√®les de langage au
116
Identification th√©matique hi√©rarchique : Application aux forums de discussions
th√®me du discours permettent une am√©lioration significative de la perplexit√©1 (Jelinek, 1990).
Souvent, ce sont des techniques issues du domaine de la recherche documentaire qui sont util-
is√©es. Le probl√®me que nous soulevons dans ces travaux concerne le fait que le th√®me est obtenu
apr√®s une analyse de la totalit√© du document, ce qui n‚Äôest √©videmment pas envisageable pour
une adaptation th√©matique dynamique dans un syst√®me de RAP. Nos travaux se focalisent donc
sur des m√©thodes statistiques qui auront pour motivation de d√©terminer au mieux le th√®me d‚Äôun
document avec le minimum d‚Äôinformation possible. Dans ce cas pr√©cis, nos travaux pr√©c√©dents
nous ont permis d‚Äôobserver que les classifieurs bay√©siens de type unigrammes obtiennent les
meilleures performances d‚Äôidentification th√©matique, par rapport √† des m√©thodes classiques.
2.1 Repr√©sentation des th√®mes dans une hi√©rarchie
Dans le cas de l‚Äôidentification th√©matique hi√©rarchique, le but est d‚Äôexploiter la repr√©sentation
arborescente des th√®mes. La litt√©rature dans le domaine de la hi√©rarchisation des th√®mes reste
assez pauvre. L‚Äôarticle principal concernant la reconnaisance automatique de la parole est celui
de (Seymore & Rosenfeld, 1997). Dans un premier temps, ils construisent une arborescence
de clusters par un syst√®me simple √† base de mots-cl√©s, puis ils apprennent le mod√®le qui corre-
spond √† chaque cluster. Pour d√©terminer le "th√®me" du texte, ils utilisent le classifieur TFIDF
(Salton, 1991). Dans le domaine de la RAP, on trouve aussi l‚Äôarticle (Galescu & Allen, 2000)
o√π les auteurs proposent un un "mod√®le de langage statistique hybride hi√©rarchique". Dans les
deux cas, la combinaison du mod√®le classique avec le mod√®le issu de la hi√©rarchie am√©liore
significativement la perplexit√©.
On retrouve des hi√©rarchies de th√®mes √©galement dans le domaine de la recherche documentaire
sur l‚Äôinternet, o√π les documents sont souvent hi√©rarchis√©s (Yahoo par exemple). (McCallum
et al., 1998) prouvent que les classifieurs bay√©siens donnent de meilleures classifications si les
donn√©es sont hi√©rarchis√©es en th√®mes. Ce r√©sultat s‚Äôappuie sur une exp√©riementation avec des
donn√©es de certaines sous-cat√©gories du web et des newsgroups.
Les travaux pr√©sent√©s dans ce document concernent le probl√®me de l‚Äôidentification th√©matique
hi√©rarchique, dont le but est de d√©terminer automatiquement le th√®me d‚Äôun texte parmi un en-
semble hi√©rarchis√© de th√®mes. Ces travaux s‚Äôinscrivent dans le cadre d‚Äôune am√©lioration des
mod√®les de langage pour la RAP (figure 1). L‚Äôobjectif est de r√©soudre les probl√®mes de dif-
f√©rences de granularit√© entre les th√®mes et de profiter des liens s√©mantiques entre les th√®mes
pour l‚Äôadaptation des mod√®les de langage. Malheureusement, les corpus traditionnellement
utilis√©s en RAP ne sont pas hi√©rarchis√©s. Cette √©tude portera sur le groupe "fr" de UseNet.
1En g√©n√©ral, les mod√®les de langage pour la reconnaissance de la parole sont √©valu√©s en fonction de leur
impact sur la pr√©cision de la reconnaissance. N√©anmoins, ils peuvent √™tre √©valu√©s s√©par√©ment si l‚Äôon consid√®re, par
exemple, leur capacit√© de pr√©diction des mots d‚Äôun texte. La mesure la plus utilis√©e est la perplexit√©. La perplexit√©
d‚Äôun mod√®le de langage d√©riv√©e d‚Äôun corpus est d√©finie comme suit :
 
 
       
 
 
            (1)

o√π     est le logarithme de la probabilit√© de la s√©quence de  mots  attribu√©e par un mod√®le de langage
 
bigramme. Pour l‚Äô√©valuation d‚Äôun mod√®le de langage, on estime les probabilit√©s du mod√®le avec un ensemble
d‚Äôapprentissage et on √©value la perplexit√© avec ce mod√®le sur un corpus de texte enti√®rement diff√©rent du corpus
d‚Äôapprentissage.
117
Brigitte Bigi, Kamel Sma√Øli
root
d2
d1 d8 clustering c1 c2 ck
dn
d5
di
Ensemble des documents Hi√©rarchie de clusters de documents
des corpus d‚Äôapprentissage
root
ml1 ml2 mlk
ML
Mod√®le de langage g√©n√©ral Hi√©rarchie de mod√®les de langage th√©matiques
Interpolation
Quel est le th√®me du document ?
Nouveau Document
(File Transfer Protocol)
Pprotocole FTP. Protocole d‚Äôapplication
faisant partie de la pile de protocole TCP/IP ;
il est utilis√© pour transf√©rer des fichiers.
Le protocole FTP est d√©fini dans la RFC 959.
Deux modes de transferts sont possibles :
‚àí binaire : transfert de tous les bits du
fichier sans aucune modification, 
‚àí caract√®re : transformation du codage des
caract√®res si besoin (ASCII ou EBCDIC) 
Figure 1: Processus d‚Äôint√©gration d‚Äôune hi√©rarchie de th√®mes pour la RAP
2.2 Application : sp√©cificit√© des forums de discussion
Les "News" sont des forums de discussion f√©d√©r√©s par th√®me, o√π, pendant une dur√©e de temps
donn√©e, tous les courriers envoy√©s sont conserv√©s. Chaque forum est appel√© en anglais news-
group, chaque article d‚Äôun newsgroup est appel√© une News. Les groupes sont subdivis√©s en
sous-groupes, etc ce qui cr√©e une arborescence dans les groupes. Dans le cadre de l‚Äôidentification
th√©matique hi√©rarchique, les news constituent une plate-forme int√©ressante. En effet, on peut
consid√©rer que les donn√©es des news se rapprochent des donn√©es de parole selon plusieurs as-
pects. En premier lieu, on peut citer le niveau de langue, qui est souvent familier. Par ailleurs,
les messages sont souvent tr√®s courts, comprennent de nombreuses abr√©viations, des fautes
d‚Äôorthographes et de grammaire, ainsi que des fichiers attach√©s de toutes natures, autant de
difficult√©s qui peuvent √™tre assimil√©es aux fautes que commet un syst√®me de RAP.
3 Mod√®les unigrammes pour la classification hi√©rarchique
3.1 Mod√®le unigramme th√©matique classique
On note                le message dont le th√®me est √† d√©terminer, parmi les 
    
 
th√®mes pr√©d√©finis. Le but de ce mod√®le est de d√©terminer       , c‚Äôest-√†-dire la probabil-

 
it√© √† attribuer √† chaque th√®me en fonction du message. Selon la r√®gle de Bayes, on √©value cette
probabilit√© telle que :
 
       
 
 
 
        (2)

 
 
   
 
o√π    est la probabilit√© a priori du th√®me   , et       repr√©sente la probabilit√© de la
  
 
s√©quence de mots    , √©tant donn√© un th√®me  . Celle-ci s‚Äôestime comme suit :

 
 
 
            
  
 
 
118
Identification th√©matique hi√©rarchique : Application aux forums de discussions
Le probl√®me est donc d‚Äôobtenir     , la probabilit√© d‚Äôun mot  dans un th√®me  , pour
 
chaque mot  du vocabulaire  de  :
 

 	    
    
  
       (3)

 

o√π  	      , 	    est la fr√©quence du mot  dans  , apprise sur un cor-
   
  
  
pus d√©di√© au th√®me  , et,  est un c≈ìfficient de normalisation qui assure que la distribution des
 
probabilit√©s      somme √† 1. Comme il n‚Äôy a pas de mot inconnu lors de l‚Äôapprentissage,

la valeur de  sera donc attribu√©e √† tous les mots n‚Äôappartenant pas au vocabulaire de  lors de

la phase de test. Elle repr√©sente la probabilit√© du mot inconnu, not√© UNK.
3.2 Mod√®les unigrammes th√©matiques hi√©rarchiques
La hi√©rarchie des th√®mes est pr√©sent√©e sous la forme d‚Äôun arbre (figure 2). A chaque niveau
, on d√©finit des groupes de n≈ìuds fr√®res, not√©s  (les fr√®res, √©tant, par d√©finition, des n≈ìuds
de m√™me p√®re). Notons √©galement  , le -√®me th√®me, du -√®me groupe de fr√®res, du -√®me

niveau, ainsi que  un groupe de fr√®res. On notera √©galement  le vocabulaire du th√®me
 
 , d√©duit du corpus d‚Äôapprentissage de ce th√®me.

k=0 fr
                          
                          
                          
                          
                          
                          
                          
                          
     
annonce bie n venue         sci       test
k                     =1                      
   
       
    
         
   
       
    
         
   
       
    
         
   
       
    
         
   
       
    
         
   
       
    
         
   
       
    
         
   
       
    
         
   
       
    
         
   
       
    
         
   
       
    
         
   
  
      
   
      
  
    
      
    
divers      newgroups newusers ue cogni     .... questions automatiq
k=2                  
                 
                 
                 
                 
                 
                 
                 
                 
                 
                 
                 
               
            
            
            
k=3 discussion  incognito  info   outils      publications
Figure 2: Exemple d‚Äôune hi√©rarchie de th√®mes, groupe ‚Äôfr‚Äô de UseNet
3.2.1 Distribution de probabilit√©s des n≈ìuds
Pour rendre compte des relations s√©mantiques fraternelles, nous proposons que chaque th√®me
d‚Äôun groupe de fr√®res donn√© soit repr√©sent√© par le m√™me vocabulaire, issu de l‚Äôunion des vo-
cabulaires de chacun des th√®mes fr√®res. Ce vocabulaire est not√©  . Ceci implique que certains

mots n‚Äôauront pas √©t√© observ√©s durant l‚Äôapprentissage et par cons√©quent 	      . Le

mod√®le classique des unigrammes de l‚Äô√©quation (3) est donc modifi√© en introduisant un deux-
i√®me niveau de repli (back-off), tel que :


 	    
    
  

 
 
    
       (4)
 



 

o√π  est un c≈ìfficient de normalisation.  repr√©sente le premier niveau de repli, qui prend
 
en compte tous les mots du vocabulaire ayant une fr√©quence nulle.  est la probabilit√© du mot
119
Brigitte Bigi, Kamel Sma√Øli
inconnu, et correspond au deuxi√®me niveau de repli. L‚Äôestimation de ,  et  reste un point
 
important du mod√®le, il est donc important d‚Äôen clarifier les diff√©rents aspects.
Le mod√®le doit respecter la contrainte suivante :          , o√π      
  
  

 

. Ce qui implique :
 
 	          
   
  
     
 

   
 
Comme il n‚Äôy a pas de mots inconnus lors de la phase d‚Äôapprentissage,  	     
 
  
 

, et donc, par cons√©quent :

      (5)
 
     
   
 
En pratique, on attribuera √†  la valeur d‚Äôun ratio par rapport √† la plus petite des probabilit√©s

observ√©es parmi les fr√®res. Ceci signifie que pour chacun des mots qui n‚Äôappartiennent pas √†
un th√®me donn√© mais √† un des th√®mes fr√®res, une distribution de probabilit√© uniforme sur les
th√®mes fr√®res sera attribu√©e. La valeur , quant √† elle, devra prendre une probabilit√© tr√®s petite
(plus petite que le plus petit des  ). On peut donc r√©sumer la particularit√© de ce mod√®le en

√©non√ßant que la valeur  est la m√™me pour tous les mots inconnus de l‚Äôarbre, alors que 

d√©pend du groupe de fr√®res.
3.2.2 Attribution d‚Äôun th√®me au nouveau document
La probabilit√© du th√®me  √©tant donn√© le document    est evalu√©e de la m√™me mani√®re que

 
dans un unigramme classique, o√π les th√®mes sont remplac√©s par les n≈ìuds de l‚Äôarbre :
 
       
 
 
 
        (6)

 
 
   
 
o√π    est la probabilit√© a priori du th√®me  .
 

De m√™me                est la probabilit√© de la s√©quence de mots
  
   
 
           , √©valu√©e comme le produit des probabilit√©s de chaque mot dans le th√®me
  
 
 . On obtient donc une distribution des probabilit√©s th√©matiques de l‚Äôarbre, √©tant donn√© un

message observ√©.
3.2.3 Auto-√©valuation du mod√®le
Notre objectif est de proposer le th√®me qui correspond au mieux au message ennonc√©. Un
apport suppl√©mentaire qui peut s‚Äôav√©rer un atout int√©ressant, est de faire en sorte que le mod√®le
associe un degr√© de fiabilit√© √† la d√©cision th√©matique qu‚Äôil prend. Nous proposons que le mod√®le
associe l‚Äôun des crit√®res suivants :
1. la d√©cision est certaine,
2. la d√©cision est m√©diane,
3. la d√©cision est incertaine.
120
Identification th√©matique hi√©rarchique : Application aux forums de discussions
Cette auto-√©valuation du mod√®le est effectu√©e avec le pouvoir discriminant relatif √† sa d√©cision.
Celui-ci est √©valu√© par l‚Äô√©cart entre la probabilit√© attribu√©e au meilleur th√®me et la probabilit√©
attribu√©e au deuxi√®me meilleur :
   
              
   
   
Cet √©cart est ensuite compar√© √† deux seuils √Ü et √Ü fix√©s empiriquement afin de d√©terminer la
  
fiabilit√©, selon l‚Äôalgorithme suivant :
si ( > √Ü )
 
alors
d√©cision certaine
sinon
si ( > √Ü )

alors
d√©cision m√©diane
sinon
d√©cision incertaine
finsi
finsi
4 Exp√©rimentations
4.1 Les donn√©es
Nous avons choisi, pour cette √©tude, de nous restreindre aux groupes de langue fran√ßaise, plac√©s
dans une hi√©rarchie dont la racine est "fr.". Nos donn√©es couvrent une p√©riode de plusieurs
mois chevauchant les ann√©es 2000 et 2001. Ce corpus repr√©sente un total de 2 Go, avec plus
d‚Äô1 million d‚Äôarticles. Il est compos√© de 365 newsgroups, dont 307 feuilles dans lesquelles on
cherchera √† poster l‚Äôarticle. Le probl√®me principal relatif √† ces donn√©es concerne le fait que les
articles post√©s dans les news sont entach√©s d‚Äôerreurs. Cette contrainte implique l‚Äôapplication
d‚Äôun ensemble de pr√©-traitements afin d‚Äôextraire, autant que possible, les donn√©es int√©ressantes
de chacun des articles, c‚Äôest-√†-dire, le corps du texte sans les "bruits" associ√©s. Parmi ces pr√©-
traitements, il y a notamment une phase de segmentation en mots r√©alis√©e en comparaison √† un
lexique, et certains mots sont regroup√©s dans des classes selon deux possibilit√©s :
 en r√©f√©rence √† un lexique sp√©cialis√©2, pour les noms personnels, les ponctuations, les
villes, les pays et les "smileys" ;
 en utilisant des informations syntaxiques dans le cas des adresses √©lectroniques, des
adresses internet, des heures, des prix et des nombres (obtenus √† partir d‚Äôun ensemble
de r√®gles).
2Les lexiques ont √©t√© constitu√©s semi-manuellement √† partir de donn√©es collect√©es sur internet.
121
Brigitte Bigi, Kamel Sma√Øli
4.2 R√©sultats
Etant donn√© un article, notre but est de proposer le (ou les) forum de discussion qui semble le
plus ad√©quat, parmi l‚Äôensemble des feuilles de l‚Äôarbre. Afin d‚Äô√©valuer notre mod√®le, nous avons
compar√© le (ou les) n≈ìud ainsi propos√© √† ceux dans lesquels les articles du corpus de test ont
√©t√© envoy√©s. Les r√©sultats sont donc donn√©s sous forme de rappel et pr√©cision tels que :
rappel : Ratio entre le nombre de th√®mes d√©tect√©s correctement et le nombre de th√®mes √†
d√©tecter ;
pr√©cision : Ratio entre le nombre de th√®mes d√©tect√©s correctement et le nombre de th√®mes
d√©tect√©s.
4.2.1 Premi√®re exp√©rimentation
Le tableau 3 pr√©sente les r√©sultats obtenus dans ce cadre d‚Äôidentification th√©matique hi√©rar-
chique. Dans ce tableau, on peut observer que le mod√®le unigramme hi√©rarchique augmente
les performances de 2 % de rappel et de pr√©cision par rapport √† un unigramme classique. Ce
r√©sultat fait mention uniquement de la prise en compte des relations s√©mantiques entre fr√®res :
l‚Äôaugmentation l√©g√®re des r√©sultats nous permet de v√©rifier positivement que nous sommes sur la
bonne voie. En particulier, on peut observer les performances obtenues par deux sous-groupes
relatifs √† linux (niveau    ), et deux autres relatifs √† la biologie (niveau    ). On ob-
serve que les groupes linux obtiennent des taux d‚Äôidentification th√©matique tr√®s int√©ressants,
alors que les groupes de biologie sont mal identifi√©s. Ces diff√©rences importantes de perfor-
mances peuvent s‚Äôexpliquer en observant la taille de leur vocabulaire  mis en rapport avec

le nombre de documents du corpus d‚Äôapprentissage. Ainsi, on peut observer qu‚Äôavec des corpus
d‚Äôapprentissage de tailles proches, les deux th√®mes se trouvent tr√®s diff√©remment repr√©sen-
t√©s d‚Äôun point de vue de leur vocabulaire, ceci √©tant d√ª √† leur niveau de sp√©cialisation. Par
cons√©quent, il semble √©vident que si linux est tr√®s bien reconnu, c‚Äôest parce qu‚Äôil dispose de
suffisamment de donn√©es pour √™tre statistiquement significatif, contrairement √† biologie.
Newsgroup Rappel Pr√©cision Nb news Nb news 

apprentissage test
Unigramme classique, sur : 0,33 0,37 +1 Million 59157 425248
- tout le corpus de test
Unigramme hi√©rarchique, sur :
- tout le corpus de test 0,35 0,39 +1 Million 59157 425248
- fr.comp.os.linux.annonces 0,71 0,71 124 7 7954
- fr.comp.os.linux.configuration 0,94 0,97 16111 931 4536
- fr.bio.pharmacie 0,05 0,06 684 35 10363
- fr.bio.medecine 0,34 0,46 14316 490 44970
Figure 3: Performances d‚Äôidentification th√©matique sur quelques th√®mes
122
Identification th√©matique hi√©rarchique : Application aux forums de discussions
4.2.2 Seconde exp√©rimentation
Dans cette seconde exp√©rimentation, nous introduisons de nouvelles notions relatives √† la valeur
de rappel :
 "rappel exact" signifie que le th√®me du mod√®le doit √™tre rigoureusement celui de la solu-
tion, celui-ci correspond au rappel de l‚Äôexp√©rience pr√©c√©dente ;
 "rappel voisin" signifie que le th√®me du mod√®le peut √™tre soit exact, soit le fr√®re, soit le
p√®re, soit le fils de la solution ;
 "rappel branche" signifie que le th√®me du mod√®le est dans la m√™me branche que le th√®me
solution (c-√†-d les niveaux     √©gaux).
Rappel Rappel Rappel Pr√©cision Nombre de
exact voisins branche documents test
Tout le corpus 0,35 0,45 0,60 0,39 59 157 (soit 100 %)
Classe incertaine 0,17 0,27 0,44 0,19 30 970 (soit 52 %)
Classe m√©diane 0,52 0,62 0,75 0,57 22 328 (soit 38 %)
Classe certaine 0,71 0,79 0,87 0,77 5 859 (soit 10 %)
Figure 4: Performances d‚Äôidentification th√©matique de l‚Äôunigramme hi√©rarchique incluant
l‚Äôauto-√©valuation
Les r√©sultats sont pr√©sent√©s √† la premi√®re ligne du tableau 4. Avec un rappel de 0,60 sur la
branche solution, on remarque que le mod√®le, m√™me s‚Äôil ne trouve pas le "bon" n≈ìud, trouve
dans la majorit√© des cas le domaine th√©matique abord√©.
Les lignes suivantes (tableau 4) d√©composent ce r√©sultat, lorsque le mod√®le propose un degr√© de
fiabilit√© associ√© √† sa d√©cision th√©matique. Ces r√©sultats sont int√©ressants, car on voit bien que
lorsque le pouvoir discriminant du mod√®le est important, la solution propos√©e par le mod√®le est
souvent correcte avec un rappel exact √©gal √† 0,71. Avec un rappel sur la branche de 0,87, on
voit √©galement que dans ces cas, m√™me lorque l‚Äôon ne pr√©dit pas le th√®me exact, on est quand
m√™me capable d‚Äôapporter une solution avoisinante, ou tout au moins d‚Äôindiquer la branche √†
suivre avec un risque d‚Äôerreur tr√®s faible.
5 Conclusion
Dans cet article, nous avons pr√©sent√© un mod√®le unigramme th√©matique hi√©rarchique pour
l‚Äôidentification th√©matique hi√©rarchique. Ce mod√®le offre des performances l√©g√®rement sup√©-
rieures √† celles obtenues avec un unigramme classique, d√ª √ªau fait que les relations entre fr√®res
sont prises en compte √† travers une union de leur vocabulaire, et √† travers l‚Äôinsertion d‚Äôun facteur
de repli √† deux niveaux. Nous avons √©galement montr√© que m√™me si les performances sur le
n≈ìud exact ne sont pas √©lev√©es, elles augmentent nettement lorque l‚Äôon se compare √† la branche
choisie. Concernant l‚Äôensemble de ces r√©sultats, il est important de rappeler que le th√®me choisi
par le mod√®le est compar√© avec celui dans lequel l‚Äôarticle a √©t√© post√© par l‚Äôexp√©diteur. Ce crit√®re
123
Brigitte Bigi, Kamel Sma√Øli
n‚Äôest pas fiable, car il existe de nombreux groupes de news, et les utilisateurs n‚Äôont pas tou-
jours connaissance de leur existence, ou de leur contenu r√©el. Ceci implique que la d√©cision
relative au groupe dans lequel son article doit √™tre exp√©di√© n‚Äôest pas fiable, mais ce crit√®re de
comparaison est le seul dont nous disposons. Nous avons √©galement pu observer que le pou-
voir discriminant du mod√®le est un crit√®re suffisamment pertinent pour permettre au mod√®le de
s‚Äôauto-√©valuer et ainsi, de donner non seulement le th√®me du nouveau document, mais aussi, d‚Äôy
associer un degr√© de confiance. Ainsi, dans pr√®s de la moiti√© des articles, on trouve la branche
avec un rappel de plus de 0,75, et le n≈ìud exact avec un rappel de plus de 0,52.
Ces r√©sultats sont encourageants. Ils laissent entrevoir, entre-autres, la possibilit√© de leur utilisa-
tion dans un syst√®me de reconnaissance automatique de la parole. Dans ce cas, l‚Äôauto-√©valuation
du mod√®le est un facteur important qui permettra de n‚Äôintroduire le mod√®le th√©matique que
dans les cas o√π le th√®me est obtenu avec confiance. Diff√©rentes voies de recherche restent √†
explorer pour am√©liorer encore ces travaux. Notamment, ils pourraient √™tre mis en place sur
une arborescence cr√©√©e automatiquement. Dans ce cas, la m√©thode d‚Äôidentification th√©matique
int√®grera certains des param√®tres qui ont permis de constituer l‚Äôarbre, afin que les m√©thodes de
classification et d‚Äôidentification soient relativement homog√®nes.
R√©f√©rences
BIGI B., DE MORI R., EL-B√àZE M. & SPRIET T. (2000). A fuzzy decision strategy for topic identi-
fication and dynamic selection of language models. Special Issue on Fuzzy Logic in Signal Processing,
Signal Processing Journal, 80(6), 1085‚Äì1097.
GALESCU L. & ALLEN J. (2000). Hierarchical statistical language models: experiments on in-domain
adaptation. In Proceedings of the 6th International Conference on Spoken Language Processing (IC-
SLP‚Äô2000), p. 16‚Äì20, Beijing, China.
JELINEK F. (1990). Self-organized language modeling for speech recognition. Readings in Speech
Recognition, A. Waibel and K-F. Lee editors, p. 450‚Äì506.
KHUDANPUR S. P. & WU J. (1999). A maximum entropy language model integrating n-gram and topic
dependencies for conversational speech recognition. In IEEE International Conference on Acoustics,
Speech and Signal Processing, volume I, p. 2192, Phoenix, Arizona.
KNESER R. & PETERS J. (1997). Semantic clustering for adaptive language modeling. In IEEE Inter-
national Conference on Acoustics, Speech and Signal Processing, p. 779‚Äì783, Munich, Germany.
LI H. & YAMAMISHI K. (1997). Documentation classification using a finite mixture model. In Confer-
ence of the Association for Computational Linguistics, p. 39‚Äì47, Madrid, Spain.
MAHAJAN M., BEEFERMAN D. & HUANG X. D. (1999). Improved topic-dependent language model-
ing using information retrieval techniques. In IEEE International Conference on Acoustics, Speech and
Signal Processing, volume I, p. 2391, Phoenix, Arizona.
MARTIN S. C., LIERMANN J. & NEY H. (1997). Adaptive topic-dependent language modeling us-
ing word-based varigrams. In Proceeding of the European Conference On Speech Communication and
Technology.
MCCALLUM A., ROSENFELD R., MITCHELL T. & NG A. (1998). Improving text classification by
shrinkage in a hierarchy of classes. In International Conference on Machine Learning.
SALTON G. (1991). Developments in automatic text retrieval. Science, 253, 974‚Äì980.
SEYMORE K. & ROSENFELD R. (1997). Using story topics for language model adaptation. In Proceed-
ing of the European Conference On Speech Communication and Technology.
124
