<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
"http://www.w3.org/TR/html4/loose.dtd">
<html><head><title>Filtrages syntaxiques de co-occurrences pour la repr&#233;sentation vectorielle de documents</title>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
</head>
<body>
<div style="page-break-before:always; page-break-after:always"><div><p>TALN 2002, Nancy, 24&#8211;27 juin 2002
</p>
<p>Filtrages syntaxiques de co-occurrences pour la
repre&#769;sentation vectorielle de documents
</p>
<p>Romaric Besanc&#807;on, Martin Rajman
Laboratoire d&#8217;Intelligence Artificielle
</p>
<p>Faculte&#769; Informatique et Communications
&#180;Ecole Polytechnique Fe&#769;de&#769;rale de Lausanne, (IN) Ecublens 1015 Lausanne
</p>
<p>&#0;Romaric.Besancon,Martin.Rajman&#1;@epfl.ch
</p>
<p>Mots-clefs &#8211; Keywords
</p>
<p>Similarite&#769;s textuelles, repre&#769;sentation vectorielle de textes, se&#769;mantique distributionnelle, contexte
de co-occurrence
Textual similarities, vector space representation, distributional semantics, co-occurrence context
</p>
<p>Re&#769;sume&#769; - Abstract
</p>
<p>L&#8217;inte&#769;gration de co-occurrences dans les mode&#768;les de repre&#769;sentation vectorielle de documents
s&#8217;est ave&#769;re&#769;e une source d&#8217;ame&#769;lioration de la pertinence des mesures de similarite&#769;s textuelles cal-
cule&#769;es dans le cadre de ces mode&#768;les (Rajman et al., 2000; Besanc&#807;on, 2001). Dans cette optique,
la de&#769;finition des contextes pris en compte pour les co-occurrences est cruciale, par son influence
sur les performances des mode&#768;les a&#768; base de co-occurrences. Dans cet article, nous proposons
d&#8217;e&#769;tudier deux me&#769;thodes de filtrage des co-occurrences fonde&#769;es sur l&#8217;utilisation d&#8217;informations
syntaxiques supple&#769;mentaires. Nous pre&#769;sentons e&#769;galement une e&#769;valuation de ces me&#769;thodes dans
le cadre de la ta&#770;che de la recherche documentaire.
</p>
<p>The integration of co-occurrence information in the vector-space representation models for texts
has proven to improve the relevance of textual similarities (Rajman et al., 2000; Besanc&#807;on,
2001). In this framework, the definition of what is the context considered for the co-occurrences
is an important issue. In this paper, we provide the study of two methods for the filtering of the
co-occurrences, both using additional syntactic information. We also present an evaluation of
these methods in the framework of information retrieval.
</p>
<p>135</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>R. Besanc&#807;on, M. Rajman
</p>
<p>1 Introduction
</p>
<p>La notion de similarite&#769; entre textes est tre&#768;s souvent utilise&#769;e dans les applications du traitement de
la langue destine&#769;es a&#768; l&#8217;exploitation de collections de documents de grande taille. Par exemple, en
recherche documentaire, les documents pertinents retourne&#769;s par le moteur de recherche peuvent
e&#770;tre de&#769;finis comme les plus proches de la reque&#770;te selon une certaine mesure de similarite&#769; (Salton
&amp; McGill, 1983) ; de me&#770;me, le regroupement incre&#769;mental de documents en classes en fonction
de leurs similarite&#769;s peut permettre une structuration automatique de bases de donne&#769;es textuelles
a&#768; l&#8217;aide de techniques de classification automatique non supervise&#769;e (Salton et al., 1975).
La notion de similarite&#769; entre documents est e&#769;videmment fortement lie&#769;e au choix de la me&#769;thode
de repre&#769;sentation des textes. La repre&#769;sentation la plus utilise&#769;e est la repre&#769;sentation vectorielle
(mise en &#339;uvre, en particulier, dans les syste&#768;mes de recherche documentaire tels que SMART
(Salton, 1971)), dans le cadre de laquelle un document est repre&#769;sente&#769; par un vecteur dans un
espace vectoriel dont les dimensions sont associe&#769;es a&#768; des unite&#769;s linguistiques spe&#769;cifiques (mot,
stems, lemmes, etc). La similarite&#769; entre documents est alors e&#769;value&#769;e par une mesure de similarite&#769;
de&#769;finie sur cet espace vectoriel.
</p>
<p>Des ame&#769;liorations peuvent e&#769;galement e&#770;tre apporte&#769;es dans ce mode&#768;le de repre&#769;sentation par
l&#8217;inte&#769;gration de connaissances externes (Besanc&#807;on, 2001). En particulier, dans l&#8217;optique de
la se&#769;mantique distributionnelle (Rajman et al., 2000), des connaissances de co-occurrences
peuvent e&#770;tre utilise&#769;es pour inte&#769;grer plus d&#8217;informations se&#769;mantiques dans la repre&#769;sentation.
L&#8217;objectif de cet article est d&#8217;e&#769;tudier l&#8217;influence de la me&#769;thode de se&#769;lection des co-occurrences
conside&#769;re&#769;es sur la qualite&#769; des repre&#769;sentations et des mesures de similarite&#769; entre documents.
</p>
<p>Dans la section 2, nous pre&#769;sentons brie&#768;vement le mode&#768;le de repre&#769;sentation vectoriel standard,
ainsi que le mode&#768;le DSIR, qui e&#769;tend le mode&#768;le standard par l&#8217;inte&#769;gration de co-occurrences dans
la repre&#769;sentation des documents. Dans la section 3, nous pre&#769;sentons deux me&#769;thodes de filtrage
des co-occurrences utilisant des informations syntaxiques pour de&#769;terminer quelles seront les co-
occurrences effectivement conside&#769;re&#769;es. Enfin, dans la section 4, nous proposons une e&#769;valuation
de ces me&#769;thodes de filtrage pour la ta&#770;che de la recherche documentaire.
</p>
<p>2 Le mode&#768;le de repre&#769;sentation DSIR
</p>
<p>2.1 Mode&#768;le vectoriel
</p>
<p>Dans le cadre du mode&#768;le vectoriel standard (VS), un document &#0; est repre&#769;sente&#769; par un vecteur
&#0;
</p>
<p>&#0; &#1;
</p>
<p>&#0; &#1;&#0;
</p>
<p>&#0; &#1;
</p>
<p>&#0;
</p>
<p>&#1; &#2; &#2; &#2; &#1; &#0;
</p>
<p>&#0; &#1;
</p>
<p>&#0;&#2; &#0;
</p>
<p>&#2;, appele&#769; profil lexical, dans lequel la &#3;e composante &#0;&#0; &#1;
&#3;
</p>
<p>repre&#769;sente le
poids (ou importance), dans le document &#0;, du terme d&#8217;indexation &#4;
</p>
<p>&#3;
</p>
<p>associe&#769; a&#768; la &#3;e dimen-
sion de l&#8217;espace vectoriel. D&#8217;une fac&#807;on ge&#769;ne&#769;rale, le poids est le plus souvent une fonction de
la fre&#769;quence du terme dans le document et se de&#769;compose habituellement en une ponde&#769;ration
locale, une ponde&#769;ration globale et un facteur de normalisation (par rapport a&#768; la longueur du
document). Pour nos expe&#769;riences, nous avons utilise&#769; le sche&#769;ma de ponde&#769;ration ltn de SMART
(Salton &amp; Buckley, 1988; Singhal, 1997) :
</p>
<p>&#0;
</p>
<p>&#0; &#1;
</p>
<p>&#3;
</p>
<p>&#0; &#5;
</p>
<p>&#3;
</p>
<p>&#0; &#0;&#1;&#2; &#0; &#1;&#3; &#4; &#6;&#7;&#8;&#1;&#3;&#2; &#2;&#2; (1)
ou&#768; &#3;&#2; est la fre&#769;quence du mot dans le document et &#0;&#1;&#2; est le facteur de fre&#769;quence en document
inverse &#0;&#1;&#2; &#0; &#5;&#6;&#7; &#0;
</p>
<p>&#0;&#1;
</p>
<p>, ou&#768; &#1;&#2; est la fre&#769;quence en documents du terme (c&#8217;est-a&#768;-dire le nombre de
</p>
<p>136</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Filtrages syntaxiques de co-occurrences
</p>
<p>documents dans lesquels le terme appara&#305;&#770;t). Dans ce cas, le facteur de ponde&#769;ration locale est
&#3; &#4; &#6;&#7;&#8;&#1;&#3;&#2; &#2; , le facteur de ponde&#769;ration globale est &#0;&#1;&#2; (ce facteur permet d&#8217;accorder un poids
plus important aux termes qui apparaissent moins fre&#769;quemment dans la collection et sont donc
plus utiles pour la discrimination). Aucun facteur de normalisation n&#8217;est inte&#769;gre&#769; directement
dans cette ponde&#769;ration mais une normalisation implicite est effectue&#769;e en utilisant la mesure de
similarite&#769; du cosinus, inde&#769;pendante de la norme.
</p>
<p>2.2 Mode&#768;le a&#768; base de co-occurrences
</p>
<p>Le mode&#768;le DSIR est un mode&#768;le vectoriel permettant d&#8217;inte&#769;grer des informations se&#769;mantiques
supple&#769;mentaires par l&#8217;utilisation de co-occurrences (Rajman &amp; Bonnet, 1992; Rajman et al.,
2000; Besanc&#807;on, 2001).
Dans le cadre de ce mode&#768;le, les unite&#769;s linguistiques &#9;
</p>
<p>&#4;
</p>
<p>conside&#769;re&#769;es sont repre&#769;sente&#769;es par un
vecteur &#10;
</p>
<p>&#4;
</p>
<p>&#0; &#1;&#10;
</p>
<p>&#4;&#0;
</p>
<p>&#1; &#2; &#2; &#2; &#1; &#10;
</p>
<p>&#4;&#0;&#2; &#0;
</p>
<p>&#2;, appele&#769; profil de co-occurrence, dont chaque composante &#10;
&#4;&#3;
</p>
<p>est la
fre&#769;quence de co-occurrence de l&#8217;unite&#769; linguistique &#9;
</p>
<p>&#4;
</p>
<p>avec un terme d&#8217;indexation &#4;
&#3;
</p>
<p>. Un docu-
ment &#0; est alors repre&#769;sente&#769; comme la somme ponde&#769;re&#769;e des profils de co-occurrence des unite&#769;s
linguistiques qu&#8217;il contient, c&#8217;est-a&#768;-dire par un vecteur &#0;&#5;&#1; &#0; &#1;&#0;&#5;&#1;
</p>
<p>&#0;
</p>
<p>&#1; &#2; &#2; &#2; &#1; &#0;
</p>
<p>&#5;&#1;
</p>
<p>&#0;&#2; &#0;
</p>
<p>&#2; ou&#768; chaque &#0;&#5;&#1;
&#3;
</p>
<p>est de&#769;fini par :
&#0;
</p>
<p>&#5;&#1;
</p>
<p>&#3;
</p>
<p>&#0;
</p>
<p>&#0;
</p>
<p>&#6;
</p>
<p>&#0;
</p>
<p>&#1;&#7;
</p>
<p>&#5;
</p>
<p>&#4;
</p>
<p>&#10;
</p>
<p>&#4;&#3;
</p>
<p>ou&#768; la ponde&#769;ration &#5;
&#4;
</p>
<p>est celle de&#769;finie par l&#8217;e&#769;quation (1).
Notons que, dans le mode&#768;le DSIR, les termes effectivement pre&#769;sents dans les documents ne sont
pris en compte qu&#8217;indirectement, par le biais de leur profil de co-occurrence. Pour cette raison,
un mode&#768;le DSIR hybride prenant en compte a&#768; la fois les occurrences et les co-occurrences des
termes dans les documents a e&#769;galement e&#769;te&#769; propose&#769; (Rungsawang, 1997; Rajman et al., 2000).
Dans ce mode&#768;le un document est repre&#769;sente&#769; par un vecteur dont la &#3;e composante est de&#769;finie
par :
</p>
<p>&#0;
</p>
<p>&#5;&#1;
</p>
<p>&#3;
</p>
<p>&#0; &#11; &#5;
</p>
<p>&#3;
</p>
<p>&#4; &#1;&#3;&#1; &#11;&#2;
</p>
<p>&#0;
</p>
<p>&#6;
</p>
<p>&#0;
</p>
<p>&#1;&#7;
</p>
<p>&#5;
</p>
<p>&#4;
</p>
<p>&#10;
</p>
<p>&#4;&#3;
</p>
<p>(2)
</p>
<p>ou&#768; &#11; est le coefficient d&#8217;hybridation entre le mode&#768;le DSIR pur et le mode&#768;le VS.
</p>
<p>3 Filtrage syntaxique des co-occurrences
</p>
<p>Le calcul des fre&#769;quences de co-occurrence &#10;
&#4;&#3;
</p>
<p>de&#769;pend bien e&#769;videmment en premier lieu des
choix effectue&#769;s pour ce qui est de la se&#769;lection des relations de co-occurrence conside&#769;re&#769;es, et
donc, en particulier, de la de&#769;finition des contextes qui seront pris en compte pour le calcul de
ces co-occurrences. Ces contextes peuvent e&#770;tre de trois types : documentaire, positionnel ou
syntaxique.
</p>
<p>L&#8217;approche la plus simple est de conside&#769;rer soit un contexte positionnel, soit un contexte docu-
mentaire et donc de calculer, a&#768; partir d&#8217;un corpus de re&#769;fe&#769;rence, toutes les co-occurrences entre
toutes les unite&#769;s linguistiques prises deux a&#768; deux dans une fene&#770;tre de taille donne&#769;e (contexte po-
sitionnel) ou sur une unite&#769; documentaire donne&#769;e, comme la phrase ou le paragraphe par exemple
(contexte documentaire).
</p>
<p>137</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>R. Besanc&#807;on, M. Rajman
</p>
<p>Ces deux approches simples peuvent ne&#769;anmoins s&#8217;ave&#769;rer insuffisantes. En effet, dans l&#8217;une
comme dans l&#8217;autre, des co-occurrences non linguistiquement pertinentes peuvent e&#770;tre prises
en compte. Prenons par exemple le contexte repre&#769;sente&#769; par la phrase suivante :
</p>
<p>&#0; L&#8217;acteur porte un masque grimac&#807;ant de the&#769;a&#770;tre antique.&#1;
</p>
<p>Une premie&#768;re phase de pre&#769;-traitement permet d&#8217;identifier les unite&#769;s linguistiques qui com-
posent la phrase. Par exemple, si les unite&#769;s conside&#769;re&#769;es sont les lemmes associe&#769;s a&#768; leur e&#769;tiquette
morpho-syntaxique, on obtient :
</p>
<p>le&#2;Ds acteur&#2;Ncms porter&#2;Vs un&#2;Dms masque&#2;Ncms grimac&#807;ant&#2;Ams de&#2;S the&#769;a&#770;tre&#2;Ncms antique&#2;As
Si l&#8217;on calcule alors les co-occurrences sur l&#8217;ensemble de la phrase, des co-occurrences per-
tinentes comme (masque&#8211;grimac&#807;ant) ou (the&#769;a&#770;tre&#8211;antique) seront effectivement se&#769;lectionne&#769;es,
mais des co-occurrences croise&#769;es comme (acteur&#8211;antique) ou (the&#769;a&#770;tre&#8211;grimac&#807;ant), qui semblent
en revanche beaucoup moins pertinentes (et, en tout cas, ne sont pas sugge&#769;re&#769;es par la structure
de la phrase) seront e&#769;galement prises en compte.
L&#8217;utilisation, dans la de&#769;finition des contextes, d&#8217;une information supple&#769;mentaire sur les de&#769;pen-
dances syntaxiques entre les unite&#769;s linguistiques de la phrase permet une de&#769;finition plus fine des
co-occurrences a&#768; conside&#769;rer (Rajman, 1995; Rungsawang, 1997).
Nous pre&#769;sentons dans les deux sections suivantes deux approches possibles : la premie&#768;re re-
pose sur l&#8217;ide&#769;e d&#8217;un filtrage des co-occurrences, l&#8217;objectif e&#769;tant d&#8217;e&#769;liminer certaines des co-
occurrences non souhaite&#769;es, sans faire d&#8217;hypothe&#768;ses sur les co-occurrences restantes ; la seconde
repose sur l&#8217;ide&#769;e d&#8217;une se&#769;lection des co-occurrences, l&#8217;objectif e&#769;tant cette fois-ci de ne garder
que les co-occurrences syntaxiquement fonde&#769;es, et de rejeter toutes les autres.
Dans les deux cas, les relations de co-occurrence prises en compte seront synthe&#769;tise&#769;es dans
un graphe de co-occurrences, dans lequel les n&#339;uds sont associe&#769;s aux unite&#769;s linguistiques
conside&#769;re&#769;es et les arcs repre&#769;sentent les relations de co-occurrence. Un exemple de tels graphes
est donne&#769; dans le tableau re&#769;capitulatif de la figure 1 a&#768; la fin de la section 3.
</p>
<p>3.1 Filtrage par les groupes syntaxiques
</p>
<p>Dans l&#8217;exemple donne&#769; ci-dessus, une cate&#769;gorie de co-occurrences qui paraissent clairement non
pertinentes sont les co-occurrences entre un nom et un adjectif qui qualifie un autre nom de la
phrase (des co-occurrences du type (acteur-antique) ou (the&#769;a&#770;tre-grimac&#807;ant)). L&#8217;objectif de la
me&#769;thode de filtrage propose&#769;e dans cette section est donc d&#8217;e&#769;liminer ce type de co-occurrences.
</p>
<p>Pour ce faire, nous utilisons un analyseur syntaxique de surface (shallow parser) pour produire
les groupes syntaxiques e&#769;le&#769;mentaires correspondant a&#768; la structure de la phrase, associe&#769;s chacun
a&#768; une unite&#769; linguistique particulie&#768;re repre&#769;sentant la te&#770;te du groupe (en pratique, la te&#770;te d&#8217;un
groupe nominal est le nom de ce groupe, et la te&#770;te d&#8217;un groupe verbal est le verbe principal
&#8211; i.e. pas les auxiliaires). Les seules co-occurrences conside&#769;re&#769;es sont alors les co-occurrences
entre unite&#769;s linguistiques d&#8217;un me&#770;me groupe syntaxique ou entre te&#770;tes de diffe&#769;rents groupes
syntaxiques (Besanc&#807;on et al., 1999). Cela permet effectivement d&#8217;e&#769;viter de prendre en compte
des co-occurrences entre des unite&#769;s linguistiques qui seraient toutes deux des de&#769;pendances dans
des groupes syntaxiques diffe&#769;rents, ou entre des unite&#769;s linguistiques qui seraient l&#8217;une la te&#770;te
d&#8217;un groupe syntaxique et l&#8217;autre une de&#769;pendance dans un autre groupe syntaxique.
</p>
<p>Apre&#768;s lemmatisation, un de&#769;coupage en groupes syntaxiques de la phrase d&#8217;exemple pourrait
</p>
<p>138</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Filtrages syntaxiques de co-occurrences
</p>
<p>e&#770;tre le suivant1 :
</p>
<p>(( le&#2;Ds *acteur&#2;Ncms ) ( *porter&#2;Vs ) ( un&#2;Dms *masque&#2;Ncms grimac&#807;ant&#2;Ams ) ( de&#2;S *the&#769;a&#770;tre&#2;Ncms
antique&#2;As ))
ou&#768; les groupes syntaxiques sont de&#769;limite&#769;s par les parenthe&#768;ses et les te&#770;tes des groupes sont iden-
tifie&#769;es par le symbole &#0; * &#1; ante&#769;pose&#769;e.
</p>
<p>3.2 Se&#769;lection par les relations syntaxiques
</p>
<p>La seconde approche propose&#769;e repose sur l&#8217;ide&#769;e de se&#769;lectionner les &#0; bonnes&#1; co-occurrences,
i.e. les co-occurrences a&#768; conserver en raison de leur pertinence syntaxique.
</p>
<p>La me&#769;thode retenue pour cette approche repose sur l&#8217;utilisation des re&#769;sultats d&#8217;une analyse syn-
taxique produisant diffe&#769;rentes relations syntaxiques entre les unite&#769;s linguistiques de la phrase,
comme par exemple les relations de type sujet-verbe (SUJ), verbe-objet (OBJ), comple&#769;ment de
nom (CNOM), ou qualification d&#8217;un nom par un adjectif (ADJ). Les seules co-occurrences qui
sont alors conside&#769;re&#769;es sont celles entre les unite&#769;s qui sont effectivement relie&#769;es par une relation
syntaxique identifie&#769;e.
</p>
<p>Par exemple, les relations syntaxiques produites par l&#8217;analyseur syntaxique XeLDA de Xerox
(Xerox, 1990) pour la phrase d&#8217;exemple sont :
SUJ(acteur, porter)
OBJ(porter, masque)
ADJ(masque, grimac&#807;ant)
ADJ(the&#769;a&#770;tre, antique)
CNOM(masque, the&#769;a&#770;tre)
</p>
<p>Les graphes de co-occurrences produits pour la phrase d&#8217;exemple pour chacune des me&#769;thodes
pre&#769;sente&#769;es sont indique&#769;s dans le tableau re&#769;capitulatif de la figure 1. Dans tous les cas, un pre&#769;-
traitement ne gardant que les lemmes des noms, des verbes et des adjectifs a e&#769;te&#769; re&#769;alise&#769;.
Deux constatations peuvent e&#770;tre faites sur la base de l&#8217;exemple traite&#769; : d&#8217;une part, les me&#769;thodes
de filtrage/se&#769;lection re&#769;duisent comme pre&#769;vu le nombre de co-occurrences prises en compte, en
e&#769;liminant les co-occurrences entre termes non lie&#769;s syntaxiquement. D&#8217;autre part, on remarque
qu&#8217;elles pre&#769;sentent ne&#769;anmoins certains inconve&#769;nients : en particulier, la se&#769;lection sur la base des
relations syntaxiques supprime les co-occurrences entre sujet et objet de l&#8217;action (ici &#0; acteur &#1;
et &#0; masque &#1;), de me&#770;me que des co-occurrences comme &#0; acteur&#8211;the&#769;a&#770;tre &#1;, qui pourraient
toutes deux sembler se&#769;mantiquement pertinentes.
</p>
<p>4 &#180;Evaluation
</p>
<p>L&#8217;e&#769;valuation des me&#769;thodes de filtrage syntaxique des co-occurrences a e&#769;te&#769; effectue&#769;e pour la
ta&#770;che de la recherche documentaire. Les re&#769;sultats pre&#769;sente&#769;s ici ont e&#769;te&#769; obtenus sur des donne&#769;es
provenant de la seconde campagne d&#8217;e&#769;valuation AMARYLLIS (Coret et al., 1997; Landi et al.,
1998), pre&#769;sente&#769;es dans le tableau 1.
</p>
<p>1Ce parenthe&#769;sage a e&#769;te&#769; produit par l&#8217;analyseur syntaxique Sylex.
</p>
<p>139</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>R. Besanc&#807;on, M. Rajman
</p>
<p>&#0; L&#8217;acteur porte un masque grimac&#807;ant de the&#769;a&#770;tre antique&#1;
contexte co-occurrences
</p>
<p>(a)
contexte
</p>
<p>positionnel
</p>
<p>acteur porter masque
grimac&#807;ant the&#769;a&#770;tre
</p>
<p>antique
</p>
<p>acteur
porter masque
</p>
<p>the&#769;a&#770;tre grimac&#807;ant
antique
</p>
<p>(b)
filtrage par
les groupes
syntaxiques
</p>
<p>(*acteur)
(*porter)
</p>
<p>(*masque grimac&#807;ant)
(*the&#769;a&#770;tre antique)
</p>
<p>acteur
porter masque
</p>
<p>the&#769;a&#770;tre grimac&#807;ant
antique
</p>
<p>(c)
se&#769;lection
par les
</p>
<p>relations
syntaxiques
</p>
<p>SUJ(acteur,porter)
OBJ(porter,masque)
ADJ(masque,grimac&#807;ant)
ADJ(the&#769;a&#770;tre,antique)
CNOM(masque,the&#769;a&#770;tre)
</p>
<p>acteur
porter masque
</p>
<p>the&#769;a&#770;tre grimac&#807;ant
antique
</p>
<p>FIG. 1 &#8211; Exemples de graphes de co-occurrences , (a) toutes les co-occurrences, (b) avec filtrage
des co-occurrences par les groupes syntaxiques, (c) avec se&#769;lection des co-occurrences sur les
relations syntaxiques.
</p>
<p>Corpus Sujet type nom Nb docs Nb mots
LRSA extraits de livres sur laMe&#769;lane&#769;sie
</p>
<p>documents md1 355 428803
reque&#770;tes mt1 15 1301
</p>
<p>OFIL
articles extraits du
journal &#0; Le Monde &#1;
</p>
<p>documents od1 11016 4915890
reque&#770;tes ot1 26 1412
</p>
<p>INIST notes bibliographiques
documents od1 163308 13678485
reque&#770;tes ot1 30 2022
</p>
<p>TAB. 1 &#8211; Donne&#769;es utilise&#769;es pour les tests, provenant de la campagne AMARYLLIS
</p>
<p>Filtrage des co-occurrences par les groupes syntaxiques Dans une premie&#768;re phase, les cor-
pus ont e&#769;te&#769; analyse&#769;s a&#768; l&#8217;aide d&#8217;un analyseur syntaxique (SYLEX, de INGENIA-LN (Constant,
1995)) pour de&#769;terminer les cate&#769;gories morpho-syntaxiques des mots ainsi que leurs lemmes.
Des lexiques ont alors e&#769;te&#769; extraits, comprenant les lemmes des noms, verbes et adjectifs appa-
raissant dans les corpus. Ces lexiques forment l&#8217;ensemble &#12; des unite&#769;s linguistiques qui seront
conside&#769;re&#769;es. L&#8217;ensemble &#13; des termes d&#8217;indexation a e&#769;te&#769; cre&#769;e&#769; a&#768; partir de &#12; en se&#769;lectionnant les
unite&#769;s linguistiques de fre&#769;quence en documents comprise entre &#0;&#5;&#0;
</p>
<p>&#0;&#1;&#1;
</p>
<p>et &#0;&#5;&#0;
&#0;&#1;
</p>
<p>, avec &#14; l&#8217;ensemble des
documents du corpus. Les matrices de co-occurrence (de dimension &#2;&#12; &#2;&#0;&#2;&#13; &#2;) ont e&#769;te&#769; construites
pour les corpus LRSA, OFIL et INIST, en utilisant des contextes positionnels (matrices &#15;
</p>
<p>&#8;
</p>
<p>, &#15;
&#9;
</p>
<p>,
</p>
<p>&#15;
</p>
<p>&#4;
</p>
<p>respectivement), puis en utilisant le filtrage par les groupes syntaxiques (matrices &#15; &#10;&#11;
&#8;
</p>
<p>, &#15;
&#10;&#11;
</p>
<p>&#9;
</p>
<p>,
</p>
<p>&#15;
</p>
<p>&#10;&#11;
</p>
<p>&#4;
</p>
<p>). Le tableau 2 pre&#769;sente la taille des lexiques et des matrices de co-occurrences, en indi-
quant e&#769;galement pour ces dernie&#768;res le taux de remplissage &#4;
</p>
<p>&#12;
</p>
<p>(c&#8217;est-a&#768;-dire le pourcentage de
co-occurrences de fre&#769;quence non nulle parmi les &#2;&#12; &#2; &#0; &#2;&#13; &#2; co-occurrences possibles), et le taux
&#4;
</p>
<p>&#10;&#11;
</p>
<p>&#7;
</p>
<p>de diminution du nombre de co-occurrences prises en compte par rapport a&#768; l&#8217;approche sans
filtrage syntaxique. Il est a&#768; noter que l&#8217;utilisation du filtrage syntaxique permet une re&#769;duction
d&#8217;environ 25 a&#768; 30% des matrices manipule&#769;es.
</p>
<p>140</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Filtrages syntaxiques de co-occurrences
</p>
<p>Les performances obtenues avec le mode&#768;le de repre&#769;sentation DSIR hybride (avec un coefficient
d&#8217;hybridation &#16;&#6;&#17;&#18;&#16; &#0; &#8;&#2;&#9;) sont pre&#769;sente&#769;s pour les trois corpus dans les tableaux 2. Les mesures
d&#8217;e&#769;valuation choisies sont les suivantes : pre&#769;cision moyenne (note&#769;e avg p), R-pre&#769;cision2 (note&#769;e
R p), les pre&#769;cisions a&#768; plusieurs points de coupure (la pre&#769;cision a&#768; &#19; documents est note&#769;e p&#19; ),
le nombre total de documents pertinents retourne&#769;s par le syste&#768;me (note&#769;e relret), et le rappel
final a&#768; 1000 documents (note&#769; r1000). Les re&#769;sultats sont pre&#769;sente&#769;s en indiquant les pourcentages
d&#8217;ame&#769;lioration par rapport a&#768; un re&#769;sultat de base (pre&#769;sent dans la premie&#768;re colonne des tableaux),
avec le coefficient de risque &#17;
</p>
<p>&#13;
</p>
<p>du test de Wilcoxon qui lui est associe&#769; (Van Rijsbergen, 1979).
Cette valeur indique la confiance que l&#8217;on accorde au fait que la diffe&#769;rence mesure&#769;e n&#8217;est pas due
au hasard (plus la valeur de &#17;
</p>
<p>&#13;
</p>
<p>est petite, plus l&#8217;hypothe&#768;se que la diffe&#769;rence me&#769;diane est nulle
peut e&#770;tre rejete&#769;e, et on peut donc conclure que les re&#769;sultats sont significativement diffe&#769;rents).
</p>
<p>taille des lexiques et des matrices
de co-occurrences
</p>
<p>LRSA OFIL INIST
unite&#769;s
</p>
<p>&#2;&#12; &#2; 9762 28691 23390
&#2;&#13; &#2; 4635 2796 3398
</p>
<p>matrice &#15;
&#8;
</p>
<p>&#15;
</p>
<p>&#9;
</p>
<p>&#15;
</p>
<p>&#4;
</p>
<p>taille 3 Mo 26 Mo 10 Mo
&#4;
</p>
<p>&#12;
</p>
<p>1.66% 8.11% 3.06%
matrice &#15;&#10;&#11;
</p>
<p>&#8;
</p>
<p>&#15;
</p>
<p>&#10;&#11;
</p>
<p>&#9;
</p>
<p>&#15;
</p>
<p>&#10;&#11;
</p>
<p>&#4;
</p>
<p>taille 2 Mo 20 Mo 7 Mo
&#4;
</p>
<p>&#12;
</p>
<p>1.14% 6.16% 3.04%
&#4;
</p>
<p>&#10;&#11;
</p>
<p>&#7;
</p>
<p>31.7% 24.1% 23.8%
</p>
<p>LRSA
&#0;
</p>
<p>&#8;
</p>
<p>&#1; &#0; &#1;&#2;&#2; &#0;
</p>
<p>&#10;&#11;
</p>
<p>&#8;
</p>
<p>&#1; &#0; &#1;&#2;&#2;
</p>
<p>avg p 0.3942 0.4048 (+2.69%)
&#14;
</p>
<p>&#1;
</p>
<p>&#2;&#1;&#15;&#1;&#0;&#3;
</p>
<p>R p 0.3993 0.4087 (+2.35%)
&#14;
</p>
<p>&#1;
</p>
<p>&#2;&#1;&#15;&#4;&#4;
</p>
<p>p5 0.6533 0.6667 (+2.05%)
&#14;
</p>
<p>&#1;
</p>
<p>&#2;&#1;&#15;&#5;&#6;
</p>
<p>p10 0.6333 0.62 (-2.15%)
&#14;
</p>
<p>&#1;
</p>
<p>&#2;&#1;&#15;&#7;&#8;
</p>
<p>p15 0.5422 0.5422 (+0%)
&#14;
</p>
<p>&#1;
</p>
<p>&#2;&#0;
</p>
<p>p20 0.47 0.4867 (+3.55%)
&#14;
</p>
<p>&#1;
</p>
<p>&#2;&#1;&#15;&#0;&#9;
</p>
<p>p30 0.3911 0.4022 (+2.84%)
&#14;
</p>
<p>&#1;
</p>
<p>&#2;&#1;&#15;&#7;&#0;
</p>
<p>p100 0.19 0.1927 (+1.42%)
&#14;
</p>
<p>&#1;
</p>
<p>&#2;&#1;&#15;&#6;&#8;
</p>
<p>p200 0.1197 0.12 (+0.251%)
&#14;
</p>
<p>&#1;
</p>
<p>&#2;&#1;&#15;&#7;&#0;
</p>
<p>p500 0.0509 0.0515 (+1.18%)
&#14;
</p>
<p>&#1;
</p>
<p>&#2;&#1;&#15;&#0;&#3;
</p>
<p>p1000 0.0255 0.0257 (+0.784%)
&#14;
</p>
<p>&#1;
</p>
<p>&#2;&#1;&#15;&#0;&#3;
</p>
<p>relret 382 386 (+1.05%)
&#14;
</p>
<p>&#1;
</p>
<p>&#2;&#1;&#15;&#0;&#3;
</p>
<p>r1000 0.9031 0.9125 (+1.05%)
&#14;
</p>
<p>&#1;
</p>
<p>&#2;&#1;&#15;&#0;&#3;
</p>
<p>INIST
&#0;
</p>
<p>&#4;
</p>
<p>&#1; &#0; &#1;&#2;&#2; &#0;
</p>
<p>&#10;&#11;
</p>
<p>&#4;
</p>
<p>&#1; &#0; &#1;&#2;&#2;
</p>
<p>avg p 0.1095 0.111 (+1.37%)
&#14;
</p>
<p>&#1;
</p>
<p>&#2;&#1;&#15;&#7;&#10;
</p>
<p>R p 0.1581 0.1595 (+0.886%)
&#14;
</p>
<p>&#1;
</p>
<p>&#2;&#1;&#15;&#6;&#7;
</p>
<p>p5 0.3 0.3133 (+4.43%)
&#14;
</p>
<p>&#1;
</p>
<p>&#2;&#1;&#15;&#6;&#6;
</p>
<p>p10 0.24 0.2633 (+9.71%)
&#14;
</p>
<p>&#1;
</p>
<p>&#2;&#1;&#15;&#0;&#6;
</p>
<p>p15 0.2156 0.2133 (-1.08%)
&#14;
</p>
<p>&#1;
</p>
<p>&#2;&#1;&#15;&#9;&#4;
</p>
<p>p20 0.2083 0.2067 (-0.774%)
&#14;
</p>
<p>&#1;
</p>
<p>&#2;&#1;&#15;&#8;&#10;
</p>
<p>p30 0.1833 0.18 (-1.83%)
&#14;
</p>
<p>&#1;
</p>
<p>&#2;&#1;&#15;&#4;&#6;
</p>
<p>p100 0.103 0.107 (+3.88%)
&#14;
</p>
<p>&#1;
</p>
<p>&#2;&#1;&#15;&#0;&#8;
</p>
<p>p200 0.071 0.0748 (+5.35%)
&#14;
</p>
<p>&#1;
</p>
<p>&#2;&#1;&#15;&#1;&#0;&#8;
</p>
<p>p500 0.0317 0.0324 (+2.21%)
&#14;
</p>
<p>&#1;
</p>
<p>&#2;&#1;&#15;&#0;&#3;
</p>
<p>p1000 0.0159 0.0162 (+1.89%)
&#14;
</p>
<p>&#1;
</p>
<p>&#2;&#1;&#15;&#1;&#5;&#4;
</p>
<p>relret 476 486 (+2.1%)
&#14;
</p>
<p>&#1;
</p>
<p>&#2;&#1;&#15;&#0;
</p>
<p>r1000 0.3383 0.3454 (+2.1%)
&#14;
</p>
<p>&#1;
</p>
<p>&#2;&#1;&#15;&#1;&#7;&#10;
</p>
<p>OFIL
&#0;
</p>
<p>&#9;
</p>
<p>&#1; &#0; &#1;&#2;&#2; &#0;
</p>
<p>&#10;&#11;
</p>
<p>&#9;
</p>
<p>&#1; &#0; &#1;&#2;&#2;
</p>
<p>0.196 0.1867 (-4.98%)
&#14;
</p>
<p>&#1;
</p>
<p>&#2;&#1;&#15;&#8;&#4;
</p>
<p>0.2333 0.2308 (-1.08%)
&#14;
</p>
<p>&#1;
</p>
<p>&#2;&#1;&#15;&#8;&#0;
</p>
<p>0.3538 0.3308 (-6.95%)
&#14;
</p>
<p>&#1;
</p>
<p>&#2;&#1;&#15;&#6;
</p>
<p>0.2769 0.2846 (+2.78%)
&#14;
</p>
<p>&#1;
</p>
<p>&#2;&#1;&#15;&#9;&#4;
</p>
<p>0.2462 0.2333 (-5.53%)
&#14;
</p>
<p>&#1;
</p>
<p>&#2;&#1;&#15;&#3;&#7;
</p>
<p>0.2269 0.2154 (-5.34%)
&#14;
</p>
<p>&#1;
</p>
<p>&#2;&#1;&#15;&#0;&#8;
</p>
<p>0.1949 0.1949 (+0%)
&#14;
</p>
<p>&#1;
</p>
<p>&#2;&#1;&#15;&#5;&#9;
</p>
<p>0.1008 0.0969 (-4.02%)
&#14;
</p>
<p>&#1;
</p>
<p>&#2;&#1;&#15;&#0;&#0;
</p>
<p>0.0604 0.0581 (-3.96%)
&#14;
</p>
<p>&#1;
</p>
<p>&#2;&#1;&#15;&#3;&#3;
</p>
<p>0.0248 0.0246 (-0.813%)
&#14;
</p>
<p>&#1;
</p>
<p>&#2;&#1;&#15;&#5;&#7;
</p>
<p>0.0124 0.0123 (-0.813%)
&#14;
</p>
<p>&#1;
</p>
<p>&#2;&#1;&#15;&#8;&#3;
</p>
<p>322 320 (-0.625%)
&#14;
</p>
<p>&#1;
</p>
<p>&#2;&#1;&#15;&#6;&#5;
</p>
<p>0.5486 0.5451 (-0.625%)
&#14;
</p>
<p>&#1;
</p>
<p>&#2;&#1;&#15;&#6;&#5;
</p>
<p>TAB. 2 &#8211; Re&#769;sultats sur les corpus LRSA, OFIL et INIST pour le filtrage des co-occurrences par
les groupes syntaxiques (les re&#769;sultats en gras indiquent les re&#769;sultats significativement meilleurs
(&#17;
</p>
<p>&#13;
</p>
<p>&#20; &#8;&#2;&#9;))
</p>
<p>2La R-pre&#769;cision est la pre&#769;cision obtenue pour un nombre de documents retourne&#769;s correspondant au nombre de
documents pertinents pre&#769;sents dans la base. Donc en particulier, dans ce cas, la pre&#769;cision est e&#769;gale au rappel.
</p>
<p>141</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>R. Besanc&#807;on, M. Rajman
</p>
<p>Une premie&#768;re analyse globale de ces re&#769;sultats indique que le filtrage par les groupes syn-
taxiques ne change pas les performances de fac&#807;on tre&#768;s significative. Ce re&#769;sultat est en lui-me&#770;me
inte&#769;ressant car il montre que, malgre&#769; une re&#769;duction de l&#8217;information de co-occurrence de l&#8217;ordre
de un quart a&#768; un tiers, le syste&#768;me ne subit aucune de&#769;gradation significative des performances.
Il semble donc que la me&#769;thode de filtrage choisie est efficace, et permet de n&#8217;e&#769;liminer majo-
ritairement que des co-occurrences qui n&#8217;apportent pas d&#8217;autres d&#8217;informations utiles pour la
repre&#769;sentation que celles de&#769;ja&#768; prises en compte par les co-occurrences conserve&#769;es.
Une analyse plus attentive montre me&#770;me qu&#8217;on observe en fait de le&#769;ge&#768;res ame&#769;liorations (&#20; 5%)
sur les corpus LRSA et INIST, pour lesquelles les coefficients de Wilcoxon indiquent qu&#8217;elles
sont significatives (i.e. elles ne sont pas dues au hasard). Cela montre que les co-occurrences
supprime&#769;es peuvent e&#769;galement correspondre a&#768; du &#0; bruit &#1;, i.e. de l&#8217;information non utile pour
la repre&#769;sentation des documents et dont la suppression permet donc une ame&#769;lioration des perfor-
mances. Notons que, pour le corpus OFIL, les performances sont pluto&#770;t le&#769;ge&#768;rement de&#769;grade&#769;es,
mais les coefficients de Wilcoxon associe&#769;s indiquent que cette de&#769;gradation n&#8217;est souvent pas
significative.
</p>
<p>Se&#769;lection des co-occurrences par les relations syntaxiques Pour cette seconde me&#769;thode, la
production des relations syntaxiques a e&#769;te&#769; re&#769;alise&#769;e a&#768; l&#8217;aide de l&#8217;analyseur syntaxique XeLDA
de Xerox (Xerox, 1990). L&#8217;ensemble des unite&#769;s linguistiques et l&#8217;ensemble des termes d&#8217;in-
dexation sont donc diffe&#769;rents de ceux utilise&#769;s dans les e&#769;valuations pre&#769;ce&#769;dentes. Les principales
caracte&#769;ristiques des donne&#769;es utilise&#769;es sont pre&#769;sente&#769;es dans le tableau 3 ou&#768; l&#8217;on trouve les tailles
des ensembles d&#8217;unite&#769;s linguistiques et celles des matrices de co-occurrences construites sur le
corpus OFIL. &#15;
</p>
<p>&#9;&#16;
</p>
<p>repre&#769;sente la matrice sans filtrage syntaxique et &#15;&#12;&#11;
&#9;&#16;
</p>
<p>la matrice avec se&#769;lection
sur les relations syntaxiques. Le pre&#769;-traitement syntaxique e&#769;tant relativement long, les tests sur
les autres corpus n&#8217;ont pas e&#769;te&#769; effectue&#769;s.
</p>
<p>L&#8217;analyse de ces donne&#769;es indique de fac&#807;on tre&#768;s claire que la se&#769;lection par les relations syn-
taxiques est beaucoup plus restrictive que le filtrage par les groupes syntaxiques. Le nombre de
co-occurrences conserve&#769;es est en effet re&#769;duit de plus de 80%.
</p>
<p>Pour ce qui est des performances, les re&#769;sultats de la recherche obtenus pour le corpus OFIL sont
pre&#769;sente&#769;s dans le tableau 3. L&#8217;analyse de ces re&#769;sultats permet de conclure a&#768; une forte diminution
(statistiquement significative) des performances. Cette diminution est tre&#768;s probablement lie&#769;e a&#768;
une trop forte re&#769;duction de l&#8217;information de co-occurrence utilise&#769;e et peut e&#770;tre e&#769;galement due
a&#768; la qualite&#769; moyenne des relations syntaxiques extraites, qui ne sont pas toutes identiquement
fiables.
</p>
<p>Il appara&#305;&#770;t donc que, si un filtrage syntaxique peut e&#770;tre be&#769;ne&#769;fique, la mise en &#339;uvre de contraintes
de se&#769;lection trop fortes entra&#305;&#770;ne une diminution des performances. Une interpre&#769;tation possible
de cet e&#769;tat de fait peut e&#770;tre que certaines co-occurrences, qui ne reposent pas sur des relations
syntaxiques, peuvent malgre&#769; tout correspondre a&#768; une information se&#769;mantique sous-jacente, et
qu&#8217;un filtrage trop brutal entra&#305;&#770;ne la perte de cette information utile a&#768; la bonne repre&#769;sentation
des documents.
</p>
<p>Le me&#769;canisme de filtrage des co-occurrences doit donc trouver un juste e&#769;quilibre entre la
ne&#769;cessaire e&#769;limination de co-occurrences inutiles ou ge&#769;ne&#769;ratrices de bruit dans la repre&#769;sentation
d&#8217;une part et la conservation de la majorite&#769; de l&#8217;information utile a&#768; la repre&#769;sentation. Si le filtrage
devient trop se&#769;lectif, la matrice de co-occurrence se creuse et devient donc plus discriminante,
mais au-dela&#768; d&#8217;un certain seuil de re&#769;duction, le pouvoir de discrimination accru ne semble plus
</p>
<p>142</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Filtrages syntaxiques de co-occurrences
</p>
<p>taille des lexiques et des matrices de
co-occurrences
</p>
<p>&#0;
</p>
<p>&#9;&#16;
</p>
<p>&#1; &#0; &#1;&#2;&#2; &#0;
</p>
<p>&#12;&#11;
</p>
<p>&#9;&#16;
</p>
<p>&#1; &#0; &#1;&#2;&#2;
</p>
<p>avg p 0.1077 0.0717 (-50.2%)
&#14;
</p>
<p>&#1;
</p>
<p>&#2;&#1;&#15;&#1;&#1;&#0;&#8;
</p>
<p>R p 0.1509 0.1051 (-43.6%)
&#14;
</p>
<p>&#1;
</p>
<p>&#2;&#1;&#15;&#1;&#1;&#10;&#10;
</p>
<p>p5 0.192 0.152 (-26.3%)
&#14;
</p>
<p>&#1;
</p>
<p>&#2;&#1;&#15;&#0;&#0;
</p>
<p>p10 0.188 0.136 (-38.2%)
&#14;
</p>
<p>&#1;
</p>
<p>&#2;&#1;&#15;&#1;&#7;&#5;
</p>
<p>p15 0.1653 0.1227 (-34.7%)
&#14;
</p>
<p>&#1;
</p>
<p>&#2;&#1;&#15;&#1;&#3;
</p>
<p>p20 0.154 0.108 (-42.6%)
&#14;
</p>
<p>&#1;
</p>
<p>&#2;&#1;&#15;&#1;&#1;&#6;&#3;
</p>
<p>p30 0.1293 0.0987 (-31%)
&#14;
</p>
<p>&#1;
</p>
<p>&#2;&#1;&#15;&#1;&#0;&#6;
</p>
<p>p100 0.0684 0.052 (-31.5%)
&#14;
</p>
<p>&#1;
</p>
<p>&#2;&#1;&#15;&#1;&#1;&#0;
</p>
<p>p200 0.0426 0.0324 (-31.5%)
&#14;
</p>
<p>&#1;
</p>
<p>&#2;&#1;&#15;&#1;&#1;&#1;&#3;
</p>
<p>p500 0.0179 0.0141 (-27%)
&#14;
</p>
<p>&#1;
</p>
<p>&#2;&#1;&#15;&#1;&#1;&#1;&#0;&#5;
</p>
<p>p1000 0.009 0.007 (-28.6%)
&#14;
</p>
<p>&#1;
</p>
<p>&#2;&#1;&#15;&#1;&#1;&#1;&#3;
</p>
<p>relret 224 176 (-27.3%)
&#14;
</p>
<p>&#1;
</p>
<p>&#2;&#1;&#15;&#1;&#1;&#1;&#0;&#5;
</p>
<p>r1000 0.3875 0.3045 (-27.3%)
&#14;
</p>
<p>&#1;
</p>
<p>&#2;&#1;&#15;&#1;&#1;&#1;&#7;&#4;
</p>
<p>OFIL
unite&#769;s
</p>
<p>&#2;&#12; &#2; 20158
&#2;&#13; &#2; 1992
</p>
<p>matrice &#15;
&#9;&#16;
</p>
<p>taille 17 Mo
&#4;
</p>
<p>&#12;
</p>
<p>10.6%
matrice &#15;&#12;&#11;
</p>
<p>&#9;&#16;
</p>
<p>taille 3 Mo
&#4;
</p>
<p>&#12;
</p>
<p>2.0%
&#4;
</p>
<p>&#7;
</p>
<p>81.1%
</p>
<p>TAB. 3 &#8211; Re&#769;sultats sur le corpus OFIL pour la se&#769;lection des co-occurrences par les relations
syntaxiques
</p>
<p>contrebalancer la perte d&#8217;information. Dans nos expe&#769;riences, le filtrage des co-occurrences par
les groupes syntaxiques semble constituer un compromis efficace alors que la se&#769;lection des
co-occurrences par les relations syntaxiques privile&#769;gie trop la re&#769;duction d&#8217;information et me&#768;ne
donc a&#768; une de&#769;gradation des performances.
</p>
<p>5 Conclusion
</p>
<p>Nous avons pre&#769;sente&#769; dans cet article l&#8217;e&#769;tude de deux me&#769;thodes de filtrage syntaxique pour le
calcul des co-occurrences prises en compte dans une repre&#769;sentation vectorielle distributionnelle
des documents : d&#8217;une part, une me&#769;thode prenant en compte un filtrage reposant sur les groupes
syntaxiques, et d&#8217;autre part une me&#769;thode prenant en compte un me&#769;canisme de se&#769;lection fonde&#769;
sur les relations syntaxiques. Ces deux me&#769;thodes ont e&#769;te&#769; teste&#769;es dans le cadre de la recherche
documentaire et ont montre&#769; que le filtrage du premier type permet non seulement d&#8217;e&#769;liminer
un nombre important de co-occurrences dont la disparition n&#8217;alte&#768;re pas les performances, mais
e&#769;galement de filtrer des co-occurrences qui introduisent du bruit dans la repre&#769;sentation et dont
l&#8217;e&#769;limination est donc be&#769;ne&#769;fique. Le filtrage du deuxie&#768;me type semble quant a&#768; lui trop restrictif
et de&#769;te&#769;riore de fac&#807;on significative les performances du syste&#768;me.
</p>
<p>Du fait du faible gain en performance observe&#769; dans nos expe&#769;riences, il appara&#305;&#770;t que les in-
formations sur les groupes syntaxiques permettent essentiellement de re&#769;duire le volume des
donne&#769;es de co-occurrences a&#768; manipuler sans de&#769;gradation des re&#769;sultats. Pour ce qui est des
relations syntaxiques, cette information nous para&#305;&#770;t inte&#769;ressante malgre&#769; les re&#769;sultats ne&#769;gatifs
observe&#769;s lors de nos expe&#769;riences et d&#8217;autres me&#769;thodes d&#8217;inte&#769;gration devraient e&#770;tre envisage&#769;es
pour ces donne&#769;es. Une piste de recherche inte&#769;ressante pourrait par exemple e&#770;tre de typer les co-
occurrences, de fac&#807;on par exemple a&#768; pre&#769;server dans la repre&#769;sentation la distinction entre les co-
occurrences de type sujet-verbe ou verbe-objet, ou entre les co-occurrences de type te&#770;te-te&#770;te ou
te&#770;te-de&#769;pendance. Une autre piste pourrait e&#770;tre d&#8217;e&#769;tudier l&#8217;apport de repre&#769;sentations se&#769;mantiques
du type pre&#769;dicat-argument pour la se&#769;lection des co-occurrences (cela permettrait en particulier
</p>
<p>143</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>R. Besanc&#807;on, M. Rajman
</p>
<p>de retrouver les co-occurrences sujet-objet a&#768; travers un pre&#769;dicat verbal).
D&#8217;autre part, les re&#769;sultats pre&#769;sente&#769;s restent des re&#769;sultats quantitatifs globaux et une e&#769;tude qua-
litative plus fine serait ne&#769;cessaire pour permettre de confirmer les intuitions de&#769;gage&#769;es de cette
premie&#768;re e&#769;valuation.
</p>
<p>Re&#769;fe&#769;rences
BESANC&#807;ON R. (2001). Inte&#769;gration de connaissances syntaxiques et s&#769;emantiques dans les
repre&#769;sentations vectorielles de textes. PhD thesis, &#180;Ecole Polytechnique Fe&#769;de&#769;rale de Lausanne.
BESANC&#807;ON R., RAJMAN M. &amp; CHAPPELIER J.-C. (1999). Textual similarities based on a distribu-
tional approach. In Proceedings of the Tenth International Workshop on Database and Expert Systems
Applications (DEXA&#8217;99), p. 180&#8211;184, Firenze (Italy).
CONSTANT P. (1995). Manuel de de&#769;veloppement SYLEX-BASE. INGE&#769;NIA-LN, Paris, France.
CORET A., KREMER P., LANDI B., SCHIBLER D. &amp; SCHMITT L. (1997). Towards a methodology
for evaluating information retrieval systemsadapted to textual documents in the french language : the
amaryllis exploratory cycle. In SALT Workshop on Evaluation in Speech and Language Technology,
Sheffield, UK.
LANDI B., KREMER P. &amp; SCHMITT L. (1998). Amaryllis : an evaluation experiment on search engine in
a french-speaking context. In Proceedings of the First International Conference on Language Resources
and Evaluation (LREC), Granada, Spain.
RAJMAN M. (1995). Apports d&#8217;une approche a&#768; base de corpus aux techniques de traitement automa-
tique de langage naturel. PhD thesis, ENST, Paris.
RAJMAN M., BESANC&#807;ON R. &amp; CHAPPELIER J.-C. (2000). Le mode&#768;le DSIR : Une approche a&#768; base
de se&#769;mantique distributionnelle pour la recherche documentaire. Traitement Automatique des Langues,
41(2), 549&#8211;578.
RAJMAN M. &amp; BONNET A. (1992). Corpora-base linguistics : new tools for natural language proces-
sing. In 1st Annual Conference of the Association for Global Strategic Information, Bad Kreuznach,
Germany.
RUNGSAWANG A. (1997). Recherche Documentaire a&#768; base de se&#769;mantique distributionnelle. PhD thesis,
ENST, Paris.
G. SALTON, Ed. (1971). The SMART Retrieval System &#8211; Experiments in Automatic Document Proces-
sing. Prentice Hall.
SALTON G. &amp; BUCKLEY C. (1988). Term weighting approaches in automatic text retrieval. Information
Processing and Management, 24, 513&#8211;523.
SALTON G. &amp; MCGILL M. (1983). Introduction to Modern Information Retrieval. McGraw Hill.
SALTON G., WONG A. &amp; YANG C. S. (1975). A vector space model for automatic indexing. Commu-
nications of the ACM, 18(11), 613&#8211;620.
SINGHAL A. (1997). Term Weighting Revisited. PhD thesis, Department of Computer Science, Cornell
University.
VAN RIJSBERGEN C. (1979). Information Retrieval. London : Buttherwords.
XEROX (1990). Xelda : Xerox linguistic development architecture.
http ://www.xrce.xerox.com/ats/xelda/.
</p>
<p>144</p>

</div></div>
</body></html>