<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
"http://www.w3.org/TR/html4/loose.dtd">
<html><head><title>WSIM : une m&#233;thode de d&#233;tection de th&#232;me fond&#233;e sur la similarit&#233; entre mots</title>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
</head>
<body>
<div style="page-break-before:always; page-break-after:always"><div><p>TALN 2002, Nancy, 24&#8211;27 juin 2002
</p>
<p>WSIM : une m&#233;thode de d&#233;tection de th&#232;me fond&#233;e sur la
similarit&#233; entre mots
</p>
<p>Armelle BRUN, Kamel SMAILI, Jean-Paul HATON
LORIA BP 239 54506 Vand&#339;uvre-L&#232;s-Nancy, France -
Tel : (33|0) 3-83-59-20-97, Fax :(33|0) 3-83-41-30-79
</p>
<p>{brun, smaili, jph}@loria.fr
</p>
<p>R&#233;sum&#233; - Abstract
</p>
<p>L&#8217;adaptation des mod&#232;les de langage dans les syst&#232;mes de reconnaissance de la parole est un
des enjeux importants de ces derni&#232;res ann&#233;es. Elle permet de poursuivre la reconnaissance en
utilisant le mod&#232;le de langage ad&#233;quat : celui correspondant au th&#232;me identifi&#233;.
Dans cet article nous proposons une m&#233;thode originale de d&#233;tection de th&#232;me fond&#233;e sur des
vocabulaires caract&#233;ristiques de th&#232;mes et sur la similarit&#233; entre mots et th&#232;mes. Cette m&#233;thode
d&#233;passe la m&#233;thode classique (TFIDF) de 14%, ce qui repr&#233;sente un gain important en terme
d&#8217;identification. Nous montrons &#233;galement l&#8217;int&#233;r&#234;t de choisir un vocabulaire ad&#233;quat. Notre
m&#233;thode de d&#233;termination des vocabulaires atteint des performances 3 fois sup&#233;rieures &#224; celles
obtenues avec des vocabulaires construits sur la fr&#233;quence des mots.
</p>
<p>Speech recognition systems benefit from statistical language model adaptation, which is cur-
rently one of the most important challenge. This adaptation may go through the use of a parti-
cular language model : the one of the topic identified. In this article, a new and original topic
identification method is presented, it is based on the similarity between words and topics. The
performance of this method overcomes the one of reference, the TFIDF. The increase is 14% of
topic identification.
The importance of the choice of topic vocabularies is also put forward. A judicious way to
create them, instead of choosing the most probable words, makes performance triple.
</p>
<p>Mots-clefs &#8211; Keywords
</p>
<p>Reconnaissance de la parole, mod&#233;lisation statistique du langage, d&#233;tection de th&#232;me, informa-
tion mutuelle, similarit&#233;.
Automatic speech recognition, statistical language modeling, topic detection, mutual informa-
tion, similarity.
</p>
<p>145</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>A. Brun, K. Sma&#239;li, J.P. Haton
</p>
<p>1 Introduction
</p>
<p>Les mod&#232;les de langage (MLs) sont utilis&#233;s dans de nombreux domaines comme la reconnais-
sance de la parole, la traduction automatique, la recherche d&#8217;informations, la reconnaissance
de l&#8217;&#233;criture, etc. Les performances d&#8217;un syst&#232;me de reconnaissance automatique de la parole,
notamment, sont fortement d&#233;pendantes des mod&#232;les de langage.
Un ML a pour but de mod&#233;liser le comportement de la langue. Ainsi, un ML statistique la repr&#233;-
sente sous la forme d&#8217;une distribution de probabilit&#233;s de s&#233;quences de mots. Dans le cas d&#8217;un
syst&#232;me de reconnaissance de la parole, le score fourni par le module acoustique, qui repr&#233;sente
la correspondance entre le signal et une suite de mots donn&#233;e, est combin&#233; avec celui fourni par
le mod&#232;le de langage, qui repr&#233;sente la vraisemblance de cette m&#234;me s&#233;quence. La s&#233;quence de
mots correspondant au meilleur score sera celle retenue par le syst&#232;me.
</p>
<p>Les mod&#232;les de langage les plus utilis&#233;s sont les mod&#232;les de type &#0;-grammes, qui &#233;valuent la
probabilit&#233; d&#8217;un mot sachant les &#0; &#0; &#0; mots pr&#233;c&#233;dents. Le principal avantage de ces mod&#232;les
r&#233;side dans leur simplicit&#233; de mise en &#339;uvre. Cependant, lors de leur construction, on est sou-
vent confront&#233; &#224; des probl&#232;mes de manque de donn&#233;es, r&#233;solus par l&#8217;utilisation de m&#233;thodes de
lissage (cf (Chen &amp; Goodman, 1996) pour une synth&#232;se de ces m&#233;thodes). Plus la valeur de &#0;
est &#233;lev&#233;e, plus le probl&#232;me du manque de donn&#233;es est important. Par cons&#233;quent, en pratique,
la valeur de &#0; exc&#232;de rarement &#1; (mod&#232;le trigrammes). Cependant, il est &#233;vident que la quantit&#233;
d&#8217;information prise en compte par ces mod&#232;les pour &#233;valuer la probabilit&#233; d&#8217;un mot est large-
ment inf&#233;rieure &#224; celle qui joue effectivement un r&#244;le lors de la pr&#233;diction d&#8217;un mot. Pour cette
raison, de nombreux travaux ont &#233;t&#233; men&#233;s dans le but d&#8217;augmenter la taille de l&#8217;historique pris
en compte : (Kuhn &amp; De Mori, 1990) int&#232;grent un cache au mod&#232;le de langage, ce qui a pour
cons&#233;quence d&#8217;augmenter la probabilit&#233; des mots d&#233;j&#224; apparus dans l&#8217;historique. Dans le m&#234;me
ordre d&#8217;id&#233;es, (Rosenfeld, 1996) y int&#232;gre des triggers de mots. R&#233;cemment, (Chelba &amp; Jelinek,
2000) ont d&#233;velopp&#233; un mod&#232;le de langage qui combine un mod&#232;le &#0;-grammes, un analyseur
et un &#233;tiqueteur, permettant ainsi d&#8217;exploiter des mots apparaissant tr&#232;s loin dans l&#8217;historique.
Dans notre cas, nous travaillons dans l&#8217;hypoth&#232;se que le langage, ou plus exactement son vo-
cabulaire caract&#233;ristique, varie en fonction du th&#232;me trait&#233;. Il est donc utile, toujours dans le
but d&#8217;augmenter l&#8217;information prise en compte pour pr&#233;dire un mot, de chercher &#224; conna&#238;tre le
th&#232;me d&#8217;un texte pour ensuite adapter le ML &#224; ce th&#232;me.
Nous nous int&#233;ressons tout particuli&#232;rement &#224; cette phase de recherche du th&#232;me d&#8217;un texte.
Dans cet article, nous proposons une nouvelle m&#233;thode de d&#233;tection de th&#232;mes, WSIM (Word
SIMilarity), fond&#233;e non seulement sur la probabilit&#233; des mots dans les th&#232;mes, unique infor-
mation habituellement exploit&#233;e, mais &#233;galement sur la similarit&#233; des mots avec les th&#232;mes et
l&#8217;utilisation de vocabulaires caract&#233;ristiques.
</p>
<p>La section 2 explique en quoi les t&#226;ches de cat&#233;gorisation de textes et d&#233;tection de th&#232;mes sont
similaires puis pr&#233;sente un &#233;tat de l&#8217;art des m&#233;thodes de cat&#233;gorisation de textes. Nous introdui-
sons, en section 3, les principes de notre m&#233;thode de d&#233;tection de th&#232;mes, dont les performances
seront &#233;tudi&#233;es, et compar&#233;es &#224; d&#8217;autres m&#233;thodes, en section 4. Enfin, nous conclurons et pr&#233;-
senterons quelques perspectives.
</p>
<p>146</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>WSIM : une m&#233;thode de d&#233;tection de th&#232;me fond&#233;e sur la similarit&#233; entre mots
</p>
<p>2 La d&#233;tection de th&#232;mes
</p>
<p>2.1 D&#233;finition
</p>
<p>Soit un document &#1;
&#0;
</p>
<p>et &#2; &#2; &#1;&#3;
&#0;
</p>
<p>&#4; &#5; &#5; &#5; &#4; &#3;
</p>
<p>&#1;
</p>
<p>&#2; un ensemble de classes. La cat&#233;gorisation de textes
est la t&#226;che qui consiste &#224; assigner une ou plusieurs classes &#224; &#1;
</p>
<p>&#0;
</p>
<p>. Pour cela, nous disposons
d&#8217;un corpus dit &#8220;d&#8217;apprentissage&#8221; compos&#233; d&#8217;un ensemble de documents dont la(es) classe(s)
d&#8217;appartenance sont connues. Le syst&#232;me de d&#233;tection de th&#232;me est ensuite entra&#238;n&#233; sur cet en-
semble d&#8217;apprentissage, dans le but de correctement cat&#233;goriser un nouveau document. Dans
notre cas, une classe peut &#234;tre assimil&#233;e &#224; un th&#232;me, l&#8217;objectif &#233;tant de retrouver le th&#232;me &#3;
</p>
<p>&#2;
</p>
<p>du
document &#1;
</p>
<p>&#0;
</p>
<p>.
</p>
<p>2.2 Les travaux en cat&#233;gorisation
</p>
<p>Les m&#233;thodes classiques de cat&#233;gorisation exploitent l&#8217;information contenue dans le document
pour d&#233;terminer sa classe. Dans la majorit&#233; des cas, ce sont les mots qui sont utilis&#233;s pour
repr&#233;senter cette information.
</p>
<p>Le probl&#232;me de la cat&#233;gorisation de textes a &#233;t&#233; largement &#233;tudi&#233;, nous en pr&#233;sentons ici plu-
sieurs grandes approches, parmi les plus utilis&#233;es.
Le document est tout d&#8217;abord transform&#233; sous la forme d&#8217;un vecteur o&#249; chaque &#233;l&#233;ment re-
pr&#233;sente &#8220;grossi&#232;rement&#8221; le poids d&#8217;un mot dans le document. Dans de rares cas, comme par
exemple les arbres de d&#233;cision binaires, ce vecteur contient des valeurs bool&#233;ennes, repr&#233;sen-
tant la pr&#233;sence ou non du mot dans le document : &#0; si le mot est pr&#233;sent, &#3; sinon, voir (Lewis
&amp; Ringuette, 1994).
Certaines m&#233;thodes sont fond&#233;es sur une approche probabiliste, elles &#233;valuent la probabilit&#233; de
chaque classe sachant le document donn&#233;. Le mod&#232;le unigramme th&#233;matique est l&#8217;exemple le
plus connu de ces classifieurs (Mc Donough &amp; Ng, 1994).
On peut &#224; nouveau citer les arbres de d&#233;cision (Mitchell, 1996). Chaque n&#339;ud repr&#233;sente un
terme et chaque branche un test sur la fr&#233;quence de ce terme dans le document. Enfin les feuilles
repr&#233;sentent une classe. La classe affect&#233;e au document est celle qui correspond &#224; la feuille ob-
tenue par parcours de l&#8217;arbre.
Dans l&#8217;approche par r&#233;seaux de neurones (Dagan et al., 1997), le document &#224; classer est pr&#233;-
sent&#233; &#224; l&#8217;entr&#233;e du r&#233;seau. La couche de sortie, quant &#224; elle, repr&#233;sente l&#8217;ensemble des classes.
Apr&#232;s activation du r&#233;seau, les valeurs de la couche de sortie repr&#233;sentent les classes possibles
du document.
Enfin les Machines &#224; Vecteur Support (SVMs), sont une famille de classifieurs qui minimise une
borne sup&#233;rieure sur l&#8217;erreur de g&#233;n&#233;ralisation. Elles sont fond&#233;es sur la s&#233;paration de donn&#233;es
par hyperplan. Elles ont &#233;t&#233; appliqu&#233;es &#224; la cat&#233;gorisation de textes dans (Joachims, 1998).
Les approches pr&#233;sent&#233;es ci-dessus ne sont pas exhaustives, on peut &#233;galement citer les classi-
fieurs &#224; base de r&#232;gles de d&#233;cision (Apt&#233; et al., 1994), &#224; base de r&#233;gression avec notamment le
mod&#232;le LLSF (Yang &amp; Chute, 1994), la m&#233;thode Rocchio (Joachims, 1997) avec tout particu-
li&#232;rement la TFIDF (Salton, 1991; Seymore &amp; Rosenfeld, 1997), etc.
Certaines &#233;tudes ont &#233;galement &#233;t&#233; men&#233;es en vue de l&#8217;exploitation d&#8217;informations de plus haut
niveau que le mot. Ainsi, (Lewis, 1992) int&#232;gre des s&#233;quences de mots extraites en accord
avec une grammaire, et (Caropreso et al., 2001) des s&#233;quences de mots de nature purement
</p>
<p>147</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>A. Brun, K. Sma&#239;li, J.P. Haton
</p>
<p>statistique. Les deux approches n&#8217;ont montr&#233; aucune am&#233;lioration des performances.
</p>
<p>3 Description de WSIM
</p>
<p>Dans cet article, nous proposons une nouvelle m&#233;thode de d&#233;tection de th&#232;me, WSIM. Nous
pouvons la classer dans la famille des m&#233;thodes probabilistes. Chaque th&#232;me est repr&#233;sent&#233;
par un vecteur, o&#249; chaque &#233;l&#233;ment repr&#233;sente un mot. Contrairement aux m&#233;thodes classiques
comme la TFIDF (Seymore &amp; Rosenfeld, 1997), les &#233;l&#233;ments du vecteur ne repr&#233;sentent pas
uniquement le poids du mot dans le th&#232;me, celui-ci est combin&#233; avec leur &#8220;similarit&#233;&#8221;. La simi-
larit&#233; entre un mot &#6; et un th&#232;me &#7;
</p>
<p>&#3;
</p>
<p>est fond&#233;e sur la similarit&#233; entre ce mot et l&#8217;ensemble des
mots caract&#233;ristiques du th&#232;me &#7;
</p>
<p>&#3;
</p>
<p>.
</p>
<p>3.1 La mesure de similarit&#233; entre deux mots
</p>
<p>(Dagan et al., 1999) introduit une mesure de similarit&#233; entre 2 mots &#6; et &#8;, &#233;valu&#233;e en se basant
sur leurs comportements respectifs en contexte (droit et gauche). Plus pr&#233;cis&#233;ment, deux mots
sont consid&#233;r&#233;s comme similaires si leurs informations mutuelles avec l&#8217;ensemble des autres
mots du vocabulaire sont proches. Cette similarit&#233; est &#233;valu&#233;e de la mani&#232;re suivante :
</p>
<p>&#9;&#10;&#11;&#10;&#12;&#13;&#14;&#10;&#15;&#16;&#4;&#6;&#4; &#8;&#5; &#2;
</p>
<p>&#0;
</p>
<p>&#6;&#17;
</p>
<p>&#0;&#4; &#0;
</p>
<p>&#0;&#1;&#0;
</p>
<p>&#11;&#10;&#0; &#4;&#18;&#4;&#19;
</p>
<p>&#0;
</p>
<p>&#4; &#6;&#5;&#4; &#18;&#4;&#19;
</p>
<p>&#0;
</p>
<p>&#4; &#8;&#5;&#5;
</p>
<p>&#11;&#13;&#6; &#4;&#18;&#4;&#19;
</p>
<p>&#0;
</p>
<p>&#4; &#6;&#5;&#4; &#18;&#4;&#19;
</p>
<p>&#0;
</p>
<p>&#4; &#8;&#5;&#5;
</p>
<p>&#7;
</p>
<p>&#11;&#10;&#0; &#4;&#18;&#4;&#6;&#4; &#19;
</p>
<p>&#0;
</p>
<p>&#5;&#4; &#18;&#4;&#8;&#4; &#19;
</p>
<p>&#0;
</p>
<p>&#5;&#5;
</p>
<p>&#11;&#13;&#6; &#4;&#18;&#4;&#6;&#4; &#19;
</p>
<p>&#0;
</p>
<p>&#5;&#4; &#18;&#4;&#8;&#4; &#19;
</p>
<p>&#0;
</p>
<p>&#5;&#5;
</p>
<p>(1)
</p>
<p>O&#249; &#17; est le vocabulaire et &#18;&#4;&#19;
&#0;
</p>
<p>&#4; &#6;&#5; est l&#8217;information mutuelle entre les mots &#19;
&#0;
</p>
<p>et &#6;.
Cette mesure a &#233;t&#233; initialement d&#233;velopp&#233;e dans le but d&#8217;estimer la probabilit&#233; de cooccurrences
de mots, non observ&#233;es &#224; l&#8217;apprentissage. Nous avons adopt&#233; cette mesure pour d&#233;velopper
une m&#233;thode permettant d&#8217;identifier le th&#232;me d&#8217;un document. Cette m&#233;thode est fond&#233;e sur
l&#8217;information mutuelle &#18; calcul&#233;e sur une fen&#234;tre glissante de &#1; mots, la nature de la similarit&#233;
est donc plus s&#233;mantique que syntaxique. &#18;&#4;&#19;
</p>
<p>&#0;
</p>
<p>&#4; &#6;&#5; est &#233;valu&#233;e de la mani&#232;re suivante :
</p>
<p>&#18;&#4;&#19;
</p>
<p>&#0;
</p>
<p>&#4; &#6;&#5; &#2; &#20;
</p>
<p>&#5;
</p>
<p>&#4;&#19;
</p>
<p>&#0;
</p>
<p>&#4; &#6;&#5; &#8;&#9;&#10;
</p>
<p>&#20;
</p>
<p>&#5;
</p>
<p>&#4;&#19;
</p>
<p>&#0;
</p>
<p>&#4; &#6;&#5;
</p>
<p>&#1;
</p>
<p>&#2;
</p>
<p>&#5;&#20; &#4;&#19;
</p>
<p>&#0;
</p>
<p>&#5;&#20; &#4;&#6;&#5;
</p>
<p>o&#249; &#1; repr&#233;sente la distance ou la taille de la fen&#234;tre glissante. &#20;
&#5;
</p>
<p>&#4;&#19;
</p>
<p>&#0;
</p>
<p>&#4; &#6;&#5; est la probabilit&#233; de
succession des mots &#19;
</p>
<p>&#0;
</p>
<p>et &#6; &#224; une distance au plus &#1;. &#20; &#4;&#6;&#5; repr&#233;sente la probabilit&#233; a priori du
mot &#6;.
Toujours dans un but de d&#233;tection de th&#232;me, nous ne cherchons pas &#224; conna&#238;tre la similarit&#233;
entre deux mots dans le langage, mais dans un th&#232;me donn&#233;. Par cons&#233;quent, la similarit&#233; entre
deux mots, pour le th&#232;me &#7;
</p>
<p>&#3;
</p>
<p>, sera &#233;valu&#233;e comme suit :
</p>
<p>&#9;&#10;&#11;&#10;&#12;&#13;&#14;&#10;&#15;&#16;
</p>
<p>&#3;
</p>
<p>&#4;&#6;&#4; &#8;&#5; &#2;
</p>
<p>&#0;
</p>
<p>&#6;&#12;
</p>
<p>&#3;
</p>
<p>&#0;&#6;
</p>
<p>&#0;
</p>
<p>&#0;
</p>
<p>&#0;&#1;&#0;
</p>
<p>&#11;&#10;&#0; &#4;&#18;
</p>
<p>&#3;
</p>
<p>&#4;&#19;
</p>
<p>&#0;
</p>
<p>&#4; &#6;&#5;&#4; &#18;
</p>
<p>&#3;
</p>
<p>&#4;&#19;
</p>
<p>&#0;
</p>
<p>&#4; &#8;&#5;&#5;
</p>
<p>&#11;&#13;&#6; &#4;&#18;
</p>
<p>&#3;
</p>
<p>&#4;&#19;
</p>
<p>&#0;
</p>
<p>&#4; &#6;&#5;&#4; &#18;
</p>
<p>&#3;
</p>
<p>&#4;&#19;
</p>
<p>&#0;
</p>
<p>&#4; &#8;&#5;&#5;
</p>
<p>&#7;
</p>
<p>&#11;&#10;&#0; &#4;&#18;
</p>
<p>&#3;
</p>
<p>&#4;&#6;&#4; &#19;
</p>
<p>&#0;
</p>
<p>&#5;&#4; &#18;
</p>
<p>&#3;
</p>
<p>&#4;&#8;&#4; &#19;
</p>
<p>&#0;
</p>
<p>&#5;&#5;
</p>
<p>&#11;&#13;&#6; &#4;&#18;
</p>
<p>&#3;
</p>
<p>&#4;&#6;&#4; &#19;
</p>
<p>&#0;
</p>
<p>&#5;&#4; &#18;
</p>
<p>&#3;
</p>
<p>&#4;&#8;&#4; &#19;
</p>
<p>&#0;
</p>
<p>&#5;&#5;
</p>
<p>(2)
</p>
<p>o&#249; &#12;
&#3;
</p>
<p>est le vocabulaire du th&#232;me &#7;
&#3;
</p>
<p>, et &#18;
&#3;
</p>
<p>&#4;&#19;
</p>
<p>&#0;
</p>
<p>&#4; &#6;&#5; est &#233;valu&#233;e sur le corpus d&#8217;apprentissage du
th&#232;me &#7;
</p>
<p>&#3;
</p>
<p>.
</p>
<p>148</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>WSIM : une m&#233;thode de d&#233;tection de th&#232;me fond&#233;e sur la similarit&#233; entre mots
</p>
<p>TAB. 1 &#8211; Label des th&#232;mes &#233;tudi&#233;s et leur taille d&#8217;apprentissage
</p>
<p>Th&#232;me Nombre de mots d&#8217;apprentissage Th&#232;me Nombre de mots d&#8217;apprentissage
Culture 25 M Politique 13 M
&#201;conomie 21 M Sciences 2 M
&#201;tranger 24 M Sports 170 K
Histoire 560 K
</p>
<p>3.2 La similarit&#233; entre un mot et un th&#232;me
</p>
<p>Soit &#17;
&#3;
</p>
<p>&#2; &#21;
</p>
<p>&#3;&#7;
</p>
<p>&#0;
</p>
<p>&#4; &#21;
</p>
<p>&#3;&#7;
</p>
<p>&#1;
</p>
<p>&#4; &#5; &#5; &#5; &#4; &#21;
</p>
<p>&#3;&#7;
</p>
<p>&#0;&#1;
</p>
<p>&#0;
</p>
<p>&#0;
</p>
<p>le vecteur repr&#233;sentant le th&#232;me &#7;
&#3;
</p>
<p>. Chaque &#233;l&#233;ment du vecteur
repr&#233;sente la similarit&#233; entre un mot et le th&#232;me.
Nous proposons d&#8217;estimer la similarit&#233; entre le mot &#6; et le th&#232;me &#7;
</p>
<p>&#3;
</p>
<p>comme &#233;tant la moyenne
des similarit&#233;s entre le mot &#6; et les mots du vocabulaire caract&#233;ristique de &#7;
</p>
<p>&#3;
</p>
<p>. Cette derni&#232;re est
ensuite pond&#233;r&#233;e par la probabilit&#233; du mot dans le th&#232;me :
</p>
<p>&#21;
</p>
<p>&#3;&#7;
</p>
<p>&#2; &#9;&#10;&#11;&#4;&#6;&#4; &#7;
</p>
<p>&#3;
</p>
<p>&#5; &#2; &#20; &#4;&#6; &#3; &#7;
</p>
<p>&#3;
</p>
<p>&#5;
</p>
<p>&#1;
</p>
<p>&#0;&#6;
</p>
<p>&#0;
</p>
<p>&#0;
</p>
<p>&#2;&#1;&#0;
</p>
<p>&#9;&#10;&#11;&#10;&#12;&#13;&#14;&#10;&#15;&#16;
</p>
<p>&#3;
</p>
<p>&#4;&#6;&#4; &#8;
</p>
<p>&#2;
</p>
<p>&#5;
</p>
<p>&#1;
</p>
<p>&#0;&#6;
</p>
<p>&#0;
</p>
<p>&#0;
</p>
<p>&#7;&#1;&#0;
</p>
<p>&#1;
</p>
<p>&#0;&#6;
</p>
<p>&#0;
</p>
<p>&#0;
</p>
<p>&#2;&#1;&#0;
</p>
<p>&#9;&#10;&#11;&#10;&#12;&#13;&#14;&#10;&#15;&#16;
</p>
<p>&#3;
</p>
<p>&#4;&#6;&#4; &#8;
</p>
<p>&#2;
</p>
<p>&#5;
</p>
<p>(3)
</p>
<p>3.3 La d&#233;termination du th&#232;me d&#8217;un document
</p>
<p>En phase de test, pour chaque th&#232;me &#7;
&#3;
</p>
<p>&#4;&#22; &#4; &#0;&#5;&#5;&#23;&#5;, nous disposons d&#8217;un vecteur &#17;
&#3;
</p>
<p>. Le score de
chaque th&#232;me sachant le document de test &#1; compos&#233; de &#24; mots &#1; &#2; &#25;
</p>
<p>&#0;
</p>
<p>&#4; &#25;
</p>
<p>&#2;
</p>
<p>&#4; &#5; &#5; &#5; &#4; &#25;
</p>
<p>&#8;
</p>
<p>est &#233;valu&#233;
comme &#233;tant la somme normalis&#233;e des similarit&#233;s entre les mots du document et le th&#232;me :
</p>
<p>&#20; &#4;&#7;
</p>
<p>&#3;
</p>
<p>&#3; &#1;&#5; &#2; &#26;
</p>
<p>&#3;
</p>
<p>&#1;
</p>
<p>&#8;
</p>
<p>&#0;&#1;&#0;
</p>
<p>&#21;
</p>
<p>&#3;&#9;
</p>
<p>&#2;
</p>
<p>&#1;
</p>
<p>&#1;
</p>
<p>&#2;&#1;&#0;
</p>
<p>&#1;
</p>
<p>&#8;
</p>
<p>&#0;&#1;&#0;
</p>
<p>&#21;
</p>
<p>&#2;&#9;
</p>
<p>&#2;
</p>
<p>&#5;
</p>
<p>&#8;
</p>
<p>&#0;
</p>
<p>&#0;&#1;&#0;
</p>
<p>&#198;
</p>
<p>&#0;&#3;
</p>
<p>(4)
</p>
<p>avec &#198;
&#0;&#3;
</p>
<p>&#2;
</p>
<p>&#2;
</p>
<p>&#0; si &#25;
&#0;
</p>
<p>&#4; &#12;
</p>
<p>&#3;
</p>
<p>&#3; sinon et
&#1;
</p>
<p>&#8;
</p>
<p>&#0;&#1;&#0;
</p>
<p>&#198;
</p>
<p>&#0;&#3;
</p>
<p>repr&#233;sente le nombre de mots de &#12;
&#3;
</p>
<p>dans &#1;, &#26;
&#3;
</p>
<p>est un
</p>
<p>coefficient de pond&#233;ration th&#233;matique avec &#1;&#1;
&#3;&#1;&#0;
</p>
<p>&#26;
</p>
<p>&#3;
</p>
<p>&#2; &#0;. Les valeurs de &#26;
&#3;
</p>
<p>sont d&#233;termin&#233;es
par validation crois&#233;e, sur un corpus d&#8217;optimisation. Finalement, le th&#232;me retenu est celui qui
maximise (4).
</p>
<p>4 Exp&#233;rimentations
</p>
<p>4.1 Les donn&#233;es
</p>
<p>Les exp&#233;riences de d&#233;tection de th&#232;mes sont &#233;valu&#233;es sur un corpus issu du journal Le Monde,
des ann&#233;es 1987 &#224; 1991 (plus de 80 M mots). Ce corpus est divis&#233; en 7 th&#232;mes, in&#233;galement
repr&#233;sent&#233;s. La liste des th&#232;mes ainsi que leur taille d&#8217;apprentissage sont pr&#233;sent&#233;es TAB. 1. Ce
corpus est disponible sous forme d&#8217;articles. Cependant, &#224; l&#8217;int&#233;rieur d&#8217;un m&#234;me article, on peut
&#234;tre confront&#233; &#224; des changements de th&#232;mes, probl&#232;me auquel nous ne nous int&#233;ressons pas. Par
</p>
<p>149</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>A. Brun, K. Sma&#239;li, J.P. Haton
</p>
<p>cons&#233;quent, nous avons extrait al&#233;atoirement &#11;&#1;&#12; paragraphes, que nous consid&#233;rons ne traiter
que d&#8217;un seul th&#232;me, et qui forment le corpus de test.
</p>
<p>4.2 Construction du vocabulaire
</p>
<p>Dans l&#8217;&#233;quation (2), la similarit&#233; entre deux mots &#6; et &#8; pour le th&#232;me &#7;
&#3;
</p>
<p>se calcule sur le vo-
cabulaire du th&#232;me, qu&#8217;il nous faut donc construire. Comme le montrent de nombreuses &#233;tudes
(Brun et al., 2000; Mladenic, 1998), le vocabulaire d&#8217;un th&#232;me constitue le noyau de base sur
lequel repose toute m&#233;thode d&#8217;identification. Par cons&#233;quent, il est indispensable de ne pas se
contenter des mots les plus fr&#233;quents des th&#232;mes, mais d&#8217;en trouver les termes caract&#233;ristiques.
Nous &#233;tudions ici deux m&#233;thodes de construction des vocabulaires de th&#232;mes : une m&#233;thode
classique et une m&#233;thode adapt&#233;e &#224; la cat&#233;gorisation de textes/d&#233;tection de th&#232;mes.
</p>
<p>4.2.1 Mots les plus fr&#233;quents de chaque th&#232;me
</p>
<p>Le premier ensemble de vocabulaires &#233;tudi&#233; est construit de fa&#231;on tr&#232;s classique, o&#249; chaque vo-
cabulaire de th&#232;me contient les &#0; mots les plus fr&#233;quents (en absolu) du corpus d&#8217;apprentissage
de ce th&#232;me. Les mots outils (non porteurs de sens) ont &#233;videmment &#233;t&#233; supprim&#233;s.
</p>
<p>4.2.2 Information mutuelle mot-th&#232;me
</p>
<p>Le second ensemble de vocabulaires de th&#232;mes est construit d&#8217;une mani&#232;re plus judicieuse. Il
s&#8217;agit d&#8217;&#233;valuer la quantit&#233; d&#8217;information apport&#233;e par la variable &#7; (th&#232;me) &#224; la variable &#27;
(mot). Cette quantit&#233; est mesur&#233;e &#224; l&#8217;aide de l&#8217;information mutuelle :
</p>
<p>&#18;&#4;&#6;&#4; &#7;
</p>
<p>&#3;
</p>
<p>&#5; &#2; &#20; &#4;&#6;&#4; &#7;
</p>
<p>&#3;
</p>
<p>&#5; &#8;&#9;&#10;
</p>
<p>&#20; &#4;&#6;&#4; &#7;
</p>
<p>&#3;
</p>
<p>&#5;
</p>
<p>&#20; &#4;&#6;&#5;&#20; &#4;&#7;
</p>
<p>&#3;
</p>
<p>&#5;
</p>
<p>(5)
</p>
<p>avec &#20; &#4;&#6;&#4; &#7;
&#3;
</p>
<p>&#5; est la probabilit&#233; conjointe d&#8217;apparition de &#6; et &#7;
&#3;
</p>
<p>, &#20; &#4;&#6;&#5; est la probabilit&#233; a priori
du mot &#6; et &#20; &#4;&#7;
</p>
<p>&#3;
</p>
<p>&#5; la probabilit&#233; a priori du th&#232;me &#7;
&#3;
</p>
<p>.
</p>
<p>Une information mutuelle &#233;lev&#233;e entre un mot et un th&#232;me est le signe d&#8217;un lien fort entre ces
deux &#233;l&#233;ments. Par cons&#233;quent, les vocabulaires de th&#232;mes seront compos&#233;s des mots d&#8217;infor-
mation mutuelle les plus &#233;lev&#233;es.
</p>
<p>Le nombre de mots par th&#232;me doit maintenant &#234;tre fix&#233;. Nous choisissons volontairement un
nombre de mots identique pour chaque th&#232;me. Concernant la m&#233;thode exploitant l&#8217;information
mutuelle, afin de d&#233;terminer le nombre de mots &#224; conserver, nous trions, pour chaque th&#232;me,
les mots par ordre d&#233;croissant de valeur d&#8217;information mutuelle avec le th&#232;me, et nous tra-
&#231;ons la courbe de cette &#233;volution (FIG. 1). A l&#8217;aide de cette courbe, nous pouvons remarquer
que globalement, au dessus de &#6;&#3;&#3;&#3; mots, l&#8217;information mutuelle des mots se &#8220;stabilise&#8221;, nous
choisissons donc de conserver &#6;&#3;&#3;&#3; mots par th&#232;me.
</p>
<p>Dans un souci de rigueur de comparaison des m&#233;thodes, nous avons &#233;galement fix&#233; &#0; &#224; &#6;&#3;&#3;&#3;
pour les vocabulaires compos&#233;s des mots les plus fr&#233;quents.
</p>
<p>Nous avons &#233;valu&#233; les performances de WSIM sur chacun des deux vocabulaires. Les r&#233;sultats
sont pr&#233;sent&#233;s TAB. 2. La diff&#233;rence spectaculaire des r&#233;sultats peut &#234;tre expliqu&#233;e par des
vocabulaires de th&#232;mes tr&#232;s diff&#233;rents : en effet, en moyenne les deux ensembles de vocabulaires
</p>
<p>150</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>WSIM : une m&#233;thode de d&#233;tection de th&#232;me fond&#233;e sur la similarit&#233; entre mots
</p>
<p>In
fo
</p>
<p>rm
at
</p>
<p>io
n 
</p>
<p>M
ut
</p>
<p>ue
lle
</p>
<p>Mots
</p>
<p>0
0 500 1000 1500 2000 2500 3000 3500 4000 4500
</p>
<p>Economie
Etranger
Histoire
</p>
<p>Politique
Sciences
</p>
<p>Sports
</p>
<p>3e&#8722;4
</p>
<p>2.5e&#8722;4
</p>
<p>2e&#8722;4
</p>
<p>1.5e&#8722;4
</p>
<p>1e&#8722;4
</p>
<p>5e&#8722;5
</p>
<p>Culture
</p>
<p>FIG. 1 &#8211; Information mutuelle class&#233;e par valeur d&#233;croissante pour chaque th&#232;me
</p>
<p>TAB. 2 &#8211; Taux de d&#233;tection des th&#232;mes en fonction du vocabulaire
Vocabulaire Performance (%)
</p>
<p>Plus fr&#233;quents 27.5
Information mutuelle 82.4
</p>
<p>ont seulement 31% des mots en commun. De plus, les vocabulaires construits avec la m&#233;thode
des &#0; plus fr&#233;quents, ont un taux de recouvrement de 64%, alors que pour les vocabulaires
construits avec l&#8217;information mutuelle, ce taux n&#8217;est que de 25%.
Nous pouvons ainsi noter que la m&#233;thode WSIM semble &#234;tre tr&#232;s d&#233;pendante du vocabulaire
choisi. En effet, la similarit&#233; entre deux mots est fond&#233;e sur leur comportement avec l&#8217;ensemble
des autres mots du vocabulaire. Par cons&#233;quent, les mots du th&#232;me doivent &#234;tre particuli&#232;rement
bien choisis pour obtenir une similarit&#233; fiable.
Nous d&#233;cidons donc de conserver les vocabulaires cr&#233;&#233;s &#224; l&#8217;aide de la mesure d&#8217;information
mutuelle.
</p>
<p>4.3 R&#233;sultats
</p>
<p>Afin d&#8217;&#233;tudier les performances de WSIM, nous proposons de la comparer avec d&#8217;autres m&#233;-
thodes de d&#233;tection de th&#232;mes.
Nous choisissons pour cela, de la comparer &#224; la TFIDF (Salton, 1991), qui est la m&#233;thode cit&#233;e
comme r&#233;f&#233;rence dans le domaine. Cette derni&#232;re &#233;value la distance cosine entre les distribu-
tions de probabilit&#233;s des mots dans les th&#232;mes et celle du document de test.
Nous comparons &#233;galement les performances (en termes de rappel) de notre m&#233;thode &#224; celle du
mod&#232;le cache (Bigi et al., 2000). Les exp&#233;riences men&#233;es dans une &#233;tude r&#233;cente (Bigi et al.,
2001), et qui comparait 5 m&#233;thodes donnait la m&#233;thode d&#8217;identification par cache en t&#234;te. La
m&#233;thode cache &#233;value la distance de Kullback-Leibler entre les distributions de mots des th&#232;mes
et celle du cache du document de test.
</p>
<p>151</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>A. Brun, K. Sma&#239;li, J.P. Haton
</p>
<p>TAB. 3 &#8211; Performance des trois m&#233;thodes &#233;tudi&#233;es
M&#233;thode Performance (%)
TFIDF 72.1
Cache 82.0
WSIM 82.4
</p>
<p>TAB. 4 &#8211; Rappel, pr&#233;cision et &#28;
&#0;
</p>
<p>pour chaque m&#233;thode et chaque th&#232;me
TFIDF Cache WSIM
</p>
<p>Th&#232;me Rap Prec &#28;
&#0;
</p>
<p>Rap Prec &#28;
&#0;
</p>
<p>Rap Prec &#28;
&#0;
</p>
<p>Culture 83.2 82.3 82.8 84.7 90.4 87.4 85.3 87.1 86.2
Economie 60.8 89.8 72.4 74.6 91.0 82 78.8 84.6 81.6
Etranger 57.8 58.4 58.2 86.3 73.9 79.6 85.3 79.8 82.5
Histoire 58.3 13.2 21.6 16.6 14.3 14.4 8.3 33.3 13.3
Politique 70.7 79.5 74.8 85.1 75.1 79.8 86.2 83.0 84.6
Sciences 90.8 66.7 76.9 88.1 82.7 85.4 83.5 79.8 81.6
Sports 66.7 59.3 62.8 75 72 73.4 83.3 62.5 71.4
</p>
<p>Dans TAB. 3, nous comparons les performances de notre m&#233;thode &#224; la TFIDF et au cache
sur l&#8217;ensemble des 835 paragraphes. Les performances de la TFIDF, qui est la m&#233;thode de
r&#233;f&#233;rence, sont largement d&#233;pass&#233;es par les deux autres m&#233;thodes. De plus, les performances du
mod&#232;le cache ont &#233;t&#233; d&#233;pass&#233;es, pour la premi&#232;re fois, par la m&#233;thode WSIM, m&#234;me si leurs
performances restent cependant tr&#232;s proches.
Afin de mieux analyser les performances de ces 3 m&#233;thodes, il serait donc int&#233;ressant de les
&#233;tudier th&#232;me par th&#232;me. Pour cette raison, nous pr&#233;sentons les performances de chacune des
m&#233;thodes en termes de rappel et pr&#233;cision, o&#249; :
</p>
<p>Rappel
&#10;
</p>
<p>&#2;
</p>
<p>Nb textes correctement &#233;tiquet&#233;s T
Nb textes d&#8217;&#233;tiquette T (6)
</p>
<p>Pr&#233;cision
&#10;
</p>
<p>&#2;
</p>
<p>Nb textes correctement &#233;tiquet&#233;s T
Nb textes &#233;tiquet&#233;s T (7)
</p>
<p>Rappelons ici que l&#8217;objectif final de la d&#233;tection de th&#232;mes est l&#8217;adaptation du mod&#232;le de lan-
gage au th&#232;me. Par cons&#233;quent, nous cherchons une m&#233;thode qui d&#233;tecte &#224; la fois le th&#232;me du
plus grand nombre de documents (rappel) mais fournisse &#233;galement une &#233;tiquette fiable (pr&#233;ci-
sion).
Pour cette raison, les r&#233;sultats seront &#233;galement pr&#233;sent&#233;s en termes de mesure &#28;
</p>
<p>&#0;
</p>
<p>, qui permet
de combiner le rappel et la pr&#233;cision dans une seule valeur.
</p>
<p>F
&#0;
</p>
<p>&#2;
</p>
<p>&#6; &#5; rappel
&#10;
</p>
<p>&#5; pr&#233;cision
&#10;
</p>
<p>rappel
&#10;
</p>
<p>&#7; pr&#233;cision
&#10;
</p>
<p>(8)
</p>
<p>TAB. 4 pr&#233;sente, par th&#232;me, les valeurs de rappel, pr&#233;cision et &#28;
&#0;
</p>
<p>pour les 3 m&#233;thodes &#233;tudi&#233;es.
</p>
<p>Nous pouvons remarquer que les performances des m&#233;thodes varient significativement d&#8217;un
th&#232;me &#224; l&#8217;autre. Une des raisons de ces diff&#233;rences peut &#234;tre la taille d&#8217;apprentissage des th&#232;mes,
les th&#232;mes bien appris ayant tendance &#224; &#234;tre bien d&#233;tect&#233;s. En effet, comme pr&#233;sent&#233; dans la
</p>
<p>152</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>WSIM : une m&#233;thode de d&#233;tection de th&#232;me fond&#233;e sur la similarit&#233; entre mots
</p>
<p>table 1, la taille d&#8217;apprentissage entre Culture et Sports diff&#232;re d&#8217;un facteur environ &#0;&#12;&#3;. Une
autre raison peut &#233;galement &#234;tre le recouvrement entre th&#232;mes.
Les performances par th&#232;me de la TFIDF sont repr&#233;sentatives de son comportement g&#233;n&#233;ral :
seules ses valeurs de rappel pour les th&#232;mes Histoire et Sciences surpassent les deux autres
m&#233;thodes.
</p>
<p>On peut remarquer que le th&#232;me Histoire n&#8217;est bien reconnu par aucune des m&#233;thodes. En
plus d&#8217;une taille d&#8217;apprentissage faible, le th&#232;me Histoire souffre d&#8217;un manque de vocabulaire
propre. En effet, on peut intuitivement dire que le th&#232;me Histoire ne peut &#234;tre repr&#233;sent&#233; &#224; l&#8217;aide
de mots de vocabulaires sp&#233;cifiques. Ce dernier est plut&#244;t repr&#233;sent&#233; par des dates (ensemble
quasi-infini) ou encore par l&#8217;emploi d&#8217;un temps pass&#233;. Il serait donc int&#233;ressant d&#8217;int&#233;grer des
informations d&#8217;un niveau sup&#233;rieur au mot et &#233;ventuellement des connaissances syntaxiques
pour am&#233;liorer les performances de d&#233;tection.
Bien que la m&#233;thode WSIM obtienne les meilleures performances sur le corpus g&#233;n&#233;ral, on peut
remarquer que le mod&#232;le cache la d&#233;passe l&#233;g&#232;rement dans certains cas, et notamment au niveau
de la pr&#233;cision. Cependant, WSIM a un taux de rappel beaucoup plus homog&#232;ne sur l&#8217;ensemble
des th&#232;mes, hors Histoire, que le mod&#232;le cache, ses performances ne semblant pas d&#233;pendre de
la taille d&#8217;apprentissage.
</p>
<p>5 Conclusions et perspectives
</p>
<p>Nous avons pr&#233;sent&#233; une nouvelle m&#233;thode de d&#233;tection de th&#232;me WSIM, originale par l&#8217;infor-
mation qu&#8217;elle exploite. En plus de la probabilit&#233; des mots dans les th&#232;mes, celle-ci utilise la
similarit&#233; mot-th&#232;me, elle-m&#234;me bas&#233;e sur la similarit&#233; inter-mots.
Cette m&#233;thode est tr&#232;s d&#233;pendante du vocabulaire utilis&#233; en raison de l&#8217;utilisation de l&#8217;int&#233;-
gralit&#233; des mots des vocabulaires pour calculer la similarit&#233; inter-mots. Nous avons montr&#233;
l&#8217;importance du choix des vocabulaires : nos tests donnent des performances 3 fois sup&#233;rieures
&#224; celles obtenues avec un vocabulaire compos&#233; des mots les plus fr&#233;quents.
Compar&#233;e &#224; la m&#233;thode TFIDF classique, la m&#233;thode WSIM obtient des r&#233;sultats meilleurs de
&#0;&#13;&#14;. Les performances en d&#233;tection de th&#232;mes d&#233;passent &#233;galement celles du mod&#232;le cache,
qui n&#8217;avait jusque l&#224; jamais &#233;t&#233; &#233;gal&#233;. M&#234;me si cette am&#233;lioration n&#8217;est pour le moment pas
spectaculaire, nous travaillons actuellement sur plusieurs pistes en vue d&#8217;am&#233;liorer nos r&#233;sul-
tats.
Nous avons volontairement choisi des vocabulaires de th&#232;mes compos&#233;s d&#8217;un nombre &#233;gal de
mots. Cependant, les vocabulaires des th&#232;mes avec une taille d&#8217;apprentissage faible, comportent
vraisemblablement des mots non caract&#233;ristiques. Il serait donc int&#233;ressant d&#8217;&#233;tudier des voca-
bulaires avec des tailles diff&#233;rentes, en fixant par exemple une valeur d&#8217;information mutuelle
normalis&#233;e minimale.
De plus, malgr&#233; des &#233;tudes prouvant la non am&#233;lioration des performances de d&#233;tection en uti-
lisant des s&#233;quences de mots, nous envisageons d&#8217;ins&#233;rer des s&#233;quences de mots. Tout d&#8217;abord
en raison de la taille d&#8217;apprentissage de nos th&#232;mes, qui est largement sup&#233;rieure &#224; celle des
&#233;tudes cit&#233;es pr&#233;c&#233;demment. De plus, la fa&#231;on dont nous souhaitons proc&#233;der pour l&#8217;extraction
de s&#233;quences est fond&#233;e sur des connaissances s&#233;mantiques.
Pour le traitement de th&#232;mes comme Histoire, il serait int&#233;ressant d&#8217;utiliser des classes s&#233;man-
tiques, pour notamment repr&#233;senter des notions de dates, lieux, noms propres, etc., le vocabu-
laire devenant ainsi un vocabulaire de classes.
</p>
<p>153</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>A. Brun, K. Sma&#239;li, J.P. Haton
</p>
<p>R&#233;f&#233;rences
APT&#201; G., DAMERAU F. &amp; WEISS S. (1994). Automated learning of decision rules for text categoriza-
tion. ACM Transactions on Information Systems, 12(3), 233&#8211;251.
BIGI B., BRUN A., HATON J., SMAILI K. &amp; ZITOUNI I. (2001). Dynamic topic identification : To-
wards combination of methods. In Proceedings of the Recent Advances in Natural Language Processing
(RANLP), p. 255&#8211;257.
BIGI B., DE MORI R. &amp; EL B&#200;ZE M. (2000). A fuzzy decision strategy for topic identification and
dynamic selection of language models. Signal Processing Journal, 80(6), 1085&#8211;1097.
BRUN A., SMAILI K. &amp; HATON J. (2000). Experiment analysis in newspaper topic detection. In 7th
International Symposium on String Processing and Information Retrieval, SPIRE-2000, p. 55&#8211;64.
CAROPRESO M., MATWIN S. &amp; SEBASTIANI F. (2001). A learner-independant evaluation of the use-
fulness of statistical phrases for automatic text categorization, p. 78&#8211;102. Hershey, US.
CHELBA C. &amp; JELINEK F. (2000). Structured language modeling. Computer Speech and Language,
14(4), 283&#8211;332.
CHEN S. F. &amp; GOODMAN J. (1996). An empirical study of smoothing techniques for language mode-
ling. In Proceedings id the 34th Annual Meeting of the ACL, p. 310&#8211;318.
DAGAN I., KAROV Y. &amp; ROTH D. (1997). Mistake-driven learning in text categorization. In Procee-
dings of the EMNLP-97, 2nd Conference on Empirical Methods in Natural Language Processing, p.
55&#8211;63.
DAGAN I., LEE L. &amp; PEREIRA F. C. N. (1999). Similarity-based models of word coocurrence proba-
bilities. Machine Learning, 34, 43&#8211;69.
JOACHIMS T. (1997). A probabilistic analysis of the rocchio algorithm with tfidf for text categorization.
In Proceedings of ICML-97, 14th International Conference on Machine Learning, p. 143&#8211;151.
JOACHIMS T. (1998). Text categorization with support vector machines : learning with many relevant
features. In Proceeding of ECML-99, 16th European Conference on Machine Learning, p. 137&#8211;142.
KUHN R. &amp; DE MORI R. (1990). A cache-based natural language model for speech reproduction. IEEE
Transactions on Pattern Analysis and Machine Intelligence, 12(6), 570&#8211;583.
LEWIS D. (1992). An evaluation of phrasal and clustered representations on a text categorization task.
In A. PRESS, Ed., Proceedings of SIGIR-92, 15th ACM International Conference on Research and De-
velopment in Information Retrieval, p. 37&#8211;50, New York, US.
LEWIS D. &amp; RINGUETTE M. (1994). A comparison of two learning algorithms for text categorization.
In Proceedings of SDAIR-94, 3rd Annual Symposium on Document Analysis and Information Retrieval,
p. 81&#8211;93.
MC DONOUGH J. &amp; NG K. (1994). Approaches to topic identification on the switchboard corpus. In
International Conference on Acoustics, Speech and Signal Processing, p. 385&#8211;388, Yokohama, Japan.
MITCHELL T. (1996). Machine Learning, chapter 3. Mc Graw Hill.
MLADENIC D. (1998). Feature subset selection in text-learning. In 10th European Conference on
Machine Learning ECML98.
ROSENFELD R. (1996). A maximum entropy approach to adaptive statistical language modeling. Com-
puter Speech and Language, 10, 187&#8211;228.
SALTON G. (1991). Developments in automatic text retrieval. Science, 253, 974&#8211;979.
SEYMORE K. &amp; ROSENFELD R. (1997). Large-scale Topic Detection And Language Model Adaptation.
Rapport interne CMU-CS-97-152, School of Computer Science,CMU.
YANG Y. &amp; CHUTE C. (1994). An example-based mapping method for text categorization. ACM
Transactions on Information Systems, 12(3), 252&#8211;277.
</p>
<p>154</p>

</div></div>
</body></html>