'1ALN zuuz, Nancy, 24-27 _]l11I1 zuuz

An Example-Based Semantic Parser for Natural Language

Michel Généreux
Austrian Research Institute for Artiﬁcial Intelligence
Schottengasse 3, A-1030, Vienna, Austria
rnichel@oefai.at

Abstract

This paper presents a method for guiding semantic parsers based on a statistical model.
The parser is example driven, that is, it learns how to interpret a new utterance by look-
ing at some examples. It is mainly predicated on the idea that similarities exist between
contexts in which individual parsing actions take place. Those similarities are then used to
compute the degree of certainty of a particular parse. The treatment of word order and the
disambiguation of meanings can therefore be learned.

Mots-clefs — Keywords

Analyseur sémantique, Corpus, Langue naturelle
Semantic parser, Corpus, Natural language

1 Introduction

In order to achieve better results in acquisition, coverage, robustness and portability, corpus-
based methods have been recently applied with success in areas like speech recognition (Ra-
biber, 1989), part-of-speech tagging (Charniak, Hendrickson, Jacobson and Perkowitz,
1993) and syntactic parsing (Manning and Carpenter, 1997). In Semantic Parsing, the
process of mapping a natural language input to some structured meaning representation,
Collins and Miller (Collins, 1998) describe a statistical model for extraction of events;
Miller, Stallard, Bobrow and Schwartz describe an approach entirely based on a trained sta-
tistical model (Miller, Stallard, Robrow and Schwartz, 1996) and Thompson, Mooney and
Tang (Thompson, Mooney and Tang, 1997) propose a novel and very interesting approach
based on a bottom-up parser and a machine learning algorithm. I propose an approach in
which a bottom-up parser is combined with a statistical model. Figure 1 shows the overall
architecture of the system.

2 Overview of the Parsing Process

This section presents the various elements of the system. The parser used is a variant of
a Shift-Reduce parser (Marcus, 1980). It actually comprises three different manageable
actions that the parser uses to get to the ﬁnal parse, which is a semantic interpretation
(in ﬁrst-order logic) of a natural language utterance. This left to right parsing makes the
process relatively intuitive for humans.

J.V11\/1l\/1 \J \/ll\/L \/ LIA

Training File Semantic Lexicon

  

Overly General Parser

Statistical File

Specialized Parser

Output: Meaning

Figure 1: The Parser Architecture

  
 

   
 

Input: Phrase

     

The Input String The input string is a list of words to give an interpretation for. When
no action is applicable and the input string is empty, then the parsing process is completed.

For examplelz
(1) [Ich,suche,einen,Artikel,iiber,Bush]

The Parse Stack Format: [conceptl:[context1],concept2:[context2],...]

The parse stack is the actual parse state, the current interpretation of the input string
found so far. It is a list of binary elements, each element representing a combination of the
introduced predicate (or concept) with its context of introduction. Here is an example:

(2) [suche( [],zeitung(_ ),zeit(_ )): [suche,einen,Artikel],start: [Ich]]

The sHIFT action Syntax: sHIFT(word_to_be_shifted)

A sHIF T action simply puts the ﬁrst word from the input string at the end of the context
of the concept on the top of the parse stack. For example, the action sHIFT(iiber) on the
parse stack 2 would result in the following new parse stack:

(3) [suche( [],zeitung(_ ),zeit(_ )): [suche,einen,Artikel,iiber],start: [Ich]]

The iNTRODUCE action Syntax: iNTRODUCE(concept_to_be_introduced)

The iNTRODUCE action takes a concept from the semantic lexicon and puts it on the
top of the parse stack, initializing its context of introduction to the word (or list of words)
that triggered this concept. For example, the action iNTRODUCE(topic(I)) on the parse
stack 2 would result in the following new parse stack:

(4) [topic(1):[topic(1)],suche([],zeitung(_ ),zeit(_ )): [suche,einen,Artikel],start: [Ich]]

The dROP action Syntax: dROP(source_term, target_term)

The dROP action attempts to place a term from the parse stack as argument to another
term of the parse stack. For example, the action dROP(topic(I),suche([],zeitungL ),zeit(_ )))
on the parse stack 4 would result in the following new parse stack:

(5) [suche([topic(1)],zeitung(_ ),zeit(_ )): [suche,einen,Artikel],start: [Ich]]

A Parse State Format: op(aCTION(arguments)#Parse_Stack#Input_String)
It indicates in which context, i.e. how the Parse Stack and the Input String looked like,
when the action took place. Op is simply a containeiz for all types of actions.

1Although not yet topicalized.
2A container is a term which embraces other terms.

F111 J._41\(l..l11l.l1\/-1J(l.D\/bl >.J\/111(l.l1L1\/ 1 (£1091 llll LVGLLILGI J._4(l.l1sbl(l.s\/

A Final State Format: ﬁnal(Parse_Stack)
It indicates the ﬁnal aspect of a parse, i.e. the meaning found for an input string.

Semantic Lexicon Format: lexicon(CONCEPT, [TRIGGERING_PHRASE])
It comprises all the concepts and their triggering phrase(s) that we W1Sh our parser

to process. For example, suche or brauche would trigger the suche concept. Here is an
example of a lexical entry:

(6) lexicon(suche([],zeitung(_ ),zeit(_ )), [suche]).

The shift-reduce parser I am now ready to present the Variant of the shift-reduce
parser I am using. The algorithm of the parser is as follows:

1. Try to introduce a new concept or shift a word.
2. If possible, make one drop action.
3. If there are more words in the input string, go back to Step 1. Otherwise stop.

T0pic_eXtracti0n Typically, topic_extraction replaces relevant noun phrases or preposi-
tional phrases in the input string by successive topic(_ ) terms. For our running example 1,
after topic_extraction, this is the following phrase which is passed on to the parser:

[Ich,suche,einen,Artikel,topic(1 )]

3 Training

The training phase records successful actions (called op), as well as the different ﬁnal states.
For each of them uniquely deﬁned, it assigns a frequency measure, deﬁned as follows:

Occu7“ence_o f _a_particul ar_acti0n_in_a_speci f ic_c0ntea7t

(7)

F :
Tequency Total _number_o f _0ccu7“ence_o f _this_action

In the following are examples of sentences on which the parser was trained (a test set).

Written Korpus

Artikel ﬁber das Wiener Neuj ahrskonzert suche ich.
Etwas uber das Konzert als festen Bestandteil des kulturellen Lebens suche ich.

Spoken Korpus

Ich mochte jetzt eine neue Suche beginnen.
Die Kosovo-Krise un und Bill Clinton.

Artiﬁcial data

Bitte geben Sie mir einen Text zum Thema Kosovo-Krise.
Aber bitte nur in der Zeitung Salzburger Nachrichten Von Vor einer Woche.

Training ﬁle The training ﬁle is the ﬁle in which training examples are stored. These
examples have the following format:

training([topicalized_phrase], meaning).
Meaning is in the form of ﬁrst order logic expressions. A training ﬁle example could be’:

training( [Ich,rnoechte, jetzt, eine, neue, Suche,beginnen] , neue_suche) .

training( [Ich, suche, einen,Artikel,topic (1) ] , suche ( [topic (1) ] , zeitung (_) , zeit (_) ) ) .

3oe stands for 6, ue stands for ii and ae stands for a

J.V11\/1l\/1 \J \/ll\/L \/ LIA

Note that the examples for which we wish to train for should be topicalized for a changing
domain. The third example could be helpful in training for a sentence like:

Ich suche einen Artikel ﬁber Bush.

While training, an overlyGeneralParser is used. It is overly general in the sense that it tries
any possible actions to get to the ﬁnal parse, without considering any information (such as
statistics) that could be helpful to guide the parsing process. In training, a training beam
can be speciﬁed. This means that only a certain number of parses will be recorded in the
statistical ﬁle for each training example.

File used by the Specialized Parser: the Statistical File The overlyGeneralParser
parses the training ﬁle to generate the statistical ﬁle. Every step needed to go from the
topicalized_phrase to the meaning is recorded, as well as ﬁnal states themselves. Final
states are simply the states of the parse stack themselves at the end of the parse. Each of
them (actions and ﬁnal states) are assigned a frequency measure as described previously.
Each line has either one of the following format (recall that op is a container for any action):

Op (1-\CTION#PARSE_STACK#INPUT_STRING#FREQUENCY) .
final (FIN1-\L_S'I'ATE#FREQUENCY) .

Here is an example:

op(sHIFT(Ich)#[start:[]]#[Ich,suche,einen,Text,for,topic(l),topic(2),
bitte,bearbeiten,Sie,meinen,Suchauftrag]#0.3333).

These lines are used by the specializedParser to compute the best parse.

4 Statistical Parsing

The actual parsing of the input phrase is done by a specializedParser. It is specialized in the
sense that it uses a statistical model to process all the information available from the training
phase in order to get the best possible parse (the one with the highest probability). This
section presents a detailed description of the statistical model used. Weighting parameters
are presented but not discussed.

The Search space Like in the training phase, the most obvious way to inﬂuence the
parse is to tell the parser how many parses it should try before taking a decision. I call it
the search beam parameter.

Measure of similarity between lists This is a crucial aspect of the specialized parser.
When the parser tries to choose a suitable parse, it must compare list of words (to compare
Actions, Parse stacks or Input strings). A good similarity measure between lists is essential,
but because computing similarity is very demanding on computer resources, one must ﬁnd
a trade-off that preserves computational efﬁciency. At the top level, the similarity measure
is simply a measure of the number of identical elements in both lists, divided by the size of
the largest list. Therefore, we have:

N umbe7“_0 f identical _el ements
S ize_0 f _the_biggest_list

(8)

Similarity =

For example, omitting case-sensitivity:

(9) similarity([Ich,suche,einen,Artikel],[Das,suche,ich]) = 2/4 = 0.5

F111 J._41\(l..l11l.l1\/-1J(l.D\/bl >.J\/111(l.l1L1\/ 1 (£1091 llll LVGLLILGI J._4(l.l1sbl(l.s\/

Comparisons sometimes involves structures. Structures can be decomposed into list in
PROLOG4, and then compared by using equation 8:

predicate(a1"gl, a1"g2,  = [predicate, argl, arg2,  (10)

Therefore, we can now roughly compare structures as lists. Preliminary empirical results
tend to show that the approximation is sensible .

Parameterizing the model I introduce all the equations for the model, explaining their
context of use. The best parse P is found by taking the highest probability 3 among the
possible parses (limited by the search beam) available:

P = ma.x¢P¢ (11)

Each of these parses B have a probability that amounts to combining the probability of
the individual op or actions together (ﬂk ak) and adding the probability of the ﬁnal state
(ProbF, see equation 15). These two components must be appropriately weighted by Pop
and Pﬁnal. Multiplying by 100 gives a more readable value between 0 and 100.

P, = (Pop * (H ak) + Pﬁnal * ProbF) * 100 (12)
k
The way each op ak is assigned a probability is by taking into account its similarity with
one of the ops in the statistical ﬁle (see equation 14) as well as the frequency of this op
(Frequency). These two components are also weighted by Pop. sim and Pop. occ.

ak = maa7m(Pop_sz'm >s< Pm + Pop_occ >s< Frequency) (13)

While looking for a suitable op in the statistical ﬁle, the parser looks for similarity. The
similarity of an op Pm with one in the statistical ﬁle is measured by multiplying together
the similarity of the op as such, the similarity of the parse stack and the similarity of the
input string (1? stands for training).

Pm : maw(5im'A(0p action! ta?) * 5im'PS(0pps7 ti’-5) * sim'I(0p input? ti"P1lt)) 

Computing the probability of a ﬁnal parse state is similar to computing the one for
actions. A ﬁnal state probability ProbF is the weighted sum of the most similar ﬁnal state
in the statistical ﬁle Pf (see 16) and the frequency of this ﬁnal state Frequency:

P7‘obF = ma,xf(Pﬁnal_sim * Pf + Pﬁnal_occ >s< Frequency) (15)

Pf = maa:n(sz'm(tn,F)) (16)

When needed, smoothing is carried out in a very conservative manner.

5 Results

I have conducted an experiment to test the performance of the parser. After training with
only 80 examples, the parser averages 62% correctness while parsing a new sentence. Re-
call is therefore slightly lower than the other approaches mentioned at the beginning of the
paper. I believe there are mainly four reasons to that:

1. The very low number (80) of training examples, compare to 560, 225 and 4000 sentences of
other approaches.

2. The lack of extensive testing on what would be the best setting for default values of weighting
parameters. Only a set of rather intuitive values were used.

3. The assimilation of natural language utterances to sets instead of lists.

4. A measure of similarity that sacriﬁces precision for computational efﬁciency.

4The programming language used.

J.V11\/1l\/1 \J \/ll\/L \/ LIA

6 Conclusion

In this paper, a new probabilistic framework for semantic parsing is presented. The com-
bination of a shift-reduce parser and a purely statistical model makes it unique. More pre-
cisely, the parser learns efﬁcient ways of parsing new sentences by collecting statistics on
the context in which each parsing action takes place. It computes probabilities on the basis
of the similarities of those contexts and their frequencies. The result is a simple and robust
parser. Its conﬁguration can be change in many ways, to ﬁt different types of corpus or do-
mains. At this point, the system has not yet been fully tested on very large corpora, to see
if the statistical model remains as efﬁcient. It does not include the treatment of variables,
which means that there is no treatment of questions. However, testing with few examples,
the parser shows promising results.

Compare to similar systems using some machine-learning techniques, ours offers an ap-
proach in which linguistics can play a decisive role; we have a more direct inﬂuence on the
role of contexts in evaluating the probability of a parse. One crucial aspect of the parser,
the computation of similarities between context, relies on a good interpretation of linguis-
tic patterns found in phrases, and how those conﬁgurations may determine the particular
meaning of a word or group of words. This is essential to interpret, and maybe understand,
natural language utterances.

Acknowledgments

This work has been sponsored by the Fonds zur Forderung der wissenschaftlichen Forschung
(FWF), Grant No. P13704. The Austrian Research Institute for Artiﬁcial Intelligence
(GFAI) is supported by the Austrian Federal Ministry of Education, Science and Culture.

References

E. Charniak, C. Hendrickson, C. Jacobson and M. Perkowitz (1993), Equations for part-
of-speech tagging, Proceedings of the 11th National Conference on AI, 784-789.

M. Collins and S. Miller (1998), Semantic Tagging using a Probalistic Context Free Gram-
mar, In Proceedings of the Sixth Workshop on Very Large Corpora.

M.J. Collins (1997), Three generative, lexicalised models for statistical parsing, Proceed-
ings of the 35th Annual Meeting of the Association for Computational Linguistics, 16-23.
S. Miller, D. Stallard, R. Bobrow and R. Schwartz (1996), A Fully Statistical Approach
to Natural Language Interfaces, Proceedings of the 34th Annual Meeting of the ACL,
Morgan Kaufmann Publishers, San Francisco, Arivind J oshi and Martha Palmer, 55-61.
L.R. Rabiner (1989), A tutorial on hidden Markov models and selected applications in
speech recognition, Proceedings of the IEEE, Vol. 77, No. 2, 257-286.

C. A. Thompson, R. J . Mooney and L. R.Tang (1997), Learning to Parse Natural Lan-
guage Database Queries into Logical Form, Proceedings of the ML-97 Workshop on
Automata Induction, Grammatical Inference, and Language Acquisition.

C.D. Manning and B. Carpenter (1997), Three generative, lexicalised models for statisti-
cal parsing, Proceedings of the 5th Int. Workshop on Parsing Technologies, 147-158.

M.P. Marcus (1980), A Theory of Syntactic Recognition for Natural Language, MIT Press.

Ulf Hermjakob and Raymond J . Mooney (1997), Learning Parse and Translation Deci-
sions from Examples with Rich Context, Proceedings of the 35th Annual Meeting of the
Association for Computational Linguistics, 482-489.

