TALN 2002, Nancy, 24-27 juin 2002

Polynomial Tree Substitution Grammars:
Characterization and New Examples

J ean-Cédric Chappelier, Martin Rajman and Antoine Rozenknop
EPFL
I&C—HF—LIA, IN (Ecublens)
CH-1015 Lausanne, Switzerland

{Jean—Cedric . Chappelier, Martin . Rajman, Antoine . Rozenknop}@epfl . ch

Mots-clefs — Keywords

Analyse Syntaxique Probabiliste, Grammaires a Substitution d’ Arbres Polynomiales, Grammaires Hors-
Contexte Probabilistes, Data-Oriented Parsing.

Stochastic Parsing, Polynomial Tree Substitution Grammars, Stochastic Context-Free Grammars, Data-
Oriented Parsing.

Résumé - Abstract

Le but de ce papier est de caractériser (au moins partiellement) les Grammaires a Substitution d’Arbres
Polynomiales (pSTSG), instances particuliéres de STSG pour lesquelles la recherche de l’analyse la plus
probable peut étre effectuée en un temps polynomial. Nous donnons tout d’abord diverses conditions
sufﬁsantes, utilisables en pratique, qui garantissent qu’une STSG est polynémiale. Une telle condition
sufﬁsante, fondée sur la notion de << téte de syntagme >>, est ensuite présentée et évaluée.

Polynomial Tree Substitution Grammars, a subclass of STSGs for which ﬁnding the most probable parse
is no longer NP-hard but polynomial, are deﬁned and characterized in terms of general properties on the
elementary trees in the grammar. Various sufﬁcient and easy to compute properties for a STSG to be
polynomial are presented. The min-max selection principle is shown to be one such sufﬁcient property.
In addition, another, new, instance of a sufﬁcient property, based on lexical heads, is presented. The
performances of both models are evaluated on several corpora.

1 Motivations

Stochastic Tree Substitution Grammars (STSG), mainly used in the Data-Oriented Parsing
(DOP) framework (Bod, 1998), are grammars the rules of which consist of syntactic trees,
called “elementary trees”. These elementary trees are combined with the substitution operator1
to give derivations of complete parse trees. In addition, a probability p(t) is assigned to each
elementary tree t of the grammarz. These probabilities serve to compute the probabilities of
parse trees. Although STSGS are equivalent to Context Free-Grammars (CFG) from a structural

1denoted by “o”
2in such a way that the sum of the probabilities of elementary trees that have the same root node is 1.

Chappelier, Rajman, Rozenknop

point of view3, STSGs bring a clear advantage at the probabilistic level. They can capture a
much larger set of probabilistic dependencies than the SCFGs, for which the probabilization is
restricted to context-free (CF) rules (i.e. depth-1 elementary trees only).

However, STSGs suffer from one major drawback: ﬁnding the most probable parse tree (MPP)
has been proved to be an NP-hard problem in the most general case (Sima’an, 1996). Various
approximated MPP search algorithms have been developed (Bod, 1992; Goodman, 1996; Chap-
pelier & Rajman, 2000), but another alternative, ﬁrst introduced in (Chappelier & Rajman,
2001), is possible. This approach consists in choosing the set of elementary trees in the STSG
in such a way that ﬁnding the MPP is no longer NP-hard but polynomial. More precisely, the
idea underlying Polynomial Tree Substitution Grammars (pSTSG) is to restrict the elementary
trees present in the grammar so as to make the MPP search equivalent to the search of a most
probable derivation (MPD) with an SCFG that can be derived from the original STSG.

A trivial example of pSTSGs are the SCFGs themselves, which can be seen as STSGs where
all the elementary trees are liIr1ited to depth-1 trees. A less trivial example of pSTSG, produced
according to the “Ir1in-max selection principle”, was presented in (Chappelier & Rajman, 2001).
In this example, the elementary trees are restricted to all depth-1 trees and all trees, the leaves
of which are exclusively terminals (also called “complete trees”).

The goal of this contribution is to generalize this work by providing explicit necessary and
sufﬁcient conditions on the elementary trees of the STSG, so that ﬁnding the MPP can be
achieved in polynomial time. We also present a new type of pSTSGs, which signiﬁcantly differs
from the grammars derived from the Inin-max selection principle.

The paper ﬁrst provides a formal deﬁnition of pSTSGs and then presents necessary and suf-
ﬁcient conditions for a STSG to be polynomial. Next, various only sufﬁcient but much easier
to compute polynomiality conditions are analyzed. The min-max selection principle is shown
to fulﬁll one such condition. Another instance of these sufﬁcient conditions, based on lexical
heads, is also presented. Finally, the performances of these two models are evaluated.

2 Formal Deﬁnition of pSTSGs

2.1 Tree Decompositions

To formalize the notion of pSTSG, we need to extend the notion of derivation to trees which are
not necessarily parse trees. To do so, we introduce the more general notion of tree decomposi-
tion:

Deﬁnition 1 A partition of a tree T into elementary trees t1, ..., tm of a given STSG Q, is called
a decomposition of T (with respect to Q) and will be denoted by (151, ..., tm)T.4

Consider for instance the tree T = S /S \ S. One possible decomposition of T is (t1, t2)T, where

I
a

t1 = /S\ and t2 = § (which are supposed to be elementary trees of the considered STSG).
S S a

3The same language is recognized and the same parse trees are produced for a given sentence.
4This notation, which is a sequence and not a set, is not at all ambiguous and has to be understood as the
depth—ﬁrst description of the corresponding partition of T.

Polynomial Tree Substitution Grammars

Notice that (t1, t2)T is different from the derivation t1 0 t2 = S /S\ .

I
a

For any parse tree T however, there is a clear one-to-one and onto mapping between deriva-
tions and decompositions of T. Therefore, we will for parse trees indifferently use either the
derivation or the decomposition point of view.

In the STSG framework, the probability of a derivation t1 o  o tn is deﬁned as p(t1 o  o tn) =
H p(t,-); and the probability of a full parse tree T, hereafter called “parse-probability” of T, is
it

deﬁned as the sum of the probabilities of all its derivations: P(T) = Z p(d) = 2 H p(t),

d=>T d=>T ted
where the subscript “d => T” means “for all derivations d leading to the parse tree T”.5

Similarly, the probability of a decomposition (t1, ..., tm)T of a tree T is deﬁned as p((t1, 

, tm)T)
H p(t,-). The generalization of the parse-probability to a tree T that is not a parse tree (i.e. that
it
does not result from a left-most derivation) is given by: P(T) = 2 p(6), where A(T) is the
6EA(T)
set of all possible decompositions of T into elementary trees of 9.

Finally, for any two decompositions 6 and 6’ of a tree T, 6 is said to be ﬁner than 6’ if and only if
(iff) every elementary tree appearing in 6 is itself a subtree6 of some elementary tree appearing
in 6’. This will be noted 6 3 6’.

Notice that ’’g’’ induces a (partial) order on the set of the decompositions of a given tree. This
allows us to deﬁne the notion of maximal decomposition:

Deﬁnition 2 A decomposition 6 of a tree T is said to be maximal if any other decomposition
of T comparable with 6 is ﬁner than 6.‘ V6’ 6 A(T), 6 g 6’ ———> 6’ = 6

2.2 Deﬁnition of pSTSGs

Let us now recall the general framework used for ﬁnding pSTSGs as it was originally described
in (Chappelier & Rajman, 2001):

For any STSG 9, an SCFG gm. can be constructed in the following way: the rules of gm.
are the root-leaves representations of the elementary trees of Q, and of these rules is associated
with the parse-probability of the corresponding elementary tree.

In such a setup, the probability of a derivation in gm. is always less than or equal to the parse-
probability of the corresponding parse tree in 9. Therefore, i_f for each parse tree T produced by
9 there exists at least one derivation in g,qu.., the probability of which i_s the parse-probability
of T in Q, the grammar gm. can be used instead of Q for ﬁnding the MPP in polynomial time.
In other words, the MPD search with gum. and the MPP search with 9 become equivalent.

An STSG for which there exists a polynomial time MPP search algorithm is called a pSTSG.
In addition, we call eﬁective pSTSG a STSG for which an equivalent SCFG parsing scheme as
described above can be build. Notice that this method cannot be applied to all STSGs, as it is

5With STSGs, a given parse tree can indeed have several different derivations, even with the “left—most non-
terminal ﬁrst” rewriting convention.
5including the tree itself

Chappelier, Rajman, Rozenknop

not always possible, in the most general case, to exhibit in polynomial time a derivation that
holds the parse-probability of the parse tree it corresponds to.

Denoting by 8(9) the set of elementary trees of Q and by 79(9) the set of the parse trees
generated by 9, let us now provide a more formal deﬁnition of effective pSTSGs:

Deﬁnition 3 A STSG Q is said to be effectively polynomial iﬁ"v’T E ’D(g), Eltl, ..., 75;, E 5(g)
s.t. T = t1 0  o 15,, and P(T) = H P(t,-).

In other words, an STSG is effectively polynomial iff, for any parse tree, there exists at least
one derivation such that the product of the parse-probabilities of its constituents is the parse-
probability of the parse tree.

Notice that the product  P(t,-) involved in the above deﬁnition, which corresponds to the
probability of a derivation in gequiv, is different from  p(t,-), the probability of the derivation
t1 o  o tk in Q : the former is the product of the E-probabilities of the elementary trees, as
deﬁned in the former section, whereas the later is the product of the elementary probabilities of
the elementary trees.

3 Conditions for Polynomiality

Theorem 1 A STSG is eﬁectively polynomial if any parse tree it can generate has a unique
maximal derivation.

This characterization of effective pSTSG, requires to consider the set of Q the parse trees that
can be generated by the STSG (which very often is an inﬁnite set). As such, it is difﬁcult to apply
in practice. For this reason, more practical (but also less general) conditions for polynomiality
are now presented. To do so, we ﬁrst introduce the notion of atomic grammars, for which we
then provide a sufﬁcient polynomiality condition, much easier to check in practice.

Deﬁnition 4 A grammar is said to be atomic if any depth-I subtree of any elementary tree is
also an elementary tree.

In other words, a grammar is atomic if it contains at least all the CF rules that appear in its
elementary trees.

Deﬁnition 5 For any proper subtree t’ of a tree 75, we deﬁne the expansion oft’ in t as the set of
leaves oft’ which are not leaves oft.

We can now give a sufﬁcient condition for atomic STSGs to be polynomial:

Theorem 2 An atomic S TSG is polynomial if the expansions of any depth-I elementary tree in
any other elementary tree are all the same.

The above theorem can be used to deﬁne the following algorithm, aiming at the automated
extraction of atomic pSTSGs from a tree-bank:

Algorithm 1
1. Extract the CF G from the tree-bank;

2. For each CF rule, deﬁne a unique expansion; and extract from the corpus all subtrees in
which CF rules have this expansion and this expansion only.

Polynomial Tree Substitution Grammars

4 Practical Examples of pSTSG

The min-max selection principle (Chappelier & Rajman, 2001) can now be reinterpreted as an
extraction procedure leading to the atomic STSG for which each CF rule is associated with the
expansion scheme consisting in always expanding all its (non-terminal) leaves. It is therefore a
pSTSG by theorem 2.

Another possible way to automatically extract pSTSG from tree-banks using Algorithm 1 is to
choose as systematic expansion scheme for depth-1 trees an expansion restricted to only one
given non-terminal leaf. One way to implement this in an automatic way is to choose the lexical
head as expansion node. For example, Collins (1999) deﬁned a set of rules to automatically
determine the lexical heads of the CF rules. For our experiments with Bod’s version of the
ATIS treebank, we adapted Collins’ rules, which where developed for the Wall-Street Journal
treebank, to this corpus.

The evaluation protocol consisted in computing an average performance on 25 runs of inde-
pendent training/test splittings of the corpus. To have an upper bound on the results, the perfor-
mance on the full corpus was also computed.

The results obtained are summarized in table 1. The head-drive expansion model appears to
outperform the basic CF model, but is less performant that the Ir1in-max model (Chappelier &
Rajman, 2001). One possible explanation could come from the number of parameters. For each
of the three models (CF, min-max, head-driven) the number of elementary trees extracted from
the whole ATIS corpus is given in table 2. This number is precisely the number of probabilistic
parameters for each model. The fact that the Ir1in-max selection principle performs better than
the head-driven approach on this corpus tends to show that there are enough training data for
this model to accurately capture the probabilistic dependencies present in the corpus.

Another aspect to keep in mind is also that the corpus used is rather ﬂat (trees are wide and
not very deep) and that the lexical-head of a CF rule is, most of the time, a terminal node.
This characteristic of the ATIS corpus implies that the head-driven expansion method does
not produce so many new elementary trees (in addition to the CFG) and that most of the new
elementary trees are of low depth.

Therefore, head-driven expansion approach to pSTSG should be explored further, on corpora
where the notion of “lexical head” is more pertinent than in the ATIS corpus.

precision precision precision
coverage of CF of head-driven of min-max
test 25% 98.5 38.5 39.7 45.5
test 10% 98.6 41.7 42.4 49.7
test 5% 98.2 44.0 45.2 49.8
self-test 100.0 51.0 54.9 88.5

TAB. 1 — Experimental results for CFG, head-driven expansion and Ir1in-max selection prin-
ciple on Bod’s version of the ATIS corpus: percentage of exact match sentences in several test
conditions is given. Notice that the coverage of the three models is, by construction, the same.

Chappelier, Rajman, Rozenknop

| CF head-driven min-max
nb. of el. trees | 381 930 2434

TAB. 2 — Size of the grammar extracted from the whole Bod’s version of the ATIS corpus. For
the three grammars, there are 12 non-terminals and 31 Part-of-Speech tags.

5 Conclusion

A complete characterization of pSTSGs has been provided and other, only sufﬁcient but more
effective, polynomiality conditions have been presented. These results extend the work pre-
viously done on the Inin-max selection principle for constructing pSTSG and open promising
perspectives concerning the production of more general pSTSGs.

The head-driven expansion approach, one new example of a pSTSG has been presented and
compared to the Inin-max selection principle. This new expansion scheme consists in systema-
tically expand (at each level) the lexical head of the all subtrees of a treebank. Although its
performance on the ATIS corpus is not as good as the one obtained with the min-max selection
principle, this approach should be tested further, for instance on corpora where the notion of
“lexical head” is more pertinent.

Another research direction opened by the theoretical results presented here consists in ﬁnding
other expansion schema in order to construct other, more performant, instances of pSTSGs.

Acknowledgments The authors would like to thank Rens Bod for providing his cleaned-up version
of the ATIS corpus, Soham Mazumdar and Evi Commins for their help in the experiments.

Références

BOD R. (1992). Applying Monte Carlo techniques to Data Oriented Parsing. In Proceedings
Computational Linguistics in the Netherlands, Tilburg (The Netherlands).

BOD R. (1998). Beyond Grammar, An Experience-Based Theory of Language. Number 88 in
CSLI Lecture Notes. Standford (CA): CSLI Publications.

CHAPPELIER J .-C. & RAJMAN M. (2000). Monte-Carlo sampling for NP-hard maximization
problems in the framework of weighted parsing. In D. CHRISTODOULAKIS, Ed., Natural
Language Processing - NLP 2000, number 1835 in Lecture Notes in Artiﬁcial Intelligence, p.
106-117. Springer.

CHAPPELIER J .-C. & RAJMAN M. (2001). Polynominal tree-substitution grammars: an efﬁ-
cient framework for Data-Oriented Parsing. In Proc. of Recent Advances in Natural Language
Processing (RANLP 2001), p. 65-71.

COLLINS M. (1999). Head-Driven Statistical Models for Natural Language Parsing. PhD
thesis, University of Pennsylvania.

GOODMAN J . (1996). Efﬁcient algorithms for parsing the DOP model. In Proc. of the Conf
on Empirical Methods in Natural Language Processing, p. 143-152.

SIMA’AN K. (1996). Computational complexity of probabilistic disambiguation by means of
tree grammars. In Proceedings of COLING’96, Copenhagen (Denmark). cmp-lg/9606019.

