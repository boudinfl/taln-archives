<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
"http://www.w3.org/TR/html4/loose.dtd">
<html><head><title>Reformuler des expressions multimodales</title>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
</head>
<body>
<div style="page-break-before:always; page-break-after:always"><div><p>TALN 2002, Nancy, 24-27juin 2002 
</p>
<p>Reformuler des expressions multimodales 
</p>
<p>Elisabeth Godbert 
</p>
<p>Laboratoire d'Informatique Fondamentale de Marseille (LIF) 
Universit&#233; de la M&#233;diterran&#233;e et CNRS 
</p>
<p>163 Avenue de Luminy  - case 901 
13288 Marseille Cedex 9 - France 
E-mail : godbert@lim.univ-mrs.fr 
</p>
<p>R&#233;sum&#233; &#8211; Abstract 
</p>
<p>Le domaine des &quot;Interfaces Utilisateur Intelligentes&quot; a vu ces derni&#232;res ann&#233;es la r&#233;alisation 
de syst&#232;mes complexes mettant en oeuvre une interaction multimodale dans laquelle les 
diff&#233;rentes techniques de communication (textes, gestes, parole, s&#233;lection graphique) sont 
coordonn&#233;es en entr&#233;e et/ou en sortie. Nous nous int&#233;ressons ici aux syst&#232;mes qui prennent en 
entr&#233;e des expressions multimodales et en produisent une reformulation en une expression 
unimodale s&#233;mantiquement &#233;quivalente. Nous proposons une mod&#233;lisation du processus de 
traduction d'expressions multimodales en expressions unimodales, et nous d&#233;crivons la mise 
en oeuvre d'un processus de ce type dans un logiciel d'aide &#224; l'apprentissage du langage. 
 
These last years, &quot;Intelligent User Interfaces&quot; have been developped, in which input and/or  
output multimodality facilitates human-machine communication (written language, speech, 
graphic selection, gestures). This paper is concerned with systems in which multimodal input 
is translated into unimodal expressions semantically equivalent. A model for such a process is 
proposed. Then, an example is described, with an educational software enabling multimodal 
communication and implementing this process. 
</p>
<p>Mots Cl&#233;s &#8211;Keywords  
</p>
<p>Langage multimodal, coordination des modes, expressions s&#233;mantiquement &#233;quivalentes. 
Multimodal language, coordination of modalities, semantic equivalence. 
 
</p>
<p>1 Introduction 
</p>
<p>Dans de tr&#232;s nombreuses situations l'activit&#233; humaine peut &#234;tre qualifi&#233;e de multimodale, en 
fait d&#232;s qu'une personne fait plusieurs choses en m&#234;me temps : marcher, r&#233;fl&#233;chir, chanter, 
parler, manger ... Chercher &#224; d&#233;finir le traitement automatique d'une activit&#233; multimodale, 
c'est tenter d'&#233;laborer un syst&#232;me capable de r&#233;agir de fa&#231;on appropri&#233;e &#224; une telle activit&#233;. 
Dans toute sa g&#233;n&#233;ralit&#233;, il est infiniment complexe, il requiert en particulier : 
</p>
<p>365 </p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Elisabeth Godbert 
</p>
<p>- l'analyse en parall&#232;le des diff&#233;rents &#233;l&#233;ments asynchrones dont est compos&#233;e cette activit&#233;,  
- leur interpr&#233;tation et traduction en une information structur&#233;e, qui pourra ensuite &#234;tre trait&#233;e 
par des processus adapt&#233;s aux buts poursuivis, pour r&#233;agir de fa&#231;on ad&#233;quate &#224; l&#8217;activit&#233; prise 
en entr&#233;e. 
Pour le cas particulier de l'expression (composition d'un message destin&#233; &#224; un interlocuteur) 
on utilise tr&#232;s souvent plusieurs modalit&#233;s, combin&#233;es ou non : parole, &#233;crit, gestes, etc. 
Depuis une quinzaine d'ann&#233;es, les recherches sur la multimodalit&#233; s'int&#233;ressent 
essentiellement aux  interfaces qualifi&#233;es de multimodales et multimedia, un m&#233;dia &#233;tant 
d&#233;fini comme un dispositif servant de support &#224; l'information, et une modalit&#233; comme une 
technique d'interaction. L'un des premiers syst&#232;mes de ce type est d&#233;crit dans (Bolt, 80). Ces 
syst&#232;mes utilisent la multimodalit&#233; en entr&#233;e et/ou en sortie, et le domaine des &quot;interfaces 
utilisateur intelligentes&quot; a vu la r&#233;alisation de syst&#232;mes complexes utilisant le multim&#233;dia, et 
mettant en oeuvre une interaction multimodale dans laquelle les diff&#233;rentes techniques de 
communication (textes, gestes, parole, vision par ordinateur, images, vid&#233;o,...) sont 
combin&#233;es : on parle alors de syst&#232;me multim&#233;dia intelligent (Nigay, Coutaz, 96) (Wahlster et 
al, 93) (Cohen et al, 98). 
Le probl&#232;me auquel nous nous int&#233;ressons ici est la mod&#233;lisation d'un syst&#232;me qui prend en 
entr&#233;e des expressions multimodales, et en produit une reformulation en une expression 
unimodale s&#233;mantiquement &#233;quivalente. Dans un certain nombre de syst&#232;mes en effet, il 
s'av&#232;re n&#233;cessaire de traduire l'ensemble des entr&#233;es en une expression unimodale, celle-ci 
&#233;tant destin&#233;e &#224; &#234;tre ensuite donn&#233;e en entr&#233;e &#224; un autre module, charg&#233; de poursuivre le 
traitement. Nous mentionnons dans la partie 2 deux classes d'applications auxquelles nous 
nous int&#233;ressons, et qui requi&#232;rent ce type de traitement : les logiciels pour l'apprentissage du 
langage et les syst&#232;mes d'aide &#224; la communication pour les personnes ayant perdu l'usage de 
la parole. Nous proposons dans la partie 3 une mod&#233;lisation du processus de traduction 
d'expressions multimodales en expressions unimodales. Pour finir, nous d&#233;crivons dans la 
partie 4 la mise en oeuvre d'un processus de ce type dans un logiciel d'aide &#224; l'apprentissage 
du langage. 
</p>
<p>2 La multimodalit&#233; : une aide &#224; l'utilisateur 
</p>
<p>L'importance pratique de l'aide &#224; l'utilisateur a &#233;t&#233; soulign&#233;e dans de nombreux travaux sur la 
conception d'interfaces intelligentes, et il est clair que la multimodalit&#233; et l'aide &#224; l'utilisateur 
sont intimement li&#233;es : la multimodalit&#233; de l'interaction facilite l'expression de l'utilisateur. 
Les recherches dans ce domaine ont mis en &#233;vidence les avantages respectifs des diff&#233;rents 
modes de communication : langage naturel &#233;crit, parole, pointage graphique, gestes, etc.  
</p>
<p>2.1 Interfaces multimodales classiques 
</p>
<p>De fa&#231;on classique, une interface multimodale est d&#233;finie pour utiliser au mieux tous les outils 
habituels des interfaces graphiques (ic&#244;nes, menus d&#233;roulants, boutons, rectangles d'&#233;dition, 
etc.), auxquels s'ajoutent &#233;ventuellement un micro et diff&#233;rents capteurs pour permettre une 
interaction multimodale dans laquelle les diff&#233;rents modes et m&#233;dia sont propos&#233;s. L'id&#233;al est 
que l'interface permette d'utiliser les modalit&#233;s d'expression non pas s&#233;quentiellement mais en 
parall&#232;le, de fa&#231;on coordonn&#233;e et compl&#233;mentaire. Par exemple, l'utilisateur doit pouvoir &#224; 
tout instant choisir et combiner, parmi les modalit&#233;s &lt;clavier, langage naturel&gt;, &lt;souris, 
&#233;l&#233;ments graphiques&gt; et &lt;micro, parole&gt;, celles qu'il pr&#233;f&#232;re pour s'exprimer. 
Les r&#233;actions de l'utilisateur &#233;tant impr&#233;visibles, plus le choix est large pour les modalit&#233;s 
d'interaction, plus la conception de ce type de syst&#232;me est complexe. Elle requiert en 
</p>
<p>366 </p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Reformuler des expressions multimodales 
</p>
<p>particulier la pr&#233;sence d'un analyseur multimodal capable de s&#233;lectionner des entr&#233;es de 
diff&#233;rents types (texte, graphique, etc.) et de les fusionner en une expression regroupant le 
contenu de toutes les entr&#233;es.  
Si, de fa&#231;on analogue, on veut mettre en place la multimodalit&#233; en sortie, celle-ci passe par la 
fission de la r&#233;action du syst&#232;me, et par des m&#233;thodes de pr&#233;sentation dans laquelle les modes 
pertinents sont utilis&#233;s de fa&#231;on parall&#232;le et compl&#233;mentaire. Et la coh&#233;rence d&#8217;un dialogue 
multimodal demandera que les modes en sortie soient choisis en fonction des modes utilis&#233;s 
en entr&#233;e. 
</p>
<p>2.2 Interfaces multimodales pour handicap&#233;s 
</p>
<p>Cette vision classique de la multimodalit&#233; doit &#234;tre &#233;largie pour y inclure les syst&#232;mes adapt&#233;s 
&#224; des besoins sp&#233;cifiques des utilisateurs, et dans lesquels on utilise donc d'autres modalit&#233;s 
d'interaction. C'est le cas des interfaces d&#233;di&#233;es &#224; des personnes souffrant de handicaps. On 
parle actuellement beaucoup de &quot;droit universel pour l'acc&#232;s &#224; la soci&#233;t&#233; de l'information&quot;. De 
nombreux syst&#232;mes ont &#233;t&#233; d&#233;velopp&#233;s pour pallier &#224; la perte de facult&#233;s de l'utilisateur soit 
pour la r&#233;ception d'informations soit pour la composition de messages : syst&#232;mes de 
reconnaissance de gestes, interfaces tactiles, divers types de capteurs, aide &#224; la composition, 
synth&#232;se vocale...(voir par exemple Jacko et al, 2001). Dans le cadre de la probl&#233;matique 
&#233;tudi&#233;e dans cet article, nous nous int&#233;ressons aux syst&#232;mes d'aide &#224; la communication pour 
des personnes ayant perdu l'usage de la parole : si un tel syst&#232;me propose une interface 
multimodale qui permet la saisie d'entr&#233;es provenant de diff&#233;rents capteurs, il est n&#233;cessaire 
d'op&#233;rer la fusion de ces entr&#233;es en une expression unimodale, qui pourra ensuite &#234;tre 
propos&#233;e &#224; son destinataire sous forme de langage &#233;crit ou sous forme orale dans le cas o&#249; le 
message &#233;crit est donn&#233; en entr&#233;e &#224; un synth&#233;tiseur de la parole. 
</p>
<p>2.3 Une n&#233;cessit&#233; : pouvoir adapter la multimodalit&#233; &#224; l'utilisateur 
</p>
<p>Les syst&#232;mes con&#231;us &#224; l'heure actuelle donnent une importance grandissante au &quot;profil 
utilisateur&quot;, dont le r&#244;le premier est de permettre l'adaptation d'un syst&#232;me &#224; son utilisateur. 
En ce qui concerne la multimodalit&#233; des entr&#233;es, cette adaptation correspond &#224; la possibilit&#233; 
de choisir la ou les modalit&#233;s d'expression que l&#8217;on pr&#233;f&#232;re.  
Cette adaptation &#224; l'utilisateur est particuli&#232;rement int&#233;ressante dans les syst&#232;mes d&#233;di&#233;s &#224; des 
personnes handicap&#233;es : selon leur handicap, ces personnes peuvent choisir l'une ou l'autre 
des modalit&#233;s offertes. De plus, pour des utilisateurs atteints d'un handicap qui &#233;volue au fil 
des ann&#233;es, comme dans le cas de maladies d&#233;g&#233;n&#233;ratives, cette adaptation a une importance 
cruciale puisqu'elle permet &#224; l'utilisateur de garder le m&#234;me syst&#232;me de base, sur lequel on 
greffe, suivant les besoins, diff&#233;rentes modalit&#233;s d'interaction. 
En ce qui concerne les logiciels &#233;ducatifs, les p&#233;dagogues insistent souvent sur la n&#233;cessit&#233; de 
pouvoir faire varier la difficult&#233; des exercices, et en particulier pouvoir graduer l&#8217;aide. Par 
exemple, il faut pouvoir choisir entre un mode d'expression multimodal ou unimodal, assist&#233; 
ou non, ou entre divers degr&#233;s d'aide au niveau cognitif.  
On pourra ainsi d&#233;finir un degr&#233; de multimodalit&#233;, qui correspondra &#224; la possibilit&#233; d'utiliser 
ou non une ou plusieurs modalit&#233;s en entr&#233;e. 
</p>
<p>367 </p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Elisabeth Godbert 
</p>
<p>3 Passer d&#8217;expressions multimodales &#224; des expressions 
unimodales 
</p>
<p>Comme nous l'avons dit, nous voulons d&#233;finir un syst&#232;me qui prend en entr&#233;e des expressions 
multimodales et les traduit en expressions unimodales s&#233;mantiquement &#233;quivalentes. Nous 
proposons dans ce qui suit une mod&#233;lisation du flux des entr&#233;es multimodales, et de leur 
traduction en expressions unimodales.  
Appelons LMM le langage multimodal que l'utilisateur utilise, et LE le langage unimodal dans 
lequel doivent &#234;tre traduites les entr&#233;es. Sans rien pr&#233;juger de sa nature, LE est dit &quot;langage de 
base&quot;. On peut penser que, dans de nombreuses applications, LE est le (ou un sous-ensemble 
du) langage naturel &#233;crit. Nous en verrons un exemple dans la partie 4. 
Les expressions exprim&#233;es dans LMM sont saisies de fa&#231;on asynchrone par diff&#233;rents capteurs  
suppos&#233;s ind&#233;pendants, not&#233;s {Capt1, Capt2,,...}, qui peuvent &#234;tre le clavier, la souris, un 
micro, ou tout autre type de capteur. Notons que faire varier le degr&#233; de multimodalit&#233;, pour 
adapter la multimodalit&#233; &#224; l'utilisateur, correspond simplement &#224; l'activation ou la non-
activation de chaque capteur. 
Appelons {m1, m2,...} les messages, indic&#233;s par leur ordre d'arriv&#233;e, que re&#231;oivent les 
capteurs : &#224; la date ti, le signal Si marque la fin de la composition du message mi et donc son 
entr&#233;e pour le traitement qui doit suivre (on vide alors le buffer du capteur consid&#233;r&#233;, qui 
devient pr&#234;t &#224; recevoir  un &#233;ventuel message suivant). 
Notons m&#8217;i la traduction dans LE du message mi. Nous supposerons que cette traduction est 
instantan&#233;e. Et notons {c0, c1,...} les expressions unimodales produites par le syst&#232;me au fur et 
&#224; mesure que les entr&#233;es sont saisies, traduites, et fusionn&#233;es avec le r&#233;sultat du traitement des 
messages pr&#233;c&#233;dents. Par d&#233;finition, l'expression unimodale ci est &#233;quivalente &#224; l'ensemble 
des messages {m1, ... mi }. Dans ce qui suit, l'op&#233;ration de fusion est not&#233;e &quot;+&quot;. 
La figure1 illustre le cas le plus simple : la nouvelle expression courante ci est obtenue par la 
fusion de l'expression courante ci-1 et de la traduction m&#8217;i du message mi.  
Dans ce cas, l'&#233;quation de fusion s'&#233;crit : ci =  ci-1 + m&#8217;i . 
 
 
</p>
<p> 
</p>
<p>Figure 1 : Passer d'une expression multimodale &#224; une expression unimodale 
</p>
<p>368 </p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Reformuler des expressions multimodales 
</p>
<p>L'&#233;tat du syst&#232;me &#233;volue donc de la fa&#231;on suivante : 
- &#224; l&#8217;instant ti, date d&#8217;arriv&#233;e d&#8217;un signal Si annon&#231;ant la saisie d&#8217;un message mi sur un 
capteur, l&#8217;expression unimodale courante est ci-1 ; 
 - &#224; partir de ti, (et instantan&#233;ment), le message mi est traduit en m&#8217;i et fusionn&#233; avec 
l&#8217;expression courante, pour donner ci. 
</p>
<p>Exemple : dans un syst&#232;me qui combine la saisie de texte au clavier et la s&#233;lection graphique 
de mots, la fusion peut tout simplement &#234;tre la concat&#233;nation de cha&#238;nes de caract&#232;res. 
D'autres situations, moins simples, peuvent se pr&#233;senter, en particulier lorsque la fusion d'un 
message mi avec l'expression ci-1 ne peut pas se faire directement pour obtenir une expression 
unimodale &#233;quivalente ; il faut alors attendre la saisie d'un ou plusieurs messages succ&#233;dant &#224; 
mi pour effectuer en m&#234;me temps la fusion de ces messages avec ci (un exemple est d&#233;crit 
dans la partie 4). Dans ce cas, l'&#233;quation de fusion s'&#233;crit :  ci+k =  ci-1+ m&#8217;i  + m&#8217;i+1  + ... + m&#8217;i+k  
</p>
<p>4 La multimodalit&#233; dans le syst&#232;me &#233;ducatif EREL 
</p>
<p>Le projet EREL d&#233;velopp&#233; ces derni&#232;res ann&#233;es, a pour objectif le d&#233;veloppement d'un 
syst&#232;me pour l'&#233;ducation et/ou la r&#233;&#233;ducation du langage et de la cognition chez des enfants 
pr&#233;sentant des troubles du d&#233;veloppement (Godbert, 1998). EREL est un syst&#232;me multimodal 
qui propose un ensemble d'exercices ludiques de type piag&#233;tien con&#231;us pour stimuler et aider 
l'utilisateur &#224; employer le langage naturel &#233;crit pour s'exprimer autour de diff&#233;rents th&#232;mes 
illustr&#233;s &#224; l'&#233;cran. C&#8217;est un syst&#232;me r&#233;actif dans lequel chaque exercice met en jeu un micro-
monde d'objets graphiques, et est organis&#233; pour que l'utilisateur s'exprime par des phrases 
simples, appel&#233;es requ&#234;tes, que le syst&#232;me l'aide &#224; composer et auxquelles il r&#233;agit 
(d&#233;placement, cr&#233;ation ou suppression d&#8217;objets graphiques). Un langage multimodal 
appropri&#233; est associ&#233; &#224; chaque jeu pour la composition de ces requ&#234;tes.  
Suivant le degr&#233; de multimodalit&#233; choisi, la d&#233;signation d&#8217;un objet &#224; l&#8217;&#233;cran peut se faire soit 
par une simple s&#233;lection graphique, soit par du texte, soit par une combinaison des deux 
modes. Par exemple, l'une des activit&#233;s propos&#233;es par EREL met en jeu des pions de 
diff&#233;rentes formes et couleurs plac&#233;s sur un damier. Selon la situation, l'utilisateur peut 
d&#233;signer une case par diff&#233;rentes expressions multimodales s&#233;mantiquement &#233;quivalentes : 
(Pose la croix) en E4, ou en &lt;clic&gt;, ou dans la case &lt;clic&gt;, ou &#224; droite du triangle bleu, etc.   
Au fur et &#224; mesure que l'utilisateur compose sa requ&#234;te, celle-ci appara&#238;t, sous forme &#233;crite, &#224; 
l'&#233;cran. L&#8217;op&#233;ration de fusion d&#8217;expressions textuelles et graphiques correspond dans EREL &#224; 
la g&#233;n&#233;ration d&#8217;expressions d&#233;finies pertinentes du langage naturel qui d&#233;notent des &#233;l&#233;ments 
qui ont &#233;t&#233; d&#233;sign&#233;s (en partie ou enti&#232;rement) graphiquement.  Un certain nombre de choix 
ont &#233;t&#233; faits parmi ces expressions discriminantes qu'il est possible de g&#233;n&#233;rer dans chaque 
cas. Ainsi, si l'utilisateur clique dans la case E5 du damier, en &lt;clic&gt; sera traduit par en E5, et 
dans la case &lt;clic&gt; sera traduit par dans la case E5 ; cette &lt;clic&gt; case  sera traduit par la 
case E5, cette &lt;clic&gt; colonne sera traduit par la colonne 5. Dans le cas o&#249; une &#233;toile verte se 
trouve en E5, nous avons choisi de traduire cette &lt;clic&gt; &#233;toile, soit par l'&#233;toile verte s'il n'y a 
qu'une &#233;toile verte dans le jeu, soit par l'&#233;toile qui se trouve en E5  dans le cas contraire.  
On voit ici que pour certaines expressions d&#233;ictiques, le clic n'est pas interpr&#233;t&#233; directement 
en langage &#233;crit d&#232;s qu'il est produit : il faut attendre le mot suivant (case, colonne, &#233;toile, ...) 
pour g&#233;n&#233;rer une expression d&#233;finie ad&#233;quate. 
 
Pour chaque activit&#233; propos&#233;e par EREL le &quot;langage de base&quot; LE est un sous-langage du 
langage naturel &#233;crit. Le langage multimodal LMM est une extension de LE, c'est l'ensemble 
des requ&#234;tes multimodales que le syst&#232;me sait traiter. L'extension LMM de LE est d&#233;finie de la 
</p>
<p>369 </p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Elisabeth Godbert 
</p>
<p>370 
</p>
<p>fa&#231;on suivante : on ajoute &#224; la grammaire de LE des r&#232;gles selon lesquelles certaines 
cat&#233;gories syntaxiques peuvent se d&#233;river en des expressions graphiques : des &quot;morceaux de 
phrases&quot; peuvent donc &#234;tre exprim&#233;s soit par du texte, soit par un pointage graphique, soit par 
une combinaison des deux. Dans la version actuelle d&#8217;EREL, les langages LMM sont 
suffisamment contraints pour que l&#8217;on &#233;vite toute ambigu&#239;t&#233; ou conflit entre les deux 
modalit&#233;s. 
Pour le traitement du langage &#233;crit, nous utilisons les outils offerts par le syst&#232;me ILLICO 
(Pasero et Sabatier, 1999) : en premier lieu des formalismes pour d&#233;finir des langages par des 
donn&#233;es lexicales, syntaxiques, s&#233;mantiques, conceptuelles et contextuelles ; ensuite, des 
algorithmes d'analyse/synth&#232;se de phrases ; enfin, des algorithmes pour la recherche des 
r&#233;f&#233;rents et pour le calcul de la repr&#233;sentation logique de la s&#233;mantique des phrases. 
</p>
<p>5 Conclusion 
</p>
<p>Nous avons propos&#233; un mod&#232;le pour la saisie d'expressions multimodales et pour leur 
traduction en expressions unimodales, pour que celles-ci puissent ensuite &#234;tre prises en entr&#233;e 
par un autre module de traitement du langage. Nous pensons en effet que cette traduction 
s'av&#232;re n&#233;cessaire dans de nombreux syst&#232;mes. Nous en avons &#233;voqu&#233; deux classes : les 
syst&#232;mes d'aide &#224; l'apprentissage du langage et les syst&#232;mes d'aide &#224; la communication pour 
les personnes handicap&#233;es. Pour finir, nous avons d&#233;crit comment ce processus de traduction 
a &#233;t&#233; mis en oeuvre dans le syst&#232;me &#233;ducatif EREL. La multimodalit&#233; propos&#233;e dans ce 
syst&#232;me, bien que tr&#232;s modeste, met en &#233;vidence des probl&#232;mes int&#233;ressants relatifs &#224; la 
g&#233;n&#233;ration d'expressions discriminantes d&#233;notant des objets d&#233;sign&#233;s par des expressions 
multimodales combinant texte et s&#233;lection graphique. 
</p>
<p>R&#233;f&#233;rences 
</p>
<p>Bolt R.A., (1980). &quot;Put-That-There&quot; : Voice and Gesture at the Graphics Interface. Computer 
Graphics, 14(3). Also in Intelligent User Interfaces, Maybury M.T. and Wahlster W. (eds), 
Morgan Kaufmann Publishers, San Francisco, 1998, pp 19-27. 
Cohen P.R., Johnson M., McGee D., Oviatt S., Pittman J., Smith I., Chen L., and Clow J., 
(1998). &quot;Multimodal Interaction for Distributed Interactive Simulation&quot;, in Intelligent User 
Interfaces, Maybury M.T. and Wahlster W. (eds), Morgan Kaufmann Publishers, San 
Francisco, 1998, pp 562-571. 
Godbert E., (1998). &quot;EREL : a multimedia CALL system devoted to children with language 
disorders&quot;. In Keith Cameron Ed., Multimedia CALL: Theory and Practice, Elm Bank 
Publications, Exeter, England, 1998, pp 207-216. 
Jacko J.A., Vitense H.S., (2001). &quot;A review and reappraisal of information technologies 
within a conceptual framework for individuals with disabilities&quot;. UAIS Journal, Springer 
Verlag 2001, 1, pp 56-76. 
Nigay L., Coutaz J., (1996). &quot;Espaces conceptuels pour l'interaction multim&#233;dia et 
multimodale&quot;. Techniques et Science Informatiques, vol. 15, n&#176; 9, 1996, pp 1195-1225. 
Pasero R., Sabatier P., (1999). &quot;Specifying and Processing Constraints on Formal 
Representations of Sentences&quot;, Proceedings of the 6th International Conference on Natural 
Language Understanding and Logic Programming, NLULP 99, Las Cruces,  pp 33-44. 
Wahlster W., Andr&#233; E., Finkler W., Profitlich H.J., Rist T., (1993). &quot;Plan-based integration of 
Natural Language and Graphics Generation&quot;. Artificial Intelligence, 1993, n&#176;63, pp 387-427.  
 </p>

</div></div>
</body></html>