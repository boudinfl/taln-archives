TALN 2002, Nancy, 24-2 7juin 2002

Reformuler des expressions multimodales

Elisabeth Godbert

Laboratoire d'Informatique Fondamentale de Marseille (LIF)
Université de la Méditerranée et CNRS
163 Avenue de Luminy - case 901

13288 Marseille Cedex 9 - France
E-mail : godbert@lim.univ-mrs.fr

Résumé — Abstract

Le domaine des "Interfaces Utilisateur Intelligentes" a vu ces dernieres années la realisation
de systemes complexes mettant en oeuvre une interaction multimodale dans laquelle les
différentes techniques de communication (textes, gestes, parole, sélection graphique) sont
coordonnées en entrée et/ou en sortie. Nous nous intéressons ici aux systemes qui prennent en
entrée des expressions multimodales et en produisent une reformulation en une expression
unimodale sémantiquement équivalente. Nous proposons une modélisation du processus de
traduction d'expressions multimodales en expressions unimodales, et nous décrivons la mise
en oeuvre d'un processus de ce type dans un logiciel d'aide a l'apprentissage du langage.

These last years, "Intelligent User Interfaces" have been developped, in which input and/or
output multimodality facilitates human-machine communication (written language, speech,
graphic selection, gestures). This paper is concerned with systems in which multimodal input
is translated into unimodal expressions semantically equivalent. A model for such a process is
proposed. Then, an example is described, with an educational software enabling multimodal
communication and implementing this process.

Mots Clés —Keywords

Langage multimodal, coordination des modes, expressions sémantiquement équivalentes.
Multimodal language, coordination of modalities, semantic equivalence.

1 Introduction

Dans de tres nombreuses situations l'activité humaine peut étre qualifiée de multimodale, en
fait des qu'une personne fait plusieurs choses en méme temps : marcher, réﬂéchir, chanter,
parler, manger  Chercher a défmir le traitement automatique d'une activité multimodale,
c'est tenter d'élaborer un systeme capable de réagir de facon appropriée a une telle activité.
Dans toute sa généralité, il est infmiment complexe, il requiert en particulier :

365

Elisabeth Godbert

- l'analyse en parallele des différents éléments asynchrones dont est composée cette activité,

- leur interprétation et traduction en une information structurée, qui pourra ensuite étre traitée
par des processus adaptés aux buts poursuivis, pour réagir de facon adéquate a l’activité prise
en entrée.

Pour le cas particulier de l'expression (composition d'un message destiné a un interlocuteur)
on utilise tres souvent plusieurs modalités, combinées ou non : parole, écrit, gestes, etc.
Depuis une quinzaine d'années, les recherches sur la multimodalité s'intéressent
essentiellement aux interfaces qualifiées de multimodales et multimedia, un média étant
défini comme un dispositif servant de support a l'inforrnation, et une modalité comme une
technique d'interaction. L'un des premiers systemes de ce type est décrit dans (Bolt, 80). Ces
systemes utilisent la multimodalité en entrée et/ou en sortie, et le domaine des "interfaces
utilisateur intelligentes" a vu la réalisation de systemes complexes utilisant le multimédia, et
mettant en oeuvre une interaction multimodale dans laquelle les différentes techniques de
communication (textes, gestes, parole, vision par ordinateur, images, Vidéo,...) sont
combinées : on parle alors de systeme multimédia intelligent (Nigay, Coutaz, 96) (VVahlster et
al, 93) (Cohen et al, 98).

Le probleme auquel nous nous intéressons ici est la modélisation d'un systeme qui prend en
entrée des expressions multimodales, et en produit une reformulation en une expression
unimodale sémantiquement équivalente. Dans un certain nombre de systemes en effet, il
s'avere nécessaire de traduire l'ensemble des entrées en une expression unimodale, celle-ci
étant destinée a étre ensuite donnée en entrée a un autre module, chargé de poursuivre le
traitement. Nous mentionnons dans la partie 2 deux classes d'applications auxquelles nous
nous intéressons, et qui requierent ce type de traitement : les logiciels pour l'apprentissage du
langage et les systemes d'aide a la communication pour les personnes ayant perdu l'usage de
la parole. Nous proposons dans la partie 3 une modélisation du processus de traduction
d'expressions multimodales en expressions unimodales. Pour finir, nous décrivons dans la
partie 4 la mise en oeuvre d'un processus de ce type dans un logiciel d'aide a l'apprentissage
du langage.

2 La multimodalité : une aide £1 l'utilisateur

L'importance pratique de l'aide a l'utilisateur a été soulignée dans de nombreux travaux sur la
conception d'interfaces intelligentes, et il est clair que la multimodalité et l'aide a l'utilisateur
sont intimement liées : la multimodalité de l'interaction facilite l'expression de l'utilisateur.
Les recherches dans ce domaine ont mis en évidence les avantages respectifs des différents
modes de communication : langage naturel écrit, parole, pointage graphique, gestes, etc.

2.1 Interfaces multimodales classiques

De facon classique, une interface multimodale est définie pour utiliser au mieux tous les outils
habituels des interfaces graphiques (ic6nes, menus déroulants, boutons, rectangles d'édition,
etc.), auxquels s'ajoutent éventuellement un micro et différents capteurs pour permettre une
interaction multimodale dans laquelle les différents modes et média sont proposés. L'idéal est
que l'interface permette d'utiliser les modalités d'expression non pas séquentiellement mais en
parallele, de facon coordonnée et complémentaire. Par exemple, l'utilisateur doit pouvoir a
tout instant choisir et combiner, parmi les modalités <clavier, langage naturel>, <souris,
éléments graphiques> et <micro, parole>, celles qu'il préfere pour s'exprimer.

Les réactions de l'utilisateur étant imprévisibles, plus le choix est large pour les modalités
d'interaction, plus la conception de ce type de systeme est complexe. Elle requiert en

366

Reformuler des expressions multimodales

particulier la présence d'un analyseur multimodal capable de sélectionner des entrées de
différents types (texte, graphique, etc.) et de les fusionner en une expression regroupant le
contenu de toutes les entrées.

Si, de facon analogue, on Veut mettre en place la multimodalité en sortie, celle-ci passe par la
fission de la réaction du systeme, et par des méthodes de présentation dans laquelle les modes
pertinents sont utilisés de facon parallele et complémentaire. Et la cohérence d’un dialogue
multimodal demandera que les modes en sortie soient choisis en fonction des modes utilisés
en entrée.

2.2 Interfaces multimodales pour handicapés

Cette Vision classique de la multimodalité doit étre élargie pour y inclure les systemes adaptés
a des besoins spécifiques des utilisateurs, et dans lesquels on utilise donc d'autres modalités
d'interaction. C'est le cas des interfaces dédiées a des personnes souffrant de handicaps. On
parle actuellement beaucoup de "droit universel pour l'acces a la société de l'information". De
nombreux systemes ont été développés pour pallier a la perte de facultés de l'utilisateur soit
pour la réception d'inforrnations soit pour la composition de messages : systemes de
reconnaissance de gestes, interfaces tactiles, divers types de capteurs, aide a la composition,
synthese Vocale...(Voir par exemple Jacko et al, 2001). Dans le cadre de la problématique
étudiée dans cet article, nous nous intéressons aux systemes d'aide a la communication pour
des personnes ayant perdu l'usage de la parole : si un tel systeme propose une interface
multimodale qui permet la saisie d'entrées provenant de différents capteurs, il est nécessaire
d'opérer la fusion de ces entrées en une expression unimodale, qui pourra ensuite étre
proposée a son destinataire sous forme de langage écrit ou sous forme orale dans le cas ou le
message écrit est donné en entrée a un synthétiseur de la parole.

2.3 Une nécessité : pouvoir adapter la multimodalité it l'utilisateur

Les systemes concus a l'heure actuelle donnent une importance grandissante au "proﬁl
utilisateur", dont le role premier est de permettre l'adaptation d'un systeme a son utilisateur.
En ce qui concerne la multimodalité des entrées, cette adaptation correspond a la possibilité
de choisir la ou les modalités d'expression que l’on préfere.

Cette adaptation a l'utilisateur est particulierement intéressante dans les systemes dédiés a des
personnes handicapées : selon leur handicap, ces personnes peuvent choisir l'une ou l'autre
des modalités offertes. De plus, pour des utilisateurs atteints d'un handicap qui évolue au ﬁl
des années, comme dans le cas de maladies dégénératives, cette adaptation a une importance
cruciale puisqu'elle permet a l'utilisateur de garder le méme systeme de base, sur lequel on
greffe, suivant les besoins, différentes modalités d'interaction.

En ce qui concerne les logiciels éducatifs, les pédagogues insistent souvent sur la nécessité de
pouvoir faire Varier la difficulté des exercices, et en particulier pouvoir graduer l’aide. Par
exemple, il faut pouvoir choisir entre un mode d'expression multimodal ou unimodal, assisté
ou non, ou entre divers degrés d'aide au niveau cognitif.

On pourra ainsi définir un degré de multimodalité, qui correspondra a la possibilité d'utiliser
ou non une ou plusieurs modalités en entrée.

367

Elisabeth Godbert

3 Passer d’expressions multimodales £1 des expressions
unimodales

Comme nous l'aVons dit, nous voulons définir un systéme qui prend en entrée des expressions
multimodales et les traduit en expressions unimodales sémantiquement équivalentes. Nous
proposons dans ce qui suit une modélisation du ﬂux des entrées multimodales, et de leur
traduction en expressions unimodales.

Appelons LMM le langage multimodal que l'utilisateur utilise, et LE le langage unimodal dans
lequel doivent étre traduites les entrées. Sans rien préjuger de sa nature, LE est dit "langage de
base". On peut penser que, dans de nombreuses applications, LE est le (ou un sous-ensemble
du) langage naturel écrit. Nous en verrons un exemple dans la partie 4.

Les expressions exprimées dans LMM sont saisies de facon asynchrone par différents capteurs
supposés indépendants, notés {Capti, Capt2,,...}, qui peuvent étre le clavier, la souris, un
micro, ou tout autre type de capteur. Notons que faire Varier le degré de multimodalité, pour
adapter la multimodalité a l'utilisateur, correspond simplement a l'actiVation ou la non-
activation de chaque capteur.

Appelons {mi, m2,...} les messages, indicés par leur ordre d'arriVée, que recoivent les
capteurs : a la date ti, le signal Si marque la fin de la composition du message mi et donc son
entrée pour le traitement qui doit suivre (on vide alors le buffer du capteur considéré, qui
devient prét a recevoir un éventuel message suivant).

Notons m’i la traduction dans LE du message mi. Nous supposerons que cette traduction est
instantanée. Et notons {c0, ci,...} les expressions unimodales produites par le systéme au fur et
a mesure que les entrées sont saisies, traduites, et fusionnées avec le résultat du traitement des
messages précédents. Par définition, l'expression unimodale ci est équivalente a l'ensemble
des messages {mi,  mi }. Dans ce qui suit, l'opération de fusion est notée "+".

La figurel illustre le cas le plus simple : la nouvelle expression courante ci est obtenue par la
fusion de l'expression courante ci_i et de la traduction m’i du message mi.

Dans ce cas, l'équation de fusion s'écrit : ci= ci_i + m’i.

T1 I22 ‘t3 I24 tem_E5
: g : :
capteurs : : : :
I I I I
I 5 I I 5'

Captk ————————  ——————————————— 
I m2 I m4 I
I I I
I ' I
Captj . . . . . . . --4 J:_---

I
I

traduction _ _ _ _ _ _ _ _ _ _ _ __ ___
cle LMM vars LE :m'1| IIEI: :m'3| E

' I _I I _' I _'
message____ ‘I: I: : ___,
unimodal J _| _ _
exprimé 90 C1 C2 C3
EIEFISLE (c1=c:O+m'1) [c2=[:-|+m'2) (c3=c2+m'3)

Figure 1 : Passer d'une expression multimodale a une expression unimodale

368

Reformuler des expressions multimodales

L'état du systeme évolue donc de la facon suivante :
- a l’instant ti, date d’arriVée d’un signal Si annoncant la saisie d’un message mi sur un
capteur, l’expression unimodale courante est ci_i ;
- a partir de ti, (et instantanément), le message mi est traduit en m’i et fusionné avec
l’expression courante, pour donner ci.
Exemple : dans un systeme qui combine la saisie de texte au clavier et la sélection graphique
de mots, la fusion peut tout simplement étre la concaténation de chaines de caracteres.
D'autres situations, moins simples, peuvent se présenter, en particulier lorsque la fusion d'un
message mi avec l'expression ci_i ne peut pas se faire directement pour obtenir une expression
unimodale équivalente ; il faut alors attendre la saisie d'un ou plusieurs messages succédant a
mi pour effectuer en méme temps la fusion de ces messages avec ci (un exemple est décrit
dans la partie 4). Dans ce cas, l'équation de fusion s'écrit : ci+k= ci_i+ m’i + m’i+i +  + m’i+k

4 La multimodalité dans le systéme éducatif EREL

Le projet EREL développé ces dernieres années, a pour objectif le développement d'un
systeme pour l'éducation et/ou la rééducation du langage et de la cognition chez des enfants
présentant des troubles du développement (Godbert, 1998). EREL est un systeme multimodal
qui propose un ensemble d'exercices ludiques de type piagétien concus pour stimuler et aider
l'utilisateur a employer le langage naturel écrit pour s'exprimer autour de différents themes
illustrés a l'écran. C’est un systeme réactif dans lequel chaque exercice met en jeu un micro-
monde d'objets graphiques, et est organisé pour que l'utilisateur s'exprime par des phrases
simples, appelées requétes, que le systeme l'aide a composer et auxquelles il réagit
(déplacement, création ou suppression d’objets graphiques). Un langage multimodal
approprié est associé a chaque jeu pour la composition de ces requétes.

Suivant le degré de multimodalité choisi, la désignation d’un obj et a l’écran peut se faire soit
par une simple sélection graphique, soit par du texte, soit par une combinaison des deux
modes. Par exemple, l'une des activités proposées par EREL met en jeu des pions de
différentes formes et couleurs placés sur un damier. Selon la situation, l'utilisateur peut
désigner une case par différentes expressions multimodales sémantiquement équivalentes :
(Pose la croix) en E4, ou en <clic>, ou dans la case <clic>, ou d droite du triangle bleu, etc.
Au fur et a mesure que l'utilisateur compose sa requéte, celle-ci apparait, sous forme écrite, a
l'écran. L’opération de fusion d’expressions textuelles et graphiques correspond dans EREL a
la génération d’expressions définies pertinentes du langage naturel qui dénotent des éléments
qui ont été désignés (en partie ou entierement) graphiquement. Un certain nombre de choix
ont été faits parmi ces expressions discriminantes qu'il est possible de générer dans chaque
cas. Ainsi, si l'utilisateur clique dans la case E5 du damier, en <clic> sera traduit par en E5, et
dans la case <clic> sera traduit par dans la case E5 ; cette <clic> case sera traduit par la
case E5, cette <clic> colonne sera traduit par la colonne 5. Dans le cas ou une étoile Verte se
trouve en E5, nous avons choisi de traduire cette <clic> étoile, soit par l’ét0ile verte s'il n'y a
qu'une étoile Verte dans le jeu, soit par l'ét0ile qui se trouve en E5 dans le cas contraire.

On Voit ici que pour certaines expressions déictiques, le clic n'est pas interprété directement
en langage écrit des qu'il est produit : il faut attendre le mot suivant (case, colonne, étoile, ...)
pour générer une expression définie adéquate.

Pour chaque activité proposée par EREL le "langage de base" LE est un sous-langage du
langage naturel écrit. Le langage multimodal LMM est une extension de LE, c'est l'ensemble
des requétes multimodales que le systeme sait traiter. L'extension LMM de LE est définie de la

369

Elisabeth Godbert

facon suivante : on ajoute a la grammaire de LE des regles selon lesquelles certaines
catégories syntaxiques peuvent se dériver en des expressions graphiques : des "morceaux de
phrases" peuvent donc étre exprimés soit par du texte, soit par un pointage graphique, soit par
une combinaison des deux. Dans la version actuelle d’EREL, les langages LMM sont
suffisamment contraints pour que l’on évite toute ambigu'1'té ou conﬂit entre les deux
modalités.

Pour le traitement du langage écrit, nous utilisons les outils offerts par le systeme ILLICO
(Pasero et Sabatier, 1999) : en premier lieu des formalismes pour définir des langages par des
données lexicales, syntaxiques, sémantiques, conceptuelles et contextuelles ; ensuite, des
algorithmes d'analyse/synthese de phrases; enfin, des algorithmes pour la recherche des
référents et pour le calcul de la représentation logique de la sémantique des phrases.

5 Conclusion

Nous avons proposé un modele pour la saisie d'expressions multimodales et pour leur
traduction en expressions unimodales, pour que celles-ci puissent ensuite étre prises en entrée
par un autre module de traitement du langage. Nous pensons en effet que cette traduction
s'avere nécessaire dans de nombreux systemes. Nous en avons évoqué deux classes : les
systemes d'aide a l'apprentissage du langage et les systemes d'aide a la communication pour
les personnes handicapées. Pour finir, nous avons décrit comment ce processus de traduction
a été mis en oeuvre dans le systeme éducatif EREL. La multimodalité proposée dans ce
systeme, bien que tres modeste, met en évidence des problemes intéressants relatifs a la
génération d'expressions discriminantes dénotant des objets désignés par des expressions
multimodales combinant texte et sélection graphique.

Références

Bolt R.A., (1980). "Put-That-There" : Voice and Gesture at the Graphics Interface. Computer
Graphics, 14(3). Also in Intelligent User Interfaces, Maybury M.T. and Wahlster W. (eds),
Morgan Kaufmann Publishers, San Francisco, 1998, pp 19-27.

Cohen P.R., Johnson M., McGee D., Oviatt S., Pittman J., Smith 1., Chen L., and Clow J.,
(1998). "Multimodal Interaction for Distributed Interactive Simulation", in Intelligent User
Interfaces, Maybury M.T. and Wahlster W. (eds), Morgan Kaufmann Publishers, San
Francisco, 1998, pp 562-571.

Godbert E., (1998). "EREL : a multimedia CALL system devoted to children with language
disorders". In Keith Cameron Ed., Multimedia CALL: Ilzeory and Practice, Elm Bank
Publications, Exeter, England, 1998, pp 207-216.

Jacko J.A., Vitense H.S., (2001). "A review and reappraisal of information technologies
within a conceptual framework for individuals with disabilities". UAIS Journal, Springer
Verlag 2001, 1, pp 56-76.

Nigay L., Coutaz J., (1996). "Espaces conceptuels pour l'interaction multimédia et
multimodale". Techniques et Science Inforrnatiques, vol. 15, n° 9, 1996, pp 1195-1225.
Pasero R., Sabatier P., (1999). "Specifying and Processing Constraints on Formal
Representations of Sentences", Proceedings of the 6th International Conference on Natural
Language Understanding and Logic Programming, NLULP 99, Las Cruces, pp 33-44.
Wahlster W., André E., Finkler W., Profitlich H.J., Rist T., (1993). "Plan-based integration of
Natural Language and Graphics Generation". Artiﬁcial Intelligence, 1993, n°63, pp 387-427.

370

