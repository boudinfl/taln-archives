<?xml version="1.0" encoding="UTF-8"?>
<!-- Fichiers problématiques OCRisés: taln-2003-long-020, taln-2003-poster-006 -->
<conference>
	<edition>
		<acronyme>TALN'2003</acronyme>
		<titre>10ème conférence sur le Traitement Automatique des Langues Naturelles</titre>
		<ville>Batz-sur-Mer</ville>
		<pays>France</pays>
		<dateDebut>2003-06-11</dateDebut>
		<dateFin>2003-06-14</dateFin>
		<presidents>
			<president>
				<prenom>Béatrice</prenom>
				<nom>Daille</nom>
			</president>
			<president>
				<prenom>Emmanuel</prenom>
				<nom>Morin</nom>
			</president>
		</presidents>
		<typeArticles>
			<type id="long">Papiers longs</type>
			<type id="poster">Posters</type>
			<type id="tutoriel">Tutoriels</type>
		</typeArticles>
		<siteWeb></siteWeb>
	</edition>
	<articles>
		<article id="taln-2003-long-001" session="">
			<auteurs>
				<auteur>
					<prenom>Jean-Yves</prenom>
					<nom>Antoine</nom>
					<email>Jean-Yves.Antoine@univ-ubs.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Jérôme</prenom>
					<nom>Goulian</nom>
					<email>Jérôme.Goulian@univ-ubs.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Jeanne</prenom>
					<nom>Villaneau</nom>
					<email>Jeanne.Villaneau@univ-ubs.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Laboratoire VALORIA  Université de Bretagne Sud Rue Yves Mainguy, F-56000 Vannes</affiliation>
			</affiliations>
			<titre>Quand le TAL robuste sattaque au langage parlé : analyse incrémentale pour la compréhension de la parole spontanée </titre>
			<type>long</type>
			<pages>25-34</pages>
			<resume>Dans cet article, nous discutons de lapplication au langage parlé des techniques danalyse syntaxique robuste développées pour lécrit. Nous présentons deux systèmes de compréhension de parole spontané en situation de dialogue homme-machine finalisé, dont les performances montrent la pertinence de ces méthodes pour atteindre une compréhension fine et robuste des énoncés oraux.</resume>
			<mots_cles>Analyse syntaxique robuste, compréhension de la parole, dialogue homme-machine</mots_cles>
			<title></title>
			<abstract>This papers discusses the relevance of robust parsing techniques for the analysis of spontaneous spoken language. It presents two speech understanding systems devoted to human-machine communication that implement such robust methods. Their performances suggest that robust parsing should apply usefully to spontaneous speech.</abstract>
			<keywords>Robust parsing, spoken language understanding, human-machine dialogue</keywords>
		</article>
		<article id="taln-2003-long-002" session="">
			<auteurs>
				<auteur>
					<prenom>Laurent</prenom>
					<nom>Audibert</nom>
					<email>laurent.audibert@up.univ-aix.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Jeune équipe DELIC  Université de Provence 29 Av. Robert SCHUMAN - 13621 Aix-en-Provence Cedex 1</affiliation>
			</affiliations>
			<titre>Etude des critères de désambiguïsation sémantique automatique : résultats sur les cooccurrences</titre>
			<type>long</type>
			<pages>35-44</pages>
			<resume>Nous présentons dans cet article une étude sur les critères de désambiguïsation sémantique automatique basés sur les cooccurrences. Lalgorithme de désambiguïsation utilisé est du type liste de décision, il sélectionne une cooccurrence unique supposée véhiculer l’information la plus fiable dans le contexte ciblé. Cette étude porte sur 60 vocables répartis, de manière égale, en trois classes grammaticales (nom, adjectif et verbe) avec une granularité fine au niveau des sens. Nous commentons les résultats obtenus par chacun des critères évalués de manière indépendante et nous nous intéressons aux particularités qui différencient les trois classes grammaticales étudiées. Cette étude sappuie sur un corpus français étiqueté sémantiquement dans le cadre du projet SyntSem.</resume>
			<mots_cles>Désambiguïsation sémantique automatique, corpus sémantiquement étiqueté, cooccurrences</mots_cles>
			<title></title>
			<abstract>This paper describes a study on cooccurrence-based criteria for automatic word sense disambiguation. We use a decision-list algorithm which selects the best disambiguating cue in the target context. The algorithm is tested on 60 words equally distributed among three parts of speech (noun, adjective and verb) with a fine sense granularity. We present the results obtained by each criterion evaluated in an independent way and we discuss the characteristics which differentiate the three parts of speech studied. The study uses a French sense-tagged corpus developed in the SyntSem project.</abstract>
			<keywords>Word sense disambiguation, sense tagged corpora, cooccurrences</keywords>
		</article>
		<article id="taln-2003-long-003" session="">
			<auteurs>
				<auteur>
					<prenom>Armelle</prenom>
					<nom>Brun</nom>
					<email>brun@loria.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Kamel</prenom>
					<nom>Smaïli</nom>
					<email>smaili@loria.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Jean-Paul</prenom>
					<nom>Haton</nom>
					<email>jph@loria.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LORIA BP 239 54506 Vandoeuvre-Lès-Nancy, France</affiliation>
			</affiliations>
			<titre>Nouvelle approche de la sélection de vocabulaire pour la détection de thème</titre>
			<type>long</type>
			<pages>45-54</pages>
			<resume>En reconnaissance de la parole, un des moyens d’améliorer les performances des systèmes est de passer par l’adaptation des modèles de langage. Une étape cruciale de ce processus consiste à détecter le thème du document traité et à adapter ensuite le modèle de langage. Dans cet article, nous proposons une nouvelle approche de création des vocabulaires utilisés pour la détection de thème. Cette dernière est fondée sur le développement de vocabulaires spécifiques et caractéristiques des différents thèmes. Nous montrons que cette approche permet non seulement d’améliorer les performances des méthodes, mais exploite également des vocabulaires de taille réduite. De plus, elle permet d’améliorer de façon très significative les performances de méthodes de détection lorsqu’elles sont combinées.</resume>
			<mots_cles>Détection de thème, création de vocabulaire, combinaison</mots_cles>
			<title></title>
			<abstract>One way to improve performance of Automatic Speech Recognition (ASR) systems consists in adapting language models. We are particularly interested in adapting language models to the topic related in data. Before adapting the language model, this topic has to be detected. In this work, we present a new way to create vocabularies used to detect the topic in a given text. This new method results in the improvement of topic detection performance of the methods studied, it also results in the reduction of the vocabulary size required. Finally, we show a large improvement of the performance when combining topic identification methods, when new vocabularies are used.</abstract>
			<keywords>Topic detection, vocabulary creation, combination</keywords>
		</article>
		<article id="taln-2003-long-004" session="">
			<auteurs>
				<auteur>
					<prenom>Jacques</prenom>
					<nom>Chauché</nom>
					<email>chauche@lirmm.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Violaine</prenom>
					<nom>Prince</nom>
					<email>prince@lirmm.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Simon</prenom>
					<nom>Jaillet</nom>
					<email>jaillet@lirmm.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Maguelonne</prenom>
					<nom>Teisseire</nom>
					<email>teisseir@lirmm.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIRMM-CNRS- Université Montpellier 2 161 rue Ada, 34392 Montpellier cedex 5</affiliation>
			</affiliations>
			<titre>Classification automatique de textes à partir de leur analyse syntaxico-sémantique</titre>
			<type>long</type>
			<pages>55-64</pages>
			<resume>L’hypothèse soutenue dans cet article est que l’analyse de contenu, quand elle est réalisée par un analyseur syntaxique robuste avec calcul sémantique dans un modèle adéquat, est un outil de classification tout aussi performant que les méthodes statistiques. Pour étudier les possibilités de cette hypothèse en matière de classification, à l’aide de l’analyseur du Français, SYGMART, nous avons réalisé un projet en grandeur réelle avec une société qui propose des sélections d’articles en revue de presse. Cet article présente non seulement les résultats de cette étude (sur 4843 articles finalement sélectionnés), mais aussi cherche à montrer que l’analyse de contenu automatisée, quand elle est possible, est un moyen fiable de produire une catégorisation issue du sens (quand il est calculable), et pas simplement créée à partir d’une reconnaissance de "similarités"de surface.</resume>
			<mots_cles>Analyse, Classification, Extraction d’information</mots_cles>
			<title></title>
			<abstract>This paper presents the assumption that discourse analysis, when perfomed by a robust parser backed up by an accurate semantic model, is a classification tool as efficient as statistical methods. To study the capabilities of discourse analysis in classification, we have used a parser for French, SYGMART, and applied it to a real project of press articles classification. This article presents the results of this research (on a corpus of 4843 texts), and tries to show that automatic discourse analysis, when possible, is an efficient way of classification through meaning discrimination, and not simply relying on surface similarities recognition.</abstract>
			<keywords>Parsing, Categorization, Information Extraction</keywords>
		</article>
		<article id="taln-2003-long-005" session="">
			<auteurs>
				<auteur>
					<prenom>Vincent</prenom>
					<nom>Claveau</nom>
					<email>Vincent.Claveau@irisa.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">IRISA - Université de Rennes 1 Campus de Beaulieu 35042 Rennes Cedex, France</affiliation>
			</affiliations>
			<titre>Extraction de couples nom-verbe sémantiquement liés : une technique symbolique automatique</titre>
			<type>long</type>
			<pages>65-74</pages>
			<resume>Dans le modèle du Lexique génératif (Pustejovsky, 1995), certaines propriétés sémantiques des noms sont exprimées à l’aide de verbes. Les couples nom-verbe ainsi formés présentent un intérêt applicatif notamment en recherche d’information. Leur acquisition sur corpus constitue donc un enjeu, mais la découverte des patrons qui les définissent en contexte est également importante pour la compréhension même du modèle du Lexique génératif. Cet article présente une technique entièrement automatique permettant de répondre à ce double besoin d’extraction sur corpus de couples et de patrons morpho-syntaxiques et sémantiques. Elle combine pour ce faire deux approches d’acquisition— l’approche statistique et l’approche symbolique— en conservant les avantages propres à chacune d’entre elles : robustesse et automatisation des méthodes statistiques, qualité et expressivité des résultats des techniques symboliques.</resume>
			<mots_cles>Acquisition de lexique, extraction de patrons morpho-syntaxiques et sémantiques, lexique génératif, programmation logique inductive, bootstrapping, apprentissage semi-supervisé</mots_cles>
			<title></title>
			<abstract>In the Generative Lexicon framework (Pustejovsky, 1995), some semantic properties of common nouns are expressed with the help of verbs. These noun-verb pairs are relevant in various domains, especially in Information Retrieval. Their corpus-based acquisition is thus an interesting issue; moreover discovering the contextual patterns in which these pairs can occur is also important in order to understand the Generative Lexicon model. This paper presents a fully automated technique that allows us to acquire from a corpus both noun-verb pairs, and semantic and morpho-syntactic patterns. This technique combines two acquisition approaches—the statistical one and the symbolic one—and keeps advantages of each approach: robustness and automation of statistical methods, quality of the results and expressiveness of symbolic ones.</abstract>
			<keywords>Lexicon acquisition, morpho-syntactic and semantic pattern extraction, Generative Lexicon, inductive logic programming, bootstrapping, semi-supervised learning</keywords>
		</article>
		<article id="taln-2003-long-006" session="">
			<auteurs>
				<auteur>
					<prenom>Benoît</prenom>
					<nom>Crabbé</nom>
					<email>crabbe@loria.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Bertrand</prenom>
					<nom>Gaiffe</nom>
					<email>gaiffe@loria.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Azim</prenom>
					<nom>Roussanaly</nom>
					<email>azim@loria.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LORIA BP 239  54506 Vandoeuvre-lès-Nancy Cedex</affiliation>
			</affiliations>
			<titre>Une plate-forme de conception et dexploitation dune grammaire darbres adjoints lexicalisés</titre>
			<type>long</type>
			<pages>75-84</pages>
			<resume>Dans cet article, nous présentons un ensemble doutils de conception et dexploitation pour des grammaires darbres adjoints lexicalisés. Ces outils sappuient sur une représentation XML des ressources (lexique et grammaire). Dans notre représentation, à chaque arbre de la grammaire est associé un hypertag décrivant les phénomènes linguistiques quil recouvre. De ce fait, la liaison avec le lexique se trouve plus compactée et devient plus aisée à maintenir. Enfin, un analyseur permet de valider les grammaires et les lexiques ainsi conçus aussi bien de façon interactive que différée sur des corpus.</resume>
			<mots_cles>Grammaire, analyse syntaxique, ressources lexicales, LTAG, représentation compacte du lexique</mots_cles>
			<title></title>
			<abstract>Grammar, parser, lexical resources, LTAG, compact lexical representation</abstract>
			<keywords></keywords>
		</article>
		<article id="taln-2003-long-007" session="">
			<auteurs>
				<auteur>
					<prenom>Éric</prenom>
					<nom>Crestan</nom>
					<email>eric.crestan@lia.univ-avignon.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Marc</prenom>
					<nom>El-Bèze</nom>
					<email>marc.elbeze@lia.univ-avignon.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Claude</prenom>
					<nom>De Loupy</nom>
					<email>loupy@sinequa.com</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Laboratoire Informatique d’Avignon 339, ch des Meinajaries, BP 1228 F-84911 Avignon Cedex 9</affiliation>
				<affiliation affiliationId="2">Sinequa S.A.S. 51-59 rue Ledru-Rollin F-94200 Ivry-sur-Seine</affiliation>
			</affiliations>
			<titre>Peut-on trouver la taille de contexte optimale en désambiguïsation sémantique?</titre>
			<type>long</type>
			<pages>85-94</pages>
			<resume>Dans la tâche de désambiguïsation sémantique, la détermination de la taille optimale de fenêtre de contexte à utiliser, a fait l'objet de plusieurs études. Dans cet article, nous proposons une approche à deux niveaux pour répondre à cette problématique de manière automatique. Trois systèmes concurrents à base d'arbres de classification sémantique sont, dans un premier temps, utilisés pour déterminer les trois sens les plus vraisemblables d'un mot. Ensuite, un système décisionnel tranche entre ces sens au regard d'un contexte plus étendu. Les améliorations constatées lors d'expériences menées sur les données de SENSEVAL-1 et vérifiées sur les données SENSEVAL-2 sont significatives.</resume>
			<mots_cles>Désambiguïsation sémantique, arbres de classification sémantique</mots_cles>
			<title></title>
			<abstract>The determination of context length to use for Word Sense Disambiguation (WSD) has been the object of several studies. In this paper, we propose to use a monitoring system in order to select automatically the optimal window size among three possibilities. We used a two-step strategy based on Semantic Classification Trees (SCT) and on a similarity measure. Whereas SCTs are employed on a short window size of 3, 5 and 7 words, the technique based on similarity measure is appllied to a ‘wider’ context size. The improvements observed in the SENSEVAL-1 lexical-sample task are verified on the SENSEVAL-2 data.</abstract>
			<keywords>Word sense disambiguation, semantic classification trees, monitoring system</keywords>
		</article>
		<article id="taln-2003-long-008" session="">
			<auteurs>
				<auteur>
					<prenom>Laurence</prenom>
					<nom>Danlos</nom>
					<email>laurence.danlos@linguist.jussieu.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">TALANA/ LATTICE Université Paris 7</affiliation>
			</affiliations>
			<titre>Représentation sémantique sous-spécifiée pour les conjonctions de subordination</titre>
			<type>long</type>
			<pages>95-104</pages>
			<resume>Cet article concerne les phrases complexes avec deux conjonctions de subordination. Nous montrerons que de telles phrases peuvent s’interpréter de quatre façons différentes. Il s’agit donc de formes fortement ambiguës pour lesquelles il est opportun d’avoir recours à des représentations sémantiques sous-spécifiées, et c’est ce que nous proposerons.</resume>
			<mots_cles>Discours, SDRT (Segmented Discourse représentation Theory), DAG (Directed Acyclic Graph), Représentation sémantique sous-spécifiée, LTAG (Lexicalized Tree Adjoining Grammar)</mots_cles>
			<title></title>
			<abstract>This paper concerns sentences with two subordination conjunctions. It shows that such sentences can be given four interpretations. They are heavily ambiguous forms for which an underspecified semantic representation is proposed.</abstract>
			<keywords>Discourses, SDRT, DAG, Underspecified semantic representation, LTAG</keywords>
		</article>
		<article id="taln-2003-long-009" session="">
			<auteurs>
				<auteur>
					<prenom>Gaël</prenom>
					<nom>de Chalendar</nom>
					<email>Chalendar@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Faïza</prenom>
					<nom>El Kateb</nom>
					<email>Kateb@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Olivier</prenom>
					<nom>Ferret</nom>
					<email>Ferret@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Brigitte</prenom>
					<nom>Grau</nom>
					<email>Grau@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Martine</prenom>
					<nom>Hurault-Plantet</nom>
					<email>Hurault-Plantet@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Laura</prenom>
					<nom>Monceaux</nom>
					<email>Monceaux@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Isabelle</prenom>
					<nom>Robba</nom>
					<email>Robba@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Anne</prenom>
					<nom>Vilnat</nom>
					<email>Vilnat@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIMSI – Groupe LIR BP 133, 91403 Orsay</affiliation>
			</affiliations>
			<titre>Confronter des sources de connaissances différentes pour obtenir une réponse plus fiable</titre>
			<type>long</type>
			<pages>105-114</pages>
			<resume>La fiabilité des réponses qu’il propose, ou un moyen de l’estimer, est le meilleur atout d’un système de question-réponse. A cette fin, nous avons choisi d’effectuer des recherches dans des ensembles de documents différents et de privilégier des résultats qui sont trouvés dans ces différentes sources. Ainsi, le système QALC travaille à la fois sur une collection finie d’articles de journaux et sur le Web.</resume>
			<mots_cles>Système de question-réponse, recherche d’information, fiabilité des réponses</mots_cles>
			<title></title>
			<abstract>A question answering system will be more convincing if it can give a user elements concerning the reliability of it propositions. In order to address this problem, we choose to take the advice of several searches. First we search for answers in a reliable document collection, and second on the Web. When the two sources of knowledge give the system QALC common answers, we are confident with them and boost them at the first places.</abstract>
			<keywords>Question answering system, information retrieval, answer reliability</keywords>
		</article>
		<article id="taln-2003-long-010" session="">
			<auteurs>
				<auteur>
					<prenom>Florence</prenom>
					<nom>Duclaye</nom>
					<email>florence.duclaye@rd.francetelecom.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Olivier</prenom>
					<nom>Collin</nom>
					<email>olivier.collin@rd.francetelecom.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>François</prenom>
					<nom>Yvon</nom>
					<email>yvon@enst.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">France Télécom R&amp;D 2 avenue Pierre Marzin 22307 Lannion Cedex</affiliation>
				<affiliation affiliationId="2">GET/ENST et LTCI, CNRS URA 820 46 rue Barrault 75624 Paris Cedex 13</affiliation>
			</affiliations>
			<titre>Apprentissage Automatique de Paraphrases pour l’Amélioration d’un Système de Questions-Réponses</titre>
			<type>long</type>
			<pages>115-124</pages>
			<resume>Dans cet article, nous présentons une méthodologie d’apprentissage faiblement supervisé pour l’extraction automatique de paraphrases à partir du Web. À partir d’un seule exemple de paire (prédicat, arguments), un corpus est progressivement accumulé par sondage duWeb. Les phases de sondage alternent avec des phases de filtrage, durant lesquelles les paraphrases les moins plausibles sont éliminées à l’aide d’une procédure de clustering non supervisée. Ce mécanisme d’apprentissage s’appuie sur un système de Questions-Réponses existant et les paraphrases apprises seront utilisées pour en améliorer le rappel. Nous nous concentrons ici sur le mécanisme d’apprentissage de ce système et en présentons les premiers résultats.</resume>
			<mots_cles>Questions-Réponses, Apprentissage Automatique, Acquisition de Paraphrase</mots_cles>
			<title></title>
			<abstract>In this paper, we present a nearly unsupervised learning methodology for automatically extracting paraphrases from theWeb. Starting with one single instance of a pair (predicate,arguments), a corpus is incrementally built by sampling the Web. Sampling stages alternate with filteringstages, during which implausible paraphrases are filtered out using an EM-based unsupervised clustering procedure. This learning machinery is built on top of an existing questionanswering system and the learnt paraphrases will eventually be used to improve its recall. We focus here on the learning aspect of this system and report preliminary results.</abstract>
			<keywords>Question Answering, Machine Learning, Paraphrase extraction</keywords>
		</article>
		<article id="taln-2003-long-011" session="">
			<auteurs>
				<auteur>
					<prenom>Kim</prenom>
					<nom>Gerdes</nom>
					<email>kim@linguist.jussieu.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Hi-Yon</prenom>
					<nom>Yoo</nom>
					<email>hi-yon.yoo@linguist.jussieu.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Lattice  Université Paris 7 2, place Jussieu 75251 Paris Cedex 05</affiliation>
				<affiliation affiliationId="2">ARP  Université Paris 7 2, place Jussieu 75251 Paris Cedex 05</affiliation>
			</affiliations>
			<titre>La topologie comme interface entre syntaxe et prosodie : un système de génération appliqué au grec moderne</titre>
			<type>long</type>
			<pages>125-134</pages>
			<resume>Dans cet article, nous développons les modules syntaxique et topologique du modèle Sens- Texte et nous montrons lutilité de la topologie comme représentation intermédiaire entre les représentations syntaxique et phonologique. Le modèle est implémenté dans un générateur et nous présentons la grammaire du grec moderne dans cette approche.</resume>
			<mots_cles>grammaires de dépendance, ordre des mots, prosodie, topologie, générateur de parole, grec</mots_cles>
			<title></title>
			<abstract>In this paper, we develop the syntactic and topological modules of the Meaning-Text model. We show the use of topology as the interface of syntax and phonology. The model is implemented in a speech generator and we present its Greek grammar.</abstract>
			<keywords>dependency grammar, word order, prosody, topology, language generation, Greek</keywords>
		</article>
		<article id="taln-2003-long-012" session="">
			<auteurs>
				<auteur>
					<prenom>Radu</prenom>
					<nom>Gramatovici</nom>
					<email>radu@funinf.cs.unibuc.ro</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Carlos</prenom>
					<nom>Martín-Vide</nom>
					<email>cmv@astor.urv.es</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Faculty of Mathematics and Computer Science, University of Bucharest Academiei 14, 70109, Bucharest, Romania</affiliation>
				<affiliation affiliationId="2">Research Group on Mathematical Linguistics, Rovira i Virgili University Pl. Imperial Tàrraco 1, 43005 Tarragona, Spain</affiliation>
			</affiliations>
			<titre></titre>
			<type>long</type>
			<pages>135-144</pages>
			<resume>On présente une nouvelle variante de grammaire contextuelle structurée, qui produit des arbres de dépendance. Le nouveau modèle génératif, appelé grammaire contextuelle de dépendance, améliore la puissance générative forte et faible des grammaires contextuelles, tout en étant un candidat potentiel pour la description mathématique des modèles syntactiques de dépendance.</resume>
			<mots_cles>Grammaire contextuelle, arbre de dépendance, arbre projectif de dépendance</mots_cles>
			<title>Contextual Grammars and Dependency Trees</title>
			<abstract>A new variant of structured contextual grammar, which generates dependency trees, is introduced. The new generative model, called dependency contextual grammar, improves both the strong and weak generative power of contextual grammars, while being a potential candidate for the mathematical description of dependency-based syntactic models.</abstract>
			<keywords>Contextual grammar, dependency tree, projective dependency tree</keywords>
		</article>
		<article id="taln-2003-long-013" session="">
			<auteurs>
				<auteur>
					<prenom>Caroline</prenom>
					<nom>Hagège</nom>
					<email>caroline.hagege@xrce.xerox.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Claude</prenom>
					<nom>Roux</nom>
					<email>claude.roux@xrce.xerox.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Xerox Research Centre Europe - XRCE 6 Chemin de Maupertuis - 38240 Meylan</affiliation>
			</affiliations>
			<titre>Entre syntaxe et sémantique : Normalisation de la sortie de l’analyse syntaxique en vue de l’amélioration de l’extraction d’information à partir de textes</titre>
			<type>long</type>
			<pages>145-154</pages>
			<resume>Cet article présente la normalisation de la sortie d’un analyseur robuste de l’anglais. Nous montrons quels sont les enrichissements que nous avons effectués afin de pouvoir obtenir à la sortie de notre analyseur des relations syntaxiques plus générales que celles que nous offrent habituellement les analyseurs robustes existants. Pour cela nous utilisons non seulement des propriétés syntaxiques, mais nous faisons appel aussi à de l’information de morphologie dérivationnelle. Cette tâche de normalisation est menée à bien grâce à notre analyseur XIP qui intègre tous les traitements allant du texte brut tout venant au texte normalisé. Nous pensons que cette normalisation nous permettra de mener avec plus de succès des tâches d’extraction d’information ou de détection de similarité entre documents.</resume>
			<mots_cles>analyse syntaxique robuste, normalisation en vue de l’extraction d’information</mots_cles>
			<title></title>
			<abstract>This article presents our work on the normalization of the output of a robust dependency parser for English. We show how we have enriched our grammar to yield syntactic relations that are more general than those usually obtained with other available robust parsers. In order to achieve this result, we use syntactic properties, together with derivational morphology information. This normalization task is carried out with XIP which handles all the process that transform our input text into its normalized output. We consider that this normalization will improve the result of information extraction and similarity detection process between documents.</abstract>
			<keywords>robust parsing, normalization for information extraction</keywords>
		</article>
		<article id="taln-2003-long-014" session="">
			<auteurs>
				<auteur>
					<prenom>Agata</prenom>
					<nom>Jackiewicz</nom>
					<email>Agata.Jackiewicz@paris4.sorbonne.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Jean-Luc</prenom>
					<nom>Minel</nom>
					<email>Jean-Luc.Minel@paris4.sorbonne.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Laboratoire LaLICC UMR 8139 (CNRS - Université Paris-Sorbonne) Université de Paris-Sorbonne (ISHA) 96, bd Raspail, 75006 Paris - France</affiliation>
			</affiliations>
			<titre>Lidentification des structures discursives engendrées par les cadres organisationnels</titre>
			<type>long</type>
			<pages>155-164</pages>
			<resume>Cet article présente tout dabord une analyse linguistique des cadres organisationnels et son implémentation informatique. Puis à partir de ce travail, une modélisation généralisable à lensemble des cadres de discours est proposée. Enfin, nous discutons du concept dindicateur proposé dans le cadre théorique de lexploration contextuelle.</resume>
			<mots_cles>Séries de cadres organisationnels, marqueurs dintégration linéaire, cadres de discours, segmentation automatique de textes, méthode dexploration contextuelle, filtrage automatique de textes</mots_cles>
			<title></title>
			<abstract>To begin with, this paper outlines a linguistic analysis of textual enumerating frameworks and its computational making. Then, from this work, a modelling for all textual frameworks is suggested. Finally, we discuss the relevance of the concept of clue which is central in the theoretical framework of contextual exploration method.</abstract>
			<keywords>Enumerating frameworks, linear integration markers, discourse frames, automatic text segmentation, contextual exploration method, automatic text filtering</keywords>
		</article>
		<article id="taln-2003-long-015" session="">
			<auteurs>
				<auteur>
					<prenom>Salma</prenom>
					<nom>Jamoussi</nom>
					<email>jamoussi@loria.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Kamel</prenom>
					<nom>Smaïli</nom>
					<email>smaili@loria.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Jean-Paul</prenom>
					<nom>Haton</nom>
					<email>jph@loria.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LORIA/INRIA-Lorraine 615 rue du Jardin Botanique, BP 101, F-54600 Villers-lès-Nancy, France</affiliation>
			</affiliations>
			<titre>Vers la compréhension automatique de la parole : extraction de concepts par réseaux bayésiens</titre>
			<type>long</type>
			<pages>165-174</pages>
			<resume>La compréhension automatique de la parole peut être considérée comme un problème d’association entre deux langages différents. En entrée, la requête exprimée en langage naturel et en sortie, juste avant l’étape d’interprétation, la même requête exprimée en terme de concepts. Un concept représente un sens bien déterminé. Il est défini par un ensemble de mots partageant les mêmes propriétés sémantiques. Dans cet article, nous proposons une méthode à base de réseau bayésien pour l’extraction automatique des concepts ainsi que trois approches différentes pour la représentation vectorielle des mots. Ces représentations aident un réseau bayésien à regrouper les mots, construisant ainsi la liste adéquate des concepts à partir d’un corpus d’apprentissage. Nous conclurons cet article par la description d’une étape de post-traitement au cours de laquelle, nous étiquetons nos requêtes et nous générons les commandes SQL appropriées validant ainsi, notre approche de compréhension.</resume>
			<mots_cles>Compréhension de la parole, concepts sémantiques, réseaux bayésiens, étiquetage sémantique, catégorisation automatique</mots_cles>
			<title></title>
			<abstract>The automatic speech understanding can be considered as association problem between two different languages. At the entry, the request expressed in natural language and at the end, just before the stage of interpretation, the same request is expressed in term of concepts. One concept represents given meaning, it is defined by the set of words sharing the same semantic properties. In this paper, we propose a new Bayesian network based method to automatically extract the underlined concepts. We also propose three different approaches for the vector representation of words. These representations help Bayesian network to build the adequate list of concepts for the considered application. We finish this paper by description of the postprocessing step during which, we label our sentences and we generate the corresponding SQL queries. This step allows us to validate our speech understanding approach.</abstract>
			<keywords>Speech understanding, semantic concepts, Bayesian networks, semantic labelling, automatic categorization</keywords>
		</article>
		<article id="taln-2003-long-016" session="">
			<auteurs>
				<auteur>
					<prenom>Sylvain</prenom>
					<nom>Kahane</nom>
					<email>sk@ccr.jussieu.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Lattice, Université Paris 7</affiliation>
			</affiliations>
			<titre>Les signes grammaticaux dans l’interface sémantique-syntaxe d’une grammaire d’unification</titre>
			<type>long</type>
			<pages>175-184</pages>
			<resume>Cet article présente une grammaire d’unification dans laquelle les morphèmes grammaticaux sont traités similairement aux morphèmes lexicaux!: les deux types de morphèmes sont traités comme des signes à part entière et sont décris par des structures élémentaires qui peuvent s’unifier directement les unes aux autres (ce qui en fait une grammaire de dépendance). Nous illustrerons notre propos par un fragment de l’interface sémantique-syntaxe du français pour le verbe et l’adjectif!: voix, modes, temps, impersonnel et tough-movement.</resume>
			<mots_cles>Morphème grammatical, interface syntaxe-sémantique, grammaire d’unification, grammaire de dépendance, HPSG, passif, impersonnel, tough-movement</mots_cles>
			<title></title>
			<abstract>This paper presents a unification grammar where the grammatical morphemes are handled as the lexical morphemes: Both kinds of morphemes are handled as signs and described by elementary structures which directly unify to each other (making the grammar a dependency grammar). Our purpose will be illustrated by a fragment of the semantics-syntax interface of French for verbs and adjectives: voices, moods, tenses, impersonal, and tough-movement.</abstract>
			<keywords>Grammatical morpheme, syntax-semantics interface, unification grammar, dependency grammar, passive, impersonal, tough-movement</keywords>
		</article>
		<article id="taln-2003-long-017" session="">
			<auteurs>
				<auteur>
					<prenom>Frédéric</prenom>
					<nom>Landragin</nom>
					<email>Frederic.Landragin@loria.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LORIA — UMR 7503 Campus scientifique — BP 239 54506 Vandoeuvre-lès-Nancy CEDEX</affiliation>
			</affiliations>
			<titre>Une caractérisation de la pertinence pour les actions de référence</titre>
			<type>long</type>
			<pages>185-194</pages>
			<resume>Que ce soit pour la compréhension ou pour la génération d’expressions référentielles, la Théorie de la Pertinence propose un critère cognitif permettant de comparer les pertinences de plusieurs expressions dans un contexte linguistique. Nous voulons ici aller plus loin dans cette voie en proposant une caractérisation précise de ce critère, ainsi que des pistes pour sa quantification. Nous étendons l’analyse à la communication multimodale, et nous montrons comment la perception visuelle, le langage et le geste ostensif interagissent dans la production d’effets contextuels. Nous nous attachons à décrire l’effort de traitement d’une expression multimodale à l’aide de traits. Nous montrons alors comment des comparaisons entre ces traits permettent d’exploiter efficacement le critère de pertinence en communication homme-machine. Nous soulevons quelques points faibles de notre proposition et nous en tirons des perspectives pour une formalisation de la pertinence.</resume>
			<mots_cles>Pertinence, référence aux objets, dialogue multimodal, effets contextuels, effort de traitement</mots_cles>
			<title></title>
			<abstract>For automatic comprehension or generation of referring expressions, Relevance Theory proposes a cognitive criterion that allows to compare the relevance of several expressions in a linguistic context. We want here to pursue this work. We propose a more precise characterization of this criterion and foundations for its computation. We extend the analysis to multimodal communication, and we show how visual perception, speech and gesture present multiple interactions between each other in the production of contextual effects. We describe with some features the processing effort of a multimodal expression. Then we show how comparisons between these features lead to an efficient exploitation of the relevance criterion in man-machine dialogue. We raise some weak points of our proposal and we deduce future plans for a formalization of relevance.</abstract>
			<keywords>Relevance, reference to objects, multimodal dialogue, contextual effects, processing effort</keywords>
		</article>
		<article id="taln-2003-long-018" session="">
			<auteurs>
				<auteur>
					<prenom>Philippe</prenom>
					<nom>Langlais</nom>
					<email>felipe@iro.umontreal.ca</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Michel</prenom>
					<nom>Simard</nom>
					<email>simardm@iro.umontreal.ca</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">RALI Département d’Informatique et de Recherche Opérationnelle Université de Montréal CP. 6128 Succursale Centre-Ville H3C3J7 Montréal, Québec, Canada</affiliation>
			</affiliations>
			<titre>De la traduction probabiliste aux mémoires de traduction (ou l’inverse)</titre>
			<type>long</type>
			<pages>195-204</pages>
			<resume>En dépit des travaux réalisés cette dernière décennie dans le cadre général de la traduction probabiliste, nous sommes toujours bien loin du jour où un engin de traduction automatique (probabiliste ou pas) sera capable de répondre pleinement aux besoins d’un traducteur professionnel. Dans une étude récente (Langlais, 2002), nous avons montré comment un engin de traduction probabiliste pouvait bénéficier de ressources terminologiques extérieures. Dans cette étude, nous montrons que les techniques de traduction probabiliste peuvent être utilisées pour extraire des informations sous-phrastiques d’une mémoire de traduction. Ces informations peuvent à leur tour s’avérer utiles à un engin de traduction probabiliste. Nous rapportons des résultats sur un corpus de test de taille importante en utilisant la mémoire de traduction d’un concordancier bilingue commercial.</resume>
			<mots_cles>Traduction automatique, mémoire de traduction sous-phrastique, alignement sous-phrastique</mots_cles>
			<title></title>
			<abstract>Despite the exciting work accomplished over the past decade in the field of Statistical Machine Translation (SMT), we are still far from the point of being able to say that machine translation fully meets the needs of real-life users. In a previous study (Langlais, 2002), we have shown how a SMT engine could benefit from terminological resources, especially when translating texts very different from those used to train the system. In the present paper, we discuss the opening of SMT to examples automatically extracted from a Translation Memory (TM). We report results on a fair-sized translation task using the database of a commercial bilingual concordancer.</abstract>
			<keywords>Automatic translation, translation memories, word alignment</keywords>
		</article>
		<article id="taln-2003-long-019" session="">
			<auteurs>
				<auteur>
					<prenom>Hélène</prenom>
					<nom>Manuélian</nom>
					<email>helene.manuelian@loria.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LORIA BP239 Campus Scientifique F-54506 Vandoeuvre-les-Nancy</affiliation>
			</affiliations>
			<titre>Une analyse des emplois du démonstratif en corpus</titre>
			<type>long</type>
			<pages>205-214</pages>
			<resume>Cet article propose une nouvelle classification des utilisations des démonstratifs, une mise en oeuvre de cette classification dans une analyse de corpus et présente les resultats obtenus au terme de cette analyse. La classification proposée est basée sur celles existant dans la littérature et étendues pour permettre la génération de groupes nominaux démonstratifs. L’analyse de corpus montre en particulier que la nature "reclassifiante" du démonstratif lui permet d’assumer deux fonctions (une fonction anaphorique et une fonction de support pour de l’information nouvelle) et qu’il existe des moyens variés de réaliser ces fonctions.</resume>
			<mots_cles>Démonstratifs, analyse de corpus, annotation de corpus, génération de texte</mots_cles>
			<title></title>
			<abstract>This paper presents a new classification for the use of demonstrative descriptions, its application in a corpus analysis and the results of this analysis. The proposed classification is based on the existing literature and extended to support the generation of demonstrative NPs. The corpus analysis shows in particular, that the "reclassifying power" of the demonstrative allows it to perform two functions (anaphora and supporting new information) and that there are various ways of realising these functions.</abstract>
			<keywords>Demonstrative, corpus analysis, corpus annotation, text generation</keywords>
		</article>
		<article id="taln-2003-long-020" session="">
			<auteurs>
				<auteur>
					<prenom>Laura</prenom>
					<nom>Monceaux</nom>
					<email>monceaux@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Anne</prenom>
					<nom>Vilnat</nom>
					<email>vilnat@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIMSI/CNRS - Université Paris Sud Bat 508 - BP 133 91403 Orsay Cedex</affiliation>
			</affiliations>
			<titre>MULTI-ANALYSE vers une analyse syntaxique plus fiable</titre>
			<type>long</type>
			<pages>215-224</pages>
			<resume>Dans cet article, nous proposons de montrer que la combinaison de plusieurs analyses syntaxiques permet d’extraire Panalyse la plus fiable pour une phrase donnée. De plus, chaque information syntaxique sera affectée d’un score de confiance déterminé selon le nombre d’analyseurs syntaxiques la confirmant. Nous verrons que cette approche implique l’étude des différents analyseurs syntaxiques existants ainsi que leur évaluation.</resume>
			<mots_cles>Analyseurs syntaxiques, Combinaison d’informations, Evaluation</mots_cles>
			<title></title>
			<abstract>In this paper, we propose an algorithm of combination between several syntactic parses in order to extract the most reliable syntactic parse for the sentence given and to affect these syntactical informations with the confidence rate, determined with regard to the number of parsers returning the same information. This approach involves the study of syntactic parsers and their evaluation.</abstract>
			<keywords>Syntactic parsers, Information combination, Evaluation</keywords>
		</article>
		<article id="taln-2003-long-021" session="">
			<auteurs>
				<auteur>
					<prenom>Antoine</prenom>
					<nom>Rozenknop</nom>
					<email>Antoine.Rozenknop@epfl.ch</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Jean-Cédric</prenom>
					<nom>Chappelier</nom>
					<email>Jean-Cedric.Chappelier@epfl.ch</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Martin</prenom>
					<nom>Rajman</nom>
					<email>Martin.Rajman@epfl.ch</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Laboratoire d'Intelligence Artificielle - Ecole Polytechnique Fédérale de Lausanne CH-1015 Lausanne, Switzerland</affiliation>
				<affiliation affiliationId="2"></affiliation>
				<affiliation affiliationId="3"></affiliation>
			</affiliations>
			<titre>Apprentissage discriminant pour les Grammaires à Substitution d'Arbres</titre>
			<type>long</type>
			<pages>225-234</pages>
			<resume>Les grammaires stochastiques standards utilisent des modèles probabilistes de nature générative, fondés sur des probabilités de récriture conditionnées par le symbole récrit. Les expériences montrent qu'elles tendent ainsi par nature à pénaliser les dérivations les plus longues pour une meme entrée, ce qui n'est pas forcément un comportement souhaitable, ni en analyse syntaxique, ni en reconnaissance de la parole. Dans cet article, nous proposons une approche probabiliste non-générative du modèle STSG (grammaire stochastique à substitution d'arbres), selon laquelle les probabilités sont conditionnées par les feuilles des arbres syntaxiques plutot que par leur racine, et qui par nature fait appel à un apprentissage discriminant. Plusieurs expériences sur ce modèle sont présentées.</resume>
			<mots_cles>STSG, Gibbs-Markov, Maximum d'Entropie, Vraisemblance Conditionnelle</mots_cles>
			<title></title>
			<abstract>Standard stochastic grammars use generative probabilistic models, focussing on rewriting probabilities conditioned by the rewritten symbol. Such grammars therefore tend to give penalty to longer derivations of the same input, which could be a drawback when they are used for analysis (e.g. speech recognition). In this contribution, we propose a novel non-generative probabilistic model of STSGs (Stochastic Tree Substitution Grammars), where probabilities are conditioned by the leaves of the syntactic trees (i.e. the input symbols) rather than by the root. Several experiments of this new model are presented.</abstract>
			<keywords>STSG, Gibbs-Markov, Maximum Entropy, Conditionnal likelihood</keywords>
		</article>
		<article id="taln-2003-long-022" session="">
			<auteurs>
				<auteur>
					<prenom>Didier</prenom>
					<nom>Schwab</nom>
					<email>schwab@lirmm.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Mathieu</prenom>
					<nom>Lafourcade</nom>
					<email>lafourca@lirmm.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Violaine</prenom>
					<nom>Prince</nom>
					<email>prince@lirmm.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIRMM Laboratoire d’informatique, de Robotique et de Microélectronique de Montpellier MONTPELLIER - FRANCE</affiliation>
			</affiliations>
			<titre>Amélioration de liens entre acceptions par fonctions lexicales vectorielles symétriques</titre>
			<type>long</type>
			<pages>235-244</pages>
			<resume>Dans le cadre du projet Papillon qui vise à la construction de bases lexicales multilingues par acceptions, nous avons défini des stratégies pour peupler un dictionnaire pivot de liens interlingues à partir d’une base vectorielle monolingue. Il peut y avoir un nombre important de sens par entrée et donc l’identification des acceptions correspondantes peut être erronée. Nous améliorons l’intégrité de la base d’acception grâce à des agents experts dans les fonctions lexicales comme la synonymie, l’antonymie, l’hypéronymie ou l’holonymie. Ces agents sont capable de calculer la pertinence d’une relation sémantique entre deux acceptions par les diverses informations lexicales récoltées et les vecteurs conceptuels. Si une certaine pertinence est au-dessus d’un seuil, ils créent un lien sémantique qui peut être utilisé par d’autres agents chargés par exemple de la désambiguïsation ou du transfert lexical. Les agents vérifiant l’intégrité de la base cherchent les incohérences de la base et en avertissent les lexicographes le cas échéant.</resume>
			<mots_cles>représentation thématique, vecteurs conceptuels, fonctions lexicales, acceptions</mots_cles>
			<title></title>
			<abstract>In the framework of the Papillon project, we have defined strategies for populating a pivot dictionnary of interlingual links from monolingual vectorial bases. There are quite a number of acceptions per entry thus, the proper identification may be quite troublesome and some added clues beside acception links may be useful. We improve the integrity of the acception base through well known semantic relations like synonymy, antonymy, hyperonymy and holonymy relying on lexical functions agents. These semantic relation agents can compute the pertinence of a semantic relation between two acceptions thanks to various lexical informations and conceptual vectors. When a given pertinence score is above a threshold they create a semantic link which can be walked through by other agents in charge of WSD ot lexical transfert. Base integrity agents walk throw the acceptions, look for incoherences in the base and emit warning toward lexicographs when needed.</abstract>
			<keywords>thematic representation, conceptuals vectors, lexical functions, acceptions</keywords>
		</article>
		<article id="taln-2003-long-023" session="">
			<auteurs>
				<auteur>
					<prenom>Martine</prenom>
					<nom>Smets</nom>
					<email>martines@microsoft.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Michael</prenom>
					<nom>Gamon</nom>
					<email>mgamon@microsoft.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Simon</prenom>
					<nom>Corston-Oliver</nom>
					<email>simonco@microsoft.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Eric</prenom>
					<nom>Ringger</nom>
					<email>ringger@microsoft.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Microsoft Research One Microsoft Way Redmond, WA 98052, U.S.A.</affiliation>
			</affiliations>
			<titre></titre>
			<type>long</type>
			<pages>245-254</pages>
			<resume>Cette communication présente la version pour le français d’Amalgam, un système de réalisation automatique de phrases. Deux des modèles du système sont décrits en détail, et nous expliquons comment la performance des modèles peut être améliorée en combinant connaissances et intuition linguistiques et méthodes statistiques.</resume>
			<mots_cles>Réalisation de phrase, génération automatique, arbres de décision, français</mots_cles>
			<title>French Amalgam: A machine-learned sentence realization system</title>
			<abstract>This paper presents the French implementation of Amalgam, a machine-learned sentence realization system. It presents in some detail two of the machine-learned models employed in Amalgam and shows how linguistic intuition and knowledge can be combined with statistical techniques to improve the performance of the models.</abstract>
			<keywords>Sentence realization, generation, machine-learning, decision trees, French</keywords>
		</article>
		<article id="taln-2003-long-024" session="">
			<auteurs>
				<auteur>
					<prenom>Pascal</prenom>
					<nom>Vaillant</nom>
					<email>pascal.vaillant@martinique.univ-ag.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université des Antilles-Guyane —UFR de Lettres et Sciences Humaines B.P. 7207— 97275 SCHOELCHER CEDEX —Martinique, France</affiliation>
			</affiliations>
			<titre>Une grammaire formelle du créole martiniquais pour la génération automatique</titre>
			<type>long</type>
			<pages>255-264</pages>
			<resume>Nous présenterons dans cette communication les premiers travaux de modélisation informatique d’une grammaire de la langue créole martiniquaise, en nous inspirant des descriptions fonctionnelles de Damoiseau (1984) ainsi que du manuel de Pinalie &amp; Bernabé (1999). Prenant appui sur des travaux antérieurs en génération de texte (Vaillant, 1997), nous utilisons un formalisme de grammaires d’unification, les grammaires d’adjonction d’arbres (TAG d’après l’acronyme anglais), ainsi qu’une modélisation de catégories lexicales fonctionnelles à base syntaxico-sémantique, pour mettre en oeuvre une grammaire du créole martiniquais utilisable dans une maquette de système de génération automatique. L’un des intérêts principaux de ce système pourrait être son utilisation comme logiciel outil pour l’aide à l’apprentissage du créole en tant que langue seconde.</resume>
			<mots_cles>Créole, Martiniquais, Grammaire, TAG, Génération</mots_cles>
			<title></title>
			<abstract>In this article, some first elements of a computational modelling of the grammar of the Martiniquese French Creole dialect are presented. The sources of inspiration for the modelling is the functional description given by Damoiseau (1984), and Pinalie’s &amp; Bernabé’s (1999) grammar manual. Based on earlier works in text generation (Vaillant, 1997), a unification grammar formalism, namely Tree Adjoining Grammars (TAG), and a modelling of lexical functional categories based on syntactic and semantic properties, are used to implement a grammar of Martiniquese Creole which is used in a prototype of text generation system. One of the main applications of the system could be its use as a tool software supporting the task of learning Creole as a second language.</abstract>
			<keywords>Creole, Martiniquese, Grammar, TAG, Generation</keywords>
		</article>
		<article id="taln-2003-long-025" session="">
			<auteurs>
				<auteur>
					<prenom>Jean</prenom>
					<nom>Véronis</nom>
					<email>Jean.Veronis@up.univ-mrs.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Equipe DELIC - Université de Provence 29, Av. Robert Schuman - 13621 Aix-en-Provence Cedex 1</affiliation>
			</affiliations>
			<titre>Cartographie lexicale pour la recherche d'information</titre>
			<type>long</type>
			<pages>265-274</pages>
			<resume>Nous décrivons un algorithme, HyperLex, de détermination automatique des différents usages dun mot dans une base textuelle sans utilisation dun dictionnaire. Cet algorithme basé sur la détection des composantes de forte densité du graphe des cooccurrences de mots permet, contrairement aux méthodes précédemment proposées (vecteurs de mots), disoler des usages très peu fréquents. Il est associé à une technique de représentation graphique permettant à lutilisateur de naviguer de façon visuelle à travers le lexique et dexplorer les différentes thématiques correspondant aux usages discriminés.</resume>
			<mots_cles>Désambiguïsation lexicale, recherche dinformation, interfaces graphiques</mots_cles>
			<title></title>
			<abstract>We describe the HyperLex algorithm for automatic discrimination of word uses in a textual database. The algorithm does not require a dictionary. It detects high density components in the word-cooccurrence graph, and, contrary to previous methods (word vectors), enables the recognition of very low frequency uses. HyperLex is associated with a graphic representation technique that makes it possible to navigate through the lexicon and explore visually the various themes corresponding to the discriminated uses.</abstract>
			<keywords>Lexical disambiguation, information retrieval, graphic interfaces</keywords>
		</article>
		<article id="taln-2003-long-026" session="">
			<auteurs>
				<auteur>
					<prenom>Romain</prenom>
					<nom>Vinot</nom>
					<email>romain.vinot@enst.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Natalia</prenom>
					<nom>Grabar</nom>
					<email>ngr@biomath.jussieu.fr</email>
					<affiliationId>2</affiliationId>
					<affiliationId>3</affiliationId>
				</auteur>
				<auteur>
					<prenom>Mathieu</prenom>
					<nom>Valette</nom>
					<email>mathieu.valette@free.fr</email>
					<affiliationId>2</affiliationId>
					<affiliationId>4</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Ecole Nationale Supérieure des Télécommunications</affiliation>
				<affiliation affiliationId="2">Centre de Recherche en Ingénierie Multilingue - INALCO</affiliation>
				<affiliation affiliationId="3">STIM - DIAM, AP-HP Pitié-Salpêtrière, Université Paris 6</affiliation>
				<affiliation affiliationId="4">UMR 7114 CNRS/Paris X (MoDyCo)</affiliation>
			</affiliations>
			<titre>Application d’algorithmes de classification automatique pour la détection des contenus racistes sur l’Internet</titre>
			<type>long</type>
			<pages>275-284</pages>
			<resume>Le filtrage de contenus illicites sur Internet est une problématique difficile qui est actuellement résolue par des approches à base de listes noires et de mots-clés. Les systèmes de classification textuelle par apprentissage automatique nécessitant peu d’interventions humaines, elles peuvent avantageusement remplacer ou compléter les méthodes précédentes pour faciliter les mises à jour. Ces techniques, traditionnellement utilisées avec des catégories définies par leur sujet (économie ou sport par exemple), sont fondées sur la présence ou l’absence de mots. Nous présentons une évaluation de ces techniques pour le filtrage de contenus racistes. Contrairement aux cas traditionnels, les documents ne doivent pas être catégorisés suivant leur sujet mais suivant le point de vue énoncé (raciste ou antiraciste). Nos résultats montrent que les classifieurs, essentiellement lexicaux, sont néanmoins bien adaptées : plus de 90% des documents sont correctement classés, voir même 99% si l’on accepte une classe de rejet (avec 20% d’exemples non classés).</resume>
			<mots_cles>Classification automatique, Rocchio, kPPV, SVM, Internet, filtrage de l’information</mots_cles>
			<title></title>
			<abstract>Filtering of illicit contents on the Internet is a difficult issue which is currently solved with black lists and keywords. Machine-learning text categorization techniques needing little human intervention can replace or complete the previous methods to keep the filtering up-to-date easily. These echniques, usually used with topic classes (economy or sport for instance), are based on the presence or absence of words.We present an evaluation of these techniques for racism filtering. Unlike the traditional systems, documents are not categorized according to their main topic but according to the expressed point of view (racist or anti-racist). Our results show that these lexical techniques are well adapted : more than 90% of the documents are correctly classified, or even 99% if a rejection class is accepted (20% of the examples are not classified).</abstract>
			<keywords>Text classification, Rocchio, kNN, SVM, Internet, information filtering</keywords>
		</article>
		<article id="taln-2003-long-027" session="">
			<auteurs>
				<auteur>
					<prenom>Pierre</prenom>
					<nom>Zweigenbaum</nom>
					<email>pz@biomath.jussieu.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Fadila</prenom>
					<nom>Hadouche</nom>
					<email>fha@biomath.jussieu.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Natalia</prenom>
					<nom>Grabar</nom>
					<email>ngr@biomath.jussieu.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Mission de recherche en Sciences et Technologies de l’Information Médicale, STIM/DPA/DSI, Assistance Publique – Hôpitaux de Paris &amp; ERM 202, INSERM</affiliation>
			</affiliations>
			<titre>Apprentissage de relations morphologiques en corpus</titre>
			<type>long</type>
			<pages>285-294</pages>
			<resume>Nous proposons une méthode pour apprendre des relations morphologiques dérivationnelles en corpus. Elle se fonde sur la cooccurrence en corpus de mots formellement proches et un filtrage complémentaire sur la forme des mots dérivés. Elle est mise en oeuvre et expérimentée sur un corpus médical. Les relations obtenues avant filtrage ont une précision moyenne de 75,6 % au 5000è rang (fenêtre de 150 mots). L’examen détaillé des dérivés adjectivaux d’un échantillon de 633 noms du champ de l’anatomie montre une bonne précision de 85–91 % et un rappel modéré de 32–34 %. Nous discutons ces résultats et proposons des pistes pour les compléter.</resume>
			<mots_cles>Morphologie, apprentissage, corpus, langue de spécialité, médecine</mots_cles>
			<title></title>
			<abstract>We propose a method to learn derivational morphological relations from a corpus. It relies on corpus cooccurrence of formally similar words, with additional filtering on the form of derived words. It is implemented and tested on a medical corpus. The relations obtained before filtering have an average precision of 75.6% at rank 5000 (150-word window). A detailed examination of derived adjectives for a sample of 633 anatomy nouns shows a good precision of 85–91% and a moderate recall of 32–34%. We discuss these results and propose directions for improvement.</abstract>
			<keywords>Morphology, learning, corpus, specialized language, medicine</keywords>
		</article>
		<article id="taln-2003-poster-001" session="">
			<auteurs>
				<auteur>
					<prenom>Glenda B.</prenom>
					<nom>Anaya</nom>
					<email>anayaca@cs.concordia.ca</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Leila</prenom>
					<nom>Kosseim</nom>
					<email>kosseim@cs.concordia.ca</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">CLaC Laboratory - Concordia University 1400 de Maisonneuve West, Montr´eal, Canada H3G 1M8</affiliation>
			</affiliations>
			<titre></titre>
			<type>poster</type>
			<pages>297-302</pages>
			<resume>Le but des systèmes de question-réponse est de trouver des réponses exactes et factuelles à des questions exprimées en langue naturelle en recherchant dans une grande collection de documents. Notre recherche vise plutôt à générer des réponses complètes, sous forme de phrases, étant donnée la réponse exacte. La génération de telles phrases-réponses est une tâche importante car ces phrases peuvent être employées par un système de question-réponse pour améliorer la recherche de réponses exactes ou bien, pour améliorer l’interface entre le système et l’utilisateur en fournissant des réponses plus naturelles. Suite à une étude de corpus de phrases réponses, nous avons développé un ensemble de patrons syntaxiques de réponses correspondant à chaque patron syntaxique de question.</resume>
			<mots_cles>Système de question-réponse, Génération de texte, Patrons syntaxiques, Paraphrases</mots_cles>
			<title>Generation of natural responses through syntactic patterns</title>
			<abstract>The goal of Question-Answering (QA) systems is to find short and factual answers to opendomain questions by searching a large collection of documents. The subject of this research is to formulate complete and natural answer-sentences to questions, given the short answer. The answer-sentences are meant to be self-sufficient; that is, they should contain enough context to be understood without needing the original question. Generating such sentences is important in question-answering as they can be used to enhance existing QA systems to provide answers to the user in a more natural way and to provide a pattern to actually extract the answer from the document collection.</abstract>
			<keywords>Question-answering, Natural Language Generation, Syntactic Patterns, Paraphrases</keywords>
		</article>
		<article id="taln-2003-poster-002" session="">
			<auteurs>
				<auteur>
					<prenom>Sébastien</prenom>
					<nom>Barrier</nom>
					<email>sbarrier@linguist.jussieu.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Nicolas</prenom>
					<nom>Barrier</nom>
					<email>nbarrier@linguist.jussieu.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Laboratoire LLF - Université Paris 7 UFR de Linguistique 2, Place Jussieu - 75251 Paris Cedex 05</affiliation>
			</affiliations>
			<titre>Une métagrammaire pour les noms prédicatifs du français</titre>
			<type>poster</type>
			<pages>303-308</pages>
			<resume>La grammaire FTAG du français a vu ces dernières années ses données s’accroître très fortement. D’abord écrits manuellement, les arbres qui la composent, ont ensuite été générés semi-automatiquement grâce à une Métagrammaire, développée tout spécialement. Après la description des verbes en 1999, puis celle des adjectifs en 2001-2002, c’est maintenant au tour des verbes supports et des noms prédicatifs de venir enrichir les descriptions syntaxiques de la grammaire. Après un rappel linguistique et technique des notions de verbe support et de métagrammaire, cet article présente les choix qui ont été entrepris en vue de la description de ces nouvelles données.</resume>
			<mots_cles>Grammaires d’arbres adjoints, FTAG, noms prédicatifs, verbes supports, métagrammaire, ordre des constituants</mots_cles>
			<title></title>
			<abstract>We present here a new implementation of support verbs for the FTAG grammar, a french implementation of the Tree Adjoining Grammar model. FTAG has know over the years many improvements. (Candito, 1999) hence integrated an additional layer of syntactic representation within the system. The layer, we called MetaGrammar let us improve the syntactic coverage of our grammar by genrating semi-automatically thousands of new elementary trees.</abstract>
			<keywords>Tree-Adjoining Grammars, FTAG, predicative noun, support verb constructions, metagrammar, constituency order</keywords>
		</article>
		<article id="taln-2003-poster-003" session="">
			<auteurs>
				<auteur>
					<prenom>Denis</prenom>
					<nom>Béchet</nom>
					<email>Denis.Bechet@irisa.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Annie</prenom>
					<nom>Foret</nom>
					<email>Annie.Foret@irisa.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">INRIA &amp; Université de Rennes 1, IRISA Campus Universitaire de Beaulieu Avenue du Général Leclerc 35042 Rennes Cedex France</affiliation>
			</affiliations>
			<titre>Remarques et perspectives sur les langages de prégroupe d’ordre 1/2</titre>
			<type>poster</type>
			<pages>309-314</pages>
			<resume>Cet article traite de l’acquisition automatique des grammaires de Lambek, utilisées pour la modélisation syntaxique des langues. Récemment, des algorithmes ont été proposés dans le modèle d’apprentissage de Gold, pour certaines classes de grammaires catégorielles. En revenche, les grammaires de Lambek rigides ou k-valuées ne sont pas apprenables à partir des chaînes. Nous nous intéressons ici au cas des grammaires de prégroupe. Nous montrons que la classe des grammaires de prégroupe n’est pas apprenable à partir des chaînes, même si on limite fortement l’ordre des types (ordre 1/2) ; notre preuve revient à construire un point limite pour cette classe.</resume>
			<mots_cles>Acquisition automatique, inférence grammaticale, modèle de Gold, prégroupe</mots_cles>
			<title></title>
			<abstract>The article is concerned by the automatic acquistion of some grammars introduced by Lambek that are used for modeling the syntax of natural languages. Recently, some algorithms have been proposed in the model of Gold for several classes of categorial grammars. On the other hand, rigid or k-valued Lambek calculus grammars are not learnable from strings. We study here pregroup grammars. We prove that rigid grammars are not learnable from strings even if the order of types are bound by a constant (1/2) ; Our proof gives a limit point for this class of grammars.</abstract>
			<keywords>Automatic acquisition, grammatical inference, model of Gold, pregoup</keywords>
		</article>
		<article id="taln-2003-poster-004" session="">
			<auteurs>
				<auteur>
					<prenom>Frédérik</prenom>
					<nom>Bilhaut</nom>
					<email>fbilhaut@info.unicaen.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Lydia-Mai</prenom>
					<nom>Ho-Dac</nom>
					<email>hodac@univ-tlse2.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Andrée</prenom>
					<nom>Borillo</nom>
					<email>aborillo@univ-tlse2.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Thierry</prenom>
					<nom>Charnois</nom>
					<email>charnois@info.unicaen.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Patrice</prenom>
					<nom>Enjalbert</nom>
					<email>patrice@info.unicaen.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Anne</prenom>
					<nom>Le Draoulec</nom>
					<email>draoulec@univ-tlse2.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Yann</prenom>
					<nom>Mathet</nom>
					<email>mathet@info.unicaen.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Hélène</prenom>
					<nom>Miguet</nom>
					<email>miguet@univ-tlse2.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Marie-Paule</prenom>
					<nom>Péry-Woodley</nom>
					<email>pery@univ-tlse2.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Laure</prenom>
					<nom>Sarda</nom>
					<email>lsarda@univ-tlse2.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">GREYC, Université de Caen, Campus II</affiliation>
				<affiliation affiliationId="2">ERSS, Université Toulouse Le Mirail</affiliation>
			</affiliations>
			<titre>Indexation discursive pour la navigation intradocumentaire : cadres temporels et spatiaux dans l’information géographique</titre>
			<type>poster</type>
			<pages>315-320</pages>
			<resume>Cet article concerne la structuration automatique de documents par des méthodes linguistiques. De telles procédures sont rendues nécessaires par les nouvelles tâches de recherche d’information intradocumentaires (systèmes de questions-réponses, navigation sélective dans des documents...). Nous développons une méthode exploitant la théorie de l’encadrement du discours de Charolles, avec une application visée en recherche d’information dans les documents géographiques - d’où l’intérêt tout particulier porté aux cadres spatiaux et temporels. Nous décrivons une implémentation de la méthode de délimitation de ces cadres et son exploitation pour une tâche d’indexation intratextuelle croisant les critères spatiaux et temporels avec des critères thématiques.</resume>
			<mots_cles>Analyse automatique de discours, Cadres de discours, Recherche d’information, Document géographique</mots_cles>
			<title></title>
			<abstract>This paper proposes linguistics-based methods for the automatic identification of text segments. Such procedures are required by new tasks appearing in intra-document information retrieval (question-answer systems, selective browsing). Our method is based on Charolles’ theory of discourse framing and focuses on temporal and spatial frames. We describe an implementation of our method for determining frame boundaries and its exploitation for intradocument indexing combining spatial, temporal and thematic criteria.</abstract>
			<keywords>Automatic discourse analysis, Discourse framing, Information retrieval, Geographical documents</keywords>
		</article>
		<article id="taln-2003-poster-005" session="">
			<auteurs>
				<auteur>
					<prenom>Philippe</prenom>
					<nom>Blache</nom>
					<email>pb@lpl.univ-aix.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LPL-CNRS, Université de Provence 29, Avenue Robert Schuman 13621 Aix-en-Provence</affiliation>
			</affiliations>
			<titre>Vers une théorie cognitive de la langue basée sur les contraintes</titre>
			<type>poster</type>
			<pages>321-326</pages>
			<resume>Cet article fournit des éléments d’explication pour la description des relations entre les différents domaines de l’analyse linguistique. Il propose une architecture générale en vue d’une théorie formée de plusieurs niveaux : d’un côté les grammaires de chacun des domaines et de l’autre des relations spécifiant les interactions entre ces domaines. Dans cette approche, chacun des domaines est porteur d’une partie de l’information, celle-ci résultant également de l’interaction entre les domaines.</resume>
			<mots_cles></mots_cles>
			<title></title>
			<abstract></abstract>
			<keywords></keywords>
		</article>
		<article id="taln-2003-poster-006" session="">
			<auteurs>
				<auteur>
					<prenom>Marie</prenom>
					<nom>Chagnoux</nom>
					<email>Marie.Chagnoux@paris4.sorbonne.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Slim</prenom>
					<nom>Ben Hazez</nom>
					<email>Slim.Ben-hazez@paris4.sorbonne.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Jean-Pierre</prenom>
					<nom>Desclés</nom>
					<email>Jean-Pierre.Descles@paris4.sorbonne.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LaLICC - UMR 8139 du CNRS - Université de Paris 4 -Sorbonne 96, Boulevard Raspail, 75006 Paris</affiliation>
			</affiliations>
			<titre>Identification automatique des valeurs temporelles dans les textes</titre>
			<type>poster</type>
			<pages>327-332</pages>
			<resume>Cet article présente une application qui associe un certain nombre de valeurs sémantiques à des segments textuels en vue de proposer un traitement automatique de la temporalité dans les textes. Il s’agit d’automatiser une analyse sémantique de surface à l’aide de règles heuristiques d’exploration contextuelle et d’une base organisée de marqueurs linguistiques.</resume>
			<mots_cles>Traitement aspectuo-temporel, temps, temporalité, exploration contextuelle, analyse sémantique de surface, ressources linguistiques</mots_cles>
			<title></title>
			<abstract>This paper presents a system which associates semantic Values with textual segments in order to propose an automatic processing of temporality in texts. The purpose is to automate a semantic analysis driven with contextual exploration heuristic rules and an organized data base of linguistic markers.</abstract>
			<keywords>Aspecto-temporal processing, time, temporality, contextual exploration, semantic analysis, linguistic resources</keywords>
		</article>
		<article id="taln-2003-poster-007" session="">
			<auteurs>
				<auteur>
					<prenom>Adil</prenom>
					<nom>El Ghali</nom>
					<email>adil@linguist.jussieu.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Laurent</prenom>
					<nom>Roussarie</nom>
					<email>laurent@linguist.jussieu.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LATTICE – PPSUniversité Paris 7 – Case 7003 2, place jussieu 75251 Paris Cedex 05</affiliation>
				<affiliation affiliationId="2">LATTICE – CNRS Université Paris 7 – Case 7003 2, place jussieu 75251 Paris Cedex 05</affiliation>
			</affiliations>
			<titre>Structuration automatique de preuves mathématiques : de la logique à la rhétorique</titre>
			<type>poster</type>
			<pages>333-338</pages>
			<resume>Nous présentons dans ses grandes lignes un modèle de structuration de documents pour la génération automatique de preuves mathématiques. Le modèle prend en entrée des sorties d’un prouveur automatique et vise à produire des textes dont le style s’approche le plus possible des démonstrations rédigées par des humains. Cela implique la mise au point d’une stratégie de planification de document capable de s’écarter de la structure purement logique de la preuve. La solution que nous proposons consiste à intégrer de manière simple des informations de type intentionnel afin d’enrichir la structure rhétorique finale du texte.</resume>
			<mots_cles>Génération automatique de textes, détermination de contenu, logiques de description, structuration de document, SDRT</mots_cles>
			<title></title>
			<abstract></abstract>
			<keywords>NLG, content determination, description logics, document structuring, SDRT</keywords>
		</article>
		<article id="taln-2003-poster-008" session="">
			<auteurs>
				<auteur>
					<prenom>Chantal</prenom>
					<nom>Enguehard</nom>
					<email>chantal.enguehard@irin.univ-nantes.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">IRIN – Université de Nantes 2 rue de la Houssinière BP 92208 Nantes cedex 3, France</affiliation>
			</affiliations>
			<titre>CoRRecT : Démarche coopérative pour l’évaluation de systèmes de reconnaissance de termes</titre>
			<type>poster</type>
			<pages>339-346</pages>
			<resume>La reconnaissance de termes dans les textes intervient dans de nombreux domaines du Traitement Automatique des Langues Naturelles, qu’il s’agisse d’indexation automatique, de traduction, ou d’extraction de connaissances. Nous présentons une méthodologie d’évaluation de Systèmes de Reconnaissance de Termes (SRT) qui vise à minimiser le temps d’expertise des spécialistes en faisant coopérer des SRT. La méthodologie est mise en oeuvre sur des textes en anglais dans le domaine de la chimie des métaux et à l’aide de deux SRT : FASTR et SYRETE. Le banc de test construit selon cette méthodologie a permis de valider les SRT et d’évaluer leurs performances en termes de rappel et de précision.</resume>
			<mots_cles>Reconnaissance de termes, évaluation de la reconnaissance de termes, SRT, FASTR, SYRETE</mots_cles>
			<title></title>
			<abstract>Recognizing terms in texts is useful in many Natural Language Processing applications : automatic indexation, summarization, translation, or knowledge extraction. We present a new methodology to evaluate Term Recognition Systems (TRS) so as to minimize the time required by experts to evaluate the results. This is done by making several TRS cooperate. This methodology is applied on English texts on metal chemistry with two systems : FASTR and SYRETE. The test bank we compiled evaluated the two systems and calculated the recall and precision rates.</abstract>
			<keywords>Recognition of terms, Term Recognition System, TRS, FASTR, SYRETE</keywords>
		</article>
		<article id="taln-2003-poster-009" session="">
			<auteurs>
				<auteur>
					<prenom>Olivier</prenom>
					<nom>Ferret</nom>
					<email>olivier.ferret@cea.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">CEA – LIST/LIC2M 92265 Fontenay-aux-Roses Cedex</affiliation>
			</affiliations>
			<titre>Filtrage thématique d’un réseau de collocations</titre>
			<type>poster</type>
			<pages>347-352</pages>
			<resume>Les réseaux lexicaux de type WordNet présentent une absence de relations de nature thématique, relations pourtant très utiles dans des tâches telles que le résumé automatique ou l’extraction d’information. Dans cet article, nous proposons une méthode visant à construire automatiquement à partir d’un large corpus un réseau lexical dont les relations sont préférentiellement thématiques. En l’absence d’utilisation de ressources de type dictionnaire, cette méthode se fonde sur un principe d’auto-amorçage : un réseau de collocations est d’abord construit à partir d’un corpus puis filtré sur la base des mots du corpus que le réseau initial a permis de sélectionner. Nous montrons au travers d’une évaluation portant sur la segmentation thématique que le réseau final, bien que de taille bien inférieure au réseau initial, permet d’obtenir les mêmes performances que celui-ci pour cette tâche.</resume>
			<mots_cles>Collocations, cooccurrences lexicales, réseaux lexicaux thématiques, analyse thématique</mots_cles>
			<title></title>
			<abstract>Lexical networks such as WordNet are known to have a lack of topical relations although these ones are very useful for tasks such as text summarization or information extraction. In this article, we present a method for automatically building from a large corpus a lexical network whose relations are preferably topical ones. As it does not rely on resources such as dictionaries, this method is based on self-bootstrapping: a collocation network is first built from a corpus and then, is filtered by using the words of the corpus that are selected by the initial network. We report an evaluation about topic segmentation showing that the results got with the filtered network are the same as the results got with the initial network although the first one is signicantly smaller than the second one.</abstract>
			<keywords>Collocations, lexical cooccurrences, topical lexical networks, topic analysis</keywords>
		</article>
		<article id="taln-2003-poster-010" session="">
			<auteurs>
				<auteur>
					<prenom>Núria</prenom>
					<nom>Gala Pavia</nom>
					<email>gala@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">XRCE et LIMSI-CNRS Bt. 508 Université de Paris-Sud 91403 Orsay Cedex</affiliation>
			</affiliations>
			<titre>Une méthode non supervisée d’apprentissage sur le Web pour la résolution d’ambiguïtés structurelles liées au rattachement prépositionnel</titre>
			<type>poster</type>
			<pages>353-358</pages>
			<resume>Dans cet article, nous proposons une méthode non supervisée d’apprentissage qui permet d’améliorer la désambiguïsation du rattachement prépositionnel dans le cadre d’un analyseur robuste à base de règles pour le français. Les rattachements ambigus d’une première analyse sont transformés en requêtes sur leWeb dans le but de créer un grand corpus qui sera analysé et d’où seront extraites automatiquement des informations lexicales et statistiques sur les rattachements. Ces informations seront ensuite utilisées dans une deuxième analyse pour lever les ambiguïtés des rattachements. L’avantage d’une telle méthode est la prise en compte de cooccurrences syntaxiques et non pas des cooccurrences purement textuelles. En effet, les mesures statistiques (poids) sont associées à des mots apparaissant initialement dans une même relation de dépendance, c’est-à-dire, des attachements produits par le parseur lors d’une première analyse.</resume>
			<mots_cles>Analyse syntaxique robuste, grammaires de dépendances, apprentissage non supervisé, désambiguïsation du rattachement prépositionnel</mots_cles>
			<title></title>
			<abstract>In this paper we describe an unsupervised method which improves the disambiguation of prepositional attachments in an existing rule-based dependency parser for French. The results obtained after a first analysis (ambiguous attachments) are transformed into queries to the Web in order to obtain a very big corpus. This big corpus being parsed, lexical and statistical information is extracted to create a database that will be used in a second analysis to disambiguate conflictual attachments. The advantage of such a method is to take into account syntactic cooccurrences as opposed to rough textual co-occurrences. That means that statistical measures (weights) are associated to words already co-occurring in a dependency relation, that is, attachments yield by the parser after a first analysis.</abstract>
			<keywords>Robust parsing, dependency grammars, unsupervised learning, prepositional phrase attachment resolution</keywords>
		</article>
		<article id="taln-2003-poster-011" session="">
			<auteurs>
				<auteur>
					<prenom>Laurent</prenom>
					<nom>Gillard</nom>
					<email>laurent.gillard@lia.univ-avignon.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Patrice</prenom>
					<nom>Bellot</nom>
					<email>patrice.bellot@lia.univ-avignon.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Marc</prenom>
					<nom>El-Bèze</nom>
					<email>marc.elbeze@lia.univ-avignon.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Laboratoire d’Informatique d’Avignon (LIA) 339 ch. des Meinajaries, BP 1228 ; F-84911 Avignon Cedex 9 (France)</affiliation>
			</affiliations>
			<titre>Bases de connaissances pour asseoir la crédibilité des réponses d’un système de Q/R </titre>
			<type>poster</type>
			<pages>359-364</pages>
			<resume>Cet article présente un prototype de Question/Réponse (Q/R) impliquant un ensemble de bases de connaissances (BC) dont l’objectif est d’apporter un crédit supplémentaire aux réponses candidates trouvées. Ces BC et leur influence sur la stratégie dordonnancement mise en uvre sont décrites dans le cadre de la participation du système à la campagne Q/R de TREC-2002.</resume>
			<mots_cles>Système de Question/Réponse, Bases de Connaissances</mots_cles>
			<title></title>
			<abstract>This paper presents a Question-Answering system using Knowledge Databases (KDB) to validate answers candidates.</abstract>
			<keywords>Question Answering system, Knowledge Databases</keywords>
		</article>
		<article id="taln-2003-poster-012" session="">
			<auteurs>
				<auteur>
					<prenom>André</prenom>
					<nom>Jaccarini</nom>
					<email>André.Jaccarini@paris4.sorbonne.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Mourad</prenom>
					<nom>Ghassan</nom>
					<email>Mourad.Ghassan@paris4.sorbonne.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Christian</prenom>
					<nom>Gaubert</nom>
					<email>cgaubert@link.net</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Brahim</prenom>
					<nom>Djioua</nom>
					<email>Brahim.Djioua@paris4.sorbonne.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LaLICC – UMR 8139 Université de Paris-Sorbonne – CNRS 96, Bd Raspail Paris 75006</affiliation>
				<affiliation affiliationId="2">IFAO Rue al-Cheikh Ali Youssef. Qasr al-Aïny 11562, Le Caire</affiliation>
			</affiliations>
			<titre>Un logiciel pour la mise au point de grammaires pour le filtrage d’information en arabe (cas de l’information citationnelle)</titre>
			<type>poster</type>
			<pages>365-372</pages>
			<resume>Nous présentons dans ce travail un logiciel de mise au point de grammaires pour le traitement morpho-syntaxique de l’arabe et l’établissement de grammaires pour le filtrage et l’extraction d’information en arabe. Ce logiciel est fondé sur le principe des automates. L’analyse morpho-syntaxique de l’arabe est réalisé sans le recours au lexique.</resume>
			<mots_cles>Sarfiyya, traitement automatique de l’arabe, automate, filtrage d’information, citation</mots_cles>
			<title></title>
			<abstract>We present in this work a software of grammars development for the arabic morpho-syntactical processing and for the construction of filtering grammars and information retrieval. This software is founded on the automata. It can parse arabic text without lexicon.</abstract>
			<keywords>Sarfiyya, Arabic processing, automaton, information retrieval</keywords>
		</article>
		<article id="taln-2003-poster-013" session="">
			<auteurs>
				<auteur>
					<prenom>Marisa</prenom>
					<nom>Jiménez</nom>
					<email>marialj@microsoft.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Martine</prenom>
					<nom>Pettenaro</nom>
					<email>martinep@microsoft.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Microsoft Research One Microsoft Way Redmond, WA 98052, USA</affiliation>
			</affiliations>
			<titre></titre>
			<type>poster</type>
			<pages>373-378</pages>
			<resume>Nous décrivons dans cet article lutilisation darbres décisionnels pour lacquisition dinformations lexicales et lenrichissement de notre système de traitement automatique des langues naturelles (NLP). Notre approche diffère dautres projets dapprentissage automatique en ce quelle repose sur lexploitation dun système danalyse linguistique profonde. Après lintroduction de notre sujet nous présentons larchitecture de notre module dapprentissage lexical. Nous présentons ensuite une situation dapprentissage lexical effectué en utilisant des arbres décisionnels; nous apprenons quels verbes prennent un sujet humain en espagnol et en français.</resume>
			<mots_cles>Apprentissage lexical, apprentissage automatique, arbres décisionnels, dictionnaires automatiquement appris</mots_cles>
			<title>Using decision trees to learn lexical information in a linguistics-based NLP system</title>
			<abstract>This paper describes the use of decision trees to learn lexical information for the enrichment of our natural language processing (NLP) system. Our approach to lexical learning differs from other approaches in the field in that our machine learning techniques exploit a deep knowledge understanding system. After the introduction we present the overall architecture of our lexical learning module. In the following sections we present a showcase of lexical learning using decision trees: we learn verbs that take a human subject in Spanish and French.</abstract>
			<keywords>Lexical learning, machine learning, decision trees, learned dictionaries</keywords>
		</article>
		<article id="taln-2003-poster-014" session="">
			<auteurs>
				<auteur>
					<prenom>Guy</prenom>
					<nom>Lapalme</nom>
					<email>lapalme@iro.umontreal.ca</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Caroline</prenom>
					<nom>Brun</nom>
					<email>Caroline.Brun@xrce.xerox.com</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Marc</prenom>
					<nom>Dymetman</nom>
					<email>Marc.Dymetman@xrce.xerox.com</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">RALI - Université de Montréal Montréal (Québec)</affiliation>
				<affiliation affiliationId="2">Xerox Research Centre Europe 6, chemin de Maupertuis, 38240 Meylan</affiliation>
				<affiliation affiliationId="3"></affiliation>
			</affiliations>
			<titre>MDA-XML : une expérience de rédaction contrôlée multilingue basée sur XML</titre>
			<type>poster</type>
			<pages>379-384</pages>
			<resume>Nous décrivons dans cet article l’implantation d’un système de rédaction contrôlée multilingue dans un environnement XML. Avec ce système, un auteur rédige interactivement un texte se conformant à des règles de bonne formation aux niveaux du contenu sémantique et de la réalisation linguistique décrites par un schéma XML. Nous discutons les avantages de cette approche ainsi que les difficultés rencontrées lors du développement de ce système. Nous concluons avec un exemple d’application à une classe de documents pharmaceutiques.</resume>
			<mots_cles>rédaction contrôlée multilingue, XML</mots_cles>
			<title></title>
			<abstract>We describe an XML implementation of a multilingual authoring system. Using this system, an author can interactively write a text conforming to well-formedness content and realization rules described by an XML schema. We discuss the advantages of such an approach as well as the problems we met during the implementation of our system. We show in the end an example of an application for a class of pharmaceutical documents.</abstract>
			<keywords>multilingual controlled authoring, XML</keywords>
		</article>
		<article id="taln-2003-poster-015" session="">
			<auteurs>
				<auteur>
					<prenom>Freddy</prenom>
					<nom>Perraud</nom>
					<email>freddy.perraud@visionobjects.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Emmanuel</prenom>
					<nom>Morin</nom>
					<email>morin@irin.univ-nantes.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Christian</prenom>
					<nom>Viard-Gaudin</nom>
					<email>christian.viard-gaudin@polytech.univ-nantes.fr</email>
					<affiliationId>3</affiliationId>
				</auteur>
				<auteur>
					<prenom>Pierre-Michel</prenom>
					<nom>Lallican</nom>
					<email>pmlallican@visionobjects.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Société Vision Objects - 9, rue du Pavillon - 44980 Sainte Luce sur Loire</affiliation>
				<affiliation affiliationId="2">Institut de Recherche en Informatique de Nantes 2, rue de la Houssinière - BP 92208 - 44322 Nantes Cedex 3</affiliation>
				<affiliation affiliationId="3">Institut de Recherche en Communications et Cybernétique de Nantes - UMR CNRS La Chantrerie - Rue Christian Pauc - BP 50609 - 44306 Nantes Cedex 3</affiliation>
			</affiliations>
			<titre>Apport dun modèle de langage statistique pour la reconnaissance de lécriture manuscrite en ligne</titre>
			<type>poster</type>
			<pages>385-390</pages>
			<resume>Dans ce travail, nous étudions lapport dun modèle de langage pour améliorer les performances des systèmes de reconnaissance de lécriture manuscrite en-ligne. Pour cela, nous avons exploré des modèles basés sur des approches statistiques construits par apprentissage sur des corpus écrits. Deux types de modèles ont été étudiés : les modèles n-grammes et ceux de type n-classes. En vue de lintégration dans un système de faible capacité (engin nomade), un modèle n-classe combinant critères syntaxiques et contextuels a été défini, il a permis dobtenir des résultats surpassant ceux donnés avec un modèle beaucoup plus lourd de type n-gramme. Les résultats présentés ici montrent quil est possible de prendre en compte les spécificités dun langage en vue de reconnaître lécriture manuscrite avec des modèles de taille tout à fait raisonnable.</resume>
			<mots_cles>Reconnaissance de lécriture manuscrite, modèle de langage, n-gramme, n-classe, perplexité</mots_cles>
			<title></title>
			<abstract>This works highlights the interest of a language model in increasing the performances of on-line handwriting recognition systems. Models based on statistical approaches, trained on written corpora, have been investigated. Two kinds of models have been studied: n-gram models and n-class models. In order to integrate it into small capacity systems (mobile device), a n-class model has been designed by combining syntactic and contextual criteria. It outperforms bulkier models based on n-gram. The results we obtain show that it is possible to take advantage of language specificities to recognize handwritten sentences by using reasonable size models.</abstract>
			<keywords>Handwriting recognition, language modelling, n-gram, n-class, perplexity</keywords>
		</article>
		<article id="taln-2003-poster-016" session="">
			<auteurs>
				<auteur>
					<prenom>Martin</prenom>
					<nom>Rajman</nom>
					<email>martin.rajman@epfl.ch</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Andréa</prenom>
					<nom>Rajman</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Florian</prenom>
					<nom>Seydoux</nom>
					<email>florian.seydoux@epfl.ch</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Alex</prenom>
					<nom>Trutnev</nom>
					<email>alex.trutnev@epfl.ch</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Laboratoire d’Intelligence Artificielle, E.P.F. Lausanne (Suisse)</affiliation>
			</affiliations>
			<titre>Prototypage rapide et évaluation de modèles de dialogue finalisés</titre>
			<type>poster</type>
			<pages>391-396</pages>
			<resume>L’objectif de cette contribution est de présenter l’intégration de la notion d’évaluation dans la méthodologie de prototypage rapide de modèles de dialogue développée et mise en oeuvre dans le cadre du projet InfoVox. L’idée centrale de cette méthodologie est de dériver un modèle de dialogue opérationnel directement à partir du modèle de la tâche à laquelle il est associé. L’intégration systématique de différents aspects de l’évaluation dans le processus de prototypage est alors utile afin d’identifier, dès la phase de conception, les qualités et défauts de l’interface. Toutes les conclusions présentées seront illustrées par des résultats concrets obtenus au cours d’expériences réalisées dans le cadre du projet InfoVox.</resume>
			<mots_cles>Evaluation, Dialogue Homme-Machine, Prototypage Rapide, Wizard-of-Oz</mots_cles>
			<title></title>
			<abstract></abstract>
			<keywords></keywords>
		</article>
		<article id="taln-2003-poster-017" session="">
			<auteurs>
				<auteur>
					<prenom>Aristomenis</prenom>
					<nom>Thanopoulos</nom>
					<email>aristom@wcl.ee.upatras.gr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Nikos</prenom>
					<nom>Fakotakis</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>George</prenom>
					<nom>Kokkinakis</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Electrical and Computer Engineering Department, University of Patras 26500, Rion, Greece</affiliation>
			</affiliations>
			<titre></titre>
			<type>poster</type>
			<pages>397-402</pages>
			<resume></resume>
			<mots_cles></mots_cles>
			<title>Text Tokenization for Knowledge-free Automatic Extraction of Lexical Similarities</title>
			<abstract>Previous studies on automatic extraction of lexical similarities have considered as semantic unit of text the word. However, the theory of contextual lexical semantics implies that larger segments of text, namely non-compositional multiwords, are more appropriate for this role. We experimentally tested the applicability of this notion applying automatic collocation extraction to identify and merge such multiwords prior to the similarity estimation process. Employing an automatic WordNet-based comparative evaluation scheme along with a manual evaluation procedure, we ascertain improvement of the extracted similarity relations.</abstract>
			<keywords>Automatic methods, lexical similarity extraction, collocation extraction, automatic evaluation</keywords>
		</article>
		<article id="taln-2003-poster-018" session="">
			<auteurs>
				<auteur>
					<prenom>Yannick</prenom>
					<nom>Toussaint</nom>
					<email>Yannick.Toussaint@loria.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LORIA  INRIA BP 239 54506 Vandoeuvre-lès-Nancy</affiliation>
			</affiliations>
			<titre>Le traitement automatique de la langue contre les erreurs judiciaires : une méthodologie danalyse systématique des textes dun dossier dinstruction</titre>
			<type>poster</type>
			<pages>403-408</pages>
			<resume>Cet article présente une méthode danalyse systématique et scientifique des documents constituant un dossier dinstruction. Lobjectif de cette approche est de pouvoir donner au juge dinstruction de nouveaux moyens pour évaluer la cohérence, les incohérences, la stabilité ou les variations dans les témoignages. Cela doit lui permettre de définir des pistes pour mener de nouvelles investigations. Nous décrivons les travaux que nous avons réalisés sur un dossier réel puis nous proposons une méthode danalyse des résultats.</resume>
			<mots_cles>Catégorisation, synthèse, questionnabilité dun texte, traitement cognitifs dun texte, traitement automatique de la langue, Justice, dossier dinstruction</mots_cles>
			<title></title>
			<abstract>This article presents a systematic and scientific method to analyse the set of documents that constitutes a file of instruction. The goal of this approach is to give the examining magistrate new means to estimate the coherence, the incoherence, the stability or the variations in the testimonies. This analysis should help him in identifying the needs for new investigations. We applied our method to a real file and then we propose a way to read the results.</abstract>
			<keywords>Text categorisation, text synthesis, text questionning, cognitive treatment of texts, Justice and natural language processing</keywords>
		</article>
		<article id="taln-2003-poster-019" session="">
			<auteurs>
				<auteur>
					<prenom>Chiraz</prenom>
					<nom>Ben Othmane Zribi</nom>
					<email>adn@gnet.tn</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Mohamed</prenom>
					<nom>Ben Ahmed</nom>
					<email>Mohamed.BenAhmed@riadi.rnu.tn</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Laboratoire de recherche RIADI Université La Manouba, ENSI, La Manouba, Tunisie</affiliation>
			</affiliations>
			<titre>Le contexte au service de la correction des graphies fautives arabes</titre>
			<type>poster</type>
			<pages>409-414</pages>
			<resume>Les mots arabes sont lexicalement beaucoup plus proches les uns des autres que les mots français et anglais. Cette proximité a pour effet un grand nombre de propositions à la correction d'une forme erronée arabe. Nous proposons dans cet article une méthode qui prend en considération le contexte de l'erreur pour éliminer certaines propositions données par le correcteur. Le contexte de l'erreur sera dans un premier temps les mots voisinant l'erreur et s'étendra jusqu'à l'ensemble des mots du texte contenant l'erreur. Ayant été testée sur un corpus textuel contenant des erreurs réelles, la méthode que nous proposons aura permis de réduire le nombre moyen de propositions d'environ 75% (de 16,8 à 3,98 propositions en moyenne).</resume>
			<mots_cles>Langue, Arabe, Erreur orthographique, Correction automatique, Contexte</mots_cles>
			<title></title>
			<abstract>Arabic words are lexically closer to each other than can be English or French words. This proximity mainly results a great number of candidates given by a spelling corrector when processing an erroneous word. We address in this paper a new method aiming to reduce the number of proposals given by automatic Arabic spelling correction tools. We suggest the use of error's context in order to eliminate some correction candidates. Context will be nearby words and can be extended to all words in the text. Our method was tested on a corpus containing genuine errors and has yield good results. The average number of proposals has been reduced of about 75% (from 16,8 to 3,98 proposals on average).</abstract>
			<keywords>Language, Arabic, Misspelled word, Automatic correction, Context</keywords>
		</article>
		<article id="taln-2003-tutoriel-001" session="">
			<auteurs>
				<auteur>
					<prenom>Michael</prenom>
					<nom>Carl</nom>
					<email>carl@iai.uni-sb.de</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Institut für Angewandte Informationsforschung, Martin-Luther-Straße 14, 66111 Saarbrücken, Germany</affiliation>
			</affiliations>
			<titre>Introduction à la traduction guidée par l’exemple (Traduction par analogie)</titre>
			<type>tutoriel</type>
			<pages>11-26</pages>
			<resume>Le nombre d’approches en traduction automatique s’est multiplié dans les dernières années. Il existe entre autres la traduction par règles, la traduction statistique et la traduction guidée par l’exemple. Dans cet article je decris les approches principales en traduction automatique. Je distingue les approches qui se basent sur des règles obtenues par l’inspection des approches qui se basent sur des exemples de traduction. La traduction guidée par l’exemple se caractérise par la phrase comme unité de traduction idéale. Une nouvelle traduction est génerée par analogie : seulement les parties qui changent par rapport à un ensemble de traductions connues sont adaptées, modifiées ou substituées. Je présente quelques techniques qui ont été utilisées pour ce faire. Je discuterai un système spécifique, EDGAR, plus en detail. Je démontrerai comment des textes traduits alignés peuvent être preparés en termes de compilation pour extraire des unités de traduction sous-phrastiques. Je présente des résultats en traduction Anglais -> Français produits avec le système EDGAR en les comparant avec ceux d’un système statistique.</resume>
			<mots_cles>traduction guidée par l’exemple, traduction par analogie, traduction statistique, induction de grammaire de traduction</mots_cles>
			<title></title>
			<abstract>In this paper I characterize a number of machine translation approaches: rule-based machine translation (RBMT), statistical machine translation (SMT) and example-based machine translation (EBMT). While RBMT systems make use of hand-build rules, SMT and EBMT systems explore and re-use a set of reference translations. EBMT systems are rooted in analogical reasoning, where the ideal translation unit is the sentence. Only if an identical sentence cannot be found in the reference material, EBMT systems modify, substitute and adapt sequences of the retrieved examples to generate a suitable translation. I discuss runtime and compilation time techniques and I present a system, EDGAR, in more detail. I show how translation units are extracted off-line and how they are re-used during translation. The description of a series of experiments for the translation English -> French conclude this paper. An extended bibliography provides further pointer for interested readers.</abstract>
			<keywords>example-based machine translation, analogical translation, statistical machine translation, induction of translation grammar</keywords>
		</article>
		<article id="taln-2003-tutoriel-002" session="">
			<auteurs>
				<auteur>
					<prenom>Didier</prenom>
					<nom>Bourigault</nom>
					<email>didier.bourigault@univ-tlse2.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Nathalie</prenom>
					<nom>Aussenac-Gilles</nom>
					<email>aussenac@irit.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">ERSS  CNRS &amp; Université Toulouse le Mirail 5, allées Antonio Machado 31 058 Toulouse Cedex 1</affiliation>
				<affiliation affiliationId="2">IRIT  Université Paul Sabatier 118, route de Narbonne, 31062 Toulouse Cedex 4</affiliation>
			</affiliations>
			<titre>Construction dontologies à partir de textes</titre>
			<type>tutoriel</type>
			<pages>27-47</pages>
			<resume>Cet article constitue le support dun cours présenté lors de la conférence TALN 2003. Il défend la place du Traitement Automatique des Langues comme discipline clé pour le développement de ressources termino-ontologiques à partir de textes. Les contraintes et enjeux de ce processus sont identifiés, en soulignant limportance de considérer cette tâche comme un processus supervisé par un analyste. Sont présentés un certain nombre doutils logiciels et méthodologiques venant de plusieurs disciplines comme le TAL et lingénierie des connaissances qui peuvent aider lanalyste dans sa tâche. Divers retours dexpérience sont présentés.</resume>
			<mots_cles>Extraction de termes, extraction de relations, Terminologie, Ontologies, Ingénierie des connaissances, méthode, modélisation de connaissances, interdisciplinarité</mots_cles>
			<title></title>
			<abstract>This paper gathers the notes of a tutorial. We advocate in favour of the role of Natural Language Processing as a key discipline for the development of terminological and ontological resources from texts. The constraints and challenges of this process are identified, and lead to underline this task as a supervised processes carried out by an analyst. We present several software and methodological tools from NLP and knowledge engineering that can be use for to assist the analyst. Our suggestion rely on various experience feed-back.</abstract>
			<keywords>Term extraction, relation extraction, Terminology, ontologies, Knowledge Engineering, method, knowledge modelling, crossdisciplinarity</keywords>
		</article>
	</articles>
</conference>
