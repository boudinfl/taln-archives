<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
"http://www.w3.org/TR/html4/loose.dtd">
<html><head><title>Nouvelle approche de la s&#233;lection de vocabulaire pour la d&#233;tection de th&#232;me</title>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
</head>
<body>
<div style="page-break-before:always; page-break-after:always"><div><p>TALN 2003, Batz-sur-Mer, 11&#8211;14 juin 2003
</p>
<p>Nouvelle approche de la s&#233;lection de vocabulaire pour la
d&#233;tection de th&#232;me
</p>
<p>Armelle BRUN, Kamel SMAILI, Jean-Paul HATON
LORIA BP 239 54506 Vand&#339;uvre-L&#232;s-Nancy, France -
Tel : (33|0) 3-83-59-20-97, Fax :(33|0) 3-83-41-30-79
</p>
<p>{brun, smaili, jph}@loria.fr
</p>
<p>Mots-clefs &#8211; Keywords
</p>
<p>D&#233;tection de th&#232;me, cr&#233;ation de vocabulaire, combinaison
Topic detection, vocabulary creation, combination
</p>
<p>R&#233;sum&#233; - Abstract
</p>
<p>En reconnaissance de la parole, un des moyens d&#8217;am&#233;liorer les performances des syst&#232;mes est
de passer par l&#8217;adaptation des mod&#232;les de langage. Une &#233;tape cruciale de ce processus consiste &#224;
d&#233;tecter le th&#232;me du document trait&#233; et &#224; adapter ensuite le mod&#232;le de langage. Dans cet article,
nous proposons une nouvelle approche de cr&#233;ation des vocabulaires utilis&#233;s pour la d&#233;tection
de th&#232;me. Cette derni&#232;re est fond&#233;e sur le d&#233;veloppement de vocabulaires sp&#233;cifiques et ca-
ract&#233;ristiques des diff&#233;rents th&#232;mes. Nous montrons que cette approche permet non seulement
d&#8217;am&#233;liorer les performances des m&#233;thodes, mais exploite &#233;galement des vocabulaires de taille
r&#233;duite. De plus, elle permet d&#8217;am&#233;liorer de fa&#231;on tr&#232;s significative les performances de m&#233;-
thodes de d&#233;tection lorsqu&#8217;elles sont combin&#233;es.
</p>
<p>One way to improve performance of Automatic Speech Recognition (ASR) systems consists in
adapting language models. We are particularly interested in adapting language models to the
topic related in data. Before adapting the language model, this topic has to be detected. In this
work, we present a new way to create vocabularies used to detect the topic in a given text.
This new method results in the improvement of topic detection performance of the methods
studied, it also results in the reduction of the vocabulary size required. Finally, we show a
large improvement of the performance when combining topic identification methods, when new
vocabularies are used.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>A. Brun, K. Sma&#239;li, J.P. Haton
</p>
<p>1 Introduction
</p>
<p>Les syst&#232;mes de Reconnaissance Automatique de la Parole (RAP) actuels atteignent des perfor-
mances int&#233;ressantes dans des applications cibl&#233;es. Les donn&#233;es en entr&#233;e d&#8217;un syst&#232;me de RAP
se pr&#233;sentent sous la forme d&#8217;un signal acoustique correspondant &#224; une phrase prononc&#233;e. Ces
donn&#233;es sont tout d&#8217;abord trait&#233;es par un module de traitement du signal acoustique. Malgr&#233;
des performances tr&#232;s &#233;lev&#233;es de ce dernier, son utilisation seule ne permet pas d&#8217;obtenir des
r&#233;sultats de reconnaissance suffisamment &#233;lev&#233;s. En effet, bien que les phrases propos&#233;es par le
syst&#232;me soient tr&#232;s proches acoustiquement de la suite de mots prononc&#233;e, ces derni&#232;res sont
bien souvent syntaxiquement incorrectes. Pour palier les faiblesses de ce module, un second
mod&#232;le est exploit&#233;, en compl&#233;ment de celui-ci. Il a pour fonction de mod&#233;liser la langue et
aura donc pour r&#244;le d&#8217;affiner les diff&#233;rents scores des phrases propos&#233;es par le module acous-
tique, c&#8217;est le mod&#232;le de langage.
</p>
<p>Les mod&#232;les statistiques de langage des syst&#232;mes de RAP mod&#233;lisent la langue sous forme
probabiliste. Plus particuli&#232;rement, ils &#233;valuent la probabilit&#233; d&#8217;apparition d&#8217;un mot sachant les
mots le pr&#233;c&#233;dant dans la phrase (son historique). Les mod&#232;les les plus utilis&#233;s &#224; l&#8217;heure actuelle
sont les mod&#232;les de langage dits &#0;&#2;&#1; grammes. Ils estiment la probabilit&#233; d&#8217;apparition d&#8217;un mot
uniquement en fonction des &#0;&#3;&#1;&#5;&#4; derniers mots le pr&#233;c&#233;dant. Pour des raisons d&#8217;estimation
de probabilit&#233;s et de stockage, la taille de l&#8217;historique pris en compte ( &#0;&#6;&#1;&#7;&#4; ) ne d&#233;passe g&#233;-
n&#233;ralement pas &#8; . Le reproche fait &#224; ce type de mod&#232;le est justement de prendre en compte un
historique trop restreint. Pour pallier cet inconv&#233;nient, de nombreux mod&#232;les ont &#233;t&#233; d&#233;velopp&#233;s
dans le but de mieux pr&#233;dire les mots.
</p>
<p>Une des approches utilis&#233;es pour l&#8217;am&#233;lioration de la qualit&#233; de ces mod&#232;les consiste &#224; adapter
le mod&#232;le de langage du syst&#232;me aux caract&#233;ristiques du texte en cours de traitement. Dans
ce cadre, nous nous int&#233;ressons &#224; l&#8217;adaptation des mod&#232;les de langage au th&#232;me trait&#233; dans le
document. Nous consid&#233;rons, en effet, que le vocabulaire utilis&#233; dans un texte est d&#233;pendant du
th&#232;me trait&#233; dans ce dernier. Dans le cadre de l&#8217;adaptation, nous choisirons donc d&#8217;exploiter un
mod&#232;le de langage repr&#233;sentatif du th&#232;me trait&#233; dans le texte. Par cons&#233;quent, l&#8217;&#233;tape cruciale
de cette adaptation est la recherche du th&#232;me trait&#233; dans le document.
</p>
<p>Dans cet article, nous allons tout d&#8217;abord pr&#233;senter le domaine de la d&#233;tection de th&#232;me, et
notamment les deux grands param&#232;tres influen&#231;ant la d&#233;tection de th&#232;me : le vocabulaire et la
m&#233;thode de d&#233;tection de th&#232;me. Apr&#232;s avoir introduit les donn&#233;es sur lesquelles nous travaillons,
nous allons exposer les performances en d&#233;tection de th&#232;me obtenues par les m&#233;thodes de l&#8217;&#233;tat
de l&#8217;art. Ensuite, nous pr&#233;senterons une nouvelle m&#233;thode de s&#233;lection de vocabulaire et nous
&#233;tudierons les performances obtenues avec les nouveaux vocabulaires. Enfin, nous concluerons
et nous pr&#233;senterons quelques perspectives &#224; nos travaux.
</p>
<p>2 La d&#233;tection de th&#232;me
</p>
<p>Soient un document donn&#233; et un ensemble pr&#233;d&#233;fini de th&#232;mes, la d&#233;tection de th&#232;me a pour
but de rechercher le(s) th&#232;me(s) trait&#233;(s) dans ce document. Cette derni&#232;re est fonction de deux
param&#232;tres principaux qui sont le vocabulaire utilis&#233; et la m&#233;thode de d&#233;tection de th&#232;me. Le
vocabulaire d&#233;finit l&#8217;ensemble des &#233;l&#233;ments caract&#233;ristiques d&#8217;un th&#232;me. Classiquement, c&#8217;est
sur la base de l&#8217;ensemble des mots le constituant que la plupart des m&#233;thodes fondent leurs
principes de d&#233;tection.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Nouvelle approche de la s&#233;lection de vocabulaire pour la d&#233;tection de th&#232;me
</p>
<p>2.1 Vocabulaire
</p>
<p>Dans un texte, toutes les informations pr&#233;sentes ne sont pas utiles pour la d&#233;tection de th&#232;me.
Par exemple, il est assez intuitif que dans un document contenant la phrase &#171; le gardien de
but n&#8217;a pas r&#233;ussi &#224; arr&#234;ter le tir &#187;, les deux occurrences du mot &#171; le &#187; n&#8217;apportent aucune
information quant au th&#232;me trait&#233; dans un texte. A l&#8217;oppos&#233;, l&#8217;expression &#171; gardien de but &#187; est
tr&#232;s importante et sugg&#232;re que le texte traite de sport.
</p>
<p>Dans le cadre de la d&#233;tection de th&#232;me, un vocabulaire recense l&#8217;ensemble des caract&#233;ristiques
des th&#232;mes utiles pour cette t&#226;che. Dans le domaine de la recherche d&#8217;informations, (Lewis,
1992) a montr&#233; que l&#8217;utilisation du mot comme unit&#233; de repr&#233;sentation d&#8217;un document semble
&#234;tre adapt&#233;e pour des t&#226;ches de classification. Pour cette raison, la majorit&#233; des m&#233;thodes de
d&#233;tection de th&#232;me utilise le mot comme unit&#233; de repr&#233;sentation du document (repr&#233;sentation
bag of words). Par cons&#233;quent, les vocabulaires utilis&#233;s seront eux aussi compos&#233;s de mots,
ceux les plus utiles pour la d&#233;tection de th&#232;me. Dans nos travaux, nous consid&#233;rons les mots
sous forme fl&#233;chie, des travaux pr&#233;c&#233;dents (Frakes &amp; Baeza-Yates, 1992) n&#8217;ayant montr&#233; aucun
gain dans l&#8217;emploi de lemmes.
</p>
<p>La question qui se pose alors est de savoir quels sont ces mots. Il existe dans la litt&#233;rature plu-
sieurs m&#233;thodes permettant de trouver ces ensembles de mots (qui composeront le vocabulaire).
Nous pr&#233;sentons ici quatre m&#233;thodes de s&#233;lection de vocabulaire, parmi les plus &#233;tudi&#233;es.
</p>
<p>2.1.1 Fr&#233;quence des mots
</p>
<p>Dans le cas d&#8217;un vocabulaire s&#233;lectionn&#233; par fr&#233;quence de mots, on calcule, pour chacun des
mots du corpus d&#8217;apprentissage, sa fr&#233;quence d&#8217;apparition. Le vocabulaire sera ensuite compos&#233;
des mots de fr&#233;quence &#233;lev&#233;e. On consid&#232;re dans ce cas que plus les mots sont fr&#233;quents &#224;
l&#8217;apprentissage, plus ils sont utiles pour la d&#233;tection de th&#232;me.
</p>
<p>2.1.2 Fr&#233;quence de document des mots
</p>
<p>Dans ce cas, on ne prend pas en compte la fr&#233;quence des mots &#224; l&#8217;apprentissage, mais le nombre
de documents dans lesquels chaque mot est apparu. Le vocabulaire r&#233;sultant sera compos&#233; des
mots apparus dans le plus grand nombre de documents.
</p>
<p>2.1.3 Information mutuelle
</p>
<p>La mesure d&#8217;information mutuelle quantifie le lien existant entre un mot et un th&#232;me. Plus
pr&#233;cis&#233;ment, elle &#233;value l&#8217;influence qu&#8217;a, sur le th&#232;me d&#8217;un texte, la pr&#233;sence d&#8217;un mot dans ce
texte. Pour un mot et un th&#232;me donn&#233;s, elle est &#233;valu&#233;e de la fa&#231;on suivante (Seymore et al.,
1998) :
</p>
<p>&#9;&#11;&#10;&#13;&#12;&#15;&#14;&#17;&#16;&#19;&#18;&#21;&#20;&#23;&#22;&#25;&#24;ff&#26;flfi&#31;ffi! &quot;&#10;&#13;&#12;&#15;&#14;&#25;#$&#18;%&#20;&amp;&#22;
</p>
<p>&#1;
</p>
<p>&#26;flfi'ffi( )&#10;*&#12;&#15;&#14;+&#22; (1)
</p>
<p>Classiquement, pour un mot donn&#233;, on calcule sa valeur d&#8217;information mutuelle avec chacun
des th&#232;mes. Ensuite, ces valeurs sont combin&#233;es afin d&#8217;obtenir une valeur unique pour chaque
mot. (Yang &amp; Pedersen, 1997) montre que dans ce cas la meilleure fa&#231;on de combiner consiste</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>A. Brun, K. Sma&#239;li, J.P. Haton
</p>
<p>&#224; retenir, pour chaque mot, la valeur d&#8217;information mutuelle maximale parmi l&#8217;ensemble des
th&#232;mes.
</p>
<p>2.1.4 Gain d&#8217;information
</p>
<p>Le gain d&#8217;information (&#233;galement appel&#233; information mutuelle moyenne) (Mitchell, 1996), per-
met, tout comme la mesure d&#8217;information mutuelle, de quantifier le lien existant entre un mot
et un th&#232;me mais ne prend pas seulement en compte l&#8217;influence qu&#8217;a l&#8217;apparition d&#8217;un mot sur
un th&#232;me, il consid&#232;re &#233;galement sa non apparition, etc. La mesure de gain d&#8217;information se
calcule de la fa&#231;on suivante :
</p>
<p>&#9;-,&quot;&#10;&#13;&#12;&#15;&#14;.&#16;/&#18;%&#20;&amp;&#22;(&#24; 0
</p>
<p>13254*1/687:91/6&#19;;
</p>
<p>0
</p>
<p>&lt;
</p>
<p>254
</p>
<p>&lt;-=
</p>
<p>7
</p>
<p>9
</p>
<p>&lt;-=
</p>
<p>;
</p>
<p> )&#10;*&#12;&gt;&#16;/&#18;?&#22;%&#26;flfi'ffi
</p>
<p> &quot;&#10;&#13;&#12;@&#16;/&#18;?&#22;
</p>
<p> &quot;&#10;&#13;&#12;A&#22;&#19; )&#10;&#13;&#18;?&#22; (2)
</p>
<p>Comme dans le cas de l&#8217;information mutuelle, pour un mot donn&#233;, on a une valeur par th&#232;me
trait&#233;. Dans ce cas, (Yang &amp; Pedersen, 1997) montre qu&#8217;il faut utiliser la moyenne pond&#233;r&#233;e des
valeurs de gain d&#8217;information entre le mot et chaque th&#232;me.
</p>
<p>Pour l&#8217;ensemble des quatre mesures pr&#233;sent&#233;es ici, la qualit&#233; de chacun des mots dans le langage
est calcul&#233;e. Celle-ci ne tient pas compte des caract&#233;ristiques des mots dans les th&#232;mes, elle est
&#233;valu&#233;e tous th&#232;mes confondus. Le vocabulaire r&#233;sultant sera compos&#233; de l&#8217;ensemble des mots
ayant, selon la mesure choisie, les valeurs les plus &#233;lev&#233;es.
</p>
<p>2.2 M&#233;thodes de d&#233;tection de th&#232;me
</p>
<p>Le second param&#232;tre dans cette approche est la m&#233;thode de d&#233;tection de th&#232;me, qui d&#233;finit la
fa&#231;on dont les informations (mots) pr&#233;sentes dans les textes (suivant le vocabulaire utilis&#233;) sont
exploit&#233;es.
</p>
<p>Nous avons d&#233;cid&#233; d&#8217;&#233;tudier un ensemble de m&#233;thodes de l&#8217;&#233;tat de l&#8217;art parmi les plus anciennes
(TFIDF), les plus performantes (cache et unigramme) ainsi que les plus r&#233;centes (SVM). Nous
avons &#233;galement &#233;tudi&#233; une m&#233;thode issue du domaine de la RAP (perplexit&#233;). Toutes ces m&#233;-
thodes ont &#233;t&#233; largement pr&#233;sent&#233;es dans la litt&#233;rature, pour cette raison nous ne nous attarderons
pas sur leur pr&#233;sentation.
</p>
<p>Sachant un vocabulaire donn&#233;, chacun des th&#232;mes est tout d&#8217;abord sch&#233;matis&#233; sous la forme
d&#8217;un vecteur o&#249; chaque &#233;l&#233;ment repr&#233;sente la fr&#233;quence d&#8217;un mot du vocabulaire dans le corpus
d&#8217;apprentissage du th&#232;me. De la m&#234;me fa&#231;on, le document de test (celui dont on recherche le
th&#232;me) est repr&#233;sent&#233; sous forme de vecteur. L&#8217;ensemble des m&#233;thodes pr&#233;sent&#233;es exploite ces
repr&#233;sentations vectorielles.
</p>
<p>2.2.1 Le classifieur TFIDF
</p>
<p>Le classifieur TFIDF (Salton, 1991) est la r&#233;f&#233;rence dans le domaine, celui-ci &#233;tant un des
premiers mod&#232;les &#224; avoir &#233;t&#233; d&#233;velopp&#233;. Dans le cas du classifieur TFIDF, chacun des &#233;l&#233;ments
des vecteurs est pond&#233;r&#233; par un facteur refl&#233;tant la proportion de th&#232;mes dans lequel le mot est
pr&#233;sent. Ensuite, une distance cosine (3) est calcul&#233;e entre le vecteur repr&#233;sentant le document</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Nouvelle approche de la s&#233;lection de vocabulaire pour la d&#233;tection de th&#232;me
</p>
<p>et celui de chacun des th&#232;mes. Le th&#232;me correspondant &#224; la distance la plus faible sera celui
affect&#233; au document.
</p>
<p>BDCFE
</p>
<p>&#10;FGH&#20;5&#16;IGJ&#14;&#13;&#22;!&#24; K&#6;L MNL
</p>
<p>OIPRQTS
</p>
<p>&#20;
</p>
<p>O
</p>
<p>S
</p>
<p>&#14;
</p>
<p>O
</p>
<p>U
</p>
<p>K
</p>
<p>L MNL
</p>
<p>OIPRQ
</p>
<p>&#10;
</p>
<p>S
</p>
<p>&#20;
</p>
<p>O
</p>
<p>&#22;:V
</p>
<p>K
</p>
<p>L MNL
</p>
<p>OWPRQ
</p>
<p>&#10;
</p>
<p>S
</p>
<p>&#14;
</p>
<p>O
</p>
<p>&#22;:V
</p>
<p>(3)
</p>
<p>2.2.2 Le mod&#232;le unigramme
</p>
<p>Dans le mod&#232;le unigramme (McDonough et al., 1994), une distribution de probabilit&#233;s des
mots est calcul&#233;e pour chaque th&#232;me. Ensuite, la probabilit&#233; de chaque th&#232;me est calcul&#233;e (4),
le th&#232;me correspondant &#224; la probabilit&#233; a posteriori la plus &#233;lev&#233;e sera le th&#232;me retenu.
</p>
<p> )&#10;&#13;&#18;%&#20;&gt;#YX[Z
</p>
<p>Q
</p>
<p>&#22;!&#24;
</p>
<p> )&#10;&#13;&#18;%&#20;&amp;&#22;&#19; )&#10;&#17;X
</p>
<p>Z
</p>
<p>Q
</p>
<p>#D&#18;%&#20;\&#22;
</p>
<p>K^]
</p>
<p>OIPRQ
</p>
<p> )&#10;&#13;&#18;
</p>
<p>O
</p>
<p>&#22;&#19; )&#10;&#17;X
</p>
<p>Z
</p>
<p>Q
</p>
<p>#$&#18;
</p>
<p>O
</p>
<p>&#22;
</p>
<p>(4)
</p>
<p>2.2.3 Le mod&#232;le cache
</p>
<p>Le mod&#232;le cache (Bigi et al., 2000), d&#233;rive lui aussi une distribution de probabilit&#233;s des mots
dans chacun des th&#232;mes, mais &#233;galement des mots dans le document de test (plus pr&#233;cis&#233;ment
d&#8217;une fen&#234;tre cache des mots du test). Ensuite, la distance de Kullback-Leibler sym&#233;trique est
calcul&#233;e entre la distribution des mots dans le document de test et celle des mots dans les th&#232;mes.
Le th&#232;me retenu sera celui correspondant &#224; la distance la plus faible. Pour de plus amples d&#233;tails
sur ce mod&#232;le, voir (Bigi et al., 2000).
</p>
<p>2.2.4 Les Machines &#224; Vecteur Support (SVM)
</p>
<p>Contrairement aux trois autres m&#233;thodes d&#233;j&#224; pr&#233;sent&#233;es, la m&#233;thode SVM (Vapnik, 1995) traite
le cas biclasse. Dans ce cas, elle oppose le th&#232;me en cours de traitement &#224; l&#8217;ensemble des autres
th&#232;mes. Sachant une repr&#233;sentation dans un espace donn&#233;, des documents du th&#232;me ainsi que de
l&#8217;ensemble des autres documents, on recherche l&#8217;hyperplan optimal s&#233;parant les deux ensembles
de donn&#233;es. L&#8217;originalit&#233; des SVM est qu&#8217;elles cherchent &#224; ma&#238;triser l&#8217;erreur en g&#233;n&#233;ralisation.
Pour traiter le cas o&#249; plus de deux classes (th&#232;mes) sont utilis&#233;es, une &#233;tape de recombinaison
des scores est ensuite n&#233;cessaire pour retrouver le th&#232;me d&#8217;un document donn&#233;.
</p>
<p>2.2.5 La perplexit&#233;
</p>
<p>La perplexit&#233; (Jelinek &amp; Mercer, 1980) est issue du domaine de la reconnaissance de la parole.
La mesure de perplexit&#233; permet de mesurer l&#8217;ad&#233;quation entre un mod&#232;le de langage et un
document donn&#233;. Si l&#8217;on d&#233;veloppe un mod&#232;le de langage par th&#232;me et que l&#8217;on calcule la
valeur de perplexit&#233; pour chacun des mod&#232;les de langage de th&#232;me sur le document de test,
alors le th&#232;me correspondant &#224; la perplexit&#233; minimale sera consid&#233;r&#233; comme &#233;tant celui du
th&#232;me.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>A. Brun, K. Sma&#239;li, J.P. Haton
</p>
<p>3 R&#233;sultats
3.1 Donn&#233;es
</p>
<p>Les donn&#233;es sur lesquelles nous travaillons sont extraites de 5 ann&#233;es (1987-1991) du journal Le
Monde et sont divis&#233;es en 7 th&#232;mes. Des ces derni&#232;res (environ _'`ba mots) nous avons extrait les
donn&#233;es de test, le reste formant le corpus d&#8217;apprentissage. Dans nos travaux, nous nous situons
dans le cadre de la recherche d&#8217;un seul th&#232;me dans un document. Cependant, les donn&#233;es sont
pr&#233;sent&#233;es sous la forme d&#8217;articles de journaux et nombre d&#8217;articles journalistiques traitent de
plusieurs th&#232;mes. Nous faisons l&#8217;hypoth&#232;se qu&#8217;il est fort probable qu&#8217;un paragraphe donn&#233; ne
traite que d&#8217;un th&#232;me. Par cons&#233;quent, nous consid&#233;rons que le corpus est constitu&#233; d&#8217;une suite
de paragraphes et que l&#8217;identification porte sur chaque paragraphe et non sur l&#8217;article entier.
Le corpus de test regroupe un peu plus de 800 paragraphes, tir&#233;s al&#233;atoirement des donn&#233;es,
&#233;tiquet&#233;s &#224; la main par une unique personne. Les figures 1 et 2 pr&#233;sentent les proportions, en
fonction des th&#232;mes, des donn&#233;es d&#8217;apprentissage et de test.
</p>
<p>0
</p>
<p>5
</p>
<p>10
</p>
<p>15
</p>
<p>20
</p>
<p>25
</p>
<p>Culture Economie Etranger Histoire Politique Sciences Sports
</p>
<p>FIG. 1 &#8211; R&#233;partition des donn&#233;es d&#8217;apprentis-
sage en fonction des th&#232;mes
</p>
<p>0
</p>
<p>50
</p>
<p>100
</p>
<p>150
</p>
<p>200
</p>
<p>Culture Economie Etranger Histoire Politique Sciences Sports
</p>
<p>FIG. 2 &#8211; R&#233;partition des donn&#233;es de test en
fonction des th&#232;mes
</p>
<p>3.2 Performances
</p>
<p>Les performances en d&#233;tection de th&#232;me sont &#233;valu&#233;es par rapport au taux de paragraphes dont
l&#8217;&#233;tiquette a &#233;t&#233; retrouv&#233;e par le module de d&#233;tection de th&#232;me. Pour une m&#233;thode de d&#233;tection
de th&#232;me et de s&#233;lection de vocabulaire fix&#233;es, nous &#233;valuons les performances en d&#233;tection de
th&#232;me. Un param&#232;tre suppl&#233;mentaire intervient, celui correspondant &#224; la taille du vocabulaire
choisi, celle-ci ayant une grande influence sur les performances. La figure 3 pr&#233;sente, sachant
une m&#233;thode de d&#233;tection de th&#232;me (unigramme) et une m&#233;thode de s&#233;lection de vocabulaire
(information mutuelle) fix&#233;es, l&#8217;&#233;volution des performances en fonction de la taille du voca-
bulaire. Dans ce cas, la taille optimale du vocabulaire est de &#4;Dc&#31;d mots. Si le nombre de mots
retenus est inf&#233;rieur ou sup&#233;rieur, les performances sont plus faibles.
</p>
<p>Par cons&#233;quent, pour chacune des m&#233;thodes de d&#233;tection de th&#232;me, chacune des m&#233;thodes de
s&#233;lection de vocabulaire et un ensemble de tailles de vocabulaire, nous avons &#233;valu&#233; les perfor-
mances en d&#233;tection de th&#232;me. Dans le tableau 1 nous pr&#233;sentons seulement les performances
maximales obtenues par association du meilleur vocabulaire et de chaque m&#233;thode. La m&#233;thode
de s&#233;lection de vocabulaire ainsi que la taille correspondantes sont &#233;galement pr&#233;cis&#233;es.
</p>
<p>Nous pouvons remarquer que l&#8217;ensemble des m&#233;thodes atteint des performances sup&#233;rieures &#224;
eYfhg
</p>
<p>. De plus, les vocabulaires optimaux sont compos&#233;s de plus de &#8; c'd mots. Les deux m&#233;-</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Nouvelle approche de la s&#233;lection de vocabulaire pour la d&#233;tection de th&#232;me
</p>
<p>55
</p>
<p>60
</p>
<p>65
</p>
<p>70
</p>
<p>75
</p>
<p>80
</p>
<p>85
</p>
<p>0 5000 10000 15000 20000 25000 30000 35000 40000
</p>
<p>FIG. 3 &#8211; Evolution des performances du mod&#232;le unigramme utilisant un vocabulaire s&#233;lectionn&#233;
par information mutuelle en fonction de la taille du vocabulaire retenue
</p>
<p>M&#233;thode de d&#233;tection M&#233;thode de s&#233;lection Taille de Performances
de th&#232;me de vocabulaire vocabulaire
Unigramme Fr&#233;quence de document &#8; c'd 83.1%
TFIDF Fr&#233;quence de mots &#8; c'd 74.3%
Cache Fr&#233;quence de mots &#8; f d 82.5%
Perplexit&#233; Fr&#233;quence de mots ` f d 79.0%
SVM Information mutuelle f c'd 78.3%
</p>
<p>TAB. 1 &#8211; Performances de chacune des m&#233;thodes de d&#233;tection de th&#232;me en fonction de la
meilleure m&#233;thode de s&#233;lection de vocabulaire
</p>
<p>thodes les plus performantes sont le mod&#232;le cache et le mod&#232;le unigramme, ce dernier atteignant
des performances de _&#31;&#8;%i &#4; g .
</p>
<p>4 Nouvelle m&#233;thode de s&#233;lection de vocabulaire
</p>
<p>Comme nous l&#8217;avons d&#233;j&#224; mentionn&#233;, dans les m&#233;thodes classiques de s&#233;lection de vocabulaire,
la mesure de la qualit&#233; d&#8217;un mot est d&#233;finie dans le langage, i.e. tous th&#232;mes confondus. Les
meilleurs mots sont ensuite retenus.
</p>
<p>Nous consid&#233;rons, de notre c&#244;t&#233;, que le vocabulaire ne doit pas &#234;tre d&#233;fini au niveau de la langue
en g&#233;n&#233;ral mais plut&#244;t au niveau des th&#232;mes. Nous sommes convaincus que chaque th&#232;me a un
vocabulaire qui lui est propre et que les performances en d&#233;tection de th&#232;me pourraient &#234;tre
am&#233;lior&#233;es si de tels vocabulaires pouvaient &#234;tre pris en compte. Prenons par exemple le cas des
deux mots &#171; temps &#187; et &#171; match &#187;. Le mot &#171; temps &#187; est un mot tr&#232;s courant dans l&#8217;ensemble des
th&#232;mes. Si la m&#233;thode de s&#233;lection de vocabulaire est celle par fr&#233;quence de mots, ce mot sera
s&#233;lectionn&#233; pour composer le vocabulaire. Cependant, la pr&#233;sence de celui-ci dans un document
de test ne nous permettra pas de d&#233;terminer de fa&#231;on efficace le th&#232;me du texte puisqu&#8217;il est
fr&#233;quent dans l&#8217;ensemble des th&#232;mes. A l&#8217;oppos&#233;, le mot &#171; match &#187; a une fr&#233;quence faible dans
l&#8217;ensemble des th&#232;mes sauf pour le th&#232;me Sports. Dans le cas classique, ce mot ne sera pas
conserv&#233; puisqu&#8217;il n&#8217;est pas assez fr&#233;quent dans le corpus d&#8217;apprentissage. A l&#8217;oppos&#233;, si l&#8217;on
raisonne au niveau du th&#232;me, le mot &#171; match &#187; sera retenu pour le vocabulaire du th&#232;me Sports,
puisque fr&#233;quent dans ce dernier.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>A. Brun, K. Sma&#239;li, J.P. Haton
</p>
<p>M&#233;thode de d&#233;tection M&#233;thode de s&#233;lection Taille de Performances
de th&#232;me de vocabulaire vocabulaire
TFIDF Information mutuelle j d 74.4%
Unigramme Information mutuelle &#4; j d 83.4%
SVM Gain d&#8217;information k'k d 78.7%
</p>
<p>TAB. 2 &#8211; Performances en d&#233;tection de th&#232;me de trois m&#233;thodes pour des vocabulaires de th&#232;me
de taille &#233;quivalente
</p>
<p>Dans cette optique, nous exploitons les mesures pr&#233;sent&#233;es dans la section 2.1. Celles-ci se-
ront d&#233;finies non plus au niveau du langage mais au niveau du th&#232;me. Ainsi, pour un th&#232;me
donn&#233;, nous &#233;valuons la mesure pour l&#8217;ensemble des mots de l&#8217;apprentissage du th&#232;me. Nous
conservons ensuite les mots ayant les valeurs les plus &#233;lev&#233;es. Nous obtenons dans ce cas un
vocabulaire pour chacun des th&#232;mes trait&#233;s. Nous effectuons ensuite l&#8217;union de ces derniers afin
de former le vocabulaire utilis&#233; pour la repr&#233;sentation des donn&#233;es.
</p>
<p>Dans ce cas, la m&#234;me question que dans le cas d&#8217;un vocabulaire d&#233;fini au niveau global se pose :
combien de mots doit-on conserver pour chaque vocabulaire ? Nous pr&#233;sentons maintenant un
cas sp&#233;cifique pour les vocabulaires de th&#232;mes : nous conservons un nombre identique de mots
par th&#232;me.
</p>
<p>4.1 Nombre identique de mots par th&#232;me
</p>
<p>Dans le cas qui vient d&#8217;&#234;tre pr&#233;sent&#233;, nous cr&#233;ons les vocabulaires de th&#232;me et nous retenons le
m&#234;me nombre de mots pour chaque th&#232;me. Ensuite, pour chacune des m&#233;thodes de d&#233;tection de
th&#232;me et pour chaque m&#233;thode de s&#233;lection de vocabulaire, nous avons &#233;tudi&#233; les performances
en d&#233;tection de th&#232;me en fonction du nombre de mots retenu par th&#232;me.
</p>
<p>Le tableau 2 pr&#233;sente les meilleures performances associ&#233;es &#224; trois des m&#233;thodes de d&#233;tection
de th&#232;me, ainsi que la m&#233;thode de s&#233;lection de vocabulaire associ&#233;e et la taille du vocabulaire
correspondante.
</p>
<p>Nous pouvons remarquer que pour les trois m&#233;thodes &#233;tudi&#233;es, les performances se sont am&#233;-
lior&#233;es. Cette am&#233;lioration n&#8217;est cependant pas statistiquement significative (entre l c i &#4; g et
l
</p>
<p>c
</p>
<p>i
</p>
<p>fbg ). Un des points importants que nous pouvons noter est que les tailles de vocabulaire re-
quises par ce nouveau type de vocabulaire se sont largement r&#233;duites. Les m&#233;thodes unigramme
et SVM voient la taille de leur vocabulaire divis&#233;e par 2 et la TFIDF par 6.
</p>
<p>4.2 Combinaison
</p>
<p>Dans cette &#233;tude, nous avons pr&#233;sent&#233; un ensemble de m&#233;thodes de d&#233;tection de th&#232;me et nous
avons pu remarquer que chacune de ces m&#233;thodes obtenait des performances maximales avec
un vocabulaire qui lui &#233;tait propre. De plus, celles-ci exploitent les donn&#233;es de fa&#231;on diff&#233;rente.
Afin d&#8217;am&#233;liorer les performances, nous envisageons d&#8217;exploiter les avantages de chacune de
ces m&#233;thodes. Pour atteindre cet objectif, nous avons d&#233;cid&#233; de combiner ces diff&#233;rentes m&#233;-
thodes.
</p>
<p>Pour les combiner, nous avons &#233;tudi&#233; un ensemble de m&#233;thodes : vote majoritaire, combinaison</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Nouvelle approche de la s&#233;lection de vocabulaire pour la d&#233;tection de th&#232;me
</p>
<p>lin&#233;aire, SVM et r&#233;seau de neurones. Nous pr&#233;sentons ici la m&#233;thode qui a permis d&#8217;obtenir les
meilleurs r&#233;sultats : le r&#233;seau de neurones (perceptron multi-couches).
Sur la couche d&#8217;entr&#233;e du perceptron, nous disposons de &#8;bj valeurs (chaque m&#233;thode fournissant
un score pour l&#8217;ensemble des 7 th&#232;mes, et nous &#233;tudions 5 m&#233;thodes). La couche de sortie
comporte 7 neurones, un pour chacun des th&#232;mes possibles. Le perceptron utilis&#233; comporte une
seule couche cach&#233;e avec 15 neurones et exploite l&#8217;algorithme de r&#233;tropropagation. Afin de fixer
les poids des neurones du perceptron, nous utilisons une m&#233;thode de validation crois&#233;e : nous
avons divis&#233; le corpus des _'&#8;'j paragraphes en e sous-ensembles de tailles quasiment &#233;gales.
Ensuite, nous avons effectu&#233; e tests : nous avons optimis&#233; les param&#232;tres du perceptron sur `'m e
</p>
<p>du corpus, puis nous avons &#233;valu&#233; la qualit&#233; de ces valeurs sur &#4; m e (le reste) du corpus.
Apr&#232;s avoir effectu&#233; la combinaison, nous avons &#233;valu&#233; le gain en performances obtenu pour
les deux types de vocabulaires &#233;tudi&#233;s : tout d&#8217;abord les vocabulaires de l&#8217;&#233;tat de l&#8217;art et ensuite
les vocabulaires que nous proposons.
</p>
<p>La combinaison des 5 m&#233;thodes en utilisant les vocabulaires d&#233;finis dans l&#8217;&#233;tat de l&#8217;art permet
d&#8217;atteindre des performances en d&#233;tection de th&#232;me de _ e ink g , ce qui correspond &#224; une am&#233;lio-
ration des performances d&#8217;environ j g (les performances les plus &#233;lev&#233;es dans ce cas &#233;taient de
_'&#8;%i
</p>
<p>&#4;
</p>
<p>g
</p>
<p>, mod&#232;le unigramme). Cette am&#233;lioration est statistiquement significative.
La m&#233;thode de contruction de vocabulaire que nous avons propos&#233; ici a permis d&#8217;am&#233;liorer l&#233;g&#232;-
rement les performances (mod&#232;le unigramme : _'&#8;%i fhg ). Lorsque nous combinons les m&#233;thodes
avec ces vocabulaires, les performances atteignent o'&#8;Ti &#4; g , ce qui correspond &#224; une am&#233;lioration
de pr&#232;s &#4;&#31;&#4; ip` g des performances en d&#233;tection de th&#232;me.
</p>
<p>Nous pouvons conclure que la m&#233;thode de s&#233;lection de vocabulaire que nous proposons per-
met non seulement d&#8217;am&#233;liorer l&#233;g&#232;rement les performances en d&#233;tection de th&#232;me. De plus,
celle-ci am&#232;ne &#224; une r&#233;duction de la taille du vocabulaire n&#233;cessaire pour atteindre les r&#233;sultats
optimaux. Enfin, elle permet d&#8217;am&#233;liorer les performances en d&#233;tection de th&#232;me de fa&#231;on tr&#232;s
cons&#233;quente ( &#4;'&#4; iq` g ) lorsque les m&#233;thodes sont combin&#233;es.
Pour essayer de comprendre cette am&#233;lioration, nous nous sommes int&#233;ress&#233;s &#224; la corr&#233;lation
existant entre les scores fournis par les diff&#233;rentes m&#233;thodes, en fonction des vocabulaires uti-
lis&#233;s. Nous avons pu remarquer que lorsque les vocabulaires utilis&#233;s &#233;taient cr&#233;&#233;s &#224; l&#8217;aide de la
m&#233;thode que nous proposons, la corr&#233;lation existant entre les m&#233;thodes est consid&#233;rablement
r&#233;duite, ce qui permet un gain potentiel plus &#233;lev&#233;. Cependant, il serait int&#233;ressant d&#8217;&#233;tudier
l&#8217;apport de chacune des m&#233;thodes dans le gain en performance, ce qui permettrait ensuite l&#8217;uti-
lisation d&#8217;un sous ensemble de celles-ci.
</p>
<p>5 Conclusion et perspectives
</p>
<p>Dans cet article, nous avons pr&#233;sent&#233; un ensemble de m&#233;thodes de d&#233;tection de th&#232;me ainsi que
des m&#233;thodes de s&#233;lection de vocabulaire. Nous avons &#233;valu&#233; les performances obtenues par
ces m&#233;thodes lorsqu&#8217;elles sont appliqu&#233;es &#224; nos donn&#233;es. L&#8217;ensemble de ces m&#233;thodes atteint
des performances sup&#233;rieures &#224; eYfhg . La m&#233;thode la plus performante est le mod&#232;le unigramme
avec _'&#8;%i &#4;
</p>
<p>g
</p>
<p>.
</p>
<p>Apr&#232;s avoir montr&#233; l&#8217;importance de la taille du vocabulaire utilis&#233; pour la d&#233;tection de th&#232;me,
nous avons pr&#233;sent&#233; une nouvelle approche de la cr&#233;ation de vocabulaires. Celle-ci passe par</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>A. Brun, K. Sma&#239;li, J.P. Haton
</p>
<p>l&#8217;exploitation de vocabulaires de th&#232;mes. Ensuite, on proc&#232;de &#224; l&#8217;union de ces derniers pour
obtenir le vocabulaire utilis&#233; pour la d&#233;tection de th&#232;me. L&#8217;utilisation de ces vocabulaires a tout
d&#8217;abord montr&#233; deux avantages, elle permet non seulement d&#8217;am&#233;liorer les performances en d&#233;-
tection de th&#232;mes des m&#233;thodes &#233;tudi&#233;es, mais &#233;galement la r&#233;duction de la taille du vocabulaire
requise pour atteindre les performances maximales (facteur variant entre 2 et 6 en fonction des
m&#233;thodes).
Dans l&#8217;optique d&#8217;am&#233;liorer les performances en d&#233;tection de th&#232;me, nous avons ensuite &#233;tudi&#233;
la combinaison des m&#233;thodes pr&#233;sent&#233;es. Nous avons cherch&#233; &#224; combiner les r&#233;sultats des m&#233;-
thodes dans le cas o&#249; les vocabulaires de la litt&#233;rature sont exploit&#233;s et &#233;galement dans le cas o&#249;
les vocabulaires utilis&#233;s sont ceux que nous proposons. Le gain obtenu dans le premier cas est
important ( j g ). Celui constat&#233; dans le cas de l&#8217;utilisation des vocabulaires que nous proposons
est beaucoup plus grand puisqu&#8217;un gain de &#4;'&#4; iq` g a &#233;t&#233; obtenu.
</p>
<p>Au vu du gain obtenu par la combinaison de m&#233;thodes, nous envisageons de nous int&#233;resser
&#224; une autre fa&#231;on de cr&#233;er les vocabulaires utilis&#233;s pour la d&#233;tection de th&#232;me. Jusqu&#8217;&#224; pr&#233;-
sent, nous avons cherch&#233; les vocabulaires qui permettaient de maximiser les performances des
m&#233;thodes ind&#233;pendamment les unes des autres. Il serait peut-&#234;tre int&#233;ressant de rechercher les
vocabulaires qui permettent d&#8217;obtenir les performances maximales en combinant les m&#233;thodes,
ces vocabulaires n&#8217;obtenant peut &#234;tre pas les meilleures performances pour chacune des m&#233;-
thodes utilis&#233;es seules.
</p>
<p>R&#233;f&#233;rences
BIGI B., DE MORI R., EL-B&#200;ZE M. &amp; SPRIET T. (2000). A fuzzy decision strategy for topic identi-
fication and dynamic selection of language models. Special issue on Fuzzy Logic in Signal Processing,
Signal Processing Journal, 80(6), 1085&#8211;1097.
FRAKES W. &amp; BAEZA-YATES R. (1992). Information Retrieval : Data Structures and Algorithms.
Prentice Hall, Englewood Cliffs,NJ.
JELINEK F. &amp; MERCER R. (1980). Interpolated estimation of markov source parameters from sparse
data. In Proceedings of Workshop Pattern Recognition in Practice, p. 381&#8211;397, Amsterdam.
LEWIS D. (1992). An Evaluation of Phrasal and Clustered Representation on a Text Categorization Task.
In Proceedings of the ACM SIGIR Conference on Research and Development in Information Retrieval,
p. 37&#8211;50.
MCDONOUGH J., NG K., JEANRENAUD P., GISH H. &amp; ROHLICEK J. (1994). Approaches to Topic
Identification On The Switchboard Corpus. In IEEE Transactions on Acoustics, Speech, and Signal
Processing, p. 385&#8211;388.
MITCHELL T. (1996). Machine Learning, chapter 3. Mc Graw Hill.
SALTON G. (1991). Developments in Automatic Text Retrieval. Science, 253, 974&#8211;979.
SEYMORE K., CHEN S. &amp; ROSENFELD R. (1998). Nonlinear Interpolation of Topic Models for Lan-
guage Model Adaptation. In Proceedings of the International Conference on Spoken Language Proces-
sing, Sydney, Australia.
VAPNIK V. (1995). The Nature of Statistical Learning Theory. Spinger, New York.
YANG Y. &amp; PEDERSEN J. (1997). A comparative study on feature selection in text categorization. In
D. H. FISHER, Ed., 14th International Conference on Machine Learning, ICML-97, p. 412&#8211;420, San
Francisco, US : Morgan Kaufmann.</p>

</div></div>
</body></html>