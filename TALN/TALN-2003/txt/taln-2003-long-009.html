<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
"http://www.w3.org/TR/html4/loose.dtd">
<html><head><title>Confronter des sources de connaissances diff&#233;rentes pour obtenir une r&#233;ponse plus fiable</title>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
</head>
<body>
<div style="page-break-before:always; page-break-after:always"><div><p>TALN 2003, Batz sur mer, 11-14 juin 2003 
</p>
<p>Confronter des sources de connaissances diff&#233;rentes pour obtenir 
une r&#233;ponse plus fiable 
</p>
<p>G. de Chalendar, F. El Kateb, O. Ferret, B. Grau, M. Hurault-Plantet,  
L. Monceaux, I. Robba, A. Vilnat 
</p>
<p>LIMSI &#8211; Groupe LIR 
BP 133, 91403 Orsay 
</p>
<p>[nom]@limsi.fr  
</p>
<p>R&#233;sum&#233; &#8211; Abstract 
La fiabilit&#233; des r&#233;ponses qu&#8217;il propose, ou un moyen de l&#8217;estimer, est le meilleur atout d&#8217;un 
syst&#232;me de question-r&#233;ponse. A cette fin, nous avons choisi d&#8217;effectuer des recherches dans 
des ensembles de documents diff&#233;rents et de privil&#233;gier des r&#233;sultats qui sont trouv&#233;s dans ces 
diff&#233;rentes sources. Ainsi, le syst&#232;me QALC travaille &#224; la fois sur une collection finie 
d&#8217;articles de journaux et sur le Web. 
A question answering system will be more convincing if it can give a user elements 
concerning the reliability of it propositions. In order to address this problem, we choose to 
take the advice of several searches. First we search for answers in a reliable document 
collection, and second on the Web. When the two sources of knowledge give the system 
QALC common answers, we are confident with them and boost them at the first places. 
</p>
<p>Mots Cl&#233;s &#8211; Keywords  
Syst&#232;me de question-r&#233;ponse, recherche d&#8217;information, fiabilit&#233; des r&#233;ponses 
Question answering system, information retrieval, answer reliability 
</p>
<p>1 Introduction 
La recherche de r&#233;ponses pr&#233;cises &#224; des questions factuelles portant sur des domaines non 
restreints constitue un champ de recherche en plein essor. Actuellement, les moteurs de 
recherche retournent au mieux des extraits, comme le fait Google par exemple1, et dans ce cas, 
leur r&#244;le consiste plus &#224; justifier le document propos&#233; qu&#8217;&#224; fournir un passage comme r&#233;ponse, 
                                                 
</p>
<p>1
 http://www.google.com </p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>de Chalendar, El Kateb, Ferret, Grau, Hurault-Plantet, Monceaux, Robba et Vilnat 
</p>
<p>m&#234;me si celle-ci figure souvent dans ces extraits. L&#8217;un des challenges de ce type de recherche 
consiste &#224; donner une seule r&#233;ponse, et non une liste plus ou moins longue de propositions, et 
pour cela, le syst&#232;me doit &#234;tre suffisamment s&#251;r de ce qu&#8217;il a trouv&#233;. La d&#233;termination de la 
fiabilit&#233; d&#8217;une r&#233;ponse consiste soit &#224; prouver sa v&#233;racit&#233;, soit &#224; estimer un degr&#233; de confiance. 
Si la r&#233;ponse est produite &#224; l&#8217;issue d&#8217;un raisonnement sur une repr&#233;sentation formelle des 
connaissances, on peut alors en &#233;laborer une preuve formelle. Ainsi, le syst&#232;me LCC 
(Moldovan et al., 2002) d&#233;rive une cha&#238;ne d&#8217;inf&#233;rences &#224; partir d&#8217;une repr&#233;sentation logique 
des connaissances extraites de WordNet2. Cette approche n&#233;cessite de poss&#233;der une base 
traitant tous les sujets dont rel&#232;vent les questions, ce qui ne peut &#234;tre garanti lorsque l&#8217;on 
fonctionne en monde ouvert. La seconde possibilit&#233;, que nous avons choisie, consiste &#224; 
estimer le degr&#233; de fiabilit&#233; d&#8217;une r&#233;ponse en lui attribuant un poids qui est fonction des 
processus et des types de connaissances utilis&#233;es. Apr&#232;s avoir constat&#233; que cette m&#233;thode 
endog&#232;ne de pond&#233;ration n&#8217;&#233;tait pas suffisante, nous avons opt&#233; pour la recherche des 
r&#233;ponses dans la collection de r&#233;f&#233;rence doubl&#233;e d'une autre dans une autre source 
d&#8217;informations afin de confronter les r&#233;sultats des deux recherches. Le principe est de 
favoriser des r&#233;ponses trouv&#233;es dans les deux sources, par rapport aux r&#233;ponses, m&#234;me 
fortement pond&#233;r&#233;es, mais trouv&#233;es dans une seule collection. Un tel raisonnement s&#8217;applique 
d&#8217;autant mieux que les sources de connaissances sont de nature diff&#233;rente, ainsi notre 
deuxi&#232;me recherche s&#8217;effectue sur le Web, qui, de surcro&#238;t, par sa diversit&#233; et sa redondance 
conduit &#224; trouver de nombreuses r&#233;ponses (Magnini et al., 2002a et 2002b ; Clarke et al., 
2001 ; Brill et al., 2001). Apr&#232;s la pr&#233;sentation g&#233;n&#233;rale de notre syst&#232;me, QALC, section 2, 
nous d&#233;crivons section 3 la reformulation des questions pour interroger le Web. La section 4 
pr&#233;sente ensuite l&#8217;extraction des r&#233;ponses pour une seule source de connaissances, et la 
section 5 les strat&#233;gies pour r&#233;aliser le choix final. Les r&#233;sultats de QALC sont d&#233;crits en 
section 6 avant de rapprocher notre travail de ce qui existe dans le domaine. 
</p>
<p>2 Le syst&#232;me QALC 
Le syst&#232;me QALC (figure 1) participe aux &#233;valuations TREC depuis 4 ans et a &#233;t&#233; con&#231;u pour 
rechercher des r&#233;ponses &#224; des questions factuelles dans une grande base de documents. Le 
principe est d'extraire un maximum d'informations des questions afin de guider la recherche 
des r&#233;ponses (voir (Ferret et al. 2003) pour une description compl&#232;te). L&#8217;analyse des questions 
vise &#224; d&#233;duire des caract&#233;ristiques permettant l&#8217;extraction de la r&#233;ponse, et &#224; donner des 
indices pour reformuler la question afin de produire une requ&#234;te sur le Web. Cette analyse 
s&#8217;appuie sur les r&#233;sultats d&#8217;un analyseur robuste de l&#8217;anglais, IFSP (A&#239;t-Mokthar et Chanod, 
1997). Les requ&#234;tes construites pour la recherche dans la collection TREC sont form&#233;es &#224; 
l&#8217;aide d&#8217;op&#233;rateurs bool&#233;ens et envoy&#233;es au moteur de recherche MG3, alors que les requ&#234;tes 
Web essaient d&#8217;approcher une formulation exacte de la r&#233;ponse. En effet, nous supposons que 
la grande taille du Web permettra de trouver des documents, m&#234;me si la requ&#234;te est tr&#232;s 
pr&#233;cise. Les documents s&#233;lectionn&#233;s (1500 passages trouv&#233;s par MG ou 20 documents 
provenant du Web) sont alors examin&#233;s. Ils sont r&#233;-index&#233;s par les termes de la question et 
leurs variantes, puis r&#233;-ordonn&#233;s suivant le type des termes trouv&#233;s dans les documents. Un 
</p>
<p>                                                 
</p>
<p>2Pour les d&#233;tails sur WordNet, voir la page : http://www.cogsci.princeton.edu/~wn/ 
</p>
<p>3Moteur  de recherche Managing Gigabytes :  http://www.mds.rmit.edu.au/mg/intro/about_mg.html </p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Confronter des sources de connaissances diff&#233;rentes pour obtenir une r&#233;ponse plus fiable 
</p>
<p>sous-ensemble de documents est s&#233;lectionn&#233; parmi les documents TREC, alors que tous les 
documents Web sont gard&#233;s, et ils sont annot&#233;s par les types d'entit&#233;s nomm&#233;es reconnues. 
Apr&#232;s pond&#233;ration des phrases candidates, la r&#233;ponse est extraite par des traitements diff&#233;rents 
selon le type attendu, et re&#231;oit un score de confiance. Enfin, les r&#233;ponses venant du corpus 
TREC et celles du Web sont compar&#233;es, afin d&#8217;en choisir une. Le principe appliqu&#233; consiste &#224; 
favoriser une r&#233;ponse obtenue dans les 5 premi&#232;res propositions des deux cha&#238;nes.  
</p>
<p> 
</p>
<p>Q
u
e
s
t
i
o
n
</p>
<p>s 
</p>
<p>R
&#233;
p
o
n
s
e
s 
</p>
<p>R&#233;ponses ordonn&#233;es 
(Web) 
</p>
<p>R&#233;ponses 
ordonn&#233;es (Trec) 
</p>
<p>Analyse des 
questions 
</p>
<p>   Termes 
   Type de r&#233;ponse 
   Focus 
   Mots pivot 
   Relations syntaxiques 
   Verbe principal 
</p>
<p>Collection TREC 
</p>
<p>Web 
</p>
<p>Moteur 
de 
</p>
<p>recherche 
</p>
<p>Traitement des documents : 
   R&#233;indexation et pond&#233;ration 
    S&#233;lection 
    Marquage des EN 
Extraction de la r&#233;ponse : 
    Pond&#233;ration des phrases 
    Extraction de la r&#233;ponse 
</p>
<p>S&#233;lection 
finale 
</p>
<p> Figure 1 &#8211; Le syst&#232;me QALC 
</p>
<p>3 Reformulation des questions pour la recherche sur le Web 
Devant la grande redondance des informations pr&#233;sentes sur le Web, nous avons suppos&#233; qu'il 
&#233;tait possible de trouver des documents pertinents m&#234;me avec une requ&#234;te tr&#232;s sp&#233;cifique et 
qu'une requ&#234;te pr&#233;cise permettrait d'obtenir dans les premi&#232;res positions les documents 
susceptibles de contenir la r&#233;ponse &#224; une question. C'est pourquoi nous avons choisi de 
reformuler les questions sous une forme affirmative avec aussi peu de variations que possible 
par rapport &#224; la formulation d'origine. Par exemple, pour la question &#171; When was Wendy&#8217;s 
founded ? &#187;, nous supposons que nous trouverons un document contenant la r&#233;ponse sous la 
forme : &#171; Wendy&#8217;s was founded on&#8230;. &#187; Nous recherchons donc les cha&#238;nes exactes fournies 
par la reformulation, comme dans (Brill et al. 2001) et non pas les diff&#233;rents mots de la 
requ&#234;te &#233;ventuellement reli&#233;s par des op&#233;rateurs AND, OR ou NEAR comme dans (Magnini, 
et al. 2002a) ou (Hermjacob et al. 2002). Ainsi, nous pouvons s&#233;lectionner un nombre r&#233;duit 
de documents, 20 dans nos exp&#233;riences.  
</p>
<p>La r&#233;&#233;criture des questions utilise des sch&#233;mas de reformulation con&#231;us &#224; partir de l'&#233;tude des 
questions de TREC9 et TREC10. Nous avons d'abord caract&#233;ris&#233; les questions en fonction du 
type de la r&#233;ponse attendue et du type de la question. Rechercher un nom de personne ou bien 
un lieu ne m&#232;nera pas &#224; la m&#234;me reformulation, m&#234;me si les deux questions sont 
syntaxiquement similaires. &#8220;Who is the governor of Alaska ?&#8221; et &#8220;Where is the Devil&#8217;s 
Tower ?&#8221; n'attendent pas des r&#233;ponses formul&#233;es exactement de la m&#234;me mani&#232;re : la requ&#234;te 
&#8220;, the first governor of Alaska&#8221; fonctionne pour la premi&#232;re question car un nom propre est 
souvent appos&#233;, tandis que &#8220;The Devil&#8217;s Tower is located&#8221; est une requ&#234;te possible pour la 
seconde car nous recherchons des r&#233;ponses qui sont la forme affirmative de la question. Nous 
avons essay&#233; manuellement avec Google ces sch&#233;mas de reformulation pour trouver les types 
les plus fr&#233;quemment couronn&#233;s de succ&#232;s. Nous avons ainsi examin&#233; environ 50 questions et 
leurs r&#233;ponses. Ces tests ont montr&#233; la n&#233;cessit&#233; d&#8217;ajouter un crit&#232;re pour obtenir une </p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>de Chalendar, El Kateb, Ferret, Grau, Hurault-Plantet, Monceaux, Robba et Vilnat 
</p>
<p>r&#233;&#233;criture plus pr&#233;cise : soit un mot introduisant un modifieur dans la forme minimale de la 
question soit un mot &#224; ajouter &#224; celui de la question (souvent une pr&#233;position ou un verbe) 
pour introduire l'information recherch&#233;e dans la forme affirmative. Par exemple, le mot 
&#171; when &#187; introduisant un modifieur est conserv&#233; et la pr&#233;sence du mot &#171; year &#187; dans une 
question portant sur une date impose l'ajout de la pr&#233;position &#171; in &#187; &#224; la forme affirmative. 
Un sch&#233;ma de r&#233;&#233;criture est donc construit en fonction des caract&#233;ristiques syntaxiques de la 
question (Ferret et al. 2002) : le focus, le verbe principal, les modifieurs et les relations 
introduisant des modifieurs du verbe ou de l'objet. La r&#233;&#233;criture la plus simple est construite 
avec tous les mots de la question hormis le pronom interrogatif et l'auxiliaire, comme pour les 
questions du type &#171; WhatBe &#187;. Par exemple, la question &#171; When was Lyndon B. Johnson 
born ? &#187; se r&#233;&#233;crit en : &#171; Lyndon B. Johnson was born on &#187;, en appliquant le sch&#233;ma 
&#171; &lt;focus&gt; &lt;verbe principal&gt; born on &#187;. Google trouve alors en premi&#232;re position, la r&#233;ponse 
&#171; Lyndon B. Johnson was born on August 27, 1908 &#187;. Pour &#233;viter d'&#234;tre trop restrictifs, nous 
soumettons les requ&#234;tes avec et sans guillemets (recherche de la cha&#238;ne exacte ou seulement 
de l'ensemble des mots) et nous associons &#224; chaque type de question un ou plusieurs sch&#233;mas, 
permettant ainsi le rel&#226;chement de contraintes par rapport au sch&#233;ma primitif. Pour &#233;valuer le 
module de r&#233;&#233;criture, nous avons cherch&#233;, dans les 20 premiers documents, les patrons des 
r&#233;ponses aux 500 questions de TREC11. 372 questions pouvaient &#234;tre r&#233;solues ainsi, soit 
74,4% des questions. Parmi celles-ci, 360 permettent de trouver plus d'un document pertinent. 
</p>
<p>4 Recherche d&#8217;une r&#233;ponse dans les documents 
</p>
<p>4.1 S&#233;lection des documents 
</p>
<p>La premi&#232;re phase dans la recherche d&#8217;une r&#233;ponse consiste &#224; restreindre le nombre de 
documents dans lesquels, compte tenu de son co&#251;t, cette recherche est men&#233;e. La m&#233;thode 
utilis&#233;e est globalement similaire &#224; celle pr&#233;sent&#233;e dans (Ferret et al., 2003). Un ensemble de 
mono et de multi-termes sont extraits de la question au moyen d&#8217;un termeur &#224; base de patrons 
morpho-syntaxiques. Les documents renvoy&#233;s par le moteur de recherche sont ensuite index&#233;s 
par FASTR (Jacquemin, 2001), qui permet de reconna&#238;tre les termes de la question ainsi que 
leurs variantes morphologiques, syntaxiques ou s&#233;mantiques. Chaque terme reconnu se voit 
attribuer un poids en fonction du type de variante qu&#8217;il repr&#233;sente, permettant le calcul d&#8217;un 
score global pour chaque document. Finalement, un ensemble restreint de documents est 
s&#233;lectionn&#233; lorsque la courbe des scores pr&#233;sente un d&#233;crochement significatif ; sinon, les 100 
premiers documents sont retenus. Dans cette version de QALC, nous nous sommes attach&#233;s &#224; 
am&#233;liorer cette s&#233;lection en augmentant la robustesse de la reconnaissance des termes r&#233;alis&#233;e 
par FASTR. Celui-ci ayant davantage &#233;t&#233; con&#231;u pour reconna&#238;tre des variantes 
terminologiques complexes que comme un outil robuste de recherche d&#8217;information, il est 
assez sensible aux erreurs de l&#8217;&#233;tiqueteur morpho-syntaxique que nous utilisons, le 
TreeTagger4. De ce fait, il lui arrive de passer &#224; c&#244;t&#233; d&#8217;occurrences de termes reconnaissables 
par un simple appariement. Sur les documents s&#233;lectionn&#233;s pour les 500 questions de 
l&#8217;&#233;valuation TREC11, nous avons &#233;valu&#233; &#224; 71% le rappel de FASTR concernant les mono et 
</p>
<p>                                                 
</p>
<p>4
 http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/DecisionTreeTagger.html </p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Confronter des sources de connaissances diff&#233;rentes pour obtenir une r&#233;ponse plus fiable 
</p>
<p>les multi-termes sans variation. La r&#233;f&#233;rence &#233;tait constitu&#233;e par les r&#233;sultats d&#8217;un m&#233;canisme 
d&#8217;appariement de termes s&#8217;appuyant sur les r&#233;sultats du TreeTagger. 
</p>
<p> 
</p>
<p>QALC TREC 10 NIST-100 QALC TREC 11 
Nb documents avec une r&#233;ponse (variation en %) 2041 2479 (+ 21,5) 2313 (+ 13,3) 
Nb documents retenus (variation en %) 30992 49900 (+ 61,0) 34568 (+ 11,5) 
Rappel (%) 46,0 55,8 52,1 
Pr&#233;cision (%) 6,6 5,0 6,7 
</p>
<p>Tableau 1 : R&#233;sultats de la s&#233;lection de documents pour les questions QA-TREC10 
</p>
<p>Pour am&#233;liorer la s&#233;lection des documents, nous avons combin&#233; les r&#233;sultats de FASTR et 
ceux de notre m&#233;canisme d&#8217;appariement de termes en &#233;liminant les doublons. L&#8217;impact de ce 
changement est illustr&#233; Tableau 1 : les r&#233;sultats s&#8217;appuient sur les jugements r&#233;alis&#233;s par le 
NIST sur les r&#233;ponses des participants &#224; l&#8217;&#233;valuation TREC10. NIST-100 correspond &#224; une 
s&#233;lection qui retiendrait les 100 premiers documents renvoy&#233;s par le moteur de recherche du 
NIST. La pr&#233;cision correspond au rapport entre le nombre de documents retenus qui 
contiennent effectivement une r&#233;ponse et le nombre de documents retenus par le syst&#232;me 
consid&#233;r&#233;. Le rappel est le rapport entre le nombre de documents retenus contenant une 
r&#233;ponse et le nombre de documents trouv&#233;s par au moins un participant &#224; TREC10 et 
contenant une r&#233;ponse. La combinaison des deux algorithmes permet donc d&#8217;am&#233;liorer le 
rappel par rapport &#224; notre version pr&#233;c&#233;dente tout en maintenant la pr&#233;cision. De plus, le 
nombre significatif de nouveaux documents pertinents retenus n&#8217;entra&#238;nent qu&#8217;un 
accroissement lin&#233;aire du nombre total de documents &#224; traiter par la suite du syst&#232;me. 
</p>
<p>4.2 Pond&#233;ration des phrases 
</p>
<p>Toutes les phrases des documents s&#233;lectionn&#233;s sont analys&#233;es afin de leur attribuer un poids 
qui &#233;value &#224; la fois la possibilit&#233; que la phrase contienne la r&#233;ponse et que QALC puisse la 
localiser. Les crit&#232;res de pond&#233;ration que nous avons choisis d&#233;coulent des informations de 
base extraites de la question, &#224; savoir les mots et le type de r&#233;ponse. Notre objectif est de 
permettre au module d&#8217;extraction de la r&#233;ponse d&#8217;augmenter le poids faible de certaines 
phrases gr&#226;ce &#224; des crit&#232;res de pond&#233;ration qui leur sont sp&#233;cifiques. La pond&#233;ration se fonde 
sur la pr&#233;sence des mots suivants dans les phrases candidates : 
</p>
<p>&#8226; mots lemmatis&#233;s de la question, pond&#233;r&#233;s5, et leurs variantes, 
</p>
<p>&#8226; mots exacts de la question et leur proximit&#233; mutuelle, 
</p>
<p>&#8226; mots dont le type correspond &#224; celui de l&#8217;entit&#233; nomm&#233;e attendue dans la r&#233;ponse. 
</p>
<p>Le premier crit&#232;re (pr&#233;sence des lemmes de la question) permet d&#8217;attribuer &#224; chaque phrase un 
poids de r&#233;f&#233;rence. Des poids sont ensuite ajout&#233;s d&#232;s qu&#8217;un des autres crit&#232;res est satisfait. 
Chacun de ces poids ne d&#233;passe pas 10% du poids de r&#233;f&#233;rence. Nous obtenons une moyenne 
de 543 phrases par question. Pour 71% des questions, au moins une phrase contient la bonne 
r&#233;ponse et 84% d&#8217;entre elles sont class&#233;es dans les 30 premi&#232;res positions. 
                                                 
</p>
<p>5
  Le poids est inversement proportionnel &#224; la fr&#233;quence des mots calcul&#233;e dans un large corpus </p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>de Chalendar, El Kateb, Ferret, Grau, Hurault-Plantet, Monceaux, Robba et Vilnat 
</p>
<p>4.3 Extraction de la r&#233;ponse 
</p>
<p>Si le type de r&#233;ponse attendue est une entit&#233; nomm&#233;e alors QALC s&#233;lectionne les mots des 
phrases qui correspondent &#224; l&#8217;entit&#233; nomm&#233;e attendue. Pour &#233;valuer la r&#233;ponse, il renforce le 
poids de la phrase par des poids suppl&#233;mentaires qui d&#233;pendent : 
</p>
<p>&#8226; du degr&#233; de pr&#233;cision de l&#8217;entit&#233; nomm&#233;e (g&#233;n&#233;rique ou sp&#233;cifique), 
</p>
<p>&#8226; de la localisation de la r&#233;ponse par rapport aux mots de la question dans la phrase 
r&#233;ponse, donn&#233;e par le barycentre des mots de la question dans la phrase r&#233;ponse. 
</p>
<p>&#8226; de la redondance de la r&#233;ponse dans les dix premi&#232;res phrases. 
</p>
<p>Quand la r&#233;ponse attendue n&#8217;est pas une entit&#233; nomm&#233;e, nous utilisons des patrons 
d&#8217;extraction. Chaque phrase candidate retenue par le module de s&#233;lection des phrases est 
analys&#233;e en utilisant des patrons d&#8217;extraction associ&#233;s au type de la question d&#233;termin&#233; lors de 
l&#8217;analyse de la question. Ces patrons repr&#233;sentent un ensemble de contraintes appliqu&#233;es aux 
phrases candidates. Ces r&#232;gles sont constitu&#233;es de patrons syntaxiques, utilis&#233;s pour localiser 
les r&#233;ponses possibles dans les phrases, et de relations s&#233;mantiques pour valider les r&#233;ponses. 
Les patrons syntaxiques rep&#232;rent les mots de liaison et simulent les paraphrases possibles de 
la r&#233;ponse. Les patrons s&#233;mantiques sont &#233;tablis en utilisant, dans Wordnet, la relation 
d&#8217;hyperonymie et la d&#233;finition des mots. Les r&#233;ponses potentielles sont alors pond&#233;r&#233;es selon 
les contraintes satisfaites. L&#8217;ordre de grandeur du poids d&#233;pend de la fiabilit&#233; des contraintes 
(pour plus de d&#233;tails sur ces 2 sous-sections, voir de Chalendar et al, 2002). 
</p>
<p>5 S&#233;lection finale de la r&#233;ponse 
Pour l'&#233;valuation TREC11, les syst&#232;mes participant devaient fournir une r&#233;ponse par question 
et l&#8217;ensemble des 500 r&#233;ponses devait &#234;tre ordonn&#233; en fonction de la confiance du syst&#232;me 
dans chaque r&#233;ponse. La m&#233;trique d'&#233;valuation est la suivante : 
</p>
<p>&#8721;
=
</p>
<p>Q
</p>
<p>iQ 1 i
rangs premiers i les dans correctes r&#233;ponses de Nb1
</p>
<p>, avec Q le nombre total de questions. 
</p>
<p>Ainsi, l&#8217;&#233;valuation tient compte non seulement de l&#8217;exactitude d&#8217;une r&#233;ponse donn&#233;e mais 
aussi de la confiance que le syst&#232;me a dans ses propres r&#233;ponses. La strat&#233;gie &#233;labor&#233;e pour la 
s&#233;lection finale est fond&#233;e sur la comparaison des r&#233;sultats de QALC appliqu&#233; &#224; chacune des 
deux sources de connaissance : la collection TREC et le Web. L'utilisation du Web vise &#224; 
confirmer des r&#233;ponses trouv&#233;es dans la collection de r&#233;f&#233;rence, mais aussi &#224; augmenter le 
nombre de r&#233;ponses trouv&#233;es par le syst&#232;me. Dans ce dernier cas, notons cependant que parmi 
les r&#233;ponses obtenues sur le Web, certaines ne sont pas retrouv&#233;es dans un document de la 
collection TREC, et ne sont donc pas retenues comme r&#233;ponse finale. Les deux applications 
du syst&#232;me QALC fournissent pour chaque question un ensemble de r&#233;ponses qui sont 
ordonn&#233;es selon le score qu&#8217;elles ont re&#231;u tout au long du processus d&#8217;extraction. Le r&#244;le de la 
s&#233;lection finale est alors d&#8217;extraire la r&#233;ponse de ces deux ensembles. Le tableau 2 donne les 
ensembles de r&#233;ponses obtenus pour la question : Who defeated the Spanish armada ? </p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Confronter des sources de connaissances diff&#233;rentes pour obtenir une r&#233;ponse plus fiable 
</p>
<p>R&#233;ponses issues de TREC R&#233;ponses issues du Web Score final 
0: Queen Elizabeth  score: 1205 0: Elizabeth I score: 1299  
1: England  score: 1202 1: Elizabeth I  score: 1297  
2 : Francis Drake score: 982 2: Philip II   score: 1282  
3: Spain score: 872 3: Francis Drake   score: 1252 1852 
</p>
<p>Tableau 2 : Exemple d&#8217;ensembles de r&#233;ponses 
Pour la s&#233;lection finale, nous avons &#233;labor&#233; deux algorithmes qui explorent ces ensembles &#224; la 
recherche de r&#233;ponses communes. Pour l&#8217;instant, seules les cinq premi&#232;res r&#233;ponses sont 
examin&#233;es, ce chiffre pouvant &#234;tre sans difficult&#233; revu &#224; la hausse. Le premier algorithme 
examine chaque couple (r&#233;ponsei, r&#233;ponsej), i &#233;tant la position d&#8217;une r&#233;ponse dans l&#8217;ensemble 
provenant de la collection TREC, j, la position d&#8217;une r&#233;ponse dans l&#8217;ensemble provenant du 
Web. Quand les deux r&#233;ponses d&#8217;un couple sont &#233;gales, le meilleur des deux scores re&#231;oit un 
bonus calcul&#233; en fonction de i et j : (11 &#8211; (i + j)) * 100. La r&#233;ponse finalement retourn&#233;e est 
celle obtenant le meilleur score. L&#8217;exemple du tableau 2 montre que la r&#233;ponse au rang 2 de la 
collection TREC et celle au rang 3 du Web sont les m&#234;mes. Le bonus re&#231;u est de 600 et la 
r&#233;ponse Francis Drake est finalement retourn&#233;e car elle obtient le meilleur score : 1852. Le 
second algorithme diff&#232;re en ce sens qu&#8217;il ne remet pas en cause l&#8217;ordre des r&#233;ponses : la 
premi&#232;re r&#233;ponse de l&#8217;un des deux ensembles est  retourn&#233;e mais avec un score renforc&#233; si 
cette r&#233;ponse appartient &#224; l&#8217;autre. Dans les deux algorithmes, l&#8217;id&#233;e sous-jacente est de 
comparer les r&#233;sultats provenant de plusieurs sources de connaissance. Dans le cas du 
premier, est renforc&#233; le score des r&#233;ponses qui appartiennent aux deux ensembles, ce qui 
permet &#224; un nombre non n&#233;gligeable de bonnes r&#233;ponses d&#8217;atteindre le premier rang (voir la 
section 6). Dans le cas du second, la r&#233;ponse retourn&#233;e &#233;tait d&#233;j&#224; au premier rang, mais son 
score est &#233;ventuellement augment&#233;.  
</p>
<p>Le tableau 3 contient les r&#233;sultats des deux algorithmes appliqu&#233;s &#224; l&#8217;ensemble des 500 
questions de TREC11. M&#234;me si les r&#233;sultats du premier ne sont pas nettement meilleurs, nous 
pensons que son approche est plus int&#233;ressante et qu&#8217;elle pourrait encore &#234;tre am&#233;lior&#233;e ; la 
comparaison des r&#233;ponses pourrait &#234;tre &#233;tendue &#224; plus de cinq r&#233;ponses et plut&#244;t que de ne 
d&#233;tecter que les cas de stricte &#233;galit&#233;, elle pourrait examiner les cas d&#8217;inclusion d&#8217;une r&#233;ponse 
dans une autre. La strat&#233;gie pourrait aussi &#234;tre appliqu&#233;e &#224; plus de deux versions de QALC. 
</p>
<p> R&#233;ponses exactes Score 
Algorithme 1 165 0.587 
Algorithme 2 159 0.574 
</p>
<p>Tableau 3 : R&#233;sultats obtenus par les deux algorithmes pour les 500 questions 
</p>
<p>6 R&#233;sultats 
Dans la derni&#232;re ligne du tableau 4, nous avons r&#233;sum&#233; (en gras) les r&#233;sultats obtenus par 
QALC lors de l&#8217;&#233;valuation TREC11, o&#249; nous avons &#233;t&#233; class&#233; 9&#232;mesur 34 participants. Nous 
donnons &#224; la fois l&#8217;&#233;valuation transmise par le Nist (le Score de Confiance SC1), et 
l&#8217;&#233;valuation obtenue &#224; l&#8217;aide des patrons de r&#233;ponses fournis &#233;galement par le Nist (SC2). 
Cette derni&#232;re &#233;valuation ne tenant compte ni des r&#233;ponses non valid&#233;es par un document de 
la collection ni des inexactes, ces r&#233;sultats sont sensiblement meilleurs, mais ils nous servent </p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>de Chalendar, El Kateb, Ferret, Grau, Hurault-Plantet, Monceaux, Robba et Vilnat 
</p>
<p>de point de comparaison pour les autres tests auxquels nous avons proc&#233;d&#233;. Nous avons 
analys&#233; les r&#233;sultats obtenus par une recherche dans les seuls documents TREC, dans les seuls 
documents trouv&#233;s sur le Web (en n&#8217;appliquant donc pas la s&#233;lection finale), ou en combinant 
ces deux sources.  
</p>
<p> Bonnes r&#233;ponses Non valid&#233;es Inexactes SC1 SC2 
TREC 128 ? ? ? 0.402 
Web 122 ? ? ? 0.436 
TREC+Web 165 20 11 0.497 0.587 
</p>
<p>Tableau 4: R&#233;sultats TREC, Web et TREC+Web 
On peut noter que la recherche TREC+Web am&#233;liore de 46% les r&#233;sultats obtenus sur les 
documents TREC seuls. Cette am&#233;lioration peut &#234;tre due &#224; des r&#233;ponses suppl&#233;mentaires 
trouv&#233;es dans les documents Web, ou alors &#224; l&#8217;algorithme de classement d&#233;crit plus haut.  
</p>
<p>Nous avons d&#8217;abord examin&#233; la source des r&#233;ponses trouv&#233;es dans la cha&#238;ne TREC+Web, et 
v&#233;rifi&#233; si elles &#233;taient ou non obtenues en commun. Sur les 165 bonnes r&#233;ponses, 106 sont 
trouv&#233;es dans les deux ensembles de documents, 42 uniquement dans les documents TREC, 
17 uniquement dans les documents Web (figure 2). Ensuite, nous avons &#233;valu&#233; l&#8217;influence des 
r&#233;ponses trouv&#233;es uniquement dans les documents Web. Pour cela, nous avons enlev&#233; les 17 
questions correspondant &#224; ces bonnes r&#233;ponses et &#233;valu&#233; les r&#233;sultats obtenus pour les 483 
questions restantes (tableau 5). Le score est encore am&#233;lior&#233; de 37% gr&#226;ce au Web, m&#234;me si 
l&#8217;&#233;valuation favorise la cha&#238;ne TREC par rapport &#224; la cha&#238;ne TREC+Web, en supprimant des 
r&#233;ponses erron&#233;es de la premi&#232;re et des bonnes de la seconde. L'am&#233;lioration est donc 
principalement due au meilleur classement des r&#233;ponses. 
</p>
<p>en commun 
</p>
<p>TREC et Web 
65%
</p>
<p>TREC
25%
</p>
<p>Web
10%
</p>
<p> 
</p>
<p>Figure 2 : Origine des bonnes r&#233;ponses 
 
</p>
<p> Bonnes r&#233;ponses Score 
TREC seul 128 0.414 
</p>
<p>TREC+Web 148 0.568 
</p>
<p>Tableau 5: R&#233;sultats sur 483 r&#233;ponses  
</p>
<p>0
5
</p>
<p>10
15
20
25
30
35
40
45
50
</p>
<p>01
 
</p>
<p>-
-
</p>
<p> 
</p>
<p>50
51
</p>
<p> 
</p>
<p>-
</p>
<p>-
</p>
<p> 
</p>
<p>10
0
</p>
<p>10
1 -
</p>
<p>-
 
</p>
<p>15
0
</p>
<p>15
1 -
</p>
<p>-
 
</p>
<p>20
0
</p>
<p>20
1 -
</p>
<p>-
 
</p>
<p>25
0
</p>
<p>25
1 -
</p>
<p>-
 
</p>
<p>30
0
</p>
<p>30
1 -
</p>
<p>-
 
</p>
<p>35
0
</p>
<p>35
1 -
</p>
<p>-
 
</p>
<p>40
0
</p>
<p>40
1 -
</p>
<p>-
 
</p>
<p>45
0
</p>
<p>45
1 -
</p>
<p>-
 
</p>
<p>50
0
</p>
<p>Positions des r&#233;ponses
</p>
<p>Bo
n
</p>
<p>n
es
</p>
<p> 
r&#233;
</p>
<p>po
n
</p>
<p>se
s
</p>
<p> Figure 3: Positions des bonnes r&#233;ponses 
Nous nous sommes ensuite int&#233;ress&#233;s aux positions des bonnes r&#233;ponses pour confirmer 
l&#8217;hypoth&#232;se qu&#8217;elles ont &#233;t&#233; bien class&#233;es. La Figure 3 illustre cette &#233;tude. Il est important de 
noter que dans les 50 premi&#232;res places se trouvent 46 bonnes r&#233;ponses. On remarque aussi que 
la premi&#232;re bonne r&#233;ponse dont le score n&#8217;a pas &#233;t&#233; modifi&#233; par la confirmation dans 
l&#8217;algorithme de classement est en 143&#232;me position (elle vient de la recherche dans les seuls 
documents Web) ou en 145&#232;me (venant des documents TREC). Enfin, nous avons fait une </p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Confronter des sources de connaissances diff&#233;rentes pour obtenir une r&#233;ponse plus fiable 
</p>
<p>&#233;valuation de nos r&#233;sultats en utilisant le protocole TREC10, c&#8217;est-&#224;-dire en donnant pour 
chaque question non plus une, mais les cinq meilleures r&#233;ponses trouv&#233;es (tableau 6). On note 
ainsi l&#8217;importance de ne pas se limiter &#224; la meilleure r&#233;ponse pour faire l&#8217;interclassement. 
</p>
<p>Tableau 6 : Bonnes r&#233;ponses entre les rangs 1 et 5 
Par ailleurs, (Chu-Carroll et al., 2002) ont propos&#233; un score mesurant la comp&#233;tence des 
syst&#232;mes pour classer leurs r&#233;ponses et l'ont appliqu&#233; aux 15 meilleurs syst&#232;mes. La 
comp&#233;tence est calcul&#233;e de la fa&#231;on suivante : 
</p>
<p>Comp&#233;tence = (SC-SCm) / (SCM-SCm), avec : SC=score de confiance TREC, SCm=SC 
moyen obtenu en ne classant pas les questions, SCM= SC maximum obtenu en classant toutes 
les bonnes r&#233;ponses en t&#234;te. 
</p>
<p>Le degr&#233; de comp&#233;tence repr&#233;sente ainsi la part de gain obtenu en classant par rapport au 
classement id&#233;al. QALC obtient un score de 0.657 et se place en t&#234;te des 15 meilleurs 
syst&#232;mes. Avant la s&#233;lection finale de la r&#233;ponse, son score sur les documents TREC seuls est 
de 0.42, soit une am&#233;lioration de 57%. 
</p>
<p>7 Travaux connexes 
</p>
<p>Magnini et al. (2002a) utilisent aussi le Web pour valider la r&#233;ponse. Ils l'interrogent avec une 
requ&#234;te compos&#233;e de mots de la question et de la r&#233;ponse reli&#233;s par des op&#233;rateurs bool&#233;ens et 
de proximit&#233;. Ils ne cherchent pas &#224; obtenir une correspondance exacte de la question. La 
validit&#233; de la r&#233;ponse est &#233;valu&#233;e par rapport au nombre de documents extraits. Cette approche 
leur a permis d&#8217;am&#233;liorer la performance de leur syst&#232;me de 28%. Dans TREC11, Magnini et 
al. (2002b) appliquent la m&#234;me approche et 40 r&#233;ponses par question sont valid&#233;es par le 
Web. La pond&#233;ration des r&#233;ponses est fond&#233;e sur le coefficient de validit&#233; et la fiabilit&#233; du 
type de la r&#233;ponse attendue. Clarke et al. (2001) s&#233;lectionnent 20 passages de la collection et 
40 passages du Web. Ce dernier n'est utilis&#233; que pour augmenter le facteur de redondance des 
r&#233;ponses candidates. Cette approche leur a permis d'am&#233;liorer leurs r&#233;sultats de 25 &#224; 30%. 
</p>
<p>Concernant la r&#233;&#233;criture de la question, Brill et al. (2001) gardent les mots de la question dans 
leur ordre original et d&#233;placent les verbes dans toutes les positions possibles. Ils effectuent, 
comme dans QALC, une comparaison entre cha&#238;nes de caract&#232;res. Hermjacob et al. (2002) 
engendrent des variantes de la question en utilisant des r&#232;gles de paraphrase syntaxique et 
s&#233;mantique. Ces paraphrases sont utilis&#233;es pour former des requ&#234;tes bool&#233;ennes (3 
paraphrases par question en moyenne) afin d'interroger le Web. 
</p>
<p>8 Conclusion 
Il est difficile d'estimer la fiabilit&#233; d'une r&#233;ponse quand chaque processus produit 
successivement des r&#233;sultats approch&#233;s. Une solution consiste &#224; confronter les r&#233;sultats que 
ces m&#234;mes processus obtiennent avec une autre source de connaissances, le Web. En nous 
</p>
<p> Rang 1 &#224;-5 % Rang 1 % Rang 2 &#224; 5 % 
TREC 177 100 128 72 49 28 
Web 177 100 122 69 55 31 </p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>de Chalendar, El Kateb, Ferret, Grau, Hurault-Plantet, Monceaux, Robba et Vilnat 
</p>
<p>fondant sur les propri&#233;t&#233;s du Web, &#224; savoir le nombre important de documents et la 
redondance des informations, nous accordons une grande confiance aux r&#233;ponses communes 
trouv&#233;es dans les cinq premi&#232;res positions. Cette strat&#233;gie nous a permis d&#8217;obtenir de 
meilleurs r&#233;sultats &#224; TREC que certains syst&#232;mes qui trouvent plus de r&#233;ponses, mais &#233;valuent 
moins bien leur fiabilit&#233;. 
</p>
<p>R&#233;f&#233;rences 
A&#239;t-Mokthar S., Chanod J-P., (1997), IFSP, Incremental finite-state parsing. Proceedings of 
Applied Natural Language Processing, Washington, DC. 
</p>
<p>Brill E., Lin J., Banko M., Dumais S., Ng A., (2001), Data-Intensive Question Answering. 
TREC 10 Notebook, Gaithersburg, USA. 
</p>
<p>de Chalendar G., Dalmas T. , Elkateb-Gara F. , Ferret O., Grau B., Hurault-Plantet M., Illouz 
G., Monceaux L., Robba I., Vilnat A., (2002), The Question Answering System QALC at 
LIMSI, Experiments in Using Web and WordNet, TREC 11 Notebook, Gaithersburg, USA. 
</p>
<p>Chu-Carroll J., Prager J., Welty C., Czuba K., Ferruci D., (2002), A Multi-Strategy and multi-
source Approach to Question Answering. TREC 11 Notebook, Gaithersburg, USA. 
</p>
<p>Clarke C.L., Cormack G.V., Lynam T.R., Li C.M., McLearn G.L., (2001), Web Reinforced 
Question Answering (MultiText Experiments for Trec 2001), TREC 10 Notebook, 
Gaithersburg, USA. 
</p>
<p>Ferret O., Grau B., Hurault-Plantet M., Illouz G., Jacquemin C., Monceaux L., Robba I.,  
Vilnat A. (&#224; para&#238;tre 2003), How a NLP approach benefits question answering. Knowledge 
Organization journal, A Special Issue on Evaluation of HLT. Guest Editor: Widad Mustafa El 
Hadi. Volume 29 N&#176; 29.3-4 
</p>
<p>Ferret O., Grau B., Hurault-Plantet M., Illouz G., Monceaux L., Robba I., Vilnat A. (2002), 
Recherche de la r&#233;ponse fond&#233;e sur la reconnaissance du focus de la question, Actes de TALN 
2002, Nancy. 
</p>
<p>Hermjakob U., Echihabi A., Marcu D., (2002), Natural Language Based Reformulation 
Resource and Web Exploitation for Question Answering, TREC 11 Notebook, Gaithersburg. 
</p>
<p>Jacquemin C., (2001), Spotting and Discovering Terms through NLP, Cambridge, MA: MIT 
Press. 
</p>
<p>Magnini B., Negri M., Prevete R., Tanev H., (2002a), Is It the Right Answer? Exploiting Web 
redundancy for Answer Validation, Proceedings of the 40th ACL, pp425-432. 
Magnini B., Negri M., Prevete R., Tanev H., (2002b), Mining Knowledge from Repeated Co-
occurrences: DIOGENE at TREC-2002, TREC 11 Notebook, Gaithersburg, USA. 
</p>
<p>Moldovan D., Harabagiu S., Girju R., Morarescu P., Lacatusu F., Novischi A., Badalescu A., 
Bolohan O., (2002), LCC Tools for Question Answering, TREC 11 Notebook, Gaithersburg. </p>

</div></div>
</body></html>