TALN 2003, Batz sur mer, II-I4 juin 2003

Confronter des sources de connaissances différentes pour obtenir
une réponse plus ﬁable

G. de Chalendar, F. El Kateb, O. Ferret, B. Grau, M. Hurau1t—P1antet,
L. Monceaux, I. Robba, A. Vilnat

LIMSI — Groupe LIR
BP 133, 91403 Orsay
[nom] @1imsi.fr

Résumé — Abstract

La fiabilité des réponses qu’il propose, ou un moyen de l’estimer, est le meilleur atout d’un
systéme de question-réponse. A cette ﬁn, nous avons choisi d’effectuer des recherches dans
des ensembles de documents différents et de privilégier des résultats qui sont trouvés dans ces
différentes sources. Ainsi, le systeme QALC travaille a la fois sur une collection ﬁnie
d’articles de journaux et sur le Web.

A question answering system will be more convincing if it can give a user elements
concerning the reliability of it propositions. In order to address this problem, we choose to
take the advice of several searches. First we search for answers in a reliable document
collection, and second on the Web. When the two sources of knowledge give the system
QALC common answers, we are conﬁdent with them and boost them at the ﬁrst places.

Mots Clés — Keywords

Systéme de question-réponse, recherche d’information, fiabilité des réponses
Question answering system, information retrieval, answer reliability

1 Introduction

La recherche de réponses précises a des questions factuelles portant sur des domaines non
restreints constitue un champ de recherche en plein essor. Actuellement, les moteurs de
recherche retournent au mieux des extraits, comme le fait Google par exemplel, et dans ce cas,
leur role consiste plus a justifier le document proposé qu’a foumir un passage comme réponse,

1 http://www.goog1e.com

de Chalendar, El Kateb, Ferret, Grau, Hurault-Plantet, Monceaux, Robba et Vilnat

meme si celle-ci ﬁgure souvent dans ces extraits. L’un des challenges de ce type de recherche
consiste a donner une seule reponse, et non une liste plus ou moins longue de propositions, et
pour cela, le systeme doit etre suffisamment sﬁr de ce qu’il a trouve. La determination de la
ﬁabilite d’une reponse consiste soit a prouver sa veracite, soit a estimer un degre de confiance.
Si la reponse est produite a l’issue d’un raisonnement sur une representation formelle des
connaissances, on peut alors en elaborer une preuve formelle. Ainsi, le systeme LCC
(Moldovan et al., 2002) derive une chaine d’inferences a partir d’une representation logique
des connaissances extraites de WordNet2. Cette approche necessite de posseder une base
traitant tous les sujets dont relevent les questions, ce qui ne peut etre garanti lorsque l’on
fonctionne en monde ouvert. La seconde possibilite, que nous avons choisie, consiste a
estimer le degre de fiabilite d’une reponse en lui attribuant un poids qui est fonction des
processus et des types de connaissances utilisees. Apres avoir constate que cette methode
endogene de ponderation n’etait pas sufﬁsante, nous avons opte pour la recherche des
reponses dans la collection de reference doublee d’une autre dans une autre source
d’informations aﬁn de confronter les resultats des deux recherches. Le principe est de
favoriser des reponses trouvees dans les deux sources, par rapport aux reponses, meme
fortement ponderees, mais trouvees dans une seule collection. Un tel raisonnement s’applique
d’autant Inieux que les sources de connaissances sont de nature differente, ainsi notre
deuxieme recherche s’effectue sur le Web, qui, de surcroit, par sa diversite et sa redondance
conduit a trouver de nombreuses reponses (Magnini et al., 2002a et 2002b ; Clarke et al.,
2001 ; Brill et al., 2001). Apres la presentation generale de notre systeme, QALC, section 2,
nous decrivons section 3 la reformulation des questions pour interroger le Web. La section 4
presente ensuite l’extraction des reponses pour une seule source de connaissances, et la
section 5 les strategies pour realiser le choix final. Les resultats de QALC sont decrits en
section 6 avant de rapprocher notre travail de ce qui existe dans le domaine.

2 Le systéme QALC

Le systeme QALC (figure 1) participe aux evaluations TREC depuis 4 ans eta ete concu pour
rechercher des reponses a des questions factuelles dans une grande base de documents. Le
principe est d'extraire un maximum d'informations des questions afin de guider la recherche
des reponses (voir (Ferret et al. 2003) pour une description complete). L’analyse des questions
vise a deduire des caracteristiques permettant l’extraction de la reponse, et a donner des
indices pour reformuler la question aﬁn de produire une requete sur le Web. Cette analyse
s’appuie sur les resultats d’un analyseur robuste de l’anglais, IFSP (A'1't-Mokthar et Chanod,
1997). Les requetes construites pour la recherche dans la collection TREC sont formees a
l’aide d’operateurs booleens et envoyees au moteur de recherche MG3, alors que les requetes
Web essaient d’approcher une formulation exacte de la reponse. En effet, nous supposons que
la grande taille du Web permettra de trouver des documents, meme si la requete est tres
precise. Les documents selectionnes (1500 passages trouves par MG ou 20 documents
provenant du Web) sont alors examines. Ils sont re-indexes par les termes de la question et
leurs variantes, puis re-ordonnes suivant le type des termes trouves dans les documents. Un

2Pour les details sur WordNet, voir la page : http://www.cogsci.princeton.edu/~wn/

3Moteur de recherche Managing Gigabytes : http://www.mds.nnit.edu.au/mg/intro/about_mg.htm1

Confronter des sources de connaissances diﬁerentes pour obtenir une reponse plus fiable

sous-ensemble de documents est selectionne parmi les documents TREC, alors que tous les
documents Web sont gardes, et ils sont annotes par les types d'entites nommees reconnues.
Apres ponderation des phrases candidates, la reponse est extraite par des traitements differents
selon le type attendu, et recoit un score de conﬁance. Enﬁn, les reponses venant du corpus
TREC et celles du Web sont comparees, aﬁn d’en choisir une. Le principe applique consiste a
favoriser une reponse obtenue dans les 5 premieres propositions des deux chaines.

Collection TREC Reponses
J ordonnees (Trec)
Q R
u Analyse des Traitement des documents : J’ 6
3 Temgelslestmns Moteur Reindexation et ponderation p
, Selection , ,
:— Type de reponse — de — Marquage des EN _ Selection 9 O
1 E/imtus - I reeherehe Extraction de la réponse ' ﬁnale n
0 s pivo , . '
0 Relations symaxiques Ponderation des p/hrases S
n Verbe principal / Extraction de la reponse e
s \ , , 5
Reponses ordonnees
W b
6 (Web)

Figure 1 — Le systeme QALC

3 Reformulation des questions pour la recherche sur le Web

Devant la grande redondance des informations presentes sur le Web, nous avons suppose qu'il
etait possible de trouver des documents pertinents meme avec une requete tres speciﬁque et
qu'une requete precise permettrait d'obtenir dans les premieres positions les documents
susceptibles de contenir la reponse a une question. C'est pourquoi nous avons choisi de
reformuler les questions sous une forme afﬁrmative avec aussi peu de variations que possible
par rapport a la formulation d'origine. Par exemple, pour la question « When was Wendy’s
founded ? », nous supposons que nous trouverons un document contenant la reponse sous la
forme : « Wendy ’s was founded on.... » Nous recherchons donc les chaines exactes fournies
par la reformulation, comme dans (Brill et al. 2001) et non pas les differents mots de la
requete eventuellement relies par des operateurs AND, OR ou NEAR comme dans (Magnini,
et al. 2002a) ou (Hermjacob et al. 2002). Ainsi, nous pouvons selectionner un nombre reduit
de documents, 20 dans nos experiences.

La reecriture des questions utilise des schemas de reformulation concus a partir de l'etude des
questions de TREC9 et TREC10. Nous avons d'abord caracterise les questions en fonction du
type de la reponse attendue et du type de la question. Rechercher un nom de personne ou bien
un lieu ne menera pas a la meme reformulation, meme si les deux questions sont
syntaxiquement similaires. “Who is the governor of Alaska ?” et “Where is the Devil’s
Tower ?” n'attendent pas des reponses formulees exactement de la meme maniere : la requete
“, the first governor of Alaska” fonctionne pour la premiere question car un nom propre est
souvent appose, tandis que “The Devil ’s Tower is located’ est une requete possible pour la
seconde car nous recherchons des reponses qui sont la forme afﬁrmative de la question. Nous
avons essaye manuellement avec Google ces schemas de reformulation pour trouver les types
les plus frequemment couronnes de succes. Nous avons ainsi examine environ 50 questions et
leurs reponses. Ces tests ont montre la necessite d’ajouter un critere pour obtenir une

de Chalendar, El Kateb, Ferret, Gran, Hurault-Plantet, Monceaux, Robba et Vilnat

reecriture plus precise : soit un mot introduisant un modiﬁeur dans la forme minimale de la
question soit un mot a ajouter a celui de la question (souvent une preposition ou un verbe)
pour introduire l'information recherchee dans la forme affirmative. Par exemple, le mot
« when » introduisant un modiﬁeur est conserve et la presence du mot « year » dans une
question portant sur une date impose l'ajout de la preposition « in » a la forme afﬁrmative.

Un schema de reecriture est donc construit en fonction des caracteristiques syntaxiques de la
question (Ferret et al. 2002): le focus, le verbe principal, les modifieurs et les relations
introduisant des modifieurs du verbe ou de l'objet. La reecriture la plus simple est construite
avec tous les mots de la question hormis le pronom interrogatif et l'auxiliaire, comme pour les
questions du type « WhatBe ». Par exemple, la question « When was Lyndon B. Johnson
born ? » se reecrit en : « Lyndon B. Johnson was born on », en appliquant le schema
« <focus> <verbe principal> born on ». Google trouve alors en premiere position, la reponse
« Lyndon B. Johnson was born on August 27, I908 ». Pour eviter d'etre trop restrictifs, nous
soumettons les requetes avec et sans guillemets (recherche de la chaine exacte ou seulement
de l'ensemble des mots) et nous associons a chaque type de question un ou plusieurs schemas,
permettant ainsi le relachement de contraintes par rapport au schema primitif. Pour evaluer le
module de reecriture, nous avons cherche, dans les 20 premiers documents, les patrons des
reponses aux 500 questions de TRECll. 372 questions pouvaient etre resolues ainsi, soit
74,4% des questions. ParIr1i celles-ci, 360 permettent de trouver plus d'un document pertinent.

4 Recherche d’une reponse dans les documents

4.1 Selection des documents

La premiere phase dans la recherche d’une reponse consiste a restreindre le nombre de
documents dans lesquels, compte tenu de son coﬁt, cette recherche est menee. La methode
utilisee est globalement similaire a celle presentee dans (Ferret et al., 2003). Un ensemble de
mono et de multi-termes sont extraits de la question au moyen d’un termeur a base de patrons
morpho-syntaxiques. Les documents renvoyes par le moteur de recherche sont ensuite indexes
par FASTR (J acqueIr1in, 2001), qui permet de reconnaitre les termes de la question ainsi que
leurs variantes morphologiques, syntaxiques ou semantiques. Chaque terme reconnu se voit
attribuer un poids en fonction du type de variante qu’il represente, permettant le calcul d’un
score global pour chaque document. Finalement, un ensemble restreint de documents est
selectionne lorsque la courbe des scores presente un decrochement signiﬁcatif ; sinon, les 100
premiers documents sont retenus. Dans cette version de QALC, nous nous sommes attaches a
ameliorer cette selection en augmentant la robustesse de la reconnaissance des termes realisee
par FASTR. Celui-ci ayant davantage ete concu pour reconnaitre des variantes
terminologiques complexes que comme un outil robuste de recherche d’information, il est
assez sensible aux erreurs de l’etiqueteur morpho-syntaxique que nous utilisons, le
TreeTagger4. De ce fait, il lui arrive de passer a cote d’occurrences de termes reconnaissables
par un simple appariement. Sur les documents selectionnes pour les 500 questions de
l’evaluation TRECll, nous avons evalue a 71% 1e rappel de FASTR concemant les mono et

4 http://www.i1ns.uni-stuttgartde/proj ekte/corplex/TreeTagger/DecisionTreeTagger.ht1nl

Confronter des sources de connaissances diﬁ‘e’rentes pour obtenir une réponse plus fiable

les multi-termes sans variation. La reference etait constituee par les resultats d’un mecanisme
d’appariement de termes s’appuyant sur les resultats du TreeTagger.

QALC TREC 10 NIST-100 QALC TREC 11
Nb documents avec une reponse (Variation en %) 2041 2479 (+ 21,5) 2313 (+ 13,3)
Nb documents retenus (Variation en %) 30992 49900 (+ 61,0) 34568 (+ 11,5)
Rappel (%) 46,0 55,8 52,1
Precision (%) 6,6 5,0 6,7

Tableau 1 : Resultats de la selection de documents pour les questions QA-TREC10

Pour ameliorer la selection des documents, nous avons combine les resultats de FASTR et
ceux de notre mecanisme d’appariement de termes en eliminant les doublons. L’impact de ce
changement est illustre Tableau 1 : les resultats s’appuient sur les jugements realises par le
NIST sur les reponses des participants a l’evaluation TREC10. NIST-100 correspond a une
selection qui retiendrait les 100 premiers documents renvoyes par le moteur de recherche du
NIST. La precision correspond au rapport entre le nombre de documents retenus qui
contiennent effectivement une reponse et le nombre de documents retenus par le systeme
considere. Le rappel est le rapport entre le nombre de documents retenus contenant une
reponse et le nombre de documents trouves par au moins un participant a TREC10 et
contenant une reponse. La combinaison des deux algorithmes permet donc d’ameliorer le
rappel par rapport a notre version precedente tout en maintenant la precision. De plus, le
nombre signiﬁcatif de nouveaux documents pertinents retenus n’entrainent qu’un
accroissement lineaire du nombre total de documents a traiter par la suite du systeme.

4.2 Pondération des phrases

Toutes les phrases des documents selectionnes sont analysees afin de leur attribuer un poids
qui evalue a la fois la possibilite que la phrase contienne la reponse et que QALC puisse la
localiser. Les criteres de ponderation que nous avons choisis decoulent des informations de
base extraites de la question, a savoir les mots et le type de reponse. Notre objectif est de
permettre au module d’eXtraction de la reponse d’augmenter le poids faible de certaines
phrases grace a des criteres de ponderation qui leur sont speciﬁques. La ponderation se fonde
sur la presence des mots suivants dans les phrases candidates :

0 mots lemmatises de la question, ponderess, et leurs variantes,
0 mots exacts de la question et leur proximite mutuelle,
0 mots dont le type correspond a celui de l’entite nommee attendue dans la reponse.

Le premier critere (presence des lemmes de la question) permet d’attribuer a chaque phrase un
poids de reference. Des poids sont ensuite ajoutes des qu’un des autres criteres est satisfait.
Chacun de ces poids ne depasse pas 10% du poids de reference. Nous obtenons une moyenne
de 543 phrases par question. Pour 71% des questions, au moins une phrase contient la bonne
reponse et 84% d’entre elles sont classees dans les 30 premieres positions.

5 Le poids est inversement proportionnel a la frequence des mots calculee dans un large corpus

de Chalendar, El Kateb, Ferret, Grau, Hurault-Plantet, Monceaux, Robba et Vilnat

4.3 Extraction de la réponse

Si le type de reponse attendue est une entite nommee alors QALC selectionne les mots des
phrases qui correspondent a l’entite nommee attendue. Pour evaluer la reponse, il renforce le
poids de la phrase par des poids supplementaires qui dependent :

0 du degre de precision de l’entite nommee (generique ou specifique),

0 de la localisation de la reponse par rapport aux mots de la question dans la phrase
reponse, donnee par le barycentre des mots de la question dans la phrase reponse.

0 de la redondance de la reponse dans les dix premieres phrases.

Quand la reponse attendue n’est pas une entite nommee, nous utilisons des patrons
d’eXtraction. Chaque phrase candidate retenue par le module de selection des phrases est
analysee en utilisant des patrons d’extraction associes au type de la question determine lors de
l’analyse de la question. Ces patrons representent un ensemble de contraintes appliquees aux
phrases candidates. Ces regles sont constituees de patrons syntaxiques, utilises pour localiser
les reponses possibles dans les phrases, et de relations semantiques pour valider les reponses.
Les patrons syntaxiques reperent les mots de liaison et simulent les paraphrases possibles de
la reponse. Les patrons semantiques sont etablis en utilisant, dans Wordnet, la relation
d’hyperonyInie et la deﬁnition des mots. Les reponses potentielles sont alors ponderees selon
les contraintes satisfaites. L’ordre de grandeur du poids depend de la fiabilite des contraintes
(pour plus de details sur ces 2 sous-sections, voir de Chalendar et al, 2002).

5 Selection ﬁnale de la réponse

Pour l'evaluation TREC11, les systemes participant devaient fournir une reponse par question
et l’ensemble des 500 reponses devait etre ordonne en fonction de la conﬁance du systeme
dans chaque reponse. La metrique d'evaluation est la suivante :

1 9 Nb d ' t d 1 ' ' .
62 e reponses COITCC CS. ans es 1 prenners rangs , avec Q 16 nombre total de queSt1OnS'
i=1 1

Ainsi, l’evaluation tient compte non seulement de l’eXactitude d’une reponse donnee mais
aussi de la confiance que le systeme a dans ses propres reponses. La strategie elaboree pour la
selection ﬁnale est fondee sur la comparaison des resultats de QALC applique a chacune des
deux sources de connaissance: la collection TREC et le Web. L'utilisation du Web vise a
conﬁrmer des reponses trouvees dans la collection de reference, mais aussi a augmenter le
nombre de reponses trouvees par le systeme. Dans ce dernier cas, notons cependant que parmi
les reponses obtenues sur le Web, certaines ne sont pas retrouvees dans un document de la
collection TREC, et ne sont donc pas retenues comme reponse finale. Les deux applications
du systeme QALC fournissent pour chaque question un ensemble de reponses qui sont
ordonnees selon le score qu’elles ont recu tout au long du processus d’eXtraction. Le role de la
selection finale est alors d’extraire la reponse de ces deux ensembles. Le tableau 2 donne les
ensembles de reponses obtenus pour la question : Who defeated the Spanish armada ?

Confronter des sources de connaissances diﬁ‘e’rentes pour obtenir une réponse plus fiable

Réponses issues de TREC Réponses issues du Web Score ﬁnal
0: Queen Elizabeth score: 1205 0: Elizabethl score: 1299
1: England score: 1202 1: Elizabethl score: 1297
2 : Francis Drake score: 982 2: Philip II score: 1282
3: Spain score: 872 3: Francis Drake score: 1252 1852

Tableau 2 : Exemple d’ensembles de réponses

Pour la selection ﬁnale, nous avons élaboré deux algorithmes qui explorent ces ensembles a la
recherche de réponses communes. Pour l’instant, seules les cinq premieres réponses sont
examinees, ce chiffre pouvant étre sans difficulté revu a la hausse. Le premier algorithme
examine chaque couple (réponsei, réponsej), i étant la position d’une réponse dans l’ensemble
provenant de la collection TREC, j, la position d’une réponse dans l’ensemble provenant du
Web. Quand les deux réponses d’un couple sont égales, le meilleur des deux scores regoit un
bonus calculé en fonction de i et j : (11 — (i + j)) * 100. La réponse finalement retournée est
celle obtenant le meilleur score. L’exemple du tableau 2 montre que la réponse au rang 2 de la
collection TREC et celle au rang 3 du Web sont les mémes. Le bonus regu est de 600 et la
réponse Francis Drake est finalement retoumée car elle obtient le meilleur score : 1852. Le
second algorithme differe en ce sens qu’il ne remet pas en cause l’ordre des réponses : la
premiere réponse de l’un des deux ensembles est retournée mais avec un score renforcé si
cette réponse appartient a l’autre. Dans les deux algorithmes, l’idée sous-jacente est de
comparer les résultats provenant de plusieurs sources de connaissance. Dans le cas du
premier, est renforcé le score des réponses qui appartiennent aux deux ensembles, ce qui
permet a un nombre non négligeable de bonnes réponses d’atteindre le premier rang (voir la
section 6). Dans le cas du second, la réponse retoumée était déja au premier rang, mais son
score est éventuellement augmenté.

Le tableau 3 contient les résultats des deux algorithmes appliqués a l’ensemble des 500
questions de TREC11. Meme si les résultats du premier ne sont pas nettement meilleurs, nous
pensons que son approche est plus intéressante et qu’elle pourrait encore étre améliorée ; la
comparaison des réponses pourrait étre étendue a plus de cinq réponses et plutot que de ne
détecter que les cas de stricte égalité, elle pourrait examiner les cas d’inclusion d’une réponse
dans une autre. La stratégie pourrait aussi étre appliquée a plus de deux versions de QALC.

Réponses exactes Score
Algorithme 1 165 0.587
Algorithme 2 159 0.574

Tableau 3 : Résultats obtenus par les deux algorithmes pour les 500 questions

6 Résultats

Dans la derniere ligne du tableau 4, nous avons résumé (en gras) les résultats obtenus par
QALC lors de l’évaluation TREC11, o1‘1 nous avons été classé 9é""sur 34 participants. Nous
donnons a la fois l’évaluation transmise par le Nist (le Score de Confiance SC1), et
l’évaluation obtenue a l’aide des patrons de réponses fournis également par le Nist (SC2).
Cette derniere évaluation ne tenant compte ni des réponses non validées par un document de
la collection ni des inexactes, ces résultats sont sensiblement meilleurs, mais ils nous servent

de Chalendar, El Kateb, Ferret, Grau, Hurault-Plantet, Monceaux, Robba et Vilnat

de point de comparaison pour les autres tests auxquels nous avons procede. Nous avons
analyse les resultats obtenus par une recherche dans les seuls documents TREC, dans les seuls
documents trouves sur le Web (en n’appliquant donc pas la selection finale), ou en combinant
ces deux sources.

Bonnes réponses Non validées Inexactes SC1 SC2
TREC 128 ? ? ? 0.402
Web 122 ? ? ? 0.436
TREC+Web 165 20 11 0.497 0.587

Tableau 4: Resultats TREC, Web et TREC+Web

On peut noter que la recherche TREC+Web ameliore de 46% les resultats obtenus sur les
documents TREC seuls. Cette amelioration peut étre due a des reponses supplementaires
trouvees dans les documents Web, ou alors a l’algorithme de classement decrit plus haut.

Nous avons d’abord examine la source des reponses trouvees dans la chaine TREC+Web, et
verifie si elles etaient ou non obtenues en commun. Sur les 165 bonnes reponses, 106 sont
trouvees dans les deux ensembles de documents, 42 uniquement dans les documents TREC,
17 uniquement dans les documents Web (figure 2). Ensuite, nous avons evalue l’inﬂuence des
reponses trouvees uniquement dans les documents Web. Pour cela, nous avons enleve les 17
questions correspondant a ces bonnes reponses et evalue les resultats obtenus pour les 483
questions restantes (tableau 5). Le score est encore ameliore de 37% grace au Web, meme si
l’evaluation favorise la chaine TREC par rapport a la chaine TREC+Web, en supprimant des
reponses erronees de la premiere et des bonnes de la seconde. L'amelioration est donc
principalement due au meilleur classement des reponses.

 

50 —
45 —
 m 40 -
65%
§ 35
3 30 —
‘2 25
3  _
E
. . . , o 15 —
Figure 2 : Or1g1ne des bonnes reponses In 10 i
5 _
0 _
Bonnes réponses Score /99 ,\o° ,3° 09° 033° ,bo° ,go° 59° 533° 030°
\ /’ /’ /’ /’ /’ /’ /’ /’ /’
TREC seul 128 0.414 ° co‘ to‘ too‘ .9‘ .33‘ .9‘ go‘ ,9‘ go‘
TREC+Web 143 0568 Positions des réponses
Tableau 53 Résultats Sur 483 réponses Figure 3: Positions des bonnes reponses

Nous nous sommes ensuite interesses aux positions des bonnes reponses pour conﬁrmer
l’hypothese qu’elles ont ete bien classees. La Figure 3 illustre cette etude. Il est important de
noter que dans les 50 premieres places se trouvent 46 bonnes reponses. On remarque aussi que
la premiere bonne reponse dont le score n’a pas ete modiﬁe par la confirmation dans
l’algorithme de classement est en 143%” position (elle vient de la recherche dans les seuls
documents Web) ou en 145%” (venant des documents TREC). Enﬁn, nous avons fait une

Confronter des sources de connaissances diﬁ‘e’rentes pour obtenir une réponse plus fiable

evaluation de nos resultats en utilisant le protocole TREC10, c’est-a-dire en donnant pour
chaque question non plus une, mais les cinq meilleures reponses trouvees (tableau 6). On note
ainsi l’importance de ne pas se limiter a la meilleure reponse pour faire l’interclassement.

Rang 1 51-5 % Rang 1 % Rang 2 a 5 %
TREC 177 100 128 72 49 28
Web 177 100 122 69 55 31

Tableau 6 : Bonnes reponses entre les rangs 1 et 5

Par ailleurs, (Chu-Carroll et al., 2002) ont propose un score mesurant la competence des
systemes pour classer leurs reponses et l'ont applique aux 15 meilleurs systemes. La
competence est calculee de la facon suivante :

Competence = (SC-SCm) / (SCM-SCm), avec : SC=score de confiance TREC, SCm=SC
moyen obtenu en ne classant pas les questions, SCM= SC maximum obtenu en classant toutes
les bonnes reponses en tete.

Le degre de competence represente ainsi la part de gain obtenu en classant par rapport au
classement ideal. QALC obtient un score de 0.657 et se place en téte des 15 meilleurs
systemes. Avant la selection finale de la reponse, son score sur les documents TREC seuls est
de 0.42, soit une amelioration de 57%.

7 Travaux connexes

Magnini et al. (2002a) utilisent aussi le Web pour valider la reponse. Ils l'interrogent avec une
requéte composee de mots de la question et de la reponse relies par des operateurs booleens et
de proximite. Ils ne cherchent pas a obtenir une correspondance exacte de la question. La
validite de la reponse est evaluee par rapport au nombre de documents extraits. Cette approche
leur a permis d’ameliorer la performance de leur systeme de 28%. Dans TREC1 1, Magnini et
al. (2002b) appliquent la meme approche et 40 reponses par question sont validees par le
Web. La ponderation des reponses est fondee sur le coefficient de validite et la ﬁabilite du
type de la reponse attendue. Clarke et al. (2001) selectionnent 20 passages de la collection et
40 passages du Web. Ce dernier n'est utilise que pour augmenter le facteur de redondance des
reponses candidates. Cette approche leur a permis d’ameliorer leurs resultats de 25 a 30%.

Concernant la reecriture de la question, Brill et al. (2001) gardent les mots de la question dans
leur ordre original et deplacent les verbes dans toutes les positions possibles. Ils effectuent,
comme dans QALC, une comparaison entre chaines de caracteres. Hermjacob et al. (2002)
engendrent des variantes de la question en utilisant des regles de paraphrase syntaxique et
semantique. Ces paraphrases sont utilisees pour former des requétes booleennes (3
paraphrases par question en moyenne) aﬁn d'interroger le Web.

8 Conclusion

Il est difficile d'estimer la fiabilite d'une reponse quand chaque processus produit
successivement des resultats approches. Une solution consiste a confronter les resultats que
ces memes processus obtiennent avec une autre source de connaissances, le Web. En nous

de Chalendar, El Kateb, Ferret, Grau, Hurault-Plantet, Monceaux, Robba et Vilnat

fondant sur les propriétés du Web, a savoir le nombre important de documents et la
redondance des informations, nous accordons une grande confiance aux réponses communes
trouvées dans les cinq premieres positions. Cette stratégie nous a permis d’obtenir de
meilleurs résultats a TREC que certains systemes qui trouvent plus de réponses, mais évaluent
moins bien leur fiabilité.

Références

A'1't-Mokthar S., Chanod J -P., (1997), IFSP, Incremental ﬁnite-state parsing. Proceedings of
Applied Natural Language Processing, Washington, DC.

Brill E., Lin J., Banko M., Dumais S., Ng A., (2001), Data-Intensive Question Answering.
TREC 10 Notebook, Gaithersburg, USA.

de Chalendar G., Dalmas T. , Elkateb-Gara F. , Ferret 0., Grau B., Hurault-Plantet M., Illouz
G., Monceaux L., Robba I., Vilnat A., (2002), The Question Answering System QALC at
LIMSI, Experiments in Using Web and WordNet, TREC 11 Notebook, Gaithersburg, USA.

Chu-Carroll J ., Prager J ., Welty C., Czuba K., Ferruci D., (2002), A Multi-Strategy and multi-
source Approach to Question Answering. TREC 11 Notebook, Gaithersburg, USA.

Clarke C.L., Cormack G.V., Lynam T.R., Li C.M., McLearn G.L., (2001), Web Reinforced
Question Answering (MultiText Experiments for Trec 2001), TREC 10 Notebook,
Gaithersburg, USA.

Ferret 0., Grau B., Hurault-Plantet M., Illouz G., Jacquemin C., Monceaux L., Robba I.,
Vilnat A. (a paraitre 2003), How a NLP approach beneﬁts question answering. Knowledge
Organization journal, A Special Issue on Evaluation of HLT. Guest Editor: Widad Mustafa El
Hadi. Volume 29 N° 29.3-4

Ferret 0., Grau B., Hurault-Plantet M., Illouz G., Monceaux L., Robba I., Vilnat A. (2002),
Recherche de la réponse fondée sur la reconnaissance du focus de la question, Actes de TALN
2002, Nancy.

Hermjakob U., Echihabi A., Marcu D., (2002), Natural Language Based Reformulation
Resource and Web Exploitation for Question Answering, TREC 11 Notebook, Gaithersburg.

Jacquemin C., (2001), Spotting and Discovering Terms through NLP, Cambridge, MA: MIT
Press.

Magnini B., Negri M., Prevete R., Tanev H., (2002a), Is It the Right Answer? Exploiting Web
redundancy for Answer Validation, Proceedings of the 40"’ ACL, pp425-432.

Magnini B., Negri M., Prevete R., Tanev H., (2002b), Mining Knowledge from Repeated Co-
occurrences: DIOGENE at TREC-2002, TREC 11 Notebook, Gaithersburg, USA.

Moldovan D., Harabagiu S., Girju R., Morarescu P., Lacatusu F., Novischi A., Badalescu A.,
Bolohan 0., (2002), LCC Tools for Question Answering, TREC 11 Notebook, Gaithersburg.

