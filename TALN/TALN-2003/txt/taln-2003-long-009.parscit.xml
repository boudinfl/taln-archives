<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Aït-Mokthar</author>
<author>J-P Chanod</author>
</authors>
<title>IFSP, Incremental finite-state parsing.</title>
<date>1997</date>
<booktitle>Proceedings of Applied Natural Language Processing,</booktitle>
<location>Washington, DC.</location>
<marker>Aït-Mokthar, Chanod, 1997</marker>
<rawString>Aït-Mokthar S., Chanod J-P., (1997), IFSP, Incremental finite-state parsing. Proceedings of Applied Natural Language Processing, Washington, DC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Brill</author>
<author>J Lin</author>
<author>M Banko</author>
<author>S Dumais</author>
<author>A Ng</author>
</authors>
<title>Data-Intensive Question Answering.</title>
<date>2001</date>
<journal>TREC</journal>
<volume>10</volume>
<location>Notebook, Gaithersburg, USA.</location>
<contexts>
<context position="3640" citStr="Brill et al., 2001" startWordPosition="570" endWordPosition="573">de référence doublée d&apos;une autre dans une autre source d’informations afin de confronter les résultats des deux recherches. Le principe est de favoriser des réponses trouvées dans les deux sources, par rapport aux réponses, même fortement pondérées, mais trouvées dans une seule collection. Un tel raisonnement s’applique d’autant mieux que les sources de connaissances sont de nature différente, ainsi notre deuxième recherche s’effectue sur le Web, qui, de surcroît, par sa diversité et sa redondance conduit à trouver de nombreuses réponses (Magnini et al., 2002a et 2002b ; Clarke et al., 2001 ; Brill et al., 2001). Après la présentation générale de notre système, QALC, section 2, nous décrivons section 3 la reformulation des questions pour interroger le Web. La section 4 présente ensuite l’extraction des réponses pour une seule source de connaissances, et la section 5 les stratégies pour réaliser le choix final. Les résultats de QALC sont décrits en section 6 avant de rapprocher notre travail de ce qui existe dans le domaine. 2 Le système QALC Le système QALC (figure 1) participe aux évaluations TREC depuis 4 ans et a été conçu pour rechercher des réponses à des questions factuelles dans une grande bas</context>
<context position="7417" citStr="Brill et al. 2001" startWordPosition="1162" endWordPosition="1165"> requête très spécifique et qu&apos;une requête précise permettrait d&apos;obtenir dans les premières positions les documents susceptibles de contenir la réponse à une question. C&apos;est pourquoi nous avons choisi de reformuler les questions sous une forme affirmative avec aussi peu de variations que possible par rapport à la formulation d&apos;origine. Par exemple, pour la question « When was Wendy’s founded ? », nous supposons que nous trouverons un document contenant la réponse sous la forme : « Wendy’s was founded on.... » Nous recherchons donc les chaînes exactes fournies par la reformulation, comme dans (Brill et al. 2001) et non pas les différents mots de la requête éventuellement reliés par des opérateurs AND, OR ou NEAR comme dans (Magnini, et al. 2002a) ou (Hermjacob et al. 2002). Ainsi, nous pouvons sélectionner un nombre réduit de documents, 20 dans nos expériences. La réécriture des questions utilise des schémas de reformulation conçus à partir de l&apos;étude des questions de TREC9 et TREC10. Nous avons d&apos;abord caractérisé les questions en fonction du type de la réponse attendue et du type de la question. Rechercher un nom de personne ou bien un lieu ne mènera pas à la même reformulation, même si les deux qu</context>
<context position="26683" citStr="Brill et al. (2001)" startWordPosition="4325" endWordPosition="4328">proche leur a permis d’améliorer la performance de leur système de 28%. Dans TREC11, Magnini et al. (2002b) appliquent la même approche et 40 réponses par question sont validées par le Web. La pondération des réponses est fondée sur le coefficient de validité et la fiabilité du type de la réponse attendue. Clarke et al. (2001) sélectionnent 20 passages de la collection et 40 passages du Web. Ce dernier n&apos;est utilisé que pour augmenter le facteur de redondance des réponses candidates. Cette approche leur a permis d&apos;améliorer leurs résultats de 25 à 30%. Concernant la réécriture de la question, Brill et al. (2001) gardent les mots de la question dans leur ordre original et déplacent les verbes dans toutes les positions possibles. Ils effectuent, comme dans QALC, une comparaison entre chaînes de caractères. Hermjacob et al. (2002) engendrent des variantes de la question en utilisant des règles de paraphrase syntaxique et sémantique. Ces paraphrases sont utilisées pour former des requêtes booléennes (3 paraphrases par question en moyenne) afin d&apos;interroger le Web. 8 Conclusion Il est difficile d&apos;estimer la fiabilité d&apos;une réponse quand chaque processus produit successivement des résultats approchés. Une </context>
</contexts>
<marker>Brill, Lin, Banko, Dumais, Ng, 2001</marker>
<rawString>Brill E., Lin J., Banko M., Dumais S., Ng A., (2001), Data-Intensive Question Answering. TREC 10 Notebook, Gaithersburg, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Ferret</author>
<author>B Grau</author>
<author>M Hurault-Plantet</author>
<author>G Illouz</author>
<author>L Monceaux</author>
<author>I Robba</author>
<author>A Vilnat</author>
</authors>
<date>2002</date>
<booktitle>The Question Answering System QALC at LIMSI, Experiments in Using Web and WordNet, TREC 11</booktitle>
<location>Notebook, Gaithersburg, USA.</location>
<contexts>
<context position="9411" citStr="Ferret et al. 2002" startWordPosition="1492" endWordPosition="1495">rault-Plantet, Monceaux, Robba et Vilnat réécriture plus précise : soit un mot introduisant un modifieur dans la forme minimale de la question soit un mot à ajouter à celui de la question (souvent une préposition ou un verbe) pour introduire l&apos;information recherchée dans la forme affirmative. Par exemple, le mot « when » introduisant un modifieur est conservé et la présence du mot « year » dans une question portant sur une date impose l&apos;ajout de la préposition « in » à la forme affirmative. Un schéma de réécriture est donc construit en fonction des caractéristiques syntaxiques de la question (Ferret et al. 2002) : le focus, le verbe principal, les modifieurs et les relations introduisant des modifieurs du verbe ou de l&apos;objet. La réécriture la plus simple est construite avec tous les mots de la question hormis le pronom interrogatif et l&apos;auxiliaire, comme pour les questions du type « WhatBe ». Par exemple, la question « When was Lyndon B. Johnson born ? » se réécrit en : « Lyndon B. Johnson was born on », en appliquant le schéma « &lt;focus&gt; &lt;verbe principal&gt; born on ». Google trouve alors en première position, la réponse « Lyndon B. Johnson was born on August 27, 1908 ». Pour éviter d&apos;être trop restrict</context>
</contexts>
<marker>Ferret, Grau, Hurault-Plantet, Illouz, Monceaux, Robba, Vilnat, 2002</marker>
<rawString>de Chalendar G., Dalmas T. , Elkateb-Gara F. , Ferret O., Grau B., Hurault-Plantet M., Illouz G., Monceaux L., Robba I., Vilnat A., (2002), The Question Answering System QALC at LIMSI, Experiments in Using Web and WordNet, TREC 11 Notebook, Gaithersburg, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Chu-Carroll</author>
<author>J Prager</author>
<author>C Welty</author>
<author>K Czuba</author>
<author>D Ferruci</author>
</authors>
<title>A Multi-Strategy and multisource Approach to Question Answering.</title>
<date>2002</date>
<journal>TREC</journal>
<volume>11</volume>
<location>Notebook, Gaithersburg, USA.</location>
<contexts>
<context position="24989" citStr="Chu-Carroll et al., 2002" startWordPosition="4041" endWordPosition="4044">-0 21 00 - 2 -5 21 50 - 3 -0 31 00 3 --5 31 50 - 4 -0 41 00 - 4 -5 41 50 - - 500 Confronter des sources de connaissances différentes pour obtenir une réponse plus fiable évaluation de nos résultats en utilisant le protocole TREC10, c’est-à-dire en donnant pour chaque question non plus une, mais les cinq meilleures réponses trouvées (tableau 6). On note ainsi l’importance de ne pas se limiter à la meilleure réponse pour faire l’interclassement. Rang 1 à-5 % Rang 1 % Rang 2 à 5 % TREC 177 100 128 72 49 28 Web 177 100 122 69 55 31 Tableau 6 : Bonnes réponses entre les rangs 1 et 5 Par ailleurs, (Chu-Carroll et al., 2002) ont proposé un score mesurant la compétence des systèmes pour classer leurs réponses et l&apos;ont appliqué aux 15 meilleurs systèmes. La compétence est calculée de la façon suivante : Compétence = (SC-SCm) / (SCM-SCm), avec : SC=score de confiance TREC, SCm=SC moyen obtenu en ne classant pas les questions, SCM= SC maximum obtenu en classant toutes les bonnes réponses en tête. Le degré de compétence représente ainsi la part de gain obtenu en classant par rapport au classement idéal. QALC obtient un score de 0.657 et se place en tête des 15 meilleurs systèmes. Avant la sélection finale de la répons</context>
</contexts>
<marker>Chu-Carroll, Prager, Welty, Czuba, Ferruci, 2002</marker>
<rawString>Chu-Carroll J., Prager J., Welty C., Czuba K., Ferruci D., (2002), A Multi-Strategy and multisource Approach to Question Answering. TREC 11 Notebook, Gaithersburg, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C L Clarke</author>
<author>G V Cormack</author>
<author>T R Lynam</author>
<author>C M Li</author>
<author>G L McLearn</author>
</authors>
<title>Web Reinforced Question Answering (MultiText Experiments for Trec</title>
<date>2001</date>
<journal>TREC</journal>
<volume>10</volume>
<location>Notebook, Gaithersburg, USA.</location>
<contexts>
<context position="3618" citStr="Clarke et al., 2001" startWordPosition="565" endWordPosition="568">es dans la collection de référence doublée d&apos;une autre dans une autre source d’informations afin de confronter les résultats des deux recherches. Le principe est de favoriser des réponses trouvées dans les deux sources, par rapport aux réponses, même fortement pondérées, mais trouvées dans une seule collection. Un tel raisonnement s’applique d’autant mieux que les sources de connaissances sont de nature différente, ainsi notre deuxième recherche s’effectue sur le Web, qui, de surcroît, par sa diversité et sa redondance conduit à trouver de nombreuses réponses (Magnini et al., 2002a et 2002b ; Clarke et al., 2001 ; Brill et al., 2001). Après la présentation générale de notre système, QALC, section 2, nous décrivons section 3 la reformulation des questions pour interroger le Web. La section 4 présente ensuite l’extraction des réponses pour une seule source de connaissances, et la section 5 les stratégies pour réaliser le choix final. Les résultats de QALC sont décrits en section 6 avant de rapprocher notre travail de ce qui existe dans le domaine. 2 Le système QALC Le système QALC (figure 1) participe aux évaluations TREC depuis 4 ans et a été conçu pour rechercher des réponses à des questions factuell</context>
<context position="26392" citStr="Clarke et al. (2001)" startWordPosition="4278" endWordPosition="4281">terrogent avec une requête composée de mots de la question et de la réponse reliés par des opérateurs booléens et de proximité. Ils ne cherchent pas à obtenir une correspondance exacte de la question. La validité de la réponse est évaluée par rapport au nombre de documents extraits. Cette approche leur a permis d’améliorer la performance de leur système de 28%. Dans TREC11, Magnini et al. (2002b) appliquent la même approche et 40 réponses par question sont validées par le Web. La pondération des réponses est fondée sur le coefficient de validité et la fiabilité du type de la réponse attendue. Clarke et al. (2001) sélectionnent 20 passages de la collection et 40 passages du Web. Ce dernier n&apos;est utilisé que pour augmenter le facteur de redondance des réponses candidates. Cette approche leur a permis d&apos;améliorer leurs résultats de 25 à 30%. Concernant la réécriture de la question, Brill et al. (2001) gardent les mots de la question dans leur ordre original et déplacent les verbes dans toutes les positions possibles. Ils effectuent, comme dans QALC, une comparaison entre chaînes de caractères. Hermjacob et al. (2002) engendrent des variantes de la question en utilisant des règles de paraphrase syntaxique</context>
</contexts>
<marker>Clarke, Cormack, Lynam, Li, McLearn, 2001</marker>
<rawString>Clarke C.L., Cormack G.V., Lynam T.R., Li C.M., McLearn G.L., (2001), Web Reinforced Question Answering (MultiText Experiments for Trec 2001), TREC 10 Notebook, Gaithersburg, USA.</rawString>
</citation>
<citation valid="false">
<authors>
<author>O Ferret</author>
<author>B Grau</author>
<author>M Hurault-Plantet</author>
<author>G Illouz</author>
<author>C Jacquemin</author>
<author>L Monceaux</author>
<author>I Robba</author>
<author>A Vilnat</author>
</authors>
<title>(à paraître 2003), How a NLP approach benefits question answering. Knowledge Organization journal, A Special Issue on Evaluation of HLT. Guest Editor: Widad Mustafa El Hadi.</title>
<date>2002</date>
<journal>Volume</journal>
<volume>29</volume>
<pages>29--3</pages>
<location>Nancy.</location>
<contexts>
<context position="9411" citStr="Ferret et al. 2002" startWordPosition="1492" endWordPosition="1495">rault-Plantet, Monceaux, Robba et Vilnat réécriture plus précise : soit un mot introduisant un modifieur dans la forme minimale de la question soit un mot à ajouter à celui de la question (souvent une préposition ou un verbe) pour introduire l&apos;information recherchée dans la forme affirmative. Par exemple, le mot « when » introduisant un modifieur est conservé et la présence du mot « year » dans une question portant sur une date impose l&apos;ajout de la préposition « in » à la forme affirmative. Un schéma de réécriture est donc construit en fonction des caractéristiques syntaxiques de la question (Ferret et al. 2002) : le focus, le verbe principal, les modifieurs et les relations introduisant des modifieurs du verbe ou de l&apos;objet. La réécriture la plus simple est construite avec tous les mots de la question hormis le pronom interrogatif et l&apos;auxiliaire, comme pour les questions du type « WhatBe ». Par exemple, la question « When was Lyndon B. Johnson born ? » se réécrit en : « Lyndon B. Johnson was born on », en appliquant le schéma « &lt;focus&gt; &lt;verbe principal&gt; born on ». Google trouve alors en première position, la réponse « Lyndon B. Johnson was born on August 27, 1908 ». Pour éviter d&apos;être trop restrict</context>
</contexts>
<marker>Ferret, Grau, Hurault-Plantet, Illouz, Jacquemin, Monceaux, Robba, Vilnat, 2002</marker>
<rawString>Ferret O., Grau B., Hurault-Plantet M., Illouz G., Jacquemin C., Monceaux L., Robba I., Vilnat A. (à paraître 2003), How a NLP approach benefits question answering. Knowledge Organization journal, A Special Issue on Evaluation of HLT. Guest Editor: Widad Mustafa El Hadi. Volume 29 N° 29.3-4 Ferret O., Grau B., Hurault-Plantet M., Illouz G., Monceaux L., Robba I., Vilnat A. (2002), Recherche de la réponse fondée sur la reconnaissance du focus de la question, Actes de TALN 2002, Nancy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>U Hermjakob</author>
<author>A Echihabi</author>
<author>D Marcu</author>
</authors>
<title>Natural Language Based Reformulation Resource and Web Exploitation for Question Answering,</title>
<date>2002</date>
<journal>TREC</journal>
<volume>11</volume>
<location>Notebook, Gaithersburg.</location>
<marker>Hermjakob, Echihabi, Marcu, 2002</marker>
<rawString>Hermjakob U., Echihabi A., Marcu D., (2002), Natural Language Based Reformulation Resource and Web Exploitation for Question Answering, TREC 11 Notebook, Gaithersburg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Jacquemin</author>
</authors>
<title>Spotting and Discovering Terms through NLP,</title>
<date>2001</date>
<publisher>MIT Press.</publisher>
<location>Cambridge, MA:</location>
<contexts>
<context position="11127" citStr="Jacquemin, 2001" startWordPosition="1774" endWordPosition="1775">s-ci, 360 permettent de trouver plus d&apos;un document pertinent. 4 Recherche d’une réponse dans les documents 4.1 Sélection des documents La première phase dans la recherche d’une réponse consiste à restreindre le nombre de documents dans lesquels, compte tenu de son coût, cette recherche est menée. La méthode utilisée est globalement similaire à celle présentée dans (Ferret et al., 2003). Un ensemble de mono et de multi-termes sont extraits de la question au moyen d’un termeur à base de patrons morpho-syntaxiques. Les documents renvoyés par le moteur de recherche sont ensuite indexés par FASTR (Jacquemin, 2001), qui permet de reconnaître les termes de la question ainsi que leurs variantes morphologiques, syntaxiques ou sémantiques. Chaque terme reconnu se voit attribuer un poids en fonction du type de variante qu’il représente, permettant le calcul d’un score global pour chaque document. Finalement, un ensemble restreint de documents est sélectionné lorsque la courbe des scores présente un décrochement significatif ; sinon, les 100 premiers documents sont retenus. Dans cette version de QALC, nous nous sommes attachés à améliorer cette sélection en augmentant la robustesse de la reconnaissance des te</context>
</contexts>
<marker>Jacquemin, 2001</marker>
<rawString>Jacquemin C., (2001), Spotting and Discovering Terms through NLP, Cambridge, MA: MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Magnini</author>
<author>M Negri</author>
<author>R Prevete</author>
<author>H Tanev</author>
</authors>
<title>Is It the Right Answer? Exploiting Web redundancy for Answer Validation,</title>
<date>2002</date>
<booktitle>Proceedings of the 40th ACL,</booktitle>
<pages>425--432</pages>
<contexts>
<context position="3586" citStr="Magnini et al., 2002" startWordPosition="558" endWordPosition="561">opté pour la recherche des réponses dans la collection de référence doublée d&apos;une autre dans une autre source d’informations afin de confronter les résultats des deux recherches. Le principe est de favoriser des réponses trouvées dans les deux sources, par rapport aux réponses, même fortement pondérées, mais trouvées dans une seule collection. Un tel raisonnement s’applique d’autant mieux que les sources de connaissances sont de nature différente, ainsi notre deuxième recherche s’effectue sur le Web, qui, de surcroît, par sa diversité et sa redondance conduit à trouver de nombreuses réponses (Magnini et al., 2002a et 2002b ; Clarke et al., 2001 ; Brill et al., 2001). Après la présentation générale de notre système, QALC, section 2, nous décrivons section 3 la reformulation des questions pour interroger le Web. La section 4 présente ensuite l’extraction des réponses pour une seule source de connaissances, et la section 5 les stratégies pour réaliser le choix final. Les résultats de QALC sont décrits en section 6 avant de rapprocher notre travail de ce qui existe dans le domaine. 2 Le système QALC Le système QALC (figure 1) participe aux évaluations TREC depuis 4 ans et a été conçu pour rechercher des r</context>
<context position="7552" citStr="Magnini, et al. 2002" startWordPosition="1186" endWordPosition="1189">ontenir la réponse à une question. C&apos;est pourquoi nous avons choisi de reformuler les questions sous une forme affirmative avec aussi peu de variations que possible par rapport à la formulation d&apos;origine. Par exemple, pour la question « When was Wendy’s founded ? », nous supposons que nous trouverons un document contenant la réponse sous la forme : « Wendy’s was founded on.... » Nous recherchons donc les chaînes exactes fournies par la reformulation, comme dans (Brill et al. 2001) et non pas les différents mots de la requête éventuellement reliés par des opérateurs AND, OR ou NEAR comme dans (Magnini, et al. 2002a) ou (Hermjacob et al. 2002). Ainsi, nous pouvons sélectionner un nombre réduit de documents, 20 dans nos expériences. La réécriture des questions utilise des schémas de reformulation conçus à partir de l&apos;étude des questions de TREC9 et TREC10. Nous avons d&apos;abord caractérisé les questions en fonction du type de la réponse attendue et du type de la question. Rechercher un nom de personne ou bien un lieu ne mènera pas à la même reformulation, même si les deux questions sont syntaxiquement similaires. “Who is the governor of Alaska ?” et “Where is the Devil’s Tower ?” n&apos;attendent pas des réponse</context>
<context position="25713" citStr="Magnini et al. (2002" startWordPosition="4164" endWordPosition="4167">aux 15 meilleurs systèmes. La compétence est calculée de la façon suivante : Compétence = (SC-SCm) / (SCM-SCm), avec : SC=score de confiance TREC, SCm=SC moyen obtenu en ne classant pas les questions, SCM= SC maximum obtenu en classant toutes les bonnes réponses en tête. Le degré de compétence représente ainsi la part de gain obtenu en classant par rapport au classement idéal. QALC obtient un score de 0.657 et se place en tête des 15 meilleurs systèmes. Avant la sélection finale de la réponse, son score sur les documents TREC seuls est de 0.42, soit une amélioration de 57%. 7 Travaux connexes Magnini et al. (2002a) utilisent aussi le Web pour valider la réponse. Ils l&apos;interrogent avec une requête composée de mots de la question et de la réponse reliés par des opérateurs booléens et de proximité. Ils ne cherchent pas à obtenir une correspondance exacte de la question. La validité de la réponse est évaluée par rapport au nombre de documents extraits. Cette approche leur a permis d’améliorer la performance de leur système de 28%. Dans TREC11, Magnini et al. (2002b) appliquent la même approche et 40 réponses par question sont validées par le Web. La pondération des réponses est fondée sur le coefficient d</context>
</contexts>
<marker>Magnini, Negri, Prevete, Tanev, 2002</marker>
<rawString>Magnini B., Negri M., Prevete R., Tanev H., (2002a), Is It the Right Answer? Exploiting Web redundancy for Answer Validation, Proceedings of the 40th ACL, pp425-432.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Magnini</author>
<author>M Negri</author>
<author>R Prevete</author>
<author>H Tanev</author>
</authors>
<date>2002</date>
<booktitle>Mining Knowledge from Repeated Cooccurrences: DIOGENE at TREC-2002, TREC 11</booktitle>
<location>Notebook, Gaithersburg, USA.</location>
<contexts>
<context position="3586" citStr="Magnini et al., 2002" startWordPosition="558" endWordPosition="561">opté pour la recherche des réponses dans la collection de référence doublée d&apos;une autre dans une autre source d’informations afin de confronter les résultats des deux recherches. Le principe est de favoriser des réponses trouvées dans les deux sources, par rapport aux réponses, même fortement pondérées, mais trouvées dans une seule collection. Un tel raisonnement s’applique d’autant mieux que les sources de connaissances sont de nature différente, ainsi notre deuxième recherche s’effectue sur le Web, qui, de surcroît, par sa diversité et sa redondance conduit à trouver de nombreuses réponses (Magnini et al., 2002a et 2002b ; Clarke et al., 2001 ; Brill et al., 2001). Après la présentation générale de notre système, QALC, section 2, nous décrivons section 3 la reformulation des questions pour interroger le Web. La section 4 présente ensuite l’extraction des réponses pour une seule source de connaissances, et la section 5 les stratégies pour réaliser le choix final. Les résultats de QALC sont décrits en section 6 avant de rapprocher notre travail de ce qui existe dans le domaine. 2 Le système QALC Le système QALC (figure 1) participe aux évaluations TREC depuis 4 ans et a été conçu pour rechercher des r</context>
<context position="7552" citStr="Magnini, et al. 2002" startWordPosition="1186" endWordPosition="1189">ontenir la réponse à une question. C&apos;est pourquoi nous avons choisi de reformuler les questions sous une forme affirmative avec aussi peu de variations que possible par rapport à la formulation d&apos;origine. Par exemple, pour la question « When was Wendy’s founded ? », nous supposons que nous trouverons un document contenant la réponse sous la forme : « Wendy’s was founded on.... » Nous recherchons donc les chaînes exactes fournies par la reformulation, comme dans (Brill et al. 2001) et non pas les différents mots de la requête éventuellement reliés par des opérateurs AND, OR ou NEAR comme dans (Magnini, et al. 2002a) ou (Hermjacob et al. 2002). Ainsi, nous pouvons sélectionner un nombre réduit de documents, 20 dans nos expériences. La réécriture des questions utilise des schémas de reformulation conçus à partir de l&apos;étude des questions de TREC9 et TREC10. Nous avons d&apos;abord caractérisé les questions en fonction du type de la réponse attendue et du type de la question. Rechercher un nom de personne ou bien un lieu ne mènera pas à la même reformulation, même si les deux questions sont syntaxiquement similaires. “Who is the governor of Alaska ?” et “Where is the Devil’s Tower ?” n&apos;attendent pas des réponse</context>
<context position="25713" citStr="Magnini et al. (2002" startWordPosition="4164" endWordPosition="4167">aux 15 meilleurs systèmes. La compétence est calculée de la façon suivante : Compétence = (SC-SCm) / (SCM-SCm), avec : SC=score de confiance TREC, SCm=SC moyen obtenu en ne classant pas les questions, SCM= SC maximum obtenu en classant toutes les bonnes réponses en tête. Le degré de compétence représente ainsi la part de gain obtenu en classant par rapport au classement idéal. QALC obtient un score de 0.657 et se place en tête des 15 meilleurs systèmes. Avant la sélection finale de la réponse, son score sur les documents TREC seuls est de 0.42, soit une amélioration de 57%. 7 Travaux connexes Magnini et al. (2002a) utilisent aussi le Web pour valider la réponse. Ils l&apos;interrogent avec une requête composée de mots de la question et de la réponse reliés par des opérateurs booléens et de proximité. Ils ne cherchent pas à obtenir une correspondance exacte de la question. La validité de la réponse est évaluée par rapport au nombre de documents extraits. Cette approche leur a permis d’améliorer la performance de leur système de 28%. Dans TREC11, Magnini et al. (2002b) appliquent la même approche et 40 réponses par question sont validées par le Web. La pondération des réponses est fondée sur le coefficient d</context>
</contexts>
<marker>Magnini, Negri, Prevete, Tanev, 2002</marker>
<rawString>Magnini B., Negri M., Prevete R., Tanev H., (2002b), Mining Knowledge from Repeated Cooccurrences: DIOGENE at TREC-2002, TREC 11 Notebook, Gaithersburg, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Moldovan</author>
<author>S Harabagiu</author>
<author>R Girju</author>
<author>P Morarescu</author>
<author>F Lacatusu</author>
<author>A Novischi</author>
<author>A Badalescu</author>
<author>O Bolohan</author>
</authors>
<date>2002</date>
<journal>LCC Tools for Question Answering, TREC</journal>
<volume>11</volume>
<location>Notebook, Gaithersburg.</location>
<contexts>
<context position="2386" citStr="Moldovan et al., 2002" startWordPosition="377" endWordPosition="380">rault-Plantet, Monceaux, Robba et Vilnat même si celle-ci figure souvent dans ces extraits. L’un des challenges de ce type de recherche consiste à donner une seule réponse, et non une liste plus ou moins longue de propositions, et pour cela, le système doit être suffisamment sûr de ce qu’il a trouvé. La détermination de la fiabilité d’une réponse consiste soit à prouver sa véracité, soit à estimer un degré de confiance. Si la réponse est produite à l’issue d’un raisonnement sur une représentation formelle des connaissances, on peut alors en élaborer une preuve formelle. Ainsi, le système LCC (Moldovan et al., 2002) dérive une chaîne d’inférences à partir d’une représentation logique des connaissances extraites de WordNet2. Cette approche nécessite de posséder une base traitant tous les sujets dont relèvent les questions, ce qui ne peut être garanti lorsque l’on fonctionne en monde ouvert. La seconde possibilité, que nous avons choisie, consiste à estimer le degré de fiabilité d’une réponse en lui attribuant un poids qui est fonction des processus et des types de connaissances utilisées. Après avoir constaté que cette méthode endogène de pondération n’était pas suffisante, nous avons opté pour la recherc</context>
</contexts>
<marker>Moldovan, Harabagiu, Girju, Morarescu, Lacatusu, Novischi, Badalescu, Bolohan, 2002</marker>
<rawString>Moldovan D., Harabagiu S., Girju R., Morarescu P., Lacatusu F., Novischi A., Badalescu A., Bolohan O., (2002), LCC Tools for Question Answering, TREC 11 Notebook, Gaithersburg.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>