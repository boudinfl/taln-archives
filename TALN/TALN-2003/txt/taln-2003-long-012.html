<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
"http://www.w3.org/TR/html4/loose.dtd">
<html><head><title>Contextual Grammars and Dependency Trees</title>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
</head>
<body>
<div style="page-break-before:always; page-break-after:always"><div><p>TALN 2003, Batz-sur-Mer, 11&#8211;14 juin 2003
</p>
<p>Contextual Grammars and Dependency Trees &#0;
</p>
<p>Radu Gramatovici (1), Carlos Mart&#305;&#769;n-Vide (2)
(1) Faculty of Mathematics and Computer Science, University of Bucharest
</p>
<p>Academiei 14, 70109, Bucharest, Romania
Email: radu@funinf.cs.unibuc.ro
</p>
<p>(2) Research Group on Mathematical Linguistics, Rovira i Virgili University
Pl. Imperial Ta&#768;rraco 1, 43005 Tarragona, Spain
</p>
<p>Email: cmv@astor.urv.es
</p>
<p>Mots-clefs &#8211; Keywords
</p>
<p>Grammaire contextuelle, arbre de dependance, arbre projectif de de&#769;pendance
Contextual grammar, dependency tree, projective dependency tree
</p>
<p>Re&#769;sume&#769; - Abstract
</p>
<p>On pre&#769;sente une nouvelle variante de grammaire contextuelle structure&#769;e, qui produit des arbres
de de&#769;pendance. Le nouveau mode&#768;le ge&#769;ne&#769;ratif, appele&#769; grammaire contextuelle de de&#769;pendance,
ame&#769;liore la puissance ge&#769;ne&#769;rative forte et faible des grammaires contextuelles, tout en e&#769;tant un
candidat potentiel pour la description mathe&#769;matique des mode&#769;les syntactiques de de&#769;pendance.
</p>
<p>A new variant of structured contextual grammar, which generates dependency trees, is intro-
duced. The new generative model, called dependency contextual grammar, improves both the
strong and weak generative power of contextual grammars, while being a potential candidate
for the mathematical description of dependency-based syntactic models.
</p>
<p>&#1;
</p>
<p>This work was written during a research visit of the first author at the Research Group on Mathematical
Linguistics of Rovira i Virgili University. The research visit was funded by the NATO Scientific Committee.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>R. Gramatovici, C. Mart&#305;&#769;n-Vide
</p>
<p>1 Introduction
</p>
<p>Contextual grammars were introduced in 1969 by Solomon Marcus (Marcus, 1969) as an at-
tempt to transform in generative devices some procedures developed within the framework of
analytical models (see (Marcus, 1997) for a comprehensive discussion on the linguistic mo-
tivations of contextual grammars). In many respects, (the mathematical model of) contextual
grammars and (the linguistic model of) dependency grammars ((Mel&#8217;c&#780;uk, 1987) and others)
have the same roots. The similitude between contextual grammars and dependency syntactic
models based on dependency trees go further, since both formalisms deal mostly with symbols
of the language and less or at all with auxiliary symbols as in the case of formalisms based on
constituents trees. A similar argument to this one is presented in (Mra&#769;z et al., 2000), where
contextual mechanism is described as dual to the analysis by reduction linguistic method.
</p>
<p>However, no relationship was established yet between contextual and dependency grammars.
This happened probably because, originally, contextual grammars developed a string generative
mechanism and structures on strings generated by contextual grammars were introduced only
recently in (Mart&#305;&#769;n-Vide &amp; Pa&#774;un, 1998).
In this paper, we propose a new approach to the generation of dependency trees (D-trees),
based on contextual grammars. The notion of internal dependency contextual grammar (DCG)
is introduced, by adding dependency descriptions to contextual rules from ordinary contextual
grammars. In a DCG, the derivation relation is constrained in two ways: contexts are selected
by (strings of) words, but also by the correct construction of the D-tree. Simplifying, string
insertion rules stand for the word order, while dependencies express mainly the dominance
relationship between words, as in unordered syntactic D-trees. Local and long distance depen-
dencies are treated in an uniform way, using the smooth contextual mechanism.
</p>
<p>Several formal examples are given through out the paper, in order to emphasize the capability
of DCGs to describe different aspects of language generation. Concerning the strong generative
power, we illustrate in Section 4 the flexibility of DCGs in describing even opposite (top-down
and bottom-up) derivation styles, as well as different kinds of dependencies (nested or cross-
serial), in a very simple and pragmatic way. In Section 5, we prove that the weak generative
power of projective internal DCGs (DCGs that generates only projective D-trees) is beyond the
generative power of internal contextual grammars with choice.
</p>
<p>2 Marcus Contextual Grammars
</p>
<p>Contextual grammars are based on the interplay between (strings of) words in a sentence. They
start from the assumption that there is a finite set of (simple) sentences (axioms), which are
accepted as well-formed without any doubt. More complex sentences are generated by the
insertion of other words, in the form of contexts (pairs of strings, possibly one-sided) selected
by the strings already present in the sentence.
</p>
<p>Formally, an internal contextual grammar (CG) is a construct &#2;&#4;&#3;&#6;&#5;&#8;&#7;&#10;&#9;&#12;&#11;&#13;&#9;&#15;&#14;&#16;&#9;&#18;&#17;&#20;&#19; , where &#7; is
an alphabet,
</p>
<p>&#11;
</p>
<p>is a finite language over
&#7; (the set of axioms), &#14; is a finite subset of &#7;&#22;&#21;&#24;&#23;&#25;&#7;&#13;&#21;
</p>
<p>(the set of contexts) and &#17;ff&#26;fi&#7; &#21;ffifl &#31; &#5; &#14;&#13;&#19; is the choice (or selection) recursive map. When
&#17;!&#5;#&quot;&#20;&#19;$&#3;%&#14;
</p>
<p>, for all
&quot;'&amp;&#25;&#7;
</p>
<p>&#21;
</p>
<p>, we say that
&#2;
</p>
<p>is a CG without choice and write
&#2;(&#3;)&#5;*&#7;+&#9;&#12;&#11;,&#9;&#15;&#14;,&#19;
</p>
<p>. The</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Contextual Grammars and Dependency Trees
</p>
<p>internal derivation style (introduced in (Pa&#774;un &amp; Nguyen, 1980)) of a CG is defined by:
&quot;.-0/
</p>
<p>iff
&quot;ffi&#3;1&quot;324&quot;65&#12;&quot;67
</p>
<p>and
/8&#3;9&quot;32;:&lt;&quot;65&#12;=&gt;&quot;67
</p>
<p>for some
&#5; :?&#9;&#12;=@&#19;$&amp;A&#17;B&#5;C&quot;65&#18;&#19;ED
</p>
<p>If
&#21;
</p>
<p>-
</p>
<p>is the reflexive and transitive closure of
-
</p>
<p>, then F
&#5;*&#2;,&#19;&#16;&#3;HGI&quot;J&amp;1&#7; &#21;LK&#20;MON &amp;P&#11;&#13;&#9; N
</p>
<p>&#21;
</p>
<p>- &quot;RQ
</p>
<p>denotes the language generated by
&#2;
</p>
<p>. We denote by S
&#14;
</p>
<p>and S
&#14;T&#14;
</p>
<p>the classes of languages
generated by internal CGs without, respectively with choice1.
</p>
<p>Example 2.1 Consider a very simple internal CG with choice
&#2;&#22;2+&#3;U&#5;&#8;&#7;&#10;&#9;&#12;&#11;&#13;&#9;&#15;&#14;&#16;&#9;&#18;&#17;&#20;&#19;
</p>
<p>, where
&#7; &#3; G
</p>
<p>John
&#9;
</p>
<p>likes
&#9;
</p>
<p>Lyn
&#9;
</p>
<p>really
QWV
</p>
<p>&#11; &#3; G
</p>
<p>John likes Lyn
QWV
</p>
<p>&#14; &#3; G@&#5;
</p>
<p>really
&#9;&#18;XY&#19;&#18;QWV
</p>
<p>&#17;!&#5;#&quot;&#20;&#19;&#4;&#3; Z
</p>
<p>G@&#5;
</p>
<p>really
&#9;&#18;X&#20;&#19;&#15;QW&#9;
</p>
<p>if
&quot;L&#3;
</p>
<p>likes
&#9;
</p>
<p>[
</p>
<p>&#9;
</p>
<p>otherwise
V
</p>
<p>The language generated by
&#2;\2
</p>
<p>is: John (really) &#21; likes Lyn.
</p>
<p>Contextual grammars. even without choice, are able to generate interesting languages, which
are spread into (not covering) the traditional Chomsky hierarchy of languages. We denote by
]
</p>
<p>S_^ , `ba
</p>
<p>&#2;
</p>
<p>,
</p>
<p>&#14;
</p>
<p>]
</p>
<p>,
</p>
<p>&#14;,c
</p>
<p>the classes of finite, regular, context-free, respectively context-sensitive
languages.
</p>
<p>Example 2.2 Consider a CG without choice
&#2;&#13;5fi&#3;d&#5;eGgf6&#9;&#12;hIQW&#9;iGjXRQW&#9;kG@&#5; f&lt;&#9;&#15;hE&#19;&#18;Qj&#19;
</p>
<p>. We have
</p>
<p>F
</p>
<p>&#5; &#2;T5&#18;&#19;&#4;&#3; G
</p>
<p>NlKmN
</p>
<p>&amp;nGgf&lt;&#9;&#15;hoQ
</p>
<p>&#21;
</p>
<p>&#9;
</p>
<p>K NpK q
</p>
<p>&#3;
</p>
<p>K NpK r
</p>
<p>and
K
</p>
<p>&quot;
</p>
<p>K q&#24;slK
</p>
<p>&quot;
</p>
<p>K r
</p>
<p>&#9;4tY&quot;'u
</p>
<p>N
</p>
<p>Q
</p>
<p>where
K N&#22;K q
</p>
<p>denotes the number of occurrences of
f
</p>
<p>in
N
</p>
<p>and
&quot;vu
</p>
<p>N
</p>
<p>denotes that
N
</p>
<p>&#3;w&quot;6/ ( &quot; is a
prefix of N ). Then, F &#5;*&#2;T5E&#19;x&amp;y&#14; ]wz `ba &#2; . The language F &#5; &#2;,5E&#19; is known as the Dyck language
over
</p>
<p>Ggf&lt;&#9;&#15;hoQ
</p>
<p>.
</p>
<p>Example 2.3 Consider a CG without choice
&#2;&#13;7_&#3;A&#5;4Ggf&lt;&#9;&#15;hI&#9;&#15;{IQW&#9;kGjX3QW&#9;kG@&#5;C&quot;6/Y&#9;&#15;|&gt;&#19;&#18;&#9;m&#5;C&quot;?&#9;&#12;/@|&gt;&#19;
</p>
<p>K
</p>
<p>GI&quot;?&#9;}/&#20;&#9;&#15;|_Qb&#3;
</p>
<p>Ggf&lt;&#9;&#15;hI&#9;&#15;{mQ&gt;Qj&#19;
</p>
<p>. We have
</p>
<p>F
</p>
<p>&#5;*&#2;&#16;7E&#19;~&#3; G
</p>
<p>NlKmN
</p>
<p>&amp;&#127;Ggf&lt;&#9;&#15;hI&#9;&#15;{mQ
</p>
<p>&#21;
</p>
<p>&#9;
</p>
<p>K N&#22;K q
</p>
<p>&#3;
</p>
<p>K NpK r
</p>
<p>&#3;
</p>
<p>K N&#22;K &#128;
</p>
<p>QWD
</p>
<p>Then, F
&#5; &#2;T7E&#19;fi&amp;'&#14;,c
</p>
<p>z
</p>
<p>&#14;
</p>
<p>]
</p>
<p>. The language F
&#5;*&#2;T7E&#19;
</p>
<p>is known as the Bach language over
Ggf&lt;&#9;&#15;hI&#9;&#15;{IQ
</p>
<p>.
</p>
<p>More expressive languages can be generated using the choice function together with a con-
dition on the type of the languages selecting contexts. A CG in the modular presentation
&#2; &#3; &#5;&#8;&#7;&#10;&#9;&#12;&#11;&#13;&#9;m&#5;*c&#129;2&#18;&#9;&#15;&#14;&#130;2&#131;&#19;&#18;&#9;oDoDoDi&#9;m&#5;&#8;c&#20;&#132;_&#9;&#15;&#14;fi&#132;&#133;&#19;}&#19;
</p>
<p>is a CG
&#2; &#3; &#5;&#8;&#7;&#10;&#9;&#12;&#11;&#13;&#9;&#12;&#14;T&#9;&#18;&#17;&#20;&#19;
</p>
<p>such that
&#14;x&#134;&#136;&#135;&#137;&#17;B&#5;C&quot;&#20;&#19;
</p>
<p>, for all
&quot;w&amp;&#321;c&#20;&#134;
</p>
<p>, &#139;
</p>
<p>u&#141;&#140;&#13;u&#141;&#142;
</p>
<p>. A pair
&#5;&#8;c&#143;&#134;&#8;&#9;&#15;&#14;fi&#134;&#144;&#19;
</p>
<p>is called a contextual production. We say that a CG has
an
</p>
<p>]
</p>
<p>choice, where
] denotes a family of languages, if
</p>
<p>cR&#134;&#24;&amp;
</p>
<p>]
</p>
<p>, for all &#139;
u&#145;&#140;8u&#146;&#142;
</p>
<p>. We de-
note by S
</p>
<p>&#14;T&#14;&#147;&#5;
</p>
<p>]
</p>
<p>&#19;
</p>
<p>the class of languages generated by internal CGs with ] -choice. For example,
S
</p>
<p>&#14;,&#14;p&#5;
</p>
<p>`ba
</p>
<p>&#2;&#13;&#19;
</p>
<p>denotes the languages generated by internal CGs with regular selection languages.
1For details on contextual grammars, the reader is referred to the monograph (Pa&#774;un, 1997).</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>R. Gramatovici, C. Mart&#305;&#769;n-Vide
</p>
<p>Example 2.4 Consider
&#2;&#16;&#148;&#149;&#3;d&#5;4Ggf&lt;&#9;&#15;hI&#9;&#15;{m&#9;&#15;&#150;&#151;QW&#9;kGgfWhE{E&#150;&#151;QW&#9;m&#5;*fWh&#18;&#152;&#143;{m&#9;kG@&#5; f6&#9;&#12;{k&#19;&#15;Qj&#19;E&#9;m&#5; hE{E&#152;&#143;&#150;&lt;&#9;kGghI&#9;&#15;&#150;_&#19;&#15;Qj&#19;}&#19;
</p>
<p>a CG in the
modular presentation. We have
</p>
<p>F
</p>
<p>&#5;*&#2;b&#148;E&#19;&#4;&#3; Ggf
</p>
<p>&#132;
</p>
<p>h&#12;&#153;fi{
</p>
<p>&#132;
</p>
<p>&#150;&#133;&#153; K &#142;!&#9;}&#154; s
</p>
<p>&#139;
</p>
<p>QWD
</p>
<p>Then, F
&#5;*&#2;b&#148;i&#19;&#155;&amp;A&#14;,c
</p>
<p>z
</p>
<p>&#14;
</p>
<p>]
</p>
<p>. F
</p>
<p>&#5;*&#2;b&#148;i&#19;
</p>
<p>is the schematic representation of linguistic cross dependen-
cies.
</p>
<p>Though, there are either simple context-free or linguistic relevant languages that cannot be
generated by any (internal) CG, even with choice.
Indeed, the language F
</p>
<p>2fi&#3;UGgf
</p>
<p>&#132;
</p>
<p>K &#142; s
</p>
<p>&#139;
</p>
<p>Q&#155;&#156;AGgf
</p>
<p>&#132;
</p>
<p>h
</p>
<p>&#132;
</p>
<p>K &#142; s
</p>
<p>&#139;
</p>
<p>Q
</p>
<p>cannot be generated with (internal)
CG. The problem comes from the fact that, in order to generate all the sentences of the form
</p>
<p>f
</p>
<p>&#132;
</p>
<p>,
&#142; s
</p>
<p>&#139; one needs to define a context
&#5;C:&#129;&#9;}=O&#19;
</p>
<p>containing only the letter
f (at least one occurrence)
</p>
<p>and selected by a set of strings also made only by
f
</p>
<p>s. Then, there is no mechanism to avoid
the insertion of the context
</p>
<p>&#5; :?&#9;&#12;=@&#19;
</p>
<p>into sentences of the form
f
</p>
<p>&#132;
</p>
<p>h
</p>
<p>&#132;
</p>
<p>(for an appropriate &#142; ) and
producing in this way sentences of the form
</p>
<p>f
</p>
<p>&#132;
</p>
<p>&#152;
</p>
<p>&#134;
</p>
<p>h
</p>
<p>&#132;
</p>
<p>, with
&#140;\&#157;&#159;&#158;
</p>
<p>, which do not belong to the
language F
</p>
<p>2 (details of the proof are in (Pa&#774;un, 1997), page 58). Thus, F 2x&amp;'&#14; ]&#160;z S &#14;T&#14; .
A similar problem occurs with the language F
</p>
<p>5&#161;&#3; Ggf
</p>
<p>&#132;
</p>
<p>h
</p>
<p>&#132;
</p>
<p>{
</p>
<p>&#132;
</p>
<p>K
</p>
<p>&#142;
</p>
<p>s
</p>
<p>&#158;_Q
</p>
<p>, a non-context-free
language, which schematically represents the multiple (triple) agreement that can be found also
in some linguistic constructions. In (Pa&#774;un, 1997), page 46, it is proved that F 5&#163;&#162;&amp; S &#14;T&#14; using
a pumping lemma, which holds for internal CG. Constructively speaking, the problem comes
again from the fact that once one considers a context of the form
</p>
<p>&#5;*f&lt;&#9;&#15;h&#18;{k&#19;
</p>
<p>to generate only strings
with an equal number of
</p>
<p>f
</p>
<p>s,
h
</p>
<p>s and
{
</p>
<p>s, this context cannot be applied anywhere, but only between
f
</p>
<p>s and
h
</p>
<p>s (the left side of the context), respectively between h s and { s (its right side). The only
way to do this would be to associate with the context a selector of the form
</p>
<p>h
</p>
<p>&#132;
</p>
<p>,
</p>
<p>&#142;
</p>
<p>s
</p>
<p>&#139; , but this is
not enough, because it would be possible to apply the context also to a shorter string of
</p>
<p>h
</p>
<p>s than
intended and generate strings of the form
</p>
<p>f
</p>
<p>&#132;
</p>
<p>h
</p>
<p>&#153;
</p>
<p>fWhe&#164;mh&#18;{ih&#12;&#165;&#18;{
</p>
<p>&#132;
</p>
<p>, with
&#154;&#145;&#166;n&#167;&#22;&#166;1&#168;&#136;&#3;&#141;&#142;
</p>
<p>, which do not
belong to F
</p>
<p>5
</p>
<p>.
</p>
<p>In (Mart&#305;&#769;n-Vide et al., 1997) (also (Marcus et al., 1998)), it is shown that languages as F 2 or F 5
can be generated by internal CGs with a condition of maximality on the length of the substrings
of the given string, which are used to select contexts at some derivation step. In Section 5, we
provide with a different method for the improvement of the (weak) generative power of CGs.
</p>
<p>3 Dependency Contextual Grammars
</p>
<p>A dependency tree (D-tree) is a tree whose nodes are labelled over an alphabet of terminal
symbols. Sometimes, its edges are also labelled over another alphabet (of syntactic categories),
but we will not use this feature here as it is not relevant for the purpose of this paper. We
will introduce D-trees using the concept of a structured string from (Marcus, 1967) (see also
(Mart&#305;&#769;n-Vide &amp; Pa&#774;un, 1998), (Marcus et al., 1998)).
Let
</p>
<p>&#7;
</p>
<p>be an alphabet. If
&#142;
</p>
<p>is a natural number, we denote by &#169;
&#142;Y&#170;
</p>
<p>the set of the first
&#142;
</p>
<p>natural
numbers. A structured string over
</p>
<p>&#7;
</p>
<p>is a pair
&#5;C&quot;?&#9;&#12;&#171;W&#172;g&#19;
</p>
<p>, where
&quot;%&amp;(&#7;
</p>
<p>&#152;
</p>
<p>is a non-empty string
over
</p>
<p>&#7;
</p>
<p>and
&#171;&#133;&#172;&#127;&#135;
</p>
<p>&#169;
</p>
<p>K
</p>
<p>&quot;
</p>
<p>K
</p>
<p>&#170;&#173;&#23;
</p>
<p>&#169;
</p>
<p>K
</p>
<p>&quot;
</p>
<p>K
</p>
<p>&#170;
</p>
<p>z
</p>
<p>G@&#5;C&#140;}&#9;&#12;&#140;&#174;&#19;
</p>
<p>K
</p>
<p>&#140;&#22;&amp;
</p>
<p>&#169;
</p>
<p>K
</p>
<p>&quot;
</p>
<p>K
</p>
<p>&#170; Q
</p>
<p>is an anti-reflexive binary relation, called
dependency relation on
</p>
<p>&quot;
</p>
<p>. If
&quot;
</p>
<p>is a string and
&#140;T&amp;
</p>
<p>&#169;
</p>
<p>K
</p>
<p>&quot;
</p>
<p>K
</p>
<p>&#170;
</p>
<p>, we denote by
&quot;B&#5; &#140;&#174;&#19;
</p>
<p>the
&#140;
</p>
<p>-th symbol of
&quot;
</p>
<p>. If
&#140;&#8;&#171;&#133;&#172;E&#175;
</p>
<p>, then we say that
&quot;B&#5;&#176;&#175;_&#19;
</p>
<p>depends on
&quot;B&#5; &#140;e&#19;
</p>
<p>. Let us denote by
&#171;O&#152;
</p>
<p>&#172; (and call dominance
relation on
</p>
<p>&quot; ) the transitive closure of &#171;W&#172; . If &#140;*&#171;_&#152;&#172; &#175; , then we say that &quot;B&#5;C&#140;e&#19; dominates &quot;B&#5;&#177;&#175;@&#19; .</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Contextual Grammars and Dependency Trees
</p>
<p>A structured string &#178;
&#3; &#5;C&quot;?&#9;&#12;&#171;&gt;&#172;g&#19;
</p>
<p>is called a D-tree iff the dependency relation
&#171;_&#172;
</p>
<p>induces a
structure of tree over
</p>
<p>&quot;
</p>
<p>, i.e. i) there is &#139; u&#159;&#168;Au &#169; K &quot; K &#170; such that &quot;B&#5; &#168;&#179;&#19; does not depend on any
symbol of
</p>
<p>&quot; ( &#168; is called the root of &#178; ); ii) for any &#140;+&amp; &#169; K &quot; K &#170; z Gm&#168;WQ , there is a unique index &#175;&#147;&amp; &#169; K &quot; K &#170;
such that
</p>
<p>&quot;B&#5;C&#140;e&#19;
</p>
<p>depends on
&quot;B&#5;&#177;&#175;@&#19;
</p>
<p>; iii) &#171; &#152;&#172; is an anti-reflexive relation, i.e. &#5;C&#140;}&#9;&#12;&#140;&#174;&#19;&#127;&#162;&amp;(&#171; &#152;&#172; , for any
&#140;+&amp;
</p>
<p>&#169;
</p>
<p>K &quot; K &#170;
</p>
<p>. We denote by &#180;
&#5;*&#7;&#13;&#19;
</p>
<p>the set of D-trees over a set
&#7;
</p>
<p>of terminals.
</p>
<p>Linguistically motivated, the structures over CGs were introduced in (Mart&#305;&#769;n-Vide &amp; Pa&#774;un,
1998) (see al(Marcus et al., 1998), or Section 7.6 in (Pa&#774;un, 1997)). Bracketed CGs, i.e. contex-
tual grammars working on strings of terminal symbols enriched with a well-formed structure
of brackets, were extensively studied in (Kappes, 2000). We are interested here in the second
type of structured CGs introduced in (Mart&#305;&#769;n-Vide &amp; Pa&#774;un, 1998), which do not use brackets
for creating tree-like structures on strings, but binary relations between terminal symbols. How-
ever, the definition of a dependency CG given below is formally different from the definition of
a structured CG given in (Mart&#305;&#769;n-Vide &amp; Pa&#774;un, 1998).
We call
</p>
<p>&#2;&#181;&#3;&#182;&#5;*&#7;+&#9;&#12;&#11;,&#9;&#15;&#183;+2&#18;&#9;oDkDoDk&#9;&#15;&#183;?&#132;&#179;&#19;
</p>
<p>an internal dependency contextual grammar (IDCG) iff &#7; is
an alphabet,
</p>
<p>&#11; &#135;
</p>
<p>&#180;
</p>
<p>&#5;&#8;&#7;,&#19;
</p>
<p>is a finite set of D-trees over
&#7; (the axioms) and for any &#140;.&amp; &#169; &#142;&#20;&#170; ,
</p>
<p>&#183;&#129;&#134;&#184;&#3;&#185;&#5;*c&#20;&#134;&#174;&#9;&#15;{&#15;&#134;&#174;&#9;&#15;&#150;&#133;&#134;&#144;&#19;
</p>
<p>is a dependency contextual production, with
c3&#134;,&#135;&#186;&#7;
</p>
<p>&#21; (the set of selectors),
{&#18;&#134;&#187;&#3;&#188;&#5; :&lt;&#134;&#8;&#9;&#12;=g&#134;#&#19;&#16;&amp;J&#7;
</p>
<p>&#21;
</p>
<p>&#23;n&#7;
</p>
<p>&#21; (the context) and &#150;&gt;&#134;$&#135; &#169; &#5; &#169; K :&lt;&#134;&#144;=m&#134; K &#170;O&#156;&#25;&#7;,&#19;&#130;&#23;J&#5; &#169; K :&lt;&#134;&#144;=m&#134; K &#170;&#151;&#156;A&#7;&#13;&#19;e&#170; z &#7;&#141;&#23;&#127;&#7; (the
set of new dependencies). A derivation in an IDCG is defined as a binary relation over the set
of D-trees over
</p>
<p>&#7;
</p>
<p>by:
&#5;C&quot;?&#9;&#12;&#171;&#133;&#172;g&#19;+-P&#189;&#184;&#5; /Y&#9;&#12;&#171;&#133;&#190;k&#19;
</p>
<p>iff
&quot;L&#3;9&quot;324&quot;65&#12;&quot;67I&#9;}/&#191;&#3;9&quot;32;:&lt;&quot;65&#12;=&gt;&quot;67
</p>
<p>and there is
&#140;+&amp;
</p>
<p>&#169;
</p>
<p>&#142;Y&#170;&#8;&#9;
</p>
<p>with
&quot;65&#130;&amp;yc&#20;&#134;&#174;&#9;
</p>
<p>{&#18;&#134;&#143;&#3;l&#5;C:?&#9;&#12;=@&#19;
</p>
<p>and
&#171;&#133;&#190;
</p>
<p>built under the following rules:
-
</p>
<p>&#171;&#133;&#190;
</p>
<p>contains all the dependencies in
&#171;W&#172;
</p>
<p>and no other
dependencies occur between the symbols originating in
</p>
<p>&quot;?&#9;
</p>
<p>-
</p>
<p>&#171;&#133;&#190;
</p>
<p>contains all the dependencies between the symbols of
{i&#134;&#174;&#9;
</p>
<p>described in
&#150;&gt;&#134;&#174;&#9;
</p>
<p>and no other dependencies occur between the
symbols originating in
</p>
<p>{E&#134;&#174;&#9;
</p>
<p>-
</p>
<p>&#171;&#133;&#190;
</p>
<p>may contain dependencies between the symbols of
&quot;
</p>
<p>and
the new symbols introduced by
</p>
<p>{E&#134;
</p>
<p>as described by
&#150;&gt;&#134;
</p>
<p>i.e.
if a dependency occurs between
</p>
<p>:Y&#134;&#192;=g&#134;e&#5;&#176;&#175;_&#19;
</p>
<p>and a symbol
f
</p>
<p>from
&quot;?&#9;
</p>
<p>then
&#5;&#177;&#175;&#133;&#9;&#15;f_&#19;x&amp;'&#150;&gt;&#134; (respectively &#5;*f&lt;&#9;e&#175;_&#19;fi&amp;y&#150;&gt;&#134;#&#19;&#18;D
</p>
<p>The difference between the above definition of a IDCG and the definition of a structured CG
from (Mart&#305;&#769;n-Vide &amp; Pa&#774;un, 1998) consists in the fact that the new symbols inserted by some
dependency contextual rule do not attach to some specified (localized) selector symbols, but to
some selector symbols having the value specified in the set of new dependencies.
</p>
<p>If
&#21;
</p>
<p>-P&#189;
</p>
<p>is the reflexive and transitive closure of
-w&#189;
</p>
<p>, then &#193;&#147;F
&#5; &#2;&#13;&#19;$&#3;dG
</p>
<p>&#178;
</p>
<p>&amp;
</p>
<p>&#180;
</p>
<p>&#5;&#8;&#7;,&#19;
</p>
<p>K&gt;M&lt;&#194;
</p>
<p>&amp;y&#11;&#13;&#9;
</p>
<p>&#194;
</p>
<p>&#21;
</p>
<p>-P&#189;
</p>
<p>&#178;
</p>
<p>Q
</p>
<p>denotes the dendrolanguage (the set of D-trees) generated by &#2; . Then, F &#5; &#2;&#13;&#19;&#149;&#3;)GI&quot;&#127;&amp;n&#7; &#152;&#160;K
M
</p>
<p>&#5;C&quot;?&#9;&#12;&#171;&#133;&#172;m&#19;&#13;&amp;
</p>
<p>&#193;&#136;F
</p>
<p>&#5;*&#2;,&#19;&#18;Q
</p>
<p>denotes the language generated by
&#2;
</p>
<p>. We denote by S@&#193;
&#14;,&#14;
</p>
<p>the class of
languages generated by IDCGs.
</p>
<p>Example 3.1 We reconsider Example 2.1, from the previous section, now introducing depen-
dencies on words. We define an internal DCG
</p>
<p>&#2;,&#195;
</p>
<p>2
</p>
<p>&#3;d&#5;&#8;&#7;&#10;&#9;&#12;&#11;&#13;&#9;&#15;&#183;,&#19;
</p>
<p>, where
&#7; &#3; G
</p>
<p>John
&#9;
</p>
<p>likes
&#9;
</p>
<p>Lyn
&#9;
</p>
<p>really
QWV
</p>
<p>&#11; &#3; G@&#5;
</p>
<p>John likes Lyn
&#9;kG@&#5;*&#196;@&#9;
</p>
<p>&#139;
</p>
<p>&#19;&#18;&#9;m&#5;&#8;&#196;_&#9;&#15;&#197;&gt;&#19;&#18;Qj&#19;&#15;QWV</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>R. Gramatovici, C. Mart&#305;&#769;n-Vide
</p>
<p>&#183; &#3; &#5;4G
</p>
<p>likes
QW&#9;m&#5;
</p>
<p>really
&#9;&#18;XY&#19;E&#9;kG@&#5;
</p>
<p>likes
&#9;
</p>
<p>&#139;
</p>
<p>&#19;&#18;Q&gt;Qj&#19;&#18;D
</p>
<p>One of the D-trees generated by
&#2; &#195;
</p>
<p>2 is:
&#5;
</p>
<p>John really likes Lyn
&#9;kG@&#5; &#197;O&#9;
</p>
<p>&#139;
</p>
<p>&#19;&#18;&#9;m&#5;*&#197;@&#9;&#12;&#198;W&#19;E&#9;I&#5;*&#197;O&#9;&#18;&#196;&#179;&#19;&#18;Qj&#19;
</p>
<p>(see Figure 1), which underlies the string: John really likes Lyn.
</p>
<p>&#199; &#140;e&#200;O&#201; &#194;
</p>
<p>&#202;R&#203;j&#204;
</p>
<p>&#142; &#205;
</p>
<p>&#205;
</p>
<p>&#205;
</p>
<p>&#205;&gt;&#206;
</p>
<p>F
</p>
<p>/@&#142;
</p>
<p>&#207;
</p>
<p>&#207;
</p>
<p>&#207;
</p>
<p>&#207;
</p>
<p>&#207;
</p>
<p>&#207;
</p>
<p>&#207;&gt;&#208;
</p>
<p>&#168;j&#201;If &#199;C&#199; / &#209;
</p>
<p>&#209;
</p>
<p>&#209;
</p>
<p>&#209; &#210;
</p>
<p>Figure 1: A D-tree for John really likes Lyn
</p>
<p>Projectivity is a very common property of D-trees, which expresses the fact that the vertical
projections of the nodes do not intersect the edges of the tree. We will consider in the following
a particular case of DCG, which generates only projective D-trees.
Let
</p>
<p>&quot;
</p>
<p>be a string. A sequence
&quot;B&#5;C&#140;e&#19;&#143;DoDoD;&quot;B&#5;&#176;&#175;_&#19;
</p>
<p>, &#139;
</p>
<p>u&#186;&#140;&#22;u&#146;&#175;&#160;u
</p>
<p>K
</p>
<p>&quot;
</p>
<p>K
</p>
<p>, of consecutive symbols of
&quot;
</p>
<p>is called an interval of
&quot;
</p>
<p>. Let
&#5;#&quot;?&#9;&#12;&#171;&gt;&#172;g&#19;
</p>
<p>be a D-tree. The maximal projection of a symbol &quot;B&#5; &#140;e&#19; ,
&#140;+&amp;
</p>
<p>&#169;
</p>
<p>K
</p>
<p>&quot;
</p>
<p>K
</p>
<p>&#170;
</p>
<p>, of
&quot;
</p>
<p>, is the sequence
&quot;B&#5; &#140;&#131;2}&#19;&#143;DoDkD&#131;&quot;B&#5; &#140; &#132;W&#19;
</p>
<p>,
</p>
<p>&#142;y&amp;
</p>
<p>&#169;
</p>
<p>K
</p>
<p>&quot;
</p>
<p>K
</p>
<p>&#170;
</p>
<p>, of (not necessarily consecutive) symbols
of
</p>
<p>&quot;
</p>
<p>, such that
&#140;#&#211;&#184;&#212;P&#140;&#8;&#213;
</p>
<p>, for any &#139;
u&#214;&#175;&#147;&#212;&#160;&#200;Luw&#142;
</p>
<p>, and
Gm&#140;;2E&#9;oDoDoDi&#9;&#12;&#140;*&#132;WQb&#3;(Gm&#140;&#131;Qx&#156;yGE&#175;
</p>
<p>K
</p>
<p>&#140;*&#171;W&#152;&lt;&#175;&#151;Q
</p>
<p>.
</p>
<p>Then, a D-tree is projective iff all the maximal projections of its symbols are intervals (see
(Dikovsky &amp; Modina, 2000)). We denote by &#183; &#180; &#5;*&#7;&#13;&#19; the set of projective D-trees over an
alphabet
</p>
<p>&#7;
</p>
<p>.
</p>
<p>An IDCG
&#2;&#141;&#3;&#159;&#5;&#8;&#7;&#10;&#9;&#12;&#11;,&#9;&#15;&#183;+2&#18;&#9;oDoDkDk&#9;&#15;&#183;?&#132;&gt;&#19;
</p>
<p>is called a projective internal dependency contextual gram-
mar (PIDCG) iff the axioms are projective D-trees, &#11;&#188;&#135;&#215;&#183; &#180; &#5;*&#7;&#13;&#19; , and the derivation relation,
-
</p>
<p>&#164;
</p>
<p>&#189;
</p>
<p>is defined as the restriction of
-&#160;&#189;
</p>
<p>on the set
&#183;
</p>
<p>&#180;
</p>
<p>&#5;&#8;&#7;,&#19;
</p>
<p>of projective D-trees. Correspond-
ingly, the dendrolanguage generated by a PIDCG is defined exactly as in the case of an IDCG,
but using the derivation relation
</p>
<p>-
</p>
<p>&#164;
</p>
<p>&#189;
</p>
<p>. We denote by
&#183;
</p>
<p>S_&#193;
</p>
<p>&#14;T&#14;
</p>
<p>the class of languages generated
by PIDCGs.
</p>
<p>The IDCG in Example 3.1 is a PIDCG.
</p>
<p>4 Strong Generative Properties
</p>
<p>After introducing structures on strings generated by CGs, one may speak about the strong gen-
erative power of the structured CGs. The strong generative power means the capacity of (depen-
dency, in our case) CGs to generate different dendrolanguages, underlying the same language.
In this section, we investigate some structural properties of IDCGs.
</p>
<p>Top-down and bottom-up derivation
</p>
<p>The derivation of phrases in a context-free grammar is a top-down process starting with the root
of the (constituent) tree and leading to its leaves. Among the novelties brought by tree-adjoining</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Contextual Grammars and Dependency Trees
</p>
<p>grammars (Joshi, 1987) and other formalisms in the area of the mild context sensitiveness, was
the fact that derivation is not necessarily a top-down process. It is important to remark the
capacity of IDCGs to construct the (same) structure of the (same) string in totally different
ways.
</p>
<p>Example 4.1 Consider
&#2;,&#216;&#130;&#3;&#146;&#5;eGgf&lt;&#9;&#15;hoQW&#9;kG@&#5;*fWhI&#9;kG@&#5;
</p>
<p>&#139;
</p>
<p>&#9;&#18;&#196;&#133;&#19;&#18;Qj&#19;&#15;QW&#9;m&#5;4GjX3QW&#9;m&#5; f6&#9;&#12;hi&#19;&#18;&#9;kG@&#5;*f&lt;&#9;
</p>
<p>&#139;
</p>
<p>&#19;E&#9;m&#5;
</p>
<p>&#139;
</p>
<p>&#9;&#18;&#196;&#133;&#19;&#15;Qj&#19;}&#19;
</p>
<p>and
&#2;T&#217;&#130;&#3;
</p>
<p>&#5;4Ggf&lt;&#9;&#15;hoQW&#9;kG@&#5; f_hI&#9;kG@&#5;
</p>
<p>&#139;
</p>
<p>&#9;&#18;&#196;&#133;&#19;&#15;Qj&#19;&#18;QW&#9;m&#5; f &#152; h &#152; &#9;m&#5; f&lt;&#9;&#15;hE&#19;E&#9;kG@&#5;
</p>
<p>&#139;
</p>
<p>&#9;&#15;f_&#19;E&#9;m&#5;
</p>
<p>&#139;
</p>
<p>&#9;&#18;&#196;&#133;&#19;&#15;Qj&#19;}&#19;
</p>
<p>two IDCGs.
</p>
<p>The D-tree represented in Figure 2 corresponding to the string
f
</p>
<p>7
</p>
<p>h
</p>
<p>7
</p>
<p>is generated in a top-down
manner (at any step, the axiom fWh rests in the top, while the new context &#5;*f&lt;&#9;&#15;hE&#19; is added at the
bottom of the tree) by &#2;,&#216; and in a bottom-up manner (the new context is added always in the
top) by &#2;&#16;&#217; .
</p>
<p>f
</p>
<p>h &#218;
</p>
<p>&#218;
</p>
<p>&#218;
</p>
<p>&#218;
</p>
<p>&#218;
</p>
<p>&#218;
</p>
<p>&#218;
</p>
<p>&#218;
</p>
<p>&#218;
</p>
<p>&#218;
</p>
<p>&#218;
</p>
<p>&#218;
</p>
<p>&#218;
</p>
<p>&#218;
</p>
<p>&#218; &#219;
</p>
<p>h
</p>
<p>&#218;
</p>
<p>&#218;
</p>
<p>&#218;
</p>
<p>&#218;
</p>
<p>&#218;
</p>
<p>&#218;
</p>
<p>&#218;
</p>
<p>&#218; &#219;
</p>
<p>f
</p>
<p>h
</p>
<p>f
</p>
<p>&#209;
</p>
<p>&#209;
</p>
<p>&#209; &#210;
</p>
<p>&#209;
</p>
<p>&#209;
</p>
<p>&#209; &#210;
</p>
<p>&#209;
</p>
<p>&#209;
</p>
<p>&#209; &#210;
</p>
<p>Figure 2: D-tree of the string
f
</p>
<p>7
</p>
<p>h
</p>
<p>7
</p>
<p>generated by
&#2;T&#216;
</p>
<p>and
&#2;T&#217;
</p>
<p>Nested and cross-serial dependencies
</p>
<p>In Example 4.1, we defined two IDCGs, which are rather ambiguous with respect to the struc-
tures associated with well-formed strings. For example, in both grammars we can associate
several D-trees with the string
</p>
<p>f
</p>
<p>7
</p>
<p>h
</p>
<p>7
</p>
<p>, and not only the D-tree in Figure 2. The D-tree from Figure
2 is a projective D-tree, which represents a set of nested dependencies between the symbols
f
</p>
<p>and
h
</p>
<p>that were inserted in the string at the same derivation step. Indeed, one may notice
that the last
</p>
<p>h
</p>
<p>depends on the first
f
</p>
<p>, the second
h
</p>
<p>depends on the second
f
</p>
<p>, while the first
h
</p>
<p>depends on the last
f
</p>
<p>. Another possible structure of
f
</p>
<p>7
</p>
<p>h
</p>
<p>7
</p>
<p>, generated by the grammar
&#2;,&#217;
</p>
<p>, is the
non-projective D-tree in Figure 3, which encounters mixed dependencies.
</p>
<p>f
</p>
<p>h
</p>
<p>&#218;
</p>
<p>&#218;
</p>
<p>&#218;
</p>
<p>&#218;
</p>
<p>&#218;
</p>
<p>&#218;
</p>
<p>&#218;
</p>
<p>&#218;
</p>
<p>&#218;
</p>
<p>&#218;
</p>
<p>&#218;
</p>
<p>&#218;
</p>
<p>&#218;
</p>
<p>&#218;
</p>
<p>&#218; &#219;
</p>
<p>h
</p>
<p>&#218;
</p>
<p>&#218;
</p>
<p>&#218;
</p>
<p>&#218;
</p>
<p>&#218;
</p>
<p>&#218;
</p>
<p>&#218;
</p>
<p>&#218;
</p>
<p>&#218;
</p>
<p>&#218;
</p>
<p>&#218;
</p>
<p>&#218;
</p>
<p>&#218;
</p>
<p>&#218; &#219;
</p>
<p>f
</p>
<p>h
</p>
<p>f
</p>
<p>&#209;
</p>
<p>&#209;
</p>
<p>&#209;
</p>
<p>&#209; &#210;
</p>
<p>&#220;
</p>
<p>&#220;
</p>
<p>&#220;
</p>
<p>&#220;
</p>
<p>&#220; &#221;
</p>
<p>&#205;
</p>
<p>&#205;
</p>
<p>&#205;&gt;&#206;
</p>
<p>Figure 3: D-tree of the phrase
f
</p>
<p>7
</p>
<p>h
</p>
<p>7
</p>
<p>generated by
&#2;T&#217;
</p>
<p>In practice, one may want to control the distribution of dependencies over the string in a very
rigorous manner. The following example illustrates the capacity of PIDCGs to generate D-trees
with specific dependencies.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>R. Gramatovici, C. Mart&#305;&#769;n-Vide
</p>
<p>Example 4.2 First, we modify the grammar
&#2;&#13;&#217;
</p>
<p>from Example 4.1 in order to obtain only D-
trees representing nested dependencies. Actually, we may keep the same grammar and consider
the projective derivation - &#164; &#189; . &#2;&#16;&#217; becomes a PIDCG (since the axiom is a projective D-tree).
Then the D-tree represented in Figure 2 is the only D-tree associated with
</p>
<p>f
</p>
<p>7
</p>
<p>h
</p>
<p>7
</p>
<p>and generated by
&#2;T&#217;
</p>
<p>in the projective derivation style. Also the language generated by &#2;8&#217; , in this case, is exactly
Ggf
</p>
<p>&#132;
</p>
<p>h
</p>
<p>&#132;
</p>
<p>K &#142; s
</p>
<p>&#139;
</p>
<p>Q
</p>
<p>.
</p>
<p>The second grammar generates the same language,
Ggf
</p>
<p>&#132;
</p>
<p>h
</p>
<p>&#132;
</p>
<p>K &#142; s
</p>
<p>&#139;
</p>
<p>Q
</p>
<p>, using a totally different
method, which hides cross-serial dependencies between the symbols
</p>
<p>f
</p>
<p>and
h
</p>
<p>introduced at the
same derivation step. Consider
</p>
<p>&#2;&#13;&#222;&#13;&#3;HGgf6&#9;&#12;hIQW&#9;iG@&#5;*fWhI&#9;kG@&#5;
</p>
<p>&#139;
</p>
<p>&#9;&#15;&#196;&#133;&#19;&#18;Qj&#19;&#15;QW&#9;m&#5;4GgfW&#152;RQW&#9;m&#5;*f&lt;&#9;&#15;hE&#19;E&#9;iG@&#5;
</p>
<p>&#139;
</p>
<p>&#9;&#15;f_&#19;&#18;&#9;m&#5;*hI&#9;&#18;&#196;&#133;&#19;&#15;Qj&#19;}&#19;
</p>
<p>a
</p>
<p>PIDCG. The (only) D-tree associated with the string f
7
</p>
<p>h
</p>
<p>7
</p>
<p>is represented in Figure 4.
</p>
<p>f
</p>
<p>h
</p>
<p>&#205;
</p>
<p>&#205;
</p>
<p>&#205;&gt;&#206;
</p>
<p>h
</p>
<p>&#218;
</p>
<p>&#218;
</p>
<p>&#218;
</p>
<p>&#218;
</p>
<p>&#218;
</p>
<p>&#218;
</p>
<p>&#218;
</p>
<p>&#218; &#219;
</p>
<p>f
</p>
<p>h
</p>
<p>f
</p>
<p>&#209;
</p>
<p>&#209;
</p>
<p>&#209; &#210;
</p>
<p>&#205;
</p>
<p>&#205;
</p>
<p>&#205;&gt;&#206;
</p>
<p>&#209;
</p>
<p>&#209;
</p>
<p>&#209; &#210;
</p>
<p>Figure 4: D-tree of the string
f
</p>
<p>7
</p>
<p>h
</p>
<p>7
</p>
<p>generated by
&#2;,&#222;
</p>
<p>5 Weak Generative Properties
</p>
<p>In this section, we will prove that using PIDCGs also the weak generative capacity of CGs is
improved.
</p>
<p>Theorem 5.1 For any internal contextual grammar
&#2;
</p>
<p>, there exists a weak equivalent internal
projective dependency contextual grammar &#2; &#195; .
</p>
<p>Proof Let
&#2; &#3; &#5;&#8;&#7;&#10;&#9;&#12;&#11;&#13;&#9;m&#5;*cB2&#15;&#9;&#15;&#14;&#130;2&#131;&#19;E&#9;kDoDoDk&#9;I&#5;&#8;c&#20;&#132;_&#9;&#15;&#14;fi&#132;&#133;&#19;}&#19;
</p>
<p>be an ICG. We may consider that each con-
textual production corresponds to one context. We construct the following PIDCG
</p>
<p>&#2;
</p>
<p>&#195;
</p>
<p>&#3;
</p>
<p>&#5;&#8;&#7;&#10;&#9;&#12;&#11;&#130;&#195;&#192;&#9;m&#5;&#8;c&#129;2&#15;&#9;&#15;&#14;&#130;2&#18;&#9;&#15;&#150;O2&#131;&#19;&#18;&#9;oDoDoDE&#9;m&#5;&#8;c&#20;&#132;@&#9;&#15;&#14;fi&#132;@&#9;&#15;&#150;&#133;&#132;&#179;&#19;}&#19;
</p>
<p>, where
&#11;
</p>
<p>&#195;
</p>
<p>&#3; G@&#5;C&quot;?&#9;}&#171;&#133;&#172;g&#19;
</p>
<p>K
</p>
<p>&quot;v&amp;v&#11;,&#9;&#12;&#171;&#133;&#172;b&#3;ffG@&#5; &#140;&#131;&#9;&#12;&#140;Y&#166;
</p>
<p>&#139;
</p>
<p>&#19;
</p>
<p>K
</p>
<p>&#140;+&amp;
</p>
<p>&#169;
</p>
<p>K
</p>
<p>&quot;
</p>
<p>Km&#223;
</p>
<p>&#139;
</p>
<p>&#170; Q&gt;Q
</p>
<p>&#150;&#133;&#134;&#146;&#3; G@&#5;*f&lt;&#9;e&#175;_&#19;
</p>
<p>K
</p>
<p>&#175;&#147;&amp;
</p>
<p>&#169;
</p>
<p>K
</p>
<p>:6&#134;&#176;=m&#134;
</p>
<p>K
</p>
<p>&#170;*&#9;&#15;fp&amp;'&#7;&#22;QW&#9;
</p>
<p>for
{&#18;&#134;&#20;&#3;l&#5;C:&lt;&#134;&#174;&#9;&#12;=m&#134;C&#19;
</p>
<p>and for all
&#140;+&amp;
</p>
<p>&#169;
</p>
<p>&#142;&#20;&#170;
</p>
<p>. Then, F
&#5;*&#2;&#16;&#195;&#224;&#19;+&#3;
</p>
<p>F
</p>
<p>&#5; &#2;&#13;&#19;
</p>
<p>. &#225;&#226;
</p>
<p>From the above proof, it is worth to note that on the contextual side nothing changes when
adding dependencies: if
</p>
<p>&#2;
</p>
<p>is without choice then
&#2;,&#195;
</p>
<p>is also without choice; if
&#2;
</p>
<p>has an
]
</p>
<p>-
</p>
<p>choice then
&#2;
</p>
<p>&#195;
</p>
<p>has also an
]
</p>
<p>-choice.
</p>
<p>Now consider the languages discussed in the end of Section 2, F
2&#24;&#3;&#188;Ggf
</p>
<p>&#132;
</p>
<p>K
</p>
<p>&#142;
</p>
<p>s
</p>
<p>&#139;
</p>
<p>Q&#130;&#156;nGgf
</p>
<p>&#132;
</p>
<p>h
</p>
<p>&#132;
</p>
<p>K
</p>
<p>&#142;
</p>
<p>s
</p>
<p>&#139;
</p>
<p>Q
</p>
<p>and F
5\&#3;&#181;Ggf
</p>
<p>&#132;
</p>
<p>h
</p>
<p>&#132;
</p>
<p>{
</p>
<p>&#132;
</p>
<p>K
</p>
<p>&#142;
</p>
<p>s
</p>
<p>&#158;@Q
</p>
<p>. These languages cannot be generated by any ICG, but
they can be generated by PIDCGs.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Contextual Grammars and Dependency Trees
</p>
<p>Example 5.1 In order to obtain the first language, let us consider the following PIDCG
&#2;&#191;&#227;R&#3;
</p>
<p>&#5;4Ggf&lt;&#9;&#15;hoQW&#9;kG@&#5; f6&#9;
</p>
<p>[
</p>
<p>&#19;E&#9;m&#5; f_hI&#9;kG@&#5;
</p>
<p>&#139;
</p>
<p>&#9;&#18;&#196;&#133;&#19;&#15;Qj&#19;&#18;QW&#9;?&#5;eGgf&#151;QW&#9;m&#5;*f&lt;&#9;&#18;XY&#19;E&#9;kG@&#5;
</p>
<p>&#139;
</p>
<p>&#9;&#15;f_&#19;&#18;Qj&#19;&#18;&#9;m&#5;4GgfWhoQW&#9;m&#5; f6&#9;&#12;hi&#19;&#18;&#9;kG@&#5;*hI&#9;
</p>
<p>&#139;
</p>
<p>&#19;E&#9;m&#5; hm&#9;&#15;&#196;&#133;&#19;&#18;Qj&#19;&#131;&#19;
</p>
<p>. We have
F
</p>
<p>&#5;*&#2;&#16;&#227;&#18;&#19;&#24;&#3;
</p>
<p>F
</p>
<p>2
</p>
<p>. Figure 5 illustrates the unique D-tree and one of the possible D-trees generated
by
</p>
<p>&#2;&#16;&#227;
</p>
<p>and associated with the strings
f
</p>
<p>7
</p>
<p>, respectively
f
</p>
<p>7
</p>
<p>h
</p>
<p>7
</p>
<p>.
</p>
<p>f
</p>
<p>f
</p>
<p>f
</p>
<p>&#209;
</p>
<p>&#209;
</p>
<p>&#209; &#210;
</p>
<p>&#209;
</p>
<p>&#209;
</p>
<p>&#209; &#210;
</p>
<p>f
</p>
<p>h
</p>
<p>&#218;
</p>
<p>&#218;
</p>
<p>&#218;
</p>
<p>&#218;
</p>
<p>&#218;
</p>
<p>&#218;
</p>
<p>&#218; &#219;
</p>
<p>h
</p>
<p>&#220;
</p>
<p>&#220;
</p>
<p>&#220;
</p>
<p>&#220;
</p>
<p>&#220; &#221;
</p>
<p>f hf
</p>
<p>&#209;
</p>
<p>&#209;
</p>
<p>&#209; &#210;
</p>
<p>&#228;
</p>
<p>&#228;
</p>
<p>&#228;
</p>
<p>&#228;
</p>
<p>&#228;&gt;&#229;
</p>
<p>&#205;
</p>
<p>&#205;
</p>
<p>&#205;&gt;&#206;
</p>
<p>Figure 5: D-trees of
f
</p>
<p>7
</p>
<p>, respectively
f
</p>
<p>7
</p>
<p>h
</p>
<p>7
</p>
<p>generated by
&#2;T&#227;
</p>
<p>Example 5.2 Let
&#2;T&#230;&#160;&#3; &#5;4Ggf&lt;&#9;&#15;hI&#9;&#15;{IQW&#9;kG@&#5;*fWh&#18;{g&#9;iG@&#5;&#8;&#196;@&#9;
</p>
<p>&#139;
</p>
<p>&#19;E&#9;m&#5;*&#196;@&#9;&#15;&#197;&gt;&#19;&#15;QW&#9;m&#5;4Ggh &#152; &#9;m&#5;*f&lt;&#9;&#15;h&#18;{i&#19;E&#9;kG@&#5; f&lt;&#9;
</p>
<p>&#139;
</p>
<p>&#19;E&#9;I&#5;*hI&#9;&#18;&#196;&#133;&#19;&#18;&#9;m&#5;*{m&#9;&#15;&#197;&gt;&#19;&#15;Qj&#19;}&#19;
</p>
<p>be another PIDCG. We obtain the second language, F
&#5;*&#2;&#13;&#230;E&#19;'&#3;
</p>
<p>F
</p>
<p>5
</p>
<p>. Figure 6 illustrates one
of the possible D-trees generated by
</p>
<p>&#2;&#13;&#230;
</p>
<p>and associated with the string
f
</p>
<p>7
</p>
<p>h
</p>
<p>7
</p>
<p>{
</p>
<p>7
</p>
<p>.
</p>
<p>f
</p>
<p>f
</p>
<p>&#218;
</p>
<p>&#218;
</p>
<p>&#218;
</p>
<p>&#218;
</p>
<p>&#218;
</p>
<p>&#218;
</p>
<p>&#218; &#219;
</p>
<p>f
</p>
<p>&#220;
</p>
<p>&#220;
</p>
<p>&#220;
</p>
<p>&#220;
</p>
<p>&#220; &#221;
</p>
<p>{
</p>
<p>h
</p>
<p>{ {
</p>
<p>&#209;
</p>
<p>&#209;
</p>
<p>&#209; &#210;
</p>
<p>h
</p>
<p>&#209;
</p>
<p>&#209;
</p>
<p>&#209; &#210;
</p>
<p>h
</p>
<p>&#220;
</p>
<p>&#220;
</p>
<p>&#220;
</p>
<p>&#220;
</p>
<p>&#220; &#221;
</p>
<p>&#218;
</p>
<p>&#218;
</p>
<p>&#218;
</p>
<p>&#218;
</p>
<p>&#218;
</p>
<p>&#218;
</p>
<p>&#218;
</p>
<p>&#218;
</p>
<p>&#218;
</p>
<p>&#218;
</p>
<p>&#218;
</p>
<p>&#218; &#219;
</p>
<p>&#228;
</p>
<p>&#228;
</p>
<p>&#228;
</p>
<p>&#228;
</p>
<p>&#228;&gt;&#229;
</p>
<p>&#205;
</p>
<p>&#205;
</p>
<p>&#205;&gt;&#206;
</p>
<p>Figure 6: D-tree of the string
f
</p>
<p>7
</p>
<p>h
</p>
<p>7
</p>
<p>{
</p>
<p>7
</p>
<p>generated by
&#2;T&#230;
</p>
<p>To conclude this section, we establish the following result.
</p>
<p>Theorem 5.2 The strict inclusion S
&#14;,&#14;U&#231;1&#183;
</p>
<p>&#193;&#147;S
</p>
<p>&#14;T&#14;
</p>
<p>holds.
</p>
<p>6 Conclusions
</p>
<p>We introduced a formal model for the generation of dependency trees, dealing with free word
order phenomena. The problem is currently under study (see (Holan et al., 1998), (Dikovsky,
2001), (Gramatovici &amp; Pla&#769;tek, 2003)) and is far to be solved.
The model of dependency contextual grammars we propose here is based on two main char-
acteristics: the intrinsic similitude between dependency trees and contextual grammars and the
flexibility of the latter in expressing non-local dependencies. Our approach is closer to the
intrinsic motivations of original dependency grammars than other current similar attempts are.
</p>
<p>The research of dependency contextual grammars just started since the model has to be further
tested on both mathematical and linguistical sides.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>R. Gramatovici, C. Mart&#305;&#769;n-Vide
</p>
<p>References
</p>
<p>Dikovsky A. (2001). Grammars for Local and Long Dependencies. Proceedings of the 39th
Annual Meeting of the ACL.
Dikovsky A. &amp; Modina L. (2000). Dependencies on the Other Side of the Curtain. In Traite-
ment Automatique des Langues (TAL), 41 (1), 79-111.
Gramatovici R. &amp; Pla&#769;tek M. (2003). On D-trivial Dependency Grammars. Submitted.
Holan T., Kubon&#780; V., Oliva K. &amp; Pla&#769;tek M. (1998). Two Useful Measures of Word Order Com-
plexity. In Proceedings of the Coling&#8217;98 Workshop &#8220;Processing of Dependency-Based Gram-
mars&#8221;, Polguere A. &amp; Kahane S. eds., University of Montreal, Montreal, 21-28.
</p>
<p>Joshi A. K. (1987). An Introduction to Tree Adjoining Grammars. In Mathematics of Lan-
guages, Manaster-Ramer A. ed., Amsterdam, Philadelphia: Johm Bejamins, 87-114.
Kappes M. (2000). Bracketed Contextual Grammars. Doctoral Dissertation, Johann Wolfgang
Goethe Universita&#776;t, Frankfurt am Main.
</p>
<p>Marcus S. (1967). Algebraic Linguistics. Analytical Models. New-York, London: Academic
Press.
</p>
<p>Marcus S. (1969). Contextual Grammars, Rev. Roum. Math. Pures Appl.. 14 (10), 69-74.
Marcus S. (1997). Contextual Grammars and Natural Languages. In The Handbook of Formal
Languages, Rozenberg G. &amp; Salomaa A. eds., Berlin, Heidelberg, New-York: Springer-Verlag,
vol. 2, 215-235.
</p>
<p>Marcus S., Mart&#305;&#769;n-Vide C. &amp; Pa&#774;un Gh. (1998). Contextual Grammars as Generative Models
of Natural Languages. Computational Linguistics, 24 (2), 245-274.
Mart&#305;&#769;n-Vide C. &amp; Pa&#774;un Gh. (1998). Structured Contextual Grammars. Grammars, 1 (1), 33-55.
Mart&#305;&#769;n-Vide C., Mateescu A., Miquel-Verges J. &amp; Pa&#774;un Gh. (1997). Internal contextual gram-
mars: minimal, maximal and scattered use of selectors. In Proceedings of The Fourth Bar-Ilan
Symposium on Foundations of Artificial Intelligence. Focusing on Natural Languages and Ar-
tificial Intelligence - Philosophical and Computational Aspects, Koppel M. &amp; Shamir E. eds.,
Menlo Park: AAAI Press, 159-168.
</p>
<p>Mel&#8217;c&#780;uk I. (1987). Dependency Syntax. Theory and Practice, Albany: State University of
New-York Press.
</p>
<p>Mra&#769;z F., Pla&#769;tek M. &amp; Procha&#769;zcha M. (2000). Restarting automata, deleting and Marcus gram-
mars. In Recent Topics in Mathematical and Computational Linguistics, Martin-Vide C. &amp;
Pa&#774;un Gh. eds., Bucharest: Romanian Academy Publishing House, 218-233.
</p>
<p>Pa&#774;un Gh. (1997). Marcus Contextual Grammars, Dordrecht, Boston, London: Kluwer.
Pa&#774;un Gh. &amp; Nguyen X. M. (1980). On the inner contextual grammars, Rev. Roum. Math. Pures
Appl., 25, 641-651.</p>

</div></div>
</body></html>