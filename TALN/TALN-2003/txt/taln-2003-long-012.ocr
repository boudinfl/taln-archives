TALN 2003, Batz-sur-Mer, 11—14juin 2003

Contextual Grammars and Dependency Trees *

Radu Gramatovici (1), Carlos Martin-Vide (2)

(1) Faculty of Mathematics and Computer Science, University of Bucharest
Academiei 14, 70109, Bucharest, Romania
Email: radu@funinf.cs.unibuc.ro

(2) Research Group on Mathematical Linguistics, Rovira i Virgili University
Pl. Imperial Tarraco 1, 43005 Tarragona, Spain
Email: cmv@astor.urv.es

Mots-clefs — Keywords

Grammaire contextuelle, arbre de dependance, arbre projectif de dépendance
Contextual grammar, dependency tree, projective dependency tree

Résumé - Abstract

On présente une nouvelle variante de grammaire contextuelle structurée, qui produit des arbres
de dépendance. Le nouveau modele génératif, appelé grammaire contextuelle de dépendance,
améliore la puissance générative forte et faible des grammaires contextuelles, tout en étant un
candidat potentiel pour la description mathématique des modéles syntactiques de dépendance.

A new variant of structured contextual grammar, which generates dependency trees, is intro-
duced. The new generative model, called dependency contextual grammar, improves both the
strong and weak generative power of contextual grammars, while being a potential candidate
for the mathematical description of dependency-based syntactic models.

This work was written during a research visit of the ﬁrst author at the Research Group on Mathematical
Linguistics of Rovira i Virgili University. The research visit was funded by the NATO Scientiﬁc Committee.

R. Gramatovici, C. Martin-Vide

1 Introduction

Contextual grammars were introduced in 1969 by Solomon Marcus (Marcus, 1969) as an at-
tempt to transform in generative devices some procedures developed within the framework of
analytical models (see (Marcus, 1997) for a comprehensive discussion on the linguistic mo-
tivations of contextual grammars). In many respects, (the mathematical model of) contextual
grammars and (the linguistic model of) dependency grammars ((Mel’éuk, 1987) and others)
have the same roots. The similitude between contextual grammars and dependency syntactic
models based on dependency trees go further, since both formalisms deal mostly with symbols
of the language and less or at all with auxiliary symbols as in the case of formalisms based on
constituents trees. A similar argument to this one is presented in (Mraz et al., 2000), where
contextual mechanism is described as dual to the analysis by reduction linguistic method.

However, no relationship was established yet between contextual and dependency grammars.
This happened probably because, originally, contextual grammars developed a string generative
mechanism and structures on strings generated by contextual grammars were introduced only
recently in (Martin-Vide & Paun, 1998).

In this paper, we propose a new approach to the generation of dependency trees (D-trees),
based on contextual grammars. The notion of internal dependency contextual grammar (DCG)
is introduced, by adding dependency descriptions to contextual rules from ordinary contextual
grammars. In a DCG, the derivation relation is constrained in two ways: contexts are selected
by (strings of) words, but also by the correct construction of the D-tree. Simplifying, string
insertion rules stand for the word order, while dependencies express mainly the dominance
relationship between words, as in unordered syntactic D-trees. Local and long distance depen-
dencies are treated in an uniform way, using the smooth contextual mechanism.

Several formal examples are given through out the paper, in order to emphasize the capability
of DCGs to describe different aspects of language generation. Concerning the strong generative
power, we illustrate in Section 4 the ﬂexibility of DCGs in describing even opposite (top-down
and bottom-up) derivation styles, as well as different kinds of dependencies (nested or cross-
serial), in a very simple and pragmatic way. In Section 5, we prove that the weak generative
power of projective internal DCGs (DCGs that generates only projective D-trees) is beyond the
generative power of internal contextual grammars with choice.

2 Marcus Contextual Grammars

Contextual grammars are based on the interplay between (strings of) words in a sentence. They
start from the assumption that there is a ﬁnite set of (simple) sentences (axioms), which are
accepted as well-formed without any doubt. More complex sentences are generated by the
insertion of other words, in the form of contexts (pairs of strings, possibly one-sided) selected
by the strings already present in the sentence.

Formally, an internal contextual grammar (CG) is a construct G = (V, A, C, qﬁ), where V is
an alphabet, A is a ﬁnite language over V (the set of axioms), C is a ﬁnite subset of V* X V*
(the set of contexts) and q5 : V* —> P(C) is the choice (or selection) recursive map. When
qS(a:) = C’, for all :13 E V*, we say that G is a CG without choice and write G = (V, A, C’). The

Contextual Grammars and Dependency Trees

internal derivation style (introduced in (Paun & Nguyen, 1980)) of a CG is deﬁned by:
:1: => 3; iff as = 3313:2333 and y = :1:1ua:2va:3 for some (11,, U) E q5(:1:2).

If =*> is the reﬂexive and transitive closure of =>, then L(G) = {a3 6 V* | Elw E A, w =*> 3:}
denotes the language generated by G. We denote by I G and I CO the classes of languages
generated by internal CGs without, respectively with choicel.

Example 2.1 Consider a very simple internal CG with choice G1 = (V, A, G, gb), where

V = {J ohn, likes, Lyn, really};
{John likes Lyn};
C = { (really, A) };

_ {(really, A)}, if :17 = likes,
(Mm) _ { (5, otherwise;

E»
II

The language generated by G1 is: John (really)* likes Lyn.

Contextual grammars. even without choice, are able to generate interesting languages, which
are spread into (not covering) the traditional Chomsky hierarchy of languages. We denote by
FIN, REG, CF, CS the classes of ﬁnite, regular, context-free, respectively context-sensitive
languages.

Example 2.2 Consider a CG without choice G2 = ({a, b}, {A}, {(a,  We have
L(G2) = {w | w 6 {a,b}*, |w|,, = |w|;, and |:1:|,, 2 |a:|;,,Va: 3 112}

where |w|,, denotes the number of occurrences of a in w and :1: 3 w denotes that w = my (:1: is a
preﬁx of 111). Then, L(G2) 6 CF \ REG. The language L(G2) is known as the Dyck language
over {a, b}.

Example 2.3 Consider a CG without choice G3 = ({a, b, c}, {A}, {(:1:y, 2:), (:1:,yz) | {as, y, z} =
{a, b,  We have

L(G3) = {w I w E {a,b,c}*= lwla = |'w|b = |w|c}-

Then, L(G3) 6 CS \ CF. The language L(G3) is known as the Bach language over {a, b, c}.

More expressive languages can be generated using the choice function together with a con-
dition on the type of the languages selecting contexts. A CG in the modular presentation
G = (V, A, (S1,G1),...,(S,,,G,,)) is a CG G = (V, A, G, gb) such that C, Q gb(:1:), for all
3: E S,-, 1 3 i 3 n. A pair (S,-, C,-) is called a contextual production. We say that a CG has
an F choice, where F denotes a family of languages, if S, E F, for all 1 3 i 3 n. We de-
note by ICC (F) the class of languages generated by internal CGs with F-choice. For example,
I G C (REG ) denotes the languages generated by internal CGs with regular selection languages.

1For details on contextual grammars, the reader is referred to the monograph (Paun, 1997).

R. Gramatovici, C. Martin-Vide

Example 2.4 Consider G4 = ({a, b, c, d}, {abcd}, (ab+c, {(a, c)}), (bc+d, {b,  a CG in the
modular presentation. We have

L(G4) = {a"b"‘c”d"‘ | n,m Z 1}.

Then, L(G4) 6 CS \ CF. L(G4) is the schematic representation of linguistic cross dependen-
cies.

Though, there are either simple context-free or linguistic relevant languages that cannot be
generated by any (internal) CG, even with choice.

Indeed, the language L1 = {a” | n 2 1} U {a”b” | n 2 1} cannot be generated with (internal)
CG. The problem comes from the fact that, in order to generate all the sentences of the form a”,
n 2 1 one needs to deﬁne a context (a, 11) containing only the letter a (at least one occurrence)
and selected by a set of strings also made only by as. Then, there is no mechanism to avoid
the insertion of the context (a, 1)) into sentences of the form a”b” (for an appropriate 71) and
producing in this way sentences of the form a"+ib”, with i > 0, which do not belong to the
language L1 (details of the proof are in (Paun, 1997), page 58). Thus, L1 6 CF \ I CC .

A similar problem occurs with the language L2 = {a"b” c” | n 2 0}, a non-context-free
language, which schematically represents the multiple (triple) agreement that can be found also
in some linguistic constructions. In (Paun, 1997), page 46, it is proved that L2 Q1 I CC using
a pumping lemma, which holds for internal CG. Constructively speaking, the problem comes
again from the fact that once one considers a context of the form (a, bc) to generate only strings
with an equal number of as, bs and cs, this context cannot be applied anywhere, but only between
as and bs (the left side of the context), respectively between bs and cs (its right side). The only
way to do this would be to associate with the context a selector of the form b”, n 2 1, but this is
not enough, because it would be possible to apply the context also to a shorter string of bs than
intended and generate strings of the form a”b"‘ab?’bcbTc”, with m + p + r = n, which do not
belong to L2.

In (Martin-Vide et al., 1997) (also (Marcus et al., 1998)), it is shown that languages as L1 or L2
can be generated by internal CGs with a condition of maximality on the length of the substrings
of the given string, which are used to select contexts at some derivation step. In Section 5, we
provide with a different method for the improvement of the (weak) generative power of CGs.

3 Dependency Contextual Grammars

A dependency tree (D-tree) is a tree whose nodes are labelled over an alphabet of terminal
symbols. Sometimes, its edges are also labelled over another alphabet (of syntactic categories),
but we will not use this feature here as it is not relevant for the purpose of this paper. We
will introduce D-trees using the concept of a structured string from (Marcus, 1967) (see also
(Martin-Vide & Paun, 1998), (Marcus et al., 1998)).

Let V be an alphabet. If n is a natural number, we denote by  the set of the ﬁrst n natural
numbers. A structured string over V is a pair (:13, p,,), where :1: E V‘‘‘ is a non-empty string
over V and p,, Q  X  \   | i E  is an anti-reﬂexive binary relation, called
dependency relation on 3:. If :1: is a string and i E  we denote by  the i-th symbol of
3:. If ipxj, then we say that :1:( j ) depends on  Let us denote by pi‘ (and call dominance

relation on 3:) the transitive closure of p$. If ipj j , then we say that  dominates :1:(j

Contextual Grammars and Dependency Trees

A structured string t = (.’L', pg) is called a D-tree iff the dependency relation pg induces a
structure of tree over as, i.e. i) there is 1 3 7* 3  such that a:(r) does not depend on any
symbol of as (r is called the root of 75); ii) for any 1' E  \ {7'}, there is a unique index j E 
such that  depends on a:(j); iii) pg‘ is an anti-reﬂexive relation, i.e. (i,  ¢ pi’, for any
1' E  We denote by A(V) the set of D-trees over a set V of terminals.

Linguistically motivated, the structures over CGs were introduced in (Martin-Vide & Paun,
1998) (see al(Marcus et al., 1998), or Section 7.6 in (Paun, 1997)). Bracketed CGs, i.e. contex-
tual grammars working on strings of terminal symbols enriched with a well-formed structure
of brackets, were extensively studied in (Kappes, 2000). We are interested here in the second
type of structured CGs introduced in (Martin-Vide & Paun, 1998), which do not use brackets
for creating tree-like structures on strings, but binary relations between terminal symbols. How-
ever, the deﬁnition of a dependency CG given below is formally different from the deﬁnition of
a structured CG given in (Martin-Vide & Paun, 1998).

We call G = (V, A, P1, . . . , Pn) an internal dependency contextual grammar (IDCG) iff V is
an alphabet, A Q A(V) is a ﬁnite set of D-trees over V (the axioms) and for any i E [77,],
Pi = (Si, cg-, dri) is a dependency contextual production, with Si Q V* (the set of selectors),
cg = (ug-,1),-) E V* X V* (the context) and dg Q  U V) X  U V)] \ V X V (the
set of new dependencies). A derivation in an IDCG is deﬁned as a binary relation over the set
of D-trees over V by:

(33, pg) =>d (y,pg) iff 3: = a:1:v2a:3,y = £lI1’U.’I32’U.’133 and there isi E [n], with 332 E S,-,

c,- = (u, v) and pg built under the following rules:

- pg contains all the dependencies in pg and no other
dependencies occur between the symbols originating in ac,

- pg contains all the dependencies between the symbols of cg,
described in di, and no other dependencies occur between the
symbols originating in cg,

- pg may contain dependencies between the symbols of III and
the new symbols introduced by cg as described by d,- i.e.
if a dependency occurs between ug-v,~(j) and a symbol a from .’L',
then (j, a) E dg (respectively (a, j) E d,-).

The difference between the above deﬁnition of a IDCG and the deﬁnition of a structured CG
from (Martin-Vide & Paun, 1998) consists in the fact that the new symbols inserted by some

dependency contextual rule do not attach to some speciﬁed (localized) selector symbols, but to
some selector symbols having the value speciﬁed in the set of new dependencies.

If =*>d is the reﬂexive and transitive closure of =>d, then DL(G) = {t E A(V) | Els E A, s =*>d
t} denotes the dendrolanguage (the set of D-trees) generated by G. Then, L(G) = {J: E V‘‘' |
E|(a:, pg) 6 DL(G)} denotes the language generated by G. We denote by I DOC the class of
languages generated by IDCGs.

Example 3.1 We reconsider Example 2.1, from the previous section, now introducing depen-
dencies on words. We deﬁne an internal DCG G’1 = (V, A, P), where

V = {John, likes, Lyn, really};
A = {(John likes Lyn,{(2, 1), (2,

R. Gramatovici, C. Martin-Vide

P = ({likes}, (really, A), {(likes, 
One of the D-trees generated by G’1 is:
(John really likes Lyn, {(3, 1), (3, 4), (3, 

(see Figure 1), which underlies the string: John really likes Lyn.

likes

//'\

John really Lyn

Figure 1: A D-tree for John really likes Lyn

Projectivity is a very common property of D-trees, which expresses the fact that the vertical
projections of the nodes do not intersect the edges of the tree. We will consider in the following
a particular case of DCG, which generates only projective D-trees.

Let :1? be a string. A sequence  . . .as(j), 1 g z’ 3 j 3 |:1:|, of consecutive symbols of .’L'
is called an interval of :13. Let (:5, pm) be a D-tree. The maximal projection of a symbol 
i E  of as, is the sequence  . . .  n E  of (not necessarily consecutive) symbols
ofas, suchthat ij < ik, for any 1 3 j < k 3 n, and {i1, . . . ,i,,} =  U {j | lp+j}.

Then, a D-tree is projective iff all the maximal projections of its symbols are intervals (see
(Dikovsky & Modina, 2000)). We denote by PA(V) the set of projective D-trees over an
alphabet V.

An IDCG G = (V, A, P1, . . . , P”) is called a projective internal dependency contextual gram-
mar (PIDCG) iff the axioms are projective D-trees, A Q PA(V), and the derivation relation,
=>,,d is deﬁned as the restriction of =>d on the set PA(V) of projective D-trees. Correspond-
ingly, the dendrolanguage generated by a PIDCG is deﬁned exactly as in the case of an IDCG,
but using the derivation relation =>,,d. We denote by PI DC 0 the class of languages generated
by PIDCGs.

The IDCG in Example 3.1 is a PIDCG.

4 Strong Generative Properties

After introducing structures on strings generated by CGs, one may speak about the strong gen-
erative power of the structured CGs. The strong generative power means the capacity of (depen-
dency, in our case) CGs to generate different dendrolanguages, underlying the same language.
In this section, we investigate some structural properties of IDCGs.

Top-down and bottom-up derivation

The derivation of phrases in a context-free grammar is a top-down process starting with the root
of the (constituent) tree and leading to its leaves. Among the novelties brought by tree-adj oining

Contextual Grammars and Dependency Trees

grammars (J oshi, 1987) and other formalisms in the area of the mild context sensitiveness, was
the fact that derivation is not necessarily a top-down process. It is important to remark the
capacity of IDCGs to construct the (same) structure of the (same) string in totally different
ways.

Example 4.1 Consider G5 = ({a, b}, {(ab, {(1, 2)})}, ({)\}, (a, b), {(a, 1), (1,  and G5 =
({a, b}, {(ab, {(1, 2)})}, (a+b+, (a, b), {(1, a), (1,  two IDCGs.

The D-tree represented in Figure 2 corresponding to the string a3b3 is generated in a top-down
manner (at any step, the axiom ab rests in the top, while the new context (a, b) is added at the
bottom of the tree) by G5 and in a bottom-up manner (the new context is added always in the

top) by G5.
(I
V 
Cl
 
b

b b

Figure 2: D-tree of the string a3b3 generated by G5 and G6

Nested and cross-serial dependencies

In Example 4.1, we deﬁned two IDCGs, which are rather ambiguous with respect to the struc-
tures associated with well-formed strings. For example, in both grammars we can associate
several D-trees with the string a3b3, and not only the D-tree in Figure 2. The D-tree from Figure
2 is a projective D-tree, which represents a set of nested dependencies between the symbols
a and b that were inserted in the string at the same derivation step. Indeed, one may notice
that the last b depends on the ﬁrst a, the second b depends on the second a, while the ﬁrst b
depends on the last (1. Another possible structure of a3b3, generated by the grammar G6, is the
non-proj ective D-tree in Figure 3, which encounters mixed dependencies.

Figure 3: D-tree of the phrase a3b3 generated by G5

In practice, one may want to control the distribution of dependencies over the string in a very
rigorous manner. The following example illustrates the capacity of PIDCGs to generate D-trees
with speciﬁc dependencies.

R. Gramatovici, C. Martin-Vide

Example 4.2 First, we modify the grammar G5 from Example 4.1 in order to obtain only D-
trees representing nested dependencies. Actually, we may keep the same grammar and consider
the projective derivation =>,,d. G5 becomes a PIDCG (since the axiom is a projective D-tree).
Then the D-tree represented in Figure 2 is the only D-tree associated with a3 b3 and generated by
G5 in the projective derivation style. Also the language generated by G6, in this case, is exactly
{a”b" | n 2 1}.

The second grammar generates the same language, {a”b” | n 2 1}, using a totally different
method, which hides cross-serial dependencies between the symbols a and b introduced at the
same derivation step. Consider G7 = {a,b}, {(ab, {(1, 2)})}, ({a“'}, (a, b), {(1, a), (b,  a
PIDCG. The (only) D-tree associated with the string a3b3 is represented in Figure 4.

Figure 4: D-tree of the string a3b3 generated by G7

5 Weak Generative Properties

In this section, we will prove that using PIDCGs also the weak generative capacity of CGs is
improved.

Theorem 5.1 For any internal contextual grammar G, there exists a weak equivalent internal
projective dependency contextual grammar G’.

Proof Let G = (V, A, (S1, C1), . . . , (Sn, Gn)) be an ICG. We may consider that each con-
textual production corresponds to one context. We construct the following PIDCG G’ =
(V, A’, (S1, C1, d1), . . . , (Sn, Gn, dn)), where

A’ = {(1171/’:I:) | 37 E Aapzc : {(’i77:+ 1) lie  _ 
dz‘ = {(0:1) He [|ui'Ui|]>a’ E V},

for c,- = (u,-,1),-) and for alli E  Then, L(G’) = L(G). III

From the above proof, it is worth to note that on the contextual side nothing changes when
adding dependencies: if G is without choice then G’ is also without choice; if G has an F-
choice then G’ has also an F-choice.

Now consider the languages discussed in the end of Section 2, L1 = {a" | n 2 1} U {a”b” |
n 2 1} and L2 = {a"b"c” | n 2 0}. These languages cannot be generated by any ICG, but
they can be generated by PIDCGs.

Contextual Grammars and Dependency Trees

Example 5.1 In order to obtain the ﬁrst language, let us consider the following PIDCG G8 =

({aab},{(a:0)a (aba{(1»2)})}a ({a}= (G,/\)a{(1»a)})a ({ab}a (a=b)a{(b»1)a (b=2)}))- We have

L(Gg) = L1. Figure 5 illustrates the unique D-tree and one of the possible D-trees generated
by G3 and associated with the strings a3, respectively a3b3.

Figure 5: D-trees of (13, respectively a3b3 generated by G3

Example 5.2 Let G9 = ({a,b,c}, {(abc,{(2, 1), (2,3)}, ({b+, (a,bc), {(a, 1), (b,2), (C,
be another PIDCG. We obtain the second language, L(G9) = L2. Figure 6 illustrates one
of the possible D-trees generated by G9 and associated with the string a3b3c3.

Figure 6: D-tree of the string a3b3c3 generated by G9

To conclude this section, we establish the following result.

Theorem 5.2 The strict inclusion ICC C PDICC holds.

6 Conclusions

We introduced a formal model for the generation of dependency trees, dealing with free word
order phenomena. The problem is currently under study (see (Holan et al., 1998), (Dikovsky,
2001), (Gramatovici & Platek, 2003)) and is far to be solved.

The model of dependency contextual grammars we propose here is based on two main char-
acteristics: the intrinsic similitude between dependency trees and contextual grammars and the
ﬂexibility of the latter in expressing non-local dependencies. Our approach is closer to the
intrinsic motivations of original dependency grammars than other current similar attempts are.

The research of dependency contextual grammars just started since the model has to be further
tested on both mathematical and linguistical sides.

R. Gramatovici, C. Martin-Vide

References

Dikovsky A. (2001). Grammars for Local and Long Dependencies. Proceedings of the 39th
Annual Meeting of the ACL.

Dikovsky A. & Modina L. (2000). Dependencies on the Other Side of the Curtain. In Traite-
ment Automatique des Langues (TAL), 41 (1), 79-111.

Gramatovici R. & Platek M. (2003). On D-trivial Dependency Grammars. Submitted.

Holan T., Kubon V., Oliva K. & Platek M. (1998). Two Useful Measures of Word Order Com-
plexity. In Proceedings of the Coling’98 Workshop “Processing of Dependency-Based Gram-
mars”, Polguere A. & Kahane S. eds., University of Montreal, Montreal, 21-28.

Joshi A. K. (1987). An Introduction to Tree Adjoining Grammars. In Mathematics of Lan-
guages, Manaster-Ramer A. ed., Amsterdam, Philadelphia: J ohm Bejamins, 87-114.

Kappes M. (2000). Bracketed Contextual Grammars. Doctoral Dissertation, Johann Wolfgang
Goethe Universitat, Frankfurt am Main.

Marcus S. (1967). Algebraic Linguistics. Analytical Models. New-York, London: Academic
Press.

Marcus S. (1969). Contextual Grammars, Rev. Roum. Math. Pures Appl.. 14 (10), 69-74.

Marcus S. (1997). Contextual Grammars and Natural Languages. In The Handbook of Formal
Languages, Rozenberg G. & Salomaa A. eds., Berlin, Heidelberg, New-York: Springer-Verlag,
vol. 2, 215-235.

Marcus S., Martin-Vide C. & Paun Gh. (1998). Contextual Grammars as Generative Models
of Natural Languages. Computational Linguistics, 24 (2), 245-274.

Martin-Vide C. & Paun Gh. (1998). Structured Contextual Grammars. Grammars, 1 (1), 33-55.

Martin-Vide C., Mateescu A., Miquel-Verges J . & Paun Gh. (1997). Internal contextual gram-
mars: minimal, maximal and scattered use of selectors. In Proceedings of The Fourth Bar-Ilan
Symposium on Foundations of Artiﬁcial Intelligence. Focusing on Natural Languages and Ar-
tiﬁcial Intelligence - Philosophical and Computational Aspects, Koppel M. & ShaInir E. eds.,
Menlo Park: AAAI Press, 159-168.

Mel’cuk I. (1987). Dependency Syntax. Theory and Practice, Albany: State University of
New-York Press.

Mraz F., Platek M. & Prochazcha M. (2000). Restarting automata, deleting and Marcus gram-
mars. In Recent Topics in Mathematical and Computational Linguistics, Martin-Vide C. &
Paun Gh. eds., Bucharest: Romanian Academy Publishing House, 218-233.

Paun Gh. (1997). Marcus Contextual Grammars, Dordrecht, Boston, London: Kluwer.

Paun Gh. & Nguyen X. M. (1980). On the inner contextual grammars, Rev. Roum. Math. Pures
Appl., 25, 641-651.

