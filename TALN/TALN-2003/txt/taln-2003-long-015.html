<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
"http://www.w3.org/TR/html4/loose.dtd">
<html><head><title>Vers la compr&#233;hension automatique de la parole : extraction de concepts par r&#233;seaux bay&#233;siens</title>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
</head>
<body>
<div style="page-break-before:always; page-break-after:always"><div><p>TALN 2003, Batz-sur-Mer, 11&#8211;14 juin 2003
</p>
<p>Vers la compr&#233;hension automatique de la parole : extraction
de concepts par r&#233;seaux bay&#233;siens
</p>
<p>Salma Jamoussi, Kamel Sma&#239;li et Jean-Paul Haton
LORIA/INRIA-Lorraine
</p>
<p>615 rue du Jardin Botanique, BP 101, F-54600 Villers-l&#232;s-Nancy, France
{jamoussi, smaili, jph}@loria.fr
</p>
<p>Mots-clefs &#8211; Keywords
</p>
<p>Compr&#233;hension de la parole, concepts s&#233;mantiques, r&#233;seaux bay&#233;siens, &#233;tiquetage s&#233;mantique,
cat&#233;gorisation automatique.
Speech understanding, semantic concepts, Bayesian networks, semantic labelling, automatic
categorization.
</p>
<p>R&#233;sum&#233; - Abstract
</p>
<p>La compr&#233;hension automatique de la parole peut &#234;tre consid&#233;r&#233;e comme un probl&#232;me d&#8217;as-
sociation entre deux langages diff&#233;rents. En entr&#233;e, la requ&#234;te exprim&#233;e en langage naturel et en
sortie, juste avant l&#8217;&#233;tape d&#8217;interpr&#233;tation, la m&#234;me requ&#234;te exprim&#233;e en terme de concepts. Un
concept repr&#233;sente un sens bien d&#233;termin&#233;. Il est d&#233;fini par un ensemble de mots partageant les
m&#234;mes propri&#233;t&#233;s s&#233;mantiques. Dans cet article, nous proposons une m&#233;thode &#224; base de r&#233;seau
bay&#233;sien pour l&#8217;extraction automatique des concepts ainsi que trois approches diff&#233;rentes pour
la repr&#233;sentation vectorielle des mots. Ces repr&#233;sentations aident un r&#233;seau bay&#233;sien &#224; regrouper
les mots, construisant ainsi la liste ad&#233;quate des concepts &#224; partir d&#8217;un corpus d&#8217;apprentissage.
Nous conclurons cet article par la description d&#8217;une &#233;tape de post-traitement au cours de laque-
lle, nous &#233;tiquetons nos requ&#234;tes et nous g&#233;n&#233;rons les commandes SQL appropri&#233;es validant
ainsi, notre approche de compr&#233;hension.
</p>
<p>The automatic speech understanding can be considered as association problem between two
different languages. At the entry, the request expressed in natural language and at the end,
just before the stage of interpretation, the same request is expressed in term of concepts. One
concept represents given meaning, it is defined by the set of words sharing the same semantic
properties. In this paper, we propose a new Bayesian network based method to automatically
extract the underlined concepts. We also propose three different approaches for the vector
representation of words. These representations help Bayesian network to build the adequate
list of concepts for the considered application. We finish this paper by description of the post-
processing step during which, we label our sentences and we generate the corresponding SQL
queries. This step allows us to validate our speech understanding approach.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Salma Jamoussi, Kamel Sma&#239;li et Jean-Paul Haton
</p>
<p>1 Introduction
</p>
<p>Dans la litt&#233;rature, plusieurs m&#233;thodes de compr&#233;hension de la parole ont &#233;t&#233; propos&#233;es. La
plupart de ces m&#233;thodes se fondent sur des approches stochastiques de d&#233;codage conceptuel qui
permettent d&#8217;approcher la compr&#233;hension automatique, r&#233;duisant ainsi le recours &#224; l&#8217;expertise
humaine. Cependant, ces m&#233;thodes n&#233;cessitent une &#233;tape d&#8217;apprentissage supervis&#233;, ce qui sig-
nifie qu&#8217;il y a une &#233;tape ant&#233;rieure d&#8217;annotation manuelle du corpus d&#8217;apprentissage (Maynard
&amp; Lef&#232;vre, 2002; Bousquet-Vernhettes &amp; Vigouroux, 2001; Pieraccini et al., 1993).
Dans de telles approches fond&#233;es sur le d&#233;codage conceptuel, l&#8217;&#233;tape d&#8217;annotation consiste &#224;
segmenter les donn&#233;es d&#8217;apprentissage en des segments conceptuels repr&#233;sentant chacun un
sens bien d&#233;termin&#233; (Bousquet-Vernhettes &amp; Vigouroux, 2001). Il s&#8217;agit donc de trouver tout
d&#8217;abord les diff&#233;rents concepts relatifs au corpus, de segmenter ensuite les phrases de ce cor-
pus, de les &#233;tiqueter en utilisant les concepts trouv&#233;s et de proc&#233;der enfin &#224; l&#8217;apprentissage
automatique. Faire tout ce travail d&#8217;une fa&#231;on manuelle constitue sans doute une phase fasti-
dieuse et co&#251;teuse. De plus, l&#8217;extraction manuelle est sujette &#224; la subjectivit&#233; et aux erreurs
humaines. Automatiser cette t&#226;che permettra donc de r&#233;duire ou d&#8217;annuler l&#8217;intervention hu-
maine et surtout de pouvoir r&#233;utiliser ce m&#234;me proc&#233;d&#233; lorsqu&#8217;on change de contexte (Siu &amp;
Meng, 1999).
Dans cet article, nous commen&#231;ons par d&#233;crire l&#8217;architecture g&#233;n&#233;rale de notre syst&#232;me de com-
pr&#233;hension de la parole, bas&#233;e sur l&#8217;approche propos&#233;e dans (Pieraccini et al., 1993). Ensuite,
nous pr&#233;sentons une nouvelle approche pour extraire automatiquement les concepts s&#233;man-
tiques. Pour ce faire, nous utilisons un r&#233;seau bay&#233;sien pour la classification non supervis&#233;e,
appel&#233; AutoClass. Puis, nous exposons trois m&#233;thodes diff&#233;rentes pour la repr&#233;sentation vecto-
rielle des mots afin de les regrouper pour former des concepts. Enfin, nous abordons la derni&#232;re
&#233;tape du processus de compr&#233;hension, au cours de laquelle nous &#233;tiquetons les requ&#234;tes et nous
g&#233;n&#233;rons les commandes SQL associ&#233;es.
</p>
<p>2 La compr&#233;hension automatique de la parole
</p>
<p>Le probl&#232;me de compr&#233;hension de la parole peut &#234;tre vu comme un probl&#232;me de mise en corre-
spondance entre une cha&#238;ne de mots en entr&#233;e et une suite de mots dans un langage plus restreint
v&#233;hiculant les id&#233;es principales d&#8217;une phrase. Il s&#8217;agit, dans un premier temps, d&#8217;associer les
mots de la phrase en entr&#233;e du syst&#232;me &#224; des messages dans un langage s&#233;mantique interm&#233;di-
aire (souvent appel&#233;s concepts). Dans un second temps, afin de satisfaire la requ&#234;te &#233;mise en
entr&#233;e, on traduit les concepts obtenus en actions ou r&#233;ponses et on parle dans ce cas de l&#8217;&#233;tape
d&#8217;interpr&#233;tation de la phrase.
</p>
<p>L&#8217;entr&#233;e du syst&#232;me peut &#234;tre donn&#233;e sous forme textuelle ou sous forme d&#8217;un signal de parole,
sa sortie exprim&#233;e en tant qu&#8217;actions ou commandes n&#8217;est qu&#8217;une conversion d&#8217;une liste de
concepts donn&#233;e par un module interm&#233;diaire de traduction s&#233;mantique et fournissant le sens
litt&#233;ral de la phrase. Un concept est une classe de mots traitant d&#8217;un m&#234;me sujet et partageant
des propri&#233;t&#233;s communes. Par exemple, les mots h&#244;tel, chambre, auberge et studio peuvent
tous correspondre au concept &#8220;h&#233;bergement&#8221; dans une application touristique. Dans (Pieraccini
et al., 1993), les auteurs d&#233;finissent un mod&#232;le g&#233;n&#233;ral pour la compr&#233;hension automatique de
la parole qui, en raison de sa simplicit&#233; et de son efficacit&#233;, a &#233;t&#233; repris dans plusieurs autres
travaux (Maynard &amp; Lef&#232;vre, 2002; Bousquet-Vernhettes &amp; Vigouroux, 2001). Nous avons</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Vers la compr&#233;hension automatique de la parole
</p>
<p>adopt&#233; la m&#234;me architecture g&#233;n&#233;rale mais nous proposons des techniques diff&#233;rentes au sein
de chacune de ses composantes. La figure 1 illustre l&#8217;architecture g&#233;n&#233;rale d&#8217;un tel syst&#232;me de
compr&#233;hension de la parole.
</p>
<p>repr&#233;sent&#233;
</p>
<p>CR
</p>
<p>Traducteur
</p>
<p>TS
</p>
<p>Convertisseur de
</p>
<p>sens
</p>
<p>concepts
appropri&#233;s
</p>
<p>Parole ou texte ACTION
Repr&#233;sentationS&#233;mantique
</p>
<p>par les 
</p>
<p>Figure 1: Architecture g&#233;n&#233;rale d&#8217;un syst&#232;me de compr&#233;hension automatique de la parole.
</p>
<p>Dans notre travail, nous nous int&#233;ressons &#224; une application de consultation de pages &#8220;Favoris&#8221;
(Bookmarks en anglais). Pour ce faire, nous utilisons un corpus du projet europ&#233;en MIAMM
dont l&#8217;objectif est de construire une plate-forme de dialogue oral multimodale. Le corpus con-
tient 71287 requ&#234;tes diff&#233;rentes exprim&#233;es en langue fran&#231;aise. Chaque requ&#234;te exprime une
mani&#232;re particuli&#232;re d&#8217;interroger la base. Des exemples de ces requ&#234;tes sont donn&#233;s dans la ta-
ble 1. Ces requ&#234;tes sont fournies au syst&#232;me de compr&#233;hension sous leur forme textuelle. Notre
but est de fournir &#224; la fin les requ&#234;tes SQL correspondantes qui, en les ex&#233;cutant, r&#233;pondront
aux demandes des utilisateurs.
</p>
<p>Montre-moi le contenu de mes favoris.
Je voudrais savoir si tu peux me prendre le contenu que j&#8217;aime.
Est-ce que tu veux me s&#233;lectionner les titres que je pr&#233;f&#232;re.
Est-il possible que tu me passes le premier de mes favoris.
Te serait-il possible de m&#8217;indiquer quelque chose de pareil.
Tu peux faire voir uniquement d&#233;cembre 2001.
Il faut que tu me pr&#233;sentes la liste que j&#8217;ai utilis&#233;e t&#244;t ce matin.
Je te demande de me passer les chansons que j&#8217;ai &#233;cout&#233;es ce matin.
Je souhaiterais que tu me montres les disques que j&#8217;ai regard&#233;s dans la matin&#233;e.
Je veux voir le deuxi&#232;me que j&#8217;ai regard&#233; dans la matin&#233;e.
</p>
<p>Table 1: Quelques exemples de requ&#234;tes du corpus MIAMM.
</p>
<p>3 Extraction des concepts : m&#233;thodes et r&#233;sultats
</p>
<p>Au cours de cette &#233;tape, nous cherchons &#224; identifier les concepts s&#233;mantiques li&#233;s &#224; notre appli-
cation. La d&#233;termination manuelle de ces concepts est une t&#226;che tr&#232;s lourde. Il nous faut donc
trouver une m&#233;thode automatique qui, pourrait ne pas donner des r&#233;sultats aussi performants que
ceux obtenus par la m&#233;thode manuelle, mais qui, en contre partie, permet une automatisation
compl&#232;te du processus de compr&#233;hension.
</p>
<p>Partant du principe d&#8217;automatisation de cette t&#226;che de cat&#233;gorisation, nous avons opt&#233; pour des
m&#233;thodes de classification non supervis&#233;e. Notre but final &#233;tant de trouver des concepts co-
h&#233;rents de l&#8217;application, le meilleur moyen d&#8217;y parvenir est de regrouper les mots en fonction
de leurs propri&#233;t&#233;s s&#233;mantiques. La m&#233;thode &#224; utiliser va donc regrouper les mots du corpus en</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Salma Jamoussi, Kamel Sma&#239;li et Jean-Paul Haton
</p>
<p>diff&#233;rentes classes, construisant ainsi les concepts de l&#8217;application. Dans ce cas, il nous reste
qu&#8217;&#224; affecter un nom de concept appropri&#233; &#224; chaque groupe de mots. Parmi les m&#233;thodes de
classification non supervis&#233;e, nous avons implant&#233; les cartes de Kohonen, les r&#233;seaux de neu-
rones de Oja et Sanger, la m&#233;thode des K-means et quelques m&#233;thodes fond&#233;es sur la mesure
d&#8217;information mutuelle moyenne entre les mots (Jamoussi et al., 2002). Les concepts obtenus
par ces m&#233;thodes &#233;taient bien significatifs, mais contenaient du &#8220;bruit&#8221;. Autrement dit, certains
mots n&#8217;avaient pas leurs places dans le sens exprim&#233; par ces concepts. Pour rem&#233;dier &#224; ce prob-
l&#232;me, nous avons explor&#233; d&#8217;autres m&#233;thodes et avons adopt&#233; les r&#233;seaux bay&#233;siens en raison de
leur fondement math&#233;matique fort et le m&#233;canisme d&#8217;inf&#233;rence puissant sous-jacent (Cheese-
man &amp; Stutz, 1996).
Dans la suite, nous pr&#233;sentons le principe de la th&#233;orie bay&#233;sienne sur laquelle se base l&#8217;outil
que nous avons utilis&#233; (AutoClass) et nous d&#233;taillons quelques &#233;tapes de calcul permettant de
trouver les concepts correspondants au corpus d&#8217;apprentissage utilis&#233;. Nous exposons aussi
trois approches diff&#233;rentes pour la repr&#233;sentation vectorielle des mots. Cette repr&#233;sentation,
qui doit &#234;tre s&#233;mantiquement significative, constitue une &#233;tape cl&#233; au sein du syst&#232;me de com-
pr&#233;hension puisqu&#8217;elle constitue l&#8217;entr&#233;e du r&#233;seau bay&#233;sien qui va d&#233;cider des groupements
des mots formant les concepts.
</p>
<p>3.1 Principe du r&#233;seau bay&#233;sien &#8220;AutoClass&#8221;
</p>
<p>AutoClass est un r&#233;seau bay&#233;sien pour la classification non supervis&#233;e qui accepte en entr&#233;e des
valeurs r&#233;elles, mais aussi des valeurs non num&#233;riques comme des mots, des caract&#232;res etc. En
r&#233;sultat, il fournit des probabilit&#233;s d&#8217;appartenance des &#233;l&#233;ments en entr&#233;e, aux classes trouv&#233;es.
Il suppose l&#8217;existence d&#8217;une variable multinomiale cach&#233;e qui peut repr&#233;senter les diff&#233;rentes
classes auxquelles appartiennent les &#233;l&#233;ments en entr&#233;e. AutoClass est bas&#233; sur le th&#233;or&#232;me de
Bayes exprim&#233; par :
</p>
<p>&#0;&#2;&#1;&#4;&#3;&#6;&#5;&#8;&#7;&#10;&#9;&#12;&#11;
</p>
<p>&#0;&#2;&#1;&#4;&#3;&#13;&#9;&#14;&#0;&#2;&#1;&#15;&#7;&#16;&#5;&#8;&#3;&#13;&#9;
</p>
<p>&#0;&#2;&#1;&#4;&#7;&#17;&#9;
</p>
<p>(1)
</p>
<p>Dans notre cas, &#7; repr&#233;sente les donn&#233;es et donc les mots &#224; classer et &#3; une hypoth&#232;se con-
cernant le nombre de classes et leurs descriptions en terme de probabilit&#233;s. Avec AutoClass, on
cherche &#224; maximiser la probabilit&#233; &#0;&#2;&#1;&#4;&#3;&#6;&#5;&#8;&#7;&#10;&#9; , c&#8217;est &#224; dire que sachant &#7; , les mots du corpus, on
doit s&#233;lectionner &#3; , l&#8217;ensemble des concepts, qui maximise cette probabilit&#233;.
</p>
<p>Dans notre r&#233;seau bay&#233;sien repr&#233;sent&#233; par la figure 2, un mot &#18;&#14;&#19; est donn&#233; sous la forme
d&#8217;un vecteur &#224; &#20; valeurs d&#8217;attributs, &#18;&#21;&#19;&#23;&#22;&#25;&#24;&#10;&#26;fiffffifl &#31;&quot;!#!$!%&#20;'&amp; . Un concept (*) est d&#233;crit, lui aussi,
par &#20; attributs, chacun est mod&#233;lis&#233; par une distribution gaussienne normale. +
</p>
<p>,&#23;-
</p>
<p>).&#22; est un
vecteur param&#232;tre d&#233;crivant le &#26; &#232;me attribut du / &#232;me concept (0) et il contient deux &#233;l&#233;ments,
la moyenne 12)3&#22; de la distribution consid&#233;r&#233;e et son &#233;cart-type 45)3&#22; . Pour l&#8217;ensemble du concept,
ce vecteur est not&#233;
</p>
<p>+
</p>
<p>,
</p>
<p>-
</p>
<p>) et il contient les
+
</p>
<p>,
</p>
<p>-
</p>
<p>).&#22; de tous ses attributs. La probabilit&#233; qu&#8217;un mot &#18;&#21;&#19;
appartienne au concept (6) , appel&#233;e la probabilit&#233; de classe et est not&#233;e 75) constitue aussi un
param&#232;tre descriptif du concept (6) .
</p>
<p>Ainsi nous avons d&#233;fini nos param&#232;tres de travail, les donn&#233;es &#7; sont bien les mots, repr&#233;sent&#233;s
par le vecteur
</p>
<p>+
</p>
<p>,
</p>
<p>&#18; &#224; 8 &#233;l&#233;ments englobant tous les &#18;&#21;&#19; . L&#8217;hypoth&#232;se &#3; correspondant &#224; la de-
scription des concepts est repr&#233;sent&#233;e par trois &#233;l&#233;ments, le nombre de concepts 9 et les deux
vecteurs
</p>
<p>+
</p>
<p>,
</p>
<p>7 et +
,
</p>
<p>-
</p>
<p>qui contiennent respectivement les 75) et les +
,
</p>
<p>-
</p>
<p>) de tous les concepts. Au-
toClass divise le probl&#232;me d&#8217;identification des concepts en deux parties : la d&#233;termination des</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Vers la compr&#233;hension automatique de la parole
</p>
<p>J
</p>
<p>I
1 2 Kr
</p>
<p>C
</p>
<p>x x x xi i i i
</p>
<p>j
</p>
<p>Figure 2: La structure g&#233;n&#233;rale du r&#233;seau bay&#233;sien AutoClass.
</p>
<p>param&#232;tres de classification (
+
</p>
<p>,
</p>
<p>7 et +
, -
</p>
<p>) pour un nombre donn&#233; de concepts et la d&#233;termination du
nombre de concepts 9 . Ce dernier probl&#232;me n&#233;cessite plusieurs approximations (pour les d&#233;-
tails, voir (Cheeseman &amp; Stutz, 1996)). Dans ce qui suit, &#3; repr&#233;sente seulement les vecteurs
+
</p>
<p>,
</p>
<p>7 et +
,:-
</p>
<p>. En rempla&#231;ant, dans l&#8217;&#233;quation 1, &#7; et &#3; par leurs valeurs, on obtient :
</p>
<p>&#0;&#2;&#1;
</p>
<p>+
</p>
<p>,&#23;-
</p>
<p>&#24;
</p>
<p>+
</p>
<p>,
</p>
<p>7
</p>
<p>&#5;
</p>
<p>+
</p>
<p>,
</p>
<p>&#18;
</p>
<p>&#9;*&#11;
</p>
<p>&#0;;&#1;
</p>
<p>+
</p>
<p>,&#23;-
</p>
<p>&#24;
</p>
<p>+
</p>
<p>,
</p>
<p>7
</p>
<p>&#9;&lt;&#0;&#2;&#1;
</p>
<p>+
</p>
<p>,
</p>
<p>&#18;
</p>
<p>&#5;
</p>
<p>+
</p>
<p>,&#23;-
</p>
<p>&#24;
</p>
<p>+
</p>
<p>,
</p>
<p>7
</p>
<p>&#9;
</p>
<p>&#0;&#2;&#1;
</p>
<p>+
</p>
<p>,
</p>
<p>&#18;
</p>
<p>&#9;
</p>
<p>(2)
</p>
<p>o&#249; &#0;&#2;&#1; +
,=-
</p>
<p>&#24;
</p>
<p>+
</p>
<p>,
</p>
<p>7
</p>
<p>&#9; est la probabilit&#233; a priori des param&#232;tres de classification, son calcul est bien d&#233;crit
dans (Cheeseman &amp; Stutz, 1996). La probabilit&#233; a priori des mots, &#0;;&#1;
</p>
<p>+
</p>
<p>,
</p>
<p>&#18;
</p>
<p>&#9; peut &#234;tre calcul&#233;e
directement. Elle est consid&#233;r&#233;e simplement comme une constante de normalisation. Dans ce
qui suit, on s&#8217;int&#233;resse au calcul de la probabilit&#233; &#0;&#2;&#1;
</p>
<p>+
</p>
<p>,
</p>
<p>&#18;
</p>
<p>&#5;
</p>
<p>+
</p>
<p>,=-
</p>
<p>&#24;
</p>
<p>+
</p>
<p>,
</p>
<p>7
</p>
<p>&#9; qui repr&#233;sente la fonction de
vraisemblance des donn&#233;es.
</p>
<p>On sait que
+
</p>
<p>,
</p>
<p>&#18; est un vecteur repr&#233;sentant tous les mots du corpus d&#8217;apprentissage, la vraisem-
blance de ce vecteur est donc calcul&#233;e comme &#233;tant le produit des probabilit&#233;s de tous les mots
s&#233;par&#233;ment comme le montre l&#8217;&#233;quation suivante :
</p>
<p>&#0;&#2;&#1;
</p>
<p>+
</p>
<p>,
</p>
<p>&#18;
</p>
<p>&#5;
</p>
<p>+
</p>
<p>,
</p>
<p>-
</p>
<p>&#24;
</p>
<p>+
</p>
<p>,
</p>
<p>7
</p>
<p>&#9;*&#11; &gt;
</p>
<p>?
</p>
<p>&#19;:@&#14;A
</p>
<p>&#0;&#2;&#1;
</p>
<p>&#18;B&#19;
</p>
<p>&#5;
</p>
<p>+
</p>
<p>,
</p>
<p>-
</p>
<p>&#24;
</p>
<p>+
</p>
<p>,
</p>
<p>7
</p>
<p>&#9; (3)
</p>
<p>&#0;&#2;&#1;
</p>
<p>&#18;B&#19;
</p>
<p>&#5;
</p>
<p>+
</p>
<p>,=-
</p>
<p>&#24;
</p>
<p>+
</p>
<p>,
</p>
<p>7
</p>
<p>&#9; est la probabilit&#233; d&#8217;observer le mot &#18;2&#19; ind&#233;pendamment du concept auquel il appar-
tient. Elle est donn&#233;e par la somme des probabilit&#233;s que ce mot appartienne &#224; chaque concept
s&#233;par&#233;ment, pond&#233;r&#233;e par les probabilit&#233;s des classes comme indiqu&#233; par l&#8217;&#233;quation suivante :
</p>
<p>&#0;&#2;&#1;
</p>
<p>&#18;B&#19;
</p>
<p>&#5;
</p>
<p>+
</p>
<p>,=-
</p>
<p>&#24;
</p>
<p>+
</p>
<p>,
</p>
<p>7C&#24;D9
</p>
<p>&#9;&#12;&#11; E
</p>
<p>F
</p>
<p>)G@&#14;A
</p>
<p>7H)
</p>
<p>&#0;&#2;&#1;
</p>
<p>&#18;B&#19;
</p>
<p>&#5;
</p>
<p>&#18;B&#19;&#2;ff&#13;(&#12;)I&#24;J+
</p>
<p>,&#23;-
</p>
<p>)
</p>
<p>&#9; (4)
</p>
<p>Puisque le mot &#18;2&#19; est d&#233;crit par un ensemble de &#20; attributs, avec la supposition, un peu forte,
que ces attributs sont ind&#233;pendants, la probabilit&#233; &#0;&#2;&#1; &#18;&#14;&#19; &#5; &#18;B&#19;&#2;ff&#13;(&#12;)I&#24;J+
</p>
<p>,&#23;-
</p>
<p>)
</p>
<p>&#9; peut s&#8217;&#233;crire donc, sous la
forme suivante :
</p>
<p>&#0;&#2;&#1;
</p>
<p>&#18;B&#19;
</p>
<p>&#5;
</p>
<p>&#18;B&#19;&#2;ff&#13;(&#12;)K&#24;J+
</p>
<p>,&#23;-
</p>
<p>)
</p>
<p>&#9;&#12;&#11; L
</p>
<p>?
</p>
<p>&#22;M@&#14;A
</p>
<p>&#0;&#2;&#1;
</p>
<p>&#18;B&#19;&#23;&#22;
</p>
<p>&#5;
</p>
<p>&#18;B&#19;NffO(&#12;)K&#24;D+
</p>
<p>,:-
</p>
<p>).&#22;
</p>
<p>&#9; (5)
</p>
<p>AutoClass mod&#233;lise les attributs &#224; valeurs r&#233;elles par une distribution gaussienne normale repr&#233;sen-
t&#233;e par le vecteur +
</p>
<p>,=-
</p>
<p>)3&#22; qui contient les deux param&#232;tres 12).&#22; et 4 ).&#22; . Dans ce cas, &#0;&#2;&#1; &#18;2&#19;&#23;&#22; &#5; &#18;B&#19;Pff
(&#12;)I&#24;J+
</p>
<p>,=-
</p>
<p>).&#22;
</p>
<p>&#9; qui correspond &#224; la distribution de classe peut s&#8217;&#233;crire sous la forme suivante :
</p>
<p>&#0;&#2;&#1;
</p>
<p>&#18;B&#19;&#23;&#22;
</p>
<p>&#5;
</p>
<p>&#18;B&#19;&#2;ff&#13;(&#12;)I&#24;Q1B)3&#22;&#25;&#24;G4 ).&#22;
</p>
<p>&#9;6&#11;
</p>
<p>&#31;
</p>
<p>R S
</p>
<p>7T4 ).&#22;VUXW5Y
</p>
<p>Z
</p>
<p>+
</p>
<p>&#31;
</p>
<p>SV[
</p>
<p>&#18;B&#19;=&#22;
</p>
<p>+
</p>
<p>1B).&#22;
</p>
<p>4 ).&#22; \&quot;]G^
</p>
<p>(6)
</p>
<p>Une fois, cette distribution de classe d&#233;termin&#233;e, il nous reste &#224; chercher l&#8217;ensemble des param&#232;tres
de concepts qui maximisent la probabilit&#233; de d&#233;part &#0;;&#1; +
</p>
<p>,
</p>
<p>-
</p>
<p>&#24;
</p>
<p>+
</p>
<p>,
</p>
<p>7
</p>
<p>&#5;
</p>
<p>+
</p>
<p>,
</p>
<p>&#18;
</p>
<p>&#9; et trouver ainsi l&#8217;ensemble des
concepts optimaux relatifs &#224; nos donn&#233;es.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Salma Jamoussi, Kamel Sma&#239;li et Jean-Paul Haton
</p>
<p>3.2 Repr&#233;sentation vectorielle des mots
</p>
<p>3.2.1 Contexte des mots
</p>
<p>Un mot peut avoir plusieurs caract&#233;ristiques possibles, mais rares sont celles qui peuvent lui
donner une repr&#233;sentation s&#233;mantique compl&#232;te. Dans une premi&#232;re &#233;tape, nous avons d&#233;cid&#233;
d&#8217;associer &#224; chaque mot ses diff&#233;rents contextes d&#8217;utilisation en &#233;mettant l&#8217;hypoth&#232;se que si
deux mots ont les m&#234;mes contextes alors ils sont s&#233;mantiquement proches. Dans cette approche,
un mot sera donc repr&#233;sent&#233; par un vecteur &#224;
</p>
<p>S`_&#6;a
</p>
<p>&#233;l&#233;ments contenant les
a
</p>
<p>mots de son
contexte gauche et les
</p>
<p>a
</p>
<p>mots de son contexte droit.
</p>
<p>La figure 3 pr&#233;sente un exemple de la repr&#233;sentation contextuelle des mots. Nous disposons
d&#8217;une phrase contenant b mots ; en fixant
</p>
<p>a
</p>
<p>&#11;
</p>
<p>S
</p>
<p>, nous repr&#233;sentons chaque mot c'&#19; par les
deux mots de ses contextes gauche et droit.
</p>
<p>1&lt;DEB&gt;&lt;DEB&gt;  W    W    W    W    &lt;FIN&gt;&lt;FIN&gt; 3 42
</p>
<p> 
</p>
<p> 
</p>
<p> 
</p>
<p> 
</p>
<p> 
</p>
<p>W
</p>
<p>W
</p>
<p>W
</p>
<p>1
</p>
<p>W 2
</p>
<p>3
</p>
<p>4
</p>
<p>&lt;DEB&gt;       W            W          W 
</p>
<p>&lt;DEB&gt; &lt;DEB&gt;         W          W3
</p>
<p>1
</p>
<p>2
</p>
<p>2
</p>
<p>2
</p>
<p>3
</p>
<p>1 3 4
</p>
<p>4 
</p>
<p>    W            W       &lt;FIN&gt;   &lt;FIN&gt;
</p>
<p>    W            W           W     &lt;FIN&gt;
</p>
<p>Figure 3: Un exemple de repr&#233;sentation de mots par leurs contextes (
a
</p>
<p>&#11;
</p>
<p>S
</p>
<p>).
</p>
<p>La valeur de
a
</p>
<p>a &#233;t&#233; fix&#233;e &#224;
S
</p>
<p>car les requ&#234;tes sont g&#233;n&#233;ralement courtes et concises. Par
cons&#233;quent, les contextes s&#233;mantiquement significatifs sont constitu&#233;s de peu de mots. Il faut
rappeler aussi que les mots outils de la langue comme : je, alors, le, mais, etc., ne sont pas
consid&#233;r&#233;s.
</p>
<p>Les classes que nous avons obtenues par cette m&#233;thode repr&#233;sentent bien des concepts s&#233;man-
tiques, mais ont l&#8217;inconv&#233;nient de se chevaucher. De plus, nous avons eu des difficult&#233;s &#224; con-
tr&#244;ler le nombre de concepts. Quelques exemples de ces concepts sont donn&#233;s dans la table 2.
</p>
<p>3.2.2 Similarit&#233; entre mots
</p>
<p>Pour trouver des concepts plus homog&#232;nes, nous avons chang&#233; compl&#232;tement la structure vec-
torielle de chaque mot. Nous avons utilis&#233; la mesure de l&#8217;information mutuelle moyenne qui
permet de trouver des similarit&#233;s contextuelles entre mots.
</p>
<p>Dans cette approche, nous associons &#224; chaque mot un vecteur &#224; d &#233;l&#233;ments, o&#249; d est la taille
du lexique. L&#8217;&#233;l&#233;ment num&#233;ro / de ce vecteur repr&#233;sente la valeur de l&#8217;information mutuelle
moyenne entre le mot num&#233;ro / du lexique et le mot &#224; repr&#233;senter comme indiqu&#233; dans la suite:
</p>
<p>c&#13;&#19;
</p>
<p>&#11;fe
</p>
<p>8
</p>
<p>&#1;&#4;g
</p>
<p>Aih
</p>
<p>g
</p>
<p>&#19;
</p>
<p>&#9;
</p>
<p>&#24;Q8
</p>
<p>&#1;&#4;g
</p>
<p>]
</p>
<p>h
</p>
<p>g
</p>
<p>&#19;
</p>
<p>&#9;
</p>
<p>&#24;J!J!J!j&#24;Q8
</p>
<p>&#1;&#4;g
</p>
<p>)kh
</p>
<p>g
</p>
<p>&#19;
</p>
<p>&#9;
</p>
<p>&#24;D!J!J!D&#24;G8
</p>
<p>&#1;lgnm
</p>
<p>h
</p>
<p>g
</p>
<p>&#19;
</p>
<p>&#9;po (7)
Ce vecteur exprime le degr&#233; de similarit&#233; du mot en question avec tous les autres mots du
corpus. La formule de l&#8217;information mutuelle moyenne (Rosenfeld, 1994) entre deux mots grq</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Vers la compr&#233;hension automatique de la parole
</p>
<p>Concept Groupe de mots
Favoris_1 Pr&#233;f&#233;r&#233;, favoris, pr&#233;f&#233;r&#233;s, choisi, appr&#233;ci&#233;, aim&#233;, ador&#233;
Favoris_2 Favoris, pr&#233;f&#233;r&#233;s, &#233;cout&#233;, vu, utilis&#233;, regard&#233;
Favoris_3 Favoris, pr&#233;f&#233;r&#233;s, choisi, appr&#233;ci&#233;, aim&#233;, ador&#233;, pr&#233;f&#233;r&#233;, similaire, semblable,
</p>
<p>pareil, &#233;quivalent, ressemblant, synonyme, proche, identique, rapproch&#233;
Demande_1 Possible, demande, veut, voudrais, aimerais, souhaite, souhaiterais,
</p>
<p>faut, d&#233;sire, d&#233;sirerais
Demande_2 Peux, pourrais, veux, voudrais, possible, cherche, demande, aimerais,
</p>
<p>souhaite, souhaiterais, faut, d&#233;sire, d&#233;sirerais, fais
Ordre Montrer, indiquer, s&#233;lectionner, trouver, donner, afficher, voir, presser,
</p>
<p>prendre, passer, chercher
</p>
<p>Table 2: Exemples de concepts obtenus avec la repr&#233;sentation bas&#233;e sur le contexte des mots.
</p>
<p>et gts est donn&#233;e par :
</p>
<p>8
</p>
<p>&#1;&#4;gtq
</p>
<p>h
</p>
<p>gtsu&#9;*&#11; vw&#1;lgtq
</p>
<p>&#24;
</p>
<p>gtsu&#9;5x$y&quot;z|{&lt;}&#127;~ &#128;D&#129; ~&#131;&#130;l&#132;
</p>
<p>{&lt;}&#127;~
</p>
<p>&#128;
</p>
<p>&#132;#{&lt;}%~H&#130;&#15;&#132;;&#133;
</p>
<p>vw&#1;lgtq
</p>
<p>&#24;
</p>
<p>g&#134;sp&#9;5x&#135;y&quot;z|{&lt;}%~ &#128;D&#129; ~&#131;&#130;&#136;&#132;
</p>
<p>{&lt;}%~
</p>
<p>&#128;
</p>
<p>&#132;:{&lt;} ~5&#130;&#136;&#132;.&#133;
</p>
<p>vw&#1; g&#134;q
</p>
<p>&#24;
</p>
<p>gtsp&#9;5x&#135;y&quot;z
</p>
<p>{&lt;} ~
</p>
<p>&#128;
</p>
<p>&#129; ~
</p>
<p>&#130;
</p>
<p>&#132;
</p>
<p>{&lt;} ~
</p>
<p>&#128;
</p>
<p>&#132;#{&lt;}%~
</p>
<p>&#130;
</p>
<p>&#132;
</p>
<p>&#133;
</p>
<p>v&#137;&#1; gkq
</p>
<p>&#24;
</p>
<p>gksu&#9;5x$y&quot;z
</p>
<p>{&lt;} ~
</p>
<p>&#128;
</p>
<p>&#129; ~
</p>
<p>&#130;
</p>
<p>&#132;
</p>
<p>{&lt;} ~
</p>
<p>&#128;
</p>
<p>&#132;:{&lt;} ~
</p>
<p>&#130;
</p>
<p>&#132;
</p>
<p>(8)
</p>
<p>o&#249; v&#137;&#1;&#4;gtq &#24; gtsu&#9; est la probabilit&#233; de trouver les deux mots gkq et gts dans la m&#234;me phrase, vw&#1;lgkq&#321;&#5;
gtsu&#9; est la probabilit&#233; de trouver le mot gnq sachant qu&#8217;on a d&#233;j&#224; rencontr&#233; le mot gks , v&#137;&#1;&#4;gtqj&#9; est
la probabilit&#233; de trouver le mot gnq et v&#137;&#1; gkqj&#9; est la probabilit&#233; de ne pas avoir rencontr&#233; le mot
gtq etc.
</p>
<p>En repr&#233;sentant ainsi chaque mot du corpus, et en utilisant le r&#233;seau bay&#233;sien, nous avons obtenu
des classes s&#233;mantiques homog&#232;nes, une classe &#233;tant constitu&#233;e de mots partageant les m&#234;mes
propri&#233;t&#233;s s&#233;mantiques. Le nombre de ces classes est tr&#232;s coh&#233;rent avec notre application. Cette
repr&#233;sentation nous a permis aussi de r&#233;soudre le probl&#232;me des chevauchements entre concepts.
Dans la table 3, nous donnons quelques exemples des concepts ainsi construits, o&#249; l&#8217;on remar-
que bien qu&#8217;il n&#8217;y a plus de chevauchement, mais qu&#8217;il existe encore quelques imperfections
comme dans le cas des concepts Demande_1, Demande_2 et Demande_3 qui auraient d&#251;s &#234;tre
regroup&#233;s ensemble.
</p>
<p>Concept Groupe de mots
Favoris Favoris, pr&#233;f&#233;r&#233;s, choisi, appr&#233;ci&#233;, ador&#233;, pr&#233;f&#233;r&#233;
Mode &#201;cout&#233;, vu, regard&#233;, utilis&#233;
Similarit&#233; Dernier, similaire, semblable, pareil, &#233;quivalent, ressemblant, synonyme,
</p>
<p>proche, identique, rapproch&#233;
Demande_1 Pourrais, veux, voudrais
Demande_2 Possible, aimerais, souhaiterais
Demande_3 Souhaite, faut, d&#233;sire, d&#233;sirerais
Ordre Montrer, indiquer, s&#233;lectionner, trouver, donner, afficher, pr&#233;senter, prendre,
</p>
<p>passer, chercher
</p>
<p>Table 3: Quelques exemples de concepts obtenus en utilisant la repr&#233;sentation bas&#233;e sur
l&#8217;information mutuelle moyenne entre les mots.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Salma Jamoussi, Kamel Sma&#239;li et Jean-Paul Haton
</p>
<p>3.2.3 Combinaison : contexte et similarit&#233;
</p>
<p>Nous avons combin&#233; les deux approches pr&#233;c&#233;dentes en vue d&#8217;am&#233;liorer les r&#233;sultats. En ef-
fet, la premi&#232;re approche travaillant au niveau occurrence, exploite directement les informa-
tions li&#233;es au contexte d&#8217;utilisation des mots, tandis que la deuxi&#232;me, utilise une mesure pour
chercher des similarit&#233;s entre deux mots. On peut ais&#233;ment comprendre que les informations
utilis&#233;es au niveau de ces deux m&#233;thodes sont diff&#233;rentes et compl&#233;mentaires.
</p>
<p>Combiner ces deux m&#233;thodes, consiste donc &#224; repr&#233;senter chaque mot par une matrice d&#8217;infor-
mation mutuelle moyenne &#224; dimension d
</p>
<p>_&#140;&#139;
</p>
<p>. La premi&#232;re colonne correspond au vecteur
d&#8217;information mutuelle moyenne pr&#233;c&#233;dent (voir section 3.2.2), la deuxi&#232;me colonne repr&#233;sente
l&#8217;information mutuelle moyenne entre un mot quelconque du vocabulaire et le contexte gauche
du mot &#224; repr&#233;senter. Idem pour la troisi&#232;me colonne mais concernant le contexte droit.
</p>
<p>La / &#232;me valeur de la deuxi&#232;me colonne est la moyenne pond&#233;r&#233;e des informations mutuelles
moyennes entre le / &#232;me mot du vocabulaire et le vecteur constituant le contexte gauche du mot
c&#13;&#19; en question. Elle est calcul&#233;e comme suit :
</p>
<p>8&#131;d&#141;d`)
</p>
<p>&#1;
</p>
<p>(
</p>
<p>&#19;
</p>
<p>&#142;
</p>
<p>&#9;*&#11;&#144;&#143;
</p>
<p>~ &#145;X&#146;&#148;&#147;l&#149;&#136;&#150;D&#151;:&#152;&#4;&#153;M&#151;:&#152;
</p>
<p>&#142;
</p>
<p>qu&#154;
</p>
<p>&#147;&#4;&#155;X&#152;H&#156;u&#152;5&#157;*&#158;
</p>
<p>8
</p>
<p>&#1;lg
</p>
<p>)&#159;h
</p>
<p>g
</p>
<p>&#142;
</p>
<p>&#9;
</p>
<p>_
</p>
<p>&#20;
</p>
<p>~
</p>
<p>&#142;
</p>
<p>a`&#160;
</p>
<p>_ &#161;&#25;&#162;&#163;&#162;
</p>
<p>(9)
</p>
<p>O&#249; 85d&#164;d`) &#1; ( &#142; &#9; repr&#233;sente l&#8217;information mutuelle moyenne entre le mot g ) du lexique et le
contexte gauche du mot c&#165;&#19; . 8 &#1;lg )&#166;h g &#142; &#9; repr&#233;sente l&#8217;information mutuelle moyenne entre le
mot num&#233;ro / du lexique et le mot g &#142; qui appartient au contexte gauche du mot c&#16;&#19; . &#20;
</p>
<p>~
</p>
<p>&#142; est
le nombre de fois o&#249; le mot g &#142; est trouv&#233; comme contexte gauche du mot c&#16;&#19; et
</p>
<p>a&#167;&#160;
</p>
<p>_ &#161;&#25;&#162;&#163;&#162; est le
nombre total d&#8217;occurrence du mot c&#16;&#19; dans le corpus. Le mot c&#13;&#19; sera donc repr&#233;sent&#233; par une
matrice comme le montre la figure 4.
</p>
<p>1 iI(w  : w  ) 1IMM  (Cd)
i
</p>
<p>2IMM  (Cd)
i
</p>
<p>IMM  (Cd)ij
</p>
<p>MIMM  (Cd)
i
</p>
<p>IMM  (Cg)1
i
</p>
<p>IMM  (Cg)2
i
</p>
<p>IMM  (Cg)j i
</p>
<p>MIMM  (Cg)
i
</p>
<p>W i =
</p>
<p>i
</p>
<p>i
</p>
<p>I(w  : w  )2
</p>
<p>j
</p>
<p>MI(w   : w  )
</p>
<p>I(w  : w  )
</p>
<p>i
</p>
<p>Figure 4: Repr&#233;sentation du mot c&#165;&#19; par la m&#233;thode combin&#233;e.
</p>
<p>La matrice utilis&#233;e pour repr&#233;senter un mot du corpus exploite un maximum d&#8217;informations sur
ce mot. Elle consid&#232;re son contexte ainsi que sa similarit&#233; avec tous les autres mots du lexique.
Une telle repr&#233;sentation des mots a pu aid&#233; le r&#233;seau bay&#233;sien dans sa t&#226;che de classification
et nous a permis d&#8217;am&#233;liorer consid&#233;rablement nos r&#233;sultats. Nous obtenons alors une liste de
concepts bien coh&#233;rente qu&#8217;on va utiliser dans la suite de nos traitements. Des exemples de ces
r&#233;sultats sont donn&#233;s au niveau de la table 4.
</p>
<p>4 &#201;tiquetage et post-traitement
La derni&#232;re &#233;tape consiste &#224; fournir les commandes SQL associ&#233;es aux requ&#234;tes textuelles
&#233;mises en entr&#233;e. C&#8217;est au cours de cette phase que nous entamons l&#8217;&#233;tape d&#8217;interpr&#233;tation</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Vers la compr&#233;hension automatique de la parole
</p>
<p>Concept Groupe de mots
Favoris Favoris, pr&#233;f&#233;r&#233;s, choisi, appr&#233;ci&#233;, ador&#233;, aim&#233;
Mode &#201;cout&#233;, vu, regard&#233;, utilis&#233;
Similarit&#233; Similaire, semblable, pareil, &#233;quivalent, ressemblant, synonyme,
</p>
<p>proche, identique, rapproch&#233;
Demande Souhaite, faut, d&#233;sire, d&#233;sirerais, peux, pourrais, veux, voudrais,
</p>
<p>possible, aimerais, souhaiterais
Ordre Montrer, indiquer, s&#233;lectionner, trouver, donner, afficher, pr&#233;senter,
</p>
<p>prendre, passer, chercher
</p>
<p>Table 4: Quelques exemples de concepts obtenus en utilisant l&#8217;approche combin&#233;e.
</p>
<p>des requ&#234;tes. En effet, disposant de l&#8217;ensemble des concepts qui r&#233;gissent notre application,
nous pouvons attribuer &#224; chaque requ&#234;te les concepts appropri&#233;s. Il s&#8217;agit de l&#8217;&#233;tape de &#8220;Tra-
duction s&#233;mantique&#8221;, la premi&#232;re composante de l&#8217;architecture g&#233;n&#233;rale de notre syst&#232;me de
compr&#233;hension (voir la figure 1). Pour ce faire, il nous suffit d&#8217;&#233;tiqueter nos phrases en asso-
ciant &#224; chaque mot dans la phrase sa classe s&#233;mantique correspondante. Puisque nos concepts ne
se chevauchent pas entre eux, &#233;tiqueter ainsi les requ&#234;tes ne pr&#233;sente aucun risque d&#8217;ambigu&#239;t&#233;.
</p>
<p>Ensuite, nous pouvons passer &#224; la deuxi&#232;me composante de notre mod&#232;le, le &#8220;Convertisseur de
repr&#233;sentation&#8221;, o&#249; il s&#8217;agit de convertir les concepts trouv&#233;s en commandes SQL permettant
d&#8217;extraire l&#8217;information requise de notre base de donn&#233;es. Pour ce faire, nous avons r&#233;alis&#233;
un moteur d&#8217;inf&#233;rence qui &#224; chaque concept, fait correspondre une ou plusieurs sous-requ&#234;tes
g&#233;n&#233;riques. Dans une requ&#234;te SQL g&#233;n&#233;rique, les concepts interviennent au niveau des condi-
tions. Ainsi, par exemple, si nous trouvons le concept &#8220;Date&#8221;, nous ne connaissons pas la valeur
de cette date mais, nous pouvons indiquer dans la requ&#234;te g&#233;n&#233;r&#233;e qu&#8217;il y a une condition sur la
date. Ce moteur d&#8217;inf&#233;rence prend en compte bien s&#251;r les r&#233;p&#233;titions, les oublis, les demandes
multiples et implicites ainsi d&#8217;autres ph&#233;nom&#232;nes de la parole spontan&#233;e. Dans la phase suiv-
ante, nous instancions chaque concept, dans la requ&#234;te g&#233;n&#233;rique obtenue, par sa valeur qui est
d&#233;duite en revenant &#224; la phrase initiale. Ainsi, nous obtenons une vraie commande SQL que
nous pouvons ex&#233;cuter pour extraire les pages recherch&#233;es. Dans la figure 5, nous donnons un
exemple illustrant les diff&#233;rentes &#233;tapes suivies afin d&#8217;aboutir &#224; une commande SQL finalis&#233;e.
Les r&#233;sultats obtenus sont encourageants, en effet, en terme de requ&#234;tes SQL correctes, nous
obtenons un taux de &#31;I&#168;&quot;&#168;&#170;&#169; avec le corpus d&#8217;apprentissage et un taux de &#171;
</p>
<p>S
</p>
<p>!&#127;&#172;&quot;&#169; avec un corpus
de test contenant b&#170;&#168;&quot;&#168; phrases diff&#233;rentes.
</p>
<p>5 Conclusion
</p>
<p>Dans cet article, nous sommes partis du principe que le probl&#232;me de la compr&#233;hension automa-
tique est un probl&#232;me d&#8217;association entre deux langages diff&#233;rents, le langage naturel et le lan-
gage des concepts. Les concepts sont des entit&#233;s s&#233;mantiques regroupant un ensemble de mots
qui partagent les m&#234;mes propri&#233;t&#233;s s&#233;mantiques et qui expriment une certaine id&#233;e. Nous avons
propos&#233; trois m&#233;thodes diff&#233;rentes pour l&#8217;extraction automatique des concepts, ainsi qu&#8217;une
approche d&#8217;&#233;tiquetage et de g&#233;n&#233;ration automatique des requ&#234;tes SQL correspondantes aux de-
mandes des utilisateurs.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Salma Jamoussi, Kamel Sma&#239;li et Jean-Paul Haton
</p>
<p>Montre moi la liste de mes pr&#233;f&#233;r&#233;s que j&#8217;ai consult&#233;e avant d&#233;cembre 2001
Identification des 
concepts
</p>
<p>Ordre, Objet, Favoris, Date
</p>
<p>select Objet from table_favoris where condition_date ;
</p>
<p>G&#233;n&#233;ration d&#8217;une 
requete g&#233;n&#233;rique^
</p>
<p>G&#233;n&#233;ration d&#8217;une
requete SQL^ 
</p>
<p>select * from favoris where date &lt; #01/12/2001# ;
</p>
<p>Figure 5: Cha&#238;ne de traitement appliqu&#233;e &#224; une requ&#234;te en langage naturel.
</p>
<p>Les t&#226;ches d&#8217;extraction de concepts et d&#8217;&#233;tiquetage sont d&#8217;habitude r&#233;alis&#233;es manuellement.
Elles constituent la phase la plus d&#233;licate et la plus co&#251;teuse dans le processus de compr&#233;hen-
sion. Les m&#233;thodes propos&#233;es dans cet article ont permis d&#8217;&#233;viter ce recours &#224; l&#8217;expertise hu-
maine et peuvent servir &#224; plusieurs autres domaines qui touchent &#224; la classification s&#233;mantique,
comme les domaines de cat&#233;gorisation de texte, d&#8217;extraction d&#8217;information et de fouille de don-
n&#233;es. La m&#233;thode combin&#233;e a donn&#233; les meilleurs r&#233;sultats gr&#226;ce aux informations qu&#8217;elle a
su exploit&#233;es pour repr&#233;senter au mieux un mot du corpus. Pour notre application de consulta-
tion de pages favorites, les concepts qu&#8217;elle a trouv&#233;s sont tr&#232;s satisfaisants. Il nous ont permis
ensuite, de mener &#224; bien l&#8217;&#233;tape d&#8217;&#233;tiquetage sans rencontrer des difficult&#233;s notables.
</p>
<p>Nous envisageons d&#8217;&#233;tendre le module de post-traitement de fa&#231;on &#224; ce qu&#8217;il puisse r&#233;agir face
&#224; de nouveaux mots cl&#233;s non pris en compte par les concepts. Pour ce faire, il faut adapter notre
mod&#232;le &#224; la phase d&#8217;exploitation pour que nous puissions ajouter des mots aux concepts. Nous
souhaitons aussi int&#233;grer notre module de compr&#233;hension dans un syst&#232;me de reconnaissance
automatique de la parole afin de r&#233;aliser une application interactive exploitable.
</p>
<p>R&#233;f&#233;rences
BOUSQUET-VERNHETTES C. &amp; VIGOUROUX N. (2001). Context use to improve the speech under-
standing processing. In International Workshop on Speech and Computer, SPECOM&#8217;01, Moscow.
CHEESEMAN P. &amp; STUTZ J. (1996). Bayesian classification (autoclass): Theory and results. In Ad-
vances in Knowledge Discovery and Data Mining. U. Fayyad, G. Shapiro, P. Smyth, R. Uthurusamy.
JAMOUSSI S., SMA&#207;LI K. &amp; HATON J. (2002). Neural network and information theory in speech
understanding. In International Workshop on Speech and Computer, SPECOM&#8217;02, St. Petersbourg.
MAYNARD H. &amp; LEF&#200;VRE F. (2002). Apprentissage d&#8217;un module stochastique de compr&#233;hension de la
parole. In 24&#232;mes Journ&#233;es d&#8217;&#201;tude sur la parole, Nancy.
PIERACCINI R., LEVIN E. &amp; VIDAL E. (1993). Learning how to understand language. In Proceedings
of the 4rd European Conference on Speech Communication and Technology, Berlin.
ROSENFELD R. (1994). Adaptive Statistical Language Modeling: A Maximum Entropy Approach. PhD
thesis, School of Computer Science Carnegie Mellon University, Pittsburgh, PA 15213.
SIU K.-C. &amp; MENG H. M. (1999). Semi-automatic acquisition of domain-specific semantic structures.
In Proceedings of the 6th European Conference on Speech Communication and Technology, Budapest.</p>

</div></div>
</body></html>