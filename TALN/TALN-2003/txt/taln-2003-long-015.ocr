TALN 2003, Batz-sur-Mer, 11—14juin 2003

Vers la compréhension automatique de la parole : extraction
de concepts par réseaux bayésiens

Salma J amoussi, Kamel Sma'1'li et J ean-Paul Haton
LORIA/INRIA-Lorraine
615 rue du J ardin Botanique, BP 101, F-54600 Villers-les-Nancy, France
{jamoussi, smaili, jph} @loria.fr

Mots-clefs — Keywords

Comprehension de la parole, concepts semantiques, reseaux bayesiens, etiquetage semantique,
categorisation automatique.

Speech understanding, semantic concepts, Bayesian networks, semantic labelling, automatic
categorization.

Résumé - Abstract

La comprehension automatique de la parole peut étre consideree comme un probleme d’as-
sociation entre deux langages differents. En entree, la requéte exprimee en langage naturel et en
sortie, juste avant l’etape d’interprétation, la meme requéte exprimee en terme de concepts. Un
concept represente un sens bien determine. Il est deﬁni par un ensemble de mots partageant les
memes proprietes semantiques. Dans cet article, nous proposons une methode a base de reseau
bayesien pour l’extraction automatique des concepts ainsi que trois approches differentes pour
la representation vectorielle des mots. Ces representations aident un reseau bayesien a regrouper
les mots, construisant ainsi la liste adequate des concepts a partir d’un corpus d’apprentissage.
Nous conclurons cet article par la description d’une etape de post-traitement au cours de laque-
lle, nous etiquetons nos requétes et nous generons les commandes SQL appropriees validant
ainsi, notre approche de comprehension.

The automatic speech understanding can be considered as association problem between two
different languages. At the entry, the request expressed in natural language and at the end,
just before the stage of interpretation, the same request is expressed in term of concepts. One
concept represents given meaning, it is deﬁned by the set of words sharing the same semantic
properties. In this paper, we propose a new Bayesian network based method to automatically
extract the underlined concepts. We also propose three different approaches for the vector
representation of words. These representations help Bayesian network to build the adequate
list of concepts for the considered application. We ﬁnish this paper by description of the post-
processing step during which, we label our sentences and we generate the corresponding SQL
queries. This step allows us to validate our speech understanding approach.

Salma J amoussi, Kamel Sma'1'li et J ean-Paul Haton

1 Introduction

Dans la litterature, plusieurs methodes de comprehension de la parole ont ete proposees. La
plupart de ces methodes se fondent sur des approches stochastiques de decodage conceptuel qui
permettent d’approcher la comprehension automatique, reduisant ainsi le recours a l’expertise
humaine. Cependant, ces methodes necessitent une etape d’apprentissage supervise, ce qui sig-
niﬁe qu’il y a une etape anterieure d’annotation manuelle du corpus d’apprentissage (Maynard
& Lefevre, 2002; Bousquet-Vemhettes & Vigouroux, 2001; Pieraccini et al., 1993).

Dans de telles approches fondees sur le decodage conceptuel, l’etape d’annotation consiste a
segmenter les donnees d’apprentissage en des segments conceptuels representant chacun un
sens bien determine (Bousquet-Vemhettes & Vigouroux, 2001). Il s’agit donc de trouver tout
d’abord les differents concepts relatifs au corpus, de segmenter ensuite les phrases de ce cor-
pus, de les etiqueter en utilisant les concepts trouves et de proceder enﬁn a l’apprentissage
automatique. Faire tout ce travail d’une fagon manuelle constitue sans doute une phase fasti-
dieuse et coﬁteuse. De plus, l’eXtraction manuelle est sujette a la subjectivite et aux erreurs
humaines. Automatiser cette tache permettra donc de reduire ou d’annuler l’intervention hu-
maine et surtout de pouvoir reutiliser ce meme procede lorsqu’on change de contexte (Siu &
Meng, 1999).

Dans cet article, nous commengons par decrire l’architecture generale de notre systeme de com-
prehension de la parole, basee sur l’approche proposee dans (Pieraccini et al., 1993). Ensuite,
nous presentons une nouvelle approche pour extraire automatiquement les concepts seman-
tiques. Pour ce faire, nous utilisons un reseau bayesien pour la classiﬁcation non supervisee,
appele AutoClass. Puis, nous exposons trois methodes differentes pour la representation vecto-
rielle des mots aﬁn de les regrouper pour former des concepts. Enﬁn, nous abordons la demiere
etape du processus de comprehension, au cours de laquelle nous etiquetons les requétes et nous
generons les commandes SQL associees.

2 La compréhension automatique de la parole

Le probleme de comprehension de la parole peut etre vu comme un probleme de mise en corre-
spondance entre une chaine de mots en entree et une suite de mots dans un langage plus restreint
vehiculant les idees principales d’une phrase. Il s’agit, dans un premier temps, d’associer les
mots de la phrase en entree du systeme a des messages dans un langage semantique intermedi-
aire (souvent appeles concepts). Dans un second temps, aﬁn de satisfaire la requete emise en
entree, on traduit les concepts obtenus en actions ou reponses et on parle dans ce cas de l’etape
d’interpretation de la phrase.

L’ entree du systeme peut etre donnee sous forme textuelle ou sous forme d’un signal de parole,
sa sortie exprimee en tant qu’actions ou commandes n’est qu’une conversion d’une liste de
concepts donnee par un module intermediaire de traduction semantique et fournissant le sens
litteral de la phrase. Un concept est une classe de mots traitant d’un meme sujet et partageant
des proprietes communes. Par exemple, les mots hotel, chambre, auberge et studio peuvent
tous correspondre au concept “he’bergement” dans une application touristique. Dans (Pieraccini
et al., 1993), les auteurs deﬁnissent un modele general pour la comprehension automatique de
la parole qui, en raison de sa simplicite et de son efﬁcacite, a ete repris dans plusieurs autres
travaux (Maynard & Lefevre, 2002; Bousquet-Vemhettes & Vigouroux, 2001). Nous avons

Vers la comprehension automatique de la parole

adopte la meme architecture generale mais nous proposons des techniques differentes au sein
de chacune de ses composantes. La ﬁgure 1 illustre l’architecture generale d’un tel systeme de
comprehension de la parole.

sens
represente
Parole ou texte Traducteur par les Convertisseur de ACTION

Semantique Representation
TS concepts CR

appropries

         
 
   

 

Figure 1: Architecture generale d’un systeme de comprehension automatique de la parole.

Dans notre travail, nous nous interessons a une application de consultation de pages “Favoris”
(Bookmarks en anglais). Pour ce faire, nous utilisons un corpus du projet europeen MIAMM
dont l’objectif est de construire une plate-forme de dialogue oral multimodale. Le corpus con-
tient 71287 requétes differentes exprimees en langue francaise. Chaque requete exprime une
maniere particuliere d’interroger la base. Des exemples de ces requétes sont donnes dans la ta-
ble 1. Ces requetes sont foumies au systeme de comprehension sous leur forme textuelle. Notre
but est de foumir a la ﬁn les requetes SQL correspondantes qui, en les executant, repondront
aux demandes des utilisateurs.

Montre-moi le contenu de mes favoris.

J e voudrais savoir si tu peux me prendre le contenu que j’aime.

Est-ce que tu veux me selectionner les titres que je prefere.

Est-il possible que tu me passes le premier de mes favoris.

Te serait-il possible de m’indiquer quelque chose de pareil.

Tu peux faire voir uniquement decembre 2001.

Il faut que tu me presentes la liste que j’ai utilisee tot ce matin.

J e te demande de me passer les chansons que j’ai ecoutees ce matin.

J e souhaiterais que tu me montres les disques que j’ai regardes dans la matinee.
J e veux voir le deuxieme que j’ai regarde dans la matinee.

Table 1: Quelques exemples de requetes du corpus MIAMM.

3 Extraction des concepts : méthodes et resultats

Au cours de cette etape, nous cherchons a identiﬁer les concepts semantiques lies a notre appli-
cation. La determination manuelle de ces concepts est une tache tres lourde. Il nous faut donc
trouver une methode automatique qui, pourrait ne pas donner des resultats aussi performants que
ceux obtenus par la methode manuelle, mais qui, en contre partie, permet une automatisation
complete du processus de comprehension.

Partant du principe d’automatisation de cette tache de categorisation, nous avons opte pour des
methodes de classiﬁcation non supervisee. Notre but ﬁnal etant de trouver des concepts co-
herents de l’application, le meilleur moyen d’y parvenir est de regrouper les mots en fonction
de leurs proprietes semantiques. La methode a utiliser Va donc regrouper les mots du corpus en

Salma J amoussi, Kamel Sma'1'li et J ean-Paul Haton

différentes classes, construisant ainsi les concepts de l’application. Dans ce cas, il nous reste
qu’a affecter un nom de concept approprié a chaque groupe de mots. Parmi les méthodes de
classiﬁcation non supervisée, nous avons implanté les cartes de Kohonen, les réseaux de neu-
rones de Oja et Sanger, la méthode des K-means et quelques méthodes fondées sur la mesure
d’information mutuelle moyenne entre les mots (J amoussi et al., 2002). Les concepts obtenus
par ces méthodes étaient bien signiﬁcatifs, mais contenaient du “bruit”. Autrement dit, certains
mots n’avaient pas leurs places dans le sens exprimé par ces concepts. Pour remédier a ce prob-
leme, nous avons exploré d’autres méthodes et avons adopté les réseaux bayésiens en raison de
leur fondement mathématique fort et le mécanisme d’inférence puissant sous-jacent (Cheese-
man & Stutz, 1996).

Dans la suite, nous présentons le principe de la théorie bayésienne sur laquelle se base l’outil
que nous avons utilisé (AutoClass) et nous détaillons quelques étapes de calcul permettant de
trouver les concepts correspondants au corpus d’apprentissage utilisé. Nous exposons aussi
trois approches différentes pour la représentation vectorielle des mots. Cette représentation,
qui doit étre sémantiquement signiﬁcative, constitue une étape clé au sein du systeme de com-
préhension puisqu’elle constitue l’entrée du réseau bayésien qui Va décider des groupements
des mots formant les concepts.

3.1 Principe du réseau bayésien “AutoClass”

AutoClass est un réseau bayésien pour la classiﬁcation non supervisée qui accepte en entrée des
valeurs réelles, mais aussi des valeurs non numériques comme des mots, des caracteres etc. En
résultat, il fournit des probabilités d’appartenance des éléments en entrée, aux classes trouvées.
I1 suppose l’existence d’une variable multinomiale cachée qui peut représenter les différentes
classes auxquelles appartiennent les éléments en entrée. AutoClass est basé sur le théoreme de
Bayes exprimé par :

p(H) p(D|H)
17(0)
Dans notre cas, D représente les données et donc les mots a classer et H une hypothese con-
cemant le nombre de classes et leurs descriptions en terme de probabilités. Avec AutoClass, on
cherche a maximiser la probabilité p(H |D), c’est a dire que sachant D, les mots du corpus, on

doit sélectionner H, l’ensemble des concepts, qui maximise cette probabilité.

P(H|D) = (1)

Dans notre réseau bayésien représenté par la ﬁgure 2, un mot 33, est donné sous la forme
d’un vecteur a K valeurs d’attributs, 3327,, k E {1...K Un concept Cj est décrit, lui aussi,
par K attributs, chacun est modélisé par une distribution gaussienne normale. 7,-k est un
vecteur parametre décrivant le keme attribut du j eme concept Cj et il contient deux éléments,
la moyenne M,-k de la distribution considérée et son écart-type 0,-1,. Pour l’ensemble du concept,
ce vecteur est noté  et il contient les  de tous ses attributs. La probabilité qu’un mot 3:,-
appartienne au concept 0 - , appelée la probabilité de classe et est notée 7r,- constitue aussi un
parametre descriptif du concept C,-.

Ainsi nous avons déﬁni nos parametres de travail, les données D sont bien les mots, représentés
par le vecteur ? a I éléments englobant tous les 93,. L’hypothese H correspondant a la de-
scription des concepts est représentée par trois éléments, le nombre de concepts J et les deux
vecteurs ? et qui contiennent respectivement les 7rj et les j de tous les concepts. Au-
toClass divise le probleme d’identiﬁcation des concepts en deux parties : la détermination des

Vers la compréhension automatique de la parole

C1" J

    

Figure 2: La structure générale du réseau bayésien AutoClass.

parametres de classiﬁcation (7 et ?) pour un nombre donné de concepts et la détermination du
nombre de concepts J. Ce dernier probleme nécessite plusieurs approximations (pour les de-
tails, voir (Cheeseman & Stutz, 1996)). Dans ce qui suit, H représente seulement les vecteurs
? et  En remplacant, dans l’équation 1, D et H par leurs valeurs, on obtient :

pm?) N17,?)
PW)

ou p(7, 7) est la probabilité a priori des parametres de classiﬁcation, son calcul est bien décrit
dans (Cheeseman & Stutz, 1996). La probabilité a priori des mots, p(?) peut étre calculée
directement. Elle est considérée simplement comme une constante de normalisation. Dans ce
qui suit, on s’intéresse au calcul de la probabilité p(?|7, ?) qui représente la fonction de
vraisemblance des données.

p(7, W?) = (2)

On sait que ? est un vecteur représentant tous les mots du corpus d’apprentissage, la vraisem-
blance de ce vecteur est donc calculée comme étant le produit des probabilités de tous les mots
séparément comme le montre l’équation suivante :

p(?I7, 7’) = f[p(x.—l7, 7’) <3)

p(a:,- | 7, 7) est la probabilité d’observer le mot xi indépendamment du concept auquel il appar-
tient. Elle est donnée par la somme des probabilités que ce mot appartienne a chaque concept
séparément, pondérée par les probabilités des classes comme indiqué par l’équation suivante :

J
p(:1:,-|?, 7, J) = Zvrj  E C’,-,  (4)
j=1

Puisque le mot mi est décrit par un ensemble de K attributs, avec la supposition, un peu forte,
que ces attributs sont indépendants, la probabilité p(m,v  E C’,-, j) peut s’écrire donc, sous la
forme suivante :

K
 E Cg‘;  = H P(33z'Ic|33z' E Cj: 73%) (5)
k=1

AutoClass modélise les attributs a valeurs réelles par une distribution gaussienne normale représen-
tée par le vecteur 3-,, qui contient les deux parametres pjk et 0,7,. Dans ce cas, p(a:,-k|a:,- E
0,, 7,7,) qui correspond a la distribution de classe peut s’écrire sous la forme suivante :
1 1 as-1. — u -1. 2
P($z'Icl90z' E Cjaﬂjkaajk) = T exp i‘ -4) J (5)
\/ 27m,-k 2 0,-1;

Une fois, cette distribution de classe déterminée, il nous reste a chercher l’ ensemble des parametres

de concepts qui maximisent la probabilité de départ p(7, ?|?) et trouver ainsi l’ensemble des
concepts optimaux relatifs a nos données.

Salma J amoussi, Kamel Sma'1'1i et J ean-Paul Haton

3.2 Représentation vectorielle des mots
3.2.1 Contexte des mots

Un mot peut avoir plusieurs caractéristiques possibles, mais rares sont celles qui peuvent lui
donner une représentation sémantique complete. Dans une premiere étape, nous avons décidé
d’associer a chaque mot ses différents contextes d’uti1isation en émettant 1’hypothese que si
deux mots ont les memes contextes alors ils sont sémantiquement proches. Dans cette approche,
un mot sera donc représenté par un vecteur a 2 X N éléments contenant les N mots de son
contexte gauche et les N mots de son contexte droit.

La ﬁgure 3 présente un exemple de la représentation contextuelle des mots. Nous disposons
d’une phrase contenant 4 mots ; en ﬁxant N = 2, nous représentons chaque mot Wi par les
deux mots de ses contextes gauche et droit.

[<DEB><DEB> W1 W2 VV3 V54 <FIN><FIN>J

W1 ?> <DEB> <DEB> W2 W3

W2 j> <DEB> W1 W 3 W4

W3j> W1 W2 W4 <FIN>
Wise» W2 W3 <FIN> <FIN>

Figure 3: Un exemple de représentation de mots par leurs contextes (N = 2).

La valeur de N a été ﬁxée a 2 car les requétes sont généralement courtes et concises. Par
conséquent, les contextes sémantiquement signiﬁcatifs sont constitués de peu de mots. I1 faut
rappeler aussi que les mots outils de la langue comme : je, alors, le, mais, etc., ne sont pas
considérés.

Les classes que nous avons obtenues par cette méthode représentent bien des concepts séman-
tiques, mais ont1’inconvénientde se chevaucher. De plus, nous avons eu des difﬁcultés a con-
troler 1e nombre de concepts. Quelques exemples de ces concepts sont donnés dans la table 2.

3.2.2 Similarité entre mots

Pour trouver des concepts plus homogenes, nous avons changé completement la structure vec-
torielle de chaque mot. Nous avons utilisé la mesure de l’information mutuelle moyenne qui
permet de trouver des similarités contextuelles entre mots.

Dans cette approche, nous associons a chaque mot un vecteur a M éléments, o1‘1 M est la taille
du lexique. L’é1ément numéro j de ce vecteur représente la valeur de l’information mutuelle
moyenne entre le mot numéro j du lexique et le mot a représenter comme indiqué dans la suite:

W1‘:iI(w15'wi)7I(w23'wi)7---aI(wj5wi)a---:I('wM3'wi)] (7)

Ce vecteur exprime 1e degré de similarité du mot en question avec tous les autres mots du
corpus. La forrnule de 1’infor1nation mutuelle moyenne (Rosenfeld, 1994) entre deux mots wa

Vers la compréhension automatique de la parole

| Concept  Groupe de mots |
Favoris_1 Préféré, favoris, préférés, choisi, apprécié, aimé, adoré
Favoris_2 Favoris, préférés, écouté, vu, utilisé, regardé
Favoris_3 Favoris, préférés, choisi, apprécié, aimé, adoré, préféré, similaire, semblable,

pareil, équivalent, ressemblant, synonyme, proche, identique, rapproché
Demande_1 Possible, demande, veut, voudrais, aimerais, souhaite, souhaiterais,
faut, désire, désirerais

Demande_2 Peux, pourrais, veux, voudrais, possible, cherche, demande, aimerais,
souhaite, souhaiterais, faut, désire, désirerais, fais

Ordre Montrer, indiquer, sélectionner, trouver, donner, afﬁcher, voir, presser,
prendre, passer, chercher

Table 2: Exemples de concepts obtenus avec la représentation basée sur le contexte des mots.

et wb est donnée par :

I(w,, : w,,) = P(w,,, wb) log %,jf—;,|;%,5 + P(w,,, mb) log  +

(8)
P(w,,, wb) log  LT’,,£) + P(w,,, Eb) log  

ou P (wa, wb) est la probabilité de trouver les deux mots 111,, et wb dans la meme phrase, P(w,, |
wb) est la probabilité de trouver le mot wa sachant qu’on a déja rencontré le mot wb, P(w,,) est
la probabilité de trouver le mot wa et P(ﬁa) est la probabilité de ne pas avoir rencontré le mot
wa etc.

En représentant ainsi chaque mot du corpus, et en utilisant le réseau bayésien, nous avons obtenu
des classes sémantiques homogenes, une classe étant constituée de mots partageant les memes
propriétés sémantiques. Le nombre de ces classes est tres cohérent avec notre application. Cette
représentation nous a permis aussi de résoudre le probleme des chevauchements entre concepts.
Dans la table 3, nous donnons quelques exemples des concepts ainsi construits, ou l’on remar-
que bien qu’il n’y a plus de chevauchement, mais qu’il existe encore quelques imperfections
comme dans le cas des concepts Demande_1 , Demande_2 et Demande_3 qui auraient dﬁs étre
regroupés ensemble.

| Concept  Groupe de mots |
Favoris Favoris, préférés, choisi, apprécié, adoré, préféré
Mode Ecouté, vu, regardé, utilisé

Similarité Demier, similaire, semblable, pareil, équivalent, ressemblant, synonyme,
proche, identique, rapproché

Demande_1 Pourrais, veux, voudrais

Demande_2 Possible, aimerais, souhaiterais

Demande_3 Souhaite, faut, désire, désirerais

Ordre Montrer, indiquer, sélectionner, trouver, donner, afﬁcher, présenter, prendre,
passer, chercher

Table 3: Quelques exemples de concepts obtenus en utilisant la représentation basée sur
l’information mutuelle moyenne entre les mots.

Salma J amoussi, Kamel Sma'1'li et J ean-Paul Haton

3.2.3 Combinaison : contexte et similarité

Nous avons combiné les deux approches précédentes en vue d’améliorer les résultats. En ef-
fet, la premiere approche travaillant au niveau occurrence, exploite directement les informa-
tions liées au contexte d’utilisation des mots, tandis que la deuxieme, utilise une mesure pour
chercher des similarités entre deux mots. On peut aisément comprendre que les informations
utilisées au niveau de ces deux méthodes sont différentes et complémentaires.

Combiner ces deux méthodes, consiste donc a représenter chaque mot par une matrice d’infor-
mation mutuelle moyenne a dimension M X 3. La premiere colonne correspond au vecteur
d’information mutuelle moyenne précédent (voir section 3.2.2), la deuxieme colonne représente
l’information mutuelle moyenne entre un mot quelconque du vocabulaire et le contexte gauche
du mot a représenter. Idem pour la troisieme colonne mais concemant le contexte droit.

La jeme valeur de la deuxieme colonne est la moyenne pondérée des informations mutuelles
moyennes entre le j eme mot du vocabulaire et le vecteur constituant le contexte gauche du mot
I/Vi en question. Elle est calculée comme suit :

211296 contexte gauche de W,-, I (wj : wg) X K 109
N b_0cc

on I M Mj(C_,,) représente l’information mutuelle moyenne entre le mot wj du lexique et le
contexte gauche du mot Wi. I (wj : wg) représente l’information mutuelle moyenne entre le
mot numéro j du lexique et le mot wg qui appartient au contexte gauche du mot W,-. K my est
le nombre de fois ou le mot wg est trouvé comme contexte gauche du mot Wi et N b_occ est le
nombre total d’occurrence du mot Wi dans le corpus. Le mot I/Vi sera donc représenté par une
matrice comme le montre la ﬁgure 4.

IMM,-(Cg) = (9)

I(w1: wi) IMM1(Cig) IMM1(Cii)
I(w2:wi) IMM2(C1g) IMM2(C1d)

i Irwjzwi) 1Miv1,- (Cig) HVIlVIj(Cid)

I(:wM: wi) IMlVIN[Cig) IlVIlVIN[Cid)

Figure 4: Representation du mot Wi par la méthode combinée.

La matrice utilisée pour représenter un mot du corpus exploite un maximum d’informations sur
ce mot. Elle considere son contexte ainsi que sa similarité avec tous les autres mots du lexique.
Une telle représentation des mots a pu aidé le réseau bayésien dans sa tache de classiﬁcation
et nous a permis d’améliorer considérablement nos résultats. Nous obtenons alors une liste de
concepts bien cohérente qu’on Va utiliser dans la suite de nos traitements. Des exemples de ces
résultats sont donnés au niveau de la table 4.

4 Etiquetage et post-traitement

La demiere étape consiste a fournir les commandes SQL associées aux requétes textuelles
émises en entrée. C’est au cours de cette phase que nous entamons l’étape d’interprétation

Vers la compréhension automatique de la parole

| Concept  Groupe de mots |

Favoris Favoris, préférés, choisi, apprécié, adoré, aimé

Mode Ecouté, vu, regardé, utilisé

Similarité Similaire, semblable, pareil, équivalent, ressemblant, synonyme,
proche, identique, rapproché

Demande Souhaite, faut, désire, désirerais, peux, pourrais, veux, voudrais,
possible, aimerais, souhaiterais

Ordre Montrer, indiquer, sélectionner, trouver, donner, afﬁcher, présenter,
prendre, passer, chercher

Table 4: Quelques exemples de concepts obtenus en utilisant l’approche combinée.

des requétes. En effet, disposant de l’ensemble des concepts qui régissent notre application,
nous pouvons attribuer a chaque requéte les concepts appropriés. Il s’agit de l’étape de “Tra-
duction sémantique”, la premiere composante de l’architecture générale de notre systeme de
compréhension (voir la ﬁgure 1). Pour ce faire, il nous sufﬁt d’étiqueter nos phrases en asso-
ciant a chaque mot dans la phrase sa classe sémantique correspondante. Puisque nos concepts ne
se chevauchent pas entre eux, étiqueter ainsi les requétes ne présente aucun risque d’ambigui'té.

Ensuite, nous pouvons passer a la deuxieme composante de notre modele, le “Convertisseur de
représentation”, ou il s’agit de convertir les concepts trouvés en commandes SQL permettant
d’extraire l’information requise de notre base de données. Pour ce faire, nous avons réalisé
un moteur d’inférence qui a chaque concept, fait correspondre une ou plusieurs sous-requétes
génériques. Dans une requéte SQL générique, les concepts interviennent au niveau des condi-
tions. Ainsi, par exemple, si nous trouvons le concept “Date”, nous ne connaissons pas la valeur
de cette date mais, nous pouvons indiquer dans la requéte générée qu’il y a une condition sur la
date. Ce moteur d’inférence prend en compte bien sur les répétitions, les oublis, les demandes
multiples et implicites ainsi d’autres phénomenes de la parole spontanée. Dans la phase suiv-
ante, nous instancions chaque concept, dans la requéte générique obtenue, par sa valeur qui est
déduite en revenant a la phrase initiale. Ainsi, nous obtenons une vraie commande SQL que
nous pouvons exécuter pour extraire les pages recherchées. Dans la ﬁgure 5, nous donnons un
exemple illustrant les différentes étapes suivies aﬁn d’aboutir a une commande SQL ﬁnalisée.
Les résultats obtenus sont encourageants, en effet, en terme de requétes SQL correctes, nous
obtenons un taux de 100% avec le corpus d’apprentissage et un taux de 92.5% avec un corpus
de test contenant 400 phrases différentes.

5 Conclusion

Dans cet article, nous sommes partis du principe que le probleme de la compréhension automa-
tique est un probleme d’association entre deux langages différents, le langage naturel et le lan-
gage des concepts. Les concepts sont des entités sémantiques regroupant un ensemble de mots
qui partagent les memes propriétés sémantiques et qui expriment une certaine idée. Nous avons
proposé trois méthodes différentes pour l’eXtraction automatique des concepts, ainsi qu’une
approche d’étiquetage et de génération automatique des requétes SQL correspondantes aux de-
mandes des utilisateurs.

Salma J amoussi, Kamel Sma'1'li et J ean-Paul Haton

Montre moi la liste de mes preferes que j’ai consultée avant décembre 2001

Identification des
concepts

Ordre, Objet, Favoris, Date
Génération d’une
requéie ge’ne’rique

select Objet from table_favoris where condition_date ;
Génération d’une
requéie SQL

select * from favoris where date < #01/12/2001# ;

Figure 5: Chaine de traitement appliquée a une requéte en langage naturel.

Les taches d’extraction de concepts et d’étiquetage sont d’habitude réalisées manuellement.
Elles constituent la phase la plus délicate et la plus coﬁteuse dans le processus de compréhen-
sion. Les méthodes proposées dans cet article ont permis d’éviter ce recours a l’expertise hu-
maine et peuvent servir a plusieurs autres domaines qui touchent a la classiﬁcation sémantique,
comme les domaines de catégorisation de texte, d’extraction d’information et de fouille de don-
nées. La méthode combinée a donné les meilleurs résultats grace aux informations qu’elle a
su exploitées pour représenter au mieux un mot du corpus. Pour notre application de consulta-
tion de pages favorites, les concepts qu’elle a trouvés sont trés satisfaisants. Il nous ont permis
ensuite, de mener a bien l’étape d’étiquetage sans rencontrer des difﬁcultés notables.

Nous envisageons d’étendre le module de post-traitement de fagon a ce qu’il puisse réagir face
a de nouveaux mots clés non pris en compte par les concepts. Pour ce faire, il faut adapter notre
modéle a la phase d’exploitation pour que nous puissions ajouter des mots aux concepts. Nous
souhaitons aussi intégrer notre module de compréhension dans un systéme de reconnaissance
automatique de la parole aﬁn de réaliser une application interactive exploitable.

Références

BOUSQUET-VERNHETTES C. & VIGOUROUX N. (2001). Context use to improve the speech under-
standing processing. In International Workshop on Speech and Computer; SPECOM’0I, Moscow.

CHEESEMAN P. & STUTZ J. (1996). Bayesian classiﬁcation (autoclass): Theory and results. In Ad-
vances in Knowledge Discovery and Data Mining. U. Fayyad, G. Shapiro, P. Smyth, R. Uthurusamy.

JAMOUSSI S., SMAILI K. & HATON J . (2002). Neural network and information theory in speech
understanding. In International Workshop on Speech and Computer; SPECOM’02, St. Petersbourg.

MAYNARD H. & LEFEVRE F. (2002). Apprentissage d’un module stochastique de compréhension de la
parole. In 24emes Joumées d ’Etude sur la parole, Nancy.

PIERACCINI R., LEVIN E. & VIDAL E. (1993). Learning how to understand language. In Proceedings
of the 4rd European Conference on Speech Communication and Technology, Berlin.

ROSENFELD R. (1994). Adaptive Statistical Language Modeling: A Maximum Entropy Approach. PhD
thesis, School of Computer Science Carnegie Mellon University, Pittsburgh, PA 15213.

SIU K.-C. & MENG H. M. (1999). Semi-automatic acquisition of domain-speciﬁc semantic structures.
In Proceedings of the 6th European Conference on Speech Communication and Technology, Budapest.

