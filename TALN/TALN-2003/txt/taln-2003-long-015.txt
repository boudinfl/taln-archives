TALN 2003, Batz-sur-Mer, 11‚Äì14 juin 2003
Vers la compr√©hension automatique de la parole : extraction
de concepts par r√©seaux bay√©siens
Salma Jamoussi, Kamel Sma√Øli et Jean-Paul Haton
LORIA/INRIA-Lorraine
615 rue du Jardin Botanique, BP 101, F-54600 Villers-l√®s-Nancy, France
{jamoussi, smaili, jph}@loria.fr
Mots-clefs ‚Äì Keywords
Compr√©hension de la parole, concepts s√©mantiques, r√©seaux bay√©siens, √©tiquetage s√©mantique,
cat√©gorisation automatique.
Speech understanding, semantic concepts, Bayesian networks, semantic labelling, automatic
categorization.
R√©sum√© - Abstract
La compr√©hension automatique de la parole peut √™tre consid√©r√©e comme un probl√®me d‚Äôas-
sociation entre deux langages diff√©rents. En entr√©e, la requ√™te exprim√©e en langage naturel et en
sortie, juste avant l‚Äô√©tape d‚Äôinterpr√©tation, la m√™me requ√™te exprim√©e en terme de concepts. Un
concept repr√©sente un sens bien d√©termin√©. Il est d√©fini par un ensemble de mots partageant les
m√™mes propri√©t√©s s√©mantiques. Dans cet article, nous proposons une m√©thode √† base de r√©seau
bay√©sien pour l‚Äôextraction automatique des concepts ainsi que trois approches diff√©rentes pour
la repr√©sentation vectorielle des mots. Ces repr√©sentations aident un r√©seau bay√©sien √† regrouper
les mots, construisant ainsi la liste ad√©quate des concepts √† partir d‚Äôun corpus d‚Äôapprentissage.
Nous conclurons cet article par la description d‚Äôune √©tape de post-traitement au cours de laque-
lle, nous √©tiquetons nos requ√™tes et nous g√©n√©rons les commandes SQL appropri√©es validant
ainsi, notre approche de compr√©hension.
The automatic speech understanding can be considered as association problem between two
different languages. At the entry, the request expressed in natural language and at the end,
just before the stage of interpretation, the same request is expressed in term of concepts. One
concept represents given meaning, it is defined by the set of words sharing the same semantic
properties. In this paper, we propose a new Bayesian network based method to automatically
extract the underlined concepts. We also propose three different approaches for the vector
representation of words. These representations help Bayesian network to build the adequate
list of concepts for the considered application. We finish this paper by description of the post-
processing step during which, we label our sentences and we generate the corresponding SQL
queries. This step allows us to validate our speech understanding approach.
Salma Jamoussi, Kamel Sma√Øli et Jean-Paul Haton
1 Introduction
Dans la litt√©rature, plusieurs m√©thodes de compr√©hension de la parole ont √©t√© propos√©es. La
plupart de ces m√©thodes se fondent sur des approches stochastiques de d√©codage conceptuel qui
permettent d‚Äôapprocher la compr√©hension automatique, r√©duisant ainsi le recours √† l‚Äôexpertise
humaine. Cependant, ces m√©thodes n√©cessitent une √©tape d‚Äôapprentissage supervis√©, ce qui sig-
nifie qu‚Äôil y a une √©tape ant√©rieure d‚Äôannotation manuelle du corpus d‚Äôapprentissage (Maynard
& Lef√®vre, 2002; Bousquet-Vernhettes & Vigouroux, 2001; Pieraccini et al., 1993).
Dans de telles approches fond√©es sur le d√©codage conceptuel, l‚Äô√©tape d‚Äôannotation consiste √†
segmenter les donn√©es d‚Äôapprentissage en des segments conceptuels repr√©sentant chacun un
sens bien d√©termin√© (Bousquet-Vernhettes & Vigouroux, 2001). Il s‚Äôagit donc de trouver tout
d‚Äôabord les diff√©rents concepts relatifs au corpus, de segmenter ensuite les phrases de ce cor-
pus, de les √©tiqueter en utilisant les concepts trouv√©s et de proc√©der enfin √† l‚Äôapprentissage
automatique. Faire tout ce travail d‚Äôune fa√ßon manuelle constitue sans doute une phase fasti-
dieuse et co√ªteuse. De plus, l‚Äôextraction manuelle est sujette √† la subjectivit√© et aux erreurs
humaines. Automatiser cette t√¢che permettra donc de r√©duire ou d‚Äôannuler l‚Äôintervention hu-
maine et surtout de pouvoir r√©utiliser ce m√™me proc√©d√© lorsqu‚Äôon change de contexte (Siu &
Meng, 1999).
Dans cet article, nous commen√ßons par d√©crire l‚Äôarchitecture g√©n√©rale de notre syst√®me de com-
pr√©hension de la parole, bas√©e sur l‚Äôapproche propos√©e dans (Pieraccini et al., 1993). Ensuite,
nous pr√©sentons une nouvelle approche pour extraire automatiquement les concepts s√©man-
tiques. Pour ce faire, nous utilisons un r√©seau bay√©sien pour la classification non supervis√©e,
appel√© AutoClass. Puis, nous exposons trois m√©thodes diff√©rentes pour la repr√©sentation vecto-
rielle des mots afin de les regrouper pour former des concepts. Enfin, nous abordons la derni√®re
√©tape du processus de compr√©hension, au cours de laquelle nous √©tiquetons les requ√™tes et nous
g√©n√©rons les commandes SQL associ√©es.
2 La compr√©hension automatique de la parole
Le probl√®me de compr√©hension de la parole peut √™tre vu comme un probl√®me de mise en corre-
spondance entre une cha√Æne de mots en entr√©e et une suite de mots dans un langage plus restreint
v√©hiculant les id√©es principales d‚Äôune phrase. Il s‚Äôagit, dans un premier temps, d‚Äôassocier les
mots de la phrase en entr√©e du syst√®me √† des messages dans un langage s√©mantique interm√©di-
aire (souvent appel√©s concepts). Dans un second temps, afin de satisfaire la requ√™te √©mise en
entr√©e, on traduit les concepts obtenus en actions ou r√©ponses et on parle dans ce cas de l‚Äô√©tape
d‚Äôinterpr√©tation de la phrase.
L‚Äôentr√©e du syst√®me peut √™tre donn√©e sous forme textuelle ou sous forme d‚Äôun signal de parole,
sa sortie exprim√©e en tant qu‚Äôactions ou commandes n‚Äôest qu‚Äôune conversion d‚Äôune liste de
concepts donn√©e par un module interm√©diaire de traduction s√©mantique et fournissant le sens
litt√©ral de la phrase. Un concept est une classe de mots traitant d‚Äôun m√™me sujet et partageant
des propri√©t√©s communes. Par exemple, les mots h√¥tel, chambre, auberge et studio peuvent
tous correspondre au concept ‚Äúh√©bergement‚Äù dans une application touristique. Dans (Pieraccini
et al., 1993), les auteurs d√©finissent un mod√®le g√©n√©ral pour la compr√©hension automatique de
la parole qui, en raison de sa simplicit√© et de son efficacit√©, a √©t√© repris dans plusieurs autres
travaux (Maynard & Lef√®vre, 2002; Bousquet-Vernhettes & Vigouroux, 2001). Nous avons
Vers la compr√©hension automatique de la parole
adopt√© la m√™me architecture g√©n√©rale mais nous proposons des techniques diff√©rentes au sein
de chacune de ses composantes. La figure 1 illustre l‚Äôarchitecture g√©n√©rale d‚Äôun tel syst√®me de
compr√©hension de la parole.
sens
repr√©sent√©
Parole ou texte Traducteur par les Convertisseur de ACTION
S√©mantique Repr√©sentation
TS concepts CR
appropri√©s
Figure 1: Architecture g√©n√©rale d‚Äôun syst√®me de compr√©hension automatique de la parole.
Dans notre travail, nous nous int√©ressons √† une application de consultation de pages ‚ÄúFavoris‚Äù
(Bookmarks en anglais). Pour ce faire, nous utilisons un corpus du projet europ√©en MIAMM
dont l‚Äôobjectif est de construire une plate-forme de dialogue oral multimodale. Le corpus con-
tient 71287 requ√™tes diff√©rentes exprim√©es en langue fran√ßaise. Chaque requ√™te exprime une
mani√®re particuli√®re d‚Äôinterroger la base. Des exemples de ces requ√™tes sont donn√©s dans la ta-
ble 1. Ces requ√™tes sont fournies au syst√®me de compr√©hension sous leur forme textuelle. Notre
but est de fournir √† la fin les requ√™tes SQL correspondantes qui, en les ex√©cutant, r√©pondront
aux demandes des utilisateurs.
Montre-moi le contenu de mes favoris.
Je voudrais savoir si tu peux me prendre le contenu que j‚Äôaime.
Est-ce que tu veux me s√©lectionner les titres que je pr√©f√®re.
Est-il possible que tu me passes le premier de mes favoris.
Te serait-il possible de m‚Äôindiquer quelque chose de pareil.
Tu peux faire voir uniquement d√©cembre 2001.
Il faut que tu me pr√©sentes la liste que j‚Äôai utilis√©e t√¥t ce matin.
Je te demande de me passer les chansons que j‚Äôai √©cout√©es ce matin.
Je souhaiterais que tu me montres les disques que j‚Äôai regard√©s dans la matin√©e.
Je veux voir le deuxi√®me que j‚Äôai regard√© dans la matin√©e.
Table 1: Quelques exemples de requ√™tes du corpus MIAMM.
3 Extraction des concepts : m√©thodes et r√©sultats
Au cours de cette √©tape, nous cherchons √† identifier les concepts s√©mantiques li√©s √† notre appli-
cation. La d√©termination manuelle de ces concepts est une t√¢che tr√®s lourde. Il nous faut donc
trouver une m√©thode automatique qui, pourrait ne pas donner des r√©sultats aussi performants que
ceux obtenus par la m√©thode manuelle, mais qui, en contre partie, permet une automatisation
compl√®te du processus de compr√©hension.
Partant du principe d‚Äôautomatisation de cette t√¢che de cat√©gorisation, nous avons opt√© pour des
m√©thodes de classification non supervis√©e. Notre but final √©tant de trouver des concepts co-
h√©rents de l‚Äôapplication, le meilleur moyen d‚Äôy parvenir est de regrouper les mots en fonction
de leurs propri√©t√©s s√©mantiques. La m√©thode √† utiliser va donc regrouper les mots du corpus en
Salma Jamoussi, Kamel Sma√Øli et Jean-Paul Haton
diff√©rentes classes, construisant ainsi les concepts de l‚Äôapplication. Dans ce cas, il nous reste
qu‚Äô√† affecter un nom de concept appropri√© √† chaque groupe de mots. Parmi les m√©thodes de
classification non supervis√©e, nous avons implant√© les cartes de Kohonen, les r√©seaux de neu-
rones de Oja et Sanger, la m√©thode des K-means et quelques m√©thodes fond√©es sur la mesure
d‚Äôinformation mutuelle moyenne entre les mots (Jamoussi et al., 2002). Les concepts obtenus
par ces m√©thodes √©taient bien significatifs, mais contenaient du ‚Äúbruit‚Äù. Autrement dit, certains
mots n‚Äôavaient pas leurs places dans le sens exprim√© par ces concepts. Pour rem√©dier √† ce prob-
l√®me, nous avons explor√© d‚Äôautres m√©thodes et avons adopt√© les r√©seaux bay√©siens en raison de
leur fondement math√©matique fort et le m√©canisme d‚Äôinf√©rence puissant sous-jacent (Cheese-
man & Stutz, 1996).
Dans la suite, nous pr√©sentons le principe de la th√©orie bay√©sienne sur laquelle se base l‚Äôoutil
que nous avons utilis√© (AutoClass) et nous d√©taillons quelques √©tapes de calcul permettant de
trouver les concepts correspondants au corpus d‚Äôapprentissage utilis√©. Nous exposons aussi
trois approches diff√©rentes pour la repr√©sentation vectorielle des mots. Cette repr√©sentation,
qui doit √™tre s√©mantiquement significative, constitue une √©tape cl√© au sein du syst√®me de com-
pr√©hension puisqu‚Äôelle constitue l‚Äôentr√©e du r√©seau bay√©sien qui va d√©cider des groupements
des mots formant les concepts.
3.1 Principe du r√©seau bay√©sien ‚ÄúAutoClass‚Äù
AutoClass est un r√©seau bay√©sien pour la classification non supervis√©e qui accepte en entr√©e des
valeurs r√©elles, mais aussi des valeurs non num√©riques comme des mots, des caract√®res etc. En
r√©sultat, il fournit des probabilit√©s d‚Äôappartenance des √©l√©ments en entr√©e, aux classes trouv√©es.
Il suppose l‚Äôexistence d‚Äôune variable multinomiale cach√©e qui peut repr√©senter les diff√©rentes
classes auxquelles appartiennent les √©l√©ments en entr√©e. AutoClass est bas√© sur le th√©or√®me de
Bayes exprim√© par :
 	 	
 
	 (1)
 	
Dans notre cas,  repr√©sente les donn√©es et donc les mots √† classer et  une hypoth√®se con-
cernant le nombre de classes et leurs descriptions en terme de probabilit√©s. Avec AutoClass, on
cherche √† maximiser la probabilit√©  
	 , c‚Äôest √† dire que sachant  , les mots du corpus, on
doit s√©lectionner  , l‚Äôensemble des concepts, qui maximise cette probabilit√©.
Dans notre r√©seau bay√©sien repr√©sent√© par la figure 2, un mot  est donn√© sous la forme
d‚Äôun vecteur √†  valeurs d‚Äôattributs, 
fiffffifl "!#!$!%'& . Un concept (*) est d√©crit, lui aussi,
,-
par  attributs, chacun est mod√©lis√© par une distribution gaussienne normale. + ). est un
vecteur param√®tre d√©crivant le  √®me attribut du / √®me concept (0) et il contient deux √©l√©ments,
la moyenne 12)3 de la distribution consid√©r√©e et son √©cart-type 45)3 . Pour l‚Äôensemble du concept,
,
- -
,
ce vecteur est not√©
+
) et il contient les
+
). de tous ses attributs. La probabilit√© qu‚Äôun mot 
appartienne au concept (6) , appel√©e la probabilit√© de classe et est not√©e 75) constitue aussi un
param√®tre descriptif du concept (6) .
Ainsi nous avons d√©fini nos param√®tres de travail, les donn√©es  sont bien les mots, repr√©sent√©s
,
par le vecteur  √† 8 √©l√©ments englobant tous les  . L‚Äôhypoth√®se  correspondant √† la de-
+
scription des concepts est repr√©sent√©e par trois √©l√©ments, le nombre de concepts 9 et les deux
,
-
,
-
,
vecteurs 7 et + qui contiennent respectivement les 75) et les + ) de tous les concepts. Au-
+
toClass divise le probl√®me d‚Äôidentification des concepts en deux parties : la d√©termination des
Vers la compr√©hension automatique de la parole
Cj J
xi1 xi2 xi xr iK
I
Figure 2: La structure g√©n√©rale du r√©seau bay√©sien AutoClass.
, -
,
param√®tres de classification ( 7 et + ) pour un nombre donn√© de concepts et la d√©termination du
+
nombre de concepts 9 . Ce dernier probl√®me n√©cessite plusieurs approximations (pour les d√©-
tails, voir (Cheeseman & Stutz, 1996)). Dans ce qui suit,  repr√©sente seulement les vecteurs
,:-
,
7 et + . En rempla√ßant, dans l‚Äô√©quation 1,  et  par leurs valeurs, on obtient :
+
,- ,-
, , ,
,-  ; 	<   	
, ,
+  7  +  7
   	*
+ + +
,
+  7  (2)
  	
+ +

+
,=-
,
o√π   	+  7 est la probabilit√© a priori des param√®tres de classification, son calcul est bien d√©crit
+
,
dans (Cheeseman & Stutz, 1996). La probabilit√© a priori des mots,  ; 	 peut √™tre calcul√©e
+
directement. Elle est consid√©r√©e simplement comme une constante de normalisation. Dans ce
,=-
, ,
qui suit, on s‚Äôint√©resse au calcul de la probabilit√©    	 +  7 qui repr√©sente la fonction de
+ +
vraisemblance des donn√©es.
,
On sait que  est un vecteur repr√©sentant tous les mots du corpus d‚Äôapprentissage, la vraisem-
+
blance de ce vecteur est donc calcul√©e comme √©tant le produit des probabilit√©s de tous les mots
s√©par√©ment comme le montre l‚Äô√©quation suivante :
,
- , -
, , ,
?
   	* >    	
 +  7 B +  7 (3)
+ + +
:@A
,=-
,
   	
B +  7 est la probabilit√© d‚Äôobserver le mot 2 ind√©pendamment du concept auquel il appar-
+
tient. Elle est donn√©e par la somme des probabilit√©s que ce mot appartienne √† chaque concept
s√©par√©ment, pond√©r√©e par les probabilit√©s des classes comme indiqu√© par l‚Äô√©quation suivante :
,=- ,-
,
F
   	 E    	
B +  7CD9 7H) B Bff()IJ+ ) (4)
+
)G@A
Puisque le mot 2 est d√©crit par un ensemble de  attributs, avec la supposition, un peu forte,
,-
que ces attributs sont ind√©pendants, la probabilit√©    	 Bff()IJ+ ) peut s‚Äô√©crire donc, sous la
forme suivante :
,- ,:-
?
   	 L    	
B Bff()KJ+ ) B BNffO()KD+ ). (5)
M@A
AutoClass mod√©lise les attributs √† valeurs r√©elles par une distribution gaussienne normale repr√©sen-
,=-
t√©e par le vecteur + )3 qui contient les deux param√®tres 12). et 4 ). . Dans ce cas,   2 BPff
,=-
	
()IJ+ ). qui correspond √† la distribution de classe peut s‚Äô√©crire sous la forme suivante :
Z
  B= 1B).
   	6
SV[
R S
+
B Bff()IQ1B)3G4 ). (6)
+
4 ). \"]G^
7T4 ).VUXW5Y
Une fois, cette distribution de classe d√©termin√©e, il nous reste √† chercher l‚Äôensemble des param√®tres
,
-
, ,
de concepts qui maximisent la probabilit√© de d√©part  ;  	+  7  et trouver ainsi l‚Äôensemble des
+ +
concepts optimaux relatifs √† nos donn√©es.
Salma Jamoussi, Kamel Sma√Øli et Jean-Paul Haton
3.2 Repr√©sentation vectorielle des mots
3.2.1 Contexte des mots
Un mot peut avoir plusieurs caract√©ristiques possibles, mais rares sont celles qui peuvent lui
donner une repr√©sentation s√©mantique compl√®te. Dans une premi√®re √©tape, nous avons d√©cid√©
d‚Äôassocier √† chaque mot ses diff√©rents contextes d‚Äôutilisation en √©mettant l‚Äôhypoth√®se que si
deux mots ont les m√™mes contextes alors ils sont s√©mantiquement proches. Dans cette approche,
S`_a a
un mot sera donc repr√©sent√© par un vecteur √† √©l√©ments contenant les mots de son
a
contexte gauche et les mots de son contexte droit.
La figure 3 pr√©sente un exemple de la repr√©sentation contextuelle des mots. Nous disposons
a S
d‚Äôune phrase contenant b mots ; en fixant  , nous repr√©sentons chaque mot c' par les
deux mots de ses contextes gauche et droit.
 
<DEB><DEB>  W1    W2    W3    W4    <FIN><FIN> 
W 1 <DEB> <DEB>         W2          W3
W 2 <DEB>       W 1           W 3         W 4
 
W 3     W 1           W 2           W 4     <FIN>
 
W 4     W 2           W 3      <FIN>   <FIN>
 
a S
Figure 3: Un exemple de repr√©sentation de mots par leurs contextes (  ).
a S
La valeur de a √©t√© fix√©e √† car les requ√™tes sont g√©n√©ralement courtes et concises. Par
cons√©quent, les contextes s√©mantiquement significatifs sont constitu√©s de peu de mots. Il faut
rappeler aussi que les mots outils de la langue comme : je, alors, le, mais, etc., ne sont pas
consid√©r√©s.
Les classes que nous avons obtenues par cette m√©thode repr√©sentent bien des concepts s√©man-
tiques, mais ont l‚Äôinconv√©nient de se chevaucher. De plus, nous avons eu des difficult√©s √† con-
tr√¥ler le nombre de concepts. Quelques exemples de ces concepts sont donn√©s dans la table 2.
3.2.2 Similarit√© entre mots
Pour trouver des concepts plus homog√®nes, nous avons chang√© compl√®tement la structure vec-
torielle de chaque mot. Nous avons utilis√© la mesure de l‚Äôinformation mutuelle moyenne qui
permet de trouver des similarit√©s contextuelles entre mots.
Dans cette approche, nous associons √† chaque mot un vecteur √† d √©l√©ments, o√π d est la taille
du lexique. L‚Äô√©l√©ment num√©ro / de ce vecteur repr√©sente la valeur de l‚Äôinformation mutuelle
moyenne entre le mot num√©ro / du lexique et le mot √† repr√©senter comme indiqu√© dans la suite:
fe g g 	 g g 	 g g 	 lgnm g 	po
c 8 Aih  Q8 h  J!J!J!jQ8 )kh  D!J!J!DG8 h  (7)
Ce vecteur exprime le degr√© de similarit√© du mot en question avec tous les autres mots du
]
corpus. La formule de l‚Äôinformation mutuelle moyenne (Rosenfeld, 1994) entre deux mots grq
Vers la compr√©hension automatique de la parole
Concept Groupe de mots
Favoris_1 Pr√©f√©r√©, favoris, pr√©f√©r√©s, choisi, appr√©ci√©, aim√©, ador√©
Favoris_2 Favoris, pr√©f√©r√©s, √©cout√©, vu, utilis√©, regard√©
Favoris_3 Favoris, pr√©f√©r√©s, choisi, appr√©ci√©, aim√©, ador√©, pr√©f√©r√©, similaire, semblable,
pareil, √©quivalent, ressemblant, synonyme, proche, identique, rapproch√©
Demande_1 Possible, demande, veut, voudrais, aimerais, souhaite, souhaiterais,
faut, d√©sire, d√©sirerais
Demande_2 Peux, pourrais, veux, voudrais, possible, cherche, demande, aimerais,
souhaite, souhaiterais, faut, d√©sire, d√©sirerais, fais
Ordre Montrer, indiquer, s√©lectionner, trouver, donner, afficher, voir, presser,
prendre, passer, chercher
Table 2: Exemples de concepts obtenus avec la repr√©sentation bas√©e sur le contexte des mots.
et gts est donn√©e par :
gtq gtsu	* vwlgtq gtsu	5x$y"z|{<}~ ¬ÄD¬Å ~¬É¬Çl¬Ñ vwlgtq g¬Üsp	5x¬áy"z|{<}%~ ¬ÄD¬Å ~¬É¬Ç¬à¬Ñ
8 h  
{<}~ ¬Ñ#{<}%~H¬Ç¬Ñ;¬Ö {<}%~ ¬Ñ:{<} ~5¬Ç¬à¬Ñ.¬Ö
¬Ä ¬Ä
(8)
{<} ~ ¬Å ~ ¬Ñ {<} ~ ¬Å ~ ¬Ñ
¬Ä ¬Ç ¬Ä ¬Ç
vw g¬Üq gtsp	5x¬áy"z v¬â gkq gksu	5x$y"z
 
{<} ~ ¬Ñ#{<}%~ ¬Ñ {<} ~ ¬Ñ:{<} ~ ¬Ñ
¬Ä ¬Ç ¬Ä ¬Ç
¬Ö
o√π v¬âgtq gtsu	 est la probabilit√© de trouver les deux mots gkq et gts dans la m√™me phrase, vwlgkq≈Å
gtsu	 est la probabilit√© de trouver le mot gnq sachant qu‚Äôon a d√©j√† rencontr√© le mot gks v¬âgtqj	, est
la probabilit√© de trouver le mot gnq et v¬â gkqj	 est la probabilit√© de ne pas avoir rencontr√© le mot
gtq etc.
En repr√©sentant ainsi chaque mot du corpus, et en utilisant le r√©seau bay√©sien, nous avons obtenu
des classes s√©mantiques homog√®nes, une classe √©tant constitu√©e de mots partageant les m√™mes
propri√©t√©s s√©mantiques. Le nombre de ces classes est tr√®s coh√©rent avec notre application. Cette
repr√©sentation nous a permis aussi de r√©soudre le probl√®me des chevauchements entre concepts.
Dans la table 3, nous donnons quelques exemples des concepts ainsi construits, o√π l‚Äôon remar-
que bien qu‚Äôil n‚Äôy a plus de chevauchement, mais qu‚Äôil existe encore quelques imperfections
comme dans le cas des concepts Demande_1, Demande_2 et Demande_3 qui auraient d√ªs √™tre
regroup√©s ensemble.
Concept Groupe de mots
Favoris Favoris, pr√©f√©r√©s, choisi, appr√©ci√©, ador√©, pr√©f√©r√©
Mode √âcout√©, vu, regard√©, utilis√©
Similarit√© Dernier, similaire, semblable, pareil, √©quivalent, ressemblant, synonyme,
proche, identique, rapproch√©
Demande_1 Pourrais, veux, voudrais
Demande_2 Possible, aimerais, souhaiterais
Demande_3 Souhaite, faut, d√©sire, d√©sirerais
Ordre Montrer, indiquer, s√©lectionner, trouver, donner, afficher, pr√©senter, prendre,
passer, chercher
Table 3: Quelques exemples de concepts obtenus en utilisant la repr√©sentation bas√©e sur
l‚Äôinformation mutuelle moyenne entre les mots.
Salma Jamoussi, Kamel Sma√Øli et Jean-Paul Haton
3.2.3 Combinaison : contexte et similarit√©
Nous avons combin√© les deux approches pr√©c√©dentes en vue d‚Äôam√©liorer les r√©sultats. En ef-
fet, la premi√®re approche travaillant au niveau occurrence, exploite directement les informa-
tions li√©es au contexte d‚Äôutilisation des mots, tandis que la deuxi√®me, utilise une mesure pour
chercher des similarit√©s entre deux mots. On peut ais√©ment comprendre que les informations
utilis√©es au niveau de ces deux m√©thodes sont diff√©rentes et compl√©mentaires.
Combiner ces deux m√©thodes, consiste donc √† repr√©senter chaque mot par une matrice d‚Äôinfor-
_¬å¬ã
mation mutuelle moyenne √† dimension d . La premi√®re colonne correspond au vecteur
d‚Äôinformation mutuelle moyenne pr√©c√©dent (voir section 3.2.2), la deuxi√®me colonne repr√©sente
l‚Äôinformation mutuelle moyenne entre un mot quelconque du vocabulaire et le contexte gauche
du mot √† repr√©senter. Idem pour la troisi√®me colonne mais concernant le contexte droit.
La / √®me valeur de la deuxi√®me colonne est la moyenne pond√©r√©e des informations mutuelles
moyennes entre le / √®me mot du vocabulaire et le vecteur constituant le contexte gauche du mot
c en question. Elle est calcul√©e comme suit :
_
lg g 	
qu¬ö
¬é ¬é
¬é
8 )¬üh 
~

~ ¬ëX¬í¬î¬ìl¬ï¬à¬ñD¬ó:¬ò¬ôM¬ó:¬ò ¬ì¬õX¬òH¬úu¬ò5¬ù*¬û
 	*¬ê¬è
a`¬†
¬é
8¬Éd¬çd`) ( (9)
_ ¬°¬¢¬£¬¢
O√π  	¬é85d¬§d`) ( repr√©sente l‚Äôinformation mutuelle moyenne entre le mot g ) du lexique et le
contexte gauche du mot lg g 	¬éc¬• . 8 )¬¶h repr√©sente l‚Äôinformation mutuelle moyenne entre le
mot num√©ro / du lexique et le mot g ¬é qui appartient au contexte gauche du mot ¬éc .  est
~
a¬ß¬†
le nombre de fois o√π le mot g ¬é est trouv√© comme contexte gauche du mot c et _ ¬°¬¢¬£¬¢ est le
nombre total d‚Äôoccurrence du mot c dans le corpus. Le mot c sera donc repr√©sent√© par une
matrice comme le montre la figure 4.
I(w 1 : w i )
i
IMM 1 (Cg) IMM 1 (C
id)
i
I(w 2 : w i ) IMM  (C
i
2 g) IMM 2 (Cd)
W i = i iI(w j : w i ) IMM j (Cg) IMM j (Cd)
i
I(w M  : w i ) IMM M (Cg) IMM M (C
id)
Figure 4: Repr√©sentation du mot c¬• par la m√©thode combin√©e.
La matrice utilis√©e pour repr√©senter un mot du corpus exploite un maximum d‚Äôinformations sur
ce mot. Elle consid√®re son contexte ainsi que sa similarit√© avec tous les autres mots du lexique.
Une telle repr√©sentation des mots a pu aid√© le r√©seau bay√©sien dans sa t√¢che de classification
et nous a permis d‚Äôam√©liorer consid√©rablement nos r√©sultats. Nous obtenons alors une liste de
concepts bien coh√©rente qu‚Äôon va utiliser dans la suite de nos traitements. Des exemples de ces
r√©sultats sont donn√©s au niveau de la table 4.
4 √âtiquetage et post-traitement
La derni√®re √©tape consiste √† fournir les commandes SQL associ√©es aux requ√™tes textuelles
√©mises en entr√©e. C‚Äôest au cours de cette phase que nous entamons l‚Äô√©tape d‚Äôinterpr√©tation
Vers la compr√©hension automatique de la parole
Concept Groupe de mots
Favoris Favoris, pr√©f√©r√©s, choisi, appr√©ci√©, ador√©, aim√©
Mode √âcout√©, vu, regard√©, utilis√©
Similarit√© Similaire, semblable, pareil, √©quivalent, ressemblant, synonyme,
proche, identique, rapproch√©
Demande Souhaite, faut, d√©sire, d√©sirerais, peux, pourrais, veux, voudrais,
possible, aimerais, souhaiterais
Ordre Montrer, indiquer, s√©lectionner, trouver, donner, afficher, pr√©senter,
prendre, passer, chercher
Table 4: Quelques exemples de concepts obtenus en utilisant l‚Äôapproche combin√©e.
des requ√™tes. En effet, disposant de l‚Äôensemble des concepts qui r√©gissent notre application,
nous pouvons attribuer √† chaque requ√™te les concepts appropri√©s. Il s‚Äôagit de l‚Äô√©tape de ‚ÄúTra-
duction s√©mantique‚Äù, la premi√®re composante de l‚Äôarchitecture g√©n√©rale de notre syst√®me de
compr√©hension (voir la figure 1). Pour ce faire, il nous suffit d‚Äô√©tiqueter nos phrases en asso-
ciant √† chaque mot dans la phrase sa classe s√©mantique correspondante. Puisque nos concepts ne
se chevauchent pas entre eux, √©tiqueter ainsi les requ√™tes ne pr√©sente aucun risque d‚Äôambigu√Øt√©.
Ensuite, nous pouvons passer √† la deuxi√®me composante de notre mod√®le, le ‚ÄúConvertisseur de
repr√©sentation‚Äù, o√π il s‚Äôagit de convertir les concepts trouv√©s en commandes SQL permettant
d‚Äôextraire l‚Äôinformation requise de notre base de donn√©es. Pour ce faire, nous avons r√©alis√©
un moteur d‚Äôinf√©rence qui √† chaque concept, fait correspondre une ou plusieurs sous-requ√™tes
g√©n√©riques. Dans une requ√™te SQL g√©n√©rique, les concepts interviennent au niveau des condi-
tions. Ainsi, par exemple, si nous trouvons le concept ‚ÄúDate‚Äù, nous ne connaissons pas la valeur
de cette date mais, nous pouvons indiquer dans la requ√™te g√©n√©r√©e qu‚Äôil y a une condition sur la
date. Ce moteur d‚Äôinf√©rence prend en compte bien s√ªr les r√©p√©titions, les oublis, les demandes
multiples et implicites ainsi d‚Äôautres ph√©nom√®nes de la parole spontan√©e. Dans la phase suiv-
ante, nous instancions chaque concept, dans la requ√™te g√©n√©rique obtenue, par sa valeur qui est
d√©duite en revenant √† la phrase initiale. Ainsi, nous obtenons une vraie commande SQL que
nous pouvons ex√©cuter pour extraire les pages recherch√©es. Dans la figure 5, nous donnons un
exemple illustrant les diff√©rentes √©tapes suivies afin d‚Äôaboutir √† une commande SQL finalis√©e.
Les r√©sultats obtenus sont encourageants, en effet, en terme de requ√™tes SQL correctes, nous
S
obtenons un taux de I¬®"¬®¬™¬© avec le corpus d‚Äôapprentissage et un taux de ¬´ !¬¨"¬© avec un corpus
de test contenant b¬™¬®"¬® phrases diff√©rentes.
5 Conclusion
Dans cet article, nous sommes partis du principe que le probl√®me de la compr√©hension automa-
tique est un probl√®me d‚Äôassociation entre deux langages diff√©rents, le langage naturel et le lan-
gage des concepts. Les concepts sont des entit√©s s√©mantiques regroupant un ensemble de mots
qui partagent les m√™mes propri√©t√©s s√©mantiques et qui expriment une certaine id√©e. Nous avons
propos√© trois m√©thodes diff√©rentes pour l‚Äôextraction automatique des concepts, ainsi qu‚Äôune
approche d‚Äô√©tiquetage et de g√©n√©ration automatique des requ√™tes SQL correspondantes aux de-
mandes des utilisateurs.
Salma Jamoussi, Kamel Sma√Øli et Jean-Paul Haton
Montre moi la liste de mes pr√©f√©r√©s que j‚Äôai consult√©e avant d√©cembre 2001
Identification des 
concepts
Ordre, Objet, Favoris, Date
G√©n√©ration d‚Äôune 
reque^te g√©n√©rique
select Objet from table_favoris where condition_date ;
G√©n√©ration d‚Äôune
reque^t e SQL
select * from favoris where date < #01/12/2001# ;
Figure 5: Cha√Æne de traitement appliqu√©e √† une requ√™te en langage naturel.
Les t√¢ches d‚Äôextraction de concepts et d‚Äô√©tiquetage sont d‚Äôhabitude r√©alis√©es manuellement.
Elles constituent la phase la plus d√©licate et la plus co√ªteuse dans le processus de compr√©hen-
sion. Les m√©thodes propos√©es dans cet article ont permis d‚Äô√©viter ce recours √† l‚Äôexpertise hu-
maine et peuvent servir √† plusieurs autres domaines qui touchent √† la classification s√©mantique,
comme les domaines de cat√©gorisation de texte, d‚Äôextraction d‚Äôinformation et de fouille de don-
n√©es. La m√©thode combin√©e a donn√© les meilleurs r√©sultats gr√¢ce aux informations qu‚Äôelle a
su exploit√©es pour repr√©senter au mieux un mot du corpus. Pour notre application de consulta-
tion de pages favorites, les concepts qu‚Äôelle a trouv√©s sont tr√®s satisfaisants. Il nous ont permis
ensuite, de mener √† bien l‚Äô√©tape d‚Äô√©tiquetage sans rencontrer des difficult√©s notables.
Nous envisageons d‚Äô√©tendre le module de post-traitement de fa√ßon √† ce qu‚Äôil puisse r√©agir face
√† de nouveaux mots cl√©s non pris en compte par les concepts. Pour ce faire, il faut adapter notre
mod√®le √† la phase d‚Äôexploitation pour que nous puissions ajouter des mots aux concepts. Nous
souhaitons aussi int√©grer notre module de compr√©hension dans un syst√®me de reconnaissance
automatique de la parole afin de r√©aliser une application interactive exploitable.
R√©f√©rences
BOUSQUET-VERNHETTES C. & VIGOUROUX N. (2001). Context use to improve the speech under-
standing processing. In International Workshop on Speech and Computer, SPECOM‚Äô01, Moscow.
CHEESEMAN P. & STUTZ J. (1996). Bayesian classification (autoclass): Theory and results. In Ad-
vances in Knowledge Discovery and Data Mining. U. Fayyad, G. Shapiro, P. Smyth, R. Uthurusamy.
JAMOUSSI S., SMA√èLI K. & HATON J. (2002). Neural network and information theory in speech
understanding. In International Workshop on Speech and Computer, SPECOM‚Äô02, St. Petersbourg.
MAYNARD H. & LEF√àVRE F. (2002). Apprentissage d‚Äôun module stochastique de compr√©hension de la
parole. In 24√®mes Journ√©es d‚Äô√âtude sur la parole, Nancy.
PIERACCINI R., LEVIN E. & VIDAL E. (1993). Learning how to understand language. In Proceedings
of the 4rd European Conference on Speech Communication and Technology, Berlin.
ROSENFELD R. (1994). Adaptive Statistical Language Modeling: A Maximum Entropy Approach. PhD
thesis, School of Computer Science Carnegie Mellon University, Pittsburgh, PA 15213.
SIU K.-C. & MENG H. M. (1999). Semi-automatic acquisition of domain-specific semantic structures.
In Proceedings of the 6th European Conference on Speech Communication and Technology, Budapest.
