TALN 2003, Batz-sur-Mer, 11—14juin 2003

Apprentissage discriminant pour les Grammaires a
Substitution d’Arbres

Antoine Rozenknop, J ean-Cedric Chappelier, Martin Rajman
Laboratoire d’Intelligence Artiﬁcielle - Ecole Polytechnique Fédérale de
Lausanne
CH-1015 Lausanne, Switzerland

{Antoine . Rozenknop, Jean—Cedric . Chappelier, Martin . Rajman}@epfl . ch

Mots-clefs — Keywords

STSG, Gibbs-Markov, Maximum d’Entropie, Vraisemblance Conditionnelle
STSG, Gibbs-Markov,MaXimum Entropy, Conditionnal likelihood

Résumé - Abstract

Les grammaires stochastiques standards utilisent des modeles probabilistes de nature générati-
ve, fondés sur des probabilités de récriture conditionnées par le symbole récrit. Les expériences
montrent qu’elles tendent ainsi par nature a pénaliser les dérivations les plus longues pour
une méme entrée, ce qui n’est pas forcément un comportement souhaitable, ni en analyse syn-
taxique, ni en reconnaissance de la parole. Dans cet article, nous proposons une approche proba-
biliste non-générative du modele STSG (grammaire stochastique a substitution d ’arbres), selon
laquelle les probabilités sont conditionnées par les feuilles des arbres syntaxiques plutot que par
leur racine, et qui par nature fait appel a un apprentissage discriminant. Plusieurs expériences
sur ce modele sont présentées.

Standard stochastic grammars use generative probabilistic models, focussing on rewriting pro-
babilities conditioned by the rewritten symbol. Such grammars therefore tend to give penalty to
longer derivations of the same input, which could be a drawback when they are used for analysis
(e. g. speech recognition). In this contribution, we propose a novel non-generative probabilistic
model of STSGS (Stochastic Tree Substitution Grammars), where probabilities are conditio-
ned by the leaves of the syntactic trees (i.e. the input symbols) rather than by the root. Several
experiments of this new model are presented.

A. Rozenknop, J .-C. Chappelier, M. Rajman

1 Motivations

Les grammaires stochastiques standards sont des modeles probabilistes generatifs dans les-
quels les probabilites sont conditionnees par le symbole a recrire. Par exemple, les probabilites
des grammaires hors-contextes stochastiques (SCFG) sont les probabilites de recrire la partie
gauche de la regle connaissant sa partie droite. Les grammaires a substitution d’arbres stochas-
tiques (STSG), utilisees dans le cadre DOP (Data-Oriented Parsing) (Bod, 1998; Chappelier &
Rajman, 2001), sont quant a elles des grammaires dont les regles sont des arbres syntaxiques,
appeles “arbres e’le’mentaires”. Ces arbres elementaires sont combines a l’aide de l’operateur de
substitution pour donner des derivations d’arbres d’analyse complets. De plus, a chaque arbre
elementaire 7' est associee la probabilite p(7') d’utiliser l’arbre elementaire pour recrire son
symbole racine dans une derivation. Les modeles SCFG et STSG sont equivalents d’un point
de vue structurell, mais clairement differents d’un point de vue probabiliste. De fait, les STSG
peuvent capturer plus de dependances probabilistes que les SCFG, qui sont restreintes a des

regles hors-contextes equivalentes a des arbres elementaires de profondeur strictement egale a
1.

Utilisees en reconnaissance de la parole ou en analyse syntaxique, ces grammaires presentent
differents inconvenients : les mecanismes d’apprentissage standards des SCFG menent a des
modeles affectant des estimations biaisees aux probabilites des arbres syntaxiques (Johnson,
1998); pour sa part, le modele DOP est connu pour sur-estimer les probabilites des arbres de
grande profondeur et critique pour le manque de justiﬁcation de sa procedure d’apprentissage2
(Bonnema et al., 1999).

Le but de cet article est de presenter des modeles log-lineaires discriminants de grammaires
a substitution d’arbres stochastiques qui evitent ces inconvenients : les modeles Gibbsiens de
Grammaires a Substitution d’arbres (GTSG). Nous en donnons d’abord une deﬁnition formelle.
Puis nous presentons les elements-clefs pour l’apprentissage des parametres de ces modeles a
partir d’un corpus, avant de conclure par une comparaison des GTSG aux STSG standards dans
une tache d’analyse syntaxique.

2 Le modele GTSG

Le point de depart du modele est de considerer une probabilisation des TSG qui ne se base pas
sur la probabilite d’un arbre conditionnellement a sa racine, mais plutet conditionnellement a
ses feuilles, ce qui semble plus naturellement correspondre a une tache d’analyse syntaxique.
M. Collins traite le probleme comme un probleme de classiﬁcation en proposant une approche
a base de “Voted Perceptron” (Collins & Duffy, 2002).

Nous utilisons ici une approche de type Gibbs-Markov (Lafferty, 1996), decrite dans (Rozenk-
nop, 2002) pour les grammaires hors-contextes, qui permet de choisir les traits pris en compte
pour le calcul des probabilites conditionnelles. Dans le cas de l’analyse, le modele repose sur des
probabilites d’analyse conditionnellement aux phrases, les traits etant les arbres elementaires
apparaissant dans les derivations des analyses.

Cette approche implique que nous imposons a priori que la probabilite conditionnelle d’une
derivation d connaissant la phrase w suit une distribution de Gibbs :

1Le meme langage est reconnu et les memes arbres sont engendres pour une meme phrase.
2i.e. la fagon de calculer ses parametres a partir d’un corpus d’exemples.

Apprentissage discriminant pour les Grammaires a Substitution d’Arbres

1 1
p(d|w) : jeﬂen >\Tfr(d) : jg)‘-f(d) (1)

Z)\(’U}) Z)\(’U})

ou Z ;‘(w) est le facteur de normalisation3 Edy” ezrek "TfT(d), f, (d) est le nombre d’occur-
rences de l’arbre élémentaire 7' dans la dérivation d, R est l’ensemble des arbres élémentai-
res de la grammaire, A = (A7-1, . . . , A7-n) est le vecteur des parametres associés aux arbres

élémentaires, et f (d) = ( f7-1 (d), . . . , f7-n(d)).

De plus, comme pour une STSG standard, la probabilité conditionnelle d’un arbre t connais-
sant la phrase w est déﬁnie comme la somme des probabilités conditionnelles de toutes ses
dérivations4 :

m(t|w) = Zpkwlw) <2)

d=>t
Nous appelons Gibbsian Tree Substitution Grammar (GTSG) une STSG ainsi probabilisée.

Notons que la difference principale entre GTSG et STSG tient dans la probabilisation du modele
et dans l’algorithme d’apprentissage associé. Cependant, des algorithmes identiques peuvent
étre utilisés pour effectuer une analyse syntaxique, i.e. determiner l’analyse la plus probable
(MPP5) d’une phrase.

Exemple jouet

L’ exemple suivant illustre les differences entre STSG et GTSG. Considérons une TSG contenant
les arbres élémentaires de la ﬁgure 1.6

La ou la STSG associe des probabilités pl, . . . , p5 aux arbres élémentaires, la GTSG utilise
des valeurs réelles non contraintes A1, . . . , A5, appelées potentiels et intégrées dans le modele
probabiliste par l’équation (1).

Supposons alors que nous voulions utiliser ces grammaires pour analyser la phrase “Regarde la
femme avec un chapeau”, qui peut étre associée aux analyses (A) et (B) de la ﬁgure 2.

Avec la STSG, les probabilités des dérivations sont respectivement :7
psTsa(d1(A)) = P1 -193 -192 ‘P5 et psTsa(d1(B)) = P2 -193 ‘P5

Avec la GTSG, l’équation (2) impose que le potentiel associé a une dérivation soit la somme
des potentiels de ses arbres élémentaires, d’ou :

 2 A1 -1- A3 + 2)\4 -1- A5 Ct  2 A2 + 2)\4 -1- A5

Les probabilités des dérivations d1 (A) et d1(B) conditionnellement aux feuilles w sont alors :

eA<d1<A>> eA<d1<B>>
pGT3G(d1(A)|“’) : eA(d1(A)) + eA(d1(B)) et pGT3G(d1(B)|“’) : eA(d1(A)) + eA(d1(B))

3d => 11) représente1’ensemb1e des derivations menant a w.

4AVec une STSG, un arbre d’ analyse peut avoir plusieurs derivations différentes, contrairement au cas des CFG.
5MPP = Most Probable Parse.

5Notez que ces arbres sont de profondeur 1, et que la grammaire peut alors aussi etre considérée comme une

CFG; ce n’est pas le cas en general, mais cela simpliﬁe ici notre illustration.
7AVec cet exemple élémentaire, chaque analyse possede une seule derivation.

A. Rozenknop, J .-C. Chappelier, M. Rajman

Arbres STSG GTSG
élémentaires proba. potentiels
VP
7'1 Z / \ P1 A1
V NP
2 Z 2 2
7' //'P\ p A
V NP PP
NP
73 Z / \ P3 A3
NP PP
NP
7'4 Z / \ P4 A4
Det N
PP
75 Z / \ P5 A5
P NP
P1 '1' P2 Z 1
Contraintes : p3 + p4 : 1 Z,\(w) :
P5 Z 1

FIG. 1 — Exemple STSG/GTSG.

VP
(B) /J \PP
\
NP NP
/ \ / \
V N P

\
NP NP Det Det N
/ \

(A)

V Det N P Det N

FIG. 2 — Deux analyses : (A) correspond par exemple a “Regarde la femme avec le chapeau”,
et (B) a “Regarde la femme avec tes lunettes”.

Illustrons sur cet exemple quelques limitations des STSG pour l’analyse. Considérons par
exemple le cas ou aucune autre information sur le langage utilisé n’est disponible. Dans ce
cas, il peut étre naturel de chercher a imposer que la grammaire affecte la meme probabilité a
chaque analyse. Avec une STSG, l’affectation la plus “impartiale” des probabilités élémentaires
semble correspondre a p1 = p2 = p3 = p4 = %; p5 = 1 : tous les arbres élémentaires de meme
racine sont équiprobables, et la somme de leurs probabilités est 1.

En réalité, cependant, la grammaire obtenue est tres peu impartiale : les contraintes stochas-
tiques choisies favorisent ici les arbres les plus petits, i.e. ceux qui sont produits avec le moins
d’étapes de dérivation. Par exemple, p3T3G (d1 et p3T3G(d1(B)) sont respectivement (%)4
et  : (B) est donc deux fois plus probable que (A) !

Avec une GTSG au contraire l’affectation impartiale A1 = A2 = A3 = A4 = A5 = 0 mene a
des probabilités conditionnelles de d1(A) et d1(B) égales a 1 / 2, ce qui apparait plus conforme
a l’intuition.

Notez que cet exemple a pour seul but d’illustrer la différence entre GTSG et STSG, non de

démontrer la supériorité de l’une sur l’autre. Toutefois, les expérimentations pratiques conﬁr-
ment que des inconvénients ﬂagrants des STSG, soulignés dans (Johnson, 1998) et (Bonnema

Apprentissage discriminant pour les Grammaires a Substitution d’Arbres

et al., 1999), sont effectivement évités avec les GTSG associées a la technique d’apprentissage
décrite ci-apres.

3 Apprentissage des paramétres a partir d’un corpus

Les parametres du modele sont appris a partir d’un corpus C selon le principe de Maximum de
Vraisemblance (Dempster et al., 1977).

Nous cherchons pour cela les parametres A* de la GTSG qui maxiIr1isent la vraisemblance
conditionnelle du corpus d’apprentissage L,; (p A), ou ﬁ(w, t) est la fréquence relative dans C de
l’arbre t de feuilles w :

X‘ = Argmax Zﬁ(w, t) 1ogp;\(t|w)
A wt

3.1 Algorithme “Improved Iterative Scaling” (IIS)

Pour résoudre ce probleme non-trivial de maximisation, nous appliquons l’algorithme IIS gene-
ralisé (Lafferty, 1996) aux STSG8 : plutot que de maximiser L,; (p A) directement, cet algorithme
recherche le maximum en améliorant itérativement le modele A, a partir d’un modele A0 initial.

Dans notre cas, cette méthode conduit a chercher pour chaque arbre élémentaire 5" la solution i‘
de l’équation suivante, ou f #(7') = ZTEGmmmaim f, (d) :

Emu, t) Z PA(d|w)f?(d)35f#(d) = Em», t) Zpx(d|t)f$(d) <3>

d=>w d=>t

Les modeles successifs sont alors obtenus en remplacant A; par A; + log in

Comme ses coefﬁcients sont positifs, l’équation (3) peut facilement se résoudre par la méthode
de Newton. La difﬁculté résiduelle est de calculer les coefﬁcients en un temps “raisonnable”,
de facon a ce que l’apprentissage puisse étre effectivement réalisé. La prise en compte de cette
notion d’efﬁcacité est l’objet de la section suivante.

3.2 Algorithme Inside-Outside

Membre de gauche Le membre de gauche de la formule de réestimation (3) repose sur une
double somme : l’une sur les exemples de la base, l’autre sur les dérivations possibles pour une
phrase donnée.

Le calcul de cette derniere doit etre factorisé pour étre effectué en pratique9.

De fait, le terme le plus problématique de la partie gauche de (3) peut se récrire :

1

d=>w d=>w

_ 1 A , fr(d) _ 1 A T

- mu) 2 f"d)”‘f#(d’ H <6” ) — ma 2 f"d) Hmf 0“
d=>w TEd d=>w TEd

sles variables cachées de 1’ article mentionné correspondent aux dérivations des arbres, qui ne sont pas explicites
dans la base d’apprentissage.

9Le nombre de dérivations d’une analyse peut étre exponentiel en sa taille, et, de plus, une phrase peut avoir un
grand nombre d’ana1yses possibles.

A. Rozenknop, J .-C. Chappelier, M. Rajman

ou 11(7) = €)‘T.'I3.

On peut alors calculer Zdgw f;(d) H76‘, v(7')fT(d) en utilisant l’algorithme Inside-Outside de-
crit dans (Goodman, 1998) sur le semi-anneau des polynomes 1P[v(7')].

De plus, nous avons par déﬁnition : Z ;‘(w) = Zdiw H76‘, (e"T)fT(d). Il apparait ainsi que
Z A (w) est la somme des coefﬁcients associés au 5" tel que f;(d) = 1, qui est déja calculée dans
l’algorithme Inside-Outside. Le membre de gauche est donc completement déterrniné.

Membre de droite De facon similaire, le membre de droite de (3) peut se récrire :

ZPx(d|t)f?(d) = zit) Z Md) Hv'<v>W>

d=>t d=>t 1-ed

ou 11’ (7') = e"T. La encore, Zdgt f;(d) H16‘, 11’ (7')fT(d) peut étre calculé par l’algorithme Inside-
Outside, en “épurant” préalablement la table utilisée pour factoriser les calculs interrnédiaires,
de facon a ce que seules les dérivations menant a l’analyse t y ﬁgurent.

3.3 Apprentissage par profondeur croissante

L’ algorithme IIS présente un inconvénient siIr1ilaire a celui décrit dans (Bonnema & Scha, 2002)
pour DOP : dans le cas ou l’ensemble des arbres élémentaires R contient les analyses completes
du corpus d’apprentissage, le modele sur-estime les parametres de telle sorte que les analyses
non présentes dans le corpus recoivent une probabilité arbitrairement petite.

Pour éviter ce comportement, on peut décider de ne pas mettre les arbres complets du corpus
dans R. Une autre possibilité est de changer légerement la méthode d’apprentissage. Plutot
que de considérer la vraisemblace du corpus avec la grammaire complete, on peut considérer,
de facon itérative, les vraisemblances du corpus en introduisant progressivement des arbres
élémentaires dans la grammaire. Nous avons choisi d’introduire les arbres élémentaires par
ordre de profondeur croissante. Notez que ce type de mécanisme n’est rien de plus qu’une
méthode de lissage de l’apprentissage.

Nous considérons donc l’ensemble des TSG suivantes : g,, est la grammaire associée a l’en-
semble 73,, des sous-arbres du corpus de profondeur maximale p (trivialement 73,, C R,,+1), et
a l’ensemble ARP des parametres correspondants. La méthode d’apprentissage par profondeur
croissante (IDL)1° consiste alors a calculer les parametres ARI de 91 par l’algorithme Improved
Iterative Scaling (IIS), et pour chaque profondeur p, a calculer les parametres ARp\ARp_1 de
gp,11 en gi A73p_1  . Les parametres ARp\ARp_1 sont déterrninés en maximisant
la probabilité conditionnelle L,;(p;‘Rp) du corpus d’apprentissage selon le modele gp, sachant

ARp_1.

En pratique, IDL ne nécessite qu’une adaptation Ininime des algorithmes précédents. Seules

les étapes d’initialisation et de Inise-a-jour sont modiﬁées; les calculs proprement dits restent

inchangés :

— Initialisation : les parametres ARWW \ARp val.ent\—oo,; les pararnetres ARp\ARp_1 valent
0 ; et les parametres ARp_1 gardent la valeur prise a l’1terat1on precedente.

1°IDL = Increasing Depth Learning.
HARP \ARp_1 represente les parametres de ARP qui ne sont pas dans ARp_1.

Apprentissage discriminant pour les Grammaires a Substitution d’Arbres

— Mise-a-jour : seuls les parametres AR sont modiﬁés.

IIS est ainsi répété pmaa: fois, pmaa: étaiit la profondeur maximale des arbres du corpus, ce qui
allonge évidemment le temps d’apprentissage12. Cependant, la convergence IIS étant d’autant
plus rapide que les arbres considérés sont de grande profondeur, on peut encore optimiser IDL
en adaptant le nombre de passes IIS a la profondeur des arbres dont on apprend les parametres.

4 Expériences

Le principe d’apprentissage exposé repose sur la probabilité d’un corpus d’analyses condition-
nellement aux phrases. Pour rester cohérent avec ce principe, l’analyse syntaxique doit repo-
ser sur cette meme probabilité. Dans cette optique, le critere de choix parmi les analyses syn-
taxiques possibles sera donc de sélectionner l’analyse la plus probable (MPP) parmi toutes les
analyses possibles13.

4.1 STSG polynomiales

Les STSG posent une grande difﬁculté : avec elles, la recherche du MPP est en effet un probleme
NP-difﬁcile dans le cas général (Sima’an, 1996). Des algorithmes approximant cette recherche
ont déja été développés (Bod, 1992; Goodman, 1996; Chappelier & Rajman, 2000). Une alter-
native, introduite dans (Chappelier & Rajman, 2001), est d’utiliser des Grammaires a Substi-
tution d’Arbres PolynoIr1iales (pSTSG14) qui, en n’utilisant comme arbres élémentaires qu’un
sous-ensemble bien choisi des sous-arbres du corpus, rendent polynomiale la complexité de la
recherche du MPP. C’est le cadre choisi pour nos expériences.

1VIin-Max pSTSG Les pSTSG “Min-Max” sont obtenues en sélectionnant du corpus deux
types d’arbres élémentaires : les sous-arbres Ininimaux, de profondeur 1, et les sous-arbres
maximaux, i.e. dont toutes les feuilles sont des terminaux. La polynomialité des pSTSG Min-
Max a été prouvée dans (Chappelier & Rajman, 2001).

Head-driven pSTSG Les “Head-driven pSTSG” sont obtenues de facon similaire : tous les
sous-arbres de profondeur 1 sont sélectionnés du corpus, les autres arbres élémentaires étant
ceux dont la “branche de téte” est étendue autant que possible. Une “branche de téte” est une
branche dont tous les noeuds, associés a des catégories syntaxiques, sont étendus seulement s’ils
correspondent a la téte lexicale du noeud qui les domine. Les tétes lexicales sont déﬁnies comme
dans (Collins, 1999).

4.2 Protocole

La version de Bod du corpus ATIS a été utilisée pour cette évaluation. Elle consiste en 750
arbres syntaxiques dans lesquels les feuilles lexicales ont été supprimées. La Min-Max pSTSG
extraite compte 2 434 arbres élémentaires : 381 sont de profondeur 1, les autres étant des arbres

12Un ou deux jours sur une Sun Sparc 10, pour un corpus d’apprentissage de 3000 arbres, pmaar: = 16, et 200
passes IIS par profondeur.

13D’ autres approches sélectionnent la dérivation la plus probable, ou 1’ analyse ayant le maximum de constituants
probablementjustes — Voir (Goodman, 1996).

14pSTSG = Polynomial Stochastic Tree Substitution Grammars.

A. Rozenknop, J .-C. Chappelier, M. Rajman

maximaux. La Head-driven pSTSG compte seulement 930 arbres élémentaires, dont 381 de
profondeur 1, et 549 “branches de tétes”.

La profondeur maximale des arbres du corpus est de 11. C’est donc également la profondeur
maximale des sous-arbres du modele Min-Max. En revanche, la profondeur maximale est de 5
pour le modele Head-driven, certaines regles n’ayant pas de téte lexicale, et les “branches de
tétes” n’étant pas forcément les plus longues dans un arbre.

Deux types d’expériences ont été réalisés. Dans “test 10%”, nous entrainons les modeles sur
90% du corpus, et nous les testons sur les 10% restant. Les résultats correspondent aux moyen-
nes des valeurs obtenues pour dix partitionnements aléatoires. Dans “Autotest”, apprentissage
et test sont réalisés sur le corpus complet; ces secondes expériences ne servent donc que de
repere sur le gain maximum que l’on peut espérer par l’utilisation des modeles gibbsiens.

Dans chaque expérience, le modele GTSG est compare a une STSG obtenue par la méthode
d’apprentissage standard, ou les arbres élémentaires recoivent des probabilités proportionnelles
a leur fréquence dans le corpus.

Pour les modeles Head-driven GCFG, deux types d’apprentissage on été testés sur la partie
“test 10%” : le premier utilise la procédure IDL, la seconde non. De fait, cette procédure peut
étre évitée pour les Head-driven GTSG, car les arbres complets du corpus ne font pas partie de
l’ensemble de leurs arbres élémentaires. De meme, IDL est inutile dans la partie “Autotest”, car
on ne cherche pas alors a observer les capacités de généralisation des grammaires.

Pour évaluer les résultats, nous avons utilisé les scores standards déﬁnis dans PARSEVAL (Man-

ning & Schiitze, 1999) :

— “Couverture” : nombre de phrase recevant au moins une analyse (correcte ou pas) ;

— “Correcte” : taux de phrases correctement analysées parmi les phrases couvertes.

— “Croisé” : taux de phrases recevant un parenthésage incorrect parmi les phrases couvertes. Un
parenthésage est incorrect lorsqu’un des groupes syntaxiques d’une analyse a une intersection
avec un groupe de l’analyse de référence, sans que l’un des deux groupes soit inclus dans
l’autre.

— Précision “P” et Rappel “R” sont obtenus en considérant la séquence d’arbres 7"‘ produits par
l’analyseur comme un ensemble E(7"') de triplets < N, p, d >, ou N est un label syntaxique
attaché a un noeud d’un arbre, et p et d sont les index dans le corpus des premier et demier
mots dominés par N. Par comparaison avec la séquence des arbres de reference 7*’, les taux
sont calculés comme :

W) : um) ﬂ~E(?’)|, W) : um) ﬂ~E(?’)l
|E(T)| |E(T’)|

— le f-score “F” est la moyenne harmonique de la précision et du rappel :

1
F-1 = §(P-1 + R-1)

4.3 Résultats

Les résultats obtenus pour les pSTSG Min-Max et Head-driven sont résumés dans la table 1.

Avec les corpus mentionnés, le processus d’apprentissage prend environ 4 heures pour chaque
modele sur une Sun Blade 60. Pour la procédure IDL, 20 iterations de IIS ont été réalisées
pour chaque profondeur d’arbre élémentaire. Sans IDL, 200 itérations IIS ont été utilisées pour
l’ensemble complet des arbres élémentaires.

Apprentissage discriminant pour les Grammaires a Substitution d’Arbres

| Min-max Head-Driven
Autotest Autotest
Couv. Corr. Croisé P R F Couv. Correct Croisé P R F

STSG 750 664 0 STSG 750 412 57
Taux (en %) 88.5 0.0 99.7 99.2 99.45 Taux (en %) 54.9 7.6 96.9 95.2 96.0

GTSG 750 691 0 GTSG 750 479 43
Taux (en %) 92.1 0.0 99.6 99.5 Taux (en %) 63.8 5.7 97.6 97.0 97.3
Gain (%) 3.6 -0.1 0.2 0.05 Gain (%) 8.9 -1.9 0.7 1.8 1.25

Test 10% Test 10%

STSG 73.9 35.6 11.6 STSG 73.9 30.4 10.4
Taux (en %) 48.2 15.7 93.6 94.4 94.0 Taux (en %) 41.1 14.0 93.7 93.6 93.6

GTSG-]DL 73.9 36.4 9.7 GTSG-HS 73.9 35.7 10.0
Taux (en %) 49.3 13.1 93.5 95.6 94.5 Taux (en %) 48.3 13.5 94.2 95.1 94.6
Gain (%) 1.1 -2.6 -0.1 1.2 0.5 Gain (%) 7.2 0.5 0.5 1.5 1

GTSG-]DL 73.9 35.2 10.2
Taux (en %) 47.6 13.8 94.2 94.8 94.4
Gain (%) 6.5 0.2 0.5 1.2 0.8

TAB. 1 — Performances en analyse : GTSG comparées aux STSG.

4.4 Discussion

Les GTSG obtiennent le score maximum en autotest pour les grammaires Min-Max”. C’est
une illustration du fait que les faiblesses théoriques soulignées dans (Bonnema et al., 1999)
affectent les STSG et non les GTSG. La suppression de ces faiblesses n’a presque aucun effet
en “test-10%”. Nous pensons que ces effets sont masqués par les autres facteurs d’erreur qui
agissent dans cette situation, comme la grande variabilité du corpus utilisé.

L’ avantage des Head-driven GTSG sur les Head-driven STSG est plus évident. Le taux d’ana-
lyses fausses est réduit de 12% sur les expériences “test-10%”. Les taux de parenthésage croisé,
de précision et de rappel en label sont également meilleurs.

Comme prévu par la théorie, la procédure IDL est inutile avec les GTSG Head-driven : les
modeles obtenus avec un IIS standard menent a de meilleurs résultats.

5 Conclusion

Pour conclure, les modeles log-linéaires associés a un apprentissage discriminant développés
dans ce document semblent d’autant plus intéressants que les grammaires comptent moins de
parametres : les grammaires Head-driven en bénéﬁcient plus que les modeles Min-max.

Les résultats obtenus en testant les modeles sur le corpus d’apprentissage semblent conﬁrmer
que les “faiblesses” théoriques connues des STSG sont supprimées dans les GTSG. Cet avantage
est dﬁ principalement a la fonction d’ apprentissage discriminante et conditionnée par les feuilles
que l’on utilise dans le modele GTSG, qui est Inieux adaptée a la tache d’analyse syntaxique a
laquelle on destine la grammaire. L’ amélioration est cependant moins sensible en généralisation,
du fait de l’insufﬁsance des données d’apprentissage par rapport a la taille de la grammaire.

Les tests menés ne montrent cependant pas l’avantage que l’on pourrait tirer d’une autre ca-
ractéristique des GTSG, a savoir l’absence de normalisation de leurs parametres. Nous pres-
sentons en effet que cette absence pourrait étre bénéﬁque dans des applications telles que la

15Le score maximal n’est pas 100%, du fait que le corpus compte 555 arbres diﬂérents sur ses 750, pour seule—
ment 512 phrases diﬂérentes, du fait de la délexicalisation appliquée.

A. Rozenknop, J .-C. Chappelier, M. Rajman

reconnaissance de la parole, ou la nature probabiliste des parametres pose des problemes lors
du mélange de modeles (modele acoustique et modele de langage). Il serait tres intéressant
d’instancier les GTSG, avec une procédure d’apprentissage adaptée, dans un tel contexte.

Références

BOD R. (1992). Applying Monte Carlo techniques to Data Oriented Parsing. In Proceedings Computa-
tional Linguistics in the Netherlands, Tilburg (The Netherlands).

BOD R. (1998). Beyond Grammar; An Experience-Based Theory of Language. Number 88 in CSLI
Lecture Notes. Standford (CA) 2 CSLI Publications.

BONNEMA R., BUYING P. & SCHA R. (1999). A new probability model for data oriented parsing.
In P.DEKKER & G.KERDILES, Eds., Proceedings of the 12th Amsterdam Colloquium, Amsterdam 2
Institute for Logic, Language and Computation.

BONNEMA R. & SCHA R. (2002). Reconsidering the probability model of data-oriented parsing. In R.
BOD, R. SCHA & K. SIMA’AN, Eds., Data-Oriented Parsing, chapter I.3, p. 25-41. CSLI Publications.

CHAPPELIER J .-C. & RAJMAN M. (2000). Monte-Carlo sampling for NP-hard maximization problems
in the framework of weighted parsing. In D. CHRISTODOULAKIS, Ed., Natural Language Processing —
NLP 2000, number 1835 in Lecture Notes in Artiﬁcial Intelligence, p. 106-117. Springer.

CHAPPELIER J .-C. & RAJMAN M. (2001). Grammaire a substitution d’arbre de complexité polyno-
miale 2 un cadre efﬁcace pour DOP. In TALN’200I, volume 1, p. 133-142.

COLLINS M. (1999). Head-Driven Statistical Models for Natural Language Parsing. PhD thesis, Uni-
versity of Pennsylvania.

COLLINS M. & DUFFY N. (2002). New ranking algorithms for parsing and tagging 2 Kernels over
discrete structures, and the voted perceptron. In ACL2002, p. 263-270.
DEMPSTER M. M., LAIRD N. M. & JAIN D. B. (1977). Maximum likelihood from incomplete data
via the em algorithm. Journal of the Royal Statistics Society, 39, 1-38.

GOODMAN J . (1996). Efﬁcient algorithms for parsing the DOP model. In Proc. of the Conf on Empiri-
cal Methods in Natural Language Processing, p. 143-152.

GOODMAN J . (1998). Parsing Inside-Out. PhD thesis, Harvard University. cmp-lg/9805007.

JOHNSON M. (1998). PCFG Models of Linguistic Tree Representations. Computational Linguistics,
24(4), 613-632.

LAFFERTY J . (1996). Gibbs-Markov models. In Computing Science and Statistics, volume 27, p. 370-
377.

MANNING C. & SCHUTZE H. (1999). Foundations of Statistical Natural Language Processing. Cam-
bridge 2 The MIT Press.

ROZENKNOP A. (2002). Une grammaire hors-contexte valuée pour l’analyse syntaxique. In 9eme
Confe’rence Annuelle sur le Traitement Automatique des Langues Naturelles, volume 1, p. 95-104,
Nancy 2 Association pour le Traitement Automatique des Langues (ATALA).

SIMA’AN K. (1996). Computational complexity of probabilistic disambiguation by means of tree gram-
mars. In Proceedings of COLING’96, Copenhagen (Denmark). cmp-lg/9606019.

