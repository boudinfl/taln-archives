<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Bangalore</author>
<author>O Rambow</author>
</authors>
<title>Exploiting a probabilistic hierarchical model for generation.</title>
<date>2000</date>
<booktitle>In Proceedings of COLING 2000,</booktitle>
<pages>42--48</pages>
<location>Saarbrücken, Germany,</location>
<contexts>
<context position="2093" citStr="Bangalore and Rambow 2000" startWordPosition="282" endWordPosition="285">tations. The mappings are performed with linguistic operations, the context for which is primarily machine-learned. The leaf nodes of the resulting syntax tree contain all necessary information from which to generate the surface string. Other systems use machine learning techniques for sentence realization in generation. The Nitrogen system, for example, uses a word bigram language model to score and rank a large set of alternative sentence realizations (Langkilde and Knight, 1998a, 1998b). Other recent Smets, Gamon, Corston-Oliver and Ringger approaches use syntactic representations: FERGUS (Bangalore and Rambow 2000) and Halogen (Langkilde 2000, Langkilde-Geary 2002) use syntax trees as an intermediate representation to determine the optimal string output. The adaptation of German Amalgam to French has been discussed elsewhere (Smets et al. 2003). In this paper, we discuss French Amalgam in some detail by presenting two of the machine-learned models and a tool which allows the researcher to manually inspect the relevant training data for each model. In this way, the researcher can understand why the model makes the decisions it makes and can improve the model by adding relevant linguistic features. The re</context>
</contexts>
<marker>Bangalore, Rambow, 2000</marker>
<rawString>Bangalore S. and Rambow O. (2000). Exploiting a probabilistic hierarchical model for generation. In Proceedings of COLING 2000, Saarbrücken, Germany, pp. 42-48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D M Chickering</author>
</authors>
<title>The WinMine Toolkit.</title>
<date>2002</date>
<tech>Microsoft Technical Report 2002-103.</tech>
<contexts>
<context position="3688" citStr="Chickering 2002" startWordPosition="519" endWordPosition="520">cture of the sentence and also includes some semantic information (e.g., the meaning of time and location prepositions). During sentence realization, the logical form is first degraphed into a tree and then augmented by the insertion of function words (determiners, auxiliaries, some prepositions, etc.) and syntactic labels. Linguistic operations such as the introduction of coordination, raising, ordering, aggregation, punctuation, inflection, etc. are performed to produce a surface syntax tree. The linguistic contexts for insertions and all other operations are learned by the WinMine toolkit (Chickering 2002) and represented in decision tree models. Finally, an output string is read off the leaf nodes. 2.1 Stages of the system Amalgam includes eight stages. The first stage involves language-neutral transformations from a graph representation to a tree representation. Further stages process that tree representation until it results in a surface syntax tree. The contexts for most linguistic operations are machine-learned. The only contexts that are not machine-learned, are those for which there is not sufficient data to train robust models. Stage 1 Pre-processing (procedural)  degraphing of the sem</context>
</contexts>
<marker>Chickering, 2002</marker>
<rawString>Chickering D. M. (2002) The WinMine Toolkit. Microsoft Technical Report 2002-103.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Corston-Oliver</author>
<author>M Gamon</author>
<author>E Ringger</author>
<author>R Moore</author>
</authors>
<title>An overview of Amalgam: a machine-learned generation module.</title>
<date>2002</date>
<booktitle>In &amp;quot;Proceedings of the International Language Generation Conference</booktitle>
<pages>33--40</pages>
<location>New York,</location>
<contexts>
<context position="1251" citStr="Corston-Oliver et al. 2002" startWordPosition="162" endWordPosition="165">. This paper presents the French implementation of Amalgam, a machine-learned sentence realization system. It presents in some detail two of the machine-learned models employed in Amalgam and shows how linguistic intuition and knowledge can be combined with statistical techniques to improve the performance of the models. Keywords – Mots Clés Réalisation de phrase, génération automatique, arbres de décision, français. Sentence realization, generation, machine-learning, decision trees, French. 1 Introduction Amalgam is a multilingual sentence realization system. Developed originally for German (Corston-Oliver et al. 2002, Gamon et al. 2002), it has been adapted to French (Smets et al. 2003). Amalgam maps a representation of propositional content (henceforth &amp;quot;logical form&amp;quot;) to a surface syntax tree via intermediate syntactic representations. The mappings are performed with linguistic operations, the context for which is primarily machine-learned. The leaf nodes of the resulting syntax tree contain all necessary information from which to generate the surface string. Other systems use machine learning techniques for sentence realization in generation. The Nitrogen system, for example, uses a word bigram language</context>
</contexts>
<marker>Corston-Oliver, Gamon, Ringger, Moore, 2002</marker>
<rawString>Corston-Oliver S., Gamon M., Ringger E. and Moore R. (2002) An overview of Amalgam: a machine-learned generation module. In &amp;quot;Proceedings of the International Language Generation Conference 2002&amp;quot;, New York, pp. 33-40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Gamon</author>
<author>E Ringger</author>
<author>S Corston-Oliver</author>
<author>R Moore</author>
</authors>
<title>Machine-learned contexts for linguistic operations in German sentence realization. In: &amp;quot;Proceedings of ACL</title>
<date>2002</date>
<pages>25--32</pages>
<contexts>
<context position="1271" citStr="Gamon et al. 2002" startWordPosition="166" endWordPosition="169">ench implementation of Amalgam, a machine-learned sentence realization system. It presents in some detail two of the machine-learned models employed in Amalgam and shows how linguistic intuition and knowledge can be combined with statistical techniques to improve the performance of the models. Keywords – Mots Clés Réalisation de phrase, génération automatique, arbres de décision, français. Sentence realization, generation, machine-learning, decision trees, French. 1 Introduction Amalgam is a multilingual sentence realization system. Developed originally for German (Corston-Oliver et al. 2002, Gamon et al. 2002), it has been adapted to French (Smets et al. 2003). Amalgam maps a representation of propositional content (henceforth &amp;quot;logical form&amp;quot;) to a surface syntax tree via intermediate syntactic representations. The mappings are performed with linguistic operations, the context for which is primarily machine-learned. The leaf nodes of the resulting syntax tree contain all necessary information from which to generate the surface string. Other systems use machine learning techniques for sentence realization in generation. The Nitrogen system, for example, uses a word bigram language model to score and </context>
</contexts>
<marker>Gamon, Ringger, Corston-Oliver, Moore, 2002</marker>
<rawString>Gamon M., Ringger E., Corston-Oliver S., Moore R. (2002): Machine-learned contexts for linguistic operations in German sentence realization. In: &amp;quot;Proceedings of ACL 2002&amp;quot;, pp. 25-32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Gamon</author>
<author>C Lozano</author>
<author>J Pinkham</author>
<author>T Reutter</author>
</authors>
<title>Practical experience with grammar sharing in multilingual NLP”.</title>
<date>1997</date>
<booktitle>In Burstein J., Leacock C., eds, Proceedings of the Workshop on Making NLP Work, ACL Conference,</booktitle>
<location>Madrid,</location>
<contexts>
<context position="7709" citStr="Gamon et al. 1997" startWordPosition="1140" endWordPosition="1143">task, such as determining the context for the insertion of the subordinate conjunctions que and si, to 1016 for the more difficult task of determining the label of a constituent. The ordering model stands apart from the others, with 4,536 branching nodes (for details see Ringger et al. (in preparation)). Smets, Gamon, Corston-Oliver and Ringger 2.2 Data and feature extraction The training data for all the models consist of a set of 100,000 sentences drawn from software manuals. The sentences are analyzed in the NLPWin system, which provides a syntactic and logical form analysis (Heidorn 2000; Gamon et al. 1997). Nodes in the logical form representation are linked to the corresponding syntactic nodes, allowing us to learn contexts for the mapping from the logical form representation to a surface syntax tree. The data is split 70/30 for training versus model parameter tuning. For each set of data we build decision trees at several different levels of granularity (by manipulating the prior probability of tree structures to favor simpler structures) and select the model with the maximal accuracy as determined on the parameter tuning set. We attempt to standardize as much as possible the set of features </context>
</contexts>
<marker>Gamon, Lozano, Pinkham, Reutter, 1997</marker>
<rawString>Gamon, M., Lozano, C., Pinkham, J. and Reutter, T. (1997) “Practical experience with grammar sharing in multilingual NLP”. In Burstein J., Leacock C., eds, Proceedings of the Workshop on Making NLP Work, ACL Conference, Madrid, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Heidorn</author>
</authors>
<title>Intelligent writing assistance. In</title>
<date>2000</date>
<booktitle>A Handbook of Natural Language Processing: Techniques and Applications for the Processing of Language as Text.</booktitle>
<pages>170--177</pages>
<publisher>Marcel Dekker,</publisher>
<location>New York</location>
<contexts>
<context position="3015" citStr="Heidorn 2000" startWordPosition="427" endWordPosition="428"> of the machine-learned models and a tool which allows the researcher to manually inspect the relevant training data for each model. In this way, the researcher can understand why the model makes the decisions it makes and can improve the model by adding relevant linguistic features. The researcher can thus leverage the richness of the linguistic information to optimize the models. 2 French Amalgam Amalgam takes as its input a logical form graph, i.e., a sentence-level dependency graph with fixed lexical choices for content words. Function words are represented as features or attributes (c.f. Heidorn 2000). The logical form represents the predicate-argument structure of the sentence and also includes some semantic information (e.g., the meaning of time and location prepositions). During sentence realization, the logical form is first degraphed into a tree and then augmented by the insertion of function words (determiners, auxiliaries, some prepositions, etc.) and syntactic labels. Linguistic operations such as the introduction of coordination, raising, ordering, aggregation, punctuation, inflection, etc. are performed to produce a surface syntax tree. The linguistic contexts for insertions and </context>
<context position="7689" citStr="Heidorn 2000" startWordPosition="1138" endWordPosition="1139">tively simple task, such as determining the context for the insertion of the subordinate conjunctions que and si, to 1016 for the more difficult task of determining the label of a constituent. The ordering model stands apart from the others, with 4,536 branching nodes (for details see Ringger et al. (in preparation)). Smets, Gamon, Corston-Oliver and Ringger 2.2 Data and feature extraction The training data for all the models consist of a set of 100,000 sentences drawn from software manuals. The sentences are analyzed in the NLPWin system, which provides a syntactic and logical form analysis (Heidorn 2000; Gamon et al. 1997). Nodes in the logical form representation are linked to the corresponding syntactic nodes, allowing us to learn contexts for the mapping from the logical form representation to a surface syntax tree. The data is split 70/30 for training versus model parameter tuning. For each set of data we build decision trees at several different levels of granularity (by manipulating the prior probability of tree structures to favor simpler structures) and select the model with the maximal accuracy as determined on the parameter tuning set. We attempt to standardize as much as possible </context>
</contexts>
<marker>Heidorn, 2000</marker>
<rawString>Heidorn, G. (2000) Intelligent writing assistance. In Dale R., Moisl H. and Somers H. (eds). A Handbook of Natural Language Processing: Techniques and Applications for the Processing of Language as Text. Marcel Dekker, New York Langkilde I. (2000): Forest-Based Statistical Sentence generation. In: &amp;quot;Proceedings of NAACL 2000&amp;quot;, pp. 170-177.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Langkilde-Geary</author>
</authors>
<title>An Empirical Verification of Coverage and Correctness for a General-Purpose Sentence Generator.</title>
<date>2002</date>
<booktitle>In &amp;quot;Proceedings of the International Language Generation Conference</booktitle>
<pages>17--24</pages>
<location>New York,</location>
<contexts>
<context position="2144" citStr="Langkilde-Geary 2002" startWordPosition="290" endWordPosition="291">ations, the context for which is primarily machine-learned. The leaf nodes of the resulting syntax tree contain all necessary information from which to generate the surface string. Other systems use machine learning techniques for sentence realization in generation. The Nitrogen system, for example, uses a word bigram language model to score and rank a large set of alternative sentence realizations (Langkilde and Knight, 1998a, 1998b). Other recent Smets, Gamon, Corston-Oliver and Ringger approaches use syntactic representations: FERGUS (Bangalore and Rambow 2000) and Halogen (Langkilde 2000, Langkilde-Geary 2002) use syntax trees as an intermediate representation to determine the optimal string output. The adaptation of German Amalgam to French has been discussed elsewhere (Smets et al. 2003). In this paper, we discuss French Amalgam in some detail by presenting two of the machine-learned models and a tool which allows the researcher to manually inspect the relevant training data for each model. In this way, the researcher can understand why the model makes the decisions it makes and can improve the model by adding relevant linguistic features. The researcher can thus leverage the richness of the ling</context>
</contexts>
<marker>Langkilde-Geary, 2002</marker>
<rawString>Langkilde-Geary I. (2002) An Empirical Verification of Coverage and Correctness for a General-Purpose Sentence Generator. In &amp;quot;Proceedings of the International Language Generation Conference 2002&amp;quot;, New York, pp. 17-24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Langkilde</author>
<author>K Knight</author>
</authors>
<title>The practical value of n-grams in generation.</title>
<date>1998</date>
<booktitle>Proceedings of the 9th International Workshop on Natural Language Generation,</booktitle>
<pages>248--255</pages>
<location>Niagara-onthe-Lake,</location>
<contexts>
<context position="1952" citStr="Langkilde and Knight, 1998" startWordPosition="265" endWordPosition="268">algam maps a representation of propositional content (henceforth &amp;quot;logical form&amp;quot;) to a surface syntax tree via intermediate syntactic representations. The mappings are performed with linguistic operations, the context for which is primarily machine-learned. The leaf nodes of the resulting syntax tree contain all necessary information from which to generate the surface string. Other systems use machine learning techniques for sentence realization in generation. The Nitrogen system, for example, uses a word bigram language model to score and rank a large set of alternative sentence realizations (Langkilde and Knight, 1998a, 1998b). Other recent Smets, Gamon, Corston-Oliver and Ringger approaches use syntactic representations: FERGUS (Bangalore and Rambow 2000) and Halogen (Langkilde 2000, Langkilde-Geary 2002) use syntax trees as an intermediate representation to determine the optimal string output. The adaptation of German Amalgam to French has been discussed elsewhere (Smets et al. 2003). In this paper, we discuss French Amalgam in some detail by presenting two of the machine-learned models and a tool which allows the researcher to manually inspect the relevant training data for each model. In this way, the </context>
</contexts>
<marker>Langkilde, Knight, 1998</marker>
<rawString>Langkilde I. and Knight K. (1998a). The practical value of n-grams in generation. Proceedings of the 9th International Workshop on Natural Language Generation, Niagara-onthe-Lake, Canada. pp. 248-255.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Langkilde</author>
<author>K Knight</author>
</authors>
<title>Generation that exploits corpus-based statistical knowledge.</title>
<date>1998</date>
<booktitle>Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics (COLING-ACL</booktitle>
<pages>704--710</pages>
<location>Montréal, Québec,</location>
<contexts>
<context position="1952" citStr="Langkilde and Knight, 1998" startWordPosition="265" endWordPosition="268">algam maps a representation of propositional content (henceforth &amp;quot;logical form&amp;quot;) to a surface syntax tree via intermediate syntactic representations. The mappings are performed with linguistic operations, the context for which is primarily machine-learned. The leaf nodes of the resulting syntax tree contain all necessary information from which to generate the surface string. Other systems use machine learning techniques for sentence realization in generation. The Nitrogen system, for example, uses a word bigram language model to score and rank a large set of alternative sentence realizations (Langkilde and Knight, 1998a, 1998b). Other recent Smets, Gamon, Corston-Oliver and Ringger approaches use syntactic representations: FERGUS (Bangalore and Rambow 2000) and Halogen (Langkilde 2000, Langkilde-Geary 2002) use syntax trees as an intermediate representation to determine the optimal string output. The adaptation of German Amalgam to French has been discussed elsewhere (Smets et al. 2003). In this paper, we discuss French Amalgam in some detail by presenting two of the machine-learned models and a tool which allows the researcher to manually inspect the relevant training data for each model. In this way, the </context>
</contexts>
<marker>Langkilde, Knight, 1998</marker>
<rawString>Langkilde I. and Knight K. (1998b). Generation that exploits corpus-based statistical knowledge. Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics (COLING-ACL 1998). Montréal, Québec, Canada. 704-710.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Palmer</author>
</authors>
<title>Mood and Modality,</title>
<date>1986</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge. Ringger</location>
<marker>Palmer, 1986</marker>
<rawString>Palmer F. (1986) Mood and Modality, Cambridge University Press, Cambridge. Ringger E., Gamon M., Smets M., Corston-Oliver S. and Moore R. (in preparation) Linguistically informed models of constituent structure for ordering in sentence realization.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Smets</author>
<author>M Gamon</author>
<author>S Corston-Oliver</author>
<author>E Ringger</author>
</authors>
<title>The adaptation of a machinelearned sentence realization system to French, to appear in</title>
<date>2003</date>
<booktitle>Proceedings of the 10th conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<contexts>
<context position="1322" citStr="Smets et al. 2003" startWordPosition="176" endWordPosition="179">entence realization system. It presents in some detail two of the machine-learned models employed in Amalgam and shows how linguistic intuition and knowledge can be combined with statistical techniques to improve the performance of the models. Keywords – Mots Clés Réalisation de phrase, génération automatique, arbres de décision, français. Sentence realization, generation, machine-learning, decision trees, French. 1 Introduction Amalgam is a multilingual sentence realization system. Developed originally for German (Corston-Oliver et al. 2002, Gamon et al. 2002), it has been adapted to French (Smets et al. 2003). Amalgam maps a representation of propositional content (henceforth &amp;quot;logical form&amp;quot;) to a surface syntax tree via intermediate syntactic representations. The mappings are performed with linguistic operations, the context for which is primarily machine-learned. The leaf nodes of the resulting syntax tree contain all necessary information from which to generate the surface string. Other systems use machine learning techniques for sentence realization in generation. The Nitrogen system, for example, uses a word bigram language model to score and rank a large set of alternative sentence realizatio</context>
</contexts>
<marker>Smets, Gamon, Corston-Oliver, Ringger, 2003</marker>
<rawString>Smets M., Gamon M., Corston-Oliver S. and Ringger E. (2003) The adaptation of a machinelearned sentence realization system to French, to appear in Proceedings of the 10th conference of the European Chapter of the Association for Computational Linguistics, 2003.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>