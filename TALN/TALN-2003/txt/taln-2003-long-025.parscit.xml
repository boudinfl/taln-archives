<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R H Baayen</author>
</authors>
<title>Nous sommes très reconnaissant à Tamara Munzner pour la mise à disposition de loutil H3Viewer, utilisé dans ce travail. Références</title>
<date>2000</date>
<publisher>Kluwer Academic Publishers.</publisher>
<location>Dordrecht:</location>
<contexts>
<context position="10965" citStr="Baayen, 2000" startWordPosition="1675" endWordPosition="1676">la détection de « communautés » ou de « sources autorisées » sur le Web, ou la parallélisation des calculs. Malheureusement, les techniques développées dans ces secteurs ne sont pas directement exploitables, étant donné que lapplicabilité des heuristiques dépend des applications et des propriétés particulières des graphes analysés. La propriété fondamentale que nous exploiterons ici est le caractère zipfien du langage, rappelé plus haut, et qui touche à la fois la fréquence des mots eux-mêmes, la fréquence des usages de chaque mot, et la fréquence des cooccurrences (voir Zipf, 1945, et aussi Baayen, 2000). 3 Détection des composantes de forte densité Nous créons le graphe des cooccurrences en ne retenant que les mots apparaissant dans au moins 10 contextes, et les cooccurrences correspondant à un poids w en deçà dun certain seuil (0.9). Lalgorithme de détection part du principe que pour chacun des usages du mot considéré (« mot-cible »), un cooccurrent (ou « mot-racine ») a une fréquence plus élevée que les autres, en vertu de lorganisation zipfienne des cooccurrences que nous avons mentionnée Cartographie lexicale pour la recherche dinformations plus haut. Ainsi, pour barrage, le cooccurr</context>
</contexts>
<marker>Baayen, 2000</marker>
<rawString>Nous sommes très reconnaissant à Tamara Munzner pour la mise à disposition de loutil H3Viewer, utilisé dans ce travail. Références Baayen, R. H. (2000). Word frequency distributions. Dordrecht: Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N M Ide</author>
<author>J Véronis</author>
</authors>
<title>Introduction to the special issue on word sense disambiguation : the state of the art.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<pages>1--40</pages>
<marker>Ide, Véronis, 1998</marker>
<rawString>Ide, N. M., Véronis, J. (1998). Introduction to the special issue on word sense disambiguation : the state of the art. Computational Linguistics, (24)1, 1-40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Munzner</author>
</authors>
<title>Interactive visualization of large graphs and networks.</title>
<date>2000</date>
<institution>Stanford University.</institution>
<contexts>
<context position="23540" citStr="Munzner, 2000" startWordPosition="3508" endWordPosition="3509">s intuitif que ligne pour lusage « station de métro ». Lusage des distances entre le mot-cible et ses cooccurrents pourrait probablement améliorer les choses, comme pour le cas précédent. 5 Visualisation et navigation « hyperlexicale » La visualisation et la navigation à lintérieur de grands graphes est un domaine de recherche en pleine expansion, lié notamment à la nécessité de représenter de façon satisfaisante les liens entre serveurs ou entre hyperdocuments à lintention des administrateurs et webmasters. Lalgorithme H3 récemment développé par Tamara Munzner dans le cadre de sa thèse (Munzner, 2000) nous paraît lun des plus convaincants, et, contrairement à dautres, directement adaptable à notre problématique. Un outil qui implémente cet algorithme, H3Viewer, est de plus disponible4, et permet une expérimentation immédiate. Le problème de la représentation de grands graphes dans un espace euclidien est tout dabord le « manque de place » de lespace euclidien, puisque le nombre de nuds dun arbre, par exemple, croît de façon exponentielle avec la profondeur, alors que lespace disponible pour les représenter croît seulement de façon polynomiale par rapport au rayon dun cercle ou dun</context>
</contexts>
<marker>Munzner, 2000</marker>
<rawString>Munzner, T. (2000). Interactive visualization of large graphs and networks. Ph. D. Dissertation, Stanford University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Schütze</author>
</authors>
<title>Automatic word sense discrimination.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<pages>97--124</pages>
<contexts>
<context position="4043" citStr="Schütze (1998)" startWordPosition="599" endWordPosition="600">(« action de barrer », par exemple), et rien ne garantit quelles reflètent le contenu exact du corpus textuel interrogé. Nous avons montré de façon expérimentale la difficulté pour des linguistes de faire correspondre correctement les « sens » dun dictionnaire et les occurrences dun corpus (Véronis, 1998). De plus, il resterait à catégoriser automatiquement les documents de la base de textes en fonction des « sens » du dictionnaire, tâche dune difficulté extrême, qui élude les efforts soutenus de la recherche depuis un demi-siècle (cf. [Ide, Véronis, 1998] pour un état de lart détaillé). Schütze (1998) a proposé une méthode permettant dextraire automatiquement la liste des « sens » (nous préférons parler d« usages ») du corpus lui-même, tout en fournissant une technique robuste de catégorisation. Ces usages correspondent à des groupes (clusters) de contextes similaires dans un espace de très grande dimensionnalité formé par des vecteurs de mots ou de cooccurrences proches du mot à désambiguïser, espace rendu utilisable par une technique de décomposition en valeurs singulières classique. Les techniques basées sur les vecteurs de mots se heurtent toutefois à une difficulté majeure et rédhib</context>
</contexts>
<marker>Schütze, 1998</marker>
<rawString>Schütze, H. (1998). Automatic word sense discrimination. Computational Linguistics, (24)1, 97-124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Véronis</author>
</authors>
<title>A study of polysemy judgements and inter-annotator agreement, Programme and advanced papers of the Senseval workshop (pp. 2-4). Herstmonceux Castle</title>
<date>1998</date>
<location>(England)</location>
<contexts>
<context position="3738" citStr="Véronis, 1998" startWordPosition="550" endWordPosition="551">uête cherche à retourner pêle-mêle des pages parlant de barrages revendicatifs de camions, ou de barrages dhommes en armes à des frontières ou des zones en conflit. Les dictionnaires classiques sont peu adaptés à la tâche. Ils contiennent la plupart du temps des définitions dune trop grande généralité (« action de barrer », par exemple), et rien ne garantit quelles reflètent le contenu exact du corpus textuel interrogé. Nous avons montré de façon expérimentale la difficulté pour des linguistes de faire correspondre correctement les « sens » dun dictionnaire et les occurrences dun corpus (Véronis, 1998). De plus, il resterait à catégoriser automatiquement les documents de la base de textes en fonction des « sens » du dictionnaire, tâche dune difficulté extrême, qui élude les efforts soutenus de la recherche depuis un demi-siècle (cf. [Ide, Véronis, 1998] pour un état de lart détaillé). Schütze (1998) a proposé une méthode permettant dextraire automatiquement la liste des « sens » (nous préférons parler d« usages ») du corpus lui-même, tout en fournissant une technique robuste de catégorisation. Ces usages correspondent à des groupes (clusters) de contextes similaires dans un espace de tr</context>
<context position="15089" citStr="Véronis, 1998" startWordPosition="2297" endWordPosition="2298">x débat congres renseignement dommage réaction... ⇒ MATCH : vainqueur victoire rencontre qualification tir football... Après cette étape, plus aucun mot ne satisfait les conditions. Au total, HyperLex a donc déterminé quatre mots-racines, eau, routier, frontière, match, qui reflètent bien les usages de barrage dans le corpus. Jean VERONIS 4 Expérimentation Nous avons expérimenté HyperLex sur 10 mots très polysémiques (Tableau 2), choisis parmi ceux qui ont servi de mots-tests lors de laction de désambiguïsation Romanseval, et qui avaient posé de grandes difficultés à des annotateurs humains (Véronis, 1998) . Un souscorpus de pages Web a été constitué pour chacun de ces mots, à laide du méta-moteur Copernic Agent2, en interrogeant tout dabord la forme singulier, puis la forme pluriel. Les pages obtenues ont été filtrées de façon à éliminer les pages qui ne contenaient pas le mot cherché (erreurs du type « Page not found », par exemple), ainsi que les doublons. Mot Pages Contextes Graphe Brutes Utiles Bruts Utiles Nuds Arêtes BARRAGE 1702 1372 7256 6924 1203 6138 DETENTION 2112 1270 8902 8728 1418 19007 FORMATION 5974 1590 5248 4885 542 1531 LANCEMENT 2828 1231 3307 3174 617 2521 ORGANE 2786 9</context>
</contexts>
<marker>Véronis, 1998</marker>
<rawString>Véronis, J. (1998). A study of polysemy judgements and inter-annotator agreement, Programme and advanced papers of the Senseval workshop (pp. 2-4). Herstmonceux Castle (England) [http:/www.up.univ-mrs.fr/veronis/pdf/1998senseval.pdf].</rawString>
</citation>
<citation valid="true">
<authors>
<author>G K Zipf</author>
</authors>
<title>The meaning-frequency relationship of words.</title>
<date>1945</date>
<journal>Journal of General Psychology,</journal>
<volume>33</volume>
<pages>251--266</pages>
<contexts>
<context position="4749" citStr="Zipf, 1945" startWordPosition="706" endWordPosition="707">ons parler d« usages ») du corpus lui-même, tout en fournissant une technique robuste de catégorisation. Ces usages correspondent à des groupes (clusters) de contextes similaires dans un espace de très grande dimensionnalité formé par des vecteurs de mots ou de cooccurrences proches du mot à désambiguïser, espace rendu utilisable par une technique de décomposition en valeurs singulières classique. Les techniques basées sur les vecteurs de mots se heurtent toutefois à une difficulté majeure et rédhibitoire : la très grande différence de fréquence entre usages dun même mot (déjà constatée par Zipf, 1945) repousse la plupart des distinctions utiles en-dessous du seuil de bruit du modèle. Ainsi, selon nos estimations, lusage « match de barrage » concerne moins de 1% des documents contenant le mot barrage. Nous proposons dans cette communication un algorithme radicalement différent, basé sur les propriété des graphes constitués par les cooccurrences de mots. Cet algorithme, HyperLex, permet dextraire des composantes de forte densité qui reflètent les différents usages des mots. Contrairement aux méthodes vectorielles, lalgorithme HyperLex est peu sensible à la fréquence relative et au nombre </context>
<context position="10941" citStr="Zipf, 1945" startWordPosition="1671" endWordPosition="1672">dans les secteurs de la détection de « communautés » ou de « sources autorisées » sur le Web, ou la parallélisation des calculs. Malheureusement, les techniques développées dans ces secteurs ne sont pas directement exploitables, étant donné que lapplicabilité des heuristiques dépend des applications et des propriétés particulières des graphes analysés. La propriété fondamentale que nous exploiterons ici est le caractère zipfien du langage, rappelé plus haut, et qui touche à la fois la fréquence des mots eux-mêmes, la fréquence des usages de chaque mot, et la fréquence des cooccurrences (voir Zipf, 1945, et aussi Baayen, 2000). 3 Détection des composantes de forte densité Nous créons le graphe des cooccurrences en ne retenant que les mots apparaissant dans au moins 10 contextes, et les cooccurrences correspondant à un poids w en deçà dun certain seuil (0.9). Lalgorithme de détection part du principe que pour chacun des usages du mot considéré (« mot-cible »), un cooccurrent (ou « mot-racine ») a une fréquence plus élevée que les autres, en vertu de lorganisation zipfienne des cooccurrences que nous avons mentionnée Cartographie lexicale pour la recherche dinformations plus haut. Ainsi, p</context>
</contexts>
<marker>Zipf, 1945</marker>
<rawString>Zipf, G. K. (1945). The meaning-frequency relationship of words. Journal of General Psychology, 33, 251-266.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>