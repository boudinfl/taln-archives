Bases de connaissances pour asseoir la crédibilité
des reponses d'un systéme de Q/R

L. Gillard, P. Bellot, M. El-Beze

Laboratoire d'Informatique d'Avignon (LIA)
339 ch. des Meinajaries, BP 1228 ; F-84911 Avignon Cedex 9 (France)
{ laurent.gillard, patrice.bellot, marc.elbeze }@lia.univ-avignon.fr

Resume — Abstract

Cet article presente un prototype de Question/Reponse (Q/R) impliquant un ensemble de bases de
connaissances (BC) dont l'objectif est d'apporter un credit supplementaire aux reponses candidates
trouvees. Ces BC et leur inﬂuence sur la strategie d’ordonnancement mise en oeuvre sont decrites dans le
cadre de la participation du systeme a la campagne Q/R de TREC-2002.

This paper presents a Question-Answering system using Knowledge Databases (KDB) to validate answers
candidates.

Mots Clés — Keywords
Systeme de Question/Reponse, Bases de Connaissances

Question Answering system, Knowledge Databases.

1 Introduction

Avec l’explosion de la quantite d'information disponible, les systemes de recherche d'information
sont devenus les principaux moyens d'acces a l'information. Pourtant, nombre d'entre eux
presentent deux faiblesses : d'une part, le nombre de documents retoumes en reponse a une
requéte utilisateur est souvent trop important ; d'autre part, c'est toujours a l'utilisateur de
localiser l'information dont il a besoin dans les documents renvoyes. Les systemes de
question/reponse (Q/R) fournissent une alternative interessante a ces faiblesses : ils proposent de
renvoyer directement a l'utilisateur LA reponse a LA question posee.

Ainsi, depuis 1998, la campagne internationale << Text Retrieval Conference >> (TREC) propose
un sous-volet thematique Q/R (« QA Track») dont le but est de faire avancer les techniques
employees dans ces systemes, notamment en proposant un referentiel et une methodologie
permettant leur evaluation et leur comparaison.C’est dans le cadre de TREC-2002 (Voorhees E.,
2002), que le LLA s’est interesse pour la premiere fois a la problematique Q/R.

Cet article decrit le prototype Q/R du LIA a travers une presentation chronologique de ses divers
constituant (en section 2). Enﬁn, l'originalité de ce papier tient en la presentation d'un composant
supplementaire : un module de bases de connaissances (BC, section 3) et son inﬂuence (section
4) sur les resultats obtenus lors de notre participation (Bellot et al., a paraitre) a TREC-2002. En
effet, pour certaines questions de << culture generale >>, il peut paraitre interessant de doter un
systeme Q/R d’un ensemble de reponses calculees a l’avance aﬁn d’en faciliter la recherche ou,
comme dans le cas de notre prototype, d’en asseoir la conﬁance.

2 Prototype QR du LIA

Etiquetage des questions : les systemes Q/R precurseurs comme celui de (Kupiec, 1993) ou les
differents systemes participants a TREC-QA integrent tous un module charge de classer les

questions (les différences résident dans les moyens d’effectuer la classiﬁcation et dans sa
granularité). A partir de la forme interrogative d’une question, il est généralement possible de
déterminer un ou plusieurs types de réponses attendues et, par conséquent, d’en faciliter la
recherche en établissant une correspondance directe entre la nature de la réponse attendue et celle
d’une entité nommée détectée. En plus de l’étiquetage, certains systemes comme (Ferret et al.,
2002) Vont plus loin dans le traitement de la question : ils identifient au sein d’une question un
<< focus >> (l’objet sur lequel porte la question), des modificateurs du << focus >> (les inforrnations
qui le qualifient) ou encore un ensemble de mots clés a trouver dans le Voisinage de la réponse.

De méme, le prototype Q/R du LLA affecte a chaque question une ou plusieurs étiquettes
(classées par ordre de probabilité parrni une hiérarchie d’étiquettes déduites des questions de
TREC-9 et 10) ou méme « unknown » en cas d’indécision. L’étiquetage de la question se fait de
préférence a partir d’un étiqueteur a base de regles (reposant sur des motifs constitués par des
mots de la question ou leur étiquette syntaxique), et sinon a partir d’un étiqueteur probabiliste
(Béchet et al., 2000).

Recherche des documents susceptibles de contenir la réponse : le prototype Q/R du LIA ne
contient pas de tel module mais dépend de la liste des 1000 premiers documents proposés par les
organisateurs de la campagne en considérant la question brute comme requéte. Il est a noter qu’il
n’y a aucune garantie que la réponse a la question soit effectivement comprise dans le jeu de
documents foumis méme si elle est comprise dans le reste du corpus de référence.

Reconnaissance des entités nommées: ce travail a été effectué par la société Sinequa
(http://www.sinequa.com). Les entités sont détectées a l'aide d'une cascade de transducteurs
utilisant le contexte avoisinant (mots et étiquettes syntaxiques) ainsi que des traits sémantiques
présents dans des lexiques. Une normalisation des noms propres ainsi qu'une résolution
élémentaire d'anaphore (pour les pronoms personnels << il >> et << elle >>) a également été faite.

Sélection de la réponse: l’approche du systeme QR du LLA pour la sélection des réponses
candidates repose sur l’utilisation conjointe du moteur de recherche vectoriel SMC (Bellot & El-
Beze, 2000) et sur la localisation d’une entité du type recherché. Ainsi, dans un premier temps,
l’ensemble des phrases de tous les documents est ordonné suivant le calcul d’une proximité avec
la question. Ensuite, un filtrage est effectué afin de conserver seulement les phrases contenant au
moins une entité correspondant au type attendu et les noms propres apparaissant éventuellement
dans la question. La réponse du systeme est alors la premiere entité du type attendu apparaissant
dans la phrase. Néanmoins, cette facon de procéder a au moins deux faiblesses: l’étape de
filtrage, en écartant toutes les phrases de maniere trop draconienne, a parfois rendu impossible
l’extraction d’une réponse ; enfin, la sélection de la << premiere >> entité pose probleme dans le cas
ou plusieurs entités du méme type sont présentes dans la méme phrase.

3 BC pour crédibiliser et ordonner les réponses

3.1 Bases de connaissances

En raison des nouvelles contraintes introduites lors de la campagne TREC-ll : celle pour un
systeme Q/R d'éValuer la << confiance>> qu'il a en une réponse, mais aussi celle d’ordonner
l’ensemble de celles-ci ar ordre de << certitude >> ' il nous a aru intéressant d’associer a notre
5
prototype Q/R une source fiable de réponses déja connues. Cette source a été modélisée au
travers d’un module de bases de connaissances.

En effet, la présence de telles bases permet de crédibiliser une réponse candidate. Ainsi, pour
notre participation a TREC-ll, ce module a simplement eu une fonction de Validation des
réponses suggérées par le reste du systeme.

De plus, le choix d’inclure un module de BC a été conforté par un examen des questions des
précédentes campagne TREC-QA, puisqu’il est aisément possible de constater qu’un certain
nombre d’entre elles releve de sujets assimilables a des thématiques de << culture générale>>
(géographie, dates et faits historiques, noms d’écrivains et d’oeuvres, etc.) mais également que
ces sujets sont fréquents (au sein d’une méme campagne ou sein de l’ensemble de celles-ci).

Il est a noter que de telles BC peuvent étre utilisées comme des listes de réponses préenregistrées
qui serviraient alors de support a une fouille dans un corpus de référence. L’un des buts du critere
de réponse supportée était d’ailleurs d’éviter dans la mesure du possible un systeme Q/R
completement architecturé sur ce principe. Cependant, dans un tel cas, il est tout de méme
intéressant de se demander, notamment au niveau applicatif, quel est l’intérét de chercher a
nouveau une réponse (ou une autre formulation d'une réponse) qui est déja connue.

A plus long terme, l'objectif visé est de disposer d’une source de connaissances de qualité pour
servir d'amorces a des itérations dont le but est d'extraire des connaissances similaires (motifs
d'extraction, motifs de réponses ; Soubbotin & Soubbotin, 2001 ; Ravichandran & Hovy, 2002)
soit a l’intérieur du corpus, soit a partir d’Internet; mais surtout d’évaluer et comparer la
crédibilité de différentes réponses candidates (notamment au travers des relations permettant de
l'établir ou liant ses différents constituants).

3.1.1 Construction des BC

Les << connaissances >> ont été extraites de tableaux et de listes provenant de différents sites
Internet de nature encyclopédique. En fait, l’un des principaux criteres était que ces informations
soient présentées sous une forme fortement structurée pour en faciliter l’extraction (manuelle) et
qu’elles soient en nombre suffisant. En outre, puisque ces connaissances sont destinées a
corroborer une réponse candidate extraite par ailleurs d’un document du corpus, le critere de la
fiabilité de leur provenance qui pourrait paraitre primordial de prime abord ne l’est pas. La
fiabilité provient plut6t de la convergence entre une réponse candidate et sa présence dans les
BC. 11 est assez improbable qu'un document donne une méme réponse fausse que celle contenue
dans des BC de large couverture (tant du point de vue des questions que des réponses). Dans un
tel cas, et si cette réponse est considérée comme

- - - - TREC
1ncorrecte, 11 est alors possible de se demander s1 les Thérnatique 8 9 10 11 Total
capacités des systemes a évaluer ne sont pas surestimées.      
Hymne 0+0 0+0 0+0 1+0 1+ 0
Arbre 0+0 1+0 o+o 0+0 1+ 0
3.1.2 Contenu et couverture des BC oiseau 0+0 1+0 3+0 1+0 5+ 0

_ , , , _ ‘ Gouverneur 0+0 0+0 1+0 3+0 4+0
Le Ch0lX du contenu des BC a ete etabl1 d’apres un Creation 0+0 1+0 3+0 2+0 s+o

- 7 - Capitale 1+5 2+1 0+5 1+6 4+17
examen rap1de de lensemble des questions des Population 0+4 4+5 1+4 1+3 6,16

campagnes TREC, en fonction : Président 1+1 2+1 4+0 5+0 12+ 2
Total 2+10 11+7 15+10 15+9 43+36
0 de la constance des thématiques d’interrogation, Tab'ea“ 1 3C°“"e"““+ des 50 “ USA” * “"95

cela afin d’obtenir un taux de couverture suffisant sur l’ensemble des précédentes campagnes
TREC-QA (un pari a été fait sur le fait que ce taux reste stable sur le corpus TREC-2002) ;

0 du fait que la réponse attendue met en jeu des n-uples d’entités nommées (par exemple :
pays/capitale, nom de l’inventeur/date/invention, etc.).

Ainsi, une partie des BC porte sur des lieux géographiques comme : des cours d’eau et les pays
traversés, les capitales des pays ; mais aussi des personnages célebres comme : les prix Nobels
par année et disciplines, des écrivains et leurs oeuvres, les dieux des panthéons grecs, romains ;
ou encore des couples de mots : abréviations et definitions, etc.

De méme, dans le cadre des TREC-QA, 43 questions ont pour principal objet les Etats-Unis
d’Amérique et portent notamment sur les attributs de ces états : capitales, nom des gouvemeurs,

emblemes (oiseau, arbre, ﬂeur), etc. Ces sujets ont également donné lieu a la création de BC et,
autant que possible, ces bases ont été enrichies par la Version << extra-américaine >> (cf Tableau
I ,' pour une thématique, le chiﬁie avant le + représente le nombre de questions centrées sur les
USA et le chiﬁie apres le + correspond au nombre portant sur a’ ’autres pays).

Enﬁn, il est apparu que certaines questions revenaient sous une forme identique lors des
campagnes TREC suivantes. Aussi chacun des jeux des couples Q/R des TREC-QA précédents
peut étre considéré comme une base d'archiVes pour la campagne suivante. Il est a noter qu'il
s'agit la d'une approche sujette a caution : la réponse a une méme question peut Varier d'année en
année, tout comme son degré d'exactitude qu'il s'agisse d'exigences provenant de l'éValuation ou
méme de Variation dans le jugement de la notion d'exactitude (Lavenus & Lapalme, a paraitre).

Au ﬁnal, le module de BC comprend 36 bases de connaissances thématiques et assure une
couverture de l’ordre de 10% a 12% des réponses par

TREC 8 9 10 11 Total
campagne TREC (cf Tableau 2). # Réponses muses
dans les BC 22 73 64 61 220
4 Stratégie d'ordonnancement des #°ue-°~“°"s 20° 694 50° 5°° 1894
 %des Q couvertes 11.0 10.5 12.8 12.2 11.6

Tableau 2 : Couverture globale des BC

Dans le prototype Q/R du LIA, les informations
disponibles qui permettent d'établir une préférence dans les réponses sont les suivantes :

0 une étiquette de réponse attendue ainsi que sa probabilité associée. Cette affectation a lieu au
niveau du module d'étiquetage. En cas d'échec ou d'incapacité, cette étiquette peut étre
« unknown » et dans ce cas le systeme ne << sait plus >> répondre (car le processus de sélection
repose sur la correspondance << étiquette attendue<—>entité nommée >>) ;

0 le fait que les bases de connaissances corroborent une réponse trouvée par SLAC. Cette
réponse peut étre jugée comme ﬁable puisque proposée par deux systemes indépendants ;

- , , , ,
. S1 au,cune relgonse n a e:[e Groupe Modmes impﬁqués Ventilation sur TREC-QA 2002
trouvee, la reponse donnee dans Ie ... de  _ _ _
‘ Posltlon Commentalres
par 16 Systeme est «  »- 1 SIAC & bases de d 1 . 30 Accord entre deux sous modules
_ connaissances e a done réponses jugées fiables
Cependant, dans ce dern1er cas, za de 31 5, 424
une nuance dolt étre apportée pour 2b SIAC de 425 a 473 Réponse « NIL » potentiellement
notre prototype  En effet, abusive car flltrage draconlen.
n m 1 ’  n‘ ’ ’ . , . , Réponse « NIL » car systéme
auf’? odu, e, Spa‘? que 3" ete 3 Q“eS”°”S 13°” ‘°"”‘7”e’ees de 474 a 500 incapable de choisir une
cree pour verifier labsence dune (“"7 ”°W””) réponse.

réponse  ne comprend pas non Tab|eau3:Stratégie d'ordonnancement

plus de processus itératif qui perrnettrait de relacher des contraintes afin d'élargir la liste des
réponses envisageables. Et, pire, il lui arrive parfois d'écarter abusivement les réponses proposées
par SIAC, en particulier durant une étape de ﬁltrage lorsque aucune entité nommée du type
attendu n'est présente dans la phrase retoumée. Ainsi, il apparait que le fait qu'une réponse ne
peut étre trouvée ne doit pas étre considérée comme une absence catégorique de réponse dans le
corpus mais juste comme une présomption d'absence.

A partir de ces considérations, une stratégie (cf Tableau 3) a été mise au point pour ordonner les
réponses du systeme et notamment positionner en premier les réponses crédibilisées par les BC.

A l’intérieur de chacun des 1” et 2nd groupes, les réponses sont ordonnées d'abord suivant la
difficulté de reconnaissance des entités nommées correspondant a l'étiquette attendue (ordre
obtenu expérimentalement) et ensuite suivant la probabilité décroissante de ces étiquettes. Enﬁn,
les réponses du 3° groupe peuvent étre considérées comme des réponses << NIL >> par défaut.

5 Résultats obtenus et apport des

Nombre de réponses fausses (W) 440
BC Nombre de réponses non supportées (U) 4
Nombre de réponses inexactes (X) 4
r r - Nombre de réponses correctes (R) 52
Le tableau 4 presente les resultats officiels obtenus « conﬁdence_We,.ghtedscore» (CW3) 0146
par le prototype Q/R du Lm a la campagne TREC- Précision des réponses « NIL » 7/75 :o.o93
QA de 2002. Notre soumission a obtenu un score de RaP|°e' des ’éP°”SeS “ N“-3> _ _ 7/ 46:0-152
Tableau 4 : Resultats offlclels

0.246 en répondant correctement a 52 questions sur
les 500 de la campagne.

Apres analyse, les BC interviennent dans 30 des Q/R et permettent d'asseoir correctement la
crédibilité de 24 réponses. Sur les six restantes et jugées non << correctes >> :

0 trois d’entre elles sont non supportées. C’est-a-dire que les BC permettent de corroborer une
bonne réponse mais que le document dont SLAC l’extrait ne permet pas de l’établir. Il s’agit
d’un effet pervers d’une sélection des réponses candidates basée seulement sur le type de
l’étiquette ;

0 deux sont incorrectes pour des problemes d’ordre de grandeur. Il s’agit de questions portant
sur les populations << actuelles >> de Mexico et d’Afrique du Sud. Les réponses retoumées
(soulignées) bien que compatibles avec les données des BC, sont des populations a une
(( échéance )) 51 savoir « Mexico's population will reach 100 million in the year 2000. » et « it expected the
total population to be more than 42 million people, based on the census in 1991, when South Africa's total
population was estimated to be 38 million. ». Les motifs des réponses correctes attendues étaient
respectivement « 98.1 million » et « (37.9l38l4I.8l42) million ».

0 enfin, la derniere « When did George W. Bush get elected as the governor of Texas? » s’est Vue
répondre par la date de la prise de fonction (« In January 1995 , when George W Bush took oﬁice as
governor ») plut6t que celle de l’élection (les réponses correctes « 1994 » ou « 1998 »).

Apres que les motifs des réponses ont été communiqués, nous avons évalué notre prototype sans
le module de BC : il n'identifie alors plus que 34 réponses correctes. Cette différence s'explique
par le fait que, dans ce cas, l'entité nommée extraite de la phrase retoumée est la premiere du type
attendue et non plus celle issue de la BC.

Enfin, le composant d'ordonnancement utilisé pour notre soumission comportait une erreur : la
probabilité décroissante des étiquettes n'était pas prise en compte comme initialement prévu sur
les groupes 2 et 3. Cette correction a entrainé une hausse d'enViron 9% dans le score (de 0,246 a
0,268) et a une inﬂuence encore plus ﬂagrante dans le cas d'une utilisation sans BC puisque la
progression est alors de +65% (de 0,084 a 0,139).

La ﬁgure 1 (page suivante, d’apres Voorhees, 2002) permet de Visualiser a quel point, une bonne
stratégie d’ordonnancement permet de s’approcher d’une courbe idéale (en noir) dans le cadre
d’une évaluation TREC-2002. Ainsi, bien que notre prototype réponde a seulement 52 questions
il parvient assez bien a classer ses bonnes réponses. En “ S°°'e“°WS" /""44

effet, un point situé plus haut et plus a gauche d’un autre
est un systeme qui ordonne mieux ses réponses qu’il ne

répond correctement.

M-

   

6 Conclusion 

Pour notre premiere participation a la piste TREC-QA,
notre objectif était de créer un prototype qui nous 4H—
permette d’étudier la problématique des systemes Q/R,
mais également d’enVisager les premieres briques d’une /. ."-°
stratégie mettant en avant la crédibilité des réponses. /
Certains modules de notre systeme Q/R doivent étre

pireﬂpossible

/ 9",  scores observéso
-.

°  nombre de réponses correctes
Hm Mn

4» lm Jun Sm»

améliorés comme celui concernant l’étiquetage des questions mais cela n’était pas une priorité
pour notre premiere participation a TREC.

Ainsi, les résultats obtenus dénotent une bonne orientation puisque les réponses trouvées le sont
avec une confiance élevée et cela grace a l’usage des BC comme moyen de certification. En
outre, dans le cadre de TREC-QA, il est possible d’inclure aisément un sous-ensemble des Q/R
d’au minimum 10% dans de telles bases (principalement les questions portant sur des
n—uples d’entités nommées ou des faits établis).

Il faut cependant nuancer l’usage de ces BC. En effet, si elles remettent partiellement en cause le
principe de la tache de Q/R, il apparait rapidement qu’il est difficile de constituer des bases
exhaustives (toutes les questions n’autorisent pas des réponses pré-calculées). Cela est d’autant
plus Vrai dans le cadre des questions ouvertes puisque leur domaine n’est pas connu (pour notre
prototype, une thématique donne lieu a une BC). De méme, dans le cadre du probleme de leur
couverture l’intérét d’utiliser de telles bases plut6t qu’une redondance issue d’Internet est a
démontrer. Et, comme pour toutes les bases de données, se pose le probleme de leur durée de Vie,
de leur mise a jour mais aussi de leur intégration/utilisation dans d’autres langues que celle de
leur contenu.

Références

Béchet F., Nasr A., Genet F. (2000), “Tagging Unknown Proper Names Using Decision Trees”, in Proc.
ofACL'2000, Hong-Kong, Chine, p.77-84.

Bellot P., Crestan E., El-Beze M., Gillard L., de Loupy C. (a paraitre), “Coupling Named Entity
Recognition, Vector—Space Model and Knowledge Bases for TREC—II Question Answering Track” in
Proceedings of the llﬂ‘ Text REtrieval Conference.

Bellot P. & El-Beze M. (2000), “Clustering by means of decision trees without learning or hierarchical
and K—Means like algorithms”, RIAO'2000, Paris, p.344-363.

Ferret 0., Grau B., Hurault-Plantet M., Illouz G., Monceaux L., Robba 1., Vilnat A. (2002), « Recherche
de la réponse fondée sur la reconnaissance du focus de la question », TALN2002, 24-27 juin Nancy,
p.307-316.

Kupiec J. (1993), ‘Murax.' A robust linguistic approach for question answering using an on—line
encyclopedia”, in Proceedings of the 16th annual international ACM SIGIR conference, Pittsburgh, PA,
USA, pp. 181-190.

Lavenus K. & Lapalme G. (a paraitre), « Evaluation des syste‘mes de question réponse — Aspects
méthodologiques », dans la revue Traitement Automatique des Langues.

Ravichandran D. & Hovy E.H. (2002), “Learning Surface Patterns for a Question Answering System.
Proceedings of the ACL conference, Philadelphie, USA, p. 41-47.

Soubbotin M.M. & Soubbotin S.M. (2001), "Patterns of Potential Answer Expressions as Clues to the
Right Answers" in Proceedings of the 10*” Text Retrieval Conference (TREC-10), p.175-182,
Gaithersburg, Maryland.

Voorhees E. (2002), “Overview of the TREC 2002 Question Answering Track”,
http://trec.nist.gov/pubs/trec1 1/papers/QA1 1.pdf.

1:

In

