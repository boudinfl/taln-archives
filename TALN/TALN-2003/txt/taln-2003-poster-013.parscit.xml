<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Atwell</author>
<author>J Hughes</author>
<author>C Souter</author>
</authors>
<title>Amalgam: Automatic Mapping among Lexicogrammatical Annotation Models.</title>
<date>1994</date>
<tech>Technical report, Internal Paper,</tech>
<institution>CCALAS, Leeds University.</institution>
<contexts>
<context position="2175" citStr="Atwell et al. (1994)" startWordPosition="288" endWordPosition="291">issage automatique, arbres décisionnels, dictionnaires automatiquement appris. Lexical learning, machine learning, decision trees, learned dictionaries. 1 Introduction In this paper we describe the use of a particular machine learning technique, decision trees (DTs), to acquire lexical information in a broad-coverage linguistics-based natural language processing (NLP) system. The manual encoding of lexical information into an online dictionary is costly and timeconsuming. Therefore, the NLP community has used various machine learning (ML) techniques to automatically learn lexical information (Atwell et al. (1994), Teufel (1995), Stevenson and Merlo (1997), Van Halteren et al. (1998), Brill and Wu (1998), Schulte im Walde (1998), Stevenson et al. (1999), Zavrel and Daelemans (2000), Van Halteren et al. (2001), among others). Most approaches consist of ML techniques that make use of linear Marisa Jiménez and Martine Pettenaro context (n-grams extracted from text) and that are not integrated in a rich NLP system. By contrast we use a particular ML technique, DTs, that exploits a deep knowledge understanding system to learn lexical information, which in turn immediately benefits our system. Our goal was t</context>
</contexts>
<marker>Atwell, Hughes, Souter, 1994</marker>
<rawString>Atwell, E., J. Hughes, and C. Souter (1994). Amalgam: Automatic Mapping among Lexicogrammatical Annotation Models. Technical report, Internal Paper, CCALAS, Leeds University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Brill</author>
<author>J Wu</author>
</authors>
<title>Classifier Combination for Improved Lexical Disambiguation.</title>
<date>1998</date>
<booktitle>COLING-ACL&apos;98. Montreal, Canada. Chikering, D. Max (n.d.) WinMine Toolkit Home</booktitle>
<location>Page: http://www.research.microsoft.com/~dmax/WinMine/Tooldoc.htm.</location>
<contexts>
<context position="2267" citStr="Brill and Wu (1998)" startWordPosition="303" endWordPosition="306">ng, machine learning, decision trees, learned dictionaries. 1 Introduction In this paper we describe the use of a particular machine learning technique, decision trees (DTs), to acquire lexical information in a broad-coverage linguistics-based natural language processing (NLP) system. The manual encoding of lexical information into an online dictionary is costly and timeconsuming. Therefore, the NLP community has used various machine learning (ML) techniques to automatically learn lexical information (Atwell et al. (1994), Teufel (1995), Stevenson and Merlo (1997), Van Halteren et al. (1998), Brill and Wu (1998), Schulte im Walde (1998), Stevenson et al. (1999), Zavrel and Daelemans (2000), Van Halteren et al. (2001), among others). Most approaches consist of ML techniques that make use of linear Marisa Jiménez and Martine Pettenaro context (n-grams extracted from text) and that are not integrated in a rich NLP system. By contrast we use a particular ML technique, DTs, that exploits a deep knowledge understanding system to learn lexical information, which in turn immediately benefits our system. Our goal was to prove that DTs offer many advantages as a lexical information acquisition technique: ! The</context>
</contexts>
<marker>Brill, Wu, 1998</marker>
<rawString>Brill, E. and J. Wu (1998). Classifier Combination for Improved Lexical Disambiguation. COLING-ACL&apos;98. Montreal, Canada. Chikering, D. Max (n.d.) WinMine Toolkit Home Page: http://www.research.microsoft.com/~dmax/WinMine/Tooldoc.htm.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Pentheroudakis</author>
</authors>
<title>Lex Rules!.</title>
<date>2001</date>
<tech>Technical report, Microsoft Research.</tech>
<contexts>
<context position="4684" citStr="Pentheroudakis (2001)" startWordPosition="679" endWordPosition="680">taining to them, and part-of-speech probabilities. Our system has two main techniques in place to perform lexical learning. The first technique consists of linguistic rules that allow us to acquire lexical information at any stage of sentence processing: either morphological, named entity, syntactic, or logical-form processing may be adequate depending on the type of information we are interested in; for example, we might want to learn the feature count noun through plural nouns found in the data. This information is stored in learned dictionaries that supplement the general dictionary (see Pentheroudakis (2001), and Wu et al. (2002)). Any subset of learned dictionaries can be used depending on corpus-specific needs, and their entries can also be permanently merged with the main dictionary if desired. The second technique consists of lexical learning through the use of DTs. The tools that we use to build our DTs are borrowed from the publicly available WinMine toolkit (Chickering, n.d.), developed at Microsoft Research. The WinMine tools automatically split the data into training and testing (70/30), and produce several DT models at different levels of granularity. DTs provide a classification of sel</context>
</contexts>
<marker>Pentheroudakis, 2001</marker>
<rawString>Pentheroudakis, J. (2001). Lex Rules!. Technical report, Microsoft Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Schulte im Walde</author>
<author>S</author>
</authors>
<title>Automatic Semantic Classification of Verbs according to their Alternation Behaviour.</title>
<date>1998</date>
<tech>AIMS Report 4(3), IMS,</tech>
<institution>Universität Stuttgart.</institution>
<marker>Walde, S, 1998</marker>
<rawString>Schulte im Walde, S., 1998. Automatic Semantic Classification of Verbs according to their Alternation Behaviour. AIMS Report 4(3), IMS, Universität Stuttgart.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Stevenson</author>
<author>P Merlo</author>
</authors>
<date>1997</date>
<booktitle>Lexical Structure and Processing Complexity. Language and Cognitive Processes,</booktitle>
<pages>12--1</pages>
<contexts>
<context position="2218" citStr="Stevenson and Merlo (1997)" startWordPosition="294" endWordPosition="297">ls, dictionnaires automatiquement appris. Lexical learning, machine learning, decision trees, learned dictionaries. 1 Introduction In this paper we describe the use of a particular machine learning technique, decision trees (DTs), to acquire lexical information in a broad-coverage linguistics-based natural language processing (NLP) system. The manual encoding of lexical information into an online dictionary is costly and timeconsuming. Therefore, the NLP community has used various machine learning (ML) techniques to automatically learn lexical information (Atwell et al. (1994), Teufel (1995), Stevenson and Merlo (1997), Van Halteren et al. (1998), Brill and Wu (1998), Schulte im Walde (1998), Stevenson et al. (1999), Zavrel and Daelemans (2000), Van Halteren et al. (2001), among others). Most approaches consist of ML techniques that make use of linear Marisa Jiménez and Martine Pettenaro context (n-grams extracted from text) and that are not integrated in a rich NLP system. By contrast we use a particular ML technique, DTs, that exploits a deep knowledge understanding system to learn lexical information, which in turn immediately benefits our system. Our goal was to prove that DTs offer many advantages as a</context>
</contexts>
<marker>Stevenson, Merlo, 1997</marker>
<rawString>Stevenson, S. and P. Merlo (1997). Lexical Structure and Processing Complexity. Language and Cognitive Processes, 12(1-2):349-399.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Stevenson</author>
<author>P Merlo</author>
<author>N Karaeva</author>
<author>K Whitehouse</author>
</authors>
<title>Supervised Learning of Lexical Semantic Verb Classes using Frequency Distributions. Procs of SigLex 99,</title>
<date>1999</date>
<location>College Park, Maryland.</location>
<contexts>
<context position="2317" citStr="Stevenson et al. (1999)" startWordPosition="311" endWordPosition="314">dictionaries. 1 Introduction In this paper we describe the use of a particular machine learning technique, decision trees (DTs), to acquire lexical information in a broad-coverage linguistics-based natural language processing (NLP) system. The manual encoding of lexical information into an online dictionary is costly and timeconsuming. Therefore, the NLP community has used various machine learning (ML) techniques to automatically learn lexical information (Atwell et al. (1994), Teufel (1995), Stevenson and Merlo (1997), Van Halteren et al. (1998), Brill and Wu (1998), Schulte im Walde (1998), Stevenson et al. (1999), Zavrel and Daelemans (2000), Van Halteren et al. (2001), among others). Most approaches consist of ML techniques that make use of linear Marisa Jiménez and Martine Pettenaro context (n-grams extracted from text) and that are not integrated in a rich NLP system. By contrast we use a particular ML technique, DTs, that exploits a deep knowledge understanding system to learn lexical information, which in turn immediately benefits our system. Our goal was to prove that DTs offer many advantages as a lexical information acquisition technique: ! They are much faster than manual encoding. ! They are</context>
</contexts>
<marker>Stevenson, Merlo, Karaeva, Whitehouse, 1999</marker>
<rawString>Stevenson, S., P. Merlo, N. Karaeva, and K. Whitehouse (1999). Supervised Learning of Lexical Semantic Verb Classes using Frequency Distributions. Procs of SigLex 99, College Park, Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Teufel</author>
</authors>
<title>A Support Tool for Tagset Mapping.</title>
<date>1995</date>
<booktitle>Procedings of the Workshop SIGDAT (EACL95).</booktitle>
<contexts>
<context position="2190" citStr="Teufel (1995)" startWordPosition="292" endWordPosition="293">bres décisionnels, dictionnaires automatiquement appris. Lexical learning, machine learning, decision trees, learned dictionaries. 1 Introduction In this paper we describe the use of a particular machine learning technique, decision trees (DTs), to acquire lexical information in a broad-coverage linguistics-based natural language processing (NLP) system. The manual encoding of lexical information into an online dictionary is costly and timeconsuming. Therefore, the NLP community has used various machine learning (ML) techniques to automatically learn lexical information (Atwell et al. (1994), Teufel (1995), Stevenson and Merlo (1997), Van Halteren et al. (1998), Brill and Wu (1998), Schulte im Walde (1998), Stevenson et al. (1999), Zavrel and Daelemans (2000), Van Halteren et al. (2001), among others). Most approaches consist of ML techniques that make use of linear Marisa Jiménez and Martine Pettenaro context (n-grams extracted from text) and that are not integrated in a rich NLP system. By contrast we use a particular ML technique, DTs, that exploits a deep knowledge understanding system to learn lexical information, which in turn immediately benefits our system. Our goal was to prove that DT</context>
</contexts>
<marker>Teufel, 1995</marker>
<rawString>Teufel, S. (1995). A Support Tool for Tagset Mapping. Procedings of the Workshop SIGDAT (EACL95).</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Van Halteren</author>
<author>J Zavrel</author>
<author>W Daelemans</author>
</authors>
<title>Improving Data Driven Wordclass Tagging by System Combination.</title>
<date>1998</date>
<booktitle>Proceedings of ACL-COLING98,</booktitle>
<location>Montreal, Canada.</location>
<marker>Van Halteren, Zavrel, Daelemans, 1998</marker>
<rawString>Van Halteren, H., J. Zavrel, and W. Daelemans (1998). Improving Data Driven Wordclass Tagging by System Combination. Proceedings of ACL-COLING98, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Van Halteren</author>
<author>J Zavrel</author>
<author>W Daelemans</author>
</authors>
<date>2001</date>
<journal>Improving Accuracy in NLP through Combination of Machine Learning Systems. Computational Linguistics</journal>
<volume>27</volume>
<issue>2</issue>
<pages>199--230</pages>
<marker>Van Halteren, Zavrel, Daelemans, 2001</marker>
<rawString>Van Halteren, H., J. Zavrel, and W. Daelemans (2001). Improving Accuracy in NLP through Combination of Machine Learning Systems. Computational Linguistics 27 (2), pp. 199-230.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Wu</author>
<author>J Pentheroudakis</author>
<author>Z Jiang</author>
</authors>
<title>Dynamic Lexical Acquisition in Chinese Sentence Analysis.</title>
<date>2002</date>
<booktitle>Proceedings of COLING 2002,</booktitle>
<location>Taipei, Taiwan.</location>
<contexts>
<context position="4706" citStr="Wu et al. (2002)" startWordPosition="682" endWordPosition="685">f-speech probabilities. Our system has two main techniques in place to perform lexical learning. The first technique consists of linguistic rules that allow us to acquire lexical information at any stage of sentence processing: either morphological, named entity, syntactic, or logical-form processing may be adequate depending on the type of information we are interested in; for example, we might want to learn the feature count noun through plural nouns found in the data. This information is stored in learned dictionaries that supplement the general dictionary (see Pentheroudakis (2001), and Wu et al. (2002)). Any subset of learned dictionaries can be used depending on corpus-specific needs, and their entries can also be permanently merged with the main dictionary if desired. The second technique consists of lexical learning through the use of DTs. The tools that we use to build our DTs are borrowed from the publicly available WinMine toolkit (Chickering, n.d.), developed at Microsoft Research. The WinMine tools automatically split the data into training and testing (70/30), and produce several DT models at different levels of granularity. DTs provide a classification of selected features and ran</context>
</contexts>
<marker>Wu, Pentheroudakis, Jiang, 2002</marker>
<rawString>Wu, A., J. Pentheroudakis, and Z. Jiang (2002). Dynamic Lexical Acquisition in Chinese Sentence Analysis. Proceedings of COLING 2002, Taipei, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H J Zavrel</author>
<author>W Daelemans</author>
</authors>
<title>Bootstrapping a Tagged Corpus through Combination of Existing Heterogeneous Taggers.</title>
<date>2000</date>
<booktitle>International Conference on Language Resources and Evaluation,</booktitle>
<location>Athens, Greece.</location>
<contexts>
<context position="2346" citStr="Zavrel and Daelemans (2000)" startWordPosition="315" endWordPosition="318">ion In this paper we describe the use of a particular machine learning technique, decision trees (DTs), to acquire lexical information in a broad-coverage linguistics-based natural language processing (NLP) system. The manual encoding of lexical information into an online dictionary is costly and timeconsuming. Therefore, the NLP community has used various machine learning (ML) techniques to automatically learn lexical information (Atwell et al. (1994), Teufel (1995), Stevenson and Merlo (1997), Van Halteren et al. (1998), Brill and Wu (1998), Schulte im Walde (1998), Stevenson et al. (1999), Zavrel and Daelemans (2000), Van Halteren et al. (2001), among others). Most approaches consist of ML techniques that make use of linear Marisa Jiménez and Martine Pettenaro context (n-grams extracted from text) and that are not integrated in a rich NLP system. By contrast we use a particular ML technique, DTs, that exploits a deep knowledge understanding system to learn lexical information, which in turn immediately benefits our system. Our goal was to prove that DTs offer many advantages as a lexical information acquisition technique: ! They are much faster than manual encoding. ! They are more flexible than purely li</context>
</contexts>
<marker>Zavrel, Daelemans, 2000</marker>
<rawString>Zavrel, H.J. and W. Daelemans (2000). Bootstrapping a Tagged Corpus through Combination of Existing Heterogeneous Taggers. International Conference on Language Resources and Evaluation, Athens, Greece.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>