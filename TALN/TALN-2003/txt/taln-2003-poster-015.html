<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
"http://www.w3.org/TR/html4/loose.dtd">
<html><head><title>Apport d&#146;un mod&#232;le de langage statistique pour la reconnaissance de l&#146;&#233;criture manuscrite en ligne</title>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
</head>
<body>
<div style="page-break-before:always; page-break-after:always"><div><p>TALN 2003, Batz-sur-Mer, 11-14 juin 2003 
</p>
<p>Apport d&#146;un mod&#232;le de langage statistique pour la  
reconnaissance de l&#146;&#233;criture manuscrite en ligne 
</p>
<p>Freddy Perraud (1), Emmanuel Morin (2),  
Christian Viard-Gaudin (3) et Pierre-Michel Lallican (1)  
</p>
<p>(1) Soci&#233;t&#233; Vision Objects - 9, rue du Pavillon - 44980 Sainte Luce sur Loire 
{freddy.perraud, pmlallican}@visionobjects.com 
</p>
<p> 
(2) Institut de Recherche en Informatique de Nantes  
</p>
<p>2, rue de la Houssini&#232;re - BP 92208 - 44322 Nantes Cedex 3  
morin@irin.univ-nantes.fr  
</p>
<p> 
(3) Institut de Recherche en Communications et Cybern&#233;tique de Nantes - UMR 
CNRS La Chantrerie - Rue Christian Pauc - BP 50609 - 44306 Nantes Cedex 3 
</p>
<p>christian.viard-gaudin@polytech.univ-nantes.fr 
</p>
<p>R&#233;sum&#233; &#150; Abstract  
Dans ce travail, nous &#233;tudions l&#146;apport d&#146;un mod&#232;le de langage pour am&#233;liorer les performances des syst&#232;mes de 
reconnaissance de l&#146;&#233;criture manuscrite en-ligne. Pour cela, nous avons explor&#233; des mod&#232;les bas&#233;s sur des 
approches statistiques construits par apprentissage sur des corpus &#233;crits. Deux types de mod&#232;les ont &#233;t&#233; &#233;tudi&#233;s : 
les mod&#232;les n-grammes et ceux de type n-classes. En vue de l&#146;int&#233;gration dans un syst&#232;me de faible capacit&#233; 
(engin nomade), un mod&#232;le n-classe combinant crit&#232;res syntaxiques et contextuels a &#233;t&#233; d&#233;fini, il a permis 
d&#146;obtenir des r&#233;sultats surpassant ceux donn&#233;s avec un mod&#232;le beaucoup plus lourd de type n-gramme. Les 
r&#233;sultats pr&#233;sent&#233;s ici montrent qu&#146;il est possible de prendre en compte les sp&#233;cificit&#233;s d&#146;un langage en vue de 
reconna&#238;tre l&#146;&#233;criture manuscrite avec des mod&#232;les de taille tout &#224; fait raisonnable. 
</p>
<p>This works highlights the interest of a language model in increasing the performances of on-line handwriting 
recognition systems. Models based on statistical approaches, trained on written corpora, have been investigated. 
Two kinds of models have been studied: n-gram models and n-class models. In order to integrate it into small 
capacity systems (mobile device), a n-class model has been designed by combining syntactic and contextual 
criteria. It outperforms bulkier models based on n-gram. The results we obtain show that it is possible to take 
advantage of language specificities to recognize handwritten sentences by using reasonable size models. 
</p>
<p>Mots Cl&#233;s &#150; Keywords  
Reconnaissance de l&#146;&#233;criture manuscrite, mod&#232;le de langage, n-gramme, n-classe, perplexit&#233;. 
</p>
<p>Handwriting recognition, language modelling, n-gram, n-class, perplexity. </p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Freddy Perraud , Emmanuel Morin, Christian Viard-Gaudin et Pierre-Michel Lallican 
</p>
<p> 2
</p>
<p>1 Introduction 
</p>
<p>Dans ce travail, nous nous int&#233;ressons au probl&#232;me de la reconnaissance de l&#146;&#233;criture dite en-
ligne. L&#146;efficacit&#233; de celle-ci peut &#234;tre renforc&#233;e &#224; l&#146;aide d&#146;un mod&#232;le renfermant des 
connaissances a priori sur le langage. Dans un mod&#232;le probabiliste, une phrase s peut &#234;tre 
repr&#233;sent&#233;e par une s&#233;quence de mots wi de longueur L, soit : s = w1 &#133; wi &#133; wL = w1,L. En 
consid&#233;rant cette s&#233;quence comme une cha&#238;ne de Markov, nous pouvons estimer la 
probabilit&#233; p(s) d&#146;une phrase s comme suit : 
</p>
<p> ( ) ( ) ( ) ( ) ( ) ( ) ( )&#8719;
=
</p>
<p>&#8722;&#8722; ===
L
</p>
<p>i
iiLLL wwwpwwwpwwwpwwpwpwpsp
</p>
<p>1
1111213121,1 .........   (1) 
</p>
<p>Lorsque la longueur de l'historique du mot &#224; pr&#233;dire devient importante, l'estimation de la 
probabilit&#233; conditionnelle p(wi|w1...wi-1) n&#146;est pas fiable. La r&#233;duction de l&#146;ordre de la cha&#238;ne 
de Markov permet alors de restreindre l&#146;historique en ne tenant compte que du contexte 
proche des mots wi (Manning et al., 2000). Dans un mod&#232;le n-gramme, seuls les n-1 
pr&#233;c&#233;dents mots sont pris en consid&#233;ration :  
</p>
<p> ( ) ( )1111 ...... &#8722;&#8722;+&#8722; &#8776; iiinii wwwpwwwp  (2) 
Les probabilit&#233;s p(wi|wi-n+1...wi-1) sont calcul&#233;es statistiquement par une simple m&#233;thode de 
comptage d&#146;&#233;v&#233;nements compl&#233;t&#233;e par la m&#233;thode de lissage absolute discounting backing-
off qui permet d&#146;estimer p(wi|wi-n+1...wi-1) pour des &#233;v&#232;nements non rencontr&#233;s sur la base 
d&#146;apprentissage. Pour &#233;valuer l&#146;ad&#233;quation du mod&#232;le, nous utilisons la traditionnelle mesure 
de perplexit&#233; : 
</p>
<p> ( ) ( ) Test
Test LL
</p>
<p>i
testM spTPP
</p>
<p>1
</p>
<p>1
</p>
<p>&#8722;
</p>
<p>=
&#63738;
&#63739;
</p>
<p>&#63737;
&#63727;
&#63728;
</p>
<p>&#63726;
= &#8719;    (o&#249; LTest est le nombre de phrases dans la base de test) (3) 
</p>
<p>Comme le nombre de n-grammes devient vite consid&#233;rable pour un lexique de taille 
importante d&#232;s lors que n augmente, nous cherchons &#224; r&#233;duire le nombre d&#146;&#233;v&#233;nements 
observables en regroupant les mots en classes (nous parlerons alors de mod&#232;les n-classes). En 
appliquant ce regroupement, le mod&#232;le pr&#233;dit non plus un mot en fonction des n-1 mots le 
pr&#233;c&#233;dant, mais en fonction des n-1 classes qui le pr&#233;c&#232;dent.  
</p>
<p> ( ) ( )( ) ( ) ( ) ( )( )&#8721; &#8719;
=
</p>
<p>&#8242;&#8242;&#8722;&#8242;=
min 1
</p>
<p>1
che
</p>
<p>L
</p>
<p>i
ikikikiki wgwgwgpwgwpsp  (4) 
</p>
<p>Dans le cas d&#146;une classification &#171; molle &#187;, chaque mot peut &#234;tre associ&#233; &#224; une ou plusieurs 
classes. Dans l&#146;&#233;quation (4), un chemin correspond &#224; une s&#233;quence g(w1)&#133; g(wL) possible de 
classes. Dans le cas d&#146;une classification &#171; dure &#187;, chaque mot est associ&#233; &#224; une et une seule 
classe; alors un seul chemin existe. Nous avons effectu&#233; la classification suivant deux 
crit&#232;res. D&#146;une part, un crit&#232;re statistique  regroupe les mots partageant les m&#234;mes contextes 
lexicaux, et d&#146;autre part, un crit&#232;re syntaxique regroupe les mots selon leurs parties du 
discours. 
</p>
<p>2 Performances des mod&#232;les du langage 
</p>
<p>Dans cette section, nous cherchons &#224; &#233;tudier le comportement de diff&#233;rents mod&#232;les de 
langage &#224; savoir n-gramme, n-classe syntaxique et n-classe statistique sur diff&#233;rentes bases de 
donn&#233;es textuelles. Le tableau 1 pr&#233;sente les principales caract&#233;ristiques des corpus utilis&#233;s </p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Apport d&#146;un mod&#232;le de langage statistique pour la reconnaissance de l&#146;&#233;criture manuscrite 
en ligne 
</p>
<p> 3
</p>
<p>pour l&#146;apprentissage, puis pour l&#146;&#233;valuation des mod&#232;les. Pour des raisons de lisibilit&#233;, nous 
introduisons les abr&#233;viations suivantes : MBG pour Mod&#232;le BiGramme ; MBCSynt pour 
Mod&#232;le BiClasse Syntaxique et MBCStatX pour Mod&#232;le BiClasse Statistique utilisant X 
classes avec X={10, 50, 100, 500, 1000}. 
</p>
<p> Nom Taille du corpus (million de mots) 
Taille du lexique 
(million de mots) 
</p>
<p>Domaine de 
discours 
</p>
<p>% mots avec 
une occurrence 
</p>
<p>ABU 4,1 0,09 romans du XIX
e et 
</p>
<p>XXe 42 % Corpus 
d&#146;apprentissage 
</p>
<p>ECI 4,2 0,1 articles issus du journal  Le Monde 42 % 
</p>
<p>Corpus de test TEST 1,6 0,072 
articles issus de 
</p>
<p>journaux et de romans 44 % 
</p>
<p>Tableau 1 : Caract&#233;ristiques des corpus utilis&#233;s pour l&#146;apprentissage et l&#146;&#233;valuation 
</p>
<p>2.1 Mod&#232;les n-classes statistiques et mod&#232;les n-classes syntaxiques 
</p>
<p>Dans le cas des mod&#232;les n-classes statistiques, nous utilisons un algorithme de classification 
dure inspir&#233; de celui des k-means pour construire les classes, k &#233;tant  le nombre de classes. 
Les r&#233;sultats pr&#233;sent&#233;s &#224; la figure 1 montrent, sans surprise, que plus le nombre de classes est 
important, meilleures sont les performances car les estimations de probabilit&#233;s sont alors plus 
pr&#233;cises. Il est int&#233;ressant de noter que les MBCStat500/1000 parviennent presque &#224; &#233;galer les 
mod&#232;les bigrammes, ce qui est un r&#233;sultat tout &#224; fait int&#233;ressant &#233;tant donn&#233; leur moindre 
encombrement m&#233;moire. 
</p>
<p>Dans le cas des mod&#232;les n-classes syntaxiques, nous utilisons des statistiques obtenues sur les 
corpus d&#146;apprentissage pr&#233;alablement &#233;tiquet&#233;s par l&#146;&#233;tiqueteur de Brill (Brill 94 ; Le Comte 
et al., 1998) et de l&#146;analyseur flexionnel Flemm (Namer 2000). Les performances de ces 
mod&#232;les qui comptent 210 classes sont inf&#233;rieures &#224; celles obtenues avec MBCStat500/1000 
mais les MBCSynt prennent tout leurs int&#233;r&#234;ts lors de la combinaison avec d&#146;autres mod&#232;les. 
</p>
<p>2.2 Combinaisons de mod&#232;les 
</p>
<p>A l&#146;instar des travaux (Niesler, 1997 ; Jardino, 1994 ; Goodman, 2000 ; El-B&#232;ze 1993), nous 
proposons d&#146;&#233;tudier la combinaison lin&#233;aire de plusieurs de ces mod&#232;les, &#224; savoir 1) mod&#232;les 
bigramme et biclasse syntaxique ; 2) mod&#232;les bigramme et biclasse statistique et enfin 3) 
mod&#232;les biclasse syntaxique et biclasse statistique. Par la suite, nous d&#233;signerons ces mod&#232;les 
sous le terme de mod&#232;le combin&#233;. On peut observer sur la figure 2 les r&#233;sultats correspondant 
&#224; la combinaison des MBG et des MBCStat. Ces deux types de mod&#232;les apparaissent tr&#232;s 
compl&#233;mentaires. Les MBCStat am&#233;liorent jusqu'&#224; 18 % les performances du mod&#232;le 
bigramme sur ABU et jusqu'&#224; 16 % sur ECI, pour les mod&#232;les avec 500 classes. 
</p>
<p>Le mod&#232;le biclasse syntaxique est &#233;galement fortement compl&#233;mentaire au mod&#232;le bigramme. 
Ainsi, la combinaison de ces deux mod&#232;les am&#233;liore de 45 % les performances sur ABU et de 
33 % sur ECI. Les MBCStat500 pr&#233;sentaient de meilleures performances que les MBCSynt, 
or la combinaison entre un MBG et un MBCStat500 est moins performante qu'une 
combinaison entre un MBG et un MBCSynt. On peut en conclure que la nature des 
informations contenues dans les MBCSynt est plus compl&#233;mentaire de celle des MBG que ne </p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Freddy Perraud , Emmanuel Morin, Christian Viard-Gaudin et Pierre-Michel Lallican 
</p>
<p> 4
</p>
<p>l'est celle des MBCStat. Un MBCStat est par nature proche des MBG et apporte donc moins 
qu'un MBCSynt &#224; un MBG. 
</p>
<p>Enfin, dans le cas d&#146;une combinaison des mod&#232;les biclasse statistique et biclasse syntaxique, 
laquelle configuration correspond &#224; une combinaison r&#233;ellement int&#233;ressante car seuls des 
mod&#232;les de faibles complexit&#233;s sont pris en compte, les performances sont notablement 
am&#233;lior&#233;es (cf. figure 2). Ses r&#233;sultats surpassent tr&#232;s significativement ceux obtenus par des 
MBG. On obtient jusqu'&#224; 47 % d'am&#233;lioration sur ABU et 35 % sur ECI avec un MBCSynt 
combin&#233; &#224; un MBCStat500 par rapport au MBG. Toutefois, on peut conjecturer que la 
relative faiblesse des mod&#232;les bigrammes par rapport aux mod&#232;les n-classes combin&#233;s est en 
partie due &#224; la taille r&#233;duite de la base d&#146;apprentissage (4 millions contre plusieurs dizaines de 
millions de mots pour obtenir des mod&#232;les bigrammes v&#233;ritablement robustes). 
</p>
<p>Nous avons enfin essay&#233; de combiner les trois mod&#232;les : MBG, MBCStat1000 et MBCSynt. 
Les performances obtenues ne sont que tr&#232;s l&#233;g&#232;rement sup&#233;rieures (1 &#224; 2 % sur ABU et 5 % 
sur ECI) &#224; celles correspondant &#224; la combinaison des mod&#232;les biclasses seuls. Les MBG 
n'apportent donc que peu d'informations compl&#233;mentaires aux mod&#232;les MBCStat et MBCSynt 
combin&#233;s. On peut en conclure que les mod&#232;les combin&#233;s MBCStat et MBCSynt se suffisent 
&#224; eux-m&#234;mes et ne n&#233;cessitent pas de combinaisons avec un MBG ce qui de toutes mani&#232;res 
constituerait alors un mod&#232;le beaucoup trop volumineux. 
</p>
<p>0
200
400
600
800
</p>
<p>1000
1200
1400
1600
1800
</p>
<p>MB
CS
</p>
<p>tat
10
</p>
<p>MB
CS
</p>
<p>tat
50
</p>
<p>MB
CS
</p>
<p>tat
10
</p>
<p>0
</p>
<p>MB
CS
</p>
<p>tat
50
</p>
<p>0
</p>
<p>MB
CS
</p>
<p>tat
10
</p>
<p>00
</p>
<p>MB
CS
</p>
<p>yn
t
</p>
<p>MB
G
</p>
<p>ABU
ECI
</p>
<p> 
</p>
<p>0
100
200
300
400
500
600
</p>
<p>MB
G+
</p>
<p>MB
CS
</p>
<p>tat
10
</p>
<p>0
</p>
<p>MB
G+
</p>
<p>MB
CS
</p>
<p>tat
50
</p>
<p>0
</p>
<p>MB
G+
</p>
<p>MB
CS
</p>
<p>yn
t
</p>
<p>MB
Sta
</p>
<p>t50
0+
</p>
<p>MB
Sy
</p>
<p>nt
</p>
<p>MB
G+
</p>
<p>MB
Sta
</p>
<p>t50
0+
</p>
<p>...
</p>
<p>ABU
ECI 
</p>
<p> 
</p>
<p>Figure 1 : Mesures de perplexit&#233; avec les 
mod&#232;les simples sur le corpus TEST 
</p>
<p>Figure 2 : Mesures de perplexit&#233; avec les 
mod&#232;les combin&#233;s sur le corpus TEST. 
</p>
<p>3 Contribution des mod&#232;les de langage dans le syst&#232;me de 
reconnaissance 
</p>
<p>Afin d&#146;&#233;valuer la contribution des mod&#232;les du langage dans le syst&#232;me de reconnaissance de 
l&#146;&#233;criture, nous avons utilis&#233; une nouvelle base de test compos&#233;e de 4 912 phrases distinctes1. 
</p>
<p>                                                 
1 Cette base de test, compos&#233;e de 37 700 mots d&#233;finissant un lexique de 8 800 mots, est issue d&#146;une collecte 
</p>
<p>r&#233;alis&#233;e &#224; l&#146;aide d&#146;une ardoise &#233;lectronique aupr&#232;s de 400 scripteurs. </p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Apport d&#146;un mod&#232;le de langage statistique pour la reconnaissance de l&#146;&#233;criture manuscrite 
en ligne 
</p>
<p> 5
</p>
<p>La figure 3 pr&#233;sente un exemple du signal d&#146;entr&#233;e du syst&#232;me correspondant aux phrases 
&#171; Mais jamais pour tr&#232;s longtemps. &#187; et &#171;  Ce n&#146;est pas si s&#251;r. &#187;. 
</p>
<p>Figure 3 : Echantillons de la base d&#146;&#233;criture manuscrite dynamique 
</p>
<p>Sur cette base, le taux d&#146;erreur de reconnaissance mot, sans aucun mod&#232;le de langage est &#233;gal 
&#224; 34 %. Cette valeur est bien sup&#233;rieure &#224; ce que l&#146;on obtient plus classiquement sur une base 
de mots. Il faut souligner en particulier ici le fait que, outre la difficult&#233; de la segmentation 
inter-mot qui n&#146;existe pas dans une reconnaissance base mot, la ponctuation est prise en 
compte dans la reconnaissance (par exemple chaque erreur sur une virgule est comptabilis&#233;e). 
Si l&#146;on rajoute un mod&#232;le de langage &#233;l&#233;mentaire, consistant en l&#146;int&#233;gration des 
monogrammes dans le treillis de reconnaissance, alors le taux d&#146;erreur chute &#224; 29 %. 
</p>
<p>La figure 4 pr&#233;sente l&#146;&#233;volution du taux d&#146;erreur avec des syst&#232;mes de reconnaissance 
int&#233;grant des mod&#232;les monogramme et des mod&#232;les biclasse, biclasse combin&#233; ou bigramme. 
La diminution du taux d&#146;erreur est tr&#232;s significative. Pour le MBCStat500 seul, le taux 
d&#146;erreur est inf&#233;rieur &#224; 23 %, cela correspond &#224; une diminution de l&#146;erreur de 32 % par 
rapport au syst&#232;me sans mod&#232;le de langage. Le meilleur r&#233;sultat est obtenu avec la 
combinaison des mod&#232;les MBCStat500 et MBCSynt o&#249; le taux d&#146;erreur est de 22,5 %.  
</p>
<p>21%
</p>
<p>22%
</p>
<p>23%
</p>
<p>24%
</p>
<p>25%
</p>
<p>26%
</p>
<p>27%
</p>
<p>MB
CS
</p>
<p>tat
10
</p>
<p>MB
CS
</p>
<p>tat
50
</p>
<p>MB
CS
</p>
<p>tat
10
</p>
<p>0
</p>
<p>MB
CS
</p>
<p>tat
50
</p>
<p>0
</p>
<p>MB
CS
</p>
<p>tat
10
</p>
<p>00
</p>
<p>MB
CS
</p>
<p>yn
t
</p>
<p>MB
G
</p>
<p>MB
G+
</p>
<p>MB
CS
</p>
<p>50
0
</p>
<p>MB
G+
</p>
<p>MB
CS
</p>
<p>yn
t
</p>
<p>MB
CS
</p>
<p>50
0+
</p>
<p>MB
CS
</p>
<p>yn
t
</p>
<p>MB
G+
</p>
<p>MB
S5
</p>
<p>00
+M
</p>
<p>BS
yn
</p>
<p>t
</p>
<p>Ta
ux
</p>
<p> d
'e
</p>
<p>rr
eu
</p>
<p>r
</p>
<p>0
100
200
300
400
500
600
700
800
900
1000
</p>
<p>Perplexit&#233;
</p>
<p>Taux
d'erreur
Perplexit&#233;
</p>
<p> 
Figure 4 : Mesures de perplexit&#233; et taux d&#146;erreur avec le mod&#232;le entra&#238;n&#233; sur le corpus ECI. 
</p>
<p>De plus, ces courbes mettent clairement en &#233;vidence la forte corr&#233;lation entre la mesure de 
perplexit&#233; et le taux d&#146;erreurs. La mesure de perplexit&#233; semble donc bien un indicateur valide 
pour mesurer la pertinence d&#146;un mod&#232;le de langage en vue de son utilisation dans un contexte 
de reconnaissance de l&#146;&#233;criture manuscrite. Elle a l&#146;avantage de pouvoir &#234;tre &#233;valu&#233;e sur des 
bases beaucoup plus faciles &#224; obtenir que des bases d&#146;&#233;criture manuscrite. Ce point est 
important et n&#146;avait pas &#224; notre connaissance &#233;t&#233; pr&#233;alablement mis en &#233;vidence 
exp&#233;rimentalement comme nous le faisons ici. 
</p>
<p>Si nous reprenons les exemples de la figure 3, le syst&#232;me de reconnaissance sans mod&#232;le de 
langage propose en sortie : &#171; Mais jamais pour tirs longtemps. &#187; et &#171; Ce n&#146;est pas si d&#251;. &#187;. Par 
exemple, le MBCSynt corrige l&#146;erreur sur ces deux exemples. D&#146;un point de vue syntaxique, </p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Freddy Perraud , Emmanuel Morin, Christian Viard-Gaudin et Pierre-Michel Lallican 
</p>
<p> 6
</p>
<p>il est effectivement plus vraisemblable que l&#146;adverbe de temps &#171; longtemps &#187; soit pr&#233;c&#233;d&#233; 
d&#146;un adverbe que d&#146;un nom commun. De m&#234;me, apr&#232;s l&#146;adverbe &#171; si &#187;, l&#146;adjectif  &#171; s&#251;r &#187; est 
plus probable que le participe pass&#233; &#171; d&#251; &#187;. 
</p>
<p>4 Conclusion et perspectives 
</p>
<p>Dans ce travail, nous avons montr&#233; l&#146;apport significatif des mod&#232;les de langage &#224; un syst&#232;me 
de reconnaissance de l&#146;&#233;criture manuscrite en ligne. Globalement, nous avons obtenu une 
diminution de plus 34 % du taux d&#146;erreur en recherchant le meilleur compromis 
performance/co&#251;t mat&#233;riel. 
</p>
<p>Il reste bien entendu un certain nombre de points &#224; am&#233;liorer. Tout d&#146;abord, il serait  
int&#233;ressant d&#146;&#233;tudier les techniques de classification permettant de travailler sur des corpus 
plus volumineux (Beaujard et al., 1999 ; Goodman, 2000). Ensuite, nous devons affiner nos 
mod&#232;les qui souffrent actuellement d&#146;un manque de robustesse lorsqu&#146;ils sont confront&#233;s &#224; 
des noms propres ou &#224; des mots d&#146;origine &#233;trang&#232;res. Enfin, nos applications &#233;tant destin&#233;es &#224; 
des appareils nomades que l&#146;utilisateur s&#146;approprie, il serait int&#233;ressant d&#146;adapter nos mod&#232;les  
au domaine de discours de l&#146;utilisateur. 
</p>
<p>R&#233;f&#233;rences 
</p>
<p>Beaujard C., Jardino M., Classification de mots non &#233;tiquet&#233;s par des m&#233;thodes statistiques, 
Math&#233;matiques informatique et Sciences Humaines, vol 147, pp. 7-23, 1999. 
</p>
<p>El-B&#232;ze M., Les mod&#232;les de langage probabilistes : quelques domaines d&#146;application, HDR, 
LIPN, 1993. 
</p>
<p>Brill E., Some Advances in Rule-Based Part of Speech Tagging, In Proceedings, Twelfth 
National Conference on Artificial Intelligence (AAAI&#146;94), pp 722-727, 1994. 
</p>
<p>Goodman J., Putting it all together : Language model combination, ICASSP-2000, Istanbul, 
2000. 
</p>
<p>Jardino M., Automatic determination of a stochastic bigram class model, International 
Colloquium on Grammatical Inference, 1994. 
</p>
<p>Le Comte J., Paroubek P., Le cat&#233;goriseur d'Eric BRILL. Mise en oeuvre de la version 
entra&#238;n&#233;e &#224; l'INALF, Rapport technique, Nancy, CNRS-InaLF, 1998. 
</p>
<p>Manning C., Scutze., H. Foundation of Statistical Natural Language Processing, The MIT 
Press, 2000. 
</p>
<p>Namer F., Flemm : Un analyseur flexionnel du fran&#231;ais &#224; base de r&#232;gles, Traitement 
Automatique des Langues (TAL), 41(2) pp. 523-548, 2000. 
</p>
<p>Niesler T., Category Based Statistical Language Models, Ph. D. thesis, University of 
Cambridge, June 1997. </p>

</div></div>
</body></html>