<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>D A Cruse</author>
</authors>
<title>Lexical Semantics, Cambridge University Press. Grefenstette G.</title>
<date>1986</date>
<location>Boston, Kluwer.</location>
<marker>Cruse, 1986</marker>
<rawString>Cruse D.A. (1986) Lexical Semantics, Cambridge University Press. Grefenstette G. (1994) Explorations in Automatic Thesaurus Discovery, Boston, Kluwer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
</authors>
<title>An Information-Theoretic Definition of Similarity.</title>
<date>1998</date>
<booktitle>Proceedings of International Conference on Machine Learning.</booktitle>
<contexts>
<context position="4029" citStr="Lin (1998" startWordPosition="659" endWordPosition="660">ll the words have the case of the initial character in common. We should note that this rule is ineffective for languages in which common nouns appear with the initial character in upper case, such as German. 4 Comparative Evaluation Several researchers have performed lexicographic evaluation of the outcome of their work on lexical acquisition (Smadja, 1993; Schütze, 1998). Such approaches guarantee a high accuracy and coverage, but they are expensive, time-consuming and not easily repeatable. This is a notable drawback in an algorithm comparison task. For this reason, Grefenstette (1994) and Lin (1998b) exploited existing lexico-semantic resources, such as WordNet and Roget thesaurus to provide comparative evaluation. Indeed, automatic evaluation, besides faster and cheaper than manual evaluation, allows repeatability of the experiments and hence comparison between alternate approaches. Therefore, we followed both evaluation approaches separately: Automatic evaluation against WordNet and additional lists of entities and manual inspection by domain experts in order to deal with economic terminology. 4.1 Automatic evaluation The main body of our gold standard is WordNet. Although several nam</context>
<context position="5332" citStr="Lin, 1998" startWordPosition="843" endWordPosition="844">ler, 1990). Since newswire text abounds in named entities, it is purposeful to utilize relevant semantic information. Considering the nature of WSJC, we used lists of entity names obtained from publicly available Internet databases; specifically American locations, companies and organizations. A similarity relation is considered correct if the lemmas of the related words are found in the same set of entities, or they appear to be synonyms or antonyms in WordNet, or the similarity of their respective concepts in the WordNet hierarchy, according to the information theoretic measure proposed in (Lin, 1998b) exceeds a certain threshold Tsim. The concept probabilities were calculated in a similar way as in (Resnik, 1999). 4.2 Manual evaluation Although manual evaluation of the extracted long list of relations is a burdensome task, it is quite easier to perform comparative evaluation between two different but largely overlapping Text Tokenization for Knowledge-free Automatic Extraction of Lexical Similarities resources. Since the largest portion of the extracted resources is common, we confine the evaluation to the different parts. That is, if S (N)J is the set of the N-best relations produced, t</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Lin D. (1998a). An Information-Theoretic Definition of Similarity. Proceedings of International Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
</authors>
<title>Automatic retrieval and clustering of similar words,</title>
<date>1998</date>
<booktitle>Proc. of COLING-ACL.</booktitle>
<contexts>
<context position="4029" citStr="Lin (1998" startWordPosition="659" endWordPosition="660">ll the words have the case of the initial character in common. We should note that this rule is ineffective for languages in which common nouns appear with the initial character in upper case, such as German. 4 Comparative Evaluation Several researchers have performed lexicographic evaluation of the outcome of their work on lexical acquisition (Smadja, 1993; Schütze, 1998). Such approaches guarantee a high accuracy and coverage, but they are expensive, time-consuming and not easily repeatable. This is a notable drawback in an algorithm comparison task. For this reason, Grefenstette (1994) and Lin (1998b) exploited existing lexico-semantic resources, such as WordNet and Roget thesaurus to provide comparative evaluation. Indeed, automatic evaluation, besides faster and cheaper than manual evaluation, allows repeatability of the experiments and hence comparison between alternate approaches. Therefore, we followed both evaluation approaches separately: Automatic evaluation against WordNet and additional lists of entities and manual inspection by domain experts in order to deal with economic terminology. 4.1 Automatic evaluation The main body of our gold standard is WordNet. Although several nam</context>
<context position="5332" citStr="Lin, 1998" startWordPosition="843" endWordPosition="844">ler, 1990). Since newswire text abounds in named entities, it is purposeful to utilize relevant semantic information. Considering the nature of WSJC, we used lists of entity names obtained from publicly available Internet databases; specifically American locations, companies and organizations. A similarity relation is considered correct if the lemmas of the related words are found in the same set of entities, or they appear to be synonyms or antonyms in WordNet, or the similarity of their respective concepts in the WordNet hierarchy, according to the information theoretic measure proposed in (Lin, 1998b) exceeds a certain threshold Tsim. The concept probabilities were calculated in a similar way as in (Resnik, 1999). 4.2 Manual evaluation Although manual evaluation of the extracted long list of relations is a burdensome task, it is quite easier to perform comparative evaluation between two different but largely overlapping Text Tokenization for Knowledge-free Automatic Extraction of Lexical Similarities resources. Since the largest portion of the extracted resources is common, we confine the evaluation to the different parts. That is, if S (N)J is the set of the N-best relations produced, t</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Lin D. (1998b). Automatic retrieval and clustering of similar words, Proc. of COLING-ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Manning</author>
<author>H Schütze</author>
</authors>
<date>1999</date>
<booktitle>Foundations of Statistical Natural Language Processing,</booktitle>
<publisher>MIT Press,</publisher>
<location>Cambridge.</location>
<marker>Manning, Schütze, 1999</marker>
<rawString>Manning C. and Schütze H. (1999) Foundations of Statistical Natural Language Processing, MIT Press, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Martin</author>
<author>J Liermann</author>
<author>H Ney</author>
</authors>
<title>Algorithms for bigram and trigram word clustering,</title>
<date>1998</date>
<journal>Speech Communication,</journal>
<volume>24</volume>
<pages>19--37</pages>
<marker>Martin, Liermann, Ney, 1998</marker>
<rawString>Martin S., Liermann J., Ney H. (1998) Algorithms for bigram and trigram word clustering, Speech Communication, Vol.24, pp.19-37.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Miller</author>
</authors>
<title>Wordnet: An on-line lexical database,</title>
<date>1990</date>
<journal>International Journal of Lexicography,</journal>
<booktitle>Proceedings of the NAACL&apos;01 Workshop on WordNet and other Lexical Resources.</booktitle>
<location>Vol.3. Pearce, D.</location>
<contexts>
<context position="4733" citStr="Miller, 1990" startWordPosition="753" endWordPosition="754">vide comparative evaluation. Indeed, automatic evaluation, besides faster and cheaper than manual evaluation, allows repeatability of the experiments and hence comparison between alternate approaches. Therefore, we followed both evaluation approaches separately: Automatic evaluation against WordNet and additional lists of entities and manual inspection by domain experts in order to deal with economic terminology. 4.1 Automatic evaluation The main body of our gold standard is WordNet. Although several named entities are included, there has been no systematic effort of storing such information (Miller, 1990). Since newswire text abounds in named entities, it is purposeful to utilize relevant semantic information. Considering the nature of WSJC, we used lists of entity names obtained from publicly available Internet databases; specifically American locations, companies and organizations. A similarity relation is considered correct if the lemmas of the related words are found in the same set of entities, or they appear to be synonyms or antonyms in WordNet, or the similarity of their respective concepts in the WordNet hierarchy, according to the information theoretic measure proposed in (Lin, 1998b</context>
</contexts>
<marker>Miller, 1990</marker>
<rawString>Miller G. (1990) Wordnet: An on-line lexical database, International Journal of Lexicography, Vol.3. Pearce, D. (2001) Synonymy in collocation extraction, Proceedings of the NAACL&apos;01 Workshop on WordNet and other Lexical Resources.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Schütze</author>
</authors>
<title>Word Sense Discrimination.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<pages>97--124</pages>
<contexts>
<context position="3795" citStr="Schütze, 1998" startWordPosition="624" endWordPosition="625">rors (compositional collocations) regard expressions consisting of a named entity and a common word, (Reagan administration, Java programmers). In order to improve the performance of multiword extraction we keep only the n-grams in which all the words have the case of the initial character in common. We should note that this rule is ineffective for languages in which common nouns appear with the initial character in upper case, such as German. 4 Comparative Evaluation Several researchers have performed lexicographic evaluation of the outcome of their work on lexical acquisition (Smadja, 1993; Schütze, 1998). Such approaches guarantee a high accuracy and coverage, but they are expensive, time-consuming and not easily repeatable. This is a notable drawback in an algorithm comparison task. For this reason, Grefenstette (1994) and Lin (1998b) exploited existing lexico-semantic resources, such as WordNet and Roget thesaurus to provide comparative evaluation. Indeed, automatic evaluation, besides faster and cheaper than manual evaluation, allows repeatability of the experiments and hence comparison between alternate approaches. Therefore, we followed both evaluation approaches separately: Automatic ev</context>
</contexts>
<marker>Schütze, 1998</marker>
<rawString>Schütze H. (1998) Word Sense Discrimination. Computational Linguistics, Vol.24, pp.97-124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Sekine</author>
<author>J J Carroll</author>
<author>S Ananiadou</author>
<author>J Tsujii</author>
</authors>
<title>Automatic Learning for Semantic Collocation,</title>
<date>1992</date>
<booktitle>Proceedings of the 3rd Conference on Applied NLP, ACL,</booktitle>
<pages>104--110</pages>
<marker>Sekine, Carroll, Ananiadou, Tsujii, 1992</marker>
<rawString>Sekine, S., Carroll J. J., Ananiadou S. and Tsujii J. (1992) Automatic Learning for Semantic Collocation, Proceedings of the 3rd Conference on Applied NLP, ACL, pp.104-110.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Smadja</author>
</authors>
<title>Retrieving Collocations from text: Xtract,</title>
<date>1993</date>
<journal>Computational Linguistics</journal>
<volume>19</volume>
<contexts>
<context position="3779" citStr="Smadja, 1993" startWordPosition="622" endWordPosition="623">at numerous errors (compositional collocations) regard expressions consisting of a named entity and a common word, (Reagan administration, Java programmers). In order to improve the performance of multiword extraction we keep only the n-grams in which all the words have the case of the initial character in common. We should note that this rule is ineffective for languages in which common nouns appear with the initial character in upper case, such as German. 4 Comparative Evaluation Several researchers have performed lexicographic evaluation of the outcome of their work on lexical acquisition (Smadja, 1993; Schütze, 1998). Such approaches guarantee a high accuracy and coverage, but they are expensive, time-consuming and not easily repeatable. This is a notable drawback in an algorithm comparison task. For this reason, Grefenstette (1994) and Lin (1998b) exploited existing lexico-semantic resources, such as WordNet and Roget thesaurus to provide comparative evaluation. Indeed, automatic evaluation, besides faster and cheaper than manual evaluation, allows repeatability of the experiments and hence comparison between alternate approaches. Therefore, we followed both evaluation approaches separate</context>
</contexts>
<marker>Smadja, 1993</marker>
<rawString>Smadja F. (1993) Retrieving Collocations from text: Xtract, Computational Linguistics vol. 19.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Thanopoulos</author>
<author>N Fakotakis</author>
<author>G Kokkinakis</author>
</authors>
<title>Comparative Evaluation of Collocation Extraction Metrics.</title>
<date>2002</date>
<booktitle>Proceedings of the LREC2002 Conference,</booktitle>
<pages>609--613</pages>
<contexts>
<context position="1059" citStr="Thanopoulos et al., 2002" startWordPosition="147" endWordPosition="150">iscipline, we did not employ them in the present study. 3.1 The measures Likelihood Ratio (LR) is a mostly employed measure of statistical significance. However, although it reveals statistically strong correlations, it is not an equally good indicator of non compositionality. For example, its 10-best list from WSJC includes the bigrams years earlier and this year, due to their very high frequencies and despite their compositionality, which is apparent in the corpus as well; e.g. consider {week, day, month, year} earlier. Mutual dependency, a measure derived from pointwise mutual information (Thanopoulos et al., 2002), promotes multiwords occurring more often than not tied together: 2   D (w (w 1 w 2 )   , where c c  12  12 1 , 2 ) log P w 2 k log 2  1   2 1 , 2 . P(w 1 ) 2 c P(w ) 1 c2 Setting min=min( 1, 2), the maximum value of D is obtained when min = 1. In this case the constituent words are unbrokenly tied together, e.g. Ku Klux Klan. In order to allow for higher coverage (i.e. more multiwords to be identified) we relax this condition (i.e min = 1) allowing for lower values of min. For example, the condition min &gt; T = 0.5 is quite safe, allowing only bigrams with both their elements occur</context>
</contexts>
<marker>Thanopoulos, Fakotakis, Kokkinakis, 2002</marker>
<rawString>Thanopoulos A., Fakotakis N. and Kokkinakis G. (2002) Comparative Evaluation of Collocation Extraction Metrics. Proceedings of the LREC2002 Conference, pp.609-613.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>