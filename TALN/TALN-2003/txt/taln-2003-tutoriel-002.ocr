TALN 2003, Batz—sur—Mer, I I -1 4 juin 2003

Construction d’ontologics £1 partir dc tcxtcs

Didier Bourigault (1) et Nathalie Aussenac-Gilles

(1) ERSS — CNRS & Universite Toulouse le Mirail
5, allees Antonio Machado
31 058 Toulouse Cedex 1
didier .bourigault@univ-tlse2 .fr
(2) IRIT — Universite Paul Sabatier
118, route de Narbonne, 31062 Toulouse Cedex 4
aussenac@irit.fr

Résumé — Abstract

Cet article constitue le support d’un cours presente lors de la conference TALN 2003. I1
defend la place du Traitement Automatique des Langues comme discipline cle pour le
developpement de ressources termino-ontologiques a partir de textes. Les contraintes et
enjeux de ce processus sont identifies, en soulignant l’importance de considerer cette tache
comme un processus supervise par un analyste. Sont presentes un certain nombre d’outils
logiciels et methodologiques venant de plusieurs disciplines comme le TAL et l’ingenierie
des connaissances qui peuvent aider l’analyste dans sa tache. Divers retours d’experience sont
presentes.

This paper gathers the notes of a tutorial. We advocate in favour of the role of Natural
Language Processing as a key discipline for the development of terminological and
ontological resources from texts. The constraints and challenges of this process are identified,
and lead to underline this task as a supervised processes carried out by an analyst. We present
several software and methodological tools from NLP and knowledge engineering that can be
use for to assist the analyst. Our suggestion rely on various experience feed-back.

Keywords — Mots Clés

Extraction de termes, extraction de relations, Terminologie, Ontologies, Ingenierie des
connaissances, methode, modelisation de connaissances, interdisciplinarite.

Term extraction, relation extraction, Terminology, ontologies, Knowledge Engineering,
method, knowledge modelling, crossdisciplinarity.

D. Bourigault et N. Aussenac—Gilles

1 Introduction

Dans cet article, nous developpons les grandes lignes du cours presente lors de la dixieme
conference Traitement Automatique des Langues le 14 juin 2003 a Batz-sur-Mer. Ce cours
fait suite aux tutoriels donnes en juin 2000 lors de la conference Ingénierie des connaissances
(IC 2000, Toulouse) et en janvier 2002 lors de la conference Reconnaissance des Formes et
Intelligence Artzﬁcielle (RFIA 2002, Angers), au cours desquels nous avons eu l’occasion de
presenter les outils developpes en Traitement Automatique des Langues (TAL) aux membres
de la communauté d’Ingenierie des Connaissances (IC). L’objectif du present cours est,
symetriquement, de presenter aux chercheurs de la communauté Traitement Automatique des
Langues les enjeux, pratiques et theoriques, l’utilisation de certains outils de TAL dans une
perspective d’ingenierie des connaissances, ceci pour encourager les travaux
interdisciplinaires autour de la problematique de construction de ressources termino-
ontologiques (RTO)l, a partir de textes. Cette problematique constitue en effet un nouvel
enjeu important aussi bien pour le Traitement Automatique des Langues que pour l’Ingenierie
des Connaissances. Les systemes de traitement de l’information qui doivent fonctionner dans
des domaines de connaissances specialises ne peuvent étre efficaces que s’ils s’appuient sur
des ressources terrnino-ontologiques, construites pour le domaine et l’application concernes.
Les recherches et realisations en TAL et en IC doivent étre menees de facon
pluridisciplinaire, pour, d’une part, developper les outils de TAL pertinents pour la tache de
construction de RTO a partir de textes, et, d’autre part, elaborer des methodes d’acquisition
des connaissances a partir de textes qui speciﬁent comment utiliser les outils de TAL et les
environnements de modelisation des connaissances dans le contexte de la construction de
RTO. Au dela, mais cet aspect sera moins developpe dans ce cours, il s’agit de s’interroger
sur le statut de la langue ecrite comme revelateur de connaissances, des lors que l’on Veut y
acceder au moyens d’outils informatiques.

2 Des ressources £1 construirc variécs, des outils génériqucs

2.1 RTO et systemes de traitements de l’information

A la suite a l’utilisation generalisee des outils de bureautique, a l’internationalisation des
echanges et au developpement d'Internet, la production de documents sous forme electronique
s’accelere sans cesse. Or pour produire, diffuser, rechercher, exploiter et traduire ces
documents, les systemes de gestion de l’information ont besoin de ressources termino-
ontologiques, qui decrivent les termes et les concepts du domaine, selon un mode propre au
type de traitement effectue par le systeme. La gamme des ressources a base terminologique et
ontologique est aussi large que celle des systemes de traitement de l’information utilises dans
les entreprises et dans les institutions :

0 bases de donnees terminologiques multilingues classiques pour l'aide a la traduction,

1 Dans ce cours, nous nous efforcerons d’u11'liser systematiquement cette expression plutét que le terme tres en
vogue d’ontologie, adopte dans le titre du cours pour des raisons de concision. Ce choix terminologique sera
justifie plus loin dans cet article.

Construction d ’0nt0l0gies dpartir de textes

0 thesaurus pour les systemes d'indexation automatique ou assistée, index
hypertextuels pour les documentations techniques,

0 terminologies de référence pour les systemes d’aide a la rédaction,
0 référentiels terminologiques pour les systemes de gestion de données techniques,

0 ontologies pour les mémoires d'entreprise, les systemes d’aide a la décision ou les
systemes d’extraction d’inforrnation,

0 ontologies pour le Web sémantique,

0 glossaires de référence, liste de terrnes pour les outils de communication interne et
externe,
0 etc.

Du c6té de la recherche, chacune de ces ressources est prise en charge par une discipline
différente. La terminologie focalise ses recherches, depuis l’aVenement des outils de
bureautique, sur les bases de données terminologiques destinée aux traducteurs humains. Les
sciences de l’information et de la documentation concentrent leurs réﬂexions sur les thesaurus
et langages de classification ou langages documentaires, exploités par les documentalistes
pour indexer et classer les éléments de fonds documentaire. En informatique, le domaine de la
recherche d’information (RI) s’intéresse a des thesaurus d’un type différent, concus pour
limiter le bruit et augmenter le rappel des outils inforrnatiques de recherche d’information.
L’intelligence Artificielle et l’Ingénierie des Connaissances travaillent sur les ontologies
formelles qui constituent le coeur des systemes a base de connaissances. Ces différentes
disciplines développent de facon relativement autonome et cloisonnée des recherches
spécifiques sur ces différents types de ressources. Or, sous la pression des besoins et des
applications, elles sont amenées a considérer que, pour des raisons de pertinence et
d’efficacité, les ressources lexicales et/ou conceptuelles qu’elles doivent construire et
exploiter peuvent ou doivent étre construites a partir de sources textuelles. Elles sont donc
naturellement amenées a procéder a un rapprochement interdisciplinaire, dont le Traitement
Automatique des Langues peut étre le catalyseur, en tant que pourvoyeur de méthodes et
outils de construction de RTO a partir de textes.

En terminologie, au cours des années 80, un rapprochement avec l’informatique s’est opéré
avec le développement de la microinformatique. On s’est intéressé a la conception de bases de
données terminologiques susceptibles d’aider les traducteurs professionnels dans les taches de
gestion et d’exploitation de lexiques multilingues. Les réﬂexions ont porté essentiellement sur
le format de la fiche terminologique : a l’aide de quels champs décrire un terme dans une base
de données qui sera utilisée par un traducteur humain ? Depuis la fin des années 90, la
terminologie classique Voit les bases théoriques de sa doctrine ainsi que ses rapports avec
l’informatique ébranlés par le renouvellement de la pratique terrninologique que suscite le
développement des nouvelles applications de la terminologie. La multiplication des types de
ressources terminologiques met a mal le principe théorique de l’unicité et de la fixité d’une
terminologie pour un domaine donné, ainsi que celui de la base de donnée terrninologique
comme seul type de ressource informatique pour la terminologie. Depuis le milieu des années
90, un courant de recherche se développe autour de la terminologie textuelle, qui préconise la

D. Bourigault et N. Aussenac—Gilles

construction de terminologies a partir de textes, et qui sollicite le TAL pour des méthodes et
outils d’analyse de corpus (Slodzian, 2000). En Intelligence Artificielle, une évolution
importante du domaine s’est produite de facon concomitante et parallele a ce renouvellement
théorique et méthodologique en terminologie. L’échec relatif des réalisations en LA a conduit
a remettre en cause l’hypothese qui était a la base du développement des systemes experts,
selon laquelle l’expert d’un domaine serait le dépositaire d’un systeme conceptuel qu’il
suffirait de mettre au jour, en interrogeant l’expert ou en l’observant au travail. L’Ingénierie
des Connaissances (IC) s'est alors imposée comme une direction de recherche en LA, avec
pour ambition de résoudre les difficultés soulevées par la construction des systemes experts,
et de proposer des concepts, méthodes et techniques perrnettant d'acquérir et de modéliser les
connaissances dans des domaines se formalisant peu ou pas. L’IC s’intéresse en particulier au
processus de construction d’ontologies formelles pour les systemes a base de connaissances
ou pour l’interopérabilité entre systemes dans le Web sémantique. Elle préconise elle aussi
que, dans certains contextes, ce processus s’appuie sur l’analyse de corpus de textes. Elle
sollicite le TAL pour des outils rendant possible et efficace la tache de construction
d’ontologies a partir de textes.

Des sollicitations analogues émanent aussi d’autres disciplines, comme les sciences de
l’information et de la documentation. Au sein méme du domaine du Traitement Automatique
des Langues, certaines applications, comme la traduction automatique, la recherche
d’information ou l’extraction d’information ont besoin de ressources termino-ontolo i ues

, 8 Q ,
Le TAL est donc ainsi doublement concemé par la problématique de la construction de RTO
a partir de textes, en tant que consommateur de ressources et en tant que pourvoyeur d’outils
pour les construire. Le TAL se trouve donc a la convergence de demandes émanant de
disciplines diverses et concemant la mise a disposition d’outils et de méthodes d’analyse de
textes pour la construction de ressources termino-ontologiques. Il peut adopter ainsi une
position décalée par rapport a chacune de ces disciplines et saisir, grace a cet angle de vue
privilégié, les proximités et les différences entre des différents types de ressources, avec une
objectivité et un recul, que ne peuvent avoir ces disciplines seules. En ce sens, le TAL peut
favoriser le décloisonnement de ces disciplines et encourager le rapprochement
pluridisciplinaire, autour d’une réﬂexion sur la notion de ressource termino-ontologique.
Cette réﬂexion doit permettre de mettre en evidence is les ressemblances et les particularités
de ces différents types de ressources, de facon a specifier les types d’outils d’analyse
relativement génériques et utilisables pour une large gamme de ressources et de contextes
d’exploitation.

2.2 Ontologie, terminologie, thesaurus, 

Le TAL se trouve donc face a des disciplines chacune préoccupée par le probleme de la
construction de ressources termino-ontologiques de types différents, puisque destinées a des
usages différents. Dans ce contexte de sollicitations diversifiées, il est non pertinent pour le
TAL de se lancer dans une réﬂexion théorique visant a caractériser formellement et de facon
générique ce qu’est une ressource termino-ontologique. L’approche consiste plut6t a mettre
en perspective les différentes definitions travaillées par ces disciplines. L’objectif est de saisir
en quoi les caractéristiques spécifiques de ces différents types de ressources dépendent des
contextes applicatifs, pour finalement identifier ce qui différencie et, surtout, ce qui rapproche
ces différents types de ressources. Il est alors possible de specifier les différents types d’outils
génériques de TAL dont il convient de promouvoir le développement.

Construction d ’0nt0l0gies dpartir de textes

Les reﬂexions sur les ontologies se sont d’abord developpees en informatique (intelligence
artificielle, sciences de la gestion), dans le cadre de travaux qui avaient comme obj ectif ﬁnal
la specification de systemes informatiques, avec plus particulierement a l’origine la volonte
de pouvoir reutiliser des composants generiques d’une application a une autre, ou encore de
favoriser la communication entre differentes applications. C’est le cas encore des travaux
menes en Ingenierie des Connaissances ou en representation des connaissances autour des
Systemes a Base de Connaissances et du Web semantique. Dans ce contexte, une ontologie
est une conceptualisation des objets du domaine selon un certain point de vue, impose par
l’application. Elle est concue comme un ensemble de concepts, organises a l’aide de relations
structurantes, dont la principale, celle avec laquelle est construite l’ossature de l’ontologie, est
la relation is-a. Cette conceptualisation est ecrite dans un langage de representation des
connaissances, qui propose des << services inferentiels >> (classification de concept, capacite de
construire des concepts definis a partir de concepts primitifs, etc.). A l’oppose, pour les
thesaurus, un haut degre de formalisation et des services d’inference ne sont pas necessaires.
Les thesaurus sont organises avec les classiques relations d’hyperonymie et de synonymie,
auxquelles s’ajoute la relation voir aussi. Neanmoins, il faut bien distinguer les thesaurus
selon qu’ils sont exploites par des indexeurs et documentalistes humains, ou par des systemes
informatiques. Au cours d’une tache d’indexation, pour choisir les meilleurs descripteurs, les
agents humains procedent a des interpretations et des inferences, qui s’appuient sur leur
connaissance du domaine et des utilisateurs, connaissances implicites qui ne sont pas
consignees dans le thesaurus. Les systemes d’indexation automatique ne peuvent approcher
de tels comportements intelligents qu’a condition que ces connaissances soient autant que
possible explicitees et representees dans les thesaurus, qui tendent ainsi a se rapprocher des
ontologies de l’Ingenierie des Connaissances.

Le principal critere de discrimination entre RTO est le type de donnees d’entree du systeme
de traitement de l’information qui exploite la RTO. Selon que ces systemes traitent de
l’information de nature textuelle ou non, les caracteristiques des RTO vont étre relativement
differentes. Si le systeme analyse des entree en langue naturelle, la premiere exigence est
qu’il soit capable de reconnaitre sous des formes linguistiques differentes des occurrences de
la meme unite et, inversement, de reconnaitre des unites differentes sous une meme forme. Il
doit pouvoir gerer, aussi bien que l’application l’exige, les phenomenes de synonymie, de
paraphrase, de variabilite linguistique aux niveaux morphologique ou syntaxique ou lexical,
presents en masse dans les textes en langues naturelles (Zweigenbaum, 1999). Ceci n’est
possible que si des regles de correspondance sont repertoriees dans la RTO que Va exploiter le
systeme. Une des taches de l’analyste qui construit la RTO est donc de decrire des liens entre
des motifs textuels et des unites de traitement, unites qui seront ensuite exploitees pour
effectuer les traitements assignes aux systeme (classification de document, expansion de
requéte, extraction d’inforrnation, etc.). Quand les motifs textuels ont la structure de noms ou
syntagmes nominaux, ils sont naturellement designes sous le nom de termes. Les unites de
traitement sont les concepts. C’est la raison pour laquelle nous parlons de ressources termino-
ontologiques. De ce point vue, le concept peut étre vu comme une classe d’equivalence de
terrnes, ou plus generalement de motifs textuels, modulo les contraintes de l’application
cible : deux motifs sont juges equivalents, ou synonymes, en fonction de traitement que doit
effectue par le systeme. Le concept est un mode de regroupement de termes. Ceci n’est pas
incompatible avec sa fonction de regroupement d’objets (informatiques) du domaine qui lui
est assignee dans les ontologies de l’Ingenierie des Connaissances. Le systeme de traitement
de l’information dispose donc pour traiter de la synonymie de regles d’appariement qui

D. Bourigault et N. Aussenac—Gilles

exploitent les liens termes/concepts présents dans la RTO. Il dispose de regles analogues pour
le traitement de la polysémie, de l’homographie.

Si l’application cible n’est pas une application textuelle, l’analyse des textes n’en est pas
moins fondamentale. Méme s’il s’agit de construire une ontologie pour un systeme
informatique, dont les données d’entrée ne seront pas textuelles, mais numériques, par
exemple des résultats de mesures de capteur, l’analyse de textes et la description du
Vocabulaire sont néanmoins primordiales pour la construction de l’ontologie. En effet,
l’analyse des textes sert d’indicateur a l’organisation d’un systeme conceptuel et donc a la
mise en relation de concepts, et, par ailleurs, le choix des étiquettes de concepts doit étre
judicieux pour assurer l’interprétabilité et l’intelligibilité du systeme, ainsi que la
maintenance de l’ontologie (Bachimont, 2000).

Cette position constructiviste et fonctionnelle des notions de terrne et de concept s’éloigne
quelque peu des positions référentialistes et ﬁxistes - le terrne comme etiquette de concept -,
qui sont classiquement adoptées dans les domaines de l’Intelligence Artiﬁcielle, de la
terrninologie ou du Traitement Automatique des Langues, disciplines qui ont longtemps été
largement inﬂuencées par une sémiotique du signe fondée sur la triade terme/concept/référent
(Rastier, 1991). La conception classique pose que le terme existe en tant que représentant
linguistique d’un concept faisant partie d’un systeme conceptuel unique et stable caractérisant
a priori le domaine. Mais le constat de la variabilité des terminologies s’impose : étant donné
un domaine d’actiVité, il n’y a pas une terminologie, qui représenterait le savoir sur le
domaine, mais autant de ressources termino-ontologiques que d’applications dans lesquelles
ces ressources sont utilisées. L’ensemble de ces constats empiriques appelle a un
renouvellement théorique de la terminologie (Rastier, 1995) (Slodzian, 2000). A rebours de la
conception fixiste et apriorique, on peut Voir le terme et le concept comme le résultat d’un
processus d’analyse termino-conceptuelle. Un mot ou une unité complexe n’acquiert le statut
de terrne que par décision. Dans le cas qui nous concerne ici, cette décision est prise par
l’analyste en charge de l’élaboration d’une RTO pour une application bien identiﬁée. Celui-ci
déﬁnit son propre référentiel de decision. 11 procede a un travail de construction d’une
ressource terrnino-ontologique pour une application dans le domaine, et non de découverte de
la terminologie du domaine. Ce travail est guidé par une double contrainte de pertinence :

0 pertinence Vis-a-Vis du corpus. Il s’agit de retenir et de décrire des structures
lexicales qui présentent des caractéristiques a la fois spéciﬁques au domaine et
stables dans le corpus ;

0 pertinence Vis-a-Vis de l’application Visée. Les unités ﬁnalement retenues doivent
l’étre en fonction de leur utilité dans l’application visée, qui s’exprime en termes
d’économie, de cohérence interne et d’efﬁcacité.

Construction d ’0nt0l0gies dpartir de textes

3 Eléments méthodologiques

3.1 Des outils d’aide

Devant la masse des données a analyser et étant donnés les délais de réalisation imposés, les
disciplines concemées par la construction de RTO se tournent vers le TAL pour des outils
informatiques d’analyse de corpus.

Les travaux de conception d’outils de TAL pour la construction de RTO doivent développer
une réﬂexion méthodologique sur l’activité de construction elle-méme. Doit s’imposer
d’emblée le postulat que cette activité est avant tout une activité humaine, intellectuelle,
menée par un individu que nous nommerons ici << analyste >>. Dans un projet de construction
de RTO, les contraintes sont multiples et multiforrnes, les choix a effectuer nombreux et de
types divers, et ces choix comme ces contraintes, de type heuristique, sont difﬁcilement
explicitables. ont jusqu’ici été peu explicités. Par conséquent cette tache ne peut en rien se
limiter a l’élaboration automatique d’un réseau de termes et de concepts par quelque outil que
ce soit. Nous défendons que la contribution du TAL doit étre la foumiture d’outils d’aide pour
l’analyste. Les recherches doivent se développer dans le paradigme de la coopération, et non
celui de l’automatisation, méme partielle, et il faut assumer, dans une perspective
ingénierique, le role central de l’analyste. Autant les outils de TAL consommateurs de
ressources termino-ontologiques doivent et peuvent approcher l’automaticité, autant les outils
de TAL d’aide a la construction de RTO exigent l’intervention d’un agent humain.

Au-dela des difﬁcultés techniques traditionnellement liées au développement d’outils en
TAL, il existe une tension particuliere propre au développement d’outils d’aide a la
construction de RTO : il s’agit de concilier le caractere ad hoc des ressources a construire
avec les outils, avec les contraintes de généricité, transportabilité, reproductibilité, qu’impose
le développement de la recherche. Autrement dit, il faut chercher a développer des outils de
TAL relativement génériques quant au domaine et au type d’application, pour des utilisations
elles tres ciblées quanta ces deux points.

Rapidement, on peut classer les types d’outils a construire selon deux axes. Du point de vue
fonctionnel, on peut distinguer les outils d’aide a l’acquisition de termes et les outils d’aide a
la structuration de termes et au regroupement conceptuel (section 4). Du point de vue du
mode d’utilisation, on peut distinguer les outils qui fonctionnent << en batch >> (ils traitent
l’ensemble du corpus, puis foumissent les résultats a l’analyste), et les outils interactifs. Par
ailleurs, puisque les décisions prises par l’expert s’appuient in ﬁne sur l’analyse de contextes
dans le corpus, a c6té des outils de traitement massif de corpus, il faut foumir a l’analyste des
moyens d’acce‘s au texte (concordanciers, outils de navigation hypertextuelle, etc.).

3.2 R6le de l’analyste

Dans l’idéal, la personne chargée de construire la RTO, l’analyste, devrait avoir a la fois des
compétences métier, des compétences en modélisation des connaissances et en linguistique et
des compétences en inforrnatique. Ce proﬁl fait-il de l’analyste un oiseau rare ? Dans la
réalité, il faut mettre en place une collaboration entre acteurs de spécialités différentes.
Plusieurs sortes de situations peuvent étre rencontrées. Pour les applications a forte dimension

D. Bourigault et N. Aussenac—Gilles

cognitive, l’expérience montre que l’efficacité maximale peut étre atteinte quand la
construction de la RTO est assurée par un spécialiste métier, passionné par les problemes de
langue et de connaissance, ou formé a ceux-ci, qui comprend bien les specifications de
l’application cible et qui est capable de dialoguer avec les informaticiens qui la développent.
A l’opposé, certaines applications, de type documentaire, ne requierent pas une implication
forte des spécialistes et la construction de la RTO peut étre réalisée par des personnes ayant le
proﬁl et l’expérience de documentaliste ou de terrninologue. Dans tous les cas, l’intervention
d’un analyste médiateur est nécessaire quand l’application exige la participation de plusieurs
spécialistes.

3.3 Place du corpus

Dans un projet de construction de RTO a partir de textes, la tache de construction du corpus
est a la fois primordiale et délicate. Puisque, d’une part, le corpus est la source d’inforrnation
essentielle pour tout le processus de construction de la RTO et que, d’autre part, il restera, une
fois le processus achevé, l’élément de documentation de la ressource construite, il doit étre
composé avec un maximum de précautions méthodologiques. Dans ce domaine, il n’est hélas
pas encore possible de déﬁnir a priori des instructions méthodologiques tres précises pour
encadrer la tache de sélection des sources textuelles qui Viendront constituer le corpus. Au-
dela des problemes techniques ou politiques de disponibilité des textes, cette collecte doit se
faire avec l’aide des spécialistes et en fonction de l’application cible Visée. Il convient en effet
de s’assurer aupres des spécialistes que les textes choisis ont un statut sufﬁsamment
consensuel pour éviter toute remise en cause ultérieure de la part d’utilisateurs ou de leur part.
Par ailleurs, il convient de prévoir d’emblée une boucle de rétroaction au cours de laquelle
une premiere Version du corpus sera modifiée et enrichie en fonction d’une premiere phase
d’analyse des résultats fournis par les outils de TAL sur cette Version initiale. Le critere de la
taille est évidemment important, méme s’il est impossible de donner un chiffre idéal. Le choix
est ici encore un compromis. Le corpus doit étre sufﬁsamment << gros >> pour justiﬁer que des
outils de traitement de la langue soient nécessaires pour le dépouiller de facon efﬁcace. Mais
il doit étre suffisamment petit et/ou redondant pour pouvoir étre appréhendé de facon globale
par l’analyste, méme a l’aide d’outils de TAL. Une fourchette entre 50 000 et 200 000 mots
semble raisonnable. Les projets prenant le Web comme source de textes font rapidement
exploser ces chiffres, posant par la méme des problemes spéciﬁques, comme celui de la
definition d’un << échantillon>> pertinent pour l’étude. Enﬁn, dans la majorité des cas, le
corpus sera hétérogene dans le sens ou il aura été constitué en rassemblant des textes
d’origine Variée. Il est alors absolument nécessaire de procéder a un balisage du corpus qui
permettra aux outils d’analyse, et ainsi qu’a l’analyste, de repérer les différents sous-corpus
pour procéder éventuellement a des analyses contrastives.

3.4 Utilisation de ressources existantes

On l’aura compris, nous ne nous intéressons ici ni aux ontologies génériques (<< a la Cyc >>)
censées représenter un ensemble maximal de connaissances, de sens commun, ni aux
ontologies formelles (au sens de Guarino) qui constitueraient un cadre référentiel universel et
formellement Valide, mais bien a des ressources terrnino-ontologiques exploitées par un
systeme particulier de traitement de l’inforrnation dans un domaine particulier. C’est l’usage
prévu de la ressource qui contraint et encadre sa construction. Pour autant, nous ne souhaitons

Construction d ’0nt0l0gies dpartir de textes

pas participer a une polemique sur l’opposition ontologies generales vs. ontologies
specialisees. Notre position est la suivante: il est primordial que les outils d’aide a la
construction de RTO puissent recycler des donnees existantes afin de tirer le meilleur parti du
patrimoine terminologique possede par les entreprises et les institutions (Jacquemin, 1997).
Pour une tache de construction de RTO, il faut faire feu de tout bois, et chercher a exploiter
autant que faire se peut toutes les ressources disponibles, et pas uniquement les textes. Sur le
plan de la politique de la recherche, nous pensons qu’il est utile de promouvoir des travaux
montrant l’utilite de ressources lexicales existantes (generales, comme la base WordNet ou
des fichiers electroniques de synonymes, ou specialisees, comme les grands thesaurus de la
medecine comme UMLS) dans la perspective d’ameliorer le rendement du couple
analyste/outils de TAL. Nous sommes plus reserves sur la necessite de degager des
ﬁnancements lourds pour la realisation de nouvelles ressources semantico-conceptuelles de
taille gigantesque, elaborees hors de toute speciﬁcation d’application cible. Il nous semble
plus pertinent que soient encourages des experiences d’evaluation, necessairement tres
lourdes, proposant des protocoles experimentaux capables de mettre en evidence a grande
echelle les gains en temps et en qualite apportes par l’introduction d’une ontologie dans tel ou
tel systeme de traitement de l’information par rapport au coﬁt de la construction de cette
ontologie (cf. section 6).

3.5 De la nécessité d’interface intégratrices

La tache de construction d’une RTO est incrementale et comporte de nombreux
enchainements d’essais/erreurs. Il faut des interfaces ergonomiques permettant une utilisation
coordonnee et optimale des differents outils de traitement et de consultation du corpus de
reference, par l’analyste qui construit une RTO, a l’instar de (Ait El Mekki et Nazarenko,
2002) pour la construction d’index d’ouvrages, de la plate-forme de modelisation TERMINAE
pour la construction de terrninologies et d’ontologies (Szulman et al., 2002). De facon plus
generale, l’utilisation de ces differents outils doit étre encadree par une methodologie
precisant a quel stade du processus et selon quelles modalites il convient de les utiliser. En
effet, la solution au probleme de l’acquisition de ressources termino-ontologiques a partir de
corpus ne reside pas uniquement en la foumiture d’un ou de plusieurs outils de traitement
automatique des langues. La mise a disposition de tels outils doit s’accompagner d’une
reﬂexion methodologique poussee, conduisant a la realisation de guides methodologiques et
de plates-formes logicielles integratrices permettant la mise en oeuvre efﬁcace des outils
proposes. Cette necessite appelle une cooperation entre TAL et IC. Cette reﬂexion sur
l’utilisation combinee de differents types d’outils d’analyse de textes en ingenierie
terrninologique est aussi tres presente dans un certain nombre de travaux en ingenierie des
connaissances (Charlet et al., 2000).

3.6 Une proposition methodologique

A titre d’exemple, nous evoquons une proposition methodologique integrant l’utilisation de
plusieurs outils de TAL et qui se veut une reponse possible aux differents problemes
evoques: la methode TERMINAE (Szulman et al., 2002). Cette methode s'appuie sur des
travaux representatifs du courant francais de travaux a la convergence entre terminologie,

D. Bourigault et N. Aussenac—Gilles

lin uisti ue in enierie des connaissances et intelli ence artificiellez. Elle s’a uie sur les
5
pnnc1pes su1Vants :

Partir de textes du domaine comme sources de connaissances : ils constituent un support
tangible, rassemblant des connaissances stabilisees qui servent de reference et ameliorent
la qualite du modele ﬁnal ;

Enrichir le modele conceptuel d’une composante linguistique : l’acces aux terrnes et aux
textes qui justiﬁent la definition des concepts garantit une meilleure comprehension du
modele ;

Utiliser des techniques et outils de TAL bases sur des travaux linguistiques: ils
permettent l'exploitation systematique des textes et leurs resultats facilitent la
modelisation ;

Construire des ontologies << regionales >>, c’est-a-dire consensuelles dans un domaine et
adaptees a une application, mais non universelles ;

Appliquer des principes de modelisation systematiques pour assurer une bonne
structuration des donnees et faciliter la maintenance de l’ontologie.

TERMINAE Vise essentiellement la constitution de terminologies, reseaux conceptuels et
ontologies. La methode comprend quatre etapes, les trois dernieres etant mises en oeuvre de
maniere cyclique. L'importance de chacune depend du produit terminologique Vise et des
obj ectifs d'utilisation de ce demier.

0 La Constitution d'un corpus Vise a choisir documents techniques, comptes rendus,
livres de cours, etc. a partir d'une analyse des besoins de l'application.

0 L'etude linguistique consiste a identifier des terrnes et des relations lexicales, en
utilisant des outils de traitement de la langue naturelle (SYNTEX comme extracteur de
terrnes, UPPERY comme outil d'analyse distributionnelle, Cameleon pour l’aide au
reperage de relations par des patrons linguistiques, YAKWA comme concordancier).

0 La normalisation semantique conduit a deﬁnir dans un langage forrnel des concepts et
des relations semantiques que nous appelons terminologiques car provenant des
terrnes et relations precedemment etudies (Biebow & Szulman, 1999). Leur
structuration en reseau s'appuie sur les resultats du depouillement des textes tout en
tenant compte de l'objectif d'utilisation de l'ontologie. Elle necessite l'ajout de
nouveaux concepts et relations dits de structuration.

0 La formalisation permet de preciser, completer et Valider le modele construit lors de la
nor-malisation. L'analyste indique si les concepts sont primitifs ou deﬁnis, Veriﬁe que
les relations sont a la bonne place pour favoriser un heritage maximum, etc.

Le logiciel TERMINAE associe a la methode fournit des aides pour toutes les etapes de
l'analyse des textes a la formalisation. Il offre un support methodologique qui permet
d'eVoluer progressivement et en conservant des liens des textes Vers les niveaux linguistique

2 Ce courant, anime au sein du GDR-I3 et de l'AF IA par le groupe TIA (http://www.bion1ath.jussieu.fr/TIA/)

dont les auteurs font partie.

Construction d ’0nt0l0gies dpartir de textes

et conceptuel. Le logiciel assure donc une continuité entre les différentes formes de
l'ontologie. Celle-ci passe d'un état proche d'une taxinomie de termes a un réseau conceptuel
enrichi de relations et de concepts de structuration pour aboutir a une ontologie forrnelle. Elle
est décrite dans un langage formel masque a l'analyste qui permet de veriﬁer des contraintes
de validité minimale.

4 Outils dc TAL pour la construction dc RTO

4.1 Une typologie fonctionnelle

Dans cette section3, nous passons en revue un certain nombre de travaux de recherche sur le
développement d’outils d’aide a la construction de RTO a partir de textes. Nous avons choisi
de les présenter selon une typologie de fonctionnelle.

0 Acquisition de termes. Une premiere classe regroupe les outils dont la visée est
l’extraction a partir du corpus analysé de candidats termes, c’est-a-dire de mots ou
groupes de mots susceptibles d’étre retenus comme termes par un analyste, et de
fournir des étiquettes de concepts. Ces outils different principalement quant au type
de techniques mises en oeuvre (syntaxique, statistique, autres).

0 Structuration de termes et regroupement conceptuel. Les ressources termino-
ontologiques se présentent rarement sous la forme d’une liste a plat. Des outils
d’aide a la structuration d’ensembles de termes sont donc nécessaires. Dans cette
classe, nous évoquerons, d’une part, des outils de classiﬁcation automatique de
termes, et, d’autre part, des outils de repérage de relation. Signalons que beaucoup
d’outils d’extraction proposent déja une structuration des candidats termes extraits.

4.2 Acquisition de termes

L'outil TERMINO est une application pionniere de l'acquisition automatique de termes (David
et Plante, 1990). Construit sur la base de l'atelier FX, un formalisme pour l'expression de
grammaires du langage naturel et un analyseur associé, TERMINO se focalise sur le repérage
des syntagmes nominaux qui sont les seules structures supposées produire des termes. Les
candidats termes extraits par TERMINO sont appelés “ synapsies” d'apres les travaux de
Benveniste. La chaine de traitement de TERMINO se compose d’une phase d’analyse
morphosyntaxique suivie d’une phase de génération des synapsies a partir des dépendances
entre téte et compléments rencontrés dans la structure de syntagme nominal retournée par
l'analyseur. ANA est un outil d'acquisition terrninologique qui extrait des candidats termes sans
effectuer d’analyse linguistique (Enguehard et Pantera, 1995). Les termes sont reconnus au
moyen d'égalités approximatives entre mots et d'une observation de répétitions de patrons.
ACABIT extrait des candidats termes a partir d’un corpus préalablement étiqueté et
désambiguisé (Daille, 1994). ACABIT méle des traitements linguistiques et des filtres

3 Cette partie est exlraite et adapte'e de (Bourigault et Jacquemin, 2000) parue dans l’ouvrage Industrie des
lcmgues (Hermes) coordonne’ par J .-M. Pierrel . Une bibliographie mise a jour sera fournie lors du cours.

D. Bourigault et N. Aussenac—Gilles

statistiques. L'acquisition terminologique dans ACABIT se deroule en deux etapes : (1) analyse
linguistique et regroupement de Variantes, au cours de laquelle n ensemble de transducteurs
analyse le corpus etiquete pour extraire des sequences nominales et les ramener a des
candidats termes binaires ; (2) filtrage statistique, au cours duquel les candidats termes
binaires produits a l'etape precedente sont tries au moyen de mesures statistiques. A l’instar
d’ACABIT, LEXTER extrait des candidats termes a partir d’un corpus prealablement etiquete et
desambigu'1'se (Bourigault, 1994). Il effectue une analyse syntaxique de surface pour reperer
les syntagmes nominaux maximaux, puis une analyse syntaxique profonde pour analyser et
decomposer ces syntagmes. Il est dote de procedures d’apprentissage endogene pour acquerir
des informations de sous-categorisation des noms et adj ectifs propres aux corpus. Il organise
l’ensemble des candidats termes extraits sous la forme d’un reseau. FASTR est un analyseur
syntaxique robuste dedie a la reconnaissance en corpus de termes appartenant a une liste
contr6lee foumie au systeme (J acquemin, 1997). Les termes n'ayant pas touj ours, en corpus,
la meme forme linguistique, le principal enjeu est de pouvoir identifier leurs Variantes. FASTR
est dote d’un ensemble elabore de metaregles, qui lui permettent de reperer differents types de
Variation : les Variantes syntaxiques, morpho-syntaxiques et semantico-syntaxiques.
L’environnement SYMONTOS (Velardi et al., 2001) propose des outils pour reperer des termes
simples et complexes dans des textes et des criteres pour decider de definir des concepts a
partir de ces termes.

4.3 Structuration de termes et regroupement conceptuel

La gamme des outils d’aide a la structuration de terminologie est tres large. Sont susceptibles
d’emarger a cette categorie un certain nombre de types d’outils qui n’etaient pas initialement
concus specifiquement pour cette tache, mais qui ont ete developpes pour des applications
d’informatique documentaire ou d’extraction d’informations, par exemple. Nous balayons
rapidement un spectre assez large, couvrant les outils de classification de termes sur la base
de cooccurrences dans des textes ou dans des fenétres, les outils de classification de termes
sur la base de distributions syntaxiques et les outils de reperage de relations. Les outils de
cooccurrence developpes dans le domaine de la recherche d’inforrnation rapprochent des
termes qui apparaissent frequemment dans les memes (portions de) documents, et qui
possedent donc sans doute une certaine proximite semantique. La technique de recherche de
cooccurrents est deja ancienne (a l'echelle de l'histoire de l'informatique) puisqu'elle a ete
promue tres tot en inforrnatique documentaire pour permettre l'expansion de requétes (Sparck
Jones, 1971). Parmi les applications dans le domaine de l’acquisition terminologique, on peut
citer le projet ILLAD (Toussaint et al., 1998), et les travaux de G. Lame (2002). Toujours dans
le domaine de l’informatique documentaire, les travaux dans le domaine de la construction
automatique de thesaurus peuvent étre reinvestis dans des applications terminologiques. Par
exemple, la chaine de traitement developpee par G. Greffenstette construit automatiquement
des classes comportant des noms qui se retrouvent regulierement comme arguments des
memes Verbes (Greffenstette, 1994). Ce reperage de la position argumentale des noms se fait
grace a l’exploitation d’un analyseur syntaxique de surface a large couverture. Ces techniques
inspirees de la linguistique harrissienne, qui Visent a rapprocher les termes qui ont des
distributions syntaxiques analogues, sont a la base de nombreux travaux depuis plusieurs
annees (Assadi, 1998) (Habert et al, 1996) (Faure, 2000).

Les outils que nous Venons d’eVoquer Visent a rapprocher des termes a partir d’une analyse
globale de l’ensemble de leurs occurrences. Ils ne touchent que les termes frequents, et donc

Construction d ’0nt0l0gies dpartir de textes

le plus souvent des noms simples, et proposent une simple relation d’équiValence
(appartenance a une classe). A c6té de ces outils qui travaillent sur les types comme
regroupement des occurrences, on trouve les outils de repérage de relations, qui travaillent au
niveau des occurrences elles-mémes. Ces outils détectent en corpus des mots ou contextes
syntaxiques répertoriés comme susceptibles de “ marquer ” telle ou telle relation entre deux
éléments. Les travaux de M. Hearst, sur l'extraction automatique des liens d’hyperonymie,
font ﬁgure de référence (Hearst, 1992). Les recherches sur ce theme se déclinent de multiples
facons. L’un des enjeux principaux conceme la généralité des relations, et celles des
marqueurs de relations. D’un c6té, il existe probablement des relations que l’on jugera
toujours pertinentes pour décrire un domaine de connaissance, par exemple les relations de
type hiérarchique ou partitive, et des marqueurs pour ces relations eux aussi généraux
(Garcia, 1998). A l’opposé, il est indéniable que chaque domaine est structuré par des
relations qui lui sont spécifiques, et qu’il convient nécessairement de prendre en compte pour
décrire le domaine. De plus méme dans le cas de relations considérées comme générales, il est
possible que les marqueurs susceptibles de conduire a les identifier different d’un corpus a
l’autre. Se pose alors le probleme de l’apprentissage inductif de ces marqueurs de relation. Un
certain nombre de travaux en TAL et en IC sont consacrés a ce probleme. Ils partent tous du
méme principe d’une recherche itérative alternée dans le corpus a la fois des marqueurs d’une
relation donnée et des couples de termes qui entrent dans cette relation (Rousselot et al.,
1996) (Séguéla et Aussenac-Gilles, 1999) (Morin, 1999) (Condamines et Rebeyrolles, 2000)
(l\/Iaedche et Staab, 2000).

5 Trois retours d’expérience

5.1 Contextes

Les exemples, démonstrations et expérimentations proposés pendant le cours sont issus
principalement de 3 expériences réelles de construction de RTO a partir de textes4. Ces trois
expériences couvrent un spectre large de types de domaines et de types d’applications : la
premiere experience a été menée dans le domaine technique de la fabrication du Verre (projet
VERRE), avec comme application cible la classification de documents ; la deuxieme
experience a été menée dans un domaine médical de la réanimation chirurgicale (projet REA),
avec comme application cible le codage d’actes médicaux; la troisieme experience a été
menée dans le domaine juridique du Droit francais codifié (projet DROIT), avec comme
application cible l’aide a la reformulation de requétes. Il s’agit a chaque fois de projets de
Recherche et Développement, dans lesquels l’application cible n’est pas strictement spécifiée
au départ du projet, comme cela devrait l’étre dans un << Vrai >> projet industriel. On doit donc
étre prudent au moment de tirer des conclusions générales. Néanmoins, chacun de ces projets
est allé a son terme, en ce sens qu’il n’a pas conduit a des RTO << jouets >>, mais a des
ressources completes qui sont ou seraient exploitables. Par ailleurs, chaque projet a permis de
tester certaines hypotheses méthodologiques faisant ainsi progresser les recherches dans le
domaine de l’acquisition des connaissances a partir de textes. C’est en multipliant ce type

4 Cette partie est extraite et adapte'e d’un article a paraitre dans un nume'ro spe'cial de la Revue d’Intelligence
Arlificielle, coordonne' par M. Slodzian et J .-M. Pierrel (Aussenac-Gilles & al, 2003)

D. Bourigault et N. Aussenac-Gilles

d’expériences que l’on avancera sur la definition d’un cadre méthodologique relativement
précis qui aille au-dela d’un simple recueil de bonnes pratiques et qui puisse satisfaire les
exigences d’un transfert vers les applications industrielles.

5.1.1 Le projet VERRE .' une ontologie dans le domaine de la fabrication et d’utilisati0n
de la ﬁbre de verre

Le premier projet vient répondre a une demande du centre de recherche du groupe Saint-
Gobain. Au sein des différentes filiales du groupe, l’avance technologique et industrielle est
primordiale pour conserver une place compétitive par rapport aux entreprises concurrentes.
Les activités de veille documentaire et technologique jouent alors un role crucial, et font
l’objet d’un outillage informatique de plus en plus performant. Parmi ces activités, une
demande récurrente des documentalistes porte sur la definition d’un outil d’aide au repérage
de nouveaux documents pertinents sur le Web (comme des brevets, des dépéches de presse,
etc.) et a leur classement en fonction des domaines d’intérét des ingénieurs qui les consultent.
Or la plupart des outils de routage de documents s’appuient sur un réseau conceptuel d’autant
plus perforrnant qu’il est enrichi des connaissances et de la terminologie du domaine de
l’entreprise. L’objectif du projet était donc de tester la faisabilité du développement d’une
ontologie dans l’objectif de l’utiliser pour guider le classement de documents en fonction des
proﬁls des utilisateurs. Dans ce projet, les aspects méthodologiques étaient tout aussi
importants que l’ontologie elle-méme. L’étude a été menée par deux chercheurs de l’IRIT, A.
Busnel pour l’analyse terrninologique et ontologique, et N. Aussenac-Gilles sur les aspects
méthodologiques. Un début d’ontologie (50 concepts, 20 relations) a été mis en forme a l’aide
du logiciel de modélisation TERMINAE, a partir de l’analyse d’un corpus de langue anglaise
composé de différents types de documents sur le domaine. Les logiciels de Traitement
Automatique des Langues SYNTEX, UPERY et YAKWA ont été utilisés pour le dépouillement
de ces corpus. Une proposition méthodologique utilisable dans le contexte de cette entreprise
et pour ce type d’application a été mise en forme (Aussenac-Gilles & Busnel, 2002).

5.1.2 Le projet REA .' une ontologie dans le domaine de la traumatologie en réanimation
chirurgicale

Le deuxieme projet a été encadré par M.-C. Jaulent et J. Charlet et a été mené a bien au sein
de l’UFR Broussais-Hotel-Dieu. Le contexte est celui du codage des actes médicaux par les
médecins. Pour leur activité de codage obligatoire, les praticiens s’aident d’un thésaurus de
spécialité qui a été élaboré de facon a ce que les séjours de réanimation soient le mieux
possible valorisés. Il est aujourd’hui reconnu que l’ambigu'1'té du thésaurus est une source
d’erreurs et de disparités de codage. Dans un domaine particulier tel que la réanimation
chirurgicale, on ne peut envisager de réaliser des outils informatiques d’aide au codage
qu’apres avoir préalablement organisé des objets du domaine, en fonction de la tache a
résoudre, par le biais d’une ontologie. L’objectif de ce deuxieme projet était donc de
construire une ontologie du domaine de la réanimation chirurgicale. Les outils de Traitement
Automatique des Langues SYNTEX et UPERY ont été utilisés pour traiter un corpus de comptes
rendus d’hospitalisation. Le travail a été réalisé par S. Le Moigno, médecin spécialiste, dans
le cadre d’un stage de DEA en informatique médicale. L’ontologie comprend environ 2 000
concepts et 200 liens (Le Moigno et al., 2002).

Construction d ’0nt0l0gies dpartir de textes

5.1.3 Le projet DROI T .' une ressource ontologique dans le domaine du Droit

Le troisieme projet a été mené par G. Larne, au cours de sa these au Centre de Recherche en
Informatique de l’Ecole des Mines de Paris (Larne, 2002). Ce centre de recherche a créé et
héberge le site juridique droit.org, qui diffuse l'édition Lois et décrets du Journal Ofﬁciel de la
République francaise, ce qui représente 95 000 documents (lois, décrets, arrétés), ainsi que les
codes du droit francais (Code civil, Code pénal, etc.) et des textes européens (directives,
reglements). L’objectif du travail était de tester l’intérét et la faisabilité d’une approche
consistant a intégrer une ontologie du Droit susceptible de faciliter l’acces au site par les
utilisateurs. Le résultat est une ressource ontologique de tres large couverture, couvrant tous
les domaines du Droit, constituée d’environ 130 000 terrnes et 200 000 liens. Cette ressource
est utilisée comme support pour un systeme d’expansion de requétes : a un mot posé par
l’utilisateur, le systeme propose tous les terrnes reliés a ce mot dans la ressource et laisse
l’utilisateur choisir ceux qu’ils souhaitent retenir pour modiﬁer sa requéte. Cette ressource a
été construite en utilisant les résultats bruts, sans aucun ﬁltrage manuel, de différents outils ou
techniques de Traitement Automatique des Langues (SYNTEX, cooccurrence statistique,
UPERY), obtenus par analyse d’un corpus constitué de l’ensemble des Codes de la législation
francaise.

5.2 Trois outils de TAL pour la construction de RTO it partir de textes

5.2.1 Extraction de termes .' SYNTEX

Dans les trois projets, les résultats de l’outil SYNTEX ont été utilisés. SYNTEX (Bourigault et
Fabre, 2000) est un analyseur syntaxique de corpus. Il existe actuellement une version pour le
francais, qui a été utilisée dans les projets REA et DROIT, et une version pour l’anglais, qui a
été utilisée dans le projet VERRE. Apres l’analyse syntaxique en dépendance de chacune des
phrases du corpus, SYNTEX construite un réseau de mots et de syntagmes (verbaux, nominaux,
adj ectivaux), dit << réseau terminologique >>, dans lequel chaque syntagme est relié d’une part
a sa téte et d’autre part a ses expansions. Les éléments du réseau (mots et syntagmes) sont
appelés << candidats terrnes >>.

A chaque candidat terme sont associées un certain nombre d’informations numériques, sur
lesquelles l’utilisateur peut se baser pour organiser son dépouillement :

0 fréquence : c’est le nombre d’occurrences du candidat terme détectées par le logiciel
dans le corpus. L’interface d’analyse des résultats perrnet a l’analyste d’accéder a
l’ensemble des contextes d’apparition du candidat terrne dans le corpus. Cet acces au
texte est d’autant plus crucial que l’utilisateur n’est pas un spécialiste du domaine.

0 productivité en T éte (resp. Expansion) : c’est le nombre de << descendants en Téte >>
(resp. << descendants en Expansion >>) du candidat terme, c’est-a-dire le nombre de
candidats termes plus complexes qui ont le candidat terme en position téte (resp.
expansion). A partir de ces informations, l’analyste peut visualiser des listes
paradigmatiques de candidats termes partageant la méme téte ou la méme expansion
(cf. figure 1), ce qui le guide vers la constitution de taxinomies locales.

D. Bourigault et N. Aussenac—Gilles

La difficulté essentielle pour l’utilisateur Vient de la masse des résultats fournis par
l’extraction. Méme s’il existe de nombreux travaux fort intéressants sur le filtrage statistique
de candidats termes extraits automatiquement de corpus, l’expérience montre qu’aucune
mesure statistique ne peut suppléer l’expertise de l’analyste, en particulier parce qu’il y a
toujours des candidats termes de fréquence 1 dont l’analyse est intéressante. De facon
générale, sachant qu’il ne pourra analyser tous les candidats termes extraits du corpus,
l’analyste doit adopter une stratégie optimale qui, étant donné le temps qu’il a choisi de
consacrer a la tache d’analyse textuelle et en fonction du type de la ressource a construire, lui
garantit que, parmi les candidats qui auront échappé a son analyse, la proportion de ceux qui
auraient pu étre pertinents est faible.

5.2.2 Analyse distributionnelle .' UPERY

Dans les trois projets, les résultats de l’outil UPERY ont été utilisés. UPERY (Bourigault, 2002)
est outil d’analyse distributionnelle. Il exploite l’ensemble des données présentes dans le
réseau de mots et syntagmes construits par SYNTEX pour effectuer un calcul des proximités
distributionnelles entre ces unités. Ce calcul s’effectue sur la base des contextes syntaxiques
partagés. Il s’agit d’une mise en oeuvre du principe de l’analyse distributionnelle du linguiste
américain Z. S. Harris, réalisée dans la lignée des travaux de H. Assadi (Assadi & Bourigault,
1996). L’analyse distributionnelle rapproche d’abord deux a deux des candidats termes qui
partagent un grand nombre de contextes syntaxiques. Par exemple, dans le corpus REA, les
candidats termes insujﬁsance rénale et détresse respiratoire sont rapprochés car on les trouve
dans les contextes syntaxiques suivants : complément de prise en charge, de apparition, de
installation, de admettre en réanimation chirurgicale pour.

Trois mesures permettent d’appréhender la proximité entre deux candidats termes. Le
coefficient a est égal au nombre de contextes syntaxiques partagés par les deux termes. Cette
mesure donne une premiere indication de la proximité entre deux termes. Mais cette mesure
reﬂete de facon insatisfaisante la proximité. Il faut tenir compte de la productivité en Téte des
contextes partagés : plus un contexte partagé par deux termes est productif, moins sa
contribution au rapprochement des deux candidats termes doit étre importante. Cette intuition
est prise en compte par le coefficient prox qui pondere chaque contexte partagé par l’inVerse
de sa productivité. Enfin, pour évaluer la proximité entre deux unités, il est important de tenir
compte non seulement de ce qu’elles partagent, mais aussi de ce qu’elles ont en propre. On
caractérise la proximité entre deux candidats termes a l’aide de deux indices
supplémentaires: pour chacun des deux candidats termes, le rapport entre le nombre de
contextes partagés et le nombre total de contextes dans lesquels apparait le candidat terme.

Le module d’analyse distributionnelle UPERY calcule chacun de ces coefficients pour chaque
couple de candidats termes, et ne sont présentés a l’utilisateur que les couples dont les
coefficients dépassent certains seuils. Ceux-ci sont définis de facon empirique et Varient en
fonction d’une part de l’homogénéité et de la redondance du corpus et d’autre part du
contexte dans lequel doivent étre exploités les résultats de l’analyse distributionnelle.
L’analyse distributionnelle implémentée dans UPERY est symétrique: on calcule aussi la
proximité entre contextes syntaxiques. Deux contextes syntaxiques sont proches si on y
trouve les mémes termes. Par exemple, dans le corpus REA, les verbes montrer et mettre en
évidence sont proches car ils partagent en position sujet les termes échographie, bilan
infectieux, tomodensitométrie, artériographie, auscultation pulmonaire, etc.

Construction d ’0nt0l0gies dpartir de textes

Il s’avere que les rapprochements effectues par UPERY sont extrémement utiles et pertinents
pour la construction de classes conceptuelles. Le nombre de rapprochements effectues depend
de la redondance du corpus. Par exemple, les corpus REA et le corpus du Code civil, l’un des
corpus exploite dans le projet DROIT, sont deux corpus differents quant a ce parametre de la
redondance. Le corpus REA est constitue dans un ensemble de comptes rendus medicaux qui
decrivent tous les memes types d’evenement et donc dans lesquels les memes structures
syntaxiques reviennent regulierement. A l’oppose, dans le Code civil, les redondances,
repetitions, reforrnulations sont evitees. Cela se repercute de facon assez sensible sur la
richesse des resultats foumis par UPERY sur chacun des 2 corpus, puisque dans le corpus REA
30% des syntagmes nominaux et 47% des noms, de frequence superieure ou egale a 5, sont
rapproches d’au moins un autre mot, alors qu’ils ne sont que respectivement 20% et 43% pour
le corpus du Code civil. Le phenomene est encore plus accentue dans le corpus LIVRE du
projet VERRE. La taille du corpus est relativement reduite (100 000 mots, contre 400 000
pour le corpus REA et 150 000 pour le Code civil) et les redondances sont tres faibles
(chaque chapitre traite d’un sujet specifique, et l’auteur s’efforce de varier son style). De ce
fait, seuls 3% des SN et 18% des noms, de frequence superieure ou egale a 5, ont des voisins,.

5.2.3 Extraction des relations .' YAKWA et CAMELEON

Developpe a l’ERSS par L. Tanguy, YAKWA est un concordancier pour corpus etiquetes
(Rebeyrolle et Tanguy, 2000). Il permet de rechercher des phrases et/ou des paragraphes
contenant une sequence deﬁnie par des marqueurs. Ce marqueurs s’appuient sur les
informations notees par l’etiqueteur dans les corpus, comme les categories grammaticales des
mots. Leur contenu peut étre forme de forrnes lexicales (tronquees, exactes, etc.), de formes
canoniques des unites lexicales du texte, de categories morpho-syntaxiques et de leur
combinaisons (disjonctions, conjonction de marqueur lexical et de marqueur morpho-
syntaxique), de la negation d'un des types de marqueurs precedents ou de jokers (mots non
comptabilises). YAKWA peut s’adapter a tout type d’etiqueteur, par exemple CORDLAL
universite pour le francais ou TREETAGGER pour l’anglais. Son interface guide la construction
de marqueurs et perrnet d’en visualiser la projection sur un corpus.

CAMELEON est un logiciel de recherche de relations lexicales a partir de marqueurs
linguistiques (Seguela, 1999). Il est associe a un module de modelisation qui permet de
valider (ou de rejeter) ces relations lexicales pour les integrer sous forme de relations
semantiques dans un modele conceptuel. Les marqueurs utilises dans CAMELEON peuvent étre
des marqueurs generiques predefinis ou leur adaptation ou encore des marqueurs speciﬁques
deﬁnis par l’utilisateur. L’idee est de rechercher des relations avec des moyens adaptes au
corpus etudie. Les relations sont donc generiques (comme EST-UN) ou specifiques au corpus
(comme << used-in >> dans le projet VERRE), et les marqueurs associes a toutes les relations
sont revus et adaptes a chaque corpus. Le langage d’expression des marqueurs est moins riche
que celui de YAKWA car CAMELEON fonctionne sur un corpus brut non etiquete. En revanche,
Cameleon presente deux points forts pour la construction de RTO: il propose une base
generique de relations et de marqueurs associes ; il s’appuie sur les resultats de SYNTEX pour
suggerer les concepts qui pourraient étre en relation a partir de la forme lexicale trouvee.

D. Bourigault et N. Aussenac—Gilles

6 Le probléme de l’évaluation

Nous terminerons par quelques réﬂexions sur le probleme de l’éValuation. Il faut distinguer
l’évaluation d’une RTO particuliere construite dans un contexte particulier, de l’éValuation de
tel outil ou tel outil de TAL d’aide a la construction de RTO. Dans les deux cas, il faut
adopter une approche ingénierique, en adoptant les principes de base du génie logiciel, ce qui
exige, a minima, de prendre en compte autant que possible le contexte global d’utilisation de
la RTO ou de l’outil.

En ce qui conceme les RTO, il faut distinguer validation et évaluation. Dans le processus de
construction d’une RTO, il y a plusieurs moment de validation de la RTO, c’est-a-dire de
moment ou l’analyste présente la ressource a l’experts (ou a des experts), et lui (leur)
demande de Valider ou d’inValider certains choix de modélisation effectués. Ces moments de
validation sont d’autant moins nombreux que les experts sont peu disponibles. Ce sont donc
des étapes tres importantes dans le processus. L’enjeu est de s’assurer avec les experts que la
conceptualisation représentée dans la RTO n’est pas en contradiction sur tel ou tel point avec
les connaissances expertes. Le probleme ne se pose pas tant en terme de Vérité, qu’en terme
de non Violation des connaissances de l’expert. En effet, pour construire la modélisation,
l’analyste a adopté un point de vue, celui de l’application cible dans laquelle sera intégrée la
ressource, qui n’est pas nécessairement exactement celui de l’expert dans son activité. La
tache n’est pas simple. L’analyste doit aider l’expert, qui ne reconnait pas nécessairement a
premiere vue ses petits, a prendre le recul nécessaire pour déceler la présence d’erreurs, Voire
d’absences, ﬂagrantes. Une fois la RTO construite, s’engage un processus d’évaluati0n.
Comme nous l’aVons déja évoqué, l’éValuation doit étre réalisée selon les procédures de base
du génie logiciel. Il s’agit de Vériﬁer si la RTO satisfait bien le cahier des charges et répond
aux attentes spécifiées au début du projet. La difficulté, habituelle, est que l’ontologie n’est
qu’un élément de l’application cible, qui est le dispositif a Valider. Il faut donc concevoir des
expériences et des bancs d’essais qui permettent de cibler l’éValuation sur la seule ressource.
Une fois ces généralités affirmées, nous pouvons difficilement aller au-dela, parce que nous
manquons encore de retour d’expérience, et parce que chaque cas étant particulier il sera de
toutes facons difficile de définir des procedures a la fois précises et relativement génériques,
et que cela dépasse quelque peu le cadre de la recherche.

L’éValuation des outils de construction de RTO est le probleme qui nous conceme ici. C’est
un probleme lui aussi difficile. La source des difficulté est double : d’abord il s’agit d’outils
d’aide, ensuite chaque outil est rarement utilisé seul. Quand il s’agit d’évaluer d’un outil
automatique, du type << boite noire », il est possible d’évaluer les performances de l’outil en
comparant les résultats qu’il fournit a des résultats attendus (<< gold standard >>). En revanche,
la situation est plus complexe dans le cas des outils d’aide qui nous intéressent ici. Les
résultats foumis par les outils sont interprétés par l’analyste, et le résultat de cette
interprétation est Variable: une modiﬁcation, un enrichissement de la ressource a un ou
plusieurs points du réseau, Voire dans certain cas l’absence d’action immédiate, sans que cela
signiﬁe nécessairement que les résultats en question soient faux ni méme on pertinents. De
plus, chaque cette interprétation s’appuie norrnalement sur une conﬁrmation par retour aux
textes. Il n’y a pas systématiquement de trace directe entre un résultats (ou un ensemble de
résultats) de l’outil et telle ou telle portion de la ressource. Si on raj oute a cela, qu’une portion
de RTO n’a de sens que dans la globalité de la ressource, et la ressource elle-méme ne peut
étre évaluée qu’en contexte, on saisit l’ampleur de la tache.. 11 y a un tel parcours interprétatif
entre les résultats de l’outil et la ressource construite que le mode d’éValuation par

Construction d ’ontologies a partir de textes

comparaison entre les résultats de l’outil et une ressource de référence ne peut apporter
limités, méme si cela peut donner des indications tres intéressantes pour faire évoluer l’outil
(Nazarenko et al., 2001). La encore, nous n’avons de solution miracle a proposer. L’idéal
serait par exemple de comparer entre terrnes de temps de réalisation et de qualité deux
ressources ontologiques, l’une construite avec tel outil, et l’autre sans. Quand on connait le
temps de développement d’une ontologie, on imagine la lourdeur, et la difﬁculté de mise en
oeuvre d’une telle méthodologie. Le probleme reste ouvert. Pour mesurer, ne serait-ce que
d’un point de Vue qualitatif, l’intérét des outils, considérons pour le moment qu’il est
primordial de les tester dans des contextes nombreux et Variés et aussi réels que possible pour
faire avancer la recherche.

Référence

Ait El Mekki T., Nazarenko A (2002), Comment aider un auteur a construire l'index d'un
ouvrage ?, Actes du Colloque International sur la Fouille de T exte CIF T '2002, Y. Toussaint
et C. Nedellec Eds., oct. 2002, pp. 141-158

Assadi H. (1998), Construction d ’ontologies a partir de textes techniques. Application aux
syste‘mes documentaires, these de l’UniVersité Paris 6

Aussenac-Gilles N. (1999), GEDITERM, un logiciel de gestion de bases de connaissances
terminologiques, in Actes des Joumées Terminologie et Intelligence Artiﬁcielle (TIA’99),
Nantes, Terminologies Nouvelles n°19, 111-123.

Aussenac N., Séguéla P. (2000), Les relations sémantiques : du linguistique au formel.
Cahiers de grammaire, N° spécial sur la linguistique de corpus. A. Condamines (Ed.) Vol 25.
Dec. 2000. Toulouse : Presse de l’UTM. Pp 175-198.

Aussenac-Gilles N., Biébow B., Szulman N. (2000), Revisiting Ontology Design: a method
based on corpus analysis. Knowledge engineering and knowledge management: methods,
models and tools, Proc. of the 12th International Conference on Knowledge Engineering and
Knowledge Management. Juan-Les-Pins (F). Oct 2000. R Dieng and O. Corby (Eds). Lecture
Notes in Artificial Intelligence Vol 1937. Berlin: Springer Verlag. pp. 172-188.

Bachimont, B. (2000), Engagement sémantique et engagement ontologique : conception et
réalisation d’ontologies en ingénierie des connaissances >>. In J. Charlet et al. (eds), Ingénierie
des Connaissances ,' Evolutions récentes et nouveaux déﬁs, Eyrolles, pp. 305-323

Bourigault D. (2002), Upery : un outil d'analyse distributionnelle étendue pour la construction
d’ontologies a partir de corpus, Actes de la 9é’"" conférence annuelle sur le Traitement
Automatique des Langues (TALN 2002), Nancy, 2002, pp. 75-84

Bourigault D., Fabre C. (2000), Approche linguistique pour l'analyse syntaxique de corpus,
Cahiers de Grammaires, n° 25, 2000, Université Toulouse - Le Mirail, pp. 131-151.

Bourigault D. & Jacquemin C. (2000), Construction de ressources terminologiques, in J.-M.
Pierrel (éd.), Industrie des langues, Hermes, Paris, pp. 215-233

D. Bourigault et N. Aussenac—Gilles

Charlet J., Zacklad M., Kassel G. & Bourigault D. (eds) (2000), Ingénierie des
connaissances .' évolutions récentes et nouveaux déﬁs, Eyrolles : Paris - Collection technique
et scientiﬁque des telecommunications

Charlet J. (2002), L ’ingénierie des connaissances .' résultats, développements et perspectives
pour la gestion des connaissances médicales. Memoire d’habilitation a diriger des recherches,
universite Pierre et Marie Curie

Chaumier J. (1988), Travail et méthodes du/de la documentaliste .' Connaissance du
probleme, Applications pratiques. 3° ed. mise a jour et completee. Paris : ESF, 1988

Condamines A. et Rebeyrolles J (2000), Construction d'une base de connaissances
terrninologiques a partir de textes : experimentation et deﬁnition d'une methode. In Charlet J,
Zacklad M., Kassel G. & Bourigault D. eds. Ingénierie des connaissances. T endances
actuelles et nouveaux déﬁs. Editions Eyrolles/France Telecom, Paris

Daille B. (1994), Approche mixte pour l'extraction de terminologie .' statistique lexicale et
ﬁltres linguistiques. These en Inforrnatique Fondamentale, Universite de Paris 7, Paris

David S. et Plante P. (1990), De la necessite d'une approche morpho-syntaxique dans l'analyse
de textes. Intelligence Artiﬁcielle et Sciences Cognitives au Quebec, 3(3): 140-154

Enguehard C. et Pantera L. (1995), Automatic natural acquisition of a terminology. Journal of
Quantitative Linguistics, 2(1):27-32

Faure D. (2000), Conception de méthode d'apprentissage symbolique et automatique pour
l'acquisition de cadres de sous-categorisation de verbes et de connaissances sémantiques a
partir de textes .' le systeme ASIMW, these de Doctorat Universite de Paris Sud

Garcia D. (1998), Analyse automatique de textes pour l’organisation causale des actions.
Réalisation du systeme informatique COATIS. These en informatique. Universite Paris IV

Grefenstette G. (1994), Explorations in Automatic Thesaurus Discovery. Kluwer Academic
Publisher, Boston, MA

Habert B., Naulleau E. et Nazarenko A . (1996), Symbolic word clustering for medium-size
corpora. Proceedings of the 16th International Conference on Computational Linguistics
(COLING’96), Copenhagen, pp 490-495

Jacquemin C. (1997), Variation terminologique .' Reconnaissance et acquisition automatiques
de termes et de leurs variantes en corpus. Memoire d'habilitation a diriger des recherches en
informatique fondamentale, Universite de Nantes

Lame G. (2002), Construction d ’ontologie a partir de textes. Une ontologie du Droitfrancais
dédiée a la recherche d ’information sur le Web, these de l’Ecole des Mines de Paris

Maedche A. & Staab S. (2000), Mining Ontologies from Text. In Knowledge Engineering and
Knowledge management: methods, models and tools, proceedings of EKA W2000. R. Dieng
and O. Corby (Eds). Bonn : Springer Verlag. LNAI 1937.

Construction d ’ontologies a partir de textes

Maynard D. et Ananiadou S. (2001), Term extraction using a similarity-based approach, in
Bourigault D., Jacquemin C. & L’Homme M.-C., Recent advances in computational
terminology, John Benj amins Publishing, Amsterdam, pp 261-278

Morin E. (1999), Des patrons lexico-syntaxiques pour aider au dépouillement
terrninologiques, T raitement Automatique des Langues, Volume 40, Numéro 1, pp. 143-166

Nazarenko A., Zweigenbaum P., Habert B. & Bouaud J. (2001), Corpus-based extension of a
terminological semantic lexicon, in Bourigault D., J acquemin C. & L’Homme M.-C., Recent
advances in computational terminology, John Benj amins Publishing, Amsterdam, pp 327-352

Rastier F. (1991), Sémantique et recherches cognitives, Presses Universitaires de France,
Paris, 1991

Rastier F. (1995), Le terme: entre ontologie et linguistique, Actes des Ie‘res Journées
“ T erminologie et Intelligence Artiﬁcielle”, Villetaneuse, avril 1995, La banque des mots,
Numéro spécial 7-1995, pp. 35-65

Rousselot F., Frath P. et Oueslati R. (1996), Extracting concepts and relations from corpora,
Procedings of the 12”’ European Conference on Artificial Intelligence (ECAI’96), workshop
on Corpus-Oriented Semantic Analysis, Budapest

Séguéla P. et Aussenac-Gilles N. (1999), Extraction de relations sémantiques entre termes et
enrichissement de modeles du domaine, Actes de la conférence Ingénierie des Connaissances
(IC'99), Paris, pp 79-88

Slodzian M. (2000), L'émergence d'une terminologie textuelle et le retour du sens, in Le sens
en terminologie, publication du Centre de Recherche en Terminologie et Traduction de
l’UniVersité Lyon 2

Sparck Jones K. (1971), Automatic Keyword Classiﬁcation for Information Retrieval.
Butterworth, London

Szulman S., Biébow B. & Aussenac-Gilles N. (2002), Structuration de Terminologies a l’aide
d’outils d’analyse de textes avec TERMINAE, Traitement Automatique de la Langue (TAL).
Numéro spécial sur le Structuration de Terminologie. Eds A. Nazarenko, T. Hammon. Vol43,
N°1; pp 103-128. 2002.

Toussaint Y., Namer F., Daille B., Jacquemin C., Royauté J. et Hathout N. (1998), Une
approche linguistique et statistique pour l’analyse de l’information en corpus. Actes de la 57""
conference annuelle sur le T raitement Automatiques des Langues Naturelles (TALN’98),
Paris, pp. 182-191

Velardi P., Missikoff M. & Basili R. (2001) Identification of relevant terms to support the
construction of domain ontologies. In ACL WS on Human Language Technologies and
Knowledge Management. Toulouse (F), July 6-7, 2001. 18-28.

Zweigenbaum P. (1999) Encoder l'inforrnation médicale : des terminologies aux systemes de
représentation des connaissances. Innovation Stratégique en Information de Santé 1999(23)

