@proceedings{TALN:2004,
  editor    = {Blache, Philippe and Nguyen, Noël and Chenfour, Nouredine and Rajouani, Abdenbi},
  title     = {Actes de la 11ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {April},
  year      = {2004},
  address   = {Fès, Maroc},
  publisher = {Association pour le Traitement Automatique des Langues},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2004}
}

@inproceedings{sitbon-bellot:2004:TALN,
  author    = {Sitbon, Laurianne and Bellot, Patrice},
  title     = {Evaluation de méthodes de segmentation thématique linéaire non supervisées après adaptation au français},
  booktitle = {Actes de la 11ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {April},
  year      = {2004},
  address   = {Fès, Maroc},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2004/taln-2004-long-001},
  language  = {french},
  resume    = {Nous proposons une évaluation de différentes méthodes et outils de segmentation thématique de textes. Nous présentons les outils de segmentation linéaire et non supervisée DotPlotting, Segmenter, C99, TextTiling, ainsi qu’une manière de les adapter et de les tester sur des documents français. Les résultats des tests montrent des différences en performance notables selon les sujets abordés dans les documents, et selon que le nombre de segments à trouver est fixé au préalable par l’utilisateur. Ces travaux font partie du projet Technolangue AGILE-OURAL.},
  abstract  = {This paper presents an empirical comparison between different methods for segmenting texts. After presenting segmentation tools and more specifically linear segmentation algorithms, we present a comparison of these methods on both French and English text corpora. This evalutation points out that the performance of each method heavilly relies on the topic of the documents, and the number of boundaries to be found. This work is part of the project Technolangue AGILE-OURAL.},
  motscles  = {Segmentation thématique, métriques de Beeferman et WindowDiff, cohésion lexicale, chaînes lexicales},
  keywords  = {Topic segmentation, WindowDiff and Beeferman measures, lexical cohesion, lexical chains},
}

@inproceedings{bourigault-frerot:2004:TALN,
  author    = {Bourigault, Didier and Frérot, Cécile},
  title     = {Ambiguïté de rattachement prépositionnel : introduction de ressources exogènes de sous-catégorisation dans un analyseur syntaxique de corpus endogène},
  booktitle = {Actes de la 11ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {April},
  year      = {2004},
  address   = {Fès, Maroc},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2004/taln-2004-long-002},
  language  = {french},
  resume    = {Nous présentons les résultats d’expérimentations visant à introduire des ressources lexicosyntaxiques génériques dans un analyseur syntaxique de corpus à base endogène (SYNTEX) pour la résolution d’ambiguïtés de rattachement prépositionnel. Les données de souscatégorisation verbale sont élaborées à partir du lexique-grammaire et d’une acquisition en corpus (journal Le Monde). Nous présentons la stratégie endogène de désambiguïsation, avant d’y intégrer les ressources construites. Ces stratégies sont évaluées sur trois corpus (scientifique, juridique et journalistique). La stratégie mixte augmente le taux de rappel (+15% sur les trois corpus cumulés) sans toutefois modifier le taux de précision (~ 85%). Nous discutons ces performances, notamment à la lumière des résultats obtenus par ailleurs sur la préposition de.},
  abstract  = {We report the results of experiments aimed at integrating general lexico-syntactic resources into a corpus syntactic parser (SYNTEX) based on endogenous learning. We tackle the issue of prepositional phrase attachment. We make use of both French lexico-syntactic resources and automatic acquisition to extract verb subcategorisation data. We describe both the endogenous and hybrid approaches and show how the latter improves the recall rate - +15% in average - but has no impact on the precision rate (~ 85%).},
  motscles  = {analyse syntaxique automatique, ambiguïté de rattachement prépositionnel, procédures endogènes, ressources exogènes, approche mixte},
  keywords  = {automatic parsing, prepositional phrase attachement disambiguation, endogenous learning, exogenous resources, hybrid approach},
}

@inproceedings{pogodalla:2004:TALN,
  author    = {Pogodalla, Sylvain},
  title     = {Vers un statut de l’arbre de dérivation : exemples de construction de representations sémantiques pour les Grammaires d’Arbres Adjoints},
  booktitle = {Actes de la 11ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {April},
  year      = {2004},
  address   = {Fès, Maroc},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2004/taln-2004-long-003},
  language  = {french},
  resume    = {Cet article propose une définition des arbres de dérivation pour les Grammaires d’Arbres Adjoints, étendant la notion habituelle. Elle est construite sur l’utilisation des Grammaires Catégorielles Abstraites et permet de manière symétrique le calcul de la représentation syntaxique (arbre dérivé) et le calcul de la représentation sémantique.},
  abstract  = {This paper suggests a definition for Tree Adjoining Grammar derivation trees, extending the usual notion. It results from using Abstract Categorial Grammars and enables, in a symmetric way, the computation of both the syntactic representation (derived tree) and the semantic representation.},
  motscles  = {Sémantique, grammaires d’arbre adjoints, grammaires catégorielles, λ-calcul},
  keywords  = {Semantics, tree adjoining grammars, categorial grammars, λ-calculus},
}

@inproceedings{claveau-sebillot:2004:TALN,
  author    = {Claveau, Vincent and Sébillot, Pascale},
  title     = {Extension de requêtes par lien sémantique nom-verbe acquis sur corpus},
  booktitle = {Actes de la 11ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {April},
  year      = {2004},
  address   = {Fès, Maroc},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2004/taln-2004-long-004},
  language  = {french},
  resume    = {En recherche d’information, savoir reformuler une idée par des termes différents est une des clefs pour l’amélioration des performances des systèmes de recherche d’information (SRI) existants. L’un des moyens pour résoudre ce problème est d’utiliser des ressources sémantiques spécialisées et adaptées à la base documentaire sur laquelle les recherches sont faites. Nous proposons dans cet article de montrer que les liens sémantiques entre noms et verbes appelés liens qualia, définis dans le modèle du Lexique génératif (Pustejovsky, 1995), peuvent effectivement améliorer les résultats des SRI. Pour cela, nous extrayons automatiquement des couples nom-verbe en relation qualia de la base documentaire à l’aide du système d’acquisition ASARES (Claveau, 2003a). Ces couples sont ensuite utilisés pour étendre les requêtes d’un système de recherche. Nous montrons, à l’aide des données de la campagne d’évaluation Amaryllis, que cette extension permet effectivement d’obtenir des réponses plus pertinentes, et plus particulièrement pour les premiers documents retournés à l’utilisateur.},
  abstract  = {In the information retrieval field, managing the equivalent reformulations of a same idea is a key point to improve the performances of existing retrieval systems. One way to reach this goal is to use specialised semantic resources that are suited to the document database on which the queries are processed. In this paper, we show that the semantic links between nouns and verbs called qualia links, defined in the Generative lexicon framework (Pustejovsky, 1995), enable us to improve the results of retrieval systems. To achieve this goal, we automatically extract from the document database noun-verb pairs that are in qualia relation with the acquisition system ASARES (Claveau, 2003a). These pairs are then used to expand the queries of a retrieval system. With the help of the Amaryllis evaluation campaign data, we show that these expansions actually lead to better results, especially for the first documents proposed to the user.},
  motscles  = {Lexique sémantique, acquisition sur corpus, recherche d’information, Lexique génératif, extension de requête},
  keywords  = {Semantic lexicon, corpus-based acquisition, information retrieval, Generative lexicon, query expansion},
}

@inproceedings{ferret:2004:TALN,
  author    = {Ferret, Olivier},
  title     = {Découvrir des sens de mots à partir d’un réseau de cooccurrences lexicales},
  booktitle = {Actes de la 11ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {April},
  year      = {2004},
  address   = {Fès, Maroc},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2004/taln-2004-long-005},
  language  = {french},
  resume    = {Les réseaux lexico-sémantiques de type WordNet ont fait l’objet de nombreuses critiques concernant la nature des sens qu’ils distinguent ainsi que la façon dont ils caractérisent ces distinctions de sens. Cet article présente une solution possible à ces limites, solution consistant à définir les sens des mots à partir de leur usage. Plus précisément, il propose de différencier les sens d’un mot à partir d’un réseau de cooccurrences lexicales construit sur la base d’un large corpus. Cette méthode a été testée à la fois pour le français et pour l’anglais et a fait l’objet dans ce dernier cas d’une première évaluation par comparaison avec WordNet.},
  abstract  = {Lexico-semantic networks such as WordNet have been criticized a lot on the nature of the senses they distinguish as well as on the way they define these senses. In this article, we present a possible solution to overcome these limits by defining the sense of words from the way they are used. More precisely, we propose to differentiate the senses of a word from a network of lexical cooccurrences built from a large corpus. This method was tested both for French and English and for English, was evaluated through a comparison with WordNet.},
  motscles  = {Sémantique lexicale, découverte du sens des mots, réseaux lexico-sémantiques},
  keywords  = {Lexical semantics, word sense discovery, lexico-semantic networks},
}

@inproceedings{gaume-hathout-muller:2004:TALN,
  author    = {Gaume, Bruno and Hathout, Nabil and Muller, Philippe},
  title     = {Désambiguïsation par proximité structurelle},
  booktitle = {Actes de la 11ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {April},
  year      = {2004},
  address   = {Fès, Maroc},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2004/taln-2004-long-006},
  language  = {french},
  resume    = {L’article présente une méthode de désambiguïsation dans laquelle le sens est déterminé en utilisant un dictionnaire. La méthode est basée sur un algorithme qui calcule une distance « sémantique » entre les mots du dictionnaire en prenant en compte la topologie complète du dictionnaire, vu comme un graphe sur ses entrées. Nous l’avons testée sur la désambiguïsation des définitions du dictionnaire elles-mêmes. L’article présente des résultats préliminaires, qui sont très encourageants pour une méthode ne nécessitant pas de corpus annoté.},
  abstract  = {This paper presents a disambiguation method in which word senses are determined using a dictionary. We use a semantic proximity measure between words in the dictionary, taking into account the whole topology of the dictionary, seen as a graph on its entries. We have tested the method on the problem of disambiguation of the dictionary entries themselves, with promising results considering we do not use any prior annotated data.},
  motscles  = {Désambiguïsation sémantique, réseaux petits mondes hiérarchiques, dictionnaires},
  keywords  = {Word sense desambiguation, hierarchical small words, dictionaries},
}

@inproceedings{brunetmanquat:2004:TALN,
  author    = {Brunet-Manquat, Francis},
  title     = {Fusionner pour mieux analyser : Conception et évaluation de la plate-forme de combinaison},
  booktitle = {Actes de la 11ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {April},
  year      = {2004},
  address   = {Fès, Maroc},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2004/taln-2004-long-007},
  language  = {french},
  resume    = {L’objectif de cet article est de présenter nos travaux concernant la combinaison d’analyseurs syntaxiques pour produire un analyseur plus robuste. Nous avons créé une plate-forme nous permettant de comparer des analyseurs syntaxiques pour une langue donnée en découpant leurs résultats en informations élémentaires, en les normalisant, et en les comparant aux résultats de référence. Cette même plate-forme est utilisée pour combiner plusieurs analyseurs pour produire un analyseur de dépendance plus couvrant et plus robuste. À long terme, il sera possible de “compiler” les connaissances extraites de plusieurs analyseurs dans un analyseur de dépendance autonome.},
  abstract  = {The goal of this article is to present our works about the combination of syntactic parsers to produce a more robust parser. We have built a platform which allows us to compare syntactic parsers for a given language by splitting their results in elementary pieces, normalizing them, and comparing them with reference results. The same platform is used to combine several parsers to produce a dependency parser, which is big construction broader and more robust than its component parsers. In the future, it should by possible to “compile” the knowledge extracted from several analyzers into an autonomous dependency parser.},
  motscles  = {Analyse de dépendance, analyse syntaxique, combinaisons d’informations},
  keywords  = {Dependency parsing, syntactic parsing, Information combination},
}

@inproceedings{dudausofronie-tellier:2004:TALN,
  author    = {Dudau Sofronie, Daniela and Tellier, Isabelle},
  title     = {Un modèle d’acquisition de la syntaxe à l’aide d’informations sémantiques},
  booktitle = {Actes de la 11ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {April},
  year      = {2004},
  address   = {Fès, Maroc},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2004/taln-2004-long-008},
  language  = {french},
  resume    = {Nous présentons dans cet article un algorithme d’apprentissage syntaxico-sémantique du langage naturel. Les données de départ sont des phrases correctes d’une langue donnée, enrichies d’informations sémantiques. Le résultat est l’ensemble des grammaires formelles satisfaisant certaines conditions et compatibles avec ces données. La stratégie employée, validée d’un point de vue théorique, est testée sur un corpus de textes français constitué pour l’occasion.},
  abstract  = {This paper presents a syntactico-semantic learning algorithm for natural languages. Input data are syntactically correct sentences of a given natural language, enriched with semantic information. The output is the set of compatible formal grammars satisfying certain conditions. The strategy used, which has been proved theoretically valid, is tested on a corpus of French texts built for this purpose.},
  motscles  = {Grammaires catégorielles, types sémantiques, apprentissage syntaxico-sémantique},
  keywords  = {Categorial grammars, semantic types, syntactico-semantic learning},
}

@inproceedings{mariage-bernard:2004:TALN,
  author    = {Mariage, Jean-Jacques and Bernard, Gilles},
  title     = {Catégorisation de patrons syntaxiques par Self Organizing Maps},
  booktitle = {Actes de la 11ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {April},
  year      = {2004},
  address   = {Fès, Maroc},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2004/taln-2004-long-009},
  language  = {french},
  resume    = {Dans cet article, nous présentons quelques résultats en catégorisation automatique de données du langage naturel sans recours à des connaissances préalables. Le système part d’une liste de formes grammaticales françaises et en construit un graphe qui représente les chaînes rencontrées dans un corpus de textes de taille raisonnable ; les liens sont pondérés à partir de données statistiques extraites du corpus. Pour chaque chaîne de formes grammaticales significative, un vecteur reflétant sa distribution est extrait et passé à un réseau de neurones de type carte topologique auto-organisatrice. Une fois le processus d’apprentissage terminé, la carte résultante est convertie en un graphe d’étiquettes générées automatiquement, utilisé dans un tagger ou un analyseur de bas niveau. L’algorithme est aisément adaptable à toute langue dans la mesure où il ne nécessite qu’une liste de marques grammaticales et un corpus important (plus il est gros, mieux c’est). Il présente en outre un intérêt supplémentaire qui est son caractère dynamique : il est extrêmement aisé de recalculer les données à mesure que le corpus augmente.},
  abstract  = {The present paper presents some results in automatic categorization of natural language data without previous knowledge. The system starts with a list of French grammatical items, builds them into a graph that represents the strings encountered in a reasonable corpus of texts; the links are weighted based upon statistical data extracted from the corpus. For each significant string of grammatical items a vector reflecting its distribution is extracted, and fed into a Self- Organizing Map neural network. Once the learning process is achieved, the resulting map will be converted into a graph of automatically generated tags, used in a tagger or a shallow parser. The algorithm may easily be adapted to any language, as it needs only the list of grammatical markers and a large corpus (the bigger the better). Another point of interest is its dynamic character: it is easy to recompute the data as the corpus grows.},
  motscles  = {Langues naturelles, réseaux neuronaux, extraction de connaissances},
  keywords  = {Natural languages, neural networks, knowledge extraction},
}

@inproceedings{nasr-volanschi:2004:TALN,
  author    = {Nasr, Alexis and Volanschi, Alexandra},
  title     = {Couplage d’un étiqueteur morpho-syntaxique et d’un analyseur partiel représentés sous la forme d’automates finis pondérés},
  booktitle = {Actes de la 11ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {April},
  year      = {2004},
  address   = {Fès, Maroc},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2004/taln-2004-long-010},
  language  = {french},
  resume    = {Cet article présente une manière d’intégrer un étiqueteur morpho-syntaxique et un analyseur partiel. Cette integration permet de corriger des erreurs effectuées par l’étiqueteur seul. L’étiqueteur et l’analyseur ont été réalisés sous la forme d’automates pondérés. Des résultats sur un corpus du français ont montré une dimintion du taux d’erreur de l’ordre de 12%.},
  abstract  = {This paper presents a method of integrating a part-of-speech tagger and a chunker. This integration lead to the correction of a number of errors made by the tagger when used alone. Both tagger and chunker are implemented as weighted finite state machines. Experiments on a French corpus showed a decrease of the word error rate of about 12%.},
  motscles  = {Analyse morpho-syntaxique, analyse syntaxique partielle, automates finis pondérés},
  keywords  = {Part-of-speech tagging, chunking, weighted finite state machines},
}

@inproceedings{blanchon-boitet:2004:TALN,
  author    = {Blanchon, Hervé and Boitet, Christian},
  title     = {Deux premières étapes vers les documents auto-explicatifs},
  booktitle = {Actes de la 11ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {April},
  year      = {2004},
  address   = {Fès, Maroc},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2004/taln-2004-long-011},
  language  = {french},
  resume    = {Dans le cadre du projet LIDIA, nous avons montré que dans de nombreuses situations, la TA Fondée sur le Dialogue (TAFD) pour auteur monolingue peut offrir une meilleure solution en traduction multicible que les aides aux traducteurs, ou la traduction avec révision, même si des langages contrôlés sont utilisés. Nos premières expériences ont mis en évidence le besoin de conserver les « intentions de l’auteur » au moyen « d’annotations de désambiguïsation ». Ces annotations permettent de transformer le document source en un Document Auto-Explicatif (DAE). Nous présentons ici une solution pour intégrer ces annotations dans un document XML et les rendre visibles et utilisables par un lecteur pour une meilleure compréhension du « vrai contenu » du document. Le concept de Document Auto-Explicatif pourrait changer profondément notre façon de comprendre des documents importants ou écrits dans un style complexe. Nous montrerons aussi qu’un DAE, traduit dans une langue cible L, pourrait aussi être transformé, sans interaction humaine, en un DAE en langue L si un analyseur et un désambiguïseur sont disponibles pour cette langue L. Ainsi, un DAE pourrait être utilisé dans un contexte monolingue, mais aussi dans un contexte multilingue sans travail humain additionnel.},
  abstract  = {In the LIDIA project, we have demonstrated that, in many situations, Dialogue-Based MT (DBMT) for monolingual author is likely to offer better solutions to multitarget translation needs than machine aids to translators or batch MT, even if controlled languages are used. First experiments have shown the need to keep a memory of the “author’s intention” by means of “disambiguating annotations” transforming the source document into a “selfexplaining document” (SED). We present ways to integrate these annotations into an XML document (SED-XML), and to make them visible and usable by readers for better understanding of the “true content” of a document. The very concept of SED might deeply change our way of understanding important or difficult written material. We also show that a SED, once translated into a target language L, might be transformed into an SED in L with no human interaction, if an analyzer and a disambiguator are available for L. Hence, the SED structure might be used in multilingual as well as in monolingual contexts, without addition of human work.},
  motscles  = {Document Auto-Explicatifs (DAE), désambiguïsation interactive, documents actifs},
  keywords  = {Self-Explaining Document (SED), interactive disambiguation, active documents},
}

@inproceedings{fafiotte:2004:TALN,
  author    = {Fafiotte, Georges},
  title     = {Interprétariat à distance et collecte de dialogues spontanés bilingues, sur une plate-forme générique multifonctionnelle},
  booktitle = {Actes de la 11ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {April},
  year      = {2004},
  address   = {Fès, Maroc},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2004/taln-2004-long-012},
  language  = {french},
  resume    = {Parallèlement à l’intégration du français en TA de Parole multilingue (projets C-STAR, NESPOLE!), nous avons développé plusieurs plates-formes, dans le cadre des projets ERIM (Environnement Réseau pour l’Interprétariat Multimodal) et ChinFaDial (collecte de dialogues parlés spontanés français-chinois), pour traiter différents aspects de la communication orale spontanée bilingue non finalisée sur le web : interprétariat humain à distance, collecte de données, intégration d’aides automatiques (serveur de TA de Parole utilisant des composants du marché, interaction multimodale entre interlocuteurs, et prochainement aides en ligne aux intervenants, locuteurs ou interprètes). Les corpus collectés devraient être disponibles sur un site DistribDial au printemps 2004. Ces plates-formes sont en cours d’intégration, en un système générique multifonctionnel unique ERIMM d’aide à la communication multilingue multimodale, dont une variante s’étendra également à la formation à distance (e-training) à l’interprétariat.},
  abstract  = {In parallel with integrating the French language into multilingual Speech Machine Translation (within the C-STAR and NESPOLE! projects), we have developed in recent years several platforms, in the framework of projects ERIM (Network-based Environment for Multimodal Interpreting) and ChinFaDial (collecting French-Chinese spontaneously spoken dialogues), allowing to handle various aspects of spontaneous, general-purpose bilingual spoken dialogues on the web: distant human interpreting, data collection, integration of machine aids including server-based speech translation based on commercial products, multimodal user interaction, and next, online aids to speakers and/or interpreters. Collected data should be available on the web (DistribDial) in spring 2004. All platforms are being integrated into one single multifunctional ERIMM generic system, which should then be extended to distant e-training in interpreting.},
  motscles  = {Interprétariat à distance sur réseau, collecte de corpus oraux bilingues, dialogues spontanés, communication multilingue, mutualisation de ressources},
  keywords  = {Web-based interpreting, bilingual spoken corpora collection, spontaneous dialogues, multilingual communication, resource mutualization},
}

@inproceedings{morin-dufourkowalski-daille:2004:TALN,
  author    = {Morin, Emmanuel and Dufour-Kowalski, Samuel and Daille, Béatrice},
  title     = {Extraction de terminologies bilingues à partir de corpus comparables},
  booktitle = {Actes de la 11ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {April},
  year      = {2004},
  address   = {Fès, Maroc},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2004/taln-2004-long-013},
  language  = {french},
  resume    = {Cet article présente une méthode pour extraire, à partir de corpus comparables d’un domaine de spécialité, un lexique bilingue comportant des termes simples et complexes. Cette méthode extrait d’abord les termes complexes dans chaque langue, puis les aligne à l’aide de méthodes statistiques exploitant le contexte des termes. Après avoir rappelé les difficultés que pose l’alignement des termes complexes et précisé notre approche, nous présentons le processus d’extraction de terminologies bilingues adopté et les ressources utilisées pour nos expérimentations. Enfin, nous évaluons notre approche et démontrons son intérêt en particulier pour l’alignement de termes complexes non compositionnels.},
  abstract  = {This article presents a method of extracting bilingual lexica composed of simple and multi-word terms from comparable corpora of a technical domain. First, this method extracts the multiword terms in each language, and then uses statistical methods to align them by exploiting the term contexts. After explaining the difficulties involved in aligning multi-word terms and specifying our approach, we show the adopted process for bilingual terminology extraction and the resources used in our experiments. Finally, we evaluate our approach and demonstrate its significance, particularly in relation to non-compositional multi-word term alignment.},
  motscles  = {Terminologie bilingue, corpus comparable, termes complexes},
  keywords  = {Bilingual terminology, comparable corpora, multi-word terms},
}

@inproceedings{wehrli:2004:TALN,
  author    = {Wehrli, Éric},
  title     = {Traduction, traduction de mots, traduction de phrases},
  booktitle = {Actes de la 11ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {April},
  year      = {2004},
  address   = {Fès, Maroc},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2004/taln-2004-long-014},
  language  = {french},
  resume    = {Une des conséquences du développement d’Internet et de la globalisation des échanges est le nombre considérable d’individus amenés à consulter des documents en ligne dans une langue autre que la leur. Après avoir montré que ni la traduction automatique, ni les aides terminologiques en ligne ne constituent une réponse pleinement adéquate à ce nouveau besoin, cet article présente un système d’aide à la lecture en langue étrangère basé sur un analyseur syntaxique puissant. Pour un mot sélectionné par l’usager, ce système analyse la phrase entière, de manière (i) à choisir la lecture du mot sélectionné la mieux adaptée au contexte morphosyntaxique et (ii) à identifier une éventuelle expression idiomatique ou une collocation dont le mot serait un élément. Une démonstration de ce système, baptisé TWiC (Translation of words in context "Traduction de mots en contexte"), pourra être présentée.},
  abstract  = {As a consequence of globalisation and the development of the Internet, an increasing number of people are struggling with on-line documents in languages other than their own. In this paper, we will first argue that neither machine translation nor existing on-line terminology tools constitute an adequate answer to this problem, and then present a new system conceived as a foreign-language reading assistant, based on a powerful syntactic parser. Given a word selected by the user, this system carries out a syntactic analysis of the whole sentence in order (i) to select the most appropriate reading of the selected word, given the morpho-syntactic context, and (ii) to identify a possible idiom or collocation the selected word might be part of. A demo of this system, dubbed TWiC (Translation of words in context) will be available.},
  motscles  = {Traduction automatique, aide à la traduction, analyse syntaxique, collocations},
  keywords  = {Machine translation, Translation aids, syntactic parsing, collocations},
}

@inproceedings{brun-hagege:2004:TALN,
  author    = {Brun, Caroline and Hagège, Caroline},
  title     = {Extraction d’information en domaine restreint pour la génération multilingue de résumés ciblés},
  booktitle = {Actes de la 11ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {April},
  year      = {2004},
  address   = {Fès, Maroc},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2004/taln-2004-long-015},
  language  = {french},
  resume    = {Dans cet article nous présentons une application de génération de résumés multilingues ciblés à partir de textes d’un domaine restreint. Ces résumés sont dits ciblés car ils sont produits d’après les spécifications d’un utilisateur qui doit décider a priori du type de l’information qu’il souhaite voir apparaître dans le résumé final. Pour mener à bien cette tâche, nous effectuons dans un premier temps l’extraction de l’information spécifiée par l’utilisateur. Cette information constitue l’entrée d’un système de génération multilingue qui produira des résumés normalisés en trois langues (anglais, français et espagnol) à partir d’un texte en anglais.},
  abstract  = {We present an application of oriented multilingual summarization from domain specific texts. These summaries are oriented because the user has to define in a first step the kind of information he/she wants to be present in the final summary. In order to acheive this task, a first step of information extraction is performed. This extracted information which corresponds to the user’s specification is then the input of a multilingual generator that produces the desired summaries in three languages (english, french and spanish) from an english input text.},
  motscles  = {Résumé multilingue ciblé, extraction d’information, génération multilingue},
  keywords  = {Multilingual oriented summaries, information extraction, multilingual generation},
}

@inproceedings{malaise-zweigenbaum-bachimont:2004:TALN,
  author    = {Malaisé, Véronique and Zweigenbaum, Pierre and Bachimont, Bruno},
  title     = {Repérage et exploitation d’énoncés définitoires en corpus pour l’aide à la construction d’ontologie},
  booktitle = {Actes de la 11ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {April},
  year      = {2004},
  address   = {Fès, Maroc},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2004/taln-2004-long-016},
  language  = {french},
  resume    = {Pour construire une ontologie, un modéliseur a besoin d’objecter des informations sémantiques sur les termes principaux de son domaine d’étude. Les outils d’exploration de corpus peuvent aider à repérer ces types d’information, et l’identification de couples d’hyperonymes a fait l’objet de plusieurs travaux. Nous proposons d’exploiter des énoncés définitoires pour extraire d’un corpus des informations concernant les trois axes de l’ossature ontologique : l’axe vertical, lié à l’hyperonymie, l’axe horizontal, lié à la co-hyponymie et l’axe transversal, lié aux relations du domaine. Après un rappel des travaux existants en repérage d’énoncés définitoires en TAL, nous développons la méthode que nous avons mise en place, puis nous présentons son évaluation et les premiers résultats obtenus. Leur repérage atteint de 10% à 69% de précision suivant les patrons, celui des unités lexicales varie de 31% à 56%, suivant le référentiel adopté.},
  abstract  = {In order to build an ontology, a modeler needs to objectivate semantic information about the main terms of his domain. Some tools meant to explore corpora can help pointing out this information, and previous work has focused on the identification of hyperonyms. We propose here to rely on lay definitions to extract the information necessary to build an ontology structure: the vertical axis, related to hypernymy, the horizontal axis, related to co-hyponymy, and the transversal axis, linked to domain-related cross relations. After a survey of previous work about the extraction of definitions in NLP, we develop the method we followed, then present its evaluation criteria and the first results. The mining of lay definitions reached from 10 to 69% of precision, depending on the pattern involved, the mining of lexical items varied from 31 to 56%, following the reference considered.},
  motscles  = {Repérage d’énoncés définitoires, relations sémantiques, patrons lexico-syntaxiques},
  keywords  = {Mining definitions, semantic relations, lexico-syntactic pattern},
}

@inproceedings{plamondon-lapalme-pelletier:2004:TALN,
  author    = {Plamondon, Luc and Lapalme, Guy and Pelletier, Frédéric},
  title     = {Anonymisation de décisions de justice},
  booktitle = {Actes de la 11ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {April},
  year      = {2004},
  address   = {Fès, Maroc},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2004/taln-2004-long-017},
  language  = {french},
  resume    = {La publication de décisions de justice sur le Web permet de rendre la jurisprudence accessible au grand public, mais il existe des domaines du droit pour lesquels la Loi prévoit que l’identité de certaines personnes doit demeurer confidentielle. Nous développons actuellement un système d’anonymisation automatique à l’aide de l’environnement de développement GATE. Le système doit reconnaître certaines entités nommées comme les noms de personne, les lieux et les noms d’entreprise, puis déterminer automatiquement celles qui sont de nature à permettre l’identification des personnes visées par les restrictions légales à la publication.},
  abstract  = {Publishing court decisions on theWeb can make case law available to the general public, but the Law sometimes prohibits the disclosure of the identity of people named in decisions. We are currently developing an automatic anonymization system, using the GATE development environment. The tasks of the system are the recognition of some named entities like person names, locations and company names, then the automatic selection of the ones that may lead to the identification of people whose identities must be legally kept confidential.},
  motscles  = {Anonymisation, désidentification, reconnaissance d’entités nommées, textes juridiques},
  keywords  = {Anonymization, de-identification, named entity recognition, law texts},
}

@inproceedings{lortal-grau-zock:2004:TALN,
  author    = {Lortal, Gaëlle and Grau, Brigitte and Zock, Michael},
  title     = {Système d’aide à l’accès lexical : trouver le mot qu’on a sur le bout de la langue},
  booktitle = {Actes de la 11ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {April},
  year      = {2004},
  address   = {Fès, Maroc},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2004/taln-2004-long-018},
  language  = {french},
  resume    = {Le Mot sur le Bout de la Langue (Tip Of the Tongue en anglais), phénomène très étudié par les psycholinguistes, nous a amené nombre d’informations concernant l’organisation du lexique mental. Un locuteur en état de TOT reconnaît instantanément le mot recherché présenté dans une liste. Il en connaît le sens, la forme, les liens avec d’autres mots... Nous présentons ici une étude de développement d’outil qui prend en compte ces spécificités, pour assister un locuteur/rédacteur à trouver le mot qu’il a sur le bout de la langue. Elle consiste à recréer le phénomène du TOT, où, dans un contexte de production un mot, connu par le système, est momentanément inaccessible. L’accès au mot se fait progressivement grâce aux informations provenant de bases de données linguistiques. Ces dernières sont essentiellement des relations de type paradigmatique et syntagmatique. Il s’avère qu’un outil, tel que SVETLAN, capable de structurer automatiquement un dictionnaire par domaine, peut être avantageusement combiné à une base de données riche en liens paradigmatiques comme EuroWordNet, augmentant considérablement les chances de trouver le mot auquel on ne peut accéder.},
  abstract  = {The study of the Tip of the Tongue phenomenon (TOT) provides valuable clues and insights concerning the organisation of the mental lexicon (meaning, number of syllables, relation with other words, etc.). This paper describes a tool based on psycho-linguistic observations concerning the TOT phenomenon. We’ve built it to enable a speaker/writer to find the word he is looking for, word he may know, but which he is unable to access in time. We try to simulate the TOT phenomenon by creating a situation where the system knows the target word, yet is unable to access it. In order to find the target word we make use of the paradigmatic and syntagmatic associations stored in the linguistic databases. Our experiment allows the following conclusion: a tool like SVETLAN, capable to structure (automatically) a dictionary by domains can be used sucessfully to help the speaker/writer to find the word he is looking for, if it is combined with a database rich in terms of paradigmatic links like EuroWordNet.},
  motscles  = {MBL, accès lexical, relations sémantiques, associations, SVETLAN, EWN},
  keywords  = {TOT, word access, semantic relations, associations, SVETLAN, EWN},
}

@inproceedings{maurel:2004:TALN,
  author    = {Maurel, Fabrice},
  title     = {De l’écrit à l’oral : analyses et générations},
  booktitle = {Actes de la 11ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {April},
  year      = {2004},
  address   = {Fès, Maroc},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2004/taln-2004-long-019},
  language  = {french},
  resume    = {Longtemps considérée comme ornementale, la structure informationnelle des documents écrits prise en charge par la morpho-disposition devient un objet d’étude à part entière dans diverses disciplines telles que la linguistique, la psycholinguistique ou l’informatique. En particulier, nous nous intéressons à l’utilité de cette dimension et, le cas échéant, son utilisabilité, dans le cadre de la transposition automatique à l’oral des textes. Dans l’objectif de fournir des solutions qui permettent de réagir efficacement à cette « inscription morphologique », nous proposons la synoptique d’un système d’oralisation. Nous avons modélisé et partiellement réalisé le module spécifique aux stratégies d’oralisation, afin de rendre « articulables » certaines parties signifiantes des textes souvent « oubliées » par les systèmes de synthèse. Les premiers résultats de cette étude ont conduit à des spécifications en cours d’intégration par un partenaire industriel. Les perspectives de ce travail peuvent intéresser la communauté TAL en reconnaissance de la parole, en génération/résumé de texte ou en multimodalité.},
  abstract  = {Considered for a long time as ornamental, the informational structure of written documents carried by texts morpho-disposition becomes a full object of investigation in various disciplines such as linguistic, psycholinguistic or computer sciences. In Particular, we are interested in the utility of these aspects of documents and, if the need arises, their usability, within the framework of their oral transposition. In the objective to provide solutions which make it possible to react effectively to this “morphological inscription”, we propose the synoptic of an oralisation system. We modelled and partially realized the module specific to the oralisation strategies, in order to render some signifying parts of the text often “forgotten” by synthesis systems. The first results of this study led to specifications in the course of integration by an industrial partner. The prospects of this work can interest NLP community in voice recognition, text generation/summarization or multimodality.},
  motscles  = {Architecture textuelle, synthèse de la parole, stratégies d’oralisation},
  keywords  = {Textual architecture, speech synthesis, oralisation strategies},
}

@inproceedings{vasilescu-langlais:2004:TALN,
  author    = {Vasilescu, Florentina and Langlais, Philippe},
  title     = {Désambiguïsation de corpus monolingues par des approches de type Lesk},
  booktitle = {Actes de la 11ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {April},
  year      = {2004},
  address   = {Fès, Maroc},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2004/taln-2004-long-020},
  language  = {french},
  resume    = {Cet article présente une analyse détaillée des facteurs qui déterminent les performances des approches de désambiguïsation dérivées de la méthode de Lesk (1986). Notre étude porte sur une série d’expériences concernant la méthode originelle de Lesk et des variantes que nous avons adaptées aux caractéristiques de WORDNET. Les variantes implémentées ont été évaluées sur le corpus de test de SENSEVAL2, English All Words, ainsi que sur des extraits du corpus SEMCOR. Notre évaluation se base d’un côté, sur le calcul de la précision et du rappel, selon le modèle de SENSEVAL, et d’un autre côté, sur une taxonomie des réponses qui permet de mesurer la prise de risque d’un décideur par rapport à un système de référence.},
  abstract  = {This paper deals with a detailed analysis of the factors determining the performances of Leskbased WSD methods. Our study consists in a series of experiments on the original Lesk algorithm and on its variants that we adapted to WORDNET. These methods were evaluated on the test corpus from SENSEVAL2, English All Words, and on excerpts from SEMCOR. The evaluation metrics are based on precision and recall, as in SENSEVAL exercises, and on a new method estimating the risk taken by each variant.},
  motscles  = {Désambiguïsation sémantique, algorithme de Lesk, naive Bayes, WORDNET},
  keywords  = {Word sense desambiguation, Lesk’s algorithm, naive Bayes, WORDNET},
}

@inproceedings{blache:2004:TALN,
  author    = {Blache, Philippe},
  title     = {Densité d'information syntaxique et gradient de grammaticalité},
  booktitle = {Actes de la 11ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {April},
  year      = {2004},
  address   = {Fès, Maroc},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2004/taln-2004-long-021},
  language  = {french},
  resume    = {Cet article propose l'introduction d'une notion de densité syntaxique permettant de caractériser la complexité d'un énoncé et au-delà d'introduire la spécification d'un gradient de grammaticalité. Un tel gradient s'avère utile dans plusieurs cas : quantification de la difficulté d'interprétation d'une phrase, gradation de la quantité d'information syntaxique contenue dans un énoncé, explication de la variabilité et la dépendances entre les domaines linguistiques, etc. Cette notion exploite la possibilité de caractérisation fine de l'information syntaxique en termes de contraintes : la densité est fonction des contraintes satisfaites par une réalisation pour une grammaire donnée. Les résultats de l'application de cette notion à quelques corpus sont analysés.},
  abstract  = {This paper introduces the notion of syntactic density that makes it possible to characterize the complexity of an utterance and to specify a gradient of grammaticality. Such a gradient is useful in several cases: quantification of the difficulty of interpreting an utterance, quantification of syntactic information of an utterance, description of variability and linguistic domains interaction, etc. This notion exploits the possibility of fine syntactic characterization in terms of constraints: density if function of satisfied constraints by an utterance for a given grammar. Some results are presented and analyzed.},
  motscles  = {Syntaxe, analyse, robustesse, contraintes, information linguistique, complexité syntaxique},
  keywords  = {Syntax, parsing, robustness, constraints, linguistic information, syntactic complexity},
}

@inproceedings{estratat-henocque:2004:TALN,
  author    = {Estratat, Mathieu and Henocque, Laurent},
  title     = {Application des programmes de contraintes orientés objet à l’analyse du langage naturel},
  booktitle = {Actes de la 11ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {April},
  year      = {2004},
  address   = {Fès, Maroc},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2004/taln-2004-long-022},
  language  = {french},
  resume    = {Les évolutions récentes des formalismes et théories linguistiques font largement appel au concept de contrainte. De plus, les caractéristiques générales des grammaires de traits ont conduit plusieurs auteurs à pointer la ressemblance existant entre ces notions et les objets ou frames. Une évolution récente de la programmation par contraintes vers les programmes de contraintes orientés objet (OOCP) possède une application possible au traitement des langages naturels. Nous proposons une traduction systématique des concepts et contraintes décrits par les grammaires de propriétés sous forme d’un OOCP. Nous détaillons l’application de cette traduction au langage "context free" archétypal anbn, en montrant que cette approche permet aussi bien l’analyse que la génération de phrases, de prendre en compte la sémantique au sein du même modèle et ne requiert pas l’utilisation d’algorithmes ad hoc pour le parsage.},
  abstract  = {Recent evolutions of linguistic theories heavily rely upon the concept of constraint. Also, several authors have pointed the similitude existing between the categories of feature based theories and the notions of objects or frames. A recent evolution of constraint programming to object oriented constraint programs (OOCP) can be applied to natural language parsing. We propose here a systematic translation of the concepts and constraints introduced by property grammars to an OOCP. We apply this translation to the archetypal context free language anbn, and show that this approach allows to both parse and generate, to account for the semantics in the same formalism, and also that it does not require the use of ad hoc algorithms.},
  motscles  = {Grammaires de propriétés, traitement du langage naturel, contraintes, configuration},
  keywords  = {Property grammars, natural language processing, constraints, configuration},
}

@inproceedings{kahane:2004:TALN,
  author    = {Kahane, Sylvain},
  title     = {Grammaires d'unification polarisées},
  booktitle = {Actes de la 11ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {April},
  year      = {2004},
  address   = {Fès, Maroc},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2004/taln-2004-long-023},
  language  = {french},
  resume    = {Cet article propose un formalisme mathématique générique pour la combinaison de structures. Le contrôle de la saturation des structures finales est réalisé par une polarisation des objets des structures élémentaires. Ce formalisme permet de mettre en évidence et de formaliser les mécanismes procéduraux masqués de nombreux formalismes, dont les grammaires de réécriture, les grammaires de dépendance, TAG, HPSG et LFG.},
  abstract  = {This paper proposes a generic mathematical formalism for the combination of structures. The control of saturation of the final structures is realized by a polarization of the objects of the elementary structures. This formalism allows us to bring to the fore and to formalize the hidden procedural mechanisms of numerous formalisms, including rewriting systems, dependency grammars, TAG, HPSG and LFG.},
  motscles  = {Grammaire formelle, unification, polarisation, grammaire de réécriture, grammaire de dépendance, TAG, HPSG, LFG, graphe, arbre, dag},
  keywords  = {Formal grammar, unification, polarization, rewriting system, dependency grammar, TAG, HPSG, LFG, graph, tree, dag},
}

@inproceedings{kallmeyer-yoon:2004:TALN,
  author    = {Kallmeyer, Laura and Yoon, SinWon},
  title     = {},
  booktitle = {Actes de la 11ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {April},
  year      = {2004},
  address   = {Fès, Maroc},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2004/taln-2004-long-024},
  language  = {french},
  note      = {Tree-local MCTAG with Shared Nodes: An Analysis ofWord Order Variation in German and Korean},
  resume    = {Les Grammaires d’Arbres Adjoints (TAG) sont connues pour ne pas être assez puissantes pour traiter le brouillage d’arguments dans des langues à ordre des mots libre. Les variantes TAG proposées jusqu’à maintenant pour expliquer le brouillage ne sont pas entièrement satisfaisantes. Nous présentons ici une extension alternative de TAG, basée sur la notion du partage de noeuds. En considerant des données de l’allemand et du coréen, on montre que cette extension de TAG peut en juste proportion analyser des données de brouillage d’arguments, également en combinaison avec l’extraposition et la topicalisation.},
  abstract  = {Tree Adjoining Grammars (TAG) are known not to be powerful enough to deal with scrambling in free word order languages. The TAG-variants proposed so far in order to account for scrambling are not entirely satisfying. Therefore, an alternative extension of TAG is introduced based on the notion of node sharing. Considering data from German and Korean, it is shown that this TAG-extension can adequately analyse scrambling data, also in combination with extraposition and topicalization.},
  motscles  = {Grammaires d’Arbres Adjoints, brouillage d’arguments, ordre des mots, allemand, coréen},
  keywords  = {Tree Adjoining Grammars, scrambling, word order, German, Korean},
}

@inproceedings{aitelmekki-nazarenko:2004:TALN,
  author    = {Ait El Mekki, Touria and Nazarenko, Adeline},
  title     = {Une mesure de pertinence pour le tri de l’information dans un index de “fin de livre”},
  booktitle = {Actes de la 11ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {April},
  year      = {2004},
  address   = {Fès, Maroc},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2004/taln-2004-long-025},
  language  = {french},
  resume    = {Nous nous intéressons à la construction des index de fin de livres. Nous avons développé le système IndDoc qui aide la construction de tels index. L’un des enjeux de la construction d’index est la sélection des informations : sélection des entrées les plus pertinentes et des renvois au texte les plus intéressants. Cette sélection est évidemment utile pour le lecteur qui doit trouver suffisamment d’information mais sans en être submergé. Elle est également précieuse pour l’auteur de l’index qui doit valider et corriger une ébauche d’index produite automatiquement par IndDoc. Nous montrons comment cette sélection de l’information est réalisée par IndDoc. Nous proposons une mesure qui permet de trier les entrées par ordre de pertinence décroissante et une méthode pour calculer les renvois au texte à associer à chaque entrée de l’index.},
  abstract  = {This paper deals with the construction of end-of-book indexes. We have developed the IndDoc system which assists the construction of such indexes. One of the stakes of the construction of an index is the information selection: selection of the most relevant entries and the most interesting textual fragments. This selection is obviously useful for the reader who is looking for information. It is also invaluable for the index author who has to validate and correct an outline of index produced automatically by IndDoc. We show how this information selection is carried out by IndDoc. We put forward a measure which sorts the entries in decreasing relevance order and a method to calculate the references to text for each entry.},
  motscles  = {Segmentation thématique de texte, extraction d’information, indexation automatique},
  keywords  = {Text segmentation, information extraction, automatic indexing},
}

@inproceedings{boufaden-bengio-lapalme:2004:TALN,
  author    = {Boufaden, Narjès and Bengio, Yoshua and Lapalme, Guy},
  title     = {Approche statistique pour le repérage de mots informatifs dans les textes oraux},
  booktitle = {Actes de la 11ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {April},
  year      = {2004},
  address   = {Fès, Maroc},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2004/taln-2004-long-026},
  language  = {french},
  resume    = {Nous présentons les résultats de l’approche statistique que nous avons développée pour le repérage de mots informatifs à partir de textes oraux. Ce travail fait partie d’un projet lancé par le département de la défense canadienne pour le développement d’un système d’extraction d’information dans le domaine de la Recherche et Sauvetage maritime (SAR). Il s’agit de trouver et annoter les mots pertinents avec des étiquettes sémantiques qui sont les concepts d’une ontologie du domaine (SAR). Notre méthode combine deux types d’information : les vecteurs de similarité générés grâce à l’ontologie du domaine et le dictionnaire-thésaurus Wordsmyth ; le contexte d’énonciation représenté par le thème. L’évaluation est effectuée en comparant la sortie du système avec les réponses de formulaires d’extraction d’information prédéfinis. Les résultats obtenus sur les textes oraux sont comparables à ceux obtenus dans le cadre de MUC7 pour des textes écrits.},
  abstract  = {We present results of a statistical method we developped for the detection of informative words from manually transcribed conversations. This work is part of an ongoing project for an information extraction system in the field of maritime Search And Rescue (SAR). Our purpose is to automatically detect relevant words and annotate them with concepts from a SAR ontology. Our approach combines similarity score vectors and topical information. Similarity vectors are generated using a SAR ontology and theWordsmyth dictionary-thesaurus. Evaluation is carried out by comparing the output of the system with key answers of predefined extraction templates. Results on speech transcriptions are comparable to those on written texts in MUC7.},
  motscles  = {Étiquetage sémantique, extraction d’information},
  keywords  = {Semantic tagging, information extraction},
}

@inproceedings{brun-smaili:2004:TALN,
  author    = {Brun, Armelle and Smaïli, Kamel},
  title     = {Fiabilité de la référence humaine dans la détection de thème},
  booktitle = {Actes de la 11ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {April},
  year      = {2004},
  address   = {Fès, Maroc},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2004/taln-2004-long-027},
  language  = {french},
  resume    = {Dans cet article, nous nous intéressons à la tâche de détection de thème dans le cadre de la reconnaissance automatique de la parole. La combinaison de plusieurs méthodes de détection montre ses limites, avec des performances de 93.1 %. Ces performances nous mènent à remetttre en cause le thème de référence des paragraphes de notre corpus. Nous avons ainsi effectué une étude sur la fiabilité de ces références, en utilisant notamment les mesures Kappa et erreur de Bayes. Nous avons ainsi pu montrer que les étiquettes thématiques des paragraphes du corpus de test comportaient vraisemblablement des erreurs, les performances de détection de thème obtenues doivent donc êtres exploitées prudemment.},
  abstract  = {In this paper, topic detection is studied in the frame of automatic speech recognition. Topic detection methods combination reaches 93.1% correct detection. This rate makes us throw the reference labeling back into question. We have then studied the reliability of the topic labeling of our test corpus, by using the Kappa statistics and the Bayes error. With these measures, we show the topic label of some paragraphs may be wrong, then performance of topic detection may be carefully exploited.},
  motscles  = {Détection de thème, Etiquetage thématique, statistique Kappa, erreur de Bayes},
  keywords  = {Topic detection, topic assignment, Kappa statistics, Bayes error},
}

@inproceedings{antoine:2004:TALN,
  author    = {Antoine, Jean-Yves},
  title     = {Résolution des anaphores pronominales : quelques postulats du TALN mis à l’épreuve du dialogue oral finalisé},
  booktitle = {Actes de la 11ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {April},
  year      = {2004},
  address   = {Fès, Maroc},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2004/taln-2004-long-028},
  language  = {french},
  resume    = {Cet article étudie l'adaptation au dialogue oral homme-machine des techniques de résolution des anaphores pronominales qui ont été développées par le TALN pour les documents écrits. A partir d'une étude de corpus de dialogue oral, il étudie la faisabilité de ce portage de l'écrit vers l'oral. Les résultats de cette étude montrent que certains indices utilisés à l'écrit (accord en nombre, distance entre le pronom est son antécédent) sont plus friables en dialogue oral finalisé. Les techniques développées pour l'écrit ne peuvent donc pas être réutilisées directement à l'oral.},
  abstract  = {In this paper, we present a corpus analysis on pronominal anaphora that investigate the adaptation of anaphora resolution techniques which have been developed for written language processing and that should apply to spoken man-machine dialogue. Unfortunately, this corpus study shows that the criteria that are used on written texts (gender and number agreement, distance between the pronoun and its antecedent) seems to lack robustness on interactive spoken language.},
  motscles  = {Référence, anaphore pronominale, dialogue oral homme-machine, analyse des usages sur corpus},
  keywords  = {Reference, pronominal anaphora, spoken man-machine dialogue, corpus analysis},
}

@inproceedings{muller-tannier:2004:TALN,
  author    = {Muller, Philippe and Tannier, Xavier},
  title     = {Une méthode pour l’annotation de relations temporelles dans des textes et son évaluation},
  booktitle = {Actes de la 11ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {April},
  year      = {2004},
  address   = {Fès, Maroc},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2004/taln-2004-long-029},
  language  = {french},
  resume    = {Cet article traite de l’annotation automatique d’informations temporelles dans des textes et vise plus particulièrement les relations entre événements introduits par les verbes dans chaque clause. Si ce problème a mobilisé beaucoup de chercheurs sur le plan théorique, il reste en friche pour ce qui est de l’annotation automatique systématique (et son évaluation), même s’il existe des débuts de méthodologie pour faire réaliser la tâche par des humains. Nous proposons ici à la fois une méthode pour réaliser la tâche automatiquement et une manière de mesurer à quel degré l’objectif est atteint. Nous avons testé la faisabilité de ceci sur des dépêches d’agence avec des premiers résultats encourageants.},
  abstract  = {This paper focuses on the automated processing of temporal information in written texts, more specifically on relations between events introduced by verbs in every clause. While this latter problem has been largely studied from a theoretical point of view, it has very rarely been applied to real texts, if ever, with quantified results. The methodology required is still to be defined, even though there have been proposals in the human annotation case. We propose here both a procedure to achieve this task and a way of measuring the results. We have been testing the feasability of this on neswire articles, with promising first results.},
  motscles  = {Annotation, Temps, Discours},
  keywords  = {Text annotation, Time, Discourse},
}

@inproceedings{roux:2004:TALN,
  author    = {Roux, Claude},
  title     = {Annoter les documents XML avec un outil d’analyse syntaxique},
  booktitle = {Actes de la 11ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {April},
  year      = {2004},
  address   = {Fès, Maroc},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2004/taln-2004-long-030},
  language  = {french},
  resume    = {Cet article présente l’intégration au sein d’un analyseur syntaxique (Xerox Incremental Parser) de règles spécifiques qui permettent de lier l’analyse grammaticale à la sémantique des balises XML spécifiques à un document donné. Ces règles sont basées sur la norme XPath qui offre une très grande finesse de description et permet de guider très précisément l’application de l’analyseur sur une famille de documents partageant une même DTD. Le résultat est alors être intégré directement comme annotation dans le document traité.},
  abstract  = {This article presents the embedding within a syntactic parser (Xerox Incremental Parser or XIP) of specific rules which are used to bind the grammatical analysis to the semantic of the XML mark up tags specific to a given document. The goal of these rules is to guide the application of a natural language processing tool through the use of XPath instructions to describe documents that share the same DTD. The result can then be embedded within the input document in order to annotate that document.},
  motscles  = {XML, analyse syntaxique, traitement automatique des langues, traitement de documents, Xpath, XIP},
  keywords  = {XML, parsing, natural language processing, document processing, XPath, XIP},
}

@inproceedings{salmonalt-EtAl:2004:TALN,
  author    = {Salmon-Alt, Susanne and Bick, Eckhard and Romary, Laurent and Pierrel, Jean-Marie},
  title     = {La FREEBANK : vers une base libre de corpus annotés},
  booktitle = {Actes de la 11ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {April},
  year      = {2004},
  address   = {Fès, Maroc},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2004/taln-2004-long-031},
  language  = {french},
  resume    = {Les corpus français librement accessibles annotés à d’autres niveaux linguistiques que morpho-syntaxique sont insuffisants à la fois quantitativement et qualitativement. Partant de ce constat, la FREEBANK -- construite sur la base d’outils d’analyse automatique dont la sortie est révisée manuellement -- se veut une base de corpus du français annotés à plusieurs niveaux (structurel, morphologique, syntaxique, coréférentiel) et à différents degrés de finesse linguistique qui soit libre d’accès, codée selon des schémas normalisés, intégrant des ressources existantes et ouverte à l’enrichissement progressif.},
  abstract  = {The few available French resources for evaluating linguistic models or algorithms on other linguistic levels than morpho-syntax are either insufficient from quantitative as well as qualitative point of view or not freely accessible. Based on this fact, the FREEBANK project intends to create French corpora constructed using manually revised output from a hybrid Constraint Grammar parser and annotated on several linguistic levels (structure, morphosyntax, syntax, coreference), with the objective to make them available on-line for research purposes. Therefore, we will focus on using standard annotation schemes, integration of existing resources and maintenance allowing for continuous enrichment of the annotations.},
  motscles  = {ressources libres, annotation multiniveau, corpus arboré, codage référentiel, normalisation},
  keywords  = {free resources, multi-level annotation, treebank, reference annotation, normalisation},
}

@inproceedings{vilnat-EtAl:2004:TALN,
  author    = {Vilnat, Anne and Monceaux, Laura and Paroubek, Patrick and Robba, Isabelle and Gendner, Véronique and Illouz, Gabriel and Jardino, Michèle},
  title     = {Annoter en constituants pour évaluer des analyseurs syntaxiques},
  booktitle = {Actes de la 11ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {April},
  year      = {2004},
  address   = {Fès, Maroc},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2004/taln-2004-long-032},
  language  = {french},
  resume    = {Cet article présente l’annotation en constituants menée dans le cadre d’un protocole d’évaluation des analyseurs syntaxiques (mis au point dans le pré-projet PEAS, puis dans le projet EASY). Le choix des constituants est décrit en détail et une première évaluation effectuée à partir des résultats de deux analyseurs est donnée.},
  abstract  = {This paper focuses on constituent annotation in a syntactic parsers evaluation protocol (which was elaborated in PEAS pre-project and EASY project). The choice of the constituents is described in details, and the results of a first evaluation between two parsers are given.},
  motscles  = {annotation en constituants, évaluation, analyseurs syntaxiques},
  keywords  = {constituent annotation, evaluation, syntactic parser},
}

@inproceedings{elghali:2004:TALN,
  author    = {El Ghali, Adil},
  title     = {Détermination de contenu dans GEPHOX},
  booktitle = {Actes de la 11ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {April},
  year      = {2004},
  address   = {Fès, Maroc},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2004/taln-2004-long-033},
  language  = {french},
  resume    = {Le générateur GEPHOX que nous réalisons a pour ambition de produire des textes pour des définition ou preuves mathématiques écrites à l’aide de l’assistant de preuve PHOX. Dans cet article nous nous concentrons sur le module de détermination de contenu ContDet de GEPHOX. Après un aperçu sur l’entrée du générateur, i.e. la preuve formelle et l’ensemble des règles ayant permis de l’obtenir, nous décrivons les base de connaissances du générateur et le fonctionnement de l’algorithme de détermination de contenu.},
  abstract  = {This paper deals with content determination in a text proofs generation system. Our system, GEPHOX produces a textual version of a mathematical proof formalized using the proof assistant PHOX. We start with a quick presentation of the input of the generator : the formal proof and the set of rules that the proof assistant user employs in order to find it. We describe the generator knowledge bases and define the reasoning tasks associated with the KB and show how the content determination algorithm work.},
  motscles  = {Génération de textes, logique de description, détermination de contenu, bases de connaissance, assistant de preuve},
  keywords  = {Natural language generation, description logic, content determination, knowledge bases, proof assistant},
}

@inproceedings{moreau:2004:TALN,
  author    = {Moreau, Erwan},
  title     = {Apprentissage partiel de grammaires catégorielles},
  booktitle = {Actes de la 11ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {April},
  year      = {2004},
  address   = {Fès, Maroc},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2004/taln-2004-long-034},
  language  = {french},
  resume    = {Cet article traite de l’apprentissage symbolique de règles syntaxiques dans le modèle de Gold. Kanazawa a montré que certaines classes de grammaires catégorielles sont apprenables dans ce modèle. L’algorithme qu’il propose nécessite une grande quantité d’information en entrée pour être efficace. En changeant la nature des informations en entrée, nous proposons un algorithme d’apprentissage de grammaires catégorielles plus réaliste dans la perspective d’applications au langage naturel.},
  abstract  = {This article deals with symbolic learning of syntactic rules in Gold’s model. Kanazawa showed that some classes of categorial grammars are learnable in this model. But the algorithm needs a high amount of information as input to be efficient. By changing the kind of information taken as input, we propose a learning algorithm for categorial grammars which is more realistic in the perspective of applications to natural language.},
  motscles  = {Apprentissage partiel, inférence grammaticale, grammaire catégorielles},
  keywords  = {Partial learning, grammatical inference, categorial grammars},
}

@inproceedings{perrier:2004:TALN,
  author    = {Perrier, Guy},
  title     = {La sémantique dans les grammaires d’interaction},
  booktitle = {Actes de la 11ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {April},
  year      = {2004},
  address   = {Fès, Maroc},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2004/taln-2004-long-035},
  language  = {french},
  resume    = {Nous proposons d’intégrer la sémantique dans les grammaires d’interaction, formalisme qui a été conçu pour représenter la syntaxe des langues. Pour cela, nous ajoutons au formalisme un niveau supplémentaire qui s’appuie sur les mêmes principes fondamentaux que le niveau syntaxique : contrôle de la composition par un système de polarités et utilisation de la notion de description de structure pour exprimer la sous-spécification. A la différence du niveau syntaxique, les structures sont des graphes acycliques orientés et non des arbres localement ordonnés. L’interface entre les deux niveaux est assurée de façon souple par une fonction de liage qui associe à tout noeud syntaxique au plus un noeud sémantique.},
  abstract  = {We propose an integration of semantics into Interaction Grammars, a formalism that was designed for representing the syntax of natural languages. It consists in the addition of a new level to the formalism and this level is based on the same fundamental principles as the syntactical level: the control of composition with a system of polarities and the use of the notion of structure description for expressing underspecification. Unlike the syntactical level, structures are directed acyclic graphs and not locally ordered trees. The interface between the two levels is performed in a flexible way by a linking function which maps every syntactical node to at most one semantical node.},
  motscles  = {formalisme grammatical, interface syntaxe-sémantique, sous-spécification, polarités},
  keywords  = {grammatical formalism, syntax-semantics interface, underspecification, polarities},
}

@inproceedings{sagot-boullier:2004:TALN,
  author    = {Sagot, Benoît and Boullier, Pierre},
  title     = {Les Grammaires à Concaténation d’Intervalles (RCG) comme formalisme grammatical pour la linguistique},
  booktitle = {Actes de la 11ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {April},
  year      = {2004},
  address   = {Fès, Maroc},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2004/taln-2004-long-036},
  language  = {french},
  resume    = {Le but de cet article est de montrer pourquoi les Grammaires à Concaténation d’Intervalles (Range Concatenation Grammars, ou RCG) sont un formalisme particulièrement bien adapté à la description du langage naturel. Nous expliquons d’abord que la puissance nécessaire pour décrire le langage naturel est celle de PTIME. Ensuite, parmi les formalismes grammaticaux ayant cette puissance d’expression, nous justifions le choix des RCG. Enfin, après un aperçu de leur définition et de leurs propriétés, nous montrons comment leur utilisation comme grammaires linguistiques permet de traiter des phénomènes syntagmatiques complexes, de réaliser simultanément l’analyse syntaxique et la vérification des diverses contraintes (morphosyntaxiques, sémantique lexicale), et de construire dynamiquement des grammaires linguistiques modulaires.},
  abstract  = {The aim of this paper is to show why Range Concatenation Grammars (RCG) are a formalism particularly suitable to describe natural language. We first explain that the power necessary to describe natural language is that of PTIME. Then, among grammatical formalisms that have this expressing power, we justify the choice of RCGs. Finally, after an overview of their definition and properties, we show how their use as linguistic grammars makes it possible to deal with complex syntactic phenomena, to achieve simultaneously both syntactic parsing and constraints checking (e.g., morphosyntactic and/or lexical semantic constraints), and to build dynamically modular linguistic grammars.},
  motscles  = {Grammaires de réécriture, Grammaires Faiblement Contextuelles, complexité du langage naturel, Grammaires à Concaténation d’Intervalles (RCG)},
  keywords  = {Rewriting Systems, Mildly Context-Sensitive Grammars, Complexity of Natural Language, Range Concatenation Grammars (RCG)},
}

@inproceedings{alvarez-langlais-nie:2004:TALN,
  author    = {Alvarez, Carmen and Langlais, Philippe and Nie, Jian-Yun},
  title     = {Mots composés dans les modèles de langue pour la recherche d’information},
  booktitle = {Actes de la 11ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {April},
  year      = {2004},
  address   = {Fès, Maroc},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2004/taln-2004-poster-001},
  language  = {french},
  resume    = {Une approche classique en recherche d’information (RI) consiste à bâtir une représentation des documents et des requêtes basée sur les mots simples les constituant. L’utilisation de modèles bigrammes a été étudiée, mais les contraintes sur l’ordre et l’adjacence des mots dans ces travaux ne sont pas toujours justifiées pour la recherche d’information. Nous proposons une nouvelle approche basée sur les modèles de langue qui incorporent des affinités lexicales (ALs), c’est à dire des paires non ordonnées de mots qui se trouvent proches dans un texte. Nous décrivons ce modèle et le comparons aux plus traditionnels modèles unigrammes et bigrammes ainsi qu’au modèle vectoriel.},
  abstract  = {Previous language modeling approaches to information retrieval have focused primarily on single terms. The use of bigram models has been studied, but the restriction on word order and adjacency may not be justified for information retrieval. We propose a new language modeling approach to information retrieval that incorporates lexical affinities (LAs), or pairs of words that occur near each other, without a constraint on word order. We explore the use of LAs in a language modeling approach, and compare our results with the vector space model, and unigram and bigram language model approaches.},
  motscles  = {Modèles de langue, recherche d’information, mots composés},
  keywords  = {Language models, information retrieval, compound terms, word pairs},
}

@inproceedings{atwell:2004:TALN,
  author    = {Atwell, Eric},
  title     = {Le Regroupement de Types de Mots et l'Unification d'Occurrences de Mots dans des Catégories grammaticales de mots},
  booktitle = {Actes de la 11ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {April},
  year      = {2004},
  address   = {Fès, Maroc},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2004/taln-2004-poster-002},
  language  = {french},
  note      = {Clustering of Word Types and Unification of Word Tokens into Grammatical Word-Classes},
  resume    = {Ce papier discute la Néoposie: l'inférence auto-adaptive de catégories grammaticales de mots de la langue naturelle. L'inférence grammaticale peut être divisée en deux parties : l'inférence de catégories grammaticales de mots et l'inférence de la structure. Nous examinons les éléments de base de l'apprentissage auto-adaptif du marquage des catégories grammaticales, et discutons l'adaptation des trois types principaux de marqueurs des catégories grammaticales à l'inférence auto-adaptive de catégories grammaticales de mots. Des marqueurs statistiques de n-grammes suggèrent une approche de regroupement statistique, mais le regroupement n'aide ni avec les types de mots peu fréquents, ni avec les types de mots nombreux qui peuvent se présenter dans plus d'une catégorie grammaticale. Le marqueur alternatif d'apprentissage basé sur la transformation suggère une approche basée sur la contrainte de l'unification de contextes d'occurrences de mots. Celle-ci présente un moyen de regrouper des mots peu fréquents, et permet aux occurrences différentes d'un seul type de mot d'appartenir à des catégories différentes selon les contextes grammaticaux où ils se présentent. Cependant, la simple unification de contextes d'occurrences de mots produit un nombre incroyablement grand de catégories grammaticales de mots. Nous avons essayé d'unifier plus de catégories en modérant le contexte de la correspondance pour permettre l'unification des catégories de mots aussi bien que des occurrences de mots, mais cela entraîne des unifications fausses. Nous concluons que l'avenir peut être un hybride qui comprend le regroupement de types de mots peu fréquents, l'unification de contextes d'occurrences de mots, et le `seeding' avec une connaissance linguistique limitée. Nous demandons un programme de nouvelles recherches pour développer une valise pour la découverte de la langue naturelle.},
  abstract  = {This paper discusses Neoposy: unsupervised inference of grammatical word-classes in Natural  Language. Grammatical Inference can be divided into inference of grammatical word-classes and inference of structure. We review the background of supervised learning of Part-of-Speech tagging; and discuss the adaptation of the three main types of Part-of-Speech tagger to unsupervised inference of grammatical word-classes. Statistical N-gram taggers suggest a statistical clustering approach, but clustering does not help with low-frequency word-types, or with the many word-types which can appear in more than one grammatical category. The alternative Transformation-Based Learning tagger suggests a constraint-based approach of unification of word-token contexts. This offers a way to group together low-frequency word-types, and allows different tokens of one word-type to belong to different categories according to grammatical contexts they appear in. However, simple unification of word-token-contexts yields an implausibly large number of Part-of-Speech categories; we have attempted to merge more categories by "relaxing" matching context to allow unification of word-categories as well as word-tokens, but this results in spurious unifications. We conclude that the way ahead may be a hybrid involving clustering of frequent word-types, unification of word-token-contexts, and "seeding" with limited linguistic knowledge. We call for a programme of further research to develop a Language Discovery Toolkit.},
  motscles  = {Corpus, marquage des catégories grammaticales, regroupement, unification, catégories de mots, type/occurrence, évaluation},
  keywords  = {Corpus, Part-of-Speech tagging, clustering, unification, word classes, type/token, evaluation},
}

@inproceedings{battistelli-EtAl:2004:TALN,
  author    = {Battistelli, Delphine and Minel, Jean-Luc and Picard, Etienne and Schwer, Sylviane R.},
  title     = {Temporalité linguistique et S-Langages},
  booktitle = {Actes de la 11ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {April},
  year      = {2004},
  address   = {Fès, Maroc},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2004/taln-2004-poster-003},
  language  = {french},
  resume    = {Après un rappel de la problématique de l’ordonnancement temporel dans un texte, nous décrivons les S-langages qui offrent une représentation unifiée des relations temporelles et une opération (la jointure) permettant de calculer les combinaisons entre celles-ci.},
  abstract  = {After a brief overview of the problem of text temporal structures calculus, we describe the formal language of S-languages. This framework offers an unified representation of temporal relations and an operation (the joint) which computes combinations between such relations.},
  motscles  = {Structure(s) temporelle(s) dans un texte narratif, S-langages},
  keywords  = {Narratives temporal structure(s), S-langages},
}

@inproceedings{bellengier-priegovalverde:2004:TALN,
  author    = {Bellengier, Emmanuel and Priego-Valverde, Béatrice},
  title     = {Modélisation de la modulation},
  booktitle = {Actes de la 11ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {April},
  year      = {2004},
  address   = {Fès, Maroc},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2004/taln-2004-poster-004},
  language  = {french},
  resume    = {Le dialogue est un processus interactif pendant lequel les différents agents impliqués vont s’engager sur un certain nombre d’éléments propositionnels. La modulation implique des ajouts propositionnels - révisés et atténués - qui ne constituent pas nécessairement une base pour un accord. L’objectif de cet article est donc de proposer une description formelle du phénomène de modulation dans le cadre du modèle de J. Ginzburg.},
  abstract  = {Dialogue is an interactive process in which agents involved must commit about some propositional elements. These elements are then available into the common ground shared by the agents. Mitigation is a particularly productive phenomenon which is problematic for the update of the common ground. We propose a formalisation of the phenomenon into the model of J. Ginzburg.},
  motscles  = {Modulation, discours, modèles de dialogue, interaction verbale, annotation},
  keywords  = {Mitigation, discourse, verbal interaction, models of dialogue, annotation},
}

@inproceedings{blanchon-besacier:2004:TALN,
  author    = {Blanchon, Hervé and Besacier, Laurent},
  title     = {Traduction de dialogue: résultats du projet NESPOLE! et pistes pour le domaine},
  booktitle = {Actes de la 11ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {April},
  year      = {2004},
  address   = {Fès, Maroc},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2004/taln-2004-poster-005},
  language  = {french},
  resume    = {Dans cet article, nous détaillons les résultats de la seconde évaluation du projet européen NESPOLE! auquel nous avons pris part pour le français. Dans ce projet, ainsi que dans ceux qui l’ont précédé, des techniques d’évaluation subjectives — réalisées par des évaluateurs humains — ont été mises en oeuvre. Nous présentons aussi les nouvelles techniques objectives — automatiques — proposées en traduction de l’écrit et mises en oeuvre dans le projet C-STAR III. Nous conclurons en proposant quelques idées et perspectives pour le domaine.},
  abstract  = {},
  motscles  = {Traduction de dialogue, évaluation subjective et objective de composants de TALN},
  keywords  = {},
}

@inproceedings{crispino-jackiewicz-minel:2004:TALN,
  author    = {Crispino, Gustavo and Jackiewicz, Agata and Minel, Jean-Luc},
  title     = {Spécification et implantation informatique d’un langage de description des structures discursives},
  booktitle = {Actes de la 11ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {April},
  year      = {2004},
  address   = {Fès, Maroc},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2004/taln-2004-poster-006},
  language  = {french},
  resume    = {Cet article présente le langage de représentation des connaissances linguistiques LangTex qui permet de spécifier d’une manière unifiée les descriptions linguistiques nécessaires au repérage d’objets textuels qui organisent les textes écrits.},
  abstract  = {This article presents LangTex, a language for linguistic knowledge representation. This language allows to specify in a unified way the linguistic descriptions necessary for the location of textual objects which organize written texts.},
  motscles  = {Représentation des structures discursives, langage de représentation des connaissances linguistiques},
  keywords  = {Representation of discourse structures, linguistic knowledge language representation},
}

@inproceedings{elamrani-delisle-biskri:2004:TALN,
  author    = {El Amrani, Mohamed Yassine and Delisle, Sylvain and Biskri, Ismaïl},
  title     = {@GEWEB : Agents personnels d’aide à la recherche sur le Web},
  booktitle = {Actes de la 11ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {April},
  year      = {2004},
  address   = {Fès, Maroc},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2004/taln-2004-poster-007},
  language  = {french},
  resume    = {Nous présentons dans cet article un logiciel permettant d’assister l’usager, de manière personnalisée lors de la recherche documentaire sur le Web. L’architecture du logiciel est basée sur l’intégration d’outils numériques de traitements des langues naturelles (TLN). Le système utilise une stratégie de traitement semi-automatique où la contribution de l’utilisateur assure la concordance entre ses attentes et les résultats obtenus.},
  abstract  = {We here present a new software that can help the user to formulate his web search queries and customize the information retrieval tasks to her individual and subjective needs. The software’s architecture is based on numeric natural language processing tools. The software involves a semi-automatic processing strategy in which the user’s contribution ensures that the results are useful and meaningful to her.},
  motscles  = {Reformulation de requêtes, Extraction de l’information, Personnalisation},
  keywords  = {Text mining, Web customization, Query reformulation, Information retrieval},
}

@inproceedings{fouquet:2004:TALN,
  author    = {Fouquet, Yannick},
  title     = {Prédiction d’actes et attentes en dialogue : expérience avec un assistant virtuel simulé},
  booktitle = {Actes de la 11ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {April},
  year      = {2004},
  address   = {Fès, Maroc},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2004/taln-2004-poster-008},
  language  = {french},
  resume    = {Dans cet article, nous présentons une plate-forme de test et de recueil de dialogue oral homme-machine. Dans son architecture générale, des magiciens d’Oz simulent la compréhension des énoncés des utilisateurs et le contrôle du dialogue. Puis, nous comparons, dans un tel corpus, la prédiction statistique d’acte de dialogue avec les attentes du locuteur.},
  abstract  = {This paper presents a platform for testing and building human-computer spoken dialog system. In the general architecture of the platform, understanding and dialog management are simulated. Thus, comparison between statistic act prediction and expectation will be made. First results obtained show a credible way of capturing and annotate spoken dialogs.},
  motscles  = {Dialogue, attentes, magicien d’Oz, pragmatique, analyse, statistique},
  keywords  = {Dialog, expectations, magicien d’Oz, pragmatics, analysis, statistics},
}

@inproceedings{gamallo-lopes-agustini:2004:TALN,
  author    = {Gamallo, Pablo and Lopes, Gabriel P. and Agustini, Alexandre},
  title     = {},
  booktitle = {Actes de la 11ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {April},
  year      = {2004},
  address   = {Fès, Maroc},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2004/taln-2004-poster-009},
  language  = {french},
  note      = {Disambiguation and Optional Co-Composition},
  resume    = {Cet article décrit une propriété sémantique propre aux dépendances syntaxiques binaires: la cocomposition. On proposera ici une définition plus générale que celle donnée par Pustejovsky et que nous appelons “co-composition optionnelle”. L’objet de cet article est de montrer les avantages apportées par la co-composition optionnelle dans deux tâches particulières en TAL: la désambiguïsation du sens des mots et la désambiguïsation structurale. Concernant cette deuxième tâche, nous décrirons les expériences faites sur un corpus.},
  abstract  = {This paper describes a specific semantic property underlying binary dependencies: co-composition. We propose a more general definition than that given by Pustejovsky, what we call “optional co-composition”. The aim of the paper is to explore the benefits of optional cocomposition in two disambiguation tasks: both word sense and structural disambiguation. Concerning the second task, some experiments were performed on large corpora.},
  motscles  = {Désambiguïsation du sens des mots, Désambiguïsation structurale, Co-composition, Acquisition de restrictions de sélection},
  keywords  = {Word sense disambiguation, Structural disambiguation, Co-composition, Selection restrictions acquisition},
}

@inproceedings{goulet-bourgeoys:2004:TALN,
  author    = {Goulet, Marie-Josée and Bourgeoys, Joël},
  title     = {Le projet GÉRAF : Guide pour l’Évaluation des Résumés Automatiques Français},
  booktitle = {Actes de la 11ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {April},
  year      = {2004},
  address   = {Fès, Maroc},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2004/taln-2004-poster-010},
  language  = {french},
  resume    = {Dans cet article, nous présentons le projet GÉRAF (Guide pour l’Évaluation des Résumés Automatiques Français), lequel vise l’élaboration de protocoles et la construction de corpus de résumés de référence pour l’évaluation des systèmes résumant des textes français. La finalité de ce projet est de mettre à la disposition des chercheurs les ressources ainsi créées.},
  abstract  = {In this paper, we introduce GÉRAF (Guide pour l’Évaluation des Résumés Automatiques Français), which aims at elaborating protocols and creating human-generated summaries for summarization evaluation in the context of French texts. The goal of this project is to provide researchers with protocols and corpora needed for French summarization evaluation.},
  motscles  = {évaluation, résumé automatique, textes français, GÉRAF},
  keywords  = {evaluation, automatic summarization, French texts, GÉRAF},
}

@inproceedings{grabar-EtAl:2004:TALN,
  author    = {Grabar, Natalia and Malaisé, Véronique and Marcus, Aurélia and Krul, Aleksandra},
  title     = {Repérage de relations terminologiques transversales en corpus},
  booktitle = {Actes de la 11ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {April},
  year      = {2004},
  address   = {Fès, Maroc},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2004/taln-2004-poster-011},
  language  = {french},
  resume    = {Les relations transversales encodent des relations spécifiques entre les termes, par exemple localisé-dans, consomme, etc. Elles sont très souvent dépendantes des domaines, voire des corpus. Les méthodes automatiques consacrées au repérage de relations terminologiques plus classiques (hyperonymie, synonymie), peuvent générer occasionnellement les relations transversales. Mais leur repérage et typage restent sujets à une conceptualisation : ces relations ne sont pas attendues et souvent pas connues à l’avance pour un nouveau domaine à explorer. Nous nous attachons ici à leur repérage mais surtout à leur typage. En supposant que les relations sont souvent exprimées par des verbes, nous misons sur l’étude des verbes du corpus et de leurs divers dérivés afin d’aborder plus directement la découverte des relations du domaine. Les expériences montrent que ce point d’attaque peut être intéressant, mais reste pourtant dépendant de la polysémie verbale et de la synonymie.},
  abstract  = {Transversal relations describe specific information existing between terms, for instance consumes, located-in, etc. They are often dependent on domains and even on corpora. Automatic methods, conceived to detect classical terminological relations (hyperonymy, synonymy), can occasionnally generate transversal relations. But their detection and typology depend on their conceptualisation : these relations are not expected and often not known for a newly explored domain. We aim here at their detection, but mainly at their typology. Since we suppose these relations are often expressed with verbs, we concentrate our investigation on the study of verbs and their derivatives to attack directly their discovering. Experiences show that this approach proposes interesting results which are nevertheless dependent on verbal polysemy and synonymy.},
  motscles  = {Terminologie, corpus spécialisés, structuration de terminologies, relations transversales, verbes},
  keywords  = {Terminology, Specialised Corpora, Terminology Structuring, Transversal Relations, Verbs},
}

@inproceedings{jalabert-lafourcade:2004:TALN,
  author    = {Jalabert, Fabien and Lafourcade, Mathieu},
  title     = {Classification automatique de définitions en sens},
  booktitle = {Actes de la 11ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {April},
  year      = {2004},
  address   = {Fès, Maroc},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2004/taln-2004-poster-012},
  language  = {french},
  resume    = {Dans le cadre de la recherche en sémantique lexicale, l’équipe TAL du LIRMM développe actuellement un système d’analyse des aspects thématiques des textes et de désambiguisation lexicale basé sur les vecteurs conceptuels. Pour la construction des vecteurs, les définitions provenant de sources lexicales différentes (dictionnaires à usage humain, listes de synonymes, définitions de thésaurus, . . .) sont analysées. Aucun découpage du sens n’est présent dans la représentation : un vecteur conceptuel est associé à chaque définition et un autre pour représenter le sens global du mot. Nous souhaitons effectuer une catégorisation afin que chaque élément ne soit plus une définition mais un sens. Cette amélioration concerne bien sur directement les applications courantes (désambiguïsation, transfert lexical, . . .) mais a aussi pour objectif majeur d’améliorer l’apprentissage de la base.},
  abstract  = {In the framework of research in meaning representation in NLP, we focus our attention on thematic aspects and conceptual vectors. A vectorial base is built upon a morphosyntactic analysis of several lexical resources to reduce isolated problems. A conceptual vector is associated with each definition and another one with the global meaning of a word. There is no effective meaning division and representation the the knowledge base. We study in the article a clustering method that merge definitions into senses. This applies on common problems (word sense disambiguation, word translation, . . .) and mainly to improve knowledge base learning.},
  motscles  = {Traitement automatique des langues naturelles, classification automatique, désambiguïsation sémantique lexicale},
  keywords  = {Natural language processing, unsupervised clustering, word sense disambiguation},
}

@inproceedings{kempe:2004:TALN,
  author    = {Kempe, André},
  title     = {},
  booktitle = {Actes de la 11ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {April},
  year      = {2004},
  address   = {Fès, Maroc},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2004/taln-2004-poster-013},
  language  = {french},
  note      = {NLP Applications Based onWeightedMulti-Tape Automata},
  resume    = {},
  abstract  = {This article describes two practical applications of weighted multi-tape automata (WMTAs) in Natural Language Processing, that demonstrate the augmented descriptive power of WMTAs compared to weighted 1-tape and 2-tape automata. The two examples concern the preservation of intermediate results in transduction cascades and the search for similar words in two languages. As a basis for these applications, the article proposes a number of operations on WMTAs. Among others, it (re-)defines multi-tape intersection, where a number of tapes of one WMTA are intersected with the same number of tapes of another WMTA. In the proposed approach, multi-tape intersection is not an atomic operation but rather a sequence of more elementary ones, which facilitates its implementation.},
  motscles  = {},
  keywords  = {finite-state automaton, weighted multi-tape automaton, transduction cascade, lexicon},
}

@inproceedings{neugebauer-wilson:2004:TALN,
  author    = {Neugebauer, Moritz and Wilson, Stephen},
  title     = {},
  booktitle = {Actes de la 11ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {April},
  year      = {2004},
  address   = {Fès, Maroc},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2004/taln-2004-poster-014},
  language  = {french},
  note      = {Multiple Lexicon Generation based on Phonological Feature Trees},
  resume    = {De manière générale, les linguistes informaticiens utilisent les structures de données arborescentes pour la documentation et l’analyse des données morphologiques et syntactiques. Dans cet article nous appliquons de telles structures sur des données phonologiques et nous démontrons comment de telles représentations peuvent avoir des applications utiles et pratiques en lexicographie informatique. À cet effet, nous décrivons trois modules intégrés: Le premier module définit un ensemble de caractéristiques multilangages dans une structure arborescente exprimée en XML; le deuxième module parcours cet arbre et établis une généralisation sur des données contenues dans cet arborescence, optimise les données phonologiques et mets en valeur les implications des caractéristiques. Le troisième module utilise l’information contenue dans l’arborescence comme une base de connaissance pour la génération de syllabes lexiques à caractéristiques multiples.},
  abstract  = {Tree-based data structures are commonly used by computational linguists for the documentation and analysis of morphological and syntactic data. In this paper we apply such structures to phonological data and demonstrate how such representations can have practical and beneficial applications in computational lexicography. To this end, we describe three integrated modules: the first defines a multilingual feature set within a tree-based structure using XML; the second module traverses this tree and generalises over the data contained within it, optimising the phonological data and highlighting feature implications. The third uses the information contained within the tree representation as a knowledge base for the generation of multiple feature-based syllable lexica.},
  motscles  = {Lexicographie, Représentations phonologiques, XML},
  keywords  = {Lexicography, Phonological representations, XML},
}

@inproceedings{caelen-nguyen:2004:TALN,
  author    = {Caelen, Jean and Nguyen, Hoâ},
  title     = {Gestion de buts de dialogue},
  booktitle = {Actes de la 11ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {April},
  year      = {2004},
  address   = {Fès, Maroc},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2004/taln-2004-poster-015},
  language  = {french},
  resume    = {La gestion du but de dialogue est une tâche délicate pour le contrôleur de dialogue, car bien souvent il est en concurrence avec le gestionnaire de tâches avec lequel on le confond parfois dans certains systèmes. Dans cet article, nous présentons une stratégie dynamique de gestion de buts qui permet au contrôleur de dialogue de réduire sa dépendance au gestionnaire de tâche et lui apporte une meilleure réutilisabilité. Nous expérimentons le système dans le cadre du projet PVE (Portail Vocal d’Entreprise) dans lequel le dialogue peut se dérouler en plusieurs sessions et avec des interlocuteurs différents.},
  abstract  = {The dialogue goal management is an difficult task because often, dialogue management and task control are strong mixed together. We present in this paper a dynamic strategy to manage dialogue goals that enhances more independence of the dialogue manager with the task manager and brings a better reusability to the whole dialogue system. Our experiment in the framework of PVE (Vocal Portal for Enterprise) shows the possibility to use the dialogue system along a series of sessions and with different speakers.},
  motscles  = {Dialogue oral homme-machine, modèle de dialogue, but de dialogue},
  keywords  = {Human-machine spoken dialogue, dialogue model, dialogue goal},
}

@inproceedings{pitel-sansonnet:2004:TALN,
  author    = {Pitel, Guillaume and Sansonnet, Jean-Paul},
  title     = {Un modèle d’interprétation constructionnelle pour les expressions référentielles extensionnelles},
  booktitle = {Actes de la 11ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {April},
  year      = {2004},
  address   = {Fès, Maroc},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2004/taln-2004-poster-016},
  language  = {french},
  resume    = {Dans le dialogue finalisé, les expressions référentielles portant sur les objets du contexte peuvent contenir des prédicats vagues ou relationnels, qu’il est difficile de traiter avec une logique propositionnelle. Inversement, les approches adaptées à ces types de prédicats sont difficilement implémentables dans un modèle générique et adaptable aux théories d’analyse linguistique. Nous proposons un modèle d’interprétation constructionnelle inspiré des grammaires de construction qui permet de modéliser le processus de résolution d’expressions référentielles extensionnelles tout en restant compatible avec la grammaire dont nous nous sommes inspirés.},
  abstract  = {In practical dialogue, vague or relational predicates play an important role in referential expressions that refer to the objects in the context. These kind of referential expressions are difficult to handle using propositional logic but in the same time, approaches dealing with such predicates are hardly adaptable to language analysis theories. We propose a model of constructional interpretation that allow modelisation of resolution of extensional referential expressions while keeping compatibility with construction grammar theory.},
  motscles  = {Grammaire de Construction, Référence Extensionnelle, Domaines de Référence},
  keywords  = {Construction Grammar, Extensionnal Reference Resolution, Reference Domains},
}

@inproceedings{poudade-paroubek:2004:TALN,
  author    = {Poudade, Julien and Paroubek, Patrick},
  title     = {Apprentissage collectif et lexique},
  booktitle = {Actes de la 11ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {April},
  year      = {2004},
  address   = {Fès, Maroc},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2004/taln-2004-poster-017},
  language  = {french},
  resume    = {Cet article présente l’influence de la zone de travail que possède une entité logicielle pour lui permettre de prédire l’état futur de son environnement, sur la constitution d’un lexique partagé par les différents membres d’une population, dans le cadre d’une variante “du jeu de désignation” (naming game).},
  abstract  = {In this paper, we show the influence that the work area used by software entities to predict the future state of their environment, has on the establishment of a common lexicon shared by the members of a population involved in a variant of the naming game.},
  motscles  = {lexique, apprentissage automatique, système multi-agents},
  keywords  = {lexicon, machine learning, multi-agent system},
}

@inproceedings{rousselot:2004:TALN,
  author    = {Rousselot, François},
  title     = {L’outil de traitement de corpus LIKES},
  booktitle = {Actes de la 11ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {April},
  year      = {2004},
  address   = {Fès, Maroc},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2004/taln-2004-poster-018},
  language  = {french},
  resume    = {LIKES (LInguistic and Knowledge Engineering Station) est une station d’ingénierie linguistique destinée à traiter des corpus, elle fonctionne pour l’instant sur la plupart des langues européennes et slaves en utilisant des ressources minimales pour chaque langue. Les corpus sont constitués d’un ou plusieurs textes en ASCII ou en HTML, l’interface donne la possibilité de constituer son corpus et d’y exécuter un certain nombre de tâches allant de simples tâches de découpage en mot, de tri ou de recherche de motifs à des tâches plus complexes d’aide à la synthèse de grammaire, d’aide au repérage de relations, d’aide à la construction d’une terminologie. Nous décrivons ici les principales fonctionnalités de LIKES en rapport avec le traitement des corpus et ce qui fait sa spécificité par rapport à d’autres environnements comparables : l’utilisation minimale de ressources linguistiques.},
  abstract  = {LIKES (Llnguistic and Knowledge Engineering Station) is a linguistic engineering environment, build for corpora processing. Its provides different modules able to process most european and slavian languages. Corpora in Likes must be constituted by Texts in TXT format or in HTML texts of one particular. Tasks available are elementary likes classical basic corpora processing tasks (making list of forms, segmenting, sorting) and also more sophisticated as term extraction, help in relation extraction, pattern search, aimed at helping terminology building and ontology building. Main functionalities usefull for corpora processing are presented here.},
  motscles  = {Traitement de corpus, segments répétés, recherches de relations, automates, transducteurs},
  keywords  = {Corpus processing, repeated segments, search of semantic relations, automata, transducers},
}

@inproceedings{salmonalt:2004:TALN,
  author    = {Salmon-Alt, Susanne},
  title     = {Résolution automatique d’anaphores infidèles en français : Quelles ressources pour quels apports ?},
  booktitle = {Actes de la 11ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {April},
  year      = {2004},
  address   = {Fès, Maroc},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2004/taln-2004-poster-019},
  language  = {french},
  resume    = {La performance d’une résolution automatique d’anaphores infidèles pour le français pourrait atteindre une F-mesure de 30%. Ce résultat repose toutefois sur une ressource équivalente à un bon dictionnaire de la langue française, une analyse syntaxique de qualité satisfaisante et un traitement performant des entités nommées. En l’absence de telles ressources, les meilleurs résultats plafonnent autour d’une F-mesure de 15%.},
  abstract  = {A system for solving indirect anaphora in French seems to be able to achieve a F-measure of 30%. However, this result supposes a high quality lexical database (equivalent to a classical dictionary), a good parser and a high precision named entity recognition. In case such resources are not available, the best results are obtained by using simple heuristics and are limited to a F-measure of 15%.},
  motscles  = {anaphore infidèle, ressource sémantique, résolution d’anaphore, corpus annoté multiniveau},
  keywords  = {indirect anaphor, lexical database, anaphora resolution, multi-level corpus annotation},
}

@inproceedings{schadle-EtAl:2004:TALN,
  author    = {Schadle, Igor and Antoine, Jean-Yves and Le Pévédic, Brigitte and Poirier, Franck},
  title     = {SibyMot : Modélisation stochastique du langage intégrant la notion de chunks},
  booktitle = {Actes de la 11ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {April},
  year      = {2004},
  address   = {Fès, Maroc},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2004/taln-2004-poster-020},
  language  = {french},
  resume    = {Cet article présente le modèle de langage développé pour le système Sibylle, un système d’aide à la communication pour les personnes handicapées. L’utilisation d’un modèle de langage permet d’améliorer la pertinence des mots proposés en tenant compte du contexte gauche de la saisie en cours. L’originalité de notre modèle se situe dans l’intégration de la notion de chunks afin d’élargir la taille du contexte pris en compte pour l’estimation de la probabilité d’apparition des mots.},
  abstract  = {We present in this article the language model of Sibyl, a new Alternative and Augmentative Communication (AAC) system. The use of language modeling improves the relevance of displayed words by taking into account the left context of the current sentence. The originality of our model is to introduce chunking. This enlarges the context taken into account to estimate the words probability.},
  motscles  = {Aide à la communication, modélisation stochastique du langage, n-gramme, chunks},
  keywords  = {AAC, stochastic language modeling, n-gram, chunks},
}

@inproceedings{silva-EtAl:2004:TALN,
  author    = {Silva, Joaquim and Kozareva, Zornitsa and Noncheva, Veska and Lopes, Gabriel},
  title     = {},
  booktitle = {Actes de la 11ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {April},
  year      = {2004},
  address   = {Fès, Maroc},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2004/taln-2004-poster-021},
  language  = {french},
  note      = {Extracting Named Entities. A Statistical Approach},
  resume    = {Les entitées nomées et plus généralement les multi-mots sont des ressources importantes pour plusieurs applications. Cependant, les métodes d’extraction automatique, indépendentes de la langue, de multi-mots, ne nous donnent pas des données 100% fiables. Dans ce papier nous proposons premièrement une méthode pour selectioner entités nomées d’entre les multi-mots extraits automatiquement et, deuxièmement, une méthode de groupement des entités nomées non-supervisionée et indépendente de la langue, en utilisant de la statistique. La deuxième phase de groupement rends l’évaluation humaine plus simple. Les traits utilisés pour le groupement sont décrits et motivés. L’analyse faite pour le groupement nous a permis d’obtenir différents groupes d’entités nomées. La méthode a été appliquée sur le bulgare et l’anglais. La précision obtenue pour certains groupes a été très haute. D’autres groupes doivent être encore rafinés. Par ailleurs, les traits discrimants appris pendant la phase de groupement nous permettent de classifier de nouvelles entités nomées.},
  abstract  = {Named entities and more generally Multiword Lexical Units (MWUs) are important for various applications. However, language independent methods for automatically extracting MWUs do not provide us with clean data. So, in this paper we propose a method for selecting possible named entities from automatically extracted MWUs, and later, a statistics-based language independent unsupervised approach is applied to possible named entities in order to cluster them according to their type. Statistical features used by our clustering process are described and motivated. The Model-Based Clustering Analysis (MBCA) software enabled us to obtain different clusters for proposed named entities. The method was applied to Bulgarian and English. For some clusters, precision is very high; other clusters still need further refinement. Based on the obtained clusters, it is also possible to classify new possible named entities.},
  motscles  = {Entités Nommées, Unités Multi-mots, Groupement, Classification},
  keywords  = {Named Entities, Multiword Units, Clustering, Classification},
}

@inproceedings{stroppa-yvon:2004:TALN,
  author    = {Stroppa, Nicolas and Yvon, François},
  title     = {Analogies dans les séquences : un solveur à états finis},
  booktitle = {Actes de la 11ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {April},
  year      = {2004},
  address   = {Fès, Maroc},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2004/taln-2004-poster-022},
  language  = {french},
  resume    = {L’apprentissage par analogie se fonde sur un principe inférentiel potentiellement pertinent pour le traitement des langues naturelles. L’utilisation de ce principe pour des tâches d’analyse linguistique présuppose toutefois une définition formelle de l’analogie entre séquences. Dans cet article, nous proposons une telle définition et montrons qu’elle donne lieu à l’implantation efficace d’un solveur d’équations analogiques sous la forme d’un transducteur fini. Munis de ces résultats, nous caractérisons empiriquement l’extension analogique de divers langages finis, correspondant à des dictionnaires de quatre langues.},
  abstract  = {Analogical reasoning provides us with an inferential mecanism of potential interest for NLP applications. An effective use of this process requires a formal definition of the notion of an analogy between strings of symbols. In this paper, we propose such a definition, from which we derive the implementation of a finite-state transducer solving analogical equations on sequences. We finally present the results of an empirical study of the analogical extension of several finite languages, corresponding to dictionnaries of four European languages.},
  motscles  = {Apprentissage par Analogie, Automates finis},
  keywords  = {Analogical Learning, Finite-State Automaton},
}

@inproceedings{tsalidis-vagelatos-orphanos:2004:TALN,
  author    = {Tsalidis, Christos and Vagelatos, Aristides and Orphanos, Giorgos},
  title     = {},
  booktitle = {Actes de la 11ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {April},
  year      = {2004},
  address   = {Fès, Maroc},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2004/taln-2004-poster-023},
  language  = {french},
  note      = {An electronic dictionary as a basis for NLP tools: The Greek case},
  resume    = {},
  abstract  = {The existence of a Dictionary in electronic form for Modern Greek (MG) is mandatory if one is to process MG at the morphological and syntactic levels since MG is a highly inflectional language with marked stress and a spelling system with many characteristics carried over from Ancient Greek. Moreover, such a tool becomes necessary if one is to create efficient and sophisticated NLP applications with substantial linguistic backing and coverage. The present paper will focus on the deployment of such an electronic dictionary for Modern Greek, which was built in two phases: first it was constructed to be the basis for a spelling correction schema and then it was reconstructed in order to become the platform for the deployment of a wider spectrum of NLP tools.},
  motscles  = {Lexique, morphologie},
  keywords  = {Lexicon, morphology},
}

@inproceedings{vuminh-EtAl:2004:TALN,
  author    = {Vu-minh, Quang and Besacier, Laurent and Blanchon, Hervé and Bigi, Brigitte},
  title     = {Modèle de langage sémantique pour la reconnaissance automatique de parole dans un contexte de traduction},
  booktitle = {Actes de la 11ème conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {April},
  year      = {2004},
  address   = {Fès, Maroc},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2004/taln-2004-poster-024},
  language  = {french},
  resume    = {Le travail présenté dans cet article a été réalisé dans le cadre d'un projet global de traduction automatique de la parole. L’approche de traduction est fondée sur un langage pivot ou Interchange Format (IF), qui représente le sens de la phrase indépendamment de la langue. Nous proposons une méthode qui intègre des informations sémantiques dans le modèle statistique de langage du système de Reconnaissance Automatique de Parole. Le principe consiste a utiliser certaines classes définies dans l'IF comme des classes sémantiques dans le modèle de langage. Ceci permet au système de reconnaissance de la parole d'analyser partiellement en IF les tours de parole. Les expérimentations realisées montrent qu’avec cette approche, le système de reconnaissance peut analyser directement en IF une partie des données de dialogues de notre application, sans faire appel au système de traduction (35% des mots ; 58% des tours de parole), tout en maintenant le même niveau de performance du système global.},
  abstract  = {This paper relates a methodology to include some semantic information early in the statistical language model for Automatic Speech Recognition (ASR). This work is done in the framework of a global speech-to-speech translation project. An Interchange Format (IF) based approach, representing the meaning of phrases independently of languages, is adopted. The methodology consists in introducing semantic information by using a class-based statistical language model for which classes directly correspond to IF entries. With this new Language Model, the ASR module can analyze into IF part of dialogue data: 35% dialogue words; 58% speaker turns, without degrading the overall system performance.},
  motscles  = {Traduction de parole, modèles de langage, représentation pivot},
  keywords  = {Speech-to-speech translation, language modeling, interchange format},
}