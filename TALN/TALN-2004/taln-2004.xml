<?xml version="1.0" encoding="UTF-8"?>
<conference>
	<!--
		Utilisation de pdfconvertonline pour supprimer les restrictions de copier-coller des fichiers pdfs...
		taln-2004-long-028, taln-2004-poster-002, taln-2004-poster-008, taln-2004-poster-015, taln-2004-poster-018 ont été OCRisés pour en extraire les informations
		L'article "Annoter en constituants pour évaluer des analyseurs syntaxiques" apparait deux fois dans le programme, une fois en long (choix pour TALN archives) et une fois en poster.
	-->
	<edition>
		<acronyme>TALN'2004</acronyme>
		<titre>11ème conférence sur le Traitement Automatique des Langues Naturelles</titre>
		<ville>Fès</ville>
		<pays>Maroc</pays>
		<dateDebut>2004-04-19</dateDebut>
		<dateFin>2004-04-22</dateFin>
		<presidents>
			<nom>Philippe Blache</nom>
			<nom>Noël Nguyen</nom>
			<nom>Nouredine Chenfour</nom>
			<nom>Abdenbi Rajouani</nom>
		</presidents>
		<typeArticles>
			<type id="long">Papiers longs</type>
			<type id="poster">Posters</type>
		</typeArticles>
		<statistiques>
			<!-- <acceptations id="long" soumissions=""></acceptations>
			<acceptations id="poster" soumissions=""></acceptations> -->
		</statistiques>
		<siteWeb>http://aune.lpl.univ-aix.fr/jep-taln04/</siteWeb>
		<meilleurArticle>
			<articleId></articleId>
		</meilleurArticle>
	</edition>
	<articles>
		<article id="taln-2004-long-001" session="Session orale 1A">
			<auteurs>
				<auteur>
					<nom>Laurianne Sitbon</nom>
					<email>laurianne.sitbon@lia.univ-avignon.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Patrice Bellot</nom>
					<email>patrice.bellot@lia.univ-avignon.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Laboratoire d’Informatique d’Avignon - Université d’Avignon, 339, chemin des Meinajaries - Agroparc BP 1228,84911 AVIGNON Cedex 9 - FRANCE</affiliation>
			</affiliations>
			<titre>Evaluation de méthodes de segmentation thématique linéaire non supervisées après adaptation au français</titre>
			<type>long</type>
			<pages></pages>
			<resume>Nous proposons une évaluation de différentes méthodes et outils de segmentation thématique de textes. Nous présentons les outils de segmentation linéaire et non supervisée DotPlotting, Segmenter, C99, TextTiling, ainsi qu’une manière de les adapter et de les tester sur des documents français. Les résultats des tests montrent des différences en performance notables selon les sujets abordés dans les documents, et selon que le nombre de segments à trouver est fixé au préalable par l’utilisateur. Ces travaux font partie du projet Technolangue AGILE-OURAL.</resume>
			<mots_cles>Segmentation thématique, métriques de Beeferman et WindowDiff, cohésion lexicale, chaînes lexicales</mots_cles>
			<title></title>
			<abstract>This paper presents an empirical comparison between different methods for segmenting texts. After presenting segmentation tools and more specifically linear segmentation algorithms, we present a comparison of these methods on both French and English text corpora. This evalutation points out that the performance of each method heavilly relies on the topic of the documents, and the number of boundaries to be found. This work is part of the project Technolangue AGILE-OURAL.</abstract>
			<keywords>Topic segmentation, WindowDiff and Beeferman measures, lexical cohesion, lexical chains</keywords>
		</article>
		<article id="taln-2004-long-002" session="Session orale 1A">
			<auteurs>
				<auteur>
					<nom>Didier Bourigault</nom>
					<email>didier.bourigault@univ-tlse2.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Cécile Frérot</nom>
					<email>cecile.frerot@univ-tlse2.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">ERSS – Université Toulouse-Le Mirail, Maison de la Recherche, 5 allées A. Machado, 31058 Toulouse Cedex</affiliation>
			</affiliations>
			<titre>Ambiguïté de rattachement prépositionnel : introduction de ressources exogènes de sous-catégorisation dans un analyseur syntaxique de corpus endogène</titre>
			<type>long</type>
			<pages></pages>
			<resume>Nous présentons les résultats d’expérimentations visant à introduire des ressources lexicosyntaxiques génériques dans un analyseur syntaxique de corpus à base endogène (SYNTEX) pour la résolution d’ambiguïtés de rattachement prépositionnel. Les données de souscatégorisation verbale sont élaborées à partir du lexique-grammaire et d’une acquisition en corpus (journal Le Monde). Nous présentons la stratégie endogène de désambiguïsation, avant d’y intégrer les ressources construites. Ces stratégies sont évaluées sur trois corpus (scientifique, juridique et journalistique). La stratégie mixte augmente le taux de rappel (+15% sur les trois corpus cumulés) sans toutefois modifier le taux de précision (~ 85%). Nous discutons ces performances, notamment à la lumière des résultats obtenus par ailleurs sur la préposition de.</resume>
			<mots_cles>analyse syntaxique automatique, ambiguïté de rattachement prépositionnel, procédures endogènes, ressources exogènes, approche mixte</mots_cles>
			<title></title>
			<abstract>We report the results of experiments aimed at integrating general lexico-syntactic resources into a corpus syntactic parser (SYNTEX) based on endogenous learning. We tackle the issue of prepositional phrase attachment. We make use of both French lexico-syntactic resources and automatic acquisition to extract verb subcategorisation data. We describe both the endogenous and hybrid approaches and show how the latter improves the recall rate - +15% in average - but has no impact on the precision rate (~ 85%).</abstract>
			<keywords>automatic parsing, prepositional phrase attachement disambiguation, endogenous learning, exogenous resources, hybrid approach</keywords>
		</article>
		<article id="taln-2004-long-003" session="Session orale 1A">
			<auteurs>
				<auteur>
					<nom>Sylvain Pogodalla</nom>
					<email>sylvain.pogodalla@loria.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LORIA – Campus Scientifique, BP239, F-54602 Vandoeuvre-lès-Nancy</affiliation>
			</affiliations>
			<titre>Vers un statut de l’arbre de dérivation : exemples de construction de representations sémantiques pour les Grammaires d’Arbres Adjoints</titre>
			<type>long</type>
			<pages></pages>
			<resume>Cet article propose une définition des arbres de dérivation pour les Grammaires d’Arbres Adjoints, étendant la notion habituelle. Elle est construite sur l’utilisation des Grammaires Catégorielles Abstraites et permet de manière symétrique le calcul de la représentation syntaxique (arbre dérivé) et le calcul de la représentation sémantique.</resume>
			<mots_cles>Sémantique, grammaires d’arbre adjoints, grammaires catégorielles, λ-calcul</mots_cles>
			<title></title>
			<abstract>This paper suggests a definition for Tree Adjoining Grammar derivation trees, extending the usual notion. It results from using Abstract Categorial Grammars and enables, in a symmetric way, the computation of both the syntactic representation (derived tree) and the semantic representation.</abstract>
			<keywords>Semantics, tree adjoining grammars, categorial grammars, λ-calculus</keywords>
		</article>
		<article id="taln-2004-long-004" session="Session orale 1B">
			<auteurs>
				<auteur>
					<nom>Vincent Claveau</nom>
					<email>Vincent.Claveau@irisa.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Pascale Sébillot</nom>
					<email>Pascale.Sébillot@irisa.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">IRISA - Université de Rennes 1, Campus de Beaulieu, 35042 Rennes Cedex, FRANCE</affiliation>
			</affiliations>
			<titre>Extension de requêtes par lien sémantique nom-verbe acquis sur corpus</titre>
			<type>long</type>
			<pages></pages>
			<resume>En recherche d’information, savoir reformuler une idée par des termes différents est une des clefs pour l’amélioration des performances des systèmes de recherche d’information (SRI) existants. L’un des moyens pour résoudre ce problème est d’utiliser des ressources sémantiques spécialisées et adaptées à la base documentaire sur laquelle les recherches sont faites. Nous proposons dans cet article de montrer que les liens sémantiques entre noms et verbes appelés liens qualia, définis dans le modèle du Lexique génératif (Pustejovsky, 1995), peuvent effectivement améliorer les résultats des SRI. Pour cela, nous extrayons automatiquement des couples nom-verbe en relation qualia de la base documentaire à l’aide du système d’acquisition ASARES (Claveau, 2003a). Ces couples sont ensuite utilisés pour étendre les requêtes d’un système de recherche. Nous montrons, à l’aide des données de la campagne d’évaluation Amaryllis, que cette extension permet effectivement d’obtenir des réponses plus pertinentes, et plus particulièrement pour les premiers documents retournés à l’utilisateur.</resume>
			<mots_cles>Lexique sémantique, acquisition sur corpus, recherche d’information, Lexique génératif, extension de requête</mots_cles>
			<title></title>
			<abstract>In the information retrieval field, managing the equivalent reformulations of a same idea is a key point to improve the performances of existing retrieval systems. One way to reach this goal is to use specialised semantic resources that are suited to the document database on which the queries are processed. In this paper, we show that the semantic links between nouns and verbs called qualia links, defined in the Generative lexicon framework (Pustejovsky, 1995), enable us to improve the results of retrieval systems. To achieve this goal, we automatically extract from the document database noun-verb pairs that are in qualia relation with the acquisition system ASARES (Claveau, 2003a). These pairs are then used to expand the queries of a retrieval system. With the help of the Amaryllis evaluation campaign data, we show that these expansions actually lead to better results, especially for the first documents proposed to the user.</abstract>
			<keywords>Semantic lexicon, corpus-based acquisition, information retrieval, Generative lexicon, query expansion</keywords>
		</article>
		<article id="taln-2004-long-005" session="Session orale 1B">
			<auteurs>
				<auteur>
					<nom>Olivier Ferret</nom>
					<email>ferreto@zoe.cea.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">CEA – LIST/LIC2M, 92265 Fontenay-aux-Roses Cedex</affiliation>
			</affiliations>
			<titre>Découvrir des sens de mots à partir d’un réseau de cooccurrences lexicales</titre>
			<type>long</type>
			<pages></pages>
			<resume>Les réseaux lexico-sémantiques de type WordNet ont fait l’objet de nombreuses critiques concernant la nature des sens qu’ils distinguent ainsi que la façon dont ils caractérisent ces distinctions de sens. Cet article présente une solution possible à ces limites, solution consistant à définir les sens des mots à partir de leur usage. Plus précisément, il propose de différencier les sens d’un mot à partir d’un réseau de cooccurrences lexicales construit sur la base d’un large corpus. Cette méthode a été testée à la fois pour le français et pour l’anglais et a fait l’objet dans ce dernier cas d’une première évaluation par comparaison avec WordNet.</resume>
			<mots_cles>Sémantique lexicale, découverte du sens des mots, réseaux lexico-sémantiques</mots_cles>
			<title></title>
			<abstract>Lexico-semantic networks such as WordNet have been criticized a lot on the nature of the senses they distinguish as well as on the way they define these senses. In this article, we present a possible solution to overcome these limits by defining the sense of words from the way they are used. More precisely, we propose to differentiate the senses of a word from a network of lexical cooccurrences built from a large corpus. This method was tested both for French and English and for English, was evaluated through a comparison with WordNet.</abstract>
			<keywords>Lexical semantics, word sense discovery, lexico-semantic networks</keywords>
		</article>
		<article id="taln-2004-long-006" session="Session orale 1B">
			<auteurs>
				<auteur>
					<nom>Bruno Gaume</nom>
					<email>gaume@irit.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Nabil Hathout</nom>
					<email>hathout@univ-tlse2.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Philippe Muller</nom>
					<email>muller@irit.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">IRIT – CNRS, UPS &amp; INPT</affiliation>
				<affiliation affiliationId="2">ERSS – CNRS &amp; UTM</affiliation>
			</affiliations>
			<titre>Désambiguïsation par proximité structurelle</titre>
			<type>long</type>
			<pages></pages>
			<resume>L’article présente une méthode de désambiguïsation dans laquelle le sens est déterminé en utilisant un dictionnaire. La méthode est basée sur un algorithme qui calcule une distance « sémantique » entre les mots du dictionnaire en prenant en compte la topologie complète du dictionnaire, vu comme un graphe sur ses entrées. Nous l’avons testée sur la désambiguïsation des définitions du dictionnaire elles-mêmes. L’article présente des résultats préliminaires, qui sont très encourageants pour une méthode ne nécessitant pas de corpus annoté.</resume>
			<mots_cles>Désambiguïsation sémantique, réseaux petits mondes hiérarchiques, dictionnaires</mots_cles>
			<title></title>
			<abstract>This paper presents a disambiguation method in which word senses are determined using a dictionary. We use a semantic proximity measure between words in the dictionary, taking into account the whole topology of the dictionary, seen as a graph on its entries. We have tested the method on the problem of disambiguation of the dictionary entries themselves, with promising results considering we do not use any prior annotated data.</abstract>
			<keywords>Word sense desambiguation, hierarchical small words, dictionaries</keywords>
		</article>
		<article id="taln-2004-long-007" session="Session orale 2A">
			<auteurs>
				<auteur>
					<nom>Francis Brunet-Manquat</nom>
					<email>Francis.Brunet-Manquat@imag.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">GETA-CLIPS – Université Joseph Fourier (Grenoble 1), BP 53 – 38041 Grenoble Cedex 9, France</affiliation>
			</affiliations>
			<titre>Fusionner pour mieux analyser : Conception et évaluation de la plate-forme de combinaison</titre>
			<type>long</type>
			<pages></pages>
			<resume>L’objectif de cet article est de présenter nos travaux concernant la combinaison d’analyseurs syntaxiques pour produire un analyseur plus robuste. Nous avons créé une plate-forme nous permettant de comparer des analyseurs syntaxiques pour une langue donnée en découpant leurs résultats en informations élémentaires, en les normalisant, et en les comparant aux résultats de référence. Cette même plate-forme est utilisée pour combiner plusieurs analyseurs pour produire un analyseur de dépendance plus couvrant et plus robuste. À long terme, il sera possible de “compiler” les connaissances extraites de plusieurs analyseurs dans un analyseur de dépendance autonome.</resume>
			<mots_cles>Analyse de dépendance, analyse syntaxique, combinaisons d’informations</mots_cles>
			<title></title>
			<abstract>The goal of this article is to present our works about the combination of syntactic parsers to produce a more robust parser. We have built a platform which allows us to compare syntactic parsers for a given language by splitting their results in elementary pieces, normalizing them, and comparing them with reference results. The same platform is used to combine several parsers to produce a dependency parser, which is big construction broader and more robust than its component parsers. In the future, it should by possible to “compile” the knowledge extracted from several analyzers into an autonomous dependency parser.</abstract>
			<keywords>Dependency parsing, syntactic parsing, Information combination</keywords>
		</article>
		<article id="taln-2004-long-008" session="Session orale 2A">
			<auteurs>
				<auteur>
					<nom>Daniela Dudau Sofronie</nom>
					<email>dudau@grappa.univ-lille3.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Isabelle Tellier</nom>
					<email>tellier@univ-lille3.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Grappa - Université Lille 3 &amp; INRIA Futurs, France</affiliation>
			</affiliations>
			<titre>Un modèle d’acquisition de la syntaxe à l’aide d’informations sémantiques</titre>
			<type>long</type>
			<pages></pages>
			<resume>Nous présentons dans cet article un algorithme d’apprentissage syntaxico-sémantique du langage naturel. Les données de départ sont des phrases correctes d’une langue donnée, enrichies d’informations sémantiques. Le résultat est l’ensemble des grammaires formelles satisfaisant certaines conditions et compatibles avec ces données. La stratégie employée, validée d’un point de vue théorique, est testée sur un corpus de textes français constitué pour l’occasion.</resume>
			<mots_cles>Grammaires catégorielles, types sémantiques, apprentissage syntaxico-sémantique</mots_cles>
			<title></title>
			<abstract>This paper presents a syntactico-semantic learning algorithm for natural languages. Input data are syntactically correct sentences of a given natural language, enriched with semantic information. The output is the set of compatible formal grammars satisfying certain conditions. The strategy used, which has been proved theoretically valid, is tested on a corpus of French texts built for this purpose.</abstract>
			<keywords>Categorial grammars, semantic types, syntactico-semantic learning</keywords>
		</article>
		<article id="taln-2004-long-009" session="Session orale 2A">
			<auteurs>
				<auteur>
					<nom>Jean-Jacques Mariage</nom>
					<email>jam@ai.univ-paris8.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Gilles Bernard</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Groupe CSAR, Laboratoire d'Intelligence Artificielle – Université Paris 8, 2, rue de la Liberté, 93526 St Denis Cdx, France</affiliation>
			</affiliations>
			<titre>Catégorisation de patrons syntaxiques par Self Organizing Maps</titre>
			<type>long</type>
			<pages></pages>
			<resume>Dans cet article, nous présentons quelques résultats en catégorisation automatique de données du langage naturel sans recours à des connaissances préalables. Le système part d’une liste de formes grammaticales françaises et en construit un graphe qui représente les chaînes rencontrées dans un corpus de textes de taille raisonnable ; les liens sont pondérés à partir de données statistiques extraites du corpus. Pour chaque chaîne de formes grammaticales significative, un vecteur reflétant sa distribution est extrait et passé à un réseau de neurones de type carte topologique auto-organisatrice. Une fois le processus d’apprentissage terminé, la carte résultante est convertie en un graphe d’étiquettes générées automatiquement, utilisé dans un tagger ou un analyseur de bas niveau. L’algorithme est aisément adaptable à toute langue dans la mesure où il ne nécessite qu’une liste de marques grammaticales et un corpus important (plus il est gros, mieux c’est). Il présente en outre un intérêt supplémentaire qui est son caractère dynamique : il est extrêmement aisé de recalculer les données à mesure que le corpus augmente.</resume>
			<mots_cles>Langues naturelles, réseaux neuronaux, extraction de connaissances</mots_cles>
			<title></title>
			<abstract>The present paper presents some results in automatic categorization of natural language data without previous knowledge. The system starts with a list of French grammatical items, builds them into a graph that represents the strings encountered in a reasonable corpus of texts; the links are weighted based upon statistical data extracted from the corpus. For each significant string of grammatical items a vector reflecting its distribution is extracted, and fed into a Self- Organizing Map neural network. Once the learning process is achieved, the resulting map will be converted into a graph of automatically generated tags, used in a tagger or a shallow parser. The algorithm may easily be adapted to any language, as it needs only the list of grammatical markers and a large corpus (the bigger the better). Another point of interest is its dynamic character: it is easy to recompute the data as the corpus grows.</abstract>
			<keywords>Natural languages, neural networks, knowledge extraction</keywords>
		</article>
		<article id="taln-2004-long-010" session="Session orale 2A">
			<auteurs>
				<auteur>
					<nom>Alexis Nasr</nom>
					<email>alexis.nasr@linguist.jussieu.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Alexandra Volanschi</nom>
					<email>alexandra.volanschi@linguist.jussieu.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LATTICE-CNRS (UMR 8094), Université Paris 7</affiliation>
			</affiliations>
			<titre>Couplage d’un étiqueteur morpho-syntaxique et d’un analyseur partiel représentés sous la forme d’automates finis pondérés</titre>
			<type>long</type>
			<pages></pages>
			<resume>Cet article présente une manière d’intégrer un étiqueteur morpho-syntaxique et un analyseur partiel. Cette integration permet de corriger des erreurs effectuées par l’étiqueteur seul. L’étiqueteur et l’analyseur ont été réalisés sous la forme d’automates pondérés. Des résultats sur un corpus du français ont montré une dimintion du taux d’erreur de l’ordre de 12%.</resume>
			<mots_cles>Analyse morpho-syntaxique, analyse syntaxique partielle, automates finis pondérés</mots_cles>
			<title></title>
			<abstract>This paper presents a method of integrating a part-of-speech tagger and a chunker. This integration lead to the correction of a number of errors made by the tagger when used alone. Both tagger and chunker are implemented as weighted finite state machines. Experiments on a French corpus showed a decrease of the word error rate of about 12%.</abstract>
			<keywords>Part-of-speech tagging, chunking, weighted finite state machines</keywords>
		</article>
		<article id="taln-2004-long-011" session="Session orale 2B">
			<auteurs>
				<auteur>
					<nom>Hervé Blanchon</nom>
					<email>Hervé.Blanchon@imag.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Christian Boitet</nom>
					<email>Christian.Boitet@imag.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">GETA, CLIPS-IMAG, BP 53, 38041 Grenoble Cedex 9</affiliation>
			</affiliations>
			<titre>Deux premières étapes vers les documents auto-explicatifs</titre>
			<type>long</type>
			<pages></pages>
			<resume>Dans le cadre du projet LIDIA, nous avons montré que dans de nombreuses situations, la TA Fondée sur le Dialogue (TAFD) pour auteur monolingue peut offrir une meilleure solution en traduction multicible que les aides aux traducteurs, ou la traduction avec révision, même si des langages contrôlés sont utilisés. Nos premières expériences ont mis en évidence le besoin de conserver les « intentions de l’auteur » au moyen « d’annotations de désambiguïsation ». Ces annotations permettent de transformer le document source en un Document Auto-Explicatif (DAE). Nous présentons ici une solution pour intégrer ces annotations dans un document XML et les rendre visibles et utilisables par un lecteur pour une meilleure compréhension du « vrai contenu » du document. Le concept de Document Auto-Explicatif pourrait changer profondément notre façon de comprendre des documents importants ou écrits dans un style complexe. Nous montrerons aussi qu’un DAE, traduit dans une langue cible L, pourrait aussi être transformé, sans interaction humaine, en un DAE en langue L si un analyseur et un désambiguïseur sont disponibles pour cette langue L. Ainsi, un DAE pourrait être utilisé dans un contexte monolingue, mais aussi dans un contexte multilingue sans travail humain additionnel.</resume>
			<mots_cles>Document Auto-Explicatifs (DAE), désambiguïsation interactive, documents actifs</mots_cles>
			<title></title>
			<abstract>In the LIDIA project, we have demonstrated that, in many situations, Dialogue-Based MT (DBMT) for monolingual author is likely to offer better solutions to multitarget translation needs than machine aids to translators or batch MT, even if controlled languages are used. First experiments have shown the need to keep a memory of the “author’s intention” by means of “disambiguating annotations” transforming the source document into a “selfexplaining document” (SED). We present ways to integrate these annotations into an XML document (SED-XML), and to make them visible and usable by readers for better understanding of the “true content” of a document. The very concept of SED might deeply change our way of understanding important or difficult written material. We also show that a SED, once translated into a target language L, might be transformed into an SED in L with no human interaction, if an analyzer and a disambiguator are available for L. Hence, the SED structure might be used in multilingual as well as in monolingual contexts, without addition of human work.</abstract>
			<keywords>Self-Explaining Document (SED), interactive disambiguation, active documents</keywords>
		</article>
		<article id="taln-2004-long-012" session="Session orale 2B">
			<auteurs>
				<auteur>
					<nom>Georges Fafiotte</nom>
					<email>georges.fafiotte@imag.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">GETA, CLIPS-IMAG (UJF - Université Grenoble 1), 385, rue de la Bibliothèque, BP 53, F-38041 GRENOBLE Cedex 9 (France)</affiliation>
			</affiliations>
			<titre>Interprétariat à distance et collecte de dialogues spontanés bilingues, sur une plate-forme générique multifonctionnelle</titre>
			<type>long</type>
			<pages></pages>
			<resume>Parallèlement à l’intégration du français en TA de Parole multilingue (projets C-STAR, NESPOLE!), nous avons développé plusieurs plates-formes, dans le cadre des projets ERIM (Environnement Réseau pour l’Interprétariat Multimodal) et ChinFaDial (collecte de dialogues parlés spontanés français-chinois), pour traiter différents aspects de la communication orale spontanée bilingue non finalisée sur le web : interprétariat humain à distance, collecte de données, intégration d’aides automatiques (serveur de TA de Parole utilisant des composants du marché, interaction multimodale entre interlocuteurs, et prochainement aides en ligne aux intervenants, locuteurs ou interprètes). Les corpus collectés devraient être disponibles sur un site DistribDial au printemps 2004. Ces plates-formes sont en cours d’intégration, en un système générique multifonctionnel unique ERIMM d’aide à la communication multilingue multimodale, dont une variante s’étendra également à la formation à distance (e-training) à l’interprétariat.</resume>
			<mots_cles>Interprétariat à distance sur réseau, collecte de corpus oraux bilingues, dialogues spontanés, communication multilingue, mutualisation de ressources</mots_cles>
			<title></title>
			<abstract>In parallel with integrating the French language into multilingual Speech Machine Translation (within the C-STAR and NESPOLE! projects), we have developed in recent years several platforms, in the framework of projects ERIM (Network-based Environment for Multimodal Interpreting) and ChinFaDial (collecting French-Chinese spontaneously spoken dialogues), allowing to handle various aspects of spontaneous, general-purpose bilingual spoken dialogues on the web: distant human interpreting, data collection, integration of machine aids including server-based speech translation based on commercial products, multimodal user interaction, and next, online aids to speakers and/or interpreters. Collected data should be available on the web (DistribDial) in spring 2004. All platforms are being integrated into one single multifunctional ERIMM generic system, which should then be extended to distant e-training in interpreting.</abstract>
			<keywords>Web-based interpreting, bilingual spoken corpora collection, spontaneous dialogues, multilingual communication, resource mutualization</keywords>
		</article>
		<article id="taln-2004-long-013" session="Session orale 2B">
			<auteurs>
				<auteur>
					<nom>Emmanuel Morin</nom>
					<email>morin@lina.univ-nantes.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Samuel Dufour-Kowalski</nom>
					<email>dufour@lina.univ-nantes.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Béatrice Daille</nom>
					<email>daille@lina.univ-nantes.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université de Nantes - LINA - FRE CNRS 2729, 2, rue de la Houssinière - BP 92208, 44322 Nantes Cedex 3, France</affiliation>
			</affiliations>
			<titre>Extraction de terminologies bilingues à partir de corpus comparables</titre>
			<type>long</type>
			<pages></pages>
			<resume>Cet article présente une méthode pour extraire, à partir de corpus comparables d’un domaine de spécialité, un lexique bilingue comportant des termes simples et complexes. Cette méthode extrait d’abord les termes complexes dans chaque langue, puis les aligne à l’aide de méthodes statistiques exploitant le contexte des termes. Après avoir rappelé les difficultés que pose l’alignement des termes complexes et précisé notre approche, nous présentons le processus d’extraction de terminologies bilingues adopté et les ressources utilisées pour nos expérimentations. Enfin, nous évaluons notre approche et démontrons son intérêt en particulier pour l’alignement de termes complexes non compositionnels.</resume>
			<mots_cles>Terminologie bilingue, corpus comparable, termes complexes</mots_cles>
			<title></title>
			<abstract>This article presents a method of extracting bilingual lexica composed of simple and multi-word terms from comparable corpora of a technical domain. First, this method extracts the multiword terms in each language, and then uses statistical methods to align them by exploiting the term contexts. After explaining the difficulties involved in aligning multi-word terms and specifying our approach, we show the adopted process for bilingual terminology extraction and the resources used in our experiments. Finally, we evaluate our approach and demonstrate its significance, particularly in relation to non-compositional multi-word term alignment.</abstract>
			<keywords>Bilingual terminology, comparable corpora, multi-word terms</keywords>
		</article>
		<article id="taln-2004-long-014" session="Session orale 2B">
			<auteurs>
				<auteur>
					<nom>Eric Wehrli</nom>
					<email>Eric.Wehrli@lettres.unige.ch</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LATL - Département de linguistique, Université de Genève, CH-1211 Genève 4</affiliation>
			</affiliations>
			<titre>Traduction, traduction de mots, traduction de phrases</titre>
			<type>long</type>
			<pages></pages>
			<resume>Une des conséquences du développement d’Internet et de la globalisation des échanges est le nombre considérable d’individus amenés à consulter des documents en ligne dans une langue autre que la leur. Après avoir montré que ni la traduction automatique, ni les aides terminologiques en ligne ne constituent une réponse pleinement adéquate à ce nouveau besoin, cet article présente un système d’aide à la lecture en langue étrangère basé sur un analyseur syntaxique puissant. Pour un mot sélectionné par l’usager, ce système analyse la phrase entière, de manière (i) à choisir la lecture du mot sélectionné la mieux adaptée au contexte morphosyntaxique et (ii) à identifier une éventuelle expression idiomatique ou une collocation dont le mot serait un élément. Une démonstration de ce système, baptisé TWiC (Translation of words in context "Traduction de mots en contexte"), pourra être présentée.</resume>
			<mots_cles>Traduction automatique, aide à la traduction, analyse syntaxique, collocations</mots_cles>
			<title></title>
			<abstract>As a consequence of globalisation and the development of the Internet, an increasing number of people are struggling with on-line documents in languages other than their own. In this paper, we will first argue that neither machine translation nor existing on-line terminology tools constitute an adequate answer to this problem, and then present a new system conceived as a foreign-language reading assistant, based on a powerful syntactic parser. Given a word selected by the user, this system carries out a syntactic analysis of the whole sentence in order (i) to select the most appropriate reading of the selected word, given the morpho-syntactic context, and (ii) to identify a possible idiom or collocation the selected word might be part of. A demo of this system, dubbed TWiC (Translation of words in context) will be available.</abstract>
			<keywords>Machine translation, Translation aids, syntactic parsing, collocations</keywords>
		</article>
		<article id="taln-2004-long-015" session="Session orale 3A">
			<auteurs>
				<auteur>
					<nom>Caroline Brun</nom>
					<email>Caroline.Brun@xrce.xerox.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Caroline Hagège</nom>
					<email>Caroline.Hagege@xrce.xerox.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Xerox Research Centre Europe, 6, chemin de Maupertuis, 38240 Meylan France</affiliation>
			</affiliations>
			<titre>Extraction d’information en domaine restreint pour la génération multilingue de résumés ciblés</titre>
			<type>long</type>
			<pages></pages>
			<resume>Dans cet article nous présentons une application de génération de résumés multilingues ciblés à partir de textes d’un domaine restreint. Ces résumés sont dits ciblés car ils sont produits d’après les spécifications d’un utilisateur qui doit décider a priori du type de l’information qu’il souhaite voir apparaître dans le résumé final. Pour mener à bien cette tâche, nous effectuons dans un premier temps l’extraction de l’information spécifiée par l’utilisateur. Cette information constitue l’entrée d’un système de génération multilingue qui produira des résumés normalisés en trois langues (anglais, français et espagnol) à partir d’un texte en anglais.</resume>
			<mots_cles>Résumé multilingue ciblé, extraction d’information, génération multilingue</mots_cles>
			<title></title>
			<abstract>We present an application of oriented multilingual summarization from domain specific texts. These summaries are oriented because the user has to define in a first step the kind of information he/she wants to be present in the final summary. In order to acheive this task, a first step of information extraction is performed. This extracted information which corresponds to the user’s specification is then the input of a multilingual generator that produces the desired summaries in three languages (english, french and spanish) from an english input text.</abstract>
			<keywords>Multilingual oriented summaries, information extraction, multilingual generation</keywords>
		</article>
		<article id="taln-2004-long-016" session="Session orale 3A">
			<auteurs>
				<auteur>
					<nom>Véronique Malaisé</nom>
					<email>vmalaise@ina.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Pierre Zweigenbaum</nom>
					<email>pz@biomath.jussieu.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Bruno Bachimont</nom>
					<email>bbachimont@ina.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">DRE de l’Institut National de l’Audiovisuel, 4, avenue de l’Europe, 94366 Bry-sur-Marne Cedex</affiliation>
				<affiliation affiliationId="2">STIM/AP-HP, ERM 202 INSERM &amp; CRIM-INaLCO, 91, boulevard de l’Hôpital, 75013 Paris</affiliation>
			</affiliations>
			<titre>Repérage et exploitation d’énoncés définitoires en corpus pour l’aide à la construction d’ontologie</titre>
			<type>long</type>
			<pages></pages>
			<resume>Pour construire une ontologie, un modéliseur a besoin d’objecter des informations sémantiques sur les termes principaux de son domaine d’étude. Les outils d’exploration de corpus peuvent aider à repérer ces types d’information, et l’identification de couples d’hyperonymes a fait l’objet de plusieurs travaux. Nous proposons d’exploiter des énoncés définitoires pour extraire d’un corpus des informations concernant les trois axes de l’ossature ontologique : l’axe vertical, lié à l’hyperonymie, l’axe horizontal, lié à la co-hyponymie et l’axe transversal, lié aux relations du domaine. Après un rappel des travaux existants en repérage d’énoncés définitoires en TAL, nous développons la méthode que nous avons mise en place, puis nous présentons son évaluation et les premiers résultats obtenus. Leur repérage atteint de 10% à 69% de précision suivant les patrons, celui des unités lexicales varie de 31% à 56%, suivant le référentiel adopté.</resume>
			<mots_cles>Repérage d’énoncés définitoires, relations sémantiques, patrons lexico-syntaxiques</mots_cles>
			<title></title>
			<abstract>In order to build an ontology, a modeler needs to objectivate semantic information about the main terms of his domain. Some tools meant to explore corpora can help pointing out this information, and previous work has focused on the identification of hyperonyms. We propose here to rely on lay definitions to extract the information necessary to build an ontology structure: the vertical axis, related to hypernymy, the horizontal axis, related to co-hyponymy, and the transversal axis, linked to domain-related cross relations. After a survey of previous work about the extraction of definitions in NLP, we develop the method we followed, then present its evaluation criteria and the first results. The mining of lay definitions reached from 10 to 69% of precision, depending on the pattern involved, the mining of lexical items varied from 31 to 56%, following the reference considered.</abstract>
			<keywords>Mining definitions, semantic relations, lexico-syntactic pattern</keywords>
		</article>
		<article id="taln-2004-long-017" session="Session orale 3A">
			<auteurs>
				<auteur>
					<nom>Luc Plamondon</nom>
					<email>plamondl@iro.umontreal.ca</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Guy Lapalme</nom>
					<email>lapalme@iro.umontreal.ca</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Frédéric Pelletier</nom>
					<email>pelletif@lexum.umontreal.ca</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">RALI - Université de Montréal, Montréal, Québec, Canada</affiliation>
				<affiliation affiliationId="2">CRDP - Université de Montréal, Montréal, Québec, Canada</affiliation>
			</affiliations>
			<titre>Anonymisation de décisions de justice</titre>
			<type>long</type>
			<pages></pages>
			<resume>La publication de décisions de justice sur le Web permet de rendre la jurisprudence accessible au grand public, mais il existe des domaines du droit pour lesquels la Loi prévoit que l’identité de certaines personnes doit demeurer confidentielle. Nous développons actuellement un système d’anonymisation automatique à l’aide de l’environnement de développement GATE. Le système doit reconnaître certaines entités nommées comme les noms de personne, les lieux et les noms d’entreprise, puis déterminer automatiquement celles qui sont de nature à permettre l’identification des personnes visées par les restrictions légales à la publication.</resume>
			<mots_cles>Anonymisation, désidentification, reconnaissance d’entités nommées, textes juridiques</mots_cles>
			<title></title>
			<abstract>Publishing court decisions on theWeb can make case law available to the general public, but the Law sometimes prohibits the disclosure of the identity of people named in decisions. We are currently developing an automatic anonymization system, using the GATE development environment. The tasks of the system are the recognition of some named entities like person names, locations and company names, then the automatic selection of the ones that may lead to the identification of people whose identities must be legally kept confidential.</abstract>
			<keywords>Anonymization, de-identification, named entity recognition, law texts</keywords>
		</article>
		<article id="taln-2004-long-018" session="Session orale 3B">
			<auteurs>
				<auteur>
					<nom>Gaëlle Lortal</nom>
					<email>gaelle.lortal@utt.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Brigitte Grau</nom>
					<email>brigitte.grau@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Michael Zock</nom>
					<email>zock@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIMSI-CNRS – Université Paris XI, BP 133 91403 Orsay</affiliation>
			</affiliations>
			<titre>Système d’aide à l’accès lexical : trouver le mot qu’on a sur le bout de la langue</titre>
			<type>long</type>
			<pages></pages>
			<resume>Le Mot sur le Bout de la Langue (Tip Of the Tongue en anglais), phénomène très étudié par les psycholinguistes, nous a amené nombre d’informations concernant l’organisation du lexique mental. Un locuteur en état de TOT reconnaît instantanément le mot recherché présenté dans une liste. Il en connaît le sens, la forme, les liens avec d’autres mots... Nous présentons ici une étude de développement d’outil qui prend en compte ces spécificités, pour assister un locuteur/rédacteur à trouver le mot qu’il a sur le bout de la langue. Elle consiste à recréer le phénomène du TOT, où, dans un contexte de production un mot, connu par le système, est momentanément inaccessible. L’accès au mot se fait progressivement grâce aux informations provenant de bases de données linguistiques. Ces dernières sont essentiellement des relations de type paradigmatique et syntagmatique. Il s’avère qu’un outil, tel que SVETLAN, capable de structurer automatiquement un dictionnaire par domaine, peut être avantageusement combiné à une base de données riche en liens paradigmatiques comme EuroWordNet, augmentant considérablement les chances de trouver le mot auquel on ne peut accéder.</resume>
			<mots_cles>MBL, accès lexical, relations sémantiques, associations, SVETLAN, EWN</mots_cles>
			<title></title>
			<abstract>The study of the Tip of the Tongue phenomenon (TOT) provides valuable clues and insights concerning the organisation of the mental lexicon (meaning, number of syllables, relation with other words, etc.). This paper describes a tool based on psycho-linguistic observations concerning the TOT phenomenon. We’ve built it to enable a speaker/writer to find the word he is looking for, word he may know, but which he is unable to access in time. We try to simulate the TOT phenomenon by creating a situation where the system knows the target word, yet is unable to access it. In order to find the target word we make use of the paradigmatic and syntagmatic associations stored in the linguistic databases. Our experiment allows the following conclusion: a tool like SVETLAN, capable to structure (automatically) a dictionary by domains can be used sucessfully to help the speaker/writer to find the word he is looking for, if it is combined with a database rich in terms of paradigmatic links like EuroWordNet.</abstract>
			<keywords>TOT, word access, semantic relations, associations, SVETLAN, EWN</keywords>
		</article>
		<article id="taln-2004-long-019" session="Session orale 3B">
			<auteurs>
				<auteur>
					<nom>Fabrice Maurel</nom>
					<email>fmaurel@irit.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">IRIT – Université Paul Sabatier, 118, route de Narbonne 31062 Toulouse - France</affiliation>
			</affiliations>
			<titre>De l’écrit à l’oral : analyses et générations</titre>
			<type>long</type>
			<pages></pages>
			<resume>Longtemps considérée comme ornementale, la structure informationnelle des documents écrits prise en charge par la morpho-disposition devient un objet d’étude à part entière dans diverses disciplines telles que la linguistique, la psycholinguistique ou l’informatique. En particulier, nous nous intéressons à l’utilité de cette dimension et, le cas échéant, son utilisabilité, dans le cadre de la transposition automatique à l’oral des textes. Dans l’objectif de fournir des solutions qui permettent de réagir efficacement à cette « inscription morphologique », nous proposons la synoptique d’un système d’oralisation. Nous avons modélisé et partiellement réalisé le module spécifique aux stratégies d’oralisation, afin de rendre « articulables » certaines parties signifiantes des textes souvent « oubliées » par les systèmes de synthèse. Les premiers résultats de cette étude ont conduit à des spécifications en cours d’intégration par un partenaire industriel. Les perspectives de ce travail peuvent intéresser la communauté TAL en reconnaissance de la parole, en génération/résumé de texte ou en multimodalité.</resume>
			<mots_cles>Architecture textuelle, synthèse de la parole, stratégies d’oralisation</mots_cles>
			<title></title>
			<abstract>Considered for a long time as ornamental, the informational structure of written documents carried by texts morpho-disposition becomes a full object of investigation in various disciplines such as linguistic, psycholinguistic or computer sciences. In Particular, we are interested in the utility of these aspects of documents and, if the need arises, their usability, within the framework of their oral transposition. In the objective to provide solutions which make it possible to react effectively to this “morphological inscription”, we propose the synoptic of an oralisation system. We modelled and partially realized the module specific to the oralisation strategies, in order to render some signifying parts of the text often “forgotten” by synthesis systems. The first results of this study led to specifications in the course of integration by an industrial partner. The prospects of this work can interest NLP community in voice recognition, text generation/summarization or multimodality.</abstract>
			<keywords>Textual architecture, speech synthesis, oralisation strategies</keywords>
		</article>
		<article id="taln-2004-long-020" session="Session orale 3B">
			<auteurs>
				<auteur>
					<nom>Florentina Vasilescu</nom>
					<email>vasilesf@iro.umontreal.ca</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Philippe Langlais</nom>
					<email>felipe@iro.umontreal.ca</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">RALI/IRO, Université de Montréal, CP. 6128, succursale Centre-ville, Montréal, Québec, H3C CJ7 Canada</affiliation>
			</affiliations>
			<titre>Désambiguïsation de corpus monolingues par des approches de type Lesk</titre>
			<type>long</type>
			<pages></pages>
			<resume>Cet article présente une analyse détaillée des facteurs qui déterminent les performances des approches de désambiguïsation dérivées de la méthode de Lesk (1986). Notre étude porte sur une série d’expériences concernant la méthode originelle de Lesk et des variantes que nous avons adaptées aux caractéristiques de WORDNET. Les variantes implémentées ont été évaluées sur le corpus de test de SENSEVAL2, English All Words, ainsi que sur des extraits du corpus SEMCOR. Notre évaluation se base d’un côté, sur le calcul de la précision et du rappel, selon le modèle de SENSEVAL, et d’un autre côté, sur une taxonomie des réponses qui permet de mesurer la prise de risque d’un décideur par rapport à un système de référence.</resume>
			<mots_cles>Désambiguïsation sémantique, algorithme de Lesk, naive Bayes, WORDNET</mots_cles>
			<title></title>
			<abstract>This paper deals with a detailed analysis of the factors determining the performances of Leskbased WSD methods. Our study consists in a series of experiments on the original Lesk algorithm and on its variants that we adapted to WORDNET. These methods were evaluated on the test corpus from SENSEVAL2, English All Words, and on excerpts from SEMCOR. The evaluation metrics are based on precision and recall, as in SENSEVAL exercises, and on a new method estimating the risk taken by each variant.</abstract>
			<keywords>Word sense desambiguation, Lesk’s algorithm, naive Bayes, WORDNET</keywords>
		</article>
		<article id="taln-2004-long-021" session="Session orale 4A">
			<auteurs>
				<auteur>
					<nom>Philippe Blache</nom>
					<email>pb@lpl.univ-aix.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Laboratoire Parole et Langage, CNRS – Université de Provence, 29, Avenue Robert Schuman, 13621 Aix-en-Provence</affiliation>
			</affiliations>
			<titre>Densité d'information syntaxique et gradient de grammaticalité</titre>
			<type>long</type>
			<pages></pages>
			<resume>Cet article propose l'introduction d'une notion de densité syntaxique permettant de caractériser la complexité d'un énoncé et au-delà d'introduire la spécification d'un gradient de grammaticalité. Un tel gradient s'avère utile dans plusieurs cas : quantification de la difficulté d'interprétation d'une phrase, gradation de la quantité d'information syntaxique contenue dans un énoncé, explication de la variabilité et la dépendances entre les domaines linguistiques, etc. Cette notion exploite la possibilité de caractérisation fine de l'information syntaxique en termes de contraintes : la densité est fonction des contraintes satisfaites par une réalisation pour une grammaire donnée. Les résultats de l'application de cette notion à quelques corpus sont analysés.</resume>
			<mots_cles>Syntaxe, analyse, robustesse, contraintes, information linguistique, complexité syntaxique</mots_cles>
			<title></title>
			<abstract>This paper introduces the notion of syntactic density that makes it possible to characterize the complexity of an utterance and to specify a gradient of grammaticality. Such a gradient is useful in several cases: quantification of the difficulty of interpreting an utterance, quantification of syntactic information of an utterance, description of variability and linguistic domains interaction, etc. This notion exploits the possibility of fine syntactic characterization in terms of constraints: density if function of satisfied constraints by an utterance for a given grammar. Some results are presented and analyzed.</abstract>
			<keywords>Syntax, parsing, robustness, constraints, linguistic information, syntactic complexity</keywords>
		</article>
		<article id="taln-2004-long-022" session="Session orale 4A">
			<auteurs>
				<auteur>
					<nom>Mathieu Estratat</nom>
					<email>mathieu.estratat@lsis.org</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Laurent Henocque</nom>
					<email>henocque@esil.univ-mrs.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LSIS, Université d’Aix-Marseille III, Avenue Escadrille Normandie-Niemen, 13397 Marseille cedex 20</affiliation>
			</affiliations>
			<titre>Application des programmes de contraintes orientés objet à l’analyse du langage naturel</titre>
			<type>long</type>
			<pages></pages>
			<resume>Les évolutions récentes des formalismes et théories linguistiques font largement appel au concept de contrainte. De plus, les caractéristiques générales des grammaires de traits ont conduit plusieurs auteurs à pointer la ressemblance existant entre ces notions et les objets ou frames. Une évolution récente de la programmation par contraintes vers les programmes de contraintes orientés objet (OOCP) possède une application possible au traitement des langages naturels. Nous proposons une traduction systématique des concepts et contraintes décrits par les grammaires de propriétés sous forme d’un OOCP. Nous détaillons l’application de cette traduction au langage "context free" archétypal anbn, en montrant que cette approche permet aussi bien l’analyse que la génération de phrases, de prendre en compte la sémantique au sein du même modèle et ne requiert pas l’utilisation d’algorithmes ad hoc pour le parsage.</resume>
			<mots_cles>Grammaires de propriétés, traitement du langage naturel, contraintes, configuration</mots_cles>
			<title></title>
			<abstract>Recent evolutions of linguistic theories heavily rely upon the concept of constraint. Also, several authors have pointed the similitude existing between the categories of feature based theories and the notions of objects or frames. A recent evolution of constraint programming to object oriented constraint programs (OOCP) can be applied to natural language parsing. We propose here a systematic translation of the concepts and constraints introduced by property grammars to an OOCP. We apply this translation to the archetypal context free language anbn, and show that this approach allows to both parse and generate, to account for the semantics in the same formalism, and also that it does not require the use of ad hoc algorithms.</abstract>
			<keywords>Property grammars, natural language processing, constraints, configuration</keywords>
		</article>
		<article id="taln-2004-long-023" session="Session orale 4A">
			<auteurs>
				<auteur>
					<nom>Sylvain Kahane</nom>
					<email>sk@ccr.jussieu.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Lattice, Université Paris 7, Modyco, Université Paris 10</affiliation>
			</affiliations>
			<titre>Grammaires d'unification polarisées</titre>
			<type>long</type>
			<pages></pages>
			<resume>Cet article propose un formalisme mathématique générique pour la combinaison de structures. Le contrôle de la saturation des structures finales est réalisé par une polarisation des objets des structures élémentaires. Ce formalisme permet de mettre en évidence et de formaliser les mécanismes procéduraux masqués de nombreux formalismes, dont les grammaires de réécriture, les grammaires de dépendance, TAG, HPSG et LFG.</resume>
			<mots_cles>Grammaire formelle, unification, polarisation, grammaire de réécriture, grammaire de dépendance, TAG, HPSG, LFG, graphe, arbre, dag</mots_cles>
			<title></title>
			<abstract>This paper proposes a generic mathematical formalism for the combination of structures. The control of saturation of the final structures is realized by a polarization of the objects of the elementary structures. This formalism allows us to bring to the fore and to formalize the hidden procedural mechanisms of numerous formalisms, including rewriting systems, dependency grammars, TAG, HPSG and LFG.</abstract>
			<keywords>Formal grammar, unification, polarization, rewriting system, dependency grammar, TAG, HPSG, LFG, graph, tree, dag</keywords>
		</article>
		<article id="taln-2004-long-024" session="Session orale 4A">
			<auteurs>
				<auteur>
					<nom>Laura Kallmeyer</nom>
					<email>laura.kallmeyer@linguist.jussieu.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>SinWon Yoon</nom>
					<email>swyoong@linguist.jussieu.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">UFRL, University Paris 7, 2 place Jussieu, Case 7003, 75251 Paris Cedex 05</affiliation>
			</affiliations>
			<titre></titre>
			<type>long</type>
			<pages></pages>
			<resume>Les Grammaires d’Arbres Adjoints (TAG) sont connues pour ne pas être assez puissantes pour traiter le brouillage d’arguments dans des langues à ordre des mots libre. Les variantes TAG proposées jusqu’à maintenant pour expliquer le brouillage ne sont pas entièrement satisfaisantes. Nous présentons ici une extension alternative de TAG, basée sur la notion du partage de noeuds. En considerant des données de l’allemand et du coréen, on montre que cette extension de TAG peut en juste proportion analyser des données de brouillage d’arguments, également en combinaison avec l’extraposition et la topicalisation.</resume>
			<mots_cles>Grammaires d’Arbres Adjoints, brouillage d’arguments, ordre des mots, allemand, coréen</mots_cles>
			<title>Tree-local MCTAG with Shared Nodes: An Analysis ofWord Order Variation in German and Korean</title>
			<abstract>Tree Adjoining Grammars (TAG) are known not to be powerful enough to deal with scrambling in free word order languages. The TAG-variants proposed so far in order to account for scrambling are not entirely satisfying. Therefore, an alternative extension of TAG is introduced based on the notion of node sharing. Considering data from German and Korean, it is shown that this TAG-extension can adequately analyse scrambling data, also in combination with extraposition and topicalization.</abstract>
			<keywords>Tree Adjoining Grammars, scrambling, word order, German, Korean</keywords>
		</article>
		<article id="taln-2004-long-025" session="Session orale 4B">
			<auteurs>
				<auteur>
					<nom>Touria Ait El Mekki</nom>
					<email>taem@lipn.univ-paris13.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Adeline Nazarenko</nom>
					<email>nazarenko@lipn.univ-paris13.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Laboratoire d’Informatique de Paris-Nord, 99 Avenue J.-B. Clément, 93430 Villetaneuse FRANCE</affiliation>
			</affiliations>
			<titre>Une mesure de pertinence pour le tri de l’information dans un index de “fin de livre”</titre>
			<type>long</type>
			<pages></pages>
			<resume>Nous nous intéressons à la construction des index de fin de livres. Nous avons développé le système IndDoc qui aide la construction de tels index. L’un des enjeux de la construction d’index est la sélection des informations : sélection des entrées les plus pertinentes et des renvois au texte les plus intéressants. Cette sélection est évidemment utile pour le lecteur qui doit trouver suffisamment d’information mais sans en être submergé. Elle est également précieuse pour l’auteur de l’index qui doit valider et corriger une ébauche d’index produite automatiquement par IndDoc. Nous montrons comment cette sélection de l’information est réalisée par IndDoc. Nous proposons une mesure qui permet de trier les entrées par ordre de pertinence décroissante et une méthode pour calculer les renvois au texte à associer à chaque entrée de l’index.</resume>
			<mots_cles>Segmentation thématique de texte, extraction d’information, indexation automatique</mots_cles>
			<title></title>
			<abstract>This paper deals with the construction of end-of-book indexes. We have developed the IndDoc system which assists the construction of such indexes. One of the stakes of the construction of an index is the information selection: selection of the most relevant entries and the most interesting textual fragments. This selection is obviously useful for the reader who is looking for information. It is also invaluable for the index author who has to validate and correct an outline of index produced automatically by IndDoc. We show how this information selection is carried out by IndDoc. We put forward a measure which sorts the entries in decreasing relevance order and a method to calculate the references to text for each entry.</abstract>
			<keywords>Text segmentation, information extraction, automatic indexing</keywords>
		</article>
		<article id="taln-2004-long-026" session="Session orale 4B">
			<auteurs>
				<auteur>
					<nom>Narjès Boufaden</nom>
					<email>boufaden@iro.umontreal.ca</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Yoshua Bengio</nom>
					<email>bengioy@iro.umontreal.ca</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Guy Lapalme</nom>
					<email>lapalme@iro.umontreal.ca</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Laboratoire RALI - Université de Montréal, Québec, Canada</affiliation>
				<affiliation affiliationId="2">Laboratoire LISA - Université de Montréal, Québec, Canada</affiliation>
			</affiliations>
			<titre>Approche statistique pour le repérage de mots informatifs dans les textes oraux</titre>
			<type>long</type>
			<pages></pages>
			<resume>Nous présentons les résultats de l’approche statistique que nous avons développée pour le repérage de mots informatifs à partir de textes oraux. Ce travail fait partie d’un projet lancé par le département de la défense canadienne pour le développement d’un système d’extraction d’information dans le domaine de la Recherche et Sauvetage maritime (SAR). Il s’agit de trouver et annoter les mots pertinents avec des étiquettes sémantiques qui sont les concepts d’une ontologie du domaine (SAR). Notre méthode combine deux types d’information : les vecteurs de similarité générés grâce à l’ontologie du domaine et le dictionnaire-thésaurus Wordsmyth ; le contexte d’énonciation représenté par le thème. L’évaluation est effectuée en comparant la sortie du système avec les réponses de formulaires d’extraction d’information prédéfinis. Les résultats obtenus sur les textes oraux sont comparables à ceux obtenus dans le cadre de MUC7 pour des textes écrits.</resume>
			<mots_cles>Étiquetage sémantique, extraction d’information</mots_cles>
			<title></title>
			<abstract>We present results of a statistical method we developped for the detection of informative words from manually transcribed conversations. This work is part of an ongoing project for an information extraction system in the field of maritime Search And Rescue (SAR). Our purpose is to automatically detect relevant words and annotate them with concepts from a SAR ontology. Our approach combines similarity score vectors and topical information. Similarity vectors are generated using a SAR ontology and theWordsmyth dictionary-thesaurus. Evaluation is carried out by comparing the output of the system with key answers of predefined extraction templates. Results on speech transcriptions are comparable to those on written texts in MUC7.</abstract>
			<keywords>Semantic tagging, information extraction</keywords>
		</article>
		<article id="taln-2004-long-027" session="Session orale 4B">
			<auteurs>
				<auteur>
					<nom>Armelle Brun</nom>
					<email>brun@loria.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Kamel Smaïli</nom>
					<email>smaili@loria.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LORIA - Université Nancy2, Campus Scientifique - BP 239, 54506 VANDOEUVRE-lès-NANCY</affiliation>
			</affiliations>
			<titre>Fiabilité de la référence humaine dans la détection de thème</titre>
			<type>long</type>
			<pages></pages>
			<resume>Dans cet article, nous nous intéressons à la tâche de détection de thème dans le cadre de la reconnaissance automatique de la parole. La combinaison de plusieurs méthodes de détection montre ses limites, avec des performances de 93.1 %. Ces performances nous mènent à remetttre en cause le thème de référence des paragraphes de notre corpus. Nous avons ainsi effectué une étude sur la fiabilité de ces références, en utilisant notamment les mesures Kappa et erreur de Bayes. Nous avons ainsi pu montrer que les étiquettes thématiques des paragraphes du corpus de test comportaient vraisemblablement des erreurs, les performances de détection de thème obtenues doivent donc êtres exploitées prudemment.</resume>
			<mots_cles>Détection de thème, Etiquetage thématique, statistique Kappa, erreur de Bayes</mots_cles>
			<title></title>
			<abstract>In this paper, topic detection is studied in the frame of automatic speech recognition. Topic detection methods combination reaches 93.1% correct detection. This rate makes us throw the reference labeling back into question. We have then studied the reliability of the topic labeling of our test corpus, by using the Kappa statistics and the Bayes error. With these measures, we show the topic label of some paragraphs may be wrong, then performance of topic detection may be carefully exploited.</abstract>
			<keywords>Topic detection, topic assignment, Kappa statistics, Bayes error</keywords>
		</article>
		<article id="taln-2004-long-028" session="Session orale 4B">
			<auteurs>
				<auteur>
					<nom>Jean-Yve Antoine</nom>
					<email>Jean-Yves.Antoine@univ-ubs.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">VALORIA (EA 2593), Université de Bretagne Sud BP 573, F-56017 Vannes, France</affiliation>
			</affiliations>
			<titre>Résolution des anaphores pronominales : quelques postulats du TALN mis à l’épreuve du dialogue oral finalisé</titre>
			<type>long</type>
			<pages></pages>
			<resume>Cet article étudie l'adaptation au dialogue oral homme-machine des techniques de résolution des anaphores pronominales qui ont été développées par le TALN pour les documents écrits. A partir d'une étude de corpus de dialogue oral, il étudie la faisabilité de ce portage de l'écrit vers l'oral. Les résultats de cette étude montrent que certains indices utilisés à l'écrit (accord en nombre, distance entre le pronom est son antécédent) sont plus friables en dialogue oral finalisé. Les techniques développées pour l'écrit ne peuvent donc pas être réutilisées directement à l'oral.</resume>
			<mots_cles>Référence, anaphore pronominale, dialogue oral homme-machine, analyse des usages sur corpus</mots_cles>
			<title></title>
			<abstract>In this paper, we present a corpus analysis on pronominal anaphora that investigate the adaptation of anaphora resolution techniques which have been developed for written language processing and that should apply to spoken man-machine dialogue. Unfortunately, this corpus study shows that the criteria that are used on written texts (gender and number agreement, distance between the pronoun and its antecedent) seems to lack robustness on interactive spoken language.</abstract>
			<keywords>Reference, pronominal anaphora, spoken man-machine dialogue, corpus analysis</keywords>
		</article>
		<article id="taln-2004-long-029" session="Session orale 5A">
			<auteurs>
				<auteur>
					<nom>Philippe Muller</nom>
					<email>muller@irit.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Xavier Tannier</nom>
					<email>tannier@emse.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">IRIT-CNRS UMR 5505, Université Paul-Sabatier, 118 Rte de Narbonne, 31062 Toulouse Cedex 04</affiliation>
			</affiliations>
			<titre>Une méthode pour l’annotation de relations temporelles dans des textes et son évaluation</titre>
			<type>long</type>
			<pages></pages>
			<resume>Cet article traite de l’annotation automatique d’informations temporelles dans des textes et vise plus particulièrement les relations entre événements introduits par les verbes dans chaque clause. Si ce problème a mobilisé beaucoup de chercheurs sur le plan théorique, il reste en friche pour ce qui est de l’annotation automatique systématique (et son évaluation), même s’il existe des débuts de méthodologie pour faire réaliser la tâche par des humains. Nous proposons ici à la fois une méthode pour réaliser la tâche automatiquement et une manière de mesurer à quel degré l’objectif est atteint. Nous avons testé la faisabilité de ceci sur des dépêches d’agence avec des premiers résultats encourageants.</resume>
			<mots_cles>Annotation, Temps, Discours</mots_cles>
			<title></title>
			<abstract>This paper focuses on the automated processing of temporal information in written texts, more specifically on relations between events introduced by verbs in every clause. While this latter problem has been largely studied from a theoretical point of view, it has very rarely been applied to real texts, if ever, with quantified results. The methodology required is still to be defined, even though there have been proposals in the human annotation case. We propose here both a procedure to achieve this task and a way of measuring the results. We have been testing the feasability of this on neswire articles, with promising first results.</abstract>
			<keywords>Text annotation, Time, Discourse</keywords>
		</article>
		<article id="taln-2004-long-030" session="Session orale 5A">
			<auteurs>
				<auteur>
					<nom>Claude Roux</nom>
					<email>claude.roux@xrce.xerox.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Xerox Research Centre Europe, 6, chemin de Maupertuis, 38240 Meylan</affiliation>
			</affiliations>
			<titre>Annoter les documents XML avec un outil d’analyse syntaxique</titre>
			<type>long</type>
			<pages></pages>
			<resume>Cet article présente l’intégration au sein d’un analyseur syntaxique (Xerox Incremental Parser) de règles spécifiques qui permettent de lier l’analyse grammaticale à la sémantique des balises XML spécifiques à un document donné. Ces règles sont basées sur la norme XPath qui offre une très grande finesse de description et permet de guider très précisément l’application de l’analyseur sur une famille de documents partageant une même DTD. Le résultat est alors être intégré directement comme annotation dans le document traité.</resume>
			<mots_cles>XML, analyse syntaxique, traitement automatique des langues, traitement de documents, Xpath, XIP</mots_cles>
			<title></title>
			<abstract>This article presents the embedding within a syntactic parser (Xerox Incremental Parser or XIP) of specific rules which are used to bind the grammatical analysis to the semantic of the XML mark up tags specific to a given document. The goal of these rules is to guide the application of a natural language processing tool through the use of XPath instructions to describe documents that share the same DTD. The result can then be embedded within the input document in order to annotate that document.</abstract>
			<keywords>XML, parsing, natural language processing, document processing, XPath, XIP</keywords>
		</article>
		<article id="taln-2004-long-031" session="Session orale 5A">
			<auteurs>
				<auteur>
					<nom>Susanne Salmon-Alt</nom>
					<email>Susanne.Salmon-Alt@atilf.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Eckhard Bick</nom>
					<email>lineb@hum.au.dk</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Laurent Romary</nom>
					<email>Laurent.Romary@loria.fr</email>
					<affiliationId>3</affiliationId>
				</auteur>
				<auteur>
					<nom>Jean-Marie Pierrel</nom>
					<email>Jean-Marie.Pierrel@atilf.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">ATILF – CNRS (UMR 7118), 44, avenue de la Libération, B.P. 30687, 54063 Nancy, France</affiliation>
				<affiliation affiliationId="2">University of Southern Denmark, Campusvej 55, DK-5230 Odense, Denmark</affiliation>
				<affiliation affiliationId="3">LORIA – INRIA (UMR 7503), Campus Scientifique, B.P. 239, 54506 Vandoeuvre Lès Nancy, France</affiliation>
			</affiliations>
			<titre>La FREEBANK : vers une base libre de corpus annotés</titre>
			<type>long</type>
			<pages></pages>
			<resume>Les corpus français librement accessibles annotés à d’autres niveaux linguistiques que morpho-syntaxique sont insuffisants à la fois quantitativement et qualitativement. Partant de ce constat, la FREEBANK -- construite sur la base d’outils d’analyse automatique dont la sortie est révisée manuellement -- se veut une base de corpus du français annotés à plusieurs niveaux (structurel, morphologique, syntaxique, coréférentiel) et à différents degrés de finesse linguistique qui soit libre d’accès, codée selon des schémas normalisés, intégrant des ressources existantes et ouverte à l’enrichissement progressif.</resume>
			<mots_cles>ressources libres, annotation multiniveau, corpus arboré, codage référentiel, normalisation</mots_cles>
			<title></title>
			<abstract>The few available French resources for evaluating linguistic models or algorithms on other linguistic levels than morpho-syntax are either insufficient from quantitative as well as qualitative point of view or not freely accessible. Based on this fact, the FREEBANK project intends to create French corpora constructed using manually revised output from a hybrid Constraint Grammar parser and annotated on several linguistic levels (structure, morphosyntax, syntax, coreference), with the objective to make them available on-line for research purposes. Therefore, we will focus on using standard annotation schemes, integration of existing resources and maintenance allowing for continuous enrichment of the annotations.</abstract>
			<keywords>free resources, multi-level annotation, treebank, reference annotation, normalisation</keywords>
		</article>
		<article id="taln-2004-long-032" session="Session orale 5A">
			<auteurs>
				<auteur>
					<nom>Anne Vilnat</nom>
					<email>Anne.Vilnat@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Laura Monceaux</nom>
					<email>Laura.Monceaux@lina.univ-nantes.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Patrick Paroubek</nom>
					<email>Patrick.Paroubek@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Isabelle Robba</nom>
					<email>Isabelle.Robba@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Véronique Gendner</nom>
					<email>Véronique.Gendner@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Gabriel Illouz</nom>
					<email>Gabriel.Illouz@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Michèle Jardino</nom>
					<email>Michèle.Jardino@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIMSI - CNRS, Université d’Orsay, BP 133, 91403 Orsay CEDEX</affiliation>
				<affiliation affiliationId="2">LINA, 2 rue de la Houssinière, BP 92208, 44322 Nantes CEDEX 03</affiliation>
			</affiliations>
			<titre>Annoter en constituants pour évaluer des analyseurs syntaxiques</titre>
			<type>long</type>
			<pages></pages>
			<resume>Cet article présente l’annotation en constituants menée dans le cadre d’un protocole d’évaluation des analyseurs syntaxiques (mis au point dans le pré-projet PEAS, puis dans le projet EASY). Le choix des constituants est décrit en détail et une première évaluation effectuée à partir des résultats de deux analyseurs est donnée.</resume>
			<mots_cles>annotation en constituants, évaluation, analyseurs syntaxiques</mots_cles>
			<title></title>
			<abstract>This paper focuses on constituent annotation in a syntactic parsers evaluation protocol (which was elaborated in PEAS pre-project and EASY project). The choice of the constituents is described in details, and the results of a first evaluation between two parsers are given.</abstract>
			<keywords>constituent annotation, evaluation, syntactic parser</keywords>
		</article>
		<article id="taln-2004-long-033" session="Session orale 5B">
			<auteurs>
				<auteur>
					<nom>Adil El Ghali</nom>
					<email>adil@linguist.jussieu.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LATTICE / PPS - Université Paris 7, 2, place jussieu, case 7003 - 75251 Paris</affiliation>
			</affiliations>
			<titre>Détermination de contenu dans GEPHOX</titre>
			<type>long</type>
			<pages></pages>
			<resume>Le générateur GEPHOX que nous réalisons a pour ambition de produire des textes pour des définition ou preuves mathématiques écrites à l’aide de l’assistant de preuve PHOX. Dans cet article nous nous concentrons sur le module de détermination de contenu ContDet de GEPHOX. Après un aperçu sur l’entrée du générateur, i.e. la preuve formelle et l’ensemble des règles ayant permis de l’obtenir, nous décrivons les base de connaissances du générateur et le fonctionnement de l’algorithme de détermination de contenu.</resume>
			<mots_cles>Génération de textes, logique de description, détermination de contenu, bases de connaissance, assistant de preuve</mots_cles>
			<title></title>
			<abstract>This paper deals with content determination in a text proofs generation system. Our system, GEPHOX produces a textual version of a mathematical proof formalized using the proof assistant PHOX. We start with a quick presentation of the input of the generator : the formal proof and the set of rules that the proof assistant user employs in order to find it. We describe the generator knowledge bases and define the reasoning tasks associated with the KB and show how the content determination algorithm work.</abstract>
			<keywords>Natural language generation, description logic, content determination, knowledge bases, proof assistant</keywords>
		</article>
		<article id="taln-2004-long-034" session="Session orale 5B">
			<auteurs>
				<auteur>
					<nom>Erwan Moreau</nom>
					<email>Erwan.Moreau@irin.univ-nantes.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LINA - Université de Nantes, 2 rue de la Houssinière - BP 92208 - 44322 Nantes cedex 3</affiliation>
			</affiliations>
			<titre>Apprentissage partiel de grammaires catégorielles</titre>
			<type>long</type>
			<pages></pages>
			<resume>Cet article traite de l’apprentissage symbolique de règles syntaxiques dans le modèle de Gold. Kanazawa a montré que certaines classes de grammaires catégorielles sont apprenables dans ce modèle. L’algorithme qu’il propose nécessite une grande quantité d’information en entrée pour être efficace. En changeant la nature des informations en entrée, nous proposons un algorithme d’apprentissage de grammaires catégorielles plus réaliste dans la perspective d’applications au langage naturel.</resume>
			<mots_cles>Apprentissage partiel, inférence grammaticale, grammaire catégorielles</mots_cles>
			<title></title>
			<abstract>This article deals with symbolic learning of syntactic rules in Gold’s model. Kanazawa showed that some classes of categorial grammars are learnable in this model. But the algorithm needs a high amount of information as input to be efficient. By changing the kind of information taken as input, we propose a learning algorithm for categorial grammars which is more realistic in the perspective of applications to natural language.</abstract>
			<keywords>Partial learning, grammatical inference, categorial grammars</keywords>
		</article>
		<article id="taln-2004-long-035" session="Session orale 5B">
			<auteurs>
				<auteur>
					<nom>Guy Perrier</nom>
					<email>perrier@loria.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LORIA - Université Nancy 2, BP 239 - 54506 Vandoeuvre-lès-Nancy cedex</affiliation>
			</affiliations>
			<titre>La sémantique dans les grammaires d’interaction</titre>
			<type>long</type>
			<pages></pages>
			<resume>Nous proposons d’intégrer la sémantique dans les grammaires d’interaction, formalisme qui a été conçu pour représenter la syntaxe des langues. Pour cela, nous ajoutons au formalisme un niveau supplémentaire qui s’appuie sur les mêmes principes fondamentaux que le niveau syntaxique : contrôle de la composition par un système de polarités et utilisation de la notion de description de structure pour exprimer la sous-spécification. A la différence du niveau syntaxique, les structures sont des graphes acycliques orientés et non des arbres localement ordonnés. L’interface entre les deux niveaux est assurée de façon souple par une fonction de liage qui associe à tout noeud syntaxique au plus un noeud sémantique.</resume>
			<mots_cles>formalisme grammatical, interface syntaxe-sémantique, sous-spécification, polarités</mots_cles>
			<title></title>
			<abstract>We propose an integration of semantics into Interaction Grammars, a formalism that was designed for representing the syntax of natural languages. It consists in the addition of a new level to the formalism and this level is based on the same fundamental principles as the syntactical level: the control of composition with a system of polarities and the use of the notion of structure description for expressing underspecification. Unlike the syntactical level, structures are directed acyclic graphs and not locally ordered trees. The interface between the two levels is performed in a flexible way by a linking function which maps every syntactical node to at most one semantical node.</abstract>
			<keywords>grammatical formalism, syntax-semantics interface, underspecification, polarities</keywords>
		</article>
		<article id="taln-2004-long-036" session="Session orale 5B">
			<auteurs>
				<auteur>
					<nom>Benoît Sagot</nom>
					<email>benoit.sagot@inria.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Pierre Boullier</nom>
					<email>pierre.boullier@inria.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Projet ATOLL - INRIA, Domaine de Voluceau, B.P. 105, 78153 Le Chesnay Cedex, France</affiliation>
				<affiliation affiliationId="2">TALaNa/Lattice - Université Paris 7, 2 place Jussieu, 75251 Paris Cedex 05, France</affiliation>
			</affiliations>
			<titre>Les Grammaires à Concaténation d’Intervalles (RCG) comme formalisme grammatical pour la linguistique</titre>
			<type>long</type>
			<pages></pages>
			<resume>Le but de cet article est de montrer pourquoi les Grammaires à Concaténation d’Intervalles (Range Concatenation Grammars, ou RCG) sont un formalisme particulièrement bien adapté à la description du langage naturel. Nous expliquons d’abord que la puissance nécessaire pour décrire le langage naturel est celle de PTIME. Ensuite, parmi les formalismes grammaticaux ayant cette puissance d’expression, nous justifions le choix des RCG. Enfin, après un aperçu de leur définition et de leurs propriétés, nous montrons comment leur utilisation comme grammaires linguistiques permet de traiter des phénomènes syntagmatiques complexes, de réaliser simultanément l’analyse syntaxique et la vérification des diverses contraintes (morphosyntaxiques, sémantique lexicale), et de construire dynamiquement des grammaires linguistiques modulaires.</resume>
			<mots_cles>Grammaires de réécriture, Grammaires Faiblement Contextuelles, complexité du langage naturel, Grammaires à Concaténation d’Intervalles (RCG)</mots_cles>
			<title></title>
			<abstract>The aim of this paper is to show why Range Concatenation Grammars (RCG) are a formalism particularly suitable to describe natural language. We first explain that the power necessary to describe natural language is that of PTIME. Then, among grammatical formalisms that have this expressing power, we justify the choice of RCGs. Finally, after an overview of their definition and properties, we show how their use as linguistic grammars makes it possible to deal with complex syntactic phenomena, to achieve simultaneously both syntactic parsing and constraints checking (e.g., morphosyntactic and/or lexical semantic constraints), and to build dynamically modular linguistic grammars.</abstract>
			<keywords>Rewriting Systems, Mildly Context-Sensitive Grammars, Complexity of Natural Language, Range Concatenation Grammars (RCG)</keywords>
		</article>
		<article id="taln-2004-poster-001" session="TALN Posters 1">
			<auteurs>
				<auteur>
					<nom>Carmen Alvarez</nom>
					<email>bissettc@iro.umontreal.ca</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Philippe Langlais</nom>
					<email>felipe@iro.umontreal.ca</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Jian-Yun Nie</nom>
					<email>nie@iro.umontreal.ca</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">RALI/IRO, Université de Montréal, CP. 6128, succursale Centre-ville, Montréal, Québec, H3C 3J7 Canada</affiliation>
			</affiliations>
			<titre>Mots composés dans les modèles de langue pour la recherche d’information</titre>
			<type>poster</type>
			<pages></pages>
			<resume>Une approche classique en recherche d’information (RI) consiste à bâtir une représentation des documents et des requêtes basée sur les mots simples les constituant. L’utilisation de modèles bigrammes a été étudiée, mais les contraintes sur l’ordre et l’adjacence des mots dans ces travaux ne sont pas toujours justifiées pour la recherche d’information. Nous proposons une nouvelle approche basée sur les modèles de langue qui incorporent des affinités lexicales (ALs), c’est à dire des paires non ordonnées de mots qui se trouvent proches dans un texte. Nous décrivons ce modèle et le comparons aux plus traditionnels modèles unigrammes et bigrammes ainsi qu’au modèle vectoriel.</resume>
			<mots_cles>Modèles de langue, recherche d’information, mots composés</mots_cles>
			<title></title>
			<abstract>Previous language modeling approaches to information retrieval have focused primarily on single terms. The use of bigram models has been studied, but the restriction on word order and adjacency may not be justified for information retrieval. We propose a new language modeling approach to information retrieval that incorporates lexical affinities (LAs), or pairs of words that occur near each other, without a constraint on word order. We explore the use of LAs in a language modeling approach, and compare our results with the vector space model, and unigram and bigram language model approaches.</abstract>
			<keywords>Language models, information retrieval, compound terms, word pairs</keywords>
		</article>
		<article id="taln-2004-poster-002" session="TALN Posters 1">
			<auteurs>
				<auteur>
					<nom>Eric Atwell</nom>
					<email>eric@comp.leeds.ac.uk</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">School of Computing — University of Leeds Leeds LS2 9JT, England</affiliation>
			</affiliations>
			<titre>Le Regroupement de Types de Mots et l'Unification d'Occurrences de Mots dans des Catégories grammaticales de mots</titre>
			<type>poster</type>
			<pages></pages>
			<resume>Ce papier discute la Néoposie: l'inférence auto-adaptive de catégories grammaticales de mots de la langue naturelle. L'inférence grammaticale peut être divisée en deux parties : l'inférence de catégories grammaticales de mots et l'inférence de la structure. Nous examinons les éléments de base de l'apprentissage auto-adaptif du marquage des catégories grammaticales, et discutons l'adaptation des trois types principaux de marqueurs des catégories grammaticales à l'inférence auto-adaptive de catégories grammaticales de mots. Des marqueurs statistiques de n-grammes suggèrent une approche de regroupement statistique, mais le regroupement n'aide ni avec les types de mots peu fréquents, ni avec les types de mots nombreux qui peuvent se présenter dans plus d'une catégorie grammaticale. Le marqueur alternatif d'apprentissage basé sur la transformation suggère une approche basée sur la contrainte de l'unification de contextes d'occurrences de mots. Celle-ci présente un moyen de regrouper des mots peu fréquents, et permet aux occurrences différentes d'un seul type de mot d'appartenir à des catégories différentes selon les contextes grammaticaux où ils se présentent. Cependant, la simple unification de contextes d'occurrences de mots produit un nombre incroyablement grand de catégories grammaticales de mots. Nous avons essayé d'unifier plus de catégories en modérant le contexte de la correspondance pour permettre l'unification des catégories de mots aussi bien que des occurrences de mots, mais cela entraîne des unifications fausses. Nous concluons que l'avenir peut être un hybride qui comprend le regroupement de types de mots peu fréquents, l'unification de contextes d'occurrences de mots, et le `seeding' avec une connaissance linguistique limitée. Nous demandons un programme de nouvelles recherches pour développer une valise pour la découverte de la langue naturelle.</resume>
			<mots_cles>Corpus, marquage des catégories grammaticales, regroupement, unification, catégories de mots, type/occurrence, évaluation</mots_cles>
			<title>Clustering of Word Types and Unification of Word Tokens into Grammatical Word-Classes</title>
			<abstract>This paper discusses Neoposy: unsupervised inference of grammatical word-classes in Natural  Language. Grammatical Inference can be divided into inference of grammatical word-classes and inference of structure. We review the background of supervised learning of Part-of-Speech tagging; and discuss the adaptation of the three main types of Part-of-Speech tagger to unsupervised inference of grammatical word-classes. Statistical N-gram taggers suggest a statistical clustering approach, but clustering does not help with low-frequency word-types, or with the many word-types which can appear in more than one grammatical category. The alternative Transformation-Based Learning tagger suggests a constraint-based approach of unification of word-token contexts. This offers a way to group together low-frequency word-types, and allows different tokens of one word-type to belong to different categories according to grammatical contexts they appear in. However, simple unification of word-token-contexts yields an implausibly large number of Part-of-Speech categories; we have attempted to merge more categories by "relaxing" matching context to allow unification of word-categories as well as word-tokens, but this results in spurious unifications. We conclude that the way ahead may be a hybrid involving clustering of frequent word-types, unification of word-token-contexts, and "seeding" with limited linguistic knowledge. We call for a programme of further research to develop a Language Discovery Toolkit.</abstract>
			<keywords>Corpus, Part-of-Speech tagging, clustering, unification, word classes, type/token, evaluation</keywords>
		</article>
		<article id="taln-2004-poster-003" session="TALN Posters 1">
			<auteurs>
				<auteur>
					<nom>Delphine Battistelli</nom>
					<email>Delphine.Battistelli@paris4.sorbonne.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Jean-Luc Minel</nom>
					<email>Jean-Luc.Minel@paris4.sorbonne.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Etienne Picard</nom>
					<email>Etienne.Picard@paris4.sorbonne.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Sylviane R. Schwer</nom>
					<email>Sylviane.Schwer@paris4.sorbonne.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Laboratoire LaLICC UMR 8139 (Université Paris IV, CNRS), 96, bd Raspail, 75006 Paris -France</affiliation>
			</affiliations>
			<titre>Temporalité linguistique et S-Langages</titre>
			<type>poster</type>
			<pages></pages>
			<resume>Après un rappel de la problématique de l’ordonnancement temporel dans un texte, nous décrivons les S-langages qui offrent une représentation unifiée des relations temporelles et une opération (la jointure) permettant de calculer les combinaisons entre celles-ci.</resume>
			<mots_cles>Structure(s) temporelle(s) dans un texte narratif, S-langages</mots_cles>
			<title></title>
			<abstract>After a brief overview of the problem of text temporal structures calculus, we describe the formal language of S-languages. This framework offers an unified representation of temporal relations and an operation (the joint) which computes combinations between such relations.</abstract>
			<keywords>Narratives temporal structure(s), S-langages</keywords>
		</article>
		<article id="taln-2004-poster-004" session="TALN Posters 1">
			<auteurs>
				<auteur>
					<nom>Emmanuel Bellengier</nom>
					<email>bellengier@lpl.univ-aix.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Béatrice Priego-Valverde</nom>
					<email>bea.priego-valverde@lpl.univ-aix.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Laboratoire Parole et Langage - Université de Provence, 29, Avenue Schuman Aix en Provence 13 621</affiliation>
			</affiliations>
			<titre>Modélisation de la modulation</titre>
			<type>poster</type>
			<pages></pages>
			<resume>Le dialogue est un processus interactif pendant lequel les différents agents impliqués vont s’engager sur un certain nombre d’éléments propositionnels. La modulation implique des ajouts propositionnels - révisés et atténués - qui ne constituent pas nécessairement une base pour un accord. L’objectif de cet article est donc de proposer une description formelle du phénomène de modulation dans le cadre du modèle de J. Ginzburg.</resume>
			<mots_cles>Modulation, discours, modèles de dialogue, interaction verbale, annotation</mots_cles>
			<title></title>
			<abstract>Dialogue is an interactive process in which agents involved must commit about some propositional elements. These elements are then available into the common ground shared by the agents. Mitigation is a particularly productive phenomenon which is problematic for the update of the common ground. We propose a formalisation of the phenomenon into the model of J. Ginzburg.</abstract>
			<keywords>Mitigation, discourse, verbal interaction, models of dialogue, annotation</keywords>
		</article>
		<article id="taln-2004-poster-005" session="TALN Posters 1">
			<auteurs>
				<auteur>
					<nom>Hervé Blanchon</nom>
					<email>Hervé.Blanchon@imag.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Laurent Besacier</nom>
					<email>Laurent.Besacier@imag.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Laboratoire CLIPS, BP 53, 38041 Grenoble Cedex 9</affiliation>
			</affiliations>
			<titre>Traduction de dialogue: résultats du projet NESPOLE! et pistes pour le domaine</titre>
			<type>poster</type>
			<pages></pages>
			<resume>Dans cet article, nous détaillons les résultats de la seconde évaluation du projet européen NESPOLE! auquel nous avons pris part pour le français. Dans ce projet, ainsi que dans ceux qui l’ont précédé, des techniques d’évaluation subjectives — réalisées par des évaluateurs humains — ont été mises en oeuvre. Nous présentons aussi les nouvelles techniques objectives — automatiques — proposées en traduction de l’écrit et mises en oeuvre dans le projet C-STAR III. Nous conclurons en proposant quelques idées et perspectives pour le domaine.</resume>
			<mots_cles>Traduction de dialogue, évaluation subjective et objective de composants de TALN</mots_cles>
			<title></title>
			<abstract></abstract>
			<keywords></keywords>
		</article>
		<article id="taln-2004-poster-006" session="TALN Posters 1">
			<auteurs>
				<auteur>
					<nom>Gustavo Crispino</nom>
					<email>Gustavo.Crispino@paris4.sorbonne.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Agata Jackiewicz</nom>
					<email>Agata.Jackiewicz@paris4.sorbonne.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Jean-Luc Minel</nom>
					<email>Jean-Luc.Minel@paris4.sorbonne.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Laboratoire LaLICC (Paris IV, CNRS) – Université de Paris-Sorbonne (ISHA), 96, bd Raspail, 75006 Paris -France</affiliation>
			</affiliations>
			<titre>Spécification et implantation informatique d’un langage de description des structures discursives</titre>
			<type>poster</type>
			<pages></pages>
			<resume>Cet article présente le langage de représentation des connaissances linguistiques LangTex qui permet de spécifier d’une manière unifiée les descriptions linguistiques nécessaires au repérage d’objets textuels qui organisent les textes écrits.</resume>
			<mots_cles>Représentation des structures discursives, langage de représentation des connaissances linguistiques</mots_cles>
			<title></title>
			<abstract>This article presents LangTex, a language for linguistic knowledge representation. This language allows to specify in a unified way the linguistic descriptions necessary for the location of textual objects which organize written texts.</abstract>
			<keywords>Representation of discourse structures, linguistic knowledge language representation</keywords>
		</article>
		<article id="taln-2004-poster-007" session="TALN Posters 1">
			<auteurs>
				<auteur>
					<nom>Mohamed Yassine El Amrani</nom>
					<email>elamrani@dmi.usherb.ca</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Sylvain Delisle</nom>
					<email>delisle@uqtr.ca</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Ismaïl Biskri</nom>
					<email>biskri@uqtr.ca</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Département de mathématiques et d’informatique – Université de Sherbrooke, 2500, Boul. de l’Université, Sherbrooke (Québec), J1K 2R1, Canada</affiliation>
				<affiliation affiliationId="2">Département de mathématiques et d’informatique – Université du Québec à Trois-Rivières, 3351, Boul. Des Forges CP 500, Trois-Rivières (Québec) G9A 5H7, Canada</affiliation>
			</affiliations>
			<titre>@GEWEB : Agents personnels d’aide à la recherche sur le Web</titre>
			<type>poster</type>
			<pages></pages>
			<resume>Nous présentons dans cet article un logiciel permettant d’assister l’usager, de manière personnalisée lors de la recherche documentaire sur le Web. L’architecture du logiciel est basée sur l’intégration d’outils numériques de traitements des langues naturelles (TLN). Le système utilise une stratégie de traitement semi-automatique où la contribution de l’utilisateur assure la concordance entre ses attentes et les résultats obtenus.</resume>
			<mots_cles>Reformulation de requêtes, Extraction de l’information, Personnalisation</mots_cles>
			<title></title>
			<abstract>We here present a new software that can help the user to formulate his web search queries and customize the information retrieval tasks to her individual and subjective needs. The software’s architecture is based on numeric natural language processing tools. The software involves a semi-automatic processing strategy in which the user’s contribution ensures that the results are useful and meaningful to her.</abstract>
			<keywords>Text mining, Web customization, Query reformulation, Information retrieval</keywords>
		</article>
		<article id="taln-2004-poster-008" session="TALN Posters 1">
			<auteurs>
				<auteur>
					<nom>Yannick Fouquet</nom>
					<email>Yannick.Fouquet@imag.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Laboratoire CLIPS-IMAG - Universite Joseph Fourier B.P. 53, 38041 Grenoble cedex 9, France</affiliation>
			</affiliations>
			<titre>Prédiction d’actes et attentes en dialogue : expérience avec un assistant virtuel simulé</titre>
			<type>poster</type>
			<pages></pages>
			<resume>Dans cet article, nous présentons une plate-forme de test et de recueil de dialogue oral homme-machine. Dans son architecture générale, des magiciens d’Oz simulent la compréhension des énoncés des utilisateurs et le contrôle du dialogue. Puis, nous comparons, dans un tel corpus, la prédiction statistique d’acte de dialogue avec les attentes du locuteur.</resume>
			<mots_cles>Dialogue, attentes, magicien d’Oz, pragmatique, analyse, statistique</mots_cles>
			<title></title>
			<abstract>This paper presents a platform for testing and building human-computer spoken dialog system. In the general architecture of the platform, understanding and dialog management are simulated. Thus, comparison between statistic act prediction and expectation will be made. First results obtained show a credible way of capturing and annotate spoken dialogs.</abstract>
			<keywords>Dialog, expectations, magicien d’Oz, pragmatics, analysis, statistics</keywords>
		</article>
		<article id="taln-2004-poster-009" session="TALN Posters 1">
			<auteurs>
				<auteur>
					<nom>Pablo Gamallo</nom>
					<email>gamallo@di.fct.unl.pt</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Gabriel P. Lopes</nom>
					<email>gpl@di.fct.unl.pt</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Alexandre Agustini</nom>
					<email>agustini@inf.pucrs.br</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">CITI, Faculdade de Ciências e Tecnologia, Universidade Nova de Lisboa, Portugal</affiliation>
				<affiliation affiliationId="2">Pontifícia Universidade Católica Rio Grande do Sul (PUCRS), Brazil.</affiliation>
			</affiliations>
			<titre></titre>
			<type>poster</type>
			<pages></pages>
			<resume>Cet article décrit une propriété sémantique propre aux dépendances syntaxiques binaires: la cocomposition. On proposera ici une définition plus générale que celle donnée par Pustejovsky et que nous appelons “co-composition optionnelle”. L’objet de cet article est de montrer les avantages apportées par la co-composition optionnelle dans deux tâches particulières en TAL: la désambiguïsation du sens des mots et la désambiguïsation structurale. Concernant cette deuxième tâche, nous décrirons les expériences faites sur un corpus.</resume>
			<mots_cles>Désambiguïsation du sens des mots, Désambiguïsation structurale, Co-composition, Acquisition de restrictions de sélection</mots_cles>
			<title>Disambiguation and Optional Co-Composition</title>
			<abstract>This paper describes a specific semantic property underlying binary dependencies: co-composition. We propose a more general definition than that given by Pustejovsky, what we call “optional co-composition”. The aim of the paper is to explore the benefits of optional cocomposition in two disambiguation tasks: both word sense and structural disambiguation. Concerning the second task, some experiments were performed on large corpora.</abstract>
			<keywords>Word sense disambiguation, Structural disambiguation, Co-composition, Selection restrictions acquisition</keywords>
		</article>
		<article id="taln-2004-poster-010" session="TALN Posters 1">
			<auteurs>
				<auteur>
					<nom>Marie-Josée Goulet</nom>
					<email>marie-josee.goulet.1@ulaval.ca</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Joël Bourgeoys</nom>
					<email>joel.bourgeoys.1@ulaval.ca</email>
					<affiliationId>1</affiliationId>
					<affiliationId>3</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">L-TAL - Université Laval, Québec, Canada</affiliation>
				<affiliation affiliationId="2">Laboratoire LaLICC - Université Paris IV-Sorbonne, France</affiliation>
				<affiliation affiliationId="3">LIC2M - CEA/LIST, Paris, France</affiliation>
			</affiliations>
			<titre>Le projet GÉRAF : Guide pour l’Évaluation des Résumés Automatiques Français</titre>
			<type>poster</type>
			<pages></pages>
			<resume>Dans cet article, nous présentons le projet GÉRAF (Guide pour l’Évaluation des Résumés Automatiques Français), lequel vise l’élaboration de protocoles et la construction de corpus de résumés de référence pour l’évaluation des systèmes résumant des textes français. La finalité de ce projet est de mettre à la disposition des chercheurs les ressources ainsi créées.</resume>
			<mots_cles>évaluation, résumé automatique, textes français, GÉRAF</mots_cles>
			<title></title>
			<abstract>In this paper, we introduce GÉRAF (Guide pour l’Évaluation des Résumés Automatiques Français), which aims at elaborating protocols and creating human-generated summaries for summarization evaluation in the context of French texts. The goal of this project is to provide researchers with protocols and corpora needed for French summarization evaluation.</abstract>
			<keywords>evaluation, automatic summarization, French texts, GÉRAF</keywords>
		</article>
		<article id="taln-2004-poster-011" session="TALN Posters 1">
			<auteurs>
				<auteur>
					<nom>Natalia Grabar</nom>
					<email>ngr@biomath.jussieu.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
					<affiliationId>3</affiliationId>
				</auteur>
				<auteur>
					<nom>Véronique Malaisé</nom>
					<email>vmalaise@ina.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>4</affiliationId>
				</auteur>
				<auteur>
					<nom>Aurélia Marcus</nom>
					<email>aurelia_m@noos.fr</email>
					<affiliationId>5</affiliationId>
				</auteur>
				<auteur>
					<nom>Aleksandra Krul</nom>
					<email>ola.krul@voila.fr</email>
					<affiliationId>3</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">STIM/Assistance Publique – Hôpitaux de Paris, ERM 202 INSERM</affiliation>
				<affiliation affiliationId="2">Département de Biomathématiques, Université Paris 6</affiliation>
				<affiliation affiliationId="3">CRIM/INaLCO</affiliation>
				<affiliation affiliationId="4">Université Paris 7 &amp; DRE de l’Institut National de l’Audiovisuel</affiliation>
				<affiliation affiliationId="5">Sinequa</affiliation>
			</affiliations>
			<titre>Repérage de relations terminologiques transversales en corpus</titre>
			<type>poster</type>
			<pages></pages>
			<resume>Les relations transversales encodent des relations spécifiques entre les termes, par exemple localisé-dans, consomme, etc. Elles sont très souvent dépendantes des domaines, voire des corpus. Les méthodes automatiques consacrées au repérage de relations terminologiques plus classiques (hyperonymie, synonymie), peuvent générer occasionnellement les relations transversales. Mais leur repérage et typage restent sujets à une conceptualisation : ces relations ne sont pas attendues et souvent pas connues à l’avance pour un nouveau domaine à explorer. Nous nous attachons ici à leur repérage mais surtout à leur typage. En supposant que les relations sont souvent exprimées par des verbes, nous misons sur l’étude des verbes du corpus et de leurs divers dérivés afin d’aborder plus directement la découverte des relations du domaine. Les expériences montrent que ce point d’attaque peut être intéressant, mais reste pourtant dépendant de la polysémie verbale et de la synonymie.</resume>
			<mots_cles>Terminologie, corpus spécialisés, structuration de terminologies, relations transversales, verbes</mots_cles>
			<title></title>
			<abstract>Transversal relations describe specific information existing between terms, for instance consumes, located-in, etc. They are often dependent on domains and even on corpora. Automatic methods, conceived to detect classical terminological relations (hyperonymy, synonymy), can occasionnally generate transversal relations. But their detection and typology depend on their conceptualisation : these relations are not expected and often not known for a newly explored domain. We aim here at their detection, but mainly at their typology. Since we suppose these relations are often expressed with verbs, we concentrate our investigation on the study of verbs and their derivatives to attack directly their discovering. Experiences show that this approach proposes interesting results which are nevertheless dependent on verbal polysemy and synonymy.</abstract>
			<keywords>Terminology, Specialised Corpora, Terminology Structuring, Transversal Relations, Verbs</keywords>
		</article>
		<article id="taln-2004-poster-012" session="TALN Posters 1">
			<auteurs>
				<auteur>
					<nom>Fabien Jalabert</nom>
					<email>fabien.jalabert@ema.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Mathieu Lafourcade</nom>
					<email>mathieu.lafourcade@lirmm.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LGI2P - Ecole des Mînes d’Alès Parc Scientifique Georges Besse 30 035 - Nimes Cedex 1</affiliation>
				<affiliation affiliationId="2">LIRMM - Université Montpellier II 34 392 - Montpellier Cedex 5</affiliation>
			</affiliations>
			<titre>Classification automatique de définitions en sens</titre>
			<type>poster</type>
			<pages></pages>
			<resume>Dans le cadre de la recherche en sémantique lexicale, l’équipe TAL du LIRMM développe actuellement un système d’analyse des aspects thématiques des textes et de désambiguisation lexicale basé sur les vecteurs conceptuels. Pour la construction des vecteurs, les définitions provenant de sources lexicales différentes (dictionnaires à usage humain, listes de synonymes, définitions de thésaurus, . . .) sont analysées. Aucun découpage du sens n’est présent dans la représentation : un vecteur conceptuel est associé à chaque définition et un autre pour représenter le sens global du mot. Nous souhaitons effectuer une catégorisation afin que chaque élément ne soit plus une définition mais un sens. Cette amélioration concerne bien sur directement les applications courantes (désambiguïsation, transfert lexical, . . .) mais a aussi pour objectif majeur d’améliorer l’apprentissage de la base.</resume>
			<mots_cles>Traitement automatique des langues naturelles, classification automatique, désambiguïsation sémantique lexicale</mots_cles>
			<title></title>
			<abstract>In the framework of research in meaning representation in NLP, we focus our attention on thematic aspects and conceptual vectors. A vectorial base is built upon a morphosyntactic analysis of several lexical resources to reduce isolated problems. A conceptual vector is associated with each definition and another one with the global meaning of a word. There is no effective meaning division and representation the the knowledge base. We study in the article a clustering method that merge definitions into senses. This applies on common problems (word sense disambiguation, word translation, . . .) and mainly to improve knowledge base learning.</abstract>
			<keywords>Natural language processing, unsupervised clustering, word sense disambiguation</keywords>
		</article>
		<article id="taln-2004-poster-013" session="TALN Posters 1">
			<auteurs>
				<auteur>
					<nom>André Kempe</nom>
					<email>andre.kempe@xrce.xerox.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Xerox Research Centre Europe – Grenoble Laboratory 6 chemin de Maupertuis – 38240 Meylan – France</affiliation>
			</affiliations>
			<titre></titre>
			<type>poster</type>
			<pages></pages>
			<resume></resume>
			<mots_cles></mots_cles>
			<title>NLP Applications Based onWeightedMulti-Tape Automata</title>
			<abstract>This article describes two practical applications of weighted multi-tape automata (WMTAs) in Natural Language Processing, that demonstrate the augmented descriptive power of WMTAs compared to weighted 1-tape and 2-tape automata. The two examples concern the preservation of intermediate results in transduction cascades and the search for similar words in two languages. As a basis for these applications, the article proposes a number of operations on WMTAs. Among others, it (re-)defines multi-tape intersection, where a number of tapes of one WMTA are intersected with the same number of tapes of another WMTA. In the proposed approach, multi-tape intersection is not an atomic operation but rather a sequence of more elementary ones, which facilitates its implementation.</abstract>
			<keywords>finite-state automaton, weighted multi-tape automaton, transduction cascade, lexicon</keywords>
		</article>
		<article id="taln-2004-poster-014" session="TALN Posters 2">
			<auteurs>
				<auteur>
					<nom>Moritz Neugebauer</nom>
					<email>moritz.neugebauer@ucd.ie</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Stephen Wilson</nom>
					<email>stephen.m.wilson@ucd.ie</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Department of Computer Science - University College Dublin Belfield, Dublin 4, Ireland</affiliation>
			</affiliations>
			<titre></titre>
			<type>poster</type>
			<pages></pages>
			<resume>De manière générale, les linguistes informaticiens utilisent les structures de données arborescentes pour la documentation et l’analyse des données morphologiques et syntactiques. Dans cet article nous appliquons de telles structures sur des données phonologiques et nous démontrons comment de telles représentations peuvent avoir des applications utiles et pratiques en lexicographie informatique. À cet effet, nous décrivons trois modules intégrés: Le premier module définit un ensemble de caractéristiques multilangages dans une structure arborescente exprimée en XML; le deuxième module parcours cet arbre et établis une généralisation sur des données contenues dans cet arborescence, optimise les données phonologiques et mets en valeur les implications des caractéristiques. Le troisième module utilise l’information contenue dans l’arborescence comme une base de connaissance pour la génération de syllabes lexiques à caractéristiques multiples.</resume>
			<mots_cles>Lexicographie, Représentations phonologiques, XML</mots_cles>
			<title>Multiple Lexicon Generation based on Phonological Feature Trees</title>
			<abstract>Tree-based data structures are commonly used by computational linguists for the documentation and analysis of morphological and syntactic data. In this paper we apply such structures to phonological data and demonstrate how such representations can have practical and beneficial applications in computational lexicography. To this end, we describe three integrated modules: the first defines a multilingual feature set within a tree-based structure using XML; the second module traverses this tree and generalises over the data contained within it, optimising the phonological data and highlighting feature implications. The third uses the information contained within the tree representation as a knowledge base for the generation of multiple feature-based syllable lexica.</abstract>
			<keywords>Lexicography, Phonological representations, XML</keywords>
		</article>
		<article id="taln-2004-poster-015" session="TALN Posters 2">
			<auteurs>
				<auteur>
					<nom>Jean Caelen</nom>
					<email>Jean.Caelen@imag.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Hoâ Nguyen</nom>
					<email>Ngoc-Hoa.Nguyen@imag.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Laboratoire CLIPS-IMAG 385, rue de la Bibliothèque - B.P. 53 - 38041 Grenoble Cedex 9 - France</affiliation>
			</affiliations>
			<titre>Gestion de buts de dialogue</titre>
			<type>poster</type>
			<pages></pages>
			<resume>La gestion du but de dialogue est une tâche délicate pour le contrôleur de dialogue, car bien souvent il est en concurrence avec le gestionnaire de tâches avec lequel on le confond parfois dans certains systèmes. Dans cet article, nous présentons une stratégie dynamique de gestion de buts qui permet au contrôleur de dialogue de réduire sa dépendance au gestionnaire de tâche et lui apporte une meilleure réutilisabilité. Nous expérimentons le système dans le cadre du projet PVE (Portail Vocal d’Entreprise) dans lequel le dialogue peut se dérouler en plusieurs sessions et avec des interlocuteurs différents.</resume>
			<mots_cles>Dialogue oral homme-machine, modèle de dialogue, but de dialogue</mots_cles>
			<title></title>
			<abstract>The dialogue goal management is an difficult task because often, dialogue management and task control are strong mixed together. We present in this paper a dynamic strategy to manage dialogue goals that enhances more independence of the dialogue manager with the task manager and brings a better reusability to the whole dialogue system. Our experiment in the framework of PVE (Vocal Portal for Enterprise) shows the possibility to use the dialogue system along a series of sessions and with different speakers.</abstract>
			<keywords>Human-machine spoken dialogue, dialogue model, dialogue goal</keywords>
		</article>
		<article id="taln-2004-poster-016" session="TALN Posters 2">
			<auteurs>
				<auteur>
					<nom>Guillaume Pitel</nom>
					<email>pitel@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Jean-Paul Sansonnet</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIMSI-CNRS, BP133 F-91403 Orsay Cedex</affiliation>
			</affiliations>
			<titre>Un modèle d’interprétation constructionnelle pour les expressions référentielles extensionnelles</titre>
			<type>poster</type>
			<pages></pages>
			<resume>Dans le dialogue finalisé, les expressions référentielles portant sur les objets du contexte peuvent contenir des prédicats vagues ou relationnels, qu’il est difficile de traiter avec une logique propositionnelle. Inversement, les approches adaptées à ces types de prédicats sont difficilement implémentables dans un modèle générique et adaptable aux théories d’analyse linguistique. Nous proposons un modèle d’interprétation constructionnelle inspiré des grammaires de construction qui permet de modéliser le processus de résolution d’expressions référentielles extensionnelles tout en restant compatible avec la grammaire dont nous nous sommes inspirés.</resume>
			<mots_cles>Grammaire de Construction, Référence Extensionnelle, Domaines de Référence</mots_cles>
			<title></title>
			<abstract>In practical dialogue, vague or relational predicates play an important role in referential expressions that refer to the objects in the context. These kind of referential expressions are difficult to handle using propositional logic but in the same time, approaches dealing with such predicates are hardly adaptable to language analysis theories. We propose a model of constructional interpretation that allow modelisation of resolution of extensional referential expressions while keeping compatibility with construction grammar theory.</abstract>
			<keywords>Construction Grammar, Extensionnal Reference Resolution, Reference Domains</keywords>
		</article>
		<article id="taln-2004-poster-017" session="TALN Posters 2">
			<auteurs>
				<auteur>
					<nom>Julien Poudade</nom>
					<email>poudade@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Patrick Paroubek</nom>
					<email>pap@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Limsi - CNRS, Batiment 508 Universite Paris XI, BP 133 - 91403 ORSAY Cedex - France</affiliation>
			</affiliations>
			<titre>Apprentissage collectif et lexique</titre>
			<type>poster</type>
			<pages></pages>
			<resume>Cet article présente l’influence de la zone de travail que possède une entité logicielle pour lui permettre de prédire l’état futur de son environnement, sur la constitution d’un lexique partagé par les différents membres d’une population, dans le cadre d’une variante “du jeu de désignation” (naming game).</resume>
			<mots_cles>lexique, apprentissage automatique, système multi-agents</mots_cles>
			<title></title>
			<abstract>In this paper, we show the influence that the work area used by software entities to predict the future state of their environment, has on the establishment of a common lexicon shared by the members of a population involved in a variant of the naming game.</abstract>
			<keywords>lexicon, machine learning, multi-agent system</keywords>
		</article>
		<article id="taln-2004-poster-018" session="TALN Posters 2">
			<auteurs>
				<auteur>
					<nom>François Rousselot</nom>
					<email>rousselot@liia.insa-strasbourg.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIIA-INSA 24, Boulevard de la Victoire 67 084 Strasbourg-CEDEX</affiliation>
			</affiliations>
			<titre>L’outil de traitement de corpus LIKES</titre>
			<type>poster</type>
			<pages></pages>
			<resume>LIKES (LInguistic and Knowledge Engineering Station) est une station d’ingénierie linguistique destinée à traiter des corpus, elle fonctionne pour l’instant sur la plupart des langues européennes et slaves en utilisant des ressources minimales pour chaque langue. Les corpus sont constitués d’un ou plusieurs textes en ASCII ou en HTML, l’interface donne la possibilité de constituer son corpus et d’y exécuter un certain nombre de tâches allant de simples tâches de découpage en mot, de tri ou de recherche de motifs à des tâches plus complexes d’aide à la synthèse de grammaire, d’aide au repérage de relations, d’aide à la construction d’une terminologie. Nous décrivons ici les principales fonctionnalités de LIKES en rapport avec le traitement des corpus et ce qui fait sa spécificité par rapport à d’autres environnements comparables : l’utilisation minimale de ressources linguistiques.</resume>
			<mots_cles>Traitement de corpus, segments répétés, recherches de relations, automates, transducteurs</mots_cles>
			<title></title>
			<abstract>LIKES (Llnguistic and Knowledge Engineering Station) is a linguistic engineering environment, build for corpora processing. Its provides different modules able to process most european and slavian languages. Corpora in Likes must be constituted by Texts in TXT format or in HTML texts of one particular. Tasks available are elementary likes classical basic corpora processing tasks (making list of forms, segmenting, sorting) and also more sophisticated as term extraction, help in relation extraction, pattern search, aimed at helping terminology building and ontology building. Main functionalities usefull for corpora processing are presented here.</abstract>
			<keywords>Corpus processing, repeated segments, search of semantic relations, automata, transducers</keywords>
		</article>
		<article id="taln-2004-poster-019" session="TALN Posters 2">
			<auteurs>
				<auteur>
					<nom>Susanne Salmon-Alt</nom>
					<email>Susanne.Salmon-Alt@atilf.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">ATILF – CNRS (UMR 7118) 44, Avenue de la Libération, B.P. 30687, 54063 Nancy Cedex</affiliation>
			</affiliations>
			<titre>Résolution automatique d’anaphores infidèles en français : Quelles ressources pour quels apports ?</titre>
			<type>poster</type>
			<pages></pages>
			<resume>La performance d’une résolution automatique d’anaphores infidèles pour le français pourrait atteindre une F-mesure de 30%. Ce résultat repose toutefois sur une ressource équivalente à un bon dictionnaire de la langue française, une analyse syntaxique de qualité satisfaisante et un traitement performant des entités nommées. En l’absence de telles ressources, les meilleurs résultats plafonnent autour d’une F-mesure de 15%.</resume>
			<mots_cles>anaphore infidèle, ressource sémantique, résolution d’anaphore, corpus annoté multiniveau</mots_cles>
			<title></title>
			<abstract>A system for solving indirect anaphora in French seems to be able to achieve a F-measure of 30%. However, this result supposes a high quality lexical database (equivalent to a classical dictionary), a good parser and a high precision named entity recognition. In case such resources are not available, the best results are obtained by using simple heuristics and are limited to a F-measure of 15%.</abstract>
			<keywords>indirect anaphor, lexical database, anaphora resolution, multi-level corpus annotation</keywords>
		</article>
		<article id="taln-2004-poster-020" session="TALN Posters 2">
			<auteurs>
				<auteur>
					<nom>Igor Schadle</nom>
					<email>igor.schadle@univ-ubs.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Jean-Yves Antoine</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Brigitte Le Pévédic</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Franck Poirier</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Laboratoire VALORIA, Université de Bretagne Sud (EA 2593)</affiliation>
			</affiliations>
			<titre>SibyMot : Modélisation stochastique du langage intégrant la notion de chunks</titre>
			<type>poster</type>
			<pages></pages>
			<resume>Cet article présente le modèle de langage développé pour le système Sibylle, un système d’aide à la communication pour les personnes handicapées. L’utilisation d’un modèle de langage permet d’améliorer la pertinence des mots proposés en tenant compte du contexte gauche de la saisie en cours. L’originalité de notre modèle se situe dans l’intégration de la notion de chunks afin d’élargir la taille du contexte pris en compte pour l’estimation de la probabilité d’apparition des mots.</resume>
			<mots_cles>Aide à la communication, modélisation stochastique du langage, n-gramme, chunks</mots_cles>
			<title></title>
			<abstract>We present in this article the language model of Sibyl, a new Alternative and Augmentative Communication (AAC) system. The use of language modeling improves the relevance of displayed words by taking into account the left context of the current sentence. The originality of our model is to introduce chunking. This enlarges the context taken into account to estimate the words probability.</abstract>
			<keywords>AAC, stochastic language modeling, n-gram, chunks</keywords>
		</article>
		<article id="taln-2004-poster-021" session="TALN Posters 2">
			<auteurs>
				<auteur>
					<nom>Joaquim Silva</nom>
					<email>jfs@di.fct.unl.pt</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Zornitsa Kozareva</nom>
					<email>zkozareva@hotmail.com</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Veska Noncheva</nom>
					<email>nonchev@plovdiv.techno-link.com</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Gabriel Lopes</nom>
					<email>gpl@di.fct.unl.pt</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Departamento de Informática, Faculdade de Ciências e Tecnologia, Universidade Nova de Lisboa Quinta da Torre, 2725 Monte da Caparica, Portugal</affiliation>
				<affiliation affiliationId="2">Faculty of Mathmatics and Informatics Plovdiv, Bulgaria</affiliation>
			</affiliations>
			<titre></titre>
			<type>poster</type>
			<pages></pages>
			<resume>Les entitées nomées et plus généralement les multi-mots sont des ressources importantes pour plusieurs applications. Cependant, les métodes d’extraction automatique, indépendentes de la langue, de multi-mots, ne nous donnent pas des données 100% fiables. Dans ce papier nous proposons premièrement une méthode pour selectioner entités nomées d’entre les multi-mots extraits automatiquement et, deuxièmement, une méthode de groupement des entités nomées non-supervisionée et indépendente de la langue, en utilisant de la statistique. La deuxième phase de groupement rends l’évaluation humaine plus simple. Les traits utilisés pour le groupement sont décrits et motivés. L’analyse faite pour le groupement nous a permis d’obtenir différents groupes d’entités nomées. La méthode a été appliquée sur le bulgare et l’anglais. La précision obtenue pour certains groupes a été très haute. D’autres groupes doivent être encore rafinés. Par ailleurs, les traits discrimants appris pendant la phase de groupement nous permettent de classifier de nouvelles entités nomées.</resume>
			<mots_cles>Entités Nommées, Unités Multi-mots, Groupement, Classification</mots_cles>
			<title>Extracting Named Entities. A Statistical Approach</title>
			<abstract>Named entities and more generally Multiword Lexical Units (MWUs) are important for various applications. However, language independent methods for automatically extracting MWUs do not provide us with clean data. So, in this paper we propose a method for selecting possible named entities from automatically extracted MWUs, and later, a statistics-based language independent unsupervised approach is applied to possible named entities in order to cluster them according to their type. Statistical features used by our clustering process are described and motivated. The Model-Based Clustering Analysis (MBCA) software enabled us to obtain different clusters for proposed named entities. The method was applied to Bulgarian and English. For some clusters, precision is very high; other clusters still need further refinement. Based on the obtained clusters, it is also possible to classify new possible named entities.</abstract>
			<keywords>Named Entities, Multiword Units, Clustering, Classification</keywords>
		</article>
		<article id="taln-2004-poster-022" session="TALN Posters 2">
			<auteurs>
				<auteur>
					<nom>Nicolas Stroppa</nom>
					<email>stroppa@enst.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>François Yvon</nom>
					<email>stroppa@enst.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">GET/ENST et LTCI, CNRS UMR 5141 46 rue Barrault - F-75 013 Paris</affiliation>
			</affiliations>
			<titre>Analogies dans les séquences : un solveur à états finis</titre>
			<type>poster</type>
			<pages></pages>
			<resume>L’apprentissage par analogie se fonde sur un principe inférentiel potentiellement pertinent pour le traitement des langues naturelles. L’utilisation de ce principe pour des tâches d’analyse linguistique présuppose toutefois une définition formelle de l’analogie entre séquences. Dans cet article, nous proposons une telle définition et montrons qu’elle donne lieu à l’implantation efficace d’un solveur d’équations analogiques sous la forme d’un transducteur fini. Munis de ces résultats, nous caractérisons empiriquement l’extension analogique de divers langages finis, correspondant à des dictionnaires de quatre langues.</resume>
			<mots_cles>Apprentissage par Analogie, Automates finis</mots_cles>
			<title></title>
			<abstract>Analogical reasoning provides us with an inferential mecanism of potential interest for NLP applications. An effective use of this process requires a formal definition of the notion of an analogy between strings of symbols. In this paper, we propose such a definition, from which we derive the implementation of a finite-state transducer solving analogical equations on sequences. We finally present the results of an empirical study of the analogical extension of several finite languages, corresponding to dictionnaries of four European languages.</abstract>
			<keywords>Analogical Learning, Finite-State Automaton</keywords>
		</article>
		<article id="taln-2004-poster-023" session="TALN Posters 2">
			<auteurs>
				<auteur>
					<nom>Christos Tsalidis</nom>
					<email>tsalidis@neurosoft.gr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Aristides Vagelatos</nom>
					<email>vagelat@cti.gr</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Giorgos Orphanos</nom>
					<email>orphan@neurosoft.gr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Neurosoft S.A. 24 Kofidou Street GR-14231 Athens, Greece</affiliation>
				<affiliation affiliationId="2">R.A. Computer Technology Institute 13 Eptachalkou Street GR-11851 Athens, Greece</affiliation>
			</affiliations>
			<titre></titre>
			<type>poster</type>
			<pages></pages>
			<resume></resume>
			<mots_cles>Lexique, morphologie</mots_cles>
			<title>An electronic dictionary as a basis for NLP tools: The Greek case</title>
			<abstract>The existence of a Dictionary in electronic form for Modern Greek (MG) is mandatory if one is to process MG at the morphological and syntactic levels since MG is a highly inflectional language with marked stress and a spelling system with many characteristics carried over from Ancient Greek. Moreover, such a tool becomes necessary if one is to create efficient and sophisticated NLP applications with substantial linguistic backing and coverage. The present paper will focus on the deployment of such an electronic dictionary for Modern Greek, which was built in two phases: first it was constructed to be the basis for a spelling correction schema and then it was reconstructed in order to become the platform for the deployment of a wider spectrum of NLP tools.</abstract>
			<keywords>Lexicon, morphology</keywords>
		</article>
		<article id="taln-2004-poster-024" session="TALN Posters 2">
			<auteurs>
				<auteur>
					<nom>Quang Vu-minh</nom>
					<email>quang.vu-minh@imag.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Laurent Besacier</nom>
					<email>laurent.besacier@imag.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Hervé Blanchon</nom>
					<email>herve.blanchon@imag.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Brigitte Bigi</nom>
					<email>brigitte.bigi@imag.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">CLIPS-IMAG Lab. UJF, BP53, 38041 Grenoble cedex 9, France</affiliation>
			</affiliations>
			<titre>Modèle de langage sémantique pour la reconnaissance automatique de parole dans un contexte de traduction</titre>
			<type>poster</type>
			<pages></pages>
			<resume>Le travail présenté dans cet article a été réalisé dans le cadre d'un projet global de traduction automatique de la parole. L’approche de traduction est fondée sur un langage pivot ou Interchange Format (IF), qui représente le sens de la phrase indépendamment de la langue. Nous proposons une méthode qui intègre des informations sémantiques dans le modèle statistique de langage du système de Reconnaissance Automatique de Parole. Le principe consiste a utiliser certaines classes définies dans l'IF comme des classes sémantiques dans le modèle de langage. Ceci permet au système de reconnaissance de la parole d'analyser partiellement en IF les tours de parole. Les expérimentations realisées montrent qu’avec cette approche, le système de reconnaissance peut analyser directement en IF une partie des données de dialogues de notre application, sans faire appel au système de traduction (35% des mots ; 58% des tours de parole), tout en maintenant le même niveau de performance du système global.</resume>
			<mots_cles>Traduction de parole, modèles de langage, représentation pivot</mots_cles>
			<title></title>
			<abstract>This paper relates a methodology to include some semantic information early in the statistical language model for Automatic Speech Recognition (ASR). This work is done in the framework of a global speech-to-speech translation project. An Interchange Format (IF) based approach, representing the meaning of phrases independently of languages, is adopted. The methodology consists in introducing semantic information by using a class-based statistical language model for which classes directly correspond to IF entries. With this new Language Model, the ASR module can analyze into IF part of dialogue data: 35% dialogue words; 58% speaker turns, without degrading the overall system performance.</abstract>
			<keywords>Speech-to-speech translation, language modeling, interchange format</keywords>
		</article>
	</articles>
</conference>