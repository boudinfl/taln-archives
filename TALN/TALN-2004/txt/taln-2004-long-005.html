<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
"http://www.w3.org/TR/html4/loose.dtd">
<html><head><title>D&#233;couvrir des sens de mots &#224; partir d&#8217;un r&#233;seau de cooccurrences lexicales</title>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
</head>
<body>
<div style="page-break-before:always; page-break-after:always"><div><p>TALN 2004, F&#232;s, 19-21 avril 2004 
</p>
<p>D&#233;couvrir des sens de mots &#224; partir d&#8217;un r&#233;seau  
</p>
<p>de cooccurrences lexicales 
</p>
<p>Olivier Ferret 
</p>
<p>CEA &#8211; LIST/LIC2M 
</p>
<p>92265 Fontenay-aux-Roses Cedex 
</p>
<p>ferreto@zoe.cea.fr 
</p>
<p>R&#233;sum&#233; &#8211; Abstract 
</p>
<p>Les r&#233;seaux lexico-s&#233;mantiques de type WordNet ont fait l&#8217;objet de nombreuses critiques 
</p>
<p>concernant la nature des sens qu&#8217;ils distinguent ainsi que la fa&#231;on dont ils caract&#233;risent ces 
</p>
<p>distinctions de sens. Cet article pr&#233;sente une solution possible &#224; ces limites, solution 
</p>
<p>consistant &#224; d&#233;finir les sens des mots &#224; partir de leur usage. Plus pr&#233;cis&#233;ment, il propose de 
</p>
<p>diff&#233;rencier les sens d&#8217;un mot &#224; partir d&#8217;un r&#233;seau de cooccurrences lexicales construit sur la 
</p>
<p>base d&#8217;un large corpus. Cette m&#233;thode a &#233;t&#233; test&#233;e &#224; la fois pour le fran&#231;ais et pour l&#8217;anglais et 
</p>
<p>a fait l&#8217;objet dans ce dernier cas d&#8217;une premi&#232;re &#233;valuation par comparaison avec WordNet. 
</p>
<p>Lexico-semantic networks such as WordNet have been criticized a lot on the nature of the 
</p>
<p>senses they distinguish as well as on the way they define these senses. In this article, we 
</p>
<p>present a possible solution to overcome these limits by defining the sense of words from the 
</p>
<p>way they are used. More precisely, we propose to differentiate the senses of a word from a 
</p>
<p>network of lexical cooccurrences built from a large corpus. This method was tested both for 
</p>
<p>French and English and for English, was evaluated through a comparison with WordNet. 
</p>
<p>Keywords &#8211; Mots Cl&#233;s 
</p>
<p>S&#233;mantique lexicale, d&#233;couverte du sens des mots, r&#233;seaux lexico-s&#233;mantiques 
</p>
<p>Lexical semantics, word sense discovery, lexico-semantic networks 
</p>
<p>1 Introduction 
</p>
<p>L&#8217;int&#233;r&#234;t de l&#8217;utilisation de ressources s&#233;mantiques en recherche ou en extraction 
</p>
<p>d&#8217;information a &#233;t&#233; montr&#233; depuis quelque temps d&#233;j&#224; au travers de travaux allant de 
</p>
<p>l&#8217;expansion de requ&#234;tes (de Loupy, El-B&#232;ze, 2002) aux syst&#232;mes de question/r&#233;ponse (Pasca, 
</p>
<p>Harabagiu, 2001). Ces travaux ont &#233;galement mis en avant le fait qu&#8217;une telle utilisation de-
</p>
<p>vait &#234;tre entour&#233;e de pr&#233;cautions : une am&#233;lioration des performances n&#8217;est observ&#233;e que si la 
</p>
<p>lev&#233;e d&#8217;ambigu&#239;t&#233; sur le sens des mots est r&#233;alis&#233;e avec une tr&#232;s bonne fiabilit&#233;. Cette obser-
</p>
<p>vation met l&#8217;accent sur l&#8217;un des r&#244;les premiers de la notion de ressource s&#233;mantique : d&#233;finir 
</p>
<p>pour chacun de ses mots un inventaire de ses sens possibles ainsi qu&#8217;une caract&#233;risation de 
</p>
<p>chacun d&#8217;entre eux. Les principales ressources s&#233;mantiques exploitables sous forme &#233;lectro-</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Olivier Ferret 
</p>
<p>nique et pr&#233;sentant une large couverture sont des r&#233;seaux lexico-s&#233;mantiques du type Word-
</p>
<p>Net (Miller, 1995). De par leur mode de construction, essentiellement manuel, ces r&#233;seaux ne 
</p>
<p>se d&#233;marquent pas fondamentalement des dictionnaires sous forme papier. Ils s&#8217;appuient 
</p>
<p>avant tout sur une formalisation et une syst&#233;matisation des pratiques lexicographiques 
</p>
<p>existantes. Les critiques formul&#233;es quant &#224; leur inad&#233;quation vis-&#224;-vis du traitement 
</p>
<p>automatique des langues, comme par exemple dans (Harabagiu et al., 1999), ne sont d&#232;s lors 
</p>
<p>pas surprenantes. Ces critiques portent &#224; la fois sur la nature des sens qu&#8217;ils distinguent et sur 
</p>
<p>leur caract&#233;risation. Ces sens sont jug&#233;s &#224; la fois trop fins et incomplets. Par ailleurs, leur 
</p>
<p>caract&#233;risation, r&#233;alis&#233;e pour l&#8217;essentiel au travers des relations de synonymie, d&#8217;hyperonymie 
</p>
<p>et d&#8217;hyponymie, manque d&#8217;&#233;l&#233;ments d&#233;finissant leur contexte d&#8217;usage. 
</p>
<p>Deux grandes solutions ont &#233;t&#233; explor&#233;es pour rem&#233;dier &#224; cette situation. La premi&#232;re d&#8217;entre 
</p>
<p>elles consiste &#224; enrichir automatiquement les r&#233;seaux de type WordNet pour y introduire les 
</p>
<p>informations permettant de r&#233;pondre aux critiques formul&#233;es. Diff&#233;rents travaux, dont r&#233;-
</p>
<p>cemment (Agirre, Lopez de Lacalle, 2003), se sont ainsi donn&#233;s pour objectif de regrouper 
</p>
<p>des sens de WordNet pour obtenir une granularit&#233; de sens &#224; plusieurs niveaux, donc adaptable 
</p>
<p>&#224; la t&#226;che consid&#233;r&#233;e. D&#8217;autres travaux, en particulier dans le cadre du projet eXtended 
</p>
<p>WordNet (Mihalcea, Moldovan, 2001), se sont orient&#233;s vers l&#8217;extraction de relations s&#233;manti-
</p>
<p>ques plus diverses &#224; partir des d&#233;finitions (les &#171; glosses &#187;) associ&#233;es aux synsets de WordNet, 
</p>
<p>ce qui permet de caract&#233;riser davantage le contexte d&#8217;usage de chacun d&#8217;entre eux. 
</p>
<p>La seconde solution consiste &#224; extraire les sens des mots automatiquement &#224; partir de corpus, 
</p>
<p>sans utilisation des dictionnaires existants. Chaque sens est alors d&#233;crit par une liste de mots 
</p>
<p>ne se limitant pas &#224; des synonymes ou des hyperonymes. Les travaux d&#233;j&#224; men&#233;s dans ce ca-
</p>
<p>dre se r&#233;partissent en trois grandes tendances. La premi&#232;re, illustr&#233;e par (Pantel, Lin, 2002), 
</p>
<p>ne place pas la d&#233;couverte des diff&#233;rents sens des mots au centre de ses pr&#233;occupations. Son 
</p>
<p>objectif premier est en effet de rassembler les mots en classes d&#8217;&#233;quivalence et donc plut&#244;t de 
</p>
<p>former des classes de synonymes. La d&#233;couverte de sens est une cons&#233;quence indirecte : la 
</p>
<p>m&#233;thode de classification utilis&#233;e, Clustering by Committee, autorisant l&#8217;appartenance d&#8217;un 
</p>
<p>mot &#224; plusieurs classes, chacune d&#8217;entre elles devient de facto un sens de ce mot. La deuxi&#232;me 
</p>
<p>tendance observ&#233;e, que l&#8217;on retrouve dans (Sch&#252;tze, 1998), (Pedersen, Bruce, 1997) et &#224; sa 
</p>
<p>suite (Purandare, 2003), caract&#233;rise pour sa part chaque occurrence d&#8217;un mot par un ensemble 
</p>
<p>de traits li&#233;s &#224; son environnement plus ou moins proche et proc&#232;de &#224; une classification non 
</p>
<p>supervis&#233;e de toutes les occurrences du mot sur la base de ces traits. Les diff&#233;rentes classes 
</p>
<p>form&#233;es constituent autant de sens du mot. La derni&#232;re approche enfin, repr&#233;sent&#233;e par (V&#233;ro-
</p>
<p>nis, 2003), (Dorow, Widdows, 2003) et (Rapp, 2003), prend comme point de d&#233;part les cooc-
</p>
<p>currents d&#8217;un mot enregistr&#233;s &#224; partir d&#8217;un corpus et forme les diff&#233;rents sens de ce mot en 
</p>
<p>regroupant ses cooccurrents suivant leur similarit&#233; ou au contraire leur dissimilarit&#233;. C&#8217;est 
</p>
<p>dans cette derni&#232;re perspective que se situe le travail que nous d&#233;crivons dans cet article. 
</p>
<p>2 Principes 
</p>
<p>Le point de d&#233;part de la m&#233;thode que nous pr&#233;sentons est un r&#233;seau de cooccurrences lexica-
</p>
<p>les, c&#8217;est-&#224;-dire un graphe dont les n&#339;uds sont les mots constituant le vocabulaire significatif 
</p>
<p>d&#8217;un corpus et les ar&#234;tes repr&#233;sentent les cooccurrences observ&#233;es entre ces mots dans le cor-
</p>
<p>pus. La d&#233;couverte des sens des mots est r&#233;alis&#233;e mot par mot et le traitement d&#8217;un mot ne fait 
</p>
<p>intervenir que le sous-graphe rassemblant les cooccurrents de ce mot. La premi&#232;re &#233;tape de la 
</p>
<p>m&#233;thode consiste &#224; construire une matrice de similarit&#233; de ces cooccurrents sur la base de 
</p>
<p>leurs relations dans le sous-graphe. Une m&#233;thode de classification automatique non supervis&#233;e </p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>D&#233;couvrir des sens de mots 
</p>
<p>est alors appliqu&#233;e afin de regrouper ces cooccurrents et former les diff&#233;rents sens du mot 
</p>
<p>consid&#233;r&#233;. L&#8217;hypoth&#232;se sous-jacente &#224; cette m&#233;thode, hypoth&#232;se qu&#8217;elle partage avec les tra-
</p>
<p>vaux relevant de la troisi&#232;me tendance d&#233;gag&#233;e dans la section pr&#233;c&#233;dente, est bien entendu 
</p>
<p>que la connectivit&#233; au sein du sous-graphe des cooccurrents formant le sens d&#8217;un mot est plus 
</p>
<p>importante que leur connectivit&#233; avec les cooccurrents d&#233;finissant les autres sens de ce mot. 
</p>
<p>La m&#233;thode de classification que nous utilisons est une adaptation de la m&#233;thode Shared Nea-
</p>
<p>rest Neighbors (SNN), expos&#233;e dans (Ert&#246;z et al., 2001). Cette m&#233;thode pr&#233;sente l&#8217;avantage 
</p>
<p>de d&#233;terminer automatiquement le nombre de classes, c&#8217;est-&#224;-dire le nombre de sens dans le 
</p>
<p>cas pr&#233;sent, et de laisser de c&#244;t&#233; les &#233;l&#233;ments les moins repr&#233;sentatifs des classes form&#233;es. Ce 
</p>
<p>dernier point est particuli&#232;rement utile pour cette application compte tenu du taux important 
</p>
<p>de &#171; bruit &#187; parmi les cooccurrents d&#8217;un mot. 
</p>
<p>3 Les r&#233;seaux de cooccurrence lexicale 
</p>
<p>Dans le cadre de ce travail, nous avons test&#233; notre m&#233;thode de d&#233;couverte de sens &#224; la fois sur 
</p>
<p>le fran&#231;ais et sur l&#8217;anglais. Nous avons donc construit un r&#233;seau de cooccurrences lexicales 
</p>
<p>pour ces deux langues. Celui pour le fran&#231;ais a &#233;t&#233; constitu&#233; &#224; partir de 24 mois du journal Le 
</p>
<p>Monde s&#233;lectionn&#233;s entre 1990 et 1994 ; celui pour l&#8217;anglais &#224; partir de deux ans du journal 
</p>
<p>Los Angeles Times, issus du corpus TREC. Dans chacun des cas, la taille du corpus est 
</p>
<p>d&#8217;environ 40 millions de mots. Pour les deux r&#233;seaux, le corpus initial a d&#8217;abord &#233;t&#233; pr&#233;trait&#233; 
</p>
<p>afin de caract&#233;riser les textes par leurs mots les plus discriminants sur le plan th&#233;matique, en 
</p>
<p>l&#8217;occurrence les noms, les verbes et les adjectifs, donn&#233;s sous forme lemmatis&#233;e. Dans le cas 
</p>
<p>du fran&#231;ais, les noms &#233;taient &#224; la fois des noms simples et des noms compos&#233;s. Les cooccur-
</p>
<p>rences ont ensuite &#233;t&#233; extraites en utilisant une fen&#234;tre glissante selon la m&#233;thode d&#233;crite dans 
</p>
<p>(Church, Hanks, 1990). Les param&#232;tres de cette extraction ont &#233;t&#233; fix&#233;s afin de favoriser la 
</p>
<p>capture de relations s&#233;mantiques et th&#233;matiques : la fen&#234;tre &#233;tait assez large (20 mots), respec-
</p>
<p>tait la fin des textes et l&#8217;ordre des cooccurrences n&#8217;&#233;tait pas conserv&#233;. Nous avons comme 
</p>
<p>Church et Hanks adopt&#233; une &#233;valuation de l&#8217;information mutuelle comme mesure de la coh&#233;-
</p>
<p>sion de chaque cooccurrence, mesure normalis&#233;e dans notre cas par l&#8217;information mutuelle 
</p>
<p>maximale relative au corpus. Apr&#232;s filtrage des cooccurrences les moins significatives (coh&#233;-
</p>
<p>sion &lt; 0,1 et moins de 10 occurrences), nous avons obtenu un r&#233;seau d&#8217;approximativement 
</p>
<p>23 000 mots et 5,2 millions de cooccurrences pour le fran&#231;ais et un r&#233;seau de 30 000 mots et 
</p>
<p>4,8 millions de cooccurrences pour l&#8217;anglais. 
</p>
<p>4 Algorithme de d&#233;couverte des sens 
</p>
<p>4.1 Construction de la matrice de similarit&#233; entre cooccurrents 
</p>
<p>Les algorithmes de classification sont en g&#233;n&#233;ral suffisamment param&#233;trables pour influer sur 
</p>
<p>le nombre et l&#8217;&#233;tendue des classes form&#233;es. Mais cette adaptabilit&#233; est implicitement limit&#233;e 
</p>
<p>par la mesure de similarit&#233; d&#233;finie pour comparer les &#233;l&#233;ments &#224; classer, d&#8217;o&#249; son importance. 
</p>
<p>Dans le cas pr&#233;sent, les &#233;l&#233;ments &#224; classer sont les cooccurrents dans le r&#233;seau de cooccur-
</p>
<p>rence lexicale du mot dont on cherche &#224; d&#233;couvrir les sens. Tout en conservant le m&#234;me cadre 
</p>
<p>g&#233;n&#233;ral, nous avons souhait&#233; tester deux mesures de similarit&#233; entre cooccurrents dans la 
</p>
<p>perspective d&#8217;obtenir diff&#233;rents niveaux de granularit&#233; quant aux sens distingu&#233;s. La premi&#232;re 
</p>
<p>mesure reprend simplement la valeur de coh&#233;sion existant dans le r&#233;seau de cooccurrence 
</p>
<p>entre les cooccurrents consid&#233;r&#233;s. S&#8217;il n&#8217;existe pas de relation entre eux dans le r&#233;seau, leur 
</p>
<p>similarit&#233; est consid&#233;r&#233;e comme nulle. Cette mesure poss&#232;de l&#8217;avantage de la simplicit&#233; et de 
</p>
<p>l&#8217;efficacit&#233; algorithmique mais elle est limit&#233;e par le fait que la relation de cooccurrence ne </p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Olivier Ferret 
</p>
<p>permet pas de capturer certaines proximit&#233;s entre mots. On constate ainsi exp&#233;rimentalement 
</p>
<p>que l&#8217;on retrouve parmi les cooccurrents d&#8217;un mot assez peu de ses synonymes reconnus
1
. On 
</p>
<p>peut donc s&#8217;attendre &#224; ce que certains sens distingu&#233;s en s&#8217;appuyant sur cette mesure ne soient 
</p>
<p>en fait qu&#8217;un seul et m&#234;me sens. 
</p>
<p>Pour pr&#233;venir ce risque, nous avons exp&#233;riment&#233; une mesure de similarit&#233; entre cooccurrents 
</p>
<p>reposant non seulement sur une relation de cooccurrence de premier niveau mais &#233;galement 
</p>
<p>de deuxi&#232;me niveau, cette derni&#232;re &#233;tant r&#233;put&#233;e plus stable (Sch&#252;tze, 1998). La mise en &#339;u-
</p>
<p>vre de cette mesure se fait de la fa&#231;on suivante : chaque cooccurrent se voit associer un vec-
</p>
<p>teur de taille &#233;gale au nombre de cooccurrents du mot trait&#233; et contenant la valeur de coh&#233;sion 
</p>
<p>entre ce cooccurrent et chacun des autres cooccurrents de ce mot. Comme pr&#233;c&#233;demment, 
</p>
<p>cette valeur est nulle s&#8217;il n&#8217;y a pas de relation dans le r&#233;seau entre deux cooccurrents. La ma-
</p>
<p>trice de similarit&#233; entre cooccurrents est simplement construite en appliquant la mesure cosi-
</p>
<p>nus entre les vecteurs de chaque couple de cooccurrents. Avec cette seconde mesure de simi-
</p>
<p>larit&#233;, deux cooccurrents n&#8217;ont plus n&#233;cessairement besoin d&#8217;entretenir une relation de cooc-
</p>
<p>currence directe pour &#234;tre jug&#233;s proches : ils peuvent se contenter de partager un ensemble de 
</p>
<p>mots avec lesquels ils entretiennent une telle relation. 
</p>
<p>4.2 Algorithme SNN (Shared Nearest Neighbors) 
</p>
<p>L&#8217;algorithme SNN (Ert&#246;z et al., 2001) s&#8217;inscrit dans la mouvance des algorithmes ramenant le 
</p>
<p>probl&#232;me de la classification &#224; celui de la d&#233;tection de composantes de forte densit&#233; dans un 
</p>
<p>graphe de similarit&#233;. Dans un tel graphe, chaque n&#339;ud repr&#233;sente un &#233;l&#233;ment &#224; classer et une 
</p>
<p>ar&#234;te relie deux n&#339;uds lorsque la similarit&#233; entre les &#233;l&#233;ments qu&#8217;ils repr&#233;sentent est non 
</p>
<p>nulle. Lorsque la matrice de similarit&#233; est sym&#233;trique, comme c&#8217;est le cas ici, le graphe obtenu 
</p>
<p>est non orient&#233;. On pourra noter que dans le cas de la d&#233;couverte des sens d&#8217;un mot, le pro-
</p>
<p>bl&#232;me est &#224; la base un probl&#232;me de d&#233;tection de composantes de forte densit&#233;, les sens, au 
</p>
<p>sein du graphe des cooccurrents de ce mot. Il est conserv&#233; tel quel avec la premi&#232;re mesure de 
</p>
<p>similarit&#233; mais transpos&#233; en un probl&#232;me plus g&#233;n&#233;ral de classification avec la seconde. 
</p>
<p>Dans son principe g&#233;n&#233;ral, l&#8217;algorithme SNN comporte deux grandes &#233;tapes : la premi&#232;re vise 
</p>
<p>&#224; mettre en &#233;vidence les &#233;l&#233;ments les plus repr&#233;sentatifs de leur voisinage en masquant les 
</p>
<p>relations les moins importantes du graphe de similarit&#233;. Ces &#233;l&#233;ments constituent les em-
</p>
<p>bryons des futures classes, form&#233;es dans un second temps en agr&#233;geant les autres &#233;l&#233;ments &#224; 
</p>
<p>ceux s&#233;lectionn&#233;s lors de la premi&#232;re phase. L&#8217;algorithme SNN, consid&#233;r&#233; dans le contexte de 
</p>
<p>la d&#233;couverte de sens, se d&#233;compose plus pr&#233;cis&#233;ment comme suit : 
</p>
<p>1. &#171; &#233;claircissement &#187; du graphe de similarit&#233; : pour chaque cooccurrent, seules les ar&#234;tes en 
</p>
<p>direction des k (k = 15 en l&#8217;occurrence) plus proches cooccurrents sont conserv&#233;es. 
</p>
<p>2. construction du graphe des plus proches voisins partag&#233;s : cette &#233;tape consiste &#224; remplacer 
</p>
<p>dans le graphe &#171; &#233;clairci &#187; la valeur port&#233;e par chaque ar&#234;te par le nombre de voisins di-
</p>
<p>rects que les deux cooccurrents reli&#233;s par l&#8217;ar&#234;te ont en commun. 
</p>
<p>3. calcul de la distribution en liens forts des cooccurrents : l&#8217;objectif de cette &#233;tape est, 
</p>
<p>comme lors de l&#8217;&#233;tape 1, de proc&#233;der &#224; une sorte d&#8217;&#233;claircissement. Il s&#8217;agit de rep&#233;rer les 
</p>
<p> 
</p>
<p>1
Constatation faite en r&#233;alisant l&#8217;intersection pour chaque mot du r&#233;seau construit &#224; partir du Los Angeles 
</p>
<p>Times entre ses cooccurrents et ses synonymes dans WordNet. </p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>D&#233;couvrir des sens de mots 
</p>
<p>cooccurrents autour desquels s&#8217;organisent un ensemble d&#8217;autres cooccurrents, i.e. des 
</p>
<p>germes de sens, mais aussi de rep&#233;rer ceux qui sont visiblement sans connexion v&#233;ritable 
</p>
<p>avec les autres. Pour ce faire, un seuil minimum est fix&#233; concernant le nombre de voisins 
</p>
<p>partag&#233;s par deux cooccurrents, seuil au-dessus duquel on consid&#232;re les deux cooccurrents 
</p>
<p>comme fortement li&#233;s. On caract&#233;rise ensuite chaque cooccurrent par le nombre de liens 
</p>
<p>forts qu&#8217;il poss&#232;de. 
</p>
<p>4. d&#233;termination des germes de sens et &#233;limination du bruit : les germes de sens et les 
</p>
<p>cooccurrents laiss&#233;s de c&#244;t&#233; sont d&#233;termin&#233;s par simple comparaison de leur nombre de 
</p>
<p>liens forts par rapport &#224; un seuil. 
</p>
<p>5. construction des sens : cette &#233;tape consiste principalement &#224; associer aux germes se sens 
</p>
<p>trouv&#233;s &#224; l&#8217;&#233;tape pr&#233;c&#233;dente les cooccurrents non d&#233;j&#224; s&#233;lectionn&#233;s comme germe de sens 
</p>
<p>ou bruit pour former des classes repr&#233;sentant les sens du mot consid&#233;r&#233;. Pour associer un 
</p>
<p>cooccurrent &#224; un germe de sens, la force du lien qui les unit doit &#234;tre sup&#233;rieure &#224; un seuil. 
</p>
<p>Si un rattachement &#224; plusieurs germes est possible, est choisi le germe avec lequel la force 
</p>
<p>du lien est la plus grande. Par ailleurs, cette &#233;tape est aussi l&#8217;occasion de rassembler plu-
</p>
<p>sieurs germes de sens consid&#233;r&#233;s comme trop proches pour former des sens distincts : le 
</p>
<p>rattachement des cooccurrents fait donc &#233;galement intervenir les germes de sens. 
</p>
<p>6. &#233;largissement des sens : &#224; l&#8217;issue des &#233;tapes pr&#233;c&#233;dentes, un nombre plus ou moins impor-
</p>
<p>tant de cooccurrents n&#8217;ayant pas &#233;t&#233; consid&#233;r&#233;s comme du bruit se retrouvent n&#233;anmoins 
</p>
<p>sans affectation &#224; un sens. Ce nombre d&#233;pend bien entendu de la s&#233;v&#233;rit&#233; du seuil de ratta-
</p>
<p>chement &#224; un germe de sens mais l&#8217;objectif &#233;tant de former des classes homog&#232;nes, celle-
</p>
<p>ci doit &#234;tre n&#233;cessairement assez forte. N&#233;anmoins, il est &#233;galement int&#233;ressant que les 
</p>
<p>sens puissent &#234;tre d&#233;crits de la fa&#231;on la plus compl&#232;te et la plus pr&#233;cise possible. Les sens 
</p>
<p>&#224; ce stade &#233;tant caract&#233;ris&#233;s de fa&#231;on plus s&#251;re qu&#8217;&#224; l&#8217;issue de l&#8217;&#233;tape 4, il est possible de 
</p>
<p>leur rattacher des cooccurrents dont la force de lien avec leurs constituants est plus faible. 
</p>
<p>4.3 Adaptation et modalit&#233;s d&#8217;application de l&#8217;algorithme SNN 
</p>
<p>Les principes de l&#8217;algorithme SNN expos&#233;s dans la section pr&#233;c&#233;dente doivent &#234;tre pr&#233;cis&#233;s 
</p>
<p>sur certains points quant &#224; leur mise en &#339;uvre. Le principal de ces points est le mode de fixa-
</p>
<p>tion de ses diff&#233;rents seuils. Nous avons opt&#233; pour un mode unique s&#8217;adaptant &#224; la distribution 
</p>
<p>des valeurs observ&#233;es : chaque seuil est exprim&#233; comme un certain quantile de ces valeurs. 
</p>
<p>Dans le cas du seuil de d&#233;termination des germes de sens (&#233;gal &#224; 0,9) et de celui de d&#233;finition 
</p>
<p>du bruit (&#233;gal &#224; 0,2), il s&#8217;agit d&#8217;un quantile s&#8217;appliquant au nombre de liens forts des cooccur-
</p>
<p>rents. Pour le seuil d&#233;finissant la notion de lien fort (&#233;gal &#224; 0,65), celui de rattachement des 
</p>
<p>coocccurrents aux germes (&#233;gal &#224; 0,5) et celui de rattachement des cooccurrents aux sens (&#233;gal 
</p>
<p>&#224; 0,7), le quantile est appliqu&#233; directement &#224; la force des liens entre cooccurrents dans le gra-
</p>
<p>phe des plus proches voisins partag&#233;s. 
</p>
<p>Au-del&#224; des modalit&#233;s de mise en &#339;uvre des principes, nous avons &#233;galement introduit des 
</p>
<p>adaptations. La plus importante d&#8217;entre elles est l&#8217;ajout d&#8217;une &#233;tape entre les deux derni&#232;res. 
</p>
<p>Nous avons en effet observ&#233; qu&#8217;en d&#233;pit de la possibilit&#233;, au niveau de la phase de construc-
</p>
<p>tion des sens, de fusionner des classes par l&#8217;interm&#233;diaire du rattachement d&#8217;un germe de sens 
</p>
<p>&#224; un autre, certains sens restent divis&#233;s en plusieurs classes. Ce ph&#233;nom&#232;ne est observable 
</p>
<p>m&#234;me en faisant initialement appel &#224; des cooccurrences d&#8217;ordre 2 et ne peut &#234;tre efficacement </p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Olivier Ferret 
</p>
<p>trait&#233;
2
</p>
<p>par le seul ajustement du seuil contr&#244;lant le rattachement des cooccurrents aux germes 
</p>
<p>de sens. Dans un nombre significatif de cas, le sens &#171; divis&#233; &#187; se r&#233;partit entre une ou plu-
</p>
<p>sieurs classes ne regroupant que 3 &#224; 4 mots et une classe de plus large ampleur. En pratique, 
</p>
<p>les germes de sens de ces classes &#171; minoritaires &#187; n&#8217;ont pas pu &#234;tre rattach&#233;s &#224; la classe 
</p>
<p>&#171; majoritaire &#187; alors que la plupart des cooccurrents qui leur &#233;taient li&#233;s s&#8217;y sont rattach&#233;s. 
</p>
<p>Plut&#244;t que de d&#233;finir un m&#233;canisme sp&#233;cifique pour regrouper ces classes &#171; minoritaires &#187; 
</p>
<p>avec la classe la plus importante, nous avons choisi de laisser l&#8217;algorithme dans sa forme ac-
</p>
<p>tuelle le faire en d&#233;truisant ces classes (taille &lt; 6) et en remettant leurs &#233;l&#233;ments dans 
</p>
<p>l&#8217;ensemble des cooccurrents non rattach&#233;s. La derni&#232;re &#233;tape de l&#8217;algorithme permet alors 
</p>
<p>dans la plupart des cas de rattacher ces cooccurrents &#224; la classe &#171; majoritaire &#187;. De plus, ce 
</p>
<p>m&#233;canisme permet d&#8217;obtenir une plus grande stabilit&#233; des sens form&#233;s lorsque les param&#232;tres 
</p>
<p>de l&#8217;algorithme sont modifi&#233;s. 
</p>
<p>Une seconde adaptation, d&#8217;impact plus faible, a &#233;t&#233; op&#233;r&#233;e afin de s&#8217;assurer que les cooccur-
</p>
<p>rents rattach&#233;s lors de la derni&#232;re &#233;tape n&#8217;introduisent pas de bruit. Nous avons ainsi impos&#233; 
</p>
<p>que la condition de rattachement ne porte pas seulement sur la force de la relation entre le 
</p>
<p>cooccurrent &#224; rattacher et l&#8217;un des membres de la classe mais sur la force moyenne des rela-
</p>
<p>tions entre ce cooccurrent et les &#233;l&#233;ments de cette classe. 
</p>
<p>5 Exp&#233;rimentation 
</p>
<p>Nous avons appliqu&#233; notre m&#233;thode de d&#233;couverte de sens aux deux r&#233;seaux de cooccurrences 
</p>
<p>lexicales (LM : fran&#231;ais ; LAT : anglais) que nous avons construits avec les valeurs de para-
</p>
<p>m&#232;tres pr&#233;cis&#233;es dans les sections pr&#233;c&#233;dentes. Pour chaque r&#233;seau, nous avons test&#233; 
</p>
<p>l&#8217;utilisation initiale de cooccurrences d&#8217;ordre 1 (LM-1 et LAT-1) et d&#8217;ordre 2 (LM-2 et 
</p>
<p>LAT-2). Pour l&#8217;anglais, la seconde modalit&#233; n&#8217;a &#233;t&#233; test&#233;e que sur le sous-ensemble des mots 
</p>
<p>utilis&#233;s pour l&#8217;&#233;valuation de la section 6 (LAT-2.no). Le tableau 1 synth&#233;tise les informations 
</p>
<p>concernant les sens d&#233;couverts dans les diff&#233;rentes configurations. On remarquera qu&#8217;un 
</p>
<p>pourcentage significatif de mots n&#8217;ont pas sens, m&#234;me avec les cooccurrences d&#8217;ordre 2. Ce 
</p>
<p>sont les mots dont les cooccurrents sont faiblement li&#233;s et dont le sens est probablement mal 
</p>
<p>repr&#233;sent&#233; au sein de leur r&#233;seau de cooccurrences. Par ailleurs, on notera que l&#8217;utilisation des 
</p>
<p>cooccurrences d&#8217;ordre 2 conduit effectivement &#224; r&#233;duire le nombre de sens par mot. 
</p>
<p> LM-1 LM-2 LAT-1 LAT-1.no LAT-2.no 
</p>
<p>nombre de mots 17.261 17.261 13.414 6.177 6.177 
</p>
<p>nombre de mots avec au 
</p>
<p>moins un sens 
</p>
<p>7.373 
</p>
<p>(44,4%) 
</p>
<p>7.376 
</p>
<p>(42,7%) 
</p>
<p>5.338 
</p>
<p>(39,8%) 
</p>
<p>2.584  
</p>
<p>(41.8%) 
</p>
<p>2.406 
</p>
<p>(39%) 
</p>
<p>nombre moyen de sens 
</p>
<p>par mot 
</p>
<p>2,8 2,2 1,6 1,9 1,5 
</p>
<p>nombre moyen de mots 
</p>
<p>d&#233;crivant un sens 
</p>
<p>16,1 16,3 18,7 20,2 18,9 
</p>
<p>Tableau 1 : Statistiques concernant les r&#233;sultats de la d&#233;couverte de sens 
</p>
<p>&#192; l&#8217;instar de V&#233;ronis (2003), nous illustrerons les r&#233;sultats de notre algorithme en donnant 
</p>
<p>quelques uns des mots caract&#233;risant les sens trouv&#233;s pour le mot barrage :
</p>
<p>2
C&#8217;est-&#224;-dire sans regrouper des classes correspondant &#224; des sens diff&#233;rents. </p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>D&#233;couvrir des sens de mots 
</p>
<p>LM-1 1.1 manifestant, forces_de_l&#8217;ordre, pr&#233;fecture, agriculteur, protester, incendier, calme, pierre 
</p>
<p> 1.2 conducteur, routier, v&#233;hicule, poids_lourd, camion, permis, trafic, bloquer, voiture, autoroute 
</p>
<p>1.3 fleuve, lac, rivi&#232;re, bassin, m&#232;tre_cube, crue, amont, pollution, affluent, saumon, poisson 
</p>
<p> 1.4 bless&#233;, casque_bleu, soldat, milicien, tir, milice, convoi, &#233;vacuer, croate, milicien, combattant 
</p>
<p>LM-2 2.1 eau, m&#232;tre, lac, pluie, rivi&#232;re, bassin, fleuve, site, poisson, affluent, montagne, crue, vall&#233;e 
</p>
<p>2.2 conducteur, trafic, routier, route, camion, chauffeur, voiture, chauffeur_routier, poids_lourd 
</p>
<p>2.3 casque_bleu, soldat, tir, convoi, milicien, blind&#233;, milice, a&#233;roport, bless&#233;, incident, croate 
</p>
<p>On retrouve dans les deux cas 3 des 4 sens distingu&#233;s dans (V&#233;ronis, 2003) : barrage hydrau-
</p>
<p>lique (sens 1.3 et 2.1), barrage routier (sens 1.2 et 2.2), barrage fronti&#232;re (sens 1.4 et 2.3). Le 
</p>
<p>sens match de barrage n&#8217;est pas repr&#233;sent&#233; car faiblement pr&#233;sent au niveau des cooccurren-
</p>
<p>ces et de plus, au travers de certains mots ambigus, comme division, qui renvoient aussi &#224; 
</p>
<p>d&#8217;autres domaines que le sport. Il faut pr&#233;ciser que barrage ne comporte ici que 1104 occur-
</p>
<p>rences, &#224; comparer avec environ 7000 occurrences pour (V&#233;ronis, 2003). Cet exemple illustre 
</p>
<p>&#233;galement la diff&#233;rence de granularit&#233; des sens induite par l&#8217;utilisation des cooccurrences 
</p>
<p>d&#8217;ordre 1 ou 2. Le sens 1.1, qui est assez proche du sens 1.2, les deux faisant r&#233;f&#233;rence &#224; des 
</p>
<p>manifestations de col&#232;re li&#233;e &#224; une profession, dispara&#238;t ainsi lorsqu&#8217;on fait appel aux cooc-
</p>
<p>currences d&#8217;ordre 2. Nous donnons &#224; la suite les sens pour d&#8217;autres mots en fran&#231;ais et en an-
</p>
<p>glais avec des cooccurrences d&#8217;ordre 1 : 
</p>
<p>organe (1300) patient, transplantation, greffe, malade, th&#233;rapeutique, m&#233;dical, m&#233;decine, greffer, rein 
</p>
<p> procr&#233;ation, embryon, &#233;thique, humain, relatif, bio&#233;thique, corps_humain, g&#232;ne, cellule 
</p>
<p>constitutionnel, consultatif, constitution, instituer, ex&#233;cutif, l&#233;gislatif, si&#233;ger, disposition 
</p>
<p> article, hebdomadaire, publication, r&#233;daction, quotidien, journal, &#233;ditorial, r&#233;dacteur  
</p>
<p>mouse (563) compatible, sofware, computer, machine, user, desktop, pc, graphics, keyboard, device 
</p>
<p>laboratory, researcher, cell, gene, generic, human, hormone, research, scientist, rat 
</p>
<p>party (16999) candidate, democrat, republican, gubernatorial, presidential, partisan, reapportionment 
</p>
<p>ballroom, cocktail, champagne, guest, bash, gala, wedding, birthday, invitation, festivity 
</p>
<p>caterer, uninvited, party-goers, black-tie, hostess, buffet, glitches, napkins, catering 
</p>
<p>6 &#201;valuation 
</p>
<p>La d&#233;couverte de sens se heurte, comme les autres t&#226;ches de construction de ressources lin-
</p>
<p>guistiques, &#224; la difficult&#233; de l&#8217;&#233;valuation du r&#233;sultat obtenu. La voie la plus directe pour ce 
</p>
<p>faire est la comparaison avec une ressource de r&#233;f&#233;rence que l&#8217;on consid&#232;re comme proche. 
</p>
<p>Dans le cas pr&#233;sent, les r&#233;seaux lexico-s&#233;mantiques de type WordNet s&#8217;imposent comme la 
</p>
<p>ressource de r&#233;f&#233;rence la plus proche. Utiliser ce type de r&#233;seaux pour &#233;valuer les sens trouv&#233;s 
</p>
<p>est certes critiquable puisqu&#8217;un des objectifs d&#8217;une telle d&#233;couverte est de d&#233;passer les limites 
</p>
<p>de ces r&#233;seaux. N&#233;anmoins, compte tenu du caract&#232;re contr&#244;l&#233; de ces derniers, une telle &#233;va-
</p>
<p>luation apporte au moins un &#233;l&#233;ment de jugement important quant &#224; la fiabilit&#233; des sens mis 
</p>
<p>en &#233;vidence. Nous avons choisi de reprendre le protocole d&#8217;&#233;valuation d&#233;fini dans (Pantel, 
</p>
<p>Lin, 2002), protocole qui s&#8217;appuie sur WordNet et dont l&#8217;accord avec un jugement manuel est 
</p>
<p>raisonnablement bon (88% pour Pantel et Lin). Notre &#233;valuation ne portera donc que sur 
</p>
<p>l&#8217;anglais et a &#233;t&#233; r&#233;alis&#233;e avec Wordnet 1.7.1. Ce protocole consiste &#224; essayer de mettre en 
</p>
<p>correspondance chaque sens trouv&#233; pour un mot avec l&#8217;un de ses synsets dans WordNet et ce, 
</p>
<p>au moyen d&#8217;une mesure de similarit&#233;. Il s&#8217;agit donc d&#8217;une mesure de pr&#233;cision. Pantel et Lin 
</p>
<p>pr&#233;cisent qu&#8217;une mesure de rappel n&#8217;est dans le cas pr&#233;sent que faiblement significative : un 
</p>
<p>sens d&#233;couvert peut &#234;tre valide et non pr&#233;sent dans WordNet et &#224; l&#8217;inverse certaines distinc-
</p>
<p>tions de sens dans WordNet ne sont pas n&#233;cessairement souhaitables. Ils d&#233;finissent n&#233;an-</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Olivier Ferret 
</p>
<p>moins une mesure de rappel mais destin&#233;e seulement &#224; classer un ensemble de syst&#232;mes. Elle 
</p>
<p>n&#8217;est donc pas applicable &#224; notre seule m&#233;thode. 
</p>
<p>La mesure de similarit&#233; entre un sens et un synset utilis&#233;e pour le calcul de la pr&#233;cision 
</p>
<p>s&#8217;appuie sur la mesure de similarit&#233; entre synsets d&#233;finie par Lin : 
</p>
<p> 
)2(log)1(log
</p>
<p>)(log2
)2,1(
</p>
<p>sPsP
</p>
<p>sP
sssim
</p>
<p>+
</p>
<p>&#215;
= (1) 
</p>
<p>o&#249; s est le synset le plus sp&#233;cifique subsumant les synset s1 et s2 dans la hi&#233;rarchie de Word-
</p>
<p>Net et o&#249; P(s) repr&#233;sente la probabilit&#233; du synset s calcul&#233;e &#224; partir d&#8217;un corpus de r&#233;f&#233;rence, 
</p>
<p>en l&#8217;occurrence le SemCor. Pour le calcul de cette mesure, nous nous avons utilis&#233; le module 
</p>
<p>Perl WordNet::Similarity v0.06 (Patwardhan, Pedersen, 2003). 
</p>
<p>La similarit&#233; entre un sens et un synset est plus pr&#233;cis&#233;ment d&#233;finie comme la moyenne des 
</p>
<p>similarit&#233;s entre les mots composant le sens, ou une partie de ceux-ci, et le synset. La similari-
</p>
<p>t&#233; entre un mot et un synset est elle-m&#234;me donn&#233;e par la plus forte des similarit&#233;s entre le syn-
</p>
<p>set et les synsets auxquels le mot consid&#233;r&#233; appartient, celles-ci reposant sur (1). Un sens est 
</p>
<p>affect&#233; au synset qui lui est le plus similaire, &#224; condition toutefois que la similarit&#233; entre les 
</p>
<p>deux soit sup&#233;rieure &#224; un seuil (&#233;gal ici &#224; 0,25 comme dans (Pantel, Lin, 2002)). Finalement, 
</p>
<p>la pr&#233;cision pour un mot est donn&#233;e par le rapport entre le nombre de ses sens s&#8217;appariant 
</p>
<p>avec un de ses synsets et le nombre total de ses sens. 
</p>
<p> LAT-1.no LAT-2.no 
</p>
<p>nombre de liens forts 19,4 20,8 
</p>
<p>choix optimum 56,2 63,7 
</p>
<p>Tableau 2 : Pr&#233;cision moyenne des sens d&#233;couverts pour l&#8217;anglais par rapport &#224; WordNet 
</p>
<p>Le tableau 2 donne le r&#233;sultat de l&#8217;&#233;valuation de notre algorithme de d&#233;couverte de sens pour 
</p>
<p>les mots du r&#233;seau de cooccurrences anglais qui ne sont que des noms et qui ont au moins un 
</p>
<p>sens. Deux mesures sont donn&#233;es. Comme Pantel et Lin, nous ne prenons en compte que 4 
</p>
<p>mots de chaque sens pour l&#8217;&#233;valuation. Mais contrairement &#224; eux, nous n&#8217;avons pas de mesure 
</p>
<p>sp&#233;cifique de la proximit&#233; des mots d&#8217;un sens par rapport au mot qu&#8217;il d&#233;crit. Nous donnons 
</p>
<p>donc la pr&#233;cision moyenne obtenue en choisissant les 4 mots d&#8217;un sens ayant le plus grand 
</p>
<p>nombre de liens forts (les crit&#232;res de fr&#233;quence ou de coh&#233;sion dans le r&#233;seau de cooccurren-
</p>
<p>ces donnent les m&#234;mes r&#233;sultats) et celle obtenue en choisissant les 4 mots d&#8217;un sens permet-
</p>
<p>tant d&#8217;avoir un score maximal. Nous constatons &#224; l&#8217;&#233;vidence un &#233;cart important entre ces deux 
</p>
<p>mesures : la pertinence des sens distingu&#233;s est comparable &#224; celle obtenue par Pantel et Lin 
</p>
<p>lorsque le choix des 4 mots repr&#233;sentatifs d&#8217;un sens est optimal (Pantel et Lin obtiennent une 
</p>
<p>pr&#233;cision de 60,8 pour un nombre de mots par sens &#233;gal &#224; 14) mais les mots choisis pour re-
</p>
<p>pr&#233;senter un sens dans notre cas ne sont pas fortement li&#233;s dans WordNet (selon la mesure de 
</p>
<p>Lin) au mot caract&#233;ris&#233; par ce sens. Cela ne signifie d&#8217;ailleurs pas que ces mots ne soient pas 
</p>
<p>int&#233;ressants pour d&#233;crire un sens mais plus s&#251;rement que leur lien avec lui repose sur des rela-
</p>
<p>tions s&#233;mantiques autres que l&#8217;hyperonymie. Le meilleur r&#233;sultat obtenu par Pantel et Lin sur 
</p>
<p>ce point s&#8217;explique par le fait que la base de leur m&#233;thode est le regroupement de mots simi-
</p>
<p>laires et non la classification des cooccurrents d&#8217;un mot, lesquels ne comportent pas beaucoup 
</p>
<p>de synonymes de leur mot source. Enfin, il est &#224; noter que leur corpus de d&#233;part est beaucoup 
</p>
<p>plus large (de l&#8217;ordre de 144 millions de mots) et qu&#8217;ils font appel &#224; des moyens d&#8217;analyse 
</p>
<p>plus &#233;labor&#233;s, en l&#8217;occurrence un analyseur syntaxique. </p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>D&#233;couvrir des sens de mots 
</p>
<p>Sans surprise, les r&#233;sultats obtenus avec les cooccurrences d&#8217;ordre 1 (LAT-1.no), qui condui-
</p>
<p>sent &#224; un nombre de sens plus important, sont inf&#233;rieurs &#224; ceux obtenus avec les cooccurren-
</p>
<p>ces d&#8217;ordre 2 (LAT-2.no). En l&#8217;absence de rappel, il est n&#233;anmoins difficile d&#8217;en tirer une 
</p>
<p>conclusion claire : il est probable que des sens se trouvent divis&#233;s de fa&#231;on artificielle dans le 
</p>
<p>cas de LAT-1.no mais ce ph&#233;nom&#232;ne peut simultan&#233;ment masquer la couverture d&#8217;un plus 
</p>
<p>grand ensemble de sens effectifs permise par la meilleure homog&#233;n&#233;it&#233; des classes form&#233;es. 
</p>
<p>7 Discussion 
</p>
<p>De par sa nature, notre m&#233;thode se compare le plus directement &#224; (V&#233;ronis, 2003) et (Dorow, 
</p>
<p>Widdows, 2003). Malgr&#233; une proximit&#233; d&#8217;approche g&#233;n&#233;rale avec (Rapp, 2003), la distance 
</p>
<p>avec ce dernier est plus grande car il ne repose pas sur la d&#233;tection de composantes de forte 
</p>
<p>densit&#233; dans un graphe de cooccurrence. (V&#233;ronis, 2003) et (Dorow, Widdows, 2003) ne pr&#233;-
</p>
<p>sentant pas d&#8217;&#233;valuation formelle, seule une comparaison qualitative est possible. Deux diff&#233;-
</p>
<p>rences principales sont &#224; noter avec notre travail. La premi&#232;re est l&#8217;utilisation directe du gra-
</p>
<p>phe de cooccurrence. Nous avons opt&#233; pour notre part pour une approche plus g&#233;n&#233;rale en 
</p>
<p>travaillant au niveau d&#8217;un graphe de similarit&#233; : lorsque la similarit&#233; entre deux mots est don-
</p>
<p>n&#233;e par leur relation de cooccurrence, notre situation est la m&#234;me que celle des travaux cit&#233;s 
</p>
<p>mais nous pouvons prendre en compte dans le m&#234;me cadre des relations de similarit&#233; plus 
</p>
<p>g&#233;n&#233;rales, telles que les cooccurrences de second ordre. La seconde diff&#233;rence est l&#8217;utilisation 
</p>
<p>d&#8217;une proc&#233;dure it&#233;rative de distinction des sens. Cette proc&#233;dure consiste &#224; s&#233;lectionner &#224; 
</p>
<p>chaque &#233;tape le sens se d&#233;tachant le plus clairement puis &#224; actualiser le graphe de cooccur-
</p>
<p>rence en en &#233;liminant les constituants du sens form&#233;, ce qui permet de faire appara&#238;tre plus 
</p>
<p>distinctement les sens r&#233;siduels. Nous avons pr&#233;f&#233;r&#233; quant &#224; nous mettre l&#8217;accent sur la possi-
</p>
<p>bilit&#233; de r&#233;unir des sens tr&#232;s proches, voire identiques, artificiellement s&#233;par&#233;s par la seule 
</p>
<p>utilisation de formes de surface (cf.  section 4.3). Plus globalement, les deux diff&#233;rences poin-
</p>
<p>t&#233;es ont pour cons&#233;quence principale de conduire &#224; des distinctions de sens plus fines que 
</p>
<p>celles que nous mettons en &#233;vidence. Cependant, les m&#233;thodes de d&#233;couverte de sens &#224; partir 
</p>
<p>de corpus ayant plut&#244;t tendance &#224; distinguer un trop grand nombre de sens proches, il nous a 
</p>
<p>sembl&#233; plus important de favoriser la mise en &#233;vidence de sens stables et nettement d&#233;limit&#233;s 
</p>
<p>que de rechercher une tr&#232;s grande finesse dans les distinctions de sens r&#233;alis&#233;es. 
</p>
<p>8 Conclusion et perspectives 
</p>
<p>Nous avons pr&#233;sent&#233; dans cet article une nouvelle m&#233;thode pour diff&#233;rencier et caract&#233;riser le 
</p>
<p>sens des mots &#224; partir d&#8217;un r&#233;seau de cooccurrences lexicales. Cette m&#233;thode applique un al-
</p>
<p>gorithme de classification non supervis&#233;, l&#8217;algorithme SNN, aux cooccurrents des mots dont 
</p>
<p>on veut diff&#233;rencier les sens en se fondant sur les relations que ces cooccurrents entretiennent 
</p>
<p>dans le r&#233;seau. Nous en avons r&#233;alis&#233; une premi&#232;re &#233;valuation suivant le protocole d&#233;fini dans 
</p>
<p>(Pantel, Lin, 2002), &#233;valuation montrant que la pertinence des sens form&#233;s est comparable &#224; 
</p>
<p>celle des sens form&#233;s par Pantel et Lin. Cette &#233;valuation doit cependant &#234;tre approfondie. Il 
</p>
<p>semble en particulier n&#233;cessaire de s&#8217;appuyer sur une mesure de similarit&#233; entre synset et sens 
</p>
<p>form&#233; permettant de prendre en compte un ensemble plus vaste de relations s&#233;mantiques telles 
</p>
<p>que celles implicitement pr&#233;sentes dans les &#171; glosses &#187; associ&#233;es aux synsets. Par ailleurs, une 
</p>
<p>&#233;valuation au travers d&#8217;une utilisation dans une t&#226;che telle que l&#8217;expansion de requ&#234;tes nous 
</p>
<p>semble &#233;galement n&#233;cessaire afin de juger de l&#8217;apport v&#233;ritable de ce type de ressource par 
</p>
<p>rapport &#224; une ressource de type WordNet. </p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Olivier Ferret 
</p>
<p>R&#233;f&#233;rences 
</p>
<p>AGIRRE E., LOPEZ DE LACALLE O. (2003), Clustering WordNet Word Senses, Actes de 
</p>
<p>RANLP 2003.
</p>
<p>CHURCH K.W., HANKS P. (1990), Word Association Norms, Mutual Information, And Lexi-
</p>
<p>cography, Computational Linguistics, Vol. 16(1), pp. 177-210. 
</p>
<p>DOROW B., WIDDOWS D. (2003), Discovering Corpus-Specific Word Senses, Actes de 
</p>
<p>EACL 2003, pp. 79-82. 
</p>
<p>ERT&#214;Z L., STEINBACH M., KUMAR V. (2001), Finding Topics in Collections of Documents: A 
</p>
<p>Shared Nearest Neighbor Approach, Actes de Text Mine&#8217;01, Workshop of the 1
st
 SIAM Inter-
</p>
<p>national Conference on Data Mining.
</p>
<p>HARABAGIU S, MILLER G.A., MOLDOVAN D (1999), WordNet 2 - A Morphologically and 
</p>
<p>Semantically Enhanced Resource, Actes de SIGLEX&#8217;99, pp. 1-8. 
</p>
<p>DE LOUPY C., EL-B&#200;ZE M. (2002), Managing Synonymy and Polysemy in a Document Re-
</p>
<p>trieval, Actes de LREC 2002 Workshop on Creating and Using Semantics for Information 
</p>
<p>Retrieval.
</p>
<p>MILLER G.A. (1995), WordNet: A lexical Database, Communications of the ACM.
</p>
<p>MIHALCEA R., MOLDOVAN D. (2001), eXtended WordNet: Progress Report, Actes de NAACL 
</p>
<p>2001 Worshop on WordNet and Other Lexical Resources, pp. 95-100. 
</p>
<p>PASCA M AND HARABAGIU S. (2001), The informative role of WordNet in Open-Domain 
</p>
<p>Question Answering, Actes de NAACL 2001 Worshop on WordNet and Other Lexical Re-
</p>
<p>sources, pp. 138-143. 
</p>
<p>PANTEL P., LIN D. (2002), Discovering Word Senses from Text, Actes de ACM SIGKDD 
</p>
<p>Conference on Knowledge Discovery and Data Mining 2002, pp. 613-619. 
</p>
<p>PATWARDHAN S., PEDERSEN T. (2003), WordNet::Similarity, http://www.d.umn.edu/ 
</p>
<p>~tpederse/similarity.html. 
</p>
<p>PEDERSEN T., BRUCE R. (1997), Distinguishing Word Senses in Untagged Text, Actes de 
</p>
<p>EMNLP'97, pp. 197-207. 
</p>
<p>PURANDARE A. (2003), Discriminating Among Word Senses Using Mcquitty's Similarity 
</p>
<p>Analysis, Actes de HLT-NAACL 03 - Student Research Workshop.
</p>
<p>RAPP R. (2003), Word Sense Discovery Based on Sense Descriptor Dissimilarity, Actes de 
</p>
<p>Machine Translation Summit IX.
</p>
<p>SCH&#220;TZE H. (1998), Automatic Word Sense Discrimination, Computational Linguistics,
</p>
<p>Vol. 24(1), pp. 97-123. 
</p>
<p>VERONIS J. (2003), Cartographie lexicale pour la recherche d&#8217;information, Actes de 
</p>
<p>TALN 2003, pp. 265-274. </p>

</div></div>
</body></html>