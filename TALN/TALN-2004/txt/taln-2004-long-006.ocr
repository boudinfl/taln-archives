TALN 2004, Fés, 19-21 avril 2004

Désambiguisation par proximité structurelle

Bruno Gaume (1), Nabil Hathout (2) & Philippe Muller (1)
(1) IRIT — CNRS, UPS & INPT
{gaume,muller} @irit.fr
(2) ERSS — CNRS & UTM
hathout@univ—tlse2.fr

Résumé - Abstract

L’article présente une méthode de désambiguisation dans laquelle le sens est déterminé en uti-
lisant un dictionnaire. La méthode est basée sur un algorithme qui calcule une distance « sé-
mantique >> entre les mots du dictionnaire en prenant en compte la topologie complete du dic-
tionnaire, vu comme un graphe sur ses entrées. Nous l’avons testée sur la désambiguisation des
déﬁnitions du dictionnaire elles-memes. L’article présente des résultats préliminaires, qui sont
tres encourageants pour une méthode ne nécessitant pas de corpus armoté.

This paper presents a disambiguation method in which word senses are determined using a
dictionary. We use a semantic proximity measure between words in the dictionary, taking into
account the whole topology of the dictionary, seen as a graph on its entries. We have tested the
method on the problem of disambiguation of the dictionary entries themselves, with promising
results considering we do not use any prior armotated data.

Mots-clefs — Keywords

Désambiguisation sémantique, réseaux petits mondes hiérarchiques, dictionnaires.
Word sense desambiguation, hierarchical small words, dictionaries.

Bruno Gaume, Nabil Hathout & Philippe Muller

1 Introduction

De nombreuses taches impliquant le traitement de donnees en langue naturelle sont rendues
difﬁciles par l’existence de sens differents pour un meme item lexical : traduction automatique,
recherche de documents ou extraction d’informations. Ce probleme, tres ancien en TAL, est
loin d’etre resolu, et l’evaluation de ses methodes est difﬁcile et relativement recente, pour des
raisons presentees notamment dans (Resnik & Yarowsky, 2000). On peut distinguer plusieurs
familles d’approches1, selon que le sens d’un mot en contexte est determine en apprenant auto-
matiquement les caracteristiques du contexte qui determine ce sens (de facon supervisee, ou non
supervisee, quand l’etude des contextes sert elle-meme a degager des familles d’usage) ou bien
que le sens soit determine en utilisant des ressources lexicales << exterieures >> : dictionnaires,
thesaurus. Le premier type d’approche necessite des donnees volumineuses difﬁciles a anno-
ter (pour les approches supervisees; les approches non supervisees sont par ailleurs sensibles
au corpus choisi, qui doit etre representatif). Le deuxieme type d’approche tente d’utiliser la
connaissance lexicale rassemblee dans les dictionnaires, les thesaurus (WordNet, par exemple),
avec une longue tradition, (Lesk, 1986; Banerjee & Pedersen, 2003) et des resultats mitiges.
Dans tous les cas, on cherche a etablir une relation de distance entre mots, susceptible de de-
terminer un sens en contexte. Dans le cas des dictionnaires, les seules methodes ayant presente
des resultats chiffres se concentrent seulement sur les mots qui apparaissent dans la deﬁnition
d’un mot ciblez.

Nous presentons ici un algorithme qui utilise un dictionnaire comme source d’information sur
les relations entre items lexicaux (cf section 3). L’algorithme calcule une distance << seman-
tique >> entre les mots du dictionnaire en prenant en compte la topologie complete du diction-
naire, ce qui lui donne une plus grande robustesse. Nous avons commence a tester cette approche
sur la desambiguisation des deﬁnitions du dictionnaire elles-memes (section 2), mais nous mon-
trons pourquoi cette methode est plus generale. La section 6 presente nos resultats preliminaires,
qui sont tres encourageants pour une methode ne necessitant pas de corpus armote (en dehors
de l’evaluation), et qui comporte de nombreux parametres d’ajustement.

2 Le graphe du dictionnaire

L’ idee de base de notre methode est de considerer qu’un dictionnaire est un graphe non oriente
dont les mots sont les sommets et tel qu’il existe un arc entre deux sommets si l’un apparait dans
la deﬁnition de l’autre. Plus precisement, le graphe du dictionnaire encode deux types d’infor-
mations lexicographiques : les deﬁnitions qui decrivent les differentes acceptions de chaque
vedette au moyen de sequences langagieres; la structure des articles qui organise ces sous-
sens3. Deux types de sommets sont ainsi necessaires : les sommets-w qui representent les mots
qui apparaissent dans les deﬁnissants, et les sommets-A qui correspondent aux sous-sens des
Vedettes. La construction du graphe se fait en trois temps :

1. Pour chaque Vedette, on cree un sommet-A qui correspond a l’article entier et autant
de sommets-A qu’il y a de sous-sens pour lesquels il existe un deﬁnissant. On cree un

1On peut se referer au numero special de Computatiormal Linguistics de 1998 et son introduction (Ide &
Veronis, 1998); cf. aussi (Manning & Schiitze, 1999, chap. 7).
2On peut citer aussi les propositions non quantiﬁees de (H.Kozima & Furugori, 1993).
3Nous adoptons ici la terminologie de (Martin, 1983) et (Henry, 1996)

Désambiguilsation par proximité structurelle

arc eI1tre chaque sommet-A et les sommets-A qui representent des sous-sens de niveau
immediatement inférieur.

2. Pour chaque mot qui apparait dans un déﬁnissant du dictionnaire, on cree un sommet-
w. On crée un arc eI1tre chaque couple de sommets (w, A) si le mot representé par le
sommet-w apparait dans le deﬁnissant du sous-sens correspondant au sommet-A.

3. On crée un arc entre chaque couple de sommets (w, A) si le sommet-A represente l’article
dont la vedette est le mot correspondant au sommet-w.

Considérons, a titre d’exemple, l’article de << daim, n. m. » (issue du dictionnaire Le Robert) :

1. Mammifere ruminant ongulé.
2. [a] Peau preparee de cet animal.
[b] Cuir suede (veau retourné).
3. Come de daim [...]
4. Bellatre.

Le graphe contiendra un premier sommet (appelons le A0) qui represente l’article dans sa to-
talite. A0 est relié par un arc a chacun des sommets A1, A2, A3et A4 qui representent respec-
tivement les sous-sens 1., 2., 3. et 4. A son tour A2 est connecte a deux sommets A2_1 et A22
correspondant aux sous-sens 2.[a] et 2.[b]. Le graphe contient ensuite des arcs qui vont de A1
vers trois sommets wl, ’LU2 et w3 qui representent les mots mammifere, ruminant et ongule’. En-
ﬁn, il y aura un arc entre wl et le sommet-A qui représente l’article de << mammifere, adj. et
I1. >>, etc.

L’experience que nous présentons a éte réalisee au moyen d’un graphe construit a partir de
deﬁnitions issues du dictionnaire Le Robert. Ce graphe est restreint aux seuls substantifs : il
n’inclut que des déﬁnissants de vedettes nominales dans lesquelles n’ont éte conservées que les
occurrences nominales.

Dans les articles, les sous-sens s’inscrivent dans des structures hierarchiques qui peuvent com-
porter jusqu’a cinq niveaux : 1, 2, 3... pour les homographes; I, II, III...; A, B, C...; 1, 2, 3...
et a, b, c... pour les acceptions. Les positions de ces sous-sens peuvent ainsi etre representees
de maniere uniforme au moyen de sequences de cinq nombres correspondant aux cinq niveaux.
Par exemple, le sous-sens 2.[a] de l’article de daim est decrit par 0_0_0_2_1 (les trois premiers
niveaux n’étant pas utilises, ils sont représentes par des zeros).

3 PROX : une méthode pour la mesure de similarité lexicale

PROX est une méthode stochastique pour l’etude de la structure des réseaux petits mondes
hierarchiques (voir section suivante). Cette méthode consiste a transformer un graphe en une
chaine de Markov dont les états sont les sommets du graphe en question et ses arétes les tran-
sitions possibles : une particule en partant a l’instant t = 0 d’un sommet 30, se deplace en
un pas sur 31 l’un des voisins de so sélectionné aléatoirement; la particule se deplace alors a
nouveau en un pas sur 32, l’un des voisins de 31 sélectionne aléatoirement etc. Si au t-ieme
pas la particule est sur le sommet st elle se deplace alors en un pas sur le sommet s,;+1 qui est
sélectionné aléatoirement parmi les voisins de 3,; tous equiprobables. Une trajectoire 31, 32, 

Bruno Gaume, Nabil Hathout & Philippe Muller

st,  ainsi sélectionnée est une << balade >> aléatoire sur le graphe, et ce sont les dynamiques de
ces trajectoires qui nous donnent les propriétes structurelles des graphes étudiés.

Posons PROX(G, 7', 7', 3) la probabilité qu’en partant a l’instant t = 0 du sommet 7' la particule
soit a l’instant t = 7' sur le sommet 3 :

1. Un graphe non oriente G = (V, E) est la donnée d’un ensemble non vide ﬁni V de
sommets, et d’un ensemble E de paires de sommets formant des aretes. Si l’arete {7', 3} E
E on dit que les sommets 7' et 3 sont voisins, le nombre de voisins d’un sommet 7' est d(7')
son degré d’incidence;

2. Soit un Graphe a n sommets G = (V, E), on notera [G] la Matrice carrée n X n telle que
pour tout 7', 3 E V X V, [G]r,s = 1 si {7', 3} E E et [G]r,s = 0 si {7', 3} E‘ E; On appellera
[G] la matrice d’adjacence de G. C’est-a-dire que [G]T,s (la valeur située a la 7'-ieme ligne
et la 3-ieme colone de la matrice [G]) est égale a 1 s’il existe une arete eI1tre les sommet
7' et 3, 0 sinon.

3. Soit G = (V, E) un graphe a n sommets. Posons  la matrice n X n de transition de la
chaine de Markov homogene dont les états sont les sommets du graphe en question telle
que la probabilité de passer d’un sommet 7' E V a l’instant 7' vers un sommet 3 E V a
l’instant 2' + 1 est égale a :

[G]T,s=0 si {7', 3} E E (3 n’est pas un voisin de 7')  = 1/d(7') si {7', 3} E E (3 est un
des d(7') voisins de 7' qui sont tous équiprobables)

Nous dirons que  est la matrice Markovienne du graphe G et que G est le graphe des
transitions possibles de cette chaine de Markov.

4. Soit G = (V, E) un graphe réﬂexif a n sommets et  sa matrice Markovienne, pour tout
7',s E V X V,onadonc:

PROX(G, 7, 7', 3) = [(“;7'].,s

on Al est la matrice A multipliée 7' fois par elle-meme.

C’est-a-dire que pour tout 7', 3, PROX(G, 7', 7', 3) est la probabilité que la particule en partant
du sommet 7' a l’instant t = 0 soit a l’instant t = 7' sur le sommet 3 quand elle se déplace
aléatoirement de sommet en sommet dans le graphe en empruntant les arétes du graphe.

Si PROX(G, 7', 7', 3) > PROX(G, 7', 7', u) cela veut donc dire que dans sa trajectoire la particule
en partant du sommet 7', a plus de chance d’étre a l’instant7' sur le sommet 3 que sur le sommet
u, et c’est la structure du graphe qui determine ces probabilités.

PROX construit ainsi une mesure de similarite entre sommets d’un graphe, en << rapprochant »
les sommets d’une meme zone dense4 en aretes, ce qui permet d’envisager une exploitation
originale et novatrice des dictionnaires électroniques (Gaume et al., 2002) avec un outil de vi-
sualisation du sens (Gaume & Ferré, 2004), mais aussi de construire des outils pour le TAL, par
exemple la désambiguisation des entrees dans un dictionnaire. Pour une presentation détaillée
de PROX voir (Gaume, a paraitre).

4En effet plus il existe un grand nombre de chemins courts entre deux sommets 7' et s, plus la probabilité
PROX(G, 7', 7', s) que la particule en partant du sommet 7' a 1’instant t = 0 soit a 1’instant t = 7' sur le sommet s est
grande.

Désambigufsation par proximite structurelle

4 Les graphes de dictionnaires sont des petits mondes hierar-
chiques

Des recherches recentes en theorie des graphes ont mis au jour un ensemble de caracteris-
tiques statistiques que partagent la plupart des grands graphes de terrain; ces caracteristiques
deﬁnissent la classe des graphes de type << reseaux petits mondes hierarchiques » (RPMH; en
anglais hierarchical small world) (Watts & Strogatz, 1998; Newman, 2003). Les RPMH pre-
sentent quatre proprietes fondamentales :

D : ils sont peu denses, c’est-a-dire qu’ils ont relativement peu d’aretes au regard du nombre
de leurs sommets;

L : la moyenne des plus courts chemins entre les sommets est petite;

C : le taux de clustering ou d’agregation, est deﬁni de la maniere suivante : Supposons qu’un
sommet S ait Ks voisins, alors il y a Ks(Ks-1)/2 aretes au maximum qui peuvent exister
entre ses Ks voisins (ce qui arrive quand chacun des voisins de S est connecte a tous les
autres voisins de S). Soit As le nombre d’aretes qu’il y a entre les voisins de S (ce nombre
est donc necessairement plus petit ou egal a Ks(Ks-1)/2). Posons Cs: As/( Ks(Ks-1)/2)
qui est donc pour tout sommet S inferieur ou egal a un. Le C d’un graphe est la moyenne
des Cs sur ses sommets. Le C d’un graphe est donc toujours compris eI1tre 0 et 1. Plus le
C d’un graphe est proche de 1, plus il forme des agregats ou clusters (des zones denses
en aretes). Dans un RPMH le C est fort, deux voisins d’un meme sommet ont tendance a
etre connectes entre eux par une arete (<< mes amis sont amis eI1tre eux >>). Par exemple,
sur Internet5, deux pages qui sont liees a une meme page ont une probabilite relativement
elevee d’inclure des liens l’une vers l’autre;

I : la distribution des degres d’incidence des sommets suit une loi de puissance (power law) :
certains noeuds tres peu nombreux ont beaucoup plus de voisins que d’autres plus nom-
breux, eux-memes ayant plus de voisins que d’autres qui eux-memes... La probabilite
P(k) qu’un sommet du graphe considere ait k voisins decroit comme une loi de puis-
sance P(k) = k ".

Le tableau 1 presente une comparaison des RPMH avec d’autres types de graphes pour ces diffe-
rentes caracteristiques : des graphes aleatoires (construit en partant d’un ensemble de sommets
isoles, puis en ajoutant aleatoirement un certains nombre determine d’aretes entre ses sommets),
et des graphes reguliers (des graphes classiquement etudies en theorie des graphes, dont tous
les sommets ont le meme degre d’incidence).

Les graphes d’origine linguistique et notamment ceux qui sont construits a partir de diction-
naires sont de type RPMH. Par exemple le graphe G1 des noms construit a partir du diction-
naire Le Robert (les sommets sont les entrees qui sont des noms, et il existe une arrete entre
deux sommets si l’un est dans la deﬁnition de l’autre - on ne tient pas compte ici de la structure
hierarchique des deﬁnitions) est un RPMH typique (voir table 2) . Dans le graphe G2 qui est
construit comme indique a la section 2, chaque sommet est remplace par l’arbre reﬂetant la
structure hierarchique de l’entree qui lui correspond, ce qui a pour consequence d’affaiblir le C
et d’allonger le L. Dans le tableau ci-dessous, * indique que les mesures sont calculees sur la
plus grande partie connexe.

5Les sommets en sont les 800 millions de pages disponibles sur intemet, et une arete est tracee e11tre A et B si
un lien hypertexte vers la page B apparait dans la page A ou si un lien hypertexte vers la page A apparait dans la
page B.

Bruno Gaume, Nabil Hathout & Philippe Muller

c‘z densite’ égale L : Moyenne des C : Taux de cluste- I : distribution des
plus courts chemins ring degrés d’incidences

Graphes aléatoires L petit C petit loi de Poisson
(chemins courts) (pas d’agrégats)

Graphes de terrain L petit C grand loi de puissance

(RPMH) (chemins courts) (des agrégats)

Graphes réguliers L grand C grand constante
(chemins longs) (des agrégats)

TAB. 1 — Comparaison de trois types de graphes en fonction des parametres L, C et I.

Graphe Nb. sommets Nb. arcs Nb. sommets* Nb. Arcs* DiaInétre* C* L*
G1 51 559 392161 51 511 392142 7 0,182 9 3,32
G2 140 080 399 969 140 026 399 941 11 0,008 1 5,21

TAB. 2 — Quelques caractéristiques des graphes G1 et G2

Nous pensons que la nature hiérarchique des dictionnaires (distribution des degrés d’incidence
des sommets en loi de puissance) est une conséquence du role de l’hyperonymie associée a
la polysémie de certains sommets, alors que le fort C (existence de zones denses en arétes)
reﬂéte le role de la cohyponymieé (Duvignau, 2002; Duvignau, 2003; Gaume et al., 2002). Par
exemple, le mot corps se trouve dans de nombreux déﬁnissants (téte, chimie, peau, division). De
ce fait, le sommet corps a une forte incidence. D’autre part on constate qu’il existe de nombreux
triangles par exemple : {e’c0rce, enveloppe}, {e’c0rce, peau}, {peau, enveloppe}, ce qui favorise
les zones denses en arétes et plus précisément un fort taux de clustering C. Ce sont ces zones
denses en arétes qui orientant la dynamique des trajectoires de la particule vont permettre la
désambiguisation.

5 Un algorithme de désambiguisation basé sur PROX

Nous allons maintenant présenter une méthode pour désambiguiser une entrée de dictionnaire
en utilisant la notion de distance sémantique introduite plus haut. On peut déﬁnir la tache comme
suit : on considére un lemme ac qui apparait dans la déﬁnition de l’un des sens d’un mot, consi-
déré comme un noeud du graphe, B. Nous voulons donc associer ac avec le sense le plus probable
qu’il a dans ce contexte. Chaque entrée du dictionnaire est codé par un arbre de sous-sens dans le
graphe du dictionnaire, avec une liste de nombres correspondants a chaque niveau de sous-sens
caractéristique.

“Par exemple « enveloppe », « peau », « bogue », « écaﬂle », « épluchure », « écorce », « vernis », « croﬁte »,
« enduit », « faux-semblant », « aspect», « apparence », « manteau », « fourrure », « toison », « pelure »
sont rattachés a un méme concept ENVELOPPE-APPARENCE constituant pour chacun d’entre eux, un noyau dc
sens commun. De tels mots constituent, de ce fait, des co-hyponymes, dont on peut distinguer deux types : - les
co-hyponymes intra domaine : [« broue », « bogue », « cosse », ] ou [« pelage », « toison», « fourure », ] ou
encore [« robe », « habit», « vétement », ] qui relévent d’un méme domaine, a savoir respectivement dans ces
exemples : VEGETAL ou ANIMAL ou HUIVIAIN. - les co-hyponymes inter-domaines : « écorce » et « pelage »
sont des co-hyponymes inter-domaines car ils relévent de domaines différents, respectivement 1e VEGETAL et
1’ANI1VIAL. Le point commun de tous les hyponymes c’est1eur potentialité a pouvoir exprimer la méme idée en
« intension ». C’est pourquoi ils peuvent étre considérée comme co-hyponymes.

Désamb1'gui'sat1'on par proximité structurelle

Soit G = (V, E) un graphe a n sommets construit comme présente section 2. L’algorithme
suivant a éte applique.

1. on supprime les voisins de B dans G Vac E V[G]g,w = [G]w,g = 0;
2. on calcule  ; nous avons pris 2' = 6 (cf. l’explication plus bas) :
3. soit L, le vecteur ligne de B alors Vk, L[k] = k ;

4.

Soit F = {$1, $2, ..., awn} les noeuds correspondant a tous les sous-sens de la deﬁnition de
04.

On prend alors wk = argmaacwe

Nous avons alors que wk est le sous-sens le plus << proche >> du noeud 3, par rapport a la mesure
Prox. Deux étapes demandent un peu plus d’explication :

1. les voisins sont supprimes pour ne pas laisser un biais favorable aux sous-sens de 3, qui
formeraient alors une sorte de cluster artiﬁciel par rapport a la tache donnee. Ainsi la
« marche aléatoire >> dans le graphe peut vraiment avoir lieu dans le graphe plus general
des autres sens.

2. choisir une bonne valeur pour la longueur de la marche aléatoire n’est pas simple, et est le
facteur essentiel de la réussite de la procedure. Si elle est trop petite, seules les relations
locales vont apparaitre (synonymes proches, etc) et ils peuvent ne pas apparaitre dans les
contextes a désambi gu'1'ser (c’est notamment le probleme de la méthode de (Lesk, 1986)) ;
si la valeur de 2' est trop grande par contre, les << distances >> entre tous les mots tendent
a converger vers une constante, faisant disparaitre les differences. Cette valeur doit donc
etre reliee d’une facon ou d’une autre a la distance moyenne entre deux sens quelconques
du graphe. Une hypothese raisonable est donc de rester proche de cette valeur, et nous
avons donc pris le nombre 6, la moyenne calculée etant de 5,21 (sur le graphe contenant
tous les sous-sens, pas sur celui-ne contenant que les entrees, pour lequel L = 3,3) 7.

6 Evaluation

Pour chaque couple de sommets (a, B) E V X V tel que ﬂ représente un déﬁnissant A et a
le lemme d’une forme qui apparait dans A, l’algorithme precedent propose un sommet 7 tel
que 7 appartient a la structure hiérarchique de l’article dont le mot-vedette est 04 et tel que 7
permet d’identiﬁer le sous-sens principals de a qui semantiquement est le plus proche de son
occurrence dans A.

L’évaluation de la desambiguisation semantique a éte realisee comme suit : Nous avons se-
lectionne aléatoirement 27 déﬁnissants de substantifs dans le dictionnaire Le Robert. Deux per-
sonnes ont armote semantiquement les formes nominales qui y apparaissent. 82 triplets ont ainsi
eté constitués, dont il est resté 72 apres avoir eliminé les mots ayant un seul sens dans le dic-
tionnaire. Nous avons constate que les desaccords eI1tre les deux armotateurs ont éte tres rares
et qu’un consensus a pu etre trouve rapidement dans les cas litigieux. Parallelement, nous avons

7La valeur de L est calculée en appliquant une variante de 1’algorithme de Dijkstra partant d’un noeudvers tous
les autres, répétée pour chaque noeuddu graphe.

8Le sous-sens principal correspond a la premiere sous-division hiérarchique d’une entrée, choisie parmi I, II ou
III, ou bien A, B,  suivant les cas, cela n’étant pas homogene dans le dictionnaire.

Bruno Gaume, Nabil Hathout & Philippe Muller

applique l’algorithme precedent aux 72 couples formes par les deux premiers elements des tri-
plets armotes. Nous avons ensuite compare les resultats de l’algorithme avec les armotations
manuelles. Ont ete comptes comme corrects les solutions telles que le numero d’homographie
et le numero de la premiere division hierarchique sont identiques a ceux qui ont ete proposes par
les armotateurs. C’est le cas par exemple pour les couples des deux premieres lignes du tableau
suivant :

B a 7 atmotateurs
correct bal#n._m.*0_0_0_3_0 lieu 1_1_0_3_0 1_1_0_1_0
correct van#n._m.* _0_0_0_0 voiture 0_2_0_0_0 0_2_0_3_0
erreur phonetisme#n._m.* _0_0_0_0 moyen 1_1_0_1_0 2_0_0_1_0
erreur creativite#n._f.*0_0_0_0_0 pouvoir 2_0_0_3_0 2_0_0_1_0
erreur acme#n._m._ou_f.* _0_0_1_0 phase 0_0_0_1_0 0_0_0_4_0

Pour les trois derniers couples, l’algorithme a propose des solutions erronees : mauvais numero
d’homographe et / ou mauvais sous-sens principal.

Pour avoir une idee de la difﬁculte de la tache, nous avons aussi calcule la moyenne des sous-
sens principaux sur les entrees considerees, la moyenne du nombre d’homographes ayant la
meme categorie grammaticale (nom commun) et la moyenne des sous-sens de niveau le plus ﬁn
des entrees considerees. Les resultats sont resumes dans la table 3. Le score de desambiguisation

hasard algorithme
homographes 0,5 0,8 (8/10)
polysemie principale 0,37 0,542 (39/72)
polysemie ﬁne 0,125 0,292 (21/72)

TAB. 3 — Premiers resultats de l’evaluation de l’algorithme, avec une baseline aleatoire

des homographes n’est pas tres signiﬁcatif VL1 le petit nombre releve dans les entrees choisies.
Nous pouvons remarquer que les autres scores, sans etre tres bons, sont plutot encourageants.
Pour donner une idee de leur valeur, (Banerjee & Pedersen, 2003) applique des notions de
distance lexicale variees issues de dictionnaire, appliquees a la desambiguisation (en anglais)
de mots selectionnes, avec des resultats qui vont de 0,2 a 0,4 par rapport aux sous-sens fourI1is
par WordNet (et une moyenne de sous-sens par noms qui equivaudrait a 0,2 pour le score au
hasard).

7 Conclusion

Nous avons presente ici un algorithme donnant une mesure de similarite lexicale a partir d’un
dictionnaire general. Cet algorithme est non supervise. Il ne necessite pas de corpus armote
et n’utilisant pas d’autres donnees qu’un dictionnaire general dont la couverture lexicale est
la seule restriction sur le vocabulaire. La methode donne des resultats prometteurs pour la
desambiguisation sur les noms seuls. Nous envisageons bien sﬁr d’etendre les tests a d’autres
categories grammaticales, mais aussi d’afﬁner la methode pour les substantifs en considerant
par exemple egalement les occurrences Verbales dans les deﬁnissants des noms. Pour etendre
cette methode au cas general de la desambiguisation, nous pensons par ailleurs considerer un

Désamb1'gui'sat1'on par proximité structurelle

contexte qui contient un mot £1 désambiguiser comme une deﬁnition virtuelle que l’on ajouterait
au graphe des mots pour appliquer ensuite exactement la meme méthode. Nous envisageons
egalement de réaliser des mesures plus ﬁnes des performances en tenant compte des degres de
conﬁance attribues £1 chaque candidat 51 la desambiguisation (Resnik & Yarowsky, 2000).

Références

BANERJEE S. & PEDERSEN T. (2003). Extended gloss overlaps as a measure of semantic relatedness. In
Proceedings of the Eighteenth International Conference on Artificial Intelligence ( IJCAI -03 ), Acapulco,
Mexico.

DUVIGNAU K. (2002). La métaphore berceau et enfant de la langue. These de doctorat, Université
Toulouse - Le Mirail.

DUVIGNAU K. (2003). Métaphore Verbale et approximation. Revue d ’Intelligence Artiﬁcielle, l7(5/6),
869-881. Regards croisés sur l’analogie.

GAUME B. (a paraitre). Balades aléatoires dans les petits mondes lexicaux. I3 Information Interaction
Intelligence.

GAUME B., DUVIGNAU K., GASQUET O. & GINESTE M.-D. (2002). Forms of meaning, meaning of
forms. Journal of Experimental and Theoretical Artiﬁcial Intelligence, 14(1), 61-74.

GAUME B. & FERRE L. (2004). Representation de graphes par acp granulaire. In Actes d ’EGC 2004 .-
4emes journées d’Extraction et de Gestion des Connaissances, Clermont-Ferrand.

HENRY F. (1996). Pour l’inforInatisation du TLF. In D. PIOTROWSKI, Ed., Lexicographie et informa-
tique. Autour de l ’informatisation du Trésor de la Langue Francaise, Paris: Didier Erudition.
H.KOZIMA & FURUGORI T. (1993). Similarity between words computed by spreading activation on an
english dictionary. In Proceedings of the conference of the European chapter of the ACL, p. 232-239.
IDE N. & VERONIS J. (1998). Introduction to the special issue on word sense disambiguation: The state
of the art. Computational Linguistics, 24(1).

LESK M. (1986). Automatic sense disambiguation using machine readable dictionaries: how to tell a
pine code from an ice cream cone. In Proceedings of the 5th annual international conference on Systems
documentation, p. 24-26, Toronto, Canada.

MANNING C. & SCHUTZE H. (1999). Foundations of Statistical Natural Language Processing. MIT
Press.

MARTIN R. (1983). Pour une logique du sens. Paris: Presses Universitaires de France.

NEWMAN M. E. J. (2003). The structure and function of complex networks. SIAM Review, volume 45,
167-256.

RESNIK P. & YAROWSKY D. (2000). Distinguishing systems and distinguishing senses: New evaluation
methods for word sense disambiguation. Natural Language Engineering, 5(2), 113-133.

WATTS D. & STROGATZ S. (1998). Collective dynamics of ‘small-world’ networks. Nature, (393),
440-442.

