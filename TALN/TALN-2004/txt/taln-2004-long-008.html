<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
"http://www.w3.org/TR/html4/loose.dtd">
<html><head><title>Un mod&#232;le d&#8217;acquisition de la syntaxe &#224; l&#8217;aide d&#8217;informations s&#233;mantiques</title>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
</head>
<body>
<div style="page-break-before:always; page-break-after:always"><div><p>TALN 2004, F&#232;s, 19&#8211;21 avril 2004
</p>
<p>Un mod&#232;le d&#8217;acquisition de la syntaxe
&#224; l&#8217;aide d&#8217;informations s&#233;mantiques&#8727;
</p>
<p>D. Dudau Sofronie, I. Tellier
Grappa - Universit&#233; Lille 3 &amp; INRIA Futurs, France
dudau@grappa.univ-lille3.fr, tellier@univ-lille3.fr
</p>
<p>R&#233;sum&#233; - Abstract
</p>
<p>Nous pr&#233;sentons dans cet article un algorithme d&#8217;apprentissage syntaxico-s&#233;mantique du lan-
gage naturel. Les donn&#233;es de d&#233;part sont des phrases correctes d&#8217;une langue donn&#233;e, enrichies
d&#8217;informations s&#233;mantiques. Le r&#233;sultat est l&#8217;ensemble des grammaires formelles satisfaisant
certaines conditions et compatibles avec ces donn&#233;es. La strat&#233;gie employ&#233;e, valid&#233;e d&#8217;un point
de vue th&#233;orique, est test&#233;e sur un corpus de textes fran&#231;ais constitu&#233; pour l&#8217;occasion.
</p>
<p>This paper presents a syntactico-semantic learning algorithm for natural languages. Input data
are syntactically correct sentences of a given natural language, enriched with semantic infor-
mation. The output is the set of compatible formal grammars satisfying certain conditions. The
strategy used, which has been proved theoretically valid, is tested on a corpus of French texts
built for this purpose.
</p>
<p>Mots-clefs &#8211; Keywords
</p>
<p>Grammaires cat&#233;gorielles, types s&#233;mantiques, apprentissage syntaxico-s&#233;mantique
Categorial grammars, semantic types, syntactico-semantic learning
</p>
<p>1 Introduction
</p>
<p>La mod&#233;lisation du ph&#233;nom&#232;ne de l&#8217;acquisition du langage par les enfants suscite un int&#233;r&#234;t
croissant ces derni&#232;res ann&#233;es (voir les conf&#233;rences CoNNL, (Brent 96)). Ce sujet se situe au
croisement de la linguistique, du traitement automatique du langage naturel et des sciences
cognitives. Les travaux &#233;voqu&#233;s ici sont issus d&#8217;une recherche ayant ses fondements dans la
th&#233;orie des langages formels et l&#8217;apprentissage automatique et se concentrent sur deux niveaux
de traitement de la langue : la syntaxe et la s&#233;mantique. L&#8217;objectif est de d&#233;finir un mod&#232;le d&#8217;ac-
quisition du langage, et plus pr&#233;cis&#233;ment de la grammaire, qui soit &#224; la fois cr&#233;dible d&#8217;un point
</p>
<p>&#8727;Recherche effectu&#233;e dans le cadre de l&#8217;ARC INRIA Gracq et de la MSH-Institut International Erasme</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>D. Dudau Sofronie, I. Tellier
</p>
<p>de vue psycholinguistique et calculable. Les mod&#232;les utilis&#233;s ici sont : les grammaires cat&#233;go-
rielles classiques (Bar Hillel et al. 60) pour repr&#233;senter la syntaxe et la logique de Montague
(Montague 74) comme source d&#8217;inspiration pour la s&#233;mantique.
</p>
<p>Les grammaires cat&#233;gorielles, par leur nature lexicalis&#233;e, sont bien adapt&#233;es &#224; un processus
d&#8217;apprentissage : en effet, leurs r&#232;gles sont exprim&#233;es par un nombre r&#233;duit et invariable de sch&#233;-
mas. Apprendre une telle grammaire signifie donc simplement apprendre &#224; associer des cat&#233;go-
ries syntaxiques aux items lexicaux. L&#8217;&#233;tude de l&#8217;apprentissage de ces grammaires a connu des
d&#233;veloppements importants ces derni&#232;res ann&#233;es (Adriaans 1992; Kanazawa 98; Bonato,Retore
01; Besombes,Marion 03). La pertinence des grammaires cat&#233;gorielles dans le contexte du lan-
gage naturel a &#233;t&#233; justifi&#233;e par plusieurs travaux (Oehrle et. al 88). Or, un des int&#233;r&#234;ts majeurs
de ce type de grammaires est leur connexion avec la s&#233;mantique, qui s&#8217;appuie sur le Principe
de Compositionnalit&#233; (Janssen 97). Ainsi, il nous semble naturel d&#8217;adopter une strat&#233;gie d&#8217;ap-
prentissage de la syntaxe bas&#233;e sur la s&#233;mantique, c&#8217;est-&#224;-dire de consid&#233;rer que la capacit&#233;
d&#8217;acqu&#233;rir une grammaire &#224; partir d&#8217;&#233;nonc&#233;s est conditionn&#233;e par celle de construire une repr&#233;-
sentation de la situation d&#233;crite par ces &#233;nonc&#233;s (Pinker 94). Nous pr&#233;sentons ici une nouvelle
mani&#232;re de concevoir le Principe de Compositionnalit&#233; et de le rendre partie prenante de la
strat&#233;gie d&#8217;apprentissage. L&#8217;information s&#233;mantique utilis&#233;e n&#8217;est pas la repr&#233;sentation logique
compl&#232;te des mots ou des phrases mais seulement leur type s&#233;mantique. Ce type distingue les
entit&#233;s (individus), les valeurs de verit&#233; et les propri&#233;t&#233;s (pr&#233;dicats). L&#8217;hypoth&#232;se faite est que
cette information est extraite de l&#8217;environnement par l&#8217;apprenant.
</p>
<p>La validation de notre mod&#232;le d&#8217;apprentissage passe par une double d&#233;marche, th&#233;orique et ex-
p&#233;rimentale. D&#8217;un point de vue th&#233;orique, nous avons d&#233;fini une nouvelle classe de grammaires
cat&#233;gorielles classiques et nous avons d&#233;j&#224; montr&#233; que cette classe est apprenable dans le mo-
d&#232;le d&#8217;apprentissage &#224; la limite de Gold (Gold 67) &#224; l&#8217;aide d&#8217;informations s&#233;mantiques (Dudau-
Sofronie et al. 01; Dudau-Sofronie et al. 03). Mais, dans cet article, nous d&#233;velopperons plut&#244;t
le versant exp&#233;rimental de notre travail. Nous avons en effet impl&#233;ment&#233; notre algorithme et
nous avons cherch&#233; &#224; le tester sur un corpus de textes en fran&#231;ais, sp&#233;cialement con&#231;u dans ce
but.
</p>
<p>Cet article d&#233;bute par une courte description des formalismes utilis&#233;s, tant au niveau de la
syntaxe que de la s&#233;mantique. Ensuite, notre algorithme d&#8217;apprentissage est expliqu&#233; sur un
exemple. Enfin, nous pr&#233;sentons les principales &#233;tapes de la constitution du corpus et les tests
exp&#233;rimentaux r&#233;alis&#233;s pour valider notre strat&#233;gie.
</p>
<p>2 Formalismes employ&#233;s
</p>
<p>2.1 Grammaires Cat&#233;gorielles Classiques
</p>
<p>Pour d&#233;finir une grammaire cat&#233;gorielle, il faut un ensemble de cat&#233;gories basiques, une as-
signation de cat&#233;gories aux mots et des r&#232;gles de r&#233;duction applicables sur les cat&#233;gories. La
notation que nous adoptons1 ici pour les cat&#233;gories est pr&#233;fix&#233;e, sous forme des termes, qui
correspondront &#224; /(A,B) pour B/A et &#224; \(A,B) pour A\B. Ainsi, la cat&#233;gorie impliqu&#233;e dans
une r&#233;duction (en tant qu&#8217;argument) se trouve toujours sur la premi&#232;re position d&#8217;un terme.
</p>
<p>Soit &#931; un vocabulaire fini. Soit B un ensemble d&#233;nombrable de cat&#233;gories basiques contenant
1Par rapport &#224; la notation classique B/A, A\B.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Un mod&#232;le d&#8217;acquisition de la syntaxe &#224; l&#8217;aide d&#8217;informations s&#233;mantiques
</p>
<p>une cat&#233;gorie sp&#233;ciale S &#8712; B, d&#233;nomm&#233;e l&#8217;axiome. L&#8217;ensemble de toutes les cat&#233;gories obte-
nues &#224; partir de B peut &#234;tre vu comme l&#8217;alg&#232;bre Cat(B) des termes construits sur B avec deux
op&#233;rateurs binaires : /, \. Cat(B) est ainsi le plus petit ensemble tel que : (1) B &#8834; Cat(B) et
(2) si A &#8712; Cat(B) et B &#8712; Cat(B) alors : /(A,B) &#8712; Cat(B) et \(A,B) &#8712; Cat(B).
Une grammaire cat&#233;gorielle sur &#931; est une relation finie entre &#931; et Cat(B) : G &#8838; &#931;&#215;Cat(B)
et G finie. Une grammaire cat&#233;gorielle classique (GCC) est une grammaire cat&#233;gorielle qui
admet seulement deux r&#232;gles de r&#233;duction pour toutes cat&#233;gories A et B de Cat(B).
&#8211; forward application FA : /(A,B) A &#8594; B ;
&#8211; backward application BA : A \(A,B) &#8594; B.
Les r&#232;gles de r&#233;duction justifient les notations fractionnelles des cat&#233;gories construites avec les
deux op&#233;rateurs / et \. Les termes /(A,B) et \(A,B) sont des foncteurs orient&#233;s attendant,
comme argument &#224; droite (resp. &#224; gauche), la cat&#233;gorie A et donnant, comme r&#233;sultat, la cat&#233;-
gorie B. Nous notons par &#12296;a,A&#12297; &#8712; G, o&#249; a &#8712; &#931; et A &#8712; Cat(B) l&#8217;assignation de la cat&#233;gorie
A au mot a, dans G. Le langage engendr&#233; par une telle grammaire est l&#8217;ensemble des suites
d&#8217;&#233;l&#233;ments du vocabulaire &#931; auxquelles on peut faire correspondre une suite de cat&#233;gories de
Cat(B) qui se r&#233;duit par les r&#232;gles de r&#233;duction FA et BA &#224; la cat&#233;gorie S.
Soit par exemple G une GCC &#233;l&#233;mentaire : G = {&#12296;homme,NC&#12297;, &#12296;Jean, T &#12297;, &#12296;petit, /(NC,NC)&#12297;,
&#12296;un, /(NC, T )&#12297;, &#12296;court, \(T, S)&#12297;, &#12296;regarde, \(T, /(T, S))&#12297;}. Les cat&#233;gories basiques T et NC
signifient respectivement &#8220;terme&#8221; et &#8220;nom commun&#8221;. Pour les d&#233;terminants, nous choisissons
ici une cat&#233;gorie simplifi&#233;e par rapport &#224; celle utilis&#233;e dans la tradition de Montague, ne ren-
dant pas compte de son caract&#232;re de quantificateur mais adapt&#233;e aux GCC. Cette grammaire
reconna&#238;t des phrases comme &#8220;un homme court&#8221;, &#8220;Jean regarde un petit homme&#8221;, etc.
</p>
<p>2.2 Types s&#233;mantiques
</p>
<p>Montague (Montague 74) a &#233;t&#233; le premier &#224; proposer une logique typ&#233;e pour repr&#233;senter la
s&#233;mantique des langues naturelles. Cette notion de type est depuis devenue classique. C&#8217;est elle
que nous retenons ici comme information s&#233;mantique. Formellement, l&#8217;ensemble des types est
construit r&#233;cursivement &#224; partir d&#8217;un ensemble de types basiques. L&#8217;ensemble basique le plus
usuel est &#920; = {e, t}. Le type basique e est traditionnellement le type des entit&#233;s &#233;l&#233;mentaires du
mod&#232;le logique et le type t d&#233;note les valeurs de v&#233;rit&#233;. Pour tout ensemble de types basiques
&#920; tel que t &#8712; &#920;, l&#8217;ensemble de tous les types Types(&#920;) est le plus petit ensemble tel que :
&#920; &#8834; Types(&#920;) et pour tout u &#8712; Types(&#920;) et v &#8712; Types(&#920;), (u, v) &#8712; Types(&#920;). Le type
(u, v) est un foncteur attendant comme argument le type u pour donner comme r&#233;sultat le type
v. Classiquement, les identifiants de personne (&#8220;Jean&#8221;, etc.) et les termes sont de type e, tandis
que les noms communs et les verbes intransitifs, qui r&#233;f&#233;rent tous les deux &#224; un pr&#233;dicat &#224; une
place, sont de type (e, t). Les verbes transitifs sont, eux, de type (e, (e, t)). Les adjectifs comme
&#8220;petit&#8221; sont des modifieurs de noms communs, ils sont donc de type ((e, t), (e, t)).
</p>
<p>Il y a bien s&#251;r un lien &#233;troit entre la notion de cat&#233;gorie utilis&#233;e dans les grammaires cat&#233;gorielles
et les types s&#233;mantiques. Les deux s&#8217;expriment par des foncteurs. Nous pouvons en fait caract&#233;-
riser plus pr&#233;cis&#233;ment ce lien gr&#226;ce &#224; la notion de fonction de typage. Une fonction de typage h
est un morphisme de Cat(B) vers Types(&#920;) satisfaisant les conditions suivantes : (1) h(S) = t ;
(2) &#8704;X &#8712; B, on a : h(X) &#8712; Types(&#920;) (3) &#8704;X, Y &#8712; Cat(B) : h(/(Y,X)) = h(\(Y,X))
= (h(Y ), h(X)). Le Principe de Compositionnalit&#233; affirme que le sens d&#8217;une phrase ne d&#233;pend
que du sens des mots qui la constituent et de sa structure syntaxique. Il se traduit habituellement
par une similarit&#233; de structure entre les arbres syntaxique et s&#233;mantique. Dans la mesure o&#249; les</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>D. Dudau Sofronie, I. Tellier
</p>
<p>cat&#233;gories et les types sont des structures lexicalis&#233;es, la fonction de typage peut &#234;tre consid&#233;r&#233;e
comme l&#8217;expression de la lexicalisation du Principe de Compositionnalit&#233;.
</p>
<p>Si nous posons : h(T ) = e, h(S) = t, h(NC) = (e, t), nous avons d&#233;fini une fonction de
typage parfaitement compatible avec la grammaire G donn&#233;e en exemple pr&#233;c&#233;demment, et
avec la s&#233;mantique de son vocabulaire. Remarquons qu&#8217;&#224; une cat&#233;gorie basique (par exemple
NC) peut &#234;tre associ&#233; un type non basique et que deux cat&#233;gories diff&#233;rentes (non basiques)
peuvent donner un type identique, puisque h(\(T, S)) = (h(T ), h(S)) = (e, t) = h(NC).
</p>
<p>3 Inf&#233;rence de grammaires &#224; partir de phrases typ&#233;es
</p>
<p>Nous cherchons &#224; mod&#233;liser et &#224; simuler l&#8217;aide qu&#8217;apporte la connaissance d&#8217;informations s&#233;-
mantiques dans le processus d&#8217;acquisition de la syntaxe. L&#8217;algorithme que nous exposons bri&#232;-
vement ici (et d&#233;crit en d&#233;tail dans (Dudau-Sofronie et al. 01)) prend comme donn&#233;es d&#8217;entr&#233;es
des phrases typ&#233;es, c&#8217;est-&#224;-dire des &#233;nonc&#233;s syntaxiquement corrects d&#8217;une langue donn&#233;e, o&#249;
chaque mot est associ&#233; &#224; son type s&#233;mantique. Il donne comme r&#233;sultat un ensemble de GCCs,
chacune associ&#233;e &#224; sa fonction de typage.
</p>
<p>Remarquons tout d&#8217;abord que les types non basiques se pr&#233;sentent sous la forme de foncteurs.
Mais, contrairement aux cat&#233;gories syntaxiques des GCCs, ces foncteurs ne sont pas orient&#233;s.
On peut n&#233;anmoins pr&#233;ciser comment ils se combinent les uns avec les autres, &#224; la fa&#231;on des
r&#232;gles FA et BA utilis&#233;es dans les GCC. &#8704;u, v &#8712; Types(&#920;), on a les r&#232;gles suivantes :
</p>
<p>- Type Forward TF : (u, v) u &#8594; v ;
- Type Backward TB : u (u, v) &#8594; v .
</p>
<p>Les types donnent donc des indications sur la nature de foncteur ou d&#8217;argument des &#233;l&#233;ments
du vocabulaire auxquels ils sont associ&#233;s, mais o&#249; la direction de l&#8217;op&#233;rateur, \ ou / est perdue.
Ils sont en quelque sorte des cat&#233;gories syntaxiques d&#233;grad&#233;es. Mais la d&#233;gradation subie est
r&#233;guli&#232;re, puisque c&#8217;est la fonction de typage, qui est un morphisme, qui en est responsable. Ce
sera tout l&#8217;enjeu de notre algorithme d&#8217;apprentissage de reconstituer des cat&#233;gories syntaxiques
&#224; partir de ces types. Ce processus sera trait&#233; en deux &#233;tapes : une &#233;tape d&#8217;analyse et une &#233;tape
de d&#233;duction des cat&#233;gories.
</p>
<p>3.1 L&#8217;analyse des phrases typ&#233;es
</p>
<p>Comme toute phrase typ&#233;e, fournie comme donn&#233;e d&#8217;entr&#233;e, est issue d&#8217;une phrase syntaxique-
ment correcte, la s&#233;quence des types associ&#233;s aux mots de la phrase peut, elle, &#234;tre r&#233;duite au
type t, en utilisant les r&#232;gles TF et TB comme le montre la Figure 1 (&#224; gauche).
</p>
<p>L&#8217;&#233;tape d&#8217;analyse de l&#8217;algorithme global est en fait plus complexe et consiste en deux &#233;tapes :
&#8211; une &#233;tape de variabilisation des types de toutes les phrases typ&#233;es, qui consiste &#224; introduire
</p>
<p>des variables distinctes dans les expressions de type, en position d&#8217;op&#233;rateur, sachant qu&#8217;une
assignation form&#233;e par le m&#234;me mot et le m&#234;me type re&#231;oit la m&#234;me variabilisation. Ainsi, la
phrase typ&#233;e &#12296;un, ((e, t), t)&#12297; &#12296;homme, (e, t)&#12297; &#12296;court, (e, t)&#12297; devient la phrase typ&#233;e variabili-
s&#233;e &#12296;un, x1(x2(e, t), t)&#12297; &#12296;homme, x3(e, t)&#12297; &#12296;court, x4(e, t)&#12297;.
</p>
<p>&#8211; une &#233;tape de recherche des r&#233;ductions &#224; t de la s&#233;quence des types dans chaque phrase typ&#233;e
variabilis&#233;e (Figure 1 &#224; droite). Cette analyse sert &#224; identifier la direction dans laquelle chaque</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Un mod&#232;le d&#8217;acquisition de la syntaxe &#224; l&#8217;aide d&#8217;informations s&#233;mantiques
</p>
<p>foncteur va trouver ses arguments. Si la r&#232;gle TF (resp. TB ) est employ&#233;e dans une analyse,
cela signifie qu&#8217;au niveau syntaxique, c&#8217;est la r&#232;gle FA (resp. BA) qui devait &#234;tre employ&#233;e, et
donc un op&#233;rateur / (resp. \) a &#233;t&#233; identifi&#233; l&#224; o&#249; une variable avait &#233;t&#233; introduite. Dans l&#8217;arbre
de la Figure 1, on a fait figurer ces identifications : x1 = / est induit lors de l&#8217;application de la
r&#232;gle TF et x4 = \ lors l&#8217;application de la r&#232;gle TB . De plus, pour que les r&#232;gles TF et TB
soient applicables sur les types variabilis&#233;s, il faut aussi que certaines conditions d&#8217;&#233;galit&#233;
entre sous-types soient v&#233;rifi&#233;es : dans l&#8217;arbre de la Figure 1, les deux sous-types soulign&#233;s
doivent &#234;tre &#233;gaux pour que TF puisse s&#8217;appliquer. Ces conditions entra&#238;nent &#224; leur tour une
(ou des) &#233;galit&#233;(s) entre variables : x2 = x3 ici. Pour impl&#233;menter cette analyse, nous nous
sommes inspir&#233;s d&#8217;algorithmes classiques dans les grammaires hors-contexte.
</p>
<p>t
TB
</p>
<p>e
TF
</p>
<p>((e, t), e)
un
</p>
<p>(e, t)
homme
</p>
<p>(e, t)
court
</p>
<p>t
TB
</p>
<p>e
TF
</p>
<p>x1(x2(e, t), e)
</p>
<p>un
x3(e, t)
</p>
<p>homme
x4(e, t)
</p>
<p>court
</p>
<p>x4 = \
</p>
<p>x1 = /
</p>
<p>FIG. 1 &#8211; arbres d&#8217;analyses pour une phrase typ&#233;e et pour une phrase typ&#233;e variabilis&#233;e
</p>
<p>&#192; l&#8217;issue de cette &#233;tape, on obtient en fait des ensembles de contraintes sur les variables intro-
duites dans les types variabilis&#233;s, qui peuvent aussi se traduire sous forme de substitution(s) sur
ces variables (on ne d&#233;taille pas cet aspect ici).
</p>
<p>3.2 La d&#233;duction des cat&#233;gories et des fonctions de typage
</p>
<p>L&#8217;algorithme ne sait rien des cat&#233;gories syntaxiques possibles : il doit les d&#233;finir. Comme un
type non basique peut correspondre &#224; une cat&#233;gorie basique (par exemple (e, t) qui correspond
&#224; NC), il est n&#233;cessaire d&#8217;appliquer une &#233;tape suppl&#233;mentaire de d&#233;duction de cat&#233;gories, apr&#232;s
l&#8217;analyse. L&#8217;id&#233;e est dans ce cas qu&#8217;un tel type ne mettra jamais en oeuvre sa nature de foncteur
dans une analyse puisque, au niveau syntaxique, il n&#8217;en est pas r&#233;ellement un.
</p>
<p>La d&#233;duction des cat&#233;gories s&#8217;effectue par les r&#232;gles suivantes : (1) chaque plus petit sous-
type variabilis&#233; distinct qui est utilis&#233; en tant qu&#8217;argument dans une analyse est associ&#233; &#224; une
cat&#233;gorie basique nouvelle (2) le type t en tant que r&#233;sultat est, par convention, toujours associ&#233;
&#224; la cat&#233;gorie axiomatique S. La fonction de typage est directement d&#233;duite de ces r&#232;gles.
</p>
<p>La GCC G induite par notre algorithme avec la phrase typ&#233;e donn&#233;e en exemple dans la Fi-
gure 1 est donc la suivante : G = {&#12296;un, /(A,B)&#12297;, &#12296;homme,A&#12297;, &#12296;court, \(B, S)&#12297;} avec A et
B cat&#233;gories basiques nouvelles v&#233;rifiant : h(A) = (e, t), h(B) = e. Cette solution co&#239;ncide,
&#224; un renommage des cat&#233;gories basique pr&#232;s (A pour NC, B pour T) avec la GCC &#8220;naturelle&#8221;
cit&#233;e en exemple jusqu&#8217;&#224; pr&#233;sent. Comme &#8220;Jean&#8221; est de type e (donc de cat&#233;gorie B dans G),
cette grammaire reconna&#238;t aussi la phrase &#171; Jean court &#187;, elle a donc g&#233;n&#233;ralis&#233; par rapport &#224;
l&#8217;exemple qui lui a &#233;t&#233; pr&#233;sent&#233;. Sur cet exemple, une seule grammaire est apprise mais dans
le cas g&#233;n&#233;ral, le r&#233;sultat de l&#8217;algorithme est un ensemble de grammaires, chacune associ&#233;e &#224;
sa fonction de typage. Cette strat&#233;gie peut &#234;tre appliqu&#233;e &#224; un &#233;chantillon de phrases en en-
tr&#233;e. Elle induit des contraintes pour chaque phrase et, de mani&#232;re incr&#233;mentale, propage les</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>D. Dudau Sofronie, I. Tellier
</p>
<p>contraintes compatibles (aucune variable ne peut &#234;tre &#224; la fois &#233;gale &#224; / et &#224; \) d&#8217;une phrase &#224;
une autre. Nous avons &#233;tudi&#233; les propri&#233;t&#233;s formelles de cet algorithme et d&#233;fini les conditions
sur les GCC et les fonctions de typage qui assurent que le r&#233;sultat fourni est correct et complet.
</p>
<p>4 Exp&#233;rimentations et r&#233;sultats
</p>
<p>Notre approche &#233;tait, &#224; l&#8217;origine, essentiellement th&#233;orique. Nous nous sommes initialement
plac&#233;s dans le cadre du mod&#232;le d&#8217;apprentissage de Gold, dans lequel le crit&#232;re d&#8217;apprenabilit&#233;
ne requiert aucune validation exp&#233;rimentale, ni m&#234;me de &#8220;praticabilit&#233;&#8221; quant &#224; la complexit&#233; al-
gorithmique. Mais l&#8217;algorithme que nous proposons m&#233;rite d&#8217;&#234;tre test&#233; sur des donn&#233;es r&#233;elles.
Son principal inconv&#233;nient est qu&#8217;il n&#233;cessite des donn&#233;es d&#8217;entr&#233;e tr&#232;s sp&#233;cifiques (des phrases
typ&#233;es), qu&#8217;aucun corpus actuel ne fournit. N&#233;anmoins, ces donn&#233;es sont moins co&#251;teuses &#224;
produire que les corpus arbor&#233;s g&#233;n&#233;ralement n&#233;cessaires pour tester les techniques d&#8217;inf&#233;rence
grammaticale appliqu&#233;es au langage naturel (Sakakibara 92; Kanazawa 98; Bonato,Retore 01;
Besombes,Marion 03). Les types, en effet, sont des donn&#233;es lexicalis&#233;es et nous avons donc
eu l&#8217;id&#233;e de produire un ensemble de phrases typ&#233;es &#224; partir de textes &#233;tiquet&#233;s par un tagger
lexical. D&#8217;un point de vue psycholinguistique, la donn&#233;e de phrases typ&#233;e est aussi plus cr&#233;dible
que la donn&#233;e de structures arborescentes. Nous d&#233;crivons ici la d&#233;marche suivie pour consti-
tuer ce corpus, et la nature des exp&#233;riences que nous avons men&#233;es pour valider notre approche
et tester certaines hypoth&#232;ses.
</p>
<p>4.1 Constitution d&#8217;un corpus
</p>
<p>Le point de d&#233;part de notre d&#233;marche est la recherche d&#8217;un ensemble de textes2. Nous avons
aussi identifi&#233; les besoins suivants : (1) Nous souhaitions un corpus de textes en fran&#231;ais, afin
d&#8217;&#234;tre &#224; m&#234;me de valider la qualit&#233; des grammaires qui seront apprises, et si possible libres de
droit. (2) Les textes doivent &#234;tre &#233;crits dans une langue homog&#232;ne. Notre algorithme n&#8217;est pas
sp&#233;cialis&#233; dans l&#8217;acquisition d&#8217;une langue plut&#244;t qu&#8217;une autre : il apprend celle &#224; laquelle ap-
partiennent les phrases qui lui sont soumises. Mais il n&#8217;apprendra quelque chose que s&#8217;il existe
des grammaires compatibles avec l&#8217;ensemble des phrases typ&#233;es qui lui sont propos&#233;es. (3)
Comme notre algorithme pr&#233;tend simuler ce que fait peut-&#234;tre un enfant en phase d&#8217;acquisition
de sa langue maternelle, il est naturel de lui fournir des phrases simples, avec un vocabulaire si
possible limit&#233; et r&#233;p&#233;titif.
</p>
<p>Les textes de livres pour enfant, auxquels nous avons d&#8217;abord pens&#233;, ne convenaient pas car
leur langue regorge de dialogues. Or, les &#233;nonc&#233;s intervenant dans les dialogues ne sont pas des
propositions au sens logique : ils ne sont pas de type t, il faudrait donc envisager un typage
sp&#233;cifique pour les prendre en compte. Nous nous sommes finalement fix&#233;s sur une collection
de textes produits par des enfants, et disponible sur le site www.momes.net. Ces textes sont des
r&#233;cits &#233;crits par des enfants de 6 &#224; 10 ans, et corrig&#233;es par des adultes. Ils sont constitu&#233;s de
phrases assez courtes, qui ne d&#233;passent pas 10 mots, et correctes du point de vue syntaxique.
</p>
<p>Ces textes ont &#233;t&#233; nettoy&#233;s &#224; la main (pour en retirer, par exemple, les parties dialogu&#233;es) et
ont subi des traitements automatiques successifs, pr&#233;sent&#233;s sur la Figure 2. Ils ont d&#8217;abord &#233;t&#233;
</p>
<p>2T. Desvenain, &#233;tudiant en Ma&#238;trise de science du langage &#224; l&#8217;Universit&#233; Lille 3 a contribu&#233; &#224; ce travail</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Un mod&#232;le d&#8217;acquisition de la syntaxe &#224; l&#8217;aide d&#8217;informations s&#233;mantiques
</p>
<p>soumis &#224; un &#233;tiqueteur lexical, en l&#8217;occurrence Tree Tagger3 (&#233;tape I sur le sch&#233;ma). De l&#8217;&#233;ti-
quetage complet fourni par le tagger, on n&#8217;a retenu que les informations concernant le r&#244;le de
chaque mot dans la phrase (nom, verbe, adjectif, adverbe, d&#233;terminant, etc.) et son lemme. La
partie cruciale et sp&#233;cifique de la constitution de notre corpus est la d&#233;finition d&#8217;une table de
correspondance entre les &#233;tiquettes lexicales et les types s&#233;mantiques. Pour les principales &#233;ti-
quettes, la correspondance est imm&#233;diate : nom propre, pronom personnel - e ; nom commun -
(e, t) ; adjectif - ((e, t), (e, t)) ; d&#233;terminant - ((e, t), e). N&#233;anmoins cette correspondance n&#8217;est
pas toujours unique : le Tree tagger ne fait par exemple pas de distinction entre les verbes in-
transitifs et les verbes transitifs, qui doivent pourtant recevoir des types s&#233;mantiques diff&#233;rents.
La table associe donc &#224; chaque &#233;tiquette un ensemble de types possibles (&#233;tape II). Une fois les
&#233;tiquettes transform&#233;es en types, il faut &#233;liminer les phrases typ&#233;es incorrectement, c&#8217;est-&#224;-dire
ne se r&#233;duisant pas &#224; t. Cette phase est la plus co&#251;teuse, elle est r&#233;alis&#233;e en appliquant un algo-
rithme d&#8217;analyse simplifi&#233;, avec les r&#232;gles TF et TB , sur tous les typages possibles (&#233;tape III).
Enfin, il reste encore une &#233;tape de s&#233;lection manuelle pour arriver &#224; un seul typage par phrase.
</p>
<p>Le corpus final, au format XML, contient du texte annot&#233; avec les &#233;tiquettes lexicales et avec
les types s&#233;mantiques. Il est constitu&#233; de 1008 phrases contenant 5851 mots parmi lesquels
2632 sont diff&#233;rents. Un point essentiel &#224; comprendre est que m&#234;me si deux mots distincts (par
exemple deux noms communs) ont &#233;t&#233; &#233;tiquet&#233;s de la m&#234;me fa&#231;on par le tagger, ils ne seront pas
consid&#233;r&#233;s par notre algorithme comme appartenant a priori &#224; la m&#234;me cat&#233;gorie syntaxique. En
effet, lors de la phase de variabilisation des types, des variables diff&#233;rentes seront introduites
dans les types de ces deux mots. Ce n&#8217;est que si une contrainte d&#8217;&#233;galit&#233; entre ces variables
est inf&#233;r&#233;e que ces mots seront reconnus comme appartenant &#224; la m&#234;me cat&#233;gorie syntaxique.
Et cette contrainte d&#8217;&#233;galit&#233; ne sera elle-m&#234;me inf&#233;r&#233;e que si les deux mots apparaissent dans
un m&#234;me contexte (deux phrases ne diff&#233;rant que par ces mots, par exemple). Ce m&#233;canisme
permet de distinguer les cat&#233;gories NC et \(T, S), correspondant pourtant au m&#234;me type (e, t).
La contrepartie est que l&#8217;apprentissage ne sera efficace qu&#8217;en pr&#233;sence de redondances et de
r&#233;p&#233;titions. C&#8217;est ce que les exp&#233;riences qui suivent vont nous permettre de mesurer.
</p>
<p>III
Corpus typ&#233; 
</p>
<p>I
</p>
<p>Tree
Tagger
</p>
<p>II
Corpus avec &#233;tiquettes 
</p>
<p>        lexicales
Corpus.txt
</p>
<p>table de 
correspondances
</p>
<p>Corpus typ&#233; nettoy&#233;
</p>
<p>FIG. 2 &#8211; Traitements automatiques successifs pour obtenir un corpus typ&#233;
</p>
<p>4.2 Efficacit&#233; et validit&#233; du programme
</p>
<p>Les propri&#233;t&#233;s de notre algorithme que des exp&#233;riences sur corpus permettront de tester sont :
son efficacit&#233; et sa validit&#233;. L&#8217;efficacit&#233; mesure la praticabilit&#233; du programme, en termes de
ressources utilis&#233;es. La validit&#233; est un crit&#232;re plus qualitatif, qui d&#233;pend de la correction des
grammaires obtenues par rapport aux grammaires usuelles du langage naturel. On propose plu-
sieurs exp&#233;riences pour tester ces param&#232;tres.
</p>
<p>Pour ce qui est de l&#8217;efficacit&#233;, il convient de mesurer tout d&#8217;abord la complexit&#233; th&#233;orique du
probl&#232;me. Si on se contente de tirer al&#233;atoirement des phrases typ&#233;es de notre corpus et de lancer
</p>
<p>3http ://www.ims.uni-stuttgart.de/projekte/tc</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>D. Dudau Sofronie, I. Tellier
</p>
<p>notre algorithme sur ces donn&#233;es, il y a fort &#224; parier que rien d&#8217;int&#233;ressant ne sortira. La plupart
du temps, en effet, les phrases n&#8217;ont aucun mot en commun, sinon quelques mots grammati-
caux. Supposons qu&#8217;on tire 15 phrases typ&#233;es et que pour chacune d&#8217;elles, l&#8217;algorithme inf&#232;re
au moins 2 grammaires diff&#233;rentes compatibles possibles (moyenne calcul&#233;e sur le corpus).
Si les phrases n&#8217;ont aucun mot en commun, alors le nombre total de grammaires compatibles
avec l&#8217;ensemble des donn&#233;es sera, par une simple combinatoire, sup&#233;rieur &#224; 215 = 32768 gram-
maires. C&#8217;est &#224; cette aune qu&#8217;il faut se comparer. Lorsque nous avons fait cette exp&#233;rience, nous
avons en fait obtenu, en moyenne sur 8 tirages diff&#233;rents, 2560 grammaires. La taille du lexique,
pour les 15 phrases tir&#233;es &#233;tait, en moyenne, de 85 mots parmi lesquels, en moyenne, 81 &#233;taient
diff&#233;rents. Chacun des 4 mots qui se r&#233;p&#232;tent a donc, en moyenne, divis&#233; par 2 de nombre de
grammaires solution. Ce genre de calculs prend beaucoup de temps. Dans ce qui suit, nous ne
nous attacherons pas &#224; la complexit&#233; en temps, en revanche, le nombre de grammaires obtenues
&#224; chaque &#233;tape en sortie du programme est une &#233;valuation des ressources de m&#233;moire mises en
oeuvre au cours de l&#8217;apprentissage. Quels sont les facteurs qui peuvent contribuer &#224; r&#233;duire ce
nombre et donc &#224; am&#233;liorer l&#8217;efficacit&#233; de l&#8217;apprentissage ? Nous faisons l&#8217;hypoth&#232;se que deux
facteurs ind&#233;pendants doivent intervenir : d&#8217;une part l&#8217;ordre de pr&#233;sentation des phrases, d&#8217;autre
part la redondance du vocabulaire.
</p>
<p>Il est naturel de supposer que face &#224; un novice, l&#8217;entourage va avoir tendance &#224; d&#8217;abord produire
des &#233;nonc&#233;s simples, comprenant un nombre limit&#233; de mots, avant de produire des phrases &#224; la
structure plus complexe. Nous voulons mesurer l&#8217;importance de l&#8217;effet facilitateur obtenu en
contr&#244;lant l&#8217;ordre de pr&#233;sentation des phrases typ&#233;es &#224; notre algorithme. Le protocole employ&#233;
est le suivant : (a) tirer al&#233;atoirement une pr&#233;sentation, contenant n phrases, du corpus initial ;
(b) calculer le nombre de grammaires obtenues apr&#232;s chaque phrase ; (c) ordonner en ordre
croissant, respectivement d&#233;croissant par taille, les phrases dans la pr&#233;sentation tir&#233;e, et r&#233;ini-
tialiser le processus d&#8217;apprentissage. Les r&#233;sultats obtenus en moyenne en suivant ce sc&#233;nario
sont pr&#233;sent&#233;s sur le premier graphique de la Figure 3, pour les 8 pr&#233;sentations de taille n = 15
tir&#233;es du corpus, &#233;voqu&#233;es pr&#233;c&#233;demment. L&#8217;effet est visible, mais limit&#233;.
</p>
<p>Mais, comme nous l&#8217;avons d&#233;j&#224; remarqu&#233;, ce dont notre algorithme a crucialement besoin, c&#8217;est
de mots qui se r&#233;p&#232;tent d&#8217;une phrase &#224; une autre. Pour g&#233;n&#233;rer artificiellement des &#233;nonc&#233;s o&#249;
de telles r&#233;p&#233;titions se produisent, nous avons automatiquement op&#233;r&#233; des substitutions de vo-
cabulaire sur les phrases extraites du corpus, avant de les soumettre au programme. Nous avons
ainsi r&#233;duit les mots &#233;tiquet&#233;s respectivement par le tagger comme pronoms personnels, noms,
d&#233;terminants, d&#233;terminants possessifs et verbes transitifs &#224; uniquement deux instances diff&#233;-
rentes possibles (tir&#233;es au sort) pour chaque &#233;tiquette. Le r&#233;sultat obtenu en moyenne, pour 5
pr&#233;sentations de 30 phrases construites sur un lexique initial de 191 mots en moyenne, rame-
n&#233;es par les substitutions &#224; 50 mots en moyenne, est donn&#233; dans le deuxi&#232;me graphique de la
Figure 3. Sur la m&#234;me figure, sont repr&#233;sent&#233;es aussi les courbes o&#249; l&#8217;ordre sur la taille &#224; &#233;t&#233;
invers&#233; et tir&#233; al&#233;atoirement. L&#8217;effet facilitateur de l&#8217;ordre de pr&#233;sentation est nettement plus
visible et le nombre final de grammaires obtenues bien plus raisonnable que pr&#233;c&#233;demment.
On constate sur ces courbes des variations importantes : la r&#233;p&#233;tition d&#8217;un seul mot typ&#233; d&#233;j&#224;
rencontr&#233; auparavant peut provoquer une &#233;limination de beaucoup de grammaires candidates
par la simple d&#233;tection d&#8217;une incompatibilit&#233; (xi = / et xi = \ pour une certaine variable xi).
Les exp&#233;riences pr&#233;c&#233;dentes ne font que compter le nombre de grammaires r&#233;sultats : elles
ne permettent pas de valider leur pertinence. Pour cela, l&#8217;id&#233;al serait de tester les grammaires
apprises sur de nouvelles phrases. Mais, en raison du nombre de grammaires r&#233;sultats et de
la difficult&#233; &#224; trouver un ensemble de phrases tests construites sur le m&#234;me vocabulaire que
celles ayant permis l&#8217;apprentissage, nous n&#8217;avons pu mener &#224; bien de telles exp&#233;riences. Nous</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Un mod&#232;le d&#8217;acquisition de la syntaxe &#224; l&#8217;aide d&#8217;informations s&#233;mantiques
</p>
<p> 0
</p>
<p> 500
</p>
<p> 1000
</p>
<p> 1500
</p>
<p> 2000
</p>
<p> 2500
</p>
<p> 3000
</p>
<p> 0  2  4  6  8  10  12  14  16
</p>
<p>N
b 
</p>
<p>gr
am
</p>
<p>m
ai
</p>
<p>re
s
</p>
<p>Nb phrases
</p>
<p>ordre croissant
ordre aleatoire
</p>
<p>ordre decroissant
</p>
<p> 0
</p>
<p> 100
</p>
<p> 200
</p>
<p> 300
</p>
<p> 400
</p>
<p> 500
</p>
<p> 600
</p>
<p> 700
</p>
<p> 800
</p>
<p> 900
</p>
<p> 0  5  10  15  20  25  30
</p>
<p>N
b 
</p>
<p>gr
am
</p>
<p>m
ai
</p>
<p>re
s
</p>
<p>Nb phrases
</p>
<p>ordre croissant
ordre decroissant
</p>
<p>ordre aleatoire
</p>
<p>FIG. 3 &#8211; Le nombre total moyen de grammaires obtenues &#224; chaque &#233;tape pour des pr&#233;sentations
de 15 phrases sur le lexique complet (&#224; gauche) et de 30 phrases sur le lexique r&#233;duit (&#224; droite)
</p>
<p>devons donc chercher d&#8217;autres moyens de mesurer la validit&#233; de notre programme. Il serait
en particulier int&#233;ressant de v&#233;rifier si les mots &#233;tiquet&#233;s de la m&#234;me fa&#231;on par le tagger sont
finalement reconnus comme appartenant &#224; la m&#234;me cat&#233;gorie syntaxique dans les grammaires
r&#233;sultats. Nous d&#233;taillons dans ce qui suit une &#233;tude de cas qui nous semble int&#233;ressante, pour
une des pr&#233;sentations de 30 phrases avec lexique r&#233;duit &#233;voqu&#233;e pr&#233;c&#233;demment. Nous nous
concentrons ici sur les noms communs auquel le type (e, t), variabilis&#233; en x(e, t), a &#233;t&#233; associ&#233;.
Le lexique des noms communs est r&#233;duit &#224; deux instances distinctes (&#8220;soleil&#8221; et &#8220;fleur&#8221;). Les
diff&#233;rentes cat&#233;gories syntaxiques qui peuvent &#234;tre inf&#233;r&#233;es par le programme &#224; partir de ce
type correspondent soit &#224; la cat&#233;gorie \(T, S), soit &#224; /(T, S) (pour h(T ) = e), soit &#224; une ou
plusieurs cat&#233;gories basiques. Nous comptons, &#224; chaque &#233;tape de la pr&#233;sentation, le nombre de
grammaires qui contiennent une de ces instanciations de cat&#233;gorie pour les noms communs (la
valeur x = /, n&#8217;a jamais &#233;t&#233; inf&#233;r&#233;e, c&#8217;est pourquoi une colonne manque dans le tableau) :
</p>
<p>&#201;tape Nom commun Cat. basique Nb occ Cat avec x = \ Nb occ
1 fleur - - - -
</p>
<p>soleil A0 2 - -
3 fleur A1 2 - -
</p>
<p>soleil A0 2 - -
4 fleur A1 3 - -
</p>
<p>soleil A0 3 - -
7 fleur A1 3 \(T, S) 1
</p>
<p>soleil A0 4 - -
10 fleur A1 9 \(T, S) 1
</p>
<p>fleur A0 8
soleil A0 10 - -
soleil A1 8
</p>
<p>18 fleur A2 6 \(T, S) 12
fleur A0 6
soleil A0 18 \(T, S) 6
</p>
<p>22 fleur A1 48 \(T, S) 48
soleil A1 18 \(T, S) 48
</p>
<p>30 fleur A2 192 \(T, S) 192
soleil A2 192 \(T, S) 192
</p>
<p>D&#232;s la phrase 3, les deux noms communs ont chacun &#233;t&#233; pr&#233;sent&#233;s au moins une fois. La phrase
7 induit une cat&#233;gorie incorrecte pour &#8220;fleur&#8221;. Cette phrase et l&#8217;analyse incorrecte (pr&#233;sent&#233;e
synth&#233;tiquement sous forme de terme) &#224; laquelle elle a donn&#233; lieu est la suivante :
TB(TB(TB(&#12296;Pierre, e&#12297;, &#12296;regarde, (e, (e, t))&#12297;), &#12296;ma, ((e, t), e)&#12297;), &#12296;fleur, (e, t)&#12297;)
L&#8217;application de la r&#232;gle TB entre le type de &#8220;Pierre regarde&#8221;, (e, t), et celui de &#8220;ma&#8221;, ((e, t), e)
n&#8217;est pas linguistiquement l&#233;gitime. Elle entra&#238;ne l&#8217;identification de \ comme valeur possible
pour la variable du type associ&#233; au nom &#8220;fleur&#8221;. Comme le d&#233;terminant &#8220;ma&#8221; a une seule occur-
rence dans toute la pr&#233;sentation, cette valeur est propag&#233;e &#224; travers d&#8217;autres analyses, conduisant
&#224; la situation finale d&#233;crite dans le tableau, o&#249; elle fait jeu &#233;gal avec la cat&#233;gorie basique l&#233;gi-
time. Notons par ailleurs qu&#8217;entre les phrases 10 et 18, deux cat&#233;gories syntaxiques basiques</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>D. Dudau Sofronie, I. Tellier
</p>
<p>ont &#233;t&#233; introduites. C&#8217;est la propagation de contraintes d&#8217;&#233;galit&#233; qui finit, &#224; partir de la phrase
22, par unifier ces deux cat&#233;gories. La m&#234;me &#233;volution a &#233;t&#233; observ&#233;e pour le nombre total d&#8217;in-
troductions de cat&#233;gories basiques lors de pr&#233;sentations quelconques. Mais, pour discr&#233;diter la
cat&#233;gorie \(T, S) associ&#233;e aux noms communs, et les associer &#224; une seule cat&#233;gorie syntaxique
basique, il suffirait d&#8217;une seule phrase ou &#8220;ma&#8221; introduit un nom commun en position sujet.
</p>
<p>5 Conclusion
</p>
<p>Nous proposons dans cet article un programme d&#8217;inf&#233;rence grammaticale dirig&#233;e par la s&#233;man-
tique. Les grammaires formelles qui repr&#233;sentent la syntaxe ont de bonnes propri&#233;t&#233;s et sont
adapt&#233;es &#224; un apprentissage syntaxico-s&#233;mantique. Les r&#233;sultats th&#233;oriques ont &#233;t&#233; test&#233;s sur
des donn&#233;es r&#233;elles du langage naturel. Les exp&#233;riences r&#233;alis&#233;es sur corpus, bien qu&#8217;encore li-
mit&#233;es, montrent que l&#8217;acquisition du premier langage est un processus complexe, tr&#232;s sensible
&#224; un environnement propice. Ainsi, la taille du lexique et l&#8217;ordre dans lequel les phrases sont
pr&#233;sent&#233;es ont une grande influence sur les performances du syst&#232;me.
</p>
<p>R&#233;f&#233;rences
</p>
<p>ADRIAANS P. W. (1992), Language Learning from a Categorial Perspective. PhD thesis, University of
Amsterdam, Amsterdam, The Netherlands.
</p>
<p>BAR-HILLEL Y., GAIFMAN C., SHAMIR E. (1960), On categorial and phrase structure grammars.
Bulletin of the Research Council of Israel, 9F.
</p>
<p>BESOMBES J., MARION J.Y. (2003), Apprentissage de langages r&#233;guliers d&#8217;arbres et applications. In
TAL, vol 44, n&#9702;1/2003, pages 121&#8211;153.
BONATO R., RETOR&#201; C. (2001) Learning rigid lambek grammars and minimalist grammars from
structured sentences. In Proceedings of the Third Learning Language in Logic Workshop, pages 23&#8211;34.
</p>
<p>BRENT M. R. (1996), Computational Approaches to Language Acquisition. MIT Press
</p>
<p>DUDAU-SOFRONIE D., TELLIER I., TOMMASI M. (2001), From logic to grammars via types. In
Proceedings of Learning Language in Logic (LLL) 2001, pages 35&#8211;46.
</p>
<p>DUDAU-SOFRONIE D., TELLIER I., TOMMASI M. (2003), A learnable class of Classical Categorial
Grammars from typed examples. In Proceedings of the 8th conference on Formal Grammar, pages
77&#8211;88
</p>
<p>GOLD E. M. (1967), Language identification in the limit. Information and Control, 10 : pages 447&#8211;474
</p>
<p>JANSSEN T. M. V. (1997) Compositionality. In J. V. Benthem and A. ter Meulen, editors, Handbook of
Logic and Language, pages 417&#8211;473. MIT Press.
</p>
<p>KANAZAWA M. (1998), Learnable Classes of Categorial Grammars, The European Association for
Logic, Language and Information. CLSI Publications.
</p>
<p>MONTAGUE R. (1974), Formal Philosophy ; Selected papers of Richard Montague, Yale University
Press.
</p>
<p>OEHRLE R.T., BACH E., WHEELER D., editors. Categorial Grammars and Natural Language Struc-
tures. D. Reidel Publishing Company, Dordrecht, 1988.
</p>
<p>PINKER S. (1994), The Language Instinct Penguin Press, London
</p>
<p>SAKAKIBARA Y. (1992), Efficient learning of context-free grammars from positive structural examples,
Information and Computation, 97(1) : pages 23&#8211;60.</p>

</div></div>
</body></html>