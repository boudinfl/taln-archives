TALN 2004, Fés, 19-21 avril 2004

Un modéle d’acquisition de la syntaxe
a l’aide d’inf0rmations sémantiques*

D. Dudau Sofronie, I. Tellier
Grappa — Université Lille 3 & INRIA Futurs, France
dudau@ grappa.uniV—lille3.fr, tellier@uniV—lille3 .fr

Résumé - Abstract

Nous présentons dans cet article un algorithme d’apprentissage syntaXico—se’mantique du lan-
gage naturel. Les données de départ sont des phrases correctes d’une langue donne’e, enrichies
d’informations sémantiques. Le re’sultat est l’ensemble des grammaires formelles satisfaisant
certaines conditions et compatibles avec ces donne’es. La stratégie employée, Valide’e d’un point
de Vue théorique, est testée sur un corpus de textes frangais constitue’ pour l’occasion.

This paper presents a syntactico—semantic learning algorithm for natural languages. Input data
are syntactically correct sentences of a given natural language, enriched with semantic infor-
mation. The output is the set of compatible formal grammars satisfying certain conditions. The
strategy used, which has been proved theoretically Valid, is tested on a corpus of French texts
built for this purpose.

Mots-clefs — Keywords

Grammaires catégorielles, types sémantiques, apprentissage syntaXico—se’mantique
Categorial grammars, semantic types, syntactico—semantic learning

1 Introduction

La mode’lisation du phénomene de l’acquisition du langage par les enfants suscite un intérét
croissant ces dernieres anne’es (Voir les conférences CoNNL, (Brent 96)). Ce sujet se situe au
croisement de la linguistique, du traitement automatique du langage naturel et des sciences
cognitives. Les travaux e’Voque’s ici sont issus d’une recherche ayant ses fondements dans la
théorie des langages formels et l’apprentissage automatique et se concentrent sur deux niveaux
de traitement de la langue : la syntaxe et la sémantique. L’ obj ectif est de de’ﬁnir un modéle d ’ac—
quisition du langage, et plus pre’cise’ment de la grammaire, qui soit a la fois crédible d’un point

Recherche effectuée dans le cadre de l’ARC INRIA Gracq et de la MSH-Inslitut International Erasme

D. Dudau Sofronie, I. T e11ier

de Vue psycholinguistique et calculable. Les modeles utilisés ici sont : les grammaires cate’g0—
rielles classiques (Bar Hillel et al. 60) pour représenter la syntaxe et la logique de Montague
(Montague 74) comme source d’inspiration pour la sémantique.

Les grammaires catégorielles, par leur nature lexicalise’e, sont bien adapte’es a un processus
d’apprentissage : en effet, leurs re gles sont exprime’es par un nombre réduit et invariable de sche’—
mas. Apprendre une telle grammaire signiﬁe donc simplement apprendre a associer des catego-
ries syntaxiques aux items lexicaux. L’étude de l’apprentissage de ces grammaires a connu des
développements importants ces dernieres anne’es (Adriaans 1992; Kanazawa 98; Bonato,Retore
O1; Besombes,Marion 03). La pertinence des grammaires catégorielles dans le contexte du lan-
gage naturel a été justiﬁe’e par plusieurs travaux (Oehrle et. al 88). Or, un des intéréts majeurs
de ce type de grammaires est leur connexion avec la sémantique, qui s’appuie sur le Principe
de Compositionnalité (Janssen 97). Ainsi, il nous semble naturel d’adopter une slratégie d ’ap—
prentissage de la syntaxe basée sur la sémantique, c’est—a—dire de conside’rer que la capacite’
d’acque’rir une grammaire a partir d’énonce’s est conditionne’e par celle de construire une repre’—
sentation de la situation décrite par ces énonce’s (Pinker 94). Nous présentons ici une nouvelle
maniere de concevoir le Principe de Compositionnalité et de le rendre partie prenante de la
stratégie d’apprentissage. L’information sémantique utilise’e n’est pas la représentation logique
complete des mots ou des phrases mais seulement leur type sémantique. Ce type distingue les
entités (individus), les Valeurs de Verite’ et les proprie’tés (prédicats). L’hypothese faite est que
cette information est extraite de l’enVironnement par l’apprenant.

La Validation de notre modele d’apprentissage passe par une double démarche, théorique et ex-
périmentale. D’un point de Vue théorique, nous avons déﬁni une nouvelle classe de grammaires
catégorielles classiques et nous avons de’ja montre’ que cette classe est apprenable dans le mo-
dele d’apprentissage a la limite de Gold (Gold 67) a l’aide d’informations se’mantiques (Dudau—
Sofronie et al. 01; Dudau—Sofronie et al. 03). Mais, dans cet article, nous développerons plutot
le Versant expe’rimental de notre travail. Nous avons en effet implémente’ notre algorithme et
nous avons cherche’ a le tester sur un corpus de textes en francais, spe’cialement concu dans ce
but.

Cet article débute par une courte description des formalismes utilise’s, tant au niveau de la
syntaxe que de la sémantique. Ensuite, notre algorithme d’apprentissage est expliqué sur un
exemple. Enﬁn, nous présentons les principales étapes de la constitution du corpus et les tests
expérimentaux re’alise’s pour Valider notre strate’gie.

2 Formalismes employés

2.1 Grammaires Catégorielles Classiques

Pour de’ﬁnir une grammaire catégorielle, il faut un ensemble de categories basiques, une as-
signation de categories aux mots et des regles de réduction applicables sur les catégories. La
notation que nous adoptonsl ici pour les categories est préﬁxe’e, sous forme des termes, qui
correspondront a / (A, B) pour B / A et a \(A, B) pour A\B. Ainsi, la catégorie implique’e dans
une réduction (en tant qu’argument) se trouve toujours sur la premiere position d’un terme.

Soit 2 un Vocabulaire ﬁni. Soit 3 un ensemble dénombrable de catégories basiques contenant

1Par rapport a la notation classique B / A, A\B.

Un modéle d ’acqu1's1't1'on de1a syntaxe a1’a1'de d ’1'nformat1'ons sémantiques

une catégorie spéciale S’ E B, dénommée l’aXiome. L’ensemble de toutes les catégories obte—
nues a partir de 3 peut étre Vu comme l’algebre C'at(B) des termes construits sur 3 avec deux
opérateurs binaires : /,  C'at(B) est ainsi le plus petit ensemble tel que : (1) B C Gat(B) et
(2) si A E Gat(B) et B E Gat(B) alors: /(A, B) E Gat(B) et \(A, B) E C'at(B).

Une grammaire catégorielle sur 2 est une relation ﬁnie entre E et C'at(B) : G Q Z X Cat (3)
et G ﬁnie. Une grammaire catégorielle classique (GCC) est une grammaire catégorielle qui
admet seulement deux re gles de réduction pour toutes categories A et B de C'at(B).

— forward application FA : / (A, B) A —> B ;

— backward application BA : A \(A, B) —> B.

Les regles de réduction justiﬁent les notations fractionnelles des catégories construites avec les
deux opérateurs / et  Les termes / (A, B) et \(A, B) sont des foncteurs orientés attendant,
comme argument it droite (resp. a gauche), la catégorie A et donnant, comme résultat, la cate’—
gorie B. Nous notons par (a, A) E G, ou a E E et A E Gat(B) l’assignation de la catégorie
A au mot a, dans G. Le langage engendré par une telle grammaire est l’ensemble des suites
d’éléments du Vocabulaire Z auxquelles on peut faire correspondre une suite de categories de
Gat(B) qui se réduit par les regles de réduction FA et BA 51 la catégorie S’.

Soit par exemple G une GCC élémentaire: G = {(h0mme, NC), (Jean, T), (petit, /(NC, NG)),
(un, /(NC, T)), (court, \(T, S')), (regarde, \(T, /(T,  Les catégories basiques T et NC
signiﬁent respectivement “terme” et “nom commun”. Pour les déterminants, nous choisissons
ici une catégorie simpliﬁe’e par rapport a celle utilise’e dans la tradition de Montague, ne ren-
dant pas compte de son caractere de quantiﬁcateur mais adapte’e aux GCC. Cette grammaire
reconnait des phrases comme “un homme court”, “Jean regarde un petit homme”, etc.

2.2 Types sémantiques

Montague (Montague 74) a été le premier a proposer une logique type’e pour représenter la
sémantique des langues naturelles. Cette notion de type est depuis devenue classique. C’est elle
que nous retenons ici comme information sémantique. Formellement, l’ensemble des types est
construit récursivement a partir d’un ensemble de types basiques. L’ensemble basique le plus
usuel est 9 = {e, 15}. Le type basique e est traditionnellement le type des entités élémentaires du
modele logique et le type t dénote les Valeurs de Vérite’. Pour tout ensemble de types basiques
9 tel que t E G, l’ensemble de tous les types Types(@) est le plus petit ensemble tel que :
9 C Types(@) et pour tout u E Types(@) et ‘U E Types(@), (u,v) E Types(@). Le type
(u, ’U) est un foncteur attendant comme argument le type u pour donner comme résultat le type
1). Classiquement, les identiﬁants de personne (“Jean”, etc.) et les termes sont de type e, tandis
que les noms communs et les Verbes intransitifs, qui référent tous les deux a un prédicat a une
place, sont de type (6, 15). Les Verbes transitifs sont, eux, de type (6, (e,  Les adjectifs comme
“petit” sont des modiﬁeurs de noms communs, ils sont donc de type ((6, t), (e, 

11 y a bien sﬁr un lien étroit entre la notion de catégorie utilise’e dans les grammaires catégorielles
et les types sémantiques. Les deux s’eXpriment par des foncteurs. Nous pouvons en fait caracte’—
riser plus précise’ment ce lien grace a la notion de fonction de typage. Une fonction de typage h
est un morphisme de C'at(B) Vers Types(@) satisfaisant les conditions suiVantes: (1) h(S') = t;
(2) VX E B, on a : h(X) E Types(@) (3) VX, Y E Gat(B) : h(/(Y, X)) = h(\(Y, X))
= (h(Y), h(X Le Principe de Compositionnalité afﬁrme que le sens d’une phrase ne dépend
que du sens des mots qui la constituent et de sa structure syntaxique. 11 se traduit habituellement
par une similarite’ de structure entre les arbres syntaxique et sémantique. Dans la mesure ou les

D. Dudau Sofronie, I. T e11ier

categories et les types sont des structures lexicalisées, la fonction de typage peut etre considére’e
comme l’expression de la lexicalisation du Principe de Compositionnalite’.

Si nous posons : h(T) = e,h(S') = t,h(NC') = (e,t), nous avons déﬁni une fonction de
typage parfaitement compatible avec la grammaire G donne’e en exemple precédemment, et
avec la sémantique de son Vocabulaire. Remarquons qu’a une catégorie basique (par exemple
N C) peut etre associe’ un type non basique et que deux categories diffe’rentes (non basiques)

peuvent donner un type identique, puisque h(\(T, S')) = (h(T), h(S')) = (e, t) = h(NC').

3 Inférence de grammaires a partir de phrases typées

Nous cherchons a modéliser et a simuler l’aide qu’apporte la connaissance d’informations se’—
mantiques dans le processus d’acquisition de la syntaxe. L’algorithme que nous exposons brie-
Vement ici (et décrit en detail dans (Dudau—Sofronie et al. 01)) prend comme données d’entre’es
des phrases typées, c’est—a—dire des e’nonce’s syntaxiquement corrects d’une langue donne’e, ou
chaque mot est associe’ a son type semantique. ll donne comme re’sultat un ensemble de GCCs,
chacune associe’e a sa fonction de typage.

Remarquons tout d’abord que les types non basiques se présentent sous la forme de foncteurs.
Mais, contrairement aux categories syntaxiques des GCCs, ces foncteurs ne sont pas orientés.
On peut neanmoins preciser comment ils se combinent les uns avec les autres, a la fagon des
regles FA et BA utilisées dans les GCC. Vu, ‘U E Types(@), on ales regles suivantes :

— Type Forward TF : (um) u —> ’U',
— Type Backward TB : u (u,v) —> ’U .

Les types donnent donc des indications sur la nature de foncteur ou d’argument des elements
du Vocabulaire auxquels ils sont associe’s, mais ou la direction de l’opérateur, \ ou / est perdue.
Ils sont en quelque sorte des catégories syntaxiques dégradées. Mais la degradation subie est
reguliere, puisque c’est la fonction de typage, qui est un morphisme, qui en est responsable. Ce
sera tout l’enj eu de notre algorithme d’apprentissage de reconstituer des categories syntaxiques
a partir de ces types. Ce processus sera traité en deux étapes : une étape d ’analyse et une étape
de déduction des catégories.

3.1 L’analyse des phrases typées

Comme toute phrase type’e, fournie comme donne’e d’entre’e, est issue d’une phrase syntaxique—
ment correcte, la sequence des types associés aux mots de la phrase peut, elle, etre réduite au
type 15, en utilisant les regles TF et TB comme le montre la Figure 1 (a gauche).

L’ étape d ’analyse de l’algorithme global est en fait plus complexe et consiste en deux étapes :

— une e’tape de Variabilisation des types de toutes les phrases typées, qui consiste a introduire
des Variables distinctes dans les expressions de type, en position d’opérateur, sachant qu’une
assignation forme’e par le meme mot et le meme type regoit la meme Variabilisation. Ainsi, la
phrase type'e (un, ((6, t), 15)) (homme, (e, 15)) (court, (6, t)) deVient la phrase type’e Variabili-
sée (un, w1(ac2(e, t), 15)) (homme, w3(e, 15)) (court, ac4(e, 

— une étape de recherche des reductions a 15 de la sequence des types dans chaque phrase type’e
Variabilisée (Figure 1 a droite). Cette analyse sert a identiﬁer la direction dans laquelle chaque

Un modéle d ’acqu1's1't1'on de1a syntaxe a1’a1'de d ’1'nformat1'ons sémantiques

foncteur Va trouver ses arguments. Si la regle TF (resp. TB) est employée dans une analyse,
cela signiﬁe qu’au niveau syntaxique, c’est la regle FA (resp. BA) qui devait étre employée, et
donc un opérateur / (resp. \) a été identiﬁe’ la ou une Variable avait e’té introduite. Dans l’arbre
de la Figure 1, on a fait ﬁgurer ces identiﬁcations : $1 = / est induit lors de l’application de la
regle TF et $4 = \ lors l’application de la regle TB. De plus, pour que les regles TF et TB
soient applicables sur les types Variabilise’s, il faut aussi que certaines conditions d’égalite’
entre sous—types soient Vériﬁe’es : dans l’arbre de la Figure 1, les deux sous—types soulignés
doivent étre égaux pour que TF puisse s’appliquer. Ces conditions entrainent a leur tour une
(ou des) e’galité(s) entre Variables : $2 = $3 ici. Pour implémenter cette analyse, nous nous
sommes inspire’s d’algorithmes classiques dans les grammaires hors—contexte.

t
TB

1'.

TB
/ ﬂ=\
5

e
TF

/\ 

((e,t),e) h(e,t) (at) w1(w2(e,t),e) w3(e,t) w4(e,t)

court homme court

FIG. 1 — arbres d’analyses pour une phrase type’e et pour une phrase type’e Variabilisée

A l’issue de cette étape, on obtient en fait des ensembles de contraintes sur les Variables intro-
duites dans les types Variabilise’s, qui peuvent aussi se traduire sous forme de substitution(s) sur
ces Variables (on ne détaille pas cet aspect ici).

3.2 La déduction des catégories et des fonctions de typage

L’algorithme ne sait rien des catégories syntaxiques possibles : il doit les déﬁnir. Comme un
type non basique peut correspondre a une catégorie basique (par exemple (e, t) qui correspond
a NC), il est nécessaire d’appliquer une e’tape supplémentaire de déduction de catégories, apres
l’analyse. L’idée est dans ce cas qu’un tel type ne mettra jamais en oeuvre sa nature de foncteur
dans une analyse puisque, au niveau syntaxique, il n’en est pas réellement un.

La déduction des catégories s’effectue par les regles suivantes : (1) chaque plus petit sous-
type Variabilisé distinct qui est utilise’ en tant qu’argument dans une analyse est associe’ a une
catégorie basique nouvelle (2) le type 15 en tant que re’sultat est, par convention, touj ours associe’
a la catégorie axiomatique S’. La fonction de typage est directement déduite de ces regles.

La GCC G induite par notre algorithme avec la phrase type’e donne’e en exemple dans la Fi-
gure 1 est donc la suivante : G = {(un, /(A, B)), (h0mme,A), (court, \(B,  avec A et
B catégories basiques nouvelles Vériﬁant : h(A) = (e, t), h(B) = e. Cette solution co'1'ncide,
a un renommage des catégories basique pres (A pour NC, B pour T) avec la GCC “naturelle”
cite’e en exemple jusqu’a pre’sent. Comme “Jean” est de type e (donc de catégorie B dans G),
cette grammaire reconnait aussi la phrase « Jean court », elle a donc généralise’ par rapport a
l’eXemple qui lui a été présente’. Sur cet exemple, une seule grammaire est apprise mais dans
le cas géne’ral, le re’sultat de l’algorithme est un ensemble de grammaires, chacune associe’e a
sa fonction de typage. Cette stratégie peut étre appliquée a un échantillon de phrases en en-
trée. Elle induit des contraintes pour chaque phrase et, de maniere incrémentale, propage les

D. Dudau Sofronie, I. T e11ier

contraintes compatibles (aucune Variable ne peut étre a la fois égale a / et a \) d’une phrase a
une autre. Nous avons étudie’ les proprie’tés formelles de cet algorithme et déﬁni les conditions
sur les GCC et les fonctions de typage qui assurent que le re’sultat fourni est correct et complet.

4 Expérimentations et résultats

Notre approche était, a l’origine, essentiellement théorique. Nous nous sommes initialement
place’s dans le cadre du modele d’apprentissage de Gold, dans lequel le critere d’apprenabilite’
ne requiert aucune Validation expérimentale, ni meme de “praticabilite”’ quanta la complexite’ al-
gorithmique. Mais l’algorithme que nous proposons mérite d’étre testé sur des données réelles.
Son principal inconVe’nient est qu’il nécessite des données d’entre’e tres spéciﬁques (des phrases
typées), qu’aucun corpus actuel ne fournit. Néanmoins, ces données sont moins coﬁteuses a
produire que les corpus arbore’s géne’ralement ne’cessaires pour tester les techniques d’inférence
grammaticale applique’es au langage naturel (Sakakibara 92; Kanazawa 98; Bonato,Retore O1;
Besombes,Marion 03). Les types, en effet, sont des données leXicalise’es et nous avons donc
eu l’idée de produire un ensemble de phrases typées a partir de textes étiquetés par un tagger
lexical. D’un point de Vue psycholinguistique, la donne’e de phrases type’e est aussi plus crédible
que la donne’e de structures arborescentes. Nous décrivons ici la démarche suivie pour consti-
tuer ce corpus, et la nature des eXpe’riences que nous avons menées pour Valider notre approche
et tester certaines hypotheses.

4.1 Constitution d’un corpus

Le point de départ de notre démarche est la recherche d’un ensemble de textesz. Nous avons
aussi identiﬁe’ les besoins suivants : (1) Nous souhaitions un corpus de textes en francais, aﬁn
d’étre a meme de Valider la qualite’ des grammaires qui seront apprises, et si possible libres de
droit. (2) Les textes doivent étre écrits dans une langue homogene. Notre algorithme n’est pas
spécialise’ dans l’acquisition d’une langue plutot qu’une autre : il apprend celle a laquelle ap-
partiennent les phrases qui lui sont soumises. Mais il n’apprendra quelque chose que s’il existe
des grammaires compatibles avec l’ensemble des phrases typées qui lui sont propose’es. (3)
Comme notre algorithme prétend simuler ce que fait peut—étre un enfant en phase d’acquisition
de sa langue maternelle, il est naturel de lui fournir des phrases simples, avec un Vocabulaire si
possible limité et répétitif.

Les textes de livres pour enfant, auxquels nous avons d’abord pensé, ne convenaient pas car
leur langue regorge de dialogues. Or, les e’nonce’s intervenant dans les dialogues ne sont pas des
propositions au sens logique : ils ne sont pas de type t, il faudrait donc envisager un typage
spéciﬁque pour les prendre en compte. Nous nous sommes ﬁnalement ﬁxes sur une collection
de textes produits par des enfants, et disponible sur le site WWW.l’1’10l’1’16S.1’16t. Ces textes sont des
récits écrits par des enfants de 6 a 10 ans, et corrige’es par des adultes. Ils sont constitués de
phrases assez courtes, qui ne dépassent pas 10 mots, et correctes du point de Vue syntaxique.

Ces textes ont été nettoyés a la main (pour en retirer, par exemple, les parties dialoguées) et
ont subi des traitements automatiques successifs, présentés sur la Figure 2. Ils ont d’abord e’té

2T. Desvenain, étudiant en Maitrise de science du langage a l’Université Lille 3 a contribué a ce travail

Un modele d ’acqu1's1't1'on de1a syntaxe a1’a1'de d ’1'nformat1'ons sémantiques

soumis a un étiqueteur lexical, en l’occurrence Tree Tagger3 (étape I sur le schema). De l’e’ti—
quetage complet fourni par le tagger, on n’a retenu que les informations concernant le rele de
chaque mot dans la phrase (nom, Verbe, adjectif, adverbe, determinant, etc.) et son lemme. La
partie cruciale et spe’ciﬁque de la constitution de notre corpus est la deﬁnition d’une table de
correspondance entre les étiquettes lexicales et les types sémantiques. Pour les principales eti-
quettes, la correspondance est immediate : nom propre, pronom personnel — e ; nom commun —
(e, t) ; adjectif — ((e, t), (e, t)) ; determinant — ((e, t), e). Néanmoins cette correspondance n’est
pas toujours unique : le Tree tagger ne fait par exemple pas de distinction entre les Verbes in-
transitifs et les Verbes transitifs, qui doivent pourtant recevoir des types se’mantiques différents.
La table associe donc a chaque etiquette un ensemble de types possibles (étape II). Une fois les
étiquettes transforme’es en types, il faut éliminer les phrases typées incorrectement, c’est—a—dire
ne se réduisant pas a t. Cette phase est la plus coﬁteuse, elle est réalise’e en appliquant un algo-
rithme d’analyse simpliﬁé, avec les regles TF et TB, sur tous les typages possibles (étape III).
Enﬁn, il reste encore une étape de selection manuelle pour arriver a un seul typage par phrase.

Le corpus ﬁnal, au format XML, contient du texte annote’ avec les étiquettes lexicales et avec
les types se’mantiques. Il est constitue’ de 1008 phrases contenant 5851 mots parmi lesquels
2632 sont différents. Un point essentiel a comprendre est que meme si deux mots distincts (par
exemple deux noms communs) ont été étiquetés de la meme fagon par le tagger, ils ne seront pas
conside’rés par notre algorithme comme appartenant a priori a la meme catégorie syntaxique. En
effet, lors de la phase de Variabilisation des types, des Variables diffe’rentes seront introduites
dans les types de ces deux mots. Ce n’est que si une contrainte d’égalite’ entre ces Variables
est infére’e que ces mots seront reconnus comme appartenant a la meme catégorie syntaxique.
Et cette contrainte d’e’galite’ ne sera elle—meme infére’e que si les deux mots apparaissent dans
un meme contexte (deux phrases ne différant que par ces mots, par exemple). Ce mécanisme
permet de distinguer les categories N C’ et \(T, S’), correspondant pourtant au meme type (e, 15).
La contrepartie est que l’apprentissage ne sera efﬁcace qu’en pre’sence de redondances et de
répe’titions. C’est ce que les experiences qui suivent Vont nous permettre de mesurer.

T.-.9 table dc
Tlgger currespundnnoes

(D

c........_. . @

cmpnun 6 Corpus lypé Corpus typé neﬂnyé

FIG. 2 — Traitements automatiques successifs pour obtenir un corpus type’

4.2 Efﬁcacité et validité du programme

Les proprie’tés de notre algorithme que des experiences sur corpus permettront de tester sont :
son eﬁicacite’ et sa validité. L’efﬁcacite’ mesure la praticabilite’ du programme, en termes de
ressources utilisées. La validité est un critere plus qualitatif, qui depend de la correction des
grammaires obtenues par rapport aux grammaires usuelles du langage naturel. On propose plu-
sieurs experiences pour tester ces parametres.

Pour ce qui est de l’efﬁcacite’, il convient de mesurer tout d’abord la complexite’ théorique du
probleme. Si on se contente de tirer aléatoirement des phrases typées de notre corpus et de lancer

3http 2//www.ims.uni-stuttgart.de/projekte/tc

D. Dudau Sofronie, I. T e11ier

notre algorithme sur ces donne’es, il y a fort a parier que rien d’intéressant ne sortira. La plupart
du temps, en effet, les phrases n’ont aucun mot en commun, sinon quelques mots grammati-
caux. Supposons qu’on tire 15 phrases typées et que pour chacune d’elles, l’algorithme infere
au moins 2 grammaires diffe’rentes compatibles possibles (moyenne calcule’e sur le corpus).
Si les phrases n’ont aucun mot en commun, alors le nombre total de grammaires compatibles
avec l’ensemble des données sera, par une simple combinatoire, supérieur a 215 = 32768 gram-
maires. C’est a cette aune qu’il faut se comparer. Lorsque nous avons fait cette expérience, nous
avons en fait obtenu, en moyenne sur 8 tirages différents, 2560 grammaires. La taille du lexique,
pour les 15 phrases tire’es était, en moyenne, de 85 mots parmi lesquels, en moyenne, 81 étaient
diffe’rents. Chacun des 4 mots qui se répetent a donc, en moyenne, divise’ par 2 de nombre de
grammaires solution. Ce genre de calculs prend beaucoup de temps. Dans ce qui suit, nous ne
nous attacherons pas a la complexite’ en temps, en revanche, le nombre de grammaires obtenues
a chaque étape en sortie du programme est une e’Valuation des ressources de mémoire mises en
oeuvre au cours de l’apprentissage. Quels sont les facteurs qui peuvent contribuer a réduire ce
nombre et donc a ame’liorer l’efﬁcacite’ de l’apprentissage ? Nous faisons l’hypothese que deux
facteurs indépendants doivent intervenir : d’une part l’ordre de pre’sentation des phrases, d’autre
part la redondance du Vocabulaire.

Il est naturel de supposer que face a un novice, l’entourage Va avoir tendance a d’abord produire
des e’nonce’s simples, comprenant un nombre limite’ de mots, avant de produire des phrases a la
structure plus complexe. Nous Voulons mesurer l’importance de l’effet facilitateur obtenu en
controlant l’ordre de pre’sentation des phrases typées a notre algorithme. Le protocole employé
est le suivant : (a) tirer aléatoirement une pre’sentation, contenant n phrases, du corpus initial ;
(b) calculer le nombre de grammaires obtenues apres chaque phrase; (c) ordonner en ordre
croissant, respectivement décroissant par taille, les phrases dans la pre’sentation tire’e, et reini-
tialiser le processus d’apprentissage. Les résultats obtenus en moyenne en suivant ce scénario
sont présentés sur le premier graphique de la Figure 3, pour les 8 présentations de taille n = 15
tire’es du corpus, e’Voque’es pre’cédemment. L’effet est Visible, mais limite’.

Mais, comme nous l’aVons de’ja remarque’, ce dont notre algorithme a crucialement besoin, c’est
de mots qui se répetent d’une phrase a une autre. Pour générer artiﬁciellement des énonce’s ou
de telles répétitions se produisent, nous avons automatiquement opére’ des substitutions de Vo-
cabulaire sur les phrases extraites du corpus, avant de les soumettre au programme. Nous avons
ainsi réduit les mots étiquetés respectivement par le tagger comme pronoms personnels, noms,
de’terminants, de’terminants possessifs et Verbes transitifs a uniquement deux instances diffe’—
rentes possibles (tire’es au sort) pour chaque etiquette. Le re’sultat obtenu en moyenne, pour 5
présentations de 30 phrases construites sur un lexique initial de 191 mots en moyenne, rame—
nées par les substitutions a 50 mots en moyenne, est donne’ dans le deuxieme graphique de la
Figure 3. Sur la meme ﬁgure, sont représente’es aussi les courbes ou l’ordre sur la taille a été
inverse’ et tiré ale’atoirement. L’effet facilitateur de l’ordre de pre’sentation est nettement plus
Visible et le nombre ﬁnal de grammaires obtenues bien plus raisonnable que pre’cédemment.
On constate sur ces courbes des Variations importantes : la répétition d’un seul mot type’ de’ja
rencontre’ auparavant peut provoquer une élimination de beaucoup de grammaires candidates
par la simple détection d’une incompatibilite’ (xi = / et 30,- = \ pour une certaine variable 30,-).

Les expériences précédentes ne font que compter le nombre de grammaires résultats : elles
ne permettent pas de Valider leur pertinence. Pour cela, l’ide’al serait de tester les grammaires
apprises sur de nouvelles phrases. Mais, en raison du nombre de grammaires résultats et de
la difﬁculte’ a trouver un ensemble de phrases tests construites sur le meme Vocabulaire que
celles ayant permis l’apprentissage, nous n’aVons pu mener a bien de telles expériences. Nous

Un modéle d ’acqu1's1't1'on de1a syntaxe a1’a1'de d ’1'nformat1'ons semantiques

900

 

 

3000 I I _ I I I I I I _ I I I I
ordre crolssanl —o— ordre crolssanl —o— X_*_x_X
ordre alealoire ---><--- 800 - ordre decroissanl ———x——— I ‘. -
250° ‘ ordre decroissanl  ,: ‘ ‘I
 70° - I
5 2ooo - '  - 5 600 - 
E /' E 5oo - '-,
E 1500 - I ' E '.
E I E 400 - '.
U) i U) I
42‘ 1ooo - - *2‘ soo - '
200 -
500 - -
100 -
0 J4 .. «I4 .. 0
0 2 16 0 5 10 15 20 25

Nb phrases Nb phrases

FIG. 3 — Le nombre total moyen de grammaires obtenues a chaque etape pour des presentations
de 15 phrases sur le lexique complet (a gauche) et de 30 phrases sur le lexique reduit (a droite)

devons donc chercher d’autres moyens de mesurer la Validite de notre programme. 11 serait
en particulier interessant de Veriﬁer si les mots etiquetes de la meme facon par le tagger sont
ﬁnalement reconnus comme appartenant a la meme categorie syntaxique dans les grammaires
resultats. Nous detaillons dans ce qui suit une etude de cas qui nous semble interessante, pour
une des presentations de 30 phrases avec lexique reduit evoquee precedemment. Nous nous
concentrons ici sur les noms communs auquel le type (6, t), Variabilise en ac(e, t), a ete associe.
Le lexique des noms communs est reduit a deux instances distinctes (“soleil” et “ﬂeur”). Les
differentes categories syntaxiques qui peuvent etre inferees par le programme a partir de ce
type correspondent soit a la categorie \(T, S’), soit a / (T, S’) (pour h(T) = e), soit a une ou
plusieurs categories basiques. Nous comptons, a chaque etape de la presentation, le nombre de
grammaires qui contiennent une de ces instanciations de categorie pour les noms communs (la
Valeur as = / , n’a jamais ete inferee, c’est pourquoi une colonne manque dans le tableau) :

Nom commuu Cat.

occ Cat avec m :

soleil - 2;
soleil 2
soleil
soleil
ﬂeur
soleil

soleil

ﬂeur
soleil

soleil

soleil

 

Des la phrase 3, les deux noms communs ont chacun ete presentes au moins une fois. La phrase
7 induit une categorie incorrecte pour “ﬂeur”. Cette phrase et l’analyse incorrecte (presentee
synthetiquement sous forme de terme) a laquelle elle a donne lieu est la suivante :

TB(TB(TB((Pi6W'6, 6), (7'69a7'd6, (6, (6, t))>), (ma, ((6, 15), 6)>), (fl6m', (6, 15»)

L’application de la regle TB entre le type de “Pierre regarde”, (e, t), et celui de “ma”, ((6, t), e)
n’est pas linguistiquement legitime. Elle entraine l’identiﬁcation de \ comme Valeur possible
pour la Variable du type associe au nom “ﬂeur”. Comme le determinant “ma” a une seule occur-
rence dans toute la presentation, cette Valeur est propagee a travers d’autres analyses, conduisant
a la situation ﬁnale decrite dans le tableau, ou elle fait jeu egal avec la categorie basique legi-
time. Notons par ailleurs qu’entre les phrases 10 et 18, deux categories syntaxiques basiques

D. Dudau Sofronie, I. T e11ier

ont été introduites. C’est la propagation de contraintes d’e’galite’ qui ﬁnit, a partir de la phrase
22, par uniﬁer ces deux categories. La meme evolution a été observe’e pour le nombre total d’in—
troductions de categories basiques lors de présentations quelconques. Mais, pour discre’diter la
catégorie \(T, S’) associe’e aux noms communs, et les associer a une seule catégorie syntaxique
basique, il sufﬁrait d’une seule phrase ou “ma” introduit un nom commun en position sujet.

5 Conclusion

Nous proposons dans cet article un programme d’inférence grammaticale dirige’e par la seman-
tique. Les grammaires formelles qui représentent la syntaxe ont de bonnes proprie’tés et sont
adapte’es a un apprentissage syntaxico—se’mantique. Les résultats théoriques ont été teste’s sur
des données réelles du langage naturel. Les expe’riences re’alise’es sur corpus, bien qu’encore li-
mitées, montrent que l’acquisition du premier langage est un processus complexe, tres sensible
a un environnement propice. Ainsi, la taille du lexique et l’ordre dans lequel les phrases sont
présente’es ont une grande inﬂuence sur les performances du systeme.

Références

ADRIAANS P. W. (1992), Language Learning from a Categorial Perspective. PhD thesis, University of
Amsterdam, Amsterdam, The Netherlands.

BAR—HILLEL Y., GAIFMAN C., SHAMIR E. (1960), On Categorial and phrase structure grammars.
Bulletin of the Research Council of Israel, 9F.

BESOMBES J., MARION J.Y. (2003), Apprentissage de langages réguliers d’arbres et applications. In
TAL, vol 44, n°1/2003, pages 121-153.

BONATO R., RETORE C. (2001) Learning rigid lambek grammars and minimalist grammars from
structured sentences. In Proceedings of the Third Learning Language in Logic Workshop, pages 23-34.

BRENT M. R. (1996), Computational Approaches to Language Acquisition. MIT Press

DUDAU—SOFRONIE D., TELLIER I., TOMMASI M. (2001), From logic to grammars via types. In
Proceedings ofLearning Language in Logic (LLL) 2001, pages 35-46.

DUDAU—SOFRONIE D., TELLIER I., TOMMASI M. (2003), A learnable class of Classical Categorial
Grammars from typed examples. In Proceedings of the 8th conference on Formal Grammar, pages
77-88

GOLD E. M. (1967), Language identiﬁcation in the limit. Information and Control, 10 : pages 447-474
JANSSEN T. M. V. (1997) Compositionality In J. V. Benthem and A. ter Meulen, editors, Handbook of
Logic and Language, pages 417-473. MIT Press.

KANAZAWA M. (1998), Learnable Classes of Categorial Grammars, The European Association for
Logic, Language and Information. CLSI Publications.

MONTAGUE R. (1974), Formal Philosophy; Selected papers of Richard Montague, Yale University
Press.

OEHRLE R.T., BACH E., WHEELER D., editors. Categorial Grammars and Natural Language Struc-
tures. D. Reidel Publishing Company, Dordrecht, 1988.

PINKER S. (1994), The Language Instinct Penguin Press, London

SAKAKIBARA Y. (1992), Efﬁcient learning of context—free grammars from positive structural examples,
Information and Computation, 97(1) : pages 23-60.

