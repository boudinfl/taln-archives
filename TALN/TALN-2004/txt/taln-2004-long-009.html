<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
"http://www.w3.org/TR/html4/loose.dtd">
<html><head><title>Cat&#233;gorisation de patrons syntaxiques par Self Organizing Maps</title>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
</head>
<body>
<div style="page-break-before:always; page-break-after:always"><div><p>TALN 2004, F&#232;s, 19-21 avril 2004
</p>
<p>Cat&#233;gorisation de patrons syntaxiques par Self Organizing Maps
</p>
<p>Jean-Jacques Mariage et Gilles Bernard
</p>
<p>Groupe CSAR, Laboratoire d'Intelligence Artificielle &#8211; Universit&#233; Paris 8
2, rue de la Libert&#233;, 93526 St Denis Cdx, France
</p>
<p>jam@ai.univ-paris8.fr
</p>
<p>R&#233;sum&#233; &#8211; Abstract
Dans cet article, nous pr&#233;sentons quelques r&#233;sultats en cat&#233;gorisation automatique de donn&#233;es
du langage naturel sans recours &#224; des connaissances pr&#233;alables. Le syst&#232;me part d&#8217;une liste de
formes grammaticales fran&#231;aises et en construit un graphe qui repr&#233;sente les cha&#238;nes
rencontr&#233;es dans un corpus de textes de taille raisonnable ; les liens sont pond&#233;r&#233;s &#224; partir de
donn&#233;es statistiques extraites du corpus. Pour chaque cha&#238;ne de formes grammaticales
significative, un vecteur refl&#233;tant sa distribution est extrait et pass&#233; &#224; un r&#233;seau de neurones de
type carte topologique auto-organisatrice. Une fois le processus d&#8217;apprentissage termin&#233;, la
carte r&#233;sultante est convertie en un graphe d&#8217;&#233;tiquettes g&#233;n&#233;r&#233;es automatiquement, utilis&#233; dans
un tagger ou un analyseur de bas niveau. L&#8217;algorithme est ais&#233;ment adaptable &#224; toute langue
dans la mesure o&#249; il ne n&#233;cessite qu&#8217;une liste de marques grammaticales et un corpus
important (plus il est gros, mieux c&#8217;est). Il pr&#233;sente en outre un int&#233;r&#234;t suppl&#233;mentaire qui est
son caract&#232;re dynamique : il est extr&#234;mement ais&#233; de recalculer les donn&#233;es &#224; mesure que le
corpus augmente.
</p>
<p>The present paper presents some results in automatic categorization of natural language data
without previous knowledge. The system starts with a list of French grammatical items, builds
them into a graph that represents the strings encountered in a reasonable corpus of texts; the
links are weighted based upon statistical data extracted from the corpus. For each significant
string of grammatical items a vector reflecting its distribution is extracted, and fed into a Self-
Organizing Map neural network. Once the learning process is achieved, the resulting map will
be converted into a graph of automatically generated tags, used in a tagger or a shallow parser.
The algorithm may easily be adapted to any language, as it needs only the list of grammatical
markers and a large corpus (the bigger the better). Another point of interest is its dynamic
character: it is easy to recompute the data as the corpus grows.
</p>
<p>Keywords &#8211; Mots Cl&#233;s
Langues naturelles, r&#233;seaux neuronaux, extraction de connaissances.
Natural languages, neural networks, knowledge extraction.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Jean-Jacques Mariage et Gilles Bernard
</p>
<p>1 Introduction
Les r&#233;sultats pr&#233;sent&#233;s ici sont la premi&#232;re &#233;tape d'un projet dont le but est d'&#233;tiqueter et
d'analyser des textes volumineux en recourant le moins possible &#224; des connaissances
pr&#233;alables qu'il est n&#233;cessaire de sp&#233;cifier manuellement, particuli&#232;rement dans des contextes
o&#249; des textes &#233;tiquet&#233;s &#224; la main sont inexistants ou tr&#232;s rares.
</p>
<p>Le seul moyen de pr&#233;dire les &#233;tiquettes pour les mots inconnus est de se baser sur des mots
connus et des r&#232;gles grammaticales, les uns comme les autres &#233;tant sp&#233;cifi&#233;s manuellement et
reposant essentiellement sur des connaissances expertes ou des textes &#233;tiquet&#233;s au pr&#233;alable,
et donc sujets &#224; contradiction, &#224; la fois entre les types de documents et les domaines et entre
experts.
</p>
<p>Le but de notre syst&#232;me est, dans une premi&#232;re &#233;tape, d'automatiser la construction des r&#232;gles
grammaticales qui peuvent ensuite &#234;tre affin&#233;es par l'expert. Les donn&#233;es initiales sont
restreintes &#224; des donn&#233;es grammaticales, ce qui constitue la partie essentielle de toute langue
naturelle, et &#224; un corpus le plus &#233;tendu possible. La langue consid&#233;r&#233;e ici est le fran&#231;ais, mais
notre syst&#232;me est actuellement en cours d'application au grec et &#224; l'arabe.
</p>
<p>2 Collecte des donn&#233;es
La liste des items grammaticaux doit &#234;tre r&#233;alis&#233;e manuellement, mais sa d&#233;finition est
relativement simple. Nous avons utilis&#233; une liste de 311 items grammaticaux ; cette liste
contient des items ambigus qui sont pr&#233;sents &#224; la fois dans l'inventaire lexical et grammatical,
comme ton en fran&#231;ais, qui peut &#234;tre un substantif ou un adjectif possessif (ton livre, son ton),
et aussi des items grammaticaux dont la fonction est ambigu&#235; (le livre et je le livre).
Dans le souci d'assurer la reproductibilit&#233; de nos exp&#233;rimentations, nous avons s&#233;lectionn&#233; le
corpus parmi les bases de documents fran&#231;ais en libre acc&#232;s, disponibles sur Internet1. Notre
s&#233;lection comporte tous les textes &#233;crits apr&#232;s 1750 (les textes r&#233;cents sont rarement
accessibles en raison des droits d'auteur). Ce corpus contient des textes &#233;lectroniques de
sources diverses (scann&#233;s ou saisis manuellement), comportant beaucoup d'erreurs. Il est
compos&#233; de divers types de documents (surtout des romans, mais aussi des documents
techniques et des p&#233;riodiques). Nous avons r&#233;alis&#233; les premi&#232;res exp&#233;rimentations sur une
partie du corpus (environ 600 000 mots), mais les tests actuels sont ex&#233;cut&#233;s sur un corpus dix
fois plus important.
</p>
<p>3 Construction du graphe de cha&#238;nes grammaticales
La premi&#232;re &#233;tape du processus, consiste &#224; extraire les formes grammaticales du corpus et &#224; en
construire un graphe qui contient tous les liens possibles entre les items grammaticaux et leurs
contextes.
</p>
<p>1 Ils proviennent essentiellement de l'Association des Bibliophiles Universels (ABU), dont le corpus est
disponible en libre acc&#232;s &#224; l'adresse suivante : http://cedric.cnam.fr/ABU.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Cat&#233;gorisation de patrons syntaxiques par Self Organizing Maps
</p>
<p>Chaque texte du corpus est d&#233;coup&#233; en paragraphes, les paragraphes en phrases, et les phrases
en segments de phrases d&#233;limit&#233;s par la ponctuation (chaque &#233;tape produisant un encodage
SGML). L'&#233;tape de d&#233;coupage en paragraphes a pour fonction essentielle de d&#233;sambigu&#239;ser la
ponctuation reli&#233;e &#224; des d&#233;buts de type &quot;A.1.2&quot; et &#224; l'usage des guillemets et des retours
chariot. L'&#233;tape suivante de d&#233;coupage en phrases d&#233;sambigu&#239;se la ponctuation finale ; la
ponctuation qui subsiste est utilis&#233;e pour produire les segments de phrases.
</p>
<p>Le processus entra&#238;ne une certaine quantit&#233; d'erreurs, due soit aux choix de programmation,
soit, plus fr&#233;quemment, &#224; des incoh&#233;rences dans certains des fichiers d'entr&#233;e ; ainsi, certaines
phrases comportent v&#233;ritablement des paragraphes entiers, ou m&#234;me plus. Mais cela ne
semble pas affecter le r&#233;sultat final.
</p>
<p>Les mots des segments de phrases sont ensuite &#233;tiquet&#233;s comme suit : chaque mot qui ne
figure pas dans la liste des items grammaticaux est associ&#233; &#224; une &#233;tiquette parmi quatre
possibles : (1) Mot initial de phrase en majuscules, (2) autre mot en majuscules, (3) nombre,
et (4) mot en minuscules. L'&#233;tiquetage des items grammaticaux est r&#233;alis&#233; dans leur forme en
minuscules.
</p>
<p>Les &#233;tiquettes sont extraites et les occurrences successives de la m&#234;me &#233;tiquette sont
remplac&#233;es par une seule &#233;tiquette. Le processus produit des patterns comme &#8220; *1* ne *2* pas
le *3* *4* &#8221;, o&#249; *2*, par exemple, repr&#233;sente une s&#233;quence de mots de type 2. Les formes ne
contenant que des &#233;tiquettes non grammaticales sont supprim&#233;es.
</p>
<p>Approximativement 70.000 formes diff&#233;rentes sont g&#233;n&#233;r&#233;es ; les items grammaticaux ont
environ 300.000 occurrences, et les 4 &#233;tiquettes lexicales mentionn&#233;s ant&#233;rieurement ont de
l'ordre de 200.000 occurrences (ces &#233;tiquettes repr&#233;sentent des cha&#238;nes d'items lexicaux, pas
seulement des items lexicaux).
</p>
<p>Un graphe est ensuite construit, avec un n&#339;ud pour chaque symbole rencontr&#233; dans le corpus,
plus un n&#339;ud pour le symbole sp&#233;cial repr&#233;sentant le d&#233;but des segments de phrases, et un
n&#339;ud pour la terminaison de segments de phrases.
</p>
<p>Les liens en sortie de chaque n&#339;ud, except&#233; le n&#339;ud de terminaison, repr&#233;sentent l'arbre des
symboles successifs : la racine est le symbole contenu dans le n&#339;ud et il y a autant de liens
que de symboles rencontr&#233;s &#224; la suite de ce symbole dans les patterns. Chaque lien est pond&#233;r&#233;
avec le nombre de fois o&#249; le symbole fils suit le symbole p&#232;re dans les patterns.
</p>
<p>(a occs
#
(b occs # (d occs #) (e occs #))
(c occs (f occs #)) )
</p>
<p>o&#249; a, b, c, &#8230; sont les symboles, # est le symbole terminal, et occs, le nombre d&#8217;occurrences.
</p>
<p>Chaque n&#339;ud re&#231;oit autant de liens entrants qu'il y a d'arbres qui le contiennent. Les liens
sortants sont &#233;tiquet&#233;s selon les liens entrants (ou le n&#339;ud racine). Pour chaque lien, un lien
inverse est construit, de cette mani&#232;re nous pourrons appliquer, dans une &#233;tape ult&#233;rieure, les
programmes qui suivent non seulement aux successeurs de tout n&#339;ud donn&#233;, mais aussi &#224; ses
pr&#233;d&#233;cesseurs.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Jean-Jacques Mariage et Gilles Bernard
</p>
<p>4 Extraction des cha&#238;nes grammaticales
Les cha&#238;nes grammaticales sont extraites du graphe comme suit : nous s&#233;lectionnons chaque
n&#339;ud qui contient un item grammatical. Pour chaque n&#339;ud, les &#233;tiquettes d&#233;terminent dans
quelle mesure les liens entrants sont en relation avec les liens sortants. De cette mani&#232;re, nous
suivons les liens et extrayons les cha&#238;nes jusqu'au dernier n&#339;ud contenant un item
grammatical ou jusqu'&#224; ce que le dernier n&#339;ud significatif (voir ci-dessous) soit atteint. Les
poids des liens sortants du dernier n&#339;ud de chaque cha&#238;ne, &#233;tiquet&#233;s avec le d&#233;but de chaque
cha&#238;ne, constituent un vecteur de la distribution des successeurs de la cha&#238;ne.
</p>
<p>5 Calcul des vecteurs
Pour chaque lien sortant, la fr&#233;quence locale (le poids du lien divis&#233; par la somme des poids
de tous les liens sortants de la m&#234;me &#233;tiquette) est calcul&#233;e et divis&#233;e par la fr&#233;quence globale
du n&#339;ud fils (le nombre total d'occurrences de son symbole, divis&#233; par le nombre total
d'occurrences de tous les symboles diff&#233;rents des symboles de d&#233;but et de fin). Nous obtenons
ainsi un vecteur de la d&#233;viation de la distribution locale par rapport &#224; la distribution globale.
</p>
<p>a : [dev(a) dev(b) dev(c) dev(d) dev(e) dev(f) dev(#)]
ab : [dev(a) dev(b) dev(c) dev(d) dev(e) dev(f) dev(#)]
abd : [dev(a) dev(b) dev(c) dev(d) dev(e) dev(f) dev(#)]
ac : [dev(a) dev(b) dev(c) dev(d) dev(e) dev(f) dev(#)]
</p>
<p>o&#249; dev(x) = locfreq (x) / globfreq (x), &#1;
</p>
<p>locfreq (x) = nombre d&#8217;occurrences de x dans le contexte (apr&#232;s a, ab, abd, etc.) / nombre
d&#8217;occurrences du pr&#233;d&#233;cesseur.
</p>
<p>globfreq (x) = nombre d&#8217;occurrences de x / nombre d&#8217;occurrences de tous les symboles
</p>
<p>Plut&#244;t que de calculer simplement les fr&#233;quences locales, nous avons choisi de calculer la
d&#233;viation, suite &#224; des exp&#233;rimentations ant&#233;rieures (avec des techniques de regroupement par
lien unique) qui montraient que les vecteurs de fr&#233;quences locales &#233;taient trop fortement
semblables pour &#234;tre s&#233;par&#233;s dans l'&#233;tape suivante (un tr&#232;s petit nombre de neurones &#233;taient
s&#233;lectionn&#233;s pour la totalit&#233; de l'ensemble d'apprentissage). Cela est d&#251; au caract&#232;re massif de
la distribution des &#233;tiquettes g&#233;n&#233;riques (repr&#233;sentant des donn&#233;es non grammaticales).
L'&#233;limination des composantes des vecteurs correspondant &#224; des &#233;tiquettes lexicales a entra&#238;n&#233;
d'importantes pertes d'information (ainsi, la distribution de Mr, Mlle et des formes semblables
refl&#232;te le fait qu'elles sont habituellement suivies par des mots en majuscules).
La m&#233;thode de calcul de la d&#233;viation que nous avons choisie pr&#233;sente toutefois un
inconv&#233;nient : l'information apport&#233;e par la fr&#233;quence locale du symbole terminal (le nombre
de fois o&#249; la fin de segment appara&#238;t) est perdue, parce que parler de fr&#233;quence globale pour la
fin de segment n'a aucune signification.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Cat&#233;gorisation de patrons syntaxiques par Self Organizing Maps
</p>
<p>6 S&#233;lection des vecteurs
Une d&#233;viation &#233;lev&#233;e peut &#234;tre due &#224; une fr&#233;quence locale importante ou &#224; une faible
fr&#233;quence globale, auquel cas les r&#233;sultats peuvent &#234;tre trompeurs. Le recours habituel &#224; un
seuil absolu ne semblait pas souhaitable, en ce qu'il &#233;liminerait, par exemple, un mot qui
appara&#238;t 15 fois avec toujours le m&#234;me successeur, ce qui semble &#234;tre un r&#233;sultat beaucoup
plus significatif que 100 occurrences d'un mot polys&#233;mique avec 50 successeurs diff&#233;rents.
Nous avons donc envisag&#233; un seuil qui soit fonction de la polys&#233;mie de la cha&#238;ne de mots,
telle qu'elle est indiqu&#233;e par la longueur de son profil, i.e. le nombre de successeurs ayant une
fr&#233;quence plus &#233;lev&#233;e qu'une valeur &#304; donn&#233;e, choisie faible.
</p>
<p>Le caract&#232;re significatif (l'importance statistique) Sign(s) d'une cha&#238;ne est mesur&#233; par la
formule :
</p>
<p>)(
)()/)(()(
</p>
<p>sL
sLMsN
</p>
<p>sSign
&#8722;
</p>
<p>=
</p>
<p>o&#249; N(s) est le nombre d'occurrences de s, L(s) la longueur de son profil, et M la valeur
minimale, consid&#233;r&#233;e comme significative par successeur (15 dans l'exemple donn&#233;).
</p>
<p>Les cha&#238;nes retenues sont celles dont le caract&#232;re significatif est sup&#233;rieur &#224; une valeur &#537;
donn&#233;e, choisie petite.
</p>
<p>Pour g&#233;n&#233;rer l'ensemble de vecteurs d'apprentissage &#224; passer au r&#233;seau de neurones, nous
avons fix&#233; ces valeurs comme suit : &#304; = 0.01, M = 20, &#537; = 0. Pour produire les donn&#233;es
utilis&#233;es en phase de test, destin&#233;es &#224; &#233;valuer la capacit&#233; de reconnaissance et de
g&#233;n&#233;ralisation du r&#233;seau, nous avons adopt&#233; les r&#233;glages suivants : &#304; = 0.05, M = 10, &#537; = -0.1.
</p>
<p>7 La carte topologique auto-organisatrice
Les vecteurs de donn&#233;es repr&#233;sentant les d&#233;viations des successeurs de chaque cha&#238;ne sont
pass&#233;s en entr&#233;e &#224; un r&#233;seau de neurones de type carte topologique auto-organisatrice (not&#233; ci-
apr&#232;s SOM) de (Kohonen, 1982). SOM est un algorithme d'apprentissage non-supervis&#233; qui
cartographie les classes de donn&#233;es d'entr&#233;e en r&#233;alisant une projection, depuis leur espace
multidimensionnel d'origine, dans l'espace interne de sa m&#233;moire qui est bi-dimensionnel.
L'algorithme construit un ordonnancement topologique des relations implicites qu'il d&#233;couvre
entre les classes de donn&#233;es. Sa repr&#233;sentation interne facilite l'analyse des relations entre
classes, par la r&#233;duction dimensionnelle qu'elle leur applique, et minimise les effets de
mauvaise classification. La structure bi-dimensionnelle de l'espace de repr&#233;sentation fournit
une interface de visualisation conviviale o&#249; la similarit&#233; entre les classes de donn&#233;es est
encod&#233;e dans la proximit&#233; entre les amas d'unit&#233;s distribu&#233;e sur la carte : des formes reli&#233;es
dans l'espace des donn&#233;es sont situ&#233;es proches les unes des autres dans l'espace de la carte.
Les principaux param&#232;tres de ce mod&#232;le sont : le nombre de neurones, le maillage des unit&#233;s
de la carte, la topologie de la carte, tore ou plane, (Mariage, 1997), le taux d'apprentissage, le
voisinage, et les fonctions qui d&#233;terminent sa d&#233;croissance dans l'espace et dans le temps, le
rayon de propagation, le nombre d'it&#233;rations, le nombre de phases d'apprentissage.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Jean-Jacques Mariage et Gilles Bernard
</p>
<p>Dans les exp&#233;rimentations rapport&#233;es ci-dessous, les param&#232;tres &#233;taient r&#233;gl&#233;s manuellement,
avec deux phases d'entra&#238;nement. En premi&#232;re phase d'apprentissage (d&#8217;ordonnancement
grossier), le nombre d&#8217;it&#233;rations &#233;tait entre 25 % et 30 % du nombre d&#8217;it&#233;rations choisi en
phase d'affinage. Nous avons utilis&#233; une carte bi-dimensionnelle. Les unit&#233;s &#233;taient organis&#233;es
en maillage hexagonal. L'entra&#238;nement &#233;tait effectu&#233; avec pr&#233;s de 7 000 vecteurs de 301
composantes pr&#233;sent&#233;s au r&#233;seau en ordre al&#233;atoire. Les m&#233;moires des unit&#233;s &#233;taient
initialis&#233;es avec des valeurs al&#233;atoires de &#177; 0.05 autour de la moyenne des valeurs des
vecteurs de donn&#233;es. La r&#232;gle d'activation &#233;tait la distance euclidienne, la plus petite d&#233;signant
l'unit&#233; de meilleur appariement. Le taux d'apprentissage &#302;(t), 0 &lt; &#302;(t) &lt; 1, &#233;voluait en fonction
du temps. Il &#233;tait affect&#233; d'une d&#233;croissance lin&#233;aire en 1 - (t / Ttotal). En premi&#232;re phase
d'entra&#238;nement, &#302; &#233;tait r&#233;gl&#233; &#224; 0.5, tandis qu'en phase d'affinage, il avait une valeur de 0.05.
Deux fonctions de voisinage ont &#233;t&#233; test&#233;es : le bubble algorithm (Kohonen, 1982, 1995) et le
voisinage gaussien (Ritter, Martinetz et Schulten, 1989) sans pr&#233;senter de diff&#233;rences
significatives. Dans les deux cas, la taille du voisinage d&#233;croissait jusqu&#8217;&#224; un rang autour de
l&#8217;unit&#233; gagnante.
</p>
<p>Plusieurs s&#233;ries d&#8217;exp&#233;rimentations ont &#233;t&#233; r&#233;alis&#233;es sur les m&#234;mes donn&#233;es avec des
configurations diff&#233;rentes de SOM. Les premiers essais avaient pour but d&#8217;estimer
approximativement la cat&#233;gorisation sur une carte r&#233;duite en fonction de la variation des
param&#232;tres de configuration de SOM. La carte comportait 96 unit&#233;s (12 * 8). Le rayon de
voisinage initial &#233;tait choisi de mani&#232;re &#224; couvrir la totalit&#233; des unit&#233;s en premi&#232;re phase. En
seconde phase, un rayon de 5 rangs d&#8217;unit&#233;s &#233;tait adopt&#233;. L'entra&#238;nement &#233;tait effectu&#233; pendant
36 000 it&#233;rations en premi&#232;re phase et 120 000 en phase d'affinage. Les r&#233;sultats obtenus sont
d&#233;crits dans la section 9 ci-dessous. Ils ont &#233;t&#233; &#233;tablis manuellement en comptant les
occurrences des cha&#238;nes grammaticales class&#233;es par les unit&#233;s. Une deuxi&#232;me s&#233;rie
d&#8217;&#233;valuations confirme ces r&#233;sultats. La carte comportait 260 unit&#233;s (26 * 10). Un voisinage
initial de 7 rangs autour de l&#8217;&#233;l&#233;ment actif &#233;tait choisi en phase d&#8217;ordonnancement (&#167; 87 %
des unit&#233;s). En phase d&#8217;affinage, un rayon de 3 rangs d&#8217;unit&#233;s (&#167; 19 %) &#233;tait adopt&#233;.
L'entra&#238;nement &#233;tait effectu&#233; pendant 80 000 it&#233;rations en premi&#232;re phase et durant 320 000 en
phase d'affinage. Nous donnons ci-dessous (section 8) les principales caract&#233;ristiques de la
cat&#233;gorisation obtenue. Une analyse automatique plus approfondie des r&#233;sultats est en cours.
Cette &#233;tape va nous permettre d&#8217;affiner encore les r&#233;glages des param&#232;tres de configuration du
r&#233;seau SOM et donc d'am&#233;liorer la r&#233;solution de la topologie.
</p>
<p>8 Estimation de la qualit&#233; d&#8217;apprentissage
La qualit&#233; d&#8217;apprentissage &#233;tait &#233;valu&#233;e en fonction de deux crit&#232;res : la r&#233;solution de la carte
et la pr&#233;servation de la topologie, calcul&#233;s sur les deux ensembles de donn&#233;es d&#8217;apprentissage
et de test. Dans les tableaux 1 et 2 ci-dessous, chaque ligne indique le meilleur r&#233;sultat obtenu
parmi 50 essais d&#8217;entra&#238;nement. Les nombres d&#8217;it&#233;rations sont exprim&#233;s en milliers.
</p>
<p>L&#8217;erreur de quantification est la distance moyenne entre chaque vecteur de donn&#233;es et l&#8217;unit&#233;
de meilleur appariement qu&#8217;il d&#233;clenche. Elle refl&#232;te la r&#233;solution de la carte topologique.
</p>
<p>( ) Xwx
X
</p>
<p>x
</p>
<p>bmui i
</p>
<p>1
</p>
<p>0
</p>
<p>2&#166;
&#8722;
</p>
<p>=
</p>
<p>&#8722;</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Cat&#233;gorisation de patrons syntaxiques par Self Organizing Maps
</p>
<p>L&#8217;erreur topographique est la proportion de vecteurs de donn&#233;es pour laquelle le premier et le
second bmus ne sont pas adjacents. Elle rend compte de la pr&#233;servation de la topologie.
</p>
<p>( ) Xbmubmud
X
</p>
<p>x
</p>
<p>node 1
1
</p>
<p>0
21&#166;
</p>
<p>&#8722;
</p>
<p>=
</p>
<p>&#8800;&#8722;
</p>
<p>Donn&#233;es d&#8217;entra&#238;nement Donn&#233;es de test
Nombre
</p>
<p>d&#8217;it&#233;rations Erreur de
quantification
</p>
<p>Erreur
topographique
</p>
<p>Erreur de
quantification
</p>
<p>Erreur
topographique
</p>
<p>10/40 71.675 0.0462 79.263 0.0464
20/80 67.665 0.0288 76.313 0.0368
30/120 65.502 0.0496 74.793 0.0570
40/160 63.936 0.0470 73.993 0.0459
50/200 63.556 0.0530 73.605 0.0544
60/240 62.842 0.0481 72.924 0.0626
70/280 61.805 0.0462 72.205 0.0509
80/320 61.558 0.0375 72.119 0.0429
</p>
<p>Tableau 1. Estimation de la qualit&#233; d&#8217;apprentissage
</p>
<p>La qualit&#233; d&#8217;apprentissage &#233;tait mesur&#233;e &#224; partir de la diff&#233;rence entre les deux meilleurs
essais d&#8217;entra&#238;nement successifs, calcul&#233;e comme : (E(t-1) - E(t)) / E(t), o&#249; E est la mesure et t
l&#8217;indice temporel de l&#8217;essai. L&#8217;&#233;volution de l&#8217;erreur &#233;tait compar&#233;e &#224; un seuil S (un param&#232;tre
d&#233;fini par l&#8217;utilisateur), utilis&#233; comme crit&#232;re d&#8217;arr&#234;t. Ici, une valeur de 0.001 &#233;tait choisie
pour S. La qualit&#233; d&#8217;entra&#238;nement &#233;tait estim&#233;e sur les donn&#233;es de test de mani&#232;re &#224;
s&#233;lectionner les essais d&#8217;apprentissage ayant la meilleure capacit&#233; de g&#233;n&#233;ralisation.
</p>
<p>Donn&#233;es d&#8217;entra&#238;nement Donn&#233;es de test
Nombre
</p>
<p>d&#8217;it&#233;rations Erreur de
quantification
</p>
<p>Erreur
topographique
</p>
<p>Erreur de
quantification
</p>
<p>Erreur
topographique
</p>
<p>10/40 - - - -
20/80 0.0559 0.604 0.0372 0.261
30/120 0.0320 -0.419 0.0199 -0.354
40/160 0.0239 0.055 0.0107 0.195
50/200 0.0059 -0.113 0.0052 -0.185
60/240 0.0113 0.102 0.0093 -0.131
70/280 0.0168 0.041 0.0010 0.230
80/320 0.0040 0.232 0.0012 0.186
</p>
<p>Tableau 2. Evolution des mesures d&#8217;erreur</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Jean-Jacques Mariage et Gilles Bernard
</p>
<p>Le seuil de diminution de l&#8217;erreur &#233;tait atteint pour 70/280 mille it&#233;rations d&#8217;entra&#238;nement. Un
essai suppl&#233;mentaire a &#233;t&#233; r&#233;alis&#233; avec 80/320 mille it&#233;rations. La stagnation de l&#8217;erreur de
quantification confirme la bonne r&#233;solution de la carte sans l&#8217;am&#233;liorer significativement.
L&#8217;erreur topographique indique le meilleur degr&#233; de pr&#233;servation de la topologie obtenu.
</p>
<p>9 Cat&#233;gories obtenues
Dans plusieurs de nos essais, les r&#233;sultats furent plut&#244;t surprenants en ce qu'une grande
quantit&#233; de cha&#238;nes grammaticales &#233;taient rassembl&#233;es selon l'influence d'un &#233;l&#233;ment dans la
cha&#238;ne, parfois le dernier, mais dans d'autres cas le plus important quelle que soit sa position.
Ainsi, des cha&#238;nes contenant je, marque ayant une forte capacit&#233; pr&#233;dictive sur son contexte,
&#233;taient concentr&#233;es dans un amas de quatre &#224; cinq unit&#233;s (Cf. Figure 1 ci dessous). Les cha&#238;nes
finissant par un syntagme nominal complet &#233;taient class&#233;es dans un groupe d'unit&#233;s couvrant
environ les deux tiers de la carte, avec de part et d&#8217;autre, les cha&#238;nes incompl&#232;tes r&#233;parties
dans deux zones distinctes de moindre importance. Un autre fait int&#233;ressant est que la
distribution des cha&#238;nes grammaticales situ&#233;es en d&#233;but de segment de phrase refl&#232;te avec
pr&#233;cision leur distribution globale : les cha&#238;nes contenant le symbole initial &#233;taient
g&#233;n&#233;ralement class&#233;es dans la m&#234;me unit&#233; que les cha&#238;nes ne le comportant pas. La
classification groupait ensemble les pronoms de la troisi&#232;me personne du singulier et les
cha&#238;nes qui les contenaient, mais dans des unit&#233;s s&#233;par&#233;es. Ainsi il, elle, on &#233;taient dans la
m&#234;me unit&#233;, et qu'il, qu'elle, qu'on &#233;taient dans une autre unit&#233; (proche), group&#233;s avec qui, et
puis il, puis elle &#233;taient encore dans une autre. Ainsi, &#233;taient pr&#233;serv&#233;es &#224; la fois la relation
entre il-elle-on, mais aussi l'influence de certains des marqueurs grammaticaux environnants
se faisait sentir et produisait une cat&#233;gorie diff&#233;rente (m&#234;me si elle est situ&#233;e &#224; proximit&#233;).
Dans un amas contigu, il en &#233;tait de m&#234;me pour les pronoms de la troisi&#232;me personne du
pluriel (ils, elles). Les cha&#238;nes termin&#233;es par des d&#233;terminants possessifs son-sa-vos, etc.
&#233;taient diff&#233;renci&#233;es des autres d&#233;terminants. Les formes verbales &#233;taient nettement s&#233;par&#233;es
des formes nominales. A l&#8217;intersection de ces deux zones, huit unit&#233;s encodaient les formes
verbales r&#233;flexives. Une forte proportion des unit&#233;s qui se d&#233;clenchaient pr&#233;f&#233;rentiellement
pour des syntagmes nominaux, encodaient un nombre de formes nettement sup&#233;rieur &#224; la
moyenne des autres unit&#233;s, refl&#233;tant la plus forte densit&#233; des syntagmes nominaux dans la
langue.
</p>
<p>Figure 1 : Exemple de topologie des principales cat&#233;gories obtenues.
</p>
<p>Le fait que SOM capture effectivement ces diff&#233;rences, et d'autres de m&#234;me nature, fut une
grande surprise, et nous n'y avons pas encore trouv&#233; une explication convaincante. Par</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Cat&#233;gorisation de patrons syntaxiques par Self Organizing Maps
</p>
<p>exemple, la diff&#233;rence entre les pronoms singuliers et pluriels ne pouvait pas &#234;tre d&#233;couverte
facilement, car les pronoms de la troisi&#232;me personne sont suivis par les m&#234;mes marqueurs
ind&#233;pendamment de leur nombre (ex. il le / ils le, il ne / ils ne, etc.), et aucune information
n'est donn&#233;e dans les items lexicaux : Les deux sortes sont suivies de l'&#233;tiquette g&#233;n&#233;rique
*4*. Seule possibilit&#233; d'explication : l'influence de l'auxiliaire (portant les marques de
nombre). Le ph&#233;nom&#232;ne est encore plus curieux pour les d&#233;terminants possessifs : nous ne
voyons pas quel contexte peut diff&#233;rencier son et le. La m&#234;me chose se produit pour les
formes d'&#234;tre et avoir. La seule hypoth&#232;se que nous pouvons proposer est que ces diff&#233;rences
n'exercent pas d'influence sur les cat&#233;gories de successeurs de ces cha&#238;nes, mais qu'elles
influencent la distribution locale de ces cat&#233;gories ; nous persistons &#224; rechercher des
explications &#224; cette influence. D'autres r&#233;sultats &#233;taient plus ais&#233;ment explicables : toutes les
cha&#238;nes n&#233;gatives du type &quot;il ne se *4*&quot; sont rassembl&#233;es dans un amas, essentiellement en
raison de la fr&#233;quence de la marque n&#233;gative pas qui suit normalement ces cha&#238;nes ; les noms
propres d&#233;terminants (Mr et semblables) &#233;taient group&#233;s ensembles, &#224; cause de la fr&#233;quence
des mots avec majuscule &#224; l'initiale...
A la suite de ces premiers r&#233;sultats plut&#244;t encourageants, une entreprise de classification
manuelle des cha&#238;nes d'entr&#233;es est en cours. Cette &#233;tape va nous permettre de s&#233;lectionner au
mieux les param&#232;tres de configuration du r&#233;seau SOM et d'&#233;valuer plus finement la qualit&#233;
des r&#233;sultats.
</p>
<p>10 Vers l'&#233;tiquetage
Quelle que soit l'explication de ces ordonnancements topologiques, une chose est s&#251;re : les
cha&#238;nes grammaticales peuvent &#234;tre group&#233;es et diff&#233;renci&#233;es en consid&#233;rant uniquement la
distribution des formes qui les suivent. Apr&#232;s avoir consid&#233;r&#233; ici uniquement le contexte
suivant, nous entendons maintenant explorer la classification des m&#234;me cha&#238;nes en prenant en
compte &#224; la fois les pr&#233;d&#233;cesseurs et les successeurs, tout en affinant notre cha&#238;ne de
traitement actuelle ; et augmenter significativement la taille de notre corpus initial. Des
r&#233;sultats d&#8217;exp&#233;rimentations ant&#233;rieures sur la d&#233;limitation des constituants de phrase r&#233;alis&#233;e
avec le processus stochastique r&#233;current de Harris (1968), peuvent aussi &#234;tre int&#233;gr&#233;es ici. Les
cat&#233;gories de formes grammaticales produites par notre cha&#238;ne de traitement, avec les
perfectionnements n&#233;cessaires mentionn&#233;s ci-dessus, peuvent &#234;tre utilis&#233;es comme &#233;tiquettes
de la mani&#232;re suivante : les propri&#233;t&#233;s des cha&#238;nes &#233;l&#233;mentaires d'un mot sont bien connues (il
est suivi par un verbe, du par un nom) et peuvent &#234;tre propag&#233;es aux cha&#238;nes regroup&#233;es dans
la m&#234;me unit&#233;.Mais, peut-&#234;tre plus important encore, les items ambigus cessent d'&#234;tre ambigus
dans les cha&#238;nes (ou au moins leur ambigu&#239;t&#233; diminue de mani&#232;re importante). Sans qu'il lui
soit donn&#233; de r&#232;gle, notre syst&#232;me a d&#233;couvert implicitement ce ph&#233;nom&#232;ne, comme le montre
le fait qu'il classe les cha&#238;nes contenant des items ambigus (en particulier le, la, les, comme
article ou comme pronom), dans des groupes compl&#232;tements diff&#233;rents, en s&#233;parant nettement
les articles, r&#233;unis avec d'autres d&#233;terminants, et les pronoms (bien que le regroupement pour
ces derniers soit moins clair, sans doute &#224; cause de la faible quantit&#233; d'emploi de le la les en
tant que pronom).
</p>
<p>Revenons, en conclusion, sur la grande simplicit&#233; de notre principe de traitement : qui est la
condition de sa portabilit&#233; (&#224; d'autres langues) et de sa reproductibilit&#233;, tout comme la
condition requise pour laisser la structure intrins&#232;que des donn&#233;es &#233;merger.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Jean-Jacques Mariage et Gilles Bernard
</p>
<p>R&#233;f&#233;rences
BERNARD G. (1997), Experiments on distributional categorization of lexical items with Self
Organizing Maps, Proceedings of WSOM&#8217;97, Helsinki, pp. 304-309.
BERNARD G. (2003), D&#233;tection automatique de structures syntaxiques, Proceedings of the 8th
International Symposium on Social Communication, Santiago de Cuba.
</p>
<p>BRILL E. (1997), Unsupervised Learning of Disambiguation Rules for Part of Speech Tagging,
Natural Language Processing Using Very Large Corpora, Kluwer Academic Press.
</p>
<p>BRILL E. 1995, Unsupervised learning of disambiguation rules for part of speech tagging.
</p>
<p>BRISCOE J. (1994), Prospects for practical parsing: robust statistical techniques, Corpus-based
Research into Language: A Feschrift for Jan Aarts, de Haan &amp; Oostdijk, Ed, Amsterdam.
HARRIS Z. (1968), Mathematical structures of language, John Wiley &amp; Sons, New York.
JONES B. (1994), Can punctuation help parsing?, Proceedings of the 15th International
Conference on Computational Linguistics, Kyoto, Japan.
JOSHI A. K. (1985), How much context-sensitivity is necessary for characterizing structural
descriptions Tree adjoining grammars?, Natural Language Processing Theoretical,
Computational and Psychological Perspectives, Dowty, Karttunen, Zwicky, Ed, Cambridge
University Press, New York.
</p>
<p>JOSHI A. K. (1987), The convergence of mildly context-sensitive grammatical formalisms,
Processing of Linguistic Structure, Santa Cruz.
KOHONEN T. (1982), Self-organized formation of topologically correct feature maps,
Biological Cybernetics, 43, 59-69.
</p>
<p>KOHONEN T. (1995), Self Organizing Maps, Springer, Heidelberg.
LARI K., YOUNG S. J. (1990), The estimation of stochastic context-free grammars using the
Inside-Outside algorithm, Computer Speech and Language Processing.
</p>
<p>MARIAGE J.-J. (1997), Dynamic neighborhoods in Self Organizing Maps, Proceedings of
WSOM&#8217;97, Helsinki, pp. 175-180.
</p>
<p>MERIALDO B. (1994), Tagging English Text with a Probabilistic Model, Computational
Linguistics (20), p. 155.
</p>
<p>RITTER H. J., MARTINETZ T. M., SCHULTEN K. J. (1989), Topology conserving maps for
learning visuo-motor coordination, Neural Networks, Vol. 2 (3), pp. 159-168.
</p>
<p>SCHUETZE H. (1995), Distributional Part-of-Speech Tagging, in EACL 7.</p>

</div></div>
</body></html>