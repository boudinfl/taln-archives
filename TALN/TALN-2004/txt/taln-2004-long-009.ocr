TALN 2004, F és, I9-21 avril 2004

Categorisation de patrons syntaxiques par Self Organizing Maps

J ean-J acques Mariage et Gilles Bernard

Groupe CSAR, Laboratoire d'Intelligence Artificielle — Universite Paris 8
2, rue de la Liberté, 93526 St Denis Cdx, France
jam@ai.univ—paris8.fr

Resume — Abstract

Dans cet article, nous presentons quelques resultats en categorisation automatique de donnees
du langage naturel sans recours a des connaissances prealables. Le systeme part d’une liste de
formes grammaticales frangaises et en construit un graphe qui represente les chaines
rencontrees dans un corpus de textes de taille raisonnable ; les liens sont ponderes a partir de
donnees statistiques extraites du corpus. Pour chaque chaine de formes grammaticales
signiﬁcative, un vecteur reﬂetant sa distribution est extrait et passe a un reseau de neurones de
type carte topologique auto-organisatrice. Une fois le processus d’apprentissage termine, la
carte resultante est convertie en un graphe d’etiquettes generees automatiquement, utilise dans
un tagger ou un analyseur de bas niveau. L’algorithme est aisement adaptable a toute langue
dans la mesure ou il ne necessite qu’une liste de marques grammaticales et un corpus
important (plus il est gros, mieux c’est). Il presente en outre un interet supplementaire qui est
son caractere dynarnique : il est extremement aise de recalculer les donnees a mesure que le
corpus augmente.

The present paper presents some results in automatic categorization of natural language data
without previous knowledge. The system starts with a list of French grammatical items, builds
them into a graph that represents the strings encountered in a reasonable corpus of texts; the
links are weighted based upon statistical data extracted from the corpus. For each signiﬁcant
string of grammatical items a vector reﬂecting its distribution is extracted, and fed into a Self-
Organizing Map neural network. Once the learning process is achieved, the resulting map will
be converted into a graph of automatically generated tags, used in a tagger or a shallow parser.
The algorithm may easily be adapted to any language, as it needs only the list of grammatical
markers and a large corpus (the bigger the better). Another point of interest is its dynamic
character: it is easy to recompute the data as the corpus grows.

Keywords — Mots Cles

Langues naturelles, reseaux neuronaux, extraction de connaissances.
Natural languages, neural networks, knowledge extraction.

Jean-Jacques Mariage et Gilles Bernard

1 Introduction

Les resultats presentes ici sont la premiere etape d'un projet dont le but est d'etiqueter et
d'analyser des textes volumineux en recourant le moins possible a des connaissances
prealables qu'il est necessaire de speciﬁer manuellement, particulierement dans des contextes
ou des textes etiquetes a la main sont inexistants ou tres rares.

Le seul moyen de predire les etiquettes pour les mots inconnus est de se baser sur des mots
connus et des regles grammaticales, les uns comme les autres etant speciﬁes manuellement et
reposant essentiellement sur des connaissances expertes ou des textes etiquetes au prealable,
et donc sujets a contradiction, a la fois entre les types de documents et les domaines et eI1tre
experts.

Le but de notre systeme est, dans une premiere etape, d'automatiser la construction des regles
grammaticales qui peuvent ensuite etre afﬁnees par l'expert. Les donnees initiales sont
restreintes a des donnees grammaticales, ce qui constitue la partie essentielle de toute langue
naturelle, et a un corpus le plus etendu possible. La langue consideree ici est le francais, mais
notre systeme est actuellement en cours d'application au grec et a l'arabe.

2 Collecte des données

La liste des items grammaticaux doit etre realisee manuellement, mais sa deﬁnition est
relativement simple. Nous avons utilise une liste de 311 items grammaticaux ; cette liste
contient des items ambigus qui sont presents a la fois dans l'inventaire lexical et grammatical,
comme ton en francais, qui peut etre un substantif ou un adjectif possessif (ton livre, son ton),
et aussi des items grammaticaux dont la fonction est ambigue (le livre et je le livre).

Dans le souci d'assurer la reproductibilite de nos experimentations, nous avons selectionne le
corpus parmi les bases de documents francais en libre acces, disponibles sur Internetl. Notre
selection comporte tous les textes ecrits apres 1750 (les textes recents sont rarement
accessibles en raison des droits d'auteur). Ce corpus contient des textes electroniques de
sources diverses (scarmes ou saisis manuellement), comportant beaucoup d'erreurs. Il est
compose de divers types de documents (surtout des romans, mais aussi des documents
techniques et des periodiques). Nous avons realise les premieres experimentations sur une
partie du corpus (environ 600 000 mots), mais les tests actuels sont executes sur un corpus dix
fois plus important.

3 Construction du graphe de chaines grammaticales
La premiere etape du processus, consiste a extraire les formes grammaticales du corpus et a en

construire un graphe qui contient tous les liens possibles entre les items grammaticaux et leurs
contextes.

1 11s proviennent essentiellement de l'Association des Bibliophiles Universels (ABU), dont le corpus est

disponible en libre acces a l'adresse suivante : http://cedric.cnam.fr/ABU.

Catégorisation de patrons syntaxiques par Self Organizing Maps

Chaque texte du corpus est decoupé en paragraphes, les paragraphes en phrases, et les phrases
en segments de phrases délimités par la ponctuation (chaque étape produisant un encodage
SGML). L'étape de découpage en paragraphes a pour fonction essentielle de desambiguiser la
ponctuation reliee a des debuts de type "A.l.2" et a l'usage des guillemets et des retours
chariot. L'étape suivante de découpage en phrases désambiguise la ponctuation ﬁnale; la
ponctuation qui subsiste est utilisée pour produire les segments de phrases.

Le processus entraine une certaine quantite d'erreurs, due soit aux choix de programmation,
soit, plus fréquemment, a des incohérences dans certains des ﬁchiers d'entrée ; ainsi, certaines
phrases comportent veritablement des paragraphes entiers, ou meme plus. Mais cela ne
semble pas affecter le resultat ﬁnal.

Les mots des segments de phrases sont ensuite etiquetés comme suit: chaque mot qui ne
ﬁgure pas dans la liste des items grammaticaux est associé a une etiquette parmi quatre
possibles : (1) Mot initial de phrase en majuscules, (2) autre mot en majuscules, (3) nombre,
et (4) mot en minuscules. L'étiquetage des items grammaticaux est realise dans leur forme en
minuscules.

Les étiquettes sont extraites et les occurrences successives de la meme etiquette sont
remplacées par une seule etiquette. Le processus produit des patterns comme “ *l* ne *2* pas
le *3* *4* ”, ou *2*, par exemple, représente une sequence de mots de type 2. Les formes ne
contenant que des etiquettes non grammaticales sont supprimées.

Approximativement 70.000 formes différentes sont générées; les items grammaticaux ont
environ 300.000 occurrences, et les 4 étiquettes lexicales mentionnés antérieurement ont de
l'ordre de 200.000 occurrences (ces étiquettes représentent des chaines d'items lexicaux, pas
seulement des items lexicaux).

Un graphe est ensuite construit, avec un noeud pour chaque symbole rencontre dans le corpus,
plus un noeud pour le symbole special représentant le debut des segments de phrases, et un
noeud pour la terminaison de segments de phrases.

Les liens en sortie de chaque noeud, excepté le noeud de terminaison, représentent l'arbre des
symboles successifs : la racine est le symbole contenu dans le noeud et il y a autant de liens
que de symboles rencontres a la suite de ce symbole dans les patterns. Chaque lien est pondéré
avec le nombre de fois ou le symbole ﬁls suit le symbole pere dans les patterns.

(a occs
#
(b occs # (d occs #) (e occs #))
(c occs (f occs #)) )

ou a, b, c,  sont les symboles, # est le symbole terminal, et occs, le nombre d’occurrences.

Chaque noeud recoit autant de liens entrants qu'il y a d'arbres qui le contiennent. Les liens
sortants sont étiquetés selon les liens entrants (ou le noeud racine). Pour chaque lien, un lien
inverse est construit, de cette maniere nous pourrons appliquer, dans une étape ultérieure, les
programmes qui suivent non seulement aux successeurs de tout noeud donné, mais aussi a ses
prédécesseurs.

Jean-Jacques Mariage et Gilles Bernard

4 Extraction des chaines grammaticales

Les chaines grammaticales sont extraites du graphe comme suit : nous selectionnons chaque
noeud qui contient un item grammatical. Pour chaque noeud, les etiquettes determinent dans
quelle mesure les liens entrants sont en relation avec les liens sortants. De cette maniere, nous
suivons les liens et extrayons les chaines jusqu'au derI1ier noeud contenant un item
grammatical ou jusqu'a ce que le dernier noeud signiﬁcatif (voir ci-dessous) soit atteint. Les
poids des liens sortants du dernier noeud de chaque chaine, etiquetes avec le debut de chaque
chaine, constituent un vecteur de la distribution des successeurs de la chaine.

5 Calcul des vecteurs

Pour chaque lien sortant, la frequence locale (le poids du lien divise par la somme des poids
de tous les liens sortants de la meme etiquette) est calculee et divisee par la frequence globale
du noeud ﬁls (le nombre total d'occurrences de son symbole, divise par le nombre total
d'occurrences de tous les symboles differents des symboles de debut et de ﬁn). Nous obtenons
ainsi un vecteur de la deviation de la distribution locale par rapport a la distribution globale.

a : [dev(a) dev(b) dev(c) dev(d) dev(e) dev(f) dev(#)]

ab : [dev(a) dev(b) dev(c) dev(d) dev(e) dev(f) dev(#)]

abd : [dev(a) dev(b) dev(c) dev(d) dev(e) dev(f) dev(#)]

ac : [dev(a) dev(b) dev(c) dev(d) dev(e) dev(f) dev(#)]
ou dev(x) = locfreq (x) / globfreq (x),

locfreq (x) = nombre d’occurrences de x dans le contexte (apres a, ab, abd, etc.) / nombre
d’occurrences du predecesseur.

globfreq (X) = nombre d’occurrences de x / nombre d’occurrences de tous les symboles

Plutot que de calculer simplement les frequences locales, nous avons choisi de calculer la
deviation, suite a des experimentations anterieures (avec des techniques de regroupement par
lien unique) qui montraient que les vecteurs de frequences locales etaient trop fortement
semblables pour etre separes dans l'etape suivante (un tres petit nombre de neurones etaient
selectionnes pour la totalite de l'ensemble d'apprentissage). Cela est dﬁ au caractere massif de
la distribution des etiquettes generiques (representant des donnees non grammaticales).
L'elimination des composantes des vecteurs correspondant a des etiquettes lexicales a entraine
d'importantes pertes d'information (ainsi, la distribution de Mr, Mlle et des formes semblables
reﬁete le fait qu'elles sont habituellement suivies par des mots en majuscules).

La methode de calcul de la deviation que nous avons choisie presente toutefois un
inconvenient : l'information apportee par la frequence locale du symbole terminal (le nombre
de fois ou la ﬁn de segment apparait) est perdue, parce que parler de frequence globale pour la
ﬁn de segment n'a aucune signiﬁcation.

Catégorisation de patrons syntaxiques par Self Organizing Maps

6 Sélection des vecteurs

Une deviation élevee peut etre due a une frequence locale importante ou a une faible
frequence globale, auquel cas les resultats peuvent etre trompeurs. Le recours habituel a un
seuil absolu ne semblait pas souhaitable, en ce qu'il éliminerait, par exemple, un mot qui
apparait 15 fois avec toujours le meme successeur, ce qui semble etre un résultat beaucoup
plus signiﬁcatif que 100 occurrences d'un mot polysemique avec 50 successeurs différents.
Nous avons donc envisage un seuil qui soit fonction de la polysémie de la chaine de mots,
telle qu'elle est indiquee par la longueur de son proﬁl, i.e. le nombre de successeurs ayant une
frequence plus élevee qu'une Valeur 3 donnée, choisie faible.

Le caractere signiﬁcatif (l'importance statistique) Sign(s) d'une chaine est mesure par la
formule:

. (N (S) / M) - L(s)
S1gn(s)=e
L(s)
ou N(s) est le nombre d'occurrences de s, L(s) la longueur de son proﬁl, et M la Valeur
minimale, considerée comme signiﬁcative par successeur (15 dans l'exemple donne).

Les chaines retenues sont celles dont le caractere signiﬁcatif est supérieur a une Valeur 6
donnée, choisie petite.

Pour générer l'ensemble de vecteurs d'apprentissage a passer au reseau de neurones, nous
avons ﬁxe ces valeurs comme suit: 3 = 0.01, M = 20, 6 = 0. Pour produire les données
utilisees en phase de test, destinees a évaluer la capacite de reconnaissance et de
generalisation du réseau, nous avons adopté les réglages suivants : 3 = 0.05, M = 10, 6 = -0.1.

7 La carte topologique auto-organisatrice

Les vecteurs de données representant les deviations des successeurs de chaque chaine sont
passes en entree a un reseau de neurones de type carte topologique auto-organisatrice (note ci-
apres SOM) de (Kohonen, 1982). SOM est un algorithme d'apprentissage non-supervise qui
cartographie les classes de donnees d'entrée en realisant une projection, depuis leur espace
multidimensionnel d'origine, dans l'espace interne de sa mémoire qui est bi-dimensionnel.
L'algorithme construit un ordonnancement topologique des relations implicites qu'il découvre
entre les classes de données. Sa representation interne facilite l'analyse des relations entre
classes, par la reduction dimensionnelle qu'elle leur applique, et minimise les effets de
mauvaise classiﬁcation. La structure bi-dimensionnelle de l'espace de representation fournit
une interface de visualisation conviviale ou la similarité entre les classes de donnees est
encodée dans la proximite entre les amas d'unites distribuée sur la carte : des formes reliées
dans l'espace des donnees sont situees proches les unes des autres dans l'espace de la carte.
Les principaux parametres de ce modele sont : le nombre de neurones, le maillage des unites
de la carte, la topologie de la carte, tore ou plane, (Mariage, 1997), le taux d'apprentissage, le
voisinage, et les fonctions qui déterminent sa decroissance dans l'espace et dans le temps, le
rayon de propagation, le nombre d'itérations, le nombre de phases d'apprentissage.

Jean-Jacques Mariage et Gilles Bernard

Dans les expérimentations rapportées ci-dessous, les parametres etaient réglés manuellement,
avec deux phases d'entrainement. En premiere phase d'apprentissage (d’ordonnancement
grossier), le nombre d’itérations était entre 25 % et 30 % du nombre d’iterations choisi en
phase d'afﬁnage. Nous avons utilise une carte bi-dimensionnelle. Les unites étaient organisées
en maillage hexagonal. L'entrainement était effectué avec pres de 7 000 vecteurs de 301
composantes présentés au réseau en ordre aléatoire. Les mémoires des unites étaient
initialisées avec des valeurs aléatoires de 1 0.05 autour de la moyenne des valeurs des
vecteurs de données. La regle d'activation était la distance euclidienne, la plus petite désignant
l'unité de meilleur appariement. Le taux d'apprentissage oz“), 0 < 0:“) < 1, évoluait en fonction
du temps. Il etait affecté d'une décroissance linéaire en 1 - (t / Ttmz). En premiere phase
d'entrainement, oz était réglé a 0.5, tandis qu'en phase d'afﬁnage, il avait une Valeur de 0.05.
Deux fonctions de voisinage ont ete testees : le bubble algorithm (Kohonen, 1982, 1995) et le
voisinage gaussien (Ritter, Martinetz et Schulten, 1989) sans presenter de differences
signiﬁcatives. Dans les deux cas, la taille du voisinage décroissait jusqu’a un rang autour de
l’unité gagnante.

Plusieurs series d’expérimentations ont été réalisees sur les memes données avec des
conﬁgurations differentes de SOM. Les premiers essais avaient pour but d’estimer
approximativement la categorisation sur une carte réduite en fonction de la Variation des
parametres de conﬁguration de SOM. La carte comportait 96 unites (12 * 8). Le rayon de
voisinage initial etait choisi de maniere a couvrir la totalité des unites en premiere phase. En
seconde phase, un rayon de 5 rangs d’unités était adopté. L'entrainement était effectué pendant
36 000 iterations en premiere phase et 120 000 en phase d'afﬁnage. Les résultats obtenus sont
décrits dans la section 9 ci-dessous. Ils ont été établis manuellement en comptant les
occurrences des chaines grammaticales classées par les unites. Une deuxieme série
d’évaluations conﬁrme ces résultats. La carte comportait 260 unites (26 * 10). Un voisinage
initial de 7 rangs autour de l’élément actif etait choisi en phase d’ordonnancement (3 87 %
des unites). En phase d’afﬁnage, un rayon de 3 rangs d’unités (z 19 %) était adopte.
L'entrainement etait effectué pendant 80 000 iterations en premiere phase et durant 320 000 en
phase d'afﬁnage. Nous donnons ci-dessous (section 8) les principales caractéristiques de la
categorisation obtenue. Une analyse automatique plus approfondie des resultats est en cours.
Cette étape Va nous permettre d’afﬁner encore les réglages des parametres de conﬁguration du
réseau SOM et donc d'améliorer la resolution de la topologie.

8 Estimation de la qualité d’apprentissage

La qualité d’apprentissage était évaluée en fonction de deux criteres : la resolution de la carte
et la preservation de la topologie, calculés sur les deux ensembles de donnees d’apprentissage
et de test. Dans les tableaux 1 et 2 ci-dessous, chaque ligne indique le meilleur résultat obtenu
parmi 50 essais d’entrainement. Les nombres d’iterations sont exprimés en milliers.

L’ erreur de quantiﬁcation est la distance moyenne eI1tre chaque vecteur de données et l’unité
de meilleur appariement qu’il declenche. Elle reﬁete la resolution de la carte topologique.

 /X

x=0

Catégorisation de patrons syntaxiques par Self Organizing Maps

L’ erreur topographique est la proportion de vecteurs de donnees pour laquelle le premier et le
second bmus ne sont pas adj acents. Elle rend compte de la preservation de la topologie.

Ednode (bmul —bmu2) ;é1/X

x:

Donnees d’entrainement Donnees de test
Nombre
d’itér-ations Erreur de Erreur Erreur de Erreur
quantiﬁcation topographique quantiﬁcation topographique

10/40 71.675 0.0462 79.263 0.0464
20/80 67.665 0.0288 76.313 0.0368
30/120 65.502 0.0496 74.793 0.0570
40/160 63.936 0.0470 73.993 0.0459
50/200 63.556 0.0530 73.605 0.0544
60/240 62.842 0.0481 72.924 0.0626
70/280 61.805 0.0462 72.205 0.0509
80/320 61.558 0.0375 72.119 0.0429

Tableau 1. Estimation de la qualite d’apprentissage

La qualité d’apprentissage etait mesurée a partir de la difference entre les deux meilleurs
essais d’entrainement successifs, calculee comme : (E("1) ' Em) / E(’), ou E est la mesure et t
l’indice temporel de l’essai. L’évolution de l’erreur etait comparée a un seuil S (un parametre
deﬁni par l’utilisateur), utilise comme critere d’arret. Ici, une valeur de 0.001 était choisie
pour S. La qualité d’entrainement etait estimee sur les donnees de test de maniere a
selectionner les essais d’apprentissage ayant la meilleure capacité de generalisation.

Donnees d’ entrainement Donnees de test
Nombre
d’itér-ations Erreur de Erreur Erreur de Erreur
quantiﬁcation topographique quantiﬁcation topographique
10/40 - - - -

20/80 0.0559 0.604 0.0372 0.261
30/120 0.0320 -0.419 0.0199 -0.354
40/160 0.0239 0.055 0.0107 0.195
50/200 ()_()()59 -0.113 0.0052 -0.185
60/240 0.0113 0.102 0.0093 -0.131
70/280 0.0168 0.041 0.0010 0.230
80/320 0.0040 0.232 0.0012 0.186

Tableau 2. Evolution des mesures d’erreur

Jean-Jacques Mariage et Gilles Bernard

Le seuil de diminution de l’erreur etait atteint pour 70/280 mille iterations d’entrainement. Un
essai supplementaire a ete realise avec 80/320 mille iterations. La stagnation de l’erreur de
quantiﬁcation conﬁrme la bonne resolution de la carte sans l’ameliorer signiﬁcativement.
L’ erreur topographique indique le meilleur degre de preservation de la topologie obtenu.

9 Categories obtenues

Dans plusieurs de nos essais, les resultats furent plutet surprenants en ce qu'une grande
quantite de chaines grammaticales etaient rassemblees selon l'inﬂuence d'un element dans la
chaine, parfois le dernier, mais dans d'autres cas le plus important quelle que soit sa position.
Ainsi, des chaines contenant je, marque ayant une forte capacite predictive sur son contexte,
etaient concentrees dans un amas de quatre a cinq unites (Cf. Figure 1 ci dessous). Les chaines
ﬁnissant par un syntagme nominal complet etaient classees dans un groupe d'unites couvrant
environ les deux tiers de la carte, avec de part et d’autre, les chaines incompletes reparties
dans deux zones distinctes de moindre importance. Un autre fait interessant est que la
distribution des chaines grammaticales situees en debut de segment de phrase reﬂete avec
precision leur distribution globale : les chaines contenant le symbole initial etaient
generalement classees dans la meme unite que les chaines ne le comportant pas. La
classiﬁcation groupait ensemble les pronoms de la troisieme personne du singulier et les
chaines qui les contenaient, mais dans des unites separees. Ainsi il, elle, on etaient dans la
meme unite, et qu’il, qu’elle, qu’on etaient dans une autre unite (proche), groupes avec qui, et
puis il, puis elle etaient encore dans une autre. Ainsi, etaient preservees a la fois la relation
entre il-elle-on, mais aussi l'inﬂuence de certains des marqueurs grammaticaux environnants
se faisait sentir et produisait une categorie differente (meme si elle est situee a proximite).
Dans un amas contigu, il en etait de meme pour les pronoms de la troisieme personne du
pluriel (ils, elles). Les chaines terminees par des determinants possessifs son-sa-vos, etc.
etaient differenciees des autres determinants. Les formes verbales etaient nettement separees
des formes nominales. A l’intersection de ces deux zones, huit unites encodaient les formes
verbales reﬂexives. Une forte proportion des unites qui se declenchaient preferentiellement
pour des syntagmes nominaux, encodaient un nombre de formes nettement superieur a la
moyenne des autres unites, reﬂetant la plus forte densite des syntagmes nominaux dans la
langue.

 

Figure 1 : Exemple de topologie des principales categories obtenues.

Le fait que SOM capture effectivement ces differences, et d'autres de meme nature, fut une
grande surprise, et nous n'y avons pas encore trouve une explication convaincante. Par

Catégorisation de patrons syntaxiques par Self Organizing Maps

exemple, la difference entre les pronoms singuliers et pluriels ne pouvait pas etre decouverte
facilement, car les pronoms de la troisieme personne sont suivis par les memes marqueurs
independarnrnent de leur nombre (ex. il le / ils le, il ne / ils ne, etc.), et aucune information
n'est donnee dans les items lexicaux: Les deux sortes sont suivies de l'etiquette génerique
*4*. Seule possibilite d'explication : l'inﬂuence de l'auxiliaire (portant les marques de
nombre). Le phenomene est encore plus curieux pour les determinants possessifs : nous ne
voyons pas quel contexte peut differencier son et le. La meme chose se produit pour les
formes d'étre et avoir. La seule hypothese que nous pouvons proposer est que ces differences
n'exercent pas d'inﬂuence sur les categories de successeurs de ces chaines, mais qu'elles
inﬂuencent la distribution locale de ces categories ; nous persistons a rechercher des
explications a cette inﬂuence. D'autres resultats etaient plus aisement explicables : toutes les
chaines negatives du type "il ne se *4 *" sont rassemblees dans un amas, essentiellement en
raison de la frequence de la marque negative pas qui suit normalement ces chaines ; les noms
propres determinants (Mr et semblables) etaient groupes ensembles, a cause de la frequence
des mots avec majuscule a l'initiale...

A la suite de ces premiers resultats plutet encourageants, une entreprise de classiﬁcation
manuelle des chaines d'entrees est en cours. Cette etape Va nous permettre de selectionner au
mieux les parametres de conﬁguration du reseau SOM et d'evaluer plus ﬁnement la qualite
des resultats.

10 Vers l'étiquetage

Quelle que soit l'explication de ces ordonnancements topologiques, une chose est sﬁre : les
chaines grammaticales peuvent etre groupees et differenciees en considerant uniquement la
distribution des formes qui les suivent. Apres avoir considere ici uniquement le contexte
suivant, nous entendons maintenant explorer la classiﬁcation des meme chaines en prenant en
compte a la fois les predecesseurs et les successeurs, tout en afﬁnant notre chaine de
traitement actuelle ; et augmenter signiﬁcativement la taille de notre corpus initial. Des
resultats d’experimentations anterieures sur la delimitation des constituants de phrase realisee
avec le processus stochastique recurrent de Harris (1968), peuvent aussi etre integrees ici. Les
categories de formes grammaticales produites par notre chaine de traitement, avec les
perfectionnements nécessaires mentionnes ci-dessus, peuvent etre utilisees comme etiquettes
de la maniere suivante : les proprietes des chaines elementaires d'un mot sont bien connues (il
est suivi par un verbe, du par un nom) et peuvent etre propagees aux chaines regroupees dans
la meme unite.Mais, peut-etre plus important encore, les items ambigus cessent d'etre ambigus
dans les chaines (ou au moins leur ambigu'1'te diminue de maniere importante). Sans qu'il lui
soit donne de regle, notre systeme a decouvert implicitement ce phenomene, comme le montre
le fait qu'il classe les chaines contenant des items ambigus (en particulier le, la, les, comme
article ou comme pronom), dans des groupes completements différents, en separant nettement
les articles, reunis avec d'autres determinants, et les pronoms (bien que le regroupement pour
ces derniers soit moins clair, sans doute a cause de la faible quantite d'emploi de le la les en
tant que pronom),

Revenons, en conclusion, sur la grande simplicite de notre principe de traitement : qui est la
condition de sa portabilite (a d'autres langues) et de sa reproductibilite, tout comme la
condition requise pour laisser la structure intrinseque des donnees emerger.

Jean-Jacques Mariage et Gilles Bernard

Références

BERNARD G. (1997), Experiments on distributional categorization of lexical items with Self
Organizing Maps, Proceedings of WSOM’97, Helsinki, pp. 304-309.

BERNARD G. (2003), Detection automatique de structures syntaxiques, Proceedings of the 8th
International Symposium on Social Communication, Santiago de Cuba.

BRILL E. (1997), Unsupervised Learning of Disambiguation Rules for Part of Speech Tagging,
Natural language Processing Using Very Large Corpora, Kluwer Academic Press.

BRILL E. 1995, Unsupervised learning of disambiguation rules for part of speech tagging.

BRISCOE J. (1994), Prospects for practical parsing: robust statistical techniques, Corpus-based
Research into language: A Feschrift for Jan Aarts, de Haan & Oostdijk, Ed, Amsterdam.

HARRIS Z. (1968), Mathematical structures of language, John Wiley & Sons, New York.

JONES B. (1994), Can punctuation help parsing?, Proceedings of the 15th International
Conference on Computational Linguistics, Kyoto, Japan.

JOSIH A. K. (1985), How much context-sensitivity is necessary for characterizing structural
descriptions Tree adjoining grammars?, Natural Language Processing Theoretical,
Computational and Psychological Perspectives, Dowty, Karttunen, Zwicky, Ed, Cambridge
University Press, New York.

JOSIH A. K. (1987), The convergence of mildly context-sensitive grammatical formalisms,
Processing of Linguistic Structure, Santa Cruz.

KOHONEN T. (1982), Self-organized formation of topologically correct feature maps,
Biological Cybernetics, 43, 59-69.

KOHONEN T. (1995), Self Organizing Maps, Springer, Heidelberg.

LARI K., YOUNG S. J. (1990), The estimation of stochastic context-free grammars using the
Inside-Outside algorithm, Computer Speech and Language Processing.

MARIAGE J .-J . (1997), Dynamic neighborhoods in Self Organizing Maps, Proceedings of
WSOM’97, Helsinki, Pp. 175-180.

MERIALDO B. (1994), Tagging English Text with a Probabilistic Model, Computational
Linguistics (20), p. 155.

RI'I'IER H. J., MARTINETZ T. M., SCHULTEN K. J. (1989), Topology conserving maps for
learning visuo-motor coordination, Neural Networks, Vol. 2 (3), pp. 159-168.

SCHUETZE H. (1995), Distributional Part-of-Speech Tagging, in EACL 7.

