<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
"http://www.w3.org/TR/html4/loose.dtd">
<html><head><title>Couplage d&#8217;un &#233;tiqueteur morpho-syntaxique et d&#8217;un analyseur partiel repr&#233;sent&#233;s sous la forme d&#8217;automates finis pond&#233;r&#233;s</title>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
</head>
<body>
<div style="page-break-before:always; page-break-after:always"><div><p>TALN 2004, F&#232;s, 19&#8211;21 avril 2004
</p>
<p>Couplage d&#8217;un &#233;tiqueteur morpho-syntaxique et d&#8217;un
analyseur partiel repr&#233;sent&#233;s sous la forme d&#8217;automates finis
</p>
<p>pond&#233;r&#233;s
</p>
<p>Alexis Nasr, Alexandra Volanschi
&#0;
</p>
<p>LATTICE-CNRS (UMR 8094)
Universit&#233; Paris 7
</p>
<p>{alexis.nasr, alexandra.volanschi}@linguist.jussieu.fr
</p>
<p>R&#233;sum&#233; - Abstract
</p>
<p>Cet article pr&#233;sente une mani&#232;re d&#8217;int&#233;grer un &#233;tiqueteur morpho-syntaxique et un analyseur
partiel. Cette integration permet de corriger des erreurs effectu&#233;es par l&#8217;&#233;tiqueteur seul. L&#8217;&#233;tique-
teur et l&#8217;analyseur ont &#233;t&#233; r&#233;alis&#233;s sous la forme d&#8217;automates pond&#233;r&#233;s. Des r&#233;sultats sur un
corpus du fran&#231;ais ont montr&#233; une dimintion du taux d&#8217;erreur de l&#8217;ordre de &#1; &#2; &#4; .
</p>
<p>This paper presents a method of integrating a part-of-speech tagger and a chunker. This in-
tegration lead to the correction of a number of errors made by the tagger when used alone.
Both tagger and chunker are implemented as weighted finite state machines. Experiments on a
French corpus showed a decrease of the word error rate of about &#1; &#2; &#4; .
</p>
<p>Mots-clefs &#8211; Keywords
</p>
<p>Analyse morpho-syntaxique, analyse syntaxique partielle, automates finis pond&#233;r&#233;s
Part-of-speech tagging, chunking, weighted finite state machines
</p>
<p>1 Introduction
</p>
<p>L&#8217;&#233;tiquetage morpho-syntaxique constitue souvent une &#233;tape pr&#233;liminaire &#224; un certain nombre
de traitements linguistiques plus pouss&#233;s tels que l&#8217;analyse syntaxique totale ou partielle. Les
processus d&#8217;&#233;tiquetage morpho-syntaxique reposent g&#233;n&#233;ralement sur l&#8217;hypoth&#232;se que la cat&#233;-
gorie d&#8217;un mot d&#233;pend d&#8217;un contexte local, qui est r&#233;duit &#224; la cat&#233;gorie du mot ou des deux mots
pr&#233;c&#233;dents, dans le cas d&#8217;&#233;tiqueteurs probabilistes fond&#233;s sur les mod&#232;les de Markov cach&#233;s
(MMC). Cette hypoth&#232;se est g&#233;n&#233;ralement correcte et a permis la r&#233;alisation d&#8217;&#233;tiqueteurs ef-
ficaces et pr&#233;cis (de l&#8217;ordre de &#7; &#8; &#4; de mots correctement &#233;tiquet&#233;s) dont les param&#232;tres sont
estim&#233;s &#224; partir d&#8217;un corpus annot&#233;. Il demeure que cette hypoth&#232;se n&#8217;est pas toujours v&#233;rifi&#233;e
</p>
<p>&#10;Ce travail a &#233;t&#233; partiellement financ&#233; par le projet WATSON dans le cadre de l&#8217;action Technolangue.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Nasr, Volanschi
</p>
<p>et est &#224; l&#8217;origine d&#8217;une partie des erreurs d&#8217;&#233;tiquetage. Ces derni&#232;res m&#232;nent g&#233;n&#233;ralement &#224;
des erreurs dans les traitements suivants, voire &#224; leur &#233;chec, en particulier pour l&#8217;analyse syn-
taxique. Cette situation est particuli&#232;rement frustrante dans la mesure o&#249; les traitements syntax-
iques poss&#232;dent souvent les connaissances qui auraient pu &#233;viter les erreurs d&#8217;&#233;tiquetage. Le but
de cet article est de pallier partiellement ce probl&#232;me en couplant les deux &#233;tapes d&#8217;&#233;tiquetage
et d&#8217;analyse partielle. Dans un tel couplage, le choix de la cat&#233;gorie d&#8217;un mot est effectu&#233;
en tenant compte des connaissances propres &#224; l&#8217;&#233;tiqueteur, mais aussi de celles provenant de
l&#8217;analyseur partiel.
</p>
<p>Le type d&#8217;erreur que l&#8217;on vise &#224; corriger peut &#234;tre illustr&#233; par la phrase suivante : La recapi-
talisation n&#8217;est pas indispensable. Lors de l&#8217;&#233;tiquetage morpho-syntaxique de cette phrase, le
choix de la cat&#233;gorie correcte pour l&#8217;adjectif indispensable (adjectif qualificatif f&#233;minin sin-
gulier) est d&#233;licat du fait que ce dernier peut &#234;tre f&#233;minin ou masculin et que le nom avec lequel
il s&#8217;accorde (recapitalisation) est relativement &#233;loign&#233; de l&#8217;adjectif, du moins pour un &#233;tiqueteur
probabiliste fond&#233; sur un MMC. Dans un tel cas, un analyseur partiel regroupera respectivement
les suites la recapitalisation, n&#8217;est pas et indispensable au sein d&#8217;unit&#233;s appel&#233;es chunks. Le
r&#233;sultat de ce regroupement est le rapprochement des deux unit&#233;s (la recapitalisation et indis-
pensable) entre lesquelles s&#8217;effectue l&#8217;accord et la possibilit&#233; de le mod&#233;liser dans un MMC.
Le mod&#232;le de couplage propos&#233; ici se pose en alternative &#224; un mod&#232;le s&#233;quentiel o&#249; l&#8217;analyseur
partiel prend en entr&#233;e la meilleure solution de l&#8217;&#233;tiqueteur. Il n&#8217;est alors plus possible de revenir
sur les choix effectu&#233;s par ce dernier.
</p>
<p>Cet article vise un autre objectif qui est de montrer l&#8217;avantage de r&#233;aliser ces traitements &#224; l&#8217;aide
d&#8217;automates finis pond&#233;r&#233;s et d&#8217;op&#233;rations sur ces derniers. Dans ce cadre, toutes les donn&#233;es
(phrase &#224; analyser, lexique, grammaire, n-grams) sont repr&#233;sent&#233;es sous la forme d&#8217;automates
et (quasiment) tous les traitements sont r&#233;alis&#233;s par des op&#233;rations standard de manipulation
d&#8217;automates. Cette homog&#233;n&#233;it&#233; poss&#232;de plusieurs avantages dont le premier est la facilit&#233;
de combiner diff&#233;rents modules entre eux gr&#226;ce aux op&#233;rations de combinaison d&#8217;automates,
combinaisons plus difficiles &#224; r&#233;aliser lorsque les diff&#233;rents modules reposent sur des mod&#232;les
formels diff&#233;rents. Un autre avantage de l&#8217;homog&#233;n&#233;it&#233; de ce cadre est la facilit&#233; de mise
en &#339;uvre : plus de formats sp&#233;cifiques &#224; concevoir pour diff&#233;rents types de donn&#233;es, plus
d&#8217;algorithmes &#224; adapter, &#224; programmer et &#224; optimiser. La r&#233;alisation de tels traitements d&#233;pend
de mani&#232;re cruciale de l&#8217;existence de biblioth&#232;ques logicielles de manipulation d&#8217;automates.
Dans le cadre de ce travail, nous avons utilis&#233; les outils FSM et GRM de ATT (8). Notre tra-
vail se situe dans la mouvance du traitement probabiliste de la langue &#224; l&#8217;aide d&#8217;automates
pond&#233;r&#233;s, dont on trouvera un apper&#231;u dans (12). Il se distingue dans son esprit d&#8217;autres ap-
proches fond&#233;es sur les automates finis non probabilistes, telles qu&#8217;INTEX (7), dans lesquelles
des r&#232;gles sont construites manuellement pour &#234;tre ensuite utilis&#233;es dans le cadre de traitements
automatiques.
</p>
<p>L&#8217;organisation de l&#8217;article est la suivante : dans la partie 2, on reprend quelques d&#233;finitions con-
cernant les automates pond&#233;r&#233;s et on introduit quelques notations. Les sections 3 et 4 d&#233;crivent
respectivement les principes d&#8217;un &#233;tiqueteur probabiliste et d&#8217;un analyseur partiel et leur im-
pl&#233;mentation sous la forme d&#8217;automates pond&#233;r&#233;s. Dans la section 5, l&#8217;integration des deux
modules est d&#233;crite. Enfin, des experiences sont pr&#233;sent&#233;es dans la partie 6 et des travaux futurs
sont annonc&#233;s dans la partie 7. La revue de la litt&#233;rature n&#8217;a pas &#233;t&#233; regroup&#233;e dans une section,
nous avons pr&#233;f&#233;r&#233; &#233;tablir des comparaisons avec d&#8217;autres travaux dans le cours de l&#8217;article.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Couplage d&#8217;un &#233;tiqueteur morpho-syntaxique et d&#8217;un analyseur partiel
</p>
<p>2 D&#233;finitions et notations
</p>
<p>Dans la suite de cet article, nous manipulerons deux types d&#8217;automates finis, des reconnaisseurs,
qui permettent de reconna&#238;tre des mots &#11; construits sur un alphabet &#12; ( &#11; &#13; &#12; &#16; ) et des trans-
ducteurs, qui permettent de reconna&#238;tre des couples de mots &#18; &#11; &#20; &#22; &#24; construits sur deux alphabets
</p>
<p>&#12; &#26; et &#12; fi ( &#18; &#11; &#20; &#22; &#24; &#13; &#12; &#16;
&#26;  
</p>
<p>&#12; &#16;
</p>
<p>fi
</p>
<p>). En plus des op&#233;rations r&#233;guli&#232;res standard (union, concat&#233;nation
et it&#233;ration) d&#233;finies sur les deux types de machines, certaines op&#233;rations sont sp&#233;cifiques aux
transducteurs, en particulier l&#8217;op&#233;ration de composition, qui joue un r&#244;le fondamental dans le
reste de cet article. Etant donn&#233;s deux transducteurs ! et &quot; reconnaissant respectivement les
couples de mots &#18; &#11; &#20; &#22; &#24; et &#18; &#22; &#20; $ &#24; , la composition de ! et &quot; (not&#233;e ! &amp; &quot; ) est un transducteur qui
reconna&#238;t le couple &#18; &#11; &#20; $ &#24; .
</p>
<p>On d&#233;finit de plus la notion de semi-anneau qui est un quintuplet &#18; * &#20; , &#20; / &#20; 23 &#20; 2 &#1; &#24; tel que * est
un ensemble de scalaires muni de deux op&#233;rations g&#233;n&#233;ralement apell&#233;es addition (not&#233;e , )
et multiplication (not&#233;e / ) ayant chacune un &#233;l&#233;ment neutre not&#233; respectivement 23 et 2 &#1; . En
associant &#224; chaque transition d&#8217;un reconnaisseur un poids prenant sa valeur dans un ensemble
</p>
<p>* , on obtient un reconnaisseur pond&#233;r&#233; construit sur un semi-anneau sur l&#8217;ensemble * . Un
reconnaisseur pond&#233;r&#233;, en conjonction avec un semi-anneau * g&#233;n&#232;re une fonction partielle
qui associe aux mots du langage reconnu par le reconnaisseur des valeurs de * . Etant donn&#233;
un reconnaisseur 5 et un mot &#11; , la valeur associ&#233;e &#224; &#11; par 5 , not&#233;e 6 6 5 8 8 &#18; &#11; &#24; , est le produit
( / ) des poids des transitions du chemin de 5 correspondant &#224; &#11; . Si plusieurs chemins de 5
permettent de reconna&#238;tre &#11; , alors 6 6 5 8 8 &#18; &#11; &#24; est &#233;gale &#224; la somme ( , ) des poids des diff&#233;rents
chemins correspondant &#224; &#11; . Etant donn&#233; un reconnaisseur pond&#233;r&#233; 5 , on d&#233;finit l&#8217;op&#233;rateur
n-meilleurs chemins, not&#233; ? @ &#18; 5 &#20; D &#24; qui retourne le reconnaisseur constitu&#233; de l&#8217;union des D
chemins les plus probables dans 5 . Toutes ces notions sont &#233;tendues aux transducteurs.
</p>
<p>Dans les exp&#233;riences d&#233;crites dans ce papier on a associ&#233; aux transitions des transducteurs
l&#8217;oppos&#233; de logarithmes de probabilit&#233;s1 ; on a utilis&#233; le semi-anneau tropical sur F G . Dans ce
dernier, l&#8217;op&#233;ration / correspond &#224; l&#8217;addition usuelle (pour conna&#238;tre le poids d&#8217;un chemin on
additionne les poids des transitions) alors que l&#8217;op&#233;ration , est le minimum (le poids associ&#233; par
un transducteur a un mot reconnu est le minimum des poids de tous les chemins du transducteur
reconnaissant le mot, c&#8217;est-&#224;-dire le chemin ayant la meilleure probabilit&#233;).
</p>
<p>3 Etiquetage morpho-syntaxique
</p>
<p>Le processus d&#8217;&#233;tiquetage morpho-syntaxique utilis&#233; dans le cadre de ce travail reprend les
principes de l&#8217;&#233;tiquetage morpho-syntaxique fond&#233; sur les cha&#238;nes de Markov cach&#233;es, introduit
dans (5). Les &#233;tats du MMC correspondent aux cat&#233;gories morpho-syntaxiques et les observ-
ables aux mots du lexique. Ces derniers constituent l&#8217;alphabet &#12; H et les &#233;tiquettes des cat&#233;gories
morpho-syntaxiques constituent l&#8217;alphabet &#12; J . Le processus d&#8217;&#233;tiquetage, dans un tel mod&#232;le,
consiste &#224; retrouver la suite d&#8217;&#233;tats la plus probable &#233;tant donn&#233; une suite d&#8217;observables.
</p>
<p>Les param&#232;tres d&#8217;un MMC se divisent en probabilit&#233;s d&#8217;&#233;mission et en probabilit&#233;s de transi-
tion. Une probabilit&#233; d&#8217;&#233;mission est la probabilit&#233; d&#8217;un mot &#233;tant donn&#233; une cat&#233;gorie ( K &#18; ? M @ &#24; )
</p>
<p>1On pr&#233;f&#232;re les logarithmes de probabilit&#233;s aux probabilit&#233;s pour des questions de stabilit&#233; num&#233;rique (les
probabilit&#233;s pouvant &#234;tre des r&#233;els tr&#232;s petits, on risque de ne pas pouvoir les repr&#233;senter en machine). L&#8217;utilisation
de l&#8217;oppos&#233; du logarithme permet d&#8217;obtenir les chemins de probabilit&#233; maximale lors de l&#8217;utilisation de l&#8217;op&#233;rateur
n-meilleurs chemins.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Nasr, Volanschi
</p>
<p>tandis qu&#8217;une probabilit&#233; de transition est la probabilit&#233; qu&#8217;une cat&#233;gorie O suive directement
une cat&#233;gorie P ( K &#18; O M P &#24; ). Ces deux ensembles de param&#232;tres permettent de calculer la prob-
abilit&#233; jointe d&#8217;une suite de cat&#233;gories @ &#26; R S (une suite d&#8217;&#233;tats du mod&#232;le) et d&#8217;une suite de
mots ? &#26; R S (une suite d&#8217;observables) en utilisant les probabilit&#233;s d&#8217;&#233;mission et de transition :
</p>
<p>K &#18; @ &#26; R S &#20; ? &#26; R S &#24; W K &#18; @ &#26; &#24; K &#18; ? &#26; M @ &#26; &#24; ]
</p>
<p>S_ `
</p>
<p>fi
</p>
<p>K &#18; ?
</p>
<p>_
</p>
<p>M @
</p>
<p>_
</p>
<p>&#24; K &#18; @
</p>
<p>_
</p>
<p>M @
</p>
<p>_ b
</p>
<p>&#26; &#24;
</p>
<p>Un tel mod&#232;le, appel&#233; mod&#232;le bigramme, repose sur l&#8217;hypoth&#232;se markovienne qu&#8217;une cat&#233;gorie
ne d&#233;pend que de la cat&#233;gorie pr&#233;c&#233;dente. Cette hypoth&#232;se, fort contraignante, peut &#234;tre assou-
plie sans changer de cadre th&#233;orique en faisant d&#233;pendre une cat&#233;gorie non plus de la cat&#233;gorie
pr&#233;c&#233;dente mais des deux cat&#233;gories pr&#233;c&#233;dentes pour aboutir &#224; un mod&#232;le trigramme qui est
le mod&#232;le g&#233;n&#233;ralement utilis&#233; pour une telle t&#226;che. Dans un mod&#232;le trigramme, un &#233;tat corre-
spond non plus &#224; une cat&#233;gorie, mais &#224; un couple de cat&#233;gories.
</p>
<p>d e f g h i k l n p d q f s
</p>
<p>t
</p>
<p>e u g h i k l n p
</p>
<p>t
</p>
<p>q u s
</p>
<p>t
</p>
<p>e f g h i k l n p
</p>
<p>t
</p>
<p>q f s
</p>
<p>d e u g h i k l n p d q u s
</p>
<p>x y
</p>
<p>f g h i k l n p f q u s
</p>
<p>u g h i k l n p u q u s
</p>
<p>u g h i k l n p u q f s
</p>
<p>f g h i k l n p f q f s
</p>
<p>u g h i k l n p u s f g h i k l n p f s
</p>
<p>Figure1 : Les transducteurs E et T
</p>
<p>Un tel MMC peut &#234;tre repr&#233;sent&#233; par deux
transducteurs pond&#233;r&#233;s. Le premier, que nous
appellerons | , et dont un exemple appara&#238;t dans
la partie gauche de la figure 1 (dans cet exem-
ple &#12; H W ~ &#128; &#20; &#129; &#130; et &#12; J W ~ ! &#20; &quot; &#130; ) permet de
repr&#233;senter les probabilit&#233;s d&#8217;&#233;mission. Son al-
phabet d&#8217;entr&#233;e est &#12; H et son alphabet de sortie
</p>
<p>&#12; J . Ce transducteur est dot&#233; d&#8217;un seul &#233;tat, et
poss&#232;de autant de transitions (de l&#8217;unique &#233;tat
vers lui m&#234;me) qu&#8217;il y a de couples &#18; ? &#20; @ &#24; o&#249; ?
</p>
<p>est un mot du lexique ( ? &#13; &#12; H ) et @ une cat&#233;gorie ( @ &#13; &#12; J ) tels que la probabilit&#233; d&#8217;&#233;mission
K &#18; ? M @ &#24; soit non nulle. L&#8217;oppos&#233; du logarithme de cette probabilit&#233; ( &#139; &#140; &#142; &#144; K &#18; ? M @ &#24; ) constitue
le poids de la transition &#233;tiquet&#233;e &#18; ? &#20; @ &#24; . Dans la figure 1, une telle transition est &#233;tiquet&#233;e
</p>
<p>? &#146; @ &#147; &#139; &#140; &#142; &#144; K &#18; ? M @ &#24; . Le second transducteur, ayant pour alphabet d&#8217;entr&#233;e et de sortie &#12; J
(partie droite de la figure 1), appel&#233; &#149; , permet de repr&#233;senter les probabilit&#233;s de transition. Il
reprend la structure du MMC : autant d&#8217;&#233;tats que de cat&#233;gories et des transitions entre tout cou-
ple d&#8217;&#233;tats &#18; O &#20; P &#24; (orient&#233; de O vers P ) tel que K &#18; P M O &#24; est non nulle. Le poids de la transition est
&#233;gal &#224; &#139; &#140; &#142; &#144; K &#18; P M O &#24; 2. Dans le cas d&#8217;un mod&#232;le trigramme, la structure de l&#8217;automate &#149; est plus
complexe : un &#233;tat correspond &#224; une s&#233;quence de deux cat&#233;gories et les poids des transitions
sont de la forme &#139; &#140; &#142; &#144; K &#18; &#152; M O P &#24; .
</p>
<p>La composition de | et de &#149; ( | &amp; &#149; ) permet de combiner probabilit&#233;s d&#8217;&#233;mission et de tran-
sition pour aboutir &#224; un transducteur dont l&#8217;alphabet d&#8217;entr&#233;e est &#12; H et l&#8217;alphabet de sortie est
</p>
<p>&#12; J . Un tel transducteur permet d&#8217;associer au couple &#18; ? &#26; R S &#20; @ &#26; R S &#24; le poids 6 6 | &amp; &#149; 8 8 &#18; @ &#26; R S &#20; ? &#26; R S &#24; W
&#139; &#158;
</p>
<p>S_ `
</p>
<p>&#26;
</p>
<p>&#140; &#142; &#144; K &#18; ?
</p>
<p>_
</p>
<p>M @
</p>
<p>_
</p>
<p>&#24; &#139; &#140; &#142; &#144; K &#18; @ &#26; &#24; &#139; &#158;
</p>
<p>S_ `
</p>
<p>fi
</p>
<p>&#140; &#142; &#144; K &#18; @
</p>
<p>_
</p>
<p>M @
</p>
<p>_ b
</p>
<p>&#26; &#24; qui n&#8217;est autre que l&#8217;oppos&#233; du loga-
rithme de la probabilit&#233; K &#18; @ &#26; R S &#20; ? &#26; R S &#24; , telle que d&#233;finie ci-dessus.
</p>
<p>L&#8217;&#233;tiquetage d&#8217;une suite de mots particuli&#232;re &#159; est r&#233;alis&#233; en repr&#233;sentant la suite &#159; sous la
forme d&#8217;un reconnaisseur de structure lin&#233;aire (une transition pour chaque mot de &#159; ), appel&#233;
lui-m&#234;me &#159; puis en effectuant la composition de &#159; avec | &amp; &#149; . La recherche de la suite de
cat&#233;gories la plus probable &#233;tant donn&#233; &#159; est alors r&#233;alis&#233;e par la recherche du meilleur chemin
dans le transducteur &#159; &amp; | &amp; &#149; . L&#8217;&#233;tiqueteur s&#8217;&#233;crit donc : ? @ &#18; &#159; &amp; | &amp; &#149; &#20; &#1; &#24;
</p>
<p>Les probabilit&#233;s des trigrammes repr&#233;sent&#233;es dans l&#8217;automate &#149; ne sont g&#233;n&#233;ralement pas
</p>
<p>2Strictement parlant, l&#8217;automate d&#233;crit est un reconnaisseur, mais il peut &#234;tre vu comme un transducteur dont
l&#8217;alphabet de sortie est &#233;gal &#224; l&#8217;alphabet d&#8217;entr&#233;e et dont chaque transition poss&#232;de le m&#234;me symbole en entr&#233;e
et en sortie. Un tel transducteur repr&#233;sente par cons&#233;quent la relation identit&#233; r&#233;duite au langage reconnu par le
reconnaisseur.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Couplage d&#8217;un &#233;tiqueteur morpho-syntaxique et d&#8217;un analyseur partiel
</p>
<p>estim&#233;es par simple maximum de vraisemblance sur un corpus d&#8217;apprentissage, car des tri-
grammes apparaissant dans les textes &#224; &#233;tiqueter peuvent n&#8217;avoir jamais &#233;t&#233; observ&#233;s dans le
corpus d&#8217;apprentissage. C&#8217;est la raison pour laquelle on a recours &#224; des m&#233;thodes de lissage des
probabilit&#233;s, telles que les m&#233;thodes de repli (10) qui consistent &#224; se replier sur la probabilit&#233;
du bigramme b c lorsque le trigramme a b c n&#8217;a pas &#233;t&#233; observ&#233; dans le corpus et, lorsque
le bigramme b c n&#8217;a pas &#233;t&#233; observ&#233;, &#224; se replier sur l&#8217;unigramme c. Un mod&#232;le de repli peut
&#234;tre directement repr&#233;sent&#233; sous la forme d&#8217;un automate comportant des transitions par d&#233;faut
comme d&#233;crit dans (4). Etant donn&#233; un symbole &#161; , une transition par d&#233;faut &#233;manant d&#8217;un &#233;tat
</p>
<p>&#162; est emprunt&#233;e lorsqu&#8217;il n&#8217;existe pas de transition &#233;manant de &#162; &#233;tiquet&#233;e par &#161; . Dans le cas du
mod&#232;le de repli, une transition par d&#233;faut est emprunt&#233;e lorsqu&#8217;un trigramme ou un bigramme
n&#8217;a jamais &#233;t&#233; observ&#233;. Il ne nous est pas possible ici de d&#233;crire plus en d&#233;tail la structure de
tels automates. Pour plus de d&#233;tails, le lecteur est invit&#233; &#224; se r&#233;f&#233;rer &#224; l&#8217;article cit&#233; ci-desssus.
</p>
<p>Plusieurs approches dans la litt&#233;rature (14; 11; 9) utilisent les automates finis pond&#233;r&#233;s afin de
simuler le fonctionnement d&#8217;un MMC. Dans les trois cas, les n-grammes sont repr&#233;sent&#233;s sous
la forme d&#8217;automates, de mani&#232;re proche de la notre. Cependant, ces travaux se distinguent du
notre en ne mod&#233;lisant pas directement les probablilit&#233;s d&#8217;&#233;mission ( K &#18; ? M @ &#24; ) estim&#233;es sur un
corpus d&#8217;apprentissage, mais en recourant &#224; des classes d&#8217;ambigu&#239;t&#233;s, qui sont des ensembles
de cat&#233;gories associ&#233;es &#224; un mot.
</p>
<p>4 Analyse syntaxique partielle
</p>
<p>L&#8217;analyse syntaxique partielle d&#233;signe un ensemble de techniques dont le but est de mettre au
jour une partie de la structure syntaxique d&#8217;une phrase, plus pr&#233;cis&#233;ment, la structure asso-
ci&#233;e aux fragments qui n&#8217;ont qu&#8217;une analyse possible. Par exemple, m&#234;me si une suite comme
&#8216;maison des sciences de l&#8217;homme&#8217; constitue dans une grammaire traditionnelle un groupe nom-
inal ayant une structure complexe avec plusieurs niveaux interm&#233;diaires, dans une analyse par-
tielle elle sera segment&#233;e en trois unit&#233;s appel&#233;es chunks : [maison] J &#163; [des sciences] J &#164; [de
l&#8217;homme] J &#164; car le rattachement des syntagmes pr&#233;postionnels est potentiellement ambigu. Ap-
pel&#233;e aussi chunking, l&#8217;analyse partielle a &#233;t&#233; introduite par (3) comme r&#233;ponse aux difficult&#233;s
d&#8217;analyse soulev&#233;es par le traitement robuste des textes tout-venants.
</p>
<p>Plusieurs approches dont (2) ont abord&#233; l&#8217;analyse partielle &#224; l&#8217;aide des automates finis, plus
pr&#233;cis&#233;ment &#224; l&#8217;aide des cascades de transducteurs finis. Une cascade de transducteurs est
une succession de transducteurs o&#249; chacun permet de reconna&#238;tre un type de chunk. L&#8217;entr&#233;e
de chaque transducteur est constitu&#233;e par la sortie du transducteur pr&#233;c&#233;dent. Notre solution
consiste en l&#8217;application simultan&#233;e, plut&#244;t que s&#233;quentielle, de tous les automates des chunks
qui sont int&#233;gr&#233;s au sein d&#8217;un MMC.
</p>
<p>Les chunks, du fait de leur caract&#232;re non r&#233;cursif, peuvent &#234;tre repr&#233;sent&#233;s sous la forme
d&#8217;automates finis construits sur l&#8217;alphabet &#12; J . A chaque type de chunk &#165; (par exemple chunk
nominal, pr&#233;positionnel, . . . ) correspond un automate appel&#233; aussi &#165; , qui reconna&#238;t toute
s&#233;quence de cat&#233;gories qui constitue un chunk bien form&#233; de type &#165; . De plus, au chunk de
type &#165; sont associ&#233;s deux symboles, un symbole de d&#233;but de chunk, not&#233; &lt;K&gt;, et un symbole
de fin de chunk, not&#233; &lt;/K&gt;. L&#8217;ensemble des symboles de d&#233;but et de fin de chunk constituent
un nouvel alphabet appel&#233; &#12; &#166; . Les diff&#233;rents automates associ&#233;s aux chunks sont regroup&#233;s
entre eux au sein d&#8217;un transducteur, appel&#233; ! , qui constitue l&#8217;analyseur et dont la structure est
repr&#233;sent&#233;e dans la figure 2.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Nasr, Volanschi
</p>
<p>c2
</p>
<p>cm
</p>
<p>c1
&#167; &#167;
</p>
<p>&#167;
</p>
<p>&#167;
</p>
<p>&#167;
</p>
<p>&#167;
</p>
<p>&#167; &#168; &#169; &#170; &#171; &#172;
</p>
<p>&#167; &#168; &#169; &#170; &#173; &#172; &#167; &#168; &#169; &#168; &#170; &#173; &#172;
</p>
<p>&#166; &#175;
</p>
<p>&#166; &#177;
</p>
<p>&#166; &#179;
</p>
<p>&#167; &#168; &#169; &#168; &#170; &#171; &#172;
</p>
<p>&#167;
</p>
<p>Figure 2 : Structure de l&#8217;analyseur partiel !
</p>
<p>I F
</p>
<p>L&#8217;alphabet d&#8217;entr&#233;e de ! est &#12; J et son al-
phabet de sortie est &#12; J &#180; &#12; &#166; . Il accepte en
entr&#233;e des s&#233;quences de cat&#233;gories et produit
des s&#233;quences m&#233;lant cat&#233;gories et symboles
de d&#233;but et de fin de chunk. Etant donn&#233; une
s&#233;quence de cat&#233;gories &#181; en entr&#233;e, ! produira
en sortie la m&#234;me s&#233;quence dans laquelle toute
occurrence d&#8217;un chunk de type &#165; sera encadr&#233;e
des deux symboles &lt;K&gt; et &lt;/K&gt;. ! est com-
pos&#233; de deux parties, une partie sup&#233;rieure qui
est elle m&#234;me compos&#233;e des diff&#233;rents auto-
mates de chunks, not&#233;s &#165;
</p>
<p>_
</p>
<p>mis en parall&#232;le.
Les transitions reliant l&#8217;&#233;tat initial de ! aux &#233;tats initaux des diff&#233;rents automates &#165;
</p>
<p>_
</p>
<p>permettent
d&#8217;introduire les symboles de d&#233;but de chunk et les transititions reliant les &#233;tats d&#8217;acceptation
des automates &#165;
</p>
<p>_
</p>
<p>&#224; l&#8217;&#233;tat F introduisent des symboles de fin de chunk. La partie inf&#233;rieure
de ! est compos&#233;e d&#8217;autant de transitions qu&#8217;il y a de cat&#233;gories morpho-syntaxiques. Enfin
une transition &#182; reliant F &#224; I permet de r&#233;aliser une boucle et de reconna&#238;tre ainsi plusieurs
occurrences de chunks dans une s&#233;quence de cat&#233;gories.
</p>
<p>L&#8217;automate ! reconna&#238;t n&#8217;importe quel mot &#181; construit sur &#12; J . L&#8217;analyse de &#181; est r&#233;alis&#233;e
en repr&#233;sentant &#181; sous la forme d&#8217;un automate lin&#233;aire (une transition pour chaque cat&#233;gorie
constituant &#181; ) appel&#233; lui aussi &#181; et en effectuant la composition &#181; &amp; ! . On pourra remarquer
que le produit de cette composition est ambigu, car pour chaque sous-mot &#184; de &#181; correspondant
&#224; un chunk &#165;
</p>
<p>_
</p>
<p>, deux r&#233;sultats seront produits : la reconnaissance de &#184; en tant que chunk (passage
&#224; travers l&#8217;automate &#165;
</p>
<p>_ ) et la reconnaissance de &#184; comme une suite de cat&#233;gories ne constituant
pas un chunk (passage dans les transitions de la partie inf&#233;rieure de ! ). Parmi ces diff&#233;rents
r&#233;sultats, un seul nous int&#233;resse, celui dans lequel toute occurrence de chunk a &#233;t&#233; marqu&#233;e par
l&#8217;introduction de balises de d&#233;but et de fin de chunk. Il est facile de limiter le produit de la
composition &#224; ce seul r&#233;sultat en associant &#224; chaque transition intra chunk un poids de 3 et aux
transitions extra chunk un poids de &#1; et en ne gardant des r&#233;sultats produits que le chemin de
poids minimal. Le processus d&#8217;analyse peut &#234;tre repr&#233;sent&#233; par l&#8217;expression : ? @ &#18; &#181; &amp; ! &#20; &#1; &#24;
</p>
<p>5 Couplage de l&#8217;&#233;tiquetage et de l&#8217;analyse partielle
</p>
<p>Les mod&#232;les d&#8217;&#233;tiquetage morpho-syntaxique &#224; l&#8217;aide des transducteurs pond&#233;r&#233;s cit&#233;s dans la
section 3, int&#233;grent aussi (ou pr&#233;voient la possibilit&#233; d&#8217;int&#233;grer) des contraintes syntaxiques
dans le processus d&#8217;&#233;tiquetage. Kempe (11) pr&#233;voit la possibilit&#233; de composer la sortie du tag-
ger avec des transducteurs encodant des r&#232;gles de correction des erreurs les plus fr&#233;quentes,
Tzoukerman (14) utilise des contraintes n&#233;gatives afin de diminuer de fa&#231;on drastique la proba-
bilit&#233; des chemins comportant des suites improbables d&#8217;&#233;tiquettes (par exemple un d&#233;terminant
suivi d&#8217;un verbe). D&#8217;un point de vue g&#233;n&#233;ral, notre travail se distingue des autres par le fait
qu&#8217;il int&#232;gre deux modules complets (un module d&#8217;&#233;tiquetage et un module d&#8217;analyse partielle)
au sein d&#8217;un seul, r&#233;alisant l&#8217;&#233;tiquetage et l&#8217;analyse partielle. Il ne s&#8217;agit pas d&#8217;int&#233;grer dans
un &#233;tiqueteur des grammaires locales con&#231;ues pour &#233;liminer certaines structures agrammati-
cales, mais d&#8217;int&#233;grer v&#233;ritablement l&#8217;information statistique avec les connaissances linguis-
tiques mod&#233;lis&#233;es par l&#8217;analyseur partiel dans le but d&#8217;am&#233;liorer la qualit&#233; de l&#8217;&#233;tiquetage.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Couplage d&#8217;un &#233;tiqueteur morpho-syntaxique et d&#8217;un analyseur partiel
</p>
<p>Le couplage de l&#8217;&#233;tiquetage morpho-syntaxique et du d&#233;coupage en chunks peut &#234;tre r&#233;alis&#233; par
simple composition des deux mod&#232;les que nous avons d&#233;crit : ? @ &#18; ? @ &#18; &#159; &amp; | &amp; &#149; &#20; &#1; &#24; &amp; ! &#20; &#1; &#24; .
Ce mod&#232;le est une instance de l&#8217;architecture s&#233;quentielle que nous avons introduite et critiqu&#233;e
dans la section 1 : la s&#233;lection d&#8217;une &#233;tiquette morpho-syntaxique est r&#233;alis&#233;e ind&#233;pendamment
de la t&#226;che d&#8217;analyse syntaxique (ici r&#233;alis&#233;e par un simple d&#233;coupage en chunks) et ne peut
&#234;tre remise en cause par cette derni&#232;re.
</p>
<p>Il est possible de fournir &#224; l&#8217;analyseur non plus le meilleur &#233;tiquetage possible mais l&#8217;ensemble
de toutes les solutions de l&#8217;&#233;tiqueteur repr&#233;sent&#233;es sous la forme d&#8217;un automate : ? @ &#18; &#159; &amp; | &amp;
</p>
<p>&#149; &amp; ! &#20;
</p>
<p>&#1;
</p>
<p>&#24; . Ceci montre la souplesse du traitement par automates finis. Mais un tel mod&#232;le
n&#8217;offre pas beaucoup d&#8217;int&#233;r&#234;t dans la mesure o&#249; l&#8217;analyseur n&#8217;a quasiment aucun pouvoir dis-
criminant permettant de favoriser certaines des sorties de l&#8217;&#233;tiqueteur. En effet, contrairement
&#224; un analyseur fond&#233; sur une grammaire hors-contexte, par exemple, qui n&#8217;associe une struc-
ture qu&#8217;aux phrases appartenant au langage reconnu par la grammaire, notre analyseur accepte
toutes les suites de cat&#233;gories, son r&#244;le se borne &#224; reconna&#238;tre certaines sous-suites de cette
derni&#232;re comme formant des chunks. C&#8217;est la raison pour laquelle nous allons introduire une
version probabiliste de l&#8217;analyseur partiel. Ce dernier effectue un d&#233;coupage en chunks d&#8217;une
suite de cat&#233;gories et lui associe de plus une probabilit&#233; d&#8217;apr&#232;s un mod&#232;le dont les param&#232;tres
ont &#233;t&#233; estim&#233;s sur un corpus. Un tel mod&#232;le n&#8217;a pas pour objectif de favoriser un d&#233;coupage
en chunks d&#8217;une m&#234;me suite de cat&#233;gories plut&#244;t qu&#8217;un autre (l&#8217;analyse en chunks est unique !).
Son objectif est de fournir un moyen de comparer entre elles diff&#233;rentes s&#233;quences de cat&#233;-
gories possibles pour une m&#234;me phrase. Pour cela, l&#8217;analyseur partiel associe &#224; toute s&#233;quence
de cat&#233;gories une probabilit&#233; qui est d&#8217;autant plus &#233;lev&#233;e que la s&#233;quence de cat&#233;gories corre-
spond &#224; des s&#233;quences de chunks bien form&#233;s, agenc&#233;s dans un ordre lin&#233;aire observ&#233; sur un
corpus d&#8217;apprentissage. Cette approche partage plusieurs points communs avec les travaux de
(6) qui utilisent eux aussi des transducteurs pond&#233;r&#233;s pour r&#233;aliser un analyseur partiel proba-
biliste. Cependant, dans leur cas, plusieurs d&#233;coupages de la phrase en chunks sont possibles
et l&#8217;objectif de l&#8217;analyseur est de fournir le d&#233;coupage le plus probable. De plus, leur analyseur
prend en entr&#233;e une s&#233;quence unique de cat&#233;gories.
</p>
<p>La probabilit&#233; d&#8217;une suite de cat&#233;gories d&#233;coup&#233;e en chunks est calcul&#233;e &#224; partir de deux types
de probabilit&#233;s : des probabilit&#233;s intra chunk et des probabilit&#233;s inter chunks. Une probabilit&#233;
intra chunk est la probabilit&#233; qu&#8217;une suite de cat&#233;gories @ &#26; R &#186; constitue un chunk d&#8217;un type &#165;
</p>
<p>_
</p>
<p>.
</p>
<p>Cette probabilit&#233; est not&#233;e K &#187; &#18; @ &#26; R &#186; M &#165;
_
</p>
<p>&#24; . Les probabilit&#233;s inter chunk sont les probabilit&#233;s condi-
tionnelles d&#8217;occurrence d&#8217;un chunk d&#8217;un type donn&#233;, &#233;tant donn&#233;s les D &#139; &#1; chunks ou cat&#233;gories
pr&#233;c&#233;dents (s&#8217;agissant d&#8217;une analyse partielle, certaines cat&#233;gories de la suite analys&#233;e ne seront
pas int&#233;gr&#233;es dans des chunks). La probabilit&#233; associ&#233;e par l&#8217;analyseur &#224; une suite de cat&#233;gories
est le produit des probabilit&#233;s internes des chunks qui le composent et des probabilit&#233;s externes
de la s&#233;quence des chunks reconnus.
</p>
<p>Etant donn&#233; la suite &lt;s&gt; D N V D N P D A N &lt;/s&gt;3. Le d&#233;coupage propos&#233; par l&#8217;analyseur
est : C = &lt;s&gt; &lt;CN&gt; D N &lt;/CN&gt; V &lt;CN&gt; D N &lt;/CN&gt; &lt;CP&gt; P D A N &lt;/CP&gt; &lt;/s&gt;
</p>
<p>La probabilit&#233; associ&#233;e &#224; cette s&#233;quence est le produit de la suite des chunks reconnus (not&#233;e
K &#188; &#18; &#189; &#24; ), et des probabilit&#233;s internes de chacun des chunks :
</p>
<p>&#191; &#192;
</p>
<p>C &#193; &#194;
&#191;
</p>
<p>&#188;
</p>
<p>&#192;
</p>
<p>&lt;s&gt; &lt;CN&gt; V &lt;CN&gt; &lt;CP&gt; &lt;/s&gt; &#193; &#195; &#191; &#187; &#192; D N|&lt;CN&gt; &#193; fi &#195; &#191; &#187; &#192; P D A N| &lt;CP&gt; &#193;
Les probabilit&#233;s internes sont estim&#233;es par maximum de vraisemblance sur un corpus d&#8217;apprentis-
</p>
<p>3O&#249; D, N, V, P et A sont les &#233;tiquettes correspondant respectivement aux cat&#233;gories d&#233;terminant, nom, verbe,
pr&#233;position et adjectif.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Nasr, Volanschi
</p>
<p>sage, comme nous le verrons en 5.1. La probabilit&#233; d&#8217;une suite d&#8217;&#233;tiquettes de chunks et
d&#8217;&#233;tiquettes morpho-syntaxique est calcul&#233;e &#224; l&#8217;aide d&#8217;un mod&#232;le D -gram, appel&#233; mod&#232;le ex-
terne, appris lui aussi sur un corpus, qui mod&#233;lise la probabilit&#233; d&#8217;un chunk &#233;tant donn&#233; les D &#139; &#1;
</p>
<p>chunks ou cat&#233;gories pr&#233;c&#233;dentes. Dans le cas d&#8217;un mod&#232;le externe bigramme, la probabilit&#233;
externe de &#181; est calcul&#233;e de la mani&#232;re suivante :
</p>
<p>&#191;
</p>
<p>&#188;
</p>
<p>&#192;
</p>
<p>C &#193; &#194;
&#191;
</p>
<p>&#188;
</p>
<p>&#192;
</p>
<p>&lt;CN&gt;|&lt;s&gt; &#193; &#195; &#191; &#188; &#192; V|&lt;CN&gt; &#193; &#195; &#191; &#188; &#192; &lt;CN&gt;|V &#193; &#195; &#191; &#188; &#192; &lt;CP&gt;|&lt;CN&gt; &#193; &#195; &#191; &#188; &#192; &lt;/s&gt;|&lt;CP&gt; &#193;
</p>
<p>5.1 Construction du mod&#232;le et estimation de ses param&#232;tres
</p>
<p>L&#8217;estimation des param&#232;tres du mod&#232;le externe et des mod&#232;les internes s&#8217;effectue en deux
&#233;tapes &#224; partir d&#8217;un corpus &#233;tiquet&#233;. Lors d&#8217;une premi&#232;re &#233;tape, le corpus est analys&#233; par
l&#8217;analyseur partiel A. Le r&#233;sultat de cette analyse est un nouveau corpus dans lequel des sym-
boles de d&#233;but et de fin de chunks ont &#233;t&#233; introduits. Deux objets sont produits &#224; partir de ce
corpus. D&#8217;une part toutes les suites de cat&#233;gories correspondant &#224; chaque type de chunk &#165;
</p>
<p>_
</p>
<p>et
d&#8217;autre part un corpus hybride dans lequel toute occurrence de chunk a &#233;t&#233; remplac&#233;e par un
seul symbole, mat&#233;rialisant le chunk (ce symbole n&#8217;est autre que la marque de d&#233;but de chunk).
Le corpus hybride se pr&#233;sente donc sous la forme d&#8217;une s&#233;quence de cat&#233;gories et de symboles
de chunk, trace du chunk qui a &#233;t&#233; d&#233;tect&#233; &#224; cet endroit. Le premier va servir &#224; estimer les
probabilit&#233;s intra chunks et le second les probabilit&#233;s inter chunks. Les diff&#233;rentes &#233;tapes de ce
traitement sont repr&#233;sent&#233;es dans la figure 3.
</p>
<p>corpus hybride
</p>
<p>mod&#232;le inter chunks
</p>
<p>corpus &#233;tiquet&#233;
</p>
<p>corpus analys&#233;
</p>
<p>analyse partielle
</p>
<p>mod&#232;le final
</p>
<p>op&#233;ration de remplacement
</p>
<p>remplacement des chunksextraction des chunks
</p>
<p>diff&#233;rentes r&#233;alisations de chaque chunk
</p>
<p>apprentissage du n-gramestimation des probabilit&#233;s intra chunks
</p>
<p>mod&#232;les intra chunks
</p>
<p>Figure 3 : Les &#233;tapes de la construction du mod&#232;le
</p>
<p>L&#8217;estimation des probabilit&#233;s inter
chunk &#224; partir du corpus hybride est
identique &#224; l&#8217;estimation des proba-
bilit&#233;s D -gram d&#233;crite en 3 et sur
laquelle nous ne reviendrons pas.
Ces probabilit&#233;s sont repr&#233;sent&#233;es
dans un transducteur (appel&#233; mod-
&#232;le externe) reprenant la structure
de &#149; dans la figure 1 et dont
les transitions sont &#233;tiquet&#233;es par
des cat&#233;gories ou des symboles de
chunks. L&#8217;estimation des probabil-
it&#233;s intra chunk est une simple es-
timation par maximum de vraisem-
blance. Etant donn&#233; un chunk &#181; &#186; et
</p>
<p>D suites diff&#233;rentes d&#8217;&#233;tiquettes
( &#184; &#26; , &#184; fi , . . . &#184; S ) repr&#233;sentant toutes les r&#233;alisations de ce chunk dans un corpus d&#8217;apprentissage,
on note D
</p>
<p>_
</p>
<p>le nombre d&#8217;occurrences de la suite @
_
</p>
<p>. La probabilit&#233; de &#184;
_
</p>
<p>n&#8217;est autre que sa
fr&#233;quence relative : K &#18; &#184;
</p>
<p>_
</p>
<p>&#24; W
</p>
<p>S &#198;
</p>
<p>&#199; &#200; &#201; &#202;
</p>
<p>&#177;
</p>
<p>S
</p>
<p>&#201;
</p>
<p>. Cette probabilit&#233; est la probabilit&#233; du chemin correspon-
dant &#224; &#184;
</p>
<p>_
</p>
<p>dans le reconnaisseur &#165;
_
</p>
<p>.
</p>
<p>Les mod&#232;les intra chunks et le mod&#232;le externe sont combin&#233;s pour former un unique trans-
ducteur &#224; l&#8217;aide de l&#8217;op&#233;ration de remplacement, intoduite dans (13). Cette derni&#232;re permet de
remplacer dans le mod&#232;le externe une transition &lt;Ki&gt; par l&#8217;automate &#165;
</p>
<p>_
</p>
<p>. Le transducteur r&#233;sul-
tant est appel&#233; ! K (pour analyseur probabiliste). Le mod&#232;le conjoint d&#8217;&#233;tiquetage et d&#8217;analyse
est maintenant ? @ &#18; &#159; &amp; | &amp; ! K &#20; &#1; &#24; .</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Couplage d&#8217;un &#233;tiqueteur morpho-syntaxique et d&#8217;un analyseur partiel
</p>
<p>6 Exp&#233;riences
</p>
<p>Les exp&#233;riences ont &#233;t&#233; men&#233;es sur le corpus &#233;tiquet&#233; Paris 7 (1). Le corpus est constitu&#233; de
&#7;
</p>
<p>3 3
</p>
<p>&#165; mots &#233;tiquet&#233;s avec un jeu de &#2; &#2; &#2; &#233;tiquettes indiquant la cat&#233;gorie et les traits mor-
phologiques des mots. On a r&#233;serv&#233; une partie du corpus de &#203; &#204; 3 &#165; mots pour l&#8217;apprentissage
(App). Les tests ont &#233;t&#233; r&#233;alis&#233;s sur un fragment de &#204; &#204; &#165; mots (Test). Le taux d&#8217;erreur du
mod&#232;le trigramme (not&#233; &#205; &#26; ), tel qu&#8217;il est d&#233;crit dans la partie 3 sur Test est de &#2; &#20; &#1; &#207; &#4; 4. Ce
chiffre constitue notre point de r&#233;f&#233;rence. &#2; &#207; grammaires de chunks diff&#233;rents ont &#233;t&#233; construites
manuellement. Ces grammaires appartiennent &#224; une sous-classe des grammaires hors-contexte
qui repr&#233;sentent des langages r&#233;guliers et qui peuvent &#234;tre compil&#233;es sous forme d&#8217;automates
afin d&#8217;effectuer l&#8217;analyse partielle du corpus. Les probabilit&#233;s intra chunks et les probabilit&#233;s
externes ont &#233;t&#233; estim&#233;es sur App. Les exp&#233;riences ont &#233;t&#233; r&#233;alis&#233;es gr&#226;ce aux librairies FSM et
GRM de AT&amp;T
</p>
<p>Les performances du mod&#232;le ? @ &#18; &#159; &amp; | &amp; ! K &#20; &#1; &#24; (not&#233; &#205; fi ) sont quasiment identiques &#224; celle
de &#205; &#26; . Cependant, les deux mod&#232;les n&#8217;effectuent pas les m&#234;mes erreurs. En effet, &#205; fi corrige
</p>
<p>&#210; 3 &#4; des erreurs effectu&#233;es par &#205; &#26; mais effectue quasiment autant d&#8217;erreurs en plus. Ces
nouvelles erreurs ont diff&#233;rentes causes dont certaines proviennent de l&#8217;hypoth&#232;se du mod&#232;le
</p>
<p>&#205; fi que la forme d&#8217;un chunk (la suite de cat&#233;gories qui constituent le chunk) est ind&#233;pendante
du contexte d&#8217;occurrence de ce dernier. Cette hypoth&#232;se n&#8217;est pas toujours valide, comme
l&#8217;illustre la phrase &#8217;la discussion a &#233;t&#233; ouverte par l&#8217;article ...&#8217;. Dans cet exemple, &#8217;ouverte&#8217; a
correctement &#233;t&#233; &#233;tiquet&#233; participe pass&#233; par &#205; &#26; , alors que &#205; fi l&#8217;a &#233;tiquet&#233; adjectif. La raison
de cette erreur provient du fait que &#205; fi a reconnu &#8217;a &#233;t&#233; ouverte&#8217; comme chunk verbal et a choisi
la cat&#233;gorie de &#8217;ouverte&#8217; ind&#233;pendamment du contexte du chunk. &#205; &#26; de son c&#244;t&#233; a tir&#233; parti
du fait que &#8217;ouverte&#8217; &#233;tait suivi d&#8217;une pr&#233;position pour lui assigner la cat&#233;gorie participe pass&#233;.
Afin de pallier partiellement ce probl&#232;me, nous avons combin&#233; les mod&#232;les &#205; &#26; et &#205; fi au sein
du mod&#232;le suivant : ? @ &#18; &#18; &#159; &amp; | &amp; ! K &#24; &#213; &#18; &#159; &amp; | &amp; &#149; &#24; &#20; &#1; &#24; , not&#233; &#205; &#215; . Ce dernier ne conserve
que les solutions communes &#224; &#205; &#26; et &#205; fi auxquelles il associe la somme des poids attibu&#233;es
par &#205; &#26; et &#205; fi ( 6 6 &#205; &#215; 8 8 &#18; O &#24; W 6 6 &#205; &#26; 8 8 &#18; O &#24; &#217; 6 6 &#205; fi 8 8 &#18; O &#24; 5). Cette combinaison permet d&#8217;attenuer
l&#8217;hypoth&#232;se d&#8217;ind&#233;pendance. En effet, la d&#233;pendance entre la forme d&#8217;un chunk et son contexte
d&#8217;occurrence est partiellement mod&#233;lis&#233; par &#205; &#26; . Le taux d&#8217;erreur de &#205; &#215; sur Test est de &#1; &#20; &#7; &#2; &#4;
</p>
<p>soit une diminution de &#1; &#1; &#20; &#7; &#4; par rapport &#224; notre mod&#232;le de r&#233;f&#233;rence : &#205; &#26; . Une analyse
d&#8217;erreurs a montr&#233; que &#205; &#215; corrige &#1; &#8; &#20; &#8; &#4; des erreurs de &#205; &#26; mais effectue &#203; &#20; &#7; &#4; de nouvelles
erreurs. Les raisons des erreurs effectu&#233;es par &#205; &#215; sont diverses, certaines proviennent toujours
de l&#8217;hypoth&#232;se d&#8217;ind&#233;pendance cit&#233;e ci-dessus, d&#8217;autres sont dues &#224; l&#8217;estimation des probabilit&#233;s
intra chunk (la probabilit&#233; qu&#8217;une s&#233;quence de cat&#233;gories donn&#233;e constitue un chunk d&#8217;une
nature donn&#233;e). Ces derni&#232;res sont en effet estim&#233;es par simple maximum de vraisemblance
et attribuent par cons&#233;quent une probabilit&#233; nulle &#224; une r&#233;alisation de chunk qui n&#8217;a jamais
&#233;t&#233; observ&#233;e dans App. Une forme de lissage de ces probabilit&#233;s semble n&#233;cessaire. D&#8217;autres
erreurs proviennent des limites th&#233;oriques du mod&#232;le et n&#233;cessiteraient pour &#234;tre corrig&#233;es une
analyse syntaxique compl&#232;te.
</p>
<p>4Ce r&#233;sultat est sup&#233;rieur au r&#233;sultat de (14) ( &#219; &#220; de taux d&#8217;erreur) sur le m&#234;me corpus et avec le m&#234;me jeu
d&#8217;&#233;tiquettes. Cette diff&#233;rence provient, au moins en partie, du fait que nous avons travaill&#233; sans mots inconnus :
tous les mots de Test apparaissent dans le dictionnaire. Nous avons effectu&#233; cette hypoth&#232;se car l&#8217;objet de notre
travail est d&#8217;&#233;tudier l&#8217;apport de l&#8217;analyseur partiel sur les performances d&#8217;un &#233;tiqueteur fond&#233; sur les MMC et nous
estimons que l&#8217;influence des mots inconnus sera quasiment la m&#234;me sur les diff&#233;rents mod&#232;les que nous avons
test&#233;.
</p>
<p>5Contrairement aux mod&#232;les &#221; &#222; et &#221; &#223; les poids associ&#233;s &#224; une s&#233;quence de mots par &#221; &#224; ne correspondent
pas &#224; des probabilit&#233;s.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Nasr, Volanschi
</p>
<p>7 Conclusion
</p>
<p>Le travail pr&#233;sent&#233; dans cet article a montr&#233; que la prise en compte de connaissances syn-
taxiques, sous la forme d&#8217;une analyse partielle, permet d&#8217;am&#233;liorer le r&#233;sultat d&#8217;un &#233;tique-
teur morpho-syntaxique. Il a aussi montr&#233; que les diff&#233;rentes &#233;tapes pouvaient &#234;tre r&#233;alis&#233;es &#224;
l&#8217;aide d&#8217;automates pond&#233;r&#233;s. De nombreuses am&#233;liorations pourraient &#234;tre apport&#233;es au mod&#232;le
d&#233;crit, telles qu&#8217;une meilleure m&#233;thode d&#8217;estimation des probabilit&#233;s intra chunk ainsi qu&#8217;une
meilleure mod&#233;lisation de l&#8217;influence du contexte sur la r&#233;alisation d&#8217;un chunk.
</p>
<p>R&#233;f&#233;rences
1. Anne Abeill&#233; and Lionel Cl&#233;ment. A tagged reference corpus for french. In Proceedings LINC-EACL,
</p>
<p>Bergen, 1999.
</p>
<p>2. S. Abney. Partial parsing via finite-state cascades. In Workshop on Robust Parsing, 8th European
Summer School in Logic, Language and Information, Prague, Czech Republic, pages 8&#8211;15., 1996.
</p>
<p>3. Steven P. Abney. Parsing by chunks. In Robert C. Berwick, Steven P. Abney, and Carol Tenny, editors,
Principle-Based Parsing: Computation and Psycholinguistics, pages 257&#8211;278. Kluwer, Dordrecht, 1991.
</p>
<p>4. Cyril Allauzen, Mehryar Mohri, and Brian Roark. Generalized algorithms for constructing statisti-
cal language models. In 41st Meeting of the Association for Computational Linguistics, pages 40&#8211;47,
Sapporo, Japon, 2003.
</p>
<p>5. L. R. Bahl and R. L. Mercer. Part of speech assignment by a statistical decision algorithm. In Pro-
ceedings IEEE International Symposium on Information Theory, pages 88&#8211;89, 1976.
</p>
<p>6. Kuang-Hua Chen and Hsin-Hsi Chen. Extracting noun phrases from large-scale texts: A hybrid ap-
proach and its automatic evaluation. In Meeting of the Association for Computational Linguistics, pages
234&#8211;241, 1994.
</p>
<p>7. http://www.nyu.edu/pages/linguistics/intex/.
</p>
<p>8. http://www.research.att.com/sw/tools/{fsm,grm}.
9. Bryan Jurish. A hybrid approach to part-of-speech tagging. Technical report, Berlin-Brandenburgishe
</p>
<p>Akademie der Wissenschaften, 2003.
</p>
<p>10. Slava M. Katz. Estimation of probabilities from sparse data for the language model component of
a speech recogniser. IEEE Transactions on Acoustics, Speech, and Signal Processing, 35(3):400&#8211;401,
1987.
</p>
<p>11. Andr&#233; Kempe. Finite state transducers approximating hidden markov models. In ACL&#8217;97, pages
460&#8211;467, Madrid, Spain, 1997.
</p>
<p>12. Mehryar Mohri. Finite-state transducers in language and speech processing. Computational Linguis-
tics, 23(2), 1997.
</p>
<p>13. Mehryar Mohri. Robustness in Language and Speech Technology, chapter Weighted Grammars Tools:
the GRM Library, pages 19&#8211;40. Jean-Claude Junqua and Gertjan Van Noord (eds) Kluwer Academic
Publishers, 2000.
</p>
<p>14. Evelyne Tzoukermann and Dragomir R. Radev. Use of weighted finite state trasducers in part of
speech tagging. Natural Language Engineering, 1997.</p>

</div></div>
</body></html>