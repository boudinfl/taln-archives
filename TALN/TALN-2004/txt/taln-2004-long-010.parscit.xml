<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Anne Abeillé</author>
<author>Lionel Clement</author>
</authors>
<title>A tagged reference corpus for french.</title>
<date>1999</date>
<booktitle>In Proceedings LINC—EACL,</booktitle>
<location>Bergen,</location>
<contexts>
<context position="28106" citStr="(1)" startWordPosition="4478" endWordPosition="4478">alisations de chaque chunk ||corpus hybride | I Les modeles intra chunks et le modele externe sont combines pour former un unique transducteur a l’aide de l’operation de remplacement, intoduite dans (13). Cette derniere permet de remplacer dans le modele externe une transition &lt; Ki &gt; par l’automate K ,-. Le transducteur resultant est appelé AP (pour analyseur probabiliste). Le modele conjoint d’étiquetage et d’analyse est maintenant mc(M o E o AP, 1). Couplage d’un étiqueteur morpho-syntaxique et d’un cmalyseurpartiel 6 Expériences Les experiences ont ete menees sur le corpus etiquete Paris 7 (1). Le corpus est constitue de 900K mots etiquetes avec un jeu de 222 etiquettes indiquant la categorie et les traits morphologiques des mots. On a reserve une partie du corpus de 760K mots pour l’apprentissage (App). Les tests ont ete realises sur un fragment de 66K mots (Test). Le taux d’erreur du modele trigramme (note M1), tel qu’il est decrit dans la partie 3 sur Test est de 2, 18%4. Ce chiffre constitue notre point de reference. 28 grammaires de chunks differents ont ete construites manuellement. Ces grammaires appartiennent a une sous-classe des grammaires hors-contexte qui representent d</context>
</contexts>
<marker>1.</marker>
<rawString>Anne Abeillé and Lionel Clement. A tagged reference corpus for french. In Proceedings LINC—EACL, Bergen, 1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Abney</author>
</authors>
<title>Partial parsing via ﬁnite-state cascades.</title>
<date>1996</date>
<booktitle>In Workshop on Robust Parsing, 8th European Summer School in Logic, Language and Information,</booktitle>
<pages>8--15</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="16128" citStr="(2)" startWordPosition="2530" endWordPosition="2530">ible. Par exemple, meme si une suite comme ‘maison des sciences de l’homme’ constitue dans une grammaire traditionnelle un groupe nominal ayant une structure complexe avec plusieurs niveaux intermediaires, dans une analyse partielle elle sera segmentee en trois unites appelees chunks : [maison]gN [des sciences]gp [de l’homme]gp car le rattachement des syntagmes prépostionnels est potentiellement ambigu. Appelée aussi chunking, l’analyse partielle a été introduite par (3) come reponse aux difﬁcultes d’analyse soulevees par le traitement robuste des textes tout-Venants. Plusieurs approches dont (2) ont aborde l’analyse partielle a l’aide des automates ﬁnis, plus précisement a l’aide des cascades de transducteurs ﬁnis. Une cascade de transducteurs est une succession de transducteurs ou chacun permet de reconnaitre un type de chunk. L’ entree de chaque transducteur est constituée par la sortie du transducteur precedent. Notre solution consiste en l’application simultanee, plutot que sequentielle, de tous les automates des chunks qui sont integrés au sein d’un MMC. Les chunks, du fait de leur caractere non récursif, peuvent etre représentés sous la forme d’automates ﬁnis construits sur l’a</context>
</contexts>
<marker>2.</marker>
<rawString>S. Abney. Partial parsing via ﬁnite-state cascades. In Workshop on Robust Parsing, 8th European Summer School in Logic, Language and Information, Prague, Czech Republic, pages 8-15., 1996.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven P Abney</author>
</authors>
<title>Parsing by chunks. In</title>
<date>1991</date>
<booktitle>Principle—Based Parsing: Computation and Psycholinguistics,</booktitle>
<pages>257--278</pages>
<editor>Robert C. Berwick, Steven P Abney, and Carol Tenny, editors,</editor>
<publisher>Kluwer,</publisher>
<location>Dordrecht,</location>
<contexts>
<context position="16000" citStr="(3)" startWordPosition="2513" endWordPosition="2513">tie de la structure syntaxique d’une phrase, plus précisement, la structure associee aux fragments qui n’ont qu’une analyse possible. Par exemple, meme si une suite comme ‘maison des sciences de l’homme’ constitue dans une grammaire traditionnelle un groupe nominal ayant une structure complexe avec plusieurs niveaux intermediaires, dans une analyse partielle elle sera segmentee en trois unites appelees chunks : [maison]gN [des sciences]gp [de l’homme]gp car le rattachement des syntagmes prépostionnels est potentiellement ambigu. Appelée aussi chunking, l’analyse partielle a été introduite par (3) come reponse aux difﬁcultes d’analyse soulevees par le traitement robuste des textes tout-Venants. Plusieurs approches dont (2) ont aborde l’analyse partielle a l’aide des automates ﬁnis, plus précisement a l’aide des cascades de transducteurs ﬁnis. Une cascade de transducteurs est une succession de transducteurs ou chacun permet de reconnaitre un type de chunk. L’ entree de chaque transducteur est constituée par la sortie du transducteur precedent. Notre solution consiste en l’application simultanee, plutot que sequentielle, de tous les automates des chunks qui sont integrés au sein d’un MMC</context>
</contexts>
<marker>3.</marker>
<rawString>Steven P. Abney. Parsing by chunks. In Robert C. Berwick, Steven P Abney, and Carol Tenny, editors, Principle—Based Parsing: Computation and Psycholinguistics, pages 257-278. Kluwer, Dordrecht, 1991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cyril Allauzen</author>
<author>Mehryar Mohri</author>
<author>Brian Roark</author>
</authors>
<title>Generalized algorithms for constructing statistical language models.</title>
<date>2003</date>
<booktitle>In 41st Meeting of the Association for Computational Linguistics,</booktitle>
<pages>40--47</pages>
<location>Sapporo, Japon,</location>
<contexts>
<context position="14290" citStr="(4)" startWordPosition="2247" endWordPosition="2247">age, car des trigrammes apparaissant dans les textes a etiqueter peuvent n’avoir jamais éte observes dans le corpus d’apprentissage. C’est la raison pour laquelle on a recours a des methodes de lissage des probabilites, telles que les methodes de repli (10) qui consistent a se replier sur la probabilité du bigramme b c lorsque le trigramme a b c n’a pas ete observe dans le corpus et, lorsque le bigramme b c n’a pas éte observe, a se replier sur l’unigramme c. Un modele de repli peut étre directement représente sous la forme d’un automate comportant des transitions par défaut comme decrit dans (4). Etant donné un symbole oz, une transition par défaut emanant d’un etat q est empruntée lorsqu’il n’existe pas de transition emanant de q etiquetee par oz. Dans le cas du modele de repli, une transition par défaut est empruntee lorsqu’un trigramme ou un bigramme n’a jamais éte observe. Il ne nous est pas possible ici de décrire plus en detail la structure de tels automates. Pour plus de details, le lecteur est invite a se reférer a l’article cite ci-desssus. Plusieurs approches dans la litterature (14; ll; 9) utilisent les automates ﬁnis ponderés aﬁn de simuler le fonctionnement d’un MMC. Dan</context>
</contexts>
<marker>4.</marker>
<rawString>Cyril Allauzen, Mehryar Mohri, and Brian Roark. Generalized algorithms for constructing statistical language models. In 41st Meeting of the Association for Computational Linguistics, pages 40-47, Sapporo, Japon, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L R Bahl</author>
<author>R L Mercer</author>
</authors>
<title>Part of speech assignment by a statistical decision algorithm.</title>
<date>1976</date>
<booktitle>In Proceedings IEEE International Symposium on Information Theory,</booktitle>
<pages>88--89</pages>
<contexts>
<context position="8891" citStr="(5)" startWordPosition="1361" endWordPosition="1361">Dans ce dernier, l’operation (8) correspond a l’addition usuelle (pour connaitre le poids d’un chemin on additionne les poids des transitions) alors que l’operation EB est le minimum (le poids associe par un transducteur a un mot reconnu est le minimum des poids de tous les chemins du transducteur reconnaissant le mot, c’est-a-dire le chemin ayant la meilleure probabilité). 3 Etiquetage morpho-syntaxique Le processus d’étiquetage morpho-syntaxique utilise dans le cadre de ce travail reprend les principes de l’etiquetage morpho-syntaxique fonde sur les chaines de Markov cachées, introduit dans (5). Les états du MMC correspondent aux categories morpho-syntaxiques et les observables aux mots du lexique. Ces derniers constituent l’alphabet EL et les etiquettes des categories morpho-syntaxiques constituent l’alphabet 20. Le processus d’etiquetage, dans un tel modele, consiste a retrouver la suite d’etats la plus probable etant donné une suite d’observables. Les parametres d’un MMC se divisent en probabilités d’emission et en probabilités de transition. Une probabilité d’émission est la probabilité d’un mot etant donné une catégorie (P(m |c)) 1On préfere les logarithmes de probabilités aux </context>
</contexts>
<marker>5.</marker>
<rawString>L. R. Bahl and R. L. Mercer. Part of speech assignment by a statistical decision algorithm. In Proceedings IEEE International Symposium on Information Theory, pages 88-89, 1976.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kuang-Hua Chen</author>
<author>Hsin-Hsi Chen</author>
</authors>
<title>Extracting noun phrases from large-scale texts: A hybrid approach and its automatic evaluation.</title>
<date>1994</date>
<booktitle>In Meeting of the Association for Computational Linguistics,</booktitle>
<pages>234--241</pages>
<contexts>
<context position="23076" citStr="(6)" startWordPosition="3648" endWordPosition="3648">s pour objectif de favoriser un découpage en chunks d’une meme suite de categories plutot qu’un autre (l’analyse en chunks est unique !). Son objectif est de fournir un moyen de comparer entre elles differentes sequences de categories possibles pour une meme phrase. Pour cela, l’analyseur partiel associe a toute sequence de categories une probabilite qui est d’autant plus élevee que la sequence de categories correspond a des sequences de chunks bien formés, agencés dans un ordre lineaire observe sur un corpus d’apprentissage. Cette approche partage plusieurs points communs avec les travaux de (6) qui utilisent eux aussi des transducteurs ponderes pour réaliser un analyseur partiel probabiliste. Cependant, dans leur cas, plusieurs découpages de la phrase en chunks sont possibles et l’objectif de l’analyseur est de fournir le decoupage le plus probable. De plus, leur analyseur prend en entree une sequence unique de categories. La probabilité d’une suite de categories découpée en chunks est calculee a partir de deux types de probabilités : des probabilités intra chunk et des probabilités inter chunks. Une probabilité intra chunk est la probabilité qu’une suite de categories cu, constitue</context>
</contexts>
<marker>6.</marker>
<rawString>Kuang-Hua Chen and Hsin-Hsi Chen. Extracting noun phrases from large-scale texts: A hybrid approach and its automatic evaluation. In Meeting of the Association for Computational Linguistics, pages 234-241, 1994.</rawString>
</citation>
<citation valid="false">
<authors>
<author>http www nyu edupageslinguisticsintex</author>
</authors>
<contexts>
<context position="5231" citStr="(7)" startWordPosition="780" endWordPosition="780">spéciﬁques a concevoir pour différents types de donnees, plus d’algorithmes a adapter, a programmer et a optimiser. La realisation de tels traitements depend de maniere cruciale de l’existence de bibliotheques logicielles de manipulation d’automates. Dans le cadre de ce travail, nous avons utilise les outils FSM et GRM de ATT (8). Notre travail se situe dans la mouvance du traitement probabiliste de la langue a l’aide d’automates pondéres, dont on trouvera un appercu dans (12). Il se distingue dans son esprit d’autres approches fondées sur les automates ﬁnis non probabilistes, telles qu’INTEX (7), dans lesquelles des regles sont construites manuellement pour etre ensuite utilisees dans le cadre de traitements automatiques. L’ orgaI1isation de l’article est la suivante : dans la partie 2, on reprend quelques deﬁnitions concemant les automates ponderes et on introduit quelques notations. Les sections 3 et 4 décrivent respectivement les principes d’un étiqueteur probabiliste et d’un analyseur partiel et leur implementation sous la forme d’automates ponderes. Dans la section 5, l’integration des deux modules est décrite. Enﬁn, des experiences sont presentees dans la partie 6 et des travau</context>
</contexts>
<marker>7.</marker>
<rawString>http://www.nyu.edu/pages/linguistics/intex/.</rawString>
</citation>
<citation valid="false">
<note>http://www.research.att.co1n/sw/tools/{fsm,grrn}.</note>
<contexts>
<context position="4959" citStr="(8)" startWordPosition="736" endWordPosition="736">ntre eux grace aux operations de combinaison d’automates, combinaisons plus difﬁciles a realiser lorsque les differents modules reposent sur des modeles formels differents. Un autre avantage de l’homogéneite de ce cadre est la facilite de mise en oeuvre : plus de formats spéciﬁques a concevoir pour différents types de donnees, plus d’algorithmes a adapter, a programmer et a optimiser. La realisation de tels traitements depend de maniere cruciale de l’existence de bibliotheques logicielles de manipulation d’automates. Dans le cadre de ce travail, nous avons utilise les outils FSM et GRM de ATT (8). Notre travail se situe dans la mouvance du traitement probabiliste de la langue a l’aide d’automates pondéres, dont on trouvera un appercu dans (12). Il se distingue dans son esprit d’autres approches fondées sur les automates ﬁnis non probabilistes, telles qu’INTEX (7), dans lesquelles des regles sont construites manuellement pour etre ensuite utilisees dans le cadre de traitements automatiques. L’ orgaI1isation de l’article est la suivante : dans la partie 2, on reprend quelques deﬁnitions concemant les automates ponderes et on introduit quelques notations. Les sections 3 et 4 décrivent re</context>
<context position="8320" citStr="(8)" startWordPosition="1277" endWordPosition="1277">u. Si plusieurs chemins de R permettent de reconnaitre u, alors |[R] |est égale a la some (69) des poids des différents chemins correspondant a u. Etant donné un reconnaisseur pondere R, on deﬁnit l’opérateur n-meilleurs chemins, note mc(R, n) qui retourne le reconnaisseur constitue de l’union des 77. chemins les plus probables dans R. Toutes ces notions sont etendues aux transducteurs. Dans les experiences décrites dans ce papier on a associe aux transitions des transducteurs l’oppose de logarithmes de probabilitésl ; on a utilise le semi-atmeau tropical sur ]R+. Dans ce dernier, l’operation (8) correspond a l’addition usuelle (pour connaitre le poids d’un chemin on additionne les poids des transitions) alors que l’operation EB est le minimum (le poids associe par un transducteur a un mot reconnu est le minimum des poids de tous les chemins du transducteur reconnaissant le mot, c’est-a-dire le chemin ayant la meilleure probabilité). 3 Etiquetage morpho-syntaxique Le processus d’étiquetage morpho-syntaxique utilise dans le cadre de ce travail reprend les principes de l’etiquetage morpho-syntaxique fonde sur les chaines de Markov cachées, introduit dans (5). Les états du MMC correspond</context>
</contexts>
<marker>8.</marker>
<rawString>http://www.research.att.co1n/sw/tools/{fsm,grrn}.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bryan Jurish</author>
</authors>
<title>A hybrid approach to part-of-speech tagging.</title>
<date>2003</date>
<booktitle>Berlin-Brandenburgishe Akademie der Wissenschaften,</booktitle>
<tech>Technical report,</tech>
<marker>9.</marker>
<rawString>Bryan Jurish. A hybrid approach to part-of-speech tagging. Technical report, Berlin-Brandenburgishe Akademie der Wissenschaften, 2003.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Slava M Katz</author>
</authors>
<title>Estimation of probabilities from sparse data for the language model component of a speech recogniser.</title>
<journal>IEEE Transactions on Acoustics, Speech, and Signal Processing,</journal>
<pages>35--3</pages>
<contexts>
<context position="13944" citStr="(10)" startWordPosition="2184" endWordPosition="2184">a1phabet d’entrée et dont chaque transition possede le meme symbole en entrée et en sortie. Un tel transducteur représente par consequent la relation identité réduite au langage reconnu par le reconnaisseur. Couplage d’un étiqueteur morpho-syntaxique et d’un cmalyseurpartiel estimees par simple maximum de vraisemblance sur un corpus d’apprentissage, car des trigrammes apparaissant dans les textes a etiqueter peuvent n’avoir jamais éte observes dans le corpus d’apprentissage. C’est la raison pour laquelle on a recours a des methodes de lissage des probabilites, telles que les methodes de repli (10) qui consistent a se replier sur la probabilité du bigramme b c lorsque le trigramme a b c n’a pas ete observe dans le corpus et, lorsque le bigramme b c n’a pas éte observe, a se replier sur l’unigramme c. Un modele de repli peut étre directement représente sous la forme d’un automate comportant des transitions par défaut comme decrit dans (4). Etant donné un symbole oz, une transition par défaut emanant d’un etat q est empruntée lorsqu’il n’existe pas de transition emanant de q etiquetee par oz. Dans le cas du modele de repli, une transition par défaut est empruntee lorsqu’un trigramme ou un</context>
</contexts>
<marker>10.</marker>
<rawString>Slava M. Katz. Estimation of probabilities from sparse data for the language model component of a speech recogniser. IEEE Transactions on Acoustics, Speech, and Signal Processing, 35(3):400-401,</rawString>
</citation>
<citation valid="true">
<authors>
<author>André Kempe</author>
</authors>
<title>Finite state transducers approximating hidden markov models.</title>
<date>1997</date>
<booktitle>In ACL’97,</booktitle>
<pages>460--467</pages>
<location>Madrid,</location>
<contexts>
<context position="19916" citStr="(11)" startWordPosition="3165" endWordPosition="3165">t facile de limiter le produit de la composition a ce seul resultat en associant a chaque transition intra chunk un poids de 0 et aux transitions extra chunk un poids de 1 et en ne gardant des résultats produits que le chemin de poids minimal. Le processus d’analyse peut etre representé par l’expression : mc(C o A, 1) 5 Couplage de l’étiquetage et de l’analyse partielle Les modeles d’étiquetage morpho-syntaxique a l’aide des transducteurs ponderes cites dans la section 3, intégrent aussi (ou prevoient la possibilite d’intégrer) des contraintes syntaxiques dans le processus d’étiquetage. Kempe (11) prevoit la possibilite de composer la sortie du tagger avec des transducteurs encodant des regles de correction des erreurs les plus fréquentes, Tzoukerman (14) utilise des contraintes negatives aﬁn de diminuer de facon drastique la probabilite des chemins comportant des suites improbables d’étiquettes (par exemple un determinant suivi d’un verbe). D’un point de vue general, notre travail se distingue des autres par le fait qu’il integre deux modules complets (un module d’etiquetage et un module d’analyse partielle) au sein d’un seul, realisant l’etiquetage et l’analyse partielle. Il ne s’agi</context>
</contexts>
<marker>11.</marker>
<rawString>André Kempe. Finite state transducers approximating hidden markov models. In ACL’97, pages 460-467, Madrid, Spain, 1997.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mehryar Mohri</author>
</authors>
<title>Finite-state transducers in language and speech processing.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>2</issue>
<contexts>
<context position="5109" citStr="(12)" startWordPosition="761" endWordPosition="761">les formels differents. Un autre avantage de l’homogéneite de ce cadre est la facilite de mise en oeuvre : plus de formats spéciﬁques a concevoir pour différents types de donnees, plus d’algorithmes a adapter, a programmer et a optimiser. La realisation de tels traitements depend de maniere cruciale de l’existence de bibliotheques logicielles de manipulation d’automates. Dans le cadre de ce travail, nous avons utilise les outils FSM et GRM de ATT (8). Notre travail se situe dans la mouvance du traitement probabiliste de la langue a l’aide d’automates pondéres, dont on trouvera un appercu dans (12). Il se distingue dans son esprit d’autres approches fondées sur les automates ﬁnis non probabilistes, telles qu’INTEX (7), dans lesquelles des regles sont construites manuellement pour etre ensuite utilisees dans le cadre de traitements automatiques. L’ orgaI1isation de l’article est la suivante : dans la partie 2, on reprend quelques deﬁnitions concemant les automates ponderes et on introduit quelques notations. Les sections 3 et 4 décrivent respectivement les principes d’un étiqueteur probabiliste et d’un analyseur partiel et leur implementation sous la forme d’automates ponderes. Dans la s</context>
</contexts>
<marker>12.</marker>
<rawString>Mehryar Mohri. Finite-state transducers in language and speech processing. Computational Linguistics, 23(2), 1997.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mehryar Mohri</author>
</authors>
<title>Robustness in Language and Speech Technology, chapter Weighted Grammars Tools: the GRM Library,</title>
<date></date>
<pages>pages</pages>
<publisher>Kluwer Academic Publishers,</publisher>
<contexts>
<context position="27706" citStr="(13)" startWordPosition="4415" endWordPosition="4415">s de la construction du modele n Suites diffél-entes (fétiquettes (51, 32, . . . sn) representant toutes les réalisations de ce chunk dans un corpus d’apprentissage, on note n,- le nombre d’occurrences de la suite c,-. La probabilité de 3, n’est autre que sa frequence relative : P(s,~) = Cette probabilité est la probabilité du chen1in correspondant a 3, dans le reconnaisseur K ,-. I |différentes réalisations de chaque chunk ||corpus hybride | I Les modeles intra chunks et le modele externe sont combines pour former un unique transducteur a l’aide de l’operation de remplacement, intoduite dans (13). Cette derniere permet de remplacer dans le modele externe une transition &lt; Ki &gt; par l’automate K ,-. Le transducteur resultant est appelé AP (pour analyseur probabiliste). Le modele conjoint d’étiquetage et d’analyse est maintenant mc(M o E o AP, 1). Couplage d’un étiqueteur morpho-syntaxique et d’un cmalyseurpartiel 6 Expériences Les experiences ont ete menees sur le corpus etiquete Paris 7 (1). Le corpus est constitue de 900K mots etiquetes avec un jeu de 222 etiquettes indiquant la categorie et les traits morphologiques des mots. On a reserve une partie du corpus de 760K mots pour l’appre</context>
</contexts>
<marker>13.</marker>
<rawString>Mehryar Mohri. Robustness in Language and Speech Technology, chapter Weighted Grammars Tools: the GRM Library, pages 19-40. Jean-Claude Junqua and Gertjan Van Noord (eds) Kluwer Academic Publishers, 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Evelyne Tzoukermann</author>
<author>Dragomir R Radev</author>
</authors>
<title>Use of weighted ﬁnite state trasducers in part of speech tagging. Natural Language Engineering,</title>
<date>1997</date>
<contexts>
<context position="20077" citStr="(14)" startWordPosition="3190" endWordPosition="3190">poids de 1 et en ne gardant des résultats produits que le chemin de poids minimal. Le processus d’analyse peut etre representé par l’expression : mc(C o A, 1) 5 Couplage de l’étiquetage et de l’analyse partielle Les modeles d’étiquetage morpho-syntaxique a l’aide des transducteurs ponderes cites dans la section 3, intégrent aussi (ou prevoient la possibilite d’intégrer) des contraintes syntaxiques dans le processus d’étiquetage. Kempe (11) prevoit la possibilite de composer la sortie du tagger avec des transducteurs encodant des regles de correction des erreurs les plus fréquentes, Tzoukerman (14) utilise des contraintes negatives aﬁn de diminuer de facon drastique la probabilite des chemins comportant des suites improbables d’étiquettes (par exemple un determinant suivi d’un verbe). D’un point de vue general, notre travail se distingue des autres par le fait qu’il integre deux modules complets (un module d’etiquetage et un module d’analyse partielle) au sein d’un seul, realisant l’etiquetage et l’analyse partielle. Il ne s’agit pas d’integrer dans un etiqueteur des grammaires locales concues pour éliminer certaines structures agrammaticales, mais d’integrer véritablement l’information</context>
<context position="31464" citStr="(14)" startWordPosition="5026" endWordPosition="5026"> citee ci-dessus, d’autres sont dues a l’estimation des probabilites intra chunk (la probabilite qu’une sequence de categories donnee constitue un chunk d’une nature donnee). Ces dernieres sont en effet estimees par simple maximum de vraisemblance et attribuent par consequent une probabilite nulle a une realisation de chunk qui n’a jamais ete observee dans App. Une forme de lissage de ces probabilites semble necessaire. D’autres erreurs proviennent des limites theoriques du modele et necessiteraient pour etre corrigees une analyse syntaxique complete. 4Ce résultat est supérieur au résultat de (14) (4% de taux d’erreur) sur le meme corpus et avec le meme jeu d’étiquettes. Cette difference provient, au moins en partie, du fait que nous avons travaillé sans mots inconnus : tous les mots de Test apparaissent dans le dictionnaire. Nous avons effectue cette hypothese car 1’obj et de notre travail est d’étudier1’apport de 1’ana1yseur pa11:ie1 sur les performances d’un étiqueteur fondé sur les MC et nous estimons que 1’inﬂuence des mots inconnus sera quasiment la meme sur les différents modeles que nous avons testé. 5Contrairement aux modeles M1 et M2 les poids associés a une sequence de mots </context>
</contexts>
<marker>14.</marker>
<rawString>Evelyne Tzoukermann and Dragomir R. Radev. Use of weighted ﬁnite state trasducers in part of speech tagging. Natural Language Engineering, 1997.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>