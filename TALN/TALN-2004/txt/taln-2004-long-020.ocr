TALN 2004, Fés, 19-21 avn'12004

Désambigu'1'sation de corpus monolingues
par des approches de type Lesk

Florentina Vasilescu, Philippe Langlais

RALI/IRO, Université de Montréal
CP. 6128, succursale Centre—ville

Montréal, Québec, H3C CJ7 Canada
{vasilesf,felipe}@iro.umontreal.ca

Résumé - Abstract

Cet article présente une analyse détaille’e des facteurs qui déterminent les performances des
approches de de’sambigu'1'sation de’rivées de la méthode de Lesk (1986). Notre étude porte sur
une se’rie d’expériences concernant la me’thode originelle de Lesk et des variantes que nous
avons adapte’es aux caractéristiques de WORDNET. Les variantes implémentées ont été évalue’es
sur le corpus de test de SENSEVAL2, English All Words, ainsi que sur des extraits du corpus
SEMCOR. Notre e’valuation se base d’un cote’, sur le calcul de la précision et du rappel, selon
le modele de SENSEVAL, et d’un autre cote’, sur une taxonomie des réponses qui permet de
mesurer la prise de risque d’un décideur par rapport a un systeme de reference.

This paper deals with a detailed analysis of the factors determining the performances of Lesk-
based WSD methods. Our study consists in a series of experiments on the original Lesk algo-
rithm and on its variants that we adapted to WORDNET. These methods were evaluated on the
test corpus from SENSEVAL2, English All Words, and on excerpts from SEMCOR. The evalua-
tion metrics are based on precision and recall, as in SENSEVAL exercises, and on a new method
estimating the risk taken by each variant.

Mots-clefs — Keywords

Désambigu'1'sation sémantique, algorithme de Lesk, naive Bayes, WORDNET
Word sense desambiguation, Lesk’s algorithm, naive Bayes, WORDNET

Florentina Vasilescu, Philippe Langlais

1 Introduction

La de’sambigu'1'sation sémantique d’un texte consiste a déterminer le sens correct des mots de ce
texte. Des campagnes d’éValuation comme SENSEVAL sont la preuve du grand intérét porté au
sein de notre communaute’ a cette tache (90 équipes ont mentionne’ leur intérét a participer a la
prochaine campagne SENSEVAL3).

Cet intérét se traduit par un foisonnement de méthodes et de ressources utilisées, comme par
exemple les dictionnaires, les thésaurus ou les lexiques se’mantiques électroniques (Lesk, 1986;
Banerjee & Pedersen, 2003), les corpus annotés, comportant des étiquettes de sens (Voir par
exemple (Crestan et al., 2003)), les corpus non annotés (Yarowsky, 1995; Schiitze, 1998) ou
une combinaison de ces ressources (Stevenson & Wilks, 2001).

Mal gre’ tous ces travaux, nous ne connaissons pas d’application réelle qui tire réellement proﬁt
de la désambigu'1'sation. C’est certes intuitivement une tache indispensable a la bonne réalisation
de toutes les applications qui nécessitent un niveau de comprehension du message d’entre’e (la
traduction automatique en téte), mais force est de constater que c’est une tache difﬁcile que
nous ne maitrisons pas completement.

L’absence de ressources étiquete’es de qualite’ et en grande quantité est une explication souvent
avancée pour rendre compte de cet échec. D’autres (e.g. (Ve’ronis, 2001)) soulignent que la tache
sur laquelle les diffe’rents systemes se comparent est mal déﬁnie. Ils mentionnent en particulier
que le niveau de granularite’ de WORDNET, la ressource utilise’e dans la campagne d’éValuation
passe’e, est souvent trop ﬁne pour que meme des humains s’accordent sur la bonne e’tiquette a
donner a un mot.

Dans ce contexte, nous avons décide’ d’implémenter un algorithme de de’sambigu'1'sation simple
et tente’ de comprendre ses limites autrement qu’en analysant ses performances brutes en terme
de précision et de rappel. Le candidat qui nous a semble’ le plus intéressant dans cette optique
exploratoire est l’algorithme propose’ par Lesk (1986) qui consiste a compter le nombre de mots
communs entre les déﬁnitions d’un mot (généralement trouVe’es dans un dictionnaire electro-
nique) et les de’ﬁnitions des mots de son contexte. Le sens retenu correspondant a la déﬁnition
pour laquelle on compte le plus grand nombre de mots communs avec le contexte. Cette ide’e
simple s’est aVére’e meilleure que bon nombre de techniques plus e’Voluées dans le cadre de la
campagne SENSEVAL1.

Nous avons testé de nombreuses Variantes de l’algorithme de Lesk adapte’es a WORDNET. Les
résultats de ces Variantes ont été compare’s avec ceux d’une Version Naive Bayes (qui peut elle—
méme étre décrite comme une Variante de l’algorithme de Lesk). Nous résumons en section 2
les Variantes et les facteurs les plus saillants que nous avons e’tudie’s.

La performance obtenue par chaque Variante n’e’tant pas l’objet principal de notre étude, nous
avons mis au point une taxonomie des réponses que peuvent faire nos décideurs et qui aide a
mieux comprendre leur performance. Nous décrivons cette taxonomie en section 3 et déﬁnissons
la notion de risque associe’e a un algorithme de de’sambigu'1'sation.

Nous décrivons ensuite en 4 notre protocole eXpe’rimental et analysons les performances de
chaque Variante décrite. Nous discutons ﬁnalement nos résultats en section 5.

Désambigufsation de corpus monolingues par des approches de type Lesk

2 Algorithmes étudiés

Les algorithmes que nous avons implémentés s’appuient tous sur un modele extremement
simple décrit en pseudo—code en ﬁgure 1. Ce modele prend en entre’e un mot a désambigu'1'ser
t ainsi qu’une liste (trie’e en ordre décroissant de fréquence) de ses sens candidats et produit en
sortie le sens sélectionne’. Toutes les Variantes teste’es ici different seulement par le choix associe’
aux fonctions Score, Description et Contexte décrites dans les sections suivantes.

Entrée:

t, un mot a désambigu'1'ser

S’ = {$1, . . . ,sN}, les sens candidats ordonne’s en ordre décroissant de fréquence
Sortie:

sens, l’indice dans S’ du sens retenu

score <— —OO

sens <— 1 //choix par de’faut du sens le plus frequent
C <— Contexte ( t) //contexte du mot cible
forall i E [1,N] do
D <— Description ( s,- ) //description de 53,- extraite de WORDNET
sup <— O
for all w E C do
W <— Description ( w) //description de w extraite de WORDNET
sup <— sup + Score (D ,W) //cumul des superpositions enlre D et W

if sup > score then
score <— sup
sens <— i //on retient le sens de plus haut score

FIG. 1 — Canevas des Variantes e’tudie’es. Les trois fonctions dont depend ce modele sont décrites
a meme le texte.

2.1 Déﬁnition du contexte

La premiere des fonctions dont depend le modele de la ﬁgure 1 — Contexte (t) — déﬁnit
l’ensemble des mots qui Vont servir a la de’sambigu'1'sation de 15. Nous avons testé deux imple’—
mentations de cette fonction. La premiere — celle utilise’e par défaut — consiste a retourner
l’ensemble des mots pleins centre's autour du mot t. Nous rapportons les résultats de nos Va-
riantes en section 4 pour des longueurs de contexte de j:2 (les deux mots pleins directement
a gauche et a droite de t), j:3, j:8, j:1O et j:25 mots. Notons qu’Audibert (2003) suggere
que choisir un contexte symétrique n’est pas optimal dans le cas des Verbes. Il montre en effet
que l’information servant a les désambigu'1'ser a tendance a se trouver dans les comple’ments
d’objets, et donc plutot a droite des Verbes. L’auteur suggere pour cela d’utiliser un contexte
< -2, +4 >. Dans les memes actes, Crestan et al. (2003) montrent qu’il est bénéﬁque — du
moins pour certaines classes syntaxiques de mots — de mettre en ceuvre une procedure auto-
matique de se’lection du contexte.

La seconde implementation de cette fonction, de’note’e CL, consiste a extraire du contexte d’oc—

Florentina Vasilescu, Philippe Langlais

currence de 15 les mots constitutifs de ce que nous appelons apres Hirst et St—Onge (1998) sa
chaine lexicale. Dans une étude sur la correction des malapropismes (confusion de deux mots
comportant la meme prononciation ou des formes orthographiques tres semblables mais des
sens différents), les auteurs s’appuient sur l’idée que pour rendre un discours cohérent, les mots
cooccurrant dans un meme contexte sont reliés entre eux par des relations de cohésion, formant
des enchainements logiques qu’ils baptisent chaines lexicales.

Nous avons adapte’ cette ide’e a la de’sambigu'1'sation sémantique, en conside’rant que la leve’e de
l’ambigu'1'té d’un mot peut se faire en déterminant la chaine lexicale de ce mot. Seuls les mots
de cette chaine sont alors retenus dans le contexte. Notre implémentation utilise les relations
de synonymie et d’hyperonymie de WORDNET ainsi qu’une mesure de similarite’ entre deux
ensembles (nous avons utilise’ la formule de Jaccard) pour tester l’appartenance d’un mot du
contexte de 15 a sa chaine lexicale. Son principe consiste a associer a chaque mot w du contexte
l’ensemble E (w) des mots des synsets rencontre’s en suivant les liens (jusqu’a la racine) d’hy—
peronymie et de synonymie déﬁnis dans WORDNET pour chaque sens de w. On de’cide alors
que w appartient a la chaine lexicale de 15 si le score de Jaccard pour les deux ensembles E (w)
et E (t) est supérieur a un seuil ﬁxe’ empiriquement. Le re’sultat de ce processus est illustré en
ﬁgure 2.

Committee approval of Gov._Price_Daniel’s “abandoned property” act seemed cer-
tain Thursday despite the adamant protests of Texas bankers. Daniel perso-
nally led the ﬁght for the measure, which he had watered_down considerably
since its rejection by two previous Legislatures, in a public hearing before the
House_Committee_on_Revenue_and_Taxation. Under committee rules, it went auto-
matically to a subcommittee for one week.

— E(c0mm2'ttee) = {committee, commission, citizens, administrative—unit, administrative-
body, organization, social—group, group, grouping}

— E(legz'slatm'e) = {legislature, legislative—assembly, general—assembly, law—makers, assem-
bly, gathering, assemblage, social—group, group, grouping}

FIG. 2 — Illustration d’une chaine lexicale. Les mots en gras forment la chaine lexicale du mot
committee. E (committee) et E (legislature) sont les ensembles de mots obtenus en suivant les
relations de synonymie et d’hyperonymie dans WORDNET.

2.2 Description associée £1 un mot

L’ al gorithme de la ﬁgure 1 applique une fonction de score a la représentation Description ( w)
d’un mot w du contexte de 15 ainsi qu’a celle d’un sens particulier 3,; de 15: Descrz'pt2'0n(s,;). Dans
tous nos tests, l’entite’ descriptive d’un sens est représentée par un sac de mots, c’est—a—dire un
ensemble de mots dont l’ordre et la dépendance sont i gnore’s. Cet ensemble contient seulement
des mots pleins (noms, verbes, adjectifs ou adverbes) dans leur forme canonique (lemme) et
l’entite’ descriptive associe’e a un mot est l’union des entités descriptives de chacun des sens de
ce mot (Description(w) = U ) Description(s) ; ou S'ens(w) est l’ensemble
des sens de w selon WORDNET).

s€Sens('w

Nous avons étudie’ trois variantes de la représentation d’un sens. La premiere, de’note’e DEF,

Désambigufsation de corpus monolingues par des approches de type Lesk

consiste a ne conserver que les mots pleins de la déﬁnition associe’e au sens selon WORDNET1.
La deuxieme, note’e REL, consiste a regrouper les mots des synsets parcourus en suivant dans
WORDNET les relations de synonymie et d’hyperonymie (c’est comme cela qu’a e’té obtenue la
représentation E (committee) dans l’exemple de la ﬁgure 2). Une troisieme variante, DEF+REL,
consiste a faire l’union des deux représentations pre’cédentes.

Une autre variante de la fonction Description permet d’implémenter la variante simpliﬁe’e
de Lesk propose’e par Kilgarriff et Rosenzweig (2000). Cette variante consiste a comptabiliser
les intersections entre l’entite’ descriptive d’un sens candidat et les mots du contexte de 15 (et
non plus leur déﬁnition). Dans ce cas, la description associe’e a un mot du contexte est tres
simple (Description (w) = {w}) ; la description d’un sens candidat n’est quant a elle pas
change’e (DEF, REL, ou DEF+REL).

2.3 Fonction de score

Nous avons testé de nombreuses variantes de la fonction de score Score(E1,E2) entre deux
entités descriptives E1 et E2 dont les détails et les performances peuvent étre lus dans (Vasi-
lescu, 2003). Nous décrivons ici les classes de variantes les plus saillantes qui sont toutes des
fonctions cumulatives du score de chaque intersection entre E1 et E2.

La variante la plus simple, désigne’e par LESK dans la suite, consiste a donner a chaque inter-
section le score unitaire. C’est le score qui correspond a l’algorithme de Lesk.

Une deuxieme classe de variantes dénomme’e PONDERE suit la suggestion faite par (Lesk, 1986)
que le score devrait tenir compte de la taille de l’entre’e du dictionnaire pour un sens donne’
aﬁn d’éviter que les descriptions trop longues ne dominent le processus de prise de de’cision
(plus une description est longue et plus les intersections sont probables). Le score associe’ a
une intersection entre deux mots est donc normalise’ par l’inverse du logarithme de la taille
(compte’e en mots) de la description du sens candidat. Nous avons étudie’ d’autres mécanismes
de pondération tenant par exemple compte de la distance du mot du contexte au mot cible, ou
encore de la fréquence d’occurrence dans la langue du mot du contexte sans obtenir de gain lors
de la désambigu'1'sation.

La troisieme fonction de score utilise’e, désigne’e par BAYES, est spe’ciﬁque a notre implemen-
tation naive Bayes qui s’inscrit également dans le canevas de la ﬁgure 1. Une telle approche
sélectionne en effet parmi les sens candidats 3 du mot cible celui qui maximise la quantité
p(s|C'0nteacte  en faisant comme hypothese que tous les mots du contexte sont indépendants.
Précise’ment, notre fonction de score est:

10gP(5)+ Z 10g(>\P(w|3)+(1->\)P(w))

w€Contea:te (t)

ou les trois distributions p(s), p(w|s) et p(w) sont déterminées par fréquence relative a partir
du corpus SEMCOR décrit brievement dans la section 4.1. Le lissage de p(w|s) par un modele
uni gramme p(w) est ici nécessaire compte tenu de la forme tres pique’e des distributions condi-
tionnelles (comme nous l’avons mentionne’ en introduction, les corpus étiquetés sont de petite
taille). Ce lissage est controle’ par un unique parametre A ﬁxe’ a 0.95 dans nos expériences.

1Cette déﬁnition peut contenir également des exemples d’usage.

Florentina Vasilescu, Philippe Langlais

3 Métriques d’évaluation

A l’instar des campagnes SENSEVAL, nous mesurons les performances de nos diffe’rents deci-
deurs a l’aide des mesures classiques de précision et de rappel. La précision (resp. le rappel)
est le ratio du nombre de réponses correctes fournies par le systeme sur le nombre de deci-
sions faites (resp. a prendre). Ces mesures ne permettent qu’indirectement d’appre’cier le mérite
de chaque décideur (au dela du fait qu’un bon décideur est celui pour lequel on mesure une
précision et un rappel éleve’). Aussi proposons—nous une taxonomie des réponses faites par nos
algorithmes qui permet de comparer un décideur a un systeme de re’férence: ici le systeme BASE
qui retourne touj ours le sens candidat le plus fréquent selon WORDNET.

Cette taxonomie fait intervenir deux caractéristiques propres a une réponse, a savoir sa cor-
rection (C'=correcte, U = incorrecte) et son effectivite’ (E=effectiVe, E = non effective) ainsi
que deux caracte’ristiques supplémentaires liées a la réponse du systeme de référence (BASE)
qui peut étre juste (B) ou fausse (F) tout en étant égale (=) ou pas (aé) a la réponse faite par le
systeme teste’. Nous qualiﬁons une réponse d’eﬂective lorsqu’au moins une intersection a été ob-
servée entre les représentations du contexte et la représentation du sens choisi (nous rappelons
qu’en cas de non décision, le sens le plus fréquent selon WORDNET est toujours sélectionne’).
En prenant comme systeme de référence le systeme BASE, nous obtenons une combinaison de
7 classes qui sont représente’es en ﬁgure 3. Nous pouvons alors de’ﬁnir deux types de risques.

decision wrrecte ?

(C) nui mm (E)
vvlps != 0 ? nvlps != 0 ?
(E) nui non (E) (E) nui mm (E)
= BASE ? = BASE ? = BASE ? == BASE ?
nui non $ nui nui non 9'"-
O O O . ms}: wrrecte .9 .
CE=B CE!=B,I_3 CE=B E1~:=§ EE=§
11+ (B) "'" Q‘: (E)
n- 

FIG. 3 — Taxonomie des réponses faites par un de’cideur. La classe CE = B caractérise par
exemple les réponses effectives correctes qui sont identiques au systeme de re’férence; alors
que la classe CE aé E désigne les réponses effectives correctes diffe’rentes de la réponse du
systeme de base qui elle est fausse. ovlps désigne le nombre de superpositions considérées lors
de la prise de décision.

Le risque positif (R+) est donne’ par le nombre (C E aé B, E) de décisions effectives correctes,
différentes des décisions de BASE. Le risque négatif (R—) est le nombre (@E aé B) de deci-
sions effectives incorrectes, pour lesquelles le systeme de référence était quanta lui correct. Ces
deux quantite’s sont normalise’es par le nombre total de décisions faites. La diffe’rence entre ces
deux mesures de’termine ce que nous appelons le gain par rapport aux performances du systeme
de re’férence.

Désamb1'gui'sat1'on de corpus monolingues par des approches de type Lesk

4 Expériences

4.1 Protocole

Nous avons utilise’ la version 1.7.1 de WORDNET pour obtenir les diffe’rents sens possibles de
chaque mot ainsi que les déﬁnitions et les relations associées a chaque sens. Les mots a de’—
sambigu'1'ser qui n’e’taient pas présents dans WORDNET (0.8% des instances) sont comptabilise’s
comme des erreurs du décideur teste’. C’est également les informations foumies par WORDNET
qui nous permettent de classer les différents sens candidats entre eux (nous nous basons sur les
champs sense_number et tag_cnt de la table d’indeX de WORDNET).

Nous avons de plus utilise’ la version 1.7.1 du corpus SEMCOR pour entrainer nos versions naive
Bayes. Dans le but de vériﬁer la stabilite’ de nos observations, nous avons testé nos décideurs
sur le corpus de test de la campagne SENSEVAL2 ainsi que sur diffe’rents jeux de test que nous
avons cre’e’s a partir du corpus SEMCOR (les approches naive Bayes entraine’es sur ce meme
corpus n’ont cependant pas e’té teste’es sur ces jeux de test SEMCOR). Les résultats que nous
rapportons ici sont ceux obtenus sur le premier corpus (nous laissons le soin au lecteur inte’resse’
de lire les différences inter—corpus dans (Vasilescu, 2003)), car c’est le corpus qui sert de point de
comparaison dans la plupart des e’tudes en de’sambigu'1'sation (2473 mots cibles). Nous veillons
cependant dans notre discussion a ne dégager que les tendances observe’es sur l’ensemble de
nos jeux de test.

Chaque décideur avait a charge de désambigu'1'ser l’ensemble des mots pleins du texte d’entre’e
(les mots étiquete’s head dans le corpus SENSEVAL2), ce que l’on désigne habituellement par la
piste English All Words.

4.2 Performances
4.2.1 Comparaison des différents décideurs

Le tableau 1 présente la précision et le rappel des décideurs décrits dans la section 2 dans leur
version DEF. Plusieurs tendances se dégagent de ce tableau. En premier lieu, il convient de
noter que l’algorithme de Lesk dans sa formulation originelle (LESK) donne les moins bons
résultats. En fait, les résultats de ce décideur sont de loin inférieurs a ceux obtenus en prenant
le sens le plus fréquent. Cette observation est cependant cohérente avec celles de Litkowski
(2002) o1‘1l’auteur analyse les différents facteurs responsables des performances de son systeme
a SENSEVAL2 (CL Research — DIMAP, 29.3% de précision et rappel English lexical samplez). ll
fait en particulier l’observation que seulement 30% des instances a désambigu'1'ser tiraient proﬁt
de l’information de type Lesk (déﬁnitions + exemples).

Il est e’galement clair que la version simpliﬁe’e (SLESK) de cet algorithme donne de meilleurs
résultats. Rappelons que la version simpliﬁe’e consiste a compter les intersections entre la des-
cription associe’e a chaque sens candidat et les mots du contexte euX—méme (et non leur descrip-
tion).

Nous constatons que de ponde’rer le score par l’inverse de la longueur de la description, tel
que le suggérait Lesk (1986) ne s’avere pas une stratégie payante, et ce aussi bien pour les

2Voir le site www. cs .unt . edu/~rada/senseval.

Horentina Vasilescu, Philippe Langlais

P j:2 R P j:3 R P j:8 R P j:10 R P j:25 R
LESK 42.64 42.26 42.96 42.58 43.21 42.82 43.29 42.90 42.39 42.01
+ PONDERE 39.29 38.94 39.41 39.06 41.21 40.84 40.76 40.40 41.49 41.12
+ CL 58.38 57.86 58.22 57.70 56.18 55.68 55.65 55.16 53.90 53.42

P j:2 R P j:3 R P j:8 R P j:10 R P j:25 R
SLESK 58.18 57.66 57.20 56.69 54.67 54.19 53.28 52.81 50.47 50.02
+ PONDERE 56.67 56.17 55.49 54.99 51.08 50.63 49.25 48.81 44.39 44.00
+ CL 59.08 58.55 59.12 58.59 58.43 57.91 58.26 57.74 57.41 56.89

P j:2 R P j:3 R P j:8 R P j:10 R P j:25 R
BAYES 57.60 57.30 58.00 57.70 56.80 56.60 57.60 57.30 58.50 58.30

TAB. 1 — Precision et rappel des differents decideurs decrits dans leur variante DEF en fonction
de la taille du contexte. Le systeme BASE obtient une precision de 57.99 et un rappel de 57.62
(performance qui ne depend d’aucun parametre).

variantes LESK que SLESK. En revanche, le ﬁltrage du contexte a l’aide des chaines lexicales
permet d’augmenter les performances de toutes les variantes testees. Ceci semble attester qu’il
n’est pas souhaitable de considerer tous les mots lors d’une prise de decision. Les ameliorations
apportees par le ﬁltre CL aux variantes LESK sont particulierement marquees. Il est toutefois
surprenant qu’un tel ﬁltre porte ses fruits dans des contextes tres resserres autour du mot cible
(j:2). L’analyse que nous fournissons plus loin propose une explication des performances de
cette variante.

Nous pouvons egalement observer qu’a l’eXception des variantes LESK et BAYES, augmenter la
taille du contexte entraine une decroissance des performances, ce qui appuie l’importance de la
selection d’un bon contexte de desambigu'1'sation. Notons enﬁn que la meilleure des variantes
testees ici ne depasse pas de facon tres marquee le systeme BASE.

4.2.2 Choix de la description associée :‘1 un sens

Il est difﬁcile de degager des conclusions claires quant a l’inﬂuence du choix de la fonction
Description. Cependant la tendance la plus marquee (et ce pour toutes les conﬁgurations
testees) est que pour un contexte tres court (j:2), il est preferable de considerer les mots de la
deﬁnition des sens (DEF) que les relations (REL). Pour des contextes plus grands, il semble en
revanche que les relations donnent de meilleurs resultats. Il est cependant difﬁcile d’interpreter
cette observation sans recourir a une analyse plus poussee.

4.2.3 Analyse des réponses

La taxonomie que nous avons presentee en section 3 nous permet de comprendre davantage
les differentes variantes testees. Le tableau 2 reporte le risque positif (CE aé B ,E) et negatif
(5E aé B) des differentes variantes SLESK etudiees. Nous pouvons observer que, a l’eXception
des variantes CL, les decideurs prennent plus de risque negatif que positif, et ce d’autant plus
que le contexte est grand.

Pour toutes les variantes testees, le taux des reponses correctes differentes de BASE est tres petit.

Désambigufsation de corpus monolingues par des approches de type Lesk

i2 j:3 i8 j:10 i25
R+ R— R+ R— R+ R— R+ R— R+ R-
SLESK 3.5 3.3 3.9 4.7 6.0 9.3 6.5 11.2 7.8 15.3
+ PONDERE 3.5 4.8 3.9 6.4 5.9 12.8 6.4 15.2 7.8 21.3
+ CL 1.1 0.2 1.2 0.2 1.7 1.3 1.7 1.5 1.9 2.5

TAB. 2 — Risque positif (R+) et négatif (R—) des diffe’rentes Variantes SLESK. Les mesures
rapportées en italique indiquent des gains négatifs par rapport au systeme BASE.

La majorite’ des réponses correctes co'1'ncide en fait avec les réponses correctes de BASE; soit
qu’elles sont prises par défaut en l’absence de superposition (CE = B, le cas le plus fréquent),
soit qu’elles sont produites par des décisions effectives (CE = B). Dans le cas des Variantes
CL, les réponses correctes sont majoritairement des réponses non effectives: cette Variante doit
ses performances a une stratégie silencieuse (quelques décisions effectives prises a bon escient,
la plupart étant des décisions non effectives). Les autres Variantes prennent quant a elles plus
de risque et la proportion de bonnes réponses effectives est donc plus grande. Elles co'1'ncident
cependant majoritairement avec le choix du sens le plus fréquent.

4.2.4 Filtrage par étiquetage morpho-syntaxique

Nous avons étudie’ l’impact du ﬁltrage des sens candidats a l’aide de l’étiquette morpho—syntaxi—
que connue (APOS) ou estime’e (RALI) du mot cible. Dans le second cas, nous avons fait usage
d’un étiqueteur développe’ au RALI; un modele markovien d’ordre 3 entrainé sur le corpus
des débats parlementaires canadiens. Soulignons que ce corpus est par nature tres différent
du corpus de test de SENSEVAL2. L’étiquette était utilise’e comme ﬁltre. Par exemple selon
WORDNET, le mot anglais house contient 12 sens en tant que nom et seulement 2 sens en tant
que Verbe. Le fait de savoir (APOS) ou de croire (RALI) qu’on est par exemple en présence d’un
Verbe permet de ne conside’rer que 2 sens candidats. Les performances de cette approche sont
rapportées dans le tableau 3. Comme on peut le constater, la prise en compte de l’information
morpho—syntaxique (estime’e ou connue) améliore les performances de l’approche de base (sens
le plus fréquent). Nous n’obserVons cependant pas de gain lorsque l’approche de base béne’ﬁcie
également de cette information.

APOS P R RALI P R P R
SLESK+ CL 61.9 61.3 60.5 59.9 59.1 58.6
BASE 61.9 61.3 60.4 59.9 57.9 57.6

TAB. 3 — Précision et rappel de la meilleure Variante testée lorsque la catégorie morpho-
syntaxique du mot a de’sambigu'1'ser est connue (APOS) ou estime’e (RALI). La derniere colonne
rappelle les performances obtenues sans ce ﬁltrage.

5 Conclusion

Nous avons fait l’étude de diffe’rentes Variantes de l’algorithme de Lesk et tente’ d’analyser
leur performance grace a une taxonomie des réponses que nous avons décrite. Nos expériences

Florentina Vasilescu, Philippe Langlais

ont montre’ qu’en géne’ral les performances diminuent avec l’élargissement du contexte, les
meilleures performances étant enregistre’es par des fenétres de 4 a 6 mots pleins autour du mot
cible. Nous avons observe’ que ﬁltrer les mots du contexte (ici par les chaines lexicales) était
bénéﬁque (le décideur prend moins de risque négatif).

La catégorie grammaticale peut de plus agir comme un ﬁltre qui réduit le nombre de sens
candidats possibles d’un mot cible. Les performances obtenues en se servant d’une étiquette
morpho—syntaxique estime’e permettent de dépasser le meilleur des décideurs qui n’utilise pas
cette information. Rappelons que dans le cadre de SENSEVAL2 (English All Words), le meilleur
systeme supervise’ a obtenu une performance (précision et rappel) de 69% alors que le meilleur
systeme non supervise’ obtenait une précision de 57.5% et un rappel de 56.9%.

Les tentatives ici de’crites suggerent que les mots des de’ﬁnitions, des exemples d’usage et des
relations ne sont pas sufﬁsants pour une bonne désambigu'1'sation. Ceci rejoint les observations
de Véronis (2001) qui souligne que les de’ﬁnitions de WORDNET (et des dictionnaires en gene’-
ral) ne fournissent pas toujours l’information nécessaire a la désambigu'1'sation. L’ajout d’une
information de type syntaxique ou pragmatique relie’e a l’usage des mots dans des contextes
réels semble nécessaire a ce type de tache.

Références

AUDIBERT L. (2003). Etude des critéres de désambigisation sémantique autornatique: résultats sur les
cooccurrences. In I 0e confe’rence T ALN, p. 35-44, Batz—sur—mer, France.

BANERJEE S. & PEDERSEN T. (2003). Extended gloss overlaps as a measure of semantic related-
ness. In Eighteenth International Conference on Artiﬁcial Intelligence (IJCAI—03), p. 805-810, Aca-
pulco, Mexico.

CRESTAN E., EL—BEZE M. & DE LOUPY C. (2003). Peut—on trouver la taille de contexte optirnale en
désambiguisation sémantique ? In I 0e confe’rence T ALN, p. 85-94, Batz—sur—mer, France.

HIRST G. & ST—ONGE D. (1998). Lexical chains as representation of context for the detection and
correction of rnalapropisms. In C. FELLBAUM, Ed., WordNet.' An electronic lexical database and some
of its applications, p. 305-331. Cambrige, MA: The MIT Press.

KILGARRIFF A. & ROSENZWEIG J . (2000). Framework and results for English SENSEVAL. In Com-
puters and the Humanities, volume 34, p. 15-48. Kluwer.

LESK M. (1986). Automatic sense disambiguation using machine readable dictionaries: How to tell a
pine cone from an ice cream cone. In The Fifth International Conference on Systems Documentation,
ACM SI GDOC.

LITKOWSKI K. (2002). Sense information for disambiguation: Conﬂuence of supervised and unsuper-
vised methods. In SIGLEX/Senseval Workshop on Word Sense Disambiguation: Recent Successes and
Future Directions, Philadelphia.

SCHUTZE H. (1998). Automatic word sense discrimination. Computational Linguistics, 24(1), 97-123.
STEVENSON M. & WILKS Y. (2001). The interaction of knowledge sources in word sense disambigua-
tion. Computational Linguistics, 27(3), 321-351.

VASILESCU F. (2003). Désambiguisation de corpus monolingues par des approches de type Lesk. Mas-
ter’s thesis, Université de Montréal.

VERONIS J . (2001). Sense tagging: does it make sense ? In The Corpus Linguistics Conference, Lan-
caster, UK.

YAROWSKY D. (1995). Unsupervised word sense disambiguation rivaling supervised methods. In 33rd
Meeting of the Association for Computational Linguistics, p. 189-196.

