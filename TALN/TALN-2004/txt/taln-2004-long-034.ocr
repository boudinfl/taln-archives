TALN 2004, Fés, 19-21 avril 2004

Apprentissage partiel de grammaires catégorielles

Erwan Moreau
LINA — Université de Nantes
2 rue de la Houssiniere — BP 92208 — 44322 Nantes cedex 3
Erwan.Moreau@irin.univ—nantes.fr

Résumé - Abstract

Cet article traite de l’apprentissage symbolique de regles syntaxiques dans le modele de Gold.
Kanazawa a montre’ que certaines classes de grammaires catégorielles sont apprenables dans ce
modele. L’ al gorithme qu’il propose nécessite une grande quantité d’information en entre’e pour
étre efﬁcace. En changeant la nature des informations en entre’e, nous proposons un algorithme
d’apprentissage de grammaires catégorielles plus réaliste dans la perspective d’applications au
langage naturel.

This article deals with symbolic learning of syntactic rules in Gold’s model. Kanazawa showed
that some classes of categorial grammars are learnable in this model. But the algorithm needs a
high amount of information as input to be efﬁcient. By changing the kind of information taken
as input, we propose a learning algorithm for categorial grammars which is more realistic in the
perspective of applications to natural language.

Mots-clefs — Keywords

Apprentissage partiel, inférence grammaticale, grammaire catégorielles.
Partial learning, grammatical inference, categorial grammars.

1 Introduction

Malgre’ la facilité presque surprenante avec laquelle un enfant est capable d’acque’rir sa langue
maternelle, la réalisation d’un tel processus par la machine est un probleme tres difﬁcile. Ce
probleme de l’apprentissage automatique de grammaires consiste a découvrir les regles (syn-
taxiques) de formation des phrases d’un langage particulier. Plusieurs modeles de formalisation
de ce processus existent. Le modele de Gold (Gold, 1967) est celui que nous utilisons dans
cet article, et plus spéciﬁquement la me’thode propose’e par Buszkowski (Buszkowski & Penn,
1989) et géne’ralise’e par Kanazawa (Kanazawa, 1998) dans ce modele.

Le modele d’apprentissage propose’ par Gold est tres restrictif, c’est pourquoi les premiers re-
sultats obtenus avec ce modele ont été négatifs. Cependant Kanazawa a montre’ que certaines
classes de langages non triviales sont apprenables, en se servant de l’algorithme propose’ par

Erwan Moreau

Buszkoswki pour les grammaires catégorielles. Le mécanisme d’apprentissage propose’ est en-
tierement symbolique, c’est—a—dire qu’aucun traitement statistique n’est utilise’ : cela implique
que le risque d’erreur est réduit a zero, mais aussi que le bon fonctionnement de l’algorithme
d’apprentissage doit satisfaire des contraintes importantes.

L’une de ces contraintes qui font obstacle a l’application de cette méthode au langage naturel
concerne la nature des données dont l’algorithme a besoin en entre’e : il ne s’agit pas seulement
de phrases simples mais de structures particulieres, ce qui permet a l’algorithme d’étre deter-
ministe et efﬁcace. Ces structures sont une forme “d’arbre de dérivation appauvri” des phrases
conside’rées, dans le formalisme des grammaires catégorielles. La disponibilite’ de telles struc-
tures dans des cas réels (i.e. pas seulement sur des exemples jouets) est loin d’étre assure’e
pour deux raisons : d’une part le formalisme des grammaires catégorielles est tres peu utilise’
(en grande partie du fait de sa faible expressivité); d’autre part la connaissance de la gram-
maire sous—j acente est quasiment indispensable a la construction de ces structures, ce qui est un
handicap majeur puisque l’obj ectif est précise’ment de déduire cette grammaire.

Nous proposons ici un compromis, base’ sur la méthode de Kanazawa, qui conserve les avan—
tages de l’apprentissage symbolique tout en éliminant cette contrainte sur les structures. En
contrepartie, on conside’rera qu’une partie de la grammaire est de’ja connue, de maniere a rem-
placer l’information apportée par les structures par celle apporte’e par la grammaire initiale.
Cette hypothese est réaliste dans la perspective de l’application aux cas réels, du fait notamment
de la lexicalisation totale des grammaires catégorielles. L’inconve’nient étant que l’efﬁcacite’ de
l’apprentissage dépend désormais beaucoup de la grammaire initiale.

2 Apprentissage de grammaires catégorielles

2.1 Grammaires AB

Les grammaires catégorielles classiques, nomrne’es aussi grammaires AB, ont été introduites
dans (Bar—Hillel et al., 1960). Ces grammaires sont totalement lexicalise’es : cela signiﬁe
qu’une grammaire est décrite uniquement par son lexique, le lexique étant l’association d’une
ou plusieurs categories a chaque mot du vocabulaire. Les regles utilisées dans les derivations
sont donc universelles. Ces regles sont :

A / B, B —> A FA (Forward Application)
B, B\A —> A BA (Backward Application)

Les catégories sont des termes utilisant les opérateurs binaires / et  Intuitivement, une expres-
sion est de type A / B (resp. B\A) si cette expression est de type A lorsqu’elle est suivie (resp.
précéde’e) par une expression de type B. Une phrase est correcte s’il est possible d’associer
a chaque mot l’une de ses catégories, de telle sorte que les regles universelles permettent de
transformer cette sequence de catégories en la catégorie spéciale S’.

Exemple .' Soit G la grammaire constitue’e du lexique suivant :
{Pierre, Marie, Paul: SN; aime, déteste : (S'N\S')/SN; qui : (S'N\S'N)/(S'N\S') 

La phrase “Pierre, qui aime Marie, déteste Paul” appartient au langage de cette grammaire,
comme le montre la dérivation suivante :

A pprentissage partiel de grammaires categorielles

SN, (SN\SN)/(SN\S), (SN\S)/SN, SN, (SN\S)/SN, SN => SN, (SN\SN)/(SN\
S), (SN\S)/SN, SN, SN\S=> SN, (SN\SN)/(SN\S), SN\S, SN\S => SN, SN\
SN, SN\S => SN, SN\S => S

Une grammaire AB est rigide si chaque mot n’est deﬁni que par une seul categorie. De meme,
une grammaire est dite k—value’e si chaque mot est deﬁni par au plus 1: categories.

On peut decrire une derivation a l’aide d’un arbre de maniere classique, en etiquetant les nceuds
par la categorie du constituant qu’ils representent. De plus, la forme des regles permet aussi
de representer un arbre de derivation en etiquetant les nceuds seulement par l’identiﬁant de la
regle utilisee (FA ou BA). Une telle structure dans laquelle les feuilles ne sont etiquetees que
par un mot est appellee FA—slructure (pour F unct0r—Argument structure). Cette represention est
unique pour un arbre de derivation donne (voir ﬁgure 1). En revanche une FA—structure donnee
peut representer un nombre inﬁni d’arbres de derivations : dans la ﬁgure 1, on peut par exemple
remplacer tous les SN par des (X1 /X2)/ /X” et la FA—structure reste identique.

Pierre qm aime Marie déteste Paul Pierre qm
SN (SN\SN)/(SN\S) (SN\S)/SN SN (SN\S)/SN SN

VV1 

déteste Paul

SN\S SN\S _;

SN\SN

T8

SN BA

Figure 1: Arbre de derivation

2.2 L’alg0rithme RG

L’ al gorithme RG (pour Rigid Grammars), propose par Buszkowski (Buszkowski & Penn, 1989),
apprend la classe des grammaires AB rigides a partir de FA—structures dans le modele de Gold
(Gold, 1967). Dans ce modele, l’algorithme d’apprentissage doit deduire la grammaire a partir
d’une suite inﬁnie de phrases appartenant au langage genere par celle—ci (exemples positifs).
Apres chaque exemple, l’algorithme effectue une hypothese, en proposant une grammaire. Si
l’algorithme ne change plus d’hypothese a partir d’une certaine etape, alors celui—ci converge.
La grammaire—cible est correctement apprise si l’algorithme converge vers cette grammaire (ou
une qui lui soit equivalente). Une classe de grammaires est apprenable s’il existe un algorithme
qui, pour toute enumeration du langage engendre par une grammaire de cette classe, converge
vers cette derniere.

2.2.1 Algorithme

L’algorithme RG comporte deux etapes. La premiere consiste a construire une grammaire
ge’ne’rale a partir de la sequence de FA—structures fournies comme exemples :

Soit D = (T1, .., Tn) la sequence de FA—structures en entree.

Erwan Moreau

1. Etiquetage d’une structure Ti :

(a) la racine de Ti est étiquete’e par le type 5' (le type primitif qui caractérise les phrases
correctes).

(b) en allant de la racine Vers les feuilles, les ﬁls de chaque nteud t sont étiquetés de la
maniere suivante : une nouvelle variable as est crée’e, avec laquelle le nteud argument
est étiqueté. L’autre nmud est étiquete’ t/ac ou ac\t selon qu’il s’agit d’un nteud FA
ou BA. Le nteud argument est celui de la branche droite s’il s’agit d’un nteud FA,
gauche si c’est un nteud BA.

2. Soit (P1..Pn) l’ensemble des arbres de derivations construits par étiquetage des FA-
structures (T1..Tn). Pour chaque arbre Pi et chaque feuille de cet arbre, une regle w I—> t
est crée’e, ou w est le mot correspondant a cette feuille et 15 1e type obtenu apres étiquetage
pour cette feuille. La grammaire ainsi obtenue est la grammaire générale GF

La grammaire ainsi construite génere bien l’ensemble des FA—structures donne’ en entre’e, et
donc aussi les phrases de’crites par ces structures. Cependant il est evident que cette seule étape
ne sufﬁt pas a apprendre le langage décrit, puisque la taille de la grammaire Va augmenter
indéﬁniment, avec chaque nouvel exemple fourni a l’algorithme. C’est la raison pour laquelle
la seconde étape d ’umﬁcati0n doit transformer GF (D) en une grammaire rigide :

1. Pour chaque mot w déﬁni dans GF (D), soit Aw l’ensemble des types associés au mot w.
Soit A l’union de tous les Aw. Soit aii l ’umﬁeur le plus ge’ne’ral (MGU) de A : un uniﬁeur
de A est une substitution 0 telle que pour tout couple de types (151,152) de tout ensemble
Aw on a a(t1) = a(t2). Un uniﬁeur aii est le plus géne’ral si pour tout autre uniﬁeur 0 il
existe une substition 7' qui permet de passer de mi 51 0, i.e. 0 = 7' o aii (Voir par exemple
(Knight, 1989)).

2. On déﬁnit la grammaire RG(D) par RG(D) = aii[GF : toute regle w I—> t de
GF (D) est remplacée par w I—> aii(t) dans RG(D). Comme tous les types d’un meme
mot ont été uniﬁes, la grammaire obtenue est rigide.

Cet algorithme a quelques qualite’s inte’ressantes : il est tout d’abord efﬁcace, puisque sa com-
plexite’ (en fonction de la taille des exemples) est seulement quadratique. Il peut étre utilise’
de maniere incrémentale, ainsi il n’est pas nécessaire de recalculer la grammaire a partir de
l’ensemble des phrases a chaque nouvel exemple. Enﬁn il produit une unique grammaire solu-
tion (la plus générale) quel que soit l’ensemble d’eXemples propose’s.

2.2.2 Exemple

On considere l’ensemble D de FA—structures représente’es (apres étiquetage des nteuds) sur la
ﬁgure 2. La phase d’étiquetage permet d’obtenir la grammaire générale GF (D) suivante :

Pierre I—) X1,X4,X9

Marie n—> X3,Xg

Paul |—) X2 X6

GFD = . ’
( ) 2111116 |—> (X3\S)[X4,X7[Xg

déteste |—> 
qlll |—) 

A pprentissage partiel de grammaires catégorielles

Paul qui aime Marie déteste Pierre
Xe (X6\X5)/X7 X7/X8 X8 (X5\S)/X9 X9

Pierre déteste Paul Marie aime Pierre V% %/
X1 (X1\S')/X2 X2 X3 (X3\S)/X4 X4
FA

X-, X5\S
W W
X1 \S X3\S X6\X5
BA
BA BA
X5
3 3
BA

Figure 2: FA—structures apres étiquetage

Le calcul du MGU uniﬁe les types suivants : X1 = X4 = X9, X3 = X8, X2 = X6, X7 =
(X3\S), X4 = X8, X1 = X5, X2 = X9, C6  dOl'll'l6I X1 = X2 = X3 = X4 = X5 = X6 =
X8 = X9 et X7 = X1\S'. Donc la grammaire RG(D) est:

Pierre n—> X1
Marie n—> X1
_ Paul n—> X1
RG(D) _ aime n—> (X1\S')/X1
déteste n—> (X1 \S')/X1
qui '—> (X1\X1)/(X1\5)

2.3 Apprentissage a partir de phrases plates

L’algorithme RG présente peu d’inte’rét, d’une part a cause de la nature des informations qu’il
nécessite, et d’autre part a cause de la faible expressivite’ des grammaires rigides. Mais Kanazawa
a aussi montre’ que la classe des grammaires AB k—Value’es est apprenable (au sens de Gold)
a partir de “phrases plates” (ﬂat strings, i.e. phrases sans FA—structure) (Kanazawa, 1998).
L’algorithme qu’il propose consiste en une réduction au cas de l’apprentissage de grammaires
rigides a partir de FA—structures. En effet, pour une phrase donne’e de longueur n, le nombre
de FA—structures possibles est borné (mais grand !), puisqu’il s’agit d’arbres binaires de n 1
nccuds, chaque nmud étant étiquete’ soit par FA soit par BA. De maniere similaire, le nombre
de grammaires k—Value’es possibles est borne’ pour une grammaire GF (D) donne’e (on considere
tous les uniﬁeurs k—partiels au lieu de l’unique MGU).

Dans ce cas, l’ensemble des solutions est calculable mais la complexite’ de l’algorithme devient
exponentielle. Costa—Floréncio montre que le probleme qui consiste a apprendre la classe des
grammaires k—Value’es (pour k > 1) a partir de FA—structures ainsi que celui qui consiste a
apprendre la classe des grammaires rigides a partir de phrases plates sont des problemes NP—durs
(Costa Floréncio, 2001), (Costa Floréncio, 2002). Nicolas a implémente’ et testé l’algorithme de
Kanazawa pour ce cas, et obtient par exemple 126775 grammaires solutions en ﬁxant seulement
la Valeur 2 a k, pour de petits exemples (Nicolas, 1999).

Erwan Moreau

3 Apprentissage partiel de grammaires rigides

L’application des algorithmes d’apprentissage de grammaires catégorielles rigides se heurte
donc au probleme classique du rapport entre quantité d’information en entre’e et efﬁcacite’ de
l’algorithme : soit l’algorithme est polynomial mais nécessite des FA—structures trop complexes,
soit l’algorithme n’utilise que des phrases plates mais est alors exponentiel, donc tout aussi peu
utilisable dans des applications réelles.

3.1 Méthode

Nous proposons ici un compromis dans lequel les informations que constituent les FA—structures
sont remplace’es par celles fournies par une grammaire initiale, de maniere a maintenir un
niveau acceptable d’efﬁcacite’. Comme les grammaires catégorielles sont totalement lexical-
isées, l’hypothese qu’une partie de la grammaire a apprendre soit connue au depart est réaliste
: en effet, cette partie est simplement constitue’e d’un ensemble de mots auxquels sont associés
un ensemble de types.

Le programme Prolog ci—contre est a la (400 f /)

_ _ 2-0 ,X X, .
fo1s un parser de gramma1res AB, un al— :_°g(400,xfx,\)_
gorithme d’apprentissage de grammaires

_ _ _ , _ regle(A/B,B,A). % Forward Application (FA)
rlgldes et un algorlthme d apprentlssage regle(B,B\A,A). % Backward Application (BA)
par_t{e1' cet algonthme nalf’ qul repose couper_liste(Partl, Part2, Liste) :-
ent1erement sur le moteur Prolog pour la append(Partl, Part2, Liste),
recherche d’une de’riVation et/ou le calcul   H’

des types inconnus du lexique, illustre bien
2- 1 - - _ deriv(Lexiq'ue, [Mot], T) :-
l importance de l uniﬁcation dans le pro member(def(M°t’T)’ Lexique)_

cessus de derivation des grammaires caté- deriv(Lexiq'ue, Constituant, T) :-
couper_liste(Cl, C2, Constituant),

gOr1eHeS' deriv(Lexiq'ue, Cl, Tl),
On peut noter que cet algorithme termine deriv(I-exique, C2, T2) .
- 1 b d h, regle(Tl,T2,T).
toujours, car e nom re e parent esages
d’une phrase en constituants (non Vides) exempletx) =- % ‘Marie’ de type incormu =
, - \ L = % ' x =
est borne. Cependant 11 est tres peu efﬁ— ex éeﬂ .Pierre.fe;1:;Te Sn
cace, surtout dans le cas ou un grand nom— def( 'aime' . (sn\s)/Sn) .
, . . def ‘Marie’ X
bre de mots sont deﬁnis dans le lexique : 1, ( ' )
tous les parenthe’sages sont teste’s jusqu’a del-”iV(1-ex: I ‘Pierre’: ‘aim’: ‘Marie’ 1: 5):
deriv(Lex, [ ‘Marie’, 'aime', ‘Pierre’ ], s).

ce que celui qui correspond a la forme des
types soit trouve’.

Aﬁn d’optimiser l’utilisation des informations fournie dans la grammaire initiale, on peut procéder

de la meme maniere que pour l’analyse syntaxique (parsing) : chercher tous les types possibles
pour tous les constituants complets (i.e. ne contenant aucun mot inconnu). Selon la proportion
de mots inconnus dans la phrase et leur répartition, cela permet de limiter l’eXplosion combi-
natoire dﬁe aux diffe’rentes structures possibles. L’algorithme propose’ ci—dessous utilise une
méthode de parsing incrémental de type CYK, de maniere a guider la construction de l’arbre de
de’riVation par les types fournis dans la grammaire initiale.

A pprentissage partiel de grammaires cate’gon'e11es

3.2 Algorithme

L’ al gorithme RGPL (Rigid Grammars Partial Learning) prend en entre’e une phrase wl, .., wn et
une grammaire initiale G0, compose’e de regles associant un type a un mot, de la forme w I—> t.
11 renvoie l’ensemble des grammaires rigides solutions, c’est—a—dire celles contenant les regles
de la grammaire initiale et acceptant cette phrase.

RGPi_(G0, [’LU1,’LU2,..,’LUn])
Lea: <— {(W, T) | (W »—> T) 6 G0} % Initialisation
créer une matrice vide M[1..n,1..n]
pour i <— 1 a n faire
si HT tel que (w,-,T) e Lea: alors
Mimi <— {(T,Id)}
sinon
créer une nouvelle variable V
Lea: <— Lea: U  V)}
Mimi <— {(V, Id)}
fin si
fin pour
pour i <— 2 a n faire % Processus de derivation/apprentissage partiel
pourj <—'i 1 a 1 faire
pour k <—j as 1 faire
pour chaque (ﬂag) e M[j, k] faire
pour chaque (Tr, ar) 6 M[k + 1, i] faire
Si Elau = m_qu(a;,aT) alors
créer deux nouvelles variables A, B
Si EIUFA : mgu({{Uu   {O-u(T7‘)a  alors
M[j, <— M[j, U {(0pA(A),apA 0 an o 01}
fin si
créer deux nouvelles variables A’, B’
Si EIUBA : 'n7*.qu({{0'u(Tl)aB,}a {O-u(TT)aB,\A,}}) alors
M[j, <— M[j, U {(0BA(A’),0BA 0 an o 01}
fin si
fin si
fin pour
fin pour
fin pour
fin pour
fin pour
Res <— (Z) % Application des substitutions compatibles
pour chaque (T, 0) e M[1,n] faire
si Hr tel que r(T) = S alors
Res <— Res U {(7 o 0) (Lea:)}
fin si
fin pour
renvoyer Res
Fin RGPL

Remarques: Id désigne la substitution identité, et (an o 01) = (an o 02) car an est déﬁni comme
le MGU de 01 et 02.

Erwan Moreau

Cet algorithme ne décrit le fonctionnement que pour une phrase : le processus d’apprentissage
sur un ensemble de phrases consiste a appliquer RGPL sur chaque phrase puis calculer le MGU
(s’il existe) sur chaque ensemble de solutions, comme dans l’algorithme de Kanazawa.

De meme que dans l’algorithme d’apprentissage a partir de FA—structures, cet algorithme utilise
des termes qui peuvent contenir des Variables. Ces Variables sont progressivement instanciées
selon les contraintes imposées par les regles universelles, en particulier dans le cas ou le type
est combine avec un type sans Variable. Chaque type “réalisable” par un constituant est ac-
compagné de la substitution qui permet de l’obtenir. Cette substitution porte sur les Variables
associées aux mots inconnus de ce constituant, aﬁn de Veriﬁer lors de chaque combinaison de
types que leurs substitutions sont compatibles (par calcul du MGU).

Il est important de noter que malgré son fonctionnement “de type CYK” la complexite’ de cet
algorithme n’est pas polynomiale en general. En effet, le nombre de couples (T, 0) dans chaque
case de la matrice M peut croitre de maniere exponentielle. C’est notamment le cas lorsque
tous les mots sont inconnus (la grammaire initiale est Vide) : on se trouve alors dans le cas
préce’dent d’apprentissage a partir de phrases plates, et l’algorithme doit calculer l’ensemble
des FA—structures possibles.

3.3 Exemple

Soit G0 la grammaire initiale déﬁnie par le lexique suivantl
{un I—> SN/N, homme I—> N, poisson I—> N, nage I—> S'N\S', vite I—> (S'N\S')\(S'N\S')}

Soit “un homme court” la phrase donne’e comme entre’e a l’algorithme, ou “court” est un mot
inconnu.

M[1,1] <— (SN/N, (2))
1. Initialisation .' M [2, 2] <— (N, 0)
M[3, 3] <— ($1, 0) et Le$ <— Le$ U {(c0m't, $1)}

2': 2,3‘ = 1, k = 1: M[1, 2] <— (SN, (2)) (FA)
2'= 3,j — 2, k = 2: JV./[2, 3] <— ($2, {$1 I—> N\$2}) (BA)
2. Dérivation .' 2': 3,j = 1, k = 1: M[1, 3] <— (SN, {$1 I—> N\N}) (FA)
2'— 3,3’ = ,k = 1: M[1, 3] <— ($3, {$1 n—> N\((S'N/N)\$3)}) (BA)
2 — 3,j = ,k = 2: M[1, 3] <— ($4, {$1 I—> S'N\$4)}) (BA)

7'(S'N) = S’ ? impossible
3. Substitutions compatibles avec S .' 7'($3) = S’ ? {$1 I—> N\((S'N/N)\S')}
7'($4) = S’ ? {$1 I—> S'N\S'}

Apres cet exemple, il y a deux grammaires dans l’ensemble des solutions : l’une déﬁnit “court”
par le type N \ ((S'N/N)\S'), l’autre par le type SN \S'. Si l’eXemple “un homme court vite”
apparait plus tard dans la sequence d’eXemples, la premiere solution sera e’limine’e, car il n’eXiste
pas de substitution sur les regles universelles permettant de combiner N \ ((S'N/N) \S') avec le
type de l’adVerbe vite, (S'N\S') \ (S'N\S').

1On peut noter dans cette grammaire que le type des noms N est un argument du type du déterrninant SN / N ,
et non l’inverse: il s’agit de la notation classique dans les grammaires catégorielles.

A pprentissage partie] de grammaires catégorielles

3.4 Discussion et perspectives
La contrainte de rigidité

L’algorithme RGPL présente’ ci—dessus apprend uniquement des grammaires rigides. Plus ex-
actement, les mots déﬁnis dans la grammaire initiale peuvent avoir plusieurs catégories, mais
l’algorithme uniﬁe tous les types d’un méme mot inconnu. ll ne serait bien sﬁr pas difﬁcile
de gérer des grammaires k—Value’es en calculant toutes les possibilités d’uniﬁcation, mais cela
serait contraire a l’objectif puisque l’algorithme deviendrait rapidement inexploitable. On peut
par contre envisager pour pallier ce probléme que des classes de mots soit déﬁnies dans la gram-
maire initiale : chaque mot inconnu devrait alors appartenir a l’une des classes prédéﬁnies, ce
qui limite fortement les combinaisons de types. Le regroupemement par classes est fre’quem—
ment utilise’ (par exemple dans (Brill, 1993)), mais suppose que les classes prédéﬁnies cou-
Vrent l’ensemble des combinaisons syntaxiques suceptibles d’apparaitre dans les exemples, et
ne causent pas de surgénération.

Formalisme

L’ étude réalisée dans cet article porte uniquement sur les grammaires AB, la forme la plus sim-
ple de grammaires catégorielles. Ce formalisme est plutot pauvre en termes de représentation
des phénoménes linguistiques, c’est pourquoi il est souhaitable d’étendre la méthode proposée
a des cadres plus adaptés au langage naturel. ll existe différents travaux qui tendent a mon-
trer que le type d’apprentissage propose’ par Kanazawa est généralisable, notamment a d’autres
formes de grammaires catégorielles telles les grammaires de Lambek et minimalistes (Bonato
& Retore’, 2001) (méme si cela pose d’autres problémes de complexite’, Voir (Pentus, 2003)).
D’autres formalismes, comme les grammaires de liens (Sleator & Temperley, 1991), sont aussi
suceptibles de permettre ce type d’apprentissage.

La grammaire initiale

L’inte’rét de la méthode d’apprentissage que nous proposons repose entiérement sur l’eXistence
d’une grammaire initiale. Celle—ci doit étre sufﬁsamment complete pour qu’un nombre signi-
ﬁcatif de mots soient connus dans les phrases de la séquence a apprendre. Dans ce cadre, on
peut tirer proﬁt de la “loi de Zipf”, utilisée notamment par l’étiqueteur de Brill (Brill, 1993).
Celle—ci garantit que, sur l’ensemble des mots d’un texte, une faible proportion des mots sufﬁt
a représenter une grande partie du texte (en nombre d’occurences). Or précisément ce sont les
mots les plus fréquents d’un langage qui sont le plus facile a répertorier et déﬁnir (mots gram-
maticaux tels que déterminants, pronoms, prépositions, conjonctions, etc.), soit manuellement
soit par conversion de dictionnaires existants dans d’autres formalismes.

4 Conclusion

Dans le domaine de l’inférence grammaticale, l’apprenabilite’ des classes de langages est sou-
Vent étudiée indépendamment d’une éventuelle mise en pratique de cet apprentissage. Du strict

Erwan Moreau

point de vue de l’apprenabilite’ dans le modele de Gold, aucune distinction n’est nécessaire en-
tre les cas d’apprentissage de grammaires rigides a partir de FA—structures et de grammaires
k—value’es a partir de phrases plates, meme si le premier est peu utile et le second quasiment
irréalisable.

Dans la perspective d’applications “réelles”, nous avons propose’ une adaptation des algorithmes
d’apprentissage de grammaires catégorielles. L’approche étudie’e est moins contraignante sur
la forme des entre’es de l’algorithme, tout en restant relativement réaliste du point de vue de
l’efﬁcacite’. Néanmoins il reste plusieurs questions a re’soudre avant de pouvoir appliquer des al-
gorithmes d’apprentissage symbolique de regles syntaxiques au langage naturel : l’eXpressivite’
des classes de langages apprenables ainsi que le formalisme utilise’ pour les représenter doivent
étre améliore’s (les pronoms, par exemple, sont difﬁcilement représentables dans les grammaires
AB), et il reste a determiner comment et selon quels criteres constituer la grammaire initiale.

Références

BAR—HILLEL Y., GAIFMAN C. & SHAMIR E. (1960). On categorial and phrase structure grammars.

BONATO R. & RETORE C. (2001). Learning rigid lambek grammars and minimalist grammars from
structured sentences. In L. POPELINSKY & M. NEPIL, Eds., Proceedings of the 3d Workshop on Learn-
ing Language in Logic, p. 23-34, Strasbourg, France.

BRILL E. (1993). A Corpus—Based Approach to Language Learning. PhD thesis, Computer and Infor-
mation Science, University of Pennsylvania.

BUSZKOWSKI W. & PENN G. (1989). Categorial grammars determined from linguistic data by uniﬁ-
cation. Rapport interne TR—89—05, Department of Computer Science, University of Chicago.

COSTA FLORENCIO C. (2001). Consistent Identiﬁcation in the Limit of the Class k—valued is NP—hard.
In P. DE GROOTE, G. MORRILL & C. RETORE, Eds., Logical Aspects of Computational Linguistics,
4th International Conference, LACL 2001, Le Croisic, France, June 27-29, 2001, Proceedings, volume
2099 of Lecture Notes in Computer Science, p. 125-138: Springer—Verlag.

COSTA FLORENCIO C. (2002). Consistent Identiﬁcation in the Limit of Rigid Grammars from Strings is
NP—hard. In P. ADRIAANS, H. FERNAU & M. VAN ZAANEN, Eds., Grammatical Inference: Algorithms
and Applications 6th International Colloquium: ICGI 2002, volume 2484 of Lecture Notes in Artiﬁcial
Intelligence, p. 49-62: Springer—Verlag.

GOLD E. (1967). Language identiﬁcation in the limit. Information and control, 10, 447-474.
KANAZAWA M. (1998). Learnable classes of categorial grammars. Cambridge University Press.
KNIGHT K. (1989). Uniﬁcation: A multidisciplinary survey. ACM Computing Surveys, 21(1), 93-124.

NICOLAS J. (1999). Grammatical inference as uniﬁcation. Rapport interne 3632, INRIA. également
rapport IRISA PI1265.

PENTUS M. (2003). Lambek calculus is NP—complete. Rapport interne TR—2003005, CUNY Ph.D.
Program in Computer Science.

SLEATOR D. D. K. & TEMPERLEY D. (1991). Parsing English with a Link Grammar. Rapport interne
CMU—CS —TR—91—126, Carnegie Mellon University, Pittsburgh, PA.

