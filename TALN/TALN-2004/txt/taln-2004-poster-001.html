<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
"http://www.w3.org/TR/html4/loose.dtd">
<html><head><title>Mots compos&#233;s dans les mod&#232;les de langue pour la recherche d&#8217;information</title>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
</head>
<body>
<div style="page-break-before:always; page-break-after:always"><div><p>TALN 2004, Session Poster, F&#232;s, 19&#8211;21 avril 2004
</p>
<p>Mots compos&#233;s dans les mod&#232;les de langue pour la recherche
d&#8217;information
</p>
<p>Carmen Alvarez, Philippe Langlais et Jian-Yun Nie
RALI/IRO, Universit&#233; de Montr&#233;al
CP. 6128, succursale Centre-ville
</p>
<p>Montr&#233;al, Qu&#233;bec, H3C 3J7 Canada
{bissettc,felipe,nie}@iro.umontreal.ca
</p>
<p>R&#233;sum&#233; - Abstract
</p>
<p>Une approche classique en recherche d&#8217;information (RI) consiste &#224; b&#226;tir une repr&#233;sentation des
documents et des requ&#234;tes bas&#233;e sur les mots simples les constituant. L&#8217;utilisation de mod&#232;-
les bigrammes a &#233;t&#233; &#233;tudi&#233;e, mais les contraintes sur l&#8217;ordre et l&#8217;adjacence des mots dans ces
travaux ne sont pas toujours justifi&#233;es pour la recherche d&#8217;information. Nous proposons une
nouvelle approche bas&#233;e sur les mod&#232;les de langue qui incorporent des affinit&#233;s lexicales (ALs),
c&#8217;est &#224; dire des paires non ordonn&#233;es de mots qui se trouvent proches dans un texte. Nous
d&#233;crivons ce mod&#232;le et le comparons aux plus traditionnels mod&#232;les unigrammes et bigrammes
ainsi qu&#8217;au mod&#232;le vectoriel.
</p>
<p>Previous language modeling approaches to information retrieval have focused primarily on sin-
gle terms. The use of bigram models has been studied, but the restriction on word order and
adjacency may not be justified for information retrieval. We propose a new language modeling
approach to information retrieval that incorporates lexical affinities (LAs), or pairs of words that
occur near each other, without a constraint on word order. We explore the use of LAs in a lan-
guage modeling approach, and compare our results with the vector space model, and unigram
and bigram language model approaches.
</p>
<p>Mots-clefs &#8211; Keywords
</p>
<p>Mod&#232;les de langue, recherche d&#8217;information, mots compos&#233;s
Language models, information retrieval, compound terms, word pairs</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Carmen Alvarez, Philippe Langlais, et Jian-Yun Nie
</p>
<p>1 Introduction
</p>
<p>L&#8217;utilisation des mod&#232;les de langue en RI a &#233;t&#233; introduite par Ponte et Croft (1998). Chaque
document est consid&#233;r&#233; comme un &#233;chantillon d&#8217;un langage particulier, et un mod&#232;le de langue
est entra&#238;n&#233; pour chaque document. Pour une requ&#234;te donn&#233;e, les documents sont tri&#233;s par ordre
d&#233;croissant de la probabilit&#233; que le mod&#232;le du document g&#233;n&#232;re la requ&#234;te. Cette approche
donne des performances comparables, voire sup&#233;rieures, au mod&#232;le vectoriel.
</p>
<p>Repr&#233;senter un document par un mod&#232;le de langue repr&#233;sente un certain nombre de d&#233;savan-
tages dont le principal est le probl&#232;me ici aigu de la sous-repr&#233;sentation des donn&#233;es d&#8217;entra&#238;-
nement. Entra&#238;ner un mod&#232;le de langue (m&#234;me un simple unigramme) sur des documents qui
contiennent quelques centaines de mots repr&#233;sente en effet un certain d&#233;fi. Ainsi, Song et Croft
(1999) &#233;tudient diff&#233;rentes techniques de lissage connues comme le lissage Good-Turing ou
la combinaison lin&#233;aire de plusieurs mod&#232;les n-grammes d&#8217;ordres diff&#233;rents. Hiemstra (2002)
propose une technique d&#8217;interpolation o&#249; un mod&#232;le unigramme du document et un mod&#232;le de
corpus sont combin&#233;s. Lavrenko et Croft (2001) font &#233;galement usage d&#8217;une combinaison de
mod&#232;les de documents et d&#8217;un mod&#232;le de corpus pour estimer un mod&#232;le de pertinence (prob-
abilit&#233; qu&#8217;un mot soit pertinent pour une requ&#234;te), sans n&#233;cessiter de donn&#233;es d&#8217;entra&#238;nement
sp&#233;cifiques.
</p>
<p>Nous proposons une approche bas&#233;e sur l&#8217;entra&#238;nement de mod&#232;les de langues qui incorporent
des paires de mots non d&#233;finies par des contraintes d&#8217;adjacence ou d&#8217;ordonnancement: les
affinit&#233;s lexicales (ALs). Nous commen&#231;ons par pr&#233;senter en section 2 le mod&#232;le unigramme
que nous utilisons &#224; des fins comparatives dans nos exp&#233;riences. Nous d&#233;crivons ensuite en
section 3 une proc&#233;dure initialement propos&#233;e par Maarek et al. (1991) qui permet d&#8217;obtenir
les ALs d&#8217;un document et pr&#233;sentons en section 4 un mod&#232;le de langue faisant usage de ces
affinit&#233;s. Nous d&#233;crivons ensuite en section 5 le cadre exp&#233;rimental qui nous a permis d&#8217;&#233;tudier
le comportement des diff&#233;rents mod&#232;les d&#233;crits et discutons nos r&#233;sultats dans la section 6. Nous
montrons en particulier qu&#8217;un mod&#232;le de langue unigramme liss&#233; rivalise avec l&#8217;approche clas-
sique du mod&#232;le vectoriel et que la prise en compte des affinit&#233;s lexicales am&#233;liore de mani&#232;re
sensible les performances.
</p>
<p>2 Mod&#232;les n-gramme sur les mots simples
</p>
<p>Le score de pertinence d&#8217;un document d pour une requ&#234;te de N mots q = wN1 = w1, . . . , wN est
donn&#233; par la probabilit&#233; que le mod&#232;le du document g&#233;n&#232;re la requ&#234;te. Dans le cas d&#8217;un mod&#232;le
n-gramme pnd , cette pertinence s&#8217;exprime simplement par l&#8217;&#233;quation 1 o&#249; n repr&#233;sente l&#8217;ordre
du mod&#232;le:
</p>
<p>score(d, q) =
N&#8719;
</p>
<p>i=1
</p>
<p>pnd(wi|wi&#8722;1i&#8722;n+1) (1)
</p>
<p>R&#233;aliser un syst&#232;me de RI &#224; l&#8217;aide de mod&#232;les de langue peut se r&#233;sumer dans sa forme la plus
simple &#224; entra&#238;ner autant de mod&#232;les que de documents. La probabilit&#233; qu&#8217;un mot w d&#8217;une
requ&#234;te soit g&#233;n&#233;r&#233; par le mod&#232;le de langue d&#8217;un document peut alors &#234;tre estim&#233;e par le max-
imum de vraisemblance (MLE), ce qui revient dans le cas unigramme &#224; calculer la fr&#233;quence
relative de w dans le document d.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Mots compos&#233;s dans les mod&#232;les de langue pour la recherche d&#8217;information
</p>
<p>L&#8217;estimateur &#224; maximum de vraisemblance directement inject&#233; dans l&#8217;&#233;quation 1 s&#8217;av&#232;re en
pratique tr&#232;s peu utile: les documents qui ne contiennent pas l&#8217;ensemble des mots de la requ&#234;te
se voient attribuer un score de pertinence nul. Ce probl&#232;me bien connu en mod&#233;lisation de
la langue (le lissage) est ici particuli&#232;rement &#233;pineux puisque les documents que nous traitons
contiennent environ 200 &#224; 400 mots.
</p>
<p>La probl&#233;matique du lissage a &#233;t&#233; et continue &#224; &#234;tre un objet d&#8217;investigation scientifique et de
nombreuses techniques ont &#233;t&#233; propos&#233;es pour l&#8217;entra&#238;nement de mod&#232;les de langue &#224; partir de
grands corpus de textes (Goodman, 2001). Nous &#233;tudions dans Alvarez et al. (2003) diff&#233;rentes
techniques de lissage sp&#233;cifiques &#224; l&#8217;entra&#238;nement de mod&#232;les de langue pour la RI et rapportons
ici les configurations pour lesquelles nous avons observ&#233; les meilleurs r&#233;sultats. Il convient de
noter que les documents ainsi que les requ&#234;tes sont soumis &#224; un pr&#233;-traitement qui consiste en
une lemmatisation et en la suppression de mots apparaissant dans une stopliste1.
</p>
<p>Dans le cas d&#8217;un mod&#232;le de langue unigramme, nous combinons lin&#233;airement le mod&#232;le MLE
avec un mod&#232;le de corpus selon l&#8217;&#233;quation 2. Ce dernier est un mod&#232;le unigramme MLE sur
l&#8217;ensemble des documents de la collection. Dans le cas d&#8217;un mod&#232;le bigramme, nous combinons
lin&#233;airement le mod&#232;le MLE du document avec le mod&#232;le unigramme selon l&#8217;&#233;quation 3:
</p>
<p>punid(w) = &#955;1pMLEd(w) + (1&#8722; &#955;1)pcorpus(w) (2)
</p>
<p>pbid(wi|wi&#8722;1) = &#955;2pMLEd(wi|wi&#8722;1) + (1&#8722; &#955;2)punid(wi) (3)
</p>
<p>3 Affinit&#233;s lexicales
</p>
<p>L&#8217;hypoth&#232;se d&#8217;ind&#233;pendance entre mots, faite par le mod&#232;le unigramme, ainsi qu&#8217;une grande
partie des approches &#224; la RI, n&#8217;est pas toujours justifi&#233;e. Les mod&#232;les bigramme (et &#224; fortiori
les mod&#232;les d&#8217;ordre sup&#233;rieur) tentent en effet de rendre compte des d&#233;pendances entre termes;
tout en supposant que l&#8217;ordre des mots est important. Tandis que cette derni&#232;re hypoth&#232;se
semble raisonnable pour des applications comme la reconnaissance de parole, elle ne s&#8217;applique
pas n&#233;cessairement &#224; la RI. Par exemple, pour une requ&#234;te &#8220;apartment rentals&#8221;, un document
contenant les termes &#8220;rent an apartment&#8221; ne doit pas &#234;tre &#224; priori moins bien class&#233; qu&#8217;un autre
document contant les termes &#8220;apartments for rent&#8221;. Notre r&#233;ponse &#224; ce probl&#232;me consiste &#224;
baser notre mod&#233;lisation sur une unit&#233; lexicale n&#8217;imposant aucune restriction sur l&#8217;ordre de ses
mots et peu de contrainte sur leur adjacence: l&#8217;affinit&#233; lexicale.
</p>
<p>Selon Martin et al. (1983), 98% des relations lexicales dans un texte mettent en jeu des mots
dans une fen&#234;tre de 5 mots. Nous adoptons cette propri&#233;t&#233; pour identifier les unit&#233;s (paires de
mots) sur lesquelles b&#226;tir nos mod&#232;les de langue. Par ailleurs, Maarek et al. (1991) introduisent
le concept de pouvoir de r&#233;solution d&#8217;une paire de mots. &#192; l&#8217;instar des facteurs tf et idf utilis&#233;s
dans le mod&#232;le vectoriel, l&#8217;id&#233;e principale derri&#232;re le pouvoir de r&#233;solution est que les paires de
mots qui caract&#233;risent le mieux un document sont celles qui ont en m&#234;me temps une fr&#233;quence
&#233;lev&#233;e dans le document et une fr&#233;quence relativement basse dans la collection. Les auteurs
sugg&#232;rent de calculer le pouvoir de r&#233;solution d&#8217;une paire &lt; u, v &gt; pour un document d selon
l&#8217;&#233;quation 4; o&#249; cd(&lt; u, v &gt;) est la fr&#233;quence de la paire dans le document d. Le terme logarith-
</p>
<p>1Une liste de 571 mots anglais fournie avec le syst&#232;me SMART a &#233;t&#233; utilis&#233;e.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Carmen Alvarez, Philippe Langlais, et Jian-Yun Nie
</p>
<p>mique dans cette &#233;quation peut &#234;tre vu comme une approximation de la quantit&#233; d&#8217;information
v&#233;hicul&#233;e par la paire, comparable au facteur idf.
</p>
<p>&#961;d(&lt; u, v &gt;) = &#8722;cd(&lt; u, v &gt;)&#215; log(pcorpus(u)&#215; pcorpus(v)) (4)
Le pouvoir de r&#233;solution de toutes les paires de mots distants d&#8217;au plus cinq mots (pleins) dans
un document est calcul&#233;. Il est important de noter que les paires &lt; u, v &gt; sont stock&#233;es par
ordre lexicographique (la paire &#8220;traduction automatique&#8221; vue dans un texte est trait&#233;e comme
&#8220;automatique,traduction&#8221;). La table 1 montre les cinq meilleurs affinit&#233;s lexicales de deux
documents de notre collection.
</p>
<p>AP900302-10 AP900427-3
AL (&lt; u, v &gt;) fr&#233;quence &#961;d AL (&lt; u, v &gt;) fr&#233;quence &#961;d
court supreme 7 43,6 union violence 6 37,7
court property 6 38,3 greyhound violence 5 37,5
public sidewalk 4 30,5 greyhound union 5 34,6
court night 5 29,7 member union 6 34,2
court justice 4 24,6 condone violence 4 33,4
</p>
<p>Table 1: Les 5 meilleures ALs selon le pouvoir de r&#233;solution pour deux documents de la collec-
tion TREC AP90: le document AP900302-10 traitant du 200&#232; anniversaire de la cour supr&#234;me
des &#201;tats-Unis et le document AP900427-3 traitant des syndicats de la soci&#233;t&#233; Greyhound.
</p>
<p>4 MLA: Un mod&#232;le de langue bas&#233; sur les affinit&#233;s lexicales
</p>
<p>Notre mod&#232;le MLA est un mod&#232;le unigramme qui estime les probabilit&#233;s des mots simples et
des paires de termes du document. Pour ce faire, on introduit les comptes d&#233;crits en &#233;quation 5;
desquels on obtient la probabilit&#233; de chaque &#233;v&#233;nement p(w&#8242;) (w&#8242; &#233;tant un mot ou une paire) en
les normalisant par la constante D =
</p>
<p>&#8721;
w&#8712;d cd(w)+
</p>
<p>&#8721;
&lt;u,v&gt;&#8712;d &#946;&#961;d(&lt; u, v &gt;). Cette approche est
</p>
<p>conceptuellement &#233;quivalente &#224; l&#8217;ajout dans chaque document du compte fractionnaire (contr&#244;l&#233;
par &#946;d, fixe pour l&#8217;ensemble des documents) des affinit&#233;s lexicales du document.
</p>
<p>c&#1;d(w
&#8242;) =
</p>
<p>{
cd(w) si w&#8242; est un mot simple w
&#946;d&#961;d(&lt; u, v &gt;) sinon
</p>
<p>(5)
</p>
<p>Ce mod&#232;le est &#224; son tour liss&#233; par le mod&#232;le de corpus d&#233;crit dans la section 2, o&#249; les probabilit&#233;s
pcorpus(w
</p>
<p>&#8242;) se basent sur les comptes c&#1;corpus(w
&#8242;), contr&#244;l&#233;s par un facteur &#946;corpus.
</p>
<p>pMLA(w
&#8242;) = &#955;MLA
</p>
<p>c&#1;(w&#8242;)
D
</p>
<p>+ (1&#8722; &#955;MLA)pcorpus(w&#8242;)) (6)
</p>
<p>Si le m&#234;me traitement est appliqu&#233; &#224; la requ&#234;te q (en contr&#244;lant les comptes fractionnaires par
un coefficient &#946;q, fixe pour toutes les requ&#234;tes), le score de pertinence est alors:
</p>
<p>score(d, q) =
&#8719;
</p>
<p>w&#8242;&#8712;q
pMLAd (w
</p>
<p>&#8242;)c
&#1;
q(w
</p>
<p>&#8242;) (7)</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Mots compos&#233;s dans les mod&#232;les de langue pour la recherche d&#8217;information
</p>
<p>5 Exp&#233;riences
</p>
<p>Nous avons &#233;tudi&#233; le comportement des diff&#233;rents mod&#232;les pr&#233;sent&#233;s sur la collection TREC
AP90, qui contient 78 321 documents en anglais de l&#8217;Associated Press newswire de 1990. 53
requ&#234;tes &#233;tiquet&#233;es manuellement pour les pistes translinguistiques des campagnes TREC-6
et TREC-7 constituaient notre corpus de test (une moyenne de 22 documents pertinents sont
associ&#233;s &#224; chaque requ&#234;te). Chaque requ&#234;te comporte un champ titre de 1 &#224; 5 mots (2.5 en
moyenne) ainsi qu&#8217;un champ description contenant de 3 &#224; 19 mots (7 en moyenne).
</p>
<p>Chaque exp&#233;rience comprend deux tests. L&#8217;un d&#233;not&#233; TITRE consiste &#224; n&#8217;utiliser que le champ
titre d&#8217;une requ&#234;te, l&#8217;autre d&#233;not&#233; DESC utilise les champs titre et description. La t&#226;che du
syst&#232;me consiste &#224; classer par ordre d&#233;croissant de pertinence les documents de la collection
pour chaque requ&#234;te. La m&#233;trique d&#8217;&#233;valuation que nous utilisons est la pr&#233;cision moyenne
habituellement utilis&#233;e dans ce type de t&#226;che et qui mesure la moyenne de pr&#233;cision obtenue
sur plusieurs points de rappel (Salton &amp; McGill, 1983), pour les 1000 premiers documents
retrouv&#233;s par le syst&#232;me. Afin de comparer nos diff&#233;rents mod&#232;les &#224; un syst&#232;me &#233;prouv&#233;, nous
avons utilis&#233; le syst&#232;me SMART qui impl&#233;mente le mod&#232;le vectoriel classique (Buckley, 1985).
</p>
<p>La table 2 montre les performances du mod&#232;le MLA. Les pr&#233;cisions indiqu&#233;es en gras corre-
spondent &#224; des variantes dont les performances mesur&#233;es d&#233;passent le mod&#232;le unigramme et
le syst&#232;me SMART. Notre mod&#232;le d&#233;passe le mod&#232;le unigramme pour plusieurs combinaisons
de &#946;d et &#946;q. Pour les requ&#234;tes TITRE, le gain relatif mesur&#233; le plus important est de 2.0%. La
meilleure pr&#233;cision moyenne obtenue avec les requ&#234;tes DESC (43.20) est sup&#233;rieure au mod&#232;le
unigramme (42.75), soit un gain relatif de 1.0%.
</p>
<p>&#955;MLA 0.1 0.2 0.4 0.5 0.6 Smart 1-gram 2-gram
&#946;d,&#946;q,&#946;corpus
</p>
<p>TITRE .01,.01,.01 39.20 40.77 40.87 41.43 41.13 33.49 40.71 40.72
.01,.01,.0001 39.39 40.92 40.95 41.54 41.39 33.49 40.71 40.72
</p>
<p>DESC .01,.005,.01 41.18 43.20 42.66 42.29 41.96 34.98 42.75 42.77
.04,.005,.0001 40.95 43.14 42.74 42.39 42.00 34.98 42.75 42.77
</p>
<p>Table 2: Pr&#233;cision moyenne obtenue par le mod&#232;le MLA en fonction des m&#233;ta-param&#232;tres &#955;MLA
et &#946;d pour le document, &#946;q pour la requ&#234;te et &#946;corpus pour le mod&#232;le de corpus. Les r&#233;sultats qui
sont significatifs, selon le test de Wilcoxon des rangs sign&#233;s avec un intervalle de confiance de
95%, sont indiqu&#233;s en italique.
</p>
<p>6 Discussion
</p>
<p>Il existe plusieurs travaux qui tentent d&#8217;am&#233;liorer les performances d&#8217;un syst&#232;me de recherche
d&#8217;information bas&#233; sur les mots simples. En particulier, Nie et Dufort (2002) &#233;tudient l&#8217;ajout
de termes compos&#233;s comme de nouveaux indices dans le mod&#232;le vectoriel. Ils montrent que
l&#8217;adjonction de termes en provenance des bases terminologiques (Termium2 et la Banque de
terminologie du Qu&#233;bec3) permet d&#8217;am&#233;liorer les performances d&#8217;une t&#226;che de RI si ces termes
</p>
<p>2http://www.termium.com
3http://www.olf.gouv.qc.ca/ressources/bibliotheque/dictionnaires/Internet/Index/</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Carmen Alvarez, Philippe Langlais, et Jian-Yun Nie
</p>
<p>sont incorpor&#233;s de mani&#232;re ad&#233;quate.
</p>
<p>Dans cette &#233;tude, nous montrons qu&#8217;il est possible d&#8217;am&#233;liorer (certes de mani&#232;re modeste)
les performances d&#8217;un mod&#232;le de langue s&#8217;il fait usage d&#8217;affinit&#233;s lexicales. Ce type d&#8217;unit&#233;
est intuitivement attirant en RI car il ne poss&#232;de pas la rigidit&#233; des s&#233;quences de mots. Nous
observons que l&#8217;augmentation en performance due aux ALs est plus marqu&#233;e pour les requ&#234;tes
courtes (TITRE). Les requ&#234;tes plus longues (DESC) contiennent en moyenne 18 paires. Bien que
n&#8217;ayant pas men&#233; d&#8217;analyse syst&#233;matique, nous pensons que dans le cas de requ&#234;tes longues,
plusieurs ALs ne sont pas pertinentes.
</p>
<p>Les perspectives que cette &#233;tude sugg&#232;re sont multiples. Les mod&#232;les que nous proposons
sont r&#233;gis par plusieurs param&#232;tres (&#955;1, &#955;2, &#955;MLA , &#946;d) que nous avons fix&#233;s empiriquement.
Une approche plus syst&#233;matique nous permettrait d&#8217;ajuster ces param&#232;tres et d&#8217;en augmenter
le nombre. Il est en effet intuitif de penser que le poids donn&#233; &#224; un mod&#232;le donn&#233; devrait
&#224; tout le moins &#234;tre conditionn&#233; par la taille du document trait&#233;. Nous souhaitons &#233;galement
&#233;tudier l&#8217;impact de differentes techniques de filtrage des ALs, notamment gr&#226;ce &#224; un &#233;tiqueteur
morpho-syntaxique.
</p>
<p>R&#233;f&#233;rences
</p>
<p>ALVAREZ C., LANGLAIS P. &amp; J.Y-NIE (2003). Word Pairs in Language Modeling for Information
Retrieval. Rapport interne, RALI.
</p>
<p>BUCKLEY C. (1985). Implementation of the SMART information retrieval system. Rapport interne,
Cornell University. Technical report 35-686.
</p>
<p>GOODMAN J. (2001). A bit of progress in language modeling. Computer Speech and Language, p.
403&#8211;434.
</p>
<p>HIEMSTRA D. (2002). Term-specific smoothing for the language modeling approach to information
retrieval: the importance of a query term. In 25th annual international ACM SIGIR conference on
Research and Development in Information Retrieval, p. 35&#8211;41, Tampere, Finland.
</p>
<p>LAVRENKO V. &amp; CROFT W. B. (2001). Relevance-based language models. In 24th annual international
ACM SIGIR conference on Research and Development in Information Retrieval, p. 120&#8211;127.
</p>
<p>MAAREK Y., BERRY D. &amp; KAISER G. (1991). An information retrieval approach for automatically
constructing software libraries. IEEE transactions on software engineering, p. 800&#8211;813.
</p>
<p>MARTIN W., AL B. &amp; VAN STERKENBURG P. (1983). On the processing of a text corpus: From textual
data to lexicographical information. In E. R.R.K. HARTMANN, Ed., Lexicography: Principles and
Practice, Applied Language Studies Series. Academic Press, London.
</p>
<p>NIE J.-Y. &amp; DUFORT J. (2002). Combining words and compound terms for monolingual and cross-
language information retrieval. In Information 2002.
</p>
<p>PONTE J. M. &amp; CROFT W. B. (1998). A language modeling approach to information retrieval. In 21st
annual international ACM SIGIR conference on Research and Development in Information Retrieval, p.
275&#8211;281, Melbourne, Australia.
</p>
<p>SALTON G. &amp; MCGILL M. J. (1983). Introduction to Modern Information Retrieval. New York:
McGraw Hill.
</p>
<p>SONG F. &amp; CROFT W. B. (1999). A general language model for information retrieval. In 22nd annual
international ACM SIGIR conference on Research and Development in Information Retrieval, p. 279&#8211;
280.</p>

</div></div>
</body></html>