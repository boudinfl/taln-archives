<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
"http://www.w3.org/TR/html4/loose.dtd">
<html><head><title>Traduction de dialogue: r&#233;sultats du projet NESPOLE! et pistes pour le domaine</title>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
</head>
<body>
<div style="page-break-before:always; page-break-after:always"><div><p>TALN 2004, Session Poster, F&#232;s, 19-21 avril 2004
</p>
<p>Traduction de dialogue: r&#233;sultats du projet NESPOLE! et pistes
pour le domaine
</p>
<p>Herv&#233; Blanchon, Laurent Besacier
</p>
<p>Laboratoire CLIPS
BP 53
</p>
<p>38041 Grenoble Cedex 9
{prenom.nom}@imag.fr
</p>
<p>R&#233;sum&#233;
Dans cet article, nous d&#233;taillons les r&#233;sultats de la seconde &#233;valuation du projet europ&#233;en
NESPOLE! auquel nous avons pris part pour le fran&#231;ais. Dans ce projet, ainsi que dans ceux qui
l&#8217;ont pr&#233;c&#233;d&#233;, des techniques d&#8217;&#233;valuation subjectives &#8212; r&#233;alis&#233;es par des &#233;valuateurs humains
&#8212; ont &#233;t&#233; mises en &#339;uvre. Nous pr&#233;sentons aussi les nouvelles techniques objectives &#8212;
automatiques &#8212; propos&#233;es en traduction de l&#8217;&#233;crit et mises en &#339;uvre dans le projet C-STAR III.
Nous conclurons en proposant quelques id&#233;es et perspectives pour le domaine.
</p>
<p>Mots Cl&#233;s
Traduction de dialogue, &#233;valuation subjective et objective de composants de TALN
Introduction
Le projet NESPOLE! [Lazzari G., 2000] visait &#224; capitaliser les efforts des partenaires europ&#233;ens et
am&#233;ricains du consortium C-STAR II et aller plus loin en termes scientifiques. Les langues
impliqu&#233;es sont l&#8217;italien, le fran&#231;ais, l&#8217;allemand et l&#8217;anglais.
</p>
<p>Les d&#233;monstrateurs NESPOLE! mettent en situation de dialogue un agent touristique italophone et
un client parlant anglais, fran&#231;ais ou allemand. Une importance particuli&#232;re a &#233;t&#233; donn&#233;e &#224;
l&#8217;&#233;valuation des deux d&#233;monstrateurs produits (2001 et 2002). Ces &#233;valuations ont &#233;t&#233; conduites
avec des &#233;valuateurs humains qui jugent la qualit&#233; de traduction du syst&#232;me en comparant
l&#8217;&#233;nonc&#233; source et l&#8217;&#233;nonc&#233; cible produit. On parle d&#8217;&#233;valuation subjective. Ces &#233;valuations ont
permis de mesurer &#224; la fois des performances brutes, ainsi que les progr&#232;s accomplis.
</p>
<p>Dans le domaine de la traduction de l&#8217;&#233;crit, afin de diminuer le co&#251;t de l&#8217;&#233;valuation, la m&#233;thode
BLEU [Papineni K., et al., 2002] a d&#8217;abord &#233;t&#233; propos&#233;e puis raffin&#233;e ensuite sous d&#8217;autres
noms. Il s&#8217;agit ici de comparer automatiquement la sortie du syst&#232;me &#224; une traduction &#233;talon
&#233;ventuellement compl&#233;t&#233;e par un ensemble de paraphrases. On parle d&#8217;&#233;valuation objective. Le
domaine de la traduction de dialogue a adopt&#233; r&#233;cemment les techniques d&#8217;&#233;valuation objectives
qui obligent &#224; produire plusieurs paraphrases d&#8217;une traduction &#233;talon.
</p>
<p>Dans cet article, nous d&#233;taillons d&#8217;abord les r&#233;sultats de la seconde &#233;valuation du projet
NESPOLE!. Nous pr&#233;sentons ensuite, en formulant quelques remarques, les nouvelles techniques
objectives propos&#233;es en traduction de l&#8217;&#233;crit. Nous commentons aussi les r&#233;sultats d&#8217;une
premi&#232;re exp&#233;rience pilote en &#233;valuation objective r&#233;alis&#233;e dans le cadre de C-STAR III. Nous
concluons avec quelques id&#233;es et perspectives plus g&#233;n&#233;rales pour le domaine de la traduction de
parole.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Herv&#233; Blanchon, Laurent Besacier
</p>
<p>1 &#201;valuation du D&#233;monstrateur en &#171; tourisme &#233;tendu &#187; (2002)
Les r&#233;sultats de l&#8217;&#233;valuation du premier d&#233;monstrateur en &#171; tourisme restreint &#187; ont &#233;t&#233; pr&#233;sent&#233;s
en d&#233;tail dans [Rossato S., et al., 2002].
</p>
<p>1.1 Donn&#233;es et protocole
Deux dialogues extraits de la seconde collecte NESPOLE! [Mana N., et al., 2003] ont &#233;t&#233; utilis&#233;s.
Ces dialogues couvrent des sc&#233;narios complexes non couverts par le premier d&#233;monstrateur,
&#233;v&#233;nements culturels, ch&#226;teaux, lacs et forfaits. Pour l&#8217;italien, il s&#8217;agit de tours de paroles d&#8217;un
agent de voyage, pour les trois autres langues, ce sont ceux d&#8217;un client.
</p>
<p>Les signaux recueillis servent d&#8217;entr&#233;e aux modules de reconnaissance vocale. Les transcriptions
manuelles de ces signaux servent de r&#233;f&#233;rence pour la traduction (elles simulent une
reconnaissance sans erreur). Les tours de parole sont segment&#233;s en unit&#233;s s&#233;mantiques de
dialogue (SDU1). Apr&#232;s avoir appliqu&#233; les modules de reconnaissance et/ou de traduction sur ces
donn&#233;es, des &#233;valuateurs humains jugent, pour chaque tour de parole transcrit, la qualit&#233; de la
traduction de chaque SDU au sein du tour. L&#8217;&#233;valuation est faite au niveau des SDU car elles
repr&#233;sentent un &#233;l&#233;ment atomique de la t&#226;che. Ne pas faire la traduction correcte de l&#8217;une d&#8217;entre
elles au sein d&#8217;un tour de parole ne signifie pas que tout le tour de parole soit mal traduit et que
la t&#226;che ne converge pas vers son objectif.
Les diff&#233;rentes classes d&#8217;&#233;valuation r&#233;alis&#233;es sont les m&#234;mes que celles de la premi&#232;re campagne
soit : &#233;valuation de la reconnaissance de la parole (Word Accuracy Rate et hypoth&#232;se comme
paraphrase2), et &#233;valuation des traductions monolingues et bilingues sur les r&#233;f&#233;rences et sur les
hypoth&#232;ses du module de reconnaissances. Afin de mesurer les progr&#232;s accomplis, nous avons
aussi utilis&#233;, sur ces m&#234;me donn&#233;es, les analyseurs et g&#233;n&#233;rateurs d&#233;velopp&#233;s pour le premier
d&#233;monstrateur pour les configurations monolingues et bilingues sur les transcriptions
uniquement (Sur Refs (01) dans la Table 1).
Nous avons chang&#233; la proc&#233;dure d&#8217;&#233;valuation sur plusieurs points. Nous avons abandonn&#233;
l&#8217;&#233;chelle &#224; trois valeurs utilis&#233;e lors de la premi&#232;re &#233;valuation. Les &#233;valuateurs choisis pour cette
&#233;valuation sont des &#233;l&#232;ves de derni&#232;re ann&#233;e d&#8217;&#233;cole de traduction (DESS pour le fran&#231;ais). Lors
de la premi&#232;re &#233;valuation, les &#233;valuateurs n&#8217;avaient pas de formation sp&#233;cifique en traduction et
les groupes n&#8217;&#233;taient pas homog&#232;nes en terme de niveau en seconde langue.
</p>
<p>La premi&#232;re &#233;chelle de notation comportait les valeurs BAD, OK et PERFECT. Les &#233;valuateurs
devaient d&#8217;abord v&#233;rifier que le sens &#233;tait pr&#233;serv&#233; (note BAD sinon). Puis, lorsque le sens &#233;tait
pr&#233;serv&#233;, ils devaient dire si la traduction &#233;tait grammaticale ou non (PERFECT vs. OK). Or,
l&#8217;environnement d&#8217;&#233;valuation pr&#233;sentait aux &#233;valuateurs les trois options en m&#234;me temps, il est
possible qu&#8217;ils aient &#233;t&#233; pouss&#233;s &#224; choisir la note m&#233;diane (OK). La premi&#232;re question &#233;tait aussi
s&#233;v&#232;rement interpr&#233;t&#233;e (toute sorte de perte de sens rangeait la SDU dans la cat&#233;gorie BAD).
Nous avons donc utilis&#233; une &#233;chelle de quatre valeurs fond&#233;e uniquement sur la pr&#233;servation du
sens : VERY GOOD (toutes les informations sont pr&#233;sentes et faciles &#224; comprendre), GOOD (toutes
les informations importantes sont pr&#233;sentes), BAD (une ou plusieurs informations importantes
ont &#233;t&#233; omises), VERY BAD (les informations importantes sont presque toutes absentes).
                                                
</p>
<p>1
 Une SDU est un segment de tour de parole de longueur maximale qui peut &#234;tre cod&#233; par une seule
</p>
<p>repr&#233;sentation dans le pivot IF que nous utilisons dans NESPOLE! Voici, un exemple de d&#233;coupage en SDU,
s&#233;par&#233;es par des # d&#8217;un tour de parole : &#171; bonjour madame # j aimerais organiser une semaine de vacances
dans un parc # et je voudrais aussi une chambre simple &#224; cavalese du quinze au vingt septembre &#187;.
</p>
<p>2
 Le WAR ne prend pas en compte le fait que certaines erreurs de reconnaissance peuvent avoir des cons&#233;quences
</p>
<p>plus ou moins importantes sur la qualit&#233; de la traduction produite par le syst&#232;me. Ainsi, nous avons aussi
v&#233;rifi&#233; si la sortie du module de reconnaissance peut &#234;tre, ou non, consid&#233;r&#233;e comme une paraphrase de la
transcription manuelle du signal.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Traduction de dialogue: r&#233;sultats du projet NESPOLE! et pistes pour le domaine
</p>
<p>Finalement, pour pouvoir comparer les r&#233;sultats de cette &#233;valuation &#224; la pr&#233;c&#233;dente, les notes
VERY GOOD et GOOD ont &#233;t&#233; additionn&#233;es comme ACCEPTABLE. Les r&#233;sultats complets de cette
seconde &#233;valuation sont donn&#233;s Table 1.
</p>
<p>1.2 R&#233;sultats
Reconnaissance WAR 58 56 51 76
</p>
<p>Hypos comme paraphase 60 67 62 76
Traduction monolingue (ACCEPTABLE) FRA-FRA ENG-ENG GER-GER ITA-ITA
Sur Refs (01) / sur Refs (02) / Hypos (02) 69 / 77 / 58 68 / 68 / 50 45 / 61 / 51 36 / 51 / 42
Traduction Bilingue (ACCEPTABLE) FRA-ITA ENG-ITA GER-ITA
Sur Refs (01) / sur Refs (02) / Hypos (02) 72 / 78 / 58 64 / 70 / 50 44 / x / x
</p>
<p>ITA-FRA ITA-ENG ITA-GER
Sur Refs (01) / sur Refs (02) / Hypos (02) 19 / 37 / 33 33 / 33 / 30 38 / 45 /38
</p>
<p>Table 1 : R&#233;sultats de la seconde campagne d'&#233;valuation Nespole!
</p>
<p>1.3 Commentaires
Pour le fran&#231;ais, on atteint, en monolingue sur les hypoth&#232;ses, un taux de 58% de traductions
acceptables alors que l&#8217;on pouvait esp&#233;rer 60%. Vers l&#8217;italien, les r&#233;sultats sont cette fois
meilleurs que depuis l&#8217;anglais. Enfin, depuis l&#8217;italien, le g&#233;n&#233;rateur vers le fran&#231;ais affiche
maintenant des performances comparables aux g&#233;n&#233;rateurs vers l&#8217;anglais et l&#8217;allemand.
</p>
<p>Les r&#233;sultats produits dans Verbmobil lors de l&#8217;&#233;valuation de masse [Tessiore L. and Hahn W.,
2000] pour un taux de reconnaissance inf&#233;rieur &#224; 75% sont de 66% en allemand-anglais et de
58% en anglais allemand. Les r&#233;sultats obtenus avec notre second d&#233;monstrateur sont du m&#234;me
ordre.
</p>
<p>1.4 Mesure des progr&#232;s accomplis
En traduction vers l&#8217;italien, les trois syst&#232;mes produisent un taux de plus de 30% de traductions
acceptables. Ces taux ne semblent pas montrer de gros progr&#232;s par rapport &#224; la premi&#232;re
&#233;valuation. Cependant, si on compare les taux d&#8217;acceptabilit&#233; sur les r&#233;f&#233;rences italiennes avec
les modules des premier et second d&#233;monstrateurs, on se rend compte que, pour le fran&#231;ais, le
g&#233;n&#233;rateur progresse de 18%, le g&#233;n&#233;rateur allemand de 7%, le g&#233;n&#233;rateur anglais restant stable.
</p>
<p>En ce qui concerne la traduction monolingue ou bilingue du c&#244;t&#233; client, on observe que les taux
de traduction sont sup&#233;rieurs ou &#233;gaux &#224; 50% sur les hypoth&#232;ses et sup&#233;rieurs &#224; 70% sur les
r&#233;f&#233;rences. Ces taux sont en augmentation de six &#224; dix points pour le fran&#231;ais par rapport aux
modules de la premi&#232;re &#233;valuation.
</p>
<p>Les tours de parole de l&#8217;agent de voyage (italien) sont devenus plus complexes dans le second
d&#233;monstrateur. Avec un taux d&#8217;hypoth&#232;ses paraphrasant l&#8217;entr&#233;e de 76%, le pourcentage de
traductions acceptables en italien semble &#234;tre nettement inf&#233;rieur aux r&#233;sultats monolingues des
autres langues. Une &#233;tude plus fine des donn&#233;es permet d&#8217;expliquer ce r&#233;sultat.
</p>
<p>Les d&#233;veloppeurs des syst&#232;mes ont class&#233; les SDU des dialogues d&#8217;&#233;valuation en trois cat&#233;gories :
SDU couvertes par le premier d&#233;monstrateur (classe 1), SDU couvertes par le second d&#233;monstrateur
uniquement (classe 2), SDU hors du domaine (classe 3). Nous avons calcul&#233; les performances des
syst&#232;mes sur ces trois groupes s&#233;par&#233;ment. Pour le fran&#231;ais, l&#8217;anglais et l&#8217;allemand (client),
seulement 5% des SDU appartiennent aux classe 2 et 3. Cependant, pour l&#8217;italien (agent), 13%
sont dans la seconde cat&#233;gorie et 25% dans la troisi&#232;me.
Les diff&#233;rences de performance apportent des r&#233;ponses int&#233;ressantes. Sur le premier groupe, on
observe une am&#233;lioration des performances de 56.6% pour le premier d&#233;monstrateur &#224; 63.2%
pour le second d&#233;monstrateur. Ce qui montre une am&#233;lioration du second d&#233;monstrateur dans la</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Herv&#233; Blanchon, Laurent Besacier
</p>
<p>couverture du domaine du premier d&#233;monstrateur. Sur les donn&#233;es du groupe 2, le premier
d&#233;monstrateur atteint seulement 14.2% de traduction acceptable alors que le second
d&#233;monstrateur atteint un score de 38.4%. Le premier d&#233;monstrateur n&#8217;&#233;tant pas pr&#233;par&#233; pour ce
type de tour de parole cela n&#8217;est pas surprenant. Bien que le second d&#233;monstrateur ait des
performances bien sup&#233;rieures au premier, celui-ci n&#8217;atteint cependant pas un niveau comparable
aux performances atteintes sur le domaine du premier d&#233;monstrateur.
</p>
<p>L&#8217;analyseur de l&#8217;italien ne couvre pas, bien s&#251;r, les SDU hors du domaine. Lorsqu&#8217;on les exclue,
les performances du syst&#232;me italien passent de 49.3% pour le premier d&#233;monstrateur &#224; 58.9
pour le second. Ces r&#233;sultats sont alors comparables &#224; ceux des autres syst&#232;mes monolingues.
De m&#234;me, sans les SDU du groupe 3, les performances des autres syst&#232;mes bilingues depuis
l&#8217;italien sont accrues de 9 points.
</p>
<p>2 Vers des &#233;valuations communes sur un m&#234;me corpus
Nous pr&#233;sentons maintenant les tendances actuelles, et leurs limites, en &#233;valuation objective.
Nous &#233;voquons aussi la proposition du consortium C-STAR III pour des &#233;valuations comp&#233;titives
sur un m&#234;me corpus.
</p>
<p>2.1 Tendances actuelles
Le co&#251;t important de l&#8217;&#233;valuation subjective a motiv&#233; le passage vers des protocoles d&#8217;&#233;valuation
objectifs (automatiques). Plusieurs m&#233;triques ont &#233;t&#233; propos&#233;es. Les plus utilis&#233;es sont BLEU
[Papineni K., et al., 2002] et NIST [Doddington G., 2002] qui calculent des distances statistiques
sur des ensembles de n-grammes entre la traduction produite par le syst&#232;me et des paraphrases
d&#8217;une traduction de r&#233;f&#233;rence.
</p>
<p>Le score BLEU est constitu&#233; de deux composants une pr&#233;cision modifi&#233;e sur les n-grammes
(calcul&#233;e pour chaque traduction) et une p&#233;nalit&#233; pour les traductions plus courte que les
r&#233;f&#233;rences (calcul&#233;e sur tout le corpus). Avec NIST, un poids plus important est donn&#233; aux n-
grammes les plus longs. De plus, le poids d&#8217;un n-gramme est calcul&#233; en fonction de sa valeur
informationnelle. Plus un n-gramme est pr&#233;sent dans les sorties, plus son poids est faible.
</p>
<p>La communaut&#233; pratique aussi des &#233;valuations en WER (taux d&#8217;erreur mesur&#233; sur un alignement
entre une sortie et une r&#233;f&#233;rence), inspir&#233;e directement du domaine de la reconnaissance
automatique de la parole, et en MWER (WER sur des r&#233;f&#233;rences multiples), ou en PER (WER
ind&#233;pendant de la position des mots) ou en MPER (PER sur des r&#233;f&#233;rences multiples). Le lecteur
peut consulter [Sugaya F., et al., 2001] pour plus d&#8217;informations.
</p>
<p>2.2 Limites
Les promoteurs et les utilisateurs de telles techniques n&#8217;oublient pas que, dans l&#8217;absolu, les
chiffres produits par les m&#233;thodes automatiques ne veulent rien dire. De nombreuses &#233;tudes
essaient d&#8217;&#233;tablir une corr&#233;lation entre les r&#233;sultats d&#8217;une &#233;valuation objective et ceux d&#8217;une
&#233;valuation subjective [Coughlin D., 2003, Doddington G., 2002, Papineni K., et al., 2002].
Cependant, de notre point de vue, si cette corr&#233;lation est utile pour v&#233;rifier les progr&#232;s r&#233;alis&#233;s au
cours du d&#233;veloppement, elle ne r&#233;pond pas &#224; la question de l&#8217;utilisabilit&#233;.
</p>
<p>En effet, nous savons qu&#8217;un taux de 100% de traductions acceptables ou un score de un &#224; une
&#233;valuation BLEU ne seront pas atteints avant longtemps, si ce n&#8217;est jamais, dans des domaines
assez largement couverts. Il serait donc important de savoir &#224; partir de quelles performances un
syst&#232;me devient utile et utilisable. On pourrait ainsi essayer de corr&#233;ler les r&#233;sultats d&#8217;&#233;valuation
objective avec l&#8217;utilisabilit&#233;. Les &#233;valuations actuelles ne traitent pas de ce sujet.
Du point de vue de leur mise en &#339;uvre pratique, les m&#233;thodes automatiques induisent aussi
quelques questions essentielles. Ainsi, on ne trouve pas dans la litt&#233;rature d&#8217;instructions pour la
fabrication des r&#233;f&#233;rences. On sait bien que la &#171; typologie &#187; des r&#233;f&#233;rences doit &#234;tre la plus</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Traduction de dialogue: r&#233;sultats du projet NESPOLE! et pistes pour le domaine
</p>
<p>proche possible de la &#171; typologie &#187; des sorties du syst&#232;me &#224; &#233;valuer. En cons&#233;quence, la
comparaison de plusieurs syst&#232;mes sur des m&#234;mes donn&#233;es de test peut devenir d&#233;licate si la
collection de paraphrases de chaque r&#233;f&#233;rence n&#8217;est pas assez exhaustive.
</p>
<p>2.3 &#201;valuations sur un m&#234;me corpus (C-STAR III)
Afin de pr&#233;parer une campagne d&#8217;&#233;valuation ouverte sur le corpus BTEC [Takezawa T., et al.,
2002], une premi&#232;re exp&#233;rience pilote interne a &#233;t&#233; conduite en 2003. Cette &#233;valuation concerne
uniquement la traduction textuelle. Les donn&#233;es de d&#233;veloppement et de test des diff&#233;rents
syst&#232;mes utilisaient BTEC et d&#8217;autres ressources monolingues. 500 phrases anglaises traduites
dans toutes les langues sources ont &#233;t&#233; utilis&#233;es comme donn&#233;es de test.
</p>
<p>Nous avons &#233;valu&#233; cinq syst&#232;mes depuis le chinois, l&#8217;italien, et le japonais vers l&#8217;anglais en
proc&#233;dant, d&#8217;une part, &#224; une &#233;valuation subjective des r&#233;sultats et d&#8217;autre part, &#224; une &#233;valuation
objective en utilisant BLEU et NIST. L&#8217;&#233;valuation subjective a &#233;t&#233; conduite selon les
recommandations publi&#233;es par le Linguistic Data Consortium pour l&#8217;&#233;valuation du projet TIDES
de la DARPA3. Les scores BLEU et NIST ont &#233;t&#233; calcul&#233;s sur les sorties brutes des syst&#232;mes.
</p>
<p>&#192; l&#8217;examen des r&#233;sultats produits, nous avons rencontr&#233; une inconsistance entre les scores pour
un syst&#232;me class&#233; 3i&#232;me par BL E U et 5i&#232;me par NIST. Les sorties de ce syst&#232;me &#233;taient
significativement plus courtes que les r&#233;f&#233;rences, ce qui influe fortement le calcul du score. &#192; ce
d&#233;tail pr&#232;s, les classements des syst&#232;mes pour chacune des &#233;valuations sont homog&#232;nes.
</p>
<p>Nous avons aussi observ&#233; des diff&#233;rences importantes dans la forme des sorties des diff&#233;rents
syst&#232;mes : usage de majuscules, rendu des num&#233;raux (lettres, chiffres s&#233;par&#233;s ou non),
abr&#233;viations, mots compos&#233;s (avec ou sans tirets), ponctuation, etc. Nous avons montr&#233; que cela
induit des diff&#233;rences de &#177;0,15 points pour BLEU et &#177;1,8 pour NIST, ce qui n&#8217;est pas n&#233;gligeable.
Pour les exp&#233;riences futures, il faudra donc normaliser les sorties des diff&#233;rents syst&#232;mes
comme cela est d&#233;j&#224; le cas dans la communaut&#233; du traitement de la parole.
Afin de mobiliser la communaut&#233; du domaine et d&#8217;apporter quelques r&#233;ponses aux questions
que nous posons dans les sections 2.2 et 3, le consortium C-STAR III organise un atelier satellite
&#224; la conf&#233;rences ICSLP-20044. Dans ce cadre, le consortium diffusera une partie du corpus
BTEC afin que des membres ext&#233;rieurs puissent &#233;valuer leurs syst&#232;mes sur nos donn&#233;es.
</p>
<p>3 Pistes pour le futur
Pour aller plus loin, il nous semble bien s&#251;r important et n&#233;cessaire de r&#233;fl&#233;chir &#224; l&#8217;&#233;valuation en
proposant un cadre qui r&#233;ponde non seulement aux besoins des d&#233;veloppeurs, mais aussi aux
besoins des utilisateurs. Il nous para&#238;t aussi n&#233;cessaire de r&#233;fl&#233;chir de nouveau au contexte de
nos travaux : nous faisons de la traduction de dialogue. Si dans le cadre du projet Verbmobil ce
contexte particulier a &#233;t&#233; exploit&#233;, il nous semble que dans les travaux post&#233;rieurs et actuels, les
syst&#232;mes propos&#233;s n&#8217;en font plus usage. Chaque tour de parole est traduit pour lui m&#234;me
comme s&#8217;il n&#8217;&#233;tait pas &#233;nonc&#233; dans un contexte (celui du dialogue). Les architectures mises en
&#339;uvre sont exclusivement des architectures pipe-line dans lesquelles les diff&#233;rents composants
s&#8217;&#233;changent des informations minimales. Cette lacune va se renforcer avec l&#8217;utilisation des
techniques statistiques qui exploitent des donn&#233;es (suites de caract&#232;res constitu&#233;es de lemmes
fl&#233;chis) align&#233;es pour lesquelles le contexte se r&#233;duit forc&#233;ment au tour de parole.
Nos propositions plus g&#233;n&#233;rales concernant la traduction de dialogue vont donc dans deux
directions : l&#8217;int&#233;gration des composants en entr&#233;e et en sortie, et la gestion du dialogue.
</p>
<p>                                                
</p>
<p>3
 http://www.ldc.upenn.edu/Projects/TIDES/Translation/TransAssess02.pdf
</p>
<p>4
 http://www.slt.atr.co.jp/IWSLT2004/</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Herv&#233; Blanchon, Laurent Besacier
</p>
<p>L&#8217;int&#233;gration des composants peut &#234;tre r&#233;alis&#233;e en entr&#233;e et en sortie. En entr&#233;e, il faut envisager
de transmettre des informations plus riches entre les modules de reconnaissance et d&#8217;analyse.
Nous proposons m&#234;me une transmission bidirectionnelle. La reconnaissance peut fournir un
treillis de mots (compl&#233;t&#233; &#233;ventuellement d&#8217;informations sur la prosodie), ou bien une sortie d&#233;j&#224;
partiellement analys&#233;e en utilisant un mod&#232;le de langage s&#233;mantique [Vu Minh Q., et al., 2004].
Inversement, le module d&#8217;analyse peut fournir au module de reconnaissance le th&#232;me en cours
dans le dialogue (pour utiliser des mod&#232;les de langage dynamique), et/ou un cache de mots d&#233;j&#224;
utilis&#233;s dans les tours de parole pr&#233;c&#233;dents des diff&#233;rents interlocuteurs (renforcement des mots
du cache lors du d&#233;codage). En sortie, il s&#8217;agit pour l&#8217;analyseur de fournir des marques dans le
texte produit afin d&#8217;obtenir une synth&#232;se plus naturelle.
</p>
<p>La gestion du dialogue peut prendre en compte plusieurs contextes [Boitet C., et al., 2000] : le
contexte global (type de dialogue, caract&#233;ristiques, r&#244;le, localisation des participants), le contexte
dialogique (repr&#233;sentation du pass&#233; et du pr&#233;sent, pr&#233;dictions sur le futur), et le contexte
linguistique (ant&#233;c&#233;dents possibles d&#8217;anaphores et d&#8217;ellipses, s&#233;lections lexicales).
</p>
<p>Conclusion
Nous avons d&#233;crit en d&#233;tail la seconde exp&#233;rience en &#233;valuation subjective du projet NESPOLE! et
montr&#233; comment nous avons &#233;valu&#233; nos progr&#232;s. Nous avons introduit, et critiqu&#233;, les pistes
actuellement suivies en &#233;valuation dans le domaine. Nous avons enfin &#233;voqu&#233; les questions en
suspens &#224; propos de l&#8217;&#233;valuation objective et fait des propositions afin d&#8217;am&#233;liorer les syst&#232;mes.
</p>
<p>R&#233;f&#233;rences
Boitet C., Blanchon H. &amp; Guilbaud J.-P. (2000). A way to integrate context processing in the MT
component of spoken, task-oriented translation systems. Proc. MSC-2000. Kyoto, Japan, October
11-13, 2000. vol. 1/1: pp. 83-87.
Coughlin D. (2003). Correlating Automated and Human Assessments of Machine Translation
Quality. Proc. MT Summit IX. September 23-27, 2003: 8 p.
Doddington G. (2002). Automatic Evaluation of Machine Translation Quality Using N-gram Co-
Occurrence Statistics. Proc. HLT 2002. San Diego, California, March 24-27, 2002. vol. 1/1: pp.
128-132 (note book proceedings).
Lazzari G. (2000). Spoken Translation: Challenges and Opportunities. Proc. ICSLP 2000.
Beijing, China, Oct. 16-20, 2000. vol. 4/4: pp. 430-435.
Mana N., Burger S., Cattoni R., Besacier L., Maclaren V., Mc Donough J. &amp; Metze F. (2003). The
Nespole! VoIP Corpora in Tourism and Medical Domains. Proc. EUROSPEECH 2003. Geneva,
Switzerland, Spetember 1-4, 2003: 4 p.
Papineni K., Roukos S., Ward T. &amp; Zhu V. (2002). BLEU: a Method for Automatic Evaluation of
Machine Translation. Proc. ACL-02. Philadelphia, USA, July 7-12, 2002. vol. 1/1: pp. 311-318.
Rossato S., Blanchon H. &amp; Besacier L. (2002). Speech-to-Speech Translation System Evaluation:
Results for French for the NESPOLE! Project First Showcase. Proc. ICSLP. Denver, USA, 16-20
September, 2002: 4p.
Sugaya F., Yasuda K., Takezawa T. &amp; Yamamoto S. (2001). Precise Measurement Method of a
Speech Translation System's Capabilities with a Paired Comparison Method between the System
and Humans. Proc. MT Summit VIII. Santiago de Compostela, Spain, 18-22 September, 2001.
vol. 1/1: pp. 345-350.
Takezawa T., Sumita E., Sugaya F., Yamamoto H. &amp; Yamamoto S. (2002). Towards a Broad-
coverage Bilingual Corpus for Speech Translation of Travel Conversation in the Real World.
Proc. LREC-2002. Las Palmas, Spain, May 29-31, 2002. vol. 1/3: pp. 147-152.
Tessiore L. &amp; Hahn W. (2000). Functional Evaluation of a Machine Interpretation System:
Verbmobil. in Verbmobil: Foundation of Speech-to-Speech Translation. Springer-Verlag. Berlin.
pp. 611-631.
Vu Minh Q., Besacier L., Blanchon H. &amp; Bigi B. (2004). Mod&#232;le de langage s&#233;mantique pour la
reconnaissance automatique de parole dans un contexte de traduction. Proc. TALN 2004. F&#232;s,
Maroc, 19-21 avril 2004: dans ce volume 6p.</p>

</div></div>
</body></html>