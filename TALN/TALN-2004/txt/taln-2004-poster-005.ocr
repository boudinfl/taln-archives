TALN 2004, Session Poster, Fe‘s, I9-21 avril 2004

Traduction de dialogue: resultats du projet NESPOLE! et pistes
pour le domaine

Herve Blanchon, Laurent Besacier

Laboratoire CLIPS
BP 5 3
38041 Grenoble Cedex 9
{prenom.nom} @ imag.fr

Résumé

Dans cet article, nous detaillons les resultats de la seconde evaluation du projet europeen
NESPOLE! auquel nous avons pris part pour le francais. Dans ce projet, ainsi que dans ceux qui
l’ont precede, des techniques d’evaluation subjectives — realisees par des evaluateurs humains
— ont ete mises en oeuvre. Nous presentons aussi les nouvelles techniques objectives —
automatiques — proposees en traduction de l’ecrit et mises en oeuvre dans le projet C-STAR III.
Nous conclurons en proposant quelques idees et perspectives pour le domaine.

Mots Clés

Traduction de dialogue, evaluation subjective et objective de composants de TALN

Introduction

Le projet NESPOLE! [Lazzari G., 2000] visait £1 capitaliser les efforts des partenaires europeens et
americains du consortium C-STAR II et aller plus loin en termes scientifiques. Les langues
irnpliquees sont l’italien, le francais, l’allemand et l’anglais.

Les demonstrateurs NESPOLE! mettent en situation de dialogue un agent touristique italophone et
un client parlant anglais, francais ou allemand. Une importance particuliere a ete donnee £1
l’evaluation des deux demonstrateurs produits (2001 et 2002). Ces evaluations ont ete conduites
avec des evaluateurs humains qui jugent la qualite de traduction du systeme en comparant
l’enonce source et l’enonce cible produit. On parle d’evaluation subjective. Ces evaluations ont
perrnis de mesurer a la fois des performances brutes, ainsi que les progres accomplis.

Dans le domaine de la traduction de l’ecrit, afin de diminuer le coﬁt de l’evaluation, la methode
BLEU [Papineni K., et al., 2002] a d’abord ete proposee puis raffinee ensuite sous d’autres
noms. I1 s’agit ici de comparer automatiquement la sortie du systeme £1 une traduction etalon
eventuellement completee par un ensemble de paraphrases. On parle d’evaluation objective. Le
domaine de la traduction de dialogue a adopte recemment les techniques d’evaluation objectives
qui obligent £1 produire plusieurs paraphrases d’une traduction etalon.

Dans cet article, nous detaillons d’abord les resultats de la seconde evaluation du projet
NESPOLE!. Nous presentons ensuite, en formulant quelques remarques, les nouvelles techniques
objectives proposees en traduction de l’ecrit. Nous commentons aussi les resultats d’une
premiere experience pilote en evaluation objective realisee dans le cadre de C-STAR III. Nous
concluons avec quelques idees et perspectives plus generales pour le domaine de la traduction de
parole.

Herve’ Blanchon, Laurent Besacier

1 Evaluation du Démonstrateur en « tourisme étendu » (2002)

Les resultats de l’evaluation du premier demonstrateur en << tourisme restreir1t » ont ete presentes
en detail dans [Rossato S., et al., 2002].

1.1 Données et protocole

Deux dialogues extraits de la seconde collecte NESPOLE! [Mana N., et al., 2003] ont ete utilises.
Ces dialogues couvrent des scenarios complexes non couverts par le premier demonstrateur,
evenements culturels, chateaux, lacs et forfaits. Pour l’italien, il s’agit de tours de paroles d’un
agent de voyage, pour les trois autres langues, ce sont ceux d’un client.

Les signaux recueillis servent d’entree aux modules de reconnaissance Vocale. Les transcriptions
manuelles de ces signaux servent de reference pour la traduction (elles simulent une
reconnaissance sans erreur). Les tours de parole sont segmentes en unites semantiques de
dialogue (SDU1). Apres avoir applique les modules de reconnaissance et/ou de traduction sur ces
donnees, des evaluateurs humains jugent, pour chaque tour de parole transcrit, la qualite de la
traduction de chaque SDU au sein du tour. L’evaluation est faite au niveau des SDU car elles
representent un element atornique de la tache. Ne pas faire la traduction correcte de l’une d’entre
elles au seir1 d’un tour de parole ne signifie pas que tout le tour de parole soit mal traduit et que
la téche ne converge pas vers son objectif.

Les differentes classes d’evaluation realisees sont les memes que celles de la premiere campagne
soit : evaluation de la reconnaissance de la parole (Word Accuracy Rate et hypothese comme
paraphrasez), et evaluation des traductions monolingues et bilingues sur les references et sur les
hypotheses du module de reconnaissances. Afm de mesurer les progres accomplis, nous avons
aussi utilise, sur ces meme donnees, les analyseurs et generateurs developpes pour le premier
demonstrateur pour les configurations monolingues et bilingues sur les transcriptions
uniquement (Sur Refs (01) dans la Table 1).

Nous avons change la procedure d’evaluation sur plusieurs points. Nous avons abandonne
l’echelle a trois valeurs utilisee lors de la premiere evaluation. Les evaluateurs choisis pour cette
evaluation sont des eleves de demiere ar1nee d’ecole de traduction (DESS pour le francais). Lors
de la premiere evaluation, les evaluateurs n’avaient pas de formation specifique en traduction et
les groupes n’etaient pas homogenes en terrne de niveau en seconde langue.

La premiere echelle de notation comportait les valeurs BAD, OK et PERFECT. Les evaluateurs
devaient d’abord veriﬁer que le sens etait preserve (note BAD sinon). Puis, lorsque le sens etait
preserve, ils devaient dire si la traduction etait grammaticale ou non (PERFECT vs. OK). Or,
l’environnement d’evaluation presentait aux evaluateurs les trois options en meme temps, il est
possible qu’ils aient ete pousses a choisir la note mediane (OK). La premiere question etait aussi
severement interpretee (toute sorte de perte de sens rangeait la SDU dans la categorie BAD).

Nous avons donc utilise une echelle de quatre valeurs fondee uniquement sur la preservation du
sens : VERY GOOD (toutes les informations sont presentes et faciles £1 comprendre), GOOD (toutes
les informations importantes sont presentes), BAD (une ou plusieurs informations importantes
ont ete omises), VERY BAD (les informations importantes sont presque toutes absentes).

1 Une SDU est un segment de tour de parole de longueur maximale qui peut etre code par une seule

representation dans le pivot IF que nous utilisons dans NESPOLE! Voici, un exemple de decoupage en SDU,
separees par des # d’un tour de parole : « bonjour madame # j aimerais organiser une semaine de vacances
dans un parc # et je voudrais aussi une charnbre simple a cavalese du quinze au vingt septembre ».

Le WAR ne prend pas en compte le fait que certaines erreurs de reconnaissance peuvent avoir des consequences
plus ou moins importantes sur la qualite de la traduction produite par le systeme. Ainsi, nous avons aussi
verifie si la sortie du module de reconnaissance peut etre, ou non, consideree comme une paraphrase de la
transcription manuelle du signal.

Traduction de dialogue: résultats du projet NESPOLE.’ et pistes pour le domaine

Finalement, pour pouvoir comparer les resultats de cette evaluation a la precedente, les notes
VERY GOOD et GOOD ont ete additionnees comme ACCEPTABLE. Les resultats complets de cette
seconde evaluation sont donnes Table 1.

1.2 Résultats

Reconnaissance WAR 5 8 5 6 5 1 7 6
Hypos comme paraphase 60 6 7 6 2 7 6
Traduction monolingue (ACCEPTABLE) F RA-F RA EN G-ENG GER-GER ITA-ITA

Sur Refs (01) / sur Refs (02) / Hypos (02) 69/77/58 68/68/50 45/61/51 36/51/42
Traduction Bilingue (ACCEPTABLE) F RA-I TA EN G-ITA GER-I TA

Sur Refs (01) / sur Refs (02) / Hypos (02) 72/78/58 64/70/50 44 / x / x
ITA-FRA ITA-ENG ITA-GER

Sur Refs (01) / sur Refs (02) / Hypos (02) 19/37/33 33/33/30 38/45/38

Table 1 : Resultats de la seconde campagne d'evaluation Nespole!

1.3 Commentaires

Pour le francais, on atteint, en monolingue sur les hypotheses, un taux de 58% de traductions
acceptables alors que l’on pouvait esperer 60%. Vers l’italien, les resultats sont cette fois
meilleurs que depuis l’anglais. Enfin, depuis l’italien, le generateur vers le francais afﬁche
maintenant des performances comparables aux generateurs vers l’anglais et l’allemand.

Les resultats produits dans Verbmobil lors de l’evaluation de masse [Tessiore L. and Hahn W.,
2000] pour un taux de reconnaissance inferieur a 75% sont de 66% en allemand-anglais et de
58% en anglais allemand. Les resultats obtenus avec notre second demonstrateur sont du meme
ordre.

1.4 Mesure des progrés accomplis

En traduction vers l’italien, les trois systemes produisent un taux de plus de 30% de traductions
acceptables. Ces taux ne semblent pas montrer de gros progres par rapport a la premiere
evaluation. Cependant, si on compare les taux d’acceptabilite sur les references italiennes avec
les modules des premier et second demonstrateurs, on se rend compte que, pour le francais, le
generateur progresse de 18%, 1e generateur allemand de 7%, le generateur anglais restant stable.

En ce qui concerne la traduction monolingue ou bilingue du cote client, on observe que les taux
de traduction sont superieurs ou egaux a 50% sur les hypotheses et superieurs a 70% sur les
references. Ces taux sont en augmentation de six a dix points pour le francais par rapport aux
modules de la premiere evaluation.

Les tours de parole de l’agent de voyage (italien) sont devenus plus complexes dans le second
demonstrateur. Avec un taux d’hypotheses paraphrasant l’entree de 76%, 1e pourcentage de
traductions acceptables en italien semble etre nettement inferieur aux resultats monolingues des
autres langues. Une etude plus fine des donnees permet d’expliquer ce resultat.

Les developpeurs des systemes ont classe les SDU des dialogues d’evaluation en trois categories :
SDU couvertes par le premier demonstrateur (classe 1), SDU couvertes par le second demonstrateur
uniquement (classe 2), SDU hors du domaine (classe 3). Nous avons calcule les performances des
systemes sur ces trois groupes separement. Pour le francais, l’anglais et l’allemand (client),
seulement 5% des SDU appartiennent aux classe 2 et 3. Cependant, pour l’italien (agent), 13%
sont dans la seconde categorie et 25% dans la troisieme.

Les differences de performance apportent des reponses interessantes. Sur le premier groupe, on
observe une amelioration des performances de 56.6% pour le premier demonstrateur a 63.2%
pour le second demonstrateur. Ce qui montre une amelioration du second demonstrateur dans la

Herve’ Blanchon, Laurent Besacier

couverture du domaine du premier demonstrateur. Sur les donnees du groupe 2, le premier
demonstrateur atteint seulement 14.2% de traduction acceptable alors que le second
demonstrateur atteint un score de 38.4%. Le premier demonstrateur n’etant pas prepare pour ce
type de tour de parole cela n’est pas surprenant. Bien que le second demonstrateur ait des
performances bien superieures au premier, celui-ci n’atteint cependant pas un niveau comparable
aux performances atteintes sur le domaine du premier demonstrateur.

L’analyseur de l’italien ne couvre pas, bien sﬁr, les SDU hors du domair1e. Lorsqu’on les exclue,
les performances du systeme italien passent de 49.3% pour le premier demonstrateur a 58.9
pour le second. Ces resultats sont alors comparables a ceux des autres systemes monolingues.
De meme, sans les SDU du groupe 3, les performances des autres systemes bilingues depuis
l’italien sont accrues de 9 points.

2 Vers des évaluations communes sur un méme corpus

Nous presentons maintenant les tendances actuelles, et leurs limites, en evaluation objective.
Nous evoquons aussi la proposition du consortium C-STAR III pour des evaluations competitives
sur un meme corpus.

2.1 Tendances actuelles

Le coﬁt important de l’evaluation subjective a motive le passage vers des protocoles d’evaluation
objectifs (automatiques). Plusieurs metriques ont ete proposees. Les plus utilisees sont BLEU
[Papir1eni K., et al., 2002] et NIST [Doddington G., 2002] qui calculent des distances statistiques
sur des ensembles de n-grammes er1tre la traduction produite par le systeme et des paraphrases
d’une traduction de reference.

Le score BLEU est constitue de deux composants une precision modifiee sur les n-grammes
(calculee pour chaque traduction) et une penalite pour les traductions plus courte que les
references (calculee sur tout le corpus). Avec NIST, un poids plus important est donne aux n-
grammes les plus longs. De plus, le poids d’un n-gramme est calcule en fonction de sa valeur
inforrnationnelle. Plus un n- gramme est present dans les sorties, plus son poids est faible.

La communaute pratique aussi des evaluations en WER(ta11X d’erreur mesure sur un alignement
entre une sortie et une reference), inspiree directement du domaine de la reconnaissance
automatique de la parole, et en MWER (WER sur des references multiples), ou en PER (WER
independant de la position des mots) ou en MPER (PER sur des references multiples). Le lecteur
peut consulter [Sugaya F., et al., 2001] pour plus d’ir1forrnations.

2.2 Limites

Les promoteurs et les utilisateurs de telles techniques n’oublient pas que, dans l’absolu, les
chiffres produits par les methodes automatiques ne veulent rien dire. De nombreuses etudes
essaient d’etablir une correlation er1tre les resultats d’une evaluation objective et ceux d’une
evaluation subjective [Coughlin D., 2003, Doddington G., 2002, Papineni K., et al., 2002].
Cependant, de notre point de vue, si cette correlation est utile pour veriﬁer les progres realises au
cours du developpement, elle ne repond pas a la question de l’utilisabilite.

En effet, nous savons qu’un taux de 100% de traductions acceptables ou un score de un a une
evaluation BLEU ne seront pas atteir1ts avant longtemps, si ce n’est jarnais, dans des domaines
assez largement couverts. Il serait donc important de savoir a partir de quelles performances un
systeme devient utile et utilisable. On pourrait ainsi essayer de correler les resultats d’evaluation
objective avec l’utilisabilite. Les evaluations actuelles ne traitent pas de ce sujet.

Du poir1t de vue de leur mise en oeuvre pratique, les methodes automatiques ir1duisent aussi
quelques questions essentielles. Ainsi, on ne trouve pas dans la litterature d’instructions pour la
fabrication des references. On sait bien que la << typologie » des references doit etre la plus

Traduction de dialogue: résultats du projet NESPOLE.’ et pistes pour le domaine

proche possible de la << typologie » des sorties du systeme a evaluer. En consequence, la
comparaison de plusieurs systemes sur des memes donnees de test peut devenir delicate si la
collection de paraphrases de chaque reference n’est pas assez exhaustive.

2.3 Evaluations sur un méme corpus (C-STAR III)

Afin de preparer une campagne d’evaluation ouve1te sur le corpus BTEC [Takezawa T., et al.,
2002], une premiere experience pilote interne a ete conduite en 2003. Cette evaluation concerne
uniquement la traduction textuelle. Les donnees de developpement et de test des differents
systemes utilisaient BTEC et d’autres ressources monolingues. 500 phrases anglaises traduites
dans toutes les langues sources ont ete utilisees comme donnees de test.

Nous avons evalue cinq systemes depuis le chinois, l’italien, et le japonais vers l’anglais en
procedant, d’une part, a une evaluation subjective des resultats et d’autre part, a une evaluation
objective en utilisant BLEU et NIST. L’evaluation subjective a ete conduite selon les
recommandations publiees par le Linguistic Data Consortium pour l’evaluation du projet TIDES
de la DARPA3. Les scores BLEU et NIST ont ete calcules sur les sorties brutes des systemes.

A l’examen des resultats produits, nous avons rencontre une inconsistance entre les scores pour
un systeme classe 3'“ par BLEU et 5‘“’“° par NIST. Les sorties de ce systeme etaient
signiﬁcativement plus cou1tes que les references, ce qui inﬂue fo1tement le calcul du score. A ce
detail pres, les classements des systemes pour chacune des evaluations sont homogenes.

Nous avons aussi observe des differences importantes dans la forme des sorties des differents
systemes : usage de majuscules, rendu des numeraux (lettres, chiffres separes ou non),
abreviations, mots composes (avec ou sans tirets), ponctuation, etc. Nous avons montre que cela
induit des differences de i0,l5 points pour BLEU et il,8 pour NIST, ce qui n’est pas negligeable.
Pour les experiences futures, il faudra donc normaliser les sorties des differents systemes
comme cela est deja le cas dans la communaute du traitement de la parole.

Afin de mobiliser la communaute du domaine et d’apporter quelques reponses aux questions
que nous posons dans les sections 2.2 et 3, le consortium C-STAR III organise un atelier satellite
a la conferences ICSLP-20044. Dans ce cadre, le consortium diffusera une partie du corpus
BTEC afin que des membres exterieurs puissent evaluer leurs systemes sur nos donnees.

3 Pistes pour le futur

Pour aller plus loin, il nous semble bien sﬁr important et necessaire de reﬂechir a l’evaluation en
proposant un cadre qui reponde non seulement aux besoins des developpeurs, mais aussi aux
besoins des utilisateurs. Il nous parait aussi necessaire de reﬂechir de nouveau au contexte de
nos travaux : nous faisons de la traduction de dialogue. Si dans le cadre du projet Verbmobil ce
contexte pa11:iculier a ete exploite, il nous semble que dans les travaux posterieurs et actuels, les
systemes proposes n’en font plus usage. Chaque tour de parole est traduit pour lui meme
comme s’il n’etait pas enonce dans un contexte (celui du dialogue). Les architectures mises en
oeuvre sont exclusivement des architectures pipe-line dans lesquelles les differents composants
s’echangent des informations minimales. Cette lacune va se renforcer avec l’utilisation des
techniques statistiques qui exploitent des donnees (suites de caracteres constituees de lemmes
ﬂechis) alignees pour lesquelles le contexte se reduit forcement au tour de parole.

Nos propositions plus generales concemant la traduction de dialogue vont donc dans deux
directions : l’integration des composants en entree et en sortie, et la gestion du dialogue.

3 http://www.ldc.upenn.edu/Projects/’I‘Il)ES/'I‘rans1ation/TransAssess02.pdf
4 http://www.s1t.atr.co.jp/IWSLT2004/

Herve’ Blanchon, Laurent Besacier

L’integration des composants peut etre realisee en entree et en sortie. En entree, il faut envisager
de transmettre des informations plus riches entre les modules de reconnaissance et d’analyse.
Nous proposons meme une transmission bidirectionnelle. La reconnaissance peut foumir un
treillis de mots (complete eventuellement d’info1mations sur la prosodie), ou bien une sortie deja
partiellement analysee en utilisant un modele de langage semantique [Vu Minh Q., et al., 2004].
Inversement, le module d’analyse peut foumir au module de reconnaissance le theme en cours
dans le dialogue (pour utiliser des modeles de langage dynamique), et/ou un cache de mots deja
utilises dans les tours de parole precedents des differents interlocuteurs (renforcement des mots
du cache lors du decodage). En sortie, il s’agit pour l’analyseur de foumir des marques dans le
texte produit afin d’obtenir une synthese plus naturelle.

La gestion du dialogue peut prendre en compte plusieurs contextes [Boitet C., et al., 2000] : le
contexte global (type de dialogue, caracteristiques, role, localisation des participants), le contexte
dialogique (representation du passe et du present, predictions sur le futur), et le contexte
linguistique (antecedents possibles d’anaphores et d’ellipses, selections lexicales).

Conclusion

Nous avons decrit en detail la seconde experience en evaluation subjective du projet NESPOLE! et
montre comment nous avons evalue nos progres. Nous avons introduit, et critique, les pistes
actuellement suivies en evaluation dans le domaine. Nous avons enfin evoque les questions en
suspens a propos de l’evaluation objective et fait des propositions afin d’ameliorer les systemes.

Références

Boitet C., Blanchon H. & Guilbaud J .-P. (2000). A way to integrate context processing in the MT
component of spoken, task-oriented translation systems. Proc. MSC-2000. Kyoto, Japan, October
11-13, 2000. vol. 1/1: pp. 83-87.

Coughlin D. (2003). Correlating Automated and Human Assessments of Machine Translation
Quality. Proc. MT Summit IX. September 23-27, 2003: 8 p.

Doddington G. (2002). Automatic Evaluation of Machine Translation Quality Using N-gram Co-
Occurrence Statistics. Proc. HLT 2002. San Diego, California, March 24-27, 2002. vol. 1/ 1: pp.
128-132 (note book proceedings).

Lazzari G. (2000). Spoken Translation: Challenges and Opportunities. Proc. ICSLP 2000.
Beijing, China, Oct. 16-20, 2000. vol. 4/4: pp. 430-435.

Mana N., Burger S., Cattoni R., Besacier L., Maclaren V., Mc Donough J . & Metze F. (2003). The
Nespole! VoIP Corpora in Tourism and Medical Domains. Proc. EUROSPEECH 2003. Geneva,
Switzerland, Spetember 1-4, 2003: 4 p.

Papineni K., Roukos S., Ward T. & Zhu V. (2002). BLEU: a Method for Automatic Evaluation of
Machine Translation. Proc. ACL-02. Philadelphia, USA, July 7-12, 2002. vol. 1/ 1: pp. 311-318.

Rossato S., Blanchon H. & Besacier L. (2002). Speech-to-Speech Translation System Evaluation:
Results for French for the NESPOLE! Project First Showcase. Proc. ICSLP. Denver, USA, 16-20
September, 2002: 4p.

Sugaya F., Yasuda K., Takezawa T. & Yamamoto S. (2001). Precise Measurement Method of a
Speech Translation System's Capabilities with a Paired Comparison Method between the System
and Humans. Proc. MT Summit VIII. Santiago de Compostela, Spain, 18-22 September, 2001.
vol. 1/1: pp. 345-350.

Takezawa T., Sumita E., Sugaya F., Yamamoto H. & Yamamoto S. (2002). Towards a Broad-
coverage Bilingual Corpus for Speech Translation of Travel Conversation in the Real World.
Proc. LREC-2002. Las Palmas, Spain, May 29-31, 2002. vol. 1/3: pp. 147-152.

Tessiore L. & Hahn W. (2000). Functional Evaluation of a Machine Interpretation System:
Verbmobil. in Verbmobil: Foundation of Speech-to-Speech Translation. Springer-Verlag. Berlin.
pp. 611-631.

Vu Minh Q., Besacier L., Blanchon H. & Bigi B. (2004). Modele de langage se’mantique pour la
reconnaissance automatique de parole dans un contexte de traduction. Proc. TALN 2004. Fes,
Maroc, 19-21 avril 2004: dans ce volume 6p.

