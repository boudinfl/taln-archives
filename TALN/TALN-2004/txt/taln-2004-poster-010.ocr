TALN 2004, Session Poster, Fes, 19-21 aVr1'12004

Le projet GERAF : Guide pour l’Evaluation des Résumés
Automatiques Frangais

Marie—Josée Goulet (1,2)
et
Joel Bourgeoys (1,3)
(1) L—TAL — Université Laval, Québec, Canada
(2) Laboratoire LaLICC — Université Paris IV—Sorbonne, France
(3) LIC2M — CEA/LIST, Paris, France
marie—josee.gou1et.1 @u1aval.ca, joe1.bourgeoys.1@u1ava1.ca

Résumé - Abstract

Dans cet article, nous présentons le projet GERAF (Guide pour l’1-Evaluation des Résumés Au-
tomatiques Francais), lequel vise l’élaboration de protocoles et la construction de corpus de
résumés de référence pour l’évaluation des systémes résumant des textes francais. La ﬁnalité de
ce projet est de mettre a la disposition des chercheurs les ressources air1si créées.

In this paper, we introduce GERAF (Guide pour l’l-Evaluation des Résumés Automatiques Fran-
cais), which aims at elaborating protocols and creating human-generated summaries for sum-
marization evaluation in the context of French texts. The goal of this project is to provide re-
searchers with protocols and corpora needed for French summarization evaluation.

Mots-clefs — Keywords

évaluation, résumé automatique, textes francais, GERAF
evaluation, automatic summarization, French texts, GERAF

1 Introduction et problématique

L’évaluation doit désorrnais faire partie intégrante de tout programme de TAL. Nos travaux de
recherche portent sur l’évaluation des systémes de résumé automatique francais. De plus en
plus de chercheurs se consacrent aujourd’hui au développement de systemes qui résument des
textes francais. Mentionnons a titre d’exemple le systeme commercial Copernic Summarizer dé-
veloppé au Québec, la plate-forme ContextO développée au laboratoire LaLICC a l’université
Paris IV (Crispino, 2003), le systeme Pertinence disponible sur intemet (www.pertinence.net),
ainsi que des systémes faisant l’objet de theses ou des systémes en développement. On peut
supposer que, dans les années a venir, d’autres systemes résumant des textes francais verront
le jour. Il est donc souhaitable, voire primordial, de mettre a la disposition des chercheurs des

Marie-Josee Goulet, Joe'1Bourgeoys

ressources pour l’evaluation des systemes de resume automatique francais. Par ressources, nous
entendons : des corpus de textes sources francais a resumer, des corpus de resumes de refe-
rence avec lesquels comparer les resumes automatiques produits par le systeme, des protocoles
d’evaluation detailles ainsi que des exemples d’application de ces protocoles.

Dans cet article, nous presentons les premieres demarches pour la creation du projet GERAF
(Guide pour l’Evaluation des Resumes Automatiques Francais). D’abord, nous decrivons les
methodes traditionnelles d’evaluation des systemes de resume automatique et proposons un
nouveau protocole d’evaluation pour les extraits automatiques francais. Ensuite, nous abordons
la construction de corpus de resumes de reference requis pour l’application de ce protocole
d’evaluation. Dans la demiere partie, une breve conclusion rappelant les points importants ouvre
la discussion sur les perspectives du projet GERAF.

2 Protocoles pour l’évaluation des résumés automatiques

Deﬁni simplement, un resume automatique est une version plus courte d’un texte source, qui
renferme l’inforrnation la plus importante de ce texte, et qui est produite par des moyens infor-
matiques. Les Anglo-saxons opposent deux types de resume automatique, l’abstract et l’extract.
Dans ce texte, nous utiliserons les expressions francisees abrége’ et extrait. L’abrege est produit
par un processus de comprehension du texte source suivi d’une generation de texte, tandis que
l’extrait est produit par un processus d’extraction des phrases saillantes a partir du texte source
(Mani et al., 2002).

Le projet GERAF, presente pour la premiere fois dans cet article, vise l’evaluation de tous les
types de resume automatique francais. Toutefois, les demarches entreprises jusqu’a maintenant
ont ete orientees principalement vers l’evaluation des resumes de type extrait, auxquels sera
consacre le reste de l’expose. Ce choix s’explique par un besoin plus pressant pour l’evaluation
des extraits automatiques, par opposition aux abreges automatiques, les chercheurs s’etant a ce
jour surtout interesses au developpement de ce type de resume (Edmundson, 1969; Klavans et
al., 1998).

2.1 Evaluation du contenu

Deux aspects generaux d’un extrait automatique peuvent etre evalues, soit le contenu et la lisi-
bilite. Le contenu correspond a l’inforrnation foumie par l’extrait tandis que la lisibilite refere a
la cohesion de l’extrait. Comme nous le verrons, il n’existe pas de consensus quant a la methode
d’evaluation a preconiser.

En ce qui conceme l’evaluation du contenu, la methode la plus repandue consiste a comparer
l’extrait automatique avec un resume de reference. Ce referentiel peut prendre diverses formes.
Ainsi, certains chercheurs ont compare des extraits automatiques avec des resumes auteurs (Teu-
fel, Moens, 1997). Le resume auteur, comme son nom l’indique, correspond au resume redige
par l’auteur d’un texte. D’autres chercheurs ont quant a eux compare des extraits automatiques
avec des resumes professionnels (Kupiec et al., 1995). Le resume professionnel, par opposition
au resume auteur, est redige par une autre personne que l’auteur du texte. Dans les deux cas
toutefois, la comparaison entre l’extrait automatique et le referentiel s’effectue sur la base des
concepts cles.

Pro jet GERAF

D’autres chercheurs ont adopte une methode ou les extraits automatiques sont compares avec
des extraits manuels (Jing et al., 1998; Rath et al., 1961). L’extrait manuel correspond a un
resume produit par un humain en selectionnant les phrases saillantes du texte source. Bien
que peu de details soient fournis concernant la production de l’extrait manuel dans les etudes
precedentes, on peut supposer que le << resumeur >> doit d’abord proceder a l’identiﬁcation des
sujets saillants, pour ensuite selectionner les phrases representant ces sujets.

Une fois l’extrait manuel produit, il s’agit de Veriﬁer si toutes ses phrases sont presentes dans
l’extrait automatique. Cette comparaison s’effectue a l’aide des mesures classiques de rappel et
de precision, empruntees au domaine du reperage d’inforrnation.

Dans le cadre du projet GERAF, nous proposons d’utiliser une autre forme de referentiel. Ce
referentiel est concu comme une liste de sujets saillants, correspondant en fait a une forme
interrnediaire entre le texte source et la liste de phrases saillantes. Bien entendu, utiliser une
autre forme de referentiel nous oblige a reconsiderer en entier la methode de comparaison entre
l’extrait manuel et l’extrait automatique. La comparaison s’effectuera en deux etapes : 1) Une
mise en correspondance directe Visant a Veriﬁer si les suj ets saillants du referentiel sont presents
textuellement dans l’extrait automatique; 2) Une mise en correspondance indirecte Visant a
Veriﬁer si les sujets saillants du referentiel sont representes par un autre segment textuel dans
l’extrait automatique, par exemple une anaphore ou un synonyme. Ces mises en correspondance
seront effectuees de maniere manuelle dans un premier temps. Dans un deuxieme temps, nous
pourrions envisager de concevoir une methode automatique ou semi-automatique de mise en
correspondance.

Cette methode pour l’eValuation du contenu des extraits automatiques presente deux avantages
non negligeables. Premierement, l’extrait manuel concu comme une liste de suj ets saillants sera
plus facile a produire que l’extrait manuel traditionnel sous forme de phrases saillantes, puis-
qu’il implique une etape de moins. Deuxiemement, notre methode permettra de tenir compte
de la dimension semantique, ce qui n’est pas possible lors d’une comparaison avec une liste de
phrases. Dans cette demiere methode, l’extrait n’est considere de bonne qualite que lorsqu’il
contient les memes phrases que le referentiel. Toutefois, une phrase de l’extrait automatique
absente du referentiel peut exprimer le meme contenu qu’une phrase de ce referentiel. Avec la
methode des sujets saillants, il sera possible de tenir compte de la synonymie inherente a tout
texte, ce qui a notre avis permettra d’eValuer a sa juste Valeur le contenu d’un extrait automa-
tique.

2.2 Evaluation de la lisibilité

Les extraits automatiques sont produits en extrayant, par des analyses statistiques ou linguis-
tiques, les phrases saillantes des textes a resumer. Cette methode entraine de nombreuses la-
cunes au niveau de la cohesion des extraits, par exemple lorsqu’une phrase extraite contient un
pronom dont l’antecedent se trouve dans une phrase non extraite. Ces lacunes nuisent eVidem-
ment a la lisibilite des extraits automatiques.

La methode la plus repandue pour l’eValuation de la lisibilite consiste a demander a des juges
d’accorder une note aux extraits automatiques selon des criteres pre-etablis (Mani, Maybury,
1999; Minel et al., 1997). Come exemples de criteres, mentionnons la presence d’anaphores
sans antecedent, les ruptures dans l’argumentation et les repetitions. Bien que ces criteres ge-
neraux puissent etre de bons indicateurs d’un manque de cohesion, nous pensons qu’une etude

Marie-Josee Goulet, Joe'1Bourgeoys

plus approfondie est necessaire dans le cas des extraits automatiques francais.

Nous effectuons presentement une etude a partir d’extraits automatiques francais produits par
le systeme ContextO1. Cette etude empirique, bien qu’inachevee, indique que les erreurs de co-
hesion dans les extraits automatiques sont plus variees que ce que laissent entrevoir les resultats
des etudes precedentes. A titre d’exemple, nous avons repere neuf types d’anaphorique sans
antecedent : 1) Nom propre, 2) Nom commun, 3) Acronyme, 4) Pronom personnel, 5) Pronom
demonstratif, 6) Adjectif demonstratif, 7) Adjectif possessif, 8) Adjectifs indeﬁnis, 9) Adverbes.
Nous avons egalement repere des connecteurs, par exemple ainsi, donc, mais, introduisant une
idee dont la premiere partie n’a pas ete incluse dans l’extrait.

Nous pensons qu’il est necessaire d’etudier la quantiﬁcation et la distribution des erreurs de
cohesion dans un corpus d’extraits automatiques francais aﬁn de degager des criteres precis pour
l’evaluation de la lisibilite, un peu comme il a ete fait dans l’etude de (Nanba, Okumura, 2000)
a partir d’extraits automatiques japonais. A notre connaissance, aucune etude empirique n’a
presente une quantiﬁcation exhaustive des erreurs de lisibilite a partir d’extraits automatiques
francais.

De plus, le projet GERAF etablira une facon de mettre en relation l’evaluation de la lisibilite
et l’evaluation du contenu, aﬁn de veriﬁer s’il existe une correlation entre les resultats de ces
deux evaluations. Plus precisement, nous cherchons a savoir si les extraits automatiques juges
faibles au niveau du contenu presentent plus de lacunes au niveau de la cohesion que ceux juges
adequats.

3 Corpus pour l’application des protocoles élaborés dans le
projet GERAF

A notre connaissance, aucun corpus de textes francais accompagnes d’extraits manuels de re-
ference n’est a la disposition des chercheurs. Aﬁn de combler cette lacune, le projet GERAF
prevoit la constitution d’un corpus d’extraits manuels de reference. Comme nous l’avons vu
dans la section 2.1, ce referentiel se presente sous forme d’une liste de sujets saillants, ce qui
constitue une idee originale. Cette liste des sujets saillants peut etre produite a partir du texte
source ou a partir d’un resume auteur.

La constitution du corpus d’extraits manuels dans le projet GERAF requiert deux types de
texte. Dans le cas ou les sujets saillants sont identiﬁes a partir des textes sources, nous pour-
rions utiliser les textes de L’Actualite’, La Recherche, Le Monde ou Le Monde diplomatique.
Il serait aussi interessant d’utiliser des textes qui ne sont pas sous droits. Dans le cas ou les
sujets saillants sont identiﬁes a partir de resumes auteurs, nous pourrions utiliser les articles des
colloques francophones qui sont accompagnes de resumes auteurs. Pour le moment, nous privi-
legions la methode de production des extraits manuels a partir des textes sources. La methode
de selection des sujets saillants a partir de resumes auteurs pourrait en effet entrainer des pertes
d’inforrnation, par exemple si des sujets saillants des articles ont ete omis par les auteurs dans
les resumes.

Par ailleurs, le choix des textes sources depend de plusieurs facteurs. Il faut tenir compte du

1ContextO a ete developpe au laboratoire LaLICC (Langages, Logiques, Informatique, Cognition et Commu-
nication) 51 1’UniVersite Paris IV-Sorbonne.

Pro jet GERAF

type de texte pour lequel le systeme a ete developpe. Par exemple, certains systemes servent a
resumer des textes scientiﬁques ou techniques, alors que d’autres servent a resumer des articles
de joumaux. Il faut egalement ter1ir compte du domaine dont traitent les textes sources, car
certains systemes utilisent des connaissances linguistiques propres a un domaine particulier
dans leur algorithme d’analyse. Enﬁn, comme de nombreux auteurs l’ont deja fait remarquer
(Minel, 2002; Spark Jones, 1999), la nature des besoins de l’utilisateur constitue indeniablement
un facteur a considerer lors la construction du systeme comme tel, lors du choix des textes a
resumer, ainsi que lors de l’evaluation.

Plusieurs facteurs devront egalement etre pris en consideration pour la constitution du corpus
d’extraits manuels, notamment la longueur et la nature des textes sources, le nombre d’extraits
a produire, le nombre et le proﬁl des << resumeurs >> et les instructions a leur donner. Dans une
etude precedente (Goulet, 2003), nous avons demontre que ces facteurs pouvaient inﬂuencer,
chacun a leur facon, le degre d’accord er1tre les << resumeurs >> lors de la selection des phrases
saillantes. Logiquement, nous pouvons supposer que ces facteurs peuvent avoir des repercus-
sions sur l’accord inter-juges lors de la selection des sujets saillants.

4 Conclusion et perspectives

Cet article avait pour but de presenter le projet GERAF, que nous pouvons concevoir comme un
coffre a outils pour l’evaluation des resumes automatiques francais. Nos propositions methodo-
logiques presentent des avantages par rapport aux methodes d’evaluation traditionnelles. Pre-
mierement, en ce qui concerne la production des referentiels, notre methode des suj ets saillants
sera plus facile a appliquer. Deuxiemement, notre methode de comparaison permettra de ter1ir
compte de la dimension semantique, en raison de la mise en correspondance indirecte entre
les extraits automatiques et les referentiels. Troisiemement, notre etude empirique des facteurs
nuisant a la cohesion des extraits automatiques permettra de degager des criteres precis pour
l’evaluation de la lisibilite. Rappelons aussi que tout au long de ce travail, une attention particu-
liere sera accordee a la mise en place de tous les moyens permettant de minimiser la subj ectivite.

Dans un contexte plus general, notre projet pourrait s’integrer au sein des campagnes d’evalua-
tion deja existantes, lesquelles servent a comparer la performance des systemes d’un meme do-
maine du TAL. Il existe de nombreuses campagnes d’evaluation, par exemple la campagne ame-
ricaine TREC (Text REtrieval Conferences) qui evalue les systemes de reperage d’information
dans les textes anglais (depuis 1992) et la campagne europeenne CLEF (Cross-Language Eva-
luation Forum) qui evalue les systemes de reperage d’information multilingues (depuis 2000).
En ce qui concerne l’evaluation des systemes de resume automatique, deux campagnes d’eva-
luation ont deja ete menees par l’agence americaine DARPA (Defense Advanced Research Pro-
jects Agency). La premiere, intitulee SUMMAC, s’est deroulee de 1996 a 1998 sous l’egide du
programme TIPSTER (Mani et al., 2002), et la deuxieme, intitulee DUC (Document Unders-
tanding Conferences) existe depuis 2000. A notre connaissance, il n’existe pas de campagne
d’evaluation speciﬁque aux systemes de resume automatique francais. Ainsi, le projet GERAF
prend toute son importance.

En terminant, nous esperons avoir bien fait ressortir l’essence du projet GERAF. Alors que cer-
taines campagnes d’evaluation s’apparentent plus a un concours (par exemple TREC et DUC),
notre proj et a pour vocation premiere de construire et de mettre a la disposition des chercheurs
des ressources pour l’evaluation des systemes de resume automatique francais. De plus, nous

Marie-Josée Goulet, Joe'1Bourgeoys

souhaitons que le projet GERAF soit 1e debut d’une suite de discussions fructueuses sur1’éva-
luation des systemes de résumé automatique francais, mais aussi sur1’éva1uation des outils de
TAL en general.

Remerciements

Nous remercions 1e Conseil de recherches en sciences humaines du Canada ainsi que le Fonds
québécois de recherche sur la société et la culture pour leur soutien ﬁnancier dans le cadre de
nos recherches doctorales.

Références

CRISPINO G. (2003), Une plate—forme informatique de l ’Exploration Contextuelle : mode’lisation, archi-
tecture et re’alisation (Context0). Application au ﬁltrage se’mantique de textes, These de doctorat, Paris,
Paris IV-Sorbonne.

EDMUNDSON H. P. (1969), New Methods in Automatic Abstracting, Journal of the Association for
Computing Machinery, Vol. 16(2), 264-285.

GOULET M.-J . (2003), Evaluation Methods for French Automatic Summaries, communication présentée
au 38th Linguistics Colloquium, Piliscsaba, Hongrie. Actes a paraitre.

JING H., BARZILAY R., MCKEOWN K., ELHADAD M. (1998), Summarization Evaluation Methods :

Experiments and Analysis, Working Notes of the Workshop on Intelligent Text Summarization, California,
60-68.

KLAVANS J. L., MCKEOWN K. R., KAN M.-Y., LEE S. (1998), Resources for Evaluation of Summa-

rization Techniques, Actes du First International Conference on Language Resources and Evaluation,
Granada, Espagne.

KUPIEC J ., PEDERSEN J., CHEN F. (1995), A Trainable Document Summarizer, Actes de SIGIR 95
(Special Interest Group on Information Retrieval), Seattle, 68-73.

MANI I., KLEIN G., HOUSE D., HIRSCHMAN L., FIRMIN T., SUNDHEIM B. (2002), SUMMAC : A

Text Sunnnarization Evaluation, Natural Language Engineering, Vol. 8(1), 43-68.

MANI I., MAYBURY M. T. (1999), Advances in Automatic Text Summarization, Cambridge, Massachu-
setts, MIT Press.

MINEL J.-L. (2002), F iltrage se’mantique : du resume automatique a la fouille de textes, Paris, Lavoisier.
MINEL J .-L., NUGIER S., PIAT G. (1997), How to Appreciate the Quality of Automatic Text Sunnnari-

zation ? Examples of FAN and MLUCE Protocols and their Results on SERAPHIN, Actes du Workshop
on Intelligent Scalable Text Summarization, EACL, Madrid, 25-31.

NANBA H., OKUMURA M. (2000), Producing More Readable Extracts by Revising them, Actes du I 8th
International Conference on Computational Linguistics, Saarbrucker, 1071-1075.

RATH G. J ., RESNICK A., SAVAGE T. R. (1961), The Formation of Abstracts by the Selection of Sen-
tences, American Documentation, Vol. 12(2), 139-143.

SPARCK JONES K. (1999), Automatic Summarization : Factors and Directions, In I. Mani et M. T.
Maybury (eds.) Advances in Automatic Text Summarization, Cambridge, Massachusetts, MIT Press, 1-
12.

TEUFEL S., MOENS M. (1997), Sentence Extraction as a Classiﬁcation Task, Actes du Workshop on
Intelligent Scalable Text Summarization, EACL, Madrid, 58-65.

