<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
"http://www.w3.org/TR/html4/loose.dtd">
<html><head><title>Classification automatique de d&#233;finitions en sens</title>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
</head>
<body>
<div style="page-break-before:always; page-break-after:always"><div><p>Classification automatique de d&#233;finitions en sens
</p>
<p>Fabien Jalabert (1 &amp; 2), Mathieu Lafourcade (2)
fabien.jalabert@ema.fr , mathieu.lafourcade@lirmm.fr
</p>
<p>(1) LGI2P - Ecole des M&#238;nes d&#8217;Al&#232;s
Parc Scientifique Georges Besse
</p>
<p>30 035 - N&#239;mes Cedex 1
www.lgi2p.ema.fr
</p>
<p>(2) LIRMM - Universit&#233; Montpellier II
34 392 - Montpellier Cedex 5
</p>
<p>www.lirmm.fr
</p>
<p>Mots-clefs &#8211; Keywords
</p>
<p>Traitement automatique des langues naturelles, classification automatique, d&#233;sambigu&#239;sation
s&#233;mantique lexicale
Natural language processing, unsupervised clustering, word sense disambiguation
</p>
<p>R&#233;sum&#233; - Abstract
</p>
<p>Dans le cadre de la recherche en s&#233;mantique lexicale, l&#8217;&#233;quipe TAL du LIRMM d&#233;veloppe
actuellement un syst&#232;me d&#8217;analyse des aspects th&#233;matiques des textes et de d&#233;sambiguisation
lexicale bas&#233; sur les vecteurs conceptuels. Pour la construction des vecteurs, les d&#233;finitions
provenant de sources lexicales diff&#233;rentes (dictionnaires &#224; usage humain, listes de synonymes,
d&#233;finitions de th&#233;saurus, . . .) sont analys&#233;es. Aucun d&#233;coupage du sens n&#8217;est pr&#233;sent dans la re-
pr&#233;sentation : un vecteur conceptuel est associ&#233; &#224; chaque d&#233;finition et un autre pour repr&#233;senter
le sens global du mot. Nous souhaitons effectuer une cat&#233;gorisation afin que chaque &#233;l&#233;ment ne
soit plus une d&#233;finition mais un sens. Cette am&#233;lioration concerne bien sur directement les ap-
plications courantes (d&#233;sambigu&#239;sation, transfert lexical, . . .) mais a aussi pour objectif majeur
d&#8217;am&#233;liorer l&#8217;apprentissage de la base.
</p>
<p>In the framework of research in meaning representation in NLP, we focus our attention on the-
matic aspects and conceptual vectors. A vectorial base is built upon a morphosyntactic analysis
of several lexical resources to reduce isolated problems. A conceptual vector is associated with
each definition and another one with the global meaning of a word. There is no effective mea-
ning division and representation the the knowledge base. We study in the article a clustering
method that merge definitions into senses. This applies on common problems (word sense di-
sambiguation, word translation, . . .) and mainly to improve knowledge base learning.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Fabien Jalabert, Mathieu Lafourcade
</p>
<p>Introduction
Dans le cadre de la recherche en s&#233;mantique lexicale, l&#8217;&#233;quipe TAL du LIRMM d&#233;veloppe
actuellement un syst&#232;me d&#8217;analyse des aspects th&#233;matiques des textes et de d&#233;sambigu&#239;sation
lexicale bas&#233; sur les vecteurs conceptuels (Lafourcade et al., 2002). Les vecteurs repr&#233;sentent
les id&#233;es associ&#233;es &#224; tout segment textuel (mots, expressions, textes, . . .) via l&#8217;activation de
concepts. Pour la construction des vecteurs, nous avons pris notamment l&#8217;hypoth&#232;se d&#8217;un ap-
prentissage multi-source afin de pallier le bruit d&#233;finitoire (par exemple les probl&#232;mes dus au
m&#233;talangage comme dans la d&#233;finition d&#8217;&#8618;aboyer&#8617; : crier en parlant du chien).
</p>
<p>L&#8217;utilisation de multiples sources fait dispara&#238;tre la notion d&#8217;atomicit&#233; du sens. Un vecteur est
associ&#233; globalement &#224; un mot et plus finement &#224; chaque d&#233;finition. Mais aucun d&#233;coupage des
sens n&#8217;appara&#238;t r&#233;ellement &#224; ce stade. Nous proposons dans cet article d&#8217;effectuer une classifi-
cation non-supervis&#233;e (cat&#233;gorisation) afin de regrouper les d&#233;finitions similaires en sens. Les
m&#233;thodes de cat&#233;gorisation sont nombreuses et b&#233;n&#233;ficient de nombreux travaux mais ne sont
pas directement adapt&#233;es pour traiter des d&#233;finitions de dictionnaires.
</p>
<p>L&#8217;&#233;tude suivante d&#233;crit la sp&#233;cificit&#233; de ce probl&#232;me ainsi que les choix propos&#233;s en r&#233;ponse
avant de pr&#233;senter en d&#233;tail la proc&#233;dure mise en &#339;uvre.
</p>
<p>1 Cat&#233;gorisation des d&#233;finitions
Les m&#233;thodes de classification automatique sont nombreuses (Alpert, Kahng, 1995), (Berkhin,
2002) mais ne sont cependant pas directement applicables dans le cadre de cette &#233;tude, car
la cat&#233;gorisation et la classification dans ces domaines traitent un grand nombre de donn&#233;es
&#224; r&#233;partir dans un faible nombre de classes en un processus unique. Dans notre cas, la masse
de donn&#233;es est importante (actuellement pour 110 000 termes, plus de 430 000 d&#233;finitions et
vecteurs conceptuels), mais la cat&#233;gorisation porte sur les d&#233;finitions d&#8217;un seul terme (environ
5 d&#233;finitions en moyennes par source, certains termes fortement polys&#233;miques peuvent en avoir
plus de 50).
</p>
<p>Cependant, dans notre cas, une cat&#233;gorisation ne s&#8217;applique qu&#8217;&#224; quelques dizaines de d&#233;fini-
tions tout au plus. Les algorithmes du domaine de la fouille de donn&#233;es recherchent une effi-
cacit&#233; globale dans un grand ensemble de donn&#233;es. Notre approche du probl&#232;me se distingue
donc par une importance moindre du co&#251;t calculatoire et s&#8217;appuie sur une profondeur d&#8217;analyse
sup&#233;rieure.
</p>
<p>1.1 Choix de l&#8217;algorithme
Le choix de l&#8217;algorithme repose de diff&#233;rents constats :
Le volume de la donn&#233;e est faible. Le probl&#232;me est donc moins restrictif concernant le choix
</p>
<p>des m&#233;thodes d&#8217;analyses et de l&#8217;algorithme. Cependant, il est impossible d&#8217;envisager un
entrainement sur lequel repose certains algorithmes dont les Support Vector Machine
(SVM) par exemple (Vapnik, Chervonenkis, 1964), (Burges, 1998).
</p>
<p>Les dictionnaires sont suppos&#233;s fiables et par cons&#233;quent deux d&#233;finitions d&#8217;un m&#234;me diction-
naire ne peuvent appartenir &#224; une m&#234;me classe r&#233;sultat.
</p>
<p>L&#8217;aspect hi&#233;rarchique n&#8217;est pas pr&#233;pond&#233;rant dans les sens. Si une relation partielle d&#8217;hyper-
onymie est pr&#233;sente en s&#233;mantique, il est fr&#233;quemment impossible de g&#233;n&#233;raliser deux
sens car leur d&#233;coupage ne repose pas exclusivement sur une hi&#233;rarchie (mais sur une
analogie par exemple).
</p>
<p>Le nombre de classes est inconnu a priori. Cependant, l&#8217;hypoth&#232;se de fiabilit&#233; des dictionnaires
d&#233;crite ci-dessus implique que le nombre de sens d&#8217;un terme donn&#233; est sup&#233;rieur ou &#233;gal au</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Classification automatique de d&#233;finitions en sens
</p>
<p>nombre maximum de d&#233;finitions pr&#233;sentes dans une des sources. Nous avons donc opt&#233; pour
l&#8217;algorithme des k-moyennes. Les centro&#239;des sont initialis&#233;s avec les d&#233;finitions du dictionnaire
qui en a le plus grand nombre. Puis, de fa&#231;on it&#233;rative pour chaque dictionnaire, et de fa&#231;on
gloutonne, chaque d&#233;finition est affect&#233;e aux diff&#233;rentes classes et les centro&#239;des sont recalcul&#233;s.
</p>
<p>Il faut noter que l&#8217;hypoth&#232;se pr&#233;c&#233;dente n&#8217;est pas vraie. Certains dictionnaires divisent deux
sens l&#224; ou certains n&#8217;en proposent qu&#8217;un plus g&#233;n&#233;ral. Lorsque cela se produit simultan&#233;ment
pour deux sens diff&#233;rents, deux dictionnaires poss&#233;dant chaque sens peuvent en r&#233;alit&#233; signifier
5 sens feuilles et deux sens hyperonymiques. Ce probl&#232;me s&#8217;av&#232;re cependant isol&#233; et dans le
cas de termes &#224; tr&#232;s forte polys&#233;mie, ces termes sont peu nombreux mais en revanchent sont
fr&#233;quents en usage. Nous recherchons actuellement des solutions face &#224; cette difficult&#233;.
</p>
<p>2 Principe g&#233;n&#233;ral de l&#8217;algorithme
</p>
<p>2.1 D&#233;roulement g&#233;n&#233;ral
</p>
<p>L&#8217;algorithme utilis&#233; suit le principe des k-moyennes. &#192; l&#8217;initialisation, soit D l&#8217;ensemble des
sources lexicales di et dmax le dictionnaire comportant le plus grand nombre nmax de d&#233;finitions.
Alors on construit nmax classes et &#224; chacune est affect&#233;e une d&#233;finition de dmax. Soit C l&#8217;en-
semble des classes ci obtenues. L&#8217;algorithme a pour objectif de rechercher un partitionnement
optimal pour la fonction d&#8217;&#233;valuation globale (Eval) suivante : Eval(C) = DInter(C)&#8721;n
</p>
<p>i=1
DIntra(ci)
</p>
<p>avec DInter la distance entre les cat&#233;gories ci &#8712; C :
DInter(C) = p
</p>
<p>&#8730;
1
n2
&#8721;n,n
</p>
<p>i=1,j=i+1 DA(ci, cj)
p avec ci, cj &#8712; C
</p>
<p>et avec DIntra la distance interne &#224; une cat&#233;gorie c &#8712; C :
DIntra(c) =
</p>
<p>p
</p>
<p>&#8730;
1
n
</p>
<p>&#8721;n
i=1 DA(di,m)
</p>
<p>p avec di &#8712; c
</p>
<p>et avec m =
&#8721;|C|
</p>
<p>i=1
ci
</p>
<p>|C| centro&#239;de de la cat&#233;gorie
</p>
<p>L&#8217;algorithme it&#232;re pour chaque source et proc&#232;ce &#224; l&#8217;affectation de valeur minimum entre les d&#233;-
finitions de la source et les diff&#233;rentes classes. Etant donn&#233; le faible nombre de sources lexicales,
il est n&#233;cessaire que toutes les sources soit affect&#233;es sur un ensemble de centro&#239;des d&#233;j&#224; amorc&#233;.
L&#8217;algorithme effectue ainsi au minimum k = 2 &#215; |{sources}| it&#233;rations. Quand une source
a d&#233;j&#224; &#233;t&#233; affect&#233;e, ses &#233;l&#233;ments sont supprim&#233;s des classes avant d&#8217;&#234;tre &#224; nouveau r&#233;affect&#233;s.
L&#8217;algorithme se termine lorsqu&#8217;il y a convergence. En pratique, il est rare que la convergence
ne soit pas obtenue apr&#232;s k it&#233;rations.
</p>
<p>2.2 Probl&#232;me de l&#8217;affectation de co&#251;t minimal
</p>
<p>A chaque &#233;tape de l&#8217;it&#233;ration, l&#8217;algorithme poss&#232;de une matrice de distances entre les cat&#233;gories
et les d&#233;finitions qui doivent &#234;tre affect&#233;es. Il se pose donc le probl&#232;me de trouver l&#8217;affectation
de co&#251;t minimal : ce probl&#232;me est &#233;quivalent au probl&#232;me couplage maximum de valeur mi-
nimum dans un graphe biparti, ou encore &#224; un probl&#232;me de combinaison lin&#233;aire. La m&#233;thode
Hongroise (Kuhn, 1955) (algorithme de complexit&#233; O(n3), o&#249; n est la cardinalit&#233; de la plus
grande des deux partitions du graphe) est un cas particulier de (Ford, Fulkerson, 1956) qui
consid&#232;re de probl&#232;me dans le cas plus g&#233;n&#233;ral d&#8217;un graphe.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Fabien Jalabert, Mathieu Lafourcade
</p>
<p>3 Profondeur d&#8217;analyse et aspect multicrit&#232;re
</p>
<p>Le dernier aspect de cet algorithme et la m&#233;thode d&#8217;analyse qui permet d&#8217;obtenir les distances.
Nous l&#8217;avons vu, les imp&#233;ratifs de complexit&#233; sont moins restrictifs que ceux g&#233;n&#233;ralement
pr&#233;sent dans la fouille de donn&#233;e. Ainsi nous avons fait le choix d&#8217;une approche multicrit&#232;re
afin de pallier statistiquement les d&#233;fauts isol&#233;s inh&#233;rents &#224; chaque type d&#8217;analyse du sens.
</p>
<p>3.1 Types de crit&#232;res
</p>
<p>Chaque crit&#232;re est sollicit&#233; &#224; deux &#233;tapes diff&#233;rentes de l&#8217;algorithme pour fournir une mesure
de distance. A chaque it&#233;ration de l&#8217;algorithme, ils doivent d&#8217;une part proposer une matrice de
distance pour l&#8217;affectation des d&#233;finitions d&#8217;une sources dans les diff&#233;rentes cat&#233;gories, puis
d&#8217;autre part &#224; la fin de l&#8217;it&#233;ration permettre d&#8217;&#233;valuer les distances inter et intra-cat&#233;gories.
Les crit&#232;res sont pond&#233;r&#233;s entre eux et suivant le type de source lexicale utilis&#233;. Ces crit&#232;res
s&#8217;articulent autour de trois axes principaux :
</p>
<p>Le crit&#232;re de distance angulaire est simplement bas&#233; sur l&#8217;utilisation de l&#8217;angle entre deux
vecteurs. Ce crit&#232;re est particuli&#232;rement important dans le cas des &#8618;d&#233;finitions manuelles&#8617;,
qui sont tr&#232;s succintes, et qui sont g&#233;n&#233;ralement ins&#233;r&#233;es dans la base pour corriger et
fixer des sens.
</p>
<p>Le crit&#232;re d&#8217;analyse ad-hoc extrait le contenu lexical des d&#233;finitions en recherchant des lo-
cutions ad-hoc. Ils recherchent par exemple des informations de domaine (&#8618;m&#233;canique&#8617;,
&#8618;biologie&#8617; . . .), d&#8217;&#233;tymologie (&#8618;du latin . . . qui signifie . . . &#8617;), d&#8217;usage (&#8618;ancien&#8617;, &#8618;argot&#8617;, . . .). Il
ne s&#8217;agit pas de proposer une mesure de distance mais de proposer un bonus ou un malus
pond&#233;r&#233; en fonction de chaque cas. Ce crit&#232;re recherche aussi d&#8217;autres motifs dans le cas
de dictionnaire semi-structur&#233;s (&#224; l&#8217;aide de XML ou SGML par exemple).
</p>
<p>Le crit&#232;re d&#8217;analyse de contenu lexical extrait les similitudes entre les contenus lexicaux des
diff&#233;rentes d&#233;finitions. Il peut &#234;tre param&#233;tr&#233; avec des fonctions de fr&#233;quence et de co-
occurence de termes qui permettent apr&#232;s un apprentissage en corpus de refl&#233;ter l&#8217;usage
des termes. Ce crit&#232;re est plus amplement d&#233;taill&#233; dans le paragraphe suivant.
</p>
<p>3.2 Crit&#232;re d&#8217;analyse de contenu lexical
</p>
<p>Ce crit&#232;re compare les d&#233;finitions par la pr&#233;sence simultan&#233;e de termes. La cooccurence obtenue
est pond&#233;r&#233;e par la position et les informations morphosyntaxiques de chaque d&#233;finition. En ef-
fet, la plupart des d&#233;finitions sont d&#233;crites en genre et diff&#233;rence, c&#8217;est &#224; dire par un hyperonyme
suivi de la description souvent ordonn&#233;e des caract&#233;ristiques propres. Cependant, l&#8217;apposition
d&#8217;un compl&#233;ment (plac&#233; en d&#233;but de d&#233;finition, avant le verbe et le sujet) est fr&#233;quente dans les
d&#233;finitions et constitue souvent une forte participation au sens.
</p>
<p>Le d&#233;roulement de cette &#233;valuation est le suivant :
&#8211; A l&#8217;aide d&#8217;une analyse morphosyntaxique (Chauch&#233;, 1984), nous supprimons les termes ap-
</p>
<p>partenant au m&#233;ta-langage, les d&#233;terminants, pronoms, pr&#233;positions et plus g&#233;n&#233;ralement les
termes qui ne participent pas fortement &#224; la th&#233;matique de la d&#233;finition. Puis en fonction de
l&#8217;arbre morphosyntaxique, nous r&#233;ordonnons tous les termes restants en traitant par exemple
les appositions de compl&#233;ments, mais aussi en pla&#231;ant les gouverneurs avant les adjoints1) . . .
</p>
<p>1Par exemple dans &#8618;voile &#224; bateau&#8617; on donne plus d&#8217;importance &#224; &#8618;voile&#8617; qui est le le gouverneur (ou t&#234;te)
qu&#8217;&#224; &#8618;bateau&#8617; qui est son adjoint, tandis que dans &#8618;bateau &#224; voile&#8617;, &#8618;bateau&#8617; est favoris&#233;</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Classification automatique de d&#233;finitions en sens
</p>
<p>Enfin, une fois cette liste de termes dont l&#8217;ordre est sens&#233; refl&#233;ter la participation d&#233;croissante
&#224; la th&#233;matique, nous indi&#231;ons2 tous ces termes.
</p>
<p>&#8211; Ensuite nous appliquons une fonction d&#233;croissante de 1 &#224; 0 en fonction de la valeur de cette
indice ind : f(ind) = k1 &#8722; k2.log(ind) o&#249; k1 et k2 sont des constantes r&#233;elles.
</p>
<p>&#8211; Pour les termes qui ont plusieurs occurences dans une d&#233;finition (dans le cas o&#249; on compare
deux d&#233;finitions entre elles) ou dans un ensemble de d&#233;finitions (dans le cas o&#249; on compare
une d&#233;finition avec une classe de d&#233;finitions), on remplace les occurences multiples par une
seule dont la valeur est la somme des pr&#233;c&#233;dentes. Soient ind1..indj les indices occurences
du mots m dans une d&#233;finition, alors ces occurences sont remplac&#233;es par
</p>
<p>&#8721;j
k=1 f(indk)
</p>
<p>&#8211; Enfin, on mesure une proximit&#233; entre documents sources par une fonction qui effectue la
somme des produits des indices pond&#233;r&#233;s pour chaque terme pr&#233;sent dans les deux d&#233;finitions
</p>
<p>ou groupes de d&#233;finitions compar&#233;s : proxd1,d2 =
&#8721;
</p>
<p>t&#8712;(d1&#8745;d2)
(
</p>
<p>f( indt&#8712;d1 ) . f( indt&#8712;d2 )
)
</p>
<p>Remarques :
&#8211; Notons que cette fonction n&#8217;est pas une mesure de similarit&#233; ni de distance. Elle ne respecte
</p>
<p>notamment pas la propri&#233;t&#233; de minimalit&#233; : prox(x, x) &#2;= 0
&#8211; di est une d&#233;finition et cj est une cat&#233;gorie, on les consid&#232;re tous deux comme des ensembles
</p>
<p>de termes indic&#233;s, les indices commen&#231;ant &#224; 1 &#224; chaque d&#233;but de d&#233;finition.
&#8211; On peut proposer plusieurs variantes en fonction des choix suivants :
</p>
<p>&#8211; La position d&#8217;un terme peut &#234;tre brute c&#8217;est &#224; dire directement dans la d&#233;finition, ou peut
&#234;tre calcul&#233;e dans l&#8217;arbre morphosyntaxique.
</p>
<p>&#8211; Il est possible de comptabiliser ou non plusieurs fois un terme qui poss&#232;de de multiples
occurences dans une m&#234;me d&#233;finition ou classe de d&#233;finition.
</p>
<p>&#8211; Enfin, pour comptabiliser ces occurence multiples, on peut consid&#233;rer l&#8217;indice minimum
ou encore effectuer la somme des valeurs obtenues en appliquant f aux indices.
</p>
<p>3.3 Pond&#233;ration distributionnelle
Enfin, les r&#233;sultats pr&#233;c&#233;dents peuvent &#234;tre pond&#233;r&#233;s en ajoutant des crit&#232;res de fr&#233;quence ou de
cooccurence dans les corpus :
</p>
<p>&#8211; Plus un terme est rare dans la totalit&#233; des dictionnaires ou du corpus, plus il est discr&#233;-
minant. On peut donc choisir de pond&#233;rer la fonction de proximit&#233; de la fa&#231;on suivante :
proxfreq(d1, d2) =
</p>
<p>&#8721;
t&#8712;(d1&#8745;d2)
</p>
<p>(
f( it&#8712;d1 ) . f( it&#8712;d2 )
</p>
<p>log(freq(t&#8712;d1)) . log(freq(t&#8712;d2))
)
</p>
<p>&#8211; La cooccurence permet de pond&#233;rer le r&#233;sultat obtenu par les d&#233;finitions en tenant compte de
l&#8217;usage de la langue dans les corpus. Plusieurs m&#233;thode d&#8217;application sont possibles :
&#8211; La premi&#232;re consiste &#224; pond&#233;rer l&#8217;importance des termes en valorisant ceux pr&#233;sents qui
</p>
<p>sont corr&#233;l&#233;s avec le mot que l&#8217;on souhaite d&#233;finir. Ainsi, soit m un terme, d1 et d2 deux
d&#233;finitions de ce terme que l&#8217;on souhaite comparer et tk les termes pr&#233;sents dans ces d&#233;fi-
</p>
<p>nitions : proxcoocc1(d1, d2) =
&#8721;
</p>
<p>t&#8712;(d1&#8745;d2)
(
</p>
<p>f(it&#8712;d1) . f(it&#8712;d2) . coocc(t,m)
)
</p>
<p>&#8211; On peut d&#8217;autre part &#233;tendre la comparaison en comparant tous les termes des d&#233;finitions et
en pond&#233;rant par la cooccurence. La similarit&#233; ne d&#233;pend plus alors de la pr&#233;sence simul-
tan&#233;e d&#8217;un m&#234;me terme dans les deux d&#233;finitions mais de la pr&#233;sence dans les d&#233;finitions
de termes s&#233;mantiquement proches :
</p>
<p>proxcoocc2(d1, d2) =
&#8721;
</p>
<p>t1&#8712;d1,t2&#8712;d2)
(
</p>
<p>f(it1) . f(it2) . coocc(t1,m) . coocc(t2,m)
)
</p>
<p>&#8211; Enfin il est possible de conjuguer les deux crit&#232;res par une pond&#233;ration suppl&#233;mentaire de
la seconde formule avec m, par exemple :
</p>
<p>proxcoocc3(d1, d2) =
&#8721;
</p>
<p>t1&#8712;d1,t2&#8712;d2)
(
</p>
<p>f(it1) . f(it2) . coocc(t1,m) . coocc(t2,m) . coocc(t1, t2)
)
</p>
<p>2Par ordre croissant en commen&#231;ant &#224; 1.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Fabien Jalabert, Mathieu Lafourcade
</p>
<p>Ces crit&#232;res pr&#233;sentes des r&#233;sultats encourageants, mais une &#233;tude comparative serait n&#233;ces-
saire afin de d&#233;terminer les param&#232;tres ad&#233;quats et les m&#233;thodes les plus performantes ou
compl&#233;mentaires.
</p>
<p>R&#233;sultats et conclusion
</p>
<p>Nous avons pr&#233;sent&#233; dans cet article une nouvelle m&#233;thode permettant de cat&#233;goriser les d&#233;fi-
nitions provenant de multiples sources lexicales afin d&#8217;obtenir des sens. Les r&#233;sultats obtenus
sont assez encourageants, et plus particuli&#232;rement, la m&#233;thode d&#8217;analyse pond&#233;r&#233;e par la cooc-
curence de termes. Cependant de nombreux travaux restent encore n&#233;cessaires : la classification
est efficace pour une bonne proportion du lexique, mais s&#8217;av&#232;rent moins pertinente pour des
termes poss&#233;dant un grand nombre de sens, qui sont moins nombreux mais tr&#232;s fr&#233;quents (les
sens sont trop nombreux et insuffisamment d&#233;marqu&#233;s). L&#8217;observation des erreurs commises
montre que les interversions rel&#232;vent souvent de cliques de sens et notre r&#233;ponse &#224; ce probl&#232;me
s&#8217;oriente donc actuellement &#224; r&#233;duire le nombre de sens et fusionner les multiples classes.
</p>
<p>Mais ceci pose de nouveaux probl&#232;mes : avant tout, r&#233;duire le nombre de sens implique que
plusieurs d&#233;finitions d&#8217;une m&#234;me source peuvent &#234;tre affect&#233;es &#224; une m&#234;me classe, dont les
cons&#233;quences peuvent &#234;tre un perte d&#8217;efficacit&#233; globale. Enfin, les m&#233;thodes na&#239;ves statistiques
que nous avons mises en jeu pour d&#233;tecter et choisir le nombre de sens n&#8217;offrent pas de r&#233;sultats
satisfaisants. Cependant, malgr&#233; toutes ces difficult&#233;s, l&#8217;obtention de sens si imparfaite qu&#8217;elle
est actuellement s&#8217;av&#232;re b&#233;n&#233;fique pour l&#8217;apprentissage de la base vectorielle. L&#8217;am&#233;lioration
de cette derni&#232;re laisse pr&#233;voir r&#233;ciproquement un impact positif sur la cat&#233;gorisation.
</p>
<p>R&#233;f&#233;rences
</p>
<p>C.J. Alpert, A.B. Kahng Recent Directions in Netlist Partitioning : A Survey Integration : VLSI
J., vol. 19, 1995, 93 pp.
</p>
<p>P. Berkhin Survey of clustering data mining techniques Accrue Software Research Paper, 56
pp.
</p>
<p>C.J.C. Burges A Tutorial on Support Vector Machines for Pattern Recognition Journal : Data
Mining and Knowledge Discovery, vol. 2, number 2, pp. 121-167, 1998.
</p>
<p>J. Chauch&#233; Un outil multidimensionnel de l&#8217;analyse du discours Coling&#8217;84, Stanford, July
1984
</p>
<p>L.R. Ford, D.R. Fulkerson Maximal Flow through a Network Candidan Journal of Mathema-
tics,p. 399, 1956.
</p>
<p>H.W. Kuhn The hungarian method for the assignment problem Naval Res. Logist. Quart.,
pages 83&#8211;98, 1955.
</p>
<p>M. Lafourcade, V. Prince, D. Schwab Vecteurs conceptuels et structuration &#233;mergente de ter-
minologies Revue TAL Volume 43 - n 1/2002, pages 43 &#224; 72
</p>
<p>V. Vapnik, A. Chervonenkis A note on one class of perceptrons Journal Automatic and Remote
Control, vol. 25, 1964.</p>

</div></div>
</body></html>