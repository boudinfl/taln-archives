TALN 2004, Session Poster, Fès, 19–21 avril 2004
NLP Applications Based on Weighted Multi-Tape Automata
André Kempe
Xerox Research Centre Europe – Grenoble Laboratory
6 chemin de Maupertuis – 38240 Meylan – France
andre.kempe@xrce.xerox.com – www.xrce.xerox.com/people/kempe/
Abstract
This article describes two practical applications of weighted multi-tape automata (WMTAs) in
Natural Language Processing, that demonstrate the augmented descriptive power of WMTAs
compared to weighted 1-tape and 2-tape automata. The two examples concern the preserva-
tion of intermediate results in transduction cascades and the search for similar words in two
languages. As a basis for these applications, the article proposes a number of operations on
WMTAs. Among others, it (re-)defines multi-tape intersection, where a number of tapes of
one WMTA are intersected with the same number of tapes of another WMTA. In the proposed
approach, multi-tape intersection is not an atomic operation but rather a sequence of more ele-
mentary ones, which facilitates its implementation.
Keywords
finite-state automaton, weighted multi-tape automaton, transduction cascade, lexicon
1 Introduction
Finite state automata (FSAs) and weighted finite state automata (WFSAs) are widely used in
language and speech processing (Kaplan & Kay, 1981; Mohri, 1997; Beesley & Karttunen,
2003). They permit, among others, the fast processing of input strings and can be easily modi-
fied and combined by well defined operations. Most systems and applications deal with 1-tape
and 2-tape automata, also called acceptors and transducers, respectively. Multi-tape automata
(MTAs) (Elgot & Mezei, 1965) offer additional advantages such as storing different types of
information on different tapes. MTAs have been implemented and used, e.g., in the morpholog-
ical analysis of Semitic languages, where the vowels, consonants, pattern, and surface form of
words have been represented on different tapes (Kay, 1987; Kiraz & Grimley-Evans, 1998).
This article describes two practical applications of weighted multi-tape automata (WMTAs)
and MTAs in Natural Language Processing (NLP). The first example shows how intermediate
results can be preserved in transduction cascades so that they can be accessed by any of the
following transductions (Sec. 4.1). The second one deals with the search for words that are
similar in two languages, and in particular in French and Spanish (Sec. 4.2). To support these
applications, the article defines WMTAs (Sec. 2) and some WMTA operations (Sec. 3). Efficient
algorithms for the these operations have been implemented in the WFSC toolkit (Kempe et al.,
2003) and will be presented in future publications.
André Kempe
2 Weighted Multi-Tape Automata
In the following we build on basic definitions of a monoid, a semiring, a weighted automaton,
and a multi-tape automaton (Elgot & Mezei, 1965; Eilenberg, 1974; Kuich & Salomaa, 1986;
Mohri et al., 1998) which we do not recall for reasons of space.
A weighted multi-tape automaton (WMTA), A(n), also called weighted n-tape automaton, over
a semiring K is defined as a six-tuple
A(n) =def 〈Σ, Q, I, F, E
(n),K〉 (1)
with Σ being a finite alphabet, Q the finite set of states, I⊆Q the set of initial states, F ⊆Q the
set of final states, n the arity, i.e., the number of tapes in A(n), E(n)⊆ (Q × (Σ∗)n × K×Q)
the finite set of n-tape transitions, and K = 〈K,⊕,⊗, 0̄, 1̄〉 the semiring of weights. For any
state q ∈ Q, we denote by λ(q) ∈ K its initial weight and by (q) ∈ K its final weight. For
any transition e(n) ∈ E(n), e(n) = 〈p, (n), w, n〉, we denote by p(e(n)) ∈ Q its source state, by
w(e(n))∈K its weight, by n(e(n))∈Q its target state, and by (e(n)) its label which is an n-tuple
of strings,  : E(n) → (Σ∗)n. A path π(n) of length r = |π(n)| is a sequence of transitions
(n) (n) (n) (n)
e1 e2 · · · er such that n(ei ) = p(
(n)
ei+1), ∀i ∈ [[1, r−1]]. A path is said to be successful iff
(n)
p(e1 )∈I and
(n)
n(er )∈F . Its label (π(n)) is an n-tuple of strings and equals the concatenation
of the labels of its transitions:
(π(n)) = s(n) = 〈s1, s2, . . . , sn〉 =
n)
( (e1 ) (
(n)
e2 ) · · · (e
(n)
r ) (2)
Its weight w(π(n)) is ⎛ ( )⎞
(n) (n) ( )
w(π(n)) = λ(p(e ⎝
⊗
1 ))⊗ w e ⎠j ⊗ (n e(n)r ) (3)
j=[[1,r]]
We denote by Π(A(n)) the set of successful paths of A(n) and by R(n) = R(A(n)) the n-tape
relation of A(n). It is the set of n-tuples of strings s(n) having successful paths in A(n):
R(A(n)) = { s(n) | ∃π(n)∈Π(A(n)) ∧ (π(n))=s(n) } (4)
The weight of any s(n) ∈R(A(n)) is the semirin⊕g sum of the weights of all paths labeled withs(n) :
w(s(n)) = w(π(n)) (5)
π(n) | (π(n))=s(n)
3 Operations
Pairing and Concatenation: We define the pairing of two string tuples, s(n) :v(m)=u(n+m), as
〈s1, . . . , sn〉 : 〈v1, . . . , vm〉 =def 〈s1, . . . , sn, v1, . . . , vm〉 (6)
w ( 〈s1, . . . , sn〉 : 〈v1, . . . , vm〉 ) =def w ( 〈s1, . . . , sn〉 )⊗ w ( 〈v1, . . . , vm〉 ) (7)
The concatenation of two string tuples of equal arity, s(n)v(n) = u(n), is defined as
〈s1, . . . , sn〉〈v1, . . . , vn〉 =def 〈s1v1, . . . , snvn〉 (8)
w ( 〈s1, . . . , sn〉〈v1, . . . , vn〉 ) =def w ( 〈s1, . . . , sn〉 )⊗ w ( 〈v1, . . . , vn〉 ) (9)
Projection and Complementary Projection: Projection, Pj,k,...(s(n)), of a string tuple is
defined as Pj,k,...( 〈s1, . . . , sn〉 ) =def 〈sj , sk, . . .〉 (10)
It retains only those strings (i.e., tapes) of the tuple that are specified by the indices j, k, . . . ∈
[[1, n]], and places them in the specified order. The weight of the tuple is not modified (if we
NLP Applications Based on Weighted Multi-Tape Automata
consider it not as a member of a relation). The projection of an n-tape relation is the projection
of all its string tuples:
Pj,k,...(R
(n)) =def { v
(m) | ∃s(n)∈R(n) ∧ Pj,k,...(s
(n))=v(m) } (11)
The weight of each v(m)∈Pj,k,...(R(n)) is the semiring sum of the weights of each s(n) ∈R(n)
leading, when projected, to v(m): ⊕
w(v(m)) =def w(s
(n)) (12)
s(n) |P n)j,k,...(s( )=v(m)
Complementary projection,Pj,k,...(s(n)), removes those strings of the tuple s(n) that are specified
by the indices j, k, . . . ∈ [[1, n]]. It is defined as
Pj,k,...( 〈s1, . . . , sn〉 ) =def 〈. . . , sj−1, sj+1, . . . , sk−1, sk+1, . . .〉 (13)
Pj,k,...(R
(n)) =def { v
(m)
⊕| ∃s
(n)∈R(n) ∧ Pj,k,...(s
(n))=v(m) } (14)
w(v(m)) =def w(s
(n)) (15)
s(n) |Pj,k,...(s(n))=v(m)
Cross-Product: The cross-product of two n-tape relations is based on pairing and is defined
as (n) (m) )
R1 ×R2 =def { s
(n) : v(m) | s(n) (n)∈ R1 , v
(m) (m∈ R2 } (16)
Auto-Intersection: We define the auto-intersection of a relation, Ij,k(R(n)), on the tapes j and
k as the subset of R(n) that contains all s(n) with equal sj and sk:
Ij,k(R
(n)) =def {s
(n)∈R(n) | sj = sk} (17)
The weight of any s(n)∈Ij,k(R(n)) is not modified. For example (Figure 1)
(3)
R1 = 〈a, x, ε〉 〈b, y, a〉
∗ 〈ε, z, b〉 = {〈abk, xykz, akb〉 | k∈N} (18)
I1,3(R
(3)
1 ) = {〈ab
1, xy1z, a1b〉} (19)
Α(3) b:y:a1 Α(3)
a:x:ε ε :z:b a:x:ε b:y:a ε :z:b
(a) 0 1 2 (b) 0 1 2 3
Figure 1: (a) A WMTA (3)A1 and (b) its auto-intersection
(3)
A(3) = I1,3(A1 ). (Weights omitted)
Single-Tape and Multi-Tape Intersection: Multi-tape intersection of two relations, R(n)1 and
R(m)2 , uses r tapes in each relation, and i(ntersects them pair-wise. We (re-)define it as )
R(n) (m)1 ∩ R2 =def Pn+k1 ,...,n+kr Ijr ,n+kr ( · · · Ij1,n+k1 ( R
(n)
1 ×R
(m)
2 ) · · · ) (20)
j1, k1
. . .
jr, kr
The operation pairs each s(n) (n)∈ R1 with each
m)
v(m)
(
∈ R2 iff sj1 = vk1 until sjr = vkr . We
speak about single-tape intersection if only one tape is used in each relation (r =1). All tapes
ki of R
(m)
2 that are used in the intersection are afterwards equal to the tapes ji of R
(n)
1 , and are
removed. The operation is conceptually similar to composition of two relations, except that the
tapes ji are preserved and hence can be (re-)used in subsequent operations. The result is
R(n+m−r) = { u(n+m−r) | ∃s(n)∈R(n)1 ∧ ∃v
(m)∈R(m)2 ∧ sj =v ∈ [[1, r]]i k , ∀ii
∧ u(n+m−r) =Pn+k1 ,...,n+kr (s
(n):v(m)) } (21)
w(u(n+m−r)) = w(s(n))⊗ w(v(m)) (22)
André Kempe
Although single-tape and multi-tape intersection include complementary projection, Eq. 22 is
not in conflict with Eq. 15 because any two u(n+m) = s(n):v(m) that differ in vk , differ also ini
sj , and hence cannot become equal when the vi k are removed.i
Two well-known special cases are the intersection of two acceptors leading to an acceptor, and
the composition of two transducers leading to a transduce(r: )
(1) (1) (1) (1) (1) (1)
A1 ∩A2 = A1 ∩ A2 = P2 I1,2( A1 ×A2 ) (23)
1,1 ( )
(2) (2) (2) (2) (2) (2)
A1 A2 = P2( A1 ∩ A2 ) = P2,3 I2,3( A1 × A2 ) (24)
2,1
4 Applications
4.1 Preserving Intermediate Transduction Results
Transduction cascades are frequently used in language and speech processing. In a (classical)
weighted transduction cascade, (2) (2)T1 . . . Tr , a set of weighted input strings, encoded as a
weighted acceptor, (1)L0 , is composed with the first transducer,
(2)
T1 , on its input tape (Figure 2).
The output projection of this composition is the first intermediate result, (1)L1 , of the cascade.
It is further composed with the second transducer, (2)T2 , which leads to the second intermediate
result, (1)L2 , etc. The output projection of the last transducer is the final result,
(1)
Lr :
(1)
Li =
(2)
P2(
(1)
Li−1  Ti ) for i ∈ [[1, r]] (25)
At any point in the cascade, previous results cannot be accessed.
(2) (2) (2)
(1) T 1 (1) T 2 (1) T (1)L 0 L 1 L
r
r−1 L r
tape  1 tape  1 tape  1
. . . . .
tape  2 tape  2 tape  2
Figure 2: Weighted transduction cascade (classical)
In a weighted transduction cascade, (n (nA 1) r)1 . . . Ar , that uses WMTAs and multi-tape inter-
section, intermediate results can be preserved and (re-)used by all subsequent transductions.
Suppose, we want to use the two previous results at each point in the cascade (except in the first
transduction) which requires all intermediate results, (2)Li , to have two tapes (Figure 3) :
(2) ) (2)
L1 =
(1
L0 ∩ A1 (26)
1,1
(2) (2) (3)
Li = P2,3( Li−1 ∩ Ai ) for i ∈ [[2, r−1]] (27)
1, 1
2, 2
(2)
L(2)r = P3( Lr−1 ∩ A
(3)
r ) (28)
1, 1
2, 2
This augmented descriptive power is also available if the whole cascade is intersected into a sin-
gle WMTA, A(2) (although A(2) has only two tapes in our example). Each of the “incorporated”
multi-tape sub-relations in A(2) (except the first one) will still refer to its two predecessors:
(3) (3)
A1...i = P1,n−1,n(
(m)
A1...i−1 ∩ Ai ) for i ∈ [[2, r]] , m ∈ {2, 3} (29)
n−1, 1
n, 2
A(2) = P1,n( A1...r ) (30)
NLP Applications Based on Weighted Multi-Tape Automata
(2) (3) (3)
(1) A 1 (2) A (2) A (1)L 0 L
2
1 L
r
r−1 L r
tape  1 tape  1
tape  1 tape  2 . . . . . tape  2
tape  2 tape  3 tape  3
Figure 3: Weighted transduction cascade using multi-tape intersection
4.2 Extracting Similar Words in French and Spanish
To extract, in general, from a relation R(n)1 all string tuples s
(n) whose strings sj1 to sjr are
similar to its strings sk1 to skr , respectively, we can compare each pair of tapes, ji and ki,
independently from all other pairs. Hence the task can be reduced to comparing two tapes, j
and k. This can be done by means of a weighted 2-tape relation, (2)RS , that describes the required
similarity between tape j and n)k of (R1 :
(n) n) (2)
R2 =
(
R1 ∩ RS (31)
j,1
k, 2
For example, if we have an French-Spanish 3-tape lexicon, FrEs(3), with entries of the form
s(3) = 〈FrenchWord, SpanishWord,PosTag〉, and want to find all words that are similar in
the two languages, we create a 2-tape automaton, S(2), describing this similarity. For that
we compile a WMTA (2)Gc that encodes various synchronic consonant correspondences,
(2)
Gv
and (2)Gvfin that describe alternations between sequences of vowels, and
(2)
Gd that admits any
diacritization (insertion of accents, cedille, tilde, etc.) :
G(2)c = {b :v} ∪ {ph :f} ∪ {ch :c} ∪ {qu :c} ∪ . . . (32)
G(2)v = A
(1)+
v × A
(1)+
v with A
(1)
v = {a} ∪ {e} ∪ {i} ∪ {o} ∪ {u} ∪ {y} (33)
(2)
Gvfin = A
(1)∗
v × A
(1)∗
v (34)
(2)
Gd = {a : à} ∪ {a : â} ∪ . . . {e : é} ∪ {e : è} ∪ . . . {n :~n} ∪ {c :ç} ∪ . . . (35)
From these sub-relations we compile S(2) describing the relation between any (hypothetical)
French word and its potential Spanis(h form:
1
)
(2)
S(2) = ( (G   +d )
−1 ∪ ? i? )  ( G
(2)
c ∪G
(2)
v ∪ ?
 
i? )
+ (2)Gvfin  (
(2)
Gd ∪ ?
 
i? )
+ (36)
To extract similar words with equal meaning from FrEs(3), we intersect it on the tapes of French
and Spanish with S(2): (3)
FrEssim = FrEs
(3) ∩ S(2) (37)
1, 1
2, 2
For better illustration, we explain this approach on a tiny example: a French-Spanish lexicon
containing only the four entries
R(FrEs(3)) = {〈chanter, cantar, VB〉, 〈manger, comer, VB〉, 〈piquer, mangar, VB〉, 〈piquer, picar, VB〉}
With the above approach (Eq. 37), we can extract a sub-lexicon of similar words with equal
meaning: (3)
R(FrEssim) = {〈chanter, cantar, VB〉, 〈piquer, picar, VB〉}
Classical composition cannot accomplish this task, even if we had only a 2-tape lexicon FrEs(2).
1Here ? means any symbol, i.e., ? ∈ {a, b, c, . . .}, and   i is an identity pairing such that (?
 
i?) ∈
{a :a, b :b, c :c, . . .}, whereas (?: ?)∈{a :a, a :b, b :a, . . .}.
André Kempe
For example (2)
D1 = P1(FrEs
(2))  S(2)  P2(FrEs
(2)) (38)
(2)
R(D1 ) = {〈chanter, cantar〉, 〈manger, mangar〉, 〈piquer, picar〉}
From a full-size French-Spanish lexicon with 45,578 entries, generated from lexical data from
ELRA, we extracted 5,624 similar entries, containing among others
〈blanche, blanca, S〉 〈cheval, caballo, S〉 〈oeuvre, obra, S〉
〈brusque, brusco, ADJ〉 〈grumeau, grumo, S〉 〈ouvrier, obrero, S〉
〈approuver, aprobar, V〉 〈nid, nido, S〉 〈poire, pera, S〉
〈chaleur, calor, S〉 〈noeud, nudo, S〉 〈pont, puente, S〉
〈chanter, cantar, V〉 〈oeil, ojo, S〉 〈trois, tres, NUM〉
5 Conclusion
We have described two practical applications of WMTAs and MTAs in NLP, demonstrating
their augmented descriptive power compared to 1-tape and 2-tape automata, namely the preser-
vation of intermediate results in transduction cascades and the search for similar words in two
languages. None of these tasks can be accomplished with 1-tape or 2-tape automata, in general.
We recalled some basic operations for WMTAs and MTAs and proposed some others such as
auto-intersection of one WMTA and multi-tape intersection of two WMTAs. In our approach,
multi-tape intersection is not an atomic operation but rather a sequence of more elementary
ones, which facilitates its implementation.
Acknowledgments I wish to thank Kenneth R. Beesley, Jean-Marc Champarnaud,
Franck Guingne, and Florent Nicart for their help.
References
BEESLEY K. R. & KARTTUNEN L. (2003). Finite State Morphology. Palo Alto, CA, USA: CSLI
Publications.
EILENBERG S. (1974). Automata, Languages, and Machines, volume A. San Diego, CA, USA: Aca-
demic Press.
ELGOT C. C. & MEZEI J. E. (1965). On relations defined by generalized finite automata. IBM Journal
of Research and Development, 9, 47–68.
KAPLAN R. M. & KAY M. (1981). Phonological rules and finite state transducers. In Winter Meeting
of the Linguistic Society of America, New York, NY, USA.
KAY M. (1987). Nonconcatenative finite-state morphology. In Proc. 3rd Int. Conf. EACL, p. 2–10.
KEMPE A., BAEIJS C., GAÁL T., GUINGNE F. & NICART F. (2003). WFSC – A new weighted finite
state compiler. In O. H. IBARRA & Z. DANG, Eds., Proc. 8th Int. Conf. CIAA, volume 2759 of Lecture
Notes in Computer Science, p. 108–119, Santa Barbara, CA, USA: Springer Verlag.
KIRAZ G. A. & GRIMLEY-EVANS E. (1998). Multi-tape automata for speech and language systems:
A prolog implementation. In D. WOODS & S. YU, Eds., Automata Implementation, number 1436 in
Lecture Notes in Computer Science. Springer Verlag.
KUICH W. & SALOMAA A. (1986). Semirings, Automata, Languages. Number 5 in EATCS Mono-
graphs on Theoretical Computer Science. Springer Verlag.
MOHRI M. (1997). Finite-state transducers in language and speech processing. Computational Linguis-
tics, 23(2), 269–312.
MOHRI M., PEREIRA F. C. N. & RILEY M. (1998). A rational design for a weighted finite-state
transducer library. Lecture Notes in Computer Science, 1436, 144–158.
