<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
"http://www.w3.org/TR/html4/loose.dtd">
<html><head><title>SibyMot : Mod&#233;lisation stochastique du langage int&#233;grant la notion de chunks</title>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
</head>
<body>
<div style="page-break-before:always; page-break-after:always"><div><p>TALN 2004, Session Poster, F&#232;s, 19-21 avril 2004
</p>
<p>SibyMot : Mod&#233;lisation stochastique du langage
</p>
<p>int&#233;grant la notion de chunks
1
</p>
<p>Igor Schadle, Jean-Yves Antoine, Brigitte Le P&#233;v&#233;dic, Franck Poirier
</p>
<p>Laboratoire VALORIA, Universit&#233; de Bretagne Sud (EA 2593)
</p>
<p>(igor.schadle@univ-ubs.fr)
</p>
<p>R&#233;sum&#233; &#8211; Abstract
</p>
<p>Cet article pr&#233;sente le mod&#232;le de langage d&#233;velopp&#233; pour le syst&#232;me Sibylle, un syst&#232;me
</p>
<p>d&#8217;aide &#224; la communication pour les personnes handicap&#233;es. L&#8217;utilisation d&#8217;un mod&#232;le de
</p>
<p>langage permet d&#8217;am&#233;liorer la pertinence des mots propos&#233;s en tenant compte du contexte
</p>
<p>gauche de la saisie en cours. L&#8217;originalit&#233; de notre mod&#232;le se situe dans l&#8217;int&#233;gration de la
</p>
<p>notion de chunks afin d&#8217;&#233;largir la taille du contexte pris en compte pour l&#8217;estimation de la
</p>
<p>probabilit&#233; d&#8217;apparition des mots.
</p>
<p>We present in this article the language model of Sibyl, a new Alternative and Augmentative
</p>
<p>Communication (AAC) system. The use of language modeling improves the relevance of
</p>
<p>displayed words by taking into account the left context of the current sentence. The originality
</p>
<p>of our model is to introduce chunking. This enlarges the context taken into account to estimate
</p>
<p>the words probability.
</p>
<p>Mots Cl&#233;s &#8211; Keywords
</p>
<p>Aide &#224; la communication, mod&#233;lisation stochastique du langage, n-gramme, chunks.
</p>
<p>AAC, stochastic language modeling, n-gram, chunks.
</p>
<p>1 Handicap et communication
</p>
<p>On appelle syst&#232;me d&#8217;aide &#224; la communication tout syst&#232;me visant &#224; suppl&#233;er ou restaurer, ne
</p>
<p>serait-ce que partiellement, la fonction de communication d&#8217;une personne handicap&#233;e. Dans le
</p>
<p>cas d&#8217;un handicap physique lourd (troubles de la parole et facult&#233;s motrices r&#233;duites), les
</p>
<p>modalit&#233;s de communication sont limit&#233;es. Sur ordinateur, une solution consiste &#224; composer
</p>
<p>les messages &#224; l&#8217;aide d&#8217;un clavier simul&#233; (clavier pr&#233;sent&#233; &#224; l&#8217;&#233;cran) via une interface adapt&#233;e.
</p>
<p>Lorsque l&#8217;interface d&#8217;acc&#232;s n&#8217;autorise que l&#8217;&#233;quivalent du simple clic, comme le bouton
</p>
<p>poussoir, la saisie sur clavier simul&#233; est r&#233;alis&#233;e par un syst&#232;me de d&#233;filement automatique.
</p>
<p>                                                
</p>
<p>1
 Activit&#233;s de recherche financ&#233;es par le Conseil R&#233;gional de Bretagne</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Schadle, Antoine, Le P&#233;v&#233;dic, Poirier
</p>
<p>Un curseur met en &#233;vidence les lettres une &#224; une, &#224; intervalle r&#233;gulier, et l&#8217;utilisateur n&#8217;a plus
</p>
<p>qu&#8217;&#224; valider lorsque le curseur pointe sur la lettre d&#233;sir&#233;e. L&#8217;inconv&#233;nient majeur de ces aides
</p>
<p>est l&#8217;extr&#234;me lenteur d&#8217;&#233;criture. L&#224; o&#249; la communication orale permet un d&#233;bit de l&#8217;ordre de
</p>
<p>150 mots &#224; la minute, ces aides ne permettent qu&#8217;une &#233;criture autour de 5 mots &#224; la minute.
</p>
<p>Pour accro&#238;tre cette vitesse, le syst&#232;me Sibylle, d&#233;velopp&#233; au laboratoire VALORIA de
</p>
<p>l&#8217;Universit&#233; de Bretagne Sud, propose deux aides compl&#233;mentaires. La premi&#232;re, SibyLettre
</p>
<p>est un syst&#232;me de pr&#233;diction de lettre qui permet une s&#233;lection plus rapide des lettres (Schadle
</p>
<p>et al., 2001). La deuxi&#232;me, SibyMot, est un syst&#232;me de pr&#233;diction de mot. Le syst&#232;me affiche
</p>
<p>une liste de mots, mots consid&#233;r&#233;s comme les plus probables en fonction du contexte gauche
</p>
<p>de la phrase. En s&#233;lectionnant les mots dans la liste, l&#8217;utilisateur &#233;vite leur saisie compl&#232;te.
</p>
<p>Comme d&#8217;autres syst&#232;mes de communication issus de la recherche, HandiAS (Maurel, Le
</p>
<p>P&#233;v&#233;dic, 2001) ou VITIPI (Boissi&#232;re, 2000), le syst&#232;me Sibylle utilise un mod&#232;le de langage
</p>
<p>avanc&#233; pour &#233;tablir une liste de mots pertinente. Ce mod&#232;le est le sujet de ce pr&#233;sent article.
</p>
<p>2 Id&#233;es et principes
</p>
<p>2.1 Mod&#233;lisation n-gramme
</p>
<p>Le point de d&#233;part de notre mod&#232;le est le mod&#232;le statistique n-gramme gramme utilis&#233; dans le
</p>
<p>cadre de la mod&#233;lisation probabiliste du langage et issu de la th&#233;orie de l&#8217;information (Jelinek,
</p>
<p>1976). Dans l&#8217;objectif de pr&#233;dire des mots, le probl&#232;me principal de ce mod&#232;le est de ne tenir
</p>
<p>compte que des derniers mots. De nombreuses variations ont &#233;t&#233; propos&#233;es pour am&#233;liorer
</p>
<p>l&#8217;utilisation de ce contexte. Cependant, malgr&#233; ces am&#233;liorations, le contexte reste &#224; tr&#232;s
</p>
<p>courte distance, de l&#8217;ordre de deux &#224; trois mots. Pour accro&#238;tre la taille du contexte et capter de
</p>
<p>mani&#232;re plus efficace les d&#233;pendances &#224; plus longue distance, le mod&#232;le pr&#233;sent&#233;, et c&#8217;est son
</p>
<p>originalit&#233;, propose d&#8217;int&#233;grer la notion de chunks.
</p>
<p>2.2 Analyse en chunks
</p>
<p>L&#8217;analyse en chunks consiste &#224; d&#233;composer une phrase en syntagmes minimaux non r&#233;cursifs.
</p>
<p>Par rapport &#224; l&#8217;analyse syntaxique, l&#8217;analyse en chunks se distingue par le fait qu&#8217;elle ne
</p>
<p>cherche ni &#224; donner les fonctions syntaxiques des syntagmes, ni &#224; en &#233;tablir les d&#233;pendances.
</p>
<p>(Abney, 1991) a utilis&#233; les chunks comme &#233;tape pr&#233;liminaire &#224; l&#8217;analyse syntaxique. Depuis,
</p>
<p>la notion de chunks a largement &#233;t&#233; r&#233;utilis&#233;e en linguistique informatique : pour la
</p>
<p>reconnaissance de la parole, l&#8217;analyse syntaxique, la compr&#233;hension de la parole, etc. Dans le
</p>
<p>cadre de notre mod&#232;le, nous utilisons les chunks pour l&#8217;estimation du mot &#224; venir. Ici, l&#8217;int&#233;r&#234;t
</p>
<p>de cette analyse est de structurer le contexte gauche du mot &#224; pr&#233;dire. En particulier, avec les
</p>
<p>t&#234;tes des chunks (leur mot principal), elle permet de mettre en avant des mots pertinents pour
</p>
<p>la pr&#233;diction. Ainsi, au contexte des n-1 derniers mots, nous associons un contexte des n-1
</p>
<p>derni&#232;res t&#234;tes de chunks. Sur l&#8217;exemple du d&#233;but de phrase : &#171; [l&#8217;ann&#233;e*] [du dragon*] [a &#8230;
</p>
<p>(commenc&#233;) &#187;, le contexte tri-gramme pour le mot &#171; commenc&#233; &#187; est &#171; dragon a &#187;, tandis que
</p>
<p>le contexte consid&#233;rant les deux derni&#232;res t&#234;tes de chunks est &#171; ann&#233;e dragon &#187;.  Ce dernier
</p>
<p>fait appara&#238;tre le mot &#171; ann&#233;e &#187;, un bon pr&#233;dicteur pour le mot &#171; commenc&#233; &#187;. Les chunks
</p>
<p>permettent donc de capter des d&#233;pendances &#224; plus longue distance que le mod&#232;le n-gramme,
</p>
<p>tout en restant dans le cadre de la mod&#233;lisation robuste du langage.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>SibyMot : Mod&#233;lisation stochastique du langage int&#233;grant la notion de chunks
</p>
<p>2.3 Les lemmes
</p>
<p>Une autre des difficult&#233;s du mod&#232;le n-gramme est li&#233;e &#224; l&#8217;espace des param&#232;tres &#224; estimer. Si
</p>
<p>V est la taille du vocabulaire, alors le nombre de param&#232;tres est de l&#8217;ordre de V
N
. Relativement
</p>
<p>&#224; l&#8217;anglais, ce probl&#232;me est accru en fran&#231;ais par sa richesse flexionnelle. (Cerf Danon, El-
</p>
<p>B&#232;ze, 1991) donnent un rapport formes fl&#233;chies/lemme de 2 en anglais contre 7 en fran&#231;ais.
</p>
<p>Nous avons donc privil&#233;gi&#233; le lemme comme unit&#233; lexicale, la probabilit&#233; d&#8217;une forme fl&#233;chie
</p>
<p>&#233;tant exprim&#233;e comme la probabilit&#233; combin&#233;e du lemme et de la flexion.
</p>
<p>3 Mod&#233;lisation
</p>
<p>Apr&#232;s avoir expos&#233; les id&#233;es principales de notre mod&#232;le, nous allons maintenant d&#233;crire de
</p>
<p>mani&#232;re plus d&#233;taill&#233;e son fonctionnement. Pour permettre une pr&#233;diction fond&#233;e sur les
</p>
<p>chunks, SibyMot est compos&#233; de deux modules : un analyseur charg&#233; de construire une
</p>
<p>repr&#233;sentation de la phrase en chunks et un pr&#233;dicteur qui d&#233;livre la probabilit&#233; d&#8217;apparition
</p>
<p>des mots du lexique. Rappelons que dans le cadre de l&#8217;application Sibylle, le mod&#232;le est
</p>
<p>utilis&#233; en mot &#224; mot. Les deux &#233;tapes analyse et pr&#233;diction sont ind&#233;pendantes et, en
</p>
<p>particulier, l&#8217;analyseur peut &#234;tre utilis&#233; seul pour des t&#226;ches d&#8217;&#233;tiquetage et de segmentation.
</p>
<p>3.1 Partie analyse
</p>
<p>En ce qui concerne l&#8217;analyseur, la segmentation d&#8217;un &#233;nonc&#233; en chunks correspond en TAL &#224;
</p>
<p>une analyse de surface (shallow parsing). Dans notre syst&#232;me, l&#8217;analyse est charg&#233;e de
</p>
<p>d&#233;terminer pour chaque mot son lemme et son &#233;tiquette grammaticale. De plus, au niveau du
</p>
<p>chunk, elle d&#233;livre l&#8217;&#233;tiquette associ&#233;e au chunk (sa cat&#233;gorie grammaticale) ainsi qu&#8217;une
</p>
<p>flexion qui correspond &#224; celle de la t&#234;te. L&#8217;analyse en elle-m&#234;me est d&#233;compos&#233;e en deux
</p>
<p>&#233;tapes : une &#233;tape d&#8217;&#233;tiquetage des mots puis une &#233;tape de segmentation (figure 1).
</p>
<p>l&#8217;ann&#233;e du dragon a
</p>
<p>&#201;tiquetage
</p>
<p>l&#8217;/l&#8217;/det ann&#233;e/ann&#233;e/nom du/du/pre dragon/dragon/nom a/avoir/aux
</p>
<p>Segmentation
</p>
<p>GN:fs[l&#8217;/l&#8217;/det/fs ann&#233;e/ann&#233;e/nom/fs]
</p>
<p>GP:ms[du/du/pre/ms dragon/dragon/nom/ms]
</p>
<p>GV:3s-indpre[a/avoir/aux/3s-indpre
</p>
<p>Figure 1 : Les &#233;tapes successives de l&#8217;analyseur
</p>
<p>1) Etiquetage. Dans le cadre de la mod&#233;lisation probabiliste, le processus d&#8217;&#233;tiquetage revient
</p>
<p>&#224; aligner une s&#233;quence de mots W = w1,&#8230;,wN et une autre de tags T = t1,&#8230;,TN, et &#224; rechercher
</p>
<p>la s&#233;quence d&#8217;&#233;tiquettes T qui maximise la probabilit&#233; conditionnelle d&#8217;association. Dans
</p>
<p>SibyMot, nous avons utilis&#233; le mod&#232;le n-POS, avec n = 3. SibyMot travaillant sur les lemmes,
</p>
<p>le mod&#232;le n-POS a &#233;t&#233; adapt&#233; pour estimer non plus la probabilit&#233; d&#8217;apparition d&#8217;un mot mais
</p>
<p>celle d&#8217;un lemme. Par rapport au jeu d&#8217;&#233;tiquettes de l&#8217;action GRACE (Rajman et al., 1997),</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Schadle, Antoine, Le P&#233;v&#233;dic, Poirier
</p>
<p>notre jeu d&#8217;&#233;tiquettes est plus r&#233;duit (une centaine d&#8217;&#233;tiquettes). En particulier, les &#233;tiquettes
</p>
<p>ne contiennent pas d&#8217;informations flexionnelles, ce qui facilite la t&#226;che de l&#8217;&#233;tiqueteur.
</p>
<p>L&#8217;&#233;valuation de l&#8217;analyseur a ainsi donn&#233; un taux de 97,9 % de mots correctement &#233;tiquet&#233;s
</p>
<p>sur un extrait du journal Le Monde d&#8217;environ 50 000 mots.
</p>
<p>2) Segmentation. Pour r&#233;aliser la segmentation, nous proposons une solution originale qui
</p>
<p>s&#8217;inspire du mod&#232;le n-POS. Dans la mod&#233;lisation adopt&#233;e, la phrase est vue non plus comme
</p>
<p>une s&#233;quence de mots, mais comme une s&#233;quence de chunks C = c
1
</p>
<p>, &#8230;, cN. Chaque chunk cj
</p>
<p>contient un ou plusieurs mots repr&#233;sent&#233;s par leur &#233;tiquette grammaticale. Par analogie au
</p>
<p>mod&#232;le n-POS, la liste des parties du discours est identifi&#233;e &#224; la liste des diff&#233;rentes classes de
</p>
<p>chunk (GN, GV, etc.) et l&#8217;ensemble des &#233;l&#233;ments d&#8217;une classe est constitu&#233; par les s&#233;quences
</p>
<p>de tags appartenant &#224; cette classe (par exemple, det_nom, det_nom_adj, &#8230; pour le groupe
</p>
<p>nominal). La liste des s&#233;quences est donn&#233;e par une grammaire des chunks, qui contient plus
</p>
<p>de 200 000 s&#233;quences et est cr&#233;&#233;e de mani&#232;re automatique &#224; partir d&#8217;une base de 200 r&#232;gles
</p>
<p>sous forme d&#8217;expressions r&#233;guli&#232;res. Pour la segmentation, l&#8217;&#233;valuation a donn&#233; un taux de
</p>
<p>93,9 % de taux de rappel sur le m&#234;me corpus que pr&#233;c&#233;demment.
</p>
<p>3.2 Partie pr&#233;diction
</p>
<p>Au sortir de l&#8217;analyseur, nous disposons d&#8217;une segmentation de la phrase en chunks et d&#8217;un
</p>
<p>&#233;tiquetage en classes grammaticales. Cette structure est ensuite utilis&#233;e par le module de
</p>
<p>pr&#233;diction pour &#233;tablir une probabilit&#233; des mots du lexique. Sans entrer dans les d&#233;tails de la
</p>
<p>r&#233;alisation (Schadle, 2003),  le processus de pr&#233;diction est r&#233;alis&#233; en cinq &#233;tapes (figure 2).
</p>
<p>1) Pr&#233;diction des chunks
</p>
<p>4) Pr&#233;diction des flexions3) Pr&#233;diction des lemmes
</p>
<p>5) Pr&#233;diction des mots
</p>
<p>2) Pr&#233;diction des relations
</p>
<p>Figure 2 : &#201;tapes de la pr&#233;diction
</p>
<p>1) Pr&#233;diction des chunks. La premi&#232;re &#233;tape de la pr&#233;diction est charg&#233;e de fournir l&#8217;ensemble
</p>
<p>des segmentations possibles pour le mot &#224; venir. Elle s&#8217;appuie sur la m&#234;me grammaire des
</p>
<p>chunks que l&#8217;analyseur et donne une estimation de la probabilit&#233; de chacune d&#8217;elles. Les
</p>
<p>segmentations produites fournissent les &#233;tiquettes grammaticales du mot et du chunk. Par
</p>
<p>exemple, apr&#232;s la s&#233;quence &#171; GN[l&#8217;ann&#233;e] &#187;, cette &#233;tape d&#233;termine les probabilit&#233;s de
</p>
<p>&#171; GN[l&#8217;ann&#233;e &lt;adjectif cardinal&gt; &#187;, &#171; GN[l&#8217;ann&#233;e] GV[&lt;verbe conjugu&#233;&gt; &#187;, etc.
</p>
<p>2) Pr&#233;diction des relations. L&#8217;objectif de cette &#233;tape est de mettre en relation le dernier chunk
</p>
<p>avec les n-1 chunks pr&#233;c&#233;dents. Le but implicite est de capter les relations entre syntagmes.
</p>
<p>Cette mise en relation est &#233;galement probabilis&#233;e et utilise uniquement l&#8217;&#233;tiquette attribu&#233;e
</p>
<p>aux chunks. Sur l&#8217;exemple &#171; GN[l&#8217;ann&#233;e] GP[du dragon] GV[a &lt;participe pass&#233;&gt; &#187;, une forte
</p>
<p>probabilit&#233; sera ainsi attribu&#233;e &#224; la relation entre le GV et le GN.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>SibyMot : Mod&#233;lisation stochastique du langage int&#233;grant la notion de chunks
</p>
<p>3) Pr&#233;diction des lemmes. &#192; ce stade, la pr&#233;diction dispose de toutes les informations
</p>
<p>n&#233;cessaires pour estimer les probabilit&#233;s des lemmes. L&#8217;estimation combine une probabilit&#233;
</p>
<p>tri-lemme (&#224; l&#8217;image du n-gramme) et une probabilit&#233; fond&#233;e sur les t&#234;tes de chunks. C&#8217;est
</p>
<p>cette derni&#232;re qui sur l&#8217;exemple &#171; GN[l&#8217;ann&#233;e] GP[du dragon] GV[a &lt;participe pass&#233;&gt; &#187; et la
</p>
<p>relation GV-GN, permet d&#8217;obtenir une forte probabilit&#233; pour &#171; commenc&#233; &#187;.
</p>
<p>4, 5) Parall&#232;lement &#224; l&#8217;estimation des lemmes, l&#8217;&#233;tape de pr&#233;diction des flexions d&#233;livre une
</p>
<p>estimation pour chaque flexion. Gr&#226;ce aux relations &#233;tablies en 2) un m&#233;canisme d&#8217;accords
</p>
<p>entre chunks est rendu possible. Au final, &#224; partir des estimations des lemmes et des flexions,
</p>
<p>ces probabilit&#233;s sont combin&#233;es pour calculer la probabilit&#233; des mots du lexique de SibyMot.
</p>
<p>4 Apprentissage
</p>
<p>Pour l&#8217;acquisition des param&#232;tres du mod&#232;le, le corpus d&#8217;apprentissage doit &#234;tre annot&#233;. &#192;
</p>
<p>chaque mot doit correspondre son lemme, sa cat&#233;gorie grammaticale, sa flexion. Les phrases
</p>
<p>doivent &#234;tre segment&#233;es et les segments mis en relation. Nous ne disposons malheureusement
</p>
<p>pas d&#8217;un tel corpus, l&#8217;apprentissage a donc &#233;t&#233; r&#233;alis&#233; sur un corpus non annot&#233; manuellement.
</p>
<p>Le corpus utilis&#233; contient pr&#232;s de deux millions de mots (un mois du journal Le Monde). Pour
</p>
<p>l&#8217;&#233;tiqueteur, l&#8217;apprentissage a &#233;t&#233; r&#233;alis&#233; sur un &#233;tiquetage produit par l&#8217;analyseur Cordial.
</p>
<p>L&#8217;acquisition des param&#232;tres des modules sup&#233;rieurs a &#233;t&#233; obtenue par apprentissage non
</p>
<p>supervis&#233;. Nous reviendrons sur cet apprentissage sous-optimal lors des r&#233;sultats de
</p>
<p>l&#8217;&#233;valuation. Quant au lexique il est extrait de ceux de l&#8217;ABU et de Lexique (accessibles sur
</p>
<p>l&#8217;internet), enrichis des donn&#233;es d&#8217;apprentissage et contient plus de 50 000 lemmes.
</p>
<p>5 &#201;valuation
</p>
<p>L&#8217;&#233;valuation adopt&#233;e est proche de celle propos&#233;e dans (Bimbot et al., 1997) qui permet de
</p>
<p>comparer des mod&#232;les probabilistes &#224; des mod&#232;les non probabilistes. Il s&#8217;agit d&#8217;une adaptation
</p>
<p>du jeu de Shannon qui consiste &#224; proposer &#224; partir d&#8217;un contexte, une liste de mots candidats.
</p>
<p>Chacun des mots candidats &#233;tant affect&#233; d&#8217;un poids, la qualit&#233; du mod&#232;le est &#233;valu&#233;e &#224; partir
</p>
<p>de la moyenne g&#233;om&#233;trique des poids accord&#233;s &#224; la solution correcte pour chaque contexte.
</p>
<p>Notre m&#233;trique se rapproche de cette derni&#232;re et est plus adapt&#233;e &#224; l&#8217;&#233;valuation des syst&#232;mes
</p>
<p>d&#8217;aide &#224; la communication. Apr&#232;s chaque lettre tap&#233;e par l&#8217;utilisateur pour composer son
</p>
<p>message, le syst&#232;me affiche une liste d&#8217;un certain nombre de mots (ici 5). Si le mot souhait&#233;
</p>
<p>appara&#238;t dans la liste, les lettres non tap&#233;es sont consid&#233;r&#233;es comme &#233;conomis&#233;es, sinon
</p>
<p>l&#8217;utilisateur tape la lettre suivante et le syst&#232;me &#233;tablit une nouvelle liste de propositions. On
</p>
<p>mesure ainsi le nombre de lettres &#233;conomis&#233;es par rapport au nombre de lettres du message.
</p>
<p>Lors de cette &#233;valuation nous avons compar&#233; notre syst&#232;me au mod&#232;le n-gramme (ordres de 1
</p>
<p>&#224; 3). Le corpus d&#8217;apprentissage est le m&#234;me pour les diff&#233;rents mod&#232;les compar&#233;s. Le corpus
</p>
<p>de test contient 50 889 mots. Les r&#233;sultats obtenus sont donn&#233;s dans le tableau ci-dessous.
</p>
<p>Mod&#232;le 1-gramme 2-gramme 3-gramme SibyMot
</p>
<p>% &#233;conomis&#233;s 43,9 % 51,2 % 55,8 % 57,1 %
</p>
<p>Tableau 1 : &#201;valuation compar&#233;e du mod&#232;le SibyMot</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Schadle, Antoine, Le P&#233;v&#233;dic, Poirier
</p>
<p>Les r&#233;sultats montrent que notre mod&#232;le obtient des performances sup&#233;rieures &#224; l&#8217;uni-gramme
</p>
<p>(+ 13,2 %), au bi-gramme (+ 5,9 %) et au tri-gramme (+ 1,3 %). Cette derni&#232;re comparaison
</p>
<p>montre que notre mod&#232;le avec des connaissances syntaxiques obtient de meilleurs r&#233;sultats
</p>
<p>qu&#8217;un mod&#232;le n-gramme simple. De plus, nous pensons que l&#8217;apprentissage a &#233;t&#233; r&#233;alis&#233;e de
</p>
<p>mani&#232;re sous optimale et que le mod&#232;le dispose ainsi d&#8217;une certaine marge de progression.
</p>
<p>6 Conclusion
</p>
<p>Nous avons pr&#233;sent&#233; dans cet article les principes du mod&#232;le de langage utilis&#233; par le syst&#232;me
</p>
<p>Sibylle. Dans le cadre de la mod&#233;lisation probabiliste du langage, nous proposons d&#8217;am&#233;liorer
</p>
<p>les capacit&#233;s pr&#233;dictives du mod&#232;le n-gramme en captant des d&#233;pendances &#224; plus longue
</p>
<p>distance avec des chunks. Les r&#233;sultats obtenus montrent ainsi que les capacit&#233;s de notre
</p>
<p>mod&#232;le sont sup&#233;rieures &#224; celle du mod&#232;le n-gramme. Ce mod&#232;le appel&#233; SibyMot est
</p>
<p>actuellement int&#233;gr&#233; dans l&#8217;application Sibylle, un syst&#232;me d&#8217;aide &#224; la communication pour
</p>
<p>les personnes handicap&#233;es. Cette application est utilis&#233;e au CMRRF de Kerpape par des
</p>
<p>Infirmes Moteurs C&#233;r&#233;braux. Le module SibyMot va &#234;tre &#233;galement commercialis&#233; dans un
</p>
<p>autre syst&#232;me d&#8217;aide &#224; la communication par la soci&#233;t&#233; Microvocal. Enfin, notons que le
</p>
<p>mod&#232;le SibyMot, dans sa partie analyseur, participe &#224; la campagne d&#8217;&#233;valuation EASY des
</p>
<p>analyseurs syntaxiques du fran&#231;ais, dans le cadre de l&#8217;action Technolangue.
</p>
<p>R&#233;f&#233;rences
</p>
<p>ABNEY S. (1991), Parsing by chunks. In R. Berwick, S. Abney, and C. Tenny (Eds.), Principle
</p>
<p>based parsing, Kluwer Academic.
</p>
<p>BIMBOT F., EL-B&#200;ZE M., JARDINO M. (1997), An alternative scheme for perplexity estimation.
</p>
<p>Proc. of the International Conference on Acoustics, Speech and Signal Processing, Munich.
</p>
<p>BOISSIERE P. (2000) VITIPI : Un syst&#232;me d&#8217;aide &#224; l&#8217;&#233;criture bas&#233; sur un principe d&#8217;auto-
</p>
<p>apprentissage et adapt&#233; &#224; tous les handicaps moteurs. Actes de Handicap&#8217;00, pp 81-86, Paris.
</p>
<p>CERF-DANON H., EL-B&#200;ZE M. (1991), Three different probabilistic language models:
</p>
<p>Comparison and combination. In Proceeding of ICASSP-91, pp 297-300, Toronto, Canada.
</p>
<p>JELINEK F. (1976), Continuous speech recognition by statistical models. Proc. of the IEEE.
</p>
<p>MAUREL D., LE P&#201;V&#201;DIC B. (2001), The syntactic prediction with Token Automata:
</p>
<p>Application to HandiAS system. Theoretical Computer Science, vol. 267, pp 121-129.
</p>
<p>RAJMAN M., LECOMTE J., PAROUBEK P. (1997), Format de description lexicale pour le
</p>
<p>fran&#231;ais. Partie 2 : Description morpho-syntaxique, r&#233;f. GRACE GTR-3-2.1.
</p>
<p>SCHADLE I., LE PEVEDIC B., ANTOINE J.-Y., POIRIER F. (2001), SibyLettre : pr&#233;diction de
</p>
<p>lettre pour l&#8217;aide &#224; la saisie de texte. Actes de TALN&#8217;2001, vol. 2, pp 233-242, Tours, France.
</p>
<p>SCHADLE I. (2003), Sibylle : Syst&#232;me linguistique d&#8217;aide &#224; la communication pour les
</p>
<p>personnes handicap&#233;es. Th&#232;se de doctorat, Universit&#233; de Bretagne Sud.</p>

</div></div>
</body></html>