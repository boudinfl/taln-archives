<?xml version="1.0" encoding="UTF-8"?>
<!-- 
	Fichier construit à partir des méta-données fournies sur le CD-ROM de la conférence.
	Mis à jour avec les fichiers pdfs envoyés par Patrick Paroubek
	Fichiers pdfs problématiques : taln-2005-long-028, taln-2005-long-006, taln-2005-long-031, taln-2005-court-006
	Convertis en texte avec http://www.newocr.com/
-->
<conference>
	<edition>
		<acronyme>TALN'2005</acronyme>
		<titre>12ème conférence sur le Traitement Automatique des Langues Naturelles</titre>
		<ville>Dourdan</ville>
		<pays>France</pays>
		<dateDebut>2005-06-06</dateDebut>
		<dateFin>2005-06-10</dateFin>
		<presidents>
			<nom>Michèle Jardino</nom>
		</presidents>
		<typeArticles>
			<type id="long">Papiers longs</type>
			<type id="court">Papiers courts</type>
		</typeArticles>
		<statistiques>
			<!-- <acceptations id="long" soumissions=""></acceptations> -->
			<!-- <acceptations id="court" soumissions=""></acceptations> -->
		</statistiques>
		<siteWeb>http://www.limsi.fr/TALN05</siteWeb>
		<meilleurArticle>
			<articleId></articleId>
		</meilleurArticle>
	</edition>
	<articles>
		<article id="taln-2005-long-001" session="Grammaires">
			<auteurs>
				<auteur>
					<nom>François Thomasset</nom>
					<email>Francois.Thomasset@inria.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Éric Villemonte De La Clergerie</nom>
					<email>Eric.De_La_Clergerie@inria.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">INRIA</affiliation>
			</affiliations>
			<titre>Comment obtenir plus des Méta-Grammaires</titre>
			<type>long</type>
			<pages>1-12</pages>
			<resume>Cet article présente un environnement de développement pour les méta-grammaires (MG), utilisé pour concevoir rapidement une grammaire d'arbres adjoints (TAG) du français à large couverture et néanmoins très compacte, grâce à des factorisations d'arbres. Exploitant les fonctionnalités fournies par le système DYALOG, cette grammaire a permis de construire un analyseur syntaxique hybride TAG/TIG utilisé dans le cadre de la campagne d'évaluation syntaxique EASY.</resume>
			<mots_cles>Méta-grammaires, Analyse Syntaxique, TAG, TIG</mots_cles>
			<title></title>
			<abstract>This paper presents a development environment for Meta-Grammars (MG), used to design, in a short period, a wide coverage but still very compact Tree Adjoining Grammar (TAG) for French, thanks to tree factorizations. Exploiting the functionalities provided by DYALOG system, an hybrid TAG/TIG parser was compiled from the grammar and used for the EASY parsing evaluation campaign.</abstract>
			<keywords>Meta-grammars, Parsing, TAG, TIG</keywords>
		</article>
		<article id="taln-2005-long-002" session="Grammaires">
			<auteurs>
				<auteur>
					<nom>Denys Duchier</nom>
					<email>duchier@lifl.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Joseph Le Roux</nom>
					<email>leroux@loria.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Yannick Parmentier</nom>
					<email>parmenti@loria.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIFL - CNRS - Université des Sciences et Technologies de Lille</affiliation>
				<affiliation affiliationId="2">INRIA / LORIA - CNRS - Institut National Polytechnique de Lorraine, Université Henri Poincaré, Nancy 1</affiliation>
			</affiliations>
			<titre>XMG : un Compilateur de Méta-Grammaires Extensible</titre>
			<type>long</type>
			<pages>13-22</pages>
			<resume>Dans cet article, nous présentons un outil permettant de produire automatiquement des ressources linguistiques, en l’occurence des grammaires. Cet outil se caractérise par son extensibilité, tant du point de vue des formalismes grammaticaux supportés (grammaires d’arbres adjoints et grammaires d’interaction à l’heure actuelle), que de son architecture modulaire, qui facilite l’intégration de nouveaux modules ayant pour but de vérifier la validité des structures produites. En outre, cet outil offre un support adapté au développement de grammaires à portée sémantique.</resume>
			<mots_cles>Grammaires, compilation, ressources linguistiques, Grammaires d’Arbres Adjoints, Grammaires d’Interaction</mots_cles>
			<title></title>
			<abstract>In this paper, we introduce a new tool for automatic generation of linguistic resources such as grammars. This tool’s main feature consists of its extensibility from different points of view. On top of supporting several grammatical formalisms (Tree Adjoining Grammars and Interaction Grammars for now), it has a modular architecture which eases the integration of modules dedicated to the checking of the output structures. Furthermore, this tool offers adapted support to the development of grammars with semantic information.</abstract>
			<keywords>Grammars, compilation, linguistic resources, Tree Adjoining Grammars, Interaction Grammars</keywords>
		</article>
		<article id="taln-2005-long-003" session="Grammaires">
			<auteurs>
				<auteur>
					<nom>Sylvain Kahane</nom>
					<email>sk@ccr.jussieu.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>François Lareau</nom>
					<email>francois.lareau@umontreal.ca</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Modyco - Université Paris 10 / Lattice - Université Paris 7</affiliation>
				<affiliation affiliationId="2">OLST - Université de Montréal / Lattice - Université Paris 7</affiliation>
			</affiliations>
			<titre>Grammaire d'Unification Sens-Texte : modularité et polarisation</titre>
			<type>long</type>
			<pages>23-32</pages>
			<resume>L’objectif de cet article est de présenter l’état actuel du modèle de la Grammaire d’Unification Sens-Texte, notamment depuis que les bases formelles du modèle ont été éclaircies grâce au développement des Grammaires d’Unification Polarisées. L’accent est mis sur l’architecture du modèle et le rôle de la polarisation dans l’articulation des différents modules — l’interface sémantique-syntaxe, l’interface syntaxe-morphotopologie et les grammaires décrivant les différents niveaux de représentation. Nous étudions comment les procédures d’analyse et de génération sont contrôlables par différentes stratégies de neutralisation des différentes polarités.</resume>
			<mots_cles>Théorie Sens-Texte, interface syntaxe-sémantique, synchronisation, grammaire d’unification polarisée, grammaire de dépendance, grammaire topologique, génération de textes</mots_cles>
			<title></title>
			<abstract>This article presents the Meaning-Text Unification Grammar’s current state, now that its formal foundations have been clarified with the development of Polarized Unification Grammars. Emphasis is put on the model’s architecture and the role of polarization in linking the various modules — semantic-syntax interface, syntax-morphotopology interface and the well-formedness grammars of each representation level. We discuss how various polarity neutralization strategies can control different analysis and generation procedures.</abstract>
			<keywords>Meaning-Text Theory, syntax-semantics interface, synchronization, polarized unification grammar, dependency grammar, topology grammar, text generation</keywords>
		</article>
		<article id="taln-2005-long-004" session="Recherche d'information">
			<auteurs>
				<auteur>
					<nom>Florian Seydoux</nom>
					<email>florian.seydoux@epfl.ch</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Jean-Cédric Chappelier</nom>
					<email>jean-cedric.chappelier@epfl.ch</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Ecole Polytechnique Fédérale de Lausanne (EPFL)</affiliation>
			</affiliations>
			<titre>Indexation Sémantique par Coupes de Redondance Minimale dans une Ontologie</titre>
			<type>long</type>
			<pages>33-42</pages>
			<resume>Plusieurs travaux antérieurs ont fait état de l’amélioration possible des performances des systèmes de recherche documentaire grace à l’utilisation d’indexation sémantique utilisant une ontologie (p.ex. WordNet). La présente contribution décrit une nouvelle méthode visant à réduire le nombre de termes d’indexation utilisés dans une indexation sémantique, en cherchant la coupe de redondance minimale dans la hiérarchie fournie par l’ontologie. Les résultats, obtenus sur diverses collections de documents en utilisant le dictionnaire EDR, sont présentés.</resume>
			<mots_cles>Indexation sémantique, Recherche documentaire, Redondance minimale, Ontologie</mots_cles>
			<title></title>
			<abstract>Several former works have shown that it is possible to improve information retrieval performances using semantic indexing, adding additional information coming from a thesaurus (e.g.WordNet). This paper presents a new method to reduce the number of "concepts"used to index the documents, by determining a minimum redundancy cut in the hierarchy provided by the thesaurus. The results of experiments carried out on several standard document collections using the EDR thesaurus are presented.</abstract>
			<keywords>Semantic Indexing, Information Retrieval, Minimal Redundancy, Ontology</keywords>
		</article>
		<article id="taln-2005-long-005" session="Recherche d'information">
			<auteurs>
				<auteur>
					<nom>Véronique Malaisé</nom>
					<email>vmalaise@ina.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Thierry Delbecque</nom>
					<email>thd@biomath.jussieu.fr</email>
					<affiliationId>2</affiliationId>
					<affiliationId>3</affiliationId>
				</auteur>
				<auteur>
					<nom>Pierre Zweigenbaum</nom>
					<email>pz@biomath.jussieu.fr</email>
					<affiliationId>2</affiliationId>
					<affiliationId>3</affiliationId>
					<affiliationId>4</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">INA</affiliation>
				<affiliation affiliationId="2">INSERM</affiliation>
				<affiliation affiliationId="3">INALCO</affiliation>
				<affiliation affiliationId="3">STIM</affiliation>
			</affiliations>
			<titre>Recherche en corpus de réponses à des questions définitoires</titre>
			<type>long</type>
			<pages>43-52</pages>
			<resume>Les systèmes de questions-réponses, essentiellement focalisés sur des questions factuelles en domaine ouvert, testent également d’autres tâches, comme le travail en domaine contraint ou la recherche de définitions. Nous nous intéressons ici à la recherche de réponses à des questions « définitoires » portant sur le domaine médical. La recherche de réponses de type définitoire se fait généralement en utilisant deux types de méthodes : celles s’appuyant essentiellement sur le contenu du corpus cible, et celles faisant appel à des connaissances externes. Nous avons choisi de nous limiter au premier de ces deux types de méthodes. Nous présentons une expérience dans laquelle nous réutilisons des patrons de repérage d’énoncés définitoires, conçus pour une autre tâche, pour localiser les réponses potentielles aux questions posées. Nous avons intégré ces patrons dans une chaîne de traitement que nous évaluons sur les questions définitoires et le corpus médical du projet EQueR sur l’évaluation de systèmes de questions-réponses. Cette évaluation montre que, si le rappel reste à améliorer, la « précision » des réponses obtenue (mesurée par la moyenne des inverses de rangs) est honorable. Nous discutons ces résultats et proposons des pistes d’amélioration.</resume>
			<mots_cles>Systèmes de questions-réponses, repérage d’énoncés définitoires, patrons lexico-syntaxiques, médecine</mots_cles>
			<title></title>
			<abstract>Question-answering systems mostly focus on open-domain, factoid questions, but also test other tasks such as restricted-domain and « definitional » questions. We address here the search for definitional questions in the medical domain. Searching for answers to definitional questions generally resorts to two kinds of methods : those which mostly rely on the contents of the target corpus, and those which call on external resources. We have chosen to limit ourselves to the first kind. We present an experiment in which we reuse lexico-syntactic patterns, formerly designed for another task, to locate answers to definitional questions. We have integrated these patterns in a processing chain which we evaluate on the medical definitional questions and corpus of project EQueR (evaluation of French QA systems). This evaluation shows that, while recall still needs to be increased, the « precision » of the obtained answers (as measured through the mean reciprocal rank) is honorable. We discuss these results and propose directions for improvement.</abstract>
			<keywords>Question-answering systems, mining definitions, lexico-syntactic patterns, medicine</keywords>
		</article>
		<article id="taln-2005-long-006" session="Recherche d'information">
			<auteurs>
				<auteur>
					<nom>Dominique Laurent</nom>
					<email>dlaurent@synapse-fr.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Patrick Séguéla</nom>
					<email>p.seguela@synapse-fr.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Synapse Développement</affiliation>
			</affiliations>
			<titre>QRISTAL, système de Questions-Réponses</titre>
			<type>long</type>
			<pages>53-62</pages>
			<resume>QRISTAL (Questions-Réponses Intégrant un Système de Traitement Automatique des Langues) est un système de questions-réponses utilisant massivement le TAL, tant pour l'indexation des documents que pour l'extraction des réponses. Ce système s’est récemment classé premier lors de l’évaluation EQueR (Evalda, Technolanguez). Après une description fonctionnelle du système, ses performances sont détaillées. Ces résultats et des tests complémentaires permettent de mieux situer l'apport des différents modules de TAL. Les réactions des premiers utilisateurs incitent enfin à une réflexion sur l'ergonomie et les contraintes des systèmes de questions-réponses, face aux outils de recherche sur le Web.</resume>
			<mots_cles>Système de questions-réponses, recherche d'information, évaluation des systèmes de questions-réponses, extraction de réponse, recherche sur le Web, QRISTAL</mots_cles>
			<title></title>
			<abstract>QRISTAL is a question answering system which makes intensive use of natural language processing techniques, for indexing documents as well as for extracting answers. This system recently ranked first in the EQueR evaluation exercise (Evalda, Technolanguez). After a functional description of the system, its results in the EQueR exercise are detailed. These results and some additional tests allow to evaluate the contribution of each NLP component. The feedback of the first QRISTAL users encourage further thoughts about the ergonomics and the constraints of question answering systems, faced with the Web search engines.</abstract>
			<keywords>Question Answering system, information retrieval, Question Answering evaluation, answer extraction, Web search strategy, QRISTAL</keywords>
		</article>
		<article id="taln-2005-long-007" session="Sémantique et terminologie">
			<auteurs>
				<auteur>
					<nom>Fiammetta Namer</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">UMR ATILF-CNRS et Université Nancy2</affiliation>
			</affiliations>
			<titre>Morphosémantique pour l'appariement de termes dans le vocabulaire médical : approche multilingue</titre>
			<type>long</type>
			<pages>63-72</pages>
			<resume>Cet article s'intéresse à la manière dont la morphosémantique peut contribuer à l'appariement multilingue de variantes terminologiques entre termes. L'approche décrite permet de relier automatiquement entre eux les noms et adjectifs composés savants d'un corpus spécialisé en médecine (synonymie, hyponymie, approximation). L'acquisition de relations lexicales est une question particulièrement cruciale lors de l'élaboration de bases de données et de systèmes de recherche d'information multilingues. La méthode est applicable à au moins cinq langues européennes dont elle exploite les caractéristiques morphologiques similaires des mots composés dans les langues de spécialité. Elle consiste en l'intéraction de trois dispositifs : (1) un analyseur morphosémantique monolingue, (2) une table multilingue qui définit des relations de base entre les racines gréco-latines des lexèmes savants, (3) quatre règles indépendantes de la langue qui infèrent, à partir de ces relations de base, les relations lexicales entre les lexèmes contenant ces racines. L'approche décrite est implémentée en français, où l'on dispose d'un analyseur morphologique capable de calculer la définition de mots construits inconnus à partir du sens de ses composants. Le corpus de travail est un lexique spécialisé médical d'environ 29000 lexèmes, que le calcul des relations de synonymie, hyponymie et approximation a permis de regrouper en plus de 3000 familles lexicales.</resume>
			<mots_cles>morphologie, sémantique, multilinguisme, composition savante, relation lexicale, terminologie médicale</mots_cles>
			<title></title>
			<abstract>This paper addresses the issue of the interaction between morphosemantics and term variants extraction. The described method enables neoclassical compound nouns and adjectives of a biomedical specialized corpus to be automatically related by synonymy, hyponymy and approximation links. Acquiring lexical relations is a particularly crucial issue when elaborating multilingual databases and when developing cross-language information retrieval systems. This method can be applied at least to five European languages and exploits the similarity between the morphological characteristics of compound words in specialized domains. It requires the interaction of three techniques: (1) a language-specific morphosemantic parser, (2) a multilingual table defining basic relations between word roots, and (3) a set of language-independant rules to draw up the list of related terms. This approach has been fully implemented for French, on an about 29,000 terms biomedical lexicon, resulting to more than 3,000 lexical families.</abstract>
			<keywords>morphology, semantics, multilingualism, neoclassical compounding, lexical relation, medical terminology</keywords>
		</article>
		<article id="taln-2005-long-008" session="Sémantique et terminologie">
			<auteurs>
				<auteur>
					<nom>Didier Schwab</nom>
					<email>schwab@lirmm.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Mathieu Lafourcade</nom>
					<email>lafourca@lirmm.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Violaine Prince</nom>
					<email>prince@lirmm.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIRMM UM2-CNRS</affiliation>
			</affiliations>
			<titre>Extraction semi-supervisée de couples d'antonymes grâce à leur morphologie</titre>
			<type>long</type>
			<pages>73-82</pages>
			<resume>Dans le cadre de la recherche sur la représentation du sens en Traitement Automatique des Langues Naturelles, nous nous concentrons sur la construction d’un système capable d’acquérir le sens des mots, et les relations entre ces sens, à partir de dictionnaires à usage humain, du Web ou d’autres ressources lexicales. Pour l’antonymie, il n’existe pas de listes séparant les antonymies complémentaire, scalaire et duale. Nous présentons dans cet article une approche semi-supervisée permettant de construire ces listes. Notre méthode est basée sur les oppositions de nature morphologique qui peuvent exister entre les items lexicaux. À partir d’un premier ensemble de couples antonymes, elle permet non seulement de construire ces listes mais aussi de trouver des oppositions morphologiques. Nous étudions les résultats obtenus par cette méthode. En particulier, nous présentons les oppositions de préfixes ainsi découvertes et leur validité sur le corpus puis nous discutons de la répartition des types d’antonymie en fonction des couples opposés de préfixes.</resume>
			<mots_cles>antonymie, morphologie, antonymie complémentaire, antonymie scalaire, antonymie duale, répartition statistique</mots_cles>
			<title></title>
			<abstract>In the framework of meaning representation in Natural Language Processing, we focus on enabling a system to autonomously learn word meanings and semantic relations from user dictionaries, web contents and other lexical resources. For antonymy, as a lexical semantic relation, no resource provides distinctions between complementary, scalar and dual antonymies. In this paper, we present a semi-supervized method to collate such lists, based on operating morphological opposition holding between lexical items. The approach presented here starts from a bootstraped initial list. It is able to augment such lists but also to find out morphological oppositions. We scrutinize the obtained results and discuss the distribution of antonymy types.</abstract>
			<keywords>antonymy, morphology, complementar antonymy, scalar antonymy, dual antonymy, statistic distribution</keywords>
		</article>
		<article id="taln-2005-long-009" session="Sémantique et terminologie">
			<auteurs>
				<auteur>
					<nom>Natalia Grabar</nom>
					<email>pz@biomath.jussieu.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Pierre Zweigenbaum</nom>
					<email>ngr@biomath.jussieu.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">STIM/AP--HP</affiliation>
				<affiliation affiliationId="2">INSERM U297</affiliation>
				<affiliation affiliationId="3">INaLCO</affiliation>
			</affiliations>
			<titre>Utilisation de corpus de spécialité pour le filtrage de synonymes de la langue générale</titre>
			<type>long</type>
			<pages>83-92</pages>
			<resume>Les ressources linguistiques les plus facilement disponibles en TAL ressortissent généralement au registre général d’une langue. Lorsqu’elles doivent être utilisées sur des textes de spécialité il peut être utile de les adapter à ces textes. Cet article est consacré à l’adaptation de ressources synonymiques générales à la langue médicale. L’adaptation est obtenue suite à une série de filtrages sur un corpus du domaine. Les synonymes originaux et les synonymes filtrés sont ensuite utilisés comme une des ressources pour la normalisation de variantes de termes dans une tâche de structuration de terminologie. Leurs apports respectifs sont évalués par rapport à la structure terminologique de référence. Cette évaluation montre que les résultats sont globalement encourageants après les filtrages, pour une tâche comme la structuration de terminologies : une amélioration de la précision contre une légère diminution du rappel.</resume>
			<mots_cles>Langue de spécialité, langue générale, structuration de terminologies, synonymes, portabilité, filtrage</mots_cles>
			<title></title>
			<abstract>General language resources are often more easily available for NLP applications. When using them to process specialized texts it might be useful to adapt them to these texts. This paper describes experiments in adapting general language synonymous resources to the medical domain. A set of filtering methods through a domain corpora is applied. Original and filtered synonyms are then used for normalizing term variation in a terminology structuring task. Their relative contributions are evaluated in comparison with the original structure of the reference terminology. This evaluation shows that the overall results are encouraging, as for the terminology structuring task : improvement of precision while recall is slightly decreased.</abstract>
			<keywords>Specialized language, general language, terminology structuring, synonyms, portability, filtering</keywords>
		</article>
		<article id="taln-2005-long-010" session="Analyse de phrase">
			<auteurs>
				<auteur>
					<nom>Philippe Blache</nom>
					<email>pb@lpl.univ-aix.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LPL-CNRS</affiliation>
			</affiliations>
			<titre>Combiner analyse superficielle et profonde : bilan et perspectives</titre>
			<type>long</type>
			<pages>93-102</pages>
			<resume>L’analyse syntaxique reste un problème complexe au point que nombre d’applications n’ont recours qu’à des analyseurs superficiels. Nous faisons dans cet article le point sur les notions d’analyse superficielles et profondes en proposant une première caractérisation de la notion de complexité opérationnelle pour l’analyse syntaxique automatique permettant de distinguer objets et relations plus ou moins difficiles à identifier. Sur cette base, nous proposons un bilan des différentes techniques permettant de caractériser et combiner analyse superficielle et profonde.</resume>
			<mots_cles>Analyse syntaxique, analyse superficielle, analyse profonde</mots_cles>
			<title></title>
			<abstract>Deep parsing remains a problem for NLP so that many applications has to use shallow parsers. We propose in this paper a presentation of the different characteristics of shallow and deep parsing techniques relying on the notion of operational complexity. We present different approaches combining these techniques and propose a new approach making it possible to use the output of a shallow parser as the input of a deep one.</abstract>
			<keywords>Parsing, shallow and deep parsing</keywords>
		</article>
		<article id="taln-2005-long-011" session="Analyse de phrase">
			<auteurs>
				<auteur>
					<nom>Pierre Boullier</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Lionel Clément</nom>
					<email>Lionel.Clement@lefff.net</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Benoît Sagot</nom>
					<email>Benoit.Sagot@inria.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Éric Villemonte De La Clergerie</nom>
					<email>Eric.De_La_Clergerie@inria.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">INRIA - Projet ATOLL</affiliation>
			</affiliations>
			<titre>Chaînes de traitement syntaxique</titre>
			<type>long</type>
			<pages>103-112</pages>
			<resume>Cet article expose l’ensemble des outils que nous avons mis en oeuvre pour la campagne EASy d’évaluation d’analyse syntaxique. Nous commençons par un aperçu du lexique morphologique et syntaxique utilisé. Puis nous décrivons brièvement les propriétés de notre chaîne de traitement pré-syntaxique qui permet de gérer des corpus tout-venant. Nous présentons alors les deux systèmes d’analyse que nous avons utilisés, un analyseur TAG issu d’une méta-grammaire et un analyseur LFG. Nous comparons ces deux systèmes en indiquant leurs points communs, comme l’utilisation intensive du partage de calcul et des représentations compactes de l’information, mais également leurs différences, au niveau des formalismes, des grammaires et des analyseurs. Nous décrivons ensuite le processus de post-traitement, qui nous a permis d’extraire de nos analyses les informations demandées par la campagne EASy. Nous terminons par une évaluation quantitative de nos architectures.</resume>
			<mots_cles>Analyse syntaxique, évaluation</mots_cles>
			<title></title>
			<abstract>This paper presents the set of tools we used for the EASy parsing evaluation campaign. We begin with an overview of the morphologic and syntactic lexicon we used. Then we briefly describe the properties of our pre-syntactic processing that allows us to deal with real-life corpus. Afterwards, we introduce the two parsers we used, namely a TAG parser based on a meta-grammar and an LFG parser. We compare these parsers, showing their common points, e.g., the extensive use of tabulation and compact representation techniques, but also their differences, concerning formalisms, grammars and parsers. We then describe the postprocessing that allowed us to extract from our analyses the data required by the EASy campaign. We conclude with a quantitative evaluation of our architectures.</abstract>
			<keywords>Parsing, Evaluation</keywords>
		</article>
		<article id="taln-2005-long-012" session="Analyse de phrase">
			<auteurs>
				<auteur>
					<nom>Jonas Granfeldt</nom>
					<email>Jonas.Granfeldt@rom.lu.se</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Pierre Nugues</nom>
					<email>Pierre.Nugues@cs.lth.se</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Emil Persson</nom>
					<email>emil.person@telia.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Lisa Persson</nom>
					<email>nossrespasil@hotmail.com</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Fabian Kostadinov</nom>
					<email>fabian.kostadinov@access.unizh.ch</email>
					<affiliationId>3</affiliationId>
				</auteur>
				<auteur>
					<nom>Malin Ågren</nom>
					<email>Malin.Agren@rom.lu.se</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Suzanne Schlytere</nom>
					<email>Suzanne.Schlyter@rom.lu.se</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Institut des langues romanes – Université de Lund Box 201, S-221 00 Lund, Suède</affiliation>
				<affiliation affiliationId="2">Institut d’informatique – Université de Lund Box 118, S-221 00 Lund, Suède</affiliation>
				<affiliation affiliationId="3">Institut d’informatique – Université de Zurich</affiliation>
			</affiliations>
			<titre>Direkt Profil : un système d'évaluation de textes d'élèves de français langue étrangère fondé sur les itinéraires d'acquisition</titre>
			<type>long</type>
			<pages>113-122</pages>
			<resume>Direkt Profil est un analyseur automatique de textes écrits en français comme langue étrangère. Son but est de produire une évaluation du stade de langue des élèves sous la forme d’un profil d’apprenant. Direkt Profil réalise une analyse des phrases fondée sur des itinéraires d’acquisition, i.e. des phénomènes morphosyntaxiques locaux liés à un développement dans l’apprentissage du français. L’article présente les corpus que nous traitons et d’une façon sommaire les itinéraires d’acquisition. Il décrit ensuite l’annotation que nous avons définie, le moteur d’analyse syntaxique et l’interface utilisateur. Nous concluons par les résultats obtenus jusqu’ici : sur le corpus de test, le système obtient un rappel de 83% et une précision de 83%.</resume>
			<mots_cles>français langue étrangère, itinéraires d’acquisition, évaluation, annotation, analyse syntaxique partielle</mots_cles>
			<title></title>
			<abstract>Direkt Profil is an automatic analyzer of texts written in French as a second language. The objective is to produce an evaluation of the development stage of the students under the form of a learner profile. Direkt Profil carries out a sentence analysis based on developmental sequences, i.e. local morphosyntactic phenomena linked to a development in the learning of French. The paper presents the corpus that we use and briefly, the developmental sequences. Furthermore, it describes the annotation that we have defined, the parser, and the user interface. We conclude by the results obtained so far: on the test corpus the systems obtains a recall of 83% and a precision of 83%.</abstract>
			<keywords>second language French, developmental sequences, evaluation, annotation, partial parsing</keywords>
		</article>
		<article id="taln-2005-long-013" session="Analyse lexicale">
			<auteurs>
				<auteur>
					<nom>Laurence Danlos</nom>
					<email>Laurence.Danlos@linguist.jussieu.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LATTICE</affiliation>
			</affiliations>
			<titre>ILIMP: Outil pour repérer les occurences du pronom impersonnel il</titre>
			<type>long</type>
			<pages>123-132</pages>
			<resume>Nous présentons un outil, ILIMP, qui prend en entrée un texte brut (sans annotation linguistique) rédigé en français et qui fournit en sortie le texte d'entrée où chaque occurrence du pronom il est décorée de la balise [ANAphorique] ou [IMPersonnel]. Cet outil a donc comme fonctionnalité de distinguer les occurrences anaphoriques du pronom il, pour lesquelles un système de résolution des anaphores doit chercher un antécédent, des occurrences où il est un pronom impersonnel (explétif) pour lequel la recherche d'antécédent ne fait pas sens. ILIMP donne un taux de précision de 97,5%. Nous présentons une analyse détaillée des erreurs et nous décrivons brièvement d'autres applications potentielles de la méthode utilisée dans ILIMP, ainsi que l’utilisation et le positionnement d’ILIMP dans un système d’analyse syntaxique modulaire.</resume>
			<mots_cles>Pronom impersonnel (explétif), Pronom anaphorique, Lexique-grammaire, Automates, Résolution d'anaphores, Analyse syntaxique modulaire</mots_cles>
			<title></title>
			<abstract>We present a tool, ILIMP, which takes as input a French raw text and which produces as output the input text in which every occurrence of the word il is tagged either with the tag [ANA] for anaphoric or [IMP] for expletive. This tool is therefore designed to distinguish the anaphoric occurrences of il, for which an anaphora resolution system has to look for an antecedent, from the expletive occurrences of this pronoun, for which it does not make sense to look for an antecedent. The precision rate for ILIMP is 97,5%. The few errors are analyzed in detail. Other tasks using the method for ILIMP are described briefly, as well as the use of ILIMP in a modular syntactic analysis system.</abstract>
			<keywords>Expletive pronouns, Anaphoric pronouns, Lexicon-Grammar, Automata, Anaphora resolution, Modular syntactic analysis</keywords>
		</article>
		<article id="taln-2005-long-014" session="Analyse lexicale">
			<auteurs>
				<auteur>
					<nom>Marie-Paule Jacques</nom>
					<email>mpjacques@univ-tlse2.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université Toulouse II le Mirail</affiliation>
			</affiliations>
			<titre>Que : la valse des étiquettes</titre>
			<type>long</type>
			<pages>133-142</pages>
			<resume>Nous présentons ici une stratégie d’étiquetage et d’analyse syntaxique de que. Cette forme est en effet susceptible d’appartenir à trois catégories différentes et d’avoir de multiples emplois pour chacune de ces catégories. Notre objectif est aussi bien d’en assurer un étiquetage correct que d’annoter les relations de dépendance que que entretient avec les autres mots de la phrase. Les deux étapes de l’analyse mobilisent des ressources différentes.</resume> <mots_cles>Analyse syntaxique automatique, étiquetage morphosyntaxique</mots_cles>
			<title></title>
			<abstract>In this paper I present a method for tagging and parsing the grammatical word que. This word is particularly difficult to tag because it may belong to three different categories and may give rise to many constructions for each category. My aim is to assign the correct tag and to annotate dependency relations between que and the other words of the sentence.</abstract> <keywords>Automatic parsing, tagging</keywords>
		</article>
		<article id="taln-2005-long-015" session="Analyse lexicale">
			<auteurs>
				<auteur>
					<nom>Chiraz Ben Othmane Zribi</nom>
					<email>Chiraz.benothmane@riadi.rnu.tn</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Fériel Ben Fraj</nom>
					<email>Feriel.BenFraj@riadi.rnu.tn</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Mohamed Ben Ahmed</nom>
					<email>Mohamed.BenAhmed@riadi.rnu.tn</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">RIADI, Université la Manouba</affiliation>
			</affiliations>
			<titre>Un système Multi-Agent pour la détection et la correction des erreurs cachées en langue Arabe</titre>
			<type>long</type>
			<pages>143-152</pages>
			<resume>Cet article s’intéresse au problème des erreurs orthographiques produisant des mots lexicalement corrects dans des textes en langue arabe. Après la description de l’influence des spécificités de la langue arabe sur l’augmentation du risque de commettre ces fautes cachées, nous proposons une classification hiérarchique de ces erreurs en deux grandes catégories ; à savoir syntaxique et sémantique. Nous présentons, également, l’architecture multi-agent que nous avons adoptée pour la détection et la correction des erreurs cachées en textes arabes. Nous examinons alors, les comportements sociaux des agents au sein de leurs organisations respectives et de leur environnement. Nous exposons vers la fin la mise en place et l’évaluation du système réalisé.</resume>
			<mots_cles>Erreurs orthographiques cachées, Détection, Correction, Système multiagent, Analyse linguistique, Langue arabe</mots_cles>
			<title></title>
			<abstract>In this paper, we address the problem of detecting and correcting hidden spelling errors in Arabic texts. Hidden spelling errors are morphologically valid words and therefore they cannot be detected or corrected by conventional spell checking programs. In the work presented here, we investigate this kind of errors as they relate to the Arabic language. We start by proposing a classification of these errors in two main categories: syntactic and semantic, then we present our multi-agent system for hidden spelling errors detection and correction. The multi-agent architecture is justified by the need for collaboration, parallelism and competition, in addition to the need for information exchange between the different analysis phases. Finally, we describe the testing framework used to evaluate the system implemented.</abstract>
			<keywords>Hidden spelling errors, Detection, Correction, Multi-Agent System, Linguistic analysis, Arabic language</keywords>
		</article>
		<article id="taln-2005-long-016" session="Représentations sémantiques">
			<auteurs>
				<auteur>
					<nom>Sylvain Kahane</nom>
					<email>sk@ccr.jussieu.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Modyco, Université Paris 10</affiliation>
				<affiliation affiliationId="2">Lattice, Université Paris 7</affiliation>
			</affiliations>
			<titre>Structure des représentations logiques et interface sémantique-syntaxe</titre>
			<type>long</type>
			<pages>153-162</pages>
			<resume>Cet article s’intéresse à la structure des représentations logiques des énoncés en langue naturelle. Par représentation logique, nous entendons une représentation sémantique incluant un traitement de la portée des quantificateurs. Nous montrerons qu’une telle représentation combine fondamentalement deux structures sous-jacentes, une structure « prédicative » et une structure hiérarchique logique, et que la distinction des deux permet, par exemple, un traitement élégant de la sous-spécification. Nous proposerons une grammaire polarisée pour manipuler directement la structure des représentations logiques (sans passer par un langage linéaire avec variables), ainsi qu’une grammaire pour l’interface sémantique-syntaxe.</resume>
			<mots_cles>Logique du premier ordre, calcul des prédicats, représentation sémantique, relation prédicatargument, quantificateur, grammaire d’unification polarisée, grammaire de dépendance, dag, interface syntaxe-sémantique</mots_cles>
			<title></title>
			<abstract>This paper aims at the structure of logic representations in natural languages. By logic representation we mean a semantic representation including a quantifier scope processing. We show that such a representation basically combines two underlying substructures, a “predicative” structure and a logic hierarchic structure, and that the identification of the two allows for an elegant processing of underspecification. We will propose a polarized grammar that directly handles the structure of logic representations (without using a linear language with variables), as well as a grammar for the semantics-syntax interface.</abstract>
			<keywords>First order logic, predicate calculus, semantic representation, predicate-argument relation, quantifier, polarized unification grammar, dependency grammar, dag, syntax-semantics interface</keywords>
		</article>
		<article id="taln-2005-long-017" session="Représentations sémantiques">
			<auteurs>
				<auteur>
					<nom>Manny Rayner</nom>
					<email>mrayner@riacs.edu</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Pierrette Bouillon</nom>
					<email>Pierrette.Bouillon@issco.unige.ch</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Marianne Santaholma</nom>
					<email>Marianne.Santaholma@eti.unige.ch</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Yukie Nakao</nom>
					<email>yukie-n@khn.nict.go.jp</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université de Genève</affiliation>
				<affiliation affiliationId="2">National Institute for Communications Technology</affiliation>
			</affiliations>
			<titre></titre>
			<type>long</type>
			<pages>163-172</pages>
			<resume>Cet article dresse un aperçu du système MedSLT, un système de traduction de la parole dans le domaine médical pour un vocabulaire limité. Il met l’accent sur le problème du choix du type de représentation pour les constructions temporelles et causales. Nous montrons que celles-ci ne peuvent pas être représentées par des structures plates, généralement utilisées pour ce type d’application, mais qu’elles nécessitent des stuctures plus riches, enchâssées, qui permettent d’obtenir une traduction plus adéquate. Nous expliquons comment produire ces représentations et écrire des règles de traduction économiques qui mettent en correspondance les représentations sources dans la représentation interlingue correspondante.</resume>
			<mots_cles>reconnaissance de la parole, traduction de la parole, aide au diagnostic médical</mots_cles>
			<title>Representational and architectural issues in a limited-domain medical speech translator</title>
			<abstract>We present an overview of MedSLT, a medium-vocabulary medical speech translation system, focussing on the representational issues that arise when translating temporal and causal concepts. Although flat key/value structures are strongly preferred as semantic representations in speech understanding systems, we argue that it is infeasible to handle the necessary range of concepts using only flat structures. By exploiting the specific nature of the task, we show that it is possible to implement a solution which only slightly extends the representational complexity of the semantic representation language, by permitting an optional single nested level representing a subordinate clause construct. We sketch our solutions to the key problems of producing minimally nested representations using phrase-spotting methods, and writing cleanly structured rule-sets that map temporal and phrasal representations into a canonical interlingual form.</abstract>
			<keywords>speech understanding, speech translation, computer-aided diagnosis</keywords>
		</article>
		<article id="taln-2005-long-018" session="Représentations sémantiques">
			<auteurs>
				<auteur>
					<nom>Thierry Poibeau</nom>
					<email>thierry.poibeau@lipn.univ-paris13.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIPN-CNRS</affiliation>
			</affiliations>
			<titre>Sur le statut référentiel des entités nommées</titre>
			<type>long</type>
			<pages>173-182</pages>
			<resume>Nous montrons dans cet article qu’une même entité peut être désignée de multiples façons et que les noms désignant ces entités sont par nature polysémiques. L’analyse ne peut donc se limiter à une tentative de résolution de la référence mais doit mettre en évidence les possibilités de nommage s’appuyant essentiellement sur deux opérations de nature linguistique : la synecdoque et la métonymie. Nous présentons enfin une modélisation permettant de rendre explicite les différentes désignations en discours, en unifiant le mode de représentation des connaissances linguistiques et des connaissances sur le monde.</resume>
			<mots_cles>Entités nommées, référence, sémantique lexicale</mots_cles>
			<title></title>
			<abstract>We show in this paper that, on the one hand, named entities can be designated using different denominations and that, on the second hand, names denoting named entities are polysemous. The analysis cannot be limited to reference resolution but should take into account naming strategies, which are mainly based on two linguistic operations: synecdoche and metonymy. Lastly, we present a model that explicitly represents the different denominations in discourse, unifying the way to represent linguistic knowledge and world knowledge.</abstract>
			<keywords>Named entities, reference, lexical semantics</keywords>
		</article>
		<article id="taln-2005-long-019" session="Texte">
			<auteurs>
				<auteur>
					<nom>Atefeh Farzindar</nom>
					<email>farzinda@iro.umontreal.ca</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Guy Lapalme</nom>
					<email>lapalme@iro.umontreal.ca</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université de Montréal</affiliation>
			</affiliations>
			<titre>Production automatique du résumé de textes juridiques: évaluation de qualité et d'acceptabilité</titre>
			<type>long</type>
			<pages>183-192</pages>
			<resume>Nous décrivons un projet de production de résumé automatique de textes pour le domaine juridique pour lequel nous avons utilisé un corpus des jugements de la cour fédérale du Canada. Nous présentons notre système de résumé LetSum ainsi que l’évaluation des résumés produits. L’évaluation de 120 résumés par 12 avocats montre que la qualité des résumés produits par LetSum est comparable avec celle des résumés écrits par des humains.</resume>
			<mots_cles>résumé automatique, fiches de résumé, textes juridiques, évaluation d’un résumé</mots_cles>
			<title></title>
			<abstract>We describe an automatic text summarisation project for the legal domain for which we use a corpus of judgments of the federal court of Canada. We present our summarization system, called LetSum and the evaluation of produced summaries. The evaluation of 120 summaries by 12 lawyers shows that the quality of the summaries produced by LetSum is approximately at the same level as the summaries written by humans.</abstract>
			<keywords>automatic text summarization, summary table, legals texts, evaluation of a summary</keywords>
		</article>
		<article id="taln-2005-long-020" session="Texte">
			<auteurs>
				<auteur>
					<nom>Mehdi Yousfi-Monod</nom>
					<email>yousfi@lirmm.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Violaine Prince</nom>
					<email>prince@lirmm.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Laboratoire LIRMM, CNRS-Université Montpellier 2</affiliation>
			</affiliations>
			<titre>Utilisation de la structure morpho-syntaxique des phrases dans le résumé automatique</titre>
			<type>long</type>
			<pages>193-202</pages>
			<resume>Nous proposons une technique de résumé automatique de textes par contraction de phrases. Notre approche se fonde sur l’étude de la fonction syntaxique et de la position dans l’arbre syntaxique des constituants des phrases. Après avoir défini la notion de constituant, et son rôle dans l’apport d’information, nous analysons la perte de contenu et de cohérence discursive que la suppression de constituants engendre. Nous orientons notre méthode de contraction vers les textes narratifs. Nous sélectionnons les constituants à supprimer avec un système de règles utilisant les arbres et variables de l’analyse morpho-syntaxique de SYGFRAN [Cha84]. Nous obtenons des résultats satisfaisants au niveau de la phrase mais insuffisants pour un résumé complet. Nous expliquons alors l’utilité de notre système dans un processus plus général de résumé automatique.</resume>
			<mots_cles>résumé automatique, compression de phrases, analyse syntaxique</mots_cles>
			<title></title>
			<abstract>We propose an automated text summarization through sentence compression. Our approach uses constituent syntactic function and position in the sentence syntactic tree. We first define the idea of a constituent as well as its role as an information provider, before analyzing contents and discourse consistency losses caused by deleting such a constituent. We explain why our method works best with narrative texts. With a rule-based system using SYGFRAN’s morpho-syntactic analysis for French [Cha84], we select removable constituents. Our results are satisfactory at the sentence level but less effective at the whole text level. So we explain the usefulness of our system in a more general automatic summarization process.</abstract>
			<keywords>automatic summarization, sentence compression, syntactic analysis</keywords>
		</article>
		<article id="taln-2005-long-021" session="Texte">
			<auteurs>
				<auteur>
					<nom>Yves Bestgen</nom>
					<email>yves.bestgen@psp.ucl.ac.be</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université catholique de Louvain</affiliation>
			</affiliations>
			<titre>Amélioration de la segmentation automatique des textes grâce aux connaissances acquises par l'analyse sémantique latente</titre>
			<type>long</type>
			<pages>203-212</pages>
			<resume>Choi, Wiemer-Hastings et Moore (2001) ont proposé d'employer l'analyse sémantique latente (ASL) pour extraire des connaissances sémantiques à partir de corpus afin d'améliorer l'efficacité d'un algorithme de segmentation des textes. En comparant l'efficacité du même algorithme selon qu'il prend en compte des connaissances sémantiques complémentaires ou non, ils ont pu montrer les bénéfices apportés par ces connaissances. Dans leurs expériences cependant, les connaissances sémantiques avaient été extraites d'un corpus qui contenait les textes à segmenter dans la phase de test. Si cette hyperspécificité du corpus d'apprentissage explique la plus grande partie de l'avantage observé, on peut se demander s'il est possible d'employer l'ASL pour extraire des connaissances sémantiques génériques pouvant être employées pour segmenter de nouveaux textes. Les deux expériences présentées ici montrent que la présence dans le corpus d'apprentissage du matériel de test a un effet important, mais également que les connaissances sémantiques génériques dérivées de grands corpus améliorent l'efficacité de la segmentation.</resume>
			<mots_cles>Segmentation automatique de textes, Analyse sémantique latente (ASL)</mots_cles>
			<title></title>
			<abstract>Choi, Wiemer-Hastings and Moore (2001) proposed to use latent Semantic Analysis to extract semantic knowledge from corpora in order to improve the accuracy of a text segmentation algorithm. By comparing the accuracy of the very same algorithm depending on whether or not it takes into account complementary semantic knowledge, they were able to show the benefit derived from such knowledge. In their experiments, semantic knowledge was, however, acquired from a corpus containing the texts to be segmented in the test phase. If this hyper-specificity of the training corpus explains the largest part of the benefit, one may wonder if it is possible to use LSA to acquire generic semantic knowledge that can be used to segment new texts. The two experiments reported here show that the presence of the test materials in the training corpus has an important effect, but also that the generic semantic knowledge derived from large corpora clearly improves the segmentation accuracy.</abstract>
			<keywords>Automatic text segmentation, Latent semantic analysis (LSA)</keywords>
		</article>
		<article id="taln-2005-long-022" session="Texte">
			<auteurs>
				<auteur>
					<nom>Nicolas Hernandez</nom>
					<email>Hernandez@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Brigitte Grau</nom>
					<email>Grau@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIMSI-CNRS</affiliation>
			</affiliations>
			<titre>Détection Automatique de Structures Fines du Discours</titre>
			<type>long</type>
			<pages>213-222</pages>
			<resume>Dans ce papier, nous présentons un système de Détection de Structures fines de Texte (appelé DST). DST utilise un modèle prédictif obtenu par un algorithme d’apprentissage qui, pour une configuration d’indices discursifs donnés, prédit le type de relation de dépendance existant entre deux énoncés. Trois types d’indices discursifs ont été considérés (des relations lexicales, des connecteurs et un parallélisme syntaxico-sémantique) ; leur repérage repose sur des heuristiques. Nous montrons que notre système se classe parmi les plus performants.</resume>
			<mots_cles>Navigation intra-documentaire, analyse thématique, structures du discours, relations discursives, subordination et coordination, parallélisme lexico-syntaxico-sémantique, modèle d’apprentissage, analyses linguistiques</mots_cles>
			<title></title>
			<abstract>In this paper, we present a system which aims at detecting fine-grained text structures (we call it DST). Based on discursive clues, DST uses a learning model to predict dependency relations between two given utterances. As discourse clues, we consider lexical relations, connectors and key phrases, and parallelism. We show that our system implements an improvement over current systems.</abstract>
			<keywords>Text browsing, topic analysis, text structures, discursive relations, subordination and coordination, lexical, syntactic and semantic parallelism, learning model, linguistic analysis</keywords>
		</article>
		<article id="taln-2005-long-023" session="Traduction">
			<auteurs>
				<auteur>
					<nom>Alexandre Patry</nom>
					<email>patryale@iro.umontreal.ca</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Philippe Langlais</nom>
					<email>felipe@iro.umontreal.ca</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université de Montréal</affiliation>
			</affiliations>
			<titre>Paradocs: un système d'identification automatique de documents parallèles</titre>
			<type>long</type>
			<pages>223-232</pages>
			<resume>Les corpus parallèles sont d’une importance capitale pour les applications multilingues de traitement automatique des langues. Malheureusement, leur rareté est le maillon faible de plusieurs applications d’intérêt. Extraire de tels corpus duWeb est une solution viable, mais elle introduit une nouvelle problématique : il n’est pas toujours trivial d’identifier les documents parallèles parmi tous ceux qui ont été extraits. Dans cet article, nous nous intéressons à l’identification automatique des paires de documents parallèles contenues dans un corpus bilingue. Nous montrons que cette tâche peut être accomplie avec précision en utilisant un ensemble restreint d’invariants lexicaux. Nous évaluons également notre approche sur une tâche de traduction automatique et montrons qu’elle obtient des résultats supérieurs à un système de référence faisant usage d’un lexique bilingue.</resume> <mots_cles>Corpus parallèles, apprentissage automatique, traduction automatique</mots_cles>
			<title></title>
			<abstract>Parallel corpora are playing a crucial role in multilingual natural language processing. Unfortunately, the availability of such a resource is the bottleneck in most applications of interest. Mining the web for such a resource is a viable solution that comes at a price : it is not always easy to identify parallel documents among the crawled material. In this study we address the problem of automatically identifying the pairs of texts that are translation of each other in a set of documents. We show that it is possible to automatically build particularly efficient content-based methods that make use of very little lexical knowledge. We also evaluate our approach toward a front-end translation task and demonstrate that our parallel text classifier yields better performances than another approach based on a rich lexicon.</abstract> <keywords>Parallel documents, machine learning, machine translation</keywords>
		</article>
		<article id="taln-2005-long-024" session="Traduction">
			<auteurs>
				<auteur>
					<nom>Michel Simard</nom>
					<email>Michel.Simard@xrce.xerox.com</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Nicola Cancedda</nom>
					<email>Nicola.Cancedda@xrce.xerox.com</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Bruno Cavestro</nom>
					<email>Bruno.Cavestro@xrce.xerox.com</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Marc Dymetman</nom>
					<email>Marc.Dymetman@xrce.xerox.com</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Eric Gaussier</nom>
					<email>Eric.Gaussier@xrce.xerox.com</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Cyril Goutte</nom>
					<email>Cyril.Goutte@xrce.xerox.com</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Philippe Langlais</nom>
					<email>felipe@iro.umontreal.ca</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Arne Mauser</nom>
					<email>arne.mauser@kullen.rwth-aachen.de</email>
					<affiliationId>4</affiliationId>
				</auteur>
				<auteur>
					<nom>Kenji Yamada</nom>
					<email>kyamada@isi.edu</email>
					<affiliationId>3</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">RALI, Université de Montréal</affiliation>
				<affiliation affiliationId="2">Xerox Research Centre Europe</affiliation>
				<affiliation affiliationId="3">ISI</affiliation>
				<affiliation affiliationId="4">RWTH</affiliation>
			</affiliations>
			<titre>Une approche à la traduction automatique statistique par segments discontinus</titre>
			<type>long</type>
			<pages>233-242</pages>
			<resume>Cet article présente une méthode de traduction automatique statistique basée sur des segments non-continus, c’est-à-dire des segments formés de mots qui ne se présentent pas nécéssairement de façon contiguë dans le texte. On propose une méthode pour produire de tels segments à partir de corpus alignés au niveau des mots. On présente également un modèle de traduction statistique capable de tenir compte de tels segments, de même qu’une méthode d’apprentissage des paramètres du modèle visant à maximiser l’exactitude des traductions produites, telle que mesurée avec la métrique NIST. Les traductions optimales sont produites par le biais d’une recherche en faisceau. On présente finalement des résultats expérimentaux, qui démontrent comment la méthode proposée permet une meilleure généralisation à partir des données d’entraînement.</resume>
			<mots_cles>traduction automatique statistique, segments discontinus, modèles log-linéaires</mots_cles>
			<title></title>
			<abstract>This paper presents a phrase-based statistical machine translation method, based on non-contiguous phrases, i.e. phrases with gaps. A method for producing such phrases from a word-aligned corpora is proposed. A statistical translation model is also presented that deals with such phrases, as well as a training method based on the maximization of translation accuracy, as measured with the NIST evaluation metric. Translations are produced by means of a beam-search decoder. Experimental results are presented, that demonstrate how the proposed method allows to better generalize from the training data.</abstract>
			<keywords>statistical machine translation, discontinuous phrases, log-linear models</keywords>
		</article>
		<article id="taln-2005-long-025" session="Traduction">
			<auteurs>
				<auteur>
					<nom>Sylwia Ozdowska</nom>
					<email>ozdowska@univ-tlse2.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Vincent Claveau</nom>
					<email>vincent.claveau@umontreal.ca</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">ERSS - Université de Toulouse le Mirail</affiliation>
				<affiliation affiliationId="2">OLST - Université de Montréal</affiliation>
			</affiliations>
			<titre>Alignement de mots par apprentissage de règles de propagation syntaxique en corpus de taille restreinte	</titre>
			<type>long</type>
			<pages>243-252</pages>
			<resume>Cet article présente et évalue une approche originale et efficace permettant d’aligner automatiquement un bitexte au niveau des mots. Pour cela, cette approche tire parti d’une analyse syntaxique en dépendances des bitextes effectuée par les outils SYNTEX et utilise une technique d’apprentissage artificiel, la programmation logique inductive, pour apprendre automatiquement des règles dites de propagation. Celles-ci se basent sur les informations syntaxiques connues pour ensuite aligner les mots avec une grande précision. La méthode est entièrement automatique, et les résultats évalués sur les données de la campagne d’alignement HLT montrent qu’elle se compare aux meilleures techniques existantes. De plus, alors que ces dernières nécessitent plusieurs millions de phrases pour s’entraîner, notre approche n’en requiert que quelques centaines. Enfin, l’examen des règles de propagation inférées permet d’identifier facilement les cas d’isomorphismes et de non-isomorphismes syntaxiques entre les deux langues traitées.</resume>
			<mots_cles>alignement de mots, corpus alignés, apprentissage artificiel, programmation logique inductive, analyse syntaxique</mots_cles>
			<title></title>
			<abstract>This paper presents and evaluates an effective yet original approach to automatically align bitexts at the word level. This approach relies on a syntactic dependency analysis of the texts provided by the tools SYNTEX and uses a machine-learning technique, namely inductive logic programming, to automatically infer rules called propagation rules. These rules make the most of the syntactic information to precisely align words. This approach is entirely automatic, and results obtained on the data of the HLT evaluation campaign rival the ones of the best existing alignment systems. Moreover, our system uses very few training data: only hundreds of sentences compared to millions for the existing systems. Furthermore, syntactic isomorphisms between the two spotted languages are easily identified through a linguistic examination of the inferred propagation rules.</abstract>
			<keywords>word alignment, aligned corpus, machine learning, inductive logic programming, syntactic analysis</keywords>
		</article>
		<article id="taln-2005-long-026" session="Traduction">
			<auteurs>
				<auteur>
					<nom>Vincent Claveau</nom>
					<email>vincent.claveau@umontreal.ca</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Pierre Zweigenbaum</nom>
					<email>pz@biomath.jussieu.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">OLST - Université de Montréal</affiliation>
				<affiliation affiliationId="2">AP-HP &amp; INSERM &amp; INaLCO</affiliation>
			</affiliations>
			<titre>Traduction de termes biomédicaux par inférence de transducteurs</titre>
			<type>long</type>
			<pages>253-262</pages>
			<resume>Cet article propose et évalue une méthode de traduction automatique de termes biomédicaux simples du français vers l’anglais et de l’anglais vers le français. Elle repose sur une technique d’apprentissage artificiel supervisée permettant d’inférer des transducteurs à partir d’exemples de couples de termes bilingues ; aucune autre ressource ou connaissance n’est requise. Ces transducteurs, capturant les grandes régularités de traduction existant dans le domaine biomédical, sont ensuite utilisés pour traduire de nouveaux termes français en anglais et vice versa. Les évaluations menées montrent que le taux de bonnes traductions de notre technique se situe entre 52 et 67%. À travers un examen des erreurs les plus courantes, nous identifions quelques limites inhérentes à notre approche et proposons quelques pistes pour les dépasser. Nous envisageons enfin plusieurs extensions à ce travail.</resume>
			<mots_cles>Traduction automatique de termes, terminologie biomédicale, apprentissage artificiel, inférence de transducteurs</mots_cles>
			<title></title>
			<abstract>This paper presents and evaluates a method to automatically translate simple terms from French into English and English into French in the biomedical domain. It relies on a machine-learning technique that infers transducers from examples of bilingual pairs of terms; no additional resources or knowledge is needed. Then, these transducers, making the most of high translation regularities in the biomedical domain, can be used to translate new French terms into English or vice versa. Evaluations reported show that our technique achieves good successful translation rates (between 52 and 67%). When examining at the most frequent errors made, some inherent limits of our approach are identified, and several avenues are proposed in order to bypass them. Finally, some perspectives are put forward to extend this work.</abstract>
			<keywords>Automatic translation of terms, biomedical terminology, machine learning, transducer induction</keywords>
		</article>
		<article id="taln-2005-long-027" session="Dialogue">
			<auteurs>
				<auteur>
					<nom>Frédéric Landragin</nom>
					<email>Frederic.Landragin@thalesgroup.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Thales Research &amp; Technology</affiliation>
			</affiliations>
			<titre>Traitement automatique de la saillance</titre>
			<type>long</type>
			<pages>263-272</pages>
			<resume>Lorsque nous écoutons un énoncé ou que nous lisons un texte, les phénomènes de saillance accaparent notre attention sur une entité du discours particulière. Cette notion de saillance comprend un grand nombre d’aspects, incluant des facteurs lexicaux, syntaxiques, sémantiques, pragmatiques, ou encore cognitifs. En tant que point de départ de l’interprétation du langage, la saillance fonctionne de pair avec la structure communicative. Dans cet article, notre but principal est de montrer comment aboutir à un modèle computationnel de la saillance, qui soit valable aussi bien pour la saillance linguistique que pour la saillance visuelle. Pour cela, nous retenons une liste de facteurs qui contribuent à rendre saillante une entité. Dans le cas d’une entité du discours, cette approche nous permet de clarifier les rapports entre saillance et structure communicative. Nous définissons nos principes de primordialité et de singularité, puis nous passons en revue les différentes méthodes de quantification de la saillance qui sont compatibles avec ces principes. Nous illustrons alors l’une d’entre elles sur un exemple linguistique et sur un exemple visuel.</resume>
			<mots_cles>facteurs de saillance, saillance linguistique, saillance visuelle, principe de primordialité, principe de singularité, structure communicative, méthodes de quantification</mots_cles>
			<title></title>
			<abstract>Salience attracts the attention on a particular discourse entity when hearing an utterance or reading a text. Salience is linked to a wide set of aspects from lexical, syntactic, semantic, and pragmatic factors to cognitive factors. Being the starting point of the interpretation process, salience works in close connection with communicative structure. In this article, our main purpose is to show how to tend to a computational model of salience, that can be used for linguistic salience as well as for visual salience. With this aim, we provide a list of factors that contribute to making an entity salient. For a discourse entity, this approach allows us to clarify the links between salience and communicative structure. We define our principles of primordiality and singularity, and we discuss the possible methods to quantify salience that are compatible with these principles. Then we illustrate one of them with a linguistic example and a visual example.</abstract>
			<keywords>salience factors, linguistic salience, visual salience, primordiality principle, singularity principle, communicative structure, quantifying methods</keywords>
		</article>
		<article id="taln-2005-long-028" session="Dialogue">
			<auteurs>
				<auteur>
					<nom>Anne Xuereb</nom>
					<email>Anne.Xuereb@imag.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Jean Caelen</nom>
					<email>Jean.Caelen@imag.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">CLIPS-IMAG</affiliation>
			</affiliations>
			<titre>Topiques dialogiques</titre>
			<type>long</type>
			<pages>273-282</pages>
			<resume>Nous présentons dans cet article une extension de la SDRT (Segmented Discourse Representation Theory), pour un modèle d'interprétation pragmatique d’un système de dialogue homme-machine. Partant d’une discussion sur les présupposés et les implicatures conversationnelles, nous analysons l’approche de Ducrot en Vue d’une intégration des topoï dans notre modèle. Nous y ajoutons la prise en compte des attentes dans le dialogue (effets projectifs des actes interlocutoires). Enfin nous proposons un mécanisme de résolution logique qui consiste à introduire plus systématiquement un nœud topique dans la SDRS (Discourse Represenlation Structure). Nous décrivons dans cet article les principes de traitement pragmatique mis en œuvre, et nous illustrons le processus d’analyse à l’aide d'un exemple.</resume>
			<mots_cles>Interprétation pragmatique, dialogue homme-machine</mots_cles>
			<title></title>
			<abstract>We present in this paper an extension of the SDRT model for the pragmatic interpretation in a man-machine dialogue system. After a discussion on presupposition and implicature phenomena we consider the Ducrot’s approach based on topos concept. We consider also the point of view in which a speech act is an “expectation” of some result in the future, in a kind of projective effect. Then we describe a logical process including systematically a “topic node” in the SDRS (Discourse Representation Structure), subsuming the rhetoric relations having implications or hypothetic implications between utterances. Our paper focuses on the pragmatic processing principles for resolving implications in the dialogue. This is illustrated by an example.</abstract>
			<keywords>Pragmatic analysis, man-machine dialogue</keywords>
		</article>
		<article id="taln-2005-long-029" session="Dialogue">
			<auteurs>
				<auteur>
					<nom>Sophie Rosset</nom>
					<email>rosset@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Delphine Tribout</nom>
					<email>tribout@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIMSI - CNRS</affiliation>
			</affiliations>
			<titre>Détection automatique d'actes de dialogue par l'utilisation d'indices multiniveaux</titre>
			<type>long</type>
			<pages>283-292</pages>
			<resume>Ces dernières années, il y a eu de nombreux travaux portant sur l’utilisation d’actes de dialogue pour caractériser les dialogues homme-homme ou homme-machine. Cet article fait état de nos travaux sur la détection automatique d’actes de dialogue dans des corpus réels de dialogue homme-homme. Notre travail est fondé essentiellement sur deux hypothèses . (i) la position des mots et la classe sémantique du mot sont plus importants que les mots eux-mêmes pour identifier l’acte de dialogue et (ii) il y a une forte prédictivité dans la succession des actes de dialogues portés sur un même segment dialogique. Une approche de type Memory Based Learning a été utilisée pour la détection automatique des actes de dialogue. Le premier modèle n’utilise pas d’autres informations que celles contenus dans le tour de parole. Dans lex expériences suivantes, des historiques dialogiques de taille variables sont utilisés. Le taux d’erreur de détection d’actes de dialogue est d’environ 16% avec le premier modèle est descend avec une utilisation plus large de l’historique du dialogue à environ 14%.</resume>
			<mots_cles>actes de dialogue, dialogue homme homme, détection automatique, indices multiniveaux</mots_cles>
			<title></title>
			<abstract>Recently there has been growing interest in using dialog acts to characterize humanhuman and human-machine dialogs. This paper reports on our experience in the annotation and the automatic detection of dialog acts in human-human spoken dialog corpora. Our work is based on two hypotheses: first, word position is more important than the exact word in identifying the dialog act; and second, there is a strong grammar constraining the sequence of dialog acts. A memory based learning approach has been used to detect dialog acts. In a first set of experiments only the information contained in each turn is used and in a second set, different histories of the dialogue are used. A dialog act error rate of about 16 % is obtained for the simplest model. Using other informations, such as history of the dialog, the results grow up to 14%.</abstract>
			<keywords>dialog acts, human human dialog, automatic detection of dialog acts, mulilevel information</keywords>
		</article>
		<article id="taln-2005-long-030" session="Sémantique et corpus">
			<auteurs>
				<auteur>
					<nom>Goritsa Ninova</nom>
					<email>cylvago@yahoo.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Adeline Nazarenko</nom>
					<email>Adeline.Nazarenko@lipn.univ-paris13.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Thierry Hamon</nom>
					<email>Thierry.Hamon@lipn.univ-paris13.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Sylvie Szulman</nom>
					<email>Sylvie.Szulman@lipn.univ-paris13.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Laboratoire d'Informatique de Paris-Nord (LIPN)</affiliation>
			</affiliations>
			<titre>Comment mesurer la couverture d'une ressource terminologique pour un corpus ?</titre>
			<type>long</type>
			<pages>193-302</pages>
			<resume>Cet article propose une définition formelle de la notion de couverture lexicale. Celleci repose sur un ensemble de quatre métriques qui donnent une vue globale de l'adéquation d'une ressource lexicale à un corpus et permettent ainsi de guider le choix d'une ressource en fonction d'un corpus donné. Les métriques proposées sont testées dans le contexte de l'analyse de corpus spécialisés en génomique : 5 terminologies différentes sont confrontées à 4 corpus. La combinaison des valeurs obtenues permet de discerner différents types de relations entre ressources et corpus.</resume>
			<mots_cles>couverture lexicale, terminologie, statistique lexicale</mots_cles>
			<title></title>
			<abstract>This paper proposes a formal definition of the notion of lexical coverage. This definition is based on four metrics that give a global view over a lexical resource to corpus relationship, thus helping the choice of a relevant resource with respect to a given corpus. These metrics have been experimented in the context of specialised corpus analysis in genomics. 5 terminologies have been confronted to 4 different corpora. The combination of resulting figures reflects various types of corpus vs . resource relationships.</abstract>
			<keywords>lexical coverage, terminology, lexical statistics</keywords>
		</article>
		<article id="taln-2005-long-031" session="Sémantique et corpus">
			<auteurs>
				<auteur>
					<nom>Guillaume Jacquet</nom>
					<email>guillaume.jacquet@ens.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Fabienne Venant</nom>
					<email>fabienne.venant@ens.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LaTTICe CNRS UMR 8094</affiliation>
			</affiliations>
			<titre>Construction automatique de classes de sélection distributionnelle</titre>
			<type>long</type>
			<pages>303-312</pages>
			<resume>Cette étude se place dans le cadre général de la désambiguïsation automatique du sens d’un Verbe dans un énoncé donné. Notre méthode de désambiguïsation prend en compte la construction du Verbe, c’est-à-dire l’influence des éléments lexicaux et syntaxiques présents dans l’énoncé (cotexte). Nous cherchons maintenant à finaliser cette méthode en tenant compte des caractéristiques sémantiques du cotexte. Pour ce faire nous associons au corpus un espace distributionnel continu dans lequel nous construisons et Visualisons des classes distributionnelles. La singularité de ces classes est qu’elles sont calculées à la Volée. Elles dépendent donc non seulement du corpus mais aussi du contexte étudié. Nous présentons ici notre méthode de calcul de classes ainsi que les premiers résultats obtenus.</resume>
			<mots_cles>classes de sélection distributionnelle, espace distributionnel, désambiguïsation, corpus, contexte</mots_cles>
			<title></title>
			<abstract>This study is placed within the general framework of the automatic Verb sense disambiguation. To assign a meaning to a Verb, we take into account the construction of the Verb, i.e. the other lexical and syntactic units within the utterance (co-text). We now seek to finalize our method by taking into account the semantic features of this co-text. We associate with the corpus a continuous distributional space in which we build and Visualize distributional classes. The singularity of these classes is that they are computed "on line" for disambiguate a given context in the given corpus. They thus depend not only on the corpus but also on the studied context. We present here our method of computation of classes and first results obtained.</abstract>
			<keywords>semantic classes, distributional space, disambiguation, corpus, context</keywords>
		</article>
		<article id="taln-2005-long-032" session="Sémantique et corpus">
			<auteurs>
				<auteur>
					<nom>Ecaterina Rascu</nom>
					<email>kati@iai.uni-sb.de</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Kai Schirmer</nom>
					<email>kaischirmer@web.de</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Johann Haller</nom>
					<email>hans@iai.uni-sb.de</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Institut für Angewandte Informationsforschung</affiliation>
				<affiliation affiliationId="2">Schirmer Media Research</affiliation>
			</affiliations>
			<titre></titre>
			<type>long</type>
			<pages>313-322</pages>
			<resume>L'identification et l'évaluation des avis, opinions ou jugements exprimés sur un sujet, une entreprise, ou un produit sont des tâches essentielles dans le domaine de l'analyse des médias. L'étude d'opinion est employée pour repérer de nouvelles tendances, mesurer le degré de satisfaction des clients ou pour alerter quand des tendances négatives risquent d'être défavorable à l'image de marque de l'entreprise. Dans cet article nous présentons un outil de veille économique qui permet de classifier très finement des documents publiés en ligne ainsi que d'identifier et d'évaluer les opinions exprimées dans des articles en ligne et des forums de discussions. Après la présentation des diverses composantes du système et des ressources linguistiques utilisées, nous décrivons en détail SentA, la composante d'étude d'opinions, et évaluons sa performance.</resume>
			<mots_cles>Etude d’opinion, outil de veille économique, classification des opinions</mots_cles>
			<title>Sentiment Analysis for Issues Monitoring Using Linguistic Resources</title>
			<abstract>Sentiment analysis dealing with the identification and evaluation of opinions towards a topic, a company, or a product is an essential task within media analysis. It is used to study trends, determine the level of customer satisfaction, or warn immediately when unfavourable trends risk damaging the image of a company. In this paper we present an issues monitoring system which, besides text categorization, also performs an extensive sentiment analysis of online news and newsgroup postings. Input texts undergo a morpho-syntactic analysis, are indexed using a thesaurus and are categorized into user-specific classes. During sentiment analysis, sentiment expressions are identified and subsequently associated with the established topics. After presenting the various components of the system and the linguistic resources used, we describe in detail SentA, its sentiment analysis component, and evaluate its performance.</abstract>
			<keywords>Sentiment analysis, issues monitoring system, fine-grained sentiment classification</keywords>
		</article>
		<article id="taln-2005-long-033" session="Grammaires">
			<auteurs>
				<auteur>
					<nom>Marie-Laure Guénot</nom>
					<email>Marie-Laure.Guénot@lpl.univ-aix.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Laboratoire Parole et Langage - CNRS / Université de Provence</affiliation>
			</affiliations>
			<titre>Parsing de l'oral: traiter les disfluences</titre>
			<type>long</type>
			<pages>323-332</pages>
			<resume>Nous proposons une réflexion théorique sur la place d’un phénomène tel que celui des disfluences au sein d’une grammaire. Les descriptions fines qui en ont été données mènent à se demander quel statut accorder aux disfluences dans une théorie linguistique complète, tout en conservant une perspective globale de représentation, c’est-à-dire sans nuire à la cohérence et à l’homogénéité générale. Nous en introduisons une représentation formelle, à la suite de quoi nous proposons quelques mécanismes de parsing permettant de les traiter.</resume>
			<mots_cles>Disfluences, Parsing, Linguistique de corpus, Linguistique formelle, Développement de grammaires, Grammaire de Construction (CxG), Grammaires de Propriétés (GP)</mots_cles>
			<title></title>
			<abstract>We propose a theoretical reflexion about the place of a phenomenon like disfluencies, in a grammar. The precise descriptions that are available leads to a question : what status shall we give to disfluencies into a complete linguistic theory ?, keeping a global point of view and without compromising the coherence and the homogeneity of its representation. We introduce a formal representation of the phenomenon, and then we propose some parsing mechanisms in order to treat it.</abstract>
			<keywords>Disfluencies, Parsing, Corpus linguistics, Formal linguistics, Grammar development, Construction Grammar (CxG), Property Grammars (PG)</keywords>
		</article>
		<article id="taln-2005-long-034" session="Grammaires">
			<auteurs>
				<auteur>
					<nom>Christophe Benzitoun</nom>
					<email>Christophe.Benzitoun@up.univ-aix.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université de Provence, Equipe DELIC</affiliation>
			</affiliations>
			<titre>Description détaillée des subordonnées non dépendantes : le cas de "quand"</titre>
			<type>long</type>
			<pages>333-342</pages>
			<resume>De nombreux linguistes ont mis en évidence des cas de « subordonnées » non dépendantes dans de multiples langues dans le monde (Mithun, 2003 ; Haiman &amp; Thompson (eds), 1988). Ce phénomène a aussi été relevé en français, notamment pour un « subordonnant » tel que parce que (Debaisieux, 2001 ; Ducrot et al., 1975). Nous nous proposons de décrire un cas de « subordonnée » en quand non dépendante et de le représenter dans le cadre formel de Gerdes &amp; Kahane (à paraître).</resume>
			<mots_cles>syntaxe, subordination, dépendance, topologie</mots_cles>
			<title></title>
			<abstract>Many linguists have pointed out instances of non dependent clauses "subordinate in form" in various languages in the world (Mithun, 2003 ; Haiman &amp; Thompson (eds), 1988). Such cases have been found and informally analysed in French, for instance parce que (Debaisieux, 2001 ; Ducrot et al., 1975). We propose here to extend the analysis to cases of non dependent subordinate clauses involving quand and to integrate it in the formal framework of Gerdes &amp; Kahane (to appear).</abstract>
			<keywords>syntax, subordination, dependency, topology</keywords>
		</article>
		<article id="taln-2005-long-035" session="Grammaires">
			<auteurs>
				<auteur>
					<nom>Djamé Seddah</nom>
					<email>djame.seddah@loria.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Bertrand Gaiffe</nom>
					<email>bertrand.gaiffe@loria.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Loria</affiliation>
			</affiliations>
			<titre>Des arbres de dérivation aux forêts de dépendance : un chemin via les forêts partagées</titre>
			<type>long</type>
			<pages>343-352</pages>
			<resume>L'objectif de cet article est de montrer comment bâtir une structure de répresentation proche d'un graphe de dépendance à l'aide des deux structures de représentation canoniques fournies par les Grammaires d'Arbres Adjoints Lexicalisées . Pour illustrer cette approche, nous décrivons comment utiliser ces deux structures à partir d'une forêt partagée.</resume>
			<mots_cles>TAG, analyse syntaxique, sémantique, arbre de dépendance,forêt partagée, forêt de dérivation</mots_cles>
			<title></title>
			<abstract>This paper aims describing an approach to semantic representation in the Lexicalized Tree Adjoining Grammars (LTAG) paradigm : we show how to use all the informations contained in the two representation structures provided by the LTAG formalism in order to provide a dependency graph.</abstract>
			<keywords>TAG, syntax, semantic, dependency tree, shared forest, derivation forest</keywords>
		</article>
		<article id="taln-2005-long-036" session="Apprentissage">
			<auteurs>
				<auteur>
					<nom>Pierre Alain</nom>
					<email>pierre.alain@irisa.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Olivier Boeffard</nom>
					<email>olivier.boeffard@irisa.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">IRISA / Université de Rennes 1 - ENSSAT</affiliation>
			</affiliations>
			<titre>Evaluation des Modèles de Langage n-gram et n/m-multigram</titre>
			<type>long</type>
			<pages>353-362</pages>
			<resume>Cet article présente une évaluation de modèles statistiques du langage menée sur la langue Française. Nous avons cherché à comparer la performance de modèles de langage exotiques par rapport aux modèles plus classiques de n-gramme à horizon fixe. Les expériences réalisées montrent que des modèles de n-gramme à horizon variable peuvent faire baisser de plus de 10% en moyenne la perplexité d’un modèle de n-gramme à horizon fixe. Les modèles de n/m-multigramme demandent une adaptation pour pouvoir être concurrentiels.</resume>
			<mots_cles>Modèles de Langage statistiques, n-gramme, multigramme, évaluation</mots_cles>
			<title></title>
			<abstract>This paper presents an evaluation of statistical language models carried out on the French language. We compared the performance of some exotic models to the one of the more traditional ngram model. The experiments show that the variable n-gram models can drop more than 10% of the average perplexity for a fixed n-gram model. n/m-multigram models require an adaptation to be able to compete.</abstract>
			<keywords>Statistical Language Models, n-grams, phrase multigrams</keywords>
		</article>
		<article id="taln-2005-long-037" session="Apprentissage">
			<auteurs>
				<auteur>
					<nom>Fathi Debili</nom>
					<email>fathi.debili@wanadoo.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Emna Souissi</nom>
					<email>emna.souissi@isgs.rnu.tn</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">CNRS – ICAR – ENS LSH, 15, Parvis René Descartes, 69342 Lyon Cedex 07, France</affiliation>
				<affiliation affiliationId="2">ISG – Université de Sousse, BP 763 Sousse 4000 - Tunisie</affiliation>
			</affiliations>
			<titre>Y a-t-il une taille optimale pour les règles de successions intervenant dans l'étiquetage grammatical ?</titre>
			<type>long</type>
			<pages>363-372</pages>
			<resume>La quasi-totalité des étiqueteurs grammaticaux mettent en oeuvre des règles qui portent sur les successions ou collocations permises de deux ou trois catégories grammaticales. Leurs performances s’établissent à hauteur de 96% de mots correctement étiquetés, et à moins de 57% de phrases correctement étiquetées. Ces règles binaires et ternaires ne représentent qu’une fraction du total des règles de succession que l’on peut extraire à partir des phrases d’un corpus d’apprentissage, alors même que la majeure partie des phrases (plus de 98% d’entre elles) ont une taille supérieure à 3 mots. Cela signifie que la plupart des phrases sont analysées au moyen de règles reconstituées ou simulées à partir de règles plus courtes, ternaires en l’occurrence dans le meilleur des cas. Nous montrons que ces règles simulées sont majoritairement agrammaticales, et que l’avantage inférentiel qu’apporte le chaînage de règles courtes pour parer au manque d’apprentissage, plus marqué pour les règles plus longues, est largement neutralisé par la permissivité de ce processus dont toutes sortes de poids, scores ou probabilités ne réussissent pas à en hiérarchiser la production afin d’y distinguer le grammatical de l’agrammatical. Force est donc de reconsidérer les règles de taille supérieure à 3, lesquelles, il y a une trentaine d’années, avaient été d’emblée écartées pour des raisons essentiellement liées à la puissance des machines d’alors, et à l’insuffisance des corpus d’apprentissage. Mais si l’on admet qu’il faille désormais étendre la taille des règles de succession, la question se pose de savoir jusqu’à quelle limite, et pour quel bénéfice. Car l’on ne saurait non plus plaider pour une portée des règles aussi longue que les plus longues phrases auxquelles elles sont susceptibles d’être appliquées. Autrement dit, y a-t-il une taille optimale des règles qui soit suffisamment petite pour que leur apprentissage puisse converger, mais suffisamment longue pour que tout chaînage de telles règles pour embrasser les phrases de taille supérieure soit grammatical. La conséquence heureuse étant que poids, scores et probabilités ne seraient plus invoqués que pour choisir entre successions d’étiquettes toutes également grammaticales, et non pour éliminer en outre les successions agrammaticales. Cette taille semble exister. Nous montrons qu’au moyen d’algorithmes relativement simples l’on peut assez précisément la déterminer. Qu’elle se situe, compte tenu de nos corpus, aux alentours de 12 pour le français, de 10 pour l’arabe, et de 10 pour l’anglais. Qu’elle est donc en particulier inférieure à la taille moyenne des phrases, quelle que soit la langue considérée.</resume>
			<mots_cles>Etiquetage grammatical, règle de succession, taille des règles, chaînage de règles, règle attestée, règle simulée, discriminance, couverture, évaluation en usage vs évaluation en définition d’un ensemble de règles</mots_cles>
			<title>Is there an optimal n for n-grams used in part-of-speech tagging?</title>
			<abstract>Almost all part-of-speech taggers apply rules about permitted successions and collocations of two or three grammatical categories. Their performance amounts to 96 percent of correctly tagged words, and to less than 57 percent of correctly tagged sentences. These binary and ternary succession rules represent a small fraction of succession rules one can extract from sentences in a learning corpus, where most sentences (more than 98 percent of them) have a length of more than three words. In other words, most sentences are processed by rules that are reconstructed, or simulated, from shorter ones, here ternary at best. We show that most such simulated rules are agrammatical, and that, if some inferential benefit comes from the chaining of short rules to compensate inexistent learning, mainly in the case of long rules, this benefit is nullified by the permissive behaviour of this process, in which a variety of weights, scores or probability are ineffective in hierarchizing its production and yield a separation between grammatical and agrammatical rules. So we feel forced to look again at larger-than-ternary rules. However, if we admit a necessity of enlarging succession rules, we must ask the question "up to which limit, and for what profit". For we also decline to argue for rules as long as the longest sentences upon which they might apply. So the real question is, can we define an optimal size for rules, short enough for learning to converge, and long enough for any chaining of rules to deal with larger sentences to be grammatical? A positive result would be that weights, scores or probability would then be invoked only to decide between equally grammatical successions of tags, and no longer to eliminate agrammatical ones. This optimal size apparently exists. We show that the use of rather simple algorithms leads to its determination. And its value, according to our corpora, is near 12 for French, 10 for Arabic and 10 for English. Therefore, it is less than the average length of sentences, for each of these three languages.</abstract>
			<keywords>Part-of-speech tagging, tag sequences, rule length, rule composition, attested rule, simulated rule, evaluation of generation vs evaluation of analysis</keywords>
		</article>
		<article id="taln-2005-long-038" session="Apprentissage">
			<auteurs>
				<auteur>
					<nom>Didier Bourigault</nom>
					<email>didier.bourigault@univ-tlse2.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Cécile Frérot</nom>
					<email>cecile.frerot@univ-tlse2.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">ERSS-CNRS &amp; Université Toulouse le Mirail</affiliation>
			</affiliations>
			<titre>Acquisition et évaluation sur corpus de propriétés de sous-catégorisation syntaxique</titre>
			<type>long</type>
			<pages>373-382</pages>
			<resume>Cette étude est menée dans le cadre du développement de l’analyseur syntaxique de corpus Syntex et porte sur la tâche de désambiguïsation des rattachements prépositionnels. Les données de sous-catégorisation syntaxique exploitées par Syntex pour la désambiguïsation se présentent sous la forme de probabilités de sous-catégorisation (que telle unité lexicale - verbe, nom ou adjectif - se construise avec telle préposition). Elles sont acquises automatiquement à partir d’un corpus de 200 millions de mots, étiqueté et partiellement analysé syntaxiquement. Pour évaluer ces données, nous utilisons 4 corpus de test de genres variés, sur lesquels nous avons annoté à la main plusieurs centaines de cas de rattachement prépositionnels ambigus. Nous testons plusieurs stratégies de désambiguïsation, une stratégie de base, une stratégie endogène qui exploite des propriétés de sous-catégorisation spécifiques acquises à partir du corpus en cours de traitement, une stratégie exogène qui exploite des propriétés de sous-catégorisation génériques acquises à partir du corpus de 200 millions de mots, et enfin une stratégie mixte qui utilisent les deux types de ressources. L’analyse des résultats montre que la stratégie mixte est la meilleure, et que les performances de l’analyseur sur la tâche de désambiguïsation des rattachements prépositionnels varient selon les corpus de 79.4 % à 87.2 %.</resume>
			<mots_cles>analyse syntaxique, ambiguïté de rattachement prépositionnel, sous-catégorisation syntaxique</mots_cles>
			<title></title>
			<abstract>We carry out an experiment aimed at using subcategorization information into a syntactic parser for PP attachment disambiguation. The subcategorization lexicon consists of probabilities between a word (verb, noun, adjective) and a preposition. The lexicon is acquired automatically from a 200 million word corpus, that is partially tagged and parsed. In order to assess the lexicon, we use 4 different corpora in terms of genre and domain. We assess various methods for PP attachment disambiguation : an exogeous method relies on the sub-categorization lexicon whereas an endogenous method relies on the corpus specific ressource only and an hybrid method makes use of both. The hybrid method proves to be the best and the results vary from 79.4 % to 87.2 %.</abstract>
			<keywords>syntactic parsing, PP attachment disambiguation, subcategorization lexicon</keywords>
		</article>
		<article id="taln-2005-court-001" session="Posters">
			<auteurs>
				<auteur>
					<nom>Ahmed Amrani</nom>
					<email>amrani@esiea.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Yves Kodratoff</nom>
					<email>yk@lri</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Oriane Matte-Tailliez</nom>
					<email>oriane@lri</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">ESIEA Recherche</affiliation>
				<affiliation affiliationId="2">Laboratoire de Recherche en Informatique</affiliation>
			</affiliations>
			<titre>Induction de règles de correction pour l'étiquetage morphosyntaxique de la littérature de biologie en utilisant l'apprentissage actif</titre>
			<type>court</type>
			<pages>385-390</pages>
			<resume>Dans le contexte de l’étiquetage morphosyntaxique des corpus de spécialité, nous proposons une approche inductive pour réduire les erreurs les plus difficiles et qui persistent après étiquetage par le système de Brill. Nous avons appliqué notre système sur deux types de confusions. La première confusion concerne un mot qui peut avoir les étiquettes ‘verbe au participe passé’, ‘verbe au passé’ ou ‘adjectif’. La deuxième confusion se produit entre un nom commun au pluriel et un verbe au présent, à la 3ème personne du singulier. A l’aide d’interface conviviale, l’expert corrige l’étiquette du mot ambigu. A partir des exemples annotés, nous induisons des règles de correction. Afin de réduire le coût d’annotation, nous avons utilisé l’apprentissage actif. La validation expérimentale a montré une amélioration de la précision de l’étiquetage. De plus, à partir de l’annotation du tiers du nombre d’exemples, le niveau de précision réalisé est équivalent à celui obtenu en annotant tous les exemples.</resume>
			<mots_cles>Etiquetage morphosyntaxique, Apprentissage de règles, Apprentissage actif, fouille de textes</mots_cles>
			<title></title>
			<abstract>In the context of Part-of-Speech (PoS)-tagging of specialized corpora, we proposed an approach focusing on the most ‘important’ PoS-tags because mistaking them can lead to a total misunderstanding of the text. After tagging a biological corpus by Brill’s tagger, we noted persistent errors that are very hard to deal with. As an application, we studied two cases of different nature: first, confusion between past participle, adjective and preterit; second, confusion between plural nouns and verbs, 3rd person singular present. With a friendly user interface, the expert corrected the examples. Then, from these well-annotated examples, we induced rules. In order to reduce the cost of annotation, we used active learning. The experimental validation showed improvement in tagging precision and that on the basis of the annotation of one third of the examples we obtain a level of precision equivalent to the one reached by annotating all the examples.</abstract>
			<keywords>Part-of-speech tagging, rule learning, active learning, text-mining</keywords>
		</article>
		<article id="taln-2005-court-002" session="Posters">
			<auteurs>
				<auteur>
					<nom>Lucie Barque</nom>
					<email>lbarque@linguist.jussieu.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Alain Polguère</nom>
					<email>alain.polguere@umontreal.ca</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Lattice - Université Paris 7</affiliation>
				<affiliation affiliationId="2">OLST - Université de Montréal</affiliation>
			</affiliations>
			<titre>Application du métalangage de la BDéf au traitement formel de la polysémie</titre>
			<type>court</type>
			<pages>391-396</pages>
			<resume>Cet article a pour objet le métalangage définitionnel de la base de données lexicale BDéf, plus précisément l’utilisation de ce métalangage dans la modélisation des structures polysémiques du français. La Bdéf encode sous forme de définitions lexicographiques les sens lexicaux d’un sous-ensemble représentatif du lexique du français parmi lequel on compte environ 500 unités polysémiques appartenant aux principales parties du discours. L’article comprend deux sections. La première présente le métalangage de la BDéf et le situe par rapport aux différents types de définitions lexicales, qu’elles soient ou non formelles, qu’elles visent ou non l’informatisation. La seconde section présente une application de la BDéf qui vise à terme à rendre compte de la polysémie régulière du français. On y présente, à partir d’un cas spécifique, la notion de patron de polysémie.</resume>
			<mots_cles>Base de données lexicale, métalangage définitionnel, Lexicologie Explicative et Combinatoire, polysémie</mots_cles>
			<title></title>
			<abstract>We present the defining metalanguage of the BDéf lexical database ; more specifically, we focus on how this metalanguage can be used to model relations of polysemy in French. The BDéf contains lexical definitions for a representative subset of the French lexicon : around 500 polysemic words belonging to all major parts of speech. This paper contains two sections. Firstly, the BDéf metalanguage is introduced and positioned relative to different types of existing lexical definitions : formal vs. non formal definitions, definitions that are tailored or not for implementation. Secondly, the paper shows how the BDéf approach is used in a research whose goal is the modeling of the regular polysemy of the French language. The notion of pattern of polysemy is introduced using a specific example.</abstract>
			<keywords>lexical database, definitionnal metalanguage, Explanatory Combinatorial Lexicology, polysemy</keywords>
		</article>
		<article id="taln-2005-court-003" session="Posters">
			<auteurs>
				<auteur>
					<nom>Narjès Boufaden</nom>
					<email>boufaden@iro.umontreal.ca</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Guy Lapalme</nom>
					<email>lapalme@iro.umontreal.ca</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Laboratoire RALI-Université de Montréal</affiliation>
			</affiliations>
			<titre>Apprentissage de relations prédicat-argument pour l'extraction d'information à partir de textes conversationnels</titre>
			<type>court</type>
			<pages>397-402</pages>
			<resume>Nous présentons les résultats de notre approche d’apprentissage de relations prédicat-argument dans le but de générer des patrons d’extraction pour des textes conversationnels. Notre approche s’effectue en trois étapes incluant la segmentation linguistique des textes pour définir des unités linguistiques à l’instar de la phrase pour les textes bien formés tels que les dépêches journalistiques. Cette étape prend en considération la dimension discursive importante dans ces types de textes. La deuxième étape effectue la résolution des anaphores pronominales en position de sujet. Cela tient compte d’une particularité importante des textes conversationnels : la pronominalisation du thème. Nous montrons que la résolution d’un sous ensemble d’anaphores pronominales améliore l’apprentissage des patrons d’extraction. La troisième utilise des modèles de Markov pour modéliser les séquences de classes de mots et leurs rôles pour un ensemble de relations données. Notre approche expérimentée sur des transcriptions de conversations téléphoniques dans le domaine de la recherche et sauvetage identifie les patrons d’extraction avec un F-score moyen de 73,75 %.</resume>
			<mots_cles>Apprentissage de relations prédicat-argument, extraction d’information</mots_cles>
			<title></title>
			<abstract>We present the results of our approach for the learning of patterns for information extraction from conversational texts. Our three step approach is based on a linguistic segmentation stage that defines units suitable for the pattern learning process. Anaphora resolution helps to identify more relevant relations hidden by the pronominalization of the topic. This stage precedes the pattern learning stage, which is based on Markov models that include wild card states designed to handle edited words and null transitions to handle omissions. We tested our approach on manually transcribed telephone conversations in the domain of maritime search and rescue, and succeeded in identifying extraction patterns with an F-score of 73.75 %.</abstract>
			<keywords>Learning predicat-argument relations, information extraction</keywords>
		</article>
		<article id="taln-2005-court-004" session="Posters">
			<auteurs>
				<auteur>
					<nom>Pierre Boullier</nom>
					<email>pierre.boullier@inria.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Benoît Sagot</nom>
					<email>benoit.sagot@inria.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Lionel Clément</nom>
					<email>lionel.clement@lefff.net</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">INRIA - Projet ATOLL</affiliation>
			</affiliations>
			<titre>Un analyseur LFG efficace pour le français : SXLFG</titre>
			<type>court</type>
			<pages>403-408</pages>
			<resume>Dans cet article, nous proposons un nouvel analyseur syntaxique, qui repose sur une variante du modèle Lexical-Functional Grammars (Grammaires Lexicales Fonctionnelles) ou LFG. Cet analyseur LFG accepte en entrée un treillis de mots et calcule ses structures fonctionnelles sur une forêt partagée. Nous présentons également les différentes techniques de rattrapage d’erreurs que nous avons mises en oeuvre. Puis nous évaluons cet analyseur sur une grammaire à large couverture du français dans le cadre d’une utilisation à grande échelle sur corpus variés. Nous montrons que cet analyseur est à la fois efficace et robuste.</resume>
			<mots_cles>syntaxe, analyseur, LFG, désambiguïsation, forêt partagée</mots_cles>
			<title></title>
			<abstract>In this paper, we introduce a new parser based on the Lexical-Functional Grammars formalism (LFG). This LFG parser accepts as input word lattices and computes functional structures on a shared forest. We also present various error recovery techniques we have implemented. Afterwards, we evaluate this parser on a large-coverage grammar for French in the framework of a large-scale use on various corpus. We show that our parser is both efficient and robust.</abstract>
			<keywords>syntax, parser, LFG, disambiguation, shared forest</keywords>
		</article>
		<article id="taln-2005-court-005" session="Posters">
			<auteurs>
				<auteur>
					<nom>Julien Bourdaillet</nom>
					<email>Julien.Bourdaillet@lip6.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Jean-Gabriel Ganascia</nom>
					<email>Jean-Gabriel.Ganascia@lip6.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIP6</affiliation>
			</affiliations>
			<titre>Etiquetage morpho-syntaxique du français à base d'apprentissage supervisé</titre>
			<type>court</type>
			<pages>409-414</pages>
			<resume>Nous présentons un étiqueteur morpho-syntaxique du français. Celui-ci utilise l’apprentissage supervisé à travers un modèle de Markov caché. Le modèle de langage est appris à partir d’un corpus étiqueté. Nous décrivons son fonctionnement et la méthode d’apprentissage. L’étiqueteur atteint un score de précision de 89 % avec un jeu d’étiquettes très riche. Nous présentons ensuite des résultats détaillés pour chaque classe grammaticale et étudions en particulier la reconnaissance des homographes.</resume>
			<mots_cles>étiquetage morpho-syntaxique, apprentissage supervisé, modèle de Markov caché, évaluation, homographes</mots_cles>
			<title></title>
			<abstract>A french part-of-speech tagger is described. It is based on supervised learning: hidden Markov model and trained using a corpus of tagged text. We describe the way the model is learnt. A 89 % precision rate is achieved with a rich tagset. Detailed results are presented for each grammatical class. We specially pay attention to homographs recognition.</abstract>
			<keywords>part-of-speech tagging, supervised learning, hidden Markov model, evaluation, homographs</keywords>
		</article>
		<article id="taln-2005-court-006" session="Posters">
			<auteurs>
				<auteur>
					<nom>Boxing Chen</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Meriam Haddara</nom>
					<email>Meriam.Haddara@laposte.net</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Olivier Kraif</nom>
					<email>Olivier.Kraif@u-grenoble3.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Grégoire Moreau de Montcheuil</nom>
					<email>Gregoire.Moreau-de-Montcheuil@univ-avignon.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Marc El-Bèze</nom>
					<email>Marc.El-Beze@univ-avignon.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université Stendhal Grenoble 3 - Laboratoire LIDILEM</affiliation>
				<affiliation affiliationId="2">Université d'Avignon et des Pays de Vaucluse - LIA (Laboratoire informatique d'Avignon)</affiliation>
			</affiliations>
			<titre>Contextes multilingues alignés pour la désambiguïsation sémantique : une étude expérimentale</titre>
			<type>court</type>
			<pages>415-420</pages>
			<resume>Cet article s'intéresse a la désambiguïsation sémantique d'unités lexicales alignées a travers un corpus multilingue. Nous appliquons une méthode automatique non supervisée basée sur la comparaison de réseaux sémantiques, et nous dégageons un critère permettant de déterminer a priori si 2 unités alignées ont une chance de se désambiguïser mutuellement. Enfin, nous développons une méthode fondée sur un apprentissage a partir de contextes bilingues. En appliquant ce critère afin de déterminer pour quelles unités l'information traductionnelle doit être prise en compte, nous obtenons une amélioration des résultats.</resume>
			<mots_cles>Désambiguïsation sémantique, alignement multilingue, lexique sémantique</mots_cles>
			<title></title>
			<abstract>This paper addresses the sense disambiguation of aligned words through a multilingual corpus. We apply an unsupervised disambiguation method using inter word net comparison. We study a criterion that allows to identify the cases for which disambiguation can take advantage of alignment. Finally, we implement a method based on a training stage using both monolingual and bilingual context, and we apply the previous criterion in order to select between monolingual or bilingual clues, showing some improvement of the results.</abstract>
			<keywords>Word sense disambiguation, multilingual aligning, semantic lexicon</keywords>
		</article>
		<article id="taln-2005-court-007" session="Posters">
			<auteurs>
				<auteur>
					<nom>Javier Couto</nom>
					<email>Javier.Couto@paris4.sorbonne.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Lita Ludnquist</nom>
					<email>ll.first@cbs.dk</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Jean-Luc Minel</nom>
					<email>Jean-Luc.Minel@paris4.sorbonne.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LaLICC, CNRS, Université Paris-Sorbonne</affiliation>
				<affiliation affiliationId="2">Institut FIRST</affiliation>
			</affiliations>
			<titre>Naviguer dans les textes pour apprendre</titre>
			<type>court</type>
			<pages>421-426</pages>
			<resume>Dans cet article nous présentons un langage de navigation textuelle et son implantation dans la plate-forme Navitexte. Nous décrivons une application de ces principes de navigation dans un cadre d’apprentissage de la bonne formation des textes, destinée à des dans un cadre d’apprentissage de la bonne formation des textes, destinée à des étudiants apprenant le français langue étrangère.</resume>
			<mots_cles>Navigation textuelle, apprentissage en lingusitique textuelle</mots_cles>
			<title></title>
			<abstract>In this article we present a declarative language, which models ways of visualising and navigating in texts, together with its implementation in a workstation, NaviTexte. First, we show how the language can be used to build an application in text linguistics. This application aims to teach foreign language students to identify different coherence creating units in a text and to navigate between them. Then we detail the declarative language.</abstract>
			<keywords>Textual navigation, teaching texts and text linguistics</keywords>
		</article>
		<article id="taln-2005-court-008" session="Posters">
			<auteurs>
				<auteur>
					<nom>Benoit Crabbé</nom>
					<email>crabbe@loria.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LORIA - Université Nancy2</affiliation>
			</affiliations>
			<titre>Projection et monotonie dans un langage de représentation lexico-grammatical</titre>
			<type>court</type>
			<pages>433-438</pages>
			<resume>Cet article apporte une méthode de développement grammatical pour la réalisation de grammaires d’arbres adjoints (TAG) de taille importante augmentées d’une dimension sémantique. La méthode que nous présentons s’exprime dans un langage informatique de représentation grammatical qui est déclaratif et monotone. Pour arriver au résultat, nous montrons comment tirer parti de la théorie de la projection dans le langage de représentation que nous utilisons. Par conséquent cet article justifie l’utilisation d’un langage monotone pour la représentation lexico-grammaticale.</resume>
			<mots_cles>Syntaxe, Lexique, Liage, Interface syntaxe sémantique, TAG</mots_cles>
			<title></title>
			<abstract>This papers provides a methodology for the grammatical development of large sized tree adjoining grammars (TAG) augmented with a semantic dimension. The provided methodology is expressed in a monotnic and declarative language designed for the compact representation of grammatical descriptions. To achieve the result, we show how to express a linking theory in the language used. Therefore this paper justifies the use of a monotonic language for lexico-grammatical representation.</abstract>
			<keywords>Syntax, Lexicon, Linking, Syntax Semantics interface, TAG</keywords>
		</article>
		<article id="taln-2005-court-009" session="Posters">
			<auteurs>
				<auteur>
					<nom>Florence Duclaye</nom>
					<email>florence.duclaye@francetelecom.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Franck Panaget</nom>
					<email>franck.panaget@francetelecom.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">France Telecom R&amp;D</affiliation>
			</affiliations>
			<titre>Dialogue automatique et personnalité : méthodologie pour l'incarnation de traits humains</titre>
			<type>court</type>
			<pages>433-438</pages>
			<resume>Cet article introduit une méthodologie d’intégration de la personnalité dans un système de dialogue automatique, en vue de l’incarnation de personnages virtuels. Notion complexe non encore épuisée dans la littérature, la personnalité d’un individu peut s’illustrer de multiples manières possibles. Notre objectif consiste à présenter une méthode générique de prise en compte de la personnalité dans un système de dialogue par modélisation et exploitation des connaissances relatives à la personnalité de l’individu à incarner. Cet article présente les avantages et inconvénients de cette méthode en l’illustrant au travers de la stylistique des énoncés générés par le système.</resume> 
			<mots_cles>dialogue incarné, personnages virtuels, personnalité, génération automatique</mots_cles>
			<title></title>
			<abstract>This article introduces a methodology to integrate personality into a dialogue system. This work constitutes a step towards the embodiment of virtual characters. The personality of an individual, which is a complex and non-exhausted concept in literature, can show through multiple possible ways. Our purpose is to describe a generic methodology of personality integration into a dialogue system based on the modelling and use of knowledge relative to the personality of a given individual. This article describes the advantages and drawbacks of the proposed methodology by illustrating it through the linguistic style of the generated messages.</abstract>
			<keywords>embodied dialogue, virtual characters, personality, natural language generation</keywords>
		</article>
		<article id="taln-2005-court-010" session="Posters">
			<auteurs>
				<auteur>
					<nom>Olivier Galibert</nom>
					<email>galibert@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Gabriel Illouz</nom>
					<email>gabrieli@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Sophie Rosset</nom>
					<email>rosset@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIMSI - CNRS</affiliation>
			</affiliations>
			<titre>Ritel : un système de dialogue homme-machine à domaine ouvert</titre>
			<type>court</type>
			<pages>439-444</pages>
			<resume>L’objectif du projet RITEL est de réaliser un système de dialogue homme-machine permettant à un utilisateur de poser oralement des questions, et de dialoguer avec un système de recherche d’information généraliste (par exemple, chercher sur l’Internet “Qui est le Président du Sénat ?”) et d’en étudier les potentialités. Actuellement, la plateforme RITEL permet de collecter des corpus de dialogue homme-machine. Les utilisateurs peuvent parfois obtenir une réponse, de type factuel (Q : qui est le président de la France ; R : Jacques Chirac.). Cet article présente brièvement la plateforme développée, le corpus collecté ainsi que les questions que soulèvent un tel système et quelques unes des premières solutions envisagées.</resume>
			<mots_cles>dialogue homme machine, recherche d’information précise, corpus</mots_cles>
			<title></title>
			<abstract>The project RITEL aims at integrating a spoken language dialog system and an open-domain question answering system to allow a human to ask a general question (f.i. “Who is currently presiding the Senate?”) and refine his research interactively. As this point in time the RITEL platform is used to collect a new human-computer dialog corpus. The user can sometimes recieve factual answers (Q : who is the president of France ; R : Jacques Chirac). This paper briefly presents the current system, the collected corpus, the problems encountered by such a system and our first answers to these problems.</abstract>
			<keywords>human machine dialog, question answering, information retrieval, corpus</keywords>
		</article>
		<article id="taln-2005-court-011" session="Posters">
			<auteurs>
				<auteur>
					<nom>Ahmed Haddad</nom>
					<email>ahmed.haddad@ensi.rnu.tn</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Mounir Zrigui</nom>
					<email>mounir.zrigui@fsm.rnu.tn</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Mohamed Ben Ahmed</nom>
					<email>mohamed.benahmed@riadi.rnu.tn</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">RIADI</affiliation>
			</affiliations>
			<titre>Un système de génération automatique de dictionnaires linguistiques de l'arabe</titre>
			<type>court</type>
			<pages>445-450</pages>
			<resume>L’objectif de cet article est la présentation d’un système de génération automatique de dictionnaires électroniques de la langue arabe classique, développé au sein du laboratoire RIADI (unité de Monastir). Ce système entre dans le cadre du projet “oreillodule”: un système embarqué de synthèse, traduction et reconnaissance de la parole arabe. Dans cet article, nous présenterons, les différentes étapes de réalisation, et notamment la génération automatique de ces dictionnaires se basant sur une théorie originale : les Conditions de Structures Morphématiques (CSM), et les matrices lexicales.</resume>
			<mots_cles>dictionnaires électroniques, Conditions de Structures Morphématiques, matrices lexicales, Restrictions combinatoires, Restrictions séquentielles</mots_cles>
			<title></title>
			<abstract>the objective of this article is the presentation of a system of automatic generation of electronic dictionaries of the classic Arabian language, developed within RIADI laboratory (unit of Monastir). This system enters in the setting of project "oreillodule": an embedded system of synthesis, translation and recognition of the Arabian word. In this article, we will present the different stages of realization, and notably the automatic generation of these dictionaries basing on an original theory: Conditions of Morphemic Structure (CSM), and the lexical matrixes.</abstract>
			<keywords>electronic dictionaries, Conditions of Morphemic Structures , lexical matrix, Combinative circumscriptions, Sequential circumscriptions</keywords>
		</article>
		<article id="taln-2005-court-012" session="Posters">
			<auteurs>
				<auteur>
					<nom>Lamia Hadrich Belguith</nom>
					<email>l.belguith@fsegs.rnu.tn</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Leila Baccour</nom>
					<email>leila_freind@techemail.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Mourad Ghassan</nom>
					<email>Ghassan.Mourad@paris4.sorbonne.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Laboratoire LARIS, FESGS, Université de Sfax</affiliation>
				<affiliation affiliationId="2">Equipe LaLICC - Paris Sorbonne</affiliation>
			</affiliations>
			<titre>Segmentation de textes arabes basée sur l’analyse contextuelle des signes de ponctuations et de certaines particules</titre>
			<type>court</type>
			<pages>451-456</pages>
			<resume>Nous proposons dans cet article une approche de segmentation de textes arabes non voyellés basée sur une analyse contextuelle des signes de ponctuations et de certaines particules, tels que les conjonctions de coordination. Nous présentons ensuite notre système STAr, un segmenteur de textes arabes basé sur l'approche proposée. STAr accepte en entrée un texte arabe en format txt et génère en sortie un texte segmenté en paragraphes et en phrases.</resume>
			<mots_cles>Segmenteur de textes arabes, segmentation en phrases, exploration contextuelle, expressions rationnelles</mots_cles>
			<title></title>
			<abstract>We propose in this paper an approach to segment non-vowelled Arabic texts. Our approach is based on a contextual analysis of the punctuation marks and a list of particles, such as the coordination conjunctions. Then, we present our system STAr, a tokenizer based on the proposed approach. The STAr input is an Arabic text (in .txt format) and its output is a segmented text into paragraphs and sentences.</abstract>
			<keywords>Arabic text tokenizer, sentence tokenization, contextual exploration, regular expressions</keywords>
		</article>
		<article id="taln-2005-court-013" session="Posters">
			<auteurs>
				<auteur>
					<nom>Laura Kallmeyer</nom>
					<email>laura.kallmeyer@linguist.jussieu.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université Paris 7, Laboratoire Lattice</affiliation>
			</affiliations>
			<titre></titre>
			<type>court</type>
			<pages>457-462</pages>
			<resume>Il a été montré que les Grammaires d’Arbres Adjoints Ensemblistes (Multicomponent Tree Adjoining Grammars, MCTAG) sont très utiles pour des applications TAL. Pourtant, la définition des MCTAG est problématique parce qu’elle fait référence au procès de dérivation même : une contrainte de simultanéité est imposée concernant la façon dont on ajoute les membres d’un même ensemble d’arbres. En regardant uniquement le résultat d’une dérivation, c’est-à-dire l’arbre dérivé et l’arbre de dérivation, cette simultanéité n’est plus visible. Par conséquent pour vérifier la contrainte de simultanéité, il faut toujours considérer l’ordre concret des pas de la dérivation. Afin d’éviter cela, nous proposons une caractérisation alternative de MCTAG qui permet une abstraction de l’ordre de dérivation : Les arbres générés par la grammaire sont caractérisés par les propriétés de leurs arbres de dérivation.</resume>
			<mots_cles>Grammaires d’Arbres Adjoints, MCTAG, formalismes grammaticaux</mots_cles>
			<title>A Descriptive Characterization of Multicomponent Tree Adjoining Grammars</title>
			<abstract>Multicomponent Tree Adjoining Grammars (MCTAG) is a formalism that has been shown to be useful for many natural language applications. The definition of MCTAG however is problematic since it refers to the process of the derivation itself: a simultaneity constraint must be respected concerning the way the members of the elementary tree sets are added. Looking only at the result of a derivation (i.e., the derived tree and the derivation tree), this simultaneity is no longer visible and therefore cannot be checked. I.e., this way of characterizing MCTAG does not allow to abstract away from the concrete order of derivation. Therefore, in this paper, we propose an alternative definition of MCTAG that characterizes the trees in the tree language of an MCTAG via the properties of the derivation trees the MCTAG licences.</abstract>
			<keywords>Tree Adjoining Grammars, MCTAG, grammar formalisms</keywords>
		</article>
		<article id="taln-2005-court-014" session="Posters">
			<auteurs>
				<auteur>
					<nom>Philippe Langlais</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Thomas Leplus</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Simona Gandrabur</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Guy Lapalme</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">RALI, Université de Montréal</affiliation>
			</affiliations>
			<titre>Approches en corpus pour la traduction : le cas MÉTÉO</titre>
			<type>court</type>
			<pages>463-468</pages>
			<resume>La traduction automatique (TA) attire depuis plusieurs années l’intérêt d’un nombre grandissant de chercheurs. De nombreuses approches sont proposées et plusieurs campagnes d’évaluation rythment les avancées faites. La tâche de traduction à laquelle les participants de ces campagnes se prêtent consiste presque invariablement à traduire des articles journalistiques d’une langue étrangère vers l’anglais; tâche qui peut sembler artificielle. Dans cette étude, nous nous intéressons à savoir ce que différentes approches basées sur les corpus peuvent faire sur une tâche réelle. Nous avons reconstruit à cet effet l’un des plus grands succès de la TA: le système MÉTÉO. Nous montrons qu’une combinaison de mémoire de traduction et d’approches statistiques permet d’obtenir des résultats comparables à celles du système MÉTÉO, tout en offrant un cycle de développement plus court et de plus grandes possibilités d’ajustements.</resume>
			<mots_cles>Mémoire de traduction, traduction probabiliste, alignements multiples, réordonnancement à postériori</mots_cles>
			<title></title>
			<abstract>Machine Translation (MT) is the focus of extensive scientific investigations driven by regular evaluation campaigns, but which are mostly oriented towards a somewhat artificial task: translating news articles into English. In this paper, we investigate how well current MT approaches deal with a real-world task. We have rationally reconstructed one of the only MT systems in daily production use: the METEO system. We show how a combination of a sentence-based memory approach, a phrase-based statistical engine and a neural-network rescorer can give results comparable to those of the current system while offering a faster development cycle and better customization possibilities.</abstract>
			<keywords>Memory-based translation, statistical translation, multiple alignment, rescoring</keywords>
		</article>
		<article id="taln-2005-court-015" session="Posters">
			<auteurs>
				<auteur>
					<nom>Aurélien Max</nom>
					<email>aurelien.max@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIMSI-CNRS</affiliation>
			</affiliations>
			<titre>Simplification interactive pour la production de textes adaptés aux personnes souffrant de troubles de la compréhension</titre>
			<type>court</type>
			<pages>469-474</pages>
			<resume>Cet article traite du problème de la compréhensibilité des textes et en particulier du besoin de simplifier la complexité syntaxique des phrases pour des lecteurs souffrant de troubles de la compréhension. Nous présentons une approche à base de règles de simplification développées manuellement et son intégration dans un traitement de texte. Cette intégration permet la validation interactive de simplifications candidates produites par le système, et lie la tâche de création de texte simplifié à celle de rédaction.</resume>
			<mots_cles>troubles du langage, simplification syntaxique, règles de réécriture, validation interactive, traitements de texte</mots_cles>
			<title></title>
			<abstract>This paper addresses the issue of text readability and in particular the need for simplifying the syntactic complexity of sentences for language-impaired readers. The proposed approach uses handcrafted simplification rules and has been integrated into a word processor. This allows interactive validation of candidate simplified sentences produced by the system, and integrates the task of creating simplified texts into that of authoring.</abstract>
			<keywords>language disorders, syntactic simplification, rewriting rules, interactive validation, word processors</keywords>
		</article>
		<article id="taln-2005-court-016" session="Posters">
			<auteurs>
				<auteur>
					<nom>Aurélie Névéol</nom>
					<email>aneveol@insa-rouen.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Alexandrina Rogozan</nom>
					<email>arogozan@insa-rouen.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Stéfan Darmoni</nom>
					<email>stefan.darmoni@univ-rouen.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Laboratoire PSI</affiliation>
				<affiliation affiliationId="2">INSA de Rouen</affiliation>
			</affiliations>
			<titre>Indexation automatique de ressources de santé à l'aide de paires de descripteurs MeSH</titre>
			<type>court</type>
			<pages>475-480</pages>
			<resume>Depuis quelques années, médecins et documentalistes doivent faire face à une demande croissante dans le domaine du codage médico-économique et de l'indexation des diverses sources d'information disponibles dans le domaine de la santé. Il est donc nécessaire de développer des outils d’indexation automatique qui réduisent les délais d’indexation et facilitent l'accès aux ressources médicales. Nous proposons deux méthodes d’indexation automatique de ressources de santé à l’aide de paires de descripteurs MeSH. La combinaison de ces deux méthodes permet d'optimiser les résulats en exploitant la complémentarité des approches. Les performances obtenues sont équivalentes à celles des outils de la littérature pour une indexation à l’aide de descripteurs seuls.</resume>
			<mots_cles>Indexation Automatique, Terminologie Médicale, Vocabulaire Contrôlé</mots_cles>
			<title></title>
			<abstract>The increasing number of health documents available in electronic form, and the demand on both practitioners and librarians to encode these documents with controlled vocabulary information calls for automatic tools and methods to help them perform this task efficiently. In this article, we are presenting and comparing two methods for the automatic indexing of heath resources with pairs of MeSH descriptors. A combination of both methods achieves better results by exploiting the complementarity of the approaches. This performance matches the tools described in the litterature for single term indexing.</abstract> <keywords>Automatic Indexing, Medical Terminology, Controlled Vocabulary</keywords>
		</article>
		<article id="taln-2005-court-017" session="Posters">
			<auteurs>
				<auteur>
					<nom>Olivier Pietquin</nom>
					<email>olivier.pietquin@supelec.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Supélec</affiliation>
			</affiliations>
			<titre>Réseau bayesien pour un modèle d'utilisateur et un module de compréhension pour l'optimisation des systèmes de dialogues</titre>
			<type>court</type>
			<pages>481-486</pages>
			<resume>Dans cet article, un environnement modulaire pour la simulation automatique de dialogues homme-machine est proposé. Cet environnement comprend notamment un modèle d’utilisateur consistant dirigé par le but et un module de simulation de compréhension de parole. Un réseau bayésien est à la base de ces deux modèles et selon les paramètres utilisés, il peut générer un comportement d’utilisateur cohérent ou servir de classificateur de concepts. L’environnement a été utilisé dans le contexte de l’optimisation de stratégies de dialogue sur une tâche simple de remplissage de formulaire et les résultats montrent qu’il est alors possible d’identifier certains dialogues problématiques du point de vue de la compréhension.</resume>
			<mots_cles>Systèmes de dialogue, simulation de dialogues, modèle d’utilisateur, optimisation</mots_cles>
			<title></title>
			<abstract>In this paper we present a modular environment for simulating human-machine dialogues by computer means. This environment includes a consistent goal-directed user model and a natural language understanding system model. Both models rely on a special Bayesian network used with different parameters in such a way that it can generate a consistent user behaviour according to a goal and the history of the interaction, and been used as a concept classifier. This environment was tested in the framework of optimal strategy learning for the simple form-filling task. The results show that the environment allows pointing out problematic dialogues that may occur because of misunderstanding between the user and the system.</abstract>
			<keywords>Spoken dialog systems, dialog simulation, user modeling, optimization</keywords>
		</article>
		<article id="taln-2005-court-018" session="Posters">
			<auteurs>
				<auteur>
					<nom>Roger Rainero</nom>
					<email>roger.r@prolexis.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">DIAGONAL SA</affiliation>
			</affiliations>
			<titre>Correction Automatique en temps réél, contraintes, méthodes et voies de recherche</titre>
			<type>court</type>
			<pages>487-492</pages>
			<resume>Cet article expose un cas concret d’utilisation d’une grammaire de contraintes. Le produit qui les applique a été commercialisé en 2003 pour corriger automatiquement et en temps réel les fautes d’accord présentes dans les sous-titres des retransmissions en direct des débats du Sénat du Canada. Avant la mise en place du système, le taux moyen de fautes était de l’ordre de 7 pour 100 mots. Depuis la mise en service, le taux d’erreurs a chuté à 1,7 %. Nous expliquons dans ce qui suit les principaux atouts des grammaires de contraintes dans le cas particulier des traitements temps réel, et plus généralement pour toutes les applications qui nécessitent une analyse au fur et à mesure du discours (c.-à-d. sans attendre la fin des phrases).</resume>
			<mots_cles>Correction automatique, temps réel, analyse syntaxique, grammaire de contraintes</mots_cles>
			<title></title>
			<abstract>This article sets out a concrete use case of a grammar of constraints. The product which applies them was commercialised in 2003 to automatically correct in real time the errors of agreement present in the sub-titles of live televised debates from the Senate of Canada. Before the introduction of this system, the average rate of mistakes was in the order of 7 per 100 words. With the introduction of this system, the rate of errors has fallen to 1.7%. In the following section, we explain the main advantages of a grammar of constraints in the specific case of real-time processing, and more generally for all applications which require an analysis during the speech (that is, without waiting until the end of sentences).</abstract>
			<keywords>Automatic correction, real-time, syntactic analysis, grammar of constraints</keywords>
		</article>
		<article id="taln-2005-court-019" session="Posters">
			<auteurs>
				<auteur>
					<nom>Benoît Sagot</nom>
					<email>benoit.sagot@inria.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">INRIA - Projet ATOLL</affiliation>
			</affiliations>
			<titre>Les Méta-RCG: description et mise en oeuvre</titre>
			<type>court</type>
			<pages>493-498</pages>
			<resume>Nous présentons dans cet article un nouveau formalisme linguistique qui repose sur les Grammaires à Concaténation d’Intervalles (RCG), appelé Méta-RCG. Nous exposons tout d’abord pourquoi la non-linéarité permet une représentation adéquate des phénomènes linguistiques, et en particulier de l’interaction entre les différents niveaux de description. Puis nous présentons les Méta-RCG et les concepts linguistiques supplémentaires qu’elles mettent en oeuvre, tout en restant convertibles en RCG classiques. Nous montrons que les analyses classiques (constituants, dépendances, topologie, sémantique prédicat-arguments) peuvent être obtenues par projection partielle d’une analyse Méta-RCG complète. Enfin, nous décrivons la grammaire du français que nous développons dans ce nouveau formalisme et l’analyseur efficace qui en découle. Nous illustrons alors la notion de projection partielle sur un exemple.</resume>
			<mots_cles>Analyse syntaxique, interface syntaxe-sémantique, grammaires non-linéaires, Grammaires à Concaténation d’Intervalles (RCG)</mots_cles>
			<title></title>
			<abstract>In this paper, we present a novel linguistic formalism based on Range Concatenation Grammars (RCG), called Meta-RCG. We first expose why non-linearity allows a satisfying representation of linguistic phenomena, and in particular of the interaction between the different levels of description. Then we introduce Meta-RCGs and the extra linguistic concepts they manipulate, while remaining compilable into classical RCGs. Moreover, we show that classical analyses (consituency, dependency, topology, predicate-arguments semantics) can be obtained by partial projection of a full Meta-RCG parse. Finally, we describe the grammar for French we develop in this new formalism and the associated efficient parser. We illustrate the notion of partial projection with an example.</abstract>
			<keywords>Parsing, syntax-semantics interface, non-linear grammars, Range Concatenation Grammars (RCG)</keywords>
		</article>
		<article id="taln-2005-court-020" session="Posters">
			<auteurs>
				<auteur>
					<nom>Izabel Christine Seara</nom>
					<email>izabels@linse.ufsc.br</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Fernando Pacheco</nom>
					<email>fernando@linse.ufsc.br</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Rui Seara jr.</nom>
					<email>ruijr@linse.ufsc.br</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Sandra Kafka</nom>
					<email>kafka@linse.ufsc.br</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Rui Seara</nom>
					<email>seara@linse.ufsc.br</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Simone Klein</nom>
					<email>klein@linse.ufsc.br</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LINSE - Circuits and Signal Processing Laboratory / Federal University of Santa Catarina</affiliation>
			</affiliations>
			<titre></titre>
			<type>court</type>
			<pages>499-504</pages>
			<resume>Dans cet article, nous avons examiné la relation entre pause et ponctuation (virgule, point et virgule, deux-points). Toutes ces pauses sont internes aux phrases. À l'aide de l'analyse de plusieurs milliers de pauses dans un corpus de presque 17 heures d'enregistrement réalisé par une locutrice professionnelle native du portugais brésilien, nous avons vérifié une proportion importante des pauses hors ponctuations (61,3%). Les données renforcent aussi la présence des structures topique/commentaire dans la lecture à haute voix. Les résultats des durées de pause correspondantes aux ponctuations sont consistants avec les données présentées dans les grammaires.</resume>
			<mots_cles>pauses, ponctuations, lecture à haute voix</mots_cles>
			<title>Pauses and punctuation marks in Brazilian Portuguese read speech</title>
			<abstract>In this paper we assess pause effects corresponding to comma, semicolon, colon and the ones that are not related to any punctuation marks, all of them within sentences. Thus, through the analysis of a corpus of approximately 17 hours of recording, carried out by a female professional speaker (native) of the Brazilian Portuguese language, we observe a large proportion of pauses without punctuation (61.3%). Besides, our data reinforce the presence of topic-comment structures in reading. The results here presented with respect to pause and punctuation are consistent with several studies about this theme.</abstract>
			<keywords>pauses, punctuation marks, read speech</keywords>
		</article>
		<article id="taln-2005-court-021" session="Posters">
			<auteurs>
				<auteur>
					<nom>Laurianne Sitbon</nom>
					<email>laurianne.sitbon@univ-avignon.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Patrice Bellot</nom>
					<email>patrice.bellot@univ-avignon.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Laboratoire d'Informatique d'Avignon</affiliation>
			</affiliations>
			<titre>Segmentation thématique par chaînes lexicales pondérées</titre>
			<type>court</type>
			<pages>505-510</pages>
			<resume>Cet article propose une méthode innovante et efficace pour segmenter un texte en parties thématiquement cohérentes, en utilisant des chaînes lexicales pondérées. Les chaînes lexicales sont construites en fonction de hiatus variables, ou bien sans hiatus, ou encore pondérées en fonction de la densité des occurrences du terme dans la chaîne. D’autre part, nous avons constaté que la prise en compte du repérage d’entités nommées dans la chaîne de traitement, du moins sans résolution des anaphores, n’améliore pas significativement les performances. Enfin, la qualité de la segmentation proposée est stable sur différentes thématiques, ce qui montre une indépendance par rapport au type de document.</resume>
			<mots_cles>segmentation thématique, chaînes lexicales, entités nommées</mots_cles>
			<title></title>
			<abstract>This paper presents an innovative and efficient topic segmentation method based on weighted lexical chains. This method comes from a study of different existing tools, and experiments where we considered the influence of a term at each precise place in the text. We build lexical chains with different kinds of hiatus (varying, none or density weighted). We demonstrate good results on a manually built french news corpus. We show that using named entities does not improve results. Finally, we show that our method tends to be domain-independent because results are similar on various topics.</abstract>
			<keywords>topic segmentation, lexical chains, named entities</keywords>
		</article>
		<article id="taln-2005-court-022" session="Posters">
			<auteurs>
				<auteur>
					<nom>Tristan Vanrullen</nom>
					<email>tristan@lpl.univ-aix.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Philippe Blache</nom>
					<email>pb@lpl.univ-aix.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Cristel Portes</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Stéphane Rauzy</nom>
					<email></email>
					<affiliationId></affiliationId>
				</auteur>
				<auteur>
					<nom>Jean-François Maeyhieux</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LPL - CNRS - Aix-en-Provence</affiliation>
			</affiliations>
			<titre>Une plateforme pour l'acquisition, la maintenance et la validation de ressources lexicales</titre>
			<type>court</type>
			<pages>511-516</pages>
			<resume>Nous présentons une plateforme de développement de lexique offrant une base lexicale accompagnée d’un certain nombre d’outils de maintenance et d’utilisation. Cette base, qui comporte aujourd’hui 440.000 formes du Français contemporain, est destinée à être diffusée et remise à jour régulièrement. Nous exposons d’abord les outils et les techniques employées pour sa constitution et son enrichissement, notamment la technique de calcul des fréquences lexicales par catégorie morphosyntaxique. Nous décrivons ensuite différentes approches pour constituer un sous-lexique de taille réduite, dont la particularité est de couvrir plus de 90% de l’usage. Un tel lexique noyau offre en outre la possibilité d’être réellement complété manuellement avec des informations sémantiques, de valence, pragmatiques etc.</resume>
			<mots_cles>dictionnaire, lexique, lexique noyau</mots_cles>
			<title></title>
			<abstract>We present a lexical development platform which comprises a lexical database of 440.000 lemmatized words of contemporary French, plus a set of maintenance tools. The lexical database is intended to be distributed and updated regularly. We present in this paper tools and techniques applied for the lexicon constitution and its enrichment, in particular the computation of lexical frequencies by morphosyntactic category. Then we describe various approaches to build an under-lexicon of reduced size, whose characteristic is to cover more than 90% of the use. Such a kernel lexicon makes it moreover possible to be really enriched by hand with semantic, valence, pragmatic information, etc.</abstract>
			<keywords>dictionary, lexicon, kernel lexicon</keywords>
		</article>
		<article id="taln-2005-court-023" session="Posters">
			<auteurs>
				<auteur>
					<nom>Antoine Widlöcher</nom>
					<email>awidloch@info.unicaen.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Frédérik Bilhaut</nom>
					<email>fbilhaut@info.unicaen.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université de Caen, Laboratoire GREYC</affiliation>
			</affiliations>
			<titre>La plate-forme LinguaStream : un outil d'exploration linguistique sur corpus</titre>
			<type>court</type>
			<pages>517-522</pages>
			<resume>À travers la présentation de la plate-forme LinguaStream, nous présentons certains principes méthodologiques et différents modèles d’analyse pouvant permettre l’articulation de traitements sur corpus. Nous envisageons en particulier les besoins nés de perspectives émergentes en TAL telles que l’analyse du discours.</resume>
			<mots_cles>Linguistique de corpus, TAL, plate-forme logicielle</mots_cles>
			<title></title>
			<abstract>By presenting the LinguaStream platform, we introduce different methodological principles and analysis models, which make it possible to articulate corpus processing tasks. More especially, we consider emerging approaches in NLP, such as discourse analysis.</abstract>
			<keywords>Corpus linguistics, NLP, software framework</keywords>
		</article>
	</articles>
</conference>