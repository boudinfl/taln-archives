TALN 2005, Dourdan, 6-10 juin 2005

Etiquetage morpho-syntaxique du frangais a base
d’apprentissage supervisé

Julien Bourdaillet, J ean-Gabriel Ganascia
LIP6 - Univesité Paris VI
8 rue du capitaine Scott - 75015 Paris
{J ulien.Bourdaillet, J ean-Gabriel.Ganascia} @lip6.fr

Mots-clefs I étiquetage morpho-syntaxique, apprentissage supervisé, modele de Markov
caché, évaluation, homographes

Keywords: part-of-speech tagging, supervised learning, hidden Markov model, eval-
uation, homographs

Résumé Nous présentons un étiqueteur morpho-syntaxique du francais. Celui-ci utilise
l’ apprentissage supervisé a travers un modele de Markov caché. Le modele de langage est appris
a partir d’un corpus étiqueté. Nous décrivons son fonctionnement et la méthode d’apprentissage.
L’ étiqueteur atteint un score de précision de 89 % avec un jeu d’étiquettes tres riche. Nous
présentons ensuite des résultats détaillés pour chaque classe grammaticale et étudions en parti-
culier la reconnaissance des homographes.

Abstract A french part-of-speech tagger is described. It is based on supervised learning:
hidden Markov model and trained using a corpus of tagged text. We describe the way the model
is learnt. A 89 % precision rate is achieved with a rich tagset. Detailed results are presented for
each grammatical class. We specially pay attention to homographs recognition.

1 Introduction

L’ étiquetage morpho-syntaxique consiste a assigner la bonne classe grammaticale, déﬁnie sui-
vant un certain niveau de granularité, a chaque mot d’un texte en entrée. Nous présentons ici
un étiqueteur (ou tagger) du francais basé sur un modele d’apprentissage supervisé. Celui-ci est
une adaptation du tagger de l’analyseur syntaxique RASP de la langue anglaise. Cet étiqueteur
apprend un modele de langage a partir d’un corpus préalablement étiqueté.

Plusieurs approches ont été présentées pour l’étiquetage morpho-syntaxique du francais. Le
Brill Tagger (Brill, 92) apprend des regles a partir d’un corpus étiqueté et a été adapté pour le
francais avec WinBrill. (Giguet, 97) et (Chanod, 95) se basent sur des propriétés de la langue
comme les mots noyaux ou des méthodes a base de contraintes. (Chanod, 95) présente un
autre étiqueteur a base d’apprentissage non-supervisé, qui apprend un modele de langage a par-
tir d’un corpus non-étiqueté via une variante de l’algorithme Estimation-Maximisation (EM).

Julien Bourdaillet, J ean-Gabriel Ganascia

(Chanod, 95) présente les limites de ce modele qui est fortement dépendant des conditions
initiales et peut rester bloqué sur un optimum local. (Stein, 95) présente une adapatation au
francais du TreeTagger. Celui-ci est basé sur un modele de Markov caché (Hidden Markov
Model ou HMM) modélisé par un arbre de décisions. Nous nous situons dans la lignée de ces
deux demiers travaux et utilisons également un HMM. Toutefois puisqu’il a été prouvé dans
(Elworthy, 94) que l’apprentissage d’un modele a partir d’un corpus manuellement étiqueté
produit de meilleurs résultats que la procédure d’apprentissage d’EM et que nous disposions
d’un tel corpus, nous avons choisi cette alternative.

2 Présentation de l’étiqueteur

Nous utilisons l’étiqueteur morpho-syntaxique de l’anglais du projet RASP (Briscoe, 02). Cet
étiqueteur, détaillé dans (Elworthy, 94), est basé sur HMM du premier ordre (bigramme). Celui-
ci est représenté par un lexique des formes ﬂéchies qui associe a chaque mot ses tags potentiels
et par une matrice de transition entre états. Nous l’avons adapté au traitement de la langue
francaise.

Nous avons utilisé le corpus GRACE qui est annoté morpho-syntaxiquement. I1 comporte env-
iron 800.000 mots et est constitué pour moitié d’articles du Monde et pour moitié d’oeuvres
et d’essais littéraires. Ce corpus est étiqueté avec un jeu de 312 étiquettes qui correspon-
dent a un étiquetage tres ﬁn. Les mots sont tout d’abord regroupés en 12 grandes classes tres
générales : adjectif, conjonction, déterminant, mot-phrase, nom, pronom, adverbe, préposition,
verbe, résidu, ponctuation et extra-lexical. Ces classes sont ensuite afﬁnées, comme par exem-
ple conjonction en conjonction de coordination et conjonction de subordination, verbe en verbe
auxiliaire et verbe principal. On aboutit ainsi a un jeu de 36 étiquettes. Enﬁn, ont été adjoints
a ces classes grammaticales des traits proprement morphologiques, tels que le genre, le nom-
bre, la personne, le mode ou encore le temps qui donnent le jeu de 312 étiquettes. Nous avons
ajouté, pour des facilités d’implémentation et sans dénaturer la cohérence de l’ensemble, huit
étiquettes composées (qui existaient dans le corpus sous la forme d’une composition de deux
étiquettes) et sommes ainsi arrivés a un jeu de 320 étiquettes.

Nous avons choisi d’effectuer l’apprentissage sur le corpus GRACE avec le jeu de 320 éti-
quettes. En effet, l’intérét de ce jeu est que les classes tres ﬁnes apportent beaucoup d’informa-
tions sur les mots et permettent de se passer d’analyseur morphologique en vue d’une étape
ultérieure d’analyse en constituants.

Ainsi la procédure d’apprentissage permet d’ apprendre un HMM basé sur 320 états. Nous avons
gardé la majeure partie du corpus comme données d’apprentissage et utilisé le reste comme
données de test, soit environ 26.000 mots. L’entrainement du modele a permis d’obtenir un
dictionnaire d’environ 44.000 formes ﬂéchies.

Nous avons modiﬁé l’ algorithme d’ étiquetage proprement dit pour que celui-ci prenne en compte
les locutions. Pour cela, nous avons appliqué l’heuristique du motif le plus long. Au moment de
lire le texte en entrée, l’étiqueteur va chercher, grace au lexique, si la combinaison du mot sui-
vant au mot courant forme une locution. Si tel est le cas, on applique ce principe itérativement
avec le mot suivant jusqu’a ce que l’application ne soit plus possible, auquel cas, on garde la
demiere locution trouvée. L’ étiquetage de la phrase par Viterbi est ensuite effectué avec celle-ci.

Etiquetage morpho-syntaxique du francais a base d’apprentissage supervise

3 Evaluation

Pour evaluer notre travail, nous utilisons la precision (proportion d’etiquetages corrects parmi
les etiquetages stricts) et la decision (proportion d’etiquetages stricts parmi l’ensemble de tous
les etiquetages). Dans un premier temps, nous avons developpe un script Perl charge de cette
evaluation, qui sera appele par la suite EVAL. Dans un second temps nous avons reutilise la
boite a outils d’evaluation des analyseurs du projet ELSE1. Notons que EVAL comporte 150
lignes de code alors que l’evaluateur ELSE en comporte plusieurs milliers, meme si ce dernier
se veut plus ambitieux et traite, par exemple, divers formats de ﬁchiers en entree.

3.1 Résultats

Lors de tous nos tests, notre analyseur atteint un score de decision de 100% et ceci pour
deux raisons. Tout d’abord, il ne renvoie qu’une seule etiquette par mot, ce qui ne genere
pas d’etiquetage ambigu. Et ensuite, notre jeu d’etiquettes est identique a celui de GRACE.
I1 n’y a donc pas de problemes de projection d’un jeu dans l’autre (phenomene qui entraine
des regroupements de plusieurs etiquettes en une seule ou inversement) lors de l’utilisation de
1’ evaluateur ELSE.

Nous avons effectue des tests avec trois niveaux de granularite de l’etiquetage. Le premier
niveau correspond au jeu complet des 320 etiquettes. Le second est ce meme jeu mais sans les
traits morphologiques, ce qui donne 36 etiquettes. Le dernier niveau est l’etiquetage le plus
general en 12 grandes classes grammaticales. Dans le tableau 1 nous presentons les scores
de precision dans differents cas. La premiere colonne correspond a l’etiquetage du corpus de
test produit par notre analyseur morpho-syntaxique avec EVAL. La deuxieme fait reference au
meme etiquetage mais avec l’evaluateur ELSE. Et la derniere presente l’evaluation par ELSE
de ce meme corpus de test mais etiquete par l’analyseur Cordial. En effet, ce dernier cas nous
a semble interessant comme element de comparaison puisque Cordial semble actuellement etre
le meilleur analyseur du francais.

A partir de ces chiffres nous pouvons

tirer plusieurs constations. Tout d’ab- I I EVAL | ELSE l ELSE / Cordial l
ord remarquons qu’entre l’evaluation 320 tags 87.65 % 89.07 % 94.44 %

de notre analyseur avec EVAL et celle 36 tags 92.24 % 93.52 % 94.63 %
avec ELSE, on a pour les trois niveaux 12 tags 94.39 % 95.69 % 97.52 %
environ 1.5 point d’ecart. Ceci vient

de la difference des methodes de seg- Table 13 SCOICS (16 précision

mentation utilisees. Avec ELSE, l’alignement entre la sortie de l’etiqueteur et le corpus cor-
rectement etiquete n’est pas tres bon car ELSE utilise un segmenteur generique et non un seg-
menteur adapte a l’analyseur comme EVAL. Ainsi plus de tokens ne sont pas correctement
realignes avec ELSE, comme certains mots composes ou contenant une apostrophe. Ceux-ci ne
sont donc pas soumis a evaluation, ce qui tend a ameliorer mecaniquement la precision. D’autre
part, cela souligne l’importance de la question de la segmentation et la difﬁculte d’y apporter
une reponse qui soit valide a travers plusieurs formalismes. Avec EVAL, les tokens incorrecte-
ment realignes sont des mots contenant des tirets, des mots composes et des expressions non
presentes dans le dictionnaire. On en compte environ 0.8 % du nombre de tokens contenus dans
le corpus detest.

lhttp://www.li1nsi.fr/'I‘I.P/ELSE

Julien Bourdaillet, J ean-Gabriel Ganascia

Ensuite, avec le jeu d’étiquettes complet (320 tags) et l’évaluateur ELSE, notre analyseur est
moins performant que Cordial. Cependant si l’on compare ce score avec ceux présentés dans
(Adda, 99), nous nous situons dans la moyenne des analyseurs évalués qui obtiennent de 82
a 96 % de précision. De plus, notons que lors de cette campagne les meilleurs résultats ont
été obtenus non par des étiqueteurs mais par des analyseurs syntaxiques complets. Ceux-ci
ayant un avantage certain sur les premiers pour désambigu'1'ser les cas les plus difﬁciles. En
effet, leur résolution peut étre reportée au niveau syntaxique, ce qui améliore de quelques (mais
précieux) points la précision. Au vu de nos résultats et du fait que nous nous limitons au niveau
de l’étiquetage, notre approche est intéressante vis-a-vis des autres.

Enﬁn, nous avons effectué l’évaluation avec des jeux d’étiquettes plus concis. En effet, ceux-ci
se rapprochant plus des jeux utilisés par les analyseurs de l’anglais, la comparaison est rendue
possible. Notre score se situe également dans la moyenne de ceux présentés dans la littéra-
ture. Notons que dans (Briscoe, 02), donc pour RASP sur l’anglais, les auteurs atteignent
une précision de 97 %. Nous nous situons en dessous, ce qui laisse entrevoir des possibilités
d’amélioration. Toutefois un tel score semble difﬁcilement atteignable par un étiqueteur seul
pour le francais. En effet, du fait de sa plus grande richesse morphologique, le francais est plus
dur a étiqueter que l’anglais. Ceci est conﬁrmé, au vu de la littérature, par le fait que globale-
ment les scores des étiqueteurs de l’anglais sont plus élevés, de quelques (et toujours précieux)
points, que ceux du francais.

3.2 Evaluation par classes

Nous cherchons dans cette partie a évaluer les points forts et les points faibles de notre étiqueteur
de facon plus précise. Dans un premier temps, nous présentons dans cette section les résultats
pour chaque classe grammaticale. Dans un second temps, nous présentons dans la section
suivante le taux de reconnaissance des homographes.

Dans le tableau 2, nous présentons pour chaque classe grammaticale, le nombre d’occurrences
dans le corpus de test, le pourcentage d’étiquetage correct et les trois types d’erreurs les plus
fréquentes par ordre décroissant.

| | Occurrences | % Correct | Erreurl | Erreur 2 | Erreur 3 |
Nom 6506 95.2 % Adj /2 % V / 0.8 % D / 0.7 %
Verbe 3184 96 % Adj / 2.4 % N/1.1 % Prep/0.3 %
Verbe auxiliaire 669 78.5 % Vp/ 21 % N/ 0.1 % Adv/ 0.1 %
Verbe principal 2515 92.8 % Adj / 3 % Val 2.3 % N/ 1.3 %
Adjectif 1773 85.4 % V / 6.6 % N/5.1 % Adv/1.2 %
Pronom 1456 89.3 % C/4.5 % D / 2 % Advl 1.7 %
Déterminant 3162 94.0 % Prepl 1.8 % Pron/ 1.7 % N/ 1.6 %
Adverbe 1170 94.9 % C/2.3 % Prep/ 1 % N/0.8 %
Conjonction 856 97.9 % Adv/ 1% Prep/ 1 % Pron / 0.6 %
Préposition 3863 81.8 % D/ 16.4 % Pron / 0.6 % N / 0.3 %
Ponctuation 3424 98.8 % Adv / 0.8 % Pron / 0.3 % N / 0.1 %

Nous considérons que les classes présentant un taux supérieur a 94 % sont bien reconnues et
que les efforts d’amélioration de l’étiqueteur devraient plutot se porter sur la reconnaissance

Table 2: Scores et types d’erreurs par classes

Etiquetage morpho-syntaxique du francais a base d’apprentissage supervisé

des autre classes. En étudiant en particulier les deux composantes de la classe Verbe (auxiliaire
et principal), on constate que les auxiliaires sont les plus mal reconnus. Néanmoins le tagger
hésite essentiellement entre Verbe auxiliaire et Verbe principal, ce qui est satisfaisant pour une
étape ultérieure d’analyse syntaxique. La ponctuation présente un score élevé mais toutefois
insufﬁsant, ce qui est dﬁ aux points de suspension mal étiquetés.

Les prépositions semblent particulierement mal reconnues. En analysant les erreurs, on constate
que ce sont les mots du type “de la, de l’, des” qui posent la plupart des problemes, c’est-a-dire
les mots homographes qui sont soit des articles partitifs, soit des prépositions contractées. Cette
difﬁculté est due a la forte ambiguité entre les deux cas, élément caractéristique de la grammaire
francaise. De plus, les adjectifs et les pronoms sont un point faible de notre étiqueteur.

3.3 Evaluation des homographes

En nous inspirant de (Vergne, 99) qui présente différents types d’homographes du francais, nous
avons cherché a afﬁner ces résultats. Le tableau 3 présente ci-dessous les taux de reconnais-
sance de différents types d’homographes. Dans la premiere colonne (H1) sont présentés les
homographes determinant/pronom (le, 1’, la, les); en H2, les homographes préposition contrac-
tée/article partitif (de, d’, du, des); en H3, les homographes conjonction/pronom relatif (que,
qu’); en H4, les homographes auxiliaire/nom (est, étre, avoir); en H5, les homographes ad-
verbe/nom (bien, mal, moins, plus, pas, point...); en H6, les homographes nom/adjectif et en
H7, les homographes nom/verbe principal. La premiere ligne présente le nombre d’occurrences
dans le corpus de test de la classe majoritaire (en effet, pour les cas d’homographes, une classe
est largement majoritaire par rapport a l’autre, de l’ordre de 70 a 95 % des cas); la seconde,
le nombre d’occurrences de la classe Ininoritaire; la troisieme, le taux de reconnaissance de la
classe majoritaire, et la derniere le taux de reconnaissance de la classe Ininoritaire.

H1 H2 H3 H4 H5 H6 H7
Occ. Maj D: 1767 Prep: 1601 C: 200 Aux: 267 Adv: 223 Adj: 290 N: 141
Occ. min Pro: 59 Part: 132 Pro: 47 N: 6 N: 27 N: 171 VP: 96

% OK Maj 97.7 % 82.5 % 98 % 99.2 % 97.3 % 91.7 % 92.9 %
% OK Inin 69.5 % 62.1 % 17 % 85.7 % 85.2 % 87.7 % 83.3 %

Table 3: Reconnaissance des homographes

Au vu de ces résultats, nous constatons que les classes majoritaires sont bien reconnues (sauf
en H2, ou on retrouve la difﬁculté précédente). Pour les classes Ininoritaires, les résultats sont
réellement encourageants pour H4, H5, H6 et H7, or ce sont ces cas qui sont les plus intéressants
pour l’étape ultérieure d’analyse syntaxique. En effet, nous pensons que discriminer un nom
d’un verbe (H7) apporte plus d’inforInation qu’une préposition d’un partitif (H2), meme si cela
semble difﬁcilement quantiﬁable. Nous avons déja discuté de la difﬁculté du cas H2. Pour
ce qui est de H1 et H3, dans les deux cas, la classe Ininoritaire est le pronom. Nous pensons
atteindre ici certaines limites de l’étiquetage pour ce qui est de la reconnaissance des pronoms.
En effet, celle-ci serait probablement plus pertinente au niveau de l’analyse en constituants .

Julien Bourdaillet, J ean-Gabriel Ganascia

4 Conclusion

A travers ce travail, nous avons cherché a développer un étiqueteur morpho-syntaxique du
frangais se fondant sur une méthode d’apprentissage supervisé. Pour ce faire, en nous basant sur
un tagger de l’anglais, nous avons adapté la procédure d’apprentissage aux exigences du traite-
ment automatique du frangais. Nous obtenons un score de précision de 89 %, score inférieur a
ceux des meilleurs étiqueteurs mais comme, d’une part ceux-ci sont des analyseurs syntaxiques
complets et d’autre part nous n’avons effectué aucune optimisation sur notre étiqueteur, ce ré-
sultat est intéressant. Apres avoir analysé en détail les erreurs d’étiquetage, il ressort que les
points faibles se situent au niveau des adjectifs, pronoms et prépositions et en particulier sur
ceux qui sont homographes avec une autre classe. Toutefois nous avons identiﬁé certains cas
d’homographes comme étant les plus intéressants et ceux-ci s’averent bien reconnus.

Dans une perspective de poursuite de ce travail, nous avons effectué des tests prospectifs avec
le dictionnaire appris a partir de tout le corpus (mais avec les mémes transitions), la précision
s’est améliorée de 1.5 a 2 points pour les trois niveaux. Cela laisse a penser que l’augmentation
de la taille du dictionnaire présenterait des gains signiﬁcatifs de précision et, plus généralement,
l’acquisition d’un modele de langage a partir d’un corpus plus consequent. D’autre part, nous
avons réutilisé tel quel le guesser qui est optimisé pour l’anglais. Puisqu’il repose sur des regles
statistiques, il fonctionne également ici mais mériterait d’étre étudié plus en détail et adapté.

Remerciements

Nous tenons a remercier John Carroll pour son concours et Emmanuel Giguet pour ses com-
mentaires sur ce travail.

Références

Adda G., Mariani J ., Paroubek P., Rajman M., Lecomte J . (1999), Métrique et premiers résultats de
1’évaluation GRACE des étiqueteurs morphosyntaxiques pour le frangais, Actes de la Sixie‘me Con-
ference sur le Traitement Automatique des Langues Naturelles.

Brill E. (1992), A simple rule-based part of speech tagger, Actes de Third Conference ofApplied Natural
Language Processing.

Briscoe T., Carroll J .(2002), Robust accurate statistical annotation of general text. Actes de Third Inter-
national Conference on Language Resources and Evaluation. p. 1499-1504.

Chanod J-P., Tapanainen P. (1995), Tagging French - comparing a statistical and a constraint-based
method, Actes de Seventh Conference of the European Chapter of the ACL.

Elworthy D. (1994), Does Baum-Welch re-estimation help taggers ?, Actes de Fourth ACL Conference
on Applied NLP.

Giguet E., Vergne J . (1997), From part of speech tagging to memory-based deep syntactic analysis, Actes
de Fifth Iternational Workshop on Parsing Technologies.

Stein A., Schmid H. (1995), Etiquetage morphologique de textes frangais avec un arbre de décisions,
Revue TA.L, Vol. 36.

Vergne J . (1999), Habilitation a diriger des recherches, p. 36.

