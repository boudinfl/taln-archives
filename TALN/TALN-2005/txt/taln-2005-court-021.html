<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
"http://www.w3.org/TR/html4/loose.dtd">
<html><head><title>Segmentation th&#233;matique par cha&#238;nes lexicales pond&#233;r&#233;es</title>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
</head>
<body>
<div style="page-break-before:always; page-break-after:always"><div><p>TALN 2005, Dourdan, 6&#8211;10 juin 2005
</p>
<p>Segmentation th&#233;matique par cha&#238;nes lexicales pond&#233;r&#233;es
</p>
<p>Laurianne Sitbon, Patrice Bellot
Laboratoire d&#8217;Informatique d&#8217;Avignon - Universit&#233; d&#8217;Avignon
</p>
<p>339, chemin des Meinajaries - Agroparc BP 1228
84911 AVIGNON Cedex 9 - FRANCE
</p>
<p>T&#233;l : +33 (0) 4 90 84 35 09
{laurianne.sitbon, patrice.bellot}@univ-avignon.fr
</p>
<p>Mots-clefs : segmentation th&#233;matique, cha&#238;nes lexicales, entit&#233;s nomm&#233;es
</p>
<p>Keywords: topic segmentation, lexical chains, named entities
</p>
<p>R&#233;sum&#233; Cet article propose une m&#233;thode innovante et efficace pour segmenter un texte
en parties th&#233;matiquement coh&#233;rentes, en utilisant des cha&#238;nes lexicales pond&#233;r&#233;es. Les cha&#238;nes
lexicales sont construites en fonction de hiatus variables, ou bien sans hiatus, ou encore pond&#233;r&#233;es
en fonction de la densit&#233; des occurrences du terme dans la cha&#238;ne. D&#8217;autre part, nous avons
constat&#233; que la prise en compte du rep&#233;rage d&#8217;entit&#233;s nomm&#233;es dans la cha&#238;ne de traitement, du
moins sans r&#233;solution des anaphores, n&#8217;am&#233;liore pas significativement les performances. Enfin,
la qualit&#233; de la segmentation propos&#233;e est stable sur diff&#233;rentes th&#233;matiques, ce qui montre une
ind&#233;pendance par rapport au type de document.
</p>
<p>Abstract This paper presents an innovative and efficient topic segmentation method based
on weighted lexical chains. This method comes from a study of different existing tools, and ex-
periments where we considered the influence of a term at each precise place in the text. We build
lexical chains with different kinds of hiatus (varying, none or density weighted). We demon-
strate good results on a manually built french news corpus. We show that using named entities
does not improve results. Finally, we show that our method tends to be domain-independent
because results are similar on various topics.
</p>
<p>1 Introduction
</p>
<p>La segmentation th&#233;matique intervient dans diff&#233;rents domaines en organisation de l&#8217;information,
tels que d&#233;terminer les limites entre des d&#233;p&#234;ches dans un flux d&#8217;informations (broadcast news,
TDT (Topic Detection and Tracking)), ou cr&#233;er un r&#233;sum&#233; automatique de textes pour lequel la
segmentation sert &#224; isoler les th&#233;matiques et les parties les plus repr&#233;sentatives (McDonald &amp;</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Laurianne Sitbon et Patrice Bellot
</p>
<p>Chen, 2002). Enfin, du point de vue d&#8217;un utilisateur, la segmentation th&#233;matique a &#233;galement
des avantages en terme de facilit&#233;s de lecture.
</p>
<p>Beaucoup de moyens ont &#233;t&#233; imagin&#233;s pour segmenter un texte en th&#232;mes coh&#233;rents. La prin-
cipale diff&#233;rence entre ces m&#233;thodes tient au fait qu&#8217;elles sont ou non supervis&#233;es. Parmi les
m&#233;thodes supervis&#233;es on trouve par exemple PLSA (Brants et al., 2002) qui apprend des prob-
abilit&#233;s d&#8217;appartenance des termes &#224; des classes s&#233;mantiques. D&#8217;autres m&#233;thodes se basent sur
un apprentissage comme (Amini et al., 2000) qui s&#8217;appuient sur des mod&#232;les de Markov cach&#233;s,
ou bien (Caillet et al., 2004) qui propose une classification des termes de m&#234;me que (Chuang &amp;
Chien, 2004) et (Mekhaldi et al., 2004). Nous avons d&#233;velopp&#233; une m&#233;thode non supervis&#233;e, &#224;
base de matrice de similarit&#233;s et de cha&#238;nes lexicales du type de celles utilis&#233;es par (Utiyama &amp;
Isahara, 2001) ou (Galley et al., 2003). Ce choix est li&#233; &#224; leurs capacit&#233;s naturelles d&#8217;adaptation
&#224; de nouvelles th&#233;matiques, et &#224; leur relative ind&#233;pendance vis &#224; vis de la langue des textes.
</p>
<p>2 M&#233;thodologie
</p>
<p>Avant de concevoir une nouvelle m&#233;thode, nous avons fait une &#233;valuation de l&#8217;&#233;tat de l&#8217;art pour
des textes en fran&#231;ais (Sitbon &amp; Bellot, 2004). Cette &#233;tude pr&#233;liminaire a notamment permis
d&#8217;analyser les diff&#233;rentes mesures d&#8217;&#233;valuation de la segmentation, et de cr&#233;er un corpus de
test en fran&#231;ais. Les outils que nous avons &#233;tudi&#233;s ont &#233;t&#233; compar&#233;es pour l&#8217;anglais par (Choi,
2000). Nos exp&#233;riences ont montr&#233; que dans les conditions o&#249; le texte &#224; segmenter est une suite
d&#8217;articles bien distincts, et o&#249; la qualit&#233; des outils est &#233;valu&#233;e automatiquement en fonction
de la distance entre les fronti&#232;res trouv&#233;es et celles &#224; trouver, l&#8217;outil le plus efficace est C99
(Choi, 2000), qui ordonne localement les similarit&#233;s entre chaque paire de phrases, puis fait
des regroupements par maximum de densit&#233;. Nous avons montr&#233; que le type de document que
l&#8217;on segmente, son th&#232;me, la taille et la variation de taille des segments &#224; rep&#233;rer, sont autant de
caract&#233;ristiques influen&#231;ant le travail des segmenteurs.
</p>
<p>2.1 Construction et pond&#233;ration des cha&#238;nes lexicales
</p>
<p>Une cha&#238;ne lexicale relie des termes de mani&#232;re lin&#233;aire dans un texte. Les m&#233;thodes actuelles
de segmentation les utilisent pour relier les occurrences d&#8217;un m&#234;me terme (ou lemme) qui sont
&quot;proches&quot;. Une cha&#238;ne est rompue lorsque le nombre de termes qui s&#233;parent deux occurrences
d&#233;passe une valeur fix&#233;e appel&#233;e hiatus. On peut alors recenser pour chaque phrase les cha&#238;nes
actives.
</p>
<p>Les applications des cha&#238;nes lexicales utilisent actuellement des hiatus d&#233;finis de mani&#232;re em-
pirique, et la notion d&#8217;activit&#233; d&#8217;une cha&#238;ne est binaire (elle est active ou non active). Notre
premier objectif est d&#8217;&#233;liminer le caract&#232;re empirique du hiatus, afin que notre outil puisse
s&#8217;adapter &#224; n&#8217;importe quel type de texte sans intervention de l&#8217;utilisateur. Pour cela on peut</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Segmentation th&#233;matique par cha&#238;nes lexicales pond&#233;r&#233;es
</p>
<p>imaginer tout simplement la suppression du hiatus, ce qui revient &#224; relier toutes les r&#233;p&#233;titions
de termes. Nous avons &#233;galement envisag&#233; l&#8217;utilisation de hiatus locaux : le hiatus moyen est
calcul&#233; pour chaque lemme. Ainsi si un mot est fortement r&#233;p&#233;t&#233; &#224; deux endroits distincts du
texte, il y aura automatiquement la cr&#233;ation de deux chaines. De plus, s&#8217;il est r&#233;p&#233;t&#233; trois fois en
d&#233;but de texte, puis une fois &#224; la fin, il n&#8217;y aura qu&#8217;une seule cha&#238;ne comprenant les occurrences
de d&#233;but de texte.
</p>
<p>Ces techniques cr&#233;ent un d&#233;s&#233;quilibre dans la significativit&#233; de l&#8217;activit&#233; des cha&#238;nes en jeu dans
le calcul des fronti&#232;res. Il faut alors pond&#233;rer les cha&#238;nes, en fonction de leur compacit&#233; (ratio
entre leur taille et le nombre d&#8217;occurrences). (Galley et al., 2003) propose une pond&#233;ration des
cha&#238;nes en fonction de la compacit&#233; et de la fr&#233;quence du terme consid&#233;r&#233;, et obtient de bons
r&#233;sultats, malgr&#233; un hiatus d&#233;termin&#233; de mani&#232;re empirique. La cat&#233;gorie des lemmes a &#233;t&#233;
int&#233;gr&#233;e &#224; cette pond&#233;ration. Le poids d&#8217;une cha&#238;ne associ&#233;e &#224; un terme m est d&#233;fini par :
</p>
<p>score(Chaine,m) = max(Chaine, cat(m)&#215; freq(Chaine,m)&#215; log( Ltexte
Lchaine
</p>
<p>) (1)
</p>
<p>o&#249; freq(Chaine, m) est le nombre d&#8217;occurrences du terme m dans la cha&#238;ne, Ltexte la longueur
du texte, Lchaine la longueur de la cha&#238;ne (on est alors ind&#233;pendant de la taille des textes &#224;
segmenter), et max(Chaine,cat(m)) le poids de la forme grammaticale la plus importante parmi
les occurrences du terme dans la cha&#238;ne.
</p>
<p>Puis on calcule les similarit&#233;s &#224; chaque fin de phrase, qui est une rupture th&#233;matique potentielle.
La similarit&#233; est calcul&#233;e avec :
</p>
<p>sim(A,B) =
</p>
<p>&#8721;
m score(A,m)&#215; score(B,m)&#8730;&#8721;
</p>
<p>m score(A,m)&#215;
&#8721;
</p>
<p>m score(B,m)
(2)
</p>
<p>o&#249; A et B sont les ensembles de vecteurs repr&#233;sentant les poids des cha&#238;nes lexicales actives
dans les n phrases avant et apr&#232;s (nous avons choisi n=2), score(X, m) &#233;tant le poids maximal
du terme m dans l&#8217;ensemble des vecteurs X.
</p>
<p>Les fronti&#232;res retenues sont alors celles pour lesquelles la similarit&#233; est en dessous d&#8217;un seuil
d&#233;termin&#233; par simlimit = &#181;+ &#963;2 o&#249; &#181; et &#963; sont la moyenne et la variance de toutes les similarit&#233;s
calcul&#233;es (Galley et al., 2003).
</p>
<p>2.2 Evaluation
</p>
<p>Afin de constituer un corpus de grande taille ais&#233;ment, on compose un corpus de test &#224; partir
d&#8217;articles journalistiques de th&#232;me globalement &#233;loign&#233;s car class&#233;s manuellement dans dif-
f&#233;rentes rubriques. Le corpus de test est compos&#233; de 4 s&#233;ries de 100 documents, chaque s&#233;rie</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Laurianne Sitbon et Patrice Bellot
</p>
<p>correspondant &#224; une taille moyenne pr&#233;-d&#233;finie des segments. Chaque document est compos&#233; de
10 segments qui sont autant d&#8217;extraits d&#8217;articles du journal Le Monde, choisis al&#233;atoirement. Ce
journal proposant des th&#233;matiques tr&#232;s vari&#233;es, on suppose alors les segments th&#233;matiquement
coh&#233;rents et diff&#233;rents.
</p>
<p>Pour &#233;valuer l&#8217;efficacit&#233; de nos nouveaux algorithmes, nous avons utilis&#233; la mesure Window
Diff propos&#233;e par (Pevzner &amp; Hearst, 2002), pr&#233;sent&#233;e et analys&#233;e dans (Sitbon &amp; Bellot, 2004).
Nous avons test&#233; les diff&#233;rentes approches pour le calcul des cha&#238;nes lexicales. L&#8217;utilisation
d&#8217;un hiatus fixe de 11 est le param&#232;tre pr&#233;conis&#233; par (Galley et al., 2003) avec l&#8217;outil LCseg.
Les r&#233;sultats montr&#233;s dans cet article et rappel&#233;s ici dans le tableau 1 affichent de meilleures
performances pour cette approche que C99 sur le Brown corpus, ainsi que sur le corpus TDT.
</p>
<p>Brown Corpus TDT Corpus
LCseg 0,1137 0,0909
C99 0,1457 0,1272
</p>
<p>Table 1: comparaison de LCseg et C99 pour un
nombre de segments inconnu, selon la mesure
WindowDiff
</p>
<p>Taille LCseg hiatus 120 hiatus locaux
9-11 0,3272 0,3187 0,3454
3-11 0,3837 0,3685 0,4016
3-5 0,4344 0,4309 0,4204
</p>
<p>Table 2: Comparaison de LCseg et de notre
m&#233;thode pour des segments de diff&#233;rentes tailles
(en nombre de phrases)
</p>
<p>Nous avons donc d&#233;cid&#233; de comparer notre approche &#224; celle de (Galley et al., 2003). Les
r&#233;sultats sont pr&#233;sent&#233;s dans le tableau 2. Etant donn&#233; que les textes ont tous moins de 110
phrases (le maximum &#233;tant 10 segments de 11 phrases chacun), le hiatus 120 correspond &#224;
une absence de hiatus. Pour ces tests, les lemmes n&#8217;ont pas &#233;t&#233; pond&#233;r&#233;s en fonction de leur
cat&#233;gorie.
</p>
<p>Pour les segments de grande taille (9-11), ou de tailles variables (3-11) , la meilleure m&#233;thode
est finalement celle qui ne coupe pas les cha&#238;nes lexicales (hiatus 120). Pour des segments de
petite taille, on observe une tr&#232;s faible am&#233;lioration lorsqu&#8217;on utilise des hiatus diff&#233;rents pour
chaque lemme (hiatus locaux).
</p>
<p>3 Exploitation d&#8217;une d&#233;tection d&#8217;entit&#233;s nomm&#233;es
</p>
<p>Si nous avons pris jusqu&#8217;ici les termes et leur fonction syntaxique comme seuls crit&#232;res, nous
pensons par ailleurs que la variation des noms propres est un indice int&#233;ressant.
</p>
<p>On appelle entit&#233; nomm&#233;e dans un texte tout ce qui fait r&#233;f&#233;rence &#224; un identifiant unique (Chin-
chor, 1997). Il peut s&#8217;agit un mot ou d&#8217;un groupe nominal. Nous avons utilis&#233; trois types
d&#8217;entit&#233;s nomm&#233;es, rep&#233;r&#233;es &#224; partir d&#8217;un lexique ferm&#233; : listes de noms de personne, noms
de lieux et noms d&#8217;organisations. Etant un identifiant unique, une entit&#233; nomm&#233;e a tendance &#224;
moins se r&#233;p&#233;ter d&#8217;un th&#232;me &#224; l&#8217;autre. De plus, il est moins malvenu en fran&#231;ais de les r&#233;p&#233;ter
que les noms communs ou adjectifs, m&#234;me si le probl&#232;me des anaphores reste &#224; r&#233;soudre comme</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Segmentation th&#233;matique par cha&#238;nes lexicales pond&#233;r&#233;es
</p>
<p>on le verra.
</p>
<p>Nous avons conduit plusieurs types d&#8217;exp&#233;riences (table 3), en fonction de poids plus ou moins
&#233;lev&#233;s attribu&#233;s aux entit&#233;s, ou en n&#8217;utilisant que les entit&#233;s. Dans un premier temps nous
avons multipli&#233; le poids des cha&#238;nes contenant des entit&#233;s nomm&#233;es, par deux (ENx2) puis
par dix (ENx10). Ensuite nous avons test&#233; la m&#233;thode en n&#8217;utilisant que les cha&#238;nes lexicales
correspondant &#224; des entit&#233;s nomm&#233;es (EN seules).
</p>
<p>Segments 9-11 Segments 3-5
M&#233;thode hiatus 120 hiatus locaux M&#233;thode hiatus 120 hiatus locaux
classique 0,3187 0,3454 classique 0,4309 0,4204
</p>
<p>ENx2 0,3211 0,3536 ENx2 0,4291 0,4128
ENx10 0,3521 0,3888 ENx10 0,4315 0,4202
</p>
<p>EN seules 0,4235 0,4975 EN seules 0,4228 0,4291
</p>
<p>Table 3: Evaluation sur un corpus journalistique avec diff&#233;rentes pond&#233;rations des
entit&#233;s nomm&#233;es pour 2 tailles moyennes des segments
</p>
<p>Les r&#233;sultats pr&#233;sent&#233;s dans la table 3 montrent que l&#8217;am&#233;lioration avec une utilisation des
entit&#233;s nomm&#233;es est tr&#232;s peu significative d&#8217;une part, et qu&#8217;il faut doser cet usage d&#8217;autre part.
En effet on observe une perte de qualit&#233; lorsqu&#8217;on leur accorde un poids trop important ou
lorsque on ne consid&#232;re qu&#8217;elles.
</p>
<p>Nous avons ensuite refait les m&#234;mes tests sur un corpus journalistique compos&#233; uniquement
d&#8217;articles traitant de sport, et sur lequel les m&#233;thodes test&#233;es dans (Sitbon &amp; Bellot, 2004)
donnaient les r&#233;sultats les plus m&#233;diocres.
</p>
<p>Segments 9-11 Segments 3-5
M&#233;thode hiatus 120 hiatus locaux M&#233;thode hiatus 120 hiatus locaux
classique 0,3202 0,3463 classique 0,4375 0,4179
</p>
<p>ENx2 0,3255 0,3321 ENx2 0,4359 0,4183
ENx10 0,3561 0,3695 ENx10 0,4393 0,4265
</p>
<p>EN seules 0,3976 0,4621 EN seules 0,4430 0,4634
</p>
<p>Table 4: Evaluation sur un corpus journalistique avec diff&#233;rentes pond&#233;rations des
entit&#233;s nomm&#233;es pour 2 tailles moyennes des segments
</p>
<p>Les r&#233;sultats pr&#233;sent&#233;s sur la table 4 montrent que l&#8217;on n&#8217;obtient pas l&#8217;am&#233;lioration attendue par
l&#8217;utilisation des entit&#233;s nomm&#233;es. Cela peut s&#8217;expliquer par une trop fr&#233;quente utilisation des
anaphores qui limite la r&#233;p&#233;tition des entit&#233;s. De plus la reconnaissance &#224; l&#8217;aide de listes limite
le nombre d&#8217;entit&#233;s utilis&#233;es, et il faudra recommencer cette &#233;tude avec un outil de d&#233;tection
automatique des entit&#233;s nomm&#233;es, afin de pouvoir en utiliser un plus grand nombre. On peut
&#233;galement utiliser des cooccurrences d&#8217;entit&#233;s pour cr&#233;er des cha&#238;nes &quot;multi-lexicales&quot;.
</p>
<p>On constate que les r&#233;sultats pour un corpus th&#233;matiquement coh&#233;rent (sport), sont du m&#234;me
ordre que ceux pour un corpus g&#233;n&#233;raliste. Cela tend &#224; montrer que ces m&#233;thodes sont in-
d&#233;pendantes du type de lexique utilis&#233; dans les documents segment&#233;s, ce qui apporte une forme
d&#8217;ind&#233;pendance dans le type de document, et qui &#233;tait un de nos objectifs initiaux.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Laurianne Sitbon et Patrice Bellot
</p>
<p>4 Conclusion
</p>
<p>Les techniques que nous avons imagin&#233;es pour s&#8217;affranchir du param&#232;tre du hiatus dans l&#8217;emploi
des cha&#238;nes lexicales pour la segmentation th&#233;matique sont efficaces. Nous pensons pouvoir
encore am&#233;liorer la qualit&#233; de la segmentation en calculant les probabilit&#233;s de rupture th&#233;-
matique &#224; partir des similarit&#233;s, en utilisant des ordonnancement locaux &#224; chaque fronti&#232;re
candidate, comme cela est fait dans C99. Le d&#233;veloppement sous forme d&#8217;API est en cours,
l&#8217;outil sera distribu&#233; prochainement dans le cadre du projet technolangue AGILE/OURAL
(http://www.technolangue.net/article79.html).
</p>
<p>R&#233;f&#233;rences
</p>
<p>AMINI M., ZARAGOZA H. &amp; GALLINARI P. (2000). Learning for sequence extraction tasks. In Pro-
ceedings RIAO&#8217;2000, Paris, France.
</p>
<p>BRANTS T., CHEN F. &amp; TSOCHANTARIDIS I. (2002). Topic-based document segmentation with prob-
abilistic latent semantic anaysis. In Proceedings of CIKM&#8217;02, McLean, Virginia, USA.
CAILLET M., PESSIOT J.-F., AMINI M. &amp; GALLINARI P. (2004). Unsupervised learning with term
clustering for thematic segmentation of texts. In Proceedings RIAO&#8217;04, Avignon, France.
</p>
<p>CHINCHOR N. (1997). Muc-7 named entity task definition. in
http://www.itl.nist.gov/iaui/894.02/related projects/muc/proceedings/ne task.html.
CHOI F. Y. Y. (2000). Advances in domain independent linear text segmentation. In Proceedings of the
1st Meeting of the North American Chapter of the Association for Computational Linguistics, USA.
CHUANG S.-L. &amp; CHIEN L.-F. (2004). A practical web-based approach to generating topic hierarchy
for text segments. In Proceedings of the Thirteenth ACM conference on Information and knowledge
management table of contents, p. 127&#8211;136, Washington, D.C, USA.
GALLEY M., MCKEOWN K., FOLSER-LUSSIER E. &amp; JING H. (2003). Discourse segmentation of
multi-party conversation. In Proceedings of ACL&#8217;03, Sapporo, Japan.
MCDONALD D. &amp; CHEN H. (2002). Using sentence selection heuristics to rank text segments in txtrac-
tor. In Proceedings of the 2nd ACM/IEEE Joint Conference on Digital Libraries, p. 28&#8211;35.
MEKHALDI D., LALANNE D. &amp; INGOLD R. (2004). Using bi-modal alignment and clustering tech-
niques for documents and speech thematic segmentations. In Proceedings of the Thirteenth ACM con-
ference on Information and knowledge management table of contents, p. 69&#8211;77, Washington, D.C, USA.
PEVZNER L. &amp; HEARST M. A. (2002). A critique and improvement of an evaluation metric for text
segmentation. Computational Linguistics, p. 19&#8211;36.
</p>
<p>SITBON L. &amp; BELLOT P. (2004). Adapting and comparig linear segmentation methods for french. In
Proceedings RIAO&#8217;04, Avignon, France.
</p>
<p>UTIYAMA M. &amp; ISAHARA H. (2001). A statistical model for domain-independent text segmentation.
In in Meeting of the Association for Computational Linguistics, p. 491&#8211;498.</p>

</div></div>
</body></html>