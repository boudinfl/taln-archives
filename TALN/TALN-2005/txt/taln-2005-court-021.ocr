TALN 2005, Dourdan, 6-10 juin 2005

Segmentation thématique par chaines lexicales pondérées

Laurianne Sitbon, Patrice Bellot

Laboratoire d’Informatique d’Avignon - Université d’Avignon
339, chemin des Meinajaries — Agroparc BP 1228
84911 AVIGNON Cedex 9 — FRANCE
Tél : +33 (0)4 90 84 35 09
{laurianne.sitbon, patrice.bellot} @univ—avignon.fr

Mots-clefs I segmentation thématique, chaines lexicales, entités nommées
Keywords: topic segmentation, lexical chains, named entities

Résumé Cet article propose une méthode innovante et efﬁcace pour segmenter un texte
en parties thématiquement cohérentes, en utilisant des chaines lexicales pondérées. Les chaines
lexicales sont construites en fonction de hiatus variables, ou bien sans hiatus, ou encore pondérées
en fonction de la densité des occurrences du terme dans la chaine. D’autre part, nous avons
constaté que la prise en compte du repérage d’entités nommées dans la chaine de traitement, du
moins sans resolution des anaphores, n’améliore pas signiﬁcativement les performances. Enﬁn,
la qualité de la segmentation proposée est stable sur différentes thématiques, ce qui montre une
indépendance par rapport au type de document.

Abstract This paper presents an innovative and efﬁcient topic segmentation method based
on weighted lexical chains. This method comes from a study of different existing tools, and ex-
periments where we considered the inﬂuence of a term at each precise place in the text. We build
lexical chains with different kinds of hiatus (varying, none or density weighted). We demon-
strate good results on a manually built french news corpus. We show that using named entities
does not improve results. Finally, we show that our method tends to be domain-independent
because results are similar on various topics.

1 Introduction

La segmentation thématique intervient dans différents domaines en organisation de l’information,
tels que determiner les limites entre des dépéches dans un ﬂux d’informations (broadcast news,
TDT (Topic Detection and Tracking)), ou créer un résumé automatique de textes pour lequel la
segmentation sert a isoler les thématiques et les parties les plus representatives (McDonald &

Laurianne Sitbon et Patrice Bellot

Chen, 2002). Enﬁn, du point de vue d’un utilisateur, la segmentation thématique a également
des avantages en terme de facilités de lecture.

Beaucoup de moyens ont été imaginés pour segmenter un texte en themes cohérents. La prin-
cipale différence entre ces méthodes tient au fait qu’elles sont ou non supervisées. Parmi les
méthodes supervisées on trouve par exemple PLSA (Brants et al., 2002) qui apprend des prob-
abilités d’appartenance des termes a des classes sémantiques. D’autres méthodes se basent sur
un apprentissage comme (Amini et al., 2000) qui s’appuient sur des modeles de Markov cachés,
ou bien (Caillet et al., 2004) qui propose une classiﬁcation des termes de méme que (Chuang &
Chien, 2004) et (Mekhaldi et al., 2004). Nous avons développé une méthode non supervisée, a
base de matrice de similarités et de chaines lexicales du type de celles utilisées par (Utiyama &
Isahara, 2001) ou (Galley et al., 2003). Ce choix est lié a leurs capacités naturelles d’adaptation
a de nouvelles thématiques, eta leur relative indépendance vis a vis de la langue des textes.

2 Méthodologie

Avant de concevoir une nouvelle méthode, nous avons fait une évaluation de l’état de l’art pour
des textes en francais (Sitbon & Bellot, 2004). Cette étude préliminaire a notamment permis
d’analyser les différentes mesures d’évaluation de la segmentation, et de créer un corpus de
test en francais. Les outils que nous avons étudiés ont été comparées pour l’anglais par (Choi,
2000). Nos expériences ont montré que dans les conditions ou le texte a segmenter est une suite
d’articles bien distincts, et ou la qualité des outils est évaluée automatiquement en fonction
de la distance entre les frontieres trouvées et celles a trouver, l’outil le plus efﬁcace est C99
(Choi, 2000), qui ordonne localement les similarités entre chaque paire de phrases, puis fait
des regroupements par maximum de densité. Nous avons montré que le type de document que
l’on segmente, son theme, la taille et la variation de taille des segments a repérer, sont autant de
caractéristiques inﬂuencant le travail des segmenteurs.

2.1 Construction et pondération des chaines lexicales

Une chaine lexicale relie des termes de maniere linéaire dans un texte. Les méthodes actuelles
de segmentation les utilisent pour relier les occurrences d’un méme terme (ou lemme) qui sont
"proches". Une chaine est rompue lorsque le nombre de termes qui séparent deux occurrences
dépasse une valeur ﬁxée appelée hiatus. On peut alors recenser pour chaque phrase les chaines
actives.

Les applications des chaines lexicales utilisent actuellement des hiatus déﬁnis de maniere em-
pirique, et la notion d’activité d’une chaine est binaire (elle est active ou non active). Notre
premier objectif est d’éliminer le caractere empirique du hiatus, aﬁn que notre outil puisse
s’adapter a n’importe quel type de texte sans intervention de l’utilisateur. Pour cela on peut

Segmentation thématique par chaines lexicales pondérées

imaginer tout simplement la suppression du hiatus, ce qui revient a relier toutes les répétitions
de terInes. Nous avons également envisage l’utilisation de hiatus locaux : le hiatus moyen est
calculé pour chaque lemme. Ainsi si un mot est fortement répété a deux endroits distincts du
texte, il y aura automatiquement la création de deux chaines. De plus, s’il est répété trois fois en
début de texte, puis une fois a la ﬁn, il n’y aura qu’une seule chaine comprenant les occurrences
de début de texte.

Ces techniques créent un déséquilibre dans la signiﬁcativité de l’activité des chaines en jeu dans
le calcul des frontieres. I1 faut alors pondérer les chaines, en fonction de leur compacité (ratio
entre leur taille et le nombre d’occurrences). (Galley et al., 2003) propose une pondération des
chaines en fonction de la compacité et de la fréquence du terme considéré, et obtient de bons
résultats, malgré un hiatus déterminé de maniere empirique. La catégorie des lemmes a été
intégrée a cette pondération. Le poids d’une chaine associée a un terme m est déﬁni par :

L C$ C
sc0re(C'haine,m) = maJ:(Chaine,cat(m) X freq(Chaine, m) X log( t t ) (1)

chaine
o1‘1 freq(Chaine, m) est le nombre d’occurrences du terme m dans la chaine, Ltem la longueur
du texte, LCM,-ne la longueur de la chaine (on est alors indépendant de la taille des textes a
segmenter), et max(Chaine,cat(m)) le poids de la forme grammaticale la plus importante parmi

les occurrences du terme dans la chaine.

Puis on calcule les similarités a chaque ﬁn de phrase, qui est une rupture thématique potentielle.
La similarité est calculée avec :

Em sc0re(A, m) X sc0re(B, m)
\/Em sc0re(A, m) X Zm sc0re(B, m)

sim(A, B) = (2)

on A et B sont les ensembles de vecteurs représentant les poids des chaines lexicales actives
dans les n phrases avant et apres (nous avons choisi n=2), score(X, m) étant le poids maximal
du terme m dans l’ensemble des vecteurs X.

Les frontieres retenues sont alors celles pour lesquelles la similarité est en dessous d’un seuil
déterminé par siml,-m,-,5 = ,u +% 011 ,u et a sont la moyenne et la variance de toutes les similarités
calculées (Galley et al., 2003).

2.2 Evaluation

Aﬁn de constituer un corpus de grande taille aisément, on compose un corpus de test a partir
d’articles joumalistiques de theme globalement éloignés car classés manuellement dans dif-
férentes rubriques. Le corpus de test est composé de 4 series de 100 documents, chaque série

Laurianne Sitbon et Patrice Bellot

correspondant a une taille moyenne pré-déﬁnie des segments. Chaque document est composé de
10 segments qui sont autant d’extraits d’articles du journal Le Monde, choisis aléatoirement. Ce
journal proposant des thématiques tres variées, on suppose alors les segments thématiquement
cohérents et différents.

Pour évaluer l’efﬁcacité de nos nouveaux algorithmes, nous avons utilisé la mesure Window
Diff proposée par (Pevzner & Hearst, 2002), présentée et analysée dans (Sitbon & Bellot, 2004).

Nous avons testé les différentes approches pour le calcul des chaines lexicales. L’utilisation
d’un hiatus ﬁxe de 11 est le parametre préconisé par (Galley et al., 2003) avec l’outil LCseg.
Les résultats montrés dans cet article et rappelés ici dans le tableau 1 afﬁchent de meilleures
performances pour cette approche que C99 sur le Brown corpus, ainsi que sur le corpus TDT.

Taille LCseg hiatus 120 hiatus locaux
Brown Corpus TDT Corpus 9-11 0,3272 0,3187 0,3454
LCseg 0,1137 0,0909 3-11 0,3837 0,3685 0,4016
C99 0,1457 0,1272 3-5 0,4344 0,4309 0,4204
Table I: comparaison de LCseg et C99 pour un Table 2: Comparaison de LCseg et de notre
nombre de segments inconnu, selon la mesure méthode pour des segments de diﬁ‘e’remfes tailles
WindowDiﬁ’ ( en nombre de phrases )

Nous avons donc décidé de comparer notre approche a celle de (Galley et al., 2003). Les
résultats sont présentés dans le tableau 2. Etant donné que les textes ont tous moins de 110
phrases (le maximum étant 10 segments de 11 phrases chacun), le hiatus 120 correspond a
une absence de hiatus. Pour ces tests, les lemmes n’ont pas été pondérés en fonction de leur
catégorie.

Pour les segments de grande taille (9-11), ou de tailles variables (3-11) , la meilleure méthode
est ﬁnalement celle qui ne coupe pas les chaines lexicales (hiatus 120). Pour des segments de
petite taille, on observe une tres faible amélioration lorsqu’on utilise des hiatus différents pour
chaque lemme (hiatus locaux).

3 Exploitation d’une détection d’entités nommées

Si nous avons pris jusqu’ici les termes et leur fonction syntaxique comme seuls criteres, nous

pensons par ailleurs que la variation des noms propres est un indice intéressant.

On appelle entité nommée dans un texte tout ce qui fait reference a un identiﬁant unique (Chin-
chor, 1997). I1 peut s’agit un mot ou d’un groupe nominal. Nous avons utilisé trois types
d’entités nommées, repérées a partir d’un lexique fermé : listes de noms de personne, noms
de lieux et noms d’organisations. Etant un identiﬁant unique, une entité nommée a tendance a
moins se répéter d’un theme a l’autre. De plus, il est moins malvenu en francais de les répéter

que les noms communs ou adjectifs, meme si le probleme des anaphores reste a résoudre comme

Segmentation thématique par chaines lexicales pondérées

on le verra.

Nous avons conduit plusieurs types d’eXpériences (table 3), en fonction de poids plus ou moins
élevés attribués aux entités, ou en n’utilisant que les entités. Dans un premier temps nous
avons multiplié le poids des chaines contenant des entités nommées, par deux (ENX2) puis
par dix (ENx10). Ensuite nous avons testé la méthode en n’utilisant que les chaines lexicales
correspondant a des entités nommées (EN seules).

Segments 9-11 Segments 3-5

Méthode hiatus 120 hiatus locaux Méthode hiatus 120 hiatus locaux
classique 0,3187 0,3454 classique 0,4309 0,4204
ENx2 0,3211 0,3536 ENX2 0,4291 0,4128
ENx10 0,3521 0,3888 ENx10 0,4315 0,4202
EN seules 0,4235 0,4975 EN seules 0,4228 0,4291

Table 3: Evaluation sur un corpus journalistique avec diﬁ‘e’rentes pondérations des

entités nommées pour 2 tailles moyennes des segments

Les résultats présentés dans la table 3 montrent que l’amélioration avec une utilisation des
entités nommées est tres peu signiﬁcative d’une part, et qu’il faut doser cet usage d’autre part.
En effet on observe une perte de qualité lorsqu’on leur accorde un poids trop important ou
lorsque on ne considere qu’elles.

Nous avons ensuite refait les memes tests sur un corpus journalistique composé uniquement
d’articles traitant de sport, et sur lequel les méthodes testées dans (Sitbon & Bellot, 2004)

donnaient les résultats les plus médiocres.

Segments 9-11 Segments 3-5
Méthode hiatus 120 hiatus locaux Méthode hiatus 120 hiatus locaux
classique 0,3202 0,3463 classique 0,4375 0,4179
ENx2 0,3255 0,3321 ENX2 0,4359 0,4183
ENx10 0,3561 0,3695 ENx10 0,4393 0,4265
EN seules 0,3976 0,4621 EN seules 0,4430 0,4634

Table 4: Evaluation sur un corpus journalistique avec diﬁ‘e’rentes pondérations des

entités nommées pour 2 tailles moyennes des segments

Les résultats présentés sur la table 4 montrent que l’on n’obtient pas l’amélioration attendue par
l’utilisation des entités nommées. Cela peut s’expliquer par une trop fréquente utilisation des
anaphores qui liIr1ite la répétition des entités. De plus la reconnaissance a l’aide de listes limite
le nombre d’entités utilisées, et il faudra recommencer cette étude avec un outil de détection
automatique des entités nommées, aﬁn de pouvoir en utiliser un plus grand nombre. On peut
également utiliser des cooccurrences d’entités pour créer des chaines "multi-lexicales".

On constate que les résultats pour un corpus thématiquement cohérent (sport), sont du meme
ordre que ceux pour un corpus généraliste. Cela tend a montrer que ces méthodes sont in-
dépendantes du type de lexique utilisé dans les documents segmentés, ce qui apporte une forme
d’indépendance dans le type de document, et qui était un de nos objectifs initiaux.

Laurianne Sitbon et Patrice Bellot

4 Conclusion

Les techniques que nous avons imaginées pour s’ affranchir du parametre du hiatus dans l’emploi
des chaines lexicales pour la segmentation thématique sont efﬁcaces. Nous pensons pouvoir
encore améliorer la qualité de la segmentation en calculant les probabilités de rupture the-
matique a partir des similarités, en utilisant des ordonnancement locaux a chaque frontiere
candidate, comme cela est fait dans C99. Le développement sous forme d’API est en cours,
l’outil sera distribué prochainement dans le cadre du projet technolangue AGILE/OURAL
(http://www.technolangue.net/article79.html)

Références

AMINI M., ZARAGOZA H. & GALLINARI P. (2000). Learning for sequence extraction tasks. In Pro-
ceedings RIAO’2000, Paris, France.

BRANTS T., CHEN F. & TSOCHANTARIDIS I. (2002). Topic-based document segmentation with prob-
abilistic latent semantic anaysis. In Proceedings of CIKM’02, McLean, Virginia, USA.

CAILLET M., PESSIOT J .-F., AMINI M. & GALLINARI P. (2004). Unsupervised learning with term
clustering for thematic segmentation of texts. In Proceedings RIAO ’04, Avignon, France.

CHINCHOR N. (1997). Muc-7 named entity task deﬁnition. in
http://www.itl.nist.gov/iaui/894.02/related projects/muc/proceedings/ne task.html.

CHOI F. Y. Y. (2000). Advances in domain independent linear text segmentation. In Proceedings of the
1 st Meeting of the North American Chapter of the Association for Computational Linguistics, USA.

CHUANG S.-L. & CHIEN L.-F. (2004). A practical web-based approach to generating topic hierarchy
for text segments. In Proceedings of the Thirteenth ACM conference on Information and knowledge
management table of contents, p. 127-136, Washington, D.C, USA.

GALLEY M., MCKEOWN K., FOLSER-LUSSIER E. & JING H. (2003). Discourse segmentation of
multi-party conversation. In Proceedings of ACL’03, Sapporo, Japan.

MCDONALD D. & CHEN H. (2002). Using sentence selection heuristics to rank text segments in txtrac-
tor. In Proceedings of the 2nd ACM/IEEE Joint Conference on Digital Libraries, p. 28-35.

MEKHALDI D., LALANNE D. & INGOLD R. (2004). Using bi-modal alignment and clustering tech-
niques for documents and speech thematic segmentations. In Proceedings of the Thirteenth ACM con-
ference on Information and knowledge management table of contents, p. 69-77, Washington, D.C, USA.

PEVZNER L. & HEARST M. A. (2002). A critique and improvement of an evaluation metric for text
segmentation. Computational Linguistics, p. 19-36.

SITBON L. & BELLOT P. (2004). Adapting and comparig linear segmentation methods for french. In
Proceedings RIAO’04, Avignon, France.

UTIYAMA M. & ISAHARA H. (2001). A statistical model for domain-independent text segmentation.
In in Meeting of the Association for Computational Linguistics, p. 491-498.

