TALN 2005, Dourdan, 6-10 juin 2005

Segmentation thématique par chaines lexicales pondérées

Laurianne Sitbon, Patrice Bellot

Laboratoire d’Informatique d’Avignon — Université d’Avignon
339, chemin des Meinajaries — Agroparc BP 1228
84911 AVIGNON Cedex 9 - FRANCE
Tél : +33 (0)4 90 84 35 09
{1aurianne.sitbon, patricebellot} @univ—avignon.fr

M0tS-Cl€fS I segmentation thématique, chaines lexicales, entités nommées
K€yWOFdS2 topic segmentation, lexical chains, named entities

Résumé Cet article propose une méthode innovante et efﬁcace pour segmenter un texte
en parties thématiquement cohérentes, en utilisant des chaines lexicales pondérées. Les chaines
lexicales sont construites en fonction de hiatus variables, ou bien sans hiatus, ou encore pondérées
en fonction de la densité des occurrences du terme dans la chaine. D’autre part, nous avons
constaté que la prise en compte du repérage d’entités nommées dans la chaine de traitement, du
moins sans resolution des anaphores, n’améliore pas signiﬁcativement les performances. Enﬁn,
la qualité de la segmentation proposée est stable sur différentes thématiques, ce qui montre une
indépendance par rapport au type de document.

Abstract This paper presents an innovative and efﬁcient topic segmentation method based
on weighted lexical chains. This method comes from a study of different existing tools, and ex-
periments where we considered the inﬂuence of a term at each precise place in the text. We build
lexical chains with different kinds of hiatus (varying, none or density weighted). We demon-
strate good results on a manually built french news corpus. We show that using named entities
does not improve results. Finally, we show that our method tends to be domain—independent

because results are similar on various topics.

1 Introduction

La segmentation thématique intervient dans différents domaines en organisation de l’information,
tels que determiner les limites entre des dépéches dans un ﬂux d’informations (broadcast news,
TDT (Topic Detection and Tracking)), ou créer un résumé automatique de textes pour lequel la
segmentation sert a isoler les thématiques et les parties les plus representatives (McDonald &

505

506

Laurianne Sitbon et Patrice Bellot

Chen, 2002). Enﬁn, du point de vue d’un utilisateur, la segmentation thematique a egalement
des avantages en terme de facilites de lecture.

Beaucoup de moyens ont ete imagines pour segmenter un texte en themes coherents. La prin-
cipale difference entre ces methodes tient au fait qu’elles sont ou non supervisees. Parmi les
methodes supervisees on trouve par exemple PLSA (Brants er al., 2002) qui apprend des prob-
abilites d’appartenance des termes a des classes semantiques. D’autres methodes se basent sur
un apprentissage comme (Amini er al., 2000) qui s’appuient sur des modeles de Markov caches,
ou bien (Caillet er al., 2004) qui propose une classiﬁcation des termes de meme que (Chuang &
Chien, 2004) et (Mekhaldi er al., 2004). Nous avons developpe une methode non supervisee, a
base de matrice de similarites et de chaines lexicales du type de celles utilisees par (Utiyama &
Isahara, 2001) ou (Galley er al., 2003). Ce choix est lie at leurs capacites naturelles d’adaptation
a de nouvelles thematiques, et a leur relative independance vis a vis de la langue des textes.

2 Méthodologie

Avant de concevoir une nouvelle methode, nous avons fait une evaluation de l’etat de l’art pour
des textes en francais (Sitbon & Bellot, 2004). Cette etude preliminaire a notamment permis
d’analyser les differentes mesures d’evaluation de la segmentation, et de creer un corpus de
test en francais. Les outils que nous avons etudies ont ete comparees pour l’anglais par (Choi,
2000). Nos experiences ont montre que dans les conditions ou le texte a segmenter est une suite
d’articles bien distincts, et ou la qualite des outils est evaluee automatiquement en fonction
de la distance entre les frontieres trouvees et celles a trouver, l’outil le plus efﬁcace est C99
(Choi, 2000), qui ordonne localement les similarites entre chaque paire de phrases, puis fait
des regroupements par maximum de densite. Nous avons montre que le type de document que
l’on segmente, son theme, la taille et la variation de taille des segments a reperer, sont autant de
caracteristiques inﬂuencant le travail des segmenteurs.

2.1 Construction et pondération des chaines lexicales

Une chaine lexicale relie des termes de maniere lineaire dans un texte. Les methodes actuelles
de segmentation les utilisent pour relier les occurrences d’un meme terme (ou lemme) qui sont
"proches". Une chaine est rompue lorsque le nombre de termes qui separent deux occurrences
depas se une valeur ﬁxee appelee hiatus. On peut alors recenser pour chaque phrase les chaines
actives.

Les applications des chaines lexicales utilisent actuellement des hiatus deﬁnis de maniere em-
pirique, et la notion d’activite d’une chaine est binaire (elle est active ou non active). Notre
premier objectif est d’eliminer le caractere empirique du hiatus, aﬁn que notre outil puisse
s’adapter a n’importe quel type de texte sans intervention de l’utilisateur. Pour cela on peut

Segmentation thematique par chaines lexicales ponderees

imaginer tout simplement la suppression du hiatus, ce qui revient a relier toutes les repetitions
de termes. Nous avons egalement envisage l’utilisation de hiatus locaux : le hiatus moyen est
calcule pour chaque lemme. Ainsi si un mot est fortement repete a deux endroits distincts du
texte, il y aura automatiquement la creation de deux chaines. De plus, s’il est repete trois fois en
debut de texte, puis une fois a la ﬁn, il n’y aura qu’une seule chaine comprenant les occurrences
de debut de texte.

Ces techniques creent un desequilibre dans la signiﬁcativite de l’actiVite des chaines en jeu dans
le calcul des frontieres. Il faut alors ponderer les chaines, en fonction de leur compacite (ratio
entre leur taille et le nombre d’ occurrences). (Galley er al., 2003) propose une ponderation des
chaines en fonction de la compacite et de la frequence du terme considere, et obtient de bons
resultats, malgre un hiatus determine de maniere empirique. La categorie des lemmes a ete

integree a cette ponderation. Le poids d’une chaine associee a un terme m est deﬁni par :

- - - L 6:3 6
3c07“e(C'hame,m) : maav(C'hame,cat(m) >< f7“eq(C'hame, m) X l0g(L: ‘t ) (1)

on freq(Chaine, m) est le nombre d’occurrences du terme m dans la chaine, Ltem la longueur
du texte, LC;,,,,~,,e la longueur de la chaine (on est alors independant de la taille des textes a
segmenter), et max(Chaine,cat(m)) le poids de la forme grammaticale la plus importante parmi

les occurrences du terme dans la chaine.

Puis on calcule les similarites a chaque ﬁn de phrase, qui est une rupture thematique potentielle.
La similarite est calculee avec :

Zn, 3c07"e(A,m) >< 3c07“e(B,m)
\/Zn, 3c07"e(A, m) x 2m 3c07“e(B, m)

s2'm(A, B) : (2)

on A et B sont les ensembles de Vecteurs representant les poids des chaines lexicales actives
dans les n phrases avant et apres (nous avons choisi n=2), score(X, m) etant le poids maximal

du terme m dans l’ensemble des Vecteurs X.

Les frontieres retenues sont alors celles pour lesquelles la similarite est en dessous d’un seuil
determine par s2'm;,-W-,5 : /L +% on /L et 0 sont la moyenne et la Variance de toutes les similarites
calculees (Galley er al., 2003).

2.2 Evaluation

Aﬁn de constituer un corpus de grande taille aisement, on compose un corpus de test a partir
d’articles joumalistiques de theme globalement eloignes car classes manuellement dans dif-
ferentes rubriques. Le corpus de test est compose de 4 series de 100 documents, chaque serie

507

508

Laurianne Sitbon et Patrice Bellot

correspondant a une taille moyenne pre-deﬁnie des segments. Chaque document est compose de
10 segments qui sont autant d’extraits d’articles du journal Le Monde, choisis aleatoirement. Ce
journal proposant des thematiques tres Variees, on suppose alors les segments thematiquement
coherents et differents.

Pour evaluer l’efﬁcacite de nos nouveaux algorithmes, nous avons utilise la mesure Window
Diff proposee par (Pevzner & Hearst, 2002), presentee et analysee dans (Sitbon & Bellot, 2004).

Nous avons teste les differentes approches pour le calcul des chaines lexicales. L’utilisation
d’un hiatus ﬁxe de 11 est le parametre preconise par (Galley er al., 2003) avec l’outil LC'seg.
Les resultats montres dans cet article et rappeles ici dans le tableau 1 afﬁchent de meilleures
performances pour cette approche que C99 sur le Brown corpus, ainsi que sur le corpus TDT.

Taille LCseg hiatus 120 hiatus locaux
Brown Corpus TDT Corpus 9-11 0,3272 0,3187 0,3454
LOseg 0,1137 0,0909 3-11 0,3837 0,3685 0,4016
O99 0,1457 0,1272 3-5 0,4344 0,4309 0,4204

Table 1: comparaison de LC'seg et C99 pour an Table 2: Comparaison de LC'seg et de notre
nombre de segments inco/mu, selon la mesure

Wind0wDiﬁ‘

methode pour des segments de diﬁ”e’remes tailles
(en nombre de phrases)

Nous avons donc decide de comparer notre approche a celle de (Galley er al., 2003). Les
resultats sont presentes dans le tableau 2. Etant donne que les textes ont tous moins de 110
phrases (le maximum etant 10 segments de 11 phrases chacun), le hiatus 120 correspond a
une absence de hiatus. Pour ces tests, les lemmes n’ont pas ete ponderes en fonction de leur
categorie.

Pour les segments de grande taille (9-11), ou de tailles Variables (3-11) , la meilleure methode
est ﬁnalement celle qui ne coupe pas les chaines lexicales (hiatus 120). Pour des segments de
petite taille, on observe une tres faible amelioration lorsqu’on utilise des hiatus differents pour
chaque lemme (hiatus locaux).

3 Exploitation d’une détection d’entités nommées

Si nous avons pris jusqu’ici les termes et leur fonction syntaxique comme seuls criteres, nous

pensons par ailleurs que la Variation des noms propres est un indice interessant.

On appelle entite nommee dans un texte tout ce qui fait reference a un identiﬁant unique (Chin-
chor, 1997). Il peut s’agit un mot ou d’un groupe nominal. Nous avons utilise trois types
d’entites nommees, reperees a partir d’un lexique ferme : listes de noms de personne, noms
de lieux et noms d’organisations. Etant un identiﬁant unique, une entite nommee a tendance a
moins se repeter d’un theme a l’autre. De plus, il est moins malvenu en francais de les repeter

que les noms communs ou adjectifs, meme si le probleme des anaphores reste a resoudre comme

Segmentation thematique par chaines lexicales ponderees

on le Verra.

Nous avons conduit plusieurs types d’experiences (table 3), en fonction de poids plus ou moins
eleves attribues aux entites, ou en n’utilisant que les entites. Dans un premier temps nous
avons multiplie le poids des chaines contenant des entites nommees, par deux (ENx2) puis
par dix (ENx10). Ensuite nous avons teste la methode en n’utilisant que les chaines lexicales
correspondant a des entites nommees (EN seules).

Segments 9-11 Segments 3-5

Méthode hiatus 120 hiatus locaux Méthode hiatus 120 hiatus locaux
classique 0,3187 0,3454 classique 0,4309 0,4204
ENX2 0,3211 0,35 36 ENX2 0,4291 0,4128
ENx10 0,3521 0,3888 ENx10 0,4315 0,4202
EN seules 0,4235 0,4975 EN seules 0,4228 0,4291

Table 3.‘ Evaluation sur an corpus jonrnalistique avec diﬁ‘e’rentes p0nde’rati0ns des
entite’s n0mme’es pour 2 tailles moyennes des segments

Les resultats presentes dans la table 3 montrent que l’amelioration avec une utilisation des
entites nommees est tres peu signiﬁcative d’une part, et qu’il faut doser cet usage d’autre part.
En effet on observe une perte de qualite lorsqu’on leur accorde un poids trop important ou
lorsque on ne considere qu’elles.

Nous avons ensuite refait les memes tests sur un corpus journalistique compose uniquement
d’articles traitant de sport, et sur lequel les methodes testees dans (Sitbon & Bellot, 2004)

donnaient les resultats les plus mediocres.

Segments 9-11 Segments 3-5
Méthode hiatus 120 hiatus locaux Méthode hiatus 120 hiatus locaux
classique 0,3202 0,3463 classique 0,4375 0,4179
ENX2 0,3255 0,3321 ENX2 0,4359 0,4183
ENx10 0,3561 0,3695 ENx10 0,4393 0,4265
EN seules 0,3976 0,4621 EN seules 0,4430 0,4634

Table 4.‘ Evaluation sur an corpus jonrnalistique avec diﬁ”e’rentes p0nde’rati0ns des
entite’s n0mme’es pour 2 tailles moyennes des segments

Les resultats presentes sur la table 4 montrent que l’on n’obtient pas l’amelioration attendue par
l’utilisation des entites nommees. Cela peut s’expliquer par une trop frequente utilisation des
anaphores qui limite la repetition des entites. De plus la reconnaissance a l’aide de listes limite
le nombre d’entites utilisees, et il faudra recommencer cette etude avec un outil de detection
automatique des entites nommees, aﬁn de pouvoir en utiliser un plus grand nombre. On peut

egalement utiliser des cooccurrences d’entites pour creer des chaines "multi—lexicales".

On constate que les resultats pour un corpus thematiquement coherent (sport), sont du meme
ordre que ceux pour un corpus generaliste. Cela tend a montrer que ces methodes sont in-
dependantes du type de lexique utilise dans les documents segmentes, ce qui apporte une forme
d’independance dans le type de document, et qui etait un de nos objectifs initiaux.

509

510

Laurianne Sitbon et Patrice Bellot

4 Conclusion

Les techniques que nous avons imaginees pour s’ affranchir du parametre du hiatus dans l’emploi
des chaines lexicales pour la segmentation thematique sont efﬁcaces. Nous pensons pouvoir
encore ameliorer la qualite de la segmentation en calculant les probabilites de rupture the-
matique a partir des similarites, en utilisant des ordonnancement locaux a chaque frontiere
candidate, comme cela est fait dans C99. Le developpement sous forme d’API est en cours,
l’outil sera distribue prochainement dans le cadre du projet technolangue AGILE/OURAL
(http://www.technolangue.net/article79.html)

Références

AMINI M., ZARAGOZA H. & GALLINARI P. (2000). Learning for sequence extraction tasks. In Pro-
ceedings RIAO’2000, Paris, France.

BRANTS T., CHEN F. & TSOCHANTARIDIS I. (2002). Topic—based document segmentation with prob-
abilistic latent semantic anaysis. In Proceedings of CIKM’02, McLean, Virginia, USA.

CAILLET M., PESSIOT J AMINI M. & GALLINARI P. (2004). Unsupervised learning with term
clustering for thematic segmentation of texts. In Proceedings RIAO’04, Avignon, France.

(1997). Muc—7 task
http://www.itl.nist.gov/iaui/894.02/related projects/muc/proceedings/ne task.html.

CHINCHOR N. named entity deﬁnition. in
CHOI F. Y. Y. (2000). Advances in domain independent linear text segmentation. In Proceedings of the

1 st Meeting of the North American Chapter of the Association for Computational Linguistics, USA.

CHUANG S.—L. & CHIEN L.—F. (2004). A practical web—based approach to generating topic hierarchy
for text segments. In Proceedings of the Thirteenth ACM conference on Information and knowledge
management table of contents, p. 127-136, Washington, D.C, USA.

GALLEY M., MCKEOWN K., FOLSER-LUSSIER E. & JING H. (2003). Discourse segmentation of
multi—party conversation. In Proceedings of ACL’03, Sapporo, Japan.

MCDONALD D. & CHEN H. (2002). Using sentence selection heuristics to rank text segments in txtrac—
tor. In Proceedings of the 2nd ACM/IEEE Joint Conference on Digital Libraries, p. 28-35.

MEKHALDI D., LALANNE D. & INGOLD R. (2004). Using bi—modal alignment and clustering tech-
niques for documents and speech thematic segmentations. In Proceedings of the Thirteenth ACM con-
ference on Information and knowledge management table of contents, p. 69-77, Washington, D.C, USA.

PEVZNER L. & HEARST M. A. (2002). A critique and improvement of an evaluation metric for text
segmentation. Computational Linguistics, p. 19-36.

SITBON L. & BELLOT P. (2004). Adapting and comparig linear segmentation methods for french. In
Proceedings RIAO’04, Avignon, France.

UTIYAMA M. & ISAHARA H. (2001). A statistical model for domain—independent text segmentation.
In in Meeting of the Association for Computational Linguistics, p. 491-498.

