TALN 2005, Dourdan, 6-10 juin 2005

Une plateforme pour l’acquisition, la maintenance et la
validation de ressources lexicales

VanRullen T. , Blache P. , Portes C. , Rauzy S. , Maeyhieux J .-F. , Guénot
M.-L. ,Balfourier J .-M. , Bellengier E.
Laboratoire Parole et Langage - CNRS - Université de Provence
29, Avenue Robert Schuman - 13100 Aix-en-Provence
{tristan,pb}@lpl.univ—aix.fr

Mots-clefs I dictionnaire, lexique, lexique noyau

Keywords: dictionary, lexicon, kernel lexicon

Résumé Nous présentons une plateforme de développement de lexique offrant une base
lexicale accompagnée d’un certain nombre d’outils de maintenance et d’utilisation. Cette base,
qui comporte aujourd’hui 440.000 formes du Frangais contemporain, est destinée a étre diffusée
et remise a jour régulierement. Nous exposons d’abord les outils et les techniques employées
pour sa constitution et son enrichissement, notamment la technique de calcul des fréquences
lexicales par catégorie morphosyntaxique. Nous décrivons ensuite différentes approches pour
constituer un sous-lexique de taille réduite, dont la particularité est de couvrir plus de 90% de
l’usage. Un tel lexique noyau offre en outre la possibilité d’étre réellement complété manuelle-
ment avec des informations sémantiques, de valence, pragmatiques etc.

Abstract We present a lexical development platform which comprises a lexical database
of 440.000 lemmatized words of contemporary French, plus a set of maintenance tools. The
lexical database is intended to be distributed and updated regularly. We present in this paper
tools and techniques applied for the lexicon constitution and its enrichment, in particular the
computation of lexical frequencies by morphosyntactic category. Then we describe various
approaches to build an under-lexicon of reduced size, whose characteristic is to cover more
than 90% of the use. Such a kernel lexicon makes it moreover possible to be really enriched by
hand with semantic, valence, pragmatic information, etc.

VanRullen T., Blache P., Portes C., Rauzy S., Maeyhieux J .-F., Guénot M.-L. ,
Balfourier J .-M., Bellengier E.

1 Introduction

L’élaboration d’un lexique électronique peut sembler une tache obsolete, de nombreux lex-
iques du francais étant référencés. Cependant, force est de constater que cette afﬁrmation doit
étre modulée. La premiere constatation est que seul un petit nombre d’entre eux est effec-
tivement accessible. Il faut de ce point de vue souligner le role considérable joué par Bdlex
(cf. [de Calmes98]) qui, dans le cadre des activités du GdR-PRC Communication Homme-
Machine, a longtemps été le lexique le plus largement diffusé en contribuant ainsi puissamment
a l’évolution du domaine en France. Le mode de diffusion constitue évidemment un aspect
critique 1. Un rapide survol des ressources lexicales libres d’acces pour le francais permet d’en
identiﬁer deux :

- Lexique : il s’agit d’un lexique comportant 130.000 formes et comportant des
informations morphosyntaxiques, phonologiques et des indications de
fréquence (cf. [New01], http : //www . lexique . org/).

- AB U : contient 300.000 formes avec indications morphosyntaxiques (cf. [ABU],
http://abu.cnam.fr/)

On peut par ailleurs trouver quelques ressources verbales, par exemple :

- Leﬁj‘ : il contient 200.000 formes verbales, avec les informations de base (temps,
nombre, personne) (cf. [Clément04], http : / /www . le f f f . net /);

- Litote : c’est une base contenant les formes conjuguées de 6.500 verbes.
(http://www.loria.fr/equipes/calligramme/litote/)

Par ailleurs, il faut également signaler la démarche initiée par le Loria dans le cadre du pro-
jet Morphalou (cf. [Romary04], http: //loreley . loria . fr/morphalou/). Ce projet
fournira également a terme un lexique morphologique de 540.000 formes. Son intérét tient
d’une part au fait qu’il est collaboratif mais également qu’il s’inscrit dans le cadre du projet
LMF (Lexical Markup Framework), proposant la normalisation du codage des informations
linguistiques.

Il reste donc un travail important pour parvenir a un lexique de qualité. Pour cela, une base
lexicale doit avant tout étre nettoyée de facon a proposer une couverture adéquate du francais. Il
ne sert a rien de constituer une ressource de 400 ou 500.000 formes si la plupart d’entre elles ne
sont pas attestées. Le second aspect concerne le type d’informations contenu dans le lexique. Il
est en effet nécessaire qu’un lexique contienne pour une méme entrée autant d’informations que
possible concernant ses propriétés morphologiques, syntaxiques, bien entendu, mais également
sémantiques, phonétiques ou phonologiques. La forme phonétisée de l’entrée, la syllabation ou
la fréquence sont par exemple autant d’informations précieuses pour la description.

Nous décrivons dans cet article la base lexicale développée au LPL. Cette base, construite au-
tour d’un lexique morphologique, présente la particularité d’étre couvrante, de contenir des
information variées et d’avoir été validée sur corpus. Cette base est associée a une véritable
plateforme de développement lexical, munie de divers outils de maintenance et d’acces. Apres
une présentation des principales caractéristiques de cette plateforme, nous en proposons une
évaluation se fondant sur différents corpus. Nous décrivons de plus l’eXploitation de cette base
dans la perspective d’une étude lexicale du francais contemporain.

1Nous nous associons de ce point de vue a la démarche aujourd’hui proposée par le projet Morphalou et nos
ressources seront distribuées dans ce cadre

Une plateforme pour l’acquisition, la maintenance et la validation de ressources
lexicales

2 Le lexique complet

Le lexique que nous avons Inis au point a fait l’objet de beaucoup d’études et de travaux
d’amélioration. Nous aboutissons actuellement a un lexique défactorisé de plus de 444.000
entrées correspondant a environ 320.000 formes orthographiques différentes. Ce lexique est
associé a un ensemble d’outils permettant sa maintenance, sa sécurisation et son interrogation.
Ce projet est la base nécessaire a des applications du TALN qui auront besoin d’une ressource
ﬁable, c’est pourquoi l’accent a été Inis sur la maintenabilité de la ressource.

Les entrées du lexique Dic0LPL sont basées sur des ressources libres et une acquisition semi-
automatique. Comme le montre la ﬁgure 1, nous avons au départ recensé et incorporé des
lexiques libres, tels ABU ou Lexiqueorg. Le formatage de notre lexique a nécessité un travail
de transformation, de catégorisation, de phonétisation etc., aﬁn de faire correspondre les entrées
acquises. L’ étape importante que constitue le calcul des fréquences lexicales est abordé dans la
prochaine section.

RGSSOUFCES

-Ressources libres:
‘ABU
‘Lexique.org
_AutreS (divers) ‘Phonétisation,

'Ressources acquises 9 E -Lemme’

Filtrage

‘Manuellement _Caté9°rie

Spécification des champs
‘Mot,
-Majuscule,

 
     

Lexique:
444 266
entrées

 

_ _ Nettoyage Syntaxique,
(c’*C)L:t[;’L:‘S‘)"t'q ”eme”t Défactorisation -Fréq Hence, em”
Figure 1: Conception du dictionnaire Figure 2: Extrait du lexique

Le format du lexique, son codage, et son stockage ont été pensés aﬁn d’accélérer son charge-
ment dans les applications qui le requierent. Ce lexique est en effet actuellement embarqué dans
des applications de communication sur des machines ayant de petites capacités. D’autre part,
il s’agit de permettre avec le meme stockage un développement et des modiﬁcations manuels.
C’est pourquoi un format ASCII, structuré en CSV tabulé classique a été choisi, plutot qu’un
standard XML ou qu’une forme binaire de type base de données. Ce choix a répondu a nos
attentes et permet une transformation rapide dans d’autres formats tels que le XML répondant
aux normes ISO (TC37/SC4) utilisées par le projet MORPHALOU par exemple.

Notre lexique se structure sous une forme défactorisée (une ligne par quadruplet [M0t, Phoneti-
sation, Categorie, Lemme] par opposition a d’autres lexiques pour lesquels une seule ligne est
réservée pour chaque forme orthographique.

L’ extrait de lexique donné dans la table 2 met en évidence les caractéristiques de son format.
On y observe la défactorisation du mot déambulais.

Certaines colonnes ont été réservées pour un usage ultérieur; les mots acceptés dans ce lex-
ique ne doivent pas étre des afﬁxes, mais toujours des mots (simples ou composés) du langage
courant. Ainsi, les préﬁxes et sufﬁxes tels que anti, hecto, isme ou able en sont rejetés.

Le codage des champs du lexique est lui aussi contraint: les fréquences correspondent au nom-
bre d’occurences de chaque entrée mesurée sur les corpus d’apprentissage. Les valeurs sont des
entiers et ne représentent pas des pourcentages. Les valeurs de traits des catégories de chaque
entrée sont formatées selon un codage dérivé de Multext et de Grace. La forme phonétisée est
exprimée a l’aide de l’alphabet standard Sampa, qui permet un codage phonétique en texte brut
sans faire appel a des polices de caracteres spéciﬁques.

VanRullen T., Blache P., Portes C., Rauzy S., Maeyhieux J .-F., Guénot M.-L. ,
Balfourier J .-M., Bellengier E.

3 Plateforme d’enrichissement du lexique

Le lexique Dic0LPL est une ressource en évolution. Nous présentons ici quelques uns des outils
qui permettent son enrichissement.

Deux outils - un segmenteur et un étiqueteur- met-
tent en relation les mots d’un texte fourni en en-
texte >5e9”‘e“te“r Eﬁqueteur Sortie XML tree avec les mots du lexique. La ﬁgure 3 illus-

j j tre leur usage. Le segmenteur, basé sur des au-
tomates simples, effectue un découpage du texte
en tokens. C’est a partir de ces informations

que l’étiqueteur effectuera la désambiguisation en
Flgure 33 Segmenteur et etlqueteur contexte des categories a attribuer a chaque token.

La technique de désambiguisation que nous utilisons s’inspire des techniques stochastiques ex-
istantes. Nous avons cependant préféré développer notre propre étiqueteur aﬁn de correspondre
au mieux avec la précision des traits morphosyntaxiques que nous employons. Une premiere
évaluation de l’étiqueteur sur le corpus du projet Multitag (cf. [Paroubek00]) a donné des ré-
sultats par catégorie variant de 60% a 99%. Le score moyen calculé sur le corpus de référence
Multitag est de 95%.

Aﬁn d’enrichir le lexique et de calculer au besoin
les fréquences lexicales spéciﬁques a un ensem-
ble de corpus, nous avons développé un outil de
fréquencage. Comme l’indique la ﬁgure 4, cet
outil fait appel aux résultats de l’étiquetage pour
en déduire les fréquences des entrées du lexique,
pour chaque couple (mot, catégorie). A partir
d’un lexique initial, étant donné un ensemble de
textes, nous obtenons en sortie du fréquenceur un
lexique des mots inconnus, un lexique des noms-
propres et une nouvelle version du lexique initial,
Figure 4: calcul des fréquences lexicales dent 133 champsfréquence Sent ms 5 joun

     

Ensemble de textes
Fréquencage étiquetés
%_f
+
fréquences

Ensemble de
textes

 
    

Lexique initial
%
Lexique fréquencé

     
     
   

Fusion

  

Mots inconnus
§——j_—J
Noms propres

La version actuelle de Dic0LPL dispose des fréquences acquises sur 153 millions de mots tirés
du journal Le Monde, de ressources littéraires gratuites, de transcriptions de corpus oraux et des
textes spéciﬁques (domaine médical, corpus de mails etc.).

D’autre part, la forme phonétisée des entrées est obtenue grace a un phonétiseur inspiré du
projet Syntaix (cf. [Di Cristo01]) pour la conception d’un systeme de synthese vocale. D’autres
champs (sémantique, valence verbale etc.) nécessitent toujours une validation manuelle.

L’ évaluation des outils et du lexique est réalisée avec les techniques suivantes: Nous pouvons
mesurer la couverture du lexique pour un corpus donné (calculer le quotient nombre de mots
rec0nnus/ nombre total de mots). La couverture actuelle du lexique représente 96% des corpus
analysés (153 millions de mots). Lorsque nous souhaitons une information plus ﬁne concernant
l’étiquetage, il faut alors disposer d’un corpus de référence, pour lequel chaque mot est associé a
une catégorie morphosyntaxique certiﬁée. Il est alors possible de mesurer les scores de rappel
et précision pour chaque catégorie. C’est dans ce cadre que nous avons pu calculer un score de
95% sur le corpus de référence Multitag.

Une plateforme pour l’acquisition, la maintenance et la validation de ressources
lexicales

4 Un lexique noyau du frangais contemporain

Une fois le lexique constitué, il est nécessaire de vériﬁer sa couverture. Par ailleurs, l’analyse
des corpus décrits plus haut permet de fournir des indications pour la constitution d’un dic-
tionnaire minimal (ou lexique noyau) du francais ayant une couverture maximale (un tel sous-
lexique est toujours spéciﬁque a un ensemble de corpus). Cette ressource est d’une grande
importance pour le futur: I1 n’est pas possible d’enrichir un grand lexique manuellement. Or,
nombre d’informations ne peuvent aujourd’hui étre acquises totalement automatiquement, no-
tamment les informations sémantiques. Un lexique noyau permet d’identiﬁer un nombre limité
d’entrées lexicales qu’il est possible d’enrichir y compris manuellement. L’ objectif est a terme
de disposer d’une ressource lexicale tres complete, comportant des informations syntaxiques,
sémantiques, voire pragmatiques. Un lexique limité aux 10.000 formes les plus fréquentes
couvre en moyenne 90% du francais. I1 s’avere donc intéressant de sélectionner un lexique
noyau du Francais contemporain avoisinant cette taille. La qualité de l’information concernant
la fréquence de chacune des entrées du lexique complet permet de concevoir un lexique noyau
(dorénavant LN) des mots les plus fréquents. C’est aussi l’occasion d’évaluer diachroniquement
l’évolution du lexique de base du francais depuis "Le Francais Fondamental" (cf. [Gougen-
heim64] et [Blache05]). Nous avons sélectionné les formes pertinentes du LN grace a une
méthode simple utilisant une fréquence seuil (une autre méthode basée sur une réﬂexion a pro-
pos des types de categories a conserver indépendamment de leur fréquence s’est révélée moins
efﬁcace et a été abandonnée). Ainsi, pour obtenir un dictionnaire de 10.000 formes (LN10)
avons nous sélectionné les 10.000 entrées les plus fréquentes du lexique général DicoLPL, c’est-
a-dire toutes les formes dont la fréquence est supérieure a 1091. Différentes versions de LN de
taille croissante ont été produites suivant la méme méthode : LN15 (fréquence>613, 15.017
formes), LN20 (fréquence>389,19.990 formes) et LN30 (fréquence>193, 30.018 formes) aﬁn
de comparer leurs couvertures et choisir le meilleur rendement taille/couverture.

Nous avons soumis les différentes versions du
LN a un test de couverture sur deux types dif-
férents de corpus: un corpus écrit (580.000
mots extraits d’articles publiés dans le journal
Le Monde) et un corpus oral (435.000 mots
et regroupe le Bristol Corpus, un ensemble de
95 entretiens enregistrés et transcrits par Kate
Figure 53 C011Ve1't11TeS Par taille (13 lexique Beeching (1988-1990), ainsi que des corpus de
et Par type (13 COFPUS parole recueillis au LPL).

 

Les résultats présentés dans la ﬁgure 5 appellent plusieurs commentaires: nous constatons
d’abord que la couverture du lexique général DicoLPL (demiere ligne) n’est pas totale et qu’elle
est meilleure pour le corpus oral que pour le corpus écrit, remarque qui vaut aussi pour les
autres dictionnaires. Ceci s’explique selon nous par le fait que l’écrit utilise un vocabulaire
beaucoup plus étendu et varié que l’oral. On constate aussi que les performances de couverture
s’améliorent régulierement au fur et a mesure que le LN contient plus de formes, ce qui est bien
sﬁr attendu. I1 faut néanmoins noter qu’il existe un saut qualitatif plus important entre LN10
et LN15 qu’entre LN15 et LN20 ou LN20 et LN30 alors méme que l’écart de taille entre ces
deux derniers est plus important. Le dictionnaire noyau de 15000 formes apparait donc comme
la version optimale pour obtenir la plus grande couverture avec un nombre réduit de formes.

VanRullen T., Blache P., Portes C., Rauzy S., Maeyhieux J .-F., Guénot M.-L. ,
Balfourier J .-M., Bellengier E.

5 Conclusion

La plateforme de développement de lexique décrite dans cet article répond a un certain nombre
de besoins a la fois en termes de richesse d’informations, mais également de développements
de lexiques spécialisés en produisant des fréquences spéciﬁques. Notre approche permet de
rationaliser le choix des entrées sur lesquelles travailler en proposant la construction d’un lex-
ique noyau élaboré sur la base d’une véritable analyse de la langue. L’enrichissement manuel
de petits lexiques avec des informations sémantiques, pragmatiques etc. s’en trouve facilité.
C’est pourquoi nous défendons la démarche qui consiste a concentrer les efforts sur un sous-
lexique dont la couverture a été vériﬁée sur corpus. D’autre part, un lexique de petite taille offre
de nombreuses possibilités d’études sur l’usage avec notamment les réseaux sémantiques, les
petits mondes etc. (voir a ce propos [Ferrer01]).

Le fait de disposer d’un grand lexique de formes n’en reste pas moins un atout, puisque c’est
a partir d’une telle ressource que peuvent étre extraits des sous-lexiques ad hoc couvrant des
types de texte de domaines divers que lefréquengage permet d’isoler.

Enﬁn, la tache de constituer une telle ressource est immense. Nous souhaitons la voir s’ améliorer
avec le temps, ce qui suppose sa diffusion, sa confrontation a l’usage et un retour de la com-
munauté. La plateforme décrite ici comportant une série d’outils de maintenance, il est ainsi
possible d’envisager une Inise a jour réguliere des informations. Au total, notre contribution
viendrait s’inscire dans le mouvement de Inise a disposition de ressources du frangais initié par
les différents projets signalés plus haut.

Références

Association des Bibliophiles Universels, “ABU. Dictionnaire des mots communs”, in La Bibliothéque
Universelle, http2//abu.cnam.fr/DICO/mots-communs.htm1. CNAM.

Blache P., M.-L. Guénot & C. Portes (2005), “Outils et ressources pour la mise a jour du Frangais
Fondamenta ”, in Proceedings of Frangais Fondamenta12 50 ans de travaux et d’enjeux.

C1émentL., B. Sagot & B. Lang (2004), “Morphology-Based Automatic Acquisition”, in proceedings of
LREC-04.

de Calmés M. & G. Pérennou (1998), “BDLEX 2 a Lexicon for Spoken and Written French”, in proceed-
ings of LREC-98

Di Cristo & P. Di Cristo (2001), “Syntaix 2 une approche métrique-autosegmentale de la prosodie”, in
revue TAL, 4221

Ferrer R., Cancho I. & Sole R. (2001), “The small-world of human language”, Proceedings of the Royal
Society of London, B 268, 2261- 2266 url = "citeseer.ist.psu.edu/ferrer01small.html"

Gougenheim, G. ; Rivenc, P. ; Michéa, R. & Sauvageot, A. (1964), “L’élaboration du Frangais Fonda-
mental”, 1er degré, Didier 2 New B.

Pallier C., L. Ferrand & R.Matos (2001), “Une base de données lexicales du Frangais contemporain sur
Internet 2 Lexique ”, in L’Année Pschologique, 101

Paroubek P. & M. Rajman (2000), “MULTITAG, une ressource linguistique produit du paradigme
d’évaluation”, in Actes de la conférence TALN-2000

Romary L., S. Salmon-Alt & G. Francopoulo (2004), “Standards going concrete: from LMF to Mor-
phalou”, in Workshop on Electronic Dictionaries, COLING-04.

