TALN 2005, Dourdan, 6-10 juin 2005

La plate-forme LinguaStream :
un outil d’exploration linguistique sur corpus

WIDLOCHER Antoine, BILHAUT Frederik
GREYC - CNRS UMR 6072 - Université de Caen
Campus 11, Sciences 3, B.P. 5186, 14032 Caen Cedex, France
{awidloch,fbilhaut} @ info.unicaen.fr

Mots-clefs I Linguistique de corpus, TAL, plate-forme logicielle

Keywords: Corpus linguistics, NLP, software framework

Résumé A travers la presentation de la plate-forme LinguaStream, nous présentons cer-
tains principes méthodologiques et différents modeles d’analyse pouvant permettre l’articula-
tion de traitements sur corpus. Nous envisageons en particulier les besoins nés de perspectives
émergentes en TAL telles que l’analyse du discours.

Abstract By presenting the LinguaStream platform, we introduce different methodologi-
cal principles and analysis models, which make it possible to articulate corpus processing tasks.
More especially, we consider emerging approaches in NLP, such as discourse analysis.

1 Introduction

Par dela la diversité des domaines d’investigation et des objets d’étude, un certain nombre de
tendances communes se conﬁrment peu a peu au sein de la communauté TAL. Se manifeste
tout d’abord, désormais distinctement, la généralisation du travail sur corpus, mouvement qui
constitue d’ailleurs un point de convergence fécond entre les travaux spéciﬁquement dédiés au
TAL et les démarches plus immédiatemment linguistiques. Les modeles théoriques proposés
doivent désormais trouver leur justiﬁcation et prouver leur validité « en corpus », et leur per-
tinence sera jugée, tantot sur leur capacité a rendre compte de la diversité dudit corpus (dans
une perspective descriptive), tantot a la lumiere de leur capacité a l’explorer « efﬁcacement »
(dans une perspective d’ingénierie documentaire). Se pose alors inévitablement la question de
la méthode et des outils a adopter pour travailler ainsi « sur corpus ».

I1 devient en effet difﬁcilement envisageable de considérer celui-ci comme une matiere brute a
laquelle devraient se référer immédiatement les différents modeles et traitements. Au contraire,
la multiplication des points de vue sur le corpus, qu’ils soient morphologiques, syntaxiques, se-
mantiques, rhétoriques ou pragmatiques, qu’ils ne visent que l’une de ces dimensions ou qu’ils
les croisent, rend pressante la question des interdépendances entre ces vues possibles, interdé-
pendances qui seront d’autant plus nombreuses que des résultats satisfaisants seront obtenus
par chacune des approches. Une récente Joumée d’Etude de l’ATALA a d’ailleurs permis de
poser tres frontalement la question désormais centrale de l’articulation des traitements sur cor-
pus. Or, si l’articulation des traitements rend indispensable une réﬂexion sur leur modularité,
elle conduit également a réinterroger l’ensemble de leur processus d’élaboration, de la prise en

WIDLOCHER Antoine, BILHAUT Frédérik

charge du corpus, jusqu’a l’e’valuati0n des résulats, en imposant que soient repensées les no-
tions méme d’0bservati0n et d’expe’rimentati0n, a travers, en particulier, une réﬂexion sur les
cycles d’e’valuati0n/validation puis d’ajustement des méthodes d’analyse.

Enﬁn, de nouvelles perspectives en TAL conﬁrment ces nouveaux besoins. Si nous considérons
l’intérét récent accordé a l’analyse automatique de l’organisation discursive, par exemple en
termes thématiques (Bilhaut, 2004) ou rhétoriques (Widliicher, 2004), i1 apparait clairement
que ces investigations sont rendues possibles par la préexistence de résultats satisfaisants aux
niveaux de granularité inférieurs, en matiere d’analyse morpho-syntaxique et sémantique, aux
niveaux lexicaux et syntagmatiques. Ces travaux se trouvent consubstantiellement liés a des
stratégies d’empilement de traitements successifs permettant l’enrichissement incremental des
vues sur le corpus et l’abstraction progressive des formes de surface par l’utilisation des analyses
préalables. A travers la présentation de LinguaStream1, nous envisageons ici différents éléments
méthodologiques et techniques pouvant permettre d’assumer ces nouvelles orientations.

2 La plate-forme LinguaStream

LinguaStream (Bilhaut & Widlocher, 2005) a pour principale ambition de faciliter la réalisa-
tion d’expériences sur corpus non triviales en TAL, ainsi que le cycle d’évaluation/ajustement
qui en découle. Sans outil adapté, le coﬁt de développement induit par chaque nouvelle ex-
périence devient en effet un frein considérable a l’approche expérimentale. Pour répondre a
ce probleme, LinguaStream facilite la mise en oeuvre de procédés complexes tout en requé-
rant des compétences informatiques minimales. Plate-forme générique fondée sur le principe
d’enrichissement incrémental des documents électroniques, elle facilite la conception et l’éva-
luation de chaines de traitements complexes, par assemblage visuel de modules d’analyse de
types et de niveaux variés : morphologique, syntaxique, sémantique, discursif... Chaque palier
de la chaine de traitement se traduit par la découverte et le marquage de nouvelles informations,
sur lesquelles pourront s’appuyer les analyseurs subséquents.

Un environnement de développement intégré permet de construire visuellement ces chaines de
traitement, a partir d’une « palette » de composants (une cinquantaine est intégrée en standard)
facilement extensible grace une API Java, un systeme de macro-composants, et des templates.
Certains sont spéciﬁquement dédiés au TAL, et d’autres permettent de résoudre différents pro-
blemes liés a la gestion des documents électroniques (traitements XML en particulier). D’autres
peuvent étre utilisés pour effectuer des calculs sur les annotations produites par les analyseurs,
ou encore générer des diagrammes. Chacun dispose d’un ou plusieurs points d’entrée et/ou de
sortie que l’on relie pour obtenir la chaine voulue, celle-ci étant représentée par un graphe ou les
divers composants apparaissent sous forme de « boites » reliées entre elles. Chaque composant
propose un nombre variable de parametres permettant d’adapter leur comportement. Les mar-
quages produits sur un méme document sont organisés en couches indépendantes, supportant
enchassements et chevauchements. La plate-forme se base systématiquement sur les standards
et outils XML, et peut traiter tout ﬁchier de ce type en préservant sa structure orginelle. A l’exé-
cution, elle se charge de l’ordonnancement des sous-taches, et différents outils permettent in
ﬁne de Visualiser les documents analysés et leurs annotations.

Principes fondamentaux

En premier lieu, la plate-forme recourt systématiquement a des représentations déclaratives
pour spéciﬁer les différents traitements, ainsi que leur enchainement sous forme de graphe.
Les différents formalismes disponibles permettent ainsi de transcrire directement l’expertise

1On trouvera une présentation complete de la plate-forme a l’adresse http : / / www . l inguast ream . org.

La plate-forme LinguaStream

linguistique a mettre en oeuvre, l’appareil procédural qui en résulte étant pris en charge par
la plate-forme. Les regles données ont donc une valeur tant descriptive, en tant que repré-
sentations formelles d’un phénomene linguistique, que prescriptive, en tant qu’instructions de
traitement foumies a un processus informatique.

De plus, la plate-forme exploite la complémentarité des modéles d’analyse, plutot que de pri-
vilégier un hypothétique modele « omnipotent » capable d’eXprimer efﬁcacement tout type de
contrainte. Nous faisons en effet l’hypothese qu’un analyseur complexe doit adopter successi-
vement plusieurs regards sur le méme matériau linguistique, auxquels répondront des forma-
lismes distincts. On po11rra combiner, au sein d’un méme traitement, des expressions régulieres
au niveau morphologique, une grammaire locale au niveau syntagmatique, un transducteur au
niveau phrastique et une grammaire DSDL (cf. infra) au niveau discursif. L’interopérabilité des
différents modeles d’analyse proposés est garantie par l’usage d’une représentation uniﬁée des
marquages et des annotations. Ces dernieres sont uniformément représentées par des structures
de traits, modele communément utilisé en TAL et en linguistique, et permettant de représen-
ter des annotations riches et structurées. Tout composant d’analyse po11rra produire son propre
marquage en s’appuyant sur les analyses précédentes : les formalismes proposés permettent de
spéciﬁer des contraintes sur les annotations existantes par uniﬁcation. La plate-forme favorise
ainsi l’abstraction progressive des formes de surface. Chaque palier d’analyse pouvant accé-
der simultanément aux annotations produites par tous les paliers antérieurs, les analyseurs de
plus haut niveau sont généralement conduits a s’abstraire progressivement du matériau textuel
pour ne plus reposer que sur des représentations symboliques antérieurement calculées.

Un autre aspect important conceme la variabilité du grain d’analyse au cours du traitement.
De nombreux modeles d’analyse imposent la déﬁnition d’un grain d’analyse minimal, dit « je-
ton » ou token. C’est par exemple le cas de toute grammaire ou transducteur : ces formalismes
supposent l’existence d’une unité textuelle (comme le caractere ou le mot) a laquelle s’ap-
pliquent les patrons. Quand la déﬁnition de ce grain minimal est nécessaire au fonctionnement
d’un composant, la plate-forme permet de spéciﬁer localement le ou les types d’unités a consi-
dérer comme jetons. Toute unité préalablement délimitée peut jouer ce role : il po11rra s’agir
du découpage habituel en mots, mais aussi de toute autre unité ayant été préalablement mar-
quée : syntagmes, phrases, cadres du discours, etc. Le grain minimal peut donc étre différent
pour chaque palier de l’analyse, ce qui augmente considérablement la portée des différents mo-
deles d’analyses utilisables dans la plate-forme. D’autre part, chaque module d’analyse spéciﬁe
les marquages antérieurs auxquels il souhaite faire référence, pouvant ainsi ne tenir compte
que des marquages qu’il estime pertinents, et donc s’affranchir partiellement de la linéarité du
texte. La combinaison de ces fonctionnalités permet d’adopter un point de vue sur le document
spéciﬁque a chaque étape d’une chaine de traitement.

La modularité des chaines de traitements favorise quant a elle la réutilisabilité des compo-
sants dans des contextes différents : un module d’analyse développé au sein d’une premiere
chaine po11rra étre réutilisé dans d’autres chaines. De facon similaire, toute chaine po11rra étre
réutilisée en tant que constituant d’une chaine de plus haut niveau, sous forme de « macro-
composant ». Réciproquement, pour une chaine donnée, on po11rra substituer a un composant
tout autre composant fonctionnellement équivalent. Pour une sous-tache donnée, un prototype
rudimentaire po11rra étre remplacé in ﬁne par un équivalent pleinement opérationnel. Ceci rend
possible la mise en comparaison des traitements, en soumettant ces derniers a des contextes
rigoureusement identiques, condition sine qua non d’une comparaison pertinente.

Modéles d’analyse

Nous avons évoqué plus haut quelques-uns des composants susceptibles de prendre part a une
chaine de traitement. Parmi ceux spéciﬁquement dédiés au TAL, on peut distinguer deux fa-
milles. La premiere regroupe les analyseurs « préts a l’emploi », dédiés a une tache précise.

WIDLOCHER Antoine, BILHAUT Frédérik

I1 s’agira par exemple de l’étiquetage morpho-syntaxique, une interface avec TreeTagger étant
intégrée par défaut. Ces composants sont paramétrables, mais il n’est pas possible de modiﬁer
fondamentalement leur fonctionnement. D’autres au contraire proposent un modele d ’analyse,
c’est-a-dire un formalisme de représentation de contraintes linguistiques, éventuellement asso-
cié a un modele opératoire, par lequel l’utilisateur peut spéciﬁer intégralement le traitement a
opérer. Ils permettent d’exprimer des contraintes tant sur les formes de surface que sur les an-
notations insérées par les analyseurs précédents. Toutes les annotations sont représentées sous
forme de structures de traits, et les contraintes sont systématiquement spéciﬁées par uniﬁcation
sur ces structures. Quelques-uns des systémes proposés sont :

— Un systeme appelé EDCG (pour Extended-DCG), permettant de décrire des grammaires lo-
cales d’uniﬁcati0n en se basant sur la syntaxe DCG (Deﬁnite Clause Grammars) de Prolog.
Une telle grammaire peut étre décrite dans le plus pur style déclaratif, bien que les spéciﬁcités
du langage logique restent accessibles aux utilisateurs expérimentés.

— Un systeme, nommé MRE (pour Macr0-Regular-Expressions), permettant de décrire des pa-
trons par transducteurs, s’appliquant aussi bien aux formes de surface qu’aux annotations
préalablement calculées. Sa syntaxe est similaire a celle des expressions régulieres commu-
nément utilisées en TAL et en linguistique sur corpus. Mais a la différence de ces dernieres,
ce formalisme ne s’applique pas spéciﬁquement aux caracteres ni aux mots, et peut porter sur
toute unité textuelle préalablement analysée.

— Un formalisme d’expression de contraintes au niveau discursif. En cours d’é1aboration,
DSDL (Discourse Structure Description Language), que nous décrirons plus loin, permet
l’exploration des organisations discursives par l’expression et la satisfaction de contraintes,
pouvant étre non séquentielles exprimées a l’aide d’un ensemble de fonctions discursives pri-
mitives (présence/absence, cohérence sémantique...), et pouvant porter en particulier sur les
annotations produites en amont et sur des relations entre ces demieres.

— Un systeme d’annotation a partir de lexiques sémantiques, un systeme de tokenisation basé
sur des expressions régulieres (au niveau caractere), un systeme permettant de délimiter des
objets linguistiques en se basant sur le balisage XML du document, etc.

3 Analyse du discours

Voyons quels avantages l’analyse automatique du discours peut tirer des principes proposés.
Un premier apport signiﬁcatif résulte de l’approche par enrichissement incrémental et par abs-
traction progressive des formes de surface. S’il est naturel d’opérer au niveau de ces dernieres
pour une analyse par exemple morphologique ou syntaxique, il va sans dire que l’analyse dis-
cursive ne peut s’accomoder de la diversité combinatoire apparaissant a ce niveau, et qu’un
ﬁltrage s’impose. En plus de la possibilité d’opérer la pure et simple occultation d’éléments peu
pertinents pour tel ou tel besoin interprétatif, la plate-forme permet d’opérer ladite abstraction
de deux manieres complémentaires. En premier lieu, l’unicité du modele de marquage et d’an-
notation donne a chaque étape d’analyse l’acces aux représentations symboliques produites en
amont, et permet ainsi de ramener la diversité combinatoire de surface a celle des valeurs in-
terprétées, généralement moins nombreuses. En second lieu, le principe de variabilité du grain
d’analyse déja évoqué permet d’exploiter au niveau discursif des modeles d’analyse habituel-
lement dédiés a des niveaux de granularité inférieurs. Par exemple, des regles EDCG pourront
aussi bien décrire des patrons syntagmatiques qu’une grammaire textuelle, selon le grain choisi.

Par ailleurs, la plate-forme propose des modeles d’analyse spéciﬁquement adaptés au niveau
discursif. Le langage DSDL en particulier, s’écarte des paradigmes généralement adoptés par
les autres formalismes (y compris MRE ou EDCG), qui reposent fondamentalement sur des prin-
cipes de line’arite’ (on tient compte de tous les éléments successifs) et de séquentialite’ (un ordre
est imposé), principes souvent inadaptés au niveau discursif. En permettant l’expression de

La plate-forme LinguaStream

contraintes non séquentielles et non linéaires, le formalisme DSDL autorise l’expression et la
detection de motifs pouvant porter sur des elements distants dans le texte, sans faire d’hypothese
sur leur ordre, ce qui s’avere particulierement adapte a l’analyse du discours.

Aﬁn de donner une idee plus concrete des principes methodologiques presentes, envisageons a
present une conﬁguration linguistique particuliere, assez representative des problemes poses par
l’analyse discursive, en abordant le probleme de l’encadrement du discours (Charolles, 1997),
et plus particulierement de la detection automatique des cadres temporels. Rappelons que cette
theorie qualiﬁe ainsi des segments textuels homogenes du point de vue d’un critere d’interpre-
tation ﬁxe dans une expression en position detachee en debut de phrase, dite introducteur de
cadre. L’ operationnalisation en TAL de ce modele psycho-linguistique impose la resolution de
deux problemes principaux : detection des introducteurs, puis evaluation de leur portée, c’est-
a-dire determination de la borne droite du cadre introduit. Bien que cette derniere tache soit
tres problematique dans la mesure ou les criteres formels de cloture des cadres sont difﬁciles a
etablir, un certain nombre d’indices ont toutefois pu etre degages dans le cas precis des cadres
temporels (Bilhaut et al., 2003), que nous evoquerons ci-apres.

Le probleme de la detection des introducteurs temporels se decline lui-meme en deux sous-
problemes : l’analyse des expressions temporelles, et celle des introducteurs s’appuyant sur
elles. Les principes de modularite evoques trouvent ici leur justiﬁcation, puisque nous souhai-
terons generalement traiter ces problemes independamment. L’ analyse semantique des expres-
sions temporelles fait l’objet d’une grammaire EDCG, exprimant des contraintes sur les resultats
d’une analyse morpho-syntaxique preliminaire, et associant aux expressions reconnues une re-
presentation de leur « sens » sous forme de structures de traits. Sur cette base, la detection
des introducteurs peut etre Inise en place a l’aide de criteres essentiellement positionnels. Les
contraintes exprimees sont fondamentalement sequentielles : nous recherchons des zones de
texte veriﬁant des motifs imposant la presence, dans un ordre ﬁxe, d’elements immediatement
successifs. Ces regles sont donc simplement exprimables a l’aide du formalisme MRE (outre
les expressions temporelles, nous exploitons ici le marquage des phrases et des connecteurs de
discours) :

{type : phrase, anchor : start}

<introducteur>

{type : connecteur}? {tag : pre} {type : temporel} /as $t
</introducteur> /sem {axe : temps, valeur : $t} ”,”

Les contraintes sur les structures de traits produites en amont (ici en gras), ainsi que sur les
formes de surface (ici, la virgule en ﬁn de motif) permettent de delimiter l’introducteur. Nous
recherchons les elements precedes d’un debut de phrase et composes, d’un eventuel connecteur
de discours, d’une preposition et d’une expression temporelle. Le reste de l’expression corres-
pond au marquage et a l’annotation produits en sortie. L’ element reconnu aura le type « intro-
ducteur » et sera associe a l’annotation semantique qui lui fait suite. Precisons que la variable
$t permet de faire « remonter » l’information contenue dans la structure de traits associee a
l’expression temporelle, pour un usage ulterieur.

Pour la determination de la portee de l’introducteur, la methode presentee dans (Bilhaut et al.,
2003) s’appuie sur des criteres enonciatifs tels que la cohesion des temps verbaux, sur la struc-
turation en paragraphes, et sur des calculs semantiques de coherence entre l’introducteur et les
autres expressions temporelles. La nature de ces contraintes differe radicalement des prece-
dentes. D’une part, nous pouvons desormais nous abstraire de la linearite du texte : contraire-
ment a une approche par expressions regulieres, nous pouvons ici ignorer un certain nombre
d’elements du ﬂot textuel. D’autre part, s’il existe bien des contraintes interpretatives entre
l’introducteur et certains elements de la zone introduite, il n’est pas souhaitable de concevoir
ces contraintes comme imposant un ordre strict entre ces elements. Pour l’expression de telles
contraintes a la fois non linéaires et non séquentielles, nous disposons du formalisme DSDL

WIDLOCHER Antoine, BILHAUT Frédérik

et pouvons formuler la « grammaire » ci-dessous. Nous recherchons une unité textuelle com-
posée de phrases completes, commencant par un élément identiﬁé comme introducteur et ne
comportant pas d’autre élément de ce type, dont tous le verbes sont au méme temps, et au sein
de laquelle les expressions temporelles portent sur une plage comprise dans l’intervalle ﬁxé par
l’introducteur, en ne retenant que le plus long des candidats partageant un meme introducteur.

Rule {type : ”cadre”} :

start({type 2 "introducteur"})

end({type 2 "phrase"})

homogeneity(comparator : portée)

not presence(pattern : {type 2 "intro"}, amount : 2)
size(mode : #LONGEST)

Comparator portée ({type 2 "verbe"} as $vl, {type 2 "verbe"} as $v2)
$vl/temps = $v2/temps

Comparator portée ({type 2 "intro"} as $i, {type 2 "tempo"} as $t) :
($t/debut >= $i/debut) and ($t/fin <= $i/fin)

Il est ainsi possible, a l’aide des principes méthodologiques promus par la plate-forme, et en
nous appuyant sur la complémentarité des modeles d’analyse, de mettre en place un analyseur
de cadres temporels, certes encore imparfait, mais ne faisant usage que de formalismes pure-
ment déclaratifs propices a la capitalisation de l’expertise linguistique mise en oeuvre.

4 Conclusion

Initialement développée dans le cadre du projet GeoSem2, la plate-forme évolue maintenant
indépendamment. Elle est aujourd’hui utilisée dans le cadre d’un projet TCAN3, de différents
travaux de recherche en TAL, notaInment en analyse sémantique du discours 2 organisation
thématique (Bilhaut, 2004), ou rhétorique (Widlocher, 2004). Le logiciel est également utilisé a
des ﬁns pédagogiques au GREYC eta l’ERSS, eta été mis a la disposition de laboratoires tels que
LIUPPA ou LATTICE. La plate-forme reste en elle-meme indépendante des modeles d’analyse
utilisés, pour peu qu’ils partagent le méme systeme de marquage et d’annotation, et il est donc
envisageable d’intégrer des modules exploitant d’autres modeles d’analyse.

Références

BILHAUT F. (2004). Analyse automatique de la structure thématique du discours pour la navigation
documentaire. In Journée ATALA « Modéliser et de’crire l ’organisation discursive 61 l ’heure du document
numérique ».

BILHAUT F., H0-DAC M., BORILLO A., CHARNOIS T., ENJALBERT P., DRAOULEC A. L., MATHET
Y., MIGUET H., PERY-WOODLEY M.-P. & SARDA L. (2003). Indexation discursive pour la navi-
gation intradocumentaire 2 cadres temporels et spatiaux dans l’information géographique. In Actes de
Traitement Automatique du Lcmgage Naturel (TALN), Batz-sur-Mer, France.

BILHAUT F. & WIDLOCHER A. (2005). La plate-forme LinguaStream. In Joumée ATAIA « Architec-
tures logicielles pour articuler les traitements sur corpus ».

CHAROLLES M. (1997). L’encadrement du discours 2 Univers, champs, domaines et espaces. Cahier de
Recherche Linguistique no 6. Université de Nancy 2.

WIDLOCHER A. (2004). Analyse macro-sémantique 2 vers une analyse rhétorique du discours. In Actes
de RECITAL 2004, p. 183-188, Fés, Maroc.

2« Traitement sémantique de l’information géographique », programme CNRS << Société de l’information ».
3« Intervalles temporels et applications a la linguistique textuelle », projet interdisciplinajre du CNRS.

