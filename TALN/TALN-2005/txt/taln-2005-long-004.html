<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
"http://www.w3.org/TR/html4/loose.dtd">
<html><head><title>Indexation S&#233;mantique par Coupes de Redondance Minimale dans une Ontologie</title>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
</head>
<body>
<div style="page-break-before:always; page-break-after:always"><div><p>TALN 2005, Dourdan, 6&#8211;10 juin 2005
</p>
<p>Indexation s&#233;mantique au moyen de coupes
de redondance minimale dans une ontologie
</p>
<p>Florian Seydoux &amp; Jean-C&#233;dric Chappelier
Facult&#233; Informatique et Communications
</p>
<p>Ecole Polytechnique F&#233;d&#233;rale de Lausanne (EPFL)
CH&#8211;1015 Lausanne, Suisse
</p>
<p>{florian.seydoux,jean-cedric.chappelier}@epfl.ch
</p>
<p>Mots-clefs : Indexation s&#233;mantique, Recherche documentaire, Redondance minimale,
Ontologie.
</p>
<p>Keywords: Semantic Indexing, Information Retrieval, Minimal Redundancy, Ontology.
R&#233;sum&#233; Plusieurs travaux ant&#233;rieurs ont fait &#233;tat de l&#8217;am&#233;lioration possible des perfor-
mances des syst&#232;mes de recherche documentaire grace &#224; l&#8217;utilisation d&#8217;indexation s&#233;mantique
utilisant une ontologie (p.ex. WordNet). La pr&#233;sente contribution d&#233;crit une nouvelle m&#233;thode
visant &#224; r&#233;duire le nombre de termes d&#8217;indexation utilis&#233;s dans une indexation s&#233;mantique, en
cherchant la coupe de redondance minimale dans la hi&#233;rarchie fournie par l&#8217;ontologie. Les r&#233;-
sultats, obtenus sur diverses collections de documents en utilisant le dictionnaire EDR, sont
pr&#233;sent&#233;s.
</p>
<p>Abstract Several former works have shown that it is possible to improve information
retrieval performances using semantic indexing, adding additional information coming from a
thesaurus (e.g. WordNet). This paper presents a new method to reduce the number of &quot;concepts&quot;
used to index the documents, by determining a minimum redundancy cut in the hierarchy pro-
vided by the thesaurus. The results of experiments carried out on several standard document
collections using the EDR thesaurus are presented.
</p>
<p>1 Introduction
</p>
<p>L&#8217;utilisation de connaissances s&#233;mantiques dans le cadre de la Recherche Documentaire (RD)
n&#8217;est pas nouvelle. On voit se d&#233;gager dans la litt&#233;rature scientifique principalement trois champs
d&#8217;application : l&#8217;expansion de requ&#234;tes (Voorhees, 1994; Moldovan &amp; Mihalcea, 2000), la d&#233;sam-
bigu&#239;sation s&#233;mantique (WSD) (Ide &amp; V&#233;ronis, 1998; Wilks &amp; Stevenson, 1998) et l&#8217;indexation
s&#233;mantique. C&#8217;est dans ce dernier cadre que se situe le travail pr&#233;sent&#233; ici.
</p>
<p>L&#8217;indexation s&#233;mantique consiste &#224; utiliser, pour indexer des documents, le(s) sens des mots
qu&#8217;ils contiennent, au lieu ou en plus des mots1 eux-m&#234;mes comme c&#8217;est le cas en RD classique,
</p>
<p>Ce travail a &#233;t&#233; financ&#233; par le projet n&#9702;200020&#8211;103529 du Fond National Suisse pour la Recherche Scientifique.
1 Habituellement, leurs lemmes ou leurs racines (stems).</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Seydoux F. &amp; Chappelier J.-C.
</p>
<p>de mani&#232;re &#224; am&#233;liorer tant le rappel (par le biais des relations de synonymie) que la pr&#233;cision
(en traitant correctement les cas d&#8217;homographie/polys&#233;mie).
Les diff&#233;rentes exp&#233;riences rapport&#233;es &#224; ce sujet dans la litt&#233;rature font cependant &#233;tat de r&#233;-
sultats peu concluants, parfois m&#234;me contradictoires : si certains observent que l&#8217;ajout de ce
type d&#8217;information, r&#233;alis&#233;e de mani&#232;re automatique, d&#233;grade les performances de leur syst&#232;me
(Salton, 1968; Harman, 1988; Voorhees, 1993; Voorhees, 1998), pour d&#8217;autres au contraire une
am&#233;lioration significative est obtenue (Richardson &amp; Smeaton, 1995; Smeaton &amp; Quigley, 1996;
Gonzalo et al., 1998a; Gonzalo et al., 1998b; Mihalcea &amp; Moldovan, 2000).
Bien qu&#8217;il semble souhaitable pour un syst&#232;me de RD de prendre en compte un maximum
d&#8217;informations, en particulier des informations de nature s&#233;mantique, un tel accroissement des
termes d&#8217;indexation peut se r&#233;v&#233;ler contre-productif, ou tout du moins ne pas d&#233;velopper son
plein potentiel. En effet, une forte augmentation du nombre de termes d&#8217;indexation a non seule-
ment comme cons&#233;quences de prolonger notablement les temps de traitement, mais surtout
affecte les performances sur le plan de la pr&#233;cision : tenter de discriminer quelques documents
parmi un ensemble sur la base d&#8217;un tr&#232;s grand nombre de crit&#232;res est difficile &#224; r&#233;aliser, la &#171; dis-
tance &#187; &#8211; g&#233;n&#233;ralement une similarit&#233; ou une dissemblance &#8211; entre chaque paire de documents
tendant &#224; devenir &#224; peu pr&#232;s la m&#234;me (effet &#171; curse of dimensionality &#187;).
Ce probl&#232;me n&#8217;est pas nouveau et il existe d&#233;j&#224; un certain nombre de techniques visant &#224; limiter
la taille du jeu d&#8217;indexation : en plus de celles proc&#233;dant par filtrage (en utilisant par exemple
un anti-dictionnaire (stoplist), la cat&#233;gorie morpho-syntaxique, ou encore les fr&#233;quences d&#8217;oc-
currence), la limitation du nombre de termes d&#8217;indexation a aussi &#233;t&#233; envisag&#233;e au moyen de
techniques statistiques issues de l&#8217;analyse des donn&#233;es (analyse en composantes principales,
analyse factorielle discriminante) (Deerwester et al., 1990; Hofmann, 1999). Cependant, la plu-
part de ces techniques ne sont pas n&#233;cessairement adapt&#233;es lorsque l&#8217;on est en pr&#233;sence d&#8217;in-
formations suppl&#233;mentaires sur les termes d&#8217;indexation ayant une structure formelle (au lieu de
statistique). L&#8217;objectif des travaux pr&#233;sent&#233;s dans cette contribution est pr&#233;cis&#233;ment d&#8217;utiliser
une ressource s&#233;mantique externe (i.e. additionnelle aux donn&#233;es de recherche documentaire
proprement dites) structur&#233;e, de type ontologie, en vue d&#8217;augmenter la richesse de l&#8217;indexation.
La sp&#233;cificit&#233; de ce travail par rapport &#224; des travaux ant&#233;rieurs similaires, qui utilisent des &#171; syn-
sets &#187; ou des hyperonymes de WordNet comme termes d&#8217;indexation (Gonzalo et al., 1998a;
Gonzalo et al., 1998b; Whaley, 1999; Mihalcea &amp; Moldovan, 2000), est d&#8217;essayer de faire
un pas suppl&#233;mentaire en s&#233;lectionnant les &#171; concepts &#187; &#224; utiliser comme termes d&#8217;indexation
au moyen d&#8217;un crit&#232;re issu de la th&#233;orie de l&#8217;information, la Coupe de Redondance Minimale
(CRM, voir figure 1), que l&#8217;on applique &#224; la relation inclusive &#171; est-un &#187; (hyperonymie) obtenue
ici par le biais de la taxonomie (anglaise) EDR (Miyoshi et al., 1996).
</p>
<p>2 Coupe de redondance minimale
</p>
<p>2.1 Objectifs
Le choix du &#171; concept hyperonyme &#187;2 &#224; utiliser pour repr&#233;senter un mot est un choix d&#233;licat : un
concept trop g&#233;n&#233;ral d&#233;gradera les performances du syst&#232;me en diminuant la pr&#233;cision, tandis
</p>
<p>2 Nous d&#233;signons par &#171; concept hyperonyme&#187; un n&#339;ud non feuille dans l&#8217;ontologie. Les feuilles de l&#8217;ontologie
repr&#233;sentent les mots.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Indexation s&#233;mantique par coupes de redondance minimale
</p>
<p>c1
</p>
<p>wi
</p>
<p>wi c1c2
</p>
<p>c3
</p>
<p>c2
</p>
<p>c3
</p>
<p>c1
</p>
<p>wi wk
</p>
<p>wj
</p>
<p>b)
</p>
<p>a)
</p>
<p>...
</p>
<p>c)
</p>
<p>d)
</p>
<p>r
</p>
<p>CRM
</p>
<p>... ...r
</p>
<p>FIG. 1 &#8211; Diff&#233;rentes m&#233;thodes d&#8217;indexation : (a) traditionnelle, au moyen des mots, racines
(stems) ou lemmes ; (b) utilisant une ontologie s&#233;mantique (illustration de droite), chaque terme
d&#8217;indexation de (a) est augment&#233; par tout ou partie des &#171; concepts &#187; le recouvrant ; cela conduit
&#224; une explosion du nombre de termes d&#8217;indexation ; (c) indexation par les concepts de plus bas
niveau (&#8776; indexation par &#171; synsets &#187;) : chaque terme d&#8217;indexation est remplac&#233; par son concept
hyperonyme direct, factorisant ainsi tous les mots domin&#233;s par ce concept ; on r&#233;duit donc le
nombre de termes d&#8217;indexation, tout en permettant de d&#233;tecter la similarit&#233; entre documents
contenant ces mots ; (d) indexation par une Coupe de Redondance Minimale (CRM) : chaque
terme d&#8217;indexation est remplac&#233; par l&#8217;un de ses concepts hyperonymes, d&#233;termin&#233; par la CRM.
Cela restreint d&#8217;avantage le nombre de termes d&#8217;indexation, le nombre de mots couverts (facto-
ris&#233;s) par chacun d&#8217;eux &#233;tant plus grand qu&#8217;avec le concept hyperonyme direct.
</p>
<p>qu&#8217;un concept trop sp&#233;cifique ne permettra pas de r&#233;duire significativement le nombre de termes
d&#8217;indexation et conservera la distinction entre mots de sens proches.
</p>
<p>Pour d&#233;terminer le niveau ad&#233;quat des concepts d&#8217;indexation, nous faisons ici le choix de ne
prendre en consid&#233;ration des coupes dans l&#8217;ontologie (une coupe &#233;tant un ensemble minimal3
de n&#339;uds d&#233;finissant une partition sur les feuilles), en consid&#233;rant que chaque n&#339;ud repr&#233;sente
alors l&#8217;ensemble des feuilles qu&#8217;il recouvre.
</p>
<p>Le probl&#232;me est de trouver une strat&#233;gie permettant d&#8217;identifier une coupe &#171; optimale &#187; en un
temps acceptable. Pour une t&#226;che relativement similaire, Li (1998) propose d&#8217;utiliser le cri-
t&#232;re MDL (Minimum Description Length). Si ce crit&#232;re est facilement calculable, il a comme
inconv&#233;nient, du moins lorsque appliqu&#233; &#224; l&#8217;ontologie EDR, de tr&#232;s souvent s&#233;lectionner la ra-
cine de l&#8217;ontologie comme coupe &#171; optimale &#187; ; ce qui n&#8217;est pas vraiment ad&#233;quat pour la t&#226;che
consid&#233;r&#233;e ! Nous nous proposons donc ici d&#8217;employer un autre crit&#232;re, fond&#233; sur la th&#233;orie de
l&#8217;information, permettant d&#8217;identifier une coupe pour laquelle la redondance d&#8217;information est
minimale, c&#8217;est-&#224;-dire une coupe qui &#233;quilibre le plus possible les degr&#233;s de description des
mots factoris&#233;s en tenant compte de la probabilit&#233; d&#8217;occurrence de ces mots.
</p>
<p>2.2 Crit&#232;re de redondance minimale
</p>
<p>SoientN = {ni} l&#8217;ensemble des n&#339;uds (concepts ou mots) etW l&#8217;ensemble des feuilles (mots
uniquement) contenus dans l&#8217;ontologie consid&#233;r&#233;e. On d&#233;finit alors une coupe &#915; comme un
sous-ensemble minimal3 de N recouvrant W . Une coupe probabilis&#233;e M = (&#915;, P ) est une
</p>
<p>3 Par &#171; minimal &#187;, on entend qu&#8217;aucun n&#339;ud de la coupe ne peut en &#234;tre retir&#233; sans en diminuer la couverture.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Seydoux F. &amp; Chappelier J.-C.
</p>
<p>paire compos&#233;e d&#8217;une coupe &#915; et d&#8217;une distribution de probabilit&#233;s P sur &#915;. On notera |&#915;| le
nombre de n&#339;uds de la coupe (et par extension: |M | = |&#915;|).
Dans la suite, nous consid&#233;rons la coupe M = (&#915;, Pf) probabilis&#233;e par les fr&#233;quences d&#8217;oc-
currences des mots correspondant aux feuilles de l&#8217;ontologie : Pf(ni) = f(ni)/|D|, o&#249; f(ni)
repr&#233;sente le nombre d&#8217;occurrences du concept (ou mot) ni dans les donn&#233;es D. Pour calculer
f(ni), on admet qu&#8217;il y a occurrence de ni lorsqu&#8217;il y a occurrence de l&#8217;un des wi &#8712; ni++ mots
hyponymes de ni, o&#249; n++ repr&#233;sente la fermeture transitive de n+, ensemble des successeurs
de n.
</p>
<p>La redondance R(M) d&#8217;une coupe probabilis&#233;e M = (&#915;, P ) est d&#233;finie par (Shannon, 1948):
</p>
<p>R(M) = 1&#8722;
H(M)
</p>
<p>log |M |
, avec H(M) = &#8722;
</p>
<p>&#8721;
n&#8712;&#915;
</p>
<p>P (n) &#183; log P (n).
</p>
<p>Minimiser la redondance revient &#224; maximiser le rapport entre l&#8217;entropie des &#233;l&#233;ments de la
coupe et sa valeur maximale possible (log |M |) ; le but est donc de trouver une coupe probabi-
lis&#233;e M qui maximise le crit&#232;re CH :
</p>
<p>CH =
</p>
<p>{
0 si |M | &#8804; 1,
H(M)
log |M |
</p>
<p>sinon.
</p>
<p>Un tel crit&#232;re pose cependant quelques difficult&#233;s en pratique: d&#8217;une part, il ne permet pas
d&#8217;identifier une coupe optimale unique, mais un ensemble de coupes possibles ; d&#8217;autre part,
l&#8217;optimum local sur une partie de l&#8217;ontologie est conditionn&#233; par l&#8217;optimum sur le reste (et
inversement). Pour identifier les mod&#232;les satisfaisant le crit&#232;re global, il faudrait donc le calculer
pour l&#8217;ensemble des coupes possibles.
</p>
<p>La premi&#232;re difficult&#233; peut &#234;tre surmont&#233;e de mani&#232;re relativement ais&#233;e, par exemple en ne
retenant qu&#8217;une coupe choisie au hasard, ou en favorisant celles admettant le plus de n&#339;uds, ou
encore en guidant le choix selon la profondeur moyenne des n&#339;uds.
</p>
<p>Pour &#234;tre calculable, la seconde difficult&#233; implique par contre de renoncer &#224; l&#8217;optimalit&#233; globale.
N&#233;anmoins, il est possible d&#8217;utiliser un algorithme de programmation dynamique permettant
d&#8217;obtenir une coupe acceptable (heuristique). Cet algorithme consiste &#224; choisir, pour un sous-
arbre4 dans l&#8217;ontologie, une coupe optimale parmi celles constitu&#233;es des successeurs directs
de la racine de ce sous-arbre et les sous-coupes &#171; optimales &#187; de chacun de ces successeurs,
obtenues de mani&#232;re similaire. Plus formellement, l&#8217;algorithme r&#233;cursif donn&#233; en table 1 est
appliqu&#233; &#224; partir de la racine de l&#8217;ontologie5.
</p>
<p>2.3 Exemple
</p>
<p>Pour illustrer le fonctionnement de la technique de s&#233;lection des coupes d&#233;crite pr&#233;c&#233;demment,
admettons que l&#8217;on dispose de l&#8217;ontologie pr&#233;sent&#233;e en figure 2 ; les valeurs indiqu&#233;es en regard
</p>
<p>4 Bien que les ontologies utilis&#233;es pr&#233;sentent usuellement une structure de graphe orient&#233; sans cycle (DAG), nous
simplifierons ici le propos en consid&#233;rant qu&#8217;il s&#8217;agit d&#8217;arbres. Cette approximation, qui n&#8217;invalide en rien les
raisonnements expos&#233;s ici, n&#8217;est &#233;videmment pas faite en pratique.
</p>
<p>5 En pratique, plusieurs optimisations sont introduites (notamment, les successeurs feuilles d&#8217;un n&#339;ud sont n&#233;-
cessairement compris dans la sous-coupe optimale pour ce n&#339;ud) ; mais elles ne changent rien &#224; l&#8217;aspect
fondamental pr&#233;sent&#233; ici.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Indexation s&#233;mantique par coupes de redondance minimale
</p>
<p>ALGORITHME CRM
Entr&#233;e : un n&#339;ud t (dans une hi&#233;rarchie).
Sortie : CRM : une coupe de redondance minimale sous ce n&#339;ud.
</p>
<p>Si t &#8712; W
CRM &#8592; {t}
</p>
<p>Sinon
Pour ni &#8712; t+
</p>
<p>&#947;i &#8592; CRM(ni)
&#977;i &#8592; {ni}
</p>
<p>Pour 1 &#8804; k &#8804; n := |t+|
&#915;k &#8592;
</p>
<p>&#8899;
j&#8712;[1:n\k] &#947;j &#8746; &#977;k
</p>
<p>&#915;n+1 &#8592;
&#8899;
</p>
<p>j&#8712;[1:n] &#947;j
&#915;n+2 &#8592;
</p>
<p>&#8899;
j&#8712;[1:n] &#977;j
</p>
<p>CRM &#8592; Argmax&#915;j :16j6n+2
</p>
<p>(
CH(&#915;j)
</p>
<p>)
o&#249; Argmax retourne une coupe possible r&#233;alisant ce maximum.
</p>
<p>TAB. 1 &#8211; Algorithme de recherche heuristique d&#8217;une CRM.
</p>
<p>des feuilles correspondent aux fr&#233;quences d&#8217;occurrences des mots y-relatifs obtenues sur un
corpus fictif.
</p>
<p>Pour la coupe &#915; = [ANIMAL, PLANTE, TRANSPORT], on obtient la valeur du crit&#232;re CH :
</p>
<p>ni ANIMAL PLANTE TRANSPORT
f(ni) 18 30 1
Pf(ni) 0.3673 0.6122 0.0204
</p>
<p>&#8722;Pf(ni) log2 Pf(ni) 0.5307 0.4334 0.1146
CH(&#915;) =
</p>
<p>1.0787
log
</p>
<p>2
(3)
</p>
<p>= 0.6806
</p>
<p>R(&#915;) = 1&#8722; CH(&#915;) = 0.3194
</p>
<p>Dans un tel cas de figure, en examinant l&#8217;ensemble des 2036 diff&#233;rentes coupes possibles, on
trouverait que le crit&#232;re sur la coupe optimale (indiqu&#233;e sur la figure 2) vaut 0.874. L&#8217;algo-
rithme de recherche par optimum local trouve une coupe pour laquelle le crit&#232;re est l&#233;g&#232;rement
inf&#233;rieur: 0.810 ; mais son obtention ne n&#233;cessite l&#8217;&#233;valuation que de 36 coupes diff&#233;rentes.
</p>
<p>3 Exp&#233;riences
</p>
<p>Nous avons effectu&#233; un jeu d&#8217;exp&#233;riences en utilisant les collections standards ADI, TIME,
MED, CACM et CISI6 du projet SMART (Salton, 1971), ainsi qu&#8217;une ontologie produite &#224; partir
du dictionnaire &#233;lectronique EDR (Miyoshi et al., 1996).
EDR est organis&#233;e en cinq dictionnaires de diff&#233;rents types, plus ou moins ind&#233;pendants les uns
</p>
<p>6 Disponibles &#224; l&#8217;adresse ftp://ftp.cs.cornell.edu/pub/smart/.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Seydoux
F
.&amp;
</p>
<p>ChappelierJ.-C.
</p>
<p>Exemple
</p>
<p>Trouv&#233;e
</p>
<p>Optimale R = 0.31943
</p>
<p>R = 0.18975
</p>
<p>R = 0.12639
</p>
<p>0
fucus diatomos
</p>
<p>0
velo
</p>
<p>1
voilier
</p>
<p>0
voiture
</p>
<p>0
avion
</p>
<p>0
</p>
<p>jacinte
0
</p>
<p>violette
2
</p>
<p>myosotis
3
</p>
<p>cypr&#232;s
2
</p>
<p>sapin
1
</p>
<p>h&#234;tre
0
</p>
<p>loup
1
</p>
<p>lion
5
</p>
<p>mouton
2
</p>
<p>cheval
4
</p>
<p>vache
1
</p>
<p>ornythorynque
2
</p>
<p>perroquet
0
</p>
<p>aigle
0
</p>
<p>corbeau
0
</p>
<p>hirondelle
3
</p>
<p>0
jonquille primev&#232;re
</p>
<p>0
tr&#232;fle
</p>
<p>8
coquelicot
</p>
<p>6
</p>
<p>h&#233;licopt&#232;re
0
</p>
<p>lis mart.
8
</p>
<p>ENTITE
</p>
<p>ANIMAL PLANTE TRANSPORT
</p>
<p>OISEAU MAMMIFERE ARBRE FLEUR ALGUE ECOLOGIQUE POLLUANT
</p>
<p>HERBIVORE CARNIVORE BLEUE ROUGE JAUNE
</p>
<p>F
IG
</p>
<p>.2
&#8211;
</p>
<p>Ex
em
</p>
<p>plede
co
</p>
<p>upesdans
u
</p>
<p>n
e
</p>
<p>o
ntologie.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Indexation s&#233;mantique par coupes de redondance minimale
</p>
<p>des autres. Parmi l&#8217;ensemble de ces dictionnaires, les deux suivants sont utilis&#233;s pour constituer
l&#8217;ontologie:
</p>
<p>le dictionnaire des mots anglais, qui rassemble les informations morphologiques (prononcia-
tion, d&#233;coupage syllabique, inflexion, ...) et syntaxiques (cat&#233;gorie morpho-syntaxique,
d&#233;nombrabilit&#233;, flexions, ...) pour un peu plus de 240&#8242;000 graphies diff&#233;rentes (corres-
pondant &#224; &#8776; 420&#8242;000 mots), et permet de relier ces graphies avec les informations du
dictionnaire des concepts. Les graphies de ce dictionnaire sont principalement (mais pas
exclusivement) des lemmes ; il comporte &#233;galement un nombre important de multi-termes
(&gt; 113&#8242;000), figurant des mots compos&#233;s et expressions idiomatiques.
</p>
<p>le dictionnaire des concepts, qui d&#233;crit &#224; peu pr&#232;s 490&#8242;000 concepts, organis&#233;s hi&#233;rarchique-
ment entre eux selon des relations d&#8217;hyponymie/hyperonymie (chaque concept pouvant
avoir plusieurs hyponymes et hyperonymes). Un certain nombre de relations s&#233;mantiques
binaires suppl&#233;mentaires (telles que objet-action, agent-action, agent-but) sont par ailleurs
d&#233;crites, mais nous ne les utilisons pas ici. Remarquons qu&#8217;un nombre important de
concepts (environ la moiti&#233;) ne sont pas directement associ&#233;s &#224; des mots ; ces concepts
ne peuvent &#234;tre d&#233;finis et appr&#233;hend&#233;s qu&#8217;au travers de leurs relations avec les autres
concepts.
</p>
<p>Le syst&#232;me de RD utilis&#233; est le mod&#232;le vectoriel SMART, combin&#233; &#224; un lemmatiseur externe7,
qui fait &#233;galement office de segmenteur (tokenizer) et d&#8217;&#233;tiqueteur morpho-syntaxique. Un
filtrage par cat&#233;gorie grammaticale est r&#233;alis&#233;s (ne sont conserv&#233;s que les noms, adjectifs et
verbes), mais nous n&#8217;utilisons pas d&#8217;anti-dictionnaire et ne faisons pas de filtrage fr&#233;quentiel.
Les transformations du jeu d&#8217;indexation sont obtenues en pr&#233;traitant les donn&#233;es soumises au
syst&#232;me de RD:
</p>
<p>1. en premier lieu, les diverses informations textuelles (principalement titre et contenu) des
documents sont agr&#233;g&#233;es, et les autres informations (auteurs, sources, etc.) supprim&#233;es ;
documents et requ&#234;tes sont ensuite segment&#233;s et lemmatis&#233;s ;
</p>
<p>2. on cherche ensuite les correspondances entre les mots contenus dans les documents et
ceux d&#233;crits dans l&#8217;ontologie ; on tente d&#8217;&#233;tablir en priorit&#233; une correspondance avec la
graphie, et s&#8217;il n&#8217;y en a pas, avec sa forme lemmatis&#233;e ; les mots sans correspondance sont
index&#233;s de mani&#232;re traditionnelle ; les taux de couverture8 sur les diff&#233;rentes collections
sont de l&#8217;ordre de 90%.
</p>
<p>3. on proc&#232;de ensuite &#224; l&#8217;expansion de la hi&#233;rarchie des concepts relatifs aux mots conserv&#233;s
pour l&#8217;ensemble des documents ; selon les diff&#233;rents cas exp&#233;riment&#233;s, on prendra soit la
totalit&#233; des concepts possibles (en tablant sur un renforcement mutuel des concepts &#171; cor-
rects &#187; induit par les multiples co-occurrences), soit uniquement le concept le plus pro-
bable (dans l&#8217;absolu pour le mot donn&#233; &#8211; cette information est pr&#233;sente dans l&#8217;ontologie
utilis&#233;e) ;
</p>
<p>4. on d&#233;termine ensuite une coupe optimale selon le crit&#232;re CH , au moyen de l&#8217;algorithme
CRM pr&#233;sent&#233; en section 2.2 ;
</p>
<p>5. finalement, on substitue les mots des documents et des requ&#234;tes par les identificateurs des
concepts de la coupe qui les subordonnent.
</p>
<p>7 Le syst&#232;me Sylex 1.7 ( c&#169; 1993-98 DECAN INGENIA).
8 Par &#171; couverture &#187;, on d&#233;signe la fraction des occurrences des mots couverts par l&#8217;ontologie.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Seydoux F. &amp; Chappelier J.-C.
</p>
<p>mesure (a) (b) (c) (d)
corpus ADI (82 documents)
</p>
<p>tous les concepts, tf.idf
taille index 1800 14748 10099 1292
pr&#233;cision 0.3578 0.3134 0.3356 0.2458
rappel 0.6984 0.7126 0.7406 0.6017
</p>
<p>tous les concepts, sans
pond&#233;ration
</p>
<p>pr&#233;cision 0.2497 0.1219 0.2550 0.1607
rappel 0.5996 0.3452 0.6708 0.5130
</p>
<p>concept le plus
probable, tf.idf
</p>
<p>taille index 1800 5255 2888 658
pr&#233;cision 0.3578 0.4060 0.4274 0.2052
rappel 0.6984 0.7306 0.7217 0.5200
</p>
<p>concept + probable,
sans pond&#233;ration
</p>
<p>pr&#233;cision 0.2497 0.1376 0.2939 0.1466
rappel 0.5996 0.3727 0.7141 0.4911
</p>
<p>corpus TIME (423 documents)
</p>
<p>tous les concepts, tf.idf
taille index 21815 93707 70091 6760
pr&#233;cision 0.5496 0.4231 0.4536 0.2683
rappel 0.8901 0.7642 0.8036 0.6026
</p>
<p>tous les concepts, sans
pond&#233;ration
</p>
<p>pr&#233;cision 0.3288 0.0337 0.2353 0.0370
rappel 0.7755 0.1021 0.5709 0.1387
</p>
<p>concept le plus
probable, tf.idf
</p>
<p>taille index 21815 53140 31612 4814
pr&#233;cision 0.5496 0.5143 0.5565 0.2729
rappel 0.8901 0.8760 0.9053 0.5162
</p>
<p>concept + probable,
sans pond&#233;ration
</p>
<p>pr&#233;cision 0.3288 0.0346 0.3692 0.0372
rappel 0.7755 0.1201 0.7590 0.1322
</p>
<p>corpus MED (1033 documents)
</p>
<p>tous les concepts, tf.idf
taille index 11893 51712 38524 4078
pr&#233;cision 0.4607 0.3029 0.2996 0.2336
rappel 0.5547 0.3903 0.3794 0.3142
</p>
<p>tous les concepts, sans
pond&#233;ration
</p>
<p>pr&#233;cision 0.3623 0.0105 0.1905 0.0229
rappel 0.4574 0.0246 0.2749 0.0513
</p>
<p>concept le plus
probable, tf.idf
</p>
<p>taille index 11893 30284 18109 2888
pr&#233;cision 0.4607 0.4266 0.4518 0.0743
rappel 0.5547 0.5169 0.5404 0.1042
</p>
<p>concept + probable,
sans pond&#233;ration
</p>
<p>pr&#233;cision 0.3623 0.0105 0.3229 0.0132
rappel 0.4574 0.0313 0.4230 0.0368
</p>
<p>corpus CISI (1460 documents)
</p>
<p>tous les concepts, tf.idf
taille index 10019 53453 39544 3516
pr&#233;cision 0.1733 0.1043 0.1139 0.0740
rappel 0.2318 0.1627 0.1675 0.1294
</p>
<p>tous les concepts, sans
pond&#233;ration
</p>
<p>pr&#233;cision 0.0687 0.0232 0.0569 0.0282
rappel 0.1239 0.0376 0.0963 0.0492
</p>
<p>concept le plus
probable, tf.idf
</p>
<p>taille index 10019 26246 14993 1894
pr&#233;cision 0.1733 0.1590 0.1825 0.0602
rappel 0.2318 0.2131 0.2313 0.0895
</p>
<p>concept + probable,
sans pond&#233;ration
</p>
<p>pr&#233;cision 0.0687 0.0201 0.0805 0.0221
rappel 0.1239 0.0403 0.1300 0.0435
</p>
<p>corpus CACM (3204 documents)
</p>
<p>tous les concepts, tf.idf
taille index 10053 51712 38524 4078
pr&#233;cision 0.2865 0.1293 0.1935 0.1089
rappel 0.4534 0.2579 0.3617 0.1999
</p>
<p>tous les concepts, sans
pond&#233;ration
</p>
<p>pr&#233;cision 0.1555 0.0133 0.1447 0.0320
rappel 0.3082 0.0306 0.2549 0.0699
</p>
<p>concept le plus
probable, tf.idf
</p>
<p>taille index 10053 25207 14681 2670
pr&#233;cision 0.2865 0.2358 0.2804 0.0645
rappel 0.4534 0.3834 0.4567 0.1090
</p>
<p>concept + probable,
sans pond&#233;ration
</p>
<p>pr&#233;cision 0.1555 0.0230 0.1472 0.0245
rappel 0.3082 0.0302 0.2926 0.0385
</p>
<p>TAB. 2 &#8211; R&#233;sultats des diff&#233;rentes exp&#233;riences sur diff&#233;rents corpus. (a) : mots uniquement ; (b) :
mots + concepts ; (c) : hyperonymes directs et (d) : hyperonymes dans CRM (cf aussi fig. 1).</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Indexation s&#233;mantique par coupes de redondance minimale
</p>
<p>On trouvera dans la table 2 les valeurs de pr&#233;cision (&#171; 11-pt prec &#187;) et de rappel (&#171; 30 doc &#187;)9
fournies par le syst&#232;me SMART. Toutes les exp&#233;riences sont par ailleurs conduites en utilisant
soit le sch&#233;ma de pond&#233;ration classique (&#171; tf.idf &#187;), soit sans pond&#233;ration.
On constate que l&#8217;indexation par hyperonymes directs obtient des r&#233;sultats sensiblement &#233;gaux
au syst&#232;me de base, mais pour un rappel plus &#233;lev&#233;. L&#8217;indexation par CRM d&#233;grade par contre
les performances.
</p>
<p>4 Conclusion
</p>
<p>Les r&#233;sultats obtenus sur ces exp&#233;riences ne sont malheureusement pas concluants quant &#224; l&#8217;uti-
lisation du crit&#232;re CRM pour l&#8217;indexation s&#233;mantique. Cependant, plusieurs remarques sont &#224;
apporter :
</p>
<p>&#8211; Le crit&#232;re utilis&#233; ici ne permet pas de s&#233;lectionner, ni m&#234;me d&#8217;influencer, le niveau de profon-
deur dans l&#8217;ontologie de la coupe obtenue. Au vu de la r&#233;duction drastique du jeu d&#8217;indexation
et des mauvaises performances obtenues, il semble que ce crit&#232;re, ou du moins l&#8217;heuristique
impl&#233;ment&#233;e, s&#233;lectionne une coupe situ&#233;e trop haut dans la hi&#233;rarchie, ce qui a comme
cons&#233;quence &#233;vidente de faire baisser la pr&#233;cision. La bonne performance de la coupe au ni-
veau des concept hyperonymes directs nous permet de croire qu&#8217;il doit y avoir un niveau plus
adapt&#233;, plus proche des feuilles, pour la CRM.
On pourrait par exemple limiter consid&#233;rablement l&#8217;espace de recherche de la coupe id&#233;ale
en emp&#234;chant de consid&#233;rer des n&#339;uds situ&#233;s &#171; trop hauts &#187; dans la hi&#233;rarchie. Une piste &#224;
explorer pour am&#233;liorer tant l&#8217;ad&#233;quation de la coupe s&#233;lectionn&#233;e avec un processus d&#8217;in-
dexation que la recherche de cette coupe elle-m&#234;me consisterait &#224; explorer les gains possibles
en terme de redondance &#224; partir de la coupe uniquement constitu&#233;e de feuilles, et en dirigeant
la recherche vers le haut de la hi&#233;rarchie, plut&#244;t que de haut en bas &#224; partir de la racine,
comme dans l&#8217;heuristique pr&#233;sent&#233;e ici.
</p>
<p>&#8211; Par ailleurs, en conservant l&#8217;id&#233;e d&#8217;une action sur le jeu d&#8217;indexation lui-m&#234;me, il serait in-
t&#233;ressant d&#8217;examiner de quelle mani&#232;re les pond&#233;rations (e.g. &#171; tf.idf &#187;), utilis&#233;es uniquement
lors de la recherche des documents proprement dite, devraient &#234;tre prises en compte lors de
la d&#233;termination de la coupe.
</p>
<p>&#8211; Finalement, les r&#233;sultats pr&#233;sent&#233;s ici restent &#224; corroborer avec ceux &#224; obtenir avec d&#8217;autres
ontologies, en particulier WordNet, qui a une structure assez diff&#233;rente d&#8217;EDR.
</p>
<p>Pour terminer, soulignons que l&#8217;int&#233;r&#234;t de la technique pr&#233;sent&#233;e d&#233;passe le cadre de la stricte
recherche documentaire. Celle-ci pourrait en effet s&#8217;av&#233;rer utile, et peut &#234;tre m&#234;me plus promet-
teuse, pour d&#8217;autres domaines d&#8217;application tels que la classification de documents ou le r&#233;sum&#233;
automatique.
</p>
<p>R&#233;f&#233;rences
DEERWESTER S. C., DUMAIS S. T., LANDAUER T. K., FURNAS G. W. &amp; HARSHMAN R. A. (1990).
Indexing by latent semantic analysis. Journal of the American Society of Information Science, 41(6),
</p>
<p>9 Il s&#8217;agit l&#224; de mesures standard: la &#171; 11-pt precision &#187; est la moyenne des pr&#233;cisions pour les taux de rappels
0.0, 0.1, . . . , 1.0, o&#249; la pr&#233;cision au taux de rappel 0.0 est la pr&#233;cision maximale obtenue sur l&#8217;ensemble des
documents pertinents retrouv&#233;s ; le &#171; rappel 30 doc &#187; est le taux de rappel apr&#232;s 30 documents retourn&#233;s.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Seydoux F. &amp; Chappelier J.-C.
</p>
<p>391&#8211;407.
GONZALO J., VERDEJO F., CHUGUR I. &amp; CIGARRAN J. (1998a). Indexing with WordNet synsets can
improve text retrieval. In Proc. of the COLING/ACL 1998 Workshop on Usage of WordNet for Natural
Language Processing, p. 38&#8211;44.
GONZALO J., VERDEJO F., PETERS C. &amp; CALZOLARI N. (1998b). Applying EuroWordNet to multi-
lingual text retrieval. Journal of Computers and the Humanities, 32(2-3), 185&#8211;207.
HARMAN D. (1988). Towards interactive query expansion. In Proc. of the 11th Annual Int. ACM-SIGIR
Conference on Research and development in information retrieval, p. 321&#8211;331.
HOFMANN T. (1999). Probabilistic latent semantic indexing. In proc. of the 22th International Confe-
rence on Research and Development in Information Retrieval (SIGIR), p. 50&#8211;57.
IDE N. &amp; V&#201;RONIS J. (1998). Word sense disambiguation: The state of the art. Computational Linguis-
tics, 24(1), 1&#8211;40.
LI H. (1998). A probabilistic approach to lexical semantic knowledge acquisition and structural disam-
biguation. Master&#8217;s thesis, Graduate School of Science, University of Tokyo.
MIHALCEA R. &amp; MOLDOVAN D. (2000). Semantic indexing using WordNet senses. In Proc. of ACL
Workshop on IR &amp; NLP.
MIYOSHI H., AMD M. KOBAYASHI K. S. &amp; OGINO T. (1996). An overview of the EDR electronic
dictionary and the current status of its utilization. In Proc. of COLING, p. 1090&#8211;1093.
MOLDOVAN D. I. &amp; MIHALCEA R. (2000). Using wordnet and lexical operators to improve internet
searches. IEEE Internet Computing, 4(1), 34&#8211;43.
RICHARDSON R. &amp; SMEATON A. F. (1995). Using WordNet in a Knowledge-Based Approach to Infor-
mation Retrieval. Rapport interne CA-0395, Dublin City University, Glasnevin, Dublin 9, Ireland.
SALTON G. (1968). Automatic Information Organization and Retrieval. McGraw-Hill.
SALTON G. (1971). The SMART Retrieval System &#8211; Experiments in Automatic Document Processing.
Prentice Hall.
SHANNON C. E. (1948). A mathematical theory of communication. The Bell System Technical Journal,
27, 379&#8211;423.
SMEATON A. F. &amp; QUIGLEY I. (1996). Experiments on using semantic distances between words in
image caption retrieval. In Proc. of 19th Int. Conf. on Research and Development in Information Retrie-
val, p. 174&#8211;180.
VOORHEES E. M. (1993). Using WordNet to disambiguate word senses for text retrieval. In Proc.
of 16th Annual International ACM-SIGIR Conference on Research and Development in Information
Retrieval, p. 171&#8211;80.
VOORHEES E. M. (1994). Query expansion using lexical-semantic relations. In Proc. 17th Annual Int.
ACM-SIGIR Conf. on Research and Development in Information Retrieval, p. 61&#8211;69.
VOORHEES E. M. (1998). Using WordNet for text retrieval. In C. FELLBAUM, Ed., WordNet: An
Electronic Lexical Database, chapter 12, p. 285&#8211;303. MIT Press.
WHALEY J. M. (1999). An Application of Word Sense Disambiguation to Information Retrieval. Rap-
port interne PCS-TR99-352, Dartmouth College, Computer Science, Hanover, NH.
WILKS Y. &amp; STEVENSON M. (1998). Word sense disambiguation using optimised combinations of
knowledge sources. In Proc. of the 17th Int. Conf. on Computational Linguistics, p. 1398&#8211;1402.</p>

</div></div>
</body></html>