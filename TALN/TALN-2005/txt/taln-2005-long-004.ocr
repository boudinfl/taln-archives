TALN 2005, Dourdan, 6-10 juin 2005

Indexation sémantique au moyen de coupes
de redondance minimale dans une ontologie

Florian Seydoux & Jean-Cédric Chappelier
Faculté Informatique et Communications
Ecole Polytechnique Fédérale de Lausanne (EPFL)
CH—1015 Lausanne, Suisse

{florian.seydoux,jean—cedric.chappelier}@epfl.ch

Mots-clefs I Indexation sémantique, Recherche documentaire, Redondance minimale,
Ontologie.

Keywords: Semantic Indexing, Information Retrieval, Minimal Redundancy, Ontology.

Résumé Plusieurs travaux antérieurs ont fait état de l’amélioration possible des perfor-
mances des systemes de recherche documentaire grace a l’utilisation d’indexation sémantique
utilisant une ontologie (p.ex. WordNet). La présente contribution décrit une nouvelle méthode
visant a réduire le nombre de termes d’indexation utilises dans une indexation sémantique, en
cherchant la coupe de redondance minimale dans la hiérarchie fournie par l’ontologie. Les re-
sultats, obtenus sur diverses collections de documents en utilisant le dictionnaire EDR, sont
présentés.

Abstract Several former works have shown that it is possible to improve information
retrieval performances using semantic indexing, adding additional information coming from a
thesaurus (e. g. WordNet). This paper presents a new method to reduce the number of "concepts"
used to index the documents, by determining a minimum redundancy cut in the hierarchy pro-
vided by the thesaurus. The results of experiments carried out on several standard document
collections using the EDR thesaurus are presented.

1 Introduction

L’ utilisation de connaissances sémantiques dans le cadre de la Recherche Documentaire (RD)
11’ est pas nouvelle. On voit se dégager dans la littérature scientiﬁque principalement trois champs
d’ application : l’expansi0n de requétes (Voorhees, 1994; Moldovan & Mihalcea, 2000), la de’sam-
biguisation sémantique (WSD) (Ide & Véronis, 1998; Wilks & Stevenson, 1998) et l ’indexati0n
sémantique. C’est dans ce dernier cadre que se situe le travail présente ici.

L’indexation sémantique consiste a utiliser, pour indexer des documents, le(s) sens des mots
qu’ils contiennent, au lieu ou en plus des motsl eux-memes comme c’est le cas en RD classique,

Ce travail a été ﬁnancé par le proj et n°200020—103529 du Fond National Suisse pour la Recherche Scientiﬁque.

1 Habituellement, leurs lemmes ou leurs racines (stems).

Seydoux F. & Chappelier J .-C.

de maniere a améliorer tant le rappel (par le biais des relations de synonymie) que la précision
(en traitant correctement les cas d’homographie/polysémie).

Les différentes expériences rapportées a ce sujet dans la littérature font cependant état de ré-
sultats peu concluants, parfois meme contradictoires: si certains observent que l’ajout de ce
type d’information, réalisée de maniere automatique, dégrade les performances de leur systeme
(Salton, 1968; Harman, 1988; Voorhees, 1993; Voorhees, 1998), pour d’autres au contraire une
amélioration signiﬁcative est obtenue (Richardson & Smeaton, 1995; Smeaton & Quigley, 1996;
Gonzalo et al., 1998a; Gonzalo et al., 1998b; Mihalcea & Moldovan, 2000).

Bien qu’il semble souhaitable pour un systeme de RD de prendre en compte un maximum
d’informations, en particulier des informations de nature sémantique, un tel accroissement des
termes d’indexation peut se révéler contre-productif, ou tout du moins ne pas développer son
plein potentiel. En effet, une forte augmentation du nombre de termes d’indexation a non seule-
ment comme conséquences de prolonger notablement les temps de traitement, mais surtout
affecte les performances sur le plan de la précision: tenter de discriIr1iner quelques documents
parmi un ensemble sur la base d’un tres grand nombre de criteres est difﬁcile a réaliser, la « dis-
tance » — généralement une similarité ou une dissemblance — entre chaque paire de documents
tendant a devenir a peu pres la méme (effet « curse of dimensionality »).

Ce probleme n’est pas nouveau et il existe déja un certain nombre de techniques visant a limiter
la taille du jeu d’indexation: en plus de celles procédant par ﬁltrage (en utilisant par exemple
un anti-dictionnaire (stoplist), la catégorie morpho-syntaxique, ou encore les fréquences d’oc-
currence), la limitation du nombre de termes d’indexation a aussi été envisagée au moyen de
techniques statistiques issues de l’analyse des données (analyse en composantes principales,
analyse factorielle discriminante) (Deerwester et al., 1990; Hofmann, 1999). Cependant, la plu-
part de ces techniques ne sont pas nécessairement adaptées lorsque l’on est en présence d’in-
formations supplémentaires sur les termes d’indexation ayant une structure formelle (au lieu de
statistique). L’ objectif des travaux présentés dans cette contribution est précisément d’utiliser
une ressource sémantique exteme (i.e. additionnelle aux données de recherche documentaire
proprement dites) structurée, de type ontologie, en vue d’augmenter la richesse de l’indexation.
La spéciﬁcité de ce travail par rapport a des travaux antérieurs similaires, qui utilisent des « syn-
sets » ou des hyperonymes de WordNet comme termes d’indexation (Gonzalo et al., 1998a;
Gonzalo et al., 1998b; Whaley, 1999; Mihalcea & Moldovan, 2000), est d’essayer de faire
un pas supplémentaire en sélectionnant les « concepts » a utiliser comme termes d’indexation
au moyen d’un critere issu de la théorie de l’information, la Coupe de Redondance Minimale
(CRM, voir ﬁgure 1), que l’on applique a la relation inclusive « est-un » (hyperonymie) obtenue
ici par le biais de la taxonomie (anglaise) EDR (Miyoshi et al., 1996).

2 Coupe de redondance minimale

2.1 Objectifs

Le choix du « concept hyperonyme »2 a utiliser pour représenter un mot est un choix délicat : un
concept trop général dégradera les performances du systeme en diIr1inuant la précision, tandis

2 Nous désignons par << concept hyperonyme» un noeud non feuille dans l’ontologie. Les feuilles de 1’onto1ogie
représentent les mots.

Indexation sémantique par coupes de redondance minimale

 

d) |:|:|:|:|:|:|
C3

FIG. 1 — Différentes méthodes d’indexation: (a) traditionnelle, au moyen des mots, racines
(stems) ou lemmes ; (b) utilisant une ontologie sémantique (illustration de droite), chaque terme
d’indexation de (a) est augmenté par tout ou partie des « concepts » le recouvrant; cela conduit
a une explosion du nombre de termes d’indexation; (c) indexation par les concepts de plus bas
niveau (m indexation par « synsets ») : chaque terme d’indexation est remplace’ par son concept
hyperonyme direct, factorisant ainsi tous les mots dominés par ce concept; on réduit donc le
nombre de termes d’indexation, tout en permettant de détecter la similarité entre documents
contenant ces mots; (d) indexation par une Coupe de Redondance Minimale (CRM): chaque
terme d’indexation est remplacé par l’un de ses concepts hyperonymes, déterminé par la CRM.
Cela restreint d’avantage le nombre de termes d’indexation, le nombre de mots couverts (facto-
risés) par chacun d’eux étant plus grand qu’avec le concept hyperonyme direct.

qu’un concept trop spéciﬁque ne permettra pas de réduire signiﬁcativement le nombre de termes
d’indexation et conservera la distinction entre mots de sens proches.

Pour déterminer le niveau adéquat des concepts d’indexation, nous faisons ici le choix de ne
prendre en considération des coupes dans l’ontologie (une coupe étant un ensemble minimal3
de noeuds déﬁnissant une partition sur les feuilles), en considérant que chaque noeud représente
alors l’ensemble des feuilles qu’il recouvre.

Le probleme est de trouver une stratégie permettant d’identiﬁer une coupe « optimale » en un
temps acceptable. Pour une tache relativement similaire, Li (1998) propose d’utiliser le cri-
tere MDL (Minimum Description Length). Si ce critere est facilement calculable, il a come
inconvénient, du moins lorsque appliqué a l’ontologie EDR, de tres souvent sélectionner la ra-
cine de l’ontologie comme coupe « optimale » ; ce qui n’est pas vraiment adéquat pour la tache
considérée ! Nous nous proposons donc ici d’employer un autre critere, fondé sur la théorie de
l’information, permettant d’identiﬁer une coupe pour laquelle la redondance d ’inf0rmati0n est
minimale, c’est-a-dire une coupe qui équilibre le plus possible les degrés de description des
mots factorisés en tenant compte de la probabilité d’occurrence de ces mots.

2.2 Critére de redondance minimale

Soient N =  l’ensemble des noeuds (concepts ou mots) et W l’ensemble des feuilles (mots
uniquement) contenus dans l’ontologie considérée. On déﬁnit alors une coupe I‘ come un
sous-ensemble minimal3 de N recouvrant W. Une coupe probabilisée M = (P, P) est une

3 Par << minimal », on entend qu’aucun noeud de la coupe ne peut en étre retiré sans en dimjnuer la couverture.

Seydoux F. & Chappelier J .-C.

paire composée d’une coupe I‘ et d’une distribution de probabilités P sur I‘. On notera |I‘| le
nombre de noeuds de la coupe (et par extension: |M | = 

Dans la suite, nous considérons la coupe M = (F, Pf) probabilisée par les fréquences d’oc-
currences des mots correspondant aux feuilles de l’ontologie : Pf  =  / |D|, o1‘1 
représente le nombre d’occurrences du concept (ou mot) n,- dans les données D. Pour calculer
f on admet qu’il y a occurrence de 12,- lorsqu’il y a occurrence de l’un des w,- E n,-++ mots
hyponymes de 72,-, o1‘1 n++ représente la fermeture transitive de 72”“, ensemble des successeurs
de n.

La redondance R(M) d’une coupe probabilisée M = (F, P) est déﬁnie par (Shannon, 1948):

H (M)
R M =1—j, avec H(M)=— P(n)-1ogP(n).
( ) log IM I Z;
Minimiser la redondance revient a maximiser le rapport entre l’entropie des éléments de la
coupe et sa valeur maximale possible (log |M |); le but est donc de trouver une coupe probabi-

lisée M qui maximise le critere C H:

C { 0 si |M| g 1,
H : HM .
J—l10g l M s1non.

Un tel critere pose cependant quelques difﬁcultés en pratique: d’une part, il ne permet pas
d’identiﬁer une coupe optimale unique, mais un ensemble de coupes possibles; d’autre part,
l’optimum local sur une partie de l’ontologie est conditionné par l’optimum sur le reste (et
inversement). Pour identiﬁer les modeles satisfaisant le critere global, il faudrait donc le calculer
pour l’ensemble des coupes possibles.

La premiere difﬁculté peut étre surmontée de maniere relativement aisée, par exemple en ne
retenant qu’une coupe choisie au hasard, ou en favorisant celles admettant le plus de noeuds, ou
encore en guidant le choix selon la profondeur moyenne des noeuds.

Pour étre calculable, la seconde difﬁculté implique par contre de renoncer a l’optimalité globale.
Néanmoins, il est possible d’utiliser un algorithme de programmation dynamique permettant
d’obtenir une coupe acceptable (heuristique). Cet algorithme consiste a choisir, pour un sous-
arbre4 dans l’ontologie, une coupe optimale parmi celles constituées des successeurs directs
de la racine de ce sous-arbre et les sous-coupes « optimales » de chacun de ces successeurs,
obtenues de maniere similaire. Plus formellement, l’algorithme récursif donné en table 1 est
appliqué a partir de la racine de l’ontologie5.

2.3 Exemple

Pour illustrer le fonctionnement de la technique de sélection des coupes décrite précédemment,
admettons que l’on dispose de l’ontologie présentée en ﬁgure 2 ; les valeurs indiquées en regard

4 Bien que les ontologies utilisées présentent usuellement une structure de graphe orienté sans cycle (DAG), nous
simpliﬁerons ici le propos en considérant qu’il s’agit d’arbres. Cette approximation, qui n’inVa1ide en rien les
raisonnements exposés ici, n’est évidemment pas faite en pratique.

5 En pratique, plusieurs optimisations sont introduites (notamment, les successeurs feuilles d’un noeud sont né-
cessairement compris dans la sous—coupe optimale pour ce noeud); mais elles ne changent rien a l’aspect
fondamental présenté ici.

Indexation sémantique par coupes de redondance minimale

ALGORITHME CRM

Entrée : un noeud t (dans une hiérarchie).
Sortie : 0 RM : une coupe de redondance minimale sous ce noeud.

Sit e W
CRM <— {t}
Sinon
Pour n, 6 15+
7, <— 
Pour 1 3 is 3 n := |t+|
Pk “ UjE[1:n\k] 73' U 7919
Pn+1 ‘— jE[1m,] fl/j

P'n+2 (— UjE[1:n] 19.7

 <— ArgmaX1«j:1<j<n+2 

ou Argmax retourne une coupe possible réalisant ce maximum.

TAB. 1 — Algorithme de recherche heuristique d’une CRM.

des feuilles correspondent aux fréquences d’occurrences des mots y-relatifs obtenues sur un
corpus ﬁctif.

Pour la coupe I‘ = [ANIMAL, PLANTE, TRANSPORT], on obtient la valeur du critere C H:

n,- ANIMAL PLANTE TRANSPORT
f(n,-) 18 30 1
Pf(n,-) 0.3673 0.6122 0.0204

—P,(n,-)1og,P,(n,-) 0.5307 0.4334 0.1146

CH(r) = 15:33?) = 0.6806
R(F) = 1 — CH(F) = 0.3194

Dans un tel cas de ﬁgure, en examinant l’ensemble des 2036 différentes coupes possibles, on
trouverait que le critere sur la coupe optimale (indiquée sur la ﬁgure 2) vaut 0.874. L’algo-
rithme de recherche par optimum local trouve une coupe pour laquelle le critere est légerement
inférieur: 0.810; mais son obtention ne nécessite l’évaluation que de 36 coupes différentes.

3 Expériences

Nous avons effectué un jeu d’eXpériences en utilisant les collections standards ADI, TIME,
MED, CACM et CISI6 du projet SMART (Salton, 1971), ainsi qu’une ontologie produite a partir
du dictionnaire électronique EDR (Miyoshi et al., 1996).

EDR est organisée en cinq dictionnaires de différents types, plus ou moins indépendants les uns

5 Disponibles a 1’adresse ftp://ftp.cs.come11.edu/pub/smart/.

'91So1o1uo sun swap sadnoo ap 9[du19xg — z '91 ._.1

R = 0.12639

R = 0.18975

Ewe @ ® 

optima/e H = 0.31943

"°”"ée @ @ @ #13 % %

hirondclle corbeau aigle perroquet omythorynquc hétre sapin cyprés _ fucus diatomos velo voilier voiture avion hé1icop.ére
3 0 0 0 2 HERBIVORE CARNIVORE 0 1 2  a 0 0 1 0 0 0 0

vache cheval mnuton lion loup myosotis violette jacime C°q“5]iC°'~ lismart. “me j°“q“m5 Pﬁmevae
1 4 2 5 1 3 2 0 6 s 8 0 0

'3-'1‘ Janaddeqg 29 '._.1 xnopﬁag

Indexation sémantique par coupes de redondance minimale

des autres. Parmi l’ensemble de ces dictionnaires, les deux suivants sont utilisés pour constituer
l’ontologie:

le dictionnaire des mots anglais, qui rassemble les informations morphologiques (prononcia-

tion, découpage syllabique, inﬂexion, ...) et syntaxiques (catégorie morpho-syntaxique,
dénombrabilité, ﬂexions, ...) pour un peu plus de 240’ 000 graphies différentes (corres-
pondant a w 420’ 000 mots), et permet de relier ces graphies avec les informations du
dictionnaire des concepts. Les graphies de ce dictionnaire sont principalement (mais pas
exclusivement) des lemmes ; il comporte également un nombre important de multi-termes
(> 113’ 000), ﬁgurant des mots composés et expressions idiomatiques.

le dictionnaire des concepts, qui décrit a peu pres 490’ 000 concepts, organises hiérarchique-

ment entre eux selon des relations d’hyponyInie/hyperonymie (chaque concept pouvant
avoir plusieurs hyponymes et hyperonymes). Un certain nombre de relations sémantiques
binaires supplémentaires (telles que objet-action, agent-action, agent-but) sont par ailleurs
décrites, mais nous ne les utilisons pas ici. Remarquons qu’un nombre important de
concepts (environ la moitié) ne sont pas directement associés a des mots; ces concepts
ne peuvent étre déﬁnis et appréhendés qu’au travers de leurs relations avec les autres
concepts.

Le systeme de RD utilisé est le modele vectoriel SMART, combine a un lemmatiseur eXteme7,
qui fait également ofﬁce de segmenteur (tokenizer) et d’étiqueteur morpho-syntaxique. Un
ﬁltrage par catégorie grammaticale est réalisés (ne sont conservés que les noms, adjectifs et
verbes), mais nous n’utilisons pas d’anti-dictionnaire et ne faisons pas de ﬁltrage fréquentiel.

Les transformations du jeu d’indexation sont obtenues en prétraitant les données soumises au
systeme de RD:

1.

en premier lieu, les diverses informations textuelles (principalement titre et contenu) des
documents sont agrégées, et les autres informations (auteurs, sources, etc.) supprimées;
documents et requétes sont ensuite segmentés et lemmatisés;

on cherche ensuite les correspondances entre les mots contenus dans les documents et
ceux décrits dans l’ontologie; on tente d’établir en priorité une correspondance avec la
graphie, et s’il n’y en a pas, avec sa forme lemmatisée; les mots sans correspondance sont
indexés de maniere traditionnelle; les taux de couvertures sur les différentes collections
sont de l’ordre de 90%.

on procede ensuite a l’eXpansion de la hiérarchie des concepts relatifs aux mots conservés
pour l’ensemble des documents; selon les différents cas expérimentés, on prendra soit la
totalite’ des concepts possibles (en tablant sur un renforcement mutuel des concepts « cor-
rects » induit par les multiples co-occurrences), soit uniquement le concept le plus pro-
bable (dans l’absolu pour le mot donné — cette information est présente dans l’ontologie
utilisée);

on détermine ensuite une coupe optimale selon le critere C H, au moyen de l’algorithme
CRM présenté en section 2.2;

ﬁnalement, on substitue les mots des documents et des requétes par les identiﬁcateurs des
concepts de la coupe qui les subordonnent.

7 Le systéme Sylex 1.7 (© 1993-98 DECAN INGENIA).

8 Par << couverture », on désigne la fraction des occurrences des mots couverts par l’ontologie.

Seydoux F. & Chappelier J .-C.

| mesure | (a) (b) (c) (d)
corpus ADI (82 documents)

taille index 1800 14748 10099 1292
tous les concepts, tf.idf precision 0.3578 0.3134 0.3356 0.2458

rappel 0.6984 0.7126 0.7406 0.6017
tous les concepts, sans precision 0.2497 0.1219 0.2550 0.1607
pondération rappel 0.5996 0.3452 0.6708 0.5130

taille index 1800 5255 2888 658

°°“°°Pt 1° P1“ precision 0.3578 0.4060 0.4274 0.2052

P“’b"‘b1°’ tfidf rappel 0.6984 0.7306 0.7217 0.5200
concept + probable, precision 0.2497 0.1376 0.2939 0.1466
sans pondération rappel 0.5 996 0.3727 0.7141 0.4911

corpus TIME (423 documents)
taille index 21815 93707 70091 6760
tous les concepts, tf.idf precision 0.5496 0.4231 0.4536 0.2683

rappel 0.8901 0.7642 0.8036 0.6026
tous les concepts, sans precision 0.3288 0.0337 0.2353 0.0370
pondération rappel 0.7755 0.1021 0.5709 0.1387

tailleindex 21815 53140 31612 4814

°°“°°Pt 1° P1“ precision 0.5496 0.5143 0.5565 0.2729

probable, tf.idf

rappel 0.8901 0.8760 0.9053 0.5162
concept + probable, precision 0.3288 0.0346 0.3692 0.0372
sans ponderation rappel 0.7755 0.1201 0.7590 0.1322

corpus MED (1033 documents)
taille index 11893 51712 38524 4078
tous les concepts, tf.idf precision 0.4607 0.3029 0.2996 0.2336

rappel 0.5547 0.3903 0.3794 0.3142
tous les concepts, sans precision 0.3623 0.0105 0.1905 0.0229
pondération rappel 0.4574 0.0246 0.2749 0.05 13

tailleindex 11893 30284 18109 2888

°°“°°Pt 1° P1“ precision 0.4607 0.4266 0.4518 0.0743

Pr°b"‘b1°’ tfidf rappel 0.5547 0.5169 0.5404 0.1042
concept + probable, precision 0.3623 0.0105 0.3229 0.0132
sans pondération rappel 0.4574 0.0313 0.4230 0.0368

corpus CISI (1460 documents)
taille index 10019 53453 39544 3516
tous les concepts, tf.idf precision 0.1733 0.1043 0.1139 0.0740

rappel 0.2318 0.1627 0.1675 0.1294
tous les concepts, sans precision 0.0687 0.0232 0.0569 0.0282
pondération rappel 0.1239 0.0376 0.0963 0.0492

taille index 10019 26246 14993 1894

°°“°°P”° Plus precision 0.1733 0.1590 0.1825 0.0602

Pr°b"‘b1°’ tfidf rappel 0.2318 0.2131 0.2313 0.0895
concept + probable, precision 0.0687 0.0201 0.0805 0.0221
sans ponderation rappel 0.1239 0.0403 0.1300 0.0435

corpus CACM (3204 documents)
taille index 10053 51712 38524 4078
tous les concepts, tf.idf precision 0.2865 0.1293 0.1935 0.1089

rappel 0.4534 0.2579 0.3617 0.1999
tous les concepts, sans precision 0.1555 0.0133 0.1447 0.0320
pondération rappel 0.3082 0.0306 0.2549 0.0699

taille index 10053 25207 14681 2670

°°“°°Pt 1° P1“ precision 0.2865 0.2358 0.2804 0.0645

P‘°b""b1"’ tﬁidf rappel 0.4534 0.3834 0.4567 0.1090
concept + probable, precision 0.1555 0.0230 0.1472 0.0245
sans ponderation rappel 0.3082 0.0302 0.2926 0.0385

TAB. 2 — Résultats des différentes expériences sur différents corpus. (a) : mots uniquement; (b) :
mots + concepts ; (c) : hyperonymes directs et ((1) : hyperonymes dans CRM (cf aussi ﬁg. 1).

Indexation sémantique par coupes de redondance minimale

On trouvera dans la table 2 les valeurs de précision (« I1-pt prec ») et de rappel (« 30 doc »)9
foumies par le systeme SMART. Toutes les expériences sont par ailleurs conduites en utilisant
soit le schéma de pondération classique (« tf.idf »), soit sans pondération.

On constate que l’indexation par hyperonymes directs obtient des résultats sensiblement égaux
au systeme de base, mais pour un rappel plus élevé. L’indexation par CRM degrade par contre
les performances.

4 Conclusion

Les résultats obtenus sur ces expériences ne sont malheureusement pas concluants quant a l’uti-
lisation du critere CRM pour l’indexation sémantique. Cependant, plusieurs remarques sont a
apporter :

— Le critere utilisé ici ne permet pas de sélectionner, ni méme d’inﬂuencer, le niveau de profon-

deur dans 1’ ontologie de la coupe obtenue. Au vu de la réduction drastique du jeu d’indexation
et des mauvaises performances obtenues, il semble que ce critere, ou du moins l’heuristique
implémentée, sélectionne une coupe située trop haut dans la hiérarchie, ce qui a come
conséquence évidente de faire baisser la précision. La bonne performance de la coupe au ni-
veau des concept hyperonymes directs nous permet de croire qu’il doit y avoir un niveau plus
adapté, plus proche des feuilles, pour la CRM.
On pourrait par exemple limiter considérablement l’espace de recherche de la coupe idéale
en empéchant de considérer des noeuds situés « trop hauts » dans la hiérarchie. Une piste a
explorer pour améliorer tant l’adéquation de la coupe sélectionnée avec un processus d’in-
dexation que la recherche de cette coupe elle-méme consisterait a explorer les gains possibles
en terme de redondance a partir de la coupe uniquement constituée de feuilles, et en dirigeant
la recherche vers le haut de la hiérarchie, plutot que de haut en bas a partir de la racine,
comme dans l’heuristique présentée ici.

— Par ailleurs, en conservant l’idée d’une action sur le jeu d’indexation lui-meme, il serait in-
téressant d’exaIniner de quelle maniere les pondérations (e. g. « tf.idf »), utilisées uniquement
lors de la recherche des documents proprement dite, devraient étre prises en compte lors de
la détermination de la coupe.

— Finalement, les résultats présentés ici restent a corroborer avec ceux a obtenir avec d’autres
ontologies, en particulier WordNet, qui a une structure assez différente d’EDR.

Pour terminer, soulignons que l’intérét de la technique présentée dépasse le cadre de la stricte
recherche documentaire. Celle-ci pourrait en effet s’avérer utile, et peut étre meme plus promet-
teuse, pour d’autres domaines d’application tels que la classiﬁcation de documents ou le résumé
automatique.

Références

DEERWESTER S. C., DUMAIS S. T., LANDAUER T. K., FURNAS G. W. & HARSHMAN R. A. (1990).
Indexing by latent semantic analysis. Journal of the American Society of Information Science, 41(6),

9 11 s’agit la de mesures standard: la << ll-pt precision » est la moyenne des précisions pour les taux de rappels
0.0, 0.1, .. ., 1.0, 011 la précision au taux de rappel 0.0 est la précision maximale obtenue sur l’ensemble des
documents pertinents retrouvés ; le << rappel 30 doc » est le taux de rappel apres 30 documents retournés.

Seydoux F. & Chappelier J .-C.

391-407.

GONZALO J ., VERDEJO F., CHUGUR I. & CIGARRAN J . (1998a). Ir1dexing with WordNet synsets can
improve text retrieval. In Proc. of the COLING/ACL 1998 Workshop on Usage of WordNet for Natural
Language Processing, p. 38-44.

GONZALO J ., VERDEJO F., PETERS C. & CALZOLARI N. (1998b). Applying EuroWordNet to multi-
lingual text retrieval. Journal of Computers and the Humanities, 32(2-3), 185-207.

HARMAN D. (1988). Towards interactive query expansion. In Proc. of the 11th Annual Int. ACM-SIGIR
Conference on Research and development in information retrieval, p. 321-331.

HOFMANN T. (1999). Probabilistic latent semantic indexing. In proc. of the 22th International Confe-
rence on Research and Development in Information Retrieval (SIGIR), p. 50-57.

IDE N. & VERONIS J . (1998). Word sense disambiguation: The state of the art. Computational Linguis-
tics, 24(1), 1-40.

LI H. (1998). A probabilistic approach to lexical semantic knowledge acquisition and structural disam-
biguation. Master’s thesis, Graduate School of Science, University of Tokyo.

MIHALCEA R. & MOLDOVAN D. (2000). Semantic indexing using WordNet senses. In Proc. of ACL
Workshop on IR & NLP.

MIYOSHI H., AMD M. KOBAYASHI K. S. & OGINO T. (1996). An overview of the EDR electronic
dictionary and the current status of its utilization. In Proc. of COLING, p. 1090- 1093.

MOLDOVAN D. I. & MIHALCEA R. (2000). Using wordnet and lexical operators to improve internet
searches. IEEE Internet Computing, 4(1), 34-43.

RICHARDSON R. & SMEATON A. F. (1995). Using WordNet in a Knowledge-Based Approach to Infor-
mation Retrieval. Rapport interne CA-0395, Dublin City University, Glasnevin, Dublin 9, Ireland.

SALTON G. (1968). Automatic Information Organization and Retrieval. McGraw-Hill.

SALTON G. (1971). The SMART Retrieval System - Experiments in Automatic Document Processing.
Prentice Hall.

SHANNON C. E. (1948). A mathematical theory of communication. The Bell System Technical Journal,
27, 379-423.

SMEATON A. F. & QUIGLEY I. (1996). Experiments on using semantic distances between words in
image caption retrieval. In Proc. of I 9th Int. Conf on Research and Development in Information Retrie-
val, p. 174-180.

VOORHEES E. M. (1993). Using WordNet to disambiguate word senses for text retrieval. In Proc.
of 16th Annual International ACM-SIGIR Conference on Research and Development in Information
Retrieval, p. 171-80.

VOORHEES E. M. (1994). Query expansion using lexical-semantic relations. In Proc. 17th Annual Int.
ACM-SIGIR Conf on Research and Development in Information Retrieval, p. 61-69.

VOORHEES E. M. (1998). Using WordNet for text retrieval. In C. FELLBAUM, Ed., WordNet: An
Electronic Lexical Database, chapter 12, p. 285-303. MIT Press.

WHALEY J . M. (1999). An Application of Word Sense Disambiguation to Information Retrieval. Rap-
port interne PCS-TR99-352, Dartmouth College, Computer Science, Hanover, NH.

WILKS Y. & STEVENSON M. (1998). Word sense disambiguation using optimised combinations of
knowledge sources. In Proc. of the 17th Int. Conf on Computational Linguistics, p. 1398- 1402.

