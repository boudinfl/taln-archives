TALN 2005, Dourdan, 6-10 juin 2005

Indexation sémantique au moyen de coupes
de redondance minimale dans une ontologie

Florian Seydoux & J ean—Cédric Chappelier
Faculté Informatique et Communications
Ecole Polytechnique Fédérale de Lausanne (EPFL)
CH—l0l5 Lausanne, Suisse

{florian . seydoux, jean—cedric . chappelier} @epfl . ch

M0tS-Cl€fS I Indexation sémantique, Recherche documentaire, Redondance minimale,
Ontologie.

K€yWOFdS2 Semantic Indexing, Information Retrieval, Minimal Redundancy, Ontology.

Résumé Plusieurs travaux antérieurs ont fait état de l’amélioration possible des perfor-
mances des systemes de recherche documentaire grace a l’utilisation d’indexation sémantique
utilisant une ontologie (p.ex. WordNet). La présente contribution décrit une nouvelle méthode
visant a réduire le nombre de terrnes d’indexation utilises dans une indexation sémantique, en
cherchant la coupe de redondance minimale dans la hiérarchie fournie par l’ontologie. Les re-
sultats, obtenus sur diverses collections de documents en utilisant le dictionnaire EDR, sont
présentés.

Abstract Several former works have shown that it is possible to improve information
retrieval performances using semantic indexing, adding additional information coming from a
thesaurus (e. g. WordNet). This paper presents a new method to reduce the number of "concepts"
used to index the documents, by determining a minimum redundancy cut in the hierarchy pro-
vided by the thesaurus. The results of experiments carried out on several standard document
collections using the EDR thesaurus are presented.

1 Introduction

L’utilisation de connaissances sémantiques dans le cadre de la Recherche Documentaire (RD)
n’est pas nouvelle. On voit se dégager dans la littérature scientiﬁque principalement trois champs
d’ application : Fexpcmsion de requétes (Voorhees, 1994; Moldovan & Mihalcea, 2000), la désam—
biguisation sémantique (WSD) (Ide & Véronis, 1998; Wilks & Stevenson, 1998) et l ’indexati0n
sémantique. C’est dans ce dernier cadre que se situe le travail présenté ici.

L’indexation sémantique consiste a utiliser, pour indexer des documents, le(s) sens des mots
qu’ils contiennent, au lieu ou en plus des motsl eux—mémes comme c’est le cas en RD classique,

Ce travail a été ﬁnance par le projet n°200020—l03529 du Fond National Suisse pour la Recherche Scientiﬁque.

1 Habituellement, leurs lemmes ou leurs racines (stems).

33

34

Seydoux F. & Chappelier J .—C.

de maniere a ameliorer tant le rappel (par le biais des relations de synonymie) que la precision
(en traitant correctement les cas d’homographie/polysemie).

Les differentes experiences rapportees a ce sujet dans la litterature font cependant etat de re-
sultats peu concluants, parfois meme contradictoires: si certains observent que l’ajout de ce
type d’information, realisee de maniere automatique, degrade les performances de leur systeme
(Salton, 1968; Harman, 1988; Voorhees, 1993; Voorhees, 1998), pour d’autres au contraire une
amelioration signiﬁcative est obtenue (Richardson & Smeaton, 1995; Smeaton & Quigley, 1996;
Gonzalo er al., 1998a; Gonzalo er al., 1998b; Mihalcea & Moldovan, 2000).

Bien qu’il semble souhaitable pour un systeme de RD de prendre en compte un maximum
d’informations, en particulier des informations de nature semantique, un tel accroissement des
termes d’indexation peut se reveler contre—productif, ou tout du moins ne pas developper son
plein potentiel. En effet, une forte augmentation du nombre de termes d’indexation a non seule—
ment comme consequences de prolonger notablement les temps de traitement, mais surtout
affecte les performances sur le plan de la precision: tenter de discrirniner quelques documents
parmi un ensemble sur la base d’un tres grand nombre de criteres est difﬁcile a realiser, la « dis-
tance >> — generalement une similarite ou une dissemblance — entre chaque paire de documents
tendant a devenir a peu pres la meme (effet << curse of dimensionality »).

Ce probleme n’est pas nouveau et il existe deja un certain nombre de techniques visant a limiter
la taille du jeu d’indexation: en plus de celles procedant par ﬁltrage (en utilisant par exemple
un anti—dictionnaire (stoplist), la categorie morpho—syntaxique, ou encore les frequences d’oc—
currence), la limitation du nombre de termes d’indexation a aussi ete envisagee au moyen de
techniques statistiques issues de l’analyse des donnees (analyse en composantes principales,
analyse factorielle discriminante) (Deerwester er al., 1990; Hofmann, 1999). Cependant, la plu-
part de ces techniques ne sont pas necessairement adaptees lorsque l’on est en presence d’in—
formations supplementaires sur les termes d’indexation ayant une structure formelle (au lieu de
statistique). L’ objectif des travaux presentes dans cette contribution est précisement d’utiliser
une ressource semantique exteme (i.e. additionnelle aux donnees de recherche documentaire
proprement dites) structuree, de type ontologie, en vue d’augmenter la riches se de l’indexation.
La speciﬁcite de ce travail par rapport a des travaux anterieurs similaires, qui utilisent des « syn-
sets» ou des hyperonymes de WordNet comme termes d’indexation (Gonzalo er al., 1998a;
Gonzalo er al., 1998b; Whaley, 1999; Mihalcea & Moldovan, 2000), est d’essayer de faire
un pas supplementaire en selectionnant les « concepts >> a utiliser comme termes d’indexation
au moyen d’un critere issu de la theorie de l’information, la Coupe de Redondcmce Minimale
(CRM, voir ﬁgure 1), que l’on applique a la relation inclusive « est—un » (hypbronymie) obtenue
ici par le biais de la taxonomie (anglaise) EDR (Miyoshi er al., 1996).

2 Coupe de redondance minimale

2.1 Objectifs

Le choix du « concept hyperonyme »2 a utiliser pour representer un mot est un choix delicat: un
concept trop general dégradera les performances du systeme en diminuant la precision, tandis

2 Nous désignons par « concept hyperonyme» un noeud non feuille dans 1’onto1ogie. Les feuilles de 1’onto1ogie
représentent les mots.

Indexation semantique par coupes de redondance minimale

 

d) |:|:|:|:|:|:|
C3

FIG. 1 — Differentes méthodes d’indexation: (a) traditionnelle, au moyen des mots, racines
(stems) ou lemmes ; (b) utilisant une ontologie semantique (illustration de droite), chaque terme
d’indexation de (a) est augmenté par tout ou partie des « concepts » le recouvrant; cela conduit
a une explosion du nombre de termes d’indexation; (c) indexation par les concepts de plus bas
niveau (m indexation par « synsets >>) : chaque terme d’indexation est remplacé par son concept
hyperonyme direct, factorisant ainsi tous les mots dominés par ce concept; on reduit donc le
nombre de termes d’indexation, tout en permettant de détecter la similarité entre documents
contenant ces mots ; (d) indexation par une Coupe de Redondance Minimale (CRM) : chaque
terme d’indexation est remplacé par l’un de ses concepts hyperonymes, determine’ par la CRM.
Cela restreint d’aVantage le nombre de termes d’indexation, le nombre de mots couverts (facto-
risés) par chacun d’eux etant plus grand qu’aVec le concept hyperonyme direct.

qu’un concept trop speciﬁque ne permettra pas de reduire signiﬁcativement le nombre de termes
d’indexation et conservera la distinction entre mots de sens proches.

Pour determiner le niveau adéquat des concepts d’indexation, nous faisons ici le choix de ne
prendre en consideration des coupes dans l’ontologie (une coupe etant un ensemble minimal3
de noeuds deﬁnissant une partition sur les feuilles), en considérant que chaque noeud represente
alors l’ensemble des feuilles qu’il recouvre.

Le probleme est de trouver une strategie permettant d’identiﬁer une coupe << optimale » en un
temps acceptable. Pour une tache relativement similaire, Li (1998) propose d’utiliser le cri-
tere MDL (Minimum Description Length). Si ce critere est facilement calculable, il a comme
inconvenient, du moins lorsque applique a l’ontologie EDR, de tres souvent selectionner la ra-
cine de l’ontologie comme coupe << optimale » ; ce qui n’est pas Vraiment adéquat pour la tache
considerée ! Nous nous proposons donc ici d’employer un autre critere, fonde sur la théorie de
l’information, permettant d’identiﬁer une coupe pour laquelle la redondance d ’inf0rmati0n est
minimale, c’est—a—dire une coupe qui equilibre le plus possible les degrés de description des
mots factorises en tenant compte de la probabilité d’occurrence de ces mots.

2.2 Critere de redondance minimale

Soient./V‘ =  l’ensemble des noeuds (concepts ou mots) et W l’ensemble des feuilles (mots
uniquement) contenus dans l’ontologie considéree. On deﬁnit alors une coupe F comme un
sous—ensemble minimal3 de ./V‘ recouvrant W. Une coupe probabilisee M = (F, P) est une

3 Par « minimal », on entend qu’aucun noeud de la coupe ne peut en etre retire sans en diminuer la couverture.

35

36

Seydoux F. & Chappelier J .—C.

paire composee d’une coupe P et d’une distribution de probabilites P sur P. On notera [P] le
nombre de notuds de la coupe (et par extension: [M I = 

Dans la suite, nous considerons la coupe M = (P, Pf) probabilisee par les frequences d’oc—
currences des mots correspondant aux feuilles de l’ontologie : Pf(n,;) =  [D], ou 
represente le nombre d’occurrences du concept (ou mot) n, dans les donnees D. Pour calculer
f on admet qu’il y a occurrence de 71,; lorsqu’il y a occurrence de l’un des w, E n,;++ mots
hyponymes de 717;, ou n++ represente la fermeture transitive de 71+, ensemble des successeurs
de 71.

La redondance R(M) d’une coupe probabilisee M = (P, P) est deﬁnie par (Shannon, 1948):

H(M) H(M) = —ZP(n) -l0gP(n).

RM = ——,
‘ ) low» 

3.V6C

Minimiser la redondance revient a maximiser le rapport entre l’entropie des elements de la
coupe et sa Valeur maximale possible (log [M I); le but est donc de trouver une coupe probabi-
lisee M qui maximise le critere C H:

0 si  3 1,
HOW)
10g|M|

CH = .
sinon.

Un tel critere pose cependant quelques difﬁcultes en pratique: d’une part, il ne permet pas
d’identiﬁer une coupe optimale unique, mais un ensemble de coupes possibles; d’autre part,
l’optimum local sur une partie de l’ontologie est conditionne par l’optimum sur le reste (et
inversement). Pour identiﬁer les modeles satisfaisant le critere global, il faudrait donc le calculer
pour l’ensemble des coupes possibles.

La premiere difﬁculte peut étre surmontee de maniere relativement aisee, par exemple en ne
retenant qu’une coupe choisie au hasard, ou en favorisant celles admettant le plus de noeuds, ou
encore en guidant le choix selon la profondeur moyenne des noeuds.

Pour étre calculable, la seconde difﬁculte implique par contre de renoncer a l’optimalite globale.
Neanmoins, il est possible d’utiliser un algorithme de programmation dynamique permettant
d’obtenir une coupe acceptable (heuristique). Cet algorithme consiste a choisir, pour un sous-
arbre4 dans l’ontologie, une coupe optimale parmi celles constituees des successeurs directs
de la racine de ce sous—arbre et les sous—coupes << optimales » de chacun de ces successeurs,
obtenues de maniere similaire. Plus formellement, l’algorithme recursif donne en table 1 est
applique a partir de la racine de l’ontologie5.

2.3 Exemple

Pour illustrer le fonctionnement de la technique de selection des coupes decrite precedemment,
admettons que l’on dispose de l’ontologie presentee en ﬁgure 2 ; les Valeurs indiquees en regard

4 Bien que les ontologies utilisees presentent usuellement une structure de graphe oriente sans cycle (DAG), nous
simpliﬁerons ici 1e propos en considerant qu’i1 s’agit d’ arbres. Cette approximation, qui n’inVa1ide en rien les
raisonnements exposes ici, n’est evidemment pas faite en pratique.

5 En pratique, plusieurs optimisations sont introduites (notamment, les successeurs feuilles d’un noeud sont ne-
cessairement compris dans la sous—coupe optimale pour ce noeud); mais elles ne changent rien a 1’aspect
fondamental presente ici.

Indexation semantique par coupes de redondance minimale

ALGORITHME CRM

Entree : un noeud t (dans une hierarchie).
Sortie : CRM : une coupe de redondance minimale sous ce noeud.

Sit E W
CRM <— {If}
Sinon
Pour n, 6 25+
7,; <— 
Pour 1 3 k: 3 n:=1t+1
Pk “ Uj€[1:n\k] 72’ U 19k
Tn-H ‘— j€[1;n] /7]’
F"+2 <— j€[1:n] 19?

 <: AI"gII1aX1«j:1<j<n+2 

ou Argmax retourne une coupe possible realisant ce maximum.

TAB. 1 — Algorithme de recherche heuristique d’une CRM.

des feuilles correspondent aux frequences d’occurrences des mots y—relatifs obtenues sur un
corpus ﬁctif.

Pour la coupe F = [AN1MAL, PLANTE, TRANSPORT], on obtient la Valeur du critere C H:

n, ANIMAL PLANTE TRANSPORT
f(n,) 18 30 1
P,(n,) 0.3673 0.6122 0.0204

—Pf(n,.) 10g, P,(n,) 0.5307 0.4334 0.1146

cH(r) = 13335;) 2 0.6806
R(r) = 1 — cH(r) = 0.3194

Dans un tel cas de ﬁgure, en examinant l’ensemble des 2036 differentes coupes possibles, on
trouverait que le critere sur la coupe optimale (indiquee sur la ﬁgure 2) Vaut 0874. L’ algo-
rithme de recherche par optimum local trouve une coupe pour laquelle le critere est legerement
inferieur: 0.810; mais son obtention ne necessite l’eValuation que de 36 coupes differentes.

3 Experiences

Nous avons effectue un jeu d’expe’riences en utilisant les collections standards ADI, TIME,
MED, CACM et CISI6 du projet SMART (Salton, 1971), ainsi qu’une ontologie produite a partir
du dictionnaire electronique EDR (Miyoshi et al., 1996).

EDR est organisee en cinq dictionnaires de differents types, plus ou moins independants les uns

6 Disponibles a1’adresse ftp://ftp.cs.cornell.edu/pub/smart/.

37

Seydoux F. & Chappelier J .—C.

o o w w o o N m H m N + H
Em>o§a oE:HEe.H owmh .:.nEm: H8:o:H:5 250% o:o§> m:89HE ans :0: E335 E>2Ho EH0?

o o o o H o o . N H o N o o o m
23.2% :35 EEE> .§:o> £o> meﬁﬁﬂu oH.H.H  mmummo Ema ohm: mmo>_zm<u mmozmmmm o:HEbEHsHEe Houwebom Emﬁ unuﬁeo o:oHEEE
@ /

mu
    m\m>:E.H

‘ ‘
9.22 n m $5.30

9% g @ ass

mamé um

%.§ 6 n I

FIG. 2 — Exemple de coupes dans une ontologie.

38

Indcxation scmantiquc par coupcs dc rcdondancc minimalc

dcs autrcs. Parmi1’cnscmb1c dc ccs dictionnaircs, lcs dcux suivants sont utiliscs pour constitucr
1’onto1ogic:

lc dictionnaire dcs mots anglais, qui rasscmblc lcs informations morphologiqucs (prononcia—

tion, dccoupagc syllabiquc, inﬂcxion, ...) ct syntaxiqucs (catcgoric morpho—syntaxiquc,
dcnombrabilitc, ﬂcxions, ...) pour un pcu plus dc 240’000 graphics diffcrcntcs (corrcs—
pondant a m 420’000 mots), ct pcrmct dc rclicr ccs graphics avcc lcs informations du
dictionnairc dcs conccpts. Les graphics dc cc dictionnairc sont principalcmcnt (mais pas
cxclusivcmcnt) dcs lcmmcs ; i1 comportc cgalcmcnt un nombrc important dc mu1ti—tcrmcs
(> 113’000), ﬁgurant dcs mots composcs ct cxprcssions idiomatiqucs.

lc dictionnaire dcs conccpts, qui dccrit a pcu prcs 490’000 conccpts, organiscs hicrarchiquc—

mcnt cntrc cux sclon dcs rclations d’hyponymic/hypcronymic (chaquc conccpt pouvant
avoir plusicurs hyponymcs ct hypcronymcs). Un ccrtain nombrc dc rclations scmantiqucs
binaircs supplcmcntaircs (tcllcs quc objct—action, agcnt—action, agcnt—but) sont par aillcurs
dccritcs, mais nous nc lcs utilisons pas ici. Rcmarquons qu’un nombrc important dc
conccpts (cnviron la moitic) nc sont pas dircctcmcnt assocics a dcs mots; ccs conccpts
nc pcuvcnt étrc dcﬁnis ct apprchcndcs qu’au travcrs dc lcurs rclations avcc lcs autrcs
conccpts.

Lc systcmc dc RD utilisc cst 1c modclc Vcctoricl SMART, combinc a un lcmmatiscur cxtcmc7,
qui fait cgalcmcnt ofﬁcc dc scgmcntcur (tokenizer) ct d’ctiquctcur morpho—syntaxiquc. Un
ﬁltragc par catcgoric grammaticalc cst rcaliscs (nc sont conscrvcs quc lcs noms, adjcctifs ct
Vcrbcs), mais nous n’uti1isons pas d’anti—dictionnairc ct nc faisons pas dc ﬁltragc frcqucnticl.

Lcs transformations du jcu d’indcxation sont obtcnucs cn prctraitant lcs donnccs soumiscs au
systcmc dc RD:

1.

cn prcmicr licu, lcs divcrscs informations tcxtucllcs (principalcmcnt titrc ct contcnu) dcs
documcnts sont agrcgccs, ct lcs autrcs informations (autcurs, sourccs, ctc.) supprimccs;
documcnts ct rcquétcs sont cnsuitc scgmcntcs ct lcmmatiscs;

on chcrchc cnsuitc lcs corrcspondanccs cntrc lcs mots contcnus dans lcs documcnts ct
ccux dccrits dans 1’onto1ogic; on tcntc d’ctab1ir cn prioritc unc corrcspondancc avcc la
graphic, ct s’i1n’y cn a pas, avcc sa formc lcmmatiscc; lcs mots sans corrcspondancc sont
indcxcs dc manicrc traditionncllc; lcs taux dc couvcrturcg sur lcs diffcrcntcs collcctions
sont dc 1’ordrc dc 90%.

on proccdc cnsuitc a1’cxpansion dc la hicrarchic dcs conccpts rclatifs aux mots conscrvcs
pour 1’cnscmb1c dcs documcnts; sclon lcs diffcrcnts cas cxpcrimcntcs, on prcndra soit la
totalité des concepts possiblcs (cn tablant sur un rcnforccmcnt mutucl dcs conccpts « cor-
rccts >> induit par lcs multiplcs co—occurrcnccs), soit uniqucmcnt 1c concept le plus pro-
bable (dans 1’abso1u pour 1c mot donnc — ccttc information cst prcscntc dans 1’onto1ogic
utiliscc) ;

on dctcrminc cnsuitc unc coupc optimalc sclon 1c critcrc C H, au moycn dc 1’a1gorithmc
CRM prcscntc cn scction 2.2;

ﬁnalcmcnt, on substituc lcs mots dcs documcnts ct dcs rcquétcs par lcs idcntiﬁcatcurs dcs
conccpts dc la coupc qui lcs subordonncnt.

7 Lc systcmc Sylcx 1.7 (© 1993-98 DECAN INGENIA).
8 Par « couvermre », on désignc la fraction dcs occurrences dcs mots couvcrts par 1’onto1ogic.

39

Seydoux F. & Chappelier J .—C.

| mesure | (a) (b) (c) (d)
corpus ADI (82 documents)

taille index 1800 14748 10099 1292
tous les concepts, tf.idf precision 0.3578 0.3134 0.3356 0.2458

rappel 0.69 84 0.7126 0.7406 0.6017
tous les concepts, sans precision 0.2497 0.1219 0.2550 0.1607
ponderation rappel 0.5996 0.3452 0.6708 0.5130

taille index 1800 5255 2888 658

°°“°ep‘ 1° plus precision 0.3578 0.4060 0.4274 0.2052

pmbable’ ‘fjdf rappel 0.6984 0.7306 0.7217 0.5200
concept + probable, precision 0.2497 0.1376 0.2939 0.1466
sans ponderation rappel 0.5996 0.3727 0.7141 0.4911

corpus TIME (423 documents)
taille index 21815 93707 70091 6760
tous les concepts, tf.idf precision 0.5496 0.4231 0.4536 0.2683

rappel 0.8901 0.7642 0.8036 0.6026
tous les concepts, sans precision 0.3288 0.0337 0.2353 0.0370
ponderation rappel 0.7755 0.1021 0.5709 0.1387

tailleindex 21815 53140 31612 4814

°°“°ep”° plus precision 0.5496 0.5143 0.5565 0.2729

probable, tf.idf

rappel 0.8901 0.8760 0.9053 0.5162
concept + probable, precision 0.3288 0.0346 0.3692 0.0372
sans ponderation rappel 0.7755 0.1201 0.7590 0.1322

corpus MED (1033 documents)
taille index 11893 51712 38524 4078
tous les concepts, tf.idf precision 0.4607 0.3029 0.2996 0.2336

rappel 0.5547 0.3903 0.3794 0.3142
tous les concepts, sans precision 0.3623 0.0105 0.1905 0.0229
ponderation rappel 0.4574 0.0246 0.2749 0.05 13

taille index 11893 30284 18109 2888

°°“°°p‘ 1° plus precision 0.4607 0.4266 0.4518 0.0743

pr°b“‘b1e’ ‘fjdf rappel 0.5547 0.5169 0.5404 0.1042
concept + probable, precision 0.3623 0.0105 0.3229 0.0132
sans ponderation rappel 0.4574 0.0313 0.4230 0.0368

corpus CIS1 (1460 documents)
taille index 10019 53453 39544 3516
tous les concepts, tf.idf precision 0.1733 0.1043 0.1139 0.0740

rappel 0.2318 0.1627 0.1675 0.1294
tous les concepts, sans precision 0.0687 0.0232 0.0569 0.0282
ponderation rappel 0.1239 0.0376 0.0963 0.0492

taille index 10019 26246 14993 1894

°°“°ep”ep1“S precision 0.1733 0.1590 0.1825 0.0602

pmbable’ ‘fjdf rappel 0.2318 0.2131 0.2313 0.0895
concept + probable, precision 0.0687 0.0201 0.0805 0.0221
sans ponderation rappel 0.1239 0.0403 0.1300 0.0435

corpus CACM (3204 documents)
taille index 10053 51712 38524 4078
tous les concepts, tf.idf precision 0.2865 0.1293 0.1935 0.1089

rappel 0.4534 0.2579 0.3617 0.1999
tous les concepts, sans precision 0.1555 0.0133 0.1447 0.0320
ponderation rappel 0.3082 0.0306 0.2549 0.0699

taille index 10053 25207 14681 2670

°°“°ep”° plus precision 0.2865 0.2358 0.2804 0.0645

pmbable’ ‘ﬁidf rappel 0.4534 0.3834 0.4567 0.1090
concept + probable, precision 0.1555 0.0230 0.1472 0.0245
sans ponderation rappel 0.3082 0.0302 0.2926 0.0385

TAB. 2 — Résultats des différentes experiences sur différents corpus. (a) : mots uniquement; (b) :
mots + concepts ; (c) : hyperonymes directs et (d) : hyperonymes dans CRM (cf aussi ﬁg. 1).
40

Indexation semantique par coupes de redondance minimale

On trouvera dans la table 2 les Valeurs de precision (« ]1—pt prec ») et de rappel (« 30 doc »)9
foumies par le systeme SMART. Toutes les experiences sont par ailleurs conduites en utilisant
soit le schema de ponderation classique (« tf.idf >>), soit sans ponderation.

On constate que l’indexation par hyperonymes directs obtient des resultats sensiblement egaux
au systeme de base, mais pour un rappel plus eleve. L’ indexation par CRM degrade par contre
les performances.

4 Conclusion

Les resultats obtenus sur ces experiences ne sont malheureusement pas concluants quant a l’uti—
lisation du critere CRM pour l’indexation semantique. Cependant, plusieurs remarques sont a
apporter :

— Le critere utilise ici ne permet pas de selectionner, ni meme d’inﬂuencer, le niveau de profon—

deur dans l’ ontologie de la coupe obtenue. Au Vu de la reduction drastique du jeu d’indexation
et des mauvaises performances obtenues, il semble que ce critere, ou du moins l’heuristique
implementee, selectionne une coupe situee trop haut dans la hierarchie, ce qui a comme
consequence evidente de faire baisser la precision. La bonne performance de la coupe au ni-
veau des concept hyperonymes directs nous permet de croire qu’il doit y avoir un niveau plus
adapte, plus proche des feuilles, pour la CRM.
On pourrait par exemple limiter considerablement l’espace de recherche de la coupe ideale
en empéchant de considerer des noeuds situes << trop hauts >> dans la hierarchie. Une piste a
explorer pour ameliorer tant l’adequation de la coupe selectionnee avec un processus d’in—
dexation que la recherche de cette coupe elle—méme consisterait a explorer les gains possibles
en terme de redondance a partir de la coupe uniquement constituee de feuilles, et en dirigeant
la recherche Vers le haut de la hierarchie, plutot que de haut en has a partir de la racine,
comme dans l’heuristique presentee ici.

— Par ailleurs, en conservant l’idee d’une action sur le jeu d’indexation lui—méme, il serait in-
teressant d’examiner de quelle maniere les ponderations (e.g. « tf.idf »), utilisees uniquement
lors de la recherche des documents proprement dite, devraient étre prises en compte lors de
la determination de la coupe.

— Finalement, les resultats presentes ici restent a corroborer avec ceux a obtenir avec d’ autres
ontologies, en particulier WordNet, qui a une structure assez differente d’EDR.

Pour terminer, soulignons que l’interét de la technique presentee depasse le cadre de la stricte
recherche documentaire. Celle—ci pourrait en effet s’ averer utile, et peut étre meme plus promet—
teuse, pour d’autres domaines d’application tels que la classiﬁcation de documents ou le resume
automatique.

Références

DEERWESTER S. C., DUMAIS S. T., LANDAUER T. K., FURNAS G. W. & HARSHMAN R. A. (1990).
Indexing by latent semantic analysis. Journal of the American Society of Information Science, 41(6),

9 I1 s’agit la de mesures standard: la « II-pt precision >> est la moyenne des precisions pour les taux de rappels
0.0, 0.1, ..., 1.0, ou la precision au taux de rappel 0.0 est la precision maximale obtenue sur l’ensemble des
documents pertinents retrouves ; le « rappel 30 doc >> est le taux de rappel apres 30 documents retournes.

41

42

Seydoux F. & Chappelier J .—C.

391-407.

GONZALO J ., VERDEJO F., CHUGUR I. & CIGARRAN J. (1998a). Indexing with WordNet synsets can
improve text retrieval. In Proc. of the COLING/ACL 1998 Workshop on Usage of WordNet for Natural
Language Processing, p. 38-44.

GONZALO J ., VERDEJO F., PETERS C. & CALZOLARI N. (1998b). Applying EuroWordNet to multi-
lingual text retrieval. Journal of Computers and the Humanities, 32(2—3), 185-207.

HARMAN D. (1988). Towards interactive query expansion. In Proc. of the I 1th Annual Int. ACM—SIGIR
Conference on Research and development in information retrieval, p. 321-331.

HOFMANN T. (1999). Probabilistic latent semantic indexing. In proc. of the 22th International Confe-
rence on Research and Development in Information Retrieval (SIGIR), p. 50-57.

IDE N. & VERONIS J. (1998). Word sense disambiguation: The state of the art. Computational Linguis-
tics, 24(1), 1-40.

LI H. (1998). A probabilistic approach to lexical semantic knowledge acquisition and structural disam-
biguation. Master’s thesis, Graduate School of Science, University of Tokyo.

MIHALCEA R. & MOLDOVAN D. (2000). Semantic indexing using WordNet senses. In Proc. of ACL
Workshop on IR & NLP.

MIYOSHI H., AMD M. KOBAYASHI K. S. & OGINO T. (1996). An overview of the EDR electronic
dictionary and the current status of its utilization. In Proc. of COLING, p. 1090-1093.

MOLDOVAN D. I. & MIHALCEA R. (2000). Using wordnet and lexical operators to improve intemet
searches. IEEE Internet Computing, 4(1), 34-43.

RICHARDSON R. & SMEATON A. F. (1995). Using WordNet in a Knowledge—Based Approach to Infor-
mation Retrieval. Rapport interne CA—0395, Dublin City University, Glasnevin, Dublin 9, Ireland.
SALTON G. (1968). Automatic Information Organization and Retrieval. McGraw—Hill.

SALTON G. (1971). The SMART Retrieval System - Experiments in Automatic Document Processing.
Prentice Hall.

SHANNON C. E. (1948). A mathematical theory of communication. The Bell System Technical Journal,
27, 379-423.

SMEATON A. F. & QUIGLEY I. (1996). Experiments on using semantic distances between words in
image caption retrieval. In Proc. of 19th Int. Conf on Research and Development in Information Retrie-
val, p. 174-180.

VOORHEES E. M. (1993). Using WordNet to disambiguate word senses for text retrieval. In Proc.
of 16th Annual International ACM—SIGIR Conference on Research and Development in Information
Retrieval, p. 171-80.

VOORHEES E. M. (1994). Query expansion using lexical—semantic relations. In Proc. 17th Annual Int.
ACM—SIGIR Conf on Research and Development in Information Retrieval, p. 61-69.

VOORHEES E. M. (1998). Using WordNet for text retrieval. In C. FELLBAUM, Ed., WordNet.‘ An
Electronic Lexical Database, chapter 12, p. 285-303. MIT Press.

WHALEY J. M. (1999). An Application of Word Sense Disambiguation to Information Retrieval. Rap-
port inteme PCS—TR99—352, Dartmouth College, Computer Science, Hanover, NH.

WILKS Y. & STEVENSON M. (1998). Word sense disambiguation using optimised combinations of
knowledge sources. In Proc. of the 17th Int. Conf on Computational Linguistics, p. 1398- 1402.

