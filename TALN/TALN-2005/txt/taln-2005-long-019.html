<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
"http://www.w3.org/TR/html4/loose.dtd">
<html><head><title>Production automatique du r&#233;sum&#233; de textes juridiques: &#233;valuation de qualit&#233; et d'acceptabilit&#233;</title>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
</head>
<body>
<div style="page-break-before:always; page-break-after:always"><div><p>TALN 2005, Dourdan, 6&#8211;10 juin 2005
</p>
<p>Production automatique du r&#233;sum&#233; de textes juridiques:
&#233;valuation de qualit&#233; et d&#8217;acceptabilit&#233;
</p>
<p>Atefeh Farzindar et Guy Lapalme
RALI
</p>
<p>D&#233;partement d&#8217;informatique et de recherche op&#233;rationnelle
Universit&#233; de Montr&#233;al
</p>
<p>C.P. 6128, succursale Centre-ville
Montr&#233;al, Qu&#233;bec, Canada H3C 3J7
</p>
<p>{farzinda, lapalme}@iro.umontreal.ca
</p>
<p>Mots-clefs : r&#233;sum&#233; automatique, fiches de r&#233;sum&#233;, textes juridiques, &#233;valuation d&#8217;un
r&#233;sum&#233;.
</p>
<p>Keywords: automatic text summarization, summary table, legals texts, evaluation of
a summary.
</p>
<p>R&#233;sum&#233;- Abstract
</p>
<p>Nous d&#233;crivons un projet de production de r&#233;sum&#233; automatique de textes pour le domaine ju-
ridique pour lequel nous avons utilis&#233; un corpus des jugements de la cour f&#233;d&#233;rale du Canada.
Nous pr&#233;sentons notre syst&#232;me de r&#233;sum&#233; LetSum ainsi que l&#8217;&#233;valuation des r&#233;sum&#233;s produits.
L&#8217;&#233;valuation de 120 r&#233;sum&#233;s par 12 avocats montre que la qualit&#233; des r&#233;sum&#233;s produits par
LetSum est comparable avec celle des r&#233;sum&#233;s &#233;crits par des humains.
</p>
<p>We describe an automatic text summarisation project for the legal domain for which we use
a corpus of judgments of the federal court of Canada. We present our summarization system,
called LetSum and the evaluation of produced summaries. The evaluation of 120 summaries by
12 lawyers shows that the quality of the summaries produced by LetSum is approximately at
the same level as the summaries written by humans.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Atefeh Farzindar et Guy Lapalme
</p>
<p>1 Introduction
</p>
<p>La jurisprudence est une r&#233;f&#233;rence importante pour les juristes. Pour cette raison les juristes
consultent quotidiennement des milliers de documents juridiques. De jour en jour, la masse
d&#8217;information textuelle sous forme de jurisprudence accessible sur internet ou dans les bases
de donn&#233;es des entreprises et des gouvernements ne cesse d&#8217;augmenter. Ce qui n&#233;cessite le
d&#233;veloppement des outils sp&#233;cifiques afin de pouvoir acc&#233;der au contenu des textes. Le but
d&#8217;un r&#233;sum&#233; d&#8217;un jugement est d&#8217;abord de livrer l&#8217;essence du texte clairement et avec concision
pour permettre une consultation facile et rapide; il doit fournir suffisamment d&#8217;informations sur
le jugement pour permettre au lecteur de d&#233;cider si celui-ci peut &#234;tre pertinent &#224; sa recherche.
Actuellement, des jugements sont r&#233;sum&#233;s manuellement par les professionnels ce qui est tr&#232;s
co&#251;teux.
</p>
<p>Notre approche au r&#233;sum&#233; automatique a l&#8217;avantage de fournir des moyens clairs de concevoir
des documents juridiques en fonction de r&#233;sum&#233;s courts pour diff&#233;rents types d&#8217;utilisateurs: des
&#233;tudiants, des avocats et des juges.
Le domaine juridique est un domaine ayant un grand besoin de r&#233;sum&#233;s mais avec des exigences
sp&#233;cifiques. Dans ce projet, nous nous sommes int&#233;ress&#233;s au traitement des d&#233;cisions des cours
judiciaires du Canada. Nous avons collabor&#233; avec les avocats du Centre de Recherche en Droit
Public (CRDP), charg&#233;s de cr&#233;er la biblioth&#232;que de droit virtuelle des d&#233;cisions judiciaires
canadiens CanLII 1.
</p>
<p>Dans cet article, nous d&#233;crivons plut&#244;t les aspects qualitatifs de l&#8217;&#233;valuation d&#8217;un r&#233;sum&#233; que la
m&#233;thodologie de la production de r&#233;sum&#233; automatique. &#192; la section 2, nous rappelons notre ap-
proche de production automatique de r&#233;sum&#233; de jugements et son l&#8217;implantation, LetSum (Legal
Text Summarizer). La section 3 pr&#233;sente les &#233;valuations effectu&#233;es avec LetSum. L&#8217;&#233;valuation
de 120 r&#233;sum&#233;s automatiques par 12 avocats montre que la qualit&#233; des r&#233;sum&#233;s produits par Let-
Sum est excellente. La comparaison des r&#233;sum&#233;s de LetSum avec cinq syst&#232;mes de recherche
ou commerciaux montre l&#8217;int&#233;r&#234;t d&#8217;utiliser d&#8217;un syst&#232;me de r&#233;sum&#233; sp&#233;cialis&#233; pour le domaine
juridique.
</p>
<p>2 R&#233;sum&#233; de textes juridiques
Notre m&#233;thode a &#233;t&#233; d&#233;velopp&#233;e suite &#224; une analyse manuelle de 75 jugements et de leurs r&#233;-
sum&#233;s r&#233;dig&#233;s par les r&#233;sumeurs professionnels. Nous avons d&#233;j&#224; pr&#233;sent&#233; la probl&#233;matique
(Farzindar, 2004) et notre m&#233;thode pour capturer la structuration th&#233;matique des documents et
identifier les unit&#233;s textuelles saillantes (Farzindar et al., 2004). Nous identifions d&#8217;abord le
plan d&#8217;organisation d&#8217;un jugement et ses diff&#233;rents th&#232;mes discursifs qui regroupent les phrases
autour d&#8217;un m&#234;me sujet. Chaque phrase dans un th&#232;me donne des informations compl&#233;men-
taires sur le sujet. Pour les phrases reli&#233;es &#224; un th&#232;me, nous pouvons en interpr&#233;ter le sens
d&#8217;apr&#232;s leur contexte afin d&#8217;en extraire les id&#233;es cl&#233;s.
</p>
<p>1Canadian Legal Information Institute http://www.canlii.org</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Production automatique du r&#233;sum&#233; de textes juridiques
</p>
<p>La cr&#233;ation du r&#233;sum&#233; par LetSum se fait en quatre &#233;tapes d&#233;crites en d&#233;tail dans (Farzindar,
2005).
</p>
<p>Segmentation th&#233;matique qui d&#233;termine l&#8217;organisation du document original et relie les seg-
ments du texte associ&#233;s avec des sept th&#232;mes suivants:
</p>
<p>&#8226; DONN&#201;ES DE LA D&#201;CISION: donne la r&#233;f&#233;rence compl&#232;te de la d&#233;cision et la rela-
tion entre les parties sur le plan juridique.
</p>
<p>&#8226; INTRODUCTION: qui? a fait quoi? &#224; qui?
&#8226; CONTEXTE: recompose l&#8217;histoire du litige et l&#8217;histoire judiciaire.
&#8226; SOUMISSION: pr&#233;sente le point de vue d&#8217;une partie sur le probl&#232;me.
&#8226; QUESTIONS DE DROIT: identifie le probl&#232;me juridique dont le tribunal est saisi.
&#8226; RAISONNEMENT JURIDIQUE: d&#233;crit l&#8217;analyse du juge, la d&#233;termination des faits et
</p>
<p>l&#8217;expression des motifs de la solution retenue.
&#8226; CONCLUSION: pr&#233;sente la d&#233;cision finale de la cour.
</p>
<p>Selon nos observations, quatre th&#232;mes jouent les r&#244;les principaux: INTRODUCTION,
CONTEXTE, RAISONNEMENT JURIDIQUE et CONCLUSION. La pr&#233;sence de ces quatre
th&#232;mes dans le jugement et dans le r&#233;sum&#233; est obligatoire. Dans la structure du r&#233;sum&#233;,
nous pr&#233;servons ces quatre th&#232;mes et nous extrayons les phrases qui leur appartiennent.
Le th&#232;me QUESTIONS DE DROIT est optionnel dans le jugement.
</p>
<p>Filtrage qui identifie les segments qui peuvent &#234;tre supprim&#233;s dans les documents, sans perdre
les informations pertinentes pour le r&#233;sum&#233;. Dans un jugement, les citations occupent un
volume important du texte soit 30% du jugement, alors que leur contenu est moins impor-
tant pour le r&#233;sum&#233;. Nous identifions les citations principalement pour les supprimer en
ne conservant que leurs r&#233;f&#233;rences juridiques. En plus, le th&#232;me SOUMISSION contenant
des discours des avocats, identifi&#233; par le segmenteur th&#233;matique, sera &#233;limin&#233; dans cette
&#233;tape.
</p>
<p>S&#233;lection des unit&#233;s textuelles candidates pour le r&#233;sum&#233; qui construit une liste d&#8217;unit&#233;s sail-
lantes pour chaque niveau structural du r&#233;sum&#233; en calculant les poids pour chaque phrase
dans le jugement. La s&#233;lection est bas&#233;e sur des r&#232;gles s&#233;mantiques et des mesures statis-
tiques.
</p>
<p>Production du r&#233;sum&#233; qui choisit les unit&#233;s pour le r&#233;sum&#233; final et les combine afin de pro-
duire un r&#233;sum&#233; repr&#233;sentant au maximum 15% du jugement. Le crit&#232;re de s&#233;lection des
unit&#233;s est bas&#233; sur l&#8217;importance du segment th&#233;matique contenant les unit&#233;s candidates.
</p>
<p>La pr&#233;sentation du r&#233;sum&#233; final est sous forme d&#8217;une fiche de r&#233;sum&#233; contenant des rubriques
homog&#232;nes d&#8217;informations. Cette fiche pr&#233;sente les informations consid&#233;r&#233;es importantes as-
soci&#233;es &#224; des th&#232;mes pr&#233;cis, ce qui en facilite la lecture et la navigation entre le r&#233;sum&#233; et le
jugement source. Pour chaque phrase du r&#233;sum&#233; produit, l&#8217;utilisateur peut en d&#233;terminer le sujet
en regardant le th&#232;me associ&#233; &#224; son segment th&#233;matique. La figure 1 montre un exemple de sor-
tie de LetSum comme une fiche de r&#233;sum&#233;. Cette fiche de r&#233;sum&#233; montre les th&#232;mes identifi&#233;s
dans le jugement qui &#233;taient pertinents pour le r&#233;sum&#233;. La taille du r&#233;sum&#233; est de 10% de celle
du jugement original (le document source a dix pages).
Dans les prochaines sections, nous pr&#233;sentons l&#8217;&#233;valuation des r&#233;sum&#233;s g&#233;n&#233;r&#233;s par LetSum.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Atefeh Farzindar et Guy Lapalme
</p>
<p>Figure 1: Fiche de r&#233;sum&#233; produit par LetSum, compos&#233; de 350 mots alors que le jugement
source avait 4500 mots
</p>
<p>3 &#201;valuation
La comparaison avec un r&#233;sum&#233; mod&#232;le comme r&#233;f&#233;rence pour des r&#233;sum&#233;s automatiques est
tr&#232;s naturelle, mais des r&#233;sum&#233;s r&#233;dig&#233;s par des personnes diff&#233;rentes ne sont pas toujours con-
vergents au niveau du contenu. La r&#233;daction d&#8217;un r&#233;sum&#233; demande une analyse du texte pour
en d&#233;gager les id&#233;es, les arguments, le style et les th&#232;mes. Les r&#233;dacteurs humains d&#233;gagent les
affirmations essentielles du document et les expriment dans leur propre style, ce qui donne lieu
&#224; plusieurs r&#233;sum&#233;s pour le m&#234;me document. Il est donc difficile de d&#233;finir une m&#233;trique claire
pour juger diff&#233;rents aspects d&#8217;un r&#233;sum&#233; comme la compl&#233;tude, la th&#233;matique et la coh&#233;rence.
Plusieurs campagnes d&#8217;&#233;valuation de syst&#232;mes de r&#233;sum&#233; comme SUMMAC2 (Mani et al.,
1998) et DUC3 (organis&#233; par NIST) ont montr&#233; l&#8217;importance de d&#233;finir des mesures pour
l&#8217;&#233;valuation d&#8217;un r&#233;sum&#233;. Spark Jones et Galliers (Spark-Jones &amp; Galliers, 1995) ont pro-
pos&#233; de diviser les &#233;valuations en deux types: intrins&#232;que et extrins&#232;que. L&#8217;&#233;valuation intrin-
s&#232;que mesure les propri&#233;t&#233;s concernant la nature du sujet &#224; &#233;valuer et son objectif, alors que
</p>
<p>2TIPSTER Text Summarization Evaluation Conference
3Document Understanding Conferences http://www-nlpir.nist.gov/projects/duc</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Production automatique du r&#233;sum&#233; de textes juridiques
</p>
<p>l&#8217;&#233;valuation extrins&#232;que mesure les aspects concernant les impacts et les effets de sa fonction.
Nous avons &#233;valu&#233; LetSum avec ces deux types d&#8217;&#233;valuations.
</p>
<p>Nos r&#233;sum&#233;s du syst&#232;me ont &#233;t&#233; &#233;valu&#233;s en deux &#233;tapes: nous avons d&#8217;abord &#233;valu&#233; les modules
du syst&#232;me s&#233;par&#233;ment, ensuite nous avons mesur&#233; la qualit&#233; globale des r&#233;sum&#233;s produits.
Nous avons &#233;galement compar&#233; les r&#233;sum&#233;s de LetSum avec des r&#233;sum&#233;s produits par quatre
autres syst&#232;mes et des r&#233;sum&#233;s manuels.
</p>
<p>Pour l&#8217;&#233;valuation des modules de LetSum, nous avons utilis&#233; une &#233;valuation intrins&#232;que &#224; trois
niveaux: la qualit&#233; des divisions en th&#232;mes par le segmenteur th&#233;matique, la d&#233;tection correcte
des citations par le module de filtrage, et le contenu des unit&#233;s s&#233;lectionn&#233;es par le module de
s&#233;lection et production.
</p>
<p>Comme &#233;valuation intrins&#232;que, nous avons utilis&#233; ROUGE (Recall-Oriented Understudy for
Gisting Evaluation) (Lin &amp; Hovy, 2003). ROUGE est maintenant bien reconnue comme mesure
d&#8217;&#233;valuation des r&#233;sum&#233;s et a &#233;t&#233; utilis&#233;e pour la premi&#232;re fois dans la comp&#233;tition de DUC 2004
comme seule mesure de fiabilit&#233; pour certaines t&#226;ches. ROUGE est bas&#233; sur le calcul statistique
de co-occurrence de n-grammes. Cette m&#233;thode dont les r&#233;sultats sont bien corr&#233;l&#233;s avec les
jugements humains permet d&#8217;optimiser les syst&#232;mes et d&#8217;acc&#233;l&#233;rer leur &#233;valuation. ROUGE
comporte deux m&#233;thodes d&#8217;&#233;valuation. ROUGE-N, dont le score est bas&#233; sur le nombre de
n-grammes (normalement 1 &#8804; n &#8804; 4) communs entre le r&#233;sum&#233; automatique est le r&#233;sum&#233;
mod&#232;le. Par exemple, ROUGE-2 calcule le nombre de paires de mots successifs communs
entre les r&#233;sum&#233;s candidat et mod&#232;le. La deuxi&#232;me est ROUGE-L, qui consid&#232;re les phrases
comme une suite des s&#233;quences des mots. Cette &#233;valuation calcule la plus longue sous-s&#233;quence
commune des mots afin d&#8217;estimer la similarit&#233; entre deux r&#233;sum&#233;s.
</p>
<p>Pour l&#8217;&#233;valuation extrins&#232;que de LetSum, nous avons demand&#233; &#224; des utilisateurs juristes de
juger le contenu des r&#233;sum&#233;s et leur acceptabilit&#233;. Pour chaque r&#233;sum&#233;, le recouvrement du
contenu sur les id&#233;es cl&#233;s du document a &#233;t&#233; &#233;valu&#233; par deux avocats.
</p>
<p>3.1 &#201;valuation des modules de LetSum
Nous avons &#233;valu&#233; les quatre modules de LetSum s&#233;par&#233;ment. Les deux premiers modules, seg-
mentation th&#233;matique et filtrage sont &#233;valu&#233;s s&#233;par&#233;ment, alors que les deux autres modules
de s&#233;lection des unit&#233;s pertinentes et production ont &#233;t&#233; &#233;valu&#233;s dans le cadre de l&#8217;&#233;valuation
des r&#233;sum&#233;s finals de LetSum. Nous avons compar&#233; les sorties de module de segmentation th&#233;-
matique avec le corpus que nous avons annot&#233; manuellement (avec validation d&#8217;un avocat du
CanLII).
Pour l&#8217;&#233;valuation du module de segmentation th&#233;matique, nous avons utilis&#233; un corpus de test
contenant 10 jugements de la cour f&#233;d&#233;rale. Ces jugements n&#8217;ont pas &#233;t&#233; utilis&#233;s pour entra&#238;ner
le syst&#232;me, ni servi &#224; la construction du dictionnaire des marqueurs. Pour l&#8217;&#233;valuation de ce
module les points consid&#233;r&#233;s importants sont: d&#233;tection des th&#232;mes, degr&#233; de pertinence d&#8217;un
th&#232;me pour un segment, couverture des segments th&#233;matiques, pr&#233;cision des fronti&#232;res entre
deux th&#232;mes. Pour cette &#233;valuation on peut calculer la pr&#233;cision et le rappel. La pr&#233;cision
mesure la proportion des unit&#233;s pertinentes parmi toutes les unit&#233;s produites par le syst&#232;me. Le
rappel mesure la proportion des unit&#233;s pertinentes parmi tous les unit&#233;s pertinentes. F-mesure
consid&#232;re les deux mesures ensemble. Nous avons obtenu une pr&#233;cision de 100% et un rappel
de 95% soit F-mesure 99% . Sur 40 th&#232;mes annot&#233;s dans le corpus, 38 th&#232;mes ont &#233;t&#233; identifi&#233;s
correctement.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Atefeh Farzindar et Guy Lapalme
</p>
<p>Pour l&#8217;&#233;valuation du module de filtrage de citations, nous avons utilis&#233; 15 jugements de la cour
f&#233;d&#233;rale qui n&#8217;ont pas servi &#224; entra&#238;ner le module de filtrage. Pour cette &#233;valuation, nous avons
compar&#233; les unit&#233;s de citations identifi&#233;es par le filtrage avec les citations annot&#233;es manuelle-
ment dans les jugements. Le r&#233;sultat d&#8217;&#233;valuation du module de filtrage est de 98% pour la
pr&#233;cision et 95% pour le rappel ce qui donne 96% pour la F-mesure. Sur 60 cas de citation, 57
unit&#233;s ont &#233;t&#233; identifi&#233;es correctement. Certaines citations n&#8217;&#233;taient pas identifi&#233;es correctement
&#224; cause la langue de r&#233;daction des r&#233;f&#233;rences. Dans les jugements canadiens, les juges citent
parfois les r&#233;f&#233;rences de droit tels quels peu importe qu&#8217;elles soient en anglais ou en fran&#231;ais.
Pour les citations en fran&#231;ais, lorsqu&#8217;il y a des marqueurs d&#8217;&#233;num&#233;ration, le syst&#232;me les identifie
mais en absence des marqueurs d&#8217;&#233;num&#233;ration, il ne peut pas les distinguer.
</p>
<p>3.2 &#201;valuation de LetSum par ROUGE
Pour le module de s&#233;lection et production, il faut mesurer les topiques extraits des documents
par le syst&#232;me. Il est possible d&#8217;aligner automatiquement les unit&#233;s de deux textes pour com-
parer la similarit&#233; entre les r&#233;sum&#233;s mod&#232;les et les r&#233;sum&#233;s produits afin de calculer la fraction
du r&#233;sum&#233; mod&#232;le exprim&#233;e dans le contenu du r&#233;sum&#233; produit par le syst&#232;me. Pour cette &#233;val-
uation des r&#233;sum&#233;s de LetSum, nous avons utilis&#233; ROUGE en les comparant avec des r&#233;sum&#233;s
mod&#232;les &#233;crits par des humains. Nous avons g&#233;n&#233;r&#233; 50 r&#233;sum&#233;s automatiques avec cinq sys-
t&#232;mes: syst&#232;me de recherche MEAD (Radev et al., 2003), un syst&#232;me de recherche et commer-
cial fran&#231;ais Pertinence Mining (Lehmam, 1995), un syst&#232;me commercial de Microsoft Word
(option de r&#233;sum&#233; dans MS Word) et une m&#233;thode StartEnd que nous avons d&#233;finie. Le Star-
tEnd est un syst&#232;me bas&#233; sur les positions des segments dans le document et LetSum afin de
comparer notre syst&#232;me avec d&#8217;autres syst&#232;mes de r&#233;sum&#233;s. Pour la m&#233;thode StartEnd, nous
avons mis au point cette approche suite &#224; nos analyses du corpus des r&#233;sum&#233;s manuels. Pour
d&#233;finir le StartEnd nous avons fait trois exp&#233;rimentations.
</p>
<p>D&#8217;apr&#232;s nos &#233;tudes, le d&#233;but du jugement situ&#233; &#224; la fin des DONN&#201;ES DE LA D&#201;CISION (nom de
la cour, lieu de l&#8217;audience, date, les r&#233;f&#233;rences et etc.) est une partie importante qui comprend
le d&#233;but du th&#232;me INTRODUCTION. Nous avons d&#233;fini un baseline qui prend 15% du d&#233;but du
texte. Ce baseline couvre des th&#232;mes INTRODUCTION et CONTEXTE.
</p>
<p>Un autre baseline prend 15% de la fin du jugement avant la signature du juge. Ce baseline
couvre les unit&#233;s des th&#232;mes RAISONNEMENT JURIDIQUE et CONCLUSION. Nos exp&#233;riences
avec ROUGE ont montr&#233; que le score de premier baseline &#233;tait plus &#233;lev&#233; que le deuxi&#232;me, ce
qui signifie l&#8217;importance du commencement du document par rapport &#224; sa fin.
</p>
<p>Cette exp&#233;rience, nous a conduit &#224; d&#233;finir une approche de r&#233;sum&#233; avec un taux de compression
15%, bas&#233; sur l&#8217;algorithme suivant: prendre 8% du d&#233;but du jugement et en compl&#233;tant la
derni&#232;re phrase si cette derni&#232;re a &#233;t&#233; coup&#233;e et prendre 4% de la fin du jugement en ajoutant la
premi&#232;re phrase compl&#232;te. Cette derni&#232;re approche, que nous avons nomm&#233;e StartEnd est donc
assez appropri&#233;e pour les documents de style juridique m&#234;me si son impl&#233;mentation est assez
simple.
</p>
<p>Nous avons compar&#233; avec ROUGE les r&#233;sum&#233;s de LetSum, StartEnd et ceux de trois autres
syst&#232;mes avec les r&#233;sum&#233;s humains. Les r&#233;sultats de l&#8217;&#233;valuation sont montr&#233;s &#224; la table 1.
Un score plus &#233;lev&#233; est meilleur et indique un syst&#232;me plus performant. LetSum est class&#233;
au premier rang avec les meilleures notes d&#8217;&#233;valuation. D&#8217;apr&#232;s cette &#233;valuation, le deuxi&#232;me
syst&#232;me est StartEnd, ce qui montre l&#8217;importance de l&#8217;&#233;tude sur les documents des domaines</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Production automatique du r&#233;sum&#233; de textes juridiques
</p>
<p>Syst&#232;me ROUGE-1 ROUGE-2 ROUGE-3 ROUGE-4 ROUGE-L
LetSum 0.57500 0.31381 0.20708 0.15036 0.45185
StartEnd 0.47244 0.27569 0.19391 0.14472 0.34683
MEAD 0.45581 0.22314 0.14241 0.10064 0.32089
MsWord 0.44473 0.21295 0.13747 0.09727 0.29652
Per. Mining 0.32833 0.15127 0.09798 0.07151 0.22375
</p>
<p>Table 1: R&#233;sultat d&#8217;&#233;valuation intrins&#232;que avec ROUGE, LetSum a des meilleurs r&#233;sultats
</p>
<p>sp&#233;cifiques. Le fait qu&#8217;une approche simple puisse d&#233;passer des m&#233;thodes complexes de pro-
duction de r&#233;sum&#233; met en &#233;vidence la diff&#233;rence entre des organisations des documents et elle
montre aussi l&#8217;int&#233;r&#234;t de d&#233;velopper un syst&#232;me sp&#233;cifique pour un domaine. Il est plus en plus
difficile de produire un r&#233;sum&#233; g&#233;n&#233;ral pour tous types d&#8217;utilisateurs sans prise en compte du
besoin des usagers et de la t&#226;che demand&#233;e.
</p>
<p>3.3 &#201;valuation extrins&#232;que de LetSum
L&#8217;objectif de cette &#233;valuation est de mesurer l&#8217;utilit&#233; du r&#233;sum&#233; automatique par rapport &#224; un
r&#233;sum&#233; &#233;crit par un humain et de comparer la qualit&#233; des r&#233;sum&#233;s automatiques g&#233;n&#233;r&#233;s par
diff&#233;rents syst&#232;mes. Ce test est bas&#233; sur un jugement humain. Cette &#233;valuation est toutefois tr&#232;s
co&#251;teuse, parce qu&#8217;elle demande des ressources humaines et un temps consid&#233;rable.
</p>
<p>Pour cette &#233;valuation, nous avons utilis&#233; les r&#233;sum&#233;s automatiques produits par cinq syst&#232;mes
pr&#233;sent&#233;s &#224; la section pr&#233;c&#233;dente et les r&#233;sum&#233;s &#233;crits par des humains. Il faut noter que dans
cette &#233;valuation, nous n&#8217;avons consid&#233;r&#233; que les textes du r&#233;sum&#233;. Nous n&#8217;avons pas g&#233;n&#233;r&#233;
le format tabulaire d&#8217;organisation du r&#233;sum&#233; comme celui qui est pr&#233;sent&#233; &#224; la figure 1. Nous
voulions aussi normaliser l&#8217;apparence de la sortie de tous les syst&#232;mes pour ne pas influencer
les juges. Ce choix p&#233;nalise toutefois LetSum car nous ne tenons pas compte de la structure
th&#233;matique extraite par notre m&#233;thodologie. Les &#233;valuateurs ne savaient pas quels r&#233;sum&#233;s
avaient &#233;t&#233; produits par ordinateur et lesquels avaient &#233;t&#233; &#233;crits manuellement.
</p>
<p>Nous avons fait &#233;valuer 120 r&#233;sum&#233;s par les juristes. Le corpus de test contient dix jugements
choisis au hasard dans diff&#233;rentes collections de jugements de la Cour f&#233;d&#233;rale du Canada. Nous
avons g&#233;n&#233;r&#233; 50 r&#233;sum&#233;s automatiques et nous avons collect&#233; 10 r&#233;sum&#233;s manuels &#233;crits par les
arr&#234;tistes de la Cour f&#233;d&#233;rale. Pour chaque r&#233;sum&#233;, nous avons r&#233;p&#233;t&#233; le test deux fois, ceci
nous donne deux avis par r&#233;sum&#233;. Chacun des 12 avocats du CanLII a &#233;valu&#233; 10 r&#233;sum&#233;s sur
une p&#233;riode d&#8217;une heure sur deux aspects: contenu et qualit&#233;.
</p>
<p>Pour l&#8217;&#233;valuation du contenu, nous avons d&#233;fini sept points importants &#224; retrouver dans un
jugement. Si un lecteur peut d&#233;terminer les points en question en lisant le r&#233;sum&#233;, on en d&#233;duit
que le r&#233;sum&#233; contient suffisamment d&#8217;informations pour couvrir les id&#233;es cl&#233;s d&#8217;un jugement.
Sept questions (pr&#233;sent&#233;es en haut de la table 2) ont &#233;t&#233; d&#233;termin&#233;es avec l&#8217;aide d&#8217;un avocat de
CanLII. L&#8217;ensemble des r&#233;ponses de ces questions montre le degr&#233; de couverture sur des id&#233;es
cl&#233;s du jugement source exprim&#233;es dans le r&#233;sum&#233;. La deuxi&#232;me partie de l&#8217;&#233;valuation portait
sur la qualit&#233; d&#8217;un r&#233;sum&#233; selon trois crit&#232;res:
</p>
<p>Lisibilit&#233; : La facilit&#233; de distinction et de perception du contenu du r&#233;sum&#233; qui en facilite la
compr&#233;hension. Ce crit&#232;re donne une appr&#233;ciation globale du r&#233;sum&#233;. On demande si le</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Atefeh Farzindar et Guy Lapalme
</p>
<p>Apr&#232;s avoir lu le r&#233;sum&#233; peut-on d&#233;terminer:
Q1. Qui sont les parties en litige?
Q2. Quel est le probl&#232;me en litige?
Q3. Les questions de droit soulev&#233;es?
Q4. Comment le juge a appliqu&#233; le droit aux faits?
Q5. Les motifs couvrent-ils les questions de droit?
Q6. Le r&#233;sum&#233; contient-il les motifs d&#233;terminants pour arriver &#224; la conclusion?
Q7. Le r&#233;sultat final de la cour?
R&#233;sum&#233; Q1 Q2 Q3 Q4 Q5 Q6 Q7 Moyenne
Humain 55,00 90,00 90,00 70,00 80,00 85,00 95,00 80,71
LetSum 50,00 90,00 80,00 75,00 75,00 85,00 85,00 77,14
StartEnd 65,00 100,00 100,00 70,00 80,00 70,00 90,00 82,14
MEAD 55,00 100,00 95,00 65,00 50,00 50,00 40,00 65,00
MsWord 30,00 80,00 85,00 60,00 45,00 45,00 60,00 57,86
Per. Mining 25,00 65,00 55,00 35,00 35,00 35,00 45,00 42,14
Moyenne 46,67 87,50 84,17 62,50 60,83 61,67 69,17 67,50
</p>
<p>Table 2: Questions juridiques utilis&#233;es lors de l&#8217;&#233;valuation du contenu du r&#233;sum&#233; par les juristes
et les r&#233;sultats d&#8217;&#233;valuation extrins&#232;que, les pourcentages des r&#233;ponses positives pour les sept
questions juridiques
</p>
<p>r&#233;sum&#233; est: clair, assez clair, peu clair ou incompr&#233;hensible.
</p>
<p>Coh&#233;rence : La pr&#233;sence simultan&#233;e d&#8217;&#233;l&#233;ments qui correspondent au m&#234;me contenu ou qui
s&#8217;accordent entre eux, qui s&#8217;harmonisent. Ce crit&#232;re contient le fil conducteur du texte
pour en assurer la continuit&#233; et la progression de l&#8217;information. On demande si la co-
h&#233;rence du texte dans le r&#233;sum&#233; est: tr&#232;s bonne, bonne, m&#233;diocre ou tr&#232;s mauvaise.
</p>
<p>Pertinence des phrases : Caract&#232;re de ce qui est plus ou moins appropri&#233;, qui s&#8217;inscrit dans la
ligne de l&#8217;objectif poursuivi. La pertinence des phrases mesure si les phrases du r&#233;sum&#233;
contiennent un lien clair et direct avec le sujet dont il est question. On demande si le
r&#233;sum&#233; est: tr&#232;s pertinent, assez pertinent, peu pertinent ou non pertinent.
</p>
<p>L&#8217;&#233;valuation comporte aussi une valeur d&#8217;acceptabilit&#233; sur la qualit&#233; g&#233;n&#233;rale du r&#233;sum&#233;. Nous
avons demand&#233; d&#8217;attribuer une valeur d&#8217;acceptabilit&#233; entre 0 et 5 pour chaque r&#233;sum&#233; (0 pour
un r&#233;sum&#233; inacceptable et 5 pour un texte acceptable) sur la qualit&#233; du texte de r&#233;sum&#233;. Les
r&#233;sum&#233;s avec valeur 3 jusqu&#8217;&#224; 5 sont consid&#233;r&#233;s acceptables.
Dans la table 2, nous pr&#233;sentons les r&#233;sultats obtenus pour l&#8217;&#233;valuation des 120 jugements o&#249;,
pour chaque question, nous avons calcul&#233; le pourcentage de r&#233;ponses positives donn&#233;es &#224; cette
question. Une r&#233;ponse positive signifie que le r&#233;sum&#233; contient assez d&#8217;informations sur le point
en question. Par exemple dans la deuxi&#232;me colonne, les r&#233;sum&#233;s produits par le moyenne de
toutes les m&#233;thodes ont couvert les informations sur la pr&#233;sentation des parties en litige (Q1)
dans 47% des cas. LetSum a donc tr&#232;s bien r&#233;pondu aux exigences des avocats pour des r&#233;sum&#233;s
automatiques. Ses r&#233;sultats sont tr&#232;s proches de ceux des r&#233;sum&#233;s manuels et sa performance
est sup&#233;rieure &#224; celle des autres syst&#232;mes commerciaux Microsoft Word et Pertinence Mining,
y compris le syst&#232;me de recherche MEAD.
</p>
<p>Notre m&#233;thode StartEnd, bas&#233;e sur la position des segments, a &#233;galement donn&#233; de bons r&#233;sul-</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Production automatique du r&#233;sum&#233; de textes juridiques
</p>
<p>R&#233;sum&#233; Lisibilit&#233; 0-3 Coh&#233;rence 0-3 Pertinence 0-3 Acceptabilit&#233; 0-5
Humain 2,00 1,95 2,15 3,43
LetSum 2,30 2,30 2,25 3,43
StartEnd 2,40 2,20 2,10 3,68
MEAD 2,15 1,90 2,05 3,23
MsWord 1,65 1,25 1,60 2,63
Per. Mining 1,40 1,10 1,40 2,23
Moyenne 1,98 1,78 1,93 3,10
</p>
<p>Table 3: R&#233;sultats d&#8217;&#233;valuation extrins&#232;que selon les valeurs qualitatives entre 0 et 3 sur lisi-
bilit&#233;, coh&#233;rence et pertinence des phrases, valeur d&#8217;acceptabilit&#233; est entre 0 et 5 sur la qualit&#233;
g&#233;n&#233;rale du r&#233;sum&#233;
</p>
<p>tats. Notre heuristique pour les positions des segments &#233;tait appropri&#233;e, m&#234;me si elle diff&#232;re
du baseline utilis&#233; normalement pour les articles journaux. Par le comportement du syst&#232;me
MEAD, sp&#233;cialis&#233; pour les articles des journaux, on peut voir que les questions qui poss&#232;dent
les r&#233;ponses plac&#233;es au d&#233;but du document sont bien r&#233;pondues alors que le recouvrement des
informations cl&#233;s sur les questions avec r&#233;ponses dans d&#8217;autres positions dans le texte n&#8217;est
pas satisfaisant. Les syst&#232;mes commerciaux comme Microsoft Word et Pertinence Mining ont
les scores les plus faibles dans l&#8217;&#233;valuation, car ils produisent des r&#233;sum&#233;s g&#233;n&#233;riques qui ne
satisfont pas vraiment les utilisateurs dans un domaine sp&#233;cifique comme droit.
</p>
<p>La table 3 montre les r&#233;sultats de l&#8217;&#233;valuation de la qualit&#233; du r&#233;sum&#233;. Pour les trois crit&#232;res,
lisibilit&#233;, coh&#233;rence et pertinence des phrases du r&#233;sum&#233;, les valeurs sont entre 0 et 3. La qualit&#233;
des r&#233;sum&#233;s produits par LetSum est sup&#233;rieure &#224; celle des autres m&#233;thodes. La lisibilit&#233; du
r&#233;sum&#233; de LetSum est jug&#233; clair, la coh&#233;rence est &#233;valu&#233;e tr&#232;s bonne et les pertinences des
phrases sont mesur&#233;es tr&#232;s pertinentes pour les besoins des avocats. Les condens&#233;s r&#233;dig&#233;s
par un humain sont jug&#233;s bons en coh&#233;rence (et non pas tr&#232;s bons) parce qu&#8217;ils sont en style
t&#233;l&#233;graphique alors que LetSum et les autres syst&#232;mes font l&#8217;extraction de phrases.
</p>
<p>Au point de vue d&#8217;acceptabilit&#233; du r&#233;sum&#233;, les r&#233;sum&#233;s de LetSum sont jug&#233;s de niveau &#233;quiv-
alent &#224; celui des r&#233;sum&#233;s &#233;crits par les arr&#234;tistes des cours. Encore une fois la m&#233;thode de
positions des phrases dans le jugement des tr&#232;s bons scores pour ce crit&#232;re d&#8217;&#233;valuation. Il faut
noter que dans cette partie de l&#8217;&#233;valuation il y a peu de diff&#233;rence entre le syst&#232;me StartEnd et
LetSum, un syst&#232;me nettement plus &#233;labor&#233;. Ceci peut en partie s&#8217;expliquer par le fait que nous
n&#8217;avons pas consid&#233;r&#233; le format tabulaire produit par LetSum bas&#233; sur l&#8217;analyse th&#233;matique du
texte qui distingue notre m&#233;thode.
</p>
<p>4 Conclusion
</p>
<p>Le domaine juridique est un vaste domaine avec un grand besoin pour le r&#233;sum&#233; automatique.
Au Canada, il y a 30 000 avocats et aux &#201;tats-Unis plus de 300 000 avocats susceptibles de
rechercher de la jurisprudence. Toutes les synth&#232;ses de jurisprudence se font manuellement par
des juristes. Lors qu&#8217;un r&#233;sum&#233; est disponible, le juriste a une id&#233;e du contenu de la d&#233;cision et
il lui est plus facile de savoir si elle a un potentiel de pertinence. Chaque r&#233;sum&#233; peut sauver,
dans la consultation d&#8217;une liste de r&#233;sultats de recherche, deux minutes &#224; la personne qui fait la
recherche. Une recherche typique dans CanLII donne plus de trente r&#233;sultats, on pourrait donc</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Atefeh Farzindar et Guy Lapalme
</p>
<p>sauver une heure environ. Comme un avocat-recherchiste facture au moins 100$ de l&#8217;heure
&#224; son client, et que plusieurs recherches peuvent &#234;tre requises pour un seul dossier, pour 20
recherches, il y aura donc 20 heures d&#8217;&#233;conomies, 2 000$ sur un seul cas. L&#8217;utilisation des
r&#233;sum&#233;s automatiques &#233;conomise du temps, des co&#251;ts et des expertises. Ces &#233;conomies de
ressources prot&#232;gent les int&#233;r&#234;ts du gouvernement et de la population en tant que des clients
attendant de recevoir un service juridique.
Nous avons d&#233;velopp&#233; LetSum, le premier syst&#232;me complet pour le r&#233;sum&#233; de textes juridiques
en anglais. Il est bas&#233; sur l&#8217;identification de la structure th&#233;matique et pr&#233;sente le r&#233;sum&#233; sous
forme d&#8217;une fiche de r&#233;sum&#233; augmentant ainsi la coh&#233;rence et la lisibilit&#233; du r&#233;sum&#233;. Dans
les diff&#233;rentes &#233;tapes de notre &#233;tude, nous avons cherch&#233; &#224; maximiser la pr&#233;cision de notre
analyse en vue de diminuer les erreurs, car les textes de lois sont tr&#232;s pr&#233;cieux. L&#8217;excellente
&#233;valuation de LetSum est le t&#233;moin de la validit&#233; de notre approche. En faisant ressortir les
points essentiels des jugements, nous esp&#233;rons avoir rendu la justice plus accessible &#224; tous et
aussi aider la soci&#233;t&#233;.
</p>
<p>Remerciements
</p>
<p>Nous tenons &#224; remercier l&#8217;&#233;quipe LexUM du laboratoire d&#8217;informatique juridique du Centre de
recherche en droit public de la facult&#233; de droit de l&#8217;Universit&#233; de Montr&#233;al pour leur collabo-
ration. La recherche pr&#233;sent&#233;e ici est soutenu financi&#232;rement par le Conseil de recherches en
sciences naturelles et en g&#233;nie du Canada (CRSNG).
</p>
<p>R&#233;f&#233;rences
FARZINDAR A. (2004). D&#233;veloppement d&#8217;un syst&#232;me de r&#233;sum&#233; automatique de textes juridiques. In
TALN-RECITAL&#8217;2004, p. 39&#8211;44, F&#232;s, Maroc.
FARZINDAR A. (2005). R&#233;sum&#233; automatique de textes juridiques. PhD thesis, Universit&#233; de Montr&#233;al
et Universit&#233; de Paris4-Sorbonne.
FARZINDAR A., LAPALME G. &amp; DESCL&#201;S J.-P. (2004). R&#233;sum&#233; de textes juridiques par identifica-
tion de leur structure th&#233;matique. Traitement Automatique des Langues (TAL), Num&#233;ro sp&#233;cial sur: Le
r&#233;sum&#233; automatique de texte : solutions et perspectives, 45(1), 39&#8211;65.
LEHMAM A. (1995). Le r&#233;sum&#233; automatique &#224; fragments indicateurs: RAFI. PhD thesis, Universit&#233; de
Nancy-II, Nancy, France.
</p>
<p>LIN C.-Y. &amp; HOVY E. (2003). Automatic evaluation of summaries using n-gram co-occurrence statis-
tics. In Proceedings of 2003 Language Technology Conference (HLT-NAACL 2003), p. 150&#8211;157, Ed-
monton, Canada.
MANI I., HOUSE D., KLEIN G., HIRSHMAN L., ORBST L., FIRMIN T., CHRZANOWSKI M. &amp; SUND-
HEIM B. (1998). The TIPSTER SUMMAC Text Summarization Evaluation. Rapport interne MTR
98W0000138, The Mitre Corporation, McLean, Virginia.
RADEV D., OTTERBACHER J., QI H. &amp; TAM D. (2003). Mead reducs: Michigan at duc 2003. In
DUC03, p. 160&#8211;167, Edmonton, Alberta, Canada: Association for Computational Linguistics.
SPARK-JONES K. &amp; GALLIERS J. R. (1995). Evaluating Natural Language Processing Systems: An
Analysis and Review. Number 1083 in Lecture Notes in Artificial Intelligence. Springer.</p>

</div></div>
</body></html>