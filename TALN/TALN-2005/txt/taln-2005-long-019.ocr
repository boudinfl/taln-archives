TALN 2005, Dourdan, 6-10 juin 2005

Production automatique du résumé de textes juridiques:
évaluation de qualité et d’acceptabilité

Atefeh Farzindar et Guy Lapalme
RALI
Département d’informatique et de recherche opérationnelle
Université de Montreal
CR 6128, succursale Centre-Ville
Montreal, Quebec, Canada H3C 3J7
{farzinda, lapalme} @iro.umontreal.ca

Mots-clefs I résumé automatique, ﬁches de résumé, textes juridiques, évaluation d’un
résumé.

Keywords: automatic text summarization, summary table, legals texts, evaluation of
a summary.

Résumé- Abstract

Nous décrivons un projet de production de résumé automatique de textes pour le domaine ju-
ridique pour lequel nous avons utilisé un corpus des jugements de la cour fédérale du Canada.
Nous présentons notre systeme de résumé LetSum ainsi que l’évaluation des résumés produits.
L’évaluation de 120 résumés par 12 avocats montre que la qualité des résumés produits par
LetSum est comparable avec celle des résumés écrits par des humains.

We describe an automatic text summarisation project for the legal domain for which we use
a corpus of judgments of the federal court of Canada. We present our summarization system,
called LetSum and the evaluation of produced summaries. The evaluation of 120 summaries by
12 lawyers shows that the quality of the summaries produced by LetSum is approximately at
the same level as the summaries written by humans.

Atefeh Farzindar et Guy Lapalme

1 Introduction

La jurisprudence est une reference importante pour les juristes. Pour cette raison les juristes
consultent quotidiennement des milliers de documents juridiques. De jour en jour, la masse
d’information textuelle sous forme de jurisprudence accessible sur internet ou dans les bases
de donnees des entreprises et des gouvernements ne cesse d’augmenter. Ce qui necessite le
developpement des outils speciﬁques aﬁn de pouvoir acceder au contenu des textes. Le but
d’un resume d’un jugement est d’abord de livrer l’essence du texte clairement et avec concision
pour permettre une consultation facile et rapide; il doit fournir sufﬁsamment d’informations sur
le jugement pour permettre au lecteur de decider si celui-ci peut etre pertinent a sa recherche.
Actuellement, des jugements sont resumes manuellement par les professionnels ce qui est tres
coﬁteux.

Notre approche au resume automatique a l’avantage de fournir des moyens clairs de concevoir
des documents juridiques en fonction de resumes courts pour differents types d’utilisateurs: des
etudiants, des avocats et des juges.

Le domaine juridique est un domaine ayant un grand besoin de resumes mais avec des exigences
speciﬁques. Dans ce projet, nous nous sommes interesses au traitement des decisions des cours
judiciaires du Canada. Nous avons collabore avec les avocats du Centre de Recherche en Droit
Public (CRDP), charges de creer la bibliotheque de droit virtuelle des decisions judiciaires
canadiens CanLII 1.

Dans cet article, nous decrivons plut6t les aspects qualitatifs de l’evaluation d’un resume que la
methodologie de la production de resume automatique. A la section 2, nous rappelons notre ap-
proche de production automatique de resume de jugements et son l’implantation, LetSum (Legal
Text Summarizer). La section 3 presente les evaluations effectuees avec LetSum. L’ evaluation
de 120 resumes automatiques par 12 avocats montre que la qualite des resumes produits par Let-
Sum est excellente. La comparaison des resumes de LetSum avec cinq systemes de recherche
ou commerciaux montre l’interet d’utiliser d’un systeme de resume specialise pour le domaine
juridique.

2 Resume de textes juridiques

Notre methode a ete developpee suite a une analyse manuelle de 75 jugements et de leurs re-
sumes rediges par les resumeurs professionnels. Nous avons deja presente la problematique
(Farzindar, 2004) et notre methode pour capturer la structuration thematique des documents et
identiﬁer les unites textuelles saillantes (Farzindar et al., 2004). Nous identiﬁons d’abord le
plan d’organisation d’un jugement et ses differents themes discursifs qui regroupent les phrases
autour d’un meme sujet. Chaque phrase dans un theme donne des informations complemen-
taires sur le sujet. Pour les phrases reliees a un theme, nous pouvons en interpreter le sens
d’apres leur contexte aﬁn d’en extraire les idees cles.

1Canadian Legal Information Institute http://www.canlii.org

Production automatique du résumé de textes juridiques

La création du résumé par LetSum se fait en quatre étapes décrites en détail dans (Farzindar,
2005)

Segmentation thématique qui détermine l’organisation du document original et relie les seg-
ments du texte associés avec des sept themes suivants:

o DONNEES DE LA DECISION: donne la référence complete de la décision et la rela-
tion entre les parties sur le plan juridique.

0 INTRODUCTION: qui? a fait quoi? a qui?

a CONTEXTE: recompose l’histoire du litige et l’histoire judiciaire.

o SOUMISSION: présente le point de vue d’une partie sur le probleme.

o QUESTIONS DE DROIT: identiﬁe le probleme juridique dont le tribunal est saisi.

o RAISONNEMENT JURIDIQUE: décrit l’analyse du juge, la détermination des faits et
l’eXpression des motifs de la solution retenue.

0 CONCLUSION: présente la décision ﬁnale de la cour.

Selon nos observations, quatre themes jouent les roles principaux: INTRODUCTION,
CONTEXTE, RAISONNEMENT JURIDIQUE et CONCLUSION. La présence de ces quatre
themes dans le jugement et dans le résumé est obligatoire. Dans la structure du résumé,
nous préservons ces quatre themes et nous extrayons les phrases qui leur appartiennent.
Le theme QUESTIONS DE DROIT est optionnel dans le jugement.

Filtrage qui identiﬁe les segments qui peuvent étre supprimés dans les documents, sans perdre
les informations pertinentes pour le résumé. Dans un jugement, les citations occupent un
volume important du texte soit 30% du jugement, alors que leur contenu est moins impor-
tant pour le résumé. Nous identiﬁons les citations principalement pour les supprimer en
ne conservant que leurs références juridiques. En plus, le theme SOUMISSION contenant
des discours des avocats, identiﬁé par le segmenteur thématique, sera éliminé dans cette
étape.

Sélection des unités textuelles candidates pour le résumé qui construit une liste d’unités sail-
lantes pour chaque niveau structural du résumé en calculant les poids pour chaque phrase
dans le jugement. La sélection est basée sur des regles sémantiques et des mesures statis-
tiques.

Production du résumé qui choisit les unités pour le résumé ﬁnal et les combine aﬁn de pro-
duire un résumé représentant au maximum 15% du jugement. Le critere de sélection des
unités est basé sur l’importance du segment thématique contenant les unités candidates.

La présentation du résumé ﬁnal est sous forme d’une ﬁche de résumé contenant des rubriques
homogenes d’informations. Cette ﬁche présente les informations considérées importantes as-
sociées a des themes précis, ce qui en facilite la lecture et la navigation entre le résumé et le
jugement source. Pour chaque phrase du résumé produit, l’utilisateur peut en déterminer le suj et
en regardant le theme associé a son segment thématique. La ﬁgure 1 montre un exemple de sor-
tie de LetSum comme une ﬁche de résumé. Cette ﬁche de résumé montre les themes identiﬁés
dans le jugement qui étaient pertinents pour le résumé. La taille du résumé est de 10% de celle
du jugement original (le document source a dix pages).

Dans les prochaines sections, nous présentons l’évaluation des résumés générés par LetSum.

Atefeh Farzindar et Guy Lapalme

   Table Style Summary
RCMPT-979-96.htrn1
INTRODUCTION [I ] This is an application by Her Majesty the Queen (Crown) for an order striking out the Statement of Claim or,
in the altemative. an extension of time to allow the Crown to file a Statement of Defence in the present action.
[T] I believe.that before I recite the facts of the present case. it is important to note that on a motion to strike a
Statement of Claim due to the fact that the Statement of Claim discloses no reasonable cause of action. it must be
plain and obvious that the claim will not succeed notwithstanding the fact that the allegations in the Statement of
Claim must be deemed to be true.

CONTEXT [I I] The plaintiff (Riabkol was a memberof the Royal Canadian Mounted Police (RCMP) from November 6.
|97"8 to September I4. I994. almost I6 years.
On May 6. I994 an Adjudication Board created under sections 43 and 44 of the According to the Crown "These
actions arose from cenain incidents in which the plaintiff was involved in and occurred in I992".
[I3] As a result ofthe Board's decision of May 6. I994. Riabko was sanctioned by requesting or ordering his
resignation froln the RCMP Force within I4 days.
[I6] On April 30. I996. Riabko filed a Statement of Claim in this action in the Federal Coun ofCanada.

ISSUE }ssue[2?] Does the Statement ofCIaim show a triable issue‘?

REASONIN-G Itake this to mean that if the sections of the Act and Regulations are followed, a member may be dismissed or
discharged and that the member would not be able to pursue the issue in the Couns by means of ﬁling a Statement of
Claim only alleging wrongfuldismissal.

[35] Because of the alleged breach of the RCMP Code of Conduct. a formal disciplinary hearing took place
pursuant to section 43 of the RCMP Act ,that is. an Adjudication Board was appointed to conduct a hearing into the
alleged complaint.

42 It is obvious that the plaintiff Riabko did not follow the procedure set out in the RCMP Act and he is now

alleging that he is claiming against Her Majesty because the process wherein he was asked to resign was an abuse of
power by the Board. that is. from the very stan. the process of the Board was flawed and he would thus have the
right to proceed in Coun.

[45] lam satisfied that by having resigned. she could not avail herself ofthe internal process as stated in the
RCMP Act and could sue for damages for sexual harassment.

It must be noted that before she commenced her action before the Federal Coun she did not avail herself or never
took part in the process set out in the "She never did anything wrong" while in the case at bar the plaintiff was found
to have contravened the RCMP Code of Conduct.

[4?] I aln satisfied that where it cannot be shown that the power with regard to the grievance process as set out in
the RCMP Act has been exceeded or abused. then there would be no cause of action.

[49] l aln satisfied there would be no purpose for Parliament to set out a grievance procedure by statute if a party
could. after taking pan in the procedure, decide to circumvent the statutory procedure.

CONCLUSION ]][5{I] As well. after a plain reading of the Statement of Claim. and particularly paragraphs 5 and (1. I am satisfied

that there is no allegation that the Adjudication Board of the RCMP abused or exceeded itsjurisdiction.
[5 I] Plaintiffs claim is struck with costs. ‘

1

Figure 1: Fiche de resume produit par LetSum, compose de 350 mots alors que le jugement
source avait 4500 mots

3 Evaluation

La comparaison avec un resume modele comme reference pour des resumes automatiques est
tres naturelle, mais des resumes rediges par des personnes differentes ne sont pas toujours con-
vergents au niveau du contenu. La redaction d’un resume demande une analyse du texte pour
en degager les idees, les arguments, le style et les themes. Les redacteurs humains degagent les
afﬁrmations essentielles du document et les expriment dans leur propre style, ce qui donne lieu
a plusieurs resumes pour le meme document. 11 est donc difﬁcile de deﬁnir une metrique claire
pour juger differents aspects d’un resume comme la completude, la thematique et la coherence.

Plusieurs campagnes d’evaluation de systemes de resume comme SUMMAC2 (Mani et al.,
1998) et DUC3 (organise par NIST) ont montre l’importance de deﬁnir des mesures pour
l’evaluation d’un resume. Spark Jones et Galliers (Spark-Jones & Galliers, 1995) ont pro-
pose de diviser les evaluations en deux types: intrinséque et extrinséque. L’ evaluation intrin-
seque mesure les proprietes concemant la nature du sujet a evaluer et son objectif, alors que

ZTIPSTER Text Summarization Evaluation Conference
3Document Understanding Conferences http://www—n1pir.nist.gov/proj ects/duc

Production automatique du résumé de textes juridiques

l’évaluation extrinseque mesure les aspects concernant les impacts et les effets de sa fonction.
Nous avons évalué LetSum avec ces deux types d’évaluations.

Nos résumés du systeme ont été évalués en deux étapes: nous avons d’abord évalué les modules
du systeme séparément, ensuite nous avons mesuré la qualité globale des résumés produits.
Nous avons également comparé les résumés de LetSum avec des résumés produits par quatre
autres systemes et des résumés manuels.

Pour l’évaluation des modules de LetSum, nous avons utilisé une évaluation intrinseque a trois
niveaux: la qualité des divisions en themes par le segmenteur thématique, la détection correcte
des citations par le module de ﬁltrage, et le contenu des unités sélectionnées par le module de
sélection et production.

Comme évaluation intrinseque, nous avons utilisé ROUGE (Recall-Oriented Understudy for
Gisting Evaluation) (Lin & Hovy, 2003). ROUGE est maintenant bien reconnue comme mesure
d’évaluation des résumés et a été utilisée pour la premiere fois dans la compétition de DUC 2004
comme seule mesure de ﬁabilité pour certaines taches. ROUGE est basé sur le calcul statistique
de co-occurrence de n-graInInes. Cette méthode dont les résultats sont bien corrélés avec les
jugements humains permet d’optimiser les systemes et d’accélérer leur évaluation. ROUGE
comporte deux méthodes d’évaluation. ROUGE-N, dont le score est basé sur le nombre de
n-grammes (normalement 1 3 n 3 4) communs entre le résumé automatique est le résumé
modele. Par exemple, ROUGE-2 calcule le nombre de paires de mots successifs communs
entre les résumés candidat et modele. La deuxieme est ROUGE-L, qui considere les phrases
comme une suite des séquences des mots. Cette évaluation calcule la plus longue sous-séquence
commune des mots aﬁn d’estimer la similarité entre deux résumés.

Pour l’évaluation extrinseque de LetSum, nous avons demandé a des utilisateurs juristes de
juger le contenu des résumés et leur acceptabilité. Pour chaque résumé, le recouvrement du
contenu sur les idées clés du document a été évalué par deux avocats.

3.1 Evaluation des modules de LetSum

Nous avons évalué les quatre modules de LetSum séparément. Les deux premiers modules, seg-
mentation thématique et ﬁltrage sont évalués séparément, alors que les deux autres modules
de sélection des unités pertinentes et production ont été évalués dans le cadre de l’évaluation
des résumés ﬁnals de LetSum. Nous avons comparé les sorties de module de segmentation thé-

matique avec le corpus que nous avons annoté manuellement (avec validation d’un avocat du
CanLII).

Pour l’évaluation du module de segmentation thématique, nous avons utilisé un corpus de test
contenant 10 jugements de la cour fédérale. Ces jugements n’ont pas été utilisés pour entrainer
le systeme, ni servi a la construction du dictionnaire des marqueurs. Pour l’évaluation de ce
module les points considérés importants sont: détection des themes, degré de pertinence d’un
theme pour un segment, couverture des segments thématiques, précision des frontieres entre
deux themes. Pour cette evaluation on peut calculer la précision et le rappel. La précision
mesure la proportion des unités pertinentes parIr1i toutes les unités produites par le systeme. Le
rappel mesure la proportion des unités pertinentes parmi tous les unités pertinentes. F-mesure
considere les deux mesures ensemble. Nous avons obtenu une précision de 100% et un rappel
de 95% soit F-mesure 99% . Sur 40 themes annotés dans le corpus, 38 themes ont été identiﬁés
correctement.

Atefeh Farzindar et Guy Lapalme

Pour l’évaluation du module de ﬁltrage de citations, nous avons utilisé 15 jugements de la cour
fédérale qui n’ont pas servi a entrainer le module de ﬁltrage. Pour cette évaluation, nous avons
comparé les unités de citations identiﬁées par le ﬁltrage avec les citations annotées manuelle-
ment dans les jugements. Le résultat d’évaluation du module de ﬁltrage est de 98% pour la
précision et 95% pour le rappel ce qui donne 96% pour la F-mesure. Sur 60 cas de citation, 57
unités ont été identiﬁées correctement. Certaines citations n’étaient pas identiﬁées correctement
a cause la langue de rédaction des références. Dans les jugements canadiens, les juges citent
parfois les références de droit tels quels peu importe qu’elles soient en anglais ou en francais.
Pour les citations en francais, lorsqu’il y a des marqueurs d’énumération, le systeme les identiﬁe
mais en absence des marqueurs d’énumération, il ne peut pas les distinguer.

3.2 Evaluation de LetSum par ROUGE

Pour le module de sélection et production, il faut mesurer les topiques extraits des documents
par le systeme. Il est possible d’aligner automatiquement les unités de deux textes pour com-
parer la similarité entre les résumés modeles et les résumés produits aﬁn de calculer la fraction
du résumé modele exprimée dans le contenu du résumé produit par le systeme. Pour cette eval-
uation des résumés de LetSum, nous avons utilisé ROUGE en les comparant avec des résumés
modeles écrits par des humains. Nous avons généré 50 résumés automatiques avec cinq sys-
temes: systeme de recherche MEAD (Radev et al., 2003), un systeme de recherche et commer-
cial francais Pertinence Mining (Lehmam, 1995), un systeme commercial de Microsoft Word
(option de résumé dans MS Word) et une méthode StartEnd que nous avons déﬁnie. Le Star-
tEnd est un systeme basé sur les positions des segments dans le document et LetSum aﬁn de
comparer notre systeme avec d’autres systemes de résumés. Pour la méthode StartEnd, nous
avons mis au point cette approche suite a nos analyses du corpus des résumés manuels. Pour
déﬁnir le StartEnd nous avons fait trois expérimentations.

D’apres nos études, le début du jugement situé a la ﬁn des DONNEES DE LA DECISION (nom de
la cour, lieu de l’audience, date, les références et etc.) est une partie importante qui comprend
le début du theme INTRODUCTION. Nous avons déﬁni un baseline qui prend 15% du début du
texte. Ce baseline couvre des themes INTRODUCTION et CONTEXTE.

Un autre baseline prend 15% de la ﬁn du jugement avant la signature du juge. Ce baseline
couvre les unités des themes RAISONNEMENT J URIDIQUE et CONCLUSION. Nos expériences
avec ROUGE ont montré que le score de premier baseline était plus élevé que le deuxieme, ce
qui signiﬁe l’importance du commencement du document par rapport a sa ﬁn.

Cette expérience, nous a conduit a déﬁnir une approche de résumé avec un taux de compression
15%, base sur l’algorithme suivant: prendre 8% du début du jugement et en complétant la
derniere phrase si cette demiere a été coupée et prendre 4% de la ﬁn du jugement en ajoutant la
premiere phrase complete. Cette derniere approche, que nous avons nommée StartEnd est donc
assez appropriée pour les documents de style juridique méme si son implémentation est assez
simple.

Nous avons comparé avec ROUGE les résumés de LetSum, Stan‘End et ceux de trois autres
systemes avec les résumés humains. Les résultats de l’évaluation sont montrés a la table 1.
Un score plus élevé est meilleur et indique un systeme plus performant. LetSum est classé
au premier rang avec les meilleures notes d’évaluation. D’apres cette évaluation, le deuxieme
systeme est StartEnd, ce qui montre l’importance de l’étude sur les documents des domaines

Production automatique du résumé de textes juridiques

Systeme ROUGE-l ROUGE-2 ROUGE-3 ROUGE-4 ROUGE-L

LetSum 0.57500 0.31381 0.20708 0.15036 0.45185
StartEnd 0.47244 0.27569 0.19391 0.14472 0.34683
MEAD 0.45581 0.22314 0.14241 0.10064 0.32089
MsW0rd 0.44473 0.21295 0.13747 0.09727 0.29652

Per. Mining 0.32833 0.15127 0.09798 0.07151 0.22375

Table 1: Résultat d’évaluation intrinseque avec ROUGE, LetSum a des meilleurs résultats

spéciﬁques. Le fait qu’une approche simple puisse dépasser des méthodes complexes de pro-
duction de résumé met en évidence la différence entre des organisations des documents et elle
montre aussi l’intérét de développer un systeme spéciﬁque pour un domaine. Il est plus en plus
difﬁcile de produire un résumé général pour tous types d’utilisateurs sans prise en compte du
besoin des usagers et de la tache demandée.

3.3 Evaluation extrinséque de LetSum

L’ objectif de cette évaluation est de mesurer l’utilité du résumé automatique par rapport a un
résumé écrit par un humain et de comparer la qualité des résumés automatiques générés par
différents systemes. Ce test est basé sur un jugement humain. Cette évaluation est toutefois tres
coﬁteuse, parce qu’elle demande des ressources humaines et un temps considérable.

Pour cette évaluation, nous avons utilisé les résumés automatiques produits par cinq systemes
présentés a la section précédente et les résumés écrits par des humains. Il faut noter que dans
cette évaluation, nous n’avons considéré que les textes du résumé. Nous n’avons pas généré
le format tabulaire d’organisation du résumé comme celui qui est présenté a la ﬁgure 1. Nous
voulions aussi normaliser l’apparence de la sortie de tous les systemes pour ne pas inﬂuencer
les juges. Ce choix pénalise toutefois LetSum car nous ne tenons pas compte de la structure
thématique extraite par notre méthodologie. Les évaluateurs ne savaient pas quels résumés
avaient été produits par ordinateur et lesquels avaient été écrits manuellement.

Nous avons fait évaluer 120 résumés par les juristes. Le corpus de test contient dix jugements
choisis au hasard dans différentes collections de jugements de la Cour fédérale du Canada. Nous
avons généré 50 résumés automatiques et nous avons collecté 10 résumés manuels écrits par les
arrétistes de la Cour fédérale. Pour chaque résumé, nous avons répété le test deux fois, ceci
nous donne deux avis par résumé. Chacun des 12 avocats du CanLII a évalué 10 résumés sur
une période d’une heure sur deux aspects: contenu et qualité.

Pour l’évaluation du contenu, nous avons déﬁni sept points importants a retrouver dans un
jugement. Si un lecteur peut déterminer les points en question en lisant le resume, on en déduit
que le résumé contient sufﬁsamment d’informations pour couvrir les idées clés d’un jugement.
Sept questions (présentées en haut de la table 2) ont été déterminées avec l’aide d’un avocat de
CanLII. L’ensemble des réponses de ces questions montre le degré de couverture sur des idées
clés du jugement source exprimées dans le résumé. La deuxieme partie de l’évaluation portait
sur la qualité d’un résumé selon trois criteres:

Lisibilité : La facilité de distinction et de perception du contenu du résumé qui en facilite la
compréhension. Ce critere donne une appréciation globale du résumé. On demande si le

Atefeh Farzindar et Guy Lapalme

Aprés avoir lu le résumé peut-on déterminer:

Q1. Qui sont les parties en litige?

Q2. Quel est le probleme en litige?

Q3. Les questions de droit soulevées?

Q4. Comment le juge a appliqué le droit aux faits?

Q5. Les motifs couvrent-ils les questions de droit?

Q6. Le résumé contient-il les motifs déterminants pour arriver a la conclusion?
Q7. Le résultat ﬁnal de la cour?

Résumé Q1 Q2 Q3 Q4 Q5 Q6 Q7 Moyenne
Humain 55,00 90,00 90,00 70,00 80,00 85,00 95,00 80,71
LetSum 50,00 90,00 80,00 75,00 75,00 85,00 85,00 77,14
StartEnd 65,00 100,00 100,00 70,00 80,00 70,00 90,00 82,14
MEAD 55,00 100,00 95,00 65,00 50,00 50,00 40,00 65,00
MsW0rd 30,00 80,00 85,00 60,00 45,00 45,00 60,00 57,86
Per. Mining 25,00 65,00 55,00 35,00 35,00 35,00 45,00 42,14
Moyenne 46,67 87,50 84,17 62,50 60,83 61,67 69,17 67,50

Table 2: Questions juridiques utilisées lors de l’évaluation du contenu du résumé par les juristes
et les résultats d’évaluation extrinseque, les pourcentages des réponses positives pour les sept
questions juridiques

résumé est: clair, assez clair, peu clair ou incompréhensible.

Cohérence : La présence simultanée d’éléments qui correspondent au meme contenu ou qui
s’accordent entre eux, qui s’harmonisent. Ce critere contient le ﬁl conducteur du texte
pour en assurer la continuité et la progression de l’information. On demande si la co-
hérence du texte dans le résumé est: tres bonne, bonne, médiocre ou tres mauvaise.

Pertinence des phrases : Caractere de ce qui est plus ou moins approprié, qui s’inscrit dans la
ligne de l’objectif poursuivi. La pertinence des phrases mesure si les phrases du résumé
contiennent un lien clair et direct avec le sujet dont il est question. On demande si le
résumé est: tres pertinent, assez pertinent, peu pertinent ou non pertinent.

L’ évaluation comporte aussi une valeur d’acceptabilité sur la qualité générale du résumé. Nous
avons demandé d’attribuer une valeur d’acceptabilité entre 0 et 5 pour chaque résumé (0 pour
un résumé inacceptable et 5 pour un texte acceptable) sur la qualité du texte de résumé. Les
résumés avec valeur 3 jusqu’a 5 sont considérés acceptables.

Dans la table 2, nous présentons les résultats obtenus pour l’évaluation des 120 jugements ou,
pour chaque question, nous avons calculé le pourcentage de réponses positives données a cette
question. Une réponse positive signiﬁe que le résumé contient assez d’informations sur le point
en question. Par exemple dans la deuxieme colonne, les résumés produits par le moyenne de
toutes les méthodes ont couvert les informations sur la présentation des parties en litige (Q1)
dans 47% des cas. LetSum a donc tres bien répondu aux exigences des avocats pour des résumés
automatiques. Ses résultats sont tres proches de ceux des résumés manuels et sa performance
est supérieure a celle des autres systemes commerciaux Microsoft Ward et Pertinence Mining,
y compris le systeme de recherche MEAD.

Notre méthode StartEnd, basée sur la position des segments, a également donné de bons résul-

Production automatique du résumé de textes juridiques

Résumé Lisibilité 0-3 Cohérence 0-3 Pertinence 0-3 Acceptabilité 0-5
Humain 2,00 1,95 2,15 3,43
LetSum 2,30 2,30 2,25 3,43
StartEnd 2,40 2,20 2,10 3,68
MEAD 2,15 1,90 2,05 3,23
MsW0rd 1,65 1,25 1,60 2,63
Per. Mining 1,40 1,10 1,40 2,23
Moyenne 1,98 1,78 1,93 3,10

Table 3: Résultats d’évaluation extrinseque selon les valeurs qualitatives entre 0 et 3 sur lisi-
bilité, cohérence et pertinence des phrases, valeur d’acceptabilité est entre 0 et 5 sur la qualité
générale du résumé

tats. Notre heuristique pour les positions des segments était appropriée, méme si elle differe
du baseline utilisé normalement pour les articles journaux. Par le comportement du systeme
MEAD, spécialisé pour les articles des journaux, on peut voir que les questions qui possedent
les réponses placées au début du document sont bien répondues alors que le recouvrement des
informations clés sur les questions avec réponses dans d’autres positions dans le texte n’est
pas satisfaisant. Les systemes commerciaux comme Microsoft Ward et Pertinence Mining ont
les scores les plus faibles dans l’évaluation, car ils produisent des résumés génériques qui ne
satisfont pas vraiment les utilisateurs dans un domaine spéciﬁque comme droit.

La table 3 montre les résultats de l’évaluation de la qualité du résumé. Pour les trois criteres,
lisibilité, cohérence et pertinence des phrases du résumé, les valeurs sont entre 0 et 3. La qualité
des résumés produits par LetSum est supérieure a celle des autres méthodes. La lisibilité du
résumé de LetSum est jugé clair, la cohérence est évaluée tres bonne et les pertinences des
phrases sont mesurées tres pertinentes pour les besoins des avocats. Les condensés rédigés
par un humain sont jugés bons en cohérence (et non pas tres bons) parce qu’ils sont en style
télégraphique alors que LetSum et les autres systemes font l’eXtraction de phrases.

Au point de vue d’acceptabilité du résumé, les résumés de LetSum sont jugés de niveau equiv-
alent a celui des résumés écrits par les arrétistes des cours. Encore une fois la méthode de
positions des phrases dans le jugement des tres bons scores pour ce critere d’évaluation. I1 faut
noter que dans cette partie de l’évaluation il y a peu de différence entre le systeme StartEnd et
LetSum, un systeme nettement plus élaboré. Ceci peut en partie s’expliquer par le fait que nous
n’avons pas considéré le format tabulaire produit par LetSum basé sur l’analyse thématique du
texte qui distingue notre méthode.

4 Conclusion

Le domaine juridique est un vaste domaine avec un grand besoin pour le résumé automatique.
Au Canada, il y a 30 000 avocats et aux l:3tats-Unis plus de 300 000 avocats susceptibles de
rechercher de la jurisprudence. Toutes les syntheses de jurisprudence se font manuellement par
des juristes. Lors qu’un résumé est disponible, le juriste a une idée du contenu de la décision et
il lui est plus facile de savoir si elle a un potentiel de pertinence. Chaque résumé peut sauver,
dans la consultation d’une liste de résultats de recherche, deux minutes a la personne qui fait la
recherche. Une recherche typique dans CanLII donne plus de trente résultats, on pourrait donc

Atefeh Farzindar et Guy Lapalme

sauver une heure environ. Comme un avocat-recherchiste facture au moins 100$ de l’heure
a son client, et que plusieurs recherches peuvent étre requises pour un seul dossier, pour 20
recherches, il y aura donc 20 heures d’éconoInies, 2 000$ sur un seul cas. L’utilisation des
résumés automatiques économise du temps, des coﬁts et des expertises. Ces économies de
ressources protegent les intéréts du gouvemement et de la population en tant que des clients
attendant de recevoir un service juridique.

Nous avons développé LetSum, le premier systeme complet pour le résumé de textes juridiques
en anglais. Il est basé sur l’identiﬁcation de la structure thématique et présente le résumé sous
forme d’une ﬁche de résumé augmentant ainsi la cohérence et la lisibilité du résumé. Dans
les différentes étapes de notre étude, nous avons cherché a maximiser la précision de notre
analyse en vue de diminuer les erreurs, car les textes de lois sont tres précieux. L’ excellente
évaluation de LetSum est le témoin de la validité de notre approche. En faisant ressortir les
points essentiels des jugements, nous espérons avoir rendu la justice plus accessible a tous et
aussi aider la société.

Remerciements

Nous tenons a remercier l’équipe LexUM du laboratoire d’informatique juridique du Centre de
recherche en droit public de la faculté de droit de l’Université de Montréal pour leur collabo-
ration. La recherche présentée ici est soutenu ﬁnancierement par le Conseil de recherches en
sciences naturelles et en génie du Canada (CRSNG).

Références

FARZINDAR A. (2004). Développement d’un systéme de résumé automatique de textes juridiques. In
TALN-RECITAL’2004, p. 39-44, Fés, Maroc.

FARZINDAR A. (2005). Résume’ automatique de textes juridiques. PhD thesis, Université de Montréal
et Université de Paris4-Sorbonne.

FARZINDAR A., LAPALME G. & DESCLES J .-P. (2004). Résumé de textes juridiques par identiﬁca-
tion de leur structure thématique. Traitement Automatique des Langues (TAL), Nume’ro spe’cial sur: Le
résumé automatique de texte : solutions et perspectives, 45(1), 39-65.

LEHMAM A. (1995). Le résumé automatique a fragments indicateurs: RAFI. PhD thesis, Université de
Nancy-H, Nancy, France.

LIN C.-Y. & HOVY E. (2003). Automatic evaluation of summaries using n-gram co-occurrence statis-
tics. In Proceedings of 2003 Language Technology Conference (HLT-NAACL 2003), p. 150-157, Ed-
monton, Canada.

MANI 1., HOUSE D., KLEIN G., HIRSHMAN L., ORBST L., FIRMIN T., CHRZANOWSKI M. & SUND-
HEIM B. (1998). The TIPSTER SUMMAC Text Summarization Evaluation. Rapport interne MTR
98W0000138, The Mitre Corporation, McLean, Virginia.

RADEV D., OTTERBACHER J ., Q1 H. & TAM D. (2003). Mead reducs: Michigan at duc 2003. In
DU C03, p. 160-167, Edmonton, Alberta, Canada: Association for Computational Linguistics.

SPARK-J ONES K. & GALLIERS J . R. (1995). Evaluating Natural Language Processing Systems: An
Analysis and Review. Number 1083 in Lecture Notes in Artiﬁcial Intelligence. Springer.

