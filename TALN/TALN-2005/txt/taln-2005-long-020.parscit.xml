<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R Barzilay</author>
<author>M Elhadad</author>
</authors>
<title>Using lexical chains for text summarization.</title>
<date>1997</date>
<booktitle>In Proceedings of the Intelligent Scalable Text Summarization Workshop (ISTS’97),</booktitle>
<publisher>ACL.</publisher>
<location>Madrid,</location>
<contexts>
<context position="5134" citStr="[Luh58, BE97, GMCK00, BN00]" startWordPosition="759" endWordPosition="762">s focaux, qui, soit expriment l’apport sémantique du texte, soit permettent de le représenter dans sa globalité. Dès lors, le résumé par extraction cherchera à repérer ces unités saillantes et proposera un texte de taille plus petite que le document initial qui garderait majoritairement ces unités. Nous faisons également l’hypothèse de l’existence de ces unités ainsi que de leur intérêt pour le résumé. Ce seront les constituants dits gouverneurs, définis en section 3.2, qui correspondront à ces unités saillantes. Parmi ces techniques de résumé, une majorité utilise l’extraction de phrase clés [Luh58, BE97, GMCK00, BN00] pour produire le résumé final. Dans cet article nous nous intéressons uniquement au résumé intra-phrase et plus précisément à la compression de phrases. [KM02] aborde le problème de la compression de phrases en utilisant un modèle de canal bruyant (noisy-channel model) qui consiste à faire l’hypothèse (1) : la phrase à comprimer 1Nous appelons constituants les syntagmes des phrases, c’est-à-dire toute unité de la phrase à laquelle on peut attribuer une fonction. Par exemple, prenons le groupe nominal “un médecin de famille”. Il est composé de deux constituants : un groupe nominal “un médecin”</context>
</contexts>
<marker>[BE97]</marker>
<rawString>R. Barzilay and M. Elhadad. Using lexical chains for text summarization. In Proceedings of the Intelligent Scalable Text Summarization Workshop (ISTS’97), Madrid, Spain, 1997. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Branimir K Boguraev</author>
<author>Mary S Neff</author>
</authors>
<title>Lexical cohesion, discourse segmentation and document summarization.</title>
<date>2000</date>
<booktitle>In RIAO-2000,</booktitle>
<location>Paris,</location>
<contexts>
<context position="5134" citStr="[Luh58, BE97, GMCK00, BN00]" startWordPosition="759" endWordPosition="762">s focaux, qui, soit expriment l’apport sémantique du texte, soit permettent de le représenter dans sa globalité. Dès lors, le résumé par extraction cherchera à repérer ces unités saillantes et proposera un texte de taille plus petite que le document initial qui garderait majoritairement ces unités. Nous faisons également l’hypothèse de l’existence de ces unités ainsi que de leur intérêt pour le résumé. Ce seront les constituants dits gouverneurs, définis en section 3.2, qui correspondront à ces unités saillantes. Parmi ces techniques de résumé, une majorité utilise l’extraction de phrase clés [Luh58, BE97, GMCK00, BN00] pour produire le résumé final. Dans cet article nous nous intéressons uniquement au résumé intra-phrase et plus précisément à la compression de phrases. [KM02] aborde le problème de la compression de phrases en utilisant un modèle de canal bruyant (noisy-channel model) qui consiste à faire l’hypothèse (1) : la phrase à comprimer 1Nous appelons constituants les syntagmes des phrases, c’est-à-dire toute unité de la phrase à laquelle on peut attribuer une fonction. Par exemple, prenons le groupe nominal “un médecin de famille”. Il est composé de deux constituants : un groupe nominal “un médecin”</context>
</contexts>
<marker>[BN00]</marker>
<rawString>Branimir K. Boguraev and Mary S. Neff. Lexical cohesion, discourse segmentation and document summarization. In RIAO-2000, Paris, April 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sergey Brin</author>
<author>Lawrence Page</author>
</authors>
<title>The anatomy of a large-scale hypertextual web search engine.</title>
<date>1998</date>
<booktitle>In WWW7 : Proceedings of the seventh international conference on World Wide Web 7,</booktitle>
<pages>107--117</pages>
<publisher>Elsevier Science Publishers</publisher>
<location>Amsterdam, The Netherlands, The</location>
<contexts>
<context position="9888" citStr="[BP98]" startWordPosition="1484" endWordPosition="1484">e se basant sur la fonction syntaxique est celle de [LBM04] qui travaille à un niveau de granularité très fin, nettement inférieur à la proposition. Dans le système des auteurs, les fonctions syntaxiques des syntagmes sont extraites par un système à base de règles. Une forme logique des phrases est produite et représentée par un arbre dont les noeuds sont les syntagmes (ou des variables si des informations sont manquantes) et les arêtes les fonctions. À Mehdi Yousfi-Monod, Violaine Prince partir des relations entre les syntagmes, un graphe du document est créé sur lequel l’algorithme Pagerank [BP98] est appliqué pour évaluer l’importance de chaque noeud. Les noeuds les plus importants sont ensuite extraits et fournis à un module de génération de phrases qui produit le résumé final. Leur système utilise la fonction syntaxique des syntagmes mais pas la structure syntaxique des phrases2, ceci laisse au module de génération la lourde tâche de produire des phrases syntaxiquement et sémantiquement cohérentes. Notre système ne fait que supprimer des sous-arbres de l’arbre syntaxique, ceci évite de tomber dans ces problèmes d’incohérence. Notre approche nécessite un outil d’analyse morpho-syntax</context>
</contexts>
<marker>[BP98]</marker>
<rawString>Sergey Brin and Lawrence Page. The anatomy of a large-scale hypertextual web search engine. In WWW7 : Proceedings of the seventh international conference on World Wide Web 7, pages 107–117, Amsterdam, The Netherlands, The Netherlands, 1998. Elsevier Science Publishers B. V.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacques Chauché</author>
</authors>
<title>Un outil multidimensionnel de l’analyse du discours.</title>
<date>1984</date>
<booktitle>In Coling’84,</booktitle>
<pages>11--15</pages>
<institution>Standford University,</institution>
<location>California,</location>
<contexts>
<context position="1119" citStr="[Cha84]" startWordPosition="158" endWordPosition="158">é automatique de textes par contraction de phrases. Notre approche se fonde sur l’étude de la fonction syntaxique et de la position dans l’arbre syntaxique des constituants des phrases. Après avoir défini la notion de constituant, et son rôle dans l’apport d’information, nous analysons la perte de contenu et de cohérence discursive que la suppression de constituants engendre. Nous orientons notre méthode de contraction vers les textes narratifs. Nous sélectionnons les constituants à supprimer avec un système de règles utilisant les arbres et variables de l’analyse morpho-syntaxique de SYGFRAN [Cha84]. Nous obtenons des résultats satisfaisants au niveau de la phrase mais insuffisants pour un ré- sumé complet. Nous expliquons alors l’utilité de notre système dans un processus plus général de résumé automatique. Abstract We propose an automated text summarization through sentence compression. Our approach uses constituent syntactic function and position in the sentence syntactic tree. We first define the idea of a constituent as well as its role as an information provider, before analyzing contents and discourse consistency losses caused by deleting such a constituent. We explain why our met</context>
<context position="10905" citStr="[Cha84]" startWordPosition="1628" endWordPosition="1628">s. Notre système ne fait que supprimer des sous-arbres de l’arbre syntaxique, ceci évite de tomber dans ces problèmes d’incohérence. Notre approche nécessite un outil d’analyse morpho-syntaxique des phrases (section 3.1) et une étude sur l’importance des constituants relativement à leur fonction syntaxique et leur position dans l’arbre syntaxique (section 3.2). Nous présentons l’architecture de notre système dans la section 3.3. 3.1 L’analyseur morpho-syntaxique Nous utilisons l’analyseur morpho-syntaxique du français SYGFRAN, basé sur le système opé- rationnel SYGMART, tous deux définis dans [Cha84]. SYGFRAN utilise un ensemble de règles de transformations d’éléments structurés, basées sur les règles de la grammaire française, qui permettent de transformer une phrase (texte brut) en un arbre syntaxique (élément structuré) enrichi d’informations sur les constituants. Cet analyseur a les avantages suivant : – la rapidité : la complexité d’analyse est en O(k ∗ n ∗ log2(n)) où k est le nombre de règles et n la donnée textuelle. Il s’agit d’une limite supérieure, car l’analyseur étant structuré en plusieurs grammaires ordonnées, le facteur multiplicatif réel est beaucoup plus petit que k. Cel</context>
</contexts>
<marker>[Cha84]</marker>
<rawString>Jacques Chauché. Un outil multidimensionnel de l’analyse du discours. In Coling’84, pages 11–15, Standford University, California, 1984.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Goldstein</author>
<author>V Mittal</author>
<author>J Carbonell</author>
<author>M Kantrowitz</author>
</authors>
<title>Multi-document summarization by sentence extraction.</title>
<date>2000</date>
<booktitle>In Hahn et al.[15],</booktitle>
<pages>40--48</pages>
<contexts>
<context position="5134" citStr="[Luh58, BE97, GMCK00, BN00]" startWordPosition="759" endWordPosition="762">s focaux, qui, soit expriment l’apport sémantique du texte, soit permettent de le représenter dans sa globalité. Dès lors, le résumé par extraction cherchera à repérer ces unités saillantes et proposera un texte de taille plus petite que le document initial qui garderait majoritairement ces unités. Nous faisons également l’hypothèse de l’existence de ces unités ainsi que de leur intérêt pour le résumé. Ce seront les constituants dits gouverneurs, définis en section 3.2, qui correspondront à ces unités saillantes. Parmi ces techniques de résumé, une majorité utilise l’extraction de phrase clés [Luh58, BE97, GMCK00, BN00] pour produire le résumé final. Dans cet article nous nous intéressons uniquement au résumé intra-phrase et plus précisément à la compression de phrases. [KM02] aborde le problème de la compression de phrases en utilisant un modèle de canal bruyant (noisy-channel model) qui consiste à faire l’hypothèse (1) : la phrase à comprimer 1Nous appelons constituants les syntagmes des phrases, c’est-à-dire toute unité de la phrase à laquelle on peut attribuer une fonction. Par exemple, prenons le groupe nominal “un médecin de famille”. Il est composé de deux constituants : un groupe nominal “un médecin”</context>
</contexts>
<marker>[GMCK00]</marker>
<rawString>J. Goldstein, V. Mittal, J. Carbonell, and M. Kantrowitz. Multi-document summarization by sentence extraction. In Hahn et al.[15], pages 40–48, 2000.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Maurice Grevisse</author>
</authors>
<title>le Bon Usage – Grammaire française. édition refondue par André Goosse, DeBoeck-Duculot,</title>
<booktitle>Louvain-la-Neuve, 13e édition, ISBN</booktitle>
<pages>2--8011</pages>
<location>Paris</location>
<contexts>
<context position="8899" citStr="[Gre97]" startWordPosition="1327" endWordPosition="1327">n inférées automatiquement de façon calculatoire. De plus, nous ne faisons pas l’hypothèse de départ (1) de [KM02] qui pour nous est très discutable. 3 La compression par élagage de l’arbre syntaxique Le point de départ de notre approche fût l’intuition que la fonction syntaxique et la position dans l’arbre syntaxique des constituants des phrases jouaient un rôle conséquent dans l’importance de ces constituants pour la compréhension d’un texte. Cette intuition prend ses racines dans l’analyse grammaticale logique enseignée depuis longtemps et dont on trouve des manuels connus (citons Grévisse [Gre97] pour mémoire). En effet, ne sont pas toujours indispensables pour comprendre le sens principal de la phrase, certains épithètes, certains compléments circonstanciels, etc. Par exemple, dans la phrase « Un chat gros et laid mange une souris. », le groupe adjectival épithète “gros et laid” peut être supprimé sans nuire réellement à la compréhension et à l’intérêt. Une autre approche se basant sur la fonction syntaxique est celle de [LBM04] qui travaille à un niveau de granularité très fin, nettement inférieur à la proposition. Dans le système des auteurs, les fonctions syntaxiques des syntagmes</context>
</contexts>
<marker>[Gre97]</marker>
<rawString>Maurice Grevisse. le Bon Usage – Grammaire française. édition refondue par André Goosse, DeBoeck-Duculot, Paris – Louvain-la-Neuve, 13e édition, ISBN 2-8011-1045-0, 1993-1997.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregory Grefenstette</author>
</authors>
<title>Producing intelligent telegraphic text reduction to provide audio scanning service for the blind. In</title>
<date>1998</date>
<booktitle>In AAAI symposium on Intelligent Text Summarisation,</booktitle>
<pages>111--117</pages>
<location>Menlo Park, California,</location>
<contexts>
<context position="6684" citStr="[Gre98]" startWordPosition="997" endWordPosition="997">s auteurs utilisent un modèle probabiliste de type modèle de Bayes qu’ils entraînent sur un corpus de documents avec leur résumé. Le moteur d’apprentissage a pour but de sélectionner les mots à conserver dans la phrase comprimée. Une faible probabilité sera attribuée à une phrase comprimée lorsque cette dernière sera incorrecte grammaticalement ou aura perdu certaines informations comme la négation. D’après leur évaluation, les résultats sont assez concluants. Relativement aux compressions réalisées par des êtres humains, une légère perte d’importance et de justesse grammaticale est observée. [Gre98] utilise la nature des syntagmes et propositions pour estimer leur importance, puis supprime les moins importants pour produire les phrases compressées. La coherence obtenue est évidemment faible mais suffisante pour l’application souhaitée qui est la réduction de textes télégraphiques destinés à être lus par les mal voyants. Ces deux approches ne prennent pas en compte les informations sur la fonction syntaxique et la position dans l’arbre syntaxique des constituants des phrases. Ces informations pourraient être grandement utiles dans l’aide au choix des constituants à supprimer. [Lin03] a év</context>
</contexts>
<marker>[Gre98]</marker>
<rawString>Gregory Grefenstette. Producing intelligent telegraphic text reduction to provide audio scanning service for the blind. In In AAAI symposium on Intelligent Text Summarisation, pages 111–117, Menlo Park, California, 1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistics-based summarization - step one : Sentence compression.</title>
<date>2000</date>
<booktitle>In Proceedings of the Seventeenth National Conference on Artificial Intelligence and Twelfth Conference on Innovative Applications of Artificial Intelligence,</booktitle>
<pages>703--710</pages>
<location>Sapporo, Japan,</location>
<contexts>
<context position="7691" citStr="[KM00]" startWordPosition="1146" endWordPosition="1146">tion syntaxique et la position dans l’arbre syntaxique des constituants des phrases. Ces informations pourraient être grandement utiles dans l’aide au choix des constituants à supprimer. [Lin03] a évalué la qualité d’un résumé produit par extraction de phrases clés puis compression des phrases extraites. L’auteur conclut, d’après les résultats de ses expérimentations, qu’on ne peut pas se fier à une compression strictement basée sur la syntaxe des phrases pour améliorer la qualité des résumés produits par extraction. Cependant, étant donné que l’auteur n’utilise qu’une seule méthode (celle de [KM00]) pour comprimer les phrases, nous ne sommes pas d’accord sur sa conclusion généralisée à l’ensemble des méthodes de compression. Ce que nous concluons c’est que la méthode de compression utilisée, qui, en pratique, mélange à la fois paradigme statistique, apprentissage, technique de “noyage” (dans le bruit) et structure syntaxique, ne satisfait pas les contraintes de conservation du contenu. Notre approche diffère grandement de celle de [KM00] sur au moins deux points : nos règles de compression sont produites manuellement, en relation avec des modèles linguistiques, puis mises en œuvre, et n</context>
</contexts>
<marker>[KM00]</marker>
<rawString>Kevin Knight and Daniel Marcu. Statistics-based summarization - step one : Sentence compression. In Proceedings of the Seventeenth National Conference on Artificial Intelligence and Twelfth Conference on Innovative Applications of Artificial Intelligence, pages 703–710, Sapporo, Japan, 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>Summarization beyond sentence extraction : a probabilistic approach to sentence compression.</title>
<date>2002</date>
<journal>Artificial Intelligence archive,</journal>
<volume>139</volume>
<issue>1</issue>
<pages>91--107</pages>
<contexts>
<context position="5294" citStr="[KM02]" startWordPosition="787" endWordPosition="787">illantes et proposera un texte de taille plus petite que le document initial qui garderait majoritairement ces unités. Nous faisons également l’hypothèse de l’existence de ces unités ainsi que de leur intérêt pour le résumé. Ce seront les constituants dits gouverneurs, définis en section 3.2, qui correspondront à ces unités saillantes. Parmi ces techniques de résumé, une majorité utilise l’extraction de phrase clés [Luh58, BE97, GMCK00, BN00] pour produire le résumé final. Dans cet article nous nous intéressons uniquement au résumé intra-phrase et plus précisément à la compression de phrases. [KM02] aborde le problème de la compression de phrases en utilisant un modèle de canal bruyant (noisy-channel model) qui consiste à faire l’hypothèse (1) : la phrase à comprimer 1Nous appelons constituants les syntagmes des phrases, c’est-à-dire toute unité de la phrase à laquelle on peut attribuer une fonction. Par exemple, prenons le groupe nominal “un médecin de famille”. Il est composé de deux constituants : un groupe nominal “un médecin” et un groupe nominal prépositionnel “de famille”. Ce dernier a un rôle de modificateur du premier. Utilisation de la structure morpho-syntaxique des phrases da</context>
<context position="8406" citStr="[KM02]" startWordPosition="1254" endWordPosition="1254">odes de compression. Ce que nous concluons c’est que la méthode de compression utilisée, qui, en pratique, mélange à la fois paradigme statistique, apprentissage, technique de “noyage” (dans le bruit) et structure syntaxique, ne satisfait pas les contraintes de conservation du contenu. Notre approche diffère grandement de celle de [KM00] sur au moins deux points : nos règles de compression sont produites manuellement, en relation avec des modèles linguistiques, puis mises en œuvre, et non inférées automatiquement de façon calculatoire. De plus, nous ne faisons pas l’hypothèse de départ (1) de [KM02] qui pour nous est très discutable. 3 La compression par élagage de l’arbre syntaxique Le point de départ de notre approche fût l’intuition que la fonction syntaxique et la position dans l’arbre syntaxique des constituants des phrases jouaient un rôle conséquent dans l’importance de ces constituants pour la compréhension d’un texte. Cette intuition prend ses racines dans l’analyse grammaticale logique enseignée depuis longtemps et dont on trouve des manuels connus (citons Grévisse [Gre97] pour mémoire). En effet, ne sont pas toujours indispensables pour comprendre le sens principal de la phras</context>
</contexts>
<marker>[KM02]</marker>
<rawString>Kevin Knight and Daniel Marcu. Summarization beyond sentence extraction : a probabilistic approach to sentence compression. Artificial Intelligence archive, 139(1) :91–107, Jully 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vanderwende Lucy</author>
<author>Michele Banko</author>
<author>Arul Menezes</author>
</authors>
<title>Event-centric summary generation.</title>
<date>2004</date>
<booktitle>In In Document Understanding Conference at HLT-NAACL,</booktitle>
<location>Boston, MA,</location>
<contexts>
<context position="9341" citStr="[LBM04]" startWordPosition="1398" endWordPosition="1398">texte. Cette intuition prend ses racines dans l’analyse grammaticale logique enseignée depuis longtemps et dont on trouve des manuels connus (citons Grévisse [Gre97] pour mémoire). En effet, ne sont pas toujours indispensables pour comprendre le sens principal de la phrase, certains épithètes, certains compléments circonstanciels, etc. Par exemple, dans la phrase « Un chat gros et laid mange une souris. », le groupe adjectival épithète “gros et laid” peut être supprimé sans nuire réellement à la compréhension et à l’intérêt. Une autre approche se basant sur la fonction syntaxique est celle de [LBM04] qui travaille à un niveau de granularité très fin, nettement inférieur à la proposition. Dans le système des auteurs, les fonctions syntaxiques des syntagmes sont extraites par un système à base de règles. Une forme logique des phrases est produite et représentée par un arbre dont les noeuds sont les syntagmes (ou des variables si des informations sont manquantes) et les arêtes les fonctions. À Mehdi Yousfi-Monod, Violaine Prince partir des relations entre les syntagmes, un graphe du document est créé sur lequel l’algorithme Pagerank [BP98] est appliqué pour évaluer l’importance de chaque noe</context>
</contexts>
<marker>[LBM04]</marker>
<rawString>Vanderwende Lucy, Michele Banko, and Arul Menezes. Event-centric summary generation. In In Document Understanding Conference at HLT-NAACL, Boston, MA, 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
</authors>
<title>Improving summarization performance by sentence compression - a pilot study.</title>
<date>2003</date>
<booktitle>In Proceedings of the Sixth International Workshop on Information Retrivial with Asian Language (IRAL 2003),</booktitle>
<location>Sapporo, Japan, Jully</location>
<contexts>
<context position="7279" citStr="[Lin03]" startWordPosition="1084" endWordPosition="1084">vée. [Gre98] utilise la nature des syntagmes et propositions pour estimer leur importance, puis supprime les moins importants pour produire les phrases compressées. La coherence obtenue est évidemment faible mais suffisante pour l’application souhaitée qui est la réduction de textes télégraphiques destinés à être lus par les mal voyants. Ces deux approches ne prennent pas en compte les informations sur la fonction syntaxique et la position dans l’arbre syntaxique des constituants des phrases. Ces informations pourraient être grandement utiles dans l’aide au choix des constituants à supprimer. [Lin03] a évalué la qualité d’un résumé produit par extraction de phrases clés puis compression des phrases extraites. L’auteur conclut, d’après les résultats de ses expérimentations, qu’on ne peut pas se fier à une compression strictement basée sur la syntaxe des phrases pour améliorer la qualité des résumés produits par extraction. Cependant, étant donné que l’auteur n’utilise qu’une seule méthode (celle de [KM00]) pour comprimer les phrases, nous ne sommes pas d’accord sur sa conclusion généralisée à l’ensemble des méthodes de compression. Ce que nous concluons c’est que la méthode de compression </context>
</contexts>
<marker>[Lin03]</marker>
<rawString>Chin-Yew Lin. Improving summarization performance by sentence compression - a pilot study. In Proceedings of the Sixth International Workshop on Information Retrivial with Asian Language (IRAL 2003), Sapporo, Japan, Jully 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H P Luhn</author>
</authors>
<title>The automatic creation of literature abstracts.</title>
<date>1958</date>
<journal>Journal of research</journal>
<contexts>
<context position="5134" citStr="[Luh58, BE97, GMCK00, BN00]" startWordPosition="759" endWordPosition="762">s focaux, qui, soit expriment l’apport sémantique du texte, soit permettent de le représenter dans sa globalité. Dès lors, le résumé par extraction cherchera à repérer ces unités saillantes et proposera un texte de taille plus petite que le document initial qui garderait majoritairement ces unités. Nous faisons également l’hypothèse de l’existence de ces unités ainsi que de leur intérêt pour le résumé. Ce seront les constituants dits gouverneurs, définis en section 3.2, qui correspondront à ces unités saillantes. Parmi ces techniques de résumé, une majorité utilise l’extraction de phrase clés [Luh58, BE97, GMCK00, BN00] pour produire le résumé final. Dans cet article nous nous intéressons uniquement au résumé intra-phrase et plus précisément à la compression de phrases. [KM02] aborde le problème de la compression de phrases en utilisant un modèle de canal bruyant (noisy-channel model) qui consiste à faire l’hypothèse (1) : la phrase à comprimer 1Nous appelons constituants les syntagmes des phrases, c’est-à-dire toute unité de la phrase à laquelle on peut attribuer une fonction. Par exemple, prenons le groupe nominal “un médecin de famille”. Il est composé de deux constituants : un groupe nominal “un médecin”</context>
<context position="30228" citStr="[Luh58]" startWordPosition="4645" endWordPosition="4645">es plus variés. Cependant, la compression de phrases ne suffit pas à produire un résumé d’une taille convenable dans la plupart des cas d’applications. Comme nous l’avons vu, elle est aussi fortement dépendante du genre de texte. Nous considérons donc notre approche intra-phrase comme une des tâches à effectuer lors de la production d’un résumé automatique, en complément avec d’autres approches qui travaillent à un niveau de granularité supérieur ou égal aux phrases. 6 Conclusion Bien que le problème du résumé automatique ait déjà été abordé par de nombreux scientifiques depuis presque 50 ans [Luh58], l’approche que nous avons adoptée est novatrice. En effet, les approches actuelles du résumé automatique utilisent des informations telles la fréquence des termes, les relations lexicales entre les termes, les étiquettes sur la nature des constituants fournis par des POS tagger (lemmatiseurs), les probabilités d’un constituant d’apparaître dans un résumé d’après des moteurs d’apprentissage, la structure rhétorique du texte, cependant, aucune d’entre elles n’utilise conjointement la fonction syntaxique et la position dans l’arbre syntaxique des constituants. Ces informations n’ont pas été rée</context>
</contexts>
<marker>[Luh58]</marker>
<rawString>H.P. Luhn. The automatic creation of literature abstracts. Journal of research and development, IBM, 1958.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Inderjeet Mani</author>
</authors>
<title>Narrative Summarization,</title>
<date>2004</date>
<volume>45</volume>
<contexts>
<context position="17402" citStr="[Man04]" startWordPosition="2623" endWordPosition="2623">cteur à être transporté dans l’histoire mais qui ne sont pas indispensables à la compréhension du cœur de l’histoire. Alors que dans un article scientifique ou technique, chaque constituant a un rôle important à jouer dans la compréhension du discours. Afin d’évaluer les qualités de la compression par suppression de constituants, nous avons donc cherché à la tester sur des corpus où elle avait un sens, en d’autre termes dans les textes de type narratif, en se proposant ultérieurement de tester d’autres paradigmes pour les textes scientifiques ou techniques. Mehdi Yousfi-Monod, Violaine Prince [Man04] aborde la problématique du résumé de textes narratifs, en s’appuyant principalement sur des indices temporels. Il étudie les événements sur trois plans : la scène, l’histoire et l’intrigue, dans le but d’extraire les événements clés, scènes clés, et les intrigues saillantes. Il compte sur les méthodes actuelles (basées sur le marquage lexical, l’étude de la structure rhé- torique, l’analyse morpho-syntaxique, ...) et futures pour extraire les indices temporels nécessaires. Notre méthode actuelle ne tient compte que des informations syntaxiques. En supprimant dans une première passe les consti</context>
</contexts>
<marker>[Man04]</marker>
<rawString>Inderjeet Mani. Narrative Summarization, volume 45/1. 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean-Luc Minel</author>
</authors>
<title>Le résumé automatique de textes : solutions et perspectives, volume 45/1.</title>
<date>2004</date>
<contexts>
<context position="4469" citStr="[Min04]" startWordPosition="661" endWordPosition="661">n de phrases (section 2) ; nous présentons ensuite notre approche (section 3), nous continuons en illustrant l’efficacité de notre système par une expérimentation basée sur une application prototype appliquée à un texte du genre conte (section 4) et enfin nous discutons sur les résultats de cette expérimentation et sur les perspectives envisagées (section 5). 2 La compression de phrases Une grande partie des techniques de résumé automatique procède par extraction de segments textuels. Ces méthodes sont fondées sur l’hypothèse « qu’il existe, dans tout texte, des unités textuelles saillantes » [Min04]. Ces dernières représentent des points focaux, qui, soit expriment l’apport sémantique du texte, soit permettent de le représenter dans sa globalité. Dès lors, le résumé par extraction cherchera à repérer ces unités saillantes et proposera un texte de taille plus petite que le document initial qui garderait majoritairement ces unités. Nous faisons également l’hypothèse de l’existence de ces unités ainsi que de leur intérêt pour le résumé. Ce seront les constituants dits gouverneurs, définis en section 3.2, qui correspondront à ces unités saillantes. Parmi ces techniques de résumé, une majorit</context>
</contexts>
<marker>[Min04]</marker>
<rawString>Jean-Luc Minel. Le résumé automatique de textes : solutions et perspectives, volume 45/1. 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William C Mann</author>
<author>Sandra A Thompson</author>
</authors>
<title>Rhetorical structure theory : toward a functionnal theory of text organization.</title>
<date>1988</date>
<booktitle>In Research Report RR-87-190, USC/Information Sciences Institute,</booktitle>
<pages>243--281</pages>
<location>Marina del Rey, CA,</location>
<contexts>
<context position="20749" citStr="[MT88]" startWordPosition="3123" endWordPosition="3123">e verbe “être”, et plus généralement après un verbe d’état, son importance s’accroît considérablement, rendant la suppression impossible. Enfin, nous avons noté que lorsque l’épithète était placé dans un groupe nominal dans lequel le déterminant était un article défini, alors sa suppression était difficile. Ceci est dû au fait que l’article défini est utilisé pour parler d’une entité particulière et que les épithètes du nom permettent de différencier cette entité des autres. Certaines propositions relatives ont aussi une fonction d’épithète. Les relatives constituent, d’après Mann et Thompson [MT88], des informations sur le contexte, elle ne sont donc pas indispensables. Les appositions. L’apposition peut avoir des natures variées, elle peut être : – un groupe nominal (« Jean, le gourmand, aime les bonbons. »), – un pronom (« Jean doit manger lui-même les bonbons. »), – une proposition relative (« Jean, qui aime les bonbons, a beaucoup de caries. »), – une proposition participale présent (« Jean, aimant les bonbons, a beaucoup de caries. »), – une proposition participale passé (« Jean, aimé des enfants, fera un bon père. »), – une proposition infinitive (« Jean, manger des légumes, cela </context>
</contexts>
<marker>[MT88]</marker>
<rawString>William C. Mann and Sandra A. Thompson. Rhetorical structure theory : toward a functionnal theory of text organization. In Research Report RR-87-190, USC/Information Sciences Institute, pages 243–281, Marina del Rey, CA, 1988.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>