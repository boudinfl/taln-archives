<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
"http://www.w3.org/TR/html4/loose.dtd">
<html><head><title>D&#233;tection Automatique de Structures Fines du Discours</title>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
</head>
<body>
<div style="page-break-before:always; page-break-after:always"><div><p>TALN 2005, Dourdan, 6&#8211;10 juin 2005
</p>
<p>D&#233;tection Automatique de Structures Fines de Texte
</p>
<p>Nicolas Hernandez et Brigitte Grau
LIMSI/CNRS - LIR &#8211; Universit&#233; de Paris-Sud
BP 133, F-91403 ORSAY CEDEX (France)
</p>
<p>Hernandez|Grau@limsi.fr
</p>
<p>Mots-clefs : Navigation intra-documentaire, analyse th&#233;matique, structures du discours,
relations discursives, subordination et coordination, parall&#233;lisme lexico-syntaxico-s&#233;mantique,
mod&#232;le d&#8217;apprentissage, analyses linguistiques
</p>
<p>Keywords: Text browsing, topic analysis, text structures, discursive relations, subordi-
nation and coordination, lexical, syntactic and semantic parallelism, learning model, linguistic
analysis
</p>
<p>R&#233;sum&#233; Dans ce papier, nous pr&#233;sentons un syst&#232;me de D&#233;tection de Structures fines de
Texte (appel&#233; DST). DST utilise un mod&#232;le pr&#233;dictif obtenu par un algorithme d&#8217;apprentissage
qui, pour une configuration d&#8217;indices discursifs donn&#233;s, pr&#233;dit le type de relation de d&#233;pendance
existant entre deux &#233;nonc&#233;s. Trois types d&#8217;indices discursifs ont &#233;t&#233; consid&#233;r&#233;s (des relations
lexicales, des connecteurs et un parall&#233;lisme syntaxico-s&#233;mantique) ; leur rep&#233;rage repose sur
des heuristiques. Nous montrons que notre syst&#232;me se classe parmi les plus performants.
</p>
<p>Abstract In this paper, we present a system which aims at detecting fine-grained text
structures (we call it DST). Based on discursive clues, DST uses a learning model to predict
dependency relations between two given utterances. As discourse clues, we consider lexical
relations, connectors and key phrases, and parallelism. We show that our system implements an
improvement over current systems.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Nicolas Hernandez et Brigitte Grau
</p>
<p>1 Introduction
</p>
<p>Comme le souligne l&#8217;annonce du 14 d&#233;cembre 2004 de la soci&#233;t&#233; Google de num&#233;riser et de
rendre disponible en ligne 15 millions de livres appartenants &#224; 5 des plus c&#233;l&#232;bres biblioth&#232;ques
anglo-saxonnes du monde1, le besoin d&#8217;acc&#233;der facilement et rapidement au contenu d&#8217;un doc-
ument &#233;lectronique est plus que jamais un enjeu d&#8217;actualit&#233;.
Dans ce papier, nous nous int&#233;ressons &#224; la d&#233;tection de l&#8217;organisation du contenu information-
nel d&#8217;un document textuel. De nombreux travaux (principalement au sein de la comunaut&#233; de
r&#233;sum&#233; automatique) ont montr&#233; l&#8217;int&#233;r&#234;t d&#8217;appr&#233;hender la structure d&#8217;un texte : afin de ma-
nipuler des unit&#233;s de texte de diff&#233;rentes granularit&#233;s (i.e. diff&#233;rents degr&#233;s informationnels), de
fournir un contexte &#224; une information cibl&#233;e, de permettre une navigation intra-documentaire,
etc. (Moens &amp; Busser, 2001; Choi, 2002; Couto et al., 2004).
En particulier nous nous focalisons sur la micro-structure d&#8217;un texte (niveau phrastique voire
propositionnel). Nous affichons ainsi une compl&#233;mentarit&#233; aux approches globales tout en of-
frant la possibilit&#233; de raffiner leur mod&#232;le. En effet qu&#8217;elles supposent une organisation plate
et lin&#233;aire du flot d&#8217;informations communiqu&#233; (Hearst, 1997; Choi, 2002), ou bien une organi-
sation plus riche en arbres (Moens &amp; Busser, 2001; Couto et al., 2004), les approches globales
sont g&#233;n&#233;ralement fond&#233;es sur des mesures de coh&#233;sion lexicale (notamment &#224; travers le suivi
de cha&#238;nes lexicales) qui souffrent d&#8217;un manque de pr&#233;cision quant &#224; la d&#233;limitation des unit&#233;s
de texte (appel&#233;es segment). De plus elles prennent rarement en compte dans leur analyse les
ph&#233;nom&#232;nes discursifs locaux (e.g. annonces th&#233;matiques &#8211; e.g. &#8220;Les points que nous allons
traiter sont :&#8221;, structures &#233;num&#233;ratives, transitions, etc.).
Notre approche se situe parmi les travaux qui proposent de rechercher le point d&#8217;attache optimal
d&#8217;un &#233;nonc&#233; entrant dans la structure en cours de construction. Parmi les approches existantes,
Marcu (1999) propose un syst&#232;me pour la d&#233;tection automatique de la structure rh&#233;torique d&#8217;un
texte, Choi (2002) s&#8217;int&#233;resse &#224; une structuration th&#233;matique fine, Kruijff-Korbayov&#225; &amp; Kruijff
(1996) analysent le discours en terme de progression th&#233;matique. Ces syst&#232;mes constituent de
s&#233;rieuses avanc&#233;es mais requi&#232;rent encore la prise en compte de plus d&#8217;indices discursifs et de
mod&#232;les plus souples pour appr&#233;hender les diff&#233;rents m&#233;canismes de structuration du discours.
</p>
<p>Dans ce papier, nous pr&#233;sentons un syst&#232;me de D&#233;tection de Structures fines de Texte (ap-
pel&#233; DST). DST utilise un mod&#232;le pr&#233;dictif obtenu par un algorithme d&#8217;apprentissage qui, pour
une configuration d&#8217;indices discursifs donn&#233;s, pr&#233;dit le type de relation de d&#233;pendance exis-
tant entre deux &#233;nonc&#233;s. L&#8217;originalit&#233; principale de notre approche est de proposer un mod&#232;le
Th&#233;orique simplifi&#233; de la Structure du Discours. En effet, nous nous int&#233;ressons seulement
au rapport structurel &#233;l&#233;mentaire liant deux &#233;nonc&#233;s (relation de subordination, de coordina-
tion, et absence de relation) ind&#233;pendamment d&#8217;un &#233;ventuel &#233;tiquetage s&#233;mantico-rh&#233;torique de
la relation2. Le fait de dissocier le mod&#232;le de d&#233;pendance de la recherche du point d&#8217;attache
de l&#8217;&#233;nonc&#233; entrant nous permet d&#8217;envisager diff&#233;rents algorithmes de structuration. Une de
nos particularit&#233;s techniques est de proposer une mesure pour appr&#233;hender le parall&#233;lisme
syntaxico-s&#233;mantique de deux &#233;nonc&#233;s, indice discursif peu consid&#233;r&#233; jusqu&#8217;&#224; pr&#233;sent. Nous
avons travaill&#233; sur des articles scientifiques en anglais mais notre d&#233;marche est adaptable &#224;
d&#8217;autres langues comme le fran&#231;ais.
</p>
<p>1New York Public Library, University of Michigan, Stanford, Harvard (USA), Oxford (GB).
2Cette t&#226;che sera abord&#233;e ult&#233;rieurement.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>D&#233;tection Automatique de Structures Fines de Texte
</p>
<p>2 L&#8217;acc&#232;s au contenu
</p>
<p>Le processus de compr&#233;hension requiert d&#8217;une part d&#8217;identifier des unit&#233;s discursives (informa-
tionnelles, intentionnelles, ayant une mise en forme visuelle, ou autres) et d&#8217;autre part d&#8217;&#233;tablir
des relations entre ces unit&#233;s. Cette reconnaissance de la coh&#233;rence peut n&#233;cessiter des connais-
sances s&#233;mantiques et pragmatiques, non disponibles dans le texte. N&#233;anmoins nous partons du
postulat qu&#8217;il est possible de mettre en place des analyses automatiques &#224; partir des indices
du discours (cha&#238;nes lexicales, connecteurs, introducteurs de cadres, etc.) pour permettre de
reconna&#238;tre cette coh&#233;rence.
</p>
<p>L&#8217;une des caract&#233;ristiques majeures transversale &#224; la plupart des th&#233;ories du discours est la con-
sid&#233;ration d&#8217;un mod&#232;le de d&#233;pendance qui d&#233;finit la nature de la relation structurelle existante
entre deux &#233;nonc&#233;s en terme de subordination ou de coordination (Mann &amp; Thompson, 1987;
Polanyi, 1988; Virbel, 1989; Asher &amp; Lascarides, 1994). Les diff&#233;rences entre th&#233;ories viennent
de la signification qu&#8217;elles donnent &#224; la nature de ces relations, mais aussi des contraintes struc-
turelles d&#8217;assemblage des unit&#233;s discursives. Au niveau de la micro-structure, les chercheurs
ont tendance &#224; consid&#233;rer que l&#8217;unit&#233; &#233;l&#233;mentaire de r&#233;f&#233;rence est proche de celle de la proposi-
tion (Mann &amp; Thompson, 1987; Polanyi, 1988). Afin de faciliter le rep&#233;rage automatique, nous
consid&#233;rons comme Choi (2002) la phrase syntaxique comme unit&#233; &#233;l&#233;mentaire.
Suivant le genre de texte consid&#233;r&#233; (expositif, narratif, dialogue, etc.), les th&#233;ories du discours
mettent en &#233;vidence un ou plusieurs plans d&#8217;organisation de l&#8217;information: rh&#233;torique, logico-
visuelle, informationnelle, etc. Les interactions entre ces diff&#233;rentes structures sont encore
tr&#232;s floues, c&#8217;est pourquoi nous avons d&#233;cid&#233; de nous concentrer sur le plan informationnel
que nous consid&#233;rons comme pertinent pour les textes scientifiques. Notre description du plan
informationnel repose sur la th&#233;orie de la RST3 (Mann &amp; Thompson, 1987), le LDM (Polanyi,
1988), et aussi la progression th&#233;matique de la phrase au discours (Kruijff-Korbayov&#225; &amp; Kruijff,
1996). Globalement cela signifie que la relation entre deux &#233;nonc&#233;s est d&#233;termin&#233;e en fonction
de leur contenu informationnel ind&#233;pendamment de l&#8217;intention rh&#233;torique de l&#8217;auteur.
</p>
<p>Dans notre mod&#232;le, un &#233;nonc&#233; entrant se rattache au discours selon une relation de subordination
ou de coordination (ou bien les deux). Un &#233;nonc&#233; est interpr&#233;t&#233; en fonction de son th&#232;me (ce
dont il parle), de son propos (ce qui est dit au sujet du th&#232;me) et de sa fonction s&#233;mantico-
rh&#233;torique. Ces &#233;l&#233;ments sont identifi&#233;s &#224; partir d&#8217;indices pr&#233;sents dans l&#8217;&#233;nonc&#233; et dans son
contexte ce qui permet de d&#233;duire avec quelles parties du discours il est li&#233; et comment.
</p>
<p>Nous illustrons ces relations &#224; l&#8217;aide du texte de la figure 1 extrait d&#8217;un passage de notre corpus.
Les indices discursifs ais&#233;ment repr&#233;sentables visuellement sont soulign&#233;s dans le texte. Les
couples d&#8217;&#233;nonc&#233;s (1, 2) et (1, 6) d&#233;crivent des relations de subordination. 1 est un mod&#232;le
classique d&#8217;annonce th&#233;matique avec un quantifieur two, une phrase syntaxiquement incom-
pl&#232;te et un caract&#232;re de ponctuation annonce &#8220;:&#8221;. Les &#233;nonc&#233;s 2 et 6, quant &#224; eux, contiennent
des marques qui caract&#233;risent des items d&#8217;une &#233;num&#233;ration (&#8220;1.&#8221; et &#8220;2.&#8221;). Ces deux &#233;nonc&#233;s
pr&#233;sentent aussi une relation de coordination explicite l&#8217;un envers l&#8217;autre, soulign&#233;e notamment
par un parall&#233;lisme syntaxique :
</p>
<p>NUM. NOM, whereby ADJ NOM be+conj=&#8217;pr&#233;sent&#8217; VERB+conj=&#8217;participe pass&#233;&#8217; PREP
Le couple d&#8217;&#233;nonc&#233;s (2, 3) constitue un exemple de subordination o&#249; le deuxi&#232;me &#233;nonc&#233; 3
correspond au d&#233;veloppement d&#8217;un des aspects du premier. Cette subordination est marqu&#233;e
par une progression th&#233;matique de rh&#232;me en th&#232;me (i.e. le terme importance qui est repris
dans 3). Le couple d&#8217;&#233;nonc&#233;s (4, 5) d&#233;crit, quant &#224; lui, un exemple de coordination implicite.
</p>
<p>3Rhetorical Structure Theory (RST), Linguistic Discourse Model (LDM).</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Nicolas Hernandez et Brigitte Grau
</p>
<p>Two traditional approaches to automatic abstracting are: (1)
1. Extraction, whereby specific sentences are selected from the source text according to some
assessment of their importance. (2)
Importance indicators include the concentration of topic-relevant terms [. . . ]; the occurrence of
expressions, such as &quot;important&quot;, &quot;to sum up&quot;; and the position of the sentence within the text. (3)
This approach is exemplified by Pollock and Zamora&#8217;s ADAM system [1]. (4)
The problems with this approach are that importance clues are often not reliable, and that the extracted
sentences do not always constitute a coherent text, since they often contain cross-references. (5)
2. Summarisation, whereby detailed semantic analysis is applied to the text, and a representation
such as a semantic net is produced, from which a summary is then generated. (6)
</p>
<p>Figure 1: Exemples de relations de subordination, et de coordination explicite et implicite
</p>
<p>En effet, tous les deux sont subordonn&#233;s &#224; une m&#234;me entit&#233;, l&#8217;approche en terme d&#8217;extraction,
et chacun d&#8217;eux en traite de mani&#232;re ind&#233;pendante, le premier en pr&#233;sentant un exemple et le
second en d&#233;crivant les probl&#232;mes.
</p>
<p>3 Algorithme de structuration &#8220;shift and reduce&#8221;
La structure de texte en arbre unique est une simplification de la r&#233;alit&#233;, n&#233;anmoins nous adop-
tons une mod&#233;lisation hi&#233;rarchique parce qu&#8217;elle reste la plus commun&#233;ment rencontr&#233;e dans
les textes. Notre algorithme de structuration reprend le principe des algorithmes de Marcu
(1999) et Choi (2002). Nous l&#8217;avons adapt&#233; afin de tenir compte de la relation de coordination.
Cet algorithme construit une structure hi&#233;rarchique du discours dont les arcs sont orient&#233;es vers
les &#233;nonc&#233;s entrants toujours attach&#233;s sur la fronti&#232;re droite de l&#8217;arbre. Un &#233;nonc&#233; entrant coor-
donn&#233; &#224; un &#233;nonc&#233; de la structure est consid&#233;r&#233; comme &#233;tant subordonn&#233; au m&#234;me &#233;nonc&#233; que
l&#8217;&#233;nonc&#233; auquel il est coordonn&#233;. Un n&#248;eud factice joue le r&#244;le de p&#232;re de tous les n&#248;euds.
L&#8217;algorithme utilise deux structures de donn&#233;es : une pile qui stocke la branche &#8220;fronti&#232;re
droite&#8221; de l&#8217;arbre en cours de construction (le dernier &#233;l&#233;ment empil&#233; est le point d&#8217;attache
le plus prioritaire), et une file qui contient la liste des &#233;nonc&#233;s tels qu&#8217;ils sont ordonn&#233;s dans
le texte et analys&#233;s successivement. La pile joue un r&#244;le de m&#233;moire dont chaque &#233;l&#233;ment
correspond &#224; une granularit&#233; inf&#233;rieure obtenue dans la structure du discours. L&#8217;objectif est
d&#8217;identifier les &#233;nonc&#233;s qui sont li&#233;s et les relations qu&#8217;ils entretiennent.
</p>
<p>Algorithme :
1. Si la pile est vide, on d&#233;file la file et empile la pile (&#233;tat initial).
2. Tant que la pile et la file ne sont pas vides, calcul de la relation entre l&#8217;&#233;l&#233;ment au sommet
</p>
<p>de la pile et le premier &#233;l&#233;ment de la file.
&#0; Si une relation de subordination est d&#233;tect&#233;e, alors l&#8217;&#233;l&#233;ment de la file est d&#233;fil&#233; et
</p>
<p>empil&#233; (on descend dans la granularit&#233; du texte) ;
&#0; Si une relation de coordination est d&#233;tect&#233;e, alors l&#8217;&#233;l&#233;ment au sommet de la pile est
</p>
<p>d&#233;pil&#233; et remplac&#233; par l&#8217;&#233;l&#233;ment de la file ;
&#0; Sinon (aucune relation) l&#8217;&#233;l&#233;ment au sommet de la pile est d&#233;pil&#233; et &#233;cart&#233; (l&#8217;id&#233;e
</p>
<p>&#233;tant de remonter jusqu&#8217;au niveau de d&#233;pendance li&#233; &#224; l&#8217;&#233;l&#233;ment en t&#234;te de file).</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>D&#233;tection Automatique de Structures Fines de Texte
</p>
<p>4 Indices discursifs
</p>
<p>La reconnaissance des relations discursives entre deux &#233;nonc&#233;s est fond&#233;e sur la pr&#233;sence, ou
l&#8217;absence, d&#8217;indices significatifs dans les textes scientifiques : relations lexicales, expressions
clefs et parall&#233;lisme de construction.
</p>
<p>4.1 Relations lexicales
</p>
<p>Les relations lexicales entre deux &#233;nonc&#233;s sont envisag&#233;es selon leur nature s&#233;mantique et selon
les parties des &#233;nonc&#233;s concern&#233;es (th&#232;me ou rh&#232;me). Nous utilisons un module de Construction
de Cha&#238;nes Lexicales (CCL) pour les rep&#233;rer. Celui-ci est fond&#233; sur une variante de l&#8217;algorithme
de (Barzilay &amp; Elhadad, 1997). CCL recherche les relations entre les lemmes associ&#233;s aux
paires de mots &#233;tudi&#233;s en tenant compte de la distance s&#233;mantique entre ces mots ainsi que de
leur distance dans le texte. Le mot le plus fr&#233;quent au sein d&#8217;une cha&#238;ne constitue son &#233;l&#233;ment
repr&#233;sentatif. Nous consid&#233;rons :
</p>
<p>&#0; Les relations morphologiques : deux mots appartenant &#224; la m&#234;me famille morphologique4,
ind&#233;pendamment de leur cat&#233;gorie lexicale ;
</p>
<p>&#0; Les relations utilis&#233;es pour r&#233;f&#233;rer &#224; un m&#234;me objet du discours, telles que la synonymie,
l&#8217;hyperonymie et l&#8217;hyponymie, la m&#233;ronymie et l&#8217;holonymie, trouv&#233;es dans WordNet ;
</p>
<p>&#0; Les relations d&#8217;antonymie trouv&#233;es gr&#226;ce &#224; WordNet ou &#224; la pr&#233;sence de pr&#233;fixes tels
que dis-, in-, un-, non-, under-, im-, a-, de-, ir-, anti- sur les m&#234;mes lemmes. Nous
construisons des cha&#238;nes lexicales sp&#233;cifiques &#224; ce type de relation.
</p>
<p>Etant donn&#233;es deux phrases constituant le contexte d&#8217;&#233;tudes, des cha&#238;nes lexicales sont calcul&#233;es
entre les deux phrases, globalement, et entre les diff&#233;rentes combinaisons des parties constituant
le th&#232;me et le rh&#232;me des deux phrases. La distinction entre les parties th&#233;matique et rh&#233;matique
d&#8217;une phrase est r&#233;alis&#233;e selon une heuristique robuste de d&#233;coupage de la phrase en deux par
rapport au verbe le plus proche de son milieu.
</p>
<p>La pr&#233;sence d&#8217;un lien lexical entre les rh&#232;mes de deux &#233;nonc&#233;s traduit g&#233;n&#233;ralement une sub-
ordination du deuxi&#232;me &#233;nonc&#233; vis-&#224;-vis de l&#8217;&#233;nonc&#233; pr&#233;c&#233;dent (e.g. une &#233;laboration ou une
reformulation). Une progression lin&#233;aire, de rh&#232;me en th&#232;me, correspond aussi au m&#234;me type
de subordination (e.g. une annonce th&#233;matique). Une relation contrastive peut d&#233;noter une co-
ordination. Dans tous les autres cas, la pr&#233;sence ou l&#8217;absence d&#8217;une relation lexicale constitue
un indice suppl&#233;mentaire qui pourra se combiner avec les suivants.
</p>
<p>4.2 Expressions clefs (essentiellement des connecteurs)
Notre liste d&#8217;expressions clefs est issue en partie de la liste de m&#233;ta-descripteurs acquise au-
tomatiquement par Hernandez &amp; Grau (2003), et de l&#8217;analyse de notre corpus. Nous l&#8217;avons
aussi compl&#233;t&#233;e &#224; partir des mots clefs fournis par Choi (2002) pour lesquels nous r&#233;assignons
la relation (subordination ou coordination) en fonction de nos observations personnelles.
En raison du nombre d&#8217;exemples que compte notre corpus (1190 couples de phrases) par rap-
port au nombre de marques que nous avons retenu (178), nous n&#8217;avons pas choisi de consid&#233;rer
chacune des marques comme une caract&#233;ristique distincte, au contraire de Choi dont le mod&#232;le
</p>
<p>4Nous utilisons la base CELEX (www.ldc.upenn.edu/readme_files/celex.readme.html).</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Nicolas Hernandez et Brigitte Grau
</p>
<p>compte 19 marques. Nous avons opt&#233; pour une pr&#233;-classification de celles-ci en 5 classes en
fonction de leur comportement pour structurer le discours et r&#233;duit ainsi la complexit&#233; du nom-
bre d&#8217;indices. Les classes rassemblent des marques ayant un m&#234;me &#8220;comportement&#8221; structurel
vis-&#224;-vis de la subordination et de la coordination entre deux &#233;nonc&#233;s. Les classes que nous
avons d&#233;finies sont les suivantes :
</p>
<p>&#0; Initie : marque le premier item d&#8217;une liste d&#8217;items (suppose une coordination) : &#8220;former,
first, on the one hand, &#8217;1.&#8217;, &#8217;a)&#8217;, begin, start&#8221; ;
</p>
<p>&#0; Continue : Coordonne mais n&#8217;initie pas une liste et n&#8217;en termine pas forc&#233;ment une :
&#8220;second, another, other, also, and, or, however, but, then, in addition, although, etc.&#8221; ;
</p>
<p>&#0; Termine : Marque le dernier item d&#8217;une liste (suppose une coordination) : &#8220;on the other
hand, last, finally, to conclude, to sum up, end, finish, latter, in conclusion, result&#8221; ;
</p>
<p>&#0; Subordonn&#233; : Appara&#238;t en d&#233;but d&#8217;un &#233;nonc&#233; subordonn&#233; : &#8220;so, the, this, these, it, he, by
this, consequently, for example, for instance, therefore, thus, note that, such, etc. &#8221; ;
</p>
<p>&#0; Subordonnant : Appara&#238;t en fin d&#8217;un &#233;nonc&#233; subordonnant (i.e. en g&#233;n&#233;ral une annonce) :
&#8220;such as, follow, as follow, see below, below, and, :, ?, etc. &#8221;.
</p>
<p>Une marque est dans une seule classe sauf si cette derni&#232;re permet de la distinguer dans sa d&#233;f-
inition (e.g. la position dans la phrase pour diff&#233;rencier les marques subordonn&#233;es des marques
subordonnantes). Les notions de d&#233;but et de fin sont relatives &#224; chaque &#233;nonc&#233; et correspondent
&#224; une distance en nombre de mots exprim&#233;e en pourcentage (respectivement fix&#233;e &#224; 40% du
d&#233;but ou de la fin). La taille maximale est fix&#233;e &#224; 10 tokens.
En plus de ces classes de marques discursives, nous rajoutons une classe de marques d&#233;sig-
nant la n&#233;gation (e.g. aren&#8217;t, can&#8217;t, nothing, nobody, rarely, etc.) et afin de prendre en compte
les formes passives, et l&#8217;inversion des parties th&#233;matiques et rh&#233;matiques qui en d&#233;coule, nous
consid&#233;rons la pr&#233;sence du verbe &#8220;&#234;tre&#8221; suivi directement d&#8217;un autre verbe comme une carac-
t&#233;ristique. Par la suite, nous appellerons ces derni&#232;res caract&#233;ristiques les indices syntaxiques.
Au final, la distribution de nos 178 marques se r&#233;partit ainsi : 7 marques pour la classe Initie,
38 pour la classe Continue, 9 pour la classe Termine, 62 pour la classe Subordonn&#233;e, 30 pour la
classe Subordonnant, 31 marques de n&#233;gation et 1 marque du verbe &#234;tre.
</p>
<p>4.3 Parall&#233;lisme
</p>
<p>Le parall&#233;lisme de construction entre deux &#233;nonc&#233;s rend compte d&#8217;une importance &#233;gale (lien
de coordination) (Hernandez, 2004). Il se traduit par a) des similarit&#233;s des constituants &#224; dif-
f&#233;rents niveaux paradigmatiques (lemme, trait s&#233;mantique, cat&#233;gorie grammaticale, fonction
syntaxique) ; b) une similarit&#233; syntagmatique qui s&#8217;exprime &#224; la fois par une similarit&#233; dans
l&#8217;ordre des constituants parall&#232;les et par une similarit&#233; dans les &#233;carts de distance entre ces
m&#234;mes constituants.
</p>
<p>Afin de calculer le degr&#233; de parall&#233;lisme entre deux &#233;nonc&#233;s, nous r&#233;duisons la complexit&#233;
du probl&#232;me d&#8217;abord en homog&#233;n&#233;isant les entit&#233;s du discours (chaque mot est remplac&#233; par
l&#8217;&#233;l&#233;ment repr&#233;sentatif de la cha&#238;ne lexicale &#224; laquelle il appartient). Ensuite, chaque structure
syntaxique hi&#233;rarchique est remplac&#233;e par une liste plate, qui correspond &#224; une notation pr&#233;fix&#233;e
de l&#8217;arbre (les n&#339;uds internes, qui sont des &#233;tiquettes, sont plac&#233;s avant les feuilles, qui sont les
lemmes). Cette liste est obtenue &#224; partir du r&#233;sultat d&#8217;analyse fourni par l&#8217;analyseur statistique
de Charniak (1997)5, en supprimant les niveaux de parenth&#232;ses.
</p>
<p>5Nous utilisons la version 2001, d&#233;velopp&#233;e pour l&#8217;anglais &#224; l&#8217;universit&#233; de Brown.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>D&#233;tection Automatique de Structures Fines de Texte
</p>
<p>Pour tout couple de phrases donn&#233;, le syst&#232;me calcule un degr&#233; de parall&#233;lisme entre toutes les
s&#233;quences extraites de chacune des phrases, comportant le m&#234;me nombre d&#8217;items similaires,
au minimum deux, diff&#233;rents ou non, plac&#233;s dans leur ordre d&#8217;apparition dans les phrases. Par
exemple, les phrases cabcad et acba partagent 4 constituants : c, a deux fois et b. Une fois
supprim&#233;s les constituants non similaires (i.e. d), on extrait de la premi&#232;re phrase caba et
abca, et de la deuxi&#232;me acba. On ne tient pas compte des &#233;l&#233;ments diff&#233;rents, qui peuvent &#234;tre
ins&#233;r&#233;s n&#8217;importe o&#249; dans les phrases. Le parall&#233;lisme est fond&#233; sur des constructions similaires
d&#8217;&#233;l&#233;ments similaires. La mesure que nous avons d&#233;finie s&#8217;inspire des mesures de distances
d&#8217;&#233;dition entre des s&#233;quences de caract&#232;res. Chaque constituant est identifi&#233; de mani&#232;re unique
par sa position. Plus un constituant est distant de son sym&#233;trique dans l&#8217;autre s&#233;quence, plus les
s&#233;quences compar&#233;es diff&#232;rent. Elle est d&#233;finie par la formule suivante :
</p>
<p>&#0;&#2;&#1;&#4;&#3;&#2;&#5;&#6;&#1;&#8;&#7;&#9;&#1;&#8;&#10;&#12;&#11;&#13;&#5;&#6;&#11;&#13;&#14;&#15;&#14;&#16;&#1;&#8;&#14;&#18;&#17;&#20;&#19;&#8;&#21;&#22;&#1;&#13;&#23;&#24;&#19;&#4;&#25;ff&#26;fi&#19;&#4;fl&#31;ffi! #&quot;%$%&amp;&#24;'
</p>
<p>(
</p>
<p>)+*-,
</p>
<p>&#23;.&#23;
</p>
<p>&#7;/&#23;&#24;&#19;0ffi213&#0;4&#23;&#18;5
</p>
<p>)
</p>
<p>ffi
</p>
<p>&#7;/&#23;6&#19;7ffi
</p>
<p>ffi.ffi
</p>
<p>avec
5
</p>
<p>)
</p>
<p>, le
&#17;68
</p>
<p>&#25;
</p>
<p>8
</p>
<p>constituant de la s&#233;quence
&#19;&#8;&#25;
</p>
<p>,
</p>
<p>&#14;9&#23;6&#19;7ffi
</p>
<p>, la longueur des s&#233;quences compar&#233;es,
&#7;/&#23;6&#19;7ffi
</p>
<p>,
</p>
<p>la distance maximale possible entre un constituant d&#8217;une s&#233;quence
&#19;
</p>
<p>et son constituant parall&#232;le
i.e.
</p>
<p>&#7;/&#23;&#24;&#19;0ffi: ;&#14;9&#23;6&#19;7ffi&lt;1&gt;=
</p>
<p>, et
&#0;
</p>
<p>, la distance effective d&#8217;un constituant courant de la s&#233;quence
&#19;0&#25;
</p>
<p>et
son constituant parall&#232;le. Le degr&#233; de parall&#233;lisme d&#8217;un couple d&#8217;&#233;nonc&#233;s correspond au degr&#233;
maximal obtenu pour les s&#233;quences extraites de ces &#233;nonc&#233;s.
</p>
<p>5 Apprentissage des relations discursives
</p>
<p>Afin de reconna&#238;tre les relations discursives, nous avons d&#233;cid&#233; d&#8217;opter, de m&#234;me que Marcu
(1999), pour un apprentissage par arbre de d&#233;cision qui poss&#232;de l&#8217;avantage d&#8217;&#234;tre compr&#233;hen-
sible par tout utilisateur (si la taille de l&#8217;arbre produit est raisonnable) et d&#8217;avoir une traduction
imm&#233;diate en terme de r&#232;gles de d&#233;cision. Nous avons utilis&#233; le classifieur C4.5 fourni dans le
logiciel WEKA6. Les caract&#233;ristiques que nous venons de d&#233;crire sont au nombre de 22 et sont
rep&#233;r&#233;es automatiquement dans le corpus.
</p>
<p>5.1 Donn&#233;es
</p>
<p>Afin de constituer un ensemble de couples de phrases et de relations correspondantes, nous
avons manuellement annot&#233; un corpus de 5 documents anglais appartenant au domaine de la
linguistique informatique. Ils font tous entre 8 et 10 pages et sont au format pdf. L&#8217;un d&#8217;eux est
en simple colonne. De fait ils couvrent la p&#233;riode 1998 et 1999 et aucun d&#8217;eux ne partage de
r&#233;f&#233;rences communes. Ces articles sont Mitkov (COLING-ACL&#8217;98), Kan et al. (WVLC&#8217;98),
Green (ACL&#8217;98), Sanderson et al. (SIGIR&#8217;99) et Oakes et al. (IRSG&#8217;99).
L&#8217;annotation a consist&#233; &#224; indiquer pour chaque phrase du texte les relations de subordination
et de coordination explicite existant avec une phrase se trouvant en amont dans le texte ; ces
deux types de relations pouvant exister pour une m&#234;me phrase. Le principe de d&#233;pendance que
nous avons suivi consiste &#224; toujours resituer un &#233;nonc&#233; vis-&#224;-vis de la th&#233;matique globale puis
d&#8217;analyser si localement il n&#8217;y a pas des d&#233;pendances plus fortes. Chaque couple d&#8217;&#233;nonc&#233;s que
nous avons li&#233;s est d&#233;crit par une d&#233;cision,
</p>
<p>&#7;
</p>
<p>, concernant le type de relation qui les unit. Ces
6Cette bo&#238;te &#224; outils est disponible &#224; l&#8217;URL suivante www.cs.waikato.ac.nz/ml/weka.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Nicolas Hernandez et Brigitte Grau
</p>
<p>couples sont ensuite repr&#233;sent&#233;s par l&#8217;ensemble des caract&#233;ristiques discursives, &#0; , que nous
avons pr&#233;c&#233;demment d&#233;finies. Sur un total de 1038 phrases7, 1190 couples exemples, &#23; &#0; &#26; &#7; ffi ,
ont &#233;t&#233; constitu&#233;s. Ils se r&#233;partissent en 632 couples li&#233;es par une relation de subordination,
285 instances &#8220;coordination&#8221; et 273 instances d&#233;crivant une absence de relation. Les instances
d&#233;crivant une absence de relation ont &#233;t&#233; engendr&#233;es automatiquement en consid&#233;rant les cou-
ples d&#8217;&#233;nonc&#233;s contigus ne poss&#233;dant pas de relation entre eux. En comparaison Choi utilise un
corpus d&#8217;apprentissage de 754 exemples.
</p>
<p>5.2 R&#233;sultats
</p>
<p>De part la quantit&#233; de nos donn&#233;es d&#8217;apprentissage (relative au co&#251;t en temps d&#8217;annotation
de corpus), nous adoptons une technique d&#8217;&#233;valuation par validation crois&#233;e sur 10 partitions.
Son principe consiste &#224; partitionner le corpus d&#8217;apprentissage en un certain nombre de parts
&#233;gales et d&#8217;utiliser tour &#224; tour une partie comme ensemble d&#8217;exemples de test et les autres
comme ensemble d&#8217;exemples d&#8217;entra&#238;nement. La moyenne des taux d&#8217;erreur correspond au
taux d&#8217;erreur global.
</p>
<p>Exp&#233;riences
coordination_et_subordination Progression Expressions Progression th&#233;matique
approche de base de 53,10% th&#233;matique clefs Et Expressions clefs
</p>
<p>Ensemble de base 52,68% 57,31% 56,13%
</p>
<p>Caract&#233;ristique
ajout&#233;e
</p>
<p>coh&#233;sion lexicale 52,68% 57,31% 57,05%
antonymie 52,43% 56,89% 55,79%
indices syntaxiques 54,70% 58,57% 55,71%
# de mots communs 52,43% 57,31% 56,47%
degr&#233; de parall&#233;lisme 52,43% 56,97% 55,12%
</p>
<p>Toutes les caract&#233;ristiques 54,62% 57,05% 55,21%
seulement_la_subordination Progression Expressions Progression th&#233;matique
approche de base de 69,83% th&#233;matique clefs Et Expressions clefs
</p>
<p>Ensemble de base 69,83% 73,14% 73,59%
</p>
<p>Caract&#233;ristique
ajout&#233;e
</p>
<p>coh&#233;sion lexicale 69,83% 72,26% 73,70%
antonymie 69,83% 73,14% 73,70%
indices syntaxiques 72,48% 76,35% 75,02%
# de mots communs 69,83% 72,81% 74,03%
degr&#233; de parall&#233;lisme 69,83% 73,59% 74,25%
</p>
<p>Toutes les caract&#233;ristiques 70,16% 75,13% 75,02%
</p>
<p>Table 1: Pr&#233;cisions de DST dans la pr&#233;diction de relation
</p>
<p>Nous avons r&#233;alis&#233; deux jeux d&#8217;exp&#233;rience : le premier en consid&#233;rant toutes les relations de
notre mod&#232;le (subordination, coordination et absence de relation), le deuxi&#232;me en ne consid-
&#233;rant plus que la relation de subordination et l&#8217;absence de relation. Ce dernier jeu d&#8217;exp&#233;riences
nous permet de comparer nos r&#233;sultats avec ceux de Choi (2002). Pour simplifier la pr&#233;senta-
tion de ces jeux d&#8217;exp&#233;riences par la suite, nous omettrons la relation &#8220;absence de relation&#8221;
dans leur d&#233;signation. Pour chacun des jeux nous proposons de comparer les r&#233;sultats sur deux
ensembles d&#8217;indices de base distincts auxquels on ajoute tour &#224; tour telle ou telle caract&#233;ristique
pour observer les am&#233;liorations &#233;ventuelles. Les performances des deux ensembles combin&#233;s
sont aussi consid&#233;r&#233;es. Ces deux ensembles de base sont : 1) les caract&#233;ristiques d&#233;crivant la
</p>
<p>7Les phrases ont &#233;t&#233; d&#233;tect&#233;es &#224; l&#8217;aide des caract&#232;res de ponctuation puis corrig&#233;es manuellement.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>D&#233;tection Automatique de Structures Fines de Texte
</p>
<p>progression th&#233;matique (de th&#232;me en th&#232;me, de th&#232;me en rh&#232;me, de rh&#232;me en th&#232;me et de rh&#232;me
en rh&#232;me) ; 2) les caract&#233;ristiques fond&#233;es sur les expressions clefs : les classes Initie, Termine,
Continue, Subordonn&#233; et Subordonnant. Les caract&#233;ristiques individuelles que nous ajoutons
sont : les liens lexicaux autre que d&#8217;antonymie (appel&#233;s par la suite &#8220;coh&#233;sion lexicale&#8221;), les
liens lexicaux d&#8217;antonymie, les indices syntaxiques (be et n&#233;gation), le degr&#233; de parall&#233;lisme et
le nombre de mots communs (approche simplifi&#233;e de notre mesure du parall&#233;lisme).
Afin de positionner l&#8217;apport des diff&#233;rents apprentissages, nous comparons leur performance
vis-&#224;-vis d&#8217;une approche de base qui correspond &#224; la pr&#233;diction de la classe majoritaire dans le
corpus d&#8217;apprentissage (c&#8217;est-&#224;-dire qu&#8217;elle correspond au taux d&#8217;erreur si l&#8217;on assigne tous les
exemples &#224; cette classe).
La table 1 d&#233;crit les r&#233;sultats que nous obtenons respectivement lorsque l&#8217;on consid&#232;re les
relations de coordination et de subordination, puis lorsque l&#8217;on ne consid&#232;re plus que la relation
de subordination. Les valeurs en gras correspondent &#224; des pr&#233;cisions maximales.
</p>
<p>Le premier constat que nous faisons est que nous obtenons des r&#233;sultats compris entre 60% et
75% similaires &#224; ceux de Choi (2002) et de Marcu (1999). Plus particuli&#232;rement, nous obtenons
des r&#233;sultats sup&#233;rieurs &#224; ceux obtenus par Choi dans des configurations exp&#233;rimentales simi-
laires : 76,35% contre 73,61% pour nos meilleures performances de pr&#233;cision respectives.
</p>
<p>Par rapport aux approches de base, les meilleurs sous-ensembles de caract&#233;ristiques augmentent
la pr&#233;cision de plus de 5% pour chacun des jeux d&#8217;exp&#233;riences. Il existe n&#233;anmoins des sous-
ensembles qui d&#233;t&#233;riorent les performances et les r&#233;sultats sont en g&#233;n&#233;ral moins bon pour le
jeu coordination_et_subordination.
Les meilleurs r&#233;sultats que nous obtenons sont &#224; partir de l&#8217;ensemble de base compos&#233; de
caract&#233;ristiques fond&#233;es sur les expressions clefs.
Les r&#233;sultats obtenus avec les caract&#233;ristiques fond&#233;es sur des liens lexicaux quels qu&#8217;ils soient,
combin&#233;es ou non, sont bien en dessous de ceux que l&#8217;on pouvait esp&#233;rer. Pour le jeu coordi-
nation_et_subordination les exp&#233;riences men&#233;es &#224; partir de l&#8217;ensemble de base &#8220;progression
th&#233;matique&#8221; d&#233;teriorent pour la plupart la pr&#233;cision de l&#8217;approche de base. Pour le jeu seule-
ment_la_subordination, la pr&#233;cision des exp&#233;riences &#224; partir de l&#8217;ensemble de base &#8220;progression
th&#233;matique&#8221; reste inchang&#233;e par rapport &#224; l&#8217;approche de base. Le gain notable de l&#8217;ensemble
&#8220;progression th&#233;matique&#8221; vient lorsqu&#8217;il est combin&#233; &#224; l&#8217;ensemble &#8220;expressions clefs&#8221;.
</p>
<p>Un gain inattendu est celui apport&#233; par le couple de pr&#233;sence du verbe &#234;tre ou d&#8217;une n&#233;gation.
Ce r&#233;sultat requiert un retour au texte pour d&#233;terminer un ph&#233;nom&#232;ne discursif &#233;ventuel.
</p>
<p>Enfin, lorsque l&#8217;on compare les caract&#233;ristiques &#8220;nombre de mots pleins communs&#8221; et &#8220;degr&#233;
de parall&#233;lisme&#8221; les diff&#233;rences sont l&#233;g&#232;res mais mettent en avant le degr&#233; de parall&#233;lisme.
</p>
<p>6 Conclusion
</p>
<p>Notre approche du discours enrichit le mod&#232;le de Choi (2002) qui ne consid&#232;re que la relation
de subordination. Nous consid&#233;rons en plus la relation de coordination ce qui nous permet de
mod&#233;liser plus finement le discours.
</p>
<p>Le syst&#232;me de Marcu (1999) se situe &#224; un degr&#233; sup&#233;rieur de complexit&#233; dans le sens o&#249; il
cherche &#224; reconna&#238;tre l&#8217;op&#233;ration de structuration &#224; r&#233;aliser en fonction du contexte et de la</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Nicolas Hernandez et Brigitte Grau
</p>
<p>configuration structurelle en cours. Marcu fait des hypoth&#232;ses tr&#232;s fortes sur le type de structure
et d&#8217;attachements possibles. En comparaison, le fait de dissocier le mod&#232;le de d&#233;pendance de
la structuration nous permet de fixer ind&#233;pendamment les contraintes de structuration, et par l&#224;
d&#8217;appr&#233;hender plus largement les diff&#233;rents ph&#233;nom&#232;nes de structuration du discours (i.e. des
structures autres que hi&#233;rarchiques orient&#233;es vers la fronti&#232;re droite). Ce type de mod&#233;lisation
peut ainsi &#234;tre utilis&#233; pour analyser par exemple des dialogues.
</p>
<p>En utilisant l&#8217;algorithme &#8220;shift and reduce&#8221;, nous obtenons une structure hi&#233;rarchique proche de
celle d&#8217;une structure d&#233;crite par une analyse RST (correspondance entre les plans information-
nelles et intentionnelles). La diff&#233;rence majeure survient au niveau de la nucl&#233;arit&#233; des relations
unissant les &#233;nonc&#233;s.
</p>
<p>Parmi nos perspectives nous envisageons d&#8217;enrichir notre mod&#232;le avec la relation de subordina-
tion dirig&#233;e vers l&#8217;aval du texte, ainsi que de nouveaux indices (comme ceux de mis en forme
visuelle) qu&#8217;ils se trouvent dans les &#233;nonc&#233;s consid&#233;r&#233;s ou dans leur contexte.
</p>
<p>R&#233;f&#233;rences
Nicholas Asher et Alex Lascarides. Intentions and information in discourse. 1994.
Regina Barzilay et Michael Elhadad. Using lexical chains for text summarization. In Proceedings of the
ACL&#8217;97/EACL&#8217;97 Workshop on Intelligent Scalable Text Summarization, Madrid, Spain, July 11 1997.
Eugene Charniak. Statistical parsing with a context-free grammar and word statistics. In Proceedings of
the Fourteenth National Conference on Artificial Intelligence AAAI, Menlo Park, 1997. MIT Press.
Freddy Y. Y. Choi. Content-based Text Navigation. PhD thesis, Department of Computer Science,
University of Manchester, 2002.
Javier Couto, Olivier Ferret, Brigitte Grau, Nicolas Hernandez, Agata Jackiewicz, Jean-Luc Minel,
et Sylvie Porhiel. R&#201;gal, un syst&#232;me pour la visualisation s&#233;lective de documents. La pr&#233;sentation
d&#8217;information sur mesure, Num&#233;ro Sp&#233;cial de RIA, pages 481&#8211;514, 2004.
Marti A. Hearst. Texttiling: Segmenting text into multi-paragraph subtopic passages. Computational
Linguistics, 23(1):33&#8211;64, March 1997.
Nicolas Hernandez et Brigitte Grau. Automatic extraction of meta-descriptors for text description. In
RANLP, Borovets, Bulgaria, 10-12 September 2003.
Nicolas Hernandez. Un indice de structuration de texte combinant finesse et disponibilit&#233; au niveau
global et local. In ATALA, La Rochelle, France, 22 juin 2004.
Ivana Kruijff-Korbayov&#225; et Geert-Jan M. Kruijff. Identification of topic-focus chains. In S. Botley,
J. Glass, T. McEnery, et A. Wilson, editors, DAARC96, volume 8, pages 165&#8211;179. July 17-18 1996.
William C. Mann et Sandra A. Thompson. Rhetorical structure theory: A theory of text organisation.
Technical report isi/rs-87-190, Information Sciences Intitute, June 1987.
Daniel Marcu. A decision-based approach to rhetorical parsing. In The 37th Annual Meeting of the
Association for Computational Linguistics (ACL&#8217;99), pages 365&#8211;372, Maryland, June 1999.
M.-F. Moens et R. De Busser. Generic topic segmentation of document texts. In ACM SIGIR, pages
418&#8211;419, New York, 2001.
Livia Polanyi. A formal model of the structure of discourse. Journal of Pragmatics, 12:601&#8211;638, 1988.
J. Virbel. The contribution of linguistic knowledge to the interpretation of text structure. In J. Andr&#233;,
V. Quint, et R. Furuta, editors, Structured Documents, pages 161&#8211;181. Cambridge University, 1989.</p>

</div></div>
</body></html>