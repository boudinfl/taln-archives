TALN 2005, Dourdan, 6‚Äì10 juin 2005
D√©tection Automatique de Structures Fines de Texte
Nicolas Hernandez et Brigitte Grau
LIMSI/CNRS - LIR ‚Äì Universit√© de Paris-Sud
BP 133, F-91403 ORSAY CEDEX (France)
Hernandez|Grau@limsi.fr
Mots-clefs : Navigation intra-documentaire, analyse th√©matique, structures du discours,
relations discursives, subordination et coordination, parall√©lisme lexico-syntaxico-s√©mantique,
mod√®le d‚Äôapprentissage, analyses linguistiques
Keywords: Text browsing, topic analysis, text structures, discursive relations, subordi-
nation and coordination, lexical, syntactic and semantic parallelism, learning model, linguistic
analysis
R√©sum√© Dans ce papier, nous pr√©sentons un syst√®me de D√©tection de Structures fines de
Texte (appel√© DST). DST utilise un mod√®le pr√©dictif obtenu par un algorithme d‚Äôapprentissage
qui, pour une configuration d‚Äôindices discursifs donn√©s, pr√©dit le type de relation de d√©pendance
existant entre deux √©nonc√©s. Trois types d‚Äôindices discursifs ont √©t√© consid√©r√©s (des relations
lexicales, des connecteurs et un parall√©lisme syntaxico-s√©mantique) ; leur rep√©rage repose sur
des heuristiques. Nous montrons que notre syst√®me se classe parmi les plus performants.
Abstract In this paper, we present a system which aims at detecting fine-grained text
structures (we call it DST). Based on discursive clues, DST uses a learning model to predict
dependency relations between two given utterances. As discourse clues, we consider lexical
relations, connectors and key phrases, and parallelism. We show that our system implements an
improvement over current systems.
Nicolas Hernandez et Brigitte Grau
1 Introduction
Comme le souligne l‚Äôannonce du 14 d√©cembre 2004 de la soci√©t√© Google de num√©riser et de
rendre disponible en ligne 15 millions de livres appartenants √† 5 des plus c√©l√®bres biblioth√®ques
anglo-saxonnes du monde1, le besoin d‚Äôacc√©der facilement et rapidement au contenu d‚Äôun doc-
ument √©lectronique est plus que jamais un enjeu d‚Äôactualit√©.
Dans ce papier, nous nous int√©ressons √† la d√©tection de l‚Äôorganisation du contenu information-
nel d‚Äôun document textuel. De nombreux travaux (principalement au sein de la comunaut√© de
r√©sum√© automatique) ont montr√© l‚Äôint√©r√™t d‚Äôappr√©hender la structure d‚Äôun texte : afin de ma-
nipuler des unit√©s de texte de diff√©rentes granularit√©s (i.e. diff√©rents degr√©s informationnels), de
fournir un contexte √† une information cibl√©e, de permettre une navigation intra-documentaire,
etc. (Moens & Busser, 2001; Choi, 2002; Couto et al., 2004).
En particulier nous nous focalisons sur la micro-structure d‚Äôun texte (niveau phrastique voire
propositionnel). Nous affichons ainsi une compl√©mentarit√© aux approches globales tout en of-
frant la possibilit√© de raffiner leur mod√®le. En effet qu‚Äôelles supposent une organisation plate
et lin√©aire du flot d‚Äôinformations communiqu√© (Hearst, 1997; Choi, 2002), ou bien une organi-
sation plus riche en arbres (Moens & Busser, 2001; Couto et al., 2004), les approches globales
sont g√©n√©ralement fond√©es sur des mesures de coh√©sion lexicale (notamment √† travers le suivi
de cha√Ænes lexicales) qui souffrent d‚Äôun manque de pr√©cision quant √† la d√©limitation des unit√©s
de texte (appel√©es segment). De plus elles prennent rarement en compte dans leur analyse les
ph√©nom√®nes discursifs locaux (e.g. annonces th√©matiques ‚Äì e.g. ‚ÄúLes points que nous allons
traiter sont :‚Äù, structures √©num√©ratives, transitions, etc.).
Notre approche se situe parmi les travaux qui proposent de rechercher le point d‚Äôattache optimal
d‚Äôun √©nonc√© entrant dans la structure en cours de construction. Parmi les approches existantes,
Marcu (1999) propose un syst√®me pour la d√©tection automatique de la structure rh√©torique d‚Äôun
texte, Choi (2002) s‚Äôint√©resse √† une structuration th√©matique fine, Kruijff-Korbayov√° & Kruijff
(1996) analysent le discours en terme de progression th√©matique. Ces syst√®mes constituent de
s√©rieuses avanc√©es mais requi√®rent encore la prise en compte de plus d‚Äôindices discursifs et de
mod√®les plus souples pour appr√©hender les diff√©rents m√©canismes de structuration du discours.
Dans ce papier, nous pr√©sentons un syst√®me de D√©tection de Structures fines de Texte (ap-
pel√© DST). DST utilise un mod√®le pr√©dictif obtenu par un algorithme d‚Äôapprentissage qui, pour
une configuration d‚Äôindices discursifs donn√©s, pr√©dit le type de relation de d√©pendance exis-
tant entre deux √©nonc√©s. L‚Äôoriginalit√© principale de notre approche est de proposer un mod√®le
Th√©orique simplifi√© de la Structure du Discours. En effet, nous nous int√©ressons seulement
au rapport structurel √©l√©mentaire liant deux √©nonc√©s (relation de subordination, de coordina-
tion, et absence de relation) ind√©pendamment d‚Äôun √©ventuel √©tiquetage s√©mantico-rh√©torique de
la relation2. Le fait de dissocier le mod√®le de d√©pendance de la recherche du point d‚Äôattache
de l‚Äô√©nonc√© entrant nous permet d‚Äôenvisager diff√©rents algorithmes de structuration. Une de
nos particularit√©s techniques est de proposer une mesure pour appr√©hender le parall√©lisme
syntaxico-s√©mantique de deux √©nonc√©s, indice discursif peu consid√©r√© jusqu‚Äô√† pr√©sent. Nous
avons travaill√© sur des articles scientifiques en anglais mais notre d√©marche est adaptable √†
d‚Äôautres langues comme le fran√ßais.
1New York Public Library, University of Michigan, Stanford, Harvard (USA), Oxford (GB).
2Cette t√¢che sera abord√©e ult√©rieurement.
D√©tection Automatique de Structures Fines de Texte
2 L‚Äôacc√®s au contenu
Le processus de compr√©hension requiert d‚Äôune part d‚Äôidentifier des unit√©s discursives (informa-
tionnelles, intentionnelles, ayant une mise en forme visuelle, ou autres) et d‚Äôautre part d‚Äô√©tablir
des relations entre ces unit√©s. Cette reconnaissance de la coh√©rence peut n√©cessiter des connais-
sances s√©mantiques et pragmatiques, non disponibles dans le texte. N√©anmoins nous partons du
postulat qu‚Äôil est possible de mettre en place des analyses automatiques √† partir des indices
du discours (cha√Ænes lexicales, connecteurs, introducteurs de cadres, etc.) pour permettre de
reconna√Ætre cette coh√©rence.
L‚Äôune des caract√©ristiques majeures transversale √† la plupart des th√©ories du discours est la con-
sid√©ration d‚Äôun mod√®le de d√©pendance qui d√©finit la nature de la relation structurelle existante
entre deux √©nonc√©s en terme de subordination ou de coordination (Mann & Thompson, 1987;
Polanyi, 1988; Virbel, 1989; Asher & Lascarides, 1994). Les diff√©rences entre th√©ories viennent
de la signification qu‚Äôelles donnent √† la nature de ces relations, mais aussi des contraintes struc-
turelles d‚Äôassemblage des unit√©s discursives. Au niveau de la micro-structure, les chercheurs
ont tendance √† consid√©rer que l‚Äôunit√© √©l√©mentaire de r√©f√©rence est proche de celle de la proposi-
tion (Mann & Thompson, 1987; Polanyi, 1988). Afin de faciliter le rep√©rage automatique, nous
consid√©rons comme Choi (2002) la phrase syntaxique comme unit√© √©l√©mentaire.
Suivant le genre de texte consid√©r√© (expositif, narratif, dialogue, etc.), les th√©ories du discours
mettent en √©vidence un ou plusieurs plans d‚Äôorganisation de l‚Äôinformation: rh√©torique, logico-
visuelle, informationnelle, etc. Les interactions entre ces diff√©rentes structures sont encore
tr√®s floues, c‚Äôest pourquoi nous avons d√©cid√© de nous concentrer sur le plan informationnel
que nous consid√©rons comme pertinent pour les textes scientifiques. Notre description du plan
informationnel repose sur la th√©orie de la RST3 (Mann & Thompson, 1987), le LDM (Polanyi,
1988), et aussi la progression th√©matique de la phrase au discours (Kruijff-Korbayov√° & Kruijff,
1996). Globalement cela signifie que la relation entre deux √©nonc√©s est d√©termin√©e en fonction
de leur contenu informationnel ind√©pendamment de l‚Äôintention rh√©torique de l‚Äôauteur.
Dans notre mod√®le, un √©nonc√© entrant se rattache au discours selon une relation de subordination
ou de coordination (ou bien les deux). Un √©nonc√© est interpr√©t√© en fonction de son th√®me (ce
dont il parle), de son propos (ce qui est dit au sujet du th√®me) et de sa fonction s√©mantico-
rh√©torique. Ces √©l√©ments sont identifi√©s √† partir d‚Äôindices pr√©sents dans l‚Äô√©nonc√© et dans son
contexte ce qui permet de d√©duire avec quelles parties du discours il est li√© et comment.
Nous illustrons ces relations √† l‚Äôaide du texte de la figure 1 extrait d‚Äôun passage de notre corpus.
Les indices discursifs ais√©ment repr√©sentables visuellement sont soulign√©s dans le texte. Les
couples d‚Äô√©nonc√©s (1, 2) et (1, 6) d√©crivent des relations de subordination. 1 est un mod√®le
classique d‚Äôannonce th√©matique avec un quantifieur two, une phrase syntaxiquement incom-
pl√®te et un caract√®re de ponctuation annonce ‚Äú:‚Äù. Les √©nonc√©s 2 et 6, quant √† eux, contiennent
des marques qui caract√©risent des items d‚Äôune √©num√©ration (‚Äú1.‚Äù et ‚Äú2.‚Äù). Ces deux √©nonc√©s
pr√©sentent aussi une relation de coordination explicite l‚Äôun envers l‚Äôautre, soulign√©e notamment
par un parall√©lisme syntaxique :
NUM. NOM, whereby ADJ NOM be+conj=‚Äôpr√©sent‚Äô VERB+conj=‚Äôparticipe pass√©‚Äô PREP
Le couple d‚Äô√©nonc√©s (2, 3) constitue un exemple de subordination o√π le deuxi√®me √©nonc√© 3
correspond au d√©veloppement d‚Äôun des aspects du premier. Cette subordination est marqu√©e
par une progression th√©matique de rh√®me en th√®me (i.e. le terme importance qui est repris
dans 3). Le couple d‚Äô√©nonc√©s (4, 5) d√©crit, quant √† lui, un exemple de coordination implicite.
3Rhetorical Structure Theory (RST), Linguistic Discourse Model (LDM).
Nicolas Hernandez et Brigitte Grau
Two traditional approaches to automatic abstracting are: (1)
1. Extraction, whereby specific sentences are selected from the source text according to some
assessment of their importance. (2)
Importance indicators include the concentration of topic-relevant terms [. . . ]; the occurrence of
expressions, such as "important", "to sum up"; and the position of the sentence within the text. (3)
This approach is exemplified by Pollock and Zamora‚Äôs ADAM system [1]. (4)
The problems with this approach are that importance clues are often not reliable, and that the extracted
sentences do not always constitute a coherent text, since they often contain cross-references. (5)
2. Summarisation, whereby detailed semantic analysis is applied to the text, and a representation
such as a semantic net is produced, from which a summary is then generated. (6)
Figure 1: Exemples de relations de subordination, et de coordination explicite et implicite
En effet, tous les deux sont subordonn√©s √† une m√™me entit√©, l‚Äôapproche en terme d‚Äôextraction,
et chacun d‚Äôeux en traite de mani√®re ind√©pendante, le premier en pr√©sentant un exemple et le
second en d√©crivant les probl√®mes.
3 Algorithme de structuration ‚Äúshift and reduce‚Äù
La structure de texte en arbre unique est une simplification de la r√©alit√©, n√©anmoins nous adop-
tons une mod√©lisation hi√©rarchique parce qu‚Äôelle reste la plus commun√©ment rencontr√©e dans
les textes. Notre algorithme de structuration reprend le principe des algorithmes de Marcu
(1999) et Choi (2002). Nous l‚Äôavons adapt√© afin de tenir compte de la relation de coordination.
Cet algorithme construit une structure hi√©rarchique du discours dont les arcs sont orient√©es vers
les √©nonc√©s entrants toujours attach√©s sur la fronti√®re droite de l‚Äôarbre. Un √©nonc√© entrant coor-
donn√© √† un √©nonc√© de la structure est consid√©r√© comme √©tant subordonn√© au m√™me √©nonc√© que
l‚Äô√©nonc√© auquel il est coordonn√©. Un n√∏eud factice joue le r√¥le de p√®re de tous les n√∏euds.
L‚Äôalgorithme utilise deux structures de donn√©es : une pile qui stocke la branche ‚Äúfronti√®re
droite‚Äù de l‚Äôarbre en cours de construction (le dernier √©l√©ment empil√© est le point d‚Äôattache
le plus prioritaire), et une file qui contient la liste des √©nonc√©s tels qu‚Äôils sont ordonn√©s dans
le texte et analys√©s successivement. La pile joue un r√¥le de m√©moire dont chaque √©l√©ment
correspond √† une granularit√© inf√©rieure obtenue dans la structure du discours. L‚Äôobjectif est
d‚Äôidentifier les √©nonc√©s qui sont li√©s et les relations qu‚Äôils entretiennent.
Algorithme :
1. Si la pile est vide, on d√©file la file et empile la pile (√©tat initial).
2. Tant que la pile et la file ne sont pas vides, calcul de la relation entre l‚Äô√©l√©ment au sommet
de la pile et le premier √©l√©ment de la file.
  Si une relation de subordination est d√©tect√©e, alors l‚Äô√©l√©ment de la file est d√©fil√© et
empil√© (on descend dans la granularit√© du texte) ;
  Si une relation de coordination est d√©tect√©e, alors l‚Äô√©l√©ment au sommet de la pile est
d√©pil√© et remplac√© par l‚Äô√©l√©ment de la file ;
  Sinon (aucune relation) l‚Äô√©l√©ment au sommet de la pile est d√©pil√© et √©cart√© (l‚Äôid√©e
√©tant de remonter jusqu‚Äôau niveau de d√©pendance li√© √† l‚Äô√©l√©ment en t√™te de file).
D√©tection Automatique de Structures Fines de Texte
4 Indices discursifs
La reconnaissance des relations discursives entre deux √©nonc√©s est fond√©e sur la pr√©sence, ou
l‚Äôabsence, d‚Äôindices significatifs dans les textes scientifiques : relations lexicales, expressions
clefs et parall√©lisme de construction.
4.1 Relations lexicales
Les relations lexicales entre deux √©nonc√©s sont envisag√©es selon leur nature s√©mantique et selon
les parties des √©nonc√©s concern√©es (th√®me ou rh√®me). Nous utilisons un module de Construction
de Cha√Ænes Lexicales (CCL) pour les rep√©rer. Celui-ci est fond√© sur une variante de l‚Äôalgorithme
de (Barzilay & Elhadad, 1997). CCL recherche les relations entre les lemmes associ√©s aux
paires de mots √©tudi√©s en tenant compte de la distance s√©mantique entre ces mots ainsi que de
leur distance dans le texte. Le mot le plus fr√©quent au sein d‚Äôune cha√Æne constitue son √©l√©ment
repr√©sentatif. Nous consid√©rons :
  Les relations morphologiques : deux mots appartenant √† la m√™me famille morphologique4,
ind√©pendamment de leur cat√©gorie lexicale ;
  Les relations utilis√©es pour r√©f√©rer √† un m√™me objet du discours, telles que la synonymie,
l‚Äôhyperonymie et l‚Äôhyponymie, la m√©ronymie et l‚Äôholonymie, trouv√©es dans WordNet ;
  Les relations d‚Äôantonymie trouv√©es gr√¢ce √† WordNet ou √† la pr√©sence de pr√©fixes tels
que dis-, in-, un-, non-, under-, im-, a-, de-, ir-, anti- sur les m√™mes lemmes. Nous
construisons des cha√Ænes lexicales sp√©cifiques √† ce type de relation.
Etant donn√©es deux phrases constituant le contexte d‚Äô√©tudes, des cha√Ænes lexicales sont calcul√©es
entre les deux phrases, globalement, et entre les diff√©rentes combinaisons des parties constituant
le th√®me et le rh√®me des deux phrases. La distinction entre les parties th√©matique et rh√©matique
d‚Äôune phrase est r√©alis√©e selon une heuristique robuste de d√©coupage de la phrase en deux par
rapport au verbe le plus proche de son milieu.
La pr√©sence d‚Äôun lien lexical entre les rh√®mes de deux √©nonc√©s traduit g√©n√©ralement une sub-
ordination du deuxi√®me √©nonc√© vis-√†-vis de l‚Äô√©nonc√© pr√©c√©dent (e.g. une √©laboration ou une
reformulation). Une progression lin√©aire, de rh√®me en th√®me, correspond aussi au m√™me type
de subordination (e.g. une annonce th√©matique). Une relation contrastive peut d√©noter une co-
ordination. Dans tous les autres cas, la pr√©sence ou l‚Äôabsence d‚Äôune relation lexicale constitue
un indice suppl√©mentaire qui pourra se combiner avec les suivants.
4.2 Expressions clefs (essentiellement des connecteurs)
Notre liste d‚Äôexpressions clefs est issue en partie de la liste de m√©ta-descripteurs acquise au-
tomatiquement par Hernandez & Grau (2003), et de l‚Äôanalyse de notre corpus. Nous l‚Äôavons
aussi compl√©t√©e √† partir des mots clefs fournis par Choi (2002) pour lesquels nous r√©assignons
la relation (subordination ou coordination) en fonction de nos observations personnelles.
En raison du nombre d‚Äôexemples que compte notre corpus (1190 couples de phrases) par rap-
port au nombre de marques que nous avons retenu (178), nous n‚Äôavons pas choisi de consid√©rer
chacune des marques comme une caract√©ristique distincte, au contraire de Choi dont le mod√®le
4Nous utilisons la base CELEX (www.ldc.upenn.edu/readme_files/celex.readme.html).
Nicolas Hernandez et Brigitte Grau
compte 19 marques. Nous avons opt√© pour une pr√©-classification de celles-ci en 5 classes en
fonction de leur comportement pour structurer le discours et r√©duit ainsi la complexit√© du nom-
bre d‚Äôindices. Les classes rassemblent des marques ayant un m√™me ‚Äúcomportement‚Äù structurel
vis-√†-vis de la subordination et de la coordination entre deux √©nonc√©s. Les classes que nous
avons d√©finies sont les suivantes :
  Initie : marque le premier item d‚Äôune liste d‚Äôitems (suppose une coordination) : ‚Äúformer,
first, on the one hand, ‚Äô1.‚Äô, ‚Äôa)‚Äô, begin, start‚Äù ;
  Continue : Coordonne mais n‚Äôinitie pas une liste et n‚Äôen termine pas forc√©ment une :
‚Äúsecond, another, other, also, and, or, however, but, then, in addition, although, etc.‚Äù ;
  Termine : Marque le dernier item d‚Äôune liste (suppose une coordination) : ‚Äúon the other
hand, last, finally, to conclude, to sum up, end, finish, latter, in conclusion, result‚Äù ;
  Subordonn√© : Appara√Æt en d√©but d‚Äôun √©nonc√© subordonn√© : ‚Äúso, the, this, these, it, he, by
this, consequently, for example, for instance, therefore, thus, note that, such, etc. ‚Äù ;
  Subordonnant : Appara√Æt en fin d‚Äôun √©nonc√© subordonnant (i.e. en g√©n√©ral une annonce) :
‚Äúsuch as, follow, as follow, see below, below, and, :, ?, etc. ‚Äù.
Une marque est dans une seule classe sauf si cette derni√®re permet de la distinguer dans sa d√©f-
inition (e.g. la position dans la phrase pour diff√©rencier les marques subordonn√©es des marques
subordonnantes). Les notions de d√©but et de fin sont relatives √† chaque √©nonc√© et correspondent
√† une distance en nombre de mots exprim√©e en pourcentage (respectivement fix√©e √† 40% du
d√©but ou de la fin). La taille maximale est fix√©e √† 10 tokens.
En plus de ces classes de marques discursives, nous rajoutons une classe de marques d√©sig-
nant la n√©gation (e.g. aren‚Äôt, can‚Äôt, nothing, nobody, rarely, etc.) et afin de prendre en compte
les formes passives, et l‚Äôinversion des parties th√©matiques et rh√©matiques qui en d√©coule, nous
consid√©rons la pr√©sence du verbe ‚Äú√™tre‚Äù suivi directement d‚Äôun autre verbe comme une carac-
t√©ristique. Par la suite, nous appellerons ces derni√®res caract√©ristiques les indices syntaxiques.
Au final, la distribution de nos 178 marques se r√©partit ainsi : 7 marques pour la classe Initie,
38 pour la classe Continue, 9 pour la classe Termine, 62 pour la classe Subordonn√©e, 30 pour la
classe Subordonnant, 31 marques de n√©gation et 1 marque du verbe √™tre.
4.3 Parall√©lisme
Le parall√©lisme de construction entre deux √©nonc√©s rend compte d‚Äôune importance √©gale (lien
de coordination) (Hernandez, 2004). Il se traduit par a) des similarit√©s des constituants √† dif-
f√©rents niveaux paradigmatiques (lemme, trait s√©mantique, cat√©gorie grammaticale, fonction
syntaxique) ; b) une similarit√© syntagmatique qui s‚Äôexprime √† la fois par une similarit√© dans
l‚Äôordre des constituants parall√®les et par une similarit√© dans les √©carts de distance entre ces
m√™mes constituants.
Afin de calculer le degr√© de parall√©lisme entre deux √©nonc√©s, nous r√©duisons la complexit√©
du probl√®me d‚Äôabord en homog√©n√©isant les entit√©s du discours (chaque mot est remplac√© par
l‚Äô√©l√©ment repr√©sentatif de la cha√Æne lexicale √† laquelle il appartient). Ensuite, chaque structure
syntaxique hi√©rarchique est remplac√©e par une liste plate, qui correspond √† une notation pr√©fix√©e
de l‚Äôarbre (les n≈ìuds internes, qui sont des √©tiquettes, sont plac√©s avant les feuilles, qui sont les
lemmes). Cette liste est obtenue √† partir du r√©sultat d‚Äôanalyse fourni par l‚Äôanalyseur statistique
de Charniak (1997)5, en supprimant les niveaux de parenth√®ses.
5Nous utilisons la version 2001, d√©velopp√©e pour l‚Äôanglais √† l‚Äôuniversit√© de Brown.
D√©tection Automatique de Structures Fines de Texte
Pour tout couple de phrases donn√©, le syst√®me calcule un degr√© de parall√©lisme entre toutes les
s√©quences extraites de chacune des phrases, comportant le m√™me nombre d‚Äôitems similaires,
au minimum deux, diff√©rents ou non, plac√©s dans leur ordre d‚Äôapparition dans les phrases. Par
exemple, les phrases cabcad et acba partagent 4 constituants : c, a deux fois et b. Une fois
supprim√©s les constituants non similaires (i.e. d), on extrait de la premi√®re phrase caba et
abca, et de la deuxi√®me acba. On ne tient pas compte des √©l√©ments diff√©rents, qui peuvent √™tre
ins√©r√©s n‚Äôimporte o√π dans les phrases. Le parall√©lisme est fond√© sur des constructions similaires
d‚Äô√©l√©ments similaires. La mesure que nous avons d√©finie s‚Äôinspire des mesures de distances
d‚Äô√©dition entre des s√©quences de caract√®res. Chaque constituant est identifi√© de mani√®re unique
par sa position. Plus un constituant est distant de son sym√©trique dans l‚Äôautre s√©quence, plus les
s√©quences compar√©es diff√®rent. Elle est d√©finie par la formule suivante :
/0ffi213 45 ffi
)
(
 	
fffiflffi! #"%$%&' . ffi.ffi
/67ffi
)+*-,

5 68 8  967ffi /67ffi
avec ) , le constituant de la s√©quence , , la longueur des s√©quences compar√©es, ,

la distance maximale possible entre un constituant d‚Äôune s√©quence et son constituant parall√®le
/0ffi: ;967ffi<1>=   0
i.e. , et , la distance effective d‚Äôun constituant courant de la s√©quence et
son constituant parall√®le. Le degr√© de parall√©lisme d‚Äôun couple d‚Äô√©nonc√©s correspond au degr√©
maximal obtenu pour les s√©quences extraites de ces √©nonc√©s.
5 Apprentissage des relations discursives
Afin de reconna√Ætre les relations discursives, nous avons d√©cid√© d‚Äôopter, de m√™me que Marcu
(1999), pour un apprentissage par arbre de d√©cision qui poss√®de l‚Äôavantage d‚Äô√™tre compr√©hen-
sible par tout utilisateur (si la taille de l‚Äôarbre produit est raisonnable) et d‚Äôavoir une traduction
imm√©diate en terme de r√®gles de d√©cision. Nous avons utilis√© le classifieur C4.5 fourni dans le
logiciel WEKA6. Les caract√©ristiques que nous venons de d√©crire sont au nombre de 22 et sont
rep√©r√©es automatiquement dans le corpus.
5.1 Donn√©es
Afin de constituer un ensemble de couples de phrases et de relations correspondantes, nous
avons manuellement annot√© un corpus de 5 documents anglais appartenant au domaine de la
linguistique informatique. Ils font tous entre 8 et 10 pages et sont au format pdf. L‚Äôun d‚Äôeux est
en simple colonne. De fait ils couvrent la p√©riode 1998 et 1999 et aucun d‚Äôeux ne partage de
r√©f√©rences communes. Ces articles sont Mitkov (COLING-ACL‚Äô98), Kan et al. (WVLC‚Äô98),
Green (ACL‚Äô98), Sanderson et al. (SIGIR‚Äô99) et Oakes et al. (IRSG‚Äô99).
L‚Äôannotation a consist√© √† indiquer pour chaque phrase du texte les relations de subordination
et de coordination explicite existant avec une phrase se trouvant en amont dans le texte ; ces
deux types de relations pouvant exister pour une m√™me phrase. Le principe de d√©pendance que
nous avons suivi consiste √† toujours resituer un √©nonc√© vis-√†-vis de la th√©matique globale puis
d‚Äôanalyser si localement il n‚Äôy a pas des d√©pendances plus fortes. Chaque couple d‚Äô√©nonc√©s que

nous avons li√©s est d√©crit par une d√©cision, , concernant le type de relation qui les unit. Ces
6Cette bo√Æte √† outils est disponible √† l‚ÄôURL suivante www.cs.waikato.ac.nz/ml/weka.
Nicolas Hernandez et Brigitte Grau
couples sont ensuite repr√©sent√©s par l‚Äôensemble des caract√©ristiques discursives,   , que nous
   ffi
avons pr√©c√©demment d√©finies. Sur un total de 1038 phrases7, 1190 couples exemples,   ,
ont √©t√© constitu√©s. Ils se r√©partissent en 632 couples li√©es par une relation de subordination,
285 instances ‚Äúcoordination‚Äù et 273 instances d√©crivant une absence de relation. Les instances
d√©crivant une absence de relation ont √©t√© engendr√©es automatiquement en consid√©rant les cou-
ples d‚Äô√©nonc√©s contigus ne poss√©dant pas de relation entre eux. En comparaison Choi utilise un
corpus d‚Äôapprentissage de 754 exemples.
5.2 R√©sultats
De part la quantit√© de nos donn√©es d‚Äôapprentissage (relative au co√ªt en temps d‚Äôannotation
de corpus), nous adoptons une technique d‚Äô√©valuation par validation crois√©e sur 10 partitions.
Son principe consiste √† partitionner le corpus d‚Äôapprentissage en un certain nombre de parts
√©gales et d‚Äôutiliser tour √† tour une partie comme ensemble d‚Äôexemples de test et les autres
comme ensemble d‚Äôexemples d‚Äôentra√Ænement. La moyenne des taux d‚Äôerreur correspond au
taux d‚Äôerreur global.
Exp√©riences
coordination_et_subordination Progression Expressions Progression th√©matique
approche de base de 53,10% th√©matique clefs Et Expressions clefs
Ensemble de base 52,68% 57,31% 56,13%
coh√©sion lexicale 52,68% 57,31% 57,05%
antonymie 52,43% 56,89% 55,79%
Caract√©ristique indices syntaxiques 54,70% 58,57% 55,71%
ajout√©e # de mots communs 52,43% 57,31% 56,47%
degr√© de parall√©lisme 52,43% 56,97% 55,12%
Toutes les caract√©ristiques 54,62% 57,05% 55,21%
seulement_la_subordination Progression Expressions Progression th√©matique
approche de base de 69,83% th√©matique clefs Et Expressions clefs
Ensemble de base 69,83% 73,14% 73,59%
coh√©sion lexicale 69,83% 72,26% 73,70%
antonymie 69,83% 73,14% 73,70%
Caract√©ristique indices syntaxiques 72,48% 76,35% 75,02%
ajout√©e # de mots communs 69,83% 72,81% 74,03%
degr√© de parall√©lisme 69,83% 73,59% 74,25%
Toutes les caract√©ristiques 70,16% 75,13% 75,02%
Table 1: Pr√©cisions de DST dans la pr√©diction de relation
Nous avons r√©alis√© deux jeux d‚Äôexp√©rience : le premier en consid√©rant toutes les relations de
notre mod√®le (subordination, coordination et absence de relation), le deuxi√®me en ne consid-
√©rant plus que la relation de subordination et l‚Äôabsence de relation. Ce dernier jeu d‚Äôexp√©riences
nous permet de comparer nos r√©sultats avec ceux de Choi (2002). Pour simplifier la pr√©senta-
tion de ces jeux d‚Äôexp√©riences par la suite, nous omettrons la relation ‚Äúabsence de relation‚Äù
dans leur d√©signation. Pour chacun des jeux nous proposons de comparer les r√©sultats sur deux
ensembles d‚Äôindices de base distincts auxquels on ajoute tour √† tour telle ou telle caract√©ristique
pour observer les am√©liorations √©ventuelles. Les performances des deux ensembles combin√©s
sont aussi consid√©r√©es. Ces deux ensembles de base sont : 1) les caract√©ristiques d√©crivant la
7Les phrases ont √©t√© d√©tect√©es √† l‚Äôaide des caract√®res de ponctuation puis corrig√©es manuellement.
D√©tection Automatique de Structures Fines de Texte
progression th√©matique (de th√®me en th√®me, de th√®me en rh√®me, de rh√®me en th√®me et de rh√®me
en rh√®me) ; 2) les caract√©ristiques fond√©es sur les expressions clefs : les classes Initie, Termine,
Continue, Subordonn√© et Subordonnant. Les caract√©ristiques individuelles que nous ajoutons
sont : les liens lexicaux autre que d‚Äôantonymie (appel√©s par la suite ‚Äúcoh√©sion lexicale‚Äù), les
liens lexicaux d‚Äôantonymie, les indices syntaxiques (be et n√©gation), le degr√© de parall√©lisme et
le nombre de mots communs (approche simplifi√©e de notre mesure du parall√©lisme).
Afin de positionner l‚Äôapport des diff√©rents apprentissages, nous comparons leur performance
vis-√†-vis d‚Äôune approche de base qui correspond √† la pr√©diction de la classe majoritaire dans le
corpus d‚Äôapprentissage (c‚Äôest-√†-dire qu‚Äôelle correspond au taux d‚Äôerreur si l‚Äôon assigne tous les
exemples √† cette classe).
La table 1 d√©crit les r√©sultats que nous obtenons respectivement lorsque l‚Äôon consid√®re les
relations de coordination et de subordination, puis lorsque l‚Äôon ne consid√®re plus que la relation
de subordination. Les valeurs en gras correspondent √† des pr√©cisions maximales.
Le premier constat que nous faisons est que nous obtenons des r√©sultats compris entre 60% et
75% similaires √† ceux de Choi (2002) et de Marcu (1999). Plus particuli√®rement, nous obtenons
des r√©sultats sup√©rieurs √† ceux obtenus par Choi dans des configurations exp√©rimentales simi-
laires : 76,35% contre 73,61% pour nos meilleures performances de pr√©cision respectives.
Par rapport aux approches de base, les meilleurs sous-ensembles de caract√©ristiques augmentent
la pr√©cision de plus de 5% pour chacun des jeux d‚Äôexp√©riences. Il existe n√©anmoins des sous-
ensembles qui d√©t√©riorent les performances et les r√©sultats sont en g√©n√©ral moins bon pour le
jeu coordination_et_subordination.
Les meilleurs r√©sultats que nous obtenons sont √† partir de l‚Äôensemble de base compos√© de
caract√©ristiques fond√©es sur les expressions clefs.
Les r√©sultats obtenus avec les caract√©ristiques fond√©es sur des liens lexicaux quels qu‚Äôils soient,
combin√©es ou non, sont bien en dessous de ceux que l‚Äôon pouvait esp√©rer. Pour le jeu coordi-
nation_et_subordination les exp√©riences men√©es √† partir de l‚Äôensemble de base ‚Äúprogression
th√©matique‚Äù d√©teriorent pour la plupart la pr√©cision de l‚Äôapproche de base. Pour le jeu seule-
ment_la_subordination, la pr√©cision des exp√©riences √† partir de l‚Äôensemble de base ‚Äúprogression
th√©matique‚Äù reste inchang√©e par rapport √† l‚Äôapproche de base. Le gain notable de l‚Äôensemble
‚Äúprogression th√©matique‚Äù vient lorsqu‚Äôil est combin√© √† l‚Äôensemble ‚Äúexpressions clefs‚Äù.
Un gain inattendu est celui apport√© par le couple de pr√©sence du verbe √™tre ou d‚Äôune n√©gation.
Ce r√©sultat requiert un retour au texte pour d√©terminer un ph√©nom√®ne discursif √©ventuel.
Enfin, lorsque l‚Äôon compare les caract√©ristiques ‚Äúnombre de mots pleins communs‚Äù et ‚Äúdegr√©
de parall√©lisme‚Äù les diff√©rences sont l√©g√®res mais mettent en avant le degr√© de parall√©lisme.
6 Conclusion
Notre approche du discours enrichit le mod√®le de Choi (2002) qui ne consid√®re que la relation
de subordination. Nous consid√©rons en plus la relation de coordination ce qui nous permet de
mod√©liser plus finement le discours.
Le syst√®me de Marcu (1999) se situe √† un degr√© sup√©rieur de complexit√© dans le sens o√π il
cherche √† reconna√Ætre l‚Äôop√©ration de structuration √† r√©aliser en fonction du contexte et de la
Nicolas Hernandez et Brigitte Grau
configuration structurelle en cours. Marcu fait des hypoth√®ses tr√®s fortes sur le type de structure
et d‚Äôattachements possibles. En comparaison, le fait de dissocier le mod√®le de d√©pendance de
la structuration nous permet de fixer ind√©pendamment les contraintes de structuration, et par l√†
d‚Äôappr√©hender plus largement les diff√©rents ph√©nom√®nes de structuration du discours (i.e. des
structures autres que hi√©rarchiques orient√©es vers la fronti√®re droite). Ce type de mod√©lisation
peut ainsi √™tre utilis√© pour analyser par exemple des dialogues.
En utilisant l‚Äôalgorithme ‚Äúshift and reduce‚Äù, nous obtenons une structure hi√©rarchique proche de
celle d‚Äôune structure d√©crite par une analyse RST (correspondance entre les plans information-
nelles et intentionnelles). La diff√©rence majeure survient au niveau de la nucl√©arit√© des relations
unissant les √©nonc√©s.
Parmi nos perspectives nous envisageons d‚Äôenrichir notre mod√®le avec la relation de subordina-
tion dirig√©e vers l‚Äôaval du texte, ainsi que de nouveaux indices (comme ceux de mis en forme
visuelle) qu‚Äôils se trouvent dans les √©nonc√©s consid√©r√©s ou dans leur contexte.
R√©f√©rences
Nicholas Asher et Alex Lascarides. Intentions and information in discourse. 1994.
Regina Barzilay et Michael Elhadad. Using lexical chains for text summarization. In Proceedings of the
ACL‚Äô97/EACL‚Äô97 Workshop on Intelligent Scalable Text Summarization, Madrid, Spain, July 11 1997.
Eugene Charniak. Statistical parsing with a context-free grammar and word statistics. In Proceedings of
the Fourteenth National Conference on Artificial Intelligence AAAI, Menlo Park, 1997. MIT Press.
Freddy Y. Y. Choi. Content-based Text Navigation. PhD thesis, Department of Computer Science,
University of Manchester, 2002.
Javier Couto, Olivier Ferret, Brigitte Grau, Nicolas Hernandez, Agata Jackiewicz, Jean-Luc Minel,
et Sylvie Porhiel. R√âgal, un syst√®me pour la visualisation s√©lective de documents. La pr√©sentation
d‚Äôinformation sur mesure, Num√©ro Sp√©cial de RIA, pages 481‚Äì514, 2004.
Marti A. Hearst. Texttiling: Segmenting text into multi-paragraph subtopic passages. Computational
Linguistics, 23(1):33‚Äì64, March 1997.
Nicolas Hernandez et Brigitte Grau. Automatic extraction of meta-descriptors for text description. In
RANLP, Borovets, Bulgaria, 10-12 September 2003.
Nicolas Hernandez. Un indice de structuration de texte combinant finesse et disponibilit√© au niveau
global et local. In ATALA, La Rochelle, France, 22 juin 2004.
Ivana Kruijff-Korbayov√° et Geert-Jan M. Kruijff. Identification of topic-focus chains. In S. Botley,
J. Glass, T. McEnery, et A. Wilson, editors, DAARC96, volume 8, pages 165‚Äì179. July 17-18 1996.
William C. Mann et Sandra A. Thompson. Rhetorical structure theory: A theory of text organisation.
Technical report isi/rs-87-190, Information Sciences Intitute, June 1987.
Daniel Marcu. A decision-based approach to rhetorical parsing. In The 37th Annual Meeting of the
Association for Computational Linguistics (ACL‚Äô99), pages 365‚Äì372, Maryland, June 1999.
M.-F. Moens et R. De Busser. Generic topic segmentation of document texts. In ACM SIGIR, pages
418‚Äì419, New York, 2001.
Livia Polanyi. A formal model of the structure of discourse. Journal of Pragmatics, 12:601‚Äì638, 1988.
J. Virbel. The contribution of linguistic knowledge to the interpretation of text structure. In J. Andr√©,
V. Quint, et R. Furuta, editors, Structured Documents, pages 161‚Äì181. Cambridge University, 1989.
