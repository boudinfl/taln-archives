<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
"http://www.w3.org/TR/html4/loose.dtd">
<html><head><title>Evaluation des Mod&#232;les de Langage n-gram et n/m-multigram</title>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
</head>
<body>
<div style="page-break-before:always; page-break-after:always"><div><p>TALN 2005, Dourdan, 6&#8211;10 juin 2005
</p>
<p>&#201;valuation des Mod&#232;les de Langage n-gramme et n/m-multigramme
</p>
<p>P. Alain, O. Bo&#235;ffard
IRISA &#8211; Universit&#233; de Rennes 1 / ENSSAT
</p>
<p>6, rue de Kerampont, 22305 Lannion
{pierre.alain,olivier.boeffard}@irisa.fr
</p>
<p>Mots-clefs : Mod&#232;les de Langage statistiques, n-gramme, multigramme, &#233;valuation
</p>
<p>Keywords: Statistical Language Models, n-grams, phrase multigrams
</p>
<p>R&#233;sum&#233; Cet article pr&#233;sente une &#233;valuation de mod&#232;les statistiques du langage men&#233;e sur la langue
Fran&#231;aise. Nous avons cherch&#233; &#224; comparer la performance de mod&#232;les de langage exotiques par rapport
aux mod&#232;les plus classiques de n-gramme &#224; horizon fixe. Les exp&#233;riences r&#233;alis&#233;es montrent que des
mod&#232;les de n-gramme &#224; horizon variable peuvent faire baisser de plus de 10% en moyenne la perplexit&#233;
d&#8217;un mod&#232;le de n-gramme &#224; horizon fixe. Les mod&#232;les de n/m-multigramme demandent une adaptation
pour pouvoir &#234;tre concurrentiels.
</p>
<p>Abstract This paper presents an evaluation of statistical language models carried out on the French
language. We compared the performance of some exotic models to the one of the more traditional n-
gram model. The experiments show that the variable n-gram models can drop more than 10% of the
average perplexity for a fixed n-gram model. n/m-multigram models require an adaptation to be able to
compete.
</p>
<p>1 Introduction
</p>
<p>La mod&#233;lisation du langage est un probl&#232;me crucial et tr&#232;s largement abord&#233; en traitement automatique
de la langue &#233;crite ou parl&#233;e1. &#192; partir de l&#8217;observation de s&#233;quences de mots, il s&#8217;agit de construire
un mod&#232;le dont l&#8217;objectif est de pr&#233;dire avec succ&#232;s de nouvelles s&#233;quences. On peut distinguer d&#233;j&#224;
deux probl&#232;mes, d&#8217;une part celui du choix du mod&#232;le et de sa m&#233;thodologie de construction et d&#8217;autre
part celui de la m&#233;thodologie d&#8217;&#233;valuation d&#8217;un mod&#232;le de langage. Concernant le premier point, on
peut distinguer des approches d&#233;terministes qui tiennent compte de l&#8217;organisation profonde des mots
li&#233;es notamment &#224; la syntaxe, des approches probabilistes qui s&#8217;int&#233;ressent essentiellement &#224; la forme de
surface (Rosenfeld, 2000).
L&#8217;&#233;valuation est un point relativement d&#233;licat dans la mesure o&#249; elle peut &#234;tre d&#233;pendante du mod&#232;le
choisi. La mesure la plus commun&#233;ment adopt&#233;e consiste &#224; calculer l&#8217;entropie crois&#233;e entre un mod&#232;le
de langage et la distribution r&#233;elle des donn&#233;es observ&#233;es, mais inconnue. En supposant que les donn&#233;es
suivent une distribution stationnaire et ergodique, le calcul de l&#8217;entropie-crois&#233;e peut &#234;tre estim&#233; &#224; partir
d&#8217;un corpus suffisamment grand2. La perplexit&#233; d&#8217;un mod&#232;le de langage n&#8217;est qu&#8217;une autre mani&#232;re de
repr&#233;senter le degr&#233; d&#8217;incertitude d&#8217;un mod&#232;le et se calcule directement &#224; partir de l&#8217;entropie-crois&#233;e du
mod&#232;le sur un jeu de phrases de test. Pour un mot &#224; pr&#233;dire, la valeur de la perplexit&#233; repr&#233;sente le
</p>
<p>1On peut citer le domaine de la reconnaissance automatique de la parole mais aussi celui de la reconnaissance de texte
manuscrit ou encore celui de la traduction automatique.
</p>
<p>2Th&#233;or&#232;me de Shannon-MacMillan-Brieman, (Shields, 1998). En respectant ces hypoth&#232;ses de stationnarit&#233; et d&#8217;ergodicit&#233;,
un corpus de parole de longueur finie peut refl&#233;ter la distribution r&#233;elle des donn&#233;es.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>P. Alain, O. Bo&#235;ffard
</p>
<p>nombre d&#8217;hypoth&#232;ses moyennes de branchement3. Plus la perplexit&#233; est faible, plus le facteur moyen de
branchements d&#8217;un mot vers un autre est bas et plus le mod&#232;le de langage est efficace. Pour les mod&#232;les
n-gramme, un mot est pr&#233;dit en tenant compte d&#8217;un historique relativement limit&#233; des mots qui le pr&#233;c&#232;-
dent. Ces mod&#232;les connaissent finalement tr&#232;s peu des raisons profondes de l&#8217;organisation des mots
dans une phrase. En revanche, l&#8217;utilisation de probabilit&#233;s conditionnelles et un apprentissage r&#233;alis&#233;
&#224; partir de quelques millions de phrases permettent d&#8217;obtenir de bonnes performances. Leur principal
d&#233;faut r&#233;side dans la complexit&#233; spatiale sous-jacente. Th&#233;oriquement, plus la s&#233;quence de l&#8217;historique
du mod&#232;le s&#8217;allonge (n augmente), plus le mod&#232;le r&#233;partit efficacement la masse de probabilit&#233;s sur des
mots qui reviennent souvent apr&#232;s une valeur particuli&#232;re de l&#8217;historique. Cependant, plus n augmente,
plus les observations se rar&#233;fient compte-tenu de la nature hyperbolique de la distribution de ces &#233;v&#233;ne-
ments4. Pour des situations exp&#233;rimentales r&#233;elles, les valeurs courantes de n d&#233;passent rarement 4 (Siu
&amp; Ostendorf, 2000). De nombreuses solutions ont &#233;t&#233; apport&#233;es au probl&#232;me de l&#8217;explosion combina-
toire et &#224; celui de la rar&#233;faction des &#233;v&#233;nements (Rosenfeld, 2000). Des techniques de lissage permettent
notamment de r&#233;pondre &#224; la difficult&#233; de l&#8217;estimation d&#8217;une distribution de probabilit&#233; lorsque les &#233;v&#233;ne-
ments sont rares. On peut citer le principe du lissage qui n&#8217;effectue l&#8217;estimation des points de la densit&#233;
au sens du maximum de vraisemblance que pour des &#233;v&#233;nements dont l&#8217;occurrence est sup&#233;rieure &#224; un
seuil de cut-off. Une partie de la masse de probabilit&#233; est r&#233;partie sur des &#233;v&#233;nements dont l&#8217;occurrence
est inf&#233;rieure au seuil, (Katz, 1987). (Chen &amp; Goodman, 1999) propose une &#233;valuation des principales
techniques de lissage les plus utilis&#233;es.
</p>
<p>Les mod&#232;les de n-gramme pour lesquels la longueur de l&#8217;historique est variable sont une alternative
aux n-gramme classiques pour lesquels la longueur de l&#8217;historique reste fixe. Le principe consiste &#224; ne
pas retenir un historique de longueur n si la contribution du n-gramme correspondant n&#8217;am&#233;liore pas la
performance du mod&#232;le. Toute la difficult&#233; r&#233;side dans la d&#233;cision d&#8217;abandon d&#8217;un n-gramme pour un
(n&#8722; k)-gramme avec 1 &#8804; k &lt; n, (Niesler &amp; Woodland, 1994)(Siu &amp; Ostendorf, 2000).
Les mod&#232;les multigramme sont des mod&#232;les de type n-gramme o&#249; la t&#234;te peut avoir une longueur
sup&#233;rieure &#224; 1.(Bimbot et al. , 1995)(Deligne &amp; Bimbot, 1995) pr&#233;sentent un cadre th&#233;orique pour
des multigramme form&#233;s sur des mod&#232;les d&#8217;uni-gramme (longueur d&#8217;historique nulle). Les exp&#233;ri-
ences rapport&#233;es concernent une application avec un vocabulaire limit&#233; de 900 mots pour un corpus
d&#8217;apprentissage de 100 000 phrases et un corpus de test de 1 000 phrases (dont 52 occurrences de mots
hors-vocabulaire). Les mod&#232;les de type multigramme obtiennent une perplexit&#233; meilleure que les n-
gramme classiques lorsque n &gt; 3. Compte-tenu de la taille relativement limit&#233;e des corpus, les conclu-
sions sont difficilement transposables directement sur des corpus plus importants. (Deligne &amp; Sagisaka,
2000) se place dans un contexte de multigramme de classes de mots sur des mod&#232;les de bi-gramme.
Les exp&#233;riences sont men&#233;es avec un vocabulaire d&#8217;environ 3 000 mots, 100 000 phrases pour le corpus
d&#8217;apprentissage et environ 700 phrases pour le test. Deux types de mesure sont rapport&#233;s : d&#8217;une part
la perplexit&#233; pour les mod&#232;les de type multigramme et d&#8217;autre part le taux d&#8217;erreur d&#8217;un syst&#232;me de re-
connaissance de la parole. Les r&#233;sultats entre multigramme et n-gramme classiques (bi- et tri-gramme)
sont difficilement comparables. En effet, pour ces derniers, les valeurs de perplexit&#233; sont absentes et les
mod&#232;les de n-gramme semblent avoir &#233;t&#233; non r&#233;duits5. (Zitouni, 2002) propose des mod&#232;les de multi-
gramme o&#249; les probabilit&#233;s de co-occurrence de mots sont conditionn&#233;es par rapport &#224; des classes. Les
exp&#233;riences concernent deux ann&#233;es du journal &quot;Le Monde&quot; (ann&#233;es 1987 et 1988) pour un vocabulaire
de 20 000 mots. L&#8217;utilisation de ces multigramme permet de r&#233;duire de 7% la perplexit&#233; des tri-gramme
classiques. Encore une fois, il est difficile de retrouver sur cette exp&#233;rience une comparaison entre mod-
&#232;les &#224; nombre de param&#232;tres constant.
</p>
<p>Cet article propose une &#233;tude exp&#233;rimentale sur les performances relatives des mod&#232;les de language
</p>
<p>3Il s&#8217;agit d&#8217;une moyenne g&#233;om&#233;trique.
4Il s&#8217;agit de distributions &#224; queue lourde o&#249; beaucoup d&#8217;&#233;v&#233;nements sont extr&#234;mement rares et peu sont tr&#232;s fr&#233;quents. La
</p>
<p>loi de Zipf est un cas particulier de lois puissances caract&#233;ristiques de ce ph&#233;nom&#232;ne.
5Le comportement d&#8217;un n-gramme est non-lin&#233;aire, il est possible de r&#233;duire de fa&#231;on importante le nombre de param&#232;tres
</p>
<p>sans trop d&#233;grader ses performances.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>&#201;valuation des Mod&#232;les de Langage n-gramme et n/m-multigramme
</p>
<p>de type n-gramme &#224; horizon fixe, &#224; horizon variable et multigramme. La section 2 pr&#233;sente un cadre
th&#233;orique pour ces trois types de mod&#232;les de langage statistiques. La section 3 expose la probl&#233;matique
d&#8217;une telle exp&#233;rimentation ainsi que nos hypoth&#232;ses de travail. Une &#233;valuation a &#233;t&#233; men&#233;e sur environ
un million de phrases en fran&#231;ais. La section 4 d&#233;crit la m&#233;thodologie suivie. La section 5 expose les ex-
p&#233;riences mises en &#339;uvre. Enfin, la section 6 pr&#233;sente les r&#233;sultats et une interpr&#233;tation du comportement
des mod&#232;les en fonction des donn&#233;es trait&#233;es.
</p>
<p>2 Cadre th&#233;orique
</p>
<p>Un mod&#232;le de langage statistique est un ensemble de distributions de probabilit&#233; sur des s&#233;quences
observ&#233;es de symboles. Comme, en pratique, il est impossible de caract&#233;riser de telles distributions,
les mod&#232;les de langage se distingueront entre eux par les hypoth&#232;ses choisies pour r&#233;duire la complexit&#233;
combinatoire et am&#233;liorer leur capacit&#233; de g&#233;n&#233;ralisation. Apr&#232;s une pr&#233;sentation des notations utilis&#233;es,
nous discutons du mod&#232;le de n-gramme &#224; horizon fixe, du mod&#232;le de n-gramme &#224; horizon variable et
enfin du mod&#232;le n/m-multigramme.
</p>
<p>Soit une s&#233;quence de motsW = (w1, w2, ..., wN ) avecwi une variable repr&#233;sentant le mot de rang i dans
la s&#233;quence W . Les valeurs possibles pour wi appartiennent &#224; un vocabulaire V. Il peut s&#8217;agit souvent
d&#8217;un vocabulaire ferm&#233; dans le cadre de syst&#232;mes de dialogue, nous consid&#233;rons ici l&#8217;&#233;tude de la langue
naturelle, nous choisissons un vocabulaire ouvert. Nous pouvons d&#233;crire cette s&#233;quence par une suite de
variables al&#233;atoires wi. La probabilit&#233; conjointe des variables de la s&#233;quence W peut se d&#233;velopper de la
mani&#232;re suivante en faisant appara&#238;tre des probabilit&#233;s conditionnelles :
</p>
<p>p(W ) = p(w1)&#215;
N&#8719;
i=2
</p>
<p>p(wi|w1, ...wi&#8722;1) (1)
</p>
<p>L&#8217;objectif d&#8217;un mod&#232;le de langage consiste &#224; calculer cette probabilit&#233; conjointe, c&#8217;est-&#224;-dire &#224; estimer
des valeurs pour chacune des probabilit&#233;s conditionnelles. L&#8217;estimation de ces probabilit&#233;s condition-
nelles est en pratique impossible car le nombre de param&#232;tres cro&#238;t de mani&#232;re exponentielle avec la
longueur de la suite de mots. Pour contrer cette difficult&#233;, un mod&#232;le de langage pose une probabilit&#233;
conditionnelle approch&#233;e p&#8727;(.) en simplifiant la loi conjointe, &#233;quation 1.
On note G l&#8217;ensemble des groupes de mots form&#233;s sur le vocabulaire V. On note S l&#8217;ensemble des
s&#233;quences form&#233;es sur les &#233;l&#233;ments de G. On note S&#8727; &#8834; S, l&#8217;ensemble des s&#233;quences de S qui corre-
spondent &#224; W . On note S une s&#233;quence particuli&#232;re de S&#8727;. Par exemple, pour W = (w1, w2, w3), on
a :
</p>
<p>S&#8727; =
</p>
<p>&#63729;&#63732;&#63732;&#63730;&#63732;&#63732;&#63731;
[w1][w2][w3]
[w1, w2][w3]
[w1, w2, w3]
[w1][w2, w3]
</p>
<p>On note |S| le nombre de groupes de mots dans la s&#233;quence S. Soit k le groupe de mots de rang k dans
la s&#233;quence S, on note i(S(k)) l&#8217;indice dans la s&#233;quence W du premier mot de S(k). On note l(S(k))
le nombre de mots de S(k).
</p>
<p>On note hi,j(W ) la cha&#238;ne des variables al&#233;atoires repr&#233;sentant l&#8217;apparition conjointe de tous les mots
wu de W pour u &#8712; [i, i+ (j &#8722; 1)].
</p>
<p>hi,j(W ) =
{
</p>
<p>wi, wi+1, ..., wi+(j&#8722;1) si i+ (j &#8722; 1) &#8804; N
wi, wi+1, ..., wN sinon
</p>
<p>On d&#233;finit &#233;galement l&#8217;op&#233;rateur ti,j(W ) qui repr&#233;sente les j mots pr&#233;c&#233;dents le mot wi. On a donc
ti,j(W ) = hi&#8722;j,j(W ). ti,j(W ) correspond &#224; un horizon ou historique (un groupe de mots qui pr&#233;c&#232;de</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>P. Alain, O. Bo&#235;ffard
</p>
<p>l&#8217;observation d&#8217;un mot). hi,j(W ) correspond &#224; la t&#234;te d&#8217;un param&#232;tre du mod&#232;le de langage (pour les
mod&#232;les n-gramme &#224; horizon fixe ou variable, la variable al&#233;atoire de t&#234;te est d&#233;g&#233;n&#233;r&#233;e, et ne contient
qu&#8217;un seul mot). Les mod&#232;les de langage cherchent d&#8217;une part &#224; r&#233;duire au maximum la longueur d&#8217;un
horizon (minimisation du nombre de param&#232;tres) et d&#8217;autre part, pour un horizon donn&#233;, &#224; estimer la
distribution de probabilit&#233; des historiques pour calculer la probabilit&#233; d&#8217;apparition du mot wi. Soit W
associ&#233;e &#224; une s&#233;quence de d&#233;coupage S, la loi conjointe estim&#233;e par le mod&#232;le de langage peut alors se
r&#233;&#233;crire sous la forme suivante avec n l&#8217;ordre du n-gramme :
</p>
<p>p&#8727;S(W ) = p(hi(S(1)),l(S(1))(W ))&#215;
|S|&#8719;
k=2
</p>
<p>p(hi(S(k)),l(S(k))(W )|ti(S(k)),n&#8722;1(W )) (2)
</p>
<p>2.1 Les mod&#232;les n-gramme &#224; horizon fixe
</p>
<p>Pour un n-gramme &#224; horizon fixe, on fait une hypoth&#232;se d&#8217;ind&#233;pendance conditionnelle du mot wi avec
les mots pr&#233;sents dans la s&#233;quence &#224; une distance de plus de n &#8722; 1 mots (pour n = 2, ce mod&#232;le est un
mod&#232;le de bi-gramme ; la probabilit&#233; P (W ) correspond &#224; celle d&#8217;une cha&#238;ne de Markov. Pour n = 3, on
parle de tri-gramme et pour n = 4 de quadri-gramme). Comme nous l&#8217;avons d&#233;j&#224; soulign&#233;, ce mod&#232;le est
tr&#232;s simple, mais le nombre de param&#232;tres cro&#238;t de mani&#232;re exponentielle avec n. Pour cette raison, les
mod&#232;les de n-gramme les plus utilis&#233;s le sont pour des valeurs de n de l&#8217;ordre de 3 ou 4. Pour corriger
le probl&#232;me des &#233;v&#233;nements rares, il existe des techniques de lissage des probabilit&#233;s conditionnelles,
coupl&#233;es &#224; des techniques de back-off permettant de corriger celles d&#8217;&#233;v&#233;nements manquants lors de
l&#8217;apprentissage, (Katz, 1987). Cette correction s&#8217;effectue en pond&#233;rant la probabilit&#233; du (n&#8722;1)-gramme
par un coefficient de back-off de telle mani&#232;re que la distribution de probabilit&#233; des n-gramme somme
toujours &#224; 1. Le terme produit de l&#8217;&#233;quation 2 se simplifie alors :
</p>
<p>p(hi(S(k)),l(S(k))(W )|ti(S(k)),n&#8722;1(W )) &#8710;= p(hi,1(W )|ti,n&#8722;1(W )) (3)
</p>
<p>Pour ce mod&#232;le, S =W , on obtient simplement :
</p>
<p>pML(W ) = p&#8727;S(W )
</p>
<p>2.2 Les n-gramme &#224; horizon variable
</p>
<p>Forcer l&#8217;estimation du terme produit de l&#8217;&#233;quation 2 &#224; un historique de longueur n introduit un double
biais. D&#8217;une part les occurrences sont plus faibles, on a donc tendance &#224; faire du lissage et &#224; &#234;tre moins
pr&#233;cis. D&#8217;autre part, on introduit des distributions conditionnelles sur wi qui ne servent pas &#224; grand
chose (augmentation injustifi&#233;e du nombre de param&#232;tres). Autoriser une variation de la longueur de
l&#8217;historique pour pr&#233;dire wi permet de r&#233;gler ce probl&#232;me de sur-apprentissage. Les n-gramme &#224; horizon
variable d&#233;finissent une probabilit&#233; en adaptant une longueur d&#8217;historique optimale en fonction de wi.
L&#8217;approche traditionnelle pour ce type de mod&#232;les consiste &#224; d&#233;terminer au moment de l&#8217;apprentissage
les longueurs optimales &#224; retenir, (Bonafonte &amp; Mari&#241;o, 1996)(Siu &amp; Ostendorf, 2000). Dans cette
situation un n-gramme est remplac&#233; par un (n&#8722; k)-gramme avec 1 &#8804; k &lt; n. Les n-gramme &#224; horizon
variable peuvent appara&#238;tre int&#233;ressants pour un double enjeu : d&#8217;une part &#224; nombre de param&#232;tres fix&#233;,
il peuvent r&#233;pondre &#224; une am&#233;lioration de la performance des n-gramme &#224; horizon fixe et d&#8217;autre part, &#224;
perplexit&#233; fix&#233;e, ils peuvent &#234;tre utiles &#224; la diminution du nombre de param&#232;tres d&#8217;un mod&#232;le de langage.
</p>
<p>Au moment du test, lors du calcul de la perplexit&#233; d&#8217;une phrase, pour ce mod&#232;le de n-gramme &#224; horizon
variable, le terme produit de l&#8217;&#233;quation 2 s&#8217;&#233;crit :
</p>
<p>p(hi(S(k)),l(S(k))(W )|ti(S(k)),n&#8722;1(W )) &#8710;= max
1&#8804;v&#8804;n&#8722;1
</p>
<p>{p(hi,1(W )|ti,v(W ))} (4)</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>&#201;valuation des Mod&#232;les de Langage n-gramme et n/m-multigramme
</p>
<p>Cette &#233;criture signifie que pour chaque mot wi &#224; pr&#233;dire, on cherche &#224; maximiser la probabilit&#233; en se
basant sur des mod&#232;les allant du bi-gramme au n-gramme (&#233;quation 3). Pour ce mod&#232;le, S = W , on
obtient simplement :
</p>
<p>pML(W ) = p&#8727;S(W )
</p>
<p>2.3 Les n/m-multigram
</p>
<p>Un n/m-multigramme correspond &#224; une probabilit&#233; conditionnelle o&#249; la t&#234;te du n-gramme peut &#234;tre plus
longue qu&#8217;un mot unique. m repr&#233;sente le nombre maximum de mots dans un groupe de mots en t&#234;te.
Lors du test du mod&#232;le de langage, pour une d&#233;coupe S donn&#233;e, nous cherchons la meilleure probabilit&#233;
suivant l&#8217;&#233;quation :
</p>
<p>p(hi(S(k)),l(S(k))(W )|ti(S(k)),n&#8722;1(W )) &#8710;= max
1&#8804;u&#8804;m,1&#8804;v&#8804;m&#215;(n&#8722;1)
</p>
<p>{p(hi,u(W )|ti,v(W ))} (5)
</p>
<p>Il suffit ensuite de prendre la meilleure solution sur toutes les s&#233;quences S &#8712; S&#8727; :
</p>
<p>pML(W ) = argmax
S&#8712;S&#8727;
</p>
<p>{p&#8727;S(W )}
</p>
<p>3 Probl&#233;matique et hypoth&#232;ses m&#233;thodologiques
</p>
<p>Notre objectif est de v&#233;rifier l&#8217;int&#233;r&#234;t des mod&#232;les n-gramme &#224; horizon variable par rapport &#224; des mod-
&#232;les &#224; horizon fixe et &#224; des mod&#232;les de type n/m-multigramme. La difficult&#233; de mise en &#339;uvre d&#8217;une
telle &#233;valuation r&#233;side dans le probl&#232;me du contr&#244;le explicite des param&#232;tres lors de la construction des
mod&#232;les. Diff&#233;rents facteurs sont responsables de la qualit&#233; d&#8217;un mod&#232;le de langage. Certains influent
directement le processus d&#8217;apprentissage alors que d&#8217;autres d&#233;terminent la mesure de performance d&#8217;un
mod&#232;le.
</p>
<p>Tout d&#8217;abord l&#8217;estimation des probabilit&#233;s conditionnelles provient directement de la d&#233;tection de n-
uplets. Avec peu de s&#233;quences, on d&#233;favorise notamment les mod&#232;les de n-gramme d&#8217;ordre sup&#233;rieur.
Le nombre de param&#232;tres d&#8217;un mod&#232;le de langage de type n-gramme est proportionnel &#224; |V|n. Le cut-
off est une technique simple et relativement efficace pour limiter le nombre de param&#232;tres (Chen &amp;
Goodman, 1999). Il s&#8217;agit de ne pas retenir les n-uplets qui apparaissent sous un seuil d&#8217;occurrence.
Ainsi, un cut-off &#224; 1 signifie qu&#8217;un mot doit appara&#238;tre au moins 2 fois pour &#234;tre int&#233;gr&#233; au mod&#232;le de
langage. Cependant, compte-tenu de la forme des distributions de probabilit&#233; (fonction puissance), la
r&#233;duction cons&#233;quente du nombre de param&#232;tres n&#8217;est pas lin&#233;aire en fonction de la valeur de cut-off.
En introduction, nous avons soulign&#233; le r&#244;le de la perplexit&#233; comme outil de mesure de la qualit&#233; d&#8217;un
mod&#232;les de langage.
</p>
<p>Un autre facteur cl&#233; que l&#8217;on doit maintenir entre les diff&#233;rents mod&#232;les de langage pour pouvoir com-
parer les valeurs de perplexit&#233; est le nombre de mots hors-vocabulaire. Plus la taille du vocabulaire est
faible (et donc plus le nombre de param&#232;tres est faible), plus le taux des mots hors-vocabulaire augmente
avec des valeurs de perplexit&#233; qui s&#8217;am&#233;liorent. Il s&#8217;agit d&#8217;un facteur calcul&#233; a posteriori, une fois le
mod&#232;le construit. Il est donc difficile d&#8217;intervenir explicitement sur cette valeur.
</p>
<p>Le calcul de la perplexit&#233; peut varier notamment par la prise en compte ou non des mots hors-vocabulaire
sur l&#8217;ensemble de test. On peut d&#233;cider de ne pas pr&#233;dire un mot hors vocabulaire ; dans ce cas
l&#8217;accumulation de la perplexit&#233; est plus faible mais le nombre de mot pr&#233;dit n&#8217;augmente pas. Le cal-
cul de la perplexit&#233; fait intervenir une hypoth&#232;se de stationnarit&#233; et d&#8217;ergodicit&#233; qu&#8217;il faudrait v&#233;ri-
fier en pratique. La performance d&#8217;un mod&#232;le de langage d&#233;pend donc &#233;troitement du couple ensem-
ble d&#8217;apprentissage/ensemble de test. Il faut que ces ensembles contiennent un nombre suffisant de</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>P. Alain, O. Bo&#235;ffard
</p>
<p>s&#233;quences pour pouvoir conclure &#224; des r&#233;sultats stables. Au cours de nos exp&#233;riences, nous avons essay&#233;
de minimiser l&#8217;influence de chacun de ces facteurs de mani&#232;re &#224; favoriser la comparaison entre structures
de mod&#232;les (n-gramme &#224; horizon fixe, n-gramme &#224; horizon variable et n/m-multigramme).
Nous avons consid&#233;r&#233; les hypoth&#232;ses m&#233;thodologiques suivantes. Une ann&#233;e du journal &quot;Le Monde&quot; a
&#233;t&#233; choisie comme univers linguistique (ann&#233;e 1997). Apr&#232;s extraction des phrases et tirage al&#233;atoire, ce
corpus est reparti en deux sous-corpus : 70% pour le corpus d&#8217;apprentissage et 30% pour le corpus de test.
Le choix d&#8217;un corpus fixe est suffisant pour valider une comparaison entre mod&#232;les, mais ne permettra
pas de conclure sur la robustesse des r&#233;sultats. Des analyses compl&#233;mentaires seront donc n&#233;cessaires.
Nous avons consid&#233;r&#233; trois ensembles de mots : un premier vocabulaire &#224; 3 000 mots, un deuxi&#232;me
&#224; 30 000 mots et un dernier &#224; 60 000 mots (il s&#8217;agit &#224; chaque fois des plus fr&#233;quents sur l&#8217;ensemble
d&#8217;apprentissage). Les valeurs de perplexit&#233; et les taux de mots hors vocabulaire d&#233;pendent directement
de ces trois ensembles. Nous avons cherch&#233; &#224; contr&#244;ler explicitement le nombre de param&#232;tres de nos
mod&#232;les. Deux approches compl&#233;mentaires ont &#233;t&#233; mises en &#339;uvre : d&#8217;une part par application de seuils
de coupure sur les diff&#233;rents types de n-gramme et d&#8217;autre part par la conservation des co-occurrences
de m-uplets de mots les plus fr&#233;quentes pour les n/m-multigramme. Dans le premier cas, nous balayons
un spectre de valeurs de cut-off et nous observons a posteriori le nombre de param&#232;tres. Ce nombre nous
sert ensuite &#224; ajuster le nombre de multigramme autoris&#233;s &#224; entrer dans le vocabulaire et se placer ainsi
&#224; nombre de param&#232;tres constant (avec une tol&#233;rance de 1%). La perplexit&#233; calcul&#233;e ne tient pas compte
des mots hors vocabulaire qu&#8217;ils soient pr&#233;sents dans la t&#234;te ou dans l&#8217;historique d&#8217;un n-gramme.
</p>
<p>Notre syst&#232;me de r&#233;f&#233;rence est celui des n-gramme classiques (que nous avons nomm&#233; n-gramme &#224;
horizon fixe). Nous avons choisi des valeurs commun&#233;ment admises pour n et introduit des mod&#232;les de
bi-, tri- et quadri-gramme. L&#8217;estimation de ces mod&#232;les utilise le lissage des probabilit&#233; de Good-Turing,
selon les recommandations classiques de lissage, discounting, et back-off (Chen &amp; Goodman, 1999).
Nous cherchons tout d&#8217;abord &#224; comparer les n-gramme &#224; horizon fixe avec des n-gramme &#224; horizon
variable. Les n-gramme &#224; horizon variable sont mis en &#339;uvre lors du test, en appliquant l&#8217;&#233;quation 4.
Notre objectif n&#8217;est pas de valider une technique de r&#233;duction de param&#232;tres au moment de la construction
du mod&#232;le, (Siu &amp; Ostendorf, 2000)(Niesler &amp; Woodland, 1994), mais plut&#244;t de d&#233;brider un mod&#232;le de
n-gramme &#224; horizon fixe pour en faire un mod&#232;le de n-gramme &#224; horizon variable. Notre mani&#232;re
de proc&#233;der introduit un co&#251;t de calcul suppl&#233;mentaire, mais il reste acceptable car les longueurs des
historiques sont faibles devant le nombre de mots &#224; traiter.
</p>
<p>Nous cherchons enfin &#224; situer les mod&#232;les n/m-multigramme par rapport aux deux approches pr&#233;c&#233;-
dentes. L&#8217;int&#233;r&#234;t du multigramme r&#233;side dans sa capacit&#233; &#224; pr&#233;dire une s&#233;quence de mots avec un seul
param&#232;tre. En moyenne on baisse le nombre de termes impliqu&#233;s dans le calcul de la perplexit&#233; ; il s&#8217;agit
alors d&#8217;une situation favorable. Cependant, le risque est de r&#233;partir une masse de probabilit&#233;s sur plus de
termes6. Pour que la comp&#233;tition entre mod&#232;les reste &#233;quitable, nous avons choisi de travailler avec des
mod&#232;les n/m-multigramme dont la taille maximale (en nombre de mots) est soumise &#224; une contrainte.
</p>
<p>4 Estimation des param&#232;tres des mod&#232;les
</p>
<p>Les exp&#233;riences sont r&#233;alis&#233;es &#224; partir de la suite de programme HTK (Woodland &amp; Young, 1993).
Cet ensemble de librairies et d&#8217;outils correspond &#224; une cha&#238;ne compl&#232;te permettant de construire et
de tester un mod&#232;le de langage. La gestion des n-gramme &#224; horizon variable n&#8217;est pas &#233;crite dans
la distribution standard de HTK. La modification du programme de test du mod&#232;le de langage a &#233;t&#233;
n&#233;cessaire pour introduire le traitement propos&#233; &#233;quation 4. La gestion des n/m-multigramme n&#8217;est pas
non plus &#233;crite. Les modifications &#224; faire sont d&#8217;une part dans le programme d&#8217;apprentissage, afin de
</p>
<p>6Un n-gramme classique estime, pour chaque historique, une densit&#233; de probabilit&#233; dont la complexit&#233; spatiale est celle
du vocabulaire. Les multigramme avec des t&#234;tes de longueur au plus m ont une complexit&#233; spatiale born&#233;e par |V|m, les
probabilit&#233;s tendent vers 0.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>&#201;valuation des Mod&#232;les de Langage n-gramme et n/m-multigramme
</p>
<p>parcourir syst&#233;matiquement toutes les unit&#233;s de multigramme possibles Il est &#233;galement n&#233;cessaire de
modifier, comme pour les n-gramme &#224; horizon variable, le programme de test, pour pouvoir parcourir
toutes les t&#234;tes et tous leurs historiques possibles.
</p>
<p>Pour les n-gramme &#224; horizon fixe, la perplexit&#233; du mod&#232;le de langage est d&#233;termin&#233;e directement gr&#226;ce &#224;
l&#8217;&#233;quation 6. Si le mot de t&#234;te du n-gramme n&#8217;est pas dans le vocabulaire s&#233;lectionn&#233;, il est alors compt&#233;
comme mot hors vocabulaire.
</p>
<p>PP = 2H
&#8727;
</p>
<p>avec (6)
H&#8727; = &#8722; 1
</p>
<p>m
log2 (P (w1, w2, . . . , wm))
</p>
<p>Pour les n-gramme &#224; horizon variable, la situation est diff&#233;rente : pour chaque mot plusieurs choix sont
possibles (le choix se fait entre un 2-gramme, un 3-gramme, ..., ou un n-gramme). Il suffit de choisir le
meilleur k-gramme parmi les n&#8722; 1 possibles (choix parmi toutes les longueurs d&#8217;historique autoris&#233;es).
Cet algorithme est appliqu&#233; phrase par phrase (hypoth&#232;se d&#8217;ind&#233;pendance des phrases entre elles). En fin
de traitement d&#8217;une phrase on conna&#238;t la perplexit&#233; &#233;valu&#233;e sur cette phrase, le nombre de mots pr&#233;dits
ainsi que le nombre de mots hors vocabulaire.
</p>
<p>Pour les n/m-multigramme, la situation est encore diff&#233;rente. Maintenant plusieurs t&#234;tes sont disponibles,
et pour chacune d&#8217;elles, plusieurs choix sont possibles. Nous avons volontairement limit&#233; la taille max-
imale du n/m-multigramme &#224; un nombre fixe de mots : avec des multigramme de taille au plus 2, nous
pourrions former 4 bi-gramme : P (wi|wi&#8722;1), P (wi|[wi&#8722;2 wi&#8722;1]), P ([wi wi+1]|wi&#8722;1), P ([wi wi+1]|[wi&#8722;2 wi&#8722;1]).
En limitant le nombre maximum de mots dans le n/m-multigramme, nous pouvons choisir le mod&#232;le
de langage avec lequel nous entrons en concurrence. Par exemple, avec des multigramme de taille au
plus 2, et une somme &#224; 3, nous n&#8217;avons plus que 3 choix possibles : P (wi|wi&#8722;1), P (wi|[wi&#8722;2 wi&#8722;1]), et
P ([wi wi+1]|wi&#8722;1). Dans le programme de test, afin de s&#233;lectionner la meilleure d&#233;coupe de la phrase
selon le max de l&#8217;&#233;quation 5, nous avons mis en place une recherche du meilleur chemin dans un graphe7
orient&#233; et valu&#233; selon l&#8217;algorithme de Dijkstra.
</p>
<p>5 M&#233;thodologie exp&#233;rimentale
</p>
<p>Les exp&#233;riences sont r&#233;alis&#233;es sur un corpus de texte du fran&#231;ais : tous les articles parus pendant l&#8217;ann&#233;e
1997 dans le journal &quot;Le Monde&quot; (ressource ELRA). Ce corpus est d&#233;coup&#233; en phrases par un logiciel
d&#8217;analyse syntaxique (logiciel Cordial de Synapse). Les phrases sont uniformis&#233;es par une r&#233;&#233;criture
syst&#233;matique en majuscules et la suppression de toute ponctuation. Le corpus ainsi obtenu contient
1 131 135 phrases pour un vocabulaire de 219 034 mots. Il s&#8217;agit de la taille exacte du vocabulaire (mots
variants en genre et en nombre, ainsi que les verbes rencontr&#233;s sous une forme conjugu&#233;e), le nombre
d&#8217;occurrence des mots est de 23 999 626. L&#8217;apprentissage se fait sur 70% du corpus, le test est r&#233;alis&#233; sur
les 30% restant. La r&#233;partition des phrases a &#233;t&#233; r&#233;alis&#233;e de mani&#232;re al&#233;atoire &#224; partir du corpus d&#8217;origine.
</p>
<p>Pour faire baisser le nombre de param&#232;tres d&#8217;un mod&#232;le de taille n, on fait &#233;voluer la valeur de cut-off sur
des n-gramme &#224; horizon fixe. On conserve une valeur de cut-off &#224; 1 sur les param&#232;tres d&#8217;ordre inf&#233;rieur
(horizon de longueur inf&#233;rieure &#224; n). Ainsi, pour faire baisser le nombre de param&#232;tres d&#8217;un mod&#232;le
de tri-gramme, on va augmenter la valeur du cut-off sur les probabilit&#233;s conditionnelles des tri-gramme,
et laisser constante la valeur de cut-off pour les probabilit&#233;s de bi-gramme et d&#8217;uni-gramme. Pour les
2/2-multigramme, on peut faire baisser le nombre de param&#232;tres en limitant le nombre de multigramme
autoris&#233;s dans le mod&#232;le de langage8. On peut ainsi fixer le nombre de param&#232;tres du mod&#232;le n/m-
</p>
<p>7L&#8217;algorithme de Viterbi permet de rechercher la meilleure solution a priori, nous lui pr&#233;f&#233;rons l&#8217;algorithme de Dijkstra qui
permet d&#8217;obtenir la meilleure solution a posteriori.
</p>
<p>8avec un nombre de multigramme &#224; 0, on obtient un mod&#232;le de bi-gramme ; cela est visible sur la figure 1 en prolongeant la
courbe de perplexit&#233; des n/m-multigramme.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>P. Alain, O. Bo&#235;ffard
</p>
<p>multigramme de fa&#231;on pr&#233;cise gr&#226;ce &#224; un algorithme de dichotomie qui s&#233;lectionne le bon nombre de
multigramme &#224; prendre en compte dans la suite.
</p>
<p>Si au moins un des mots de l&#8217;historique n&#8217;est pas pr&#233;sent dans le vocabulaire alors le mod&#232;le de langage
d&#233;clare ne pas pouvoir pr&#233;dire le n-gramme. Le mot de t&#234;te du n-gramme est alors d&#233;clar&#233; non pr&#233;dit,
et la perplexit&#233; n&#8217;&#233;volue pas. Dans la situation o&#249; tous les mots sont pr&#233;sents dans le vocabulaire, mais
o&#249; la probabilit&#233; du n-gramme n&#8217;a pas &#233;t&#233; apprise par le mod&#232;le de langage, le syst&#232;me de back-off d&#233;j&#224;
pr&#233;sent&#233; s&#8217;applique. Dans le cas des n-gramme &#224; horizon variable, la probabilit&#233; est &#233;valu&#233;e de mani&#232;re
identique, le mot en t&#234;te du n-gramme est d&#233;clar&#233; non pr&#233;dit si au moins un mot de son horizon est hors
vocabulaire. Si tous les mots de l&#8217;horizon sont dans le vocabulaire, le choix de la meilleure probabilit&#233;
est r&#233;alis&#233; selon l&#8217;&#233;quation 4. Pour les multigramme, l&#8217;algorithme de Dijkstra permet de d&#233;terminer la
meilleure solution au sens de l&#8217;&#233;quation 5, parmi toutes les solutions possibles.
</p>
<p>6 R&#233;sultats et commentaires
</p>
<p>La figure 1 pr&#233;sente l&#8217;&#233;volution de la perplexit&#233; en fonction du nombre de param&#232;tres des diff&#233;rents mod-
&#232;les pour diff&#233;rentes tailles de vocabulaire. Le mod&#232;le de bi-gramme &#224; horizon variable est exactement
le mod&#232;le de bi-gramme &#224; horizon fixe, les courbes de perplexit&#233; sont donc confondues. On peut ob-
server que le mod&#232;le de n/m-multigramme tend &#224; avoir un comportement de bi-gramme de mots quand
le nombre de multigramme autoris&#233;s diminue.
</p>
<p>La perplexit&#233; d&#8217;un mod&#232;le de langage augmente quand le nombre de param&#232;tres utilis&#233; baisse. Cela est
parfaitement normal, car le pouvoir de pr&#233;diction d&#8217;un mot de la langue est moins important avec un
nombre de param&#232;tres inf&#233;rieur. Un mod&#232;le de n-gramme semble avoir une perplexit&#233; plus importante
qu&#8217;un mod&#232;le de n + 1-gramme. Cependant (Bonafonte &amp; Mari&#241;o, 1996) rapporte que la perplexit&#233;
des n-gramme augmente &#224; partir de n = 5. Un mod&#232;le de tri-gramme avec un seuil de cut-off &#224; 2 a
une perplexit&#233; et un nombre de param&#232;tres plus faible qu&#8217;un mod&#232;le de bi-gramme avec un seuil &#224; 0 ;
le mod&#232;le de tri-gramme est donc pr&#233;f&#233;rable dans ce cas. Selon (Rosenfeld, 2000), l&#8217;int&#233;r&#234;t compar&#233;
d&#8217;un mod&#232;le de langage appara&#238;t lorsque la mesure de perplexit&#233; baisse de plus de 10%. Le mod&#232;le de
tri-gramme est donc notablement plus int&#233;ressant que le mod&#232;le de bi-gramme. Tout comme le mod&#232;le
de quadri-gramme est plus int&#233;ressant que le mod&#232;le de tri-gramme.
</p>
<p>Un mod&#232;le de n-gramme &#224; horizon variable, comparativement au n-gramme concurrent, &#224; horizon fixe,
obtient une perplexit&#233;9 plus faible. Cette baisse de la perplexit&#233; est due pour partie au calcul de la
probabilit&#233; maximum ; en effet, par construction, on obtient une probabilit&#233; au moins sup&#233;rieure &#224; celle
d&#233;termin&#233;e par le mod&#232;le &#224; horizon fixe. Le gain obtenu par des n-gramme &#224; horizon variable provient
&#233;galement de l&#8217;utilisation du coefficient de back-off par le mod&#232;le de n-gramme. En effet, le mod&#232;le de
n-gramme utilise un coefficient de back-off pour obtenir une probabilit&#233; de n-gramme &#224; horizon fixe &#224;
partir de la probabilit&#233; du n&#8722; 1-gramme qui lui correspond si le n-gramme n&#8217;est pas trouv&#233;. Le mod&#232;le
de n-gramme &#224; horizon variable permet de m&#233;langer les probabilit&#233;s des diff&#233;rents (n&#8722;k)-gramme avec
m &#8712; [1, n&#8722; 2], et ce sans p&#233;naliser des (n&#8722; k)-gramme d&#8217;ordre inf&#233;rieur.
Les n/m-multigramme se montrent moins performants que le mod&#232;le de n-gramme de m&#234;me ordre (c&#8217;est
&#224; dire a nombre de mot consid&#233;r&#233;s constants). En effet, l&#8217;&#233;quation 5 semble indiquer que le choix de la
meilleure probabilit&#233; se fait entre un bi-gramme de mots, un tri-gramme de mots, et un bi-gramme ayant
2 mots en t&#234;te (dans le cas o&#249; la taille maximum d&#8217;un multigramme est de 2 mots, et la somme des mots
du bi-gramme est d&#8217;au plus 3). Le choix ne peut donc par construction qu&#8217;&#234;tre au moins aussi bon qu&#8217;un
tri-gramme de mots. Cependant, nous pouvons constater que pour obtenir un nombre de param&#232;tres
&#233;quivalent afin de comparer les diff&#233;rents mod&#232;les, il faut interdire un nombre cons&#233;quent de multi-
gramme parmi ceux disponibles. Nous devons alors chercher &#224; am&#233;liorer ce mod&#232;le n/m-multigramme.
</p>
<p>9Bien s&#251;r, il ne s&#8217;agit pas exactement d&#8217;une mesure de perplexit&#233; qui devrait &#234;tre calcul&#233;e &#224; partir d&#8217;une distribution de
probabilit&#233;.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>&#201;valuation des Mod&#232;les de Langage n-gramme et n/m-multigramme
</p>
<p>0 0.5 1 1.5 2 2.5 3 3.5 4
x 106
</p>
<p>100
</p>
<p>120
</p>
<p>140
</p>
<p>160
</p>
<p>180
</p>
<p>200
</p>
<p>220
</p>
<p>240
</p>
<p>Nombre de param&#232;tres
</p>
<p>Pe
rp
</p>
<p>le
xit
</p>
<p>&#233;
</p>
<p>Vocabulaire : 30 000 mots
</p>
<p>2&#8722;gramme f
3&#8722;gramme f
3&#8722;gramme v
4&#8722;gramme f
4&#8722;gramme v
2/2&#8722;multigramme
</p>
<p>0.5 1 1.5 2 2.5 3 3.5 4 4.5
x 106
</p>
<p>120
</p>
<p>140
</p>
<p>160
</p>
<p>180
</p>
<p>200
</p>
<p>220
</p>
<p>240
</p>
<p>260
</p>
<p>280
</p>
<p>Nombre de param&#232;tres
</p>
<p>Pe
rp
</p>
<p>le
xit
</p>
<p>&#233;
</p>
<p>Vocabulaire : 60 000 mots
</p>
<p>2&#8722;gramme f
3&#8722;gramme f
3&#8722;gramme v
4&#8722;gramme f
4&#8722;gramme v
2/2&#8722;multigramme
</p>
<p>0 0.5 1 1.5 2 2.5
x 106
</p>
<p>50
</p>
<p>60
</p>
<p>70
</p>
<p>80
</p>
<p>90
</p>
<p>100
</p>
<p>110
</p>
<p>Nombre de param&#232;tres
</p>
<p>Pe
rp
</p>
<p>le
xit
</p>
<p>&#233;
</p>
<p>Vocabulaire : 3 000 mots
</p>
<p>2&#8722;gramme f
3&#8722;gramme f
3&#8722;gramme v
4&#8722;gramme f
4&#8722;gramme v
2/2&#8722;multigramme
</p>
<p>0 0.5 1 1.5 2 2.5 3 3.5 4
x 106
</p>
<p>120
</p>
<p>140
</p>
<p>160
</p>
<p>180
</p>
<p>200
</p>
<p>220
</p>
<p>240
</p>
<p>Nombre de param&#232;tres
</p>
<p>Pe
rp
</p>
<p>le
xit
</p>
<p>&#233;
</p>
<p>Vocabulaire : 30 000 mots
</p>
<p>2&#8722;gramme f
3&#8722;gramme f
4&#8722;gramme f
2/2&#8722;multigramme
2/2&#8722;multigramme cut&#8722;off
</p>
<p>Figure 1: Comparaison de l&#8217;influence du nombre de param&#232;tres sur la perplexit&#233; des mod&#232;les de n-
gramme &#224; horizon fixe (n-gramme-f) ou variable(n-gramme-v) pour n &#8712; [2, 4], et du 2/2-multigramme
pour diff&#233;rentes tailles de vocabulaire, et influence sur la perplexit&#233; de la m&#233;thode de cut-off pour r&#233;duire
les param&#232;tres du mod&#232;le de 2/2-multigramme avec un vocabulaire de 30 000 mots.
</p>
<p>Pour am&#233;liorer la situation, on peut tout d&#8217;abord chercher &#224; n&#8217;inclure dans les multigramme autoris&#233;s
que ceux qui apportent un gain vis &#224; vis de l&#8217;&#233;quation5. Nous avons constat&#233; par des exp&#233;riences que ces
multigramme n&#8217;am&#233;liorent pas significativement la perplexit&#233; (nous n&#8217;avons pas la place pour rapporter
ces exp&#233;riences). Cela semble indiquer que les multigramme qui apportent le plus gros gain en terme de
perplexit&#233; sont d&#233;j&#224; inclus dans la liste des plus fr&#233;quents. Une exp&#233;rience similaire consiste &#224; d&#233;finir la
liste des multigramme en changeant le seuil de cut-off. En effet, on peut observer une baisse significative
du nombre de param&#232;tres, qui s&#8217;accompagne d&#8217;une augmentation de la perplexit&#233; (environ 10 points)
quand on passe d&#8217;un bi-gramme de mots avec un cut-off &#224; 0 (respectivement 1) &#224; un bi-gramme de mots
avec un cut-off &#224; 1 (respectivement 2). La figure 1 montre l&#8217;&#233;volution de la perplexit&#233; en conservant
les multigramme les plus fr&#233;quents (100 000 multigramme pour un vocabulaire de 30 000 mots). On
peut constater une baisse du nombre de param&#232;tres sans hausse de la perplexit&#233; ; cette solution semble
donc convenir. Enfin, &#233;tant donn&#233;e la baisse significative de la perplexit&#233; observ&#233;e avec peu de multi-
gramme entre un bi-gramme de mots avec un cut-off &#224; 1, et un 2/2-multigramme avec le m&#234;me cut-off.
On peut souhaiter g&#233;n&#233;raliser l&#8217;usage des multigramme aux n-gramme. Cependant la complexit&#233; risque
d&#8217;augmenter de mani&#232;re exponentielle avec n.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>P. Alain, O. Bo&#235;ffard
</p>
<p>7 Conclusion
</p>
<p>Cet article a pr&#233;sent&#233; des r&#233;sultats concernant des mod&#232;les de langage statistiques de type n-gramme &#224;
horizon fixe ou variable et des n/m-multigramme. &#192; taux de mots hors-vocabulaire fixe, le comporte-
ment des n-gramme classiques fait baisser la perplexit&#233; pour des valeurs de n de 3 &#224; 4, mais au prix d&#8217;une
baisse du nombre de mots pr&#233;dits (environ 7 millions pour un mod&#232;le de bi-gramme, 6.8 millions pour
un tri-gramme, et un peu plus de 6.6 millions pour un quadri-gramme). Plus on reconna&#238;t des mots, plus
la probabilit&#233; conjointe va &#234;tre faible, on peut donc trouver discutable de comparer entre eux des mod&#232;les
de n-gramme qui ne se trouvent pas tout &#224; fait sur le m&#234;me pied d&#8217;&#233;galit&#233;. Ce probl&#232;me ne se pose pas
pour les n-gramme &#224; horizon variable, ou les n/m-multigramme, car le nombre de mots pr&#233;dits est &#224;
chaque fois celui du mod&#232;le de bi-gramme. Les r&#233;sultats de perplexit&#233; obtenus avec des vocabulaires de
taille plus importante nous montrent &#224; la fois une augmentation de la perplexit&#233;, et une augmentation du
nombre de param&#232;tres. Cette augmentation est due encore une fois &#224; une augmentation du nombre de
mots pr&#233;dits (pour un mod&#232;le de bi-gramme, nous avons pr&#232;s de 4.9 millions de mots pr&#233;dits pour un
vocabulaire de 3 000 mots, 7 millions pour 30 000 mots, et 7.3 millions pour 60 000 mots). Le taux de
mots hors vocabulaire sur le corpus de test baisse de 19.38% pour 3 000 mots &#224; 1.65% pour 60 000 mots.
Nous avons montr&#233; que le mod&#232;le de multigramme le plus simple, un 2/2-multigramme (c&#8217;est-&#224;-dire un
bi-gramme de s&#233;quences comprenant au plus deux mots) se comporte comme un mod&#232;le situ&#233; entre un
bi-gramme et un tri-gramme classique. Notre objectif consiste &#224; pousser un peu plus loin ces mod&#232;les en
augmentant notamment l&#8217;ordre et en r&#233;glant le nombre de param&#232;tres par des techniques de cut-off.
</p>
<p>R&#233;f&#233;rences
</p>
<p>BIMBOT, F., PIERACCINI, R., LEVIN, E., &amp; ATAL, B. 1995. Variable-Length Sequence Modeling: Multigrams.
IEEE Signal Processing Letters, 2(6), 111&#8211;113.
BONAFONTE, A., &amp; MARI&#209;O, J. 1996. Language Modeling Using X-grams. Pages 394&#8211;397 of: Proceedings of
the International Conference on Spoken Language Processing.
CHEN, S.F., &amp; GOODMAN, J. 1999. An empirical study of smoothing techniques for language modeling. Com-
puter Speech and Language, 13(4), 359&#8211;394.
DELIGNE, S., &amp; BIMBOT, F. 1995. Language modeling by variable length sequences: theoretical formulation
and evaluation of multigrams. In: IEEE International Conference on Acoustics and Speech Signal Processing.
DELIGNE, S., &amp; SAGISAKA, Y. 2000. Statistical language modeling with a class-based n-multigram model.
Computer Speech and Language, 14, 261&#8211;279.
KATZ, S.M. 1987. Estimation of Probailities from Sparse Data for the Language Model Component of a Speech
Recognizer. IEEE transactions on Acoustics, Speech and Signal Processing, 35, 400&#8211;401.
NIESLER, T.R., &amp; WOODLAND, P.C. 1994. Variabl-length category n-gram language models. Computer Speech
and Language, 13, 99&#8211;124.
ROSENFELD, R. 2000. Two decades of statistical language modeling: where do we go from here? Proceedings of
the IEEE, 88(8), 1270&#8211;1278.
SHIELDS, P.C. 1998. The Interactions Between Ergodic Theory and Information Theory. IEEE Transactions on
Information Theory, 44, 2079&#8211;2093.
SIU, M., &amp; OSTENDORF, M. 2000. Variable n-grams and extensions for conversational speech language model-
ing. IEEE transactions on Speech and Audio Processing, 8(1), 63&#8211;75.
WOODLAND, P.C., &amp; YOUNG, S.J. 1993. The HTK Continuous Speech Recogniser. Pages 2207&#8211;2219 of:
Proceedings of the Eurospeech conference.
ZITOUNI, I. 2002. A Hierarchical Language Model Based on Variable-Length Class Sequences: The MC&#957;n
Approach. IEEE Transactions on Speech and Audio Processing, 10(3), 193&#8211;198.</p>

</div></div>
</body></html>