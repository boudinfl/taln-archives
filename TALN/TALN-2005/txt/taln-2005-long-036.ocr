TALN 2005, Dourdan, 6-10 juin 2005

Evaluation des Modéles de Langage n-gramme et n/m-multigramme

P. Alain, O. Boéffard
IRISA — Université de Rennes 1 / ENSSAT
6, rue de Kerampont, 22305 Lannion
{pierre.alain,olivier.boeffard} @irisa.fr

Mots-clefs : Modeles de Langage statistiques, n-gramme, multigrarnme, évaluation

Keywords: Statistical Language Models, n-grams, phrase multigrams

Résumé Cet article présente une évaluation de modeles statistiques du langage menée sur la langue
Francaise. Nous avons cherché a comparer la performance de modeles de langage exotiques par rapport
aux modeles plus classiques de n-gramme a horizon ﬁxe. Les expériences réalisées montrent que des
modeles de 71- gramme a horizon Variable peuvent faire baisser de plus de 10% en moyenne la perplexité
d’un modele de n- gramme a horizon ﬁxe. Les modeles de n / m-multigramme demandent une adaptation
pour pouvoir étre concurrentiels.

Abstract This paper presents an evaluation of statistical language models carried out on the French
language. We compared the performance of some exotic models to the one of the more traditional n-
gram model. The experiments show that the Variable n-gram models can drop more than 10% of the
average perplexity for a ﬁxed 71- gram model. n / m-multigram models require an adaptation to be able to
compete.

1 Introduction

La modélisation du langage est un probleme crucial et tres largement abordé en traitement automatique
de la langue écrite ou parléel. A partir de l’observation de séquences de mots, il s’agit de construire
un modele dont l’objectif est de prédire avec succes de nouvelles séquences. On peut distinguer déja
deux problemes, d’une part celui du choix du modele et de sa méthodologie de construction et d’autre
part celui de la méthodologie d’ évaluation d’un modele de langage. Concernant le premier point, on
peut distinguer des approches déterministes qui tiennent compte de l’organisation profonde des mots
liées notamment a la syntaxe, des approches probabilistes qui s’intéressent essentiellement a la forme de
surface (Rosenfeld, 2000).

L’ évaluation est un point relativement délicat dans la mesure o1‘1 elle peut étre dépendante du modele
choisi. La mesure la plus communément adoptée consiste a calculer l’entropie croisée e11tre un modele
de langage et la distribution réelle des données observées, mais inconnue. En supposant que les données
suivent une distribution stationnaire et ergodique, le calcul de l’entropie-croisée peut étre estimé a partir
d’un corpus sufﬁsamment grandz. La perplexité d’un modele de langage n’est qu’une autre maniere de
représenter le degré d’incertitude d’un modele et se calcule directement a partir de l’entropie-croisée du
modele sur un jeu de phrases de test. Pour un mot a prédire, la Valeur de la perplexité représente le

1On peut citer le domaine de la reconnaissance automatique de la parole mais aussi celui de la reconnaissance de texte
manuscrit ou encore celui de la traduction automatique.

2Théoreme de Shannon-MacMi1lan-Brieman, (Shields, 1998). En respectant ces hypotheses de stationnarité et d’ergodicité,
un corpus de parole de longueur ﬁnie peut reﬂéter la distribution réelle des données.

P. Alain, O. Boeffard

nombre d’ hypotheses moyennes de branchement3. Plus la perplexite est faible, plus le facteur moyen de
branchements d’un mot vers un autre est bas et plus le modele de langage est efﬁcace. Pour les modeles
n- gramme, un mot est predit en tenant compte d’un historique relativement limite des mots qui le prece-
dent. Ces modeles connaissent ﬁnalement tres peu des raisons profondes de l’organisation des mots
dans une phrase. En revanche, l’utilisation de probabilites conditionnelles et un apprentissage realise
a partir de quelques millions de phrases permettent d’obtenir de bonnes performances. Leur principal
defaut reside dans la complexite spatiale sous-jacente. Theoriquement, plus la sequence de l’historique
du modele s’allonge (n augmente), plus le modele repartit efﬁcacement la masse de probabilites sur des
mots qui reviennent souvent apres une valeur particuliere de l’historique. Cependant, plus n augmente,
plus les observations se rareﬁent compte-tenu de la nature hyperbolique de la distribution de ces evene-
ments4. Pour des situations experimentales reelles, les valeurs courantes de n depassent rarement 4 (Siu
& Ostendorf, 2000). De nombreuses solutions ont ete apportees au probleme de l’explosion combina-
toire et a celui de la rarefaction des evenements (Rosenfeld, 2000). Des techniques de lissage permettent
notamment de repondre a la difﬁculte de l’estimation d’ une distribution de probabilite lorsque les evene-
ments sont rares. On peut citer le principe du lissage qui n’effectue l’estimation des points de la densite
au sens du maximum de vraisemblance que pour des evenements dont l’occurrence est superieure a un
seuil de cut-oﬁ. Une partie de la masse de probabilite est repartie sur des evenements dont l’occurrence
est inferieure au seuil, (Katz, 1987). (Chen & Goodman, 1999) propose une evaluation des principales
techniques de lissage les plus utilisees.

Les modeles de n-gramme pour lesquels la longueur de l’historique est variable sont une alternative
aux n-gramme classiques pour lesquels la longueur de l’historique reste ﬁxe. Le principe consiste a ne
pas retenir un historique de longueur n si la contribution du n-gramme correspondant n’ameliore pas la
performance du modele. Toute la difﬁculte reside dans la decision d’abandon d’un n-gramme pour un
(n — k)-gramme avec 1 3 k < n, (Niesler & Woodland, l994)(Siu & Ostendorf, 2000).

Les modeles multigramme sont des modeles de type n-gramme ou la tete peut avoir une longueur
superieure a l.(Bimbot et al. , l995)(Deligne & Bimbot, 1995) presentent un cadre theorique pour
des multigramme formes sur des modeles d’uni-gramme (longueur d’historique nulle). Les experi-
ences rapportees concement une application avec un vocabulaire limite de 900 mots pour un corpus
d’apprentissage de 100 000 phrases et un corpus de test de 1 000 phrases (dont 52 occurrences de mots
hors-vocabulaire). Les modeles de type multigramme obtiennent une perplexite meilleure que les n-
gramme classiques lorsque n > 3. Compte-tenu de la taille relativement limitee des corpus, les conclu-
sions sont difﬁcilement transposables directement sur des corpus plus importants. (Deligne & Sagisaka,
2000) se place dans un contexte de multigramme de classes de mots sur des modeles de bi-gramme.
Les experiences sont menees avec un vocabulaire d’ environ 3 000 mots, 100 000 phrases pour le corpus
d’apprentissage et environ 700 phrases pour le test. Deux types de mesure sont rapportes 2 d’une part
la perplexite pour les modeles de type multigramme et d’autre part le taux d’erreur d’un systeme de re-
connaissance de la parole. Les resultats entre multigramme et n-gramme classiques (bi- et tri-gramme)
sont difﬁcilement comparables. En effet, pour ces demiers, les valeurs de perplexite sont absentes et les
modeles de n-gramme semblent avoir ete non reduits5. (Zitouni, 2002) propose des modeles de multi-
gramme ou les probabilites de co-occurrence de mots sont conditionnees par rapport a des classes. Les
experiences concement deux annees du journal "Le Monde" (annees 1987 et 1988) pour un vocabulaire
de 20 000 mots. L’utilisation de ces multigramme permet de reduire de 7% la perplexite des tri- gramme
classiques. Encore une fois, il est difﬁcile de retrouver sur cette experience une comparaison entre mod-
eles a nombre de parametres constant.

Cet article propose une etude experimentale sur les performances relatives des modeles de language

311 s’agit d’une moyenne geometrique.

411 s’agit de distributions a queue lourde o1‘1 beaucoup d’ evenements sont extremement rares et peu sont tres frequents. La
loi de Zipf est un cas patticulier de lois puissances caracteristiques de ce phenomene.

5Le comportement d’un n-gramme est non-lineaire, il est possible de reduire de facon impoitante le nombre de parametres
sans trop degrader ses performances.

Evaluation des Modeles de Langage n-gramme et n / m-multigramme

de type n-gramme a horizon ﬁxe, a horizon variable et multigramme. La section 2 presente un cadre
theorique pour ces trois types de modeles de langage statistiques. La section 3 expose la problematique
d’une telle experimentation ainsi que nos hypotheses de travail. Une evaluation a ete menee sur environ
un million de phrases en francais. La section 4 decrit la methodologie suivie. La section 5 expose les ex-
periences rnises en oeuvre. Enﬁn, la section 6 presente les resultats et une interpretation du comportement
des modeles en fonction des donnees traitees.

2 Cadre théorique

Un modele de langage statistique est un ensemble de distributions de probabilite sur des sequences
observees de symboles. Comme, en pratique, il est impossible de caracteriser de telles distributions,
les modeles de langage se distingueront entre eux par les hypotheses choisies pour reduire la complexite
combinatoire et ameliorer leur capacite de generalisation. Apres une presentation des notations utilisees,
nous discutons du modele de n-gramme a horizon ﬁxe, du modele de n-gramme a horizon variable et
enﬁn du modele n / m-multigramme.

Soit une sequence de mots W = (wl, U12, ..., wN) avec wi une variable representant le mot de rang 2' dans
la sequence W. Les valeurs possibles pour wi appartiennent a un vocabulaire V. I1 peut s’agit souvent
d’un vocabulaire ferme dans le cadre de systemes de dialogue, nous considerons ici l’etude de la langue
naturelle, nous choisissons un vocabulaire ouvert. Nous pouvons decrire cette sequence par une suite de
variables aleatoires wi. La probabilite conj ointe des variables de la sequence W peut se developper de la
maniere suivante en faisant apparaitre des probabilites conditionnelles 2

N
p(W) = p(w1) >< Hp(wi|w1, ...w,-_1) (1)
i=2
L’objectif d’un modele de langage consiste a calculer cette probabilite conjointe, c’est-a-dire a estimer
des valeurs pour chacune des probabilites conditionnelles. L’estimation de ces probabilites condition-
nelles est en pratique impossible car le nombre de parametres croit de maniere exponentielle avec la
longueur de la suite de mots. Pour contrer cette difﬁculte, un modele de langage pose une probabilite
conditionnelle approchee  en simpliﬁant la loi conjointe, equation 1.

On note 9 l’ensemble des groupes de mots formes sur le vocabulaire V. On note 8 l’ensemble des
sequences formees sur les elements de 9. On note 8* C 8, l’ensemble des sequences de 8 qui corre-
spondent a W. On note S une sequence particuliere de 8*. Par exemple, pour W = (wl, U12, U13), on
a 2

F01] [102]] [[103]]
3* : [wi: 10:, 10:]
[W1] [W2» W3]

On note |S | le nombre de groupes de mots dans la sequence S. Soit [<1 le groupe de mots de rang k dans
la sequence S, on note i(S l’indice dans la sequence W du premier mot de S On note l(S
le nombre de mots de S

On note hi, la chaine des variables aleatoires representant l’apparition conjointe de tous les mots
wu de W pouru E [i,z' + (j — 

h_  = wl-,w,-+1, ...,w,-+0-_1) Si '1: +  — 1) f N
W wl-,w1-+1, ...,wN sinon

On deﬁnit egalement l’operateur t,-,3-(W) qui represente les j mots precedents le mot wi. On a donc
t,-,3-(W) = h,-_]-,3-(W). t,-,3-(W) correspond a un horizon ou historique (un groupe de mots qui precede

P. Alain, O. Boeffard

l’obserVation d’un mot). hm-(W) correspond 5 la téte d’un parametre du modele de langage (pour les
modeles n-gramme 5 horizon ﬁxe ou Variable, la Variable aléatoire de téte est dégénérée, et ne contient
qu’un seul mot). Les modeles de langage cherchent d’une part 5 réduire au maximum la longueur d’un
horizon (minimisation du nombre de parametres) et d’autre part, pour un horizon donné, 5 estimer la
distribution de probabilité des historiques pour calculer la probabilité d’apparition du mot w,-. Soit W
associée 5 une séquence de découpage S, la loi conjointe estimée par le modele de langage peut alors se
réécrire sous la forme suivante avec n l’ordre du 71- gramme 2

ISI

P§(W) = P(hz'(s(1)),z(s(1))(W)) >< Hp(hi(S(k)),l(S(k))(W)|ti(S(k)),n—1(W)) (2)
k=2

2.1 Les modéles n-gramme 5 horizon ﬁxe

Pour un n-gramme 5 horizon ﬁxe, on fait une hypothese d’indépendance conditionnelle du mot wi avec
les mots présents dans la sequence 5 une distance de plus de n — 1 mots (pour n = 2, ce modele est un
modele de bi-gramme ; la probabilité P(W) correspond 5 celle d’une chaine de Markov. Pour n = 3, on
parle de tri-gramme et pour n = 4 de quadri-gramme). Comme nous l’aVons déj5 souligné, ce modele est
tres simple, mais le nombre de parametres croit de maniere exponentielle avec n. Pour cette raison, les
modeles de n-gramme les plus utilisés le sont pour des Valeurs de n de l’ordre de 3 ou 4. Pour corriger
le probleme des événements rares, il existe des techniques de lissage des probabilités conditionnelles,
couplées 5 des techniques de back-of permettant de corriger celles d’éVénements manquants lors de
l’apprentissage, (Katz, 1987). Cette correction s’effectue en pondérant la probabilité du (n — 1)-gramme
par un coefﬁcient de back-of de telle maniere que la distribution de probabilité des n-gramme somme
touj ours 5 1. Le terme produit de l’équation 2 se simpliﬁe alors 2

||l>

p(hi(S(k)),l(S(k)) (W) |ti(S'(k;)),n—1 (W)) P(hz',1(W) |ti,'n.—1(VV)) (3)

Pour ce modele, S = W, on obtient simplement 2

PML(W) = P§(W)

2.2 Les n-gramme 5 horizon variable

Forcer l’estimation du terme produit de l’équation 2 5 un historique de longueur n introduit un double
biais. D’une part les occurrences sont plus faibles, on a donc tendance 5 faire du lissage et 5 étre moins
précis. D’autre part, on introduit des distributions conditionnelles sur wi qui ne servent pas 5 grand
chose (augmentation injustiﬁée du nombre de parametres). Autoriser une Variation de la longueur de
l’historique pour prédire wi permet de régler ce probleme de sur-apprentissage. Les n- gramme 5 horizon
Variable déﬁnissent une probabilité en adaptant une longueur d’historique optimale en fonction de 112,-.
L’ approche traditionnelle pour ce type de modeles consiste 5 déterminer au moment de l’apprentissage
les longueurs optimales 5 retenir, (Bonafonte & Mariﬁo, l996)(Siu & Ostendorf, 2000). Dans cette
situation un n- gramme est remplacé par un (n — k)-gramme avec 1 3 k < n. Les n-gramme 5 horizon
Variable peuvent apparaitre intéressants pour un double enjeu 2 d’une part 5 nombre de parametres ﬁxe,
il peuvent répondre 5 une amélioration de la performance des n-gramme 5 horizon ﬁxe et d’autre part, 5
perplexité ﬁxée, ils peuvent étre utiles 5 la diminution du nombre de parametres d’un modele de langage.

Au moment du test, lors du calcul de la perplexité d’une phrase, pour ce modele de n- gramme 5 horizon
Variable, le terme produit de l’équation 2 s’écrit 2

P(hi(S(k)),l(S(k))(W)|ti(S(k)),n—1(W)) é 1<{f1<3ff_1{P(hi,1(W)|ti,u(W))} (4)

Evaluation des Modeles de Langage n-gramme et n / m-multigramme

Cette écriture signiﬁe que pour chaque mot wi a prédire, on cherche a maximiser la probabilité en se
basant sur des modeles allant du bi-gramme au n-gramme (equation 3). Pour ce modele, S = W, on
obtient simplement 2

PML(W) = P§(W)

2.3 Les n / m-multigram

Un n / m-multigramme correspond a une probabilité conditionnelle ou la téte du 71- gramme peut étre plus
longue qu’un mot unique. m représente le nombre maximum de mots dans un groupe de mots en téte.
Lors du test du modele de langage, pour une découpe S donnée, nous cherchons la meilleure probabilité
suivant l’équation 2

||l>

max {P(hz',u(W) |tz',v(W))} (5)

p(hi(S(k)),l(S(k))(W)|ti(S(k)),n—1(W)) 1<u<m 1<v<mX(n_1)

I1 sufﬁt ensuite de prendre la meilleure solution sur toutes les séquences S E 8* 2

1>ML(W) = 3«r§Eg1*aX{p§(W)}

3 Problématique et hypotheses méthodologiques

Notre objectif est de Vériﬁer l’intérét des modeles n-gramme a horizon Variable par rapport a des mod-
eles a horizon ﬁxe et a des modeles de type n/m-multigramme. La difﬁculté de mise en oeuvre d’une
telle évaluation réside dans le probleme du controle explicite des parametres lors de la construction des
modeles. Différents facteurs sont responsables de la qualité d’un modele de langage. Certains inﬂuent
directement le processus d’apprentissage alors que d’autres déterminent la mesure de performance d’un
modele.

Tout d’abord l’estimation des probabilités conditionnelles provient directement de la détection de n-
uplets. Avec peu de sequences, on défavorise notamment les modeles de n- gramme d’ordre supérieur.
Le nombre de parametres d’un modele de langage de type n-gramme est proportionnel a |V|". Le cut-
oﬁ est une technique simple et relativement efﬁcace pour limiter le nombre de parametres (Chen &
Goodman, 1999). Il s’agit de ne pas retenir les n-uplets qui apparaissent sous un seuil d’ occurrence.
Ainsi, un cut-of a 1 signiﬁe qu’un mot doit apparaitre au moins 2 fois pour étre intégré au modele de
langage. Cependant, compte-tenu de la forme des distributions de probabilité (fonction puissance), la
réduction conséquente du nombre de parametres n’est pas linéaire en fonction de la Valeur de cut-of.
En introduction, nous avons souligné le role de la perplexité comme outil de mesure de la qualité d’un
modeles de langage.

Un autre facteur clé que l’on doit maintenir entre les différents modeles de langage pour pouvoir com-
parer les Valeurs de perplexité est le nombre de mots hors-Vocabulaire. Plus la taille du Vocabulaire est
faible (et donc plus le nombre de parametres est faible), plus le taux des mots hors-Vocabulaire augmente
aVec des Valeurs de perplexité qui s’améliorent. Il s’agit d’un facteur calculé a posteriori, une fois le
modele construit. Il est donc difﬁcile d’ intervenir explicitement sur cette Valeur.

Le calcul de la perplexité peut Varier notamment par la prise en compte ou non des mots hors-Vocabulaire
sur l’ensemble de test. On peut décider de ne pas prédire un mot hors Vocabulaire ; dans ce cas
l’accumulation de la perplexité est plus faible mais le nombre de mot prédit n’augmente pas. Le cal-
cul de la perplexité fait intervenir une hypothese de stationnarité et d’ergodicité qu’il faudrait Véri-
ﬁer en pratique. La performance d’un modele de langage dépend donc étroitement du couple ensem-
ble d’apprentissage/ensemble de test. 11 faut que ces ensembles contiennent un nombre sufﬁsant de

P. Alain, O. Boeffard

sequences pour pouvoir conclure a des resultats stables. Au cours de nos experiences, nous avons essaye
de minimiser l’inﬂuence de chacun de ces facteurs de maniere a favoriser la comparaison e11tre structures
de modeles (n-gramme a horizon ﬁxe, n-gramme a horizon variable et n/m-multigramme).

Nous avons considere les hypotheses methodologiques suivantes. Une annee du journal "Le Monde" a
ete choisie comme univers linguistique (annee 1997). Apres extraction des phrases et tirage aleatoire, ce
corpus est reparti en deux sous-corpus 2 70% pour le corpus d’ apprentissage et 30% pour le corpus detest.
Le choix d’un corpus ﬁxe est sufﬁsant pour valider une comparaison entre modeles, mais ne permettra
pas de conclure sur la robustesse des resultats. Des analyses complementaires seront donc necessaires.
Nous avons considere trois ensembles de mots 2 un premier vocabulaire a 3 000 mots, un deuxieme
a 30 000 mots et un dernier a 60 000 mots (il s’agit a chaque fois des plus frequents sur l’ensemble
d’apprentissage). Les valeurs de perplexite et les taux de mots hors vocabulaire dependent directement
de ces trois ensembles. Nous avons cherche a controler explicitement le nombre de parametres de nos
modeles. Deux approches complementaires ont ete mises en oeuvre 2 d’une part par application de seuils
de coupure sur les differents types de n-gramme et d’ autre part par la conservation des co-occurrences
de m-uplets de mots les plus frequentes pour les n / m-multigramme. Dans le premier cas, nous balayons
un spectre de valeurs de cut-oﬁ‘ et nous observons a posteriori le nombre de parametres. Ce nombre nous
sert ensuite a ajuster le nombre de multigramme autorises a entrer dans le vocabulaire et se placer ainsi
a nombre de parametres constant (avec une tolerance de 1%). La perplexite calculee ne tient pas compte
des mots hors vocabulaire qu’ils soient presents dans la tete ou dans l’historique d’un n-gramme.

Notre systeme de reference est celui des n-gramme classiques (que nous avons nomme n-gramme a
horizon ﬁxe). Nous avons choisi des valeurs communement admises pour n et introduit des modeles de
bi-, tri- et quadri- gramme. L’estimation de ces modeles utilise le lissage des probabilite de Good-Turing,
selon les recommandations classiques de lissage, discounting, et back-of (Chen & Goodman, 1999).
Nous cherchons tout d’abord a comparer les n-gramme a horizon ﬁxe avec des n-gramme a horizon
variable. Les n-gramme a horizon variable sont mis en oeuvre lors du test, en appliquant l’equation 4.
Notre obj ectif n’est pas de valider une technique de reduction de parametres au moment de la construction
du modele, (Siu & Ostendorf, 2000)(Niesler & Woodland, 1994), mais plutot de debrider un modele de
n-gramme a horizon ﬁxe pour en faire un modele de n-gramme a horizon variable. Notre maniere
de proceder introduit un coﬁt de calcul supplementaire, mais il reste acceptable car les longueurs des
historiques sont faibles devant le nombre de mots a traiter.

Nous cherchons enﬁn a situer les modeles n / m-multigramme par rapport aux deux approches prece-
dentes. L’ interet du multigramme reside dans sa capacite a predire une sequence de mots avec un seul
parametre. En moyenne on baisse le nombre de termes impliques dans le calcul de la perplexite ; il s’agit
alors d’une situation favorable. Cependant, le risque est de repartir une masse de probabilites sur plus de
termes6. Pour que la competition entre modeles reste equitable, nous avons choisi de travailler avec des
modeles n / m-multigramme dont la taille maximale (en nombre de mots) est soumise a une contrainte.

4 Estimation des paramétres des modéles

Les experiences sont realisees a partir de la suite de programme HTK (Woodland & Young, 1993).
Cet ensemble de librairies et d’outils correspond a une chaine complete permettant de construire et
de tester un modele de langage. La gestion des n-gramme a horizon variable n’est pas ecrite dans
la distribution standard de HTK. La modiﬁcation du programme de test du modele de langage a ete
necessaire pour introduire le traitement propose equation 4. La gestion des n / m-multigramme n’est pas
non plus ecrite. Les modiﬁcations a faire sont d’une part dans le programme d’apprentissage, aﬁn de

6Un n-gramme classique estime, pour chaque historique, une densite de probabilité dont la complexite spatiale est celle
du Vocabulaire. Les multigramme avec des tétes de longueur au plus m ont une complexite spatiale bomee par |V|'”, les
probabilites tendent vers 0.

Evaluation des Modeles de Langage n-gramme et n / m-multigramme

parcourir systematiquement toutes les unites de multigramme possibles Il est egalement necessaire de
modiﬁer, comme pour les 71- gramme a horizon Variable, le programme de test, pour pouvoir parcourir
toutes les tetes et tous leurs historiques possibles.

Pour les 71- gramme a horizon ﬁxe, la perplexite du modele de langage est determinee directement grace a
l’equation 6. Si le mot de tete du n-gramme n’est pas dans le Vocabulaire selectionne, il est alors compte
comme mot hors Vocabulaire.

PP = 2H* aVec (6)

1
H* = —R log2 (P(w1,w2, . . . ,wm))

Pour les n-gramme a horizon Variable, la situation est differente 2 pour chaque mot plusieurs choix sont
possibles (le choix se fait entre un 2-gramme, un 3- gramme, ..., ou un n-gramme). Il sufﬁt de choisir le
meilleur k-gramme parmi les n — 1 possibles (choix parmi toutes les longueurs d’historique autorisees).
Cet algorithme est applique phrase par phrase (hypothese d’independance des phrases entre elles). En ﬁn
de traitement d’une phrase on connait la perplexite evaluee sur cette phrase, le nombre de mots predits
ainsi que le nombre de mots hors Vocabulaire.

Pour les n / m-multigramme, la situation est encore differente. Maintenant plusieurs tetes sont disponibles,

et pour chacune d’elles, plusieurs choix sont possibles. Nous avons Volontairement lirnite la taille max-

imale du n / m-multigramme a un nombre ﬁxe de mots 2 aVec des multigramme de taille au plus 2, nous
pourrions former4bi-gramme: P(’LUi|’LUi_1), P(’LUi| [’LUi_2 w,-_1]), P([’LUi w,-+1]|w,-_1), P([’LUi w,-+1] I [wl-_2 w,-_1]).
En limitant le nombre maximum de mots dans le n / m-multigramme, nous pouvons choisir le modele

de langage avec lequel nous entrons en concurrence. Par exemple, aVec des multigramme de taille au

plus 2, et une some a 3, nous n’aVons plus que 3 choix possibles 2 P(w,-|w,-_1), P(w,-| [w,-_2 w,-_1]), et

P([w,- w,-+1]|w,-_1). Dans le programme de test, aﬁn de selectionner la meilleure decoupe de la phrase

selon le max de l’equation 5, nous avons mis en place une recherche du meilleur chemin dans un graphe7

oriente et Value selon l’algorithme de Dijkstra.

5 Méthodologie expérimentale

Les experiences sont realisees sur un corpus de texte du francais 2 tous les articles parus pendant l’armee
1997 dans le journal "Le Monde" (ressource ELRA). Ce corpus est decoupe en phrases par un logiciel
d’analyse syntaxique (logiciel Cordial de Synapse). Les phrases sont uniformisees par une reecriture
systematique en majuscules et la suppression de toute ponctuation. Le corpus ainsi obtenu contient
1 131 135 phrases pour un Vocabulaire de 219 034 mots. I1 s’agit de la taille exacte du Vocabulaire (mots
Variants en genre et en nombre, ainsi que les Verbes rencontres sous une forme conjuguee), le nombre
d’occurrence des mots est de 23 999 626. L’ apprentissage se fait sur 70% du corpus, le test est realise sur
les 30% restant. La repartition des phrases a ete realisee de maniere aleatoire a partir du corpus d’ origine.

Pour faire baisser le nombre de parametres d’un modele de taille n, on fait evoluer la Valeur de cut-of sur
des n-gramme a horizon ﬁxe. On conserve une Valeur de cut-of a 1 sur les parametres d’ordre inferieur
(horizon de longueur inferieure a n). Ainsi, pour faire baisser le nombre de parametres d’un modele
de t1i- gramme, on Va augmenter la Valeur du cut-of sur les probabilites conditionnelles des tri- gramme,
et laisser constante la Valeur de cut-of pour les probabilites de bi-gramme et d’uni-gramme. Pour les
2/ 2-multigramme, on peut faire baisser le nombre de parametres en limitant le nombre de multigramme
autorises dans le modele de langages. On peut ainsi ﬁxer le nombre de parametres du modele n / m-

7L’algorithme de Viterbi permet de rechercher la meilleure solution a priori, nous lui preferons l’algorithme de Dijkstra qui
permet d’obtenir la meilleure solution a posteriori.

8avec un nombre de multigramme a 0, on obtient un modele de bi- gramme ; cela est visible sur la ﬁgure 1 en prolongeant la
courbe de perplexite des n / m-multigramme.

P. Alain, O. Boeffard

multigramme de facon precise grace a un algorithme de dichotomie qui selectionne le bon nombre de
multigramme a prendre en compte dans la suite.

Si au moins un des mots de l’historique n’est pas present dans le Vocabulaire alors le modele de langage
declare ne pas pouvoir predire le 71- gramme. Le mot de tete du n-gramme est alors declare non predit,
et la perplexite n’eVolue pas. Dans la situation ou tous les mots sont presents dans le Vocabulaire, mais
ou la probabilite du n-gramme n’a pas ete apprise par le modele de langage, le systeme de back-of deja
presente s’applique. Dans le cas des n- gramme a horizon Variable, la probabilite est evaluee de maniere
identique, le mot en tete du n-gramme est declare non predit si au moins un mot de son horizon est hors
Vocabulaire. Si tous les mots de l’horizon sont dans le Vocabulaire, le choix de la meilleure probabilite
est realise selon l’equation 4. Pour les multigramme, l’algorithme de Dijkstra permet de determiner la
meilleure solution au sens de l’equation 5, parmi toutes les solutions possibles.

6 Résultats et commentaires

La ﬁgure 1 presente l’eVolution de la perplexite en fonction du nombre de parametres des differents mod-
eles pour differentes tailles de Vocabulaire. Le modele de bi- gramme a horizon Variable est exactement
le modele de bi-gramme a horizon ﬁxe, les courbes de perplexite sont donc confondues. On peut ob-
server que le modele de n / m-multigramme tend a avoir un comportement de bi-gramme de mots quand
le nombre de multigramme autorises diminue.

La perplexite d’un modele de langage augmente quand le nombre de parametres utilise baisse. Cela est
parfaitement normal, car le pouvoir de prediction d’un mot de la langue est moins important avec un
nombre de parametres inferieur. Un modele de n-gramme semble avoir une perplexite plus importante
qu’un modele de n + 1-gramme. Cependant (Bonafonte & Mariﬁo, 1996) rapporte que la perplexite
des n-gramme augmente a partir de n = 5. Un modele de tri-gramme avec un seuil de cut-oﬁ‘ a 2 a
une perplexite et un nombre de parametres plus faible qu’un modele de bi-gramme avec un seuil a 0 ;
le modele de tri-gramme est donc preferable dans ce cas. Selon (Rosenfeld, 2000), l’interet compare
d’un modele de langage apparait lorsque la mesure de perplexite baisse de plus de 10%. Le modele de
tri-gramme est donc notablement plus interessant que le modele de bi-gramme. Tout comme le modele
de quadri-gramme est plus interessant que le modele de tri-gramme.

Un modele de 71- gramme a horizon Variable, comparativement au n- gramme concurrent, a horizon ﬁxe,
obtient une perplexite9 plus faible. Cette baisse de la perplexite est due pour partie au calcul de la
probabilite maximum ; en effet, par construction, on obtient une probabilite au moins superieure a celle
determinee par le modele a horizon ﬁxe. Le gain obtenu par des n-gramme a horizon Variable provient
egalement de l’utilisation du coefﬁcient de back-of par le modele de n-gramme. En effet, le modele de
n-gramme utilise un coefﬁcient de back-of pour obtenir une probabilite de n-gramme a horizon ﬁxe a
partir de la probabilite du n — 1-gramme qui lui correspond si le n-gramme n’est pas trouve. Le modele
de n- gramme a horizon Variable permet de melanger les probabilites des differents (n — k)- gramme avec
m E [1, n — 2], et ce sans penaliser des (n — k)-gramme d’ordre inferieur.

Les n / m-multigramme se montrent moins performants que le modele de 71- gramme de meme ordre (c’est
a dire a nombre de mot consideres constants). En effet, l’equation 5 semble indiquer que le choix de la
meilleure probabilite se fait entre un bi- gramme de mots, un tri- gramme de mots, et un bi- gramme ayant
2 mots en tete (dans le cas ou la taille maximum d’un multigramme est de 2 mots, et la somme des mots
du bi-gramme est d’au plus 3). Le choix ne peut donc par construction qu’etre au moins aussi bon qu’un
tri-gramme de mots. Cependant, nous pouvons constater que pour obtenir un nombre de parametres
equivalent aﬁn de comparer les differents modeles, il faut interdire un nombre consequent de multi-
gramme parmi ceux disponibles. Nous devons alors chercher a ameliorer ce modele n / m-multigramme.

9Bien sﬁr, il ne s’agit pas exactement d’une mesure de perplexite qui devrait etre calculee a partir d’une distribution de
probabilité.

Evaluation des Modeles de Langage n-gramme et n / m-multigramme

V°°abU'ai'6 I 30 00° "1015 Vocabulaire : so 000 mots
240 — 250 _
A Zﬁgrammef 4* Zagrammef
+ Sgrammef _x_ 3_grammef
x 34_;ramme v X gﬂramme V
220 _ —-é-— 44_;ramme1 260 —A— 443.-ammef
A 44_;ramme v A 44_1,.amme V
5' 2/2*mU"i9|'3mm9 El 22—mu|tigramme

240 7
200 —

n
n
c
1
:{

Perplexité
§ §
1 1
Perplexité
l\)
o
o
1
1:{

180' \ :+

140'

 

120 — M A 140 _ x
A A AAAA A A
A
100 1 1 1 1 1 1 1 1 120 1 1 1 1 1 1 1 1
0 0.5 1 1.5 2 2.5 3 3.5 4 0.5 1 1.5 2 2.5 3 3.5 4 4.5
Nombre de paramétres X 106 Nombre de paramétres X 106
Vocabulaire 3 3 000 mots Vocabulaire : 30 000 mots
no - 240 -
* 2_g,.ammef * Zagrammef
+ 3—grammef _"— sﬂrammel
>< 3—gramme v ‘A’ 4*.?"am"_‘el
& 4_g1,amme1 E 2/2—mu|tIgramme
100 _ A 4,g,.amme V 229 — 9 2/2—mu|tigramme cut—oﬁ
El 2/2—"" '
90 _ 200 —

Perplexne
av
o
1
D
Perplexne
53
0
1

70'

   

60 7 X  LA 140 _
Am
A A A A
50 1 1 1 1 1 120 1 1 1 1 1 1 1
0 0.5 1 1.5 2 2.5 0 0.5 1 1.5 2 2.5 3 3.5
Nombre de taramétres X 106 Nombre de paramétres X 106

Figure 1: Comparaison de l’inﬂuence du nombre de parametres sur la perplexité des modeles de n-
grarnme a horizon ﬁxe (n-gramme-f) ou variable(n-gramme-v) pour n E [2, 4], et du 2/ 2-multigramme
pour différentes tailles de vocabulaire, et inﬂuence sur la perplexité de la méthode de cut-oﬁ‘ pour réduire
les parametres du modele de 2/ 2-multigramme avec un vocabulaire de 30 000 mots.

Pour améliorer la situation, on peut tout d’abord chercher a n’inclure dans les multigramme autorisés
que ceux qui apportent un gain vis a vis de l’équation5. Nous avons constaté par des experiences que ces
multigramme n’améliorent pas signiﬁcativement la perplexité (nous n’avons pas la place pour rapporter
ces expériences). Cela semble indiquer que les multigramme qui apportent le plus gros gain en terme de
perplexité sont déja inclus dans la liste des plus fréquents. Une expérience similaire consiste a déﬁnir la
liste des multigramme en changeant le seuil de cut-oﬁ‘. En effet, on peut observer une baisse signiﬁcative
du nombre de parametres, qui s’accompagne d’une augmentation de la perplexité (environ 10 points)
quand on passe d’un bi-gramme de mots avec un cut-of a 0 (respectivement 1) a un bi-gramme de mots
avec un cut-of a 1 (respectivement 2). La ﬁgure 1 montre l’évolution de la perplexité en conservant
les multigramme les plus fréquents (100 000 multigramme pour un vocabulaire de 30 000 mots). On
peut constater une baisse du nombre de parametres sans hausse de la perplexité ; cette solution semble
donc convenir. Enﬁn, étant donnée la baisse signiﬁcative de la perplexité observée avec peu de multi-
gramme entre un bi- gramme de mots avec un cut-of a 1, et un 2/ 2-multigramme avec le meme cut-oﬁ.
On peut souhaiter généraliser l’usage des multigramme aux n-gramme. Cependant la complexité risque
d’augmenter de maniere exponentielle avec n.

P. Alain, O. Boeffard

7 Conclusion

Cet article a présenté des résultats concernant des modeles de langage statistiques de type n-gramme a
horizon ﬁxe ou variable et des n/m-multigramme. A taux de mots hors-Vocabulaire ﬁxe, le comporte-
ment des n-gramme classiques fait baisser la perplexité pour des valeurs de n de 3 a 4, mais au prix d’ une
baisse du nombre de mots prédits (environ 7 millions pour un modele de bi-gramme, 6.8 millions pour
un tri-gramme, et un peu plus de 6.6 millions pour un quadri-gramme). Plus on reconnait des mots, plus
la probabilité conj ointe Va étre faible, on peut donc trouver discutable de comparer entre eux des modeles
de n-gramme qui ne se trouvent pas tout a fait sur le meme pied d’ égalité. Ce probleme ne se pose pas
pour les n-gramme a horizon variable, ou les n/m-multigramme, car le nombre de mots prédits est a
chaque fois celui du modele de bi- gramme. Les résultats de perplexité obtenus avec des Vocabulaires de
taille plus irnportante nous montrent a la fois une augmentation de la perplexité, et une augmentation du
nombre de parametres. Cette augmentation est due encore une fois a une augmentation du nombre de
mots prédits (pour un modele de bi-gramme, nous avons pres de 4.9 millions de mots prédits pour un
Vocabulaire de 3 000 mots, 7 millions pour 30 000 mots, et 7.3 millions pour 60 000 mots). Le taux de
mots hors Vocabulaire sur le corpus de test baisse de 19.38% pour 3 000 mots a 1.65% pour 60 000 mots.
Nous avons montré que le modele de multigramme le plus simple, un 2/2-multigramme (c’est-a-dire un
bi-gramme de sequences comprenant au plus deux mots) se comporte comme un modele situé entre un
bi-gramme et un tri-gramme classique. Notre objectif consiste a pousser un peu plus loin ces modeles en
augmentantnotarr1rnentl’ordre et en réglant le nombre de parametres par des techniques de cut-off.

Références

BIMBOT, F., PIERACCINI, R., LEVIN, E., & ATAL, B. 1995. Variable—Length Sequence Modeling: Multigrams.
IEEE Signal Processing Letters, 2(6), 111-113.

BONAFONTE, A., & MARINO, J. 1996. Language Modeling Using X—grams. Pages 394-397 of: Proceedings of
the International Conference on Spoken language Processing.

CHEN, S.F., & GOODMAN, J. 1999. An empirical study of smoothing techniques for language modeling. Com-
puter Speech and language, 13(4), 359-394.

DELIGNE, S., & BIMBOT, F. 1995. Language modeling by variable length sequences: theoretical formulation
and evaluation of multigrams. In: IEEE International Conference on Acoustics and Speech Signal Processing.

DELIGNE, S., & SAGISAKA, Y. 2000. Statistical language modeling with a c1ass—based n—multigram model.
Computer Speech and language, 14, 261-279.

KATZ, S.M. 1987. Estimation of Probailities from Sparse Data for the Language Model Component of a Speech
Recognizer. IEEE transactions on Acoustics, Speech and Signal Processing, 35, 400-401.

NIESLER, T.R., & WOODLAND, P.C. 1994. Variabl-length category n—gram language models. Computer Speech
and language, 13, 99-124.

ROSENFELD, R. 2000. Two decades of statistical language modeling: where do we go from here? Proceedings of
the IEEE, 88(8), 1270-1278.

SHIELDS, P.C. 1998. The Interactions Between Ergodic Theory and Information Theory. IEEE Transactions on
Information Theory, 44, 2079-2093.

SIU, M., & OSTENDORF, M. 2000. Variable n-grams and extensions for conversational speech language model-
ing. IEEE transactions on Speech and Audio Processing, 8(1), 63-75.

WOODLAND, P.C., & YOUNG, S.J. 1993. The HTK Continuous Speech Recogniser. Pages 2207-2219 of:
Proceedings of the Eurospeech conference.

ZITOUNI, I. 2002. A Hierarchical Language Model Based on Variable—Length Class Sequences: The M C’;
Approach. IEEE Transactions on Speech and Audio Processing, 10(3), 193-198.

