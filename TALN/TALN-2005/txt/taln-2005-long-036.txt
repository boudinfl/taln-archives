TALN 2005, Dourdan, 6-10 juin 2005

Evaluation des Modéles de Langage n-gramme et n/m-multigramme

P. Alain, O. Boeffard
IRISA — Université de Rennes 1 / ENSSAT
6, rue de Kerampont, 22305 Lannion
{pierre.alain,olivier.boeffard} @irisa.fr

Mots-clefs :

Modeles de Langage statistiques, n—gramme, multigramme, evaluation

Keywords: Statistical Language Models, n—grams, phrase multigrams

Résumé Cet article presente une evaluation de modeles statistiques du langage menee sur la langue
Frangaise. Nous avons cherche a comparer la performance de modeles de langage exotiques par rapport
aux modeles plus classiques de n—gramme a horizon ﬁxe. Les experiences realisees montrent que des
modeles de n— gramme a horizon variable peuvent faire baisser de plus de 10% en moyenne la perplexite
d’un modele de n—gramme a horizon ﬁxe. Les modeles de n/m—multigramme demandent une adaptation
pour pouvoir étre concurrentiels.

Abstract This paper presents an evaluation of statistical language models carried out on the French
language. We compared the performance of some exotic models to the one of the more traditional n-
gram model. The experiments show that the variable n—gram models can drop more than 10% of the
average perplexity for a ﬁxed n—gram model. n/m—multigram models require an adaptation to be able to
compete.

1 Introduction

La modelisation du langage est un probleme crucial et tres largement aborde en traitement automatique
de la langue ecrite ou parleel. A partir de l’observation de sequences de mots, il s’agit de construire
un modele dont l’objectif est de predire avec succes de nouvelles sequences. On peut distinguer deja
deux problemes, d’une part celui du choix du modele et de sa methodologie de construction et d’autre
part celui de la methodologie d’ evaluation d’un modele de langage. Concernant le premier point, on
peut distinguer des approches deterministes qui tiennent compte de l’organisation profonde des mots
liees notamment a la syntaxe, des approches probabilistes qui s’interessent essentiellement a la forme de
surface (Rosenfeld, 2000).

L’evaluation est un point relativement delicat dans la mesure ou elle peut étre dependante du modele
choisi. La mesure la plus communement adoptee consiste a calculer l’entropie croisee e11tre un modele
de langage et la distribution réelle des donnees observees, mais inconnue. En supposant que les donnees
suivent une distribution stationnaire et ergodique, le calcul de l’entropie—croisee peut étre estime a partir
d’un corpus sufﬁsamment grandz. La perplexite d’ un modele de langage n’est qu’une autre maniere de
representer le degre d’incertitude d’un modele et se calcule directement a partir de l’entropie—croisee du
modele sur un jeu de phrases de test. Pour un mot a predire, la valeur de la perplexite represente le

1On peut citer le domaine de la reconnaissance automatique de la parole mais aussi celui de la reconnaissance de texte
manuscrit ou encore celui de la traduction automatique.

2Théoréme de Shannon—MacMillan—Brieman, (Shields, 1998). En respectant ces hypotheses de stationnarité et d’ergodicité,
un corpus de parole de longueur ﬁnie peut reﬂéter la distribution réelle des données.

353

354

P. Alain, O. Boeffard

nombre d’ hypotheses moyennes de branchement3. Plus la perplexite est faible, plus le facteur moyen de
branchements d’un mot vers un autre est bas et plus le modele de langage est efﬁcace. Pour les modeles
n—gramme, un mot est predit en tenant compte d’un historique relativement limite des mots qui le prece-
dent. Ces modeles connaissent ﬁnalement tres peu des raisons profondes de l’organisation des mots
dans une phrase. En revanche, l’utilisation de probabilites conditionnelles et un apprentissage realise
a partir de quelques millions de phrases permettent d’obtenir de bonnes performances. Leur principal
defaut reside dans la complexite spatiale sous—jacente. Theoriquement, plus la sequence de l’historique
du modele s’allonge (n augmente), plus le modele repartit efﬁcacement la masse de probabilites sur des
mots qui reviennent souvent apres une valeur particuliere de l’historique. Cependant, plus n augmente,
plus les observations se rareﬁent compte—tenu de la nature hyperbolique de la distribution de ces evene-
ments4. Pour des situations experimentales reelles, les valeurs courantes de n depassent rarement 4 (Siu
& Ostendorf, 2000). De nombreuses solutions ont ete apportees au probleme de l’explosion combina-
toire eta celui de la rarefaction des evenements (Rosenfeld, 2000). Des techniques de lissage permettent
notamment de repondre a la difﬁculte de l’estimation d’une distribution de probabilite lorsque les evene-
ments sont rares. On peut citer le principe du lissage qui n’effectue l’estimation des points de la densite
au sens du maximum de vraisemblance que pour des evenements dont l’occurrence est superieure a un
seuil de cut—0ﬁ‘. Une partie de la masse de probabilite est repartie sur des evenements dont l’occurrence
est inferieure au seuil, (Katz, 1987). (Chen & Goodman, 1999) propose une evaluation des principales
techniques de lissage les plus utilisees.

Les modeles de n—gramme pour lesquels la longueur de l’historique est variable sont une alternative
aux n—gramme classiques pour lesquels la longueur de l’historique reste ﬁxe. Le principe consiste a ne
pas retenir un historique de longueur n si la contribution du n—gramme correspondant n’ameliore pas la
performance du modele. Toute la difﬁculte reside dans la decision d’abandon d’un n—gramme pour un
(n — k)—gramme avec 1 3 k < n, (Niesler & Woodland, l994)(Siu & Ostendorf, 2000).

Les modeles multigramme sont des modeles de type n—gramme ou la tete peut avoir une longueur
superieure a 1.(Bimbot et al. , 1995)(Deligne & Bimbot, 1995) presentent un cadre theorique pour
des multigramme formes sur des modeles d’uni—gramme (longueur d’historique nulle). Les experi-
ences rapportees concernent une application avec un vocabulaire limite de 900 mots pour un corpus
d’apprentissage de 100 000 phrases et un corpus de test de 1 000 phrases (dont 52 occurrences de mots
hors—vocabulaire). Les modeles de type multigramme obtiennent une perplexite meilleure que les n-
gramme classiques lorsque n > 3. Compte—tenu de la taille relativement limitee des corpus, les conclu-
sions sont difﬁcilement transposables directement sur des corpus plus importants. (Deligne & Sagisaka,
2000) se place dans un contexte de multigramme de classes de mots sur des modeles de bi—gramme.
Les experiences sont menees avec un vocabulaire d’ environ 3 000 mots, 100 000 phrases pour le corpus
d’apprentissage et environ 700 phrases pour le test. Deux types de mesure sont rapportes : d’une part
la perplexite pour les modeles de type multigramme et d’autre part le taux d’erreur d’un systeme de re-
connaissance de la parole. Les resultats entre multigramme et n—gramme classiques (bi— et tri—gramme)
sont difﬁcilement comparables. En effet, pour ces demiers, les valeurs de perplexite sont absentes et les
modeles de n—gramme semblent avoir ete non reduits5. (Zitouni, 2002) propose des modeles de multi-
gramme ou les probabilites de co—occurrence de mots sont conditionnees par rapport a des classes. Les
experiences concernent deux annees du journal "Le Monde" (annees 1987 et 1988) pour un vocabulaire
de 20 000 mots. L’utilisation de ces multigramme permet de réduire de 7% la perplexite des tri—gramme
classiques. Encore une fois, il est difﬁcile de retrouver sur cette experience une comparaison entre mod-
eles a nombre de parametres constant.

Cet article propose une etude experimentale sur les performances relatives des modeles de language

311 s’agit d’une moyenne géométrique.

411 s’agit de distributions a queue lourde ou beaucoup d’événements sont extrémement rares et peu sont trés fréquents. La
loi de Zipf est un cas particulier de lois puissances caractéristiques de ce phénoméne.

5Le comportement d’un n—gramme est non—linéaire, il est possible de réduire de fagon importante le nombre de paramétres
sans trop dégrader ses performances.

Evaluation des Modeles de Langage n—gramme et n/m—multigramme

de type n—gramme a horizon ﬁxe, a horizon variable et multigramme. La section 2 presente un cadre
theorique pour ces trois types de modeles de langage statistiques. La section 3 expose la problematique
d’une telle experimentation ainsi que nos hypotheses de travail. Une evaluation a ete menee sur environ
un million de phrases en francais. La section 4 decrit la methodologie suivie. La section 5 expose les ex-
periences mises en oeuvre. Enﬁn, la section 6 presente les resultats et une interpretation du comportement
des modeles en fonction des donnees traitees.

2 Cadre théorique

Un modele de langage statistique est un ensemble de distributions de probabilite sur des sequences
observees de symboles. Comme, en pratique, il est impossible de caracteriser de telles distributions,
les modeles de langage se distingueront e11tre eux par les hypotheses choisies pour reduire la complexite
combinatoire et ameliorer leur capacite de generalisation. Apres une presentation des notations utilisees,
nous discutons du modele de n—gramme a horizon ﬁxe, du modele de n—gramme a horizon variable et
enﬁn du modele n/m—multigramme.

Soit une sequence de mots W : (w1, w2, ..., wN) avec wi une variable representant le mot de rang 1' dans
la sequence W. Les valeurs possibles pour wi appartiennent a un vocabulaire V. I1 peut s’agit souvent
d’un vocabulaire ferme dans le cadre de systemes de dialogue, nous considerons ici l’etude de la langue
naturelle, nous choisissons un vocabulaire ouvert. Nous pouvons decrire cette sequence par une suite de
variables aleatoires w,-. La probabilite conjointe des variables de la sequence W peut se developper de la
maniere suivante en faisant apparaitre des probabilites conditionnelles :

N
MW) : W01) >< HP(wz'|w1» ---wz‘—1) (1)
i:2
L’objectif d’un modele de langage consiste a calculer cette probabilite conjointe, c’est—a—dire a estimer
des valeurs pour chacune des probabilites conditionnelles. L’estimation de ces probabilites condition-
nelles est en pratique impossible car le nombre de parametres croit de maniere exponentielle avec la
longueur de la suite de mots. Pour contrer cette difﬁculte, un modele de langage pose une probabilite
conditionnelle approchee  en simpliﬁant la loi conjointe, equation 1.

On note 9 l’ensemble des groupes de mots formes sur le vocabulaire V. On note 8 l’ensemble des
sequences formees sur les elements de 9. On note 8* C 8, l’ensemble des sequences de 8 qui corre-
spondent a W. On note 8 une sequence particuliere de 8*. Par exemple, pour W : (w1, w2, w3), on
a :

On note |S | le nombre de groupes de mots dans la sequence 8. Soit k le groupe de mots de rang k dans
la sequence 8, on note i(S l’indice dans la sequence W du premier mot de 8 On note l(S
le nombre de mots de 8

On note hi’j(W) la chaine des variables aleatoires representant l’apparition conjointe de tous les mots
wu de Wpouru E [i,i+ (j — 

11),‘, ’U),‘+1, ..., w1~+(j_1) Sl ’i +  — 1) g N
w,~,w,~+1, ...,wN sinon

hz',j(W) : {

On deﬁnit egalement l’operateur ti,j(W) qui represente les j mots precedents le mot w,-. On a donc
ti’j(W) : hi_j’j(W). ti’j(W) correspond a un horizon ou historique (un groupe de mots qui precede

355

356

P. Alain, O. Boeffard

l’observation d’un mot). hi,j(W) correspond a la tete d’un parametre du modele de langage (pour les
modeles n—gramme a horizon ﬁxe ou variable, la variable aleatoire de tete est degeneree, et ne contient
qu’un seul mot). Les modeles de langage cherchent d’une part a reduire au maximum la longueur d’un
horizon (minimisation du nombre de parametres) et d’autre part, pour un horizon donne, a estimer la
distribution de probabilite des historiques pour calculer la probabilite d’apparition du mot w,~. Soit W
associee a une sequence de decoupage 8, la loi conjointe estimee par le modele de langage peut alors se
reecrire sous la forme suivante avec n l’ordre du n—gramme :

ISI

l9§(W) : p(hi(S(1)),l(S(1))(W)) >< Hp(hi(S(k)),l(S(k))(W)|ti(S(k)),n—1(W)) (2)
19:2

2.1 Les modéles n-gramme a horizon ﬁxe

Pour un n—gramme a horizon ﬁxe, on fait une hypothese d’ independance conditionnelle du mot w, avec
les mots presents dans la sequence a une distance de plus de n — l mots (pour n : 2, ce modele est un
modele de bi—gramme ; la probabilite P(W) correspond a celle d’une chaine de Markov. Pour n : 3, on
parle de tri—gramme et pour n : 4 de quadri—gramme). Comme nous l’avons deja souligne, ce modele est
tres simple, mais le nombre de parametres croit de maniere exponentielle avec n. Pour cette raison, les
modeles de n—gramme les plus utilises le sont pour des valeurs de n de l’ordre de 3 ou 4. Pour corriger
le probleme des evenements rares, il existe des techniques de lissage des probabilites conditionnelles,
couplees a des techniques de back—0ﬁ‘ permettant de corriger celles d’evenements manquants lors de
l’apprentissage, (Katz, 1987). Cette correction s’effectue en ponderant la probabilite du (n — l)—gramme
par un coefﬁcient de back—0ﬁ‘ de telle maniere que la distribution de probabilite des n—gramme somme
toujours a 1. Le terme produit de l’equation 2 se simpliﬁe alors :

p(hi(S(k)),l(S(k))(W)|ti(S(k)),n—1(W)) é l9(hz',1(W)|75z',n—1(W)) (3)
Pour ce modele, S : W, on obtient simplement :
pML(W) : p§(W)

2.2 Les n-gramme a horizon variable

Forcer l’estimation du terme produit de l’equation 2 a un historique de longueur n introduit un double
biais. D’une part les occurrences sont plus faibles, on a donc tendance a faire du lissage eta etre moins
precis. D’autre part, on introduit des distributions conditionnelles sur w, qui ne servent pas a grand
chose (augmentation injustiﬁee du nombre de parametres). Autoriser une variation de la longueur de
l’historique pour predire w, permet de regler ce probleme de sur—apprentissage. Les n— gramme a horizon
variable deﬁnissent une probabilite en adaptant une longueur d’historique optimale en fonction de 11),.
L’approche traditionnelle pour ce type de modeles consiste a determiner au moment de l’apprentissage
les longueurs optimales a retenir, (Bonafonte & Mariﬁo, l996)(Siu & Ostendorf, 2000). Dans cette
situation un n—gramme est remplace par un (n — k)—gramme avec 1 3 k < n. Les n—gramme a horizon
variable peuvent apparaitre interessants pour un double enjeu : d’une part a nombre de parametres ﬁxe,
il peuvent repondre a une amelioration de la performance des n—gramme a horizon ﬁxe et d’autre part, a
perplexite ﬁxee, ils peuvent etre utiles a la diminution du nombre de parametres d’un modele de langage.

Au moment du test, lors du calcul de la perplexite d’une phrase, pour ce modele de n— gramme a horizon
variable, le terme produit de l’equation 2 s’ecrit :

max {l9(hi,1(W)|75z',v(W))}

1§v§n—1

(4)

l l

p(hi(S(k)),l(S(k)) (W) |ti(S(k)),n—1(W))

Evaluation des Modeles de Langage n—gramme et n/m—multigramme

Cette ecriture signiﬁe que pour chaque mot w, a predire, on cherche a maximiser la probabilite en se
basant sur des modeles allant du bi—gramme au n—gramme (equation 3). Pour ce modele, S : W, on
obtient simplement :

pML(W) : l9§(W)

2.3 Les n/m-multigram

Un n/m—multigramme correspond a une probabilite conditionnelle ou la tete du n—gramme peut étre plus
longue qu’un mot unique. m represente le nombre maximum de mots dans un groupe de mots en téte.
Lors du test du modele de langage, pour une decoupe S donnee, nous cherchons la meilleure probabilite
suivant l’equation :

A
: max {p(hi,u(W) |ti,v(W))} (5)

1§u§m,1§v§m><(n—1)

p(hi(S(k)),l(S(k))(W) |ti(S(k)),n—1(W))

Il sufﬁt ensuite de prendre la meilleure solution sur toutes les sequences 8 E 8* :

pML(W) : argmaX{p§(W)}
Ses*

3 Problématique et hypotheses méthodologiques

Notre objectif est de veriﬁer l’interét des modeles n—gramme a horizon variable par rapport a des mod-
eles a horizon ﬁxe et a des modeles de type n/m—multigramme. La difﬁculte de mise en oeuvre d’une
telle evaluation reside dans le probleme du controle explicite des parametres lors de la construction des
modeles. Differents facteurs sont responsables de la qualite d’un modele de langage. Certains inﬂuent
directement le processus d’apprentissage alors que d’autres determinent la mesure de performance d’un
modele.

Tout d’abord l’estimation des probabilites conditionnelles provient directement de la detection de n-
uplets. Avec peu de sequences, on défavorise notamment les modeles de n—gramme d’ordre superieur.
Le nombre de parametres d’un modele de langage de type n—gramme est proportionnel a V|”. Le cut-
oﬁ‘ est une technique simple et relativement efﬁcace pour limiter le nombre de parametres (Chen &
Goodman, 1999). Il s’agit de ne pas retenir les n—uplets qui apparaissent sous un seuil d’occurrence.
Ainsi, un cut—0ﬁ‘ a 1 signiﬁe qu’un mot doit apparaitre au moins 2 fois pour étre integre au modele de
langage. Cependant, compte—tenu de la forme des distributions de probabilite (fonction puissance), la
reduction consequente du nombre de parametres n’est pas lineaire en fonction de la valeur de cut—0ﬁ‘.
En introduction, nous avons souligne le role de la perplexite comme outil de mesure de la qualite d’un
modeles de langage.

Un autre facteur cle que l’on doit maintenir entre les differents modeles de langage pour pouvoir com-
parer les valeurs de perplexite est le nombre de mots hors—vocabulaire. Plus la taille du vocabulaire est
faible (et donc plus le nombre de parametres est faible), plus le taux des mots hors—vocabulaire augmente
avec des valeurs de perplexite qui s’ameliorent. Il s’agit d’un facteur calcule a posteriori, une fois le
modele construit. Il est donc difﬁcile d’ intervenir explicitement sur cette valeur.

Le calcul de la perplexite peut varier notamment par la prise en compte ou non des mots hors—vocabulaire
sur l’ensemble de test. On peut decider de ne pas predire un mot hors vocabulaire ; dans ce cas
l’accumulation de la perplexite est plus faible mais le nombre de mot predit n’augmente pas. Le cal-
cul de la perplexite fait intervenir une hypothese de stationnarite et d’ergodicite qu’il faudrait veri-
ﬁer en pratique. La performance d’un modele de langage depend donc etroitement du couple ensem-
ble d’apprentissage/ensemble de test. Il faut que ces ensembles contiennent un nombre sufﬁsant de

357

358

P. Alain, O. Boeffard

sequences pour pouvoir conclure a des resultats stables. Au cours de nos experiences, nous avons essaye
de minimiser l’inﬂuence de chacun de ces facteurs de maniere a favoriser la comparaison entre structures
de modeles (n—gramme a horizon ﬁxe, n—gramme a horizon variable et n/m—multigramme).

Nous avons considere les hypotheses methodologiques suivantes. Une annee du journal "Le Monde" a
ete choisie comme univers linguistique (annee 1997). Apres extraction des phrases et tirage aleatoire, ce
corpus est reparti en deux sous—corpus : 70% pour le corpus d’ apprentissage et 30% pour le corpus de test.
Le choix d’un corpus ﬁxe est sufﬁsant pour valider une comparaison entre modeles, mais ne permettra
pas de conclure sur la robustesse des resultats. Des analyses complementaires seront donc necessaires.
Nous avons considere trois ensembles de mots : un premier vocabulaire a 3 000 mots, un deuxieme
a 30 000 mots et un dernier a 60 000 mots (il s’agit a chaque fois des plus frequents sur l’ensemble
d’apprentissage). Les valeurs de perplexite et les taux de mots hors vocabulaire dependent directement
de ces trois ensembles. Nous avons cherche a controler explicitement le nombre de parametres de nos
modeles. Deux approches complementaires ont ete mises en oeuvre : d’une part par application de seuils
de coupure sur les differents types de n—gramme et d’autre part par la conservation des co—occurrences
de m—uplets de mots les plus frequentes pour les n/m—multigramme. Dans le premier cas, nous balayons
un spectre de valeurs de cut—0ﬁ‘ et nous observons a posteriori le nombre de parametres. Ce nombre nous
sert ensuite a ajuster le nombre de multigramme autorises a entrer dans le vocabulaire et se placer ainsi
a nombre de parametres constant (avec une tolerance de 1%). La perplexite calculee ne tient pas compte
des mots hors vocabulaire qu’ils soient presents dans la tete ou dans l’historique d’un n—gramme.

Notre systeme de reference est celui des n—gramme classiques (que nous avons nomme n—gramme a
horizon ﬁxe). Nous avons choisi des valeurs communement admises pour n et introduit des modeles de
bi—, tri— et quadri—gramme. L’estimation de ces modeles utilise le lissage des probabilite de Good—Turing,
selon les recommandations classiques de lissage, discounting, et back—0ﬁ‘ (Chen & Goodman, 1999).
Nous cherchons tout d’abord a comparer les n—gramme a horizon ﬁxe avec des n—gramme a horizon
variable. Les n—gramme a horizon variable sont mis en oeuvre lors du test, en appliquant l’equation 4.
Notre obj ectif n’est pas de valider une technique de reduction de parametres au moment de la construction
du modele, (Siu & Ostendorf, 2000)(Niesler & Woodland, 1994), mais plutot de debrider un modele de
n—gramme a horizon ﬁxe pour en faire un modele de n—gramme a horizon variable. Notre maniere
de proceder introduit un coﬁt de calcul supplementaire, mais il reste acceptable car les longueurs des
historiques sont faibles devant le nombre de mots a traiter.

Nous cherchons enﬁn a situer les modeles n/m—multigramme par rapport aux deux approches prece-
dentes. L’ interet du multigramme reside dans sa capacite a predire une sequence de mots avec un seul
parametre. En moyenne on baisse le nombre de termes impliques dans le calcul de la perplexite ; il s’agit
alors d’une situation favorable. Cependant, le risque est de repartir une masse de probabilites sur plus de
termes6. Pour que la competition entre modeles reste equitable, nous avons choisi de travailler avec des
modeles n/m—multigramme dont la taille maximale (en nombre de mots) est soumise a une contrainte.

4 Estimation des paramétres des modéles

Les experiences sont realisees a partir de la suite de programme HT K (Woodland & Young, 1993).
Cet ensemble de librairies et d’outils correspond a une chaine complete permettant de construire et
de tester un modele de langage. La gestion des n—gramme a horizon variable n’est pas ecrite dans
la distribution standard de HT K. La modiﬁcation du programme de test du modele de langage a ete
necessaire pour introduire le traitement propose equation 4. La gestion des n/m—multigramme n’est pas
non plus ecrite. Les modiﬁcations a faire sont d’une part dans le programme d’apprentissage, aﬁn de

6Un n—gramme classique estime, pour chaque historique, une densité de probabilité dont la complexité spatiale est celle
du vocabulaire. Les multigramme avec des tétes de longueur au plus m ont une complexité spatiale bornée par |V|m, les
probabilités tendent vers 0.

Evaluation des Modeles de Langage n—gramme et n/m—multigramme

parcourir systematiquement toutes les unites de multigramme possibles Il est egalement necessaire de
modiﬁer, comme pour les n—gramme a horizon variable, le programme de test, pour pouvoir parcourir
toutes les tetes et tous leurs historiques possibles.

Pour les n—gramme a horizon ﬁxe, la perplexite du modele de langage est determinee directement grace a
l’equation 6. Si le mot de tete du n—gramme n’est pas dans le vocabulaire selectionne, il est alors compte
comme mot hors vocabulaire.

PP : 2H* avec (6)

1
H* : —R logg (P(w1, wg, . . . ,wm))

Pour les n—gramme a horizon variable, la situation est differente : pour chaque mot plusieurs choix sont
possibles (le choix se fait entre un 2—gramme, un 3—gramme, ..., ou un n—gramme). Il sufﬁt de choisir le
meilleur k—gramme parmi les n — 1 possibles (choix parmi toutes les longueurs d’historique autorisees).
Cet algorithme est applique phrase par phrase (hypothese d’ independance des phrases e11tre elles). En ﬁn
de traitement d’une phrase on connait la perplexite evaluee sur cette phrase, le nombre de mots predits
ainsi que le nombre de mots hors vocabulaire.

Pour les n / m—multi gramme, la situation est encore differente. Maintenant plusieurs tetes sont disponibles,
et pour chacune d’elles, plusieurs choix sont possibles. Nous avons volontairement limite la taille max-
imale du n/m—multigramme a un nombre ﬁxe de mots : avec des multigramme de taille au plus 2, nous

pourrions former4bi—gramme: P(w,~|w,~_1), P(w,~| [w,~_2 w,_1]), P([w,~ w,+1] |w,_1), P([w,~ w,+1] | [w,~_2 w,_1]).

En limitant le nombre maximum de mots dans le n/m—multigramme, nous pouvons choisir le modele
de langage avec lequel nous entrons en concurrence. Par exemple, avec des multigramme de taille au
plus 2, et une somme a 3, nous n’avons plus que 3 choix possibles : P(w,~|w,~_1), P(w,~| [w,~_2 w,~_1]), et
P([w,- w,~+1] |w,~_1). Dans le programme de test, aﬁn de selectionner la meilleure decoupe de la phrase
selon le max de l’equation 5, nous avons mis en place une recherche du meilleur chemin dans un graphe7
oriente et value selon l’algorithme de Dijkstra.

5 Méthodologie expérimentale

Les experiences sont realisees sur un corpus de texte du francais : tous les articles parus pendant l’annee
1997 dans le journal "Le Monde" (ressource ELILA). Ce corpus est decoupe en phrases par un logiciel
d’analyse syntaxique (logiciel Cordial de Synapse). Les phrases sont uniformisees par une reecriture
systematique en majuscules et la suppression de toute ponctuation. Le corpus ainsi obtenu contient
1 131 135 phrases pour un vocabulaire de 219 034 mots. Il s’agit de la taille exacte du vocabulaire (mots
variants en genre et en nombre, ainsi que les verbes rencontres sous une forme conjuguee), le nombre
d’occurrence des mots est de 23 999 626. L’apprentissage se fait sur 70% du corpus, le test est realise sur
les 30% restant. La repartition des phrases a ete realisee de maniere aleatoire a partir du corpus d’ origine.

Pour faire baisser le nombre de parametres d’un modele de taille n, on fait evoluer la valeur de cut—0ﬁ‘ sur
des n—gramme a horizon ﬁxe. On conserve une valeur de cut—0ﬁ" a 1 sur les parametres d’ordre inferieur
(horizon de longueur inferieure a n). Ainsi, pour faire baisser le nombre de parametres d’un modele
de tri—gramme, on Va augmenter la valeur du cut—0ﬁ‘ sur les probabilites conditionnelles des tri—gramme,
et laisser constante la valeur de cut—0ﬁ‘ pour les probabilites de bi—gramme et d’uni—gramme. Pour les
2/ 2—multigramme, on peut faire baisser le nombre de parametres en limitant le nombre de multigramme
autorises dans le modele de langageg. On peut ainsi ﬁxer le nombre de parametres du modele n/m-

7L’algorithme de Viterbi permet de rechercher la meilleure solution a priori, nous lui préférons l’algorithme de Dijkstra qui
perrnet d’obtenir la meilleure solution a posteriori.

savec un nombre de multigramme a 0, on obtient un modéle de bi—gramme ; cela est Visible sur la ﬁgure 1 en prolongeant la
courbe de perplexité des n/m—multigramme.

359

360

P. Alain, O. Boeffard

multigramme de fagon precise grace a un algorithme de dichotomie qui selectionne le bon nombre de
multigramme a prendre en compte dans la suite.

Si au moins un des mots de l’historique n’est pas present dans le vocabulaire alors le modele de langage
declare ne pas pouvoir predire le n—gramme. Le mot de tete du n—gramme est alors declare non predit,
et la perplexite n’evolue pas. Dans la situation ou tous les mots sont presents dans le vocabulaire, mais
ou la probabilite du n—gramme n’a pas ete apprise par le modele de langage, le systeme de back—0ﬁ‘ deja
presente s’applique. Dans le cas des n—gramme a horizon variable, la probabilite est evaluee de maniere
identique, le mot en tete du n—gramme est declare non predit si au moins un mot de son horizon est hors
vocabulaire. Si tous les mots de l’horizon sont dans le vocabulaire, le choix de la meilleure probabilite
est realise selon l’equation 4. Pour les multigramme, l’algorithme de Dijkstra permet de determiner la
meilleure solution au sens de l’equation 5, parmi toutes les solutions possibles.

6 Résultats et commentaires

La ﬁgure 1 presente l’evolution de la perplexite en fonction du nombre de parametres des differents mod-
eles pour differentes tailles de vocabulaire. Le modele de bi— gramme a horizon variable est exactement
le modele de bi—gramme a horizon ﬁxe, les courbes de perplexite sont donc confondues. On peut ob-
server que le modele de n/m—multigramme tend a avoir un comportement de bi—gramme de mots quand
le nombre de multigramme autorises diminue.

La perplexite d’un modele de langage augmente quand le nombre de parametres utilise baisse. Cela est
parfaitement normal, car le pouvoir de prediction d’un mot de la langue est moins important avec un
nombre de parametres inferieur. Un modele de n—gramme semble avoir une perplexite plus importante
qu’un modele de n + l—gramme. Cependant (Bonafonte & Mariﬁo, 1996) rapporte que la perplexite
des n—gramme augmente a partir de n : 5. Un modele de tri—gramme avec un seuil de cut—0ﬁ‘ a 2 a
une perplexite et un nombre de parametres plus faible qu’un modele de bi—gramme avec un seuil a 0 ;
le modele de tri—gramme est donc preferable dans ce cas. Selon (Rosenfeld, 2000), l’interet compare
d’un modele de langage apparait lorsque la mesure de perplexite baisse de plus de 10%. Le modele de
tri—gramme est donc notablement plus interessant que le modele de bi—gramme. Tout comme le modele
de quadri—gramme est plus interessant que le modele de tri—gramme.

Un modele de n— gramme a horizon variable, comparativement au n— gramme concurrent, a horizon ﬁxe,
obtient une perplexiteg plus faible. Cette baisse de la perplexite est due pour partie au calcul de la
probabilite maximum ; en effet, par construction, on obtient une probabilite au moins superieure a celle
determinee par le modele a horizon ﬁxe. Le gain obtenu par des n—gramme a horizon variable provient
egalement de l’utilisation du coefﬁcient de back—0ﬁ” par le modele de n—gramme. En effet, le modele de
n—gramme utilise un coefﬁcient de back—0ﬁ‘ pour obtenir une probabilite de n—gramme a horizon ﬁxe a
partir de la probabilite du n — l—gramme qui lui correspond si le n—gramme n’est pas trouve. Le modele
de n—gramme a horizon variable permet de melanger les probabilites des differents (n — k)—gramme avec
m E [1, n — 2], et ce sans penaliser des (n — k)—gramme d’ordre inferieur.

Les n/m—multigramme se montrent moins performants que le modele de n—gramme de meme ordre (c’est
a dire a nombre de mot consideres constants). En effet, l’equation 5 semble indiquer que le choix de la
meilleure probabilite se fait entre un bi—gramme de mots, un tri—gramme de mots, et un bi—gramme ayant
2 mots en tete (dans le cas ou la taille maximum d’un multigramme est de 2 mots, et la somme des mots
du bi—gramme est d’au plus 3). Le choix ne peut donc par construction qu’etre au moins aussi bon qu’un
tri—gramme de mots. Cependant, nous pouvons constater que pour obtenir un nombre de parametres
equivalent aﬁn de comparer les differents modeles, il faut interdire un nombre consequent de multi-
gramme parmi ceux disponibles. Nous devons alors chercher a ameliorer ce modele n/m—multigramme.

9Bien sﬁr, il ne s’agit pas exactement d’une mesure de perplexité qui devrait étre calculée a partir d’une distribution de
probabilité.

Evaluation des Modeles de Langage n—gramme et n/m—multigramme

V°°ab”'ai’9 5 3° 000 ’"°t5 Vocabulaire : 60 000 mots

240 — 280 ,

+ 2—grammeI

+ 3—grammel

x 3—gramme v
220 _ —-é-— 4—grammeI 260 .3. 4,grammef
A 4'9’a’"""9 V A 44_;ramme v

El 2/2—mu|ti ramme E. 2/2_mu|1igramme

+ 24_;rammef
—><— Sgrammef
X Sgrammev

200 7

Perplexité
Eu‘ 6:?
o o

1 1

140*

 

120- x

100 1 1 1 1 1 1 1 1 120 1 1 1 1 1 1 1 1
0 0.5 1 1.5 2 2.5 3 3.5 4 0.5 1 1.5 2 2.5 3 3.5 4 4.5

Nombre de paramétres X 105 Nombre de paramétres X 106

Vocabulaire :3 000 mots V°°ab”'ai’9 5 3° 000 ’"°5

+ 2—grammef * 2'9’a’"’"9'
—x— 3—grammef _"_ 3‘9’a’"'"9'
x 3—grammev "A" 4‘9’a’"’'_‘9'
\\ & 4_grammef El 2/2—muIt|gramme
100 _ A 4_g,.amme V 220 — G 2/2—mu|tigramme cut—oﬂ
E. 0') -

90 _ zoo —

Perplexlte
on
o
1
u
Perplexlte
6:?
o
1

70*

 

‘EP~~1:+——1:1
60— 140-

X 

AAAAA A

 

A

50 1 1 1 1 1 120 1 1 1 1 1 1 1
0 0.5 1 1.5 2 2.5 0 0.5 1 1.5 2 2.5 3 3.5 4
Nombre de paramétres X 106 Nombre de paramétres

Figure l: Comparaison de l’inﬂuence du nombre de parametres sur la perplexite des modeles de n-
gramme a horizon ﬁxe (n—gramme—i) ou variable(n—gramme—v) pour n E [2, 4], et du 2/ 2—multigramme
pour differentes tailles de vocabulaire, et inﬂuence sur la perplexite de la methode de cut—0ﬁ‘ pour reduire
les parametres du modele de 2/ 2—multigramme avec un vocabulaire de 30 000 mots.

Pour ameliorer la situation, on peut tout d’abord chercher a n’inclure dans les multigramme autorises
que ceux qui apportent un gain vis a vis de l’equation5. Nous avons constate par des experiences que ces
multigramme n’ameliorent pas signiﬁcativement la perplexite (nous n’avons pas la place pour rapporter
ces experiences). Cela semble indiquer que les multigramme qui apportent le plus gros gain en terme de
perplexite sont deja inclus dans la liste des plus frequents. Une experience similaire consiste a deﬁnir la
liste des multigramme en changeant le seuil de cut—0ﬁ‘. En effet, on peut observer une baisse signiﬁcative
du nombre de parametres, qui s’accompagne d’une augmentation de la perplexite (environ 10 points)
quand on passe d’un bi—gramme de mots avec un cut—0ﬁ" a 0 (respectivement 1) a un bi—gramme de mots
avec un cut—0ﬁ‘ a 1 (respectivement 2). La ﬁgure 1 montre l’evolution de la perplexite en conservant
les multigramme les plus frequents (100 000 multigramme pour un vocabulaire de 30 000 mots). On
peut constater une baisse du nombre de parametres sans hausse de la perplexite ; cette solution semble
donc convenir. Enﬁn, etant donnee la baisse signiﬁcative de la perplexite observee avec peu de multi-
gramme entre un bi— gramme de mots avec un cut—0ﬁ‘ a 1, et un 2/ 2—multigramme avec le meme cut—0ﬁ‘.
On peut souhaiter generaliser l’usage des multigramme aux n—gramme. Cependant la complexite risque
d’augmenter de maniere exponentielle avec n.

361

362

P. Alain, O. Boeffard

7 Conclusion

Cet article a presente des resultats concemant des modeles de langage statistiques de type n—gramme a
horizon ﬁxe ou variable et des rt/m—multigramme. A taux de mots hors—vocabulaire ﬁxe, le comporte-
ment des n—gramme classiques fait baisser la perplexite pour des valeurs de rt de 3 a 4, mais au prix d’ une
baisse du nombre de mots predits (environ 7 millions pour un modele de bi—gramme, 6.8 millions pour
un tri—gramme, et un peu plus de 6.6 millions pour un quadri—gramme). Plus on reconnait des mots, plus
la probabilite conjointe va etre faible, on peut donc trouver discutable de comparer e11tre eux des modeles
de n—gramme qui ne se trouvent pas tout a fait sur le meme pied d’egalite. Ce probleme ne se pose pas
pour les n—gramme a horizon variable, ou les rt/m—multigramme, car le nombre de mots predits est a
chaque fois celui du modele de bi— gramme. Les resultats de perplexite obtenus avec des vocabulaires de
taille plus importante nous montrent a la fois une augmentation de la perplexite, et une augmentation du
nombre de parametres. Cette augmentation est due encore une fois a une augmentation du nombre de
mots predits (pour un modele de bi—gramme, nous avons pres de 4.9 millions de mots predits pour un
vocabulaire de 3 000 mots, 7 millions pour 30 000 mots, et 7.3 millions pour 60 000 mots). Le taux de
mots hors vocabulaire sur le corpus de test baisse de 19.38% pour 3 000 mots a 1.65% pour 60 000 mots.
Nous avons montre que le modele de multigramme le plus simple, un 2/2—multigramme (c’est—a—dire un
bi—gramme de sequences comprenant au plus deux mots) se comporte comme un modele situe entre un
bi—gramme et un tri—gramme classique. Notre objectif consiste a pousser un peu plus loin ces modeles en
augmentant notamment l’ordre et en reglant le nombre de parametres par des techniques de cut—off.

Références

BIMB OT, F., PIERACCINI, R., LEVIN, E., & ATAL, B. 1995. Variable—Length Sequence Modeling: Multigrams.
IEEE Signal Processing Letters, 2(6), 111-113.

BONAEONTE, A., & MARINO, J. 1996. Language Modeling Using X—grams. Pages 394-397 of.‘ Proceedings of
the International Conference on Spoken Language Processing.

CHEN, S.F., & GOODMAN, J. 1999. An empirical study of smoothing techniques for language modeling. Com-
puter Speech and Language, 13(4), 359-394.

DELIGNE, S., & BIMB OT, F. 1995. Language modeling by variable length sequences: theoretical formulation
and evaluation of multigrams. In.‘ IEEE International Conference on Acoustics and Speech Signal Processing.

DELIGNE, S., & SAGISAKA, Y. 2000. Statistical language modeling with a class—based n—multigram model.
Computer Speech and Language, 14, 261-279.

KATZ, S .M. 1987. Estimation of Probailities from Sparse Data for the Language Model Component of a Speech
Recognizer. IEEE transactions on Acoustics, Speech and Signal Processing, 35, 400-401.

NIESLER, T.R., & WOODLAND, P.C. 1994. Variabl—length category n—gram language models. Computer Speech
and Language, 13, 99-124.

ROSENEELD, R. 2000. Two decades of statistical language modeling: where do we go from here? Proceedings of
the IEEE, 88(8), 1270-1278.

SHIELDS, P.C. 1998. The Interactions Between Ergodic Theory and Information Theory. IEEE Transactions on
Information Theory, 44, 2079-2093.

SIU, M., & OSTENDORE, M. 2000. Variable n—grams and extensions for conversational speech language model-
ing. IEEE transactions on Speech and Audio Processing, 8(1), 63-75.

WOODLAND, P.C., & YOUNG, S.J. 1993. The HTK Continuous Speech Recogniser. Pages 2207-2219 of‘
Proceedings of the Eurospeech conference.

ZITOUNI, I. 2002. A Hierarchical Language Model Based on Variable—Length Class Sequences: The M 0,’;
Approach. IEEE Transactions on Speech and Audio Processing, 10(3), 193-198.

