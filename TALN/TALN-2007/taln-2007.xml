<?xml version="1.0" encoding="UTF-8"?>
<conference>
	<edition>
		<acronyme>TALN'2007</acronyme>
		<titre>14ème conférence sur le Traitement Automatique des Langues Naturelles</titre>
		<ville>Toulouse</ville>
		<pays>France</pays>
		<dateDebut>2007-06-05</dateDebut>
		<dateFin>2007-06-08</dateFin>
		<presidents>
			<nom>Nabil Hathout</nom>
			<nom>Philippe Muller</nom>
		</presidents>
		<typeArticles>
			<type id="long">Papiers longs</type>
			<type id="poster">Posters</type>
			<type id="démonstration">Démonstrations</type>
		</typeArticles>
		<statistiques>
			<!-- <acceptations id="long" soumissions=""></acceptations> -->
			<!-- <acceptations id="court" soumissions=""></acceptations> -->
		</statistiques>
		<siteWeb>http://www.irit.fr/taln07/</siteWeb>
		<meilleurArticle>
			<articleId>taln-2007-long-009</articleId>
		</meilleurArticle>
	</edition>
	<articles>
		<article id="taln-2007-long-001" session="Segmentation">
			<auteurs>
				<auteur>
					<nom>Maria Georgescul</nom>
					<email>maria.georgescul@eti.unige.ch</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Alexander Clarck</nom>
					<email>alexc@cs.rhul.ac.uk</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Susan Armstrong</nom>
					<email>susan.armstrong@issco.unige.ch</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">ISSCO/TIM/ETI, University of Geneva</affiliation>
				<affiliation affiliationId="2">Department of Computer Science, Royal Holloway University of London</affiliation>
			</affiliations>
			<titre></titre>
			<type>long</type>
			<pages>15-24</pages>
			<resume>Dans cet article, nous traitons de la segmentation automatique des textes en épisodes thématiques non superposés et ayant une structure linéaire. Notre étude porte sur l’utilisation des traits lexicaux, acoustiques et syntaxiques et sur l’influence de ces traits sur la performance d’un système automatique de segmentation thématique. Nous appliquons notre approche, basée sur des machines à vecteurs support, à des transcriptions des dialogues multilocuteurs.</resume>
			<mots_cles>segmentation automatique en épisodes thématiques, machines à vecteurs support, dialogues multi-locuteurs</mots_cles>
			<title>Exploiting structural meeting-specific features for topic segmentation</title>
			<abstract>In this article we address the task of automatic text structuring into linear and non-overlapping thematic episodes. Our investigation reports on the use of various lexical, acoustic and syntactic features, and makes a comparison of how these features influence performance of automatic topic segmentation. Using datasets containing multi-party meeting transcriptions, we base our experiments on a proven state-of-the-art approach using support vector classification.</abstract>
			<keywords>automatic topic segmentation, support vector machines, multi-party dialogues</keywords>
		</article>
		<article id="taln-2007-long-002" session="Segmentation">
			<auteurs>
				<auteur>
					<nom>Silvia Fernández</nom>
					<email>silvia.fernandez@univ-avignon.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Eric Sanjuan</nom>
					<email>eric.sanjuan@univ-avignon.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Juan-Manuel Torres-Moreno</nom>
					<email>juan-manuel.torres@univ-avignon.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>3</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Laboratoire Informatique d’Avignon, BP 1228 84911 Avignon FRANCE</affiliation>
				<affiliation affiliationId="2">LPM UHP-Nancy, BP 239 54506 Vandoeuvre les Nancy FRANCE</affiliation>
				<affiliation affiliationId="3">École Polytechnique de Montréal, CP 6079 Centre-ville, Montréal, Québec, CANADA H3C3A7</affiliation>
			</affiliations>
			<titre>Énergie textuelle de mémoires associatives</titre>
			<type>long</type>
			<pages>25-34</pages>
			<resume>Dans cet article, nous présentons une approche de réseaux de neurones inspirée de la physique statistique de systèmes magnétiques pour étudier des problèmes fondamentaux du Traitement Automatique de la Langue Naturelle. L’algorithme modélise un document comme un système de neurones où l’on déduit l’énergie textuelle. Nous avons appliqué cette approche aux problèmes de résumé automatique et de détection de frontières thématiques. Les résultats sont très encourageants.</resume>
			<mots_cles>réseaux de neurones, réseaux de Hopfield, résumé, frontière thématiques</mots_cles>
			<title></title>
			<abstract>In this paper we present a neural networks approach, inspired by statistical physics of magnetic systems, to study fundamental problems in Natural Language Processing. The algorithm models documents as neural network whose textual energy is studied. We obtained good results on the application of this method to automatic summarization and thematic borders detection.</abstract>
			<keywords>neural networks, Hopfield network, summarization, thematic boundary</keywords>
		</article>
		<article id="taln-2007-long-003" session="Acquisition">
			<auteurs>
				<auteur>
					<nom>Mehdi Embarek</nom>
					<email>embarekm@zoe.cea.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Olivier Ferret</nom>
					<email>ferreto@zoe.cea.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">CEA LIST, LIC2M, 18 route du Panorama, BP6, FONTENAY AUX ROSES, F- 92265 France</affiliation>
			</affiliations>
			<titre>Une expérience d’extraction de relations sémantiques à partir de textes dans le domaine médical</titre>
			<type>long</type>
			<pages>37-46</pages>
			<resume>Dans cet article, nous présentons une méthode permettant d’extraire à partir de textes des relations sémantiques dans le domaine médical en utilisant des patrons linguistiques. La première partie de cette méthode consiste à identifier les entités entre lesquelles les relations visées interviennent, en l’occurrence les maladies, les examens, les médicaments et les symptômes. La présence d’une des relations sémantiques visées dans les phrases contenant un couple de ces entités est ensuite validée par l’application de patrons linguistiques préalablement appris de manière automatique à partir d’un corpus annoté. Nous rendons compte de l’évaluation de cette méthode sur un corpus en Français pour quatre relations.</resume>
			<mots_cles>extraction de relations sémantiques, patrons lexico-syntaxiques, domaine médical</mots_cles>
			<title></title>
			<abstract>In this article, we present a method to extract semantic relations automatically in the medical domain using linguistic patterns. This method consists first in identifying the entities that are part of the relations to extract, that is to say diseases, exams, treatments, drugs and symptoms. Thereafter, sentences that contain these entities are extracted and the presence of a semantic relation is validated by applying linguistic patterns that were automatically learnt from an annotated corpus. We report the results of an evaluation of our extraction method on a French corpus for four relations.</abstract>
			<keywords>extraction of semantic relations, lexico-syntactic patterns, medical domain</keywords>
		</article>
		<article id="taln-2007-long-004" session="Acquisition">
			<auteurs>
				<auteur>
					<nom>Davy Weissenbacher</nom>
					<email>dw@lipn.univ-paris13.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Adeline Nazarenko</nom>
					<email>nazarenko@lipn.univ-paris13.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université Paris-Nord, LIPN, 99 av. J-B. Clément, F-93430 Villetaneuse</affiliation>
			</affiliations>
			<titre>Identifier les pronoms anaphoriques et trouver leurs antécédents : l’intérêt de la classification bayésienne</titre>
			<type>long</type>
			<pages>47-56</pages>
			<resume>On oppose souvent en TAL les systèmes à base de connaissances linguistiques et ceux qui reposent sur des indices de surface. Chaque approche a ses limites et ses avantages. Nous proposons dans cet article une nouvelle approche qui repose sur les réseaux bayésiens et qui permet de combiner au sein d’une même représentation ces deux types d’informations hétérogènes et complémentaires. Nous justifions l’intérêt de notre approche en comparant les performances du réseau bayésien à celles des systèmes de l’état de l’art, sur un problème difficile du TAL, celui de la résolution d’anaphore.</resume>
			<mots_cles>réseaux bayésiens, résolution des anaphores, connaissance linguistique, indice de surface</mots_cles>
			<title></title>
			<abstract>In NLP, a traditional distinction opposes linguistically-based systems and knowledge-poor ones, which mainly rely on surface clues. Each approach has its drawbacks and its advantages. In this paper, we propose a new approach based on Bayes Networks that allows to combine both types of information. As a case study, we focus on the anaphora resolution which is known as a difficult NLP problem. We show that our bayesain system performs better than a state-of-the art one for this task.</abstract>
			<keywords>bayesian network, anaphora resolution, linguistic knowledge, surface clue</keywords>
		</article>
		<article id="taln-2007-long-005" session="Morphologie">
			<auteurs>
				<auteur>
					<nom>Bruno Cartoni</nom>
					<email>bruno.cartoni@eti.unige.ch</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">ISSCO/TIM/ETI – Université de Genève, 40 bd du Pont d’Arve, 1205 Genève</affiliation>
			</affiliations>
			<titre>Régler les règles d’analyse morphologique</titre>
			<type>long</type>
			<pages>59-68</pages>
			<resume>Dans cet article, nous présentons différentes contraintes mécaniques et linguistiques applicables à des règles d’analyse des mots inconnus afin d’améliorer la performance d’un analyseur morphologique de l’italien. Pour mesurer l’impact de ces contraintes, nous présentons les résultats d’une évaluation de chaque contrainte qui prend en compte les gains et les pertes qu’elle engendre. Nous discutons ainsi de la nécessaire évaluation de chaque réglage apporté aux règles afin d’en déterminer la pertinence.</resume>
			<mots_cles>évaluation, analyse morphologique, mots inconnus, morphologie constructionnelle</mots_cles>
			<title></title>
			<abstract>In this article, we present various constraints, mechanical and linguistic, that can be applied to analysing rules for unknown words in order to improve the performance of a morphological analyser for Italian. To measure the impact of these constraints, we present an evaluation for each constraint, taking into account the gains and losses which they generate. We then discuss the need to evaluate any fine-tuning of these kinds of rules in order to decide whether they are appropriate or not.</abstract>
			<keywords>evaluation, morphological analysis, unknown words, constructional morphology</keywords>
		</article>
		<article id="taln-2007-long-006" session="Morphologie">
			<auteurs>
				<auteur>
					<nom>François Barthélemy</nom>
					<email>barthe@cnam.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">CNAM, Cédric, 292 rue Saint-Martin, 75003 Paris</affiliation>
				<affiliation affiliationId="2">INRIA, Atoll, 78153 Le Chesnay cedex</affiliation>
			</affiliations>
			<titre>Structures de traits typées et morphologie à partitions</titre>
			<type>long</type>
			<pages>69-78</pages>
			<resume>Les structures de traits typées sont une façon abstraite et agréable de représenter une information partielle. Dans cet article, nous montrons comment la combinaison de deux techniques relativement classiques permet de définir une variante de morphologie à deux niveaux intégrant harmonieusement des structures de traits et se compilant en une machine finie. La première de ces techniques est la compilation de structure de traits en expressions régulières, la seconde est la morphologie à partition. Nous illustrons au moyen de deux exemples l’expressivité d’un formalisme qui rapproche les grammaires à deux niveaux des grammaires d’unification.</resume>
			<mots_cles>morphologie à deux niveaux, transducteurs finis à états, structure de traits</mots_cles>
			<title></title>
			<abstract>Feature Structures are an abstract and convenient way of representing partial information. In this paper, we show that the combination of two relatively classical techniques makes possible the definition of a variant of two-level morphology which integrates harmoniously feature structures and compiles into finite-state machines. The first technique is the compilation of feature structures into regular expressions, the second one is partition-based morphology. Two examples are given, which show that our formalism is close to unification grammars.</abstract>
			<keywords>two-level morphology, finite-state transducers, feature structures</keywords>
		</article>
		<article id="taln-2007-long-007" session="Morphologie">
			<auteurs>
				<auteur>
					<nom>Louise Deléger</nom>
					<email>louise.deleger@spim.jussieu.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Fiammetta Namer</nom>
					<email>fiammetta.namer@univ-nancy2.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Pierre Zweigenbaum</nom>
					<email>pz@limsi.fr</email>
					<affiliationId>3</affiliationId>
					<affiliationId>4</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">INSERM, UMR_S 872, Éq. 20, Les Cordeliers, 75006 Paris, Université Pierre et Marie Curie-Paris6, UMR_S 872, 75006 Paris, Université Paris Descartes, UMR_S 872, 75006 Paris</affiliation>
				<affiliation affiliationId="2">ATILF et Université Nancy 2, CLSH, 54015 Nancy</affiliation>
				<affiliation affiliationId="3">CNRS, UPR3251, LIMSI, 91403 Orsay</affiliation>
				<affiliation affiliationId="4">INALCO, CRIM, 75343 Paris Cedex 07</affiliation>
			</affiliations>
			<titre>Analyse morphosémantique des composés savants : transposition du français à l’anglais</titre>
			<type>long</type>
			<pages>79-88</pages>
			<resume>La plupart des vocabulaires spécialisés comprennent une part importante de lexèmes morphologiquement complexes, construits à partir de racines grecques et latines, qu’on appelle « composés savants ». Une analyse morphosémantique permet de décomposer et de donner des définitions à ces lexèmes, et semble pouvoir être appliquée de façon similaire aux composés de plusieurs langues. Cet article présente l’adaptation d’un analyseur morphosémantique, initialement dédié au français (DériF), à l’analyse de composés savants médicaux anglais, illustrant ainsi la similarité de structure de ces composés dans des langues européennes proches. Nous exposons les principes de cette transposition et ses performances. L’analyseur a été testé sur un ensemble de 1299 lexèmes extraits de la terminologie médicale WHO-ART : 859 ont pu être décomposés et définis, dont 675 avec succès. Outre une simple transposition d’une langue à l’autre, la méthode montre la potentialité d’un système multilingue.</resume>
			<mots_cles>analyse morphosémantique, composition savante, terminologie médicale</mots_cles>
			<title></title>
			<abstract>Medical language, as many technical languages, is rich with morphologically complex words, many of which take their roots in Greek and Latin – in which case they are called neoclassical compounds. Morphosemantic analysis can help generate decompositions and definitions of such words, and is likely to be similarly applicable to compounds from different languages. This paper reports work on the adaptation of a morphosemantic analyzer dedicated to French (DériF) to analyze English medical neoclassical compounds, and shows the similarity in structure of compounds from related European languages. It presents the principles of this transposition and its current performance. The analyzer was tested on a set of 1,299 compounds extracted from theWHO-ART terminology: 859 could be decomposed and defined, 675 of which successfully. Aside from simple transposition from one language to another, the method also emphasizes the potentiality for a multilingual system.</abstract>
			<keywords>morphosemantic analysis, neo-classical compounding,medical terminology</keywords>
		</article>
		<article id="taln-2007-long-008" session="Traduction">
			<auteurs>
				<auteur>
					<nom>Oana Frunza</nom>
					<email>ofrunza@site.uottawa.ca</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Diana Inkpen</nom>
					<email>diana@site.uottawa.ca</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">School of Information Technology and Engineering, University of Ottawa, Ottawa, ON, K1N 6N5, Canada</affiliation>
			</affiliations>
			<titre></titre>
			<type>long</type>
			<pages>91-100</pages>
			<resume>Les congénères sont des mots qui ont au moins un sens en commun entre deux langues en plus d‘avoir une orthographie semblable. La reconnaissance de ce type de mots permet aux apprenants de langue seconde ou étrangère d‘enrichir plus rapidement leur vocabulaire et d‘améliorer leur compréhension écrite. Toutefois, les faux amis sont des paires de mots qui à l‘écrit ont des similarités, mais ils ont des significations différentes. Pour leur part, les congénères partiels sont des mots qui ont la même signification dans certains contextes dans chacune des deux langues. Cet article présente une méthode pour la classification automatique des paires des mots classées en congénères ou faux amis, en utilisant des mesures de similarité orthographiques et des méthodes d‘apprentissage automatique. Ainsi, nous construisons des listes complètes des congénères et des faux amis entre les deux langues. Nous désambiguisons les congénères partiels dans des contextes spécifiques. Nos méthodes sont évaluées pour le français et l‘anglais, mais elles seraient applicables à d‘autres paires des langues. Nous avons construit un outil qui prend ces listes et marque dans un texte français les mots qui ont des congénères ou des faux amis en anglais, dans le but d‘aider les apprenants en français langue seconde ou étrangère à améliorer leur compréhension écrite et à développer une meilleure rétention.</resume>
			<mots_cles>congénères, faux amis, congénères partiels, mesures de similarité orthographiques, apprentissage automatique, apprentissage des langues assisté par ordinateur</mots_cles>
			<title>A tool for detecting French-English cognates and false friends</title>
			<abstract>Cognates are pairs of words in different languages similar in spelling and meaning. They can help a second-language learner on the tasks of vocabulary expansion and reading comprehension. False friends are pairs of words that have similar spelling but different meanings. Partial cognates are pairs of words in two languages that have the same meaning in some, but not all contexts. In this article we present a method to automatically classify a pair of words as cognates or false friends, by using several measures of orthographic similarity as features for classification. We use this method to create complete lists of cognates and false friends between two languages. We also disambiguate partial cognates in context. We applied all our methods to French and English, but they can be applied to other pairs of languages as well. We built a tool that takes the produced lists and annotates a French text with equivalent English cognates or false friends, in order to help second-language learners improve their reading comprehension skills and retention rate.</abstract>
			<keywords>cognates, false friends, partial cognates, orthographic similarity measures, machine learning (ML), computer-assisted language learning (CALL)</keywords>
		</article>
		<article id="taln-2007-long-009" session="Traduction">
			<auteurs>
				<auteur>
					<nom>Philippe Langlais</nom>
					<email>felipe@iro.umontreal.ca</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Alexandre Patry</nom>
					<email>patryale@iro.umontreal.ca</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université de Montréal, CP. 6128 succursale centre-ville</affiliation>
			</affiliations>
			<titre>Enrichissement d’un lexique bilingue par analogie</titre>
			<type>long</type>
			<pages>101-110</pages>
			<resume>La présence de mots inconnus dans les applications langagières représente un défi de taille bien connu auquel n’échappe pas la traduction automatique. Les systèmes professionnels de traduction offrent à cet effet à leurs utilisateurs la possibilité d’enrichir un lexique de base avec de nouvelles entrées. Récemment, Stroppa et Yvon (2005) démontraient l’intérêt du raisonnement par analogie pour l’analyse morphologique d’une langue. Dans cette étude, nous montrons que le raisonnement par analogie offre également une réponse adaptée au problème de la traduction d’entrées lexicales inconnues.</resume>
			<mots_cles>analogie formelle, enrichissement de lexiques bilingues, traduction automatique</mots_cles>
			<title></title>
			<abstract>Unknown words are a well-known hindrance to natural language applications. In particular, they drastically impact machine translation quality. An easy way out commercial translation systems usually offer their users is the possibility to add unknown words and their translations into a dedicated lexicon. Recently, Stroppa et Yvon (2005) shown how analogical learning alone deals nicely with morphology in different languages. In this study we show that analogical learning offers as well an elegant and efficient solution to the problem of identifying potential translations of unknown words.</abstract>
			<keywords>formal analogy, bilingual lexicon projection, machine translation</keywords>
		</article>
		<article id="taln-2007-long-010" session="Traduction">
			<auteurs>
				<auteur>
					<nom>Vincent Claveau</nom>
					<email>Vincent.Claveau@irisa.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">IRISA - CNRS, Campus de Beaulieu, 35042 Rennes cedex, France</affiliation>
			</affiliations>
			<titre>Inférence de règles de réécriture pour la traduction de termes biomédicaux</titre>
			<type>long</type>
			<pages>111-120</pages>
			<resume>Dans le domaine biomédical, le caractère multilingue de l’accès à l’information est un problème d’importance. Dans cet article nous présentons une technique originale permettant de traduire des termes simples du domaine biomédical de et vers de nombreuses langues. Cette technique entièrement automatique repose sur l’apprentissage de règles de réécriture à partir d’exemples et l’utilisation de modèles de langues. Les évaluations présentées sont menées sur différentes paires de langues (français-anglais, espagnol-portugais, tchèque-anglais, russe-anglais...). Elles montrent que cette approche est très efficace et offre des performances variables selon les langues mais très bonnes dans l’ensemble et nettement supérieures à celles disponibles dans l’état de l’art. Les taux de précision de traductions s’étagent ainsi de 57.5% pour la paire russe-anglais jusqu’à 85% pour la paire espagnol-portugais et la paire françaisanglais.</resume>
			<mots_cles>traduction artificielle, terminologie biomédicale, apprentissage artificiel, modèles de langue</mots_cles>
			<title></title>
			<abstract>In the biomedical domain, offering a multilingual access to specialized information is a major issue. In this paper, we present an original approach to translate simple biomedical terms between several languages. This fully automatic approach is based on a machine learning technique inferring rewriting rules and on language models. The experiments that are presented are done onn several language pairs (French-English, Spanish-Portuguese, Czech-English, Russian-English...). They demonstrate the efficiency of our approach by yielding translation performances that vary according to the languages but are always very good and better than those of state-of-art techniques. Indeed, the translation precision rates go from 57.5% for translation from Russian to English up to 85% for Spanish-Portuguese and French-English language pairs.</abstract>
			<keywords>machine translation, biomedical terminology, machine learning, language models</keywords>
		</article>
		<article id="taln-2007-long-011" session="Outils">
			<auteurs>
				<auteur>
					<nom>Émilie Guimier De Neef</nom>
					<email>emilie.guimierdeneef@orange-ftgroup.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Arnaud Debeurme</nom>
					<email>arnaud.debeurme@orange-ftgroup.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Jungyeul Park</nom>
					<email>jungyeul.park@orange-ftgroup.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">FTR&amp;D/TECH/EASY – France Telecom R&amp;D, 2, avenue Pierre Marzin, 22300 Lannion Cedex, France</affiliation>
			</affiliations>
			<titre>TiLT correcteur de SMS : évaluation et bilan qualitatif</titre>
			<type>long</type>
			<pages>123-132</pages>
			<resume>Nous présentons le logiciel TiLT pour la correction des SMS et évaluons ses performances sur le corpus de SMS du DELIC. L'évaluation utilise la distance de Jaccard et la mesure BLEU. La présentation des résultats est suivie d'une analyse qualitative du système et de ses limites.</resume>
			<mots_cles>SMS, SMS corpus, correction orthographique, TiLT, evaluation</mots_cles>
			<title></title>
			<abstract>This paper presents TiLT system which allows us to correct spelling errors in SMS messages to standard French. We perform Jaccard and Bleu metrics for its evaluation using the DELIC SMS corpus as a reference. We discuss qualitative analyses of system and its limits.</abstract>
			<keywords>SMS, SMS corpus, spelling correction, TiLT, evaluation</keywords>
		</article>
		<article id="taln-2007-long-012" session="Outils">
			<auteurs>
				<auteur>
					<nom>Hong-Thai Nguyen</nom>
					<email>Hong-Thai.Nguyen@imag.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Christian Boitet</nom>
					<email>Christian.Boitet@imag.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">GETALP, LIG, 385, av. de la Bibliothèque, BP 53 F-38041 Grenoble cedex 9</affiliation>
			</affiliations>
			<titre>Vers un méta-EDL complet, puis un EDL universel pour la TAO</titre>
			<type>long</type>
			<pages>133-142</pages>
			<resume>Un “méta-EDL” (méta-Environnement de Développement Linguiciel) pour la TAO permet de piloter à distance un ou plusieurs EDL pour construire des systèmes de TAO hétérogènes. Partant de CASH, un méta-EDL dédié à Ariane-G5, et de WICALE 1.0, un premier méta-EDL générique mais aux fonctionnalités minimales, nous dégageons les problèmes liés à l’ajout de fonctionnalités riches comme l’édition et la navigation en local, et donnons une solution implémentée dans WICALE 2.0. Nous y intégrons maintenant une base lexicale pour les systèmes à « pivot lexical », comme UNL/U++. Un but à plus long terme est de passer d’un tel méta-EDL générique multifonctionnel à un EDL « universel », ce qui suppose la réingénierie des compilateurs et des moteurs des langages spécialisés pour la programmation linguistique (LSPL) supportés par les divers EDL.</resume>
			<mots_cles>génie linguiciel, langages spécialisés pour la programmation linguistique, LSPL, environnement de développement, EDL, TAO, systèmes distribués hétérogènes</mots_cles>
			<title></title>
			<abstract>A “meta-EDL” (meta-Environment for Developing Lingware) for MT allows to pilot one or more distant EDL in order to build heterogeneous MT systems. Starting from CASH, a meta-EDL dedicated to Ariane-G5, and from WICALE 1.0, a first meta-EDL, generic but offering minimal functionalities, we study the problems arising when adding rich functionalities such as local editing and navigation, and give a solution implemented in WICALE 2.0. We are now integrating to it a lexical database for MT systems relying on a “lexical pivot”, such as UNL/U++. A longer-term goal is to evolve from such a multifunctional generic meta-EDL to a “universal” EDL, which would imply the reengineering of the compilers and engines of the specialized languages (SLLPs) supported by the various EDLs.</abstract>
			<keywords>lingware engineering, specialized languages for linguistic programming, development environment, EDL, MT, heterogeneous distributed MT systems</keywords>
		</article>
		<article id="taln-2007-long-013" session="Outils">
			<auteurs>
				<auteur>
					<nom>Frederik Cailliau</nom>
					<email>cailliau@sinequa.com</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Claude De Loupy</nom>
					<email>loupy@syllabs.com</email>
					<affiliationId>3</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIPN – Institut Galilée – Université Paris-Nord, 99, avenue Jean-Baptiste Clément, 93430 Villetaneuse</affiliation>
				<affiliation affiliationId="2">Sinequa Labs – 51 rue Ledru-Rollin, 94200 Ivry-sur-Seine</affiliation>
				<affiliation affiliationId="3">Syllabs – 3 rue Castex, c/o Agoranov, 75004 Paris</affiliation>
			</affiliations>
			<titre>Aides à la navigation dans un corpus de transcriptions d’oral</titre>
			<type>long</type>
			<pages>143-152</pages>
			<resume>Dans cet article, nous évaluons les performances de fonctionnalités d’aide à la navigation dans un contexte de recherche dans un corpus audio. Nous montrons que les particularités de la transcription et, en particulier les erreurs, conduisent à une dégradation parfois importante des performances des outils d’analyse. Si la navigation par concepts reste dans des niveaux d’erreur acceptables, la reconnaissance des entités nommées, utilisée pour l’aide à la lecture, voit ses performances fortement baisser. Notre remise en doute de la portabilité de ces fonctions à un corpus oral est néanmoins atténuée par la nature même du corpus qui incite à considérer que toute méthodes permettant de réduire le temps d’accès à l’information est pertinente, même si les outils utilisés sont imparfaits.</resume>
			<mots_cles>évaluation, moteur de recherche, corpus oral</mots_cles>
			<title></title>
			<abstract>In this paper we evaluate the performances of navigation facilities within the context of information retrieval performed on an audio corpus. We show that the issues about transcription, especially the errors, lead to a sometimes important deterioration of the performances of the analysing tools. While the navigation by concepts remains within an acceptable error rate, the recognition of named entities used in fast reading undergo a performance drop. Our caution to the portability of these functions to a speech corpus is attenuated by the nature of the corpus: access time to a speech corpus can be very long, and therefore all methods that reduce access time are good to take.</abstract>
			<keywords>evaluation, search engine, speech corpus</keywords>
		</article>
		<article id="taln-2007-long-014" session="Syntaxe">
			<auteurs>
				<auteur>
					<nom>Marie-Laure Guénot</nom>
					<email>marie-laure.guenot@u-bordeaux3.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Signes, Université Michel de Montaigne Bordeaux 3</affiliation>
			</affiliations>
			<titre>Une grammaire du français pour une théorie descriptive et formelle de la langue</titre>
			<type>long</type>
			<pages>155-164</pages>
			<resume>Dans cet article, nous présentons une grammaire du français qui fait l’objet d’un modèle basé sur des descriptions linguistiques de corpus (provenant notamment des travaux de l’Approche Pronominale) et représentée selon le formalisme des Grammaires de Propriétés. Elle constitue une proposition nouvelle parmi les grammaires formelles du français, participant à la mise en convergence de la variété des travaux de description linguistique, et de la diversité des possibilités de représentation formelle. Cette grammaire est mise à disposition publique sur le Centre de Ressources pour la Description de l’Oral en tant que ressource pour la représentation et l’analyse.</resume>
			<mots_cles>développement de grammaire, ressource pour le TAL, grammaire du français, syntaxe, linguistique formelle, linguistique descriptive, grammaires de propriétés (GP)</mots_cles>
			<title></title>
			<abstract>In this paper I present a grammar for French, which is the implementation of a linguistic model based on corpus descriptions (notably coming from Approche Pronominale) and represented into the Property Grammars formalism. It accounts for a new proposition among formal grammars, taking part into the works that aim to promote convergence between the various researchs of descriptive linguistics and the diversity of formal representation possibilities. It is freely available on the Spoken Data Resource Center (CRDO), as a representation and analysis resource.</abstract>
			<keywords>grammar development, resource for NLP, French grammar, syntax, formal linguistics, descriptive linguistics, property grammars (PG)</keywords>
		</article>
		<article id="taln-2007-long-015" session="Syntaxe">
			<auteurs>
				<auteur>
					<nom>Alexandre Dikovsky</nom>
					<email>Alexandre.Dikovsky@univ-nantes.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LINA-FRE CNRS 2729, Université de Nantes</affiliation>
			</affiliations>
			<titre>Architecture compositionnelle pour les dépendances croisées</titre>
			<type>long</type>
			<pages>165-174</pages>
			<resume>L’article présente les principes généraux sous-jacent aux grammaires catégorielles de dépendances : une classe de grammaires de types récemment proposée pour une description compositionnelle et uniforme des dépendances continues et discontinues. Ces grammaires très expressives et analysées en temps polynomial, adoptent naturellement l’architecture multimodale et expriment les dépendances croisées illimitées.</resume>
			<mots_cles>grammaires catégorielles de dépendances, grammaires multimodales, analyseur syntaxique</mots_cles>
			<title></title>
			<abstract>This article presents the general principles underlying the categorial dependency grammars : a class of type logical grammars recently introduced as a compositional and uniform definition of continuous and discontinuous dependences. These grammars are very expressive, are parsed in a reasonable polynomial time, naturally adopt the multimodal architecture and explain unlimited cross-serial dependencies.</abstract>
			<keywords>categorial dependency grammars, multimodal grammars, syntactic parser</keywords>
		</article>
		<article id="taln-2007-long-016" session="Syntaxe">
			<auteurs>
				<auteur>
					<nom>Claire Gardent</nom>
					<email>gardent@loria.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Yannick Parmentier</nom>
					<email>parmenti@loria.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">CNRS / LORIA, Campus scientifique – BP 259, F-54 506 Vandoeuvre-Lès-Nancy CEDEX</affiliation>
				<affiliation affiliationId="2">INRIA / LORIA – Nancy Université, Campus scientifique, BP 259, F-54 506 Vandoeuvre-Lès-Nancy CEDEX</affiliation>
			</affiliations>
			<titre>SemTAG, une architecture pour le développement et l’utilisation de grammaires d’arbres adjoints à portée sémantique</titre>
			<type>long</type>
			<pages>175-184</pages>
			<resume>Dans cet article, nous présentons une architecture logicielle libre et ouverte pour le développement de grammaires d’arbres adjoints à portée sémantique. Cette architecture utilise un compilateur de métagrammaires afin de faciliter l’extension et la maintenance de la grammaire, et intègre un module de construction sémantique permettant de vérifier la couverture aussi bien syntaxique que sémantique de la grammaire. Ce module utilise un analyseur syntaxique tabulaire généré automatiquement à partir de la grammaire par le système DyALog. Nous présentons également les résultats de l’évaluation d’une grammaire du français développée au moyen de cette architecture.</resume>
			<mots_cles>analyseur syntaxique, grammaires d’arbres adjoints, construction sémantique, architecture logicielle</mots_cles>
			<title></title>
			<abstract>In this paper, we introduce a free and open software architecture for the development of Tree Adjoining Grammars equipped with semantic information. This architecture uses a metagrammar compiler to facilitate the grammar extension and maintnance, and includes a semantic construction module allowing to check both the syntactic and semantic coverage of the grammar. This module uses a tabular syntactic parser generated automatically from this grammar using the DyALog system. We also give the results of the evaluation of a real-size TAG for French developed using this architecture.</abstract>
			<keywords>syntactic parser, tree adjoining grammars, semantic construction, software architecture</keywords>
		</article>
		<article id="taln-2007-long-017" session="Désambiguïsation">
			<auteurs>
				<auteur>
					<nom>Fabienne Venant</nom>
					<email>fabienne.venant@ens.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LaLIC – Université Paris IV, Maison de la recherche, 28 rue Serpente 75006 Paris</affiliation>
			</affiliations>
			<titre>Utiliser des classes de sélection distributionnelle pour désambiguïser les adjectifs</titre>
			<type>long</type>
			<pages>187-196</pages>
			<resume>La désambiguïsation lexicale présente un intérêt considérable pour un nombre important d’applications, en traitement automatique des langues comme en recherche d'information. Nous proposons un modèle d’un genre nouveau, fondé sur la théorie de la construction dynamique du sens (Victorri et Fuchs, 1996). Ce modèle donne une place centrale à la polysémie et propose une représentation géométrique du sens. Nous présentons ici une application de ce modèle à la désambiguïsation automatique des adjectifs. La méthode utilisée s'appuie sur une pré-désambiguïsation du nom régissant l'adjectif, par le biais de classes de sélection distributionnelle. Elle permet aussi de prendre en compte les positions relatives du nom et de l'adjectif (postpostion ou antéposition) dans le calcul du sens.</resume>
			<mots_cles>traitement automatique des langues, désambiguïsation, sémantique, polysémie adjectivale, construction dynamique du sens, synonymie, classes distributionnelles, corpus, espace sémantique, espace distributionnel</mots_cles>
			<title></title>
			<abstract>Automatic word sense disambiguation represents an important issue for many applications, in Natural Language Processing as in Information Retrieval. We propose a new kind of model, within the framework of Dynamical Construction of Meaning (Victorri and Fuchs, 1996). This model gives a central place to polysemy and proposes a geometric representation of meaning. We present here an application of this model to adjective sense disambiguation. The method we used relies on a pre-disambiguation of the noun used with the adjective under study, using distributionnal classes. It can also take into account the changes in the meaning of the adjective, whether it is placed before or after the noun.</abstract>
			<keywords>natural language processing, word sense disambiguation, semantics, adjectival polysemy, dynamical construction of meaning, synonymy, distributionnal classes, corpus, semantic space, distributional space</keywords>
		</article>
		<article id="taln-2007-long-018" session="Désambiguïsation">
			<auteurs>
				<auteur>
					<nom>Véronique Malaisé</nom>
					<email>vmalaise@few.vu.nl</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Luit Gazendam</nom>
					<email>Luit.Gazendam@telin.nl</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Hennie Brugman</nom>
					<email>Hennie.Brugman@mpi.nl</email>
					<affiliationId>3</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Vrije Universiteit, Amsterdam</affiliation>
				<affiliation affiliationId="2">Telematica Institute, Enschedé, Netherlands</affiliation>
				<affiliation affiliationId="3">Max Planck Institute for Psycholinguistics, Nijmegen, Netherlands</affiliation>
			</affiliations>
			<titre></titre>
			<type>long</type>
			<pages>197-206</pages>
			<resume>La relation voir/employé pour d’un thesaurus est souvent plus complexe que la (para-)synonymie recommandée par l’ISO-2788, standard décrivant le contenu de ces vocabulaires contrôlés. Le fait qu’un non descripteur puisse renvoyer à plusieurs descripteurs (seuls les descripteurs sont pertinents dans le cadre de l’indexation contrôlée) fait que cette relation est complexe à utiliser dans un contexte d’annotation automatique : elle génère des cas d’ambiguité. Dans ce papier, nous présentons CARROT, un algorithme que nous avons mis au point pour classer les résultats de notre chaîne de traitements pour l’Extraction d’Information, et son utilisation dans le cadre de la sélection du descripteur pertinent lorsque plusieurs choix sont possibles. Cette sélection s’adresse à des documentalistes, dans le but de simplifier et d’accélérer leur travail, et se base sur la structure de leur thesaurus. Nous arrivons à un succès de 95 % dans nos suggestions ; nous discutons ces résultats et présentons des perspectives à cette expérimentation.</resume>
			<mots_cles>désambiguisation sémantique, algorithme de classement, annotation automatique</mots_cles>
			<title>Disambiguating automatic semantic annotation based on a thesaurus structure</title>
			<abstract>The use/use for relationship a thesaurus is usually more complex than the (para-) synonymy recommended in the ISO-2788 standard describing the content of these controlled vocabularies. The fact that a non preferred term can refer to multiple preferred terms (only the latter are relevant in controlled indexing) makes this relationship difficult to use in automatic annotation applications : it generates ambiguity cases. In this paper, we present the CARROT algorithm, meant to rank the output of our Information Extraction pipeline, and how this algorithm can be used to select the relevant preferred term out of different possibilities. This selection is meant to provide suggestions of keywords to human annotators, in order to ease and speed up their daily process and is based on the structure of their thesaurus. We achieve a 95 % success, and discuss these results along with perspectives for this experiment.</abstract>
			<keywords>word sense disambiguation, ranking algorithm, automatic annotation</keywords>
		</article>
		<article id="taln-2007-long-019" session="Désambiguïsation">
			<auteurs>
				<auteur>
					<nom>Marianna Apidianaki</nom>
					<email>Marianna.Apidianaki@linguist.jussieu.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Lattice, Université Paris 7, CNRS, ENS-1 rue Maurice Arnoux, F-92120, Montrouge</affiliation>
			</affiliations>
			<titre>Repérage de sens et désambiguïsation dans un contexte bilingue</titre>
			<type>long</type>
			<pages>207-216</pages>
			<resume>Les besoins de désambiguïsation varient dans les différentes applications du Traitement Automatique des Langues (TAL). Dans cet article, nous proposons une méthode de désambiguïsation lexicale opératoire dans un contexte bilingue et, par conséquent, adéquate pour la désambiguïsation au sein d’applications relatives à la traduction. Il s’agit d’une méthode contextuelle, qui combine des informations de cooccurrence avec des informations traductionnelles venant d’un bitexte. L’objectif est l’établissement de correspondances de traduction au niveau sémantique entre les mots de deux langues. Cette méthode étend les conséquences de l’hypothèse contextuelle du sens dans un contexte bilingue, tout en admettant l’existence d’une relation de similarité sémantique entre les mots de deux langues en relation de traduction. La modélisation de ces correspondances de granularité fine permet la désambiguïsation lexicale de nouvelles occurrences des mots polysémiques de la langue source ainsi que la prédiction de la traduction la plus adéquate pour ces occurrences.</resume>
			<mots_cles>désambiguïsation contextuelle, similarité sémantique, substituabilité, traduction</mots_cles>
			<title></title>
			<abstract>Word Sense Disambiguation (WSD) needs vary greatly in different Natural Language Processing (NLP) applications. In this article, we propose a WSD method which operates in a bilingual context and is, thus, adequate for disambiguation in applications relative to translation. It is a contextual method which combines cooccurrence information with translation information found in a bitext. The goal is the establishment of translation correspondences at the sense level between the lexical items of two languages. This method extends the consequences of the contextual hypothesis in a bilingual framework assuming, at the same time, the existence of a semantic similarity relation between words of two languages being in a translation relation. The modelling of fine-grained correspondences allows for the disambiguation of new occurrences of the polysemous source language lexical items as well as for the prediction of the most adequate translation for those occurrences.</abstract>
			<keywords>contextual disambiguation, semantic similarity, substitutability, translation</keywords>
		</article>
		<article id="taln-2007-long-020" session="Syntaxe &amp; ressources">
			<auteurs>
				<auteur>
					<nom>Karën Fort</nom>
					<email>Karen.Fort@loria.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Bruno Guillaume</nom>
					<email>Bruno.Guillaume@loria.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">projets Calligramme et TALARIS, LORIA/INRIA Lorraine, UMR 7503, Nancy</affiliation>
				<affiliation affiliationId="2">projet Calligramme, LORIA/INRIA Lorraine, UMR 7503, Nancy</affiliation>
			</affiliations>
			<titre>PrepLex : un lexique des prépositions du français pour l’analyse syntaxique</titre>
			<type>long</type>
			<pages>219-228</pages>
			<resume>PrepLex est un lexique des prépositions du français. Il contient les informations utiles à des systèmes d’analyse syntaxique. Il a été construit en comparant puis fusionnant différentes sources d’informations lexicales disponibles. Ce lexique met également en évidence les prépositions ou classes de prépositions qui apparaissent dans la définition des cadres de sous-catégorisation des ressources lexicales qui décrivent la valence des verbes.</resume>
			<mots_cles>prépositions, lexique, analyse syntaxique</mots_cles>
			<title></title>
			<abstract>PrepLex is a lexicon of French prepositions which provides all the information needed for parsing. It was built by comparing and merging several authoritative lexical sources. This lexicon also shows the prepositions or classes of prepositions that appear in verbs subcategorization frames.</abstract>
			<keywords>prepositions, lexicon, parsing</keywords>
		</article>
		<article id="taln-2007-long-021" session="Syntaxe &amp; ressources">
			<auteurs>
				<auteur>
					<nom>Laurence Danlos</nom>
					<email>laurence.danlos@linguist.jussieu.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Benoît Sagot</nom>
					<email>benoit.sagot@inria.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Lattice - Université Paris 7 - Institut Universitaire de France, 2 place Jussieu, case 7003, 75251 Paris Cedex 05, France</affiliation>
				<affiliation affiliationId="2">Projet Signes - INRIA, Dom. Universitaire, 351 cours de la Libération, 33405 Talence Cedex, France</affiliation>
			</affiliations>
			<titre>Comparaison du Lexique-Grammaire des verbes pleins et de DICOVALENCE : vers une intégration dans le Lefff</titre>
			<type>long</type>
			<pages>229-238</pages>
			<resume>Cet article compare le Lexique-Grammaire des verbes pleins et DICOVALENCE, deux ressources lexicales syntaxiques pour le français développées par des linguistes depuis de nombreuses années. Nous étudions en particulier les divergences et les empiètements des modèles lexicaux sous-jacents. Puis nous présentons le Lefff , lexique syntaxique à grande échelle pour le TAL, et son propre modèle lexical. Nous montrons que ce modèle est à même d’intégrer les informations lexicales présentes dans le Lexique-Grammaire et dans DICOVALENCE. Nous présentons les résultats des premiers travaux effectués en ce sens, avec pour objectif à terme la constitution d’un lexique syntaxique de référence pour le TAL.</resume>
			<mots_cles>lexique syntaxique, Lexique-Grammaire, DICOVALENCE, Lefff</mots_cles>
			<title></title>
			<abstract>This paper compares the Lexicon-Grammar of full verbs and DICOVALENCE, two syntactic lexical resources for French developed by linguists for numerous years. We focus on differences and overlaps between both underlying lexical models. Then we introduce the Lefff , large-coverage syntactic lexicon for NLP, and its own lexical model. We show that this model is able to integrate lexical information present in the Lexicon-Grammar and in DICOVALENCE. We describe the results of the first work done in this direction, the long term goal being the consitution of a high-quality syntactic lexicon for NLP.</abstract>
			<keywords>syntactic lexicon, Lexicon-Grammar, DICOVALENCE, Lefff</keywords>
		</article>
		<article id="taln-2007-long-022" session="Syntaxe &amp; ressources">
			<auteurs>
				<auteur>
					<nom>Pierre-André Buvet</nom>
					<email>Pierre-Andre.Buvet@lli.univ-paris13.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Emmanuel Cartier</nom>
					<email>Emmanuel.Cartier@lli.univ-paris13.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Fabrice Issac</nom>
					<email>Fabrice.Issac@lli.univ-paris13.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Salah Mejri</nom>
					<email>Salah.Mejri@lli.univ-paris13.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LDI UMR 7187– Université Paris 13</affiliation>
			</affiliations>
			<titre>Dictionnaires électroniques et étiquetage syntactico-sémantique</titre>
			<type>long</type>
			<pages>239-248</pages>
			<resume>Nous présentons dans cet article le prototype d’un système d’étiquetage syntactico-sémantique des mots qui utilise comme principales ressources linguistiques différents dictionnaires du laboratoire Lexiques, Dictionnaires, Informatique (LDI). Dans un premier temps, nous mentionnons des travaux sur le même sujet. Dans un deuxième temps, nous faisons la présentation générale du système. Dans un troisième temps, nous exposons les principales caractéristiques des dictionnaires syntactico-sémantiques utilisés. Dans un quatrième temps, nous détaillons un exemple de traitement.</resume>
			<mots_cles>étiqueteur sémantique, dictionnaire, LMF, XML, XPATH</mots_cles>
			<title></title>
			<abstract>We present in this paper a syntactico-semantics tagger prototype which uses as first linguistic resources various dictionaries elaborated at LDI. First, we mention several related works. Second, we present the overall sketch of the system. Third, we expose the main characteristics of the syntactico-semantic dictionaries implied in the processes. Last, using an example, we explicit the main stages of the analysis.</abstract>
			<keywords>word sense disambiguation (WSD), dictionnary, LMF, XML, XPATH</keywords>
		</article>
		<article id="taln-2007-long-023" session="Sémantique">
			<auteurs>
				<auteur>
					<nom>Chiraz Ben Othmane Zribi</nom>
					<email>Chiraz.benothmane@riadi.rnu.tn</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Hanène Mejri</nom>
					<email>Hanene.mejri@riadi.rnu.tn</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Mohamed Ben Ahmed</nom>
					<email>Mohamed.benahmed@riadi.rnu.tn</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Laboratoire de recherche RIADI, Université La Manouba, ENSI, La Manouba, Tunisie</affiliation>
			</affiliations>
			<titre>Un analyseur hybride pour la détection et la correction des erreurs cachées sémantiques en langue arabe</titre>
			<type>long</type>
			<pages>251-260</pages>
			<resume>Cet article s’intéresse au problème de la détection et de la correction des erreurs cachées sémantiques dans les textes arabes. Ce sont des erreurs orthographiques produisant des mots lexicalement valides mais invalides sémantiquement. Nous commençons par décrire le type d’erreur sémantique auquel nous nous intéressons. Nous exposons par la suite l’approche adoptée qui se base sur la combinaison de plusieurs méthodes, tout en décrivant chacune de ces méthodes. Puis, nous évoquons le contexte du travail qui nous a mené au choix de l’architecture multi-agent pour l’implémentation de notre système. Nous présentons et commentons vers la fin les résultats de l’évaluation dudit système.</resume>
			<mots_cles>erreur cachée, erreur sémantique, détection, correction, système multi-agent, langue arabe</mots_cles>
			<title></title>
			<abstract>In this paper, we address the problem of detecting and correcting hidden semantic spelling errors in Arabic texts. Hidden semantic spelling errors are morphologically valid words causing invalid semantic irregularities. After the description of this type of errors, we propose and argue the combined method that we adopted in this work to realize a hybrid spell checker for detecting and correcting hidden spelling errors. Afterward, we present the context of this work and show the multi-agent architecture of our system. Finally, we expose and comment the obtained results.</abstract>
			<keywords>hidden error, semantic error, detection, correction, multi-agent system, Arabic language</keywords>
		</article>
		<article id="taln-2007-long-024" session="Sémantique">
			<auteurs>
				<auteur>
					<nom>Alexandre Denis</nom>
					<email>alexandre.denis@loria.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Frédéric Béchet</nom>
					<email>frederic.bechet@univ-avignon.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Matthieu Quignard</nom>
					<email>matthieu.quignard@loria.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">UMR 7503 LORIA/CNRS – Campus scientifique, 56 506 Vandoeuvre-lès-Nancy Cedex</affiliation>
				<affiliation affiliationId="2">LIA – 339, chemin des Meinajaries, BP 1228,, 84 911 Avignon Cedex 9</affiliation>
			</affiliations>
			<titre>Résolution de la référence dans des dialogues homme-machine : évaluation sur corpus de deux approches symbolique et probabiliste</titre>
			<type>long</type>
			<pages>261-270</pages>
			<resume>Cet article décrit deux approches, l’une numérique, l’autre symbolique, traitant le problème de la résolution de la référence dans un cadre de dialogue homme-machine. L’analyse des résultats obtenus sur le corpus MEDIA montre la complémentarité des deux systèmes développés : robustesse aux erreurs et hypothèses multiples pour l’approche numérique ; modélisation de phénomènes complexes et interprétation complète pour l’approche symbolique.</resume>
			<mots_cles>dialogue homme-machine, résolution de la référence, évaluation, compréhension dans le dialogue</mots_cles>
			<title></title>
			<abstract>This paper presents two approaches, one symbolic, the other one probabilistic, for processing reference resolution in the framework of human-machine spoken dialogues. The results obtained by both systems on the French MEDIA corpus points out the complementarity of the two approaches : robustness and multiple hypotheses generation for the probabilistic one ; global interpretation and modeling of complex phenomenon for the symbolic one.</abstract>
			<keywords>human-machine dialogue, reference resolution, dialogue understanding, evaluation</keywords>
		</article>
		<article id="taln-2007-long-025" session="Sémantique">
			<auteurs>
				<auteur>
					<nom>Sebastian Padó</nom>
					<email>pado@coli.uni-sb.de</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Guillaume Pitel</nom>
					<email>Guillaume.Pitel@loria.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Computerlinguistik – Université de la Sarre</affiliation>
				<affiliation affiliationId="2">Équipe TALARIS, LORIA – INRIA</affiliation>
			</affiliations>
			<titre>Annotation précise du français en sémantique de rôles par projection cross-linguistique</titre>
			<type>long</type>
			<pages>271-280</pages>
			<resume>Dans le paradigme FrameNet, cet article aborde le problème de l’annotation précise et automatique de rôles sémantiques dans une langue sans lexique FrameNet existant. Nous évaluons la méthode proposée par Padó et Lapata (2005, 2006), fondée sur la projection de rôles et appliquée initialement à la paire anglais-allemand. Nous testons sa généralisabilité du point de vue (a) des langues, en l'appliquant à la paire (anglais-français) et (b) de la qualité de la source, en utilisant une annotation automatique du côté anglais. Les expériences montrent des résultats à la hauteur de ceux obtenus pour l'allemand, nous permettant de conclure que cette approche présente un grand potentiel pour réduire la quantité de travail nécessaire à la création de telles ressources dans de nombreuses langues.</resume>
			<mots_cles>multilingue, FrameNet, annotation sémantique automatique, sémantique lexicale, projection d’annotation de rôles, rôles sémantiques</mots_cles>
			<title></title>
			<abstract>This paper considers the task of the automatic induction of role-semantic annotations for new languages with high precision. To this end we test the generalisability of the language-independent, projection-based annotation framework introduced by Padó and Lapata (2005, 2006) by (a) applying it to a new, more distant, language pair (English-French), and (b), using automatic, and thus noisy, input annotation. We show that even under these conditions, high-quality role annotations for French can be obtained that rival existing results for German. We conclude that the framework has considerable potential in reducing the manual effort involved in creating role-semantic resources for a wider range of languages.</abstract>
			<keywords>multilingual, FrameNet, automatic semantic annotation, lexical semantics, annotation projection, semantic roles</keywords>
		</article>
		<article id="taln-2007-long-026" session="Acquisition">
			<auteurs>
				<auteur>
					<nom>Simon Charest</nom>
					<email>developpement@druide.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Éric Brunelle</nom>
					<email>developpement@druide.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Jean Fontaine</nom>
					<email>developpement@druide.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Bertrand Pelletier</nom>
					<email>developpement@druide.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Druide informatique inc., 1435, rue St-Alexandre, bureau 1040, Montréal (Québec) H3A 2G4, Canada</affiliation>
			</affiliations>
			<titre>Élaboration automatique d’un dictionnaire de cooccurrences grand public</titre>
			<type>long</type>
			<pages>283-292</pages>
			<resume>Antidote RX, un logiciel d’aide à la rédaction grand public, comporte un nouveau dictionnaire de 800 000 cooccurrences, élaboré essentiellement automatiquement. Nous l’avons créé par l’analyse syntaxique détaillée d’un vaste corpus et par la sélection automatique des cooccurrences les plus pertinentes à l’aide d’un test statistique, le rapport de vraisemblance. Chaque cooccurrence est illustrée par des exemples de phrases également tirés du corpus automatiquement. Les cooccurrences et les exemples extraits ont été révisés par des linguistes. Nous examinons les choix d’interface que nous avons faits pour présenter ces données complexes à un public non spécialisé. Enfin, nous montrons comment nous avons intégré les cooccurrences au correcteur d’Antidote pour améliorer ses performances.</resume>
			<mots_cles>antidote, cooccurrences, collocations, corpus, analyseur, correcteur</mots_cles>
			<title></title>
			<abstract>Antidote is a complete set of software reference tools for writing French that includes an advanced grammar checker. Antidote RX boasts a new dictionary of 800,000 cooccurrences created mostly automatically. The approach we chose is based on the syntactic parsing of a large corpus and the automatic selection of the most relevant co-occurrences using a statistical test, the log-likelihood ratio. Example sentences illustrating each cooccurrence in context are also automatically selected. The extracted co-occurrences and examples were revised by linguists. We examine the various choices that were made to present this complex data to a non-specialized public. We then show how we use the cooccurrence data to improve the performance of Antidote’s grammar checker.</abstract>
			<keywords>antidote, co-occurrences, collocations, corpus, parser, grammar checker</keywords>
		</article>
		<article id="taln-2007-long-027" session="Acquisition">
			<auteurs>
				<auteur>
					<nom>Didier Schwab</nom>
					<email>didier@cs.usm.my</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Lim Lian Tze</nom>
					<email>liantze@cs.usm.my</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Mathieu Lafourcade</nom>
					<email>lafourcade@lirmm.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Computer-Aided Translation Unit (UTMK), School of Computer Sciences, Universiti Sains Malaysia, Penang, Malaysia</affiliation>
				<affiliation affiliationId="2">TAL-LIRMM, Université Montpellier II – CNRS, 161 rue ada, 34392 Montpellier Cedex 5, France</affiliation>
			</affiliations>
			<titre>Les vecteurs conceptuels, un outil complémentaire aux réseaux lexicaux</titre>
			<type>long</type>
			<pages>293-302</pages>
			<resume>Fréquemment utilisés dans le Traitement Automatique des Langues Naturelles, les réseaux lexicaux font aujourd’hui l’objet de nombreuses recherches. La plupart d’entre eux, et en particulier le plus célèbre WordNet, souffrent du manque d’informations syntagmatiques mais aussi d’informations thématiques (« problème du tennis »). Cet article présente les vecteurs conceptuels qui permettent de représenter les idées contenues dans un segment textuel quelconque et permettent d’obtenir une vision continue des thématiques utilisées grâce aux distances calculables entre eux. Nous montrons leurs caractéristiques et en quoi ils sont complémentaires des réseaux lexico-sémantiques. Nous illustrons ce propos par l’enrichissement des données de WordNet par des vecteurs conceptuels construits par émergence.</resume>
			<mots_cles>WordNet, vecteurs conceptuels, informations lexicales, informations thématiques</mots_cles>
			<title></title>
			<abstract>There is currently much research in natural language processing focusing on lexical networks. Most of them, in particular the most famous, WordNet, lack syntagmatic information and but also thematic information (« Tennis Problem »). This article describes conceptual vectors that allows the representation of ideas in any textual segment and offers a continuous vision of related thematics, based on the distances between these thematics. We show the characteristics of conceptual vectors and explain how they complement lexico-semantic networks. We illustrate this purpose by adding conceptual vectors to WordNet by emergence.</abstract>
			<keywords>WordNet, conceptual vectors, lexical information, thematic information</keywords>
		</article>
		<article id="taln-2007-long-028" session="Acquisition">
			<auteurs>
				<auteur>
					<nom>Julien Bourdaillet</nom>
					<email>julien.bourdaillet@lip6.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Jean-Gabriel Ganascia</nom>
					<email>jean-gabriel.ganascia@lip6.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Laboratoire d’Informatique de Paris 6, Université Pierre et Marie Curie, 104 Quai Kennedy, 75016 Paris</affiliation>
			</affiliations>
			<titre>Alignements monolingues avec déplacements</titre>
			<type>long</type>
			<pages>303-312</pages>
			<resume>Ce travail présente une application d’alignement monolingue qui répond à une problématique posée par la critique génétique textuelle, une école d’études littéraires qui s’intéresse à la genèse textuelle en comparant les différentes versions d’une oeuvre. Ceci nécessite l’identification des déplacements, cependant, le problème devient ainsi NP-complet. Notre algorithme heuristique est basé sur la reconnaissance des homologies entre séquences de caractères. Nous présentons une validation expérimentale et montrons que notre logiciel obtient de bons résultats ; il permet notamment l’alignement de livres entiers.</resume>
			<mots_cles>alignement monolingue, distance d’édition avec déplacements, critique génétique textuelle</mots_cles>
			<title></title>
			<abstract>This paper presents a monolingual alignment application that addresses a problem which occurs in textual genetic criticism, a humanities discipline of literary studies which compares texts’ versions to understand texts’ genesis. It requires the move detection, but this characteristic makes the problem NP-complete. Our heuristic algorithm is based on pattern matching in character sequences. We present an experimental validation where we show that our application obtains good results ; in particular it enables whole book alignment.</abstract>
			<keywords>monolingual alignment, edit distance with moves, textual genetic criticism</keywords>
		</article>
		<article id="taln-2007-long-029" session="Syntaxe">
			<auteurs>
				<auteur>
					<nom>Lionel Nicolas</nom>
					<email>lnicolas@i3s.unice.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Jacques Farré</nom>
					<email>jf@i3s.unice.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Éric Villemonte De La Clergerie</nom>
					<email>Eric.De_La_Clergerie@inria.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Laboratoire I3S, Université de Nice-Sophia Antipolis, CNRS, 2000 route des Lucioles, B.P. 121, 06903 Sophia Antipolis Cedex, France</affiliation>
				<affiliation affiliationId="2">Projet ATOLL - INRIA, Domaine de Voluceau, B.P. 105, 78153 Le Chesnay Cedex, France</affiliation>
			</affiliations>
			<titre>Confondre le coupable : corrections d’un lexique suggérées par une grammaire</titre>
			<type>long</type>
			<pages>315-324</pages>
			<resume>Le succès de l’analyse syntaxique d’une phrase dépend de la qualité de la grammaire sous-jacente mais aussi de celle du lexique utilisé. Une première étape dans l’amélioration des lexiques consiste à identifier les entrées lexicales potentiellement erronées, par exemple en utilisant des techniques de fouilles d’erreurs sur corpus (Sagot &amp; Villemonte de La Clergerie, 2006). Nous explorons ici l’étape suivante : la suggestion de corrections pour les entrées identifiées. Cet objectif est atteint au travers de réanalyses des phrases rejetées à l’étape précédente, après modification des informations portées par les entrées suspectées. Un calcul statistique sur les nouveaux résultats permet ensuite de mettre en valeur les corrections les plus pertinentes.</resume>
			<mots_cles>analyse syntaxique, lexique, apprentissage, correction</mots_cles>
			<title></title>
			<abstract>Successful parsing depends on the quality of the underlying grammar but also on the quality of the lexicon. A first step towards the improvement of lexica consists in identifying potentially erroneous lexical entries, for instance by using error mining techniques on corpora (Sagot &amp; Villemonte de La Clergerie, 2006). we explores the next step, namely the suggestion of corrections for those entries. This is achieved by parsing the sentences rejected at the previous step anew, after modifying the information carried by the suspected entries. Afterwards, a statistical computation on the parsing results exhibits the most relevant corrections.</abstract>
			<keywords>parsing, lexicon, machine learning, correction</keywords>
		</article>
		<article id="taln-2007-long-030" session="Syntaxe">
			<auteurs>
				<auteur>
					<nom>Sylvain Pogodalla</nom>
					<email>sylvain.pogodalla@loria.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LORIA/INRIA Lorraine</affiliation>
			</affiliations>
			<titre>Ambiguïté de portée et approche fonctionnelle des grammaires d’arbres adjoints</titre>
			<type>long</type>
			<pages>325-334</pages>
			<resume>En s’appuyant sur la notion d’arbre de dérivation des Grammaires d’Arbres Adjoints (TAG), cet article propose deux objectifs : d’une part rendre l’interface entre syntaxe et sémantique indépendante du langage de représentation sémantique utilisé, et d’autre part offrir un noyau qui permette le traitement sémantique des ambiguïtés de portée de quantificateurs sans utiliser de langage de représentation sous-spécifiée.</resume>
			<mots_cles>interface syntaxe et sémantique, sémantique formelle, grammaires d’arbres adjoints, grammaires catégorielles</mots_cles>
			<title></title>
			<abstract>Relying on the derivation tree of the Tree Adjoining Grammars (TAG), this paper has to goals : on the one hand, to make the syntax/semantics interface independant from the semantic representation language, and on the other hand to propose an architecture that enables the modeling of scope ambguities without using underspecified representation formalisms.</abstract>
			<keywords>syntax/semantics interface, formal semantics, tree adjoining grammars, categorial grammars</keywords>
		</article>
		<article id="taln-2007-long-031" session="Syntaxe">
			<auteurs>
				<auteur>
					<nom>Ingrid Falk</nom>
					<email>Ingrid.Falk-@loria.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Gil Francopoulo</nom>
					<email>Gil.Francopoulo@wanadoo.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Claire Gardent</nom>
					<email>Claire.Gardent@loria.fr</email>
					<affiliationId>3</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">CNRS/ATILF, Nancy</affiliation>
				<affiliation affiliationId="2">INRIA/LORIA, Nancy</affiliation>
				<affiliation affiliationId="3">CNRS/LORIA, Nancy</affiliation>
			</affiliations>
			<titre>Évaluer SYNLEX</titre>
			<type>long</type>
			<pages>335-344</pages>
			<resume>SYNLEX est un lexique syntaxique extrait semi-automatiquement des tables du LADL. Comme les autres lexiques syntaxiques du français disponibles et utilisables pour le TAL (LEFFF, DICOVALENCE), il est incomplet et n’a pas fait l’objet d’une évaluation permettant de déterminer son rappel et sa précision par rapport à un lexique de référence. Nous présentons une approche qui permet de combler au moins partiellement ces lacunes. L’approche s’appuie sur les méthodes mises au point en acquisition automatique de lexique. Un lexique syntaxique distinct de SYNLEX est acquis à partir d’un corpus de 82 millions de mots puis utilisé pour valider et compléter SYNLEX. Le rappel et la précision de cette version améliorée de SYNLEX sont ensuite calculés par rapport à un lexique de référence extrait de DICOVALENCE.</resume>
			<mots_cles>lexique syntaxique, évaluation</mots_cles>
			<title></title>
			<abstract>SYNLEX is a syntactic lexicon extracted semi-automatically from the LADL tables. Like the other syntactic lexicons for French which are both available and usable for NLP (LEFFF, DICOVALENCE), it is incomplete and its recall and precision wrt a gold standard are unknown. We present an approach which goes some way towards adressing these shortcomings. The approach draws on methods used for the automatic acquisition of syntactic lexicons. First, a new syntactic lexicon is acquired from an 82 million words corpus. This lexicon is then used to validate and extend SYNLEX. Finally, the recall and precision of the extended version of SYNLEX is computed based on a gold standard extracted from DICOVALENCE.</abstract>
			<keywords>syntactic lexicon, evaluation</keywords>
		</article>
		<article id="taln-2007-long-032" session="Morphologie">
			<auteurs>
				<auteur>
					<nom>Fathi Debili</nom>
					<email>fathi.debili@wanadoo.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Zied Ben Tahar</nom>
					<email>bentaharzied@gmail.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Emna Souissi</nom>
					<email>emna.souissi@planet.tn</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LLACAN, INALCO, CNRS, 7, rue Guy Môquet, 94801 Villejuif cedex, France</affiliation>
				<affiliation affiliationId="2">ESSTT, 5, Avenue Taha Hussein – 1008 Tunis</affiliation>
			</affiliations>
			<titre>Analyse automatique vs analyse interactive : un cercle vertueux pour la voyellation, l’étiquetage et la lemmatisation de l’arabe</titre>
			<type>long</type>
			<pages>347-356</pages>
			<resume>Comment produire de façon massive des textes annotés dans des conditions d’efficacité, de reproductibilité et de coût optimales ? Plutôt que de corriger les sorties d’analyse automatique moyennant des outils d’éditions éventuellement dédiés, ainsi qu’il estcommunément préconisé, nous proposons de recourir à des outils d’analyse interactive où la correction manuelle est au fur et à mesure prise en compte par l’analyse automatique. Posant le problème de l’évaluation de ces outils interactifs et du rendement de leur ergonomie linguistique, et proposant pour cela une métrique fondée sur le calcul du coût qu’exigent ces corrections exprimé en nombre de manipulations (frappe au clavier, clic de souris, etc.), nous montrons, au travers d’un protocole expérimental simple orienté vers la voyellation, l’étiquetage et la lemmatisation de l’arabe, que paradoxalement, les meilleures performances interactives d’un système ne sont pas toujours corrélées à ses meilleures performances automatiques. Autrement dit, que le comportement linguistique automatique le plus performant n’est pas toujours celui qui assure, dès lors qu’il y a contributions manuelles, le meilleur rendement interactif.</resume>
			<mots_cles>analyse automatique vs interactive, annotation séquentielle, parallèle, voyellation, lemmatisation, étiquetage de l’arabe, métrique pour l’évaluation de l’analyse interactive</mots_cles>
			<title></title>
			<abstract>How can we massively produce annotated texts, with optimal efficiency, reproducibility and cost? Rather than correcting the output of automatic analysis by means of possibly dedicated tools, as is currently suggested, we find it more advisable to use interactive tools for analysis, where manual editing is fed in real time into automatic analysis. We address the issue of evaluating these tools, along with their performance in terms of linguistic ergonomy, and propose a metric for calculating the cost of editing as a number of keystrokes and mouse clicks. We show, by way of a simple protocol addressing Arabic vowellation, tagging and lemmatization, that, surprisingly, the best interactive performance of a system is not always correlated to its best automatic performance. In other words, the most performing automatic linguistic behavior of a system is not always yielding the best interactive behavior, when manual editing is involved.</abstract>
			<keywords>automatic versus interactive analysis of Arabic, proposal of metrics for evaluating the interactive analysis, design and implementation of software for interactive vowellation, lemmatisation and POS-tagging of Arabic, evaluation</keywords>
		</article>
		<article id="taln-2007-long-033" session="Morphologie">
			<auteurs>
				<auteur>
					<nom>Jonas Granfeldt</nom>
					<email>Jonas.Granfeldt@rom.lu.se</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Pierre Nugues</nom>
					<email>Pierre.Nugues@cs.lth.se</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Centre de langues et de littérature, Université de Lund, S-221 00 Lund</affiliation>
				<affiliation affiliationId="2">Institut d’informatique, Institut de Technologie de Lund, S-221 00 Lund</affiliation>
			</affiliations>
			<titre>Évaluation des stades de développement en français langue étrangère</titre>
			<type>long</type>
			<pages>357-366</pages>
			<resume>Cet article décrit un système pour définir et évaluer les stades de développement en français langue étrangère. L’évaluation de tels stades correspond à l’identification de la fréquence de certains phénomènes lexicaux et grammaticaux dans la production des apprenants et comment ces fréquences changent en fonction du temps. Les problèmes à résoudre dans cette démarche sont triples : identifier les attributs les plus révélateurs, décider des points de séparation entre les stades et évaluer le degré d’efficacité des attributs et de la classification dans son ensemble. Le système traite ces trois problèmes. Il se compose d’un analyseur morphosyntaxique, appelé Direkt Profil, auquel nous avons relié un module d’apprentissage automatique. Dans cet article, nous décrivons les idées qui ont conduit au développement du système et son intérêt. Nous présentons ensuite le corpus que nous avons utilisé pour développer notre analyseur morphosyntaxique. Enfin, nous présentons les résultats sensiblement améliorés des classificateurs comparé aux travaux précédents (Granfeldt et al., 2006). Nous présentons également une méthode de sélection de paramètres afin d’identifier les attributs grammaticaux les plus appropriés.</resume>
			<mots_cles>analyseur morphosyntaxique, apprentissage automatique, acquisition des langues</mots_cles>
			<title></title>
			<abstract>This paper describes a system to define and evaluate stages of development in second language French. The task of identifying such stages can be formulated as identifying the frequency of some lexical and grammatical features in the learners’ production and how they vary over time. The problems in this procedure are threefold : identify the relevant features, decide on cutoff points for the stages, and evaluate the degree of efficiency of the attributes and of the overall classification. The system addresses these three problems. It consists of a morphosyntactic analyzer called Direkt Profil and a machine-learning module connected to it. We first describe the usefulness and rationale behind the development of the system. We then present the corpus we used to develop our morphosyntactic analyzer called Direkt Profil. Finally, we present new and substantially improved results on training machine-learning classifiers compared to previous experiments (Granfeldt et al., 2006). We also introduce a method of attribute selection in order to identify the most relevant grammatical features.</abstract>
			<keywords>morphosyntactic parser, machine learning, language acquisition</keywords>
		</article>
		<article id="taln-2007-long-034" session="Morphologie">
			<auteurs>
				<auteur>
					<nom>Delphine Bernhard</nom>
					<email>Delphine.Bernhard@imag.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">TIMC-IMAG, Institut d’Ingénierie de l’Information de Santé, Faculté de Médecine, 38706 La Tronche cedex</affiliation>
			</affiliations>
			<titre>Apprentissage non supervisé de familles morphologiques par classification ascendante hiérarchique</titre>
			<type>long</type>
			<pages>367-376</pages>
			<resume>Cet article présente un système d’acquisition de familles morphologiques qui procède par apprentissage non supervisé à partir de listes de mots extraites de corpus de textes. L’approche consiste à former des familles par groupements successifs, similairement aux méthodes de classification ascendante hiérarchique. Les critères de regroupement reposent sur la similarité graphique des mots ainsi que sur des listes de préfixes et de paires de suffixes acquises automatiquement à partir des corpus traités. Les résultats obtenus pour des corpus de textes de spécialité en français et en anglais sont évalués à l’aide de la base CELEX et de listes de référence construites manuellement. L’évaluation démontre les bonnes performances du système, indépendamment de la langue, et ce malgré la technicité et la complexité morphologique du vocabulaire traité.</resume>
			<mots_cles>familles morphologiques, classification, apprentissage non supervisé</mots_cles>
			<title></title>
			<abstract>This article describes a method for the unsupervised acquisition of morphological families using lists of words extracted from text corpora. It proceeds by incrementally grouping words in families, similarly to agglomerative hierarchical clustering methods. Clustering criteria rely on graphical similarity as well as lists of prefixes and suffix pairs which are automatically acquired from the target corpus. Results obtained for specialised text corpora in French and English are evaluated using the CELEX database and manually built reference lists. The evaluation shows that the system perfoms well for both languages, despite the morphological complexity of the technical vocabulary used for the evaluation.</abstract>
			<keywords>morphological families, clustering, unsupervised learning</keywords>
		</article>
		<article id="taln-2007-long-035" session="Discours">
			<auteurs>
				<auteur>
					<nom>Catherine Recanati</nom>
					<email>Catherine.Recanati@lipn.univ-paris13.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Nicoleta Rogovschi</nom>
					<email>Nicoleta.Rogovschi@lipn.univ-paris13.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIPN – UMR 7030 du CNRS, Institut Galilée, Université Paris 13, 99, avenue J-B. Clément, 93430 Villetaneuse, France</affiliation>
			</affiliations>
			<titre>Enchaînements verbaux – étude sur le temps et l'aspect utilisant des techniques d’apprentissage non supervisé</titre>
			<type>long</type>
			<pages>379-388</pages>
			<resume>L’apprentissage non supervisé permet la découverte de catégories initialement inconnues. Les techniques actuelles permettent d'explorer des séquences de phénomènes alors qu'on a tendance à se focaliser sur l'analyse de phénomènes isolés ou sur la relation entre deux phénomènes. Elles offrent ainsi de précieux outils pour l'analyse de données organisées en séquences, et en particulier, pour la découverte de structures textuelles. Nous présentons ici les résultats d’une première tentative de les utiliser pour inspecter les suites de verbes provenant de phrases de récits d’accident de la route. Les verbes étaient encodés comme paires (cat, temps), où cat représente la catégorie aspectuelle d’un verbe, et temps son temps grammatical. L’analyse, basée sur une approche originale, a fourni une classification des enchaînements de deux verbes successifs en quatre groupes permettant de segmenter les textes. Nous donnons ici une interprétation de ces groupes à partir de statistiques sur des annotations sémantiques indépendantes.</resume>
			<mots_cles>temps, aspect, sémantique, apprentissage non supervisé, fouille de données</mots_cles>
			<title></title>
			<abstract>Unsupervised learning allows the discovery of initially unknown categories. Current techniques make it possible to explore sequences of phenomena whereas one tends to focus on the analysis of isolated phenomena or on the relation between two phenomena. They offer thus invaluable tools for the analysis of sequential data, and in particular, for the discovery of textual structures. We report here the results of a first attempt at using them for inspecting sequences of verbs coming from sentences of French accounts of road accidents. Verbs were encoded as pairs (cat, tense) – where cat is the aspectual category of a verb, and tense its grammatical tense. The analysis, based on an original approach, provided a classification of the links between two successive verbs into four distinct groups (clusters) allowing texts segmentation. We give here an interpretation of these clusters by using statistics on semantic annotations independent of the training process.</abstract>
			<keywords>time, tense, aspect, semantics, unsupervised learning, data mining</keywords>
		</article>
		<article id="taln-2007-long-036" session="Discours">
			<auteurs>
				<auteur>
					<nom>Laurence Danlos</nom>
					<email>Laurence.Danlos@linguist.jussieu.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LATTICE – Université Paris 7, Institut Universitaire de France</affiliation>
			</affiliations>
			<titre>D-STAG : un formalisme pour le discours basé sur les TAG synchrones</titre>
			<type>long</type>
			<pages>389-398</pages>
			<resume>Nous proposons D-STAG, un formalisme pour le discours qui utilise les TAG synchrones. Les analyses sémantiques produites par D-STAG sont des structures de discours hiérarchiques annotées de relations de discours coordonnantes ou subordonnantes. Elles sont compatibles avec les structures de discours produites tant en RST qu’en SDRT. Les relations de discours coordonnantes et subordonnantes sont modélisées respectivement par les opérations de substitution et d’adjonction introduites en TAG.</resume>
			<mots_cles>discours, grammaires d’arbres adjoints (synchrones), interface syntaxe/sémantique</mots_cles>
			<title></title>
			<abstract>We propose D-STAG, a framework which uses Synchronous TAG for discourse. D-STAG semantic analyses are hierarchical discourse structures richly annotated with coordinating and subordinating discourse relations. They are compatible both with RST and SDRT discourse structures. Coordinating and subordinating relations are respectively modeled with the TAG substitution and adjunction operations.</abstract>
			<keywords>discourse, (synchronous) tree adjoining grammars, syntax/semantic interface</keywords>
		</article>
		<article id="taln-2007-long-037" session="Traduction &amp; alignement">
			<auteurs>
				<auteur>
					<nom>Violeta Seretan</nom>
					<email>Violeta.Seretan@lettres.unige.ch</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Éric Wehrli</nom>
					<email>Eric.Wehrli@lettres.unige.ch</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Language Technology Laboratory (LATL) - University of Geneva, 2 Rue de Candolle, 1211 Geneva, Switzerland</affiliation>
			</affiliations>
			<titre>Collocation translation based on sentence alignment and parsing</titre>
			<type>long</type>
			<pages>401-410</pages>
			<resume>Bien que de nombreux efforts aient été déployés pour extraire des collocations à partir de corpus de textes, seule une minorité de travaux se préoccupent aussi de rendre le résultat de l’extraction prêt à être utilisé dans les applications TAL qui pourraient en bénéficier, telles que la traduction automatique. Cet article décrit une méthode précise d’identification de la traduction des collocations dans un corpus parallèle, qui présente les avantages suivants : elle peut traiter des collocation flexibles (et pas seulement figées) ; elle a besoin de ressources limitées et d’un pouvoir de calcul raisonnable (pas d’alignement complet, pas d’entraînement) ; elle peut être appliquée à plusieurs paires des langues et fonctionne même en l’absence de dictionnaires bilingues. La méthode est basée sur l’information syntaxique provenant du parseur multilingue Fips. L’évaluation effectuée sur 4000 collocations de type verbe-objet correspondant à plusieurs paires de langues a montré une précision moyenne de 89.8% et une couverture satisfaisante (70.9%). Ces résultats sont supérieurs à ceux enregistrés dans l’évaluation d’autres méthodes de traduction de collocations.</resume>
			<mots_cles>traduction de collocations, extraction de collocations, parsing, alignement de textes</mots_cles>
			<title></title>
			<abstract>To date, substantial efforts have been devoted to the extraction of collocations from text corpora. However, only a few works deal with the subsequent processing of results in order for these to be successfully integrated into the NLP applications that could benefit from them (e.g., machine translation). This paper presents an accurate method for identifying translation equivalents of collocations in parallel text, whose main strengths are that : it can handle flexible (not only rigid) collocations ; it only requires limited resources and computation (no full alignment, no training needed) ; it deals with several language pairs, and it can even work when no bilingual dictionary is available. The method relies heavily on syntactic information provided by the Fips multilingual parser. Evaluation performed on 4000 verb-object collocations for different language pairs showed an average accuracy of 89.8% and a reasonable coverage (70.9%). These figures are higher that those reported in the evaluation of related work in collocation translation.</abstract>
			<keywords>collocation translation, collocation extraction, parsing, text alignment</keywords>
		</article>
		<article id="taln-2007-long-038" session="Traduction &amp; alignement">
			<auteurs>
				<auteur>
					<nom>Nasredine Semmar</nom>
					<email>nasredine.semmar@cea.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Christian Fluhr</nom>
					<email>christian.fluhr@cea.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">CEA, LIST, LIC2M, 18 route du Panorama, BP6, FONTENAY AUX ROSES, F- 92265 France</affiliation>
			</affiliations>
			<titre>Utilisation d’une approche basée sur la recherche cross-lingue d’information pour l’alignement de phrases à partir de textes bilingues Arabe-Français</titre>
			<type>long</type>
			<pages>411-420</pages>
			<resume>L’alignement de phrases à partir de textes bilingues consiste à reconnaître les phrases qui sont traductions les unes des autres. Cet article présente une nouvelle approche pour aligner les phrases d’un corpus parallèle. Cette approche est basée sur la recherche crosslingue d’information et consiste à construire une base de données des phrases du texte cible et considérer chaque phrase du texte source comme une requête à cette base. La recherche crosslingue utilise un analyseur linguistique et un moteur de recherche. L’analyseur linguistique traite aussi bien les documents à indexer que les requêtes et produit un ensemble de lemmes normalisés, un ensemble d’entités nommées et un ensemble de mots composés avec leurs étiquettes morpho-syntaxiques. Le moteur de recherche construit les fichiers inversés des documents en se basant sur leur analyse linguistique et retrouve les documents pertinents à partir de leur indexes. L’aligneur de phrases a été évalué sur un corpus parallèle Arabe-Français et les résultats obtenus montrent que 97% des phrases ont été correctement alignées.</resume>
			<mots_cles>alignement de phrases, corpus parallèle, recherche cross-lingue d’information</mots_cles>
			<title></title>
			<abstract>Sentence alignment consists in identifying correspondences between sentences in one language and sentences in the other language. This paper describes a new approach to aligning sentences from a parallel corpora. This approach is based on cross-language information retrieval and consists in building a database of sentences of the target text and considering each sentence of the source text as a query to that database. Cross-language information retrieval uses a linguistic analyzer and a search engine. The linguistic analyzer processes both documents to be indexed and queries to produce a set of normalized lemmas, a set of named entities and a set of nominal compounds with their morpho-syntactic tags. The search engine builds the inverted files of the documents on the basis of their linguistic analysis and retrieves the relevant documents from the indexes. An evaluation of the sentence aligner was performed based on a Arabic to French parallel corpus and results show that 97% of sentences were correctly aligned.</abstract>
			<keywords>sentence alignment, parallel corpora, cross-lingual information retrieval</keywords>
		</article>
		<article id="taln-2007-poster-001" session="Posters">
			<auteurs>
				<auteur>
					<nom>Laurent Audibert</nom>
					<email>laurent.audibert@lipn.univ-paris13.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Laboratoire d’Informatique de l’université Paris-Nord (LIPN), 99, avenue Jean-Baptiste Clément – 93430 Villetaneuse, France</affiliation>
			</affiliations>
			<titre>Désambiguïsation lexicale automatique : sélection automatique d’indices</titre>
			<type>poster</type>
			<pages>13-22</pages>
			<resume>Nous exposons dans cet article une expérience de sélection automatique des indices du contexte pour la désambiguïsation lexicale automatique. Notre point de vue est qu’il est plus judicieux de privilégier la pertinence des indices du contexte plutôt que la sophistication des algorithmes de désambiguïsation utilisés. La sélection automatique des indices par le biais d’un algorithme génétique améliore significativement les résultats obtenus dans nos expériences précédentes tout en confortant des observations que nous avions faites sur la nature et la répartition des indices les plus pertinents.</resume>
			<mots_cles>désambiguïsation lexicale automatique, corpus sémantiquement étiqueté, cooccurrences, sélection d’indices, algorithmes génétiques</mots_cles>
			<title></title>
			<abstract>This article describes an experiment on automatic features selection for word sense disambiguation. Our point of view is that word sense disambiguation success is more dependent on the features used to represent the context in which an ambiguous word occurs than on the sophistication of the learning techniques used. Automatic features selection using a genetic algorithm improves significantly our last experiment bests results and is consistent with the observations we have made on the nature and space distribution of the most reliable features.</abstract>
			<keywords>word sense disambiguation, sense tagged corpora, cooccurrences, features selection, genetic algorithms</keywords>
		</article>
		<article id="taln-2007-poster-002" session="Posters">
			<auteurs>
				<auteur>
					<nom>Delphine Battistelli</nom>
					<email>Delphine.Battistelli@paris4.sorbonne.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Marie Chagnoux</nom>
					<email>Marie.Chagnoux@free.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Lalic – Université Paris IV-Sorbonne, 28 rue Serpente 75006 Paris</affiliation>
				<affiliation affiliationId="2">France Télécom – Div. R&amp;D, TECH/EASY/Langues Naturelles, 2 avenue Pierre Marzin, 22307 Lannion Cedex</affiliation>
			</affiliations>
			<titre>Représenter la dynamique énonciative et modale de textes</titre>
			<type>poster</type>
			<pages>23-32</pages>
			<resume>Nous proposons d’exposer ici une méthodologie d’analyse et de représentation d’une des composantes de la structuration des textes, celle liée à la notion de prise en charge énonciative. Nous mettons l’accent sur la structure hiérarchisée des segments textuels qui en résulte ; nous la représentons d’une part sous forme d’arbre et d’autre part sous forme de graphe. Ce dernier permet d’appréhender la dynamique énonciative et modale de textes comme un cheminement qui s’opère entre différents niveaux de discours dans un texte au fur et à mesure de sa lecture syntagmatique.</resume>
			<mots_cles>linguistique textuelle, énonciation, représentation sémantique</mots_cles>
			<title></title>
			<abstract>We propose a methodological framework for analyzing and representing the concept of commitment, which is one of the features characterizing textual structure. We emphasize the hierarchical structure of textual segments commitment conveys to. We represent it first as a tree and then as a graph. The latter enables us to access the modal and enunciative textual dynamics, as it shows the path followed through different discursive levels during the syntagmatic reading of a text.</abstract>
			<keywords>textual linguistics, enunciation, semantical representation</keywords>
		</article>
		<article id="taln-2007-poster-003" session="Posters">
			<auteurs>
				<auteur>
					<nom>Olivier Blanc</nom>
					<email>oblanc@univ-mlv.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Matthieu Constant</nom>
					<email>mconstan@univ-mlv.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Patrick Watrin</nom>
					<email>watrin@univ-mlv.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">IGM, Université de Marne-la-Vallée &amp; CNRS</affiliation>
			</affiliations>
			<titre>Segmentation en super-chunks</titre>
			<type>poster</type>
			<pages>33-42</pages>
			<resume>Depuis l’analyseur développé par Harris à la fin des années 50, les unités polylexicales ont peu à peu été intégrées aux analyseurs syntaxiques. Cependant, pour la plupart, elles sont encore restreintes aux mots composés qui sont plus stables et moins nombreux. Toutefois, la langue est remplie d’expressions semi-figées qui forment également des unités sémantiques : les expressions adverbiales et les collocations. De même que pour les mots composés traditionnels, l’identification de ces structures limite la complexité combinatoire induite par l’ambiguïté lexicale. Dans cet article, nous détaillons une expérience qui intègre ces notions dans un processus de segmentation en super-chunks, préalable à l’analyse syntaxique. Nous montrons que notre chunker, développé pour le français, atteint une précision et un rappel de 92,9 % et 98,7 %, respectivement. Par ailleurs, les unités polylexicales réalisent 36,6 % des attachements internes aux constituants nominaux et prépositionnels.</resume>
			<mots_cles>chunker, super-chunks, analyse syntaxique, patrons lexico-syntaxiques</mots_cles>
			<title></title>
			<abstract>Since Harris’ parser in the late 50’s, multiword units have been progressively integrated in parsers. Nevertheless, in the most part, they are still restricted to compound words, that are more stable and less numerous. Actually, language is full of semi-frozen expressions that also form basic semantic units : semi-frozen adverbial expressions (e.g. time), collocations. Like compounds, the identification of these structures limits the combinatorial complexity induced by lexical ambiguity. In this paper, we detail an experiment that largely integrates these notions in a procedure of segmentation into super-chunks, preliminary to a parser. We show that the chunker, developped for French, reaches 92.9% precision and 98.7% recall. Moreover, multiword units realize 36.6% of the attachments within nominal and prepositional phrases.</abstract>
			<keywords>chunker, super-chunks, syntactic analysis, lexico-syntactic patterns</keywords>
		</article>
		<article id="taln-2007-poster-004" session="Posters">
			<auteurs>
				<auteur>
					<nom>Narjès Boufaden</nom>
					<email>Narjes.Boufaden@crim.ca</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Truong Le Hoang</nom>
					<email>LeHoang.Truong@crim.ca</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Pierre Dumouchel</nom>
					<email>Pierre.Dumouchel@crim.ca</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">École de Technologie Supérieure et Centre de Recherche Informatique de Montréal</affiliation>
			</affiliations>
			<titre>Détection et prédiction de la satisfaction des usagers dans les dialogues Personne-Machine</titre>
			<type>poster</type>
			<pages>43-52</pages>
			<resume>Nous étudions le rôle des entités nommées et marques discursives de rétroaction pour la tâche de classification et prédiction de la satisfaction usager à partir de dialogues. Les expériences menées sur 1027 dialogues Personne-Machine dans le domaine des agences de voyage montrent que les entités nommées et les marques discursives n’améliorent pas de manière significative le taux de classification des dialogues. Par contre, elles permettent une meilleure prédiction de la satisfaction usager à partir des premiers tours de parole usager.</resume>
			<mots_cles>prédiction de la satisfaction usager, classification des dialogues Personne-Machine</mots_cles>
			<title></title>
			<abstract>We study the usefulness of named entities and acknowldgment words for user satisfaction classification and prediction from Human-Computer dialogs. We show that named entities and acknowledgment words do not enhance baseline classification performance. However, they allow a better prediction of user satisfaction in the beginning of the dialogue.</abstract>
			<keywords>prediction of user satisfaction, Human-Computer dialog classification</keywords>
		</article>
		<article id="taln-2007-poster-005" session="Posters">
			<auteurs>
				<auteur>
					<nom>Pierrette Bouillon</nom>
					<email>Pierrette.Bouillon@issco.unige.ch</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Manny Rayner</nom>
					<email>Emmanuel.Rayner@issco.unige.ch</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Marianne Starlander</nom>
					<email>Marianne.Starlander@eti.unige.ch</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Marianne Santaholma</nom>
					<email>Marianne.Santaholma@eti.unige.ch</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">University of Geneva, TIM/ISSCO, 40, bvd du Pont-d’Arve, CH-1211 Geneva 4, Switzerland</affiliation>
				<affiliation affiliationId="2">Powerset Inc, 475 Brannan Street, San Francisco, CA 94107, US</affiliation>
			</affiliations>
			<titre>Les ellipses dans un système de traduction automatique de la parole</titre>
			<type>poster</type>
			<pages>53-62</pages>
			<resume>Dans tout dialogue, les phrases elliptiques sont très nombreuses. Dans cet article, nous évaluons leur impact sur la reconnaissance et la traduction dans le système de traduction automatique de la parole MedSLT. La résolution des ellipses y est effectuée par une méthode robuste et portable, empruntée aux systèmes de dialogue homme-machine. Cette dernière exploite une représentation sémantique plate et combine des techniques linguistiques (pour construire la représentation) et basées sur les exemples (pour apprendre sur la base d’un corpus ce qu’est une ellipse bien formée dans un sous-domaine donné et comment la résoudre).</resume>
			<mots_cles>traduction automatique de la parole, reconnaissance de la parole, ellipses, évaluation, traitement du dialogue, modèle du language fondé sur les grammaire</mots_cles>
			<title></title>
			<abstract>Elliptical phrases are frequent in all genres of dialogue. In this paper, we describe an evaluation of the speech understanding component of the MedSLT medical speech translation system, which focusses on the contrast between system performance on elliptical phrases and full utterances. Ellipsis resolution in the system is handled by a robust and portable method, adapted from similar methods commonly used in spoken dialogue systems, which exploits the flat representation structures used. The resolution module combines linguistic methods, used to construct the representations, with an example-based approach to defining the space of well-formed ellipsis resolutions in a subdomain.</abstract>
			<keywords>speech recognition, speech translation, ellipsis, dialogue processing, grammar-based language modelling, evaluation</keywords>
		</article>
		<article id="taln-2007-poster-006" session="Posters">
			<auteurs>
				<auteur>
					<nom>Nathalie Camelin</nom>
					<email>nathalie.camelin@univ-avignon.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Frédéric Béchet</nom>
					<email>frederic.bechet@univ-avignon.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Géraldine Damnati</nom>
					<email>geraldine.damnati@francetelecom.com</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Renato De Mori</nom>
					<email>renato.demori@univ-avignon.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIA/CNRS, University of Avignon, BP1228, 84911 Avignon cedex 09 France</affiliation>
				<affiliation affiliationId="2">France Télécom R&amp;D – TECH/SSTP/RVA, 2 av. Pierre Marzin, 22307 Lannion Cedex 07, France</affiliation>
			</affiliations>
			<titre>Analyse automatique de sondages téléphoniques d’opinion</titre>
			<type>poster</type>
			<pages>63-72</pages>
			<resume>Cette étude présente la problématique de l’analyse automatique de sondages téléphoniques d’opinion. Cette analyse se fait en deux étapes : tout d’abord extraire des messages oraux les expressions subjectives relatives aux opinions de utilisateurs sur une dimension particulière (efficacité, accueil, etc.) ; puis sélectionner les messages fiables, selon un ensemble de mesures de confiance, et estimer la distribution des diverses opinions sur le corpus de test. Le but est d’estimer une distribution aussi proche que possible de la distribution de référence. Cette étude est menée sur un corpus de messages provenant de vrais utilisateurs fournis par France Télécom R&amp;D.</resume>
			<mots_cles>détection d’opinions, classification automatique, reconnaissance automatique de la parole, champs conditionnels aléatoires</mots_cles>
			<title></title>
			<abstract>This paper introduces the context of the automatic analysis of opinion telephone surveys. This analysis is done by means of two stages : firstly the subjective expressions, related to the expression of an opinion on a particular dimension (efficiency, courtesy, . . . ), are extracted from the audio messages ; secondly the reliable messages, according to a set of confidence measures, are selected and the distribution of the positive and negative opinions in these messages is estimated. The goal is to obtain a distribution as close as possible to the reference one. This study is carried on a telephone survey corpus, provided by France Télécom R&amp;D, obtained in real field conditions.</abstract>
			<keywords>opinion extraction, automatic classification, automatic speech recognition, conditional random fields</keywords>
		</article>
		<article id="taln-2007-poster-007" session="Posters">
			<auteurs>
				<auteur>
					<nom>Claire Gardent</nom>
					<email>Claire.Gardent@loria.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Éric Kow</nom>
					<email>Eric.Kow@loria.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">CNRS/LORIA, Nancy</affiliation>
				<affiliation affiliationId="2">INRIA/LORIA, Nancy</affiliation>
			</affiliations>
			<titre>Une réalisateur de surface basé sur une grammaire réversible</titre>
			<type>poster</type>
			<pages>73-82</pages>
			<resume>En génération, un réalisateur de surface a pour fonction de produire, à partir d’une représentation conceptuelle donnée, une phrase grammaticale. Les réalisateur existants soit utilisent une grammaire réversible et des méthodes statistiques pour déterminer parmi l’ensemble des sorties produites la plus plausible ; soit utilisent des grammaires spécialisées pour la génération et des méthodes symboliques pour déterminer la paraphrase la plus appropriée à un contexte de génération donné. Dans cet article, nous présentons GENI, un réalisateur de surface basé sur une grammaire d’arbres adjoints pour le français qui réconcilie les deux approches en combinant une grammaire réversible avec une sélection symbolique des paraphrases.</resume>
			<mots_cles>réalisation de surface, grammaire d’arbres adjoints, réversibilité</mots_cles>
			<title></title>
			<abstract>In generation, a surface realiser takes as input a conceptual representation and outputs a grammatical sentence. Existing realisers fall into two camps. Either they are based on a reversible grammar and use statistical filtering to determine among the several outputs the most plausible one. Or they combine a grammar tailored for generation and a symbolic means of choosing the paraphrase most appropriate to a given generation context. In this paper, we present GENI, a surface realiser based on a Tree Adjoining Grammar for French which reconciles both approaches in that (i) the grammar used is réversible and (ii) paraphrase selection is based on symbolic means.</abstract>
			<keywords>surface realisation, tree adjoining grammar, reversibility</keywords>
		</article>
		<article id="taln-2007-poster-008" session="Posters">
			<auteurs>
				<auteur>
					<nom>Laurent Gillard</nom>
					<email>laurent.gillard@univ-avignon.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Patrice Bellot</nom>
					<email>patrice.bellot@univ-avignon.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Marc El-Bèze</nom>
					<email>marc.elbeze@univ-avignon.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Laboratoire d'Informatique d'Avignon (LIA), Université d’Avignon et des Pays de Vaucluse, 339 ch. des Meinajaries, BP 1228, F-84911 Avignon Cedex 9 (France)</affiliation>
			</affiliations>
			<titre>Analyse des échecs d’une approche pour traiter les questions définitoires soumises à un système de questions/réponses</titre>
			<type>poster</type>
			<pages>83-92</pages>
			<resume>Cet article revient sur le type particulier des questions définitoires étudiées dans le cadre des campagnes d’évaluation des systèmes de Questions/Réponses. Nous présentons l’approche développée suite à notre participation à la campagne EQueR et son évaluation lors de QA@CLEF 2006. La réponse proposée est la plus représentative des expressions présentes en apposition avec l’objet à définir, sa sélection est faite depuis des indices dérivés de ces appositions. Environ 80% de bonnes réponses sont trouvées sur les questions définitoires des volets francophones de CLEF. Les cas d’erreurs rencontrés sont analysés et discutés en détail.</resume>
			<mots_cles>système de questions/réponses, questions définitoires</mots_cles>
			<title></title>
			<abstract>This paper proposes an approach to deal with definitional question answering. Our system extracts answers to these questions from appositives appearing closed to the subject to define. Results are presented for CLEF campaigns. Next, failures are discussed.</abstract>
			<keywords>question answering, definitional question answering</keywords>
		</article>
		<article id="taln-2007-poster-009" session="Posters">
			<auteurs>
				<auteur>
					<nom>Lorraine Goeuriot</nom>
					<email>lorraine.goeuriot@univ-nantes.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Natalia Grabar</nom>
					<email>natalia.grabar@biomath.jussieu.fr</email>
					<affiliationId>2</affiliationId>
					<affiliationId>3</affiliationId>
				</auteur>
				<auteur>
					<nom>Béatrice Daille</nom>
					<email>beatrice.daille@univ-nantes.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LINA/Nantes</affiliation>
				<affiliation affiliationId="2">INSERM, UMR_S 872, Eq. 20, Paris, F-75006, Université René Descartes, Paris, F-75006</affiliation>
				<affiliation affiliationId="3">Health on the Net Foundation, SIM/HUG, Genève, Suisse</affiliation>
			</affiliations>
			<titre>Caractérisation des discours scientifiques et vulgarisés en français, japonais et russe</titre>
			<type>poster</type>
			<pages>93-102</pages>
			<resume>L’objectif principal de notre travail consiste à étudier la notion de comparabilité des corpus, et nous abordons cette question dans un contexte monolingue en cherchant à distinguer les documents scientifiques et vulgarisés. Nous travaillons séparément sur des corpus composés de documents du domaine médical dans trois langues à forte distance linguistique (le français, le japonais et le russe). Dans notre approche, les documents sont caractérisés dans chaque langue selon leur thématique et une typologie discursive qui se situe à trois niveaux de l’analyse des documents : structurel, modal et lexical. Le typage des documents est implémenté avec deux algorithmes d’apprentissage (SVMlight et C4.5). L’évaluation des résultats montre que la typologie discursive proposée est portable d’une langue à l’autre car elle permet en effet de distinguer les deux discours. Nous constatons néanmoins des performances très variées selon les langues, les algorithmes et les types de caractéristiques discursives.</resume>
			<mots_cles>linguistique des corpus, corpus comparable, algorithmes d’apprentissage, analyse stylistique, degré de comparabilité</mots_cles>
			<title></title>
			<abstract>The main objective of our study consists to characterise the comparability of corpora, and we address this issue in the monolingual context through the disctinction of expert and non expert documents. We work separately with corpora composed of medical area documents in three languages, which show an important linguistic distance between them (French, Japanese and Russian). In our approach, documents are characterised in each language through their thematic topic and through a discursive typology positioned at three levels of document analysis : structural, modal and lexical. The document typology is implemented with two learning algorithms (SVMlight and C4.5). Evaluation of results shows that the proposed discursive typology can be transposed from one language to another, as it indeed allows to distinguish the two aimed discourses. However, we observe that performances vary a lot according to languages, algorithms and types of discursive characteristics.</abstract>
			<keywords>corpus linguistics, comparable corpora, learning algorithms, stylistic analysis, degree of comparability</keywords>
		</article>
		<article id="taln-2007-poster-010" session="Posters">
			<auteurs>
				<auteur>
					<nom>Thierry Hamon</nom>
					<email>Thierry.Hamon@lipn.univ-paris13.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Julien Derivière</nom>
					<email>Julien.Derivière@lipn.univ-paris13.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Adeline Nazarenko</nom>
					<email>Adeline.Nazarenko@lipn.univ-paris13.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIPN – UMR CNRS 7030, 99 av. J.B. Clément, F-93430 Villetaneuse, FRANCE</affiliation>
			</affiliations>
			<titre>OGMIOS : une plate-forme d’annotation linguistique de collection de documents issus du Web</titre>
			<type>poster</type>
			<pages>103-112</pages>
			<resume>L’un des objectifs du projet ALVIS est d’intégrer des informations linguistiques dans des moteurs de recherche spécialisés. Dans ce contexte, nous avons conçu une plate-forme d’enrichissement linguistique de documents issus du Web, OGMIOS, exploitant des outils de TAL existants. Les documents peuvent être en français ou en anglais. Cette architecture est distribuée, afin de répondre aux contraintes liées aux traitements de gros volumes de textes, et adaptable, pour permettre l’analyse de sous-langages. La plate-forme est développée en Perl et disponible sous forme de modules CPAN. C’est une structure modulaire dans lequel il est possible d’intégrer de nouvelles ressources ou de nouveaux outils de TAL. On peut ainsi définir des configuration différentes pour différents domaines et types de collections. Cette plateforme robuste permet d’analyser en masse des données issus du web qui sont par essence très hétérogènes. Nous avons évalué les performances de la plateforme sur plusieurs collections de documents. En distribuant les traitements sur vingt machines, une collection de 55 329 documents du domaine de la biologie (106 millions de mots) a été annotée en 35 heures tandis qu’une collection de 48 422 dépêches relatives aux moteurs de recherche (14 millions de mots) a été annotée en 3 heures et 15 minutes.</resume>
			<mots_cles>plateforme d’annotation linguistique, passage à l’échelle, robustesse</mots_cles>
			<title></title>
			<abstract>In the context of the ALVIS project, which aims at integrating linguistic information in topic-specific search engines, we developed an NLP architecture, OGMIOS, to linguistically annotate large collections of web documents with existing NLP tools. Documents can be written in French or English. The distributed architecture allows us to take into account constraints related to the scalability problem of Natural Language Processing and the domain specific tuning of the linguistic analysis. The platform is developed in Perl and is available as CPAN modules. It is a modularized framework where new resources or NLP tools can be integrated. Then, various configurations are easy to define for various domains and collections. This platform is robust to massively analyse web document collections which are heterogeneous in essence. We carried out experiments on two different collections of web documents on 20 computers. A 55,329 web documents collection dealing with biology (106 millions of words) has been annotated in 35 hours, whereas a 48,422 search engine news collection (14 millions of word) has been annotated in 3 hours and 15 minutes.</abstract>
			<keywords>linguistic annotation, NLP platform, process scability, robustess</keywords>
		</article>
		<article id="taln-2007-poster-011" session="Posters">
			<auteurs>
				<auteur>
					<nom>Sébastien Haton</nom>
					<email>sebastien.haton@atilf.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Jean-Marie Pierrel</nom>
					<email>jean-marie.pierrel@atilf.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Laboratoire ATILF Nancy Université – CNRS, 44 avenue de la Libération, BP 30687, 54063 Nancy CEDEX</affiliation>
			</affiliations>
			<titre>Les Lexiques-Miroirs. Du dictionnaire bilingue au graphe multilingue</titre>
			<type>poster</type>
			<pages>113-122</pages>
			<resume>On observe dans les dictionnaires bilingues une forte asymétrie entre les deux parties d’un même dictionnaire et l’existence de traductions et d’informations « cachées », i.e. pas directement visibles à l’entrée du mot à traduire. Nous proposons une méthodologie de récupération des données cachées ainsi que la « symétrisation » du dictionnaire grâce à un traitement automatique. L’étude d’un certain nombre de verbes et de leurs traductions en plusieurs langues a conduit à l’intégration de toutes les données, visibles ou cachées, au sein d’une base de données unique et multilingue. L’exploitation de la base de données a été rendue possible par l’écriture d’un algorithme de création de graphe synonymique qui lie dans un même espace les mots de langues différentes. Le programme qui en découle permettra de générer des dictionnaires paramétrables directement à partir du graphe.</resume>
			<mots_cles>dictionnaires bilingues, traduction automatique, graphe multilingue, algorithme, polysémie verbale, dissymétrie lexicographique</mots_cles>
			<title></title>
			<abstract>Lexical asymmetry and hidden data, i.e. not directly visible into one lexical entry, are phenomena peculiar to most of the bilingual dictionaries. Our purpose is to establish a methodology to highlight both phenomena by extracting hidden data from the dictionary and by re-establishing symmetry between its two parts. So we studied a large number of verbs and integrated them into a unique multilingual database. At last, our database is turned into a “multilexical” graph thanks to an algorithm, which is binding together words from different languages into the same semantic space. This will allow us to generate automatically dictionaries straight from the graph.</abstract>
			<keywords>bilingual dictionaries, machine translation, multilingual graph, algorithm, verbal polysemy, lexicographical asymmetry</keywords>
		</article>
		<article id="taln-2007-poster-012" session="Posters">
			<auteurs>
				<auteur>
					<nom>Sylvain Kahane</nom>
					<email>sk@ccr.jussieu.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Modyco, Université Paris 10 - Nanterre &amp; CNRS</affiliation>
			</affiliations>
			<titre>Traduction, restructurations syntaxiques et grammaires de correspondance</titre>
			<type>poster</type>
			<pages>123-132</pages>
			<resume>Cet article présente une nouvelle formalisation du modèle de traduction par transfert de la Théorie Sens-Texte. Notre modélisation utilise les grammaires de correspondance polarisées et fait une stricte séparation entre les modèles monolingues, un lexique bilingue minimal et des règles de restructuration universelles, directement associées aux fonctions lexicales syntaxiques.</resume>
			<mots_cles>traduction automatique, paraphrase, restructuration syntaxique, TST (Théorie Sens-Texte), grammaire de dépendance, fonction lexicale, lexique bilingue, GUP (Grammaire d’Unification Polarisée), grammaire de correspondance, grammaires synchrones</mots_cles>
			<title></title>
			<abstract>This paper presents a new formalisation of transfer-based translation model of the Meaning-Text Theory. Our modelling is based on polarized correspondence grammars and observes a strict separation between monolingual models, the bilingual lexicon and universal restructuring rules, directly associated with syntactic lexical functions.</abstract>
			<keywords>machine translation, paraphrase, syntactic restructuring, MTT (Meaning-Text Theory), dependency grammar, lexical function, bilingual lexicon, PUG (Polarized Unification Grammar), correspondence grammar, synchronous grammars</keywords>
		</article>
		<article id="taln-2007-poster-013" session="Posters">
			<auteurs>
				<auteur>
					<nom>Aïda Khemakhem</nom>
					<email>khemakhem.aida@gnet.tn</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Bilel Gargouri</nom>
					<email>Bilel.Gargouri@fsegs.rnu.tn</email>
					<affiliationId></affiliationId>
				</auteur>
				<auteur>
					<nom>Abdelhamid Abdelwahed</nom>
					<email>abdelhamid.abdelwahed@yahoo.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Gil Francopoulo</nom>
					<email>gil.francopoulo@wanadoo.fr</email>
					<affiliationId>3</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Laboratoire MIRACL, FSEG-SFAX B.P. 1088, 3018 SFAX – TUNISIE</affiliation>
				<affiliation affiliationId="2">Unité de recherche LSCA, FLSH-SFAX B.P. 553, 3018 SFAX – TUNISIE</affiliation>
				<affiliation affiliationId="3">INRIA-Loria</affiliation>
			</affiliations>
			<titre>Modélisation des paradigmes de flexion des verbes arabes selon la norme LMF - ISO 24613</titre>
			<type>poster</type>
			<pages>133-142</pages>
			<resume>Dans cet article, nous spécifions les paradigmes de flexion des verbes arabes en respectant la version 9 de LMF (Lexical Markup Framework), future norme ISO 24613 qui traite de la standardisation des bases lexicales. La spécification de ces paradigmes se fonde sur une combinaison des racines et des schèmes. En particulier, nous mettons en relief les terminaisons de racines sensibles aux ajouts de suffixes et ce, afin de couvrir les situations non considérées dans les travaux existants. L’élaboration des paradigmes de flexion verbale que nous proposons est une description en intension d'ArabicLDB (Arabic Lexical DataBase) qui est une base lexicale normalisée pour la langue arabe. Nos travaux sont illustrés par la réalisation d’un conjugueur des verbes arabes à partir d'ArabicLDB.</resume>
			<mots_cles>langue arabe, paradigmes de flexion verbale, base lexicale, norme ISO 24613, LMF, lexical markup framework, conjugueur des verbes arabes</mots_cles>
			<title></title>
			<abstract>In this paper, we specify the inflected paradigms of Arabic verbs with respect to the version 9 of LMF (Lexical Markup Framework) which is the expected ISO 24613 standard dealing with the standardisation of lexical databases. The specification of these paradigms is based on a combination of schemes and roots. In particular, we highlight the role of root endings that is not considered in other researches and that may generate erroneous forms while concatenating suffixes. The development of verbal inflected paradigms that we propose is an intentional component of ArabicLDB (Arabic Lexical DataBase) which is a normalized Arabic lexical database that we developed according to LMF. Our works are illustrated by the realization of a conjugation tool for Arabic verbs using ArabicLDB.</abstract>
			<keywords>Arabic, inflected paradigms of verb, lexical database, norm ISO 24613, LMF, lexical markup framework, conjugation of arabic verbs</keywords>
		</article>
		<article id="taln-2007-poster-014" session="Posters">
			<auteurs>
				<auteur>
					<nom>Olivier Kraif</nom>
					<email>Olivier.Kraif@u-grenoble3.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Claude Ponton</nom>
					<email>Claude.Ponton@u-grenoble3.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIDILEM - Laboratoire de linguistique et didactique des langues étrangères et maternelles (http://www.u-grenoble3.fr/lidilem/labo)</affiliation>
			</affiliations>
			<titre>Du bruit, du silence et des ambiguïtés : que faire du TAL pour l'apprentissage des langues ?</titre>
			<type>poster</type>
			<pages>143-152</pages>
			<resume>Nous proposons une nouvelle approche pour l’intégration du TAL dans les systèmes d’apprentissage des langues assisté par ordinateur (ALAO), la stratégie « moinsdisante ». Cette approche tire profit des technologies élémentaires mais fiables du TAL et insiste sur la nécessité de traitements modulaires et déclaratifs afin de faciliter la portabilité et la prise en main didactique des systèmes. Basé sur cette approche, ExoGen est un premier prototype pour la génération automatique d’activités lacunaires ou de lecture d’exemples. Il intègre un module de repérage et de description des réponses des apprenants fondé sur la comparaison entre réponse attendue et réponse donnée. L’analyse des différences graphiques, orthographiques et morphosyntaxiques permet un diagnostic des erreurs de type fautes d’orthographe, confusions, problèmes d’accord, de conjugaison, etc. La première évaluation d’ExoGen sur un extrait du corpus d’apprenants FRIDA produit des résultats prometteurs pour le développement de cette approche « moins-disante », et permet d'envisager un modèle d'analyse performant et généralisable à une grande variété d'activités.</resume>
			<mots_cles>ALAO, apprentissage des langues, diagnostic d'erreur, feed-back d'erreur</mots_cles>
			<title></title>
			<abstract>This paper presents the so-called "moins-disante" strategy, a new approach for NLP integrating in Computer Assisted Language Learning (CALL) systems. It is based on the implementation of basic but reliable NLP techniques, and put emphasis on declarative and modular processing, for the sake of portability and didactic implementation. Based on this approach, ExoGen is a prototype for generating activities such as gap filling exercises. It integrates a module for error detection and description, which checks learners' answers against expected ones. Through the analysis of graphic, orthographic and morphosyntactic differences, it is able to diagnose problems like spelling errors, lexical mix-up, error prone agreement, bad conjugations, etc. The first evaluation of ExoGen outputs, based on the FRIDA learner corpus, has yielded very promising results, paving the way for the development of an efficient and general model tailored to a wide variety of activities.</abstract>
			<keywords>CALL, language learning, error diagnosis, error feedback</keywords>
		</article>
		<article id="taln-2007-poster-015" session="Posters">
			<auteurs>
				<auteur>
					<nom>Anna Kupsc</nom>
					<email>akupsc@univ-paris3.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université Paris3 / LLF, UMR 7110 et Académie Polonaise des Sciences</affiliation>
			</affiliations>
			<titre>Extraction automatique de cadres de sous-catégorisation verbale pour le français à partir d’un corpus arboré</titre>
			<type>poster</type>
			<pages>153-162</pages>
			<resume>Nous présentons une expérience d’extraction automatique des cadres de souscatégorisation pour 1362 verbes français. Nous exploitons un corpus journalistique richement annoté de 15 000 phrases dont nous extrayons 12 510 occurrences verbales. Nous évaluons dans un premier temps l’extraction des cadres basée sur la fonction des arguments, ce qui nous fournit 39 cadres différents avec une moyenne de 1.54 cadres par lemme. Ensuite, nous adoptons une approche mixte (fonction et catégorie syntaxique) qui nous fournit dans un premier temps 925 cadres différents, avec une moyenne de 3.44 cadres par lemme. Plusieurs méthodes de factorisation, neutralisant en particulier les variantes de réalisation avec le passif ou les pronoms clitiques, sont ensuite appliquées et nous permettent d’aboutir à 235 cadres différents avec une moyenne de 1.94 cadres par verbe. Nous comparons brièvement nos résultats avec les travaux existants pour le français et pour l’anglais.</resume>
			<mots_cles>français, corpus arboré, sous-catégorisation verbale, lexique-grammaire</mots_cles>
			<title></title>
			<abstract>We present our work on automatic extraction of subcategorisation frames for 1362 French verbs. We use a treebank of 15000 sentences from which we extract 12510 verb occurrences. We evaluate the results based on a functional representation of frames and we acquire 39 different frames, 1.54 per lemma on average. Then, we adopt a mixed representation (functions and categories), which leads to 925 different frames, 3.44 frames on average. We investigate several methods to reduce the ambiguity (e.g., neutralisation of passive forms or clitic arguments), which allows us to arrive at 235 frames, with 1.94 frames per lemma on average. We present a brief comparison with the existing work on French and English.</abstract>
			<keywords>French, treebank, verbal subcategorization, lexicon grammar</keywords>
		</article>
		<article id="taln-2007-poster-016" session="Posters">
			<auteurs>
				<auteur>
					<nom>François Lareau</nom>
					<email>francois.lareau@umontreal.ca</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Lattice - U. Paris 7, UFRL, Case 7003, 2 pl. Jussieu, 75251 Paris cedex 5 </affiliation>
				<affiliation affiliationId="2">OLST - U. de Montréal, Ling., CP 6128 succ C.-V., Montréal QC, H3C 3J7</affiliation>
			</affiliations>
			<titre>Vers une formalisation des décompositions sémantiques dans la Grammaire d’Unification Sens-Texte</titre>
			<type>poster</type>
			<pages>163-172</pages>
			<resume>Nous proposons une formalisation de la décomposition du sens dans le cadre de la Grammaire d’Unification Sens-Texte. Cette formalisation vise une meilleure intégration des décompositions sémantiques dans un modèle global de la langue. Elle repose sur un jeu de saturation de polarités qui permet de contrôler la construction des représentations décomposées ainsi que leur mise en correspondance avec des arbres syntaxiques qui les expriment. Le formalisme proposé est illustré ici dans une perspective de synthèse, mais il s’applique également en analyse.</resume>
			<mots_cles>Grammaire d’Unification Sens-Texte, Théorie Sens-Texte, sémantique, représentation du sens, paraphrasage</mots_cles>
			<title></title>
			<abstract>We propose a formal representation of meaning decomposition in the framework of the Meaning-Text Unification Grammar. The proposed technique aims at offering a better integration of such semantic decompositions into a global model of the language. It relies on the saturation of polarities to control the construction of decomposed respresentations as well as their mapping to the syntactic trees that express them. The proposed formalism is discussed from the viewpoint of generation, but it applies to analysis as well.</abstract>
			<keywords>Meaning-Text Unification Grammar, Meaning-Text Theory, semantics, representation of meaning, paraphrasing</keywords>
		</article>
		<article id="taln-2007-poster-017" session="Posters">
			<auteurs>
				<auteur>
					<nom>Anne-Laure Ligozat</nom>
					<email>Anne-Laure.Ligozat@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Brigitte Grau</nom>
					<email>Brigitte Grau@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Isabelle Robba</nom>
					<email>Isabelle.Robba@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Anne Vilnat</nom>
					<email>Anne.Vilnat@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIMSI-CNRS - BP 133, 91403 Orsay Cedex</affiliation>
			</affiliations>
			<titre>Systèmes de questions-réponses : vers la validation automatique des réponses</titre>
			<type>poster</type>
			<pages>173-182</pages>
			<resume>Les systèmes de questions-réponses (SQR) ont pour but de trouver une information précise extraite d’une grande collection de documents comme le Web. Afin de pouvoir comparer les différentes stratégies possibles pour trouver une telle information, il est important d’évaluer ces systèmes. L’objectif d’une tâche de validation de réponses est d’estimer si une réponse donnée par un SQR est correcte ou non, en fonction du passage de texte donné comme justification. En 2006, nous avons participé à une tâche de validation de réponses, et dans cet article nous présentons la stratégie que nous avons utilisée. Celle-ci est fondée sur notre propre système de questions-réponses. Le principe est de comparer nos réponses avec les réponses à valider. Nous présentons les résultats obtenus et montrons les extensions possibles. À partir de quelques exemples, nous soulignons les difficultés que pose cette tâche.</resume>
			<mots_cles>systèmes de questions-réponses, validation de réponses</mots_cles>
			<title></title>
			<abstract>Question answering aims at retrieving precise information from a large collection of documents, typically theWeb. Different techniques can be used to find relevant information, and to compare these techniques, it is important to evaluate question answering systems. The objective of an Answer Validation task is to estimate the correctness of an answer returned by a QA system for a question, according to the text snippet given to support it.We participated in such a task in 2006. In this article, we present our strategy for deciding if the snippets justify the answers. We used a strategy based on our own question answering system, and compared the answers it returned with the answer to judge. We discuss our results, and show the possible extensions of our strategy. Then we point out the difficulties of this task, by examining different examples.</abstract>
			<keywords>question answering, answer validation</keywords>
		</article>
		<article id="taln-2007-poster-018" session="Posters">
			<auteurs>
				<auteur>
					<nom>Huei-Chi Lin</nom>
					<email>lin_huei_chi@yahoo.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Max Silberztein</nom>
					<email>max.silberztein@univ-fcomte.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Laboratoire de Sémiolinguistique, Didactique, Informatique (LASELDI) – Université de Franche-Comté, 30 rue Mégevand 25000 Besançon</affiliation>
			</affiliations>
			<titre>Ressources lexicales chinoises pour le TALN</titre>
			<type>poster</type>
			<pages>183-192</pages>
			<resume>Nous voulons traiter des textes chinois automatiquement ; pour ce faire, nous formalisons le vocabulaire chinois, en utilisant principalement des dictionnaires et des grammaires morphologiques et syntaxiques formalisés avec le logiciel NooJ. Nous présentons ici les critères linguistiques qui nous ont permis de construire dictionnaires et grammaires, sachant que l’application envisagée (linguistique de corpus) nous impose certaines contraintes dans la formalisation des unités de la langue, en particulier des composés.</resume>
			<mots_cles>ressources linguistiques pour le chinois, linguistique de corpus, NooJ</mots_cles>
			<title></title>
			<abstract>In order to parse Chinese texts automatically, we need to formalize the Chinese vocabulary by using electronic dictionaries and morphological and syntactic grammars. We have used the NooJ software to enter the formalization. We present here the set of linguistic criteria used to construct these dictionaries and grammars, so that they can be used by corpus-linguistic applications. We focus our discussion on the characterization of Chinese linguistic units, specifically compounds.</abstract>
			<keywords>linguistic resources for chinese, corpus linguistics, NooJ</keywords>
		</article>
		<article id="taln-2007-poster-019" session="Posters">
			<auteurs>
				<auteur>
					<nom>Sinikka Loikkanen</nom>
					<email>sinikka.loikkanen@helsinki.fi</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université d’Helsinki</affiliation>
			</affiliations>
			<titre>Étiquetage morpho-syntaxique de textes kabyles</titre>
			<type>poster</type>
			<pages>193-202</pages>
			<resume>Cet article présente la construction d’un étiqueteur morpho-syntaxique développé pour annoter un corpus de textes kabyles (1 million de mots). Au sein de notre projet, un étiqueteur morpho-syntaxique a été développé et implémenté. Ceci inclut un analyseur morphologique ainsi que l’ensemble de règles de désambiguïsation qui se basent sur l’approche supervisée à base de règles. Pour effectuer le marquage, un jeu d’étiquettes morpho-syntaxiques pour le kabyle est proposé. Les résultats préliminaires sont très encourageants. Nous obtenons un taux d’étiquetage réussi autour de 97 % des textes en prose.</resume>
			<mots_cles>Étiquetage morpho-syntaxique, corpus de textes, langue kabyle, berbère</mots_cles>
			<title></title>
			<abstract>This paper describes the construction of a morpho-syntactic tagger developed to annotate our Kabyle text corpus (1 million words).Within our project, a part-of-speech tagger has been developed and implemented. That includes a morphological analyser and a set of disambiguation rules based on supervised rule-based tagging. To realise the annotation, a POS tagset for Kabyle is proposed. The first results of tests are very encouraging. At present stage, our tagger reaches 97 % of success.</abstract>
			<keywords>Part of speech tagging, text corpus, kabyle language, berber</keywords>
		</article>
		<article id="taln-2007-poster-020" session="Posters">
			<auteurs>
				<auteur>
					<nom>Athina Michou</nom>
					<email>Athina.Michou@lettres.unige.ch</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LATL – Université de Genève</affiliation>
			</affiliations>
			<titre>Analyse syntaxique et traitement automatique du syntagme nominal grec moderne</titre>
			<type>poster</type>
			<pages>203-212</pages>
			<resume>Cet article décrit le traitement automatique du syntagme nominal en grec moderne par le modèle d’analyse syntaxique multilingue Fips. L’analyse syntaxique linguistique est focalisée sur les points principaux du DP grec : l’accord entre les constituants fléchis, l’ordre flexible des constituants, la cliticisation sur les noms et le phénomène de la polydéfinitude. Il est montré comment ces phénomènes sont traités et implémentés dans le cadre de l’analyseur syntaxique FipsGreek, qui met en oeuvre un formalisme inspiré de la grammaire générative chomskyenne.</resume>
			<mots_cles>analyseur grec, analyse morphosyntaxique, syntagme nominal, grec moderne</mots_cles>
			<title></title>
			<abstract>This article describes an automatic treatment to the Modern Greek noun phrase in terms of the Fips multilingual syntactic parser. The syntactic analysis focuses on the main issues related to the Greek DP: the agreement among the inflected constituents, the relatively free constituent order, noun cliticisation, and the polydefiniteness phenomenon. The paper discusses how these processes are treated and implemented within the FipsGreek parser, which puts forth a formalism relying on Chomsky’s generative grammar.</abstract>
			<keywords>Greek parser, morphosyntactic analysis, determiner phrase, modern Greek</keywords>
		</article>
		<article id="taln-2007-poster-021" session="Posters">
			<auteurs>
				<auteur>
					<nom>Erwan Moreau</nom>
					<email>Erwan.Moreau@univ-nantes.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LINA - FRE 2729, Université de Nantes, 2 rue de la Houssinière, BP 92208, F-44322 Nantes cedex 3</affiliation>
			</affiliations>
			<titre>Apprentissage symbolique de grammaires et traitement automatique des langues</titre>
			<type>poster</type>
			<pages>213-222</pages>
			<resume>Le modèle de Gold formalise le processus d’apprentissage d’un langage. Nous présentons dans cet article les avantages et inconvénients de ce cadre théorique contraignant, dans la perspective d’applications en TAL. Nous décrivons brièvement les récentes avancées dans ce domaine, qui soulèvent selon nous certaines questions importantes.</resume>
			<mots_cles>apprentissage symbolique, modèle de Gold, grammaires catégorielles</mots_cles>
			<title></title>
			<abstract>Gold’s model formalizes the learning process of a language. In this paper we present the advantages and drawbacks of this restrictive theoretical framework, in the viewpoint of applications to NLP.We briefly describe recent advances in the domain which, in our opinion, raise some important questions.</abstract>
			<keywords>symbolic learning, Gold’s model, categorial grammars</keywords>
		</article>
		<article id="taln-2007-poster-022" session="Posters">
			<auteurs>
				<auteur>
					<nom>Yayoi Nakamura-Delloye</nom>
					<email>yayoi@free.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université Paris 7 – LATTICE, 1 rue Maurice Arnoux 92120 Montrouge</affiliation>
			</affiliations>
			<titre>Méthodes d’alignement des propositions : un défi aux traductions croisées</titre>
			<type>poster</type>
			<pages>223-232</pages>
			<resume>Le présent article décrit deux méthodes d’alignement des propositions : l’une basée sur les méthodes d’appariement des graphes et une autre inspirée de la classification ascendante hiérarchique (CAH). Les deux méthodes sont caractérisées par leur capacité d’alignement des traductions croisées, ce qui était impossible pour beaucoup de méthodes classiques d’alignement des phrases. Contrairement aux résultats obtenus avec l’approche spectrale qui nous paraissent non satisfaisants, l’alignement basé sur la méthode de classification ascendante hiérarchique est prometteur dans la mesure où cette technique supporte bien les traductions croisées.</resume>
			<mots_cles>alignement des corpus parallèles, appariement de graphes, classification ascendante hiérarchique, proposition syntaxique, mémoire de traduction, linguistique contrastive</mots_cles>
			<title></title>
			<abstract>The present paper describes two methods for clauses alignment. The first one uses a graph matching approach, while the second one relies on agglomerative hirerarchical clustering (AHC). Both methods are characterized by the fact they can align cross translations, which was impossible for previous classic sentence alignment methods. Though the results given by the spectral method are unsatisfactory, the method based on AHC is very promising. It handles correctly cross translations.</abstract>
			<keywords>parallel corpora alignment, graph matching, agglomerative hierarchical clustering, syntactic clause, translation memory, contrastive linguistics</keywords>
		</article>
		<article id="taln-2007-poster-023" session="Posters">
			<auteurs>
				<auteur>
					<nom>Fiammetta Namer</nom>
					<email>fiammetta.namer@univ-nancy2.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Pierrette Bouillon</nom>
					<email>pierrette.bouillon@issco.unige.ch</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Evelyne Jacquey</nom>
					<email>evelyne.jacquey@atilf.fr</email>
					<affiliationId>3</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université Nancy2 et ATILF</affiliation>
				<affiliation affiliationId="2">ISSCO</affiliation>
				<affiliation affiliationId="3">ATILF</affiliation>
			</affiliations>
			<titre>Un Lexique Génératif de référence pour le français</titre>
			<type>poster</type>
			<pages>233-242</pages>
			<resume>Cet article propose une approche originale visant la construction d’un lexique sémantique de référence sur le français. Sa principale caractéristique est de pouvoir s’appuyer sur les propriétés morphologiques des lexèmes. La méthode combine en effet des résultats d’analyse morphologique (Namer, 2002;2003), à partir de ressources lexicales de grande taille (nomenclatures du TLF) et des méthodologies d’acquisition d’information lexicale déjà éprouvées (Namer 2005; Sébillot 2002). Le format de représentation choisi, dans le cadre du Lexique Génératif, se distingue par ses propriétés d’expressivité et d’économie. Cette approche permet donc d’envisager la construction d’un lexique de référence sur le français caractérisé par une forte homogénéité tout en garantissant une couverture large, tant du point de vue de la nomenclature que du point de vue des contenus sémantiques. Une première validation de la méthode fournit une projection quantitative et qualitative des résultats attendus.</resume>
			<mots_cles>acquisition lexicale, lexique de référence du français, modèle du lexique génératif, morphologie constructionnelle, corpus, sémantique</mots_cles>
			<title></title>
			<abstract>This paper describes an original approach aiming at building a reference semantic lexicon for French. Its main characteristic is that of being able to rely on morphological properties. The method thus combines morphological analyses results (Namer 2002;2003;2005) from large scale lexical resources (i.e. TLF word lists) with already tested acquisition methodologies on lexical information (Sébillot, 2002). The representation format, within the Generative Lexicon framework, has been chosen for its expressiveness and economy features. So, this approach allows us to consider building a reference lexicon for French, which is fundamentally homogeneous as well as of a large coverage. A feasability study of the described method provides a projection of expected results, from both quantitative and qualitative points of view.</abstract>
			<keywords>lexical acquisition, reference lexicon for French, generative lexicon model, word formation, corpora, semantics</keywords>
		</article>
		<article id="taln-2007-poster-024" session="Posters">
			<auteurs>
				<auteur>
					<nom>Patrick Paroubek</nom>
					<email>pap@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Anne Vilnat</nom>
					<email>anne@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Isabelle Robba</nom>
					<email>isabelle@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Christelle Ayache</nom>
					<email>ayache@elda.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIMSI-CNRS Bât. 508 Université Paris XI, BP 133 - 91403 ORSAY Cedex</affiliation>
				<affiliation affiliationId="2">ELRA-ELDA 55-57, rue Brillat Savarin 75013 Paris</affiliation>
			</affiliations>
			<titre>Les résultats de la campagne EASY d’évaluation des analyseurs syntaxiques du français</titre>
			<type>poster</type>
			<pages>243-252</pages>
			<resume>Dans cet article, nous présentons les résultats de la campagne d’évaluation EASY des analyseurs syntaxiques du français. EASY a été la toute première campagne d’évaluation comparative des analyseurs syntaxiques du français en mode boîte noire utilisant des mesures objectives quantitatives. EASY fait partie du programme TECHNOLANGUE du Ministère délégué à la Recherche et à l’Éducation, avec le soutien du ministère de délégué à l’industrie et du ministère de la culture et de la communication. Nous exposons tout d’abord la position de la campagne par rapport aux autres projets d’évaluation en analyse syntaxique, puis nous présentos son déroulement, et donnons les résultats des 15 analyseurs participants en fonction des différents types de corpus et des différentes annotations (constituants et relations). Nous proposons ensuite un ensemble de leçons à tirer de cette campagne, en particulier à propos du protocole d’évaluation, de la définition de la segmentation en unités linguistiques, du formalisme et des activités d’annotation, des critères de qualité des données, des annotations et des résultats, et finalement de la notion de référence en analyse syntaxique. Nous concluons en présentant comment les résultats d’EASY se prolongent dans le projet PASSAGE (ANR-06-MDCA-013) qui vient de débuter et dont l’objectif est d’étiqueter un grand corpus par plusieurs analyseurs en les combinant selon des paramètres issus de l’évaluation.</resume>
			<mots_cles>analyseur syntaxique, évaluation, français</mots_cles>
			<title></title>
			<abstract>In this paper, we present the results of the EASY evaluation campaign on parsers of French. EASY has been the very first black-box comparative evaluation campaign for parsers of French, with objective quantitative performance measures. EASY was part of the TECHNOLANGUE program of the Delegate Ministry of Research, jointly supported by the Delegate Ministry of Industry and the ministry of Culture and Communication. After setting EASY in the context of parsing evaluation and giving an account of the campaign, we present the results obtained by 15 parsers according to syntactic relation and subcorpus genre. Then we propose some lessons to draw from this campaign, in particular about the evaluation protocole, the segmenting into linguistic units, the formalism and the annotation activities, the quality criteria to apply for data, annotations and results and finally about the notion of reference for parsing. We conclude by showing how EASY results extend through the PASSAGE project (ANR-06-MDCA-013), which has just started and whose aim is the automatic annotation of a large corpus by several parsers, the combination of which being parametrized by results stemming from evaluation.</abstract>
			<keywords>parser, evaluation, french</keywords>
		</article>
		<article id="taln-2007-poster-025" session="Posters">
			<auteurs>
				<auteur>
					<nom>Holger Schwenk</nom>
					<email>schwenk@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Daniel Déchelotte</nom>
					<email>dechelot@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Hélène Bonneau-Maynard</nom>
					<email>hbm@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Alexandre Allauzen</nom>
					<email>allauzen@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIMSI-CNRS, B.P. 133, 91403 Orsay cedex</affiliation>
			</affiliations>
			<titre>Modèles statistiques enrichis par la syntaxe pour la traduction automatique</titre>
			<type>poster</type>
			<pages>253-262</pages>
			<resume>La traduction automatique statistique par séquences de mots est une voie prometteuse. Nous présentons dans cet article deux évolutions complémentaires. La première permet une modélisation de la langue cible dans un espace continu. La seconde intègre des catégories morpho-syntaxiques aux unités manipulées par le modèle de traduction. Ces deux approches sont évaluées sur la tâche Tc-Star. Les résultats les plus intéressants sont obtenus par la combinaison de ces deux méthodes.</resume>
			<mots_cles>traduction automatique, approche statistique, modélisation linguistique dans un espace continu, analyse morpho-syntaxique, désambiguïsation lexicale</mots_cles>
			<title></title>
			<abstract>Statistical phrase-based translation models are very efficient. In this paper, we present two complementary methods. The first one consists in a a statistical language model that is based on a continuous representation of the words in the vocabulary. By these means we expect to take better advantage of the limited amount of training data. In the second method, morpho-syntactic information is incorporated into the translation model in order to obtain lexical disambiguation. Both approaches are evaluated on the Tc-Star task. Most promising results are obtained by combining both methods.</abstract>
			<keywords>statistical machine translation, continuous space language model, POS tagging, lexical disambiguation</keywords>
		</article>
		<article id="taln-2007-poster-026" session="Posters">
			<auteurs>
				<auteur>
					<nom>Laurianne Sitbon</nom>
					<email>laurianne.sitbon@univ-avignon.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Patrice Bellot</nom>
					<email>patrice.bellot@univ-avignon.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Philippe Blache</nom>
					<email>blache@lpl.univ-aix.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Laboratoire d’Informatique d’Avignon - Université d’Avignon</affiliation>
				<affiliation affiliationId="2">Laboratoire Parole et Langage - Université de Provence</affiliation>
			</affiliations>
			<titre>Traitements phrastiques phonétiques pour la réécriture de phrases dysorthographiées</titre>
			<type>poster</type>
			<pages></pages>
			<resume>Cet article décrit une méthode qui combine des hypothèses graphémiques et phonétiques au niveau de la phrase, à l’aide d’une réprésentation en automates à états finis et d’un modèle de langage, pour la réécriture de phrases tapées au clavier par des dysorthographiques. La particularité des écrits dysorthographiés qui empêche les correcteurs orthographiques d’être efficaces pour cette tâche est une segmentation en mots parfois incorrecte. La réécriture diffère de la correction en ce sens que les phrases réécrites ne sont pas à destination de l’utilisateur mais d’un système automatique, tel qu’un moteur de recherche. De ce fait l’évaluation est conduite sur des versions filtrées et lemmatisées des phrases. Le taux d’erreurs mots moyen passe de 51 % à 20 % avec notre méthode, et est de 0 % sur 43 % des phrases testées.</resume>
			<mots_cles>réécriture de phrases, dyslexie, automates, correction orthographique</mots_cles>
			<title></title>
			<abstract>This paper introduces a sentence level method combining written correction and phonetic interpretation in order to automatically rewrite sentences typed by dyslexic spellers. The method uses a finite state automata framework and a language model. Dysorthographics refers to incorrect word segmentation which usually causes classical spelling correctors fail. Our approach differs from spelling correction in that we aim to use several rewritings as an expression of the user need in an information retrieval context. Our system is evaluated on questions collected with the help of an orthophonist. The word error rate on lemmatised sentences falls from 51 % to 20 % (falls to 0 % on 43 % of sentences).</abstract>
			<keywords>sentence level rewriting, dyslexia, FSM, spell checking</keywords>
		</article>
		<article id="taln-2007-poster-027" session="Posters">
			<auteurs>
				<auteur>
					<nom>Grégory Smits</nom>
					<email>gsmits@info.unicaen.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Christine Chardenon</nom>
					<email>christine.chardenon@orange-ftgroup.com</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">GREYC–Université de Caen, F-14032 CAEN cedex</affiliation>
				<affiliation affiliationId="2">France Télécom R&amp;D TECH/EASY/LN, 2, avenue Pierre Marzin, 22307 Lannion Cedex</affiliation>
			</affiliations>
			<titre>Vers une méthodologie générique de contrôle basée sur la combinaison de sources de jugement</titre>
			<type>poster</type>
			<pages>273-282</pages>
			<resume>Le contrôle des hypothèses concurrentes générées par les différents modules qui peuvent intervenir dans des processus de TALN reste un enjeu important malgré de nombreuses avancées en terme de robustesse. Nous présentons dans cet article une méthodologie générique de contrôle exploitant des techniques issues de l’aide multicritère à la décision. À partir de l’ensemble des critères de comparaison disponibles et la formalisation des préférences d’un expert, l’approche proposée évalue la pertinence relative des différents objets linguistiques générés et conduit à la mise en place d’une action de contrôle appropriée telle que le filtrage, le classement, le tri ou la propagation.</resume>
			<mots_cles>méthodologie de contrôle, aide multicritère à la décision, apprentissage automatique de métriques</mots_cles>
			<title></title>
			<abstract>The control of concurrent hypotheses generated by the different modules which compose NLP processes is still an important issue despite advances concerning robustness. In this article, we present a generic methodology of control inspired from multicriteria decision aid methods. Based on available comparison criteria and formalized expert knowledge, the proposed approach evaluate the relevancy of each generated linguistic object and lead to the decision of an appropriate control action such as filtering, ordering, sorting or propagating.</abstract>
			<keywords>control methodology, multicriteria decision aid, metrics automatic learning</keywords>
		</article>
		<article id="taln-2007-poster-028" session="Posters">
			<auteurs>
				<auteur>
					<nom>Agnès Tutin</nom>
					<email>agnes.tutin@u-grenoble3.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIDILEM, Université Grenoble 3, BP 25, 38040 Grenoble Cedex 09</affiliation>
			</affiliations>
			<titre>Traitement sémantique par analyse distributionnelle des noms transdisciplinaires des écrits scientifiques</titre>
			<type>poster</type>
			<pages>283-292</pages>
			<resume>Dans cette étude sur le lexique transdisciplinaire des écrits scientifiques, nous souhaitons évaluer dans quelle mesure les méthodes distributionnelles de TAL peuvent faciliter la tâche du linguiste dans le traitement sémantique de ce lexique. Après avoir défini le champ lexical et les corpus exploités, nous testons plusieurs méthodes basées sur des dépendances syntaxiques et observons les proximités sémantiques et les classes établies. L’hypothèse que certaines relations syntaxiques - en particulier les relations de sous-catégorisation – sont plus appropriées pour établir des classements sémantiques n’apparaît qu’en partie vérifiée. Si les relations de sous-catégorisation génèrent des proximités sémantiques entre les mots de meilleure qualité, cela ne semble pas le cas pour la classification par voisinage.</resume>
			<mots_cles>corpus, écrits scientifiques, classes sémantiques, analyse distributionnelle</mots_cles>
			<title></title>
			<abstract>In this study about general scientific lexicon, we aim at evaluating to what extent distributional methods in NLP can enhance the linguist’s task in the semantic treatment. After a definition of our lexical field and a presentation of our corpora, we evaluate several methods based on syntactic dependencies for establishing semantic similarities and semantic classes. Our hypothesis that some syntactic relations – namely subcategorized relations – is more relevant to establish semantic classes does not entirely appears valid. If subcategorized relations produce better semantic links between words, this is not the case with neighbour joigning clustering method.</abstract>
			<keywords>corpus, scientific writings, semantic classes, distributional analysis</keywords>
		</article>
		<article id="taln-2007-poster-029" session="Posters">
			<auteurs>
				<auteur>
					<nom>Jeanne Villaneau</nom>
					<email>Jeanne.Villaneau@univ-ubs.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Valoria Université de Bretagne Sud</affiliation>
			</affiliations>
			<titre>Une expérience de compréhension en contexte de dialogue avec le système LOGUS, approche logique de la compréhension de la langue orale</titre>
			<type>poster</type>
			<pages>293-302</pages>
			<resume>LOGUS est un système de compréhension de la langue orale dans le cadre d’un dialogue homme-machine finalisé. Il est la mise en oeuvre d’une approche logique qui utilise différents formalismes afin d’obtenir un système robuste mais néanmoins relativement extensible. Cet article décrit essentiellement l’étape de compréhension en contexte de dialogue implémentée sur LOGUS, développée et testée à partir d’un corpus de réservation hôtelière enregistré et annoté lors des travaux du groupe MEDIA du projet technolangue. Il décrit également les différentes interrogations et conclusions que peut susciter une telle expérience et les résultats obtenus par le système dans la résolution des références. Concernant l’approche elle-même, cette expérience semble montrer que le formalisme adopté pour la représentation sémantique des énoncés est bien adapté à la compréhension en contexte.</resume>
			<mots_cles>compréhension automatique de la parole, résolution des références, dialogue oral homme-machine</mots_cles>
			<title></title>
			<abstract>LOGUS is a spoken language understanding system usable in a man-machine dialogue. It is based on a logical approach where various formalisms are used, in order to achieve a robust but generic and extensible system. Implementation of a context-sensitive understanding is the main topic of this paper. Processing and tests were carried out from a hotel reservation corpus which was recorded and annotated as part of the work handled by the technolangue consortium’s MEDIA subgroup. This paper also describes the various questions raised and conclusions drawn from such an experiment, as well as the results achieved by the system for anaphora resolution. This experiment shows that the formalism used in order to represent the meaning of the utterances is relevant for anaphora resolution and in-context understanding.</abstract>
			<keywords>man-machine dialogue, spoken language understanding, anaphora resolution</keywords>
		</article>
		<article id="taln-2007-poster-030" session="Posters">
			<auteurs>
				<auteur>
					<nom>Anis Zouaghi</nom>
					<email>Anis.Zouaghi@riadi.rnu.tn</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Mounir Zrigui</nom>
					<email>Mounir.Zrigui@fsm.rnu.tn</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Mohamed Ben Ahmed</nom>
					<email>Mohamed.Benahmed@riadi.rnu.tn</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Labo RIADI (Unité de Monastir), Université de Monastir, Faculté des sciences de Monastir</affiliation>
				<affiliation affiliationId="2">Labo RIADI – Université de la Mannouba, École nationale des sciences de l’informatique</affiliation>
			</affiliations>
			<titre>Évaluation des performances d’un modèle de langage stochastique pour la compréhension de la parole arabe spontanée</titre>
			<type>poster</type>
			<pages>303-312</pages>
			<resume>Les modèles de Markov cachés (HMM : Hidden Markov Models) (Baum et al., 1970), sont très utilisés en reconnaissance de la parole et depuis quelques années en compréhension de la parole spontanée latine telle que le français ou l’anglais. Dans cet article, nous proposons d’utiliser et d’évaluer la performance de ce type de modèle pour l’interprétation sémantique de la parole arabe spontanée. Les résultats obtenus sont satisfaisants, nous avons atteint un taux d’erreur de l’ordre de 9,9% en employant un HMM à un seul niveau, avec des probabilités tri_grammes de transitions.</resume>
			<mots_cles>analyse sémantique, modèle de langage stochastique, contexte pertinent, information mutuelle moyenne, parole arabe spontanée</mots_cles>
			<title></title>
			<abstract>The HMM (Hidden Markov Models) (Baum et al., 1970), are frequently used in speech recognition and in the comprehension of foreign spontaneous speech such us the french or the english. In this article, we propose using and evaluating the performance of this model type for the semantic interpretation of the spontaneous arabic speech. The obtained results are satisfying; we have achieved an error score equal to 9.9%, by using HMM with trigrams probabilities transitions.</abstract>
			<keywords>semantic analysis, stochastic language model, pertinent context, overage mutual information, spontaneous arabic speech</keywords>
		</article>
		<article id="taln-2007-demo-001" session="Démonstrations">
			<auteurs>
				<auteur>
					<nom>Éric Brunelle</nom>
					<email>developpement@druide.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Simon Charest</nom>
					<email>developpement@druide.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Druide informatique inc., 1435, rue St-Alexandre, bureau 1040, Montréal (Québec) H3A 2G4, Canada</affiliation>
			</affiliations>
			<titre>Présentation du logiciel Antidote RX</titre>
			<type>démonstration</type>
			<pages>315-317</pages>
			<resume>Antidote RX est la sixième édition d’Antidote, un logiciel d’aide à la rédaction développé et commercialisé par la société Druide informatique. Antidote RX comporte un correcteur grammatical avancé, dix dictionnaires de consultation et dix guides linguistiques. Il fonctionne sous les systèmes d’exploitation Windows, Mac OS X et Linux.</resume>
			<mots_cles></mots_cles>
			<title></title>
			<abstract></abstract>
			<keywords></keywords>
		</article>
		<article id="taln-2007-demo-002" session="Démonstrations">
			<auteurs>
				<auteur>
					<nom>Dominique Laurent</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Sophie Nègre</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Patrick Séguéla</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Synapse Développement, 33 Rue Maynard, 31000 Toulouse</affiliation>
			</affiliations>
			<titre>Logiciel Cordial</titre>
			<type>démonstration</type>
			<pages>319-321</pages>
			<resume>Cordial est un correcteur efficace et discret enrichi d'un grand nombre de fonctions d'aide à la rédaction et d'analyse de documents. Très riche avec ces multiples dictionnaires et souvent pertinent dans ses propositions, Cordial est un compagnon précieux qui vous permet d'assurer la qualité de vos écrits. La version 2007 de Cordial s'intègre dans un vaste éventail de logiciels comme les traitements de texte (Word, Open Office, Word Perfect...), clients de messagerie (Outlook, Notes, Thunderbird, webmails...) ou navigateurs (Explorer, Mozilla).</resume>
			<mots_cles></mots_cles>
			<title></title>
			<abstract></abstract>
			<keywords></keywords>
		</article>
		<article id="taln-2007-demo-003" session="Démonstrations">
			<auteurs>
				<auteur>
					<nom>Elliott Macklovitch</nom>
					<email>macklovi@iro.umontreal.ca</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Guy Lapalme</nom>
					<email>lapalme@iro.umontreal.ca</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Laboratoire RALI – Université de Montréal, Montréal, Canada</affiliation>
			</affiliations>
			<titre>TransCheck : un vérificateur automatique de traductions</titre>
			<type>démonstration</type>
			<pages>323-325</pages>
			<resume>Nous offrirons une démonstration de la dernière version de TransCheck, un vérificateur automatique de traductions que le RALI est en train de développer. TransCheck prend en entrée deux textes, un texte source dans une langue et sa traduction dans une autre, les aligne au niveau de la phrase et ensuite vérifie les régions alignées pour s’assurer de la présence de certains équivalents obligatoires (p. ex. la terminologie normalisée) et de l’absence de certaines interdictions de traduction (p. ex. des interférences de la langue source). Ainsi, TransCheck se veut un nouveau type d’outil d’aide à la traduction qui pourra à réduire le fardeau de la révision et diminuer le coût du contrôle de la qualité.</resume>
			<mots_cles>traduction assistée par ordinateur, vérification automatique de traductions, révision de traduction</mots_cles>
			<title></title>
			<abstract>We will present a demonstration of the latest version of TransCheck, an automatic translation checker that the RALI is currently developing. TransCheck takes as input two texts, a source text in one language and its translation in another, aligns them at the sentence level and then verifies the aligned regions to ensure that they contain certain obligatory equivalents (e.g. standardized terminology) and do not contain certain prohibited translations (e.g. source language interference). TransCheck is thus intended to be a new type of tool for assisting translators which has the potential to ease the burden of revision and diminish the costs of quality control.</abstract>
			<keywords>machine-aided translation, automatic translation checking, translation revision</keywords>
		</article>
		<article id="taln-2007-demo-004" session="Démonstrations">
			<auteurs>
				<auteur>
					<nom>Jean-Marie Pierrel</nom>
					<email>Jean-Marie.Pierrel@atilf.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Etienne Petitjean</nom>
					<email>Etienne.Petitjean@atilf.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">CNRTL/ATILF CNRS – Nancy Université, 44 avenue de la Libération, BP 30687, 54063 Nancy CEDEX</affiliation>
			</affiliations>
			<titre>Le CNRTL, Centre National de Ressources Textuelles et Lexicales, un outil de mutualisation de ressources linguistiques</titre>
			<type>démonstration</type>
			<pages>327-329</pages>
			<resume>Créé en 2005 à l’initiative du Centre National de la Recherche Scientifique, le CNRTL propose une plate-forme unifiée pour l’accès aux ressources et documents électroniques destinés à l’étude et l’analyse de la langue française. Les services du CNRTL comprennent le recensement, la documentation (métadonnées), la normalisation, l’archivage, l’enrichissement et la diffusion des ressources. La pérennité du service et des données est garantie par le soutien institutionnel du CNRS, l’adossement à un laboratoire de recherche en linguistique et informatique du CNRS et de Nancy Université (ATILF – Analyse et Traitement Informatique de la Langue Française), ainsi que l’intégration dans le réseau européen CLARIN (common language resources and technology infrastructure european).</resume>
			<mots_cles></mots_cles>
			<title></title>
			<abstract></abstract>
			<keywords></keywords>
		</article>
	</articles>
</conference>