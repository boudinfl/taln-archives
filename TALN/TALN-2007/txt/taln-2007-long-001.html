<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
"http://www.w3.org/TR/html4/loose.dtd">
<html><head><title>Exploiting structural meeting-specific features for topic segmentation</title>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
</head>
<body>
<div style="page-break-before:always; page-break-after:always"><div><p>TALN 2007, Toulouse, 5&#8211;8 juin 2007
</p>
<p>Exploiting structural meeting-specific features
for topic segmentation
</p>
<p>Maria GEORGESCUL1, Alexander CLARK2, Susan ARMSTRONG1
1 ISSCO/TIM/ETI, University of Geneva
</p>
<p>2 Department of Computer Science, Royal Holloway University of London
maria.georgescul@eti.unige.ch, alexc@cs.rhul.ac.uk,
</p>
<p>susan.armstrong@issco.unige.ch
</p>
<p>R&#233;sum&#233;. Dans cet article, nous traitons de la segmentation automatique des textes en &#233;pi-
sodes th&#233;matiques non superpos&#233;s et ayant une structure lin&#233;aire. Notre &#233;tude porte sur l&#8217;uti-
lisation des traits lexicaux, acoustiques et syntaxiques et sur l&#8217;influence de ces traits sur la
performance d&#8217;un syst&#232;me automatique de segmentation th&#233;matique. Nous appliquons notre
approche, bas&#233;e sur des machines &#224; vecteurs support, &#224; des transcriptions des dialogues multi-
locuteurs.
</p>
<p>Abstract. In this article we address the task of automatic text structuring into linear
and non-overlapping thematic episodes. Our investigation reports on the use of various lexi-
cal, acoustic and syntactic features, and makes a comparison of how these features influence
performance of automatic topic segmentation. Using datasets containing multi-party meeting
transcriptions, we base our experiments on a proven state-of-the-art approach using support
vector classification.
</p>
<p>Mots-cl&#233;s : segmentation automatique en &#233;pisodes th&#233;matiques, machines &#224; vecteurs
support, dialogues multi-locuteurs.
</p>
<p>Keywords: automatic topic segmentation, support vector machines, multi-party dia-
logues.
</p>
<p>1 Introduction
</p>
<p>Georgescul et al. (2006b) proposed a support vector machine approach to the task of text
segmentation which demonstrates improvements over state-of-the-art techniques, by modeling
large scale (merely lexical) features and non-linear relations in an efficient and stable way. Their
experimental results showed that word distributions in texts provide relevant information for the
detection of boundaries between thematic episodes in data sets covering different domains. In
this paper, we put the emphasis on tackling the topic segmentation problem in the context of
recorded and transcribed multi-party dialogs. In particular, we extend the work of Georges-
cul et al. (2006b) by exploring potential information provided by &#8216;surface&#8217; cues in multi-party
dialogues such as syntactic knowledge, cue-phrases and acoustic cues. We investigate the per-
tinence of these factors individually and in combination with information provided by word
distributions through the intermedium of transductive support vector machines.
</p>
<p>15</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Maria GEORGESCUL, Alexander CLARK, Susan ARMSTRONG
</p>
<p>In order to identify boundaries, we model the thematic segmentation task as a binary classi-
fication problem. The features considered for designing the classifier are described in Section
2. In Section 3 we highlight how the classification model is constructed by using transductive
support vector learning. A comparative analysis of the support vector classifier performance by
using these cues is provided in Section 4.
</p>
<p>2 Input features
</p>
<p>As in (Georgescul et al., 2006b), we consider the thematic segmentation task as a binary clas-
sification problem, where each utterance should be classified as a topic boundary or not. As
explained in Section 3, we employ a support vector machine classifier which is given as input a
vectorial representation of the utterance to be classified and its context. Each dimension of the
input vector indicates the value of a certain feature characterizing the utterance. For utterance
characterization, Georgescul et al. (2006b) only considered features based on observations of
patterns in vocabulary use. Here, in addition to these lexical features, we consider meeting-
specific features as described in the following.
</p>
<p>Note that, similar types of features examined in our study have been previously proposed for
analyzing discourse structure in state-of-the-art studies like those described in (Litman &amp; Pas-
sonneau, 1995; Hirschberg &amp; Nakatani, 1996; Galley et al., 2003). These include speaker acti-
vity, discourse markers, prosodic and syntactic features.
</p>
<p>2.1 Speaker activity
</p>
<p>According to (Pfau et al., 2001), patterns of speech activity are valuable data for discourse ana-
lysis. In order to explore this claim, the first pattern we chose to investigate is speaker activity. In
particular, we start with the hypothesis that in meeting data the contribution of each participant
in the discussion can signal a new topic. For instance, some participants could have a preference
for certain subjects of discussion.
</p>
<p>We take into account the changes in speaker activity by measuring the number of words each
participant uttered before and after each utterance candidate to a thematic boundary. This is
formalized in the following manner. LetAsk be the number of words that the participant s said in
utterance uk. For each meeting participant s and for each i-th utterance ui, we take into account
the number of words that the participant s uttered before and during ui in an interval of size
activityWS, by considering the vector !fl
</p>
<p>s
</p>
<p>i as: !fl
s
</p>
<p>i =
(
Asi&#8722;activityWS+1, A
</p>
<p>s
i&#8722;activityWS+2, ..., A
</p>
<p>s
i
</p>
<p>)
.
</p>
<p>We also store in a vector !fr
s
</p>
<p>i the number of words that the participant s uttered after ui in
an interval of size activityWS: !fr
</p>
<p>s
</p>
<p>i =
(
Asi+1, A
</p>
<p>s
i+2, ..., A
</p>
<p>s
i+activityWS
</p>
<p>)
. We then normalize the
</p>
<p>two vectors !fl
s
</p>
<p>i and !fr
s
</p>
<p>i to form two probability distributions lsi and rsi , respectively. That is,
we perform the normalization by simply dividing each element in the vector by the sum of all
entries in the vector.
</p>
<p>We measure significant changes in speaker activity by using the information radius between the
probability distributions given by the speaker activity at the left and right side of the current (ui)
</p>
<p>16</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Exploiting structural meeting-specific features for topic segmentation
</p>
<p>utterance:
</p>
<p>IRad(lsi , r
s
i ) =
</p>
<p>1
</p>
<p>2
</p>
<p>&#63726;&#63727;&#63728;&#8721;
i
</p>
<p>lsi !=0
</p>
<p>lsi log
lsi
mi
</p>
<p>+
&#8721;
</p>
<p>i
rsi !=0
</p>
<p>rsi log
rsi
mi
</p>
<p>&#63737;&#63738;&#63739; (1)
wheremi =
</p>
<p>lsi+r
s
i
</p>
<p>2 is the average distribution of the two random variables l
s
i and rsi .
</p>
<p>Finally, IRad(lsi , rsi )) will constitute the entry for one dimension of the vectorial representation
for utterance ui.
</p>
<p>2.2 Discourse markers
</p>
<p>Previous studies (Litman &amp; Passonneau, 1995; Marcu, 2000) addressed questions regarding dis-
course relations and their realization by discourse markers. Here, we are interested in finding
those discourse markers that indicate thematic shifts in our data. We started with the following
list of discourse markers that has been synthesized from a commonly used list of discourse
markers: &#8220;accordingly&#8221;, &#8220;actually&#8221;, &#8220;after all&#8221;, &#8220;also&#8221;, &#8220;although&#8221;, &#8220;anyway&#8221;, &#8220;back to&#8221;, &#8220;basi-
cally&#8221;, &#8220;but&#8221;, &#8220;fine&#8221;, &#8220;for example&#8221;, &#8220;furthermore&#8221;, &#8220;generally&#8221;, &#8220;however&#8221;, &#8220;like&#8221;, &#8220;moreover&#8221;,
&#8220;nevertheless&#8221;, &#8220;nor&#8221;, &#8220;now&#8221;, &#8220;of course&#8221;, &#8220;okay&#8221;, &#8220;really&#8221;, &#8220;similarly&#8221;, &#8220;since&#8221;, &#8220;speaking of&#8221;,
&#8220;so&#8221;, &#8220;still&#8221;, &#8220;that&#8217;s all&#8221;, &#8220;then&#8221;, &#8220;therefore&#8221;, &#8220;well&#8221;. For each discourse marker in this list, we
automatically examine if it occurs in each utterance that is a candidate for marking a thematic
boundary. That is, our SVM takes as input binary features indicating whether each discourse
marker occurs in the current utterance. We retain as input features to our system only those
discourse markers that occur at least once in our corpus.
</p>
<p>2.3 Syntactic features
</p>
<p>The use of syntax-based features is to a large extent motivated by previous work (Passonneau &amp;
Litman, 1993; Litman &amp; Passonneau, 1995) relating discourse structure and noun phrase ana-
phora. Regarding the pronominal reference, we are mainly following the intuitive assumption
that nouns and verbs appear more frequently at the beginning of a new topic, while pronouns
appear more frequently in the middle of a thematic episode.
</p>
<p>The syntactic features considered in our study are the distributions of different part-of-speech
categories before and after a potential thematic boundary. That is, we extracted frequencies
of pronouns, (proper) nouns and verbs before and after each utterance candidate to a thema-
tic boundary. For the annotation of part-of-speech information, we used TreeTagger (Schmid,
1994).
</p>
<p>This component was formalized as follows. Let Pi,Ni, Vi be the number of pronouns, nouns and
verbs, respectively, in utterance ui. We store in a vector !fl
</p>
<p>p
</p>
<p>i the number of pronouns occurring
in utterances situated before ui in an interval of size synWS: !fl
</p>
<p>p
</p>
<p>i = (Pi&#8722;synWS+1, Pi&#8722;synWS+2,
..., Pi). We also store in a vector !fr
</p>
<p>p
</p>
<p>i the number of pronouns occurring in utterances situated
after ui in an interval of size synWS: !fr
</p>
<p>p
</p>
<p>i = (Pi+1, Pi+2, ..., Pi+synWS). Similarly, we store in
vectors !fl
</p>
<p>n
</p>
<p>i , !fl
v
</p>
<p>i the number of nouns and verbs, respectively occurring before ui in an interval
of size synWS utterances. Then, the vectors !fr
</p>
<p>n
</p>
<p>i , !fr
v
</p>
<p>i will contain the number of nouns and
verbs, respectively occurring after ui in an interval of size synWS utterances.
</p>
<p>17</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Maria GEORGESCUL, Alexander CLARK, Susan ARMSTRONG
</p>
<p>As in Section 2.1, we normalize the resulting vectors of counts !fl
p
</p>
<p>i , !fr
p
</p>
<p>i , !fl
n
</p>
<p>i , !fr
n
</p>
<p>i , !fl
v
</p>
<p>i , !fr
v
</p>
<p>i to
obtain probability distributions lpi , r
</p>
<p>p
i , lni , rni , lvi , rvi , respectively. Finally, we measure changes in
</p>
<p>the distribution of pronouns, nouns and verbs at the left and right side of the current utterance
by using the information radius (see Equation 1). That is, for each utterance ui, we measure
IRad (lpi , r
</p>
<p>p
i ), IRad (lni , rni ), IRad (lvi , rvi ), which will constitute entries for three dimensions of
</p>
<p>the vectorial representation for utterance ui (taken as input to the SVM classifier).
</p>
<p>2.4 Silences and overlaps
</p>
<p>Silences and overlaps, as well as other acoustic information can also give evidence whether a
major topic shift occurred. In particular, studies on discourse structure (Hirschberg &amp; Naka-
tani, 1996) exploit various prosodical information such as pitch range (raised at segment-initial
phrases and lower at segment-final phrases), speech rate (accelerating at segment-final phrases),
amplitude and contour.
</p>
<p>We investigated the pertinence of these features with the following formalization. Let Si be
the silence duration between utterance ui&#8722;1 and ui. Let Oi be the speaker overlap duration
between utterance ui&#8722;1 and ui. We normalize the Si and Oi values by speaker and the resulting
values S &#8242;i , O
</p>
<p>&#8242;
i are used to compute the following quantities: sli =
</p>
<p>&#8721;i
k=i&#8722;silenceWSL+1
</p>
<p>(
S
</p>
<p>&#8242;
i
</p>
<p>)
;
</p>
<p>sri =
&#8721;i+silenceWSR
</p>
<p>k=i+1
</p>
<p>(
S
</p>
<p>&#8242;
i
</p>
<p>)
; oli =
</p>
<p>&#8721;i
k=i&#8722;overlapWSL+1
</p>
<p>(
O
</p>
<p>&#8242;
i
</p>
<p>)
; and ori =
</p>
<p>&#8721;i+overlapWSR
k=i+1
</p>
<p>(
O
</p>
<p>&#8242;
i
</p>
<p>)
.
</p>
<p>We include silences and overlaps as part of the utterance context representation by considering
the sli, sri, oli, ori quantities as dimensions of the vector characterizing the utterance ui.
</p>
<p>3 Methodology
</p>
<p>As introduced in the previous section, we employ a vectorial representation containing lexical,
acoustic and syntactic information to characterize each utterance. The topic segmentation task
is thus reduced to a binary classification problem: each utterance has to be classified as marking
the presence or the absence of a topic shift in the text.
</p>
<p>In order to infer eventual dependencies between the binary class label and observations of pat-
terns (provided by the lexical, acoustic and syntactic information), we employ a discriminative
approach based on transductive support vector learning. A brief overview on inductive support
vector learning for topic segmentation has been described in (Georgescul et al., 2006b). In this
section, we give some highlights representing the main elements in using transductive support
vector learning for topic segmentation.
</p>
<p>The support vector learner L is given a training set Strain = ((!u1, y1), ..., (!un, yn)) &#8838; (U &#215;Y )n
containing n examples drawn independently and identically distributed (i.i.d.) according to a
fixed distribution Pr(u, y) = Pr(y|u)Pr(u). Following the transductive setting proposed by
Joachims (1999), the learner is also given an i.i.d. sample, Stest = (!u&#8727;1, !u&#8727;2, ..., !u&#8727;k), containing
k test examples from the same distribution as !u1, !u2, ... ,!un. Each training example from Strain
consists of a high-dimensional vector !u describing an utterance and the class label y. The class
label y has only two possible values: +1 (corresponding to a &#8216;thematic boundary&#8217;) or -1 (corres-
ponding to a &#8216;non-thematic boundary&#8217;). We represent each utterance instance by a feature vector
!u with attributes containing &#8216;surface&#8217; meeting-specific information (as described in Section 2)
</p>
<p>18</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Exploiting structural meeting-specific features for topic segmentation
</p>
<p>plus the attributes given by the bag-of-words representation of word frequencies, as described
in (Georgescul et al., 2006b).
</p>
<p>Given a hypothesis space H, of functions h : U &#8594; {&#8722;1,+1} having the form h(!u) =
sign(&#12296;!w, !u&#12297;+b), the transductive learnerLtransd seeks a decision function htransd fromH, using
Strain and Stest so that the expected number of erroneous predictions on the test examples is mi-
nimized. Using the structural risk minimization principle (Vapnik, 1998), the smallest bound on
the test error is calculated by minimizing the following cost functionW transd:
</p>
<p>W transd(y&#8727;1, ...y&#8727;k, !w, b, &#958;1, &#958;2, ..., &#958;n, &#958;&#8727;1 , &#958;&#8727;2 , &#958;&#8727;k) =
</p>
<p>=
1
</p>
<p>2
&#12296;!w, !w&#12297;+ C+
</p>
<p>n&#8721;
i=0,yi=1
</p>
<p>&#958;i + C
&#8722;
</p>
<p>n&#8721;
i=0,yi=&#8722;1
</p>
<p>&#958;i + C
&#8727;
</p>
<p>k&#8721;
j=0
</p>
<p>&#958;&#8727;j ,
(2)
</p>
<p>subject to: &#63729;&#63732;&#63732;&#63732;&#63732;&#63732;&#63732;&#63732;&#63732;&#63732;&#63732;&#63732;&#63732;&#63730;&#63732;&#63732;&#63732;&#63732;&#63732;&#63732;&#63732;&#63732;&#63732;&#63732;&#63732;&#63732;&#63731;
</p>
<p>yi [&#12296;!w &#183; !ui&#12297;+ b] &#8804; 1&#8722; &#958;i for i = 1, 2, ..., n;
</p>
<p>y&#8727;j [&#12296;!w &#183; !u&#8727;i &#12297;+ b] &#8804; 1&#8722; &#958;&#8727;i for j = 1, 2, ..., k;
</p>
<p>y&#8727;j &#8712; {&#8722;1, 1} for j = 1, 2, ..., k.
</p>
<p>&#958;i &#8805; 0 for i = 1, 2, ..., n;
</p>
<p>&#958;&#8727;j &#8805; 0 for j = 1, 2, ..., k;
</p>
<p>The so-called slack variables &#958;i and &#958;&#8727;j are introduced in order to be able to handle non-separable
data. The regularization parameters C&#8722; and C+ are tuned as described in Section 4.1.
</p>
<p>4 Experiments and results
</p>
<p>4.1 Parameter estimation
</p>
<p>We train and evaluate the effectiveness of our technique on the ICSI-MR dataset (Janin et al.,
2004) containing transcribed multi-party dialogs.
</p>
<p>We divide the ICSI-MR data set into two disjoint parts: a training dataset composed of 80% of
the initial data set, while the remaining 20% is held out for testing purposes. That is, the training
set is used to determine the best model settings for the SVM classifier, while the test set is used
to determine the final topic segmentation error rate.
</p>
<p>We select the best model parameters, by running five-fold cross validation for SVM parameter
estimation, using the Gaussian RBF kernel. During this preliminary step we estimate the perfor-
mance of the SVM classifier by using the precision and recall, i.e. the precision/recall-breakeven
point (Joachims, 1999). The choice of binary evaluation metrics in this step was motivated by
the fact that posing the topic segmentation task as a classification problem involves a loss of the
sequential nature of the data, which is an inconvenience in computing the Pk (Beeferman et al.,
1999) or Prerror (Georgescul et al., 2006a) measures.
</p>
<p>19</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Maria GEORGESCUL, Alexander CLARK, Susan ARMSTRONG
</p>
<p>Parameter Interval for grid search Best window size
activityWS 5 . . . 50 step 5 35 utterances
synWS 5 . . . 50 step 5 30 utterances
silenceWSL 2 . . . 10 step 1 6 utterances
silenceWSR 2 . . . 10 step 1 3 utterances
overlapWSL 2 . . . 10 step 1 2 utterances
overlapWSR 2 . . . 10 step 1 4 utterances
</p>
<p>TAB. 1 &#8211; Grid search interval over parameters involved in data representation.
</p>
<p>Given that the data used in our experiments contains only about 0.07% utterances marking
thematic boundaries relative to the total number of utterances in the corpus, we handle the
imbalance between the number of positive and negative examples for the SVM classifier by
using an assymetric soft margin optimization, which charges more for false negatives than for
false positives. That is, we set the regularization parameter C+ several times larger than C&#8722;:
C+ =
</p>
<p>&#8968;
n
</p>
<p>n+&#8722;1 &#8722; 1
&#8969; &#183;C&#8722;, where n is the total number of training examples and n+ is the number
</p>
<p>of positive training examples.
</p>
<p>Model selection is done in two phases, as described below.
</p>
<p>The first step in model selection consists of searching for the most appropriate utterance repre-
sentation by using each individual category of features. That is, we look for appropriate values
for the size of the windows (intervals) considered when measuring &#8220;speaker activity&#8221; and when
taking into account &#8220;syntactic information&#8221; and &#8220;silences and overlaps&#8221; for the utterance ins-
tance (cf. Section 2). This is determined by performing a grid search interval over various values
for activityWS, synWS, silenceWSL, silenceWSR, overlapWSL and overlapWSR. For each &#8220;win-
dow size (WS)&#8221; parameter, the range of values we select from is given in the second column
of Table 1. Note that for the features based on lexical reiteration, we have used the optimal pa-
rameter settings that have been determined in (Georgescul et al., 2006b). In this step, we train
the SVMs with fixed values for both the RBF kernel parameter and the regularization parame-
ters C+ and C&#8722;, i.e. the magnitude of the penalty for violating the soft margin has been set to:
C&#8722; = 1 ; while the RBF kernel parameter has been set to: &#947; = 1.
</p>
<p>Using the entire set of features with the representations selected in the first step (cf. the third
column of Table 1), the second step in model selection consists in optimizing the parameters of
the classifier, i.e. the regularization parameters C+ and C&#8722; and the RBF kernel parameter &#947;.
That is, we perform grid search interval over the following values: C&#8722;1 &#8712; {10&#8722;3, 10&#8722;2, 10&#8722;1, 1,
10, 102, 103}, &#947; &#8712; {2&#8722;6, 2&#8722;5, 2&#8722;4, 2&#8722;3, 2&#8722;2, 2&#8722;1, 1, 2, 22, 23, 24, 25, 26}.
</p>
<p>4.2 Results
</p>
<p>The results obtained on the ICSI-MR corpus using only the proposed surface conversational
cues, (i.e. excluding the features based on lexical reiteration), in our SVM approach for thematic
segmentation are illustrated in Figure 1. The table gives means for the percentage error rates
given by Pk metric (Beeferman et al., 1999) and the Prerror metric (Georgescul et al., 2006a)
for the systems we have used throughout our work. We provide as baselines the error rates
obtained when using TextTiling (Hearst, 1997), C99 (Choi, 2000), TextSeg (Utiyama &amp; Isahara,
2001) and Random, a naive segmentation algorithm (by which the number of boundaries is
randomly selected and boundaries are randomly distributed throughout text).
</p>
<p>20</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Exploiting structural meeting-specific features for topic segmentation
</p>
<p>0
</p>
<p>20
</p>
<p>40
</p>
<p>60
</p>
<p>80
</p>
<p>Random 68.48 53.38
</p>
<p>TextTiling 38.22 40.17
</p>
<p>C99 54.62 40.27
</p>
<p>TextSeg 40.82 35.45
</p>
<p>SVMStructFeat 32.17 31.45
</p>
<p>P_{k} Pr_{error}
</p>
<p>FIG. 1 &#8211; Comparative performance of our SVM approach using only structural features with
various topic segmentation systems run on ICSI-MR data.
</p>
<p>From Figure 1, we observe that by following the quantitative assessment of both Pk error and
the Prerror, our method, labeled as SVMStructFeat, using only surface-features outperforms
other topic segmentation systems reported on in the literature.
</p>
<p>The error values for topic segmentation on the ICSI-MR corpus when using the entire set of
features (i.e. lexical, syntactic and prosodic information) are given in the first row of Table
2. The error rates of our method using both lexical and structural features, i.e the error rates
of SVMLexical+StructFeat in the first row of Table 2, as compared to those obtained in (Geor-
gescul et al., 2006b), i.e. the error rates of SVMLexicalFeat in second row of Table 2, show
that performance gains can be achieved with the help of surface features in addition to word
distribution-based features.
</p>
<p>System Pk error rate Prerror error rate
SVMLexical+StructFeat 20.94 % 20.17%
</p>
<p>SVMLexicalFeat 21.68% 21.83%
</p>
<p>TAB. 2 &#8211; Comparative performance of our SVM approach when using only lexical features
(second row) and when using both lexical and structural features (first row).
</p>
<p>0
</p>
<p>0.1
</p>
<p>0.2
</p>
<p>0.3
</p>
<p>0.4
</p>
<p>0.5
</p>
<p>0.6
</p>
<p>0.7
</p>
<p>0.8
</p>
<p>0.9
</p>
<p>0.0
156
</p>
<p>3
0.0
</p>
<p>625 0.2
5 1 4 16 64
</p>
<p>Testing Error
</p>
<p>Normalized number of
support vectors
</p>
<p>FIG. 2 &#8211; Plotting the error rates on testing data and the normalized number of support vectors
when tuning &#947;, the RBF kernel parameter.
</p>
<p>21</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Maria GEORGESCUL, Alexander CLARK, Susan ARMSTRONG
</p>
<p>Figure 2 shows the influence of the kernel width both on the testing error curves and on the
number of support vectors when C&#8722; = 10&#8722;2 (the optimal value selected through the procedure
described in Section 4.1). We observe that in the optimality region the curve representing the er-
ror rates has a similar behavior as the curve corresponding to the normalized number of support
vectors. That is, the minimum area in the number of support vectors corresponds to minimum
error values of SVM-based topic segmentation on testing data. Therefore the number of support
vectors is a good indicator of the optimality region.
</p>
<p>We also observe from Figure 2 that the number of support vectors is rather large for all tuning
values of &#947;. This reflects the fact that the positive samples (corresponding to the &#8216;topic bounda-
ry&#8217; class) are not easily separable from the negative examples (corresponding to the &#8216;non-topic
boundary&#8217; class) due to noise. Moreover, our SVM approach has the critical property of dif-
ferentiating between positive and negative class members by effectively removing the existing
uninformative patterns from the data.
</p>
<p>5 Comparison to other work
</p>
<p>Comparing the performance of our model to other similar existing studies is not straightfor-
ward due to differences in corpora, in experimental design, and/or different input assumptions.
Nevertheless, in the following we discuss some related work, by exemplifying some common
aspects of the work and the experimental results.
</p>
<p>Kauchak and Chen (2005) examined how the boundaries of thematic episodes can be detec-
ted in encyclopedia articles and in two books. They employ a supervised technique based on
support vector machines using a variety of information including, for instance, features based
on the presence of paragraph breaks, pronouns and named entities. When evaluating their topic
segmentation model on encyclopedia articles, they obtained a Pk error rate of 39.8%.
</p>
<p>Note that, in the context of spontaneous multiparty dialogue, the lack of paragraphs makes the
topic segmentation task more difficult than the topic segmentation of narrative written text. For
instance the chance of each paragraph break being a topic boundary is about 39.1% in expo-
sitory texts (Hearst, 1997), while in the ICSI-MR corpus, the chance of each utterance to be a
subtopic segment boundary is approximately 0.07% for top-level boundaries. Moreover, mee-
ting dialogues provide particular challenges since topic changes are not always clearly delimited
in contrast to e.g. broadcast news or written texts.
</p>
<p>The model proposed in (Galley et al., 2003) is the most similar to our model in terms of incor-
porating multi-party meeting specific features such as cue phrases, silences and conversation
overlaps. Using such strucural features in addition to lexical chains, Galley et al. (2003) trained
a decision tree which achieved a Pk error rate of 23% on a subset of the ICSI-MR corpus.
</p>
<p>6 Conclusions and future work
</p>
<p>In this article, we have presented an approach to learn the thematic structure of texts in the
context of recorded and transcribed multi-party dialogs. Each utterance is represented as a
collection of features obtained from lexical, syntactic and prosodic information. A SVM-
based classifier has been trained to discriminate between utterances marking thematic and non-
</p>
<p>22</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Exploiting structural meeting-specific features for topic segmentation
</p>
<p>thematic boundaries in meeting transcriptions.
</p>
<p>Our contribution is fivefold. First, we introduce a series of different linguistic and acoustic cues
to represent each utterance and we evaluate whether the proposed surface (meeting-specific)
cues are useful for thematic segmentation. Second, we check the suitability of our SVM ap-
proach combining meeting-specific surface features with large-scale lexical features. Third, we
evaluate the compatibility of SVM classification for various thresholds. Fourth, we study the
influence of the kernel width on the testing error rate and on the (normalized) number of sup-
port vectors. Fifth, we compare the results with existing state-of-the-art methods for topic seg-
mentation. We demonstrate that using &#8216;surface&#8217; meeting specific features, our SVM approach
generates competitive results on meeting data sets.
</p>
<p>As a continuation of this work, it would be interesting to replicate our experiments on larger
training sets. The proposed method can potentially be improved by exploiting additional sources
of information, including for instance other prosodic information such as speech pitch range and
speech rate. It would be also interesting to evaluate whether our topic segmentation approach
can be further improved via other kernel methods.
</p>
<p>Aknowledgments
</p>
<p>This work is part of the Swiss National Center of Competence in Research on &#8220;Interactive
Multimodal Information Management&#8221; (IM2, http://www.im2.ch), funded by the Swiss
National Science Foundation.
</p>
<p>References
</p>
<p>BEEFERMAN D., BERGER A. &amp; LAFFERTY J. (1999). Statistical Models for Text Segmenta-
tion. Machine Learning, 34(Special Issue on Natural Language Learning), 177&#8211;210.
</p>
<p>CHOI F. (2000). Advances in Domain Independent Linear Text Segmentation. In Proceedings
of the 1st Conference of the North American Chapter of the Association for Computational
Linguistics (NAACL), Seattle, USA.
</p>
<p>GALLEY M., MCKEOWN K., FOSLER-LUISSIER E. &amp; JING H. (2003). Discourse Seg-
mentation of Multi-Party Conversation. In Proceedings of the 41st Annual Meeting of the
Association for Computational Linguistics (ACL), p. 562&#8211;569, Sapporo, Japan.
</p>
<p>GEORGESCUL M., CLARK A. &amp; ARMSTRONG S. (2006a). An Analysis of Quantitative
Aspects in the Evaluation of Thematic Segmentation Algorithms. In Proceedings of the 7th
SIGdial Workshop on Discourse and Dialogue, p. 144&#8211;151, Sydney, Australia: Association for
Computational Linguistics.
</p>
<p>GEORGESCUL M., CLARK A. &amp; ARMSTRONG S. (2006b). Word Distributions for Thematic
Segmentation in a Support Vector Machine Approach. In Proceedings of the 10th Conference
on Computational Natural Language Learning (CoNLL), p. 101&#8211;108, New York City, USA.
</p>
<p>HEARST M. (1997). TextTiling: Segmenting Text into Multi-Paragraph Subtopic Passages.
Computational Linguistics, 23(1), 33&#8211;64.
</p>
<p>23</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Maria GEORGESCUL, Alexander CLARK, Susan ARMSTRONG
</p>
<p>HIRSCHBERG J. &amp; NAKATANI C. (1996). A Prosodic Analysis of Discourse Segments in
Direction-Giving Monologues. In Proceedings of the 34th Annual Meeting on Association for
Computational Linguistics (ACL), p. 286&#8211;293, Santa Cruz, California, USA.
JANIN A., ANG J., BHAGAT S., DHILLON R., EDWARDS J., MACIAS-GUARASA J., MOR-
GAN N., PESKIN B., SHRIBERG E., STOLCKE A., WOOTERS C. &amp; WREDE B. (2004). The
ICSI Meeting Project: Resources and Research. In Proceedings of the International Confe-
rence on Acoustics, Speech and Signal Processing (ICASSP), Meeting Recognition Workshop,
Montreal, Quebec, Canada.
JOACHIMS T. (1999). Making Large-Scale Support Vector Machine Learning Practical. In B.
SCH&#214;LKOPF, C. BURGES &amp; A. SMOLA, Eds., Advances in Kernel Methods - Support Vector
Learning. Cambridge, MA: MIT Press.
KAUCHAK D. &amp; CHEN F. (2005). Feature-Based Segmentation of Narrative Documents. In
Proceedings of the ACL Workshop on Feature Engineering for Machine Learning in Natural
Language Processing, p. 32&#8211;39, Ann Arbor, Michigan, USA.
LITMAN D. J. &amp; PASSONNEAU R. J. (1995). Combining Multiple Knowledge Sources for
Discourse Segmentation. In Proceedings of the 33rd Annual Meeting of the Association for
Computational Linguistics (ACL), p. 108&#8211;115, Cambridge, Massachusetts, USA.
MARCU D. (2000). The Theory and Practice of Discourse Parsing and Summarization. MIT
Press Cambridge, MA, USA.
PASSONNEAU R. J. &amp; LITMAN D. J. (1993). Intention-based Segmentation: Human Re-
liability and Correlation with Linguistic Cues. In Proceedings of the 31st Conference on
Association for Computational Linguistics (ACL), p. 148 &#8211; 155, Columbus, Ohio, USA.
PFAU T., ELLIS D. P. &amp; STOLCKE A. (2001). Multispeaker Speech Activity Setection for the
ICSI Meeting Recorder. In Proceedings of the IEEE Workshop on Automatic Speech Recogni-
tion and Understanding, p. 107&#8211;110.
SCHMID H. (1994). Probabilistic Part-of-Speech Tagging Using Decision Trees. In Procee-
dings of the International Conference on New Methods in Language Processing, Stuttgart,
Germany.
UTIYAMA M. &amp; ISAHARA H. (2001). A Statistical Model for Domain-Independent Text
Segmentation. In Proceedings of the 39th Annual Meeting of the Association for Computa-
tional Linguistics and the 10th Conference of the European Chapter of the Association for
Computational Linguistics (ACL/EACL), p. 491&#8211;498, Toulouse, France.
VAPNIK V. N. (1998). Statistical Learning Theory. A Volume in the Wiley Series on Adaptive
and Learning Systems for Signal Processing, Communications, and Control. Berlin: Springer-
Verlag.
</p>
<p>24</p>

</div></div>
</body></html>