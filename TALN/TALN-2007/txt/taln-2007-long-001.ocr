TALN 2007, Toulouse, 5-8 juin 2007

Exploiting structural meeting-speciﬁc features
for topic segmentation

Maria GEORGESCUL1, Alexander CLARK2, Susan ARMSTRONG1
1 ISSCO/TIIVI/ETI, University of Geneva
2 Department of Computer Science, Royal Holloway University of London
maria . georgescul@eti . unige . ch, alexc@cs . rhul . ac . uk,
Susan . armstrongféissco . unige . ch

Résumé. Dans cet article, nous traitons de la segmentation automatique des textes en épi-
sodes tlrématiques non superposés et ayant une structure linéaire. Notre étude porte sur l’uti-
lisation des traits lexicaux, acoustiques et symtaxiques et sur l’inﬂuence de ces traits sur la
performance d’un systéme automatique de segmentation thérnatique. Nous appliquons notre
approche, basée sur des machines a vecteurs support, a des transcriptions des dialogues multi-
locuteurs.

Abstract. In this article we address the task of automatic text structuring into linear
and non-overlapping thematic episodes. Our investigation reports on the use of various lexi-
cal, acoustic and syntactic features, and makes a comparison of how these features inﬂuence
performance of automatic topic segmentation. Using datasets containing multi-party meeting
transcriptions, we base our experiments on a proven state-of-the-art approach using support
vector classiﬁcation.

Mots-clés 2 segmentation automatique en épisodes thématiques, machines a vecteurs
support, dialogues m11lti-locuteurs.

Keywords: automatic topic segmentation, support vector machines, multi-party dia-
logues.

1 Introduction

Georgescul et al. (2006b) proposed a support vector machine approach to the task of text
segmentation which demonstrates improvements over state-of-the-art techniques, by modeling
large scale (merely lexical) features and non-linear relations in an efﬁcient and stable way. Their
experimental results showed that word distributions in texts provide relevant information for the
detection of boundaries between thematic episodes in data sets covering different domains. In
this paper, we put the emphasis on tackling the topic segmentation problem in the context of
recorded and transcribed multi-party dialogs. In particular, we extend the work of Georges-
cul et al. (2006b) by exploring potential information provided by ‘surface’ cues in multi-party
dialogues such as symtactic knowledge, cue-phrases and acoustic cues. We investigate the per-
tinence of these factors individually and in combination with information provided by word
distributions through the interrnediurn of transductive support vector machines.

15

Maria GEORGESCUL, Alexander CLARK, Susan ARMSTRONG

In order to identify boundaries, we model the thematic segmentation task as a binary classi-
ﬁcation problem. The features considered for designing the classiﬁer are described ir1 Section
2. In Section 3 we highlight how the classiﬁcation model is constructed by using transductive
support vector learning. A comparative analysis of the support vector classiﬁer performance by
using these cues is provided in Section 4.

2 Input features

As in (Georgescul et al., 2006b), we consider the thematic segmentation task as a binary clas-
siﬁcation problem, where each utterance should be classiﬁed as a topic boundary or not. As
explained ir1 Section 3, we employ a support vector machine classiﬁer which is given as input a
vectorial representation of the utterance to be classiﬁed and its context. Each dimension of the
input vector indicates the value of a certain feature characterizing the utterance. For utterance
characterization, Georgescul et al. (2006b) only considered features based on observations of
patterns in vocabulary use. Here, in addition to these lexical features, we consider meeting-
speciﬁc features as described in the following.

Note that, similar types of features examined ir1 our study have been previously proposed for
analyzing discourse structure in state-of-the-art studies like those described in (Litman & Pas-
sonneau, 1995; Hirschberg & Nakatani, 1996; Galley et al., 2003). These include speaker acti-
vity, discourse markers, prosodic and syntactic features.

2.1 Speaker activity

According to (Pfau et al., 2001), patterns of speech activity are valuable data for discourse ana-
lysis. In order to explore this claim, the ﬁrst pattern we chose to investigate is speaker activity. In
particular, we start with the hypothesis that in meeting data the contribution of each participant
in the discussion can signal a new topic. For instance, some participants could have a preference
for certain subjects of discussion.

We take into account the changes in speaker activity by measuring the number of words each
participant uttered before and after each utterance candidate to a thematic boundary. This is
formalized in the following manner. Let A}: be the number of words that the participant .9 said hi
utterance uk. For each meeting participant 3 and for each i-th utterance u,, we take into account
the number of words that the participant .9 uttered before and during u, in an interval of size
activityWS, by considering the vector  as:  = (Af_mmtyWS+1, Af_mmtyWS+2, ..., Af).
We also store in a vector  the number of words that the participant 3 uttered after u, ir1
an interval of size activityWS:  = (Af+1, Af+2, ..., Af+ammtyWS). We then normalize the
two vectors  and  to form two probability distributions lf and rf, respectively. That is,
we perform the normalization by simply dividing each element in the vector by the sum of all
entries in the vector.

We measure signiﬁcant changes ir1 speaker activity by using the information radius between the
probability distributions given by the speaker activity at the left and right side of the current (u,)

16

Exploiting structural meeting-speciﬁc features for topic segmentation

utterance:

1 -F -?
IRad(lf,rf) = 5 2lflog% + Z rflog% (1)
. ’I. . ’I.
l;?;0 rg;é0

where m, =  is the average distribution of the two random variables lf and rf.

Finally, I Rad (lf ,  will constitute the entry for one dimension of the vectorial representation
for utterance u,.

2.2 Discourse markers

Previous studies (Litman & Passonneau, 1995; Marcu, 2000) addressed questions regarding dis-
course relations and their realization by discourse markers. Here, we are interested in ﬁnding
those discourse markers that indicate thematic shifts in our data. We started with the following
list of discourse markers that has been symthesized from a commonly used list of discourse
markers: “accordingly”, “actually”, “after all”, “also”, “although”, “anyway”, “back to”, “basi-

n u

cally”, “but”, “ﬁne”, “for example”, “furthermore”, “generally”, “however”, “like , moreover”,
“nevertheless”, “nor”, “now”, “of course”, “okay”, “really”, “similarly”, “since”, “speaking of”,
“so”, “still”, “that’s all”, “then”, “therefore”, “well”. For each discourse marker in this list, we
automatically examine if it occurs in each utterance that is a candidate for marking a thematic
boundary. That is, our SVM takes as input binary features indicating whether each discourse
marker occurs in the current utterance. We retain as input features to our system only those

discourse markers that occur at least once in our corpus.

2.3 Syntactic features

The use of symtax-based features is to a large extent motivated by previous work (Passonneau &
Litman, 1993; Litman & Passonneau, 1995) relating discourse structure and noun phrase ana-
phora. Regarding the pronominal reference, we are mainly following the intuitive assumption
that nouns and verbs appear more frequently at the beginning of a new topic, while pronouns
appear more frequently in the middle of a thematic episode.

The symtactic features considered in our study are the distributions of different part-of-speech
categories before and after a potential thematic boundary. That is, we extracted frequencies
of pronouns, (proper) nouns and verbs before and after each utterance candidate to a thema-
tic boundary. For the annotation of part-of-speech information, we used TreeTagger (Sclnnid,
1994).

This component was formalized as follows. Let P,, N,, V, be the number of pronouns, nouns and
verbs, respectively, in utterance u,<. We store in a vector  the number of pronouns occurring
in utterances situated before u, in an interval of size synWS 2  = (P¢_s,,nws+1, Pz‘—synWS+2:
..., Pi). We also store in a vector fr: the number of pronouns occurring in utterances situated
after u, in an interval of size synWS: fig) = (PH1, P,<+2, ..., Pi+3ynWS). Similarly, we store in
vectors   the number of nouns and verbs, respectively occurring before u, in an interval
of size synWS utterances. Then, the vectors f_7'":,  will contain the number of nouns and
verbs, respectively occurring after u, in an interval of size synWS utterances.

l7

Maria GEORGESCUL, Alexander CLARK, Susan ARMSTRONG

As in Section 2.1, we normalize the resulting vectors of counts 137?, f_2'":,  fry,   to
obtain probability distributions lf, rf, lg, 2",", lg, rf, respectively. Finally, we measure changes in
the distribution of pronouns, nouns and verbs at the left and right side of the current utterance
by using the information radius (see Equation 1). That is, for each utterance 27,, we measure
I Rad (lf, ), I Rad (Z3, 2","), I Rad (lf, 2",?’ ), which will constitute entries for three dimensions of
the vectorial representation for utterance 27, (taken as input to the SVM classiﬁer).

2.4 Silences and overlaps

Silences and overlaps, as well as other acoustic information can also give evidence whether a
major topic shift occurred. In particular, studies on discourse structure (Hirschberg & Naka-
tani, 1996) exploit various prosodical information such as pitch range (raised at segment-initial
phrases and lower at segrnent-ﬁnal phrases), speech rate (accelerating at segrnent-ﬁnal phrases),
amplitude and contour.

We investigated the pertinence of these features with the following formalization. Let S, be
the silence duration between utterance 27,_1 and 27,. Let 0, be the speaker overlap duration
between utterance 271-1 and 27,. We normalize the S, and O, values by speaker and the resulting

values S15, 0; are used to compute the following quantities: sl, = 22:1; silemews L +1  ;

‘ ‘z s r ‘ r ‘ z s r
5“ = CEW R  ? dz‘ = ZIr:='i—oruerlapWSL+1  i and 07% =  WW R  -
We include silences and overlaps as part of the utterance context representation by considering
the sli, an, ol,<, on quantities as dimensions of the vector characterizing the utterance 27,.

3 Methodology

As introduced in the previous section, we employ a vectorial representation containing lexical,
acoustic and syntactic information to characterize each utterance. The topic segmentation task
is thus reduced to a binary classiﬁcation problem: each utterance has to be classiﬁed as marking
the presence or the absence of a topic shift in the text.

In order to infer eventual dependencies between the binary class label and observations of pat-
terns (provided by the lexical, acoustic and symtactic information), we employ a discriminative
approach based on transductive support vector learning. A brief overview on inductive support
vector learning for topic segmentation has been described in (Georgescul et al., 2006b). In this
section, we give some highlights representing the main elements in using transductive support
vector learning for topic segmentation.

The support vector learner E is given a training set Strum = ((271,y1), ..., (27,,, y,,)) Q (U X Y)"
containing 21 examples drawn independently and identically distributed (i.i.d.) according to a
ﬁxed distribution Pr(27, y) = Pr(y|27)Pr(27). Following the transductive setting proposed by
Joachims (1999), the learner is also given an i.i.d. sample, Stat = (27{, 275, ..., 27;), containing
k test examples from the same distribution as 271, 272,  ,27,,. Each training example from Strum
consists of a high-dimensional vector 27' describing an utterance and the class label 2;. The class
label 2; has only two possible values: +1 (corresponding to a ‘thematic boundary’) or -1 (corres-
ponding to a ‘non-thematic boundary’). We represent each utterance instance by a feature vector
27 with attributes containing ‘surface’ meeting-speciﬁc information (as described in Section 2)

18

Exploiting structural meeting-speciﬁc features for topic segmentation

plus the attributes given by the bag-of-words representation of word frequencies, as described
in (Georgescul et al., 2006b).

Given a hypothesis space H, of functions h : U —> {—1, +1} having the form h(z2') =
sign((zD', 22’) +b), the transductive learner £t,,,,,_,d seeks a decision function ht,,,,,_,d from H, using
Strum and Stat so that the expected number of erroneous predictions on the test examples is mi-
nimized. Using the structural risk minimization principle (Vapnik, 1998), the smallest bound on
the test error is calculated by minimizing the following cost function W‘"""’d:

WtTansd(yl7  13: b7€17€27 "'7 En: £17 £2:  =
1

n n k:
- <zv,zv>+C+ Z &+C- 2 ;+C*§j£;,

_ 2 . . .
7-=02?/i=1 'l=0:!Iz'="1 J=0

(2)

subjectto:
y, +b] 3 1 -5, for 2' = 1,2,...,n;
  +b] 3 1 -5,?‘ forj = 1,2,...,k;
 E {—1, 1} forj = 1,2, ..., k:.
51 2 Ofor 2' = 1,2,...,n;

5;‘ Z Oforj = 1,2,...,k;

The so-called slack variables 5, and E; are introduced in order to be able to handle non-separable
data. The regularization parameters C‘ and C+ are tuned as described in Section 4.1.

4 Experiments and results

4.1 Parameter estimation

We train and evaluate the effectiveness of our technique on the ICSI-MR dataset (Janin et al.,
2004) containing transcribed multi-party dialogs.

We divide the ICSI-MR data set into two disjoint parts: a training dataset composed of 80% of
the initial data set, while the remaining 20% is held out for testing purposes. That is, the training
set is used to determine the best model settings for the SVM classiﬁer, while the test set is used
to determine the ﬁnal topic segmentation error rate.

We select the best model parameters, by running ﬁve-fold cross validation for SVM parameter
estimation, using the Gaussian RBF kernel. During this preliminary step we estimate the perfor-
mance of the SVM classiﬁer by using the precision and recall, i.e. the precision/recall-breakeven
point (Joachin1s, 1999). The choice of binary evaluation metrics in this step was motivated by
the fact that posing the topic segmentation task as a classiﬁcation problem involves a loss of the
sequential nature of the data, which is an inconvenience in computing the Pk (Beeferman et al.,
1999) or Pram, (Georgescul et al., 2006a) measures.

19

Maria GEORGESCUL, Alexander CLARK, Susan ARMSTRONG

Parameter Interval for grid search Best window size
activityWS 5. . .50 step 5 35 utterances
synWS 5 . . . 50 step 5 30 utterances
silenceWSL 2 . . . 10 step 1 6 utterances
silenceWSR 2 . . . 10 step 1 3 utterances
over-lapWSL 2 . . . 10 step 1 2 utterances
over-lapWSR 2 . . . 10 step 1 4 utterances

TAB. 1 — Grid search interval over parameters involved in data representation.

Given that the data used in our experiments contains only about 0.07% utterances marking
thematic boundaries relative to the total number of utterances in the corpus, we handle the
imbalance between the number of positive and negative examples for the SVM classiﬁer by
using an assymetric soft margin optimization, which charges more for false negatives than for
false positives. That is, we set the regularization parameter C+ several times larger than C‘:
C+ = [n+"_1 — 1-| - C‘, where n is the total number of training examples and 71”“ is the number
of positive training examples.

Model selection is done in two phases, as described below.

The ﬁrst step in model selection consists of searching for the most appropriate utterance repre-
sentation by using each individual category of features. That is, we look for appropriate values
for the size of the windows (intervals) considered when measuring “speaker activity” and when
taking into account “symtactic information” and “silences and overlaps” for the utterance ins-
tance (cf. Section 2). This is determined by performing a grid search interval over various values
for activityWS, synWS, silenceWSL, silenceWSR, overlapWSL and overlapWSR. For each “win-
dow size (WS)” parameter, the range of values we select from is given in the second colurrm
of Table 1. Note that for the features based on lexical reiteration, we have used the optimal pa-
rameter settings that have been determined in (Georgescul et al., 2006b). In this step, we train
the SVMs with ﬁxed values for both the RBF kernel parameter and the regularization parame-
ters C+ and C‘, i.e. the magnitude of the penalty for violating the soft margin has been set to:
C‘ = 1 ; while the RBF kernel parameter has been set to: 7 = 1.

Using the entire set of features with the representations selected in the ﬁrst step (cf. the third
colurrm of Table 1), the second step in model selection consists in optimizing the parameters of
the classiﬁer, i.e. the regularization parameters C+ and C‘ and the RBF kernel parameter 7.
That is, we perform grid search interval over the following values: C‘1 6 {10‘3, 10‘2, 10‘1, 1,
10,102,103},'y E {2‘5,2‘5,2‘4,2‘3,2‘2, 2‘1, 1,2, 22,23, 24,25, 25}.

4.2 Results

The results obtained on the ICSI-MR corpus using only the proposed surface conversational
cues, (i.e. excluding the features based on lexical reiteration), in our SVM approach for thematic
segmentation are illustrated in Figure 1. The table gives means for the percentage error rates
given by P), metric (Beeferrnan et al., 1999) and the Pram, metric (Georgescul et al., 2006a)
for the systems we have used throughout our work. We provide as baselines the error rates
obtained when using Texﬂiling (Hearst, 1997), C99 (Choi, 2000), TextSeg (U tiyarna & Isahara,
2001) and Random, a naive segmentation algorithm (by which the number of boundaries is
randomly selected and boundaries are randomly distributed throughout text).

20

Exploiting structural meeting-speciﬁc features for topic segmentation

 

FIG. 1 — Comparative performance of our SVM approach using only structural features with
various topic segmentation systems run on ICSI-MR data.

From Figure 1, we observe that by following the quantitative assessment of both Pk error and
the Pram”, our method, labeled as SVMStructFeat, using only surface-features outperforms
other topic segmentation systems reported on in the literature.

The error values for topic segmentation on the ICSI-MR corpus when using the entire set of
features (i.e. lexical, syntactic and prosodic information) are given in the ﬁrst row of Table
2. The error rates of our method using both lexical and structural features, i.e the error rates
of SVMLm<a,,+_gt,,wtpe,,t in the ﬁrst row of Table 2, as compared to those obtained in (Geor-
gescul et al., 2006b), i.e. the error rates of SVMLe,m;pwt in second row of Table 2, show
that performance gains can be achieved with the help of surface features in addition to word
distribution-based features.

System Pk error rate Pram, error rate
S VM Lexical +Str1wtFeat  %  17 %
SVMLm«c,,lpe,,t 21.68% 21.83%

TAB. 2 — Comparative performance of our SVM approach when using only lexical features
(second row) and when using both lexical and structural features (ﬁrst row).

—o— Testing Error

M —u— ' number of
SUDDOI1 \BClOrS
0.3 - 9: i : :
0.2 —

&,g;»d.¢>°—_a= ~ "‘>3’2:"‘
Q.

0.

FIG. 2 — Plotting the error rates on testing data and the normalized number of support vectors
when tuning 7, the RBF kernel parameter.

21

Maria GEORGESCUL, Alexander CLARK, Susan ARMSTRONG

Figure 2 shows the inﬂuence of the kernel width both on the testing error curves and on the
number of support vectors when C‘ = l0‘2 (the optimal value selected through the procedure
described in Section 4.1). We observe that in the optimality region the curve representing the er-
ror rates has a similar behavior as the curve corresponding to the normalized number of support
vectors. That is, the minimum area hi the number of support vectors corresponds to minimum
error values of SVM-based topic segmentation on testing data. Therefore the number of support
vectors is a good indicator of the optimality region.

We also observe from Figure 2 that the number of support vectors is rather large for all tuning
values of 7. This reﬂects the fact that the positive samples (corresponding to the ‘topic bounda-
ry’ class) are not easily separable from the negative examples (corresponding to the ‘non-topic
boundary’ class) due to noise. Moreover, our SVM approach has the critical property of dif-
ferentiating between positive and negative class members by effectively removing the existing
uninformative patterns from the data.

5 Comparison to other work

Comparing the performance of our model to other similar existing studies is not straightfor-
ward due to differences in corpora, ir1 experimental design, and/or different input assumptions.
Nevertheless, in the following we discuss some related work, by exemplifying some common
aspects of the work and the experimental results.

Kauchak and Chen (2005) examined how the boundaries of thematic episodes can be detec-
ted in encyclopedia articles and in two books. They employ a supervised technique based on
support vector machines using a variety of information including, for instance, features based
on the presence of paragraph breaks, pronouns and named entities. When evaluating their topic
segmentation model on encyclopedia articles, they obtained a Pk error rate of 39.8%.

Note that, in the context of spontaneous multiparty dialogue, the lack of paragraphs makes the
topic segmentation task more difﬁcult than the topic segmentation of narrative written text. For
instance the chance of each paragraph break being a topic boundary is about 39.1% in expo-
sitory texts (Hearst, 1997), while in the ICSI-MR corpus, the chance of each utterance to be a
subtopic segment boundary is approximately 0.07% for top-level boundaries. Moreover, mee-
ting dialogues provide particular challenges since topic changes are not always clearly delimited
in contrast to e. g. broadcast news or written texts.

The model proposed ir1 (Galley et al., 2003) is the most similar to our model ir1 terms of incor-
porating multi-party meeting speciﬁc features such as cue phrases, silences and conversation
overlaps. Using such strucural features in addition to lexical chains, Galley et al. (2003) trained
a decision tree which achieved a Pk error rate of 23% on a subset of the ICSI-MR corpus.

6 Conclusions and future work

In this article, we have presented an approach to learn the thematic structure of texts in the
context of recorded and transcribed multi-party dialogs. Each utterance is represented as a
collection of features obtained from lexical, symtactic and prosodic information. A SVM-
based classiﬁer has been trained to discriminate between utterances marking thematic and non-

22

Exploiting structural meeting-speciﬁc features for topic segmentation

thematic boundaries in meeting transcriptions.

Our contribution is ﬁvefold. First, we introduce a series of different linguistic and acoustic cues
to represent each utterance and we evaluate whether the proposed surface (meeting-speciﬁc)
cues are useful for thematic segmentation. Second, we check the suitability of our SVM ap-
proach combining meeting-speciﬁc surface features with large-scale lexical features. Third, we
evaluate the compatibility of SVM classiﬁcation for various thresholds. Fourth, we study the
inﬂuence of the kernel width on the testing error rate and on the (normalized) number of sup-
port vectors. Fifth, we compare the results with existing state-of-the-art methods for topic seg-
mentation. We demonstrate that using ‘surface’ meeting speciﬁc features, our SVM approach
generates competitive results on meeting data sets.

As a continuation of this work, it would be interesting to replicate our experiments on larger
training sets. The proposed method can potentially be improved by exploiting additional sources
of information, including for instance other prosodic information such as speech pitch range and
speech rate. It would be also interesting to evaluate whether our topic segmentation approach
can be further improved via other kernel methods.

Aknowledgments

This work is part of the Swiss National Center of Competence in Research on “Interactive
Multimodal Information Management” (IM2, http : / / www. im2 . ch), funded by the Swiss
National Science Foundation.

References

BEEFERMAN D., BERGER A. & LAFFERTY J. (1999). Statistical Models for Text Segrnenta-
tion. Machine Learning, 34(Special Issue on Natural Language Learning), 177-210.

CHOI F. (2000). Advances in Domain Independent Linear Text Segmentation. In Proceedings
of the I st Conference of the North American Chapter of the Association for Computational
Linguistics (NAACL), Seattle, USA.

GALLEY M., MCKEOWN K., FOSLER-LUISSIER E. & JING H. (2003). Discourse Seg-
mentation of Multi-Party Conversation. In Proceedings of the 41st Annual Meeting of the
Association for Computational Linguistics (ACL), p. 562-569, Sapporo, Japan.

GEORGESCUL M., CLARK A. & ARMSTRONG S. (2006a). An Analysis of Quantitative
Aspects in the Evaluation of Thematic Segmentation Algoritlnns. In Proceedings of the 7th
SIGdial Workshop on Discourse and Dialogue, p. 144-151, Sydney, Australia: Association for
Computational Linguistics.

GEORGESCUL M., CLARK A. & ARMSTRONG S. (2006b). Word Distributions for Thematic
Segmentation in a Support Vector Machine Approach. In Proceedings of the I 0th Conference
on Computational Natural Language Learning (CoNLL), p. 101-108, New York City, USA.

HEARST M. (1997). TextTiling: Segmenting Text into Multi-Paragraph Subtopic Passages.
Computational Linguistics, 23(1), 33-64.

23

Maria GEORGESCUL, Alexander CLARK, Susan ARMSTRONG

HIRSCHBERG J. & NAKATANI C. (1996). A Prosodic Analysis of Discourse Segments in
Direction-Giving Monologues. In Proceedings of the 34th Annual Meeting on Association for
Computational Linguistics (ACL), p. 286-293, Santa Cruz, California, USA.

JANIN A., ANG J., BHAGAT S., DHILLON R., EDWARDS J., MACIAS-GUARASA J., MOR-
GAN N., PESKIN B., SHRIBERG E., STOLCKE A., WOOTERS C. & WREDE B. (2004). The
ICSI Meeting Project: Resources and Research. Ir1 Proceedings of the International Confe-
rence on Acoustics, Speech and Signal Processing ( I CASSP), Meeting Recognition Workshop,
Montreal, Quebec, Canada.

J OACHIMS T. (1999). Making Large-Scale Support Vector Machine Learning Practical. In B.
SCH(")LKOPF, C. BURGES & A. SMOLA, Eds., Advances in Kernel Methods - Support Vector
Learning. Cambridge, MA: MIT Press.

KAUCHAK D. & CHEN F. (2005). Feature-Based Segmentation of Narrative Documents. In
Proceedings of the ACL Workshop on Feature Engineering for Machine Learning in Natural
Language Processing, p. 32-39, Ann Arbor, Michigan, USA.

LITMAN D. J . & PASSONNEAU R. J . (1995). Combining Multiple Knowledge Sources for
Discourse Segmentation. In Proceedings of the 33rd Annual Meeting of the Association for
Computational Linguistics (ACL), p. 108-115, Cambridge, Massachusetts, USA.

MARCU D. (2000). The Theory and Practice of Discourse Parsing and Summarization. MIT
Press Cambridge, MA, USA.

PASSONNEAU R. J . & LITMAN D. J . (1993). Intention-based Segmentation: Human Re-
liability and Correlation with Linguistic Cues. In Proceedings of the 31st Conference on
Association for Computational Linguistics (ACL), p. 148 - 155, Columbus, Ohio, USA.

PFAU T., ELLIS D. P. & STOLCKE A. (2001). M11ltispeaker Speech Activity Setection for the
ICSI Meeting Recorder. In Proceedings of the IEEE Workshop on Automatic Speech Recogni-
tion and Understanding, p. 107-110.

SCHMID H. (1994). Probabilistic Part-of-Speech Tagging Using Decision Trees. In Procee-
dings of the International Conference on New Methods in Language Processing, Stuttgart,
Germany.

UTIYAMA M. & ISAHARA H. (2001). A Statistical Model for Domain-Ir1dependent Text
Segmentation. In Proceedings of the 39th Annual Meeting of the Association for Computa-
tional Linguistics and the I 0th Conference of the European Chapter of the Association for
Computational Linguistics (ACL/EACL), p. 491-498, Toulouse, France.

VAPNIK V. N. (1998). Statistical Learning Theory. A Volume in the Wiley Series on Adaptive
and Learning Systems for Signal Processing, Communications, and Control. Berlin: Springer-
Verlag.

24

