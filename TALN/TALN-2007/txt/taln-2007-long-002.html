<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
"http://www.w3.org/TR/html4/loose.dtd">
<html><head><title>&#201;nergie textuelle de m&#233;moires associatives</title>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
</head>
<body>
<div style="page-break-before:always; page-break-after:always"><div><p>TALN 2007, Toulouse, 5&#8211;8 juin 2007
</p>
<p>&#201;nergie textuelle de m&#233;moires associatives
</p>
<p>Silvia FERN&#193;NDEZ1,2, Eric SANJUAN1, Juan Manuel TORRES-MORENO1,3
1 Laboratoire Informatique d&#8217;Avignon, BP 1228 84911 Avignon FRANCE
</p>
<p>2 LPM UHP-Nancy, BP 239 54506 Vand&#339;uvre les Nancy FRANCE
3 &#201;cole Polytechnique de Montr&#233;al, CP 6079 Centre-ville, Montr&#233;al, Qu&#233;bec
</p>
<p>CANADA H3C3A7
{silvia.fernandez,eric.sanjuan,juan-manuel.torres}@
</p>
<p>univ-avignon.fr
</p>
<p>R&#233;sum&#233;. Dans cet article1, nous pr&#233;sentons une approche de r&#233;seaux de neurones inspi-
r&#233;e de la physique statistique de syst&#232;mes magn&#233;tiques pour &#233;tudier des probl&#232;mes fondamen-
taux du Traitement Automatique de la Langue Naturelle. L&#8217;algorithme mod&#233;lise un document
comme un syst&#232;me de neurones o&#249; l&#8217;on d&#233;duit l&#8217;&#233;nergie textuelle. Nous avons appliqu&#233; cette
approche aux probl&#232;mes de r&#233;sum&#233; automatique et de d&#233;tection de fronti&#232;res th&#233;matiques. Les
r&#233;sultats sont tr&#232;s encourageants.
Abstract. In this paper we present a neural networks approach, inspired by statistical phy-
sics of magnetic systems, to study fundamental problems in Natural Language Processing. The
algorithm models documents as neural network whose textual energy is studied. We obtained
good results on the application of this method to automatic summarization and thematic borders
detection.
Mots-cl&#233;s : r&#233;seaux de neurones, r&#233;seaux de Hopfield, r&#233;sum&#233;, fronti&#232;re th&#233;matiques.
Keywords: neural networks, Hopfield network, summarization, thematic boundary.
</p>
<p>1 Introduction
Hopfield (Hopfield, 1982; Hertz et al., 1991) s&#8217;est inspir&#233; des syst&#232;mes physiques comme le
mod&#232;le magn&#233;tique d&#8217;Ising (formalisme issu de la physique statistique decrivant un syst&#232;me
avec des unit&#233;s &#224; deux &#233;tats nomm&#233;es spins) pour construire un r&#233;seau neuronal avec des ca-
pacit&#233;s d&#8217;apprentissage et de r&#233;cup&#233;ration de patrons. Les capacit&#233;s et limitations de ce r&#233;seau,
appel&#233; m&#233;moire associative, ont &#233;t&#233; bien &#233;tablies de fa&#231;on th&#233;orique dans plusieurs &#233;tudes (Hop-
field, 1982; Hertz et al., 1991) : les patrons doivent &#234;tre non corr&#233;l&#233;s afin que leur r&#233;cup&#233;ration
soit sans erreur, le syst&#232;me sature rapidement et seulement une fraction des patrons peut &#234;tre
stock&#233;e correctement. D&#232;s que leur nombre d&#233;passe &#8776; 0, 14N , aucun des patrons n&#8217;est plus
reconnu. Cette situation restreint fortement leurs applications pratiques. Cependant, dans le cas
du traitement automatique de la langue naturelle (TALN), nous pensons que l&#8217;on peut exploi-
ter ce comportement. Le mod&#232;le vectoriel de textes (Salton &amp; McGill, 1983), transforme les
</p>
<p>1Ce travail a &#233;t&#233; realis&#233; en partie gr&#226;ce au financement du CONACYT (Mexico), bourse 175225.
25</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Silvia FERN&#193;NDEZ, Eric SANJUAN, Juan Manuel TORRES-MORENO
</p>
<p>phrases d&#8217;un document en vecteurs. Ces vecteurs peuvent &#234;tre trait&#233;s comme un r&#233;seaux de
neurones type Hopfield. Si l&#8217;on d&#233;finit un vocabulaire de tailleN , o&#249;N est le nombre de termes
uniques d&#8217;un document, on peut repr&#233;senter une phrase comme une cha&#238;ne de N neurones ac-
tifs, i = 1, &#183; &#183; &#183; , N (le mot i &#233;tant pr&#233;sent) ou inactifs (le mot i &#233;tant absent). Un document de
P phrases, est compos&#233; de P cha&#238;nes dans l&#8217;espace vectoriel &#926; de dimension N . Ces vecteurs
sont plus ou moins corr&#233;l&#233;s, selon les mots qu&#8217;ils partagent. Si les th&#233;matiques sont proches, il
est raisonable de supposer que le degr&#233; de corr&#233;lation sera tr&#232;s &#233;lev&#233;. Cela pose des probl&#232;mes
si on essaie de stocker et de r&#233;cup&#233;rer ces repr&#233;sentations dans un r&#233;seau type Hopfield. Cepen-
dant notre int&#233;r&#234;t porte non pas sur la r&#233;cup&#233;ration, mais sur les interactions entre les mots et
entre les phrases. Cette interaction nous allons la d&#233;finir comme l&#8217;&#233;nergie textuelle d&#8217;un docu-
ment. Elle peut servir, entre autres, &#224; pond&#233;rer les phrases ou &#224; d&#233;tecter des changements entre
des cha&#238;nes de neurones. Nous d&#233;v&#233;loppons une m&#233;taphore qui permet d&#8217;utiliser le concept
d&#8217;en&#233;rgie textuelle pour son application dans le r&#233;sume g&#233;n&#233;rique ou la segmentation th&#233;ma-
tique. Nous pr&#233;sentons en Section 2 une br&#232;ve introduction au mod&#232;le de Hopfield. En Section
3, nous faisons une extension de cette approche dans le traitement automatique de la langue
naturelle. Nous utilisons ainsi des notions &#233;l&#233;mentaires de la th&#233;orie des graphes pour donner
une interpr&#233;tation de l&#8217;&#233;nergie textuelle comme une nouvelle mesure de similarit&#233;. En Section
4 nous appliquons nos algorithmes &#224; la g&#233;n&#233;ration de r&#233;sum&#233;s automatiques et &#224; la d&#233;tection de
fronti&#232;res th&#233;matiques, avant de conclure et presenter quelques perspectives.
</p>
<p>2 L&#8217;approche &#233;nerg&#233;tique de Hopfield
La contribution la plus importante de Hopfield &#224; la th&#233;orie de r&#233;seaux de neurones a &#233;t&#233; l&#8217;intro-
duction de la notion d&#8217;&#233;nergie issue de l&#8217;analogie avec les syst&#232;mes magn&#233;tiques. Un syst&#232;me
magn&#233;tique est constitu&#233; d&#8217;un ensemble de N petits aimants appel&#233;s spins. Ces spins peuvent
s&#8217;orienter selon plusieurs directions. Le cas le plus simple est repr&#233;sent&#233; par le mod&#232;le d&#8217;Ising
qui consid&#233;re seulement deux directions possibles : vers le haut (&#8593;, +1 ou 1) ou vers le bas
(&#8595;, -1 ou 0). Le mod&#232;le d&#8217;Ising a &#233;t&#233; utilis&#233; dans une grande vari&#233;t&#233; de syst&#232;mes qui peuvent
&#234;tre d&#233;crits par des variables binaires (Ma, 1985). Un syst&#232;me de N unit&#233;s binaires poss&#232;de
&#957; = 1, ..., 2N configurations (patrons) possibles. Dans le mod&#232;le de Hopfield les spins corres-
pondent aux neurones qui interagissent selon la r&#232;gle d&#8217;apprentissage d&#8217;Hebb2 :
</p>
<p>J i,j =
P&#8721;
</p>
<p>&#181;=1
</p>
<p>si&#181;s
j
&#181; (1)
</p>
<p>si et sj sont les &#233;tats des neurones i et j. Les autocorrelations ne sont pas calcul&#233;es (i #= j). La
sommation porte sur les P patrons a stocker. Cette r&#232;gle d&#8217;interaction est locale, car J i,j d&#233;pend
seulement des &#233;tats des unit&#233;s connect&#233;es. Ce mod&#232;le est connu aussi comme m&#233;moire associa-
tive. Elle poss&#232;de la capacit&#233; de stocker et de r&#233;cup&#233;rer un certain nombre de configurations du
syst&#232;me, car la r&#232;gle de Hebb transforme ces configurations en attracteurs (minimaux locaux)
de la fonction d&#8217;&#233;nergie (Hopfield, 1982) :
</p>
<p>E = &#8722;
1
</p>
<p>2
</p>
<p>N&#8721;
i=1
</p>
<p>N&#8721;
j=1
</p>
<p>si J i,j sj (2)
</p>
<p>2Hebb (Hertz et al., 1991) a sugg&#233;r&#233; que les connexions synaptiques changent proportionnellement &#224; la corr&#233;-
lation entre les &#233;tats des neurones.
</p>
<p>26</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>&#201;nergie textuelle de m&#233;moires associatives
</p>
<p>L&#8217;&#233;nergie est fonction de la configuration du syst&#232;me, c&#8217;est-&#224;-dire, de l&#8217;&#233;tat (d&#8217;activation ou non
activation) de toutes ces unit&#233;s. Si on pr&#233;sente un patron &#957;, chaque spin subira un champ local
hi =
</p>
<p>&#8721;N
j=1 J
</p>
<p>i,jsj induit par les autres N spins (voir figure 1). Les spins s&#8217;aligneront selon hi
</p>
<p>FIG. 1 &#8211; Champ hi subi par le spin sj ,&#8712; la cha&#238;ne (patron) &#957; produit par les autres N spins &#8712; &#181;.
</p>
<p>pour restituer le patron stock&#233; qui est le plus proche au patron pr&#233;sent&#233; &#957;. Nous n&#8217;allons pas
d&#233;tailler la m&#233;thode de recup&#233;ration de patrons3, car notre int&#233;r&#234;t va porter sur la distribution
et les propri&#233;t&#233;s de l&#8217;&#233;nergie du syst&#232;me (2). Cette fonction monotone et d&#233;croissante avait &#233;t&#233;
utilis&#233;e uniquement pour montrer que l&#8217;apprentissage est born&#233;. D&#8217;un autre c&#244;t&#233;, le mod&#232;le
vectoriel (Salton &amp; McGill, 1983) transforme un document dans un espace ad&#233;quat o&#249; une
matrice S contient l&#8217;information du texte sous forme de sacs de mots. On peut consid&#233;rer S
comme l&#8217;ensemble des configurations d&#8217;un syst&#232;me dont on peut calculer l&#8217;&#233;nergie.
</p>
<p>3 Applications au TALN
Les documents sont pr&#233;-trait&#233;s avec des algorithmes classiques de filtrage de mots fonction-
nels4, de normalisation et de lemmatisation (Porter, 1980; Manning &amp; Schutze, 2000) afin de
r&#233;duire la dimensionnalit&#233;. Une repr&#233;sentation en sac de mots produit une matrice S[P&#215;N ] de
fr&#233;quences/absences compos&#233;e de &#181; = 1, &#183; &#183; &#183; , P phrases (lignes) ; &quot;&#963;&#181; = {s1&#181;, &#183; &#183; &#183; , si&#181;, &#183; &#183; &#183; , sN&#181; }
et un vocabulaire de i = 1, &#183; &#183; &#183; , N termes (colonnes).
</p>
<p>S =
</p>
<p>&#63723;&#63724;&#63724;&#63724;&#63725;
s11 s
</p>
<p>2
1 &#183; &#183; &#183; s
</p>
<p>N
1
</p>
<p>s12 s
2
2 &#183; &#183; &#183; s
</p>
<p>N
2... ... . . . ...
</p>
<p>s1P s
2
P &#183; &#183; &#183; s
</p>
<p>N
P
</p>
<p>&#63734;&#63735;&#63735;&#63735;&#63736; ; si&#181; =
{
</p>
<p>TF i si le terme i existe
0 autrement (3)
</p>
<p>La pr&#233;sence du mot i repr&#233;sente un spin si &#8593; avec une magnitude donn&#233;e par sa fr&#233;quence
TF i (son absence par &#8595; respectivement), et une phrase &quot;&#963;&#181; est donc une cha&#238;ne de N spins.
Nous allons nous diff&#233;rencier de (Hopfield, 1982) sur deux points : S est une matrice enti&#232;re
(ses &#233;l&#233;ments prennent des valeurs fr&#233;quentielles absolues) et nous utilisons les &#233;l&#233;ments J i,i
car cette auto-corr&#233;lation permet d&#8217;&#233;tablir l&#8217;interaction du mot i parmi les P phrases, ce qui est
important en TALN. Pour calculer les interations entre lesN termes du vocabulaire, on applique
la r&#232;gle de correlation de Hebb, qui en forme matricielle est &#233;gale &#224; :
</p>
<p>J = ST &#215; S (4)
3Cependant le lecteur int&#233;ress&#233; peut consulter, par exemple (Hopfield, 1982; Kosko, 1988; Hertz et al., 1991).
4Nous avons effectu&#233; le filtrage de chiffres et l&#8217;utilisation d&#8217;anti-dictionnaires.
</p>
<p>27</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Silvia FERN&#193;NDEZ, Eric SANJUAN, Juan Manuel TORRES-MORENO
</p>
<p>Chaque &#233;l&#233;ment J i,j &#8712; J[N&#215;N ] est &#233;quivalent au calcul de (1). L&#8217;&#233;nergie textuelle d&#8217;interaction
(2) peut alors s&#8217;exprimer comme :
</p>
<p>E = &#8722;
1
</p>
<p>2
S &#215; J &#215; ST (5)
</p>
<p>Un &#233;l&#233;ment E&#181;,&#957; &#8712; E[P&#215;P ] repr&#233;sente l&#8217;&#233;nergie d&#8217;interaction entre les patrons &#181; et &#957; (figure 1).
</p>
<p>3.1 L&#8217;&#233;nergie textuelle : une nouvelle mesure de similarit&#233;
Nous allons expliquer th&#233;oriquement la nature des liens entre phrases que la mesure d&#8217;&#233;ner-
gie textuelle induit. Pour cela nous utilisons quelques notions &#233;l&#233;mentaires de la th&#233;orie des
graphes. L&#8217;interpr&#233;tation que nous allons faire repose sur le fait que la matrice (5) peut s&#8217;&#233;crire :
</p>
<p>E = &#8722;
1
</p>
<p>2
S &#215; (ST &#215; S)&#215; ST = &#8722;
</p>
<p>1
</p>
<p>2
(S &#215; ST )2 (6)
</p>
<p>Consid&#233;rons les phrases comme des ensembles &#963; de mots. Ces ensembles constituent les som-
mets du graphe. On trace une ar&#234;te entre deux de ces sommets &#963;&#181;, &#963;&#957; chaque fois qu&#8217;ils par-
tagent au moins un mot en commun &#963;&#181; &#8745;&#963;&#957; %= &#8709;. On obtient ainsi le graphe I(S) d&#8217;intersection
des phrases (voir un exemple &#224; quatre phrases en figure 2). On value ces paires {&#963;1, &#963;2} que
l&#8217;on appelle ar&#234;tes par le nombre exact |&#963;1&#8745;&#963;2| de mots que partagent les deux sommets reli&#233;s.
Enfin, on ajoute &#224; chaque sommet &#963; une ar&#234;te de r&#233;flexivit&#233; {&#963;} valu&#233;e par le cardinal |&#963;| de &#963;.
Ce graphe d&#8217;intersection valu&#233; est isomorphe au graphe G(S &#215; ST ) d&#8217;adjacence de la matrice
carr&#233;e S&#215;ST . En effet,G(S&#215;ST ) contient P sommets. Il existe une ar&#234;te entre deux sommets
&#181;, &#957; si et seulement si [S &#215; ST ]&#181;,&#957; &gt; 0. Si c&#8217;est le cas, cette ar&#234;te est valu&#233;e par [S &#215; ST ]&#181;,&#957; ,
valeur qui correspond au nombre de mots en commun entre les phrases &#181; et &#957;. Chaque sommet
&#181; est pond&#233;r&#233; par [S&#215;ST ]&#181;,&#181; ce qui correspond &#224; l&#8217;ajout d&#8217;une ar&#234;te de reflexivit&#233;. Il en r&#233;sulte
</p>
<p>FIG. 2 &#8211; Graphes d&#8217;adjacence issus de la matrice d&#8217;&#233;nergie.
</p>
<p>que la matrice d&#8217;&#233;nergie textuelle E est la matrice d&#8217;adjacence du graphe G(S &#215; ST )2 dont :
&#8211; les sommets sont les m&#234;mes que ceux du graphe d&#8217;intersection I(S) ;
&#8211; il existe une ar&#234;te entre deux sommets chaque fois qu&#8217;il existe un chemin de longeur au plus
2 dans le graphe d&#8217;intersection ;
</p>
<p>&#8211; la valeur d&#8217;une ar&#234;te : a) boucle sur un sommet &#963; est la somme des carr&#233;s des valeurs des
ar&#234;tes adjacentes au sommet et b) entre deux sommets distincts &#963;&#181; et &#963;&#957; adjacents est la
somme des produits des valeurs des ar&#234;tes sur tout chemin de longueur 2 entre les deux
sommets. Ces chemins pouvant comprendre des boucles.
</p>
<p>28</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>&#201;nergie textuelle de m&#233;moires associatives
</p>
<p>De cette repr&#233;sentation on en d&#233;duit que la matrice d&#8217;&#233;nergie textuelle relie &#224; la fois des phrases
ayant des mots communs puisque elle englobe le graphe d&#8217;intersection, ainsi que des phrases qui
partagent un m&#234;me voisinage sans pour autant partager n&#233;cessairement un m&#234;me vocabulaire.
C&#8217;est &#224; dire que deux phrases &#963;1, &#963;3 ne partageant aucun mot en commun mais pour lesquelles
il existe au moins une troisi&#232;me phrase &#963;2 telle que &#963;1 &#8745; &#963;2 &quot;= &#8709; et &#963;3 &#8745; &#963;2 &quot;= &#8709; seront tout
de m&#234;me reli&#233;es. La force de ce lien d&#233;pend premi&#232;rement du nombre de phrases &#963;2 dans leur
voisinage commun, et donc du vocabulaire apparaissant dans un contexte commun.
</p>
<p>4 Exp&#233;riences et r&#233;sultats
L&#8217;&#233;nergie textuelle peut &#234;tre utilis&#233;e commemesure de similarit&#233; dans les applications du TALN.
De fa&#231;on intuitive, cette similarit&#233; peut servir &#224; scorer les phrases d&#8217;un document et s&#233;parer ainsi
celles qui sont pertinentes de celles qui ne le sont pas. Ceci conduit immediatement &#224; une stra-
t&#233;gie de r&#233;sum&#233; automatique par extraction de phrases. Une autre approche, moins &#233;vidente,
consiste &#224; utiliser l&#8217;information de cette &#233;nergie (vue comme un spectre ou signal num&#233;rique de
la phrase) et de la comparer au spectre de toutes les autres. Un test statistique peut alors indiquer
si ce signal est semblable &#224; celui d&#8217;autres phrases regroup&#233;s en segments ou pas. Ceci peut &#234;tre
vu comme une d&#233;tection de fronti&#232;res th&#233;matiques dans un document.
</p>
<p>4.1 R&#233;sum&#233; automatique
</p>
<p>Sous l&#8217;hypoth&#232;se que l&#8217;&#233;nergie d&#8217;une phrase &#181; refl&#232;te son poids dans le document, nous avons
appliqu&#233; (6) au r&#233;sum&#233; par extraction de phrases (Mani &amp; Maybury, 1999; Radev et al., 2002).
Cette m&#233;thode est orient&#233;e, pour le moment, &#224; la g&#233;n&#233;ration de r&#233;sum&#233;s g&#233;n&#233;riques monodo-
cument. Cependant, nous pensons qu&#8217;une modification de l&#8217;approche (voir Section 5) pourrait
nous permettre d&#8217;obtenir des r&#233;sum&#233;s guid&#233;s par une requ&#234;te ou un sujet d&#233;fini par l&#8217;utilisateur
(ce qui correspond au protocole des conf&#233;r&#233;nces DUC5). L&#8217;algorithme de r&#233;sum&#233; comprend
trois modules. Le premier r&#233;alise la transformation vectorielle du texte avec des processus de
filtrage, de lemmatisation/stemming et de normalisation. Le second module applique le mod&#232;le
de spins et r&#233;alise le calcul de la matrice d&#8217;&#233;nergie textuelle (6). Nous obtenons la pond&#233;ration
de la phrase &#957; en utilisant ses valeurs absolues d&#8217;&#233;nergie, c&#8217;est-&#224;-dire, en triant selon&#8721;&#181; |E&#181;,&#957;|.
Ainsi, les phrases pertinentes seront selectionn&#233;es comme ayant la plus grande &#233;nergie absolue.
Finalement, le troisi&#232;me module g&#233;n&#232;re les r&#233;sum&#233;s par affichage et concat&#233;nation des phrases
pertinentes. Les deux premiers modules reposent sur le syst&#232;me Cortex6. Pour les tests en fran-
&#231;ais7 nous avons choisi les textes : &#171; 3-m&#233;langes &#187; compos&#233; de trois th&#233;matiques, &#171; puces &#187; de
deux th&#233;matiques et &#171; J&#8217;accuse &#187; (lettre d&#8217;&#201;mile Zola). Deux textes de la wikipedia en anglais
ont &#233;t&#233; analys&#233;s, &#171; Lewinksky &#187; et &#171; Qu&#233;bec &#187;8. Nous avons evalu&#233; les r&#233;sum&#233;s produits par
notre syst&#232;me avec ROUGE (Lin, 2004), qui mesure la similarit&#233;, suivant plusieurs strat&#233;gies,
entre un r&#233;sum&#233; candidat (produit automatiquement) et des r&#233;sum&#233;s de r&#233;f&#233;rence (cr&#233;&#233;s par des
humains). Nous comparons dans les tables 1 &#224; 5 les performances de la m&#233;thode d&#8217;&#233;nergie, de
</p>
<p>5Document Understandig Conferences http ://www-nlpir.nist.gov/projects/duc/index.html
6Le syst&#232;me Cortex (Torres-Moreno et al., 2002) effectue une extraction non supervis&#233;e de phrases pertinentes
</p>
<p>en utilisant plusieurs m&#233;triques pilot&#233;es par un algorithme de d&#233;cision.
7Recup&#233;rables &#224; l&#8217;adresse http ://www.lia.univ-avignon.fr.
8http ://en.wikipedia.org/wiki/Monica_Lewinsky, http ://en.wikipedia.org/wiki/Quebec_sovereignty_movement
</p>
<p>29</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Silvia FERN&#193;NDEZ, Eric SANJUAN, Juan Manuel TORRES-MORENO
</p>
<p>Cortex et d&#8217;une baseline o&#249; les phrases ont &#233;t&#233; choisies au hasard. Nous constatons que notre
m&#233;thode est comparable au syst&#232;me Cortex en termes de pr&#233;cision, de rappel et de F -score.
</p>
<p>&#201;nergie Cortex Baseline
Rouge-2 Rouge-SU4 Rouge-2 Rouge-SU4 Rouge-2 Rouge-SU4
</p>
<p>Rappel 0,49577 0,50635 0,49676 0,50643 0,29125 0,3117
Pr&#233;cision 0,43229 0,44114 0.42288 0,43068 0,32801 0,35191
F -score 0,46186 0,47150 0.45685 0,46549 0,30744 0,32936
</p>
<p>TAB. 1 &#8211; Texte &#171; 3-m&#233;langes &#187; (27 phrases, 826 mots ; r&#233;sum&#233; au 25% ; 8 r&#233;sum&#233;s r&#233;f&#233;rence).
</p>
<p>&#201;nergie Cortex Baseline
Rouge-2 Rouge-SU4 Rouge-2 Rouge-SU4 Rouge-2 Rouge-SU4
</p>
<p>Rappel 0,52040 0,53353 0,53595 0,55878 0,25938 0,27721
Pr&#233;cision 0,52469 0,53796 0,53120 0,55380 0,37589 0,40474
F -score 0,52254 0,53574 0,53356 0,55628 0,30530 0,32723
</p>
<p>TAB. 2 &#8211; Texte &#171; puces &#187; (29 phrases, 653 mots ; r&#233;sum&#233; au 25% ; 8 r&#233;sum&#233;s r&#233;f&#233;rence).
</p>
<p>&#201;nergie Cortex Baseline
Rouge-2 Rouge-SU4 Rouge-2 Rouge-SU4 Rouge-2 Rouge-SU4
</p>
<p>Rappel 0,61457 0,64192 0,63160 0,65987 0,18690 0,20185
Pr&#233;cision 0,51425 0,53700 0,52725 0,55071 0,30920 0,37195
F -score 0,55995 0,58479 0,57473 0,60037 0,21766 0,26152
</p>
<p>TAB. 3 &#8211; Texte &#171; J&#8217;accuse &#187; (206 phrases, 4936 mots ; r&#233;sum&#233; au 12% ; 6 r&#233;sum&#233;s r&#233;f&#233;rence).
</p>
<p>4.2 D&#233;tection de fronti&#232;res th&#233;matiques
Plusieurs strat&#233;gies ont &#233;t&#233; d&#233;velopp&#233;es pour segmenter th&#233;matiquement un texte. Elles peuvent
&#234;tre supervis&#233;es ou non. On trouve PLSA (Brants et al., 2002) qui estime les probabilit&#233;s d&#8217;ap-
partenance des termes &#224; des classes s&#233;mantiques, des m&#233;thodes s&#8217;appuyant sur des mod&#232;les de
Markov (Amini et al., 2000), sur une classification des termes (Caillet et al., 2004; Chuang &amp;
Chien, 2004) ou sur des cha&#238;nes lexicales (Sitbon &amp; Bellot, 2005). De fa&#231;on originale, nous
avons utilis&#233; la matrice d&#8217;&#233;nergie E (6). Ce choix permet de s&#8217;adapter &#224; de nouvelles th&#233;ma-
tiques et de rester ind&#233;pendant vis &#224; vis de la langue des documents. Pour pouvoir comparer
les &#233;nergies entre elles nous introduisons le coefficient de concordance W de Kendall (Siegel
&amp; Castellan, 1988) et le calcul de sa p&#8722;valeur. Ils permettent de d&#233;finir un test statistique de
concordance entre k juges qui classent un ensemble de P objets. Nous avons utilis&#233; ce test pour
trouver les fronti&#232;res th&#233;matiques entre segments. Nous montrons en figure (3) l&#8217;&#233;nergie d&#8217;in-
teraction entre quelques phrases d&#8217;un texte compos&#233; de deux th&#233;matiques. &#201;tant donn&#233; que (6)
est capable de d&#233;tecter et de pond&#233;rer le voisinage d&#8217;une phrase, on peut constater une similarit&#233;
entre les courbes de l&#8217;une (gras) et de l&#8217;autre th&#233;matique (pointill&#233;). Voici le protocole de test
que nous avons adopt&#233;.
</p>
<p>1. Selon la nature du texte (homog&#232;ne ou h&#233;t&#233;roclite) on &#233;met a priori l&#8217;une des deux hypo-
th&#232;ses initiales H0 qui suivent : i) la phrase &#181;+ 1 appartient &#224; la m&#234;me th&#233;matique que la
phrase pr&#233;c&#233;dente &#181; ou au contraire ii) la phrase &#181;+ 1 marque une rupture avec &#181;.
</p>
<p>30</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>&#201;nergie textuelle de m&#233;moires associatives
</p>
<p>&#201;nergie Cortex Baseline
Rouge-2 Rouge-SU4 Rouge-2 Rouge-SU4 Rouge-2 Rouge-SU4
</p>
<p>Rappel 0,56107 0,57859 0,61832 0,62705 0,24227 0,25584
Pr&#233;cision 0,39516 0,40658 0,42587 0,43085 0,32490 0,34393
F -score 0,46372 0,47757 0,50436 0,51076 0,27671 0,29248
</p>
<p>TAB. 4 &#8211; Texte &#171; Lewinsky &#187; (30 phrases, 816 mots ; r&#233;sum&#233; au 20% ; 7 r&#233;sum&#233;s r&#233;f&#233;rence).
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>24                                                    
                                                    
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>23                                                    
                                                    
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>22                                                    
                                                    
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>19                                                    
                                                    
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>18                                                    
                                                    
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>16
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>15                                                    
                                                    
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>14                                                    
                                                    
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>13                                                    
                                                    
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>12                                                    
                                                    
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>10                                                    
                                                    
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>9
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>7                                                    
                                                    
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>5                                                    
                                                    
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>4                                                    
                                                    
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>3                                                    
                                                    
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>2                                                    
                                                    
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>1
</p>
<p>FIG. 3 &#8211; &#201;nergie textuelle de &#171; 2-m&#233;langes &#187;. En trait continu l&#8217;&#233;nergie des phrases de la 1&#232;re
th&#233;matique, en pointill&#233; celle de la 2&#232;me. Le changement d&#8217;allure des courbes entre les phrases
14-15 correspond &#224; un changement th&#233;matique. L&#8217;axe horizontal indique le num&#233;ro de phrase
dans l&#8217;ordre du document. L&#8217;axe vertical, l&#8217;&#233;nergie textuelle de la phrase affich&#233;e vs. les autres.
</p>
<p>2. On estime alors la probabilit&#233; p que l&#8217;hypoth&#232;se H0 choisie soit v&#233;rifi&#233;e en calculant le
coefficient de concordanceW de Kendall entre les deux classements par proximit&#233; induits
par les phrases &#181; et &#181; + 1 sur les autres phrases. Le coefficient W de Kendall vaut 1 en
cas d&#8217;accord total entre les classements et 0 dans la cas de d&#233;saccord total. La probabilit&#233;
p est alors estim&#233;e en utilisant l&#8217;approximation de la loi duW par une loi du &#967;2.
</p>
<p>3. Finalement, si p &lt; 0, 1 on rejette H0 et l&#8217;on adopte l&#8217;hypoth&#232;se alternative (son compl&#233;-
mentaire) avec un risque p de se tromper. Il est important de pr&#233;ciser que la valeur seuil
0, 1 est fix&#233;e a priori conform&#233;m&#233;nt &#224; la m&#233;thodologie statistique inf&#233;rentielle.
</p>
<p>Il s&#8217;agit donc de tests non-param&#233;triques qui ne requi&#232;rent aucune supposition sur une &#233;ven-
tuelle distribution gaussienne des donn&#233;es. Pour chaque document, nous avons &#233;limin&#233; les
phrases dont l&#8217;&#233;nergie est inf&#233;rieure &#224; un seuil. Ces phrases sont celles qui contiennent un
nombre de mots &lt; 2 (patrons &#224; spins 0) ou trop longues (si l&#8217;on a suffisamment de phrases
par segment), et qui induisent un fort bruit dans E. Les figures (4) et (5) montrent la d&#233;tection
</p>
<p>&#201;nergie Cortex Baseline
Rouge-2 Rouge-SU4 Rouge-2 Rouge-SU4 Rouge-2 Rouge-SU4
</p>
<p>Rappel 0,50945 0,53773 0,56364 0,58716 0,27344 0,32127
Pr&#233;cision 0,46276 0,48824 0,50803 0,52899 0,33254 0,39092
F -score 0,48498 0,51179 0,53439 0,55656 0,29991 0,35244
</p>
<p>TAB. 5 &#8211; Texte &#171; Qu&#233;bec &#187; (44 phrases, 1190 mots ; r&#233;sum&#233; au 25% ; 8 r&#233;sum&#233;s r&#233;f&#233;rence).
31</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Silvia FERN&#193;NDEZ, Eric SANJUAN, Juan Manuel TORRES-MORENO
</p>
<p>des fronti&#232;res pour les textes &#224; 2 et 3 th&#233;matiques. Les v&#233;ritables fronti&#232;res sont indiqu&#233;es en
pointill&#233;. Ce protocole de test, en adoptant l&#8217;hypoth&#232;se ii) comme H0, a d&#233;tect&#233; une fronti&#232;re
entre les phrases 14-15 pour le texte &#171; 2-m&#233;langes &#187;. Pour le texte &#171; 3-m&#233;langes &#187;, le test a
trouv&#233; deux fronti&#232;res entre les segments 8-9 et 16-18. Dans les deux cas, cela correspond ef-
fectivement aux fronti&#232;res th&#233;matiques. Une troisi&#232;me (fausse) fronti&#232;re a &#233;t&#233; signal&#233;e entre les
phrases 23-24 du texte &#171; 2-m&#233;langes &#187;. Cela m&#233;rite d&#8217;&#234;tre comment&#233; : si on regarde sur la figure
(3) l&#8217;&#233;nergie de la phrase 23, elle est bien diff&#233;rente de celle des phrases 22 ou 24. La phrase
23 pr&#233;sente une courbe chevauchant les deux th&#233;matiques. C&#8217;est pourquoi le test ne peut pas
l&#8217;identifier comme appartenant &#224; la m&#234;me classe. Le m&#234;me raisonnemment tient pour toutes les
fausses fronti&#232;res. Pour le texte &#171; physique-climat-chanel &#187; le test du W de Kendall a d&#233;tect&#233;
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>01
-0
</p>
<p>2
</p>
<p>02
-0
</p>
<p>3
</p>
<p>03
-0
</p>
<p>4
</p>
<p>04
-0
</p>
<p>5
</p>
<p>05
-0
</p>
<p>6
</p>
<p>06
-0
</p>
<p>7
</p>
<p>07
-0
</p>
<p>9
</p>
<p>09
-1
</p>
<p>0
</p>
<p>10
-1
</p>
<p>1
</p>
<p>11
-1
</p>
<p>2
</p>
<p>12
-1
</p>
<p>3
</p>
<p>13
-1
</p>
<p>4
</p>
<p>14
-1
</p>
<p>5
</p>
<p>15
-1
</p>
<p>6
</p>
<p>16
-1
</p>
<p>8
</p>
<p>18
-1
</p>
<p>9
</p>
<p>19
-2
</p>
<p>2
</p>
<p>22
-2
</p>
<p>3
</p>
<p>23
-2
</p>
<p>4
</p>
<p>0,00
</p>
<p>0,05
</p>
<p>0,10
</p>
<p>0,15
</p>
<p>0,20
</p>
<p>0,25
</p>
<p>0,30
</p>
<p>0,35
</p>
<p>0,40
</p>
<p>0,45
</p>
<p>0,50
</p>
<p>p (
Te
</p>
<p>st 
de
</p>
<p> K
en
</p>
<p>da
ll)
</p>
<p>Phrase (i, j)
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>00
-1
</p>
<p>01
-2
</p>
<p>02
-3
</p>
<p>03
-4
</p>
<p>04
-5
</p>
<p>05
-6
</p>
<p>06
-7
</p>
<p>07
-8
</p>
<p>08
-9
</p>
<p>09
-1
</p>
<p>0
10
</p>
<p>-1
1
</p>
<p>11
-1
</p>
<p>3
</p>
<p>13
-1
</p>
<p>4
14
</p>
<p>-1
5
</p>
<p>15
-1
</p>
<p>6
16
</p>
<p>-1
8
</p>
<p>18
-1
</p>
<p>9
19
</p>
<p>-2
0
</p>
<p>20
-2
</p>
<p>3
</p>
<p>23
-2
</p>
<p>5
25
</p>
<p>-2
6
</p>
<p>0,00
</p>
<p>0,05
</p>
<p>0,10
</p>
<p>0,15
</p>
<p>0,20
</p>
<p>0,25
</p>
<p>0,30
</p>
<p>0,35
</p>
<p>0,40
</p>
<p>0,45
</p>
<p>0,50
</p>
<p>p (
Te
</p>
<p>st 
de
</p>
<p> K
en
</p>
<p>da
ll)
</p>
<p>Phrase (i, j)
</p>
<p>FIG. 4 &#8211; D&#233;tection des fronti&#232;res pour le texte &#171; 2-m&#233;langes &#187; (2 th&#233;matiques, &#224; gauche) et &#171; 3-
m&#233;langes &#187; (3 th&#233;matiques, &#224; droite).
</p>
<p>trois fronti&#232;res entre les phrases 5-6 et 12-15, qui correspondent aux fronti&#232;res effectives. Pour
le texte en anglais &#224; deux th&#233;matiques le test a trouv&#233; une fronti&#232;re entre les segments 44-45 qui
correspond &#224; la vraie fronti&#232;re. Nous avons calcul&#233; le F -score de fa&#231;on similaire &#224; DEFT 2005
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>00
-0
</p>
<p>2
</p>
<p>02
-0
</p>
<p>5
</p>
<p>05
-0
</p>
<p>6
</p>
<p>06
-0
</p>
<p>7
</p>
<p>07
-0
</p>
<p>8
</p>
<p>08
-1
</p>
<p>0
</p>
<p>10
-1
</p>
<p>1
</p>
<p>11
-1
</p>
<p>2
</p>
<p>12
-1
</p>
<p>5
</p>
<p>15
-1
</p>
<p>6
</p>
<p>16
-1
</p>
<p>7
</p>
<p>17
-1
</p>
<p>8
</p>
<p>18
-1
</p>
<p>9
</p>
<p>19
-2
</p>
<p>2
</p>
<p>0,00
</p>
<p>0,05
</p>
<p>0,10
</p>
<p>0,15
</p>
<p>0,20
</p>
<p>0,25
</p>
<p>0,30
</p>
<p>0,35
</p>
<p>0,40
</p>
<p>0,45
</p>
<p>0,50
</p>
<p>p (
Te
</p>
<p>st 
de
</p>
<p> K
en
</p>
<p>da
ll)
</p>
<p>Phrase (i, j)
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>01
-2
</p>
<p>03
-4
</p>
<p>05
-6
</p>
<p>08
-9
</p>
<p>11
-1
</p>
<p>2
13
</p>
<p>-1
4
</p>
<p>15
-1
</p>
<p>6
17
</p>
<p>-1
8
</p>
<p>20
-2
</p>
<p>1
23
</p>
<p>-2
4
</p>
<p>25
-2
</p>
<p>6
27
</p>
<p>-2
9
</p>
<p>30
-3
</p>
<p>1
32
</p>
<p>-3
3
</p>
<p>34
-3
</p>
<p>5
36
</p>
<p>-3
7
</p>
<p>38
-3
</p>
<p>9
40
</p>
<p>-4
1
</p>
<p>42
-4
</p>
<p>4
45
</p>
<p>-4
6
</p>
<p>47
-4
</p>
<p>8
51
</p>
<p>-5
2
</p>
<p>53
-5
</p>
<p>4
55
</p>
<p>-5
6
</p>
<p>57
-5
</p>
<p>8
59
</p>
<p>-6
0
</p>
<p>61
-6
</p>
<p>2
63
</p>
<p>-6
4
</p>
<p>65
-6
</p>
<p>6
67
</p>
<p>-6
8
</p>
<p>69
-7
</p>
<p>0
71
</p>
<p>-7
2
</p>
<p>73
-7
</p>
<p>4
</p>
<p>0,00
</p>
<p>0,05
</p>
<p>0,10
</p>
<p>0,15
</p>
<p>0,20
</p>
<p>0,25
</p>
<p>0,30
</p>
<p>0,35
</p>
<p>0,40
</p>
<p>0,45
</p>
<p>0,50
</p>
<p>p (
Te
</p>
<p>st 
de
</p>
<p> K
en
</p>
<p>da
ll)
</p>
<p>Phrase (i, j)
</p>
<p>FIG. 5 &#8211; D&#233;tection des fronti&#232;res pour le texte en fran&#231;ais &#224; 3 th&#233;matiques &#171; physique-climat-chanel &#187; &#224;
gauche et en anglais &#171; qu&#233;bec-lewinsky &#187; &#224; droite.
</p>
<p>(Alphonse et al., 2005) 9. Ainsi pour &#171; 2-m&#233;langes &#187; F -score = 0,66 ; &#171; 3-m&#233;langes &#187; F -score =
9En consid&#233;rant &#946; = 1,F -score(&#946;) = 2&#215;Nb_fronti&#232;res_correctes_extraitesNb_total_fronti&#232;res_extraites + Nb_total_v&#233;ritables
</p>
<p>32</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>&#201;nergie textuelle de m&#233;moires associatives
</p>
<p>0,66 ; &#171; physique-climat-chanel &#187; F -score = 0,80 et &#171; qu&#233;bec-lewinsky &#187; F -score = 1. Dans une
autre exp&#233;rience, nous avons compar&#233; notre syst&#232;me &#224; deux autres : LCseg (Galley et al., ) et
LIA_seg (Sitbon &amp; Bellot, 2005), qui utilisent tous les deux des cha&#238;nes lexicales. Le corpus de
ref&#233;rence a &#233;t&#233; construit &#224; partir d&#8217;articles du journal Le Monde. Il est compos&#233; d&#8217;ensembles de
100 documents o&#249; chacun correspond &#224; la taille moyenne des segments pr&#233;-d&#233;finie. Un docu-
ment est compos&#233; de 10 segments extraits d&#8217;articles th&#233;matiquement diff&#233;rentes tir&#233;s au hasard.
Compte tenu des particularit&#233;s de ce corpus nous avons adopt&#233; i comme hypoth&#232;se initiale H0.
Les scores sont calcul&#233;s avec Windiff (Pevzner &amp; Hearst, 2002), utilis&#233;e dans la segmentation
th&#233;matique. Cette fonction mesure la diff&#233;rence entre les fronti&#232;res v&#233;ritables et celles trouv&#233;es
automatiquement dans une fen&#234;tre glissante : plus la valeur est petite, plus le syst&#232;me est perfor-
mant. LIA_seg depend d&#8217;un param&#232;tre qui donne lieu &#224; diff&#233;rentes performances (d&#8217;o&#249; la plage
de valeurs affich&#233;e). Notre m&#233;thode obtient des performances comparables aux syst&#232;mes dans
l&#8217;&#233;tat de l&#8217;art mais en utilisant bien moins de param&#232;tres, en particulier nous ne faisons aucune
supposition sur le nombre de th&#233;matiques &#224; d&#233;tecter.
</p>
<p>Taille du segment (en phrases) LCseg LIA_seg &#201;nergie
9-11 0,3272 (0,3187-0,4635) 0,4419
3-11 0,3837 (0,3685-0,5105) 0,4403
3-5 0,4344 (0,4204-0,5856) 0,4167
</p>
<p>TAB. 6 &#8211; Mesure Windiff pour LCseg, LIA_seg et &#201;nergie (segments de diff&#233;rentes tailles).
</p>
<p>5 Conclusion et perspectives
Nous avons introduit le concept d&#8217;&#233;nergie textuelle bas&#233; sur des approches des r&#233;seaux de neu-
rones. Cela nous a permis de d&#233;velopper un nouvel algorithme de r&#233;sum&#233; automatique. Des tests
effectu&#233;s ont montr&#233; que notre algorithme est adapt&#233; &#224; la recherche de segments pertinents. On
obtient des r&#233;sum&#233;s &#233;quilibr&#233;s o&#249; la plupart des th&#232;mes sont abord&#233;s dans le condens&#233; final.
Les avantages suppl&#233;mentaires consistent &#224; ce que les r&#233;sum&#233;s sont obtenus de fa&#231;on ind&#233;pen-
dante de la taille du texte, des sujets abord&#233;s, d&#8217;une certaine quantit&#233; de bruit et de la langue
(sauf pour la partie pr&#233;-traitement). Nous pensons que l&#8217;algorithme d&#8217;&#233;nergie pourrait &#234;tre in-
corpor&#233; au syst&#232;me Cortex, o&#249; il jouerait le r&#244;le d&#8217;une des m&#233;triques pilot&#233;e par un algorithme
de d&#233;cision. Ceci permettrait d&#8217;obtenir des r&#233;sum&#233;s &#224; l&#8217;aide d&#8217;une requ&#234;te de l&#8217;utilisateur ou
des r&#233;sum&#233;s multi-documents. Une autre voie int&#233;ressante est le calcul des propri&#233;t&#233;s comme la
&#171; magn&#233;tisation &#187; d&#8217;un document. (Shukla, 1997) a etudi&#233; des ph&#233;nom&#232;nes magn&#233;tiques dans
les r&#233;seaux de neurones type Hopfield dont on pense se servir. On &#233;tudiera la r&#233;ponse du sys-
t&#232;me face &#224; l&#8217;application d&#8217;un champ externe. Ce champ, repr&#233;sent&#233; par le vecteur des termes
d&#8217;un texte decrivant une th&#233;matique (topique) sera mis en relation avec un document. Ainsi, les
phrases du document pourraient, ou non, s&#8217;aligner selon leur degr&#233; de pertinence par rapport
&#224; la th&#233;matique. Ceci permettrait de g&#233;n&#233;rer des r&#233;sum&#233;s personnalis&#233;s, telles que d&#233;finis dans
les t&#226;ches DUC. Nous avons &#233;galement abord&#233; le probl&#232;me de la d&#233;tection des fronti&#232;res th&#233;-
matiques des documents. La m&#233;thode, bas&#233;e sur la matrice d&#8217;&#233;nergie du syst&#232;me de spins, est
coupl&#233;e &#224; un test statistique non-param&#232;trique robuste. Les r&#233;sultats sont tr&#232;s encourageants.
Une critique de la m&#233;thode d&#8217;&#233;nergie pourrait &#234;tre qu&#8217;elle n&#233;cessite la manipulation (produits,
transpos&#233;e) d&#8217;une matrice de taille [P &#215; P ]. Cependant la repr&#233;sentation sous forme de graphe
nous permet d&#8217;ex&#233;cuter ces calcus en temps P log(P ) et en espace P 2.
</p>
<p>33</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Silvia FERN&#193;NDEZ, Eric SANJUAN, Juan Manuel TORRES-MORENO
</p>
<p>R&#233;f&#233;rences
ALPHONSE E., AMRANI A., AZ&#201; J., HEITZ T., MEZAOUR A.-D. &amp; ROCHE M. (2005). Pr&#233;-
paration des donn&#233;es et analyse des r&#233;sultats de DEFT&#8217;05. In TALN 2005 - Atelier DEFT&#8217;05,
volume 2, p. 95&#8211;97.
AMINI M.-R., ZARAGOZA H. &amp; GALLINARI P. (2000). Learning for sequence extraction
tasks. In RIAO 2000, p. 476&#8211;489.
BRANTS T., CHEN F. &amp; TSOCHANTARIDIS I. (2002). Topic-based document segmentation
with probabilistic latent semantic anaysis. In CIKM&#8217;02, p. 211&#8211;218, McLean, Virginia, USA.
CAILLET M., PESSIOT J.-F., AMINI M. &amp; GALLINARI P. (2004). Unsupervised learning
with term clustering for thematic segmentation of texts. In RIAO&#8217;04, p. 648&#8211;657, France.
CHUANG S.-L. &amp; CHIEN L.-F. (2004). A practical web-based approach to generating Topic
hierarchy for Text segments. In Thirteenth ACM conference on Information and knowledge
management, p. 127&#8211;136, Washington, D.C., USA.
GALLEY M., MCKEOWN K. R., FOSLER-LUSSIER E. &amp; JING H. Discourse segmentation
of multi-party conversation. In Proceedings of the 41st Annual Meeting of the Association for
Computational Linguistics (ACL&#8211;03), Sapporo, Japan.
HERTZ J., KROGH A. &amp; PALMER G. (1991). Introduction to the theorie of Neural Computa-
tion. Redwood City, CA : Addison Wesley.
HOPFIELD J. (1982). Neural networks and physical systems with emergent collective compu-
tational abilities. Proceedings of the National Academy of Sciences of the USA, 9, 2554&#8211;2558.
KOSKO B. (1988). Bidirectional associative memories. IEEE Transactions Systems Man,
Cybernetics, 18, 49&#8211;60.
LIN C.-Y. (2004). Rouge : A package for automatic evaluation of summaries. In Workshop
on Text Summarization Branches Out (WAS 2004).
MA S. (1985). Statistical Mechanics. Philadelphia, CA : World Scientific.
MANI I. &amp; MAYBURY M. T. (1999). Advances in Automatic Text Summarization. MIT Press.
MANNING C. D. &amp; SCHUTZE H. (2000). Foundations of Statistical Natural Language Pro-
cessing. The MIT Press.
PEVZNER L. &amp; HEARST M. (2002). A critique and improvement of an evaluation metric for
text segmentation. In Computational Linguistic, volume 1, p. 19&#8211;36.
PORTER M. (1980). An algorithm for suffix stripping. Program, 14(3), 130&#8211;137.
RADEV D., WINKEL A. &amp; TOPPER M. (2002). Multi Document Centroid-based Text Sum-
marization. In ACL 2002.
SALTON G. &amp; MCGILL M. (1983). Introduction to modern information retrieval. Computer
Science Series McGraw Hill Publishing Company.
SHUKLA P. (1997). Response of the Hopfield-Little model in an applied field. Physical Review
E, 56(2), 2265&#8211;2268.
SIEGEL S. &amp; CASTELLAN N. (1988). Nonparametric statistics for the behavioral sciences.
McGraw Hill.
SITBON L. &amp; BELLOT P. (2005). Segmentation th&#233;matique par cha&#238;nes lexicales pond&#233;r&#233;es.
In TALN 2005, volume 1, p. 505&#8211;510.
TORRES-MORENO J.-M., VEL&#193;ZQUEZ-MORALES P. &amp; MEUNIER J. (2002). Condens&#233;s de
textes par des m&#233;thodes num&#233;riques. In JADT, volume 2, p. 723&#8211;734.
</p>
<p>34</p>

</div></div>
</body></html>