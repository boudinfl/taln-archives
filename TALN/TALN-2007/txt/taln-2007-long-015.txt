TALN 2007, Toulouse, 5–8 juin 2007
Architecture compositionnelle pour les dépendances croisées
Alexandre DIKOVSKY
LINA-FRE CNRS 2729, Université de Nantes
Alexandre.Dikovsky@univ-nantes.fr
Résumé. L’article présente les principes généraux sous-jacent aux grammaires catégo-
rielles de dépendances : une classe de grammaires de types récemment proposée pour une des-
cription compositionnelle et uniforme des dépendances continues et discontinues. Ces gram-
maires très expressives et analysées en temps polynomial, adoptent naturellement l’architecture
multimodale et expriment les dépendances croisées illimitées.
Abstract. This article presents the general principles underlying the categorial depen-
dency grammars : a class of type logical grammars recently introduced as a compositional and
uniform definition of continuous and discontinuous dependences. These grammars are very
expressive, are parsed in a reasonable polynomial time, naturally adopt the multimodal archi-
tecture and explain unlimited cross-serial dependencies.
Mots-clés : grammaires catégorielles de dépendances, grammaires multimodales, ana-
lyseur syntaxique.
Keywords: categorial dependency grammars, multimodal grammars, syntactic parser.
1 Introduction
L’intérêt principal des grammaires de types logiques dont les grammaires catégorielles (GC) est
leur lien direct et transparent avec la sémantique formelle compositionnelle. Ce lien est établi
pour une phrase générée à travers l’isomorphisme entre une preuve de correction du choix des
types pour les mots dans la phrase et l’expession sémantique extraite de cette preuve. Les rela-
tions syntaxiques entre les mots définies par les types sont formaliseés par un calcul logique de
types qui n’est pas spécifique à une grammaire mais à une classe de grammaires. On construit
ainsi des interfaces simples et élégantes entre la syntaxe et la sémantique à la base de principes
plus ou moins universels. Tant que les relations entre les mots (dépendances) s’accordent bien
avec les relations de précédence (ordre des mots), à savoir lorsqu’elles ne dépassent jamais
les limites des domaines syntaxiques l ocaux des mots (dépendances projectives), les preuves
de correction sont isomorphes aux systèmes de constituents des phrases. A ce niveau de re-
présentation syntaxique il est en principe possible de définir les types directement en termes de
dépendances. En fait, les premières définitions des grammaires de dépendances (GD) (Gaifman,
1961) ont été similaires à celle des GC classiques (Bar-Hillel, 1953) 1. Cependant, il existe dans
1Or, même à ce niveau, on peut remarquer qu’à la différence des grammaires de types logiques, les GD traitent
les modifieurs comme adjoints.
165
Alexandre DIKOVSKY
toute langue des dépendances non bornées par les domaines locaux (dépendances non projec-
tives). Elles sont dues aux formes et aux mots fonctionnels discontinus (comme les particules
négatives, les pronoms comparatifs, etc.), ou à l’interférence des éléments des structures extra-
syntaxiques telles que la structure communicative (cf. la topicalisation), la co-référence, les
relations de portée, etc. ou, au contraire, sont dues au manque en surface, des membres des
relations sémantiques (comme c’est le cas de la relativisation ou de l’extraction au cours de
la coordination). Pour y faire face les calculs logiques sont complétés par des règles qui, d’un
côté, rendent les preuves plus flexibles au détriment du lien direct avec les constituents, e.g. en
montant les types (Lambek, 1961; Steedman, 1996), et d’un autre côté, les rendent plus sélec-
tives, e.g. en choisissant les règles structurelles spécifiques en foncion de connecteurs différents
(règles multimodales dues à Oehrle, Morrill, Moortgat et Hepple (Morrill, 1994; Moortgat,
1997)). Avec ces moyens on peut exprimer les dépendances non bornées tout en gardant l’in-
terprétation sémantique compositionnelle. En même temps, à cause de l’expressivité accrue, la
complexité des preuves devient exponentielle, voire pire.
Dans l’article (Dikovsky, 2004), nous avons proposé une nouvelle architecture compositionnelle
de types invariables de dépendances (sans montée des types). Elle est établie sur la base de la
distinction faite entre les types neutres des dépendances projectives, qui sont formalisés par la
règle classique d’élimination d’arguments, et les types des dépendances non bornées (valences)
dotés de polarisation et d’orientation, qui sont formalisés par une règle appelée FA (first avai-
lable) de saturation (appariement) des valences. La base psycholinguistique de cette règle est
l’hypothèse que les dépendances non bornées sont gérées par les piles dans la mémoire dyna-
mique d’analyse. FA sélectionne la plus proche valence polarisée duale dans la direction indi-
quée. Elle est conforme avec la majorité des dépendances non projectives dans maintes langues.
On a élaboré différents calculs de dépendances avec la règle FA (Dekhtyar & Dikovsky, 2004;
Dekhtyar & Dikovsky, 2007). Les Grammaires Catégorielles de Dépendances (CDG) corres-
pondantes s’avèrent expressives. En même temps, elles disposent d’algorithmes d’analyse en
temps polynomial. Tout de même, la règle FA n’est pas universelle. Par exemple, elle n’est pas
adaptée aux dépendances croisées illimitées du hollandais exposées dans (Bresnan et al., 1982).
C’est pourquoi, dans cet article nous explorons une autre règle d’appariement FC (first cross)
qui sélectionne la première valence polarisée duale croisée dans la direction indiquée. Ainsi, la
structure dynamique de mémoire qui correspond à cette règle est la file d’attente. FC explique
les dépendances croisées illimitées en termes d’un langage simple de structures de dépendances
et non en termes du langage de copies, comme d’habitude. A l’instar de grammaires multimo-
dales de types, nous définissons les CDG multimodales (mmCDG) où les règles d’appariement
sont considérées comme les modes de compositionnalité propres aux dépendances non projec-
tives. Nous montrons que la règle FC est aussi efficace que la règle FA et nous présentons un
algorithme d’analyse syntaxique de ces grammaires en temps polynomial.
2 Grammaires catégorielles de dépendances
Les CDG sont des grammaires catégorielles (GC) qui, à la différence des GC classiques, dé-
finissent explicitement les relations de dépendance entre les mots dans la phrase et non les
relations de dominance entre les constituants. Elles peuvent déterminer les structures de dé-
pendances (SD) plus générales que les arbres de dépendances (AD). Une SD d’une phrase
w = w1 . . . wn est un graphe orienté dont les nœux sont les mots w1, . . . , wn ordonnés par
l’ordre dans w, avec un nœux sélectionné (la tête) et dont les arcs sont étiquetés par les noms
166
Architecture compositionnelle pour les dépendances croisées
Figure 1
des dépendances. E.g., la SD en figure 1 est un AD dont la tête (sa racine) est le mot était.
Comme toutes les GC, les CDG n’ont pas de règles. Une CDG peut être vue comme un lexique
qui affecte à chaque mot un ensemble de types de dépendances. La particularité essentielle des
types des CDG est la distinction faite entre les types de dépendances projectives qui relient le
gouverneur à ses subordonnés appartenants à son domaine local, et les types de dépendances
non projectives (non bornées) qui le relient aux subordonnés déplacés vers les domaines des
autres mots. Les premiers sont définis par les sous types arguments des types du gouverneur,
tandis que les derniers sont définis par les valences dotées d’une polarisation et d’une orienta-
tion (gauche / droite) dont l’ensemble constitue pour chaque type son potentiel. Formellement,
les types de dépendances sont construits à partir d’un ensemble C de types primitifs et d’un
ensemble V (C) de valences polarisées. Les éléments de C sont les noms des relations de
dépendance, dont un type sélectionné S (l’axiome). Les valences dans V (C) sont orientées :
V (C) = V l(C) ? V r(C), où V l(C) consiste des valences gauches?d (négative),?d (posi-
tive) et V r(C) consiste des valences droites?d (positive),?d (négative) où d ? C.
Un type (de dépendance) est une expression ?P , où ? est un type basique et P est un potentiel.
gCat(C) va noter l’ensemble des types sur C. Les types basiques B(C) sur C sont les types
fonctionnels traditionnels du 1r ordre destinés à définir les dépendances projectives :
1. C ? B(C). 2. Si ? ? C et ? ? B(C), alors [?\?], [?? \?], [?/??], [?/?] ? B(C). !
Les constructeurs \, / étant supposés associatifs, tout type basique peut être représenté sous la
forme [alm\...\al1\f/ar1/.../arn]. Intuitivement, f est la dépendance du gouverneur et ali, arj
correspondent aux dépendances des subordonnés gauches et droites. d? correspond à la dépen-
dance d itérée. f = S est le type des SD correctes. Les potentiels sont les suites de valences
polarisées. Ils sont destinés à définir les dépendances non projectives. Dans le cas de dépen-
dances projectives, ils sont vides. Les types avec le potentiel vide sont neutres. Par exemple,
l’AD projectif en figure 1 est défini par les types neutres suivants :
au )? [c?copul/prepos?a] commencement )? [prepos?a] le )? [det]
était )? [c?copul\S/pred] Verbe )? [det\pred]
Les valences ? d et ? d, d ? C, peuvent être vues comme les parenthèses gauches. Res-
pectivement, ? d et ? d sont les parenthèses droites. Pour une valence gauche, e.g. ?d, la
valence correspondante (duale) droite,?d, est notée ?˘d =?d. Ensemble ces valences duales
appariées définissent la dépendance non projective d. L’adjacence est exprimée en utilisant les
types primitifs d’ancrage : pour ancrer une valence négative v ? {?d,?d | d ? C} (la fin
d’une dépendance non projective), c’est-à-dire la placer auprès d’un mot d’appui, sont utilisés
les types primitifs particuliers d’ancrage :#(v) dont l’élimination signifie l’adjacence des mots
et ne crée aucune dépendance. E.g., l’AD non projectif en figure 2 est défini par
Figure 2
167
Alexandre DIKOVSKY
les types qui ancrent les clitiques la, lui sur l’auxiliaire a :
elle !? [pred] a !? [#(?clit?iobj)\#(?clit?dobj)\pred\S/aux]
la !? [#(?clit?dobj)]?clit?dobj lui !? [#(?clit?iobj)]?clit?iobj
donnée !? [aux]?clit?iobj?clit?dobj
Le sens exact des types est défini par le calcul de dépendances suivant 2 :
Ll. CP1 [C\?]P2 % [?]P1P2
Il. CP1 [C?\?]P2 % [C?\?]P1P2
?l. [C?\?]P % [?]P
DlM. ?
P1(?C)P (?C)P2 % ?P1PP2 , si P1(?C)P (?C)P2 satisfait la règle d’appariementM.
Ll est la règle classique d’élimination. En éliminant le sous-type argument C '= #(?), elle crée
la dépendance projective C et concatène les potentiels. C = #(?) ne crée aucune dépendance.
Il crée k > 0 exemplaires de C. ?l sert pour le cas k = 0 et pour éliminer le sous-type itéré.
DlM apparie et élimine deux valences duales?C et?C selon la règle d’appariementM et crée
la dépendance non projective C. Voici deux règles importantes d’appariement :
FAl : P n’a pas d’occurrence de?C,?C (apparier à la plus proche valence duale disponible).
FCl : P1 et P n’ont pas d’occurrences, respectivement, de?C et de?C (apparier à la premiere
valence duale croisée, c’est-à-dire à la plus lointaine disponible).
On voit que les valences ressamblent aux traits Slash des GPSG, HPSG, mais à la place de règles
complexes de « propagation »des traits Slash les CDG utilisent les règles simples d’appariement
FA et FC. En admettant que toute dépendance non projective C peut avoir sa propre règle
d’appariementMC nous considérons cette règle comme un mode de compositionnalité à travers
C. Nous obtenons ainsi par analogie avec l’architecture multimodale pour les grammaires de
Lambek (Morrill, 1994; Moortgat, 1997) la notion suivante de grammaire.
Définition 1 Une grammaire catégorielle multimodale de dépendances (mmCDG) est une
structure G = (W,C, S, ?, µ), où W est un vocabulaire, ? (le lexique) est une fonction qui
affecte à chaque mot dansW un sous ensemble fini de types dans gCat(C) et µ est une fonction
qui affecte une règle d’appariement à toute dépendance non projective dans C.
Le calcul de dépendances détermine la relation de prouvabilité correspondante %µ sur les suites
de types. La prouvabilité sans règles D (c’est-à-dire, au cas de dépendances projectives) est
notée %c . Pour une SDD et une phrase w, la relationG(D,w) signifie : «D est créée au cours
d’une preuve ? %µ S pour une suite de types ? ? ?(w) ».
Le langage et le langage des SD générés parG sont respectivement les ensembles L(G)=df {w |
?D G(D,w)} et ∆(G)=df {D | ?w G(D,w)}. mmCDGµ et L(mmCDGµ) sont respective-
ment la famille des grammaires et des langages correspondants.
3 Expressivité des mmCDG
Les mmCDG sont très expressives. Avec la règle FA elles génèrent tous les langages non
contextuels (algébriques), mais aussi maints langages contextuels dont {anbncn | n > 0}, les
langages L(m) = {an1an2 ...anm | n ≥ 1} (Dikovsky, 2004) qui sont faiblement contextuels mais
non-TAG à partir de m > 4, le langage MIX, qui contient toutes permutations des motifs
anbncn, n > 0, MIX = {w ? {a, b, c}+ | |w|a = |w|b = |w|c}. Or, selon l’hypothèse de E.
2Nous exposons les règles gauches. Les règles droites sont symétriques.
168
Architecture compositionnelle pour les dépendances croisées
Bach, MIX n’est pas faiblement contextuel, ainsi il ne serait pas généré par une grammaire
minimaliste, ou multi-TAG, etc. Dans (Dekhtyar & Dikovsky, 2007) on peut trouver d’autres
exemples et une preuve du fait que L(mmCDGFA) est une famille abstraite de langages (AFL).
D’un autre côté, nous croyons (Dikovsky, 2004; Dekhtyar & Dikovsky, 2004) que le langage de
copies Lcopy = {ww |w ? {a, b}+}, qui est généré par une grammaire TAG, n’appartient pas
à la famille L(mmCDGFA,FC). Ce langage est d’un intérêt particulier parce qu’on croit qu’il
est un modèle de la construction en néerlandais dite des « dépendances croisées illimitées ».
Il s’agit des phrases n1n2 . . . nmnm+1v1v(inf)2 . . . v(inf)m, dont un exemple est en figure 3, où
il y a une dépendance prédicative n1 pred?? v1 entre le verbe v1 en forme finie et le nom n1,
les dépendances prédicatives ni pred?? v(inf)i entre les verbes v(inf)i à l’infinitif et les noms ni,
pour tout 2 ≤ i ≤ m, et éventuellement, une dépendance d’objet direct nm+1 dobj?? v(inf)m si le
verbe v(inf)m est transitif et le nom nm+1 est présent (c’est-à-dire, nm+1 %= ?).
!
!"
!
!"
!
!"#$!
%&
!
%&
!
%&
!
%&
!
Jan Piet Marie de kinderen zag helpen laten zwemmen
det
pred
pred
pred
inf?dobj inf?dobj inf?dobj
inf?dobj
?Jan Piet Marie les enfants a vu aider faire nager
Figure 3.
Par ailleurs, une analyse plus approfondie de cette construction (Pulman & Ritchie, 1985)
montre que l’accord des formes existe seulement entre n1 et v1. Sinon, la forme du nom su-
bordonné est déterminée seulement par le verbe transitif v(inf)m et son argument nm+1. Cela
implique que le vrai modèle de cette construction n’est point le langage Lcopy, mais le langage
des SD ∆cross = {D(m) |m > 0} surW = N ? V, où N ? V = ?, D(m) est la SD en figure 4
et nil ? N, vjr ? V. En même temps, le langage correspondant est algébrique (voire linéaire).
!
'(
!
'(
%&
!
%&
!
%&
!
ni1 ni2 nim vj2vj1. . . vjm
R R
. . .
. . .
L
L
D(m) =
L
Figure 4. AD D(m)
Le langage ∆cross est généré par lammCDGFC suivante :
Gcross =
{
n )? [#(L)]?L, [#(L)\#(L)]?L, pour n ? N
v )? [#(L)\S/R]?L, [R/R]?L, [R]?L, pour v ? V
E.g., une preuve de D(3) ? ∆cross est montrée en figure 5.
[#(L)]?L[#(L)\#(L)]?L
(Ll)
[#(L)]?L?L [#(L)\#(L)]?L
(Ll)
[#(L)]?L?L?L
[#(L)\S/R]?L
[R/R]?L[R]?L
(Lr)
[R]?L?L
(Lr)
[#(L)\S]?L?L?L
(Ll)
[S]?L?L?L?L?L?L
(DlFC ? 3)
[S]
Figure 5.
169
Alexandre DIKOVSKY
4 Fondements théoriques
Notre solution du problème des dépendances croisées repose sur l’indépendance des types ba-
siques et des valences polarisées dans les preuves du calcul de dépendances. Cette propriété est
exprimée en termes de projections et de suites de catégories bien appariées.
Pour une suite de catégories ? ? gCat(C)? ses projections locale ???l et de valences ???v sont
définies ainsi : pour tous ? ? gCat(C), ? ? gCat(C)? et CP ? gCAT (C),
1. ???l = ???v = ?; ????l = ???l???l et ????v = ???v???v
2. ?CP?l = C et ?CP?v = P.
Pour un potentiel P, sa projection ?P?d sur une paire de valences duales vd, v˘d est définie
comme h(P ) pour l’homomorphisme h(?) = ? si ? ? {vd, v˘d} et h(?) = ? sinon. P est dit
équilibré si toute projection ?P?d est bien appariée au sens habituel.
Soit |P |x le nombre d’occurrences de x dans P. Alors l’équilibre d’un potentiel P est incrémen-
talement vérifiable en utilisant les quantités suivantes pour toute ? ? V l(C) et ?˘ ? V r(C) :
∆?(P ) = max{|P ?|? ? |P ?|?˘ | P ? est un suffixe de P},
∆?˘(P ) = max{|P ?|?˘ ? |P ?|? | P ? est un pre´fixe de P}.
Elles expriment respectivement le déficit des ??parenthèses droites et gauches dans P (c’est-
à-dire, le nombre de parenthèses droites (gauches) qu’il faut rajouter à P de droite (de gauche)
pour qu’il devienne équilibré. Les propriétés suivantes sont vérifiées (Dekhtyar & Dikovsky,
2004; Dekhtyar & Dikovsky, 2007) :
Lemme 1 1. Quels que soient des potentiels P1, P2 et des valences ? ? V l(C), ?˘ ? V r(C),
∆?(P1P2) = ∆?(P2) +max{∆?(P1)?∆?˘(P2), 0},
∆?˘(P1P2) = ∆?˘(P1) +max{∆?˘(P2)?∆?(P1), 0}.
2. Un potentiel P est équilibré ssi ∑
??V (C)
∆?(P ) = 0.
La propriété suivante d’indépendance des projections (Dekhtyar & Dikovsky, 2004; Dekhtyar
& Dikovsky, 2007) garantit l’existence d’un algorithme polynomial d’analyse demmCDGFA.
Théorème 1 Pour une mmCDG G = (W,C, S, ?, µ) avec le mode FA et x ? W+, x ? L(G)
ssi il y a une suite ? ? ?(x) telle que ???l $c S et ???v est équilibré.
Le seul point de sa preuve sensible aux modes est la proposition suivante vraie pour FA :
Lemme 2 Un potentiel P est équilibré ssi pour toute catégorie ?P il y a une preuve ?P $ ?
utilisant exclusivement les règlesDlM etDrM.
Pour garantir l’indépendance des projections (et par conséquent, une analyse polynomiale) pour
une mmCDGM , il faut prouver ce lemme pour tout modeM ? M. En prouvant le lemme 2
pour FC, nous avons étendu le théorème 1 aux mmCDG avec les modes FA,FC :
Théorème 2 Pour x ? W+ et pour une mmCDGM G = (W,C, S, ?, µ) avec M = {FA},
ouM = {FC} ouM = {FA,FC}, x ? L(G) ssi il y a une suite ? ? ?(x) telle que ???l $c S
et ???v est équilibré.
Corollaire 1 L(mmCDGFA) = L(mmCDGFC) = L(mmCDGFA,FC).
170
Architecture compositionnelle pour les dépendances croisées
5 Analyse syntaxique, complexité
Dans l’article (Dekhtyar & Dikovsky, 2004) un algorithme d’analyse en temps polynomial a été
décrit pour une version sous commutative du calcul de dépendances 3. Dans l’article (Dekhtyar
& Dikovsky, 2007) cet algorithme a été étendu aux mmCDGFA. Ce même algorithme à un
détail près s’applique aussi auxmmCDGFA,FC. Nous l’exposons en figure 6.
Fonctions d’échec. Soit une mmCDGM G = (W,C, S, ?, µ) avec les valences polarisées
gauches V l(C) = {v1, . . . , vp} et droites V r(C) = {v˘1, . . . , v˘p}. Nous allons d’abord définir
deux fonctions d’échec qui vont servir pour une optimisation de l’analyse. Soit w = w1w2...wn
? W+. Alors, pour 1 ≤ i ≤ n, ? ? V l(C) et ? ? V r(C),
piL(?, i) = max{∆?(???v) | ? ? ?(w1...wi)},
piR(?, i) = max{∆?(???v) | ? ? ?(wn?i+1...wn)}
sont les fonction d’échec gauche et droite. On suppose que piL(?, 0) = piR(?, 0) = 0.
Algorithme d’analyse syntaxique. mmCdgPars est un algorithme typique de « program-
mation dynamique ». Il s’applique à une mmCDGM et à une phrase w = w1w2...wn ? W+ et
remplit une matrice triangulaireM dont la dimension est n?n. L’élémentM [i, j], i ≤ j, deM
correspond à l’intervalle wi...wj de la phrase et représente un ensemble fini d’« items ». Un item
est une expression I = ?C,∆L,∆R, I l, Ir? qui code une catégorie CP , où C est une catégorie
basique (C ? B(C)), ∆L = (∆v1 , . . . ,∆vp) et ∆R = (∆v˘1 , . . . , ∆v˘p) sont les vecteurs entiers
dont chaque composante i correspond à la valence vi, respectivement v˘i, et vaut le déficit cor-
respondant des vi-parenthèses droites (gauches) dans le potentiel P. Finalement, I l, Ir sont les
identificateurs des items dans les angles gauches et droites deM à partir desquelles est calculé
l’item I (pour tout I ?M [i, i] I l = Ir = ?).
Complexité. Pour unemmCDGM G = (W,C, S, ?, µ), soit lG = |?| le nombre d’affectations
des catégories aux mots dans le lexique, soit aG = max{k | ?x ? W ([?k\...\?1\C/?]P ?
?(x) ? [?\C/?1/.../?k]P ? ?(x))} le nombre maximal de sous types arguments dans les
catégories affectées, soit pG = |V l(C)| = |V r(C)| le nombre de valences polarisées et ∆G =
max{∆?(P ) | ?x ?W (CP ? ?(x) ? ? ? V (C))} le déficit maximal des valences parenthèses
dans les catégories affectées. Finalement, soit n la longueur de la phrase analysée.
Théorème 3 L’algorithmemmCdgPars a une complexité en tempsO(lG ·a2G ·(∆G ·n)2pG ·n3).
Remarque 1 1. Pour une grammaire fixée G, les valeurs lG, aG, pG et ∆G sont constantes. Si
G varie, alors le problème d’appartenance devient NP -complèt (Dekhtyar & Dikovsky, 2004).
2. Si G est sans valence polarisée, alors la complexité est O(n3).
3. Soit le déficit maximal de valences ?G(n) des potentiels survenants dans les preuves des
phrases dont la longueur est limitée par n. Si ?G(n) est bornée par une constante c, alors G
peut être transformée en une mmCDGG? sans valence polarisée dont le langage est algébrique
(Dikovsky, 2001). Or, la taille deG? est exponentielle par rapport àG. Si, de plus, le nombre des
dépendances non bornées dans une SD engendrée par G n’est jamais supérieur à une borne
constante uniforme (ce qui est typique pour maintes langues), alors la complexité est O(n3)
pour la même grammaire G.
4. D’un autre côté, même si toute dépendance de G (sauf S) était définie par une valence
polarisée, la complexité serait toujours polynomiale. Cette remarque explique que les mmCDG
sont bien adaptées aux langages avec l’ordre flexible. Les limites de cet article ne nous laissent
pas faire une analyse plus détaillée de ce cas important.
3L’algorithme a été réalisé en LISP par Darin et Hristian Todorov et en en C# par Ilya Zaytsev.
171
Alexandre DIKOVSKY
AlgorithmemmCdgPars
//Entrée : mmCDG G, phrase w = w1...wn
//Sortie : ?“succe`s”, DS D? ssi w ? L(G)
{
CalcFailFuncL() ;
CalcFailFuncR() ;
for (k = 1, . . . , n)
{
Propose( k )
}
for (l = 2, . . . , n)
{
for (i = 1, . . . , n? l)
{
j := i+ l ? 1;
for (k = i, . . . , j ? l)
{
SubordinateL(i, k, j) ;
SubordinateR(i, k, j) ;
}
}
}
if (I = ?S, (0, 0, . . . , 0), (0, 0, . . . , 0), I l, Ir? ?M [1, n])
return ?“succe`s”, Expand(I)? ;
//procedure Expand( I ) calcule la SD de sortie.
//Elle seule est sensible aux règles d’appariement
//FA,FC. Elle est technique et n’est pas incluse
else
return ?“e´chec”, ?? ;
}
//For 1 ≤ i ≤ n
Propose( i )
{
(loop) foreach (CP ? ?(wi)
{
foreach (v ? V l(C))
{
∆L[v] := ∆v(P ) ;
if (∆L[v] > piR[v˘, n? j]) next (loop) ;
∆R[v˘] := ∆v˘(P ) ;
if (∆R[v˘] > piL[v, i? 1]) next (loop) ;
}
AddItem( M [i, i], ?C,∆L,∆R, ?, ?? ) ;
}
}
AddItem( M [i, j], ?C,∆L,∆R, I l, Ir? )
{
M [i, j] := M [i, j] ? {?C,∆L,∆R, I l, Ir?} ;
if (C = [C ? ? \?])
{
AddItem( M [i, j], ?[?],∆L,∆R, I l, Ir? ) ;
}
if (C = [?/C ??])
{
AddItem( M [i, j], ?[?],∆L,∆R, I l, Ir? ) ;
}
}
CalcFailFuncL()
{
foreach (v ? V l(C))
{
piL[v, 0] := 0;
for (i = 1, . . . , n)
{
pimax := 0;
foreach (CP ? ?(wi))
{
pimax := max{pimax,∆v(P )+
max{piL[v, i? 1]?∆v˘(P ), 0}};
}
piL[v, i] := pimax;
}
}
}
CalcFailFuncR() est similaire.
//For 1 ≤ i ≤ k ≤ j ≤ n
SubordinateL( i, k, j )
{
(loop) foreach (I1 = ??1,∆L1 ,∆R1 , I l1, Ir1? ?M [i, k],
I2 = ??2,∆L2 ,∆
R
2 , I
l
2, I
r
2? ?M [k + 1, j])
{
foreach (v ? V l(C))
{
∆L[v] := ∆L2 (v) +max{∆
L
1 (v)?∆
R
2 (v), 0} ;
if (∆L[v] > piR[v˘, n? j]) next (loop) ;
∆R[v˘] := ∆R1 (v˘) +max{∆
R
2 (v˘)?∆
L
1 (v˘), 0} ;
if (∆R[v˘] > piL[v, i? 1]) next (loop) ;
}
if ( ?1 = C and ?2 = [C\?] )
{
AddItem( M [i, j], ?[?],∆L,∆R, I1, I2? ) ;
}
elseif ( (?1 = C and ?2 = [C ? \?]) or ?1 = [?] )
{
AddItem( M [i, j], ??2,∆L,∆R, I1, I2? ) ;
}
}
}
SubordinateR( i, k, j ) est similaire.
Figure 6. AlgorithmemmCdgPars
172
Architecture compositionnelle pour les dépendances croisées
6 Comparaison, discussion
Certes, il y a des grammaires où l’expression des dépendances non bornées ne pose pas pro-
blème, e.g. HPSG (Pollard & Sag, 1988), les extensions multimodales des grammaires de Lam-
bek (Morrill, 1994; Moortgat, 1997), dont certaines visent notamment les dépendances (Kruijff,
2001) et leur fournissent une interface compositionnelle avec la sémantique. Or, l’analyse avec
ces formalismes expressifs est très complexe et parfois nécessite l’utilisation des systèmes de
démonstration des théorèmes. C’est aussi le cas des grammaires qui représentent PTIME,
dont RCG (Boullier, 2003). A la différence de mmCDG, ces grammaires n’ont pas d’algo-
rithme universel d’analyse en temps O(nk), où k dépend de l’alphabet. Cela concerne aussi les
grammaires basées sur l’unification et les contraintes, e.g. (Duchier, 1999). Contrairement à ces
formalismes, les mmCDG n’utilisent que les moyens primitifs d’une complexité faible. E.g.,
les Grammaires Topologiques de Dépendances (Duchier & Debusmann, 2001) (voir aussi (Brö-
ker, 1998; Duchier et al., 2004)) utilisent les hierarchies des domaines de l’ordre des mots
(WO-domains) qui, en cas de discontinuité, servent à exprimer les contraintes de contiguïté,
de distance entre un gouverneur et son modifieur etc. Dans beaucoup des cas, ces contraintes
sont exprimées dans mmCDG par le moyen de sous types d’ancrage placés dans les positions
correspondantes d’un type du gouverneur.
Les mmCDG représentent une alternative intéressante aux TAG (et équivalentes : CCG, HG
(Vijay-Shanker & Weir, 1994)) et aux grammaires faiblement contextuelles (Joshi et al., 1991),
telles multi-TAG, non contextuelles multi-composantes, minimalistes, etc. Tout comme ces der-
nières, les mmCDG disposent d’une analyse syntaxique en temps polynomial. On peut même
constater, qu’en pratique l’algorithmemmCdgPars va avoir une complexitéO(n3). Leur avan-
tage décisif est l’architecture compositionnelle de dépendances où toutes les dépendances, pro-
jectives comme non bornées, sont définies par les types fonctionnels, ce qui crée la base né-
cessaire pour une sémantique fonctionnelle de dépendances. En même temps, cette architecture
adopte naturellement la multimodalité des dépendances non bornées correspondant aux règles
de saturation des valences spécifiques aux différentes langues. Il est important de noter que cette
flexibilité syntaxique est atteinte sans explosion du coût de l’analyse syntaxique (par contraste
avec les grammaires de Lambek). Malgré leur simplicité, les mmCDG sont très expressives.
On a vu que pour exprimer les dépendances croisées illimitées on n’a pas besoin du langage de
copies, mais d’un langage des SD facilement exprimé par les mmCDG. Et le fait queMIX est
un langage mmCDGFA montre que ces grammaires sont adaptées aux langues naturelles avec
l’ordre des mots flexible.
Enfin, il est difficile de comparer les mmCDG par l’expressivité avec les autres GD qui traitent
les dépendances non bornées et qui les analysent en temps polynomial, e.g. (Kahane et al.,
1998; Bröker, 2000). Le pouvoir de ces grammaires n’est pas déterminée. Leurs définitions
sont opérationnelles (cf. le « lifting »). L’avantage des mmCDG est leur transparence et leur
architecture compositionnelle de dépendances.
Références
BAR-HILLEL Y. (1953). A quasi-arithmetical notation for syntactic description. Language,
29(1), 47–58.
BOULLIER P. (2003). Counting with range concatenation grammars. Theoretical Computer
Science, 293, 391–416.
173
Alexandre DIKOVSKY
BRESNAN J., KAPLAN R., PETERS S. & ZAENEN A. (1982). Cross-serial dependencies in
dutch. Linguistic Inquiry, 13(4), 613–635.
BRÖKER N. (1998). Separating surface order and syntactic relations in a dependency gram-
mar. In Proc. COLING-ACL, p. 174–180, Montreal.
BRÖKER N. (2000). Unordered and non-projective dependency grammars. Traitement Auto-
matique des Langues (TAL), 41(1), 245–272.
DEKHTYAR M. & DIKOVSKY A. (2004). Categorial dependency grammars. In M. MOORT-
GAT & V. PRINCE, Eds., Proc. of Intern. Conf. on Categorial Grammars, p. 76–91.
DEKHTYAR M. & DIKOVSKY A. (2007). Generalized categorial dependency gram-
mars. In submission, www.sciences.univ-nantes.fr/info/perso/permanents/
dikovsky/.
DIKOVSKY A. (2001). Polarized non-projective dependency grammars. In P. DE GROOTE, G. MORILL
& C. RETORÉ, Eds., Proc. of the Fourth Intern. Conf. on Logical Aspects of Computational Linguistics,
volume 2099 of LNAI, p. 139–157 : Springer.
DIKOVSKY A. (2004). Dependencies as categories. In “Recent Advances in Dependency Grammars".
COLING’04 Workshop, p. 90–97.
DUCHIER D. (1999). Axiomatizing dependency parsing using set constraints. In Sixth Meeting on
Mathematics of Language (MOL-6), p. 115–126, Orlando, Florida.
DUCHIER D. & DEBUSMANN R. (2001). Topological dependency trees : A constraint-based account
of linear precedence. In Proc. of the Intern. Conf. ACL’2001, p. 180–187 : ACL & Morgan Kaufman.
DUCHIER D., DEBUSMANN R. & KRUIJFF G.-J. M. (2004). Extensible dependency grammar : A
new methodology. In COLING’04 Workshop, p. 78–84, Geneva.
GAIFMAN H. (1961). Dependency systems and phrase structure systems. Report p-2315, RAND Corp.
Santa Monica (CA). Published in Information and Control, 1965, v. 8, n ? 3, pp. 304-337.
JOSHI A. K., SHANKER V. K. & WEIR D. J. (1991). The convergence of mildly context-sensitive
grammar formalisms. In P. SELLS, S. SHIEBER & T. WASOW, Eds., Foundational issues in natural
language processing, p. 31–81, Cambridge, MA : MIT Press.
KAHANE S., NASR A. & RAMBOW O. (1998). Pseudo-projectivity : A polynomially parsable non-
projective dependency grammar. In Proc. COLING-ACL, p. 646–652, Montreal.
KRUIJFF G.-J. M. (2001). A Categorial-Modal Logical Architecture of Informativity : Dependency
Grammar Logic & Information Structure. PhD thesis, Charles University, Prague.
LAMBEK J. (1961). On the calculus of syntactic types. In R. JAKOBSON, Ed., Structure of languages
and its mathematical aspects, p. 166–178. Providence RI : American Mathematical Society.
MOORTGAT M. (1997). Categorial type logics. In J. VAN BENTHEM & A. TER MEULEN, Eds.,
Handbook of Logic and Language, chapter 2, p. 93–177. Elsevier, The MIT Press.
MORRILL G. V. (1994). Type Logical Grammar. Categorial Logic of Signs. Kluwer.
POLLARD C. & SAG I. (1988). An Information Based Approach to Syntax and Semantics, Part I.
Stanford, California : CSLI.
PULMAN S. & RITCHIE G. (1985). Indexed grammars and interesting dependencies. UEA Papers in
Linguistics, 23, 21–38.
M. STEEDMAN, Ed. (1996). Surface Structure and Interpretation. The MIT Press.
VIJAY-SHANKER K. & WEIR D. (1994). The equivalence of four extensions of context-free grammars.
Mathematical Systems Theory, 27, 511–545.
174
