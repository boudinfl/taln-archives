TALN 2007, Toulouse, 5–8 juin 2007 
Analyse des échecs d’une approche pour traiter les questions 
 définitoires soumises à un système de questions/réponses
Laurent GILLARD, Patrice BELLOT, Marc EL-BÈZE
Laboratoire d'Informatique d'Avignon (LIA) 
Université d’Avignon et des Pays de Vaucluse 
339 ch. des Meinajaries, BP 1228 
F-84911 Avignon Cedex 9 (France) 
{laurent.gillard,patrice.bellot,marc.elbeze}@univ-avignon.fr
Résumé. Cet article revient sur le type particulier des questions définitoires étudiées dans 
le cadre des campagnes d’évaluation des systèmes de Questions/Réponses. Nous présentons 
l’approche développée suite à notre participation à la campagne EQueR et son évaluation lors 
de QA@CLEF 2006. La réponse proposée est la plus représentative des expressions présentes 
en apposition avec l’objet à définir, sa sélection est faite depuis des indices dérivés de ces 
appositions. Environ 80% de bonnes réponses sont trouvées sur les questions définitoires des 
volets francophones de CLEF. Les cas d’erreurs rencontrés sont analysés et discutés en détail. 
Abstract. This paper proposes an approach to deal with definitional question answering. 
Our system extracts answers to these questions from appositives appearing closed to the 
subject to define. Results are presented for CLEF campaigns. Next, failures are discussed. 
Mots-clés : système de questions/réponses, questions définitoires. 
Keywords: question answering, definitional question answering. 
1 Introduction
Les systèmes de Questions/Réponses (sQR) se proposent d’aller au delà de la recherche de 
documents pertinents afin de répondre, précisément et avec concision, à une question 
directement formulée en langue naturelle. L’étude de ces systèmes est encouragée par des 
campagnes d’évaluation qui spécifient des axes de recherche comme la nature des questions à 
considérer. Cet article s’intéresse tout particulièrement aux questions « définitoires » (QD), en 
domaine ouvert, telles que Qui était Alfred Nobel ? (CLEF06/28), qui interroge sur les aspects 
biographiques d’un individu, ou les questions Qu'est-ce que la RKA ? (CLEF06/95) et Qu'est-ce que 
Hubble ? (CLEF06/02), qui attendent une forme étendue ou encore une (voire La) caractéristique 
remarquable de l’objet à définir. Ces questions ont été introduites lors du volet 
Questions/Réponses (QR) de la campagne TREC-9. Il est à noter que dans ce qui suit, nous 
nous limitons au contexte des campagnes EQueR (Ayache et al., 2006) et plus spécifiquement 
CLEF (Vallin et al., 2006), où une seule et unique réponse est à produire. En effet, depuis 
2003 (Voorhees, 2003) et dans les campagnes TREC (Voorhees, 2005), les réponses 
attendues aux QD sont constituées de l’intégralité des faits pertinents connus sur le sujet à 
définir ; cela, au travers d’un découpage en « pépites » d’informations vitales (à maximiser), 
non vitales (indifférentes) et inintéressantes (à minimiser et pénalisantes). A Contrario, dans 
83
Laurent GILLARD, Patrice BELLOT, Marc EL-BÈZE
ce travail, notre objectif est d’extraire LA meilleure des réponses pour une QD ; et nous 
souhaitons y parvenir depuis la détection de mises en apposition et l’emploi d’un minimum de 
ressources. De plus, ce travail, préliminaire du point de vue de la tâche, nous permet 
d’explorer l’utilisation de la proximité immédiate des objets à définir avec leur définition. 
Une autre motivation provenait des faibles performances obtenues par notre système lors 
d’EQueR sur ces QD : seulement 7% de réponses courtes correctes pour les QD concernant 
des personnes et 42% pour les autres. Cela est d’autant peu qu’une partie était obtenue grâce à 
des bases de connaissances et par conséquent la projection de ressources exogènes (couteuses 
à maintenir). La méthode employée était basée sur un appariement entre un type de réponse 
attendu et la détection au sein des documents d’Entités du type adéquat. La principale 
difficulté rencontrée était liée à la l’identification des limites précises de l’énoncé 
correspondant à la définition. Pour illustrer cette détection parfois mal aisée, considérons une 
fonction (ou profession), qui débute pourtant par chef (une telle construction est assez 
fréquente en QR) : 
- Bouraima Koné, [Fonctionchef] des opérations techniques] de lutte d'urgence] contre les criquets] au 
ministère malien] de l'agriculture], au micro de Jean Paul Ade.  
(Lu en ligne, http://www2.dw-world.de/french/Politik_Afrika/1.173728.1.html)
Il apparaît, comme le signale les crochets fermants, que la frontière droite de l’expression est 
délicate à apprécier, et donc sujette à erreurs lors d’un étiquetage en Entités Nommées (ou 
même une extraction à l’aide de patrons). Par conséquent, le risque existe de faire glisser la 
réponse extraite vers une réponse incorrecte ou inexacte. Pourtant, cet exemple reste un cas 
simple : pour les QD qui commencent par Qui, la définition à trouver n’est pas 
systématiquement un rôle social mais peut être n’importe quelle raison pour laquelle une 
personnalité est connue (de telles QD sont parfois étiquetées « WhyFamous »). Et pour les 
Qu’est-ce que, les possibilités sont encore plus vastes. Aussi, notre approche a été de partir des 
expressions mises en apposition avec les objets à définir pour sélectionner celle qui semble la 
meilleure (selon différents critères et indices, qui sont présentés en section 3) et de la proposer 
comme réponse. L’utilisation d’une apposition permet de s’affranchir d’une détection plus 
hasardeuse, et puisqu’il s’agit d’un extrait du document, cela nous permet de supposer une 
réponse mieux construite. L’approche que nous présentons est évaluée dans le cadre de notre 
participation à la campagne QA@CLEF-2006 (Gillard et al., 2006) et obtient autour de 80% 
de bonnes réponses. Ensuite (section 4), les cas d’échecs que nous avons rencontrés sur 
CLEF-2006 sont analysés et discutés en détails afin de mettre en évidence des améliorations à 
mettre en œuvre ou des points importants à considérer pour traiter de telles questions. 
2 Autres travaux sur les questions définitoires 
Un travail similaire à celui-ci a été fait par (Malaisé et al., 2005) mais il porte sur la détection 
d’énoncé définitoires dans le domaine médical de la tâche spécialisée de la campagne EQueR. 
Étrangement, les constructions faisant intervenir des appositions ne sont pas listées dans les 
nombreux patrons d’extraction lexico-syntaxiques utilisés pour répondre aux questions. 
De nombreuses approches ont été envisagées, et pratiquement chaque système emploie une 
stratégie différente pour traiter les questions définitoires. Par exemple, (Greenwood, Saggion, 
2004) utilisent une étape préalable d’acquisition de termes secondaires depuis des ressources 
exogènes (WordNet, l’encyclopédie Britannica et le Web, ce dernier contribue d’ailleurs a 
78%) pour aider à sélectionner des définitions d’abord extraites avec l’aide de patrons. 
(Fleischman et al., 2003) centrent également leur travaux sur des patrons pour la collecte des 
réponses candidates, mais se limitent à deux : la succession Nom Commun Nom Propre et la 
84
Questions définitoires : analyses des échecs d’un système de questions/réponses 
mise en apposition ; ensuite un apprentissage automatique permet un filtrage. L’apposition est 
également l’un des patrons de (Hildebrandt et al., 2004) parmi 11 autres constructions pour 
extraire a priori des connaissances du corpus ; ils s’aident également d’une projection des 
définitions de dictionnaires en ligne. (Prager et al., 2001) utilisent les liens d’hyperonymie de 
WordNet pour localiser un passage contenant une réponse. D’autres, comme (Cui et al., 2005) 
proposent d’utiliser des patrons lexico-syntaxique probabilistes aux travers de deux modèles 
(bigrammes et PHMM). (Han et al., 2006) proposent un modèle purement probabiliste basé 
sur une séparation entre les modèles pour la question et la définition. 
3 Une approche et son évaluation pour les questions définitoires 
Comme dans tous les systèmes de Questions/Réponses, les questions définitoires sont d’abord 
identifiées comme telles puis classées suivant quatre catégories (au moyen de motifs 
d’expressions régulières) : D+Personne, pour les questions telles que Qui est Neil Armstrong ?
(CLEF06/51) ; D+Acronyme pour les questions telles que Qu'est-ce que l'OUA ? (CLEF06/48) ; 
D+minuscules pour celles comme Qu'est-ce que l'effet de serre ? (CLEF06/189) ; et enfin, ce qui 
constitue le choix par défaut, la catégorie D pour les questions définitoires qui n’entrent dans 
aucune des précédentes catégories comme par exemple Qu'est-ce que Challenger ? (CLEF06/81),
Qu'est-ce qu'Euro Disney ? (CLEF06/29) ou Qu'appelle-t-on le Knesset ? (CLEF06/103). Ensuite, l’objet à 
définir est obtenu en filtrant, après un étiquetage morphosyntaxique (TreeTagger), les 
différents pronoms interrogatifs et mots vides de sens comme être, appeler, acronyme, sigle,
etc. Puis toutes les phrases du corpus contenant l’objet à définir sont conservées et étiquetées 
morphosyntaxiquement. Si aucune phrase n’est trouvée la réponse NIL est retournée. 
Ce n’est qu’après ces différents prétraitements que les différentes expressions candidates sont 
extraites. Chacune d’entre elles est accompagnée de différents critères qu’il est possible de 
percevoir comme des juges dont les votes pondérés permettent de faire préférer in fine l’une 
plutôt que l’autre. La fusion de ces différents jugements est variable suivant l’appartenance de 
la question à l’une des quatre catégories initiales. Les réglages utilisés ont été obtenus de 
manière empirique sur les QR des campagnes CLEF-2004, 2005 et EQueR. 
Les expressions apposées à celle à définir et séparées d’elle par des virgules sont extraites et 
constituent l’ensemble préférentiel dans lequel la réponse devrait être extraite. Un critère 
correspondant à leur fréquence d’apparition dans l’ensemble des phrases leur est associé. Un 
autre ensemble plus particulièrement adapté aux acronymes et abréviations est défini et 
correspond à une construction Expression (CAPITALES) ou CAPITALES (Expression), par 
l’intermédiaire de deux extractions de ces Expressions : l’une est obtenue depuis un 
alignement dynamique entre la forme CAPITALES et l’Expression, l’autre depuis la partie 
gauche (respectivement droite) la plus redondante ; là encore, un critère de fréquence est 
associé avec chacune d’elles. D’autres juges sont définis : la présence en tête de l’expression 
du nom le plus fréquent à la position immédiatement à gauche ; de même, et après application 
d’un motif morphosyntaxique minimal pour détecter des groupes nominaux, la présence en 
tête de ce groupe nominal du nom principal déterminé le plus fréquent ; un taux de couverture 
avec le centroïde des noms les plus fréquents au sein d’une fenêtre de 10 mots autour de 
l’objet à définir ; un fonction de la longueur de l’expression ; un fonction du nombre de 
noms ; et deux autres juges binaires pour la présence des noms président ou société en tête 
d’expression. Enfin, depuis ces expressions, et une stratégie de fusion des provenances, 
catégories et juges, les meilleures sont proposées comme réponse avec un comportement par 
défaut qui consiste à répondre par : l’expression apposée qui est à la fois la plus longue et en 
adéquation avec le motif de détection des groupes nominaux, si elle existe, ou le nom le plus 
85
Laurent GILLARD, Patrice BELLOT, Marc EL-BÈZE
fréquent précédant l’objet à définir. Cette réponse par défaut est systématiquement proposée 
en position 5. 
Contexte et évaluation de la méthode : Le tableau 1 propose un référentiel pour l’évaluation 
de notre méthode au travers d’un bilan sur les résultats obtenus par l’ensemble des 
participants aux questions définitoires en français des 3 campagnes QA@CLEF passées, mais 
selon les ventilations que nous avons retenues. En 1ière colonne (#Q) est présenté le nombre de 
QD pour chacune des campagnes et catégories : 20 en 2004, 50 en 2005 et 42 en 2006. Le 
2ième groupe de colonnes correspond aux nombres et pourcentages de ces questions pour 
lesquelles une réponse correcte a été trouvée (RC) par au moins l’un des participants. Il 
ressort de cette colonne que l’ensemble des stratégies mises en œuvre par tous les systèmes 
permet, à partir de 2005, de s’approcher de la totalité des réponses à obtenir (94%). Enfin le 
dernier groupe (RC par soumission) permet de situer les performances des systèmes puisque 
figure le nombre de réponses correctes obtenues par : la/les moins bonnes des soumissions 
(Min.), la/les meilleures (Max.), ainsi que leur moyenne arithmétique (Moy.). Il est à noter que 
l’année 2004 est moins représentative puisqu’un seul système a participé. Également, si le (ou 
les) meilleur des systèmes dépasse 80% de bonnes réponses, la moyenne de ceux-ci se situe 
en deçà (34% et 50%). Les questions portant sur les mots/concepts les plus généraux 
(D+minuscules), et dont la réponse devrait être proche d’une définition du type dictionnaire, 
rencontrent une réussite moindre, il est possible d’envisager deux raisons à cela : le fait que 
les corpus journalistiques employées se prêtent peu à ce type d’extraction (contrairement à 
des d’informations plus biographiques), ou tout simplement leur relative nouveauté en QR. 
Tableau 1: Bilan sur les réponses correctes(RC) proposées par les participants 
aux questions définitoires des campagnes QA@CLEF. 
Le tableau 2 présente les résultats obtenus par notre méthode sur les mêmes jeux de QD que 
le tableau 1. Les questions des campagnes CLEF 2004, 2005 et EQueR (non présentées) ont 
été utilisées pour raffiner la méthode mise au point après notre participation à EQueR. 
L’évaluation de ces résultats a été faite manuellement. Il en est de même pour la dernière 
colonne (Au moins une réponse correcte dans les 5 premières réponses). En revanche pour les 
autres données de CLEF 2006, les résultats présentés sont ceux obtenus lors de notre 
participation à la campagne au volet Français-Français (les résultats en Anglais-Français sont 
inférieurs en raison d’erreurs de traduction ; 67% de réponses correctes sont trouvées au rang 
1 au lieu de 79%). Les (+1) et (+2) qui figurent dans le tableau correspondent à des réponses 
qui auraient dû être extraites mais qui ont été perdues à cause de problèmes d’ingénierie, ou 
pour l’une de la ligne D+Personne en raison d’une incertitude que nous avons : est-il 
86
Questions définitoires : analyses des échecs d’un système de questions/réponses 
acceptable de définir Boris Becker comme une tête de série nº7 ? (cf. discussion en section suivante).
Il apparaît de ce tableau que les taux de bonnes réponses sont constants et aux alentours de 
80% au premier rang et dépassent 83% pour une réponse correcte placée parmi les 5 
premières. Cependant, et comme précédemment souligné par le tableau 1, les questions 
définitoires portant sur des concepts génériques (D+minuscules) ou qui ne concerne ni les 
personnes ni les acronymes, rencontrent également moins de succès avec notre méthode. 
Tableau 2: Résultats obtenus par notre méthode sur les questions définitoires
des campagnes QA@CLEF ; évaluation officielle pour 2006 (soumission FR-FR). 
4 Analyse et discussion des cas d’échecs sur CLEF-2006 
Cette partie propose une discussion sur les cas d’échecs que nous avons rencontrés. Aussi, 
elle s’articule autour de quelques questions qui illustrent des difficultés représentatives pour 
notre système et parfois la tâche elle-même. Il faut également rappeler que notre méthode 
n’utilise pas d’analyse syntaxique et repose essentiellement sur une recherche d’expressions 
apposées depuis un découpage en phrases, et, par conséquent, avec un contexte limité. 
Qui est Boris Becker ? (CLEF2006/90) Au delà de la simplicité évidente de cette question pour un 
« lecteur moyen de journaux », la difficulté est réelle puisqu’aucun des systèmes n’a trouvé 
une réponse correcte. En effet, la réponse retournée par notre système est une nationalité au 
travers du nom Allemand soit le nom qui qualifie le plus fréquemment Boris Becker dans le 
corpus (l’Allemand Boris Becker est présent 21 fois sur les 104 occurrences de Boris Becker).
Cette réponse n’a pas été jugée correcte, la raison étant qu’une nationalité n’est pas suffisante, 
à elle seule, pour qualifier convenablement une personne. Cette règle connue, il apparaît 
possible, notamment dans ce cas simple, de filtrer les « mauvais » candidats à l’aide de règles 
de rejet. Aussi, il faut poursuivre l’investigation. L’une des premières définitions qui viendrait 
à l’esprit pour définir Boris Becker serait très probablement sa qualité de joueur de tennis. Mais, 
au sein d’un découpage en phrase du corpus, Boris Becker entre que très rarement en 
cooccurrence avec un motif comprenant tennis (qu’il soit issu de l’expression joueur de tennis
ou même tennism(a|e)n). Et, sur les neuf fois où cela se produit sur les 104 phrases contenant 
Boris Becker, une seule permet effectivement de le définir directement au travers de ce trait : 
- Sa fortune , il l' a bâtie sur les courts de tennis aux côtés de son partenaire de double , le fantasque 
Ilie Nastase , dans les années 60 ; puis en gérant les affaires des meilleurs tennismen , tels que l' 
Allemand Boris Becker hier ou le Croate Goran Ivanisevic aujourd'hui . (LEMONDE94-000881-19940108)
Cependant, malgré la présence du tels introductif à une explicitation, il faut souligner ici à 
quel point le processus de réponse apparaît délicat : il est nécessaire de ne pas tenir compte de 
87
Laurent GILLARD, Patrice BELLOT, Marc EL-BÈZE
la présence du nom Allemand pour revenir en arrière jusqu’à celui de tennismen. En outre, une 
inversion de la position des deux joueurs Goran Ivanisevic et Boris Becker dans la phrase aurait 
encore complexifié son analyse et aurait nécessité la mise en balance des deux groupes 
nominaux par la conjonction ou. Et dans ce dernier cas, la tâche aurait été compliquée par la 
présence des mots hier et aujourd’hui puisqu’ils apparaissent comme des éléments 
perturbateurs difficiles à prévoir (notamment dans le cas d’une extraction à partir de patrons 
morphosyntaxiques) mais pourtant à ignorer. Enfin, il faut se remémorer qu’il s’agit de 
l’unique cooccurrence entre tennismen et Boris Becker dans le corpus et par conséquent une 
prise en compte fréquentielle n’est pas envisageable dans ce cas (cependant une parenthèse 
mérite d’être ouverte : des procédés de résolutions d’anaphores pourraient augmenter le 
nombre de candidats mais notre système en est actuellement dépourvu). 
Cette (probable) bonne réponse étant écartée, il est possible de s’intéresser à une autre  
réponse candidate ou plutôt un autre lot de réponses envisageables. En effet, Boris Becker est à 
plusieurs reprises qualifié de tête de série comme explicité dans les exemples ci-dessous : 
- Victime d' une blessure au dos à l' échauffement , l' Allemand Boris Becker , tête de série numéro 10 
, a déclaré forfait avant le match contre Stark (LEMONDE94-002991-19940525)
- Pete Sampras a gagné , dimanche 15 mai , les Internationaux d' Italie de tennis en battant en finale l' 
Allemand Boris Becker , tête de série n 8 ( 6-1 , 6-2 , 6-2 ) . (LEMONDE94-001994-19940517)
- Sur le court nº 1 , dos-à-dos à deux sets partout , l' Allemand Boris Becker , tête de série nº 7 , et l' 
Ukrainien Andrei Medvedev ( nº 9 ) ont vu leur rencontre interrompue […]  (LEMONDE94-003405-19940629)
Ainsi, une première difficulté survient dans le cas d’une éventuelle factorisation fréquentielle 
sur tête de série : l’expression n’est pas suffisante si le qualifié n’est pas tête de série numéro 1.
Aussi, il peut être nécessaire de la complémenter. Dans ce cas, cela suppose d’être en mesure 
de prendre en compte les différentes écritures de numéro. Pourtant cela serait une erreur de 
penser qu’il s’agit d’un même classement comparable et qu’il est possible de suivre sa 
variation dans le temps au travers des différents documents (auquel cas, une décision aurait pu 
être de ne considérer que le plus récent). En effet, une tête de série correspond à un classement 
préalable à un tournoi, sorte d’estimation faite en fonction du niveau d’un participant, pour 
faire en sorte que les meilleurs d’entre eux ne se rencontrent qu’à la fin de la compétition. 
Aussi la notion de tête de série n’a de sens que vis-à-vis d’une compétition sportive. 
Cependant, comme il est possible de le voir sur ces exemples, ces références ne sont pas 
toujours présentes dans la fenêtre de la phrase : seul le deuxième exemple propose à la fois la 
compétition et le domaine au travers des Internationaux d'Italie de tennis. Aussi, et finalement, 
l’ensemble de ces ambiguïtés liées à autant d’élisions peut diminuer considérablement 
l’intérêt d’une réponse extraite de ces phrases (sauf à pouvoir synthétiser une réponse telle 
que tête de série n°8 aux Internationaux d’Italie de tennis mais dans ce cas il serait probablement 
préférable d’aller jusqu’à demi-finaliste aux Internationaux d’Italie de tennis en 1994).
Il est d’ailleurs à noter qu’une partie de cette discussion eut été différente si plutôt que tête de 
série n°X, l’expression n°X mondial avait été présente, puisque alors il aurait fallu prendre 
justement en compte une variabilité dans le temps de ce classement parmi les meilleurs 
joueurs mondiaux. Et de s’interroger sur l’opportunité de qualifier Boris Becker avec autant de 
classements différents malgré un dénominateur commun d’être « l’un des 10 premiers joueurs 
mondiaux de tennis en 1994 » (d’ailleurs, n’est-ce pas la réponse, actuellement hors de 
portée, qu’il aurait fallu pouvoir inférer de ces différentes réalisations ?). 
Après avoir considéré ces candidats de réponses propulsés en tête des possibles en raison de 
leur répétition, il apparaît parmi les appositions restantes quelques autres susceptibles de 
donner lieu à des réponses correctes. Cependant, dans trois de celles-ci, le rapprochement 
88
Questions définitoires : analyses des échecs d’un système de questions/réponses 
avec le monde du tennis n’est pas présent, ce qui par conséquent amènera à présupposer cette 
connaissance (qui peut ne pas aller de soi). Il est aussi intéressant de noter que trois d’entres 
elles commencent par des adjectifs numéraux ou multiplicateurs (peut-être faut-il y voir une 
particularité de l’univers sportif ?). Enfin, et comme pour étayer la discussion passée, une 
erreur de typographie peut compliquer les rapprochements des tête de sréie numéro 3. Tout 
comme il apparaît délicat de décider si Boris Becker est huitième joueur mondial ou bien no 3 
mondial (respectivement dans un document de 1994 et 1995, aussi a-t-il été successivement, 
l’un puis l’autre ; du moins au moment de l’écriture de chacun de ces documents). 
- En déclarant , […] sans toutefois apporter de preuves , l' Allemand Boris Becker , triple champion 
de Wimbledon , a relancé la rumeur autour du dopage dans le monde du tennis (LEMONDE94-000256-19940104)
- L' Allemand Boris Becker , huitième joueur mondial et tête de sréie numéro 3 , s' est imposé en finale 
du tournoi de New-Haven ( Connecticut ) en battant […] (LEMONDE94-001895-19940823)
- La fédération accéderait ainsi aux exigences de Stich , qui réclame une somme identique à celle 
accordée à son compatriote Boris Becker , no 3 mondial , soit 1 million […]. (LEMONDE95-010221)
- Boris Becker , multiple vainqueur dans les autres tournois du Grand Chelem , a fait une nouvelle 
fois son deuil des Internationaux de France . (LEMONDE95-022102)
Enfin, il est possible de remarquer que, depuis ce dernier exemple, et puisque la réponse est à 
fournir hors du contexte du document sans doute serait-il préférable d’être capable de perdre 
le autres pour ne conserver que multiple vainqueur dans les tournois du Grand Chelem.
Qui était Alexander Graham Bell ? (CLEF2006/0050) C’est un problème d’ingénierie lié aux 
nombreuses ponctuations qui a empêché notre système de répondre à cette question depuis : 
- Parmi les lieux historiques , le canal de St-Peters ( entre le lac du Bras-d' Or et l' Atlantique ) , le 
site dédié à l' inventeur du téléphone , Alexander Graham Bell ( à Baddeck , à l' ouest de Sydney ) , 
Louisbourg ( lire notre reportage ) , la citadelle de Halifax ( fortifications du XIXe ) , Port-Royal , à 
210 km à l' ouest de Halifax […] . (LEMONDE95-023507)
Mais cette question nous permet d’illustrer une autre difficulté qu’il ne faut pas oublier de 
considérer lors des étapes de recherche ou d’appariement, qu’il s’agisse de la question, des 
documents voire de l’un avec l’autre. En effet, il faut autoriser une certaine variabilité dans 
les noms propres afin de s’assurer qu’Alexandre et Alexander soit une même personne, qu’un 
éventuel nom intermédiaire, deuxième prénom, etc. puisse également être facultatif ou 
présent. Cela pour qu’Arantxa Sanchez Vicario (CLEF05/175) puisse s’écrire Arantxa Sanchez-Vicario
mais n’être parfois qu’Arantxa Sanchez. S’il en doit en être de même pour Bill, William
Jefferson, W.J. ou même William J. Clinton, force est de constater que dans ce cas la solution 
n’apparaît pas triviale (quelle forme canonique pour les noms propres ?). Enfin, dans certains 
cas, plus chanceux, si l’écriture fautive apparaît à la fois dans la question et les documents, le 
système peut établir qu’Hil(l)ary Clinton (CLEF06/120) est tout de même l’épouse du président 
américain. Néanmoins, il est aussi possible d’utiliser des algorithmes de tolérance aux fautes. 
- Un compromis […]sans délai en échange de la confirmation par Washington de la présence de 
Hilary Clinton , l' épouse du président américain , à la conférence […]. (LEMONDE95-031398)
Qu’est-ce que le Crédit Suisse ? (CLEF2006/0107) Cette question est particulièrement intéressante : 
comment peut-on définir quelque chose qui apparaît comme déjà transparent ? Notre système 
n’y est pas parvenu et s’y est même trompé : le groupe Crédit Suisse est effectivement un 
groupe (et à de nombreuses reprises), mais ce n’est pas très satisfaisant et si c’est le seul
groupe, c’est déjà trop. En effet, il faut prendre garde à certains adjectifs qui nuisent plus 
qu’ils n’apportent. Nous avions envisagé ce problème mais oublié de l’implémenter : s’il est 
intéressant de savoir que Stephen Hawking (CLEF06/0027) est un célèbre physicien anglais, ou que 
89
Laurent GILLARD, Patrice BELLOT, Marc EL-BÈZE
Nick Leeson (CLEF06/0041) est un ancien courtier de la banque Barings ; d’autres adjectifs tels nouveau
doivent être soigneusement évités (et à plus forte raison pour un corpus ancien). Ainsi notre 
système aurait pu filtrer Windows (CLEF06/107), le nouveau système d' exploitation jugé inexact (mais 
aussi parce qu’il était question de l’une des versions de Windows dans le document). En outre, 
et pour revenir au cas Crédit Suisse, les systèmes participants n’ont pu faire mieux que de 
répondre groupe qui a été la seule bonne réponse acceptée, à deux reprises, lors de 
l’évaluation (mais il reste possible de s’interroger sur l’intérêt de cette définition, surtout 
lorsque Prix de Le Prix Crédit Suisse est refusé, peut être parce qu’il s’agit d’une « copie » ou
une « imitation »).
Il est à noter que notre système propose en 4ième position, banque qui a placé les emprunts Biber,
depuis :
- Ces créanciers spéculent […] hausse des cours , a expliqué Dieter Enkelmann , membre de la 
direction du Crédit Suisse , banque [est-ce à conserver ? qui a placé les emprunts Biber] . (ATS.940426.0097)
mais la fréquence banque (7) n’a pu prendre le pas sur celle de groupe (31). Un autre point est 
qu’il peut apparaître opportun (d’ailleurs plus par conformité avec les spécifications des 
réponses à produire dans le cadre des campagnes d’évaluations CLEF que du point de vue de 
la pertinence de la « pépite » d’information elle-même) d’éloigner la proposition subordonnée 
relative pour ne conserver que banque. Et encore une fois, dans le cas de Windows (CLEF06/107) : 
- Microsoft Network […] que la prochaine version de Windows , le système d' exploitation
[est-ce à conserver ? qui a permis au groupe d' asseoir sa domination du marché des logiciels ] (ATS.950516.0148)
Pour conclure avec l’institution financière, dans l’exemple ci-dessous, il ne faut pas extraire 
trop rapidement banque bâloise puisqu’il s’agit en fait de la Société de Banque Suisse ( SBS ) et
non du Crédit Suisse, l’une des grandes concurrentes du SBS. Mais la compréhension nécessaire 
n’apparait pas dans la fenêtre de la phrase.
- A l' instar de ses deux grandes concurrentes , l' UBS et le Crédit Suisse , la banque bâloise a subi le 
contre-coup des turbulences qui ont affecté l' an passé les marchés financiers . (ATS.950315.0084)
Qu’est-ce qu'un samovar ? (CLEF2006/0188) Cette question cristallise les difficultés. Dans le corpus, 
il n’est pas possible d’obtenir la définition à laquelle on s’attend. Pourtant, c’est justement 
grâce à une apposition qu’il est envisageable de répondre depuis l’une des 9 phrases utilisant 
le mot (les 8 autres sont susceptibles de brouiller les pistes). Mais l’analyse pour aboutir 
apparaît particulièrement complexe : il faut considérer la seconde occurrence du mot plutôt 
que la première (laquelle seconde est à ignorer sinon), passer outre la forme plurielle, les 
guillemets fermants, une partie de la première apposition, pour s’arrêter après la troisième, et 
non plus loin. Ensuite, un samovar peut être ceux qui, au front, avaient perdu bras ou jambe.
Aucun des systèmes participants n’y est parvenu. Et finalement, cette question pose un autre 
problème sous-jacent à toute la tâche Questions/Réponses : est-il toujours possible d’extraire 
automatiquement une réponse concise depuis un passage qui la contient manifestement ? 
- Près du [1samovar] , ou entourée de " [2samovars] " , comme on appelait ceux qui , au front , 
avaient perdu bras ou jambe , la voilà soudain loin de la littérature , se souvenant […](LEMONDE95-036764)
Qu’est-ce qu'un t-shirt ? (CLEF2006/0144) Cette question a donné lieu à une réponse vestimentaire 
sans intérêt liée à une énumération : veste et cravate. La réponse attendue était NIL, soit celle 
correspondant à une absence de réponse dans le corpus. Cette bonne absence de réponse a été 
proposée par 5 systèmes, dont le nôtre mais uniquement dans sa version anglais vers français 
et cela, seulement à cause d’une erreur de traduction. En effet, notre approche tend à toujours 
90
Questions définitoires : analyses des échecs d’un système de questions/réponses 
proposer une réponse (par défaut l’expression apposée la plus longue ou le nom le plus 
fréquent précédant l’objet de la question). Le seul cas où un NIL est retourné survient lorsque 
l’expression à définir est absente du corpus (comme ce fut le cas à raison pour Linux (CLEF06/03),
pas encore assez populaire en 1994 et 95, dates des documents de la campagne). 
- " Casquette de base-ball , jeans , T-shirt , veste et cravate , tenue de jogging : il est impossible de 
dresser un portrait-robot de ces candidats à une arme . (LEMONDE95-028295)
5 Perspectives : vers une synthèse de définitions 
Un tel processus de réponse à des questions définitoires ouvre une perspective intéressante : il 
permet d’améliorer la notion de satisfaction en s’essayant à une génération ou plutôt à une 
synthèse pour la réponse proposée. En effet, les réponses extraites depuis un contexte unique 
ne sont que très rarement exhaustives (surtout s’il est limité à quelques phrases). Pourtant, et à 
leur lecture, il apparaît évident qu’il est possible d’améliorer LA réponse grâce à l’ensemble 
des réponses candidates (mais il est alors nécessaire de s’assurer de la qualité de ces réponses 
candidates, par exemple, depuis des critères fréquentiels ou des coefficients d’association). 
Ainsi, lorsque nous cherchons à définir Airbus (CLEF06/53), le nom le plus fréquemment associé 
est Consortium et parmi les expressions les plus fréquentes sont Consortium européen,
Consortium civil, et Consortium aéronautique (Consortium, Programme et Consortium européen ont
étés présentées par les systèmes et jugées correctes lors de l’évaluation). D’autre part 
Consortium aéronautique européen apparaît dans le corpus mais épisodiquement. Toutefois, 
force est de constater que ce dernier semble plus satisfaisant (même si sa fréquence moindre 
le rend peu sujet à extraction). En outre, il peut être synthétisé depuis les premiers. Il suffit de 
complémenter le nom le plus fréquemment apposé par tous les adjectifs épithètes qui 
l’accompagnent dans ses réalisations. Un simple étiquetage morphosyntaxique est suffisant. 
Enfin, une projection (sur le web ou dans le corpus) des expressions ainsi créées peut 
permettre de vérifier leur validité du point de vue de leur construction et, notamment, dans ce 
cas, fixer l’ordre des adjectifs (sauf à définir des règles : une nationalité apparaît souvent en 
dernier). Il en est de même pour le classique Bill Clinton (CLEF06/91) tour à tour président américain
et président démocrate mais pourtant les deux à la fois ou des navettes Atlantis (CLEF06/01) et
Challenger (CLEF06/81), avant tout spatiales et américaines. Par ailleurs, il peut être opportun de 
disposer d’une ressource, même limitée, afin, par exemple, de manipuler comme un même 
concept des noms tels que Chef et Leader (certaines fonctions reviennent particulièrement dans 
les QD de ces campagnes), et/ou éviter une éventuelle redite des adjectifs. 
Idéalement une telle méthode pourrait être appliquée avec des subordonnées relatives mais 
ces « pépites » d’informations seraient probablement jugées comme inexactes dans le 
contexte actuel des campagnes, tout en étant susceptibles d’induire plus d’erreurs durant leur 
génération. Cependant, cela permettrait d’apporter une réponse concise adéquate à la question 
(fictive) Qu’est-ce que Bell’s Beach ?, depuis le seul passage du corpus (EQueR) contenant 
l’expression Bell’s Beach, puisqu’elle serait alors (la|une) plage où éclatent les plus grosses 
vagues d'Australie. Il est à noter qu’ici, le procédé de réponse implique une mise en relation 
entre Beach et plage puis d’utiliser ce qui joue le rôle d’un générique comme antécédent de la 
proposition relative. En d’autres termes, cela revient à complémenter une sorte de 
classe/hyperonyme par la proposition relative initialement apposée à l’expression à définir. 
C' est du côté de Torquay , petite ville côtière du Victoria au sud -est du pays , située à quelques 
kilomètres de Bell' s Beach , où éclatent les plus grosses vagues d' Australie , que […] (LEMONDE99-19935)
91
Laurent GILLARD, Patrice BELLOT, Marc EL-BÈZE
6 Conclusion
Dans cet article nous avons étudié l’utilisation de l’apposition pour répondre à des questions 
définitoires telles qu’elles sont proposées dans les campagnes d’évaluation des systèmes de 
Questions/Réponses. En effet, dans notre système, les réponses candidates pour ces questions 
sont extraites depuis leur mise en apposition (ou entre parenthèses) avec les objets à définir. 
Ensuite, afin de filtrer et retenir la meilleure des expressions apposées comme réponse, un 
choix est effectué depuis une stratégie impliquant différents indices (principalement 
fréquentiels) dérivés du voisinage des objets à définir. L’hypothèse forte de notre approche 
est qu’elle ne nécessite pas de connaissances externes ni même une analyse syntaxique. 
Évaluée dans le cadre d’une participation à la campagne QA@CLEF-2006, elle trouve 
environ 80% de bonnes réponses. Cependant une analyse détaillée de quelques cas d’échecs 
rencontrés nous a amené au constat qu’il n’est pas toujours possible d’aboutir à une réponse. 
Enfin, et parce que les réponses aux questions définitoires s’y prêtent particulièrement, nous 
avons esquissé une perspective quant à la synthèse de réponses (depuis les meilleures 
réponses extraites) pour aller plus avant vers des réponses plus satisfaisantes. 
Références
AYACHE C., GRAU B., VILNAT A. (2006). EQueR: The French Evaluation campaign of 
Question Answering Systems. Actes de LREC’2006.
CUI H., KAN M.-Y., CHUA T.-S. (2005). Generic  Soft Pattern Models for Definitional 
Question Answering. Actes de The 28th ACM SIGIR Conference, 384-391. 
FLEISCHMAN M., HOVY E., ECHIHABI A. (2003). Offline Strategies for Online Question 
Answering: Answering Questions Before they Are Asked. Actes de ACL-2003, 1-7. 
GILLARD L., SITBON L., BLAUDEZ E., BELLOT P., EL-BÈZE M. (2006). The LIA at 
QA@CLEF2006. The Working Notes for the CLEF 2006 Workshop.
GREENWOOD M.A., SAGGION H. (2004). A Pattern Based Approach to Answering Factoid, 
List and Definition Questions. Actes de The 7th RIAO Conference, 232-243. 
HAN K.-S., SONG Y.-I., RIM H.-C. (2006). Probabilistic model for definitional question 
answering. Actes de The 29th ACM SIGIR Conference, 212-219. 
HILDEBRANDT W., KATZ B., LIN J. (2004). Answering Definition Questions Using Multiple 
Knowledge Sources. Actes de HLT-NAACL 2004, 49-56. 
MALAISÉ V., DELBECQUE T., ZWEIGENBAUM P. (2005). Recherche en corpus de réponses à 
des questions définitoires. Actes de la Conférence TALN 2005, 43-52. 
PRAGER J., RADEV D., CZUBA K. (2001). Answering What-Is Questions by Virtual 
Annotation. Actes de HLT-2001 Conference, 26-30. 
VALLIN A., MAGNINI B., GIAMPICCOLO D., AUNIMO L., AYACHE C., OSENOVA P., PEÑAS A.,
DE RIJKE M., SACALEANU B., SANTOS D., SUTCLIFFE R. (2006). Overview of the CLEF 2006 
Multilingual Question Answering Track. The Working Notes for the CLEF 2006 Workshop.
VOORHEES E.M. (2003). Overview of the TREC 2003 Question Answering track, Actes de 
The 12th TREC Conference, 54-68. 
VOORHEES E.M. (2005). Chapter 10: Question Answering in TREC. Dans VOORHEES E. M.,
HARMAN D. (éd.): TREC Experiment and Evaluation in Information Retrieval. 233-257. 
92
