<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
"http://www.w3.org/TR/html4/loose.dtd">
<html><head><title>Syst&#232;mes de questions-r&#233;ponses : vers la validation automatique des r&#233;ponses</title>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
</head>
<body>
<div style="page-break-before:always; page-break-after:always"><div><p>TALN 2007, Toulouse, 5&#8211;8 juin 2007
</p>
<p>Syst&#232;mes de questions-r&#233;ponses : vers la validation
automatique des r&#233;ponses
</p>
<p>Anne-Laure LIGOZAT, Brigitte GRAU, Isabelle ROBBA, Anne VILNAT
LIMSI-CNRS - BP 133, 91403 Orsay Cedex
</p>
<p>prenom.nom@limsi.fr
</p>
<p>R&#233;sum&#233;. Les syst&#232;mes de questions-r&#233;ponses (SQR) ont pour but de trouver une infor-
mation pr&#233;cise extraite d&#8217;une grande collection de documents comme le Web. Afin de pouvoir
comparer les diff&#233;rentes strat&#233;gies possibles pour trouver une telle information, il est important
d&#8217;&#233;valuer ces syst&#232;mes. L&#8217;objectif d&#8217;une t&#226;che de validation de r&#233;ponses est d&#8217;estimer si une
r&#233;ponse donn&#233;e par un SQR est correcte ou non, en fonction du passage de texte donn&#233; comme
justification. En 2006, nous avons particip&#233; &#224; une t&#226;che de validation de r&#233;ponses, et dans cet
article nous pr&#233;sentons la strat&#233;gie que nous avons utilis&#233;e. Celle-ci est fond&#233;e sur notre propre
syst&#232;me de questions-r&#233;ponses. Le principe est de comparer nos r&#233;ponses avec les r&#233;ponses &#224;
valider. Nous pr&#233;sentons les r&#233;sultats obtenus et montrons les extensions possibles. &#192; partir de
quelques exemples, nous soulignons les difficult&#233;s que pose cette t&#226;che.
</p>
<p>Abstract. Question answering aims at retrieving precise information from a large collec-
tion of documents, typically the Web. Different techniques can be used to find relevant informa-
tion, and to compare these techniques, it is important to evaluate question answering systems.
The objective of an Answer Validation task is to estimate the correctness of an answer returned
by a QA system for a question, according to the text snippet given to support it. We participated
in such a task in 2006. In this article, we present our strategy for deciding if the snippets justify
the answers. We used a strategy based on our own question answering system, and compared
the answers it returned with the answer to judge. We discuss our results, and show the possible
extensions of our strategy. Then we point out the difficulties of this task, by examining different
examples.
</p>
<p>Mots-cl&#233;s : syst&#232;mes de questions-r&#233;ponses, validation de r&#233;ponses.
</p>
<p>Keywords: question answering, answer validation.
</p>
<p>1 Introduction
</p>
<p>Les syst&#232;mes de questions-r&#233;ponses (SQR par la suite) ont pour but de trouver une information
pr&#233;cise dans une grande collection de documents. L&#8217;hypoth&#232;se sous-jacente au d&#233;veloppement
de tels syst&#232;mes est que les utilisateurs pr&#233;f&#232;rent en g&#233;n&#233;ral recevoir une r&#233;ponse pr&#233;cise &#224;
la question qu&#8217;ils se posent plut&#244;t qu&#8217;un ensemble de documents &#224; explorer, comme le pro-
posent habituellement les moteurs de recherche (Voorhees, 1999). Cependant, pour &#234;tre consi-
d&#233;r&#233; comme fiable par un utilisateur, un SQR doit &#234;tre capable de donner des &#233;l&#233;ments permet-
tant d&#8217;&#233;valuer ses r&#233;ponses. L&#8217;objectif d&#8217;un syst&#232;me ne doit donc pas seulement &#234;tre de trouver
</p>
<p>173</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Anne-Laure LIGOZAT, Brigitte GRAU, Isabelle ROBBA, Anne VILNAT
</p>
<p>les r&#233;ponses, mais aussi de les exprimer d&#8217;une fa&#231;on qui permette &#224; l&#8217;utilisateur de savoir s&#8217;il
peut avoir confiance en ces r&#233;ponses. Ces &#233;l&#233;ments de justification donnent &#224; l&#8217;utilisateur un
moyen de v&#233;rifier que la r&#233;ponse fournie correspond bien &#224; l&#8217;information qu&#8217;il cherche, et ainsi
de donner une valeur de v&#233;rit&#233; &#224; cette r&#233;ponse, en supposant que l&#8217;utilisateur a des connaissances
&#171; standard &#187;.
</p>
<p>Une bonne justification doit &#234;tre concise et compl&#232;te. Le but est de ne fournir que les extraits
de documents qui permettent &#224; l&#8217;utilisateur de retrouver toutes les informations qu&#8217;il a donn&#233;es,
sans avoir &#224; lire un document entier. Voici un exemple d&#8217;une telle justification.
</p>
<p>Question : Quand a eu lieu la chute du mur de Berlin ?
</p>
<p>R&#233;ponse : en 1989
Justification (passage d&#8217;un document) : Cette &#232;re de la dissuasion, fond&#233;e sur l&#8217;&#233;qui-
libre de la terreur entre deux grands blocs antagonistes, est remise en question en
1989, avec la chute symbolique du mur de Berlin.
</p>
<p>2 Validation de r&#233;ponses
</p>
<p>(Lin &amp; Pantel, 2001) soulignent la possible distance linguistique entre les questions et leurs
r&#233;ponses accompagn&#233;es de leur justification, en prenant l&#8217;exemple de la phrase &#171; Stendhal a
&#233;crit &#8217;La chartreuse de Parme&#8217; en 1838 &#187; justifiant la r&#233;ponse &#171; Stendhal &#187; &#224; la question &#171; Qui
est l&#8217;auteur de &#8217;La chartreuse de Parme&#8217; ? &#187;. Ils d&#233;finissent les liens entre une question et sa
r&#233;ponse justifi&#233;e par le terme d&#8217;inf&#233;rence. Ils proposent alors de d&#233;finir des r&#232;gles d&#8217;inf&#233;rence
pour reconna&#238;tre par exemple la relation entre &#171; X a &#233;crit Y &#187; et &#171; X est l&#8217;auteur de Y &#187;. Ces
r&#232;gles correspondent plus ou moins &#224; ce qui est appel&#233; paraphrases ou variantes dans d&#8217;autres
travaux (Jones &amp; Tait, 1984; Fabre &amp; Jacquemin, 2000).
</p>
<p>Le lien entre question et r&#233;ponses correspond &#224; la notion de textual entailment telle qu&#8217;elle est
d&#233;finie par Pascal Recognizing Textual Entailement Challenge 1 (RTE). L&#8217;implication textuelle
est d&#233;finie comme une t&#226;che de d&#233;cision qui &#224; partir de deux fragments de texte, estime si d&#8217;un
point de vue s&#233;mantique on peut d&#233;duire l&#8217;un de l&#8217;autre. Ainsi le passage de texte suivant (appel&#233;
justification) : &#171; Yoko Ono a inaugur&#233; une statue de bronze repr&#233;sentant son mari d&#233;c&#233;d&#233;,
John Lennon, pour compl&#233;ter le changement de nom officiel de l&#8217;a&#233;roport de Liverpool qui
devient l&#8217;a&#233;roport John Lennon de Liverpool &#187; implique la phrase &#171; Yoko Ono est la veuve de
John Lennon &#187; (appel&#233;e hypoth&#232;se dans le contexte de l&#8217;implication textuelle). Dans RTE, les
participants re&#231;oivent des paires justification-hypoth&#232;se de ce type et doivent ensuite d&#233;cider si
les hypoth&#232;ses peuven ! t ou non &#234;tre d&#233;duites des justification. Cette t&#226;che est similaire &#224; la
t&#226;che de r&#233;ponses aux questions en ce qui concerne les questions bool&#233;ennes (attendant oui ou
non en r&#233;ponse), car r&#233;pondre &#224; ces questions revient en fait &#224; d&#233;cider si la justification de la
r&#233;ponse implique la r&#233;ponse.
</p>
<p>En 2006, un nouvel exercice de validation des r&#233;ponses, AVE 2, a &#233;t&#233; introduit dans la cam-
pagne de questions-r&#233;ponses de CLEF. Le but de cet exercice est d&#8217;une part d&#8217;am&#233;liorer les
performances des SQR, en d&#233;veloppant des m&#233;thodes automatiques d&#8217;&#233;valuation des r&#233;ponses,
et d&#8217;autre part de rendre le jugement humain semi-automatique &#224; la condition que l&#8217;exercice
produise des m&#233;thodes fiables d&#8217;&#233;valuation. Pour cet exercice, les organisateurs ont produit un
</p>
<p>1http://www.pascal-network.org/Challenges/RTE
2Answer Validation Exercise, http://nlp.uned.es/QA/AVE/
</p>
<p>174</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Validation automatique de r&#233;ponses
</p>
<p>corpus &#224; partir des r&#233;ponses des participants &#224; la t&#226;che de questions-r&#233;ponses et des passages
de texte donn&#233;s comme justification. Les participants avaient alors pour t&#226;che de d&#233;cider pour
chaque r&#233;ponse si elle &#233;tait correcte ou non en fonction du passage justificatif.
</p>
<p>Les premiers travaux de validation automatique de r&#233;ponses ont eu lieu au cours de la campagne
AVE en 2006 ; cependant, les campagnes d&#8217;implication textuelle RTE avaient d&#233;j&#224; propos&#233; ce
type de t&#226;che.
</p>
<p>Voici un exemple de couple (hypoth&#232;se, justification) d&#8217;AVE :
</p>
<p>Hypoth&#232;se : Yasser Arafat &#233;tait leader de l&#8217;Organisation de Lib&#233;ration de la
Palestine 3
</p>
<p>Justification : Le pr&#233;sident Clinton a fait appel personnellement au leader de l&#8217;Or-
ganisation de Lib&#233;ration de la Palestine Yasser Arafat et aux Palestiniens mer-
credi pour qu&#8217;ils reprennent les pourparlers en faveur de la paix avec Isra&#235;l
</p>
<p>Ici l&#8217;hypoth&#232;se est une reformulation de la question &#171; Qui &#233;tait Yasser Arafat ? &#187; dans laquelle
a &#233;t&#233; ins&#233;r&#233;e une r&#233;ponse propos&#233;e par un syst&#232;me &#171; leader de l&#8217;Organisation de Lib&#233;ration de
la Palestine &#187;.
</p>
<p>Dans AVE, le corpus de paires justification-hypoth&#232;se a &#233;t&#233; construit semi-automatiquement &#224;
partir des r&#233;ponses obtenues par les participants lors de QA@CLEF 2006, campagne d&#8217;&#233;valua-
tion des SQR. Le corpus contient environ 3000 paires. Les participants &#224; AVE ont &#233;t&#233; &#233;valu&#233;s
sur leur capacit&#233; &#224; pr&#233;dire si une r&#233;ponse (attest&#233;e par des juges humains) &#233;tait correcte ou non.
Ils avaient donc pour chaque paire deux possibilit&#233;s de r&#233;ponse : OUI ou NON.
</p>
<p>Les r&#233;sultats ont &#233;t&#233; &#233;valu&#233;s par la pr&#233;cision, le rappel et la f-mesure qui ont &#233;t&#233; calcul&#233;s de la
fa&#231;on suivante :
</p>
<p>pre&#769;cision = #paires juge&#769;es OUI correctement#juge&#769;es comme OUI , rappel =
#paires juge&#769;es comme OUI correctement
</p>
<p>#paires OUI
</p>
<p>et f-mesure = 2&#8727;pre&#769;cision&#8727;rappel
pre&#769;cision+rappel
</p>
<p>3 Travaux en validation de r&#233;ponses
</p>
<p>(Pe&#241;as et al., 2006) pr&#233;sentent le d&#233;roulement de la premi&#232;re campagne AVE. 11 groupes ont
particip&#233; &#224; ce premier essai en soumettant 38 runs dans 7 langues diff&#233;rentes. L&#8217;anglais et l&#8217;espa-
gnol &#233;taient les langues les plus repr&#233;sent&#233;es avec respectivement 11 et 9 runs soumis. 2 groupes
ont propos&#233; des runs dans les 7 langues : ce sont les universit&#233;s de Twente et d&#8217;Alicante.
</p>
<p>Dans chaque langue, les paires justification-hypoth&#232;se ont &#233;t&#233; construites &#224; partir des soumis-
sions &#224; la t&#226;che questions-r&#233;ponses de la campagne CLEF 2006. De ce fait, le pourcentage de
paires positives, n&#233;gatives et non &#233;valu&#233;es 4 peut-&#234;tre variable d&#8217;une langue &#224; l&#8217;autre, ce qui
ne permet pas r&#233;ellement la comparaison des syst&#232;mes ayant particip&#233; dans des langues dis-
tinctes. Voici par exemple les pourcentages pour les 3 langues o&#249; les diff&#233;rences sont les plus
importantes :
</p>
<p>3Dans nos exemples, la r&#233;ponse est &#233;crite en gras.
4Les paires non &#233;valu&#233;es de AVE proviennent de runs qui n&#8217;ont pu &#234;tre &#233;valu&#233;s lors de la campagne QA@CLEF.
</p>
<p>En anglais et en portugais ce nombre est tr&#232;s &#233;lev&#233; : 35% et 40%.
</p>
<p>175</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Anne-Laure LIGOZAT, Brigitte GRAU, Isabelle ROBBA, Anne VILNAT
</p>
<p>&#8211; en hollandais, OUI : 10%, NON : 86%, NON &#201;VALU&#201;ES : 4% ;
&#8211; en anglais, OUI : 10%, NON : 55%, NON &#201;VALU&#201;ES : 35% ;
&#8211; en espagnol, OUI : 28%, NON : 68%, NON &#201;VALU&#201;ES : 4% ;
</p>
<p>Diff&#233;rentes approches ont &#233;t&#233; adopt&#233;es dans cette campagne. L&#8217;approche logique obtient les
meilleurs r&#233;sultats (Tatu et al., 2006) 5, soulignons qu&#8217;elle est tr&#232;s souvent accompagn&#233;e de
connaissances linguistiques : elles servent &#224; transformer les &#233;l&#233;ments textuels en repr&#233;sentation
logique. Au moins 3 &#233;quipes ont utilis&#233; logique et connaissances linguistiques. Les approches
qui utilisent de l&#8217;apprentissage sont &#233;galement au nombre de 3 et l&#8217;une d&#8217;entre elle s&#8217;est atta-
qu&#233;e aux 7 langues propos&#233;es. Elles utilisent des corpus d&#233;j&#224; annot&#233;s comme ceux des cam-
pagnes RTE. Une approche, qui a particip&#233; elle aussi dans les 7 langues, adopte une m&#233;thode
fond&#233;e sur les paraphrases : celles-ci sont engendr&#233;es automatiquement &#224; partir de corpus bi-
lingues align&#233;s. Deux approches au moins utilisent des connaissances linguistiques sans faire
r&#233;f&#233;rence &#224; l&#8217;utilisation de la logique. Partant du constat qu&#8217;en espagnol 75% des questions
de la campagne QA@CLEF &#233;taient factuelles, une approche s&#8217;est fond&#233;e uniquement sur la
reconnaissance d&#8217;entit&#233;s nomm&#233;s.
</p>
<p>(Tatu et al., 2006) utilisent un m&#233;canisme de reconnaissance des entit&#233;s nomm&#233;es, un analyseur
syntaxique et un analyseur s&#233;mantique pour transformer le passage justificatif et l&#8217;hypoth&#232;se en
une repr&#233;sentation logique qu&#8217;ils qualifient de riche. Les repr&#233;sentations sont ensuite soumises
&#224; COGEX, qui d&#233;termine si oui ou non la justification implique l&#8217;hypoth&#232;se. La plupart des er-
reurs commises par ce syst&#232;me sont dues &#224; une mauvaise syntaxe des hypoth&#232;ses (celles-ci sont
construites automatiquement), qui entra&#238;ne la construction de repr&#233;sentations logiques erron&#233;es.
N&#233;anmoins ce syst&#232;me obtient les meilleurs r&#233;sultats dans les 2 langues dans lesquelles il
a particip&#233;. En anglais, il obtient une f-mesure de 0.4393 et en espagnol une f-mesure de 0.6063.
</p>
<p>(Ferrandez et al., 2006) d&#233;rivent &#233;galement une forme logique &#224; partir du passage justificatif et
de l&#8217;hypoth&#232;se. Pour cela, ils utilisent l&#8217;analyseur de Lin, MINIPAR (Lin, 2005), et obtiennent
une repr&#233;sentation des phrases sous la forme d&#8217;un ensemble de relations de d&#233;pendances.
Les relations sont ensuite transcrites dans des formes logiques, puis une mesure de similarit&#233;
est calcul&#233;e, celle-ci produit un poids s&#233;mantique utilis&#233; pour juger si le passage justificatif
implique ou non l&#8217;hypoth&#232;se. Ils ont soumis des runs dans toutes les langues et obtenu les
meilleurs r&#233;sultats en fran&#231;ais (f-mesure : 0.4693) et en italien (f-mesure : 0.4066).
</p>
<p>Pour leur participation &#224; AVE, (Kouylekov et al., 2006) ont adopt&#233; une approche fond&#233;e sur
la notion de distance : ils essayent d&#8217;effectuer un mapping entre le contenu de l&#8217;hypoth&#232;se
et la justification. Ils soulignent que plus ce mapping est direct plus il est probable que la
justification implique l&#8217;hypoth&#232;se. Le mapping consiste ici en une s&#233;quence d&#8217;op&#233;rations
d&#8217;&#233;dition, chacune ayant un co&#251;t. Les op&#233;rations (insertion, suppression, substitution) sont
appliqu&#233;es sur les arbres de d&#233;pendances du passage justificatif et de l&#8217;hypoth&#232;se. Quand le
co&#251;t total de ces op&#233;rations est en dessous d&#8217;un seuil fix&#233;, le passage justificatif est consid&#233;r&#233;
comme impliquant l&#8217;hypoth&#232;se. Malgr&#233; diff&#233;rents probl&#232;mes dans la mise en place de ces
modules, ils ont obtenu la 3&#232;me place en anglais avec une f-mesure de 0.3776.
</p>
<p>5Tous les articles &#233;voqu&#233;s dans ce paragraphe ne seront pas tous r&#233;f&#233;renc&#233;s, mais ils sont rassembl&#233;s dans les
notes de travail du workshop CLEF 2006 et sont consultables &#224; l&#8217;adresse http://www.clef-campaign.
org/2006/working_notes/CLEF2006WN-Contents.html
</p>
<p>176</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Validation automatique de r&#233;ponses
</p>
<p>Comme cela a &#233;t&#233; dit dans l&#8217;introduction, il existe une forte connexion entre AVE et RTE. La
proposition &#224; l&#8217;origine d&#8217;AVE &#233;tait que l&#8217;on pouvait reformuler la t&#226;che de validation de r&#233;ponse
comme un probl&#232;me d&#8217;implication textuelle. Et, plusieurs groupes ont d&#8217;ailleurs particip&#233; aux
deux &#233;valuations en utilisant la m&#234;me approche.
</p>
<p>En 2006, a &#233;t&#233; organis&#233; le second RTE. (Bar-Haim et al., 2006) soulignent les particularit&#233;s
des deux syst&#232;mes qui ont obtenu les meilleurs r&#233;sultats. L&#8217;un a utilis&#233; de fa&#231;on extensive des
connaissances s&#233;mantiques, l&#8217;autre a favoris&#233; l&#8217;utilisation de grands corpus d&#8217;entra&#238;nement.
</p>
<p>Dans notre travail, nous ne faisons pas l&#8217;hypoth&#232;se d&#8217;une source de connaissances s&#233;mantiques
existante qui permettrait des d&#233;ductions logiques. Aussi, nous reposons nous sur des crit&#232;res
linguistiques, qui peuvent &#234;tre v&#233;rifi&#233;s en domaine ouvert, et qui permettent d&#8217;exprimer des
relations s&#233;mantiques entre le sens des mots.
</p>
<p>4 Valider des r&#233;ponses avec un SQR
</p>
<p>Notre objectif &#233;tait d&#8217;utiliser notre propre SQR pour le fran&#231;ais : FRASQUES, et d&#8217;utiliser ses
r&#233;sultats, c&#8217;est-&#224;-dire &#224; la fois les r&#233;ponses extraites et les types d&#8217;informations de la questions
pr&#233;sentes dans les justifications, pour &#233;valuer la pertinence des justifications par rapport aux
hypoth&#232;ses.
</p>
<p>4.1 FRASQUES : notre syst&#232;me de questions-r&#233;ponses pour le fran&#231;ais
</p>
<p>Nous pr&#233;sentons tout d&#8217;abord bri&#232;vement FRASQUES avant de pr&#233;senter comment il a &#233;t&#233;
adapt&#233; pour la t&#226;che de validation.
</p>
<p>Le syst&#232;me se divise en 4 composants :
&#8211; Analyse de la question : ce premier module effectue l&#8217;analyse syntaxique de la question pour
</p>
<p>en d&#233;tecter certaines de ses caract&#233;ristiques telles que :
&#8211; ses mot-cl&#233;s, utilis&#233;s ult&#233;rieurement lors de la recherche des documents,
&#8211; le type attendu de la r&#233;ponse, qui peut-&#234;tre une entit&#233; nomm&#233;e (une personne, un pays, une
</p>
<p>date...) ou un type g&#233;n&#233;ral comme conf&#233;rence ou adresse,
&#8211; le focus de la question, que nous d&#233;finissons comme le terme de la question qui sera vrai-
</p>
<p>semblablement pr&#233;sent dans la phrase contenant la r&#233;ponse,
&#8211; le verbe principal de la question.
</p>
<p>&#8211; S&#233;lection des documents : le moteur de recherche Lucene 6 cherche dans la collection les
documents pertinents.
</p>
<p>&#8211; Traitement des documents : ce module utilise Fastr 7 pour reconna&#238;tre les variantes linguis-
tiques des termes de la question : par exemple, &#171; monnaie de l&#8217;Europe &#187; sera reconnue comme
une variante de &#171; monnaie europ&#233;enne &#187;. Ensuite, les entit&#233;s nomm&#233;es du document sont &#233;ti-
quet&#233;es, nous utilisons environ une vingtaine de type d&#8217;entit&#233;s nomm&#233;es. Les phrases conte-
nant au moins une variante des termes de la question sont gard&#233;es.
</p>
<p>&#8211; Extraction de la r&#233;ponse : ce dernier module extrait les r&#233;ponses pr&#233;cises des phrases candi-
dates. La strat&#233;gie d&#8217;extraction d&#233;pend du type attendu de la r&#233;ponse. Si la r&#233;ponse est une
entit&#233; nomm&#233;e, l&#8217;entit&#233; nomm&#233;e qui est du type attendu et qui est la plus proche des mots de
</p>
<p>6Moteur de recherche enti&#232;rement &#233;crit en Java http://lucene.apache.org/
7http://www.limsi.fr/Individu/jacquemi/FASTR/
</p>
<p>177</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Anne-Laure LIGOZAT, Brigitte GRAU, Isabelle ROBBA, Anne VILNAT
</p>
<p>Analyse de la question
</p>
<p>Extraction de la r&#233;ponse
</p>
<p>Extraction de la r&#233;ponse pr&#233;cise
</p>
<p>pour le m&#234;me passage
</p>
<p>Passage &#224; juger
</p>
<p>R&#233;ponse &#224; juger
</p>
<p>Evaluation de la r&#233;ponse
</p>
<p>Question (Q)
</p>
<p>(S)
Traitement des documents
</p>
<p>Reconnaissance de variations
par Fastr
</p>
<p>S&#233;lection de phrases
Etiquetage des entit&#233;s nomm&#233;es
</p>
<p>(R2)(R1) R&#233;ponses du SQR
</p>
<p>Verbe principal
Type de r&#233;ponse attendu
Cat&#233;gorie
</p>
<p>Reconnaissance de : 
Mots&#358;clefs
</p>
<p>FIG. 1 &#8211; Architecture du syst&#232;me de validation de la r&#233;ponse
</p>
<p>la question est s&#233;lectionn&#233;e. Sinon, des patrons d&#8217;extraction sont utilis&#233;s, ils sont &#233;crits dans
le format Cass 8, un analyseur syntaxique qui est utilis&#233; ici pour extraire la r&#233;ponse plut&#244;t
que comme analyseur. Ces patrons expriment la position possible de la r&#233;ponse par rapport
au focus ou au type attendu de la r&#233;ponse.
</p>
<p>4.2 Le syst&#232;me de validation des r&#233;ponses
</p>
<p>Le syst&#232;me de validation des r&#233;ponses utilise trois de ces quatre composants, ce que montre la
figure 1. L&#8217;entr&#233;e du syst&#232;me est une paire justification-hypoth&#232;se, ainsi que la question d&#8217;ori-
gine Q et la r&#233;ponse &#224; juger R1. La question est d&#8217;abord analys&#233;e puis le composant qui traite
les documents est appliqu&#233; &#224; la justification. Le module d&#8217;extraction de la r&#233;ponse extrait les
r&#233;ponses R2 des passages justificatifs. Enfin, la paire hypoth&#232;se-justification est &#233;valu&#233;e en te-
nant compte des diff&#233;rentes informations de l&#8217;hypoth&#232;se trouv&#233;es dans l&#8217;extrait et de la r&#233;ponse
trouv&#233;e par FRASQUES. Le syst&#232;me retourne OUI si elle est consid&#233;r&#233;e comme justifi&#233;e, NON
dans le cas contraire. Un score de confiance est &#233;galement attribu&#233; &#224; chaque jugement.
</p>
<p>L&#8217;algorithme de d&#233;cision se d&#233;roule en 2 &#233;tapes. La premi&#232;re a pour but de d&#233;tecter les erreurs
les plus triviales, par exemple une r&#233;ponse qui serait compl&#232;tement incluse dans la question ou
qui ne serait pas pr&#233;sente dans la justification. Dans le cas o&#249; la question contient une date,
le contexte temporel de la question et l&#8217;extrait sont compar&#233;s. Pour l&#8217;instant, le contexte est
form&#233; par les dates reconnues comme telles pr&#233;sentes dans la description du document ou dans
le passage. S&#8217;ils sont contradictoires, la paire est rejet&#233;e.
</p>
<p>8http://www.sfs.nphil.uni-tuebingen.de/~abney/
</p>
<p>178</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Validation automatique de r&#233;ponses
</p>
<p>La seconde &#233;tape consiste en des v&#233;rifications plus complexes. Dans un cas id&#233;al, un passage
justificatif correct correspond &#224; la reformulation de la question sous forme d&#233;clarative avec
la r&#233;ponse qui y est donn&#233;e. Chaque terme de la question, ou de l&#8217;hypoth&#232;se, figure dans le
passage, li&#233;s par les m&#234;mes relations.
</p>
<p>En ce qui concerne les termes, dans la grande majorit&#233; des cas, le passage justificatif ne com-
porte pas tous les termes de la questions sous leur forme d&#8217;origine : ils subissent des variations
de diff&#233;rentes natures : flexionnelles, morphologiques, syntaxiques, s&#233;mantiques ou des combi-
naisons de ces variations si on recherche des groupes nominaux complexes. Dans FRASQUES,
ces variations sont reconnues par Fastr. Parmi les termes de la question, certains jouent un
r&#244;le plus important. Il en est ainsi de l&#8217;objet de la question, que nous appelons focus dans
FRASQUES. Le focus correspond &#224; l&#8217;entit&#233; sur laquelle porte la question, que l&#8217;on en cherche
une caract&#233;ristique ou une d&#233;finition. Aussi, selon les types de question, le focus n&#8217;est pas tou-
jours pr&#233;sent, mais s&#8217;il l&#8217;est, il doit figurer dans le passage justificatif. Un autre terme qui, s&#8217;il
est pr&#233;sent, a une grande importance, est le type de r&#233;ponse attendu, quand ce type n&#8217;est pas un
nom d&#8217;entit&#233; nomm&#233;e. Ce type est nomm&#233; type g&#233;n&#233;ral. Ainsi, dans &#171; De quel parti politique
Lionel Jospin est-il membre ? &#187; Le focus est &#171; Lionel Jospin &#187; et le type g&#233;n&#233;ral est &#171; parti
politique &#187;. Lorsqu&#8217;il est pr&#233;sent dans le passage r&#233;ponse, le type g&#233;n&#233;ral sera souvent plac&#233; &#224;
proximit&#233; de la r&#233;ponse ou m&#234;me fera partie de celle-ci, comme dans &#171; Lionel Jospin, membre
du parti socialiste &#187;.
</p>
<p>Lorsqu&#8217;il s&#8217;agit du verbe, celui-ci a tendance &#224; subir plus de variations que les termes nominaux ;
il est souvent exprim&#233; par une pr&#233;position ou un verbe proche mais non synonyme. C&#8217;est le cas
par exemple si on demande &#171; qui a r&#233;alis&#233; un film &#187; et que la r&#233;ponse est exprim&#233;e par &#171; le film
de X ... &#187; ou &#171; quelle entreprise a chang&#233; son nom &#187; et la r&#233;ponse est donn&#233;e par &#171; le groupe X
a adopt&#233; le nom de la filiale ... &#187;. On retrouve ici les variations trait&#233;es par (Lin &amp; Pantel, 2001).
Ne disposant pas de telles ressources, nous avons consid&#233;r&#233; que l&#8217;absence du verbe n&#8217;influerait
pas sur la d&#233;cision finale.
</p>
<p>Enfin les derniers types de termes jouant un r&#244;le primordial sont les noms propres : ils sont
toujours pr&#233;sents dans le passage et subissent peu de variations, sauf en ce qui concerne les
noms de pays souvent repris par l&#8217;adjectif correspondant, comme dans &#171; qui est le pr&#233;sident de
l&#8217;Egypte &#187; avec &#171; le pr&#233;sident &#233;gyptien &#187; repris dans le passage.
</p>
<p>En ce qui concerne les relations entre termes, celles-ci seront souvent v&#233;rifi&#233;es par leur mani-
festation en langue, c&#8217;est-&#224;-dire par un ensemble de relations syntaxiques. Nous avons vu que
certains travaux s&#8217;appuient sur une notion de distance syntaxique. Mais pour cela, il est n&#233;ces-
saire de disposer d&#8217;une analyse compl&#232;te des phrases. Afin de ne pas reposer sur cette hypoth&#232;se
souvent non v&#233;rifi&#233;e, nous avons choisi de ne v&#233;rifier que certaines relations en les exprimant
sous forme de patrons d&#8217;extraction. Ces relations sont celles qui lient la r&#233;ponse avec certains
&#233;l&#233;ments de la phrase : le focus ou le type g&#233;n&#233;ral.
</p>
<p>L&#8217;&#233;l&#233;ment pr&#233;pond&#233;rant, malgr&#233; tout, reste la r&#233;ponse : est-elle du type attendu ou non ? Lorsque
ce type est une entit&#233; nomm&#233;e, la v&#233;rification consistera &#224; retrouver une entit&#233; nomm&#233;e d&#8217;un
type ad&#233;quat. Lorsque celui-ci est d&#233;sign&#233; par le type g&#233;n&#233;ral, ou bien il figure &#224; proximit&#233; de la
r&#233;ponse, ou bien il est implicite et la r&#233;ponse en est une instance. Cette relation d&#8217;instanciation
pourrait &#234;tre inf&#233;r&#233;e par l&#8217;utilisation de ressources externes, par exemple Wikipedia, qui poss&#232;de
un grand nombre de cat&#233;gories et de d&#233;finitions leur correspondant.
</p>
<p>La mise en oeuvre de ces crit&#232;res de justification donne lieu dans notre syst&#232;me &#224; un calcul
de 2 scores qui permet ensuite de conclure positivement ou n&#233;gativement. Le premier score
</p>
<p>179</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Anne-Laure LIGOZAT, Brigitte GRAU, Isabelle ROBBA, Anne VILNAT
</p>
<p>porte sur l&#8217;&#233;valuation de la correspondance entre la r&#233;ponse trouv&#233;e par notre syst&#232;me R2 et
la r&#233;ponse propos&#233;e dans l&#8217;hypoth&#232;se R1. Si FRASQUES trouve une r&#233;ponse diff&#233;rente, alors
la paire hypoth&#232;se-justification est r&#233;fut&#233;e. Si les 2 r&#233;ponses sont proches ou s&#8217;il n&#8217;y a pas de
r&#233;ponse trouv&#233;e par FRASQUES, la d&#233;cision va &#234;tre conditionn&#233;e par la pr&#233;sence des diff&#233;rents
termes que nous avons privil&#233;gi&#233;s. Le score attribu&#233; &#224; l&#8217;&#233;valuation de la qualit&#233; de la r&#233;ponse
sera positif pour une r&#233;ponse exacte ou approch&#233;e, et n&#233;gatif quand la distance est assez grande,
par exemple, l&#8217;approximation d&#8217;une date ou d&#8217;une quantit&#233; par un nombre.
</p>
<p>Le deuxi&#232;me score &#233;value les termes pr&#233;sents. Il est calcul&#233; en combinant le nombre de crit&#232;res
pr&#233;sents et leurs valeurs. Il est n&#233;gatif si aucun des crit&#232;res n&#8217;est trouv&#233;, et positif sinon.
</p>
<p>Un passage constitue une justification acceptable :
&#8211; si R2 est absente et le score des termes est positif. Ce dernier fournit le score final,
&#8211; si R2 = R1 et il y a des crit&#232;res pr&#233;sents. Dans ce cas le score final est la valeur maximale des
deux crit&#232;res,
</p>
<p>&#8211; si les 2 scores vont dans des sens oppos&#233;s, on prend le meilleur des deux, s&#8217;il est positif.
Regardons l&#8217;exemple suivant :
</p>
<p>Justification : Trois candidats, Tony Blair, Margaret Beckett et John Prescott, se
disputeront la succession de John Smith &#224; la t&#234;te du parti travailliste, a annonc&#233; le
Labour jeudi, &#224; l&#8217;issue du processus de nominations des candidats par les d&#233;put&#233;s
du parti. M. Blair, ministre de l&#8217;Int&#233;rieur du cabinet fant&#244;me repr&#233;sentant l&#8217;a
</p>
<p>Hypoth&#232;se : le parti politique de Tony Blair, le LABOUR .
</p>
<p>Dans ce passage, tous les termes de la question sont pr&#233;sents (Tony Blair, parti politique), mais
le type de la r&#233;ponse Labour n&#8217;est pas &#233;tiquet&#233; par notre SQR comme une entit&#233; nomm&#233;e de
type organisation. De ce fait, l&#8217;algorithme de d&#233;cision re&#231;oit 2 scores oppos&#233;s, dans cet exemple
prenant en compte le non-marquage de Labour comme une organisation, il r&#233;pond n&#233;gative-
ment.
</p>
<p>4.3 R&#233;sultats
</p>
<p>Le corpus d&#8217;&#233;valuation contenait 3266 paires, parmi lesquelles 202 paires n&#8217;ont pas &#233;t&#233; ju-
g&#233;es. Les hypoth&#232;ses &#233;tant form&#233;es automatiquement, elles comportaient beaucoup d&#8217;erreurs
de syntaxe, aussi nous sommes-nous fond&#233;s uniquement sur les questions, l&#8217;hypoth&#232;se ne nous
permettant que d&#8217;extraire la r&#233;ponse.
</p>
<p>Lors de notre participation &#224; AVE, beaucoup d&#8217;erreurs restaient dans nos programmes, qui ont
&#233;t&#233; corrig&#233;s depuis. Les organisateurs ayant fourni les valeurs de validation attendues pour
chaque paire du corpus, nous avons pu r&#233;&#233;valuer notre cha&#238;ne. Un examen approfondi de ces
r&#233;sultats nous a permis de constater qu&#8217;il y avait certaines erreurs sur ces valeurs, notamment
en ce qui concerne les r&#233;ponses positives : des r&#233;ponses exactes aux questions n&#8217;&#233;taient pas du
tout valid&#233;es par le passage justificatif. Nous avons corrig&#233; 82 d&#8217;entre elles dans le corpus.
</p>
<p>La table 1 pr&#233;sente nos r&#233;sultats sur la version officielle, les corrections apport&#233;es au corpus
ne modifiant pas les ordres de grandeur des r&#233;sultats. La premi&#232;re ligne donne le nombre de
paires &#233;valu&#233;es positivement et n&#233;gativement par les juges humains. La seconde ligne contient
tous nos r&#233;sultats et la suivante le nombre de nos r&#233;sultats corrects. La derni&#232;re ligne contient
le rappel et la f-mesure correspondant &#224; ces r&#233;sultats en utilisant la formule expos&#233;e dans la
section 2.
</p>
<p>180</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Validation automatique de r&#233;ponses
</p>
<p># OUI # NON Total
&#201;valu&#233;s par les organisateurs 705 2359 3064
</p>
<p>Tous nos r&#233;sultats 142 2922 3064
Nos r&#233;sultats corrects 82 2266 2348
</p>
<p>Pr&#233;cision 0.58 0.77
Rappel 0.12 0.96
F-mesure 0.2 0.85
</p>
<p>TAB. 1 &#8211; AVE results at CLEF 2006
</p>
<p>Parmi nos r&#233;ponses NON, nous distinguons celles qui sont s&#251;res des autres : ce sont les r&#233;futa-
tions d&#233;cid&#233;es lors de la premi&#232;re &#233;tape pr&#233;sent&#233;es dans la section 4.2. La r&#233;ponse est consid&#233;r&#233;e
comme &#233;tant non justifi&#233;e et ce de fa&#231;on s&#251;re, donc avec un score de confiance &#233;lev&#233;. Nous avons
trouv&#233; 1637 paires de &#171; NON &#187; s&#251;rs. Parmi elles, 1415 &#233;taient bien jug&#233;es, la pr&#233;cision pour ces
r&#233;ponses est donc de 0,87.
</p>
<p>La seconde observation est que notre syst&#232;me a plus de facilit&#233;s pour r&#233;futer les justifications
plut&#244;t que pour les accepter. La pr&#233;cision et le rappel de nos r&#233;ponses n&#233;gatives sont bons. Et
nous nous trompons rarement quand nous donnons des r&#233;ponses OUI, mais nous en trouvons
tr&#232;s peu, notre rappel est donc tr&#232;s faible sur ces r&#233;ponses.
</p>
<p>Certaines erreurs pourraient &#234;tre corrig&#233;es en approfondissant les v&#233;rifications des relations por-
tant sur la r&#233;ponse. Par exemple, pour la question &#171; Quel est le nom de la femme de George W.
Bush ? &#187;, une des hypoth&#232;ses construites &#233;tait &#171; Norman Schwarzkopf, la femme de George W.
Bush. &#187;. On pourrait alors interroger le Web avec la requ&#234;te femme de George W. Bush et consta-
ter que la ou les r&#233;ponses obtenues sont fortement incompatibles avec Norman Schwarzkopf.
</p>
<p>5 Conclusion
</p>
<p>Nous avons pr&#233;sent&#233; une strat&#233;gie de validation des r&#233;ponses issues d&#8217;un SQR. Cette strat&#233;gie est
fond&#233;e sur FRASQUES, notre propre SQR monolingue : l&#8217;hypoth&#232;se et l&#8217;extrait sont analys&#233;s
par FRASQUES et nous utilisons des crit&#232;res qui permettent de d&#233;tecter si l&#8217;extrait justifie
ou non la r&#233;ponse. Dans notre &#233;valuation des paires hypoth&#232;ses-extrait, nous distinguons avec
une bonne pr&#233;cision les cas dans lesquels l&#8217;extrait ne justifie pas la r&#233;ponse. Des possibilit&#233;s
d&#8217;extension de notre strat&#233;gie, utilisant des ressources externes et nous permettant d&#8217;acqu&#233;rir de
nouvelles connaissances ont &#233;galement &#233;t&#233; pr&#233;sent&#233;es.
</p>
<p>Cette premi&#232;re exp&#233;rience en validation de r&#233;ponses constitue une &#233;tape vers la validation semi-
automatique en questions-r&#233;ponses. Elle nous permettra &#224; terme d&#8217;am&#233;liorer les performances
de notre SQR puisque certains des crit&#232;res que nous utilisons pour la validation n&#8217;y avaient pas
&#233;t&#233; mis en &#339;uvre.
</p>
<p>R&#233;f&#233;rences
</p>
<p>BAR-HAIM R., DAGAN I., DOLAN B., FERRO L., GIAMPICCOLO D., MAGNINI B. &amp; SZ-
PEKTOR I. (2006). The second pascal recognising textual entailment challenge. In The Second
</p>
<p>181</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Anne-Laure LIGOZAT, Brigitte GRAU, Isabelle ROBBA, Anne VILNAT
</p>
<p>PASCAL Challenges Workshop on Recognising Textual Entailment.
</p>
<p>FABRE C. &amp; JACQUEMIN C. (2000). Boosting Variant Recognition with Light Semantics. In
Proceedings of 18th International Conference on Computational Linguistics (COLING-2000),
Sarrebr&#252;ck, Allemagne.
</p>
<p>FERRANDEZ O., TEROL R. M., MUNOZ R., MARTINEZ-BARCO P. &amp; PALOMAR M. (2006).
A knowledge-based textual entailment approach applied to the qa answer validation at clef
2006. In Workshop CLEF 2006, Alicante, Spain.
</p>
<p>JONES K. S. &amp; TAIT J. I. (1984). Automatic Search Term Variant Generation. Journal of
Documentation, p. 50&#8211;66.
</p>
<p>KOUYLEKOV M., NEGRI M., MAGNINI B. &amp; COPPOLA B. (2006). Towards entailment-
based question answering : Itc-irst at clef 2006. In Workshop CLEF 2006, Alicante, Spain.
</p>
<p>LIN D. (2005). Dependancy-based evaluation of minipar. In Workshop on the Evaluation of
Parsing Systems, Southampton, UK.
</p>
<p>LIN D. &amp; PANTEL P. (2001). Discovery of inference rules for question-answering. Natural
Language Engineering, 7(04), 343&#8211;360.
</p>
<p>PE&#209;AS A., RODRIGO A., SAMA V. &amp; VERDEJO F. (2006). Overview of the answer validation
exercise 2006. InWorkshop CLEF 2006, Alicante, Spain.
</p>
<p>TATU M., ILES B. &amp; MOLDOVAN D. (2006). Automatic answer validation using cogex. In
Workshop CLEF 2006, Alicante, Spain.
</p>
<p>VOORHEES E. M. (1999). TREC-8 Question Answering Track Evaluation. In Proceedings of
the Eighth Text REtrieval Conference (TREC-8) : Department of Commerce, National Institute
of Standards and Technology.
</p>
<p>182</p>

</div></div>
</body></html>