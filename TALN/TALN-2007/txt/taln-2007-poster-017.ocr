TALN 2007, Toulouse, 5-8 juin 2007

Systémes de questions-réponses : vers la validation
automatique des réponses

Anne—Laure LIGOZAT, Brigitte GRAU, Isabelle ROBBA, Anne VILNAT
LIMSI-CNRS - BP 133, 91403 Orsay Cedex

prenom . nom@limsi . fr

Résumé. Les systemes de questions-réponses (SQR) ont pour but de trouver une infor-
mation précise extraite d’ une grande collection de documents comme le Web. Aﬁn de pouvoir
comparer les différentes strategies possibles pour trouver une telle information, il est important
d’évaluer ces systemes. L’objectif d’une tache de validation de réponses est d’estimer si une
réponse donnée par un SQR est correcte ou non, en fonction du passage de texte donné comme
justiﬁcation. En 2006, nous avons participé a une tache de validation de réponses, et dans cet
article nous présentons la stratégie que nous avons utilisée. Celle-ci est fondée s11r notre propre
systeme de questions-réponses. Le principe est de comparer nos réponses avec les réponses a
valider. Nous présentons les résultats obtenus et montrons les extensions possibles. A partir de
quelques exemples, nous so11lignons les difﬁcultés que pose cette tache.

Abstract. Question answering aims at retrieving precise information from a large collec-
tion of documents, typically the Web. Different techniques can be used to ﬁnd relevant informa-
tion, and to compare these techniques, it is important to evaluate question answering systems.
The objective of an Answer Validation task is to estimate the correctness of an answer retumed
by a QA system for a question, according to the text snippet given to support it. We participated
in such a task in 2006. In this article, we present our strategy for deciding if the snippets justify
the answers. We used a strategy based on our own question answering system, and compared
the answers it returned with the answer to judge. We discuss o11r results, and show the possible
extensions of o11r strategy. Then we point out the difﬁc11lties of this task, by examining different
examples.

Mots-clés 2 systemes de questions-réponses, validation de réponses.

Keywords: question answering, answer validation.

1 Introduction

Les systemes de questions-réponses (SQR par la suite) ont po11r but de trouver une information
précise dans une grande collection de documents. L’hypothese sous-jacente au développement
de tels systemes est que les utilisateurs préferent en general recevoir une réponse precise a
la question qu’ils se posent plut6t qu’un ensemble de documents a explorer, comme le pro-
posent habituellement les moteurs de recherche (Voorhees, 1999). Cependant, pour étre consi-
déré comme ﬁable par un utilisate11r, un SQR doit étre capable de donner des éléments permet-
tant d’évaluer ses réponses. L’objectif d’un systeme ne doit donc pas se11lement étre de trouver

173

Anne-Laure LIGOZAT, Brigitte GRAU, Isabelle ROBBA, Anne VILNAT

les réponses, mais aussi de les exprimer d’une facon q11i perrnette 51’utilis5teur de savoir s’il
peut avoir conﬁance en ces réponses. Ces éléments de justiﬁcation donnent 5 1’uti1isateur un
moyen de vériﬁer que la réponse foumie correspond bien 5 1’inform5tion qu’il cherche, et ainsi
de donner une va1e11r de vérité 5 cette réponse, en supposant que 1’uti1isateur 5 des connaissances
« standard ».

Une bonne justiﬁcation doit étre concise et complete. Le but est de ne foumir que les extraits
de documents q11i permettent 5 1’uti1isateur de retrouver toutes les informations qu’il 5 données,
sans avoir 5 lire un document entier. Voici un exernple d’ une telle justiﬁcation.

Question : Quand 5 eu lieu 15 chute du m11r de Berlin ?
Réponse : en 1989

J ustiﬁcation (passage d’un document) : Cette ere de 15 dissuasion, fondée s11r l’équi-
libre de 15 terreur entre deux grands blocs antagonistes, est remise en question en
1989, avec 15 chute symbolique du m11r de Berlin.

2 Validation de réponses

(Lin & Pantel, 2001) so111ignent 15 possible distance ling11istique entre les questions et 1e11rs
réponses accompagnées de leur justiﬁcation, en prenant l’exernple de la phrase « Stendhal a
écrit ’La chartreuse de Parme’ en 1838 » justiﬁant la réponse « Stendhal » 515 question « Qui
est l’auteur de ’La chartreuse de Parme’ ? ». Ils déﬁnissent les liens entre une question et sa
réponse justiﬁée par le terme d’ inference. Ils proposent alors de déﬁnir des re‘gles d ’inférence
pour reconnaitre par exemple la relation entre « X 5 écrit Y » et « X est 1’ 5ute11r de Y ». Ces
regles correspondent plus ou moins 5 ce q11i est appelé paraphrases ou variantes dans d’ autres
travaux (Jones & Tait, 1984; Fabre & Jacquemin, 2000).

Le lien entre question et réponses correspond 515 notion de textual entailment telle qu’elle est
déﬁnie par Pascal Recognizing Textual Entailement Challenge 1 (RTE). L’implication textuelle
est déﬁnie comme une t5che de decision q11i 5 partir de deux fragments de texte, estime si d’un
point de vue sémantique on peut déduire 1’un de 1’ autre. Ainsi le passage de texte s11iv5nt (appelé
justification) : « Yoko Ono a inaugure’ une statue de bmnze représentant son marl de’ce’de’,
John Lennon, pour compléter le changement de nom oﬁiciel de l’ae’roport de Liverpool qui
devient l ’ae’mport John Lennon de Liverpool » implique 15 phrase « Yoko Ono est la veuve de
John Lennon » (appelée hypothese dans le contexte de l’irnplic5tion text11elle). Dans RTE, les
participants regoivent des paires justiﬁcation-hypothese de ce type et doivent ens11ite décider si
les hypotheses peuven ! t ou non étre déduites des justiﬁcation. Cette t5che est similaire 5 la
t5che de réponses aux questions en ce q11i conceme les questions booléennes (attendant oui ou
non en réponse), car répondre 5 ces questions revient en fait 5 décider si la justiﬁcation de la
réponse irnplique la réponse.

En 2006, un nouvel exercice de validation des réponses, AVE 2, 5 été introd11it dans la cam-
pagne de questions-réponses de CLEF. Le but de cet exercice est d’une part d’améliorer les
performances des SQR, en développant des méthodes automatiques d’évaluation des réponses,
et d’ autre part de rendre le jugement humain serni-autornatique 5 la condition que 1’exercice
prod11ise des méthodes ﬁables d’éva1uation. Po11r cet exercice, les organisateurs ont prod11it un

1http:/ /www . pas cal—network . org/Challenges /RTE
2Answer Validation Exercise, http : / /nlp . uned . es /QA/AVE/

174

Validation auton1atique de réponses

corpus a partir des réponses des par1icipants a la tache de questions-réponses et des passages
de texte domes comme justiﬁcation. Les participants avaient alors pour tache de decider pour
chaque réponse si elle était correcte ou non en fonction du passage justiﬁcatif.

Les premiers travaux de validation automatique de réponses ont eu lieu au cours de la campagne
AVE en 2006 ; cependant, les campagnes d’implication teXt11elle RTE avaient déja propose ce
type de tache.

Voici un exemple de couple (hypothese, justiﬁcation) d’AVE :

Hypothese : Yasser Arafat était leader de l’O1-ganisation de Libération de la
Palestine 3

J ustiﬁcation : Le président Clinton a fait appel personnellement au leader de l’O1-
ganisaﬁon de Libération de la Palestine Yasser Arafat et aux Palestiniens mer-
credi pour qu’ils reprennent les pourparlers en faveur de la paix avec Israel

Ici l’hypothese est une reformulation de la question « Qui était Yasser Arafat ? » dans laquelle
a été insérée une réponse proposée par un systeme « leader de l’0rganisation de Liberation de
la Palestine ».

Dans AVE, le corpus de paires justiﬁcation-hypothese a été construit sen1i-automatiquement a
partir des réponses obtenues par les participants lors de QA@CLEF 2006, campagne d’ evalua-
tion des SQR. Le corpus contient environ 3000 paires. Les participants 51 AVE ont été évalués
s11r leur capacité a prédire si une réponse (attestée par des juges humains) était correcte ou non.
Ils avaient donc pour chaque paire deux possibilités de réponse : OUI ou NON.

Les résultats ont été évalués par la précision, le rappel et la f-mesure q11i ont été calculés de la
fagon s11ivante :

r - - _ ﬁmires jugées OU I czrrrecternent _ jfmires jugées comme OUI cmvecternent
preczszon _ #jugées comme OUI ’ rappel _ #pai'res OUI

_ = ">o<ﬂéc'is'iorn>rramel
etf mesure p'réc'isio'n+'rappel

3 Travaux en validation de réponses

(Peﬁas et al., 2006) présentent le déroulement de la premiere campagne AVE. 11 groupes ont
participé a ce premier essai en soumettant 38 rtms dans 7 langues différentes. L’ anglais et l’espa-
gnol étaient les langues les plus représentées avec respectivement 11 et 9 runs soumis. 2 groupes
ont propose des rlms dans les 7 langues : ce sont les universités de Tvvente et d’Alicante.

Dans chaque langue, les paires justification-hypothese ont été construites a partir des soumis-
sions a la tache questions-réponses de la campagne CLEF 2006. De ce fait, le pourcentage de
paires positives, negatives et non évaluées 4 peut-étre variable d’une langue a l’autre, ce q11i
ne permet pas réellement la comparaison des systemes ayant parlicipé dans des langues dis-
tinctes. Voici par exemple les pourcentages pour les 3 langues ou les differences sont les plus
importantes :

3Da:1s nos exemples, la réponse est écrite en gras.
4Les paires non évaluées de AVE proviennent de runs qui n’ont pu étre évalués lors de la campagne QA@CLEF.
En anglais et en portugais ce nombre est trés élevé : 35% et 40%.

175

Anne-Laure LIGOZAT, Brigitte GRAU, Isabelle ROBBA, Anne VILNAT

— en hollandais, OUI : 10%, NON : 86%, NON EVALUEES : 4%;
— en anglais, OUI : 10%, NON : 55%, NON EVALUEES : 35%;
— en espagnol, OUI : 28%, NON : 68%, NON EVALUEES : 4%;

Différentes approches ont été adoptées dans cette campagne. L’approche logique obtient les
meilleurs résultats (Tatu et al., 2006) 5, so11lignons qu’elle est tres souvent accompagnée de
connaissances linguistiques : elles servent a transformer les éléments teXt11els en représentation
logique. Au moins 3 équipes ont utilisé logique et connaissances linguistiques. Les approches
q11i utilisent de l’apprentissage sont également au nombre de 3 et l’une d’entre elle s’est atta-
quée aux 7 langues proposées. Elles utilisent des corpus déja armotés comme ceux des carn-
pagnes RTE. Une approche, q11i a parlicipé elle aussi dans les 7 langues, adopte une méthode
fondée s11r les paraphrases : celles-ci sont engendrées autornatiquement a partir de corpus bi-
lingues alignés. Deux approches au moins utilisent des connaissances ling11istiques sans faire
reference a l’utilisation de la logique. Partant du constat qu’en espagnol 75% des questions
de la campagne QA@CLEF étaient fact11elles, une approche s’est fondée uniquement sur la
reconnaissance d’ entités nommés.

(Tatu et al., 2006) utilisent un mécanisme de reconnaissance des entités nommées, un analyseur
syntaxique et un analyse11r sémantique pour transformer le passage justiﬁcatif et l’hypothese en
une représentation logique qu’ils qualiﬁent de riche. Les représentations sont ens11ite soumises
a COGEX, q11i détermine si o11i ou non la justiﬁcation irnplique l’hypothese. La plupart des er-
reurs commises par ce systeme sont dues a une rnauvaise syntaxe des hypotheses (celles-ci sont
construites automatiquement), q11i entraine la construction de representations lo giques erronées.
Néanmoins ce systeme obtient les meilleurs résultats dans les 2 langues dans lesquelles il
a participé. En anglais, il obtient une f-mesure de 0.4393 et en espagnol une f-mesure de 0.6063.

(Ferrandez et al., 2006) dérivent également une forme logique a partir du passage justiﬁcatif et
de l’hypothese. Pour cela, ils utilisent l’analyseur de Lin, MINIPAR (Lin, 2005), et obtiennent
une représentation des phrases sous la forme d’un ensemble de relations de dépendances.
Les relations sont ens11ite transcrites dans des formes logiques, p11is une mes11re de similarité
est calculée, celle-ci prod11it un poids sémantique utilisé pour juger si le passage justiﬁcatif
irnplique ou non l’hypothese. Ils ont soumis des rlms dans toutes les langues et obtenu les
meilleurs résultats en francais (f-mesure : 0.4693) et en italien (f-mes11re : 0.4066).

Pour leur participation at AVE, (Kouylekov et al., 2006) ont adopté une approche fondée sur
la notion de distance : ils essayent d’effectuer un mapping entre le contenu de l’hypothése
et la justiﬁcation. Ils so11lignent que plus ce mapping est direct plus il est probable que la
justiﬁcation irnplique l’hypothese. Le mapping consiste ici en une séquence d’opérations
d’édition, chacune ayant un coﬁt. Les opérations (insertion, suppression, substitution) sont
appliquées s11r les arbres de dépendances du passage justiﬁcatif et de l’hypothése. Quand le
coﬁt total de ces opérations est en dessous d’un se11il ﬁxé, le passage justiﬁcatif est considéré
comme impliquant l’hypothese. Malgré différents problemes dans la mise en place de ces
modules, ils ont obtenu la 3eme place en anglais avec une f-mesure de 0.3776.

5Tbus les articles évoqués dans ce paragraphe ne seront pas tous référencés, mais ils sont rassemblés dans les
notes de travail du workshop CLEF 2006 et sont consultables 5 1’adresse http : / /www . clef — campaign .
org/2 0 0 6 /working_notes/CLEF2 00 6WN—Contents . html

176

Validation auton1atique de reponses

Comme cela a ete dit dans l’introduction, il existe une forte connexion entre AVE et RTE. La
proposition a l’origine d’ AVE etait que l’on pouvait reform11ler la tache de validation de reponse
comme un probleme d’irnplication textuelle. Et, plusie11rs groupes ont d’ ailleurs par1icipe aux
deux evaluations en utilisant la meme approche.

En 2006, a ete organise’ le second RTE. (Bar-Haim et al., 2006) soulignent les partic11larites
des deux systemes q11i ont obtenu les meilleurs resultats. L’un a utilise de facon extensive des
connaissances semantiques, l’autre a favorise l’utilisation de grands corpus d’entrainement.

Dans notre travail, nous ne faisons pas l’hypothese d’une source de connaissances semantiques
existante q11i permettrait des deductions logiques. Aussi, nous reposons nous sur des criteres
ling11istiques, q11i peuvent etre veriﬁes en domaine ouvert, et q11i perrnettent d’exprimer des
relations semantiques entre le sens des mots.

4 Valider des réponses avec un SQR

Notre objectif etait d’utiliser notre propre SQR pour le francais : FRASQUES, et d’utiliser ses
res11ltats, c’est-a-dire a la fois les reponses extraites et les types d’informations de la questions
presentes dans les justiﬁcations, pour evaluer la pertinence des justiﬁcations par rapport aux
hypotheses.

4.1 FRASQUES : notre systéme de questions-réponses pour le frangais

Nous presentons tout d’abord brievement FRASQUES avant de presenter comment il a ete
adapte pour la tache de validation.

Le systeme se divise en 4 composants :

— Analyse de la question : ce premier module effectue l’analyse syntaxique de la question pour
en detecter certaines de ses caracteristiques telles que :

— ses mot-cles, utilises 11lterie11rement lors de la recherche des documents,

— le type attendu de la reponse, q11i peut-etre une entite r1ommee (une personne, un pays, une
date...) ou un type general comme conférence ou adresse,

— le focus de la question, que nous deﬁnissons comme le terme de la question q11i sera vrai-
sernblablement present dans la phrase contenant la reponse,

— le verbe principal de la question.

— Selection des documents : le mote11r de recherche Lucene 5 cherche dans la collection les
documents pertinents.

— Traitement des documents : ce module utilise Fastr 7 pour reconnaitre les variantes linguis-
tiques des termes de la question : par exen1ple, « monnaie del’E11rope » sera reconnue comme
une variante de « monnaie europeenne ». Ens11ite, les entites r1ommees du document sont eti-
quetees, nous utilisons environ une vingtaine de type d’entites nommees. Les phrases conte-
nant au moins une variante des termes de la question sont gardees.

— Extraction de la reponse : ce demier module extrait les reponses precises des phrases candi-
dates. La strategie d’extraction depend du type attendu de la reponse. Si la reponse est une
entite nommee, l’entite r1ommee q11i est du type attendu et q11i est la plus proche des mots de

°Moteur de recherche entierement ecrit en Java http : / / lucene . apache . org/
7http : / /www . limsi . fr/Individu/j acquemi /FASTR/

177

Anne-Laure LIGOZAT, Brigitte GRAU, Isabelle ROBBA, Anne VILNAT

Question (Q)

Analyse de la question

Reconnaissance de :
Mots-clefs
Categorte
Type de reponse attendu
Verbe principal

Passage in juger Traitement des documents

(9

Reconnaissance de Variations
par Faslr

Ettquetage des entttés nommees
Selection de phrases

Extraction de la réponse

Extraction de la réponse precise

Répanses du SQR (R2)
pour le ﬁrm passage

Evaluation de la réponse

FIG. 1 — Architecture du systeme de validation de la réponse

Répcmse in juger (R1)

la question est sélectionnée. Sinon, des patrons d’extraction sont utilisés, ils sont écrits dans
le format Cass 8, un analyseur syntaxique q11i est utilisé ici pour extraire la réponse plutot
que comme analyseur. Ces patrons expriment la position possible de la réponse par rapport
au focus ou au type attendu de la réponse.

4.2 Le systéme de validation des réponses

Le systeme de validation des réponses utilise trois de ces quatre cornposants, ce que montre la
ﬁgure 1. L’entrée du systeme est une paire justiﬁcation-hypothese, ainsi que la question d’ori-
gine Q et la réponse a juger R] . La question est d’ abord analysée p11is le cornposant q11i traite
les documents est appliqué a la justiﬁcation. Le module d’extraction de la réponse extrait les
réponses R2 des passages justiﬁcatifs. Enﬁn, la paire hypothese-justiﬁcation est évaluée en te-
nant compte des différentes informations de l’hypothese trouvées dans l’eXtrait et de la réponse
trouvée par FRASQUES. Le systeme retourne OUI si elle est considérée comme justiﬁée, NON
dans le cas contraire. Un score de conﬁance est également attribué a chaque jugement.

L’ algorithme de décision se déroule en 2 étapes. La premiere a pour but de détecter les erreurs
les plus triviales, par exemple une réponse q11i serait completement incluse dans la question ou
q11i ne serait pas présente dans la justiﬁcation. Dans le cas oh la question contient une date,
le contexte temporel de la question et l’eXtrait sont comparés. Po11r l’instant, le contexte est
formé par les dates reconnues comme telles présentes dans la description du document ou dans
le passage. S’ils sont contradictoires, la paire est rejetée.

Ehttp : / /www . s fs .nphil . u.ni— tuebingen . de/ ~—abney/

178

Validation auton1atique de reponses

La seconde etape consiste en des veriﬁcations plus complexes. Dans un cas ideal, un passage
justiﬁcatif correct correspond a la reformulation de la question sous forme declarative avec
la reponse q11i y est donnee. Chaque terme de la question, ou de l’hypothese, ﬁgure dans le
passage, lies par les memes relations.

En ce q11i conceme les termes, dans la grande n1ajorite des cas, le passage justiﬁcatif ne com-
porte pas tous les termes de la questions sous leur forme d’origine : ils subissent des variations
de differentes natures : ﬂexionnelles, morphologiques, syntaxiques, semantiques ou des combi-
naisons de ces variations si on recherche des groupes nominaux complexes. Dans FRASQUES,
ces variations sont reconnues par Fastr. Parmi les termes de la question, certains jouent un
role plus important. Il en est ainsi de l’objet de la question, que nous appelons focus dans
FRASQUES. Le focus correspond a l’entite sur laquelle porte la question, que l’on en cherche
une caracteristique ou une deﬁnition. Aussi, selon les types de question, le focus n’est pas tou-
jo11rs present, mais s’il l’est, il doit ﬁgurer dans le passage justiﬁcatif. Un autre terme q11i, s’il
est present, a une grande importance, est le type de reponse attendu, quand ce type n’est pas un
nom d’entite nommee. Ce type est nomme type general. Ainsi, dans « De quel parti politique
Lionel Jospin est-il membre ? » Le focus est « Lionel Jospin » et le type general est « parti
politique ». Lorsqu’il est present dans le passage reponse, le type general sera souvent place a
proximite de la reponse ou meme fera paI1ie de celle-ci, comme dans « Lionel J ospin, membre
du parti socialiste ».

Lorsqu’il s’ agit du verbe, celui-ci a tendance a subir plus de variations que les termes nominaux ;
il est souvent exprime par une preposition ou un verbe proche mais non synonyme. C’est le cas
par exemple si on demande « q11i a realise un ﬁlm » et que la reponse est exprimee par « le ﬁlm
de X  » ou « quelle entreprise a change son nom » et la reponse est donnee par « le groupe X
a adopte le nom de la ﬁliale  ». On retrouve ici les variations traitees par (Lin & Pantel, 2001).
Ne disposant pas de telles resso11rces, nous avons considere que l’absence du verbe n’inﬂuerait
pas s11r la decision ﬁnale.

Enﬁn les derniers types de termes jouant un role primordial sont les non1s propres : ils sont
toujours presents dans le passage et subissent peu de variations, sauf en ce q11i conceme les
non1s de pays souvent repris par l’adjectif correspondant, comme dans « q11i est le president de
l’Egypte » avec « le president egyptien » repris dans le passage.

En ce q11i concerne les relations entre termes, celles-ci seront souvent veriﬁees par le11r mani-
festation en langue, c’est-a-dire par un ensemble de relations syntaxiques. Nous avons vu que
certains travaux s’app11ient s11r une notion de distance syntaxique. Mais pour cela, il est neces-
saire de disposer d’ une analyse complete des phrases. Aﬁn de ne pas reposer sur cette hypothese
souvent non veriﬁee, nous avons choisi de ne veriﬁer que certaines relations en les exprimant
sous forme de patrons d’extraction. Ces relations sont celles q11i lient la reponse avec certains
elements de la phrase : le focus ou le type general.

L’ element preponderant, malgre tout, reste la reponse : est-elle du type attendu ou non ? Lorsque
ce type est une entite nommee, la veriﬁcation consistera a retrouver une entite nommee d’un
type adequat. Lorsque cel11i-ci est designe par le type general, ou bien il ﬁgure a proximite de la
reponse, ou bien il est iInplicite et la reponse en est une instance. Cette relation d’ instanciation
pourrait etre inferee par l’utilisation de ressources externes, par exemple Wilcipedia, q11i possede
un grand nombre de categories et de deﬁnitions leur correspondant.

La mise en oeuvre de ces criteres de justiﬁcation donne lieu dans notre systeme a un calc11l
de 2 scores q11i permet ens11ite de conclure positivement ou negativement. Le premier score

179

Anne-Laure LIGOZAT, Brigitte GRAU, Isabelle ROBBA, Anne VILNAT

porte sur l’évaluation de la correspondance entre la réponse trouvée par notre systeme R2 et
la réponse proposée dans l’hypothese R] . Si FRASQUES trouve une réponse différente, alors
la paire hypothése-justiﬁcation est réfutée. Si les 2 réponses sont proches ou s’il n’y a pas de
réponse trouvée par FRASQUES, la décision va étre conditionnée par la présence des différents
termes que nous avons privilégiés. Le score attribué a l’évaluation de la qualité de la réponse
sera positif pour une réponse exacte ou approchée, et négatif quand la distance est assez grande,
par exemple, l’approxirnation d’ une date ou d’une quantité par un nombre.

Le deuxieme score évalue les termes présents. Il est calculé en combinant le nombre de criteres
présents et le11rs vale11rs. Il est négatif si aucun des criteres n’est trouvé, et positif sinon.

Un passage constitue une justiﬁcation acceptable :

— si R2 est absente et le score des termes est positif. Ce demier foumit le score ﬁnal,

— si R2 = R] et il y a des criteres présents. Dans ce cas le score ﬁnal est la valeur rnaximale des

deux criteres,

— si les 2 scores vont dans des sens opposés, on prend le meille11r des deux, s’il est positif.
Regardons l’eXernple suivant :

Justiﬁcation : Trois candidats, Tony Blair, Margaret Beckett et John Prescott, se
disputeront la succession de John Smith a la téte du parti travailliste, a armoncé le
Labour jeudi, a l’issue du processus de nominations des candidats par les députés
du parti. M. Blair, rninistre de l’Intérieur du cabinet fantome représentant l’a

Hypothése : le par1i politique de Tony Blair, le LABOUR .

Dans ce passage, tous les termes de la question sont présents (Tony Blair, parti politique), mais
le type de la réponse Labour n’est pas étiqueté par notre SQR comme une entité nommée de
type organisation. De ce fait, l’algorithme de décision recoit 2 scores opposés, dans cet exemple
prenant en cornpte le non-marquage de Labour comme une organisation, il répond negative-
ment.

4.3 Résultats

Le corpus d’évaluation contenait 3266 paires, parmi lesquelles 202 paires n’ont pas été ju-
gées. Les hypotheses étant formées autornatiquement, elles cornportaient beaucoup d’erreurs
de syntaxe, aussi nous sommes-nous fondés uniquement s11r les questions, l’hypothese ne nous
perrnettant que d’ extraire la réponse.

Lors de notre participation at AVE, beaucoup d’erreurs restaient dans nos programmes, q11i ont
été corrigés dep11is. Les organisateurs ayant fourni les valeurs de validation attendues pour
chaque paire du corpus, nous avons pu réévaluer notre chaine. Un examen approfondi de ces
résultats nous a permis de constater qu’il y avait certaines erreurs s11r ces valeurs, notamment
en ce q11i conceme les réponses positives : des réponses exactes aux questions n’étaient pas du
tout validées par le passage justiﬁcatif. Nous avons corrigé 82 d’ entre elles dans le corpus.

La table 1 présente nos résultats s11r la version ofﬁcielle, les corrections apportées au corpus
ne modiﬁant pas les ordres de grandeur des résultats. La premiere ligne donne le nombre de
paires évaluées positivement et négativement par les juges humains. La seconde ligne contient
tous nos résultats et la suivante le nombre de nos résultats corrects. La demiere ligne contient
le rappel et la f-mesure correspondant a ces résultats en utilisant la forrn11le exposée dans la
section 2.

180

Validation auton1atique de reponses

# OUI # NON Total
Evalues par les organisateurs 705 2359 3064
Tous nos res11ltats 142 2922 3064
Nos res11ltats corrects 82 2266 2348
Precision 0.58 0.77
Rappel 0.12 0.96
F-mesure 0.2 0.85

TAB. 1 — AVE results at CLEF 2006

Parmi nos reponses NON, nous distinguons celles q11i sont sﬁres des autres : ce sont les refuta-
tions decidees lors de la premiere etape presentees dans la section 4.2. La reponse est consideree
comme etant non justiﬁee et ce de facon sﬁre, donc avec un score de conﬁance eleve. Nous avons
trouve 1637 paires de « NON » sﬁrs. Parmi elles, 1415 etaient bien jugees, la precision pour ces
reponses est donc de 0,87.

La seconde observation est que notre systeme a plus de facilites pour refuter les justiﬁcations
plutot que pour les accepter. La precision et le rappel de nos reponses negatives sont bons. Et
nous nous tron1pons rarement quand nous donnons des reponses OUI, mais nous en trouvons
tres peu, notre rappel est donc tres faible s11r ces reponses.

Certaines erreurs pourraient étre corrigees en approfondissant les veriﬁcations des relations por-
tant s11r la reponse. Par exemple, pour la question « Quel est le nom de la femme de George W
Bush ? », une des hypotheses construites etait « Norman Schwarzkopf; la femme de George W
Bush. ». On pourrait alors interroger le Web avec la requétefemme de George W Bush et consta-
ter que la ou les reponses obtenues sont fortement incompatibles avec Norman Schwarzkopf.

5 Conclusion

Nous avons presente une strategie de validation des reponses issues d’un SQR. Cette strategie est
fondee s11r FRASQUES, notre propre SQR monolingue : l’hypothese et l’extrait sont analyses
par FRASQUES et nous utilisons des criteres q11i permettent de detecter si l’extrait justiﬁe
ou non la reponse. Dans notre evaluation des paires hypotheses-extrait, nous distinguons avec
une bonne precision les cas dans lesquels l’extrait ne justiﬁe pas la reponse. Des possibilites
d’eXtension de notre strategie, utilisant des ressources externes et nous perrnettant d’acquerir de
nouvelles connaissances ont egalement ete presentees.

Cette premiere experience en validation de reponses constitue une etape vers la validation semi-
auton1atique en questions-reponses. Elle nous permettra a terme d’ameliorer les performances
de notre SQR puisque certains des criteres que nous utilisons pour la validation n’y avaient pas
ete n1is en oeuvre.

Références

BAR-HAIM R., DAGAN 1., DOLAN B., FERRO L., GIAMPICCOLO D., MAGNINI B. & Sz-
PEKTOR I. (2006). The second pascal recognising textual entailment challenge. In The Second

181

Anne-Laure LIGOZAT, Brigitte GRAU, Isabelle ROBBA, Anne VILNAT

PASCAL Challenges Workshop on Recognising Textual Entailment.

FABRE C. & JACQUEMIN C. (2000). Boosting Variant Recognition with Light Semantics. In
Pmceedings of I 8th International Conference on Computational Linguistics (COLING-2000),
Sarrebrﬁck, Allemagne.

FERRANDEZ 0., TEROL R. M., MUNOZ R., MARTINEZ-BARCO P. & PALOMAR M. (2006).
A knowledge-based textual entailment approach applied to the qa answer validation at clef
2006. In Workshop CLEF 2006, Alicante, Spain.

JONES K. S. & TAIT J . I. (1984). Automatic Search Term Variant Generation. Journal of
Documentation, p. 50-66.

KOUYLEKOV M., NEGRI M., MAGNINI B. & COPPOLA B. (2006). Towards entailment-
based question answering : Itc-irst at clef 2006. In Workshop CLEF 2006, Alicante, Spain.
LIN D. (2005). Dependancy-based evaluation of minipar. In Workshop on the Evaluation of
Parsing Systems, Southampton, UK.

LIN D. & PANTEL P. (2001). Discovery of inference rules for question-answering. Natural
Language Engineering, 7(04), 343-360.

PENAS A., RODRIGO A. , SAMA V. & VERDEJO F. (2006). Overview of the answer validation
exercise 2006. In Workshop CLEF 2006, Alicante, Spain.

TATU M., ILES B. & MOLDOVAN D. (2006). Automatic answer validation using cogex. In
Workshop CLEF 2006, Alicante, Spain.

VOORHEES E. M. (1999). TREC-8 Question Answering Track Evaluation. In Proceedings of
the Eighth Text REtrieval Conference (TREC-8) : Department of Commerce, National Institute
of Standards and Technology.

182

