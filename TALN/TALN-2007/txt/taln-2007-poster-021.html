<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
"http://www.w3.org/TR/html4/loose.dtd">
<html><head><title>Apprentissage symbolique de grammaires et traitement automatique des langues</title>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
</head>
<body>
<div style="page-break-before:always; page-break-after:always"><div><p>TALN 2007, Toulouse, 5&#8211;8 juin 2007
</p>
<p>Apprentissage symbolique de grammaires
et traitement automatique des langues
</p>
<p>Erwan MOREAU
LINA - FRE 2729, Universit&#233; de Nantes
</p>
<p>2 rue de la Houssini&#232;re, BP 92208, F-44322 Nantes cedex 3
Erwan.Moreau@univ-nantes.fr
</p>
<p>R&#233;sum&#233;. Le mod&#232;le de Gold formalise le processus d&#8217;apprentissage d&#8217;un langage. Nous
pr&#233;sentons dans cet article les avantages et inconv&#233;nients de ce cadre th&#233;orique contraignant,
dans la perspective d&#8217;applications en TAL. Nous d&#233;crivons bri&#232;vement les r&#233;centes avanc&#233;es
dans ce domaine, qui soul&#232;vent selon nous certaines questions importantes.
</p>
<p>Abstract. Gold&#8217;s model formalizes the learning process of a language. In this paper we
present the advantages and drawbacks of this restrictive theoretical framework, in the viewpoint
of applications to NLP.We briefly describe recent advances in the domain which, in our opinion,
raise some important questions.
</p>
<p>Mots-cl&#233;s : apprentissage symbolique, mod&#232;le de Gold, grammaires cat&#233;gorielles.
Keywords: symbolic learning, Gold&#8217;s model, categorial grammars.
</p>
<p>1 Introduction
</p>
<p>L&#8217;apprentissage symbolique automatique de grammaires pour les langues naturelles est un do-
maine relativement m&#233;connu, assez peu &#233;tudi&#233; et tr&#232;s peu avanc&#233; sur le plan applicatif. Cet &#233;tat
de fait s&#8217;explique assez facilement : tout d&#8217;abord, il s&#8217;agit d&#8217;une t&#226;che extr&#234;mement complexe,
aussi bien dans sa d&#233;finition pr&#233;cise que dans sa mise en &#339;uvre. Ce sont surtout les aspects
th&#233;oriques qui en sont &#233;tudi&#233;s, et il semble jusqu&#8217;&#224; pr&#233;sent tr&#232;s difficile d&#8217;y obtenir des r&#233;sul-
tats pratiques dignes d&#8217;int&#233;r&#234;t (pour le langage naturel). D&#8217;un point de vue optimiste, la relative
lenteur &#224; passer du stade de l&#8217;&#233;tude th&#233;orique au stade des applications dans ce domaine s&#8217;ex-
plique par sa grande complexit&#233;. En ce sens, ce domaine serait simplement encore trop jeune
scientifiquement, mais pourrait prendre de l&#8217;ampleur &#224; l&#8217;avenir une fois que les bases en seront
bien &#233;tablies. Mais d&#8217;un point de vue pessimiste, la complexit&#233; excessive de la t&#226;che peut &#234;tre
vue tout simplement comme un obstacle r&#233;dhibitoire &#224; d&#8217;&#233;ventuelles applications.
</p>
<p>Pourtant ce domaine est potentiellement riche en applications, si toutefois on admet l&#8217;hypoth&#232;se
quelque peu id&#233;aliste selon laquelle il est possible de construire un algorithme d&#8217;apprentissage
&#171; parfait &#187;. Celui-ci serait donc capable de donner une grammaire pr&#233;cise d&#8217;un langage naturel,
pourvu qu&#8217;on lui fournisse un nombre suffisant de phrases appartenant &#224; celui-ci. Sous cette
hypoth&#232;se, la premi&#232;re application (et la plus &#233;vidente) est l&#8217;analyse syntaxique, elle-m&#234;me uti-
lis&#233;e sous diff&#233;rentes formes dans de nombreux outils de traitement des langues. On pourrait
</p>
<p>213</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Erwan MOREAU
</p>
<p>alors envisager de construire assez facilement des analyseurs, y compris pour des langues pour
lesquelles peu d&#8217;&#233;tudes linguistiques existent. On peut &#233;galement penser &#224; coupler l&#8217;analyse et
l&#8217;apprentissage, de fa&#231;on &#224; mieux prendre en compte la cat&#233;gorie syntaxique des mots inconnus
de l&#8217;analyseur. D&#8217;autres applications li&#233;es &#224; l&#8217;analyse, telles que la correction orthographique
et syntaxique, sont &#233;galement susceptibles de b&#233;n&#233;ficier des apports de l&#8217;apprentissage auto-
matique. Si l&#8217;on dispose aussi des moyens permettant de g&#233;rer l&#8217;aspect s&#233;mantique des langues,
l&#8217;autre grande application de l&#8217;apprentissage est la g&#233;n&#233;ration (passage du sens d&#8217;un &#233;nonc&#233; &#224; sa
r&#233;alisation dans une langue pr&#233;cise). Celle-ci est elle-m&#234;me proche du probl&#232;me de la traduction
automatique, qui est bien s&#251;r une application d&#8217;une tr&#232;s grande utilit&#233;.
</p>
<p>L&#8217;inf&#233;rence grammaticale d&#233;signe la probl&#233;matique qui consiste &#224; apprendre des langages &#224;
partir de donn&#233;es. Tout cadre formel pour ce probl&#232;me doit donc avant tout d&#233;finir les termes
apprentissage, langages et donn&#233;es, c&#8217;est-&#224;-dire r&#233;pondre aux questions suivantes : nature des
donn&#233;es dont on dispose ? simples s&#233;quences de mots (cha&#238;nes), arbres, termes, graphes ou tout
autre type de structures, mais aussi quantit&#233;, qualit&#233;, compl&#233;tude des donn&#233;es. Type de langages
consid&#233;r&#233;, et repr&#233;sentation des langages ? restrictions &#233;ventuelles, niveau d&#8217;abstraction (e.g.
sans contrainte particuli&#232;re sur la relation entre langages et grammaires, ou au contraire forma-
lisme grammatical pr&#233;cis). Nature du processus d&#8217;inf&#233;rence ? Fini ou non, Solution unique ou
multiple, processus automatique ou semi-automatique, r&#233;sultat pr&#233;cis ou approximation, limites
&#233;ventuelles sur le temps ou le nombre d&#8217;essais.
</p>
<p>Le mod&#232;le d&#8217;identification &#224; la limite, aussi appel&#233; du nom de son auteur mod&#232;le de Gold,
est l&#8217;une des principales repr&#233;sentations formelles du processus d&#8217;apprentissage. La premi&#232;re
d&#233;finition en est donn&#233;e dans (Gold, 1967). L&#8217;auteur lui-m&#234;me est d&#8217;abord pessimiste quant &#224;
l&#8217;int&#233;r&#234;t de ce mod&#232;le, &#224; cause de l&#8217;apparente impossibilit&#233; d&#8217;y obtenir des r&#233;sultats positifs pour
des classes de langages &#171; int&#233;ressantes &#187;. Plus tard, les r&#233;sultats positifs obtenus par Angluin
dans ce mod&#232;le montreront sa pertinence (Angluin, 1980). Le mod&#232;le sera ensuite &#233;tudi&#233; plus
en d&#233;tail : ainsi, plusieurs autres r&#233;sultats encourageants viendront soutenir l&#8217;id&#233;e que l&#8217;iden-
tification &#224; la limite constitue bien un cadre th&#233;orique adapt&#233; &#224; la repr&#233;sentation du processus
d&#8217;apprentissage, en particulier celui des langages, voire des langues naturelles.
</p>
<p>La question de la pertinence du mod&#232;le de Gold par rapport &#224; l&#8217;acquisition humaine du lan-
gage est plut&#244;t bien &#233;tudi&#233;e au niveau linguistique et cognitif (Johnson, 2004), mais cette m&#234;me
question est assez peu discut&#233;e dans la perspective de l&#8217;apprentissage symbolique automatique.
C&#8217;est pourquoi nous proposons ici une relecture des principaux r&#233;sultats li&#233;s &#224; ce mod&#232;le, vu
sous l&#8217;angle du traitement automatique des langues. Dans cet article nous essaierons donc d&#8217;ex-
pliquer de fa&#231;on concise et claire les bases, les outils et les enjeux du mod&#232;le de Gold par rapport
au TAL. Nous proposons ensuite un point de vue particulier sur les int&#233;r&#234;ts et limites des r&#233;sul-
tats obtenus jusqu&#8217;ici dans ce cadre, en tentant de donner un peu de recul &#224; cette modeste &#233;tude.
L&#8217;objectif de cet article est donc aussi de soumettre &#224; la discussion quelques questions relatives
au domaine, qui nous semblent pertinentes compte tenu de ses r&#233;centes &#233;volutions.
</p>
<p>2 Identification &#224; la limite
</p>
<p>Le principe de l&#8217;identification &#224; la limite est la convergence : &#224; partir d&#8217;une s&#233;quence infinie
d&#8217;&#233;l&#233;ments qui caract&#233;risent le langage &#224; deviner, l&#8217;apprenant &#233;met des hypoth&#232;ses. Ces hypo-
th&#232;ses prennent la forme d&#8217;une grammaire, cens&#233;e correspondre au langage observ&#233; jusqu&#8217;alors
par l&#8217;apprenant. Comme l&#8217;&#233;num&#233;ration est infinie, l&#8217;apprenant r&#233;pond lui aussi sous forme d&#8217;une
</p>
<p>214</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Apprentissage symbolique de grammaires et TAL
</p>
<p>s&#233;quence infinie de grammaires hypoth&#232;ses. Finalement, l&#8217;apprentissage est r&#233;ussi si, &#224; partir
d&#8217;un certain point, l&#8217;apprenant &#233;met toujours la m&#234;me hypoth&#232;se (convergence), et que celle-ci
correspond bien au langage attendu. Le fait que l&#8217;apprenant ignorera toujours s&#8217;il a atteint ou
non la solution est un aspect important de ce formalisme. Gold en donne la justification (d&#8217;ordre
linguistique) suivante : &#171; une personne ne sait jamais si elle parle correctement un langage. &#187;.
</p>
<p>Une classe de langages est un ensemble de langages1 fix&#233;, parfois aussi appel&#233; famille de lan-
gages. G&#233;n&#233;ralement il s&#8217;agit d&#8217;un ensemble de langages partageant une propri&#233;t&#233; particuli&#232;re.
L&#8217;apprenabilit&#233;2 d&#8217;une classe de langages d&#233;signe son aptitude &#224; &#234;tre apprise selon la d&#233;finition
2.1 ci-dessous.
</p>
<p>Un syst&#232;me de grammaires est sp&#233;cifi&#233; par un triplet &#12296;U ,G,M&#12297;, dans lequel l&#8217;univers U est
un ensemble d&#8217;objets, G un ensemble de grammaires et M une fonction qui associe &#224; chaque
grammaire de G un sous-ensemble de U . Dans un syst&#232;me de grammaires &#12296;U ,G,M&#12297;, une fonc-
tion d&#8217;apprentissage est une fonction partielle &#966; qui associe &#224; des s&#233;quences finies non-vides
d&#8217;objets de U des grammaire de G.
</p>
<p>D&#233;finition 2.1 (Identification &#224; la limite) Soit &#12296;U ,G,M&#12297; un syst&#232;me de grammaires, &#966; une
fonction d&#8217;apprentissage et L &#8838; U un langage. Soit &#12296;a0, a1, a2, . . .&#12297; une s&#233;quence infinie d&#8217;ob-
jets de U , telle que a &#8712; { ai | i &#8712; N } si et seulement si a &#8712; L.
&#966; converge vers G s&#8217;il existe n &#8712; N tel que pour tout i &#8805; n Gi = &#966;(&#12296;a1, a2, . . . , ai&#12297;) est d&#233;finie
et Gi = G.
</p>
<p>&#966; apprend un langage L si, pour toute &#233;num&#233;ration de L, &#966; converge vers une grammaire G
telle queM(G) = L.
Une classe de langages L &#8838; P(U) est dite apprenable s&#8217;il existe une fonction d&#8217;apprentissage
&#966; telle que &#966; apprend L pour tout langage L &#8712; L.
</p>
<p>On voit dans cette d&#233;finition que la s&#233;quence d&#8217;exemples a quelques caract&#233;ristiques notables :
</p>
<p>&#8211; Elle ne contient que des exemples positifs, c&#8217;est-&#224;-dire des &#233;l&#233;ments du langage. La fonc-
tion d&#8217;apprentissage n&#8217;a donc aucune information ext&#233;rieure au langage, ce qui constitue la
principale difficult&#233; de cette forme d&#8217;apprentissage.
</p>
<p>&#8211; La s&#233;quence d&#8217;exemples est suppos&#233;e ne comporter aucune erreur3.
&#8211; La s&#233;quence d&#8217;exemples est une &#233;num&#233;ration du langage : tous les objets du langage doivent
</p>
<p>obligatoirement y appara&#238;tre.
&#8211; Les exemples peuvent appara&#238;tre dans un ordre quelconque dans la s&#233;quence, et &#233;ventuelle-
</p>
<p>ment plusieurs fois (ce qui permet notamment d&#8217;&#233;num&#233;rer ind&#233;finiment un langage fini).
</p>
<p>Dans ce mod&#232;le, la convergence d&#8217;une fonction d&#8217;apprentissage n&#8217;a d&#8217;int&#233;r&#234;t que si elle s&#8217;ap-
plique &#224; un ensemble de langages, et non &#224; un seul langage. Intuitivement, plus la classe de
langages est grande, plus il est difficile de reconna&#238;tre un langage pr&#233;cis dans cette classe.
</p>
<p>1Dans la litt&#233;rature, le terme langage est fr&#233;quemment d&#233;fini comme un ensemble de phrases, chaque phrase
&#233;tant une s&#233;quence finie de mots. Mais dans la mesure o&#249; on peut envisager diff&#233;rents niveaux de repr&#233;sentation
de la phrase, nous d&#233;finissons un langage comme un ensemble d&#8217;objets (ce terme abstrait laissant volontairement
la possibilit&#233; d&#8217;utiliser diff&#233;rents types d&#8217;&#233;l&#233;ments : arbres, structures, etc.).
</p>
<p>2On trouve aussi dans la litt&#233;rature diff&#233;rents termes d&#233;signant le caract&#232;re apprenable d&#8217;une classe de langages :
inf&#233;rable, identifiable [&#224; la limite] ou acqui&#233;rable (le terme acquisition fait cependant plus souvent r&#233;f&#233;rence &#224;
l&#8217;apprentissage humain du langage).
</p>
<p>3Ce qui limite les applications potentielles : ce mod&#232;le est par d&#233;finition inadapt&#233; aux donn&#233;es bruit&#233;es.
</p>
<p>215</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Erwan MOREAU
</p>
<p>Cette d&#233;finition de l&#8217;identification &#224; la limite a d&#8217;importantes cons&#233;quences imm&#233;diates, d&#233;mon-
tr&#233;es par Gold dans (Gold, 1967). La premi&#232;re est un r&#233;sultat positif pour les langages finis :
La classe des langages de cardinalit&#233; finie est apprenable. En effet, intuitivement il suffit dans
ce cas que l&#8217;apprenant ajoute un par un les exemples pr&#233;sent&#233;s &#224; la grammaire hypoth&#232;se : la
fonction d&#8217;apprentissage converge d&#232;s que le langage a &#233;t&#233; &#233;num&#233;r&#233; en totalit&#233;. En revanche
la seconde cons&#233;quence de la d&#233;finition du mod&#232;le est un r&#233;sultat n&#233;gatif : toute classe de lan-
gages contenant tous les langages finis et au moins un langage infini n&#8217;est pas apprenable. Nous
illustrons ce r&#233;sultat &#224; l&#8217;aide de l&#8217;exemple ci-dessous :
</p>
<p>Exemple 2.1 (Langages r&#233;guliers) Pour tout n &#8805; 1 on d&#233;finit Ln = { xi | i &#8804; n } comme
le langage des cha&#238;nes de x de longueur inf&#233;rieure ou &#233;gale &#224; n. Soit L&#8734; = x&#8727; le langage de
toutes les cha&#238;nes de x.
</p>
<p>Supposons que la s&#233;quence d&#8217;exemples commence par &#12296;x, xx, xxx, . . .&#12297; :
&#8211; Si l&#8217;apprenant est prudent, il ne propose jamais comme hypoth&#232;se un langage qui va au del&#224;
</p>
<p>des exemples propos&#233;s : il propose donc Lk, avec k la longueur maximale parmi les exemples
vus. Cet algorithme ne peut jamais trouver L&#8734;.
</p>
<p>&#8211; Si &#224; l&#8217;inverse l&#8217;algorithme &#171; g&#233;n&#233;ralise &#187;, alors &#224; partir d&#8217;un certain point il propose L&#8734;.
Mais c&#8217;est une erreur s&#8217;il s&#8217;av&#232;re que la s&#233;quence ne d&#233;passe pas une certaine longueur de
phrase.
</p>
<p>Une erreur est donc possible dans les deux cas, et rien ne permet de faire le bon choix : si la
classe de langage contient tous les Ln et L&#8734;, celle-ci n&#8217;est pas apprenable.
</p>
<p>Les langages { Ln | n &#8712; N } et L&#8734; d&#233;finis dans l&#8217;exemple 2.1 &#233;tant tous r&#233;guliers, toutes les
classes de la hi&#233;rarchie de Chomsky les contiennent, et ne sont par cons&#233;quent pas apprenables.
Le fait que m&#234;me la classe la plus simple de la hi&#233;rarchie de Chomsky, celle des langages
r&#233;guliers, ne soit pas apprenable dans le mod&#232;le de Gold a longtemps constitu&#233; un obstacle ma-
jeur au d&#233;veloppement du mod&#232;le, consid&#233;r&#233; comme trop contraignant. Gold lui-m&#234;me notait :
&#171; Cependant, les r&#233;sultats pr&#233;sent&#233;s dans la derni&#232;re section montrent que seule la classe de
langages la plus triviale 4 consid&#233;r&#233;e5 est apprenable &#224; partir d&#8217;exemples positifs[..]. &#187;
</p>
<p>L&#8217;exemple 2.1 illustre la principale difficult&#233; de l&#8217;apprentissage &#224; partir d&#8217;exemples positifs, &#224;
savoir la surg&#233;n&#233;ralisation. La surg&#233;n&#233;ralisation est l&#8217;erreur qui consiste &#224; trop g&#233;n&#233;raliser (ex-
trapoler) &#224; partir des donn&#233;es fournies, ce qui signifie inf&#233;rer un langage qui est un sur-ensemble
strict du langage cible. Par exemple, on peut supposer que l&#8217;ensemble {11, 23, 5, 17, 7} est le
d&#233;but d&#8217;une &#233;num&#233;ration de l&#8217;ensemble des nombres impairs. Mais s&#8217;il s&#8217;agit en fait de l&#8217;en-
semble des nombres premiers sup&#233;rieurs &#224; 2, alors il y a surg&#233;n&#233;ralisation : l&#8217;ensemble des
nombres repr&#233;sent&#233; est un sous-ensemble (strict) de l&#8217;ensemble propos&#233;. Comme on ne dispose
que d&#8217;exemples positifs, il n&#8217;y aura jamais de contre-exemple dans la s&#233;quence permettant de
corriger l&#8217;erreur. Bien entendu, la g&#233;n&#233;ralisation est indispensable dans le processus d&#8217;appren-
tissage, puisqu&#8217;une m&#233;thode d&#8217;apprentissage &#171; trop prudente &#187; qui ne g&#233;n&#233;raliserait jamais ne
ferait pas de v&#233;ritable apprentissage (au sens d&#8217;une d&#233;couverte de quelque chose de nouveau) :
il s&#8217;agirait simplement d&#8217;une sorte de compilation des exemples propos&#233;s. Surtout, il est &#233;vident
qu&#8217;une telle m&#233;thode serait incapable d&#8217;identifier un langage infini.
</p>
<p>Sauf cas particuliers, la g&#233;n&#233;ralisation doit donc bien &#234;tre utilis&#233;e au cours de l&#8217;apprentissage.
La question qui se pose d&#8217;un point de vue algorithmique est : quand faut-il g&#233;n&#233;raliser ? (ou
</p>
<p>4On peut consid&#233;rer de mani&#232;re informelle qu&#8217;une classe de langages est non triviale (pour l&#8217;apprentissage) si
elle comporte au moins un nombre infinis de langages, dont certains sont infinis.
</p>
<p>5Il s&#8217;agit de la classe des langages de cardinalit&#233; finie.
</p>
<p>216</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Apprentissage symbolique de grammaires et TAL
</p>
<p>quand faut-il ne pas g&#233;n&#233;raliser, selon ce qu&#8217;on consid&#232;re comme &#233;tant l&#8217;action par d&#233;faut). Mais
avant de se poser cette question, il faut s&#8217;assurer qu&#8217;il est possible de savoir quand g&#233;n&#233;raliser,
car lorsqu&#8217;on ne dispose pas d&#8217;exemples n&#233;gatifs on n&#8217;a aucun indice sur la position de la
fronti&#232;re du langage &#224; deviner. C&#8217;est pr&#233;cis&#233;ment ce point qui pose probl&#232;me au d&#233;part avec
l&#8217;identification &#224; la limite &#224; partir d&#8217;exemples positifs : m&#234;me les classes de langages qu&#8217;on
croyait simples (les langages r&#233;guliers, voir exemple 2.1) ne sont pas apprenables, parce qu&#8217;on
ne peut pas savoir quand [ne pas] g&#233;n&#233;raliser.
</p>
<p>3 Techniques th&#233;oriques d&#8217;apprentissage de grammaires
</p>
<p>3.1 Ensembles r&#233;v&#233;lateurs
</p>
<p>Au d&#233;but des ann&#233;es 1980, Angluin apporte au mod&#232;le de Gold ses premiers r&#233;sultats posi-
tifs : dans (Angluin, 1980), elle propose de &#171; consid&#233;rer le cas particulier d&#8217;inf&#233;rence &#224; partir
d&#8217;exemples positifs qui &#233;vite la surg&#233;n&#233;ralisation [et donne] des conditions suffisantes pour
cela. &#187; Le crit&#232;re qu&#8217;elle propose a donn&#233; lieu ensuite &#224; de nombreuses utilisations ou ex-
tensions, d&#233;montrant finalement la richesse du mod&#232;le de Gold. Sommairement, un ensemble
r&#233;v&#233;lateur (telltale set) est une sorte de &#171; signature &#187; d&#8217;un langage qui le distingue de tous
les autres langages de la classe dont il est un sur-ensemble strict. Ainsi, lorsque cette signature
appara&#238;t dans la s&#233;quence d&#8217;exemples, on peut proposer ce langage sans risque de surg&#233;n&#233;ralisa-
tion. Formellement, soit L une classe de langages : un ensemble fini d&#8217;objetsD est un ensemble
r&#233;v&#233;lateur du langage L &#8712; L si D &#8838; L et L&#8242; &#8834; L&#8658; D ! L&#8242; pour tout langage L&#8242; &#8712; L.
</p>
<p>Th&#233;or&#232;me 3.1 (Angluin) Soit L &#8838; P(U) une famille index&#233;e de langages r&#233;cursifs6 dans le
syst&#232;me de grammaires &#12296;U ,G,M&#12297; : L = {M(G) |G &#8712; {G0, G1, G2, . . .} }.
Il existe une fonction &#966; qui apprend L si et seulement s&#8217;il existe un algorithme calculable qui,
pour tout indice I tel que LI =M(GI) &#8712; L &#233;num&#232;re un ensemble r&#233;v&#233;lateur de LI .
</p>
<p>Supposons qu&#8217;il existe un algorithme EnumRevel(I, n) qui &#233;num&#232;re r&#233;cursivement les n pre-
miers &#233;l&#233;ments d&#8217;un ensemble r&#233;v&#233;lateur DI de LI .
</p>
<p>&#966;(&#12296;a0, . . . , an&#12297;)
i&#8592; 0
E &#8592; EnumRevel(i, n)
Tant que (i &#8804; n et non({a0, . . . , an} &#8838;M(Gi) et E &#8838; {a0, . . . , an})) faire
</p>
<p>i&#8592; i+ 1
E &#8592; EnumRevel(i, n)
</p>
<p>Fin Tant Que
Renvoyer Gi
</p>
<p>L&#8217;algorithme ci-dessus apprend la classe L de la fa&#231;on suivante : un ensemble r&#233;v&#233;lateur &#233;tant
n&#233;cessairement fini, pour tout i il existe une &#233;tape n &#224; partir de laquelle l&#8217;ensemble r&#233;v&#233;lateur
E de Li est &#233;num&#233;r&#233; en totalit&#233;. Dans ce cas, la boucle s&#8217;arr&#234;tera sur la premi&#232;re grammaire
Gi telle que les exemples fournis appartiennent au langage de la grammaire d&#8217;une part, et dont
</p>
<p>6Classe de langage pour laquelle le probl&#232;me de l&#8217;appartenance (x &#8712; L) est d&#233;cidable.
</p>
<p>217</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Erwan MOREAU
</p>
<p>l&#8217;ensemble r&#233;v&#233;lateur est enti&#232;rement inclus dans les exemples d&#8217;autre part. Ainsi il est impos-
sible que le langage cible soit un sous-ensemble strict deM(Gi) (surg&#233;n&#233;ralisation). Si aucun
contre-exemple aj /&#8712;M(Gi) n&#8217;appara&#238;t par la suite, l&#8217;algorithme s&#8217;arr&#234;tera toujours sur Gi.
L&#8217;apprentissage par &#233;num&#233;ration, illustr&#233; ci-dessus, d&#233;signe une m&#233;thode g&#233;n&#233;rale qui consiste
&#224; faire une recherche syst&#233;matique dans l&#8217;ensemble des grammaires possibles, jusqu&#8217;&#224; en trou-
ver une qui v&#233;rifie une propri&#233;t&#233; particuli&#232;re, et la renvoyer comme hypoth&#232;se. L&#8217;&#233;num&#233;ration
n&#8217;est pas une m&#233;thode d&#8217;apprentissage universelle, parce qu&#8217;il existe des classes de langages
apprenables qui ne sont pas apprenables par &#233;num&#233;ration (Costa Flor&#234;ncio, 2003). Il va de soi
que ce type d&#8217;algorithme est totalement inutilisable en pratique : m&#234;me si on peut am&#233;liorer
sensiblement la m&#233;thode (notamment en &#233;vitant de faire l&#8217;&#233;num&#233;ration compl&#232;te apr&#232;s chaque
exemple), le seul fait d&#8217;avoir &#224; parcourir de fa&#231;on exhaustive l&#8217;ensemble des grammaires poten-
tielles est r&#233;dhibitoire. En effet, imaginons une repr&#233;sentation textuelle simple des grammaires
(de type grammaires syntagmatiques), de fa&#231;on &#224; les &#233;num&#233;rer selon l&#8217;ordre de taille puis lexi-
cographique : en premi&#232;re approximation il existe de l&#8217;ordre de nm grammaires diff&#233;rentes de
taillem, avec n le nombre total de symboles (comprenant entre autres tous les mots du vocabu-
laire). Ce type de fonctionnement est &#233;videmment radicalement inadapt&#233; &#224; l&#8217;apprentissage de
langues naturelles7. Il faut donc souligner cette caract&#233;ristique essentielle du mod&#232;le de Gold :
il s&#8217;agit avant tout d&#8217;un mod&#232;le th&#233;orique, qui ne garantit que la d&#233;cidabilit&#233; du probl&#232;me de
l&#8217;apprentissage. Autrement dit, le fait qu&#8217;une classe de langages soit apprenable n&#8217;implique en
aucun cas la faisabilit&#233; pratique (en temps raisonnable) du processus d&#8217;apprentissage.
</p>
<p>3.2 &#201;lasticit&#233; finie
</p>
<p>L&#8217;&#233;lasticit&#233; [in]finie est une propri&#233;t&#233; d&#233;finie par (Motoki et al., 1991) de la fa&#231;on suivante : Soit
&#12296;U ,G,M&#12297; un syst&#232;me de grammaires. Une classe L &#8838; P(U) a l&#8217;&#233;lasticit&#233; infinie s&#8217;il existe une
s&#233;quence infinie &#12296;a0, a1, a2, . . .&#12297; d&#8217;objets dans U et une s&#233;quence infinie &#12296;L1, L2, . . .&#12297; de lan-
gages dans L tels que ai /&#8712; Li et {a0, . . . , ai&#8722;1} &#8838; Li pour tout i &gt; 0. Une classe de langages
L a la propri&#233;t&#233; d&#8217;&#233;lasticit&#233; finie si elle n&#8217;a pas l&#8217;&#233;lasticit&#233; infinie. L&#8217;&#233;lasticit&#233; finie est donc une
condition suffisante pour l&#8217;apprenabilit&#233;. De fait, il s&#8217;agit d&#8217;une propri&#233;t&#233; tr&#232;s utile pour d&#233;mon-
trer l&#8217;apprenabilit&#233; de nouvelles classes de langages, car cette condition est souvent plus simple
&#224; v&#233;rifier que l&#8217;existence globale d&#8217;un algorithme convergent. L&#8217;&#233;lasticit&#233; finie est notamment
utilis&#233;e par Shinohara pour d&#233;finir une nouvelle condition suffisante &#224; l&#8217;aide du concept de
densit&#233; finie born&#233;e, qui lui permet de d&#233;montrer l&#8217;apprenabilit&#233; de la classe des grammaires
syntagmatiques contextuelles d&#8217;au plus k r&#232;gles (pour tout k &#8805; 0) (Shinohara, 1991). De plus,
Kanazawa a d&#233;montr&#233; une propri&#233;t&#233; tr&#232;s pratique, qui permet de montrer l&#8217;&#233;lasticit&#233; finie (donc
aussi l&#8217;apprenabilit&#233;) d&#8217;une classe de langages complexe &#224; partir du cas d&#8217;une classe plus simple
poss&#233;dant la propri&#233;t&#233; (voir ci-dessous). D&#8217;un point de vue algorithmique, on notera que tous
les r&#233;sultats d&#8217;apprenabilit&#233; obtenus &#224; l&#8217;aide de l&#8217;&#233;lasticit&#233; finie reposent finalement sur l&#8217;algo-
rithme d&#8217;apprentissage par &#233;num&#233;ration des ensembles r&#233;v&#233;lateurs (pr&#233;sent&#233; plus haut).
</p>
<p>Th&#233;or&#232;me 3.2 (Kanazawa) Soient U et U &#8242; deux ensembles d&#8217;objets, et L une classe de lan-
gages d&#233;finie sur U qui a l&#8217;&#233;lasticit&#233; finie. S&#8217;il existe une relation R &#8838; U &#8242; &#215; U finiment valu&#233;e,
alors la classe de langages L&#8242; = {R&#8722;1[L] | L &#8712; L } a aussi l&#8217;&#233;lasticit&#233; finie8.
</p>
<p>7Notons que l&#8217;acquisition humaine du langage n&#8217;a certainement rien &#224; voir non plus avec cette m&#233;thode.
8Une relation binaire R sur A &#215; B est finiment valu&#233;e si et seulement si pour tout a &#8712; A il n&#8217;existe qu&#8217;un
</p>
<p>nombre fini de b &#8712; B tels que a R b. Si L est un langage sur U et R une relation sur U &#8242; &#215; U , l&#8217;image inverse de L
par rapport &#224; R est le langage R&#8722;1[L] = { a &#8712; U &#8242; | &#8707;b &#8712; L tel que a R b }.
</p>
<p>218</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Apprentissage symbolique de grammaires et TAL
</p>
<p>4 Apprentissage de grammaires cat&#233;gorielles
</p>
<p>En 1998, Kanazawa propose plusieurs r&#233;sultats importants concernant l&#8217;apprenabilit&#233; des gram-
maires AB dans le mod&#232;le de Gold (Kanazawa, 1998). Les grammaires AB, la forme la plus
simple de grammaires cat&#233;gorielles, sont (totalement) lexicalis&#233;es : &#224; chaque mot sont associ&#233;s
un ou plusieurs types syntaxiques dans le lexique (r&#232;gles lexicales), et deux r&#232;gles universelles
d&#233;finissent la fa&#231;on dont ces types peuvent se combiner entre eux dans les d&#233;rivations9.
</p>
<p>Les apports de Kanazawa sont multiples : il montre de nouveaux r&#233;sultats et d&#233;veloppe de nou-
velles techniques de preuve. Surtout, ses r&#233;sultats sont les premiers pour le mod&#232;le de Gold
&#224; traiter d&#8217;un formalisme grammatical pr&#233;sentant certaines pr&#233;dispositions &#224; la repr&#233;sentation
des langues naturelles, &#224; savoir les grammaires cat&#233;gorielles. Plus pr&#233;cis&#233;ment, les grammaires
AB sont assez pauvres sur le plan de la repr&#233;sentation linguistique. Mais la famille des gram-
maires cat&#233;gorielles contient d&#8217;autres formalismes beaucoup plus puissants pour repr&#233;senter des
langues naturelles, c&#8217;est pourquoi le premier r&#233;sultat prometteur de Kanazawa a donn&#233; lieu &#224;
d&#8217;autres travaux visant &#224; &#233;tendre l&#8217;apprenabilit&#233; &#224; des formes plus riches de grammaires.
</p>
<p>4.1 Apprenabilit&#233; des grammaires AB
</p>
<p>Parmi ses r&#233;sultats, il faut distinguer deux aspects tr&#232;s diff&#233;rents du point de vue applicatif :
</p>
<p>Il y a tout d&#8217;abord un aspect algorithmique, bas&#233; sur l&#8217;algorithme d&#8217;apprentissage RG propos&#233;
dans (Buszkowski &amp; Penn, 1989). Cet algorithme apprend efficacement des grammaires AB
rigides10 &#224; partir de FA-structures. Ces structures sont une forme &#171; d&#8217;arbre de d&#233;rivation appau-
vri &#187; des phrases, c&#8217;est-&#224;-dire qu&#8217;elles ne contiennent pas toutes les informations d&#8217;un arbre de
d&#233;rivation (sans quoi il n&#8217;y aurait aucun apprentissage, puisque les types seraient d&#233;j&#224; donn&#233;s),
mais tout de m&#234;me beaucoup plus d&#8217;information que de simples cha&#238;nes : parenth&#233;sage des
constituants, ainsi qu&#8217;une forme particuli&#232;re d&#8217;orientation des d&#233;pendances entre constituants.
Il est donc plus facile d&#8217;apprendre lorsqu&#8217;on dispose en plus de cette information structur&#233;e.
</p>
<p>Le second aspect concerne l&#8217;apprenabilit&#233; d&#8217;une classe de langages plus &#233;tendue. Kanazawa ne
montre pas seulement l&#8217;apprenabilit&#233; de la classe des langages de FA-structures de grammaires
AB rigides, il d&#233;montre aussi que cette classe a l&#8217;&#233;lasticit&#233; finie. Or gr&#226;ce au th&#233;or&#232;me 3.2,
il prouve que cette propri&#233;t&#233; est &#233;galement v&#233;rifi&#233;e par la classe des langages de cha&#238;nes des
grammaires AB k-valu&#233;es11 (pour tout k &#8805; 0), donc cette classe est elle aussi apprenable. Ce
r&#233;sultat est beaucoup plus int&#233;ressant pour deux raisons : d&#8217;une part la contrainte de rigidit&#233;
est lev&#233;e, ce qui permet d&#8217;envisager de repr&#233;senter un langage naturel avec ces grammaires12.
D&#8217;autre part il n&#8217;est plus n&#233;cessaire de disposer des FA-structures avec les exemples de phrases,
ce qui est un avantage important puisque celles-ci constituent une information sp&#233;cifique au
formalisme, en pratique tr&#232;s difficile &#224; obtenir en quantit&#233;. En revanche, on ne dispose pas dans
ce cas d&#8217;algorithme d&#8217;apprentissage efficace13.
</p>
<p>9Voir par exemple (Moreau, 2006) pour une d&#233;finition compl&#232;te.
10Une grammaire est rigide si &#224; chaque mot du vocabulaire n&#8217;est associ&#233; qu&#8217;un seul type syntaxique.
11Une grammaire est k-valu&#233;e si &#224; chaque mot du vocabulaire n&#8217;est associ&#233; qu&#8217;au plus k types diff&#233;rents.
12La rigidit&#233; emp&#234;che en effet toute forme d&#8217;homonymie. Mais surtout elle ne permet pas de repr&#233;senter de
</p>
<p>mani&#232;re satisfaisante la plupart des mots grammaticaux, car leur usage syntaxique prend souvent des formes vari&#233;s.
13Au contraire, Costa-Flor&#234;ncio d&#233;montre qu&#8217;il s&#8217;agit d&#8217;un probl&#232;me NP-dur (Costa Flor&#234;ncio, 2003).
</p>
<p>219</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Erwan MOREAU
</p>
<p>4.2 Extensions &#224; d&#8217;autres formalismes
</p>
<p>Les bons r&#233;sultats obtenus par Kanazawa avec les grammaires AB posent la question de savoir
si les grammaires cat&#233;gorielles ont certaines propri&#233;t&#233;s qui feraient d&#8217;elles de bonnes candidates
&#224; l&#8217;apprentissage dans le mod&#232;le de Gold. Cette question du formalisme grammatical est impor-
tante pour d&#8217;&#233;ventuelles applications aux langues naturelles, puisque celles-ci n&#233;cessitent une
repr&#233;sentation &#224; la fois linguistiquement fiable et aussi utilisable le plus facilement possible.
En ce qui concerne l&#8217;apprenabilit&#233; efficace &#224; partir de structures (de type FA-structures, mais
la forme peut varier selon les formalismes), plusieurs r&#233;sultats viendront montrer ensuite que
ce type d&#8217;apprentissage peut &#234;tre &#233;tendu &#224; d&#8217;autre formalismes sans grande difficult&#233;. Kana-
zawa donne lui-m&#234;me l&#8217;exemple des grammaires combinatoires g&#233;n&#233;rales (GCG). Des r&#233;sultats
&#233;quivalents sont obtenus avec diff&#233;rents formalismes, notamment les grammaires de Lambek et
les grammaires minimalistes (Bonato &amp; Retor&#233;, 2001), mais toujours au prix d&#8217;une contrainte
similaire &#224; la rigidit&#233; (limitations sur le nombre ou la forme des r&#232;gles lexicales associ&#233;es &#224; un
mot), et toujours avec l&#8217;aide d&#8217;informations structurelles assez pr&#233;cises.
</p>
<p>Mais le passage du cas &#171; grammaires rigides et avec structures &#187; au cas &#171; grammaires k-valu&#233;es
ou sans structure &#187;, qui constitue le point fort des r&#233;sultats de Kanazawa, s&#8217;av&#232;re nettement
plus difficile lorsqu&#8217;on s&#8217;&#233;loigne du cas des grammaires AB. On aurait pu supposer que les
propri&#233;t&#233;s logiques des grammaires AB jouaient un r&#244;le pour l&#8217;apprenabilit&#233;, mais cette hypo-
th&#232;se est invalid&#233;e par les r&#233;sultats n&#233;gatifs des grammaires de Lambek (Foret &amp; Le Nir, 2002).
Une autre hypoth&#232;se de travail a consist&#233; &#224; consid&#233;rer les grammaires AB comme un syst&#232;me
de grammaires (lexicalis&#233;es) sp&#233;cifi&#233; par un ensemble particulier de r&#232;gles universelles (de r&#233;-
&#233;criture par substitution). On peut alors &#233;tudier ce qui les distingue des autres syst&#232;mes dans le
cadre plus large des GCG propos&#233; par Kanazawa (Moreau, 2006) : on cherche ainsi des condi-
tions, portant sur les r&#232;gles universelles, qui sont suffisantes pour l&#8217;apprenabilit&#233; des classes
de langages correspondantes (on esp&#232;re trouver de cette mani&#232;re des ensembles de r&#232;gles plus
fines qui permettent l&#8217;apprenabilit&#233;). En se basant sur la m&#233;thode employ&#233;e par Kanazawa, nous
avons ainsi montr&#233; que certaines classes de GCG ont l&#8217;&#233;lasticit&#233; finie (donc sont apprenables) :
les grammaires &#224; arguments born&#233;s k-valu&#233;es, qui repr&#233;sentent des classes de langages assez
vastes, mais souffrent d&#8217;une limitation &#171; technique &#187; (sur la taille des arguments) difficile &#224; jus-
tifier au niveau linguistique. Les grammaires par consommation stricte d&#8217;arguments k-valu&#233;es
sont en revanche apprenables sans limitation, mais sont d&#233;finies par un crit&#232;re tellement strict
qu&#8217;on ne s&#8217;&#233;loigne pas beaucoup du cas des grammaires AB. De plus, il ne s&#8217;agit pas que d&#8217;une
limite &#171; temporaire &#187; (c&#8217;est-&#224;-dire susceptible d&#8217;&#234;tre repouss&#233;e &#224; l&#8217;avenir) car les grammaires
par consommation d&#8217;arguments (rigides), qui en sont un sur-ensemble tr&#232;s peu &#233;largi, n&#8217;ont
pas l&#8217;&#233;lasticit&#233; finie : cela signifie qu&#8217;on atteint ici, entre ces deux cas relativement proches, les
fronti&#232;res de l&#8217;apprenabilit&#233; des GCG (du moins selon la m&#233;thode de Kanazawa).
</p>
<p>4.3 Applications &#224; l&#8217;apprentissage symbolique du langage naturel ?
</p>
<p>Compte tenu des contraintes du mod&#232;le et des r&#233;sultats pr&#233;sent&#233;s ci-dessus, il est compr&#233;hen-
sible que les applications de ce type d&#8217;apprentissage au langage naturel demeurent tr&#232;s mo-
destes. De fait, le premier probl&#232;me &#224; r&#233;soudre est cette &#233;quation apparemment insoluble : soit
on cherche &#224; apprendre &#224; l&#8217;aide d&#8217;informations structur&#233;es, mais le type d&#8217;information requis
n&#8217;existe pas en quantit&#233; suffisante a priori ; ou bien on tente d&#8217;apprendre &#224; partir de simples
phrases, mais alors on ne dispose que d&#8217;algorithmes de complexit&#233; exponentielle, incapables de
r&#233;aliser le processus en temps raisonnable.
</p>
<p>220</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Apprentissage symbolique de grammaires et TAL
</p>
<p>Diff&#233;rentes m&#233;thodes ont &#233;t&#233; envisag&#233;es, qui font toutes appel &#224; des ressources structur&#233;es, de
fa&#231;on plus ou moins directe. Quelques unes utilisent des corpus de structures sp&#233;cifiques, ob-
tenus manuellement ou par conversion plus ou moins automatique de ressources arbor&#233;es exis-
tantes (Dudau-Sofronie, 2004). Nous avions &#233;galement propos&#233; une approche interm&#233;diaire, &#224;
partir de cha&#238;nes mais avec l&#8217;apport d&#8217;un sous-ensemble de la grammaire cible (Moreau, 2006),
en utilisant un lexique existant sous forme de grammaires de liens. Le fait qu&#8217;il soit n&#233;cessaire
de faire appel &#224; des ressources externes, souvent exprim&#233;es dans un formalisme grammatical
particulier, pose un probl&#232;me th&#233;orique de fond du point de vue du mod&#232;le de Gold : o&#249; s&#8217;arr&#234;te
la notion d&#8217;inf&#233;rence grammaticale, c&#8217;est-&#224;-dire d&#8217;apprentissage symbolique de la syntaxe, et o&#249;
commence la &#171; simple &#187; extraction d&#8217;informations syntaxiques ? En effet, l&#8217;usage de ressources
externes facilite bien s&#251;r l&#8217;apprentissage, mais introduit aussi un biais dans le processus : &#224; partir
d&#8217;un certain niveau d&#8217;informations syntaxiques fournies, il ne s&#8217;agit plus d&#8217;apprentissage mais
de reconstitution de la grammaire qui a servi &#224; produire les exemples, qu&#8217;elle soit formellement
&#233;tablie ou sous-jacente. On risque alors de ne faire que reproduire des sch&#233;mas syntaxiques
pr&#233;&#233;tablis, la grammaire r&#233;sultante n&#8217;aurait donc pas beaucoup d&#8217;int&#233;r&#234;t : dans ce cas elle peut
&#234;tre construite directement de fa&#231;on semi-automatique, &#224; partir des r&#232;gles qui ont d&#233;fini la cr&#233;a-
tion des donn&#233;es. Un autre travers plus subtil peut &#233;galement appara&#238;tre : le simple &#233;tiquetage
syntaxique par des cat&#233;gories pr&#233;d&#233;finies (nom, verbe, adjectif, etc.) est une forme appauvrie
d&#8217;apprentissage de la syntaxe, car ce cadre emp&#234;che de tenir compte d&#8217;&#233;ventuelles variations
par rapport aux cat&#233;gories de d&#233;part. Dans ce cas, il n&#8217;est pas certain que le mod&#232;le de Gold ait
quelque chose de plus &#224; apporter au probl&#232;me que les techniques existantes en TALN.
</p>
<p>5 Conclusion
</p>
<p>L&#8217;acquisition automatique de grammaires ne se limite pas &#224; l&#8217;apprentissage (au sens de Gold).
Par exemple, pour certaines formes de grammaires cat&#233;gorielles, les travaux d&#8217;Hockenmaier
(Hockenmaier, 2003) ou de Moot (Moortgat &amp; Moot, 2001) montrent qu&#8217;il est possible d&#8217;obte-
nir une grammaire &#224; large couverture d&#8217;un langage naturel, &#224; partir de corpus structur&#233;s. Mais
leur approche est &#224; notre sens plus proche de l&#8217;extraction automatique que de l&#8217;inf&#233;rence gram-
maticale, car dans les deux cas des techniques ad hoc de conversion des donn&#233;es sont utilis&#233;es.
</p>
<p>L&#8217;utilisation d&#8217;un mod&#232;le contraignant comme le mod&#232;le de Gold constitue une garantie de
&#171; pr&#233;cision &#187; de la grammaire obtenue, parce qu&#8217;il donne une direction g&#233;n&#233;rale au processus
de l&#8217;apprentissage : l&#8217;existence d&#8217;un objectif (qu&#8217;on peut consid&#233;rer comme id&#233;al) d&#233;finissant
ce que doit &#234;tre la grammaire apprise diff&#232;re de la simple extraction d&#8217;information syntaxique,
dans laquelle on obtient toujours un r&#233;sultat (quelles que soient les donn&#233;es), et ce r&#233;sultat
n&#8217;est justifi&#233; qu&#8217;a posteriori (parfois selon une &#233;valuation sp&#233;cifique, souvent simplement par
son utilit&#233;). Typiquement, le probl&#232;me de la surg&#233;n&#233;ralisation est difficile voire impossible &#224;
d&#233;tecter dans le cas de l&#8217;extraction, tandis que le mod&#232;le de Gold impose d&#8217;en tenir compte a
priori dans l&#8217;algorithme d&#8217;apprentissage (sans quoi la convergence ne serait pas v&#233;rifi&#233;e).
</p>
<p>Il est vrai que le mod&#232;le de Gold est avant tout un mod&#232;le th&#233;orique, et le crit&#232;re de conver-
gence sur lequel il repose ne semble pas vraiment appropri&#233; pour des applications de traitement
automatique. Dans (Angluin &amp; Smith, 1983), Angluin concluait son &#233;tat de l&#8217;art sur l&#8217;inf&#233;rence
inductive par la remarque suivante : &#171; Le probl&#232;me ouvert le plus important n&#8217;est sans doute
pas une quelconque question technique sp&#233;cifique, mais le foss&#233; entre les r&#233;sultats abstraits et
concrets. &#187; Force est de constater que, malgr&#233; quelques progr&#232;s ind&#233;niables sur le plan th&#233;o-
rique, les tentatives d&#8217;applications concr&#232;tes de cette forme d&#8217;apprentissage restent encore peu
</p>
<p>221</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Erwan MOREAU
</p>
<p>concluantes, parce qu&#8217;on ne parvient pas &#224; (on ne peut pas ?) apprendre sur des donn&#233;es r&#233;elles
sans rel&#226;cher tout ou partie des contraintes du mod&#232;le. Cela ne signifie pas n&#233;cessairement que
l&#8217;on perde ainsi tout l&#8217;int&#233;r&#234;t du mod&#232;le, mais dans ces conditions il nous semble judicieux de
red&#233;finir l&#8217;objectif de la t&#226;che d&#8217;apprentissage : inf&#233;rence grammaticale, extraction, ou approche
mixte ? &#201;tant donn&#233; les difficult&#233;s rencontr&#233;es lorsqu&#8217;on s&#8217;en tient strictement au mod&#232;le, cette
derni&#232;re possibilit&#233; semble la plus r&#233;aliste.
</p>
<p>Toutefois, peut-&#234;tre que l&#8217;algorithme d&#8217;apprentissage id&#233;al n&#8217;est tout simplement pas encore
d&#233;couvert : dans ce cas, &#171; les g&#233;n&#233;rations futures riront bien de notre ignorance actuelle. &#187;
(Angluin &amp; Smith, 1983).
</p>
<p>R&#233;f&#233;rences
ANGLUIN D. (1980). Inductive inference of formal languages from positive data. Information
and Control, 48, 117&#8211;135.
ANGLUIN D. &amp; SMITH C. H. (1983). Inductive inference : Theory and methods. ACM
Computing Surveys, 15(3), 237&#8211;269.
BONATO R. &amp; RETOR&#201; C. (2001). Learning rigid lambek grammars and minimalist grammars
from structured sentences. In Proc. of 3d Workshop on Learning Language in Logic, p. 23&#8211;34.
BUSZKOWSKI W. &amp; PENN G. (1989). Categorial grammars determined from linguistic data
by unification. Rapport interne TR-89-05, Dpt of Computer Science, University of Chicago.
COSTA FLOR&#202;NCIO C. (2003). Learning categorial grammars. PhD thesis, Utrecht Univer-
sity.
DUDAU-SOFRONIE D. (2004). Apprentissage de grammaires cat&#233;gorielles pour simuler l&#8217;ac-
quisition du langage naturel &#224; l&#8217;aide d&#8217;informations s&#233;mantiques. PhD thesis, Univ. Lille 1.
FORET A. &amp; LE NIR Y. (2002). Lambek rigid grammars are not learnable from strings. In
COLING&#8217;2002, 19th International Conference on Computational Linguistics, Taipei, Taiwan.
GOLD E. (1967). Language identification in the limit. Information and control, 10(5), 447&#8211;
474.
HOCKENMAIER J. (2003). Data and models for statistical parsing with Combinatory Cate-
gorial Grammar. PhD thesis, School of Informatics, The University of Edinburgh.
JOHNSON K. (2004). Gold&#8217;s theorem and cognitive science. Philosophy of Science, 71, 571&#8211;
592.
KANAZAWA M. (1998). Learnable classes of categorial grammars. Cambridge University
Press.
MOORTGAT M. &amp; MOOT R. (2001). CGN to Grail : Extracting a type-logical lexicon from
the CGN annotation. In Proceedings of CLIN 2000 : W. Daelemans.
MOREAU E. (2006). Acquisition de grammaires lexicalis&#233;es pour les langues naturelles. PhD
thesis, Universit&#233; de Nantes.
MOTOKI T., SHINOHARA T. &amp; WRIGHT K. (1991). The correct definition of finite elasticity :
corrigendum to Identification of unions. In Proceedings of the Fourth Annual Workshop on
Computational Learning Theory, p. 375, San Mateo, CA : Morgan Kaufmann.
SHINOHARA T. (1991). Inductive inference of monotonic formal systems from positive data.
New Generation Computing, 8(4), 371&#8211;384.
</p>
<p>222</p>

</div></div>
</body></html>