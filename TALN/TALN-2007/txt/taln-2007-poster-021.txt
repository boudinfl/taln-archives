TALN 2007, Toulouse, 5-8 juin 2007

Apprentissage symbolique de grammaires
et traitement automatique des langues

Erwan MOREAU
LINA - FRE 2729, Université de Nantes
2 rue de la Houssiniere, BP 92208, F-44322 Nantes cedex 3

Erwan . Moreau@univ—nantes . fr

Résumé. Le modéle de Gold formalise le processus d’apprentissage d’un langage. Nous
présentons dans cet article les avantages et inconvénients de ce cadre théorique contraignant,
dans la perspective d’ applications en TAL. Nous décrivons briévement les récentes avancées
dans ce domaine, qui soulévent selon nous certaines questions importantes.

Abstract. Gold’s model formalizes the learning process of a language. In this paper we
present the advantages and drawbacks of this restrictive theoretical framework, in the viewpoint
of applications to NLP. We brieﬂy describe recent advances in the domain which, in our opinion,
raise some important questions.

Mots-clés 2 apprentissage symbolique, modéle de Gold, grarmnaires catégorielles.

Keywords: symbolic learning, Gold’s model, categorial grammars.

1 Introduction

L’ apprentissage symbolique automatique de gramrnaires pour les langues naturelles est un do-
rnaine relativement méconnu, assez peu étudié et trés peu avancé sur le plan applicatif. Cet état
de fait s’explique assez facilement 2 tout d’ abord, il s’agit d’une tache extrémement complexe,
aussi bien dans sa déﬁnition précise que dans sa mise en oeuvre. Ce sont surtout les aspects
théoriques qui en sont étudiés, et il semble jusqu’a présent trés difﬁcile d’y obtenir des résu1-
tats pratiques dignes d’ intérét (pour le langage naturel). D’un point de vue optimiste, la relative
lenteur a passer du stade de l’étude théorique au stade des applications dans ce domaine s’ex-
plique par sa grande complexité. En ce sens, ce domaine serait simplement encore trop jeune
scientiﬁquement, mais pourrait prendre de l’ampleur a1’avenir une fois que les bases en seront
bien établies. Mais d’un point de vue pessimiste, la complexité excessive de la téiche peut étre
vue tout simplement comme un obstacle rédhibitoire a d’ éventuelles applications.

Pourtant ce domaine est potentiellement riche en applications, si toutefois on admet l’hypothése
quelque peu idéaliste selon laquelle il est possible de construire un algorithme d’ apprentissage
« parfait ». Celui-ci serait donc capable de donner une grammaire précise d’un langage nature],
pourvu qu’on lui foumisse un nombre sufﬁsant de phrases appartenant a celui-ci. Sous cette
hypothése, la premiere application (et la plus évidente) est 1’ analyse syntaxique, elle-méme uti-
lisée sous différentes formes dans de nombreux outils de traitement des langues. On pourrait

213

Erwan MOREAU

alors envisager de construire assez facilement des analyseurs, y compris pour des langues pour
lesquelles peu d’ etudes linguistiques existent. On peut egalement penser a coupler l’analyse et
l’apprentissage, de facon a n1ieux prendre en compte la categorie syntaxique des mots inconnus
de l’analyseur. D’autres applications liees a l’ analyse, telles que la correction orthographique
et syntaxique, sont egalement susceptibles de beneﬁcier des apports de l’apprentissage auto-
matique. Si 1’ on dispose aussi des moyens permettant de gerer l’ aspect semantique des langues,
l’autre grande application de l’apprentissage est la generation (passage du sens d’un enonce a sa
realisation dans une langue precise). Celle-ci est elle-meme proche du probleme de la traduction
automatique, qui est bien sur une application d’ une tres grande utilite.

L’inférence grammaticale designe la problematique qui consiste a apprendre des langages a
partir de donnees. Tout cadre fonnel pour ce probleme doit donc avant tout deﬁnir les tennes
apprentissage, langages et données, c’ est-a-dire repondre aux questions suivantes 2 nature des
donnees dont on dispose ? simples sequences de mots (chaines), arbres, termes, graphes ou tout
autre type de structures, mais aussi quantite, qualite, completude des donnees. Type de langages
considere, et representation des langages? restrictions eventuelles, niveau d’abstraction (e.g.
sans contrainte particuliere sur la relation entre langages et grarmnaires, ou au contraire fonna-
lisme grammatical precis). Nature du processus d’inference ? Fini ou non, Solution unique ou
multiple, processus automatique ou sen1i-auton1atique, resultat precis ou approximation, lin1ites
eventuelles sur le temps ou le nombre d’ essais.

Le modele d’identiﬁcation a la limite, aussi appele du nom de son auteur modele de Gold,
est l’une des principales representations formelles du processus d’ apprentissage. La premiere
deﬁnition en est donnee dans (Gold, 1967). L’ auteur lui-meme est d’abord pessimiste quant a
l’interet de ce modele, a cause de l’apparente impossibilite d’y obter1ir des res11ltats positifs pour
des classes de langages « interessantes ». Plus tard, les res11ltats positifs obtenus par Angluin
dans ce modele montreront sa pertinence (Angluin, 1980). Le modele sera ensuite etudie plus
en detail 2 ainsi, plusieurs autres resultats encourageants viendront souter1ir l’idee que l’iden-
tiﬁcation a la lin1ite constitue bien un cadre theorique adapte a la representation du processus
d’apprentissage, en particulier celui des langages, voire des langues naturelles.

La question de la pertinence du modele de Gold par rapport a l’acquisition humaine du lan-
gage est plut6t bien etudiee au r1iveau linguistique et cognitif (Johnson, 2004), mais cette meme
question est assez peu discutee dans la perspective de l’apprentissage symbolique automatique.
C’est pourquoi nous proposons ici une relecture des principaux resultats lies a ce modele, vu
sous 1’ angle du traitement automatique des langues. Dans cet article nous essaierons donc d’ ex-
pliquer de facon concise et claire les bases, les outils et les enj eux du modele de Gold par rapport
au TAL. Nous proposons ensuite un point de vue particulier sur les interets et limites des resul-
tats obtenus jusqu’ici dans ce cadre, en tentant de donner un peu de recul a cette modeste etude.
L’ obj ectif de cet article est donc aussi de soumettre a la discussion quelques questions relatives
au domaine, qui nous semblent pertinentes compte tenu de ses recentes evolutions.

2 Identiﬁcation in la limite

Le principe de l’identiﬁcation a la lin1ite est la convergence 2 a partir d’une sequence inﬁnie
d’elements qui caracterisent le langage a deviner, l’apprenant emet des hypotheses. Ces hypo-
theses prennent la fonne d’une grannnaire, censee correspondre au langage observe jusqu’alors
par l’apprenant. Come 1’ enumeration est inﬁnie, l’ apprenant repond lui aussi sous forme d’ une

214

Apprentissage symbolique de grarmnaires et TAL

sequence inﬁnie de grarmnaires hypotheses. Finalement, l’apprentissage est reussi si, a partir
d’un certain point, l’apprenant emet toujours la meme hypothese (convergence), et que celle-ci
correspond bien au langage attendu. Le fait que l’apprenant ignorera toujours s’ﬂ a atteint ou
non la solution est un aspect important de ce formalisme. Gold en donne la justiﬁcation (d’ordre
linguistique) suivante 2 « une personne ne saitjamais si elle parle correctement un langage. ».

Une classe de langages est un ensemble de langagesl ﬁxe, parfois aussi appelefamille de lan-
gages. Generalement il s’agit d’un ensemble de langages partageant une propriete particuliere.
L’ apprenabilite‘ d’une classe de langages designe son aptitude a etre apprise selon la deﬁnition
2.1 ci-dessous.

Un systeme de grammaires est speciﬁe par un triplet (U, G, M), dans lequel l’univers Ll est
un ensemble d’ objets, Q un ensemble de grarmnaires et M une fonction qui associe a chaque
grammaire de G un sous-ensemble de M. Dans un systeme de grammaires (U, Q, M), unefonc-
tion d’apprentissage est une fonction partielle ¢ qui associe a des sequences ﬁr1ies non-vides
d’objets de M des grarmnaire de G.

Déﬁnition 2.1 (Identiﬁcation in la limite) Soit (U, G, M) un systeme de grammaires, ¢ une
fonction d’apprentissage et L Q U un langage. Soit (a0, a1, a2, . .  une sequence inﬁnie d’ob-
jets de M, telle que a E { ai | 2' E N } si et seulement si a E L.

¢ converge vers G s’il existe n E N tel que pour touti 2 11 G1 = ¢((a1,a2, . . . ,a,<)) est deﬁnie
El  = 

¢ apprend un langage L si, pour toute enumeration de L, ¢ converge vers une grammaire G
telle que M(G) = L.

Une classe de langages E Q ’P(I/I) est dite apprenable s’il existe unefonction d’apprentissage
¢ telle que ¢ apprend L pour tout langage L E E.

On voit dans cette deﬁnition que la sequence d’exemples a quelques caracteristiques notables 2

— Elle ne contient que des exemples positifs, c’est-a-dire des elements du langage. La fonc-
tion d’ apprentissage n’a donc aucune information exterieure au langage, ce qui constitue la
principale difﬁculte de cette forme d’apprentissage.

— La sequence d’exemples est supposee ne comporter aucune erreur3.

— La sequence d’ exemples est une enumeration du langage 2 tous les objets du langage doivent
obligatoirement y appara1"tre.

— Les exemples peuvent apparaitre dans un ordre quelconque dans la sequence, et eventuelle-
ment plusieurs fois (ce qui permet notannnent d’enumerer indeﬁniment un langage ﬁni).

Dans ce modele, la convergence d’une fonction d’apprentissage n’a d’interet que si elle s’ap-
plique a un ensemble de langages, et non a un seul langage. Intuitivement, plus la classe de
langages est grande, plus il est difﬁcile de reconnaitre un langage precis dans cette classe.

1Dans la litterature, le terme langage est frequemment deﬁni comme un ensemble de phrases, chaque phrase
etant une sequence ﬁnie de mots. Mais dans la mesure ou on peut envisager diﬂerents niveaux de representation
de la phrase, nous deﬁnissons un langage comme un ensemble d‘objets (ce terme abstrait laissant volontairement
la possibilite d‘utiliser diﬂerents types d‘elements : arbres, structures, etc.).

20n trouve aussi dans la litterature diﬂerents termes designant le caractere apprenable d‘une classe de langages :
inferable, identiﬁable [a la limite] ou acquierable (le terme acquisition fait cependant plus souvent reference a
l‘apprentissage humain du langage).

3Ce qui limite les applications potentielles : ce modele est par deﬁnition inadapte aux donnees bruitees.

215

Erwan MOREAU

Cette deﬁnition de l’identiﬁcation a la limite a d’importantes consequences immediates, demon-
trees par Gold dans (Gold, 1967). La premiere est un resultat positif pour les langages ﬁnis 2
La classe des langages de cardinalite ﬁnie est apprenable. En effet, intuitivement il sufﬁt dans
ce cas que l’apprenant ajoute un par un les exemples presentes a la grammaire hypothese 2 la
fonction d’apprentissage converge des que le langage a ete enumere en totalite. En revanche
la seconde consequence de la deﬁnition du modele est un resultat negatif 2 toute classe de lan-
gages contenant tous les langages ﬁnis et au moins un langage inﬁni n’est pas apprenable. Nous
illustrons ce resultat a l’aide de l’exemple ci-dessous 2

Exemple 2.1 (Langages réguliers) Pour tout n 2 1 on deﬁnit Ln = { xi | 2' 5 n } comme
le langage des chaines de x de longueur infe’rieure ou e’gale a n. Soit Loo = x* le langage de
toutes les chaines de x.

Supposons que la sequence d’exemples commence par (x, xx, xxx, . .  2

— Si l’apprenant est prudent, il ne propose jamais comme hypothese un langage qui va au dela
des exemples propose’s .2 il propose donc Lk, avec k la longueur maximale parmi les exemples
vus. Cet algorithme ne peutjamais trouver L°°.

— Si a l’inverse l’algorithme « ge’ne’ralise », alors a partir d’un certain point il propose L°°.
Mais c’est une erreur s’il s’avere que la sequence ne de’passe pas une certaine longueur de
phrase.

Une erreur est donc possible dans les deux cas, et rien ne permet de faire le bon choix .2 si la

classe de langage contient tous les L" et L°°, celle-ci n’est pas apprenable.

Les langages { L” | n E N } et Loo deﬁnis dans l’exemple 2.1 etant tous reguliers, toutes les
classes de la hierarchie de Chomsky les contiennent, et ne sont par consequent pas apprenables.
Le fait que meme la classe la plus simple de la hierarchie de Chomsky, celle des langages
reguliers, ne soit pas apprenable dans le modele de Gold a longtemps constitue un obstacle ma-
jeur au developpement du modele, considere comme trop contraignant. Gold lui-meme notait 2
« Cependant, les re’sultats pre’sente’s dans la demiere section montrent que seule la classe de
langages la plus triviale 4 conside’re’e5 est apprenable a partir d’exemples positifs[..]. »

L’ exemple 2.1 illustre la piincipale difﬁculte de l’apprentissage a partir d’ exemples positifs, a
savoir la surge’ne’ralisation. La surgeneralisation est l’erreur qui consiste a trop generaliser (ex-
trapoler) a partir des donnees fournies, ce qui signiﬁe inferer un langage qui est un sur-ensemble
strict du langage cible. Par exemple, on peut supposer que l’ensemble {ll, 23, 5, 17, 7} est le
debut d’une enumeration de l’ensemble des nombres impairs. Mais s’il s’ agit en fait de l’en-
semble des nombres premiers supeiieurs a 2, alors il y a surgeneralisation 2 l’ensemble des
nombres represente est un sous-ensemble (strict) de l’ensemble propose. Come on ne dispose
que d’ exemples positifs, il n’y aura jamais de contre-exemple dans la sequence permettant de
cor1igerl’erreur. Bien entendu, la generalisation est indispensable dans le processus d’ appren-
tissage, puisqu’une methode d’ apprentissage « trop prudente » qui ne generaliserait jamais ne
ferait pas de veritable apprentissage (au sens d’une decouverte de quelque chose de nouveau) 2
il s’agirait simplement d’une sorte de compilation des exemples proposes. Surtout, il est evident
qu’une telle methode serait incapable d’identiﬁer un langage inﬁni.

Sauf cas particuliers, la generalisation doit donc bien etre utilisee au cours de l’apprentissage.
La question qui se pose d’un point de vue algoiithmique est 2 quand faut-il generaliser ? (ou

40n peut considérer dc maniére informelle qu‘une classe dc langages est non triviale (pour Papprentissage) si
elle compoite au moins un nombre inﬁnis dc langages, dont ceitains sont inﬁnis.
5]] s‘agit de la classe des langages dc cardinalite ﬁnie.

216

Apprentissage symbolique de grannnaires et TAL

quand faut-il ne pas généraliser, selon ce qu’on considere comme étant l’action par défaut). Mais
avant de se poser cette question, il faut s’ assurer qu’il est possible de savoir quand ge’ne’raliser,
car lorsqu’on ne dispose pas d’exemples négatifs on n’a aucun indice sur la position de la
frontiere du langage a deviner. C’ est précisément ce point qui pose probleme au depart avec
l’identiﬁcation a la 1in1ite a partir d’exemples positifs 2 meme les classes de langages qu’on
croyait simples (les langages réguliers, voir exemple 2.1) ne sont pas apprenables, parce qu’ on
ne peut pas savoir quand [ne pas] généraliser.

3 Techniques théoriques d’apprentissage de grammaires

3.1 Ensembles révélateurs

Au début des armées 1980, Angluin apporte au modele de Gold ses premiers résultats posi-
tifs 2 dans (Angluin, 1980), elle propose de « considérer le cas particulier d’infe’rence a partir
d’exemples positifs qui e’vite la surge’ne’ralisation [et donne] des conditions suﬂisantes pour
cela. » Le critere qu’elle propose a donné lieu ensuite a de nombreuses utilisations ou ex-
tensions, démontrant ﬁnalement la richesse du modele de Gold. Sommairement, un ensemble
révélateur (telltale set) est une sorte de « signature » d’un langage qui le distingue de tous
les autres langages de la classe dont il est un sur-ensemble strict. Ainsi, lorsque cette signature
appara’1‘t dans la séquence d’exemples, on peut proposer ce langage sans risque de surgénéralisa-
tion. Formellement, soit E une classe de langages 2 un ensemble ﬁni d’objets D est un ensemble
re’ve’lateur du langage L E E si D Q L et L’ C L => D Q L’ pour tout langage L’ E E.

Théoréme 3.1 (Angluin) Soit E Q ’P(l/I) une famille indexe’e de langages re’cursifs° dans le
systeme de grammaires (Z/{,Q,./\/1) : E =  | G 6 {G0, G1,G2, . .  

Il existe unefonction ¢ qui apprend E si et seulement s’il existe un algorithme calculable qui,
pour tout indice I tel que L1 = .M(G1) E L‘, e’numere un ensemble re’ve’lateur de L1.

Supposons qu’il existe un algorithme EnumRevel(I, n) qui énumere récursivement les 11 pre-
miers éléments d’un ensemble révélateur D; de L1.

¢((a0, . . . ,an))
i <— 0
E <— EnumRe'uel(i, n)
Tant que (i 3 n et non({a0, . . . ,an} Q .M(G,<) et E Q {(10, .. .,a,L}))fai1-e
i <— i + 1
E <— EnumRe'uel(i, n)
Fin Tant Que
Renvoyer G1

L’ algorithme ci-dessus apprend la classe E de la fagon suivante 2 un ensemble révélateur étant
nécessairement ﬁni, pour tout 2' il existe une étape n a partir de laquelle l’ensemble révélateur
E de Li est énuméré en totalité. Dans ce cas, la boucle s’arrétera sur la premiere grammaire
G1 telle que les exemples foumis appartiennent au langage de la grammaire d’une part, et dont

‘Class:-, dc langage pour laquelle le probléme dc l‘appartena.nce (as E L) est décidable.

217

Erwan MOREAU

l’ensemble revelateur est entierement inclus dans les exemples d’autre part. Ainsi il est impos-
sible que le langage cible soit un sous-ensemble strict de .M  (surgeneralisation). Si aucun
contre-exemple aj ¢  n’ appara’1‘t par la suite, l’algorithme s’ arretera toujours sur G1.

L’ apprentissage par enumeration, illustre ci-dessus, designe une methode generale qui consiste
a faire une recherche systematique dans l’ ensemble des grarmnaires possibles, jusqu’a en trou-
ver une qui veriﬁe une propriete particuliere, et la renvoyer comme hypothese. L’enumeration
n’est pas une methode d’apprentissage universelle, parce qu’il existe des classes de langages
apprenables qui ne sont pas apprenables par enumeration (Costa Florencio, 2003). 11 Va de soi
que ce type d’ algorithme est totalement inutilisable en pratique : meme si on peut ameliorer
sensiblement la methode (notannnent en evitant de faire l’enumeration complete apres chaque
exemple), le seul fait d’ avoir a parcourir de facon exhaustive l’ensemble des grammaires poten-
tielles est redhibitoire. En effet, imaginons une representation textuelle simple des grammaires
(de type grarmnaires syntagmatiques), de facon ales enumerer selon l’ordre de taille puis lexi-
cographique : en premiere approximation il existe de l’ordre de 11'” grarmnaires differentes de
taille m, avec 11 1e nombre total de symboles (comprenant entre autres tous les mots du vocabu-
laire). Ce type de fonctionnement est evidemment radicalement inadapte a l’apprentissage de
langues naturelles7. ll faut donc souligner cette caracteristique essentielle du modele de Gold 2
il s’ agit avant tout d’un modele theorique, qui ne garantit que la decidabilite’ du probleme de
l’apprentissage. Autrement dit, le fait qu’une classe de langages soit apprenable n’implique en
aucun cas la faisabilite pratique (en temps raisonnable) du processus d’ apprentissage.

3.2 Elasticité ﬁnie

L’ elasticite [in] ﬁnie est une propriete deﬁnie par (Motoki et al., 1991) de la facon suivante 2 Soit
(U, G, M) un systeme de grarmnaires. Une classe E Q ’P(I/I) a l’elasticite inﬁnie s’il existe une
sequence inﬁnie (a0, a1, a2, . .  d’objets dans U et une sequence inﬁnie (L1, L2, . .  de lan-
gages dans E tels que ai ¢ Li et {(10, . . . , a,<_1} Q Li pour tout 2' > 0. Une classe de langages
E a la propriete d’ e’lasticite’ ﬁnie si elle n’a pas l’elasticite inﬁnie. L’ elasticite ﬁnie est donc une
condition sufﬁsante pour l’apprenabilite. De fait, il s’agit d’une propriete tres utile pour demon-
trer l’apprenabilite de nouvelles classes de langages, car cette condition est souvent plus simple
a veriﬁer que 1’ existence globale d’un algorithme convergent. L’ elasticite ﬁnie est notanunent
utilisee par Shinohara pour deﬁnir une nouvelle condition sufﬁsante a l’ aide du concept de
densite’ ﬁnie bomee, qui lui permet de demontrer l’apprenabilite de la classe des grammaires
symtagmatiques contextuelles d’ au plus k regles (pour tout k 2 0) (Shinohara, 1991). De plus,
Kanazawa a demontre une propriete tres pratique, qui permet de montrer l’elasticite ﬁnie (donc
aussi l’apprenabilite) d’une classe de langages complexe a partir du cas d’une classe plus simple
possedant la propriete (voir ci-dessous). D’un point de vue algorithmique, on notera que tous
les resultats d’apprenabilite obtenus a l’ aide de l’elasticite ﬁnie reposent ﬁnalement sur l’algo-
rithme d’ apprentissage par enumeration des ensembles revelateurs (presente plus haut).

Théoréme 3.2 (Kanazawa) Soient U et Ll’ deux ensembles d’objets, et E une classe de lan-
gages deﬁnie sur?/I qui a l’e’lasticite’ﬁnie. S’il existe une relation R Q H’ X I/{ﬁniment value’e,
alors la classe de langages E’ = { R‘1[L] | L E E } a aussi l’e’lasticite’ﬁnie8.

7Notons que Pacquisition humaine du langage n‘a certainement rien A voir non plus avec cette methode.

8Une relation binaire R sur A X B est ﬁniment valuée si et seulement si pour tout a E A il n‘existe qu‘un
nombre ﬁni de b E B tels que a R b. Si L est un langage sur U et R une relation sur Ll’ >< Ll, l‘image inverse de L
parrapport 3 Restlelangage R‘1[L] = {a E U’ | Elb E L telque 11 Rb}.

218

Apprentissage symbolique de grammaires et TAL

4 Apprentissage de grammaires catégorielles

En 1998, Kanazawa propose plusieurs resultats importants concemant l’apprenabilite des gram-
maires AB dans le modele de Gold (Kanazawa, 1998). Les grarmnaires AB, la forme la plus
simple de grarmnaires categorielles, sont (totalement) lexicalisees 2 a chaque mot sont associes
un ou plusieurs types symtaxiques dans le lexique (regles laxicales), et deux regles universelles
deﬁnissent la facon dont ces types peuvent se combiner entre eux dans les derivationsg.

Les apports de Kanazawa sont multiples 2 il montre de nouveaux resultats et developpe de nou-
velles techniques de preuve. Surtout, ses resultats sont les premiers pour le modele de Gold
at traiter d’un formalisme grammatical presentant certaines predispositions a la representation
des langues naturelles, a savoir les grarmnaires categorielles. Plus precisement, les grammaires
AB sont assez pauvres sur le plan de la representation linguistique. Mais la famille des gram-
maires categorielles contient d’ autres formalismes beaucoup plus puissants pour representer des
langues naturelles, c’est pourquoi le premier resultat prometteur de Kanazawa a donne lieu a
d’autres travaux visant a etendre l’apprenabilite a des formes plus riches de grarmnaires.

4.1 Apprenabilité des grammaires AB

Parmi ses res11ltats, il faut distinguer deux aspects tres differents du point de vue applicatif 2

11 y a tout d’abord un aspect algorithmique, base sur l’algorithme d’apprentissage RG propose
dans (Buszkowski & Penn, 1989). Cet algorithme apprend efﬁcacement des grammaires AB
rigides1° a partir de FA-structures. Ces structures sont une forme « d’arbre de derivation appau-
vri » des phrases, c’est-a-dire qu’elles ne contiennent pas toutes les informations d’un arbre de
derivation (sans quoi il n’y aurait aucun apprentissage, puisque les types seraient deja donnes),
mais tout de meme beaucoup plus d’information que de simples cha’1‘nes 2 parenthesage des
constituants, ainsi qu’une forme particuliere d’orientation des dependances entre constituants.
Il est donc plus facile d’ apprendre lorsqu’on dispose en plus de cette information structuree.

Le second aspect conceme l’apprenabilite d’une classe de langages plus etendue. Kanazawa ne
montre pas seulement l’apprenabilite de la classe des langages de FA-structures de grammaires
AB rigides, il demontre aussi que cette classe a l’elasticite ﬁnie. Or grace au theoreme 3.2,
il prouve que cette propriete est egalement veriﬁee par la classe des langages de cha’1‘nes des
grammaires AB k-valuees“ (pour tout k 2 0), donc cette classe est elle aussi apprenable. Ce
resultat est beaucoup plus interessant pour deux raisons 2 d’une part la contrainte de rigidite
est levee, ce qui permet d’ envisager de representer un langage naturel avec ces grarmnairesu.
D’autre part il n’est plus necessaire de disposer des FA-structures avec les exemples de phrases,
ce qui est un avantage important puisque celles-ci constituent une information speciﬁque au
formalisme, en pratique tres difﬁcile a obtenir en quantite. En revanche, on ne dispose pas dans
ce cas d’ algorithme d’ apprentissage efﬁcace13.

9Voir par exemple (Moreau, 2006) pour une deﬁnition complete.
1°Une grammaire est rigide si a chaque mot du vocabulaire n‘est associé qu‘un seul type syntaxique.
“Une grammaire est Is:-valuée si a chaque mot du vocabulaire n‘est associé qu‘au plus /4: types différents.
12La rigidite empéche en effet toute forme d‘homonymie. Mais surtout elle ne permet pas de representer de
maniere satisfaisante la plupart des mots grammaticaux, car leur usage syntaxique prend souvent des formes varies.
13Au contraire, Costa-Floréncio démontre qu‘il s‘agit d‘un probleme NP-dur (Costa Florencia, 2003).

219

Erwan MOREAU

4.2 Extensions :1 d’autres formalismes

Les bons résultats obtenus par Kanazawa avec les grarmnaires AB posent la question de savoir
si les grarmnaires catégorielles ont certaines propriétés qui feraient d’elles de bonnes candidates
a l’apprentissage dans le modele de Gold. Cette question du fonnalisme grammatical est impor-
tante pour d’ éventuelles applications aux langues naturelles, puisque celles-ci nécessitent une
representation a la fois linguistiquement ﬁable et aussi utilisable le plus facilement possible.
En ce qui conceme l’apprenabilité efﬁcace a partir de structures (de type FA-structures, mais
la forme peut varier selon les formalismes), plusieurs résultats viendront montrer ensuite que
ce type d’ apprentissage peut étre étendu a d’ autre fonnalismes sans grande difﬁculté. Kana-
zawa donne lui-méme l’exemple des grammaires combinatoires générales (GCG). Des résultats
équivalents sont obtenus avec différents foimalismes, notamment les grarmnaires de Lambek et
les grammaires minimalistes (Bonato & Retoré, 2001), mais toujours au prix d’une contrainte
similaire a la rigidité (limitations sur le nombre ou la forme des regles lexicales associées a un
mot), et toujours avec l’aide d’info1mations structurelles assez précises.

Mais le passage du cas « grarmnaires rigides et avec structures » au cas « grarmnaires k-valuées
ou sans structure », qui constitue le point fort des résultats de Kanazawa, s’avere nettement
plus difﬁcile lorsqu’on s’éloigne du cas des grammaires AB. On aurait pu supposer que les
propriétés logiques des grammaires AB jouaient un role pour l’apprenabilité, mais cette hypo-
these est invalidée par les résultats négatifs des grarmnaires de Lambek (Foret & Le Nir, 2002).
Une autre hypothese de travail a consisté a considérer les grarmnaires AB comme un systeme
de grarmnaires (lexicalisées) spéciﬁé par un ensemble particulier de regles universelles (de ré-
écriture par substitution). On peut alors étudier ce qui les distingue des autres systemes dans le
cadre plus large des GCG proposé par Kanazawa (Moreau, 2006) 2 on cherche ainsi des condi-
tions, portant sur les regles universelles, qui sont sufﬁsantes pour l’apprenabilité des classes
de langages correspondantes (on espere trouver de cette maniere des ensembles de regles plus
ﬁnes qui permettent l’apprenabilité). En se basant sur la méthode employée par Kanazawa, nous
avons ainsi montré que certaines classes de GCG ont l’élasticité ﬁnie (donc sont apprenables) :
les grammaires a arguments bomés k-valuées, qui représentent des classes de langages assez
vastes, mais souffrent d’une limitation « technique » (sur la taille des arguments) difﬁcile a jus-
tiﬁer au niveau linguistique. Les grammaires par consommation stricte d’arguments k-valuées
sont en revanche apprenables sans limitation, mais sont déﬁnies par un critere tellement strict
qu’ on ne s’ éloigne pas beaucoup du cas des grarmnaires AB. De plus, il ne s’ agit pas que d’une
limite « temporaire » (c’est-a-dire susceptible d’ étre repoussée a l’avenir) car les grammaires
par consommation d’arguments (rigides), qui en sont un sur-ensemble tres peu élargi, n’ont
pas l’élasticité ﬁnie : cela signiﬁe qu’on atteint ici, entre ces deux cas relativement proches, les
frontieres de l’apprenabilité des GCG (du moins selon la méthode de Kanazawa).

4.3 Applications 2‘1l’apprentissage symbolique du langage naturel ?

Compte tenu des contraintes du modele et des résultats présentés ci-dessus, il est comprehen-
sible que les applications de ce type d’apprentissage au langage naturel demeurent tres mo-
destes. De fait, le premier probleme a résoudre est cette équation apparemment insoluble 2 soit
on cherche a apprendre a l’ aide d’informations structurées, mais le type d’infonnation requis
n’existe pas en quantité sufﬁsante a priori; ou bien on tente d’ apprendre a partir de simples
phrases, mais alors on ne dispose que d’ algorithmes de complexité exponentielle, incapables de
réaliser le processus en temps raisonnable.

220

Apprentissage symbolique de grannnaires et TAL

Differentes methodes ont ete envisagees, qui font toutes appel a des ressources structurees, de
facon plus ou moins directe. Quelques unes utilisent des corpus de structures speciﬁques, ob-
tenus manuellement ou par conversion plus ou moins automatique de ressources arborees exis-
tantes (Dudau-Sofronie, 2004). Nous avions egalement propose une approche intermediaire, a
partir de cha’1‘nes mais avec l’apport d’un sous-ensemble de la grarmnaire cible (Moreau, 2006),
en utilisant un lexique existant sous forme de grarmnaires de liens. Le fait qu’il soit necessaire
de faire appel a des ressources extemes, souvent exprimees dans un formalisme grammatical
particulier, pose un probleme theorique de fond du point de vue du modele de Gold 2 oil s’ arrete
la notion d’inference grarnmaticale, c’est-a-dire d’ apprentissage symbolique de la syntaxe, et ou
commence la « simple » extraction d’informations switaxiques ? En effet, l’usage de ressources
extemes facilite bien sﬁr l’apprentissage, mais introduit aussi un biais dans le processus 2 a partir
d’un certain niveau d’informations switaxiques foumies, il ne s’agit plus d’ apprentissage mais
de reconstitution de la grarmnaire qui a servi a produire les exemples, qu’elle soit formellement
etablie ou sous-jacente. On risque alors de ne faire que reproduire des schemas switaxiques
preetablis, la grarmnaire resultante n’aurait donc pas beaucoup d’interet 2 dans ce cas elle peut
etre construite directement de facon semi-automatique, a partir des regles qui ont deﬁni la crea-
tion des donnees. Un autre travers plus subtil peut egalement appara1"tre 2 le simple etiquetage
switaxique par des categories predeﬁnies (nom, verbe, adjectif, etc.) est une forme appauvrie
d’apprentissage de la syntaxe, car ce cadre empeche de tenir compte d’eventuelles variations
par rapport aux categories de depart. Dans ce cas, il n’est pas certain que le modele de Gold ait
quelque chose de plus a apporter au probleme que les techniques existantes en TALN.

5 Conclusion

L’ acquisition automatique de grannnaires ne se limite pas a l’apprentissage (au sens de Gold).
Par exemple, pour certaines formes de grammaires categorielles, les travaux d’Hockenmaier
(Hockenmaier, 2003) ou de Moot (Moortgat & Moot, 2001) montrent qu’il est possible d’ obte-
nir une grannnaire a large couverture d’un langage nature], a partir de corpus structures. Mais
leur approche est a r1otre sens plus proche de l’extraction automatique que de l’inference gram-
maticale, car dans les deux cas des techniques ad hoc de conversion des donnees sont utilisees.

L’utilisation d’un modele contraignant comme le modele de Gold constitue une garantie de
« precision » de la grannnaire obtenue, parce qu’il donne une direction generale au processus
de l’apprentissage 2 l’existence d’un objectif (qu’on peut considerer comme ideal) deﬁnissant
ce que doit e‘tre la grarmnaire apprise differe de la simple extraction d’information switaxique,
dans laquelle on obtient toujours un resultat (quelles que soient les donnees), et ce resultat
n’est justiﬁe qu’a posteriori (parfois selon une evaluation speciﬁque, souvent simplement par
son utilite). Typiquement, le probleme de la surgeneralisation est difﬁcile voire impossible a
detecter dans le cas de l’extraction, tandis que le modele de Gold impose d’ en tenir compte a
priori dans l’algorithme d’ apprentissage (sans quoi la convergence ne serait pas veriﬁee).

Il est vrai que le modele de Gold est avant tout un modele theorique, et le critere de conver-
gence sur lequel il repose ne semble pas vraiment approprie pour des applications de traitement
automatique. Dans (Angluin & Smith, 1983), Angluin concluait son etat de l’art sur l’inference
inductive par la remarque suivante 2 « Le probleme ouvert le plus important n’est sans doute
pas une quelconque question technique speciﬁque, mais le fosse’ entre les resultats abstraits et
concrets. » Force est de constater que, malgre quelques progres indeniables sur le plan theo-
rique, les tentatives d’applications concretes de cette fonne d’ apprentissage restent encore peu

221

Erwan MOREAU

concluantes, parce qu’ on ne parvient pas a (on ne peut pas ?) apprendre sur des données réelles
sans relacher tout ou partie des contraintes du modele. Cela ne signiﬁe pas nécessairement que
l’on perde ainsi tout l’intérét du modele, mais dans ces conditions il nous semble judicieux de
redéﬁnir l’obj ectif de la tache d’apprentissage 2 inférence grannnaticale, extraction, ou approche
n1ixte ? Etant donné les difﬁcultés rencontrées lorsqu’on s’ en tient strictement au modele, cette
demiere possibilité semble la plus réaliste.

Toutefois, peut-étre que l’algorithme d’ apprentissage idéal n’est tout simplement pas encore
découvert 2 dans ce cas, « les ge’ne’rations futures riront bien de notre ignorance actuelle. »
(Angluin & Smith, 1983).

Références

ANGLUIN D. (1980). Inductive inference of formal languages from positive data. Information
and Control, 48, 117-135.

ANGLUIN D. & SMITH C. H. (1983). Inductive inference 2 Theory and methods. ACM
Computing Surveys, 15(3), 237-269.

BONATO R. & RETORE C. (2001). Learning rigid larnbek grarmnars and minimalist grarmnars
from structured sentences. In Proc. of 3d Workshop on Learning Language in Logic, p. 23-34.
BUSZKOWSKI W. & PENN G. (1989). Categorial grammars determined from linguistic data
by uniﬁcation. Rapport inteme TR-89-05, Dpt of Computer Science, University of Chicago.
COSTA FLORENCIO C. (2003). Learning categorial grammars. PhD thesis, Utrecht Univer-
sity.

DUDAU-SOFRONIE D. (2004). Apprentissage de grammaires cate’gorielles pour simuler l ’ac-
quisition du langage naturel a l ’aide d ’informations sémantiques. PhD thesis, Univ. Lille 1.
FORET A. & LE NIR Y. (2002). Lambek rigid grarmnars are not leamable from strings. In
COLING’2002, I 9th International Conference on Computational Linguistics, Taipei, Taiwan.
GOLD E. (1967). Language identiﬁcation in the lin1it. Information and control, 10(5), 447-
474.

HOCKENMAIER J. (2003). Data and models for statistical parsing with Combinatory Cate-
gorial Grammar. PhD thesis, School of Informatics, The University of Edinburgh.

JOHNSON K. (2004). Gold’s theorem and cognitive science. Philosophy of Science, 71, 571-
592.

KANAZAWA M. (1998). Leamable classes of categorial grammars. Cambridge University
Press.

MOORTGAT M. & MOOT R. (2001). CGN to Grail 2 Extracting a type-logical lexicon from
the CGN annotation. In Proceedings of CLIN 2000 2 W. Daelemans.

MOREAU E. (2006). Acquisition de grammaires lexicalise’es pour les langues naturelles. PhD
thesis, Université de Nantes.

MOTOKI T., SHINOHARA T. & WRIGHT K. (1991). The correct deﬁnition of ﬁnite elasticity 2
coriigendum to Identiﬁcation of ur1ions. In Proceedings of the Fourth Annual Workshop on
Computational Learning Theory, p. 375, San Mateo, CA 2 Morgan Kaufmann.

SHINOHARA T. (1991). Inductive inference of monotonic formal systems from positive data.
New Generation Computing, 8(4), 371-384.

222

