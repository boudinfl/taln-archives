<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
"http://www.w3.org/TR/html4/loose.dtd">
<html><head><title>Les r&#233;sultats de la campagne EASY d&#8217;&#233;valuation des analyseurs syntaxiques du fran&#231;ais</title>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
</head>
<body>
<div style="page-break-before:always; page-break-after:always"><div><p>TALN 2007, Toulouse, 5&#8211;8 juin 2007
</p>
<p>Les r&#233;sultats de la campagne EASY d&#8217;&#233;valuation des
analyseurs syntaxiques du fran&#231;ais
</p>
<p>Patrick PAROUBEK1, Anne VILNAT1, Isabelle ROBBA1, Christelle AYACHE2
1 LIMSI-CNRS B&#226;t. 508 Universit&#233; Paris XI, BP 133 - 91403 ORSAY Cedex
</p>
<p>2 ELRA-ELDA 55-57, rue Brillat Savarin 75013 Paris
{pap,anne,isabelle}@limsi.frayache@elda.fr
</p>
<p>R&#233;sum&#233;. Dans cet article, nous pr&#233;sentons les r&#233;sultats de la campagne d&#8217;&#233;valuation EASY
des analyseurs syntaxiques du fran&#231;ais. EASY a &#233;t&#233; la toute premi&#232;re campagne d&#8217;&#233;valuation
comparative des analyseurs syntaxiques du fran&#231;ais en mode bo&#238;te noire utilisant des mesures
objectives quantitatives. EASY fait partie du programme TECHNOLANGUE du Minist&#232;re d&#233;l&#233;-
gu&#233; &#224; la Recherche et &#224; l&#8217;&#201;ducation, avec le soutien du minist&#232;re de d&#233;l&#233;gu&#233; &#224; l&#8217;industrie et du
minist&#232;re de la culture et de la communication. Nous exposons tout d&#8217;abord la position de la
campagne par rapport aux autres projets d&#8217;&#233;valuation en analyse syntaxique, puis nous pr&#233;sentos
son d&#233;roulement, et donnons les r&#233;sultats des 15 analyseurs participants en fonction des diff&#233;-
rents types de corpus et des diff&#233;rentes annotations (constituants et relations). Nous proposons
ensuite un ensemble de le&#231;ons &#224; tirer de cette campagne, en particulier &#224; propos du protocole
d&#8217;&#233;valuation, de la d&#233;finition de la segmentation en unit&#233;s linguistiques, du formalisme et des
activit&#233;s d&#8217;annotation, des crit&#232;res de qualit&#233; des donn&#233;es, des annotations et des r&#233;sultats, et
finalement de la notion de r&#233;f&#233;rence en analyse syntaxique. Nous concluons en pr&#233;sentant com-
ment les r&#233;sultats d&#8217;EASY se prolongent dans le projet PASSAGE (ANR-06-MDCA-013) qui
vient de d&#233;buter et dont l&#8217;objectif est d&#8217;&#233;tiqueter un grand corpus par plusieurs analyseurs en
les combinant selon des param&#232;tres issus de l&#8217;&#233;valuation.
</p>
<p>Abstract. In this paper, we present the results of the EASY evaluation campaign on parsers
of French. EASY has been the very first black-box comparative evaluation campaign for parsers
of French, with objective quantitative performance measures. EASY was part of the TECHNO-
LANGUE program of the Delegate Ministry of Research, jointly supported by the Delegate Mi-
nistry of Industry and the ministry of Culture and Communication. After setting EASY in the
context of parsing evaluation and giving an account of the campaign, we present the results ob-
tained by 15 parsers according to syntactic relation and subcorpus genre. Then we propose some
lessons to draw from this campaign, in particular about the evaluation protocole, the segmenting
into linguistic units, the formalism and the annotation activities, the quality criteria to apply for
data, annotations and results and finally about the notion of reference for parsing. We conclude
by showing how EASY results extend through the PASSAGE project (ANR-06-MDCA-013),
which has just started and whose aim is the automatic annotation of a large corpus by several
parsers, the combination of which being parametrized by results stemming from evaluation.
</p>
<p>Mots-cl&#233;s : analyseur syntaxique, &#233;valuation, fran&#231;ais.
</p>
<p>Keywords: parser, evaluation, french.
</p>
<p>243</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Patrick PAROUBEK, Anne VILNAT, Isabelle ROBBA, Christelle AYACHE
</p>
<p>1 L&#8217;&#233;valuation des analyseurs syntaxiques
</p>
<p>Les premi&#232;res tentatives d&#8217;&#233;valuation des analyseurs ont &#233;t&#233; le fait d&#8217;experts qui fondaient leur
appr&#233;ciation d&#8217;un analyseur sur les observations qu&#8217;ils avaient faites de ses sorties sur diff&#233;-
rentes phrases de test, parfois aid&#233;s d&#8217;une grille d&#8217;analyse (Blache &amp; Morin, 2003). Pour le
fran&#231;ais, &#224; notre connaissance la premi&#232;re tentative d&#8217;&#233;valuation comparative a &#233;t&#233; faite par A.
Abeill&#233; (Abeill&#233;, 1991). Dans le souci de r&#233;duire la part de subjectivit&#233; dans le processus d&#8217;&#233;va-
luation et pour r&#233;utiliser les connaissances acquises lors d&#8217;une &#233;valuation, les chercheurs se sont
ensuite tourn&#233;s vers des jeux de test pr&#233;d&#233;finis, dont TSNLP (Oepen et al., 1996), qui contient
des exemples d&#8217;analyses correctes et erron&#233;es class&#233;s par type de constructions linguistiques,
est un arch&#233;type. Cependant les jeux de test ne peuvent pas rendre compte de la distribution
des ph&#233;nom&#232;nes dans un corpus. De plus leur utilit&#233; &#224; des fin d&#8217;&#233;valuation dans des campagnes
ouvertes est limit&#233;e d&#232;s lors qu&#8217;ils sont rendus publics. En effet, il sont de petite taille et para-
m&#233;trer un analyseur en fonction d&#8217;un jeu de test donn&#233; devient alors une t&#226;che ais&#233;e.
Avec le d&#233;veloppement conjoint des standards pour les m&#233;ta-donn&#233;es et des capacit&#233;s des ordi-
nateurs, nous avons vu appra&#238;tre les corpus arbor&#233;s (treebanks), dont le plus c&#233;l&#232;bre est certaine-
ment le Penn Treebank (Marcus et al., 1993). Depuis sa cr&#233;ation de nombreux d&#233;veloppements
pour diff&#233;rents formalismes et pour diff&#233;rentes langues ont vu le jour, dont certains pour le fran-
&#231;ais (Brant et al., 2002) (Abeill&#233; et al., 2000). Cependant, si les corpus arbor&#233;s peuvent apporter
un &#233;l&#233;ment de r&#233;ponse en ce qui concerne la repr&#233;sentativit&#233; des diff&#233;rents genres de texte et
la distribution des ph&#233;nom&#232;nes linguistiques, ils n&#8217;apportent pas de r&#233;ponse au probl&#232;me du
formalisme pivot, pour lequel il n&#8217;existe &#224; ce jour aucun standard 1.
</p>
<p>Comparer des analyseurs implique donc de pouvoir projeter leurs annotations dans une repr&#233;-
sentation unique, ce qui en g&#233;n&#233;ral ne peut se faire sans perte d&#8217;information. Pour r&#233;soudre ce
probl&#232;me, certains (Gaizauskas et al., 1998) proposent de d&#233;finir une fonction entre syst&#232;mes
d&#8217;annotation, d&#8217;autres de tenir compte de la quantit&#233; d&#8217;information (Musillo &amp; Sima&#8217;an, 2002)
(m&#233;thode qui a le d&#233;savantage de n&#233;cessiter la construction d&#8217;un corpus parall&#232;le par formalisme
d&#8217;annotation), d&#8217;autres encore proposent d&#8217;utiliser des m&#233;canismes d&#8217;apprentissage grammati-
cal ou des mesures bas&#233;es sur la distance d&#8217;&#233;dition (Roark, 2002). En remontant un peu plus
dans le pass&#233;, (Black et al., 1991) fut le premier &#224; proposer une mesure d&#8217;&#233;valuation fond&#233;e
sur les limites des constituants pour comparer les analyseurs en mesurant le taux de croisement
des fronti&#232;res avec les annotations de r&#233;f&#233;rence (crossing brackets) et le rappel. En ajoutant la
pr&#233;cision aux deux mesures pr&#233;c&#233;dentes, on obtient le protocole GEIG (Grammar Evaluation
Interest Group) (Srinivas et al., 1996), ou mesures PARSEVAL (Carroll et al., 2002). Cependant
ces mesures ont &#233;t&#233; appliqu&#233;es uniquement sur des constituants non &#233;tiquet&#233;s, car il &#233;tait im-
possible alors de d&#233;finir un jeu d&#8217;&#233;tiquettes commun (Black et al., 1991).
</p>
<p>&#192; part quelques tentatives ponctuelles, de comparaisons d&#8217;analyseurs syntaxiques, comme celle
du projet SPARKLE qui a compar&#233; des analyseurs syntaxiques pour d&#233;terminer le plus appro-
pri&#233; pour une t&#226;che d&#8217;extraction terminologique, ou encore les exp&#233;riences d&#233;velopp&#233;es r&#233;cem-
ment sur des transcriptions orales (Roark et al., 2006), le paradigme d&#8217;&#233;valuation n&#8217;a jusqu&#8217;&#224;
pr&#233;sent pas &#233;t&#233; appliqu&#233; &#224; l&#8217;analyse syntaxique sur une grande &#233;chelle, &#224; l&#8217;exception du projet
EASY (Vilnat et al., 2004) (Paroubek et al., 2005) qui concerne les analyseurs du fran&#231;ais.
</p>
<p>1Une proposition est en cours d&#8217;&#233;laboration &#224; l&#8217;ISO.
</p>
<p>244</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Les r&#233;sultats de la campagne EASY
</p>
<p>2 La campagne EASY
</p>
<p>La campagne EASY &#233;tait une des 8 campagnes d&#8217;&#233;valuation des technologies de la langue du
projet EVALDA du programme TECHNOLANGUE (d&#233;cembre 2002 - avril 2006). Dans cette
campagne, 15 analyseurs provenant de 13 participants diff&#233;rents : ERSS, FT R&amp;D, INRIA,
LATL, LIC2M, LIRMM, LORIA, LPL, STIM, SYNAPSE, SYSTAL, TAGMATICA, VALO-
RIA et XRCE ont &#233;t&#233; &#233;valu&#233;s sur les donn&#233;es fournies par les 5 fournisseurs de corpus que sont
l&#8217;ATILF, le LLF, le DELIC, le STIM et ELDA. La t&#226;che des fournisseurs de corpus a consist&#233;
en la collecte du corpus de diff&#233;rents genres de textes et en leur annotation. Le rapport entre
la portion de texte annot&#233; et la taille totale du corpus est choisie de mani&#232;re &#224; d&#233;courager une
annotation manuelle de l&#8217;int&#233;gralit&#233; du corpus. Le corpus contient des articles de journaux (Le
Monde), des textes litt&#233;raires (issus de la base Frantext de l&#8217;ATILF), des textes m&#233;dicaux (pa-
thologies et traitements), des questions (issues de la campagne EQUER de TECHNOLANGUE),
des transcriptions de d&#233;bats parlementaires (S&#233;nat fran&#231;ais et Parlement Europ&#233;en), des pages
WEB du site ELDA, des courriers &#233;lectroniques et des transcriptions de parole2. On pourra
trouver dans le tableau 4 plus loin dans l&#8217;article, les tailles respectives de ces diff&#233;rents corpus.
</p>
<p>Le protocole d&#8217;&#233;valuation EASY suppose que tous les participants adoptent la m&#234;me segmenta-
tion en mots et en &#233;nonc&#233;s (voir (Roark, 2002) pour les probl&#232;mes que cela pose). Le formalisme
inspir&#233; de (Carroll et al., 2002) et d&#233;fini en collaboration avec les participants doit permettre
d&#8217;exprimer l&#8217;essentiel d&#8217;une annotation syntaxique quelle que soit son type (de surface ou pro-
fonde, compl&#232;te ou partielle), ceci sans privil&#233;gier une approche particuli&#232;re. Le formalisme
d&#8217;annotation EASY permet d&#8217;annoter des constituants continus et non-r&#233;cursifs ainsi que des
relations repr&#233;sentant les fonctions syntaxiques. Les relations (binaires pour la plupart ou ter-
naires) peuvent associer indiff&#233;remment des formes individuelles ou des constituants. Notons,
qu&#8217;EASY ne connait pas la notion de t&#234;te lexicale (Gendner et al., 2003) (Vilnat et al., 2004).
</p>
<p>Dans EASY, il y a 6 types de constituants : (1) nominal, (2) adjectival, (3) pr&#233;positionnel, (4)
adverbial, (5) verbal et (6) pr&#233;positionnel-verbal, le dernier &#233;tant utilis&#233; pour les verbes &#224; l&#8217;in-
finitif introduits par une pr&#233;position, et 14 types de relations de d&#233;pendance : (1) sujet-verbe,
(2) auxilliaire-verbe, (3) c-o-d, (4) compl&#233;ment-verbe, (5) modifieur de non, (6) modifieur de
verbe, (7) modifieur d&#8217;adjectif, (8) modifieur d&#8217;adverbe, (9) modifieur de pr&#233;position, (10) com-
pl&#233;menteur, (11) attribut du sujet/objet, (12) coordination, (13) apposition, (14) juxtaposition.
Le choix de ces constituants et de ces relations a &#233;t&#233; fait &#224; la suite de discussions avec l&#8217;en-
semble des participants &#224; la campagne. Il a ensuite fait l&#8217;objet d&#8217;une description plus d&#8217;&#233;taill&#233;e
&#224; la fois pour les participants et pour les annotateurs dans un guide3, Ils sont &#233;galement d&#233;crits
dans (Vilnat et al., 2004). La figure 1 donne un exemple d&#8217;annotation d&#8217;une phrase issue du
corpus litt&#233;raire.
</p>
<p>Pour comparer les r&#233;sultats des diff&#233;rents analyseurs, les mesures d&#8217;&#233;valuation sont la pr&#233;cision
et le rappel (ainsi que la f-mesure qui les combine) sur lesquelles nous avons exp&#233;riment&#233; 15 re-
l&#226;chements de contrainte diff&#233;rents (Paroubek et al., 2006), obtenus en combinant les 5 mani&#232;res
pr&#233;sent&#233;es dans la table 1 de comparer les empans de textes correspondant soit aux constituants
soit aux cibles de relations, avec les 3 fa&#231;ons de consid&#233;rer les d&#233;finitions des constituants (ceux
de l&#8217;hypoth&#232;se, ceux de la r&#233;f&#233;rence, ou ceux de l&#8217;hypoth&#232;se lorsqu&#8217;ils existent sinon ceux de la
</p>
<p>2Les transcriptions d&#8217;&#233;mission radio-t&#233;l&#233;vis&#233;es fournies par le projet ESTER de TECHNOLANGUE sur l&#8217;&#233;va-
luation de la transcription de parole automatique n&#8217;ont finalement pas &#233;t&#233; prises en compte dans le calcul des
performances en raison d&#8217;un probl&#232;me dans la segementation des &#233;nonc&#233;s.
</p>
<p>3Le guide d&#8217;annotation est disponible &#224; l&#8217;URL www.limsi.fr/Recherche/CORVAL/easy
</p>
<p>245</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Patrick PAROUBEK, Anne VILNAT, Isabelle ROBBA, Christelle AYACHE
</p>
<p>FIG. 1 &#8211; Exemple d&#8217;annotation d&#8217;un &#233;nonc&#233; extrait du corpus litt&#233;raire (Copp&#233;).
</p>
<p>r&#233;f&#233;rence). L&#8217;&#233;valuation a &#233;t&#233; men&#233;e ind&#233;pendamment sur les constituants et les relations. Les
r&#233;sultats ont &#233;t&#233; calcul&#233;s individuellement pour chaque constituant, chaque relation et chaque
type de corpus ainsi que de mani&#232;re globale.
</p>
<p>Fonction Formule
&#201;GALIT&#201; H = R
FLOU UNITAIRE |H\R| &#8804; 1
INCLUSION H &#8834; R
</p>
<p>INTERSECTION R &#8745;H $= &#8709;
BARYCENTRE
</p>
<p>2&#8727;|R&#8745;H|
|R|+|H| &gt; 0.25
</p>
<p>avec :
H Empan de texte hypoth&#232;se
</p>
<p>et
R Empan de texte r&#233;f&#233;rence,
</p>
<p>TAB. 1 &#8211; Comparaison des empans correspondant aux constituants et aux cibles des relations.
</p>
<p>3 Les r&#233;sultats de la campagne EASY
</p>
<p>Dans tout cette partie qui illustre les r&#233;sultats des participants, nous ne donnerons pas directe-
mentleurs noms, nous y ferons r&#233;f&#233;rence par le biais de noms anonymis&#233;s Pi. Notre but n&#8217;est pas
de donner un classement de ces participants mais d&#8217;indiquer les performances obtenues, ainsi
que les &#233;carts observ&#233;s entre ces performances dans les diff&#233;rents domaines de l&#8217;&#233;valuation.
</p>
<p>3.1 Les mesures sur les constituants
</p>
<p>Pour les constituants c&#8217;est le syst&#232;me P10 qui obtient les meilleurs r&#233;sultats pour les 3 mesures
(pr&#233;cision, rappel, f-mesure), tous constituants et tous genres de corpus confondus avec la com-
paraison barycentre pour les empans de texte des constituants (voir table 1 pour la d&#233;finition
de ces notions). La figure 2 illustre les r&#233;sultats obtenus par ce participant, avec les diff&#233;rents
corpus et les constituants annot&#233;s sur le plan horizontal (respectivement axe des x et des y) et la
performance calcul&#233;e en vertical (axe des z). Le graphe de gauche correspond &#224; une vue avant,
celui de droite &#224; une vue arri&#232;re, comme l&#8217;illustrent les petits sch&#233;mas au-dessus des graphes..
</p>
<p>Nous avons utilis&#233; la mesure barycentrique, car c&#8217;est celle qui, tout en permettant un certain re-
l&#226;chement des contraintes impos&#233;es sur les fronti&#232;res de constituant (qui sont parfois le r&#233;sultat
d&#8217;un choix arbitraire), sans toutefois &#234;tre aussi laxiste que l&#8217;intersection (o&#249; il suffit qu&#8217;un seul
mot soit partag&#233;).
</p>
<p>246</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Les r&#233;sultats de la campagne EASY
</p>
<p>annotations
</p>
<p>performance
</p>
<p>corpus
</p>
<p> Consituents f-measure (front view)
</p>
<p>ALL
MED
</p>
<p>ORAL
MAIL
</p>
<p>QUEST
PARLM
</p>
<p>MONDE
LITTR
</p>
<p>    ALL
</p>
<p>    GP
</p>
<p>    NV
</p>
<p>    GN
</p>
<p>    GA
</p>
<p>    GR
</p>
<p>    PV
</p>
<p> 0
</p>
<p> 0.2
</p>
<p> 0.4
</p>
<p> 0.6
</p>
<p> 0.8
</p>
<p> 1
</p>
<p>annotations
</p>
<p>performance
</p>
<p>corpus
</p>
<p> Consituents f-measure (back view)
</p>
<p>ALL
</p>
<p>MED
</p>
<p>ORAL
</p>
<p>MAIL
</p>
<p>QUEST
</p>
<p>PARLM
</p>
<p>MONDE
</p>
<p>LITTR
</p>
<p>    ALL
    GP    NV
</p>
<p>    GN
    GA    GR
</p>
<p>    PV
 0
</p>
<p> 0.2
</p>
<p> 0.4
</p>
<p> 0.6
</p>
<p> 0.8
</p>
<p> 1
</p>
<p>FIG. 2 &#8211; Vue avant et arri&#232;re sur les performances en f-mesure de P10 pour les constituants.
</p>
<p> 0
</p>
<p> 0.2
</p>
<p> 0.4
</p>
<p> 0.6
</p>
<p> 0.8
</p>
<p> 1
</p>
<p>P15P14P13P12P11P10P9P8P7P6P5P4P3P2P1
</p>
<p>CONSTITUANTS
</p>
<p>&quot;allprf.dat&quot; using 1:2
</p>
<p> 0
</p>
<p> 0.2
</p>
<p> 0.4
</p>
<p> 0.6
</p>
<p> 0.8
</p>
<p> 1
</p>
<p>P15P14P13P12P11P10P9P8P7P6P5P4P3P2P1
</p>
<p>RELATIONS
</p>
<p>&quot;allprf.dat&quot; using 1:2
</p>
<p>FIG. 3 &#8211; R&#233;sultats des 15 analyseurs pour les constituants (&#224; gauche) et les relations (&#224; droite)
en pr&#233;cision/rappel/f-mesure, tous corpus et toutes annotations confondus
</p>
<p>La table 2 donne les r&#233;sultats de tous les analyseurs par type de corpus pour tous les consti-
tuants, en pr&#233;cision et f-mesure, pour distinguer les analyseurs visant &#224; la correction de ceux
visant &#224; l&#8217;exhaustivit&#233;. Comme nous pouvions nous y attendre, les mesures de performance sur
les constituants s&#8217;apparentent fortement aux types de r&#233;sultat que l&#8217;on obtient avec un simple
&#233;tiquetage morpho-syntaxique, les probl&#233;mes &#233;tant assez similaires. Le profil des r&#233;sultats est
assez plat et d&#233;pend peu du type d&#8217;annotation ou du type de corpus trait&#233;, au contraire de ce qui
se passe pour les relations comme nous le verrons plus loin.
</p>
<p>La figure 3 illustre les r&#233;sultats des diff&#233;rents analyseurs en combinant tous les corpus et toutes
les annotations, &#224; la fois en pr&#233;cision, rappel et f-mesure. Sur la figure de gauche, on peut
observer 12 colonnes, car trois participants n&#8217;ont pas fourni de r&#233;sultats pour les annotations
en constituants mais uniquement l&#8217;annotation des relations de d&#233;pendance. De m&#234;me sur la
figure de droite, on voit que l&#8217;un des participants n&#8217;a pas fourni d&#8217;annotation en relations de
d&#233;pendance.
</p>
<p>247</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Patrick PAROUBEK, Anne VILNAT, Isabelle ROBBA, Christelle AYACHE
</p>
<p>lemonde litt&#233;raire m&#233;dical oral_delic parlement questions web
P1 p=0 p=0 p=0 p=0 p=0 p=0 p=0
</p>
<p>f=0 f=0 f=0 f=0 f=0 f=0 f=0
P2 p=0.717 p=0.329 p=0.332 p=0.612 p=0.702 p=0.395 p=0.719
</p>
<p>f=0.690 f=0.320 f=0.312 f=0.591 f=0.644 f=0.373 f=0.679
P3 p=0.920 p=0.901 p=0.907 p=0.752 p=0.923 p=0.931 p=0
</p>
<p>f=0.926 f=0.912 f=0.913 f=0.760 f=0.930 f=0.935 f=0
P4 p=0.813 p=0.802 p=0.459 p=0.787 p=0.808 p=0.877 p=0.841
</p>
<p>f=0.660 f=0.770 f=0.436 f=0.717 f=0.653 f=0.856 f=0.696
P5 p=0.883 p=0.847 p=0.882 p=0.714 p=0.876 p=0.901 p=0.877
</p>
<p>f=0.878 f=0.824 f=0.873 f=0.713 f=0.868 f=0.894 f=0.880
P6 p=0.837 p=0 p=0 p=0 p=0.849 p=0 p=0.903
</p>
<p>f=0.782 f=0 f=0 f=0 f=0.803 f=0 f=0.893
P7 p=0.832 p=0.838 p=0.825 p=0.784 p=0.833 p=0.826 p=0.739
</p>
<p>f=0.832 f=0.845 f=0.805 f=0.743 f=0.831 f=0.822 f=0.734
P8 p=0 p=0 p=0 p=0 p=0 p=0 p=0
</p>
<p>f=0 f=0 f=0 f=0 f=0 f=0 f=0
P9 p=0.141 p=0.145 p=0.191 p=0.336 p=0.175 p=0.305 p=0.856
</p>
<p>f=0.137 f=0.152 f=0.183 f=0.334 f=0.159 f=0.301 f=0.866
P10 p=0.904 p=0.910 p=0.909 p=0.849 p=0.921 p=0.913 p=0.924
</p>
<p>f=0.904 f=0.909 f=0.902 f=0.794 f=0.917 f=0.902 f=0.922
P11 p=0 p=0 p=0 p=0 p=0 p=0 p=0
</p>
<p>f=0 f=0 f=0 f=0 f=0 f=0 f=0
P12 p=0.737 p=0.714 p=0.806 p=0.605 p=0.712 p=0.832 p=0.801
</p>
<p>f=0.685 f=0.681 f=0.733 f=0.562 f=0.649 f=0.767 f=0.749
P13 p=0.888 p=0.901 p=0.903 p=0.803 p=0.907 p=0.910 p=0.913
</p>
<p>f=0.884 f=0.910 f=0.892 f=0.763 f=0.909 f=0.903 f=0.911
P14 p=0.855 p=0.887 p=0.879 p=0.775 p=0.867 p=0.873 p=0.879
</p>
<p>f=0.855 f=0.895 f=0.869 f=0.731 f=0.867 f=0.866 f=0.875
P15 p=0.802 p=0.795 p=0.835 p=0.770 p=0.835 p=0.860 p=0.808
</p>
<p>f=0.836 f=0.839 f=0.870 f=0.747 f=0.868 f=0.878 f=0.843
</p>
<p>TAB. 2 &#8211; Mesures en pr&#233;cision (p) et f-mesure (f) par type de corpus pour tous les constituants
</p>
<p>3.2 Les mesures sur les relations
</p>
<p>Pour les relations, c&#8217;est le syst&#232;me P8 qui obtient la meilleure pr&#233;cision, le syst&#232;me P3 qui
obtient le meilleur rappel et le syst&#232;me P10 qui obtient la meilleure f-mesure toutes relations
et tous genres de corpus confondus en tenant compte des constituants de l&#8217;hypoth&#232;se lorsqu&#8217;ils
existent sinon de ceux de la r&#233;f&#233;rence et avec la comparaison barycentre pour les empans de
texte des constituants (voir table 1). On voit dans le figure 4 les graphes respectifs de ces trois
participants, avec les m&#234;mes conventions que dans la figure 2 .
</p>
<p>Le tableau 3 pr&#233;sente les r&#233;sultats de tous les analyseurs en pr&#233;cision et f-mesure, pour toutes
les relations, par type de corpus.
</p>
<p>248</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Les r&#233;sultats de la campagne EASY
</p>
<p> Relations precision (front view)
</p>
<p>ALL
MED
</p>
<p>ORAL
MAIL
</p>
<p>WEB
QUEST
</p>
<p>PARLM
MONDE
</p>
<p>LITTR
</p>
<p>    ALL
    SV
</p>
<p>    XV
    COD
</p>
<p>    CV
     ATB
</p>
<p>    CMP
    MN
</p>
<p>    MV
    MA
</p>
<p>    MR
    MP
</p>
<p>    CRD
    AP
</p>
<p>    JXT
</p>
<p> 0
</p>
<p> 0.2
</p>
<p> 0.4
</p>
<p> 0.6
</p>
<p> 0.8
</p>
<p> 1
</p>
<p> Relations recall (front view)
</p>
<p>ALL
MED
</p>
<p>ORAL
MAIL
</p>
<p>WEB
QUEST
</p>
<p>PARLM
MONDE
</p>
<p>LITTR
</p>
<p>    ALL
    SV
</p>
<p>    XV
    COD
</p>
<p>    CV
     ATB
</p>
<p>    CMP
    MN
</p>
<p>    MV
    MA
</p>
<p>    MR
    MP
</p>
<p>    CRD
    AP
</p>
<p>    JXT
</p>
<p> 0
</p>
<p> 0.2
</p>
<p> 0.4
</p>
<p> 0.6
</p>
<p> 0.8
</p>
<p> 1
</p>
<p> Relations f-measure (front view)
</p>
<p>ALL
MED
</p>
<p>ORAL
MAIL
</p>
<p>WEB
QUEST
</p>
<p>PARLM
MONDE
</p>
<p>LITTR
</p>
<p>    ALL
    SV
</p>
<p>    XV
    COD
</p>
<p>    CV
     ATB
</p>
<p>    CMP
    MN
</p>
<p>    MV
    MA
</p>
<p>    MR
    MP
</p>
<p>    CRD
    AP
</p>
<p>    JXT
</p>
<p> 0
</p>
<p> 0.2
</p>
<p> 0.4
</p>
<p> 0.6
</p>
<p> 0.8
</p>
<p> 1
</p>
<p>FIG. 4 &#8211; Vues avant sur les performances toutes relations et tous genres de corpus confondus
pour les meilleures performances en pr&#233;cision (P8), rappel (P3) et f-mesure (P10)
</p>
<p>lemonde litt&#233;raire m&#233;dical oral_delic parlement questions web
</p>
<p>P1 p=0.571 p=0.611 p=0.599 p=0.608 p=0.579 p=0.683 p=0.594
f=0.543 f=0.576 f=0.561 f=0.544 f=0.546 f=0.648 f=0.549
</p>
<p>P2 p=0.319 p=0.083 p=0.068 p=0.333 p=0.29 p=0.158 p=0.418
f=0.173 f=0.054 f=0.046 f=0.144 f=0.163 f=0.089 f=0.226
</p>
<p>P3 p=0.628 p=0.577 p=0.641 p=0.555 p=0.593 p=0.662 p=0
f=0.616 f=0.596 f=0.634 f=0.513 f=0.590 f=0.635 f=0
</p>
<p>P4 p=0.583 p=0.529 p=0.277 p=0.563 p=0.551 p=0.669 p=0.554
f=0.409 f=0.429 f=0.231 f=0.459 f=0.400 f=0.607 f=0.415
</p>
<p>P5 p=0.562 p=0.507 p=0.564 p=0.514 p=0.529 p=0.447 p=0.553
f=0.508 f=0.456 f=0.524 f=0.425 f=0.472 f=0.412 f=0.489
</p>
<p>P6 p=0.419 p=0 p=0 p=0 p=0.410 p=0 p=0.463
f=0.377 f=0 f=0 f=0 f=0.372 f=0 f=0.433
</p>
<p>P7 p=0.663 p=0.681 p=0.652 p=0.633 p=0.644 p=0.665 p=0.608
f=0.521 f=0.524 f=0.527 f=0.434 f=0.498 f=0.521 f=0.472
</p>
<p>P8 p=0.762 p=0.797 p=0.790 p=0 p=0.746 p=0.771 p=0.795
f=0.656 f=0.651 f=0.699 f=0 f=0.644 f=0.696 f=0.686
</p>
<p>P9 p=0.004 p=0.023 p=0.042 p=0.257 p=0.003 p=0.110 p=0.688
f=0.003 f=0.015 f=0.026 f=0.128 f=0.002 f=0.065 f=0.416
</p>
<p>P10 p=0.610 p=0.640 p=0.605 p=0.522 p=0.582 p=0.635 p=0.595
f=0.599 f=0.624 f=0.597 f=0.502 f=0.568 f=0.622 f=0.573
</p>
<p>P11 p=0.604 p=0.640 p=0.622 p=0.646 p=0.597 p=0.605 p=0.670
f=0.131 f=0.160 f=0.169 f=0.175 f=0.137 f=0.161 f=0.111
</p>
<p>P12 p=0.406 p=0.389 p=0.433 p=0.337 p=0.365 p=0.483 p=0.406
f=0.338 f=0.320 f=0.375 f=0.258 f=0.289 f=0.402 f=0.337
</p>
<p>P13 p=0.355 p=0.429 p=0.359 p=0 p=0.337 p=0.354 p=0.268
f=0.338 f=0.404 f=0.343 f=0 f=0.321 f=0.330 f=0.255
</p>
<p>P14 p=0 p=0 p=0 p=0 p=0 p=0 p=0
f=0 f=0 f=0 f=0 f=0 f=0 f=0
</p>
<p>P15 p=0.336 p=0.381 p=0.326 p=0 p=0.335 p=0.358 p=0.337
f=0.312 f=0.340 f=0.302 f=0 f=0.311 f=0.319 f=0.329
</p>
<p>TAB. 3 &#8211; Mesures en pr&#233;cision (p) et f-mesure (f) par type de corpus pour toutes les relations
</p>
<p>4 Les le&#231;ons &#224; tirer
</p>
<p>Tout d&#8217;abord, rappelons que ce n&#8217;est pas parce qu&#8217;un syst&#232;me a une valeur de performance
0 pour un sous-corpus ou une relation particuli&#232;re qu&#8217;il a de mauvaises performances, il peut
</p>
<p>249</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Patrick PAROUBEK, Anne VILNAT, Isabelle ROBBA, Christelle AYACHE
</p>
<p>genre &#233;nonc&#233;s mots relations &#233;nonc&#233;s relations
nb total nb total nb total erron&#233;s/test&#233;s erron&#233;es/test&#233;es
</p>
<p>WEB 77 2104 113 3/7 = 43% 4/77= 03%
LE MONDE 380 10081 5072 12/39= 30% 22/519= 04%
PARLEMENT 276 7551 3884 14/28= 50% 57/366= 15%
LITT&#201;RATURE 892 24358 12725 36/93= 38% 92/1196= 07%
EMAILS 852 9243 3960 21/75= 28% 30/421= 07%
M&#201;DICAL 554 11799 5595 16/54= 29% 28/518= 05%
ORAL_DELIC 505 8117 4591 10/50= 20% 14/462= 03%
QUESTIONS 203 4116 2165 9/20= 45% 20/217= 09%
</p>
<p>TAB. 4 &#8211; Nombres d&#8217;&#233;nonc&#233;s et de mots par genre de sous-corpus dans la r&#233;f&#233;rence.
</p>
<p>s&#8217;agir d&#8217;un choix d&#233;lib&#233;r&#233; de son concepteur de ne pas traiter un ph&#233;nom&#232;ne particulier ou de ne
retourner qu&#8217;une sorte d&#8217;annotation, par exemple seulement les relations. Ensuite, de mauvaises
performances peuvent provenir de probl&#232;mes d&#8217;alignement entre les donn&#233;es du participant et
celles de r&#233;f&#233;rences et non d&#8217;un mauvais analyseur. Rappelons que dans EASY, contraitement &#224;
ce qui avait &#233;t&#233; fait dans GRACE (Adda et al., 1999) ou dans (Roark et al., 2006), il n&#8217;y a pas
de proc&#233;dure de r&#233;alignement automatique des donn&#233;es du participant, celui-ci doit respecter la
segmentation en mots et en phrases des donn&#233;es qu&#8217;il traite.
</p>
<p>Concernant les r&#233;sultats, nous constatons, comme cela &#233;tait &#224; pr&#233;voir, une plus grande variabilit&#233;
et de moins bonnes performances pour les relations que pour les constituants. Bien entendu, ces
r&#233;sultats ne sont qu&#8217;un point de vue ponctuel et sont &#224; relativiser (comme dans toute &#233;valuation
quantitative) en fonction des facteurs d&#233;crits ci-apr&#232;s. Tout d&#8217;abord, la qualit&#233; des annotations
de r&#233;f&#233;rence : nous avons r&#233;alis&#233; une premi&#232;re estimation du taux d&#8217;erreur d&#8217;annotation sur les
relations, par type de corpus en demandant &#224; un expert d&#8217;examiner &#224; la main un &#233;chantillon
repr&#233;sentant environ un dixi&#232;me de chaque corpus annot&#233;. Les r&#233;sultats sont donn&#233;s dans la
table 4. Un &#233;nonc&#233; est consid&#233;r&#233; comme erron&#233; s&#8217;il contient au moins une erreur d&#8217;annotation
en relation.
</p>
<p>Pour les sous-corpus ayant un taux d&#8217;erreur en relation sup&#233;rieur &#224; 6%, nous avons effectu&#233;
des corrections syst&#233;matiques des erreurs les plus fr&#233;quentes avant de lancer les calculs de
performance 4. L&#8217;estimation du taux d&#8217;erreur d&#8217;annotation pour tous les sous-corpus permettra
de d&#233;terminer des classes de performance parmi les diff&#233;rents syst&#232;mes sans prendre en compte
des diff&#233;rences de performance inf&#233;rieures aux taux d&#8217;erreur estim&#233;.
</p>
<p>Le second point dont il faut tenir compte concerne les erreurs de segmentation en mots/phrases
encore pr&#233;sentes dans la r&#233;f&#233;rence et qui nous ont conduit en particulier &#224; abandonner le traite-
ment du corpus oral provenant de la campagne ESTER. Ces erreurs (auxquelles parfois s&#8217;ajoutent
les erreurs de format des donn&#233;es des participants) sont &#224; notre avis le r&#233;sultat de divers fac-
teurs : l&#8217;absence de tests &#224; blanc du protocole (par manque de temps) et le fait d&#8217;avoir impos&#233;
une segmentation en mots et phrases de la r&#233;f&#233;rence, qui se heurte au probl&#232;me de d&#233;terminer
une d&#233;finition acceptable par tous.
</p>
<p>Dans le projet PASSAGE, qui regroupe certains des participants d&#8217;EASY, nous annoterons un
</p>
<p>4Pour le moment nous n&#8217;avons pas estim&#233; le taux d&#8217;erreur d&#8217;annotation pour les sous-corpus web et emails, ni
effectu&#233; une nouvelle estimation pour les sous-corpus dont les erreurs les plus fr&#233;quentes ont &#233;t&#233; corrig&#233;es.
</p>
<p>250</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Les r&#233;sultats de la campagne EASY
</p>
<p>grand corpus en combinant automatiquement des analyseurs syntaxiques. Pour les deux cam-
pagnes d&#8217;&#233;valuation pr&#233;vues, nous envisageons de recourir &#224; des proc&#233;dure d&#8217;alignement au-
tomatique &#224; partir du texte comme dans GRACE (Adda et al., 1999) ou (Roark et al., 2006).
Les participants pourront ainsi conserver leurs propres algorithmes de segmentation en mots
et phrases. La phrase dans les donn&#233;es de r&#233;f&#233;rence sera d&#233;termin&#233;e &#224; partir des annotations
elles-m&#234;mes, une phrase &#233;tant constitu&#233;e par l&#8217;empan de texte sur lequel un arbre syntaxique se
projette, comme cela a d&#233;j&#224; &#233;t&#233; fait dans EASY pour le sous-corpus ORAL-DELIC.
</p>
<p>Bien entendu, le formalisme d&#8217;annotation EASY s&#8217;il semble suffisament abouti pour les relations
les plus fr&#233;quentes comme la relation sujet-verbe, n&#233;cessite d&#8217;&#234;tre approfondi pour les autres ; ce
qui sera fait dans le cadre du projet PASSAGE, o&#249; cette fois nous consid&#232;rerons des constitutants
admettant plusieurs niveaux de r&#233;cursivit&#233;.
</p>
<p>5 Conclusion
</p>
<p>EASY a permis de poser les bases d&#8217;un protocole d&#8217;&#233;valuation des analyseurs syntaxiques du
fran&#231;ais en mode bo&#238;te noire avec des mesures quantitatives objectives. Il a surtout &#233;t&#233; l&#8217;oc-
casion de former un groupe autour du probl&#232;me de l&#8217;&#233;valuation comparative des technologies
d&#8217;analyse syntaxique et d&#8217;acqu&#233;rir une premi&#232;re exp&#233;rience dans le cadre d&#8217;une campagne d&#8217;en-
vergure qui d&#233;j&#224; trouve des prolongements dans le projet PASSAGE. Concernant les mesures de
performances proprement dites, l&#8217;image ponctuelle qu&#8217;elles donnent des performances des ana-
lyseurs syntaxiques &#224; un instant particulier, nous montre qu&#8217;il reste encore un fort potentiel de
d&#233;veloppement dans la combinaison des approches pour l&#8217;annotation de relations syntaxiques,
car ce sont 3 syst&#232;mes diff&#233;rents qui obtiennent chacun les meilleurs r&#233;sultats pour la pr&#233;ci-
sion, le rappel et la f-mesure. Ce qui laisse &#224; penser que ces syst&#232;mes ont des caract&#233;ristiques
compl&#233;mentaires, il reste encore &#224; les identifier et &#224; trouver le moyen de les combiner harmo-
nieusement.
</p>
<p>R&#233;f&#233;rences
</p>
<p>ABEILL&#201; A. (1991). Analyseurs syntaxiques du fran&#231;ais. Bulletin Semestriel de l&#8217;Association
pour le Traitement Automatique des Langues, 32, 107&#8211;120.
</p>
<p>ABEILL&#201; A., CL&#201;MENT L. &amp; KINYON A. (2000). Building a treebank for french. In Procee-
dings of the 2nd International Conference on Language Ressources and Evaluation (LREC),
p. 1251&#8211;1254, Athen, Greece.
</p>
<p>ADDA G., MARIANI J., PAROUBEK P., RAJMAN M. &amp; LECOMTE J. (1999). L&#8217;action grace
d&#8217;&#233;valuation de l&#8217;assignation des parties du discours pour le fran&#231;ais. Langues, 2(2), 119&#8211;129.
</p>
<p>BLACHE P. &amp; MORIN J. (2003). Une grille d&#8217;&#233;valuation pour les analyseurs syntaxiques. In
Acte de l&#8217;atelier sur l&#8217;Evaluation des Analyseurs Syntaxiques dans les actes de la 10e conf&#233;-
rence annuelle sur le Traitement Automatique des Langues Naturelles (TALN), Batz-sur-Mer.
</p>
<p>BLACK E., ABNEY S., FLICKENGER D., GDANIEC C., GRISHMAN R., HARISON P., ,
HINDLE D., INGRIA R., JELINECK F., KLAVAN J., LIBERMAN M., MARCUS M., ROUKOS
S., SANTORINI B. &amp; STRZALKOZSKIJL (1991). A procedure for quantitatively comparing
the syntactic coverage of english grammars. In Proceedings of the 4th DARPA Speech and
Natural Language Workshop, p. 306&#8211;311, Pacific Grove, California : Morgan Kaufman.
</p>
<p>251</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Patrick PAROUBEK, Anne VILNAT, Isabelle ROBBA, Christelle AYACHE
</p>
<p>BRANT S., DIPPER S., HANSEN S., LEZIUS W. &amp; SIMTH G. (2002). The tiger treebank. In
Proceedings of the 1st Workshop on Treebank and Linguistics Thories (TLT), Sozopol, Bulga-
ria.
CARROLL J., LIN D., PRESCHER D. &amp; USZKOREIT H. (2002). Proceedings of the workshop
beyond parseval - toward improved evaluation measures for parsing systems. In Proceedings of
the 3rd International Conference on Language Resources and Evaluation (LREC), Las Palmas,
Spain.
GAIZAUSKAS R., HEPPLE M. &amp; HUYCK C. (1998). Modifying existing annotated corpora
for general comparative evaluation of parsing. In Proceedings fo the Workshop on Evalua-
tion of Parsing Systems in the Proceedings of the 1st International Conference on Language
Resources and Evaluation (LREC), Granada, Spain.
GENDNER V., ILLOUZ G., JARDINO M., MONCEAUX L., PAROUBEK P., ROBBA I. &amp; VIL-
NAT A. (2003). Peas the first instanciation of a comparative framework for evaluating parsers
of french. In Proceedings of the 10th Conference of the European Chapter fo the Association
for C omputational Linguistics, p. 95&#8211;98, Budapest, Hungarie. Companion Volume.
MARCUS M., SANTORINI B. &amp; MARCINKIEWCIZ M. (1993). Building a large annotated
corpus of english : The penn treebank. Computational Linguistics, 19, 313&#8211;330.
MUSILLO G. &amp; SIMA&#8217;AN K. (2002). Toward comparing parsers from different linguistic fra-
meworks - an information theoretic approach. In Proceedings of the Workshop Beyon Parseval
- Toward improved evaluation measures for parsing systems at the 3rd International Confe-
rence on Language Resources and Evaluation (LREC), Las Palmas, Spain.
OEPEN S., NETTER K. &amp; KLEIN J. (1996). Test suites for natural language processing. In
CSLI Lecture Notes. Center for the Study of Language and Information.
PAROUBEK P., POUILLOT L.-G., ROBBA I. &amp; VILNAT A. (2005). Easy : Campagne d&#8217;&#233;va-
luation des analyseurs syntaxiques. In Proceedings of the 12e conf&#233;rence annuelle sur le
Traitement Automatique des Langues Naturelles (TALN), p. 3&#8211;12, Dourdan, France.
PAROUBEK P., ROBBA I., VILNAT A. &amp; AYACHE C. (2006). Data, annotations and measures
in EASY - the evaluation campaign for parsers of French. In ELRA, Ed., In proceedings
of the fifth international conference on Language Resources and Evaluation (LREC 2006), p.
315&#8211;320, Genoa, Italy : ELRA.
ROARK B. (2002). Evaluating parser accuracy using edit distance. In Proceedings of the
Workshop Beyon Parseval - Toward improved evaluation measures for parsing systems at the
3rd International Conference on Language Resources and Evaluation (LREC), Las Palmas,
Spain.
ROARK B., HARPER M., CHARNIAK E., DORR B., JOHNSON M., KAHN J., LIN Y., OS-
TENDORF M., HALE J., KRANYANSKAYA A., LEASE M., SHAFRAN I., SNOVER M., STE-
WARD R. &amp; YUNG L. (2006). Sparseval : Evaluation metrics for parsing speech. In Procee-
dings of the 5th International Conference on Language Ressources and Evaluation (LREC),
Genoa, Italy.
SRINIVAS B., DORAN C., HOCKEY B. &amp; JOSHI K. (1996). An approach to robust partial
parsing and evaluation metrics. In Proceedings of the Workshop on Robust Parsing, Pragues :
ESSLI.
VILNAT A., PAROUBEK P., MONCEAUX L., ROBBA I., GENDNER V., ILLOUZ G. &amp; JAR-
DINO M. (2004). The ongoing evaluation campaign of syntactic parsing of french : Easy.
In Proceedings of the 4th International Conference on Language Resources and Evaluation
(LREC), p. 2023&#8211;2026, Lisboa, Portugal.
</p>
<p>252</p>

</div></div>
</body></html>