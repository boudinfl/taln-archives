TALN 2007, Toulouse, 5-8 juin 2007

Les résultats de la campagne EASY d’évaluation des
analyseurs syntaxiques du frangais

Patrick PAROUBEK1, Anne VILNAT1, Isabelle ROBBA1, Christelle AYACHE2
1 LIMSI-CNRS Bﬁt. 508 Universite Paris XI, BP 133 - 91403 ORSAY Cedex
2 ELRA—ELDA 55-57, rue Brillat Savarin 75013 Paris
{pap , anne , isabelle}@limsi . frayache@elda . fr

Résumé. Dans cet ar1icle, nous presentons les resultats de la campagne d’evaluation EASY
des analyseurs syntaxiques du francais. EASY a ete la toute premiere campagne d’evaluation
comparative des analyseurs syntaxiques du frangais en mode boite noire utilisant des mes11res
objectives quantitatives. EASY fait partie du programme TECHNOLANGUE du Ministere dele-
gue 5 la Recherche et 5 l’Education, avec le soutien du ministere de delegue 5 l’industrie et du
ministere de la culture et de la communication. Nous exposons tout d’abord la position de la
campagne par rapport aux autres projets d’evaluation en analyse syntaxique, p11is nous presentos
son deroulement, et donnons les resultats des 15 analyseurs participants en fonction des diffe-
rents types de corpus et des differentes annotations (constituants et relations). Nous proposons
ens11ite un ensemble de lecons 5 tirer de cette campagne, en partic11lier 5 propos du protocole
d’evaluation, de la deﬁnition de la segmentation en unites linguistiques, du formalisme et des
activites d’ armotation, des criteres de qualite des donnees, des annotations et des res11ltats, et
ﬁnalement de la notion de reference en analyse syntaxique. Nous concluons en presentant com-
ment les resultats d’EASY se prolongent dans le projet PASSAGE (ANR-06-MDCA-013) q11i
vient de debuter et dont l’objectif est d’etiqueter un grand corpus par plusieurs analyseurs en
les combinant selon des parametres issus del’eva1uation.

Abstract. In this paper, we present the results of the EASY evaluation campaign on parsers
of French. EASY has been the very ﬁrst black-box comparative evaluation campaign for parsers
of French, with objective quantitative performance measures. EASY was part of the TECHNO-
LANGUE program of the Delegate Ministry of Research, jointly supported by the Delegate Mi-
nistry of Industry and the ministry of Culture and Communication. After setting EASY in the
context of parsing evaluation and giving an account of the campaign, we present the results ob-
tained by 15 parsers according to syntactic relation and subcorpus genre. Then we propose some
lessons to draw from this campaign, in particular about the evaluation protocole, the segmenting
into ling11istic units, the formalism and the annotation activities, the quality criteria to apply for
data, annotations and results and ﬁnally about the notion of reference for parsing. We conclude
by showing how EASY results extend through the PASSAGE project (ANR-06-MDCA-013),
which has just started and whose aim is the automatic armotation of a large corpus by several
parsers, the combination of which being parametrized by results stemming from evaluation.

Mots-clés 2 analyseur syntaxique, evaluation, francais.
Keywords: parser, evaluation, french.

243

Patrick PAROUBEK, Anne VILNAT, Isabelle ROBBA, Christelle AYACHE

1 L’évaluation des analyseurs syntaxiques

Les premieres tentatives d’évaluation des analyse11rs ont été le fait d’eXperts q11i fondaient leur
appréciation d’un analyse11r s11r les observations qu’ils avaient faites de ses sorties s11r diffe-
rentes phrases de test, parfois aidés d’une grille d’analyse (Blache & Morin, 2003). Pour le
frangais, a notre connaissance la premiere tentative d’évaluation comparative a été faite par A.
Abeillé (Abeillé, 1991). Dans le souci de réduire la part de subjectivité dans le processus d’ éva-
luation et pour réutiliser les connaissances acq11ises lors d’une évaluation, les chercheurs se sont
ens11ite tournés vers des jeux de test prédéﬁnis, dont TSNLP (Oepen et al., 1996), q11i contient
des exemples d’analyses correctes et erronées classés par type de constructions linguistiques,
est un archétype. Cependant les jeux de test ne peuvent pas rendre compte de la distribution
des phénomenes dans un corpus. De plus leur utilité a des ﬁn d’évaluation dans des campagnes
ouvertes est limitée des lors qu’ils sont rendus publics. En effet, il sont de petite taille et para-
métrer un analyse11r en fonction d’un jeu detest donné devient alors une tache aisée.

Avec le développement conjoint des standards pour les méta-données et des capacités des ordi-
nate11rs, nous avons vu appraitre les corpus arborés (treebanks), dont le plus célebre est certaine-
ment le Penn Treebank (Marcus et al., 1993). Dep11is sa création de nombreux développements
pour différents formalismes et pour différentes langues ont vu le jour, dont certains pour le fran-
gais (Brant et al., 2002) (Abeillé et al., 2000). Cependant, si les corpus arborés peuvent apporter
un élément de réponse en ce q11i conceme la représentativité des différents genres de texte et
la distribution des phénomenes linguistiques, ils n’apportent pas de réponse au probleme du
formalisme pivot, pour lequel il n’eXiste a ce jour aucun standard 1.

Comparer des analyseurs implique donc de pouvoir projeter leurs annotations dans une repre-
sentation unique, ce q11i en général ne peut se faire sans perte d’information. Pour résoudre ce
probleme, certains (Gaizauskas et al., 1998) proposent de déﬁnir une fonction entre systemes
d’annotation, d’ autres de tenir compte de la quantité d’information (Musillo & Sima’ an, 2002)
(méthode q11i a le désavantage de nécessiter la construction d’un corpus parallele par formalisme
d’annotation), d’ autres encore proposent d’utiliser des mécanismes d’ apprentissage grammati-
cal ou des mesures basées sur la distance d’édition (Roark, 2002). En remontant un peu plus
dans le passé, (Black et al., 1991) fut le premier a proposer une mes11re d’évaluation fondée
s11r les limites des constituants pour comparer les analyse11rs en mes11rant le taux de croisement
des frontieres avec les annotations de référence (cmssing brackets) et le rappel. En ajoutant la
précision aux deux mes11res précédentes, on obtient le protocole GEIG (Grammar Evaluation
Interest Group) (Srinivas et al., 1996), ou mesures PARSEVAL (Carroll et al., 2002). Cependant
ces mes11res ont été appliquées uniquement sur des constituants non étiquetés, car il était iIn-
possible alors de déﬁnir un jeu d’étiquettes commun (Black et al., 1991).

A part quelques tentatives ponct11elles, de comparaisons d’ analyseurs syntaxiques, comme celle
du projet SPARKLE q11i a comparé des analyseurs syntaxiques pour déterminer le plus appro-
prié pour une tache d’ extraction terminologique, ou encore les experiences développées récem-
ment sur des transcriptions orales (Roark et al., 2006), le paradigme d’évaluation n’a jusqu’a
présent pas été appliqué a l’analyse syntaxique sur une grande échelle, a l’exception du projet
EASY (Vilnat et al., 2004) (Paroubek et al., 2005) q11i conceme les analyse11rs du frangais.

1Une proposition est en cours d’é1abora1.ion z‘a1’ISO.

244

Les resultats de la campagne EASY

2 La campagne EASY

La campagne EASY etait une des 8 campagnes d’evaluation des technologies de la langue du
projet EVALDA du programme TECHNOLANGUE (decen1bre 2002 - avril 2006). Dans cette
campagne, 15 analyseurs provenant de 13 participants differents : ERSS, FT R&D, INRIA,
LATL, LICZM, LIRMM, LORIA, LPL, STIM, SYNAPSE, SYSTAL, TAGMATICA, VALO-
RIA et XRCE ont ete evalues sur les donnees fournies par les 5 fournisseurs de corpus que sont
l’ATlLF, le LLF, le DELIC, le STIM et ELDA. La tache des fournisseurs de corpus a consiste
en la collecte du corpus de differents genres de textes et en leur armotation. Le rapport entre
la portion de texte annote et la taille totale du corpus est choisie de maniere a decourager une
armotation manuelle de l’integralite du corpus. Le corpus contient des articles de journaux (Le
Monde), des textes litteraires (issus de la base Frantext de l’ATILF), des textes medicaux (pa-
thologies et traitements), des questions (issues de la campagne EQUER de TECHNOLANGUE),
des transcriptions de debats parlementaires (Senat frangais et Parlement Europeen), des pages
WEB du site ELDA, des courriers electroniques et des transcriptions de parole2. On po11rra
trouver dans le tableau 4 plus loin dans l’article, les tailles respectives de ces differents corpus.

Le protocole d’ evaluation EASY suppose que tous les participants adoptent la meme segmenta-
tion en mots et en enonces (voir (Roark, 2002) pour les problemes que cela pose). Le formalisme
inspire de (Carroll et al., 2002) et deﬁni en collaboration avec les participants doit permettre
d’exprimer l’essentiel d’une armotation syntaxique quelle que soit son type (de surface ou pro-
fonde, complete ou partielle), ceci sans privilegier une approche particuliere. Le formalisme
d’annotation EASY permet d’ annoter des constituants continus et non-recursifs ainsi que des
relations representant les fonctions syntaxiques. Les relations (binaires pour la plupart outer-
naires) peuvent associer indifferemment des formes individuelles ou des constituants. Notons,
qu’ EASY ne connait pas la notion de téte lexicale (Gendner et al., 2003) (Vilnat et al., 2004).

Dans EASY, il y a 6 types de constituants : (1) nominal, (2) adjectival, (3) prepositionnel, (4)
adverbial, (5) verbal et (6) prepositionnel-verbal, le demier etant utilise pour les verbes a l’in-
ﬁnitif introduits par une preposition, et 14 types de relations de dependance : (1) sujet-verbe,
(2) auxilliaire-verbe, (3) c-o-d, (4) complement-verbe, (5) modiﬁeur de non, (6) modiﬁeur de
verbe, (7) modiﬁeur d’adjectif, (8) modiﬁeur d’ adverbe, (9) modiﬁeur de preposition, (10) com-
plemente11r, (11) attribut du sujet/objet, (12) coordination, (13) apposition, (14) juxtaposition.
Le choix de ces constituants et de ces relations a ete fait a la suite de discussions avec l’en-
semble des participants a la campagne. Il a ens11ite fait l’objet d’une description plus d’ etaillee
a la fois pour les participants et pour les armotateurs dans un guide3, Ils sont egalement decrits
dans (Vilnat et al., 2004). La ﬁgure 1 donne un exemple d’armotation d’une phrase issue du
corpus litteraire.

Pour comparer les resultats des differents analyseurs, les mes11res d’ evaluation sont la precision
et le rappel (ainsi que la f-mesure q11i les combine) s11r lesquelles nous avons experimente 15 re-
lachements de contrainte differents (Paroubek et al., 2006), obtenus en con1binant les 5 manieres
presentees dans la table 1 de comparer les empans de textes correspondant soit aux constit11ants
soit aux cibles de relations, avec les 3 facons de considerer les deﬁnitions des constituants (ceux
de l’hypothese, ceux de la reference, ou ceux de l’hypothese lorsqu’ils existent sinon ceux de la

2Les transcriptions d’emission radio-televisées fournies par le projet ESTER de TECHNOLANGUE sur 1’eva-
luation de la transcription de parole automatique n’ont ﬁnalement pas ete prises en compte dans le calcul des
performances en raison d’un probleme dans la segementation des enonces.

3Le guide d’annotation est disponible a1’URL www.1in1si.fr/Recherche/CORVAL/easy

245

Patrick PAROUBEK, Anne VILNAT, Isabelle ROBBA, Chn'ste11e AYACHE

mod—v WW

mod—n

 

aux—\.r aux—\r

FIG. 1 — Exemple d’ annotation d’un énoncé extrait du corpus littéraire (Coppé).

référence). L’évaluation a été menée indépendamment sur les constituants et les relations. Les
résultats ont été calculés individuellement pour chaque constituant, chaque relation et chaque
type de corpus ainsi que de maniere globale.

Fonction Formule

EGALITE H = R

FLOU UNITAIRE |H\R| 3 1 aVeC_
INCLUSION H C R ' \
INTERSECTION R n H aé 0 H Empan dc zxte hypothese
BARYCENTRE   > 0'25 R Empan de texte référence

TAB. 1 — Comparaison des empans cornespondant aux constituants et aux cibles des relations.

3 Les résultats de la campagne EASY

Dans tout cette partie q11i illustre les résultats des participants, nous ne donnerons pas directe-
mentle11rs noms, nous y ferons référence par le biais de noms anonymisés Pi. Notre but n’est pas
de donner un classement de ces participants mais d’indiquer les performances obtenues, ainsi
que les écarts observés entre ces performances dans les différents domaines de l’évaluation.

3.1 Les mesures sur les constituants

Pour les constituants c’est le systeme P10 q11i obtient les meille11rs résultats pour les 3 mesures
(précision, rappel, f-mesure), tous constituants et tous genres de corpus confondus avec la com-
paraison baxycentre pour les empans de texte des constituants (voir table 1 pour la déﬁnition
de ces notions). La ﬁgure 2 illustre les résultats obtenus par ce participant, avec les différents
corpus et les constituants armotés sur le plan horizontal (respectivement axe des x et des y) et la
performance calculée en vertical (axe des z). Le graphe de gauche correspond a une vue avant,
celui de droite a une vue arriere, comme l’illustrent les petits schémas au-dessus des graphes..

Nous avons utilisé la mesure baxycentrique, car c’est celle q11i, tout en permettant un certain re-
lachement des contraintes imposées sur les frontieres de constituant(q11i sont parfois le résultat
d’un choix arbitraire), sans toutefois étre aussi laxiste que l’intersection (ou il sufﬁt qu’un seul
mot soit partagé).

246

Les résultats de la campagne EASY

Consxtuaxts measure (back vxe'I'7

Cmsxtumts :—-measure mm V1917

 

FIG. 2 — Vue avant et arriere s11r les performances en f-mesure de P10 pour les constituants.

nmmons

oonsrmmms

's11pKf.dst' using 1:2 E

 

  

v‘r4>‘D4r4>‘D4>‘)a>‘
u.uou.

   

          

.uV4uo~.uou\buououVluo

  

 

wavarcrasrdraurdra
Eaaaga
w4\>I4v4’4v4\>4v4\>

.4v4\>4v4\>I ’4v4\>4v4\>
«u94u\§n9{5 bnfluitnf

papa»! >

,‘>a>‘>a»a>«\>a>‘>a»‘>a>f4§»a>‘>

        

  

vuou.uou.
Iona

29 no 211 212 213 914 ms

's11pKf.dst' using 1:2 mm

 

«N54914:ua\>§§§«a\>§\>§§ua\>§\>
Magggagggaougga
.«v4’4v4’4v4>‘>4v4’4v4’4v4

‘E! E’
is‘? ONON!N!
-woe»!

s">C>">Q>‘>Q~>">C>">Q">
!Nsg‘NENs§'

one

 

  

.494

tiauoiaui

‘E

l4v4v4\>4v4’4v4’4v4r4

   

4§?wwoVowoM»!4§?4wow?¢w¢N»

‘re?

   

    

95 29 no 211 212 213 214 215

FIG. 3 — Résultats des 15 analyseurs pour les constituants (a gauche) et les relations (a droite)
en précision

/rappel/f-mesure, tous corpus et toutes armotations confondus

La table 2 donne les résultats de tous les analyseurs par type de corpus pour tous les consti-

t f-mes11re, pour distinguer les analyseurs visant a la correction de ceux
les mesures de performance sur

tuants, en precision e

visant a l’exhaustivité. Comme nous pouvions nous y attendre,
les constituants s apparentent fortement aux types de résultat que 1

’on obtient avec un simple
tant assez similaires. Le proﬁl des résultats est

2

étiquetage morpho-syntaxique, les problémes é

assez plat et depend peu du type d’annotation ou du type de corpus traité, au contraire de ce q11i
se passe pour les relations comme nous le verrons plus loin.

La ﬁgure 3 illustre les résultats des différents analyseurs en combinant tous les corpus et toutes

les armotations, a la fois en precision, rappel et f-mesure. Sur la ﬁgure de gauche, on peut
observer 12 colonnes, car trois participants n’ont pas foumi de résultats pour les annotations

la
ﬁgure de droite, on voit que l’un des participants n’a pas fourni d’annotation en relations de
dépendance.

en constituants mais uniquement l’annotation des relations de dépendance. De meme s11r

247

Patrick PAROUBEK, Anne VILNAT, Isabelle ROBBA, Christelle AYACHE

lemonde littéraire médical oral_delic parlement questions web
P1 p=0 p=0 p=0 p=0 p=0 p=0 p=0
f=0 f=0 f=0 f=0 f=0 f=0 f=0
P2 p=0.717 p=0.329 p=0.332 p=0.612 p=0.702 p=0.395 p=0.719
f=0.690 f=0.320 f=0.312 f=0.591 f=0.644 f=0.37 3 f=0.679
P3 p=0.920 p=0.901 p=0.907 p=0.7 52 p=0.923 p=0.931 p=0
f=0.926 f=0.912 f=0.913 f=0.760 f=0.930 f=0.935 f=0
P4 p=0.813 p=0.802 p=0.459 p=0.7 87 p=0.808 p=0.877 p=0.841
f=0.660 f=0.770 f=0.436 f=0.717 f=0.653 f=0.856 f=0.696
P5 p=0.883 p=0.847 p=0.882 p=0.714 p=0.876 p=0.901 p=0.877
f=0.878 f=0.824 f=0.873 f=0.713 f=0.868 f=0.894 f=0.880

P6 p=0.837 p=0 p=0 p=0 p=0.849 p=0 p=0.903
f=0.782 f=0 f=0 f=0 f=0.803 f=0 f=0.893
P7 p=0.832 p=0.838 p=0.825 p=0.784 p=0.833 p=0.826 p=0.739
f=0.832 f=0.845 f=0.805 f=0.743 f=0.831 f=0.822 f=0.734
P8 p=0 p=0 p=0 p=0 p=0 p=0 p=0
f=0 f=0 f=0 f=0 f=0 f=0 f=0

P9 p=0. 141 p=0. 145 p=0. 191 p=0.336 p=0. 175 p=0.305 p=0.856
f=0. 137 f=0. 152 f=0. 183 f=0.334 f=0. 159 f=0.301 f=0.866
P10 p=0.904 p=0.910 p=0.909 p=0.849 p=0.921 p=0.913 p=0.924
f=0.904 f=0.909 f=0.902 f=0.794 f=0.917 f=0.902 f=0.922
P11 p=0 p=0 p=0 p=0 p=0 p=0 p=0

f=0 f=0 f=0 f=0 f=0 f=0 f=0

P12 p=0.737 p=0.714 p=0.806 p=0.605 p=0.712 p=0.832 p=0.801
f=0.685 f=0.681 f=0.733 f=0.562 f=0.649 f=0.767 f=0.749
P13 p=0.888 p=0.901 p=0.903 p=0.803 p=0.907 p=0.910 p=0.913
f=0.884 f=0.910 f=0.892 f=0.763 f=0.909 f=0.903 f=0.911
P14 p=0.855 p=0.887 p=0.879 p=0.775 p=0.867 p=0.873 p=0.879
f=0.855 f=0.895 f=0.869 f=0.7 31 f=0.867 f=0.866 f=0.875
P15 p=0.802 p=0.795 p=0.835 p=0.770 p=0.835 p=0.860 p=0.808
f=0.836 f=0.839 f=0.870 f=0.747 f=0.868 f=0.87 8 f=0.843

TAB. 2 — Mesures en précision (p) et f-mesure (f) par type de corpus pour tous les constit11ants

3.2 Les mesures sur les relations

Pour les relations, c’est le systeme P8 q11i obtient la meilleure précision, le systeme P3 q11i
obtient le meilleur rappel et le systeme P10 q11i obtient la meilleure f-mesure toutes relations
et tous genres de corpus confondus en tenant compte des constituants de l’hypothese lorsqu’ils
existent sinon de ceux de la référence et avec la comparaison barycentre pour les empans de
texte des constituants (voir table 1). On voit dans le ﬁgure 4 les graphes respectifs de ces trois
participants, avec les mémes conventions que dans la ﬁgure 2 .

Le tableau 3 présente les résultats de tous les analyseurs en précision et f-mesure, pour toutes
les relations, par type de corpus.

248

Les résultats de la campagne EASY

um... ..¢.u IE-vs: mu -.w... s....“.. mm VIII)

 
  
  

 

 
 

         

.ﬁ' 
:1:  :: ~0,ez'~’«'¢\'.'Il]»«
Qt‘;/t\\\‘1v:r.¢>~.~4 :;

 

 

FIG. 4 — Vues avant s11r les performances toutes relations et tous genres de corpus confondus
pour les meilleures performances en précision (P8), rappel (P3) et f-mesure (P10)

lemonde littéraire médical ora1_de1ic parlement questions web
P1 p=0.571 p=0.611 p=0.599 p=0.608 p=0.579 p=0.683 p=0.594
f=0.543 f=0.576 f=0.561 f=0.544 f=0.546 f=0.648 f=0.549
P2 p=0.319 p=0.083 p=0.068 p=0.333 p=0.29 p=0.158 p=0.418
f=0. 173 f=0.054 f=0.046 f=0. 144 f=0. 163 f=0.089 f=0.226
P3 p=0.628 p=0.577 p=0.641 p=0.555 p=0.593 p=0.662 p=0
f=0.616 f=0.596 f=0.634 f=0.513 f=0.590 f=0.635 f=0
P4 p=0.5 83 p=0.529 p=0.277 p=0.563 p=0.551 p=0.669 p=0.554
f=0.409 f=0.429 f=0.231 f=0.459 f=0.400 f=0.607 f=0.415
P5 p=0.562 p=0.507 p=0.564 p=0.514 p=0.529 p=0.447 p=0.553
f=0.508 f=0.456 f=0.524 f=0.425 f=0.472 f=0.412 f=0.489
P6 p=0.419 p=0 p=0 p=0 p=0.410 p=0 p=0.463
f=0.377 f=0 f=0 f=0 f=0.372 f=0 f=0.433
P7 p=0.663 p=0.681 p=0.652 p=0.633 p=0.644 p=0.665 p=0.608
f=0.521 f=0.524 f=0.527 f=0.434 f=0.498 f=0.521 f=0.472
P8 p=0.762 p=0.797 p=0.790 p=0 p=0.746 p=0.771 p=0.795
f=0.656 f=0.651 f=0.699 f=0 f=0.644 f=0.696 f=0.686
P9 p=0.004 p=0.023 p=0.042 p=0.257 p=0.003 p=0.110 p=0.688
f=0.003 f=0.015 f=0.026 f=0. 128 f=0.002 f=0.065 f=0.416
P10 p=0.610 p=0.640 p=0.605 p=0.522 p=0.582 p=0.635 p=0.595
f=0.599 f=0.624 f=0.597 f=0.502 f=0.568 f=0.622 f=0.573
P11 p=0.604 p=0.640 p=0.622 p=0.646 p=0.597 p=0.605 p=0.670
f=0.131 f=0.160 f=0.169 f=0.175 f=0.137 f=0.161 f=0.111
P12 p=0.406 p=0.389 p=0.433 p=0.337 p=0.365 p=0.483 p=0.406
f=0.338 f=0.320 f=0.375 f=0.258 f=0.289 f=0.402 f=0.337
P13 p=0.355 p=0.429 p=0.359 p=0 p=0.337 p=0.354 p=0.268
f=0.338 f=0.404 f=0.343 f=0 f=0.321 f=0.330 f=0.255
P14 p=0 p=0 p=0 p=0 p=0 p=0 p=0
f=0 f=0 f=0 f=0 f=0 f=0 f=0
P15 p=0.336 p=0.381 p=0.326 p=0 p=0.335 p=0.358 p=0.337
f=0.312 f=0.34O f=0.302 f=0 f=0.311 f=0.319 f=0.329

TAB. 3 — Mesures en précision (p) et f-mesure (f) par type de corpus pour toutes les relations

4 Les legons £1 tirer

Tout d’ abord, rappelons que ce n’est pas parce qu’un systeme a une valeur de performance
0 pour un sous-corpus ou une relation particuliere qu’i1 a de mauvaises performances, i1 peut

249

Patrick PAROUBEK, Anne VILNAT, Isabelle ROBBA, Christelle AYACHE

genre enonces mots relations enonces relations

nb total nb total nb total errones/testes erronees/testees
WEB 77 2104 113 3/7 = 43% 4/77: 03%
LE MONDE 380 10081 5072 12/39: 30% 22/519: 04%
PARLEMENT 276 7551 3884 14/28: 50% 57/366: 15%
LITTERATURE 892 24358 12725 36/93: 38% 92/ 1 196: 07%
EMAILS 852 9243 3960 21/75: 28% 30/421: 07 %
MEDICAL 554 11799 5595 16/54: 29% 28/518: 05%
ORAL_DELIC 505 8117 4591 10/50: 20% 14/462: 03%
QUESTIONS 203 4116 2165 9/20: 45% 20/217: 09%

TAB. 4 — Nombres d’ enonces et de mots par genre de sous-corpus dans la reference.

s’ agir d’un choix delibere de son concepte11r de ne pas traiter un phenomene particulier ou de ne
retourner qu’une sorte d’ armotation, par exemple se11lement les relations. Ensuite, de mauvaises
performances peuvent provenir de problemes d’ alignement entre les donnees du participant et
celles de references et non d’un mauvais analyse11r. Rappelons que dans EASY, contraitement a
ce q11i avait ete fait dans GRACE (Adda et al., 1999) ou dans (Roark et al., 2006), i1 n’y a pas
de procedure de realignement automatique des donnees du participant, cel11i-ci doit respecter la
segmentation en mots et en phrases des donnees qu’il traite.

Concemant les res11ltats, nous constatons, comme cela etait a prevoir, une plus grande variabilite
et de moins bonnes performances pour les relations que pour les constituants. Bien entendu, ces
res11ltats ne sont qu’un point de vue ponctuel et sont a relativiser (comme dans toute evaluation
quantitative) en fonction des facteurs decrits ci-apres. Tout d’ abord, la qualite des annotations
de reference : nous avons realise une premiere estimation du taux d’erre11r d’ annotation s11r les
relations, par type de corpus en demandant a un expert d’eXaminer a la main un echantillon
representant environ un dixieme de chaque corpus annote. Les resultats sont donnes dans la
table 4. Un enonce est considere comme errone s’il contient au moins une erre11r d’armotation
en relation.

Pour les sous-corpus ayant un taux d’erre11r en relation superieur a 6%, nous avons effectue
des corrections systematiques des erreurs les plus frequentes avant de lancer les calc11ls de
performance 4. L’estimation du taux d’erre11r d’ annotation pour tous les sous-corpus permettra
de determiner des classes de performance parmi les differents systemes sans prendre en compte
des differences de performance inferieures aux taux d’ erreur estime.

Le second point dont il faut tenir compte conceme les erreurs de segmentation en mots/phrases
encore presentes dans la reference et q11i nous ont conduit en partic11lier a abandonner le traite-
ment du corpus oral provenant de la campagne ESTER. Ces erreurs (auxquelles parfois s’ajoutent
les erreurs de format des donnees des par1icipants) sont a notre avis le res11ltat de divers fac-
te11rs : l’absence de tests (2 blanc du protocole (par manque de temps) et le fait d’avoir impose
une segmentation en mots et phrases de la reference, q11i se heurte au probleme de determiner
une deﬁnition acceptable par tous.

Dans le projet PASSAGE, q11i regroupe certains des participants d’EASY, nous armoterons un

4Pour1e moment nous n’avons pas estimé 1e taux d’erreur d’annotation pour les sous-corpus web et emails, ni
effectué une nouvelle estimation pour les sous-corpus dont les erreurs les plus fréquentes ont été corrigées.

250

Les résultats de la campagne EASY

grand corpus en con1binant automatiquement des analyseurs syntaxiques. Po11r les deux cam-
pagnes d’évaluation prévues, nous envisageons de recourir a des procedure d’a1ignement au-
ton1atique a partir du texte comme dans GRACE (Adda et al., 1999) ou (Roark et al., 2006).
Les participants pourront ainsi conserver leurs propres algorithmes de segmentation en mots
et phrases. La phrase dans les données de référence sera déterminée a partir des annotations
elles-memes, une phrase étant constituée par l’en1pan de texte sur lequel un arbre syntaxique se

».\ ;

projette, comme cela a deJa eté fait dans EASY po11r le sous-corpus ORAL-DELIC.

Bien entendu, le formalisme d’ annotation EASY s’il sen1ble sufﬁsament abouti pour les relations
les plus fréquentes comme la relation sujet-verbe, nécessite d’étre approfondi pour les autres ; ce
q11i sera fait dans le cadre du projet PASSAGE, ou cette fois nous considererons des constit11tants
admettant plusieurs niveaux de récursivité.

5 Conclusion

EASY a permis de poser les bases d’un protocole d’évaluation des analyse11rs syntaxiques du
francais en mode boite noire avec des mes11res quantitatives objectives. Il a surtout été l’oc-
casion de former un groupe autour du probleme de l’évaluation comparative des technologies
d’analyse syntaxique et d’acquérir une premiere experience dans le cadre d’ une campagne d’ en-
vergure q11i déja trouve des prolongements dans le projet PASSAGE. Concernant les mes11res de
performances proprement dites, l’in1age ponctuelle qu’elles donnent des performances des ana-
lyse11rs syntaxiques a un instant par1iculier, nous montre qu’il reste encore un fort potentiel de
développement dans la combinaison des approches pour l’armotation de relations syntaxiques,
car ce sont 3 systemes différents q11i obtiennent chacun les meilleurs résultats po11r la preci-
sion, le rappel et la f-mesure. Ce q11i laisse a penser que ces systemes ont des caractéristiques
complémentaires, il reste encore ales identiﬁer et a trouver le moyen de les combiner harmo-
nieusement.

Références

AB EILLE A. (1991). Analyseurs syntaxiques du francais. Bulletin Semestriel de l’Association
pour le Traiternent Automatique des Langues, 32, 107-120.

ABEILLE A., CLEMENT L. & KINYON A. (2000). B11ilding a treebank for french. In Procee-
dings of the 2nd International Conference on Language Ressources and Evaluation (LREC),
p. 1251-1254, Athen, Greece.

ADDA G., MARIANI J ., PAROUBEK P., RAJMAN M. & LECOMTE J . (1999). L’action grace
d’évaluation de l’assignation des parties du discours pour le francais. Langues, 2(2), 119-129.

BLACHE P. & MORIN J . (2003). Une grille d’évaluation po11r les analyse11rs syntaxiques. In
Acte de l ’atelier sur l ’Evaluation des Analyseurs Syntaxiques dans les actes de la 109 confe’-
rence annuelle sur le Traiternent Automatique des Langues Naturelles (TALN), Batz-sur-Mer.

BLACK E., ABNEY S., FLICKENGER D., GDANIEC C., GRISHMAN R., HARISON P., ,
HINDLE D., INGRIA R., JELINECK F., KLAVAN J ., LIBERMAN M., MARCUS M., ROUKOS
S., SANTORINI B. & STRZALKOZSKIJL (1991). A proced11re for quantitatively comparing
the syntactic coverage of english grammars. In Proceedings of the 4th DARPA Speech and
Natural Language Workshop, p. 306-311, Paciﬁc Grove, California: Morgan Kaufman.

251

Patrick PAROUBEK, Anne VILNAT, Isabelle ROBBA, Christelle AYACHE

BRANT S., DIPPER S., HANSEN S., LEZIUS W. & SIMTH G. (2002). The tiger treebank. In
Pmceedings of the I 5 ’ Workshop on Treebank and Linguistics Ihories (TLT), Sozopol, Bulga-
ria.

CARROLL J ., LIN D., PREscHER D. & USZKOREIT H. (2002). Proceedings of the workshop
beyond parseval - toward improved evaluation measures for parsing systems. In Proceedings of
the 37d International Conference on Language Resources and Evaluation (LREC), Las Palmas,
Spain.

GAIZAUSKAS R., HEPPLE M. & HUYCK C. (1998). Modifying existing annotated corpora
for general comparative evaluation of parsing. In Proceedings fo the Workshop on Evalua-
tion of Parsing Systems in the Proceedings of the I 5’ International Conference on Language
Resources and Evaluation (LREC), Granada, Spain.

GENDNER V., ILLOUZ G., JARDINo M., MONCEAUX L., PAROUBEK P., ROBBA I. & VIL-
NAT A. (2003). Peas the ﬁrst instanciation of a comparative framework for evaluating parsers
of french. In Proceedings of the 10th Conference of the European Chapter fo the Association
for C omputational Linguistics, p. 95-98, Budapest, Hungarie. Companion Volume.
MARCUS M., SANTORINI B. & MARCINKIEWCIZ M. (1993). Building a large annotated
corpus of english : The penn treebank. Computational Linguistics, 19, 313-330.

MUSILLO G. & SIMA’ AN K. (2002). Toward comparing parsers from different linguistic fra-
meworks - an information theoretic approach. In Proceedings of the Workshop Beyon Parseval
- Toward improved evaluation measures for parsing systems at the 37d International Confe-
rence on Language Resources and Evaluation (LREC), Las Palmas, Spain.

OEPEN S., NETTER K. & KLEIN J . (1996). Test suites for natural language processing. In
CSLI Lecture Notes. Center for the Study of Language and Information.

PAROUBEK P., PoUILLoT L.-G., ROBBA I. & VILNAT A. (2005). Easy : Campagne d’eva-
luation des analyse11rs syntaxiques. In Proceedings of the 126' confe’rence annuelle sur le
Traitement Automatique des Langues Naturelles (TALN), p. 3-12, Dourdan, France.
PAROUBEK P., ROBBA I., VILNAT A. & AYACHE C. (2006). Data, annotations and measures
in EASY - the evaluation campaign for parsers of French. In ELRA, Ed., In proceedings
of the ﬁfth international conference on Language Resources and Evaluation (LREC 2006 ), p.
315-320, Genoa, Italy : ELRA.

ROARK B. (2002). Evaluating parser accuracy using edit distance. In Proceedings of the
Workshop Beyon Parseval - Toward improved evaluation measures for parsing systems at the
37d International Conference on Language Resources and Evaluation (LREC), Las Palmas,
Spain.

ROARK B., HARPER M., CHARNIAK E., DoRR B., JoHNsoN M., KAHN J., LIN Y., Os-
TENDORF M., HALE J ., KRANYANSKAYA A., LEASE M., SHAFRAN I., SNovER M., STE-
WARD R. & YUNG L. (2006). Sparseval : Evaluation metrics for parsing speech. In Procee-
dings of the 5th International Conference on Language Ressources and Evaluation (LREC),
Genoa, Italy.

SRINIvAs B., DoRAN C., HOCKEY B. & JosHI K. (1996). An approach to robust partial
parsing and evaluation metrics. In Proceedings of the Workshop on Robust Parsing, Pragues :
ESSLI.

VILNAT A., PAROUBEK P., MONCEAUX L., ROBBA I., GENDNER V., ILLOUZ G. & JAR-
DINO M. (2004). The ongoing evaluation campaign of syntactic parsing of french : Easy.
In Proceedings of the 4th International Conference on Language Resources and Evaluation
(LREC), p. 2023-2026, Lisboa, Portugal.

252

