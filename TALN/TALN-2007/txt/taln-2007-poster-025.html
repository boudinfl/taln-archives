<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
"http://www.w3.org/TR/html4/loose.dtd">
<html><head><title>Mod&#232;les statistiques enrichis par la syntaxe pour la traduction automatique</title>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
</head>
<body>
<div style="page-break-before:always; page-break-after:always"><div><p>TALN 2007, Toulouse, 5&#8211;8 juin 2007
</p>
<p>Mode&#768;les statistiques enrichis par la syntaxe pour la
</p>
<p>traduction automatique
</p>
<p>Holger Schwenk, Daniel De&#769;chelotte
He&#769;le&#768;ne Bonneau-Maynard, Alexandre Allauzen
</p>
<p>LIMSI-CNRS, B.P. 133, 91403 Orsay cedex
{schwenk,dechelot,hbm,allauzen}@limsi.fr
</p>
<p>Re&#769;sume&#769;. La traduction automatique statistique par se&#769;quences de mots est une
voie prometteuse. Nous pre&#769;sentons dans cet article deux e&#769;volutions comple&#769;mentaires. La
premie&#768;re permet une mode&#769;lisation de la langue cible dans un espace continu. La seconde
inte&#768;gre des cate&#769;gories morpho-syntaxiques aux unite&#769;s manipule&#769;es par le mode&#768;le de tra-
duction. Ces deux approches sont e&#769;value&#769;es sur la ta&#770;che Tc-Star. Les re&#769;sultats les plus
inte&#769;ressants sont obtenus par la combinaison de ces deux me&#769;thodes.
</p>
<p>Abstract. Statistical phrase-based translation models are very efficient. In this
paper, we present two complementary methods. The first one consists in a a statistical
language model that is based on a continuous representation of the words in the vocabu-
lary. By these means we expect to take better advantage of the limited amount of training
data. In the second method, morpho-syntactic information is incorporated into the trans-
lation model in order to obtain lexical disambiguation. Both approaches are evaluated on
the Tc-Star task. Most promising results are obtained by combining both methods.
</p>
<p>Mots-cle&#769;s : traduction automatique, approche statistique, mode&#769;lisation linguis-
tique dans un espace continu, analyse morpho-syntaxique, de&#769;sambigu&#776;&#305;sation lexicale.
</p>
<p>Keywords: statistical machine translation, continuous space language model,
POS tagging, lexical disambiguation.
</p>
<p>1 Introduction
</p>
<p>La traduction automatique est un the&#768;me de recherche depuis plusieurs de&#769;cennies et dif-
fe&#769;rentes approches ont e&#769;te&#769; propose&#769;es, telles que la traduction par re&#768;gles, la traduction a&#768;
base d&#8217;exemples ou la traduction statistique. Les travaux re&#769;cents en traduction statistique
confirment que les mode&#768;les fonde&#769;s sur des se&#769;quences de mots (Och et al., 1999; Koehn
et al., 2003) obtiennent des performances significativement meilleures que ceux fonde&#769;s
sur des mots (Brown et al., 1993). En utilisant des se&#769;quences de mots, les syste&#768;mes de
traduction parviennent a&#768; pre&#769;server certaines contraintes locales sur l&#8217;ordre des mots. L&#8217;en-
tra&#770;&#305;nement d&#8217;un tel mode&#768;le ne&#769;cessite l&#8217;alignement d&#8217;un corpus paralle&#768;le. Les re&#769;gularite&#769;s
du langage naturel comme celles de la syntaxe, ou, encore a&#768; un niveau supe&#769;rieur, celles de
la se&#769;mantique sont ainsi, en principe, implicitement capture&#769;es par les mode&#768;les.
</p>
<p>253</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>H. Schwenk, D. De&#769;chelotte, H. Bonneau-Maynard, A. Allauzen
</p>
<p>Depuis les de&#769;buts de l&#8217;approche statistique en traduction automatique, les efforts de mo-
de&#769;lisation se sont principalement concentre&#769;s sur les mode&#768;les de traduction et d&#8217;alignement,
comme en te&#769;moignent les nombreuses publications sur ces sujets. Dans cet article, nous
explorons deux pistes comple&#769;mentaires pour l&#8217;ame&#769;lioration des mode&#768;les de traduction
statistique : d&#8217;une part, l&#8217;exploration d&#8217;une mode&#769;lisation statistique du langage dans un
espace continu, et d&#8217;autre part l&#8217;inte&#769;gration d&#8217;informations syntaxiques dans le mode&#768;le de
traduction.
</p>
<p>Traditionnellement, les syste&#768;mes de traduction statistiques utilisent des mode&#768;les de lan-
gage trigramme a&#768; repli. Dans ces mode&#768;les classiques, les mots sont repre&#769;sente&#769;s par un
indice dans un espace discret, le vocabulaire. Ceci ne permet pas de faire de ve&#769;ritables
interpolations des probabilite&#769;s d&#8217;un n-gramme non observe&#769; puisqu&#8217;un changement dans
l&#8217;espace des mots peut entra&#770;&#305;ner un changement arbitraire de la probabilite&#769;. Nous propo-
sons ici d&#8217;appre&#769;hender dans un domaine continu le proble&#768;me de l&#8217;estimation d&#8217;un mode&#768;le
linguistique. L&#8217;ide&#769;e consiste a&#768; projeter les indices des mots dans une repre&#769;sentation conti-
nue (un espace vectoriel) et d&#8217;estimer les probabilite&#769;s dans cet espace (Bengio et al., 2003).
Actuellement, un re&#769;seau de neuronesmulti-couches comple&#768;tement connecte&#769; est utilise&#769; pour
apprendre conjointement la projection des mots sur un espace continu et l&#8217;estimation des
probabilite&#769;s n-grammes.
</p>
<p>La lecture humaine des sorties d&#8217;un syste&#768;me statistique de traduction, me&#770;me base&#769; sur
des se&#769;quences de mots, ne&#769;cessite parfois un difficile exercice de re&#769;ordonnancement et de
restructuration syntaxique pour restituer le sens de l&#8217;e&#769;nonce&#769; d&#8217;origine. La mode&#769;lisation
du langage comme une source markovienne (mode&#768;le de langage n-gramme), avec comme
unite&#769; le mot ou la se&#769;quence de mots, ne permet pas de prendre en compte les contraintes
syntaxiques ou les de&#769;pendances a&#768; long terme entre les mots. Il appara&#770;&#305;t donc ne&#769;ces-
saire d&#8217;utiliser des me&#769;thodes dans lesquelles les proprie&#769;te&#769;s structurelles des langues sont
explicitement repre&#769;sente&#769;es. Plusieurs tentatives sur l&#8217;utilisation d&#8217;informations morpho-
syntaxiques dans la traduction statistique ont de&#769;ja&#768; e&#769;te&#769; mene&#769;es. (Och et al., 2004) ont
explore&#769; de nombreuses fonctions caracte&#769;ristiques, dont certaines d&#8217;ordre syntaxique. La
re&#769;e&#769;valuation des n meilleures hypothe&#768;ses avec des e&#769;tiquettes morpho-syntaxiques a e&#769;gale-
ment e&#769;te&#769; e&#769;tudie&#769;e par (Hasan et al., 2006). Dans (Kirchhoff &amp; Yang, 2005), un mode&#768;le de
langage factorise&#769; quadrigramme utilisant des informations syntaxiques n&#8217;a pas montre&#769; des
performances meilleures qu&#8217;un mode&#768;le n-gramme de mots. Les mode&#768;les de langage fonde&#769;s
sur la syntaxe ont enfin e&#769;te&#769; explore&#769;s par (Charniak et al., 2003). Tous ces travaux ont en
commun d&#8217;utiliser des se&#769;quences de mots comme unite&#769;s du syste&#768;me de traduction et de
n&#8217;introduire les cate&#769;gories morpho-syntaxiques que dans une seconde passe de traitement.
</p>
<p>Dans ce travail, nous proposons d&#8217;inte&#769;grer les informations syntaxiques dans le mode&#768;le
de traduction lui-me&#770;me. De plus, nous proposons de combiner cette approche avec les
me&#769;thodes classiques de re&#769;e&#769;valuation de listes de n meilleures hypothe&#768;ses. A&#768; notre connais-
sance, cette approche n&#8217;a pas e&#769;te&#769; e&#769;value&#769;e sur une large ta&#770;che (elle a e&#769;te&#769; applique&#769;e par
(Hwang et al., 2007) a&#768; la ta&#770;che BTEC (Basic Travel Expression Corpus) beaucoup plus
re&#769;duite). Nous pre&#769;sentons ici des re&#769;sultats sur la ta&#770;che Tc-Star (traduction des trans-
criptions des sessions ple&#769;nie&#768;res du Parlement europe&#769;en).
</p>
<p>Cet article est organise&#769; comme suit. Dans la section suivante, nous pre&#769;sentons d&#8217;abord la
structure du syste&#768;me de traduction automatique et ses diffe&#769;rentes extensions. Les re&#769;sultats
expe&#769;rimentaux sont re&#769;sume&#769;s et discute&#769;s dans la section 3. La dernie&#768;re section conclue cet
article et sugge&#768;re des extensions et travaux futurs.
</p>
<p>254</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Mode&#768;les statistiques enrichis par la syntaxe pour la traduction automatique
</p>
<p>2 Description du syste&#768;me
</p>
<p>L&#8217;objectif d&#8217;un syste&#768;me de traduction automatique est de proposer pour une phrase f en
langue &#171; source &#187; sa traduction en une phrase e dans la langue &#171; cible &#187;. L&#8217;approche
statistique consiste a&#768; choisir, parmi les phrases possibles, la plus probable. Le proble&#768;me
se de&#769;compose de la manie&#768;re suivante :
</p>
<p>e&#8727; = argmax
e
</p>
<p>Pr(e|f) = argmax
e
</p>
<p>Pr(f |e) Pr(e),
</p>
<p>ou&#768; la probabilite&#769; Pr(f |e) est estime&#769;e par le mode&#768;le de traduction et Pr(e) par le mo-
de&#768;le de langage de la langue cible. Cette e&#769;quation re&#769;sume l&#8217;approche source/canal his-
torique (Brown et al., 1993) qui conside&#768;re le mot comme unite&#769; et la phrase comme une
se&#769;quence de mots. Le mode&#768;le de traduction peut e&#770;tre estime&#769; automatiquement a&#768; partir de
textes paralle&#768;les aligne&#769;s au niveau de la phrase. Ce calcul est effectue&#769; par le logiciel libre
GIZA++.
</p>
<p>Ces dernie&#768;res anne&#769;es, les travaux en traduction statistique ont e&#769;tendu avec succe&#768;s l&#8217;unite&#769;
qu&#8217;e&#769;tait le mot a&#768; la se&#769;quence de mots (Och et al., 1999; Koehn et al., 2003). Cette nouvelle
unite&#769; se de&#769;finit alors comme un groupe de mots successifs f&#771; de la langue source. Sa
traduction est e&#769;galement une se&#769;quence de mots e&#771; dans la phrase cible. Les se&#769;quences
de mots peuvent e&#770;tre extraites automatiquement a&#768; partir de donne&#769;es bilingues aligne&#769;es
au niveau du mot dans les deux sens. L&#8217;utilisation du principe du maximum d&#8217;entropie
permet de de&#769;composer le proble&#768;me de la manie&#768;re suivante (Och &amp; Ney, 2002) :
</p>
<p>e&#8727; = argmax p(e|f) = argmax
e
</p>
<p>{exp
&#8721;
i
</p>
<p>&#955;ihi(e, f)} (1)
</p>
<p>ou&#768; chaque fonction hi quantifie l&#8217;ade&#769;quation des phrases f et e1. Les coefficients &#955;i pon-
de&#768;rent l&#8217;importance relative de ces fonctions.
</p>
<p>2.1 De&#769;codeur Moses
</p>
<p>Moses2 est un syste&#768;me de traduction automatique a&#768; base de se&#769;quences de mots a&#768; l&#8217;e&#769;tat de
l&#8217;art. Il est distribue&#769; librement avec les scripts ne&#769;cessaires a&#768; l&#8217;entra&#770;&#305;nement d&#8217;un syste&#768;me
de traduction complet, ainsi qu&#8217;une mise en &#339;uvre efficace d&#8217;un algorithme de recherche
de type recherche en faisceau pour produire les traductions. Le de&#769;codeur Moses peut
e&#769;galement ge&#769;ne&#769;rer une liste des n hypothe&#768;ses envisage&#769;es les plus probables. Cette liste
des n meilleures hypothe&#768;ses contient en ge&#769;ne&#769;ral plusieurs fois la me&#770;me phrase, avec des
probabilite&#769;s diffe&#769;rentes, puisque plusieurs segmentations de la phrase source en se&#769;quences
de mots peuvent aboutir a&#768; une me&#770;me phrase cible. Comme effectue&#769; dans les expe&#769;riences
ci-dessous, il est possible de contraindre le de&#769;codeur pour que cette liste contienne n
hypothe&#768;ses distinctes.
</p>
<p>Dans sa version standard, Moses utilise huit fonctions caracte&#769;ristiques mode&#769;lisant le pro-
cessus de traduction. Ces fonctions permettent d&#8217;inte&#769;grer a&#768; la recherche de la phrase cible
les contraintes suivantes : les probabilite&#769;s de traduction des se&#769;quences de mots dans les
</p>
<p>1Cette &#171; ade&#769;quation &#187; est a&#768; prendre au sens large, puisqu&#8217;un syste&#768;me de traduction inclut toujours un
mode&#768;le de langage cible hi(e, f) = p(e).
</p>
<p>2http://www.statmt.org/moses/
</p>
<p>255</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>H. Schwenk, D. De&#769;chelotte, H. Bonneau-Maynard, A. Allauzen
</p>
<p>deux sens, les probabilite&#769;s de traduction des mots dans les deux sens, une mesure de
distorsion, deux pe&#769;nalite&#769;s d&#8217;insertion de mots et de se&#769;quences de mots, et la probabilite&#769;
calcule&#769;e par le mode&#768;le de langage de la langue cible.
</p>
<p>L&#8217;approche couramment employe&#769;e pour optimiser les poids &#955;i des fonctions caracte&#769;ris-
tiques est la maximisation sur un corpus de de&#769;veloppement de la mesure BLEU (Papineni
et al., 2002). Pour cela, l&#8217;outil d&#8217;optimisation nume&#769;rique Condor (Berghen &amp; Bersini,
2005) est inte&#769;gre&#769; a&#768; l&#8217;algorithme ite&#769;ratif suivant :
</p>
<p>1. Partant d&#8217;un jeu de poids initial, les listes des n = 1000 meilleures hypothe&#768;ses sont
ge&#769;ne&#769;re&#769;es avec Moses (une liste par phrase source).
</p>
<p>2. Ces listes sont re&#769;e&#769;value&#769;es en utilisant le jeu de poids courant.
</p>
<p>3. Les meilleures hypothe&#768;ses sont extraites et e&#769;value&#769;es.
</p>
<p>4. A&#768; partir du score BLEU aisni calcule&#769;, Condor calcule un nouveau jeu de poids
(l&#8217;algorithme retourne alors a&#768; l&#8217;e&#769;tape 2), sauf si un maximum local est de&#769;tecte&#769; ce
qui met fin a&#768; l&#8217;algorithme.
</p>
<p>Le jeu de poids solution est en ge&#769;ne&#769;ral trouve&#769; apre&#768;s une centaine d&#8217;ite&#769;rations. Remarquons
que les listes des 1000 meilleures hypothe&#768;ses sont ge&#769;ne&#769;re&#769;es une seule fois lors de l&#8217;initiali-
sation et que les ite&#769;rations re&#769;e&#769;valuent les listes des 1000 meilleures hypothe&#768;ses en fonction
des poids propose&#769;s par Condor.
</p>
<p>2.2 De&#769;sambigu&#776;&#305;sation lexicale par cate&#769;gories syntaxiques
</p>
<p>D&#8217;une langue a&#768; l&#8217;autre, les structures et les proprie&#769;te&#769;s syntaxiques diffe&#768;rent, par exemple
l&#8217;espagnol est une langue fortement fle&#769;chie alors que l&#8217;anglais l&#8217;est peu. Or ces structures
syntaxiques induisent des ambigu&#776;&#305;te&#769;s lexicales qui ne sont pas explicitement prises en
compte par la mode&#769;lisation statistique du processus de traduction de&#769;crit dans la section
ci-dessus.
</p>
<p>Il est toujours possible d&#8217;utiliser des mode&#768;les de langage n-grammes de cate&#769;gories morpho-
syntaxiques pour re&#769;e&#769;valuer les listes des n meilleures hypothe&#768;ses de mots ge&#769;ne&#769;re&#769;es par un
syste&#768;me de traduction. Ce processus ne&#769;cessite alors d&#8217;e&#769;tiqueter les hypothe&#768;ses contenues
dans les listes. Cependant, les e&#769;tiqueteursmorpho-syntaxiques ont e&#769;te&#769; appris sur des e&#769;non-
ce&#769;s correctement forme&#769;s, ce qui n&#8217;est pas toujours le cas des hypothe&#768;ses provenant d&#8217;un
syste&#768;me de traduction automatique. Cette e&#769;tape peut ainsi e&#770;tre une source d&#8217;erreurs qui
limite les performances de la re&#769;e&#769;valuation. Nous proposons donc d&#8217;inte&#769;grer les cate&#769;gories
morpho-syntaxiques au c&#339;ur du mode&#768;le de traduction, ce qui permet d&#8217;e&#769;viter cet e&#769;cueil.
L&#8217;e&#769;tiqueteur est alors utilise&#769; sur des e&#769;nonce&#769;s syntaxiquement corrects (en tout cas, des
e&#769;nonce&#769;s re&#769;ellement produits), ici sur les corpus paralle&#768;les. Par ailleurs, utiliser lors de
l&#8217;apprentissage des corpus e&#769;tiquete&#769;s morpho-syntaxiquement dans les deux langues per-
met de prendre en compte les spe&#769;cificite&#769;s syntaxiques des deux langues et leur interaction,
alors que dans le cas de la re&#769;e&#769;valuation des listes de meilleures hypothe&#768;ses, seules les
spe&#769;cificite&#769;s de la langue cible interviennent.
</p>
<p>Nous proposons d&#8217;utiliser dans le mode&#768;le de traduction des unite&#769;s enrichies constitue&#769;es
des formes de surface des mots, auxquelles sont agglutine&#769;es leurs cate&#769;gories morpho-
syntaxiques respectives. Cette me&#769;thode permet une de&#769;sambigu&#776;&#305;sation des mots tenant
</p>
<p>256</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Mode&#768;les statistiques enrichis par la syntaxe pour la traduction automatique
</p>
<p>compte de leurs ro&#770;les et de leurs contextes grammaticaux. Un exemple d&#8217;e&#769;nonce&#769;, avec les
unite&#769;s enrichies, est donne&#769; a&#768; la Figure 1 en anglais et en espagnol.
</p>
<p>Anglais : IPP declareV V P resumedV V D theDT sessionNN ofIN theDT
EuropeanNP ParliamentNP
</p>
<p>Espagnol : declaroV Lfin reanudadoV Ladj elART per&#769;&#305;odoNC dePREP sesionesNC
delPDEL ParlamentoNC EuropeoADJ
</p>
<p>Fig. 1 &#8211; Exemple d&#8217;un texte paralle&#768;le compose&#769; d&#8217;unite&#769;s enrichies utilise&#769; pour entra&#770;&#305;ner le
mode&#768;le de traduction.
</p>
<p>Lorsque les mode&#768;les de traduction et de langage sont fonde&#769;s sur les unite&#769;s enrichies, le
syste&#768;me de traduction attend en entre&#769;e et produit en sortie des se&#769;quences d&#8217;unite&#769;s enri-
chies. Ainsi les phrases a&#768; traduire doivent e&#770;tre pre&#769;alablement e&#769;tiquete&#769;es. Re&#769;ciproquement,
si une traduction classique est requise en sortie, il est ne&#769;cessaire de retirer les cate&#769;gories
morpho-syntaxiques de l&#8217;hypothe&#768;se propose&#769;e.
</p>
<p>Par ailleurs, il devient possible, sur la base des n meilleures hypothe&#768;ses enrichies, d&#8217;effec-
tuer une re&#769;e&#769;valuation en utilisant un mode&#768;le n-gramme de cate&#769;gories morpho-syntaxiques,
sans avoir a&#768; utiliser a posteriori un e&#769;tiqueteur sur ces hypothe&#768;ses.
</p>
<p>Pour les expe&#769;riences pre&#769;sente&#769;es dans cet article, nous avons utilise&#769; TreeTagger (Schmid,
1994), un e&#769;tiqueteur markovien utilisant des arbres de de&#769;cision pour estimer les probabi-
lite&#769;s trigramme de transition. Ce logiciel est librement disponible pour les deux langues
conside&#769;re&#769;es dans cet article. La version anglaise a e&#769;te&#769; entra&#770;&#305;ne&#769;e sur le corpus PENN tree-
bank 3, et la version espagnole sur le corpus CRATER4. Le nombre de cate&#769;gories est assez
restreint : 59 pour l&#8217;anglais et 69 pour l&#8217;espagnol. Notons que les cate&#769;gories espagnoles ne
contiennent pas de distinction en genre et en nombre.
</p>
<p>2.3 Mode&#768;le de langage neuronal
</p>
<p>L&#8217;architecture du mode&#768;le de langage neuronal est re&#769;sume&#769;e a&#768; la Figure 2. Un re&#769;seau de
neurones multi-couches comple&#768;tement connecte&#769; est utilise&#769; pour apprendre conjointement
la projection des mots dans un espace continu et l&#8217;estimation des probabilite&#769;s n-grammes.
</p>
<p>Les entre&#769;es du re&#769;seau sont les n&#8722;1 mots pre&#769;ce&#769;dents du vocabulaire et les sorties sont les
probabilite&#769;s a-posteriori pour tous les mots du vocabulaire :
</p>
<p>P (wj = i|wj&#8722;n+1, ..., wj&#8722;2, wj&#8722;1) = P (wj = i|hj) &#8704;i &#8712; [1, N ] (2)
</p>
<p>ou&#768; N est la taille du vocabulaire et hj le contexte wj&#8722;n+1, ..., wj&#8722;1. Ces entre&#769;es sont
projete&#769;es sur un espace continu (couche P dans la Figure 2). Les autres couches servent
a&#768; l&#8217;estimation non-line&#769;aire des probabilite&#769;s. La valeur de la i-e&#768;me sortie correspond a&#768; la
probabilite&#769; du n-gramme P (wj= i|hj). Le re&#769;seau calcule donc directement les probabilite&#769;s
de tous les mots du vocabulaire pour le me&#770;me contexte. L&#8217;apprentissage se fait par re&#769;tro-
propagation du gradient, en utilisant la cross-entropie comme fonction d&#8217;erreur.
</p>
<p>3http://www.cis.upenn.edu/~treebank
4http://www.comp.lancs.ac.uk/linguistics/crater/corpus.html
</p>
<p>257</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>H. Schwenk, D. De&#769;chelotte, H. Bonneau-Maynard, A. Allauzen
</p>
<p>couche de
projection couche
</p>
<p>cach&#233;e
</p>
<p>couche de
sortie
</p>
<p>entr&#233;e
</p>
<p>partag&#233;es
projections
</p>
<p>probabilit&#233;s
pour tous les mots
</p>
<p>R&#233;seau de Neurones
</p>
<p>repr&#233;sentation:
discr&#232;te
</p>
<p>indices dans le vocab.
</p>
<p>repr&#233;sentation:
continue
</p>
<p>vecteurs de dim. P
</p>
<p>estimation des probabilit&#233;s 
</p>
<p>N
</p>
<p>wj&#8722;1 P
</p>
<p>H
</p>
<p>N
</p>
<p>P (wj=1|hj)
wj&#8722;n+1
</p>
<p>wj&#8722;n+2
</p>
<p>P (wj=i|hj)
</p>
<p>P (wj=N|hj)
</p>
<p>cl
</p>
<p>oiM
</p>
<p>Vdj
</p>
<p>p1 =
</p>
<p>pN =
</p>
<p>pi =
</p>
<p>Fig. 2 : Architecture du
mode&#768;le de langage neuro-
nal. hj de&#769;nomme le contexte
wj&#8722;n+1, . . . , wj&#8722;1. P est la
taille d&#8217;une projection, et H
et N correspondent a&#768; la di-
mension de la couche cache&#769;e
et de sortie, respectivement.
</p>
<p>Dans ce mode&#768;le, la complexite&#769; est domine&#769;e par la taille importante de la couche de sortie.
Ainsi, nous proposons de limiter l&#8217;estimation des probabilite&#769;s aux 8 192 mots les plus fre&#769;-
quents, les autres mots e&#769;tant traite&#769;s par le mode&#768;le a&#768; repli standard. Dans nos expe&#769;riences,
environ 90% des reque&#770;tes de probabilite&#769;s sont traite&#769;es par le re&#769;seau de neurones. Il est
important de noter que tous les mots du vocabulaire sont conside&#769;re&#769;s a&#768; l&#8217;entre&#769;e du re&#769;seau.
</p>
<p>Ce mode&#768;le de langage a e&#769;te&#769; utilise&#769; avec succe&#768;s dans un syste&#768;me de reconnaissance de la
parole a&#768; grand vocabulaire (Schwenk, 2007), et dans un syste&#768;me de traduction statistique
pour la ta&#770;cheBtec avec un nombre tre&#768;s limite&#769; de donne&#769;es d&#8217;apprentissage (Schwenk et al.,
2006). Cet article de&#769;crit la premie&#768;re application du mode&#768;le de langage neuronal dans un
syste&#768;me de traduction statistique avec plusieurs milliers d&#8217;exemples d&#8217;apprentissage.
</p>
<p>3 Re&#769;sultats expe&#769;rimentaux
</p>
<p>Les expe&#769;riences de&#769;crites dans cet article ont e&#769;te&#769; effectue&#769;es dans le cadre des e&#769;valuations
internationales organise&#769;es par le projet europe&#769;en Tc-Star5. L&#8217;objectif de ce projet est de
motiver, fe&#769;de&#769;rer, et promouvoir les recherches sur la traduction automatique de la parole.
La ta&#770;che principale de ce projet est la traduction des transcriptions des sessions ple&#769;nie&#768;res
du Parlement europe&#769;en (SPPE). La communaute&#769; europe&#769;ennemet a&#768; disposition lesminutes
de ces sessions en plusieurs langues, aussi connues sous le nom &#171; E&#769;ditions du texte final &#187;
(ETF). Ces textes, aligne&#769;s au niveau des phrases, sont utilise&#769;s pour apprendre les mode&#768;les
statistiques. Nous disposons e&#769;galement d&#8217;environ 100 heures d&#8217;enregistrement des sessions
ple&#769;nie&#768;res du Parlement europe&#769;en. Ces donne&#769;es audio ont e&#769;te&#769; transcrites manuellement et
servent principalement au de&#769;veloppement des syste&#768;mes de reconnaissance de la parole,
mais elles sont aussi utilise&#769;es pour entra&#770;&#305;ner les mode&#768;les de langage cible dans le syste&#768;me
de traduction.
</p>
<p>Trois conditions sont conside&#769;re&#769;es dans les e&#769;valuations Tc-Star : la traduction des mi-
nutes ETF (texte), la traduction des transcriptions des donne&#769;es acoustiques (verbatim)
et la traduction des hypothe&#768;ses du syste&#768;me de reconnaissance de la parole (parole). Dans
ce travail, nous ne conside&#769;rons que la condition verbatim, pour la paire de langues espa-
gnol/anglais. Nous donnons des re&#769;sultats sur les donne&#769;es de de&#769;veloppement et de test de
</p>
<p>5http://www.tc-star.org/
</p>
<p>258</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Mode&#768;les statistiques enrichis par la syntaxe pour la traduction automatique
</p>
<p>l&#8217;e&#769;valuation organise&#769;e en 2007. Deux traductions de re&#769;fe&#769;rence sont disponibles pour les
deux jeux de test. Plusieurs e&#769;tapes de normalisation ont e&#769;te&#769; applique&#769;es aux minutes des
sessions ple&#769;nie&#768;res afin d&#8217;approcher la condition verbatim ou parole, notamment la trans-
formation en mots des nombres. Les mode&#768;les de traduction sont estime&#769;s sur les donne&#769;es
SPPE qui repre&#769;sentent 1,2M de phrases paralle&#768;les, soit environ 35M de mots en anglais.
</p>
<p>3.1 Apprentissage des mode&#768;les de langage
</p>
<p>Pour l&#8217;apprentissage des mode&#768;les de langage, nous avons utilise&#769; la partie monolingue des
donne&#769;es paralle&#768;les SPPE ainsi que les transcriptions des donne&#769;es acoustiques. Des donne&#769;es
exte&#769;rieures ont e&#769;galement e&#769;te&#769; utilise&#769;es pour une estimation plus robuste desmode&#768;les : deux
corpus de textes provenant des parlements espagnol (49M mots) et britannique (55M
mots). Ainsi, pour chaque langue, nous disposons de trois sources de texte donnant lieu
a&#768; l&#8217;estimation de trois mode&#768;les inde&#769;pendants. Ces trois mode&#768;les sont in fine interpole&#769;s
line&#769;airement pour cre&#769;er un mode&#768;le de la langue cible. Les coefficients d&#8217;interpolation sont
estime&#769;s via l&#8217;algorithme E.M. de manie&#768;re a&#768; minimiser la perplexite&#769; sur les donne&#769;es de
de&#769;veloppement. Les coefficients obtenus sont 0,81 pour le mode&#768;le SPPE, 0,12 pour le
mode&#768;le estime&#769; sur les donne&#769;es additionnelles du parlement et 0,07 pour celui utilisant les
transcriptions acoustiques.
</p>
<p>Tous les mode&#768;les de langage n-grammes utilise&#769;s, hormis le mode&#768;le neuronal, sont des
mode&#768;les classiques avec repli utilisant le lissage de Kneser-Ney modifie&#769;. Le SRI LM-toolkit
(Stolcke, 2002) a e&#769;te&#769; utilise&#769; pour leur construction.
</p>
<p>Les caracte&#769;ristiques des donne&#769;es et les perplexite&#769;s des mode&#768;les de langage sont re&#769;sume&#769;es
dans le Tableau 1. Lesmode&#768;les trigrammes interviennent pendant le de&#769;codage, alors que les
mode&#768;les quadrigrammes sont utilise&#769;s pour re&#769;e&#769;valuer les listes de n meilleures hypothe&#768;ses.
Lemode&#768;le de langage neuronal obtient une re&#769;duction de la perplexite&#769; de 15% environ. Il est
a&#768; noter que les donne&#769;es de de&#769;veloppement en anglais, donc la traduction de l&#8217;espagnol vers
l&#8217;anglais, proviennent de deux sources diffe&#769;rentes (parlements europe&#769;en et espagnol). Cette
diffe&#769;rence explique les perplexite&#769;s relativement e&#769;leve&#769;es. Les perplexite&#769;s sur les donne&#769;es
du Parlement europe&#769;en uniquement sont plus basses : 85.0, 77.8 et 64.3 pour le tri-,
quadrigramme a&#768; repli et le quadrigramme neuronal respectivement.
</p>
<p>Anglais Espagnol
Textes du Parlement europe&#769;en 35,3M 36,6M
Textes parlementaires supple&#769;mentaires 55,1M 48,9M
Transcriptions acoustiques 1,5M 777k
Vocabulaire 82,6k 132,5k
Perplexite&#769; trigramme 134,5 69,7
Quadrigramme a&#768; repli 123,4 64,0
Quadrigramme neuronal 102,8 54,6
</p>
<p>Tab. 1 &#8211; Donne&#769;es d&#8217;apprentissage (en nombre de mots) utilise&#769;es pour l&#8217;estimation des
mode&#768;les de langage et perplexite&#769;s obtenues sur les donne&#769;es de de&#769;veloppement.
</p>
<p>259</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>H. Schwenk, D. De&#769;chelotte, H. Bonneau-Maynard, A. Allauzen
</p>
<p>3.2 Re&#769;sultats sur les donne&#769;es de de&#769;veloppement
</p>
<p>Nous avons effectue&#769; de nombreuses e&#769;tudes comparatives sur les donne&#769;es de de&#769;veloppement
pour e&#769;valuer les apports des diffe&#769;rentes techniques. Les re&#769;sultats principaux sont re&#769;sume&#769;s
dans le Tableau 2. En ce qui concerne la de&#769;sambigu&#776;&#305;sation lexicale, seul le sens de tra-
duction de l&#8217;anglais vers l&#8217;espagnol (vers la langue la plus infle&#769;chie) a e&#769;te&#769; e&#769;value&#769; a&#768; ce jour.
Pour chaque sens de traduction, le score Bleu du mode&#768;le de base avec un trigramme est
donne&#769;, ainsi qu&#8217;apre&#768;s la re&#769;e&#769;valuation avec un quadrigramme a&#768; repli et neuronal.
</p>
<p>L&#8217;utilisation d&#8217;un quadrigramme permet d&#8217;augmenter le score Bleu d&#8217;environ 0,4 points
pour la traduction vers l&#8217;anglais et de 0,6 points vers l&#8217;espagnol. Nous avons e&#769;galement
essaye&#769; de re&#769;e&#769;valuer les n meilleures hypothe&#768;ses avec des mode&#768;les de langage n-grammes
de cate&#769;gories morpho-syntaxiques, mais sans effet sur les performances du syste&#768;me. L&#8217;uti-
lisation du mode&#768;le de langage neuronal, par ailleurs, produit une ame&#769;lioration du score
BLEU de plus de 0,6 points pour les deux directions.
</p>
<p>Espagnol &#8594; anglais Anglais &#8594; espagnol
</p>
<p>Sans de&#769;sambigu&#776;&#305;sation Sans de&#769;sambigu&#776;&#305;sation Avec de&#769;sambigu&#776;&#305;sation
</p>
<p>base 4-gram NNLM base 4-gram NNLM base 4-gram NNLM
</p>
<p>BLEU 47,20 47,64 48,26 48,78 49,39 50,15 48,92 49,45 50,30
</p>
<p>Tab. 2 &#8211; Scores BLEU sur les donne&#769;es de de&#769;veloppement. NNLM de&#769;nomme le mode&#768;le de
langage neuronal.
</p>
<p>Les gains apporte&#769;s par la de&#769;sambigu&#776;&#305;sation lexicale par cate&#769;gories syntaxiques sont relati-
vement faibles lorsqu&#8217;on conside&#768;re les syste&#768;mes avec un tri- ou quadrigramme a&#768; repli. La&#768;
encore, une re&#769;e&#769;valuation avec des mode&#768;les n-grammes de cate&#769;gories syntaxique n&#8217;est pas
efficace. Cependant, les re&#769;sultats sont inte&#769;ressants lorsqu&#8217;on combine la mode&#769;lisation de
langage neuronal et la de&#769;sambigu&#776;&#305;sation lexicale : le score Bleu passe de 49,39 a&#768; 50,30.
Ceci montre bien l&#8217;inte&#769;re&#770;t de travailler conjointement sur une ame&#769;lioration des techniques
statistiques et sur l&#8217;incorporation de connaissances lexicales ou syntaxiques. En effet, la
re&#769;e&#769;valuation des n meilleures hypothe&#768;ses avec un mode&#768;le de langage semble e&#770;tre plus
efficace si les mots propose&#769;s par le mode&#768;le de traduction sont mieux choisis.
</p>
<p>3.3 Re&#769;sultats sur les donne&#769;es de test
</p>
<p>Les performances sur les donne&#769;es de test de l&#8217;e&#769;valuationTc-Star 2007 sont re&#769;sume&#769;es dans
le Tableau 3. Les coefficients &#955;i des fonctions caracte&#769;ristiques sont les me&#770;mes que ceux du
syste&#768;me optimise&#769; sur les donne&#769;es de de&#769;veloppement. Le syste&#768;me n&#8217;a donc pas e&#769;te&#769; adapte&#769;
sur les donne&#769;es de test. Sept centres de recherche publiques et industriels ont participe&#769;
a&#768; l&#8217;e&#769;valuation qui s&#8217;est de&#769;roule&#769;e en fe&#769;vrier 2007. Les scores BLEU varient entre 42.95
et 49.60 (espagnol/anglais) et entre 37.39 et 51.04 (anglais/espagnol). Les performances
du syste&#768;me avec de&#769;sambigu&#776;&#305;sation lexicale sont tre&#768;s le&#769;ge&#768;rement au-dessous du syste&#768;me de
base, dans le cas de l&#8217;utilisation d&#8217;un mode&#768;le de langage a&#768; repli. Cependant la combinaison
avec un mode&#768;le de langage neuronal donne de bons re&#769;sultats, sans pour autant pouvoir
de&#769;passer le syste&#768;me sans de&#769;sambigu&#776;&#305;sation.
</p>
<p>260</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Mode&#768;les statistiques enrichis par la syntaxe pour la traduction automatique
</p>
<p>Espagnol &#8594; anglais Anglais &#8594; espagnol
Sans de&#769;sambigu&#776;&#305;sation Sans de&#769;sambigu&#776;&#305;sation Avec de&#769;sambigu&#776;&#305;sation
base 4-gram NNLM base 4-gram NNLM base 4-gram NNLM
</p>
<p>BLEU 48,42 48,67 49,19 49,19 50,17 51,04 49,13 49,91 51,04
</p>
<p>Tab. 3 &#8211; Scores BLEU sur les donne&#769;es de test.
</p>
<p>4 Conclusion
</p>
<p>Nous avons pre&#769;sente&#769; et e&#769;value&#769; deux e&#769;volutions d&#8217;un syste&#768;me de traduction statistique.
L&#8217;une propose une mode&#769;lisation linguistique dans un espace continu et la seconde inte&#768;gre
les cate&#769;gories morpho-syntaxiques des mots dans le mode&#768;le de traduction. La combinaison
des deux me&#769;thodes donne des re&#769;sultats inte&#769;ressants. Notre syste&#768;me a obtenu de tre&#768;s bons
re&#769;sultats a&#768; l&#8217;e&#769;valuation Tc-Star organise&#769;e de&#769;but 2007.
</p>
<p>Nous e&#769;tudions aussi l&#8217;application des me&#770;mes techniques a&#768; la traduction automatique
d&#8217;autres paires de langues, notamment la traduction entre l&#8217;anglais et le franc&#807;ais. Pour
cela le corpus Europarl est utilise&#769; (Koehn, 2006). Nous sommes en train de produire une
deuxie&#768;me re&#769;fe&#769;rence de traduction qui sera librement disponible pour d&#8217;autres laboratoires
de recherche inte&#769;resse&#769;s dans la traduction automatique du franc&#807;ais6.
</p>
<p>Plusieurs extensions du syste&#768;me de&#769;crit dans cet article sont actuellement a&#768; l&#8217;e&#769;tude. Nous
travaillons sur une meilleure incorporation des connaissances linguistiques, notamment
sur l&#8217;utilisation d&#8217;e&#769;tiqueteurs prenant en compte le genre et le nombre, voire le sens des
mots, afin d&#8217;ame&#769;liorer la de&#769;sambigu&#776;&#305;sation dans le mode&#768;le de traduction. Un logiciel de
visualisation des erreurs de traduction est en cours de de&#769;veloppement afin de permettre
une analyse qualitative des erreurs pour affiner le choix des e&#769;tiquettes, notamment pour le
franc&#807;ais. En ce qui concerne l&#8217;ame&#769;lioration des techniques statistiques, nous sommes tre&#768;s
inte&#769;resse&#769;s par une repre&#769;sentation factorise&#769;e desmots, incluant notamment des informations
morpho-syntaxiques et linguistiques, aussi bien pour le mode&#768;le de traduction que pour le
mode&#768;le de la langue cible.
</p>
<p>Remerciements
</p>
<p>Ces recherches ont e&#769;te&#769; partiellement finance&#769;es par le projet europe&#769;en Tc-Star et par le
projet Anr Instar, JCJC06 143038.
</p>
<p>Re&#769;fe&#769;rences
</p>
<p>Bengio Y., Ducharme R., Vincent P. &amp; Jauvin C. (2003). A neural probabilistic
language model. Journal of Machine Learning Research, 3(2), 1137&#8211;1155.
</p>
<p>Berghen F. V. &amp; Bersini H. (2005). CONDOR, a new parallel, constrained extension
of powell&#8217;s UOBYQA algorithm : Experimental results and comparison with the DFO
algorithm. Journal of Computational and Applied Mathematics, 181, 157&#8211;175.
</p>
<p>6Donne&#769;es disponibles a&#768; partir de la page internet http://instar.limsi.fr
</p>
<p>261</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>H. Schwenk, D. De&#769;chelotte, H. Bonneau-Maynard, A. Allauzen
</p>
<p>Brown P., Della Pietra S., Della Pietra V. J. &amp; Mercer R. (1993). The
mathematics of statistical machine translation. Computational Linguistics, 19(2), 263&#8211;
311.
</p>
<p>Charniak E., Knight K. &amp; Yamada K. (2003). Syntax-based language models for
machine translation. In MT Summit.
</p>
<p>Hasan S., Bender O. &amp; Ney H. (2006). Reranking translation hypothesis using
structural properties. In EACL Workshop on Learning Structured Information in Natural
Language Applications.
</p>
<p>Hwang Y., Finch A. &amp; Sasaki Y. (2007). Improving statistical machine translation
using shallow linguistic knowledge. Computer Speech &amp; Language, 21(2), 350&#8211;372.
</p>
<p>Kirchhoff K. &amp; Yang M. (2005). Improved languagemodeling for statisticalmachine
translation. In ACL&#8217;05 workshop on Building and Using Parallel Text, p. 125&#8211;128.
</p>
<p>Koehn P. (2006). Europarl : A parallel corpus for statistical machine translation. In
MT Summit.
</p>
<p>Koehn P., Och F. J. &amp; Marcu D. (2003). Statistical phrased-based machine trans-
lation. In Joint Conference on Human Language Technology and of the North American
Chapter of the Asociation for Computational Lingustics, p. 127&#8211;133.
</p>
<p>Och F.-J., Gildea D., Khudanpur S., Sarkar A., Yamada K., Fraser A.,
Kumar S., Shen L., Smith D., Eng K., Jain V., Jin Z. &amp; Radev D. (2004). A
smorgasbord of features for statistical machine translation. In Proceedings of the North
American Chapter of the Asociation for Computational Lingustics, p. 161&#8211;168.
</p>
<p>Och F. J. &amp; Ney H. (2002). Discriminative training and maximum entropy models
for statistical machine translation. In Proceedings of ACL, p. 295&#8211;302.
</p>
<p>Och F. J., Tillmann C. &amp; Ney H. (1999). Improved alignment models for statistical
machine translation. In Joint SIGDAT Conference on Empirical Methods in Natural
Language Processing and Very Large Copora, p. 20&#8211;28.
</p>
<p>Papineni K., Roukos S., Ward T. &amp; Zhu W. (2002). BLEU : a method for auto-
matic evaluation of machine translation. In Proceedings of ACL, p. 311&#8211;318.
</p>
<p>Schmid H. (1994). Probabilistic part-of-speech tagging using decision trees. In Procee-
dings of International Conference on New Methods in Language Processing.
</p>
<p>Schwenk H. (2007). Continuous space language models. Computer Speech and Lan-
guage, 21, 492&#8211;518.
</p>
<p>Schwenk H., Costa-jussa&#768; M. R. &amp; Fonollosa J. A. R. (2006). Continuous
space language models for the IWSLT 2006 task. In International Workshop on Spoken
Language Translation, p. 166&#8211;173.
</p>
<p>Stolcke A. (2002). SRILM - an extensible language modeling toolkit. In International
Conference on Speech and Language Processing, p. II : 901&#8211;904.
</p>
<p>262</p>

</div></div>
</body></html>