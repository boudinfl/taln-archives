<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Y Bengio</author>
<author>R Ducharme</author>
<author>P Vincent</author>
<author>C Jauvin</author>
</authors>
<title>A neural probabilistic language model.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>3</volume>
<issue>2</issue>
<pages>1137--1155</pages>
<contexts>
<context position="3924" citStr="Bengio et al., 2003" startWordPosition="539" endWordPosition="542">e à repli. Dans ces modèles classiques, les mots sont représentés par un indice dans un espace discret, le vocabulaire. Ceci ne permet pas de faire de véritables interpolations des probabilités d’un n-gramme non observé puisqu’un changement dans l’espace des mots peut entrâıner un changement arbitraire de la probabilité. Nous proposons ici d’appréhender dans un domaine continu le problème de l’estimation d’un modèle linguistique. L’idée consiste à projeter les indices des mots dans une représentation continue (un espace vectoriel) et d’estimer les probabilités dans cet espace (Bengio et al., 2003). Actuellement, un réseau de neuronesmulti-couches complètement connecté est utilisé pour apprendre conjointement la projection des mots sur un espace continu et l’estimation des probabilités n-grammes. La lecture humaine des sorties d’un système statistique de traduction, même basé sur des séquences de mots, nécessite parfois un difficile exercice de réordonnancement et de restructuration syntaxique pour restituer le sens de l’énoncé d’origine. La modélisation du langage comme une source markovienne (modèle de langage n-gramme), avec comme unité le mot ou la séquence de mots,</context>
</contexts>
<marker>Bengio, Ducharme, Vincent, Jauvin, 2003</marker>
<rawString>Bengio Y., Ducharme R., Vincent P. &amp; Jauvin C. (2003). A neural probabilistic language model. Journal of Machine Learning Research, 3(2), 1137–1155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F V Berghen</author>
<author>H Bersini</author>
</authors>
<title>CONDOR, a new parallel, constrained extension of powell’s UOBYQA algorithm : Experimental results and comparison with the DFO algorithm.</title>
<date>2005</date>
<journal>Journal of Computational and Applied Mathematics,</journal>
<volume>181</volume>
<pages>157--175</pages>
<contexts>
<context position="10217" citStr="Berghen &amp; Bersini, 2005" startWordPosition="1479" endWordPosition="1482">angage cible hi(e, f) = p(e). 2http://www.statmt.org/moses/ 255 H. Schwenk, D. Déchelotte, H. Bonneau-Maynard, A. Allauzen deux sens, les probabilités de traduction des mots dans les deux sens, une mesure de distorsion, deux pénalités d’insertion de mots et de séquences de mots, et la probabilité calculée par le modèle de langage de la langue cible. L’approche couramment employée pour optimiser les poids λi des fonctions caractéristiques est la maximisation sur un corpus de développement de la mesure BLEU (Papineni et al., 2002). Pour cela, l’outil d’optimisation numérique Condor (Berghen &amp; Bersini, 2005) est intégré à l’algorithme itératif suivant : 1. Partant d’un jeu de poids initial, les listes des n = 1000 meilleures hypothèses sont générées avec Moses (une liste par phrase source). 2. Ces listes sont réévaluées en utilisant le jeu de poids courant. 3. Les meilleures hypothèses sont extraites et évaluées. 4. À partir du score BLEU aisni calculé, Condor calcule un nouveau jeu de poids (l’algorithme retourne alors à l’étape 2), sauf si un maximum local est détecté ce qui met fin à l’algorithme. Le jeu de poids solution est en général trouvé après une centaine d’ité</context>
</contexts>
<marker>Berghen, Bersini, 2005</marker>
<rawString>Berghen F. V. &amp; Bersini H. (2005). CONDOR, a new parallel, constrained extension of powell’s UOBYQA algorithm : Experimental results and comparison with the DFO algorithm. Journal of Computational and Applied Mathematics, 181, 157–175.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Schwenk</author>
<author>D Déchelotte</author>
<author>H Bonneau-Maynard</author>
<author>A Allauzen Brown P</author>
<author>Della Pietra S</author>
<author>Della Pietra V J</author>
<author>R Mercer</author>
</authors>
<title>The mathematics of statistical machine translation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<pages>263--311</pages>
<marker>Schwenk, Déchelotte, Bonneau-Maynard, P, S, J, Mercer, 1993</marker>
<rawString>H. Schwenk, D. Déchelotte, H. Bonneau-Maynard, A. Allauzen Brown P., Della Pietra S., Della Pietra V. J. &amp; Mercer R. (1993). The mathematics of statistical machine translation. Computational Linguistics, 19(2), 263– 311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
<author>K Knight</author>
<author>K Yamada</author>
</authors>
<title>Syntax-based language models for machine translation.</title>
<date>2003</date>
<booktitle>In MT Summit.</booktitle>
<contexts>
<context position="5460" citStr="Charniak et al., 2003" startWordPosition="750" endWordPosition="753">ns morphosyntaxiques dans la traduction statistique ont déjà été menées. (Och et al., 2004) ont exploré de nombreuses fonctions caractéristiques, dont certaines d’ordre syntaxique. La réévaluation des n meilleures hypothèses avec des étiquettes morpho-syntaxiques a également été étudiée par (Hasan et al., 2006). Dans (Kirchhoff &amp; Yang, 2005), un modèle de langage factorisé quadrigramme utilisant des informations syntaxiques n’a pas montré des performances meilleures qu’un modèle n-gramme de mots. Les modèles de langage fondés sur la syntaxe ont enfin été explorés par (Charniak et al., 2003). Tous ces travaux ont en commun d’utiliser des séquences de mots comme unités du système de traduction et de n’introduire les catégories morpho-syntaxiques que dans une seconde passe de traitement. Dans ce travail, nous proposons d’intégrer les informations syntaxiques dans le modèle de traduction lui-même. De plus, nous proposons de combiner cette approche avec les méthodes classiques de réévaluation de listes de n meilleures hypothèses. À notre connaissance, cette approche n’a pas été évaluée sur une large tâche (elle a été appliquée par (Hwang et al., 2007) à la tâche</context>
</contexts>
<marker>Charniak, Knight, Yamada, 2003</marker>
<rawString>Charniak E., Knight K. &amp; Yamada K. (2003). Syntax-based language models for machine translation. In MT Summit.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Hasan</author>
<author>O Bender</author>
<author>H Ney</author>
</authors>
<title>Reranking translation hypothesis using structural properties.</title>
<date>2006</date>
<booktitle>In EACL Workshop on Learning Structured Information in Natural Language Applications.</booktitle>
<contexts>
<context position="5166" citStr="Hasan et al., 2006" startWordPosition="707" endWordPosition="710">re en compte les contraintes syntaxiques ou les dépendances à long terme entre les mots. Il apparâıt donc nécessaire d’utiliser des méthodes dans lesquelles les propriétés structurelles des langues sont explicitement représentées. Plusieurs tentatives sur l’utilisation d’informations morphosyntaxiques dans la traduction statistique ont déjà été menées. (Och et al., 2004) ont exploré de nombreuses fonctions caractéristiques, dont certaines d’ordre syntaxique. La réévaluation des n meilleures hypothèses avec des étiquettes morpho-syntaxiques a également été étudiée par (Hasan et al., 2006). Dans (Kirchhoff &amp; Yang, 2005), un modèle de langage factorisé quadrigramme utilisant des informations syntaxiques n’a pas montré des performances meilleures qu’un modèle n-gramme de mots. Les modèles de langage fondés sur la syntaxe ont enfin été explorés par (Charniak et al., 2003). Tous ces travaux ont en commun d’utiliser des séquences de mots comme unités du système de traduction et de n’introduire les catégories morpho-syntaxiques que dans une seconde passe de traitement. Dans ce travail, nous proposons d’intégrer les informations syntaxiques dans le modèle de traduction </context>
</contexts>
<marker>Hasan, Bender, Ney, 2006</marker>
<rawString>Hasan S., Bender O. &amp; Ney H. (2006). Reranking translation hypothesis using structural properties. In EACL Workshop on Learning Structured Information in Natural Language Applications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Hwang</author>
<author>A Finch</author>
<author>Y Sasaki</author>
</authors>
<title>Improving statistical machine translation using shallow linguistic knowledge.</title>
<date>2007</date>
<journal>Computer Speech &amp; Language,</journal>
<volume>21</volume>
<issue>2</issue>
<pages>350--372</pages>
<contexts>
<context position="6047" citStr="Hwang et al., 2007" startWordPosition="838" endWordPosition="841">rés par (Charniak et al., 2003). Tous ces travaux ont en commun d’utiliser des séquences de mots comme unités du système de traduction et de n’introduire les catégories morpho-syntaxiques que dans une seconde passe de traitement. Dans ce travail, nous proposons d’intégrer les informations syntaxiques dans le modèle de traduction lui-même. De plus, nous proposons de combiner cette approche avec les méthodes classiques de réévaluation de listes de n meilleures hypothèses. À notre connaissance, cette approche n’a pas été évaluée sur une large tâche (elle a été appliquée par (Hwang et al., 2007) à la tâche BTEC (Basic Travel Expression Corpus) beaucoup plus réduite). Nous présentons ici des résultats sur la tâche Tc-Star (traduction des transcriptions des sessions plénières du Parlement européen). Cet article est organisé comme suit. Dans la section suivante, nous présentons d’abord la structure du système de traduction automatique et ses différentes extensions. Les résultats expérimentaux sont résumés et discutés dans la section 3. La dernière section conclue cet article et suggère des extensions et travaux futurs. 254 Modèles statistiques enrichis par la syntax</context>
</contexts>
<marker>Hwang, Finch, Sasaki, 2007</marker>
<rawString>Hwang Y., Finch A. &amp; Sasaki Y. (2007). Improving statistical machine translation using shallow linguistic knowledge. Computer Speech &amp; Language, 21(2), 350–372.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Kirchhoff</author>
<author>M Yang</author>
</authors>
<title>Improved languagemodeling for statisticalmachine translation.</title>
<date>2005</date>
<booktitle>In ACL’05 workshop on Building and Using Parallel Text,</booktitle>
<pages>125--128</pages>
<contexts>
<context position="5197" citStr="Kirchhoff &amp; Yang, 2005" startWordPosition="712" endWordPosition="715">s syntaxiques ou les dépendances à long terme entre les mots. Il apparâıt donc nécessaire d’utiliser des méthodes dans lesquelles les propriétés structurelles des langues sont explicitement représentées. Plusieurs tentatives sur l’utilisation d’informations morphosyntaxiques dans la traduction statistique ont déjà été menées. (Och et al., 2004) ont exploré de nombreuses fonctions caractéristiques, dont certaines d’ordre syntaxique. La réévaluation des n meilleures hypothèses avec des étiquettes morpho-syntaxiques a également été étudiée par (Hasan et al., 2006). Dans (Kirchhoff &amp; Yang, 2005), un modèle de langage factorisé quadrigramme utilisant des informations syntaxiques n’a pas montré des performances meilleures qu’un modèle n-gramme de mots. Les modèles de langage fondés sur la syntaxe ont enfin été explorés par (Charniak et al., 2003). Tous ces travaux ont en commun d’utiliser des séquences de mots comme unités du système de traduction et de n’introduire les catégories morpho-syntaxiques que dans une seconde passe de traitement. Dans ce travail, nous proposons d’intégrer les informations syntaxiques dans le modèle de traduction lui-même. De plus, nous propos</context>
</contexts>
<marker>Kirchhoff, Yang, 2005</marker>
<rawString>Kirchhoff K. &amp; Yang M. (2005). Improved languagemodeling for statisticalmachine translation. In ACL’05 workshop on Building and Using Parallel Text, p. 125–128.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
</authors>
<title>Europarl : A parallel corpus for statistical machine translation.</title>
<date>2006</date>
<booktitle>In MT Summit.</booktitle>
<contexts>
<context position="26361" citStr="Koehn, 2006" startWordPosition="3826" endWordPosition="3827">́volutions d’un système de traduction statistique. L’une propose une modélisation linguistique dans un espace continu et la seconde intègre les catégories morpho-syntaxiques des mots dans le modèle de traduction. La combinaison des deux méthodes donne des résultats intéressants. Notre système a obtenu de très bons résultats à l’évaluation Tc-Star organisée début 2007. Nous étudions aussi l’application des mêmes techniques à la traduction automatique d’autres paires de langues, notamment la traduction entre l’anglais et le français. Pour cela le corpus Europarl est utilisé (Koehn, 2006). Nous sommes en train de produire une deuxième référence de traduction qui sera librement disponible pour d’autres laboratoires de recherche intéressés dans la traduction automatique du français6. Plusieurs extensions du système décrit dans cet article sont actuellement à l’étude. Nous travaillons sur une meilleure incorporation des connaissances linguistiques, notamment sur l’utilisation d’étiqueteurs prenant en compte le genre et le nombre, voire le sens des mots, afin d’améliorer la désambigüısation dans le modèle de traduction. Un logiciel de visualisation des erreurs de tr</context>
</contexts>
<marker>Koehn, 2006</marker>
<rawString>Koehn P. (2006). Europarl : A parallel corpus for statistical machine translation. In MT Summit.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>F J Och</author>
<author>D Marcu</author>
</authors>
<title>Statistical phrased-based machine translation.</title>
<date>2003</date>
<booktitle>In Joint Conference on Human Language Technology and of the North American Chapter of the Asociation for Computational Lingustics,</booktitle>
<pages>127--133</pages>
<contexts>
<context position="2054" citStr="Koehn et al., 2003" startWordPosition="279" endWordPosition="282">ique, modélisation linguistique dans un espace continu, analyse morpho-syntaxique, désambigüısation lexicale. Keywords: statistical machine translation, continuous space language model, POS tagging, lexical disambiguation. 1 Introduction La traduction automatique est un thème de recherche depuis plusieurs décennies et différentes approches ont été proposées, telles que la traduction par règles, la traduction à base d’exemples ou la traduction statistique. Les travaux récents en traduction statistique confirment que les modèles fondés sur des séquences de mots (Och et al., 1999; Koehn et al., 2003) obtiennent des performances significativement meilleures que ceux fondés sur des mots (Brown et al., 1993). En utilisant des séquences de mots, les systèmes de traduction parviennent à préserver certaines contraintes locales sur l’ordre des mots. L’entrâınement d’un tel modèle nécessite l’alignement d’un corpus parallèle. Les régularités du langage naturel comme celles de la syntaxe, ou, encore à un niveau supérieur, celles de la sémantique sont ainsi, en principe, implicitement capturées par les modèles. 253 H. Schwenk, D. Déchelotte, H. Bonneau-Maynard, A. Allauzen Depuis l</context>
<context position="7696" citStr="Koehn et al., 2003" startWordPosition="1097" endWordPosition="1100">́ Pr(f |e) est estimée par le modèle de traduction et Pr(e) par le modèle de langage de la langue cible. Cette équation résume l’approche source/canal historique (Brown et al., 1993) qui considère le mot comme unité et la phrase comme une séquence de mots. Le modèle de traduction peut être estimé automatiquement à partir de textes parallèles alignés au niveau de la phrase. Ce calcul est effectué par le logiciel libre GIZA++. Ces dernières années, les travaux en traduction statistique ont étendu avec succès l’unité qu’était le mot à la séquence de mots (Och et al., 1999; Koehn et al., 2003). Cette nouvelle unité se définit alors comme un groupe de mots successifs f̃ de la langue source. Sa traduction est également une séquence de mots ẽ dans la phrase cible. Les séquences de mots peuvent être extraites automatiquement à partir de données bilingues alignées au niveau du mot dans les deux sens. L’utilisation du principe du maximum d’entropie permet de décomposer le problème de la manière suivan∑te (Och &amp; Ney, 2002) : e∗ = argmax p(e|f) = argmax{exp λihi(e, f)} (1) e i où chaque fonction hi quantifie l’adéquation des phrases f et e1. Les coefficients λi pondèrent l’</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Koehn P., Och F. J. &amp; Marcu D. (2003). Statistical phrased-based machine translation. In Joint Conference on Human Language Technology and of the North American Chapter of the Asociation for Computational Lingustics, p. 127–133.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F-J Och</author>
<author>D Gildea</author>
<author>S Khudanpur</author>
<author>A Sarkar</author>
<author>K Yamada</author>
<author>A Fraser</author>
<author>S Kumar</author>
<author>L Shen</author>
<author>D Smith</author>
<author>K Eng</author>
<author>V Jain</author>
<author>Z Jin</author>
<author>D Radev</author>
</authors>
<title>A smorgasbord of features for statistical machine translation.</title>
<date>2004</date>
<booktitle>In Proceedings of the North American Chapter of the Asociation for Computational Lingustics,</booktitle>
<pages>161--168</pages>
<contexts>
<context position="4934" citStr="Och et al., 2004" startWordPosition="677" endWordPosition="680">ucturation syntaxique pour restituer le sens de l’énoncé d’origine. La modélisation du langage comme une source markovienne (modèle de langage n-gramme), avec comme unité le mot ou la séquence de mots, ne permet pas de prendre en compte les contraintes syntaxiques ou les dépendances à long terme entre les mots. Il apparâıt donc nécessaire d’utiliser des méthodes dans lesquelles les propriétés structurelles des langues sont explicitement représentées. Plusieurs tentatives sur l’utilisation d’informations morphosyntaxiques dans la traduction statistique ont déjà été menées. (Och et al., 2004) ont exploré de nombreuses fonctions caractéristiques, dont certaines d’ordre syntaxique. La réévaluation des n meilleures hypothèses avec des étiquettes morpho-syntaxiques a également été étudiée par (Hasan et al., 2006). Dans (Kirchhoff &amp; Yang, 2005), un modèle de langage factorisé quadrigramme utilisant des informations syntaxiques n’a pas montré des performances meilleures qu’un modèle n-gramme de mots. Les modèles de langage fondés sur la syntaxe ont enfin été explorés par (Charniak et al., 2003). Tous ces travaux ont en commun d’utiliser des séquences de mots comme u</context>
</contexts>
<marker>Och, Gildea, Khudanpur, Sarkar, Yamada, Fraser, Kumar, Shen, Smith, Eng, Jain, Jin, Radev, 2004</marker>
<rawString>Och F.-J., Gildea D., Khudanpur S., Sarkar A., Yamada K., Fraser A., Kumar S., Shen L., Smith D., Eng K., Jain V., Jin Z. &amp; Radev D. (2004). A smorgasbord of features for statistical machine translation. In Proceedings of the North American Chapter of the Asociation for Computational Lingustics, p. 161–168.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>Discriminative training and maximum entropy models for statistical machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>295--302</pages>
<contexts>
<context position="8140" citStr="Och &amp; Ney, 2002" startWordPosition="1168" endWordPosition="1171"> dernières années, les travaux en traduction statistique ont étendu avec succès l’unité qu’était le mot à la séquence de mots (Och et al., 1999; Koehn et al., 2003). Cette nouvelle unité se définit alors comme un groupe de mots successifs f̃ de la langue source. Sa traduction est également une séquence de mots ẽ dans la phrase cible. Les séquences de mots peuvent être extraites automatiquement à partir de données bilingues alignées au niveau du mot dans les deux sens. L’utilisation du principe du maximum d’entropie permet de décomposer le problème de la manière suivan∑te (Och &amp; Ney, 2002) : e∗ = argmax p(e|f) = argmax{exp λihi(e, f)} (1) e i où chaque fonction hi quantifie l’adéquation des phrases f et e1. Les coefficients λi pondèrent l’importance relative de ces fonctions. 2.1 Décodeur Moses Moses2 est un système de traduction automatique à base de séquences de mots à l’état de l’art. Il est distribué librement avec les scripts nécessaires à l’entrâınement d’un système de traduction complet, ainsi qu’une mise en œuvre efficace d’un algorithme de recherche de type recherche en faisceau pour produire les traductions. Le décodeur Moses peut également générer u</context>
</contexts>
<marker>Och, Ney, 2002</marker>
<rawString>Och F. J. &amp; Ney H. (2002). Discriminative training and maximum entropy models for statistical machine translation. In Proceedings of ACL, p. 295–302.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>C Tillmann</author>
<author>H Ney</author>
</authors>
<title>Improved alignment models for statistical machine translation.</title>
<date>1999</date>
<booktitle>In Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Copora,</booktitle>
<pages>p.</pages>
<contexts>
<context position="2033" citStr="Och et al., 1999" startWordPosition="275" endWordPosition="278">, approche statistique, modélisation linguistique dans un espace continu, analyse morpho-syntaxique, désambigüısation lexicale. Keywords: statistical machine translation, continuous space language model, POS tagging, lexical disambiguation. 1 Introduction La traduction automatique est un thème de recherche depuis plusieurs décennies et différentes approches ont été proposées, telles que la traduction par règles, la traduction à base d’exemples ou la traduction statistique. Les travaux récents en traduction statistique confirment que les modèles fondés sur des séquences de mots (Och et al., 1999; Koehn et al., 2003) obtiennent des performances significativement meilleures que ceux fondés sur des mots (Brown et al., 1993). En utilisant des séquences de mots, les systèmes de traduction parviennent à préserver certaines contraintes locales sur l’ordre des mots. L’entrâınement d’un tel modèle nécessite l’alignement d’un corpus parallèle. Les régularités du langage naturel comme celles de la syntaxe, ou, encore à un niveau supérieur, celles de la sémantique sont ainsi, en principe, implicitement capturées par les modèles. 253 H. Schwenk, D. Déchelotte, H. Bonneau-Maynard,</context>
<context position="7675" citStr="Och et al., 1999" startWordPosition="1093" endWordPosition="1096">où la probabilité Pr(f |e) est estimée par le modèle de traduction et Pr(e) par le modèle de langage de la langue cible. Cette équation résume l’approche source/canal historique (Brown et al., 1993) qui considère le mot comme unité et la phrase comme une séquence de mots. Le modèle de traduction peut être estimé automatiquement à partir de textes parallèles alignés au niveau de la phrase. Ce calcul est effectué par le logiciel libre GIZA++. Ces dernières années, les travaux en traduction statistique ont étendu avec succès l’unité qu’était le mot à la séquence de mots (Och et al., 1999; Koehn et al., 2003). Cette nouvelle unité se définit alors comme un groupe de mots successifs f̃ de la langue source. Sa traduction est également une séquence de mots ẽ dans la phrase cible. Les séquences de mots peuvent être extraites automatiquement à partir de données bilingues alignées au niveau du mot dans les deux sens. L’utilisation du principe du maximum d’entropie permet de décomposer le problème de la manière suivan∑te (Och &amp; Ney, 2002) : e∗ = argmax p(e|f) = argmax{exp λihi(e, f)} (1) e i où chaque fonction hi quantifie l’adéquation des phrases f et e1. Les coeffici</context>
</contexts>
<marker>Och, Tillmann, Ney, 1999</marker>
<rawString>Och F. J., Tillmann C. &amp; Ney H. (1999). Improved alignment models for statistical machine translation. In Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Copora, p. 20–28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W Zhu</author>
</authors>
<title>BLEU : a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>311--318</pages>
<contexts>
<context position="10138" citStr="Papineni et al., 2002" startWordPosition="1469" endWordPosition="1472"> sens large, puisqu’un système de traduction inclut toujours un modèle de langage cible hi(e, f) = p(e). 2http://www.statmt.org/moses/ 255 H. Schwenk, D. Déchelotte, H. Bonneau-Maynard, A. Allauzen deux sens, les probabilités de traduction des mots dans les deux sens, une mesure de distorsion, deux pénalités d’insertion de mots et de séquences de mots, et la probabilité calculée par le modèle de langage de la langue cible. L’approche couramment employée pour optimiser les poids λi des fonctions caractéristiques est la maximisation sur un corpus de développement de la mesure BLEU (Papineni et al., 2002). Pour cela, l’outil d’optimisation numérique Condor (Berghen &amp; Bersini, 2005) est intégré à l’algorithme itératif suivant : 1. Partant d’un jeu de poids initial, les listes des n = 1000 meilleures hypothèses sont générées avec Moses (une liste par phrase source). 2. Ces listes sont réévaluées en utilisant le jeu de poids courant. 3. Les meilleures hypothèses sont extraites et évaluées. 4. À partir du score BLEU aisni calculé, Condor calcule un nouveau jeu de poids (l’algorithme retourne alors à l’étape 2), sauf si un maximum local est détecté ce qui met fin à l’algorithm</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Papineni K., Roukos S., Ward T. &amp; Zhu W. (2002). BLEU : a method for automatic evaluation of machine translation. In Proceedings of ACL, p. 311–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Schmid</author>
</authors>
<title>Probabilistic part-of-speech tagging using decision trees.</title>
<date>1994</date>
<booktitle>In Proceedings of International Conference on New Methods in Language Processing.</booktitle>
<contexts>
<context position="14357" citStr="Schmid, 1994" startWordPosition="2071" endWordPosition="2072">es d’unités enrichies. Ainsi les phrases à traduire doivent être préalablement étiquetées. Réciproquement, si une traduction classique est requise en sortie, il est nécessaire de retirer les catégories morpho-syntaxiques de l’hypothèse proposée. Par ailleurs, il devient possible, sur la base des n meilleures hypothèses enrichies, d’effectuer une réévaluation en utilisant un modèle n-gramme de catégories morpho-syntaxiques, sans avoir à utiliser a posteriori un étiqueteur sur ces hypothèses. Pour les expériences présentées dans cet article, nous avons utilisé TreeTagger (Schmid, 1994), un étiqueteur markovien utilisant des arbres de décision pour estimer les probabilités trigramme de transition. Ce logiciel est librement disponible pour les deux langues considérées dans cet article. La version anglaise a été entrâınée sur le corpus PENN treebank 3, et la version espagnole sur le corpus CRATER4. Le nombre de catégories est assez restreint : 59 pour l’anglais et 69 pour l’espagnol. Notons que les catégories espagnoles ne contiennent pas de distinction en genre et en nombre. 2.3 Modèle de langage neuronal L’architecture du modèle de langage neuronal est résumée</context>
</contexts>
<marker>Schmid, 1994</marker>
<rawString>Schmid H. (1994). Probabilistic part-of-speech tagging using decision trees. In Proceedings of International Conference on New Methods in Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Schwenk</author>
</authors>
<title>Continuous space language models.</title>
<date>2007</date>
<journal>Computer Speech and Language,</journal>
<volume>21</volume>
<pages>492--518</pages>
<contexts>
<context position="17271" citStr="Schwenk, 2007" startWordPosition="2532" endWordPosition="2533">m. P Dans ce modèle, la complexité est dominée par la taille importante de la couche de sortie. Ainsi, nous proposons de limiter l’estimation des probabilités aux 8 192 mots les plus fré- quents, les autres mots étant traités par le modèle à repli standard. Dans nos expériences, environ 90% des requêtes de probabilités sont traitées par le réseau de neurones. Il est important de noter que tous les mots du vocabulaire sont considérés à l’entrée du réseau. Ce modèle de langage a été utilisé avec succès dans un système de reconnaissance de la parole à grand vocabulaire (Schwenk, 2007), et dans un système de traduction statistique pour la tâcheBtec avec un nombre très limité de données d’apprentissage (Schwenk et al., 2006). Cet article décrit la première application du modèle de langage neuronal dans un système de traduction statistique avec plusieurs milliers d’exemples d’apprentissage. 3 Résultats expérimentaux Les expériences décrites dans cet article ont été effectuées dans le cadre des évaluations internationales organisées par le projet européen Tc-Star5. L’objectif de ce projet est de motiver, fédérer, et promouvoir les recherches sur la traduct</context>
</contexts>
<marker>Schwenk, 2007</marker>
<rawString>Schwenk H. (2007). Continuous space language models. Computer Speech and Language, 21, 492–518.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Schwenk</author>
<author>M R Costa-jussà</author>
<author>J A R Fonollosa</author>
</authors>
<title>Continuous space language models for the IWSLT</title>
<date>2006</date>
<booktitle>In International Workshop on Spoken Language Translation,</booktitle>
<pages>166--173</pages>
<marker>Schwenk, Costa-jussà, Fonollosa, 2006</marker>
<rawString>Schwenk H., Costa-jussà M. R. &amp; Fonollosa J. A. R. (2006). Continuous space language models for the IWSLT 2006 task. In International Workshop on Spoken Language Translation, p. 166–173.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
</authors>
<title>SRILM - an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In International Conference on Speech and Language Processing, p. II :</booktitle>
<pages>901--904</pages>
<contexts>
<context position="20834" citStr="Stolcke, 2002" startWordPosition="3036" endWordPosition="3037">ont in fine interpolés linéairement pour créer un modèle de la langue cible. Les coefficients d’interpolation sont estimés via l’algorithme E.M. de manière à minimiser la perplexité sur les données de développement. Les coefficients obtenus sont 0,81 pour le modèle SPPE, 0,12 pour le modèle estimé sur les données additionnelles du parlement et 0,07 pour celui utilisant les transcriptions acoustiques. Tous les modèles de langage n-grammes utilisés, hormis le modèle neuronal, sont des modèles classiques avec repli utilisant le lissage de Kneser-Ney modifié. Le SRI LM-toolkit (Stolcke, 2002) a été utilisé pour leur construction. Les caractéristiques des données et les perplexités des modèles de langage sont résumées dans le Tableau 1. Lesmodèles trigrammes interviennent pendant le décodage, alors que les modèles quadrigrammes sont utilisés pour réévaluer les listes de n meilleures hypothèses. Lemodèle de langage neuronal obtient une réduction de la perplexité de 15% environ. Il est à noter que les données de développement en anglais, donc la traduction de l’espagnol vers l’anglais, proviennent de deux sources différentes (parlements européen et espagnol). </context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Stolcke A. (2002). SRILM - an extensible language modeling toolkit. In International Conference on Speech and Language Processing, p. II : 901–904.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>