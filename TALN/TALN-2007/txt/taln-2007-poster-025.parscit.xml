<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Y Bengio</author>
<author>R Ducharme</author>
<author>P Vincent</author>
<author>C Jauvin</author>
</authors>
<title>A neural probabilistic language model.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>3</volume>
<issue>2</issue>
<pages>1137--1155</pages>
<contexts>
<context position="3839" citStr="Bengio et al., 2003" startWordPosition="539" endWordPosition="542">langage trigramme à repli. Dans ces modèles classiques, les mots sont représentés par un indice dans un espace discret, le vocabulaire. Ceci ne permet pas de faire de véritables interpolations des probabilités d’un n-gramme non observé puisqu’un changement dans l’espace des mots peut entrâıner un changement arbitraire de la probabilité. Nous proposons ici d’appréhender dans un domaine continu le problème de l’estimation d’un modèle linguistique. L’idée consiste à projeter les indices des mots dans une représentation continue (un espace vectoriel) et d’estimer les probabilités dans cet espace (Bengio et al., 2003). Actuellement, un réseau de neuronesmulti-couches complètement connecté est utilisé pour apprendre conjointement la projection des mots sur un espace continu et l’estimation des probabilités n-grammes. La lecture humaine des sorties d’un système statistique de traduction, même basé sur des séquences de mots, nécessite parfois un difficile exercice de réordonnancement et de restructuration syntaxique pour restituer le sens de l’énoncé d’origine. La modélisation du langage comme une source markovienne (modèle de langage n-gramme), avec comme unité le mot ou la séquence de mots, ne permet pas de</context>
</contexts>
<marker>Bengio, Ducharme, Vincent, Jauvin, 2003</marker>
<rawString>Bengio Y., Ducharme R., Vincent P. &amp; Jauvin C. (2003). A neural probabilistic language model. Journal of Machine Learning Research, 3(2), 1137–1155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F V Berghen</author>
<author>H Bersini</author>
</authors>
<title>CONDOR, a new parallel, constrained extension of powell’s UOBYQA algorithm : Experimental results and comparison with the DFO algorithm.</title>
<date>2005</date>
<journal>Journal of Computational and Applied Mathematics,</journal>
<volume>181</volume>
<pages>157--175</pages>
<contexts>
<context position="9941" citStr="Berghen &amp; Bersini, 2005" startWordPosition="1479" endWordPosition="1482"> modèle de langage cible hi(e, f) = p(e). 2http://www.statmt.org/moses/ 255 H. Schwenk, D. Déchelotte, H. Bonneau-Maynard, A. Allauzen deux sens, les probabilités de traduction des mots dans les deux sens, une mesure de distorsion, deux pénalités d’insertion de mots et de séquences de mots, et la probabilité calculée par le modèle de langage de la langue cible. L’approche couramment employée pour optimiser les poids λi des fonctions caractéristiques est la maximisation sur un corpus de développement de la mesure BLEU (Papineni et al., 2002). Pour cela, l’outil d’optimisation numérique Condor (Berghen &amp; Bersini, 2005) est intégré à l’algorithme itératif suivant : 1. Partant d’un jeu de poids initial, les listes des n = 1000 meilleures hypothèses sont générées avec Moses (une liste par phrase source). 2. Ces listes sont réévaluées en utilisant le jeu de poids courant. 3. Les meilleures hypothèses sont extraites et évaluées. 4. À partir du score BLEU aisni calculé, Condor calcule un nouveau jeu de poids (l’algorithme retourne alors à l’étape 2), sauf si un maximum local est détecté ce qui met fin à l’algorithme. Le jeu de poids solution est en général trouvé après une centaine d’itérations. Remarquons que le</context>
</contexts>
<marker>Berghen, Bersini, 2005</marker>
<rawString>Berghen F. V. &amp; Bersini H. (2005). CONDOR, a new parallel, constrained extension of powell’s UOBYQA algorithm : Experimental results and comparison with the DFO algorithm. Journal of Computational and Applied Mathematics, 181, 157–175.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Schwenk</author>
<author>D Déchelotte</author>
<author>H Bonneau-Maynard</author>
<author>A Allauzen Brown P</author>
<author>Della Pietra S</author>
<author>Della Pietra V J</author>
<author>R Mercer</author>
</authors>
<title>The mathematics of statistical machine translation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<pages>263--311</pages>
<marker>Schwenk, Déchelotte, Bonneau-Maynard, P, S, J, Mercer, 1993</marker>
<rawString>H. Schwenk, D. Déchelotte, H. Bonneau-Maynard, A. Allauzen Brown P., Della Pietra S., Della Pietra V. J. &amp; Mercer R. (1993). The mathematics of statistical machine translation. Computational Linguistics, 19(2), 263– 311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
<author>K Knight</author>
<author>K Yamada</author>
</authors>
<title>Syntax-based language models for machine translation.</title>
<date>2003</date>
<booktitle>In MT Summit.</booktitle>
<contexts>
<context position="5324" citStr="Charniak et al., 2003" startWordPosition="750" endWordPosition="753">’utilisation d’informations morphosyntaxiques dans la traduction statistique ont déjà été menées. (Och et al., 2004) ont exploré de nombreuses fonctions caractéristiques, dont certaines d’ordre syntaxique. La réévaluation des n meilleures hypothèses avec des étiquettes morpho-syntaxiques a également été étudiée par (Hasan et al., 2006). Dans (Kirchhoff &amp; Yang, 2005), un modèle de langage factorisé quadrigramme utilisant des informations syntaxiques n’a pas montré des performances meilleures qu’un modèle n-gramme de mots. Les modèles de langage fondés sur la syntaxe ont enfin été explorés par (Charniak et al., 2003). Tous ces travaux ont en commun d’utiliser des séquences de mots comme unités du système de traduction et de n’introduire les catégories morpho-syntaxiques que dans une seconde passe de traitement. Dans ce travail, nous proposons d’intégrer les informations syntaxiques dans le modèle de traduction lui-même. De plus, nous proposons de combiner cette approche avec les méthodes classiques de réévaluation de listes de n meilleures hypothèses. À notre connaissance, cette approche n’a pas été évaluée sur une large tâche (elle a été appliquée par (Hwang et al., 2007) à la tâche BTEC (Basic Travel Ex</context>
</contexts>
<marker>Charniak, Knight, Yamada, 2003</marker>
<rawString>Charniak E., Knight K. &amp; Yamada K. (2003). Syntax-based language models for machine translation. In MT Summit.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Hasan</author>
<author>O Bender</author>
<author>H Ney</author>
</authors>
<title>Reranking translation hypothesis using structural properties.</title>
<date>2006</date>
<booktitle>In EACL Workshop on Learning Structured Information in Natural Language Applications.</booktitle>
<contexts>
<context position="5039" citStr="Hasan et al., 2006" startWordPosition="707" endWordPosition="710">s, ne permet pas de prendre en compte les contraintes syntaxiques ou les dépendances à long terme entre les mots. Il apparâıt donc nécessaire d’utiliser des méthodes dans lesquelles les propriétés structurelles des langues sont explicitement représentées. Plusieurs tentatives sur l’utilisation d’informations morphosyntaxiques dans la traduction statistique ont déjà été menées. (Och et al., 2004) ont exploré de nombreuses fonctions caractéristiques, dont certaines d’ordre syntaxique. La réévaluation des n meilleures hypothèses avec des étiquettes morpho-syntaxiques a également été étudiée par (Hasan et al., 2006). Dans (Kirchhoff &amp; Yang, 2005), un modèle de langage factorisé quadrigramme utilisant des informations syntaxiques n’a pas montré des performances meilleures qu’un modèle n-gramme de mots. Les modèles de langage fondés sur la syntaxe ont enfin été explorés par (Charniak et al., 2003). Tous ces travaux ont en commun d’utiliser des séquences de mots comme unités du système de traduction et de n’introduire les catégories morpho-syntaxiques que dans une seconde passe de traitement. Dans ce travail, nous proposons d’intégrer les informations syntaxiques dans le modèle de traduction lui-même. De pl</context>
</contexts>
<marker>Hasan, Bender, Ney, 2006</marker>
<rawString>Hasan S., Bender O. &amp; Ney H. (2006). Reranking translation hypothesis using structural properties. In EACL Workshop on Learning Structured Information in Natural Language Applications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Hwang</author>
<author>A Finch</author>
<author>Y Sasaki</author>
</authors>
<title>Improving statistical machine translation using shallow linguistic knowledge.</title>
<date>2007</date>
<journal>Computer Speech &amp; Language,</journal>
<volume>21</volume>
<issue>2</issue>
<pages>350--372</pages>
<contexts>
<context position="5891" citStr="Hwang et al., 2007" startWordPosition="838" endWordPosition="841">e ont enfin été explorés par (Charniak et al., 2003). Tous ces travaux ont en commun d’utiliser des séquences de mots comme unités du système de traduction et de n’introduire les catégories morpho-syntaxiques que dans une seconde passe de traitement. Dans ce travail, nous proposons d’intégrer les informations syntaxiques dans le modèle de traduction lui-même. De plus, nous proposons de combiner cette approche avec les méthodes classiques de réévaluation de listes de n meilleures hypothèses. À notre connaissance, cette approche n’a pas été évaluée sur une large tâche (elle a été appliquée par (Hwang et al., 2007) à la tâche BTEC (Basic Travel Expression Corpus) beaucoup plus réduite). Nous présentons ici des résultats sur la tâche Tc-Star (traduction des transcriptions des sessions plénières du Parlement européen). Cet article est organisé comme suit. Dans la section suivante, nous présentons d’abord la structure du système de traduction automatique et ses différentes extensions. Les résultats expérimentaux sont résumés et discutés dans la section 3. La dernière section conclue cet article et suggère des extensions et travaux futurs. 254 Modèles statistiques enrichis par la syntaxe pour la traduction </context>
</contexts>
<marker>Hwang, Finch, Sasaki, 2007</marker>
<rawString>Hwang Y., Finch A. &amp; Sasaki Y. (2007). Improving statistical machine translation using shallow linguistic knowledge. Computer Speech &amp; Language, 21(2), 350–372.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Kirchhoff</author>
<author>M Yang</author>
</authors>
<title>Improved languagemodeling for statisticalmachine translation.</title>
<date>2005</date>
<booktitle>In ACL’05 workshop on Building and Using Parallel Text,</booktitle>
<pages>125--128</pages>
<contexts>
<context position="5070" citStr="Kirchhoff &amp; Yang, 2005" startWordPosition="712" endWordPosition="715"> en compte les contraintes syntaxiques ou les dépendances à long terme entre les mots. Il apparâıt donc nécessaire d’utiliser des méthodes dans lesquelles les propriétés structurelles des langues sont explicitement représentées. Plusieurs tentatives sur l’utilisation d’informations morphosyntaxiques dans la traduction statistique ont déjà été menées. (Och et al., 2004) ont exploré de nombreuses fonctions caractéristiques, dont certaines d’ordre syntaxique. La réévaluation des n meilleures hypothèses avec des étiquettes morpho-syntaxiques a également été étudiée par (Hasan et al., 2006). Dans (Kirchhoff &amp; Yang, 2005), un modèle de langage factorisé quadrigramme utilisant des informations syntaxiques n’a pas montré des performances meilleures qu’un modèle n-gramme de mots. Les modèles de langage fondés sur la syntaxe ont enfin été explorés par (Charniak et al., 2003). Tous ces travaux ont en commun d’utiliser des séquences de mots comme unités du système de traduction et de n’introduire les catégories morpho-syntaxiques que dans une seconde passe de traitement. Dans ce travail, nous proposons d’intégrer les informations syntaxiques dans le modèle de traduction lui-même. De plus, nous proposons de combiner </context>
</contexts>
<marker>Kirchhoff, Yang, 2005</marker>
<rawString>Kirchhoff K. &amp; Yang M. (2005). Improved languagemodeling for statisticalmachine translation. In ACL’05 workshop on Building and Using Parallel Text, p. 125–128.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
</authors>
<title>Europarl : A parallel corpus for statistical machine translation.</title>
<date>2006</date>
<booktitle>In MT Summit.</booktitle>
<contexts>
<context position="25576" citStr="Koehn, 2006" startWordPosition="3826" endWordPosition="3827">enté et évalué deux évolutions d’un système de traduction statistique. L’une propose une modélisation linguistique dans un espace continu et la seconde intègre les catégories morpho-syntaxiques des mots dans le modèle de traduction. La combinaison des deux méthodes donne des résultats intéressants. Notre système a obtenu de très bons résultats à l’évaluation Tc-Star organisée début 2007. Nous étudions aussi l’application des mêmes techniques à la traduction automatique d’autres paires de langues, notamment la traduction entre l’anglais et le français. Pour cela le corpus Europarl est utilisé (Koehn, 2006). Nous sommes en train de produire une deuxième référence de traduction qui sera librement disponible pour d’autres laboratoires de recherche intéressés dans la traduction automatique du français6. Plusieurs extensions du système décrit dans cet article sont actuellement à l’étude. Nous travaillons sur une meilleure incorporation des connaissances linguistiques, notamment sur l’utilisation d’étiqueteurs prenant en compte le genre et le nombre, voire le sens des mots, afin d’améliorer la désambigüısation dans le modèle de traduction. Un logiciel de visualisation des erreurs de traduction est en</context>
</contexts>
<marker>Koehn, 2006</marker>
<rawString>Koehn P. (2006). Europarl : A parallel corpus for statistical machine translation. In MT Summit.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>F J Och</author>
<author>D Marcu</author>
</authors>
<title>Statistical phrased-based machine translation.</title>
<date>2003</date>
<booktitle>In Joint Conference on Human Language Technology and of the North American Chapter of the Asociation for Computational Lingustics,</booktitle>
<pages>127--133</pages>
<contexts>
<context position="2015" citStr="Koehn et al., 2003" startWordPosition="279" endWordPosition="282">pproche statistique, modélisation linguistique dans un espace continu, analyse morpho-syntaxique, désambigüısation lexicale. Keywords: statistical machine translation, continuous space language model, POS tagging, lexical disambiguation. 1 Introduction La traduction automatique est un thème de recherche depuis plusieurs décennies et différentes approches ont été proposées, telles que la traduction par règles, la traduction à base d’exemples ou la traduction statistique. Les travaux récents en traduction statistique confirment que les modèles fondés sur des séquences de mots (Och et al., 1999; Koehn et al., 2003) obtiennent des performances significativement meilleures que ceux fondés sur des mots (Brown et al., 1993). En utilisant des séquences de mots, les systèmes de traduction parviennent à préserver certaines contraintes locales sur l’ordre des mots. L’entrâınement d’un tel modèle nécessite l’alignement d’un corpus parallèle. Les régularités du langage naturel comme celles de la syntaxe, ou, encore à un niveau supérieur, celles de la sémantique sont ainsi, en principe, implicitement capturées par les modèles. 253 H. Schwenk, D. Déchelotte, H. Bonneau-Maynard, A. Allauzen Depuis les débuts de l’ap</context>
<context position="7488" citStr="Koehn et al., 2003" startWordPosition="1097" endWordPosition="1100">), e e où la probabilité Pr(f |e) est estimée par le modèle de traduction et Pr(e) par le modèle de langage de la langue cible. Cette équation résume l’approche source/canal historique (Brown et al., 1993) qui considère le mot comme unité et la phrase comme une séquence de mots. Le modèle de traduction peut être estimé automatiquement à partir de textes parallèles alignés au niveau de la phrase. Ce calcul est effectué par le logiciel libre GIZA++. Ces dernières années, les travaux en traduction statistique ont étendu avec succès l’unité qu’était le mot à la séquence de mots (Och et al., 1999; Koehn et al., 2003). Cette nouvelle unité se définit alors comme un groupe de mots successifs f̃ de la langue source. Sa traduction est également une séquence de mots ẽ dans la phrase cible. Les séquences de mots peuvent être extraites automatiquement à partir de données bilingues alignées au niveau du mot dans les deux sens. L’utilisation du principe du maximum d’entropie permet de décomposer le problème de la manière suivan∑te (Och &amp; Ney, 2002) : e∗ = argmax p(e|f) = argmax{exp λihi(e, f)} (1) e i où chaque fonction hi quantifie l’adéquation des phrases f et e1. Les coefficients λi pondèrent l’importance relat</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Koehn P., Och F. J. &amp; Marcu D. (2003). Statistical phrased-based machine translation. In Joint Conference on Human Language Technology and of the North American Chapter of the Asociation for Computational Lingustics, p. 127–133.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F-J Och</author>
<author>D Gildea</author>
<author>S Khudanpur</author>
<author>A Sarkar</author>
<author>K Yamada</author>
<author>A Fraser</author>
<author>S Kumar</author>
<author>L Shen</author>
<author>D Smith</author>
<author>K Eng</author>
<author>V Jain</author>
<author>Z Jin</author>
<author>D Radev</author>
</authors>
<title>A smorgasbord of features for statistical machine translation.</title>
<date>2004</date>
<booktitle>In Proceedings of the North American Chapter of the Asociation for Computational Lingustics,</booktitle>
<pages>161--168</pages>
<contexts>
<context position="4818" citStr="Och et al., 2004" startWordPosition="677" endWordPosition="680">ancement et de restructuration syntaxique pour restituer le sens de l’énoncé d’origine. La modélisation du langage comme une source markovienne (modèle de langage n-gramme), avec comme unité le mot ou la séquence de mots, ne permet pas de prendre en compte les contraintes syntaxiques ou les dépendances à long terme entre les mots. Il apparâıt donc nécessaire d’utiliser des méthodes dans lesquelles les propriétés structurelles des langues sont explicitement représentées. Plusieurs tentatives sur l’utilisation d’informations morphosyntaxiques dans la traduction statistique ont déjà été menées. (Och et al., 2004) ont exploré de nombreuses fonctions caractéristiques, dont certaines d’ordre syntaxique. La réévaluation des n meilleures hypothèses avec des étiquettes morpho-syntaxiques a également été étudiée par (Hasan et al., 2006). Dans (Kirchhoff &amp; Yang, 2005), un modèle de langage factorisé quadrigramme utilisant des informations syntaxiques n’a pas montré des performances meilleures qu’un modèle n-gramme de mots. Les modèles de langage fondés sur la syntaxe ont enfin été explorés par (Charniak et al., 2003). Tous ces travaux ont en commun d’utiliser des séquences de mots comme unités du système de t</context>
</contexts>
<marker>Och, Gildea, Khudanpur, Sarkar, Yamada, Fraser, Kumar, Shen, Smith, Eng, Jain, Jin, Radev, 2004</marker>
<rawString>Och F.-J., Gildea D., Khudanpur S., Sarkar A., Yamada K., Fraser A., Kumar S., Shen L., Smith D., Eng K., Jain V., Jin Z. &amp; Radev D. (2004). A smorgasbord of features for statistical machine translation. In Proceedings of the North American Chapter of the Asociation for Computational Lingustics, p. 161–168.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>Discriminative training and maximum entropy models for statistical machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>295--302</pages>
<contexts>
<context position="7919" citStr="Och &amp; Ney, 2002" startWordPosition="1168" endWordPosition="1171">iel libre GIZA++. Ces dernières années, les travaux en traduction statistique ont étendu avec succès l’unité qu’était le mot à la séquence de mots (Och et al., 1999; Koehn et al., 2003). Cette nouvelle unité se définit alors comme un groupe de mots successifs f̃ de la langue source. Sa traduction est également une séquence de mots ẽ dans la phrase cible. Les séquences de mots peuvent être extraites automatiquement à partir de données bilingues alignées au niveau du mot dans les deux sens. L’utilisation du principe du maximum d’entropie permet de décomposer le problème de la manière suivan∑te (Och &amp; Ney, 2002) : e∗ = argmax p(e|f) = argmax{exp λihi(e, f)} (1) e i où chaque fonction hi quantifie l’adéquation des phrases f et e1. Les coefficients λi pondèrent l’importance relative de ces fonctions. 2.1 Décodeur Moses Moses2 est un système de traduction automatique à base de séquences de mots à l’état de l’art. Il est distribué librement avec les scripts nécessaires à l’entrâınement d’un système de traduction complet, ainsi qu’une mise en œuvre efficace d’un algorithme de recherche de type recherche en faisceau pour produire les traductions. Le décodeur Moses peut également générer une liste des n hyp</context>
</contexts>
<marker>Och, Ney, 2002</marker>
<rawString>Och F. J. &amp; Ney H. (2002). Discriminative training and maximum entropy models for statistical machine translation. In Proceedings of ACL, p. 295–302.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>C Tillmann</author>
<author>H Ney</author>
</authors>
<title>Improved alignment models for statistical machine translation.</title>
<date>1999</date>
<booktitle>In Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Copora,</booktitle>
<pages>p.</pages>
<contexts>
<context position="1994" citStr="Och et al., 1999" startWordPosition="275" endWordPosition="278">ion automatique, approche statistique, modélisation linguistique dans un espace continu, analyse morpho-syntaxique, désambigüısation lexicale. Keywords: statistical machine translation, continuous space language model, POS tagging, lexical disambiguation. 1 Introduction La traduction automatique est un thème de recherche depuis plusieurs décennies et différentes approches ont été proposées, telles que la traduction par règles, la traduction à base d’exemples ou la traduction statistique. Les travaux récents en traduction statistique confirment que les modèles fondés sur des séquences de mots (Och et al., 1999; Koehn et al., 2003) obtiennent des performances significativement meilleures que ceux fondés sur des mots (Brown et al., 1993). En utilisant des séquences de mots, les systèmes de traduction parviennent à préserver certaines contraintes locales sur l’ordre des mots. L’entrâınement d’un tel modèle nécessite l’alignement d’un corpus parallèle. Les régularités du langage naturel comme celles de la syntaxe, ou, encore à un niveau supérieur, celles de la sémantique sont ainsi, en principe, implicitement capturées par les modèles. 253 H. Schwenk, D. Déchelotte, H. Bonneau-Maynard, A. Allauzen Depu</context>
<context position="7467" citStr="Och et al., 1999" startWordPosition="1093" endWordPosition="1096">rgmaxPr(f |e) Pr(e), e e où la probabilité Pr(f |e) est estimée par le modèle de traduction et Pr(e) par le modèle de langage de la langue cible. Cette équation résume l’approche source/canal historique (Brown et al., 1993) qui considère le mot comme unité et la phrase comme une séquence de mots. Le modèle de traduction peut être estimé automatiquement à partir de textes parallèles alignés au niveau de la phrase. Ce calcul est effectué par le logiciel libre GIZA++. Ces dernières années, les travaux en traduction statistique ont étendu avec succès l’unité qu’était le mot à la séquence de mots (Och et al., 1999; Koehn et al., 2003). Cette nouvelle unité se définit alors comme un groupe de mots successifs f̃ de la langue source. Sa traduction est également une séquence de mots ẽ dans la phrase cible. Les séquences de mots peuvent être extraites automatiquement à partir de données bilingues alignées au niveau du mot dans les deux sens. L’utilisation du principe du maximum d’entropie permet de décomposer le problème de la manière suivan∑te (Och &amp; Ney, 2002) : e∗ = argmax p(e|f) = argmax{exp λihi(e, f)} (1) e i où chaque fonction hi quantifie l’adéquation des phrases f et e1. Les coefficients λi pondère</context>
</contexts>
<marker>Och, Tillmann, Ney, 1999</marker>
<rawString>Och F. J., Tillmann C. &amp; Ney H. (1999). Improved alignment models for statistical machine translation. In Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Copora, p. 20–28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W Zhu</author>
</authors>
<title>BLEU : a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>311--318</pages>
<contexts>
<context position="9863" citStr="Papineni et al., 2002" startWordPosition="1469" endWordPosition="1472"> à prendre au sens large, puisqu’un système de traduction inclut toujours un modèle de langage cible hi(e, f) = p(e). 2http://www.statmt.org/moses/ 255 H. Schwenk, D. Déchelotte, H. Bonneau-Maynard, A. Allauzen deux sens, les probabilités de traduction des mots dans les deux sens, une mesure de distorsion, deux pénalités d’insertion de mots et de séquences de mots, et la probabilité calculée par le modèle de langage de la langue cible. L’approche couramment employée pour optimiser les poids λi des fonctions caractéristiques est la maximisation sur un corpus de développement de la mesure BLEU (Papineni et al., 2002). Pour cela, l’outil d’optimisation numérique Condor (Berghen &amp; Bersini, 2005) est intégré à l’algorithme itératif suivant : 1. Partant d’un jeu de poids initial, les listes des n = 1000 meilleures hypothèses sont générées avec Moses (une liste par phrase source). 2. Ces listes sont réévaluées en utilisant le jeu de poids courant. 3. Les meilleures hypothèses sont extraites et évaluées. 4. À partir du score BLEU aisni calculé, Condor calcule un nouveau jeu de poids (l’algorithme retourne alors à l’étape 2), sauf si un maximum local est détecté ce qui met fin à l’algorithme. Le jeu de poids sol</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Papineni K., Roukos S., Ward T. &amp; Zhu W. (2002). BLEU : a method for automatic evaluation of machine translation. In Proceedings of ACL, p. 311–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Schmid</author>
</authors>
<title>Probabilistic part-of-speech tagging using decision trees.</title>
<date>1994</date>
<booktitle>In Proceedings of International Conference on New Methods in Language Processing.</booktitle>
<contexts>
<context position="13937" citStr="Schmid, 1994" startWordPosition="2071" endWordPosition="2072">t en sortie des séquences d’unités enrichies. Ainsi les phrases à traduire doivent être préalablement étiquetées. Réciproquement, si une traduction classique est requise en sortie, il est nécessaire de retirer les catégories morpho-syntaxiques de l’hypothèse proposée. Par ailleurs, il devient possible, sur la base des n meilleures hypothèses enrichies, d’effectuer une réévaluation en utilisant un modèle n-gramme de catégories morpho-syntaxiques, sans avoir à utiliser a posteriori un étiqueteur sur ces hypothèses. Pour les expériences présentées dans cet article, nous avons utilisé TreeTagger (Schmid, 1994), un étiqueteur markovien utilisant des arbres de décision pour estimer les probabilités trigramme de transition. Ce logiciel est librement disponible pour les deux langues considérées dans cet article. La version anglaise a été entrâınée sur le corpus PENN treebank 3, et la version espagnole sur le corpus CRATER4. Le nombre de catégories est assez restreint : 59 pour l’anglais et 69 pour l’espagnol. Notons que les catégories espagnoles ne contiennent pas de distinction en genre et en nombre. 2.3 Modèle de langage neuronal L’architecture du modèle de langage neuronal est résumée à la Figure 2.</context>
</contexts>
<marker>Schmid, 1994</marker>
<rawString>Schmid H. (1994). Probabilistic part-of-speech tagging using decision trees. In Proceedings of International Conference on New Methods in Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Schwenk</author>
</authors>
<title>Continuous space language models.</title>
<date>2007</date>
<journal>Computer Speech and Language,</journal>
<volume>21</volume>
<pages>492--518</pages>
<contexts>
<context position="16781" citStr="Schwenk, 2007" startWordPosition="2532" endWordPosition="2533">s le vocab. vecteurs de dim. P Dans ce modèle, la complexité est dominée par la taille importante de la couche de sortie. Ainsi, nous proposons de limiter l’estimation des probabilités aux 8 192 mots les plus fré- quents, les autres mots étant traités par le modèle à repli standard. Dans nos expériences, environ 90% des requêtes de probabilités sont traitées par le réseau de neurones. Il est important de noter que tous les mots du vocabulaire sont considérés à l’entrée du réseau. Ce modèle de langage a été utilisé avec succès dans un système de reconnaissance de la parole à grand vocabulaire (Schwenk, 2007), et dans un système de traduction statistique pour la tâcheBtec avec un nombre très limité de données d’apprentissage (Schwenk et al., 2006). Cet article décrit la première application du modèle de langage neuronal dans un système de traduction statistique avec plusieurs milliers d’exemples d’apprentissage. 3 Résultats expérimentaux Les expériences décrites dans cet article ont été effectuées dans le cadre des évaluations internationales organisées par le projet européen Tc-Star5. L’objectif de ce projet est de motiver, fédérer, et promouvoir les recherches sur la traduction automatique de la</context>
</contexts>
<marker>Schwenk, 2007</marker>
<rawString>Schwenk H. (2007). Continuous space language models. Computer Speech and Language, 21, 492–518.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Schwenk</author>
<author>M R Costa-jussà</author>
<author>J A R Fonollosa</author>
</authors>
<title>Continuous space language models for the IWSLT</title>
<date>2006</date>
<booktitle>In International Workshop on Spoken Language Translation,</booktitle>
<pages>166--173</pages>
<contexts>
<context position="16922" citStr="Schwenk et al., 2006" startWordPosition="2552" endWordPosition="2555">proposons de limiter l’estimation des probabilités aux 8 192 mots les plus fré- quents, les autres mots étant traités par le modèle à repli standard. Dans nos expériences, environ 90% des requêtes de probabilités sont traitées par le réseau de neurones. Il est important de noter que tous les mots du vocabulaire sont considérés à l’entrée du réseau. Ce modèle de langage a été utilisé avec succès dans un système de reconnaissance de la parole à grand vocabulaire (Schwenk, 2007), et dans un système de traduction statistique pour la tâcheBtec avec un nombre très limité de données d’apprentissage (Schwenk et al., 2006). Cet article décrit la première application du modèle de langage neuronal dans un système de traduction statistique avec plusieurs milliers d’exemples d’apprentissage. 3 Résultats expérimentaux Les expériences décrites dans cet article ont été effectuées dans le cadre des évaluations internationales organisées par le projet européen Tc-Star5. L’objectif de ce projet est de motiver, fédérer, et promouvoir les recherches sur la traduction automatique de la parole. La tâche principale de ce projet est la traduction des transcriptions des sessions plénières du Parlement européen (SPPE). La commun</context>
</contexts>
<marker>Schwenk, Costa-jussà, Fonollosa, 2006</marker>
<rawString>Schwenk H., Costa-jussà M. R. &amp; Fonollosa J. A. R. (2006). Continuous space language models for the IWSLT 2006 task. In International Workshop on Spoken Language Translation, p. 166–173.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
</authors>
<title>SRILM - an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In International Conference on Speech and Language Processing, p. II :</booktitle>
<pages>901--904</pages>
<contexts>
<context position="20237" citStr="Stolcke, 2002" startWordPosition="3036" endWordPosition="3037">Ces trois modèles sont in fine interpolés linéairement pour créer un modèle de la langue cible. Les coefficients d’interpolation sont estimés via l’algorithme E.M. de manière à minimiser la perplexité sur les données de développement. Les coefficients obtenus sont 0,81 pour le modèle SPPE, 0,12 pour le modèle estimé sur les données additionnelles du parlement et 0,07 pour celui utilisant les transcriptions acoustiques. Tous les modèles de langage n-grammes utilisés, hormis le modèle neuronal, sont des modèles classiques avec repli utilisant le lissage de Kneser-Ney modifié. Le SRI LM-toolkit (Stolcke, 2002) a été utilisé pour leur construction. Les caractéristiques des données et les perplexités des modèles de langage sont résumées dans le Tableau 1. Lesmodèles trigrammes interviennent pendant le décodage, alors que les modèles quadrigrammes sont utilisés pour réévaluer les listes de n meilleures hypothèses. Lemodèle de langage neuronal obtient une réduction de la perplexité de 15% environ. Il est à noter que les données de développement en anglais, donc la traduction de l’espagnol vers l’anglais, proviennent de deux sources différentes (parlements européen et espagnol). Cette différence expliqu</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Stolcke A. (2002). SRILM - an extensible language modeling toolkit. In International Conference on Speech and Language Processing, p. II : 901–904.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>