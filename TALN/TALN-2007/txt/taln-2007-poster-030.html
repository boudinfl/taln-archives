<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
"http://www.w3.org/TR/html4/loose.dtd">
<html><head><title>&#201;valuation des performances d&#8217;un mod&#232;le de langage stochastique pour la compr&#233;hension de la parole arabe spontan&#233;e</title>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
</head>
<body>
<div style="page-break-before:always; page-break-after:always"><div><p>TALN 2007, Toulouse, 5&#8211;8 juin 2007 
</p>
<p>&#201;valuation des performances d&#8217;un mod&#232;le de langage 
stochastique pour la compr&#233;hension de la parole arabe spontan&#233;e 
</p>
<p>Anis ZOUAGHI1, Mounir ZRIGUI1, Mohamed BEN AHMED2
</p>
<p>1 Labo RIADI (Unit&#233; de Monastir) 
Universit&#233; de Monastir, Facult&#233; des sciences de Monastir 
</p>
<p> 2 Labo RIADI &#8211; Universit&#233; de la Mannouba,
&#201;cole nationale des sciences de l&#8217;informatique 
</p>
<p>Anis.Zouaghi@riadi.rnu.tn, Mounir.Zrigui@fsm.rnu.tn,
Mohamed.Benahmed@riadi.rnu.tn
</p>
<p>R&#233;sum&#233;.  Les mod&#232;les de Markov cach&#233;s (HMM : Hidden Markov Models) (Baum et al., 
1970), sont tr&#232;s utilis&#233;s en reconnaissance de la parole et depuis quelques ann&#233;es en 
compr&#233;hension de la parole spontan&#233;e latine telle que le fran&#231;ais ou l&#8217;anglais. Dans cet article, 
nous proposons d&#8217;utiliser et d&#8217;&#233;valuer la performance de ce type de mod&#232;le pour 
l&#8217;interpr&#233;tation s&#233;mantique de la parole arabe spontan&#233;e. Les r&#233;sultats obtenus sont 
satisfaisants, nous avons atteint un taux d&#8217;erreur de l&#8217;ordre de 9,9% en employant un HMM &#224; 
un seul niveau, avec des probabilit&#233;s tri_grammes de transitions.  
</p>
<p>Abstract. The HMM (Hidden Markov Models) (Baum et al., 1970), are frequently used in 
speech recognition and in the comprehension of foreign spontaneous speech such us the 
french or the english. In this article, we propose using and evaluating the performance of this 
model type for the semantic interpretation of the spontaneous arabic speech. The obtained 
results are satisfying; we have achieved an error score equal to 9.9%, by using HMM with tri-
grams probabilities transitions.     
</p>
<p>Mots-clefs : analyse s&#233;mantique, mod&#232;le de langage stochastique, contexte pertinent, 
information mutuelle moyenne, parole arabe spontan&#233;e. 
</p>
<p>Keywords: semantic analysis, stochastic language model, pertinent context, overage 
mutual information, spontaneous arabic speech.  
</p>
<p>1 Introduction
On distingue deux grands courants d'approches pour la compr&#233;hension de la parole : les 
approches symboliques linguistiques (ou par r&#232;gles), et les approches stochastiques. Le 
premier type d&#8217;approches se base sur une repr&#233;sentation pr&#233;alable de la grammaire. Pour 
d&#233;crire cette grammaire, on utilise g&#233;n&#233;ralement l&#8217;un des formalismes existants tels que : 
HPSG, les grammaires lexicales fonctionnelles (LFG), etc. Quand au deuxi&#232;me type 
</p>
<p>303</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Anis ZOUAGHI, Mounir ZRIGUI, Mohamed BEN AHMED
</p>
<p>d&#8217;approche, les r&#232;gles sont d&#233;duites directement &#224; partir d&#8217;un corpus d&#8217;apprentissage. Depuis 
quelques ann&#233;es, la tendance est vers l&#8217;utilisation des mod&#232;les de langages stochastiques dans 
le domaine de la compr&#233;hension de la parole spontan&#233;e (Schwartz et al., 1996), (Minker, 
1999), (Bousquet, 2002), etc. Cette tendance s&#8217;explique par le faite que les approches 
stochastiques offrent une alternative efficace aux approches par r&#232;gles, concernant le co&#251;t 
global de d&#233;veloppement du mod&#232;le, et la portabilit&#233; vers d&#8217;autres domaines. De plus, du fait 
que le locuteur parle d&#8217;une mani&#232;re spontan&#233;e, les fautes de syntaxe ou de grammaire sont 
beaucoup plus fr&#233;quentes &#224; l&#8217;oral qu'&#224; l'&#233;crit. C&#8217;est pour cela, qu&#8217;une analyse portant 
uniquement sur la syntaxe n&#8217;est souvent pas efficace. Ainsi, certains proposent pour faire face 
&#224; ce probl&#232;me, une analyse plus fine des ph&#233;nom&#232;nes linguistiques de l&#8217;oral tels que (Van 
Noord et al., 1999) et (Antoine et al., 2003), ou une combinaison d'une analyse syntaxique et 
s&#233;mantique tels que (Villaneau et al., 2001), (Seneff, 1992), etc. Contrairement &#224; la langue 
latine, la compr&#233;hension automatique de la parole arabe spontan&#233;e reste encore tr&#232;s peu 
abord&#233;e au niveau de la recherche scientifique. Durant les deux derni&#232;res d&#233;cennies les efforts 
ont &#233;t&#233; plut&#244;t concentr&#233;s sur la r&#233;alisation des analyseurs morphologiques et syntaxiques pour 
l&#8217;arabe tel que (Ouersighni, 2001). Malgr&#233; l&#8217;importance de la repr&#233;sentation et de l&#8217;analyse 
s&#233;mantique pour la r&#233;alisation de n&#8217;importe quel syst&#232;me de compr&#233;hension, il n&#8217;existe que 
quelques travaux qui s&#8217;int&#233;ressent &#224; ce domaine en vue du traitement automatique de la 
langue arabe &#233;crite et non pas parl&#233;e tels que (Haddad et al., 2005), (Meftouh et al., 2001), 
etc. Dans cet article, nous pr&#233;sentons le mod&#232;le de langage stochastique employ&#233; pour 
l&#8217;analyse s&#233;mantique de la parole arabe spontan&#233;e dans le cadre d&#8217;une application finalis&#233;e, 
ainsi que les r&#233;sultats d&#8217;&#233;valuation obtenus.
</p>
<p>2 L&#8217;application finalis&#233;e consid&#233;r&#233;e
</p>
<p>2.1 Le domaine de l&#8217;application 
</p>
<p>Pour tester et estimer les param&#232;tres du mod&#232;le de langage stochastique, nous avons utilis&#233; un 
corpus repr&#233;sentant le domaine des renseignements ferroviaires. La principale raison de ce 
choix est la taille statistiquement repr&#233;sentative du corpus d&#8217;apprentissage dont nous 
disposons (voir tables 1 et 2). Ce corpus a &#233;t&#233; collect&#233; en demandant &#224; cent personnes 
diff&#233;rentes de formuler des &#233;nonc&#233;s relatifs aux renseignements ferroviaires. Donc c&#8217;est un 
corpus simul&#233; et non pas r&#233;el. 
</p>
<p>Table 1 : Caract&#233;ristiques du corpus de point de vue volume.
</p>
<p>Table 2 : Caract&#233;ristiques du corpus de point de vue contenu.
</p>
<p>304</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>&#201;valuation des performances d&#8217;un mod&#232;le de langage stochastique  
</p>
<p>2.2 Le corpus d&#8217;apprentissage
</p>
<p>Le mod&#232;le de langage va servir &#224; attribuer &#224; chaque mot de l&#8217;&#233;nonc&#233; transcrit par le module 
de reconnaissance de la parole un couple de traits s&#233;mantiques not&#233; TS. Chaque couple TS est 
constitu&#233; de deux traits &#233;l&#233;mentaires : TS = (classe s&#233;mantique TSC, trait micro s&#233;mantique 
TSM). Le premier trait sert &#224; d&#233;terminer la classe s&#233;mantique &#224; laquelle appartient le mot &#224; 
interpr&#233;ter. Par exemple, toutes les villes du r&#233;seau ferroviaire sont repr&#233;sent&#233;es par la classe 
s&#233;mantique &#916;&#1000;&#1011;&#938;&#995; &quot;medina &quot;(ville). Pour l&#8217;application consid&#233;r&#233;e, nous avons utilis&#233; en tout 
12 classes s&#233;mantiques diff&#233;rentes (voir table 3 ci-dessous).  
</p>
<p>Classes s&#233;mantiques TCS Exemples d&#8217;instanciations 
</p>
<p>&#912;&#992;&#963;  (demande) &#1008;&#920;&#995;  (quand)- &#994;&#988;&#913; (combien)- &#912;&#931;&#899; (je veux)- &#1006;&#1011;&#938;&#927;  (existe)- etc. 
</p>
<p>&#916;&#987;&#942;&#931; (mouvement) &#912;&#1003;&#909;&#940;&#991;&#909; (qui va)- &#912;&#1003;&#939;&#909; (je vais)- &#990;&#956;&#1011; (arrive)- etc. 
</p>
<p>&#942;&#951;&#902;&#995;_&#916;&#987;&#942;&#931;  (Indice_mouvement) &#1008;&#991;&#903; (&#224;)- &#1006;&#932;&#999; (vers)- &#942;&#914;&#971; (&#224; travers)- &#998;&#995; (de)- etc. 
</p>
<p>&#942;&#951;&#902;&#995;_&#918;&#1012;&#983;&#1006;&#919;  (Indice_horaire) &#934;&#1011;&#941;&#910;&#920;&#913; (&#224; la date)- &#916;&#971;&#910;&#948;&#991;&#909; (l&#8217;heure)- etc. 
</p>
<p>&#944;&#995;&#941; (r&#233;f&#233;rence) &#909;&#940;&#1003; (ce)- &#1002;&#919;&#910;&#1003; (cette)- etc.
</p>
<p>&#916;&#1000;&#1011;&#938;&#995; (ville) &#916;&#947;&#1006;&#947; (sousse)- &#946;&#999;&#1006;&#919; (Tunis)- etc. 
</p>
<p>&#962;&#913;&#941; (liason) &#1005; (et)- etc. 
</p>
<p>&#937;&#938;&#971;_&#942;&#987;&#909;&#940;&#919;  (nombre_billets) &#915;&#942;&#987;&#940;&#919; (biellet)- &#997;&#910;&#988;&#995; (place)&#8211; &#998;&#1012;&#919;&#942;&#987;&#940;&#919; (deux billets)&#8211; etc.
</p>
<p>&#946;&#931; (bruit) &#997;&#899; (que)- &#994;&#987;&#941;&#910;&#1004;&#999; (journ&#233;e)- etc. 
</p>
<p>&#969;&#1006;&#999;_&#915;&#942;&#987;&#940;&#920;&#991;&#909;  (type_billet) &#916;&#914;&#992;&#964;&#992;&#991; - &#941;&#910;&#976;&#956;&#992;&#991; - &#911;&#910;&#1003;&#939; _&#1005;_&#911;&#910;&#1011;&#903;  -&#989; &#945;&#942;&#920;&#948;&#995; - etc. 
</p>
<p>&#961;&#942;&#951; (condition) &#1019; _&#943;&#1005;&#910;&#928;&#920;&#919;_&#994;&#1003;&#941;&#910;&#996;&#971;&#899;  (qui_ne_d&#233;passent_pas_l&#8217;age)- etc. 
</p>
<p>&#937;&#938;&#971; (nombre) 2 &#8211; 1 etc. 
</p>
<p>Table 3 : Les classes s&#233;mantiques consid&#233;r&#233;es. 
</p>
<p>La m&#233;thode d&#8217;identification ou d&#8217;extraction de ces classes est pr&#233;sent&#233;e dans le paragraphe 
suivant (2.3). En ce qui concerne le deuxi&#232;me trait du couple TS, c&#8217;est un trait micro 
s&#233;mantique qui permet de diff&#233;rencier le sens des mots appartenant &#224; une m&#234;me classe 
s&#233;mantique. Par exemple, ce trait permet de distinguer une ville de d&#233;part d&#8217;une ville de 
destination dans un &#233;nonc&#233; donn&#233;. Nous signalons que les mots synonymiques ou poss&#233;dant 
un m&#234;me r&#244;le s&#233;mantique poss&#232;dent le m&#234;me couple de traits TS. Le nombre total des traits 
micro s&#233;mantiques TMS utilis&#233;s est 20 traits, soit presque le double des TCS. Ces traits sont 
les suivants : &#912;&#992;&#963; (demande_g&#233;n&#233;rale) - _&#993;&#910;&#971; &#912;&#992;&#963; (demande_prix) &#8211; &#998;&#996;&#923; _ &#918;&#1012;&#983;&#1006;&#919; _ &#912;&#992;&#963;
(demande_horaire) &#8211;  (destination) &#8211;  (d&#233;part) &#8211;  (correspondance) &#8211; &#916;&#1004;&#927;&#1005; &#981;&#1020;&#964;&#999;&#909; &#941;&#1006;&#914;&#971; &#916;&#968;&#932;&#991;
(moment) &#8211;  (heure) &#8211;  (date) &#8211;  (jour) &#8211; &#947;&#916;&#971;&#910; &#934;&#1011;&#941;&#910;&#919; &#993;&#1006;&#1011; &#916;&#927;&#941;&#937; (classe) &#8211; etc. Ainsi, pour estimer 
les param&#232;tres du mod&#232;le de langage stochastique, nous avons cr&#233;e un corpus d&#8217;apprentissage 
(voir figure 1). Ce corpus a &#233;t&#233; obtenu en &#233;tiquetant au d&#233;but manuellement une quantit&#233; 
(500) des &#233;nonc&#233;s du corpus collect&#233; par un expert humain. Le principe d&#8217;&#233;tiquetage est 
d&#8217;attribuer &#224; chaque mot significatif pour l&#8217;application un couple TS tel que d&#233;fini ci haut. 
Les mots non significatifs ou vides sont &#233;limin&#233;s lors de la phase du pr&#233;traitement du corpus 
</p>
<p>305</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Anis ZOUAGHI, Mounir ZRIGUI, Mohamed BEN AHMED
</p>
<p>initial, et certains mots sont regroup&#233;s en une seule entr&#233;e. L&#8217;&#233;limination des mots vides nous 
a permis de simplifier la complexit&#233; et r&#233;duire la taille du mod&#232;le. Ensuite, nous avons 
appliqu&#233; ce mod&#232;le pour l&#8217;&#233;tiquetage s&#233;mantique des 9000 &#233;nonc&#233;s restants, et ce par groupes 
de 500. Entre chaque &#233;tape d&#8217;&#233;tiquetage automatique, nous avons proc&#233;d&#233; &#224; une v&#233;rification 
des r&#233;sultats obtenus et une correction des param&#232;tres a &#233;t&#233; &#233;tablie chaque fois qu&#8217;il y a une 
d&#233;tection d&#8217;erreurs. Enfin, les 500 &#233;nonc&#233;s restants nous ont servi pour l&#8217;&#233;valuation de la 
performance du mod&#232;le. Ainsi, 95% du corpus a &#233;t&#233; consacr&#233; &#224; l&#8217;apprentissage et 5% aux 
tests.
</p>
<p>Figure 1 : Principe de l&#8217;estimation des param&#232;tres du mod&#232;le de langage. 
</p>
<p>Parole
</p>
<p>Corpus repr&#233;sentatif de 
l&#8217;application  finalis&#233;e Expert humain
</p>
<p>Ensembles des 
couples TS 
</p>
<p>Mots vides 
</p>
<p>Corpus 
pr&#233;trait&#233;
</p>
<p>Corpus 
&#233;tiquet&#233;
</p>
<p>Estimation des 
param&#232;tres 
</p>
<p>Mod&#232;le de 
langage
</p>
<p>Analyse
s&#233;mantique 
</p>
<p>Enonc&#233; 
transcrit Reconnaissance de la parole
</p>
<p>Les fl&#232;ches en pointill&#233;s dans la figure 1, correspondent aux informations qui d&#233;pendent du 
domaine de l&#8217;application &#224; mod&#233;liser.    
</p>
<p>2.3 L&#8217;extraction des classes s&#233;mantiques 
</p>
<p>Pour extraire les classes s&#233;mantiques de l&#8217;application, nous avons appliqu&#233; l&#8217;algorithme des 
K-means propos&#233; par (McQueen, 1967), en utilisant l&#8217;information mutuelle moyenne IMm de 
(Rosenfeld, 1994) au lieu de la distance euclidienne pour mesurer la distance s&#233;mantique 
entre les diff&#233;rents mots du vocabulaire de l&#8217;application finalis&#233;e. Ceci, nous a amen&#233; &#224; 
remplacer dans l&#8217;algorithme le crit&#232;re d&#8217;&#233;valuation arg minj= 1, &#8230;,k d2(mi, cgj)  par arg maxj= 1, 
&#8230;,k d(mi, cgj)   (voir figure 2). A part que cet algorithme, permet de faciliter la t&#226;che 
d&#8217;identification des classes s&#233;mantiques, il a l&#8217;avantage d&#8217;&#234;tre :  
- Rapide face &#224; des donn&#233;es de taille importante, puisqu&#8217;il converge &#224; une vitesse lin&#233;aire de 
l&#8217;ordre de O(n.k.t) ; o&#249; n, k et t d&#233;signent respectivement le nombre des mots &#224; classer, le 
nombre des classes s&#233;mantiques et le nombre d&#8217;it&#233;rations maximales.  
- Et simple &#224; impl&#233;menter. 
</p>
<p>Pr&#233;sentation de l&#8217;algorithme des k-means :                                                                                                   - 
Choisir d&#8217;une mani&#232;re arbitraire les centres de gravit&#233; (cg1, cg2, cg3, &#8230;, cgk) des k classes s&#233;mantiques 
(cs1, cs2, cs3, &#8230;, csk). 
D&#233;but 
- Etiquette : 
    Pour tout mot mi de m1 &#224; mn faire
         Chercher la classe csk du mot mi en question : 
                    csk = arg maxj= 1, &#8230;,k d(mi, cgj) ; 
o&#249;, d (mi, cgj) =  IMm(mi, cgj) = P(mi, cgj)u  Log [P(mi / cgj) / P(mi).P(cgj)] + P(mi, cgj)u  Log
                           [P(mi / cgj) / P(mi).P(cgj)] + P(mi, cgj)u Log [P(mi / cgj) / P(mi).P(cgj)] + P(mi,  
</p>
<p>306</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>&#201;valuation des performances d&#8217;un mod&#232;le de langage stochastique  
</p>
<p>                            cgj)u  Log [P(mi / cgj) / P(mi).P(cgj)] 
          Recalculer le centre de gravit&#233; de la classe csk : 
          cgk =  1/Nk&#166;mi&#143;csk mi ; o&#249; Nk d&#233;signe dans cet algorithme le nombre de mots  dans la classe csk. 
 Fin Pour. 
- Arr&#234;t du traitement si les centres de gravit&#233; sont inchang&#233;s. 
- Retourner &#224; Etiquette sinon. 
Fin
</p>
<p>Figure 2 : l&#8217;algorithme des k-means en utilisant l&#8217;IMm comme m&#233;trique. 
</p>
<p>Cependant, le probl&#232;me principal de cette m&#233;thode est la d&#233;pendance du r&#233;sultat du 
classement final des informations donn&#233;es en entr&#233;e (les k centres de gravit&#233; des k classes 
s&#233;mantiques &#224; d&#233;terminer sont choisis d&#8217;une mani&#232;re totalement arbitraire). Cette limite ne 
pose pas de probl&#232;mes pour nous, puisque nous avons utilis&#233; cette m&#233;thode rien que pour 
aider et donner une id&#233;e &#224; l&#8217;utilisateur (surtout si cet utilisateur n&#8217;est pas un expert du 
domaine) sur la classification possible des mots de l&#8217;application d&#8217;un point de vue 
s&#233;mantique. Cependant les cartes auto organisatrices de (kohonen, 1989) offrent une 
alternative efficace, pour ceux qui cherchent des meilleurs r&#233;sultats de partitionnement 
(Jamoussi, 2004). 
</p>
<p>3 Mod&#233;lisation stochastique 
</p>
<p>3.1 Description du syst&#232;me de compr&#233;hension 
</p>
<p>Le syst&#232;me de compr&#233;hension con&#231;u permet de construire la repr&#233;sentation s&#233;mantique d&#8217;un 
&#233;nonc&#233;, sous la forme d&#8217;un ensemble d&#8217;associations attributs/valeurs (ou formulaire), comme 
le montre l&#8217;exemple suivant : Enonc&#233; transcrit : &#946;&#999;&#1006;&#919; &#1008;&#991;&#903; &#912;&#1003;&#909;&#940;&#991;&#909; &#941;&#910;&#964;&#984;&#991;&#910;&#913; &#997;&#910;&#988;&#995; &#944;&#928;&#931; &#938;&#1011;&#941;&#899;. souridou
hajza makan bilqitar athaheb ila tuwniss &#198; Je veux r&#233;server une place dans le train allant &#224; 
Tunis.
Repr&#233;sentation s&#233;mantique : &#916;&#1012;&#971;&#1006;&#999; (Type) = &#912;&#992;&#963;&#944;&#928;&#931;  (demande de r&#233;servation) 
</p>
<p>&#916;&#1000;&#1011;&#938;&#995;_&#981;&#1020;&#964;&#999;&#909;  (ville_ d&#233;part) = &amp;Villecourante 
&#993;&#1006;&#1011;_&#981;&#1020;&#964;&#999;&#909;  (jour_ d&#233;part) = ? 
&#916;&#971;&#910;&#947;_&#981;&#1020;&#964;&#999;&#909;  (heure_ d&#233;part) = ? 
&#916;&#1000;&#1011;&#938;&#995; _&#916;&#1004;&#927;&#1005;  (ville_ destination) = &#946;&#999;&#1006;&#919; (Tunis) 
&#937;&#938;&#971;_&#938;&#971;&#910;&#984;&#995;  (nombre_places) = 1 
</p>
<p>La figure 3 ci-dessous, pr&#233;sente l&#8217;architecture g&#233;n&#233;rale du syst&#232;me de compr&#233;hension. On 
remarque bien que la d&#233;duction du sens d&#8217;un &#233;nonc&#233; par ce syst&#232;me est le r&#233;sultat de 
l&#8217;accomplissement des traitements successifs suivants :  
- La segmentation de l&#8217;&#233;nonc&#233; transcrit par le module de reconnaissance de la parole : ce 
traitement permet d&#8217;identifier les mots ainsi que les diff&#233;rentes phrases du message du 
locuteur. Un m&#234;me message peut &#234;tre constitu&#233; d&#8217;un ou plusieurs requ&#234;tes &#224; la fois. D&#8217;o&#249;, il 
est n&#233;cessaire que le syst&#232;me puisse identifier les diff&#233;rentes requ&#234;tes du message, afin qu&#8217;il 
puisse interpr&#233;ter la demande de l&#8217;utilisateur dans toute son int&#233;gralit&#233;.   
- Le pr&#233;traitement de l&#8217;&#233;nonc&#233; : ce pr&#233;traitement consiste comme pour le pr&#233;traitement du 
corpus collect&#233; &#224; &#233;liminer par exemple les mots vides, &#224; regrouper certains mots en une seule 
entr&#233;e, etc. Ce mod&#232;le permet de simplifier la complexit&#233; de la t&#226;che de compr&#233;hension. 
- Le d&#233;codage s&#233;mantique de l&#8217;&#233;nonc&#233; : c'est-&#224;-dire l&#8217;&#233;tiquetage de chaque mot de l&#8217;&#233;nonc&#233; 
pr&#233;trait&#233; avec les couples TS correspondants.
- La construction du sens de l&#8217;&#233;nonc&#233;, cette &#233;tape correspond &#224; la phase de g&#233;n&#233;ration de 
</p>
<p>307</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Anis ZOUAGHI, Mounir ZRIGUI, Mohamed BEN AHMED
</p>
<p>l&#8217;ensemble des paires attribut/valeur (ou formulaire). 
Le d&#233;codage s&#233;mantique des &#233;nonc&#233;s pr&#233;trait&#233;s repose sur un mod&#232;le de langage stochastique  
qui permet d&#8217;encoder les r&#232;gles de la grammaire (voir paragraphe suivant) et sur un lexique 
s&#233;mantique d&#233;crit dans un fichier et contient tous les mots du vocabulaire de l&#8217;application. Ce 
lexique est un ensemble d&#8217;associations de la forme : Mot M / TS d&#233;crivant le sens du mot + 
P(W / TSC, TSM) qui est la probabilit&#233; d&#8217;utilisation de TS = (TSC, TSM) pour la description 
du sens du mot M. 
</p>
<p>Enonc&#233; reconnu 
Enonc&#233; 
pr&#233;trait&#233;
</p>
<p>Enonc&#233; 
segment&#233;
</p>
<p>Segmentation 
</p>
<p>Pr&#233;traitement de l&#8217;&#233;nonc&#233; D&#233;codage s&#233;mantique 
</p>
<p>Corpus pr&#233;trait&#233; et &#233;tiquet&#233;
Estimation des param&#232;tres
</p>
<p>Construction du sens de 
l&#8217;&#233;nonc&#233; 
</p>
<p>Enonc&#233; 
d&#233;cod&#233;
</p>
<p>Interpr&#233;tation de l&#8217;&#233;nonc&#233; sous 
la forme d&#8217;un formulaire.
</p>
<p>Lexique s&#233;mantique Mod&#232;le de langage 
stochastique
</p>
<p>Figure 3 : Architecture du syst&#232;me de compr&#233;hension 
</p>
<p>3.2 Le mod&#232;le de langage 
</p>
<p>3.2.1 Le principe du d&#233;codage 
</p>
<p>Le mod&#232;le de langage que nous pr&#233;sentons ici, permet d&#8217;attribuer &#224; chaque mot significatif un 
couple TS permettant de d&#233;crire sons sens. Comme nous l&#8217;avons signal&#233; auparavant, nous 
avons choisi de repr&#233;senter ce mod&#232;le &#224; l&#8217;aide d&#8217;un mod&#232;le de Markov cach&#233;. Le principe du 
d&#233;codage s&#233;mantique est le suivant : 
Nous consid&#233;rons un &#233;nonc&#233; constitu&#233; d&#8217;une suite de n mots : W = w1 w2 &#8230; wn 
Cette suite de n mots est r&#233;duite &#224; une suite de m mots apr&#232;s la phase du pr&#233;traitement de 
l&#8217;&#233;nonc&#233; (&#233;limination et regroupement de certains mots), o&#249; m d n : W = w1 w2 &#8230; wm 
Supposons que cette suite a &#233;t&#233; d&#233;cod&#233;e via la suite de m couples de traits s&#233;mantiques 
suivante:TS=TS1TS2&#8230;TSm, ou encore TS= (TSC1,TSM1)(TSC2,TSM2)&#8230;(TSCm, TSMm). 
Le but est alors de trouver les meilleures suites TS' connaissant W. Cette probabilit&#233; est 
calcul&#233;e gr&#226;ce au crit&#232;re du maximum a posteriori : P(TS&#8217; / W') = Max TS P(TS / W) =  
                                                                                                 Max TSCuTSM P(TSC, TSM / W) 
Ce qui donne en appliquant la formule de Bayes : P(TSC, TSM / W) = P(W / TSC, TSM) u
                                                                                                              P(TSC, TSM) / P(W)
Nous avons ensuite utilis&#233; l&#8217;algorithme de Viterbi (Rabiner et al., 1986), pour r&#233;aliser ce 
d&#233;codage.
</p>
<p>3.2.2 La topologie du mod&#232;le 
</p>
<p>Nous avons consid&#233;r&#233; un mod&#232;le de Markov cach&#233; (HMM) &#224; un seul niveau pour r&#233;aliser 
notre d&#233;codeur (voir figure 4). Chaque &#233;tat du mod&#232;le markovien repr&#233;sente un couple TS et 
</p>
<p>308</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>&#201;valuation des performances d&#8217;un mod&#232;le de langage stochastique  
</p>
<p>les probabilit&#233;s de transitions repr&#233;sentent les probabilit&#233;s de passage d&#8217;un TS vers un autre. 
L&#8217;interpr&#233;tation d&#8217;un mot d&#233;pend du contexte de l&#8217;&#233;nonc&#233;, c&#8217;est-&#224;-dire des relations de 
d&#233;pendances qu&#8217;il entretient avec les autres mots de l&#8217;&#233;nonc&#233;. Comme ll montre la figure 4 
suivante, nous avons consid&#233;r&#233; un HMM avec des probabilit&#233;s tri-grammes de transitions 
entre les couples de traits s&#233;mantiques TSi des mots. Ce mod&#232;le contribue ainsi &#224; la pr&#233;diction 
d&#8217;un couple de traits s&#233;mantiques TSi &#224; partir des deux couples pr&#233;c&#233;dents TSi-1 et TSi-2.  
</p>
<p>!
</p>
<p>!
</p>
<p>&quot; # # $!
</p>
<p>&quot; # # %!
</p>
<p>&amp; ' ( ) !
</p>
<p>&quot; $
</p>
<p>&quot; %
</p>
<p>&amp; ' ( ) !
</p>
<p>* ' + ' !
</p>
<p>, - , ' , + .
</p>
<p>* ' + ' !
</p>
<p>/ , - +.
</p>
<p>0 1 $!
0 1 %! 0 1 2!
</p>
<p>&quot; # $!
</p>
<p>&quot; # %!
</p>
<p>&amp; ' ( )
</p>
<p>Figure 4 : Exemple de mod&#233;lisation &#224; l&#8217;aide d&#8217;un mod&#232;le de Markov cach&#233; 
&#224; un niveau avec des probabilit&#233;s tri-grammes de transitions entre les TSi 
</p>
<p>La r&#233;alisation de ce mod&#232;le n&#233;cessite principalement deux types d&#8217;informations :  
- La mani&#232;re d&#8217;agencement des couples TSi entre eux, sous la forme de probabilit&#233;s tri-
grammes de transitions entre les TSi :
P(TSi / TSi-1, TSi-2) = N (TSi, TSi-1, TSi-2) / N (TSi-1, TSi-2) ; o&#249; N (TSi, TSi-1,TSi-2) 
(resp. N (TSi-1, TSi-2)) est le nombre d&#8217;occurrence de TSi, TSi-1 et TSi-2 (resp. TSi-1 et 
TSi-2) ensemble. 
- Et la probabilit&#233; d&#8217;&#233;mission de chaque mot du vocabulaire de l&#8217;application par chacun des 
couples TS d&#233;finis. Un mot peut &#234;tre d&#233;crit s&#233;mantiquement par plusieurs couples TS.  
P (W/ TS) = N (W, TS) / N (TS) ; o&#249; N (W, TS) est le nombre fois de description de W par 
TS et N (TS) est le nombre total d&#8217;utilisation de TS.
</p>
<p>3.2.3 Am&#233;lioration du mod&#232;le 
</p>
<p>En remarquant que ce n&#8217;est pas obligatoirement les mots pr&#233;c&#233;dant imm&#233;diatement le mot &#224; 
interpr&#233;ter qui ont une influence s&#233;mantique sur ce dernier, nous avons d&#233;cid&#233; d&#8217;employer 
lors de la phase de d&#233;codage du sens d&#8217;un mot que les TS des deux mots poss&#233;dant la plus 
grande affinit&#233; s&#233;mantique avec celui-ci. Pour atteindre cet objectif, nous nous sommes bas&#233;s 
sur la notion d&#8217;information mutuelle moyenne (Rosenfeld, 1994) qui permet de calculer le 
degr&#233; de corr&#233;lation ou de co-occurrence de deux mots donn&#233;s. Cette m&#233;thode nous a permis 
de ne plus utiliser syst&#233;matiquement les TS des deux mots qui pr&#233;c&#233;dent imm&#233;diatement le 
mot &#224; d&#233;coder. 
</p>
<p>309</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Anis ZOUAGHI, Mounir ZRIGUI, Mohamed BEN AHMED
</p>
<p>4 Application du mod&#232;le et r&#233;sultats 
Pour tester la performance du mod&#232;le stochastique d&#233;fini, nous avons utilis&#233; les 500 &#233;nonc&#233;s 
du corpus collect&#233; qui n&#8217;ont pas &#233;t&#233; employ&#233;s lors de la phase d&#8217;estimation des param&#232;tres du 
mod&#232;le de langage stochastique (voir paragraphes 2.1 et 2.2). Nous avons utilis&#233; comme 
mesures de performances :                                                                                                       - - 
Le nombre total de mauvaises interpr&#233;tations Nf  d&#233;fini comme suit : Nf = NCS + NMS, o&#249; NCS
et NMS sont respectivement le nombre de TSC et le nombre de TSM incorrectement attribu&#233;s 
par le syst&#232;me aux mots de l&#8217;&#233;nonc&#233;.  
- Le taux d&#8217;erreur du d&#233;codage s&#233;mantique : Tauxerreur = Nf / N ; o&#249; N est le nombre total de 
traits TSC et TSM attribu&#233;s &#224; l&#8217;&#233;nonc&#233; &#224; interpr&#233;ter.
- Le taux de pr&#233;cision est : Tauxpr&#233;cision = NC / N ; o&#249;  NC est le nombre des traits TSC et TSM 
correctement attribu&#233;s. 
La figure 5 suivante, pr&#233;sente les taux d&#8217;erreur et de pr&#233;cision trouv&#233;s. Ces taux sont r&#233;partis 
selon le type de renseignement demand&#233; par l&#8217;utilisateur : demande de r&#233;servation (DR), ou 
de renseignements sur le trajet (DT), l&#8217;horaire (DH), le prix (DP), ou la dur&#233;e du voyage 
(DD). On peut toujours aussi relever le taux d&#8217;erreurs des &#233;nonc&#233;s incorrectement d&#233;cod&#233;s 
s&#233;mantiquement, en consid&#233;rant le rapport entre les &#233;nonc&#233;s mal interpr&#233;t&#233;s et le nombres 
total d&#8217;&#233;nonc&#233;s consid&#233;r&#233;s dans le test (ici 500). 
</p>
<p>Le taux d&#8217;erreur r&#233;ellement trouv&#233; lors de la mesure de la performance de notre syst&#232;me est 
de l&#8217;ordre de 21,1%. En analysant davantage les r&#233;sultats, nous avons conclu qu&#8217;un mauvais 
d&#233;codage est obtenu chaque fois qu&#8217;il y a un manque de donn&#233;es d&#8217;apprentissage. La figure 5 
ci-dessus illustre bien ceci. En effet, d&#8217;apr&#232;s cette figure, nous remarquons que les r&#233;sultats de 
d&#233;codage sont bons dans presque tous les types de renseignements demand&#233;s par l&#8217;utilisateur 
(DT, DR, DP et DH). Le plus mauvais d&#233;codage correspond aux &#233;nonc&#233;s de type DD. Ceci 
est d&#251; au fait, que le nombre des &#233;nonc&#233;s DD consid&#233;r&#233;s (3,12% du corpus) lors de la phase 
d&#8217;apprentissage du mod&#232;le de langage est insuffisant. En effet, nous avons constat&#233; que 
seulement &#224; partir de 1000 &#233;nonc&#233;s appris que notre syst&#232;me devienne performant. A partir de 
ce seuil, le taux d&#8217;erreur est inf&#233;rieur &#224; 11%. Au dessous de la barre de 500 &#233;nonc&#233;s, les 
r&#233;sultats deviennent inacceptables. Le taux d&#8217;erreurs atteint 36,7% pour 296 &#233;nonc&#233;s appris, 
alors qu&#8217;il se restreint &#224; 9,9% pour 2726 &#233;nonc&#233;s appris (voir figure 5). Donc, une mauvaise 
interpr&#233;tation par notre syst&#232;me est due essentiellement &#224; un manque de donn&#233;es 
d&#8217;apprentissage, et non pas au type ou &#224; la topologie du mod&#232;le de langage utilis&#233;.  Nous 
avons aussi compar&#233; ce mod&#232;le de langage employ&#233; par rapport &#224; un mod&#232;le de langage avec 
des probabilit&#233;s bi-grammes de transitions entre les TSi (1) et un mod&#232;le de langage avec des 
probabilit&#233;s tri-grammes de transitions sans am&#233;lioration (2) (c-&#224;-d sans consid&#233;ration des TS 
</p>
<p>Figure 5 : Taux d&#8217;erreur et de pr&#233;cision selon le type de la  demande de l&#8217;utilisateur 
et le nombre d&#8217;&#233;nonc&#233;s appris.
</p>
<p>310</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>&#201;valuation des performances d&#8217;un mod&#232;le de langage stochastique  
</p>
<p>des 2 mots influant s&#233;mantiquement sur le mot &#224; interpr&#233;ter). Nous avons trouv&#233; que mod&#232;le 
(1) est efficace seulement lorsque le corpus d&#8217;apprentissage n&#8217;est pas assez volumineux (voir 
figure 6). En effet, plus l&#8217;ordre n d&#8217;un mod&#232;le n-grammes est petit, moins on a besoin de 
donn&#233;es d&#8217;apprentissage. Donc le mod&#232;le (1) peut &#234;tre une alternative efficace au mod&#232;le 
utilis&#233; (avec tri-grammes de transitions am&#233;lior&#233;), dans le cas o&#249; on ne dispose pas de corpus 
assez volumineux repr&#233;sentatif du domaine de l&#8217;application &#224; mod&#233;liser. Mais nous avons 
constat&#233; que d&#233;s qu&#8217;il y a occurrence d&#8217;h&#233;sitations ou de mots inconnus pr&#233;c&#233;dant le mot &#224; 
interpr&#233;ter les mod&#232;les (1) et (2) deviennent aussi inefficaces.   
</p>
<p>Figure 6 : R&#233;sultats de d&#233;codage obtenus en utilisant des mod&#232;les de Markov avec bi-   
         grammes et tri-grammes de transitions avec et sans consid&#233;ration du contexte pertinent.
</p>
<p>5 Conclusion
Nous avons pr&#233;sent&#233; dans cet article le mod&#232;le de langage stochastique que nous avons 
employ&#233; pour le d&#233;codage s&#233;mantique de la parole arabe spontan&#233;e. Pour cela, nous avons 
utilis&#233; un mod&#232;le de Markov cach&#233; &#224; un seul niveau, avec des probabilit&#233;s tri-grammes de 
transitions entre les couples de traits s&#233;mantiques TS. L&#8217;&#233;valuation du mod&#232;le, en l&#8217;appliquant 
dans le domaine des renseignements ferroviaires a montr&#233; son efficacit&#233;. Nous avons atteint 
un taux de pr&#233;cision de l&#8217;ordre de 90,1% avec 2726 &#233;nonc&#233;s appris de type demandes 
d&#8217;horaires. Nous avons montr&#233; qu&#8217;en cas de manque de donn&#233;es d&#8217;apprentissage, un mod&#232;le 
de Markov cach&#233; &#224; un seul niveau, avec des probabilit&#233;s bi-grammes de transitions entre les 
TS est plus puissant. Ceci est vrai malheureusement que dans le cas d&#8217;&#233;nonc&#233;s non spontan&#233;s, 
c'est-&#224;-dire ne contenant ni des h&#233;sitations ni des mots inconnus. Pour identifier les couples 
TS &#224; employer pour l&#8217;interpr&#233;tation des mots de l&#8217;&#233;nonc&#233;, nous avons employ&#233; l&#8217;information 
mutuelle moyenne IMm de (Rosenfeld, 1994). Pour faciliter la t&#226;che d&#8217;extraction des traits 
TSC d&#8217;une application, nous avons utilis&#233; l&#8217;algorithme de partitionnement des K-means 
propos&#233; par (McQueen, 1967). Cependant comme nous l&#8217;avons d&#233;j&#224; signal&#233;, nous avons 
utilis&#233; cette m&#233;thode rien que pour aider et donner une id&#233;e &#224; l&#8217;utilisateur sur la classification 
possible des mots de l&#8217;application d&#8217;un point de vue s&#233;mantique. Cependant les cartes auto 
organisatrices de (kohonen, 1989) offrent une alternative efficace, pour ceux qui cherchent 
des meilleurs r&#233;sultats de partitionnement (Jamoussi, 2004). 
</p>
<p>311</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Anis ZOUAGHI, Mounir ZRIGUI, Mohamed BEN AHMED
</p>
<p>R&#233;f&#233;rences
ANTOINE J-Y., GOULIAN J., VILLANEAU J. (2003), Quand le TAL robuste s&#8217;attaque au langage 
parl&#233;: analyse incr&#233;mentale pour la compr&#233;hension de la parole spontan&#233;e, Actes de TALN.
</p>
<p>Baum L.E., Petrie T., Soules G., Weiss N. (1970), A maximisation technique occurring in 
statistical analysis of probabilistic functions in Markov chains, The Annals of Mathematical 
Statistics.
</p>
<p>BOUSQUET-VERNHETTES C. (2002), Compr&#233;hension robuste de la parole spontan&#233;e dans le 
dialogue oral homme-machine &#8211; D&#233;codage conceptuel stochastique, Th&#232;se de doctorat de 
l&#8217;universit&#233; de Toulouse  III.
</p>
<p>HADDAD B., YASEEN M. (2005), A Compositional Approach Towards Semantic 
Representation and Construction of ARABIC, Actes de LACL.
</p>
<p>JAMOUSSI S. (2004), M&#233;thodes statistiques pour la compr&#233;hension automatique de la parole, 
Th&#232;se de doctorat de l&#8217;universit&#233; Henri Poincar&#233;.
</p>
<p>Kohonen T. (1998), Self-organisation and associative memory. Berlin, Spring-Verlag. 
</p>
<p>McQueen J. (1967), Some methods for classification and analysis of multivariate 
observations, Actes de the Berkeley Symposium on Mathematical Statistics and Probability.
</p>
<p>MEFTOUH K., LASKRI M.T. (2001), Generation of the Sense of a Sentence in Arabic 
Language with a Connectionist Approach, Actes de AICCSA.
</p>
<p>MINKER W. (1999), Compr&#233;hension automatique de la parole spontan&#233;e, Paris, L&#8217;Harmattan.  
</p>
<p>OUERSIGHNI R. (2001), A major offshoot of the Dinar-MBC project: AraParse, a 
morphosyntactic analyzer for unvowelled Arabic texts, Actes de ACL/EACL.
</p>
<p>Rabiner L.R., Juang B.H. (1986), Introduction to Hidden Markov Models, IEEE Transactions 
on Acoustics, Speech and Signal processing.
</p>
<p>ROSENFELD R. (1994), Adaptive statistical language modelling: A maximum entropy 
approach., Th&#232;se de doctorat de l&#8217;universit&#233; de Carnegie Mellon.
</p>
<p>Schwartz R., Miller S., Stallard D., Makhoul J. (1996), Language Understanding Using 
Hidden Understanding Models, Actes de ICSLP.
</p>
<p>SENEFF S. (1992), Robust parsing for spoken language systems, Actes de ICASSP, 189-192. 
</p>
<p>Van Noord G., Bouma G., Koeling R., Nederhof M.J.  (1999), Robust grammatical analysis 
for spoken dialogue systems, Natural Language Engineering 5(1).
</p>
<p>Villaneau J., Antoine J.Y., Ridoux O. (2001), Combining Syntax and Pragmatic Knowledge 
for the Understanding of Spontaneous Spoken Sentences, Actes de LACL&#8217;01.
</p>
<p>312</p>

</div></div>
</body></html>