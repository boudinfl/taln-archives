<?xml version="1.0" encoding="UTF-8"?>
<conference>
	<edition>
		<acronyme>TALN'2008</acronyme>
		<titre>15ème conférence sur le Traitement Automatique des Langues Naturelles</titre>
		<ville>Avignon</ville>
		<pays>France</pays>
		<dateDebut>2008-06-09</dateDebut>
		<dateFin>2008-06-13</dateFin>
		<presidents>
			<nom>Frédéric Bechet</nom>
			<nom>Jean-Francois Bonastre</nom>
		</presidents>
		<typeArticles>
			<type id="long">Papiers longs</type>
			<type id="court">Papiers courts</type>
		</typeArticles>
		<statistiques>
			<!-- <acceptations id="" soumissions=""></acceptations> -->
		</statistiques>
		<siteWeb>http://www.lia.univ-avignon.fr/jep-taln08/</siteWeb>
		<meilleurArticle>
			<articleId>taln-2008-long-014</articleId>
			<articleId>taln-2008-long-027</articleId>
		</meilleurArticle>
	</edition>
	<articles>
		<article id="taln-2008-long-001" session="Sémantique">
			<auteurs>
				<auteur>
					<nom>Amanda Rocha-Chaves</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Lucia-Helena Machado-Rino</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Universidade Federal de Sao Carlos</affiliation>
			</affiliations>
			<titre></titre>
			<type>long</type>
			<pages></pages>
			<resume></resume>
			<mots_cles></mots_cles>
			<title>The Mitkov algorithm for anaphora resolution in Portuguese</title>
			<abstract>This paper reports on the use of the Mitkov´s algorithm for pronoun resolution in texts written in Brazilian Portuguese. Third person pronouns are the only ones focused upon here, with noun phrases as antecedents. A system for anaphora resolution in Brazilian Portuguese texts was built that embeds most of the Mitkov’s features. Some of his resolution factors were directly incorporated into the system; others had to be slightly modified for language adequacy. The resulting approach was intrinsically evaluated on hand-annotated corpora. It was also compared to Lappin &amp; Leass’s algorithm for pronoun resolution, also customized to Portuguese. Success rate was the evaluation measure used in both experiments. The results of both evaluations are discussed here.</abstract>
			<keywords>Pronoun resolution, anaphora resolution</keywords>
		</article>
		<article id="taln-2008-long-002" session="Sémantique">
			<auteurs>
				<auteur>
					<nom>Paul Bédaride</nom>
					<email>Paul.Bedaride@loria.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Claire Gardent</nom>
					<email>Claire.Gardent@loria.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université Henri Poincaré/LORIA, Nancy</affiliation>
				<affiliation affiliationId="2">CNRS/LORIA, Nancy</affiliation>
			</affiliations>
			<titre>Réécriture et Détection d’Implication Textuelle</titre>
			<type>long</type>
			<pages></pages>
			<resume>Nous présentons un système de normalisation de la variation syntaxique qui permet de mieux reconnaître la relation d’implication textuelle entre deux phrases. Le système est évalué sur une suite de tests comportant 2 520 paires test et les résultats montrent un gain en précision par rapport à un système de base variant entre 29.8 et 78.5 points la complexité des cas considérés.</resume>
			<mots_cles>Normalisation syntaxique, Détection d’implication textuelle, Réécriture de graphe</mots_cles>
			<title></title>
			<abstract>We present a system for dealing with syntactic variation which significantly improves the treatment of textual implication. The system is evaluated on a testsuite of 2 520 sentence pairs and the results show an improvment in precision over the baseline which varies between 29.8 and 78.5 points depending on the complexity of the cases being considered.</abstract>
			<keywords>Syntactic normalisation, Recognising Textual Entailment, Graph rewriting</keywords>
		</article>
		<article id="taln-2008-long-003" session="Sémantique">
			<auteurs>
				<auteur>
					<nom>Delphine Battistelli</nom>
					<email>delphine.battistelli@paris-sorbonne.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Javier Couto</nom>
					<email>jcouto@fing.edu.uy</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Jean-Luc Minel</nom>
					<email>jminel@u-paris10.fr</email>
					<affiliationId>3</affiliationId>
				</auteur>
				<auteur>
					<nom>Sylviane Schwer</nom>
					<email>schwer@lipn.univ-paris13.fr</email>
					<affiliationId>4</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Lalic – Université Paris-Sorbonne – 28 rue Serpente 75006 Paris</affiliation>
				<affiliation affiliationId="2">INCO – Universidad de la Republica – 565 Herrera y Reissig 11300 Montevideo – Uruguay</affiliation>
				<affiliation affiliationId="3">MoDyCo – Université Paris X, CNRS UMR 7114 – 200 avenue de la République, 92001 Nanterre Cedex</affiliation>
				<affiliation affiliationId="4">LIPN – Université Paris XIII, CNRS UMR 7030 – 99 Bd Jean-Baptiste Clément, 93240 Villetaneuse</affiliation>
			</affiliations>
			<titre>Représentation algébrique des expressions calendaires et vue calendaire d’un texte</titre>
			<type>long</type>
			<pages></pages>
			<resume>Cet article aborde l’étude des expressions temporelles qui font référence directement à des unités de temps relatives aux divisions courantes des calendriers, que nous qualifions d’expressions calendaires (EC). Nous proposons une modélisation de ces expressions en définissant une algèbre d’opérateurs qui sont liés aux classes de marqueurs linguistiques qui apparaissent dans les EC. A partir de notre modélisation, une vue calendaire est construite dans la plate-forme de visualisation et navigation textuelle NaviTexte, visant le support à la lecture de textes. Enfin, nous concluons sur les perspectives offertes par le développement d’une première application de navigation temporelle.</resume>
			<mots_cles>expressions temporelles calendaires, modélisation algébrique, visualisation</mots_cles>
			<title></title>
			<abstract>In this paper we address the study of temporal expressions that refer directly to text units concerning common calendar divisions, that we name “calendar expressions” (EC). We propose to model these expressions by defining an operator algebra, the operators being related to different linguistics marker classes that occur in the EC. Based on our model, a calendar view is built up into the text visualization and navigation framework NaviTexte, aiming the support of text reading. Finally, we discuss the perspectives offered by the development of a first temporal navigation application.</abstract>
			<keywords>temporal calendar expressions, algebraic modelization, vizualisation</keywords>
		</article>
		<article id="taln-2008-long-004" session="Sémantique">
			<auteurs>
				<auteur>
					<nom>Gabriel Parent</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Michel Gagnon</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Philippe Muller</nom>
					<email></email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">École Polytechnique de Montréal</affiliation>
				<affiliation affiliationId="2">Université Paul-Sabatier, Toulouse</affiliation>
			</affiliations>
			<titre>Annotation d’expressions temporelles et d’événements en français</titre>
			<type>long</type>
			<pages></pages>
			<resume>Dans cet article, nous proposons une méthode pour identifier, dans un texte en français, l’ensemble des expressions adverbiales de localisation temporelle, ainsi que tous les verbes, noms et adjectifs dénotant une éventualité (événement ou état). Cette méthode, en plus d’identifier ces expressions, extrait certaines informations sémantiques : la valeur de la localisation temporelle selon la norme TimeML et le type des éventualités. Pour les expressions adverbiales de localisation temporelle, nous utilisons une cascade d’automates, alors que pour l’identification des événements et états nous avons recours à une analyse complète de la phrase. Nos résultats sont proches de travaux comparables sur l’anglais, en l’absence d’évaluation quantitative similaire sur le français.</resume>
			<mots_cles>Extraction d’informations temporelle, TimeML</mots_cles>
			<title></title>
			<abstract>We propose a method to extract expressions of temporal location in French texts, and all verbs, nouns and adjectived that denote an event or a state. This method also computes some semantic information : the value of the temporal location according to the TimeML standard and the types of eventualities. For temporal location expression, we use a cascade of transducers, wheras event identification is based on full syntactic parsing. Our result are compared to similar work on English, as no other empirical evaluation has been done on French before.</abstract>
			<keywords>Temporal information extraction, TimeML</keywords>
		</article>
		<article id="taln-2008-long-005" session="Extraction d'information">
			<auteurs>
				<auteur>
					<nom>Stéphane Huet</nom>
					<email>stephane.huet@irisa.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Guillaume Gravier</nom>
					<email>guillaume.gravier@irisa.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Pascale Sébillot</nom>
					<email>pascale.sebillot@irisa.fr</email>
					<affiliationId>3</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université de Rennes 1, Institut de Recherche en Informatique et Systèmes Aléatoires, Rennes</affiliation>
				<affiliation affiliationId="2">CNRS, Institut de Recherche en Informatique et Systèmes Aléatoires, Rennes</affiliation>
				<affiliation affiliationId="3">INSA de Rennes, Institut de Recherche en Informatique et Systèmes Aléatoires, Rennes</affiliation>
			</affiliations>
			<titre>Un modèle multi-sources pour la segmentation en sujets de journaux radiophoniques</titre>
			<type>long</type>
			<pages></pages>
			<resume>Nous présentons une méthode de segmentation de journaux radiophoniques en sujets, basée sur la prise en compte d’indices lexicaux, syntaxiques et acoustiques. Partant d’un modèle statistique existant de segmentation thématique, exploitant la notion de cohésion lexicale, nous étendons le formalisme pour y inclure des informations d’ordre syntaxique et acoustique. Les résultats expérimentaux montrent que le seul modèle de cohésion lexicale ne suffit pas pour le type de documents étudié en raison de la taille variable des segments et de l’absence d’un lien direct entre segment et thème. L’utilisation d’informations syntaxiques et acoustiques permet une amélioration substantielle de la segmentation obtenue.</resume>
			<mots_cles>segmentation en sujets, corpus oraux, cohésion lexicale, indices acoustiques, indices syntaxiques</mots_cles>
			<title></title>
			<abstract>We present a method for story segmentation of radio broadcast news, based on lexical, syntactic and audio cues. Starting from an existing statistical topic segmentation model which exploits the notion of lexical cohesion, we extend the formalism to include syntactic and acoustic knwoledge sources. Experimental results show that the sole use of lexical cohesion is not efficient for the type of documents under study because of the variable size of the segments and the lack of direct relation between topics and stories. The use of syntactics and acoustics enables a consequent improvement of the quality of the segmentation.</abstract>
			<keywords>story segmentation, spoken documents, lexical cohesion, acoustic cues, syntactic cues</keywords>
		</article>
		<article id="taln-2008-long-006" session="Extraction d'information">
			<auteurs>
				<auteur>
					<nom>Cédric Vidrequin</nom>
					<email>cedric.vidrequin@univ-avignon.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Juan-Manuel Torres-Moreno</nom>
					<email>juan-manuel.torres@univ-avignon.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Jean-Jacques Schneider</nom>
					<email>jjschneider@semantia.com</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Marc El-Bèze</nom>
					<email>marc.elbeze@univ-avignon.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Laboratoire Informatique d'Avignon, Agroparc BP1228, 84911 Avignon CEDEX 9, France</affiliation>
				<affiliation affiliationId="2">Société SEMANTIA, Parc d'activité de Gémenos,30 avenue du château de Jouques, 13420 Gémenos, France</affiliation>
			</affiliations>
			<titre>Extraction automatique d'informations à partir de micro-textes non structurés</titre>
			<type>long</type>
			<pages></pages>
			<resume>Nous présentons dans cet article une méthode d'extraction automatique d'informations sur des textes de très petite taille, faiblement structurés. Nous travaillons sur des textes dont la rédaction n'est pas normalisée, avec très peu de mots pour caractériser chaque information. Les textes ne contiennent pas ou très peu de phrases. Il s'agit le plus souvent de morceaux de phrases ou d'expressions composées de quelques mots. Nous comparons plusieurs méthodes d'extraction, dont certaines sont entièrement automatiques. D'autres utilisent en partie une connaissance du domaine que nous voulons réduite au minimum, de façon à minimiser le travail manuel en amont. Enfin, nous présentons nos résultats qui dépassent ce dont il est fait état dans la littérature, avec une précision équivalente et un rappel supérieur.</resume>
			<mots_cles>extraction automatique, micro-texte, texte non structuré, petites annonces</mots_cles>
			<title></title>
			<abstract>In this article, we present a method of automatic extraction of informations on very small-sized and weakly structured texts. We work on texts whose drafting is not normalised, with very few words to characterize each information. Texts does not contain sentences, or only few. There are mostly about fragments of sentences or about expressions of some words. We compare several extracting methods, some completely automatic and others using an small domain knowledge. We want this knowledge to be minimalistic to reduce as much as possible any manual work. Then, we present our results, witch are better than those published in the literature, with an equivalent precision and a greater recall.</abstract>
			<keywords>automatique extraction, micro-text, unstructured text, adds</keywords>
		</article>
		<article id="taln-2008-long-007" session="Extraction d'information">
			<auteurs>
				<auteur>
					<nom>Laurent Gillard</nom>
					<email>Laurent.Gillard@univ-avignon.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Patrice Bellot</nom>
					<email>Patrice.Bellot@univ-avignon.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Marc El-Bèze</nom>
					<email>marc.elbeze@univ-avignon.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">CEA, LIST, Laboratoire d’ingénierie de la connaissance multi-média multilingue, 18 route du Panorama, BP6, FONTENAY AUX ROSES, F- 92265 France</affiliation>
				<affiliation affiliationId="2">Laboratoire d'Informatique d'Avignon (LIA), Université d’Avignon et des Pays de Vaucluse, F-84911 Avignon Cedex 9 (France)</affiliation>
			</affiliations>
			<titre>Quelles combinaisons de scores et de critères numériques pour un système de Questions/Réponses ?</titre>
			<type>long</type>
			<pages></pages>
			<resume>Dans cet article, nous présentons une discussion sur la combinaison de différents scores et critères numériques pour la sélection finale d’une réponse dans la partie en charge des questions factuelles du système de Questions/Réponses développé au LIA. Ces scores et critères numériques sont dérivés de ceux obtenus en sortie de deux composants cruciaux pour notre système : celui de sélection des passages susceptibles de contenir une réponse et celui d’extraction et de sélection d’une réponse. Ils sont étudiés au regard de leur expressivité. Des comparaisons sont faites avec des approches de sélection de passages mettant en oeuvre des scores conventionnels en recherche d’information. Parallèlement, l’influence de la taille des contextes (en nombre de phrases) est évaluée. Cela permet de mettre en évidence que le choix de passages constitués de trois phrases autour d’une réponse candidate, avec une sélection des réponses basée sur une combinaison entre un score de passage de type Lucene ou Cosine et d’un score de compacité apparaît comme un compromis intéressant.</resume>
			<mots_cles>Système de Questions/Réponses, compacité, densité, combinaison de scores</mots_cles>
			<title></title>
			<abstract>This article discusses combinations of scores for selecting the best answer in a factual question answering system. Two major components of our QA system: (i) relevant passage selection, and (ii) answer extraction, produce a variety of scores. Here we study the expressivity of these scores, comparing our passage density score (i) to more conventional ranking techniques in information retrieval. In addition, we study varying the length (in number of sentences) of context retained in the relevant passages. We find that a three sentences window, and a mixing of Lucene or Cosine ranking with our compactness score (ii) provides the best results.</abstract>
			<keywords>Question Answering, compactness, density, combination of scores</keywords>
		</article>
		<article id="taln-2008-long-008" session="Dialogue Homme-Machine">
			<auteurs>
				<auteur>
					<nom>Vladimir Popescu</nom>
					<email>vladimir.popescu@imag.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Jean Caelen</nom>
					<email>jean.caelen@imag.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">GETALP/LIG, Institut National Polytechnique de Grenoble, France</affiliation>
				<affiliation affiliationId="2">Université « Politehnica » de Bucarest, Roumanie</affiliation>
			</affiliations>
			<titre>Contrôle rhétorique de la génération des connecteurs concessifs en dialogue homme-machine</titre>
			<type>long</type>
			<pages></pages>
			<resume>Les connecteurs discursifs ont on rôle important dans l’interprétation des discours (dialogiques ou pas), donc lorsqu’il s’agit de produire des énoncés, le choix des mots qui relient les énoncés (par exemple, en dialogue oral) s’avère essentiel pour assurer la compréhension des visées illocutoires des locuteurs. En linguistique computationnelle, le problème a été abordé surtout au niveau de l’interprétation des discours monologiques, tandis que pour le dialogue, les recherches se sont limitées en général à établir une correspondance quasiment biunivoque entre relations rhétoriques et connecteurs. Dans ce papier nous proposons un mécanisme pour guider la génération des connecteurs concessifs en dialogue, à la fois du point de vue discursif et sémantique ; chaque connecteur considéré sera contraint par un ensemble de conditions qui prennent en compte la cohérence du discours et la pertinence sémantique de chaque mot concerné. Les contraintes discursives, exprimées dans un formalisme dérivé de la SDRT (« Segmented Discourse Representation Theory ») seront plongées dans des contraintes sémantiques sur les connecteurs, proposées par l’école genevoise (Moeschler), pour enfin évaluer la cohérence du discours résultant de l’emploi de ces connecteurs.</resume>
			<mots_cles>Dialogue homme-machine, cohérence discursive, connecteurs concessifs, sémantique, pragmatique</mots_cles>
			<title></title>
			<abstract>Cue words play an important part in discourse interpretation (whether dialogues are concerned or not), hence when utterances have to be produced, the choice of the words that connect these utterances (for example, in spoken dialogue) is essential for ensuring the comprehension of the illocutionary goals of the speakers. In computational linguistics, the issue has been mitigated particularly in the context of monologue discourse interpretation, whereas for dialogues, research is usually limited to establishing an almost one-to-one mapping between rhetorical relations and cue words. In this paper we propose a mechanism for guiding concessive connectors in dialogue, at the same time from a discourse and from a semantic point of view; each considered connector will be constrained via a set of conditions that take into account discourse coherence and the semantic relevance of each word concerned. Discourse constraints, expressed in a formalism derived from SDRT (“Segmented Discourse Representation Theory”) will be mapped to semantic constraints on the connectors ; these semantic constraints are proposed by the Geneve linguistics school (Moeschler). Finally, the coherence of the discourse resulted from the use of these connectors will be assessed.</abstract>
			<keywords>Human-computer dialogue, discourse coherence, concessive connectors, semantics, pragmatics</keywords>
		</article>
		<article id="taln-2008-long-009" session="Dialogue Homme-Machine">
			<auteurs>
				<auteur>
					<nom>Alexandre Denis</nom>
					<email>alexandre.denis@loria.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Matthieu Quignard</nom>
					<email>matthieu.quignard@loria.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">UMR 7503 LORIA/CNRS – Campus scientifique, 56 506 Vandoeuvre-lès-Nancy Cedex</affiliation>
			</affiliations>
			<titre>Modélisation du principe d’ancrage pour la robustesse des systèmes de dialogue homme-machine finalisés</titre>
			<type>long</type>
			<pages></pages>
			<resume>Cet article présente une modélisation du principe d’ancrage (grounding) pour la robustesse des systèmes de dialogue finalisés. Ce principe, décrit dans (Clark &amp; Schaefer, 1989), suggère que les participants à un dialogue fournissent des preuves de compréhension afin d’atteindre la compréhension mutuelle. Nous explicitons une définition computationnelle du principe d’ancrage fondée sur des jugements de compréhension qui, contrairement à d’autres modèles, conserve une motivation pour l’expression de la compréhension. Nous déroulons enfin le processus d’ancrage sur un exemple tiré de l’implémentation du modèle.</resume>
			<mots_cles>dialogue homme-machine, robustesse, ancrage, compréhension mutuelle</mots_cles>
			<title></title>
			<abstract>This paper presents a grounding model for robustness in dialogue systems. The grounding process (Clark &amp; Schaefer, 1989) suggests that dialogue participants provide evidence of understanding in order to reach mutual understanding. We propose here a computational definition of the grounding criterion based on understanding judgments that keeps a motivation for providing evidence of understanding, as opposed to some existing models. Eventually, we detail the grounding process on a dialogue generated by the actual implementation.</abstract>
			<keywords>human-machine dialogue, robustness, grounding, mutual understanding</keywords>
		</article>
		<article id="taln-2008-long-010" session="Résumé Automatique">
			<auteurs>
				<auteur>
					<nom>Silvia Fernández</nom>
					<email>silvia.fernandez@univ-avignon.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Eric Sanjuan</nom>
					<email>eric.sanjuan@univ-avignon.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Juan-Manuel Torres-Moreno</nom>
					<email>juan-manuel.torres@univ-avignon.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>3</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Laboratoire Informatique d’Avignon, BP 1228 84911 Avignon France</affiliation>
				<affiliation affiliationId="2">LPM UHP-Nancy, BP 239 54506 Vandoeuvre lès Nancy France</affiliation>
				<affiliation affiliationId="3">École Polytechnique de Montréal, CP 6079 Montréal, Canada H3C3A7</affiliation>
			</affiliations>
			<titre>Enertex : un système basé sur l’énergie textuelle</titre>
			<type>long</type>
			<pages></pages>
			<resume>Dans cet article, nous présentons des applications du système Enertex au Traitement Automatique de la Langue Naturelle. Enertex est basé sur l’énergie textuelle, une approche par réseaux de neurones inspirée de la physique statistique des systèmes magnétiques. Nous avons appliqué cette approche aux problèmes du résumé automatique multi-documents et de la détection de frontières thématiques. Les résultats, en trois langues : anglais, espagnol et français, sont très encourageants.</resume>
			<mots_cles>Énergie textuelle, Réseaux de neurones, Modèle de Hopfield, Résumé automatique, Frontières thématiques</mots_cles>
			<title></title>
			<abstract>In this paper we present Enertex applications to study fundamental problems in Natural Language Processing. Enertex is based on textual energy, a neural networks approach, inspired by statistical physics of magnetic systems.We obtained good results on the application of this method to automatic multi-document summarization and thematic border detection in three languages : English, Spanish and French.</abstract>
			<keywords>Textual Energy, Neural Networks, Hopfield Model, Automatic Summarization, Thematic Boundaries</keywords>
		</article>
		<article id="taln-2008-long-011" session="Résumé Automatique">
			<auteurs>
				<auteur>
					<nom>Fatma Kallel Jaoua</nom>
					<email>fatma_fseg@yahoo.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Lamia Hadrich Belguith</nom>
					<email>l.belguith@fsegs.rnu.tn</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Maher Jaoua</nom>
					<email>maher.jaoua@fsegs.rnu.tn</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Abdelmajid Ben Hamadou</nom>
					<email>abdelmajid.benhamadou@fsegs.rnu.tn</email>
					<affiliationId>3</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Laboratoire LARIS-MIRACL, ISG Université de Gabès, Tunisie</affiliation>
				<affiliation affiliationId="2">Laboratoire LARIS-MIRACL, FSEGS, Université de Sfax, Tunisie</affiliation>
				<affiliation affiliationId="3">Laboratoire LARIM-MIRACL – ISIMS, Université de Sfax, Tunisie</affiliation>
			</affiliations>
			<titre>Intégration d’une étape de pré-filtrage et d’une fonction multiobjectif en vue d’améliorer le système ExtraNews de résumé de documents multiples</titre>
			<type>long</type>
			<pages></pages>
			<resume>Dans cet article, nous présentons les améliorations que nous avons apportées au système ExtraNews de résumé automatique de documents multiples. Ce système se base sur l’utilisation d’un algorithme génétique qui permet de combiner les phrases des documents sources pour former les extraits, qui seront croisés et mutés pour générer de nouveaux extraits. La multiplicité des critères de sélection d’extraits nous a inspiré une première amélioration qui consiste à utiliser une technique d’optimisation multi-objectif en vue d’évaluer ces extraits. La deuxième amélioration consiste à intégrer une étape de pré-filtrage de phrases qui a pour objectif la réduction du nombre des phrases des textes sources en entrée. Une évaluation des améliorations apportées à notre système est réalisée sur les corpus de DUC’04 et DUC’07.</resume>
			<mots_cles>Résumé automatique, pré-filtrage de phrases, optimisation multi-objectif, algorithme génétique</mots_cles>
			<title></title>
			<abstract>In this paper, we present the improvements that we brought to the ExtraNews system dedicated for automatic summarisation of multiple documents. This system is based on the use of a genetic algorithm that combines sentences of the source documents to form the extracts. These extracts are crossed and transferred to generate new ones. The multiplicity of the extract selection criteria inspired us the first improvement that consists in the use of a multi-objectif optimization technique to evaluate these extracts. The second improvement consists of the integration of a sentence pre-filtering step which is based on the notion of dominance between sentences. Our objective is to reduce the sentence number of the source texts. An evaluation of the proposed improvements to our system is realized on DUC' 04 and DUC' 07 corpus.</abstract>
			<keywords>Automatic summarization, sentences pre-filtering, multi-objective optimization, genetic algorithm</keywords>
		</article>
		<article id="taln-2008-long-012" session="Session commune JEP/TALN">
			<auteurs>
				<auteur>
					<nom>Philippe Langlais</nom>
					<email>felipe@iro.umontreal.ca</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Alexandre Patry</nom>
					<email>patryale@iro.umontreal.ca</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Fabrizio Gotti</nom>
					<email>gottif@iro.umontreal.ca</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Département d’Informatique et de Recherche Opérationnelle, Université de Montréal, C.P. 6128, succursale Centre-Ville, H3C 3J7, Montréal, Québec, Canada</affiliation>
			</affiliations>
			<titre>Recherche locale pour la traduction statistique à base de segments</titre>
			<type>long</type>
			<pages></pages>
			<resume>Dans cette étude, nous nous intéressons à des algorithmes de recherche locale pour la traduction statistique à base de segments (phrase-based machine translation). Les algorithmes que nous étudions s’appuient sur une formulation complète d’un état dans l’espace de recherche contrairement aux décodeurs couramment utilisés qui explorent l’espace des préfixes des traductions possibles. Nous montrons que la recherche locale seule, permet de produire des traductions proches en qualité de celles fournies par les décodeurs usuels, en un temps nettement inférieur et à un coût mémoire constant. Nous montrons également sur plusieurs directions de traduction qu’elle permet d’améliorer de manière significative les traductions produites par le système à l’état de l’art Pharaoh (Koehn, 2004).</resume>
			<mots_cles>Traduction statistique, recherche locale, post-traitement</mots_cles>
			<title></title>
			<abstract>Most phrase-based statistical machine translation decoders rely on a dynamicprogramming technique for maximizing a combination of models, including one or several language models and translation tables. One implication of this choice is the design of a scoring function that can be computed incrementally on partial translations, a restriction a search engine using a complete-state formulation does not have. In this paper, we present experiments we conducted with a simple, yet effective greedy search engine.We report significant improvements in translation quality over a state-of-the-art beam-search decoder, for some configurations.</abstract>
			<keywords>Statistical Machine Translation, local search, post-processing</keywords>
		</article>
		<article id="taln-2008-long-013" session="Session commune JEP/TALN">
			<auteurs>
				<auteur>
					<nom>Catherine Kobus</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>François Yvon</nom>
					<email></email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Géraldine Damnati</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Orange Labs / 2, avenue Pierre Marzin, 22300 Lannion</affiliation>
				<affiliation affiliationId="2">Univ. Paris Sud 11 &amp; LIMSI-CNRS, BP 133, 91403 Orsay Cedex</affiliation>
			</affiliations>
			<titre>Transcrire les SMS comme on reconnaît la parole</titre>
			<type>long</type>
			<pages></pages>
			<resume>Cet article présente une architecture inspirée des systèmes de reconnaissance vocale pour effectuer une normalisation orthographique de messages en « langage SMS ». Nous décrivons notre système de base, ainsi que diverses évolutions de ce système, qui permettent d’améliorer sensiblement la qualité des normalisations produites.</resume>
			<mots_cles>SMS, décodage phonétique, modèles de langage, transducteurs finis</mots_cles>
			<title></title>
			<abstract>This paper presents a system aiming at normalizing the orthography of SMS messages, using techniques that are commonly used in automatic speech recognition devices. We describe a baseline system and various evolutions, which are shown to improve significantly the quality of the output normalizations.</abstract>
			<keywords>SMS, phonetic decoding, language models, finite-state transducers</keywords>
		</article>
		<article id="taln-2008-long-014" session="Syntaxe">
			<auteurs>
				<auteur>
					<nom>Laura Kallmeyer</nom>
					<email>lk@sfs.uni-tuebingen.de</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Yannick Parmentier</nom>
					<email>parmenti@sfs.uni-tuebingen.de</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">SFB 441 / Université de Tübingen, Nauklerstr. 35, 72074 Tübingen, Allemagne</affiliation>
				<affiliation affiliationId="2">SfS-CL / SFB 441 / Université de Tübingen, Nauklerstr. 35, 72074 Tübingen, Allemagne</affiliation>
			</affiliations>
			<titre>Convertir des grammaires d’arbres adjoints à composantes multiples avec tuples d’arbres (TT-MCTAG) en grammaires à concaténation d’intervalles (RCG)</titre>
			<type>long</type>
			<pages></pages>
			<resume>Cet article étudie la relation entre les grammaires d’arbres adjoints à composantes multiples avec tuples d’arbres (TT-MCTAG), un formalisme utilisé en linguistique informatique, et les grammaires à concaténation d’intervalles (RCG). Les RCGs sont connues pour décrire exactement la classe PTIME, il a en outre été démontré que les RCGs « simples » sont même équivalentes aux systèmes de réécriture hors-contextes linéaires (LCFRS), en d’autres termes, elles sont légèrement sensibles au contexte. TT-MCTAG a été proposé pour modéliser les langages à ordre des mots libre. En général ces langages sont NP-complets. Dans cet article, nous définissons une contrainte additionnelle sur les dérivations autorisées par le formalisme TT-MCTAG. Nous montrons ensuite comment cette forme restreinte de TT-MCTAG peut être convertie en une RCG simple équivalente. Le résultat est intéressant pour des raisons théoriques (puisqu’il montre que la forme restreinte de TT-MCTAG est légèrement sensible au contexte), mais également pour des raisons pratiques (la transformation proposée ici a été utilisée pour implanter un analyseur pour TT-MCTAG).</resume>
			<mots_cles>Grammaires d’arbres adjoints à composantesmultiples, grammaires à concaténation d’intervalles, légère sensibilité au contexte</mots_cles>
			<title></title>
			<abstract>This paper investigates the relation between TT-MCTAG, a formalism used in computational linguistics, and RCG. RCGs are known to describe exactly the class PTIME ; « simple » RCG even have been shown to be equivalent to linear context-free rewriting systems, i.e., to be mildly context-sensitive. TT-MCTAG has been proposed to model free word order languages. In general, it is NP-complete. In this paper, we will put an additional limitation on the derivations licensed in TT-MCTAG. We show that TT-MCTAG with this additional limitation can be transformed into equivalent simple RCGs. This result is interesting for theoretical reasons (since it shows that TT-MCTAG in this limited form is mildly context-sensitive) and also for practical reasons (the proposed transformation has been used for implementing a parser for TT-MCTAG).</abstract>
			<keywords>Multicomponent Tree Adjoining Grammars, Range Concatenation Grammars, mild context-sensitivity</keywords>
		</article>
		<article id="taln-2008-long-015" session="Syntaxe">
			<auteurs>
				<auteur>
					<nom>Piet Mertens</nom>
					<email>piet.mertens@arts.kuleuven.be</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Département de Linguistique, Université de Leuven, Belgique</affiliation>
			</affiliations>
			<titre>Factorisation des contraintes syntaxiques dans un analyseur de dépendance</titre>
			<type>long</type>
			<pages></pages>
			<resume>Cet article décrit un analyseur syntaxique pour grammaires de dépendance lexicalisées. Le formalisme syntaxique se caractérise par une factorisation des contraintes syntaxiques qui se manifeste dans la séparation entre dépendance et ordre linéaire, la spécification fonctionnelle (plutôt que syntagmatique) des dépendants, la distinction entre dépendants valenciels (la sous-catégorisation) et non valenciels (les circonstants) et la saturation progressive des arbres. Ceci résulte en une formulation concise de la grammaire à un niveau très abstrait et l’élimination de la reduplication redondante des informations due aux réalisations alternatives des dépendants ou à leur ordre. Les arbres élémentaires (obtenus à partir des formes dans l’entrée) et dérivés sont combinés entre eux par adjonction d’un arbre dépendant saturé à un arbre régissant, moyennant l’unification des noeuds et des relations. La dérivation est réalisée grâce à un analyseur chart bi-directionnel.</resume>
			<mots_cles>Analyseur syntaxique, dépendance</mots_cles>
			<title></title>
			<abstract>We describe a parser for lexicalized dependency grammar. The formalism is characterized by a factorization of the syntactic constraints, based on the separation between dependency and word order, the functional (rather than phrasal) specification of dependents, the distinction between valency and non valency dependents, and the incremental saturation of the trees. These features enable a concise formulation of the grammar at a very abstract level and eliminate syntactic information redundancy due to alternative forms of dependents and word order. Each word form produces one or more elementary dependency trees. Trees, both elementary and derived, are combined by adjoining a saturated dependent to a governing tree, after unification of shared nodes and relations. This is achieved using a bi-directional chart parser.</abstract>
			<keywords>Syntactic parser, dependency</keywords>
		</article>
		<article id="taln-2008-long-016" session="Syntaxe">
			<auteurs>
				<auteur>
					<nom>Pascal Vaillant</nom>
					<email>pascal.vaillant@guyane.univ-ag.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">GRIMAAG, Université des Antilles-Guyane, B.P. 792, 97337 Cayenne cedex</affiliation>
			</affiliations>
			<titre>Grammaires factorisées pour des dialectes apparentés</titre>
			<type>long</type>
			<pages></pages>
			<resume>Pour la formalisation du lexique et de la grammaire de dialectes étroitement apparentés, il peut se révéler utile de factoriser une partie du travail de modélisation. Les soussystèmes linguistiques isomorphes dans les différents dialectes peuvent alors faire l’objet d’une description commune, les différences étant spécifiées par ailleurs. Cette démarche aboutit à un modèle de grammaire à couches : le noyau est commun à la famille de dialectes, et une couche superficielle détermine les caractéristiques de chacun. Nous appliquons ce procédé à la famille des langues créoles à base lexicale française de l’aire américano-caraïbe.</resume>
			<mots_cles>TAG, modélisation, grammaire, variation dialectale</mots_cles>
			<title></title>
			<abstract>The task of writing formal lexicons and grammars for closely related dialects can benefit from factoring part of the modelling. Isomorphic linguistic subsystems from the different dialectsmay have a common description, while the differences are specified aside. This process leads to a layered grammar model: a kernel common to the whole family of dialects, and a superficial skin specifying the particular properties of each one of them. We apply this principle to the family of French-lexifier creole languages of the American-Caribbean area.</abstract>
			<keywords>TAG, grammar, modelling, dialectal variation</keywords>
		</article>
		<article id="taln-2008-long-017" session="Syntaxe">
			<auteurs>
				<auteur>
					<nom>Benoît Crabbé</nom>
					<email>bcrabbe@linguist.jussieu.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Marie Candito</nom>
					<email>candito@linguist.jussieu.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université Paris 7 (UFRL) et INRIA (Alpage), 30 rue du Château des Rentiers 75013 Paris</affiliation>
			</affiliations>
			<titre>Expériences d’analyse syntaxique statistique du français</titre>
			<type>long</type>
			<pages></pages>
			<resume>Nous montrons qu’il est possible d’obtenir une analyse syntaxique statistique satisfaisante pour le français sur du corpus journalistique, à partir des données issues du French Treebank du laboratoire LLF, à l’aide d’un algorithme d’analyse non lexicalisé.</resume>
			<mots_cles>Analyseur syntaxique statistique, Analyse syntaxique non lexicalisée, Analyse du français</mots_cles>
			<title></title>
			<abstract>We show that we can acquire satisfactory parsing results for French from data induced from the French Treebank using an unlexicalised parsing algorithm.</abstract>
			<keywords>Statistical parser, Unlexicalised parsing, French parsing</keywords>
		</article>
		<article id="taln-2008-long-018" session="Ressources">
			<auteurs>
				<auteur>
					<nom>Benoît Sagot</nom>
					<email>benoit.sagot@inria.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Darja Fišer</nom>
					<email>darja.fiser@guest.arnes.si</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Alpage, INRIA / Paris 7, 30 rue du Ch. des rentiers, 75013 Paris, France</affiliation>
				<affiliation affiliationId="2">Fac. des Lettres, Univ. de Ljubljana, Aškerˇceva 2, 1000 Ljubljana, Slovénie</affiliation>
			</affiliations>
			<titre>Construction d’un wordnet libre du français à partir de ressources multilingues</titre>
			<type>long</type>
			<pages></pages>
			<resume>Cet article décrit la construction d’un Wordnet Libre du Français (WOLF) à partir du Princeton WordNet et de diverses ressources multilingues. Les lexèmes polysémiques ont été traités au moyen d’une approche reposant sur l’alignement en mots d’un corpus parallèle en cinq langues. Le lexique multilingue extrait a été désambiguïsé sémantiquement à l’aide des wordnets des langues concernées. Par ailleurs, une approche bilingue a été suffisante pour construire de nouvelles entrées à partir des lexèmes monosémiques. Nous avons pour cela extrait des lexiques bilingues à partir deWikipédia et de thésaurus. Le wordnet obtenu a été évalué par rapport au wordnet français issu du projet EuroWordNet. Les résultats sont encourageants, et des applications sont d’ores et déjà envisagées.</resume>
			<mots_cles>Wordnet, corpus alignés, Wikipédia, sémantique lexicale</mots_cles>
			<title></title>
			<abstract>This paper describes the construction of a freely-available wordnet for French (WOLF) based on Princeton WordNet by using various multilingual resources. Polysemous words were dealt with an approach in which a parallel corpus for five languages was wordaligned and the extracted multilingual lexicon was disambiguated with the existing wordnets for these languages. On the other hand, a bilingual approach sufficed to acquire equivalents for monosemous words. Bilingual lexicons were extracted from Wikipedia and thesauri. The merged wordnet was evaluated against the French WordNet. The results are promising, and applications are already intended.</abstract>
			<keywords>Wordnet, aligned corpora, Wikipedia, lexical semantics</keywords>
		</article>
		<article id="taln-2008-long-019" session="Ressources">
			<auteurs>
				<auteur>
					<nom>Mathieu Lafourcade</nom>
					<email>lafourcade@lirmm.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Alain Joubert</nom>
					<email>joubert@lirmm.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIRMM – Univ. Montpellier 2 - CNRS, Laboratoire d’Informatique, de Robotique et de Microélectronique de Montpellier, 161, rue Ada – 34392 Montpellier Cédex 5 – France</affiliation>
			</affiliations>
			<titre>Détermination des sens d’usage dans un réseau lexical construit à l’aide d’un jeu en ligne</titre>
			<type>long</type>
			<pages></pages>
			<resume>Les informations lexicales, indispensables pour les tâches réalisées en TALN, sont difficiles à collecter. En effet, effectuée manuellement, cette tâche nécessite la compétence d’experts et la durée nécessaire peut être prohibitive, alors que réalisée automatiquement, les résultats peuvent être biaisés par les corpus de textes retenus. L’approche présentée ici consiste à faire participer un grand nombre de personnes à un projet contributif en leur proposant une application ludique accessible sur le web. A partir d’une base de termes préexistante, ce sont ainsi les joueurs qui vont construire le réseau lexical, en fournissant des associations qui ne sont validées que si elles sont proposées par au moins une paire d’utilisateurs. De plus, ces relations typées sont pondérées en fonction du nombre de paires d’utilisateurs qui les ont proposées. Enfin, nous abordons la question de la détermination des différents sens d’usage d’un terme, en analysant les relations entre ce terme et ses voisins immédiats dans le réseau lexical, avant de présenter brièvement la réalisation et les premiers résultats obtenus.</resume>
			<mots_cles>Traitement Automatique du Langage Naturel, réseau lexical, relations typées pondérées, sens d’usage d’un terme, jeu en ligne</mots_cles>
			<title></title>
			<abstract>Lexical information is indispensable for the tasks realized in NLP, but collecting lexical information is a difficult work. Indeed, when done manually, it requires the competence of experts and the duration can be prohibitive. When done automatically, the results can be biased by the corpus of texts. The approach we present here consists in having people take part in a collective project by offering them a playful application accessible on the web. From an already existing base of terms, the players themselves thus build the lexical network, by supplying associations which are validated only if they are suggested by a pair of users. Furthermore, these typed relations are weighted according to the number of pairs of users who provide them. Finally, we approach the question of the word usage determination for a term, by searching relations between this term and its neighbours in the network, before briefly presenting the realization and the first obtained results.</abstract>
			<keywords>Natural Language Processing, lexical network, typed and weighted relations, word usage, webbased game</keywords>
		</article>
		<article id="taln-2008-long-020" session="Ressources">
			<auteurs>
				<auteur>
					<nom>Feten Baccar</nom>
					<email>baccarf@yahoo.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Aïda Khemakhem</nom>
					<email>aida_khemakhem@yahoo.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Bilel Gargouri</nom>
					<email>bilel.gargouri@fsegs.rnu.tn</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Kais Haddar</nom>
					<email>kais.haddar@fss.rnu.tn</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Abdelmajid Ben Hamadou</nom>
					<email>abdelmajid.benhamadou@isimsf.rnu.tn</email>
					<affiliationId>3</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Laboratoire MIRACL, FSEGS, B.P. 1088, 3018 Sfax, Tunisie</affiliation>
				<affiliation affiliationId="2">Laboratoire MIRACL, FSS, B.P. 802, 3038 Sfax, Tunisie</affiliation>
				<affiliation affiliationId="3">Laboratoire MIRACL, ISIMS, B.P. 242, 3021 Sakiet-Ezzit Sfax, Tunisie</affiliation>
			</affiliations>
			<titre>Modélisation normalisée LMF des dictionnaires électroniques éditoriaux de l’arabe</titre>
			<type>long</type>
			<pages></pages>
			<resume>Le présent papier s’intéresse à l’élaboration des dictionnaires électroniques arabes à usage éditorial. Il propose un modèle unifié et normalisé de ces dictionnaires en se référant à la future norme LMF (Lexical Markup Framework) ISO 24613. Ce modèle permet de construire des dictionnaires extensibles, sur lesquels on peut réaliser, grâce à une structuration fine et standard, des fonctions de consultation génériques adaptées aux besoins des utilisateurs. La mise en oeuvre du modèle proposé est testée sur des dictionnaires existants de la langue arabe en utilisant, pour la consultation, le système ADIQTO (Arabic DIctionary Query TOols) que nous avons développé pour l’interrogation générique des dictionnaires normalisés de l’arabe.</resume>
			<mots_cles>Dictionnaire électronique Arabe, usage éditorial, modèle normalisé, LMF, interrogation générique</mots_cles>
			<title></title>
			<abstract>This paper is interested in the development of the Arabic electronic dictionaries of human use. It proposes a unified and standardized model for these dictionaries according to the future standard LMF (Lexical Markup Framework) ISO 24613. Thanks to its subtle and standardized structure, this model allows the development of extendable dictionaries on which generic interrogation functions adapted to the user’s needs can be implemented. This model has already been carried out on some existing Arabic dictionaries using the ADIQTQ (Arabic DIctionary Query Tool) system, which we developed for the generic interrogation of standardized dictionaries of Arabic.</abstract>
			<keywords>Arabic electronic dictionary, human use, standardized model, LMF, generic interrogation</keywords>
		</article>
		<article id="taln-2008-long-021" session="Ressources">
			<auteurs>
				<auteur>
					<nom>Lucie Barque</nom>
					<email>lucie.barque@linguist.jussieu.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>François-Régis Chaumartin</nom>
					<email>frc@proxem.com</email>
					<affiliationId>2</affiliationId>
					<affiliationId>3</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Lattice - Université Paris 7, UFRL, Case 7003, 75251 Paris cedex 5</affiliation>
				<affiliation affiliationId="2">Proxem - 7 impasse Dumur, 92110 Clichy</affiliation>
				<affiliation affiliationId="3">Alpage - Université Paris 7, UFRL, Case 7003, 75251 Paris cedex 5</affiliation>
			</affiliations>
			<titre>La polysémie régulière dans WordNet</titre>
			<type>long</type>
			<pages></pages>
			<resume>Cette étude propose une analyse et une modélisation des relations de polysémie dans le lexique électronique anglais WordNet. Elle exploite pour cela la hiérarchie des concepts (représentés par des synsets), et la définition associée à chacun de ces concepts. Le résultat est constitué d'un ensemble de règles qui nous ont permis d'identifier d’une façon largement automatisée, avec une précision voisine de 91%, plus de 2100 paires de synsets liés par une relation de polysémie régulière. Notre méthode permet aussi une désambiguïsation lexicale partielle des mots de la définition associée à ces synsets.</resume>
			<mots_cles>polysémie régulière, métaphore, métonymie, WordNet, désambiguïsation lexicale</mots_cles>
			<title></title>
			<abstract>This paper presents an analysis and modeling of polysemy in the WordNet English lexical database. It exploits the concepts hierarchy (constituted by synsets), and the gloss defining each of these concepts. The result consists of rules set which enabled us to identify in a largely automated way, with a precision close to 91%, more than 2100 synsets pairs, connected by a regular polysemy relation. Our method also allows a partial word sense disambiguation of the definition associated with these synsets.</abstract>
			<keywords>regular polysemy, metaphor, metonymy, WordNet, word sense disambiguation</keywords>
		</article>
		<article id="taln-2008-long-022" session="Traduction automatique">
			<auteurs>
				<auteur>
					<nom>Caroline Lavecchia</nom>
					<email></email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Kamel Smaïli</nom>
					<email></email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>David Langlois</nom>
					<email></email>
					<affiliationId>1</affiliationId>
					<affiliationId>3</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LORIA/Speech Group, Campus scientifique, BP 239, 54506 Vandoeuvre lès Nancy Cedex, France</affiliation>
				<affiliation affiliationId="2">Université Nancy2</affiliation>
				<affiliation affiliationId="3">IUFM de Lorraine</affiliation>
			</affiliations>
			<titre>Une alternative aux modèles de traduction statistique d’IBM: Les triggers inter-langues</titre>
			<type>long</type>
			<pages></pages>
			<resume>Dans cet article, nous présentons une nouvelle approche pour la traduction automatique fondée sur les triggers inter-langues. Dans un premier temps, nous expliquons le concept de triggers inter-langues ainsi que la façon dont ils sont déterminés. Nous présentons ensuite les différentes expérimentations qui ont été menées à partir de ces triggers afin de les intégrer au mieux dans un processus complet de traduction automatique. Pour cela, nous construisons à partir des triggers inter-langues des tables de traduction suivant différentes méthodes. Nous comparons par la suite notre système de traduction fondé sur les triggers interlangues à un système état de l’art reposant sur le modèle 3 d’IBM (Brown &amp; al., 1993). Les tests menés ont montré que les traductions automatiques générées par notre système améliorent le score BLEU (Papineni &amp; al., 2001) de 2, 4% comparé à celles produites par le système état de l’art.</resume>
			<mots_cles>Traduction Automatique Statistique, Triggers Inter-Langues, Information Mutuelle, Corpus parallèle, Décodage</mots_cles>
			<title></title>
			<abstract>In this paper, we present an original approach for machine translation based on inter-lingual triggers. First, we describe the idea of inter-lingual triggers and how to determine them. Then, we present the way to make good use of them in order to integrate them in an entire translation process. We used inter-lingual triggers to estimate different translation tables. Then we compared our translation system based on triggers to a state-of-the-art system based on IBM model 3 (Brown &amp; al., 1993). The experiments showed that automatic translations generated by our system outperform model 3 of IBM by 2.4% in terms of BLEU (Papineni &amp; al., 2001).</abstract>
			<keywords>StatisticalMachine Translation, Inter-Lingual Triggers,Mutual Information, Parallel corpus, Decoding process</keywords>
		</article>
		<article id="taln-2008-long-023" session="Traduction automatique">
			<auteurs>
				<auteur>
					<nom>Aurélien Max</nom>
					<email>aurelien.max@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIMSI-CNRS et Université Paris-Sud 11 Orsay, France</affiliation>
			</affiliations>
			<titre>Génération de reformulations locales par pivot pour l’aide à la révision</titre>
			<type>long</type>
			<pages></pages>
			<resume>Cet article présente une approche pour obtenir des paraphrases pour de courts segments de texte qui peuvent aider un rédacteur à reformuler localement des textes. La ressource principale utilisée est une table d’alignements bilingues de segments d’un système de traduction automatique statistique. Un segment marqué par le rédacteur est tout d’abord traduit dans une langue pivot avant d’être traduit à nouveau dans la langue d’origine, ce qui est permis par la nature même de la ressource bilingue utilisée sans avoir recours à un processus de traduction complet. Le cadre proposé permet l’intégration et la combinaison de différents modèles d’estimation de la qualité des paraphrases. Des modèles linguistiques tentant de prendre en compte des caractéristiques des paraphrases de courts segments de textes sont proposés, et une évaluation est décrite et ses résultats analysés. Les domaines d’application possibles incluent, outre l’aide à la reformulation, le résumé et la réécriture des textes pour répondre à des conventions ou à des préférences stylistiques. L’approche est critiquée et des perspectives d’amélioration sont proposées.</resume>
			<mots_cles>Paraphrase, Traduction Automatique Statistique basée sur les segments, Aide à la rédaction</mots_cles>
			<title></title>
			<abstract>In this article, we present a method to obtain paraphrases for short text spans that can be useful to help a writer in reformulating text. The main resource used is a bilingual phrase table containing aligned phrases, a common resource in statistical machine translation. The writer can mark a segment for paraphrasing, and this segment is first translated into a pivot language before being back-translated into the original language, which is possible without performing a full translation of the input. Our proposed framework allows integrating and combining various models for estimating paraphrase quality. We propose linguistic models which permits to conduct empirical experiments about the characteristics of paraphrases for short text spans. Application domains include, in addition to paraphrasing aids, summarization and rephrasing of text for conforming to conventional or stylistic guidelines. We finally discuss the limitations of our work and describe possible ways of improvement.</abstract>
			<keywords>Paraphrasing, Phrase-Based Statistical Machine Translation (PBSMT), Authoring aids</keywords>
		</article>
		<article id="taln-2008-long-024" session="Traduction automatique">
			<auteurs>
				<auteur>
					<nom>Christian Boitet</nom>
					<email>Christian.Boitet@imag.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Laboratoire LIG, GETALP – Université Joseph Fourier, 385 rue de la bibliothèque, BP 53, 38041 Grenoble, Cedex 9, France</affiliation>
			</affiliations>
			<titre>Les architectures linguistiques et computationnelles en traduction automatique sont indépendantes</titre>
			<type>long</type>
			<pages></pages>
			<resume>Contrairement à une idée répandue, les architectures linguistiques et computationnelles des systèmes de traduction automatique sont indépendantes. Les premières concernent le choix des représentations intermédiaires, les secondes le type d'algorithme, de programmation et de ressources utilisés. Il est ainsi possible d'utiliser des méthodes de calcul « expertes » ou « empiriques » pour construire diverses phases ou modules de systèmes d'architectures linguistiques variées. Nous terminons en donnant quelques éléments pour le choix de ces architectures en fonction des situations traductionnelles et des ressources disponibles, en termes de dictionnaires, de corpus, et de compétences humaines.</resume>
			<mots_cles>Traduction Automatique, TA, TAO, architecture linguistique, architecture computationnelle, TA experte, TA par règles, TA empirique, TA statistique, TA par l'exemple</mots_cles>
			<title></title>
			<abstract>Contrary to a wide-spread idea, the linguistic and computational architectures of MT systems are independent. The former concern the choice of the intermediate representations, the latter the type of algorithm, programming, and resources used. It is thus possible to use "expert" or "empirical" computational methods to build various phases or modules of systems having various linguistic architectures. We finish by giving some elements for choosing these architectures depending on the translational situations and the available resources, in terms of dictionaries, corpora, and human competences.</abstract>
			<keywords>Machine Translation, MT, linguistic architecture, computational architecture, expert MT, rule-based MT, empirical MT, statistical MT, example-based MT</keywords>
		</article>
		<article id="taln-2008-long-025" session="Entités Nommées">
			<auteurs>
				<auteur>
					<nom>Caroline Brun</nom>
					<email>Caroline.Brun@xrce.xerox.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Caroline Hagège</nom>
					<email>Caroline.Hagege@xrce.xerox.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Xerox Research Centre Europe – 6, chemin de Maupertuis, 38240 Meylan France</affiliation>
			</affiliations>
			<titre>Vérification sémantique pour l’annotation d’entités nommées</titre>
			<type>long</type>
			<pages></pages>
			<resume>Dans cet article, nous proposons une méthode visant à corriger et à associer dynamiquement de nouveaux types sémantiques dans le cadre de systèmes de détection automatique d’entités nommées (EN). Après la détection des entités nommées et aussi de manière plus générale des noms propres dans les textes, une vérification de compatibilité de types sémantiques est effectuée non seulement pour confirmer ou corriger les résultats obtenus par le système de détection d’EN, mais aussi pour associer de nouveaux types non couverts par le système de détection d’EN. Cette vérification est effectuée en utilisant l’information syntaxique associée aux EN par un système d’analyse syntaxique robuste et en confrontant ces résultats avec la ressource sémantique WordNet. Les résultats du système de détection d’EN sont alors considérablement enrichis, ainsi que les étiquettes sémantiques associées aux EN, ce qui est particulièrement utile pour l’adaptation de systèmes de détection d’EN à de nouveaux domaines.</resume>
			<mots_cles>Entités nommées, Analyse syntaxique robuste, Types sémantiques</mots_cles>
			<title></title>
			<abstract>In this paper we propose a new method that enables to correct and to associate new semantic types in the context of Named Entity (NE) Recognition Systems. After named entities (and more generally proper nouns) have been detected in texts, a semantic compatibility checking is performed. This checking can not only confirm or correct previous results of the NER system but also associate new NE types that have not been previously foreseen. This checking is performed using information associated to the NE by a robust syntactic analyzer and confronting this information to WordNet. After this checking is performed, final results of the NER system are better and new NE semantic tags are created. This second point is particularly useful when adapting existing NER systems to new domains.</abstract>
			<keywords>Named Entities, Robust Parsing, Semantic Types</keywords>
		</article>
		<article id="taln-2008-long-026" session="Entités Nommées">
			<auteurs>
				<auteur>
					<nom>Thomas Girault</nom>
					<email>thomas.girault@orange-ftgroup.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">France Télécom R&amp;D, 2, avenue Pierre Marzin 22307 Lannion Cedex</affiliation>
			</affiliations>
			<titre>Exploitation de treillis de Galois en désambiguïsation non supervisée d’entités nommées</titre>
			<type>long</type>
			<pages></pages>
			<resume>Nous présentons une méthode non supervisée de désambiguïsation d’entités nommées, basée sur l’exploitation des treillis de Galois. Nous réalisons une analyse de concepts formels à partir de relations entre des entités nommées et leurs contextes syntaxiques extraits d’un corpus d’apprentissage. Le treillis de Galois résultant fournit des concepts qui sont utilisés comme des étiquettes pour annoter les entités nommées et leurs contextes dans un corpus de test. Une évaluation en cascade montre qu’un système d’apprentissage supervisé améliore la classification des entités nommées lorsqu’il s’appuie sur l’annotation réalisée par notre système de désambiguïsation non supervisée.</resume>
			<mots_cles>Désambiguïsation non supervisée, treillis de Galois, entités nommées</mots_cles>
			<title></title>
			<abstract>We present an unsupervised method for named entities disambiguation, based on concept lattice mining.We perform a formal concept analysis from relations between named entities and their syntactic contexts observed in a training corpora. The resulting lattice produces concepts which are considered as labels for named entities and context annotation. Our approach is validated through a cascade evaluation which shows that supervised named entity classification is improved by using the annotation produced by our unsupervised disambiguation system.</abstract>
			<keywords>Unsupervised word sense disambiguation, concept lattice, named entities</keywords>
		</article>
		<article id="taln-2008-long-027" session="Entités Nommées">
			<auteurs>
				<auteur>
					<nom>Caroline Brun</nom>
					<email>Caroline.Brun@xrce.xerox.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Maud Ehrmann</nom>
					<email>Maud.Ehrmann@xrce.xerox.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Guillaume Jacquet</nom>
					<email>Guillaume.Jacquet@xrce.xerox.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Xerox Research Center Europe - XRCE, 6, Chemin de Maupertuis, 38240 Meylan</affiliation>
			</affiliations>
			<titre>Résolution de Métonymie des Entités Nommées : proposition d’une méthode hybride</titre>
			<type>long</type>
			<pages></pages>
			<resume>Dans cet article, nous décrivons la méthode que nous avons développée pour la résolution de métonymie des entités nommées dans le cadre de la compétition SemEval 2007. Afin de résoudre les métonymies sur les noms de lieux et noms d’organisation, tel que requis pour cette tâche, nous avons mis au point un système hybride basé sur l’utilisation d’un analyseur syntaxique robuste combiné avec une méthode d’analyse distributionnelle. Nous décrivons cette méthode ainsi que les résultats obtenus par le système dans le cadre de la compétition SemEval 2007.</resume>
			<mots_cles>Entités Nommées, métonymie, méthode hybride, analyse syntaxique robuste, approche distributionnelle</mots_cles>
			<title></title>
			<abstract>In this paper, we describe the method we develop in order to solve Named entity metonymy in the framework of the SemEval 2007 competition. In order to perform Named Entity metonymy resolution on location names and company names, as required for this task, we developed a hybrid system based on the use of a robust parser that extracts deep syntactic relations combined with a non supervised distributional approach, also relying on the relations extracted by the parser.We describe this methodology as well as the results obtained at SemEval 2007.</abstract>
			<keywords>Named Entities, metonymy, hybrid method, robust parsing, distributional approach</keywords>
		</article>
		<article id="taln-2008-long-028" session="Etiquetage et indexation">
			<auteurs>
				<auteur>
					<nom>Tatiana El-Khoury</nom>
					<email>tatiana.elkhoury@imag.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Laboratoire TIMC–IMAG - Université Joseph Fourier-Grenoble 1, BP 53-38041 Grenoble</affiliation>
			</affiliations>
			<titre>Etude de la corrélation entre morphosyntaxe et sémantique dans une perspective d’étiquetage automatique de textes médicaux arabes</titre>
			<type>long</type>
			<pages></pages>
			<resume>Cet article se propose d’étudier les relations sémantiques reliant base et expansion au sein des termes médicaux arabes de type « N+N », particulièrement ceux dont la base est un déverbal. En étudiant les relations sémantiques établies par une base déverbale, ce travail tente d’attirer l’attention sur l’interpénétration du sémantique et du morphosyntaxique ; il montre que, dans une large mesure, la structure morphosyntaxique de la base détermine l’éventail des possibilités relationnelles. La découverte de régularités dans le comportement de la base déverbale permet de prédire le type de relations que peut établir cette base avec son expansion pavant ainsi la voie à un traitement automatique et un travail d’étiquetage sémantique des textes médicaux arabes.</resume>
			<mots_cles>étiquetage automatique, terminologie médicale arabe, morphosyntaxe, sémantique</mots_cles>
			<title></title>
			<abstract>This paper examines the semantic relations existing in Arabic medical texts between the head and its extension in a two-noun compound, particularly when the head is a deverbal noun or a nominalization. By studying semantic relations encoded by nominalizations, this research work aims at underlining the correlation between morphosyntax and semantics notably the influence of the head noun structure on the set of semantic relations that can be established. The discovery of regularities in the functioning of the head noun allows thus to predict the type of relation that will be encoded. Such data are a pre-requisite for natural language processing and automatic part-to-speech tagging of medical Arabic texts.</abstract>
			<keywords>Part-of-speech tagging, Arabic medical terminology, morphosyntax, semantics</keywords>
		</article>
		<article id="taln-2008-long-029" session="Etiquetage et indexation">
			<auteurs>
				<auteur>
					<nom>Philippe Blache</nom>
					<email>philippe.blache@lpl-aix.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Stéphane Rauzy</nom>
					<email>stephane.rauzy@lpl-aix.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Laboratoire Parole et Langage, CNRS &amp; Université de Provence</affiliation>
			</affiliations>
			<titre>Influence de la qualité de l’étiquetage sur le chunking : une corrélation dépendant de la taille des chunks</titre>
			<type>long</type>
			<pages></pages>
			<resume>Nous montrons dans cet article qu’il existe une corrélation étroite existant entre la qualité de l’étiquetage morpho-syntaxique et les performances des chunkers. Cette corrélation devient linéaire lorsque la taille des chunks est limitée. Nous appuyons notre démonstration sur la base d’une expérimentation conduite suite à la campagne d’évaluation Passage 2007 (de la Clergerie et al., 2008). Nous analysons pour cela les comportements de deux analyseurs ayant participé à cette campagne. L’interprétation des résultats montre que la tâche de chunking, lorsqu’elle vise des chunks courts, peut être assimilée à une tâche de “super-étiquetage”.</resume>
			<mots_cles>Analyse syntaxique, étiquetage morphosyntaxique, analyseur stochastique, analyseur symbolique superficiel, chunker</mots_cles>
			<title></title>
			<abstract>We show in this paper that a strong correlation exists between the performance of chunk parsers and the quality of the tagging task in input. This dependency becomes linear when the size of the chunks is small. Our demonstration is based on an experiment conducted at the end of the Passage 2007 shared task evaluation initiative (de la Clergerie et al., 2008). The performance of two parsers which took part in this evaluation has been investigated. The results indicate that the chunking task, for sufficiently short chunks, is similar to a super-tagging task.</abstract>
			<keywords>Parsing, tagging, stochastic parser, symbolic shallow parser, chunker</keywords>
		</article>
		<article id="taln-2008-long-030" session="Etiquetage et indexation">
			<auteurs>
				<auteur>
					<nom>Aurélie Névéol</nom>
					<email>neveola@nlm.nih.gov</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Vincent Claveau</nom>
					<email>Vincent.Claveau@irisa.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">National Library of Medicine, 8600 Rockville Pike, Bethesda, MD 20894, USA</affiliation>
				<affiliation affiliationId="2">IRISA - CNRS, Campus de Beaulieu, 35042 Rennes cedex, France</affiliation>
			</affiliations>
			<titre>Apprentissage artificiel de règles d’indexation pour MEDLINE</titre>
			<type>long</type>
			<pages></pages>
			<resume>L’indexation est une composante importante de tout système de recherche d’information. Dans MEDLINE, la base documentaire de référence pour la littérature du domaine biomédical, le contenu des articles référencés est indexé à l’aide de descripteurs issus du thésaurus MeSH. Avec l’augmentation constante de publications à indexer pour maintenir la base à jour, le besoin d’outils automatiques se fait pressant pour les indexeurs. Dans cet article, nous décrivons l’utilisation et l’adaptation de la Programmation Logique Inductive (PLI) pour découvrir des règles d’indexation permettant de générer automatiquement des recommandations d’indexation pour MEDLINE. Les résultats obtenus par cette approche originale sont très satisfaisants comparés à ceux obtenus à l’aide de règles manuelles lorsque celles-ci existent. Ainsi, les jeux de règles obtenus par PLI devraient être prochainement intégrés au système produisant les recommandations d’indexation automatique pour MEDLINE.</resume>
			<mots_cles>Analyse et Indexation/méthodes, Medical Subject Headings, Apprentissage Artificiel, Programmation Logique Inductive</mots_cles>
			<title></title>
			<abstract>Indexing is a crucial step in any information retrieval system. In MEDLINE, a widely used database of the biomedical literature, the indexing process involves the selection of Medical Subject Headings in order to describe the subject matter of articles. The need for automatic tools to assist human indexers in this task is growing with the increasing amount of publications to be referenced in MEDLINE. In this paper, we describe the use and the customization of Inductive Logic Programming (ILP) to infer indexing rules that may be used to produce automatic indexing recommendations for MEDLINE indexers. Our results show that this original ILP-based approach overperforms manual rules when they exist. We expect the sets of ILP rules obtained in this experiment to be integrated in the system producing automatic indexing recommendations for MEDLINE.</abstract>
			<keywords>Abstracting and Indexing/methods, Medical Subject Headings, Machine Learning, Inductive Logic Programming</keywords>
		</article>
		<article id="taln-2008-court-001" session="Poster">
			<auteurs>
				<auteur>
					<nom>Yayoi Nakamura-Delloye</nom>
					<email>yayoi@free.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université Paris VII - Lattice (UMR 8094), 75013 Paris</affiliation>
				<affiliation affiliationId="2">Université Paris X - MoDyCo (UMR 7114), 92001 Nanterre Cedex</affiliation>
			</affiliations>
			<titre>Y a-t-il une véritable équivalence entre les propositions syntaxiques du français et du japonais ?</titre>
			<type>court</type>
			<pages></pages>
			<resume>La présente contribution part de nos constats réalisés à partir des résultats d’évaluation de notre système d’alignement des propositions de textes français-japonais. La présence importante de structures fondamentalement difficiles à aligner et les résultats peu satisfaisants de différentes méthodes de mise en correspondance des mots nous ont finalement amenés à remettre en cause l’existence même d’équivalence au niveau des propositions syntaxiques entre le français et le japonais. Afin de compenser les défauts que nous avons découverts, nous proposons des opérations permettant de restaurer l’équivalence des propositions alignées et d’améliorer la qualité des corpus alignés.</resume>
			<mots_cles>Alignement, proposition syntaxique, études contrastives français-japonais, similarité lexicale</mots_cles>
			<title></title>
			<abstract>This paper is based on our observations obtained from the results of our French-Japanese clause alignment system. Structures fundamentally difficult to align were so numerous and results obtained by various word-matching methods were so unsatisfactory that we questioned the existence of equivalence at the syntactic clause level between French and Japanese. In order to compensate the defect that we discovered, we propose some operations to restore aligned clause equivalence to improve the quality of aligned corpora.</abstract>
			<keywords>Alignment, syntactic clause, French-Japanese contrastive study, lexical similarty</keywords>
		</article>
		<article id="taln-2008-court-002" session="Poster">
			<auteurs>
				<auteur>
					<nom>Sylvain Schmitz</nom>
					<email>Sylvain.Schmitz@loria.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Joseph Le Roux</nom>
					<email>Joseph.LeRoux@loria.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LORIA, INRIA Nancy Grand Est, Nancy</affiliation>
				<affiliation affiliationId="2">LORIA, Université Nancy 2, Nancy</affiliation>
			</affiliations>
			<titre>Calculs d’unification sur les arbres de dérivation TAG</titre>
			<type>court</type>
			<pages></pages>
			<resume>Nous définissons un formalisme, les grammaires rationnelles d’arbres avec traits, et une traduction des grammaires d’arbres adjoints avec traits vers ce nouveau formalisme. Cette traduction préserve les structures de dérivation de la grammaire d’origine en tenant compte de l’unification de traits. La construction peut être appliquée aux réalisateurs de surface qui se fondent sur les arbres de dérivation.</resume>
			<mots_cles>Unification, grammaire d’arbres adjoints, arbre de dérivation, grammaire rationnelle d’arbres</mots_cles>
			<title></title>
			<abstract>The derivation trees of a tree adjoining grammar provide a first insight into the sentence semantics, and are thus prime targets for generation systems. We define a formalism, feature based regular tree grammars, and a translation from feature based tree adjoining grammars into this new formalism. The translation preserves the derivation structures of the original grammar, and accounts for feature unification.</abstract>
			<keywords>Unification, tree adjoining grammar, derivation tree, regular tree grammar</keywords>
		</article>
		<article id="taln-2008-court-003" session="Poster">
			<auteurs>
				<auteur>
					<nom>Alexandre Labadié</nom>
					<email>labadie@lirmm.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Violaine Prince</nom>
					<email>prince@lirmm.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIRMM, 161 rue ADA, 34392 Montpellier cedex</affiliation>
			</affiliations>
			<titre>Comparaison de méthodes lexicales et syntaxico-sémantiques dans la segmentation thématique de texte non supervisée</titre>
			<type>court</type>
			<pages></pages>
			<resume>Cet article présente une méthode basée sur des calculs de distance et une analyse sémantique et syntaxique pour la segmentation thématique de texte. Pour évaluer cette méthode nous la comparons à un un algorithme lexical très connu : c99. Nous testons les deux méthodes sur un corpus de discours politique français et comparons les résultats. Les deux conclusions qui ressortent de notre expérience sont que les approches sont complémentaires et que les protocoles d’évaluation actuels sont inadaptés.</resume>
			<mots_cles>Méthodes d’évaluation, segmentation de texte, segmentation thématique</mots_cles>
			<title></title>
			<abstract>This paper present a semantic and syntactic distance based method in topic text segmentation and compare it to a very well known text segmentation algorithm : c99. To do so we ran the two algorithms on a corpus of twenty two French political discourses and compared their results. Our two conclusions are that the two approaches are complementary and that evaluation methods in this domain should be revised.</abstract>
			<keywords>Evaluation methods, text segmentation, topic segmentation</keywords>
		</article>
		<article id="taln-2008-court-004" session="Poster">
			<auteurs>
				<auteur>
					<nom>Jérôme Lehuen</nom>
					<email>Jerome.Lehuen@lium.univ-lemans.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIUM - Université du Maine, Avenue Laënnec, 72085 Le Mans Cedex 9</affiliation>
			</affiliations>
			<titre>Un modèle de langage pour le DHM : la Grammaire Sémantique Réversible</titre>
			<type>court</type>
			<pages></pages>
			<resume>Cet article propose un modèle de langage dédié au dialogue homme-machine, ainsi que des algorithmes d’analyse et de génération. L’originalité de notre approche est de faire reposer l’analyse et la génération sur les mêmes connaissances, essentiellement sémantiques. Celles-ci sont structurées sous la forme d’une bibliothèque de concepts, et de formes d’usage associées aux concepts. Les algorithmes, quant à eux, sont fondés sur un double principe de correspondance entre des offres et des attentes, et d’un calcul heuristique de score.</resume>
			<mots_cles>Grammaire Sémantique, Réversibilité, Analyse, Génération, Dialogue</mots_cles>
			<title></title>
			<abstract>In this paper we present a language model for man-machine dialogue, as well as algorithms for analysis and text generation. The originality of our approach is to base analysis and generation on the same knowledge. These one is structured like a library of concepts and syntactic patterns. The algorithms are based on a principle of correspondence between offers and expectations, and calculation of a heuristic scoring.</abstract>
			<keywords>Semantic Grammar, Reversibility, Analysis, Generation, Dialogue</keywords>
		</article>
		<article id="taln-2008-court-005" session="Poster">
			<auteurs>
				<auteur>
					<nom>Maxime Amblard</nom>
					<email>maxime.amblard@orange-ftgroup.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Johannes Heinecke</nom>
					<email>johannes.heinecke@orange-ftgroup.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Estelle Maillebuau</nom>
					<email>estelle.maillebuau@orange-ftgroup.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Orange Labs, 2 av. Pierre Marzin, 22307 Lannion cedex</affiliation>
			</affiliations>
			<titre>Discourse Representation Theory et graphes sémantiques : formalisation sémantique en contexte industriel</titre>
			<type>court</type>
			<pages></pages>
			<resume>Ces travaux présentent une extension des représentations formelles pour la sémantique, de l’outil de traitement automatique des langues de Orange Labs1. Nous abordons ici uniquement des questions relatives à la construction des représentations sémantiques, dans le cadre de l’analyse linguistique. Afin d’obtenir des représentations plus fines de la structure argumentale des énoncés, nous incluons des concepts issus de la DRT dans le système de représentation basé sur les graphes sémantiques afin de rendre compte de la notion de portée.</resume>
			<mots_cles>Modèle sémantique, analyse syntaxique en dépendance, DRT</mots_cles>
			<title></title>
			<abstract>This works present an extension of the formal representation of semantic for the natural language processing tool developped at Orange Labs. We cover here only issues relating to build the semantic representations, based on linguistic parsing. To obtain more detailed representations of the argumental structure statements, we include insights from the DRT in the system of representation based on the semantic graphs to give an account of scope.</abstract>
			<keywords>Semantic modelling, parsing, DRT</keywords>
		</article>
		<article id="taln-2008-court-006" session="Poster">
			<auteurs>
				<auteur>
					<nom>Karën Fort</nom>
					<email>Karen.Fort@loria.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Bruno Guillaume</nom>
					<email>Bruno.Guillaume@loria.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LORIA / INRIA Nancy Grand-Est</affiliation>
			</affiliations>
			<titre>Sylva : plate-forme de validation multi-niveaux de lexiques</titre>
			<type>court</type>
			<pages></pages>
			<resume>La production de lexiques est une activité indispensable mais complexe, qui nécessite, quelle que soit la méthode de création utilisée (acquisition automatique ou manuelle), une validation humaine. Nous proposons dans ce but une plate-forme Web librement disponible, appelée Sylva (Systematic lexicon validator). Cette plate-forme a pour caractéristiques principales de permettre une validation multi-niveaux (par des validateurs, puis un expert) et une traçabilité de la ressource. La tâche de l’expert(e) linguiste en est allégée puisqu’il ne lui reste à considérer que les données sur lesquelles il n’y a pas d’accord inter-validateurs.</resume>
			<mots_cles>Lexiques, plate-forme de validation, cadres de sous-catégorisation</mots_cles>
			<title></title>
			<abstract>Lexicon production is essential but complex and all creation methods (automatic acquisition or manual creation) require human validation. For this purpose, we propose a freely available Web-based framework, named Sylva (Systematic lexicon validator). The main point of our framework is that it handles multi-level validations and keeps track of the resource’s history. The expert linguist task is made easier : (s)he has only to consider data on which validators disagree.</abstract>
			<keywords>Lexicons, validation framework, subcategorization frames</keywords>
		</article>
		<article id="taln-2008-court-007" session="Poster">
			<auteurs>
				<auteur>
					<nom>Rémy Kessler</nom>
					<email>remy.kessler@univ-avignon.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Juan-Manuel Torres-Moreno</nom>
					<email>juan-manuel.torres@univ-avignon.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Marc El-Bèze</nom>
					<email>marc.elbeze@univ-avignon.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIA / Université d’Avignon, 339 chemin des Meinajariès, 84911 Avignon</affiliation>
				<affiliation affiliationId="2">AKTOR 12, allée Irène Joliot Curie 69800 Saint Priest</affiliation>
			</affiliations>
			<titre>E-Gen : Profilage automatique de candidatures</titre>
			<type>court</type>
			<pages></pages>
			<resume>La croissance exponentielle de l’Internet a permis le développement de sites d’offres d’emploi en ligne. Le système E-Gen (Traitement automatique d’offres d’emploi) a pour but de permettre l’analyse et la catégorisation d’offres d’emploi ainsi qu’une analyse et classification des réponses des candidats (Lettre de motivation et CV). Nous présentons les travaux réalisés afin de résoudre la seconde partie : on utilise une représentation vectorielle de texte pour effectuer une classification des pièces jointes contenus dans le mail à l’aide de SVM. Par la suite, une évaluation de la candidature est effectuée à l’aide de différents classifieurs (SVM et n-grammes de mots).</resume>
			<mots_cles>Classification de textes, Modèle probabiliste, Ressources humaines, Offres d’emploi</mots_cles>
			<title></title>
			<abstract>The exponential growth of the Internet has allowed the development of a market of on-line job search sites. This paper presents the E-Gen system (Automatic Job Offer Processing system for Human Resources). E-Gen will perform two complex tasks : an analysis and categorisation of job postings, which are unstructured text documents, an analysis and a relevance ranking of the candidate answers (cover letter and curriculum vitae). Here we present the work related to the second task : we use vectorial representation before generating a classification with SVM to determine the type of the attachment. In the next step, we try to classify the candidate answers with different classifiers (SVM and ngrams of words).</abstract>
			<keywords>Text Classification, Probabilistic Model, Human Ressources, Job Offer</keywords>
		</article>
		<article id="taln-2008-court-008" session="Poster">
			<auteurs>
				<auteur>
					<nom>François Barthélemy</nom>
					<email>barthe@cnam.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">CNAM, Cédric, 292 rue Saint-Martin, 75003 Paris</affiliation>
				<affiliation affiliationId="2">INRIA, Alpage, 78153 Le Chesnay cedex</affiliation>
			</affiliations>
			<titre>Typage, produit cartésien et unités d’analyse pour les modèles à états finis</titre>
			<type>court</type>
			<pages></pages>
			<resume>Dans cet article, nous présentons un nouveau langage permettant d’écrire des relations rationnelles compilées en automates finis. Les deux caractéristiques innovantes de ce langage sont de pourvoir décrire des relations à plusieurs niveaux, pas nécessairement deux et d’utiliser diverses unités d’analyse pour exprimer les liens entre niveaux. Cela permet d’aligner de façon fine des représentations multiples.</resume>
			<mots_cles>Machine finie à états, morphologie à deux niveau</mots_cles>
			<title></title>
			<abstract>In this paper, we present a new language to write rational relations compiled into finite state automata. There are two main novelties in the language. Firstly, the descriptions may have more than two levels. Secondly, various units may be used to express the relationships between the levels. Using these features, it is possible to align finely multiple representations.</abstract>
			<keywords>Finite-state machine, two-level morphology</keywords>
		</article>
		<article id="taln-2008-court-009" session="Poster">
			<auteurs>
				<auteur>
					<nom>Frédéric Landragin</nom>
					<email>frederic.landragin@linguist.jussieu.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">CNRS – Laboratoire LaTTICe (UMR 8094), 1 rue Maurice Arnoux – 92120 Montrouge</affiliation>
			</affiliations>
			<titre>Vers l’évaluation de systèmes de dialogue homme-machine : de l’oral au multimodal</titre>
			<type>court</type>
			<pages></pages>
			<resume>L’évaluation pour le dialogue homme-machine ne se caractérise pas par l’efficacité, l’objectivité et le consensus que l’on observe dans d’autres domaines du traitement automatique des langues. Les systèmes de dialogue oraux et multimodaux restent cantonnés à des domaines applicatifs restreints, ce qui rend difficiles les évaluations comparatives ou normées. De plus, les avancées technologiques constantes rendent vite obsolètes les paradigmes d’évaluation et ont pour conséquence une multiplication de ceux-ci. Des solutions restent ainsi à trouver pour améliorer les méthodes existantes et permettre des diagnostics plus automatisés des systèmes. Cet article se veut un ensemble de réflexions autour de l’évaluation de la multimodalité dans les systèmes à forte composante linguistique. Des extensions des paradigmes existants sont proposées, en particulier DQR/DCR, sachant que certains sont mieux adaptés que d’autres au dialogue multimodal. Des conclusions et perspectives sont tirées sur l’avenir de l’évaluation pour le dialogue homme-machine.</resume>
			<mots_cles>Dialogue finalisé, multimodalité, évaluation pour le dialogue hommemachine, paradigme d’évaluation, test utilisateur, diagnostic, paraphrase multimodale</mots_cles>
			<title></title>
			<abstract>Evaluating human-machine dialogue systems is not so efficient, objective, and consensual than evaluating other natural language processing systems. Oral and multimodal dialogue systems are still working within reduced applicative domains. Comparative and normative evaluations are then difficult. Moreover, the continuous technological progress makes obsolete and numerous the evaluating paradigms. Some solutions are still to be identified to improve existing methods and to allow a more automatic diagnosis of systems. The aim of this paper is to provide a set of remarks dealing with the evaluation of multimodal spoken language dialogue systems. Some extensions of existing paradigms are presented, in particular DQR/DCR, considering that some paradigms fit better multimodal issues than others. Some conclusions and perspectives are then drawn on the future of the evaluation of human-machine dialogue systems.</abstract>
			<keywords>Task-driven dialogue, multimodality, evaluating human-machine dialogue, evaluation paradigm, user test, diagnosis</keywords>
		</article>
		<article id="taln-2008-court-010" session="Poster">
			<auteurs>
				<auteur>
					<nom>Nuria Gala</nom>
					<email>nuria.gala@univ-provence.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Véronique Rey</nom>
					<email>veronique.rey@univ-provence.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIF – CNRS, Aix – Marseille Univ., 163 av. de Luminy, 13288 Marseille</affiliation>
				<affiliation affiliationId="2">SHADYC – CNRS, Aix – Marseille Univ., 29 av. R. Schuman, 13100 Aix</affiliation>
			</affiliations>
			<titre>POLYMOTS : une base de données de constructions dérivationnelles en français à partir de radicaux phonologiques</titre>
			<type>court</type>
			<pages></pages>
			<resume>Cet article présente POLYMOTS, une base de données lexicale contenant huit mille mots communs en français. L’originalité de l’approche proposée tient à l'analyse des mots. En effet, à la différence d’autres bases lexicales représentant la morphologie dérivationnelle des mots à partir d’affixes, ici l’idée a été d’isoler un radical commun à un ensemble de mots d’une même famille. Nous avons donc analysé les formes des mots et, par comparaison phonologique (forme phonique comparable) et morphologique (continuité de sens), nous avons regroupé les mots par familles, selon le type de radical phonologique. L’article présente les fonctionnalités de la base et inclut une discussion sur les applications et les perspectives d’une telle ressource.</resume>
			<mots_cles>ressource lexicale, morphologie dérivationnelle, traitement automatique des familles de mots</mots_cles>
			<title></title>
			<abstract>In this paper we present POLYMOTS, a lexical database containing eight thousand common nouns in French. Whereas most of the existing lexicons for derivational morphology take affixes as starting point for producing paradigms of words, we defend here the idea that it is possible to isolate a morpho-phonological stem and produce a paradigm of words belonging to the same family. This point leads us to describe three types of stems according to their phonological and morphological form. The article presents the different features of such a lexical database and discusses the applications and future work using and enriching this resource.</abstract>
			<keywords>lexical resource, derivational morphology, word families processing</keywords>
		</article>
		<article id="taln-2008-court-011" session="Poster">
			<auteurs>
				<auteur>
					<nom>Bruno Cartoni</nom>
					<email>bruno.cartoni@eti.unige.ch</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">ISSCO/TIM-ETI – Université de Genève, 40 bd du Pont d’Arve, CH-1205 Genève</affiliation>
			</affiliations>
			<titre>Mesure de l’alternance entre préfixes pour la génération en traduction automatique</titre>
			<type>court</type>
			<pages></pages>
			<resume>La génération de néologismes construits pose des problèmes dans un système de traduction automatique, notamment au moment de la sélection du préfixe dans les formations préfixées, quand certains préfixes paraissent pouvoir alterner. Nous proposons une étude « extensive », qui vise à rechercher dans de larges ressources textuelles (l’Internet) des formes préfixées générées automatiquement, dans le but d’individualiser les paramètres qui favorisent l’un des préfixes ou qui, au contraire, permettent cette alternance. La volatilité de cette ressource textuelle nécessite certaines précautions dans la méthodologie de décompte des données extraites.</resume>
			<mots_cles>morphologie, traduction automatique, génération, néologisme, études empiriques</mots_cles>
			<title></title>
			<abstract>Generating constructed neologisms in a machine translation system is confronted to the issue of selecting the right affixes, especially when some affixes can be used alternately. We propose here an “extensive” study that looks into large textual data collections (web) for prefixed forms that have been automatically generated, in order to find out parameters that allow the use of both prefixes or, on the contrary, that prevent one or the other prefixation. The volatility of web resources requires methodological precautions, especially in data counting.</abstract>
			<keywords>morphology, machine translation, generation, neologism, empirical studies</keywords>
		</article>
		<article id="taln-2008-court-012" session="Poster">
			<auteurs>
				<auteur>
					<nom>Abdenour Mokrane</nom>
					<email>Abdenour.Mokrane@univ-tours.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Nathalie Friburger</nom>
					<email>Nathalie.Friburger@univ-tours.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Jean-Yves Antoine</nom>
					<email>Jean-Yves.Antoine@univ-tours.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université François Rabelais Tours – LI, IUP Blois, France</affiliation>
			</affiliations>
			<titre>Cascades de transducteurs pour le chunking de la parole conversationnelle : l’utilisation de la plateforme CasSys dans le projet EPAC</titre>
			<type>court</type>
			<pages></pages>
			<resume>Cet article présente l’utilisation de la plate-forme CasSys pour la segmentation de la parole conversationnelle (chunking) à l’aide de cascades de transducteurs Unitex. Le système que nous présentons est utilisé dans le cadre du projet ANR EPAC. Ce projet a pour objectif l’indexation et l’annotation automatique de grands flux de parole issus d’émissions télévisées ou radiophoniques. Cet article présente tout d’abord l’adaptation à ce type de données d’un système antérieur de chunking (Romus) qui avait été développé pour le dialogue oral homme-machine. Il décrit ensuite les principaux problèmes qui se posent à l’analyse : traitement des disfluences de l’oral spontané, mais également gestion des erreurs dues aux étapes antérieures de reconnaissance de la parole et d’étiquetage morphosyntaxique.</resume>
			<mots_cles>Traitement Automatique du Langage Parlé (TALP), segmentation, chunks, parole conversationnelle, transducteurs, Unitex</mots_cles>
			<title></title>
			<abstract>This paper describes the use of the CasSys platform in order to achieve the chunking of conversational speech transcripts by means of cascades of Unitex transducers. Our system is involved in the EPAC project of the French National Agency of Research (ANR). The aim of this project is to develop robust methods for the annotation of audio/multimedia document collections which contains conversational speech sequences such as TV or radio programs. At first, this paper presents the adaptation of a former chunking system (Romus) which was developed in the restricted framework of dedicated spoken manmachine dialogue. Then, it describes the problems that are arising due to 1) spontaneous speech disfluencies and 2) errors for the previous stages of processing (automatic speech recognition and POS tagging).</abstract>
			<keywords>Spoken Language Processing, chunking, conversational speech, transducers, Unitex</keywords>
		</article>
		<article id="taln-2008-court-013" session="Poster">
			<auteurs>
				<auteur>
					<nom>Aurélien Bossard</nom>
					<email>Aurelien.Bossard@lipn.univ-paris13.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Thierry Poibeau</nom>
					<email>Thierry.Poibeau@lipn.univ-paris13.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIPN - UMR 7030, CNRS - Université Paris 13, F-93430 Villetaneuse, France</affiliation>
			</affiliations>
			<titre>Regroupement automatique de documents en classes événementielles</titre>
			<type>court</type>
			<pages></pages>
			<resume>Cet article porte sur le regroupement automatique de documents sur une base événementielle. Après avoir précisé la notion d’événement, nous nous intéressons à la représentation des documents d’un corpus de dépêches, puis à une approche d’apprentissage pour réaliser les regroupements de manière non supervisée fondée sur k-means. Enfin, nous évaluons le système de regroupement de documents sur un corpus de taille réduite et nous discutons de l’évaluation quantitative de ce type de tâche.</resume>
			<mots_cles>Regroupement de documents, Suivi d’événement</mots_cles>
			<title></title>
			<abstract>This paper analyses the problem of automatic document clustering based on events. We first specify the notion of event. Then, we detail the document modelling method and the learning approach for document clustering based on k-means. We finally evaluate our document clustering system on a small corpus and discuss the quantitative evaluation for this kind of task.</abstract>
			<keywords>Document clustering, Event tracking</keywords>
		</article>
		<article id="taln-2008-court-014" session="Poster">
			<auteurs>
				<auteur>
					<nom>Mary Hearne</nom>
					<email>mhearne@computing.dcu.ie</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Sylwia Ozdowska</nom>
					<email>sozdowska@computing.dcu.ie</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>John Tinsley</nom>
					<email>jtinsley@computing.dcu.ie</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">National Centre for Language Technology, Dublin City University, Glasnevin, Dublin 9, Ireland</affiliation>
			</affiliations>
			<titre></titre>
			<type>court</type>
			<pages></pages>
			<resume>Nous évaluons le recours à des techniques de traduction à base de segments syntaxiquement motivés, seules ou en combinaison avec des techniques à base de segments non motivés, et nous comparons les apports respectifs de l’analyse en constituants et de l’analyse en dépendances dans ce cadre. À partir d’un corpus parallèle Anglais–Français, nous construisons automatiquement deux corpus d’entraînement arborés, en constituants et en dépendances, alignés au niveau sous-phrastique et en extrayons des correspondances bilingues entre mots et syntagmes motivées syntaxiquement. Nous mesurons automatiquement la qualité de la traduction obtenue par un système à base de segments. Les résultats montrent que la combinaison des correspondances bilingues non motivées et motivées sur le plan syntaxique améliore la qualité de la traduction quel que soit le type d’analyse considéré. Par ailleurs, le gain en qualité est plus important avec le recours à l’analyse en dépendances au regard des constituants.</resume>
			<mots_cles>Traduction statistique à base de segments, annotation en constituants, annotation en dépendances, corpus parallèles arborés alignés au niveau sousphrastique</mots_cles>
			<title>Comparing Constituency and Dependency Representations for SMT Phrase-Extraction</title>
			<abstract>We consider the value of replacing and/or combining string-basedmethods with syntax-based methods for phrase-based statistical machine translation (PBSMT), and we also consider the relative merits of using constituency-annotated vs. dependency-annotated training data. We automatically derive two subtree-aligned treebanks, dependency-based and constituency-based, from a parallel English–French corpus and extract syntactically motivated word- and phrase-pairs. We automatically measure PB-SMT quality. The results show that combining string-based and syntax-based word- and phrase-pairs can improve translation quality irrespective of the type of syntactic annotation. Furthermore, using dependency annotation yields greater translation quality than constituency annotation for PB-SMT.</abstract>
			<keywords>PB-SMT, constituency annotation, dependency annotation, subtreealigned parallel treebanks</keywords>
		</article>
		<article id="taln-2008-court-015" session="Poster">
			<auteurs>
				<auteur>
					<nom>Fabien Poulard</nom>
					<email>fabien.poulard@univ-nantes.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Thierry Waszak</nom>
					<email>thierry.waszak@univ-avignon.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Nicolas Hernandez</nom>
					<email>nicolas.hernandez@univ-nantes.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Patrice Bellot</nom>
					<email>patrice.bellot@univ-avignon.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LINA UMR 6241 / Université de Nantes</affiliation>
				<affiliation affiliationId="2">LIA / Université d’Avignon</affiliation>
			</affiliations>
			<titre>Repérage de citations, classification des styles de discours rapporté et identification des constituants citationnels en écrits journalistiques</titre>
			<type>court</type>
			<pages></pages>
			<resume>Dans le contexte de la recherche de plagiat, le repérage de citations et de ses constituants est primordial puisqu’il peut amener à évaluer le caractère licite ou illicite d’une reprise (source citée ou non). Nous proposons ici une comparaison de méthodes automatiques pour le repérage de ces informations et rapportons une évaluation quantitative de celles-ci. Un corpus d’écrits journalistiques français a été manuellement annoté pour nous servir de base d’apprentissage et de test.</resume>
			<mots_cles>détection de citations, classification des styles de discours rapporté, identification du locuteur, techniques par apprentissage et base de règles, écrits journalistiques</mots_cles>
			<title></title>
			<abstract>In the application context of reported content, that includes plagiarism and impact of textual information searched, citations finding and its fundamentals is essential as it may help estimating legal value of a citation (with or without specifying original source). We propose here a comparison between automatic methods for finding up those elements and we quantitatively evaluate them. A French journalistic corpus has been manually annotated to be used as learning base and for testing.</abstract>
			<keywords>detection of citations, reported speech style classification, source identification, machine learning and rules-based techniques, news corpus</keywords>
		</article>
		<article id="taln-2008-court-016" session="Poster">
			<auteurs>
				<auteur>
					<nom>Frédéric Landragin</nom>
					<email>frederic.landragin@linguist.jussieu.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">CNRS – Laboratoire LaTTICe (UMR 8094), 1 rue Maurice Arnoux – 92120 Montrouge</affiliation>
			</affiliations>
			<titre>Vers l’identification et le traitement des actes de dialogue composites</titre>
			<type>court</type>
			<pages></pages>
			<resume>Il peut être difficile d’attribuer une seule valeur illocutoire à un énoncé dans un dialogue. En premier lieu, un énoncé peut comporter plusieurs segments de discours ayant chacun leur valeur illocutoire spécifique. De plus, un seul segment peut s’analyser en tant qu’acte de langage composite, regroupant par exemple la formulation d’une question et l’émission simultanée d’une information. Enfin, la structure du dialogue en termes d’échanges et de séquences peut être déterminante dans l’identification de l’acte, et peut également apporter une valeur illocutoire supplémentaire, comme celle de clore la séquence en cours. Dans le but de déterminer la réaction face à un tel acte de dialogue composite, nous présentons une approche théorique pour l’analyse des actes de dialogue en fonction du contexte de tâche et des connaissances des interlocuteurs. Nous illustrons sur un exemple nos choix de segmentation et d’identification des actes composites, et nous présentons les grandes lignes d’une stratégie pour déterminer la réaction qui semble être la plus pertinente.</resume>
			<mots_cles>Actes de langage complexes, structure du dialogue, terrain commun</mots_cles>
			<title></title>
			<abstract>Attributing one illocutionary value to a utterance in a dialogue can be difficult. First, a utterance can include several discourse segments, each one with a specific illocutionary value. Moreover, one discourse segment can be linked to a complex speech act that groups for instance a question together with the assertion of new information. Finally, the dialogue structure with the various exchanges and sequences can be decisive when identifying the dialogue act, and can also bring an additional illocutionary value, for instance the one consisting of closing the current sequence. With the aim to determine how to react to such a composite dialogue act, we present a theoretical approach to dialogue act analysis considering the task context and the dialogue participants’ knowledge. We illustrate our choices in terms of segmentation and identification of composite acts, and we present the main features of a strategy for determining the most relevant reaction.</abstract>
			<keywords>Complex speech acts, dialogue structure, common ground</keywords>
		</article>
		<article id="taln-2008-court-017" session="Poster">
			<auteurs>
				<auteur>
					<nom>Manal El Zant</nom>
					<email>el.zant@medecine.univ-mrs.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Jean Royauté</nom>
					<email>jean.royaute@lif.univ-mrs.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Michel Roux</nom>
					<email>michel.roux@medecine.univ-mrs.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIF / Université de la Méditerranée, 27 Bd Jean Moulin, 13005 Marseille</affiliation>
			</affiliations>
			<titre>Représentation évènementielle des déplacements dans des dépêches épidémiologiques</titre>
			<type>court</type>
			<pages></pages>
			<resume>La représentation évènementielle des déplacements de personnes dans des dépêches épidémiologiques est d’une grande importance pour une compréhension détaillée du sens de ces dépêches. La dissémination des composants d’une telle représentation dans les dépêches rend difficile l’accès à leurs contenus. Ce papier décrit un système d’extraction d’information utilisant des cascades de transducteurs à nombre d’états fini qui ont permis la réalisation de trois tâches : la reconnaissance des entités nommées, l’annotation et la représentation des composants ainsi que la représentation des structures évènementielles. Nous avons obtenu une moyenne de rappel de 80, 93% pour la reconnaissance des entités nommées et de 97, 88% pour la représentation des composants. Ensuite, nous avons effectué un travail de normalisation de cette représentation par la résolution de certaines anaphores pronominales. Nous avons obtenu une valeur moyenne de précision de 81, 72% pour cette résolution.</resume>
			<mots_cles>Sous-langage, représentation évènementielle, extraction d’information, structure prédicative, structure predicate-arguments</mots_cles>
			<title></title>
			<abstract>The representation of motion events is important for an automatic comprehension of disease outbreak reports. The dispersion of components in this type of reports makes it difficult to have such a representation. This paper describes an automatic extraction of event structures representation of these texts.We built an information extraction system by using cascaded finite state transducers which allowed the realization of three tasks : the named entity recognition, the component annotation and representation and the event structure representation. We obtained a recall of 80, 93% for the named entity recognition task and a recall of 97, 88% for argument representation task. Thereafter, we worked in anaphoric pronouns resolution where we obtained a precision of 81.83%.</abstract>
			<keywords>Sublanguage, event structure representation, information extraction, predicative structure, predicate-arguments structure</keywords>
		</article>
		<article id="taln-2008-court-018" session="Poster">
			<auteurs>
				<auteur>
					<nom>Éric Wehrli</nom>
					<email>Eric.Wehrli@lettres.unige.ch</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Luka Nerima</nom>
					<email>Luka.Nerima@lettres.unige.ch</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LATL-Département de linguistique, Université de Genève</affiliation>
			</affiliations>
			<titre>Traduction multilingue : le projet MulTra</titre>
			<type>court</type>
			<pages></pages>
			<resume>L’augmentation rapide des échanges et des communications pluriculturels, en particulier sur internet, intensifie les besoins d’outils multilingues y compris de traduction. Cet article décrit un projet en cours au LATL pour le développement d’un système de traduction multilingue basé sur un modèle linguistique abstrait et largement générique, ainsi que sur un modèle logiciel basé sur la notion d’objet. Les langues envisagées dans la première phase de ce projet sont l’allemand, le français, l’italien, l’espagnol et l’anglais.</resume>
			<mots_cles>Traduction automatique multilingue, approche par objets, génération de lexiques bilingues</mots_cles>
			<title></title>
			<abstract>The increase of cross-cultural communication triggered notably by the Internet intensifies the needs for multilingual linguistic tools, in particular translation systems for several languages. The LATL has developed an efficient multilingual parsing technology based on an abstract and generic linguistic model and on object-oriented software design. The proposed project intends to apply a similar approach to the problem of multilingual translation (German, French, Italian and English).</abstract>
			<keywords>Multilingual machine translation, object design, bilingual dictionary derivation</keywords>
		</article>
		<article id="taln-2008-court-019" session="Poster">
			<auteurs>
				<auteur>
					<nom>Erwan Moreau</nom>
					<email>emoreau@enst.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>François Yvon</nom>
					<email>yvon@limsi.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Olivier Cappé</nom>
					<email>cappe@enst.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Institut Télécom ParisTech &amp; LTCI CNRS</affiliation>
				<affiliation affiliationId="2">Univ. Paris Sud &amp; LIMSI CNRS</affiliation>
			</affiliations>
			<titre>Appariement d’entités nommées coréférentes : combinaisons de mesures de similarité par apprentissage supervisé</titre>
			<type>court</type>
			<pages></pages>
			<resume>L’appariement d’entités nommées consiste à regrouper les différentes formes sous lesquelles apparaît une entité. Pour cela, des mesures de similarité textuelle sont généralement utilisées. Nous proposons de combiner plusieurs mesures afin d’améliorer les performances de la tâche d’appariement. À l’aide d’expériences menées sur deux corpus, nous montrons la pertinence de l’apprentissage supervisé dans ce but, particulièrement avec l’algorithme C4.5.</resume>
			<mots_cles>Entités nommées, Appariement, Mesures de similarité textuelle, Apprentissage supervisé</mots_cles>
			<title></title>
			<abstract>Matching named entities consists in grouping the different forms under which an entity may occur. Textual similarity measures are the usual tools for this task. We propose to combine several measures in order to improve the performance. We show the relevance of supervised learning in this objective through experiences with two corpora, especially in the case of the C4.5 algorithm.</abstract>
			<keywords>Named entities,Matching, Textual similaritymeasures, Supervised learning</keywords>
		</article>
		<article id="taln-2008-court-020" session="Poster">
			<auteurs>
				<auteur>
					<nom>Renaud Marlet</nom>
					<email>renaud.marlet@inria.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LaBRI / INRIA Bordeaux – Sud-Ouest</affiliation>
			</affiliations>
			<titre>Un sens logique pour les graphes sémantiques</titre>
			<type>court</type>
			<pages></pages>
			<resume>Nous discutons du sens des graphes sémantiques, notamment de ceux utilisés en Théorie Sens-Texte. Nous leur donnons un sens précis, éventuellement sous-spécifié, grâce à une traduction simple vers une formule de Minimal Recursion Semantics qui couvre les cas de prédications multiples sur plusieurs entités, de prédication d’ordre supérieur et de modalités.</resume>
			<mots_cles>Graphe sémantique, logique, quantification, Théorie Sens-Texte (TST)</mots_cles>
			<title></title>
			<abstract>We discuss the meaning of semantic graphs, in particular of those used in Meaning-Text Theory. We provide a precise, possibly underspecified, meaning to such graphs through a simple translation into a Minimal Recursion Semantics formula. This translation covers cases of multiple predications over several entities, higher order predication and modalities.</abstract>
			<keywords>Semantic graph, logic, quantification, Meaning-Text Theory (MTT)</keywords>
		</article>
		<article id="taln-2008-court-021" session="Poster">
			<auteurs>
				<auteur>
					<nom>Marie-Jean Meurs</nom>
					<email>marie-jean.meurs@univ-avignon.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Frédéric Duvert</nom>
					<email>frederic.duvert@univ-avignon.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Frédéric Béchet</nom>
					<email>frederic.bechet@univ-avignon.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Fabrice Lefèvre</nom>
					<email>fabrice.lefevre@univ-avignon.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Renato De Mori</nom>
					<email>renato.demori@univ-avignon.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université d’Avignon et des Pays de Vaucluse, Laboratoire Informatique d’Avignon (EA 931), F-84911 Avignon, France.</affiliation>
			</affiliations>
			<titre>Annotation en Frames Sémantiques du corpus de dialogue MEDIA</titre>
			<type>court</type>
			<pages></pages>
			<resume>Cet article présente un formalisme de représentation des connaissances qui a été utilisé pour fournir des annotations sémantiques de haut niveau pour le corpus de dialogue oral MEDIA. Ces annotations en structures sémantiques, basées sur le paradigme FrameNet, sont obtenues de manière incrémentale et partiellement automatisée. Nous décrivons le processus d’interprétation automatique qui permet d’obtenir des compositions sémantiques et de générer des hypothèses de frames par inférence. Le corpus MEDIA est un corpus de dialogues en langue française dont les tours de parole de l’utilisateur ont été manuellement transcrits et annotés (niveaux mots et constituants sémantiques de base). Le processus proposé utilise ces niveaux pour produire une annotation de haut niveau en frames sémantiques. La base de connaissances développée (définitions des frames et règles de composition) est présentée, ainsi que les résultats de l’annotation automatique.</resume>
			<mots_cles>compréhension automatique de la parole, système de dialogue oral, frames sémantiques, décodage conceptuel, annotation sémantique, inférence sémantique</mots_cles>
			<title></title>
			<abstract>This paper introduces a knowledge representation formalism, used for incremental and partially automated annotation of the French MEDIA dialogue corpus in terms of semantic structures. We describe an automatic interpretation process for composing semantic structures from basic semantic constituents using patterns involving constituents and words. The process has procedures for obtaining semantic compositions and generating frame hypotheses by inference. This process is applied to MEDIA, a dialogue corpus manually annotated at the word and semantic constituent levels, and thus produces a higher level semantic frame annotation. The Knowledge Source defined and the results obtained on the automatically-derived annotation are reported.</abstract>
			<keywords>spoken language understanding, spoken dialogue system, semantic structures, semantic frames, conceptual decoding, semantic annotation, semantic inference</keywords>
		</article>
		<article id="taln-2008-court-022" session="Poster">
			<auteurs>
				<auteur>
					<nom>Ramzi Abbès</nom>
					<email>ramzi.abbes@univ-lyon2.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Malek Boualem</nom>
					<email>malek.boualem@orange-ftgroup.com</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">ICAR-CNRS/ Lyon 2</affiliation>
				<affiliation affiliationId="2">Orange Labs – France Telecom R&amp;D</affiliation>
			</affiliations>
			<titre>Dissymétrie entre l'indexation des documents et le traitement des requêtes pour la recherche d’information en langue arabe</titre>
			<type>court</type>
			<pages></pages>
			<resume>Les moteurs de recherches sur le web produisent des résultats comparables et assez satisfaisants pour la recherche de documents écrits en caractères latins. Cependant, ils présentent de sérieuses lacunes dès que l'ont s'intéresse à des langues peu dotées ou des langues sémitiques comme l'arabe. Dans cet article nous présentons une étude analytique et qualitative de la recherche d’information en langue arabe en mettant l'accent sur l'insuffisance des outils de recherche actuels, souvent mal adaptés aux spécificités de la langue arabe. Pour argumenter notre analyse, nous présentons des résultats issus d’observations et de tests autour de certains phénomènes linguistiques de l’arabe écrit. Pour la validation des ces observations, nous avons testé essentiellement le moteur de recherche Google.</resume>
			<mots_cles>recherche d’information, langue arabe, indexation, lemmatisation, Google</mots_cles>
			<title></title>
			<abstract>Web search engines provide quite good results for Latin characters-based languages. However, they still show many weaknesses when searching in other languages such as Arabic. This paper discusses a qualitative analysis of information retrieval in Arabic language, highlighting some of the numerous limitations of available search engines, mainly when they are not properly adapted to the Arabic language specificities. To argue our analysis, we present some results based on quite sufficient observations and tests on various Arabic linguistic phenomena. To validate these observations, we essentially have tested the Google search engine.</abstract>
			<keywords>information retrieval, Arabic, indexation, lemmatization, Google</keywords>
		</article>
	</articles>
</conference>