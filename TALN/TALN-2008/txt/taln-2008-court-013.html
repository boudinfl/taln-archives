<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
"http://www.w3.org/TR/html4/loose.dtd">
<html><head><title>Regroupement automatique de documents en classes &#233;v&#233;nementielles</title>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
</head>
<body>
<div style="page-break-before:always; page-break-after:always"><div><p>TALN 2008, Avignon, 9&#8211;13 juin 2008
</p>
<p>Regroupement automatique de documents en classes
&#233;v&#233;nementielles
</p>
<p>Aur&#233;lien Bossard Thierry Poibeau
LIPN - UMR 7030
</p>
<p>CNRS - Universit&#233; Paris 13
F-93430 Villetaneuse, France
</p>
<p>{prenom.nom}@lipn.univ-paris13.fr
</p>
<p>R&#233;sum&#233;. Cet article porte sur le regroupement automatique de documents sur une base
&#233;v&#233;nementielle. Apr&#232;s avoir pr&#233;cis&#233; la notion d&#8217;&#233;v&#233;nement, nous nous int&#233;ressons &#224; la repr&#233;-
sentation des documents d&#8217;un corpus de d&#233;p&#234;ches, puis &#224; une approche d&#8217;apprentissage pour
r&#233;aliser les regroupements de mani&#232;re non supervis&#233;e fond&#233;e sur k-means. Enfin, nous &#233;valuons
le syst&#232;me de regroupement de documents sur un corpus de taille r&#233;duite et nous discutons de
l&#8217;&#233;valuation quantitative de ce type de t&#226;che.
</p>
<p>Abstract. This paper analyses the problem of automatic document clustering based on
events. We first specify the notion of event. Then, we detail the document modelling method
and the learning approach for document clustering based on k-means. We finally evaluate our
document clustering system on a small corpus and discuss the quantitative evaluation for this
kind of task.
</p>
<p>Mots-cl&#233;s : Regroupement de documents, Suivi d&#8217;&#233;v&#233;nement.
</p>
<p>Keywords: Document clustering, Event tracking.
</p>
<p>1 Introduction
</p>
<p>La veille est devenue un enjeu majeur pour les entreprises, qu&#8217;il s&#8217;agisse de veille technique ou
scientifique, commerciale ou strat&#233;gique. Les &#171; veilleurs &#187; manipulent des masses de donn&#233;es
de plus en plus importantes et ont besoin d&#8217;aides automatiques afin d&#8217;explorer au mieux ces
donn&#233;es. Dans cet esprit, de nouvelles perspectives de recherche ont vu le jour afin de faciliter
l&#8217;acc&#232;s &#224; un contenu noy&#233; dans un flot d&#8217;informations trop important. C&#8217;est notamment le cas
des t&#226;ches de d&#233;tection et de suivi d&#8217;&#233;v&#233;nement (en anglais topic detection and tracking &#8211; TDT).
</p>
<p>La d&#233;tection et le suivi d&#8217;&#233;v&#233;nement consistent &#224; regrouper dans une m&#234;me classe les documents
qui traitent d&#8217;un m&#234;me &#233;v&#233;nement. A titre d&#8217;exemple, deux d&#233;p&#234;ches ayant pour titre &#171; Arriv&#233;e
en France de Laurent Gbagbo en vue d&#8217;une table ronde &#224; Marcoussis &#187; et &#171; Ouverture des
n&#233;gociations entre rebelles et gouvernement ivoirien &#224; Marcoussis &#187; se rapportent &#224; un m&#234;me
&#233;v&#233;nement : &#171; La Table ronde de Marcoussis &#187;. La notion d&#8217;&#233;v&#233;nement est cependant une notion
vague, qu&#8217;il nous appartiendra de pr&#233;ciser au cours de cet article.
</p>
<p>On distingue deux cadres applicatifs aux t&#226;ches de d&#233;tection et de suivi d&#8217;&#233;v&#233;nement : le cadre
&#171; en ligne &#187; et le cadre &#171; hors ligne &#187;. Dans le premier cas, des documents arrivent les uns &#224;</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Aur&#233;lien Bossard, Thierry Poibeau
</p>
<p>la suite des autres, et les syst&#232;mes mis en &#339;uvre pour traiter la d&#233;tection d&#8217;&#233;v&#233;nement doivent
tenir compte de la sp&#233;cificit&#233; des flux d&#8217;informations continus. Dans le deuxi&#232;me cas, les do-
cuments &#224; traiter sont d&#233;j&#224; tous pr&#233;sents, et la d&#233;tection d&#8217;&#233;v&#233;nement consiste alors &#224; regrouper
les documents dans diff&#233;rentes classes qui correspondent chacune &#224; un &#233;v&#233;nement diff&#233;rent.
</p>
<p>Nous pr&#233;sentons dans cet article nos travaux sur l&#8217;aide &#224; l&#8217;analyse de corpus et la d&#233;tection
hors ligne d&#8217;&#233;v&#233;nements, c&#8217;est-&#224;-dire le regroupement non supervis&#233; de documents selon les
&#233;v&#233;nements dont ils traitent. Nous nous fondons pour cela sur une analyse automatique des
entit&#233;s nomm&#233;es, pour esquisser des liens entre documents. Cette analyse n&#8217;&#233;tant pas suffisante,
nous &#233;tudions plusieurs techniques permettant de pond&#233;rer les diff&#233;rents types d&#8217;entit&#233;s lors
de l&#8217;&#233;tape de regroupement automatique (classification). Ces techniques permettent d&#8217;obtenir
des &#171; paquets &#187; de documents homog&#232;nes du point de vue de l&#8217;&#233;v&#233;nement trait&#233;, ainsi qu&#8217;une
visualisation du fonds documentaire dans son ensemble, sous forme d&#8217;un r&#233;seau social.
</p>
<p>Apr&#232;s avoir pr&#233;sent&#233; un &#233;tat de l&#8217;art des technique de d&#233;tection hors ligne d&#8217;&#233;v&#233;nements, nous
essayons de mieux caract&#233;riser la notion d&#8217;&#233;v&#233;nement avant de pr&#233;senter notre syst&#232;me de d&#233;-
tection automatique. Nous d&#233;taillons ensuite la fa&#231;on dont les documents sont caract&#233;ris&#233;s, puis
l&#8217;algorithme de classification. Enfin, nous pr&#233;sentons l&#8217;&#233;valuation de notre syst&#232;me sur un cor-
pus de d&#233;p&#234;ches AFP.
</p>
<p>2 Etat de l&#8217;art
</p>
<p>La d&#233;tection d&#8217;&#233;v&#233;nements permet de suivre en direct des flux de d&#233;p&#234;ches et de les classer en
fonction du th&#232;me trait&#233;. Nous nous int&#233;ressons ici &#224; la d&#233;tection d&#8217;&#233;v&#233;nements hors ligne. Ce
th&#232;me a &#233;t&#233; moins trait&#233; que la d&#233;tection en ligne mais il est important, au moins dans deux cas
de figure bien identifi&#233;s :
</p>
<p>1. les analystes sont souvent confront&#233;s &#224; des masses de documents traitant de plusieurs
th&#232;mes. Avant d&#8217;acc&#233;der aux documents pertinents, une structuration du fonds documen-
taire est n&#233;cessaire.
</p>
<p>2. les syst&#232;mes automatiques d&#8217;extraction d&#8217;information n&#233;cessitent des masses de docu-
ments homog&#232;nes en entr&#233;e. Il faut donc les structurer par th&#232;me ou par &#233;v&#233;nement avant
de passer &#224; la phase d&#8217;extraction proprement dite.
</p>
<p>Nous poursuivons ces deux buts &#224; la fois, le but de notre application &#233;tant in fine de produire
des synth&#232;ses sommaires &#224; partir de masses de documents non structur&#233;s. La t&#226;che s&#8217;apparente
donc &#224; du r&#233;sum&#233; multi-documents &#224; partir d&#8217;un fonds documentaire non homog&#232;ne en entr&#233;e.
La visualisation des donn&#233;es permet en outre &#224; l&#8217;analyste de contr&#244;ler le processus de regrou-
pement de documents en ensembles pertinents. Nous ne nous int&#233;ressons ici qu&#8217;&#224; l&#8217;&#233;tape de
regroupement des documents.
</p>
<p>Plusieurs auteurs ont d&#233;crit des syst&#232;mes li&#233;s &#224; la d&#233;tection d&#8217;&#233;v&#233;nements hors ligne. Il s&#8217;agit
de (Yang et al., 1999), (Hatzivassiloglou et al., 2000), (Zhiwei Li &amp; Ma, 2005). Les premiers
et les seconds utilisent des algorithmes de classification hi&#233;rarchique, tandis que les troisi&#232;mes
utilisent des mod&#232;les probabilistes.
</p>
<p>(Hatzivassiloglou et al., 2000) se sont pos&#233; la question des donn&#233;es &#224; utiliser pour la d&#233;tection
d&#8217;&#233;v&#233;nements : vaut-il mieux utiliser la totalit&#233; des mots/phrases, exclure des mots qui ne sont</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Regroupement automatique de documents en classes &#233;v&#233;nementielles
</p>
<p>pas cat&#233;gorisables comme termes uniques (e.g. le Palais, peut &#234;tre du Luxembourg, de l&#8217;Ely-
s&#233;e...), ou ne tenir compte que des noms propres ? Les auteurs arrivent &#224; la conclusion que les
jeux de donn&#233;es avec lesquels ils obtiennent les meilleurs r&#233;sultats sont ceux prenant en compte
tous les mots sans exception. Ils attribuent cela au fait que les outils d&#8217;extraction de termes ou
de noms propres qu&#8217;ils utilisent ne sont pas assez robustes pour ce type de t&#226;che.
</p>
<p>(Yang et al., 1999) proposent une m&#233;thode de regroupement de documents o&#249; chacun des do-
cuments est repr&#233;sent&#233; par une liste de termes pond&#233;r&#233;s par leur tf.idf (cf. infra, fig. 2). Sur
le programme TDT, en utilisant un algorithme k-means multi-passes, les auteurs arrivent &#224; des
r&#233;sultats de 61 % et 69 % en pr&#233;cision/rappel.
</p>
<p>(Zhiwei Li &amp; Ma, 2005) proposent quant &#224; eux une approche probabiliste pour le regroupe-
ment de documents en utilisant comme repr&#233;sentation d&#8217;un document une matrice compos&#233;e de
quatre vecteurs : les noms de personnes, de lieux, les dates et des mots-clefs. Leur mod&#232;le pro-
babiliste appliqu&#233; &#224; un extrait du corpus du programme TDT4 produit des r&#233;sultats de l&#8217;ordre
de 85 % de pr&#233;cision et 67 % de rappel, en fixant &#224; la main le nombre de classes dans lesquelles
ranger les documents. Sur des jeux de donn&#233;es ne s&#233;parant pas les entit&#233;s nomm&#233;es des mots-
clefs, les r&#233;sultats sont inf&#233;rieurs de 10 %. Les auteurs l&#8217;expliquent par le fait que lorqu&#8217;elles ne
sont pas distingu&#233;es des mots-clefs, les entit&#233;s nomm&#233;es se retrouvent noy&#233;es dans les donn&#233;es,
alors que ce sont les &#233;l&#233;ments cl&#233;s pour la construction d&#8217;un mod&#232;le d&#8217;&#233;v&#233;nement.
</p>
<p>Les exp&#233;riences visant &#224; regrouper dynamiquement un flux de documents en ligne utilisent glo-
balement les m&#234;mes m&#233;thodes, &#224; l&#8217;instar de (Binsztok et al., 2004) : il s&#8217;agit g&#233;n&#233;ralement d&#8217;ap-
proches probabilistes combinant des sacs de mots et une fen&#234;tre temporelle associ&#233;e &#224; chaque
groupe de documents.
</p>
<p>Toutes les approches pr&#233;sent&#233;es ici, particuli&#232;rement (Hatzivassiloglou et al., 2000), utilisent
pour caract&#233;riser un document des vocabulaires assez &#233;tendus. La taille des donn&#233;es induite par
ce type de caract&#233;risation fait chuter les performances et la vitesse des syst&#232;mes de classifica-
tion. Par ailleurs, il a &#233;t&#233; montr&#233; dans (Zhiwei Li &amp; Ma, 2005) que la prise en compte de tout
le vocabulaire est moins pertinente que la focalisation sur les seuls &#233;l&#233;ments cl&#233;s, notamment
les entit&#233;s nomm&#233;es. Celles-ci ont par ailleurs un r&#244;le d&#233;terminant puisque les fondre dans la
masse de donn&#233;es fait chuter les performances.
</p>
<p>Enfin, par rapport &#224; des approches comme (Zhiwei Li &amp; Ma, 2005), nous souhaitons &#233;laborer
une m&#233;thode qui &#233;vite d&#8217;avoir &#224; pr&#233;ciser &#224; la main a priori le nombre de classes vis&#233;es. Plu-
sieurs solutions existent pour cela, dont l&#8217;utilisation de l&#8217;indice de Davies-Bouldin (Davies &amp;
Bouldin, 1979), une mesure permettant de quantifier la validit&#233; d&#8217;un clustering. Cet indice cor-
respond au rapport des inerties inter et intra-classes. Parmi les autres m&#233;thodes de s&#233;lection du
nombre de classes, on peut citer l&#8217;approche de (Hamerly &amp; Feng, 2006), qui permet d&#8217;obtenir
une meilleure mesure de la validit&#233; du clustering dans des cas difficiles, comme les grandes
dimensions. Dans notre cas, l&#8217;indice de Davies-Bouldin semble appropri&#233; : nous travaillons sur
des donn&#233;es de taille modeste. Nous projetons d&#8217;examiner ces autres mesures sur des donn&#233;es
plus volumineuses.
</p>
<p>3 Comment caract&#233;riser la notion d&#8217;&#233;v&#233;nement ?
</p>
<p>Si tous les travaux pr&#233;sent&#233;s dans l&#8217;&#233;tat de l&#8217;art obtiennent des r&#233;sultats satisfaisants, aucun n&#8217;a
tent&#233; de d&#233;crire formellement ce qu&#8217;est un &#233;v&#233;nement. Donner une d&#233;finition d&#8217;un tel concept</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Aur&#233;lien Bossard, Thierry Poibeau
</p>
<p>est certes difficile, mais nous avons cependant consid&#233;r&#233; qu&#8217;il &#233;tait n&#233;cessaire de caract&#233;riser un
&#233;v&#233;nement, ne serait-ce que pour rendre plus objective l&#8217;&#233;valuation.
</p>
<p>Nous avons explor&#233; diff&#233;rentes d&#233;finitions d&#8217;un &#233;v&#233;nement, notamment d&#8217;un point de vue so-
ciologique et d&#8217;un point de vue linguistique. D&#8217;un point de vue sociologique tout d&#8217;abord, l&#8217;&#233;v&#233;-
nement prend autant de d&#233;finitions que de champs disciplinaires dans lesquels il est consid&#233;r&#233;
(Prestini-Christophe, 2006). Il existe cependant des points communs &#224; toutes ces d&#233;finitions :
</p>
<p>&#8211; un &#233;v&#233;nement est un fait inattendu et correspond &#224; une rupture,
&#8211; un fait devient &#233;v&#233;nement en fonction du monde dans lequel il advient : l&#8217;&#233;v&#233;nement est
</p>
<p>subjectif.
</p>
<p>D&#8217;un point de vue linguistique, Pustejosky a &#233;labor&#233; une th&#233;orie (Pustejovsky, 2000) dans la-
quelle il fonde sa d&#233;finition de l&#8217;&#233;v&#233;nement sur le rep&#233;rage de structures pr&#233;dicat/arguments et
sur la notion de changement. L&#8217;&#233;v&#233;nement est identifiable gr&#226;ce &#224; un pr&#233;dicat d&#8217;&#233;v&#233;nement (i.e.
un verbe qui implique un changement) et une structure argumentale &#233;quivalente (c&#8217;est-&#224;-dire
une identit&#233; r&#233;f&#233;rentielle des arguments du pr&#233;dicat, m&#234;me si les formes de surface employ&#233;es
sont diff&#233;rentes). Une impl&#233;mentation de cette th&#233;orie n&#233;cessiterait des connaissances s&#233;man-
tiques tr&#232;s fines sur les verbes, mais &#233;galement des connaissances sur les diff&#233;rents arguments
possibles.
</p>
<p>Etant donn&#233; le nombre de d&#233;finitions et le peu de formalisation du concept d&#8217;&#233;v&#233;nement, il est
s&#251;rement plus judicieux de partir de nos besoins afin de d&#233;finir le concept d&#8217;&#233;v&#233;nement dans
le cadre de notre travail. Notre t&#226;che consiste &#224; regrouper les documents d&#8217;actualit&#233; (des d&#233;-
p&#234;ches) qui traitent du m&#234;me &#233;v&#233;nement, afin de r&#233;aliser une synth&#232;se automatique des groupes
cr&#233;&#233;s. On part du principe qu&#8217;une d&#233;p&#234;che traite d&#8217;un &#233;v&#233;nement unique (ceci est g&#233;n&#233;ralement
v&#233;rifi&#233;, sauf pour certaines d&#233;p&#234;ches qui retracent tous les faits marquants d&#8217;une journ&#233;e, ou
le d&#233;roulement d&#8217;une succession de faits plus ou moins li&#233;s entre eux). Les d&#233;p&#234;ches sont en
outre r&#233;dig&#233;es en forme de pyramide invers&#233;e : le premier paragraphe doit normalement conte-
nir l&#8217;information sur l&#8217;&#233;v&#233;nement brut, les d&#233;tails, commentaires et opinions &#233;tant normalement
d&#233;taill&#233;s dans la suite du document.
</p>
<p>Il s&#8217;agit alors, afin de r&#233;aliser notre t&#226;che, de trouver les traits communs entre les d&#233;p&#234;ches
qui traitent d&#8217;un m&#234;me &#233;v&#233;nement. Un &#233;v&#233;nement consiste en une action r&#233;alis&#233;e par une ou
plusieurs personnes, &#224; une certaine date, dans un certain lieu. Il est donc assez intuitif d&#8217;utiliser
ces marqueurs afin de reconna&#238;tre que deux d&#233;p&#234;ches traitent du m&#234;me sujet. Nous retrouvons
ici les &#233;l&#233;ments mis en &#233;vidence par (Pustejovsky, 2000) concernant les arguments du pr&#233;dicat.
On peut par ailleurs utiliser des mots clefs, tels que &#171; proc&#232;s &#187;, &#171; &#233;lections &#187;, &#171; bombardement &#187;,
afin de d&#233;partager des d&#233;p&#234;ches partageant des entit&#233;s nomm&#233;es mais dont le lecteur a l&#8217;intuition
qu&#8217;ils se rapportent malgr&#233; tout &#224; des &#233;v&#233;nements diff&#233;rents.
</p>
<p>Reste &#224; d&#233;finir la port&#233;e d&#8217;un &#233;v&#233;nement. En d&#8217;autres termes, quelle fen&#234;tre temporelle doit-on
adopter pour exclure d&#8217;un groupe un document qui traite du m&#234;me &#233;v&#233;nement qu&#8217;un document
pr&#233;c&#233;dent ? Contrairement &#224; (Binsztok et al., 2004), nous consid&#233;rons la notion de port&#233;e in-
d&#233;pendante de toute notion temporelle. Un fait directement li&#233; &#224; un &#233;v&#233;nement initial peut se
produire longtemps apr&#232;s le fait initial, comme la reconnaissance de l&#8217;innocence d&#8217;un homme
vingt ans apr&#232;s sa condamnation.
</p>
<p>Afin d&#8217;&#233;viter toute ambigu&#239;t&#233;, nous appellerons dor&#233;navant l&#8217;ensemble des d&#233;p&#234;ches regroup&#233;es
ensemble, parlant de faits directement li&#233;s entre eux, le &#171; sujet &#187;, et le fait initial, celui qui aura
conduit &#224; cette succession de faits, l&#8217;&#171; &#233;v&#233;nement &#187;. Les regroupements que nous chercherons
&#224; effectuer seront donc des regroupements par sujet, et non des regroupements par &#233;v&#233;nement.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Regroupement automatique de documents en classes &#233;v&#233;nementielles
</p>
<p>Cette notion de sujet se rapproche de la notion d&#8217;&#171; activit&#233; &#187; telle que d&#233;finie dans TDT4, par
opposition &#224; la notion d&#8217;&#171; &#233;v&#233;nement &#187;, toujours dans TDT4.
</p>
<p>Malgr&#233; cette tentative d&#8217;&#171; objectivisation &#187; des notions d&#8217;&#233;v&#233;nement et de sujet, ces notions
restent largement subjectives. A titre d&#8217;exemple, prenons deux documents : l&#8217;un parlant des
&#233;lections anticip&#233;es en C&#244;te d&#8217;Ivoire cons&#233;cutives aux accords de Marcoussis, l&#8217;autre parlant de
la table ronde de Marcoussis. Faut-il cr&#233;er deux sujets pour ces deux d&#233;p&#234;ches &#8211; l&#8217;un concer-
nant la r&#233;union de Marcoussis, l&#8217;autre l&#8217;application des accords d&#233;cid&#233;s &#224; Marcoussis, voire les
&#233;lection anticip&#233;es &#8211; ou les r&#233;unir au sein d&#8217;un m&#234;me sujet concernant les accords de Marcous-
sis, de leur n&#233;gociation &#224; leur application ? Il y a l&#224; une part de subjectivit&#233; qui ne peut &#234;tre
compl&#232;tement &#233;vit&#233;e.
</p>
<p>4 Mod&#232;le de description des documents
</p>
<p>Afin de regrouper les documents par sujet, mais &#233;galement afin d&#8217;obtenir une repr&#233;sentation gra-
phique qui permette &#224; un utilisateur de prendre rapidement connaissance du contenu d&#8217;un fonds
documentaire, celui-ci est repr&#233;sent&#233; &#224; la mani&#232;re d&#8217;un r&#233;seau social (fig 1). Chaque document
a des liens avec les autres documents du corpus. Le poids des liens entre deux documents est
calcul&#233; selon les entit&#233;s nomm&#233;es qu&#8217;ils partagent.
</p>
<p>FIG. 1 &#8211; Un corpus vu comme un r&#233;seau social : les noeuds sont les documents, leurs forme
et couleur d&#233;notent leur appartenance &#224; une classe, et la couleur des liens est fonction de leur
poids
</p>
<p>Nous avons utilis&#233;, pour calculer la similarit&#233; de contenu entre deux documents, un indice Jac-
card pond&#233;r&#233;. L&#8217;indice Jaccard est une m&#233;trique utilis&#233;e en statistique pour comparer la simila-
rit&#233; de deux ensembles, fond&#233;e sur le rapport entre la cardinalit&#233; de l&#8217;intersection et la cardinalit&#233;
de l&#8217;union des ensembles.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Aur&#233;lien Bossard, Thierry Poibeau
</p>
<p>Chaque ensemble est form&#233; des entit&#233;s nomm&#233;es contenues dans chaque document. Afin d&#8217;af-
finer l&#8217;importance relative des diff&#233;rentes entit&#233;s, il est n&#233;cessaire de les pond&#233;rer. En effet,
les corpus privil&#233;gient souvent certains th&#232;mes : certaines entit&#233;s centrales apparaissent alors
de mani&#232;re relativement uniformes et ont de ce fait un pouvoir discriminant tr&#232;s faible. Leur
participation au calcul de similarit&#233; doit &#234;tre en cons&#233;quence. Pour cela, nous avons test&#233; deux
mesures : la mesure idf et la mesure du tf.idf (Salton &amp; Buckley, 1988) (nous utilisons le
mode de calcul classique de ces deux mesures, cf. figure 2). Cette pond&#233;ration permet en outre
de prendre en compte la fr&#233;quence d&#8217;un terme. Il y a en effet une tr&#232;s forte probabilit&#233; que deux
documents qui contiennent les m&#234;mes entit&#233;s nomm&#233;es avec des fr&#233;quences proches fassent
partie d&#8217;un m&#234;me sujet, et la mesure jaccard pond&#233;r&#233;e par le tf-idf permet de rendre compte
de ce fait.
</p>
<p>Cette pond&#233;ration selon le tf.idf comme r&#233;alis&#233;e dans (Yang et al., 1999) ne suffit cependant
pas : certains types d&#8217;entit&#233;s nomm&#233;es ont un r&#244;le plus important que d&#8217;autres dans la descrip-
tion d&#8217;un &#233;v&#233;nement. En effet, les types d&#8217;entit&#233;s les plus instables au sein d&#8217;un m&#234;me sujet
sont les types &#171; Personnes &#187; et &#171; Organisations &#187;. Si les r&#233;f&#233;rences aux dates et aux lieux restent
globalement inchang&#233;es dans le suivi d&#8217;un sujet, les intervenants sont en revanche multiples et
variables. Il faut donc distinguer les entit&#233;s suivant leur type afin d&#8217;&#233;viter la cr&#233;ation de sujets
distincts correspondant &#224; chaque intervenant. Nous avons alors affect&#233; un poids &#224; chaque type
d&#8217;entit&#233;s nomm&#233;es, en privil&#233;giant les lieux et les dates par rapport aux noms de personnes et
d&#8217;organisations. Le calcul de la similarit&#233; entre documents est celui de la figure 3, le tf.idf
d&#8217;une entit&#233; pouvant &#234;tre remplac&#233; par l&#8217;idf.
</p>
<p>5 Classification
</p>
<p>Nous r&#233;alisons un regroupement des documents en utilisant l&#8217;algorithme k-means (Forgy, 1965).
Cet algorithme r&#233;alise une classification en k classes, en minimisant la variance intra-classe.
L&#8217;algorithme se d&#233;roule en 4 &#233;tapes :
</p>
<p>1. Choisir al&#233;atoirement k objets qui seront les centres de k classes ;
</p>
<p>2. Parcourir tous les objets, et les affecter ou les r&#233;affecter &#224; la classe qui minimise la dis-
tance entre l&#8217;objet et le centre de la classe ;
</p>
<p>3. Calculer les barycentres de chaque classe, ils deviennent les nouveaux centres ;
</p>
<p>Calcul du tf d&#8217;un terme ti dans le document dj :
tfti,dj =
</p>
<p>|ti|j&#8721;n
k=0
</p>
<p>|tk|dj
</p>
<p>Calcul de l&#8217;idf d&#8217;un terme ti au sein d&#8217;un corpus D de documents dj :
idfti =log
</p>
<p>(|D|)
{|dj :ti&#8712;dj}|
</p>
<p>Calcul du tf.idf du terme ti pour un document dj :
tf.idfti,dj = tfti,dj &#215; idfti
</p>
<p>FIG. 2 &#8211; Calcul du tf.idf</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Regroupement automatique de documents en classes &#233;v&#233;nementielles
</p>
<p>&#8211; S(i, j) = N11(i,j)(N11(i,j)+N10(i,j)+N01(i,j))
</p>
<p>&#8211; N11(i, j) =
n&#8721;
</p>
<p>k=0
</p>
<p>poids(ek)&#215; tf.idf(ek) , o&#249; les ek sont les entit&#233;s nomm&#233;es pr&#233;sentes dans i et j.
</p>
<p>&#8211; N10(i, j) =
n&#8721;
</p>
<p>k=0
</p>
<p>poids(ek)&#215; tf.idf(ek) , o&#249; les ek sont les EN pr&#233;sentes seulement dans i.
</p>
<p>&#8211; N01(i, j) =
n&#8721;
</p>
<p>k=0
</p>
<p>poids(ek)&#215; tf.idf(ek) , o&#249; les ek sont les EN pr&#233;sentes seulement dans j.
</p>
<p>FIG. 3 &#8211; Calcul de la mesure de similarit&#233; entre documents
</p>
<p>4. R&#233;p&#233;ter les &#233;tapes 2 et 3 jusqu&#8217;&#224; convergence. La convergence est atteinte lorsque les
classes deviennent stables.
</p>
<p>L&#8217;algorithme k-means prend comme param&#232;tre le nombre de classes (k). Dans le cadre du re-
groupement non-supervis&#233; de documents, il est n&#233;cessaire de laisser ce param&#232;tre libre, et de
trouver le meilleur k possible. Afin d&#8217;automatiser la recherche du meilleur param&#232;tre k, l&#8217;algo-
rithme k-means est appliqu&#233; n fois en incr&#233;mentant k &#224; chaque fois. Finalement, le param&#232;tre
k minimisant l&#8217;indice de Davies-Bouldin (Davies &amp; Bouldin, 1979) est retenu (cet indice per-
met de calculer la validit&#233; de la classification, en mesurant la coh&#233;rence des regroupements ;
les regroupements qui minimisent la distance entre objets de la m&#234;me classe et maximisent la
distance entre objets de classes diff&#233;rentes ont le meilleur indice).
</p>
<p>6 Evaluation
</p>
<p>Nous avons choisi d&#8217;&#233;valuer le travail de classification par un ensemble de mesures quantita-
tives. Une &#233;valuation qualitative des r&#233;sultats reste &#224; mener.
</p>
<p>6.1 Description du cadre applicatif
</p>
<p>Cette recherche s&#8217;inscrit dans le cadre du projet Infomagic, du p&#244;le de comp&#233;titivit&#233; Cap Di-
gital1. Ce cadre nous permet d&#8217;avoir acc&#232;s &#224; des besoins op&#233;rationnels pr&#233;cis et &#224; des corpus
vari&#233;s.
</p>
<p>Un de ces corpus largement mis &#224; contribution dans le cadre d&#8217;Infomagic est un ensemble de
d&#233;p&#234;ches AFP portant sur la C&#244;te d&#8217;Ivoire. Le corpus compte 15000 documents. L&#8217;annotation
des documents avec les entit&#233;s nomm&#233;es nous a &#233;t&#233; fourni par la soci&#233;t&#233; Arisem (partenaire du
projet Infomagic). Nous avons travaill&#233; sur un extrait du corpus comptant 200 d&#233;p&#234;ches, afin de
pouvoir &#233;valuer au mieux la classification. Ainsi, nous avons fait le choix de r&#233;aliser une double
annotation, afin de pouvoir comparer notre approche &#224; deux r&#233;sultats obtenus manuellement
par des personnes diff&#233;rentes. Le corpus compte 1030 entit&#233;s nomm&#233;es diff&#233;rentes. Nous avons
</p>
<p>1Cap Digital porte sur l&#8217;indexation multimedia. Cette recherche s&#8217;ins&#232;re dans le cadre de l&#8217;&#171; axe texte &#187; du pro-
jet, qui regroupe des entreprises et des laboratoires de recherche en traitement des langues, ainsi que des industriels
ayant des besoins sp&#233;cifiques qui servent de cadres d&#8217;application communs.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Aur&#233;lien Bossard, Thierry Poibeau
</p>
<p>&#233;tabli un corpus de r&#233;f&#233;rence en classant les d&#233;p&#234;ches par sujet, et obtenu 44 classes avec une
r&#233;partition tr&#232;s in&#233;gale.
</p>
<p>Nous avons voulu &#233;valuer toutes les &#233;tapes de la classification, afin d&#8217;identifier les faiblesses de
la m&#233;thode utilis&#233;e. Dans un premier temps, nous avons &#233;valu&#233; la qualit&#233; de la classification en
examinant les r&#233;sultats de l&#8217;algorithme k-means en faisant varier k de 2 &#224; 100, ce qui correspond
&#224; diviser le corpus en un nombre de classes allant de 2 &#224; 100. Dans un second temps, nous avons
&#233;valu&#233; le r&#233;sultat &#171; optimal &#187; de l&#8217;apprentissage, qui correspond &#224; la classification qui minimise
l&#8217;indice de Davies-Bouldin en le comparant au meilleur r&#233;sultat obtenu sur tous les lancements
de k-means, qualitativement parlant.
</p>
<p>6.2 Evaluation globale
</p>
<p>Nous avons &#233;valu&#233; la pertinence de la classification par deux m&#233;thodes :
</p>
<p>&#8211; les micro-moyennes ;
&#8211; les macro-moyennes.
</p>
<p>Ces deux mesures donnent des r&#233;sultats tr&#232;s diff&#233;rents : le r&#233;sultat de la macro-moyenne tient
plus compte des cat&#233;gories ayant peu de documents pertinents, tandis que le r&#233;sultat des micro-
moyennes fait plus ressortir les r&#233;sultats sur les plus grosses classes. Ces deux mesures sont
donc n&#233;cessaires pour la bonne interpr&#233;tation des r&#233;sultats. La m&#233;thode des macro-moyennes
consiste &#224; comparer chaque classe Ci du corpus &#233;tiquet&#233; automatiquement &#224; la classe du corpus
de r&#233;f&#233;rence majoritaire dans Ci. Les deux classifications compar&#233;es doivent avoir le m&#234;me
nombre de classes. Nous avons donc fix&#233; k au nombre de classes du corpus de r&#233;f&#233;rence. Une
moyenne est effectu&#233;e sur les mesures de chaque classe, avec un poids &#233;gal pour chaque classe.
La m&#233;thode des micro-moyennes consiste &#224; fusionner les tables de contingence de toutes les
classes et &#224; calculer les mesures sur la table fusionn&#233;e. Nous avons utilis&#233; les mesures classiques
de pr&#233;cision, rappel et F-mesure. La F-Mesure est la moyenne harmonique de la pr&#233;cision et du
rappel, et favorise les syst&#232;mes qui ont des mesures de rappel et de pr&#233;cision proches.
</p>
<p>Pr&#233;cision Rappel F-Mesure
Macro-Moyenne 61.4% 65.3% 63.2%
Micro-Moyennes 52.3% 55.6% 53.9%
</p>
<p>FIG. 4 &#8211; R&#233;sultats de l&#8217;&#233;valuation avec le nombre de classes fix&#233;
</p>
<p>Il est int&#233;ressant de noter que deux annotateurs humains ont obtenu sur ce m&#234;me corpus, des
r&#233;sultats dont la F-Mesure est &#224; 44%. Les deux annotations diff&#233;rentes se d&#233;fendent, les r&#233;sul-
tats de celles-ci d&#233;pendant fortement du choix de granularit&#233; utilis&#233; par les annotateurs dans
les sujets, et du choix de raccorder certains &#233;v&#233;nements &#224; un sujet ou de cr&#233;er un novueau su-
jet pour ceux-ci. Ceci pose le probl&#232;me de l&#8217;&#233;valuation d&#8217;une classification par des m&#233;thodes
quantitatives.
</p>
<p>6.3 S&#233;lection du k par l&#8217;indice de Davies-Bouldin
</p>
<p>Nous avons &#233;valu&#233; la s&#233;lection du k selon les deux points suivants :
&#8211; la diff&#233;rence entre le nombre de classes choisi automatiquement et le nombre de classes du
</p>
<p>corpus de r&#233;f&#233;rence ;</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Regroupement automatique de documents en classes &#233;v&#233;nementielles
</p>
<p>&#8211; le rapport entre l&#8217;indice de Davies-Bouldin et une mesure d&#8217;&#233;valuation de la classification.
En moyenne sur 15 lancements de l&#8217;algorithme (donc 15 configurations de d&#233;part diff&#233;rentes),
nous avons trouv&#233; 37,4 classes contre 44 classes dans le corpus de r&#233;f&#233;rence. Le m&#234;me corpus
annot&#233; par un deuxi&#232;me annotateur contient quant &#224; lui 65 classes, l&#8217;annotateur ayant fait des
choix diff&#233;rents concernant les sujets existants. On constate donc une erreur dans le choix au-
tomatique du nombre de classes pouvant aller de 16% &#224; 57%. Cette mesure n&#8217;est donc pas asez
significative pour &#233;valuer la qualit&#233; du choix du nombre de classes. Nous avons &#233;valu&#233; la perti-
nence de l&#8217;utilisation de l&#8217;indice de Davies-Bouldin ; la figure 5 montre l&#8217;&#233;volution de l&#8217;indice
pour tous les k ainsi qu&#8217;une mesure d&#8217;&#233;valuation du clustering, qui consiste &#224; consid&#233;rer tous
les documents li&#233;s au sein d&#8217;un m&#234;me cluster, et &#224; effectuer les calculs de pr&#233;cision/rappel sur la
pr&#233;sence/absence des liens du corpus de r&#233;f&#233;rence dans le corpus clusteris&#233; automatiquement.
</p>
<p>FIG. 5 &#8211; Indice de Davies-Bouldin et Choix du k
</p>
<p>On constate la non-corr&#233;lation de l&#8217;indice de Davies-Bouldin et le choix d&#8217;un nombre de classes
qui maximise la F-mesure. Ceci est en grande partie d&#251; au fait que le d&#233;coupage du corpus
de r&#233;f&#233;rence n&#8217;est pas homog&#232;ne : une des classes contient &#224; elle seule 1/6 des documents
du corpus. Or, le choix d&#8217;un k par Davies-Bouldin et la classification des documents par k-
means seront optimaux dans le cas o&#249; les classes ont toutes approximativement le m&#234;me nombre
d&#8217;objets.
</p>
<p>7 Conclusion
</p>
<p>Nous avons pr&#233;sent&#233; dans cet article un syst&#232;me de regroupement de d&#233;p&#234;ches, fond&#233; sur les
entit&#233;s nomm&#233;es partag&#233;es entre documents. Les r&#233;sultats obtenus suite &#224; la classification auto-
matique sont sup&#233;rieurs &#224; ceux obtenus par des annotateurs humains.
</p>
<p>L&#8217;&#233;valuation quantitative devrait &#224; l&#8217;avenir &#234;tre compl&#233;t&#233;e par une &#233;valuation qualitative. En ef-
fet, les syst&#232;mes d&#8217;&#233;valuation quantitative ne permettent pas de valider l&#8217;utilisabilit&#233; des r&#233;sul-
tats : la s&#233;paration d&#8217;une classe en deux par un algorithme peut faire chuter le rappel de 50%. Les
r&#233;sultats obtenus automatiquement sont toutefois int&#233;ressants et peuvent faire ressortir d&#8217;autres
regroupements que ceux choisis par les auteurs du corpus de r&#233;f&#233;rence, le regroupement de
documents &#233;tant tr&#232;s subjectif.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Aur&#233;lien Bossard, Thierry Poibeau
</p>
<p>L&#8217;algorithme utilis&#233; pour la classification, k-means, n&#8217;est pas exempt de d&#233;fauts : moins efficace
sur des donn&#233;es &#224; regrouper dans des classes non-homog&#232;nes, il devient &#233;galement moins per-
formant sur des corpus de grande dimension. Pour passer &#224; l&#8217;&#233;chelle sup&#233;rieure, il nous faudra
donc explorer d&#8217;autres m&#233;thodes de classification, comme les SVM et les cartes de Kohonen
non supervis&#233;es.
</p>
<p>Remerciements
</p>
<p>Ces recherches sont en partie financ&#233;es &#224; travers le projet Infomagic de P&#244;le de comp&#233;titivit&#233;
Cap Digital. Nous remercions en particulier la soci&#233;t&#233; Arisem qui nous a fourni l&#8217;annotation des
entit&#233;s nomm&#233;es.
</p>
<p>R&#233;f&#233;rences
</p>
<p>BINSZTOK H., ARTI&#200;RES T. &amp; GALLINARI P. (2004). Un mod&#232;le probabiliste de d&#233;tection
en ligne de nouvel ev&#233;nement. In Reconnaissance des Formes et Intelligence Artificielle (RFIA
2004), Toulouse, France.
</p>
<p>DAVIES D. L. &amp; BOULDIN D. W. (1979). A cluster separation measure. In IEEE Trans. on
Pattern Analysis and Machine Intelligence, p. 224&#8211;227.
</p>
<p>FORGY E. (1965). Cluster analysis of multivariate data : Efficiency vs. interpretability of
classifications. Biometrics, p. 21&#8211;768.
</p>
<p>HAMERLY G. &amp; FENG Y. (2006). Pg-means : learning the number of clusters in data. In The
Twentieth Annual Conference on Neural Information processing systems, Vancouver, Canada.
</p>
<p>HATZIVASSILOGLOU V., GRAVANO L. &amp; MAGANTI A. (2000). An investigation of linguistic
features and clustering algorithms for topical document clustering. In SIGIR 2000 : Procee-
dings of the 23rd Annual International ACM SIGIR Conference on Research and Development
in Information Retrieval, p. 224&#8211;231, Athens, Greece : ACM.
</p>
<p>PRESTINI-CHRISTOPHE M. (2006). La notion d&#8217;ev&#233;nement dans diff&#233;rents champs discipli-
naires. Pens&#233;e Plurielle, 13, 21&#8211;29.
PUSTEJOVSKY J. (2000). Events and the semantics of opposition. In C. TENNY &amp; J. PUS-
TEJOVSKY, Eds., Events as Grammatical Objects, chapter 13, p. 445&#8211;482. CSLI Publications.
</p>
<p>SALTON G. &amp; BUCKLEY C. (1988). Term-weighting approaches in automatic text retrieval.
Information Processing and Management : an International Journal, 24, 513&#8211;523.
YANG Y., CARBONEL J. G., BROWN R. D., PIERCE T. &amp; BRIAN T. ARCHIBALD X. L.
(1999). Learning approaches for detecting and tracking news events. In IEEE Intelligent
Systems, p. 32&#8211;43, Cambridge, Massachusetts.
</p>
<p>ZHIWEI LI, BIN WANG M. L. &amp; MA W.-Y. (2005). A probabilistic model for retrospective
news event detection. In SIGIR 2005 : Proceedings of the 28th Annual International ACM
SIGIR Conference on Research and Development in Information Retrieval, Salvador, Brazil :
ACM.</p>

</div></div>
</body></html>