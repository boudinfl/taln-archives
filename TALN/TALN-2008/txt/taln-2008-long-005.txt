TALN 2008, Avignon, 9-13 juin 2008

Un modele multi-sources pour la segmentation en sujets de
joumaux radiophoniques

Stéphane Huetl, Guillaume Gravierz, Pascale Sébillot3
Institut de Recherche en Informatique et Systemes Aléatoires, Rennes

(1) Université de Rennes 1, (2) CNRS, (3) INSA de Rennes
{stephane.huet,guillaume.graVier,pascale.sebillot} @irisa.fr

Résumé. Nous présentons une méthode de segmentation de joumaux radiophoniques en
sujets, basée sur la prise en compte d’indices lexicaux, syntaxiques et acoustiques. Partant d’un
modele statistique existant de segmentation thématique, exploitant la notion de cohésion lexi-
cale, nous étendons le formalisme pour y inclure des informations d’ordre syntaxique et acous-
tique. Les résultats expérimentaux montrent que le seul modele de cohésion lexicale ne sufﬁt
pas pour le type de documents étudié en raison de la taille variable des segments et de l’absence
d’un lien direct entre segment et theme. L’ utilisation d’informations syntaxiques et acoustiques
permet une amelioration substantielle de la segmentation obtenue.

Abstract. We present a method for story segmentation of radio broadcast news, based on
lexical, syntactic and audio cues. Starting from an existing statistical topic segmentation model
which exploits the notion of lexical cohesion, we extend the formalism to include syntactic and
acoustic knwoledge sources. Experimental results show that the sole use of lexical cohesion is
not efﬁcient for the type of documents under study because of the variable size of the segments
and the lack of direct relation between topics and stories. The use of syntactics and acoustics
enables a consequent improvement of the quality of the segmentation.

M0tS-CléS I segmentation en sujets, corpus oraux, cohésion lexicale, indices acoustiques,
indices syntaxiques.

Keywords: story segmentation, spoken documents, lexical cohesion, acoustic cues, syn-
tactic cues.

Stephane Huet, Guillaume Gravier, Pascale Sebillot

1 Introduction

Le traitement de ﬂux de donnees sonores ou encore multimedias a des ﬁns d’indexation neces-
site une etape prealable de structuration de ce ﬂux, basee sur une segmentation en composantes
elementaires puis en sujets speciﬁques au sein des composantes. Typiquement, analyser un ﬂux
sonore diffuse a la radio ou a la television requiert la detection des differentes emissions puis,
lorsque cela a un sens, une segmentation de chaque emission selon les sujets abordes. Cette der-
niere etape est particulierement importante pour les informations (joumaux televises ou ﬂash
d’informations) qui abordent differents sujets lies a l’actualite du moment (Allan et al., 1998) :
leur indexation ou leur structuration necessite leur decoupage en titres, reportages et transitions.

Pour mener a bien cette segmentation en sujets, on utilise classiquement des methodes develop-
pees en traitement automatique des langues pour la segmentation thematique de corpus ecrits,
que l’on applique sur la transcription automatique de la parole des emissions, consideree comme
un texte a part entiere. La majorite de ces methodes se base sur la notion de cohesion lexicale qui
mesure a quel point l’usage fait du vocabulaire change entre deux themes. L’ application directe
de ces techniques dans le cadre d’une segmentation en sujets des informations pose toutefois
plusieurs problemes. Tout d’abord, la transcription automatique presente des caracteristiques
differentes des textes ecrits. En plus des erreurs de transcription — entre 10 % et 20 % des mots
des emissions d’information sont generalement mal reconnus avec les systemes de transcription
actuels —, le texte produit automatiquement ne possede pas de marques de structure syntaxique
fortes : notions de phrase et paragraphe non deﬁnies, pas de ponctuation... En revanche, on peut
associer a la transcription des informations d’ordre acoustique comme la duree des pauses ou
les changements de locuteurs. Par ailleurs, la nature meme des documents implique certaines
difﬁcultes. Le temps alloue a chaque sujet varie fortement et peut etre extremement court, ce qui
rend les mesures de cohesion lexicale peu efﬁcaces. Si les reportages y ont en general une duree
consequente, des nouvelles breves emaillent aussi leur contenu. Les titres du journal constituent
un cas typique ou un sujet est aborde en quelques mots.

Ce demier exemple souleve d’ailleurs deux questions fondamentales, fortement liees, a savoir
d’une part la relation entre theme et sujet d’actualite et, d’autre part, la granularite visee de la
segmentation. L’utilisation de techniques de segmentation thematique pour detecter les chan-
gements de sujets se justiﬁe par le fait que, dans la plupart des cas, deux sujets consecutifs ne
traitent pas du meme theme. Cependant, les notions de sujet et de theme, bien que proches, ne
sont pas strictement equivalentes. Le cas des titres est emblematique : faut-il considerer les titres
dans leur ensemble ou bien chaque titre comme un sujet ? Du point de vue de la structure de
l’emission, le premier choix peut paraitre plus pertinent, alors que c’est l’inverse si l’on consi-
dere la question d’un point de vue du theme. Par ailleurs, la notion de theme n’est elle-meme
pas clairement deﬁnie et sujette a interpretation quant a la granularite envisagee. Un reportage
traitant de deux aspects proches, par exemple le clan Saddam Hussein et le role de l’ONU en
Irak, doit-il étre considere comme une unique entite sur la guerre en Irak ou comme deux en-
tites ? A defaut d’avoir une solution tranchee pour ces deux questions, nous prenons le parti
ici de nous baser sur une methode de type segmentation thematique pour notre probleme de
decoupage en sujets joumalistiques et nous laissons, a l’instar de (Rossignol & Sebillot, 2005),
la granularite des themes emerger des donnees.

Pour repondre a notre tache de segmentation de transcriptions automatiques d’eInissions d’ac-
tualites, nous decrivons, dans cet article, une extension du modele de cohesion lexicale de
Utiyama et Isahara (Utiyama & Isahara, 2001), modele qui a fait ses preuves pour la segmenta-

Segmentation en sujets de corpus oraux d’informations

tion thématique de l’écrit. Nous tirons proﬁt du cadre probabiliste de ce modele pour proposer
une technique permettant d’y combiner différents types d’information, a savoir lexicale, syn-
taxique et acoustique. Cette combinaison inédite d’indices nous permet d’obtenir un découpage
en sujets performant, sans faire d’hypotheses a priori sur les sujets présents, tout en étant ro-
buste aux erreurs de transcription eta la durée potentiellement courte des segments.

Dans la suite, nous passons tout d’abord en revue les techniques classiquement utilisées pour
détecter des changements de themes, avant de présenter en détail le modele de cohésion lexicale
utilisé et l’extension proposée aux autres sources d’information. Nous décrivons ensuite notre
cadre experimental et notre méthodologie de mesure des performances avant de présenter des
résultats expérimentaux sur le corpus ESTER (Galliano et al., 2006).

2 Techniques de segmentation thématique

La segmentation thématique consiste a localiser dans un document les points de changement
de themes, de maniere a obtenir des entités homogenes du point de vue du theme. Cette pro-
blématique a principalement été étudiée sur des données textuelles dans différents contextes
applicatifs comme la recherche d’information ou le résumé automatique. Dans le cadre des do-
cuments écrits, les indices permettant de détecter ces points de changement thématique sont de
plusieurs natures. Tout d’abord, au niveau lexical, un changement de theme s’accompagne en
général d’une modiﬁcation du vocabulaire utilisé. Des lors, la notion de cohésion lexicale d’un
segment, qui reﬂete l’homogénéité du vocabulaire — homogénéité « graphique » (répétitions)
mais également sémantique, les memes lexies ou des lexies d’un méme paradigme étant em-
ployées conjointement et différemment de leur distribution dans les segments adjacents —, est
a la base de nombreuses méthodes que nous discutons par la suite. Au-dela de la Inise au jour de
ruptures de similarité lexicale entre portions consécutives, la segmentation thématique peut éga-
lement s’appuyer sur le repérage de marqueurs discursifs, tant de continuité (« de plus ») que de
changement (« et maintenant »), mais également sur des informations de type syntaxique telles
la présence de pronoms référant des éléments antérieurs et suggérant la continuité d’un theme.

Dans le cas de documents sonores ou multimédias contenant de la parole, d’autres indices que
ceux liés au langage naturel peuvent apporter une information sur les frontieres de segments.
Par exemple, lors des informations, les changements de locuteurs, la présence de musique et,
dans le cas de la télévision, les changements de plans ou l’apparition du plateau sont des indices
forts marquant un changement de sujet (Tiir et al., 2001; Galley et al. , 2003). De maniere moins
spéciﬁque au type de documents étudiés, la prosodie fournit de précieux indices qui, en dehors
de la durée des pauses silencieuses, restent peu utilisés du fait de la difﬁculté a les mesurer de
facon ﬁable (Passonneau & Litman, 1997; Tiir et al., 2001).

Concretement, un ou plusieurs des indices que nous venons de mentionner sont exploités par les
diverses méthodes de segmentation thématique de la littérature. Lorsque les themes susceptibles
d’étre abordés sont connus a l’avance, il est possible de s’appuyer sur des modeles de langue
thématiques pour la segmentation et la caractérisation des themes. Cette nécessité de déﬁnir a
priori les themes limite toutefois les applications possibles de ces approches et les rend difﬁci-
lement exploitables dans le cadre de la segmentation en suj ets des informations. En l’absence de
ces connaissances a priori, de nombreux travaux fondent leur repérage de ruptures thématiques
sur la cohésion lexicale, et plus particulierement sur la détection de Ininima locaux de simila-
rité lexicale entre parties de textes consécutives. Outre le choix de la représentation des mots

Stephane Huet, Guillaume Gravier, Pascale Sebillot

par des lexies, des lemmes ou des racines, un aspect delicat de ces methodes reste la deﬁnition
des unites elementaires pour la comparaison de la distribution lexicale. Dans le cas de textes
structures, le paragraphe peut étre utilise comme unite elementaire, une mesure de rupture the-
matique etant effectue entre paragraphes successifs. A l’oppose, la technique peut se baser sur
des fenétres d’analyse contenant un nombre ﬁxe d’unites (Hearst, 1997), ces dernieres pouvant
étre des mots, des phrases ou encore des segments decoupes lors de l’etape de la transcription
automatique dans le cas de documents oraux. Ces methodes necessitent des fenetres d’ana-
lyse sufﬁsamment etendues ou l’apport de connaissances exterieures pour evaluer correctement
les zones de ruptures de cohesion. Utiyama et Isahara proposent une approche originale dans
laquelle l’ensemble des segmentations possibles est considere, le probleme de segmentation
thematique consistant a trouver celle conduisant aux segments les plus homogenes (Utiyama
& Isahara, 2001). A l’inverse des techniques exploitant une mesure de cohesion lexicale entre
segments successifs, cette approche ne se base que sur la mesure de la cohesion au sein d’un
segment. Lorsque des indices autres que lexicaux sont disponibles, ces differentes methodes a
base de cohesion lexicale peuvent etre etendues pour les prendre en compte. Diverses techniques
telles les arbres de decision (Tiir et al., 2001) ou les modeles de maximum d’entropie (Beefer-
man et al. , 1999), ont ete ainsi etudiees pour melanger des informations lexicales et acoustiques.

Notre cadre applicatif d’emissions d’actualites implique le reperage de segments de longueurs
tres variables, possiblement tres courts, potentiellement problematiques pour la plupart des tech-
niques « standards » de cohesion lexicale s’appuyant sur des fenétres de taille ﬁxe. Nous avons
choisi d’exploiter pour l’oral la methode de cohesion lexicale de Utiyama et Isahara, et de
l’etendre en tirant proﬁt de sa ﬂexibilite, de ses bonnes performances et de son potentiel — non
encore explore — pour l’integration d’informations syntaxiques et acoustiques.

3 Un modéle statistique multi-sources

Nous rappelons tout d’abord le principe propose par Utiyama et Isahara avant de decrire une
extension de ce formalisme.

3.1 Modéle statistique basé sur la cohesion lexicale

L’ idee de la methode de Utiyama et Isahara est de rechercher la segmentation qui conduit aux
segments les plus homogenes sur le plan lexical tout en respectant une distribution a priori de
la longueur des segments. La methode se place dans un cadre probabiliste et consiste a trouver
la meilleure segmentation § d’une sequence de l unites elementaires (mots, phrases...) W =
W‘, parIr1i toutes les segmentations possibles. La loi de Laplace est utilisee pour modeliser la
cohesion lexicale des segments, ces demiers etant vus comme des « sacs de mots ». De maniere
formelle, la meilleure segmentation est donnee par le critere du maximum a posteriori, soit, par
application de la regle de Bayes,

/\

S = argm§.xP[W|S]P[S] . (1)

En supposant d’une part que chaque segment forme une unite independante du reste du texte et,
d’autre part, que les mots au sein d’un segment sont independants — ce qui revient a representer
un segment comme un « sac de mots » —, la probabilite d’un texte W pour une segmentation

Segmentation en sujets de corpus oraux d’informations

S = Sf‘ est alors donnée par

m TL»;

P[W 181"] = H H P[w,‘-"’lSi] , (2)

i=1 j=1

ou ni est le nombre de mots dans le segment Si et wag) le j-eme mot de Si. La probabilité

P[wJ(-Z.)  est foumie par une loi de Laplace dont les parametres sont estimés sur Si, soit

fi(w§i>) + 1

Piwamlsil : n- +19

(3)

ou fi(w§-1.)) est le nombre d’occurrences de wail) dans Si et is est le nombre total de mots différents

dans W. En d’autres termes, on mesure a quel point un modele de la distribution des mots appris
sur un segment Si permet de prédire les mots de ce demier. Intuitivement, cette probabilité
favorise les segments homogenes du point de vue lexical puisqu’elle augmente lorsque les mots
apparaissent plusieurs fois et qu’elle diminue si beaucoup de mots sont différents.

La mesure de cohésion lexicale précédente est complétée par une distribution a priori de la
durée des segments, donnée en l’absence de connaissances explicites sur les documents a traiter,
par P[S{”] = n‘"‘ ou 12 est le nombre total de mots. Sa valeur est élevée lorsque le nombre de
segments est petit, ce qui compense le fait que les valeurs de P[W|S] sont fortes lorsque le
nombre de segments est grand.

Sur le plan de la mise en oeuvre, cette méthode de segmentation peut étre vue comme la re-
cherche du meilleur chemin dans un graphe valué représentant l’ensemble des segmentations
possibles. Chaque noeud du graphe modélise une frontiere de segment entre deux unités élé-
mentaires, un arc entre deux noeuds 1' et j représentant alors un segment regroupant les unités
élémentaires Wi+1 a Wj. La valeur associée a un arc entre deux noeuds 1' et j est donnée par

'u(i,j) =  1I1(P[W;i|Si_,j]) — a111(n) (4)

en notant Si_;j le segment correspondant a l’arc. Le facteur 04 est ajouté en pratique pour ponde-
rer la probabilité a priori et permettre un controle de la taille moyenne des segments retournés.

3.2 Introduction d’informations syntaxiques et acoustiques

Nous étendons le modele précédent de maniere a prendre en compte de nouvelles sources d’in-
formation. Nous considérons ici deux nouvelles sources, a savoir la syntaxe et l’acoustique,
sans toutefois limiter la généralisation du formalisme proposé. En notant A l’ensemble des in-
formations acoustiques disponibles et M les étiquettes morpho-syntaxiques associées aux mots
de W, le critere a optimiser pour la segmentation s’écrit sous la forme

s = P[W, A, M|S]P[S] = P[W|S]P[A|S]P[M|S]P[S] , (5)

en supposant les sources d’information indépendantes.

Le calcul des probabilités P[A|S] et P[M |S] suit le meme principe que nous illustrons ici pour
A. Dans la mesure ou P[A|S] est proportionnel a P[S |A] /P[S], déterminer P[A|S] se résume
au calcul de P[S Nous proposons d’utiliser les informations acoustiques ou syntaxiques

Stephane Huet, Guillaume Gravier, Pascale Sebillot

pour predire la probabilite qu’une frontiere de segment se trouve entre chacune des unites ele-
mentaires. En notant Bi la variable aleatoire binaire telle que Bi = 1 s’il existe une frontiere
entre Wi et Wi+1 et en supposant les Bi independants, nous obtenons

Pwm=ﬁHmm. (o

Dans les experiences presentees par la suite, nous utilisons un arbre de decision pour le cal-
cul des probabilites P[Bi|A] a partir des caracteristiques acoustiques au voisinage de la fron-
tiere consideree. Pour les informations syntaxiques, un modele N-gramme cache (Stolcke et al. ,
1998) est utilise pour calculer la probabilite d’une frontiere P[Bi|M] entre chaque unite ele-
mentaire.

Si l’hypothese d’independance entre les sources d’information permet d’employer independam-
ment un modele pour chacune d’elles dans l’equation (5), elle est quelque peu reductrice. En
particulier, les informations morpho-syntaxiques et les mots sont fortement lies. Cependant, la
representation de W sous forme de sac de mots est relativement distincte de la notion de se-
quence d’etiquettes morpho-syntaxiques consideree pour M, justiﬁant ainsi l’hypothese faite.
Dans le cas de deux sources d’information fortement correlees, par exemple X et Y, il est
toujours possible de les integrer conjointement en calculant par une methode appropriee les
probabilites P[Bi |X, Y] .

En pratique, les informations acoustiques et syntaxiques sont utilisees pour modiﬁer les valeurs
associees aux arcs dans le graphe des segmentations possibles. La valeur de l’arc i —> j est alors
donnee par

11(i,j) =  111(P[W,i|Sii]) + BA <111(P[Bj = 1|A]) +  1n(P[B,i = 0|A]))
+ BM (111(P[Bj =  +  lI1(P[Bk =  — a1I1(n) , (7)

les parametres BA et BM ayant pour but de ponderer l’apport de chacune des sources d’informa-
tion. Comme precedemment, le parametre oz permet de controler la taille moyenne des segments
retournes en donnant plus ou moins d’importance a la distribution a priori de la segmentation.

4 Conditions expérimentales

Les experiences presentees dans cet article ont ete menees sur les journaux de France Inter et de
France Info du corpus de developpement ESTER (Galliano et al., 2006), soit quatre journaux
d’une heure chacun enregistres le meme jour. Deux des journaux, un par radio, sont utilises
comme donnees de developpement pour regler les parametres du modele tandis que les deux
autres sont utilises comme donnees detest. Nous avons etabli une segmentation de reference des
quatre emissions considerees en distinguant cinq types de segments, correspondant a la struc-
ture de montage des emissions : les titres, les reportages, les breves, les segments de remplissage
et les publicites. Si les titres du journal forment un ensemble sur le plan de la structure, chaque
titre aborde un sujet different. Nous avons choisi d’etiqueter chacun comme un segment, suivant
ainsi un critere thematique, meme si ces titres sont consideres differemment des autres sections
lors de l’evaluation des performances comme discute ulterieurement. Les reportages corres-
pondent au developpement des sujets d’actualites. Ils sont souvent longs, parfois entrecoupes

Segmentation en sujets de corpus oraux d’informations

d’interviews ou encore d’interventions de spécialistes. Cependant, plusieurs sujets peuvent étre
abordés dans un reportage. Par exemple, un reportage sur la guerre en Irak aborde la traque des
proches de Saddam Hussein, le role de l’ONU en Irak et enﬁn les conditions sanitaires a l’issue
des combats. Comme pour les titres, nous avons choisi d’avoir un segment pour chacun des su-
jets abordés dans un reportage. Les breves correspondent a des nouvelles courtes, généralement
diffusées entre deux reportages. La difﬁculté de ce type de segment tient d’une part a leur durée
extrémement courte, de l’ordre d’une phrase ou deux, et, d’autre part, a l’agrégation possible de
plusieurs nouvelles breves dans une meme phrase. Les segments de remplissage correspondent
a des annonces de la radio pour donner l’heure, rappeler le nom des joumalistes ou annoncer
une rubrique (la météo, la bourse...). De par leur nature, ces segments ne sont associés a aucun
sujet. En revanche, ils marquent la plupart du temps un changement de segment. Finalement,
chaque publicité est annotée comme un segment a part entiere.

La transcription automatique requiert une étape préalable de segmentation du ﬂux sonore en
pseudo-phrases, appelés groupes de soufﬂe, chaque groupe de soufﬂe correspondant a un énoncé
entre deux respirations. Nous considérons dans cet article une segmentation manuelle en groupes
de soufﬂe, telle qu’établie selon les conventions de transcription du corpus ESTER, aﬁn de pou-
voir mesurer la dégradation de performance due a la transcription automatique. Les différentes
étapes du processus de transcription automatique des journaux radiophoniques sont décrites en
détail dans (Huet et al., 2007). Pour les quatre journaux considérés, le taux d’erreur sur les
mots est d’environ 20 %. Pour le modele de cohésion lexicale employé dans la segmentation en
sujets, nous ne conservons que les noms communs, les noms propres et les adjectifs apres lem-
matisation. Le taux d’erreur sur les lemmes conservés est d’environ 17 % sur les quatre heures
d’émissions.

L’ évaluation des performances se fait sur la base du nombre de frontieres entre groupes de
soufﬂe détectées correctement, les résultats étant donnés en termes de rappel et précision ainsi
que selon la métrique Wind0wDzﬁ° (Pevzner & Hearst, 2002). En raison des difﬁcultés évoquées
précédemment concernant les titres et la granularité considérée pour la segmentation des re-
portages, la notion de frontieres correctes n’est pas immédiate. Nous distinguons deux types
de frontieres selon qu’elles sont considérées comme facultatives ou obligatoires. Les frontieres
entre titres successifs ainsi qu’entre sujets proches au sein d’un meme reportage sont consi-
dérées comme facultatives. Si de telles frontieres n’entrent pas dans la comptabilisation des
erreurs, les détecter ne constitue pas pour autant une erreur d’insertion. Enﬁn, les segment de
remplissage posent un probleme particulier car ils ne sont pas lies a un sujet particulier. La
détection de tels segments nécessite des méthodes ad hoc et peu portables, par exemple basées
sur une liste de marqueurs pré-établis, que nous ne souhaitons pas considérer dans ce travail
par souci de généricité. Nous adoptons plutot la notion de frontiere ﬂoue en acceptant comme
correcte non seulement la détection des deux frontieres du segment de remplissage mais aussi
l’agrégation de ce dernier au segment précédent ou suivant. La métrique Wind0wDzﬁ" a été
adaptée pour prendre en compte les considérations que nous venons d’énumérer.

5 Résultats

Nous présentons tout d’abord quelques résultats basés sur le seul modele de cohésion lexi-
cale. Le poids oz de la distribution a priori dans l’équation (7) permet de faire varier la durée
moyenne des segmentations et donc d’atteindre différents compromis entre rappel et précision.
Nous avons réglé ce poids de maniere a minimiser le critere Wind0wDzﬁ‘ sur le corpus de deve-

Stéphane Huet, Guillaume Gravier, Pascale Sébillot

loppement. Sur ce dernier, les valeurs de rappel et de précision sur les frontieres de segments,
apres optimisation de oz, sont respectivement de 37,5 % et 54,1 % pour une durée moyenne des
segments de 26,5 groupes de soufﬂe (Wind0wDzﬁ‘ =0,110). Ces résultats mettent en évidence
d’une part les performances limitées de la seule cohésion lexicale pour segmenter des journaux
radiophoniques et, d’autre part, la degradation de performances liée aux erreurs de transcription.
En effet, en remplacant la transcription automatique par la référence, nous obtenons un rappel
de 43,8 % pour une précision de 54,5 %, correspondant a un critere Wind0wDzﬁ° de 0,105. Les
faibles valeurs de rappel et de précision, par opposition aux relativement bonnes performances
obtenues selon le critere Wind0wDzﬁ‘, montrent que nombre d’erreurs sont dues a une légere
imprécision sur la position des frontieres. En effet, en acceptant une tolérance d’un groupe de
soufﬂe sur la position des frontieres, on obtient alors un rappel de 58,8 % et une précision de
79,7% pour la transcription automatique, ce qui illustre bien le probleme de précision sur la
position des frontieres de segments.

Pour améliorer les premiers résultats basés sur la seule cohésion lexicale, nous étudions l’apport
des informations syntaxiques et acoustiques. Les modeles utilisés pour prédire la probabilité
d’une frontiere a partir de l’une ou l’autre des sources d’information ont été appris sur les an-
notations de référence des 80 heures du corpus d’apprentissage ESTER en se basant sur la seg-
mentation en sujets présente dans le corpus. Bien que sujette a caution sur certains pointsl , cette
derniere indique clairement les frontieres de reportages et les alternances reportages, breves et
segments de remplissage. Nous utilisons un arbre de décision pour calculer la probabilité d’une
frontiere de segment entre groupes de soufﬂe sur la base des informations acoustiques. Les in-
dices acoustiques considérés sont la présence d’un jingle a la frontiere, les durées du jingle en
cours respectivement avant et apres la ﬁn du groupe de soufﬂe, l’alternance ou pas de locu-
teurs homme/femmez et la durée des pauses en ne considérant que les pauses supérieures a 1,5
secondes. De maniere surprenante, la durée des pauses silencieuses n’a pas été retenu comme
critere dans l’arbre de décision pour prédire la présence d’une frontiere de segment. En effet,
nous avons constaté que de longues plages de silence intervenaient lors des interviews, expli-
quant ainsi la faible corrélation entre durée des pauses et frontiere thématique. L’ alternance de
locuteurs homme/femme a été jugée comme le critere le plus pertinent, suivie par la présence de
musique. Les informations syntaxiques sont quant a elles prises en compte par l’intermédiaire
d’un modele N-gramme caché, avec N=6, sur les étiquettes morpho-syntaxiques.

Les résultats obtenus sur le corpus de développement, en tenant compte des informations lexi-
cales, syntaxiques et acoustiques, sont donnés dans la ﬁgure 1. Pour l’ensemble des courbes
présentées, les poids BA et BM ont été déterminés de maniere a Ininimiser le critere Wind0wDzﬁ‘
sur les données de développement. Ces résultats montrent clairement l’apport des informations
syntaxiques par rapport au modele initial de cohésion lexicale. Bien que moindre, l’apport des
informations acoustiques est également Inis en évidence. Ce dernier résultat s’explique en par-
tie par le fait que les indices acoustiques permettent surtout d’éviter les frontieres considérées
comme optionnelles, notaInInent entre titres. Cette amélioration n’a qu’un faible impact sur la
mesure du rappel et de la précision en raison de la métrique utilisée pour laquelle la détection
des frontieres dites optionnelles n’est pas considérée comme une erreur. Enﬁn, la combinai-
son de l’ensemble des sources d’information permet d’obtenir les meilleurs résultats, ce qui est

1A1’inVerse des choix que nous avons effectués, les titres sont considérés comme un unique segment. 11 en est
de meme pour les reportages abordant plusieurs sujets.

2L’a1ternance de locuteurs homme/femme a été préférée aux changements de locuteurs pour deux raisons :
d’une part, la detection automatique des changements de sexe du locuteur est déja un indice intéressant pour le
passage d’un sujet a un autre. D’autre part, cette detection est plus ﬁable que celle des locuteurs.

Segmentation en sujets de corpus oraux d’informations

courbes obtenues sur des transcriptions automatiques

70

        

60 ,,,,, N  , , , , , , , V V , , , , , , , , , , , N  LEX+AC+SYN

50 __________ .._
’ LEX+SYN
LE >(+AC
\

40

30 \
,1

20

précision
/2*/7

lex —I—
|ex+ac -0-
|ex+syn -0-

I x n
10 20 30 40 50 60 70

IEDDSI

FIG. 1 — Rappel et précision pour la cohésion lexicale seule (LEX), avec l’acoustique
(LEX+AC), la syntaxe (LEX+SYN) ou encore les deux (LEX+AC+SYN). Les courbes sont
obtenues en faisant varier les valeurs des pondérations sur les deux émissions du corpus de
développement.

longueur pureté
moyenne segment rappel précision F wDiff

lex 23,8 0,76 45,2 51,5 48,1 0,098
leX+ac 38,5 0,69 43,5 70,0 53,7 0,121
leX+syn 29,9 0,73 45,2 61,5 52,1 0,097
leX+ac+syn 20,5 0,81 56,5 62,3 59,3 0,090

TAB. 1 — Résultats sur les deux émissions de test. Le nombre moyen de groupes de soufﬂe par
segment selon la segmentation de référence est sur ce corpus de 24,7.

conﬁrmé sur les données detest (cf. tableau 1). L’ utilisation d’indices acoustiques permet d’ob-
tenir des segments plus longs, augmentant ainsi grandement la précision pour une faible baisse
du rappel; ceci se fait au détriment de la pureté qui mesure la part des segments qui ne traitent
que d’un seul sujet. Les informations syntaxiques permettent également une augmentation de la
précision sans toutefois modiﬁer le rappel ni outre mesure la longueur moyenne des segments.
L’ utilisation conjointe des indices acoustiques et syntaxiques permet une amélioration a la fois
de la précision et du rappel, ce qui se traduit par une amélioration sensible du critere Window-
Dzﬁ‘ par rapport a la seule cohésion lexicale. La pureté des segments obtenus est également
grandement améliorée.

6 Conclusion

Nous avons proposé dans cet article un modele statistique permettant la prise en compte simul-
tanée d’indices lexicaux, syntaxiques et acoustiques pour la segmentation en sujets de journaux
radiophoniques. Les résultats expérimentaux ont permis de mettre en évidence que les limites
du modele de cohésion lexicale pour le type de documents utilisés ainsi que sa sensibilité aux
erreurs de transcription sont compensées par les informations syntaxiques et acoustiques.

Stéphane Huet, Guillaume Gravier, Pascale Sébillot

Bien qu’appliqué a deux sources de connaissances, notre modele offre un cadre générique per-
mettant d’accueillir d’autres indices. Il convient de noter toutefois que subsiste un aspect su-
pervisé qui pourrait limiter la robustesse de la méthode par rapport a la diversité des types de
documents : l’apprentissage des modeles de prédiction des frontieres effectué sur un corpus
segmenté manuellement. Si les indices considérés ici sont sufﬁsamment généraux pour donner
a penser que l’extension a d’autres documents est possible, une vériﬁcation est toutefois ne-
cessaire. C’est d’ailleurs l’une des premieres perspectives du travail présenté. Une autre piste
consiste a étudier l’apport de connaissances sémantiques, syntagmatiques et paradigmatiques, a
la segmentation, connaissances que notre modele est a meme de représenter. Ces informations
permettront entre autres de compenser la faible taille de certains segments. Enﬁn, il s’avere
important d’atténuer l’impact des erreurs de transcription, par exemple en ne se limitant pas
a la seule transcription mais en exploitant d’autres représentations plus riches fournies par le
systeme de reconnaissance automatique (graphes de mots, indices de conﬁance...).

Références

ALLAN J ., CARBONELL J ., DODDINGTON G., YAMRON J. & YANG Y. (1998). Topic de-
tection and tracking pilot study ﬁnal report. In Proc. DARPA Broadcast News Transcription
and Understanding Workshop.

BEEFERMAN D., BERGER A. & LAFFERTY J. (1999). Statistical models for text segmenta-
tion. Machine Learning, 34(1-3), 177-210.

GALLEY M., MCKEOWN K., FOSLER-LUSSIER E. & J ING H. (2003). Discourse segmenta-
tion of multi-party conversation. In Proc. Association for Computational Linguistics.
GALLIANo S., GEOFFROIS E., BONASTRE J GRAVIER G., MOSTEFA D. & CHOUKRI
K. (2006). Corpus description of the ESTER evaluation campaign for the rich transcription of
French broadcast news. In Proc. Language Resources and Evaluation Conference.

HEARST M. A. (1997). TextTiling : Segmenting text into multi-paragraph subtopic passages.
Computational Linguistics, 23(1), 33-64.

HUET S., GRAVIER G. & SEBILLOT P. (2007). Morphosyntactic processing of N-best lists
for improved recognition and conﬁdence measure computation. In Proc. European Conf on
Speech Communication and Technology.

PASSONNEAU R. J . & LITMAN D. J . (1997). Discourse segmentation by human and automa-
ted means. Computational Linguistics, 23(1), 103-139.

PEVZNER L. & HEARST M. A. (2002). A critique and improvement of an evaluation metric
for text segmentation. Computational Linguistics, 28(1), 19-36.

ROSSIGNOL M. & SEBILLOT P. (2005). Combining statistical data analysis techniques to
extract topical keyword classes from corpora. Intelligent Data Analysis, 9(1), 105-127.
STOLCKE A., SHRIBERG E., BATES R., OSTENDORF M., HAKKANI D., PLAUCHE M.,
TUR G. & LU Y. (1998). Automatic detection of sentence boundaries and disﬂuencies based
on recognized words. In Proc. Intl. Conf on Spoken Language Processing.

TUR G., HAKKANI-TUR D., STOLCKE A. & SHRIBERG E. (2001). Integrating prosodic and
lexical cues for automatic topic segmentation. Computational Linguistics, 21(1), 31-57.

UTIYAMA M. & ISAHARA H. (2001). A statistical model for domain-independent text seg-
mentation. In Proc. Association for Computational Linguistics.

