<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
"http://www.w3.org/TR/html4/loose.dtd">
<html><head><title>Enertex : un syst&#232;me bas&#233; sur l&#8217;&#233;nergie textuelle</title>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
</head>
<body>
<div style="page-break-before:always; page-break-after:always"><div><p>TALN 2008, Avignon, 9&#8211;13 juin 2008
</p>
<p>Enertex : un syst&#232;me bas&#233; sur l&#8217;&#233;nergie textuelle
</p>
<p>Silvia FERN&#193;NDEZ1,2, Eric SANJUAN1, Juan Manuel TORRES-MORENO1,3
1 Laboratoire Informatique d&#8217;Avignon, BP 1228 84911 Avignon France
</p>
<p>2 LPM UHP-Nancy, BP 239 54506 Vand&#339;uvre l&#232;s Nancy France
3 &#201;cole Polytechnique de Montr&#233;al, CP 6079 Montr&#233;al, Canada H3C3A7
</p>
<p>{silvia.fernandez, eric.sanjuan, juan-manuel.torres}@univ-avignon.fr
</p>
<p>R&#233;sum&#233;. Dans cet article, nous pr&#233;sentons des applications du syst&#232;me Enertex au Trai-
tement Automatique de la Langue Naturelle. Enertex est bas&#233; sur l&#8217;&#233;nergie textuelle, une ap-
proche par r&#233;seaux de neurones inspir&#233;e de la physique statistique des syst&#232;mes magn&#233;tiques.
Nous avons appliqu&#233; cette approche aux probl&#232;mes du r&#233;sum&#233; automatique multi-documents et
de la d&#233;tection de fronti&#232;res th&#233;matiques. Les r&#233;sultats, en trois langues : anglais, espagnol et
fran&#231;ais, sont tr&#232;s encourageants.
</p>
<p>Abstract. In this paper we present Enertex applications to study fundamental problems in
Natural Language Processing. Enertex is based on textual energy, a neural networks approach,
inspired by statistical physics of magnetic systems. We obtained good results on the application
of this method to automatic multi-document summarization and thematic border detection in
three languages : English, Spanish and French.
</p>
<p>Mots-cl&#233;s : &#201;nergie textuelle, R&#233;seaux de neurones, Mod&#232;le de Hopfield, R&#233;sum&#233; auto-
matique, Fronti&#232;res th&#233;matiques.
</p>
<p>Keywords: Textual Energy, Neural Networks, Hopfield Model, Automatic Summari-
zation, Thematic Boundaries.
</p>
<p>1 Introduction
</p>
<p>Des id&#233;es emprunt&#233;es &#224; la physique ont d&#233;j&#224; &#233;t&#233; utilis&#233;es dans l&#8217;analyse de textes. Les exemples
plus notables sont l&#8217;approche entropique de (Shannon, 1948), les travaux de (Zipf, 1935; Zipf,
1949) et de (Mandelbrot, 1953) o&#249; les auteurs font des consid&#233;rations thermodynamiques d&#8217;&#233;ner-
gie et de temp&#233;rature dans leurs &#233;tudes sur la Statistique Textuelle. Derni&#232;rement (Takamura
et al., 2005) se sont servi des notions de polarisation des syst&#232;mes de spins pour trouver les
orientations s&#233;mantiques des mots (d&#233;sirable ou ind&#233;sirable) &#224; partir de mots amorce. La sortie
de ce syst&#232;me est une liste de mots indiquant leurs orientations estim&#233;s selon l&#8217;approximation
du champ moyen. Dans notre travail, nous avons utilis&#233; diff&#233;remment la notion de spin. Elle
nous a permis de repr&#233;senter les pr&#233;sences (&#8593;) o&#249; absences (&#8595;) des mots dans les documents. &#192;
partir de cet image, on aper&#231;oit le document comme un mat&#233;riaux compos&#233; d&#8217;un ensemble de
unit&#233;s en interaction dont l&#8217;&#233;nergie peut &#234;tre calcul&#233;e. Nous avons &#233;tudi&#233; les probl&#232;mes du Trai-
tement Automatique de la Langue Naturelle (TALN) en utilisant la notion d&#8217;&#233;nergie textuelle.
R&#233;cemment introduite (Fern&#225;ndez et al., 2007a; Fern&#225;ndez et al., 2007b), l&#8217;&#233;nergie textuelle a
&#233;t&#233; appliqu&#233;e au r&#233;sum&#233; automatique et &#224; la d&#233;tection de fronti&#232;res sur des corpus en fran&#231;ais</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Silvia FERN&#193;NDEZ, Eric SANJUAN, Juan Manuel TORRES-MORENO
</p>
<p>et en anglais. Elle est aussi un des algorithmes utilis&#233;s dans (da Cunha et al., 2007) o&#249; des
m&#233;thodes statistiques et linguistiques sont combin&#233;es pour r&#233;sumer des articles m&#233;dicaux en
espagnol. Dans cet article nous &#233;tudions l&#8217;influence de deux facteurs, inspir&#233;s aussi de la phy-
sique, sur l&#8217;&#233;nergie textuelle. Il s&#8217;agit d&#8217;un champ externe et de la temp&#233;rature. Cette d&#233;marche a
permis d&#8217;am&#233;liorer les performances du mod&#232;le. Les r&#233;sultats sur des corpus multi-documents
et trilingues (fran&#231;ais, anglais et espagnol) sont tr&#232;s encourageants. Nous pr&#233;sentons dans la
Section 2 une br&#232;ve introduction au mod&#232;le neuronal de Hopfield ainsi que son extension au
TALN. Nous appliquons l&#8217;&#233;nergie textuelle &#224; deux t&#226;ches bien distinctes : la g&#233;n&#233;ration de
r&#233;sum&#233;s multi-documents guid&#233;s par une th&#233;matique dans la Section 3 et l&#8217;am&#233;lioration de l&#8217;al-
gorithme de d&#233;tection de fronti&#232;res th&#233;matiques dans la Section 4. Finalement nous pr&#233;sentons
les conclusions et quelques perspectives.
</p>
<p>2 L&#8217;&#233;nergie textuelle des documents
</p>
<p>La contribution la plus importante de Hopfield &#224; la th&#233;orie des r&#233;seaux neuronaux a &#233;t&#233; l&#8217;in-
troduction du concept d&#8217;&#233;nergie issu de l&#8217;analogie avec les syst&#232;mes magn&#233;tiques : syst&#232;mes
constitu&#233;s d&#8217;un ensemble de N petits aimants appel&#233;s spins qui peuvent s&#8217;orienter selon plu-
sieurs directions. Le cas le plus simple est repr&#233;sent&#233; par le mod&#232;le d&#8217;Ising, avec deux directions
possibles : vers le haut (&#8593;, +1 ou 1) ou vers le bas (&#8595;, -1 ou 0). Ce mod&#232;le a &#233;t&#233; utilis&#233; dans une
grande vari&#233;t&#233; de syst&#232;mes qui peuvent &#234;tre d&#233;crits par des variables binaires (Ma, 1985). Un
syst&#232;me de N unit&#233;s binaires poss&#232;de &#957; = 1, ..., 2N configurations (patrons) possibles. Dans
le mod&#232;le de Hopfield, les spins correspondent aux neurones qui interagissent selon la r&#232;gle
d&#8217;apprentissage de Hebb1 :
</p>
<p>J i,j =
</p>
<p>P&#8721;
&#181;=1
</p>
<p>si&#181;s
j
&#181; (1)
</p>
<p>si et sj sont les &#233;tats des neurones i et j. La sommation porte sur les P patrons &#224; stocker. Ce
mod&#232;le est aussi connu sous le nom de m&#233;moire associative. Il poss&#232;de la capacit&#233; de stocker et
de r&#233;cup&#233;rer un certain nombre de configurations du syst&#232;me, car la r&#232;gle de Hebb transforme
ces configurations en attracteurs (minimaux locaux) de la fonction d&#8217;&#233;nergie (Hopfield, 1982) :
</p>
<p>E&#181;,&#957; = &#8722;
1
</p>
<p>2
</p>
<p>N&#8721;
i=1
</p>
<p>N&#8721;
j=1
</p>
<p>si&#181; J
i,j sj&#957; (2)
</p>
<p>Si on pr&#233;sente un patron proche &#224; &#957;, chaque spin subira un champ local hi&#181; =
&#8721;N
</p>
<p>j=1 J
i,jsj&#181;
</p>
<p>induit par les N spins des autres patrons &#181;. Les spins s&#8217;aligneront selon hi&#181; pour restituer le
patron stock&#233; &#957;. Hopfield a d&#233;montr&#233; que l&#8217;&#233;nergie du syst&#232;me diminue toujours pendant le
processus de r&#233;cup&#233;ration. Nous ne d&#233;velopperons pas la m&#233;thode de r&#233;cup&#233;ration de patrons2,
car l&#8217;int&#233;r&#234;t porte sur la distribution et les propri&#233;t&#233;s de l&#8217;&#233;nergie du syst&#232;me. Cette fonction
monotone et d&#233;croissante a &#233;t&#233; utilis&#233;e uniquement pour montrer que l&#8217;apprentissage est born&#233;.
</p>
<p>D&#8217;un autre c&#244;t&#233;, le mod&#232;le vectoriel de textes (Salton &amp; McGill, 1983) transforme un document
dans un espace ad&#233;quat o&#249; une matrice S contient l&#8217;information du texte sous forme de sacs de
mots. On peut consid&#233;rer S comme l&#8217;ensemble des configurations d&#8217;un syst&#232;me dont on peut
</p>
<p>1Hebb (Hertz et al., 1991) a sugg&#233;r&#233; que les connexions synaptiques changent proportionnellement &#224; la corr&#233;-
lation entre les &#233;tats des neurones.
</p>
<p>2Cependant le lecteur int&#233;ress&#233; peut consulter, par exemple (Hopfield, 1982; Hertz et al., 1991).</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Enertex : un syst&#232;me bas&#233; sur l&#8217;&#233;nergie textuelle
</p>
<p>calculer l&#8217;&#233;nergie. Les documents sont pr&#233;-trait&#233;s avec des algorithmes classiques de filtrage de
mots fonctionnels3, de normalisation et de lemmatisation (Porter, 1980; Manning &amp; Sch&#252;tze,
1999) afin de r&#233;duire la dimensionnalit&#233;. Une repr&#233;sentation en sac de mots produit une matrice
S[P&#215;N ] de fr&#233;quences/absences :
</p>
<p>S = [si&#181;] =
</p>
<p>{
TF i si le terme i existe
0 autrement
</p>
<p>(3)
</p>
<p>o&#249; &#181; = 1, &#183; &#183; &#183; , P phrases et i = 1, &#183; &#183; &#183; , N termes. La pr&#233;sence du mot i repr&#233;sente un spin
si &#8593; avec une magnitude donn&#233;e par sa fr&#233;quence TF i (son absence par &#8595; respectivement), et
une phrase est donc une cha&#238;ne de N spins. Pour calculer les interactions entre les N termes du
vocabulaire, on applique la r&#232;gle de Hebb, qui sous forme matricielle se traduit par :
</p>
<p>J = ST &#215; S (4)
Chaque &#233;l&#233;ment J i,j &#8712; J[N&#215;N ] est &#233;quivalent au calcul de (1). Enfin, l&#8217;&#233;nergie textuelle d&#8217;inter-
action (2) peut alors s&#8217;exprimer comme :
</p>
<p>E = &#8722;
1
</p>
<p>2
S &#215; J &#215; ST (5)
</p>
<p>Un &#233;l&#233;ment E&#181;,&#957; &#8712; E[P&#215;P ] repr&#233;sente l&#8217;&#233;nergie textuelle entre les phrases &#181; et &#957;. La repr&#233;senta-
tion sous forme de graphe (Fern&#225;ndez et al., 2007b) nous a permis d&#8217;expliquer la nature des liens
que la mesure d&#8217;&#233;nergie textuelle induit. On a d&#233;duit qu&#8217;elle relie &#224; la fois des phrases ayant des
mots communs, ainsi que des phrases qui partagent un m&#234;me voisinage sans pour autant par-
tager n&#233;cessairement un m&#234;me vocabulaire. C&#8217;est pour cette raison que l&#8217;&#233;nergie textuelle peut
&#234;tre utilis&#233;e comme mesure de similarit&#233; dans les applications du TALN. Nous avons d&#233;velopp&#233;
l&#8217;algorithme Enertex bas&#233; sur cette mesure de similarit&#233;. Les premi&#232;res applications ont port&#233;
sur le r&#233;sum&#233; mono-document gen&#233;rique et sur la d&#233;tection de fronti&#232;res th&#233;matiques. Dans la
section suivante nous montrons une modification qui consiste en mettre un champ externe en
rapport avec un corpus multi-document. Cette strat&#233;gie permet de g&#233;n&#233;rer des r&#233;sum&#233;s guid&#233;s
par les besoins de l&#8217;utilisateur. Une autre approche, montr&#233;e dans la section 4, utilise l&#8217;&#233;ner-
gie textuelle repr&#233;sent&#233;e comme un spectre de la phrase. Ceci permet la d&#233;tection de fronti&#232;res
th&#233;matiques d&#8217;un document au moyen d&#8217;un test de concordance de Kendall. Nous montrons ici
comment l&#8217;introduction d&#8217;une temp&#233;rature modifie les spectres des phrases afin que le test de
Kendall puisse mieux les identifier.
</p>
<p>3 R&#233;sum&#233; multi-document guid&#233; par une requ&#234;te
</p>
<p>Les premiers syst&#232;mes de r&#233;sum&#233; automatique multi-documents ont &#233;t&#233; d&#233;velopp&#233;s dans les
ann&#233;es 90 (McKeown &amp; Radev, 1995). Les conf&#233;rences DUC portant sur la t&#226;che de r&#233;sum&#233;
automatique sont organis&#233;es depuis 2001 par le NIST4. La t&#226;che principale de DUC consiste &#224;
traiter des questions complexes et r&#233;elles. Le type de r&#233;ponse attendue ne peut pas &#234;tre une entit&#233;
simple (un nom, une date ou une quantit&#233; telle que classiquement d&#233;fini dans les conf&#233;rences
TREC Question-Answering5). Le probl&#232;me peut se poser comme ceci : &#233;tant donn&#233;e une th&#233;-
matique et un ensemble L avec D documents pertinents, g&#233;n&#233;rer un court r&#233;sum&#233; de 250 mots,
</p>
<p>3Nous avons effectu&#233; le filtrage de chiffres et l&#8217;utilisation d&#8217;anti-dictionnaires.
4http://www-nlpir.nist.gov/projects/duc
5http://trec.nist.gov/data/qamain.html</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Silvia FERN&#193;NDEZ, Eric SANJUAN, Juan Manuel TORRES-MORENO
</p>
<p>coh&#233;rent et bien organis&#233; qui r&#233;pondra aux questions de la th&#233;matique. Les D = 25 documents
proviennent du corpus AQUAINT : articles d&#8217;Associated Press, New York Times (1998-2000)
et Xinhua News Agency (1996-2000). L&#8217;&#233;valuation de la qualit&#233; des r&#233;sum&#233;s mono-document
reste une t&#226;che difficile. En multi-documents le probl&#232;me n&#8217;est pas plus simple. Des approches
manuelles et semi-automatiques ont &#233;t&#233; utilis&#233;es &#224; ce propos. Ainsi Pyramid (Passonneau et al.,
2005), Basic Elements (Hovy et al., 2005) et ROUGE (Lin, 2004) ont &#233;t&#233; employ&#233;es. Plusieurs
mesures manuelles ont &#233;t&#233; &#233;valu&#233;es : coh&#233;rence, grammaticalit&#233;, non-redondance, pertinence
au sujet, qualit&#233; linguistique. ROUGE est utilis&#233;e par la communaut&#233; comme mesure d&#8217;&#233;valua-
tion semi-automatique. Elle mesure l&#8217;intersection d&#8217;ensembles de n-grammes entre les r&#233;sum&#233;s
candidats et ceux de r&#233;f&#233;rence. Les m&#233;triques les plus populaires sont ROUGE-2 (bigrammes)
et SU4 (bigrammes s&#233;par&#233;s par un intervalle &#8804; 4 mots). Nous avons utilis&#233; l&#8217;&#233;nergie textuelle
pour la t&#226;che de r&#233;sum&#233; guid&#233; par une th&#233;matique ou sujet. L&#8217;id&#233;e est d&#8217;observer la r&#233;ponse
du syst&#232;me face &#224; un champ externe. Ce champ, repr&#233;sent&#233; par le vecteur des termes d&#8217;un texte
d&#233;crivant un sujet a &#233;t&#233; mis en relation avec le corpus multi-document. La figure 1 illustre le pro-
cessus d&#8217;obtention du r&#233;sum&#233; guid&#233; par un sujet. Les documents sont concat&#233;n&#233;s dans un seul
document et un pr&#233;traitement standard (filtrage et stemming (Porter, 1980)) lui est appliqu&#233;.
L&#8217;&#233;nergie textuelle entre le sujet et les phrases du document concat&#233;n&#233; est calcul&#233;e selon :
</p>
<p>E(sujet, phrase) = &#8722;
1
</p>
<p>2
</p>
<p>N&#8721;
i=1
</p>
<p>N&#8721;
j=1
</p>
<p>sisujet J
i,j s
</p>
<p>j
phrase (6)
</p>
<p>Finalement, le r&#233;sum&#233; est form&#233; avec les phrases pr&#233;sentant la plus haute &#233;nergie textuelle avec
le sujet. Un post-traitement de diminution de la redondance lui a &#233;t&#233; appliqu&#233;.
</p>
<p>FIG. 1 &#8211; R&#233;sum&#233; guid&#233; par une th&#233;matique et un ensemble de documents.
</p>
<p>Diminution de la redondance
</p>
<p>Dans un r&#233;sum&#233; multi-document il y a une probabilit&#233; significative de re-inclure l&#8217;informa-
tion d&#233;j&#224; pr&#233;sente. Pour diminuer ce probl&#232;me il faut une strat&#233;gie de diminution de la redon-
dance. Notre syst&#232;me ne poss&#232;de pas un traitement linguistique et la strat&#233;gie d&#8217;anti-redondance
consiste &#224; comparer les valeurs d&#8217;&#233;nergie des phrases candidates et leur longueur. Nous suppo-
sons que (dans de grands corpus) la probabilit&#233; que 2 phrases aient les m&#234;mes valeurs d&#8217;&#233;nergie
est tr&#232;s faible. Ainsi, nous avons &#233;limin&#233; la pr&#233;sence de doublons (phrases avec exactement la
m&#234;me valeur d&#8217;&#233;nergie). Peut-on aller encore plus loin et d&#233;tecter avec ce m&#234;me crit&#232;re des
phrases &#233;gales &#224; quelques mots pr&#232;s ? Pour le tester, on consid&#232;re que si 2 phrases partagent
la plus grande partie de leurs mots, elles apportent la m&#234;me information. On construit donc le
r&#233;sum&#233; avec la phrase la plus &#233;nerg&#233;tique (en valeur absolue), puis la suivante dans le score (la
candidate) fera partie du r&#233;sume si |E2 &#8722; E1| &#8805; &#491;. E1 est l&#8217;&#233;nergie de la phrase d&#233;j&#224; pr&#233;sente.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Enertex : un syst&#232;me bas&#233; sur l&#8217;&#233;nergie textuelle
</p>
<p>La 3&#232;me phrase candidate fera partie du r&#233;sum&#233; si |E3 &#8722;E1| &#8805; &#491; et si |E3 &#8722;E2| &#8805; &#491;. Les &#233;ner-
gies E1 et E2 sont consid&#233;r&#233;es comme celles des phrases de r&#233;f&#233;rence. En g&#233;n&#233;ral, une phrase
candidate i sera ajout&#233;e au r&#233;sum&#233;, si pour chaque phrase de r&#233;f&#233;rence (i&#8722; 1) :
</p>
<p>|Ei &#8722;Ei&#8722;1| = &#8710;E &#8805; &#491;; i = 2, 3, ... (7)
Le cas contraire signifie que les &#233;nergies sont tr&#232;s proches avec une haute probabilit&#233; de redon-
dance. On pr&#233;sente sur la figure 2 &#224; gauche les valeurs du rappel du produit ROUGE-2 &#215; SU4
pour diff&#233;rentes valeurs de &#8710;E. Le meilleur r&#233;sultat sur les corpus DUC&#8217;05-07 est obtenu avec
&#8710;E &#8776; 0, 003. Cela correspond aux phrases &#224; 2 mots pr&#232;s. Une autre strat&#233;gie permettant de
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>0,000 0,003 0,006 0,009 0,012 0,015 0,018 0,021 0,024
0,008
</p>
<p>0,009
</p>
<p>0,010
</p>
<p>0,011
</p>
<p>0,012
</p>
<p>0,013
</p>
<p>0,014
</p>
<p>0,015
</p>
<p>0,016
</p>
<p>0,017
</p>
<p>DUC 2005
</p>
<p>DUC 2006
</p>
<p>DUC 2007
</p>
<p>Moyenne
 
</p>
<p>R
O
</p>
<p>UG
E-
</p>
<p>2*
SU
</p>
<p>4 
(R
</p>
<p>a
pp
</p>
<p>e
l)
</p>
<p>&#8710;E (E
candidate-Ereference)
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>0 1 2 3 4 5 6 7 8 9 10
0,000
</p>
<p>0,002
</p>
<p>0,004
</p>
<p>0,006
</p>
<p>0,008
</p>
<p>0,010
</p>
<p>0,012
</p>
<p>0,014
</p>
<p>0,016
</p>
<p>Moyenne
</p>
<p>DUC 2007
</p>
<p>DUC 2006
</p>
<p>DUC 2005
</p>
<p>R
O
</p>
<p>UG
E-
</p>
<p>2*
SU
</p>
<p>4 
(ra
</p>
<p>pp
e
</p>
<p>l)
</p>
<p>k
</p>
<p>FIG. 2 &#8211; Diminution de la redondance : &#8710;E d&#8217;&#233;nergie des phrases et moyenne des longueurs de phrases.
</p>
<p>diversifier le contenu, consiste &#224; &#233;carter du r&#233;sum&#233; les phrases longues (dans les documents il y
a des phrases de taille &#8776; &#224; celle du r&#233;sum&#233; demand&#233;). On a d&#233;fini la taille maximale de phrase
comme k&#215;M o&#249; M = nombre moyen de mots par phrase dans les documents originaux. Nous
avons fait varier k par petits pas en mesurant &#224; chaque moment le produit de ROUGE-2 &#215; SU4.
Le comportement est montr&#233; sur la figure 2 &#224; droite. Le meilleur r&#233;sultat est avec k &#8776; 1, 6. Nous
avons fix&#233; k, puis le seuil d&#8217;&#233;nergie &#8710;E = 0, 003 en maximisant le produit ROUGE-2&#215;SU4.
En DUC&#8217;07 il y avait 2 baselines : la 1&#232;re est tir&#233;e au hasard. La 2&#232;me est un syst&#232;me de r&#233;-
sum&#233; g&#233;n&#233;rique. La figure 3 montre la position d&#8217;Enertex compar&#233; aux participants apr&#232;s les
campagnes DUC&#8217;05-07. Le cosinus obtient des performances ROUGE &#233;tonnament hautes, mais
qui peuvent donner lieu &#224; des r&#233;sum&#233;s avec beaucoup de redondance, car toutes les phrases se-
lectionn&#233;es sont proches de la th&#233;matique. Par contre l&#8217;&#233;nergie textuelle capture l&#8217;information
entre 2 phrases calcul&#233;&#233; parmi toutes les autres. De ce fait, la similarit&#233; tient en compte pas uni-
quement du nombre de mots partag&#233;s (le recouvrement et le cosinus sont des mesures locales)
mais des interactions indirectes (chemins de longueur 2).
</p>
<p>4 Fronti&#232;res th&#233;matiques
</p>
<p>Plusieurs strat&#233;gies ont &#233;t&#233; d&#233;velopp&#233;es pour segmenter th&#233;matiquement un texte. On trouve
PLSA (Brants et al., 2002) qui estime les probabilit&#233;s d&#8217;appartenance des termes &#224; des classes
s&#233;mantiques, des m&#233;thodes s&#8217;appuyant sur des mod&#232;les de Markov (Amini et al., 2000), sur
une classification des termes (Caillet et al., 2004; Chuang &amp; Chien, 2004) ou sur des cha&#238;nes
lexicales (Sitbon &amp; Bellot, 2005). Plus r&#233;cemment, (Ferret, 2007) a propos&#233; l&#8217;identification</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Silvia FERN&#193;NDEZ, Eric SANJUAN, Juan Manuel TORRES-MORENO
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>0,024 0,032 0,040 0,048 0,056 0,064 0,072
</p>
<p>0,056
</p>
<p>0,064
</p>
<p>0,072
</p>
<p>0,080
</p>
<p>0,088
</p>
<p>0,096
</p>
<p>0,104
</p>
<p>0,112
</p>
<p>0,120
</p>
<p>0,128
</p>
<p>0,136
</p>
<p>Cosine
</p>
<p>Baseline
</p>
<p>EnertexDUC 2005
</p>
<p>SU
4 
</p>
<p>R
a
</p>
<p>pp
e
</p>
<p>l
</p>
<p>ROUGE-2 Rappel
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>0,027 0,036 0,045 0,054 0,063 0,072 0,081 0,090
0,06
</p>
<p>0,07
</p>
<p>0,08
</p>
<p>0,09
</p>
<p>0,10
</p>
<p>0,11
</p>
<p>0,12
</p>
<p>0,13
</p>
<p>0,14
</p>
<p>0,15
</p>
<p>0,16
</p>
<p>Baseline
</p>
<p>Cosinus
</p>
<p>Enertex
DUC 2006
</p>
<p>SU
4 
</p>
<p>R
a
</p>
<p>pp
e
</p>
<p>l
ROUGE-2 Rappel
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>                                                    
</p>
<p>0,04 0,05 0,06 0,07 0,08 0,09 0,10 0,11 0,12
0,07
</p>
<p>0,08
</p>
<p>0,09
</p>
<p>0,10
</p>
<p>0,11
</p>
<p>0,12
</p>
<p>0,13
</p>
<p>0,14
</p>
<p>0,15
</p>
<p>0,16
</p>
<p>0,17
</p>
<p>0,18
</p>
<p>Baseline2
</p>
<p>Baseline1
</p>
<p>Cosinus
</p>
<p>Enertex
</p>
<p>DUC 2007
</p>
<p>SU
4 
</p>
<p>R
a
</p>
<p>pp
e
</p>
<p>l
</p>
<p>ROUGE-2 Rappel
</p>
<p>FIG. 3 &#8211; Aper&#231;u du rappel SU4 vs ROUGE-2 des participants au-dessus des deux baselines.
</p>
<p>pr&#233;alable des sujets pr&#233;sentes dans le document comme strat&#233;gie pour am&#233;liorer la d&#233;tection de
ruptures th&#233;matiques. L&#8217;identification de sujets est faite &#224; partir d&#8217;une analyse contextuelle ba-
s&#233;e sur la co-occurrence de mots. L&#8217;id&#233;e est que si deux segments n&#8217;ont pas une forte coh&#233;sion
lexicale entre eux, mais ils apparaissent dans le m&#234;me contexte, alors ils appartiennent au m&#234;me
sujet et la rupture th&#233;matique n&#8217;existe pas. Dans ce travil, nous avons utilis&#233; la matrice d&#8217;&#233;nergie
E (5). Chaque ligne de cette matrice produit un spectre qui r&#233;presente l&#8217;interaction de la phrase i
avec les autres. La figure 4 montre les spectres de quelques phrases d&#8217;un texte compos&#233; de deux
th&#233;matiques. &#201;tant donn&#233; que l&#8217;&#233;nergie textuelle d&#233;tecte et pond&#232;re le voisinage d&#8217;une phrase,
on constate une similarit&#233; entre les courbes de l&#8217;une (en gras) et de l&#8217;autre th&#233;matique (en poin-
till&#233;es). Pour comparer les spectres nous avons utilis&#233; (Fern&#225;ndez et al., 2007a) le coefficient
de concordance &#964; de Kendall (Siegel &amp; Castellan, 1988) et le calcul de sa p&#8722;valeur qui per-
mettent de d&#233;finir un test statistique de concordance entre 2 juges qui classent un ensemble de
P objets. Nous avons utilis&#233; ce test pour trouver les fronti&#232;res th&#233;matiques entre segments. Ces
ruptures entre segments sont bien d&#233;tect&#233;es si le voisinage commun entre les phrases est bien
rep&#233;r&#233;. Mais il se trouve que des phrases chevauchant les th&#233;matiques pr&#233;sentent des courbes
d&#8217;&#233;nergie que le test du &#964; de Kendall s&#8217;av&#232;re incapable de distinguer. C&#8217;est le cas du spectre de
la phrase 23 de la figure 4. Pour diminuer cet effet nous avons propos&#233; (Fern&#225;ndez et al., 2007b)
une variation du test de Kendall avec l&#8217;utilisation d&#8217;une fen&#234;tre glissante : la phrase centrale est
compar&#233;e aux autres dans la fen&#234;tre Cette strat&#233;gie a permis une meilleure d&#233;tection des rup-
tures. Mais nous pensons qu&#8217;on peut faire mieux. Dans ce travail nous introduisons une strat&#233;gie
portant directement sur la modification des spectres des courbes : le lissage par un param&#232;tre de
bruit &#946; qui peut &#234;tre assimil&#233;, en termes physiques, &#224; l&#8217;inverse d&#8217;une temp&#233;rature T .
</p>
<p>D&#233;croissance exponentielle : distance et temp&#233;rature
</p>
<p>La figure 4 montre que les spectres qui expriment correctement leur appartenance &#224; une th&#233;-
matique ont une forme d&#233;croissante par rapport &#224; un maximum. Ce maximum correspond &#224;
l&#8217;expression d&#8217;une forte interaction entre un couple de phrases. &#192; partir de ce point maximal,
les autres interactions diminuent rapidement jusqu&#8217;&#224; la fin de la th&#233;matique. Cette d&#233;croissance
de l&#8217;&#233;nergie textuelle peut &#234;tre contr&#244;l&#233;e avec un facteur exp&#8722;r/T o&#249; r est la distance entre la
phrase &#181; et la phrase voisine qui pr&#233;sente la plus haute interaction avec elle et T un param&#232;tre de
bruit pr&#233;sent dans les spectres6. La figure 5 montre le lissage induit dans les spectres pour deux
</p>
<p>6Dans la litt&#233;rature de r&#233;seaux de neurones on trouve souvent &#946; = 1/T</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Enertex : un syst&#232;me bas&#233; sur l&#8217;&#233;nergie textuelle
</p>
<p>FIG. 4 &#8211; &#201;nergie textuelle : en trait continu l&#8217;&#233;nergie des phrases de la 1&#232;re th&#233;matique, en pointill&#233; celle
de la 2&#232;me. Le changement d&#8217;allure correspond &#224; un changement th&#233;matique. L&#8217;axe horizontal indique le
num&#233;ro de phrase et l&#8217;axe vertical, l&#8217;&#233;nergie textuelle de la phrase par rapport aux autres.
</p>
<p>phrases de la figure 4 par le facteur exp&#8722;r/T (la phrase 23 est difficile &#224; classer en fonction de
ses pics). Nous avons diminu&#233; T progressivement afin d&#8217;analyser l&#8217;&#233;volution du chevauchement
des courbes. Cette diminution lisse les courbes de fa&#231;on efficace : &#224; T &#8776; 8 le bruit de la courbe
23 est r&#233;duit et un classement correct a &#233;t&#233; obtenu. Le spectre de la phrase 10 a aussi &#233;t&#233; liss&#233;
sans perte d&#8217;information. Nous faisons l&#8217;hypoth&#232;se que avec ce lissage, le test de concordance
</p>
<p>FIG. 5 &#8211; Lissage par exp&#8722;r/T des spectres. En trait continu le spectre d&#8217;une phrase th&#233;matiquement bien
d&#233;finie et en pointill&#233; celui d&#8217;une phrase inclassable. r est la distance au maximum et T la t&#233;mp&#233;rature.
</p>
<p>de Kendall identifiera mieux les phrases selon leur th&#233;matique. Mais quelle est la valeur cor-
recte de T ? Pour l&#8217;estimer nous avons r&#233;alis&#233; des exp&#233;riences sur des corpus multi-th&#233;matique
en anglais, espagnol et fran&#231;ais. Les corpus ont &#233;t&#233; construits &#224; partir d&#8217;articles journalistiques
du BROWN CORPUS, LA JORNADA et LE MONDE7. Les corpus comportent 4 ensembles de 100
documents qui correspondent &#224; une taille de segments fix&#233;e. Un document est constitu&#233; de 10
segments extraits d&#8217;articles th&#233;matiquement diff&#233;rents tir&#233;s au hasard. Pour chaque document
on a calcul&#233; l&#8217;&#233;nergie textuelle &#224; diff&#233;rents temp&#233;ratures : T = 1, &#183; &#183; &#183; , 180. Les spectres ont &#233;t&#233;
compar&#233;s par le test de Kendall et les fronti&#232;res d&#233;tect&#233;es ont &#233;t&#233; mesur&#233;s par Windiff (WD)
(Pevzner &amp; Hearst, 2002)8. Plus la valeur WD est basse, mieux la segmentation a &#233;t&#233; r&#233;alis&#233;e.
La figure 6 montre les r&#233;sultats de 100 documents en fran&#231;ais et une taille de segments de 6-8
phrases. En trait continu l&#8217;&#233;volution de la valeur moyenne de WD et en pointill&#233; le nombre
de fronti&#232;res trouv&#233;es. On observe qu&#8217;&#224; temp&#233;ratures tr&#232;s basses les courbes d&#8217;&#233;nergie perdent
leurs pics (sauf le maximum). Le test de Kendall ne d&#233;tecte plus de fronti&#232;res et la valeur WD
est &#233;lev&#233;e. En augmentant la temp&#233;rature les courbes voisines se ressemblente de plus en plus et
le nombre de fronti&#232;res augmente. Nous avons retenu la valeur T = 80 qui maximise le nombre
de fronti&#232;res trouv&#233;es en minimisant la valeur WD.
</p>
<p>7http ://khnt.aksis.uib.no/icame/manuals/brown, http ://www.jornada.unam.mx et http ://www.lemonde.fr
8Windiff mesure la diff&#233;rence entre les fronti&#232;res v&#233;ritables et celles trouv&#233;es dans une fen&#234;tre glissante.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Silvia FERN&#193;NDEZ, Eric SANJUAN, Juan Manuel TORRES-MORENO
</p>
<p>FIG. 6 &#8211; &#201;volution de WD et du nombre de fronti&#232;res en fonction de T . La ligne horizontale repr&#233;sente
la valeur de WD &#224; temp&#233;rature infinie. Taille des segment entre 6 et 8 phrases pour le corpus en fran&#231;ais.
</p>
<p>La mesure &#948;-Front
</p>
<p>(Pevzner &amp; Hearst, 2002) ont montr&#233; que WD est peu sensible aux variations de la taille de
segments et plus &#233;quilibr&#233; que d&#8217;autres mesures dans la p&#233;nalisation des erreurs. Cependant
elle a ses faiblesses. WD ne peut pas &#234;tre assimil&#233;e &#224; un taux d&#8217;erreur (sa valeur peut &#234;tre &gt;
1) et elle n&#8217;est qu&#8217;un &#233;l&#233;ment de comparaison de la fiabilit&#233; des m&#233;thodes et non un param&#232;tre
absolu de sa qualit&#233; (Sitbon &amp; Bellot, 2004). De plus, nous avons trouv&#233; qu&#8217;une m&#234;me valeur
de WD pouvait correspondre aux segmentations diff&#233;rentes du document. Nous introduisons
ici la mesure &#948;-Front qui calcule la distance euclidienne d(&#8226;) entre les vecteurs A et B de
dimension P (nombre des phrases du document) : A correspond aux fronti&#232;res v&#233;ritables et B
&#224; celles d&#233;tect&#233;es. La valeur de la composante i est le nombre de phrases s&#233;parant la phrase i
de la fronti&#232;re la plus proche (figure 7). Le facteur de normalisation est calcul&#233; avec le vecteur
nul : ne contenant aucune fronti&#232;re sauf les extr&#234;mes. Plus la valeur &#948;-Front est basse, mieux la
segmentation a &#233;t&#233; r&#233;alis&#233;e.
</p>
<p>&#948;-Front(A,B) = d(A,B)
d(A,C) (8)
</p>
<p>On observe au tableau 1 que la valeur de T pour la meilleur segmentation d&#233;pend de la longueur
</p>
<p>FIG. 7 &#8211; La mesure &#948;-Front.
</p>
<p>du document. Plus la taille du segment est grande (plus le document est long) plus la valeur T
est elev&#233;e. Les deux mesures ne sont pas toujours en accord. En fran&#231;ais WD obtient la valeur
la plus haute pour des segments de taille 9-11 et &#948;-Front pour 3-5. Cette diff&#233;rence peut &#234;tre due</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Enertex : un syst&#232;me bas&#233; sur l&#8217;&#233;nergie textuelle
</p>
<p>au nombre de v&#233;ritables fronti&#232;res trouv&#233;es : &#948;-Front consid&#232;re plus finement ce facteur. Les
m&#233;thodes cit&#233;s rapportent des meilleures performances en anglais qu&#8217;en fran&#231;ais, peut &#234;tre d&#251;
aux diff&#233;rences structurales et de r&#233;p&#233;tition de mots entre ces langues. Cependant nos r&#233;sultats
sont comparables dans les 3 langues. Cette stabilit&#233; d&#233;coule du calcul d&#8217;interactions des mots
combin&#233; au processus de comparaison de segmentes. (Ferret, 2007) constate en partie cet effet.
</p>
<p>Taille du T Fran&#231;ais Espagnol Anglais Nb. fronti&#232;res
segment WD &#948;-Front WD &#948;-Front WD &#948;-Front trouv&#233;es
</p>
<p>9-11 120 0,4109 0,1817 0,3897 0,2069 0,3925 0,1524 &#8776;6/9
6-8 80 0,3638 0,1957 0,3601 0,2031 0,3804 0,1640 &#8776;5/9
3-11 40 0,3885 0,1974 0,3646 0,2043 0,3709 0,1634 &#8776;5/9
3-5 20 0,3851 0,4540 0,3598 0,3257 0,3786 0,3864 &#8776;3/9
</p>
<p>TAB. 1 &#8211; Mesures WD et &#948;-Front pour des corpus en 3 langues et segments de tailles variables.
</p>
<p>5 Conclusions
</p>
<p>Nous avons pr&#233;sent&#233; le syst&#232;me Enertex bas&#233; sur le concept d&#8217;&#233;nergie textuelle. L&#8217;&#233;nergie tex-
tuelle est bien adapt&#233;e &#224; la recherche de segments porteurs d&#8217;information d&#8217;un texte et &#224; sa pon-
d&#233;ration. L&#8217;extraction et l&#8217;assemblage de ces segments donne le condens&#233; d&#8217;un document. Nous
avons &#233;largi la port&#233;e de cet id&#233;e pour d&#233;velopper un algorithme de r&#233;sum&#233; multi-document
guid&#233; par une th&#233;matique : un champ externe, repr&#233;sent&#233; par le vecteur des termes d&#233;crivant
une th&#233;matique a &#233;t&#233; mis en r&#233;lation avec les phrases d&#8217;un corpus multi-document. Ceci a per-
mis de g&#233;n&#233;rer des r&#233;sum&#233;s personnalis&#233;s que nous avons evalu&#233; dans le cadre des t&#226;ches DUC.
La position d&#8217;Enertex est excellente par rapport &#224; la trentaine de participants compte tenu que
l&#8217;&#233;nergie textuelle est exprim&#233;e comme un simple produit matriciel. Aucune autre mesure de
pond&#233;ration de phrases a &#233;t&#233; incluse. En segmentation th&#233;matique, nous avons am&#233;lior&#233; la d&#233;-
tection de fausses fronti&#232;res au travers d&#8217;une fonction pilot&#233;e par une t&#233;mperature. Cela &#224; permis
de surpasser nos r&#233;sultats pr&#233;c&#233;dents. Compte tenu des faiblesses de WD, nous avons introduit
&#948;-Front, une nouvelle mesure d&#8217;&#233;valuation de segmentation th&#233;matique. Nous envisageons de
tester le mod&#232;le de Potts, autre mod&#232;le d&#8217;interaction entre spins qui favorise l&#8217;interaction entre
mots de m&#234;me fr&#233;quence, ainsi qu&#8217;une &#233;tude des chemins de longueur &gt; 2 dans le graphe (int&#233;-
ractions d&#8217;ordre &#8805; 3). Des applications en classification de textes sont aussi envisag&#233;es.
</p>
<p>R&#233;f&#233;rences
AMINI M.-R., ZARAGOZA H. &amp; GALLINARI P. (2000). Learning for sequence extraction
tasks. In RIAO 2000, p. 476&#8211;489.
BRANTS T., CHEN F. &amp; TSOCHANTARIDIS I. (2002). Topic-based document segmentation
with probabilistic latent semantic anaysis. In CIKM&#8217;02, p. 211&#8211;218, McLean, Virginia, USA.
CAILLET M., PESSIOT J.-F., AMINI M. &amp; GALLINARI P. (2004). Unsupervised learning
with term clustering for thematic segmentation of texts. In RIAO&#8217;04, p. 648&#8211;657, France.
CHUANG S.-L. &amp; CHIEN L.-F. (2004). A practical web-based approach to generating Topic
hierarchy for Text segments. In 30th ACM IKM, p. 127&#8211;136, Washington DC, USA.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Silvia FERN&#193;NDEZ, Eric SANJUAN, Juan Manuel TORRES-MORENO
</p>
<p>DA CUNHA I., FERN&#193;NDEZ S., VEL&#193;ZQUEZ MORALES P., VIVALDI J., SANJUAN E. &amp;
TORRES MORENO J. M. (2007). A new hybrid summarizer based on Vector Space model,
Statistical Physics and Linguistics. In LNAI 4287, MICAI&#8217;07, Mexico, p. 872&#8211;882.
FERN&#193;NDEZ S., SANJUAN E. &amp; TORRES-MORENO J. M. (2007a). Energie textuelle des
m&#233;mories associatives. In TALN 2007, p. 25&#8211;34.
FERN&#193;NDEZ S., SANJUAN E. &amp; TORRES-MORENO J. M. (2007b). Textual Energy of As-
sociative Memories : performants applications of ENERTEX algorithm in text summarization
and topic segmentation. In LNAI 4287, MICAI&#8217;07, Mexico, p. 861&#8211;871.
FERRET O. (2007). Finding document topics for improving topic segmentation. In ACL&#8217;07,
p. 480&#8211;487.
HERTZ J., KROGH A. &amp; PALMER G. (1991). Introduction to the theorie of Neural Computa-
tion. Redwood City, CA : Addison Wesley.
HOPFIELD J. (1982). Neural networks and physical systems with emergent collective compu-
tational abilities. Proceedings of the National Academy of Sciences of the USA, 9, 2554&#8211;2558.
HOVY E., LIN C. &amp; ZHOU L. (2005). Evaluating DUC 2005 using Basic Elements. In DUC
2005.
LIN C.-Y. (2004). ROUGE : A Package for Automatic Evaluation of Summaries. In Text
Summarization Branches Out : ACL-04 Workshop, p. 74&#8211;81, Spain.
MA S. (1985). Statistical Mechanics. Philadelphia, CA : World Scientific.
MANDELBROT B. (1953). An informational theory of the statistical structure of languages. In
Communication Theory, ed. By Willis Jackson, p. 486&#8211;502, New York : Academic Press.
MANNING C. D. &amp; SCH&#220;TZE H. (1999). Foundations of Statistical Natural Language Pro-
cessing. Cambridge, Massachusetts : The MIT Press.
MCKEOWN K. &amp; RADEV D. (1995). Generating summaries of multiple news articles. In
18th ACM SIGIR, p. 74&#8211;82.
PASSONNEAU R., NENKOVA A., MCKEOWN K. &amp; SIGLEMAN S. (2005). Applying the
Pyramid Method in DUC 2005.
PEVZNER L. &amp; HEARST M. (2002). A critique and improvement of an evaluation metric for
text segmentation. In Computational Linguistic, volume 1, p. 19&#8211;36.
PORTER M. (1980). An algorithm for suffix stripping. Program, 14(3), 130&#8211;137.
SALTON G. &amp; MCGILL M. (1983). Introduction to modern information retrieval. Computer
Science Series McGraw Hill Publishing Company.
SHANNON C. (1948). A mathematical theory of communication. Bell System Technical Jour-
nal, 27, 79&#8211;423, 623&#8211;656.
SIEGEL S. &amp; CASTELLAN N. (1988). Nonparametric statistics for the behavioral sciences.
McGraw Hill.
SITBON L. &amp; BELLOT P. (2004). Evaluation de m&#233;thodes de segmentation th&#233;matique lin&#233;aire
non supervis&#233;es apr&#232;s adaptation au fran&#231;ais. In TALN 2004, p. 10&#8211;19.
SITBON L. &amp; BELLOT P. (2005). Segmentation th&#233;matique par cha&#238;nes lexicales pond&#233;r&#233;es.
In TALN 2005, volume 1, p. 505&#8211;510.
TAKAMURA H., INUI T. &amp; MANABU O. (2005). Extracting semantic orientations of words
using spin model. In ACL&#8217;05, p. 133&#8211;140.
ZIPF G. (1935). Psycho-biology of languages. Houghton-Mifflin, Boston, MA.
ZIPF G. (1949). Human behavior and the principle of least effort. Addison-Wesley, MA.</p>

</div></div>
</body></html>