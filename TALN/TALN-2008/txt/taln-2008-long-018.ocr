TALN 2008, Avignon, 9-13 juin 2008

Construction d’un wordnet libre du francais
a partir de ressources multilingues

Benoit Sagotl Darja Fiserz
(1) Alpage, INRIA / Paris 7, 30 rue du Ch. des rentiers, 75013 Paris, France
(2) Fac. des Lettres, Univ. de Ljubljana, Askeréeva 2, 1000 Ljubljana, Slovénie
benoit.sagot@inria.fr, darj a.ﬁser@ guest.arnes.si

Résumé. Cet article décrit la construction d’un Wordnet Libre du Frangais (WOLF) a
partir du Princeton WordNet et de diverses ressources multilingues. Les lexemes polysémiques
ont été traités au moyen d’une approche reposant sur l’alignement en mots d’un corpus parallele
en cinq langues. Le lexique multilingue extrait a été désambiguisé sémantiquement a l’aide
des wordnets des langues concernées. Par ailleurs, une approche bilingue a été sufﬁsante pour
construire de nouvelles entrées a partir des lexemes monosémiques. Nous avons pour cela extrait
des lexiques bilingues a partir de Wikipédia et de thésaurus. Le wordnet obtenu a été évalué par
rapport au wordnet frangais issu du projet EuroWordNet. Les résultats sont encourageants, et
des applications sont d’ores et déja envisagées.

Abstract. This paper describes the construction of a freely-available wordnet for French
(WOLF) based on Princeton WordNet by using various multilingual resources. Polysemous
words were dealt with an approach in which a parallel corpus for ﬁve languages was word-
aligned and the extracted multilingual lexicon was disambiguated with the existing wordnets
for these languages. On the other hand, a bilingual approach sufﬁced to acquire equivalents
for monosemous words. Bilingual lexicons were extracted from Wikipedia and thesauri. The
merged wordnet was evaluated against the French WordNet. The results are promising, and
applications are already intended.

M0tS-CléS I Wordnet, corpus alignés, Wikipédia, sémantique lexicale.

Keywords: Wordnet, aligned corpora, Wikipedia, lexical semantics.

1 Introduction

Wordnet (Fellbaum, 1998) est une base de données lexicale a large couverture, dans laquelle
les mots sont répartis en catégories et organisés en une hiérarchie de noeuds. Chaque noeud a un
identiﬁant unique, et représente un concept, ou synset (ensemble de synonymes). I1 regroupe un
certain nombre de lexemes synonymes dénotant ce concept. Ainsi, dans le Princeton WordNet
(voir ci-dessous), le synset ENG20-02853224-n comprend les lexemes {car, auto, automobile,
machine, motorcar}. Les synsets sont précisés par une breve déﬁnition et sont liés a d’autres
synsets (ainsi, ce synset est lié au synset {motor vehicle, automotive vehicle} par un lien d’hypé-
ronymie, et au synset {cab, hack, taxi, taxicab} par un lien d’hyponyInie). Les lexemes peuvent
étre simples ou composés. Les usages métaphoriques et idiomatiques sont pris en compte.

Benoit Sagot, Darja Fiser

Historiquement, le premier wordnet est le Princeton WordNet (PWN), développé pour l’anglais
a la Princeton University. Au ﬁl du temps, il est devenu une des ressources les plus utiles pour de
nombreuses applications de compréhension et d’interprétation automatique des langues, telles
que la désambiguisation sémantique, l’extraction d’informations, la traduction automatique, la
classiﬁcation de documents et le résumé de textes. Ceci a provoqué le développement de word-
nets pour de nombreuses autres langues (Vossen, P., 1999; Tuﬁs, 2000).

A l’heure actuelle, la Global WordNet Associationl répertorie des wordnets pour plus de 50
langues. Bien que la construction manuelle d’un wordnet produise les meilleurs résultats en
termes de pertinence et de précision linguistiques, une telle entreprise est tres coﬁteuse en temps
et en ressources. C’est pourquoi des approches seIr1i-automatiques ou totalement automatiques
ont été proposées, qui tirent parti des ressources existantes. Mis a part le « goulot d’étranglement
de l’acquisition de connaissance » (knowledge acquisition bottleneck), un probleme majeur au
sein de la communauté wordnet est la disponibilité des wordnets développés. Actuellement,
seuls quelques-uns d’entre eux sont librement disponibles (le PWN, mais également les word-
nets de l’arabe, de l’hébreu et de l’irlandais). Bien qu’un wordnet pour le francais ait été déve-
loppé dans le cadre du projet EuroWordNet (EWN), il ne comporte que des lexemes verbaux et
nominaux, mais ni adjectif ni adverbe. De plus, il n’a pas été largement utilisé, principalement
en raison de problemes de licence. Enﬁn, aucun projet n’a pris le relais pour poursuivre l’ex-
tension et l’amélioration de cet EWN francais (J acquin et al., 2007). Ce sont la les motivations
du travail présenté dans cet article : exploiter les ressources multilingues librement disponibles
pour construire automatiquement un wordnet a large couverture et librement disponible pour le
francais, nommé WOLF (Wordnet Libre du Francais)2.

Cet article est organise comme suit : une breve description des travaux antérieurs est foumie a
la section suivante. La section 3 décrit la méthodologie utilisée. La section 4 décrit et évalue la
ressource obtenue. La derniere section présente les conclusions et les perspectives envisagées.

2 Travaux antérieurs

On peut répartir les techniques automatiques de développement ou d’aide au développement
de wordnets en deux familles : les approches par fusion (merge approach) et les approches par
extension (extend approach). Les approches par fusion, ou l’on constr11it un wordnet a partir de
ressources monolingues avant de le mettre en relation avec d’autres wordnets, sont tres com-
plexes et coﬁteuses en ressources. Nous avons donc opté pour l’approche par extension (Vossen,
P., 1999). Cette approche prend en entrée un ensemble donné de synsets du Princeton WordNet
(PWN) et les traduit dans la langue cible, en préservant la structure du PWN. L’ inconvénient de
l’approche par extension est que les wordnets obtenus sont biaisés par rapport au PWN, ce qui
est moins problématique lorsque les langues source et cible ne sont pas trop éloignées l’une de
l’autre, comme c’est le cas pour l’anglais et le francais. C’est en raison de sa grande simplicité
que l’approche par extension a été utilisée dans nombre de projets, tels que BalkaNet (Tuﬁs,
2000) et MultiWordNet (Pianta et al., 2002).

Les équipes de recherche développant des wordnets de cette facon tirent parti de toutes les
ressources a leur disposition, qu’il s’agisse de dictionnaires électroniques bilingues ou mono-
lingues utilisés pour la désambiguisation et la structuration du lexique ou bien de taxonomies

lhttpz//www.globalwordnet.org/
2Le WOLF est disponible sous licence Cecill—C (compatible LGPL), at http : / /wol f . gforge . inria . fr

Construction d’un wordnet libre du francais a partir de ressources multilingues

et d’ontologies qui foumissent en général une description plus détaillée et plus formalisée des
termes. Le wordnet francais développé dans le cadre du projet EuroWordNet a été construit
ainsi : un sous-ensemble des synsets du PWN a été automatiquement traduit au moyen d’une
base de données sémantique multilingue propriétaire, puis validé manuellement.

Pour la construction du WOLF, nous avons exploité trois types de ressources librement dispo-
nibles : le corpus parallele multilingue J RC-Acquis, Wikipédia et d’autres ressources wiki, ainsi
que les descripteurs EUROVOC (utilisé dans le thésaurus européen EUROVOC). Des traduc-
tions pour les lexemes monosémiques, qui ne requierent pas de désambiguisation sémantique,
ont été extraites des ressources wiki et EUROVOC. Le corpus parallele a été utilisé pour obtenir
des informations sémantiquement pertinentes a partir de la relation multilingue de traduction,
aﬁn de traiter également les lexemes polysémiques. L’idée que des informations sémantiques
peuvent étre extraites de la relation de traduction a été déja explorée par (Resnik & Yarowsky,
1997; Ide et al., 2002; Diab, 2004). Elle a également déja donné des résultats prometteurs pour
la construction de synsets pour le wordnet slovene (Fiser, 2007).

3 Méthodologie

3.1 Présentation générale

La difﬁculté principale lors de la construction d’un wordnet est la polysémie du vocabulaire de
base. Dans ce travail, nous avons traité cette question par l’approche par alignement. L’ idée re-
pose sur l’hypothese que les différents sens des mots ambigus dans une langue donnée donnent
souvent lieu a des traductions différentes dans une autre langue. A l’inverse, nous supposons
que si deux mots ou plus sont traduits par le meme mot dans une autre langue, ils partagent
souvent un élément de sens. En outre, ces phénomenes sont renforcés par l’utilisation de plus
de deux langues, d’o1‘1 l’intérét d’une approche par alignement multilingue. La méthode repose
sur le lexique multilingue extrait a partir d’un corpus aligné automatiquement en mots, et uti-
lise les wordnets des autres langues pour désambiguiser les entrées lexicales et leur assigner
un identiﬁant de sens. Les relations sémantiques entre synsets sont directement récupérées du
PWN (version 2.0, celle utilisé par BalkaNet).

Cependant, cette approche est limitée au vocabulaire du corpus, et il n’est pas réaliste d’espérer
la disponibilité de corpus alignés pour un large éventail de domaines dans le but d’accroitre
la couverture. Heureusement, tous les lexemes du PWN ne sont pas polysémiques : parmi les
145 627 lexemes du PWN, 82% n’apparaissent que dans un seul synset. Ces lexemes ne neces-
sitent pas une technique aussi élaborée que celle décrite précédemment : ils peuvent étre traduits
directement a l’aide de ressources bilingues, construisant ainsi par correspondance directe de
nouveaux synsets sans que des erreurs de sens ne soient a craindre.

3.2 Approche par alignement

Pour l’approche par alignement, nous avons utilisé le corpus SEE-ERA.NET3, sous-corpus du
corpus JRC-Acquis (Ralf et al., 2006) aligné en phrases. Parmi les 8 langues du corpus, et Inis

3Corpus créé au sein du projet SEE—ERA.NET ICT 10503 RP (2007-2008), Building language Resources and
Translation Models for Machine Translation focused on South Slavic and Balkan languages.

Benoit Sagot, Darja Fiser

a part le francais, nous avons utilisé les langues pour lesquelles nous disposions de toutes les
ressources nécessaires : l’anglais, le roumain, le tcheque et le bulgare. Ce corpus fait environ 1
million et demi de mots, le chiffre exact variant d’une langue a l’autre. Lors des travaux décrits
dans cet article, ce corpus n’avait pas encore été annoté morphosyntaxiquement dans le cadre
du projet SEE-ERA.NET. Nous avons donc utilisé TreeTagger4 pour étiqueter et lemmatiser le
corpus. Les ﬁchiers de parametres par défaut ont été utilisés pour l’anglais et le francais, alors
que les ﬁchiers pour le roumain et le bulgare ont été entrainés sur le corpus MULTEXT-East5.
Pour le tcheque, nous avons utilisé l’analyseur morphologique FMorph pour annoter le corpus
de facon ambigue, puis l’étiqueteur Morce pour désambigu'1'ser cette annotation (Hajic, 2008).

Le corpus a été aligné en mots par Uplug (Tiedemann, 2003), qui repose sur l’outil standard
GIZA++. Pour augmenter la qualité de l’alignement, l’anglais n’a pas été utilisé comme langue
pivot : nous avons aligné des paires de langues appartenant a la méme famille (francais-roumain
et tcheque-bulgare), puis nous avons utilisé l’anglais comme pont (francais-anglais et tcheque-
anglais). Le résultat de l’alignement est un ensemble de liens entre mots qui, dans une phrase
donnée, sont en relation de traduction. Chacun de ces liens est complété d’identiﬁants uniques
(numéro de phrase, numéro de mots) et d’un indicateur du degré de conﬁance qui lui est attribué.

Les identiﬁants des mots ont permis de remplacer, dans ces liens, les formes ﬂéchies par les
lemmes tels que foumis par les lemmatiseurs. Aﬁn de réduire le bruit dans les lexiques obtenus,
seuls les liens entre lexemes de méme partie du discours ont été pris en compte, et toutes les
entrées contenant des caracteres autres que des lettres ont été éliminées. Les lexiques bilingues
obtenus contiennent toutes les variantes de traduction d’un lexeme source dans le corpus, assor-
ties d’informations de fréquence, de partie du discours, ainsi que des identiﬁants des phrases et
mots concernés. La taille de ces lexiques va de 43 024 entrées pour le lexique tcheque-anglais
a 50 289 entrées pour le lexique tcheque-bulgare.

Les lexiques bilingues extraits ont été alors combinées pour créer cinq lexiques multilingues
comportant de trois a cinq langues. Les lexemes anglais et leurs identiﬁants de mots ont été
utilisés comme pivots. La taille des lexiques multilingues va de 49 356 entrées pour le lexique
regroupant toutes les langues (francais-roumain-tcheque-bulgare-anglais) a 59 019 entrées pour
le lexique francais-tcheque-bulgare-anglais6.

Une fois les lexiques obtenus, il reste a associer un synset a chacune de leurs entrées. Pour cela,
nous recherchons pour chaque composant d’une entrée donnée, a l’exception du lexeme en
francais, l’ensemble des identiﬁants des synsets auxquels il appartient. Nous avons utilisé pour
cela le PWN 2.0 et les wordnets développés dans le cadre du projet BalkaNet (Tuﬁs, 2000), qui
utilisent les mémes identiﬁants de synsets que le PWN 2.0. On calcule alors, pour chaque entrée
de lexique multilingue, l’intersection des ensembles d’identiﬁants de synsets associés aux dif-
férents composants de l’entrée. Si l’intersection est non vide, les synsets qu’elle contient sont
attribués au lexeme francais de l’entrée. L’utilisation de plusieurs langues permet de désambi-
gu'1'ser les lexemes polysémiques et élimine la plupart des erreurs d’alignement. Il est en effet
peu probable qu’une meme polysémie se retrouve dans de nombreuses langues différentes, ou
qu’une erreur d’alignement induise une intersection non vide.

4http://www.ims.uni—stuttgart.de/projekte/corplex/TreeTagger/

5http://nl.ijs.si/ME/

5Nous utilisons une option du logiciel d’alignement Uplug qui ne produit que les liens les plus vraisemblables.
C’est pour cette raison que le nombre d’entrées varie peu lorsque le nombre de langues augmente : Paugmentation
a laquelle on pourrait s’attendre est compensée par la diminution du nombre d’entrées qui incluent un lexéme pour
chacune des langues.

Construction d’un wordnet libre du francais a partir de ressources multilingues

| ﬁq | pos| Eh | Cs | Bg | En | I
n 3aK0H0AaTencTB0 | Fr:dIo1t | Cs:prévo | Bgzripano | Enzlaw |

npano
3a.KoH
3a.KOH0,I[a.TeJ'ICTBO

-n

05791721-n 07928837-n 05531141-n
-n

3a.KOH

3a.KoHa

3a.KOH0,I[a.Te.J'ICTBO

-n -n
-n -n
-n

 

H
H
H
H
H
H
H
H
H

 

3aKOH0ﬂaTeHCTBO

TAB. 1 — Exemple d’entrées lexicales et de désambiguisation pour le nom droit (le lexeme bul-
gare identiﬁé par une étoile résulte d’une erreur de lemmatisation; la partie commune ENG20
a été omise dans les identiﬁants de synsets).

Illustrons ce processus sur un exemple. Soit le nom francais droit, qui est polysémique (il
peut ainsi étre traduit en anglais, entre autres, par right, law, droit, royalty, entitlement, claim).
Comme le montre la table 1(a), 56 de ses occurrences ont été alignées avec prdvo en tcheque,
npaBo en bulgare et law en anglais. L’intersection des ensembles d’identiﬁants de synsets
contenant chacun de ces mots dans le wordnet correspondant ne contient qu’un seul identiﬁant
de synset, ENG20-05791721-n. Il est donc attribué aux occurrences correspondantes du mot
droit (cf. table 1(b)). Ce synset, déﬁni dans le PWN comme déﬁnissant the branch of philoso-
phy concerned with the law and the principles that lead courts to make the decisions they do,
correspond bien a un des sens de droit.

Appliquée aux lexiques multilingues décrits précédemment, cette technique nous a permis de
construire cinq ensembles différents de synsets comportant au moins un lexeme francais. Ils
contiennent entre 1 338 (francais-roumain-tcheque-bulgare-anglais) et 5 073 synsets (francais-
roumain-anglais). En raison des erreurs induites par les phases de pré-traitement (étiquetage,
lemmatisation) et d’alignement, on s’attend a ce que ces synsets comportent certaines erreurs.
Cependant, cette approche permet de traiter les lexemes polysémiques, fréquents dans le voca-
bulaire de base, ce que l’approche par traduction, décrite ci-dessous, ne permet pas.

3.3 Approche par traduction

Wikipédia (ht tp : / /www . wikipedia . org) est une encyclopédie collaborative et multi-
lingue. Elle contient actuellement plus de deux millions d’articles en anglais, et plus de 600 000
en francais. Les articles écrits dans différentes langues sont reliés entre eux a l’aide de liens
inter-langues, qui permettent l’extraction d’informations multilingues. L’idée de faire corres-
pondre des articles de Wikipédia aux synsets d’un wordnet n’est pas nouvelle, eta été utilisée a
diverses ﬁns (Declerck et al., 2006; Ruiz-Casado et al., 2005). Nous l’avons appliquée au PWN
pour la création de nouveaux synsets dans le WOLF.

Comme dans toute encyclopédie, les titres des articles de Wikipédia sont principalement des
noms (communs et propres), simples ou composés. Leur casse a été normalisée automatique-
ment a partir du corps des articles. Nous avons alors extrait un lexique bilingue a partir des
titres anglais et de leurs équivalents francais en suivant les liens inter-langues. Le lexique ob-
tenu comporte 314 713 entrées. Nous l’avons enrichi par des synonymes et des déﬁnitions a
l’aide de la premiere phrase de chaque article7.

7Par exemple, Particle intitulé langue construite débute par la phrase suivante : Une langue construite ou

Benoit Sagot, Darja Fiser

Une autre ressource que nous avons utilisée pour extraire des équivalents de traduction pour les
lexemes monosémiques du PWN est Wiktionary (http : / /www . wi kt ionary . org). Com-
plément lexical de Wikipédia, Wiktionary existe aussi pour différentes langues. Chaque Wik-
tionary contient pour chaque mot répertorié une déﬁnition ainsi que différentes informations
complémentaires, telles que son étymologie, sa prononciation, des exemples, des synonymes
et antonymes, et, plus important pour nous, des traductions dans d’autres langues. Les Wik-
tionaries anglais et francais (« le Wiktionnaire », http : / /www . wiktionary . org) nous
ont permis d’extraire deux lexiques bilingues de 24 464 et 24 873 entrées respectivement. Ces
lexiques couvrent toutes les parties du discours, contrairement a Wikipédia.

Nous avons également exploité Wikispecies, taxonomie des especes vivantes qui inclut a la fois
les noms des especes en latin, que l’on retrouve dans le PWN, et (pour les plus communes) en
anglais ou en francais. Ceci nous a permis d’identiﬁer 129 509 termes latins indépendants de la
langue ainsi qu’un équivalent francais pour 2 648 d’entre eux.

Enﬁn, les descripteurs EUROVOC (http : / / europa . eu/ eurovoc) font partie d’un the-
saurus multilingue qui couvrent les champs d’activité de l’Union Européenne. Ils sont dispo-
nibles dans les 21 langues ofﬁcielles de l’Union. La version 4.2 du thésaurus contient une liste
de 6 802 descripteurs et de leurs équivalents dans les autres langues. En revanche, ils recouvrent
une large variété de themes, et incluent de nombreuses expressions multi-mots.

Puisque nous avons utilisé ces ressources pour traduire les lexemes monosémiques du PWN,
l’identiﬁcation du sens (c’est-a-dire de l’identiﬁant du synset) de leur équivalent francais est
triviale. Wikipédia a permis la construction de 18 721 synsets, Wikispecies de 6 848 synsets, le
Wiktionnaire francais de 6 215 synsets, le Wiktionary anglais de 4 363 synsets et les descripteurs
EUROVOC de 1 319 synsets.

3.4 Fusion des résultats

L’ ensemble des synsets obtenus par les deux approches ont été fusionnés. Si un meme synset a
été créé via plusieurs approches, ou au moyen de plusieurs sources (ensemble de langues pour
l’approche par alignement, ressource lexicale pour l’approche par traduction), les ensembles
de lexemes sont uniﬁés. L’information sur les sources d’o1‘1 provient chacun des lexemes est
conservée, pour permettre un ﬁltrage en fonction de leur nombre de la ﬁabilité de chacune
d’entre elles. En effet, les différentes sources de lexemes ne donnent pas des résultats homo-
genes en termes de précision (cf. section 4). Si un lexeme est associé a un identiﬁant de synset
par tous les lexiques multilingues, et que la fréquence des entrées correspondantes dans ces
lexiques est élevée, le résultat est ﬁable. Si a l’inverse un lexeme n’est affecté a un synset que
par un seul des lexiques multilingues (probablement par un lexique trilingue) et que l’entrée
correspondante a une fréquence faible, alors le résultat est douteux. Nous avons donc Inis en
place un ﬁltre pour éliminer les lexemes les plus douteux, en nous fondant sur le nombre de
sources, la nature des sources, et la fréquence des entrées correspondantes. Enﬁn, d’autres in-
formations indépendantes de la langue (domaine, relations sémantiques) ont été héritées des
synsets du PWN.

langue artiﬁcielle (étymologiquement « faite par I ’art ») est une langue créée par une ou plusieurs personnes
dans un temps relativement breﬁ contrairement aux langues naturelles dont I elaboration est largement incons-
ciente. Ce texte, y compris les informations typographiques, nous on permis de considérer langue construite, mais
également langue artiﬁcielle, comme des lexémes associés au concept décrit par 1’artic1e, et d’extraire la déﬁnition
correspondante, destinée a compléter le futur synset : langue cre’e’e (. . .) largement inconsciente.

Construction d’un wordnet libre du francais a partir de ressources multilingues

La construction automatique de synsets conduit inévitablement a des trous dans la hiérarchie, un
certain nombre de synsets n’ayant été attribués a aucun lexeme francais par manque de couver-
ture des ressources de départ. En raison de l’importance des principes de densité conceptuelle
et de préservation de la hiérarchie pour les applications des wordnets (Tuﬁs, 2000), nous avons
récupéré les synsets manquants du PWN, en en éliminant les lexemes anglais, mais en conser-
vant toutes les relations structurelles et les autres informations indépendantes de la langue. Ces
synsets vides seront remplis par des travaux futurs, mais l’intérét est que l’ontologie obtenue est
dense, en ce sens qu’elle ne contient aucun trou structurel. Si une application utilisant WOLF
était amenée a rencontrer un de ces synsets vides, elle pourrait malgré tout utiliser les inforrna-
tions relationnelles pour accéder a un concept plus général ou plus spéciﬁque.

4 Résultats et évaluation

4.1 Résultats

Nous avons étudié les caractéristiques de la ressource obtenue, le WOLF, par rapport au PWN
et a l’EWN francais. L’ evaluation a été effectuée par rapport a l’EWN francaiss.

Le WOLF contient actuellement 32 351 synsets non vides regroupant 38 001 lexemes distincts.
Ce nombre est bien plus élevé que le nombre de synsets de l’EWN francais (22 1219), en raison
des résultats de l’approche par traduction : rappelons que le PWN contient 115 424 synsets
pour 145 627 lexemes, dont 82% sont monosémiques, soit 119 528 lexemes que nous avons pu
essayer de traduire directement. Toutefois, ces synsets ne sont pas toujours les plus intéressants.

Pour évaluer la couverture du WOLF sur les concepts de base, nous nous sommes appuyés sur
les BCS (Basic Concept Sets, Ensembles de Concepts de Base) déﬁnis par le projet BalkaNet
(Tuﬁs, 2000). Les synsets de base sont répartis en trois niveaux de BCS : BCS1 rassemble les
1 218 concepts les plus importants, BCS2 regroupe 3 471 autres concepts importants, et BCS3
inventorie 3 827 autres concepts relativement importants. Tous les autres concepts sont hors
BCS. Outre la couverture du WOLF, les résultats présentés a la table 2, ces chiffres valident a
posteriori la déﬁnition de ces 3 BCS, les synsets BCS1 étant les rnieux couverts (71,4%). Le
détail par approche montre sans surprise que l’approche par alignement a permis la construction
des synsets les plus fondamentaux, alors que l’approche par traduction, restreinte aux lexemes
monosémiques, a fait rnieux pour BCS3 et hors BCS que pour BCS1 et BCS2.

Contrairement a WOLF, l’EWN francais ne comporte ni adjectifs ni adverbes. Les résultats
par partie du discours sont donnés a la table 2. La polysémie moyenne est de 1,23% synsets
par lexeme (10,5% des lexemes sont polysémiques, dont 1,2% des lexemes composés1°). Ces

8L’EWN frangais n’est utilise que pour l’éVa1uation. Aucune information issue de cette ressource n’a été utilisée
dans le WOLF, ni pour y etre intégrée, ni pour éliminer des lexemes erronés du WOLF. Du reste, tous les scripts
ayant servi a construire le WOLF sont librement disponibles sur son site. Les ressources d’entrée étant également
librement disponibles, tout un chacun peut reconstr11ire le WOLF tel qu’il est aujourd’hui. Nous envisageons bien
sur un travail manuel de Validation et de completion dans l’aVenir, mais la mise a disposition de ﬁchiers décrivant
chaque modiﬁcation manuelle permettra de perpétuer la reproductibilité de la construction du WOLF.
9L’EWN frangais comporte en réalité 22 857 synsets, mais ils ne sont pas en correspondance biunivoque avec
les synsets du PWN 2.0 (Contrairement au WOLF ou aux wordnets du projet Ba1kaN et). Une fois rnis en corres-
pondance via un mapping entre PWN 1.5 et PWN 2.0, il arrive que plusieurs synsets d’EWN correspondent a un
seul synset du PWN 2.0. Les chiffres que nous donnons sont pour l’EWN frangais aprés ce mapping.
1°Les lexemes composes sont tous obtenus par traduction de lexemes anglais monosémiques. Mais parfois, deux

Benoit Sagot, Darja Fiser

| wordnet | PWN | WOLF | EWN francais |
BCS1 1 218 870 71,4% 1 211 99,4%
BCS2 3 471 1 668 48,0% 3 022 87,1%
BCS3 3 827 1 801 47,1% 2 304 60,2%
hors BCS 106 908 28 012 26,2% 15 584 14,6%
synset nominal 79 689 25 559 35,8% 17 381 21,8%
synset Verbal 13 508 1 544 11,5% 4 740 35,1%
synset adjectival 18 563 1 562 8,4% 0 0,0%
synset adverbial 3 664 676 18,4% 0 0,0%

| total | 115,424 | 32,351 28.0% | 22,121 19.2% |

TAB. 2 — Comparaison du nombre de synsets du WOLF et de l’EWN francais par rapport au
PWN, par BCS et par partie du discours.

| || WOLF/align | WOLF/transl | WOLF 1
|  Précision | Rappel  Précision | Rappel  Précision | Rappel |
noms 77,2% 68,7% 82,6% 74,9% 80,4% 74,5%
verbes 65,8% 54,7% 54,8% 35,8% 63,2% 52,5%
total 74,6% 65,4% 78,8% 69,6% 77,1% 70,3%

TAB. 3 — Evaluation par rapport a l’EWN francais.

chiffres sont a comparer aux 1,74 synsets par lexeme de l’EWN francais (1,39 dans le PWN).

4.2 Evaluation par rapport £1 l’EWN frangais

Nous avons effectué deux types d’évaluation. Tout d’abord, nous avons compare automatique-
ment, pour chaque lexeme présent dans WOLF et dans l’EWN francais, les synsets qui lui
sont attribués. Cette évaluation n’a pu se faire que sur les noms et les verbes, seules parties du
discours présentes dans l’EWN francais. De plus, les synsets absents de l’EWN francais mais
présents dans WOLF sont inévitablement laissés de cote. Puis nous avons étudié manuellement
quelques cas, pour identiﬁer les sources de non-correspondance entre WOLF et l’EWN francais.

Le tableau 3 présente les résultats de l’évaluation automatique. La précision est le pourcentage
moyen de synsets attribués par WOLF a un lexeme et que l’EWN francais lui attribue également.
Le rappel est le pourcentage moyen des synsets attribués a un lexeme par l’EWN francais et que
WOLF lui attribue également. On constate que les verbes ont conduit a des résultats moins bons
que les noms, en raison de leur forte polysémie“.

Cependant, une précision inférieure a 100% pour un lexeme donné ne signiﬁe pas nécessaire-
ment une erreur : WOLF peut attribuer a juste titre un synset a un lexeme, la ou l’EWN francais
ne le fait pas. En revanche, un rappel inférieur a de bonnes chances d’étre dﬁ a une incomple-
tude du WOLF. C’est ce que montre l’évaluation manuelle que nous avons effectuée sur 100
lexemes choisis aléatoirement pour lesquels le WOLF et l’EWN francais sont en désaccord. Ils

lexemes anglais monosémiques se traduisent en un meme lexeme, qui est donc polysémique. Exemple : fatty liver
etfoie gras, tous deux monosémiques en anglais, se traduisent en le tenne frangais polysémiquefoie gras.

“Cette évaluation laisse de cote les synsets non Vides du WOLF absents de l’EWN frangais. Pourtant, la maj orité
de ces synsets sont remplis grace a Wikipédia, et sont donc de bonne qualité, comme le montrent la precision de
94,6% obtenue par cette seule source par rapport e‘1l’EWN frangais et son rappel de 87,8%.

Construction d’un wordnet libre du francais a partir de ressources multilingues

| Catégorie | nom | Verbe | adjectif | adverbe | all |

dans l’EWN francais } 76 (88%) l 33 (46%) | 0 (0%) l 0 (0%) l 109 (60%)
hors de l’EWN francais

correct 16 18 4 0 38

sem. proche 10 6 0 0 17

sem. relie 2 6 0 0 7

morph. relié 2 0 0 0 2

non relié 5 5 0 0 10

total 1 1 1 68 183

total correct (precision réelle du WOLF) 92 (83%) 51 (75%) 4 0 147 (80%)

TAB. 4 — Evaluation manuelle du WOLF (les nombres en italique sont a prendre avec precaution
compte tenu du nombre insufﬁsant de couples lexeme-synset concemes).

correspondent a 183 couples lexeme-synset. Nous avons veriﬁe, pour chacun de ceux qui ne

sont pas presents dans les deux ressources, s’ils sont bel et bien corrects ou non. Les erreurs du

WOLF ont ete classees en quatre categories :

— le lexeme est semantiquement proche du synset (hyperonyme, hyponyme, quasi-synonyme;
p.ex. absence dans le synset dont l’identiﬁant est celui de {lack, deﬁciency, want} du PWN),

— il est relie semantiquement (autre relation semantique ; p.ex. abri dans le synset dont l’iden-
tiﬁant est celui du synset {penthouse} du PWN),

— il est relie morphologiquement (il fait partie d’un compose qui aurait ete correct dans le
synset, ou il s’agit d’une variante morphologique d’un mot qui aurait ete correct ; p.eX. aﬁaire
dans le synset correspondant a {things}, alors que le pluriel aﬁaires serait correct; aisance
dans le synset correspondant a {toilet, lavatory, lav, can, john, privy, bathroom} alors que le
compose cabinet d ’aisances serait correct),

— il n’est pas relie du tout (ceci peut venir d’une erreur d’a1ignement ou de desambigu'1'sation,
p.ex. abattre dans le synset correspondant a {excavate, dig up, turn up}).

Les resultats pour les differentes categories sont donnes a la table 4. Environ la moitie des desac-

cords sont des couples synset-lexeme qui manquent dans l’EWN francais, et non des erreurs de

WOLF. Sans surprise, les synsets les moins problematiques sont les concepts les plus speci-

ﬁques, et les plus difﬁciles sont ceux qui contiennent des lexemes tres polysemiques decrivant

des concepts vagues. Pour une evaluation plus detaillee, y compris une evaluation source par
source, on po11rra se reporter a (Fiser et Sagot, 2008, soumis).

5 Conclusion

Nous avons presente la methodologie employee pour construire la premiere version d’un nou-
veau wordnet pour le francais, le WOLF (Wordnet Libre du Francais). Deux approches ont ete
utilisees de facon complementaire : une approche par alignement de corpus multilingues, pour
extraire des informations lexicales semantiques pertinentes sur tous les types de lexemes, y com-
pris polysemiques; une approche par traduction, utilisant des ressources telles que Wikipedia
ou le Wiktionnaire, a permis la traduction directe d’un grand nombre de lexemes polysemiques.
Nous avons alors evalue le WOLF ainsi construit par rapport a l’EWN francais.

Outre la validation et la correction manuelle (deja en cours sur les BCS1), nous envisageons
plusieurs pistes pour ameliorer la methodologie et la ressource construite. Naturellement, l’ap-

Benoit Sagot, Darja Fiser

proche par alignement pourrait tirer parti de l’utilisation de corpus plus importants (J RC-Acquis
dans son ensemble), bien que l’impact réel de la taille du corpus utilisé, lorsqu’il est aussi speci-
ﬁque, n’est pas clair. Par ailleurs, nous avons déja commencé des expériences préliminaires pour
généraliser aux lexemes polysémiques l’approche par traduction, par assignation automatique
de synsets du PWN a des entrées de Wikipédia (Ruiz-Casado et al., 2005). De plus, l’utilisation
de telles ressources a un avantage au ﬁl du temps : puisqu’elles sont en permanente évolution,
utiliser notre méthodologie sur une future version de Wikipédia ou des Wiktionnaires devrait
également permettre de construire de nouveaux synsets.

Enﬁn, nous souhaitons valoriser la ressource, notaInInent en désambigu'1'sation syntaxique et
sémantique et en extraction et recherche d’informations. Outre une validation de l’intérét de la
ressource, nous attendons de ces travaux des retours permettant d’en améliorer la qualité.

Références

DECLERCK T., PEREZ A. G., VELA 0., GANTNER Z. & MANZANO-MACHO D. (2006).
Multilingual lexical semantic resources for ontology translation. In Proc. of LREC ’06, Genes,
Italie.

DIAB M. (2004). The feasibility of bootstrapping an arabic wordnet leveraging parallel cor-
pora and an english wordnet. In Proc. of the Arabic Lang. Tech. and Res., Le Caire, Egypte.
FELLBAUM C. (1998). WordNet .' An Electronic Lexical Database. MIT Press.

FISER D. (2007). Leveraging parallel corpora and existing wordnets for automatic construc-
tion of the slovene wordnet. In Proc. of L&TC ’07, Poznan, Pologne.

HAJICVJ J. (2008). Disambiguation of Rich Inﬂection — Computational Morphology of Czech.
Charles University Press — Karolinum. A paraitre.

IDE N., ERJAVEC T. & TUFIS D. (2002). Sense discrimination with parallel corpora. In Proc.
of ACL’02 Workshop on Word Sense Disambiguation, Philadelphia, PA, USA.

JACQUIN C., DESMONTILS E., & MONCEAUX L. (2007). French eurowordnet lexical data-
base improvements. In Proc. of CI CLing ’07 ( LNCS 4394), Mexico City, Mexico.

PIANTA E., BENTIVOGLI L. & GIRARDI C. (2002). Multiwordnet : developing an aligned
multilingual database. In Proc. of the First Global WordNet Conference, Mysore, Inde.

RALF S., POULIQUEN B., WIDIGER A., IGNAT C., ERJAVEC T., TUFIS D. & VARGA D.
(2006). The J RC Acquis : A multilingual aligned parallel corpus with 20+ languages. In Proc.
of LREC ’06, Genes, Italie.

RESNIK P. & YAROWSKY D. (1997). A perspective on word sense disambiguation methods
and their evaluation. In ACL SIGLEX Workshop Tagging Text with Lexical Semantics .' Why,
What, and How ?, Washington, D.C., I-/3tats-Unis.

RUIZ-CASADO M., ALFONSECA E. & CASTELLS P. (2005). Automatic assignment of Wiki-
pedia encyclopedic entries to wordnet synsets. In Proc. of Advances in Web Intelligence.
TIEDEMANN J . (2003). Combining clues for word alignment. In Proc. of EACL’03, Budapest,
Hongrie.

TUFIS D. (2000). Balkanet design and development of a multilingual balkan wordnet. Roma-
nian Journal of Information Science and Technology, 7 (1-2).

VOSSEN, P. (1999). EuroWordNet .' a multilingual database with lexical semantic networks
for European Languages. Kluwer, Dordrecht.

