TALN 2009 – Session posters , Senlis, 24–26 juin 2009
Analyse en dépendances à l’aide des grammaires
d’interaction
Jonathan Marchand1 Bruno Guillaume2 Guy Perrier1
(1) LORIA / Université Nancy 2
(2) LORIA / INRIA Nancy Grand-Est
prénom.nom@loria.fr
Résumé. Cet article propose une méthode pour extraire une analyse en dépendances d’un
énoncé à partir de son analyse en constituants avec les grammaires d’interaction. Les gram-
maires d’interaction sont un formalisme grammatical qui exprime l’interaction entre les mots à
l’aide d’un système de polarités. Le mécanisme de composition syntaxique est régi par la sa-
turation des polarités. Les interactions s’effectuent entre les constituants, mais les grammaires
étant lexicalisées, ces interactions peuvent se traduire sur les mots. La saturation des polarités
lors de l’analyse syntaxique d’un énoncé permet d’extraire des relations de dépendances entre
les mots, chaque dépendance étant réalisée par une saturation. Les structures de dépendances
ainsi obtenues peuvent être vues comme un raffinement de l’analyse habituellement effectuée
sous forme d’arbre de dépendance. Plus généralement, ce travail apporte un éclairage nouveau
sur les liens entre analyse en constituants et analyse en dépendances.
Abstract. This article proposes a method to extract dependency structures from phrase-
structure level parsing with Interaction Grammars. Interaction Grammars are a formalism which
expresses interactions among words using a polarity system. Syntactical composition is led by
the saturation of polarities. Interactions take place between constituents, but as grammars are
lexicalized, these interactions can be translated at the level of words. Dependency relations are
extracted from the parsing process : every dependency is the consequence of a polarity satura-
tion. The dependency relations we obtain can be seen as a refinement of the usual dependency
tree. Generally speaking, this work sheds new light on links between phrase structure and de-
pendency parsing.
Mots-clés : Analyse syntaxique, grammaires de dépendances, grammaires d’interaction,
polarité.
Keywords: Syntactic analysis, dependency grammars, interaction grammars, polarity.
Jonathan Marchand, Bruno Guillaume, Guy Perrier
1 Introduction
Les grammaires de constituants et les grammaires de dépendances sont souvent présentées
comme orthogonales : les premières organisent les groupes de mots en syntagmes alors que
les secondes mettent la dépendance entre mots au centre de la structure syntaxique. Avec les
grammaires de constituants lexicalisées, telles que les grammaires d’arbres adjoints (TAG) et
les grammaires catégorielles (CG), où chaque élément de la grammaire est associé à un mot,
la composition syntaxique lors de l’analyse met en évidence des liens entre les mots. Ces liens
présentent des similitudes avec les relations de dépendances et ont fait l’objet de différentes
études.
A. Dikovsky et L. Modina ont étudié du point de vue formel le passage d’une analyse en consti-
tuants à une analyse en dépendances et vice versa (Dikovsky & Modina, 2000). O. Rambow
et A. Joshi ont expliqué comment retrouver une analyse en dépendances à partir d’une analyse
dans les TAG où les substitutions et les adjonctions sont vues comme des relations de dépen-
dances entre les mots (Rambow & Joshi, 1997). Enfin, l’article (Clark et al., 2002) propose une
méthode similaire pour les grammaires catégorielles combinatoires où l’application des règles
combinatoires donne lieu à des relations de dépendances entre les mots.
Les grammaires d’interaction (IG) sont des grammaires de constituants lexicalisées qui étendent
par un système de polarités plus riche le système besoins/ressources employé dans les gram-
maires catégorielles. Dans cet article, nous généralisons, les résultats (Clark et al., 2002) cités
plus haut pour les CG au cas des IG, en révisant la méthode : en effet, cette dernière impose
d’étendre les entrées lexicales avec des marqueurs pour aider à la construction des dépendances
lors de l’analyse, et produit trop de dépendances, alors que notre méthode s’appuie plus simple-
ment sur le lien entre polarités et dépendances.
Dans la section 2, les IG sont présentées et illustrées par un exemple. La section 3 décrit la
méthode d’extraction des dépendances à partir d’une analyse avec une IG. Finalement, dans la
section 4, nous étudions les structures de dépendances obtenues et nous mettons en perspective
avec d’autres travaux.
2 Les grammaires d’interaction
Les grammaires d’interaction (Perrier, 2003) sont un formalisme grammatical s’appuyant sur
la notion de description d’arbres. Cette notion a été introduite par M. Marcus, D. Hindle et
M. Fleck en 1983 (Marcus et al., 1983), et K. Vijay-Shanker l’a utilisée pour représenter de
façon monotone l’opération d’adjonction des TAG (Vijay-Shanker, 1992).
Une description d’arbres est définie par un ensemble de nœuds et de relations d’ascendance,
de parenté et de précédence entre ces nœuds. Les nœuds représentent des syntagmes (éventuel-
lement vides) et les relations expriment les dépendances entre ces syntagmes. Les propriétés
morpho-syntaxiques de ces syntagmes sont décrites par des structures de traits.
Cette approche flexible est bien adaptée à l’ambiguïté des langues naturelles. Cependant, l’ana-
lyse syntaxique fondée sur des descriptions d’arbres est très coûteuse (Koller et al., 1998). En
effet, dans cette approche, l’analyse syntaxique consiste à chercher des modèles de descrip-
tions d’arbres sous forme d’arbres syntaxiques complètement spécifiés, ce qui est un problème
NP-complet.
Analyse en dépendances à l’aide des grammaires d’interaction
Dans les formalismes opérationnels fondés sur les descriptions d’arbres (comme les D-tree sub-
stitution grammars (Rambow et al., 2001) ou les TT-MCTAG (Kallmeyer, 2005)), cet indéter-
minisme est limité en contraignant la syntaxe des descriptions et le mécanisme de composition
syntaxique.
L’originalité des grammaires d’interaction est de proposer un mécanisme de composition syn-
taxique très souple qui consiste à superposer les descriptions d’arbres mais qui est guidé par
une contrainte de saturation de polarités. Cette contrainte fait référence à l’idée de valence de
Tesnière (Tesnière, 1934) et est essentielle dans les CG : chaque mot est équipé d’une valence
exprimant ses possibilités d’interaction avec les autres mots. La composition syntaxique est
contrôlée pas une dualité besoins/ressources : certaines ressources munies de polarités néga-
tives sont attendues alors que d’autres, munies de polarités positives, sont disponibles. Dans les
IG, cette idée de valence est reprise et généralisée.
2.1 Système de polarités
Contrairement aux CG, les IG attachent les polarités aux traits qui décorent les nœuds. Mais
nous nous en tiendrons ici à une version simplifiée des IG où les polarités sont accrochées aux
nœuds. Une autre différence avec les CG est que le système de polarités est plus riche. En effet,
les IG proposent deux types d’interaction à base de polarités :
– les interactions linéaires : chaque nœud portant une polarité positive (notée +) doit fusion-
ner avec exactement un nœud portant une polarité négative (notée ?) et réciproquement.
– les interactions non-linéaires : chaque nœud portant une polarité virtuelle (notée ?) doit
fusionner exactement, soit avec un nœud positif et un nœud négatif, soit avec un nœud portant
la polarité saturée (notée =). En revanche, un nombre quelconque de noeuds virtuels peuvent
fusionner avec le même couple positif/négatif ou avec le même nœud saturé.
? ? + =
? ? ? + =
? ? =
+ + =
= =
Lors de la fusion de deux nœuds, le nœud résultant porte la polarité
issue de la composition des polarités des nœuds initiaux. La com-
position d’une polarité positive et d’une polarité négative donne une
polarité saturée alors que la polarité virtuelle est l’élément neutre
de cette opération. Toute autre composition provoque l’échec de la
fusion (cf. tableau ci-contre). L’opération de composition est asso-
ciative et commutative, l’ordre de fusion des nœuds n’a donc pas
d’importance dans le processus de composition syntaxique.
2.2 Analyse avec les IG
La structure syntaxique élémentaire manipulée dans les IG est appelée description d’arbre
polarisée (DAP). Une IG particulière est définie par un ensemble de DAP ; chaque DAP est
associée à un mot1. La grammaire est ainsi un lexique où un mot peut avoir plusieurs entrées.
Pour analyser une phrase, il faut choisir pour chaque mot l’une des DAP associée à ce mot.
Un tel choix est appelée une sélection lexicale. L’analyse consiste ensuite à composer ces DAP
pour obtenir un arbre d’analyse.
L’opération atomique de composition syntaxique consiste à superposer deux nœuds pour saturer
1Dans nos grammaires, il n’y a pas de co-ancre et donc un seul mot par DAP (l’usage de co-ancre peut-être
simulé en utilisant des polarités).
Jonathan Marchand, Bruno Guillaume, Guy Perrier
leurs polarités. On itère l’opération de saturation de nœuds pour construire progressivement
l’analyse de la phrase sous forme d’un arbre.
Les DAP d’une sélection lexicale pour la phrase “Jean en connaît la couleur” sont représentées
sur la figure 1.
B1+NP
Jean
A2~S
C2~V D2~NP
E2=PRO F2~V H2~N
en
I2~N J2=PP
K2=PREP L2=NP
? ?
A3=S
B3-NP C3=V D3-NP
F3=V
connaît
G4+DET
la
D5+NP
G5-DET H5=N
I5=N
couleur
FIG. 1 – DAP associées à la phrase “Jean en connaît la couleur”
La DAP représentant le mot “en” décrit l’intuition linguistique suivante : le pronom “en”
est utilisé comme complément du nom “couleur” mais il vient s’adjoindre devant le verbe
“connaît” qui admet “la couleur” comme objet direct. La DAP montre à droite du noyau verbal
C2 un syntagme objet D2 attendu comportant un complément du nom J2 qui est déjà complè-
tement saturé mais sans réalisation phonologique. Le nœud I2 renseigne la positionnement du
nom dans le syntagme H2. Au niveau du noyau verbal C2, le pronom “en” E2 est positionné à
gauche du verbe F2.
À partir de l’ensemble des DAP de la figure 1, on peut construire l’arbre syntaxique saturé
représenté sur la figure 2. Sur chaque nœud de l’arbre est indiqué l’ensemble des nœuds des
DAP qui ont été superposés. Par exemple le nœud A2-A3 représente la superposition du nœud
A2 de la DAP de “en” et du nœud A3 de la DAP de “connaît”.
A2-A3=S
B1-B3=NP C2-C3=V D2-D3-D5=NP
Jean
E2=PRO F2-F3=V G4-G5=DET H2-H5=N
en connaît la
I2-I5=N J2=PP
couleur
K2=PREP L2=NP
? ?
FIG. 2 – Arbre d’analyse de “Jean en connaît la couleur”
2.3 Grammaire du français
Pour valider le formalisme, nous avons développé une grammaire du français (Perrier, 2007).
Cette grammaire a été évaluée sur la TSNLP (Lehmann et al., 1996) qui contient 1690 énoncés
Analyse en dépendances à l’aide des grammaires d’interaction
grammaticaux et 1935 énoncés agrammaticaux. Ce jeu de tests ne couvre pas toute la langue
française, il y a peu de phrases complexes mais il insiste sur certains phénomènes comme la
coordination ou la position des compléments adverbiaux. Cependant, notre grammaire couvre
d’autres phénomènes dont la TSNLP ne tient pas compte, comme par exemple : la voix passive,
la sous-catégorisation des noms et des adjectifs prédicatifs, le contrôle du sujet des compléments
infinitifs, les propositions relatives et interrogatives. 88% des 1690 phrases grammaticales sont
modélisées et 85% des 1935 phrases agrammaticales sont rejetées. Les 15% d’énoncés agram-
maticaux sont acceptés car la grammaire ne modélise pas les règles phonologiques et la séman-
tique. Les raisons pour lesquelles 12% des énoncés grammaticaux ne sont pas analysés sont
diverses (phrases retranscrites de l’oral, expressions figées, causatifs, superlatifs).
3 Analyse en dépendances
L’analyse syntaxique est obtenue par superposition des DAP d’une sélection lexicale. La super-
position est guidée par la fusion des nœuds portant des polarités se saturant. Au niveau d’une
DAP, ces polarités représentent les besoins/ressources du mot dans un énoncé. La saturation
de ces dernières peut alors se voir comme la résolution d’une dépendance entre ces mots. On
peut alors retrouver les relations de dépendances d’une phrase à partir de l’ensemble des DAP
associées aux mots de la phrase et de son analyse.
3.1 Dépendances linéaires
Nous nous intéressons dans un premier temps au cas simple de la saturation linéaire de deux
polarités + et ?. Par exemple, dans la phrase “Jean en connaît la couleur” (Figure 2), la DAP
représentant le déterminant “la” possède un nœud G4 portant une polarité positive et étiqueté
par la catégorie syntaxique DET. La DAP du nom “couleur” comporte quant à elle un nœud G5
portant une polarité négative qui est aussi étiqueté par DET.
Lors de l’analyse, ces deux nœuds fusionnent pour obtenir un nœud saturé, la DAP résultante
représentant une analyse partielle de “la couleur”. La saturation de ces deux nœuds peut être
vue comme la réalisation d’une relation de dépendances entre les deux mots correspondants.
L’opération de saturation représente la satisfaction d’une contrainte de besoins/ressources. Un
élément qui se présente comme nécessitant une ressource est considéré comme le gouverneur
de la relation de dépendances, tandis qu’un élément qui se présente comme fournissant une
ressource se retrouve comme le dépendant de cette relation. Ainsi, en cas d’interaction linéaire,
le mot dont la DAP contient le nœud négatif est le gouverneur et le mot dont la DAP contient
le nœud positif est le dépendant de la relation de dépendances.
Les relations de dépendances engendrées par la saturation des nœuds portant des polarités op-
posées seront appelées dépendances linéaires. Dans la grammaire actuelle du français, elles
représentent les relations tête-complément et tête-spécifieur.
Dans l’exemple “Jean en connaît la couleur”, les dépendances linéaires obtenues sont repré-
sentées sur la figure 3 (les arcs portent la catégorie qui a fait l’objet d’une saturation2).
2en pratique, il est possible d’utiliser d’autres étiquettes sur les arcs comme les fonctions syntaxiques (elles
sont indiquées dans les structures de traits).
Jonathan Marchand, Bruno Guillaume, Guy Perrier
Jean en connaît la couleur
NP
DETNP
FIG. 3 – Relations de dépendances linéaires dans la phrase “Jean en connaît la couleur”
Cette analyse possède un nœud “en” isolé. La DAP du pronom “en” ne porte en effet pas de
polarité positive ou négative et ainsi ne produit pas de relation de dépendances linéaires avec
le reste de la phrase. La saturation des polarités positives et négatives ne suffit donc pas pour
exprimer toutes les relations de dépendances d’une phrase. Nous allons donc voir comment
certaines relations de dépendances peuvent être produites pas des interactions non-linéaires.
3.2 Dépendances non-linéaires
H6~N
L6=ADJ I6~N
belle
Dans la grammaire actuelle, la DAP pour l’adjectif “belle” dans le groupe no-
minal “la belle couleur” est donnée par la figure ci-contre. De façon habituelle
en dépendances, on considère qu’il y a une dépendance de “belle” vis à vis de
“couleur”. Deux nœuds sont non-saturés (H6 et I6) et ils portent tous les deux
une polarités virtuelle, c’est donc la saturation de l’une de ces deux polarités
qui doit induire la dépendance. Dans ce cas, les deux polarités peuvent être
à l’origine de la dépendance. Cependant, il ne doit être produit qu’une seule
relation de dépendances, il faut choisir alors quelle polarité engendrera une
dépendance lors de sa saturation.
A7~S
C7~V D7+NP
M7=PRO F7~V
le
Un autre exemple d’usage des polarités virtuelles dans la grammaire du
français est illustré par l’exemple “Jean le connaît”. Dans cette phrase,
le pronom “le” (ci-contre) est l’objet direct du verbe “connaît” : cette re-
lation de dépendances est produite par la saturation linéaire des polarités
du nœud D7 de la DAP du pronom “le” et du nœud D3 de la DAP du mot
“connaît”. Mais la DAP du pronom “le” possède trois nœuds A7, C7,
F7 portant une polarité virtuelle dont la saturation non-linéaire n’apporte
aucune information de dépendances entre les mots “le” et “connaît”.
Ces polarités contrôlent simplement la place des syntagmes lors de la
superposition des deux DAP et gèrent le fait que le pronom “le” se place
avant le verbe alors que la place canonique du groupe nominal objet dans
la phrase est après le verbe. Dans l’arbre d’analyse de “Jean le connaît” les trois nœuds vir-
tuels A7, C7, F7 sont saturés, respectivement, par les trois nœuds A3, C3, F3 de la DAP du
mot “connaît”.
Les deux derniers exemples montrent bien qu’il y a deux usages distincts des polarités virtuelles
qui ne se comportent pas de la même manière pour la production de relation de dépendances :
– les polarités virtuelles de dépendances qui portent une information sur les relations de
dépendances d’un mot avec son environnement ;
– les polarités virtuelles de contexte qui imposent des contraintes sur le contexte syntaxique
d’un mot.
Il n’est pas possible de distinguer automatiquement, dans une grammaire donnée, les deux types
de polarités virtuelles. C’est donc à l’auteur de la grammaire de se baser sur des critères lin-
guistiques pour distinguer ces deux usages. Cependant, dans la pratique, l’utilisation de méta-
Analyse en dépendances à l’aide des grammaires d’interaction
grammaires (notre grammaire, par exemple, est construite avec XMG (Duchier et al., 2005))
permet de faire ce travail rapidement, de façon cohérente sur l’ensemble de la grammaire. Cela
concerne uniquement les mots jouant le rôle de modificateurs (adjectifs épithètes, adverbes,
prépositions introduisant des compléments adjoints, pronoms relatifs dans leur rôle par rapport
à leur antécédent, pronom clitique "il" utilisé en redoublement du sujet, etc.). Il s’agit, dans la
DAP associé au mot concerné de marquer le nœud considéré comme nœud privilégié de ratta-
chement au mot qui est modifié. Le travail a été effectué sur notre grammaire du français à large
couverture en moins d’une heure.
Ainsi, dans la DAP de “en” (figure 1), toutes les polarités virtuelles des nœuds A2, C2, D2 et
F2 sont des polarités virtuelles de contexte qui gèrent le positionnement de “en”. Pour rendre
compte du fait que “en” dépend de “couleur”, il faut que l’une des deux polarités virtuelles H2
ou I2 soit une polarité virtuelle de dépendances. Dans notre cas, nous avons choisi arbitraire-
ment la polarité H2.
Dans le cas où une relation de dépendances est produite, le dépendant est le mot dont la DAP
porte le nœud virtuel de dépendances. La polarité virtuelle peut être saturé :
– soit par un nœud portant la polarité =, dans ce cas ce nœud est le gouverneur ;
– soit par un couple de nœud (un positif et un négatif), dans ce cas le gouverneur est le nœud
portant la polarité positive.
Les relations ainsi engendrées sont appelées dépendances non-linéaires. En effet, un nœud
saturé pouvant se composer avec zéro ou plusieurs nœuds virtuels, plusieurs relations de dépen-
dances peuvent avoir comme gouverneur le même mot. Ces relations de dépendances expriment
généralement une relation modifieur-modifié.
Dans les structures de dépendances (figures 4 ci-dessous et 5 plus loin) les dépendances linéaires
sont représentées au-dessus de la phrase et les non-linéaires au-dessous.
La procédure d’extraction des dépendances se résume ainsi :
– La saturation de polarités opposées engendre une relation de dépendances linéaire entre les
mots ; la relation va de la polarité négative vers la polarité positive.
– La saturation d’une polarité virtuelle de dépendances avec une polarité positive ou saturée
engendre une relation de dépendances non linéaire entre les mots ; la relation va de la polarité
saturée ou positive vers la polarité virtuelle de dépendances.
Cette procédure permet d’obtenir l’analyse en dépendances de “Jean en connaît la couleur”
représentée figure 4.
Jean en connaît la couleur
NP
DETNP
N
FIG. 4 – Relations de dépendances dans la phrase “Jean en connaît la couleur”
Pour garantir que les structures de dépendances sont connexes (chaque mot est en relation
avec au moins un autre mot de l’énoncé), il suffit d’imposer que chaque DAP de la grammaire
contienne au moins un nœud positif, un nœud négatif ou un nœud portant une polarité virtuelle
de dépendances. C’est la cas de la grammaire actuellement implantée pour le français.
Jonathan Marchand, Bruno Guillaume, Guy Perrier
4 Structures de dépendances obtenues
Le choix du type de structures pour représenter les dépendances d’une phrase est une question
épineuse qui divisent les linguistes. L’idée de départ des grammaires de dépendances est de
considérer que chaque mot de la phrase (sauf le verbe principal) est gouverné par exactement
un autre mot de la même phrase. Cette hypothèse conduit à considérer que les bonnes structures
de dépendances sont les arbres, nous allons voir ce qu’il en est avec notre méthode qui permet
d’observer, sans a priori, les structures obtenues.
4.1 Graphes orientés
De façon générale, la structure en dépendances que l’on obtient est un graphe orienté ; de plus,
avec la restriction imposée sur les DAP de la grammaire, on sait que ce graphe est connexe.
L’analyse en dépendances pour la phrase “Jean en connaît la couleur” donnée par la figure 4 est
un arbre ; en effet, tous les mots, sauf “connaît”, ont un et un seul gouverneur. Il existe cepen-
dant des exemples pour lesquels la structure de dépendances n’est pas un arbre. L’application
de notre méthode à la phrase “la fille que Jean aime vient” produit l’analyse de la figure 5.
Cette structure n’est pas un arbre car elle contient un cycle3 et le pronom relatif “que” a deux
gouverneurs.
la fille que Jean aime vient
DET NP
S
NP
NP
NP
FIG. 5 – Relations de dépendances dans la phrase “la fille que Jean aime vient”
On retrouve ainsi avec notre méthode le problème qui se pose habituellement en grammaire de
dépendances pour gérer les phénomènes d’extraction. Par exemple, si nous reprenons la phrase
“la fille que Jean aime vient”, le mot “que” remplit ici deux rôles, il est l’objet anaphorique
de “aime” et subordonne la relative à l’antécédent. Ce double rôle suppose naturellement deux
relations de dépendances distinctes qui contredisent le principe de représentation en arbre, alors
que l’analyse présentée figure 5 rend bien compte de ce double emploi. D’autres approches
d’analyses en dépendances utilise également des structures qui ne sont pas des arbres : S. Ka-
hane (Kahane, 2000) propose une analyse dans laquelle un pronom relatif a deux gouverneurs ;
R. Hudson (Hudson, 1990) utilise également souvent des structures dans lesquelles un mot peut
avoir plusieurs gouverneurs.
4.2 Projectivité
Une autre question récurrente à propos des structures de dépendances à considérer pour la des-
cription de la langue est celle de la projectivité. En effet une structure projective induit que les
3“aime” gouverne “que” car “que” est le complément d’objet de “aime” ;“que” gouverne “aime” car c’est
le pronom relatif qui introduit la relative où “aime” est le verbe.
Analyse en dépendances à l’aide des grammaires d’interaction
relations de dépendances restent à un niveau local, ce qui permet une analyse simple et efficace.
Cette notion initialement définie pour les arbres peut se transposer sur les graphes : une structure
de dépendances est dite projective si pour tout mot donné, l’ensemble des nœuds atteignables
depuis ce mot dans la structure de dépendances (qu’on appellera emprise du mot) correspond
à un segment continu de l’énoncé. Par exemple la structure de la figure 5 est projective alors
que celle de la figure 4 ne l’est pas : dans cette analyse, l’emprise du mot “couleur” est formé
de deux segments “en” et “la couleur” séparés par le mot “connaît”.
4.3 Classes de structures de dépendances
R. Debusmann and M. Kuhlmann proposent des critères qui permettent de classer plus fine-
ment les analyses non-projectives. Il obtiennent ainsi une hiérarchisation en classes du pouvoir
expressif que permettent les différentes structures de dépendances (Debusmann & Kuhlmann,
2009). La notion de degré de discontinuité (block-degree) associe à une structure un entier qui
est le nombre maximum de segments continus disjoints dans l’emprise d’un mot (un degré de
discontinuité de 1 correspond exactement à la projectivité). Pour les structures dont le degré de
discontinuité est au moins 2, ils distinguent celles qui sont bien imbriquées (well-nestedness)
c’est-à-dire celle qui sont telles que les emprises des deux mots ne se croisent pas (soit elles
sont disjointes, soit l’une est entièrement incluse entre deux segments de l’autre). Sur le Prague
Dependency Treebank, les auteurs montrent que 99,5% des analyses sont bien imbriquées et de
degré de discontinuité au plus deux (ce qui est équivalent à être dans la classe de langages des
TAG).
L’application de notre méthode à cette grammaire du français sur la TSNLP ne produit pas
d’analyse mal imbriquées. On obtient dans la plupart des cas des structures de dépendances pro-
jectives. Les exemples pour lesquels le degré de discontinuité est de 2 sont dûs principalement
au placement de l’auxiliaire dans le noyau verbal. Les mots “en” ou “y” ainsi que l’inversion
sujet/verbe lors de l’emploi de pronoms interrogatifs sont d’autres sources de discontinuité,
mais nous n’avons pas trouvé d’exemple qui aille au-delà d’un degré de discontinuité de 2.
5 Conclusion
Dans cet article, nous avons proposé une méthode pour construire une analyse en dépendances
d’un énoncé à partir de son analyse en constituants dans les IG. Cette méthode, basée sur la satu-
ration des polarités, a mis en évidence deux types de dépendances : les dépendances linéaires qui
représentent les relations tête-complément ou tête-spécifieur et les dépendances non-linéaires
qui représentent les relations modifieur-modifié.
Les structures de dépendances obtenues par cette méthode sont des graphes orientés, elles sont
plus riches que les structures obtenues habituellement par des grammaires de dépendances.
Cette représentation permet de gérer simplement les phénomènes linguistiques posant habituel-
lement des difficultés dans les grammaires de dépendances.
Pour la suite du travail, nous souhaitons étudier dans quelle mesure il est possible de transposer
les méthodes d’analyses d’un formalisme à l’autre. Par exemple, Nous avons remarqué que très
peu d’analyses sont non-projectives ; on pourrait donc isoler les cas non-projectifs et adapter un
algorithme d’analyse des grammaires de dépendances qui servirait de guide à l’analyse dans les
Jonathan Marchand, Bruno Guillaume, Guy Perrier
IG. Il serait également intéressant d’étudier l’application de nos méthodes d’analyse spécifiques
aux IG à l’analyse pour des grammaires de dépendances lexicalisées.
Références
CLARK S., HOCKENMAIER J. & STEEDMAN M. (2002). Building deep dependency struc-
tures with a wide-coverage CCG parser. In In Proceedings of the 40th Meeting of the ACL, p.
327–334.
DEBUSMANN R. & KUHLMANN M. (2009). Dependency grammar : Classification and ex-
ploration. In Resource-Adaptive Cognitive Processes : Springer.
DIKOVSKY A. & MODINA L. (2000). Dependencies on the other Side of the Curtain. T.A.L,
1, 79–111.
DUCHIER D., LE ROUX J. & PARMENTIER Y. (2005). XMG : Un Compilateur de Méta-
Grammaires Extensible. In M. JARDINO, Ed., Actes de TALN 2005 (Traitement automatique
des langues naturelles), Dourdan : ATALA LIMSI.
HUDSON R. A. (1990). English Word Grammar. Blackwell.
KAHANE S. (2000). Extractions dans une grammaire de dépendance à bulles. T.A.L., 41(1),
187–216.
KALLMEYER L. (2005). Tree-local Multicomponent Tree Adjoining Grammars with Shared
Nodes. Computational Linguistics, 31(2), 187–225.
KOLLER A., NIEHREN J. & TREINEN R. (1998). Dominance constraints : Algorithms and
complexity. In LACL’98, p. 106–125, Heidelberg.
LEHMANN S., OEPEN S., C I., BAUR H. H., LBDKAN O. & ARNOLD D. (1996). tsnlp —
test suites for natural language processing. In In J. Nerbonne (Ed.), Linguistic Databases (pp.
13 - 36, p. 711–716 : CSLI Publications.
MARCUS M. P., HINDLE D. & FLECK M. M. (1983). D-theory : talking about talking about
trees. In Proceedings of the 21st annual meeting on Association for Computational Linguistics,
p. 129–136, Morristown, NJ, USA : Association for Computational Linguistics.
PERRIER G. (2003). Les grammaires d’interaction. Habilitation à diriger les recherches,
Université Nancy 2.
PERRIER G. (2007). A French Interaction Grammar. In RANLP 2007, p. 463–467, Borovets
Bulgarie.
RAMBOW O. & JOSHI A. (1997). A Formal Look at Dependency Grammars and Phrase-
Structure Grammars, with Special Consideration of Word-Order Phenomena. In Current Is-
sues in Meaning-Text Theory, London : Pinter.
RAMBOW O., WEIR D. & VIJAY-SHANKER K. (2001). D-tree substitution grammars. Com-
put. Linguist., 27(1), 89–121.
TESNIÈRE L. (1934). Comment construire une syntaxe. Bulletin de la Faculté des Lettres de
Strasbourg, 12(7), 219–229.
VIJAY-SHANKER K. (1992). Using Description of Trees in Tree Adjoining Grammar frame-
work. Computational Linguistics, 18(4), 481–518.
