<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
"http://www.w3.org/TR/html4/loose.dtd">
<html><head><title>Association automatique de lemmes et de paradigmes de flexion &#224; un mot inconnu</title>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
</head>
<body>
<div style="page-break-before:always; page-break-after:always"><div><p>TALN 2009&#8211; Session posters, Senlis, 24-26 juin 2009 
</p>
<p>Association automatique de lemmes et de paradigmes de flexion &#224; 
</p>
<p>un mot inconnu 
</p>
<p>Claude de Loupy (1,2), Micha&#235;l Bagur (1), Helena Blancafort (1,3) 
</p>
<p>(1)  Syllabs &#8211; 15, rue Jean Baptiste Berlier, 75013 Paris 
blancafort@syllabs.com; bagur@syllabs.com; loupy@syllabs.com 
</p>
<p>(2) MoDyCo &#8211; Universit&#233; Paris 10, 200 Av. de la R&#233;publique, Nanterre 
(3) Universitat Pompeu Fabra, Roc Boronat 138, 08018 Barcelona  
</p>
<p> 
</p>
<p>R&#233;sum&#233;  
</p>
<p>La maintenance et l&#8217;enrichissement des lexiques morphosyntaxiques sont souvent des t&#226;ches 
fastidieuses. Dans cet article nous pr&#233;sentons la mise en place d&#8217;une proc&#233;dure de guessing de 
flexion afin d&#8217;aider les linguistes dans leur travail de lexicographes. Le guesser d&#233;velopp&#233; ne 
fait pas qu&#8217;&#233;valuer l&#8217;&#233;tiquette morphosyntaxique comme c&#8217;est g&#233;n&#233;ralement le cas. Il propose 
pour un mot fran&#231;ais inconnu, un ou plusieurs candidats-lemmes, ainsi que les paradigmes de 
</p>
<p>flexion associ&#233;s (formes fl&#233;chies et &#233;tiquettes morphosyntaxiques). Dans cet article, nous 
</p>
<p>d&#233;crivons le mod&#232;le probabiliste utilis&#233; ainsi que les r&#233;sultats obtenus. La m&#233;thode utilis&#233;e 
</p>
<p>permet de r&#233;duire consid&#233;rablement le nombre de r&#232;gles &#224; valider, permettant ainsi un gain de 
</p>
<p>temps important. 
</p>
<p>Abstract  
</p>
<p>Lexicon maintenance and lexicon enrichment is a labour-intensive task. In this paper, we 
</p>
<p>present preliminary work on an inflectional guessing procedure for helping the linguist in 
</p>
<p>lexicographic tasks. The guesser presented here does not only output morphosyntactic tags, 
</p>
<p>but also suggests for an unknown French word one or more lemma candidates as well as their 
</p>
<p>corresponding inflectional rules and morphosyntactic tags that the linguist has to validate. In 
</p>
<p>this article, we present the probabilistic model we used as well as obtained results. The 
</p>
<p>method allows a drastic reduction of the number of rules to validate.  
</p>
<p>Mots-cl&#233;s :   guesser, lexiques morphosyntaxiques, aide aux linguistes, induction des 
r&#232;gles de flexion 
</p>
<p> 
</p>
<p>Keywords:   guesser, morphosyntactic lexica, aide to the linguist, induction of inflection 
rules 
</p>
<p> </p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Claude de Loupy, Micha&#235;l Bagur, Helena Blancafort 
</p>
<p>1 Introduction 
</p>
<p>Les ressources lexicales sont fondamentalement importantes en Traitement Automatique des 
</p>
<p>Langues (TAL). Les lexiques morphosyntaxiques en particulier sont tr&#232;s utilis&#233;s par des 
</p>
<p>traitements de relativement bas niveau qui sont donc la base de traitements ult&#233;rieurs
1
. Or, la 
</p>
<p>gestion et la maintenance de tels lexiques est tr&#232;s consommatrice en temps et en &#233;nergie. Il 
</p>
<p>n&#8217;est pas rare qu&#8217;un organisme travaillant en TAL ait &#224; g&#233;rer plusieurs lexiques de ce type, 
que ce soit pour une m&#234;me langue avec des sp&#233;cialisations de ressources ou pour le traitement 
</p>
<p>de plusieurs langues. Du fait de la cr&#233;ativit&#233; des langues, les lexiques doivent &#234;tre en 
</p>
<p>permanence mis &#224; jour et il est important de penser des m&#233;thodes permettant de faciliter ce 
</p>
<p>travail.  
</p>
<p>L&#8217;un des moyens les plus courants pour enrichir ces lexiques consiste &#224; analyser des corpus, &#224; 
r&#233;cup&#233;rer les formes inconnues du lexique &#224; enrichir et &#224; pr&#233;senter les plus courantes &#224; un/e 
</p>
<p>linguiste qui se charge alors d&#8217;introduire non seulement la forme en question mais &#233;galement 
ses &#233;tiquettes morphosyntaxiques, son ou ses lemmes ainsi que toutes les formes fl&#233;chies du 
</p>
<p>ou des lemmes en question. Cette t&#226;che est g&#233;n&#233;ralement effectu&#233;e &#224; la main ou en utilisant 
</p>
<p>des outils sp&#233;cifiques (fl&#233;chisseur) permettant de fl&#233;chir un lemme selon des paradigmes de 
</p>
<p>flexion connus. M&#234;me dans ce dernier cas. L&#8217;ensemble de la t&#226;che prend beaucoup de temps 
car il est n&#233;cessaire de d&#233;cider quel est le lemme correspondant &#224; une forme ainsi que le 
</p>
<p>num&#233;ro ou la d&#233;nomination de la r&#232;gle de flexion qui doit lui &#234;tre associ&#233;e.  
</p>
<p>L&#8217;objet de cet article est d&#8217;&#233;valuer les performances de m&#233;thodes classiques utilis&#233;es dans les 
guessers de cat&#233;gories grammaticales lorsqu&#8217;on les applique &#224; l&#8217;association de couples 
(lemme, paradigme de flexion) &#224; un mot inconnu. Un &#233;tat de l&#8217;art est donn&#233; en section 2. Les 
fondements et les buts de l&#8217;exp&#233;rience d&#233;crite sont ensuite expos&#233;s en section 3 afin 
d&#8217;expliquer pourquoi les m&#233;thodes pr&#233;c&#233;dentes ne sont pas satisfaisantes pour les conditions 
vis&#233;es. La section 4 pr&#233;sente la m&#233;thode utilis&#233;e. Enfin, la section 5 pr&#233;sente l&#8217;&#233;valuation des 
r&#233;sultats.  
</p>
<p>2 &#201;tat de l&#8217;art 
</p>
<p>L&#8217;id&#233;e de fournir une aide suppl&#233;mentaire aux linguistes dans cette &#233;tape d&#8217;enrichissement 
court depuis d&#233;j&#224; un certain temps. Les premiers travaux en construction automatique de 
</p>
<p>lexiques morphosyntaxiques n&#8217;utilisaient qu&#8217;un corpus comme source d&#8217;information. Ainsi, 
Jacquemin (1997) compare les terminaisons des mots de son corpus de mani&#232;re &#224; grouper des 
</p>
<p>mots ayant un m&#234;me stem puis regroupe les groupes de mots ayant la m&#234;me suite de 
</p>
<p>terminaisons. Goldsmith (1997 ; 2000) effectue &#233;galement ce type de proc&#233;dure en appliquant 
</p>
<p>les m&#233;thodes de Minimum Description Length (Rissanen, 1989) et d&#8217;Expectation 
Maximization (Dempster et al., 1977)
</p>
<p>2
. Schone &amp; Jurasky (2000 ; 2001) utilisent la m&#233;thode 
</p>
<p>du Latent Semantic Analysis (Deerwester et al., 1990).  
</p>
<p>                                                 
1
  Nous n&#8217;entrerons pas ici dans un d&#233;bat sur l&#8217;utilit&#233; ou non de ces lexiques en comparaison &#224; d&#8217;autres moyens 
</p>
<p>d&#8217;analyse plus sommaire comme les stemmers de type Porter (1980). Nous pensons que ces lexiques sont 
utiles et de nombreux autres travaux les utilisent, ce qui suffit &#224; justifier un travail sur leur cr&#233;ation.  
</p>
<p>2
  L&#8217;outil Linguistica r&#233;sultant de ces travaux est t&#233;l&#233;chargeable &#224; l&#8217;adresse suivante : 
</p>
<p>http://linguistica.uchicago.edu/.  </p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Association automatique de lemmes et de paradigmes de flexion &#224; un mot inconnu 
</p>
<p>Dans tous les cas, les syst&#232;mes propos&#233;s produisent des listes de stems associ&#233;s &#224; des 
</p>
<p>paradigmes de flexion permettant de g&#233;n&#233;rer les formes fl&#233;chies trouv&#233;es dans les corpus 
</p>
<p>analys&#233;s. Le principal probl&#232;me de ces m&#233;thodes est que les r&#233;sultats g&#233;n&#233;r&#233;s sont tr&#232;s 
</p>
<p>difficilement utilisables par des linguistes car ils n&#8217;ont que peu de liens avec les r&#232;gles 
morphologiques auxquelles on s&#8217;attend. Ces travaux sont donc principalement utilisables dans 
un contexte totalement automatique avec toutes les erreurs que cela peut comporter.  
</p>
<p>D&#8217;autres exp&#233;riences utilisent des ressources lexicales existantes ou des paradigmes de 
flexion d&#233;j&#224; connus en plus des corpus &#224; analyser. Nakov et al. (2003), travaillent sur 
</p>
<p>l&#8217;allemand et commencent par extraire, de mani&#232;re automatique, toutes les terminaisons 
possibles pour les mots pr&#233;sents dans un lexique. Ils extraient ensuite les mots pr&#233;sents dans 
</p>
<p>un corpus qui sont inconnus dudit lexique, puis g&#233;n&#232;rent tous les stems possibles &#224; partir des 
</p>
<p>terminaisons pour chacun de ces mots. Ils utilisent enfin l&#8217;algorithme de Maximum Likelihood 
Estimation avec des param&#232;tres pr&#233;conis&#233;s par Mikheev (1997) afin de r&#233;cup&#233;rer les r&#232;gles de 
</p>
<p>flexion les plus int&#233;ressantes (notions de qualit&#233;, de longueur et de fr&#233;quences).  
</p>
<p>Oliver &amp; Tadi&#231; (2004) pr&#233;sentent l&#8217;application sur le Croate de m&#233;thodes test&#233;es auparavant 
sur le russe (Oliver et al., 2003). Ils s&#8217;appuient sur un lexique morphosyntaxique existant dont 
ils extraient des paradigmes de flexion et sur un corpus &#224; partir duquel ils compl&#232;tent le 
</p>
<p>lexique pr&#233;c&#233;dent. Le processus est d&#233;coup&#233; en 4 &#233;tapes : 1. D&#233;coupage des formes en un 
</p>
<p>couple (stem, ending) &#224; partir de toutes les terminaisons possibles puis regroupement de ces 
</p>
<p>couples dans des paradigmes possibles issus du lexique existant. 2. S&#233;lection pour chaque mot 
</p>
<p>du paradigme permettant d&#8217;obtenir le plus de formes pr&#233;sentes dans le corpus. 3. 
R&#233;cup&#233;ration des cas non d&#233;cidables (m&#234;me nombre d&#8217;entr&#233;es entre deux paradigmes 
possibles pour un mot). 4. Utilisation d&#8217;Internet pour r&#233;soudre les cas pr&#233;c&#233;dents (recherche 
de la pr&#233;sence sur Internet des diff&#233;rentes formes fl&#233;chies possibles &#224; partir d&#8217;un mot et d&#8217;un 
paradigme) : les paradigmes ayant les formes les plus pr&#233;sentes sont valid&#233;s.  
</p>
<p>Cl&#233;ment (2004) travaille sur l&#8217;extraction d&#8217;un lexique morphologique fran&#231;ais &#224; partir d&#8217;un 
corpus en s&#8217;appuyant sur des couples (lemme, forme) et en associant &#224; un lemme une 
probabilit&#233; d&#8217;autant plus forte que beaucoup de formes peuvent lui &#234;tre associ&#233;es3. Ces 
travaux ont &#233;galement &#233;t&#233; effectu&#233;s sur le slovaque (Sagot, 2005) et le polonais (Sagot, 2007). 
</p>
<p>Cette m&#233;thode, aussi puissante soit-elle et bien que permettant de produire rapidement des 
</p>
<p>lexiques tr&#232;s volumineux, pr&#233;sente l&#8217;inconv&#233;nient de g&#233;n&#233;rer des entr&#233;es incompl&#232;tes. Ainsi, 
le Lefff contient des verbes dont toutes les formes ne sont pas pr&#233;sentes mais seulement celles 
</p>
<p>qui ont &#233;t&#233; rep&#233;r&#233;es dans le corpus. Il s&#8217;agit donc d&#8217;une m&#233;thode permettant de cr&#233;er un 
lexique d&#233;di&#233; &#224; un corpus et non un lexique de langue.  
</p>
<p>Zanchetta &amp; Baroni (2005) se base &#233;galement sur un corpus pour extraire un lexique 
</p>
<p>morphosyntaxique de l&#8217;italien4. Pour cela, ils commencent par utiliser TreeTagger (Schmid, 
1994) afin d&#8217;obtenir la cat&#233;gorie grammaticale et le lemme d&#8217;une forme donn&#233;e. Les lemmes 
ainsi obtenus ont ensuite &#233;t&#233; fl&#233;chis &#224; l&#8217;aide de r&#232;gles de flexions de l&#8217;italien. Nous noterons 
dans ce cas que le lexique g&#233;n&#233;r&#233; contient l&#8217;ensemble des flexions d&#8217;un lemme donn&#233;. En 
revanche, la m&#233;thode d&#8217;application des r&#232;gles est tr&#232;s manuelle puisque les auteurs ont 
</p>
<p>                                                 
3
  Ces travaux ont permis la cr&#233;ation d&#8217;un lexique morphosyntaxique du fran&#231;ais, le Lefff, disponible 
</p>
<p>gratuitement &#224; l&#8217;adresse suivante : http://www.labri.fr/perso/clement/lefff.  
</p>
<p>4
 Ces travaux ont permis la production du lexique Morph-it!, disponible &#224; l&#8217;adresse 
</p>
<p>http://dev.sslmit.unibo.it/linguistics/morph-it.php.  </p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Claude de Loupy, Micha&#235;l Bagur, Helena Blancafort 
</p>
<p>appliqu&#233; des r&#232;gles d&#8217;analyse des formes &#224; fl&#233;chir avec de nombreuses corrections manuelles 
de mani&#232;re &#224; obtenir un r&#233;sultat satisfaisant.  
</p>
<p>Par ailleurs, plusieurs exp&#233;riences ont &#233;t&#233; effectu&#233;es dans un contexte li&#233; plut&#244;t &#224; la 
</p>
<p>morphologie d&#233;rivationnelle qui nous int&#233;resse moins ici comme par exemple Dal &amp; Namer 
</p>
<p>(2000), Namer (1999), Hathout (2005), Hathout &amp; Tanguy (2005).  
</p>
<p>3 Fondements et buts de l&#8217;exp&#233;rience 
</p>
<p>Les travaux pr&#233;sent&#233;s ici se place dans un contexte plus large de construction de cha&#238;ne de 
</p>
<p>traitement permettant d&#8217;aider les linguistes dans leur t&#226;che de codage de ressources 
linguistiques (Loupy &amp; Gon&#231;alves, 2008). L&#8217;un des points de cette cha&#238;ne concerne 
l&#8217;inclusion de mots inconnus dans un lexique morphosyntaxique afin d&#8217;en augmenter sa 
couverture, soit sur un corpus sp&#233;cialis&#233;, soit sur la langue g&#233;n&#233;rale. Ces ajouts doivent &#234;tre 
</p>
<p>propos&#233;s aux linguistes selon un format correspondant &#224; celui utilis&#233; pour coder le lexique. 
</p>
<p>Habituellement, les lexiques morphosyntaxiques sont constitu&#233;s de triplet (forme, lemme, 
</p>
<p>cat&#233;gorie). La figure suivante montre un exemple typique issu du lexique fran&#231;ais MulText 
</p>
<p>(Ide and V&#233;ronis, 1994).  
</p>
<p>abaisse abaisser  Vmip3s- 
</p>
<p>abaissons abaisser  Vmip1p- 
</p>
<p>brioche brioche  Ncfs-- 
</p>
<p>brioches brioche  Ncfp-- 
</p>
<p>rends rendre  Vmip1s- 
</p>
<p>rends rendre  Vmip2s- 
</p>
<p>rend rendre  Vmip3s- 
</p>
<p>rendons rendre  Vmip1p- 
</p>
<p>statisticienne statisticien  Ncfs-- 
</p>
<p>statisticiennes statisticien  Ncfp&#8212; 
</p>
<p>Figure 1 : Structure habituelle d&#8217;un lexique morphosyntaxique 
</p>
<p>Ce type de format est difficile &#224; manipuler et rend les lexiques difficiles &#224; maintenir, &#224; 
</p>
<p>enrichir et &#224; contr&#244;ler pour &#233;viter des erreurs. Chaque forme fl&#233;chie est elle-m&#234;me une entr&#233;e 
</p>
<p>du lexique et est li&#233;e &#224; son lemme et &#224; l&#8217;ensemble de ses &#233;tiquettes. Pour des langues &#224; flexion 
riche, cela conduit &#224; un tr&#232;s grand nombre d&#8217;entr&#233;es pouvant aller jusqu&#8217;&#224; plusieurs millions 
dans certaines langues comme le russe.  
</p>
<p>Afin d&#8217;avoir un meilleur contr&#244;le de nos ressources lexicales, notre lexique SylLex est b&#226;ti 
sur une structure de type (lemme, paradigme) dans lequel les paradigmes d&#233;crivent 
</p>
<p>l&#8217;ensemble des op&#233;rations de flexion &#224; associer au lemme afin de g&#233;n&#233;rer ses flexions. Un 
format similaire est utilis&#233; pour coder le DELAS (G&#225;lvez, 2003). La figure suivante donne un 
</p>
<p>exemple de ce format.  
</p>
<p>abaisser V1 
</p>
<p>brioche N1 
</p>
<p>rendre V9 
</p>
<p>statisticien N13 
</p>
<p>Figure 2 : Structure de SylLex </p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Association automatique de lemmes et de paradigmes de flexion &#224; un mot inconnu 
</p>
<p>Les r&#232;gles sont d&#233;finies de fa&#231;on &#224; d&#233;crire les op&#233;rations de construction de flexions &#224; partir 
</p>
<p>des r&#232;gles comme indiqu&#233; dans la figure suivante
5
.  
</p>
<p>V1 0/a/Vmif3s--|0/ont/Vmif3p--|0/ai/Vmif1s--|0/ons/Vmif1p--|0/as/Vmif2s--
</p>
<p>|0/ez/Vmif2p--|2/ait/Vmii3s--|... 
</p>
<p>Figure 3 : Description des paradigmes 
</p>
<p>Les paradigmes et le lexique peuvent &#234;tre acc&#233;d&#233;s via une interface rendant plus simple la 
</p>
<p>manipulation des informations. Cette architecture de lexique est beaucoup plus facile &#224; 
</p>
<p>manipuler et &#224; appr&#233;hender pour les linguistes.  
</p>
<p>C&#8217;est donc dans ce contexte qu&#8217;il a &#233;t&#233; d&#233;cid&#233; de construire une cha&#238;ne de traitement des mots 
inconnus. Les m&#233;thodes pr&#233;sent&#233;es plus haut pr&#233;sentent toutes des inconv&#233;nients par rapport &#224; 
</p>
<p>cela. Celles qui n&#8217;utilisent que les corpus ne peuvent permettre de proposer des suggestions 
propres par rapport &#224; un format faisant r&#233;f&#233;rence &#224; des paradigmes de flexions &#233;tablis. Les 
</p>
<p>m&#233;thodes qui se basent sur une association (forme, lemme) ne permettent pas non plus de se 
</p>
<p>projeter de mani&#232;re fiable sur ces paradigmes. L&#8217;extraction automatique de paradigmes &#224; 
partir d&#8217;un lexique de type (forme, lemme, tag) pr&#233;sente d&#8217;ailleurs toujours des erreurs ou des 
complexit&#233;s inutiles du fait de la pr&#233;sence de mots ambigus. Ainsi, le verbe payer, du fait de 
</p>
<p>ses variantes (paye/paie) g&#233;n&#232;rera un paradigme diff&#233;rent du paradigme canonique du verbe 
</p>
<p>aimer alors que la prise en compte de variantes et donc d&#8217;un stem suppl&#233;mentaire permet 
d&#8217;&#233;liminer ce probl&#232;me.  
</p>
<p>Seule la m&#233;thode de Zanchetta &amp; Baroni pourrait convenir mais elle est beaucoup trop 
</p>
<p>manuelle et demanderait un travail tr&#232;s important de constitution d&#8217;un fl&#233;chisseur automatique 
pour chaque nouvelle langue.  
</p>
<p>Nous avons donc mis en place une m&#233;thode permettant d&#8217;associer directement un couple 
(lemme, paradigme) &#224; un mot inconnu. Cette m&#233;thode peut ensuite &#234;tre coupl&#233;e avec les 
</p>
<p>processus pr&#233;sent&#233;s dans les autres travaux afin de donner plus de poids &#224; des couples dont les 
</p>
<p>formes g&#233;n&#233;r&#233;s sont pr&#233;sentes dans un corpus ou sur Internet.  
</p>
<p>Le lexique fran&#231;ais sur lequel ont &#233;t&#233; faites les exp&#233;riences pr&#233;sent&#233;es dans cet article 
</p>
<p>contenait 60 000 couples (lemme, paradigme), il s&#8217;agit donc d&#8217;un petit lexique qui demande 
justement un important travail pour en augmenter la taille.  
</p>
<p>4 M&#233;thode utilis&#233;e 
</p>
<p>Le travail pr&#233;sent&#233; ici est bas&#233; sur des m&#233;thodes classiquement utilis&#233;es dans les guessers et 
</p>
<p>adapt&#233; &#224; la g&#233;n&#233;ration de couples (lemme, paradigme).  
</p>
<p>4.1 Les guessers 
</p>
<p>Il y a peu de publications d&#233;di&#233;es aux guessers. La plupart du temps, les proc&#233;dures de 
</p>
<p>guessing sont d&#233;crites &#224; l&#8217;int&#233;rieur de descriptions de taggers d&#232;s que l&#8217;on parle de mots 
</p>
<p>                                                 
5
  On pourra noter que l&#8217;encodage des &#233;tiquettes est bas&#233; sur les sp&#233;cifications de MulText (Ide &amp; V&#233;ronis, 
</p>
<p>1994). Malgr&#233; quelques changements, ce format nous a paru le plus pratique et il a surtout l&#8217;avantage d&#8217;avoir 
&#233;t&#233; test&#233; sur au mois 20 langues (V&#233;ronis &amp; Khouri, 1995).  </p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Claude de Loupy, Micha&#235;l Bagur, Helena Blancafort 
</p>
<p>inconnus (Chanod &amp; Tapanainen, 1995 ; Schmid, 1995). Mikheev (1997) consid&#232;re que les 
</p>
<p>guessers de cat&#233;gorie (Part Of Speech ou POS) peuvent utiliser trois indices :  
</p>
<p>1. Les pr&#233;fixes. Si impossible est un mot inconnu mais que possible est connu dans le 
lexique, il est probable que le POS du mot impossible soit le m&#234;me que celui du mot 
</p>
<p>possible. La pr&#233;cision de cet indice est &#233;lev&#233;e mais la couverture est en revanche 
</p>
<p>tr&#232;s faible (respectivement 93,5% et 6,5% selon Mikheev).  
</p>
<p>2. Les suffixes. Dans beaucoup de langues, les suffixes indiquent des propri&#233;t&#233;s 
grammaticales (pluriel, temps, etc.). La pr&#233;cision de cet indice est encore plus 
</p>
<p>&#233;lev&#233;e (96,8%) mais la couverture reste limit&#233;e (26,5%). 
</p>
<p>3. Les terminaisons (endings). Les terminaisons sont les derni&#232;res lettres des mots. Il 
ne s&#8217;agit pas de suffixes (ou alors elles le sont par hasard) car elles peuvent &#234;tre plus 
longues ou plus courtes que les suffixes r&#233;els des mots dont elles sont extraites. 
</p>
<p>Elles n&#8217;ont pas de signification grammaticale. Selon Mikheev, les terminaisons 
permettent d&#8217;obtenir des performances de l&#8217;ordre de 91,9% en pr&#233;cision et 78,2% en 
rappel.  
</p>
<p>Le guesser d&#233;crit ici ne fait appel qu&#8217;aux terminaisons mais nous comptons utiliser d&#8217;autres 
informations, les suffixes pouvant facilement &#234;tre extraits de nos paradigmes.   
</p>
<p>4.2 Description du guesser utilis&#233; 
</p>
<p>Habituellement, les guessers sont utilis&#233;s pour calculer le POS le plus probable pour un mot 
</p>
<p>inconnu, c'est-&#224;-dire &#55349;&#56387; &#55349;&#56417; &#55349;&#56420;  o&#249; &#55349;&#56417; repr&#233;sente une &#233;tiquette morphosyntaxique et  &#55349;&#56420; le mot 
inconnu. Ici, nous devons &#233;valuer la probabilit&#233; &#55349;&#56387; &#55349;&#56409;, &#55349;&#56389; &#55349;&#56420;  o&#249; &#55349;&#56409;&#55349;&#56407; repr&#233;sente un lemme et &#55349;&#56389; un 
paradigme.  
</p>
<p>En fait, &#233;tant donn&#233; que les paradigmes de flexion donnent toutes les formes qui sont 
</p>
<p>associ&#233;es &#224; un lemme, il suffit de calculer la probabilit&#233; &#55349;&#56387; &#55349;&#56417;, &#55349;&#56389; &#55349;&#56420; . Le fait de conna&#238;tre le 
POS et la r&#232;gle permet de retrouver tr&#232;s facilement le lemme tout en conservant l&#8217;information 
sur le mot analys&#233;. Une fois le paradigme et l&#8217;&#233;tiquette trouv&#233;s, le lemme peut &#234;tre d&#233;duit sans 
risque d&#8217;erreur.  
</p>
<p>Les statistiques sont apprises sur le lexique existant en utilisant un arbre classique sur les 
</p>
<p>terminaisons. Les terminaisons consid&#233;r&#233;es sont de longueur 1 &#224; 5. Pour chaque mot inconnu, 
</p>
<p>5 terminaisons sont utilis&#233;es, repr&#233;sent&#233;es par les n (1 &#8804; &#55349;&#56411; &#8804; 5) derni&#232;res lettres du mot 
inconnu. Pour chaque terminaison, un ou plusieurs lemmes candidats sont associ&#233;s avec une 
</p>
<p>fr&#233;quence calcul&#233;e sur le lexique. Une terminaison peut &#234;tre associ&#233;e &#224; plus d&#8217;un couple 
(lemme, paradigme).  
</p>
<p>L&#8217;approximation suivante est effectu&#233;e :  
</p>
<p>&#55349;&#56387; &#55349;&#56417;, &#55349;&#56389; &#55349;&#56420; &#8776;  &#55349;&#57100;&#55349;&#56406; &#8727; &#55349;&#56387; &#55349;&#56417;, &#55349;&#56389; [&#55349;&#56420;]&#55349;&#56406; 
</p>
<p>5
</p>
<p>&#55349;&#56406;=1
</p>
<p> 
</p>
<p>o&#249; [&#55349;&#56420;]&#55349;&#56406; repr&#233;sente la terminaison de longueur i du mot w.  
</p>
<p>et &#55349;&#57100;&#55349;&#56406; est un facteur de lissage. Elle est bas&#233;e sur l&#8217;entropie selon la formule suivante :  </p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Association automatique de lemmes et de paradigmes de flexion &#224; un mot inconnu 
</p>
<p>&#55349;&#57100;&#55349;&#56406; =  
1 &#8722; &#55349;&#56379;&#55349;&#56406;
</p>
<p>  1 &#8722; &#55349;&#56379;&#55349;&#56407;  
5
&#55349;&#56407;=1
</p>
<p> 
</p>
<p>o&#249; &#55349;&#56379;&#55349;&#56406;  repr&#233;sente l&#8217;entropie des terminaisons de longueur j au sein du lexique (vis-&#224;-vis de leur 
association &#224; un couple  &#55349;&#56417;, &#55349;&#56389; ). On a  
</p>
<p>&#55349;&#56379;&#55349;&#56406; =
 &#8462; &#55349;&#56411; &#55349;&#56411;&#8712;&#55349;&#56489;&#55349;&#56406;
&#55349;&#56374;&#55349;&#56398;&#55349;&#56415;&#55349;&#56401; &#55349;&#56489;&#55349;&#56406; 
</p>
<p> 
</p>
<p>&#8462; &#55349;&#56411; =  &#8722;  &#55349;&#56387; &#55349;&#56417;, &#55349;&#56389; &#8727; log   &#55349;&#56387; &#55349;&#56417;, &#55349;&#56389;  
 &#55349;&#56417; ,&#55349;&#56389; &#8712; &#55349;&#56495;&#55349;&#56411;
</p>
<p> 
</p>
<p>O&#249; &#55349;&#56489;&#55349;&#56406;  repr&#233;sente l&#8217;ensemble des n&#339;uds &#224; la profondeur &#55349;&#56406; (chaque n&#339;ud contient une seule 
terminaison [&#55349;&#56420;]&#55349;&#56406;) et &#55349;&#56495;&#55349;&#56411;  est l&#8217;ensemble des paires  &#55349;&#56417;, &#55349;&#56389;  possibles au n&#339;ud &#55349;&#56411;.  
</p>
<p>Lorsqu&#8217;un mot inconnu est rencontr&#233;, toutes les probabilit&#233;s &#55349;&#56387; &#55349;&#56417;, &#55349;&#56389; &#55349;&#56420;  sont calcul&#233;es pour 
tout couple  &#55349;&#56417;, &#55349;&#56389;  &#233;tant associ&#233; &#224; l&#8217;une des terminaison [&#55349;&#56420;]&#55349;&#56406;  du mot &#55349;&#56420;. Seuls les couples dont 
le score est sup&#233;rieur &#224; un certain seuil &#55349;&#57091; sont conserv&#233;s.  
</p>
<p>En dernier lieu, une simple v&#233;rification de coh&#233;rence est effectu&#233;e. Le calcul des probabilit&#233;s 
</p>
<p>est effectu&#233; sur les terminaisons mais la mise en relation avec un paradigme &#55349;&#56389; permet 
d&#8217;acc&#233;der au suffixe de la forme associ&#233;e au tag &#55349;&#56417; correspondant. Il suffit alors de v&#233;rifier que 
le suffixe du paradigme est conforme au mot inconnu. Si ce n&#8217;est pas le cas, le couple  &#55349;&#56417;, &#55349;&#56389;  
est &#233;limin&#233; des possibles.  
</p>
<p>Une fois un couple  &#55349;&#56417;, &#55349;&#56389;  il est alors imm&#233;diat de r&#233;cup&#233;rer un couple  &#55349;&#56409;, &#55349;&#56389;  donnant le lemme 
du mot inconnu. Ce couple est pr&#233;sent&#233; aux linguistes avec l&#8217;ensemble des flexions possibles.  
</p>
<p>Comme pr&#233;cis&#233; plus haut, il est tout &#224; fait possible (et nous le ferons) dans cette derni&#232;re 
</p>
<p>phase, de v&#233;rifier que les formes ainsi g&#233;n&#233;r&#233;es existent dans un corpus ou sur le web.  
</p>
<p>5 &#201;valuation 
</p>
<p>Seules les classes ouvertes (adjectif, adverbe, non, verbe) ont &#233;t&#233; trait&#233;es. L&#8217;&#233;valuation a &#233;t&#233; 
effectu&#233;e en prenant al&#233;atoirement 90% du lexique pour l&#8217;entra&#238;nement et 10% pour les tests. 
10 permutations ont &#233;t&#233; effectu&#233;es afin d&#8217;&#233;viter des probl&#232;mes locaux sp&#233;cifiques. Les 
performances sont indiqu&#233;es dans le tableau suivant. Les chiffres repr&#233;sentent les scores 
</p>
<p>moyens sur les 10 passes et la premi&#232;re colonne indique le seuil &#55349;&#57091; utilis&#233; pour la s&#233;lection des 
solutions.  
</p>
<p>&#55349;&#57149; Pr&#233;cision Rappel 
Nombre de couples  
</p>
<p>(lemme, paradigme) propos&#233;s 
</p>
<p>0 14.3% 91.5% 14.8 
</p>
<p>0.025 51.8% 90.5% 2.7 
</p>
<p>0.05 68.4% 84.5% 1.8 
</p>
<p>0.075 75.6% 73.7% 1.3 
</p>
<p>0.1 80.2% 66.9% 1 
</p>
<p>0.15 85.8% 52.6% 0.7 
</p>
<p>0.2 88.9% 40.2% 0.5 
</p>
<p>Table 1 : Performances du guesser </p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Claude de Loupy, Micha&#235;l Bagur, Helena Blancafort 
</p>
<p>Ces r&#233;sultats sont du m&#234;me ordre que ceux trouv&#233;s par Oliver &amp; Tadi&#231; (2004) puisque pour 
</p>
<p>une pr&#233;cision de 84,5, ils obtiennent un rappel de 38,4. Il est impossible de pousser la 
</p>
<p>comparaison plus loin car les langues de travail sont diff&#233;rentes mais nos r&#233;sultats semblent 
</p>
<p>donc coh&#233;rents.  
</p>
<p>&#201;tant donn&#233; que notre but est d&#8217;aider les linguistes dans leur t&#226;che, il n&#8217;est pas envisageable 
</p>
<p>de proposer pr&#232;s de 15 couples &#224; valider (tous les couples possibles sont fournis avec &#55349;&#57091; = 0). 
De plus, plusieurs m&#233;thodes pr&#233;sent&#233;es dans l&#8217;&#233;tat de l&#8217;art v&#233;rifient l&#8217;existence d&#8217;une forme 
fl&#233;chie g&#233;n&#233;r&#233;e au sein d&#8217;un corpus ou sur Internet. Si 15 paradigmes de flexion sont associ&#233;s 
en moyenne &#224; un mot inconnu, sachant qu&#8217;il y a en moyenne 25 flexions par paradigmes, cela 
fait 375 requ&#234;tes par mot inconnu en moyenne. Le temps n&#233;cessaire devient alors beaucoup 
</p>
<p>trop important, si l&#8217;on veut g&#233;n&#233;rer des propositions rapidement. En particulier, pour des 
recherches sur Internet, 1000 mots inconnus demandent 375 000 requ&#234;tes, ce qui ne se fait pas 
</p>
<p>en une nuit et risque de poser des soucis avec le moteur utilis&#233;.  
</p>
<p>Nous pouvons constater que l&#8217;utilisation d&#8217;un seuil tr&#232;s faible permet &#224; la fois de diminuer 
consid&#233;rablement le nombre de propositions (plus de 5 fois moins), d&#8217;augmenter de mani&#232;re 
tr&#232;s nette la pr&#233;cision (3,6 fois plus &#233;lev&#233;e) tout en ne diminuant quasiment pas le rappel (1 
</p>
<p>point de perte).  
</p>
<p>Le calcul d&#8217;une probabilit&#233; &#55349;&#56387; &#55349;&#56409;, &#55349;&#56389; &#55349;&#56420;  permet donc d&#8217;am&#233;liorer de mani&#232;re tr&#232;s nette la vitesse 
de codage, que ce soit par la pr&#233;sentation de moins de possibilit&#233;s aux linguistes ou par la 
</p>
<p>diminution des v&#233;rifications &#224; effectuer. Par ailleurs, cette m&#233;thode permet d&#8217;obtenir des 
entr&#233;e de lexique qui sont compl&#232;tes puisque g&#233;n&#233;r&#233;es &#224; partir de paradigmes de flexion 
</p>
<p>valid&#233;s au pr&#233;alable.  
</p>
<p>6 Conclusion et perspectives 
</p>
<p>Le syst&#232;me d&#233;crit dans cet article permet d&#8217;associer &#224; un mot inconnu des couples (lemme, 
paradigme de flexion). Les performances ne sont pas tr&#232;s &#233;lev&#233;es si on les compare &#224; celles 
</p>
<p>qu&#8217;obtiennent les guessers sur les &#233;tiquettes morphosyntaxiques. N&#233;anmoins, l&#8217;utilisation de 
cette m&#233;thode permet de proposer peu de choix &#224; la validation humaine tout en gardant une 
</p>
<p>tr&#232;s bonne couverture. Cela permet &#233;galement de diminuer consid&#233;rablement le nombre de 
</p>
<p>formes &#224; tester, que ce soit sur un corpus local ou sur le web.  
</p>
<p>Par ailleurs, de nombreuses pistes d&#8217;am&#233;lioration sont &#224; envisager. D&#233;j&#224;, la v&#233;rification de 
</p>
<p>l&#8217;existence des formes fl&#233;chies g&#233;n&#233;r&#233;es par les couples  &#55349;&#56409;, &#55349;&#56389; , telle que plusieurs travaux la 
pratiquent, devrait apporter une nette am&#233;lioration. De plus, les indices que repr&#233;sentent les 
</p>
<p>pr&#233;fixes, les suffixes grammaticaux et le contexte dans lesquels apparaissent les mots 
</p>
<p>inconnus devraient &#233;galement donner des r&#233;sultats encore plus int&#233;ressants.  
</p>
<p>Enfin, les exp&#233;riences pr&#233;sent&#233;es ici ont &#233;t&#233; men&#233;es en utilisant l&#8217;ensemble des r&#232;gles de 
flexion. Or, certains paradigmes non productifs (verbe &#234;tre par exemple) polluent les 
</p>
<p>probabilit&#233;s ci-dessus. Il convient donc de refaire ces exp&#233;riences en &#233;valuant la productivit&#233; 
</p>
<p>des paradigmes utilis&#233;s. Dans le m&#234;me ordre d&#8217;id&#233;e, la fr&#233;quence des paradigmes dans le 
lexique d&#8217;apprentissage n&#8217;a pas &#233;t&#233; utilis&#233;e. Or, par analogie avec d&#8217;autres ph&#233;nom&#232;nes 
linguistiques, nous pouvons supposer que le simple fait d&#8217;associer aux inconnus les r&#232;gles 
selon leur fr&#233;quence devrait permettre d&#8217;augmenter encore les performances.  
</p>
<p>Le traitement des formes polylexicales demandera une attention particuli&#232;re. Cependant, le 
</p>
<p>codage adopt&#233; pour ces formes dans notre lexique (r&#232;gle de flexion &#224; appliquer aux </p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Association automatique de lemmes et de paradigmes de flexion &#224; un mot inconnu 
</p>
<p>composants si c&#8217;est le cas + connaissance de la t&#234;te du mot compos&#233; s&#8217;il y en a une) nous 
permet de faire un apprentissage assez simple en reprenant la m&#233;thode pr&#233;sent&#233;e ici. Nous 
</p>
<p>testerons les r&#233;sultats obtenus ainsi.  
</p>
<p>Lors de prochaines exp&#233;riences, nous &#233;valuerons les lexiques produits en termes de 
</p>
<p>couverture et le temps que de tels outils peuvent faire gagner lors du codage d&#8217;informations 
morphosyntaxiques par des linguistes. Bien que cette &#233;valuation puisse &#234;tre biais&#233;e par 
</p>
<p>l&#8217;interface qui sera utilis&#233;e, les r&#233;sultats en sont importants pour d&#233;terminer s&#8217;il est possible de 
cr&#233;er des lexiques fiables et complets de mani&#232;re rapide en conservant une validation 
</p>
<p>manuelle dans la boucle.  
</p>
<p>Enfin, nous comptons &#233;galement effectuer des comparaisons de performances de langue &#224; 
</p>
<p>langue dans le m&#234;me esprit que de pr&#233;c&#233;dentes exp&#233;riences de comparaison (Blancafort &amp; 
</p>
<p>Loupy, 2008).  
</p>
<p>R&#233;f&#233;rences 
</p>
<p>BLANCAFORT H., LOUPY C. DE, (2008). Comparing languages from vocabulary growth to 
</p>
<p>inflection paradigms &#8211; A study run on parallel corpora and multilingual lexicons. Actes de 
SEPLN'2008. Madrid, Espagne. 
</p>
<p>CHANOD J.P., TAPANAINEN P., (1995). Creating a Tagset, Lexicon and Guesser for a French 
</p>
<p>Tagger; Proceedings of the European Chapter of the ACL SIGDAT Workshop From text to 
</p>
<p>tags : Issues in Multilingual Language Analysis, pp. 51-57, Dublin, Irelande.  
</p>
<p>CL&#201;MENT L., SAGOT B., LANG B., (2004). Morphology based automatic acquisition of large-
</p>
<p>coverage lexica. In Proceedings of LREC'04, Lisbonne, Portugal. pp. 1841-1844. 
</p>
<p>CUCERZAN S., YAROWSKY D., (2000). Language Independent, Minimally Supervised 
</p>
<p>Induction of Lexical Probabilities. Proceedings of ACL-2000, Hong Kong, pp. 270-277. 
</p>
<p>DAL G., NAMER F., (2000). G&#233;D&#233;riF: automatic generation and analysis of morphologically 
</p>
<p>constructed lexical resources. In actes de Second International Conference on Language 
</p>
<p>Resources and Evaluation, Athens, Gr&#232;ce, pp. 1447-1454. 
</p>
<p>DEMPSTER A.P., LAIRD N. M., RUBIN D. B., (1977). Maximum likelihood from incomplete 
</p>
<p>data via the EM algorithm. Journal of the Royal Statistical Society, B 39(1):1-38. 
</p>
<p>GALVEZ, C., (2006). El diccionario electr&#243;nico: un instrumento para la unificaci&#243;n de 
</p>
<p>t&#233;rminos en la indizaci&#243;n autom&#225;tica. Linguax: Revista de Lenguas Aplicadas (ISSN 1695-
</p>
<p>632X). 
</p>
<p>HATHOUT N., TANGUY L., (2005). Webaffix : une bo&#238;te &#224; outils d'acquisition lexicale &#224; partir 
</p>
<p>du Web. In Revue Qu&#233;b&#233;quoise de Linguistique. Volume 32, num&#233;ro 1. 
</p>
<p>HATHOUT N., (2005). Exploiter la structure analogique du lexique construit : une approche 
</p>
<p>computationnelle. In Cahiers de Lexicologie. Volume 87, num&#233;ro 2. 
</p>
<p>IDE N., V&#201;RONIS J., (1994). MULTEXT: Multilingual Text Tools and Corpora. Proceedings of 
</p>
<p>the 15th International Conference on Computational Linguistics, COLING'94, Kyoto, Japon, 
</p>
<p>588-92. </p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Claude de Loupy, Micha&#235;l Bagur, Helena Blancafort 
</p>
<p>LOUPY C. DE, GON&#199;ALVES S., (2008). Aide &#224; la construction de lexiques morphosyntaxiques. 
</p>
<p>Actes de EURALEX 2008. Barcelone, Espagne. 
</p>
<p>MIKHEEV A., (1997). Automatic Rule Induction for Unknown-Word Guessing. In 
</p>
<p>Computational Linguistics vol 23(3), ACL 1997. pp. 405-423. 
</p>
<p>NAKOV P., BONEV Y., ANGELOVA G., GIUS E., VON HAHN W., (2003). Guessing 
</p>
<p>Morphological Classes of Unknown German Nouns. In Proceedings of Recent Advances in 
</p>
<p>Natural Language Processing (RANLP&#8217;03). pp. 319-326. Borovetz, Bulgarie. 
</p>
<p>NAMER F., (1999). Le traitement automatique des mots d&#233;riv&#233;s : le cas des noms et adjectifs 
</p>
<p>en -et(te). in D. Corbin, G. Dal, B. Fradin, B. Habert., F. Kerleroux, M. Pl&#233;nat &amp; M. Roch&#233; 
</p>
<p>&#233;ds, La morphologie des d&#233;riv&#233;s &#233;valuatifs, Silexicales 2, pp. 169-179. Villeneuve d&#8217;Ascq.  
</p>
<p>OLIVER A., TADI&#262; M., (2004). Enlarging the Croatian Morphological Lexicon by Automatic 
</p>
<p>Lexical Acquisition from Raw Corpora. In: Proceedings of the 4
th
</p>
<p> International Conference of 
</p>
<p>Language Resources and Evaluation (LREC 2004), p. 1259&#8211;1262. Lisbonne, Portugal. 
</p>
<p>OLIVER A., CASTELL&#211;N I., M&#192;RQUEZ L., (2003). Use of internet for augmenting coverage in a 
</p>
<p>lexical acquisition system from raw corpora. In: Proceedings of the RANLP 2003 
</p>
<p>International Workshop on Information Extraction for Slavonic and Other Central and Eastern 
</p>
<p>European Languages (IESL 2003). Borovets, Bulgarie. 
</p>
<p>PORTER M., (1980). An algorithm for suffix stripping. in Program, n&#176;14, 130-137. 
</p>
<p>RISSANEN J., (1989). Stochastic Complexity in Statistical Inquiry. World Scientific Publishing 
</p>
<p>Co, Singapour. 
</p>
<p>SAGOT B., (2005). Automatic acquisition of a Slovak Lexicon from a Raw Corpus. In Lecture 
</p>
<p>Notes in Artificial Intelligence 3658 (Springer-Verlag), Proceedings of TSD'05, Karlovy 
</p>
<p>Vary, R&#233;publique Tch&#232;que. pp. 156-163.  
</p>
<p>SAGOT B., (2007). Building a morphosyntactic lexicon and a pre-syntactic processing chain 
</p>
<p>for Polish. In: Proceedings of LTC 2007, Pozna&#324;, Pologne. 
</p>
<p>SCHMID H., (1994). Probabilistic Part-of-Speech Tagging Using Decision Trees. Proceedings 
</p>
<p>of International Conference on New Methods in Language Processing. September 1994. 
</p>
<p>SCHMID H., (1995). Improvements in part-of-speech tagging with an application to German. 
</p>
<p>in Proceedings of the ACL SIGDAT-Workshop, pp. 47-50. 
</p>
<p>V&#201;RONIS J., KHOURI L., (1995). &#201;tiquetage grammatical multilingue: le projet Multext. 
</p>
<p>Traitement Automatique des Langues, 36(1/2), 233-248. 
</p>
<p> </p>

</div></div>
</body></html>