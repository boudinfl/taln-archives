TALN 2009â€“ Session posters, Senlis, 24-26 juin 2009 
Association automatique de lemmes et de paradigmes de flexion Ã  
un mot inconnu 
Claude de Loupy (1,2), MichaÃ«l Bagur (1), Helena Blancafort (1,3) 
(1)  Syllabs â€“ 15, rue Jean Baptiste Berlier, 75013 Paris 
blancafort@syllabs.com; bagur@syllabs.com; loupy@syllabs.com 
(2) MoDyCo â€“ UniversitÃ© Paris 10, 200 Av. de la RÃ©publique, Nanterre 
(3) Universitat Pompeu Fabra, Roc Boronat 138, 08018 Barcelona  
 
RÃ©sumÃ©  
La maintenance et lâ€™enrichissement des lexiques morphosyntaxiques sont souvent des tÃ¢ches 
fastidieuses. Dans cet article nous prÃ©sentons la mise en place dâ€™une procÃ©dure de guessing de 
flexion afin dâ€™aider les linguistes dans leur travail de lexicographes. Le guesser dÃ©veloppÃ© ne 
fait pas quâ€™Ã©valuer lâ€™Ã©tiquette morphosyntaxique comme câ€™est gÃ©nÃ©ralement le cas. Il propose 
pour un mot franÃ§ais inconnu, un ou plusieurs candidats-lemmes, ainsi que les paradigmes de 
flexion associÃ©s (formes flÃ©chies et Ã©tiquettes morphosyntaxiques). Dans cet article, nous 
dÃ©crivons le modÃ¨le probabiliste utilisÃ© ainsi que les rÃ©sultats obtenus. La mÃ©thode utilisÃ©e 
permet de rÃ©duire considÃ©rablement le nombre de rÃ¨gles Ã  valider, permettant ainsi un gain de 
temps important. 
Abstract  
Lexicon maintenance and lexicon enrichment is a labour-intensive task. In this paper, we 
present preliminary work on an inflectional guessing procedure for helping the linguist in 
lexicographic tasks. The guesser presented here does not only output morphosyntactic tags, 
but also suggests for an unknown French word one or more lemma candidates as well as their 
corresponding inflectional rules and morphosyntactic tags that the linguist has to validate. In 
this article, we present the probabilistic model we used as well as obtained results. The 
method allows a drastic reduction of the number of rules to validate.  
Mots-clÃ©s :   guesser, lexiques morphosyntaxiques, aide aux linguistes, induction des 
rÃ¨gles de flexion 
 
Keywords:   guesser, morphosyntactic lexica, aide to the linguist, induction of inflection 
rules 
 
Claude de Loupy, MichaÃ«l Bagur, Helena Blancafort 
1 Introduction 
Les ressources lexicales sont fondamentalement importantes en Traitement Automatique des 
Langues (TAL). Les lexiques morphosyntaxiques en particulier sont trÃ¨s utilisÃ©s par des 
1
traitements de relativement bas niveau qui sont donc la base de traitements ultÃ©rieurs . Or, la 
gestion et la maintenance de tels lexiques est trÃ¨s consommatrice en temps et en Ã©nergie. Il 
nâ€™est pas rare quâ€™un organisme travaillant en TAL ait Ã  gÃ©rer plusieurs lexiques de ce type, 
que ce soit pour une mÃªme langue avec des spÃ©cialisations de ressources ou pour le traitement 
de plusieurs langues. Du fait de la crÃ©ativitÃ© des langues, les lexiques doivent Ãªtre en 
permanence mis Ã  jour et il est important de penser des mÃ©thodes permettant de faciliter ce 
travail.  
Lâ€™un des moyens les plus courants pour enrichir ces lexiques consiste Ã  analyser des corpus, Ã  
rÃ©cupÃ©rer les formes inconnues du lexique Ã  enrichir et Ã  prÃ©senter les plus courantes Ã  un/e 
linguiste qui se charge alors dâ€™introduire non seulement la forme en question mais Ã©galement 
ses Ã©tiquettes morphosyntaxiques, son ou ses lemmes ainsi que toutes les formes flÃ©chies du 
ou des lemmes en question. Cette tÃ¢che est gÃ©nÃ©ralement effectuÃ©e Ã  la main ou en utilisant 
des outils spÃ©cifiques (flÃ©chisseur) permettant de flÃ©chir un lemme selon des paradigmes de 
flexion connus. MÃªme dans ce dernier cas. Lâ€™ensemble de la tÃ¢che prend beaucoup de temps 
car il est nÃ©cessaire de dÃ©cider quel est le lemme correspondant Ã  une forme ainsi que le 
numÃ©ro ou la dÃ©nomination de la rÃ¨gle de flexion qui doit lui Ãªtre associÃ©e.  
Lâ€™objet de cet article est dâ€™Ã©valuer les performances de mÃ©thodes classiques utilisÃ©es dans les 
guessers de catÃ©gories grammaticales lorsquâ€™on les applique Ã  lâ€™association de couples 
(lemme, paradigme de flexion) Ã  un mot inconnu. Un Ã©tat de lâ€™art est donnÃ© en section 2. Les 
fondements et les buts de lâ€™expÃ©rience dÃ©crite sont ensuite exposÃ©s en section 3 afin 
dâ€™expliquer pourquoi les mÃ©thodes prÃ©cÃ©dentes ne sont pas satisfaisantes pour les conditions 
visÃ©es. La section 4 prÃ©sente la mÃ©thode utilisÃ©e. Enfin, la section 5 prÃ©sente lâ€™Ã©valuation des 
rÃ©sultats.  
2 Ã‰tat de lâ€™art 
Lâ€™idÃ©e de fournir une aide supplÃ©mentaire aux linguistes dans cette Ã©tape dâ€™enrichissement 
court depuis dÃ©jÃ  un certain temps. Les premiers travaux en construction automatique de 
lexiques morphosyntaxiques nâ€™utilisaient quâ€™un corpus comme source dâ€™information. Ainsi, 
Jacquemin (1997) compare les terminaisons des mots de son corpus de maniÃ¨re Ã  grouper des 
mots ayant un mÃªme stem puis regroupe les groupes de mots ayant la mÃªme suite de 
terminaisons. Goldsmith (1997 ; 2000) effectue Ã©galement ce type de procÃ©dure en appliquant 
les mÃ©thodes de Minimum Description Length (Rissanen, 1989) et dâ€™Expectation 
2
Maximization (Dempster et al., 1977) . Schone & Jurasky (2000 ; 2001) utilisent la mÃ©thode 
du Latent Semantic Analysis (Deerwester et al., 1990).  
                                                 
1
  Nous nâ€™entrerons pas ici dans un dÃ©bat sur lâ€™utilitÃ© ou non de ces lexiques en comparaison Ã  dâ€™autres moyens 
dâ€™analyse plus sommaire comme les stemmers de type Porter (1980). Nous pensons que ces lexiques sont 
utiles et de nombreux autres travaux les utilisent, ce qui suffit Ã  justifier un travail sur leur crÃ©ation.  
2
  Lâ€™outil Linguistica rÃ©sultant de ces travaux est tÃ©lÃ©chargeable Ã  lâ€™adresse suivante : 
http://linguistica.uchicago.edu/.  
Association automatique de lemmes et de paradigmes de flexion Ã  un mot inconnu 
Dans tous les cas, les systÃ¨mes proposÃ©s produisent des listes de stems associÃ©s Ã  des 
paradigmes de flexion permettant de gÃ©nÃ©rer les formes flÃ©chies trouvÃ©es dans les corpus 
analysÃ©s. Le principal problÃ¨me de ces mÃ©thodes est que les rÃ©sultats gÃ©nÃ©rÃ©s sont trÃ¨s 
difficilement utilisables par des linguistes car ils nâ€™ont que peu de liens avec les rÃ¨gles 
morphologiques auxquelles on sâ€™attend. Ces travaux sont donc principalement utilisables dans 
un contexte totalement automatique avec toutes les erreurs que cela peut comporter.  
Dâ€™autres expÃ©riences utilisent des ressources lexicales existantes ou des paradigmes de 
flexion dÃ©jÃ  connus en plus des corpus Ã  analyser. Nakov et al. (2003), travaillent sur 
lâ€™allemand et commencent par extraire, de maniÃ¨re automatique, toutes les terminaisons 
possibles pour les mots prÃ©sents dans un lexique. Ils extraient ensuite les mots prÃ©sents dans 
un corpus qui sont inconnus dudit lexique, puis gÃ©nÃ¨rent tous les stems possibles Ã  partir des 
terminaisons pour chacun de ces mots. Ils utilisent enfin lâ€™algorithme de Maximum Likelihood 
Estimation avec des paramÃ¨tres prÃ©conisÃ©s par Mikheev (1997) afin de rÃ©cupÃ©rer les rÃ¨gles de 
flexion les plus intÃ©ressantes (notions de qualitÃ©, de longueur et de frÃ©quences).  
Oliver & TadiÃ§ (2004) prÃ©sentent lâ€™application sur le Croate de mÃ©thodes testÃ©es auparavant 
sur le russe (Oliver et al., 2003). Ils sâ€™appuient sur un lexique morphosyntaxique existant dont 
ils extraient des paradigmes de flexion et sur un corpus Ã  partir duquel ils complÃ¨tent le 
lexique prÃ©cÃ©dent. Le processus est dÃ©coupÃ© en 4 Ã©tapes : 1. DÃ©coupage des formes en un 
couple (stem, ending) Ã  partir de toutes les terminaisons possibles puis regroupement de ces 
couples dans des paradigmes possibles issus du lexique existant. 2. SÃ©lection pour chaque mot 
du paradigme permettant dâ€™obtenir le plus de formes prÃ©sentes dans le corpus. 3. 
RÃ©cupÃ©ration des cas non dÃ©cidables (mÃªme nombre dâ€™entrÃ©es entre deux paradigmes 
possibles pour un mot). 4. Utilisation dâ€™Internet pour rÃ©soudre les cas prÃ©cÃ©dents (recherche 
de la prÃ©sence sur Internet des diffÃ©rentes formes flÃ©chies possibles Ã  partir dâ€™un mot et dâ€™un 
paradigme) : les paradigmes ayant les formes les plus prÃ©sentes sont validÃ©s.  
ClÃ©ment (2004) travaille sur lâ€™extraction dâ€™un lexique morphologique franÃ§ais Ã  partir dâ€™un 
corpus en sâ€™appuyant sur des couples (lemme, forme) et en associant Ã  un lemme une 
probabilitÃ© dâ€™autant plus forte que beaucoup de 3 formes peuvent lui Ãªtre associÃ©es . Ces 
travaux ont Ã©galement Ã©tÃ© effectuÃ©s sur le slovaque (Sagot, 2005) et le polonais (Sagot, 2007). 
Cette mÃ©thode, aussi puissante soit-elle et bien que permettant de produire rapidement des 
lexiques trÃ¨s volumineux, prÃ©sente lâ€™inconvÃ©nient de gÃ©nÃ©rer des entrÃ©es incomplÃ¨tes. Ainsi, 
le Lefff contient des verbes dont toutes les formes ne sont pas prÃ©sentes mais seulement celles 
qui ont Ã©tÃ© repÃ©rÃ©es dans le corpus. Il sâ€™agit donc dâ€™une mÃ©thode permettant de crÃ©er un 
lexique dÃ©diÃ© Ã  un corpus et non un lexique de langue.  
Zanchetta & Baroni (2005) se base Ã©galement sur un corpus pour extraire un lexique 
morphosyntaxique de lâ€™italien4. Pour cela, ils commencent par utiliser TreeTagger (Schmid, 
1994) afin dâ€™obtenir la catÃ©gorie grammaticale et le lemme dâ€™une forme donnÃ©e. Les lemmes 
ainsi obtenus ont ensuite Ã©tÃ© flÃ©chis Ã  lâ€™aide de rÃ¨gles de flexions de lâ€™italien. Nous noterons 
dans ce cas que le lexique gÃ©nÃ©rÃ© contient lâ€™ensemble des flexions dâ€™un lemme donnÃ©. En 
revanche, la mÃ©thode dâ€™application des rÃ¨gles est trÃ¨s manuelle puisque les auteurs ont 
                                                 
3
  Ces travaux ont permis la crÃ©ation dâ€™un lexique morphosyntaxique du franÃ§ais, le Lefff, disponible 
gratuitement Ã  lâ€™adresse suivante : http://www.labri.fr/perso/clement/lefff.  
4
 Ces travaux ont permis la production du lexique Morph-it!, disponible Ã  lâ€™adresse 
http://dev.sslmit.unibo.it/linguistics/morph-it.php.  
Claude de Loupy, MichaÃ«l Bagur, Helena Blancafort 
appliquÃ© des rÃ¨gles dâ€™analyse des formes Ã  flÃ©chir avec de nombreuses corrections manuelles 
de maniÃ¨re Ã  obtenir un rÃ©sultat satisfaisant.  
Par ailleurs, plusieurs expÃ©riences ont Ã©tÃ© effectuÃ©es dans un contexte liÃ© plutÃ´t Ã  la 
morphologie dÃ©rivationnelle qui nous intÃ©resse moins ici comme par exemple Dal & Namer 
(2000), Namer (1999), Hathout (2005), Hathout & Tanguy (2005).  
3 Fondements et buts de lâ€™expÃ©rience 
Les travaux prÃ©sentÃ©s ici se place dans un contexte plus large de construction de chaÃ®ne de 
traitement permettant dâ€™aider les linguistes dans leur tÃ¢che de codage de ressources 
linguistiques (Loupy & GonÃ§alves, 2008). Lâ€™un des points de cette chaÃ®ne concerne 
lâ€™inclusion de mots inconnus dans un lexique morphosyntaxique afin dâ€™en augmenter sa 
couverture, soit sur un corpus spÃ©cialisÃ©, soit sur la langue gÃ©nÃ©rale. Ces ajouts doivent Ãªtre 
proposÃ©s aux linguistes selon un format correspondant Ã  celui utilisÃ© pour coder le lexique. 
Habituellement, les lexiques morphosyntaxiques sont constituÃ©s de triplet (forme, lemme, 
catÃ©gorie). La figure suivante montre un exemple typique issu du lexique franÃ§ais MulText 
(Ide and VÃ©ronis, 1994).  
abaisse abaisser  Vmip3s- 
abaissons abaisser  Vmip1p- 
brioche brioche  Ncfs-- 
brioches brioche  Ncfp-- 
rends rendre  Vmip1s- 
rends rendre  Vmip2s- 
rend rendre  Vmip3s- 
rendons rendre  Vmip1p- 
statisticienne statisticien  Ncfs-- 
statisticiennes statisticien  Ncfpâ€” 
Figure 1 : Structure habituelle dâ€™un lexique morphosyntaxique 
Ce type de format est difficile Ã  manipuler et rend les lexiques difficiles Ã  maintenir, Ã  
enrichir et Ã  contrÃ´ler pour Ã©viter des erreurs. Chaque forme flÃ©chie est elle-mÃªme une entrÃ©e 
du lexique et est liÃ©e Ã  son lemme et Ã  lâ€™ensemble de ses Ã©tiquettes. Pour des langues Ã  flexion 
riche, cela conduit Ã  un trÃ¨s grand nombre dâ€™entrÃ©es pouvant aller jusquâ€™Ã  plusieurs millions 
dans certaines langues comme le russe.  
Afin dâ€™avoir un meilleur contrÃ´le de nos ressources lexicales, notre lexique SylLex est bÃ¢ti 
sur une structure de type (lemme, paradigme) dans lequel les paradigmes dÃ©crivent 
lâ€™ensemble des opÃ©rations de flexion Ã  associer au lemme afin de gÃ©nÃ©rer ses flexions. Un 
format similaire est utilisÃ© pour coder le DELAS (GÃ¡lvez, 2003). La figure suivante donne un 
exemple de ce format.  
abaisser V1 
brioche N1 
rendre V9 
statisticien N13 
Figure 2 : Structure de SylLex 
Association automatique de lemmes et de paradigmes de flexion Ã  un mot inconnu 
Les rÃ¨gles sont dÃ©finies de faÃ§on Ã  dÃ©crire les opÃ©rations de construction de flexions Ã  partir 
5
des rÃ¨gles comme indiquÃ© dans la figure suivante .  
V1 0/a/Vmif3s--|0/ont/Vmif3p--|0/ai/Vmif1s--|0/ons/Vmif1p--|0/as/Vmif2s--
|0/ez/Vmif2p--|2/ait/Vmii3s--|... 
Figure 3 : Description des paradigmes 
Les paradigmes et le lexique peuvent Ãªtre accÃ©dÃ©s via une interface rendant plus simple la 
manipulation des informations. Cette architecture de lexique est beaucoup plus facile Ã  
manipuler et Ã  apprÃ©hender pour les linguistes.  
Câ€™est donc dans ce contexte quâ€™il a Ã©tÃ© dÃ©cidÃ© de construire une chaÃ®ne de traitement des mots 
inconnus. Les mÃ©thodes prÃ©sentÃ©es plus haut prÃ©sentent toutes des inconvÃ©nients par rapport Ã  
cela. Celles qui nâ€™utilisent que les corpus ne peuvent permettre de proposer des suggestions 
propres par rapport Ã  un format faisant rÃ©fÃ©rence Ã  des paradigmes de flexions Ã©tablis. Les 
mÃ©thodes qui se basent sur une association (forme, lemme) ne permettent pas non plus de se 
projeter de maniÃ¨re fiable sur ces paradigmes. Lâ€™extraction automatique de paradigmes Ã  
partir dâ€™un lexique de type (forme, lemme, tag) prÃ©sente dâ€™ailleurs toujours des erreurs ou des 
complexitÃ©s inutiles du fait de la prÃ©sence de mots ambigus. Ainsi, le verbe payer, du fait de 
ses variantes (paye/paie) gÃ©nÃ¨rera un paradigme diffÃ©rent du paradigme canonique du verbe 
aimer alors que la prise en compte de variantes et donc dâ€™un stem supplÃ©mentaire permet 
dâ€™Ã©liminer ce problÃ¨me.  
Seule la mÃ©thode de Zanchetta & Baroni pourrait convenir mais elle est beaucoup trop 
manuelle et demanderait un travail trÃ¨s important de constitution dâ€™un flÃ©chisseur automatique 
pour chaque nouvelle langue.  
Nous avons donc mis en place une mÃ©thode permettant dâ€™associer directement un couple 
(lemme, paradigme) Ã  un mot inconnu. Cette mÃ©thode peut ensuite Ãªtre couplÃ©e avec les 
processus prÃ©sentÃ©s dans les autres travaux afin de donner plus de poids Ã  des couples dont les 
formes gÃ©nÃ©rÃ©s sont prÃ©sentes dans un corpus ou sur Internet.  
Le lexique franÃ§ais sur lequel ont Ã©tÃ© faites les expÃ©riences prÃ©sentÃ©es dans cet article 
contenait 60 000 couples (lemme, paradigme), il sâ€™agit donc dâ€™un petit lexique qui demande 
justement un important travail pour en augmenter la taille.  
4 MÃ©thode utilisÃ©e 
Le travail prÃ©sentÃ© ici est basÃ© sur des mÃ©thodes classiquement utilisÃ©es dans les guessers et 
adaptÃ© Ã  la gÃ©nÃ©ration de couples (lemme, paradigme).  
4.1 Les guessers 
Il y a peu de publications dÃ©diÃ©es aux guessers. La plupart du temps, les procÃ©dures de 
guessing sont dÃ©crites Ã  lâ€™intÃ©rieur de descriptions de taggers dÃ¨s que lâ€™on parle de mots 
                                                 
5
  On pourra noter que lâ€™encodage des Ã©tiquettes est basÃ© sur les spÃ©cifications de MulText (Ide & VÃ©ronis, 
1994). MalgrÃ© quelques changements, ce format nous a paru le plus pratique et il a surtout lâ€™avantage dâ€™avoir 
Ã©tÃ© testÃ© sur au mois 20 langues (VÃ©ronis & Khouri, 1995).  
Claude de Loupy, MichaÃ«l Bagur, Helena Blancafort 
inconnus (Chanod & Tapanainen, 1995 ; Schmid, 1995). Mikheev (1997) considÃ¨re que les 
guessers de catÃ©gorie (Part Of Speech ou POS) peuvent utiliser trois indices :  
1. Les prÃ©fixes. Si impossible est un mot inconnu mais que possible est connu dans le 
lexique, il est probable que le POS du mot impossible soit le mÃªme que celui du mot 
possible. La prÃ©cision de cet indice est Ã©levÃ©e mais la couverture est en revanche 
trÃ¨s faible (respectivement 93,5% et 6,5% selon Mikheev).  
2. Les suffixes. Dans beaucoup de langues, les suffixes indiquent des propriÃ©tÃ©s 
grammaticales (pluriel, temps, etc.). La prÃ©cision de cet indice est encore plus 
Ã©levÃ©e (96,8%) mais la couverture reste limitÃ©e (26,5%). 
3. Les terminaisons (endings). Les terminaisons sont les derniÃ¨res lettres des mots. Il 
ne sâ€™agit pas de suffixes (ou alors elles le sont par hasard) car elles peuvent Ãªtre plus 
longues ou plus courtes que les suffixes rÃ©els des mots dont elles sont extraites. 
Elles nâ€™ont pas de signification grammaticale. Selon Mikheev, les terminaisons 
permettent dâ€™obtenir des performances de lâ€™ordre de 91,9% en prÃ©cision et 78,2% en 
rappel.  
Le guesser dÃ©crit ici ne fait appel quâ€™aux terminaisons mais nous comptons utiliser dâ€™autres 
informations, les suffixes pouvant facilement Ãªtre extraits de nos paradigmes.   
4.2 Description du guesser utilisÃ© 
Habituellement, les guessers sont utilisÃ©s pour calculer le POS le plus probable pour un mot 
inconnu, c'est-Ã -dire ğ‘ƒ ğ‘¡ ğ‘¤  oÃ¹ ğ‘¡ reprÃ©sente une Ã©tiquette morphosyntaxique et  ğ‘¤ le mot 
inconnu. Ici, nous devons Ã©valuer la probabilitÃ© ğ‘ƒ ğ‘™, ğ‘… ğ‘¤  oÃ¹ ğ‘™ğ‘— reprÃ©sente un lemme et ğ‘… un 
paradigme.  
En fait, Ã©tant donnÃ© que les paradigmes de flexion donnent toutes les formes qui sont 
associÃ©es Ã  un lemme, il suffit de calculer la probabilitÃ© ğ‘ƒ ğ‘¡, ğ‘… ğ‘¤ . Le fait de connaÃ®tre le 
POS et la rÃ¨gle permet de retrouver trÃ¨s facilement le lemme tout en conservant lâ€™information 
sur le mot analysÃ©. Une fois le paradigme et lâ€™Ã©tiquette trouvÃ©s, le lemme peut Ãªtre dÃ©duit sans 
risque dâ€™erreur.  
Les statistiques sont apprises sur le lexique existant en utilisant un arbre classique sur les 
terminaisons. Les terminaisons considÃ©rÃ©es sont de longueur 1 Ã  5. Pour chaque mot inconnu, 
5 terminaisons sont utilisÃ©es, reprÃ©sentÃ©es par les n (1 â‰¤ ğ‘› â‰¤ 5) derniÃ¨res lettres du mot 
inconnu. Pour chaque terminaison, un ou plusieurs lemmes candidats sont associÃ©s avec une 
frÃ©quence calculÃ©e sur le lexique. Une terminaison peut Ãªtre associÃ©e Ã  plus dâ€™un couple 
(lemme, paradigme).  
Lâ€™approximation suivante est effectuÃ©e :  
5
ğ‘ƒ ğ‘¡, ğ‘… ğ‘¤ â‰ˆ  ğœŒğ‘– âˆ— ğ‘ƒ ğ‘¡, ğ‘… [ğ‘¤]ğ‘–  
ğ‘–=1
oÃ¹ [ğ‘¤]ğ‘– reprÃ©sente la terminaison de longueur i du mot w.  
et ğœŒğ‘– est un facteur de lissage. Elle est basÃ©e sur lâ€™entropie selon la formule suivante :  
Association automatique de lemmes et de paradigmes de flexion Ã  un mot inconnu 
1 âˆ’ ğ»ğ‘–
ğœŒğ‘– =   
 5ğ‘—=1 1 âˆ’ ğ»ğ‘—  
oÃ¹ ğ»ğ‘–  reprÃ©sente lâ€™entropie des terminaisons de longueur j au sein du lexique (vis-Ã -vis de leur 
association Ã  un couple  ğ‘¡, ğ‘… ). On a  
 ğ‘›âˆˆğ’© â„ ğ‘› 
ğ» ğ‘–ğ‘– =  ğ¶ğ‘ğ‘Ÿğ‘‘ ğ’©ğ‘– 
â„ ğ‘› =  âˆ’  ğ‘ƒ ğ‘¡, ğ‘… âˆ— log   ğ‘ƒ ğ‘¡, ğ‘…   
 ğ‘¡ ,ğ‘… âˆˆ ğ’¯ğ‘›
OÃ¹ ğ’©ğ‘–  reprÃ©sente lâ€™ensemble des nÅ“uds Ã  la profondeur ğ‘– (chaque nÅ“ud contient une seule 
terminaison [ğ‘¤]ğ‘–) et ğ’¯ğ‘›  est lâ€™ensemble des paires  ğ‘¡, ğ‘…  possibles au nÅ“ud ğ‘›.  
Lorsquâ€™un mot inconnu est rencontrÃ©, toutes les probabilitÃ©s ğ‘ƒ ğ‘¡, ğ‘… ğ‘¤  sont calculÃ©es pour 
tout couple  ğ‘¡, ğ‘…  Ã©tant associÃ© Ã  lâ€™une des terminaison [ğ‘¤]ğ‘–  du mot ğ‘¤. Seuls les couples dont 
le score est supÃ©rieur Ã  un certain seuil ğœƒ sont conservÃ©s.  
En dernier lieu, une simple vÃ©rification de cohÃ©rence est effectuÃ©e. Le calcul des probabilitÃ©s 
est effectuÃ© sur les terminaisons mais la mise en relation avec un paradigme ğ‘… permet 
dâ€™accÃ©der au suffixe de la forme associÃ©e au tag ğ‘¡ correspondant. Il suffit alors de vÃ©rifier que 
le suffixe du paradigme est conforme au mot inconnu. Si ce nâ€™est pas le cas, le couple  ğ‘¡, ğ‘…  
est Ã©liminÃ© des possibles.  
Une fois un couple  ğ‘¡, ğ‘…  il est alors immÃ©diat de rÃ©cupÃ©rer un couple  ğ‘™, ğ‘…  donnant le lemme 
du mot inconnu. Ce couple est prÃ©sentÃ© aux linguistes avec lâ€™ensemble des flexions possibles.  
Comme prÃ©cisÃ© plus haut, il est tout Ã  fait possible (et nous le ferons) dans cette derniÃ¨re 
phase, de vÃ©rifier que les formes ainsi gÃ©nÃ©rÃ©es existent dans un corpus ou sur le web.  
5 Ã‰valuation 
Seules les classes ouvertes (adjectif, adverbe, non, verbe) ont Ã©tÃ© traitÃ©es. Lâ€™Ã©valuation a Ã©tÃ© 
effectuÃ©e en prenant alÃ©atoirement 90% du lexique pour lâ€™entraÃ®nement et 10% pour les tests. 
10 permutations ont Ã©tÃ© effectuÃ©es afin dâ€™Ã©viter des problÃ¨mes locaux spÃ©cifiques. Les 
performances sont indiquÃ©es dans le tableau suivant. Les chiffres reprÃ©sentent les scores 
moyens sur les 10 passes et la premiÃ¨re colonne indique le seuil ğœƒ utilisÃ© pour la sÃ©lection des 
solutions.  
Nombre de couples  
ğœ½ PrÃ©cision Rappel 
(lemme, paradigme) proposÃ©s 
0 14.3% 91.5% 14.8 
0.025 51.8% 90.5% 2.7 
0.05 68.4% 84.5% 1.8 
0.075 75.6% 73.7% 1.3 
0.1 80.2% 66.9% 1 
0.15 85.8% 52.6% 0.7 
0.2 88.9% 40.2% 0.5 
Table 1 : Performances du guesser 
Claude de Loupy, MichaÃ«l Bagur, Helena Blancafort 
Ces rÃ©sultats sont du mÃªme ordre que ceux trouvÃ©s par Oliver & TadiÃ§ (2004) puisque pour 
une prÃ©cision de 84,5, ils obtiennent un rappel de 38,4. Il est impossible de pousser la 
comparaison plus loin car les langues de travail sont diffÃ©rentes mais nos rÃ©sultats semblent 
donc cohÃ©rents.  
Ã‰tant donnÃ© que notre but est dâ€™aider les linguistes dans leur tÃ¢che, il nâ€™est pas envisageable 
de proposer prÃ¨s de 15 couples Ã  valider (tous les couples possibles sont fournis avec ğœƒ = 0). 
De plus, plusieurs mÃ©thodes prÃ©sentÃ©es dans lâ€™Ã©tat de lâ€™art vÃ©rifient lâ€™existence dâ€™une forme 
flÃ©chie gÃ©nÃ©rÃ©e au sein dâ€™un corpus ou sur Internet. Si 15 paradigmes de flexion sont associÃ©s 
en moyenne Ã  un mot inconnu, sachant quâ€™il y a en moyenne 25 flexions par paradigmes, cela 
fait 375 requÃªtes par mot inconnu en moyenne. Le temps nÃ©cessaire devient alors beaucoup 
trop important, si lâ€™on veut gÃ©nÃ©rer des propositions rapidement. En particulier, pour des 
recherches sur Internet, 1000 mots inconnus demandent 375 000 requÃªtes, ce qui ne se fait pas 
en une nuit et risque de poser des soucis avec le moteur utilisÃ©.  
Nous pouvons constater que lâ€™utilisation dâ€™un seuil trÃ¨s faible permet Ã  la fois de diminuer 
considÃ©rablement le nombre de propositions (plus de 5 fois moins), dâ€™augmenter de maniÃ¨re 
trÃ¨s nette la prÃ©cision (3,6 fois plus Ã©levÃ©e) tout en ne diminuant quasiment pas le rappel (1 
point de perte).  
Le calcul dâ€™une probabilitÃ© ğ‘ƒ ğ‘™, ğ‘… ğ‘¤  permet donc dâ€™amÃ©liorer de maniÃ¨re trÃ¨s nette la vitesse 
de codage, que ce soit par la prÃ©sentation de moins de possibilitÃ©s aux linguistes ou par la 
diminution des vÃ©rifications Ã  effectuer. Par ailleurs, cette mÃ©thode permet dâ€™obtenir des 
entrÃ©e de lexique qui sont complÃ¨tes puisque gÃ©nÃ©rÃ©es Ã  partir de paradigmes de flexion 
validÃ©s au prÃ©alable.  
6 Conclusion et perspectives 
Le systÃ¨me dÃ©crit dans cet article permet dâ€™associer Ã  un mot inconnu des couples (lemme, 
paradigme de flexion). Les performances ne sont pas trÃ¨s Ã©levÃ©es si on les compare Ã  celles 
quâ€™obtiennent les guessers sur les Ã©tiquettes morphosyntaxiques. NÃ©anmoins, lâ€™utilisation de 
cette mÃ©thode permet de proposer peu de choix Ã  la validation humaine tout en gardant une 
trÃ¨s bonne couverture. Cela permet Ã©galement de diminuer considÃ©rablement le nombre de 
formes Ã  tester, que ce soit sur un corpus local ou sur le web.  
Par ailleurs, de nombreuses pistes dâ€™amÃ©lioration sont Ã  envisager. DÃ©jÃ , la vÃ©rification de 
lâ€™existence des formes flÃ©chies gÃ©nÃ©rÃ©es par les couples  ğ‘™, ğ‘… , telle que plusieurs travaux la 
pratiquent, devrait apporter une nette amÃ©lioration. De plus, les indices que reprÃ©sentent les 
prÃ©fixes, les suffixes grammaticaux et le contexte dans lesquels apparaissent les mots 
inconnus devraient Ã©galement donner des rÃ©sultats encore plus intÃ©ressants.  
Enfin, les expÃ©riences prÃ©sentÃ©es ici ont Ã©tÃ© menÃ©es en utilisant lâ€™ensemble des rÃ¨gles de 
flexion. Or, certains paradigmes non productifs (verbe Ãªtre par exemple) polluent les 
probabilitÃ©s ci-dessus. Il convient donc de refaire ces expÃ©riences en Ã©valuant la productivitÃ© 
des paradigmes utilisÃ©s. Dans le mÃªme ordre dâ€™idÃ©e, la frÃ©quence des paradigmes dans le 
lexique dâ€™apprentissage nâ€™a pas Ã©tÃ© utilisÃ©e. Or, par analogie avec dâ€™autres phÃ©nomÃ¨nes 
linguistiques, nous pouvons supposer que le simple fait dâ€™associer aux inconnus les rÃ¨gles 
selon leur frÃ©quence devrait permettre dâ€™augmenter encore les performances.  
Le traitement des formes polylexicales demandera une attention particuliÃ¨re. Cependant, le 
codage adoptÃ© pour ces formes dans notre lexique (rÃ¨gle de flexion Ã  appliquer aux 
Association automatique de lemmes et de paradigmes de flexion Ã  un mot inconnu 
composants si câ€™est le cas + connaissance de la tÃªte du mot composÃ© sâ€™il y en a une) nous 
permet de faire un apprentissage assez simple en reprenant la mÃ©thode prÃ©sentÃ©e ici. Nous 
testerons les rÃ©sultats obtenus ainsi.  
Lors de prochaines expÃ©riences, nous Ã©valuerons les lexiques produits en termes de 
couverture et le temps que de tels outils peuvent faire gagner lors du codage dâ€™informations 
morphosyntaxiques par des linguistes. Bien que cette Ã©valuation puisse Ãªtre biaisÃ©e par 
lâ€™interface qui sera utilisÃ©e, les rÃ©sultats en sont importants pour dÃ©terminer sâ€™il est possible de 
crÃ©er des lexiques fiables et complets de maniÃ¨re rapide en conservant une validation 
manuelle dans la boucle.  
Enfin, nous comptons Ã©galement effectuer des comparaisons de performances de langue Ã  
langue dans le mÃªme esprit que de prÃ©cÃ©dentes expÃ©riences de comparaison (Blancafort & 
Loupy, 2008).  
RÃ©fÃ©rences 
BLANCAFORT H., LOUPY C. DE, (2008). Comparing languages from vocabulary growth to 
inflection paradigms â€“ A study run on parallel corpora and multilingual lexicons. Actes de 
SEPLN'2008. Madrid, Espagne. 
CHANOD J.P., TAPANAINEN P., (1995). Creating a Tagset, Lexicon and Guesser for a French 
Tagger; Proceedings of the European Chapter of the ACL SIGDAT Workshop From text to 
tags : Issues in Multilingual Language Analysis, pp. 51-57, Dublin, Irelande.  
CLÃ‰MENT L., SAGOT B., LANG B., (2004). Morphology based automatic acquisition of large-
coverage lexica. In Proceedings of LREC'04, Lisbonne, Portugal. pp. 1841-1844. 
CUCERZAN S., YAROWSKY D., (2000). Language Independent, Minimally Supervised 
Induction of Lexical Probabilities. Proceedings of ACL-2000, Hong Kong, pp. 270-277. 
DAL G., NAMER F., (2000). GÃ©DÃ©riF: automatic generation and analysis of morphologically 
constructed lexical resources. In actes de Second International Conference on Language 
Resources and Evaluation, Athens, GrÃ¨ce, pp. 1447-1454. 
DEMPSTER A.P., LAIRD N. M., RUBIN D. B., (1977). Maximum likelihood from incomplete 
data via the EM algorithm. Journal of the Royal Statistical Society, B 39(1):1-38. 
GALVEZ, C., (2006). El diccionario electrÃ³nico: un instrumento para la unificaciÃ³n de 
tÃ©rminos en la indizaciÃ³n automÃ¡tica. Linguax: Revista de Lenguas Aplicadas (ISSN 1695-
632X). 
HATHOUT N., TANGUY L., (2005). Webaffix : une boÃ®te Ã  outils d'acquisition lexicale Ã  partir 
du Web. In Revue QuÃ©bÃ©quoise de Linguistique. Volume 32, numÃ©ro 1. 
HATHOUT N., (2005). Exploiter la structure analogique du lexique construit : une approche 
computationnelle. In Cahiers de Lexicologie. Volume 87, numÃ©ro 2. 
IDE N., VÃ‰RONIS J., (1994). MULTEXT: Multilingual Text Tools and Corpora. Proceedings of 
the 15th International Conference on Computational Linguistics, COLING'94, Kyoto, Japon, 
588-92. 
Claude de Loupy, MichaÃ«l Bagur, Helena Blancafort 
LOUPY C. DE, GONÃ‡ALVES S., (2008). Aide Ã  la construction de lexiques morphosyntaxiques. 
Actes de EURALEX 2008. Barcelone, Espagne. 
MIKHEEV A., (1997). Automatic Rule Induction for Unknown-Word Guessing. In 
Computational Linguistics vol 23(3), ACL 1997. pp. 405-423. 
NAKOV P., BONEV Y., ANGELOVA G., GIUS E., VON HAHN W., (2003). Guessing 
Morphological Classes of Unknown German Nouns. In Proceedings of Recent Advances in 
Natural Language Processing (RANLPâ€™03). pp. 319-326. Borovetz, Bulgarie. 
NAMER F., (1999). Le traitement automatique des mots dÃ©rivÃ©s : le cas des noms et adjectifs 
en -et(te). in D. Corbin, G. Dal, B. Fradin, B. Habert., F. Kerleroux, M. PlÃ©nat & M. RochÃ© 
Ã©ds, La morphologie des dÃ©rivÃ©s Ã©valuatifs, Silexicales 2, pp. 169-179. Villeneuve dâ€™Ascq.  
OLIVER A., TADIÄ† M., (2004). Enlarging the Croatian Morphological Lexicon by Automatic 
th
Lexical Acquisition from Raw Corpora. In: Proceedings of the 4  International Conference of 
Language Resources and Evaluation (LREC 2004), p. 1259â€“1262. Lisbonne, Portugal. 
OLIVER A., CASTELLÃ“N I., MÃ€RQUEZ L., (2003). Use of internet for augmenting coverage in a 
lexical acquisition system from raw corpora. In: Proceedings of the RANLP 2003 
International Workshop on Information Extraction for Slavonic and Other Central and Eastern 
European Languages (IESL 2003). Borovets, Bulgarie. 
PORTER M., (1980). An algorithm for suffix stripping. in Program, nÂ°14, 130-137. 
RISSANEN J., (1989). Stochastic Complexity in Statistical Inquiry. World Scientific Publishing 
Co, Singapour. 
SAGOT B., (2005). Automatic acquisition of a Slovak Lexicon from a Raw Corpus. In Lecture 
Notes in Artificial Intelligence 3658 (Springer-Verlag), Proceedings of TSD'05, Karlovy 
Vary, RÃ©publique TchÃ¨que. pp. 156-163.  
SAGOT B., (2007). Building a morphosyntactic lexicon and a pre-syntactic processing chain 
for Polish. In: Proceedings of LTC 2007, PoznaÅ„, Pologne. 
SCHMID H., (1994). Probabilistic Part-of-Speech Tagging Using Decision Trees. Proceedings 
of International Conference on New Methods in Language Processing. September 1994. 
SCHMID H., (1995). Improvements in part-of-speech tagging with an application to German. 
in Proceedings of the ACL SIGDAT-Workshop, pp. 47-50. 
VÃ‰RONIS J., KHOURI L., (1995). Ã‰tiquetage grammatical multilingue: le projet Multext. 
Traitement Automatique des Langues, 36(1/2), 233-248. 
 
