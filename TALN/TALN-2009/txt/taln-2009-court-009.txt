TALN 2009— Session posters, Senlis, 24-26 juin 2009

Association automatique de lemmes et de paradigmes de ﬂexion a
un mot inconnu

Claude de Loupy (1,2), Michael Bagur (1), Helena Blancafort (1,3)

(1) Syllabs — 15, rue Jean Baptiste Berlier, 75013 Paris
blancafort@syllabs.com; bagur@syllabs.com; loupy@syllabs.com
(2) MoDyCo — Université Paris 10, 200 AV. de la République, Nanterre
(3) Universitat Pompeu Fabra, Roc Boronat 138, 08018 Barcelona

Résumé

La maintenance et l’enrichissement des lexiques morphosyntaxiques sont souvent des taches
fastidieuses. Dans cet article nous présentons la mise en place d’une procédure de guessing de
ﬂexion afm d’aider les linguistes dans leur travail de lexicographes. Le guesser développé ne
fait pas qu’éValuer l’étiquette morphosyntaxique comme c’est généralement le cas. Il propose
pour un mot francais inconnu, un ou plusieurs candidats-lemmes, ainsi que les paradigmes de
ﬂexion associés (formes ﬂéchies et étiquettes morphosyntaxiques). Dans cet article, nous
décrivons le modele probabiliste utilisé ainsi que les résultats obtenus. La méthode utilisée
permet de réduire considérablement le nombre de regles a Valider, permettant ainsi un gain de
temps important.

Abstract

Lexicon maintenance and lexicon enrichment is a labour-intensive task. In this paper, we
present preliminary work on an inﬂectional guessing procedure for helping the linguist in
lexicographic tasks. The guesser presented here does not only output morphosyntactic tags,
but also suggests for an unknown French word one or more lemma candidates as well as their
corresponding inﬂectional rules and morphosyntactic tags that the linguist has to Validate. In
this article, we present the probabilistic model we used as well as obtained results. The
method allows a drastic reduction of the number of rules to Validate.

Mots-clés 2 guesser, lexiques morphosyntaxiques, aide aux linguistes, induction des
regles de ﬂexion

Keywords: guesser, morphosyntactic lexica, aide to the linguist, induction of inﬂection
rules

Claude de Loupy, 1\4ichae'lBagur, Helena Blancafort

1 Introduction

Les ressources lexicales sont fondamentalement importantes en Traitement Automatique des
Langues (TAL). Les lexiques morphosyntaxiques en particulier sont tres utilisés par des
traitements de relativement bas niveau qui sont donc la base de traitements ultérieursl. Or, la
gestion et la maintenance de tels lexiques est tres consommatrice en temps et en énergie. Il
n’est pas rare qu’un organisme travaillant en TAL ait a gérer plusieurs lexiques de ce type,
que ce soit pour une méme langue avec des spécialisations de ressources ou pour le traitement
de plusieurs langues. Du fait de la créativité des langues, les lexiques doivent étre en
permanence mis a jour et il est important de penser des méthodes permettant de faciliter ce
travail.

L’un des moyens les plus courants pour enrichir ces lexiques consiste a analyser des corpus, a
récupérer les formes inconnues du lexique a enrichir et a presenter les plus courantes a un/e
linguiste qui se charge alors d’introduire non seulement la forme en question mais également
ses étiquettes morphosyntaxiques, son ou ses lemmes ainsi que toutes les formes ﬂéchies du
ou des lemmes en question. Cette tache est généralement effectuée a la main ou en utilisant
des outils spéciﬁques (ﬂéchisseur) permettant de ﬂéchir un lemme selon des paradigmes de
ﬂexion connus. Méme dans ce dernier cas. L’ensemble de la tache prend beaucoup de temps
car il est nécessaire de décider quel est le lemme correspondant a une forme ainsi que le
numéro ou la dénomination de la regle de ﬂexion qui doit lui étre associée.

L’objet de cet article est d’éValuer les performances de méthodes classiques utilisées dans les
guessers de catégories grammaticales lorsqu’on les applique a l’association de couples
(lemme, paradigme de ﬂexion) a un mot inconnu. Un état de l’art est donné en section 2. Les
fondements et les buts de l’expérience décrite sont ensuite exposés en section 3 aﬁn
d’expliquer pourquoi les méthodes précédentes ne sont pas satisfaisantes pour les conditions
visées. La section 4 présente la méthode utilisée. Enﬁn, la section 5 présente l’éValuation des
résultats.

2 Etat de l’art

L’idée de foumir une aide supplémentaire aux linguistes dans cette étape d’enrichissement
court depuis déja un certain temps. Les premiers travaux en construction automatique de
lexiques morphosyntaxiques n’utilisaient qu’un corpus comme source d’inforrnation. Ainsi,
Jacquemin (1997) compare les terminaisons des mots de son corpus de maniere a grouper des
mots ayant un méme stem puis regroupe les groupes de mots ayant la méme suite de
terminaisons. Goldsmith (1997 ; 2000) effectue également ce type de procédure en appliquant
les méthodes de [Minimum Description Length (Rissanen, 1989) et d’Expectati0n
Maximization (Dempster et al., l977)2. Schone & Jurasky (2000 ; 2001) utilisent la méthode
du Latent Semantic Analysis (Deerwester et al., 1990).

1 Nous n’entrerons pas ici dans un débat sur l’utilité on non de ces lexiques en comparaison a d’autres moyens

d’analyse plus sommaire comme les stemmers de type Porter (1980). Nous pensons que ces lexiques sont
utiles et de nombreux autres travaux les utilisent, ce qui sufﬁt a justifier un travail sur leur création.

2 L’outil Linguistica résultant de ces travaux est téléchargeable a l’adresse suivante:
http://linguistica.uchicago.edu/.

Association automatique de lemmes et de paradigmes de ﬂexion a un mot inconnu

Dans tous les cas, les systemes proposes produisent des listes de stems associes a des
paradigmes de ﬂexion permettant de generer les formes ﬂechies trouvees dans les corpus
analyses. Le principal probleme de ces methodes est que les resultats generes sont tres
difﬁcilement utilisables par des linguistes car ils n’ont que peu de liens avec les regles
morphologiques auxquelles on s’attend. Ces travaux sont donc principalement utilisables dans
un contexte totalement automatique avec toutes les erreurs que cela peut comporter.

D’autres experiences utilisent des ressources lexicales existantes ou des paradigmes de
ﬂexion deja connus en plus des corpus a analyser. Nakov et al. (2003), travaillent sur
l’allemand et commencent par extraire, de maniere automatique, toutes les terminaisons
possibles pour les mots presents dans un lexique. Ils extraient ensuite les mots presents dans
un corpus qui sont inconnus dudit lexique, puis generent tous les stems possibles a partir des
terminaisons pour chacun de ces mots. Ils utilisent enﬁn l’algorithme de Maximum Likelihood
Estimation avec des parametres preconises par Mikheev (1997) afin de recuperer les regles de
ﬂexion les plus interessantes (notions de qualite, de longueur et de frequences).

Oliver & Tadic (2004) presentent l’application sur le Croate de methodes testees auparavant
sur le russe (Oliver et al., 2003). 11s s’appuient sur un lexique morphosyntaxique existant dont
ils extraient des paradigmes de ﬂexion et sur un corpus a partir duquel ils completent le
lexique precedent. Le processus est decoupe en 4 etapes : 1. Decoupage des formes en un
couple (stem, ending) a partir de toutes les terminaisons possibles puis regroupement de ces
couples dans des paradigmes possibles issus du lexique existant. 2. Selection pour chaque mot
du paradigme permettant d’obtenir le plus de formes presentes dans le corpus. 3.
Recuperation des cas non decidables (meme nombre d’entrees entre deux paradigmes
possibles pour un mot). 4. Utilisation d’Intemet pour resoudre les cas precedents (recherche
de la presence sur Internet des differentes formes ﬂechies possibles a partir d’un mot et d’un
paradigme) : les paradigmes ayant les formes les plus presentes sont Valides.

Clement (2004) travaille sur l’extraction d’un lexique morphologique francais a partir d’un
corpus en s’appuyant sur des couples (lemme, forme) et en associant a un lemme une
probabilite d’autant plus forte que beaucoup de formes peuvent lui étre associees3. Ces
travaux ont egalement ete effectues sur le slovaque (Sagot, 2005) et le polonais (Sagot, 2007).
Cette methode, aussi puissante soit-elle et bien que permettant de produire rapidement des
lexiques tres Volumineux, presente l’inconvenient de generer des entrees incompletes. Ainsi,
le Lefff contient des Verbes dont toutes les formes ne sont pas presentes mais seulement celles
qui ont ete reperees dans le corpus. Il s’agit donc d’une methode permettant de creer un
lexique dedie a un corpus et non un lexique de langue.

Zanchetta & Baroni (2005) se base egalement sur un corpus pour extraire un lexique
morphosyntaxique de l’italien4. Pour cela, ils commencent par utiliser TreeTagger (Schmid,
1994) afin d’obtenir la categorie grammaticale et le lemme d’une forme donnee. Les lemmes
ainsi obtenus ont ensuite ete ﬂechis a l’aide de regles de ﬂexions de l’italien. Nous noterons
dans ce cas que le lexique genere contient l’ensemble des ﬂexions d’un lemme donne. En
revanche, la methode d’application des regles est tres manuelle puisque les auteurs ont

Ces travaux ont permis la creation d’un lexique morphosyntaxique du francais, le Lefff, disponible
gratuitement a l’adresse suivante : http://www.labri.fr/perso/clement/lefff.

4 Ces travaux ont permis la production du lexique Morph-it!, disponible a l’adresse
http://dev.ssln1it.unibo.it/linguistics/morph-it.php.

Claude de Loupy, 1\4ichae'lBagur, Helena Blancafort

appliqué des regles d’analyse des formes a ﬂéchir avec de nombreuses corrections manuelles
de maniere a obtenir un résultat satisfaisant.

Par ailleurs, plusieurs expériences ont été effectuées dans un contexte lié plut6t a la
morphologie dérivationnelle qui nous intéresse moins ici comme par exemple Dal & Namer
(2000), Namer (1999), Hathout (2005), Hathout & Tanguy (2005).

3 Fondements et buts de l’expérience

Les travaux présentés ici se place dans un contexte plus large de construction de chaine de
traitement permettant d’aider les linguistes dans leur tache de codage de ressources
linguistiques (Loupy & Goncalves, 2008). L’un des points de cette chaine conceme
l’inclusion de mots inconnus dans un lexique morphosyntaxique afin d’en augmenter sa
couverture, soit sur un corpus spécialisé, soit sur la langue générale. Ces ajouts doivent étre
proposés aux linguistes selon un format correspondant a celui utilisé pour coder le lexique.

Habituellement, les lexiques morphosyntaxiques sont constitués de triplet (forme, lemme,
catégorie). La ﬁgure suivante montre un exemple typique issu du lexique francais MulText
(Ide and Véronis, 1994).

abaisse abaisser Vmip3s—
abaissons abaisser Vmip1p—
brioche brioche Ncfs——
brioches brioche Ncfp——
rends rendre Vmip1s—
rends rendre Vmip2s—
rend rendre Vmip3s—
rendons rendre Vmip1p—
statisticienne statisticien Ncfs——
statisticiennes statisticien Ncfp-

Figure I .' Structure habituelle d ’un lexique morphosyntaxique

Ce type de format est difficile a manipuler et rend les lexiques difficiles a maintenir, a
enrichir et a contr6ler pour éviter des erreurs. Chaque forme ﬂéchie est elle-méme une entrée
du lexique et est liée a son lemme et a l’ensemble de ses étiquettes. Pour des langues a ﬂexion
riche, cela conduit a un tres grand nombre d’entrées pouvant aller jusqu’a plusieurs millions
dans certaines langues comme le russe.

Afin d’avoir un meilleur controle de nos ressources lexicales, notre lexique SylLex est bati
sur une structure de type (lemme, paradigme) dans lequel les paradigmes décrivent
l’ensemble des opérations de ﬂexion a associer au lemme afin de générer ses ﬂexions. Un
format similaire est utilisé pour coder le DELAS (Galvez, 2003). La figure suivante donne un
exemple de ce format.

abaisser V1
brioche N1
rendre V9
statisticien N13

Figure 2 .' Structure de SylLex

Association automatique de lemmes et de paradigmes de ﬂexion a un mot inconnu

Les regles sont definies de facon a decrire les operations de construction de ﬂexions a partir
‘ . . , . 5
des regles comme 1nd1que dans la ﬁgure su1vante .

V1 O/a/Vmif3s——l O/ont/Vmif3p—— I O/ai/Vmifls—— I O/ons/Vm_"Lf1p——| O/as/Vmif2s——
IO/ez/Vmif2p——I2/ait/Vmii3s——| . . .

Figure 3 .' Description des paradigmes

Les paradigmes et le lexique peuvent étre accedes via une interface rendant plus simple la
manipulation des inforrnations. Cette architecture de lexique est beaucoup plus facile a
manipuler et a apprehender pour les linguistes.

C’est donc dans ce contexte qu’il a ete decide de construire une chaine de traitement des mots
inconnus. Les methodes presentees plus haut presentent toutes des inconvenients par rapport a
cela. Celles qui n’utilisent que les corpus ne peuvent permettre de proposer des suggestions
propres par rapport a un format faisant reference a des paradigmes de ﬂexions etablis. Les
methodes qui se basent sur une association (forme, lemme) ne perrnettent pas non plus de se
projeter de maniere ﬁable sur ces paradigmes. L’extraction automatique de paradigmes a
partir d’un lexique de type (forme, lemme, tag) presente d’ailleurs touj ours des erreurs ou des
complexites inutiles du fait de la presence de mots ambigus. Ainsi, le verbe payer, du fait de
ses variantes (paye/paie) generera un paradigme different du paradigme canonique du verbe
aimer alors que la prise en compte de variantes et donc d’un stem supplementaire permet
d’eliminer ce probleme.

Seule la methode de Zanchetta & Baroni pourrait convenir mais elle est beaucoup trop
manuelle et demanderait un travail tres important de constitution d’un ﬂechisseur automatique
pour chaque nouvelle langue.

Nous avons donc mis en place une methode permettant d’associer directement un couple
(lemme, paradigme) a un mot inconnu. Cette methode peut ensuite étre couplee avec les
processus presentes dans les autres travaux afin de donner plus de poids a des couples dont les
formes generes sont presentes dans un corpus ou sur Internet.

Le lexique francais sur lequel ont ete faites les experiences presentees dans cet article

contenait 60 000 couples (lemme, paradigme), il s’agit donc d’un petit lexique qui demande
justement un important travail pour en augmenter la taille.

4 Méthode utilisée

Le travail presente ici est base sur des methodes classiquement utilisees dans les guessers et
adapte a la generation de couples (lemme, paradigme).

4.1 Les guessers

11 y a peu de publications dediees aux guessers. La plupart du temps, les procedures de
guessing sont decrites a l’interieur de descriptions de taggers des que l’on parle de mots

On pourra noter que l’encodage des etiquettes est base sur les speciﬁcations de MulText (Ide & Veronis,
1994). Malgre quelques changements, ce format nous a paru le plus pratique et il a surtout l’avantage d’avoir
ete teste sur au mois 20 langues (Veronis & Khouri, 1995).

Claude de Loupy, 1\4ichae'lBagur, Helena Blancafort

inconnus (Chanod & Tapanainen, 1995 ; Schmid, 1995). Mikheev (1997) considere que les
guessers de catégorie (Part Of Speech ou POS) peuvent utiliser trois indices :

1. Les préfixes. Si impossible est un mot inconnu mais que possible est connu dans le
lexique, il est probable que le POS du mot impossible soit le méme que celui du mot
possible. La précision de cet indice est élevée mais la couverture est en revanche
tres faible (respectivement 935% et 65% selon Mikheev).

2. Les suffixes. Dans beaucoup de langues, les suffixes indiquent des propriétés
grammaticales (pluriel, temps, etc.). La précision de cet indice est encore plus
élevée (96,8%) mais la couverture reste limitée (26,5%).

3. Les terminaisons (endings). Les terminaisons sont les demieres lettres des mots. Il
ne s’agit pas de suffixes (ou alors elles le sont par hasard) car elles peuvent étre plus
longues ou plus courtes que les suffixes réels des mots dont elles sont extraites.
Elles n’ont pas de signiﬁcation grammaticale. Selon Mikheev, les terminaisons
permettent d’obtenir des performances de l’ordre de 919% en précision et 78,2% en
rappel.

Le guesser décrit ici ne fait appel qu’aux terminaisons mais nous comptons utiliser d’autres
informations, les suffixes pouvant facilement étre extraits de nos paradigmes.

4.2 Description du guesser utilisé

Habituellement, les guessers sont utilisés pour calculer le POS le plus probable pour un mot
inconnu, c'est-a-dire P(t|w) ou t représente une étiquette morphosyntaxique et w le mot
inconnu. Ici, nous devons évaluer la probabilité P(l, Rlw) ou lj représente un lemme et R un
paradigme.

En fait, étant donné que les paradigmes de ﬂexion donnent toutes les formes qui sont
associées a un lemme, il suffit de calculer la probabilité P(t,R|w). Le fait de connaitre le
POS et la regle permet de retrouver tres facilement le lemme tout en conservant l’information
sur le mot analysé. Une fois le paradigme et l’étiquette trouvés, le lemme peut étre déduit sans
risque d’erreur.

Les statistiques sont apprises sur le lexique existant en utilisant un arbre classique sur les
terminaisons. Les terminaisons considérées sont de longueur 1 a 5. Pour chaque mot inconnu,
5 terminaisons sont utilisées, représentées par les n (1 S n S 5) dernieres lettres du mot
inconnu. Pour chaque terminaison, un ou plusieurs lemmes candidats sont associés avec une
fréquence calculée sur le lexique. Une terminaison peut étre associée a plus d’un couple
(lemme, paradigme).

L’approximation suivante est effectuée :

5
P(t.R|w) ~ Zpi =« P(t.R|[w]i)
i=1

ou [w]i représente la terminaison de longueur i du mot w.

et pi est un facteur de lissage. Elle est basée sur l’entropie selon la forrnule suivante :

Association automatique de lemmes et de paradigmes de ﬂexion a un mot inconnu

1-H,-

p. :  
' Z?=1(1 - 171-)
ou Hi représente l’entropie des terminaisons de longueur j au sein du lexique (Vis-a-Vis de leur
association a un couple (t, R)). On a

_ Em. hm)
i_ Card(]\f,-)

h(n)= — Z P(t,R)*1og(P(t,R))
<t.R>ea;.

on M représente l’ensemble des noeuds a la profondeur i (chaque noeud contient une seule
terrninaison [W],-) et .7}, est l’ensemble des paires (t, R) possibles au noeud n.

Lorsqu’un mot inconnu est rencontré, toutes les probabilités P(t,R|w) sont calculées pour
tout couple (t, R) étant associé a l’une des terrninaison [w],- du mot W. Seuls les couples dont
le score est supérieur a un certain seuil 6 sont conservés.

En dernier lieu, une simple verification de cohérence est effectuée. Le calcul des probabilités
est effectué sur les terminaisons mais la mise en relation avec un paradigme R permet
d’accéder au suffixe de la forme associée au tag t correspondant. Il sufﬁt alors de Veriﬁer que
le sufﬁxe du paradigme est conforme au mot inconnu. Si ce n’est pas le cas, le couple (t, R)
est éliminé des possibles.

Une fois un couple (t, R) il est alors immédiat de récupérer un couple (l, R) donnant le lemme
du mot inconnu. Ce couple est présenté aux linguistes avec l’ensemble des ﬂexions possibles.

Comme précisé plus haut, il est tout a fait possible (et nous le ferons) dans cette derniere
phase, de Veriﬁer que les formes ainsi générées existent dans un corpus ou sur le web.

5 Evaluation

Seules les classes ouvertes (adjectif, adverbe, non, Verbe) ont été traitées. L’éValuation a été
effectuée en prenant aléatoirement 90% du lexique pour l’entrainement et 10% pour les tests.
10 permutations ont été effectuées afin d’éViter des problemes locaux spéciﬁques. Les
performances sont indiquées dans le tableau suivant. Les chiffres représentent les scores
moyens sur les 10 passes et la premiere colonne indique le seuil 6 utilisé pour la sélection des
solutions.

, . . Nombre de cou les

9 Precision Rappel (lemme, paradigme) Iproposés
0 14.3% 91.5% 14.8
0.025 51.8% 90.5% 2.7
0.05 68.4% 84.5% 1.8
0.075 75.6% 73.7% 1.3
0.1 80.2% 66.9% 1
0.15 85.8% 52.6% 0.7
0.2 88.9% 40.2% 0.5

Table I .' Performances du guesser

Claude de Loupy, 1\4ichae'lBagur, Helena Blancafort

Ces résultats sont du méme ordre que ceux trouvés par Oliver & Tadic (2004) puisque pour
une précision de 84,5, ils obtiennent un rappel de 38,4. 11 est impossible de pousser la
comparaison plus loin car les langues de travail sont différentes mais nos résultats semblent
donc cohérents.

Etant donné que notre but est d’aider les linguistes dans leur tache, il n’est pas envisageable
de proposer pres de 15 couples a valider (tous les couples possibles sont fournis avec 6 = 0).
De plus, plusieurs méthodes présentées dans l’état de l’art vérifient l’existence d’une forme
ﬂéchie générée au sein d’un corpus ou sur Internet. Si 15 paradigmes de ﬂexion sont associés
en moyenne a un mot inconnu, sachant qu’il y a en moyenne 25 ﬂexions par paradigmes, cela
fait 375 requétes par mot inconnu en moyenne. Le temps nécessaire devient alors beaucoup
trop important, si l’on veut générer des propositions rapidement. En particulier, pour des
recherches sur Internet, 1000 mots inconnus demandent 375 000 requétes, ce qui ne se fait pas
en une nuit et risque de poser des soucis avec le moteur utilisé.

Nous pouvons constater que l’utilisation d’un seuil tres faible permet a la fois de diminuer
considérablement le nombre de propositions (plus de 5 fois moins), d’augmenter de maniere
tres nette la précision (3,6 fois plus élevée) tout en ne diminuant quasiment pas le rappel (1
point de perte).

Le calcul d’une probabilité P(l, R Iw) permet donc d’améliorer de maniere tres nette la vitesse
de codage, que ce soit par la présentation de moins de possibilités aux linguistes ou par la
diminution des vérifications a effectuer. Par ailleurs, cette méthode permet d’obtenir des
entrée de lexique qui sont completes puisque générées a partir de paradigmes de ﬂexion
validés au préalable.

6 Conclusion et perspectives

Le systeme décrit dans cet article permet d’associer a un mot inconnu des couples (lemme,
paradigme de ﬂexion). Les performances ne sont pas tres élevées si on les compare a celles
qu’obtiennent les guessers sur les étiquettes morphosyntaxiques. Néanmoins, l’utilisation de
cette méthode permet de proposer peu de choix a la validation humaine tout en gardant une
tres bonne couverture. Cela permet également de diminuer considérablement le nombre de
formes a tester, que ce soit sur un corpus local ou sur le web.

Par ailleurs, de nombreuses pistes d’amélioration sont a envisager. Déja, la vérification de
l’existence des formes ﬂéchies générées par les couples (l, R), telle que plusieurs travaux la
pratiquent, devrait apporter une nette amélioration. De plus, les indices que représentent les
préfixes, les suffixes grammaticaux et le contexte dans lesquels apparaissent les mots
inconnus devraient également donner des résultats encore plus intéressants.

Enfin, les expériences présentées ici ont été menées en utilisant l’ensemble des regles de
ﬂexion. Or, certains paradigmes non productifs (verbe étre par exemple) polluent les
probabilités ci-dessus. Il convient donc de refaire ces expériences en évaluant la productivité
des paradigmes utilisés. Dans le méme ordre d’idée, la fréquence des paradigmes dans le
lexique d’apprentissage n’a pas été utilisée. Or, par analogie avec d’autres phénomenes
linguistiques, nous pouvons supposer que le simple fait d’associer aux inconnus les regles
selon leur fréquence devrait perrnettre d’augmenter encore les performances.

Le traitement des formes polylexicales demandera une attention particuliere. Cependant, le
codage adopté pour ces formes dans notre lexique (regle de ﬂexion a appliquer aux

Association automatique de lemmes et de paradigmes de ﬂexion a un mot inconnu

composants si c’est le cas + connaissance de la téte du mot composé s’il y en a une) nous
perrnet de faire un apprentissage assez simple en reprenant la méthode présentée ici. Nous
testerons les résultats obtenus ainsi.

Lors de prochaines expériences, nous évaluerons les lexiques produits en terrnes de
couverture et le temps que de tels outils peuvent faire gagner lors du codage d’inforrnations
morphosyntaxiques par des linguistes. Bien que cette évaluation puisse étre biaisée par
l’interface qui sera utilisée, les résultats en sont importants pour déterminer s’il est possible de
créer des lexiques fiables et complets de maniere rapide en conservant une validation
manuelle dans la boucle.

Enfin, nous comptons également effectuer des comparaisons de performances de langue a
langue dans le méme esprit que de précédentes expériences de comparaison (Blancafort &
Loupy, 2008).

Références

BLANCAFORT H., LOUPY C. DE, (2008). Comparing languages from vocabulary growth to
inﬂection paradigms — A study run on parallel corpora and multilingual lexicons. Actes de
SEPLN'2008. Madrid, Espagne.

CHANOD J.P., TAPANAINEN P., (1995). Creating a Tagset, Lexicon and Guesser for a French
Tagger; Proceedings of the European Chapter of the ACL SIGDAT Workshop From text to
tags : Issues in Multilingual Language Analysis, pp. 51-57, Dublin, Irelande.

CLEMENT L., SAGOT B., LANG B., (2004). Morphology based automatic acquisition of large-
coverage lexica. In Proceedings of LREC'O4, Lisbonne, Portugal. pp. 1841-1844.

CUCERZAN S., YAROWSKY D., (2000). Language Independent, Minimally Supervised
Induction of Lexical Probabilities. Proceedings of ACL-2000, Hong Kong, pp. 270-277.

DAL G., NAMER F., (2000). GéDériF: automatic generation and analysis of morphologically
constructed lexical resources. In actes de Second International Conference on Language
Resources and Evaluation, Athens, Gréce, pp. 1447-1454.

DEMPSTER A.P., LAIRD N. M., RUBIN D. B., (1977). Maximum likelihood from incomplete
data via the EM algorithm. Journal of the Royal Statistical Society, B 39(1): 1-38.

GALVEZ, C., (2006). El diccionario electronico: un instrumento para la unificacion de
térrninos en la indizacion automatica. Linguax: Revista de Lenguas Aplicadas (ISSN 1695-
632X).

HATHOUT N., TANGUY L., (2005). Webaffix : une boite a outils d'acquisition lexicale a partir
du Web. In Revue Québéquoise de Linguistique. Volume 32, numéro 1.

HATHOUT N., (2005). Exploiter la structure analogique du lexique construit : une approche
computationnelle. In Cahiers de Lexicologie. Volume 87, numéro 2.

IDE N., VERONIS J., (1994). MULTEXT: Multilingual Text Tools and Corpora. Proceedings of
the 15th International Conference on Computational Linguistics, COLING'94, Kyoto, Japon,
588-92.

Claude de Loupy, 1\4ichae'lBagur, Helena Blancafort

LOUPY C. DE, GONCALVES S., (2008). Aide a la construction de lexiques morphosyntaxiques.
Actes de EURALEX 2008. Barcelone, Espagne.

MIKHEEV A., (1997). Automatic Rule Induction for Unknown-Word Guessing. In
Computational Linguistics vol 23(3), ACL 1997. pp. 405-423.

NAKOV P., BONEV Y., ANGELOVA G., GIUS E., VON HAHN W., (2003). Guessing
Morphological Classes of Unknown German Nouns. In Proceedings of Recent Advances in
Natural Language Processing (RANLP’03). pp. 319-326. Borovetz, Bulgarie.

NAMER F., (1999). Le traitement automatique des mots dérivés : le cas des noms et adjectifs
en -et(te). in D. Corbin, G. Dal, B. Fradin, B. Habert., F. Kerleroux, M. Plénat & M. Roché
éds, La morphologie des dérivés évaluatifs, Silexicales 2, pp. 169-179. Villeneuve d’Ascq.

OLIVER A., TADIC M., (2004). Enlarging the Croatian Morphological Lexicon by Automatic
Lexical Acquisition from Raw Corpora. In: Proceedings of the 4th International Conference of
Language Resources and Evaluation (LREC 2004), p. 1259-1262. Lisbonne, Portugal.

OLIVER A., CASTELLON I., MARQUEZ L., (2003). Use of intemet for augmenting coverage in a
lexical acquisition system from raw corpora. In: Proceedings of the RANLP 2003
International Workshop on Information Extraction for Slavonic and Other Central and Eastern
European Languages (IESL 2003). Borovets, Bulgarie.

PORTER M., (1980). An algorithm for suffix stripping. in Program, n°14, 130-137.

RISSANEN J., (1989). Stochastic Complexity in Statistical Inquiry. World Scientific Publishing
Co, Singapour.

SAGOT B., (2005). Automatic acquisition of a Slovak Lexicon from a Raw Corpus. In Lecture
Notes in Artificial Intelligence 3658 (Springer-Verlag), Proceedings of TSD'05, Karlovy
Vary, République Tcheque. pp. 156-163.

SAGOT B., (2007). Building a morphosyntactic lexicon and a pre-syntactic processing chain
for Polish. In: Proceedings of LTC 2007, Poznan, Pologne.

SCHMID H., (1994). Probabilistic Part-of-Speech Tagging Using Decision Trees. Proceedings
of International Conference on New Methods in Language Processing. September 1994.

SCHMID H., (1995). Improvements in part-of-speech tagging with an application to German.
in Proceedings of the ACL SIGDAT-Workshop, pp. 47-50.

VERONIS J., KHOURI L., (1995). Etiquetage grammatical multilingue: le projet Multext.
Traitement Automatique des Langues, 36(1/2), 233-248.

