TALN 2009 — Session posters, Senlis, 24-26 juin 2009

Grammaires d’erreur — correction grammaticale avec analyse
profonde et proposition de corrections minimales

Lionel Clementl Kim Gerdesz Renaud Marlet3
[1] Universite Bordeaux 1, LaBRI [2] ILPGA, LPP, Sorbonne Nouvelle [3] INRIA, LaBRI

Résumé. Nous presentons un systeme de correction grammatical ouvert, base sur des ana-
lyses syntaxiques profondes. La speciﬁcation grammaticale est une grammaire hors-contexte
equipee de structures de traits plates. Apres une analyse en foret partagee ou les contraintes
d’accord de traits sont relachees, la detection d’erreur minimise globalement les corrections a
effectuer et des phrases alternatives correctes sont automatiquement proposees.

Abstract. We present an open system for grammar checking, based on deep parsing. The
grammatical speciﬁcation is a contex-free grammar with ﬂat feature structures. After a shared-
forest analysis where feature agreement constraints are relaxed, error detection globally mini-
mizes the number of ﬁxes and alternate correct sentences are automatically proposed.

Mots-clés : Correcteur grammatical, analyse syntaxique, forét partagee.

Keywords : Grammar checker, parsing, shared forest.

1 Introduction

La correction grammaticale est une des technologies du TAL les plus utilisees du grand public.
Pourtant, elle a suscite comparativement peu de recherches, au moins en nombre de publica-
tions. 11 y a sans doute plusieurs raisons a cela.

Tout d’abord, d’un point de vue pratique, les systemes semblent relativement dependants de
la langue a corriger. En outre, une grande partie du travail de deﬁnition d’un correcteur gram-
matical consiste en un recueil patient d’erreurs idiosyncrasiques, qui peuvent dependre de la
langue maternelle du locuteur et de son niveau de connaissance. Cette tache est difﬁcilement
automatisable faute de larges corpus d’erreurs disponibles. Enﬁn, l’utilite d’un tel systeme re-
pose beaucoup sur son integration dans un traitement de texte, et ce n’est que depuis la montee
d’OpenOfﬁce qu’un tel travail peut facilement etre mis a la disposition du public. 11 est desor-
mais imaginable qu’une communaute se cree autour de la recherche en correction grammaticale,
qui partage ouvertement ses ressources et resultats, comme dans d’autres domaines du TAL.

Il existe peut-etre aussi des raisons plus profondes. La linguistique a pI'lS du temps pour se deﬁ-
nir en tant que science de la Langue reellement parlee, et un retour du debat sur la normativite
n’est pas a l’ordre du jour, meme si certaines recherches (en sociolinguistique, en psycholin-
guistique, en recherche sur le FLE ou encore en lexicologie) sur les derivations de la norme
peuvent avoir un interet indirect pour le developpement d’un correcteur grammatical.

Cet article plaide pour une analyse profonde et complete de la phrase, par opposition aux gram-
maires superﬁcielles et locales frequemment utilisees, et pour une minimisation globale des er-
reurs a corriger. Sur la base d’un analyseur de graInInaire de reectiture amenage, nous montrons
comment determiner les erreurs et faire des propositions concretes de meilleures corrections.

Lionel Clément, Kim Gerdes, Renaud Marlet

2 Probléme général de la correction grammaticale

Un correcteur grammatical est un outil qui a deux fonctionnalités principales :

— Il signale les phrases ou fragments incorrects (probablement erronés) dans un document.

— Il propose des corrections, avec éventuellement une explication « linguistique » de l’erreur.
En pratique, on demande des propriétés additionnelles a un correcteur grammatical :

— qu’il soit rapide : la veriﬁcation doit s’effectuer « en temps réel » au cours de la frappe ;

— qu’il minimise le bruit : s’il y a trop de fausses altertes, les utilisateurs le débrancheront;

— qu’il minimise le silence : il doit laisser passer peu d’erreurs.

Ce ne sont bien sur la que les fonctionnalités et exigences principales.

2.1 Grammaire positive vs grammaire négative

Il y a deux moyens pour décider si une phrase est correcte : utiliser une grammaire positive, qui
décrit toutes les phrases correctes, ou une grammaire négative, qui décrit les phrases incorrectes.
Du fait des régularités de la langue, il est beaucoup plus facile d’écrire une grammaire positive.
Une grammaire négative n’est cependant pas inutile. Les deux sont en fait complémentaires et
permettent d’ approcher les phrases correctes par un double encadrement. La graInmaire positive
peut notamment vériﬁer la structure et la cohérence générale de la phrase, et la gramInaire nega-
tive se concentrer sur les erreurs courantes. Cette séparation des concepts est aussi motivée sur
le plan de l’ingénierie linguistique : la graInmaire positive est propre a la langue, indépendante
de l’énonciateur, alors que la grammaire négative peut étre choisie en fonction de l’utilisateur,
par exemple pour signaler des faux aInis graInmaticaux propres a une autre langue. Contrai-
rement a d’autres, nous n’incluons pas dans la graInmaire négative la veriﬁcation des accords
(traitée par la grammaire positive), mais des erreurs plus stylistiques, comme les barbarismes
(par ex. l’anglicisme « faire une décision ») ou pléonasmes (par ex. « danger potentiel »).

Un locuteur natif commet moins d’erreurs qu’un apprenant, et elles ont une répartition diffe-
rente. On peut penser qu’employer une forme marquée (par ex., en francais, une marque de
féminin) est plus signiﬁcatif pour un locuteur natif que pour un apprenant, que la corriger est
donc moins plausible, et ce d’autant plus que la marque est particulierement visible ou audible
(par ex. « actrice / acteur », par opposition a « jolie / joli »). Des gradations propres a l’utilisateur
sont donc envisageables. En outre, les possibilités d’édition locale de la modalité textuelle, y
compris le « copier / coller », peuvent conduire du fait de fautes d’inattention a des incohérences
globales. Aucun type d’erreur ne peut donc étre exclu, quel que soit l’utilisateur.

2.2 Grammaire de surface vs grammaire profonde

Une panoplie d’outils sont disponibles pour analyser une phrase aﬁn de préparer un jugement
de correction grammaticale : étiqueteur de parties du discours, chunker, expressions régulieres,
grammaires d’uniﬁcation, etc. Plus une analyse est superﬁcielle, plus elle est propice au bruit et
au silence. Les grammaires de surface restent de ce fait cantonnées aux graInmaires négatives,
ou la localité des regles minimise cet effet, sans l’annuler. Des attributs pauvres (par ex. de
simples étiquettes) augmentent aussi le nombre de regles a écrire (Souque, 2008). Par ailleurs,
l’éclatement en regles indépendantes est source de signalements multiples pour un meme mot.

La plupart des correcteurs grammaticaux (par exemple Antidote, Cordial, Microsoft Ofﬁce et
Prolexis pour le francais) sont des produits commerciaux dont le fonctionnement est opaque. Le
type de données manipulées est parfois connu, mais pas les calculs faits pour corriger. Langua-
geTool (Naber, 2007) est actuellement un des rares correcteurs grammaticaux libres et ouverts ;

Correction grammaticale avec analyse profonde et proposition de corrections minimales

il est notamment interface a OpenOfﬁce. L’ infrastructure de correction grammaticale est basée
sur un étiqueteur de parties du discours et des regles locales construites a l’aide d’expressions
régulieres. Plus de 1600 regles sont disponibles pour le francais. Considérons par exemple :

(1) Le chien de mes voisins mordent.
(2) Les voitures qu’il a font du bruit.

Avec des regles locales, LanguageTools, a tort, reste silencieux sur (1) et signale deux erreurs
dans (2). Microsoft Ofﬁce (Fontenelle, 2006), qui semble faire des analyses plus profondes,
signale bien l’erreur dans (1), mais demande a tort dans (2) a remplacer « font» par « fait ».

2.3 Nos objectifs

Pour répondre aux questions qui precedent, nous nous donnons les objectifs suivants.

— Nous voulons un systeme ouvert, indépendant de la langue, o1‘1 l’on peut spéciﬁer facilement
lexiques et grammaires, et les compléter incrémentalement par simple ajout de regles.

— Nous voulons une analyse profonde, capable de modéliser des phénomenes complexes comme
des dépendances a longue distance, inaccessibles a de simples expressions régulieres.

— Nous voulons pouvoir modéliser une grammaire positive et une grammaire négative, cette
demiere s’appuyant sur les analyses profondes de l’analyseur de grammaire positive.

— Nous voulons automatiser la localisation des erreurs, leur hiérarchisation, et pour certaines les
propositions de correction : le travail principal du linguiste doit étre de modéliser la langue.

— Nous voulons un correcteur rapide, capable d’analyser un texte au cours de la frappe.

— Nous voulons un systeme librement disponible, facilement intégrable dans un éditeur.

Un correcteur performant pour une langue donnée nécessite une étude de domaine poussée et

la spéciﬁcation d’une large quantité de regles (Fontenelle, 2006). Tel n’est pas notre propos ici.

Ce que nous présentons dans cet article sont les principes et le moteur de notre systeme d’ana-
lyse de grammaire positive, indépendamment de la langue. Le francais sert d’illustration. En
bref, le processus de correction est le suivant. Une phrase est d’abord segmentée en un graphe
acyclique orienté qui représente toutes les séquences possibles des formes lemmatisées des mots
simples et composés, auxquelles s’ajoutent des lemmes supplémentaires (par ex. homophones)
représentant des substitutions plausiblesl. L’ analyseur syntaxique construit ensuite une forét
partagée d’analyses en ignorant les phénomenes d’accord. Puis un parcours montant de la forét
attribue aux analyses alternatives des coﬁts minimums de modiﬁcation de traits et d’utilisation
de lemmes substitués, aﬁn de satisfaire les accords. Une phrase qui a une analyse de coﬁt Ini-
nimum nul est correcte; sinon, selon le coﬁt de correction établi au niveau global, un parcours
descendant détermine les ﬂexions et substitutions qui reconstruisent une phrase correcte.

3 Correction grammaticale

Un correcteur grammatical localise des erreurs et propose des corrections vraisemblables, sug-
gérant d’abord les plus plausibles. Le probleme est donc de trouver des phrases correctes au
voisinage d’une phrase incorrecte et de les classer par ordre de proximité.

3.1 Le principe du reléichement de contraintes

Les notions de voisinage et de proximité sont subjectives. Les circonscrire et les formaliser
pour une langue demande un travail considérable d’expérimentation sur corpus. Nous faisons

1Le séquencement en << mots » pour le frangais recherche des caracteres séparateurs. Nous travaillons sur une
méthode pour traiter de fagon plus générale et robuste la composition, 1’ agglutination, etc. pour des langues variées.

Lionel Clement, Kim Gerdes, Renaud Marlet

l’hypothese que l’on peut modeliser un voisinage avec une grammaire positive dont on relache
certaines contraintes aﬁn de construire davantage d’analyses modulo erreur, et que l’attribution
d’un poids aux contraintes insatisfaites renseigne sur la proximite. La speciﬁcation du voisinage
est en realite inversee : on decrit alors les phrases incorrectes a proximite des phrases correctes.

Les grammaires intrinsequement basees sur des contraintes, comme les grammaires de pro-
prietes, sont particulierement bien adaptees a cette vision du probleme (Prost, 2008), mais
elles posent encore des problemes de performance et sont moins repandues. Le relachement
de contraintes a toutefois fait ses preuves pour la robustesse des analyseurs syntaxiques qui
utilisent des systemes de traits. Par exemple, (Vogel & Cooper, 1995) appliquent cette idee a
HPSG, et (Fouvry, 2003) ajoute des ponderations sur l’importance des traits. Sur ce principe,
nous avons opte pour un formalisme connu et eprouve, qui permet une implementation efﬁcace.
Notre speciﬁcation grammaticale est une grammaire non contextuelle dont les termes sont equi-
pes d’une structure de traits. Pour un bon compromis efﬁcacite-expressivite, nos structures de
trait sont plates et construites sur des ensembles de valeurs ﬁnis. En voici un exemple simpliﬁe.

gn [nb=N ; gen=G ;pers=3] —> det [nb=N ; gen=G] sadj [nb=N ; gen=G ; type=anté] *
nc [nb=N ; gen=G] sadj [nb=N ; gen=G;type=post] * gp [] ? rel [nb=N; gen=G] ? ;

Nous relachons les contraintes d’accord liees aux traits. L’ analyse de la structure hors-contexte
reste en revanche rigide. La grammaire peut neanmoins etre ecrite pour accepter des phrases
incorrectes, par exemple en rendant certains termes optionnels (evaluables par la grammaire
negative). Nous conservons la totalite des ambiguites structurelles d’analyse : nous constr11isons
une forét partagee sur le squelette hors-contexte de la grammaire (Billot & Lang, 1989). Nous
reposons pour cela sur une variante de l’algorithme d’Earley (Earley, 1970) capable de traiter
directement l’etoile de Kleene et les termes optionnels, de complexite cubique en la longueur
de la phrase dans le pire cas. Qui plus est, nous considerons qu’un axiome de la grammaire peut
debuter a chaque mot d’une phrase. Nous pouvons ainsi construire toutes les analyses partielles
d’une phrase non couverte par la grammaire et signaler les desaccords de traits locaux.

On peut diviser les erreurs grammaticales en erreurs structurelles (pas de structure de consti-
tuants) et erreurs non structurelles (l’uniﬁcation echoue) (Bustamante & Leon, 1996). Pour les
premieres, on ne peut pas proposer de correction; on ne peut que signaler la presence d’une er-
reur. Pour les secondes en revanche, on peut lister des propositions alternatives en jouant sur les
valeurs de traits. Nous elargissons cette notion d’erreur non structurelle en autorisant des sub-
stitutions lemmatiques plausibles (par ex. des homonophones), qui permettent de proposer des
corrections meme si la phrase analysee n’a pas de structure de constitutants. Certaines erreurs
structurelles, comme l’anteposition adjectivale, peuvent aussi etre modelisees avec des traits.

3.2 Correction minimale

Uszkoreit, cite dans (Sagvall Hein, 1998), propose de distinguer 4 niveaux dans la correc-
tion grammaticale : (1) identiﬁcation de segments possiblement errones, (2) identiﬁcation de
contraintes possiblement violees, (3) identiﬁcation de sources d’erreur possibles, et (4) construc-
tion et hierarchisation d’alternatives de correction. C’est sur ce dernier point que nous mettons
l’accent. Nous modelisons la plausibilite d’une correction en terme de com‘ minimum. Plusieurs
notions de Ininimalite sont concevables. L’ exemple suivant illustre la question de la localite.

(3) Les cheval blanc sont salissants.
(4) Le cheval blanc est salissant.
(5) Les chevaux blancs sont salissants.

Correction grammaticale avec analyse profonde et proposition de corrections minimales

Avec une decision locale, la meilleure correction de « les cheval blanc » dans (3) est un deter-
minant singulier; puis pour la phrase, 3 mots au singulier et 2 au pluriel font preferer (4). Mais
en fait, a un niveau global, on recense 5 termes a accorder en nombre; 3 sont au pluriel, 2 au
singulier. La correction globalement minimale en nombre de modiﬁcations est donc (5). Une
decision locale est ainsi inexacte, et peut aussi conduire a de mauvais signalements en cascade.

Deux modeles de proximite permettent d’ordonner les corrections plausibles : l’un est base sur
le nombre de mots a corriger, l’autre sur le nombre de traits. Bien que souvent en accord, ces
deux modeles ne sont pas comparables :

(6) C’est encore une histoire de cliente arrivee mecontent mais repartis satisfaits.
(7) C’est encore une histoire de client arrive mecontent mais reparti satisfait.

(8) C’est encore une histoire de cliente arrivee mecontente mais repartie satisfaite.
(9) C’est encore une histoire de clients arrives mecontents mais repartis satisfaits.

Dans (6), le syntagme nominal de « client » est plutot au masculin, 3 votes contre 2 et plutot au
singulier, 3 votes contre 2 egalement. Cette minimisation du nombre de traits modiﬁes corrige
4 occurrences de traits et 4 mots (7). Mais deux alternatives sont plus economiques en nombre
de mots corriges : (8) et (9). Elles corrigent 5 occurrences de traits et mais seulement de 3 mots.
Nous optons ici pour la minimisation du nombre de traits a corriger, qui nous semble « cogniti-
vement » plus motivee, mais l’autre choix peut neanmoins etre encode dans notre proposition.

4 Correction lexicale

Voisinage et proximite jouent aussi au niveau du mot. Le lexique doit permettre de deﬁnir
quelles formes peuvent en corriger d’autres, eta quel coﬁt. Un premier type de correction cor-
respond a des ﬂexions alternatives d’un meme lemme, par ex. « jolie » pour « joli ». Mais tout
trait ne varie pas librement, par ex. rendre « crayon » feminin a un coﬁt inﬁni. Un deuxieme type
de correction correspond a une substitution plausible de lemmes, notamment des homophones :
on/ ont, est/ aitl ai, al a, etc. Comme les categories peuvent ici varier, a la difference du cas
precedent, ces corrections potentielles construisent des analyses alternatives supplementaires.
Cela augmente donc la combinatoire de l’analyse et il convient d’en bien mesurer l’usage.

La frontiere entre grammaire positive et grammaire negative est ici tenue car on souhaite inclure
au voisinage des phrases correctes (grammaire positive) un maximum de phrases incorrectes
plausibles. Sans faire appel a une grammaire negative, on peut par exemple vouloir corriger un
« que » errone en « dont », soit en creant une classe lemmatique pour certains pronoms relatifs,
que l’on ﬂechit avec un trait de cas, soit en les considerant explicitement comme substituables
les uns aux autres. On peut faire de meme pour les prepositions aﬁn de corriger la rection
verbale. Par ailleurs, le coﬁt d’une correction peut dependre de l’utilisateur, du type de lemme,
et de ses formes (cf. §2.1). Le coﬁt de substitution n’est donc pas symetrique, ni ﬁge dans le
lexique; une partie doit pouvoir étre calculee dynamiquement. Pour cela, nous deﬁnissons :

— un lexique de ﬂexions, qui speciﬁe les lemmes, leur formes et les variations de leurs traits,

— un lexique de substitutions, qui speciﬁe des substitutions entre formes de lemmes differents,
— un modéle de proximite’ qui associe des coﬁts aux substitutions de traits ou de lemmes.

Notre lexique de ﬂexions actuel a un format extensionnel :

# Forme Lemme Catégorie et traits

heureux heureux adj [gen=masc; nb=sing|p1ur]

portions portion nc [gen=fem!; nb=p1ur]

portions porter v[pers=1; nb=p1ur; mode=ind|subj; tps=imp]

Lionel Clément, Kim Gerdes, Renaud Marlet

Si plusieurs lemmes correspondent a une forme, on crée autant d’entrées. Pour chaque forme,
nous indiquons le lemme correspondant, sa catégorie et ses valeurs de traits actuelles. L’ opera-
teur « I » permet de spéciﬁer un ensemble de valeurs. Par défaut, toutes les valeurs possibles
d’un trait sont autorisées; la marque « ! » interdit les valeurs autres que celle indiquées. Les
traits et valeurs sont pour cela déclarés au préalable. Peu coﬁteuse et structurante pour le lin-
guiste, cette déclaration permet aussi de signaler des incohérences dans le lexique ou la gram-
maire. Construire un tel lexique (pour une langue peu dotée) est en grande partie automatisable
(Clément et al., 2004). (De fait, notre lexique pour le francais est déduit du Lefff.) Quant au
lexique de substitutions, il liste les formes substituables les unes aux autres, par ex., « onl ont »,
« a I a ». Le lexeur introduit systématiquement ces formes commes alternatives avant l’analyse.
(Notre lexique de substitutions du francais provient des homophones de Lexique 3 (New, 2006).)

Notre intention est d’ offrir des outils pour paramétrer facilement un modele de coﬁt, par exemple
une distance de Levenshtein pour pondérer un coﬁt par le degré de marquage des formes. Ac-
tuellement, c’est un programme ad hoc qui attribue un coﬁt, selon un schéma ﬁxe tres simple. A
toute ﬂexion (c.-a-d. assignation d’une valeur de trait), est associé le coﬁt de cette correction :
— Le coﬁt de ﬂexion est 1, quel que soit le type de trait, pour toute assignation a une valeur
autre que celles indiquées (par ex. 1 pour attribuer fem a « heureux »).
— L’ assignation d’un trait a une valeur interdite a un coﬁt inﬁni (par ex. fem pour « crayon »).
— Une substitution a un coﬁt (arbitraire) de 1. Elle est modélisée par un trait implicite addition-
nel, noté « $ », et qui ne peut avoir qu’une valeur : son assignation est donc obligatoire.
On peut aussi coupler ici le correcteur orthographique au correcteur grammatical en incluant des
substitutions proposées pour des mots hors lexique. Cela permet de traiter a un meme niveau
les corrections grammaticales et les ﬂexions erronées comme « chevals » ou « faisez ». (Une
simpliﬁcation consiste a laisser l’utilisateur corriger d’abord l’orthographe, puis la grammaire.)

5 Signalement d’erreur

Supposons le texte segmenté en phrasesz. Toute phrase est d’abord elle-meme segmentée par un
lexeur qui construit un graphe orienté acyclique (DAG) de lemmes alternatifs (dus aux ambigu'1'-
tés lexicales et aux substitutions plausibles), auxquels sont ajoutées des informations morpho-
syntaxiques (notamment les catégories ﬂexionnelles).

L’ analyse syntaxique structurelle de ce DAG construit ensuite un graphe et/ou qui représente
une forét partagée d’arbres d’analyse (cf. ﬁg. 1). Les traits sont totalement ignorés a ce stade;
seule est prise en compte et conservée la structure syntagmatique de la phrase (arbres de deri-
vation du squelette non contextuel de la grammaire). Le nombre d’analyses différentes dans le
pire cas est exponentiel en la taille de la phrase. Les différents arbres d’analyse ne sont toutefois
pas énumérés ici. C’est la forét partagée qui les représente qui est construite, en un temps au
plus cubique en la taille de la phrase. Le parseur s’accomode aussi d’analyses inﬁnies, comme
il peut s’en produire (généralement involontairement) avec des regles comme np -> np pp ?.
Il génere dans ce cas des cycles dans la forét, qui peuvent aisément étre éliminés.

Faute de place, nous ne détaillons pas ici l’algorithme complet de détermination des corrections
de coﬁt global minimum (soumis pour publication); nous en présentons juste l’idée intuitive,
informellement. Nous n’explicitons pas non plus l’ensemble des constructions grammaticales
disponibles ; nous nous concentrons sur la construction principale.

2Des heuristiques de découpage basées sur la ponctuation fournissent une segmentation raisonnable. Nous
travaillons néanmoins sur une technique plus robuste qui exploite la capacité de notre analyseur a travailler en ﬂux.

Correction grammaticale avec analyse profonde et proposition de corrections minimales

Phrase

GN GV

   

pronom

/
_]e

nom préposition GN
honlime avlec  
déterminant N
.1; @
nom
lunettes

FIG. 1 — graphe et-ou de la forét partagée pour « je vois un homme avec des lunettes »

5.1 Recherche d’erreur

La recherche d’erreur est un parcours de la forét d’analyse pour déterminer les coﬁts de correc-
tion minimums des différentes analyses alternatives. A la racine, une forét qui contient un arbre
de coﬁt de correction nul est considérée sans erreur — au risque d’étre parfois silencieux du fait
de possibilités de rattachement erronées, et en l’absence, pour le moment dans notre systeme,
d’un jugement de plausibilité syntaxico-sémantique des analyses. Sinon, les coﬁts de correction
des alternatives permettent d’ordonner les propositions de correction par ordre de plausibilité.

Une correction est représentée par une assignation de traits, c’est-a-dire un choix de valeur pour
un ensemble de traits. Par exemple, pour un syntagme nominal : (genre n—> masc, nombre n—> pl).

Par ailleurs, un coﬁt d ’assignati0n de traits associe a toute valeur possible d’un trait un coﬁt qui
représente la plausibilité de modiﬁer ce trait pour qu’il prenne la valeur en question dans une
correction éventuelle (plus le coﬁt est faible, plus la plausibilité est grande). Par exemple, pour
un adjectifmasculin singulier: (genre n—> (masc I—> 0, fem n—> 1), nombre n—> (sg n—> 0, pl n—> 
Un coﬁt inﬁni correspond a une affectation de trait impossible, comme d’imposer le trait féminin
a nom commun exclusivement masculin (par ex. « crayon »).

Enﬁn, effectuer une assignation de traits, sur la base d’un certain coﬁt d’assignation de traits, a
un coﬁt global qui est la somme des coﬁts des valeurs de traits assignées. Par exemple, effectuer
l’assignation de traits (genre n—> masc, nombre n—> pl) sur la base du coﬁt d’assignation de traits
(genre n—> (masc n—> 0,fem n—> 1), nombre n—> (sg n—> 0, pl n—> 1)) aun coﬁt global de 0 + 1 = 1 :
le coﬁt de la modiﬁcation du genre (ici nul) ajouté au coﬁt de modiﬁcation du nombre (ici 1).

Lionel Clément, Kim Gerdes, Renaud Marlet

Pour rechercher et ordonner les corrections, nous examinons les coﬁts d’assignation de traits des
analyses alternatives. L’ absence de monotonie des coﬁts minimums de correction et la recherche
d’un optimum global obligent a priori a examiner toutes les alternatives. Dans le pire cas, elles
peuvent etre en nombre exponentiel en la longueur de la phrase. Dans cette sous-section, nous
considérons que nous les examinons effectivement toutes; nous verrons a la §5.2 comment re-
duire et contreler cette combinatoire. Dans tous les cas, nous exploitons néanmoins la structure
de partage syntaxique pour factoriser le calcul des coﬁts d’assignation de traits alternatifs.

Pour tout noeud du graphe et/ou de la foret d’analyse, on calcule un ensemble de coﬁts alter-
natifs d ’assignati0n de traits qui représentent l’ensemble des coﬁts minimums d’utilisation de
ce noeud dans une alternative d’analyse, et pour les différentes affectations de traits. Le calcul
opere « de bas en haut » (des feuilles vers les racines), de la maniere suivante3.

(a) Une feuille de la foret est un item lexical, qui ne représente qu’une alternative. Son coﬁt
d’assignation de traits est déﬁni par le modele de coﬁt pour les corrections lexicales (cf. §4).

(b) Un noeud-ou représente un ensemble de sous-arbres alternatifs (dont la tete est associée a
un meme non-terminal de la grammaire). L’ ensemble des coﬁts d’assignation de traits pour
un noeud-ou est la réunion de tous les coﬁts alternatifs de ses noeuds ﬁls.

(c) Un noeud-et représente la tete d’un sous-arbre. Il correspond a l’application d’une regle
de grammaire, regle de la forme A —> B1B; . . .Bn, ou chaque élément B, = b,-[. .  (et de
meme pour A = a[. . .]) est un non-terminal b,- (resp. a) associé a un ensemble d’équations de
traits de la forme «trait = variable ». La tete du sous-arbre que représente un tel noeud-et est
associée au non-terminal a; ses ﬁls sont des noeuds-ou associés aux non-terminaux b,-.

L’ idée intuitive est que le coﬁt de correction d’un syntagme, c’est-a-dire le coﬁt d’assignation
de certains traits a des valeurs choisies, est la somme des coﬁts de correction de chaque
élément de ce syntagme. En l’occurrence, le coﬁt de correction du syntagme associé a A est
la somme des coﬁts de correction des B,-. En pratique, il faut tenir compte des points suivants :

1. A chaque B, est associé non pas un unique coﬁt d’assignation de traits, mais un en-
semble K ,- de tel coﬁts, correspondant a des analyses alternatives du noeud-ou associé.
L’ensemble des coﬁts d’assignation de traits du noeud-et est basé sur l’ensemble des
combinaisons possibles de choix d’un n-uplet de coﬁts dans K1 >< - - - >< Kn. (C’est la
source de l’explosion combinatoire, que nous montrons comment contreler en §5.2.)

2. Les coﬁts d’assignation de traits a additionner sont ceux pour lesquels un trait apparait
plusieurs fois avec une meme variable dans la partie droite de la regle, comme par
exemple le trait nb dans la regle de la section §3.l, associé a la variable N. Ce traitement
remplace en quelque sorte l’opération d’uniﬁcation d’une analyse syntaxique ordinaire :
au lieu de veriﬁer que les valeurs associées aux différentes occurrences de N sont égales,
on calcule pour chaque valeur possible (ici sg ou pl) le coﬁt de l’ affecter a N dans la partie
droite de la regle. Et ce coﬁt, qui peut etre non nul pour une valeur de trait donnée, est la
somme des coﬁts correspondants déja calculés pour les éléments de cette partie droite.

3. On peut noter que les equations de traits associées a un B,-, de la forme «trait = va-
riable », ne couvrent pas nécessairement tous les traits qui ﬁgurent dans l’ensemble des
coﬁts d’assignation calculés pour B,-. Certains traits, en quelque sorte, ne sont donc pas
propagés au niveau de la regle. Nous pouvons statuer sur eux localement, des ce ni-

3Nous ne décrivons pas ici le traitement d’un certain nombre de constructions : ﬁltrage (equivalent du =0 de
LFG), contrainte par des constantes (<< trait = valeurs » en partie gauche ou droite de regle), idem avec variable
(note << trait = variable & valeurs) », Le cas << trait = variable », que nous decrivons ici, est le coeur de l’algorithme.

Correction grammaticale avec analyse profonde et proposition de corrections minimales

veau. En revanche, on ne peut pas encore statuer sur les traits qui sont propagés, car des
contraintes additionnelles peuvent ultérieurement en modiﬁer les coﬁts d’assignation.

Pour garder la trace des traits non propagés sur lesquels on a statue, nous nous repo-
sons sur le trait spécial $ (aussi utilisé dans le lexique de substitution, cf. §4), que l’on
propage systématiquement jusqu’en haut de la forét. Pour effectuer cette propagation
systématique, le trait $ est implicitement considéré comme présent dans chaque non-
terminal, associé a une meme variable de trait a:$. Autrement dit, toute regle déﬁnie
come a[...] —> b1[...] .  représente en fait la regle a[...;$ = 113$] —> b1[...;$ =
x$] . . . bn  ..; $ = J:$]. Statuer sur les traits non propagés consiste a intégrer (additionner)
leur coﬁt d’assignation minimum dans le trait spécial $. Ils pourront ainsi contribuer au
coﬁt global lorsque l’on statuera sur l’ensemble des traits, a la racine de la forét.

4. Il faut également prendre en compte le fait que certaines variables peuvent n’apparaitre
qu’en partie droite de la regle, et non en partie gauche. En ce cas, on peut également
statuer localement sur les traits auxquelles elles sont associées. Les coﬁts minimums
d’assignation de traits correspondant sont alors aussi cumulés dans le trait spécial $.

Arrivé a la racine, on dispose d’un ensemble de coﬁts d’assignation de traits alternatifs, dont
on calcule le minimum. Une fois choisie une assignation de valeur de traits de coﬁt minimum,
un parcours inverse de haut en bas (de la racine vers les feuilles) énumere les alternatives de
correction effectives qui ont ce coﬁt minimum, aﬁn de les présenter a l’utilisateur. S’il y a de trop
nombreuses alternatives de correction de coﬁt minimum, on peut aussi en seuiller le nombre.

5.2 Réduction et contrﬁle de la combinatoire

Grace a la forét partagée, l’algorithme en §5.1 partage des calculs et structures de données. Il
n’en énumere pas moins toutes les alternatives d’analyse, potentiellement en nombre exponen-
tiel. Il est capital de réduire cette combinatoire, sans altérer l’optimum ou via des heuristiques.

On peut tout d’abord éliminer les coﬁts d’assignation de traits dont on sait qu’ils seront de toute
facon plus mauvais que tout autre coﬁt d’assignation de traits dans d’un méme ensemble d’al-
ternatives : soit parce que leurs coﬁts individuels (pour toute valeur de trait) sont uniformément
plus élévés, soit parce que leur coﬁt minimum (sur l’ensemble des traits) est supérieur au coﬁt
maximum de chaque autre alternative. La combinatoire est réduite sans modiﬁer l’optimum.

Bien qu’expérimentalement efﬁcace sur le type de phrases et de grammaire utilisées, cette re-
duction du nombre d’alternatives a considérer ne garantit toutefois pas un temps de correction
polynomial. Pour se prémunir d’une explosion combinatoire en toute circonstance, on peut en
outre (ou altemativement) appliquer une ou plusieurs des heuristiques suivantes :

— éliminer toute alternative dont le coﬁt minimum est au dessus d’un certain seuil,

— éliminer toute alternative dont le coﬁt maximum est au dessus d’un certain seuil,

— borner la taille des ensembles d’altematives. (Dans le cas particulier ou cette borne est 1, on

retrouve comme cas particulier le calcul d’un optimum purement local.)

L’ optimum n’est alors plus nécessairement global, mais la complexité devient polynomiale en
la taille de la phrase. Seule la derniere heuristique garantit qu’une solution est toujours trouvée.

6 Conclusion

Nous avons posé le probleme central de la correction grammaticale et argumenté en faveur
d’une correction globale, basée sur l’accord des traits syntaxiques. Nous avons présenté pour
cela un formalisme grammatical simple mais expressif, et appliqué un principe d’analyse en

Lionel Clément, Kim Gerdes, Renaud Marlet

structure rigide (mais ambigue) avec relachement des contraintes d’accord. Nous avons détaillé
l’algorithme qui calcule une correction optimale, ainsi que des propriétés et heuristiques qui
permettent de réduire et de controler sa combinatoire. Au ﬁnal, des alternatives de correction de
coﬁts minimal sont automatiquement proposées, en une seule passe sur la structure d’analyse.
Le systeme et la grammaire actuellement implémentés corrigent par exemple (10) en (11) :
(10) Les enfants ont mangé ces cerise rouge qui étaient juteuses et sucrées et qu’ils ont vu que
j’avais cueillis.
(11) Les enfants ont mangé ces cerises rouges qui étaient juteuses et sucrées et qu’ils ont vu
que j’avais cueillies.
(12) Les enfants ont mangé cette cerise rouge qui était juteuse et sucrée et qu’ils ont vu que
j’avais cueilli.
Cette correction minimale (3 traits) nécessite une analyse a la fois globale et profonde. La forét
est ici assez peu ambigue, grace a l’usage massif de l’étoile de Kleene et des termes optionnels.
La correction, analyses syntaxiques partielles comprises, s’effectue en moins de 300 ms. Par
comparaison, Microsoft Ofﬁce, qui pourtant semble avoir une certaine vision globale, fait des
erreurs en cascade et corrige (10) en (12), ce qui n’est pas minimal (5 traits modiﬁés) et meme
faux (mauvais accord du participe passé de la relative). LanguageTool reste silencieux.

Des expériences sont en cours pour mettre le systeme a l’épreuve, a la fois en termes de per-
formance et de facilité d’écriture de grammaire. Nous voulons aussi expérimenter avec d’autres
langues, et implémenter une intégration dans OpenOfﬁce.

Références

BILLOT S. & LANG B. (1989). The structure of shared forests in ambiguous parsing. In 27th
annual meeting on Association for Computational Linguistics (ACL), p. 143-151 : ACL.

BUSTAMANTE F. R. & LEON F. S. (1996). Gramcheck : A grammar and style checker. In
COLING, p. 175-181.

CLEMENT L., SAGOT B. & LANG B. (2004). Morphology based automatic acquisition of
large-coverage lexica. In LREC ’04. Voir http : / /alpage . inria . f r/ ”sagot/ lef f f .htm1.
EARLEY J. (1970). An efﬁcient context-free parsing algorithm. Comm. of the ACM, 13(2).
FONTENELLE T. (2006). Les nouveaux outils de correction linguistique de Microsoft. In
Confe’rence sur le Traitement Automatique des Langues Naturelles (TALN), p. 3-19, Louvain.
FOUVRY F. (2003). Constraint relaxation with weighted feature structures. In 8th Intematio-
nal Workshop on Parsing Technologies.

NABER D. (2007). Integrated tools for spelling, style, and grammar checking. OpenOfﬁce.org
Conference, Barcelona. Outil disponible a l’URL http : / /www . languagetool . org.

NEW B. (2006). Lexique 3 : une nouvelle base de données lexicales. In TALN ’06, p. 892-900.
PROST J .-P. (2008). Modélisation de la gradience syntaxique par analyse relachée a base de
contraintes. These de doctorat, Université de Provence et Macquarie University.

SOUQUE A. (2008). Vers une nouvelle approche de la correction grammaticale automatique.
In Rencontre des Etudiants Chercheurs en Informatique pour le TAL, p. 121-130, Avignon.
SAGVALL HEIN A. (1998). A chart-based framework for grammar checking - initial studies.
In 11th Nordic Conference in Computational Linguistic, p. 68-80.

VOGEL C. & COOPER R. (1995). Robust chart parsing with mildly inconsistent feature struc-
tures. Edinburh Working Papers in Cognitive Science .' Nonclassical Feature Systems, 10.

