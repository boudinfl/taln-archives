TALN 2009 — Session posters , Senlis, 24-26 juin 2009

La complémentarité des approches manuelle et automatique
en acquisition lexicale

Cédric Messiant 1,Takuya Nakamuraz, Stavroula Voyatziz
(1) Laboratoire d’Informatique de Paris-Nord
CNRS UMR 7030 et Université Paris 13
99, avenue J ean-Baptiste Clément, F-93430 Villetaneuse France
(2) Laboratoire d’Informatique Gaspard-Monge
CNRS UMR 8049 IGM-LabInfo et Université de Marne-la-Vallée
5 Bd Descartes, Champs-sur-Marne
77454 Marne-la-Vallée Cedex 2
cedric.messiant@lipn.uniV-paris13.fr, takuya.nakamura@uniV-rnlV.fr,
Voyatzi@uniV-mlV.fr

Résumé. Les ressources lexicales sont essentielles pour obtenir des systemes de traitement
des langues performants. Ces ressources peuvent étre soit construites a la main, soit acquises
automatiquement a partir de gros corpus. Dans cet article, nous montrons la complémenta-
rité de ces deux approches. Pour ce faire, nous utilisons l’exemple de la sous-catégorisation
verbale en comparant un lexique acquis par des méthodes automatiques (LexSchem) avec un
lexique construit manuellement (Le Lexique-Grammaire). Nous montrons que les informations
acquises par ces deux méthodes sont bien distinctes et qu’elles peuvent s’enrichir mutuellement.

Abstract. Lexical resources are essentially created to obtain efﬁcient text-processing sys-
tems. These resources can be constructed either manually or automatically from large cor-
pora. In this paper, we show the complementarity of these two types of approaches, compa-
ring an automatically constructed lexicon (LexSchem) to a manually constructed one (Lexique-
Grammaire), on examples of verbal subcategorization. The results show that the information
retained by these two resources is in fact different and that they can be mutually enhanced.

M0tS-CléS I verbe, syntaxe, lexique, sous-catégorisation.

Keywords: verb, syntax, lexicon, subcategorization.

Cedric Messiant, Takuya Nakamura, Stavroula Voyatzi

1 Introduction

Les applications de traitement automatique des langues (TAL) ont de plus en plus besoin d’in-
formations lexicales. La disponibilité de telles ressources et leur utilisabilité dans les applica-
tions de TAL restent pourtant relatives.

Deux approches sont possibles pour la constitution de telles ressources : la construction de
lexiques “a la main” et l’acquisition automatique de ressources a partir de corpus. Le plus
souvent, ces deux approches sont opposées et il existe a l’heure actuelle peu de collaborations
pour étudier les differences dans les informations obtenues et ce que les approches pourraient
apporter l’une a l’autre. L’ obj ectif de ce travail est de confronter les approches automatiques aux
besoins des linguistes. Pour ce faire, nous utilisons l’exemple de la sous-categorisation verbale
en comparant un lexique acquis par des méthodes automatiques (LeXSchem (Messiant et al.,
2008)) avec un lexique construit manuellement (Le Lexique-Grammaire (Gross, 1975)). Nous
avons observé leurs differences et leurs similitudes pour 20 verbes variés du francais.

Apres avoir dressé un bref état de l’art des lexiques et des méthodes d’acquisition existantes,
nous présentons les deux ressources qui ont été choisies pour cette étude. La section 4 montre
que ces deux approches sont complémentaires. La conclusion donne des perspectives pour l’uti-
lisation conjointe de ces approches.

2 Etat de l’art

La constitution de ressources lexicales a été dans un premier temps manuelle mais des méthodes
d’acquisition automatique ont été par la suite explorées.

2.1 Les lexiques existants pour le francais

Plusieurs ressources lexicales syntaxiques pour le francais ont été développées depuis de nom-
breuses années. Les objectifs de ces lexiques sont de déﬁnir, pour chaque lemme verbal donné,
ses différents emplois et, pour chacun de ces emplois, son (ou ses) cadre(s) de sous-categorisation
spéciﬁant le nombre et le type de ses arguments, et les informations complémentaires qui s’y
rapportent.

Le dictionnaire syntaxique des verbes francais créé par Jean Dubois et Francoise Dubois-
Charlier (Dubois & Dubois-Charlier, 1997), Inis a la disposition du grand public sur le site
internet du laboratoire MoDyCo, est une classiﬁcation sémantico-syntaxique des verbes ma-
nuellement construite par ces deux linguistes, dont les principes sont proches de ceux du LG.
On compte dans ce dictionnaire 12 130 verbes et 25 610 entrées (chaque entrée correspond a un
couple verbe - schéma de sous-categorisation).

DicoValence (van den Eynde & Mertens, 2006), successeur du lexique PROTON, est un diction-
naire syntaxique manuellement construit dans le cadre méthodologique de l’Approche Prono-
minale (van den Eynde & Blanche-Benveniste, 1978). Pour identiﬁer la valence d’un prédicat,
i.e. ses dépendants et leurs caractéristiques, l’Approche Pronominale exploite la relation qui
existe entre les dépendants dits lexicalisés (réalisés sous forme de syntagmes) et les pronoms
qui couvrent « en intention » ces lexicalisations possibles. DicoValence comporte les cadres

La complémentarité des approches manuelle et automatique en acquisition lexicale

de valence de 3 738 verbes, répartis en 8 313 entrées. Nous pouvons également mentionner
d’autres ressources comme LexValf (Salkoff & Valli, 2005) dont les principes de base sont
ceux de grammaire en chaine, DiCo-LAF (Mel’cuk & Polguere, 2006), centré sur la modélisa-
tion formelle des collocations et de la dérivation sémantique du francais, DicoLPL (Vanrullen
et al., 2005) ou encore le Trésor de la Langue Franc-aise informatisé (TLFI) (Dendien &
Pierrel, 2003).

Des ressources lexicales ont également été acquises semi-automatiquement. C’est le cas notam-
ment de TreeLex (Kupsé, 2007), acquis automatiquement a partir du corpus arboré de Paris 7
(Abeillé et al., 2003) ou du Lefff 2 (lexique des formes ﬂéchies du francais) (Sagot et al., 2006).
SynLex (Gardent et al., 2006) est un lexique de sous-catégorisation verbale du francais, créé a
partir des tables du LexiqueGran1Inaire, et complété manuellement.

2.2 Les méthodes d’acquisition automatique de ressources

Depuis le début des années 90, des chercheurs en traitement automatique des langues ont ex-
ploré des méthodes d’acquisition d’informations lexicales a partir de corpus et en particulier de
lexiques de schémas de sous-catégorisation (Brent, 1993; Manning, 1993; Briscoe & Carroll,
1997)

Dans un premier temps, ces méthodes ne permettaient d’acquérir qu’un nombre réduit de sché-
mas contenant peu d’informations sur les arguments. La disponibilité de corpus de taille conse-
quente et les performances des analyseurs syntaxiques ont permis a ces méthodes de devenir
de plus en plus efﬁcaces et d’obtenir des informations plus détaillées sur les arguments (Preiss
et al., 2007). En effet, la plupart de ces méthodes consistent a acquérir des schémas de sous-
catégorisation a partir des sorties d’un analyseur syntaxique et d’en ﬁltrer les erreurs par des
méthodes statistiques. La principale difﬁculté provient alors des erreurs effectuées par l’analy-
seur syntaxique et de la mise en place du ﬁltrage adéquat.

Dans un premier temps, ces méthodes ne permettaient d’acquérir qu’un nombre réduit de sché-
mas contenant peu d’informations sur les arguments. La disponibilité de corpus de taille conse-
quente et les performances des analyseurs syntaxiques ont permis a ces méthodes de devenir de
plus en plus efﬁcaces et d’obtenir des informations plus détaillées sur les arguments (Korho-
nen et al., 2000; Preiss et al., 2007). En effet, la plupart de ces méthodes consistent a acquérir
des schémas de sous-catégorisation a partir des sorties d’un analyseur syntaxique et d’en ﬁl-
trer les erreurs par des méthodes statistiques. La principale difﬁculté provient alors des erreurs
effectuées par l’analyseur syntaxique et de la mise en place du ﬁltrage adéquat.

Si la plupart des travaux ont d’ abord porté sur l’anglais, ces méthodes ont également été utilisées
pour acquérir des informations lexicales pour d’autres langues, comme l’allemand (Schulte im
Walde, 2002) ou l’italien (Dino Ienco & Bosco, 2008). J usqu’en 2006, i1 y avait peu d’études de
ce type pour le francais. Le premier travail publié est celui de Paula Chesley et Suzanne Salmon-
Alt (Chesley & Salmon-Alt, 2006). Cependant, cette étude ne concernait qu’une centaine de
verbes et n’a, a notre connnaissance, pas été poursuivie.

Les ressources acquises a partir de corpus ne sont pas aussi completes et précises que les res-
sources construites a la main mais elles apportent des informations qui ne sont souvent pas
disponibles dans les travaux manuels comme par exemple la fréquence des schémas de sous-
catégorisation en corpus.

Cédric Messiant, Takuya Nakamura, Stavroula Voyatzi

3 Une expérience a partir du Lexique-Grammaire et de Lex-
Schem

Aﬁn d’explorer la complémentarité de l’approche manuelle et de l’approche automatique, nous
avons travaillé sur deux ressources : 1e Lexique-Grammaire et LexSchem.

3.1 Le Lexique-Grammaire

Le lexique syntaxique du Lexique-Grammaire 1 (LG) est un dictionnaire syntaxique constitué
d’un ensemble de tables (matrices binaires). Chaque table regroupe les éléments prédicatifs
(verbes, adjectifs, noms), qui partagent tous la propiété de base d’entrer dans une structure de
phrase simple déﬁnitoire de la table (construction type). Une phrase simple est déﬁnie par le
nombre et la nature morpho-syntactique et sémantique des arguments. Chaque table comprend
également un ensemble de propriétés distributionnelles, transformationnelles et sémantiques,
que vériﬁent (+), ou non (-), les éléments prédicatifs qui ﬁgurent en en-tétes des lignes.

A peu pres 6 000 verbes simples graphiquement différents ont été examinés pour le francais et
donnent lieu a environ 11 000 emplois verbaux simples (ou entrées), dont :

— 3 000 emplois verbaux simples a constructions complétives répartis dans 18 tables,

— 8 000 emplois verbaux simples a constructions non complétives répartis dans 43 tables.

Une sélection des tables du LG du francais (60%) est Inise a disposition du grand public 2 sous
la licence LGPL-LR : en ce qui concerne les verbes simples, on peut compter plus de 7 000
entrées qui proviennent de 35 tables différentes.

3.2 LexSchem

Nous avons Inis au point une méthode d’acquisition automatique de schémas de sous-catégorisation
a partir de corpus pour le francais (Messiant, 2008). Notre systeme d’acquisition, appelé ASSCI,
prend en entrée un corpus analysé par l’analyseur syntaxique Syntex (Bourigault et al., 2005)

et produit un lexique de sous-catégorisation de verbes.

Une premiere expérience, menée sur le corpus LM10 (10 ans du journal Le Monde) a per-
mis d’acquérir LexSchem, un lexique de schémas de sous-catégorisation de verbes francais
(Messiant et al., 2008). Le lexique est composé de plus de 11000 entrées qui correspondent a
des couples verbe - schéma. LexSchem concerne plus de 3200 verbes francais et pres de 300
schémas de sous-catégorisation différents. Le lexique est téléchargeable librement et une inter-
face web permet de le consulter facilement3. Pour chacune des entrées, le lexique fournit son
nombre d’occurences en corpus, sa fréquence relative (fréquence d’apparition dans le corpus
d’un schéma de sous-catégorisation pour un verbe) ainsi que 5 exemples tirés du corpus. Nous
avons comparé les entrées de ce lexique pour 25 verbes francais choisis pour leur hétérogénéité

1Le lexique-grammaire est une théorie et une pratique de la description exhaustive des langues, inspirée de
la théorie distributionnelle et transformationnelle de Zellig S. Harris. La description du frangais a d’abord été
développée au LADL par une équipe de linguistes et d’informaticiens dirigée par Maurice Gross depuis la ﬁn
des armées 1960, et continue d’etre maintenue et enrichie par une équipe informatique—1inguistique de 1’Institut
Gaspard—Monge de l’Université Paris—Est Marne—1a—Va11ée.

Zhttp://infolingu.univ—mlv.fr

3http://www—lipn.univ—paris13.fr/~messiant/lexschem.html

La complementarite des approches manuelle et automatique en acquisition lexicale

avec une ressource reference construite a partir du TLFI 4. Nous avons choisi le TLFI car il est
librement disponible et que son format permettait de rapidement produire une reference pour un
nombre liIr1ite de verbes 5. Cette evaluation a donne une precision de 0.83, un rappel de 0.59 et
une F-mesure de 0.69. Ces resultats correspondent a ceux de l’etat de l’art pour des experiences
comparables (par exemple, (Preiss et al., 2007) malgre les differences liees a la langue ). Nean-
moins, de tels resultats posent la question de l’utilisabilite d’une ressource avec environ 30%
d’erreurs. Les erreurs observees dans LexSchem proviennent de sources variees : presence de
modiﬁeurs dans le schema, erreurs d’analyse syntaxique, etc... Certaines de ces erreurs seront
detaillees dans la section 4.1.

4 Limites et complémentarité des deux approches

Nous avons compare les entrees de LexSchem6 avec celles du LG pour 15 verbes separes en
3 groupes selon leur productivite dans LMIO : haute, moyenne et basse frequence. Nous avons
ensuite choisi 5 verbes pour chacun de ces groupes : savoir, demander, appeler, tourner et tra-
vailler pour les verbes a haute frequence; de’cerner, fasciner, appuyer, re’mune’rer et valider
pour les verbes a frequence moyenne; contrecarrer, ﬂuctuer, approvisionner, inse’rer et cerner
pour les verbes a basse frequence. Nous avons egalement ajoute a cette liste cinq verbes sus-
ceptibles d’etre utilises dans le domaine economique et boursier (augmenter, baisser, chuter,
cléturer etﬂuctuer).

Cette comparaison nous a perIr1is de mettre en evidence les limites des deux approches. Elle
nous a aussi montre pourquoi ces approches sont complementaires et comment elles peuvent
s’enrichir mutuellement.

4.1 Les limites de l’acquisition £1 partir de corpus et de LexSchem

Malgre les progres recents, l’acquisition de ressources lexicales a partir de corpus produit un
grand nombre d’erreurs (voir section 3.2). La confrontation des lexiques extraits par ces me-
thodes a des ressources constr11ites manuellement permet de mettre en evidence les sources des
erreurs et les lacunes de l’approche automatique.

De nombreuses erreurs d’etiquetage morpho-syntaxique ou d’analyse syntaxique sont repercu-
tees dans le lexique. Pour ce qui concerne les erreurs d’etiquetage, il s’agit dans la plupart des
cas de noms etiquetes verbe. Par exemple, dans le groupe nominal le programme d ’armement,
le mot programme est etiquete verbe et ASSCI produit alors le schema de sous-categorisation
SUJ : SN_P—OBJ : SP [de+SN] .Autre exemple, le CERNA (Centre d ’e’c0n0mie industrielle et
de ﬁnance de MINES ParisTech) apparait dans les exemples pour le verbe cemer. Notons que
ces exemples sont sufﬁsamment frequents pour produire des schemas de sous-categorisation
incorrects.

4http://atilf.atilf.fr/tlf.htm

5Une etude sur 25 verbes n’est pas sufﬁsante. C’est pourquoi nous travaillons actuellement a 1’eVa1uation de
LexSchem sur environ 1500 verbes frangais en uti1isantTreeLex comme ressource reference.

5La Version de LexSchem utilisee pour les experiences de cette section n’est pas celle qui est telechargeable et
consultable a 1’adresse donnee a la section 3.2. Les ameliorations de cette nouvelle Version comprennent notam—
ment1’integration des fonctions syntaxiques aux schemas de sous—categorisation et le traitement de cas jusqu’a1ors
mal traites.

Cedric Messiant, Takuya Nakamura, Stavroula Voyatzi

Par ailleurs, de nombreuses erreurs proviennent de l’analyse syntaxique. Par exemple, les pro-
nominalisations sont souvent mal traitées par l’analyseur comme dans Leurs obsessions nous
cement qui produit le schéma SUJ : SN (c’est-a-dire une construction de type intransitive). I1
existe plus généralement de nombreux compléments d’objets non rattachés, notamment quand
des incises brouillent l’analyse. Ce probleme est tres fréquent et on pourrait imaginer un trai-
tement a posteriori pour tenter de rattacher ces compléments au verbe. Cependant, un tel post-
traitement poserait alors la question de la “généricité” de la méthode et de son indépendance
par rapport a l’analyseur syntaxique utilisé.

Certains phénomenes linguistiques sont assez difﬁciles a prendre en compte par un systeme
d’acquisition qui se fonde sur des indices de surface. C’est le cas notamment des adverbes (qui
sont souvent des modiﬁeurs mais qui sont susceptibles d’étre inclus dans la structure argumen-
tale comme dans Le couteau coupe bien) ou les propositions interrogatives indirectes. En effet,
comment différencier Il se demande qui est passe’ en premier. de Le pre’sident nomme qui est le
plus me’ritant. de maniere automatique ?

La prise en compte de ces phénomenes par un systeme d’acquisition automatique est possible
mais nécessiterait d’introduire la notion de classe sémantique sur le verbe (verbe de type interro-
gation) pour acquérir les bonnes informations. Cet exemple montre bien les limites de l’acquisi-
tion automatique : lorsqu’on connait les verbes concernés par un phénomene précis, n’est-il pas
préférable de corriger la ressource a posteriori plutot que d’ajouter des informations a priori ?

Certains schémas de sous-catégorisation sont valides mais nécessiteraient d’étre accompagnés
d’informations sur les restrictions de sélection. Par exemple, le verbe toumer accepte un em-
ploi absolu (sans complément) comme dans Le vent tourne ou Le lait tourne mais cet emploi
impose des restrictions quanta la sémantique du nom en position sujet. Méme si les principales
tétes nominales apparaissant dans les différentes positions argumentales sont conservées par
ASSCI, aucun calcul n’est effectué a l’heure actuelle pour repérer les restrictions de selection
(on envisage de donner une approximation de ce degré de ﬁgement a partir des informations
enregistrées par le systeme). Dans le LG, chacune de ces expressions est considérée comme
ﬁgée et est répertoriée dans une table qui lui est propre (voir section 4.2).

Les informations disponibles dans un lexique acquis a partir de corpus ne sont pas aussi riches
que celles qu’on trouve dans une ressource comme le LG. En particulier, LeXSchem ne four-
nit aucune information sur la sémantique des arguments du verbe dans les schémas de sous-
catégorisation. Par exemple, dans le LG, le verbe savoir a un schéma NOhum V Vinf (le
sujet du verbe doit dans ce cas avoir le trait “humain”). Ce type d’information est totalement
absent d’un lexique acquis automatiquement comme LexSchem et il semble pour l’instant dif-
ﬁcile de l’ajouter sans intervention humaine. Toutefois, ce type d’information n’est pas sans
poser de probleme dans le cas de métonymies ou de métaphores.

4.2 Les limites des approches manuelles et du Lexique-Grammaire

La création des données syntaxiques du LG est entierement manuelle. Indépendamment de la
question de coﬁts humains et temporels, cette méthode recele quelques problemes. Nous en
discutons quelques uns :

— Il est relativement difﬁcile de maintenir la cohérence de classiﬁcation : par souci d’exhaus-
tivité, la stratégie de classiﬁcation était de conserver, plutot que d’exclure. Ainsi, un meme
verbe risque de se retrouver dans deux classes différentes. Par exemple, le verbe savoir qui

La complémentarité des approches manuelle et automatique en acquisition lexicale

sélectionne une complétive directe possede deux emplois : l’un classé dans la construction
NOhum V Que P = : Luc sait que Léa est a Paris, et l’autre classé dans la table 10, entrant
dans la construction NOhum V par N2hum Que P = : Luc a su par Max que Léa est a
Paris. S’agit-il vraiment d’homonymie ? Nous ne pouvons pas entrer dans le détail mais en
tout cas, pour l’acquisition automatique de valence, les deux emplois sont regroupés sous un
seul.

— La distinction en plusieurs emplois doit étre minutieusement controlée, sinon elle créera des
homonymes a profusion. Les criteres essentiels sont, par déﬁnition, formels, de telle sorte
que l’ensemble de propriétés vériﬁées par un verbe donné doit étre clairement distinct de
celui de son (ou ses) homonyme(s), si le verbe possede plusieurs emplois. Ce critere peut
étre difﬁcile a respecter s’il s’agit d’un simple verbe bi-valent, par exemple. Ainsi, pour le
verbe tres courant comme travailler, le LG dispose de 15 emplois différents dont quatre
appartiennent a la classe appelée 32R3 (classe de constructions transitives résiduelles). Les
quatre emplois du verbe travailler classés dans 32R3 sont tous distingués par différents objets
directs lexicaux :

(1) Max travaille la balle

(2) Max travaille son texte

(3) Max travaille cette discipline

(4) Max travaille Z ’0pini0n publique

Le sens du verbe travailler est certes différent dans chaque cas, mais les criteres syntaxiques
qui les différencient sont minces. Ce sont des exemples a la frontiere des expressions libres-
expressions ﬁgées. A propos de ce verbe, LexSchem donne simplement deux sous-catégorisations
possibles : SUJ : SN_OBJ : SN et SUJ : SN. 11 manque totalement d’eXemple transitif simple.
Cela étant, on peut se poser la question légitime de savoir si cette "ﬁnesse" de distinction en
plusieurs emplois d’une construction transitive est nécessaire pour les besoins du TAL ?

— Par ailleurs, les classes dites "résiduelles", marquées par R comme 32R3, ont tendance a
regrouper tous les exemples qui n’obéissent pas strictement a des criteres de classiﬁcation.
Elles servent un peu de "fourre-tout" du LG. Généralement les exemples accumulés dans
cette classe attendent d’étre reclassés dans d’autres classes. Seule l’amélioration continue du
LG allégera ce probleme.

4.3 La complémentarité des méthodes

Le plus souvent, les ressources construites a la main ne comprennent aucune information sur
la fréquence des schémas de sous-categorisation. Il est possible d’utiliser des méthodes d’ac-
quisition automatique pour acquérir ces informations statistiques sur la langue et inclure ces
informations dans des ressources manuelles.

Lors de la comparaison de LexSchem et du LG, nous avons remarqué que les schémas de
sous-categorisation recensés dans les deux ressources ne sont pas toujours les memes. Dans la
plupart des cas, ces schémas sont présents dans le LG et absents de LeXSchem. Par exemple,
dans LeXSchem, le verbe valider est assorti de trois schémas de sous-categorisation, dont deux
correspondent aux constructions respectivement du passif SUJ : SN_P—OBJ : SP [par+SN] ,
et du participe passé supporté par le verbe étre SUJ : SN. LeXSchem, ne pouvant pas différen-
cier de maniere automatique ce type de variantes transformationnelles des variantes formelles
signiﬁcatives pour la valence verbale, il produit systématiquement ces deux cadres de sous-

Cedric Messiant, Takuya Nakamura, Stavroula Voyatzi

categorisation pour tous les verbes veriﬁant ces constructions dans le corpus. En revanche, dans
le LG, le verbe valider est represente dans deux classes differentes qui sont deﬁnies par les
constructions type :

— Table6NO V Nl,avecNl=:Qu P + si P + N
= : L’entreprise va valider (que vos competences sont adaptees £1 son projet + si vos compe-
tences sont adaptees £1 son projet + vos competences)

— Table 32RA NO V Ni (E + de N2) , avec V = : rendre (E + plus) adjectif
= : Le tuteur a valide le certiﬁcat de stage d ’un tampon

Neanmoins, il arrive egalement qu’on detecte un schema de sous-categorisation valide (se-
lon les exemples extraits du corpus) dans LexSchem mais absent du LG. Par exemple, dans
LexSchem, le verbe toumer produit quatre schemas de sous-categorisation, dont le schema
SUJ: SN_P—OBJ: SP [autour de+SN] = : Le deﬁcit public tournera autour des 2% du
PIB. (le verbe toumer a ici le sens de se derouler, evoluer). Mais, cette construction est absente
du LG. L’utilisation conjointe des deux approches permettrait donc d’obtenir un lexique plus
complet.

Des etudes pour l’anglais ont montre que les schemas de sous-categorisation d’un verbe pou-
vaient varier lorsque ce verbe est employe dans une langue de specialite (Wattarujeekrit et al.,
2004). En observant le verbe ﬂuctuer, nous avons remarque que certains schemas de sous-
categorisation etaient presents dans LexSchem mais pas dans le LG. Ce schema est absent du
LG en raison de sa nature speciﬁque au domaine boursier. Une experience sur cinq verbes uti-
lises le plus souvent dans un cadre economique ou boursier dans le corpus LM10 (augmenter,
baisser, chuter, cléturer et ﬂuctuer) nous a permis de veriﬁer cette hypothese. Etant donne la
nature et l’objectif de cette tache, nous avons modiﬁe les parametres d’ASSCI aﬁn qu’il soit
plus “permissif” et qu’il fournisse un plus grand nombre de schemas de sous-categorisation
pour ces verbes. Ainsi, nous avons pu observer dans quelles proportions l’etude a partir de cor-
pus differe de la construction manuelle de ressources dans le cadre d’un domaine precis de la
langue.

Les verbes economiques exaInines ont ceci de particulier qu’ils expriment a peu pres tous la
variation en quantite ou en nombre. Les complements prepositionnels qui les accompagnent ne
sont pas des complements essentiels au sens strict du terme : pratiquement tous sont des adver-
biaux. Ils echappent, par consequent, a la grille de complements essentiels qui deﬁnit une struc-
ture de phrase du LG. Par consequent, la methode statistique qui puisse les detecter est com-
plementaire de la methode manuelle, du point de vue de leur detection. Il n’en reste pas moins
qu’il est difﬁcile de les valider d’emblee comme complements sous-categorises pour plusieurs
raisons. Ils realisent souvent des arguments semantiques et la correspondance entre forme et no-
tion semantique n’est pas bi-univoque : a un argument peut correspondre plusieurs formes. On
peut enumerer, a propos des verbes examines, des arguments semantiques comme “mesure rela-
tive”, “mesure absolue”, etc. Par exemple, un verbe foncierement intransitif comme augmenter
est souvent accompagne d’un complement de mesure relative adverbiale, mais ce dernier peut
prendre plusieurs formes, a part la realisation canonique sous forme de de DetNum\ % :

(1) La valeur X a augmente de 5%.

(2) La valeur X augmente £1 un rythme annuel de 4%.

de 5% et 61 un rythme annuel de 4% sont, plus ou moins, des variantes formelles de l’argument

La complémentarité des approches manuelle et automatique en acquisition lexicale

de mesure relative, la différence de formes n’est pas foncierement signiﬁcative du point de
vue de la valence du verbe augmenter. Pour le LexSchem, ils sont deux sous-catégorisations
différentes, cependant. L’ intervention manuelle pour regrouper ce genre de variantes formelles
semble nécessaire.

Cette étude devra étre approfondie, notamment par 1’ acquisition de schémas de sous-catégorisation
a partir d’un corpus de spécialité comme le droit ou la médecine. Cette ressource po11rra fournir
une base de travail a des linguistes qui pourront la valider, l’enrichir et l’afﬁner.

5 Conclusion

Nous avons montré que la construction manuelle de ressources lexicales et l’acquisition au-
tomatique de ressources sont deux approches complémentaires. Plutot que d’opposer ces ap-
proches, il peut étre bénéﬁque de travailler a un rapprochement des approches. Les ressources
manuelles peuvent servir a valider les lexiques acquis automatiquement et a leur apporter des
informations qu’il est difﬁcile d’acquérir a partir de corpus (par exemple, le role sémantique des
arguments, ...). Les ressources acquises a partir de corpus peuvent constituer une base de travail
pour la constitution de ressources pour un domaine de spécialité ou pour enrichir des lexiques
construits a la main avec des informations statistiques par exemple.

A la suite de cette étude, les perspectives sont multiples : déﬁnition d’une méthode pour fa-
ciliter l’utilisation conjointe des deux approches, par exemple par la Inise en place d’une “in-
terface d’enrichissement de ressource” ou harmonisation des formats utilisés. I-/Etant donné les
différences notables observées entre LexSchem et le Lexique-Grammaire pour des verbes du
domaine économique ou boursier, nous envisagons d’acquérir automatiquement une ressource
a partir d’un corpus de spécialité.

Références

ABEILLE A., CLEMENT L. & TOUSSENEL F. (2003). Building a Treebank for French, In
A. ABEILLE, Ed., Treebanks .' Building and Using Parsed Corpora, p. 165-187. Kluwer
Academic Publishers : Dordrecht.

BOURIGAULT D., JACQUES M.—P., FABRE C., FREROT C. & OZDOWSKA S. (2005). Syntex,
analyseur syntaxique de corpus. In Actes des 12emes journe’es sur le Traitement Automatique
des Langues Naturelles, Dourdan.

BRENT M. R. (1993). From grammar to lexicon : Unsupervised learning of lexical syntax.
Computational Linguistics, 19, 203-222.

BRISCOE T. & CARROLL J . (1997). Automatic extraction of subcategorization from corpora.

In Proceedings of the 5th ACL Conference on Applied Natural Language Processing, p. 356-
363, Washington, DC.

CHESLEY P. & SALMON-ALT S. (2006). Automatic extraction of subcategorization frames
for french. In Language Resources and Evaluation Conference (LREC), Genua (Italy).

DENDIEN J. & PIERREL J .-M. (2003). Le trésor de la langue francaise informatisé : un

exemple d’informatisation d’un dictionnaire de langue de référence. Traitement Automatique
des Langues, 44 (2).

Cédric Messiant, Takuya Nakamura, Stavroula Voyatzi

DINO IENCO S. V. & BOSCO C. (2008). Automatic extraction of subcategorization frames
for italian. In E. L. R. A. (ELRA), Ed., Proceedings of the Sixth International Language
Resources and Evaluation (LREC’08), Marrakech, Morocco.

DUBOIS J. & DUBOIS-CHARLIER F. (1997). Les verbesfrancais. Paris : Larousse.
GARDENT C., GUILLAUME B., PERRIER G. & FALK I. (2006). Extraction d’information
de sous-catégorisation a partir des tables du ladl. In Actes de Traitement Automatique des
Langues Naturelles, Louvain, Belgique.

GROSS M. (1975). Méthodes en syntaxe. Paris : Hermann.

KORHONEN A., GORRELL G. & MCCARTHY D. (2000). Statistical ﬁltering and subcatego-
rization frame acquisition. In Proceedings of the Conference on Empirical Methods in Natural
Language Processing and Very Large Corpora, Hong Kong.

KUPSC A. (2007). Extraction automatique de cadres de sous-catégorisation verbale pour le
francais a partir d’un corpus arboré. In TALN 2007, Toulouse.

MANNING C. D. (1993). Automatic acquisition of a large subcategorization dictionary from
corpora. In Meeting of the Association for Computational Linguistics, p. 235-242.

MEL’CUK I. & POLGUERE A. (2006). Dérivations sémantiques et collocations dans le
DiCo/LAF. Languefrancaise, 150, 66-83.

MESSIANT C. (2008). A subcategorization acquisition system for French verbs. In Procee-
dings of the ACL-08 .' HLT Student Research Workshop, p. 55-60, Columbus, Ohio : Associa-
tion for Computational Linguistics.

MESSIANT C., KORHONEN A. & POIBEAU T. (2008). Lexschem : A large subcategoriza-
tion lexicon for french Verbs. In Language Resources and Evaluation Conference (LREC),
Marrakech.

PREISS J ., BRISCOE T. & KORHONEN A. (2007). A system for large-scale acquisition of
verbal, nominal and adjectival subcategorization frames from corpora. In Meeting of the As-
sociation for Computational Linguistics, p. 912-918, Prague.

SAGOT B., CLEMENT L., DE LA CLERGERIE E. & BOULLIER P. (2006). The Lefff 2 syn-
tactic lexicon for French : architecture, acquisition, use. In Proceedings of the Language
Resources and Evaluation Conference (LREC), Genua (Italy).

SALKOFF M. & VALLI A. (2005). A dictionary of french verbal complementation. In Actes de
Language and Technology Conference. Human Language and Technologies as a Challenge for
Computer Science and Linguistics. In memory of M. Gross and A. Zampolli, Poznan, Poland.
SCHULTE IM WALDE S. (2002). A Subcategorisation Lexicon for German Verbs induced
from a Lexicalised PCFG. In Proceedings of the 3rd Conference on Language Resources and
Evaluation, volume IV, p. 1351-1357, Las Palmas de Gran Canaria, Spain.

VAN DEN EYNDE K. & BLANCHE-BENVENISTE C. (1978). Syntaxe et mécanismes descrip-
tifs : présentation de l’approche pronominale. Cahiers de Lexicologie, 32, 3-27.

VAN DEN EYNDE K. & MERTENS P. (2006). Le dictionnaire de valence Dicovalence .' manuel
d ’utilisation. Leuven : Manuscript.

VANRULLEN T., BLACHE P., PORTES C., RAUZY S., MAEYHIEUX J GUENoT M.—L.,
J EAN-MARIE-BALFOURIER & BELLENGIER E. (2005). Une plateforme pour l’acquisition,
la maintenance et la validation de ressources lexicales. In Actes des 12emes journe’es sur le
Traitement Automatique des Langues Naturelles, Dourdan.

WATTARUJEEKRIT T., SHAH P. K. & COLLIER N. (2004). Pasbio : predicate-argument
structures for event extraction in molecular biology. BM C Bioinformatics, 5.

