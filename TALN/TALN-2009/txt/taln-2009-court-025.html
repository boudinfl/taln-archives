<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
"http://www.w3.org/TR/html4/loose.dtd">
<html><head><title>Nouvelles consid&#233;rations pour la d&#233;tection de r&#233;utilisation de texte</title>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
</head>
<body>
<div style="page-break-before:always; page-break-after:always"><div><p>TALN 2009 &#8211; Session posters, Senlis, 24&#8211;26 juin 2009
</p>
<p>Nouvelles consid&#233;rations
pour la d&#233;tection de r&#233;utilisation de texte
</p>
<p>Fabien Poulard, Stergos Afantenos et Nicolas Hernandez
LINA (CNRS - UMR 6241)
</p>
<p>2 rue de la Houssini&#232;re &#8211; B.P. 92208, 44322 NANTES Cedex 3
{prenom.nom}@univ-nantes.fr
</p>
<p>R&#233;sum&#233;. Dans cet article nous nous int&#233;ressons au probl&#232;me de la d&#233;tection de r&#233;utilisa-
tion de texte. Plus particuli&#232;rement, &#233;tant donn&#233; un document original et un ensemble de do-
cuments candidats &#8212; th&#233;matiquement similaires au premier &#8212; nous cherchons &#224; classer ceux
qui sont d&#233;riv&#233;s du document original et ceux qui ne le sont pas. Nous abordons le probl&#232;me
selon deux approches : dans la premi&#232;re, nous nous int&#233;ressons aux similarit&#233;s discursives entre
les documents, dans la seconde au recouvrement de n-grams hapax. Nous pr&#233;sentons le r&#233;sul-
tat d&#8217;exp&#233;rimentations men&#233;es sur un corpus de presse francophone construit dans le cadre du
projet ANR PIITHIE.
</p>
<p>Abstract. In this article we are interested in the problem of text reuse. More specifically,
given an original document and a set of candidate documents &#8212; which are thematically similar
to the first one &#8212; we are interested in classifying them into those that have been derived from
the original document and those that are not. We are approaching the problem in two ways :
firstly we are interested in the discourse similarities between the documents, and secondly we
are interested in the overlap of n-grams that are hapax. We are presenting the results of the
experiments that we have performed on a corpus constituted from articles of the French press
which has been created in the context of the PIITHIE project funded by the French National
Agency for Research (Agence National de la Recherche, ANR).
</p>
<p>Mots-cl&#233;s : r&#233;utilisation de texte, recouvrement de n-grams hapax, similarit&#233;s discur-
sives, corpus journalistique francophone.
</p>
<p>Keywords: text reuse, hapax n-grams overlap, discourse similarities, french journalistic
corpus.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Fabien Poulard &amp; Stergos Afantenos &amp; Nicolas Hernandez
</p>
<p>1 Introduction
</p>
<p>&#171; La r&#233;utilisation de texte est l&#8217;activit&#233; par laquelle des textes &#233;crits pr&#233;-existants sont r&#233;utilis&#233;s
pour cr&#233;er de nouveaux textes ou versions [. . .] il y a r&#233;utilisation quand il y a une r&#233;alisation
consciente d&#8217;une transformation d&#8217;un texte pour en arriver &#224; un autre &#187; (Clough &amp; Gaizaus-
kas, 2008). La duplication (copie &#224; l&#8217;identique), la r&#233;vision, l&#8217;adaptation de genre, le r&#233;sum&#233;, la
traduction, la citation,. . . sont autant de formes diff&#233;rentes de r&#233;utilisation d&#8217;un texte original.
Les chercheurs comme les industriels sont conscients depuis de nombreuses ann&#233;es de l&#8217;int&#233;r&#234;t
d&#8217;&#233;tudier cette activit&#233; qui correspond &#224; des enjeux applicatifs r&#233;els : la d&#233;tection de documents
dupliqu&#233;s sur le web a des cons&#233;quences sur l&#8217;efficacit&#233; des moteurs du recherche aussi bien
pour leur traitement (co&#251;t d&#8217;indexation et de stockage) que sur la pr&#233;cision des r&#233;ponses rame-
n&#233;es. La d&#233;tection de plagiats pr&#233;sente aussi un grand int&#233;r&#234;t pour le respect du droit d&#8217;auteur
que cela concerne le code source de logiciels ou tout document servant &quot;de base&quot; &#224; des devoirs
d&#8217;&#233;tudiants par exemples. Le suivi d&#8217;impact d&#8217;une communication sur un produit ou sur une
information rendue publique pr&#233;sente aussi des int&#233;r&#234;ts commerciaux et scientifiques dans une
perspective de veille.
</p>
<p>En pratique, les syst&#232;mes de d&#233;tection de r&#233;utilisation de texte proc&#232;dent selon trois &#233;tapes :
</p>
<p>1. d&#8217;abord ils s&#233;lectionnent des types d&#8217;unit&#233;s textuelles &#224; observer (mot, syntagme, phrase,
paragraphe, document, n-gram avec/sans recouvrement) dans les documents manipul&#233;s ;
</p>
<p>2. ensuite ils construisent un mod&#232;le de chaque document par normalisation linguistique
(lexicale, syntaxique, s&#233;mantique) ou num&#233;rique (condensation par algorithme de ha-
shage) et par filtrage (les mots pleins, un n-gram donn&#233;, les n-premiers rencontr&#233;s dans
le texte, pond&#233;r&#233;s par tf.idf , . . .) des observables ;
</p>
<p>3. enfin ils comparent effectivement les documents sur la base de ces repr&#233;sentations.
</p>
<p>Le choix de la repr&#233;sentation est bien entendu d&#233;pendant de la m&#233;thode de comparaison utilis&#233;e.
Celles-ci varient suivant diff&#233;rents co&#251;ts de traitement : de mesures de similarit&#233;s rencontr&#233;es en
Classification ou en Recherche d&#8217;Information (RI) (ratio des mat&#233;riaux partag&#233;s, distance vec-
torielle) aux comparaisons plus complexes et sp&#233;cifiques (plus longues sous cha&#238;nes communes,
distance d&#8217;&#233;dition) (Uzuner et al., 2004; Metzler et al., 2005; Seo &amp; Croft, 2008; Bendersky &amp;
Croft, 2009; Clough &amp; Gaizauskas, 2008).
</p>
<p>Les diff&#233;rentes &#233;tapes de cette proc&#233;dure sont suj&#232;tes &#224; de nombreux enjeux techniques : com-
ment choisir les unit&#233;s textuelles les plus repr&#233;sentatives du contenu du document ? Les moins
co&#251;teuses &#224; extraire (en terme de ressources requises, de m&#233;thodes &#224; mettre en place, de temps
de calcul) ? Les plus caract&#233;ristiques des ph&#233;nom&#232;nes de r&#233;utilisation ? Quelles sont les m&#233;-
thodes les plus ad&#233;quates en termes de pr&#233;cision et de temps de traitement pour d&#233;tecter une
forme donn&#233;e de r&#233;utilisation ?. . .
</p>
<p>Le contexte applicatif du pr&#233;sent article est celui de la d&#233;tection de r&#233;utilisations &#224; partir d&#8217;un
&#233;crit original, dans des textes journalistiques francophones pr&#233;sentant des similarit&#233;s th&#233;ma-
tiques avec le document source1. En particulier notre t&#226;che consistait &#224; classer les documents
candidats comme &#233;tant des r&#233;utilisations ou non d&#8217;un document original connu. La figure 1
fournit un exemple issu de notre corpus d&#8217;un texte original, d&#8217;une r&#233;utilisation de celui-ci et
d&#8217;une similarit&#233; th&#233;matique (cas de non r&#233;utilisation).
</p>
<p>Dans cette article, nous proposons de nouvelles consid&#233;rations th&#233;oriques afin de mieux cadrer
1Ce travail a b&#233;n&#233;fici&#233; du soutien de l&#8217;Agence Nationale de la Recherche, projet PIITHIE (www.piithie.com)
</p>
<p>portant la r&#233;f&#233;rence 2006 TLOG 013 03.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Nouvelles consid&#233;rations pour la d&#233;tection de r&#233;utilisation de texte
</p>
<p>Exemple (1) Texte original :
Le groupe Carrefour va prochainement se lancer dans la VOD en France mais pas
seulement. Fier de ses parts de march&#233; dans la vente de DVD dans d&#8217;autres pays
d&#8217;europe (autour de 13%), Carrefour va aussi ouvrir son service de Vid&#233;o &#224; la De-
mande en Espagne, Italie et Belgique.
Exemple (2) R&#233;utilisation :
Le g&#233;ant national de la grande distribution fran&#231;aise lancera une offre de VOD,
vid&#233;o &#224; la demande en France, Belgique, Italie et Espagne, o&#249; par-ailleurs il d&#233;tient
une part de march&#233; de 13,3% dans la vente de DVD.
Exemple (3) Similarit&#233; th&#233;matique :
Le 8 novembre 2006 en partenariat avec Orange, Carrefour lancera son offre de
t&#233;l&#233;phonie mobile : Carrefour Mobile. Le groupe de distribution devient ainsi op&#233;-
rateur virtuel de t&#233;l&#233;phonie mobile (MVNO) avec les m&#234;mes ambitions que son
concurrent direct Auchan.
</p>
<p>FIG. 1 &#8211; Classification de documents candidats comme &#233;tant des r&#233;utilisations ou non d&#8217;un
document original connu
</p>
<p>le probl&#232;me de la d&#233;tection de r&#233;utilisation de textes. Nous introduisons notamment la notion
de singularit&#233;s d&#8217;un document vis-&#224;-vis d&#8217;une collection. Nous avan&#231;ons que cette consid&#233;ra-
tion permet de s&#233;lectionner plus finement les unit&#233;s textuelles repr&#233;sentant un document. Cela
offre de nouvelles perspectives telles que : l&#8217;indexation des documents du web sans filtrage des
r&#233;utilisations par post-traitement, l&#8217;exploitation des moteurs de recherche traditionnels pour la
recherche directe de r&#233;utilisations, la r&#233;cup&#233;ration de cas de r&#233;utilisations avec transformations
importantes, la distinction des documents th&#233;matiquement similaires de ceux qui constituent
effectivement des r&#233;utilisations.
</p>
<p>L&#8217;&#233;tude de cette propri&#233;t&#233; est envisag&#233;e autour de deux exp&#233;riences de d&#233;tection de r&#233;utilisation
de texte. Celles-ci utilisent des unit&#233;s textuelles jusqu&#8217;alors non consid&#233;r&#233;es dans la litt&#233;rature
pour repr&#233;senter les documents : des marques discursives et des n-grams hapax2. Nous ob-
servons leur capacit&#233; de d&#233;tection de r&#233;utilisations de par leur singularit&#233;. En pratique, elles
seront utilis&#233;es en compl&#233;ment avec d&#8217;autres observables tels que des termes s&#233;lectionn&#233;s sur
leur tf.idf afin de capturer par les similarit&#233;s th&#233;matiques inter-documents.
</p>
<p>1.1 Cadre th&#233;orique et terminologie
</p>
<p>Les caract&#233;ristiques d&#8217;une r&#233;utilisation de texte doivent remplir deux t&#226;ches : d&#8217;une part d&#233;finir
si un document candidat est une r&#233;utilisation et d&#8217;autre part d&#233;terminer de quel document il est
une r&#233;utilisation. Par caract&#233;ristiques, nous entendons toute marque linguistique telle que les
termes, la syntaxe des phrases, ou des marques de plus haut niveau telles que les r&#233;f&#233;rences &#224;
des entit&#233;s ou l&#8217;organisation des id&#233;es.
</p>
<p>La premi&#232;re t&#226;che n&#233;cessite de consid&#233;rer les caract&#233;ristiques que l&#8217;on retrouve &#224; la fois dans
le document d&#233;riv&#233; et dans les documents originaux, nous les appelons : les invariants. Il n&#8217;est
pas possible de s&#233;lectionner les invariants pour un document original si on n&#8217;a pas d&#233;fini &#224; quel
document candidat on le comparait. Nous parlerons de classes d&#8217;invariants pour d&#233;signer des
</p>
<p>2Hapax signifie &#171; qui a &#233;t&#233; dit qu&#8217;une fois &#187;.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Fabien Poulard &amp; Stergos Afantenos &amp; Nicolas Hernandez
</p>
<p>invariants de diff&#233;rentes natures linguistiques, sans nous limiter aux formes de surface. Ainsi,
une r&#233;f&#233;rence &#224; une m&#234;me entit&#233; entre deux documents &#224; l&#8217;aide de formes de surfaces diff&#233;rentes
est consid&#233;r&#233;e comme un invariant ; l&#8217;invariant &#233;tant la r&#233;f&#233;rence &#224; ladite entit&#233;, et non la forme
employ&#233;e pour la d&#233;nomm&#233;e.
</p>
<p>La seconde t&#226;che n&#233;cessite de consid&#233;rer les caract&#233;ristiques pr&#233;sentes uniquement dans le do-
cument et absentes de la collection homog&#232;ne &#224; laquelle celui-ci appartient, nous les appelons :
les singularit&#233;s. Cette collection homog&#232;ne se d&#233;finit par des crit&#232;res sp&#233;cifiques s&#233;lectionn&#233;s
selon la finalit&#233; d&#233;sir&#233;e. Il n&#8217;est pas possible de calculer les singularit&#233;s d&#8217;un document sans
avoir auparavant d&#233;fini la collection de documents dans laquelle il s&#8217;inscrivait. Les singularit&#233;s
peuvent prendre des formes aussi diverses que les invariants et de la m&#234;me fa&#231;on nous parlerons
de classes de singularit&#233;s pour d&#233;signer les singularit&#233;s de diff&#233;rentes natures linguistiques.
</p>
<p>Une combinaison de marques peut correspondre &#224; une singularit&#233; ou un invariant m&#234;me si ce
n&#8217;est pas le cas des marques prises individuellement. Nous essayons d&#8217;estimer dans cet article
dans quelle proportion certaines classes de singularit&#233;s sont g&#233;n&#233;ralement invariantes et per-
mettent donc d&#8217;identifier des r&#233;utilisations.
</p>
<p>2 Etat de l&#8217;art
</p>
<p>Notre &#233;nonciation du principe de singularit&#233; est appuy&#233;e par divers travaux de la litt&#233;rature.
</p>
<p>La prise en compte de l&#8217;importance d&#8217;un terme dans une collection est un principe consid&#233;r&#233;
d&#232;s les premiers travaux en RI (Jones, 1972; Salton &amp; Buckley, 1988). Ainsi la bien connue
inverse de la fr&#233;quence des documents idf est un facteur utilis&#233; avec la fr&#233;quence des termes
d&#8217;un document tf pour relativiser cette derni&#232;re mais aussi pour favoriser les termes pr&#233;sents
dans peu de document d&#8217;une collection. Elle se calcule en g&#233;n&#233;rale en prenant le logarithme3
</p>
<p>du ratio du nombre de documents de la collection sur le nombre de documents distincts dans
lesquels on retrouve un terme donn&#233;.
</p>
<p>Pour la t&#226;che de reconnaissance du style d&#8217;un auteur, (van Halteren, 2004) montre que le
&#171; comptage de [combinaison de] traits linguistiques d&#8217;un texte normalis&#233; par sa longueur et
leur d&#233;viation par rapport &#224; la moyenne observ&#233;e sur un corpus de r&#233;f&#233;rence &#187; permet d&#8217;obtenir
de meilleurs r&#233;sultats que les m&#233;thodes fond&#233;es sur des analyses en composante principale, des
analyses discriminantes lin&#233;aires, ou des distributions probabilistes.
</p>
<p>Afin de mesurer l&#8217;appartenance d&#8217;un terme au genre d&#8217;une collection dans laquelle il appara&#238;t,
(Hernandez, 2004) a utilis&#233; avec succ&#232;s une des composante de l&#8217;idf , le nombre de documents
distincts dans lesquels on retrouve un terme, pour filtrer les termes des documents.
</p>
<p>Dans le contexte de la d&#233;tection de r&#233;utilisation le long d&#8217;un spectre de degr&#233;s de similarit&#233;s,
(Metzler et al., 2005) observent qu&#8217;une mesure fond&#233;e sur le recoupement de mots pond&#233;r&#233;s
par idf est l&#8217;une des deux m&#233;thodes les plus performantes pour la d&#233;tection de r&#233;utilisation avec
fortes transformations (reprise partielle des faits). Suivant les m&#233;thodes consid&#233;r&#233;s, leurs taux
de pr&#233;cision varie de 40&#8211;60% lorsque la similarit&#233; traduit un lien th&#233;matique ou qu&#8217;il s&#8217;agit
d&#8217;une reprise partielle de faits, &#224; 80&#8211;100% pour les reprises identit&#233;s.
</p>
<p>3Car le simple ratio donne des valeurs tr&#232;s grandes.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Nouvelles consid&#233;rations pour la d&#233;tection de r&#233;utilisation de texte
</p>
<p>3 M&#233;thodes de d&#233;tection de r&#233;utilisation
</p>
<p>Nous pr&#233;sentons ci-apr&#232;s deux m&#233;thodes se fondant sur le cadre th&#233;orique que nous avons d&#233;fini.
La premi&#232;re utilise des marques discursives et la seconde des hapax.
</p>
<p>3.1 &#192; l&#8217;aide de marques discursives singuli&#232;res
</p>
<p>Nous d&#233;crivons dans cette section une approche bas&#233;e sur la mod&#233;lisation des documents par
une repr&#233;sentation partielle de leur structure discursive. Nous pensons que cette structure est
suffisamment singuli&#232;re (cf. section 1.1) pour permettre de d&#233;tecter automatiquement les re-
prises globales de documents. Nous d&#233;taillons ci-dessous la r&#233;flexion qui nous a men&#233; &#224; cette
hypoth&#232;se puis nous en pr&#233;sentons une mod&#233;lisation afin de mener nos exp&#233;rimentations.
</p>
<p>La restructuration discursive globale d&#8217;un document est une t&#226;che difficile qui n&#233;cessite une
r&#233;&#233;criture partielle de ce dernier et une r&#233;organisation des id&#233;es. Dans le cadre des articles
de presse, il s&#8217;agit de retravailler la structure argumentative ou descriptive de l&#8217;article original.
Nous choisissons de nous int&#233;resser aux &#233;l&#233;ments de structuration discursifs car nous supposons
qu&#8217;ils varient peu lors d&#8217;une reprise globale d&#8217;un document.
</p>
<p>La structure discursive est complexe &#224; extraire et &#224; caract&#233;riser, except&#233; lorsqu&#8217;elle est marqu&#233;e
par des connecteurs discursifs non ambigus (Sporleder &amp; Lascarides, 2008). Nous choisissons
alors plus particuli&#232;rement de travailler &#224; partir des connecteurs discursifs et de ne consid&#233;-
rer uniquement la structure discursive des documents rendue visible par ces connecteurs. Nous
sommes tout &#224; fait conscient que ces derniers ne sont que la surface &#233;merg&#233;e de l&#8217;iceberg du
discours. Toutefois, ce racourci nous permet d&#8217;exp&#233;rimenter l&#8217;id&#233;e de la d&#233;tection automatique
de reprise par comparaison des structures discursives sans n&#233;cessiter d&#8217;appliquer des m&#233;thodes
complexes &#224; mettre en &#339;uvre. En effet, les connecteurs discursifs appartiennent &#224; une classe
lexicale ais&#233;ment observable par des techniques automatiques, peu ambigue (Sporleder &amp; Las-
carides, 2008) et dont l&#8217;utilisation peu fr&#233;quente dans les documents est en accord avec le prin-
cipe de singularit&#233; &#233;nonc&#233; pr&#233;c&#233;demment.
</p>
<p>Nous choisissons de repr&#233;senter la structuve discursive sans tenir compte de l&#8217;ordre d&#8217;appari-
tion desdits connecteurs et en consid&#233;rant ces derniers dans leur forme lexicale observ&#233;e. Nous
sommes conscients de la na&#239;vet&#233; de cette approche, la division en classe s&#233;mantique, la position
relative ou absolue et la s&#233;quence d&#8217;apparition sont des &#233;l&#233;ments &#224; prendre en compte. Cepen-
dant, dans le cadre d&#8217;une reprise globale du document, notre approche fonctionne assez bien
comme le montre les r&#233;sultats de la section 4.3 et la simplicit&#233; de l&#8217;approche est un atout pour
sa mise en &#339;uvre.
</p>
<p>Chaque document est mod&#233;lis&#233; par un vecteur du nombre absolu des occurrences de connecteurs
discursifs. Ces connecteurs recherch&#233;s sont au nombre de 90 et ont &#233;t&#233; collect&#233;s ou traduits de
la litt&#233;rature (Knott, 1996; Marcu, 1997) dans (Hernandez, 2004). Il s&#8217;agit principalement de
connecteurs (conjonctions, pr&#233;positions, adverbes et locutions conjonctives, pr&#233;positionnelles
et adverbiales) tels que premi&#232;rement, ensuite, dans un premier temps, tout d&#8217;abord, . . .
</p>
<p>Nous comparons ensuite le vecteur d&#8217;un document consid&#233;r&#233; original et celui d&#8217;un document
candidat &#224; l&#8217;aide d&#8217;une mesure de similarit&#233; cosinus. Les valeurs varient de 0 &#224; 1 o&#249; 0 signifie
que les deux vecteurs sont ind&#233;pendants et 1 signifie que les vecteurs sont identiques. Nous
choisissons de retourner 0 lorsqu&#8217;aucun connecteur n&#8217;appara&#238;t dans un des deux documents, ce</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Fabien Poulard &amp; Stergos Afantenos &amp; Nicolas Hernandez
</p>
<p>qui n&#8217;est pas enti&#232;rement satisfaisant &#233;tant donn&#233; que 22% des documents de notre corpus ont
cette caract&#233;ristique.
</p>
<p>En r&#233;sum&#233;, nous cherchons &#224; d&#233;finir si une r&#233;utilisation r&#233;uni deux documents. Nous comparons
pour cela les structures discursives rendues visibles par les connecteurs de ces documents. Nous
exp&#233;rimentons cette m&#233;thode dans la section 4.3.
</p>
<p>3.2 &#192; l&#8217;aide de marques lexicales singuli&#232;res
</p>
<p>Nous posons ici l&#8217;hypoth&#232;se que des hapax ont un pouvoir discriminant pour la d&#233;tection de
r&#233;utilisation. Nous d&#233;crivons dans cette section notre processus pour obtenir des hapax ainsi
que notre technique de comparaison de documents pour d&#233;tecter des r&#233;utilisations &#224; partir de
ces hapax.
</p>
<p>Dans le cadre de ce travail, nous avons choisi d&#8217;observer les n-grams hapax comme instance
des hapax. Pour ce faire nous avons produit des unigrams, bigrams, trigrams, ..., sept-grams &#224;
partir de notre corpus en filtrant les mots vides. Nous avons ensuite cherch&#233; &#224; identifier les n-
grams hapax de chaque document en sommant pour chaque n-gram son nombre d&#8217;occurrences
dans le document et son nombre d&#8217;occurrences dans les documents des autres r&#233;pertoires de
notre corpus ; un r&#233;pertoire r&#233;unie un document source original et un ensemble de documents
candidats d&#233;riv&#233;s ou non du document source. Seuls les n-grams qui apparaissent une seul fois
ont &#233;t&#233; conserv&#233;s (hapax).
</p>
<p>En ce qui concerne la m&#233;thode de comparaison nous avons abord&#233; le probl&#232;me comme une
t&#226;che de classification binaire o&#249; il s&#8217;agissait de classer un document candidat en document
d&#233;riv&#233; ou en document non-d&#233;riv&#233;. Pour chaque paire de documents original et candidat, nous
avons constitu&#233; un vecteur de deux attributs : le nombre de hapax en commun et le nombre des
hapax distincts.
</p>
<p>4 Exp&#233;rimentations
</p>
<p>Dans cette section, nous d&#233;crivons le corpus utilis&#233; pour nos exp&#233;rimentations lesquelles sont
pr&#233;sent&#233;es &#224; la suite.
</p>
<p>4.1 Construction et composition du corpus PIITHIE
</p>
<p>Le corpus utilis&#233; lors des exp&#233;rimentations a &#233;t&#233; construit dans le cadre du projet ANR PII-
THIE4. Nous pr&#233;sentons ci-dessous sa construction et sa composition.
</p>
<p>Dans un premier temps, des documents r&#233;cents consid&#233;r&#233;s comme originaux ont &#233;t&#233; manuel-
lement s&#233;lectionn&#233;s sur des sites de presse en ligne. Un article &#233;tait consid&#233;r&#233; comme original
s&#8217;il avait pour source une agence de presse (AFP, REUTERS). Dans une p&#233;riode post&#233;rieure
imm&#233;diate, des documents d&#233;riv&#233;s candidats ont &#233;t&#233; r&#233;cup&#233;r&#233;s &#224; l&#8217;aide de moteurs de recherche
sur des sites web s&#233;lectionn&#233;s. La s&#233;lection des sites de recherche visait &#224; garantir une homog&#233;-
n&#233;it&#233; de genre des documents ramen&#233;s. Les requ&#234;tes des moteurs &#233;taient produites &#224; partir des
</p>
<p>4Le corpus produit sera prochainement distribu&#233; sur http://www.piithie.com.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Nouvelles consid&#233;rations pour la d&#233;tection de r&#233;utilisation de texte
</p>
<p>Classifieur Classe Pr&#233;cision Rappel F-mesure
</p>
<p>App. R&#233;f&#233;rence DR 0.86 0.44 0.58
DR&#772; 0.48 0.88 0.62
</p>
<p>Connecteurs DR 0.94 0.56 0.70
DR&#772; 0.56 0.94 0.70
</p>
<p>Hapax DR 0.923 0.895 0.909
DR&#772; 0.83 0.873 0.851
</p>
<p>TAB. 1 &#8211; Comparaison des r&#233;sultats des diff&#233;rentes exp&#233;rimentations
</p>
<p>cinq mots les plus &#171; rares &#187; des documents originaux. Le degr&#233; de raret&#233; d&#8217;un mot &#233;tait calcul&#233;
a priori sur la base du nombre de documents qu&#8217;une requ&#234;te avec ce mot ramenait dans les mo-
teurs de recherche utilis&#233;s. Les documents r&#233;cup&#233;r&#233;s ont ensuite &#233;t&#233; annot&#233;s manuellement par
deux annotateurs afin de classer les documents comme d&#233;riv&#233;s ou non-d&#233;riv&#233;s. Un document
&#233;tait consid&#233;r&#233; comme d&#233;riv&#233; s&#8217;il reprenait les &#233;v&#232;nements d&#233;crits dans le document source et
s&#8217;il pr&#233;sentait des sous-cha&#238;nes de mots communes.
</p>
<p>Au total, le corpus se compose de 77 documents originaux, 496 documents d&#233;riv&#233;s et 293 do-
cuments non-d&#233;riv&#233;s. En pratique, le corpus est divis&#233; en 77 r&#233;pertoires contenant chacun un
document original et un ensemble de documents candidats (d&#233;riv&#233;s et non-d&#233;riv&#233;s).
</p>
<p>4.2 Approche de r&#233;f&#233;rence
</p>
<p>Dans cette section, nous d&#233;crivons une m&#233;thode dont nous utiliserons les r&#233;sultats comme r&#233;-
f&#233;rence pour nos exp&#233;rimentations. Cette m&#233;thode na&#239;ve reprend les principes de la m&#233;thode
discursive pr&#233;sent&#233;e &#224; la section 3.1, la diff&#233;rence r&#233;side dans le choix des observables. En ef-
fet, nous n&#8217;utilisons pas ici la structure discursive mais un sous-ensemble du lexique du corpus
pour caract&#233;riser les documents.
</p>
<p>Nous utilisons un lexique de 90 mots extraits al&#233;atoirement du corpus et comparable en taille
avec la signature discursive, en filtrant les mots outils &#224; l&#8217;aide d&#8217;une heuristique. Chaque docu-
ment est alors caract&#233;ris&#233; par le nombre d&#8217;occurrences des &#233;l&#233;m&#233;nts de ce lexique qui y appara&#238;t.
Les vecteurs obtenus sont alors compar&#233;s &#224; l&#8217;aide d&#8217;une mesure cosinus et l&#8217;on classe comme
repris les documents qui pr&#233;sentent une valeur sup&#233;rieure &#224; un seuil fix&#233;. Inversement les do-
cuments obtenant une valeur inf&#233;rieure sont consid&#233;r&#233;s comme des non-reprises. Le seuil a &#233;t&#233;
s&#233;lectionn&#233; de mani&#232;re &#224; maximiser la pr&#233;cision et le rappel de cette m&#233;thode sur le corpus.
</p>
<p>Le tableau 1 expose les r&#233;sultats de l&#8217;approche de r&#233;f&#233;rence pour la classe des documents repris
(DR) et celle des documents non-repris (DR&#772;). Outre l&#8217;obtention de r&#233;sultats pour comparaison,
la similarit&#233; de cette approche avec la m&#233;thode discursive permet de tester le r&#244;le particulier des
connecteurs dans la section suivante.
</p>
<p>4.3 Exp&#233;rimentation sur les connecteurs discursifs
</p>
<p>Nous exp&#233;rimentons dans cette section la d&#233;tection automatique de reprise sur le corpus pr&#233;-
sent&#233; &#224; la section 4.1 par la m&#233;thode discursive d&#233;crite &#224; la section 3.1.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Fabien Poulard &amp; Stergos Afantenos &amp; Nicolas Hernandez
</p>
<p>Nous avons observ&#233; au pr&#233;alable que les formes graphiques des connecteurs de notre diction-
naire sont pr&#233;sentes dans environ 80% des documents. Certaines de ces formes sont ambigues
mais nous n&#8217;en tenons pas compte (cf. section 3.1). Nous avons consid&#233;r&#233; chaque document
original du corpus Piithie, et pour chacun de ces documents nous avons calcul&#233; le cosinus de
son vecteur et celui de chacun des documents candidats associ&#233;s.
</p>
<p>Nous avons compar&#233; les valeurs du cosinus entre les originaux et les candidats en diff&#233;renciant
les d&#233;riv&#233;s des non-d&#233;riv&#233;s. Nous observons que ces valeurs pour les non-d&#233;riv&#233;s sont r&#233;parties
de mani&#232;re assez homog&#232;ne sur tout l&#8217;espace image alors qu&#8217;elles se densifient autour de 1 (vec-
teurs identiques) pour les d&#233;riv&#233;s. Cette observation supporte notre hypoth&#232;se que les structures
discursives sont proches entre les documents originaux et les d&#233;riv&#233;s et qu&#8217;il s&#8217;agit donc d&#8217;un
invariant potentiel.
</p>
<p>&#201;tant donn&#233; un document original et sa mod&#233;lisation selon notre m&#233;thode, nous classons comme
repris un document candidat dont le cosinus appliqu&#233; entre son mod&#232;le et celui de l&#8217;original est
sup&#233;rieur &#224; un seuil fix&#233;. Par opposition, les documents pour lesquels le cosinus est inf&#233;rieur au
m&#234;me seuil sont consid&#233;r&#233;s comme des non-d&#233;riv&#233;s. Nous avons choisi le seuil de 0.8 car c&#8217;est
la valeur du cosinus qui maximise la pr&#233;cision du classifieur sur le corpus. Nous avons d&#233;fini
cette valeur en faisant varier le cosinus par dizi&#232;me. Le classifieur obtient alors une pr&#233;cision
de 94% et un rappel de 56% comme le montre le tableau 1.
</p>
<p>La caract&#233;risation des documents par un vecteur d&#8217;occurrence des connecteurs donne des r&#233;sul-
tats meilleurs que l&#8217;approche de r&#233;f&#233;rence pour les deux classes, autant en terme de pr&#233;cision
que de rappel, comme le montre le tableau 1. Ainsi il est de 8 points sup&#233;rieurs en pr&#233;cision
et 12 points sup&#233;rieurs en rappel pour la classe des documents d&#233;riv&#233;s. Ces r&#233;sultats semblent
supporter notre hypoth&#232;se d&#8217;autant plus que la seule variation avec l&#8217;approche de r&#233;f&#233;rence
provient des observables (lexiques al&#233;atoire vs. connecteurs discursifs). Les meilleurs r&#233;sultats
confirment le r&#244;le particuli&#232;rement discriminant des connecteurs.
</p>
<p>En r&#233;sum&#233;, nous avons rapproch&#233; originaux et d&#233;riv&#233;s en nous basant sur la distribution des
connecteurs discursifs. L&#8217;exp&#233;rience montre que les documents qui sont des d&#233;riv&#233;s d&#8217;un origi-
nal conservent globalement les connecteurs de ce dernier. &#192; l&#8217;oppos&#233;, les documents qui sont
des faux positifs partagent al&#233;atoirement ces connecteurs. Ceci supporte notre hypoth&#232;se que la
structuration du discours permet de distinguer les d&#233;riv&#233;s parmi les documents candidats.
</p>
<p>4.4 Exp&#233;rimentation sur les n-grams hapax
</p>
<p>Nous rapportons ici nos exp&#233;rimentations de classification de documents en d&#233;riv&#233;s et non-
d&#233;riv&#233;s d&#8217;un document original selon l&#8217;approche d&#233;crite &#224; la section 3.2.
</p>
<p>Pour ce faire, nous avons utilis&#233; trois algorithmes de classification diff&#233;rents5 : Na&#239;ve Bayes,
Sequential Minimal Optimization for training support vector machines (SMO) et LogitBoost.
Pour ce dernier algorithme, nous avons fait varier le nombre d&#8217;it&#233;rations de boosting.
</p>
<p>Nous avons conduit une &#233;valuation par validation crois&#233;e &#224; dix partitions. Globalement les dif-
f&#233;rents algorithmes donnent de bons r&#233;sultats compris entre 85% et 90% ; ce qui d&#233;passe l&#8217;ap-
proche de r&#233;f&#233;rence d&#233;finie &#224; la section 4.2 de 3 points. Dans le tableau 1 (troisi&#232;me ligne : &#171; Ha-
pax &#187;), nous pr&#233;sentons seulement les r&#233;sultats du meilleur algorithme (meilleure F-mesure).
</p>
<p>5Nous avons utilis&#233;s les algorithmes sus-mentionn&#233;s au sein de la plate-forme d&#8217;apprentissage automatique
WEKA (Witten &amp; Frank, 2005).</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Nouvelles consid&#233;rations pour la d&#233;tection de r&#233;utilisation de texte
</p>
<p>Ces r&#233;sultats ont &#233;t&#233; obtenus avec LogitBoost en utilisant 15 it&#233;rations &#224; la fois pour l&#8217;identifi-
cation de documents d&#233;riv&#233;s DR et non-d&#233;riv&#233;s DR&#772;.
</p>
<p>5 Discussion et perspectives
</p>
<p>Dans cet article, nous avons propos&#233; de nouvelles bases th&#233;oriques pour cadrer le probl&#232;me de
d&#233;tection de r&#233;utilisation textuelle. Nous avons ainsi d&#233;fini deux notions capitales, l&#8217;invariance
et la singularit&#233;, dont la consid&#233;ration permet d&#8217;envisager autrement les &#233;tapes de la proc&#233;dure
de d&#233;tection (par exemple le degr&#233; de singularit&#233; d&#8217;un trait d&#8217;un document constitue un nouveau
crit&#232;re de s&#233;lection pour repr&#233;senter ce document). Nous montrons notamment que des hapax ou
des marques singuli&#232;res de nature discursive sont des indices probants pour diff&#233;rencier un do-
cument d&#233;riv&#233; d&#8217;un document non-d&#233;riv&#233; &#224; partir d&#8217;un document original. Nous avons observ&#233;
que les connecteurs discursifs et les n-grams hapax, singularit&#233;s des documents originaux, se
retrouvaient dans les documents d&#233;riv&#233;s, ce qui nous a permis de les rep&#233;rer. L&#8217;utilisation des
connecteurs discursifs singuliers et des n-grams hapax constitue en soi une originalit&#233; de ce tra-
vail puisque ces marques n&#8217;avaient jusqu&#8217;&#224; pr&#233;sent pas &#233;t&#233; consid&#233;r&#233;es dans la litt&#233;rature pour
la d&#233;tection de r&#233;utilisation de texte.
</p>
<p>Les r&#233;sultats &#233;lev&#233;s que nous obtenons sur notre corpus nous conduisent &#224; vouloir confronter
nos m&#233;thodes &#224; d&#8217;autres donn&#233;es. En perspective &#224; ce travail, nous projetons de r&#233;it&#233;rer nos
exp&#233;riences sur le corpus anglais METER (Clough, 2003; Clough &amp; Gaizauskas, 2008).
</p>
<p>Remerciements
</p>
<p>Nous tenons &#224; remercier nos relecteurs pour leurs critiques constructives et pour leurs sugges-
tions d&#8217;am&#233;lioration.
</p>
<p>R&#233;f&#233;rences
</p>
<p>BENDERSKY M. &amp; CROFT W. B. (2009). Finding text reuse on the web. In WSDM &#8217;09 :
Proceedings of the Second ACM International Conference on Web Search and Data Mining,
p. 262&#8211;271, New York, NY, USA : ACM.
</p>
<p>CLOUGH P. &amp; GAIZAUSKAS R. (2008). Corpora and text re-use. In A. L&#220;DELING &amp; M.
KYT&#214;, Eds., Corpus Linguistics : An International Handbook, Handb&#252;cher zur Sprache und
Kommunikationswissenschaft/Handbooks of Linguistics and Communication Science, chap-
ter 59. Berlin : Mouton de Gruyter.
</p>
<p>CLOUGH P. D. (2003). Measuring Text Reuse. PhD thesis, University of Sheffield.
</p>
<p>HERNANDEZ N. (2004). Description et D&#233;tection Automatique de Structures de Texte. PhD
thesis, Universit&#233; Paris-Sud XI.
</p>
<p>JONES K. S. (1972). A statistical interpretation of term specificity and its application in
retrieval. Journal of Documentation, 28, 11&#8211;21.
KNOTT A. (1996). A Data-Driven Methodology for Motivating a Set of Coherence Relations.
PhD thesis, Department of Artificial Intelligence, University of Edinburgh.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Fabien Poulard &amp; Stergos Afantenos &amp; Nicolas Hernandez
</p>
<p>MARCU D. (1997). The Rhetorical Parsing, Summarization, and Generation of Natural Lan-
guage Texts. PhD thesis.
</p>
<p>METZLER D., BERNSTEIN Y., CROFT W. B., MOFFAT A. &amp; ZOBEL J. (2005). Similarity
measures for tracking information flow. In CIKM &#8217;05 : Proceedings of the 14th ACM inter-
national conference on Information and knowledge management, p. 517&#8211;524, New York, NY,
USA : ACM.
</p>
<p>SALTON G. &amp; BUCKLEY C. (1988). Term-weighting approaches in automatic text retrieval.
In Information Processing and Management, p. 513&#8211;523.
</p>
<p>SEO J. &amp; CROFT W. B. (2008). Local text reuse detection. In SIGIR &#8217;08 : Proceedings of the
31st annual international ACM SIGIR conference on Research and development in information
retrieval, p. 571&#8211;578, New York, NY, USA : ACM.
</p>
<p>SPORLEDER C. &amp; LASCARIDES A. (2008). Using automatically labelled examples to classify
rhetorical relations : An assessment. 14(3), 369&#8212;416.
UZUNER O., DAVIS A. &amp; KATZ B. (2004). Using empirical methods for evaluating expres-
sion and content similarity. In In 37th Hawaiian International Conference on System Sciences
(HICSS-37). IEEE Computer Society.
</p>
<p>VAN HALTEREN H. (2004). Linguistic profiling for author recognition and verification. In
ACL &#8217;04 : Proceedings of the 42nd Annual Meeting on Association for Computational Lin-
guistics, p. 199, Morristown, NJ, USA : Association for Computational Linguistics.
</p>
<p>WITTEN I. H. &amp; FRANK E. (2005). Data Mining : Practical Machine Learning Tools and
Techniques. San Francisco : Morgan Kaufmann, second edition.</p>

</div></div>
</body></html>