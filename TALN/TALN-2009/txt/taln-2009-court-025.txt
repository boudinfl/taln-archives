TALN 2009 — Session posters, Senlis, 24-26 juin 2009

Nouvelles considérations
pour la détection de réutilisation de texte

Fabien Poulard, Stergos Afantenos et Nicolas Hernandez
LINA (CNRS - UMR 6241)
2 rue de la Houssiniere — B.P. 92208, 44322 NANTES Cedex 3
{prenom.nom} @univ-nantes.fr

Résumé. Dans cet article nous nous intéressons au probleme de la détection de réutilisa-
tion de texte. Plus particulierement, étant donné un document original et un ensemble de do-
cuments candidats — thématiquement similaires au premier — nous cherchons a classer ceux
qui sont dérivés du document original et ceux qui ne le sont pas. Nous abordons le probleme
selon deux approches : dans la premiere, nous nous intéressons aux similarités discursives entre
les documents, dans la seconde au recouvrement de n-grams hapax. Nous présentons le résul-
tat d’expérimentations menées sur un corpus de presse francophone construit dans le cadre du
projet ANR PIIT HIE.

Abstract. In this article we are interested in the problem of text reuse. More speciﬁcally,
given an original document and a set of candidate documents — which are thematically similar
to the ﬁrst one — we are interested in classifying them into those that have been derived from
the original document and those that are not. We are approaching the problem in two ways :
ﬁrstly we are interested in the discourse similarities between the documents, and secondly we
are interested in the overlap of n-grams that are hapax. We are presenting the results of the
experiments that we have performed on a corpus constituted from articles of the French press
which has been created in the context of the PIITHIE project funded by the French National
Agency for Research (Agence National de la Recherche, ANR).

M0tS-CléS I réutilisation de texte, recouvrement de n-grams hapax, similarités discur-
sives, corpus joumalistique francophone.

Keywords: text reuse, hapax n-grams overlap, discourse similarities, french journalistic
corpus.

Fabien Poulard & Stergos Afantenos & Nicolas Hernandez

1 Introduction

« La reutilisation de texte est l’activite par laquelle des textes ecrits pre-existants sont reutilises
pour creer de nouveaux textes ou versions [. . .] il y a reutilisation quand il y a une realisation
consciente d’une transformation d’un texte pour en arriver a un autre » (Clough & Gaizaus-
kas, 2008). La duplication (copie a l’identique), la revision, l’adaptation de genre, le resume, la
traduction, la citation,. .. sont autant de formes differentes de reutilisation d’un texte original.
Les chercheurs comme les industriels sont conscients depuis de nombreuses annees de l’interet
d’etudier cette activite qui correspond a des enjeux applicatifs reels : la detection de documents
dupliques sur le web a des consequences sur l’efﬁcacite des moteurs du recherche aussi bien
pour leur traitement (coﬁt d’indeXation et de stockage) que sur la precision des reponses rame-
nees. La detection de plagiats presente aussi un grand interet pour le respect du droit d’auteur
que cela concerne le code source de logiciels ou tout document servant "de base" a des devoirs
d’etudiants par exemples. Le suivi d’impact d’une communication sur un produit ou sur une
information rendue publique presente aussi des interets commerciaux et scientiﬁques dans une
perspective de veille.

En pratique, les systemes de detection de reutilisation de texte procedent selon trois etapes :

1. d’abord ils selectionnent des types d’unites textuelles a observer (mot, syntagme, phrase,
paragraphe, document, n-gram avec/sans recouvrement) dans les documents manipules;

2. ensuite ils construisent un modele de chaque document par normalisation linguistique
(lexicale, syntaxique, semantique) ou numerique (condensation par algorithme de ha-
shage) et par ﬁltrage (les mots pleins, un n-gram donne, les n-premiers rencontres dans
le texte, ponderes par t f.idf , ...) des observables;

3. enﬁn ils comparent effectivement les documents sur la base de ces representations.

Le choix de la representation est bien entendu dependant de la methode de comparaison utilisee.
Celles-ci varient suivant differents coﬁts de traitement : de mesures de similarites rencontrees en
Classiﬁcation ou en Recherche d’Information (RI) (ratio des materiaux partages, distance vec-
torielle) aux comparaisons plus complexes et speciﬁques (plus longues sous chaines communes,
distance d’edition) (Uzuner et al., 2004; Metzler et al., 2005; Seo & Croft, 2008; Bendersky &
Croft, 2009; Clough & Gaizauskas, 2008).

Les differentes etapes de cette procedure sont sujetes a de nombreux enjeux techniques : com-
ment choisir les unites textuelles les plus representatives du contenu du document? Les moins
coﬁteuses a extraire (en terme de ressources requises, de methodes a mettre en place, de temps
de calcul) ? Les plus caracteristiques des phenomenes de reutilisation ? Quelles sont les me-
thodes les plus adequates en termes de precision et de temps de traitement pour detecter une
forme donnee de reutilisation ?. . .

Le contexte applicatif du present article est celui de la detection de reutilisations a partir d’un
ecrit original, dans des textes journalistiques francophones presentant des similarites thema-
tiques avec le document sourcel. En particulier notre tache consistait a classer les documents
candidats comme etant des reutilisations ou non d’un document original connu. La ﬁgure 1
foumit un exemple issu de notre corpus d’un texte original, d’une reutilisation de celui-ci et
d’une similarite thematique (cas de non reutilisation).

Dans cette article, nous proposons de nouvelles considerations theoriques aﬁn de mieux cadrer

1Ce travail a beneﬁcie du soutien de l’Agence Nationale de la Recherche, proj et PIITHIE (www.piithie.com)
portant la reference 2006 TLOG 013 03.

Nouvelles considérations pour la détection de réutilisation de texte

Exemple (I) Texte original .'

Le groupe Carrefour va prochainement se lancer dans la VOD en France mais pas
seulement. Fier de ses parts de marché dans la vente de DVD dans d’autres pays
d’europe (autour de 13%), Carrefour va aussi ouvrir son service de Vidéo a la De-
mande en Espagne, Italie et Belgique.

Exemple (2) Réutilisation .'

Le géant national de la grande distribution francaise lancera une offre de VOD,
video a la demande en France, Belgique, Italie et Espagne, o1‘1 par-ailleurs il détient
une part de marché de 13,3% dans la vente de DVD.

Exemple (3) Similarite’ thématique .'

Le 8 novembre 2006 en partenariat avec Orange, Carrefour lancera son offre de
téléphonie mobile : Carrefour Mobile. Le groupe de distribution devient ainsi ope-
rateur virtuel de téléphonie mobile (MVNO) avec les mémes ambitions que son
concurrent direct Auchan.

FIG. 1 — Classiﬁcation de documents candidats comme étant des réutilisations ou non d’un
document original connu

le probleme de la détection de réutilisation de textes. Nous introduisons notamment la notion
de singularités d’un document vis-a-vis d’une collection. Nous avancons que cette considera-
tion permet de sélectionner plus ﬁnement les unités textuelles représentant un document. Cela
offre de nouvelles perspectives telles que : l’indexation des documents du web sans ﬁltrage des
réutilisations par post-traitement, l’eXploitation des moteurs de recherche traditionnels pour la
recherche directe de réutilisations, la récupération de cas de réutilisations avec transformations
importantes, la distinction des documents thématiquement similaires de ceux qui constituent
effectivement des réutilisations.

L’ étude de cette propriété est envisagée autour de deux expériences de détection de réutilisation
de texte. Celles-ci utilisent des unités textuelles jusqu’alors non considérées dans la littérature
pour représenter les documents : des marques discursives et des n-grams hapaxz. Nous ob-
servons leur capacité de détection de réutilisations de par leur singularité. En pratique, elles
seront utilisées en complément avec d’autres observables tels que des termes sélectionnés sur
leur t f .idf aﬁn de capturer par les similarités thématiques inter-documents.

1.1 Cadre théorique et terminologie

Les caractéristiques d’une réutilisation de texte doivent remplir deux taches : d’une part déﬁnir
si un document candidat est une réutilisation et d’autre part déterminer de quel document il est
une réutilisation. Par caractéristiques, nous entendons toute marque linguistique telle que les
termes, la syntaxe des phrases, ou des marques de plus haut niveau telles que les references a
des entités ou l’organisation des idées.

La premiere tache nécessite de considérer les caractéristiques que l’on retrouve a la fois dans
le document dérivé et dans les documents originaux, nous les appelons : les invariants. I1 n’est
pas possible de sélectionner les invariants pour un document original si on n’a pas déﬁni a quel
document candidat on le comparait. Nous parlerons de classes d’invariants pour désigner des

2Hapax signiﬁe << qui a été dit qu’une fois ».

Fabien Poulard & Stergos Afantenos & Nicolas Hernandez

invariants de differentes natures linguistiques, sans nous limiter aux formes de surface. Ainsi,
une reference a une meme entite entre deux documents a l’aide de formes de surfaces differentes
est consideree comme un invariant ; l’invariant etant la reference a ladite entite, et non la forme
employee pour la denommee.

La seconde tache necessite de considerer les caracteristiques presentes uniquement dans le do-
cument et absentes de la collection homogene a laquelle celui-ci appartient, nous les appelons :
les singularités. Cette collection homogene se deﬁnit par des criteres speciﬁques selectionnes
selon la ﬁnalite desiree. I1 n’est pas possible de calculer les singularites d’un document sans
avoir auparavant deﬁni la collection de documents dans laquelle il s’inscrivait. Les singularites
peuvent prendre des formes aussi diverses que les invariants et de la meme facon nous parlerons
de classes de singularites pour designer les singularites de differentes natures linguistiques.

Une combinaison de marques peut correspondre a une singularite ou un invariant meme si ce
n’est pas le cas des marques prises individuellement. Nous essayons d’estimer dans cet article
dans quelle proportion certaines classes de singularites sont generalement invariantes et per-
mettent donc d’identiﬁer des reutilisations.

2 Etat de l’art

Notre enonciation du principe de singularite est appuyee par divers travaux de la litterature.

La prise en compte de l’importance d’un terme dans une collection est un principe considere
des les premiers travaux en RI (Jones, 1972; Salton & Buckley, 1988). Ainsi la bien connue
inverse de la frequence des documents idf est un facteur utilise avec la frequence des termes
d’un document t f pour relativiser cette derniere mais aussi pour favoriser les termes presents
dans peu de document d’une collection. Elle se calcule en generale en prenant le logarithme3
du ratio du nombre de documents de la collection sur le nombre de documents distincts dans
lesquels on retrouve un terme donne.

Pour la tache de reconnaissance du style d’un auteur, (van Halteren, 2004) montre que le
« comptage de [combinaison de] traits linguistiques d’un texte normalise par sa longueur et
leur deviation par rapport a la moyenne observee sur un corpus de reference » permet d’obtenir
de meilleurs resultats que les methodes fondees sur des analyses en composante principale, des
analyses discriIr1inantes lineaires, ou des distributions probabilistes.

Aﬁn de mesurer l’appartenance d’un terme au genre d’une collection dans laquelle il apparait,
(Hernandez, 2004) a utilise avec succes une des composante de l’idf, le nombre de documents
distincts dans lesquels on retrouve un terme, pour ﬁltrer les termes des documents.

Dans le contexte de la detection de reutilisation le long d’un spectre de degres de siIr1ilarites,
(Metzler et al., 2005) observent qu’une mesure fondee sur le recoupement de mots ponderes
par idf est l’une des deux methodes les plus performantes pour la detection de reutilisation avec
fortes transformations (reprise partielle des faits). Suivant les methodes consideres, leurs taux
de precision varie de 40-60% lorsque la siIr1ilarite traduit un lien thematique ou qu’il s’agit
d’une reprise partielle de faits, a 80—100% pour les reprises identites.

3Car le simple ratio donne des valeurs trés grandes.

Nouvelles considérations pour la détection de réutilisation de texte

3 Méthodes de détection de réutilisation

Nous présentons ci-apres deux méthodes se fondant sur le cadre théorique que nous avons déﬁni.
La premiere utilise des marques discursives et la seconde des hapax.

3.1 A l’aide de marques discursives singuliéres

Nous décrivons dans cette section une approche basée sur la modélisation des documents par
une représentation partielle de leur structure discursive. Nous pensons que cette structure est
sufﬁsamment singuliere (cf section 1.1) pour permettre de détecter automatiquement les re-
prises globales de documents. Nous détaillons ci-dessous la réﬂexion qui nous a mené a cette
hypothese puis nous en présentons une modélisation aﬁn de mener nos expérimentations.

La restructuration discursive globale d’un document est une tache difﬁcile qui nécessite une
réécriture partielle de ce demier et une réorganisation des idées. Dans le cadre des articles
de presse, il s’agit de retravailler la structure argumentative ou descriptive de l’article original.
Nous choisissons de nous intéresser aux éléments de structuration discursifs car nous supposons
qu’ils varient peu lors d’une reprise globale d’un document.

La structure discursive est complexe a extraire eta caractériser, excepté lorsqu’elle est marquée
par des connecteurs discursifs non ambigus (Sporleder & Lascarides, 2008). Nous choisissons
alors plus particulierement de travailler a partir des connecteurs discursifs et de ne considé-
rer uniquement la structure discursive des documents rendue visible par ces connecteurs. Nous
sommes tout a fait conscient que ces derniers ne sont que la surface émergée de l’iceberg du
discours. Toutefois, ce racourci nous permet d’eXpérimenter l’idée de la détection automatique
de reprise par comparaison des structures discursives sans nécessiter d’appliquer des méthodes
complexes a mettre en oeuvre. En effet, les connecteurs discursifs appartiennent a une classe
lexicale aisément observable par des techniques automatiques, peu ambigue (Sporleder & Las-
carides, 2008) et dont l’utilisation peu fréquente dans les documents est en accord avec le prin-
cipe de singularité énoncé précédemment.

Nous choisissons de représenter la structuve discursive sans tenir compte de l’ordre d’appari-
tion desdits connecteurs et en considérant ces derniers dans leur forme lexicale observée. Nous
sommes conscients de la nai'veté de cette approche, la division en classe sémantique, la position
relative ou absolue et la séquence d’apparition sont des elements a prendre en compte. Cepen-
dant, dans le cadre d’une reprise globale du document, notre approche fonctionne assez bien
comme le montre les résultats de la section 4.3 et la simplicité de l’approche est un atout pour
sa Inise en oeuvre.

Chaque document est modélisé par un vecteur du nombre absolu des occurrences de connecteurs
discursifs. Ces connecteurs recherchés sont au nombre de 90 et ont été collectés ou traduits de
la littérature (Knott, 1996; Marcu, 1997) dans (Hernandez, 2004). Il s’agit principalement de
connecteurs (conjonctions, prépositions, adverbes et locutions conjonctives, prépositionnelles
et adverbiales) tels que premierement, ensuite, dans un premier temps, tout d ’ab0rd, .. .

Nous comparons ensuite le vecteur d’un document considéré original et celui d’un document
candidat a l’aide d’une mesure de similarité cosinus. Les valeurs varient de 0 a 1 ou 0 signiﬁe
que les deux vecteurs sont indépendants et 1 signiﬁe que les vecteurs sont identiques. Nous
choisissons de retoumer 0 lorsqu’aucun connecteur n’apparait dans un des deux documents, ce

Fabien Poulard & Stergos Afantenos & Nicolas Hernandez

qui n’est pas entierement satisfaisant étant donné que 22% des documents de notre corpus ont
cette caractéristique.

En résumé, nous cherchons a déﬁnir si une réutilisation réuni deux documents. Nous comparons
pour cela les structures discursives rendues visibles par les connecteurs de ces documents. Nous
expérimentons cette méthode dans la section 4.3.

3.2 A l’aide de marques lexicales singuliéres

Nous posons ici l’hypothese que des hapax ont un pouvoir discriminant pour la détection de
réutilisation. Nous décrivons dans cette section notre processus pour obtenir des hapax ainsi
que notre technique de comparaison de documents pour détecter des réutilisations a partir de
ces hapax.

Dans le cadre de ce travail, nous avons choisi d’observer les n-grams hapax comme instance
des hapax. Pour ce faire nous avons produit des unigrams, bigrams, trigrams, ..., sept-grams a
partir de notre corpus en ﬁltrant les mots vides. Nous avons ensuite cherché a identiﬁer les n-
grams hapax de chaque document en sommant pour chaque n-gram son nombre d’occurrences
dans le document et son nombre d’occurrences dans les documents des autres répertoires de
notre corpus; un répertoire réunie un document source original et un ensemble de documents
candidats dérivés ou non du document source. Seuls les n-grams qui apparaissent une seul fois
ont été conservés (hapax).

En ce qui concerne la méthode de comparaison nous avons abordé le probleme comme une
tache de classiﬁcation binaire ou il s’agissait de classer un document candidat en document
dérivé ou en document non-dérivé. Pour chaque paire de documents original et candidat, nous
avons constitué un vecteur de deux attributs : le nombre de hapax en commun et le nombre des
hapax distincts.

4 Expérimentations

Dans cette section, nous décrivons le corpus utilisé pour nos expérimentations lesquelles sont
présentées a la suite.

4.1 Construction et composition du corpus PIITHIE

Le corpus utilisé lors des expérimentations a été construit dans le cadre du projet ANR PII-
T HIE4. Nous présentons ci-dessous sa construction et sa composition.

Dans un premier temps, des documents récents considérés comme originaux ont été manuel-
lement sélectionnés sur des sites de presse en ligne. Un article était considéré comme original
s’il avait pour source une agence de presse (AFP, REUTERS). Dans une période postérieure
immédiate, des documents dérivés candidats ont été récupérés a l’aide de moteurs de recherche
sur des sites web sélectionnés. La sélection des sites de recherche visait a garantir une homoge-
néité de genre des documents ramenés. Les requétes des moteurs étaient produites a partir des

4Le corpus produit sera prochainement distribué sur http : / /www . pi ithie . com.

Nouvelles considérations pour la détection de réutilisation de texte

Classiﬁeur Classe | Precision Rappel F-mesure

App.Référence DB1 0.86 0.44 0.58

DR 0.48 0.88 0.62
Connecteurs DR 0.94 0.56 0.70
DR 0.56 0.94 0.70
Hapax DR 0.923 0.895 0.909
DR 0.83 0.873 0.851

TAB. 1 — Comparaison des résultats des différentes expérimentations

cinq mots les plus « rares » des documents originaux. Le degré de rareté d’un mot était calculé
a priori sur la base du nombre de documents qu’une requéte avec ce mot ramenait dans les mo-
teurs de recherche utilisés. Les documents récupérés ont ensuite été annotés manuellement par
deux annotateurs aﬁn de classer les documents comme dérivés ou non-dérivés. Un document
était considéré comme dérivé s’il reprenait les évenements décrits dans le document source et
s’il présentait des sous-chaines de mots communes.

Au total, le corpus se compose de 77 documents originaux, 496 documents dérivés et 293 do-
cuments non-dérivés. En pratique, le corpus est divisé en 77 répertoires contenant chacun un
document original et un ensemble de documents candidats (dérivés et non-dérivés).

4.2 Approche de référence

Dans cette section, nous décrivons une méthode dont nous utiliserons les résultats comme ré-
férence pour nos expérimentations. Cette méthode na'1've reprend les principes de la méthode
discursive présentée a la section 3.1, la différence réside dans le choix des observables. En ef-
fet, nous n’utilisons pas ici la structure discursive mais un sous-ensemble du lexique du corpus
pour caractériser les documents.

Nous utilisons un lexique de 90 mots extraits aléatoirement du corpus et comparable en taille
avec la signature discursive, en ﬁltrant les mots outils a l’aide d’une heuristique. Chaque docu-
ment est alors caractérisé par le nombre d’occurrences des éléménts de ce lexique qui y apparait.
Les vecteurs obtenus sont alors compares a l’aide d’une mesure cosinus et l’on classe comme
repris les documents qui présentent une valeur supérieure a un seuil ﬁxé. Inversement les do-
cuments obtenant une valeur inférieure sont considérés comme des non-reprises. Le seuil a été
sélectionné de maniere a maximiser la précision et le rappel de cette méthode sur le corpus.

Le tableau 1 expose les résultats de l’approche de référence pour la classe des documents repris
(D R) et celle des documents non-repris (DR). Outre l’obtention de résultats pour comparaison,
la similarité de cette approche avec la méthode discursive permet de tester le role particulier des
connecteurs dans la section suivante.

4.3 Expérimentation sur les connecteurs discursifs

Nous expérimentons dans cette section la détection automatique de reprise sur le corpus pre-
senté a la section 4.1 par la méthode discursive décrite a la section 3.1.

Fabien Poulard & Stergos Afantenos & Nicolas Hernandez

Nous avons observé au préalable que les formes graphiques des connecteurs de notre diction-
naire sont présentes dans environ 80% des documents. Certaines de ces formes sont ambigues
mais nous n’en tenons pas compte (cf section 3.1). Nous avons considéré chaque document
original du corpus Piithie, et pour chacun de ces documents nous avons calculé le cosinus de
son vecteur et celui de chacun des documents candidats associés.

Nous avons comparé les valeurs du cosinus entre les originaux et les candidats en différenciant
les dérivés des non-dérivés. Nous observons que ces valeurs pour les non-dérivés sont réparties
de maniere assez homogene sur tout l’espace image alors qu’elles se densiﬁent autour de 1 (vec-
teurs identiques) pour les dérivés. Cette observation supporte notre hypothese que les structures
discursives sont proches entre les documents originaux et les dérivés et qu’il s’agit donc d’un
invariant potentiel.

Etant donné un document original et sa modélisation selon notre méthode, nous classons comme
repris un document candidat dont le cosinus appliqué entre son modele et celui de l’original est
supérieur a un seuil ﬁxé. Par opposition, les documents pour lesquels le cosinus est inférieur au
meme seuil sont considérés comme des non-dérivés. Nous avons choisi le seuil de 0.8 car c’est
la valeur du cosinus qui maximise la précision du classiﬁeur sur le corpus. Nous avons déﬁni
cette valeur en faisant varier le cosinus par dizieme. Le classiﬁeur obtient alors une précision
de 94% et un rappel de 56% come le montre le tableau 1.

La caractérisation des documents par un vecteur d’occurrence des connecteurs donne des résul-
tats meilleurs que l’approche de référence pour les deux classes, autant en terme de précision
que de rappel, comme le montre le tableau 1. Ainsi il est de 8 points supérieurs en précision
et 12 points supérieurs en rappel pour la classe des documents dérivés. Ces résultats semblent
supporter notre hypothese d’autant plus que la seule variation avec l’approche de référence
provient des observables (lexiques aléatoire vs. connecteurs discursifs). Les meilleurs résultats
conﬁrment le role particulierement discriminant des connecteurs.

En résumé, nous avons rapproché originaux et dérivés en nous basant sur la distribution des
connecteurs discursifs. L’ expérience montre que les documents qui sont des dérivés d’un origi-
nal conservent globalement les connecteurs de ce demier. A l’opposé, les documents qui sont
des faux positifs partagent aléatoirement ces connecteurs. Ceci supporte notre hypothese que la
structuration du discours permet de distinguer les dérivés parmi les documents candidats.

4.4 Expérimentation sur les n-grams hapax

Nous rapportons ici nos expérimentations de classiﬁcation de documents en dérivés et non-
dérivés d’un document original selon l’approche décrite a la section 3.2.

Pour ce faire, nous avons utilisé trois algorithmes de classiﬁcation différents5 : Naive Bayes,
Sequential Minimal Optimization for training support vector machines (SM 0 ) et LogitBoost.
Pour ce demier algorithme, nous avons fait varier le nombre d’itérations de boosting.

Nous avons conduit une évaluation par validation croisée a dix partitions. Globalement les dif-
férents algorithmes donnent de bons résultats compris entre 85% et 90% ; ce qui dépasse l’ap-
proche de référence déﬁnie a la section 4.2 de 3 points. Dans le tableau 1 (troisieme ligne : « Ha-
pax »), nous présentons seulement les résultats du meilleur algorithme (meilleure F-mesure).

5Nous avons utilisés les algorithmes sus-mentionnés au sein de la plate—forme d’apprentissage automatique
WEKA (Witten & Frank, 2005).

Nouvelles considérations pour la détection de réutilisation de texte

Ces résultats ont été obtenus avec LogitBoost en utilisant 15 iterations a la fois pour l’identiﬁ-
cation de documents dérivés D R et non-dérivés D R.

5 Discussion et perspectives

Dans cet article, nous avons proposé de nouvelles bases théoriques pour cadrer le probleme de
détection de réutilisation textuelle. Nous avons ainsi déﬁni deux notions capitales, l’invariance
et la singularité, dont la considération permet d’envisager autrement les étapes de la procédure
de détection (par exemple le degré de singularité d’un trait d’un document constitue un nouveau
critere de sélection pour représenter ce document). Nous montrons notamment que des hapax ou
des marques singulieres de nature discursive sont des indices probants pour différencier un do-
cument dérivé d’un document non-dérivé a partir d’un document original. Nous avons observé
que les connecteurs discursifs et les n-grams hapax, singularités des documents originaux, se
retrouvaient dans les documents dérivés, ce qui nous a permis de les repérer. L’utilisation des
connecteurs discursifs singuliers et des n-grams hapax constitue en soi une originalité de ce tra-
vail puisque ces marques n’avaient jusqu’a présent pas été considérées dans la littérature pour
la détection de réutilisation de texte.

Les résultats élevés que nous obtenons sur notre corpus nous conduisent a vouloir confronter
nos méthodes a d’autres données. En perspective a ce travail, nous projetons de réitérer nos
experiences sur le corpus anglais METER (Clough, 2003; Clough & Gaizauskas, 2008).

Remerciements

Nous tenons a remercier nos relecteurs pour leurs critiques constructives et pour leurs sugges-
tions d’amélioration.

Références

BENDERSKY M. & CROFT W. B. (2009). Finding text reuse on the web. In WSDM ’09 .-
Proceedings of the Second ACM International Conference on Web Search and Data Mining,
p. 262-271, New York, NY, USA : ACM.

CLOUGH P. & GAIZAUSKAS R. (2008). Corpora and text re-use. In A. LUDELING & M.
KYTC"), Eds., Corpus Linguistics .' An International Handbook, Handbiicher zur Sprache und
Kommunikationswissenschaftﬂ-Iandbooks of Linguistics and Communication Science, chap-
ter 59. Berlin : Mouton de Gruyter.

CLOUGH P. D. (2003). Measuring Text Reuse. PhD thesis, University of Shefﬁeld.

HERNANDEZ N. (2004). Description et Détection Automatique de Structures de Texte. PhD
thesis, Université Paris-Sud XI.

JONES K. S. (1972). A statistical interpretation of term speciﬁcity and its application in
retrieval. Journal of Documentation, 28, 11-21.

KNOTT A. (1996). A Data-Driven Methodology for Motivating a Set of Coherence Relations.
PhD thesis, Department of Artiﬁcial Intelligence, University of Edinburgh.

Fabien Poulard & Stergos Afantenos & Nicolas Hernandez

MARCU D. (1997). The Rhetorical Parsing, Summarization, and Generation of Natural Lan-
guage Texts. PhD thesis.

METZLER D., BERNSTEIN Y., CROFT W. B., MOFFAT A. & ZOBEL J. (2005). Similarity
measures for tracking information ﬂow. In CIKM ’05 .' Proceedings of the I 4th ACM inter-

national conference on Information and knowledge management, p. 517-524, New York, NY,
USA : ACM.

SALTON G. & BUCKLEY C. (1988). Term-weighting approaches in automatic text retrieval.
In Information Processing and Management, p. 513-523.

SEO J. & CROFT W. B. (2008). Local text reuse detection. In SIGIR ’08 .' Proceedings of the
3 I st annual international ACM SIGIR conference on Research and development in information
retrieval, p. 571-578, New York, NY, USA : ACM.

SPORLEDER C. & LASCARIDES A. (2008). Using automatically labelled examples to classify
rhetorical relations : An assessment. 14(3), 369—416.

UZUNER 0., DAVIS A. & KATZ B. (2004). Using empirical methods for evaluating expres-
sion and content similarity. In In 37th Hawaiian International Conference on System Sciences
(HICSS-37). IEEE Computer Society.

VAN HALTEREN H. (2004). Linguistic proﬁling for author recognition and veriﬁcation. In
ACL ’04 .' Proceedings of the 42nd Annual Meeting on Association for Computational Lin-
guistics, p. 199, Morristown, NJ, USA : Association for Computational Linguistics.

WITTEN I. H. & FRANK E. (2005). Data Mining .' Practical Machine Learning Tools and
Techniques. San Francisco : Morgan Kaufmann, second edition.

