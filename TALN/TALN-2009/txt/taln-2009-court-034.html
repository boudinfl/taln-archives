<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
"http://www.w3.org/TR/html4/loose.dtd">
<html><head><title>Relevance of ASR for the Automatic Generation of Keywords Suggestions for TV programs</title>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
</head>
<body>
<div style="page-break-before:always; page-break-after:always"><div><p>TALN 2009 &#8211; Session posters , Senlis, 24&#8211;26 juin 2009
</p>
<p>Relevance of ASR for the Automatic Generation of Keywords
Suggestions for TV programs
</p>
<p>V&#233;ronique Malais&#233;1 Luit Gazendam2 Willemijn Heeren3 Roeland
Ordelman3,4 Hennie Brugman5
</p>
<p>(1) VU University Amsterdam, (2) Telematica Instituut, Enschede
</p>
<p>(3) University of Twente, Enschede
</p>
<p>(4) Netherlands Institute for Sound and Vision, Hilversum
</p>
<p>(5) MPI for Psycholinguistics, Nijmegen
</p>
<p>vmalaise@few.vu.nl
</p>
<p>R&#233;sum&#233;. L&#8217;acc&#232;s aux documents multim&#233;dia, dans une archive audiovisuelle, d&#233;pend en grande
partie de la quantit&#233; et de la qualit&#233; des m&#233;tadonn&#233;es attach&#233;es aux documents, notamment la description
de leur contenu. Cependant, l&#8217;annotation manuelle des collections est astreignante pour le personnel.
De nombreuses archives &#233;voluent vers des m&#233;thodes d&#8217;annotation (semi-)automatiques pour la cr&#233;ation
et/ou l&#8217;am&#233;lioration des m&#233;tadonn&#233;es. Le project CATCH-CHOICE, fond&#233; par NWO, s&#8217;est pench&#233; sur
l&#8217;extraction de mots cl&#233;s &#224; partir de resources textuelles li&#233;es aux programmes TV destin&#233;s &#224; &#234;tre archi-
v&#233;s (p&#233;ritextes), en collaboration avec les archives audiovisuelles n&#233;erlandaises, Sound and Vision. Cet
article se penche sur la question de l&#8217;ad&#233;quation des transcriptions de Reconnaissance Automatique de
la Parole d&#233;velopp&#233;s dans le projet CATCH-CHoral pour la g&#233;n&#233;ration automatique de mots-cl&#233;s : les
mots-cl&#233;s extraits de ces ressources sont &#233;valu&#233;s par rapport &#224; des annotations manuelles et par rapport &#224;
des mots-cl&#233;s g&#233;n&#233;r&#233;s &#224; partir de p&#233;ritextes d&#233;crivant les programmes t&#233;l&#233;visuels.
</p>
<p>Abstract. Semantic access to multimedia content in audiovisual archives is to a large extent de-
pendent on quantity and quality of the metadata, and particularly the content descriptions that are at-
tached to the individual items. However, the manual annotation of collections puts heavy demands on
resources. A large number of archives are introducing (semi) automatic annotation techniques for ge-
nerating and/or enhancing metadata. The NWO funded CATCH-CHOICE project has investigated the
extraction of keywords from textual resources related to TV programs to be archived (context docu-
ments), in collaboration with the Dutch audiovisual archives, Sound and Vision. This paper investigates
the suitability of Automatic Speech Recognition transcripts produced in the CATCH-CHoral project for
generating such keywords, which we evaluate against manual annotations of the documents, and against
keywords automatically generated from context documents describing the TV programs&#8217; content.
</p>
<p>Mots-cl&#233;s : Extraction de mots cl&#233;s, Reconnaissance Automatique de la Parole, Documents
Audiovisuels.
</p>
<p>Keywords: Keyword extraction, Automatic Speech Recognition, Audiovisual Documents.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>V&#233;ronique Malais&#233;, Luit Gazendam, Willemijn Heeren, Roeland Ordelman, Hennie Brugman
</p>
<p>1 Introduction
</p>
<p>Improving semantic access to multimedia content in audiovisual (AV) archives is to a large
extent dependent on quantity and quality of the metadata, and particularly the content descrip-
tions that are attached to the individual items. However, given the growing amount of materials
that are being created on a daily basis and the digitization of existing analogue collections, the
traditional manual annotation of collections puts heavy demands on resources, especially for
large audiovisual archives. One way to address this challenge, is to introduce (semi) automatic
annotation techniques for generating and/or enhancing metadata : either by doing semantic ana-
lysis on the AV items themselves (e.g., automatic speech recognition technology (Kohler et al.,
2008) or visual concept detection (Smeulders et al., 2000)) or on textual resources related to the
AV items : context documents (e.g., online TV guides, broadcaster&#8217;s websites or collateral data
such as subtitles (Lespinasse &amp; Bachimont, 2001)).
</p>
<p>This paper investigates the suitability of Automatic Speech Transcripts as a source forautoma-
tically generating keywords suggestions. We evaluated the results extracted from these docu-
ments (1) against manual annotation of the same set of data, and (2) against keywords sug-
gestions extracted from more canonical written documents : online Websites describing the
semantic content of TV programs form our corpus. This investigation combines research from
two NWO-CATCH projects : CHOICE 1, and CHoral 2, in collaboration with the Netherlands
Institute for Sound and Vision, the Dutch NAtional Audiovisual Archives 3.
</p>
<p>In addition to large quantities of audiovisual content (a.o. from Dutch public broadcasters) that
are flowing in digitally on a daily basis, Sound and Vision is retrospectively digitizing thou-
sands of hours of archival content. The automation of semantic annotation seems necessary to
guarantee access to the content. In order to investigate how information from broadcasters&#8217; web-
sites can be used for (semi) automatic content annotation, Sound and Vision started archiving
these websites in the LiWa project 4. In CHOICE, the textual content from such broadcasters&#8217;
websites was used for recommending cataloguers terms from the Sound and Vision thesaurus
(GTAA, see (Gazendam et al., 2006), (Brugman et al., 2008)).
</p>
<p>Another source for keyword recommendation is the automatic analysis of the content itself,
such as speech transcripts. Indeed, CHoral focuses on the use of automatic speech recognition
(ASR) technology for indexing audiovisual documents, generally referred to as Spoken Docu-
ment Retrieval (SDR). Speech recognition produces a word-level, textual representation of the
audio, which &#8211;after conversion to an index&#8211; can be used for word-level search that retrieves the
exact fragment in which a match for the query was found. In addition to using ASR to generate
a time-stamped index for spoken word documents, it may also be useful for other purposes,
such as keyword recommendation.
</p>
<p>Whereas websites show a high level of abstraction over the content of the program (a few
lines to some pages of textual description, underlining the broadcaster&#8217;s opinion about the main
focus of the program), the ASR transcripts are long documents, which do not follow the strict
rules of written language, as they transcribe speech, and they are close to the content level of
</p>
<p>1. http://www.nwo.nl/CATCH/CHOICE, funded by the Netherlands Organization for Scientific Re-
search (NWO).
</p>
<p>2. http://www.nwo.nl/CATCH/choral/, funded by the Netherlands Organization for Scientific Re-
search (NWO) (2006-2010)
</p>
<p>3. http://www.beeldengeluid.nl
4. http://www.liwa-project.eu/</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Relevance of ASR for the Automatic Generation of Keywords Suggestions for TV programs
</p>
<p>the program itself. Indexes for audiovisual collections generated on the basis of ASR output are
already being used as an alternative to standard metadata for searching spoken word documents.
</p>
<p>In the use-case of a disclosed set of audiovisual documents provided by Sound and Vision, we
discuss in this paper the usefulness of ASR transcripts for automatically generating keyword-
based indexes for audiovisual documents. We use two sources of evaluation : (i) evaluation of
the ASR-based keywords in terms of precision and recall against the manual annotations ; (ii)
a comparison of this set with keywords suggestions generated from websites related to the TV
programs (the approach investigated by CHOICE so far). This enables us to check the com-
patibility and complementarity of the two approaches for automatically generating multimedia
documents&#8217; meta-data.
</p>
<p>The remainder of the paper is organized as follows : in section 2 we present our pipeline for
extracting keywords from textual resources related to TV programs. We then detail the ASR
method that was used for this experiment in section 3. The experiment is presented in sec-
tion 4 : the proposition of keywords that correspond to the in-house thesaurus of the Dutch AV
archives, based on strings matched in the ASR transcript. We conclude the paper in section 5.
We conclude the paper in section 5.
</p>
<p>2 Keywords Recommendation
</p>
<p>The extraction of keywords from textual resources has been implemented in several tools and
platforms, which belong roughly to three categories : fully manual, semi-automatic and fully
automatic. From a documentalist&#8217;s point of view it is crucial that the extraction process does
not put extra demands on the (already heavy) work load. Hence, keyword extraction systems
that rely on fully manual input, such as the Annotea (Kahan &amp; Koivunen, 2001) system, are not
considered for our use case. Instead, the aim is to provide documentalists with a pre-processed
list of recommended keywords from a controlled vocabulary. Semi-automatic annotation pipe-
lines do not seem to be suitable either, although they keep a human intervention in the loop, as
we wish to do. Tools from this category (like Amilcare (Ciravegna &amp; Wilks, 2003)) help anno-
tating the textual documents faster and better, but it would take an additional effort for Sound
and Vision&#8217;s cataloguers to annotate texts they will not archive. In our context, the documents
to be annotated are the TV programs the texts refer to, not the texts themselves.
</p>
<p>We opted for a fully automatic approach, such as used in the KIM system (Kiryakov et al.,
2005), that generates a list of keyword suggestions without human intervention. The cataloguers
only need to select the items they consider relevant in the list. The top level of KIM&#8217;s annotation
ontology however, is fixed and cannot be changed. Sound and Vision&#8217;s thesaurus, the GTAA,
which we want our annotations to refer to (see section 4), has its own specific structure that
does not comply to the model used in KIM. We therefore implemented the keyword extraction
tool in GATE and co-designed the Apolda plugin 5, described in the next section.
</p>
<p>2.1 CHOICE&#8217;s Keyword Extraction Setup
</p>
<p>CHOICE&#8217;s annotation pipeline, or automatic keywords extraction pipeline, consists of two main
parts : a lookup module that generates annotations referring to strings in the processed text and a
</p>
<p>5. http://apolda.sourceforge.net/</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>V&#233;ronique Malais&#233;, Luit Gazendam, Willemijn Heeren, Roeland Ordelman, Hennie Brugman
</p>
<p>ranking module. The lookup, done with the Apolda plug-in mentioned above, is based on simple
string matching linked with some heuristics, like preferring the longest possible match for an
annotation. Because the lookup does not include linguistic rules, it is language and domain-
independent, and can also apply to textual resources that are not conform to standard written
language (blogs on Internet or ASR transcripts for example). However, as simple string mat-
ching leads to synonymy and polysemy problems, ranking algorithms have to be implemented
to filter out errors in the keywords proposition list. We have shown in (Malais&#233; et al., 2007) that
a ranking algorithm can be used as a Word Sense Disambiguation module. After comparing dif-
ferent methods based on the thesaurus&#8217; graph structure (Gazendam et al., 2009), we found out
that a classic tf.idf ranking performed equally good (although the generated lists are different).
Therefore, for this experiment, we use the classic tf.idf ranking. tf measures the frequency of
a possible keyword in a document, df is its frequency in the corpus taken into account (in our
case a disclosed subset of the audiovisual archives, described in section 4).
</p>
<p>3 Automatic Speech Recognition Setup
</p>
<p>An alternative to broadcasters&#8217; websites for generating keyword recommendation is to use
speech transcripts of the audiovisual content. Speech in audiovisual documents may represent
the content of an item very well and, once converted from audio samples into a textual represen-
tation, could serve as a useful source for keyword selection. Different types of speech transcripts
can be thought of : they may be generated manually in the production stage of the AV docu-
ment (e.g., teleprompts, scripts, summaries) or afterwards (subtitles), or generated automatically
using automatic speech recognition technology. In our use case, the Dutch SHoUT (Huijbregts,
2008) speech recognition toolkit is deployed. Its performance on the TRECVID2007 6 data set
that is used for our keyword selection experiments, is elaborately discussed in (Huijbregts et al.,
2007), but we will give a summary here.
</p>
<p>The SHoUT transcription setup consists of Speech Activity Detection, speaker segmentation
and clustering, and multi-pass speech recognition. During Speech Activity Detection speech is
separated from non-speech segments. Especially in the broadcast domain, this step is important
for filtering out e.g., music, environmental sounds, and long stretches of silence, before the au-
dio is processed. Within the segments identified as speech, speaker segmentation and clustering
is used to group intervals of speech spoken by the same speaker. Moreover, those clusters can be
employed for model adaptation for, e.g., male versus female speakers, or individual speakers.
The automatic speech recognition generates a 1-best transcript, in this case using acoustic mo-
dels trained on 79 hours of read and spontaneous speech, and language models trained on 500
million words, mainly from newspaper texts. For each individual item, topic specific language
models were created by interpolating item-specific models with a general background model.
With state-of-the-art systems for speech activity detection and speaker clustering the automatic
speech recognition output showed an average word error rate (WER) of 60.6%. Note that in
comparison with ASR performance of WERs between 10 and 20% on broadcast news, error
rates on the heterogeneous speech in the TRECVID data set (historical content, background
noise, spontaneous speech) are much higher. This is taken to be due to remaining mismatches
between the data and the models that were used.
</p>
<p>Improvement of the ASR performance in such heterogeneous domains is not trivial, however,
</p>
<p>6. The TRECVID 2007 data consists of content from the (Dutch) Sound and Vision Academia collection.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Relevance of ASR for the Automatic Generation of Keywords Suggestions for TV programs
</p>
<p>because it would require large amounts of text data that both match the characteristics of spon-
taneous speech and the topic of the broadcasts, which is hard to find. System performance is
comparable to ASR performance on other (non-Dutch) collections of more spontaneous speech
materials, such as the National Gallery of the Spoken Word collection (Hansen et al., 2005),
and the MALACH interview collection (Byrne et al., 2004).
</p>
<p>4 Experiment
</p>
<p>To evaluate the suitability of ASR transcripts for annotating TV programs with keywords from
a given thesaurus, we ran the experiment detailed below.
</p>
<p>4.1 Material
</p>
<p>The Academia collection of TV programs 7 has been cleared from intellectual property rights by
Sound and Vision in order to create an open accessible collection for educational and research
purposes. This collection is a subset of the Sound and Vision&#8217;s catalogue, and has therefore
been manually annotated by professional cataloguers. This set has been subject to the TREC-
VID 2007 competition. We use in this experiment a set of 110 documents from this Academia
collection, which has been processed by the ASR system described in section 3. In a second
step, to compare the results of the keywords suggestion based on this source with our previous
experiments, we also looked for the corresponding set of Website descriptions. Unfortunately,
we could only find 13 Website texts corresponding to Academia&#8217;s TV programs as some broad-
caster&#8217;s websites do not archive the descriptions of their programs. This low number is due to
the variety of genres that can be found in this corpus ; our previous experiments were taking
only documentaries into account, which have extensive textual descriptions that can date back
several years. Therefore the suitability of the websites&#8217; text as source for generating keywords
is highly correlated to the TV program&#8217;s genre.
</p>
<p>The keywords lists that are generated correspond to the controlled vocabulary used for manual
indexing at Sound and Vision : the GTAA thesaurus. GTAA is a Dutch acronym for &#8220;Com-
mon thesaurus for Audiovisual Archives&#8221; ; it contains about 160 000 terms, divided in different
facets. In this experiment, we only take into account keywords from the facet describing the
subject of a TV program, which contains about 3800 preferred terms and 2000 non-prefered
terms, i.e. other words that correspond to the same notion or term. We added to these sets a list
of synonyms computed from online dictionaries and the singular forms of the terms (which are
mostly represented in the plural form, following the recommendation of the ISO 2788 :1986),
from the CELEX lexical resource (Baayen et al., 1995). Our documents and thesaurus are in
Dutch.
</p>
<p>4.2 Evaluation Metrics
</p>
<p>The evaluation of the keywords suggestion based on the strict (i.e. classic) measure of precision
and recall is not doing justice to the usability of these lists : suggestions that are different from
</p>
<p>7. Dutch news magazines, science news, documentaries, educational programs and archival video, see http:
//www.academia.nl</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>V&#233;ronique Malais&#233;, Luit Gazendam, Willemijn Heeren, Roeland Ordelman, Hennie Brugman
</p>
<p>the manually assigned keywords are sometimes relevant. This can be compared with the typical
values of inter-cataloguer consistency, which range from 13% to 77% (with an average of 44%)
when a controlled vocabulary is used (Leininger, 2000) : two cataloguers would, to a large ex-
tend, not agree with each other&#8217;s annotations. The topology of disagreement shows that a portion
of these differences are small semantic differences. To reduce the shortcomings of an evalua-
tion based on a strict string-based comparison, we also performed a second type of evaluation
according to the research of Medelyan and Witten (Medelyan &amp; Witten, 2005). Their adapted
measure of precision and recall takes a semantic distance into account : suggested keywords
(extracted automatically) and manually assigned ones that have one thesaurus relationship bet-
ween them are also counted as correct. Hence, if the keyword selected by the cataloguer is Salsa
and we propose Latin American Music, we consider the suggestion as semantically correct.
</p>
<p>Hence we have used the following evaluation scheme : (1) classic and semantic precision and
recall of the ASR-based keywords suggestion against manually assigned ones (for 110 items),
(2) comparison of these precision and recall measures with the ones of the keywords suggestions
extracted from online websites related to the same TV programs (for 13 items only).
</p>
<p>4.3 Results
</p>
<p>Strict Precision and Recall of the ASR and Context Documents-based Keywords Sugges-
tions Table 1 shows the &#8220;strict&#8221; precision, recall and F-score of the keywords suggestions,
both the keywords generated from ASR transcripts and from context documents. Examples of
such keywords (translated from Dutch) are :
&#8211; Keywords from cataloguers : Plane Industry, Planes, Aviation ;
&#8211; Keywords from ASR : Men, Passports, Cities, Aiports, Lakes, Airplanes, Young men, Co-
</p>
<p>lor 8 ;
&#8211; Keywords from Context documents : Factories, Interview, Fighters (Planes), Hunters, Armed
</p>
<p>forces, Models, Airplanes, Seas, People&#8222; Documentaries, Bees, Bombardments, Cameras 9.
The measures are evaluated against the cataloguer&#8217;s manually assigned keywords. The table
displays the general trends of the Figure 1 10 : the values for the ranks 1, 3, 5 and 10. The
scores are quite low, but the results from the ASR files are comparable to the ones from the
context documents. These two sources for generating keywords have opposite shortcomings :
the ASR files are very long, hence jeopardizing the precision of the results, whereas the context
documents are very short, generating only few keywords 11 and leading to a low recall.
</p>
<p>Besides these three measures, we also evaluated the proportion of retrieved keywords : the num-
ber of keywords that should be proposed (i.e. the ones selected by the professional cataloguers)
and that indeed figure in the list of propositions. This time, it is much higher for the ASR set
than those based on the context documents. This proportion ranges, when not null, from 25% to
80% and is higher than the Apolda-based set for all but two cases. The Apolda-based set ranges
from 12.5 to 33%, except for one case where the annotation consists of only one keyword, which
happens to be part of the context document, and is null in 5 of the 13 cases. These numbers seem
to hint that the cataloguers take more inspiration in what is said in the TV program to annotate
than in the texts provided by broadcasters for describing them.
</p>
<p>8. Non exhaustive list.
9. Non exhaustive list.
</p>
<p>10. The Figure represents the values of the f-measure in percentages, for ranks one to ten.
11. A text of a few lines only has few chances to contain strings that refer to GTAA terms.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Relevance of ASR for the Automatic Generation of Keywords Suggestions for TV programs
</p>
<p>ASR @1 @3 @5 @10
precision 0.22 0.17 0.14 0.09
recall 0.07 0.13 0.18 0.24
F-score 0.11 0.15 0.16 0.14
Context documents @1 @3 @5 @10
precision 0.23 0.18 0.14 0.13
recall 0.02 0.06 0.08 0.22
F-score 0.04 0.09 0.10 0.16
</p>
<p>TABLE 1 &#8211; Classical evaluation of ASR and Context Documents based keywords, over 110
documents
</p>
<p>0
</p>
<p>0.05
</p>
<p>0.1
</p>
<p>0.15
</p>
<p>0.2
</p>
<p>0.25
</p>
<p>0.3
</p>
<p>0.35
</p>
<p>1 2 3 4 5 6 7 8 9 10
</p>
<p>Rank
</p>
<p>F-s
co
</p>
<p>re
</p>
<p>ASR classic F-score
</p>
<p>ASR semantic F-score
</p>
<p>Context  classic F-score
</p>
<p>Context  semantic F-
score
</p>
<p>FIGURE 1 &#8211; The f-measure for the ASR-based keywords and context document-based key-
words, over 110 documents
</p>
<p>Precision and Recall for the Two Sets of Extracted Keywords with Semantic Distance
Table 2 shows that we improve the results when we allow a distance of one thesaurus rela-
tionship between the extracted keyword and the manual annotation of reference. If we look for
example at the precision and recall @5 of the ASR-based keywords, we see that on average 1
in 5 suggestions is semantically correct and that we retrieve one third of the keywords in the
catalogue description. We get similar results for the set generated from the context documents,
despite the fact that the context documents on average are less than a tenth of the ASR text in
length.
</p>
<p>These results are worse than the ones of previous experiments on context documents (Malais&#233;
et al., 2007) ; the reason for this low performance is the fact that the context documents taken
into account here are short, hence giving few hits and few context to each other to generate a
good ranking. The ASR have the inverse shortcoming : the files are very long, hence, the ranked
precision and recall achieve about the same score as the context documents-based keywords
suggestion. In the latter case, a more fine grained ranking algorithm would possibly give better
results. But in either case, it is quite interesting to see that in this realistic setting (for most ar-
chived programs the available context documents are short), the ASR gives results of equivalent
quality as the context documents. ASR is also an interesting alternative for the generation of
keywords, even though it is error prone, because it enables to generate annotations from the tv
programs&#8217; content itself, not on context information which might not be archived or accessible
anymore.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>V&#233;ronique Malais&#233;, Luit Gazendam, Willemijn Heeren, Roeland Ordelman, Hennie Brugman
</p>
<p>ASR @1 @3 @5 @10
semantic precision 0.25 0.23 0.21 0.17
semantic recall 0.10 0.21 0.31 0.44
semantic F-score 0.15 0.22 0.25 0.25
Context documents @1 @3 @5 @10
semantic precision 0.23 0.28 0.25 0.20
semantic recall 0.03 0.12 0.16 0.39
semantic F-score 0.05 0.17 0.20 0.26
</p>
<p>TABLE 2 &#8211; Semantic evaluation of ASR and Context Documents based keywords, over 13
documents
</p>
<p>Discussion The fact that the precision and recall figures are equivalent for the two sets is
due to the fact that the ASR is producing long lists of possible keywords that contain a small
number of correct ones, whereas this proportion is higher for the context documents, although
they generate only small lists. In the case of ASR, the challenge lies in finding the most relevant
(and correct) suggestions at the top of the list, as the ASR-based keyword suggestions contains
a higher number of keywords from the reference set (made by the cataloguers). This observation
stresses the fact that the ranking algorithm is the crucial part of our keyword suggestion pipeline,
especially in the case of using ASR as a basis for keyword suggestions.
</p>
<p>5 Conclusion
</p>
<p>In this paper we evaluated the suitability of automatic speech recognition transcripts as a textual
source for automatic keyword suggestion. This approach has the benefit that it can also be used
for keyword suggestion when audiovisual documents cannot easily be connected to collateral
textual context, e.g., on the web. A possible caveat when using speech recognition technology
could be that transcription accuracy varies a lot across collections, ranging form 10&#8722;20% Word
Error Rate in the broadcast news domain to above 50% in more difficult domains (heterogeneous
spontaneous speech, historical recordings). Especially for the latter category, transcript error
rates might simply be too high to be useful for keyword suggestion.
</p>
<p>To get an idea of the value of ASR transcripts for our purpose we compared its use with that
of available text documents associated with the broadcasts. The analysis of context documents
showed good results in a previous study on documentary programs. For these documentaries,
the context documents were broadly available, each containing also a lot of relevant information
for our purpose. However, we have seen that this might not always be the case : for the Academia
collection we were able to find only 13 context documents for our corpus of 110 broadcasts.
Furthermore, the average length of these context documents was much shorter than for the study
on documentary programs. It shows that in practice it is hard to find textual data usable for the
suggestion of keywords for audiovisual documents.
</p>
<p>The present experiment showed that the actual value of the context documents for keyword
suggestion was not better than ASR output. Although the context documents attain similar per-
formance with much less information, the total amount of good suggestions contained by the
ASR lists is larger. The problem is that the number of wrong suggestions is also larger, so the
ratio&#8217;s in terms of precision, recall and F-score are the same for the context documents and the</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Relevance of ASR for the Automatic Generation of Keywords Suggestions for TV programs
</p>
<p>ASR output. This means that there is ground for improvement in the form of better ranking al-
gorithms. This is an interesting option for further research. Another way of improvement could
be optimization of the ASR engine itself. The system setup used for these experiments was not
optimized towards recognition of the GTAA thesaurus terms.
</p>
<p>There is also ground for improvement for the context documents, but the possible gain is smaller
as the length of their suggestion lists is smaller : the recall of their total suggestion list is lower
than for the ASR. In general we can conclude that ASR transcripts seem to be a useful alternative
for keyword suggestion. Even though the precision and recall numbers are generally not very
high, they still seem valuable, especially when no other text sources are available, which actually
may be quite common. For situations where long context documents can be found, keyword
extraction and ranking performed on these context documents should still be preferred.
</p>
<p>Acknowledgements
</p>
<p>This paper is based on research funded by the NWO program CATCH (http://www.nwo.
nl/catch) and the EU FP7 project Living Web Archives (http://liwa-project.eu).
</p>
<p>R&#233;f&#233;rences
</p>
<p>BAAYEN R. H., PIEPENBROCK R. &amp; GULIKERS L. (1995). The CELEX Lexical Database.
Philadelphia, PA : Linguistic Data Consortium, University of Pennsylvania, (release 2) [cd-
rom] edition.
</p>
<p>BRUGMAN H., MALAIS&#201; V., GAZENDAM L. &amp; SCHREIBER G. (2008). The documentalist
support system : a web-services based tool for semantic annotation and browsing. Semantic
Web Challenge track of the International Semantic Web Conference 2008 (ISWC 2008).
</p>
<p>BYRNE W., D.DOERMANN, FRANZ M., GUSTMAN S., HAJIC J., OARD D., PICHENY M.,
PSUTKA J., RAMABHADRAN B., SOERGEL D., WARD T. &amp; ZHU W.-J. (2004). Automa-
tic recognition of spontaneous speech for access to multilingual oral history archives. IEEE
Transactions on Speech and Audio Processing, 12(4), 420 &#8211; 435.
CIRAVEGNA F. &amp; WILKS Y. (2003). Annotations for the Semantic Web, volume 1, chapter
Designing Adaptive Information Extraction for the Semantic Web in Amilcare. IOS press.
</p>
<p>GAZENDAM L., MALAIS&#201; V., DE JONG A., WARTENA C., BRUGMAN H. &amp; SCHREIBER
G. (2009). Automatic annotation suggestions for audiovisual archives : Evaluation aspects.
Interdisciplinary Science Reviews, p. In press.
</p>
<p>GAZENDAM L., MALAIS&#201; V., SCHREIBER G. &amp; BRUGMAN H. (2006). Deriving semantic
annotations of an audiovisual program from contextual texts. In Proceedings of First Interna-
tional workshop on Semantic Web Annotations for Multimedia (SWAMM 2006).
</p>
<p>HANSEN J., HUANG R., ZHOU B., DEADLE M., DELLER J., GURIJALA A. R., KURIMO
M. &amp; ANGKITITRAKUL P. (2005). Speechfind : Advances in spoken document retrieval for
a national gallery of the spoken word. IEEE Transactions on Speech and Audio Processing,
13(5), 712&#8211;730.
HUIJBREGTS M. (2008). Segmentation, Diarization and Speech Transcription : surprise data
unraveled. PhD thesis, University of Twente.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>V&#233;ronique Malais&#233;, Luit Gazendam, Willemijn Heeren, Roeland Ordelman, Hennie Brugman
</p>
<p>HUIJBREGTS M., ORDELMAN R. &amp; DE JONG F. (2007). Annotation of heterogeneous mul-
timedia content using automatic speech recognition. In Proceedings of SAMT 2007, Genova,
Italy.
</p>
<p>KAHAN J. &amp; KOIVUNEN M.-R. (2001). Annotea : an open rdf infrastructure for shared web
annotations. In World Wide Web, p. 623&#8211;632.
</p>
<p>KIRYAKOV A., POPOV B., TERZIEV I., MANOV D. &amp; OGNYANOFF D. (2005). Semantic
annotation, indexing, and retrieval. Journal of Web Semantics, 2(1), 49&#8211;79.
KOHLER J., LARSON M., DE JONG F., KRAAIJ W. &amp; ORDELMAN R. (2008). Spoken content
retrieval : Searching spontaneous conversational speech. ACM SIGIR Forum, 42(2), 67&#8211;76.
LEININGER K. (2000). Inter-indexer consistency in psycinfo. Journal of Librarianship and
Information Science, 32(1), 4&#8211;8.
LESPINASSE K. &amp; BACHIMONT B. (2001). Is peritext a key for audiovisual documents ?
the use of texts describing television programs to assist indexing. In CICLing &#8217;01 : Procee-
dings of the Second International Conference on Computational Linguistics and Intelligent
Text Processing, p. 505&#8211;506, London, UK : Springer-Verlag.
</p>
<p>MALAIS&#201; V., GAZENDAM L. &amp; BRUGMAN H. (2007). Disambiguating automatic semantic
annotation based on a thesaurus structure. In 14e conference sur le Traitement Automatique
des Langues Naturelles (TALN).
</p>
<p>MEDELYAN O. &amp; WITTEN I. H. (2005). Thesaurus-based index term extraction for agricul-
tural documents. In Proceedings of the 6th Agricultural Ontology Service (AOS) workshop at
EFITA/WCCA.
</p>
<p>SMEULDERS A. W. M., WORRING M., SANTINI S., GUPTA A. &amp; JAIN R. (2000). Content-
based image retrieval at the end of the early years. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 22(12), 1349&#8211;1380.</p>

</div></div>
</body></html>