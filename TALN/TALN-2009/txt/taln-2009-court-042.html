<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
"http://www.w3.org/TR/html4/loose.dtd">
<html><head><title>Une approche exploratoire de compression automatique de phrases bas&#233;e sur des crit&#232;res thermodynamiques</title>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
</head>
<body>
<div style="page-break-before:always; page-break-after:always"><div><p>TALN 2009 &#8211; Session posters , Senlis, 24&#8211;26 juin 2009
</p>
<p>Une approche exploratoire de compression automatique de
phrases bas&#233;e sur des crit&#232;res thermodynamiques
</p>
<p>Silvia Fern&#225;ndez Sabido1,2 Juan-Manuel Torres-Moreno1
</p>
<p>(1) Laboratoire Informatique d&#8217;Avignon, BP 1228 84911 Avignon
(2) Laboratoire de Physique de Mat&#233;riaux, UHP-Nancy, 54506 Vand&#339;uvre
</p>
<p>{silvia.fernandez, juan-manuel.torres}@univ-avignon.fr
</p>
<p>R&#233;sum&#233;. Nous pr&#233;sentons une approche exploratoire bas&#233;e sur des notions thermodyna-
miques de la Physique statistique pour la compression de phrases. Nous d&#233;crivons le mod&#232;le
magn&#233;tique des verres de spins, adapt&#233; &#224; notre conception de la probl&#233;matique. Des simula-
tions M&#233;tropolis Monte-Carlo permettent d&#8217;introduire des fluctuations thermiques pour piloter
la compression. Des comparaisons int&#233;ressantes de notre m&#233;thode ont &#233;t&#233; r&#233;alis&#233;es sur un cor-
pus en fran&#231;ais.
</p>
<p>Abstract. We present an exploratory approach based on thermodynamic concepts of Sta-
tistical Physics for sentence compression. We describe the magnetic model of spin glasses, well
suited to our conception of problem. The Metropolis Monte-Carlo simulations allow to intro-
duce thermal fluctuations to drive the compression. Interesting comparisons of our method were
performed on a French text corpora.
</p>
<p>Mots-cl&#233;s : Compression de phrases, R&#233;sum&#233; automatique, R&#233;sum&#233; par extraction, Ener-
tex, M&#233;canique statistique.
</p>
<p>Keywords: Sentence Compression, Automatic Summarization, Extraction Summariza-
tion, Enertex, Statistical Mechanics.
</p>
<p>1 Introduction
</p>
<p>La compression d&#8217;une phrase consiste en la suppression de certains de ses constituants non es-
sentiels avec le but d&#8217;obtenir une phrase plus courte en conservant le sens et la grammaticalit&#233;.
Il existe deux grandes approches pour la compression de phrases : l&#8217;approche linguistique qui
consiste &#224; d&#233;finir des r&#232;gles et celle statistique qui utilise un corpus pour d&#233;tecter des r&#233;gula-
rit&#233;s afin de produire automatiquement les r&#232;gles. Nous pr&#233;sentons une approche statistique-
thermodynamique pour la compression de phrases en fran&#231;ais. L&#8217;id&#233;e est d&#8217;&#233;tablir une concor-
dance entre la compression d&#8217;une phrase &#224; N termes et le processus par lequel, une cha&#238;ne de
N spins magn&#233;tiques, tous orient&#233;s initialement vers le haut (tous les termes sont pr&#233;sents),
subissent des fluctuations thermiques qui inversent quelques spins (suppression de quelques
termes). Un tel syst&#232;me poss&#232;de 2N configurations o&#249; seulement un petit sous-ensemble cor-
respond aux compressions acceptables de la phrase initiale. R&#233;duire un espace si &#233;norme, tout
en favorisant les configurations correctes, est le d&#233;fi commun aux m&#233;thodes de compression.
Nous proposons d&#8217;utiliser les interactions entre termes voisins pour contr&#244;ler leurs retourne-
ments et r&#233;duire l&#8217;espace des configurations. Ces couplages seront mesur&#233;s sur un corpus align&#233;</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Silvia Fern&#225;ndez Sabido, Juan-Manuel Torres-Moreno
</p>
<p>de phrases compl&#232;tes/compress&#233;es.
</p>
<p>En Section 2 nous faisons un parcours des m&#233;thodes statistiques de compression de phrases. En
Section 3 nous d&#233;crivons le mod&#232;le magn&#233;tique des verres de spins. Nous pr&#233;sentons en Section
4 notre strat&#233;gie pour calculer le couplage entre les termes. Les simulations Monte-Carlo, qui
nous ont permis d&#8217;introduire des fluctuations thermiques, seront aussi d&#233;crites. Enfin, avant de
conclure, nous pr&#233;sentons en Section 5 l&#8217;&#233;valuation de notre algorithme.
</p>
<p>2 La compression statistique de phrases
</p>
<p>Le mod&#232;le du canal bruit&#233; (Knight &amp; Marcu, 2000) consid&#232;re que la compression c est la phrase
originale, &#224; laquelle a &#233;t&#233; ajout&#233; du bruit pour produire une phrase longue l. Le mod&#232;le est
constitu&#233; d&#8217;une source P (c) o&#249; les phrases bien form&#233;es ont la plus grande probabilit&#233; ; du
canal P (l/c), qui privil&#233;gie les phrases en pr&#233;servant l&#8217;information essentielle ; et de P (c/l) le
d&#233;codeur. Celui-ci cherche la meilleure compression : la phrase c qui maximise P (c/l). Ces
probabilit&#233;s sont appliqu&#233;es aux arbres syntaxiques repr&#233;sentant les phrases. La m&#233;thode des
arbres de d&#233;cision (Knight &amp; Marcu, 2000) part d&#8217;un arbre repr&#233;sentant la structure d&#8217;une phrase
en produisant un autre plus petit, correspondant &#224; la compression. (Jing, 2000) utilise plusieurs
sources de connaissance : la syntaxe, le contexte et l&#8217;analyse statistique d&#8217;un corpus. L&#8217;id&#233;e est
de supprimer les &#233;l&#233;ments qui ne sont pas porteurs du sujet principal du document.
</p>
<p>L&#8217;analyse syntaxique a &#233;t&#233; privil&#233;gi&#233;e pour d&#233;terminer les &#233;l&#233;ments dont leur disparition affecte-
ront le moins le sens et la grammaticalit&#233; des phrases. Il existe des &#233;tudes qui se sont pass&#233;es des
arbres syntaxiques et qui ont obtenu des r&#233;sultats comparables. On trouve le travail de (Nguyen
et al., 2004) bas&#233; sur des templates de traduction. Il consid&#232;re que les phrases non compress&#233;es
sont &#233;crites dans une langue source et les phrases compress&#233;es dans une langue cible. Un cor-
pus align&#233; de phrases compl&#232;tes/compress&#233;es est utilis&#233; pour g&#233;n&#233;rer des r&#232;gles qui consid&#232;rent
les similarit&#233;s entre phrases comme constantes et les diff&#233;rences comme variables. Le syst&#232;me
ENTROPIE (Waszak &amp; Torres-Moreno, 2008) utilise aussi un corpus align&#233; pour apprendre un
mod&#232;le de langage qui sert &#224; d&#233;terminer quels termes ont une forte probabilit&#233; d&#8217;&#234;tre supprim&#233;s.
Ce choix est r&#233;alis&#233; en utilisant des crit&#232;res entropiques.
</p>
<p>3 Les verres de spin
</p>
<p>Les verres de spins sont des mat&#233;riaux constitu&#233;s d&#8217;unit&#233;s magn&#233;tiques dont leurs interactions
sont soit positives soit n&#233;gatives, de fa&#231;on al&#233;atoire. Si le couplage entre deux spins est positif,
ils ont tendance &#224; s&#8217;orienter vers la m&#234;me direction (interaction ferromagn&#233;tique). Par contre,
si le couplage entre eux est n&#233;gatif ils auront tendance &#224; s&#8217;orienter en sens oppos&#233;s (interaction
antiferromagn&#233;tique). Il existe donc une comp&#233;tition locale entre ces forces. Les spins peuvent
se r&#233;v&#233;ler incapables de satisfaire simultan&#233;ment les interactions contradictoires auxquelles ils
sont soumis par leurs voisins. Ce comportement peut donner lieu &#224; ce qu&#8217;on appelle la frustra-
tion.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Une approche thermodynamique de compression automatique de phrases
</p>
<p>3.1 Le texte vu comme un verre textuel
</p>
<p>Un terme peut &#234;tre vu comme un spin &#224; deux &#233;tats : &#8593; (+1) indiquant sa pr&#233;sence dans une
phrase ou &#8595; (-1) son absence (Fern&#225;ndez et al., 2007). Une phrase de N termes sera donc co-
d&#233;e comme une cha&#238;ne de N spins orient&#233;s vers le haut, et sa compression correspond &#224; la
m&#234;me cha&#238;ne o&#249; quelques spins ont chang&#233; d&#8217;orientation vers le bas. Le calcul d&#8217;&#233;nergie tex-
tuelle (Fern&#225;ndez et al., 2008) dans un tel syst&#232;me &#233;tabli une connectivit&#233; totale entre couples
de termes. Or, dans notre mod&#232;le nous limitons les interactions aux couples de voisins proches.
La dimension de chaque vecteur est donc le nombre de termes de la phrase repr&#233;sent&#233;e. Le sys-
t&#232;me de compression de phrases que nous proposons utilise un corpus align&#233; de phrases com-
pl&#232;tes/compress&#233;es en fran&#231;ais 1. Nous avons mesur&#233; les couplages entre termes adjacents. Pour
supprimer les termes accessoires tout en gardant ceux pertinents, il faut avoir des interactions
positives et n&#233;gatives. Par exemple, si l&#8217;on veut que la maison rouge soit compress&#233;e en la mai-
son, les interactions Ji,j entre termes voisins doivent &#234;tre : Jla,maison = +x et Jmaison,rouge = &#8722;y.
La vari&#233;t&#233; en valeur et en signe des interactions entre termes produit des comp&#233;titions internes
dans la phrase. Cette situation fait du syst&#232;me une sorte de verre de spins ou verre textuel.
</p>
<p>4 Calcul des r&#232;gles d&#8217;interaction
</p>
<p>Le corpus MYRIAM est compos&#233; de 219 phrases issues de sources journalistiques. Pour chaque
phrase, une version compress&#233;e a &#233;t&#233; produite manuellement. Nous avons &#233;clat&#233; le corpus en
deux ensembles : 80% pour l&#8217;apprentissage des couplages et 20% pour les tests de compression.
Nous avons d&#233;duit des relations Jtermei,termej entre les termes voisins i et j, selon leurs &#233;tats dans
les versions compress&#233;es des phrases. Par exemple dans la phrase suivante, quelques termes ont
disparu pendant la compression (leurs &#233;tats ont chang&#233; de &#8593; &#224; &#8595;) :
</p>
<p>&#8595; &#8595; &#8593; &#8593; &#8595; &#8595; &#8595; &#8593; &#8593; &#8593;
Enfin, , je souhaite &#224; notre terre la paix .
</p>
<p>Nous avons d&#233;duit les r&#232;gles suivantes entre termes adjacents : Jtermei,termej = +1 (ferromagn&#233;-
tique) si les deux termes sont pr&#233;sents ou absents ; Jtermei,termej = &#8722;1 (antiferromagn&#233;tique) si l&#8217;un des
termes est pr&#233;sent et l&#8217;autre absent. Ces r&#232;gles indiquent &#224; chaque terme de suivre ou pas l&#8217;orienta-
tion de ses voisins. Or, en raison de la petite taille du corpus, une grande partie du vocabulaire
des phrases de test n&#8217;existe pas dans le corpus d&#8217;apprentissage et, par cons&#233;quent, aucune r&#232;gle
ne les concerne. Pour surmonter les probl&#232;mes li&#233;s au manque de termes, nous avons d&#233;cid&#233;
de grouper les termes selon leur cat&#233;gorie grammaticale. Cette strat&#233;gie nous a permis d&#8217;&#233;lar-
gir la validit&#233; des r&#232;gles obtenues. TreeTagger (Schmid, 1994) &#233;tiquette les termes selon leurs
cat&#233;gories grammaticales pour produire des relations du type :
</p>
<p>&#8211; Jmais,partout &#8594; JPREP , ADV
&#8211; Jfait,sentir &#8594; JVERB :PRE , VERB :INF
&#8211; June,fois &#8594; JART , NOM
&#8211; Jla,p&#233;nurie &#8594; JART , NOM
Ainsi, on regroupe plusieurs r&#232;gles en une seule qui repr&#233;sente le couplage entre deux types
de termes. Nous avons choisi d&#8217;en utiliser leur valeur moyenne. Par exemple, June,fois = +1 et
</p>
<p>1. Le corpus MYRIAM d&#233;velopp&#233; par Michel Gagnon &#224; l&#8217;&#201;cole Polytechnique de Montr&#233;al.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Silvia Fern&#225;ndez Sabido, Juan-Manuel Torres-Moreno
</p>
<p>Jla,p&#233;nurie = +2, alors JART,NOM = +1.5. Nous avons ainsi obtenu 400 relations entre &#233;tiquettes
grammaticales. Nous avons appliqu&#233; les r&#232;gles apprises pour compresser les phrases du corpus
de test. Le tableau 1 montre un exemple de cette application. On observe les 7 valeurs de cou-
plages entre voisins proches. Appliqu&#233;s sur la phrase originale, ces couplages produisent une
compression acceptable. Malheureusement ce n&#8217;est pas le cas pour toutes les autres. M&#234;me en
</p>
<p>JKON,ADV = +0, 6154
JADV,DET = &#8722;0, 2381
JDET,NOM = +1, 1725
JNOM,PRO = +0, 4500
JPRO,V ER = +1, 0000
JV ER,V ER = +1, 0000
JV ER,SENT = +1, 0000
</p>
<p>Conf. &#8593; &#8593; &#8593; &#8593; &#8593; &#8593; &#8593; &#8593;
initiale Mais partout la p&#233;nurie se fait sentir .
</p>
<p>KON ADV DET NOM PRO VER VER SENT
Etat &#8595; &#8595; &#8593; &#8593; &#8593; &#8593; &#8593; &#8593;
fond. la p&#233;nurie se fait sentir .
</p>
<p>TABLE 1 &#8211; Application du couplage entre &#233;tiquettes grammaticales pour la phrase du tableau ??. Nous avons les
sept valeurs des couplages qui produisent une compression bien acceptable de la phrase.
</p>
<p>ayant toutes les valeurs d&#8217;&#233;change permettant d&#8217;obtenir les &#233;tats fondamentaux, nous serions
confront&#233;s &#224; deux probl&#232;mes : i) Les sous-phrases obtenues avec les &#233;tats fondamentaux ne sont
pas syst&#233;matiquement des bonnes compressions. Cet effet peut &#234;tre li&#233; &#224; la petite taille du cor-
pus d&#8217;apprentissage qui produit des r&#232;gles rigides. ii) La frustration (impossibilit&#233; de satisfaire
toutes les r&#232;gles d&#8217;&#233;change) est pr&#233;sente dans 13% des phrases de test. Dans ce cas, il y a plus
d&#8217;une solution pour une m&#234;me phrase. Les simulations du type M&#233;tropolis Monte-Carlo nous
ont permis d&#8217;introduire des fluctuations thermiques qui apporteront de la flexibilit&#233; &#224; l&#8217;applica-
tion des r&#232;gles et d&#8217;utiliser le recuit simul&#233; pour faire face &#224; la frustration des verres textuels.
</p>
<p>4.1 Simulations M&#233;tropolis Monte-Carlo
</p>
<p>L&#8217;id&#233;e principale d&#8217;une simulation Monte-Carlo est d&#8217;imiter les fluctuations thermiques d&#8217;un
syst&#232;me qui parcourt plusieurs &#233;tats pendant une exp&#233;rience. La probabilit&#233; p&#181; de trouver le
syst&#232;me dans un &#233;tat &#181; est donn&#233;e par la distribution de Gibbs-Boltzmann : p&#181; &#8733; exp(&#8722;E&#181;/kT )
o&#249; E&#181; est l&#8217;&#233;nergie du syst&#232;me dans l&#8217;&#233;tat &#181;, k est la constante de Boltzmann et T la temp&#233;ra-
ture. Pour faire la transition entre &#233;tats nous avons utilis&#233; la dynamique de M&#233;tropolis :
</p>
<p>1. Soit une cha&#238;ne de N spins dans un &#233;tat initial &#181; d&#8217;&#233;nergie E&#181;. &#192; chaque pas de la simu-
lation, (on fait N pas afin de donner &#224; tous les spins la possibilit&#233; de se retourner), choisir
un spin au hasard dont le retournement am&#232;ne &#224; un nouvel &#233;tat &#957; d&#8217;&#233;nergie E&#957; ;
</p>
<p>2. calculer &#8710;E = E&#957; &#8722;E&#181; pour savoir si un tel retournement (flip) de spin fait diminuer ou
augmenter l&#8217;&#233;nergie du syst&#232;me ;
&#8211; si l&#8217;&#233;nergie diminue (&#8710;E &lt; 0), on accepte de mani&#232;re d&#233;finitive le flip car l&#8217;&#233;tat produit
</p>
<p>est plus stable que le pr&#233;c&#233;dant ;
&#8211; si l&#8217;&#233;nergie augmente (&#8710;E &gt; 0), on g&#233;n&#232;re un num&#233;ro al&#233;atoire r, tel que 0 &#8804; r &#8804; 1 ;
</p>
<p>et on le compare avec le facteur exp(&#8722;&#8710;E/kT ) ;
&#8211; si r &lt; exp(&#8722;&#8710;E/kT ) on accepte le flip, autrement, on reste dans le m&#234;me &#233;tat &#181;.
</p>
<p>3. r&#233;p&#233;ter la simulation un nombre suffisant de fois, pour permettre au syst&#232;me d&#8217;atteindre
l&#8217;&#233;quilibre &#224; une temp&#233;rature &#233;tablie.
</p>
<p>4. quand le syst&#232;me est d&#233;j&#224; &#224; l&#8217;&#233;quilibre on pourrait commence la mesure des valeurs
d&#8217;expectation (moyennes temporales) de quantit&#233;s comme l&#8217;&#233;nergie, la magn&#233;tisation,
la chaleur sp&#233;cifique. On aura une valeur pour chaque temp&#233;rature.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Une approche thermodynamique de compression automatique de phrases
</p>
<p>&#202;tre en &#233;quilibre signifie que le syst&#232;me ne fera plus de transitions importantes et la valeur de
l&#8217;&#233;nergie devient quasi constante. Les conditions des simulations ont &#233;t&#233; les suivantes.
</p>
<p>&#201;tat initial : ferromagn&#233;tique o&#249; tous les termes sont pr&#233;sents.
Spins fix&#233;s : afin de ne pas confondre une configuration donn&#233;e avec sa configuration sym&#233;-
</p>
<p>trique de m&#234;me &#233;nergie, nous avons fix&#233; dans l&#8217;&#233;tat &#8593; le symbole de ponctuation final qui
ne dispara&#238;t jamais. Nous avons fix&#233; aussi un spin dans l&#8217;&#233;tat &#8595;. Pour choisir l&#8217;&#233;l&#233;ment
avec la possibilit&#233; la plus haute de dispara&#238;tre, nous avons introduit un indice de suppres-
sion (IS) :IS(termej,i) =
</p>
<p>&#8721;P
i=1
</p>
<p>ns(termej,i)
</p>
<p>|phri| o&#249; ns(termej,i) est le nombre de fois que le
terme j a &#233;t&#233; supprim&#233; de la phrase i, et |phri| est le nombre de termes de la phrase. La
somme parcourt les P phrases du corpus d&#8217;apprentissage. Le spin qui sera fix&#233; &#8595; dans la
configuration initiale correspond au terme d&#8217;IS plus &#233;lev&#233;.
</p>
<p>Temp&#233;rature : elle prend des valeurs allant de 1 &#224; 0 en pas de 0,01.
Frustration : pour en faire face, nous avons utilis&#233; la technique du recuit simul&#233;.
Nombre d&#8217;it&#233;rations : 1 000 it&#233;rations par temp&#233;rature.
Les configurations retenues : nous r&#233;cup&#233;rons les &#233;tats finaux &#224; chaque temp&#233;rature. Cela
</p>
<p>produit un ensemble de variantes de compression de la phrase initiale. Nous avons uti-
lis&#233; deux crit&#232;res diff&#233;rents pour choisir les meilleures compressions des phrases, &#224; sa-
voir : l&#8217;&#233;tat d&#8217;&#233;nergie minimale et la magn&#233;tisation maximale (qui correspond au taux de
compression minimum). L&#8217;&#233;nergie d&#8217;une cha&#238;ne de spins est E = &#8722;1
</p>
<p>2
</p>
<p>&#8721;
i,j sisjJi,j et la
</p>
<p>magn&#233;tisation M =
&#8721;
</p>
<p>i si, o&#249; si est l&#8217;&#233;tat du spin i.
</p>
<p>5 &#201;valuation de la compression
</p>
<p>Le syst&#232;me Bilingual Language Evaluation Understudy (BLEU) (Papineni et al., 2001) mesure
la concordance entre une phrase candidate (faite par un syst&#232;me) et une r&#233;f&#233;rence (faite par un
humain). Les tableaux 2 et 3 montrent les scores BLEU obtenus par notre syst&#232;me sans et avec
recuit simul&#233;. Nous utilisons des n-grammes (n = 3, 4) tel que sugg&#233;r&#233; par (Papineni et al.,
2001). Pour la plupart des simulations, le crit&#232;re de magn&#233;tisation maximale obtient des scores
plus &#233;lev&#233;s que celui d&#8217;&#233;nergie minimale. Nous avons r&#233;alis&#233; trois simulations (s1, s2 et s3
aux tableaux). Nous comparons nos r&#233;sultats avec ceux produits par ENTROPIE de (Waszak &amp;
Torres-Moreno, 2008). Nous ajoutons aussi une baseline construite &#224; partir d&#8217;une simulation
o&#249; les couplages sont des valeurs al&#233;atoires entre -1 et +1. Plus les valeurs BLEU sont &#233;lev&#233;es,
plus les compressions candidates sont proches du mod&#232;le de r&#233;f&#233;rence. Les scores obtenus par
notre syst&#232;me sont l&#233;g&#232;rement sup&#233;rieurs &#224; ceux obtenus par ENTROPIE. Le recuit simul&#233; ne
semble pas avoir un effet significatif dans le r&#233;sultat. BLEU est d&#8217;avantage une mesure de la
</p>
<p>Deux spins fix&#233;s : symbol de ponctuation final (&#8593;) et terme d&#8217;ISmax (&#8595;)
Crit&#232;re : &#233;nergie minimale
</p>
<p>Unit&#233; Baseline ENTROPIE VERRE TEXTUEL VERRE TEXTUEL
BLEU avec recuit
</p>
<p>s1 s2 s3 s1 s2 s3
3-gramme 0,3767 0,7479 0,7470 0,6676 0,7200 0,7083 0,7446 0,7337
4-gramme 0,2990 0,7018 0,7158 0,6319 0,6936 0,6821 0,7150 0,7057
</p>
<p>TABLE 2 &#8211; Scores BLEU pour notre syst&#232;me VERRE TEXTUEL utilisant le crit&#232;re d&#8217;&#233;nergie minimale. Pour
chaque simulation si, nous montrons aussi les r&#233;sultats pour le syst&#232;me ENTROPIE propos&#233; par (Waszak &amp; Torres-
Moreno, 2008) et une baseline o&#249; les valeurs des couplages Ji,j sont des valeurs al&#233;atoires entre -1 et +1.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Silvia Fern&#225;ndez Sabido, Juan-Manuel Torres-Moreno
</p>
<p>Deux spins fix&#233;s : symbol de ponctuation final (&#8593;) et terme d&#8217;ISmax (&#8595;)
Crit&#232;re : magn&#233;tisation maximale
</p>
<p>Unit&#233; Baseline ENTROPIE VERRE TEXTUEL VERRE TEXTUEL
BLEU avec recuit
</p>
<p>s1 s2 s3 s1 s2 s3
3-gramme 0,3767 0,7479 0,7560 0,7597 0,7527 0,7331 0,7567 0,7381
4-gramme 0,2990 0,7018 0,6715 0,7097 0,7058 0,6825 0,7064 0,6836
</p>
<p>TABLE 3 &#8211; Scores BLEU pour notre syst&#232;me VERRE TEXTUEL utilisant le crit&#232;re de magn&#233;tisation maximale.
Pour chaque simulation si, nous montrons aussi les r&#233;sultats pour le syst&#232;me ENTROPIE propos&#233; par (Waszak &amp;
Torres-Moreno, 2008) et une baseline o&#249; les valeurs des couplages Ji,j sont des valeurs al&#233;atoires entre -1 et +1.
</p>
<p>pertinence de l&#8217;information que de la qualit&#233; grammaticale des textes. Nous pouvons dire que
notre syst&#232;me produit des compressions dans lesquelles l&#8217;information essentielle est conserv&#233;e.
Or pour v&#233;rifier la qualit&#233; des phrases une &#233;valuation manuelle s&#8217;av&#232;re n&#233;cessaire. Pour avoir
un aper&#231;u de la performance globale des syst&#232;mes, nous avons calcul&#233; le nombre de phrases
compress&#233;s correctes, le nombre de phrases compress&#233;s incorrectes et le nombre de phrases
non compress&#233;s pour chaque syst&#232;me. Les r&#233;sultats sont pr&#233;sent&#233;s au tableau 4. Des exemples
</p>
<p>% des phrases non % des phrases % des phrases
Syst&#232;me compress&#233;es compress&#233;es correctes compress&#233;es incorrectes
</p>
<p>ENTROPIE &#8776; 30% &#8776; 30% &#8776; 40%
VERRE TEXT. &#8776; 40% &#8776; 40% &#8776; 20%
</p>
<p>Baseline &#8776; 5% &#8776; 5% &#8776; 90%
</p>
<p>TABLE 4 &#8211; Pourcentages de phrases du corpus qui ont &#233;t&#233; compress&#233;es par le syst&#232;me ENTROPIE et VERRE
TEXTUEL pendant une simulation.
</p>
<p>de compressions sont montr&#233;es dans le tableau 5. Il est important de signaler que le corpus
utilis&#233; contient des expressions agrammaticales d&#251; &#224; l&#8217;&#233;clatement de certains contractions (par
exemple &#224; les &#224; la place de aux, de le au lieu de du). Cette s&#233;paration de termes se trouve sur
le corpus d&#8217;origine o&#249; ses auteurs ont eu l&#8217;id&#233;e de permettre aux m&#233;thodes de compresser au
maximum les phrases. Notre processus de compression exploite cette propri&#233;t&#233; afin de capturer
plus finement les interactions des tous les termes. D&#8217;o&#249; la pr&#233;sence de, par exemple, de et ici &#224;
la place de d&#8217;ici dans les exemples.
</p>
<p>Originale De ci de l&#224;, certains fabricants adoptent des mesures .
Humain certains fabricants adoptent des mesures .
</p>
<p>ENTROPIE de l&#224;, certains fabricants des mesures .
VERRE TEXTUEL 2 SPINS certains fabricants adoptent des mesures .
</p>
<p>Originale Moyennant quoi , la culture &quot; int&#233;gr&#233;e &quot; utilise beaucoup moins de intrants .
Humain la culture &quot; int&#233;gr&#233;e &quot; utilise moins de intrants .
</p>
<p>ENTROPIE la culture &quot; int&#233;gr&#233;e &quot; utilise moins de intrants .
VERRE TEXTUEL 2 SPINS Moyennant quoi , la culture &quot; int&#233;gr&#233;e &quot; utilise moins de intrants .
</p>
<p>TABLE 5 &#8211; Compressions g&#233;n&#233;r&#233;es par le syst&#232;me VERRE TEXTUEL. Nous montrons la phrase originale, la
compression faite par un humain et celle produite par ENTROPIE. En gras, la meilleure compression. Les termes
comme &#171; d&#8217;ici &#187; ont &#233;t&#233; separ&#233;s en &#171; de &#187; et &#171; ici &#187; pendant le processus de compression.
</p>
<p>Pendant ces exp&#233;riences, on a observ&#233; que le syst&#232;me ENTROPIE semble &#234;tre plus robuste pour
garder la grammaticalit&#233; que le n&#244;tre. Il est possible que cela soit d&#251; &#224; l&#8217;utilisation de bigrammes
et trigrammes de termes comme unit&#233; de base. Dans notre cas, nous nous sommes int&#233;ress&#233;s
uniquement &#224; l ?exploration des interactions des termes isol&#233;s (unigrammes).</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Une approche thermodynamique de compression automatique de phrases
</p>
<p>6 Conclusion
</p>
<p>Un syst&#232;me thermodynamique de compression de phrases en fran&#231;ais a &#233;t&#233; propos&#233;. Les phrases
sont cod&#233;es comme des cha&#238;nes de verres de spins. Les couplages entre les &#233;tiquettes gramma-
ticales des termes ont &#233;t&#233; calcul&#233;s sur un corpus d&#8217;apprentissage. Les phrases de test ont &#233;t&#233;
compress&#233;es en appliquant les couplages appris avec une dynamique thermique de M&#233;tropolis.
Pour chaque phrase et chaque temp&#233;rature cette approche g&#233;n&#232;re un ensemble de choix. Cet
ensemble est diff&#233;rente pour chaque simulation car le syst&#232;me n&#8217;est pas d&#233;terministe. Ce com-
portement est en accord avec la t&#226;che de compression de texte, qui n&#8217;a pas une solution unique.
Deux personnes ne compressent pas une phrase de la m&#234;me fa&#231;on, et plus encore, la m&#234;me
personne peut faire des compressions diff&#233;rentes &#224; des moments diff&#233;rents. Les compressions,
&#233;valu&#233;es par rapport &#224; celles faites par des humains, ont des scores BLEU comparables &#224; ceux
du syst&#232;me ENTROPIE. M&#234;me si le groupement par cat&#233;gorie grammaticale g&#233;n&#232;re des r&#232;gles
d&#8217;&#233;change g&#233;n&#233;rales qui produisent des compressions acceptables, nous pensons que le calcul
des couplages avec des termes &#224; longueur variable (1, 2 ou 3 mots) pourrait am&#233;liorer la qualit&#233;
de nos compressions. Des &#233;tudes sont actuellement en cours.
</p>
<p>Remerciements
</p>
<p>Ce travail a &#233;t&#233; realis&#233; en partie gr&#226;ce au financement du CONACYT (Mexique), bourse 175225.
</p>
<p>R&#233;f&#233;rences
</p>
<p>FERN&#193;NDEZ S., SANJUAN E. &amp; TORRES-MORENO J. M. (2007). Textual energy of asso-
ciative memories : performants applications of enertex algorithm in text summarization and
topic segmentation. In MICAI &#8217;07, Aguascalientes (Mexico), p. 861&#8211;871.
</p>
<p>FERN&#193;NDEZ S., SANJUAN E. &amp; TORRES-MORENO J. M. (2008). Enertex : un syst&#232;me bas&#233;
sur l&#8217;&#233;nergie textuelle. In TALN 2008, p. 99&#8211;108.
</p>
<p>JING H. (2000). Sentence reduction for automatic text summarization. In 6th Applied Natural
Language Processing Conference (ANLP&#8217;00), p. 310&#8211;315.
</p>
<p>KNIGHT K. &amp; MARCU D. (2000). Statistics-based summarization - step one : Sentence com-
pression. In AAAI/IAAI, p. 703&#8211;710.
</p>
<p>NGUYEN M. L., HORIGUCHI S., SHIMAZU A. &amp; HO B. T. (2004). Example-based sentence
reduction using the hidden markov model. ACM TALIP, 3(2), 146&#8211;158.
PAPINENI K., ROUKOS S., WARD T. &amp; ZHU W. (2001). Bleu : a method for automatic
evaluation of machine translation.
</p>
<p>SCHMID H. (1994). Probabilistic partofspeech tagging using decision trees. In International
Conference on New Methods in Language Processing, p. 44&#8211;49, Manchester, UK.
</p>
<p>WASZAK T. &amp; TORRES-MORENO J.-M. (2008). Compression entropique de phrases contro-
l&#233;e par un perceptron. JADT, 2, 1163&#8211;1173.</p>

</div></div>
</body></html>