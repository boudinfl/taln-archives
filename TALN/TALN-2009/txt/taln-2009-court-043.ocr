TALN 2009 — Session posters , Senlis, 24-26 juin 2009

Un nouveau schéma de pondération pour la categorisation de
documents manuscrits

Sebastian Peﬁa Saldarriagal EmmanuelMorin1 Christian Viard—Gaudin2
(l) LINA - UMR CNRS 6241, Universite de Nantes
(2) IRCCyN - UMR CNRS 6597, Universite de Nantes
{Pre’n0m. Nam} @uniV—nantes.fr

Résumé. Les schemas de ponderation utilises habituellement en categorisation de textes,
et plus generalement en recherche d’information (RI), ne sont pas adaptes a l’utilisation de
donnees liees 21 des textes issus d’un processus de reconnaissance de l’ecriture. En particulier,
les candidats—mot a la reconnaissance ne pourraient etre exploites sans introduire de fausses
occurrences de termes dans le document. Dans cet article nous presentons un nouveau schema
de ponderation per1nettantd’exploiter les listes de candidats—mot. Il permet d’estimer le pouvoir
discriminant d’un terme en fonction de la probabilite a posteriori d’un candidat—mot dans une
liste de candidats. Les resultats montrent que le taux de classiﬁcation de documents fortement
degrades peut etre ameliore en utilisant le schema propose.

Abstract. The traditional weighting schemes used in information retrieval, and especially
in text categorization cannot exploit information intrinsic to texts obtained through an on—line
handwriting recognition process. In particular, top n (n > 1) candidates could not be used
without introducing false occurrences of spurious terms thus making the resulting text noisier.
In this paper, we propose an improved weighting scheme for text categorization, that estimates
a term importance from the posterior probabilities of the top n candidates. The experimental
results show that the categorization rate of poorly recognized texts increases when our weighting
model is applied.

M0tS-CléS I Categorisation de textes, ecriture en—ligne, n—best candidats, ponderation.

K€yWOFdS2 Text categorization, on—line handwriting, n—best candidates, weighting.

1 Introduction

Les avancees dans le domaine de la reconnaissance de l’ecriture en—ligne permettent de pro-
duire, 21 partir d’un signal manuscrit, des textes en langue naturelle. Il devient alors possible
d’appliquer des technologies de gestion du contenu normalement utilisees pour des textes elec-
troniques tels que pages web et e—mails (Vinciarelli, 2006). Cependant, l’exploitation des don-
nees issues de la reconnaissance n’est pas aussi simple qu’il y parait. En effet, les transcriptions
sont souvent bruitées, c’est—a—dire qu’elles contiennent des suppressions, insertions et rempla—
cements de mots par rapport au texte correspondant reellement au signal manuscrit.

La categorisation automatique de textes est une problematique classique en intelligence arti-
ﬁcielle liee au traitement automatique des langues. Toutefois, ce domaine n’a ete explore de

Pena Saldarriaga et al.

facon approfondie que pour les documents electroniques (Sebastiani, 2002), et peu de travaux
existent sur la categorisation de documents manuscrits. Des travaux recents se sont interesses a
cette problematique (Vinciarelli, 2005; Peﬁa Saldarriaga er al., 2009) et ont mis en evidence une
difference, pouvant etre signiﬁcative, entre les resultats obtenus avec les donnees manuscrites et
les textes electroniques originaux. L’ ampleur de cette difference depend de la quantite de bruit
existant dans les documents issus du processus de reconnaissance de l’ecriture.

Mais, si un systeme de reconnaissance induit du bruit dans les transcriptions qu’il produit, il
peut egalement estimer la qualite du texte en sortie. En particulier, une probabilite peut etre
associee a chacun des mots du texte. De plus, une liste de candidats—mot a la reconnaissance
peut egalement etre obtenue comme le montre la ﬁgure 1.

NOTE ? Pm - gka/w. 0/mouh/to 
(1) NOTE i per—shone ameunts adjusted
(2) VIOTE is per—share amounts adjured

(3) ulotE i pen—shane remounts abjured

FIGURE 1 — Reconnaissance avec 3 candidats—mots

Nous pensons que l’utilisation de ces candidats—mot pour la categorisation peut aider a reduire
la difference de performances observee dans les travaux precites. Le travail propose ici a pour
but d’apporter une fonctionnalite de categorisation en utilisant les listes successives de n—best
candidats—mots a la reconnaissance, la ou les approches explorees jusqu’ici se contentent d’uti—
liser la sequence de mots la plus probable donnee par le systeme de reconnaissance, contenant
la plupart du temps le candidat—mot arrive en tete de la liste.

Cependant, si l’utilisation des n—best candidats peut permettre de conserver l’information cor-
respondant a un terme qui ne serait pas arrive en premiere position, elle introduit egalement de
fausses apparitions de mots avec un poids egal : le contenu du document s’en trouve altere et
la categorisation aussi. Aﬁn de reduire l’impact de ces fausses apparitions, il faut redeﬁnir les
schemas de ponderation classiques utilises avec le formalisme vectoriel de representation des
donnees (Salton et al., 1975), en nous basant sur la probabilite associee a chaque candidat—mot.
De plus il serait convenable d’ajuster dynamiquement le nombre de candidats—mot, en seuillant
sur la valeur des probabilites, limitant ainsi l’incidence des candidats tres peu probables.

La section 2 s’attache a introduire la problematique de la reconnaissance de l’ecriture en-
ligne. Nous y decrivons egalement le moteur de reconnaissance utilise et les ressources lin-
guistiques qui lui sont associees. Le nouveau schema de ponderation base sur les probabilites
des candidats—mot est presente dans la section 3. Aﬁn de montrer l’interet du schema de ponde-
ration propose, nous decrivons en section 4 les experiences realisees sur une base de documents
reproduisant sous forme manuscrite les depeches de l’agence Reuters bien connues dans le do-
maine de la categorisation de textes (Debole & Sebastiani, 2005). Enﬁn, dans la section 5, nous
concluons en evoquant les perspectives de ce travail.

2 Reconnaissance et écriture en—ligne

Souvent cantonnee a la saisie sur des terminaux de petite taille (PDA, smartphone), l’ecriture
en—ligne devient aujourd’hui une nouvelle source d’information en langue naturelle. Cela resulte

Ponderation pour la categorisation de documents manuscrits

de l’emergence des dispositifs de saisie que sont les stylos electroniques couples a des supports
papier. Ils permettent de produire des Veritables documents de diverses natures : notes de cours,
copies d’examens, articles, formulaires, etc.

Dans le domaine de l’ecriture en—ligne, un document se presente sous la forme d’une sequence
de points ordonnes dans le temps (:c(t),  Le trace correspond a la trajectoire echantillonnee
de l’instrument d’ecriture, chaque point etant une position ou le crayon s’est trouve pose (cf
ﬁgure 2).

o Poser du stylo
E‘ Lever du stylo

En-Iigne

FIGURE 2 — Exemple de trace en—ligne pour la lettre i

L’ objectif de la reconnaissance en—ligne est de determiner la suite de caracteres la plus Vraisem—
blable etant donne le signal correspondant au trace manuscrit a l’aide d’informations fournies
par un ensemble de connaissances a priori sur la langue (Perraud et al., 2005).

Dans le cadre de cette etude, nous avons privilegie l’utilisation d’un moteur de reconnaissance
stable et pret a l’emploi : MyScript Builder 1. Ce moteur de reconnaissance permet d’associer
differentes ressources linguistiques aﬁn de guider et d’optimiser la reconnaissance (cf ﬁgure 3).

Ressources Linguistiques

¥%M}WJLij NWSamtBmMa Handwriting

Entrée Sortie

FIGURE 3 — Reconnaissance avec MyScript Builder SDK

Il est possible de deﬁnir des ressources speciﬁques, soit sous forme de lexiques ou encore d’au—
tomates lexicaux, ou bien d’utiliser les deux ressources standard livrees avec MyScript Builder :

— lk-text est une ressource constituee d’un lexique standard et d’un modele statistique du lan-
gage au niveau mot. Ce dernier permet de favoriser la reconnaissance des sequences de mots
les plus probables. Ainsi, ‘je me’ sera prioritaire par rapport a ‘je tu’. Cette ressource permet
egalement de reconnaitre des elements hors—lexique comme les dates, les codes postaux, etc.

— lk-free apporte peu de contraintes sur ce que l’on Veut reconnaitre. Il n’y pas de lexique mais
seulement un modele de langage au niveau caractere. Cette ressource permet de favoriser les
sequences de caracteres les plus Vraisemblables, par exemple ‘MATIN’ sera prioritaire par
rapport a ‘MATIN’.

1.http://www.visionobjects.com/products/software—development—kits/
myscript—builder/

Peﬁa Saldarriaga et al.

2.1 Evaluation de la reconnaissance

Le bruit induit par la reconnaissance est souvent mesure au niveau des mots. Le taux d’erreur
au niveau mot ou Word Error Rate (WER) correspond au pourcentage de mots mal reconnus
sur la totalite de mots a reconnaitre pour une sequence donnee :

2? min (wf(i),wf’(i))
2? wf(i)

Avec wf and wf’ les frequences du mot 2' dans le texte d’origine et le texte reconnu
respectivement, et N le nombre de mots a reconnaitre.

WEB = 1- (1)

Une autre facon de mesurer le bruit, est de travailler au niveau terme. Le taux d’erreur au
niveau terme ou Term Error Rate (TER) est plus adapte a la categorisation car il tient compte
de la normalisation de textes (cf section 4.1). Puisque ‘révas’ et ‘réves’ ont la meme racine,
reconnaitre l’un Ea la place de l’autre ne modiﬁe pas la liste de termes reconnus. Reconnaitre
‘pour’ Ea la place de ‘par’ ne la modiﬁe pas non plus, car quelque soit le mot reconnu, il sera
ﬁltre puisque c’est un mot outil.

Le TER est calcule grace a la formule suivante (Vinciarelli, 2005) :
_ 2? min (tf(2'>,tf’(2'>>
2? W)

Avec tf and tf’ les frequences du terme 2' dans le texte d’origine et le texte reconnu
respectivement, et N le nombre de termes de reference.

TER = 1 (2)

Dans nos experiences, nous utilisons ces deux mesures comme indicateurs de la qualite des do-
cuments produits en fonction de la ressource linguistique associee au moteur de reconnaissance.

3 Pondération et seuillage de candidats-termes

La mauvaise reconnaissance des documents engendre une categorisation moins bonne (Vincia—
relli, 2005; Peﬁa Saldarriaga et al., 2009). En effet, suite Ea la reconnaissance, un terme pertinent
peut ne pas se trouver dans un document alors qu’il aurait dﬁ y etre. Or, les occurrences des
termes sont au coeur de la reussite des algorithmes de categorisation, et ce d’autant plus qu’ils
utilisent le formalisme Vectoriel et des schemas de ponderation comme t f X idf (Sparck Jones,
1979). L’utilisation des n—best candidats—mot peut permettre de capturer l’information corres-
pondant Ea un terme qui ne serait pas arrive en premiere position. En effet, plus la liste de n—best
est grande, plus le mot attendu a des chances de s’y trouver. Cependant, l’introduction artiﬁ-
cielle de mots fausserait les resultats d’un algorithme de categorisation. Dans ce contexte nous
redeﬁnissons la ponderation t f X idf pour tenir compte des probabilites des candidats des dif-
ferentes listes de n—best. Dans un second temps, une strategie de seuillage est proposee aﬁn de
ﬁltrer des candidats tres peu probables.

3.1 Pondération

Dans la suite de ce document nous considerons qu’un candidat—terme est simplement l’entite
representative du sens d’un candidat—mot dans l’espace Vectoriel. Autrement dit, il s’agit de la

Ponderation pour la categorisation de documents manuscrits

racine (Porter, 1980) ou du lemme (Namer, 2000) d’un candidat—mot.

Déﬁnition 1 Fréquence d ’un candidat-terme

Soit pn(i) la probabilité d ’un candidat-terme i dans la n-ieme liste de candidats, et N le
nombre de listes de candidats-terme dans lesquels 7} apparaft au sein d ’un document. La fre-
quence du candidat-terme i est donnée par la formule suivante :

wm=§mm m

1121

Nous pouvons multiplier la frequence ainsi obtenue par le facteur idf classique pour obtenir
une mesure ct f X idf adaptée a 1’exp1oitation des listes de candidats—mots.

Déﬁnition 2 Mesure candidate-tf >< idf

Soit K le nombre de documents dans un corpus, et k:,- le nombre de documents dans lesquels
le candidat-terme i apparaft au moins une fois. La pondération ct f X idf peut étre calculée
grdce 61 la formule suivante :

ctf.z'df(z') = ctf(2') >< log? (4)

Aﬁn de reduire les effets engendres par les differences de longueurs des documents, i1 convient
de normaliser cette mesure, en particulier 1orsqu’e11e est utilisee avec des approches a base de
distances ou mesures de similarite.

Déﬁnition 3 Mesure candidate-tf >< idfnormalisée

Soit M le nombre de termes de l’espace de representation vectorielle et i un candidat-terme
donné. La mesure ct f X idf normalisée est donnée par la formule suivante :

ctf(z') >< log 
:£1<ctf(.7> ><1ogg>2

nctf.2'df(2') = (5)

La deﬁnition de ce nouveau schema de ponderation Va permettre de calculer facilement 1e poids
d’un candidat—terme 2' dans un Vecteur. Des methodes de categorisation standard ou des systemes
existants (Pena Saldarriaga et al., 2009) peuvent alors etre utilises sans modiﬁcation majeure.

3.2 Seuillage

Le but de la strategie de seuillage proposee ci—dessous est d’ajuster dynamiquement la taille des
listes de candidats. Nous supposons que la liste de n—best candidats est triee par ordre décroissant
probabilité et que 217:0  = 1 avec 71 1e nombre de candidats dans la liste et  la probabilite
d’un candidat—terme 2'. Nous cherchons a trouver les k (k < n) premiers candidats tels que
Ego p( j ) w t, 0 < t 3 1 ou t est le seuil desire. La ﬁgure 4 montre 1e comportement de notre
stratégie de seuillage pour t = 0, 8 et differentes listes de n—best candidats.

Peﬁa Saldarriaga et al.

exchequer 0,61 implication 0,54 fuel 0,23
exoneration 0,22 implications 0,14 gull 0,21
§  imputation 0,12 gulf 0,19
excheqwr 0, 04  mm full 0,19
exchcqwr 0,02 implicated 0,09  

FIGURE 4 — Exemple de seuillage sur différentes listes de candidats
4 Expériences

Aﬁn de Valider le nouveau schema de pondération, nous avons mené plusieurs experiences sur
le corpus présenté ci—dessous. En premier lieu, nous avons effectué la reconnaissance des docu-
ments manuscrits et observe l’éVolution des taux d’erreur en fonction du nombre de candidats-
mot acceptés. Ensuite, nous avons categorise les documents en utilisant la sortie standard du
systeme de reconnaissance ainsi que la sortie comportant des listes de candidats—mot a la recon-
naissance. Les résultats de ces experiences sont présentés et commentés dans les sous—sections
4.2 et suivantes.

4.1 Données et paramétres expérimentaux

Pour la realisation des experiences, nous avons utilise un jeu de données compose de 2 029 de-
péches du corpus Reuters—21578 reproduites sous forme manuscrite et réparties sur 10 classes.
L’ ensemble d’entrainement est constitué de 1 625 documents et celui de test de 404 dépeches.
La partition en ensembles d’entrainement et de test suit le protocole ModApté (Apte et al.,
1994). Les données sont mono—catégorie, c’est—a—dire que les documents n’appartiennent qu’a
une seule classe. La ﬁgure 5 montre un exemple de document manuscrit de notre base.

TCF>‘/ €>v77aa7zpru‘95s in/c. my A57’ om F€E>2sA,e-r
$’LuL64‘<;/£4/5’ db vs ‘Le

/we X/3F‘l‘D, 268 vs 81%, ﬁiii?

Spam 97986, B: o: L), $65 ,%2§

yam avrﬂl/Z/, 33% \5 49,097, 23;

NOTE; F0/L—$}’\a/U2. a/moumto 
 i 9/oLr‘[2 M, /}7arF4  I %‘?’7é

FIGURE 5 — Document du corpus manuscrit

Nous avons choisi d’utiliser deux méthodes de categorisation simples mais performantes. I1
s’agit de la méthode des k—Plus Proches Voisins (kPPV) et des Séparateurs a Vaste Marge
(SVM) (Vapnik, 1995), ces deux approches étant reconnues parmi les approches les plus per-
formantes développées durant la décennie (Yang & Liu, 1999; Joachims, 2002; Debole & Se-
bastiani, 2005).

Avant l’application de ces algorithmes, une étape de normalisation a lieu. Elle consiste :21 seg-
menter les textes en occurrences de forme, ﬁltrer les mots outils et appliquer l’algorithme de
racinisation de Porter (1980). Durant la phase d’entrainement, l’ensemble des termes de l’espace
de representation des documents est choisi en utilisant la statistique du X2 (Yang & Pedersen,
1997) couplée a l’algorithme de Forman (2004).

Ponderation pour la categorisation de documents manuscrits

L’éValuation du systeme se fait sur la base du document, c’est—a—dire en utilisant la micro-
moyenne de la precision et du rappel. Comme les données sont mono—catégorie, sans rejet,
la precision et le rappel inter—classes sont égaux (Beney, 2008). De ce fait, nous donnerons
une seule mesure de la qualite d’un classiﬁeur que nous appellerons taux de classiﬁcation,
correspondant a la micro—moyenne de la precision ou le rappel.

4.2 Reconnaissance

Les documents du corpus manuscrit sont reconnus en utilisant les deux ressources décrites
précédemment : lk-text et lk-free. La ﬁgure 6 montre l’eVolution du WER et du TER en fonction
du nombre de candidats—mot.

Les textes reconnus avec la ressource lk-free sont fortement degrades. En effet plus d’un mot sur
deux est perdu en moyenne, alors qu’aVec lk-text 77% des mots et autant de termes presents dans
les textes sont correctement reconnus. Introduire des modeles de langage permet d’ameliorer
considérablement le taux d’erreur (Perraud et al., 2005). Quand il n’y a pas d’a priori apporte
par un tel modele, comme c’est le cas de la ressource lk-free les performances d’un systeme de
reconnaissance sont tres mauvaises.

Nous observons egalement que plus la liste de candidats—mots est grande, plus le terme attendu
a des chances de s’y trouver, le taux d’erreur s’en trouve alors diminue comme le montre la
ﬁgure 6.

+ lk-text/WER —A— lk-text/TER + svm/lk-text —A— kppv/lk-text
+ lk-free/WER —><— lk-free/T ER + svm/lk-free —><— kppv/lk-free
60 % 1
50% 90% — K ‘
.5
E 40% §
E 5
=3 3 * *
§ 30% — — 4: 85% ‘ ‘
E E
20 % 7 .  7 F“
80%
10 % ‘
12345 10 15 12345 10 15
Nombre de candidats—mot (n) Nombre de candidats—mot (11)
FIGURE 6 — Taux d’erreur en fonction du FIGURE 7 — Taux de classiﬁcation en fonc-
nombre de candidats—mot tion du nombre de candidats

4.3 Catégorisation

La categorisation des 404 documents d’eValuation a ete effectuee aussi bien sur les documents
issus de la reconnaissance avec un seul ou avec plusieurs candidats—mot.

Pena Saldarriaga et al.

       

867
90% _ 0 _

.5 E

§ 89% E 84%
U2: :
 2

o :
-8 88% E 82%

>< :
5 E _ _

877 — E *
0 g : 80% — g —
  ¢ 2   H
u u u u u u u
lk—text/n= l0 lk—text/n= l 5 lk—free/n= l 0 lk—free/n= l 5 lk—text/n= l 0 lk—text/n= l 5 lk—free/n= l 0 lk—free/n= l 5
1 1] D sans seuillageE E t=0,9 E E t=0,8 1 1 1] I] sans seuillage E E t=0,9 E E t=0,8 1
(a) SVM (b) kPPV

FIGURE 8 — Taux de classiﬁcation avec seuillage

Les parametres des classiﬁeurs ont ete optimises aﬁn d’obtenir le meilleur taux de classiﬁcation.
Nous avons utilise pour cela, un sous—ensemble de l’ensemble d’entrainement reconnu avec un
seul candidat—mot. Les SVM sont utilises avec un espace Vectoriel de 1000 termes, et les kPPV
avec 300 termes et is = 15.

La ﬁgure 7 montre le taux de classiﬁcation en fonction de l’algorithme, de la ressource et du
nombre de candidats—mot utilises.

Lorsque nous utilisons une liste de candidats—ter1ne avec lk-text le taux de classiﬁcation baisse
avec l’augmentation du nombre de candidats. En revanche, lorsque les documents de lk-free
sont utilises, pour tout n > 1 le taux de classiﬁcation est superieur a celui obtenu en ne prenant
que le premier candidat. Une augmentation moyenne de 2,1 % avec un ecart—type de 1,2 peut
etre signalee et ce, quel que soit le classiﬁeur utilise.

Nous avons utilise la strategie de seuillage presentee en 3.2 pour n = 10 et 71 = 15, les resultats
obtenus en fonction de la Valeur du seuil t utilise sont donnes par la ﬁgure 8. L’application
du seuillage n’a pas permis d’ameliorer, de facon generale, le taux de classiﬁcation avec la
ressource lk-text. L’ amelioration observee pour les SVM et 71 = 15 parait logique : le seuillage
a eu pour effet de reduire n, le point le plus bas de la courbe sum/lk —tea:t de la ﬁgure 7 Va donc
remonter car la courbe decroit de facon quasi—monotone en fonction du nombre de candidats.

Notre strategie apparait plus efﬁcace lorsqu’elle est appliquee avec la ressource lk-free. Avec
SVM et 71 = 15, une amelioration importante est observee, le taux de classiﬁcation est meme
superieur a celui obtenu avec des documents moins degrades (lk-text) dans une conﬁguration
identique. Une legere augmentation avec kPPV et 71 = 10 se produit egalement, mais ne permet
pas de depasser les resultats obtenus avec les documents de lk-text dans la meme conﬁguration.

Nous pouvons observer qu’un seuil fort (t = 0,9) permet d’obtenir des meilleurs resultats par
rapport a des seuils plus faibles. Cependant, les seuils utilises ont ete deﬁnis manuellement et
peuvent ne pas etre optimaux, ce qui pourrait expliquer en partie les resultats de notre strategie
de seuillage.

Ponderation pour la categorisation de documents manuscrits

5 Conclusion

Le travail présenté dans cet article decrit un nouveau schema de pondération pour l’utilisation
des listes de n—best candidats—ter1ne dans un processus de categorisation textuelle de documents
manuscrits en—ligne. Nous pensions que l’utilisation de ces listes permettrait d’atteindre un taux
de categorisation superieur a celui obtenu avec juste le premier candidat.

Notre hypothese de depart n’est pas entierement conﬁrmée par les resultats experimentaux. En
effet, l’utilisation des candidats—tennes n’a pas permis d’améliorer le taux de categorisation des
documents ou environ 77% des termes d’indexation sont correctement reconnus. En revanche,
l’utilisation de la liste des n—best mots s’est revélée bénéﬁque pour des documents ou plus de la
moitie de l’information est perdue. Aussi bien avec les kPPV qu’avec les SVM, une augmenta-
tion moyenne de 2,1 % du taux de categorisation a été observée.

La strategie de seuillage proposee ne semble pas sufﬁsante pour limiter l’inﬂuence des candi-
dats tres peu probables dans la categorisation des documents de lk-text. Elle a permis cependant
d’ameliorer les resultats obtenus avec lk-free et un nombre de candidats important (71 2 10).
Ces résultats apparaissent prometteurs mais une question se pose, comment deﬁnir le seuil op-
timal ? De nouvelles experiences doivent etres effectuees aﬁn d’explorer differentes techniques
d’estimation de ce seuil (validation croisée, leave—one—out, méta—heuristiques).

De maniere generale, les resultats présentes dans cette contribution montrent que l’utilisation
des listes de n—best candidats—terme permettent d’obtenir des niveaux convenables de categori-
sation sur des documents fortement degrades.

Remerciements

Ces travaux ont ete soutenus par la Region Pays de la Loire a travers le Projet MH.ES et par
l’Agence Nationale de la Recherche a travers le programme Technologies Logicielles (ANR—
06—TLOG—009).

Références

APTE C., DAMERAU F. & WEISS S. M. (1994). Towards language independent automated
learning of text categorization models. In Proceedings of the I 7th Annual International ACM
SIGIR Conference (SIGIR ’94), p. 23-30.

BEN EY J. (2008). Classiﬁcation supervisée de documents. Hermes Science / Lavoisier.

DEBOLE F. & SEBASTIANI F. (2005). An analysis of the relative hardness of reuters—21578

subsets. Journal of the American Society for Information Science and Technology, 56(6), 584-
596.

FORMAN G. (2004). A pitfall and solution in multi—class feature selection for text classi-

ﬁcation. In Proceedings of the 21st International Conference on Machine Learning (ICML
’04).

J OACHIMS T. (2002). Learning to Classify Text using Support Vector Machines. Kluwer
Academic Publishers.

Pena Saldarriaga et al.

NAMER F. (2000). Flemm : Un analyseur ﬂexionnel du francais a base de regles. Traitement
Automatique des Langues, 41(2), 523-547.

PENA SALDARRIAGA S., VIARD-GAUDIN C. & MORIN E. (2009). On—line handwritten
text categorization. In Proceedings of Document Recognition and Retrieval XVI, IS &T/SPIE
International Symposium on E.I. (DRR ’09), Volume 7247, p. 724709.

PERRAUD F., VIARD—GAUDIN C., MORIN E. & LALLICAN P. M. (2005). Statistical lan-
guage models for on—line handwriting recognition. IEICE Transactions on Information and
Systems, E88-D(8), 1807-1814.

PORTER M. F. (1980). An algorithm for sufﬁx stripping. Program, 14(3), 130-137.

SALTON G., WONG A. & WANG C. S. (1975). A Vector space model for automatic indexing.
Communications of the ACM, 18(11), 613-620.

SEBASTIANI F. (2002). Machine learning in automated text categorization. ACM Computing
Surveys, 34(1), 1-47.

SPARCK JONES K. (1979). Experiments in relevance weighting of search terms. Information
Processing & Management, 15, 133-144.

VAPNIK V. (1995). The Nature of Statistical Learning Theory. Springer—Verlag.

VINCIARELLI A. (2005). Noisy text categorization. IEEE Transactions on Pattern Analysis
and Machine Intelligence, 27(12), 1882-1895.

VINCIARELLI A. (2006). Indexation de documents manuscrits. In Actes du 9eme Colloque
International Francophone sur l ’Ecrit et le Document (CIFED ’06), p. 49-54.

YANG Y. & LIU X. (1999). A re—examination of text categorization methods. In Proceedings
of the 22nd Annual International ACM SIGIR Conference (SIGIR ’99), p. 42-49.

YANG Y. & PEDERSEN J. O. (1997). A comparative study on feature selection in text cate-

gorization. In Proceedings of the 14th International Conference on Machine Learning (ICML
’97), p. 412-420.

