TALN 2009, Senlis, 24-26 juin 2009

Trouver et confondre les coupables : un processus sophistiqué
de correction de lexique *

Lionel Nicolas*, Benoit Sagot®, Miguel A. Molinero°,
Jacques Farré*, Eric de La Clergerie®.

* Fquipe RL, Laboratoire I3S, UNSA+CNRS, France {lnicolas,jf} @i3s.unice.fr
O Grupo LYS, Univ. de A Coruﬁa, Espaﬁa mmolinero@udc.es

63 Proj et ALPAGE, INRIA Rocquencourt + Paris 7, France {benoit.sagot, Eric.De_La_Clergerie} @inria.fr

Résumé. La couverture d’un analyseur syntaxique dépend avant tout de la grammaire et
du lexique sur lequel il repose. Le développement d’un lexique complet et précis est une tache
ardue et de longue haleine, surtout lorsque le lexique atteint un certain niveau de qualité et de
couverture. Dans cet article, nous présentons un processus capable de détecter automatique-
ment les entrées manquantes ou incompletes d’un lexique, et de suggérer des corrections pour
ces entrées. La détection se réalise au moyen de deux techniques reposant soit sur un modele
statistique, soit sur les informations fournies par un étiqueteur syntaxique. Les hypotheses de
corrections pour les entrées lexicales détectées sont générées en étudiant les modiﬁcations qui
permettent d’améliorer le taux d’analyse des phrases dans lesquelles ces entrées apparaissent.
Le processus global met en oeuvre plusieurs techniques utilisant divers outils tels que des éti-
queteurs et des analyseurs syntaxiques ou des classiﬁeurs d’entropie. Son application au Leﬁj‘,
un lexique morphologique et syntaxique a large couverture du frangais, nous a déja permis de
réaliser des améliorations notables.

Abstract. The coverage of a parser depends mostly on the quality of the underlying gram-
mar and lexicon. The development of a lexicon both complete and accurate is an intricate and
demanding task, overall when achieving a certain level of quality and coverage. We introduce
an automatic process able to detect missing or incomplete entries in a lexicon, and to suggest
corrections hypotheses for these entries. The detection of dubious lexical entries is tackled by
two techniques relying either on a speciﬁc statistical model, or on the information provided by a
part-of-speech tagger. The generation of correction hypotheses for the detected entries is achie-
ved by studying which modiﬁcations could improve the parse rate of the sentences in which the
entries occur. This process brings together various techniques based on different tools such as
taggers, parsers and entropy classiﬁers. Applying it on the Leﬁf, a large-coverage morphologi-
cal and syntactic French lexicon, has already allowed us to perfom noticeable improvements.

M0tS-CléS 2 Acquisition et correction lexicale, lexique a large couverture, fouille d’er-
reurs, étiqueteur syntaxique, classiﬁeur d’entropie, analyseur syntaxique.

Keywords: Lexical acquisition and correction, wide coverage lexicon, error mining,
tagger, entropy classiﬁer, syntactic parser.

Ces travaux ont notamment pu étre réalisés grace au soutient du ministere de l’éducation et des sciences d’Es—
pagne, FEDER (HUM2007—66607—C04—02), du Gouvemement Regional de Galice (INCITE08PXIB302l79PR,
lNCITE08E1Rl04022ES) et du Galician Network for Language Processing and Information Retrieval 2006-2009.

Lionel Nicolas, Benoit Sagot, Miguel A. Molinero, Jacques Farré, Eric de La Clergerie

1 Introduction

Le développement manuel d’un lexique précis et a large couverture est une tache fastidieuse,
complexe et sujette a erreurs nécessitant une coﬁteuse expertise humaine. Les développements
manuels de lexiques n’atteignent généralement pas les objectifs attendus et progressent tres
lentement une fois un certain niveau de couverture et de qualité atteint. Cette tache manuelle
peut cependant étre simpliﬁée et améliorée par l’utilisation d’outils automatisant les taches
d’acquisition et de correction. Nous présentons un ensemble combiné de techniques permettant
de détecter les entrées manquantes, incompletes ou erronées d’un lexique et de proposer des
corrections. La chaine logique du processus global se résume ainsi :

1. Donner en entrée a un analyseur syntaxique un grand nombre de phrases non-annotées
considérées comme respectueuses de la langue, aﬁn d’attribuer un échec d’analyse aux
manques de l’analyseur et non aux textes qu’il recoit en entréel.

2. Pour chaque phrase non-analysable, tenter de déterminer automatiquement si l’échec
d’analyse est dﬁ a des manques de la grammaire ou du lexique utilisés par l’analyseur.

3. Suspecter des entrées lexicales d’étre manquantes, incompletes ou erronées.

4. Générer des hypotheses de correction en observant les attentes de la grammaire vis a vis
des formes suspectées lors des analyses de phrases dans lesquelles elles apparaissent.

5. Evaluer et classer les hypotheses de correction aﬁn de procéder a une validation manuelle.

Bien que tous nos exemples et résultats soient lies a la langue francaise, cet ensemble de tech-
niques est indépendant du systeme, c.a.d, il est facilement adaptable pour la plupart des eti-
queteurs syntaxiques, classiﬁeurs d’entropie, lexiques et analyseurs profonds existants, et par
conséquent, a la plupart des langues informatiquement décrites.

Cet ensemble de techniques est l’un des points de départ du récent projet Victoriaz, dont le
but est de développer un ensemble d’outils permettant la construction efﬁcace de ressources
morphologiques, lexicales et grammaticales. Ces travaux font suite aux travaux présentés dans
(2007a; 2007b; 2008) ou des modeles plus simples avait été décrits.

Pour des raisons de clarte’, les résultats pratiques de chaque étape ainsi que les améliorations
possibles sont donnés conjointement a sa présentation. Nous commencons donc par décrire le
contexte pratique de nos expériences (sect. 2). Nous décrivons ensuite chaque étape énumérée
ci dessus (sections 3,4,5,6,7) et concluons (sect.8).

2 Contexte pratique

Nous utilisons un corpus joumalistique francais non-annoté extrait du monde diplomatique. Ce
corpus contient 280 000 phrases de 25 mots ou moins, totalisant 4,3 millions de mots.

Le lexique utilisé que l’on cherche a amélioré se nomme le Leﬁj“. Ce lexique morphologique
et syntaxique a large couverture du francais contenant plus de 600 000 entrées.

Deux analyseurs syntaxiques sont utilisés aﬁn de générer des corrections :

ltextes de lois, joumaux, etc.
Zhttp : //www . victoria—project . org, octobre 2008.
3Lexique des formes ﬂéchies du frangais. http : //alpage . inria . fr/~sagot/lefff—en . html.

Trouver et confondre les coupables: un processus sophistique de correction de lexique

— FRMG (French Meta-Grammar) se base sur une meta-grammaire abstraite avec des arbres
hautement factorises (Thomasset & Villemonte de La Clergerie, 2005) compilee en un ana-
lyseur hybride TAGfI‘IG grace au systeme DYALOG.

— SXLFG-FR (Boullier & Sagot, 2006) est une grammaire LFG profonde efﬁcace non proba-
biliste compilee en analyseur LFG par SXLFG, un systeme base sur SYNTAX.

Nous utilisons aussi de facon ponctuelle l’etiqueteur syntaxique MrTagoo (Molinero et al.,

2007; Graﬁa, 2000) et le classiﬁeur d’entropie MegaM (Daume III, 2004).

3 Classiﬁcation des phrases non analysables

Nous partons des resultats d’ analyse syntaxique d’un grand nombre de phrases. Certaines phrases
ont ete pu etre analysees, d’autres non. Les phrases analysables sont considerees comme cou-
vertes lexicalement et grammaticalement (meme si les analyses obtenues ne coi'ncident pas tou-
jours avec leur sens veritable). Les phrases non-analysables sont par contre non couvertes lexi-
calement et/ou grammaticalement. Aﬁn de generer des corrections pour un lexique, il nous est
preferable d’isoler les phrases qui ne sont non-analysables que pour des raisons lexicales. Pour
ce faire, nous cherchons d’abord a identiﬁer les phrases grammaticalement non-analysables.

Cette etape est realisee grace a un classiﬁeur d’entropie, c.a.d, un outil statistique qui permet
de calculer une adequation (une entropie) entre les donnees qu’il recoit a l’evaluation et les
donnees sur lesquelles il a ete entraine (Daume III, 2004).

Dans ce but, nous proﬁtons du fait que les constructions syntaxiques sont plus frequentes et

bien moins diverses que les formes lexicales. Celles non couvertes tendent donc a etre recur-

rentes et systematiques dans les phrases grammaticalement non-analysables. Aﬁn d’identiﬁer

ces constructions problematiques, nous entrainons un classiﬁeur d’entropie de la sorte :

— nous reduisons toutes les phrases a des sequences de 3-grams obtenues soit a partir des cate-
gories syntaxiques pour les mots des categories ouvertes (c.a.d., verbes, adjectifs, etc.) et soit
a partir de formes lexicales pour les mots des categories fermees (prepositions, determinants,
etc.) auxquelles nous rajoutons des marqueurs de debut et de ﬁn de phrase.

— nous associons a chaque sequence une classe (analysable/non-analysable) correspondant au
resultat de l’analyse de la phrase dont cette sequence a ete extraite.

Pour “je(cln) mange(v) une(det) pomme(nc)” nous generons donc une sequence de 3-grams

d’entrainement: <deb-je-v> <je-v-une> <v-une-nc> <une-nc-ﬁn> dont la classe est analysable.

Le classiﬁeur differencie donc, a partir des 3-grams qui les composent, les phrases qui paraissent
etre grammaticalement analysables de celles qui ne le sont pas. Les phrases non-analysables
declarees comme grammaticalement analysables sont alors considerees comme lexicalement
non-analysables.

Il est a noter que l’entrainement n’est pas optimal a cause de deux aspects. Premierement, la
categorie de chaque mot dans les phrases est obtenue par le biais d’un etiqueteur syntaxique.
Les etiqueteurs ne sont clairement pas des outils parfaits. Cependant, leurs erreurs sont gram-
maticalement aleatoires car elles dependent avant tout des formes lexicales rencontrees. Ce
caractere aleatoire permet donc aux erreurs de ne pas trop perturber la coherence globale de
l’entrainement du classiﬁeur d’entropie. Deuxiemement, les phrases non-analysables donnees
en entrainement ne sont pas toutes grammaticalement non-analysables, certaines sont seule-
ment lexicalement non-analysables. On l’entraine donc en partie a considerer injustement des
phrases comme grammaticalement non-analysables. Cependant, les calculs sur les 3-grams pre-

Lionel Nicolas, Benoit Sagot, Miguel A. Molinero, Jacques Farré, Eric de La Clergerie

sents dans ces phrases injustement catégorisées sont contrebalancés par leur présence logique
dans des phrases analysables.

Pour évaluer cette technique, nous avons oté 5% des phrases analysables a l’entrainement et
avons observé si le classiﬁeur les déclare comme analysables. Les taux de précision avant la
premiere session de correction, puis apres la premiere, seconde et troisieme session étaient
respectivement de 92,7%, 93,8%, 94,1% et 94,9%. La précision du classiﬁeur augmente logi-
quement car, apres chaque session, certaines phrases dont l’analyse échouait pour des raisons
lexicales deviennent analysables et ne perturbent donc plus l’entrainement. La génération des
séquences de 3-grams étant la méme pour l’ensemble des phrases, ces taux de précision de-
vraient s’appliquer de facon équivalente aux phrases grammaticalement non-analysables.

Finalement, le taux d’erreur de (pour l’instant) 5,1% est un manque considéré comme accep-
table étant donné l’impact positif que l’étape de ﬁltrage a sur nos techniques de détection. Puis-
qu’il n’y a pas de raison pour qu’une forme particuliere se retrouve plus que de raison dans des
phrases classiﬁées incorrectement, il est possible de contrebalancer la perte de certaines phrases
par une simple augmentation de la taille du corpus donné en entrée.

4 Détection des manques lexicaux

La détection d’entrées lexicales douteuses est réalisée par le bais de deux techniques comple-
mentaires qui identiﬁent des formes et les associent a des phrases dont elles sont suspectées
d’étre responsables de l’échec d’analyse.

4.1 Détection d’information lexicale £1 courte portée via un étiqueteur

Nous appelons information lexicale de courte portée toute information pouvant étre déterminée
par un étiqueteur syntaxique. Pour l’instant, nous ne considérons que la catégorie syntaxique.

Aﬁn de détecter les problemes lexicaux concemant ce type d’information, nous utilisons un
étiqueteur syntaxique conﬁguré de facon particuliere dont nous court-circuitons ponctuellement
le lexique interne aﬁn de le forcer a considérer comme inconnue, une a la fois, chaque forme
d’une phrase. Nous nous reposons donc sur sa capacité a s’inspirer du contexte d’une forme
pour supposer l’étiquette la plus probable. Les informations portées par ces étiquettes supposées
sont ensuite comparées aux informations existantes dans le lexique. Si ces informations sont
manquantes et concement des classes ouvertes, la forme correspondante est déclarée comme
suspecte. Appliquée aux catégories syntaxiques, cette technique nous permet de détecter les
homonymes manquants d’un lexique en plus des formes totalement inconnues.

Bien entendu, les étiqueteurs commettent des erreurs, surtout lorsqu’on courcuite ainsi leurs
lexiques internes. La précision de 1’ étiqueteur modiﬁé pour n’importe quel type de forme (méme
celles appartenant aux classes fermées) sur 5% des phrases non utilisées a l’entrainement est
de 47,34%. Le nombre de faux positifs est donc tres important. En étudiant les résultats, nous
avons pu observer le caractere systématique de certaines erreurs. Par exemple, un nom propre est
souvent considéré comme un nom commun, un participe passé comme un adjectif etc. Pour ré-
duire le nombres de formes suspectées incorrectement, nous avons développé quatre surcouches
proﬁtant du fait que nous ne travaillions pas a l’échelle d’une phrase solitaire (comme le font
généralement les étiqueteurs) mais d’un ensemble de phrases.

Trouver et confondre les coupables: un processus sophistiqué de correction de lexique

La premiere surcouche choisie simplement l’étiquette la plus fréquement donnée a une forme.

La deuxieme surcouche calcule des patrons de réponses de l’étiqueteur par type d’étiquette et
par fréquence d’apparition de la forme (indexées sur les valeurs entieres du logarithme népé-
rien). On cherche donc durant l’entrainement a savoir, par exemple, combien de fois un nom
propre est déclaré par l’étiqueteur comme nom propre, comme nom commun, comme adjectif,
etc. A l’évaluation, nous calculons une afﬁnité entre les nouveaux ensembles de réponses don-
nées par l’étiqueteur pour chaque forme et les patrons calculés a l’entrainement et on attribue
ensuite a la forme l’étiquette du patron avec lequel elle a le plus d’afﬁnité. Le calcul d’afﬁnité
A f fpat /rep et le choix du meilleur patron B estpat Se calculent ainsi :

Affpat/rep : Z ab3(Pateti _ Repeti)a Bestpat : ma"E(Affpat/rep * l0g(OCCpat»

Patet, et Repet, sont la part d’une étiquette eti, et Occpat est le poids d’éIr1ission du patron égale
a la somme des occurrences des formes qui ont permis sa construction.

La troisieme surcouche applique la meme idée mais laisse le calcul d’afﬁnité a un classiﬁeur
d’entropie. Le classiﬁeur est entrainé a reconnaitre des patrons et a les associer a une classe
représentant une étiquette et un index népérien.

La derniere surcouche s’appuie sur les trois premieres pour réaliser un « vote a la crédibilité »o1‘1
l’« opinion » de chaque surcouche est valorisée a partir des taux d’erreurs par type de réponse.

Le défaut majeur de ces surcouches est qu’actuellement, elles considerent que chaque forme
représente un seule lemme, ce qui est faux bien que vrai dans la grande majorité des cas.

L’étiqueteur est alors entrainé avec 50% des phrases du corpus, l’entrainement des surcouches
se réalise ensuite sur les réponses foumis par l’évaluation de l’étiqueteur sur 47,5% des phrases
et leur évaluation sur 2,5% des phrases restantes. Les précisions respectives sur ces 2,5% de
phrases de l’étiqueteur, de la premiere, seconde, troisieme et quatrieme surcouche sont res-
pectivement de 40,17%, 43,6%, 77,61%, 74,09%4 et 89,78%. L’application de ces surcouches
permet donc de passer d’une précision originelle de 47,34% de l’étiqueteur modiﬁé a 89,78%,
réduisant ainsi fortement le nombre de faux positifs.

Dans une premiere version basée uniquement sur l’étiqueteur sans surcouche et considérant
toute les formes comme inconnues en méme temps, nous avons pu identiﬁer 182 lemmes man-
quants. Cette nouvelle version nous a permis d’en trouver 358 autres. Le tout correspond a un
total de 1168 formes lexicales, pour la plupart adjectifs ou noms propres manquants.

4.2 Approche statistique pour la détection de défauts lexicaux

Cette technique de détection de défauts lexicaux, décrite dans (Sagot & Villemonte de La Cler-

gerie, 2006; Sagot & de La Clergerie, 2008), repose sur les hypotheses suivantes :

— Si une forme lexicale apparait plus souvent dans des phrases non-analysables que dans des
phrases analysables, il est raisonnable de la suspecter d’étre incorrectement décrite dans le
lexique (van Noord, 2004).

— Le taux de suspicion peut étre renforcé si la forme apparait dans des phrases non-analysables
a coté d’autres formes présentes dans des phrases analysables.

4Ce résultat moins important que la précédente surcouche est probablement dﬁ a une conﬁguration insufﬁsante
du classiﬁeur d’entropie

Lionel Nicolas, Benoit Sagot, Miguel A. Molinero, Jacques Farré, Eric de La Clergerie

L’ avantage de cette technique par rapport a la précédente est sa capacité a prendre en compte
tout type d’erreurs lexicales. Cependant, puisque qu’elle part du précepte que toute phrase non-
analysable ne l’est que pour des raisons lexicales, la qualité de la liste de suspects fournie
dépend directement de la qualité de la grammaire utilisée. En effet, si une forme spéciﬁque
est particulierement liée a une construction syntaxique non couverte par la grammaire, on la
retrouvera souvent dans des phrases non analysables et elle sera alors injustement suspectée.

Nous atténuons ce probleme de deux facons. Premierement, nous excluons du calcul statis-
tique toutes les phrases considérées comme grammaticalement non-analysables. Deuxieme-
ment, comme cela a déja été fait dans (Sagot & Villemonte de La Clergerie, 2006), nous com-
binons les résultats d’analyse fournis par différents analyseurs reposant sur des formalismes et
grammaires différents, et donc avec des manques grammaticaux différents.

Cette technique nous a permis de détecter 72 lemmes décrits de facon incomplete correspondant
a un total de 1693 formes lexicales, pour la plupart des verbes.

Pour l’instant, les deux techniques de détection identiﬁent des formes lexicales. I1 serait intéres-
sant de monter au niveau du lemme en appliquant en post-traitement l’idée décrite dans (Sagot,
2005) ou la validité d’un lemme est favorisée ou pénalisée suivant la présence ou l’absence de
ses formes lexicales.

Autre amélioration possible, l’efﬁcacité/l’intérét de ces techniques ne sont valorisés que par
les corrections qu’elles ont permis de réaliser. I1 serait intéressant d’établir une métrique aﬁn
d’évaluer la qualité des formes suspectes fournies. Cette métrique permettrait aussi de quantiﬁer
formellement l’impact positif de l’étape de ﬁltrage sur l’étape de détection. Le classement a
chaque session des formes corrigées pourrait étre un point de départ.

5 Génération des hypotheses de correction lexicale : analyse
de phrases initialement non-analysables

La génération de corrections lexicales a partir du contexte grammatical a été utilisé pour la
premiere fois en 1990 (Erbach, 1990). Elle suit l’idée suivante : suivant la qualité du lexique
et de la grammaire, la probabilité que ces deux ressources soient simultanément erronées au
sujet d’une forme donnée dans une phrase donnée peut étre faible. Si une phrase ne peut pas
étre analysée a cause d’une forme suspecte, cela implique que les deux ressources n’ont pas
pu s’accorder sur le role que la forme peut avoir dans la phrase. Puisque que le probleme est
d’origine lexical, il est possible de générer des corrections en étudiant les attentes de la gram-
maire pour chaque forme suspectée lorsqu’elle analyse les phrases qui leur sont associées. De
maniere métaphorique, on « demande » a la grammaire son opinion sur les formes suspectées.
Originellement, les formes suspectes étaient déterminées manuellement puis, a partir de 2006
(van de Cruys, 2006; Yi & Kordoni, 2006; Nicolas et al., 2007a; Nicolas et al., 2007b) , cette
idée a été combinée avec des techniques de fouille d’erreurs telles que (van Noord, 2004; Sagot
& Villemonte de La Clergerie, 2006; Sagot & de La Clergerie, 2008).

Pour générer des corrections, nous nous approchons au Inieux de des analyses que la gram-
maire aurait permises avec un lexique sans erreur. Puisque nous pensons que les informations
lexicales associées a la forme suspecte ont coupé le cheIr1in vers une possible analyse, nous
diminuons les restrictions imposées par les informations lexicales : pendant l’analyse, chaque
fois qu’une information lexicale associée a une forme suspectée est vériﬁée, le lexique est court-

Trouver et confondre les coupables: un processus sophistiqué de correction de lexique

circuité et toutes les contraintes sont considérées comme satisfaites. La forme devient alors tout
ce que peut souhaiter la grammaire. En réalité, cette opération est effectuée en échangeant les
formes suspectes dans les phrases associées par des formes sous-spéciﬁées appelées jokers. Si
une forme a été correctement suspectée, et si c’est l’unique cause d’échec de certaines analyses
de phrases, remplacer cette forme par un joker permet aux phrases de devenir analysables. Dans
ces nouvelles analyses, des entrées « instanciées » du joker sont partie prenante des structures
grammaticales produites en sortie. Ces entrées instanciées représentent les informations man-
quantes du lexique, nous les traduisons donc au format du lexique aﬁn d’établir les corrections.

Comme expliqué dans (Barg & Walther, 1998), l’utilisation de jokers totalement sous-spéciﬁés
peut introduire une ambigu'1'té trop grande dans le processus d’analyse. Cela entraine souvent
des échecs d’analyse pour des contraintes de temps ou de mémoire (pas de corrections), ou
des analyses surgénérative (trop de corrections). Contrairement a (Yi & Kordoni, 2006), ou
les auteurs utilisent les jokers totalement spéciﬁés les plus probables, nous n’ajoutons que peu
d’information lexicale aux jokers et nous nous reposons sur la capacité de nos analyseurs a gérer
des formes sous spéciﬁées. Pour des raisons pratiques, nous avons choisi d’ajouter aux jokers
une catégorie syntaxique. L’ ambiguité introduite reste conséquente et aboutit généralement a un
nombre important de corrections. Néanmoins cet aspect peut étre facilement contrebalancé pour
peu qu’il y ait assez de phrases non-analysables associées a une forme suspecte (voir sect 6).
La catégorie syntaxique ajoutée aux jokers dépend de la technique de détection utilisée pour
suspecter la forme. Lorsque nous utilisons la détection basée sur un étiqueteur, nous générons
des jokers avec des catégories syntaxiques en accord avec les étiquettes foumies pour la forme.
Quand nous utilisons l’approche de détection statistique, nous produisons des jokers avec les
catégories syntaxiques déja présentes dans le lexique pour la forme suspectée.

6 Extraction et classement des corrections

Le lecteur peut noter qu’un joker inadéquat peut parfaitement mener a de nouvelles analyses
et donc permettre la génération de corrections incorrectes. Nous commencons donc par séparer
les corrections suivant le joker qui a permis leur génération.

Classiﬁcation mono-analyseur. A l’échelle d’une seule phrase, rien de permet de differen-
cier les corrections valides des corrections erronées dont la génération résulte de l’ambigu'1'té
introduite par les jokers. Cette ambigu'1'té ayant permis a l’analyse d’emprunter des regles de
grammaires qu’elle n’aurait pas du. Le type de correction erronée dépend donc des regles de
grammaires empruntées, c.a.d, de la structure syntaxique véritable de la phrase. Si la forme cor-
rigée appartient a une catégorie ouverte, elle a de forte chance de pouvoir se retrouver au sein
de structures variées. Par conséquent, plus le nombre de phrases est élevé, plus les structures
syntaxiques au sein desquelles la forme est présente sont variées et plus les corrections erronées
ont tendance a se disperser. Les correction valides, au contraire, tendent a étre récurrentes.

Nous considérons donc toutes les corrections d’une forme w issue d’une meme phrase comme
un groupe de corrections. Chaque groupe recoit un poids P = c” variant selon sa taille n,
avec c une constante numérique entre ]0, 1[ proche de 1. Plus le groupe est grand, plus bas sera

son poids car plus forte sera la probabilité qu’il soit la conséquence de squelettes syntaxiques
P

permisszfs. Chaque correction a du groupe recoit ensuite un poids pg” = 5 =  Tous les poids

d’une correction sont ﬁnalement additionnés aﬁn de calculer le poids global 3., = Egpgau

Lionel Nicolas, Benoit Sagot, Miguel A. Molinero, Jacques Farré, Eric de La Clergerie

Classiﬁcation multi-analyseur. I-/3tant donné que les corrections erronées générées dépendent
des regles de grammaire empruntées durant les analyses, l’utilisation des résultats provenant de
plusieurs analyseurs avec des grammaires différentes permet d’accentuer leur dispersion, alors
que les correction pertinentes restent habituellement stables. Des corrections sont donc consi-
dérées comme moins pertinentes si elles ne sont pas proposées par l’ensemble des analyseurs.
Nous obtenons donc séparément les corrections de chaque analyseur comme décrit ci dessus et
fusionnons les résultats a l’aide d’une simple moyenne harmonique.

7 Validation manuelle des corrections

Contrairement a (van de Cruys, 2006; Yi & Kordoni, 2006), nous privilégions une approche
semi-automatique impliquant une étape de validation manuelle. Lors de la validation manuelle,
nous avons identiﬁé trois situations possibles.

Soit il n’y a pas de corrections : la détection des formes suspectes a été inadéquate ou la forme
suspectée n’est pas l’unique raison des échecs d’analyse associés.

Soit il y a des corrections pertinentes : la forme a été correctement détectée, la forme est l’unique
raison de (certains) échecs d’analyse associés.

Soit il n’y a que des corrections erronées : l’ambigu'1'té introduite par les jokers a ouvert la voie
vers des analyses erronées fournissant des corrections erronées. Si la grammaire ne couvre pas
toutes les structures syntaxiques possibles, il n’y a aucune garantie qu’il y ait des corrections
pertinentes produites.

Les résultats donnés dans (van de Cruys, 2006) démontrent clairement cet aspect : on peut y voir
que pour les catégories syntaxiques complexes comme les verbes, il est impossible d’appliquer
un tel ensemble de techniques de facon automatisée sans nuire a la qualité du lexique. Si le
but du processus de correction est d’améliorer la qualité du lexique et non pas d’augmenter
artiﬁciellement sa couverture, un tel processus devrait toujours étre seIr1i-automatique.

Comme nous 1’ expliquons plus loin, la validation manuelle n’est pas un tres lourd tribut a payer.
De plus, elle ouvre la possibilité suivante : les lemmes sémantiquement reliés d’une méme
catégorie syntaxique tendent a avoir des comportements syntaxiques similaires. Cette similarité
pourrait étre utilisée pour attirer l’attention du correcteur ou méme générer des corrections pour
des formes non rencontrés/détectés.

Voici quelques exemples de correction validées :

— israélien, portugais, parabolique, pittoresque, minutieux étaient des adjectifs manquants;
revenir ne traitait pas les constructions telles que revenir vers ou revenir de ;

— se partager ne traitait pas les constructions telles que partager (quelque chose) entre ;

— aimer était décrit comme attendant obligatoirement un COD et un attribut;

— livrer ne traitait pas les constructions telles que livrer (quelque chose) £1 quelqu’un.

Le Tableau 1 donne les résultats de 4 sessions de correction. Les premiere et troisieme sessions
ont été réalisées avec la technique statistique de détection. La seconde avec une version brute
de la technique de détection basée sur un étiqueteur et la quatrieme avec la version décrite
précédemment (voir sect 4).

Apres ces quelques sessions, les techniques de détections nous fournissent encore des formes
suspectes mais nous n’obtenons plus de nouvelles corrections valides. Cela peut s’expliquer par
plusieurs raisons. Bien que peu probable, les phrases non analysables restantes peuvent posse-

Trouver et confondre les coupables: un processus sophistiqué de correction de lexique

Session| 1 |2 | 3  4 |total|

nc 30 99 1 6 136
adj 66 694 27 14 801
Verbs 1183 0 385 0 1568
adv 1 7 0 0 8
np 0 0 0 348 348

| total | 1280 | 800 | 413 || 368 | 2861 |

TAB. 1 — Formes lexicales mises a jour a chaque session.

der deux formes erronées; l’introduction d’un seul joker ne sufﬁt donc pas a rendre la phrase
analysable. On peut aussi penser que les couvertures de nos grammaires sont insufﬁsantes, elles
ne sont donc pas en mesure de nous fournir de nouvelles corrections. Cette derniere explication
est privilégiée car, apres la demiere session, l’étape de ﬁltrage des phrases non analysables a
classiﬁée l’essentiel des phrases restantes comme grammaticalement non analysables. Des ses-
sions de correction futures n’auront donc de sens qu’apres des améliorations des grammaires
ou l’application a de nouveaux corpus.

Cependant, cette constatation nous met en mesure de produire des corpus globalement repre-
sentatifs de manques grammaticaux. Si une technique était capable d’utiliser ce corpus pour
suggérer des corrections grammaticales, la Inise a jour de la grammaire nous permettrait de ge-
nérer a nouveau des corrections pour le lexique. Ce qui a nouveau nous permettrait de générer
un corpus représentatif des manques de la grammaire et ainsi de suite. Il serait alors possible de
mettre au point un processus itératif améliorant alternativement et incrémentalement la gram-
maire et le lexique. Le modele d’entropie construit par le classiﬁcateur pourrait étre un bon
point de départ pour établir les manques d’une grammaire.

Pour résumer nos résultats, nous avons déja détecté et corrigé 612 lemmes correspondant a 2861
formes. Il est important de noter que ces corrections ont été obtenues apres seulement quelques
heures de travail manuel. L’ aspect semi-automatique de notre approche n’est donc pas un tres
lourd tribut a payer.

8 Conclusion

Depuis ses premieres versions (Nicolas et al., 2007a; Nicolas et al., 2007b), cet ensemble de
techniques a fortement évolué et les résultats obtenus démontrent sa cohérence et sa viabilité.
Les améliorations prévues devraient renforcer ces résultats et accroitre l’efﬁcacité globale. Une
effort important sera de formaliser l’intérét et l’efﬁcacité des techniques par des métriques qui,
a ce jour, n’eXistent pas pour ce type de probleme.

Pour conclure, cet ensemble de techniques présente actuellement trois avantages importants :

1. Il prend en entrée du texte « non-annoté » produits quotidiennement par des sources
joumalistiques ou techniques facilement accessibles a travers des initiatives tel que le
projet francais Passage5, qui juxtapose des fragments du Wikipedia francais, de sources
Wiki francais, du journal régional L’Est Républicain, d’Europarl et de J RC Acquis.

2. Il permet d’améliorer de facon signiﬁcative un lexique morphologique et syntaxique a

Shttp://atoll . inria. fr/passage.

Lionel Nicolas, Benoit Sagot, Miguel A. Molinero, Jacques Farré, Bric de La Clergerie

large couverture en peu de temps.
3. Enﬁn, son application répétée sur un corpus peut rendre ce corpus représentatif des manques

de la grammaire utilisée. Un tel corpus pourrait étre un point de départ pour le develop-
pement d’un processus dédié a l’amélioration d’une grammaire.

Références

BARG P. & WALTHER M. (1998). Processing unkonwn words in hpsg. In Proceedings of
the 36th Conference of the ACL and the 17th International Conference on Computational
Linguistics.

BOULLIER P. & SAGOT B. (2006). Efﬁcient parsing of large corpora with a deep LFG parser.
In Proceedings of LREC ’06.

DAUME III H. (2004). Notes on CG and LM-BFGS optimization of logistic regression. Paper
http://pub.hal3.name/daume04cg-bfgs, implementation http://hal3.name/megaIn/.

ERBACH G. (1990). Syntactic processing of unknown words. In I WBS Report 131 .

GRANA J. (2000). Te’cnicas de Andlisis Sintactico Robusto para la Etiquetacio’n del Lenguaje
Natural ( robust syntactic analysis methods for natural language tagging). Doctoral thesis,
Universidad de A Coruﬁa, Spain.

MOLINERO M. A., BARCALA F. M., OTERO J. & GRANA J. (2007). Practical application
of one-pass viterbi algorithm in tokenization and pos tagging. Recent Advances in Natural
Language Processing (RANLP). Proceedings, pp. 35-40.

NICOLAS L., FARRE J. & VILLEMONTE DE LA CLERGERIE E. (2007a). Confondre le cou-
pable. In Proceedings of TALN’07, p. 315-324, Toulouse, France.

NICOLAS L., FARRE J. & VILLEMONTE DE LA CLERGERIE E. (2007b). Correction Inining
in parsing results. In Proceedings of LTC ’07, Poznan, Poland.

NICOLAS L., SAGOT B., MOLINERO M. A., FARRE J. & VILLEMONTE DE LA CLERGERIE
E. (2008). Computer aided correction and extension of a syntactic wide-coverage lexicon. In
Proceedings of Coling 2008, Manchester.

SAGOT B. (2005). Automatic acquisition of a Slovak lexicon from a raw corpus. In Lecture
Notes in Artiﬁcial Intelligence 3658 (© Springer- Verlag), Proceedings of TSD ’05, p. 156-163,
Karlovy Vary, République Tcheque.

SAGOT B. & DE LA CLERGERIE E. (2008). Fouille d’erreurs sur des sorties d’analyseurs
syntaxiques. Traitement Automatique des Langues, 49(1). (to appear).

SAGOT B. & VILLEMONTE DE LA CLERGERIE E. (2006). Error Inining in parsing results.
In Proceedings of ACI/COLING’06, p. 329-336, Sydney, Australia.

THOMASSET F. & VILLEMONTE DE LA CLERGERIE E. (2005). Comment obtenir plus des
méta-grammaires. In Proceedings of TALN’05.

VAN DE CRUYS T. (2006). Automatically extending the lexicon for parsing. In Proceedings
of the eleventh ESSLLI student session.

VAN NOORD G. (2004). Error mining for wide-coverage grammar engineering. In Procee-
dings of ACL 2004, Barcelona, Spain.

YI Z. & KORDONI V. (2006). Automated deep lexical acquisition for robust open texts pro-
cessing. In Proceedings of LREC-2006.

