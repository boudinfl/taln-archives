TALN 2009, Senlis, 24-26 juin 2009

Vers une méthodologie d’annotation des entités nommées en
corpus ?

Karen Fort1’3 Maud Ehrmannz Adeline Nazarenko3
(1) INIST, 2 allée du Parc de Brabois, 54500 Vandoeuvre-les-Nancy
karen.fort@inist.fr
(2) XRCE, 6 Chemin de Maupertuis, 38240 Meylan
maud.ehrmann@xrce.xerox.com
(3) LIPN, Université Paris 13 & CNRS, 99 av. J .B. Clément, 93430
Villetaneuse
adeline.nazarenko@lipn.univ-paris13.fr

Résumé. La tache, aujourd’hui considérée comme fondamentale, de reconnaissance d’en-
tités nommées, présente des difﬁcultés spéciﬁques en matiere d’annotation. Nous les précisons
ici, en les illustrant par des expériences d’annotation manuelle dans le domaine de la microbi-
ologie. Ces problemes nous amenent a reposer la question fondamentale de ce que les anno-
tateurs doivent annoter et surtout, pour quoi faire. Nous identiﬁons pour cela les applications
nécessitant l’eXtraction d’entités nommées et, en fonction des besoins de ces applications, nous
proposons de déﬁnir sémantiquement les éléments a annoter. Nous présentons ensuite un cer-
tain nombre de recommandations méthodologiques permettant d’assurer un cadre d’annotation
cohérent et évaluable.

Abstract. Today, the named entity recognition task is considered as fundamental, but it
involves some speciﬁc difﬁculties in terms of annotation. We list them here, with illustrations
taken from manual annotation experiments in microbiology. Those issues lead us to ask the fun-
damental question of what the annotators should annotate and, even more important, for which
purpose. We thus identify the applications using named entity recognition and, according to the
real needs of those applications, we propose to semantically deﬁne the elements to annotate.
Finally, we put forward a number of methodological recommendations to ensure a coherent and
reliable annotation scheme.

M0tS-CléS I annotation, reconnaissance d’entités nommées.

Keywords 2 annotation, named entities extraction.

1 Introduction

Si l’eXtraction d’entités nommées (EN), apparue au milieu des années 1990 a la faveur des
dernieres conférences MUC1, fait aujourd’hui ﬁgure d’incontoumable en Traitement Automa-
tique des Langues (TAL), l’annotation de corpus qui la sous-tend est encore peu étudiée en tant

1Message Understanding Conferences, (MUC, 1995), (MUC, 1998)

Karen Fort, Maud Ehrmann, Adeline Nazarenko

que telle. Les enjeux de l’annotation manuelle sont pourtant importants. Qu’il s’agisse de la
performance des systemes mis au point (a partir d’un travail de modelisation ou d’apprentis-
sage automatique sur corpus), de l’evaluation de ces demiers, ou encore de la bonne reponse
apportee a des besoins applicatifs, l’annotation de corpus est une composante fondamentale.
Au coeur des divers processus constituant la reconnaissance d’entites nommees (REN), nous
souhaitons examiner de plus pres la problematique de l’annotation manuelle, laquelle conduit a
s’interroger sur ce que sont les entites nommees et ce a quoi elles servent.

Nous presentons cette pratique etablie qu’est aujourd’hui l’annotation d’EN (2) puis nous en
detaillons les principales difﬁcultes (3), aussi bien sur des textes de langue generale que de
specialite. Nous examinons ensuite les applications dans lesquelles les EN sont utilisees et en
deduisons les differents types d’annotation (4). Enﬁn, nous proposons des recommandations
methodologiques permettant d’assurer un cadre d’annotation coherent et evaluable (5).

2 Annotation d’entités nommées, une pratique établie

La reconnaissance d’entites nommees est une tache bien connue : initiee il y a une ving-
taine d’annees a l’occasion des conferences americaines MUC, elle fut rapidement reprise lors
d’autres campagnes d’evaluation, suscitant des travaux toujours plus nombreux. Sans revenir
ici sur le “ succes” de cette tache (Nadeau & Sekine, 2007), on peut retracer son evolution selon
trois directions principales. La premiere correspond a des travaux dans le domaine “ general”,
avec la poursuite de la tache deﬁnie par MUC pour d’autres langues que l’anglais, selon un
jeu de categories plus ou moins revisite et pour annoter des entites dans des corpus de nature
journalistique essentiellementz. La seconde direction concerne des travaux dans des domaines
dits “de specialite”, avec la reconnaissance d’entites dans les domaines de la medecine, de la
chimie ou de la Inicrobiologie. I1 fut ainsi propose de reconnaitre des noms de genes, de pro-
teines, etc. dans de la litterature specialisee, lors des campagnes JNLPBA (Kim et al., 2004) et
BioCreAtIvE (Hirschman et al. , 2005). La derniere direction, transversale aux domaines general
et de specialite, correspond a des travaux sur la desambigu'1'sation : resolution de metonymie des
EN dans SemEval2007 (Markert & Nissim, 2007) et desambigu'1'sation de noms de personnes
(Artiles et al., 2007)3.

A l’occasion de chacune de ces campagnes, des corpus furent constitues et annotes manuelle-
ment. De maniere generale, ces corpus annotes servent a la Ir1ise au point d’outils d’annotation
automatique. "Mise au point" est a entendre ici au sens large : il s’agit de decrire le plus precise-
ment possible ce que les systemes doivent faire pour guider le travail d’ecriture des regles sur
lesquelles ils reposent, pour apprendre automatiquement ces regles de fonctionnement ou des
criteres de decision et, enﬁn, pour evaluer les resultats obtenus en les confrontant a une analyse
de reference. Le processus d’annotation met en jeu deux acteurs, un annotateur et un texte, et
aboutit a une annotation du texte dont la qualite doit repondre a une méthodologie et supporter
un protocole d evaluation, et le contenu suivre un guide d ’ann0tati0n.

Pour le domaine general, les campagnes MUC, CoNLL et ACE ont travaille avec des cor-
pus issus de la presse. Ces campagnes semblent avoir porte attention au processus d’annota-
tion manuelle des EN, avec la redaction de guides d’annotation et des calculs d’accord inter-
annotateurs (mais non intra-annotateur), procedant par allers-retours entre le corpus a annoter et

2Voir les campagnes d’eVa1uation MET, IREX, CoNNL, ACE, ESTER et HAREM (Ehrmann, 2008, pp. 19-21).
3La campagne Web People Search fut par la suite reeditee, Voir http : / / nlp . uned . es /weps / .

Vers une méthodologie d’annotation des entités nommées en corpus ?

le guide d’annotation a réviser, mais des points d’hésitations ont perduré quant a l’annotation,
dus principalement a “ diﬁerent interpretations of vague portions of the guidelines ” (Sundheim,
1995) ou a des phénomenes de superposition de sens (Doddington et al., 2004). Dans les do-
maines de la biologie et de la biomedecine, des textes des bases de données de publications
scientiﬁques (PubMed et MedLine4) ont été annotés : on observe que les guides d’annotation —
quand ils eXistent5 — laissent des zones d’ombre quant a la maniere d’annoter les entités et que
peu d’études ont porté sur la qualité de l’annotation. Que ce soit pour les corpus GENIA (Kim
et al., 2003), PennBioIE (Kulick et al., 2004) ou GENETAG (Tanabe et al. , 2005), aucun accord
inter ou intra-annotateur n’est rapporté. A la ﬁn de leur expérience d’annotation, les auteurs de
(Tanabe et al., 2005) constatent que “ a more detailed deﬁnition of a gene/protein name, as well
as additional annotation rules, could improve interannotator agreement and help solve some of
the tagging inconsistencies ”.

On observe par ailleurs des pratiques d’annotation manuelle et une réﬂexion méthodologique
intéressantes dans d’autres domaine du TAL, notaInInent dans les travaux issus de la commu-
nauté parole (Gut & Bayerl, 2004). La campagne EVALDA MEDIA (Bonneau-Maynard et al.,
2005), par exemple, a mis en oeuvre une annotation manuelle de corpus en deux passes, la pre-
miere sur un échantillon pour faire le point sur les éventuels désaccords entre annotateurs, et
une seconde grandeur nature, apres réajustement des guides d’annotation.

3 Difﬁcultés rencontrées dans l’annotation

Pour les corpus de langue générale, un certain nombre de difﬁcultés sont identiﬁées (Ehrmann,
2008). La premiere concerne le choix des catégories et la détermination de ce qu’elles recou-
vrent. En effet, au-dela de la triade “universelle” déﬁnie par MUC (PERSONNE, LIEU et ORGAN-
ISATION), l’inventaire des categories a annoter est difﬁcile a stabiliser eta déﬁnir. Prenons l’ex-
emple de la catégorie PERSONNE : s’il est évident qu’un nom d’individu tel que Lionel Jospin
est a annoter a l’aide de cette catégorie, que faut-il faire des Kennedys, de Zorro, des De’mocrates
ou de St. Nicolas ? Pour les autres catégories, il est également difﬁcile de choisir la granular-
ité des catégories et de déterminer ce qu’elles recouvrent vraiment. Un autre type de difﬁcuté
concerne la sélection des mentions a annoter ainsi que la délimitation des frontieres des EN.
A titre d’exemple, considérons l’EN “Barack Obama” et les diverses unités lexicales suivantes
permettant d’y faire référence : Barack Obama, Monsieur Obama, le Pre’sident des Etats-Unis,
le nouveau president, il. Faut-il annoter les noms propres uniquement, ou peut-on également
considérer les descriptions déﬁnies permettant d’identiﬁer cette personne, voire les pronoms
qui, contextuellement, y renvoient ? Et que fait-on des différents attributs accompagnant l’EN
(“monsieur” et “président”) ? De nombreux autres cas, plus ou moins compliqués (la Maire du
7e arrondissement de Paris et Garde des Sceaux Rachida Dati) se rencontrent en corpus. Dans
le meme ordre d’idée, des phénomenes de coordination et d’imbrication peuvent poser prob-
leme aux annotateurs (une ou plusieurs entités pour Bill et Hillary Clinton et l’Universite’ de
Corte ?). Enﬁn, une derniere difﬁculté résulte de phénomenes de pluralité référentielle, avec des
EN homonymes (Orange ville et Orange compagnie) et des glissements métonymiques, parfois
difﬁciles a distinguer (France en tant que lieu géographique, gouvemement ou équipe sportive).
Meme si elles sont surmontables grace aux guides d’annotation, ces difﬁcultés entrainent un
coﬁt supplémentaire et une baisse de qualité de l’annotation.

4Respectivement : http: //www . ncbi . nlm . nih . gov/pubmed/ et http : //medline . cos . com/
5Certaines campagnes comme JNLPBA et BioCreAtIvE I n’en ont pas foumi.

Karen Fort, Maud Ehrmann, Adeline Nazarenko

Notre expérience en Inicrobiologie montre que ces difﬁcultés sont plus aigues encore dans les
langues de spécialité. Une expérience d’annotation a été réalisée a l’INIST pour le programme
Quaero, en collaboration avec l’équipe MIG. Un corpus anglais de 499 notices PubMed (titres
et résumés, soit environ 110 000 “mots”), pré-annotées par application d’un dictionnaire et d’un
anti-dictionnaire, a été fourni a deux experts de 1’ INIST, dont le travail a consisté en une révision
de ces pré-annotations. La principale difﬁculté rencontrée a concerné la distinction qui était de-
mandée entre noms propres et noms communs, la liIr1ite morphologique entre les deux étant peu
marquée dans ces domaines ou les noms communs sont souvent reclassés comme "noms pro-
pres", comme en atteste la présence de ces noms dans des nomenclatures (“small, acid-soluble
spore protein A” est ici un cas extreme) ou les phénomenes d’acronymisation (on trouve par
exemple "across the outer membrane (OM)"). Dans ces cas, la consigne donnée aux annota-
teurs était de se référer a des listes d’autorité, telle que Swiss-Prot6, ce qui entraine une perte de
temps conséquente. La délimitation des frontieres des elements a annoter a elle-aussi soulevé de
nombreux questionnements, les annotateurs se demandant ce qu’il fallait inclure ou non dans le
segment annoté. On peut ainsi choisir d’annoter “nith messenger RNA" si on considere que la
mention de l’état "messenger RNA" entre dans la détermination de la référence, ou seulement
“nifh”, si on considere que le nom propre sufﬁt a construire la référence. Le typage sémantique
choisi a aussi posé probleme aux annotateurs, notamment pour les éléments génétiques mobiles,
comme les plasmides ou les transposons. En effet, ceux-ci devaient étre annotés dans les taxons
et non dans les genes alors que ce sont des fragments d’ADN, donc des parties de génome. Une
directive particulierement perturbante pour les annotateurs a été d’annoter l’acronyme “KGFR”
comme nom propre et sa forme développée “keratinocyte growth factor receptor” comme nom
commun. Ce type de consigne, préconisée au départ pour entrainer plus efﬁcacement les outils
de REN (Nédellec et al., 2006), est difﬁcile a appréhender et aurait dﬁ étre Inieux documentée.

Ces problemes se traduisent par un coﬁt d’annotation élevé, des guides d’annotation de taille
trop grande par rapport au corpus et trop d’hésitations de la part des annotateurs, ce qui induit
des incohérences et une qualité moindre de l’annotation. Cette expérience nous a ainsi permis
de reposer la question de ce que les annotateurs doivent annoter et surtout, pour quoi faire.

4 Annoter quoi ?

Pour mieux comprendre "quoi annoter dans quel texte", nous revenons sur les criteres linguis-
tiques qui permettent de déﬁnir la notion d’EN : l’importance de ces criteres varie d’une appli-
cation a l’autre et les annotations résultantes en dépendent.

4.1 Ditférents critéres déﬁnitoires

(Ehrmann, 2008) propose une analyse linguistique de la notion d’EN qu’elle présente comme
une "création" du TAL. Dans ce qui suit, nous reprenons la distinction introduite dans (LDC,
2004) : les EN sont des "mentions" qui renvoient a des "entités" du domaine, ces mentions
pouvant relever de différentes catégories linguistiques : des noms propres ("Rabelais"), mais
aussi les pronoms ("il"), et plus largement des descriptions déﬁnies ("le pere de Gargantua").
On peut identiﬁer plusieurs criteres déﬁnitoires des EN.

Ghttp://www.expasy.org/sprot/

Vers une méthodologie d’annotation des entités nommées en corpus ?

Unicité référentielle L’une des caractéristiques principales des noms propres est leur fonction-
nement référentiel : un nom propre renvoie a une entité référentielle unique, meme si cette
unicité est contextelle. A la difference de (Poibeau, 2005), nous considérons que cette propriété
est essentielle dans l’utilisation que fait le TAL des EN.

Autonomie référentielle Les EN sont de surcroit autonomes du point de vue référentiel. C’est
évident dans le cas du nom propre qui permet a lui seul l’identiﬁcation du référent, tout au moins
dans une situation de communication donnée (Eurotunnel). Le cas des descriptions déﬁnies
(l’0pe’rateur du tunnel sous la Manche) est un peu different : si elles sufﬁsent a identiﬁer le
référent, c’est par le truchement de connaissances extérieures.

Stabilité dénominative Les noms propres sont également des dénominations stables. Meme s’il
y a des variations (Angela Merkel/Mme Merkel/A. Merkel), elles sont plus régulieres et moins
nombreuses que pour les autres syntagmes nominaux7.

Relativité référentielle L’interprétation se fait toujours relativement a un modele du domaine,
qui peut étre implicite dans les cas simples (on suppose une connaissance partagée de ce qu’est
une personne ou un pays) mais qui doit étre explicité des que la diversité des entités a prendre
en compte s’accroit (il faut au moins une typologie pour les catégoriser).

4.2 Différentes visées applicatives

La REN voit le jour dans un cadre d’extraction d’information ou il s’agit d’identiﬁer les actants
de certaines situations (des attentats aux interactions géniques), cette situation étant décrite par
un formulaire a instancier avec les informations extraites du texte. La sémantique sous-jacente
est clairement référentielle : on identiﬁe dans le texte les "mentions" des "entités" qui jouent un
role dans les situations considérées (LDC, 2004). Dans les systemes de questions/réponses, les
EN jouent le meme role mais on a souvent recours a des typologies plus ﬁness.

Les EN sont également utilisées comme "descripteurs" dans de nombreuses applications d’in-
dexation. On a des index de noms propres dans certains ouvrages et moteurs de recherche et on
sait qu’une importante proportion des requétes adressées aux moteurs de recherche sont des EN.
On surligne les EN pour aider la lecture ou la navigation dans de gros volumes documentaires.
On utilise enﬁn la REN pour construire et mettre a jour des nomenclatures (Tran & Maurel,
2006) utilisées pour l’indexation, la traduction, etc. Dans ce deuxieme type d’application aussi,
la sémantique sous-jacente est référentielle : si les EN sont de bons descripteurs, c’est qu’elles
fonctionnent comme des ancres référentielles qui permettent de situer ce a quoi le texte fait
référenceg. En revanche, on s’intéresse exclusivement aux mentions de type nom propre.

Troisieme type d’applications, les EN sont utilisées pour l’intégration de données. Cela con-
ceme a la fois l’analyse des bases documentaires (suivi de themes, découverte de communautés
de pratiques (Li & Liu, 2005) et l’articulation des documents avec d’autres sources de connais-
sances (bases de données, bases d’images, etc.) pour interroger les unes et les autres de maniere
homogene et naviguer facilement de l’une a l’autre (Dragos & Nazarenko, 2009). On s’appuie
alors sur les EN référencées dans les bases de données pour établir des liens entre les différentes
sources. Pour ces taches d’intégration, on se contente des EN qui ﬁgurent dans les nomencla-

7Ceci explique a contrario l’importance du repérage de la synonymie dans les domaines o1‘1 les dénominations
sont peu stables (la génomique, par exemple).

8Ce qui pose des problémes de désainbiguisation et de résolution des métonymies.

9Deux noms propres (AZF et septembre 2001) sufﬁsent souvent a faire comprendre de quoi parle une dépéche !

Karen Fort, Maud Ehrmann, Adeline Nazarenko

tures, l’objectif étant d’articuler les sources entre elles et pas d’en décrire le contenu : il sufﬁt
de savoir qu’une dépéche parle de Nelson Mandela, inutile de trouver toutes les mentions qui
en sont faites dans le texte.

L’anonyIr1isation des documents est un quatrieme champ d’application (Plamondon et al. , 2004).
On veut éviter qu’on puisse identiﬁer des entités (des personnes, notamment) a partir des men-
tions qui en sont faites dans le texte. I1 faut repérer toutes les formes de mentions (la ville qui a
regu la premiére bombe atomique) et pas seulement les noms propres.

Derniere famille d’applications, la REN est utilisée pour "peupler" des ontologies. I1 s’agit alors
de modéliser un domaine, et le modele formel des ontologies impose de distinguer les entités
du domaine, qui sont représentées comme des instances, des concepts ou classes auxquelles
ces instances se rattachent. La REN est alors utilisée pour enrichir la structure conceptuelle
avec des instances de concepts ou de roles (relations entre instances) (Amardeilh et al., 2005).
Selon les cas, on privilégie les entités nommées ou on s’intéresse a toutes les mentions d’entités.
Dans le premier cas, l’ontologie résultante peut étre vue comme une nomenclature hiérarchisée
ou comme un thesaurus formalisé. Dans le second, on cherche a modéliser un domaine en
identiﬁant les entités qui le composent et les relations qu’elles entretiennent. Dans les deux
cas, le typage des EN est essentiel parce qu’on doit relier l’instance nommée a un concept de
l’ontologie.

4.3 Des perspectives d’annotation différentes

Les criteres déﬁnitoires cités en 4.1 ne jouent pas tous le méme role pour toutes les applica-
tions. Dans certains cas (indexation et intégration de connaissances), on s’intéresse a des entités
référentielles qui sont désignées par des descripteurs stables et non ambigus. Ce sont donc les
EN de type noms propres ou "catalogables" qui sont a retenir et il est important de les normaliser
pour s’affranchir des variations qui peuvent apparaitre malgré leur stabilité référentielle. Pour
ce type d’application, l’essentiel n’est pas de repérer toutes les mentions de telle entité dans un
document mais de repérer que ce document mentionne telle entité. I1 faut donc privilégier la
précision sur le rappel de l’annotation.

A l’autre extréme, on trouve les taches d’extraction d’information et de modélisation du do-
Inaine ou toutes les mentions sont importantes a repérer, y compris celles qui sont des descrip-
tions déﬁnies (il faut d’ailleurs repérer des relations de coréférence entre les mentions qui ne
sont pas sufﬁsamment autonomes référentiellement). La ﬁgure 1 montre l’impact de ces deux
perspectives d’annotation sur les résultats de l’annotation d’un tout petit exemple1°.

Comme il est impossible de repérer les mentions de toutes les entités référentielles, le modele
du domaine détermine quelles sont les entités "d’intérét" et la limite entre ce qui doit ou non
étre annoté. Illustrons ce point sur un exemple. Quand un directeur des ressources humaines
s’intéresse aux grilles salariales de son organisation, il raisonne sur des catégories de personnels
et pas sur les personnes physiques que sont les employés. Cela se reﬂete dans le modele du
domaine : les différentes catégories de personnes (techniciens, ingénieurs, etc.) sont modélisées
comme des instances rattachées au concept CAT-DE-PERSONNEL et les personnes physiques ne
sont pas représentées. A l’inverse, quand il s’occupe des ﬁches de paye et de la progression
des employés, il s’intéresse aux individus. Dans ce cas, le modele doit considérer les personnes
comme instances et les catégories de personnels comme des concepts. Le méme probleme se

whttp ://www.ncbi.nl1n.nih. govlpubmedl 1 33 15 32

Vers une méthodologie d’annotation des entités nommées en corpus ?

ANNOTATION D’INDEXATION; types gene et protein
We conclude that <gene>3CDproMdgene> can process both structural and nonstructural precursors of the <EukVirus
uncertainty-type="too-generic">po|iovirus po|yproteindEukVirus> and that it is active against a synthetic peptide substrate.

ANNOTATION DE MODELISATION; types taxon, gene et protein

We conclude that <EukVirus>3CDproMdEukVirus> can process both structural and nonstructural precursors of the <EukVirus
uncertainty-type="too-generic"><taxon>po|iovirus</taxon> po|yprotein</EukVirus> and that <EukVirus>itdEukVirus> is active
against a synthetic peptide substrate.

FIG. 1 — Exemple d’annotation en biologie. La premiere annotation est moins riche que la seconde qui
considere plus de types sémantiques (t axon) avec une granularité plus ﬁne (EuKVirus is a subtype
of gene), ce qui introduit des enchassements de balises. Par ailleurs, toutes les mentions sont annotées
dans la seconde annotation alors que seuls les assimilés noms propres le sont dans la premiere.

rencontre en biologie ou la mention du gene G peut renvoyer au gene G de l’espece E, au gene
G d’un individu de l’espece E ou in un gene particulier parmi les genes G de cet individu.

Modéliser suppose de faire des choix explicites 1a ou les textes peuvent étre ﬂous et méler les
points de vue. Il est donc impossible d’annoter les EN d’un texte indépendamment d’un modele
de référence. Dans le cas de l’expérience décrite plus haut, le modele était, comme souvent,
simplement décrit par une liste de concepts : il fallait nommer les genes et protéines, mais aussi
leurs familles, compositions, et composants. Par ailleurs, comme la REN doit servir a modéliser
le réseau d’interactions entre genes, il était essentiel de repérer toutes les mentions des entités
considérées, chacune pouvant contribuer a enrichir sa modélisation.

5 Recommandations méthodologiques

Annoter est difﬁcile. I1 faut donc guider le travail des annotateurs, ce qui passe par des guides
et des outils d’annotation, mais aussi par l’évaluation de la qualité des annotations.

Guides d’ann0tati0ns Comme le type d’annotation a produire dépend de ce qu’on cherche a
annoter et de ce 5; quoi cette annotation doit servir, il est essentiel de foumir aux annotateurs des
guides (ou conventions) d’annotation qui indiquent ce qu’il faut annoter plut6t que comment
annoter. Trop souvent en effet, les contraintes de faisabilité prennent le pas sur les criteres
sémantiques“ ce qui brouille l’objectif pour les annotateurs. Par ailleurs, il est important de
prendre la mesure de la tache dans toute sa complexité sans exclure a priori ce qui serait douteux
ou trop difﬁcile a reproduire automatiquement. C’est méme l’intérét de l’annotation manuelle
que de donner une idée de l’ampleur de la tache d’annotation.

I1 faut donner aux annotateurs une vision claire de l’application visée. Cette vision doit s’ap-
puyer sur un modele de référence explicite, du type de celui donné dans la campagne GENIA,
avec des déﬁnitions précises et des explications sur les choix méthodologiques réalisés (cate-
gories, typage sémantique, etc.). Des exemples peuvent étre ajoutés a titre d’illustration mais ils
ne doivent pas se substituer a la déﬁnition des objectifs. Ces informations permettont de limiter
les incompréhensions, mais aussi de responsabiliser et de motiver les annotateurs en leur don-
nant acces a la logique sous-jacente. On entre ainsi dans une démarche didactique, permettant
de passer d’un “rapport de pere a un rapport de pair" (Akrich & Boullier, 1991), ce qui est
d’autant plus nécessaire que l’annotation porte sur des corpus spécialisés, les annotateurs étant

11 "In [src homology 2 and 3], it seems excessive to require an NER program to recognize the entire fragment,
however, 3 alone is not a Valid gene name." (Tanabe et al., 2005).

Karen Fort, Maud Ehrmann, Adeline Nazarenko

des experts a qui on a interet a donner la plus grande autonoInie possible. Dans certains cas, la
tache d’annotation etant trop complexe, on doit se restreindre a une annotation exploitable pour
l’apprentissage automatique — en se limitant par exemple aux noms propres — mais cela doit étre
fait de maniere explicite pour que les annotateurs aient une vue claire des choix a faire.

Outils d’annotation Le travail d’annotation doit etre outille. On utilise des dispositifs tech-
niques pour annoter les textes mais aussi pour preparer le travail d’annotation par une pre-
annotation (projection d’un dictionnaire, par exemple). Ces outils sont cependant a manier avec
precaution du fait des biais qu’ils introduisent. Dans l’experience que nous avons menee, le
corpus d’annotation avait ete pre-annote par projection d’un dictionnaire de noms de genes et
de proteines, pour alleger le travail des annotateurs, mais cette pre-annotation a inﬂuence l’an-
notation. Elle a introduit un biais en faveur de la correction des pre-annotations, au detriment de
la recherche de nouvelles EN. Une solution consiste a proceder en deux temps, en etiquetant a
la main un echantillon du corpus, puis en le pre-annotant pour comparer les resultats et evaluer
le biais introduit. On peut ensuite faire une annotation a plus grande echelle, precedee par une
pre-annotation dont on connaitra cette fois les biais.

En ce qui concerne le processus d’annotation lui-meme, s’il existe aujourd’hui de nombreux
outils d’aide a l’annotation manuelle, peu sont effectivement disponibles, c’est-a-dire gratuits,
telechargeables et utilisables . On peut citer, entre autres, Callisto, MMAX2, Knowtator ou en-
core Cadixe” qui a ete utilise dans notre experience. Les fonctionnalites et l’expressivite du
langage d’annotation doivent etre adaptees a la tache d’annotation visee : selon les cas, il faut
typer des segments de textes ou les mettre en relation, on a des annotations concurrentes, dis-
jointes ou superposables, etc. Dans notre experience en microbiologie, par exemple, le fait de
ne pas pouvoir annoter les coordinations d’EN a pose probleme. Les criteres ergonomiques sont
egalement importants : on sait que les fonctionnalites difﬁciles d’acces ne sont pas utilisees, ce
qui biaise la aussi les resultats. Dans notre cas, les annotateurs ont souvent neglige de mention-
ner leur incertitude parce qu’ajouter l’attribut correspondant a leurs annotations etait malaise.

Evaluation de l’annotation Il est important de mesurer la qualite de l’annotation. (Gut & Bay-
erl, 2004) distingue l’accord inter-annotateur, qui permet de mesurer la stabilite de l’annotation,
et l’accord intra-annotateur, qui donne une indication de la reproductibilite de l’annotation. S’il
est important de calculer l’accord inter-annotateur, il n’est pas necessaire de le realiser sur tout
le corpus, ne serait-ce que pour des raisons de coﬁt. Il est en revanche conseille de le calculer
tot, aﬁn d’identiﬁer les problemes rapidement et de modiﬁer l’annotation en consequence. 11 en
va de meme en ce qui concerne l’accord intra-annoteur.

Un autre moyen d’evaluation consiste a faire ajouter a l’annotateur, si necessaire, une note d’in-
certitude, de preference typee, sur ses annotations. En l’absence de calcul de l’accord inter ou
intra-annotateur pour cette annotation nous avons ainsi fait annoter les incertitudes des anno-
tateurs a posteriori sur un sous-ensemble du corpus. Pour s’assurer de la representativite de
l’echantillon, les annotateurs ont identiﬁe une typologie des ﬁchiers du corpus (six “types”).
Nous avons ensuite extrait 25 ﬁchiers de types varies parmi les 499 notices du corpus (soit 5%).
L’ objectif de cette seconde validation etant d’evaluer la conﬁance des annotateurs en leur val-
idation, nous avons choisi de proﬁter du tag uncertainty propose dans Cadixe et de l’utiliser
de maniere systematique en cas de doute. Nous avons donc redeﬁni les types d’uncertaim‘y en
fonction de l’experience des annotateurs. Cette evaluation n’a necessite que quelques heures de
travail et nous a perInis de mieux qualiﬁer et quantiﬁer leurs doutes. Au ﬁnal, sur 555 tags, les

urespectivement: http : //callisto .mitre . org/, http : //mmax2 . sourceforge . net /, http:
//knowtator . sourceforge . net/, http : //caderige . imag . fr/Cadixe/

Vers une méthodologie d’annotation des entités nommées en corpus ?

annotateurs ont déclaré 113 uncertainty, soit environ 20% des tags. On observe que plus de 75%
des incertitudes concernent les noms communs de type bacteria, et que ces incertitudes sont tres
largement (77%) liées a une difﬁculté a distinguer les noms communs des noms propres.

Plus généralement, pour s’assurer a la fois de la qualité du guide d’annotation et des possibilités
d’évaluation, une bonne méthode consiste, en tout début de projet, a faire travailler les annota-
teurs chacun de leur cote sur un échantillon du corpus. Cela permet d’identiﬁer rapidement les
désaccords, puis de les trancher, soit en faisant intervenir un autre expert, soit par concertation
des annotateurs. Ces décisions sont ensuite reportées dans le guide d’annotation.

6 Conclusion et perspectives

Au ﬁnal, une démarche rigoureuse et efﬁcace d’annotation d’EN en corpus doit préter attention
principalement a deux points. Au regard du contenu tout d’abord, il importe de se focaliser non
pas tant sur comment annoter, mais sur quoi annoter, en fonction de l’application visée. Nous
avons vu que chaque famille d’application spéciﬁe un certain nombre de criteres linguistiques,
et que les mentions a annoter different relativement a ces derniers. Une fois spéciﬁe ce qu’il faut
annoter, il faut étre prudent en termes de méthodologie et envisager des le départ l’évaluation de
l’annotation. Il est avantageux, du point de vue de la qualité de l’annotation, de réaliser un ou
plusieurs galops d’essai, aﬁn d’ajuster certains éléments du guide d’annotation et d’évaluer les
biais éventuels introduits par l’outil d’annotation et/ou une mauvaise compréhension de la tache
par les annotateurs. Relativement aux enj eux et aux difﬁcultés de l’annotation des EN en corpus,
l’attention portée a la spéciﬁcation du contenu et au respect d’une méthodologie d’annotation
permet d’assurer un cadre de travail stable et cohérent a chacune des étapes de la tache de REN.
Nous comptons appliquer cette méthodologie dans le cadre des campagnes d’annotation de
Quaero, en pharmacologie et en économie. Couvrant la terminologie et l’extraction de relations
sémantiques, ces campagnes dépassent largement le cadre des EN, nous allons donc élargir
notre méthode a ces applications.

Remerciements

Ce travail a été réalisé en partie dans le cadre du programme Quaero 13, ﬁnancé par OSEO,
agence nationale de valorisation de la recherche. Nous en remercions les participants, en par-
ticulier l’équipe MIG de l’INRA. Nous remercions également F. Tisserand et B. Taliercio, les
annotateurs experts de l’INIST.

Références

(1995). Proc. of the 6th Message Understanding Conference. Morgan Kaufmann Publishers.
(1998). Proc. of the 7th Message Understanding Conference. Morgan Kaufmann Publishers.

AKRICH M. & BOULLIER D. (1991). Savoirfaire et pouvoir transmettre, chapter Le mode
d’emploi, genese, forme et usage, p. 113-131. éd. de la MSH (coll. Ethnologie de la France).

Bhttp://www.quaero.org

Karen Fort, Maud Ehrmann, Adeline Nazarenko

AMARDEILH F., LAUBLET P. & MINEL J .-L. (2005). Document annotation and ontology
population from linguistic extractions. In Proc. of K- CAP’05 , p. 161-168, New York : ACM.
ARTILES J ., GONZALO J . & SEKINE S. (2007). The semeval-2007 WePS evaluation : estab-
lishing a benchmark for the web people search task. In Proc. of SemEval, ACL, Prague.

BONNEAU-MAYNARD H., RossET S., AYACHE C., KUHN A. & MOSTEFA D. (2005). Se-
mantic annotation of the french media dialog corpus. In InterSpeech, Lisbonne, Portugal.

DODDINGTON G., MITCHELL A., PRZYBOCKI M., RAMsHAw L., STRASSEL S. &
WEISCHEDEL R. (2004). The ACE program tasks, data, and evaluation. In Proc. of LREC ’04,
Lisbonne, Portugal.

DRAGOS V. & NAZARENKO A. (2009). Towards a semantic model to enhance knowledge
sharing and discovery in organic chemistry. In Proc. of the IADIS IS ’09, Barcelone, Espagne.
EHRMANN M. (2008). Les entités nomme’es, de la linguistique au TAL .' statut théorique et
me’thodes de de’sambigu'i'sation. PhD thesis, Université Paris 7.

GUT U. & BAYERL P. S. (2004). Measuring the reliability of manual annotations of speech
corpora. In Proc. of Speech Prosody, p. 565-568, Nara, J apon.

HIRSCHMAN L., YEH A., BLASCHKE C. & VALENCIA A. (2005). Overview of biocreative :
critical assessment of information extraction for biology. BM C Bioinformatics, 6(1).

KIM J .-D., OHTA T., TATEISI Y. & TSUJII J . (2003). Genia corpus-a semantically annotated
corpus for bio-textmining. Bioinformatics, 19, 180-182.

KIM J .—D., OHTA T., TSURUOKA Y., TATEISI Y. & COLLIER N. (2004). Introduction to the
bio-entity recognition task at J NLPBA. In Proc. of JNLPBA COLING’04 Workshop, p. 70-75.

KULICK S., BIEs A., LIBERMAN M., MANDEL M., MCDONALD R., PALMER M., SCHEIN
A. & UNGAR L. (2004). Integrated annotation for biomedical information extraction. In HLT-
NAACL 2004 Workshop .' Biolink : ACL.

LDC (2004). ACE (Automatic Content Extraction) English Annotation Guidelines for Entities.
Livrable version 5.6.1 2005.05.23, Linguistic Data Consortium.

LI X. & LIU B. (2005). Mining community structure of named entities from free text. In
Proc. of CIKM’05, p. 275-276, New York, NY, USA : ACM Press.

MARKERT K. & NISSIM M. (2007). Semeval-2007 task 08 : Metonymy resolution at semeval-
2007. In Proc. of SemEval, ACL, Prague.

NADEAU D. & SEKINE S. (2007). A survey of named entity recognition and classiﬁcation.
Linguisticae Investigaciones, 30(1), 3-26.

NEDELLEC C., BEssIEREs P., BOSSY R., KOTOUJANSKY A. & MANINE A.—P. (2006).
Annotation guidelines for machine learning-based named entity recognition in microbiology.
In Proc. of the Data and text mining in integrative biology workshop, p. 40-54, Berlin.

PLAMoNDoN L., LAPALME G. & PELLETIER F. (2004). AnonyIr1isation de décisions de
justice. In Proc. of TALN’04, p. 367-376, Fes, Maroc.

POIBEAU T. (2005). Sur le statut référentiel des entités nommées. In Proc. of TALN’05, p.
173-182, Dourdan, France.

SUNDHEIM B. (1995). Overview of results of the MUC-6 evaluation. In (MUC, 1995).

TANABE L., XIE N., THoM L., MATTEN W. & WILBUR1 J . (2005). Genetag : a tagged
corpus for gene/protein named entity recognition. Bioinformatics, 6.

TRAN M. & MAUREL D. (2006). Prolexbase : un dictionnaire relationnel multilingue de
noms propres. TAL, 47(3), 115-139.

