TALN 2009, Senlis, 24-26 juin 2009

Nouveau paradigme d’évaluation des systemes de dialogue
homme-machine

Marianne Laurentl Ghislain Putoisl Philippe Bretier 1 Thierry Moudencl
(1) Orange Labs, Lannion, 2 avenue Pierre Marzin, 22307 Lannion Cedex

Résumé. L’ evaluation des systemes de dialogue homme-machine est un probleme difﬁ-
cile et pour lequel ni les objectifs ni les solutions proposées ne font aujourd’hui l’unanimité. Les
approches ergonomiques traditionnelles soumettent le systeme de dialogue au regard critique de
l’utilisateur et tente d’en capter l’expression, mais l’absence d’un cadre objectivable des usages
de ces utilisateurs empéche une comparaison entre systemes différents, ou entre evolutions d’un
méme systeme. Nous proposons d’inverser cette vision et de mesurer le comportement de l’uti-
lisateur au regard du systeme de dialogue. Aussi, au lieu d’évaluer l’adéquation du systeme
a ses utilisateurs, nous mesurons l’adéquation des utilisateurs au systeme. Ce changement de
paradigme permet un changement de référentiel qui n’est plus les usages des utilisateurs mais
le cadre du systeme. Puisque le systeme est completement déﬁni, ce paradigme permet des
approches quantitatives et donc des evaluations comparatives de systemes.

Abstract. Evaluation of a human-machine dialogue system is a difﬁcult problem for
which neither the objectives nor the proposed solutions gather a unanimous support. Traditio-
nal approaches in the ergonomics ﬁeld evaluate the system by describing how it ﬁts the user
in the user referential of practices. However, the user referential is even more complicated to
formalise, and one cannot ground a common use context to enable the comparison of two sys-
tems, even if they are merely an evolution of the same service. We propose to shift the point of
View on the evaluation problem : instead of evaluating the system in interaction with the user
in the user’s referential, we will now measure the user’s adequacy to the system in the system
referential. This is our Copernician revolution : for the evaluation purpose, our system is no
longer user-centric, because the user referential is not properly objectiﬁable, while the system
referential is completely known by design.

M0tS-CléS 2 Evaluation, Dialogue.

Keywords: Evaluation, Dialogue.

Marianne Laurent, Ghislain Putois, Philippe Bretier et Thierry Moudenc

1 Introduction

La question de l’évaluation est aujourd’hui un enjeu majeur pour la recherche en dialogue vocal.
Pour preuve, nombreuses sont les manifestations aujourd’hui dédiées a ce sujet (J okinen et al.,
2007) (McTear et al., 2008). L’ étude critique réalisée par (Paek, 2007; Paek, 2001) a identiﬁé
deux perspectives différentes face a cette problématique. D’un cote, la recherche académique
s’attache a identiﬁer quelle est la meilleure démarche d’évaluation pour comparer précisément
des systemes de dialogue entre eux. De l’autre, l’industrie, qui est confrontée a des objectifs
opérationnels de déploiement en situation réelle (contrainte de coﬁt et nécessité de qualité),
réﬂéchit a des outils pour l’aider a concevoir de meilleurs systemes de dialogue vocal.

De fagon générale, avec les outils dont elle dispose, l’industrie n’a qu’une vue partielle de la
qualité de ses services. Certes elle sait collecter une grande variété de mesures, de maniere a
la fois quantitative sur des sélections d’indicateurs, et qualitative pour estimer la satisfaction
utilisateur, mais elle bute sur la déﬁnition de processus qui puissent répondre de maniere ﬁable
a ses besoins. Ce manque de métriques de qualité complique la tache des concepteurs, mais nuit
également a l’essor des systemes de dialogue vocal par frilosité des entreprises susceptibles
de les déployer. Aussi, en marge de la recherche d’un paradigme universel d’évaluation pour
la comparaison entre systemes, nous présentons ici une méthode pragmatique proposant aux
concepteurs des clés pour évaluer leur systeme face aux attentes opérationnelles.

Dans les sections suivantes, nous présentons notre contexte industriel de développement de
systeme de dialogue, puis nous introduisons une nouvelle approche au probleme de l’évaluation,
et présentons enﬁn des méthodes d’évaluation associées a ce changement d’approche, et qui
viennent soutenir l’industrie tout au long du cycle de vie de ses systemes.

2 Développement industriel d’un systéme de dialogue vocal

Les systemes de dialogue vocaux sont des objets complexes. Aussi, leur évaluation, qui, selon
l’objectif, peut intervenir a tout moment du cycle de développement, requiert la prise en compte
de différents points de vue (en particulier logiciel, fonctionnel, pragmatique, sociologique). Po-
sons tout d’abord quelques déﬁnitions associées a ces points de vue. Nous désignons par systéme
de dialogue la plateforme matérielle et les composantes logicielles associées : moteurs de recon-
naissance et synthese vocales, gestion des lignes téléphoniques, composants de comprehension
et de génération en langue naturelle, moteur de dialogue. La logique de dialogue désigne la 10-
gique suivie par le systeme de dialogue lors de ses interactions avec l’utilisateur. Elle est dédiée
a une tache et suit une logique métier déﬁnie pour l’accomplir. Enﬁn, l’applicati0n englobe
l’ensemble formé par le systeme de dialogue et la logique de dialogue.

Chez Orange, la conception industrielle d’une application de dialogue s’organise en quatre
étapes. Tout d’abord, la phase de réalisation correspond a la création d’une premiere version
de l’application. Puis celle-ci est ensuite progressivement améliorée par les phases d’expéri-
mentation et pilote au travers une séries d’itérations. Enﬁn la phase d’exploitation marque son
déploiement. A chaque étape on effectue une analyse des interactions menées entre l’appli-
cation et ses utilisateurs de sorte, notaInment, a identiﬁer les écarts entre les comportements
utilisateurs attendus et ceux observés, tant en matiere de type de comportement (typologie de
réponses et de questions) que d’occurrence (fréquence, délais) ou de forme (choix lexicaux,
forme syntaxique). Cette analyse nourrit ainsi les itérations qui correspondent a des retours

Nouveau paradigme d’évaluation des systemes de dialogue homme-machine

de l’application en conception durant lesquels on modiﬁe son comportement en fonction des
expériences observées dans les traces de dialogue.

La phase de réalisation s’articule autour des étapes de conception (réﬂexions, spéciﬁcations,
modélisation), de développement et de test de l’application au sein méme de l’équipe de déve-
loppement. Celle-ci met en place tous les aspects d’une version initiale de l’application : design,
linguistique, ergonomie et technique (notamment lié a l’intégration de l’application dans son en-
vironnement informatique). L’ evaluation a cette étape repose sur des tests effectués par l’équipe
de développement et constitue une évaluation du bon fonctionnement technique.

Lors de la phase d’ expérimentation, l’application est testée par un nombre restreint d’utilisateurs
puis ouvert a un faible ﬂux de traﬁc réel. On collecte alors la matiere nécessaire a l’analyse des
comportements utilisateur face a l’application. Cette analyse permet d’alimenter les boucles
d’itérations qui vont se succéder pour améliorer l’application jusqu’a l’atteinte d’un niveau de
performance jugé satisfaisant. La collecte et l’analyse du corpus d’interaction permettent ainsi
de capturer des usages et des réactions des utilisateurs et donc d’adapter les différents para-
metres du design, tels que les modeles de reconnaissance vocale, la syntaxe et la terminologie
des prompts ou la temporisation du dialogue. L’ evaluation a cette étape vise donc l’amélioration
de l’application etnotaIr1mentl’a1ignementdes prévisions et des observations.

Lors de la phase pilote, les tests sont effectués dans les conditions réelles d’eXploitation. D’abord,
le panel d’utilisateurs peut couvrir une zone géographique (souvent région ou département), une
zone téléphonique, une zone de plateau de téléconseillers ou un panel de clients cibles, mais elle
couvre toujours une partie du ﬂux réel. Ensuite, l’évaluation est réalisée a partir de l’architecture
technique complete de l’application. Cette étape correspond donc a une phase de beta-testing
ou les problemes majeurs sont décelés et comblés avant la Inise en production qui ouvre l’ap-
plication a l’ensemble des usagers. L’ evaluation a cette étape a pour but de valider globalement
l’application.

Enﬁn, la phase d’eXploitation correspond a la mise a disposition totale de l’application avec
maintien opérationnel. L’ évaluation se focalise alors sur la supervision. Cette évaluation permet
notamment d’analyser la facon dont l’application est utilisée en situation réelle par les utili-
sateurs, d’analyser l’usage et les retours d’expérience utilisateurs et leurs évolutions dans le
temps.

3 Changement de paradigme

Comme pour tout processus d’évaluation, évaluer une application de dialogue nécessite un ré-
férentiel d’étude. Les tentatives traditionnelles cherchent a appréhender les usages des utilisa-
teurs pour mesurer l’adéquation de l’application a ces usages (Norros & Savioja, 2007). Ces
méthodes d’évaluation recensées par (Grislin & Kolski, 1996) et (Paek, 2001) s’articulent en
deux étapes. D’abord on collecte un corpus de travail par le biais d’études qualitatives telles
que des tests terrains, des interviews, des questionnaires de satisfaction ou des simulations en
Magicien d’Oz (Caelen et al., 1997). Puis les ergonomes interpretent des usages et des attentes
utilisateurs a partir de ce corpus pour former le référentiel d’étude.

Les pratiques actuelles sont focalisées sur les besoins de la recherche, a savoir un outil précis
pour le benchmark de solutions. Par exemple, un paradigme répandu pour l’évaluation compa-
rative d’applications de dialogue homme-machine est PARADISE de (Walker et al., 1997). I1

Marianne Laurent, Ghislain Putois, Philippe Bretier et Thierry Moudenc

consiste a réaliser des mesures quantitatives sur les traces d’interaction d’une application aﬁn
d’approximer la satisfaction utilisateur par une combinatoire de métriques quantitatives. Ces
méthodes cherchent a batir leur référentiel d’évaluation sur l’utilisateur humain, ce qui rend
la tache complexe et difﬁcilement objectivable. La constitution de ce référentiel devient donc
artisanale, spéciﬁque a une application donnée, et en conséquence, la tache doit étre réitérée
a chaque nouvelle campagne d’évaluation. Pour autant, étudier une application nécessite de
prendre en compte ses utilisateurs, puisque c’est en interagissant avec eux que l’application
prend tout son sens et devient obj et propice a l’évaluation.

Nous avons bien conscience qu’une évaluation se fait toujours par rapport a un cadre, mais nous
souhaitons ici rappeler que dans le domaine des systemes de dialogue, chaque interaction entre
l’utilisateur et l’application présente le caractere unique d’une performance. Chaque nouveau
dialogue est co-construit différemment selon les capacités propres a la fois a l’application et
a l’utilisateur, ce dernier sachant s’adapter tres vite, et changer ses pratiques. Pour pouvoir
réaliser une évaluation comparative entre deux applications ou entre deux évolutions d’une
méme application, nous avons impérativement besoin que ce cadre d’évaluation soit stable et
applicable aux deux. Or les pratiques utilisateurs ne sont pas assez stables pour constituer ce
cadre, justement a cause des grandes facultés d’adaptation humaines.

Dans l’industrie, les exigences de commensurabilité au regard de l’évaluation sont moins grandes.
L’ industrie cherche avant tout a mesurer l’adéquation d’une application face aux attentes opé-
rationnelles (contraintes de coﬁt et nécessité de qualité). En se placant dans cette logique indus-
trielle, il nous semble donc pertinent d’inverser notre approche sur la relation entre application
et utilisateur pour 1’ evaluation. Au lieu de tenter de mesurer a quel point 1’ application est proche
de l’utilisateur et de ses pratiques, envisageons plutot de déterminer dans quelle mesure l’utilisa-
teur est proche de l’application et des pratiques qu’elle propose. Le cadre d’évaluation a prendre
en compte est alors beaucoup plus stable, car l’application s’adapte beaucoup moins vite que
l’utilisateur, et seulement dans la mesure ou on la fait évoluer ou on la dote de capacités d’ap-
prentissage (toujours assez limitées). Elle est donc entierement maitrisable, car l’application est
concue pour répondre a un ensemble de besoins fonctionnels.

Parmi ces besoins, il y a la nécessité a traiter les comportements utilisateurs (des mots aux
enchainements d’actes de langage). Le niveau technologique actuel ne permettant pas de traiter
tous les usages de la langue, le design nécessite de circonscrire les comportements a considérer,
c’est-a-dire de prédire les comportements utilisateurs. De plus, la connaissance issue des études
centrées utilisateur sert a la prédiction des comportements utilisateurs, et in ﬁne a la déﬁnition
des besoins fonctionnels. Ces besoins, comme dans tout processus industriel, doivent également
pouvoir se traduire par un ensemble de métriques pour vériﬁer qu’ils sont adressés. De plus, ils
sont les invariants principaux qui déﬁnissent la raison d’ étre d’une application, et donc ils restent
majoritairement valables tout au long du cycle de vie de l’application, ce qui permet d’utiliser
le meme cadre d’évaluation pour comparer deux évolutions d’une méme application.

4 Les mesures industrielles

Pour refonder notre évaluation, placons-nous maintenant dans le référentiel de l’application qui
a été déﬁni dans la section précédente. Dans ce nouveau référentiel, il est possible d’extraire des
caractéristiques récurrentes de l’application et des mesures objectivables associées, qui consti-
tueront la matiere premiere pour l’évaluation quantitative des applications. Ainsi, pour évaluer

Nouveau paradigme d’évaluation des systemes de dialogue homme-machine

la pertinence d’une application de dialogue, nous choisissons des indicateurs qui mesurent l’ac-
complissement de la tache et la maniere dont le dialogue est mené avec l’utilisateur. Des corpus
de mesure sont ainsi constitués a partir d’une sélection d’indicateurs. Une multitude d’indi-
cateurs peuvent ainsi étre sollicités, parmi lesquels : les performances de chaque composant
logiciel, le nombre d’appels au service, le nombre moyen de tours de dialogue pour accomplir
une tache, la durée de chaque tour de dialogue, le nombre et l’instant des raccrochés en cours
de dialogue et les taux d’incompréhension de la reconnaissance vocale et des composants de
compréhension du langage naturel.

Pour passer de la mesure a l’évaluation, il faut déﬁnir un cadre d’évaluation. On sélectionne
d’abord un ensemble d’indicateurs qui apparaissent a priori représentatifs du bon fonctionne-
ment de l’application, puis on associe des interprétations aux ensembles de valeurs possibles de
ces indicateurs. Le cadre ainsi construit est indépendant du systeme initial, il déﬁnit des seuils
sur les indicateurs. Ce cadre d’évaluation peut alors étre appliqué a un autre systeme de dialogue
ou a une version alternative de l’application.

Rappelons que des processus d’évaluation interviennent a plusieurs endroits distincts du cycle
de vie d’une application. Ceci implique autant de redéﬁnitions d’un cadre d’évaluation propre
aux métiers concernés (design, marketing, exploitation, etc.), en fonction de leurs objectifs res-
pectifs et des moyens de mesure a leur disposition. Ainsi, lors de la phase de réalisation, les
designers vériﬁent le comportement nominal de l’application par rapport aux pratiques qu’ils
ont préalablement déﬁnies dans le cahier des charges. L’ application n’étant bien souvent testée
ni sur l’architecture cible, ni par des utilisateurs réels, seuls quelques indicateurs clés sont uti-
lisés. Ensuite, on mobilise davantage d’indicateurs pour les phases d’expérimentation et pilote
aﬁn d’évaluer l’écart entre les usages réels et les prédictions d’usages tels qu’implémentés dans
l’application. Enﬁn, en phase d’exploitation, seuls quelques indicateurs clés sont utilisés pour
la supervision. L’ application étant censée bien fonctionner, on vériﬁe alors principalement que
les utilisateurs continuent d’interagir avec l’application conformément aux possibilités d’inter-
action.

Nous voyons donc comment les processus de conception de l’application et de conception de
son évaluation peuvent étre étroitement entremélés dans le cadre d’un développement industriel.
Ceci est d’autant plus vrai que les spéciﬁcations de l’application se focalisent sur des aspects
locaux de l’application. A titre d’exemple, notre application automatique de renseignement par
téléphone propose une Inise en communication avec l’interlocuteur recherché moyennant une
surtaxe. Une évaluation locale sur les taux de réponses positives a la demiere question de l’ap-
plication a fait opter les designers pour le prompt systeme «Pour étre mis en relation, dites oui.»
qui avait un meilleur score que son prédécesseur «Souhaitez-vous étre mis en relation ?>>.

5 Conclusion

Consciente des économies que les systemes de dialogue vocal peuvent générer, et de l’impact
de la satisfaction client en termes d’image, l’industrie des services cherche donc a rationaliser
l’évaluation de ses applications pour pouvoir maitriser leur exploitation et leurs usages.

Les utilisateurs ont de fortes capacités d’adaptation aux contraintes imposées par les applica-
tions. Nous suggérons donc de ne pas chercher a évaluer les applications de dialogue unique-
ment avec des méthodes centrées utilisateur, mais au contraire d’exploiter la construction des
systemes autour d’un ensemble d’usages prédéﬁnis, et de chercher alors a évaluer si les utili-

Marianne Laurent, Ghislain Putois, Philippe Bretier et Thierry Moudenc

sateurs s’accommodent des usages proposés. La conception des évaluations est alors plus aisée
pour chaque phase de décision du cycle de vie d’un systeme, puisqu’elle se réfere directement
a des objectifs déﬁnis dans le cahier des charges, eux-mémes quantiﬁables par des indicateurs
de performance du systeme évalué.

La méthode proposée répond donc a un besoin industriel. Elle repose sur des optimisations
locales dans le but de réduire l’écart entre les interactions observées entre l’application et ses
utilisateurs et celles attendues. Ainsi, si elle integre la notion de progres dans la comparaison
de plusieurs versions d’une meme application, elle n’adresse cependant pas la question de re-
cherche d’un optimum global de qualité, notion que nous ne savons pas déﬁnir aujourd’hui.

Remerciements

Nous souhaitons remercier les équipes Dialogue et Synthese Vocale, les équipes multidiscipli-
naires et opérationnelles d’Orange pour leur contribution a ces réﬂexions.

Références

CAELEN J ., ZEILIGER J ., BESSAC M., SIROUX J . & PERENNOU G. (1997). Les corpus
pour l’évaluation du dialogue homme-machine. In Actes des Ieres JST FRANCIL 1997,
Journées Scientiﬁques et Techniques du Réseau Francophone de l ’Ingénierie de la Langue
de l’AUPELF-URE, Avignon, I5/04/97-I6/04/97, p. 215-222 : -.

GRISLIN M. & KOLSKI C. (1996). Evaluation des interfaces homme-machine lors du dé-
veloppement des systemes interactifs = human-machine interface evaluation during the de-
velopment of interactives systems. TSI. Technique et science informatiques ISSN 0752-4072
CODEN TTSIDJ, 15(3), 265-296.

J OKINEN K., MCTEAR M. & LARSON J . (2007). Dialogue on dialogues - multidisciplinary
evaluation of advanced speech-based interactive systems : A report on the interspeech 2006
satellite event. AI Magazine, 28(2).

MCTEAR M., J OKINEN K. & LARSON J . (2008). Special issue on evaluating new methods
and models for advanced speech-based interactive systems. Speech Communication, 50(8-9).

NORROS L. & SAVIOJA P. (2007). Vers une théorie et une méthode d’évaluation de l’utilisa-
bilité des systemes complexes homme-technologie. @ctivite’s, 4(2).

PAEK T. (2001). Empirical methods for evaluating dialog systems. In Proceedings of the
workshop on Evaluation for Language and Dialogue Systems, p. 1-8, Morristown, NJ, USA :
Association for Computational Linguistics.

PAEK T. (2007). Toward evaluation that leads to best practices : Reconciling dialog evaluation
in research and industry. In Proceedings of the Workshop on Bridging the Gap .' Academic
and Industrial Research in Dialog Technologies, p. 40-47, Rochester, NY : Association for
Computational Linguistics.

WALKER M. A., LITMAN D. J ., KAMM C. A. & ABELLA A. (1997). Paradise : a framework
for evaluating spoken dialogue agents. In Proceedings of the eighth conference on European
chapter of the Association for Computational Linguistics, p. 271-280, Morristown, NJ, USA :
Association for Computational Linguistics.

