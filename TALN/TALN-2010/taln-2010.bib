@proceedings{TALN:2010,
  editor    = {Langlais, Philippe and Gagnon, Michel},
  title     = {Actes de la 17e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {July},
  year      = {2010},
  address   = {Montréal, Canada},
  publisher = {Association pour le Traitement Automatique des Langues},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2010}
}

@inproceedings{mel'cuk:2010:TALN,
  author    = {Mel'čuk, Igor},
  title     = {La phraséologie en langue, en dictionnaire et en TALN},
  booktitle = {Actes de la 17e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {July},
  year      = {2010},
  address   = {Montréal, Canada},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2010/taln-2010-invite-001},
  language  = {french},
  resume    = {},
  abstract  = {},
  motscles  = {},
  keywords  = {},
}

@inproceedings{isabelle:2010:TALN,
  author    = {Isabelle, Pierre},
  title     = {La montée en puissance des recherches en traduction automatique statistique},
  booktitle = {Actes de la 17e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {July},
  year      = {2010},
  address   = {Montréal, Canada},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2010/taln-2010-invite-002},
  language  = {french},
  resume    = {},
  abstract  = {},
  motscles  = {},
  keywords  = {},
}

@inproceedings{penn:2010:TALN,
  author    = {Penn, Gerald},
  title     = {},
  booktitle = {Actes de la 17e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {July},
  year      = {2010},
  address   = {Montréal, Canada},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2010/taln-2010-invite-003},
  language  = {french},
  note      = {The Quantitative Study of Writing Systems},
  resume    = {},
  abstract  = {},
  motscles  = {},
  keywords  = {},
}

@inproceedings{schwenk:2010:TALN,
  author    = {Schwenk, Holger},
  title     = {Adaptation d’un Système de Traduction Automatique Statistique avec des Ressources monolingues},
  booktitle = {Actes de la 17e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {July},
  year      = {2010},
  address   = {Montréal, Canada},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2010/taln-2010-long-001},
  language  = {french},
  resume    = {Les performances d’un système de traduction statistique dépendent beaucoup de la qualité et de la quantité des données d’apprentissage disponibles. La plupart des textes parallèles librement disponibles proviennent d’organisations internationales. Le jargon observé dans ces textes n’est pas très adapté pour construire un système de traduction pour d’autres domaines. Nous présentons dans cet article une technique pour adapter le modèle de traduction à un domaine différent en utilisant des textes dans la langue source uniquement. Nous obtenons des améliorations significatives du score BLEU dans des systèmes de traduction de l’arabe vers le français et vers l’anglais.},
  abstract  = {The performance of a statistical machine translation system depends a lot on the quality and quantity of the available training data. Most of the existing, easily available parallel texts come from international organizations and the jargon observed in those texts is not very appropriate to build a machine translation system for other domains. In this paper, we present a technique to automatically adapt the translation model to a new domain using monolingual data in the source language only. We observe significant improvements in the BLEU score in statistical machine translation systems from Arabic to French and English respectively.},
  motscles  = {Traduction statistique, adaptation du modèle de traduction, corpus monolingue, apprentissage non-supervisé},
  keywords  = {Statistical machine translation, translation model adaptation, monolingual data, unsupervised training},
}

@inproceedings{bourdaillet-huet-langlais:2010:TALN,
  author    = {Bourdaillet, Julien and Huet, Stéphane and Langlais, Philippe},
  title     = {Alignement de traductions rares à l’aide de paires de phrases non alignées},
  booktitle = {Actes de la 17e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {July},
  year      = {2010},
  address   = {Montréal, Canada},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2010/taln-2010-long-002},
  language  = {french},
  resume    = {Bien souvent, le sens d’un mot ou d’une expression peut être rendu dans une autre langue par plusieurs traductions. Parmi celles-ci, certaines se révèlent très fréquentes alors que d’autres le sont beaucoup moins, conformément à une loi zipfienne. La googlisation de notre monde n’échappe pas aux mémoires de traduction, qui mettent souvent à mal ou simplement ignorent ces traductions rares qui sont souvent de bonne qualité. Dans cet article, nous nous intéressons à ces traductions rares sous l’angle du repérage de traductions. Nous argumentons qu’elles sont plus difficiles à identifier que les traductions plus fréquentes. Nous décrivons une approche originale qui permet de mieux les identifier en tirant profit de l’alignement au niveau des mots de paires de phrases qui ne sont pas alignées. Nous montrons que cette approche permet d’améliorer l’identification de ces traductions rares.},
  abstract  = {There generally exist numerous ways to translate a word or a phrase in another language. Among these translations, some are very common while others are far less so, according to a zipfian law. As with the rest of the world, translation memories are googlized, leading to poorly handled or even simply ignored rare translations, while they are often of good quality. In this paper, we tackle this problem in a transpotting framework. We show that these rare translations are harder to identify than common translations. We describe an original approach based on the word alignment of sentences which are not aligned. We show that this approach significantly improves the identification of those rare translations.},
  motscles  = {Traduction automatique statistique, alignement de mots, traduction rares, contrôle de pertinence},
  keywords  = {Statistical machine translation, word alignment, rare translations, relevance feedback},
}

@inproceedings{denis-sagot:2010:TALN,
  author    = {Denis, Pascal and Sagot, Benoît},
  title     = {Exploitation d’une ressource lexicale pour la construction d’un étiqueteur morpho-syntaxique état-de-l’art du français},
  booktitle = {Actes de la 17e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {July},
  year      = {2010},
  address   = {Montréal, Canada},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2010/taln-2010-long-003},
  language  = {french},
  resume    = {Cet article présente MEltfr, un étiqueteur morpho-syntaxique automatique du français. Il repose sur un modèle probabiliste séquentiel qui bénéficie d’informations issues d’un lexique exogène, à savoir le Lefff. Evalué sur le FTB, MEltfr atteint un taux de précision de 97.75% (91.36% sur les mots inconnus) sur un jeu de 29 étiquettes. Ceci correspond à une diminution du taux d’erreur de 18% (36.1% sur les mots inconnus) par rapport au même modèle sans couplage avec le Lefff. Nous étudions plus en détail la contribution de cette ressource, au travers de deux séries d’expériences. Celles-ci font apparaître en particulier que la contribution des traits issus du Lefff est de permettre une meilleure couverture, ainsi qu’une modélisation plus fine du contexte droit des mots.},
  abstract  = {This paper presents MEltfr, an automatic POS tagger for French. This system relies on a sequential probabilistic model that exploits information extracted from an external lexicon, namely Lefff. When evaluated on the FTB corpus, MEltfr achieves an accuracy of 97.75% (91.36% on unknow words) using a tagset of 29 categories. This corresponds to an error rate decrease of 18% (36.1% on unknow words) compared to the same model without Lefff information. We investigate in more detail the contribution of this resource through two sets of experiments. These reveal in particular that the Lefff features allow for an increased coverage and a finer-grained modeling of the context at the right of a word.},
  motscles  = {Etiquetage morpho-syntaxique, modèles à maximisation d’entropie, français, lexique},
  keywords  = {POS tagging, maximum entropy models, French, lexicon},
}

@inproceedings{ferret:2010:TALN,
  author    = {Ferret, Olivier},
  title     = {Similarité sémantique et extraction de synonymes à partir de corpus},
  booktitle = {Actes de la 17e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {July},
  year      = {2010},
  address   = {Montréal, Canada},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2010/taln-2010-long-004},
  language  = {french},
  resume    = {La définition de mesures sémantiques au niveau lexical a fait l’objet de nombreux travaux depuis plusieurs années. Dans cet article, nous nous focalisons plus spécifiquement sur les mesures de nature distributionnelle. Bien que différentes évaluations ont été réalisées les concernant, il reste difficile à établir si une mesure donnant de bons résultats dans un cadre d’évaluation peut être appliquée plus largement avec le même succès. Dans le travail présenté, nous commençons par sélectionner une mesure de similarité sur la base d’un test de type TOEFL étendu. Nous l’appliquons ensuite au problème de l’extraction de synonymes à partir de corpus en comparant nos résultats avec ceux de (Curran \& Moens, 2002). Enfin, nous testons l’intérêt pour cette tâche d’extraction de synonymes d’une méthode d’amélioration de la qualité des données distributionnelles proposée dans (Zhitomirsky-Geffet \& Dagan, 2009).},
  abstract  = {The definition of lexical semantic measures has been the subject of lots of works for many years. In this article, we focus more specifically on distributional semantic measures. Although several evaluations about this kind of measures were already achieved, it is still difficult to determine if a measure that performs well in an evaluation framework can be applied more widely with the same success. In the work we present here, we first select a similarity measure by testing it against an extended TOEFL test. Then, we apply this measure for extracting automatically synonyms from a corpus and we compare our results to those of (Curran \& Moens, 2002). Finally, we test the interest for synonym extraction of a method proposed in (Zhitomirsky-Geffet \& Dagan, 2009) for improving the quality of distributional data.},
  motscles  = {extraction de synonymes, similarité sémantique, méthodes distributionnelles},
  keywords  = {synonym extraction, semantic similarity, distributional methods},
}

@inproceedings{charest-brunelle-fontaine:2010:TALN,
  author    = {Charest, Simon and Brunelle, Éric and Fontaine, Jean},
  title     = {Au-delà de la paire de mots : extraction de cooccurrences syntaxiques multilexémiques},
  booktitle = {Actes de la 17e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {July},
  year      = {2010},
  address   = {Montréal, Canada},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2010/taln-2010-long-005},
  language  = {french},
  resume    = {Cet article décrit l’élaboration de la deuxième édition du dictionnaire de cooccurrences du logiciel d’aide à la rédaction Antidote. Cette nouvelle mouture est le résultat d’une refonte complète du processus d’extraction, ayant principalement pour but l’extraction de cooccurrences de plus de deux unités lexicales. La principale contribution de cet article est la description d’une technique originale pour l’extraction de cooccurrences de plus de deux mots conservant une structure syntaxique complète.},
  abstract  = {This article describes the elaboration of the second edition of the co-occurrence dictionary included in Antidote HD, a commercial software tool for writing in French. This second edition is the result of a complete overhaul of the extraction process, with the objective of extracting co-occurrences of more than two lexical units. The main contribution of this article is the description of an original method for extracting co-occurrences of more than two words retaining their full syntactic structure.},
  motscles  = {Antidote, cooccurrences, collocations, expressions multimots},
  keywords  = {Antidote, co-occurrences, collocations, multi-word expressions (MWE)},
}

@inproceedings{elghali-vigilehoareau:2010:TALN,
  author    = {El Ghali, Adil and Vigile Hoareau, Yann},
  title     = {Une approche cognitive de la fouille de grandes collections de documents},
  booktitle = {Actes de la 17e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {July},
  year      = {2010},
  address   = {Montréal, Canada},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2010/taln-2010-long-006},
  language  = {french},
  resume    = {La récente éclosion du Web2.0 engendre un accroissement considérable de volumes textuels et intensifie ainsi l’importance d’une réflexion sur l’exploitation des connaissances à partir de grandes collections de documents. Dans cet article, nous présentons une approche de rechercher d’information qui s’inspire des certaines recherches issues de la psychologie cognitive pour la fouille de larges collections de documents. Nous utilisons un document comme requête permettant de récupérer des informations à partir d’une collection représentée dans un espace sémantique. Nous définissons les notions d’identité sémantique et de pollution sémantique dans un espace de documents. Nous illustrons notre approche par la description d’un système appelé BRAT (Blogosphere Random Analysis using Texts) basé sur les notions préalablement introduites d’identité et de pollution sématique appliquées à une tâche d’identification des actualités dans la blogosphère mondiale lors du concours TREC’09. Les premiers résultats produits sont tout à fait encourageant et indiquent les pistes des recherches à mettre en oeuvre afin d’améliorer les performances de BRAT.},
  abstract  = {MiningWeb 2.0 content become nowadays an important task in Information Retrieval and Search communities. The work related in this paper present an original approach of blogs mining, inspired from researches in cognitive psychology. We define the notions of semantic identity of blogs, and the semantic pollution in a semantic space. Then, we describe a system called BRAT (Blogosphere Random Analysis using Texts) based on these notions that has been applied to the Top Stories identification task of the Blog Track at the TREC’09 contest. The performance of BRAT at TREC’09 in its preliminary stage of development are very encouraging and the results of the experiences described here-after draw the lines of the future researches that should be realized in order to upgrade its performances.},
  motscles  = {Fouille de textes, Random-Indexing, Cognition, Marche aléatoire},
  keywords  = {Text-Mining, Random-Indexing, Cognition, Random walk},
}

@inproceedings{marchand-guillaume-perrier:2010:TALN,
  author    = {Marchand, Jonathan and Guillaume, Bruno and Perrier, Guy},
  title     = {Motifs de graphe pour le calcul de dépendances syntaxiques complètes},
  booktitle = {Actes de la 17e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {July},
  year      = {2010},
  address   = {Montréal, Canada},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2010/taln-2010-long-007},
  language  = {french},
  resume    = {Cet article propose une méthode pour calculer les dépendances syntaxiques d’un énoncé à partir du processus d’analyse en constituants. L’objectif est d’obtenir des dépendances complètes c’est-à-dire contenant toutes les informations nécessaires à la construction de la sémantique. Pour l’analyse en constituants, on utilise le formalisme des grammaires d’interaction : celui-ci place au cœur de la composition syntaxique un mécanisme de saturation de polarités qui peut s’interpréter comme la réalisation d’une relation de dépendance. Formellement, on utilise la notion de motifs de graphes au sens de la réécriture de graphes pour décrire les conditions nécessaires à la création d’une dépendance.},
  abstract  = {This article describes a method to build syntactical dependencies starting from the phrase structure parsing process. The goal is to obtain all the information needed for a detailled semantical analysis. Interaction Grammars are used for parsing; the saturation of polarities which is the core of this formalism can be mapped to dependency relation. Formally, graph patterns are used to express the set of constraints which control dependency creations.},
  motscles  = {Analyse syntaxique, dépendance, grammaires d’interaction, polarité},
  keywords  = {Syntactic analysis, dependency, interaction grammars, polarity},
}

@inproceedings{thuilier-fox-crabbe:2010:TALN,
  author    = {Thuilier, Juliette and Fox, Gwendoline and Crabbé, Benoît},
  title     = {Approche quantitative en syntaxe : l’exemple de l’alternance de position de l’adjectif épithète en français},
  booktitle = {Actes de la 17e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {July},
  year      = {2010},
  address   = {Montréal, Canada},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2010/taln-2010-long-008},
  language  = {french},
  resume    = {Cet article présente une analyse statistique sur des données de syntaxe qui a pour but d’aider à mieux cerner le phénomène d’alternance de position de l’adjectif épithète par rapport au nom en français. Nous montrons comment nous avons utilisé les corpus dont nous disposons (French Treebank et le corpus de l’Est-Républicain) ainsi que les ressources issues du traitement automatique des langues, pour mener à bien notre étude. La modélisation à partir de 13 variables relevant principalement des propriétés du syntagme adjectival, de celles de l’item adjectival, ainsi que de contraintes basées sur la fréquence, permet de prédire à plus de 93% la position de l’adjectif. Nous insistons sur l’importance de contraintes relevant de l’usage pour le choix de la position de l’adjectif, notamment à travers la fréquence d’occurrence de l’adjectif, et la fréquence de contextes dans lesquels il apparaît.},
  abstract  = {This article presents a statistical analysis of syntactic data that aims to better understand the phenomenon of position alternation displayed by attributive adjectives with respect to nouns in French. We show how we used the corpora available for French (the French Treebank and the Est-Républicain corpus) as well as ressources provided by Natural Language Processing for our study. The proposed model contains 13 variables based on properties of the adjectival phrase, the adjectival item and on frequency constraints. This model is capable to predict the position of adjectives at more than a 93% rate. We especially focus on the importance of constraints based on usage for the choice of position for the adjective, in particular the frequency of contexts in which it appears.},
  motscles  = {Syntaxe probabiliste, linguistique de corpus, adjectif épithète, régression logistique},
  keywords  = {Probabilistic syntax, corpus linguistics, attributive adjective, logistic regression},
}

@inproceedings{blache:2010:TALN,
  author    = {Blache, Philippe},
  title     = {Un modèle de caractérisation de la complexité syntaxique},
  booktitle = {Actes de la 17e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {July},
  year      = {2010},
  address   = {Montréal, Canada},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2010/taln-2010-long-009},
  language  = {french},
  resume    = {Cet article présente un modèle de la complexité syntaxique. Il réunit un ensemble d’indices de complexité et les représente à l’aide d’un cadre formel homogène, offrant ainsi la possibilité d’une quantification automatique : le modèle proposé permet d’associer à chaque phrase un indice reflétant sa complexité.},
  abstract  = {This paper proposes a model of syntactic complexity. It brings together a set of complexity parameters and represnt them thanks to a unique formal framework. This approach makes it possible an automatic evaluation : a complexity index can be associated to each sentence.},
  motscles  = {Complexité syntaxique, analyse syntaxique automatique, parser humain},
  keywords  = {Syntactic complexity, parsing, human parser},
}

@inproceedings{villemontedelaclergerie:2010:TALN,
  author    = {Villemonte De La Clergerie, Éric},
  title     = {Convertir des dérivations TAG en dépendances},
  booktitle = {Actes de la 17e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {July},
  year      = {2010},
  address   = {Montréal, Canada},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2010/taln-2010-long-010},
  language  = {french},
  resume    = {Les structures de dépendances syntaxiques sont importantes et bien adaptées comme point de départ de diverses applications. Dans le cadre de l’analyseur TAG FRMG, nous présentons les détails d’un processus de conversion de forêts partagées de dérivations en forêts partagées de dépendances. Des éléments d’information sont fournis sur un algorithme de désambiguisation sur ces forêts de dépendances.},
  abstract  = {Syntactic dependency structures are important and adequate as starting point for various NLP applications. In the context of the French TAG FRMG parser, we present the details of a conversion process from shared derivation forests into shared dependency forests. Some information are also provided about a disambiguisation algorithm for these dependency forests.},
  motscles  = {dépendances, analyse syntaxique, TAG, forêt partagée},
  keywords  = {dependencies, parsing, TAG, shared forest},
}

@inproceedings{mithun-kosseim:2010:TALN,
  author    = {Mithun, Shamima and Kosseim, Leila},
  title     = {},
  booktitle = {Actes de la 17e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {July},
  year      = {2010},
  address   = {Montréal, Canada},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2010/taln-2010-long-011},
  language  = {french},
  note      = {A Hybrid Approach to Utilize Rhetorical Relations for Blog Summarization},
  resume    = {},
  abstract  = {The availability of huge amounts of online opinions has created a new need to develop effective query-based opinion summarizers to analyze this information in order to facilitate decision making at every level. To develop an effective opinion summarization approach, we have targeted to resolve specifically Question Irrelevancy and Discourse Incoherency problems which have been found to be the most frequently occurring problems for opinion summarization. To address these problems, we have introduced a hybrid approach by combining text schema and rhetorical relations to exploit intra-sentential rhetorical relations. To evaluate our approach, we have built a system called BlogSum and have compared BlogSum-generated summaries after applying rhetorical structuring to BlogSum-generated candidate sentences without utilizing rhetorical relations using the Text Analysis Conference (TAC) 2008 data for summary contents. Evaluation results show that our approach improves summary contents by reducing question irrelevant sentences.},
  motscles  = {},
  keywords  = {Blog Summarization, Rhetorical Relations, Text Schema},
}

@inproceedings{beaufort-EtAl:2010:TALN,
  author    = {Beaufort, Richard and Roekhaut, Sophie and Cougnon, Louise-Amélie and Fairon, Cédrick},
  title     = {Une approche hybride traduction/correction pour la normalisation des SMS},
  booktitle = {Actes de la 17e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {July},
  year      = {2010},
  address   = {Montréal, Canada},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2010/taln-2010-long-012},
  language  = {french},
  resume    = {Cet article présente une méthode hybride de normalisation des SMS, à mi-chemin entre correction orthographique et traduction automatique. La partie du système qui assure la normalisation utilise exclusivement des modèles entraînés sur corpus. Evalué en français par validation croisée, le système obtient un taux d’erreur au mot de 9.3% et un score BLEU de 0.83.},
  abstract  = {This paper presents a method of normalizing SMS messages that shares similarities with both spell checking and machine translation approaches. The normalization part of the system is entirely based on models trained from a corpus. Evaluated in French by ten-fold cross-validation, the system achieves a 9.3% Word Error Rate and a 0.83 BLEU score.},
  motscles  = {SMS, normalisation, machines à états finis, approche hybride, orienté traduction, orienté correction, apprentissage sur corpus},
  keywords  = {SMS messages, normalization, finite-state machines, hybrid approach, machine translationlike, spell checking-like, corpus-based learning},
}

@inproceedings{wisniewski-max-yvon:2010:TALN,
  author    = {Wisniewski, Guillaume and Max, Aurélien and Yvon, François},
  title     = {Recueil et analyse d’un corpus écologique de corrections orthographiques extrait des révisions de Wikipédia},
  booktitle = {Actes de la 17e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {July},
  year      = {2010},
  address   = {Montréal, Canada},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2010/taln-2010-long-013},
  language  = {french},
  resume    = {Dans cet article, nous introduisons une méthode à base de règles permettant d’extraire automatiquement de l’historique des éditions de l’encyclopédie collaborative Wikipédia des corrections orthographiques. Cette méthode nous a permis de construire un corpus d’erreurs composé de 72 483 erreurs lexicales (non-word errors) et 74 100 erreurs grammaticales (real-word errors). Il n’existe pas, à notre connaissance, de plus gros corpus d’erreurs écologiques librement disponible. En outre, les techniques mises en oeuvre peuvent être facilement transposées à de nombreuses autres langues. La collecte de ce corpus ouvre de nouvelles perspectives pour l’étude des erreurs fréquentes ainsi que l’apprentissage et l’évaluation des correcteurs orthographiques automatiques. Plusieurs expériences illustrant son intérêt sont proposées.},
  abstract  = {This paper describes a French spelling error corpus we built by miningWikipedia revision history. This corpus contains 72,493 non-word errors and 74,100 real-word errors. To the best of our knowledge, this is the first time that such a large corpus of naturally occurring errors is collected and made publicly available, which opens new possibilities for the evaluation of spell checkers and the study of error patterns. In the second part of this work, a first study of french spelling error patterns and of the performance of a spell checker is presented.},
  motscles  = {ressources pour le TAL, correction orthographique, Wikipédia},
  keywords  = {resources for NLP, spelling correction, Wikipedia},
}

@inproceedings{charton-gagnon-ozell:2010:TALN,
  author    = {Charton, Eric and Gagnon, Michel and Ozell, Benoit},
  title     = {Extension d’un système d’étiquetage d’entités nommées en étiqueteur sémantique},
  booktitle = {Actes de la 17e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {July},
  year      = {2010},
  address   = {Montréal, Canada},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2010/taln-2010-long-014},
  language  = {french},
  resume    = {L’étiquetage sémantique consiste à associer un ensemble de propriétés à une séquence de mots contenue dans un texte. Bien que proche de la tâche d’étiquetage par entités nommées, qui revient à attribuer une classe de sens à un mot, la tâche d’étiquetage ou d’annotation sémantique cherche à établir la relation entre l’entité dans son texte et sa représentation ontologique. Nous présentons un étiqueteur sémantique qui s’appuie sur un étiqueteur d’entités nommées pour mettre en relation un mot ou un groupe de mots avec sa représentation ontologique. Son originalité est d’utiliser une ontologie intermédiaire de nature statistique pour établir ce lien.},
  abstract  = {Semantic labelling consist to associate a set of properties to a sequence of words from a text. Although its proximity with the named entity labelling task, which is equivalent to associate a class meaning to a sequence of word, the task of semantic labelling try to establish the relation between the entity in the text and it’s ontological representation. We present a semantic labelling system based on a named entity recognition step. The originality of our system is that the link between named entity and its semantic representation is obtained trough the use of an intermediate statistical ontology.},
  motscles  = {Étiqueteur sémantique, Entités nommées, Analyse sémantique, Ontologie},
  keywords  = {Semantic parser, Named entities, Semantic annotation},
}

@inproceedings{milicevic:2010:TALN,
  author    = {Milićević, Jasmina},
  title     = {Extraction de paraphrases sémantiques et lexico-syntaxiques de corpus parallèles bilingues},
  booktitle = {Actes de la 17e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {July},
  year      = {2010},
  address   = {Montréal, Canada},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2010/taln-2010-long-015},
  language  = {french},
  resume    = {Nous présentons le travail en cours effectué dans le cadre d’un projet d’extraction de paraphrases à partir de textes parallèles bilingues. Nous identifions des paraphrases sémantiques et lexico-syntaxiques, qui mettent en jeu des opérations relativement complexes sur les structures sémantiques et syntaxiques de phrases, et les décrivons au moyen de règles de paraphrasage de type Sens-Texte, utilisables dans diverses applications de TALN.},
  abstract  = {We present work in progress done within a project of extracting paraphrases from parallel bilingual texts. We identify semantic and lexical-syntactic paraphrases, which imply relatively complex operations on semantic and syntactic structures of sentences, and describe them by means of paraphrasing rules of Meaning-Text type, usable in various NLP applications.},
  motscles  = {paraphrase lexico-syntaxique, paraphrase sémanique, règles de paraphrasage, corpus bilingues, théorie Sens-Texte},
  keywords  = {lexical-syntactic paraphrase, semantic paraphrase, paraphrasing rules, bilingual corpora, Meaning-Text theory},
}

@inproceedings{hodac-perywoodley-tanguy:2010:TALN,
  author    = {Ho-Dac, Lydia-Mai and Péry-Woodley, Marie-Paule and Tanguy, Ludovic},
  title     = {Anatomie des structures énumératives},
  booktitle = {Actes de la 17e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {July},
  year      = {2010},
  address   = {Montréal, Canada},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2010/taln-2010-long-016},
  language  = {french},
  resume    = {Cet article présente les premiers résultats d’une campagne d’annotation de corpus à grande échelle réalisée dans le cadre du projet ANNODIS. Ces résultats concernent la partie descendante du dispositif d’annotation, et plus spécifiquement les structures énumératives. Nous nous intéressons à la structuration énumérative en tant que stratégie de base de mise en texte, apparaissant à différents niveaux de granularité, associée à différentes fonctions discursives, et signalée par des indices divers. Avant l’annotation manuelle, une étape de pré-traitement a permis d’obtenir le marquage systématique de traits associés à la signalisation de l’organisation du discours. Nous décrivons cette étape de marquage automatique, ainsi que la procédure d’annotation. Nous proposons ensuite une première typologie des structures énumératives basée sur la description quantitative des données annotées manuellement, prenant en compte la couverture textuelle, la composition et les types d’indices.},
  abstract  = {This paper presents initial results from a large scale discourse annotation project, the ANNODIS project. These results concern the top-down part of the annotation scheme, and more specifically enumerative structures. We are interested in enumerative structures as a basic text construction strategy, occurring at different levels of granularity, associated with various discourse functions, and signalled by a broad range of cues. Before manual annotation via a purpose-built interface, a pre-processing phase produced a systematic mark-up of features associated to the signalling of discourse organisation. We describe this markup phase and the annotation procedure. We then propose a first typology of enumerative structures based on a quantitative description of the manually annotated data, taking into account textual coverage, composition, types of cues.},
  motscles  = {Annotation de corpus, organisation du discours, structure énumérative, signalisation},
  keywords  = {Corpus annotation, discourse organisation, enumerative structure, signalling text structures},
}

@inproceedings{hadouche-lapalme-l'homme:2010:TALN,
  author    = {Hadouche, Fadila and Lapalme, Guy and L’Homme, Marie-Claude},
  title     = {Identification des actants et circonstants par apprentissage machine},
  booktitle = {Actes de la 17e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {July},
  year      = {2010},
  address   = {Montréal, Canada},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2010/taln-2010-long-017},
  language  = {french},
  resume    = {Dans cet article, nous traitons de l’identification automatique des participants actants et circonstants de lexies prédicatives verbales tirées d’un corpus spécialisé en langue française. Les actants contribuent à la réalisation du sens de la lexie alors que les circonstants sont optionnels : ils ajoutent une information supplémentaire qui ne fait pas partie intégrante du sémantisme de la lexie. Nous proposons une classification de ces participants par apprentissage machine basée sur un corpus de lexies verbales du domaine de l’informatique, lexies qui ont été annotées manuellement avec des rôles sémantiques. Nous présentons des features qui nous permettent d’identifier les participants et de distinguer les actants des circonstants.},
  abstract  = {In this paper we discuss the identification of participants actants and circumstants of specialized verbal lexical units in a French specialised corpus. The actants are linguistic units that contribute to the sense of the verbal lexical unit while circumstants are optional: they add extra information that is not part of the meaning of the verbal unit. In this work, we propose a classification of participants using machine learning based on a specialized corpus of verbal lexical items in the field of computing which are annotated manually with semantic roles labels. We defined features to identify participants and distinguish actants from circumstants.},
  motscles  = {Structure actancielle, actants et circonstants, features de classification},
  keywords  = {Actantial structure, actants and circumstants, classification features},
}

@inproceedings{bessagnet-EtAl:2010:TALN,
  author    = {Bessagnet, Marie-Noëlle and Gaio, Mauro and Kergosien, Eric and Sallaberry, Christian},
  title     = {Extraction automatique d'un lexique à connotation géographique à des fins ontologiques dans un corpus de récits de voyage},
  booktitle = {Actes de la 17e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {July},
  year      = {2010},
  address   = {Montréal, Canada},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2010/taln-2010-long-018},
  language  = {french},
  resume    = {Le but de ces travaux est d’extraire un lexique en analysant les relations entre des syntagmes nominaux et des syntagmes verbaux dans les textes de notre corpus, essentiellement des récits de voyage. L’hypothèse que nous émettons est de pouvoir établir une catégorisation des syntagmes nominaux associés à des Entités Nommées de type lieu à l’aide de l’analyse des relations verbales. En effet, nous disposons d’une chaine de traitement automatique qui extrait, interprète et valide des Entités Nommées de type lieu dans des documents textuels. Ce travail est complété par l’analyse des relations verbales associées à ces EN, candidates à l’enrichissement d’une ontologie.},
  abstract  = {The aim of this research work is to extract a lexicon by analyzing the relationship between nominal syntagms and verb construction within our corpus, namely travel stories. We would like to establish a categorization of nominal syntagms linked to Named Entity (NE) (type space) thanks to verbal relationships analysis. In fact, we develop a computerized process flow in order to extract, to interpret and to validate NE of type space in textual documents. This research work is completed by the analyze of verbal relationships linked to these EN which could enrich our ontology.},
  motscles  = {Entité nommée, ontologie, relations verbales, patrons linguistiques},
  keywords  = {Named Entity, ontology, verbal relations, language patterns},
}

@inproceedings{oger-rouvier-linares:2010:TALN,
  author    = {Oger, Stanislas and Rouvier, Mickael and Linarès, Georges},
  title     = {Classification du genre vidéo reposant sur des transcriptions automatiques},
  booktitle = {Actes de la 17e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {July},
  year      = {2010},
  address   = {Montréal, Canada},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2010/taln-2010-long-019},
  language  = {french},
  resume    = {Dans cet article nous proposons une nouvelle méthode pour l’identification du genre vidéo qui repose sur une analyse de leur contenu linguistique. Cette approche consiste en l’analyse des mots apparaissant dans les transcriptions des pistes audio des vidéos, obtenues à l’aide d’un système de reconnaissance automatique de la parole. Les expériences sont réalisées sur un corpus composé de dessins animés, de films, de journaux télévisés, de publicités, de documentaires, d’émissions de sport et de clips de musique. L’approche proposée permet d’obtenir un taux de bonne classification de 74% sur cette tâche. En combinant cette approche avec des méthodes reposant sur des paramètres acoustiques bas-niveau, nous obtenons un taux de bonne classification de 95%.},
  abstract  = {In this paper, we present a new method for video genre identification based on the linguistic content analysis. This approach relies on the analysis of the words in the video transcriptions provided by an automatic speech recognition system. Experiments are conducted on a corpus composed of cartoons, movies, news, commercials, documentary, sport and music. On this 7-genre identification task, the proposed transcription-based method obtains up to 74% of correct identification. Finally, this rate is increased to 95% by combining the proposed linguistic-level features with low-level acoustic features.},
  motscles  = {classification de genre vidéo, traitement audio de la vidéo, extraction de paramètres linguistiques},
  keywords  = {video genre classification, audio-based video processing, linguistic feature extraction},
}

@inproceedings{raymond-fayolle:2010:TALN,
  author    = {Raymond, Christian and Fayolle, Julien},
  title     = {Reconnaissance robuste d’entités nommées sur de la parole transcrite automatiquement},
  booktitle = {Actes de la 17e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {July},
  year      = {2010},
  address   = {Montréal, Canada},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2010/taln-2010-long-020},
  language  = {french},
  resume    = {Les transcriptions automatiques de parole constituent une ressource importante, mais souvent bruitée, pour décrire des documents multimédia contenant de la parole (e.g. journaux télévisés). En vue d’améliorer la recherche documentaire, une étape d’extraction d’information à caractère sémantique, précédant l’indexation, permet de faire face au problème des transcriptions imparfaites. Parmis ces contenus informatifs, on compte les entités nommées (e.g. noms de personnes) dont l’extraction est l’objet de ce travail. Les méthodes traditionnelles de reconnaissance basées sur une définition manuelle de grammaires formelles donnent de bons résultats sur du texte ou des transcriptions propres manuellement produites, mais leurs performances se trouvent fortement affectées lorsqu’elles sont appliquées sur des transcriptions automatiques. Nous présentons, ici, trois méthodes pour la reconnaissance d’entités nommées basées sur des algorithmes d’apprentissage automatique : les champs conditionnels aléatoires, les machines à de support, et les transducteurs à états finis. Nous présentons également une méthode pour rendre consistantes les données d’entrainement lorsqu’elles sont annotées suivant des conventions légèrement différentes. Les résultats montrent que les systèmes d’étiquetage obtenus sont parmi les plus robustes sur les données d’évaluation de la campagne ESTER 2 dans les conditions où la transcription automatique est particulièrement bruitée.},
  abstract  = {Automatic speech transcripts are an important, but noisy, ressource to index spoken multimedia documents (e.g. broadcast news). In order to improve both indexation and information retrieval, extracting semantic information from these erroneous transcripts is an interesting challenge. Among these meaningful contents, there are named entities (e.g. names of persons) which are the subject of this work. Traditional named entity taggers are based on manual and formal grammars. They obtain correct performance on text or clean manual speech transcripts, but they have a lack of robustness when applied on automatic transcripts. We are introducing, in this work, three methods for named entity recognition based on machine learning algorithms, namely conditional random fields, support vector machines, and finitestate transducers. We are also introducing a method to make consistant the training data when they are annotated with slightly different conventions. We show that our tagger systems are among the most robust when applied to the evaluation data of the French ESTER 2 campaign in the most difficult conditions where transcripts are particularly noisy.},
  motscles  = {étiqueteur d’entités nommées, transcription automatique de parole, apprentissage automatique, champs conditionnels aléatoires, machines à vecteurs de support, transducteurs à états finis},
  keywords  = {named entity tagger, automatic speech recognition transcripts, machine learning, conditionnal random fields, support vector machines, finite-state transducers},
}

@inproceedings{bahou-masmoudi-hadrichbelguith:2010:TALN,
  author    = {Bahou, Younès and Masmoudi, Abir and Hadrich Belguith, Lamia},
  title     = {Traitement des disfluences dans le cadre de la compréhension automatique de l’oral arabe spontané},
  booktitle = {Actes de la 17e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {July},
  year      = {2010},
  address   = {Montréal, Canada},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2010/taln-2010-long-021},
  language  = {french},
  resume    = {Les disfluences inhérents de toute parole spontanée sont un vrai défi pour les systèmes de compréhension de la parole. Ainsi, nous proposons dans cet article, une méthode originale pour le traitement des disfluences (plus précisément, les autocorrections, les répétitions, les hésitations et les amorces) dans le cadre de la compréhension automatique de l’oral arabe spontané. Notre méthode est basée sur une analyse à la fois robuste et partielle, des énoncés oraux arabes. L’idée consiste à combiner une technique de reconnaissance de patrons avec une analyse sémantique superficielle par segments conceptuels. Cette méthode a été testée à travers le module de compréhension du système SARF, un serveur vocal interactif offrant des renseignements sur le transport ferroviaire tunisien (Bahou et al., 2008). Les résultats d’évaluation de ce module montrent que la méthode proposée est très prometteuse. En effet, les mesures de rappel, de précision et de F-Measure sont respectivement de 79.23%, 74.09% et 76.57%.},
  abstract  = {The disfluencies inherent in spontaneous speaking are a real challenge for speech understanding systems. Thus, we propose in this paper, an original method for processing disfluencies (more precisely, self-corrections, repetitions, hesitations and word-fragments) in the context of automatic spontaneous Arabic speech understanding. Our method is based on a robust and partial analysis of Arabic oral utterances. The main idea is to combine a pattern matching technique and a surface semantic analysis with conceptual segments. This method has been evaluated through the understanding module of SARF system, an interactive vocal server offering Tunisian railway information (Bahou et al., 2008). The evaluation results of this module show that the proposed method is very promising. Indeed, the measures of recall, precision and F-Measure are respectively 79.23%, 74.09% and 76.57%.},
  motscles  = {disfluences, segment conceptuel, reconnaissance de patrons, parole arabe spontanée},
  keywords  = {disfluencies, conceptual segment, pattern matching, spontaneous Arabic speech},
}

@inproceedings{guinaudeau-gravier-sebillot:2010:TALN,
  author    = {Guinaudeau, Camille and Gravier, Guillaume and Sébillot, Pascale},
  title     = {Utilisation de relations sémantiques pour améliorer la segmentation thématique de documents télévisuels},
  booktitle = {Actes de la 17e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {July},
  year      = {2010},
  address   = {Montréal, Canada},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2010/taln-2010-long-022},
  language  = {french},
  resume    = {Les méthodes de segmentation thématique exploitant une mesure de la cohésion lexicale peuvent être appliquées telles quelles à des transcriptions automatiques de programmes télévisuels. Cependant, elles sont moins efficaces dans ce contexte, ne prenant en compte ni les particularités des émissions TV, ni celles des transcriptions. Nous étudions ici l’apport de relations sémantiques pour rendre les techniques de segmentation thématique plus robustes. Nous proposons une méthode pour exploiter ces relations dans une mesure de la cohésion lexicale et montrons qu’elles permettent d’augmenter la F1-mesure de +1.97 et +11.83 sur deux corpus composés respectivement de 40h de journaux télévisés et de 40h d’émissions de reportage. Ces améliorations démontrent que les relations sémantiques peuvent rendre les méthodes de segmentation moins sensibles aux erreurs de transcription et au manque de répétitions constaté dans certaines émissions télévisées.},
  abstract  = {Topic segmentation methods based on a measure of the lexical cohesion can be applied as is to automatic transcripts of TV programs. However, these methods are less effective in this context as neither the specificities of TV contents, nor those of automatic transcripts are considered. The aim of this paper is to study the use of semantic relations to make segmentation techniques more robust.We propose a method to account for semantic relations in a measure of the lexical cohesion.We show that such relations increase the F1-measure by +1.97 and +11.83 for two data sets consisting of respectively 40h of news and 40h of longer reports on current affairs. These results demonstrate that semantic relations can make segmentation methods less sensitive to transcription errors or to the lack of repetitions in some television programs.},
  motscles  = {Segmentation thématique, documents oraux, cohésion lexicale, relations sémantiques},
  keywords  = {Topic segmentation, spoken document, lexical cohesion, semantic relations},
}

@inproceedings{adam-muller-fabre:2010:TALN,
  author    = {Adam, Clémentine and Muller, Philippe and Fabre, Cécile},
  title     = {Une évaluation de l’impact des types de textes sur la tâche de segmentation thématique},
  booktitle = {Actes de la 17e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {July},
  year      = {2010},
  address   = {Montréal, Canada},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2010/taln-2010-long-023},
  language  = {french},
  resume    = {Cette étude a pour but de contribuer à la définition des objectifs de la segmentation thématique (ST), en incitant à prendre en considération le paramètre du type de textes dans cette tâche. Notre hypothèse est que, si la ST est certes pertinente pour traiter certains textes dont l’organisation est bien thématique, elle n’est pas adaptée à la prise en compte d’autres modes d’organisation (temporelle, rhétorique), et ne peut pas être appliquée sans précaution à des textes tout-venants. En comparant les performances d’un système de ST sur deux corpus, à organisation thématique "forte" et "faible", nous montrons que cette tâche est effectivement sensible à la nature des textes.},
  abstract  = {This paper aims to contribute to a better definition of the requirements of the text segmentation task, by stressing the need for taking into account the types of texts that can be appropriately considered. Our hypothesis is that while TS is indeed relevant to analyse texts with a thematic organisation, this task is ill-fitted to deal with other modes of text organisation (temporal, rhetorical, etc.). By comparing the performance of a TS system on two corpora, with either a "strong" or a "weak" thematic organisation, we show that TS is sensitive to text types.},
  motscles  = {Segmentation thématique, organisation textuelle, cohésion lexicale, voisins distributionnels},
  keywords  = {Text segmentation, textual organisation, lexical cohesion, distributional neighbours},
}

@inproceedings{jeanlouis-besancon-ferret:2010:TALN,
  author    = {Jean-Louis, Ludovic and Besançon, Romaric and Ferret, Olivier},
  title     = {Utilisation d’indices temporels pour la segmentation événementielle de textes},
  booktitle = {Actes de la 17e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {July},
  year      = {2010},
  address   = {Montréal, Canada},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2010/taln-2010-long-024},
  language  = {french},
  resume    = {Dans le domaine de l’Extraction d’Information, une place importante est faite à l’extraction d’événements dans des dépêches d’actualité, particulièrement justifiée dans le contexte d’applications de veille. Or il est fréquent qu’une dépêche d’actualité évoque plusieurs événements de même nature pour les comparer. Nous proposons dans cet article d’étudier des méthodes pour segmenter les textes en séparant les événements, dans le but de faciliter le rattachement des informations pertinentes à l’événement principal. L’idée est d’utiliser des modèles d’apprentissage statistique exploitant les marqueurs temporels présents dans les textes pour faire cette segmentation. Nous présentons plus précisément deux modèles (HMM et CRF) entraînés pour cette tâche et, en faisant une évaluation de ces modèles sur un corpus de dépêches traitant d’événements sismiques, nous montrons que les méthodes proposées permettent d’obtenir des résultats au moins aussi bons que ceux d’une approche ad hoc, avec une approche beaucoup plus générique.},
  abstract  = {One of the early application of Information Extraction, motivated by the needs for intelligence tools, is the detection of events in news articles. But this detection may be difficult when news articles mention several occurrences of events of the same kind, which is often done for comparison purposes. We propose in this article new approaches to segment the text of news articles in units relative to only one event, in order to help the identification of relevant information associated to the main event of the news. We present two approaches that use statistical machine learning models (HMM and CRF) exploiting temporal information extracted from the texts as a basis for this segmentation. The evaluation of these approaches in the domain of seismic events show that with a robust and generic approach, we can achieve results at least as good as results obtained with an ad hoc approach.},
  motscles  = {Extraction d’information, extraction d’événements, segmentation de textes, indices temporels, apprentissage statistique},
  keywords  = {Information extraction, event extraction, text segmentation, temporal cues, statistical machine learning},
}

@inproceedings{torresmoreno-EtAl:2010:TALN,
  author    = {Torres-Moreno, Juan-Manuel and Saggion, Horacio and da Cunha, Iria and Velázquez-Morales, Patricia and Sanjuan, Eric},
  title     = {Évaluation automatique de résumés avec et sans référence},
  booktitle = {Actes de la 17e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {July},
  year      = {2010},
  address   = {Montréal, Canada},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2010/taln-2010-long-025},
  language  = {french},
  resume    = {Nous étudions différentes méthodes d’évaluation de résumé de documents basées sur le contenu. Nous nous intéressons en particulier à la corrélation entre les mesures d’évaluation avec et sans référence humaine. Nous avons développé FRESA, un nouveau système d’évaluation fondé sur le contenu qui calcule les divergences entre les distributions de probabilité. Nous appliquons notre système de comparaison aux diverses mesures d’évaluation bien connues en résumé de texte telles que la Couverture, Responsiveness, Pyramids et Rouge en étudiant leurs associations dans les tâches du résumé multi-document générique (francais/anglais), focalisé (anglais) et résumé mono-document générique (français/espagnol).},
  abstract  = {We study document-summary content-based evaluation methods in text summarization and we investigate the correlation among evaluation measures with and without human models. We apply our comparison framework to various well-established content-based evaluation measures in text summarization such as Coverage, Responsiveness, Pyramids and Rouge studying their associations in various text summarization tasks including generic (English/French) and focus-based (English) multi-document summarization and generic multi and single-document summarization (French/Spanish). The research is carried out using the new content-based evaluation framework FRESA to compute the divergences among probability distributions.},
  motscles  = {Mesures d’évaluation, Résumé automatique de textes},
  keywords  = {Evaluation measures, Text Automatic Summarization},
}

@inproceedings{genest-lapalme-yousfimonod:2010:TALN,
  author    = {Genest, Pierre-Etienne and Lapalme, Guy and Yousfi-Monod, Mehdi},
  title     = {Jusqu’où peut-on aller avec les méthodes par extraction pour la rédaction de résumés?},
  booktitle = {Actes de la 17e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {July},
  year      = {2010},
  address   = {Montréal, Canada},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2010/taln-2010-long-026},
  language  = {french},
  resume    = {La majorité des systèmes de résumés automatiques sont basés sur l’extraction de phrases, or on les compare le plus souvent avec des résumés rédigés manuellement par abstraction. Nous avons mené une expérience dans le but d’établir une limite supérieure aux performances auxquelles nous pouvons nous attendre avec une approche par extraction. Cinq résumeurs humains ont composé 88 résumés de moins de 100 mots, en extrayant uniquement des phrases présentes intégralement dans les documents d’entrée. Les résumés ont été notés sur la base de leur contenu, de leur niveau linguistique et de leur qualité globale par les évaluateurs de NIST dans le cadre de la compétition TAC 2009. Ces résumés ont obtenus de meilleurs scores que l’ensemble des 52 systèmes automatiques participant à la compétition, mais de nettement moins bons que ceux obtenus par les résumeurs humains pouvant formuler les phrases de leur choix dans le résumé. Ce grand écart montre l’insuffisance des méthodes par extraction pure.},
  abstract  = {The majority of automatic summarization systems are based on sentence extraction, whereas they are usually compared with human-written, abstractive summaries. We have thus conducted an experiment to establish an upper bound on the expected performance of extractive summarization. 5 human summarizers completed 88 summaries of no more than 100 words from unedited sentences of the source documents. The summaries were scored based on their content, linguistic quality and overall responsiveness by NIST annotators in the context of the TAC 2009 competition. The human extracts received better scores than all of the 52 participating automatic systems, but much lower scores than those obtained by human summarizers free to use abstraction. This large gap shows that pure extraction methods are insufficient for summarization.},
  motscles  = {Résumés automatiques, résumés par extraction, résumés manuels},
  keywords  = {Automatic summarization, extractive summarization, manual summarization},
}

@inproceedings{garciafernandez-rosset-vilnat:2010:TALN,
  author    = {Garcia-Fernandez, Anne and Rosset, Sophie and Vilnat, Anne},
  title     = {Comment formule-t-on une réponse en langue naturelle ?},
  booktitle = {Actes de la 17e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {July},
  year      = {2010},
  address   = {Montréal, Canada},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2010/taln-2010-long-027},
  language  = {french},
  resume    = {Cet article présente l’étude d’un corpus de réponses formulées par des humains à des questions factuelles. Des observations qualitatives et quantitatives sur la reprise d’éléments de la question dans les réponses sont exposées. La notion d’information-réponse est introduite et une étude de la présence de cet élément dans le corpus est proposée. Enfin, les formulations des réponses sont étudiées.},
  abstract  = {This paper presents the study of a corpus of human answers to factual questions. Observations of how and how much question elements are used in the answer are done. We define the concept of “information-answer” and study its presence in the corpus. Finally, answer formulations are shown.},
  motscles  = {systèmes de réponse à une question, variations linguistiques, réponse en langue naturelle, oral et écrit},
  keywords  = {question-answering systems, linguistics variations, natural language answer, oral and written},
}

@inproceedings{do-besacier-castelli:2010:TALN,
  author    = {Do, Thi Ngoc Diep and Besacier, Laurent and Castelli, Eric},
  title     = {Apprentissage non supervisé pour la traduction automatique : application à un couple de langues peu doté},
  booktitle = {Actes de la 17e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {July},
  year      = {2010},
  address   = {Montréal, Canada},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2010/taln-2010-long-028},
  language  = {french},
  resume    = {Cet article présente une méthode non-supervisée pour extraire des paires de phrases parallèles à partir d’un corpus comparable. Un système de traduction automatique est utilisé pour exploiter le corpus comparable et détecter les paires de phrases parallèles. Un processus itératif est exécuté non seulement pour augmenter le nombre de paires de phrases parallèles extraites, mais aussi pour améliorer la qualité globale du système de traduction. Une comparaison avec une méthode semi-supervisée est présentée également. Les expériences montrent que la méthode non-supervisée peut être réellement appliquée dans le cas où on manque de données parallèles. Bien que les expériences préliminaires soient menées sur la traduction français-anglais, cette méthode non-supervisée est également appliquée avec succès à un couple de langues peu doté : vietnamien-français.},
  abstract  = {This paper presents an unsupervised method for extracting parallel sentence pairs from a comparable corpus. A translation system is used to mine and detect the parallel sentence pairs from the comparable corpus. An iterative process is implemented not only to increase the number of extracted parallel sentence pairs but also to improve the overall quality of the translation system. A comparison between this unsupervised method and a semi-supervised method is also presented. The experiments conducted show that the unsupervised method can be really applied in cases where parallel data are not available. While preliminary experiments are conducted on French-English translation, this unsupervised method is also applied successfully to a low e-resourced language pair (Vietnamese-French).},
  motscles  = {apprentissage non-supervisé, système de traduction automatique, corpus comparable, paires de phrases parallèles},
  keywords  = {unsupervised training, machine translation, comparable corpus, parallel sentence pairs},
}

@inproceedings{elkholy-habash:2010:TALN,
  author    = {El Kholy, Ahmed and Habash, Nizar},
  title     = {},
  booktitle = {Actes de la 17e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {July},
  year      = {2010},
  address   = {Montréal, Canada},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2010/taln-2010-long-029},
  language  = {french},
  note      = {Orthographic and Morphological Processing for English-Arabic Statistical Machine Translation},
  resume    = {De nombreux travaux en Traduction Automatique Statistique (TAS) pour des langues d’entrée morphologiquement riches montrent que la ségmentation morphologique et la normalisation orthographique améliorent la qualité des traductions en diminuant la sparsité des données. Dans cet article, nous étudions l’impact de ce prétraitement pour la TAS vers une langue de sortie riche morphologiquement, comme l’Arabe. Nous explorons l’espace des schémas de segmentation et des options de normalisation possibles. Nous évaluons seulement la sortie sous une forme désegmentée et enrichie orthographiquement. Nos résultats montrent d’une part que le meilleur schéma pour la ségmentation est celui de la Penn Arabic Treebank. D’autre part, la meilleure procédure de prétraitement consiste à entraîner le système sur des données normalisées orthographiquement, puis à enrichir et désegmenter les traductions en sortie.},
  abstract  = {Much of the work on Statistical Machine Translation (SMT) from morphologically rich languages has shown that morphological tokenization and orthographic normalization help improve SMT quality because of the sparsity reduction they contribute. In this paper, we study the effect of these processes on SMT when translating into a morphologically rich language, namely Arabic.We explore a space of tokenization schemes and normalization options. We only evaluate on detokenized and orthographically correct (enriched) output. Our results show that the best performing tokenization scheme is that of the Penn Arabic Treebank. Additionally, training on orthographically normalized (reduced) text then jointly enriching and detokenizing the output outperforms training on enriched text.},
  motscles  = {Langue Arabe, Morphologie, Ségmentation, Désegmentation, La Traduction Automatique Statistique},
  keywords  = {Arabic Language, Morphology, Tokenization, Detokenization, Statistical Machine Translation},
}

@inproceedings{carpuat-marton-habash:2010:TALN,
  author    = {Carpuat, Marine and Marton, Yuval and Habash, Nizar},
  title     = {},
  booktitle = {Actes de la 17e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {July},
  year      = {2010},
  address   = {Montréal, Canada},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2010/taln-2010-long-030},
  language  = {french},
  note      = {Reordering Matrix Post-verbal Subjects for Arabic-to-English SMT},
  resume    = {Distinguer les constructions verbe-sujet (VS) des propositions principales (“matrice”) et subordonnées (“non-matrice”) améliore notre nouveau modèle de réordonnancement pour l’alignement des mots en Traduction Automatique Statistique (TAS) arabe-anglais (Carpuat et al., 2010). D’une part, la majorité des constructions verbe-sujet (VS) dans les propositions principales doivent être réordonnancées en anglais, alors que l’ordre du verbe et du sujet est préservé dans la moitié des cas de constructions VS subordonnées. D’autre part, nous constatons que notre analyseur syntaxique parvient à mieux identifier les constructions VS des propositions principales. Ces observations nous amènent à limiter le réordonnancement des constructions VS à celles des propositions principales lors de l’alignement des mots. Cette technique améliore substantiellement la performance d’un système de TAS conventionnel, et d’un système qui réordonnance toutes les constructions VS. L’amélioration des mesures BLEU et TER obtenue par simple réordonnancement représente presque la moitié de l’amélioration obtenue lorsque le modèle d’alignement des mots est entraîné sur un corpus parallèle d’une taille cinq fois supérieure.},
  abstract  = {We improve our recently proposed technique for integrating Arabic verb-subject constructions in SMT word alignment (Carpuat et al., 2010) by distinguishing between matrix (or main clause) and non-matrix Arabic verb-subject constructions. In gold translations, most matrix VS (main clause verb-subject) constructions are translated in inverted SV order, while non-matrix (subordinate clause) VS constructions are inverted in only half the cases. In addition, while detecting verbs and their subjects is a hard task, our syntactic parser detects VS constructions better in matrix than in non-matrix clauses. As a result, reordering only matrix VS for word alignment consistently improves translation quality over a phrase-based SMT baseline, and over reordering all VS constructions, in both medium- and large-scale settings. In fact, the improvements obtained by reordering matrix VS on the medium-scale setting remarkably represent 44% of the gain in BLEU and 51% of the gain in TER obtained with a word alignment training bitext that is 5 times larger.},
  motscles  = {Analyse morpho-syntaxique de l’arabe, Traduction automatique statistique, VS, VSO},
  keywords  = {},
}

@inproceedings{zock-lapalme:2010:TALN,
  author    = {Zock, Michael and Lapalme, Guy},
  title     = {Du TAL au TIL},
  booktitle = {Actes de la 17e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {July},
  year      = {2010},
  address   = {Montréal, Canada},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2010/taln-2010-long-031},
  language  = {french},
  resume    = {Historiquement deux types de traitement de la langue ont été étudiés: le traitement par le cerveau (approche psycholinguistique) et le traitement par la machine (approche TAL). Nous pensons qu’il y a place pour un troisième type: le traitement interactif de la langue (TIL), l’ordinateur assistant le cerveau. Ceci correspond à un besoin réel dans la mesure où les gens n’ont souvent que des connaissances partielles par rapport au problème à résoudre. Le but du TIL est de construire des ponts entre ces connaissances momentanées d’un utilisateur et la solution recherchée. À l'aide de quelques exemples, nous essayons de montrer que ceci est non seulement faisable et souhaitable, mais également d’un coût très raisonnable.},
  abstract  = {Historically two types of NLP have been investigated: fully automated processing of language by machines (NLP) and autonomous processing of natural language by people, i.e. the human brain (psycholinguistics). We believe that there is room and need for another kind, INLP: interactive natural language processing. This intermediate approach starts from peoples’ needs, trying to bridge the gap between their actual knowledge and a given goal. Given the fact that peoples’ knowledge is variable and often incomplete, the aim is to build bridges linking a given knowledge state to a given goal. We present some examples, trying to show that this goal is worth pursuing, achievable and at a reasonable cost.},
  motscles  = {traitement interactif de la langue, prise en compte de l'usager, outils de traitement de la langue, apprentissage des langues, dictionnaires, livres de phrases, concordanciers, traduction},
  keywords  = {interactive NLP (INLP), user interaction, tools for NLP, language learning, dictionaries, phrase books, concordancers, translation},
}

@inproceedings{mertens:2010:TALN,
  author    = {Mertens, Piet},
  title     = {Restrictions de sélection et réalisations syntagmatiques dans DICOVALENCE Conversion vers un format utilisable en TAL},
  booktitle = {Actes de la 17e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {July},
  year      = {2010},
  address   = {Montréal, Canada},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2010/taln-2010-long-032},
  language  = {french},
  resume    = {Cet article décrit des modifications du dictionnaire de valence des verbes du français DICOVALENCE qui visent à le rendre neutre par rapport aux modèles syntaxiques, à expliciter certaines informations sur le cadre de sous-catégorisation et à le rendre ainsi directement utilisable en TAL. Les informations explicitées sont les suivantes : (a) les fonctions syntaxiques des arguments verbaux, (b) les restrictions de sélection portant sur ces arguments et (c) leurs réalisations syntagmatiques possibles. Les restrictions sont exprimées à l’aide de traits sémantiques. L’article décrit aussi le calcul de ces traits sémantiques à partir des paradigmes des pronoms (et d’éléments similaires) associés aux arguments. On obtient un format indépendant du modèle syntaxique, dont l’interprétation est transparente.},
  abstract  = {This paper describes modifications to the verbal valency dictionary for French, known as DICOVALENCE, in order to obtain a theory-independent syntactic lexicon, to make explicit certain information about the slots in the valency frame, to facilitate the use of the lexicon in natural language processing. The modifications make explicit the following aspects: (a) the syntactic function of the slots, (b) the selection restrictions on these verbal arguments, (c) their possible phrasal realizations. Selection restrictions are expressed using semantic features. The article describes how these features are obtained from the paradigms of pronouns (and similar elements) associated with the valency slots. This results in a format which is theoryindependent, with a transparent interpretation.},
  motscles  = {lexiques syntaxiques, restrictions de sélection, traits sémantiques},
  keywords  = {lexical databases, selection restrictions, semantic features},
}

@inproceedings{barriere:2010:TALN,
  author    = {Barrière, Caroline},
  title     = {Recherche contextuelle d’équivalents en banque de terminologie},
  booktitle = {Actes de la 17e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {July},
  year      = {2010},
  address   = {Montréal, Canada},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2010/taln-2010-long-033},
  language  = {french},
  resume    = {Notre recherche démontre que l’utilisation du contenu d’un texte à traduire permet de mieux cibler dans une banque de terminologie les équivalents terminologiques pertinents à ce texte. Une banque de terminologie a comme particularité qu’elle catégorise ses entrées (fiches) en leur assignant un ou des domaines provenant d’une liste de domaines préétablie. La stratégie ici présentée repose sur l’utilisation de cette information sur les domaines. Un algorithme a été développé pour l’assignation automatique d’un profil de domaines à un texte. Celui-ci est combiné à un algorithme d’appariement entre les domaines d’un terme présent dans la banque de terminologie et le profil de domaines du texte. Pour notre expérimentation, des résumés bilingues (français et anglais) provenant de huit revues scientifiques nous fournissent un ensemble de 1130 paires d’équivalents terminologiques et le Grand Dictionnaire Terminologique (Office Québécois de la Langue Française) nous sert de ressource terminologique. Sur notre ensemble, nous démontrons une réduction de 75% du rang moyen de l’équivalent correct en comparaison avec un choix au hasard.},
  abstract  = {Our research shows the usefulness of taking into account the context of a term within a text to be translated to better find an appropriate term equivalent for it in a term bank. A term bank has the particularity of categorising its records by assigning them one or more domains from a pre-established list of domains. The strategy presented here uses this domain information. An algorithm has been developed to automatically assign a domain profile to a source text. It is then combined with another algorithm which finds a match between a term’s domains (as found in the term bank) and the text’s domain profile. For our experimentation, bilingual abstracts (French-English) from eight scientific journals provide 1130 pairs of term equivalents. The Grand Dictionnaire Terminologique (Office Québécois de la Langue Française) is used as a terminological ressource. On our data set, we show a reduction of 75% in the average rank of the correct equivalent, in comparison to a random choice.},
  motscles  = {recherche contextuelle, équivalents terminologiques, banque de terminologie, désambiguïsation par domaine},
  keywords  = {contextual search, term equivalents, term bank, domain-based disambiguation},
}

@inproceedings{bonfante-EtAl:2010:TALN,
  author    = {Bonfante, Guillaume and Guillaume, Bruno and Morey, Mathieu and Perrier, Guy},
  title     = {Réécriture de graphes de dépendances pour l’interface syntaxe-sémantique},
  booktitle = {Actes de la 17e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {July},
  year      = {2010},
  address   = {Montréal, Canada},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2010/taln-2010-long-034},
  language  = {french},
  resume    = {Nous définissons le beta-calcul, un calcul de réécriture de graphes, que nous proposons d’utiliser pour étudier les liens entre différentes représentations linguistiques. Nous montrons comment transformer une analyse syntaxique en une représentation sémantique par la composition de deux jeux de règles de beta-calcul. Le premier souligne l’importance de certaines informations syntaxiques pour le calcul de la sémantique et explicite le lien entre syntaxe et sémantique sous-spécifiée. Le second décompose la recherche de modèles pour les représentations sémantiques sous-spécifiées.},
  abstract  = {We define the beta-calculus, a graph-rewriting calculus, which we propose to use to study the links between different linguistic representations. We show how to transform a syntactic analysis into a semantic analysis via the composition of two sets of beta-calculus rules. The first one underlines the importance of some syntactic information to compute the semantics and clearly expresses the link between syntax and underspecified semantics. The second one breaks up the search for models of underspecified semantic representations.},
  motscles  = {Dépendances, réécriture de graphes, interface syntaxe-sémantique, DMRS},
  keywords  = {Dependencies, graph rewriting, syntax-semantics interface, DMRS},
}

@inproceedings{fort-francois-ghribi:2010:TALN,
  author    = {Fort, Karën and François, Claire and Ghribi, Maha},
  title     = {Évaluer des annotations manuelles dispersées : les coefficients sont-ils suffisants pour estimer l’accord inter-annotateurs ?},
  booktitle = {Actes de la 17e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {July},
  year      = {2010},
  address   = {Montréal, Canada},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2010/taln-2010-long-035},
  language  = {french},
  resume    = {L’objectif des travaux présentés dans cet article est l’évaluation de la qualité d’annotations manuelles de relations de renommage de gènes dans des résumés scientifiques, annotations qui présentent la caractéristique d’être très dispersées. Pour cela, nous avons calculé et comparé les coefficients les plus communément utilisés, entre autres kappa (Cohen, 1960) et pi (Scott, 1955), et avons analysé dans quelle mesure ils sont adaptés à nos données. Nous avons également étudié les différentes pondérations applicables à ces coefficients permettant de calculer le kappa pondéré (Cohen, 1968) et l’alpha (Krippendorff, 1980, 2004). Nous avons ainsi étudié le biais induit par la grande prévalence d’une catégorie et défini un mode de calcul des distances entre catégories reposant sur les annotations réalisées.},
  abstract  = {This article details work aiming at evaluating the quality of the manual annotation of gene renaming relations in scientific abstracts, which generates sparse annotations. To evaluate these annotations, we computed and compared the results obtained using the commonly advocated inter-annotator agreement coefficients such as kappa (Cohen, 1960) or pi (Scott, 1955) and analyzed to which extent they are relevant for our data.We also studied the different weighting computations applicable to kappa! (Cohen, 1968) and alpha (Krippendorff, 1980, 2004) and estimated the bias introduced by prevalence. We then define a way to compute distances between categories based on the produced annotations.},
  motscles  = {Annotation manuelle, évaluation, accord inter-annotateurs},
  keywords  = {Manual annotation, evaluation, inter-annotator agreement},
}

@inproceedings{lehong-EtAl:2010:TALN,
  author    = {Le-Hong, Phuong and Roussanaly, Azim and Nguyen, Thi Minh Huyen and Rossignol, Mathias},
  title     = {},
  booktitle = {Actes de la 17e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {July},
  year      = {2010},
  address   = {Montréal, Canada},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2010/taln-2010-long-036},
  language  = {french},
  note      = {An empirical study of maximum entropy approach for part-of-speech tagging of Vietnamese texts},
  resume    = {Nous présentons dans cet article une étude empirique de l’application de l’approche de l’entropie maximale pour l’étiquetage syntaxique de textes vietnamiens. Le vietnamien est une langue qui possède des caractéristiques spéciales qui la distinguent largement des langues occidentales. Notremeilleur étiqueteur explore et inclut des connaissances utiles qui, en terme de performance pour l’étiquetage de textes vietnamiens, fournit un taux de précision globale de 93.40% et de 80.69% pour les mots inconnus sur un ensemble de test du corpus arboré vietnamien. Notre étiqueteur est nettement supérieur à celui qui est en train d’être utilisé pour développer le corpus arboré vietnamien, et à l’heure actuelle c’est le meilleur résultat obtenu pour l’étiquetage de textes vietnamiens.},
  abstract  = {This paper presents an empirical study on the application of the maximum entropy approach for part-of-speech tagging of Vietnamese text, a language with special characteristics which largely distinguish it from occidental languages. Our best tagger explores and includes useful knowledge sources for tagging Vietnamese text and gives a 93.40%overall accuracy and a 80.69%unknown word accuracy on a test set of the Vietnamese treebank. Our tagger significantly outperforms the tagger that is being used for building the Vietnamese treebank, and as far as we are aware, this is the best tagging result ever published for the Vietnamese language.},
  motscles  = {Etiqueteur syntaxique, entropie maximale, texte, vietnamien},
  keywords  = {Part-of-speech tagger, maximum entropy, text, Vietnamese},
}

@inproceedings{dasylva:2010:TALN,
  author    = {Da Sylva, Lyne},
  title     = {Extraction semi-automatique d’un vocabulaire savant de base pour l’indexation automatique},
  booktitle = {Actes de la 17e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {July},
  year      = {2010},
  address   = {Montréal, Canada},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2010/taln-2010-long-037},
  language  = {french},
  resume    = {Le projet décrit vise à soutenir les efforts de constitution de ressources lexicales utiles à l’indexation automatique. Un type de vocabulaire utile à l’indexation est défini, le vocabulaire savant de base, qui peut s’articuler avec le vocabulaire spécialisé pour constituer des entrées d’index structurées. On présente les résultats d’ une expérimentation d’ extraction (semi-)automatique des mots du vocabulaire savant de base à partir d’un corpus ciblé, constitué de résumés d’articles scientifiques en français et en anglais. La tâche d’extraction a réussi à doubler une liste originale constituée manuellement pour le français. La comparaison est établie avec une expérimentation similaire effectuée pour l’anglais sur un corpus plus grand et contenant des résumés d’articles non seulement en sciences pures mais aussi en sciences humaines et sociales.},
  abstract  = {This project aims to help develop lexical resources useful for automatic indexing. A type of useful vocabulary for indexing is defined, the basic scholarly vocabulary, which can combine with specialized vocabulary items to form evocative, structured index entries. The article presents the results of an experiment of (semi-)automatic extraction of the basic scholarly vocabulary lexical items from a large corpus. The corpus is especially suited to the task; it consists of abstracts of scientific articles in French and English. The extraction task was successful in doubling the size of a previously manually compiled list. A comparison is made with a similar experiment conducted for English on a larger corpus which also contained summaries of articles in the humanities and social sciences.},
  motscles  = {classes de vocabulaire, indexation automatique, extraction automatique, corpus, approche basée sur les corpus, vocabulaire savant de base, ressources lexicales, français},
  keywords  = {vocabulary classes, automatic indexing, automatic extraction, corpus, corpus-based approach, basic scholarly vocabulary, lexical resources, French},
}

@inproceedings{lavallee-langlais:2010:TALN,
  author    = {Lavallée, Jean-François and Langlais, Philippe},
  title     = {Apprentissage non supervisé de la morphologie d’une langue par généralisation de relations analogiques},
  booktitle = {Actes de la 17e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {July},
  year      = {2010},
  address   = {Montréal, Canada},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2010/taln-2010-long-038},
  language  = {french},
  resume    = {Bien que les approches fondées sur la théorie de l’information sont prédominantes dans le domaine de l’analyse morphologique non supervisée, depuis quelques années, d’autres approches ont gagné en popularité, dont celles basées sur l’analogie formelle. Cette dernière reste tout de même marginale due notamment à son coût de calcul élevé. Dans cet article, nous proposons un algorithme basé sur l’analogie formelle capable de traiter les lexiques volumineux. Nous introduisons pour cela le concept de règle de cofacteur qui permet de généraliser l’information capturée par une analogie tout en contrôlant les temps de traitement. Nous comparons notre système à 2 systèmes : Morfessor (Creutz \& Lagus, 2005), un système de référence dans de nombreux travaux sur l’analyse morphologique et le système analogique décrit par Langlais (2009). Nous en montrons la supériorité pour 3 des 5 langues étudiées ici : le finnois, le turc, et l’allemand.},
  abstract  = {Although approaches based on information theory are prominent in the field of unsupervised morphological analysis, in recent years, other approaches have gained in popularity. Those based on formal analogy remain marginal partly because of their high computational cost. In this paper we propose an algorithm based on formal analogy able to handle large lexicons. We introduce the concept of cofactor rule which allows the generalization of the information captured by analogy, while controlling the processing time. We compare our system to 2 others : Morfessor (Creutz \& Lagus, 2005), a reference in many studies on morphological analysis and the analogical system described by Langlais (2009). We show the superiority of our approach for 3 out of the 5 languages studied here : Finnish, Turkish, and German.},
  motscles  = {Analyse morphologique non supervisée, Analogie formelle, Approche à base de graphe},
  keywords  = {Unsupervised Learning of Morphology, Formal Analogy, Graph-Based Approach},
}

@inproceedings{claveau-kijak:2010:TALN,
  author    = {Claveau, Vincent and Kijak, Ewa},
  title     = {Analyse morphologique en terminologie biomédicale par alignement et apprentissage non-supervisé},
  booktitle = {Actes de la 17e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {July},
  year      = {2010},
  address   = {Montréal, Canada},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2010/taln-2010-long-039},
  language  = {french},
  resume    = {Dans le domaine biomédical, beaucoup de termes sont des composés savants (composés de plusieurs racines gréco-latines). L’étude de leur morphologie est importante pour de nombreuses applications puisqu’elle permet de structurer ces termes, de les rechercher efficacement, de les traduire... Dans cet article, nous proposons de suivre une démarche originale mais fructueuse pour mener cette analyse morphologique sur des termes simples en français, en nous appuyant sur une langue pivot, le japonais, et plus précisément sur les termes écrits en kanjis. Pour cela nous avons développé un algorithme d’alignement de termes spécialement adapté à cette tâche. C’est cet alignement d’un terme français avec sa traduction en kanjis qui fournit en même temps une décomposition en morphe et leur étiquetage par les kanjis correspondants. Évalué sur un jeu de données conséquent, notre approche obtient une précision supérieure à 70% et montrent son bien fondé en comparaison avec les techniques existantes. Nous illustrons également l’intérêt de notre démarche au travers de deux applications directes de ces alignements : la traduction de termes inconnus et la découverte de relations entre morphes pour la tructuration terminologique.},
  abstract  = {In the biomedical domain, many terms are neoclassical compounds (composed of several Greek or Latin roots). The study of their morphology is important for numerous applications since it makes it possible to structure them, retrieve them efficiently, translate them... In this paper, we propose an original yet fruitful approach to carry out this morphological analysis by relying on Japanese, more precisely on terms written in kanjis, as a pivot language. In order to do so, we have developed a specially crafted alignment algorithm. This alignment process of French terms with their kanji-based counterparts provides at the same time a decomposition of the French term into morphs, and a kanji label for each morph. Evaluated on a big dataset, our approach yields a precision greater than 70% and shows its the relevance compared with existing techniques. We also illustrate the validity of our reasoning through two direct applications of the produced alignments: translation of unknown terms and discovering of relationships between morphs for terminological structuring.},
  motscles  = {Alignement, terminologie, morphologie, analogie, traduction de terme, kanji},
  keywords  = {Alignment, terminology, morphology, analogy, term translation, kanji},
}

@inproceedings{sagot-walther:2010:TALN,
  author    = {Sagot, Benoît and Walther, Géraldine},
  title     = {Développement de ressources pour le persan: lexique morphologique et chaîne de traitements de surface},
  booktitle = {Actes de la 17e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {July},
  year      = {2010},
  address   = {Montréal, Canada},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2010/taln-2010-long-040},
  language  = {french},
  resume    = {Nous présentons PerLex, un lexique morphologique du persan à large couverture et librement disponible, accompagné d’une chaîne de traitements de surface pour cette langue. Nous décrivons quelques caractéristiques de la morphologie du persan, et la façon dont nous l’avons représentée dans le formalisme lexical Alexina, sur lequel repose PerLex. Nous insistons sur la méthodologie que nous avons employée pour construire les entrées lexicales à partir de diverses sources, ainsi que sur les problèmes liés à la normalisation typographique. Le lexique obtenu a une couverture satisfaisante sur un corpus de référence, et devrait donc constituer un bon point de départ pour le développement d’un lexique syntaxique du persan.},
  abstract  = {We introduce PerLex, a large-coverage and freely-available morphological lexicon for the Persian language, as well as a corresponding surface processing chain. We describe the main features of the Persian morphology, and the way we have represented it within the Alexina formalism, on which PerLex is based. We focus on the methodology we used for constructing lexical entries from various sources, as well as on the problems related to typographic normalisation. The resulting lexicon shows a satisfying coverage on a reference corpus and should therefore be a good starting point for developing a syntactic lexicon for the Persian language.},
  motscles  = {Lexique morphologique, Persan, Développement de lexiques, Traitements de surface},
  keywords  = {Morphological lexicon, Persian language, Lexical development, Surface processing.},
}

@inproceedings{deleger-cartoni:2010:TALN,
  author    = {Deléger, Louise and Cartoni, Bruno},
  title     = {Adjectifs relationnels et langue de spécialité : vérification d’une hypothèse linguistique en corpus comparable médical},
  booktitle = {Actes de la 17e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {July},
  year      = {2010},
  address   = {Montréal, Canada},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2010/taln-2010-court-001},
  language  = {french},
  resume    = {Cet article présente une étude en corpus comparable médical pour confirmer la préférence d’utilisation des adjectifs relationnels dans les langues de spécialité et examiner plus finement l’alternance entre syntagmes nominaux avec adjectifs relationnels et syntagmes avec complément prépositionnel.},
  abstract  = {This paper presents a study in medical comparable corpora that aims to confirm the preferred use of relational adjectives in specialised languages and to examine in a more fine-grained manner the alternance between phrases with adjective and noun phrases with prepositional complement.},
  motscles  = {corpus comparables monolingues, morphologie constructionnelle, langue de spécialité},
  keywords  = {monolingual comparable corpora, constructional morphology, specialised language},
}

@inproceedings{lafourcade-joubert:2010:TALN,
  author    = {Lafourcade, Mathieu and Joubert, Alain},
  title     = {Détermination et pondération des raffinements d’un terme à partir de son arbre des usages nommés},
  booktitle = {Actes de la 17e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {July},
  year      = {2010},
  address   = {Montréal, Canada},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2010/taln-2010-court-002},
  language  = {french},
  resume    = {Grâce à la participation d’un grand nombre de personnes via des jeux accessibles sur le web, nous avons construit un réseau lexical évolutif de grande taille pour le Français. A partir de cette ressource, nous avons abordé la question de la détermination des sens d’usage d’un terme, puis après avoir introduit la notion de similarité entre ces différents usages, nous avons pu obtenir pour un terme son arbre des usages : la racine regroupe tous les usages du terme et une descente dans l’arbre correspond à un raffinement de ces usages. Le nommage des différents noeuds est effectué lors d’une descente en largeur. En simplifiant l’arbre des usages nommés, nous déterminons les différents sens d’un terme, sens que nous introduisons dans le réseau lexical en tant que noeuds de raffinement du terme considéré. Nous terminons par une évaluation empirique des résultats obtenus.},
  abstract  = {Thanks to the participation of a large number of persons via web-based games, a largesized evolutionary lexical network is available for French. With this resource, we approached the question of the determination of the word usages of a term, and after introducing the notion of similarity between these various word usages, we were able to build for a term its word usage tree: the root groups together all possible usages of this term and a search in the tree corresponds to a refinement of these word usages. The labelling of the various nodes is made during a width-first search. From its labelled word usage tree, we obtain the different meanings of a term, which can be inserted in the lexical network as refinement nodes for this term. Lastly, we present an evaluation of the results we obtain.},
  motscles  = {réseau lexical, arbre des usages nommés d’un terme, pondération des sens d’un terme},
  keywords  = {lexical network, tree of labelled word usages for a term, weighting of the meanings},
}

@inproceedings{chevelu-EtAl:2010:TALN,
  author    = {Chevelu, Jonathan and Lepage, Yves and Moudenc, Thierry and Putois, Ghislain},
  title     = {L’évaluation des paraphrases : pour une prise en compte de la tâche},
  booktitle = {Actes de la 17e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {July},
  year      = {2010},
  address   = {Montréal, Canada},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2010/taln-2010-court-003},
  language  = {french},
  resume    = {Les définitions des paraphrases privilégient généralement la conservation du sens. Cet article démontre par l’absurde qu’une évaluation uniquement basée sur la conservation du sens permet à un système inutile de production de paraphrase d’être jugé meilleur qu’un système au niveau de l’état de l’art. La conservation du sens n’est donc pas l’unique critère des paraphrases. Nous exhibons les trois objectifs des paraphrases : la conservation du sens, la naturalité et l’adaptation à la tâche. La production de paraphrase est alors un compromis dépendant de la tâche entre ces trois critères et ceux-ci doivent être pris en compte lors des évaluations.},
  abstract  = {Meaning preservation is generally rooted in the paraphrase definitions. This article proves by reductio ad absurdum that an evaluation based only on the meaning preservation can rank a dummy and useless system better than a state-of-the-art system. Meaning preservation is therefore not the one and only criterion for a paraphrase system. We exhibit the three objectives of paraphrase : meaning preservation, sentence naturalness and adequacy for the task. Paraphrase generation consists actually in reaching a taskdependent compromise between these three criteria, and they have to be taken into account during each evaluation process.},
  motscles  = {Générateur de paraphrase, évaluation des paraphrases},
  keywords  = {Paraphrase generator, paraphrase evaluation},
}

@inproceedings{collin-gaillard-bouraoui:2010:TALN,
  author    = {Collin, Olivier and Gaillard, Benoît and Bouraoui, Jean-Léon},
  title     = {Constitution d'une ressource sémantique issue du treillis des catégories de Wikipedia},
  booktitle = {Actes de la 17e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {July},
  year      = {2010},
  address   = {Montréal, Canada},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2010/taln-2010-court-004},
  language  = {french},
  resume    = {Le travail présenté dans cet article s'inscrit dans le thème de l'acquisition automatique de ressources sémantiques s'appuyant sur les données de Wikipedia. Nous exploitons le graphe des catégories associées aux pages de Wikipedia à partir duquel nous extrayons une hiérarchie de catégories parentes, sémantiquement et thématiquement liées. Cette extraction est le résultat d'une stratégie de plus court chemin appliquée au treillis global des catégories. Chaque page peut ainsi être représentée dans l'espace de ses catégories propres, ainsi que des catégories parentes. Nous montrons la possibilité d'utiliser cette ressource pour deux applications. La première concerne l'indexation et la classification des pages de Wikipedia. La seconde concerne la désambiguïsation dans le cadre d'un traducteur de requêtes français/anglais. Ce dernier travail a été réalisé en exploitant les catégories des pages anglaises.},
  abstract  = {This work is closely related to the domain of automatic acquisition of semantic resources exploiting Wikipedia data. More precisely, we exploit the graph of parent categories linked to each Wikipedia page to extract the semantically and thematically related parent categories. This extraction is the result of a shortest path length calculus applied to the global lattice of Wikipedia categories. So, each page can be projected within its first level categories, and in addition their parent categories. This resource has been used for two kinds of applications. The first one concerns the indexation and classification of Wikipedia pages. The second one concerns a disambiguation task applied to a query translator for cross lingual search engine. This last work has been performed by using English categories lattice.},
  motscles  = {Wikipedia, plus court chemin, désambiguïsation, classification, traduction de requête},
  keywords  = {Wikipedia, shortest path, disambiguation, classification, query translation},
}

@inproceedings{danlos-sagot:2010:TALN,
  author    = {Danlos, Laurence and Sagot, Benoît},
  title     = {Ponctuations fortes abusives},
  booktitle = {Actes de la 17e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {July},
  year      = {2010},
  address   = {Montréal, Canada},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2010/taln-2010-court-005},
  language  = {french},
  resume    = {Certaines ponctuations fortes sont « abusivement » utilisées à la place de ponctuations faibles, débouchant sur des phrases graphiques qui ne sont pas des phrases grammaticales. Cet article présente une étude sur corpus de ce phénomène et une ébauche d’outil pour repérer automatiquement les ponctuations fortes abusives.},
  abstract  = {Some strong punctuation signs are “wrongly” used instead of weak punctuation signs, leading to graphic sentences which are not grammatical sentences. This paper presents a corpus study of this phenomenon and a tool in the early stages to automatically detect wrong strong punctuation signs.},
  motscles  = {pseudo-phrase, phrase averbale, analyse syntaxique et sémantique},
  keywords  = {pseudo-sentence, verbless utterance, syntactic and semantic analysis},
}

@inproceedings{moriceau-tannier-falco:2010:TALN,
  author    = {Moriceau, Véronique and Tannier, Xavier and Falco, Mathieu},
  title     = {Une étude des questions “complexes” en question-réponse},
  booktitle = {Actes de la 17e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {July},
  year      = {2010},
  address   = {Montréal, Canada},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2010/taln-2010-court-006},
  language  = {french},
  resume    = {La plupart des systèmes de question-réponse ont été conçus pour répondre à des questions dites “factuelles” (réponses précises comme des dates, des lieux), et peu se sont intéressés au traitement des questions complexes. Cet article présente une typologie des questions en y incluant les questions complexes, ainsi qu’une typologie des formes de réponses attendues pour chaque type de questions. Nous présentons également des expériences préliminaires utilisant ces typologies pour les questions complexes, avec de bons résultats.},
  abstract  = {Most question-answering systems have been designed to answer “factual” questions (short and precise answers as dates, locations), and only a few researches concern complex questions. This article presents a typology of questions, including complex questions, as well as a typology of answers that should be expected for each type of questions. We also present preliminary experiments using these typologies for answering complex questions and leading to good results.},
  motscles  = {Système de question-réponse, questions complexes},
  keywords  = {Question-answering systems, complex questions},
}

@inproceedings{malik-EtAl:2010:TALN,
  author    = {Malik, Muhammad Ghulam Abbas and Boitet, Christian and Bhattacharyya, Pushpak and Besacier, Laurent},
  title     = {},
  booktitle = {Actes de la 17e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {July},
  year      = {2010},
  address   = {Montréal, Canada},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2010/taln-2010-court-007},
  language  = {french},
  note      = {Weak Translation Problems – a case study of Scriptural Translation},
  resume    = {La TA généraliste de haute qualité et totalement automatique est considérée comme impossible. Nous nous intéressons aux problèmes de traduction scripturale, qui sont des sous-problèmes faibles du problème général de la traduction. Nous présentons les caractéristiques des problèmes faibles de traduction et les problèmes de traduction scripturale, décrivons différentes approches computationnelles (à états finis, statistiques, et hybrides) et présentons nos résultats sur différentes combinaisons de langues et systèmes d’écriture Indo-Pak.},
  abstract  = {General purpose, high quality and fully automatic MT is believed to be impossible. We are interested in scriptural translation problems, which are weak sub-problems of the general problem of translation. We introduce the characteristics of the weak problems of translation and of the scriptural translation problems, describe different computational approaches (finite-state, statistical and hybrid) to solve these problems, and report our results on several combinations of Indo-Pak languages and writing systems.},
  motscles  = {problèmes faibles de traduction, traduction scripturale, traduction interdialectal, transcriptions, translittérations},
  keywords  = {weak problems of translation, scriptural translation, interdialectal translation, transcription, transliteration},
}

@inproceedings{ataaallah-boulaknadel:2010:TALN,
  author    = {Ataa Allah, Fadoua and Boulaknadel, Siham},
  title     = {Pseudo-racinisation de la langue amazighe},
  booktitle = {Actes de la 17e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {July},
  year      = {2010},
  address   = {Montréal, Canada},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2010/taln-2010-court-008},
  language  = {french},
  resume    = {Dans le cadre de la promotion de la langue amazighe, nous avons voulu lui apporter des ressources et outils linguistiques pour son traitement automatique et son intégration dans le domaine des nouvelles technologies de l'information et de la communication. Partant de ce principe, nous avons opté, au sein de l’Institut Royal de la Culture Amazighe, pour une démarche innovante de réalisations progressives de ressources linguistiques et d’outils de base de traitement automatique, qui permettront de préparer le terrain pour d’éventuelles recherches scientifiques. Dans cette perspective, nous avons entrepris de développer, dans un premier temps, un outil de pseudoracinisation basé sur une approche relevant du cas de la morphologie flexionnelle et reposant sur l’élimination d’une liste de suffixes et de préfixes de la langue amazighe. Cette approche permettra de regrouper les mots sémantiquement proches à partir de ressemblances afin d’être exploités dans des applications tel que la recherche d’information et la classification.},
  abstract  = {In the context of promoting the Amazigh language, we would like to provide this language with linguistic resources and tools in the aim to enable its automatic processing and its integration in the field of Information and Communication Technology. Thus, we have opted, in the Royal Institute of Amazigh Culture, for an innovative approach of progressive realizations of linguistic resources and basic natural language processing tools that will pave the way for further scientific researches. In this perspective, we are trying initially to develop a light stemmer based on an approach dealing with inflectional morphology, and on stripping a list of Amazigh suffixes and prefixes. This approach will conflate word variants into a common stem that will be used in many applications such as information retrieval and classification.},
  motscles  = {Langue amazighe, Pseudo-racinisation, Morphologie flexionnelle},
  keywords  = {Amazigh language, Light stemming, Inflectional morphology},
}

@inproceedings{chaumartin-kahane:2010:TALN,
  author    = {Chaumartin, François-Régis and Kahane, Sylvain},
  title     = {Une approche paresseuse de l’analyse sémantique ou comment construire une interface syntaxe-sémantique à partir d’exemples},
  booktitle = {Actes de la 17e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {July},
  year      = {2010},
  address   = {Montréal, Canada},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2010/taln-2010-court-009},
  language  = {french},
  resume    = {Cet article montre comment calculer une interface syntaxe-sémantique à partir d’un analyseur en dépendance quelconque et interchangeable, de ressources lexicales variées et d’une base d’exemples associés à leur représentation sémantique. Chaque exemple permet de construire une règle d’interface. Nos représentations sémantiques sont des graphes hiérarchisés de relations prédicat-argument entre des acceptions lexicales et notre interface syntaxe-sémantique est une grammaire de correspondance polarisée. Nous montrons comment obtenir un système très modulaire en calculant certaines règles par « soustraction » de règles moins modulaires.},
  abstract  = {This article shows how to extract a syntax-semantics interface starting from an interchangeable dependency parser, various lexical resources and from samples associated with their semantic representations. Each example allows us to build an interface rule. Our semantic representations are hierarchical graphs of predicate-argument relations between lexical meanings and our syntax-semantics interface is a polarized unification grammar. We show how to obtain a very modular system by computing some rules by “subtraction” of less modular rules.},
  motscles  = {Interface syntaxe-sémantique, graphe sémantique, grammaires de dépendance, GUP (Grammaire d’unification polarisée), GUST (Grammaire d’unification Sens-Texte)},
  keywords  = {Syntax-semantics Interface, Semantic Graph, Dependency Grammar, PUG (Polarized Unification Grammar), MTUG (Meaning-Text Unification Grammar)},
}

@inproceedings{russo:2010:TALN,
  author    = {Russo, Lorenza},
  title     = {La traduction automatique des pronoms clitiques. Quelle approche pour quels résultats?},
  booktitle = {Actes de la 17e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {July},
  year      = {2010},
  address   = {Montréal, Canada},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2010/taln-2010-court-010},
  language  = {french},
  resume    = {Dans cet article, nous abordons la problématique de la traduction automatique des pronoms clitiques, en nous focalisant sur la traduction de l’italien vers le français et en comparant les résultats obtenus par trois systèmes : Its-2, développé au LATL (Laboratoire d’Analyse et de Technologie du Langage) et basé sur un analyseur syntaxique profond ; Babelfish, basé sur des règles linguistiques ; et Google Translate, caractérisé par une approche statistique.},
  abstract  = {In this article, we discuss the problem of automatic translation of clitic pronouns, focalysing our attention on the translation from Italian to French and comparing the results obtained by three MT systems : Its-2, developed at LATL (Language Technology Laboratory) and based on a syntactic parser ; Babelfish, a rule-based system ; and Google Translate, caracterised by a statistical approach.},
  motscles  = {Analyseur syntaxique, traduction automatique, pronoms clitiques, proclise, enclise},
  keywords  = {Syntactic parser, automatic translation, clitic pronouns, proclisis, enclisis},
}

@inproceedings{morlanehondere-fabre:2010:TALN,
  author    = {Morlane-Hondère, François and Fabre, Cécile},
  title     = {L’antonymie observée avec des méthodes de TAL : une relation à la fois syntagmatique et paradigmatique ?},
  booktitle = {Actes de la 17e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {July},
  year      = {2010},
  address   = {Montréal, Canada},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2010/taln-2010-court-011},
  language  = {french},
  resume    = {Cette étude utilise des outils de TAL pour tester l’hypothèse avancée par plusieurs études linguistiques récentes selon laquelle la relation antonymique, classiquement décrite comme une relation paradigmatique, a la particularité de fonctionner également sur le plan syntagmatique, c’est-à-dire de réunir des mots qui sont non seulement substituables mais qui apparaissent également régulièrement dans des relations contextuelles. Nous utilisons deux méthodes – l’analyse distributionnelle pour le plan paradigmatique, la recherche par patrons antonymiques pour le plan syntagmatique. Les résultats montrent que le diagnostic d’antonymie n’est pas significativement meilleur lorsqu’on croise les deux méthodes, puisqu’une partie des antonymes identifiés ne répondent pas au test de substituabilité, ce qui semble confirmer la prépondérance du plan syntagmatique pour l’étude et l’acquisition de cette relation.},
  abstract  = {In this paper, we use NLP methods to test the hypothesis, suggested by several linguistic studies, that antonymy is not only a paradigmatic but also a syntagmatic relation : antonym pairs, that have been classically described by their ability to be substituted for each other, also tend to frequently co-occur in texts. We use two methods – distributional analysis on the paradigmatic level, lexico-syntactic pattern recognition on the syntagmatic level. Results show that antonym detection is not significantly improved by combining the two methods : a set of antonyms do not satisfy the test for substitutability, which tends to confirm the predominance of the syntagmatic level for studying and identifying antonymy.},
  motscles  = {sémantique lexicale, antonymie, analyse distributionnelle, patrons lexico-syntaxiques},
  keywords  = {lexical semantics, antonymy, distributional analysis, lexico-grammatical patterns},
}

@inproceedings{kuznik-EtAl:2010:TALN,
  author    = {Kuznik, Ludivine and Guénet, Anne-Laure and Peradotto, Anne and Clavel, Chloé},
  title     = {L’apport des concepts métiers pour la classification des questions ouvertes d’enquête},
  booktitle = {Actes de la 17e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {July},
  year      = {2010},
  address   = {Montréal, Canada},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2010/taln-2010-court-012},
  language  = {french},
  resume    = {EDF utilise les techniques de Text Mining pour optimiser sa relation client, en analysant des réponses aux questions ouvertes d'enquête de satisfaction, et des retranscriptions de conversations issues des centres d'appels. Dans cet article, nous présentons les différentes contraintes applicatives liées à l’utilisation d’outils de text mining pour l’analyse de données clients. Après une analyse des différents outils présents sur le marché, nous avons identifié la technologie Skill CartridgeTM fournie par la société TEMIS comme la plus adaptée à nos besoins. Cette technologie nous permet une modélisation sémantique de concepts liés au motif d’insatisfaction. L’apport de cette modélisation est illustrée pour une tâche de classification de réponses d’enquêtes de satisfaction chargée d’évaluer la fidélité des clients EDF. La modélisation sémantique a permis une nette amélioration des scores de classification (F-mesure = 75,5%) notamment pour les catégories correspondant à la satisfaction et au mécontentement.},
  abstract  = {The French power supply company EDF uses text mining tools to improve customer insight by analysing satisfaction inquiries or transcriptions of call-centre conversations. In this paper, we present the various application needs for text mining tools. After an analysis of the various existing industrial tools, we identify the Skill Cartridge tool provided by TEMIS company as the more relevant to our needs. This tool offers the capability to model expressions linked to reason for satisfaction/dissatisfaction. The contribution of this modelling is illustrated here for the classification of satisfaction inquiries dedicated to the evaluation of customer loyalty. The semantic models provide a marked improvement of classification scores (F-mesure = 75.5%) for the satisfaction/dissatisfaction categories in particular.},
  motscles  = {outils de text mining, modélisation de concepts métier, classification supervisée},
  keywords  = {text mining tools, business concept modelling, supervised classification},
}

@inproceedings{dejean-EtAl:2010:TALN,
  author    = {Dejean, Charles and Fortun, Manoel and Massot, Clotilde and Pottier, Vincent and Poulard, Fabien and Vernier, Matthieu},
  title     = {Un étiqueteur de rôles grammaticaux libre pour le français intégré à Apache UIMA},
  booktitle = {Actes de la 17e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {July},
  year      = {2010},
  address   = {Montréal, Canada},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2010/taln-2010-court-013},
  language  = {french},
  resume    = {L’étiquetage des rôles grammaticaux est une tâche de pré-traitement récurrente. Pour le français, deux outils sont majoritairement utilisés : TreeTagger et Brill. Nous proposons une démarche, ne nécessitant aucune ressource, pour la création d’un modèle de Markov caché (HMM) pour palier les problèmes de ces outils, et de licences notamment. Nous distribuons librement toutes les ressources liées à ce travail.},
  abstract  = {Part-of-speech tagging is a common preprocessing task. For the French language, Brill and TreeTagger are the most often used tools. We propose a method, requiring no resource, to create a Hidden Markov Model to get rid of the problems and licences of these tools. We freely distribute all the resources related to this work.},
  motscles  = {étiquetage grammatical, Modèle de Markov caché, UIMA, Brill, TreeTagger},
  keywords  = {grammatical tagging, Hidden Markov Model, UIMA, Brill, TreeTagger},
}

@inproceedings{genereux-marquilhas-hendrickx:2010:TALN,
  author    = {Généreux, Michel and Marquilhas, Rita and Hendrickx, Iris},
  title     = {Segmentation Automatique de Lettres Historiques},
  booktitle = {Actes de la 17e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {July},
  year      = {2010},
  address   = {Montréal, Canada},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2010/taln-2010-court-014},
  language  = {french},
  resume    = {Cet article présente une approche basée sur la comparaison fréquentielle de modèles lexicaux pour la segmentation automatique de textes historiques Portugais. Cette approche traite d’abord le problème de la segmentation comme un problème de classification, en attribuant à chaque élément lexical présent dans la phase d’apprentissage une valeur de saillance pour chaque type de segment. Ces modèles lexicaux permettent à la fois de produire une segmentation et de faire une analyse qualitative de textes historiques. Notre évaluation montre que l’approche adoptée permet de tirer de l’information sémantique que des approches se concentrant sur la détection des frontières séparant les segments ne peuvent acquérir.},
  abstract  = {This article presents an approach based on the frequency comparison of lexical models for the automatic segmentation of historical texts. This approach first addresses the problem of segmentation as a classification problem by assigning each token present in the learning phase a value of salience for each type of segment. These lexical patterns can both produce a segmentation and make possible a qualitative analysis of historical texts. Our evaluation shows that the approach can extract semantic information that approaches focusing on the detection of boundaries between segments cannot capture.},
  motscles  = {Corpus comparables, Saillance, Segmentation, Textes historiques},
  keywords  = {Comparable corpora, Salience, Segmentation, Historical Texts},
}

@inproceedings{blancafort-EtAl:2010:TALN,
  author    = {Blancafort, Helena and Recourcé, Gaëlle and Couto, Javier and Sagot, Benoît and Stern, Rosa and Teyssou, Denis},
  title     = {Traitement des inconnus : une approche systématique de l’incomplétude lexicale},
  booktitle = {Actes de la 17e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {July},
  year      = {2010},
  address   = {Montréal, Canada},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2010/taln-2010-court-015},
  language  = {french},
  resume    = {Cet article aborde le phénomène de l’incomplétude des ressources lexicales, c’est-à-dire la problématique des inconnus, dans un contexte de traitement automatique. Nous proposons tout d’abord une définition opérationnelle de la notion d’inconnu. Nous décrivons ensuite une typologie des différentes classes d’inconnus, motivée par des considérations linguistiques et applicatives ainsi que par l’annotation des inconnus d’un petit corpus selon notre typologie. Cette typologie sera mise en oeuvre et validée par l’annotation d’un corpus important de l’Agence France-Presse dans le cadre du projet EDyLex.},
  abstract  = {This paper addresses the incompleteness of lexical resources, i.e., the problem of unknown words, in the context of natural language processing. First, we put forward an operational definition of the notion of unknown words. Next, we describe a typology of the various classes of unknown words, motivated by linguistic and applicative considerations as well as the annotation of unknown words in a small-scale corpus w.r.t. our typology. This typology shall be applied and validated through the annotation of a large corpus from the Agence France-Presse as part of the EDyLex project.},
  motscles  = {mots inconnus, incomplétude lexicale, acquisition dynamique des ressources lexicales},
  keywords  = {unknown words, lexical incompleteness, dynamic acquisition of lexical information},
}

@inproceedings{jacquey-EtAl:2010:TALN,
  author    = {Jacquey, Évelyne and Kister, Laurence and Grzesitchak, Mick and Gaiffe, Bertrand and Reutenauer, Coralie and Ollinger, Sandrine and Valette, Mathieu},
  title     = {Thésaurus et corpus de spécialité sciences du langage : approches lexicométriques appliquées à l’analyse de termes en corpus},
  booktitle = {Actes de la 17e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {July},
  year      = {2010},
  address   = {Montréal, Canada},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2010/taln-2010-court-016},
  language  = {french},
  resume    = {Cet article s'inscrit dans les recherches sur l'exploitation de ressources terminologiques pour l'analyse de textes de spécialité, leur annotation et leur indexation. Les ressources en présence sont, d'une part, un thesaurus des Sciences du Langage, le Thesaulangue et, d'autre part, un corpus d’échantillons issus de cinq ouvrages relevant du même domaine. L'article a deux objectifs. Le premier est de déterminer dans quelle mesure les termes de Thesaulangue sont représentés dans les textes. Le second est d'évaluer si les occurrences des unités lexicales correspondant aux termes de Thesaulangue relèvent majoritairement d'emplois terminologiques ou de langue courante. A cette fin, les travaux présentés utilisent une mesure de richesse lexicale telle qu'elle a été définie par Brunet (rapporté dans Muller, 1992) dans le domaine de la lexicométrie, l'indice W. Cette mesure est adaptée afin de mesurer la richesse terminologie (co-occurrents lexicaux et sémantiques qui apparaissent dans Thesaulangue).},
  abstract  = {This article aims to contribute to the field of the exploitation of terminological resources for the analysis of technical and scientific texts, their annotation and their indexation. The available resources are on one hand a thesaurus, Thesaulangue, which deals with Linguistics, and on the other hand, a corpus made of samples extracted from five books about Linguistics. More precisely, the article has two goals: first, studying how to determine which terms of Thesaulangue occur in texts. Second, attempting to measure if the lexical units which correspond to terms of Thesaulangue are used in texts in a terminological way or not. In this perspective, the presented work uses and adapts the Brunet’s W-index designed in the area of lexicometry.},
  motscles  = {sémantique lexicale, terminologie, corpus, richesse lexicale, lexicométrie},
  keywords  = {lexical semantics, terminology, corpora, lexical richness, lexicometry},
}

@inproceedings{letallec-EtAl:2010:TALN,
  author    = {Le Tallec, Marc and Villaneau, Jeanne and Antoine, Jean-Yves and Savary, Agata and Syssau-Vaccarella, Arielle},
  title     = {Détection hors contexte des émotions à partir du contenu linguistique d’énoncés oraux : le système EmoLogus},
  booktitle = {Actes de la 17e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {July},
  year      = {2010},
  address   = {Montréal, Canada},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2010/taln-2010-court-017},
  language  = {french},
  resume    = {Le projet EmotiRob, soutenu par l'agence nationale de la recherche, s'est donné pour objectif de détecter des émotions dans un contexte d'application original : la réalisation d'un robot compagnon émotionnel pour des enfants fragilisés. Nous présentons dans cet article le système qui caractérise l'émotion induite par le contenu linguistique des propos de l'enfant. Il se base sur un principe de compositionnalité des émotions, avec une valeur émotionnelle fixe attribuée aux mots lexicaux, tandis que les verbes et les adjectifs agissent comme des fonctions dont le résultat dépend de la valeur émotionnelle de leurs arguments. L'article présente la méthode de calcul utilisée, ainsi que la norme lexicale émotionnelle correspondante. Une analyse quantitative et qualitative des premières expérimentations présente les différences entre les sorties du module de détection et l'annotation d'experts, montrant des résultats satisfaisants, avec la bonne détection de la valence émotionnelle dans plus de 90% des cas.},
  abstract  = {The ANR Emotirob project aims at detecting emotions in an original application context: realizing an emotional companion robot for weakened children. This paper presents a system which aims at characterizing emotions by only considering linguistic content. It is based on the assumption that emotions can be compound: simple lexical words have an intrinsic emotional value, while verbal and adjectival predicates act as a function on the emotional values of their arguments. The paper describes the algorithm of compositional computation of the emotion and the lexical emotional norm used by this algorithm. A quantitative and qualitative analysis of the differences between system outputs and expert annotations is given, which shows satisfactory results, with a good detection of emotional valency in 90.0% of the test utterances.},
  motscles  = {Emotion, valence émotionnelle, norme lexicale émotionnelle, robot compagnon, compréhension de parole},
  keywords  = {Emotion, Emotional valency, Emotional lexical standard, companion robot, spoken language understanding},
}

@inproceedings{bouamor-max-vilnat:2010:TALN,
  author    = {Bouamor, Houda and Max, Aurélien and Vilnat, Anne},
  title     = {Acquisition de paraphrases sous-phrastiques depuis des paraphrases d’énoncés},
  booktitle = {Actes de la 17e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {July},
  year      = {2010},
  address   = {Montréal, Canada},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2010/taln-2010-court-018},
  language  = {french},
  resume    = {Dans cet article, nous présentons la tâche d’acquisition de paraphrases sous-phrastiques (impliquant des paires de mots ou de groupes de mots), et décrivons plusieurs techniques opérant à différents niveaux. Nous décrivons une évaluation visant à comparer ces techniques et leurs combinaisons sur deux corpus de paraphrases d’énoncés obtenus par traduction multiple. Les conclusions que nous tirons peuvent servir de guide pour améliorer des techniques existantes.},
  abstract  = {In this article, the task of acquiring sub-sentential paraphrases (word or phrase pairs) is discussed and several automatic techniques operating at different levels are presented. We describe an evaluation methodology to compare these techniques and their combination that is applied on two corpora of sentential paraphrases obtained by multiple translation. The conclusions that are drawn can be used to guide future work for improving existing techniques.},
  motscles  = {Paraphrase, Patrons de correspondances de segments monolingues},
  keywords  = {Paraphrase, Monolingual bi-phrase patterns},
}

@inproceedings{mouton-dechalendar:2010:TALN,
  author    = {Mouton, Claire and de Chalendar, Gaël},
  title     = {JAWS : Just Another WordNet Subset},
  booktitle = {Actes de la 17e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {July},
  year      = {2010},
  address   = {Montréal, Canada},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2010/taln-2010-court-019},
  language  = {french},
  resume    = {WordNet, une des ressources lexicales les plus utilisées aujourd’hui a été constituée en anglais et les chercheurs travaillant sur d’autres langues souffrent du manque d’une telle ressource. Malgré les efforts fournis par la communauté française, les différents WordNets produits pour la langue française ne sont toujours pas aussi exhaustifs que le WordNet de Princeton. C’est pourquoi nous proposons une méthode novatrice dans la production de termes nominaux instanciant les différents synsets de WordNet en exploitant les propriétés syntaxiques distributionnelles du vocabulaire français. Nous comparons la ressource que nous obtenons avecWOLF et montrons que notre approche offre une couverture plus large.},
  abstract  = {WordNet, one of the most used lexical resource until today has been made up for the English language and scientists working on other languages suffer from the lack of such a resource. Despite the efforts performed by the French community, the differentWordNets produced for the French language are still not as exhaustive as the original Princeton WordNet. We propose a new approach in the way of producing nominal terms filling the synset slots. We use syntactical distributional properties of French vocabulary to determine which of the candidates given by a bilingual dictionary matches the best. We compare the resource we obtain withWOLF and show that our approach provides a much larger coverage.},
  motscles  = {ressources lexicales françaises, WordNet, relations sémantiques, distributions syntaxiques},
  keywords  = {French lexical resources, WordNet, semantic relations, syntactical distributionality},
}

@inproceedings{brun-ehrmann:2010:TALN,
  author    = {Brun, Caroline and Ehrmann, Maud},
  title     = {Un système de détection d’entités nommées adapté pour la campagne d’évaluation ESTER 2},
  booktitle = {Actes de la 17e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {July},
  year      = {2010},
  address   = {Montréal, Canada},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2010/taln-2010-court-020},
  language  = {french},
  resume    = {Dans cet article nous relatons notre participation à la campagne d’évaluation ESTER 2 (Evaluation des Systèmes de Transcription Enrichie d’Emissions Radiophoniques). Après avoir décrit les objectifs de cette campagne ainsi que ses spécificités et difficultés, nous présentons notre système d’extraction d’entités nommées en nous focalisant sur les adaptations réalisées dans le cadre de cette campagne. Nous décrivons ensuite les résultats obtenus lors de la compétition, ainsi que des résultats originaux obtenus par la suite. Nous concluons sur les leçons tirées de cette expérience.},
  abstract  = {In this paper, we report our participation to the ESTER 2 (Evaluation des Systèmes de Transcription Enrichie d’Emissions Radiophoniques) evaluation campaign. After describing the goals, specificities and challenges of the campaign, we present our named entity detection system and focus on the adaptations made in the framework of the campaign. We present the results obtained during the competition and then new results obtained afterward. We then conclude by the lessons we learned from this experiment.},
  motscles  = {entités nommées, évaluation, extraction d’information},
  keywords  = {named entities, evaluation, information extraction},
}

@inproceedings{muller-langlais:2010:TALN,
  author    = {Muller, Philippe and Langlais, Philippe},
  title     = {Comparaison de ressources lexicales pour l’extraction de synonymes},
  booktitle = {Actes de la 17e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {July},
  year      = {2010},
  address   = {Montréal, Canada},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2010/taln-2010-court-021},
  language  = {french},
  resume    = {},
  abstract  = {},
  motscles  = {},
  keywords  = {},
}

@inproceedings{longo-todirascu:2010:TALN,
  author    = {Longo, Laurence and Todiraşcu, Amalia},
  title     = {RefGen : un module d’identification des chaînes de référence dépendant du genre textuel},
  booktitle = {Actes de la 17e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {July},
  year      = {2010},
  address   = {Montréal, Canada},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2010/taln-2010-court-022},
  language  = {french},
  resume    = {Dans cet article, nous présentons RefGen, un module d’identification des chaînes de référence pour le français. RefGen effectue une annotation automatique des expressions référentielles puis identifie les relations de coréférence établies entre ces expressions pour former des chaînes de référence. Le calcul de la référence utilise des propriétés des chaînes de référence dépendantes du genre textuel, l’échelle d’accessibilité d’(Ariel, 1990) et une série de filtres lexicaux, morphosyntaxiques et sémantiques. Nous évaluons les premiers résultats de RefGen sur un corpus issu de rapports publics.},
  abstract  = {We present RefGen, a reference chain identification module for French. RefGen automatically annotates referential expressions then identifies coreference relations between these expressions to make reference chains. Reference calculus uses textual genre specific properties of reference chains, (Ariel, 1990)’s accessibility theory and applies lexical, morphosyntactic and semantic filters. We evaluate the first results obtained by RefGen from a public reports corpus.},
  motscles  = {Chaînes de référence, relation de coréférence, saillance, genre textuel},
  keywords  = {Reference chain identification, coreference relation, salience, genre},
}

@inproceedings{stern-sagot:2010:TALN,
  author    = {Stern, Rosa and Sagot, Benoît},
  title     = {Détection et résolution d’entités nommées dans des dépêches d’agence},
  booktitle = {Actes de la 17e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {July},
  year      = {2010},
  address   = {Montréal, Canada},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2010/taln-2010-court-023},
  language  = {french},
  resume    = {Nous présentons NP, un système de reconnaissance d’entités nommées. Comprenant un module de résolution, il permet d’associer à chaque occurrence d’entité le référent qu’elle désigne parmi les entrées d’un référentiel dédié. NP apporte ainsi des informations pertinentes pour l’exploitation de l’extraction d’entités nommées en contexte applicatif. Ce système fait l’objet d’une évaluation grâce au développement d’un corpus annoté manuellement et adapté aux tâches de détection et de résolution.},
  abstract  = {We introduce NP, a system for named entity recognition. It includes a resolution module for linking each entity occurrence to its matching entry in a dedicated reference base. NP thus brings information relevant for using named entity extraction in an applicative context. We have evaluated NP by the means of a manually annotated corpus designed for the tasks of recognition and resolution.},
  motscles  = {résolution d’entités nommées, détection d’entités nommées, extraction d’information},
  keywords  = {named entity resolution, named entity recognition, information extraction},
}

@inproceedings{meurs-lefevre:2010:TALN,
  author    = {Meurs, Marie-Jean and Lefèvre, Fabrice},
  title     = {Processus de décision à base de SVM pour la composition d’arbres de frames sémantiques},
  booktitle = {Actes de la 17e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {July},
  year      = {2010},
  address   = {Montréal, Canada},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2010/taln-2010-court-024},
  language  = {french},
  resume    = {Cet article présente un processus de décision basé sur des classifieurs à vaste marge (SVMDP) pour extraire l’information sémantique dans un système de dialogue oral. Dans notre composant de compréhension, l’information est représentée par des arbres de frames sémantiques définies selon le paradigme FrameNet. Le processus d’interprétation est réalisé en deux étapes. D’abord, des réseaux bayésiens dynamiques (DBN) sont utilisés comme modèles de génération pour inférer des fragments d’arbres de la requête utilisateur. Ensuite, notre SVMDP dépendant du contexte compose ces fragments afin d’obtenir la représentation sémantique globale du message. Les expériences sont menées sur le corpus de dialogue MEDIA. Une procédure semi-automatique fournit une annotation de référence en frames sur laquelle les paramètres des DBN et SVMDP sont appris. Les résultats montrent que la méthode permet d’améliorer les performances d’identification de frames pour les exemples de test les plus complexes par rapport à un processus de décision déterministe ad hoc.},
  abstract  = {This paper presents a decision process based on Support Vector Machines to extract the semantic information from the user’s input in a spoken dialog system. In our interpretation component, the information is represented by means of trees of semantic frames, as defined in the Berkeley FrameNet paradigm, and the understanding process is performed in two steps. First Dynamic Bayesian Networks are used as generative models to sequentially infer tree fragments from the users’ inputs. Then the contextsensitive SVMDP introduced in this paper is applied to detect the relations between the frames hypothesized in the fragments and compose them to obtain the overall semantic representation of the user’s request. Experiments are reported on the French MEDIA dialogue corpus. A semi-automatic process provides a reference frame annotation of the speech training data. The parameters of DBNs and SVMDP are learned from these data. The method is shown to outperform an ad-hoc deterministic decision process on the most complex test examples for frame identification.},
  motscles  = {système de dialogue oral, compréhension de la parole, composition sémantique, frame sémantique, séparateur à vaste marge},
  keywords  = {spoken dialogue system, spoken language understanding, semantic composition, semantic frame, support vector machines},
}

@inproceedings{arnulphy-tannier-vilnat:2010:TALN,
  author    = {Arnulphy, Béatrice and Tannier, Xavier and Vilnat, Anne},
  title     = {Les entités nommées événement et les verbes de cause-conséquence},
  booktitle = {Actes de la 17e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {July},
  year      = {2010},
  address   = {Montréal, Canada},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2010/taln-2010-court-025},
  language  = {french},
  resume    = {L’extraction des événements désignés par des noms est peu étudiée dans des corpus généralistes. Si des lexiques de noms déclencheurs d’événements existent, les problèmes de polysémie sont nombreux et beaucoup d’événements ne sont pas introduits par des déclencheurs. Nous nous intéressons dans cet article à une hypothèse selon laquelle les verbes induisant la cause ou la conséquence sont de bons indices quant à la présence d’événements nominaux dans leur cotexte.},
  abstract  = {Few researches focus on nominal event extraction in open-domain corpora. Lists of cue words for events exist, but raise many problems of polysemy. In this article, we focus on the following hypothesis : verbs introducing cause or consequence links have good chances to have an event noun around them.},
  motscles  = {Entité nommée, événement, rapports de cause et conséquence},
  keywords  = {Named entity, event, cause and consequence links},
}

@inproceedings{pak-paroubek:2010:TALN,
  author    = {Pak, Alexander and Paroubek, Patrick},
  title     = {Construction d’un lexique affectif pour le français à partir de Twitter},
  booktitle = {Actes de la 17e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {July},
  year      = {2010},
  address   = {Montréal, Canada},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2010/taln-2010-court-026},
  language  = {french},
  resume    = {Un lexique affectif est un outil utile pour l’étude des émotions ainsi que pour la fouille d’opinion et l’analyse des sentiments. Un tel lexique contient des listes de mots annotés avec leurs évaluations émotionnelles. Il existe un certain nombre de lexiques affectifs pour la langue anglaise, espagnole, allemande, mais très peu pour le français. Un travail de longue haleine est nécessaire pour construire et enrichir un lexique affectif. Nous proposons d’utiliser Twitter, la plateforme la plus populaire de microblogging de nos jours, pour recueillir un corpus de textes émotionnels en français. En utilisant l’ensemble des données recueillies, nous avons estimé les normes affectives de chaque mot. Nous utilisons les données de la Norme Affective desMots Anglais (ANEW, Affective Norms of EnglishWords) que nous avons traduite en français afin de valider nos résultats. Les valeurs du coefficient tau de Kendall et du coefficient de corrélation de rang de Spearman montrent que nos scores estimés sont en accord avec les scores ANEW.},
  abstract  = {Affective lexicons are a useful tool for emotion studies as well as for opinion mining and sentiment analysis. Such lexicons contain lists of words annotated with their emotional assessments. There exist a number of affective lexicons for English, Spanish, German and other languages. However, only a few of such resources are available for French. A lot of human efforts are needed to build and extend an affective lexicon. We propose to use Twitter, the most popular microblogging platform nowadays, to collect a dataset of emotional texts in French. Using the collected dataset, we estimated the affective norms of words present in our corpus. We used the dataset of Affective Norms of English Words (ANEW) that we translated into French to validate our results. Values of Kendall’s tau coefficient and Spearman’s rank correlation coefficient show that our estimated scores correlate well with the ANEW scores.},
  motscles  = {Analyse de sentiments, ANEW, Twitter},
  keywords  = {Sentiment analysis, ANEW, Twitter},
}

@inproceedings{zhang-ferrari:2010:TALN,
  author    = {Zhang, Lei and Ferrari, Stéphane},
  title     = {Analyse d’opinion : annotation sémantique de textes chinois},
  booktitle = {Actes de la 17e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {July},
  year      = {2010},
  address   = {Montréal, Canada},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2010/taln-2010-court-027},
  language  = {french},
  resume    = {Notre travail concerne l’analyse automatique des énoncés d’opinion en chinois. En nous inspirant de la théorie linguistique de l’Appraisal, nous proposons une méthode fondée sur l’usage de lexiques et de règles locales pour déterminer les caractéristiques telles que la Force (intensité), le Focus (prototypicalité) et la polarité de tels énoncés. Nous présentons le modèle et sa mise en oeuvre sur un corpus journalistique. Si pour la détection d’énoncés d’opinion, la précision est bonne (94 %), le taux de rappel (67 %) pose cependant des questions sur l’enrichissement des ressources actuelles.},
  abstract  = {Our work concerns automatic analysis of opinion in texts. Based on the Appraisal linguistic theory, our method uses lexical and syntactic resources to process such properties as the Force, the Focus and the polarity of an opinion. We present our model and its implementation on a journalistic corpus. The precision for detecting opinion expressions is high (94%), but the recall (67%) raises the question of how to enhance the resources.},
  motscles  = {Analyse d’opinion, théorie de l’Appraisal},
  keywords  = {Opinion analysis, Appraisal theory},
}

@inproceedings{cellier-charnois:2010:TALN,
  author    = {Cellier, Peggy and Charnois, Thierry},
  title     = {Fouille de données séquentielles d’itemsets pour l’apprentissage de patrons linguistiques},
  booktitle = {Actes de la 17e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {July},
  year      = {2010},
  address   = {Montréal, Canada},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2010/taln-2010-court-028},
  language  = {french},
  resume    = {Dans cet article nous présentons une méthode utilisant l’extraction de motifs séquentiels d’itemsets pour l’apprentissage automatique de patrons linguistiques. De plus, nous proposons de nous appuyer sur l’ordre partiel existant entre les motifs pour les énumérer de façon structurée et ainsi faciliter leur validation en tant que patrons linguistiques.},
  abstract  = {In this paper, we present a method based on the extraction of itemset sequential patterns in order to automatically generate linguistic patterns. In addition, we propose to use the partial ordering between sequential patterns to enumerate and validate them.},
  motscles  = {Fouille de données, motifs séquentiels, extraction d’information, apprentissage de patrons linguistiques},
  keywords  = {Data mining, sequential patterns, information extraction, linguistic pattern learning},
}

@inproceedings{benhassena-miclet:2010:TALN,
  author    = {Ben Hassena, Anouar and Miclet, Laurent},
  title     = {},
  booktitle = {Actes de la 17e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {July},
  year      = {2010},
  address   = {Montréal, Canada},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2010/taln-2010-court-029},
  language  = {french},
  note      = {Tree analogical learning. Application in NLP},
  resume    = {En intelligence artificielle, l’analogie est utilisée comme une technique de raisonnement non exact pour la résolution de problèmes, la compréhension du langage naturel, l’apprentissage des règles de classification, etc. Cet article s’intéresse à la proportion analogique, une forme simple du raisonnement par analogie, et présente son application en apprentissage automatique pour le TALN. La proportion analogique est une relation entre quatre objets qui exprime que la manière de transformer le premier objet en le second est la même que la façon de transformer le troisième en le quatrième. Premièrement, nous définissons formellement la proportion analogique entre quatre objets. Nous nous intéressons particulièrement aux objets structurés que sont les arbres ordonnés et étiquetés, avec une définition originale de l’analogie fondée sur l’alignement optimal. Ensuite, nous présentons deux algorithmes qui calculent la dissemblance analogique entre quatre arbres et qui trouvent des solutions, éventuellement approchées, à une équation analogique entre arbres. Nous montrons leur utilisation dans deux applications : l’apprentissage de l’arbre syntaxique d’une phrase et la génération de la prosodie dans la synthèse de parole.},
  abstract  = {In Artificial Intelligence, analogy is used as a non exact reasoning technique to solve problems, for natural language processing, for learning classification rules, etc. This paper is interested in the analogical proportion, a simple form of the reasoning by analogy, and presents some of its uses in machine learning for NLP. The analogical proportion is a relation between four objects that expresses that the way to transform the first object into the second is the same as the way to transform the third in the fourth. We firstly give definitions about the general notion of analogical proportion between four objects. We give a special focus on objects structured as ordered and labeled trees, with an original definition of analogy based on optimal alignment. Secondly, we present two algorithms which deal with tree analogical matching and solving analogical equations between trees. We show their use in two applications : the learning of the syntactic tree (parsing) of a sentence and the generation of prosody for synthetic speech.},
  motscles  = {Proportion analogique, arbre syntaxique, analyseur syntaxique analogique},
  keywords  = {Analogical proportion, syntactic tree, analogical syntactic parser},
}

@inproceedings{embarek-ferret:2010:TALN,
  author    = {Embarek, Mehdi and Ferret, Olivier},
  title     = {Adapter un système de question-réponse en domaine ouvert au domaine médical},
  booktitle = {Actes de la 17e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {July},
  year      = {2010},
  address   = {Montréal, Canada},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2010/taln-2010-court-030},
  language  = {french},
  resume    = {Dans cet article, nous présentons Esculape, un système de question-réponse en français dédié aux médecins généralistes et élaboré à partir d’OEdipe, un système de question-réponse en domaine ouvert. Esculape ajoute à OEdipe la capacité d’exploiter la structure d’un modèle du domaine, le domaine médical dans le cas présent. Malgré l’existence d’un grand nombre de ressources dans ce domaine (UMLS, MeSH ...), il n’est pas possible de se reposer entièrement sur ces ressources, et plus spécifiquement sur les relations qu’elles abritent, pour répondre aux questions. Nous montrons comment surmonter cette difficulté en apprenant de façon supervisée des patrons linguistiques d’extraction de relations et en les appliquant à l’extraction de réponses.},
  abstract  = {In this article, we present Esculape, a question-answering system for French dedicated to family doctors and built from OEdipe, an open-domain system. Esculape adds to OEdipe the capability to exploit the concepts and relations of a domain model, the medical domain in the present case. Although a large number of resources exist in this domain (UMLS, MeSH ...), it is not possible to rely only on them, and more specifically on the relations they contain, to answer questions. We show how this difficulty can be overcome by learning linguistic patterns for identifying relations and applying them to extract answers.},
  motscles  = {systèmes de question-réponse, extraction de relations, domaine médical},
  keywords  = {question-answering systems, relation extraction, medical domain},
}

@inproceedings{zribi-mezghanihammami-hadrichbelguith:2010:TALN,
  author    = {Zribi, Inès and Mezghani Hammami, Souha and Hadrich Belguith, Lamia},
  title     = {L’apport d’une approche hybride pour la reconnaissance des entités nommées en langue arabe},
  booktitle = {Actes de la 17e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {July},
  year      = {2010},
  address   = {Montréal, Canada},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2010/taln-2010-court-031},
  language  = {french},
  resume    = {Dans cet article, nous proposons une méthode hybride pour la reconnaissance des entités nommées pour la langue arabe. Cette méthode profite, d’une part, des avantages de l’utilisation d’une méthode d’apprentissage pour extraire des règles permettant l’identification et la classification des entités nommées. D’autre part, elle repose sur un ensemble de règles extraites manuellement pour corriger et améliorer le résultat de la méthode d’apprentissage. Les résultats de l’évaluation de la méthode proposée sont encourageants. Nous avons obtenu un taux global de F-mesure égal à 79.24%.},
  abstract  = {In this paper, we propose a hybrid method for Arabic named entities recognition. This method takes advantage of the use of a learning method to extract rules for the identification and classification of named entities. Moreover, it is based on a set of rules extracted manually to correct and improve the outcome of the learning method. The evaluation results are encouraging as we get an overall F-measure equal to 79.24%.},
  motscles  = {Traitement de la langue arabe, reconnaissance des entités nommées, méthode d’apprentissage},
  keywords  = {Arabic language processing, named entity recognition, learning method},
}

@inproceedings{moot:2010:TALN,
  author    = {Moot, Richard},
  title     = {},
  booktitle = {Actes de la 17e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {July},
  year      = {2010},
  address   = {Montréal, Canada},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2010/taln-2010-court-032},
  language  = {french},
  note      = {Semi-automated Extraction of a Wide-Coverage Type-Logical Grammar for French},
  resume    = {Cet article décrit le développement d’une grammaire catégorielle à large couverture du Français, extraite à partir du corpus arboré de Paris 7 et vérifiée et corrigée manuellement. Le grammaire catégorielle résultant est évaluée en utilisant un supertagger et obtient des résultats comparables aux meilleurs supertaggers pour l’Anglais.},
  abstract  = {The paper describes the development of a wide-coverage type-logical grammar for French, which has been extracted from the Paris 7 treebank and received a significant amount of manual verification and cleanup. The resulting treebank is evaluated using a supertagger and performs at a level comparable to the best supertagging results for English.},
  motscles  = {Extraction de grammaires, grammaires catégorielles, supertagging},
  keywords  = {Categorial grammar, grammar extraction, supertagging, type-logical grammar},
}

@inproceedings{nakamuradelloye-villemontedelaclergerie:2010:TALN,
  author    = {Nakamura-Delloye, Yayoi and Villemonte De La Clergerie, Éric},
  title     = {Exploitation de résultats d’analyse syntaxique pour extraction semi-supervisée des chemins de relations},
  booktitle = {Actes de la 17e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {July},
  year      = {2010},
  address   = {Montréal, Canada},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2010/taln-2010-court-033},
  language  = {french},
  resume    = {Le présent article décrit un travail en cours sur l’acquisition des patrons de relations entre entités nommées à partir de résultats d’analyse syntaxique. Sans aucun patron prédéfini, notre méthode fournit des chemins syntaxiques susceptibles de représenter une relation donnée à partir de quelques exemples de couples d’entités nommées entretenant la relation en question.},
  abstract  = {This paper describes our current work on the acquisition of named entity relation patterns from parsing results. Without any predefined pattern, our method provides candidate syntactic paths that represent a given relationship with a small seed set of named entity pairs on this relationship.},
  motscles  = {Extraction des connaissances, extraction des patrons, relation des entités nommées, arbre syntaxique dépendanciel},
  keywords  = {Knowledge extraction, pattern extraction, named entity relation, syntactic dependency tree},
}

@inproceedings{nouvel-EtAl:2010:TALN,
  author    = {Nouvel, Damien and Soulet, Arnaud and Antoine, Jean-Yves and Friburger, Nathalie and Maurel, Denis},
  title     = {Reconnaissance d’entités nommées : enrichissement d’un système à base de connaissances à partir de techniques de fouille de textes},
  booktitle = {Actes de la 17e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {July},
  year      = {2010},
  address   = {Montréal, Canada},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2010/taln-2010-court-034},
  language  = {french},
  resume    = {Dans cet article, nous présentons et analysons les résultats du système de reconnaissance d’entités nommées CasEN lors de sa participation à la campagne d’évaluation Ester2. Nous identifions quelles ont été les difficultés pour notre système, essentiellement : les mots hors-vocabulaire, la métonymie, les frontières des entités nommées. Puis nous proposons une approche pour améliorer les performances de systèmes à base de connaissances, en utilisant des techniques exhaustives de fouille de données séquentielles afin d’extraire des motifs qui représentent les structures linguistiques en jeu lors de la reconnaissance d’entités nommées. Enfin, nous décrivons l’expérimentation menée à cet effet, donnons les résultats obtenus à ce jour et en faisons une première analyse.},
  abstract  = {In this paper, we present and analyze the results obtained by our named entity recognition system, CasEN, during the Ester2 evaluation campaign.We identify on what difficulties our system was the most challenged, which mainly are : out-of-vocabulary words, metonymy and detection of the boundaries of named entities. Next, we propose a direction which may help us for improving performances of our system, by using exhaustive hierarchical and sequential data mining algorithms. This approach aims at extracting patterns corresponding to useful linguistic constructs for recognizing named entities. Finaly, we describe our experiments, give the results we currently obtain and analyze those results.},
  motscles  = {Reconnaissance d’Entités Nommées, Séquences Hiérarchiques, Motifs, Ester2},
  keywords  = {Named Entity Recognition, Hierarchical Sequences, Patterns, Ester2},
}

@inproceedings{gaillard-collin-boualem:2010:TALN,
  author    = {Gaillard, Benoît and Collin, Olivier and Boualem, Malek},
  title     = {Traduction de requêtes basée sur Wikipédia},
  booktitle = {Actes de la 17e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {July},
  year      = {2010},
  address   = {Montréal, Canada},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2010/taln-2010-court-035},
  language  = {french},
  resume    = {Cet article s'inscrit dans le domaine de la recherche d'information multilingue. Il propose une méthode de traduction automatique de requêtes basée sur Wikipédia. Une phase d'analyse permet de segmenter la requête en syntagmes ou unités lexicales à traduire en s'appuyant sur les liens multilingues entre les articles de Wikipédia. Une deuxième phase permet de choisir, parmi les traductions possibles, celle qui est la plus cohérente en s'appuyant sur les informations d'ordre sémantique fournies par les catégories associées à chacun des articles de Wikipédia. Cet article justifie que les données issues de Wikipédia sont particulièrement pertinentes pour la traduction de requêtes, détaille l'approche proposée et son implémentation, et en démontre le potentiel par la comparaison du taux d'erreur du prototype de traduction avec celui d'autres services de traduction automatique.},
  abstract  = {This work investigates query translation using only Wikipedia-based resources in a two steps approach: analysis and disambiguation. After arguing that data mined from Wikipedia is particularly relevant to query translation, we detail the implementation of the approach. In the analysis phase, queries are segmented into lexical units that are associated to several possible translations using a bilingual dictionary extracted from Wikipedia. During the second phase, one translation is chosen amongst the various candidates, based on consistency, asserted with the help of semantic information carried by categories associated to Wikipedia articles. These two steps take advantage of data mined from Wikipedia, which is very rich and detailed, constantly updated but also easy and free to access. We report promising results regarding translation accuracy.},
  motscles  = {recherche d'information multilingue, traduction de requêtes, Wikipédia},
  keywords  = {cross language information retrieval, query translation, Wikipedia},
}

@inproceedings{ali-chali-hasan:2010:TALN,
  author    = {Ali, Husam and Chali, Yllias and Hasan, Sadid A.},
  title     = {},
  booktitle = {Actes de la 17e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {July},
  year      = {2010},
  address   = {Montréal, Canada},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2010/taln-2010-court-036},
  language  = {french},
  note      = {Automatic Question Generation from Sentences},
  resume    = {},
  abstract  = {Question Generation (QG) and Question Answering (QA) are some of the many challenges for natural language understanding and interfaces. As humans need to ask good questions, the potential benefits from automated QG systems may assist them in meeting useful inquiry needs. In this paper, we consider an automatic Sentence-to-Question generation task, where given a sentence, the Question Generation (QG) system generates a set of questions for which the sentence contains, implies, or needs answers. To facilitate the question generation task, we build elementary sentences from the input complex sentences using a syntactic parser. A named entity recognizer and a part of speech tagger are applied on each of these sentences to encode necessary information.We classify the sentences based on their subject, verb, object and preposition for determining the possible type of questions to be generated. We use the TREC-2007 (Question Answering Track) dataset for our experiments and evaluation.},
  motscles  = {Génération de questions, Analyseur syntaxique, Phrases élémentaires, POS Tagging},
  keywords  = {Question Generation, Syntactic Parsing, Elementary Sentence, POS Tagging},
}

@inproceedings{brunelle-charest:2010:TALN,
  author    = {Brunelle, Éric and Charest, Simon},
  title     = {Présentation du logiciel Antidote HD},
  booktitle = {Actes de la 17e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {July},
  year      = {2010},
  address   = {Montréal, Canada},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2010/taln-2010-demo-001},
  language  = {french},
  resume    = {},
  abstract  = {},
  motscles  = {},
  keywords  = {},
}

@inproceedings{barriere:2010:TALN,
  author    = {Barrière, Caroline},
  title     = {TerminoWeb : recherche et analyse d’information thématique},
  booktitle = {Actes de la 17e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {July},
  year      = {2010},
  address   = {Montréal, Canada},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2010/taln-2010-demo-002},
  language  = {french},
  resume    = {Notre démonstration porte sur le prototype TerminoWeb, une plateforme Web qui permet (1) la construction automatique d’un corpus thématique à partir d’une recherche de documents sur le Web, (2) l’extraction de termes du corpus, et (3) la recherche d’information définitionnelle sur ces termes en corpus. La plateforme intégrant les trois modules, elle aidera un langagier (terminologue, traducteur, rédacteur) à découvrir un nouveau domaine (thème) en facilitant la recherche et l’analyse de documents informatifs pertinents à ce domaine.},
  abstract  = {Our demonstration shows the TerminoWeb prototype, a Web platform which can (1) automatically assemble a thematic corpus from Web documents, (2) extract terms from that corpus, and (3) find definitional information in the corpus about terms of interest. As the platform integrates all three modules, it can help a language worker (terminologist, translator, writer) to explore a new domain (theme) as it facilitates the gathering and analysis of informative documents about that domain.},
  motscles  = {information thématique, construction de corpus, extraction de termes, découverte de contextes définitionnels},
  keywords  = {thematic information, corpus construction, term extraction, definitional contexts discovery},
}

@inproceedings{boitet-EtAl:2010:TALN,
  author    = {Boitet, Christian and Huynh, Cong Phap and Nguyen, Hong Thai and Bellynck, Valérie},
  title     = {},
  booktitle = {Actes de la 17e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {July},
  year      = {2010},
  address   = {Montréal, Canada},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2010/taln-2010-demo-003},
  language  = {french},
  note      = {The iMAG concept: multilingual access gateway to an elected Web sites with incremental quality increase through collaborative post-edition of MT pretranslations},
  resume    = {},
  abstract  = {We will demonstrate iMAGs (interactive Multilingual Access Gateways), in particular on a scientific laboratory web site and on the Greater Grenoble (La Métro) web site.},
  motscles  = {},
  keywords  = {Interactive translation gateway, iMAG, MT post-editing, collaborative translation},
}

@inproceedings{zaghouani:2010:TALN,
  author    = {Zaghouani, Wajdi},
  title     = {L'intégration d'un outil de repérage d'entités nommées pour la langue arabe dans un système de veille},
  booktitle = {Actes de la 17e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {July},
  year      = {2010},
  address   = {Montréal, Canada},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2010/taln-2010-demo-004},
  language  = {french},
  resume    = {Dans cette démonstration, nous présentons l'implémentation d'un outil de repérage d'entités nommées à base de règle pour la langue arabe dans le système de veille médiatique EMM (Europe Media Monitor).},
  abstract  = {We will present in this demo an Arabic rule-based named entity recogntion tool which is integrated within the news monitoring system EMM (Europe Media Montior).},
  motscles  = {Étiquetage des entités nommées, langue arabe, système de veille médiatique},
  keywords  = {Named entity recognition, Arabic language, news monitoring system},
}

@inproceedings{blanc-EtAl:2010:TALN,
  author    = {Blanc, Olivier and Boubel, Noémi and Goldman, Jean-Philippe and Roekhaut, Sophie and Simon, Anne Catherine and Fairon, Cédrick and Beaufort, Richard},
  title     = {Expressive : Génération automatique de parole expressive à partir de données non linguistiques},
  booktitle = {Actes de la 17e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {July},
  year      = {2010},
  address   = {Montréal, Canada},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2010/taln-2010-demo-005},
  language  = {french},
  resume    = {Nous présentons Expressive, un système de génération de parole expressive à partir de données non linguistiques. Ce système est composé de deux outils distincts : Taittingen, un générateur automatique de textes d’une grande variété lexico-syntaxique produits à partir d’une représentation conceptuelle du discours, et StyloPhone, un système de synthèse vocale multi-styles qui s’attache à rendre le discours produit attractif et naturel en proposant différents styles vocaux.},
  abstract  = {We present Expressive, a system that converts non-linguistic data into expressive speech. This system is made of two distinct parts : Taittingen, a natural language generation tool able to produce lexically and syntactically rich texts from a discourse abstract representation, and StyloPhone, a text-to-speech synthesis system that proposes varying speaking styles, to make the speech sound both more attractive and natural.},
  motscles  = {Génération de texte, synthèse vocale, expressivité},
  keywords  = {Natural language generation, text-to-speech synthesis, expressiveness},
}

@inproceedings{sadat-terrasa:2010:TALN,
  author    = {Sadat, Fatiha and Terrasa, Alexandre},
  title     = {Exploitation de Wikipédia pour l’Enrichissement et la Construction des Ressources Linguistiques},
  booktitle = {Actes de la 17e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {July},
  year      = {2010},
  address   = {Montréal, Canada},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2010/taln-2010-demo-006},
  language  = {french},
  resume    = {Cet article présente une approche et des résultats utilisant l'encyclopédie en ligne Wikipédia comme ressource semi-structurée de connaissances linguistiques et en particulier comme un corpus comparable pour l’extraction de terminologie bilingue. Cette approche tend à extraire d’abord des paires de terme et traduction à partir de types des informations, liens et textes de Wikipédia. L’étape suivante consiste à l’utilisation de l’information linguistique afin de ré-ordonner les termes et leurs traductions pertinentes et ainsi éliminer les termes cibles inutiles. Les évaluations préliminaires utilisant les paires de langues français-anglais, japonais-français et japonais-anglais ont montré une bonne qualité des paires de termes extraits. Cette étude est très favorable pour la construction et l’enrichissement des ressources linguistiques tels que les dictionnaires et ontologies multilingues. Aussi, elle est très utile pour un système de recherche d’information translinguistique (RIT).},
  abstract  = {Multilingual linguistic resources are usually constructed from parallel corpora, but since these corpora are available only for selected text domains and language pairs, the potential of other resources is being explored as well. This article seeks to explore and exploit the idea of using multilingual web-based encyclopaedias such as Wikipedia as comparable corpora for bilingual terminology extraction. We propose an approach to extract terms and their translations from different types of Wikipedia link information and texts. The next step will be using a linguistic-based information to re-rank and filter the extracted term candidates in the target language. Preliminary evaluations using the combined statisticsbased and linguistic-based approaches were applied on Japanese-French, French-English and Japanese- French. These evaluations showed a real open improvement and good quality of the extracted term candidates for building or enriching multilingual ontologies, dictionaries or feeding a cross-language information retrieval system with the related expansion terms of the source query.},
  motscles  = {Terminologie bilingue, corpus comparable, Wikipédia, ontologie multilingue},
  keywords  = {Bilingual terminology, comparable corpora, Wikipedia, mulilingual ontologies},
}

@inproceedings{braffort-filhol-segouat:2010:TALN,
  author    = {Braffort, Annelies and Filhol, Michael and Segouat, Jérémie},
  title     = {Traitement automatique des langues des signes : le projet Dicta-Sign, des corpus aux applications},
  booktitle = {Actes de la 17e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {July},
  year      = {2010},
  address   = {Montréal, Canada},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2010/taln-2010-demo-007},
  language  = {french},
  resume    = {Cet article présente Dicta-Sign, un projet de recherche sur le traitement automatique des langues des signes (LS), qui aborde un grand nombre de questions de recherche : linguistique de corpus, modélisation linguistique, reconnaissance et génération automatique. L’objectif de ce projet est de réaliser trois applications prototypes destinées aux usagers sourds : un traducteur de termes de LS à LS, un outil de recherche par l’exemple et un Wiki en LS. Pour cela, quatre corpus comparables de cinq heures de dialogue seront produits et analysés. De plus, des avancées significatives sont attendues dans le domaine des outils d’annotation. Dans ce projet, le LIMSI est en charge de l’élaboration des modèles linguistiques et participe aux aspects corpus et génération automatique. Nous nous proposons d’illustrer l’état d’avancement de Dicta-Sign au travers de vidéos extraites du corpus et de démonstrations des outils de traitement et de génération d’animations de signeur virtuel.},
  abstract  = {This paper presents Dicta-Sign, a research project related to sign language (SL) processing. It covers numerous research topics: corpus linguistics, linguistic modelling, automatic recognition and generation. The aim of this project is to design three proof-of-concept end user applications: an SL-to-SL term translator, a search-by-example tool, and a SL wiki. For that, four comparable corpora of five hours will be produced and analysed. Aside from these applications, we also expect major improvements to be integrated to annotation tools. In this project, LIMSI is in charge of the linguistic modelling, and participates in building the corpus and in the generation efforts. We propose to illustrate the current work with excerpts from the corpus and demonstrations of the processing and generation tools.},
  motscles  = {Langue des signes, corpus vidéo comparables, reconnaissance automatique, génération automatique},
  keywords  = {Sign languages, comparable video corpora, automatic recognition, automatic generation},
}

@inproceedings{goldman-nebhi-laenzlinger:2010:TALN,
  author    = {Goldman, Jean-Philippe and Nebhi, Kamel and Laenzlinger, Christopher},
  title     = {FipsColor : grammaire en couleur interactive pour l’apprentissage du français},
  booktitle = {Actes de la 17e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {July},
  year      = {2010},
  address   = {Montréal, Canada},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2010/taln-2010-demo-008},
  language  = {french},
  resume    = {L'analyseur multilingue FiPS permet de transformer une phrase en une structure syntaxique riche et accompagnée d'informations lexicales, grammaticales et thématiques. On décrit ici une application qui adapte les structures en constituants de l’analyseur FiPS à une nomenclature grammaticale permettant la représentation en couleur. Cette application interactive et disponible en ligne (http://latl.unige.ch/fipscolor) peut être utilisée librement par les enseignants et élèves de primaire.},
  abstract  = {The FiPS parser analyzes a sentence into a syntactic structure reflecting lexical, grammatical and thematic information. The present paper describes the adaptation of the structures in terms of constituents as existent in FiPS to a grammatical annotation, as well as its coloured representation. This online interactive application (available at http://latl.unige.ch/fipscolor) can be freely used by teachers and pupils of primary education.},
  motscles  = {analyse syntaxique, grammaire générative, services web, tei},
  keywords  = {chart parser, generative grammar, web services, tei},
}

@inproceedings{scherrer:2010:TALN,
  author    = {Scherrer, Yves},
  title     = {Des cartes dialectologiques numérisées pour le TALN},
  booktitle = {Actes de la 17e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {July},
  year      = {2010},
  address   = {Montréal, Canada},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2010/taln-2010-demo-009},
  language  = {french},
  resume    = {Cette démonstration présente une interface web pour des données numérisées de l’atlas linguistique de la Suisse allemande. Nous présentons d’abord l’intégration des données brutes et des données interpolées de l’atlas dans une interface basée sur Google Maps. Ensuite, nous montrons des prototypes de systèmes de traduction automatique et d’identification de dialectes qui s’appuient sur ces données dialectologiques numérisées.},
  abstract  = {This demonstration presents a web interface for digitized data of the linguistic atlas of German-speaking Switzerland. First, we present the integration of raw and interpolated atlas data with an interface based on Google Maps. Then, we show prototypes of machine translation and dialect identification systems which rely on the digitized dialectological data.},
  motscles  = {Dialectologie, atlas linguistique, traduction automatique, identification de dialectes},
  keywords  = {Dialectology, linguistic atlas, machine translation, dialect identification},
}

@inproceedings{beaufort-mace-fairon:2010:TALN,
  author    = {Beaufort, Richard and Macé, Kévin and Fairon, Cédrick},
  title     = {Text-it /Voice-it Une application mobile de normalisation des SMS},
  booktitle = {Actes de la 17e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {July},
  year      = {2010},
  address   = {Montréal, Canada},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2010/taln-2010-demo-010},
  language  = {french},
  resume    = {Cet article présente Text-it / Voice-it, une application de normalisation des SMS pour téléphone mobile. L’application permet d’envoyer et de recevoir des SMS normalisés, et offre le choix entre un résultat textuel (Text-it) et vocal (Voice-it).},
  abstract  = {This paper presents Text-it / Voice-it, an application that makes it possible to normalize text messages directly from mobile phones. The application allows the user to both send and receive normalized text messages, and gives the choice between a textual (Text-it) and a vocal (Voice-it) result.},
  motscles  = {SMS, normalisation, application, plugin, serveur},
  keywords  = {Text messages, normalization, application, plugin, server},
}

@inproceedings{moot:2010:TALN,
  author    = {Moot, Richard},
  title     = {},
  booktitle = {Actes de la 17e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {July},
  year      = {2010},
  address   = {Montréal, Canada},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2010/taln-2010-demo-011},
  language  = {french},
  note      = {Wide-Coverage French Syntax and Semantics using Grail},
  resume    = {Cette démonstration décrit Grail : un analyseur syntaxique pour grammaires catégorielles. Elle met l’accent sur les recherches récentes qui ont permis à Grail de donner des analyses syntaxiques et sémantiques du Français. Ces développements sont possibles grâce à une grammaire extraite semiautomatiquement du corpus de Paris 7 ainsi qu’un lexique sémantique qui traduit des combinaisons de mots, des étiquettes syntaxiques et des formules en Discourse Representation Structures.},
  abstract  = {The system demo introduces Grail, a general-purpose parser for multimodal categorial grammars, with special emphasis on recent research which makes Grail suitable for wide-coverage French syntax and semantics. These developments have been possible thanks to a categorial grammar which has been extracted semi-automatically from the Paris 7 treebank and a semantic lexicon which maps word, part-of-speech tags and formulas combinations to Discourse Representation Structures.},
  motscles  = {Discourse Representation Theory, grammaires catégorielles},
  keywords  = {Categorial grammar, Discourse Representation Theory, type-logical grammar},
}

@inproceedings{benabacha-zweigenbaum:2010:TALN,
  author    = {Ben Abacha, Asma and Zweigenbaum, Pierre},
  title     = {MeTAE : Plate-forme d’annotation automatique et d’exploration sémantiques pour le domaine médical},
  booktitle = {Actes de la 17e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {July},
  year      = {2010},
  address   = {Montréal, Canada},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2010/taln-2010-demo-012},
  language  = {french},
  resume    = {Nous présentons une plate-forme d’annotation sémantique et d’exploration de textes médicaux, appelée « MeTAE ». Le processus d’annotation automatique comporte une première étape de reconnaissance des entités médicales présentes dans les textes suivie d’une étape d’identification des relations sémantiques qui les relient. Cette identification se fonde sur des patrons linguistiques construits manuellement pour chaque type de relation. MeTAE génère des annotations RDF à partir des informations extraites et offre une interface d’exploration des textes annotés avec des requêtes sous forme de formulaire. La plate-forme peut être utilisée pour analyser sémantiquement les textes médicaux ou interroger la base d’annotation disponible pour avoir une/des réponses à une requête donnée (e.g. « ?X prévient maladie d’Alzheimer », équivalent à la question « comment prévenir la maladie d’Alzheimer ? »). Cette application peut être la base d’un système de questions-réponses pour le domaine médical.},
  abstract  = {This paper presents MeTAE, a platform for semantic annotation and exploration of medical texts. The annotation process encompasses medical entity recognition and semantic relationship identification between the retrieved entities. This identification is based on linguistic patterns constructed manually for each type of relation. MeTAE generates RDF annotations from the extracted information and allows semantic exploration of the annotated texts through a form-based interface. The platform can be used to semantically analyze medical texts or to explore the available annotation base through structured queries (e.g. “?X Prevents Alzheimer’s disease” for its natural-language equivalent: “how to prevent Alzheimer’s disease?”). MeTAE can be a basis for a medical question-answering system.},
  motscles  = {Annotation sémantique, interrogation sémantique, domaine médical},
  keywords  = {Semantic annotation, semantic querying, medical domain},
}

@inproceedings{guillaume-perrier:2010:TALN,
  author    = {Guillaume, Bruno and Perrier, Guy},
  title     = {LEOPAR, un analyseur syntaxique pour les grammaires d’interaction},
  booktitle = {Actes de la 17e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {July},
  year      = {2010},
  address   = {Montréal, Canada},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2010/taln-2010-demo-013},
  language  = {french},
  resume    = {Nous présentons ici l’analyseur syntaxique LEOPAR basé sur les grammaires d’interaction ainsi que d’autres outils utiles pour notre chaîne de traitement syntaxique.},
  abstract  = {We present the parser LEOPAR which is based on the Interaction Grammars formalism. We present also other tools used in our framework for parsing.},
  motscles  = {Analyse syntaxique, grammaires d’interaction, polarités},
  keywords  = {Parsing, Interaction Grammars, polarities},
}

@inproceedings{bourdaillet-EtAl:2010:TALN,
  author    = {Bourdaillet, Julien and Gotti, Fabrizio and Huet, Stéphane and Langlais, Philippe and Lapalme, Guy},
  title     = {TransSearch : un moteur de recherche de traductions},
  booktitle = {Actes de la 17e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {July},
  year      = {2010},
  address   = {Montréal, Canada},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2010/taln-2010-demo-014},
  language  = {french},
  resume    = {Malgré les nombreuses études visant à améliorer la traduction automatique, la traduction assistée par ordinateur reste la solution préférée des traducteurs lorsqu’une sortie de qualité est recherchée. Cette démonstration vise à présenter le moteur de recherche de traductions TransSearch. Cetteapplication commerciale, accessible sur leWeb, repose d’une part sur l’exploitation d’un bitexte aligné au niveau des phrases, et d’autre part sur des modèles statistiques d’alignement de mots.},
  abstract  = {Despite the impressive amount of studies devoted to improving the state of the art of machine translation, computer assisted translation tools remain the preferred solution of human translators when publication quality is of concern. This demonstration presents the translation search engine TransSearch. This web-based commercial application relies on a sentence-aligned bitext and a statistical word alignment techniques.},
  motscles  = {Traduction automatique statistique, repérage de traductions, alignement de mots, requêtes linguistiques},
  keywords  = {Statistical machine translation, translation spotting, word alignment, linguistic queries},
}

@inproceedings{russell:2010:TALN,
  author    = {Russell, Graham},
  title     = {},
  booktitle = {Actes de la 17e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {July},
  year      = {2010},
  address   = {Montréal, Canada},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2010/taln-2010-demo-015},
  language  = {french},
  note      = {Moz: Translation of Structured Terminology-Rich Text},
  resume    = {Description de Moz, un système d’aide à la traduction conçu pour le traitement de textes structurés ou semi-structurés avec une forte proportion de contenu terminologique. Le système comporte une mémoire de traduction collaborative, qui atteint un niveau élevé de rappel grâce à l’analyse sousphrastique ; il fournit également des dispositifs de communication et de révision. Le système est en production et traduit 140 000 mots par semaine.},
  abstract  = {Description of Moz, a translation support system designed for texts exhibiting a high proportion of structured and semi-structured terminological content. The system comprises a web-based collaborative translation memory, with high recall via subsentential linguistic analysis and facilities for messaging and quality assurance. It is in production use, translating some 140,000 words per week.},
  motscles  = {Aides à la traduction, sous-langage, analyse conceptuelle},
  keywords  = {Translation aids, sublanguage, conceptual analysis},
}

@inproceedings{nasr-bechet-rey:2010:TALN,
  author    = {Nasr, Alexis and Béchet, Frédéric and Rey, Jean-François},
  title     = {MACAON Une chaîne linguistique pour le traitement de graphes de mots},
  booktitle = {Actes de la 17e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {July},
  year      = {2010},
  address   = {Montréal, Canada},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2010/taln-2010-demo-016},
  language  = {french},
  resume    = {},
  abstract  = {},
  motscles  = {},
  keywords  = {},
}