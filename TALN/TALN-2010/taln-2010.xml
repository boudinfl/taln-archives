<?xml version="1.0" encoding="UTF-8"?>
<conference>
	<edition>
		<acronyme>TALN'2010</acronyme>
		<titre>17e conférence sur le Traitement Automatique des Langues Naturelles</titre>
		<ville>Montréal</ville>
		<pays>Canada</pays>
		<dateDebut>2010-07-19</dateDebut>
		<dateFin>2010-07-23</dateFin>
		<presidents>
			<president>
				<prenom>Philippe</prenom>
				<nom>Langlais</nom>
			</president>
			<president>
				<prenom>Michel</prenom>
				<nom>Gagnon</nom>
			</president>
		</presidents>
		<typeArticles>
			<type id="invite">Invités</type>
			<type id="long">Papiers longs</type>
			<type id="court">Papiers courts</type>
			<type id="démonstration">Démonstrations</type>
		</typeArticles>
		<siteWeb>http://www.groupes.polymtl.ca/taln2010/</siteWeb>
	</edition>
	<articles>
		<article id="taln-2010-invite-001" session="Conférence invitée ">
			<auteurs>
				<auteur>
					<prenom>Igor</prenom>
					<nom>Mel'čuk</nom>
					<email>Igor.Melcuk@umontreal.ca</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">OLST, Université de Montréal</affiliation>
			</affiliations>
			<titre>La phraséologie en langue, en dictionnaire et en TALN</titre>
			<type>invite</type>
			<pages></pages>
			<resume></resume>
			<mots_cles></mots_cles>
			<title></title>
			<abstract></abstract>
			<keywords></keywords>
		</article>
		<article id="taln-2010-invite-002" session="Conférence invitée ">
			<auteurs>
				<auteur>
					<prenom>Pierre</prenom>
					<nom>Isabelle</nom>
					<email>Pierre.Isabelle@cnrc-nrc.gc.ca</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">National Research Council Canada</affiliation>
			</affiliations>
			<titre>La montée en puissance des recherches en traduction automatique statistique</titre>
			<type>invite</type>
			<pages></pages>
			<resume></resume>
			<mots_cles></mots_cles>
			<title></title>
			<abstract></abstract>
			<keywords></keywords>
		</article>
		<article id="taln-2010-invite-003" session="Conférence invitée ">
			<auteurs>
				<auteur>
					<prenom>Gerald</prenom>
					<nom>Penn</nom>
					<email>frank@cs.toronto.edu</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">University of Toronto, 10 King’s College Rd., Toronto, M5S 3G4, ON, Canada</affiliation>
			</affiliations>
			<titre></titre>
			<type>invite</type>
			<pages></pages>
			<resume></resume>
			<mots_cles></mots_cles>
			<title>The Quantitative Study of Writing Systems</title>
			<abstract></abstract>
			<keywords></keywords>
		</article>
		<article id="taln-2010-long-001" session="Plénière">
			<auteurs>
				<auteur>
					<prenom>Holger</prenom>
					<nom>Schwenk</nom>
					<email>Holger.Schwenk@lium.univ-lemans.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIUM, Université du Maine, 72085 Le Mans cedex, France</affiliation>
			</affiliations>
			<titre>Adaptation d’un Système de Traduction Automatique Statistique avec des Ressources monolingues</titre>
			<type>long</type>
			<pages></pages>
			<resume>Les performances d’un système de traduction statistique dépendent beaucoup de la qualité et de la quantité des données d’apprentissage disponibles. La plupart des textes parallèles librement disponibles proviennent d’organisations internationales. Le jargon observé dans ces textes n’est pas très adapté pour construire un système de traduction pour d’autres domaines. Nous présentons dans cet article une technique pour adapter le modèle de traduction à un domaine différent en utilisant des textes dans la langue source uniquement. Nous obtenons des améliorations significatives du score BLEU dans des systèmes de traduction de l’arabe vers le français et vers l’anglais.</resume>
			<mots_cles>Traduction statistique, adaptation du modèle de traduction, corpus monolingue, apprentissage non-supervisé</mots_cles>
			<title></title>
			<abstract>The performance of a statistical machine translation system depends a lot on the quality and quantity of the available training data. Most of the existing, easily available parallel texts come from international organizations and the jargon observed in those texts is not very appropriate to build a machine translation system for other domains. In this paper, we present a technique to automatically adapt the translation model to a new domain using monolingual data in the source language only. We observe significant improvements in the BLEU score in statistical machine translation systems from Arabic to French and English respectively.</abstract>
			<keywords>Statistical machine translation, translation model adaptation, monolingual data, unsupervised training</keywords>
		</article>
		<article id="taln-2010-long-002" session="Plénière">
			<auteurs>
				<auteur>
					<prenom>Julien</prenom>
					<nom>Bourdaillet</nom>
					<email>bourdaij@iro.umontreal.ca</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Stéphane</prenom>
					<nom>Huet</nom>
					<email>huetstep@iro.umontreal.ca</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Philippe</prenom>
					<nom>Langlais</nom>
					<email>felipe@iro.umontreal.ca</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">RALI - DIRO - Université de Montréal, C.P. 6128, succursale centre-ville, H3C 3J7, Montréal, Québec, Canada</affiliation>
			</affiliations>
			<titre>Alignement de traductions rares à l’aide de paires de phrases non alignées</titre>
			<type>long</type>
			<pages></pages>
			<resume>Bien souvent, le sens d’un mot ou d’une expression peut être rendu dans une autre langue par plusieurs traductions. Parmi celles-ci, certaines se révèlent très fréquentes alors que d’autres le sont beaucoup moins, conformément à une loi zipfienne. La googlisation de notre monde n’échappe pas aux mémoires de traduction, qui mettent souvent à mal ou simplement ignorent ces traductions rares qui sont souvent de bonne qualité. Dans cet article, nous nous intéressons à ces traductions rares sous l’angle du repérage de traductions. Nous argumentons qu’elles sont plus difficiles à identifier que les traductions plus fréquentes. Nous décrivons une approche originale qui permet de mieux les identifier en tirant profit de l’alignement au niveau des mots de paires de phrases qui ne sont pas alignées. Nous montrons que cette approche permet d’améliorer l’identification de ces traductions rares.</resume>
			<mots_cles>Traduction automatique statistique, alignement de mots, traduction rares, contrôle de pertinence</mots_cles>
			<title></title>
			<abstract>There generally exist numerous ways to translate a word or a phrase in another language. Among these translations, some are very common while others are far less so, according to a zipfian law. As with the rest of the world, translation memories are googlized, leading to poorly handled or even simply ignored rare translations, while they are often of good quality. In this paper, we tackle this problem in a transpotting framework. We show that these rare translations are harder to identify than common translations. We describe an original approach based on the word alignment of sentences which are not aligned. We show that this approach significantly improves the identification of those rare translations.</abstract>
			<keywords>Statistical machine translation, word alignment, rare translations, relevance feedback</keywords>
		</article>
		<article id="taln-2010-long-003" session="Plénière">
			<auteurs>
				<auteur>
					<prenom>Pascal</prenom>
					<nom>Denis</nom>
					<email>pascal.denis@inria.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Benoît</prenom>
					<nom>Sagot</nom>
					<email>benoit.sagot@inria.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Alpage, INRIA Paris–Rocquencourt &amp; Université Paris 7, Rocquencourt, BP 105, 78153 Le Chesnay Cedex, France</affiliation>
			</affiliations>
			<titre>Exploitation d’une ressource lexicale pour la construction d’un étiqueteur morpho-syntaxique état-de-l’art du français</titre>
			<type>long</type>
			<pages></pages>
			<resume>Cet article présente MEltfr, un étiqueteur morpho-syntaxique automatique du français. Il repose sur un modèle probabiliste séquentiel qui bénéficie d’informations issues d’un lexique exogène, à savoir le Lefff. Evalué sur le FTB, MEltfr atteint un taux de précision de 97.75% (91.36% sur les mots inconnus) sur un jeu de 29 étiquettes. Ceci correspond à une diminution du taux d’erreur de 18% (36.1% sur les mots inconnus) par rapport au même modèle sans couplage avec le Lefff. Nous étudions plus en détail la contribution de cette ressource, au travers de deux séries d’expériences. Celles-ci font apparaître en particulier que la contribution des traits issus du Lefff est de permettre une meilleure couverture, ainsi qu’une modélisation plus fine du contexte droit des mots.</resume>
			<mots_cles>Etiquetage morpho-syntaxique, modèles à maximisation d’entropie, français, lexique</mots_cles>
			<title></title>
			<abstract>This paper presents MEltfr, an automatic POS tagger for French. This system relies on a sequential probabilistic model that exploits information extracted from an external lexicon, namely Lefff. When evaluated on the FTB corpus, MEltfr achieves an accuracy of 97.75% (91.36% on unknow words) using a tagset of 29 categories. This corresponds to an error rate decrease of 18% (36.1% on unknow words) compared to the same model without Lefff information. We investigate in more detail the contribution of this resource through two sets of experiments. These reveal in particular that the Lefff features allow for an increased coverage and a finer-grained modeling of the context at the right of a word.</abstract>
			<keywords>POS tagging, maximum entropy models, French, lexicon</keywords>
		</article>
		<article id="taln-2010-long-004" session="Analyse textuelle">
			<auteurs>
				<auteur>
					<prenom>Olivier</prenom>
					<nom>Ferret</nom>
					<email>olivier.ferret@cea.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">CEA, LIST, Laboratoire Vision et Ingénierie des Contenus, Fontenay-aux-Roses, F-92265 France</affiliation>
			</affiliations>
			<titre>Similarité sémantique et extraction de synonymes à partir de corpus</titre>
			<type>long</type>
			<pages></pages>
			<resume>La définition de mesures sémantiques au niveau lexical a fait l’objet de nombreux travaux depuis plusieurs années. Dans cet article, nous nous focalisons plus spécifiquement sur les mesures de nature distributionnelle. Bien que différentes évaluations ont été réalisées les concernant, il reste difficile à établir si une mesure donnant de bons résultats dans un cadre d’évaluation peut être appliquée plus largement avec le même succès. Dans le travail présenté, nous commençons par sélectionner une mesure de similarité sur la base d’un test de type TOEFL étendu. Nous l’appliquons ensuite au problème de l’extraction de synonymes à partir de corpus en comparant nos résultats avec ceux de (Curran &amp; Moens, 2002). Enfin, nous testons l’intérêt pour cette tâche d’extraction de synonymes d’une méthode d’amélioration de la qualité des données distributionnelles proposée dans (Zhitomirsky-Geffet &amp; Dagan, 2009).</resume>
			<mots_cles>extraction de synonymes, similarité sémantique, méthodes distributionnelles</mots_cles>
			<title></title>
			<abstract>The definition of lexical semantic measures has been the subject of lots of works for many years. In this article, we focus more specifically on distributional semantic measures. Although several evaluations about this kind of measures were already achieved, it is still difficult to determine if a measure that performs well in an evaluation framework can be applied more widely with the same success. In the work we present here, we first select a similarity measure by testing it against an extended TOEFL test. Then, we apply this measure for extracting automatically synonyms from a corpus and we compare our results to those of (Curran &amp; Moens, 2002). Finally, we test the interest for synonym extraction of a method proposed in (Zhitomirsky-Geffet &amp; Dagan, 2009) for improving the quality of distributional data.</abstract>
			<keywords>synonym extraction, semantic similarity, distributional methods</keywords>
		</article>
		<article id="taln-2010-long-005" session="Analyse textuelle">
			<auteurs>
				<auteur>
					<prenom>Simon</prenom>
					<nom>Charest</nom>
					<email>developpement@druide.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Éric</prenom>
					<nom>Brunelle</nom>
					<email>developpement@druide.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Jean</prenom>
					<nom>Fontaine</nom>
					<email>developpement@druide.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Druide informatique inc., 1435, rue Saint-Alexandre, bureau 1040, Montréal (Québec) H3A 2G4, Canada</affiliation>
			</affiliations>
			<titre>Au-delà de la paire de mots : extraction de cooccurrences syntaxiques multilexémiques</titre>
			<type>long</type>
			<pages></pages>
			<resume>Cet article décrit l’élaboration de la deuxième édition du dictionnaire de cooccurrences du logiciel d’aide à la rédaction Antidote. Cette nouvelle mouture est le résultat d’une refonte complète du processus d’extraction, ayant principalement pour but l’extraction de cooccurrences de plus de deux unités lexicales. La principale contribution de cet article est la description d’une technique originale pour l’extraction de cooccurrences de plus de deux mots conservant une structure syntaxique complète.</resume>
			<mots_cles>Antidote, cooccurrences, collocations, expressions multimots</mots_cles>
			<title></title>
			<abstract>This article describes the elaboration of the second edition of the co-occurrence dictionary included in Antidote HD, a commercial software tool for writing in French. This second edition is the result of a complete overhaul of the extraction process, with the objective of extracting co-occurrences of more than two lexical units. The main contribution of this article is the description of an original method for extracting co-occurrences of more than two words retaining their full syntactic structure.</abstract>
			<keywords>Antidote, co-occurrences, collocations, multi-word expressions (MWE)</keywords>
		</article>
		<article id="taln-2010-long-006" session="Analyse textuelle">
			<auteurs>
				<auteur>
					<prenom>Adil</prenom>
					<nom>El Ghali</nom>
					<email>elghali@lutin-userlab.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Yann</prenom>
					<nom>Vigile Hoareau</nom>
					<email>hoareau@lutin-userlab.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Lutin User Lab, Cité des Sciences, Av C. Cariou, 75019 Paris</affiliation>
				<affiliation affiliationId="2">Université Paris 8, rue de la Liberté, 93200 Saint Denis</affiliation>
			</affiliations>
			<titre>Une approche cognitive de la fouille de grandes collections de documents</titre>
			<type>long</type>
			<pages></pages>
			<resume>La récente éclosion du Web2.0 engendre un accroissement considérable de volumes textuels et intensifie ainsi l’importance d’une réflexion sur l’exploitation des connaissances à partir de grandes collections de documents. Dans cet article, nous présentons une approche de rechercher d’information qui s’inspire des certaines recherches issues de la psychologie cognitive pour la fouille de larges collections de documents. Nous utilisons un document comme requête permettant de récupérer des informations à partir d’une collection représentée dans un espace sémantique. Nous définissons les notions d’identité sémantique et de pollution sémantique dans un espace de documents. Nous illustrons notre approche par la description d’un système appelé BRAT (Blogosphere Random Analysis using Texts) basé sur les notions préalablement introduites d’identité et de pollution sématique appliquées à une tâche d’identification des actualités dans la blogosphère mondiale lors du concours TREC’09. Les premiers résultats produits sont tout à fait encourageant et indiquent les pistes des recherches à mettre en oeuvre afin d’améliorer les performances de BRAT.</resume>
			<mots_cles>Fouille de textes, Random-Indexing, Cognition, Marche aléatoire</mots_cles>
			<title></title>
			<abstract>MiningWeb 2.0 content become nowadays an important task in Information Retrieval and Search communities. The work related in this paper present an original approach of blogs mining, inspired from researches in cognitive psychology. We define the notions of semantic identity of blogs, and the semantic pollution in a semantic space. Then, we describe a system called BRAT (Blogosphere Random Analysis using Texts) based on these notions that has been applied to the Top Stories identification task of the Blog Track at the TREC’09 contest. The performance of BRAT at TREC’09 in its preliminary stage of development are very encouraging and the results of the experiences described here-after draw the lines of the future researches that should be realized in order to upgrade its performances.</abstract>
			<keywords>Text-Mining, Random-Indexing, Cognition, Random walk</keywords>
		</article>
		<article id="taln-2010-long-007" session="Syntaxe">
			<auteurs>
				<auteur>
					<prenom>Jonathan</prenom>
					<nom>Marchand</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Bruno</prenom>
					<nom>Guillaume</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Guy</prenom>
					<nom>Perrier</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">INRIA Nancy-Grand Est - LORIA - Nancy-Université</affiliation>
			</affiliations>
			<titre>Motifs de graphe pour le calcul de dépendances syntaxiques complètes</titre>
			<type>long</type>
			<pages></pages>
			<resume>Cet article propose une méthode pour calculer les dépendances syntaxiques d’un énoncé à partir du processus d’analyse en constituants. L’objectif est d’obtenir des dépendances complètes c’est-à-dire contenant toutes les informations nécessaires à la construction de la sémantique. Pour l’analyse en constituants, on utilise le formalisme des grammaires d’interaction : celui-ci place au cœur de la composition syntaxique un mécanisme de saturation de polarités qui peut s’interpréter comme la réalisation d’une relation de dépendance. Formellement, on utilise la notion de motifs de graphes au sens de la réécriture de graphes pour décrire les conditions nécessaires à la création d’une dépendance.</resume>
			<mots_cles>Analyse syntaxique, dépendance, grammaires d’interaction, polarité</mots_cles>
			<title></title>
			<abstract>This article describes a method to build syntactical dependencies starting from the phrase structure parsing process. The goal is to obtain all the information needed for a detailled semantical analysis. Interaction Grammars are used for parsing; the saturation of polarities which is the core of this formalism can be mapped to dependency relation. Formally, graph patterns are used to express the set of constraints which control dependency creations.</abstract>
			<keywords>Syntactic analysis, dependency, interaction grammars, polarity</keywords>
		</article>
		<article id="taln-2010-long-008" session="Syntaxe">
			<auteurs>
				<auteur>
					<prenom>Juliette</prenom>
					<nom>Thuilier</nom>
					<email>jthuilier@linguist.jussieu.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Gwendoline</prenom>
					<nom>Fox</nom>
					<email></email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Benoît</prenom>
					<nom>Crabbé</nom>
					<email>benoit.crabbe@linguist.jussieu.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université Paris VII Denis Diderot, UFRL et INRIA (Alpage)</affiliation>
				<affiliation affiliationId="2">Université Paris III Sorbonne-Nouvelle, ILPGA et EA 1483</affiliation>
			</affiliations>
			<titre>Approche quantitative en syntaxe : l’exemple de l’alternance de position de l’adjectif épithète en français</titre>
			<type>long</type>
			<pages></pages>
			<resume>Cet article présente une analyse statistique sur des données de syntaxe qui a pour but d’aider à mieux cerner le phénomène d’alternance de position de l’adjectif épithète par rapport au nom en français. Nous montrons comment nous avons utilisé les corpus dont nous disposons (French Treebank et le corpus de l’Est-Républicain) ainsi que les ressources issues du traitement automatique des langues, pour mener à bien notre étude. La modélisation à partir de 13 variables relevant principalement des propriétés du syntagme adjectival, de celles de l’item adjectival, ainsi que de contraintes basées sur la fréquence, permet de prédire à plus de 93% la position de l’adjectif. Nous insistons sur l’importance de contraintes relevant de l’usage pour le choix de la position de l’adjectif, notamment à travers la fréquence d’occurrence de l’adjectif, et la fréquence de contextes dans lesquels il apparaît.</resume>
			<mots_cles>Syntaxe probabiliste, linguistique de corpus, adjectif épithète, régression logistique</mots_cles>
			<title></title>
			<abstract>This article presents a statistical analysis of syntactic data that aims to better understand the phenomenon of position alternation displayed by attributive adjectives with respect to nouns in French. We show how we used the corpora available for French (the French Treebank and the Est-Républicain corpus) as well as ressources provided by Natural Language Processing for our study. The proposed model contains 13 variables based on properties of the adjectival phrase, the adjectival item and on frequency constraints. This model is capable to predict the position of adjectives at more than a 93% rate. We especially focus on the importance of constraints based on usage for the choice of position for the adjective, in particular the frequency of contexts in which it appears.</abstract>
			<keywords>Probabilistic syntax, corpus linguistics, attributive adjective, logistic regression</keywords>
		</article>
		<article id="taln-2010-long-009" session="Syntaxe">
			<auteurs>
				<auteur>
					<prenom>Philippe</prenom>
					<nom>Blache</nom>
					<email>blache@lpl-aix.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Laboratoire Parole et Langage, CNRS &amp; Université de Provence, 5, Avenue Pasteur, 13604 Aix en Provence - France</affiliation>
			</affiliations>
			<titre>Un modèle de caractérisation de la complexité syntaxique</titre>
			<type>long</type>
			<pages></pages>
			<resume>Cet article présente un modèle de la complexité syntaxique. Il réunit un ensemble d’indices de complexité et les représente à l’aide d’un cadre formel homogène, offrant ainsi la possibilité d’une quantification automatique : le modèle proposé permet d’associer à chaque phrase un indice reflétant sa complexité.</resume>
			<mots_cles>Complexité syntaxique, analyse syntaxique automatique, parser humain</mots_cles>
			<title></title>
			<abstract>This paper proposes a model of syntactic complexity. It brings together a set of complexity parameters and represnt them thanks to a unique formal framework. This approach makes it possible an automatic evaluation : a complexity index can be associated to each sentence.</abstract>
			<keywords>Syntactic complexity, parsing, human parser</keywords>
		</article>
		<article id="taln-2010-long-010" session="Syntaxe">
			<auteurs>
				<auteur>
					<prenom>Éric</prenom>
					<nom>Villemonte De La Clergerie</nom>
					<email>eric.de_la_clergerie@inria.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Alpage, INRIA / Univ. Paris 7, 30 rue du Château-des-rentiers, 75013 Paris</affiliation>
			</affiliations>
			<titre>Convertir des dérivations TAG en dépendances</titre>
			<type>long</type>
			<pages></pages>
			<resume>Les structures de dépendances syntaxiques sont importantes et bien adaptées comme point de départ de diverses applications. Dans le cadre de l’analyseur TAG FRMG, nous présentons les détails d’un processus de conversion de forêts partagées de dérivations en forêts partagées de dépendances. Des éléments d’information sont fournis sur un algorithme de désambiguisation sur ces forêts de dépendances.</resume>
			<mots_cles>dépendances, analyse syntaxique, TAG, forêt partagée</mots_cles>
			<title></title>
			<abstract>Syntactic dependency structures are important and adequate as starting point for various NLP applications. In the context of the French TAG FRMG parser, we present the details of a conversion process from shared derivation forests into shared dependency forests. Some information are also provided about a disambiguisation algorithm for these dependency forests.</abstract>
			<keywords>dependencies, parsing, TAG, shared forest</keywords>
		</article>
		<article id="taln-2010-long-011" session="TALN pour les TIC">
			<auteurs>
				<auteur>
					<prenom>Shamima</prenom>
					<nom>Mithun</nom>
					<email>s_mithun@encs.concordia.ca</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Leila</prenom>
					<nom>Kosseim</nom>
					<email>kosseim@encs.concordia.ca</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Concordia University, Department of Computer Science and Software Engineering, Montreal, Quebec, Canada</affiliation>
			</affiliations>
			<titre></titre>
			<type>long</type>
			<pages></pages>
			<resume></resume>
			<mots_cles></mots_cles>
			<title>A Hybrid Approach to Utilize Rhetorical Relations for Blog Summarization</title>
			<abstract>The availability of huge amounts of online opinions has created a new need to develop effective query-based opinion summarizers to analyze this information in order to facilitate decision making at every level. To develop an effective opinion summarization approach, we have targeted to resolve specifically Question Irrelevancy and Discourse Incoherency problems which have been found to be the most frequently occurring problems for opinion summarization. To address these problems, we have introduced a hybrid approach by combining text schema and rhetorical relations to exploit intra-sentential rhetorical relations. To evaluate our approach, we have built a system called BlogSum and have compared BlogSum-generated summaries after applying rhetorical structuring to BlogSum-generated candidate sentences without utilizing rhetorical relations using the Text Analysis Conference (TAC) 2008 data for summary contents. Evaluation results show that our approach improves summary contents by reducing question irrelevant sentences.</abstract>
			<keywords>Blog Summarization, Rhetorical Relations, Text Schema</keywords>
		</article>
		<article id="taln-2010-long-012" session="TALN pour les TIC">
			<auteurs>
				<auteur>
					<prenom>Richard</prenom>
					<nom>Beaufort</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Sophie</prenom>
					<nom>Roekhaut</nom>
					<email></email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Louise-Amélie</prenom>
					<nom>Cougnon</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Cédrick</prenom>
					<nom>Fairon</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">CENTAL, Université catholique de Louvain, 1348 Louvain-la-Neuve, Belgique</affiliation>
				<affiliation affiliationId="2">TCTS Lab, Université de Mons, 7000 Mons, Belgique</affiliation>
			</affiliations>
			<titre>Une approche hybride traduction/correction pour la normalisation des SMS</titre>
			<type>long</type>
			<pages></pages>
			<resume>Cet article présente une méthode hybride de normalisation des SMS, à mi-chemin entre correction orthographique et traduction automatique. La partie du système qui assure la normalisation utilise exclusivement des modèles entraînés sur corpus. Evalué en français par validation croisée, le système obtient un taux d’erreur au mot de 9.3% et un score BLEU de 0.83.</resume>
			<mots_cles>SMS, normalisation, machines à états finis, approche hybride, orienté traduction, orienté correction, apprentissage sur corpus</mots_cles>
			<title></title>
			<abstract>This paper presents a method of normalizing SMS messages that shares similarities with both spell checking and machine translation approaches. The normalization part of the system is entirely based on models trained from a corpus. Evaluated in French by ten-fold cross-validation, the system achieves a 9.3% Word Error Rate and a 0.83 BLEU score.</abstract>
			<keywords>SMS messages, normalization, finite-state machines, hybrid approach, machine translationlike, spell checking-like, corpus-based learning</keywords>
		</article>
		<article id="taln-2010-long-013" session="TALN pour les TIC">
			<auteurs>
				<auteur>
					<prenom>Guillaume</prenom>
					<nom>Wisniewski</nom>
					<email>guillaume.wisniewski@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Aurélien</prenom>
					<nom>Max</nom>
					<email>aurelien.max@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>François</prenom>
					<nom>Yvon</nom>
					<email>francois.yvon@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIMSI—CNRS, B.P. 133 91 403 ORSAY CEDEX, Université Paris Sud 11</affiliation>
			</affiliations>
			<titre>Recueil et analyse d’un corpus écologique de corrections orthographiques extrait des révisions de Wikipédia</titre>
			<type>long</type>
			<pages></pages>
			<resume>Dans cet article, nous introduisons une méthode à base de règles permettant d’extraire automatiquement de l’historique des éditions de l’encyclopédie collaborative Wikipédia des corrections orthographiques. Cette méthode nous a permis de construire un corpus d’erreurs composé de 72 483 erreurs lexicales (non-word errors) et 74 100 erreurs grammaticales (real-word errors). Il n’existe pas, à notre connaissance, de plus gros corpus d’erreurs écologiques librement disponible. En outre, les techniques mises en oeuvre peuvent être facilement transposées à de nombreuses autres langues. La collecte de ce corpus ouvre de nouvelles perspectives pour l’étude des erreurs fréquentes ainsi que l’apprentissage et l’évaluation des correcteurs orthographiques automatiques. Plusieurs expériences illustrant son intérêt sont proposées.</resume>
			<mots_cles>ressources pour le TAL, correction orthographique, Wikipédia</mots_cles>
			<title></title>
			<abstract>This paper describes a French spelling error corpus we built by miningWikipedia revision history. This corpus contains 72,493 non-word errors and 74,100 real-word errors. To the best of our knowledge, this is the first time that such a large corpus of naturally occurring errors is collected and made publicly available, which opens new possibilities for the evaluation of spell checkers and the study of error patterns. In the second part of this work, a first study of french spelling error patterns and of the performance of a spell checker is presented.</abstract>
			<keywords>resources for NLP, spelling correction, Wikipedia</keywords>
		</article>
		<article id="taln-2010-long-014" session="TALN pour les TIC">
			<auteurs>
				<auteur>
					<prenom>Eric</prenom>
					<nom>Charton</nom>
					<email>eric.charton@polymtl.ca</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Michel</prenom>
					<nom>Gagnon</nom>
					<email>michel.gagnon@polymtl.ca</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Benoit</prenom>
					<nom>Ozell</nom>
					<email>benoit.ozell@polymtl.ca</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">École Polytechnique, 2900 boul. Edouard Montpetit, Montréal, Canada H3T 1J4</affiliation>
			</affiliations>
			<titre>Extension d’un système d’étiquetage d’entités nommées en étiqueteur sémantique</titre>
			<type>long</type>
			<pages></pages>
			<resume>L’étiquetage sémantique consiste à associer un ensemble de propriétés à une séquence de mots contenue dans un texte. Bien que proche de la tâche d’étiquetage par entités nommées, qui revient à attribuer une classe de sens à un mot, la tâche d’étiquetage ou d’annotation sémantique cherche à établir la relation entre l’entité dans son texte et sa représentation ontologique. Nous présentons un étiqueteur sémantique qui s’appuie sur un étiqueteur d’entités nommées pour mettre en relation un mot ou un groupe de mots avec sa représentation ontologique. Son originalité est d’utiliser une ontologie intermédiaire de nature statistique pour établir ce lien.</resume>
			<mots_cles>Étiqueteur sémantique, Entités nommées, Analyse sémantique, Ontologie</mots_cles>
			<title></title>
			<abstract>Semantic labelling consist to associate a set of properties to a sequence of words from a text. Although its proximity with the named entity labelling task, which is equivalent to associate a class meaning to a sequence of word, the task of semantic labelling try to establish the relation between the entity in the text and it’s ontological representation. We present a semantic labelling system based on a named entity recognition step. The originality of our system is that the link between named entity and its semantic representation is obtained trough the use of an intermediate statistical ontology.</abstract>
			<keywords>Semantic parser, Named entities, Semantic annotation</keywords>
		</article>
		<article id="taln-2010-long-015" session="Sémantique">
			<auteurs>
				<auteur>
					<prenom>Jasmina</prenom>
					<nom>Milićević</nom>
					<email>jmilicev@dal.ca</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">French Department, Dalhousie University, 6135 University Avenue, Halifax (N.S.) CANADA B3H 4P9</affiliation>
			</affiliations>
			<titre>Extraction de paraphrases sémantiques et lexico-syntaxiques de corpus parallèles bilingues</titre>
			<type>long</type>
			<pages></pages>
			<resume>Nous présentons le travail en cours effectué dans le cadre d’un projet d’extraction de paraphrases à partir de textes parallèles bilingues. Nous identifions des paraphrases sémantiques et lexico-syntaxiques, qui mettent en jeu des opérations relativement complexes sur les structures sémantiques et syntaxiques de phrases, et les décrivons au moyen de règles de paraphrasage de type Sens-Texte, utilisables dans diverses applications de TALN.</resume>
			<mots_cles>paraphrase lexico-syntaxique, paraphrase sémanique, règles de paraphrasage, corpus bilingues, théorie Sens-Texte</mots_cles>
			<title></title>
			<abstract>We present work in progress done within a project of extracting paraphrases from parallel bilingual texts. We identify semantic and lexical-syntactic paraphrases, which imply relatively complex operations on semantic and syntactic structures of sentences, and describe them by means of paraphrasing rules of Meaning-Text type, usable in various NLP applications.</abstract>
			<keywords>lexical-syntactic paraphrase, semantic paraphrase, paraphrasing rules, bilingual corpora, Meaning-Text theory</keywords>
		</article>
		<article id="taln-2010-long-016" session="Sémantique">
			<auteurs>
				<auteur>
					<prenom>Lydia-Mai</prenom>
					<nom>Ho-Dac</nom>
					<email>lydia.ho-dac@uclouvain.be</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Marie-Paule</prenom>
					<nom>Péry-Woodley</nom>
					<email>pery@univ-tlse2.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Ludovic</prenom>
					<nom>Tanguy</nom>
					<email>tanguy@univ-tlse2.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">VALIBEL, UCL et FNRS</affiliation>
				<affiliation affiliationId="2">CLLE-ERSS, Université de Toulouse</affiliation>
			</affiliations>
			<titre>Anatomie des structures énumératives</titre>
			<type>long</type>
			<pages></pages>
			<resume>Cet article présente les premiers résultats d’une campagne d’annotation de corpus à grande échelle réalisée dans le cadre du projet ANNODIS. Ces résultats concernent la partie descendante du dispositif d’annotation, et plus spécifiquement les structures énumératives. Nous nous intéressons à la structuration énumérative en tant que stratégie de base de mise en texte, apparaissant à différents niveaux de granularité, associée à différentes fonctions discursives, et signalée par des indices divers. Avant l’annotation manuelle, une étape de pré-traitement a permis d’obtenir le marquage systématique de traits associés à la signalisation de l’organisation du discours. Nous décrivons cette étape de marquage automatique, ainsi que la procédure d’annotation. Nous proposons ensuite une première typologie des structures énumératives basée sur la description quantitative des données annotées manuellement, prenant en compte la couverture textuelle, la composition et les types d’indices.</resume>
			<mots_cles>Annotation de corpus, organisation du discours, structure énumérative, signalisation</mots_cles>
			<title></title>
			<abstract>This paper presents initial results from a large scale discourse annotation project, the ANNODIS project. These results concern the top-down part of the annotation scheme, and more specifically enumerative structures. We are interested in enumerative structures as a basic text construction strategy, occurring at different levels of granularity, associated with various discourse functions, and signalled by a broad range of cues. Before manual annotation via a purpose-built interface, a pre-processing phase produced a systematic mark-up of features associated to the signalling of discourse organisation. We describe this markup phase and the annotation procedure. We then propose a first typology of enumerative structures based on a quantitative description of the manually annotated data, taking into account textual coverage, composition, types of cues.</abstract>
			<keywords>Corpus annotation, discourse organisation, enumerative structure, signalling text structures</keywords>
		</article>
		<article id="taln-2010-long-017" session="Sémantique">
			<auteurs>
				<auteur>
					<prenom>Fadila</prenom>
					<nom>Hadouche</nom>
					<email>hadouchf@iro.umontreal.ca</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Guy</prenom>
					<nom>Lapalme</nom>
					<email>lapalme@iro.umontreal</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Marie-Claude</prenom>
					<nom>L’Homme</nom>
					<email>mc.lhomme@umontreal.ca</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">RALI, Université de Montréal, C.P. 6128, succursale Centre-ville, Montréal, Québec, Canada H3C 3J7</affiliation>
				<affiliation affiliationId="2">OLST, Université de Montréal, C.P. 6128, succursale Centre-ville, Montréal, Québec, Canada H3C 3J7</affiliation>
			</affiliations>
			<titre>Identification des actants et circonstants par apprentissage machine</titre>
			<type>long</type>
			<pages></pages>
			<resume>Dans cet article, nous traitons de l’identification automatique des participants actants et circonstants de lexies prédicatives verbales tirées d’un corpus spécialisé en langue française. Les actants contribuent à la réalisation du sens de la lexie alors que les circonstants sont optionnels : ils ajoutent une information supplémentaire qui ne fait pas partie intégrante du sémantisme de la lexie. Nous proposons une classification de ces participants par apprentissage machine basée sur un corpus de lexies verbales du domaine de l’informatique, lexies qui ont été annotées manuellement avec des rôles sémantiques. Nous présentons des features qui nous permettent d’identifier les participants et de distinguer les actants des circonstants.</resume>
			<mots_cles>Structure actancielle, actants et circonstants, features de classification</mots_cles>
			<title></title>
			<abstract>In this paper we discuss the identification of participants actants and circumstants of specialized verbal lexical units in a French specialised corpus. The actants are linguistic units that contribute to the sense of the verbal lexical unit while circumstants are optional: they add extra information that is not part of the meaning of the verbal unit. In this work, we propose a classification of participants using machine learning based on a specialized corpus of verbal lexical items in the field of computing which are annotated manually with semantic roles labels. We defined features to identify participants and distinguish actants from circumstants.</abstract>
			<keywords>Actantial structure, actants and circumstants, classification features</keywords>
		</article>
		<article id="taln-2010-long-018" session="Sémantique">
			<auteurs>
				<auteur>
					<prenom>Marie-Noëlle</prenom>
					<nom>Bessagnet</nom>
					<email>marie-noelle.bessagnet@univ-pau.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Mauro</prenom>
					<nom>Gaio</nom>
					<email>mauro.gaio@univ-pau.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Eric</prenom>
					<nom>Kergosien</nom>
					<email>eric.kergosien@univ-pau.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Christian</prenom>
					<nom>Sallaberry</nom>
					<email>christian.sallaberry@univ-pau.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Laboratoire LIUPPA, UPPA, IAE, Avenue du doyen Poplawski, 64012 PAU</affiliation>
				<affiliation affiliationId="2">Laboratoire LIUPPA, UPPA, Faculté des Sciences, Département Informatique, 64000 PAU</affiliation>
			</affiliations>
			<titre>Extraction automatique d'un lexique à connotation géographique à des fins ontologiques dans un corpus de récits de voyage</titre>
			<type>long</type>
			<pages></pages>
			<resume>Le but de ces travaux est d’extraire un lexique en analysant les relations entre des syntagmes nominaux et des syntagmes verbaux dans les textes de notre corpus, essentiellement des récits de voyage. L’hypothèse que nous émettons est de pouvoir établir une catégorisation des syntagmes nominaux associés à des Entités Nommées de type lieu à l’aide de l’analyse des relations verbales. En effet, nous disposons d’une chaine de traitement automatique qui extrait, interprète et valide des Entités Nommées de type lieu dans des documents textuels. Ce travail est complété par l’analyse des relations verbales associées à ces EN, candidates à l’enrichissement d’une ontologie.</resume>
			<mots_cles>Entité nommée, ontologie, relations verbales, patrons linguistiques</mots_cles>
			<title></title>
			<abstract>The aim of this research work is to extract a lexicon by analyzing the relationship between nominal syntagms and verb construction within our corpus, namely travel stories. We would like to establish a categorization of nominal syntagms linked to Named Entity (NE) (type space) thanks to verbal relationships analysis. In fact, we develop a computerized process flow in order to extract, to interpret and to validate NE of type space in textual documents. This research work is completed by the analyze of verbal relationships linked to these EN which could enrich our ontology.</abstract>
			<keywords>Named Entity, ontology, verbal relations, language patterns</keywords>
		</article>
		<article id="taln-2010-long-019" session="Parole">
			<auteurs>
				<auteur>
					<prenom>Stanislas</prenom>
					<nom>Oger</nom>
					<email>stanislas.oger@univ-avignon.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Mickael</prenom>
					<nom>Rouvier</nom>
					<email>mickael.rouvier@univ-avignon.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Georges</prenom>
					<nom>Linarès</nom>
					<email>georges.linares@univ-avignon.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIA, Université d’Avignon, France</affiliation>
			</affiliations>
			<titre>Classification du genre vidéo reposant sur des transcriptions automatiques</titre>
			<type>long</type>
			<pages></pages>
			<resume>Dans cet article nous proposons une nouvelle méthode pour l’identification du genre vidéo qui repose sur une analyse de leur contenu linguistique. Cette approche consiste en l’analyse des mots apparaissant dans les transcriptions des pistes audio des vidéos, obtenues à l’aide d’un système de reconnaissance automatique de la parole. Les expériences sont réalisées sur un corpus composé de dessins animés, de films, de journaux télévisés, de publicités, de documentaires, d’émissions de sport et de clips de musique. L’approche proposée permet d’obtenir un taux de bonne classification de 74% sur cette tâche. En combinant cette approche avec des méthodes reposant sur des paramètres acoustiques bas-niveau, nous obtenons un taux de bonne classification de 95%.</resume>
			<mots_cles>classification de genre vidéo, traitement audio de la vidéo, extraction de paramètres linguistiques</mots_cles>
			<title></title>
			<abstract>In this paper, we present a new method for video genre identification based on the linguistic content analysis. This approach relies on the analysis of the words in the video transcriptions provided by an automatic speech recognition system. Experiments are conducted on a corpus composed of cartoons, movies, news, commercials, documentary, sport and music. On this 7-genre identification task, the proposed transcription-based method obtains up to 74% of correct identification. Finally, this rate is increased to 95% by combining the proposed linguistic-level features with low-level acoustic features.</abstract>
			<keywords>video genre classification, audio-based video processing, linguistic feature extraction</keywords>
		</article>
		<article id="taln-2010-long-020" session="Parole">
			<auteurs>
				<auteur>
					<prenom>Christian</prenom>
					<nom>Raymond</nom>
					<email>Christian.Raymond@irisa.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Julien</prenom>
					<nom>Fayolle</nom>
					<email>Julien.Fayolle@irisa.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université Européenne de Bretagne, INRIA,IRISA, UMR 6074, France</affiliation>
				<affiliation affiliationId="2">INSA de Rennes, 20 Avenue des buttes de coesme, Rennes, France</affiliation>
			</affiliations>
			<titre>Reconnaissance robuste d’entités nommées sur de la parole transcrite automatiquement</titre>
			<type>long</type>
			<pages></pages>
			<resume>Les transcriptions automatiques de parole constituent une ressource importante, mais souvent bruitée, pour décrire des documents multimédia contenant de la parole (e.g. journaux télévisés). En vue d’améliorer la recherche documentaire, une étape d’extraction d’information à caractère sémantique, précédant l’indexation, permet de faire face au problème des transcriptions imparfaites. Parmis ces contenus informatifs, on compte les entités nommées (e.g. noms de personnes) dont l’extraction est l’objet de ce travail. Les méthodes traditionnelles de reconnaissance basées sur une définition manuelle de grammaires formelles donnent de bons résultats sur du texte ou des transcriptions propres manuellement produites, mais leurs performances se trouvent fortement affectées lorsqu’elles sont appliquées sur des transcriptions automatiques. Nous présentons, ici, trois méthodes pour la reconnaissance d’entités nommées basées sur des algorithmes d’apprentissage automatique : les champs conditionnels aléatoires, les machines à de support, et les transducteurs à états finis. Nous présentons également une méthode pour rendre consistantes les données d’entrainement lorsqu’elles sont annotées suivant des conventions légèrement différentes. Les résultats montrent que les systèmes d’étiquetage obtenus sont parmi les plus robustes sur les données d’évaluation de la campagne ESTER 2 dans les conditions où la transcription automatique est particulièrement bruitée.</resume>
			<mots_cles>étiqueteur d’entités nommées, transcription automatique de parole, apprentissage automatique, champs conditionnels aléatoires, machines à vecteurs de support, transducteurs à états finis</mots_cles>
			<title></title>
			<abstract>Automatic speech transcripts are an important, but noisy, ressource to index spoken multimedia documents (e.g. broadcast news). In order to improve both indexation and information retrieval, extracting semantic information from these erroneous transcripts is an interesting challenge. Among these meaningful contents, there are named entities (e.g. names of persons) which are the subject of this work. Traditional named entity taggers are based on manual and formal grammars. They obtain correct performance on text or clean manual speech transcripts, but they have a lack of robustness when applied on automatic transcripts. We are introducing, in this work, three methods for named entity recognition based on machine learning algorithms, namely conditional random fields, support vector machines, and finitestate transducers. We are also introducing a method to make consistant the training data when they are annotated with slightly different conventions. We show that our tagger systems are among the most robust when applied to the evaluation data of the French ESTER 2 campaign in the most difficult conditions where transcripts are particularly noisy.</abstract>
			<keywords>named entity tagger, automatic speech recognition transcripts, machine learning, conditionnal random fields, support vector machines, finite-state transducers</keywords>
		</article>
		<article id="taln-2010-long-021" session="Parole">
			<auteurs>
				<auteur>
					<prenom>Younès</prenom>
					<nom>Bahou</nom>
					<email>bahou_younes@yahoo.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Abir</prenom>
					<nom>Masmoudi</nom>
					<email>masmoudiabir@gmail.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Lamia</prenom>
					<nom>Hadrich Belguith</nom>
					<email>l.belguith@fsegs.rnu.tn</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">ANLP Research Group – Laboratoire MIRACL, Faculté des Sciences Economiques et de Gestion de Sfax, B.P. 1088, 3018 - Sfax – TUNISIE</affiliation>
			</affiliations>
			<titre>Traitement des disfluences dans le cadre de la compréhension automatique de l’oral arabe spontané</titre>
			<type>long</type>
			<pages></pages>
			<resume>Les disfluences inhérents de toute parole spontanée sont un vrai défi pour les systèmes de compréhension de la parole. Ainsi, nous proposons dans cet article, une méthode originale pour le traitement des disfluences (plus précisément, les autocorrections, les répétitions, les hésitations et les amorces) dans le cadre de la compréhension automatique de l’oral arabe spontané. Notre méthode est basée sur une analyse à la fois robuste et partielle, des énoncés oraux arabes. L’idée consiste à combiner une technique de reconnaissance de patrons avec une analyse sémantique superficielle par segments conceptuels. Cette méthode a été testée à travers le module de compréhension du système SARF, un serveur vocal interactif offrant des renseignements sur le transport ferroviaire tunisien (Bahou et al., 2008). Les résultats d’évaluation de ce module montrent que la méthode proposée est très prometteuse. En effet, les mesures de rappel, de précision et de F-Measure sont respectivement de 79.23%, 74.09% et 76.57%.</resume>
			<mots_cles>disfluences, segment conceptuel, reconnaissance de patrons, parole arabe spontanée</mots_cles>
			<title></title>
			<abstract>The disfluencies inherent in spontaneous speaking are a real challenge for speech understanding systems. Thus, we propose in this paper, an original method for processing disfluencies (more precisely, self-corrections, repetitions, hesitations and word-fragments) in the context of automatic spontaneous Arabic speech understanding. Our method is based on a robust and partial analysis of Arabic oral utterances. The main idea is to combine a pattern matching technique and a surface semantic analysis with conceptual segments. This method has been evaluated through the understanding module of SARF system, an interactive vocal server offering Tunisian railway information (Bahou et al., 2008). The evaluation results of this module show that the proposed method is very promising. Indeed, the measures of recall, precision and F-Measure are respectively 79.23%, 74.09% and 76.57%.</abstract>
			<keywords>disfluencies, conceptual segment, pattern matching, spontaneous Arabic speech</keywords>
		</article>
		<article id="taln-2010-long-022" session="Segmentation">
			<auteurs>
				<auteur>
					<prenom>Camille</prenom>
					<nom>Guinaudeau</nom>
					<email>camille.guinaudeau@irisa.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Guillaume</prenom>
					<nom>Gravier</nom>
					<email>guillaume.gravier@irisa.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Pascale</prenom>
					<nom>Sébillot</nom>
					<email>pascale.sebillot@irisa.fr</email>
					<affiliationId>3</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">INRIA Rennes, France</affiliation>
				<affiliation affiliationId="2">IRISA (CNRS), France</affiliation>
				<affiliation affiliationId="3">IRISA (INSA), France</affiliation>
			</affiliations>
			<titre>Utilisation de relations sémantiques pour améliorer la segmentation thématique de documents télévisuels</titre>
			<type>long</type>
			<pages></pages>
			<resume>Les méthodes de segmentation thématique exploitant une mesure de la cohésion lexicale peuvent être appliquées telles quelles à des transcriptions automatiques de programmes télévisuels. Cependant, elles sont moins efficaces dans ce contexte, ne prenant en compte ni les particularités des émissions TV, ni celles des transcriptions. Nous étudions ici l’apport de relations sémantiques pour rendre les techniques de segmentation thématique plus robustes. Nous proposons une méthode pour exploiter ces relations dans une mesure de la cohésion lexicale et montrons qu’elles permettent d’augmenter la F1-mesure de +1.97 et +11.83 sur deux corpus composés respectivement de 40h de journaux télévisés et de 40h d’émissions de reportage. Ces améliorations démontrent que les relations sémantiques peuvent rendre les méthodes de segmentation moins sensibles aux erreurs de transcription et au manque de répétitions constaté dans certaines émissions télévisées.</resume>
			<mots_cles>Segmentation thématique, documents oraux, cohésion lexicale, relations sémantiques</mots_cles>
			<title></title>
			<abstract>Topic segmentation methods based on a measure of the lexical cohesion can be applied as is to automatic transcripts of TV programs. However, these methods are less effective in this context as neither the specificities of TV contents, nor those of automatic transcripts are considered. The aim of this paper is to study the use of semantic relations to make segmentation techniques more robust.We propose a method to account for semantic relations in a measure of the lexical cohesion.We show that such relations increase the F1-measure by +1.97 and +11.83 for two data sets consisting of respectively 40h of news and 40h of longer reports on current affairs. These results demonstrate that semantic relations can make segmentation methods less sensitive to transcription errors or to the lack of repetitions in some television programs.</abstract>
			<keywords>Topic segmentation, spoken document, lexical cohesion, semantic relations</keywords>
		</article>
		<article id="taln-2010-long-023" session="Segmentation">
			<auteurs>
				<auteur>
					<prenom>Clémentine</prenom>
					<nom>Adam</nom>
					<email>adam@univ-tlse2.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Philippe</prenom>
					<nom>Muller</nom>
					<email>muller@irit.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Cécile</prenom>
					<nom>Fabre</nom>
					<email>cfabre@univ-tlse2.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">CLLE / Université de Toulouse</affiliation>
				<affiliation affiliationId="2">IRIT / Université de Toulouse &amp; Alpage / INRIA</affiliation>
			</affiliations>
			<titre>Une évaluation de l’impact des types de textes sur la tâche de segmentation thématique</titre>
			<type>long</type>
			<pages></pages>
			<resume>Cette étude a pour but de contribuer à la définition des objectifs de la segmentation thématique (ST), en incitant à prendre en considération le paramètre du type de textes dans cette tâche. Notre hypothèse est que, si la ST est certes pertinente pour traiter certains textes dont l’organisation est bien thématique, elle n’est pas adaptée à la prise en compte d’autres modes d’organisation (temporelle, rhétorique), et ne peut pas être appliquée sans précaution à des textes tout-venants. En comparant les performances d’un système de ST sur deux corpus, à organisation thématique "forte" et "faible", nous montrons que cette tâche est effectivement sensible à la nature des textes.</resume>
			<mots_cles>Segmentation thématique, organisation textuelle, cohésion lexicale, voisins distributionnels</mots_cles>
			<title></title>
			<abstract>This paper aims to contribute to a better definition of the requirements of the text segmentation task, by stressing the need for taking into account the types of texts that can be appropriately considered. Our hypothesis is that while TS is indeed relevant to analyse texts with a thematic organisation, this task is ill-fitted to deal with other modes of text organisation (temporal, rhetorical, etc.). By comparing the performance of a TS system on two corpora, with either a "strong" or a "weak" thematic organisation, we show that TS is sensitive to text types.</abstract>
			<keywords>Text segmentation, textual organisation, lexical cohesion, distributional neighbours</keywords>
		</article>
		<article id="taln-2010-long-024" session="Segmentation">
			<auteurs>
				<auteur>
					<prenom>Ludovic</prenom>
					<nom>Jean-Louis</nom>
					<email>ludovic.jean-louis@cea.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Romaric</prenom>
					<nom>Besançon</nom>
					<email>romaric.besancon@cea.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Olivier</prenom>
					<nom>Ferret</nom>
					<email>olivier.ferret@cea.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">CEA, LIST, Laboratoire Vision et Ingénierie des Contenus, Fontenay-aux-Roses, F-92265, France</affiliation>
			</affiliations>
			<titre>Utilisation d’indices temporels pour la segmentation événementielle de textes</titre>
			<type>long</type>
			<pages></pages>
			<resume>Dans le domaine de l’Extraction d’Information, une place importante est faite à l’extraction d’événements dans des dépêches d’actualité, particulièrement justifiée dans le contexte d’applications de veille. Or il est fréquent qu’une dépêche d’actualité évoque plusieurs événements de même nature pour les comparer. Nous proposons dans cet article d’étudier des méthodes pour segmenter les textes en séparant les événements, dans le but de faciliter le rattachement des informations pertinentes à l’événement principal. L’idée est d’utiliser des modèles d’apprentissage statistique exploitant les marqueurs temporels présents dans les textes pour faire cette segmentation. Nous présentons plus précisément deux modèles (HMM et CRF) entraînés pour cette tâche et, en faisant une évaluation de ces modèles sur un corpus de dépêches traitant d’événements sismiques, nous montrons que les méthodes proposées permettent d’obtenir des résultats au moins aussi bons que ceux d’une approche ad hoc, avec une approche beaucoup plus générique.</resume>
			<mots_cles>Extraction d’information, extraction d’événements, segmentation de textes, indices temporels, apprentissage statistique</mots_cles>
			<title></title>
			<abstract>One of the early application of Information Extraction, motivated by the needs for intelligence tools, is the detection of events in news articles. But this detection may be difficult when news articles mention several occurrences of events of the same kind, which is often done for comparison purposes. We propose in this article new approaches to segment the text of news articles in units relative to only one event, in order to help the identification of relevant information associated to the main event of the news. We present two approaches that use statistical machine learning models (HMM and CRF) exploiting temporal information extracted from the texts as a basis for this segmentation. The evaluation of these approaches in the domain of seismic events show that with a robust and generic approach, we can achieve results at least as good as results obtained with an ad hoc approach.</abstract>
			<keywords>Information extraction, event extraction, text segmentation, temporal cues, statistical machine learning</keywords>
		</article>
		<article id="taln-2010-long-025" session="Résumé/Extraction">
			<auteurs>
				<auteur>
					<prenom>Juan-Manuel</prenom>
					<nom>Torres-Moreno</nom>
					<email>juan-manuel.torres@univ-avignon.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Horacio</prenom>
					<nom>Saggion</nom>
					<email>horacio.saggion@upf.edu</email>
					<affiliationId>3</affiliationId>
				</auteur>
				<auteur>
					<prenom>Iria</prenom>
					<nom>da Cunha</nom>
					<email>iria.dacunha@upf.edu</email>
					<affiliationId>1</affiliationId>
					<affiliationId>4</affiliationId>
				</auteur>
				<auteur>
					<prenom>Patricia</prenom>
					<nom>Velázquez-Morales</nom>
					<email></email>
					<affiliationId>5</affiliationId>
				</auteur>
				<auteur>
					<prenom>Eric</prenom>
					<nom>Sanjuan</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIA, Université d’Avignon et des Pays de Vaucluse, Avignon, France</affiliation>
				<affiliation affiliationId="2">Ecole Polytechnique de Montréal, (Québec) Canada</affiliation>
				<affiliation affiliationId="3">DTIC, Universtitat Pompeu Fabra, Barcelona, Espagne</affiliation>
				<affiliation affiliationId="4">IULA, Universitat Pompeu Fabra, Barcelona, Espagne</affiliation>
				<affiliation affiliationId="5">VM Labs, Avignon, France</affiliation>
			</affiliations>
			<titre>Évaluation automatique de résumés avec et sans référence</titre>
			<type>long</type>
			<pages></pages>
			<resume>Nous étudions différentes méthodes d’évaluation de résumé de documents basées sur le contenu. Nous nous intéressons en particulier à la corrélation entre les mesures d’évaluation avec et sans référence humaine. Nous avons développé FRESA, un nouveau système d’évaluation fondé sur le contenu qui calcule les divergences entre les distributions de probabilité. Nous appliquons notre système de comparaison aux diverses mesures d’évaluation bien connues en résumé de texte telles que la Couverture, Responsiveness, Pyramids et Rouge en étudiant leurs associations dans les tâches du résumé multi-document générique (francais/anglais), focalisé (anglais) et résumé mono-document générique (français/espagnol).</resume>
			<mots_cles>Mesures d’évaluation, Résumé automatique de textes</mots_cles>
			<title></title>
			<abstract>We study document-summary content-based evaluation methods in text summarization and we investigate the correlation among evaluation measures with and without human models. We apply our comparison framework to various well-established content-based evaluation measures in text summarization such as Coverage, Responsiveness, Pyramids and Rouge studying their associations in various text summarization tasks including generic (English/French) and focus-based (English) multi-document summarization and generic multi and single-document summarization (French/Spanish). The research is carried out using the new content-based evaluation framework FRESA to compute the divergences among probability distributions.</abstract>
			<keywords>Evaluation measures, Text Automatic Summarization</keywords>
		</article>
		<article id="taln-2010-long-026" session="Résumé/Extraction">
			<auteurs>
				<auteur>
					<prenom>Pierre-Etienne</prenom>
					<nom>Genest</nom>
					<email>genestpe@iro.umontreal.ca</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Guy</prenom>
					<nom>Lapalme</nom>
					<email>lapalme@iro.umontreal.ca</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Mehdi</prenom>
					<nom>Yousfi-Monod</nom>
					<email>yousfim@iro.umontreal.ca</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">RALI-DIRO, Université de Montréal, B.P. 6128, Centre-Ville, Montréal, Québec, Canada, H3C 3J7</affiliation>
			</affiliations>
			<titre>Jusqu’où peut-on aller avec les méthodes par extraction pour la rédaction de résumés?</titre>
			<type>long</type>
			<pages></pages>
			<resume>La majorité des systèmes de résumés automatiques sont basés sur l’extraction de phrases, or on les compare le plus souvent avec des résumés rédigés manuellement par abstraction. Nous avons mené une expérience dans le but d’établir une limite supérieure aux performances auxquelles nous pouvons nous attendre avec une approche par extraction. Cinq résumeurs humains ont composé 88 résumés de moins de 100 mots, en extrayant uniquement des phrases présentes intégralement dans les documents d’entrée. Les résumés ont été notés sur la base de leur contenu, de leur niveau linguistique et de leur qualité globale par les évaluateurs de NIST dans le cadre de la compétition TAC 2009. Ces résumés ont obtenus de meilleurs scores que l’ensemble des 52 systèmes automatiques participant à la compétition, mais de nettement moins bons que ceux obtenus par les résumeurs humains pouvant formuler les phrases de leur choix dans le résumé. Ce grand écart montre l’insuffisance des méthodes par extraction pure.</resume>
			<mots_cles>Résumés automatiques, résumés par extraction, résumés manuels</mots_cles>
			<title></title>
			<abstract>The majority of automatic summarization systems are based on sentence extraction, whereas they are usually compared with human-written, abstractive summaries. We have thus conducted an experiment to establish an upper bound on the expected performance of extractive summarization. 5 human summarizers completed 88 summaries of no more than 100 words from unedited sentences of the source documents. The summaries were scored based on their content, linguistic quality and overall responsiveness by NIST annotators in the context of the TAC 2009 competition. The human extracts received better scores than all of the 52 participating automatic systems, but much lower scores than those obtained by human summarizers free to use abstraction. This large gap shows that pure extraction methods are insufficient for summarization.</abstract>
			<keywords>Automatic summarization, extractive summarization, manual summarization</keywords>
		</article>
		<article id="taln-2010-long-027" session="Résumé/Extraction">
			<auteurs>
				<auteur>
					<prenom>Anne</prenom>
					<nom>Garcia-Fernandez</nom>
					<email>annegf@limsi.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Sophie</prenom>
					<nom>Rosset</nom>
					<email>rosset@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Anne</prenom>
					<nom>Vilnat</nom>
					<email>vilnat@limsi.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIMSI-CNRS B.P. 133 91403 Orsay Cedex</affiliation>
				<affiliation affiliationId="2">Université Paris Sud 11 Orsay</affiliation>
			</affiliations>
			<titre>Comment formule-t-on une réponse en langue naturelle ?</titre>
			<type>long</type>
			<pages></pages>
			<resume>Cet article présente l’étude d’un corpus de réponses formulées par des humains à des questions factuelles. Des observations qualitatives et quantitatives sur la reprise d’éléments de la question dans les réponses sont exposées. La notion d’information-réponse est introduite et une étude de la présence de cet élément dans le corpus est proposée. Enfin, les formulations des réponses sont étudiées.</resume>
			<mots_cles>systèmes de réponse à une question, variations linguistiques, réponse en langue naturelle, oral et écrit</mots_cles>
			<title></title>
			<abstract>This paper presents the study of a corpus of human answers to factual questions. Observations of how and how much question elements are used in the answer are done. We define the concept of “information-answer” and study its presence in the corpus. Finally, answer formulations are shown.</abstract>
			<keywords>question-answering systems, linguistics variations, natural language answer, oral and written</keywords>
		</article>
		<article id="taln-2010-long-028" session="Traduction">
			<auteurs>
				<auteur>
					<prenom>Thi Ngoc Diep</prenom>
					<nom>Do</nom>
					<email>thi-ngoc-diep.do@imag.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Laurent</prenom>
					<nom>Besacier</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Eric</prenom>
					<nom>Castelli</nom>
					<email></email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Laboratoire LIG, GETALP, Grenoble, France</affiliation>
				<affiliation affiliationId="2">Centre MICA, CNRS/UMI-2954, Hanoi, Vietnam</affiliation>
			</affiliations>
			<titre>Apprentissage non supervisé pour la traduction automatique : application à un couple de langues peu doté</titre>
			<type>long</type>
			<pages></pages>
			<resume>Cet article présente une méthode non-supervisée pour extraire des paires de phrases parallèles à partir d’un corpus comparable. Un système de traduction automatique est utilisé pour exploiter le corpus comparable et détecter les paires de phrases parallèles. Un processus itératif est exécuté non seulement pour augmenter le nombre de paires de phrases parallèles extraites, mais aussi pour améliorer la qualité globale du système de traduction. Une comparaison avec une méthode semi-supervisée est présentée également. Les expériences montrent que la méthode non-supervisée peut être réellement appliquée dans le cas où on manque de données parallèles. Bien que les expériences préliminaires soient menées sur la traduction français-anglais, cette méthode non-supervisée est également appliquée avec succès à un couple de langues peu doté : vietnamien-français.</resume>
			<mots_cles>apprentissage non-supervisé, système de traduction automatique, corpus comparable, paires de phrases parallèles</mots_cles>
			<title></title>
			<abstract>This paper presents an unsupervised method for extracting parallel sentence pairs from a comparable corpus. A translation system is used to mine and detect the parallel sentence pairs from the comparable corpus. An iterative process is implemented not only to increase the number of extracted parallel sentence pairs but also to improve the overall quality of the translation system. A comparison between this unsupervised method and a semi-supervised method is also presented. The experiments conducted show that the unsupervised method can be really applied in cases where parallel data are not available. While preliminary experiments are conducted on French-English translation, this unsupervised method is also applied successfully to a low e-resourced language pair (Vietnamese-French).</abstract>
			<keywords>unsupervised training, machine translation, comparable corpus, parallel sentence pairs</keywords>
		</article>
		<article id="taln-2010-long-029" session="Traduction">
			<auteurs>
				<auteur>
					<prenom>Ahmed</prenom>
					<nom>El Kholy</nom>
					<email>akholy@ccls.columbia.edu</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Nizar</prenom>
					<nom>Habash</nom>
					<email>habash@ccls.columbia.edu</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Center for Computational Learning Systems, Columbia University</affiliation>
			</affiliations>
			<titre></titre>
			<type>long</type>
			<pages></pages>
			<resume>De nombreux travaux en Traduction Automatique Statistique (TAS) pour des langues d’entrée morphologiquement riches montrent que la ségmentation morphologique et la normalisation orthographique améliorent la qualité des traductions en diminuant la sparsité des données. Dans cet article, nous étudions l’impact de ce prétraitement pour la TAS vers une langue de sortie riche morphologiquement, comme l’Arabe. Nous explorons l’espace des schémas de segmentation et des options de normalisation possibles. Nous évaluons seulement la sortie sous une forme désegmentée et enrichie orthographiquement. Nos résultats montrent d’une part que le meilleur schéma pour la ségmentation est celui de la Penn Arabic Treebank. D’autre part, la meilleure procédure de prétraitement consiste à entraîner le système sur des données normalisées orthographiquement, puis à enrichir et désegmenter les traductions en sortie.</resume>
			<mots_cles>Langue Arabe, Morphologie, Ségmentation, Désegmentation, La Traduction Automatique Statistique</mots_cles>
			<title>Orthographic and Morphological Processing for English-Arabic Statistical Machine Translation</title>
			<abstract>Much of the work on Statistical Machine Translation (SMT) from morphologically rich languages has shown that morphological tokenization and orthographic normalization help improve SMT quality because of the sparsity reduction they contribute. In this paper, we study the effect of these processes on SMT when translating into a morphologically rich language, namely Arabic.We explore a space of tokenization schemes and normalization options. We only evaluate on detokenized and orthographically correct (enriched) output. Our results show that the best performing tokenization scheme is that of the Penn Arabic Treebank. Additionally, training on orthographically normalized (reduced) text then jointly enriching and detokenizing the output outperforms training on enriched text.</abstract>
			<keywords>Arabic Language, Morphology, Tokenization, Detokenization, Statistical Machine Translation</keywords>
		</article>
		<article id="taln-2010-long-030" session="Traduction">
			<auteurs>
				<auteur>
					<prenom>Marine</prenom>
					<nom>Carpuat</nom>
					<email>marine@ccls.columbia.edu</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Yuval</prenom>
					<nom>Marton</nom>
					<email>ymarton@ccls.columbia.edu</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Nizar</prenom>
					<nom>Habash</nom>
					<email>habash@ccls.columbia.edu</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Columbia University, Center for Computational Learning Systems, 475 Riverside Drive, New York, NY 10115</affiliation>
			</affiliations>
			<titre></titre>
			<type>long</type>
			<pages></pages>
			<resume>Distinguer les constructions verbe-sujet (VS) des propositions principales (“matrice”) et subordonnées (“non-matrice”) améliore notre nouveau modèle de réordonnancement pour l’alignement des mots en Traduction Automatique Statistique (TAS) arabe-anglais (Carpuat et al., 2010). D’une part, la majorité des constructions verbe-sujet (VS) dans les propositions principales doivent être réordonnancées en anglais, alors que l’ordre du verbe et du sujet est préservé dans la moitié des cas de constructions VS subordonnées. D’autre part, nous constatons que notre analyseur syntaxique parvient à mieux identifier les constructions VS des propositions principales. Ces observations nous amènent à limiter le réordonnancement des constructions VS à celles des propositions principales lors de l’alignement des mots. Cette technique améliore substantiellement la performance d’un système de TAS conventionnel, et d’un système qui réordonnance toutes les constructions VS. L’amélioration des mesures BLEU et TER obtenue par simple réordonnancement représente presque la moitié de l’amélioration obtenue lorsque le modèle d’alignement des mots est entraîné sur un corpus parallèle d’une taille cinq fois supérieure.</resume>
			<mots_cles>Analyse morpho-syntaxique de l’arabe, Traduction automatique statistique, VS, VSO</mots_cles>
			<title>Reordering Matrix Post-verbal Subjects for Arabic-to-English SMT</title>
			<abstract>We improve our recently proposed technique for integrating Arabic verb-subject constructions in SMT word alignment (Carpuat et al., 2010) by distinguishing between matrix (or main clause) and non-matrix Arabic verb-subject constructions. In gold translations, most matrix VS (main clause verb-subject) constructions are translated in inverted SV order, while non-matrix (subordinate clause) VS constructions are inverted in only half the cases. In addition, while detecting verbs and their subjects is a hard task, our syntactic parser detects VS constructions better in matrix than in non-matrix clauses. As a result, reordering only matrix VS for word alignment consistently improves translation quality over a phrase-based SMT baseline, and over reordering all VS constructions, in both medium- and large-scale settings. In fact, the improvements obtained by reordering matrix VS on the medium-scale setting remarkably represent 44% of the gain in BLEU and 51% of the gain in TER obtained with a word alignment training bitext that is 5 times larger.</abstract>
			<keywords></keywords>
		</article>
		<article id="taln-2010-long-031" session="Table ronde">
			<auteurs>
				<auteur>
					<prenom>Michael</prenom>
					<nom>Zock</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Guy</prenom>
					<nom>Lapalme</nom>
					<email></email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Equipe TALEP, LIF, CNRS, UMR 6166, Case 901 - 163 Avenue de Luminy, F-13288 Marseille Cedex 9</affiliation>
				<affiliation affiliationId="2">RALI-DIRO, Université de Montréal, CP 6128, Succ. Centre-Ville, Montréal, Québec, Canada H3C 3J7</affiliation>
			</affiliations>
			<titre>Du TAL au TIL</titre>
			<type>long</type>
			<pages></pages>
			<resume>Historiquement deux types de traitement de la langue ont été étudiés: le traitement par le cerveau (approche psycholinguistique) et le traitement par la machine (approche TAL). Nous pensons qu’il y a place pour un troisième type: le traitement interactif de la langue (TIL), l’ordinateur assistant le cerveau. Ceci correspond à un besoin réel dans la mesure où les gens n’ont souvent que des connaissances partielles par rapport au problème à résoudre. Le but du TIL est de construire des ponts entre ces connaissances momentanées d’un utilisateur et la solution recherchée. À l'aide de quelques exemples, nous essayons de montrer que ceci est non seulement faisable et souhaitable, mais également d’un coût très raisonnable.</resume>
			<mots_cles>traitement interactif de la langue, prise en compte de l'usager, outils de traitement de la langue, apprentissage des langues, dictionnaires, livres de phrases, concordanciers, traduction</mots_cles>
			<title></title>
			<abstract>Historically two types of NLP have been investigated: fully automated processing of language by machines (NLP) and autonomous processing of natural language by people, i.e. the human brain (psycholinguistics). We believe that there is room and need for another kind, INLP: interactive natural language processing. This intermediate approach starts from peoples’ needs, trying to bridge the gap between their actual knowledge and a given goal. Given the fact that peoples’ knowledge is variable and often incomplete, the aim is to build bridges linking a given knowledge state to a given goal. We present some examples, trying to show that this goal is worth pursuing, achievable and at a reasonable cost.</abstract>
			<keywords>interactive NLP (INLP), user interaction, tools for NLP, language learning, dictionaries, phrase books, concordancers, translation</keywords>
		</article>
		<article id="taln-2010-long-032" session="Plénière">
			<auteurs>
				<auteur>
					<prenom>Piet</prenom>
					<nom>Mertens</nom>
					<email>Piet.Mertens@arts.kuleuven.be</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Département de Linguistique, Université de Leuven, Belgique</affiliation>
			</affiliations>
			<titre>Restrictions de sélection et réalisations syntagmatiques dans DICOVALENCE Conversion vers un format utilisable en TAL</titre>
			<type>long</type>
			<pages></pages>
			<resume>Cet article décrit des modifications du dictionnaire de valence des verbes du français DICOVALENCE qui visent à le rendre neutre par rapport aux modèles syntaxiques, à expliciter certaines informations sur le cadre de sous-catégorisation et à le rendre ainsi directement utilisable en TAL. Les informations explicitées sont les suivantes : (a) les fonctions syntaxiques des arguments verbaux, (b) les restrictions de sélection portant sur ces arguments et (c) leurs réalisations syntagmatiques possibles. Les restrictions sont exprimées à l’aide de traits sémantiques. L’article décrit aussi le calcul de ces traits sémantiques à partir des paradigmes des pronoms (et d’éléments similaires) associés aux arguments. On obtient un format indépendant du modèle syntaxique, dont l’interprétation est transparente.</resume>
			<mots_cles>lexiques syntaxiques, restrictions de sélection, traits sémantiques</mots_cles>
			<title></title>
			<abstract>This paper describes modifications to the verbal valency dictionary for French, known as DICOVALENCE, in order to obtain a theory-independent syntactic lexicon, to make explicit certain information about the slots in the valency frame, to facilitate the use of the lexicon in natural language processing. The modifications make explicit the following aspects: (a) the syntactic function of the slots, (b) the selection restrictions on these verbal arguments, (c) their possible phrasal realizations. Selection restrictions are expressed using semantic features. The article describes how these features are obtained from the paradigms of pronouns (and similar elements) associated with the valency slots. This results in a format which is theoryindependent, with a transparent interpretation.</abstract>
			<keywords>lexical databases, selection restrictions, semantic features</keywords>
		</article>
		<article id="taln-2010-long-033" session="Plénière">
			<auteurs>
				<auteur>
					<prenom>Caroline</prenom>
					<nom>Barrière</nom>
					<email>caroline.barriere@nrc-cnrc.gc.ca</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">ITI-CNR, Gatineau, Canada</affiliation>
			</affiliations>
			<titre>Recherche contextuelle d’équivalents en banque de terminologie</titre>
			<type>long</type>
			<pages></pages>
			<resume>Notre recherche démontre que l’utilisation du contenu d’un texte à traduire permet de mieux cibler dans une banque de terminologie les équivalents terminologiques pertinents à ce texte. Une banque de terminologie a comme particularité qu’elle catégorise ses entrées (fiches) en leur assignant un ou des domaines provenant d’une liste de domaines préétablie. La stratégie ici présentée repose sur l’utilisation de cette information sur les domaines. Un algorithme a été développé pour l’assignation automatique d’un profil de domaines à un texte. Celui-ci est combiné à un algorithme d’appariement entre les domaines d’un terme présent dans la banque de terminologie et le profil de domaines du texte. Pour notre expérimentation, des résumés bilingues (français et anglais) provenant de huit revues scientifiques nous fournissent un ensemble de 1130 paires d’équivalents terminologiques et le Grand Dictionnaire Terminologique (Office Québécois de la Langue Française) nous sert de ressource terminologique. Sur notre ensemble, nous démontrons une réduction de 75% du rang moyen de l’équivalent correct en comparaison avec un choix au hasard.</resume>
			<mots_cles>recherche contextuelle, équivalents terminologiques, banque de terminologie, désambiguïsation par domaine</mots_cles>
			<title></title>
			<abstract>Our research shows the usefulness of taking into account the context of a term within a text to be translated to better find an appropriate term equivalent for it in a term bank. A term bank has the particularity of categorising its records by assigning them one or more domains from a pre-established list of domains. The strategy presented here uses this domain information. An algorithm has been developed to automatically assign a domain profile to a source text. It is then combined with another algorithm which finds a match between a term’s domains (as found in the term bank) and the text’s domain profile. For our experimentation, bilingual abstracts (French-English) from eight scientific journals provide 1130 pairs of term equivalents. The Grand Dictionnaire Terminologique (Office Québécois de la Langue Française) is used as a terminological ressource. On our data set, we show a reduction of 75% in the average rank of the correct equivalent, in comparison to a random choice.</abstract>
			<keywords>contextual search, term equivalents, term bank, domain-based disambiguation</keywords>
		</article>
		<article id="taln-2010-long-034" session="Plénière">
			<auteurs>
				<auteur>
					<prenom>Guillaume</prenom>
					<nom>Bonfante</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Bruno</prenom>
					<nom>Guillaume</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Mathieu</prenom>
					<nom>Morey</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Guy</prenom>
					<nom>Perrier</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">INRIA Nancy-Grand Est - LORIA - Nancy-Université</affiliation>
			</affiliations>
			<titre>Réécriture de graphes de dépendances pour l’interface syntaxe-sémantique</titre>
			<type>long</type>
			<pages></pages>
			<resume>Nous définissons le beta-calcul, un calcul de réécriture de graphes, que nous proposons d’utiliser pour étudier les liens entre différentes représentations linguistiques. Nous montrons comment transformer une analyse syntaxique en une représentation sémantique par la composition de deux jeux de règles de beta-calcul. Le premier souligne l’importance de certaines informations syntaxiques pour le calcul de la sémantique et explicite le lien entre syntaxe et sémantique sous-spécifiée. Le second décompose la recherche de modèles pour les représentations sémantiques sous-spécifiées.</resume>
			<mots_cles>Dépendances, réécriture de graphes, interface syntaxe-sémantique, DMRS</mots_cles>
			<title></title>
			<abstract>We define the beta-calculus, a graph-rewriting calculus, which we propose to use to study the links between different linguistic representations. We show how to transform a syntactic analysis into a semantic analysis via the composition of two sets of beta-calculus rules. The first one underlines the importance of some syntactic information to compute the semantics and clearly expresses the link between syntax and underspecified semantics. The second one breaks up the search for models of underspecified semantic representations.</abstract>
			<keywords>Dependencies, graph rewriting, syntax-semantics interface, DMRS</keywords>
		</article>
		<article id="taln-2010-long-035" session="Corpus">
			<auteurs>
				<auteur>
					<prenom>Karën</prenom>
					<nom>Fort</nom>
					<email>karen.fort@inist.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Claire</prenom>
					<nom>François</nom>
					<email>claire.francois@inist.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Maha</prenom>
					<nom>Ghribi</nom>
					<email>maha.ghribi@inist.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">INIST / CNRS, 2 allée de Brabois, 54500 Vandoeuvre-lès-Nancy</affiliation>
				<affiliation affiliationId="2">LIPN, Université Paris 13 &amp; CNRS, 99 av. J.B. Clément, 93430 Villetaneuse</affiliation>
			</affiliations>
			<titre>Évaluer des annotations manuelles dispersées : les coefficients sont-ils suffisants pour estimer l’accord inter-annotateurs ?</titre>
			<type>long</type>
			<pages></pages>
			<resume>L’objectif des travaux présentés dans cet article est l’évaluation de la qualité d’annotations manuelles de relations de renommage de gènes dans des résumés scientifiques, annotations qui présentent la caractéristique d’être très dispersées. Pour cela, nous avons calculé et comparé les coefficients les plus communément utilisés, entre autres kappa (Cohen, 1960) et pi (Scott, 1955), et avons analysé dans quelle mesure ils sont adaptés à nos données. Nous avons également étudié les différentes pondérations applicables à ces coefficients permettant de calculer le kappa pondéré (Cohen, 1968) et l’alpha (Krippendorff, 1980, 2004). Nous avons ainsi étudié le biais induit par la grande prévalence d’une catégorie et défini un mode de calcul des distances entre catégories reposant sur les annotations réalisées.</resume>
			<mots_cles>Annotation manuelle, évaluation, accord inter-annotateurs</mots_cles>
			<title></title>
			<abstract>This article details work aiming at evaluating the quality of the manual annotation of gene renaming relations in scientific abstracts, which generates sparse annotations. To evaluate these annotations, we computed and compared the results obtained using the commonly advocated inter-annotator agreement coefficients such as kappa (Cohen, 1960) or pi (Scott, 1955) and analyzed to which extent they are relevant for our data.We also studied the different weighting computations applicable to kappa! (Cohen, 1968) and alpha (Krippendorff, 1980, 2004) and estimated the bias introduced by prevalence. We then define a way to compute distances between categories based on the produced annotations.</abstract>
			<keywords>Manual annotation, evaluation, inter-annotator agreement</keywords>
		</article>
		<article id="taln-2010-long-036" session="Corpus">
			<auteurs>
				<auteur>
					<prenom>Phuong</prenom>
					<nom>Le-Hong</nom>
					<email>lehong@loria.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Azim</prenom>
					<nom>Roussanaly</nom>
					<email>azim@loria.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Thi Minh Huyen</prenom>
					<nom>Nguyen</nom>
					<email>huyenntm@vnu.edu.vn</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Mathias</prenom>
					<nom>Rossignol</nom>
					<email>mathias.rossignol@gmail.com</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LORIA, Nancy, France</affiliation>
				<affiliation affiliationId="2">Hanoi University of Science, Hanoi, Vietnam</affiliation>
			</affiliations>
			<titre></titre>
			<type>long</type>
			<pages></pages>
			<resume>Nous présentons dans cet article une étude empirique de l’application de l’approche de l’entropie maximale pour l’étiquetage syntaxique de textes vietnamiens. Le vietnamien est une langue qui possède des caractéristiques spéciales qui la distinguent largement des langues occidentales. Notremeilleur étiqueteur explore et inclut des connaissances utiles qui, en terme de performance pour l’étiquetage de textes vietnamiens, fournit un taux de précision globale de 93.40% et de 80.69% pour les mots inconnus sur un ensemble de test du corpus arboré vietnamien. Notre étiqueteur est nettement supérieur à celui qui est en train d’être utilisé pour développer le corpus arboré vietnamien, et à l’heure actuelle c’est le meilleur résultat obtenu pour l’étiquetage de textes vietnamiens.</resume>
			<mots_cles>Etiqueteur syntaxique, entropie maximale, texte, vietnamien</mots_cles>
			<title>An empirical study of maximum entropy approach for part-of-speech tagging of Vietnamese texts</title>
			<abstract>This paper presents an empirical study on the application of the maximum entropy approach for part-of-speech tagging of Vietnamese text, a language with special characteristics which largely distinguish it from occidental languages. Our best tagger explores and includes useful knowledge sources for tagging Vietnamese text and gives a 93.40%overall accuracy and a 80.69%unknown word accuracy on a test set of the Vietnamese treebank. Our tagger significantly outperforms the tagger that is being used for building the Vietnamese treebank, and as far as we are aware, this is the best tagging result ever published for the Vietnamese language.</abstract>
			<keywords>Part-of-speech tagger, maximum entropy, text, Vietnamese</keywords>
		</article>
		<article id="taln-2010-long-037" session="Corpus">
			<auteurs>
				<auteur>
					<prenom>Lyne</prenom>
					<nom>Da Sylva</nom>
					<email>lyne.da.sylva@umontreal.ca</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Ecole de bibliothéconomie et des sciences de l'information, Université de Montréal, C.P. 6128, succ. Centre-ville, Montréal, Canada H3C 3J7</affiliation>
			</affiliations>
			<titre>Extraction semi-automatique d’un vocabulaire savant de base pour l’indexation automatique</titre>
			<type>long</type>
			<pages></pages>
			<resume>Le projet décrit vise à soutenir les efforts de constitution de ressources lexicales utiles à l’indexation automatique. Un type de vocabulaire utile à l’indexation est défini, le vocabulaire savant de base, qui peut s’articuler avec le vocabulaire spécialisé pour constituer des entrées d’index structurées. On présente les résultats d’ une expérimentation d’ extraction (semi-)automatique des mots du vocabulaire savant de base à partir d’un corpus ciblé, constitué de résumés d’articles scientifiques en français et en anglais. La tâche d’extraction a réussi à doubler une liste originale constituée manuellement pour le français. La comparaison est établie avec une expérimentation similaire effectuée pour l’anglais sur un corpus plus grand et contenant des résumés d’articles non seulement en sciences pures mais aussi en sciences humaines et sociales.</resume>
			<mots_cles>classes de vocabulaire, indexation automatique, extraction automatique, corpus, approche basée sur les corpus, vocabulaire savant de base, ressources lexicales, français</mots_cles>
			<title></title>
			<abstract>This project aims to help develop lexical resources useful for automatic indexing. A type of useful vocabulary for indexing is defined, the basic scholarly vocabulary, which can combine with specialized vocabulary items to form evocative, structured index entries. The article presents the results of an experiment of (semi-)automatic extraction of the basic scholarly vocabulary lexical items from a large corpus. The corpus is especially suited to the task; it consists of abstracts of scientific articles in French and English. The extraction task was successful in doubling the size of a previously manually compiled list. A comparison is made with a similar experiment conducted for English on a larger corpus which also contained summaries of articles in the humanities and social sciences.</abstract>
			<keywords>vocabulary classes, automatic indexing, automatic extraction, corpus, corpus-based approach, basic scholarly vocabulary, lexical resources, French</keywords>
		</article>
		<article id="taln-2010-long-038" session="Morphologie">
			<auteurs>
				<auteur>
					<prenom>Jean-François</prenom>
					<nom>Lavallée</nom>
					<email>lavalljf@iro.umontreal.ca</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Philippe</prenom>
					<nom>Langlais</nom>
					<email>felipe@iro.umontreal.ca</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université de Montréal, C.P. 6128, succ. Centre-ville, Montréal, Qc, Canada, H3C 3J7</affiliation>
			</affiliations>
			<titre>Apprentissage non supervisé de la morphologie d’une langue par généralisation de relations analogiques</titre>
			<type>long</type>
			<pages></pages>
			<resume>Bien que les approches fondées sur la théorie de l’information sont prédominantes dans le domaine de l’analyse morphologique non supervisée, depuis quelques années, d’autres approches ont gagné en popularité, dont celles basées sur l’analogie formelle. Cette dernière reste tout de même marginale due notamment à son coût de calcul élevé. Dans cet article, nous proposons un algorithme basé sur l’analogie formelle capable de traiter les lexiques volumineux. Nous introduisons pour cela le concept de règle de cofacteur qui permet de généraliser l’information capturée par une analogie tout en contrôlant les temps de traitement. Nous comparons notre système à 2 systèmes : Morfessor (Creutz &amp; Lagus, 2005), un système de référence dans de nombreux travaux sur l’analyse morphologique et le système analogique décrit par Langlais (2009). Nous en montrons la supériorité pour 3 des 5 langues étudiées ici : le finnois, le turc, et l’allemand.</resume>
			<mots_cles>Analyse morphologique non supervisée, Analogie formelle, Approche à base de graphe</mots_cles>
			<title></title>
			<abstract>Although approaches based on information theory are prominent in the field of unsupervised morphological analysis, in recent years, other approaches have gained in popularity. Those based on formal analogy remain marginal partly because of their high computational cost. In this paper we propose an algorithm based on formal analogy able to handle large lexicons. We introduce the concept of cofactor rule which allows the generalization of the information captured by analogy, while controlling the processing time. We compare our system to 2 others : Morfessor (Creutz &amp; Lagus, 2005), a reference in many studies on morphological analysis and the analogical system described by Langlais (2009). We show the superiority of our approach for 3 out of the 5 languages studied here : Finnish, Turkish, and German.</abstract>
			<keywords>Unsupervised Learning of Morphology, Formal Analogy, Graph-Based Approach</keywords>
		</article>
		<article id="taln-2010-long-039" session="Morphologie">
			<auteurs>
				<auteur>
					<prenom>Vincent</prenom>
					<nom>Claveau</nom>
					<email>Vincent.Claveau@irisa.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Ewa</prenom>
					<nom>Kijak</nom>
					<email>Ewa.Kijak@irisa.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">IRISA-CNRS, Campus de Beaulieu, F-35042 Rennes, France</affiliation>
				<affiliation affiliationId="2">IRISA-Univ. Rennes 1, Campus de Beaulieu, F-35042 Rennes, France</affiliation>
			</affiliations>
			<titre>Analyse morphologique en terminologie biomédicale par alignement et apprentissage non-supervisé</titre>
			<type>long</type>
			<pages></pages>
			<resume>Dans le domaine biomédical, beaucoup de termes sont des composés savants (composés de plusieurs racines gréco-latines). L’étude de leur morphologie est importante pour de nombreuses applications puisqu’elle permet de structurer ces termes, de les rechercher efficacement, de les traduire... Dans cet article, nous proposons de suivre une démarche originale mais fructueuse pour mener cette analyse morphologique sur des termes simples en français, en nous appuyant sur une langue pivot, le japonais, et plus précisément sur les termes écrits en kanjis. Pour cela nous avons développé un algorithme d’alignement de termes spécialement adapté à cette tâche. C’est cet alignement d’un terme français avec sa traduction en kanjis qui fournit en même temps une décomposition en morphe et leur étiquetage par les kanjis correspondants. Évalué sur un jeu de données conséquent, notre approche obtient une précision supérieure à 70% et montrent son bien fondé en comparaison avec les techniques existantes. Nous illustrons également l’intérêt de notre démarche au travers de deux applications directes de ces alignements : la traduction de termes inconnus et la découverte de relations entre morphes pour la tructuration terminologique.</resume>
			<mots_cles>Alignement, terminologie, morphologie, analogie, traduction de terme, kanji</mots_cles>
			<title></title>
			<abstract>In the biomedical domain, many terms are neoclassical compounds (composed of several Greek or Latin roots). The study of their morphology is important for numerous applications since it makes it possible to structure them, retrieve them efficiently, translate them... In this paper, we propose an original yet fruitful approach to carry out this morphological analysis by relying on Japanese, more precisely on terms written in kanjis, as a pivot language. In order to do so, we have developed a specially crafted alignment algorithm. This alignment process of French terms with their kanji-based counterparts provides at the same time a decomposition of the French term into morphs, and a kanji label for each morph. Evaluated on a big dataset, our approach yields a precision greater than 70% and shows its the relevance compared with existing techniques. We also illustrate the validity of our reasoning through two direct applications of the produced alignments: translation of unknown terms and discovering of relationships between morphs for terminological structuring.</abstract>
			<keywords>Alignment, terminology, morphology, analogy, term translation, kanji</keywords>
		</article>
		<article id="taln-2010-long-040" session="Morphologie">
			<auteurs>
				<auteur>
					<prenom>Benoît</prenom>
					<nom>Sagot</nom>
					<email>benoit.sagot@inria.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Géraldine</prenom>
					<nom>Walther</nom>
					<email>geraldine.walther@linguist.jussieu.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Alpage, INRIA Paris–Rocquencourt &amp; Université Paris 7, Rocquencourt, BP 105, 78153 Le Chesnay Cedex, France</affiliation>
				<affiliation affiliationId="2">LLF, Université Paris 7, 30 rue du Château des Rentiers, 75013 Paris, France</affiliation>
			</affiliations>
			<titre>Développement de ressources pour le persan: lexique morphologique et chaîne de traitements de surface</titre>
			<type>long</type>
			<pages></pages>
			<resume>Nous présentons PerLex, un lexique morphologique du persan à large couverture et librement disponible, accompagné d’une chaîne de traitements de surface pour cette langue. Nous décrivons quelques caractéristiques de la morphologie du persan, et la façon dont nous l’avons représentée dans le formalisme lexical Alexina, sur lequel repose PerLex. Nous insistons sur la méthodologie que nous avons employée pour construire les entrées lexicales à partir de diverses sources, ainsi que sur les problèmes liés à la normalisation typographique. Le lexique obtenu a une couverture satisfaisante sur un corpus de référence, et devrait donc constituer un bon point de départ pour le développement d’un lexique syntaxique du persan.</resume>
			<mots_cles>Lexique morphologique, Persan, Développement de lexiques, Traitements de surface</mots_cles>
			<title></title>
			<abstract>We introduce PerLex, a large-coverage and freely-available morphological lexicon for the Persian language, as well as a corresponding surface processing chain. We describe the main features of the Persian morphology, and the way we have represented it within the Alexina formalism, on which PerLex is based. We focus on the methodology we used for constructing lexical entries from various sources, as well as on the problems related to typographic normalisation. The resulting lexicon shows a satisfying coverage on a reference corpus and should therefore be a good starting point for developing a syntactic lexicon for the Persian language.</abstract>
			<keywords>Morphological lexicon, Persian language, Lexical development, Surface processing.</keywords>
		</article>
		<article id="taln-2010-court-001" session="Poster">
			<auteurs>
				<auteur>
					<prenom>Louise</prenom>
					<nom>Deléger</nom>
					<email>louise.deleger@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Bruno</prenom>
					<nom>Cartoni</nom>
					<email>bruno.cartoni@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIMSI-CNRS, BP 133, 91403 Orsay Cedex, France</affiliation>
			</affiliations>
			<titre>Adjectifs relationnels et langue de spécialité : vérification d’une hypothèse linguistique en corpus comparable médical</titre>
			<type>court</type>
			<pages></pages>
			<resume>Cet article présente une étude en corpus comparable médical pour confirmer la préférence d’utilisation des adjectifs relationnels dans les langues de spécialité et examiner plus finement l’alternance entre syntagmes nominaux avec adjectifs relationnels et syntagmes avec complément prépositionnel.</resume>
			<mots_cles>corpus comparables monolingues, morphologie constructionnelle, langue de spécialité</mots_cles>
			<title></title>
			<abstract>This paper presents a study in medical comparable corpora that aims to confirm the preferred use of relational adjectives in specialised languages and to examine in a more fine-grained manner the alternance between phrases with adjective and noun phrases with prepositional complement.</abstract>
			<keywords>monolingual comparable corpora, constructional morphology, specialised language</keywords>
		</article>
		<article id="taln-2010-court-002" session="Poster">
			<auteurs>
				<auteur>
					<prenom>Mathieu</prenom>
					<nom>Lafourcade</nom>
					<email>lafourcade@lirmm.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Alain</prenom>
					<nom>Joubert</nom>
					<email>joubert@lirmm.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIRMM – Univ. Montpellier 2 - CNRS, Laboratoire d’Informatique, de Robotique et de Microélectronique de Montpellier, 161, rue Ada – 34392 Montpellier Cédex 5 – France</affiliation>
			</affiliations>
			<titre>Détermination et pondération des raffinements d’un terme à partir de son arbre des usages nommés</titre>
			<type>court</type>
			<pages></pages>
			<resume>Grâce à la participation d’un grand nombre de personnes via des jeux accessibles sur le web, nous avons construit un réseau lexical évolutif de grande taille pour le Français. A partir de cette ressource, nous avons abordé la question de la détermination des sens d’usage d’un terme, puis après avoir introduit la notion de similarité entre ces différents usages, nous avons pu obtenir pour un terme son arbre des usages : la racine regroupe tous les usages du terme et une descente dans l’arbre correspond à un raffinement de ces usages. Le nommage des différents noeuds est effectué lors d’une descente en largeur. En simplifiant l’arbre des usages nommés, nous déterminons les différents sens d’un terme, sens que nous introduisons dans le réseau lexical en tant que noeuds de raffinement du terme considéré. Nous terminons par une évaluation empirique des résultats obtenus.</resume>
			<mots_cles>réseau lexical, arbre des usages nommés d’un terme, pondération des sens d’un terme</mots_cles>
			<title></title>
			<abstract>Thanks to the participation of a large number of persons via web-based games, a largesized evolutionary lexical network is available for French. With this resource, we approached the question of the determination of the word usages of a term, and after introducing the notion of similarity between these various word usages, we were able to build for a term its word usage tree: the root groups together all possible usages of this term and a search in the tree corresponds to a refinement of these word usages. The labelling of the various nodes is made during a width-first search. From its labelled word usage tree, we obtain the different meanings of a term, which can be inserted in the lexical network as refinement nodes for this term. Lastly, we present an evaluation of the results we obtain.</abstract>
			<keywords>lexical network, tree of labelled word usages for a term, weighting of the meanings</keywords>
		</article>
		<article id="taln-2010-court-003" session="Poster">
			<auteurs>
				<auteur>
					<prenom>Jonathan</prenom>
					<nom>Chevelu</nom>
					<email>jonathan.chevelu@orange-ftgroup.com</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Yves</prenom>
					<nom>Lepage</nom>
					<email>yves.lepage@info.unicaen.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Thierry</prenom>
					<nom>Moudenc</nom>
					<email>thierry.moudenc@orange-ftgroup.com</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Ghislain</prenom>
					<nom>Putois</nom>
					<email>ghislain.putois@orange-ftgroup.com</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université de Caen, France</affiliation>
				<affiliation affiliationId="2">Orange Labs, France</affiliation>
			</affiliations>
			<titre>L’évaluation des paraphrases : pour une prise en compte de la tâche</titre>
			<type>court</type>
			<pages></pages>
			<resume>Les définitions des paraphrases privilégient généralement la conservation du sens. Cet article démontre par l’absurde qu’une évaluation uniquement basée sur la conservation du sens permet à un système inutile de production de paraphrase d’être jugé meilleur qu’un système au niveau de l’état de l’art. La conservation du sens n’est donc pas l’unique critère des paraphrases. Nous exhibons les trois objectifs des paraphrases : la conservation du sens, la naturalité et l’adaptation à la tâche. La production de paraphrase est alors un compromis dépendant de la tâche entre ces trois critères et ceux-ci doivent être pris en compte lors des évaluations.</resume>
			<mots_cles>Générateur de paraphrase, évaluation des paraphrases</mots_cles>
			<title></title>
			<abstract>Meaning preservation is generally rooted in the paraphrase definitions. This article proves by reductio ad absurdum that an evaluation based only on the meaning preservation can rank a dummy and useless system better than a state-of-the-art system. Meaning preservation is therefore not the one and only criterion for a paraphrase system. We exhibit the three objectives of paraphrase : meaning preservation, sentence naturalness and adequacy for the task. Paraphrase generation consists actually in reaching a taskdependent compromise between these three criteria, and they have to be taken into account during each evaluation process.</abstract>
			<keywords>Paraphrase generator, paraphrase evaluation</keywords>
		</article>
		<article id="taln-2010-court-004" session="Poster">
			<auteurs>
				<auteur>
					<prenom>Olivier</prenom>
					<nom>Collin</nom>
					<email>olivier.collin@orange-ftgroup.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Benoît</prenom>
					<nom>Gaillard</nom>
					<email>benoit.gaillard@orange-ftgroup.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Jean-Léon</prenom>
					<nom>Bouraoui</nom>
					<email>jeanleon.bouraoui@orange-ftgroup.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Orange Labs – Av Pierre Marzin, 22300 Lannion</affiliation>
			</affiliations>
			<titre>Constitution d'une ressource sémantique issue du treillis des catégories de Wikipedia</titre>
			<type>court</type>
			<pages></pages>
			<resume>Le travail présenté dans cet article s'inscrit dans le thème de l'acquisition automatique de ressources sémantiques s'appuyant sur les données de Wikipedia. Nous exploitons le graphe des catégories associées aux pages de Wikipedia à partir duquel nous extrayons une hiérarchie de catégories parentes, sémantiquement et thématiquement liées. Cette extraction est le résultat d'une stratégie de plus court chemin appliquée au treillis global des catégories. Chaque page peut ainsi être représentée dans l'espace de ses catégories propres, ainsi que des catégories parentes. Nous montrons la possibilité d'utiliser cette ressource pour deux applications. La première concerne l'indexation et la classification des pages de Wikipedia. La seconde concerne la désambiguïsation dans le cadre d'un traducteur de requêtes français/anglais. Ce dernier travail a été réalisé en exploitant les catégories des pages anglaises.</resume>
			<mots_cles>Wikipedia, plus court chemin, désambiguïsation, classification, traduction de requête</mots_cles>
			<title></title>
			<abstract>This work is closely related to the domain of automatic acquisition of semantic resources exploiting Wikipedia data. More precisely, we exploit the graph of parent categories linked to each Wikipedia page to extract the semantically and thematically related parent categories. This extraction is the result of a shortest path length calculus applied to the global lattice of Wikipedia categories. So, each page can be projected within its first level categories, and in addition their parent categories. This resource has been used for two kinds of applications. The first one concerns the indexation and classification of Wikipedia pages. The second one concerns a disambiguation task applied to a query translator for cross lingual search engine. This last work has been performed by using English categories lattice.</abstract>
			<keywords>Wikipedia, shortest path, disambiguation, classification, query translation</keywords>
		</article>
		<article id="taln-2010-court-005" session="Poster">
			<auteurs>
				<auteur>
					<prenom>Laurence</prenom>
					<nom>Danlos</nom>
					<email>laurence.danlos@linguist.jussieu.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Benoît</prenom>
					<nom>Sagot</nom>
					<email>benoit.sagot@inria.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Alpage, INRIA Paris–Rocquencourt &amp; Université Paris 7, Rocquencourt, BP 105, 78153 Le Chesnay Cedex, France</affiliation>
			</affiliations>
			<titre>Ponctuations fortes abusives</titre>
			<type>court</type>
			<pages></pages>
			<resume>Certaines ponctuations fortes sont « abusivement » utilisées à la place de ponctuations faibles, débouchant sur des phrases graphiques qui ne sont pas des phrases grammaticales. Cet article présente une étude sur corpus de ce phénomène et une ébauche d’outil pour repérer automatiquement les ponctuations fortes abusives.</resume>
			<mots_cles>pseudo-phrase, phrase averbale, analyse syntaxique et sémantique</mots_cles>
			<title></title>
			<abstract>Some strong punctuation signs are “wrongly” used instead of weak punctuation signs, leading to graphic sentences which are not grammatical sentences. This paper presents a corpus study of this phenomenon and a tool in the early stages to automatically detect wrong strong punctuation signs.</abstract>
			<keywords>pseudo-sentence, verbless utterance, syntactic and semantic analysis</keywords>
		</article>
		<article id="taln-2010-court-006" session="Poster">
			<auteurs>
				<auteur>
					<prenom>Véronique</prenom>
					<nom>Moriceau</nom>
					<email>Veronique.Moriceau@limsi.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Xavier</prenom>
					<nom>Tannier</nom>
					<email>Xavier.Tannier@limsi.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Mathieu</prenom>
					<nom>Falco</nom>
					<email>Mathieu.Falco@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIMSI-CNRS, B.P. 133, 91403 ORSAY, FRANCE</affiliation>
				<affiliation affiliationId="2">Université Paris-Sud 11, 91405 ORSAY</affiliation>
			</affiliations>
			<titre>Une étude des questions “complexes” en question-réponse</titre>
			<type>court</type>
			<pages></pages>
			<resume>La plupart des systèmes de question-réponse ont été conçus pour répondre à des questions dites “factuelles” (réponses précises comme des dates, des lieux), et peu se sont intéressés au traitement des questions complexes. Cet article présente une typologie des questions en y incluant les questions complexes, ainsi qu’une typologie des formes de réponses attendues pour chaque type de questions. Nous présentons également des expériences préliminaires utilisant ces typologies pour les questions complexes, avec de bons résultats.</resume>
			<mots_cles>Système de question-réponse, questions complexes</mots_cles>
			<title></title>
			<abstract>Most question-answering systems have been designed to answer “factual” questions (short and precise answers as dates, locations), and only a few researches concern complex questions. This article presents a typology of questions, including complex questions, as well as a typology of answers that should be expected for each type of questions. We also present preliminary experiments using these typologies for answering complex questions and leading to good results.</abstract>
			<keywords>Question-answering systems, complex questions</keywords>
		</article>
		<article id="taln-2010-court-007" session="Poster">
			<auteurs>
				<auteur>
					<prenom>Muhammad Ghulam Abbas</prenom>
					<nom>Malik</nom>
					<email>Abbas.Malik@imag.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Christian</prenom>
					<nom>Boitet</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Pushpak</prenom>
					<nom>Bhattacharyya</nom>
					<email></email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Laurent</prenom>
					<nom>Besacier</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">GETALP-LIG, Université de Grenoble (UJF), FranceIndian Institute of Technology Bombay (IITB), India</affiliation>
				<affiliation affiliationId="2">Indian Institute of Technology Bombay (IITB), India</affiliation>
			</affiliations>
			<titre></titre>
			<type>court</type>
			<pages></pages>
			<resume>La TA généraliste de haute qualité et totalement automatique est considérée comme impossible. Nous nous intéressons aux problèmes de traduction scripturale, qui sont des sous-problèmes faibles du problème général de la traduction. Nous présentons les caractéristiques des problèmes faibles de traduction et les problèmes de traduction scripturale, décrivons différentes approches computationnelles (à états finis, statistiques, et hybrides) et présentons nos résultats sur différentes combinaisons de langues et systèmes d’écriture Indo-Pak.</resume>
			<mots_cles>problèmes faibles de traduction, traduction scripturale, traduction interdialectal, transcriptions, translittérations</mots_cles>
			<title>Weak Translation Problems – a case study of Scriptural Translation</title>
			<abstract>General purpose, high quality and fully automatic MT is believed to be impossible. We are interested in scriptural translation problems, which are weak sub-problems of the general problem of translation. We introduce the characteristics of the weak problems of translation and of the scriptural translation problems, describe different computational approaches (finite-state, statistical and hybrid) to solve these problems, and report our results on several combinations of Indo-Pak languages and writing systems.</abstract>
			<keywords>weak problems of translation, scriptural translation, interdialectal translation, transcription, transliteration</keywords>
		</article>
		<article id="taln-2010-court-008" session="Poster">
			<auteurs>
				<auteur>
					<prenom>Fadoua</prenom>
					<nom>Ataa Allah</nom>
					<email>ataaallah@ircam.ma</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Siham</prenom>
					<nom>Boulaknadel</nom>
					<email>boulaknadel@ircam.ma</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">CEISIC, IRCAM, Avenue Allal El Fassi, Madinat Al Irfane, Rabat, Morocco</affiliation>
			</affiliations>
			<titre>Pseudo-racinisation de la langue amazighe</titre>
			<type>court</type>
			<pages></pages>
			<resume>Dans le cadre de la promotion de la langue amazighe, nous avons voulu lui apporter des ressources et outils linguistiques pour son traitement automatique et son intégration dans le domaine des nouvelles technologies de l'information et de la communication. Partant de ce principe, nous avons opté, au sein de l’Institut Royal de la Culture Amazighe, pour une démarche innovante de réalisations progressives de ressources linguistiques et d’outils de base de traitement automatique, qui permettront de préparer le terrain pour d’éventuelles recherches scientifiques. Dans cette perspective, nous avons entrepris de développer, dans un premier temps, un outil de pseudoracinisation basé sur une approche relevant du cas de la morphologie flexionnelle et reposant sur l’élimination d’une liste de suffixes et de préfixes de la langue amazighe. Cette approche permettra de regrouper les mots sémantiquement proches à partir de ressemblances afin d’être exploités dans des applications tel que la recherche d’information et la classification.</resume>
			<mots_cles>Langue amazighe, Pseudo-racinisation, Morphologie flexionnelle</mots_cles>
			<title></title>
			<abstract>In the context of promoting the Amazigh language, we would like to provide this language with linguistic resources and tools in the aim to enable its automatic processing and its integration in the field of Information and Communication Technology. Thus, we have opted, in the Royal Institute of Amazigh Culture, for an innovative approach of progressive realizations of linguistic resources and basic natural language processing tools that will pave the way for further scientific researches. In this perspective, we are trying initially to develop a light stemmer based on an approach dealing with inflectional morphology, and on stripping a list of Amazigh suffixes and prefixes. This approach will conflate word variants into a common stem that will be used in many applications such as information retrieval and classification.</abstract>
			<keywords>Amazigh language, Light stemming, Inflectional morphology</keywords>
		</article>
		<article id="taln-2010-court-009" session="Poster">
			<auteurs>
				<auteur>
					<prenom>François-Régis</prenom>
					<nom>Chaumartin</nom>
					<email>frc@proxem.com</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Sylvain</prenom>
					<nom>Kahane</nom>
					<email>sylvain@kahane.fr</email>
					<affiliationId>2</affiliationId>
					<affiliationId>3</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Proxem, 7 impasse Dumur, 92110 Clichy</affiliation>
				<affiliation affiliationId="2">Alpage, Université Paris 7 &amp; INRIA</affiliation>
				<affiliation affiliationId="3">Modyco, Université Paris Ouest Nanterre &amp; CNRS</affiliation>
			</affiliations>
			<titre>Une approche paresseuse de l’analyse sémantique ou comment construire une interface syntaxe-sémantique à partir d’exemples</titre>
			<type>court</type>
			<pages></pages>
			<resume>Cet article montre comment calculer une interface syntaxe-sémantique à partir d’un analyseur en dépendance quelconque et interchangeable, de ressources lexicales variées et d’une base d’exemples associés à leur représentation sémantique. Chaque exemple permet de construire une règle d’interface. Nos représentations sémantiques sont des graphes hiérarchisés de relations prédicat-argument entre des acceptions lexicales et notre interface syntaxe-sémantique est une grammaire de correspondance polarisée. Nous montrons comment obtenir un système très modulaire en calculant certaines règles par « soustraction » de règles moins modulaires.</resume>
			<mots_cles>Interface syntaxe-sémantique, graphe sémantique, grammaires de dépendance, GUP (Grammaire d’unification polarisée), GUST (Grammaire d’unification Sens-Texte)</mots_cles>
			<title></title>
			<abstract>This article shows how to extract a syntax-semantics interface starting from an interchangeable dependency parser, various lexical resources and from samples associated with their semantic representations. Each example allows us to build an interface rule. Our semantic representations are hierarchical graphs of predicate-argument relations between lexical meanings and our syntax-semantics interface is a polarized unification grammar. We show how to obtain a very modular system by computing some rules by “subtraction” of less modular rules.</abstract>
			<keywords>Syntax-semantics Interface, Semantic Graph, Dependency Grammar, PUG (Polarized Unification Grammar), MTUG (Meaning-Text Unification Grammar)</keywords>
		</article>
		<article id="taln-2010-court-010" session="Poster">
			<auteurs>
				<auteur>
					<prenom>Lorenza</prenom>
					<nom>Russo</nom>
					<email>Lorenza.Russo@unige.ch</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Laboratoire d’Analyse et de Technologie du Langage, Département de linguistique – Université de Genève, 2, rue de Candolle – CH-1211 Genève 4</affiliation>
			</affiliations>
			<titre>La traduction automatique des pronoms clitiques. Quelle approche pour quels résultats?</titre>
			<type>court</type>
			<pages></pages>
			<resume>Dans cet article, nous abordons la problématique de la traduction automatique des pronoms clitiques, en nous focalisant sur la traduction de l’italien vers le français et en comparant les résultats obtenus par trois systèmes : Its-2, développé au LATL (Laboratoire d’Analyse et de Technologie du Langage) et basé sur un analyseur syntaxique profond ; Babelfish, basé sur des règles linguistiques ; et Google Translate, caractérisé par une approche statistique.</resume>
			<mots_cles>Analyseur syntaxique, traduction automatique, pronoms clitiques, proclise, enclise</mots_cles>
			<title></title>
			<abstract>In this article, we discuss the problem of automatic translation of clitic pronouns, focalysing our attention on the translation from Italian to French and comparing the results obtained by three MT systems : Its-2, developed at LATL (Language Technology Laboratory) and based on a syntactic parser ; Babelfish, a rule-based system ; and Google Translate, caracterised by a statistical approach.</abstract>
			<keywords>Syntactic parser, automatic translation, clitic pronouns, proclisis, enclisis</keywords>
		</article>
		<article id="taln-2010-court-011" session="Poster">
			<auteurs>
				<auteur>
					<prenom>François</prenom>
					<nom>Morlane-Hondère</nom>
					<email>francois.morlane@univ-tlse2.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Cécile</prenom>
					<nom>Fabre</nom>
					<email>cecile.fabre@univ-tlse2.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">CLLE-ERSS, Université de Toulouse</affiliation>
			</affiliations>
			<titre>L’antonymie observée avec des méthodes de TAL : une relation à la fois syntagmatique et paradigmatique ?</titre>
			<type>court</type>
			<pages></pages>
			<resume>Cette étude utilise des outils de TAL pour tester l’hypothèse avancée par plusieurs études linguistiques récentes selon laquelle la relation antonymique, classiquement décrite comme une relation paradigmatique, a la particularité de fonctionner également sur le plan syntagmatique, c’est-à-dire de réunir des mots qui sont non seulement substituables mais qui apparaissent également régulièrement dans des relations contextuelles. Nous utilisons deux méthodes – l’analyse distributionnelle pour le plan paradigmatique, la recherche par patrons antonymiques pour le plan syntagmatique. Les résultats montrent que le diagnostic d’antonymie n’est pas significativement meilleur lorsqu’on croise les deux méthodes, puisqu’une partie des antonymes identifiés ne répondent pas au test de substituabilité, ce qui semble confirmer la prépondérance du plan syntagmatique pour l’étude et l’acquisition de cette relation.</resume>
			<mots_cles>sémantique lexicale, antonymie, analyse distributionnelle, patrons lexico-syntaxiques</mots_cles>
			<title></title>
			<abstract>In this paper, we use NLP methods to test the hypothesis, suggested by several linguistic studies, that antonymy is not only a paradigmatic but also a syntagmatic relation : antonym pairs, that have been classically described by their ability to be substituted for each other, also tend to frequently co-occur in texts. We use two methods – distributional analysis on the paradigmatic level, lexico-syntactic pattern recognition on the syntagmatic level. Results show that antonym detection is not significantly improved by combining the two methods : a set of antonyms do not satisfy the test for substitutability, which tends to confirm the predominance of the syntagmatic level for studying and identifying antonymy.</abstract>
			<keywords>lexical semantics, antonymy, distributional analysis, lexico-grammatical patterns</keywords>
		</article>
		<article id="taln-2010-court-012" session="Poster">
			<auteurs>
				<auteur>
					<prenom>Ludivine</prenom>
					<nom>Kuznik</nom>
					<email>ludivine-externe.kuznik@edf.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>3</affiliationId>
				</auteur>
				<auteur>
					<prenom>Anne-Laure</prenom>
					<nom>Guénet</nom>
					<email>anne-laure.guenet@edf.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Anne</prenom>
					<nom>Peradotto</nom>
					<email>anne.peradotto@edf.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Chloé</prenom>
					<nom>Clavel</nom>
					<email>chloe.clavel@edf.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Electricité de France, Direction Commerce, 92050 Paris La Défense, France</affiliation>
				<affiliation affiliationId="2">Electricité de France R&amp;D, 92141 Clamart, France</affiliation>
				<affiliation affiliationId="3">Société Lincoln , 92570 Boulogne-Billancourt, France</affiliation>
			</affiliations>
			<titre>L’apport des concepts métiers pour la classification des questions ouvertes d’enquête</titre>
			<type>court</type>
			<pages></pages>
			<resume>EDF utilise les techniques de Text Mining pour optimiser sa relation client, en analysant des réponses aux questions ouvertes d'enquête de satisfaction, et des retranscriptions de conversations issues des centres d'appels. Dans cet article, nous présentons les différentes contraintes applicatives liées à l’utilisation d’outils de text mining pour l’analyse de données clients. Après une analyse des différents outils présents sur le marché, nous avons identifié la technologie Skill CartridgeTM fournie par la société TEMIS comme la plus adaptée à nos besoins. Cette technologie nous permet une modélisation sémantique de concepts liés au motif d’insatisfaction. L’apport de cette modélisation est illustrée pour une tâche de classification de réponses d’enquêtes de satisfaction chargée d’évaluer la fidélité des clients EDF. La modélisation sémantique a permis une nette amélioration des scores de classification (F-mesure = 75,5%) notamment pour les catégories correspondant à la satisfaction et au mécontentement.</resume>
			<mots_cles>outils de text mining, modélisation de concepts métier, classification supervisée</mots_cles>
			<title></title>
			<abstract>The French power supply company EDF uses text mining tools to improve customer insight by analysing satisfaction inquiries or transcriptions of call-centre conversations. In this paper, we present the various application needs for text mining tools. After an analysis of the various existing industrial tools, we identify the Skill Cartridge tool provided by TEMIS company as the more relevant to our needs. This tool offers the capability to model expressions linked to reason for satisfaction/dissatisfaction. The contribution of this modelling is illustrated here for the classification of satisfaction inquiries dedicated to the evaluation of customer loyalty. The semantic models provide a marked improvement of classification scores (F-mesure = 75.5%) for the satisfaction/dissatisfaction categories in particular.</abstract>
			<keywords>text mining tools, business concept modelling, supervised classification</keywords>
		</article>
		<article id="taln-2010-court-013" session="Poster">
			<auteurs>
				<auteur>
					<prenom>Charles</prenom>
					<nom>Dejean</nom>
					<email></email>
					<affiliationId></affiliationId>
				</auteur>
				<auteur>
					<prenom>Manoel</prenom>
					<nom>Fortun</nom>
					<email></email>
					<affiliationId></affiliationId>
				</auteur>
				<auteur>
					<prenom>Clotilde</prenom>
					<nom>Massot</nom>
					<email></email>
					<affiliationId></affiliationId>
				</auteur>
				<auteur>
					<prenom>Vincent</prenom>
					<nom>Pottier</nom>
					<email></email>
					<affiliationId></affiliationId>
				</auteur>
				<auteur>
					<prenom>Fabien</prenom>
					<nom>Poulard</nom>
					<email>Fabien.Poulard@univ-nantes.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Matthieu</prenom>
					<nom>Vernier</nom>
					<email>Matthieu.Vernier@univ-nantes.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LINA, UMR6241, 44322 Nantes</affiliation>
			</affiliations>
			<titre>Un étiqueteur de rôles grammaticaux libre pour le français intégré à Apache UIMA</titre>
			<type>court</type>
			<pages></pages>
			<resume>L’étiquetage des rôles grammaticaux est une tâche de pré-traitement récurrente. Pour le français, deux outils sont majoritairement utilisés : TreeTagger et Brill. Nous proposons une démarche, ne nécessitant aucune ressource, pour la création d’un modèle de Markov caché (HMM) pour palier les problèmes de ces outils, et de licences notamment. Nous distribuons librement toutes les ressources liées à ce travail.</resume>
			<mots_cles>étiquetage grammatical, Modèle de Markov caché, UIMA, Brill, TreeTagger</mots_cles>
			<title></title>
			<abstract>Part-of-speech tagging is a common preprocessing task. For the French language, Brill and TreeTagger are the most often used tools. We propose a method, requiring no resource, to create a Hidden Markov Model to get rid of the problems and licences of these tools. We freely distribute all the resources related to this work.</abstract>
			<keywords>grammatical tagging, Hidden Markov Model, UIMA, Brill, TreeTagger</keywords>
		</article>
		<article id="taln-2010-court-014" session="Poster">
			<auteurs>
				<auteur>
					<prenom>Michel</prenom>
					<nom>Généreux</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Rita</prenom>
					<nom>Marquilhas</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Iris</prenom>
					<nom>Hendrickx</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Centro de Linguistica da Universidade de Lisboa, Av. Prof. Gama Pinto, 2, 1649-003 Lisboa - Portugal</affiliation>
			</affiliations>
			<titre>Segmentation Automatique de Lettres Historiques</titre>
			<type>court</type>
			<pages></pages>
			<resume>Cet article présente une approche basée sur la comparaison fréquentielle de modèles lexicaux pour la segmentation automatique de textes historiques Portugais. Cette approche traite d’abord le problème de la segmentation comme un problème de classification, en attribuant à chaque élément lexical présent dans la phase d’apprentissage une valeur de saillance pour chaque type de segment. Ces modèles lexicaux permettent à la fois de produire une segmentation et de faire une analyse qualitative de textes historiques. Notre évaluation montre que l’approche adoptée permet de tirer de l’information sémantique que des approches se concentrant sur la détection des frontières séparant les segments ne peuvent acquérir.</resume>
			<mots_cles>Corpus comparables, Saillance, Segmentation, Textes historiques</mots_cles>
			<title></title>
			<abstract>This article presents an approach based on the frequency comparison of lexical models for the automatic segmentation of historical texts. This approach first addresses the problem of segmentation as a classification problem by assigning each token present in the learning phase a value of salience for each type of segment. These lexical patterns can both produce a segmentation and make possible a qualitative analysis of historical texts. Our evaluation shows that the approach can extract semantic information that approaches focusing on the detection of boundaries between segments cannot capture.</abstract>
			<keywords>Comparable corpora, Salience, Segmentation, Historical Texts</keywords>
		</article>
		<article id="taln-2010-court-015" session="Poster">
			<auteurs>
				<auteur>
					<prenom>Helena</prenom>
					<nom>Blancafort</nom>
					<email></email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Gaëlle</prenom>
					<nom>Recourcé</nom>
					<email></email>
					<affiliationId>3</affiliationId>
				</auteur>
				<auteur>
					<prenom>Javier</prenom>
					<nom>Couto</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Benoît</prenom>
					<nom>Sagot</nom>
					<email></email>
					<affiliationId>3</affiliationId>
				</auteur>
				<auteur>
					<prenom>Rosa</prenom>
					<nom>Stern</nom>
					<email></email>
					<affiliationId>3</affiliationId>
					<affiliationId>4</affiliationId>
				</auteur>
				<auteur>
					<prenom>Denis</prenom>
					<nom>Teyssou</nom>
					<email></email>
					<affiliationId>4</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Syllabs, 15, rue Jean-Baptiste Berlier, 75013 Paris, France</affiliation>
				<affiliation affiliationId="2">Universitat Pompeu Fabra, Roc Boronat, 08013 Barcelona, Espagne</affiliation>
				<affiliation affiliationId="3">Alpage, INRIA Paris–Rocquencourt &amp; Université Paris 7, Rocquencourt, BP 105, 78153 Le Chesnay Cedex, France</affiliation>
				<affiliation affiliationId="4">Agence France-Presse – Medialab, 2 place de la Bourse, 75002 Paris, France</affiliation>
			</affiliations>
			<titre>Traitement des inconnus : une approche systématique de l’incomplétude lexicale</titre>
			<type>court</type>
			<pages></pages>
			<resume>Cet article aborde le phénomène de l’incomplétude des ressources lexicales, c’est-à-dire la problématique des inconnus, dans un contexte de traitement automatique. Nous proposons tout d’abord une définition opérationnelle de la notion d’inconnu. Nous décrivons ensuite une typologie des différentes classes d’inconnus, motivée par des considérations linguistiques et applicatives ainsi que par l’annotation des inconnus d’un petit corpus selon notre typologie. Cette typologie sera mise en oeuvre et validée par l’annotation d’un corpus important de l’Agence France-Presse dans le cadre du projet EDyLex.</resume>
			<mots_cles>mots inconnus, incomplétude lexicale, acquisition dynamique des ressources lexicales</mots_cles>
			<title></title>
			<abstract>This paper addresses the incompleteness of lexical resources, i.e., the problem of unknown words, in the context of natural language processing. First, we put forward an operational definition of the notion of unknown words. Next, we describe a typology of the various classes of unknown words, motivated by linguistic and applicative considerations as well as the annotation of unknown words in a small-scale corpus w.r.t. our typology. This typology shall be applied and validated through the annotation of a large corpus from the Agence France-Presse as part of the EDyLex project.</abstract>
			<keywords>unknown words, lexical incompleteness, dynamic acquisition of lexical information</keywords>
		</article>
		<article id="taln-2010-court-016" session="Poster">
			<auteurs>
				<auteur>
					<prenom>Évelyne</prenom>
					<nom>Jacquey</nom>
					<email>Evelyne.Jacquey@atilf.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Laurence</prenom>
					<nom>Kister</nom>
					<email>Laurence.Kister@atilf.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Mick</prenom>
					<nom>Grzesitchak</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Bertrand</prenom>
					<nom>Gaiffe</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Coralie</prenom>
					<nom>Reutenauer</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Sandrine</prenom>
					<nom>Ollinger</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Mathieu</prenom>
					<nom>Valette</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">UMR ATILF-CNRS-Nancy Université, 44 avenue de la libération 54000 NANCY</affiliation>
			</affiliations>
			<titre>Thésaurus et corpus de spécialité sciences du langage : approches lexicométriques appliquées à l’analyse de termes en corpus</titre>
			<type>court</type>
			<pages></pages>
			<resume>Cet article s'inscrit dans les recherches sur l'exploitation de ressources terminologiques pour l'analyse de textes de spécialité, leur annotation et leur indexation. Les ressources en présence sont, d'une part, un thesaurus des Sciences du Langage, le Thesaulangue et, d'autre part, un corpus d’échantillons issus de cinq ouvrages relevant du même domaine. L'article a deux objectifs. Le premier est de déterminer dans quelle mesure les termes de Thesaulangue sont représentés dans les textes. Le second est d'évaluer si les occurrences des unités lexicales correspondant aux termes de Thesaulangue relèvent majoritairement d'emplois terminologiques ou de langue courante. A cette fin, les travaux présentés utilisent une mesure de richesse lexicale telle qu'elle a été définie par Brunet (rapporté dans Muller, 1992) dans le domaine de la lexicométrie, l'indice W. Cette mesure est adaptée afin de mesurer la richesse terminologie (co-occurrents lexicaux et sémantiques qui apparaissent dans Thesaulangue).</resume>
			<mots_cles>sémantique lexicale, terminologie, corpus, richesse lexicale, lexicométrie</mots_cles>
			<title></title>
			<abstract>This article aims to contribute to the field of the exploitation of terminological resources for the analysis of technical and scientific texts, their annotation and their indexation. The available resources are on one hand a thesaurus, Thesaulangue, which deals with Linguistics, and on the other hand, a corpus made of samples extracted from five books about Linguistics. More precisely, the article has two goals: first, studying how to determine which terms of Thesaulangue occur in texts. Second, attempting to measure if the lexical units which correspond to terms of Thesaulangue are used in texts in a terminological way or not. In this perspective, the presented work uses and adapts the Brunet’s W-index designed in the area of lexicometry.</abstract>
			<keywords>lexical semantics, terminology, corpora, lexical richness, lexicometry</keywords>
		</article>
		<article id="taln-2010-court-017" session="Poster">
			<auteurs>
				<auteur>
					<prenom>Marc</prenom>
					<nom>Le Tallec</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Jeanne</prenom>
					<nom>Villaneau</nom>
					<email></email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Jean-Yves</prenom>
					<nom>Antoine</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Agata</prenom>
					<nom>Savary</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Arielle</prenom>
					<nom>Syssau-Vaccarella</nom>
					<email></email>
					<affiliationId>3</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université François Rabelais Tours – LI</affiliation>
				<affiliation affiliationId="2">Université Européenne de Bretagne – VALORIA</affiliation>
				<affiliation affiliationId="3">Université Montpellier 3</affiliation>
			</affiliations>
			<titre>Détection hors contexte des émotions à partir du contenu linguistique d’énoncés oraux : le système EmoLogus</titre>
			<type>court</type>
			<pages></pages>
			<resume>Le projet EmotiRob, soutenu par l'agence nationale de la recherche, s'est donné pour objectif de détecter des émotions dans un contexte d'application original : la réalisation d'un robot compagnon émotionnel pour des enfants fragilisés. Nous présentons dans cet article le système qui caractérise l'émotion induite par le contenu linguistique des propos de l'enfant. Il se base sur un principe de compositionnalité des émotions, avec une valeur émotionnelle fixe attribuée aux mots lexicaux, tandis que les verbes et les adjectifs agissent comme des fonctions dont le résultat dépend de la valeur émotionnelle de leurs arguments. L'article présente la méthode de calcul utilisée, ainsi que la norme lexicale émotionnelle correspondante. Une analyse quantitative et qualitative des premières expérimentations présente les différences entre les sorties du module de détection et l'annotation d'experts, montrant des résultats satisfaisants, avec la bonne détection de la valence émotionnelle dans plus de 90% des cas.</resume>
			<mots_cles>Emotion, valence émotionnelle, norme lexicale émotionnelle, robot compagnon, compréhension de parole</mots_cles>
			<title></title>
			<abstract>The ANR Emotirob project aims at detecting emotions in an original application context: realizing an emotional companion robot for weakened children. This paper presents a system which aims at characterizing emotions by only considering linguistic content. It is based on the assumption that emotions can be compound: simple lexical words have an intrinsic emotional value, while verbal and adjectival predicates act as a function on the emotional values of their arguments. The paper describes the algorithm of compositional computation of the emotion and the lexical emotional norm used by this algorithm. A quantitative and qualitative analysis of the differences between system outputs and expert annotations is given, which shows satisfactory results, with a good detection of emotional valency in 90.0% of the test utterances.</abstract>
			<keywords>Emotion, Emotional valency, Emotional lexical standard, companion robot, spoken language understanding</keywords>
		</article>
		<article id="taln-2010-court-018" session="Poster">
			<auteurs>
				<auteur>
					<prenom>Houda</prenom>
					<nom>Bouamor</nom>
					<email>Houda.Bouamor@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Aurélien</prenom>
					<nom>Max</nom>
					<email>Aurelien.Max@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Anne</prenom>
					<nom>Vilnat</nom>
					<email>Anne.Vilnat@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIMSI-CNRS, Univ. Paris-Sud, Orsay, F-91403, France</affiliation>
			</affiliations>
			<titre>Acquisition de paraphrases sous-phrastiques depuis des paraphrases d’énoncés</titre>
			<type>court</type>
			<pages></pages>
			<resume>Dans cet article, nous présentons la tâche d’acquisition de paraphrases sous-phrastiques (impliquant des paires de mots ou de groupes de mots), et décrivons plusieurs techniques opérant à différents niveaux. Nous décrivons une évaluation visant à comparer ces techniques et leurs combinaisons sur deux corpus de paraphrases d’énoncés obtenus par traduction multiple. Les conclusions que nous tirons peuvent servir de guide pour améliorer des techniques existantes.</resume>
			<mots_cles>Paraphrase, Patrons de correspondances de segments monolingues</mots_cles>
			<title></title>
			<abstract>In this article, the task of acquiring sub-sentential paraphrases (word or phrase pairs) is discussed and several automatic techniques operating at different levels are presented. We describe an evaluation methodology to compare these techniques and their combination that is applied on two corpora of sentential paraphrases obtained by multiple translation. The conclusions that are drawn can be used to guide future work for improving existing techniques.</abstract>
			<keywords>Paraphrase, Monolingual bi-phrase patterns</keywords>
		</article>
		<article id="taln-2010-court-019" session="Poster">
			<auteurs>
				<auteur>
					<prenom>Claire</prenom>
					<nom>Mouton</nom>
					<email>claire.mouton@cea.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Gaël</prenom>
					<nom>de Chalendar</nom>
					<email>gael.de-chalendar@cea.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">CEA, LIST, Laboratoire Vision et Ingénierie des Contenus, Fontenay aux Roses, F-92265, France</affiliation>
				<affiliation affiliationId="2">Exalead, 10 place de la Madeleine, 75008 Paris</affiliation>
			</affiliations>
			<titre>JAWS : Just Another WordNet Subset</titre>
			<type>court</type>
			<pages></pages>
			<resume>WordNet, une des ressources lexicales les plus utilisées aujourd’hui a été constituée en anglais et les chercheurs travaillant sur d’autres langues souffrent du manque d’une telle ressource. Malgré les efforts fournis par la communauté française, les différents WordNets produits pour la langue française ne sont toujours pas aussi exhaustifs que le WordNet de Princeton. C’est pourquoi nous proposons une méthode novatrice dans la production de termes nominaux instanciant les différents synsets de WordNet en exploitant les propriétés syntaxiques distributionnelles du vocabulaire français. Nous comparons la ressource que nous obtenons avecWOLF et montrons que notre approche offre une couverture plus large.</resume>
			<mots_cles>ressources lexicales françaises, WordNet, relations sémantiques, distributions syntaxiques</mots_cles>
			<title></title>
			<abstract>WordNet, one of the most used lexical resource until today has been made up for the English language and scientists working on other languages suffer from the lack of such a resource. Despite the efforts performed by the French community, the differentWordNets produced for the French language are still not as exhaustive as the original Princeton WordNet. We propose a new approach in the way of producing nominal terms filling the synset slots. We use syntactical distributional properties of French vocabulary to determine which of the candidates given by a bilingual dictionary matches the best. We compare the resource we obtain withWOLF and show that our approach provides a much larger coverage.</abstract>
			<keywords>French lexical resources, WordNet, semantic relations, syntactical distributionality</keywords>
		</article>
		<article id="taln-2010-court-020" session="Poster">
			<auteurs>
				<auteur>
					<prenom>Caroline</prenom>
					<nom>Brun</nom>
					<email>Caroline.Brun@xrce.xerox.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Maud</prenom>
					<nom>Ehrmann</nom>
					<email>maud.ehrmann@jrc.ec.europa.eu</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">XRCE, 6, Chemins de Maupertuis, Meylan, France</affiliation>
				<affiliation affiliationId="2">JRC – European Commission, Ispra, Italie</affiliation>
			</affiliations>
			<titre>Un système de détection d’entités nommées adapté pour la campagne d’évaluation ESTER 2</titre>
			<type>court</type>
			<pages></pages>
			<resume>Dans cet article nous relatons notre participation à la campagne d’évaluation ESTER 2 (Evaluation des Systèmes de Transcription Enrichie d’Emissions Radiophoniques). Après avoir décrit les objectifs de cette campagne ainsi que ses spécificités et difficultés, nous présentons notre système d’extraction d’entités nommées en nous focalisant sur les adaptations réalisées dans le cadre de cette campagne. Nous décrivons ensuite les résultats obtenus lors de la compétition, ainsi que des résultats originaux obtenus par la suite. Nous concluons sur les leçons tirées de cette expérience.</resume>
			<mots_cles>entités nommées, évaluation, extraction d’information</mots_cles>
			<title></title>
			<abstract>In this paper, we report our participation to the ESTER 2 (Evaluation des Systèmes de Transcription Enrichie d’Emissions Radiophoniques) evaluation campaign. After describing the goals, specificities and challenges of the campaign, we present our named entity detection system and focus on the adaptations made in the framework of the campaign. We present the results obtained during the competition and then new results obtained afterward. We then conclude by the lessons we learned from this experiment.</abstract>
			<keywords>named entities, evaluation, information extraction</keywords>
		</article>
		<article id="taln-2010-court-021" session="Poster">
			<auteurs>
				<auteur>
					<prenom>Philippe</prenom>
					<nom>Muller</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Philippe</prenom>
					<nom>Langlais</nom>
					<email></email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">IRIT, Université de Toulouse &amp; Alpage, INRIA</affiliation>
				<affiliation affiliationId="2">DIRO, Université de Montr´eal</affiliation>
			</affiliations>
			<titre>Comparaison de ressources lexicales pour l’extraction de synonymes</titre>
			<type>court</type>
			<pages></pages>
			<resume></resume>
			<mots_cles></mots_cles>
			<title></title>
			<abstract></abstract>
			<keywords></keywords>
		</article>
		<article id="taln-2010-court-022" session="Poster">
			<auteurs>
				<auteur>
					<prenom>Laurence</prenom>
					<nom>Longo</nom>
					<email>longo@unistra.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Amalia</prenom>
					<nom>Todiraşcu</nom>
					<email>todiras@unistra.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Laboratoire LiLPa, EA 1339, Université de Strasbourg, 67000 Strasbourg Cedex, France</affiliation>
			</affiliations>
			<titre>RefGen : un module d’identification des chaînes de référence dépendant du genre textuel</titre>
			<type>court</type>
			<pages></pages>
			<resume>Dans cet article, nous présentons RefGen, un module d’identification des chaînes de référence pour le français. RefGen effectue une annotation automatique des expressions référentielles puis identifie les relations de coréférence établies entre ces expressions pour former des chaînes de référence. Le calcul de la référence utilise des propriétés des chaînes de référence dépendantes du genre textuel, l’échelle d’accessibilité d’(Ariel, 1990) et une série de filtres lexicaux, morphosyntaxiques et sémantiques. Nous évaluons les premiers résultats de RefGen sur un corpus issu de rapports publics.</resume>
			<mots_cles>Chaînes de référence, relation de coréférence, saillance, genre textuel</mots_cles>
			<title></title>
			<abstract>We present RefGen, a reference chain identification module for French. RefGen automatically annotates referential expressions then identifies coreference relations between these expressions to make reference chains. Reference calculus uses textual genre specific properties of reference chains, (Ariel, 1990)’s accessibility theory and applies lexical, morphosyntactic and semantic filters. We evaluate the first results obtained by RefGen from a public reports corpus.</abstract>
			<keywords>Reference chain identification, coreference relation, salience, genre</keywords>
		</article>
		<article id="taln-2010-court-023" session="Poster">
			<auteurs>
				<auteur>
					<prenom>Rosa</prenom>
					<nom>Stern</nom>
					<email>rosa.stern@afp.com</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Benoît</prenom>
					<nom>Sagot</nom>
					<email>benoit.sagot@inria.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Alpage, INRIA Paris–Rocquencourt &amp; Université Paris 7, Rocquencourt, BP 105, 78153 Le Chesnay Cedex, France</affiliation>
				<affiliation affiliationId="2">Agence France-Presse – Medialab, 2 place de la Bourse, 75002 Paris, France</affiliation>
			</affiliations>
			<titre>Détection et résolution d’entités nommées dans des dépêches d’agence</titre>
			<type>court</type>
			<pages></pages>
			<resume>Nous présentons NP, un système de reconnaissance d’entités nommées. Comprenant un module de résolution, il permet d’associer à chaque occurrence d’entité le référent qu’elle désigne parmi les entrées d’un référentiel dédié. NP apporte ainsi des informations pertinentes pour l’exploitation de l’extraction d’entités nommées en contexte applicatif. Ce système fait l’objet d’une évaluation grâce au développement d’un corpus annoté manuellement et adapté aux tâches de détection et de résolution.</resume>
			<mots_cles>résolution d’entités nommées, détection d’entités nommées, extraction d’information</mots_cles>
			<title></title>
			<abstract>We introduce NP, a system for named entity recognition. It includes a resolution module for linking each entity occurrence to its matching entry in a dedicated reference base. NP thus brings information relevant for using named entity extraction in an applicative context. We have evaluated NP by the means of a manually annotated corpus designed for the tasks of recognition and resolution.</abstract>
			<keywords>named entity resolution, named entity recognition, information extraction</keywords>
		</article>
		<article id="taln-2010-court-024" session="Poster">
			<auteurs>
				<auteur>
					<prenom>Marie-Jean</prenom>
					<nom>Meurs</nom>
					<email>marie-jean.meurs@univ-avignon.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Fabrice</prenom>
					<nom>Lefèvre</nom>
					<email>fabrice.lefevre@univ-avignon.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Laboratoire Informatique d’Avignon (EA 931), F-84911 Avignon, France.</affiliation>
			</affiliations>
			<titre>Processus de décision à base de SVM pour la composition d’arbres de frames sémantiques</titre>
			<type>court</type>
			<pages></pages>
			<resume>Cet article présente un processus de décision basé sur des classifieurs à vaste marge (SVMDP) pour extraire l’information sémantique dans un système de dialogue oral. Dans notre composant de compréhension, l’information est représentée par des arbres de frames sémantiques définies selon le paradigme FrameNet. Le processus d’interprétation est réalisé en deux étapes. D’abord, des réseaux bayésiens dynamiques (DBN) sont utilisés comme modèles de génération pour inférer des fragments d’arbres de la requête utilisateur. Ensuite, notre SVMDP dépendant du contexte compose ces fragments afin d’obtenir la représentation sémantique globale du message. Les expériences sont menées sur le corpus de dialogue MEDIA. Une procédure semi-automatique fournit une annotation de référence en frames sur laquelle les paramètres des DBN et SVMDP sont appris. Les résultats montrent que la méthode permet d’améliorer les performances d’identification de frames pour les exemples de test les plus complexes par rapport à un processus de décision déterministe ad hoc.</resume>
			<mots_cles>système de dialogue oral, compréhension de la parole, composition sémantique, frame sémantique, séparateur à vaste marge</mots_cles>
			<title></title>
			<abstract>This paper presents a decision process based on Support Vector Machines to extract the semantic information from the user’s input in a spoken dialog system. In our interpretation component, the information is represented by means of trees of semantic frames, as defined in the Berkeley FrameNet paradigm, and the understanding process is performed in two steps. First Dynamic Bayesian Networks are used as generative models to sequentially infer tree fragments from the users’ inputs. Then the contextsensitive SVMDP introduced in this paper is applied to detect the relations between the frames hypothesized in the fragments and compose them to obtain the overall semantic representation of the user’s request. Experiments are reported on the French MEDIA dialogue corpus. A semi-automatic process provides a reference frame annotation of the speech training data. The parameters of DBNs and SVMDP are learned from these data. The method is shown to outperform an ad-hoc deterministic decision process on the most complex test examples for frame identification.</abstract>
			<keywords>spoken dialogue system, spoken language understanding, semantic composition, semantic frame, support vector machines</keywords>
		</article>
		<article id="taln-2010-court-025" session="Poster">
			<auteurs>
				<auteur>
					<prenom>Béatrice</prenom>
					<nom>Arnulphy</nom>
					<email>Beatrice.Arnulphy@limsi.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Xavier</prenom>
					<nom>Tannier</nom>
					<email>Xavier.Tannier@limsi.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Anne</prenom>
					<nom>Vilnat</nom>
					<email>Anne.Vilnat@limsi.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Univ. Paris-Sud, Orsay, France</affiliation>
				<affiliation affiliationId="2">LIMSI-CNRS, B.P. 133, 91403 Orsay Cedex, France</affiliation>
			</affiliations>
			<titre>Les entités nommées événement et les verbes de cause-conséquence</titre>
			<type>court</type>
			<pages></pages>
			<resume>L’extraction des événements désignés par des noms est peu étudiée dans des corpus généralistes. Si des lexiques de noms déclencheurs d’événements existent, les problèmes de polysémie sont nombreux et beaucoup d’événements ne sont pas introduits par des déclencheurs. Nous nous intéressons dans cet article à une hypothèse selon laquelle les verbes induisant la cause ou la conséquence sont de bons indices quant à la présence d’événements nominaux dans leur cotexte.</resume>
			<mots_cles>Entité nommée, événement, rapports de cause et conséquence</mots_cles>
			<title></title>
			<abstract>Few researches focus on nominal event extraction in open-domain corpora. Lists of cue words for events exist, but raise many problems of polysemy. In this article, we focus on the following hypothesis : verbs introducing cause or consequence links have good chances to have an event noun around them.</abstract>
			<keywords>Named entity, event, cause and consequence links</keywords>
		</article>
		<article id="taln-2010-court-026" session="Poster">
			<auteurs>
				<auteur>
					<prenom>Alexander</prenom>
					<nom>Pak</nom>
					<email>alexpak@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Patrick</prenom>
					<nom>Paroubek</nom>
					<email>pap@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université de Paris-Sud, Laboratoire LIMSI-CNRS, Bâtiment 508, F-91405 Orsay Cedex, France</affiliation>
			</affiliations>
			<titre>Construction d’un lexique affectif pour le français à partir de Twitter</titre>
			<type>court</type>
			<pages></pages>
			<resume>Un lexique affectif est un outil utile pour l’étude des émotions ainsi que pour la fouille d’opinion et l’analyse des sentiments. Un tel lexique contient des listes de mots annotés avec leurs évaluations émotionnelles. Il existe un certain nombre de lexiques affectifs pour la langue anglaise, espagnole, allemande, mais très peu pour le français. Un travail de longue haleine est nécessaire pour construire et enrichir un lexique affectif. Nous proposons d’utiliser Twitter, la plateforme la plus populaire de microblogging de nos jours, pour recueillir un corpus de textes émotionnels en français. En utilisant l’ensemble des données recueillies, nous avons estimé les normes affectives de chaque mot. Nous utilisons les données de la Norme Affective desMots Anglais (ANEW, Affective Norms of EnglishWords) que nous avons traduite en français afin de valider nos résultats. Les valeurs du coefficient tau de Kendall et du coefficient de corrélation de rang de Spearman montrent que nos scores estimés sont en accord avec les scores ANEW.</resume>
			<mots_cles>Analyse de sentiments, ANEW, Twitter</mots_cles>
			<title></title>
			<abstract>Affective lexicons are a useful tool for emotion studies as well as for opinion mining and sentiment analysis. Such lexicons contain lists of words annotated with their emotional assessments. There exist a number of affective lexicons for English, Spanish, German and other languages. However, only a few of such resources are available for French. A lot of human efforts are needed to build and extend an affective lexicon. We propose to use Twitter, the most popular microblogging platform nowadays, to collect a dataset of emotional texts in French. Using the collected dataset, we estimated the affective norms of words present in our corpus. We used the dataset of Affective Norms of English Words (ANEW) that we translated into French to validate our results. Values of Kendall’s tau coefficient and Spearman’s rank correlation coefficient show that our estimated scores correlate well with the ANEW scores.</abstract>
			<keywords>Sentiment analysis, ANEW, Twitter</keywords>
		</article>
		<article id="taln-2010-court-027" session="Poster">
			<auteurs>
				<auteur>
					<prenom>Lei</prenom>
					<nom>Zhang</nom>
					<email>Lei.Zhang@unicaen.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Stéphane</prenom>
					<nom>Ferrari</nom>
					<email>Stephane.Ferrari@unicaen.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">GREYC - Département Informatique, Université de Caen - Campus 2, 14032 Caen</affiliation>
			</affiliations>
			<titre>Analyse d’opinion : annotation sémantique de textes chinois</titre>
			<type>court</type>
			<pages></pages>
			<resume>Notre travail concerne l’analyse automatique des énoncés d’opinion en chinois. En nous inspirant de la théorie linguistique de l’Appraisal, nous proposons une méthode fondée sur l’usage de lexiques et de règles locales pour déterminer les caractéristiques telles que la Force (intensité), le Focus (prototypicalité) et la polarité de tels énoncés. Nous présentons le modèle et sa mise en oeuvre sur un corpus journalistique. Si pour la détection d’énoncés d’opinion, la précision est bonne (94 %), le taux de rappel (67 %) pose cependant des questions sur l’enrichissement des ressources actuelles.</resume>
			<mots_cles>Analyse d’opinion, théorie de l’Appraisal</mots_cles>
			<title></title>
			<abstract>Our work concerns automatic analysis of opinion in texts. Based on the Appraisal linguistic theory, our method uses lexical and syntactic resources to process such properties as the Force, the Focus and the polarity of an opinion. We present our model and its implementation on a journalistic corpus. The precision for detecting opinion expressions is high (94%), but the recall (67%) raises the question of how to enhance the resources.</abstract>
			<keywords>Opinion analysis, Appraisal theory</keywords>
		</article>
		<article id="taln-2010-court-028" session="Poster">
			<auteurs>
				<auteur>
					<prenom>Peggy</prenom>
					<nom>Cellier</nom>
					<email>Peggy.Cellier@info.unicaen.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Thierry</prenom>
					<nom>Charnois</nom>
					<email>Thierry.Charnois@info.unicaen.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">GREYC – CNRS UMR 6072, Université de Caen – Bd Mal Juin 14032 Caen, France</affiliation>
			</affiliations>
			<titre>Fouille de données séquentielles d’itemsets pour l’apprentissage de patrons linguistiques</titre>
			<type>court</type>
			<pages></pages>
			<resume>Dans cet article nous présentons une méthode utilisant l’extraction de motifs séquentiels d’itemsets pour l’apprentissage automatique de patrons linguistiques. De plus, nous proposons de nous appuyer sur l’ordre partiel existant entre les motifs pour les énumérer de façon structurée et ainsi faciliter leur validation en tant que patrons linguistiques.</resume>
			<mots_cles>Fouille de données, motifs séquentiels, extraction d’information, apprentissage de patrons linguistiques</mots_cles>
			<title></title>
			<abstract>In this paper, we present a method based on the extraction of itemset sequential patterns in order to automatically generate linguistic patterns. In addition, we propose to use the partial ordering between sequential patterns to enumerate and validate them.</abstract>
			<keywords>Data mining, sequential patterns, information extraction, linguistic pattern learning</keywords>
		</article>
		<article id="taln-2010-court-029" session="Poster">
			<auteurs>
				<auteur>
					<prenom>Anouar</prenom>
					<nom>Ben Hassena</nom>
					<email>benhasse@enssat.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Laurent</prenom>
					<nom>Miclet</nom>
					<email>miclet@enssat.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">ENSSAT / IRISA, Lannion, France</affiliation>
			</affiliations>
			<titre></titre>
			<type>court</type>
			<pages></pages>
			<resume>En intelligence artificielle, l’analogie est utilisée comme une technique de raisonnement non exact pour la résolution de problèmes, la compréhension du langage naturel, l’apprentissage des règles de classification, etc. Cet article s’intéresse à la proportion analogique, une forme simple du raisonnement par analogie, et présente son application en apprentissage automatique pour le TALN. La proportion analogique est une relation entre quatre objets qui exprime que la manière de transformer le premier objet en le second est la même que la façon de transformer le troisième en le quatrième. Premièrement, nous définissons formellement la proportion analogique entre quatre objets. Nous nous intéressons particulièrement aux objets structurés que sont les arbres ordonnés et étiquetés, avec une définition originale de l’analogie fondée sur l’alignement optimal. Ensuite, nous présentons deux algorithmes qui calculent la dissemblance analogique entre quatre arbres et qui trouvent des solutions, éventuellement approchées, à une équation analogique entre arbres. Nous montrons leur utilisation dans deux applications : l’apprentissage de l’arbre syntaxique d’une phrase et la génération de la prosodie dans la synthèse de parole.</resume>
			<mots_cles>Proportion analogique, arbre syntaxique, analyseur syntaxique analogique</mots_cles>
			<title>Tree analogical learning. Application in NLP</title>
			<abstract>In Artificial Intelligence, analogy is used as a non exact reasoning technique to solve problems, for natural language processing, for learning classification rules, etc. This paper is interested in the analogical proportion, a simple form of the reasoning by analogy, and presents some of its uses in machine learning for NLP. The analogical proportion is a relation between four objects that expresses that the way to transform the first object into the second is the same as the way to transform the third in the fourth. We firstly give definitions about the general notion of analogical proportion between four objects. We give a special focus on objects structured as ordered and labeled trees, with an original definition of analogy based on optimal alignment. Secondly, we present two algorithms which deal with tree analogical matching and solving analogical equations between trees. We show their use in two applications : the learning of the syntactic tree (parsing) of a sentence and the generation of prosody for synthetic speech.</abstract>
			<keywords>Analogical proportion, syntactic tree, analogical syntactic parser</keywords>
		</article>
		<article id="taln-2010-court-030" session="Poster">
			<auteurs>
				<auteur>
					<prenom>Mehdi</prenom>
					<nom>Embarek</nom>
					<email>mehdi.embarek@cea.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Olivier</prenom>
					<nom>Ferret</nom>
					<email>olivier.ferret@cea.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">CEA, LIST, Laboratoire Vision et Ingénierie des Contenus, Fontenay-aux-Roses, F-92265 France.</affiliation>
			</affiliations>
			<titre>Adapter un système de question-réponse en domaine ouvert au domaine médical</titre>
			<type>court</type>
			<pages></pages>
			<resume>Dans cet article, nous présentons Esculape, un système de question-réponse en français dédié aux médecins généralistes et élaboré à partir d’OEdipe, un système de question-réponse en domaine ouvert. Esculape ajoute à OEdipe la capacité d’exploiter la structure d’un modèle du domaine, le domaine médical dans le cas présent. Malgré l’existence d’un grand nombre de ressources dans ce domaine (UMLS, MeSH ...), il n’est pas possible de se reposer entièrement sur ces ressources, et plus spécifiquement sur les relations qu’elles abritent, pour répondre aux questions. Nous montrons comment surmonter cette difficulté en apprenant de façon supervisée des patrons linguistiques d’extraction de relations et en les appliquant à l’extraction de réponses.</resume>
			<mots_cles>systèmes de question-réponse, extraction de relations, domaine médical</mots_cles>
			<title></title>
			<abstract>In this article, we present Esculape, a question-answering system for French dedicated to family doctors and built from OEdipe, an open-domain system. Esculape adds to OEdipe the capability to exploit the concepts and relations of a domain model, the medical domain in the present case. Although a large number of resources exist in this domain (UMLS, MeSH ...), it is not possible to rely only on them, and more specifically on the relations they contain, to answer questions. We show how this difficulty can be overcome by learning linguistic patterns for identifying relations and applying them to extract answers.</abstract>
			<keywords>question-answering systems, relation extraction, medical domain</keywords>
		</article>
		<article id="taln-2010-court-031" session="Poster">
			<auteurs>
				<auteur>
					<prenom>Inès</prenom>
					<nom>Zribi</nom>
					<email>ineszribi@gmail.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Souha</prenom>
					<nom>Mezghani Hammami</nom>
					<email>souha.mezghani@fsegs.rnu.tn</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Lamia</prenom>
					<nom>Hadrich Belguith</nom>
					<email>l.belguith@fsegs.rnu.tn</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">ANLP Research Group – Laboratoire MIRACL, Faculté des Sciences Economiques et de Gestion de Sfax, B.P. 1088, 3018 - Sfax – TUNISIE</affiliation>
			</affiliations>
			<titre>L’apport d’une approche hybride pour la reconnaissance des entités nommées en langue arabe</titre>
			<type>court</type>
			<pages></pages>
			<resume>Dans cet article, nous proposons une méthode hybride pour la reconnaissance des entités nommées pour la langue arabe. Cette méthode profite, d’une part, des avantages de l’utilisation d’une méthode d’apprentissage pour extraire des règles permettant l’identification et la classification des entités nommées. D’autre part, elle repose sur un ensemble de règles extraites manuellement pour corriger et améliorer le résultat de la méthode d’apprentissage. Les résultats de l’évaluation de la méthode proposée sont encourageants. Nous avons obtenu un taux global de F-mesure égal à 79.24%.</resume>
			<mots_cles>Traitement de la langue arabe, reconnaissance des entités nommées, méthode d’apprentissage</mots_cles>
			<title></title>
			<abstract>In this paper, we propose a hybrid method for Arabic named entities recognition. This method takes advantage of the use of a learning method to extract rules for the identification and classification of named entities. Moreover, it is based on a set of rules extracted manually to correct and improve the outcome of the learning method. The evaluation results are encouraging as we get an overall F-measure equal to 79.24%.</abstract>
			<keywords>Arabic language processing, named entity recognition, learning method</keywords>
		</article>
		<article id="taln-2010-court-032" session="Poster">
			<auteurs>
				<auteur>
					<prenom>Richard</prenom>
					<nom>Moot</nom>
					<email>Richard.Moot@labri.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LaBRI (CNRS, Bordeaux) &amp; SIGNES (INRIA Bordeaux SW), 351 cours de la Libération, 33405 Talence, FRANCE</affiliation>
			</affiliations>
			<titre></titre>
			<type>court</type>
			<pages></pages>
			<resume>Cet article décrit le développement d’une grammaire catégorielle à large couverture du Français, extraite à partir du corpus arboré de Paris 7 et vérifiée et corrigée manuellement. Le grammaire catégorielle résultant est évaluée en utilisant un supertagger et obtient des résultats comparables aux meilleurs supertaggers pour l’Anglais.</resume>
			<mots_cles>Extraction de grammaires, grammaires catégorielles, supertagging</mots_cles>
			<title>Semi-automated Extraction of a Wide-Coverage Type-Logical Grammar for French</title>
			<abstract>The paper describes the development of a wide-coverage type-logical grammar for French, which has been extracted from the Paris 7 treebank and received a significant amount of manual verification and cleanup. The resulting treebank is evaluated using a supertagger and performs at a level comparable to the best supertagging results for English.</abstract>
			<keywords>Categorial grammar, grammar extraction, supertagging, type-logical grammar</keywords>
		</article>
		<article id="taln-2010-court-033" session="Poster">
			<auteurs>
				<auteur>
					<prenom>Yayoi</prenom>
					<nom>Nakamura-Delloye</nom>
					<email>yayoi@yayoi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Éric</prenom>
					<nom>Villemonte De La Clergerie</nom>
					<email>eric.de_la_clergerie@inria.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">ALPAGE, INRIA-Rocquencourt, Domaine de Voluceau Rocquencourt B.P.105, 78153 Le Chesnay</affiliation>
			</affiliations>
			<titre>Exploitation de résultats d’analyse syntaxique pour extraction semi-supervisée des chemins de relations</titre>
			<type>court</type>
			<pages></pages>
			<resume>Le présent article décrit un travail en cours sur l’acquisition des patrons de relations entre entités nommées à partir de résultats d’analyse syntaxique. Sans aucun patron prédéfini, notre méthode fournit des chemins syntaxiques susceptibles de représenter une relation donnée à partir de quelques exemples de couples d’entités nommées entretenant la relation en question.</resume>
			<mots_cles>Extraction des connaissances, extraction des patrons, relation des entités nommées, arbre syntaxique dépendanciel</mots_cles>
			<title></title>
			<abstract>This paper describes our current work on the acquisition of named entity relation patterns from parsing results. Without any predefined pattern, our method provides candidate syntactic paths that represent a given relationship with a small seed set of named entity pairs on this relationship.</abstract>
			<keywords>Knowledge extraction, pattern extraction, named entity relation, syntactic dependency tree</keywords>
		</article>
		<article id="taln-2010-court-034" session="Poster">
			<auteurs>
				<auteur>
					<prenom>Damien</prenom>
					<nom>Nouvel</nom>
					<email>Damien.Nouvel@univ-tours.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Arnaud</prenom>
					<nom>Soulet</nom>
					<email>Arnaud.Soulet@univ-tours.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Jean-Yves</prenom>
					<nom>Antoine</nom>
					<email>Jean-Yves.Antoine@univ-tours.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Nathalie</prenom>
					<nom>Friburger</nom>
					<email>Nathalie.Friburger@univ-tours.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Denis</prenom>
					<nom>Maurel</nom>
					<email>Denis.Maurel@univ-tours.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université François Rabelais Tours, LI, Antenne Universitaire de Blois, 3 place Jean Jaurès, F-41000 Blois, France</affiliation>
			</affiliations>
			<titre>Reconnaissance d’entités nommées : enrichissement d’un système à base de connaissances à partir de techniques de fouille de textes</titre>
			<type>court</type>
			<pages></pages>
			<resume>Dans cet article, nous présentons et analysons les résultats du système de reconnaissance d’entités nommées CasEN lors de sa participation à la campagne d’évaluation Ester2. Nous identifions quelles ont été les difficultés pour notre système, essentiellement : les mots hors-vocabulaire, la métonymie, les frontières des entités nommées. Puis nous proposons une approche pour améliorer les performances de systèmes à base de connaissances, en utilisant des techniques exhaustives de fouille de données séquentielles afin d’extraire des motifs qui représentent les structures linguistiques en jeu lors de la reconnaissance d’entités nommées. Enfin, nous décrivons l’expérimentation menée à cet effet, donnons les résultats obtenus à ce jour et en faisons une première analyse.</resume>
			<mots_cles>Reconnaissance d’Entités Nommées, Séquences Hiérarchiques, Motifs, Ester2</mots_cles>
			<title></title>
			<abstract>In this paper, we present and analyze the results obtained by our named entity recognition system, CasEN, during the Ester2 evaluation campaign.We identify on what difficulties our system was the most challenged, which mainly are : out-of-vocabulary words, metonymy and detection of the boundaries of named entities. Next, we propose a direction which may help us for improving performances of our system, by using exhaustive hierarchical and sequential data mining algorithms. This approach aims at extracting patterns corresponding to useful linguistic constructs for recognizing named entities. Finaly, we describe our experiments, give the results we currently obtain and analyze those results.</abstract>
			<keywords>Named Entity Recognition, Hierarchical Sequences, Patterns, Ester2</keywords>
		</article>
		<article id="taln-2010-court-035" session="Poster">
			<auteurs>
				<auteur>
					<prenom>Benoît</prenom>
					<nom>Gaillard</nom>
					<email>benoit.gaillard@univ-tlse2.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Olivier</prenom>
					<nom>Collin</nom>
					<email>olivier.collin@univ-tlse2.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Malek</prenom>
					<nom>Boualem</nom>
					<email>malek.boualem@orange-ftgroup.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Orange Labs – 2, Avenue Pierre Marzin, 22300 Lannion, France</affiliation>
			</affiliations>
			<titre>Traduction de requêtes basée sur Wikipédia</titre>
			<type>court</type>
			<pages></pages>
			<resume>Cet article s'inscrit dans le domaine de la recherche d'information multilingue. Il propose une méthode de traduction automatique de requêtes basée sur Wikipédia. Une phase d'analyse permet de segmenter la requête en syntagmes ou unités lexicales à traduire en s'appuyant sur les liens multilingues entre les articles de Wikipédia. Une deuxième phase permet de choisir, parmi les traductions possibles, celle qui est la plus cohérente en s'appuyant sur les informations d'ordre sémantique fournies par les catégories associées à chacun des articles de Wikipédia. Cet article justifie que les données issues de Wikipédia sont particulièrement pertinentes pour la traduction de requêtes, détaille l'approche proposée et son implémentation, et en démontre le potentiel par la comparaison du taux d'erreur du prototype de traduction avec celui d'autres services de traduction automatique.</resume>
			<mots_cles>recherche d'information multilingue, traduction de requêtes, Wikipédia</mots_cles>
			<title></title>
			<abstract>This work investigates query translation using only Wikipedia-based resources in a two steps approach: analysis and disambiguation. After arguing that data mined from Wikipedia is particularly relevant to query translation, we detail the implementation of the approach. In the analysis phase, queries are segmented into lexical units that are associated to several possible translations using a bilingual dictionary extracted from Wikipedia. During the second phase, one translation is chosen amongst the various candidates, based on consistency, asserted with the help of semantic information carried by categories associated to Wikipedia articles. These two steps take advantage of data mined from Wikipedia, which is very rich and detailed, constantly updated but also easy and free to access. We report promising results regarding translation accuracy.</abstract>
			<keywords>cross language information retrieval, query translation, Wikipedia</keywords>
		</article>
		<article id="taln-2010-court-036" session="Poster">
			<auteurs>
				<auteur>
					<prenom>Husam</prenom>
					<nom>Ali</nom>
					<email>ali@cs.uleth.ca</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Yllias</prenom>
					<nom>Chali</nom>
					<email>chali@cs.uleth.ca</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Sadid A.</prenom>
					<nom>Hasan</nom>
					<email>hasan@cs.uleth.ca</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">University of Lethbridge, Lethbridge, AB, Canada</affiliation>
			</affiliations>
			<titre></titre>
			<type>court</type>
			<pages></pages>
			<resume></resume>
			<mots_cles>Génération de questions, Analyseur syntaxique, Phrases élémentaires, POS Tagging</mots_cles>
			<title>Automatic Question Generation from Sentences</title>
			<abstract>Question Generation (QG) and Question Answering (QA) are some of the many challenges for natural language understanding and interfaces. As humans need to ask good questions, the potential benefits from automated QG systems may assist them in meeting useful inquiry needs. In this paper, we consider an automatic Sentence-to-Question generation task, where given a sentence, the Question Generation (QG) system generates a set of questions for which the sentence contains, implies, or needs answers. To facilitate the question generation task, we build elementary sentences from the input complex sentences using a syntactic parser. A named entity recognizer and a part of speech tagger are applied on each of these sentences to encode necessary information.We classify the sentences based on their subject, verb, object and preposition for determining the possible type of questions to be generated. We use the TREC-2007 (Question Answering Track) dataset for our experiments and evaluation.</abstract>
			<keywords>Question Generation, Syntactic Parsing, Elementary Sentence, POS Tagging</keywords>
		</article>
		<article id="taln-2010-demo-001" session="Démonstrations">
			<auteurs>
				<auteur>
					<prenom>Éric</prenom>
					<nom>Brunelle</nom>
					<email>developpement@druide.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Simon</prenom>
					<nom>Charest</nom>
					<email>developpement@druide.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Druide informatique inc., 1435, rue St-Alexandre, bureau 1040, Montréal (Québec) H3A 2G4, Canada</affiliation>
			</affiliations>
			<titre>Présentation du logiciel Antidote HD</titre>
			<type>démonstration</type>
			<pages></pages>
			<resume></resume>
			<mots_cles></mots_cles>
			<title></title>
			<abstract></abstract>
			<keywords></keywords>
		</article>
		<article id="taln-2010-demo-002" session="Démonstrations">
			<auteurs>
				<auteur>
					<prenom>Caroline</prenom>
					<nom>Barrière</nom>
					<email>caroline.barriere@nrc-cnrc.gc.ca</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">ITI-CNR, Gatineau, Canada</affiliation>
			</affiliations>
			<titre>TerminoWeb : recherche et analyse d’information thématique</titre>
			<type>démonstration</type>
			<pages></pages>
			<resume>Notre démonstration porte sur le prototype TerminoWeb, une plateforme Web qui permet (1) la construction automatique d’un corpus thématique à partir d’une recherche de documents sur le Web, (2) l’extraction de termes du corpus, et (3) la recherche d’information définitionnelle sur ces termes en corpus. La plateforme intégrant les trois modules, elle aidera un langagier (terminologue, traducteur, rédacteur) à découvrir un nouveau domaine (thème) en facilitant la recherche et l’analyse de documents informatifs pertinents à ce domaine.</resume>
			<mots_cles>information thématique, construction de corpus, extraction de termes, découverte de contextes définitionnels</mots_cles>
			<title></title>
			<abstract>Our demonstration shows the TerminoWeb prototype, a Web platform which can (1) automatically assemble a thematic corpus from Web documents, (2) extract terms from that corpus, and (3) find definitional information in the corpus about terms of interest. As the platform integrates all three modules, it can help a language worker (terminologist, translator, writer) to explore a new domain (theme) as it facilitates the gathering and analysis of informative documents about that domain.</abstract>
			<keywords>thematic information, corpus construction, term extraction, definitional contexts discovery</keywords>
		</article>
		<article id="taln-2010-demo-003" session="Démonstrations">
			<auteurs>
				<auteur>
					<prenom>Christian</prenom>
					<nom>Boitet</nom>
					<email>Christian.Boitet@imag.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Cong Phap</prenom>
					<nom>Huynh</nom>
					<email>Cong-Phap.Huynh@imag.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Hong Thai</prenom>
					<nom>Nguyen</nom>
					<email>Hong-Thai.Nguyen@imag.fr</email>
					<affiliationId></affiliationId>
				</auteur>
				<auteur>
					<prenom>Valérie</prenom>
					<nom>Bellynck</nom>
					<email>Valerie.Bellynck@imag.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">GETALP, LIG-campus (UJF, CNRS, INPG, INRIA, UPMF), BP 53, 38041 GRENOBLE cedex 9, France</affiliation>
			</affiliations>
			<titre></titre>
			<type>démonstration</type>
			<pages></pages>
			<resume></resume>
			<mots_cles></mots_cles>
			<title>The iMAG concept: multilingual access gateway to an elected Web sites with incremental quality increase through collaborative post-edition of MT pretranslations</title>
			<abstract>We will demonstrate iMAGs (interactive Multilingual Access Gateways), in particular on a scientific laboratory web site and on the Greater Grenoble (La Métro) web site.</abstract>
			<keywords>Interactive translation gateway, iMAG, MT post-editing, collaborative translation</keywords>
		</article>
		<article id="taln-2010-demo-004" session="Démonstrations">
			<auteurs>
				<auteur>
					<prenom>Wajdi</prenom>
					<nom>Zaghouani</nom>
					<email>wajdiz@ldc.upenn.edu</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Linguistic Data Consortium, 3600 Market street suite 810, Philadelphia, USA</affiliation>
			</affiliations>
			<titre>L'intégration d'un outil de repérage d'entités nommées pour la langue arabe dans un système de veille</titre>
			<type>démonstration</type>
			<pages></pages>
			<resume>Dans cette démonstration, nous présentons l'implémentation d'un outil de repérage d'entités nommées à base de règle pour la langue arabe dans le système de veille médiatique EMM (Europe Media Monitor).</resume>
			<mots_cles>Étiquetage des entités nommées, langue arabe, système de veille médiatique</mots_cles>
			<title></title>
			<abstract>We will present in this demo an Arabic rule-based named entity recogntion tool which is integrated within the news monitoring system EMM (Europe Media Montior).</abstract>
			<keywords>Named entity recognition, Arabic language, news monitoring system</keywords>
		</article>
		<article id="taln-2010-demo-005" session="Démonstrations">
			<auteurs>
				<auteur>
					<prenom>Olivier</prenom>
					<nom>Blanc</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Noémi</prenom>
					<nom>Boubel</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Jean-Philippe</prenom>
					<nom>Goldman</nom>
					<email></email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Sophie</prenom>
					<nom>Roekhaut</nom>
					<email></email>
					<affiliationId>3</affiliationId>
				</auteur>
				<auteur>
					<prenom>Anne Catherine</prenom>
					<nom>Simon</nom>
					<email></email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Cédrick</prenom>
					<nom>Fairon</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Richard</prenom>
					<nom>Beaufort</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">CENTAL, Université catholique de Louvain, 1348 Louvain-la-Neuve, Belgique</affiliation>
				<affiliation affiliationId="2">Valibel, Université catholique de Louvain, 1348 Louvain-la-Neuve, Belgique</affiliation>
				<affiliation affiliationId="3">TCTS Lab, Université de Mons, 7000 Mons, Belgique</affiliation>
			</affiliations>
			<titre>Expressive : Génération automatique de parole expressive à partir de données non linguistiques</titre>
			<type>démonstration</type>
			<pages></pages>
			<resume>Nous présentons Expressive, un système de génération de parole expressive à partir de données non linguistiques. Ce système est composé de deux outils distincts : Taittingen, un générateur automatique de textes d’une grande variété lexico-syntaxique produits à partir d’une représentation conceptuelle du discours, et StyloPhone, un système de synthèse vocale multi-styles qui s’attache à rendre le discours produit attractif et naturel en proposant différents styles vocaux.</resume>
			<mots_cles>Génération de texte, synthèse vocale, expressivité</mots_cles>
			<title></title>
			<abstract>We present Expressive, a system that converts non-linguistic data into expressive speech. This system is made of two distinct parts : Taittingen, a natural language generation tool able to produce lexically and syntactically rich texts from a discourse abstract representation, and StyloPhone, a text-to-speech synthesis system that proposes varying speaking styles, to make the speech sound both more attractive and natural.</abstract>
			<keywords>Natural language generation, text-to-speech synthesis, expressiveness</keywords>
		</article>
		<article id="taln-2010-demo-006" session="Démonstrations">
			<auteurs>
				<auteur>
					<prenom>Fatiha</prenom>
					<nom>Sadat</nom>
					<email>sadat.fatiha@uqam.ca</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Alexandre</prenom>
					<nom>Terrasa</nom>
					<email>alexandre.terrasa@viacesi.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université du Québec à Montréal, 201 av. President Kennedy, Montréal, QC, H3X 2Y3, Canada</affiliation>
				<affiliation affiliationId="2">École Supérieure d’Informatique Appliquée EXIA.CESI, 11 avenue Neil Armstrong, 33700 Mérignac, Bordeaux, France</affiliation>
			</affiliations>
			<titre>Exploitation de Wikipédia pour l’Enrichissement et la Construction des Ressources Linguistiques</titre>
			<type>démonstration</type>
			<pages></pages>
			<resume>Cet article présente une approche et des résultats utilisant l'encyclopédie en ligne Wikipédia comme ressource semi-structurée de connaissances linguistiques et en particulier comme un corpus comparable pour l’extraction de terminologie bilingue. Cette approche tend à extraire d’abord des paires de terme et traduction à partir de types des informations, liens et textes de Wikipédia. L’étape suivante consiste à l’utilisation de l’information linguistique afin de ré-ordonner les termes et leurs traductions pertinentes et ainsi éliminer les termes cibles inutiles. Les évaluations préliminaires utilisant les paires de langues français-anglais, japonais-français et japonais-anglais ont montré une bonne qualité des paires de termes extraits. Cette étude est très favorable pour la construction et l’enrichissement des ressources linguistiques tels que les dictionnaires et ontologies multilingues. Aussi, elle est très utile pour un système de recherche d’information translinguistique (RIT).</resume>
			<mots_cles>Terminologie bilingue, corpus comparable, Wikipédia, ontologie multilingue</mots_cles>
			<title></title>
			<abstract>Multilingual linguistic resources are usually constructed from parallel corpora, but since these corpora are available only for selected text domains and language pairs, the potential of other resources is being explored as well. This article seeks to explore and exploit the idea of using multilingual web-based encyclopaedias such as Wikipedia as comparable corpora for bilingual terminology extraction. We propose an approach to extract terms and their translations from different types of Wikipedia link information and texts. The next step will be using a linguistic-based information to re-rank and filter the extracted term candidates in the target language. Preliminary evaluations using the combined statisticsbased and linguistic-based approaches were applied on Japanese-French, French-English and Japanese- French. These evaluations showed a real open improvement and good quality of the extracted term candidates for building or enriching multilingual ontologies, dictionaries or feeding a cross-language information retrieval system with the related expansion terms of the source query.</abstract>
			<keywords>Bilingual terminology, comparable corpora, Wikipedia, mulilingual ontologies</keywords>
		</article>
		<article id="taln-2010-demo-007" session="Démonstrations">
			<auteurs>
				<auteur>
					<prenom>Annelies</prenom>
					<nom>Braffort</nom>
					<email>annelies.braffort@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Michael</prenom>
					<nom>Filhol</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Jérémie</prenom>
					<nom>Segouat</nom>
					<email></email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIMSI-CNRS, Campus d'Orsay bat. 508, BP 133, F691403 Orsay cx, France</affiliation>
				<affiliation affiliationId="2">WebSourd, 99 route d’Espagne, F631100 Toulouse, France</affiliation>
			</affiliations>
			<titre>Traitement automatique des langues des signes : le projet Dicta-Sign, des corpus aux applications</titre>
			<type>démonstration</type>
			<pages></pages>
			<resume>Cet article présente Dicta-Sign, un projet de recherche sur le traitement automatique des langues des signes (LS), qui aborde un grand nombre de questions de recherche : linguistique de corpus, modélisation linguistique, reconnaissance et génération automatique. L’objectif de ce projet est de réaliser trois applications prototypes destinées aux usagers sourds : un traducteur de termes de LS à LS, un outil de recherche par l’exemple et un Wiki en LS. Pour cela, quatre corpus comparables de cinq heures de dialogue seront produits et analysés. De plus, des avancées significatives sont attendues dans le domaine des outils d’annotation. Dans ce projet, le LIMSI est en charge de l’élaboration des modèles linguistiques et participe aux aspects corpus et génération automatique. Nous nous proposons d’illustrer l’état d’avancement de Dicta-Sign au travers de vidéos extraites du corpus et de démonstrations des outils de traitement et de génération d’animations de signeur virtuel.</resume>
			<mots_cles>Langue des signes, corpus vidéo comparables, reconnaissance automatique, génération automatique</mots_cles>
			<title></title>
			<abstract>This paper presents Dicta-Sign, a research project related to sign language (SL) processing. It covers numerous research topics: corpus linguistics, linguistic modelling, automatic recognition and generation. The aim of this project is to design three proof-of-concept end user applications: an SL-to-SL term translator, a search-by-example tool, and a SL wiki. For that, four comparable corpora of five hours will be produced and analysed. Aside from these applications, we also expect major improvements to be integrated to annotation tools. In this project, LIMSI is in charge of the linguistic modelling, and participates in building the corpus and in the generation efforts. We propose to illustrate the current work with excerpts from the corpus and demonstrations of the processing and generation tools.</abstract>
			<keywords>Sign languages, comparable video corpora, automatic recognition, automatic generation</keywords>
		</article>
		<article id="taln-2010-demo-008" session="Démonstrations">
			<auteurs>
				<auteur>
					<prenom>Jean-Philippe</prenom>
					<nom>Goldman</nom>
					<email>Jean-Philippe.Goldman@unige.ch</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Kamel</prenom>
					<nom>Nebhi</nom>
					<email>Kamel.Nebhi@unige.ch</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Christopher</prenom>
					<nom>Laenzlinger</nom>
					<email>Christopher.Laenzlinger@unige.ch</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LATL - Laboratoire d’Analyse et de Technologie du Langage, Département de Linguistique, Faculté des Lettres, Université de Genève</affiliation>
			</affiliations>
			<titre>FipsColor : grammaire en couleur interactive pour l’apprentissage du français</titre>
			<type>démonstration</type>
			<pages></pages>
			<resume>L'analyseur multilingue FiPS permet de transformer une phrase en une structure syntaxique riche et accompagnée d'informations lexicales, grammaticales et thématiques. On décrit ici une application qui adapte les structures en constituants de l’analyseur FiPS à une nomenclature grammaticale permettant la représentation en couleur. Cette application interactive et disponible en ligne (http://latl.unige.ch/fipscolor) peut être utilisée librement par les enseignants et élèves de primaire.</resume>
			<mots_cles>analyse syntaxique, grammaire générative, services web, tei</mots_cles>
			<title></title>
			<abstract>The FiPS parser analyzes a sentence into a syntactic structure reflecting lexical, grammatical and thematic information. The present paper describes the adaptation of the structures in terms of constituents as existent in FiPS to a grammatical annotation, as well as its coloured representation. This online interactive application (available at http://latl.unige.ch/fipscolor) can be freely used by teachers and pupils of primary education.</abstract>
			<keywords>chart parser, generative grammar, web services, tei</keywords>
		</article>
		<article id="taln-2010-demo-009" session="Démonstrations">
			<auteurs>
				<auteur>
					<prenom>Yves</prenom>
					<nom>Scherrer</nom>
					<email>yves.scherrer@unige.ch</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université de Genève, Rue de Candolle 5, 1211 Genève 4, Suisse</affiliation>
			</affiliations>
			<titre>Des cartes dialectologiques numérisées pour le TALN</titre>
			<type>démonstration</type>
			<pages></pages>
			<resume>Cette démonstration présente une interface web pour des données numérisées de l’atlas linguistique de la Suisse allemande. Nous présentons d’abord l’intégration des données brutes et des données interpolées de l’atlas dans une interface basée sur Google Maps. Ensuite, nous montrons des prototypes de systèmes de traduction automatique et d’identification de dialectes qui s’appuient sur ces données dialectologiques numérisées.</resume>
			<mots_cles>Dialectologie, atlas linguistique, traduction automatique, identification de dialectes</mots_cles>
			<title></title>
			<abstract>This demonstration presents a web interface for digitized data of the linguistic atlas of German-speaking Switzerland. First, we present the integration of raw and interpolated atlas data with an interface based on Google Maps. Then, we show prototypes of machine translation and dialect identification systems which rely on the digitized dialectological data.</abstract>
			<keywords>Dialectology, linguistic atlas, machine translation, dialect identification</keywords>
		</article>
		<article id="taln-2010-demo-010" session="Démonstrations">
			<auteurs>
				<auteur>
					<prenom>Richard</prenom>
					<nom>Beaufort</nom>
					<email>richard.beaufort@uclouvain.be</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Kévin</prenom>
					<nom>Macé</nom>
					<email>kevin.mace@uclouvain.be</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Cédrick</prenom>
					<nom>Fairon</nom>
					<email>cedrick.fairon@uclouvain.be</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">CENTAL, Université catholique de Louvain, 1348 Louvain-la-Neuve, Belgique</affiliation>
			</affiliations>
			<titre>Text-it /Voice-it Une application mobile de normalisation des SMS</titre>
			<type>démonstration</type>
			<pages></pages>
			<resume>Cet article présente Text-it / Voice-it, une application de normalisation des SMS pour téléphone mobile. L’application permet d’envoyer et de recevoir des SMS normalisés, et offre le choix entre un résultat textuel (Text-it) et vocal (Voice-it).</resume>
			<mots_cles>SMS, normalisation, application, plugin, serveur</mots_cles>
			<title></title>
			<abstract>This paper presents Text-it / Voice-it, an application that makes it possible to normalize text messages directly from mobile phones. The application allows the user to both send and receive normalized text messages, and gives the choice between a textual (Text-it) and a vocal (Voice-it) result.</abstract>
			<keywords>Text messages, normalization, application, plugin, server</keywords>
		</article>
		<article id="taln-2010-demo-011" session="Démonstrations">
			<auteurs>
				<auteur>
					<prenom>Richard</prenom>
					<nom>Moot</nom>
					<email>Richard.Moot@labri.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LaBRI (CNRS, Bordeaux) &amp; SIGNES (INRIA Bordeaux SW), 351 cours de la Libération, 33405 Talence, FRANCE</affiliation>
			</affiliations>
			<titre></titre>
			<type>démonstration</type>
			<pages></pages>
			<resume>Cette démonstration décrit Grail : un analyseur syntaxique pour grammaires catégorielles. Elle met l’accent sur les recherches récentes qui ont permis à Grail de donner des analyses syntaxiques et sémantiques du Français. Ces développements sont possibles grâce à une grammaire extraite semiautomatiquement du corpus de Paris 7 ainsi qu’un lexique sémantique qui traduit des combinaisons de mots, des étiquettes syntaxiques et des formules en Discourse Representation Structures.</resume>
			<mots_cles>Discourse Representation Theory, grammaires catégorielles</mots_cles>
			<title>Wide-Coverage French Syntax and Semantics using Grail</title>
			<abstract>The system demo introduces Grail, a general-purpose parser for multimodal categorial grammars, with special emphasis on recent research which makes Grail suitable for wide-coverage French syntax and semantics. These developments have been possible thanks to a categorial grammar which has been extracted semi-automatically from the Paris 7 treebank and a semantic lexicon which maps word, part-of-speech tags and formulas combinations to Discourse Representation Structures.</abstract>
			<keywords>Categorial grammar, Discourse Representation Theory, type-logical grammar</keywords>
		</article>
		<article id="taln-2010-demo-012" session="Démonstrations">
			<auteurs>
				<auteur>
					<prenom>Asma</prenom>
					<nom>Ben Abacha</nom>
					<email>asma.benabacha@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Pierre</prenom>
					<nom>Zweigenbaum</nom>
					<email>pz@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIMSI - CNRS, B.P. 133 91403 ORSAY CEDEX FRANCE</affiliation>
			</affiliations>
			<titre>MeTAE : Plate-forme d’annotation automatique et d’exploration sémantiques pour le domaine médical</titre>
			<type>démonstration</type>
			<pages></pages>
			<resume>Nous présentons une plate-forme d’annotation sémantique et d’exploration de textes médicaux, appelée « MeTAE ». Le processus d’annotation automatique comporte une première étape de reconnaissance des entités médicales présentes dans les textes suivie d’une étape d’identification des relations sémantiques qui les relient. Cette identification se fonde sur des patrons linguistiques construits manuellement pour chaque type de relation. MeTAE génère des annotations RDF à partir des informations extraites et offre une interface d’exploration des textes annotés avec des requêtes sous forme de formulaire. La plate-forme peut être utilisée pour analyser sémantiquement les textes médicaux ou interroger la base d’annotation disponible pour avoir une/des réponses à une requête donnée (e.g. « ?X prévient maladie d’Alzheimer », équivalent à la question « comment prévenir la maladie d’Alzheimer ? »). Cette application peut être la base d’un système de questions-réponses pour le domaine médical.</resume>
			<mots_cles>Annotation sémantique, interrogation sémantique, domaine médical</mots_cles>
			<title></title>
			<abstract>This paper presents MeTAE, a platform for semantic annotation and exploration of medical texts. The annotation process encompasses medical entity recognition and semantic relationship identification between the retrieved entities. This identification is based on linguistic patterns constructed manually for each type of relation. MeTAE generates RDF annotations from the extracted information and allows semantic exploration of the annotated texts through a form-based interface. The platform can be used to semantically analyze medical texts or to explore the available annotation base through structured queries (e.g. “?X Prevents Alzheimer’s disease” for its natural-language equivalent: “how to prevent Alzheimer’s disease?”). MeTAE can be a basis for a medical question-answering system.</abstract>
			<keywords>Semantic annotation, semantic querying, medical domain</keywords>
		</article>
		<article id="taln-2010-demo-013" session="Démonstrations">
			<auteurs>
				<auteur>
					<prenom>Bruno</prenom>
					<nom>Guillaume</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Guy</prenom>
					<nom>Perrier</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">INRIA Nancy-Grand Est - LORIA - Nancy-Université</affiliation>
			</affiliations>
			<titre>LEOPAR, un analyseur syntaxique pour les grammaires d’interaction</titre>
			<type>démonstration</type>
			<pages></pages>
			<resume>Nous présentons ici l’analyseur syntaxique LEOPAR basé sur les grammaires d’interaction ainsi que d’autres outils utiles pour notre chaîne de traitement syntaxique.</resume>
			<mots_cles>Analyse syntaxique, grammaires d’interaction, polarités</mots_cles>
			<title></title>
			<abstract>We present the parser LEOPAR which is based on the Interaction Grammars formalism. We present also other tools used in our framework for parsing.</abstract>
			<keywords>Parsing, Interaction Grammars, polarities</keywords>
		</article>
		<article id="taln-2010-demo-014" session="Démonstrations">
			<auteurs>
				<auteur>
					<prenom>Julien</prenom>
					<nom>Bourdaillet</nom>
					<email>bourdaij@iro.umontreal.ca</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Fabrizio</prenom>
					<nom>Gotti</nom>
					<email>gottif@iro.umontreal.ca</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Stéphane</prenom>
					<nom>Huet</nom>
					<email>huetstep@iro.umontreal.ca</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Philippe</prenom>
					<nom>Langlais</nom>
					<email>felipe@iro.umontreal.ca</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Guy</prenom>
					<nom>Lapalme</nom>
					<email>lapalme@iro.umontreal.ca</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">RALI - DIRO - Université de Montréal, C.P. 6128, succursale centre-ville, H3C 3J7, Montréal, Québec, Canada</affiliation>
			</affiliations>
			<titre>TransSearch : un moteur de recherche de traductions</titre>
			<type>démonstration</type>
			<pages></pages>
			<resume>Malgré les nombreuses études visant à améliorer la traduction automatique, la traduction assistée par ordinateur reste la solution préférée des traducteurs lorsqu’une sortie de qualité est recherchée. Cette démonstration vise à présenter le moteur de recherche de traductions TransSearch. Cetteapplication commerciale, accessible sur leWeb, repose d’une part sur l’exploitation d’un bitexte aligné au niveau des phrases, et d’autre part sur des modèles statistiques d’alignement de mots.</resume>
			<mots_cles>Traduction automatique statistique, repérage de traductions, alignement de mots, requêtes linguistiques</mots_cles>
			<title></title>
			<abstract>Despite the impressive amount of studies devoted to improving the state of the art of machine translation, computer assisted translation tools remain the preferred solution of human translators when publication quality is of concern. This demonstration presents the translation search engine TransSearch. This web-based commercial application relies on a sentence-aligned bitext and a statistical word alignment techniques.</abstract>
			<keywords>Statistical machine translation, translation spotting, word alignment, linguistic queries</keywords>
		</article>
		<article id="taln-2010-demo-015" session="Démonstrations">
			<auteurs>
				<auteur>
					<prenom>Graham</prenom>
					<nom>Russell</nom>
					<email>grussell@onscope.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Onscope Group Inc., 651 Notre-Dame Ouest, Montréal QC, Canada H3C 1J1</affiliation>
			</affiliations>
			<titre></titre>
			<type>démonstration</type>
			<pages></pages>
			<resume>Description de Moz, un système d’aide à la traduction conçu pour le traitement de textes structurés ou semi-structurés avec une forte proportion de contenu terminologique. Le système comporte une mémoire de traduction collaborative, qui atteint un niveau élevé de rappel grâce à l’analyse sousphrastique ; il fournit également des dispositifs de communication et de révision. Le système est en production et traduit 140 000 mots par semaine.</resume>
			<mots_cles>Aides à la traduction, sous-langage, analyse conceptuelle</mots_cles>
			<title>Moz: Translation of Structured Terminology-Rich Text</title>
			<abstract>Description of Moz, a translation support system designed for texts exhibiting a high proportion of structured and semi-structured terminological content. The system comprises a web-based collaborative translation memory, with high recall via subsentential linguistic analysis and facilities for messaging and quality assurance. It is in production use, translating some 140,000 words per week.</abstract>
			<keywords>Translation aids, sublanguage, conceptual analysis</keywords>
		</article>
		<article id="taln-2010-demo-016" session="Démonstrations">
			<auteurs>
				<auteur>
					<prenom>Alexis</prenom>
					<nom>Nasr</nom>
					<email>alexis.nasr@lif.univ-mrs.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Frédéric</prenom>
					<nom>Béchet</nom>
					<email>frederic.bechet@lif.univ-mrs.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Jean-François</prenom>
					<nom>Rey</nom>
					<email>jean-francois.rey@lif.univ-mrs.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Laboratoire d’Informatique Fondamentale de Marseille, Université Aix-Marseille</affiliation>
			</affiliations>
			<titre>MACAON Une chaîne linguistique pour le traitement de graphes de mots</titre>
			<type>démonstration</type>
			<pages></pages>
			<resume></resume>
			<mots_cles></mots_cles>
			<title></title>
			<abstract></abstract>
			<keywords></keywords>
		</article>
	</articles>
</conference>
