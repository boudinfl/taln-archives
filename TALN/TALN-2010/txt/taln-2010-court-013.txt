TALN 2010 — Session Posters, Montreal, 19-23 juillet 2010

Un étiqueteur de roles grammaticaux libre pour le frangais intégré 51
Apache UIMA

Charles Dejean Manoel Fortun Clotilde Massot Vincent Pottier
Fabien Poulardl Matthieu Vernierl
(1) LINA, UMR6241, 44322 Nantes
{Fabien.Poulard, Matthieu.Vernier} @univ-nantes.fr

Résumé. L’ étiquetage des roles grammaticaux est une tache de pré-traitement récurrente. Pour le
frangais, deux outils sont majoritairement utilisés : TreeTagger et Brill. Nous proposons une démarche,
ne nécessitant aucune ressource, pour la création d’un modele de Markov caché (HMM) pour palier les
problémes de ces outils, et de licences notamment. Nous distribuons librement toutes les ressources liées
a ce travail.

Abstract. Part-of-speech tagging is a common preprocessing task. For the French language, Brill
and TreeTagger are the most often used tools. We propose a method, requiring no resource, to create a
Hidden Markov Model to get rid of the problems and licences of these tools. We freely distribute all the
resources related to this work.

M0tS-CléS I étiquetage grammatical, Modéle de Markov caché, UIlV[A, Brill, TreeTagger.

Keywords: grammatical tagging, Hidden Markov Model, UIlV[A, Brill, TreeTagger.

1 Introduction et besoins

Dans les travaux en traitement automatique des langues, l’étiquetage des roles grammaticaux dans les
textes est une tache de pré-traitement récurrente. Les résultats de l’étiquetage servent de support a des
taches plus complexes ou de plus haut niveau linguistique : l’eXtraction terminologique, la recherche d’in-
formations, la recherche de patrons grammaticaux et sémantiques, la fouille d’opinions, la catégorisation
de textes, la détection de dérivation de textes, etc. Autrement dit, de l’efﬁcacité et de la qualité de cette
étiquetage dépendent une majorité des problémes applicatifs ou de recherche. D’une part, les erreurs de
résolution des roles grammaticaux sont répercutées sur les taches suivantes. D’autre part, la performance
des outils d’étiquetage grammatical ont un impact non-négligeable sur le temps de calcul global des traite-
ments. Des temps de calcul trop importants sont un frein au passage a l’échelle des prototypes développés
dans un cadre scientiﬁque vers des outils industriels.

Pour le traitement du frangais, deux outils sont majoritairement utilisés : TreeTagger (Schmid, 1994) et

Brill (Brill, 1992). Les motivations qui expliquent notre intérét pour développer un nouvel étiqueteur de

roles grammaticaux pour le frangais sont essentiellement de trois types :

— La licence de TreeTagger est restreinte au cadre de la recherche académique. La version UNIX de Brill
est distribuée librement mais a notre connaissance les ressources pour le frangais ne sont pas distribuées

UN ETIQUETEUR DE RGLES GRAMMATICAUX LIBRE POUR LE FRANCAIS

sous une licence permissive, le droit par défaut (restrictif) s’applique alors. Dans ce contexte, il est
donc difﬁcile de promouvoir des prototypes d’applications scientiﬁques dans un contexte industriel
sans trouver une alternative et revoir alors son intégration avec le reste de l’application.

— L’ utilisation automatisée de Brill et TreeTagger dans une chaine de composants est d’ailleurs un aspect
critique du point de vue de l’interopérabilité. Les formats d’entrée/sortie de ces outils leurs sont propres
et requierent le développement de composants d’encapsulation. Par ailleurs, quelques cas de bogues
(ou des temps de calcul tres long) existent notamment pour les textes tres volumineux, peu ponctués ou
encodés de maniere non-homogene dans le format attendu par l’outil.

— Enﬁn, d’un point de vue scientiﬁque, nous souhaitons observer la faisabilité de tirer proﬁt des deci-
sions différentes prises par Brill et TreeTagger pour améliorer la qualité des résultats en confrontant
et en apprenant de leurs erreurs. Nous n’utilisons aucune nouvelle ressource pour cela, mais annotons
automatiquement un corpus a l’aide de ces derniers.

Dans cet article, nous présentons deux expérimentations. Premierement, la comparaison de divergences

des étiqueteurs Brill et TreeTagger aﬁn de les évaluer comparativement (cf section 3.1). Deuxiemement,

l’apprentissage d’un modele de Markov caché pour le francais permettant d’annoter les roles grammati-
caux via l’outil HmmTagger1, écrit en langage Java et disponible sous la forme d’un composant Apache

UIMA2 (cf section 3.2).

2 Brill et TreeTagger

Brill (Brill, 1992) est un des systemes d’annotation de roles grammaticaux les plus souvent cité dans les
travaux francophones. Brill a fait l’objet d’un entrainement spéciﬁque pour le francais (Lecomte & Parou-
bek, 1998) a partir d’un corpus de 417 370 mots développé a cette occasion, et qui a les caractéristiques
suivantes : (i) il n’est pas distribuable, il contient des morceaux de texte dont les ayant droits ne permettent
pas la diffusion. (ii) 11 est essentiellement composé de textes littéraires en francais (Balzac, Zola, Dumas,
F laubert, etc.). (iii) Chacun des mots de ces textes a été étiqueté manuellement selon le jeux d’étiquettes
choisi pour le francais. Brill utilise un lexique et des ﬁchiers de regles. Le lexique contient une liste de
mots, chacun d’euX est associé a une liste d’étiquettes classées dans l’ordre décroissant de fréquences
d’apparitions dans le corpus annoté. Les regles permettent de déterminer une étiquette probable a partir
du contexte quand un mot n’est pas dans le lexique (ex : DTN .'sg PRV .'sg NEXTTAG VCJ .'sg).

Le TreeTagger (Schmid, 1994) remplit la meme tache avec un jeu d’étiquettes légerement different. 11 est
constitué de deux parties : l’une pour l’apprentissage d’une langue et l’autre pour l’annotation a propre-
ment parlé. Le systeme d’apprentissage requiert, pour une langue donnée, un lexique de formes ﬂéchies,
un jeu d’étiquettes et un corpus d’entrainement. L’ apprentissage effectué par TreeTagger consiste ensuite
a évaluer la probablité d’une transition entre un mot (ou sa catégorie grammaticale) et un autre mot (ou
sa catégorie grammaticale), puis a générer un arbre de décision binaire a partir des probalités calculées.
Pour le francais, le jeu est composé de 33 étiquettes grammaticales3. La phase d’entrainement permet de
produire un lexique contenant la liste des possibilités d’étiquetage pour chaque mot. 11 se scinde en trois
parties : un lexique de formes ﬂéchies, un lexique de sufﬁxes, une entrée par défaut. Pour chaque mot,
le TreeTagger sélectionne d’abord l’étiquette la plus probable selon le lexique des formes ﬂéchies (ou la
déduit a partir du sufﬁxe), puis il la corrige en mesurant la probabilité que l’étiquette c suive une séquence

de deux autres étiquettes ab.
1
2

3

http://uima.apache.org/downloads/sandbox/hmmTaggerUsersGuide/hmmTaggerUsersGuide.html
http://uima.apache.org
http://www.ims.uni—stuttgart.de/~schmid/french—tagset.html

UN ETIQUETEUR DE RGLES GRAMMATICAUX LIBRE POUR LE FRANCAIS

Commun Treetagger Brill Déﬁnition

ABR ABR ABR Abréviation

ADJ ADJ Adjectif

ADJ ADJ :sg Adjectif (sauf Participe passé) au singulier
ADJ ADJ :pl Adjectif (sauf Participe Passé) au pluriel
NUM NUM Numéral

NUM CAR Cardinal (en chiffres ou en lettres)

TAB. 1 — Extrait des correspondances considérées entre les jeux d’étiquettes de Brill et TreeTagger

3 Expérimentations

La plateforme UIlV[A (Unstructured Information Management Architecture) constitue notre paillasse.
UIMA est un environnement de développement dédié a la structuration de documents initié par [BM
et repris par la communauté Apache. Elle permet notamment, grace a la norme UIMA approuvée par
l’OASIS, d’assurer l’échange des données entre les composants d’une méme chaine de traitement et leur
interopérabilité, de ré-utiliser des composants existants et de construire des applications robustes com-
plexes destinées a la recherche ou a l’industrie.

Dans la boite a outils de UIlVIA, le composantHMMTagger4 implémente un modele de Markov caché
(HMM) qui permet d’annoter les roles grammaticaux d’un texte préalablement découpé en mots. Le
HMMTagger nécessite pour cela un modele de Markov caché pour la langue a traiter. I1 n’existe pas
de tel modele pour le francais. Notre objectif est d’entrainer un modele HMM pour le francais et de l’uti-
liser avec le composant HMMTagger de UIMA. Nous ne disposons d’aucune ressource (corpus annoté,
lexique, regles) en amont de ce travail. Des lors, la méthode envisagée consiste a construire un corpus
annoté sans le coﬁt d’une annotation manuelle en (i) annotant un corpus automatiquement a l’aide du
TreeTagger et de Brill (encapsulés dans UIlV[A par le LINA (Hernandez et al., 2010)); (ii) en regroupant
les jeux d’étiquettes de ces deux outils sous un jeu commun aﬁn de les comparer. Nous proﬁtons de cet
objectif principal pour comparer la qualité des annotations de TreeTagger par rapport a Brill, et comparer
leurs performances d’un point de vue du temps d’exécution.

Données expérimentales : un corpus et un jeu d’étiquettes commun Nous avons extrait des articles
dits de qualité sur Wikipédia dans des thématiques variées (31 documents). Ceux-ci correspondent selon
Wikipédia a un contenu riche et rédigé dans un bon francais. Nous ajoutons a ce corpus, des cours extraits
de la Wikiversity (10 documents), et quelques news de Wikinews (3 documents). Le corpus constitué
est composé de 496 886 mots, soit une taille similaire a celle du corpus utilisé pour l’entrainement de
Brill pour le francais. Ces textes ont également l’avantage d’étre librement utilisables et distribuables sous
licence CC-by-sa 3.05 pour Wikiversity et Wikipedia et CC-by 2.56 pour Wikinews.

Aﬁn de permettre une évaluation comparative de Brill et TreeTagger, nous alignons au préalable leur jeu

4http://uima.apache.org/downloads/sandbox/hmmTaggerUsersGuide/hmmTaggerUsersGuide.html

http://creativecommons.org/licenses/by—sa/3.0/deed.fr

Ghttp://creativecommons.org/licenses/by/2.5/deed.fr

UN ETIQUETEUR DE RGLES GRAMMATICAUX LIBRE POUR LE FRANCAIS

Exp. A Exp. B

Nombre prédictions évaluées 4453 2805
(0,89% corpus; 3,17% incohérences) (0,55% du total; 3,57% incohérences)

Nombre total d’évaluations 5486 4921
... dont erreur de Brill 961 (17,52 %) 739 (15,02 %)
. . . dont erreur de TTg(A)ﬂ-IMM(B) 3145 (57,33 %) 3177 (64,56 %)
... dont erreur des deux 801 (14,60 %) 618 (12,56 %)
... dont erreur dﬁ a la tokenisation 579 (10,55 %) 387 (7,86 %)
Evaluations qui se recouvrent 887 460
Kappa 0,99 0,96

TAB. 2 — Synthese des évaluations comparatives entre Brill et TreeTagger (Exp. A) et Brill et HMMTagger
(Exp. B) sur le jeu d’étiquettes commun.

d’étiquettes respectif pour établir un socle commun. Le tableau 1 présente un extrait7 de cette table de
correspondances. Il n’y a pas de probleme d’ambigu'1'té, il s’agit en general de renommer des étiquettes ou
d’enlever un niveau de spéciﬁcité propre a TreeTagger ou Brill selon les cas (par exemple, contrairement
a TreeTagger, Brill ajoute l’inforInation du nombre sur un adjectif).

3.1 Expérience A : Comparaison de TreeTagger et Brill

Evaluation des incohérences entre Brill et TreeTagger Une fois la projection vers le jeu commun
réalisée, deux cas peuvent se présenter pour un mot donné. Premierement, les prédictions de Brill et de
TreeTagger sont cohérentes : les deux étiquettes sont identiques. Ceci ne garantit pas la justesse de la
prédiction, mais montre que, pour le mot concerné, les deux outils concordent et qu’ils sont corrects
ou bien en erreur tous les deux. Deuxiemement, les prédictions des deux outils sont incohérentes : les
étiquettes sont différentes. Dans cette conﬁguration, il est nécessaire d’évaluer manuellement. Sur les 496
886 mots extraits de notre corpus, les outils sont en désaccord sur 103 598 (env. 20,30 %) soit un peu
plus de 5 mots par phrase. Six annotateurs humains ont évalué ces incohérences, en connaissance du mot
dans son contexte et des étiquettes du jeu commun posées par les deux outils. Ils pouvaient décider si l’un
ou l’autre était correct, si les deux avaient tort ou bien si l’erreur avait été provoquée par le découpage
en mots. Au total l’évaluation manuelle des 4 453 mots, soit 3,17 % des incohérences identiﬁées, a été
répartie entre les annotateurs. L’ accord inter-annotateur est excellent : l’indice Kappa est de 0,99. II a été
calculé sur 887 instances évaluées communément par deux des six annotateurs (cf Tableau 2).

Comparaison des résultats Habituellement, l’évaluation d’un outil se fait en absolu : on évalue l’outil
sur l’ensemble de son fonctionnement aﬁn d’obtenir un score estimant le nombre d’erreurs ou de succes.
Dans le cas présent, nous voulons évaluer les outils l’un par rapport a l’autre, nous avons donc besoin d’une
métrique permettant de comparer les performances des deux outils sans connaitre leur nombre d’erreurs
sur l’intégralité du corpus. Tout d’abord nous supposons que les résultats du Tableau 2 sont valables pour
toutes les incohérences (140 400 sur les 496 886 mots du corpus). Ainsi, le taux d’erreur de Brill sur les
incohérences serait de 63,,” = 17, 52% + 14, 60% + 10, 55% = 42, 67% (erreurs de Brill, erreurs des deux

7Le tableau dc correspondances complet est disponible dans les ressources distribuées

UN ETIQUETEUR DE RGLES GRAMMATICAUX LIBRE POUR LE FRANCAIS

et erreurs dﬁ a la tokenisation). Sur le reste du corpus, les prédictions étant cohérentes, le taux d’erreur de
Brill et de TreeTagger est identique et égal a oz. Le taux d’erreur de Brill sur le corpus complet est donc,

avec Cc et 0, respectivement le nombre d’éléments cohérents et incohérents, A3,,” = e*O°)gf(J‘:‘§/.T"“*C"),
C ’L
.\ . . . _ (a*C'c)+(6TTg*C',-) . ATTg _ (o:*C'c)+(6TTg*C',-)
de man1ere s1Imla1re ATTQ — -/.c+C1_ , so1t encore ABM“ — (a*Oc)+(6BM_”*Ci). Or 0 3 oz 3 100,

donc ATTQ = BABT,-ll avec 1.13 3 B 3 1.93. En d’autres termes, le taux d’erreur de TreeTagger est entre
1,13 fois et 1,93 fois plus élevé que celui de Brill. Ce résultat est notaInment pertinent pour l’évaluation
des performances du HMMTagger (cf section 3.2).

Fusion plus ﬁne des étiquettes vers le jeu commun A partir des évaluations manuelles des incohe-
rences, nous avons repris la méthode d’annotation automatique du corpus : lorsque Brill et TreeTagger
s’accordent sur une étiquette nous la retenons, lorsqu’ils sont en désaccord nous appliquons des regles
ad-hoc issues de l’évaluation manuelle. Cette fusion plus ﬁne nous a permis de réduire le nombre d’inco-
hérences de pres de 25 %, passant ainsi de 103 598 a 78 578.

Les regles de résolution sont issues de regles d’associations8 extraites de l’ensemble des évaluations ma-
nuelles. Chaque mot dont l’étiquette a pu étre corrigée lors de l’évaluation a été caractérisé par les éti-
quettes du jeu commun posées par Brill et TreeTagger, leurs étiquettes propres et le mot lui-méme (p. ex.
Tag0rigBrill=ECJ .'sg Tag0rigTTG=VER .'pres —> TagRetenu=VER .'ET). Nous n’avons conservé que
les regles dont la conﬁance est de 1.0 aﬁn de limiter l’introduction d’erreurs. Sur la soixantaine de regles
extraites, nous en avons implémenté ll, les autres regles subsumant les premieres.

3.2 Expérience B : Evaluation comparative du modéle HMM frangais avec Brill

A des ﬁns d’expérimentations, nous scindons notre corpus en deux (90 %/10 %) : un corpus d’apprentis-
sage, et un corpus de test pour l’évaluation comparative de Brill et HMMTagger.

Apprentissage du modéle HMM pour le frangais Il est a noter que 15 % des mots ne sont pas étiquetés
lorsque Brill et TreeTagger ne sont pas d’accord et qu’aucune regle de fusion pertinente n’a pu étre Inise en
oeuvre. Nous avons choisi de ne pas adopter de stratégie par défaut de type dans le doute, faire conﬁance
£1 Brill, pour ne pas biaiser notre approche. Le module d’apprentissage du HMMTagger utilise ensuite les
mots étiquetés apres fusion (mot + étiquette) pour générer un modele HMM.

Tests et évaluation A l’image de l’expérimentation A (cf section 3.1), nous effectuons une évaluation
comparative entre le HMMTagger développé et Brill, que nous considérons alors comme une meilleure
référence que TreeTagger. Nous appliquons sur les résultats présentés dans le Tableau 2 la métrique dis-
cutée précédemment. Ainsi, le taux d’erreur du HMMTagger (ﬁﬁ) est entre 1,03 fois et 2,4 fois plus
élevé que celui de Brill.

4 Discussion

Les résultats de l’évaluation comparative entre Brill et TreeTagger puis entre Brill et le HMMTagger sont
dans la meme fourchette (respectivement [1, 13..1, 93] et [1, 03.2, 4]). Brill est donc meilleur que les deux

8Les régles d’ association ont été calculées dans Weka a l’aide de l’algorithme weka.associations.Apriori

UN ETIQUETEUR DE RGLES GRAMMATICAUX LIBRE POUR LE FRANCAIS

Outil Treetagger Brill HMMTagger

Temps de traitement sur notre corpus 5 min 42 s. 6 Inin 51 s. 11 s.

TAB. 3 — Performance des composants Brill, TreeTagger et HMMTagger sur l’annotation du corpus test.

autres outils, le HMMTagger faisant potentiellement un peu plus d’erreurs que le TreeTagger. Le choix
de ne pas apprendre les annotations incohérentes a privé le HMMTagger de la connaissance de mots
fréquents pour lesquels il se résigne a un choix par défaut (p. ex. la préposition a, les ﬂexions du verbe
avoir, ...). L’implémentation de regles manuelles pour ces quelques cas devrait permettre d’améliorer
signiﬁcativement les résultats.

Les outils TreeTagger et Brill utilisés dans ces expérimentations sont des encapsulations en composants
UIMA des programmes originaux (Hernandez et al., 2010). De telles encapsulations posent des problemes
d’interopérabilité (les programmes de Brill et TreeTagger ne sont pas présents sur toutes les plateformes),
et de performances (chargement des ressources et des programmes en mémoire a chaque traitement). Au
contraire le HMMTagger, nativement implémenté en Java et intégré a UIIVIA, est de 29 a 35 fois plus
rapide que les outils encapsulés (cf Tableau 3).

D’apres Karl Popper (Popper, 1963), « une théorie n’est scientiﬁque si et seulement si elle peut étre refu-
tée ». Cette réfutabilité n’est possible qu’en réitérant les expérimentations. Dans ce sens, nous attachons
une importance particuliere a distribuer librement nos ressources (le modele, le code source et le corpus
permettant de le générer, ainsi que les résultats intermédiaires de nos expérimentations)9Nous encoura-
geons d’ailleurs la généralisation de cette démarche dans le cadre des travaux scientiﬁques en traitement
automatique des langues.

Références

BRILL E. (1992). A simple rule-based part of speech tagger. In Proceedings of the third conference on
Applied natural language processing, p. 152-155, Morristown, NJ, USA : Association for Computational
Linguistics.

HERNANDEZ N., POULARD F., VERNIER M. & ROCHETEAU J. (2010). New challenges for nlp fra-
meworks. In Proceedings of New Challenges for NLP Frameworks workshop in LREC’I0, Marrakech,
Morocco : European Language Resources Association (ELRA).

LECOMTE J. & PAROUBEK P. (1998). Le catégoriseur d’Eric BRILL. mise en oeuvre de la version
entrainée a l’INALF, rapport technique. Nancy.

POPPER K. (1963). Conjectures and Refutations.

SCHMID H. (1994). Probabilistic part-of-speech tagging using decision trees. In Proceedings of the
International Conference on New Methods in Language Processing, p. 44-49.

9Tous ces éléments sont archives at l’adresse http : //recherche . fabienpoulard. info/taln2010. Le com-
posant HMMTagger pour UIMA est disponible a l’adresse http : //uima . apache . org/sandbox . html#tagger .
annot at or, i1 sufﬁt de le conﬁgurer pour utiliser notre modéle hmmtagger_model_fr_20l 00501 .dat présent dans 1’archiVe.

