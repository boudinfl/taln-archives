<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
"http://www.w3.org/TR/html4/loose.dtd">
<html><head><title>Un syst&#232;me de d&#233;tection d&#8217;entit&#233;s nomm&#233;es adapt&#233; pour la campagne d&#8217;&#233;valuation ESTER 2</title>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
</head>
<body>
<div style="page-break-before:always; page-break-after:always"><div><p> </p>
<p>Un syst&#232;me de d&#233;tection d&#8217;entit&#233;s nomm&#233;es adapt&#233; pour la campagne 
d&#8217;&#233;valuation ESTER 2 
</p>
<p>Caroline Brun1   Maud Ehrmann2 
</p>
<p>(1)  XRCE, 6, Chemins de Maupertuis, Meylan, France 
(2) JRC &#8211; European Commission, Ispra, Italie 
</p>
<p>Caroline.Brun@xrce.xerox.com, maud.ehrmann@jrc.ec.europa.eu 
</p>
<p>R&#233;sum&#233; Dans cet article nous relatons notre participation &#224; la campagne d&#8217;&#233;valuation ESTER 2 
(Evaluation des Syst&#232;mes de Transcription Enrichie d&#8217;Emissions Radiophoniques). Apr&#232;s avoir d&#233;crit les 
objectifs de cette campagne ainsi que ses sp&#233;cificit&#233;s et difficult&#233;s, nous pr&#233;sentons notre syst&#232;me 
d&#8217;extraction d&#8217;entit&#233;s nomm&#233;es en nous focalisant sur les adaptations r&#233;alis&#233;es dans le cadre de cette 
campagne. Nous d&#233;crivons ensuite les r&#233;sultats obtenus lors de la comp&#233;tition, ainsi que des r&#233;sultats 
originaux obtenus par la suite. Nous concluons sur les le&#231;ons tir&#233;es de cette exp&#233;rience.  
</p>
<p>Abstract In this paper, we report our participation to the ESTER 2 (Evaluation des Syst&#232;mes de 
Transcription Enrichie d&#8217;Emissions Radiophoniques) evaluation campaign. After describing the goals, 
specificities and challenges of the campaign, we present our named entity detection system and focus on 
the adaptations made in the framework of the campaign. We present the results obtained during the 
competition and then new results obtained afterward. We then conclude by the lessons we learned from 
this experiment.    
</p>
<p>Mots-cl&#233;s :   entit&#233;s nomm&#233;es, &#233;valuation, extraction d&#8217;information. 
Keywords:   named entities,  evaluation, information extraction. 
</p>
<p>1 Introduction 
La campagne d&#8217;&#233;valuation ESTER 2 s&#8217;est d&#233;roul&#233;e de janvier 2008 &#224; avril 2009, dans la continuit&#233; de la 
premi&#232;re campagne ESTER1. L&#8217;objectif principal &#233;tait &#171;de promouvoir une dynamique de l'&#233;valuation en 
France, autour du traitement de la parole de langue fran&#231;aise, de mettre en place une structure p&#233;renne 
d'&#233;valuation et de diffuser le plus largement possible les informations et les ressources concern&#233;es par ces 
&#233;valuations&#187;2
</p>
<p>                                                 
1  Ces campagnes furent organis&#233;es par la DGA et l&#8217;AFCP. Site internet : http://www.afcp-parole.org/ester/index.html 
</p>
<p>2  http://www.afcp-parole.org/ester/present.html 
</p>
<p>. Ces campagnes visaient &#224; &#233;valuer les performances des syst&#232;mes de transcription de la 
parole,  les performances des syst&#232;mes de segmentation en tours de paroles, et la capacit&#233; &#224; extraire 
automatiquement des informations, en particulier les entit&#233;s nomm&#233;es (EN). Cette troisi&#232;me t&#226;che, &#224; 
laquelle se sont attel&#233;s 7 participants dans le cadre d&#8217;ESTER 2, est l&#8217;objet de cet article. Elle &#233;tait divis&#233;e </p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>CAROLINE BRUN, MAUD EHRMANN 
en deux sous-t&#226;ches : la d&#233;tection d&#8217;entit&#233;s nomm&#233;es sur transcriptions de r&#233;f&#233;rence (NE-ref) et sur 
transcriptions automatiques (NE-asr).   
</p>
<p>2 ESTER 2 en d&#233;tails 
</p>
<p>2.1 Sp&#233;cificit&#233;s et difficult&#233;s de la t&#226;che de d&#233;tection d&#8217;EN 
</p>
<p>Dans le cadre d&#8217;ESTER2, il s&#8217;agissait d&#8217;extraire et de cat&#233;goriser des mentions directes d&#8217;EN, selon un 
guide d&#8217;annotation comprenant 7 cat&#233;gories principales et 38 sous-cat&#233;gories :  
</p>
<p>Personnes : pers.hum (r&#233;elles ou fictives), pers.anim (animaux r&#233;els ou fictifs) 
</p>
<p>Fonctions : fonc.pol (politique), fonc.mil (militaire), fonc.admi (administrative), fonc.rel (religieuse), fonc.ari (aristocratique) 
Lieux : loc.geo (lieu g&#233;ographique), loc.admi (administratif), loc.line (voies de circulation), loc.adr (adresses), loc.adr.post (adresses postales), 
loc.adr.tel (fax et t&#233;l&#233;phones), loc.adr.elec (adresse &#233;lectroniques), loc.fac (b&#226;timents) 
Organisations : org.pol (politique), org.mil (militaire), org.edu (&#233;ducation), org.com (commerciale), org.non-profit (sans but lucratif), org.div 
(divertissement), org.gsp (g&#233;opolitique)  
Produits: prod.vehic (vehicules), prod.award (r&#233;compenses), prod.art (produits artistiques), prod.doc (documents) 
Temps : time.date (date), time.date.abs (date absolue), time.date.rel (date relative), hour (heures) 
Quantit&#233;s : amount.age (&#226;ge), amount.dur (dur&#233;e), amount.temp (temp&#233;rature), amount.len (longueur), amount.area (surface), amount.phys.vol 
(volume), amount.weight (poids), amount.spd (speed), amount.phys.cur (monnaies), amount.phys.other (autres) 
 
La principale instruction d&#8217;annotation est de consid&#233;rer les entit&#233;s en contexte, avec la prise en compte des 
ph&#233;nom&#232;nes d&#8217;ambigu&#239;t&#233;s et de m&#233;tonymie : par exemple, selon les contextes, &#171; Charles de Gaulle &#187; doit 
&#234;tre annot&#233; en tant que personne (le pr&#233;sident), v&#233;hicule (le porte-avion) ou encore lieu (l&#8217;a&#233;roport). 
L&#8217;annotation des noms de personnes inclut celle des fonctions et l&#8217;annotation des expressions temporelles 
couvre pour sa part un large &#233;ventail de possibilit&#233;s, des classiques &#171; Lundi matin &#187; aux plus complexes 
&#171; Il y a un peu moins de trois jours environ &#187;. Par ailleurs, dans la mesure o&#249; l&#8217;extraction d&#8217;entit&#233;s est 
r&#233;alis&#233;e sur des transcriptions de la parole, certains ph&#233;nom&#232;nes propres &#224; l&#8217;oral (h&#233;sitations ou r&#233;p&#233;titions) 
doivent &#234;tre inclus dans les annotations (&lt;pers.hum&gt; Jacques heu Chirac&lt;/pers.hum&gt;). Ces directives 
d&#8217;annotation sp&#233;cifiques, combin&#233;es au nombre important de cat&#233;gories &#224; prendre en compte, 
complexifient la t&#226;che d&#8217;annotation. En effet, les quantit&#233;s de type &#226;ge et dur&#233;e sont particuli&#232;rement 
difficiles &#224; distinguer des expressions temporelles, tout comme les lieux administratifs des entit&#233;s 
g&#233;opolitiques, puisqu&#8217;il s&#8217;agit de noms de villes ou de pays fr&#233;quemment employ&#233;s en tant que l&#8217;un ou 
l&#8217;autre. On peut donc constater que cette t&#226;che est plus ambitieuse que l&#8217;extraction d&#8217;EN &#171; classique &#187; (i.e. 
&#224; la MUC). 
</p>
<p>2.2 Questions ouver tes concernant l&#8217;annotation 
</p>
<p>Se mettre d&#8217;accord sur la mani&#232;re d&#8217;annoter des EN n&#8217;est pas chose facile. Ce probl&#232;me bien connu n&#8217;a 
pas manqu&#233; d&#8217;appara&#238;tre durant ESTER2, avec de nombreuses discussions et remises en cause du guide 
d&#8217;annotation, modifi&#233; au fur et &#224; mesure de la campagne jusqu&#8217;&#224; une version d&#233;finitive en janvier 2009. 
Les points d&#233;licats ont concern&#233;, parmi d&#8217;autres, les expressions temporelles et les fonctions. Pour ce qui 
est des premi&#232;res, il fut principalement question de l&#8217;extension des expressions temporelles (inclusion ou 
non des pr&#233;positions, d&#233;terminants et relatives), des difficiles distinctions entre dates et dur&#233;es, et entre une 
expression temporelle et une autre qui ne l&#8217;est pas. Concernant les fonctions,  deux points furent soulev&#233;s: 
le manque de crit&#232;res pour d&#233;finir la port&#233;e de cette cat&#233;gorie d&#8217;une part (il est facile de lister des fonctions 
&#171; standards &#187; mais bien d&#8217;autres posent probl&#232;mes) et, d&#8217;autre part, la pertinence d&#8217;annoter conjointement, 
comme il &#233;tait demand&#233; dans certains cas, personnes et fonctions (n&#8217;est-t-il pas pr&#233;f&#233;rable, d&#8217;un point de 
vue s&#233;mantique, d&#8217;annoter des relations entre noms de personnes et noms de fonction ?). </p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p> 
3 XIP &#224; ESTER 2 
Nous avons particip&#233; &#224; la campagne d&#8217;&#233;valuation ESTER2 en adaptant l&#8217;analyseur syntaxique robuste 
&#171; Xerox Incremental Parser &#187; (XIP,  (Ait-Mokthar et al., 2002)). XIP prend en entr&#233;e du texte tout venant, 
sous format texte ou XML,  et produit en sortie de fa&#231;on robuste une analyse syntaxique profonde. A partir 
d'un ensemble de r&#232;gles, l'analyseur d&#233;sambiguise les cat&#233;gories, construit les syntagmes noyaux et extrait 
des relations de d&#233;pendances syntaxiques. En plus de l'analyse des relations syntaxiques de surface, XIP 
effectue &#233;galement une analyse syntaxique dite &#171; profonde &#187;  ou  &#171; normalis&#233;e &#187;  (prise en compte des 
sujets et objets de verbes non finis, normalisation de la forme passive en forme active, etc.). Cet analyseur 
int&#232;gre &#233;galement un module de reconnaissance des entit&#233;s nomm&#233;es (Rebotier 2006), prenant en compte 
les types classiques d&#8217;entit&#233;s nomm&#233;es, &#224; savoir les expressions num&#233;riques, les monnaies, les dates, ainsi 
que les noms de lieux, de personnes et d'organisations.  Il s'agit d'un module &#224; base de r&#232;gles, consistant en 
un ensemble de r&#232;gles locales ordonn&#233;es utilisant des informations lexicales et des informations 
contextuelles concernant les parties du discours, les formes lemmatis&#233;es et un ensemble de traits lexico-
s&#233;mantiques.  
</p>
<p>Nous avons d&#251; adapter le syst&#232;me d&#233;velopp&#233; pour le fran&#231;ais aux consignes d&#8217;annotation ESTER 2, selon 
les axes suivants :  
</p>
<p>&#8226; Adaptation aux sp&#233;cificit&#233;s de la transcription : Les transcriptions de la parole, manuelles ou 
automatiques, ont des particularit&#233;s que l&#8217;on ne retrouve pas dans les textes &#171; standards &#187; ; il  s&#8217;agit de 
disfluences, de r&#233;p&#233;titions, ou encore de bruits : 
</p>
<p>&#171; Il y a encore euh quelques mois&#8230; &#187;, &#171; Une forme de de journalisme &#8230; &#187;, &#171; [r ir es-en-fond-] Voila ! 
[-r ir e-en-fond] &#187;  
 
Afin d&#8217;ignorer les disfluences et les bruits, nous avons converti les fichiers d&#8217;entr&#233;e originaux sous 
format XML, en marquant ces &#233;l&#233;ments comme des balises ouvrantes/fermantes (&lt;/heu&gt;, &lt;/[rires]&gt;)  
totalement transparentes pour les traitements linguistiques. Dans le cas des r&#233;p&#233;titions, nous avons 
d&#233;velopp&#233; des r&#232;gles qui groupent ces &#233;l&#233;ments sous un n&#339;ud de m&#234;me cat&#233;gorie qui h&#233;rite des traits 
du premier &#233;l&#233;ment.    
&#8226; Adaptation pour les cat&#233;gories &#171;standards &#187; : Nous avons tout d&#8217;abord utilis&#233; les corpus 
d&#8217;entra&#238;nement et de d&#233;veloppement pour collecter semi-automatiquement le vocabulaire inconnu 
(noms de lieux, d&#8217;organisations, etc.) et l&#8217;int&#233;grer &#224; nos lexiques. Nous avons ensuite adapt&#233; le syst&#232;me 
pour prendre en compte de nouvelles cat&#233;gories, telles que les fonctions, les &#226;ges, les productions 
humaines, et la plupart des quantit&#233;s, qui n&#8217;&#233;taient pas pr&#233;alablement couvertes par notre syst&#232;me. 
Nous avons &#233;galement adapt&#233; les r&#232;gles existantes selon le guide d&#8217;annotation, en particulier pour 
couvrir la port&#233;e des entit&#233;s ; par exemple, les d&#233;terminants et pr&#233;positions sont inclus dans les 
quantit&#233;s et les noms de fonction en apposition d&#8217;un nom de personne sont inclus dans ce dernier :    
</p>
<p>&#171; Il est &#226;g&#233;  &lt;amount.age&gt; de 18 ans &lt;/amount.age&gt; 
&#171; &lt;pers.hum&gt; Nicolas Sarkozy, pr&#233;sident de la r&#233;publique &lt;/pers.hum&gt; &#8230; &#187; 
Il s&#8217;est principalement agit ici de d&#233;velopper et de modifier des r&#232;gles locales de regroupement des 
noms propres, en amont de l&#8217;analyse en syntagmes noyaux (chunks).  
&#8226; Traitement des expressions temporelles : Le vocabulaire relatif aux expressions temporelles &#233;tant 
une liste ferm&#233;e, le c&#339;ur du travail fut l&#8217;&#233;criture de r&#232;gles locales et de chunking. L&#8217;attention fut port&#233;e 
sur les pr&#233;positions et adverbes principalement, ces derniers affectant radicalement le sens de telle ou 
telle expression (Il est parti &lt;amount.phys.dur&gt; pendant 10 mois &lt;/amount.phys.dur&gt;  vs. Il est parti </p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>CAROLINE BRUN, MAUD EHRMANN 
&lt;time.date.rel&gt;10 mois apr&#232;s &lt; /time.date.rel&gt;). Nous avons &#233;galement d&#251; prendre en compte certaines 
incidences de la transcription de parole, comme par exemple avec l&#8217;expression &#171; 19 cent 97 &#187;. 
</p>
<p>&#8226; Ambigu&#239;t&#233;s et m&#233;tonymies : Une des sp&#233;cificit&#233;s les plus int&#233;ressantes d&#8217;ESTER 2 est la prise en 
compte des ambiguit&#233;s et des ph&#233;nom&#232;nes de m&#233;tonymies. Afin d&#8217;&#234;tre &#224; m&#234;me de traiter ces cas, nous 
avons utilis&#233; les r&#233;sultats de l&#8217;analyse syntaxique profonde fournis par XIP. En nous r&#233;f&#233;rant au guide 
d&#8217;annotation, nous avons r&#233;alis&#233; une &#233;tude de corpus pour d&#233;tecter les r&#233;gularit&#233;s syntaxiques et 
lexicales d&#233;clenchant un glissement m&#233;tonymique ou permettant de r&#233;soudre une ambigu&#239;t&#233;, selon la 
m&#233;thodologie appliqu&#233;e dans (Brun et al 2007). Cette &#233;tude a conduit &#224; des hypoth&#232;ses telles que &#171; Si 
un nom de lieu de type administratif est sujet d&#8217;un verbe de communication, il est employ&#233; comme 
nom d&#8217;organisation g&#233;opolitique&#187;. L&#8217;analyseur fut alors enrichi par des lexiques s&#233;mantiques d&#233;di&#233;s et 
par des r&#232;gles de d&#233;pendance modifiant l&#8217;interpr&#233;tation des entit&#233;s, appliqu&#233;es en aval de l&#8217;analyse 
syntaxique, par exemple :  
</p>
<p>If (^LIEU[ADMI](#1) &amp; SUBJ(#2[v_communication],#1)) &#61664; ORG[GSP=+](#1)3
</p>
<p>4 Evaluation(s) 
</p>
<p> 
 
</p>
<p>Cette r&#232;gle peut s&#8217;appliquer sur une phrase comme &#171; Dakar parle de 28 millions d'euros &#187;, alors 
annot&#233;e &#171; &lt;org.gsp&gt; Dakar &lt;/org.gsp&gt; parle de 28 millions d'euros &#187;. Notre &#233;tude s&#8217;est concentr&#233;e  
sur les relations de type sujet, objet, modifieur (nominal et propositionnel) et attribut, et nous a 
conduites &#224; d&#233;velopper environ 150 r&#232;gles de d&#233;pendances suppl&#233;mentaires.   
</p>
<p>4.1 Corpus et calcul des scores 
Comme dit pr&#233;c&#233;demment, nous avons utilis&#233; les corpus d&#8217;entra&#238;nement (100 heures d&#8217;&#233;missions de 
</p>
<p>radio transcrites et annot&#233;es manuellement) et de d&#233;veloppement (6 heures de journaux radiophoniques 
transcrits et annot&#233;s manuellement)   pour la mise au point du syst&#232;me.  Le corpus de test &#233;tait constitu&#233; de 
7 heures de journaux radiophoniques datant de 2008. L&#8217;ensemble des corpus provenait de diff&#233;rentes 
sources : France Culture, France Inter,  Radio France International, Radio Classique, Africa 1, Radio 
Congo et Radio T&#233;l&#233;vision Marocaine. Du point de vue de l&#8217;&#233;valuation quantitative, m&#234;me si les mesures 
classiques de pr&#233;cision et rappel &#233;taient calcul&#233;es, la mesure &#171; officielle &#187; &#233;tait le &#171; Slot Error Rate &#187; (SER, 
voir (MAKHOUL et al. 1999)), qui combine et pond&#232;re les diff&#233;rents type d&#8217;erreurs (insertion, effacement, 
erreur de type) : SER = (Insertions+Effacements+Substitutions) / nb entit&#233;s ref. C&#8217;est une mesure analogue 
au &#171; Word Error rate &#187; (WER) utilis&#233; pour mesurer les performances des syst&#232;mes de transcriptions de la 
parole. D&#8217;autre part, si au d&#233;but de la campagne il &#233;tait pr&#233;vu d&#8217;&#233;valuer sur l&#8217;ensemble des sous-types, 
c&#8217;est seulement sur les 7 cat&#233;gories principales que les r&#233;sultats ont &#233;t&#233; calcul&#233;s. Enfin, les r&#233;sultats 
d&#233;finitifs ont &#233;t&#233; obtenus apr&#232;s une phase d&#8217;adjudication qui permettait aux participants de contester les 
annotations du corpus de test (sans bien &#233;videmment changer les r&#233;sultats de leur syst&#232;me). 
</p>
<p>4.2 R&#233;sultats obtenus dans le cadre d&#8217;ESTER 2 
</p>
<p>Le tableau I pr&#233;sente les r&#233;sultats obtenus par notre syst&#232;me sur transcriptions de r&#233;f&#233;rence (NE-Ref) en 
termes de pr&#233;cision, rappel, f-mesure et slot error rate. Les r&#233;sultats de notre syst&#232;me sur les transcriptions 
automatiques (NE-Asr) sont publi&#233;s dans (Brun et Ehrmann, 2009), les r&#233;sultats complets de la campagne 
dans (Galliano et al, 2009).  Avec un SER de 9.80 (et une f-mesure de 0,93), ces r&#233;sultats s&#8217;av&#232;rent tr&#232;s 
</p>
<p>                                                 
3  Cette r&#232;gle se lit de la mani&#232;re suivante : si le parseur a d&#233;tect&#233; un nom de LIEU avec l&#8217;attribut &#171; admi &#187; (#1), et que ce nom 
</p>
<p>est le sujet d&#8217;un verbe de communication (#2), alors une relation unaire ORG avec l&#8217;attribut &#171; gsp &#187; est cr&#233;e pour ce nom 
(#1). </p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p> 
satisfaisants. On remarque que les scores pour les cat&#233;gories org et loc sont quelque peu inf&#233;rieurs aux 
r&#233;sultats &#171; standards&#187; dans ce genre de comp&#233;tition, ce qui montre l&#8217;impact (et la difficult&#233;) du traitement 
de la m&#233;tonymie, beaucoup d&#8217;erreurs venant de la confusion loc.admi et org.gsp. Un constat &#233;quivalent 
pour la cat&#233;gorie amount,  habituellement assez simple, peut &#234;tre fait, d&#251; aux ambigu&#239;t&#233;s entre dur&#233;es et 
&#226;ges d&#8217;un cot&#233; et expressions temporelles de l&#8217;autre. Une derni&#232;re remarque concerne les noms de 
productions humaines, dont le score est faible, en raison de leur faible repr&#233;sentation dans le corpus et de 
la diversit&#233; des &#233;l&#233;ments que cette cat&#233;gorie est cens&#233;e couvrir : des v&#233;hicules aux titre d&#8217;&#339;uvres d&#8217;art en 
passant par les documents l&#233;gaux.  
</p>
<p>Type Nb mots Prec. Recall F-Meas Ser 
</p>
<p>Pers 3110 97.76 95.57 0.97 3.63 
</p>
<p>Fonc 754 81.81 89.46 0.85 24.90 
</p>
<p>Org 2663 89.24 83.97 0.87 16.08 
</p>
<p>Loc 1875 89.01 88.73 0.89 7.09 
</p>
<p>Prod 191 100 42.11 0.59 46.03 
</p>
<p>Time 3235 95.63 95.69 0.96 5.85 
</p>
<p>Amount 939 93.76 86.57 0.90 15.27 
</p>
<p>TOUT 12767 93.61 91.50 0.93 9.80 
</p>
<p>Tableau 1: r&#233;sultats sur transcriptions de r&#233;f&#233;rence(NE-Ref) 
</p>
<p>4.3 Exp&#233;rience post-ESTER 2 
</p>
<p>Nous avons trouv&#233; int&#233;ressant de poursuivre de notre cot&#233; la campagne ESTER 2 en calculant les scores 
(initialement planifi&#233;s) pour l&#8217;ensemble des sous-cat&#233;gories.  
</p>
<p>Type Prec. Rec. F-meas Ser Type prec Rec. F-meas Ser 
</p>
<p>Pers.hum 97.8 95.6 0.97 3.63 Prod.art 100 8.6 0.16 78.3 
</p>
<p>Fonc.admi 51.2 71.8 0.60 48.7 Prod.award 100 65.5 0.79 28.3 
</p>
<p>Fonc.mil 0 0 0 200 Prod.doc 100 31.6 0.48 56.4 
</p>
<p>Fonc.pol 78.5 72.0 0.75 25.9 Prod.vehic 100 87.5 0.93 10 
</p>
<p>Fonc.reli 65.4 77.3 0.71 95.7 Time.date.abs 94.3 90.2 0.92 9.72 
</p>
<p>Org.com 81.4 57.4 0.67 15.2 Time.date.rel 94.2 87.9 0.91 9.15 
</p>
<p>Org.edu 100 32.7 0.49 25 Time.hour 86.6 95.5 0.91 4.12 
</p>
<p>Org.gsp 60 68.1 0.64 15.4 Amount.cur 96.9 92.5 0.95 14.5 
</p>
<p>Org.div  96.3 62.12 0.76 26.1 Amount.age 92 56.1 0.70 26.9 
</p>
<p>Org.non-profit 57.8 50.7 0.54 36.3 Amount.len 100 87.5 0.93 12.5 
</p>
<p>Loc.admi 83 86.4 0.85 6.3 Amount.area  100 95.2 0.98 7.7 
</p>
<p>Loc.fac 89.7 64.6 0.75 33.6 Amount.vol 100 100 1 0 
</p>
<p>Loc.geo 48.4 16.4 0.25 46.6 Amount.wei 100 91.1 0.95 9.1 
</p>
<p>Loc.line 82.5 60 0.69 27.2 Amount.temp 71.4 53.6 0.61 66.7 
</p>
<p>Loc.addr.elec 100 72.27 0.84 20 Amount.dur 83.3 81.8 0.83 18.9 
</p>
<p>Loc.addr.tel 100 100 1 0 TOUT 89.6 82.8 0.86 14.22 
</p>
<p>Tableau II: r&#233;sultats par cat&#233;gorie fine </p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>CAROLINE BRUN, MAUD EHRMANN 
Nous avons donc appliqu&#233; le script d&#8217;&#233;valuation sur les m&#234;mes corpus, l&#8217;&#233;valuation &#233;tant stricte car une 
erreur est compt&#233;e si les cat&#233;gories hypoth&#232;se et r&#233;f&#233;rence ne sont pas exactement les m&#234;mes, m&#234;me si la 
cat&#233;gorie g&#233;n&#233;rale est commune. Le tableau II montre que les r&#233;sultats restent tr&#232;s satisfaisants 
globalement, mais on constate cependant une chute importante du rappel. Cette chute est particuli&#232;rement 
marqu&#233;e pour les noms d&#8217;organisations, ce qui indique que leurs sous-types sont encore mal distingu&#233;s par 
notre syst&#232;me.  
</p>
<p>5 Bilan et conclusions 
Cet article d&#233;crit notre participation &#224; la t&#226;che de reconnaissance des entit&#233;s nomm&#233;es de la campagne 
d&#8217;&#233;valuation ESTER 2, qui s&#8217;est termin&#233;e en juin 2009. Nous avons adapt&#233; un syst&#232;me d&#8217;extraction 
d&#8217;entit&#233;s nomm&#233;es pr&#233;existant au sein d&#8217;un analyseur robuste, XIP. La finesse d&#8217;annotation requise lors de 
cette campagne nous a ainsi pouss&#233;es &#224; utiliser les r&#233;sultats de l&#8217;analyse syntaxique profonde, en 
particulier pour le traitement des probl&#232;mes d&#8217;ambig&#252;it&#233;s s&#233;mantiques et de m&#233;tonymies. Les r&#233;sultats 
obtenus sur transcriptions manuelles, pour l&#8217;annotation en cat&#233;gories g&#233;n&#233;rales,  &#233;taient tr&#232;s satisfaisants. 
L&#8217;exp&#233;rience que nous avons men&#233;e a posteriori sur l&#8217;annotation en cat&#233;gories fines a permis de mettre en 
&#233;vidence certains &#233;l&#233;ments &#224; am&#233;liorer dans notre syst&#232;me.  
</p>
<p>D&#8217;une fa&#231;on g&#233;n&#233;rale, la participation &#224; cette campagne s&#8217;est av&#233;r&#233;e extr&#234;mement b&#233;n&#233;fique pour notre 
syst&#232;me de rep&#233;rage des entit&#233;s nomm&#233;es. Mais peut-&#234;tre encore plus crucialement, cette &#233;valuation a 
permis aux participants de mener une r&#233;flexion approfondie sur les probl&#232;mes d&#8217;annotations des entit&#233;s 
nomm&#233;es : quels sont les crit&#232;res pour d&#233;cider qu&#8217;une unit&#233; linguistique est une entit&#233; nomm&#233;e, quel est 
l&#8217;&#233;tiquette &#224; donner dans un contexte donn&#233;, quels sont leurs fronti&#232;res, etc. Cette r&#233;flexion a permis 
d&#8217;aboutir &#224; une premi&#232;re version d&#8217;un guide d&#8217;annotation qui vise &#224; devenir un standard pour le fran&#231;ais.  
</p>
<p>R&#233;f&#233;rences 
AIT-MOKTHAR, S. CHANOD J.P, ROUX C. (2002). Robustness beyond Shallowness: Incremental 
Dependency Parsing. Special Issue of NLE Journal. 
</p>
<p>BRUN C., EHRMANN. (2009). Adaptation of a Named Entity Recognition System for the ESTER 2 
Evaluation Campaign. IEEE NLP-KE 2009 (IEEE International Conference on Natural Language 
Processing and Knowledge Engineering), Dalian, China, Sep 24-27. 
</p>
<p>BRUN C., EHRMANN M. , JACQUET G. (2007)  , XRCE-M : A hybrid system for named entity metonymy 
resolution, Actes de  4th International Workshop on Semantic Evaluations, ACL-SemEval 200, Prague.  
 
GALLIANO S.,  GRAVIER G.  AND  CHAUBARD  L. (2009). The ESTER 2 Evaluation Campaign for the Rich 
Transcription of French Radio Broadcasts&#8221;, 10th Annual Conference of the International Speech 
Communication Association , InterSpeech 2009, Brighton UK.  
 
 MAKHOUL J., KUBALA F., SCHWARTZ R., WEISCHEDEL R.  (1999). Performance Measures For Information 
Extraction, dans les actes du DARPA Broadcast News Workshop, 249&#8212;252.   
 
REBOTIER A.  (2006). D&#233;veloppement d&#8217;un module d&#8217;extraction d&#8217;Entit&#233;s Nomm&#233;es pour le fran&#231;ais, 
M&#233;moire de DEA, Universit&#233; Stendhal Grenoble III. 
 
 
 </p>

</div></div>
</body></html>