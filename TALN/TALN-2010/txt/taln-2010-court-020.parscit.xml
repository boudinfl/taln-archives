<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S CHANOD J P AIT-MOKTHAR</author>
<author>C ROUX</author>
</authors>
<title>Robustness beyond Shallowness: Incremental Dependency Parsing.</title>
<date>2002</date>
<journal>Special Issue of NLE Journal.</journal>
<marker>AIT-MOKTHAR, ROUX, 2002</marker>
<rawString>AIT-MOKTHAR, S. CHANOD J.P, ROUX C. (2002). Robustness beyond Shallowness: Incremental Dependency Parsing. Special Issue of NLE Journal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C BRUN</author>
<author>EHRMANN</author>
</authors>
<title>Adaptation of a Named Entity Recognition System for the ESTER 2 Evaluation Campaign.</title>
<date>2009</date>
<booktitle>IEEE NLP-KE 2009 (IEEE International Conference on Natural Language Processing and Knowledge Engineering),</booktitle>
<pages>24--27</pages>
<location>Dalian, China,</location>
<marker>BRUN, EHRMANN, 2009</marker>
<rawString>BRUN C., EHRMANN. (2009). Adaptation of a Named Entity Recognition System for the ESTER 2 Evaluation Campaign. IEEE NLP-KE 2009 (IEEE International Conference on Natural Language Processing and Knowledge Engineering), Dalian, China, Sep 24-27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C BRUN</author>
<author>M EHRMANN</author>
</authors>
<title>XRCE-M : A hybrid system for named entity metonymy resolution,</title>
<date>2007</date>
<booktitle>Actes de 4th International Workshop on Semantic Evaluations, ACL-SemEval 200,</booktitle>
<location>Prague.</location>
<marker>BRUN, EHRMANN, 2007</marker>
<rawString>BRUN C., EHRMANN M. , JACQUET G. (2007) , XRCE-M : A hybrid system for named entity metonymy resolution, Actes de 4th International Workshop on Semantic Evaluations, ACL-SemEval 200, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S GALLIANO</author>
<author>G GRAVIER</author>
<author>L CHAUBARD</author>
</authors>
<date>2009</date>
<booktitle>The ESTER 2 Evaluation Campaign for the Rich Transcription of French Radio Broadcasts”, 10th Annual Conference of the International Speech Communication Association , InterSpeech</booktitle>
<publisher>UK.</publisher>
<location>Brighton</location>
<marker>GALLIANO, GRAVIER, CHAUBARD, 2009</marker>
<rawString>GALLIANO S., GRAVIER G. AND CHAUBARD L. (2009). The ESTER 2 Evaluation Campaign for the Rich Transcription of French Radio Broadcasts”, 10th Annual Conference of the International Speech Communication Association , InterSpeech 2009, Brighton UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J MAKHOUL</author>
<author>F KUBALA</author>
<author>R SCHWARTZ</author>
<author>R WEISCHEDEL</author>
</authors>
<title>Performance Measures For Information Extraction, dans les actes du DARPA Broadcast News Workshop,</title>
<date>1999</date>
<pages>249--252</pages>
<contexts>
<context position="12706" citStr="MAKHOUL et al. 1999" startWordPosition="1838" endWordPosition="1841">manuellement) et de développement (6 heures de journaux radiophoniques transcrits et annotés manuellement) pour la mise au point du système. Le corpus de test était constitué de 7 heures de journaux radiophoniques datant de 2008. L’ensemble des corpus provenait de différentes sources : France Culture, France Inter, Radio France International, Radio Classique, Africa 1, Radio Congo et Radio Télévision Marocaine. Du point de vue de l’évaluation quantitative, même si les mesures classiques de précision et rappel étaient calculées, la mesure « officielle » était le « Slot Error Rate » (SER, voir (MAKHOUL et al. 1999)), qui combine et pondère les différents type d’erreurs (insertion, effacement, erreur de type) : SER = (Insertions+Effacements+Substitutions) / nb entités ref. C’est une mesure analogue au « Word Error rate » (WER) utilisé pour mesurer les performances des systèmes de transcriptions de la parole. D’autre part, si au début de la campagne il était prévu d’évaluer sur l’ensemble des sous-types, c’est seulement sur les 7 catégories principales que les résultats ont été calculés. Enfin, les résultats définitifs ont été obtenus après une phase d’adjudication qui permettait aux participants de conte</context>
</contexts>
<marker>MAKHOUL, KUBALA, SCHWARTZ, WEISCHEDEL, 1999</marker>
<rawString>MAKHOUL J., KUBALA F., SCHWARTZ R., WEISCHEDEL R. (1999). Performance Measures For Information Extraction, dans les actes du DARPA Broadcast News Workshop, 249—252.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A REBOTIER</author>
</authors>
<title>Développement d’un module d’extraction d’Entités Nommées pour le français, Mémoire de DEA, Université Stendhal Grenoble III.</title>
<date>2006</date>
<marker>REBOTIER, 2006</marker>
<rawString>REBOTIER A. (2006). Développement d’un module d’extraction d’Entités Nommées pour le français, Mémoire de DEA, Université Stendhal Grenoble III.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>