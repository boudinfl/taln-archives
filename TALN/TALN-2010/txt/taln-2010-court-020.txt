Un systéme de détection d’entités nommées adapté pour la campagne
d’évaluation ESTER 2

Caroline Brunl Maud Ehrmannz

(1) XRCE, 6, Chemins de Maupertuis, Meylan, France
(2) JRC — European Commission, Ispra, Italie
Caroline.Brun@xrce.xerox.com, maud.ehrmann@jrc.ec.europa.eu

Résumé Dans cet article nous relatons notre participation a la campagne d’évaluation ESTER 2
(Evaluation des Systemes de Transcription Enrichie d’Emissions Radiophoniques). Apres avoir décrit les
objectifs de cette campagne ainsi que ses spécificités et difficultés, nous présentons notre systeme
d’extraction d’entités nommées en nous focalisant sur les adaptations réalisées dans le cadre de cette
campagne. Nous décrivons ensuite les résultats obtenus lors de la competition, ainsi que des résultats
originaux obtenus par la suite. Nous concluons sur les lecons tirées de cette expérience.

Abstract In this paper, we report our participation to the ESTER 2 (Evaluation des Systemes de
Transcription Enrichie d’Emissions Radiophoniques) evaluation campaign. After describing the goals,
specificities and challenges of the campaign, we present our named entity detection system and focus on
the adaptations made in the framework of the campaign. We present the results obtained during the
competition and then new results obtained afterward. We then conclude by the lessons we learned from
this experiment.

Mots-clés I entités nommées, évaluation, extraction d’information.
Keywords: named entities, evaluation, information extraction.

1 Introduction

La campagne d’évaluation ESTER 2 s’est déroulée de janvier 2008 a avril 2009, dans la continuité de la
premiere campagne ESTER1. L’objectif principal était «depro111ouVo1'r une dy11a1111'que de l’éValuat1'o11 en
France, autour du lra1'te111e11t de la parole de langue 1?a11;:a1'se, de mettre e11 place une structure pe're1111e
d’e'Valuat1'o11 et de dzﬂizser le plus largementpossible les 1'11for121at1'o11s et les ressources co11cer11e'es par ces
éValuat1'o11s»2. Ces campagnes visaient a évaluer les performances des systemes de transcription de la
parole, les performances des systemes de segmentation en tours de paroles, et la capacité a extraire
automatiquement des inforrnations, en particulier les entités nommées (EN). Cette troisieme tache, a
laquelle se sont attelés 7 participants dans le cadre d’ESTER 2, est l’objet de cet article. Elle était divisée

Ces campagnes furent organisées par la DGA et l’AFCP. Site intemet : http://www.afcp-parole.org/ester/index.html

2

http://www.afcp-parole.org/ester/presenthtml

CAROLINE BRUN, MAUD EHRMANN
en deux sous-taches: la détection d’entités nommées sur transcriptions de référence (NE-ref) et sur
transcriptions automatiques (NE-asr).

2 ESTER 2 en détails
2.1 Spéciﬁcités et difﬁcultés de la téiche de détection d’EN

Dans le cadre d’ESTER2, il s’agissait d’extraire et de catégoriser des mentions directes d’EN, selon un
guide d’annotation comprenant 7 categories principales et 38 sous-catégories :

Personnes : pers.hum (re'elles ou ﬁctives), pers.anim (anirrraux re'els ou ﬁctifs)

Fonctions : fonc.pol (politique), fonc.rr1il (rnilitaire), fonc.adrr1i (administrative), fonc.rel (religieuse), fonc.ar1' (aristocratique)

Lieux : loc.geo (lieu geographique), loc.adrr1i (adrninistratif), loc.line (voies de circulation), loc.adr (adresses), loc.adr.post (adresses postales),
loc.adr.tel (fax et telephones), loc.adr.elec (adresse e'lectroniques), loc.fac (batirnents)

Organisations : org.pol (politique), org.rr1il (rnilitaire), org.edu (education), org.com (cornrnerciale), org.non-proﬁt (sans but lucratif), org.div
(divertissement), org.gsp (ge'opolitique)

Produits: prod.vehic (vehicules), prod.award (re'compenses), prod.aIt (produits artistiques), prod.doc (documents)

Temps : time.date (date), time.date.abs (date absolue), time.date.rel (date relative), hour (heures)

Quantités : amount.age (age), arnount.dur (dure'e), amount.temp (ternpe'rature), arnount.len (longueur), arnount.area (surface), amount.phys.vol
(volume), amount.weight (poids), amount.spd (speed), amount.phys.cur(mom1aies), arnount.phys.other (autres)

La principale instruction d’annotation est de considérer les entités e11 contexte, avec la prise en compte des
phénomenes d’ambigu'1'tés et de métonymie : par exemple, selon les contextes, << Charles de Gaulle >> doit
étre annoté en tant que personne (le président), véhicule (le porte-avion) ou encore lieu (l’aéroport).
L’annotation des noms de personnes inclut celle des fonctions et l’annotation des expressions temporelles
couvre pour sa part un large éventail de possibilités, des classiques << Lu11d1'111at1'z1>> aux plus complexes
<< [1 ya L111 peu 12101115 de lroisjours e11V1iro11». Par ailleurs, dans la mesure ou l’extraction d’entités est
réalisée sur des transcriptions de la parole, certains phénomenes propres a l’oral (hésitations ou répétitions)
doivent étre inclus dans les annotations (<pers.hum> Jacques bell CI11irac</pers.hum>). Ces directives
d’annotation spéciﬁques, combinées au nombre important de categories a prendre en compte,
complexiﬁent la tache d’annotation. En effet, les quantités de type age et durée sont particulierement
difficiles a distinguer des expressions temporelles, tout comme les lieux administratifs des entités
géopolitiques, puisqu’il s’agit de noms de villes ou de pays fréquemment employés en tant que l’un ou
l’autre. On peut donc constater que cette tache est plus ambitieuse que l’extraction d’EN << classique >> (i.e.
a la MUC).

2.2 Questions ouvertes concernant l’annotation

Se mettre d’accord sur la maniere d’annoter des EN n’est pas chose facile. Ce probleme bien connu n’a
pas manqué d’apparaitre durant ESTER2, avec de nombreuses discussions et remises en cause du guide
d’annotation, modiﬁé au fur et a mesure de la campagne jusqu’a une version deﬁnitive en janvier 2009.
Les points délicats ont concemé, parmi d’autres, les expressions temporelles et les fonctions. Pour ce qui
est des premieres, il fut principalement question de l’extension des expressions temporelles (inclusion ou
non des prépositions, déterminants et relatives), des difficiles distinctions entre dates et durées, et entre une
expression temporelle et une autre qui ne l’est pas. Concemant les fonctions, deux points furent soulevés:
le manque de criteres pour définir la portée de cette catégorie d’une part (il est facile de lister des fonctions
<< standards >> mais bien d’autres posent problemes) et, d’autre part, la pertinence d’annoter conjointement,
comme il était demandé dans certains cas, personnes et fonctions (n’est-t-il pas préférable, d’un point de
vue sémantique, d’annoter des relations entre noms de personnes et noms de fonction ?).

3 XIP a ESTER 2

Nous avons participé a la campagne d’évaluation ESTER2 en adaptant l’analyseur syntaxique robuste
<< Xerox Incremental Parser >> (XIP, (Ait-Mokthar et al., 2002)). XIP prend en entrée du texte tout venant,
sous format texte ou Xl\/IL, et produit en sortie de facon robuste une analyse syntaxique profonde. A partir
d'un ensemble de regles, l'analyseur désambiguise les catégories, construit les syntagmes noyaux et extrait
des relations de dépendances syntaxiques. En plus de l'analyse des relations syntaxiques de surface, XIP
effectue également une analyse syntaxique dite << profonde >> ou << normalisée >> (prise en compte des
sujets et obj ets de verbes non finis, normalisation de la forme passive en forme active, etc.). Cet analyseur
integre également un module de reconnaissance des entités nommées (Rebotier 2006), prenant en compte
les types classiques d’entités nommées, a savoir les expressions numériques, les monnaies, les dates, ainsi
que les noms de lieux, de personnes et d'organisations. Il s'agit d'un module a base de regles, consistant en
un ensemble de regles locales ordonnées utilisant des informations lexicales et des inforrnations
contextuelles concemant les parties du discours, les formes lemmatisées et un ensemble de traits lexico-
sémantiques.

Nous avons dﬁ adapter le systeme développé pour le francais aux consignes d’annotation ESTER 2, selon
les axes suivants :

0 Adaptation aux spéciifcités de la transcription: Les transcriptions de la parole, manuelles ou
automatiques, ont des particularités que l’on ne retrouve pas dans les textes << standards >> ; il s’agit de
disﬂuences, de répétitions, ou encore de bruits :

<< [1 ya encore eub quelques Inois. .. >>, << Une forme de dejournaliszne  >>, « [tires-en-fond-] Voda ./
[-rire-en-fond] »

Afin d’ignorer les disﬂuences et les bruits, nous avons converti les fichiers d’entrée originaux sous
format XML, en marquant ces éléments comme des balises ouvrantes/fermantes (</heu>, </[rires]>)
totalement transparentes pour les traitements linguistiques. Dans le cas des répétitions, nous avons
développé des regles qui groupent ces éléments sous un noeud de méme catégorie qui hérite des traits
du premier élément.

0 Adaptation pour les categories «standards»: Nous avons tout d’abord utilisé les corpus
d’entrainement et de développement pour collecter semi-automatiquement le vocabulaire inconnu
(noms de lieux, d’organisations, etc.) et l’intégrer a nos lexiques. Nous avons ensuite adapté le systeme
pour prendre en compte de nouvelles catégories, telles que les fonctions, les ages, les productions
humaines, et la plupart des quantités, qui n’étaient pas préalablement couvertes par notre systeme.
Nous avons également adapté les regles existantes selon le guide d’annotation, en particulier pour
couvrir la portée des entités; par exemple, les déterminants et prépositions sont inclus dans les
quantités et les noms de fonction en apposition d’un nom de personne sont inclus dans ce dernier :

<< I] est age’ <amount.age> de 18 ans </amount.age>

<< <pers.hum> Nicolas Sarkoq/, president de la republique </pers.hum> ... >>

11 s’est principalement agit ici de développer et de modifier des regles locales de regroupement des
noms propres, en amont de l’analyse en syntagmes noyaux (chunks).

0 7Iraite1nent des expressions temporelles: Le vocabulaire relatif aux expressions temporelles étant
une liste fermée, le coeur du travail fut l’écriture de regles locales et de chunking. L’attention fut portée
sur les prépositions et adverbes principalement, ces derniers affectant radicalement le sens de telle ou
telle expression ([1 est parti <amount.phys.dur> pendant I01nois </amount.phys.dur> vs. I] est parti

CAROLINE BRUN, MAUD EI-IRMANN
<time.date.rel> 10111015 aprés </time.date.rel>). Nous avons également dﬁ prendre en compte certaines
incidences de la transcription de parole, comme par exemple avec l’expression << 19 cent 97».

0 A111b1gu1"tés et111éto11y1111'es .' Une des spécificités les plus intéressantes d’ESTER 2 est la prise en
compte des ambiguités et des phénomenes de métonymies. Afin d’étre a meme de traiter ces cas, nous
avons utilisé les résultats de l’analyse syntaxique profonde fournis par XIP. En nous référant au guide
d’annotation, nous avons réalisé une étude de corpus pour détecter les régularités syntaxiques et
lexicales déclenchant un glissement métonymique ou perrnettant de résoudre une ambigu'1'té, selon la
méthodologie appliquée dans (Brun et al 2007). Cette étude a conduit a des hypotheses telles que << Si
un nom de lieu de type administratif est sujet d’un Verbe de communication, il est employé comme
nom d’organisation géopolitique>>. L’analyseur fut alors enrichi par des lexiques sémantiques dédiés et
par des regles de dépendance modiﬁant l’interprétation des entités, appliquées en aval de l’analyse
syntaxique, par exemple :

If ("LIEU[ADMI](#1) & SUBJ(#2[v_communication],#1)) 9 ORG[GSP=+](#1)3

Cette regle peut s’appliquer sur une phrase comme << Dakar parle de 28 11117110115 d’euros», alors
annotée << <org.gsp> Dakar </org.gsp> parle de 28 11117110115 d’euros». Notre étude s’est concentrée
sur les relations de type sujet, objet, modiﬁeur (nominal et propositionnel) et attribut, et nous a
conduites a développer environ 150 regles de dépendances supplémentaires.

4 Eva1uation(s)

4.1 Corpus et calcul des scores

Comme dit précédemment, nous avons utilisé les corpus d’entrainement (100 heures d’émissions de
radio transcrites et annotées manuellement) et de développement (6 heures de journaux radiophoniques
transcrits et annotés manuellement) pour la mise au point du systeme. Le corpus detest était constitué de
7 heures de joumaux radiophoniques datant de 2008. L’ensemble des corpus provenait de différentes
sources: France Culture, France Inter, Radio France International, Radio Classique, Africa 1, Radio
Congo et Radio Television Marocaine. Du point de vue de l’évaluation quantitative, méme si les mesures
classiques de précision et rappel étaient calculées, la mesure << ofﬁcielle >> était le << Slot Error Rate >> (SER,
voir (MAKHOUL et al. 1999)), qui combine et pondere les différents type d’erreurs (insertion, effacement,
erreur de type) : SER = (Insertions+Effacements+Substitutions) / nb entités ref. C’est une mesure analogue
au << Word Error rate >> (VVER) utilisé pour mesurer les performances des systemes de transcriptions de la
parole. D’autre part, si au début de la campagne il était prévu d’éValuer sur l’ensemble des sous-types,
c’est seulement sur les 7 categories principales que les résultats ont été calculés. Enfin, les résultats
déﬁnitifs ont été obtenus apres une phase d’adjudication qui perrnettait aux participants de contester les
annotations du corpus detest (sans bien évidemment changer les résultats de leur systeme).

4.2 Résultats obtenus dans le cadre d’ESTER 2

Le tableau I présente les résultats obtenus par notre systeme sur transcriptions de référence (NE-Ref) en
termes de précision, rappel, f-mesure et slot error rate. Les résultats de notre systeme sur les transcriptions
automatiques (NE-Asr) sont publiés dans (Brun et Ehrrnann, 2009), les résultats complets de la campagne
dans (Galliano et al, 2009). Avec un SER de 9.80 (et une f-mesure de 0,93), ces résultats s’aVerent tres

3 Cette regle se lit de la maniere suivante : si le parseur a détecté un nom de LIEU avec l’attribut << admi » (#1), et que ce nom
est le sujet d’un Verbe de communication (#2), alors une relation unaire ORG avec l’attribut << gsp » est crée pour ce nom
(#1).

satisfaisants. On remarque que les scores pour les catégories org et Ioc sont quelque peu inférieurs aux
résultats << standards» dans ce genre de competition, ce qui montre 1’impact (et la difficulté) du traitement
de la métonymie, beaucoup d’erreurs Venant de la confusion 1oc.adm1' et orggsp. Un constat équivalent
pour la catégorie amount; habituellement assez simple, peut étre fait, dﬁ aux ambigu'1'tés entre durées et
ages d’un coté et expressions temporelles de 1’autre. Une derniere remarque concerne les noms de
productions humaines, dont 1e score est faible, en raison de leur faible représentation dans le corpus et de
la diversité des éléments que cette catégorie est censée couvrir : des Véhicules aux titre d’oeuVres d’art en
passant par les documents légaux.

Pers 3110 97.76 95.57 0.97 3.63
Fonc 754 81.81 89.46 0.85 24.90
Org 2663 89.24 83.97 0.87 16.08
Loc 1875 89.01 88.73 0.89 7.09
Prod 191 100 42.11 0.59 46.03
Time 3235 95.63 95.69 0.96 5.85
Amount 939 93.76 86.57 0.90 15.27
TOUT 12767 93.61 91.50 0.93 9.80

Tableau 1.’ résultats sur tra11scr1'plio11s dc re'fé're11ceﬂ\ﬂE -R619

4.3 Expérience post-ESTER 2

Nous avons trouvé intéressant de poursuivre de notre coté la campagne ESTER 2 en calculant les scores
(initialement planiﬁés) pour 1’ensemb1e des sous-catégories.

Pers.hum 97.8 95.6 0.97 3.63 Prod.art 100 8.6 0.16 78.3
Fonc.admi 51.2 71.8 0.60 48.7 Prod.award 100 65.5 0.79 28.3
Fonc.mil 0 0 0 200 Prod.doc 100 31.6 0.48 56.4
Fonc.pol 78.5 72.0 0.75 25.9 Prod.vehic 100 87.5 0.93 10
Fonc.reli 65.4 77.3 0.71 95.7 Time.date.abs 94.3 90.2 0.92 9.72
Org.com 81.4 57.4 0.67 15.2 Time.date.rel 94.2 87.9 0.91 9.15
Org.edu 100 32.7 0.49 25 Time.hour 86.6 95.5 0.91 4.12
Org.gsp 60 68.1 0.64 15.4 Amount.cur 96.9 92.5 0.95 14.5
Org.div 96.3 62.12 0.76 26.1 Amount.age 92 56.1 0.70 26.9
Org.non-profit 57.8 50.7 0.54 36.3 Amount.|en 100 87.5 0.93 12.5
Loc.admi 83 86.4 0.85 6.3 Amount.area 100 95.2 0.98 7.7
Loc.fac 89.7 64.6 0.75 33.6 Amount.vo| 100 100 1 0
Loc.geo 48.4 16.4 0.25 46.6 Amount.wei 100 91.1 0.95 9.1
Loc.line 82.5 60 0.69 27.2 Amount.temp 71.4 53.6 0.61 66.7
Loc.addr.eIec 100 72.27 0.84 20 Amount.dur 83.3 81.8 0.83 18.9
Loc.addr.tel 100 100 1 0 TOUT 89.6 82.8 0.86 14.22

Tableau II.’ résultats par catégorie ﬁne

CAROLINE BRUN, MAUD EHRMANN

Nous avons donc applique le script d’evaluation sur les memes corpus, l’evaluation etant stricte car une
erreur est comptee si les categories hypothese et reference ne sont pas exactement les memes, meme si la
categorie generale est commune. Le tableau II montre que les resultats restent tres satisfaisants
globalement, mais on constate cependant une chute importante du rappel. Cette chute est particulierement
marquee pour les noms d’organisations, ce qui indique que leurs sous-types sont encore mal distingues par
notre systeme.

5 Bilan ct conclusions

Cet article decrit notre participation a la tache de reconnaissance des entites nommees de la campagne
d’evaluation ESTER 2, qui s’est terminee en juin 2009. Nous avons adapte un systeme d’extraction
d’entites nommees preexistant au sein d’un analyseur robuste, XIP. La ﬁnesse d’annotation requise lors de
cette campagne nous a ainsi poussees a utiliser les resultats de l’analyse syntaxique profonde, en
particulier pour le traitement des problemes d’ambig1"1ites semantiques et de metonymies. Les resultats
obtenus sur transcriptions manuelles, pour l’annotation en categories generales, etaient tres satisfaisants.
L’experience que nous avons menee a posteriori sur l’annotation en categories ﬁnes a permis de mettre en
evidence certains elements a ameliorer dans notre systeme.

D’une facon generale, la participation a cette campagne s’est averee extrémement beneﬁque pour notre
systeme de reperage des entites nommees. Mais peut-étre encore plus crucialement, cette evaluation a
permis aux participants de mener une reﬂexion approfondie sur les problemes d’annotations des entites
nommees : quels sont les criteres pour decider qu’une unite linguistique est une entite nommee, quel est
l’etiquette a donner dans un contexte donne, quels sont leurs frontieres, etc. Cette reﬂexion a permis
d’aboutir a une premiere version d’un guide d’annotation qui vise a devenir un standard pour le francais.

References

Arr-MOKTHAR, S. CHANOD J.P, ROUX C. (2002). Robustness beyond Shallowness: Incremental
Dependency Parsing. Special Issue ofl\/IE Journal.

BRUN C., EHRMANN. (2009). Adaptation of a Named Entity Recognition System for the ESTER 2
Evaluation Campaign. IEEE NLP-KE 2009 (IEEE International Conference on Natural Language
Processing and Knowledge Engineering), Dalian, China, Sep 24-27.

BRUN C., EHRMANN M. , JACQUET G. (2007) , XRCE-M : A hybrid system for named entity metonymy
resolution, Actes de 4th International I/VbI'l£§l10p on Semantic Evaluations, ACL-SemEVal 200, Prague.

GALLLANO S., GRAVIER G. AND CHAUBARD L. (2009). The ESTER 2 Evaluation Campaign for the Rich
Transcription of French Radio Broadcasts”, l0tl1 Annual Conference of tlie International Speecli
Communication Association , lnterSpeecl1 2002 Brigliton UK.

MAKHOUL J., KUBALA F., SCHWARTZ R., WEISCHEDEL R. (1999). Performance Measures For Information
Extraction, dans les actes du DARPA Broadcast News I/VOI'l(§l10p, 249—252.

REBOTIER A. (2006). Developpement d’un module d’extraction d’Entites Nommees pour le francais,
Memoire de DEA, Universite Stendhal Grenoble III.

