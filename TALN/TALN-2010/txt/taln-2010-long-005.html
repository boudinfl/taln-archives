<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
"http://www.w3.org/TR/html4/loose.dtd">
<html><head><title>Au-del&#224; de la paire de mots : extraction de cooccurrences syntaxiques multilex&#233;miques</title>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
</head>
<body>
<div style="page-break-before:always; page-break-after:always"><div><p>TALN 2010, Montr&#233;al, 19-23 juillet 2010 
 
</p>
<p>Au-del&#224; de la paire de mots : 
extraction de cooccurrences syntaxiques multilex&#233;miques 
</p>
<p>Simon CHAREST, &#201;ric BRUNELLE, Jean FONTAINE 
</p>
<p>Druide informatique inc. 
1435, rue Saint-Alexandre, bureau 1040 
Montr&#233;al (Qu&#233;bec) H3A 2G4, Canada 
developpement@druide.com 
</p>
<p> 
</p>
<p>R&#233;sum&#233; 
Cet article d&#233;crit l&#8217;&#233;laboration de la deuxi&#232;me &#233;dition du dictionnaire de cooccurrences du logiciel d&#8217;aide &#224; 
la r&#233;daction Antidote. Cette nouvelle mouture est le r&#233;sultat d&#8217;une refonte compl&#232;te du processus 
d&#8217;extraction, ayant principalement pour but l&#8217;extraction de cooccurrences de plus de deux unit&#233;s lexicales. 
La principale contribution de cet article est la description d&#8217;une technique originale pour l&#8217;extraction de 
cooccurrences de plus de deux mots conservant une structure syntaxique compl&#232;te. 
</p>
<p>Abstract 
This article describes the elaboration of the second edition of the co-occurrence dictionary included in 
Antidote HD, a commercial software tool for writing in French. This second edition is the result of a 
complete overhaul of the extraction process, with the objective of extracting co-occurrences of more than 
two lexical units. The main contribution of this article is the description of an original method for 
extracting co-occurrences of more than two words retaining their full syntactic structure. 
</p>
<p>Mots-cl&#233;s :   Antidote, cooccurrences, collocations, expressions multimots. 
Keywords:   Antidote, co-occurrences, collocations, multi-word expressions (MWE). 
 </p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Simon CHAREST, &#201;ric BRUNELLE, Jean FONTAINE 
 
</p>
<p>1 Introduction 
En 2006 paraissait avec Antidote RX1 le premier dictionnaire &#233;lectronique grand public de cooccurrences2 
du fran&#231;ais (Charest et coll., 2007). Ce dictionnaire a &#233;t&#233; &#233;labor&#233; en utilisant l&#8217;analyseur syntaxique 
d&#8217;Antidote pour extraire, d&#8217;un corpus de 500 millions de mots, 17 millions de paires de mots diff&#233;rentes 
li&#233;es par diverses relations syntaxiques. Un score correspondant au rapport de vraisemblance (log-
likelihood ratio) avait &#233;t&#233; calcul&#233; sur chacune de ces cooccurrences candidates afin d&#8217;en d&#233;gager les plus 
significatives. Au final, apr&#232;s validation par une &#233;quipe de linguistes, 800 000 cooccurrences avaient &#233;t&#233; 
retenues, r&#233;parties en 36 000 entr&#233;es, class&#233;es par sens et relations syntaxiques et illustr&#233;es par 880 000 
exemples tir&#233;s du corpus. 
</p>
<p>Parmi les limitations de ce dictionnaire, nous avions not&#233; la pr&#233;sence de cooccurrences incompl&#232;tes : 
l&#8217;extracteur ne consid&#233;rant que les associations de deux mots, il lui arrivait de g&#233;n&#233;rer des combinaisons 
incompl&#232;tes, auxquelles il manque un &#233;l&#233;ment essentiel, comme dans prendre le taureau ___, nager en 
___ d&#233;lire ou ___ le feu aux poudres. Ces cooccurrences avaient &#233;t&#233; soit rejet&#233;es par le linguiste, soit 
augment&#233;es manuellement par l&#8217;ajout du mot manquant. 
</p>
<p>Pour la deuxi&#232;me &#233;dition de notre dictionnaire de cooccurrences, parue en 2009 avec Antidote HD, nous 
avons revu notre processus d&#8217;extraction afin d&#8217;extraire automatiquement des cooccurrences de plus de 
deux mots. Le pr&#233;sent article d&#233;crit la d&#233;marche entreprise pour arriver &#224; ce r&#233;sultat. 
</p>
<p>2 Travaux ant&#233;rieurs 
La plupart des travaux sur les collocations (en anglais multi-word expression [MWE]) ont port&#233; sur les 
combinaisons de deux mots. Tutin (2008) analyse les collocations de plus de deux mots et conclut qu&#8217;elles 
peuvent le plus souvent &#234;tre consid&#233;r&#233;es comme des combinaisons binaires respectant une structure 
pr&#233;dicat-argument. Selon elle, la majorit&#233; des collocations n-aires peuvent &#234;tre analys&#233;es comme des 
superpositions de collocations ou comme des collocations r&#233;cursives, tandis que quelques cas sont de 
vraies collocations n-aires, o&#249; la s&#233;quence ne peut pas &#234;tre d&#233;compos&#233;e. 
</p>
<p>Dans la t&#226;che d&#8217;extraction de collocations &#224; partir de corpus, les mesures statistiques d&#8217;association 
employ&#233;es pour quantifier le degr&#233; de coh&#233;sion &#8211; et donc de pertinence &#8211; des combinaisons candidates 
sont pour la plupart formul&#233;es pour &#233;valuer les associations de deux &#233;l&#233;ments (ou bigrammes). De 
nombreuses &#233;tudes ont compar&#233; les performances de diverses mesures d&#8217;association. Dans l&#8217;une des plus 
ambitieuses, Pecina et Schlesinger (2006) ont &#233;valu&#233; 82 mesures dans une t&#226;che d&#8217;extraction de 
collocations en tch&#232;que, et ont combin&#233; les plus performantes &#224; l&#8217;aide de techniques d&#8217;apprentissage. 
</p>
<p>Les choses se compliquent lorsque vient le temps de mesurer la force d&#8217;association de combinaisons de 
plus de deux mots. Deux grandes approches sont possibles : 1) employer des mesures statistiques plus 
                                                
1  Logiciel d&#8217;aide &#224; la r&#233;daction grand pulic (www.antidote.info), sixi&#232;me &#233;dition. 
2  Par cooccurrence, nous entendons la pr&#233;sence simultan&#233;e et statistiquement significative, dans un corpus, de deux unit&#233;s 
</p>
<p>linguistiques ou plus en relation syntaxique. Notre concept de cooccurrence englobe des combinaisons lexicales dont le 
degr&#233; de figement est variable : nous y incluons &#224; la fois des combinaisons libres (entendre un cri), des combinaisons semi-
fig&#233;es ou collocations au sens strict (pousser un cri) et des locutions fig&#233;es courantes (cri du coeur) ou terminologiques (cri 
primal). </p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>AU-DELA DE LA PAIRE DE MOTS : EXTRACTION DE COOCCURRENCES SYNTAXIQUES MULTILEXEMIQUES 
 
</p>
<p>complexes formul&#233;es pour &#233;valuer les associations d&#8217;un nombre arbitraire d&#8217;&#233;l&#233;ments; ou 2) employer des 
mesures d&#8217;association de bigrammes sur des groupes de mots plut&#244;t que sur des mots individuels. 
</p>
<p>La premi&#232;re approche a &#233;t&#233; suivie par Dias et coll. (2000), qui ont propos&#233; une normalisation de quatre 
mesures d&#8217;association courantes (le coefficient de Dice, l&#8217;information mutuelle sp&#233;cifique, &#934;2 et le rapport 
de vraisemblance) afin de pouvoir calculer la coh&#233;sion de n-grammes pour n &gt;= 2. La normalisation est 
effectu&#233;e en consid&#233;rant toutes les fa&#231;ons possibles de diviser un n-gramme en 2 sous-groupes (cr&#233;ant 
ainsi des &#171; pseudobigrammes &#187;), et en prenant la moyenne arithm&#233;tique des fr&#233;quences de toutes les 
combinaisons possibles. Ils ont en outre introduit une nouvelle mesure, l&#8217;expectative mutuelle, calcul&#233;e 
selon le m&#234;me principe et donnant, selon leurs tests, de meilleurs r&#233;sultats que les mesures classiques. 
</p>
<p>Dans la m&#234;me veine, Blaheta et Johnson (2001) ont employ&#233; un mod&#232;le log-lin&#233;aire pour l&#8217;extraction de 
locutions verbales en anglais. Les mod&#232;les log-lin&#233;aires permettent de mettre en &#233;vidence des interactions 
complexes entre plusieurs variables. Villada Moiron (2005) a compar&#233; des mod&#232;les bigrammes et 
trigrammes pour l&#8217;extraction de locutions verbales et pr&#233;positionnelles en n&#233;erlandais. Parmi les mod&#232;les 
trigrammes, elle a test&#233; deux mesures (l&#8217;information mutuelle sp&#233;cifique et le test du Chi-carr&#233; de Pearson) 
qu&#8217;elle a &#233;tendues aux trigrammes, de m&#234;me que le mod&#232;le log-lin&#233;aire de Blaheta et Johnson (2001). 
D&#8217;apr&#232;s ses tests, le mod&#232;le bigramme utilisant le rapport de vraisemblance a donn&#233; les meilleurs r&#233;sultats. 
</p>
<p>&#192; propos du rapport de vraisemblance, il est int&#233;ressant de noter qu&#8217;il est en fait math&#233;matiquement tr&#232;s 
similaire &#224; l&#8217;information mutuelle3. Dans la formulation de Dunning (1993), reprise dans Manning et 
Sch&#252;tze (1999), le rapport de vraisemblance est exprim&#233; comme le rapport entre les probabilit&#233;s de deux 
hypoth&#232;ses, mod&#233;lisant respectivement l&#8217;ind&#233;pendance et la d&#233;pendance de deux &#233;v&#232;nements. Comme 
telle, la formule s&#8217;applique mal &#224; plus de deux &#233;v&#232;nements. Mais, comme le mentionnent Moore (2004) et 
Evert (2005), le rapport de vraisemblance (G2) peut &#234;tre reformul&#233;, par une s&#233;rie de d&#233;rivations, sous une 
forme presque &#233;quivalente &#224; celle de l&#8217;information mutuelle de deux variables al&#233;atoires (M.I.). 
</p>
<p>&#201;quation 1 : information mutuelle   &#201;quation 2 : rapport de vraisemblance  
de deux variables al&#233;atoires   (log-likelihood ratio) pour deux &#233;v&#232;nements 
</p>
<p>  
</p>
<p>! 
</p>
<p>M.I.=
1
</p>
<p>N
fij log
</p>
<p>f ij
) 
f iji, j
</p>
<p>&quot;     
  
</p>
<p>! 
</p>
<p>G
2
</p>
<p>= 2 f ij log
f ij
) 
f iji, j
</p>
<p>&quot;  
</p>
<p>L&#8217;avantage de cette reformulation est qu&#8217;elle peut s&#8217;&#233;tendre aux n-grammes de degr&#233; sup&#233;rieur. 
Cependant, avec n &gt;= 3, il existe plus d&#8217;une fa&#231;on de mod&#233;liser l&#8217;hypoth&#232;se nulle (Banerjee et Pedersen, 
2003). Par exemple, pour n = 3, on peut consid&#233;rer le mod&#232;le o&#249; les trois mots sont mutuellement 
ind&#233;pendants, mais aussi trois autres mod&#232;les o&#249; une paire de mots est d&#233;pendante, mais ind&#233;pendante du 
troisi&#232;me. 
</p>
<p>Le nombre de mod&#232;les &#224; consid&#233;rer, de m&#234;me que le nombre de fr&#233;quences marginales &#224; calculer, 
augmente rapidement avec n, ce qui rend l&#8217;approche n-gramme plus complexe. C&#8217;est pourquoi plusieurs 
auteurs ont pr&#233;f&#233;r&#233; le mod&#232;le bigramme. 
</p>
<p>                                                
3  &#192; ne pas confondre avec l&#8217;&#171; information mutuelle sp&#233;cifique &#187; (PMI, pointwise mutual information), qui est une mesure de 
</p>
<p>d&#233;pendance entre deux &#233;v&#233;nements X = x et Y = y. En th&#233;orie de l&#8217;information, l&#8217;information mutuelle mesure la 
d&#233;pendance de deux variables al&#233;atoires X et Y, et correspond &#224; la valeur esp&#233;r&#233;e des PMI sur l&#8217;ensemble de toutes les 
&#233;ventualit&#233;s de X et Y. </p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Simon CHAREST, &#201;ric BRUNELLE, Jean FONTAINE 
 
</p>
<p>Mettant &#224; contribution l&#8217;analyseur syntaxique profond Fips, Seretan et coll. (2003) d&#233;crivent une m&#233;thode 
it&#233;rative pour extraire des cooccurrences multimots. Tous les bigrammes candidats sont d&#8217;abord extraits, 
puis, dans un processus it&#233;ratif, des (n + 1)-grammes sont construits &#224; partir de n-grammes partageant n - 1 
mots. Les n-grammes ainsi combin&#233;s sont exclus des r&#233;sultats, la proc&#233;dure ne conservant que les 
combinaisons les plus longues. Le rapport de vraisemblance est employ&#233; comme mesure de pertinence des 
combinaisons extraites. Pour un n-gramme de plus de 2 mots, ce score est calcul&#233; &#224; partir des fr&#233;quences 
des (n - 1)-grammes le composant. Par rapport &#224; des techniques d&#8217;extraction bas&#233;es sur des patrons 
syntaxiques, leur m&#233;thode a l&#8217;avantage d&#8217;&#234;tre souple, car aucune restriction n&#8217;est impos&#233;e sur la structure 
syntaxique des combinaisons extraites. De plus, des relations syntaxiques &#233;loign&#233;es sont prises en compte. 
</p>
<p>L&#8217;approche que nous avons employ&#233;e se rapproche de celle de Seretan et coll. (2003). Comme eux, nous 
employons un analyseur syntaxique. Nous avons aussi choisi de nous en tenir au mod&#232;le bigramme, pour 
sa simplicit&#233;. La prochaine section d&#233;crit la m&#233;thodologie que nous avons mise au point pour extraire les 
cooccurrences multimots. 
</p>
<p>3 M&#233;thodologie 
</p>
<p>3.1 Analyse syntaxique 
De 500 millions de mots pour la premi&#232;re &#233;dition, nous avons augment&#233; le corpus &#224; 1,8 milliard de mots 
ou 92 millions de phrases. Ces phrases ont &#233;t&#233; analys&#233;es par le moteur syntaxique d&#8217;Antidote, qui effectue 
une analyse en d&#233;pendance et g&#233;n&#232;re des arbres syntaxiques complets. Lorsque plusieurs analyses sont 
trouv&#233;es pour une m&#234;me phrase, l&#8217;arbre le plus probable, selon la pond&#233;ration de l&#8217;analyseur, est choisi. 
Un arbre syntaxique complet correspond en fait &#224; un ensemble de liens de d&#233;pendance entre les divers 
mots de la phrase. Chaque lien unit un gouverneur (que nous appelons m&#232;re) &#224; un d&#233;pendant (que nous 
appelons fille), au moyen d&#8217;une relation syntaxique. Les liens correspondant aux relations syntaxiques les 
plus pertinentes (sujet, COD, COI, &#233;pith&#232;te, compl&#233;ment du nom, etc.) sont alors extraits de ces arbres et 
entrepos&#233;s dans une base de donn&#233;es. Au total, 440 millions de liens sont ainsi extraits, correspondant &#224; 60 
millions de combinaisons &lt;m&#232;re &#8211; relation &#8211; fille&gt; diff&#233;rentes, chaque combinaison apparaissant en 
moyenne 7,3 fois dans le corpus. Par exemple, prenons la phrase &#171; Le d&#233;partement joue un r&#244;le de premier 
plan. &#187;. L&#8217;arbre produit par l&#8217;analyseur syntaxique, ainsi que les relations pertinentes qui en sont extraites, 
sont illustr&#233;s &#224; la Figure 1. 
</p>
<p> 
</p>
<p>Figure 1 : extraction des relations pertinentes 
Au-del&#224; des relations syntaxiques directes, le syst&#232;me s&#8217;efforce d&#8217;extraire des relations plus profondes. 
Ainsi, on substitue &#224; un pronom relatif son ant&#233;c&#233;dent afin d&#8217;extraire la relation entre l&#8217;ant&#233;c&#233;dent et le 
verbe de la relative (la femme qu&#8217;il a s&#233;duite &#61664; s&#233;duire une femme). Lorsqu&#8217;un nom agit comme agent </p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>AU-DELA DE LA PAIRE DE MOTS : EXTRACTION DE COOCCURRENCES SYNTAXIQUES MULTILEXEMIQUES 
 
</p>
<p>d&#8217;un verbe &#224; l&#8217;infinitif, ce lien est transform&#233; en relation &#171; sujet &#187; (il incombe au juge de trancher &#61664; le 
juge tranche). Lorsqu&#8217;une relation met en jeu un nom collectif ayant un compl&#233;ment, une relation directe 
avec le compl&#233;ment est aussi g&#233;n&#233;r&#233;e (un groupe d&#8217;&#233;tudiants revendique &#61664; les &#233;tudiants revendiquent). 
</p>
<p>3.2 Inf&#233;rence des cooccurrences int&#233;ressantes 
L&#8217;id&#233;e principale de notre approche est la suivante : plut&#244;t que d&#8217;utiliser, comme base de nos calculs 
statistiques, des combinaisons de deux mots simples, nous travaillons avec des combinaisons de deux 
arbres. Comme pour les combinaisons de mots, nous avons une m&#232;re et une fille, unies par une relation 
syntaxique. Or, si chaque occurrence d&#8217;une relation correspond &#224; une seule combinaison de mots, elle peut 
correspondre &#224; plusieurs combinaisons d&#8217;arbres. Par exemple, &#224; la Figure 2, pour la relation entre B et D, 
nous obtenons 8 combinaisons diff&#233;rentes, selon les branches que l&#8217;on inclut sous B et D. 
</p>
<p> 
</p>
<p>Figure 2 : toutes les combinaisons d&#8217;arbres pour une relation donn&#233;e 
Notre approche consid&#232;re toutes les combinaisons possibles de tous les sous-arbres ayant pour t&#234;te chacun 
des deux mots li&#233;s par une relation de d&#233;pendance. En th&#233;orie, la taille des arbres g&#233;n&#233;r&#233;s n&#8217;est limit&#233;e que 
par la longueur et la structure des phrases du corpus. En pratique, nous avons d&#233;cid&#233; de limiter les arbres 
consid&#233;r&#233;s &#224; 5 mots maximum, pour raccourcir le temps de calcul et parce que les combinaisons trop 
longues ont tendance &#224; &#234;tre moins int&#233;ressantes linguistiquement. Les 60 millions de combinaisons de 
mots diff&#233;rentes trouv&#233;es dans le corpus correspondent &#224; 1,1 milliard de combinaisons d&#8217;arbres de 
longueur 5 ou moins, chaque combinaison d&#8217;arbres apparaissant en moyenne 1,5 fois. 
</p>
<p>3.3 Calcul de la force en tenant compte de la dispersion 
Comme pour les combinaisons de mots, nous calculons la force des combinaisons d&#8217;arbres &#224; l&#8217;aide de la 
version bigramme du rapport de vraisemblance. Nous avons fix&#233; empiriquement un seuil, de fa&#231;on &#224; ne 
conserver que les combinaisons les plus fortes. Nous excluons aussi les combinaisons ayant une fr&#233;quence 
inf&#233;rieure &#224; 4, car les mesures d&#8217;association sont moins fiables lorsque les fr&#233;quences sont tr&#232;s faibles. En 
effet, Evert (2005) mentionne que les estimations de probabilit&#233;s sont biais&#233;es pour les &#233;v&#232;nements de 
faible fr&#233;quence, et sugg&#232;re fortement de toujours exclure les donn&#233;es de fr&#233;quence 1 ou 2. Le biais 
diminue progressivement &#224; mesure que monte la fr&#233;quence. &#192; partir d&#8217;une fr&#233;quence de 5, le biais devient 
minime et les mesures d&#8217;association se comportent bien. 
</p>
<p>Nous avions remarqu&#233;, pour la premi&#232;re &#233;dition de notre dictionnaire, que certaines cooccurrences ayant 
une fr&#233;quence &#233;tonnamment &#233;lev&#233;e provenaient en fait d&#8217;un seul site Web (ou d&#8217;un nombre restreint de 
sites) traitant d&#8217;un sujet particulier. Ces cooccurrences sont souvent inint&#233;ressantes linguistiquement, car </p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Simon CHAREST, &#201;ric BRUNELLE, Jean FONTAINE 
 
</p>
<p>elles ne sont pas le reflet de la langue en g&#233;n&#233;ral, mais d&#8217;une terminologie associ&#233;e &#224; un contexte 
sp&#233;cifique. Comme nous l&#8217;avions fait alors, nous avons adapt&#233; le calcul de la force pour tenir compte de la 
dispersion d&#8217;une combinaison &#224; travers plusieurs sources, et ainsi r&#233;duire l&#8217;effet des cooccurrences trop 
d&#233;pendantes du choix du corpus. &#192; l&#8217;&#233;poque, nous avions impl&#233;ment&#233; un calcul de fr&#233;quence pond&#233;r&#233;e qui 
correspondait &#224; la somme des racines carr&#233;es des fr&#233;quences pour chaque source (Charest et coll., 2007). 
Or, cette technique, en plus de ne pas &#234;tre fond&#233;e math&#233;matiquement, avait le d&#233;savantage de compliquer 
le calcul des fr&#233;quences, en particulier pour la phase de l&#8217;&#233;lagage (voir section 3.4). 
</p>
<p>Cette fois-ci, nous employons comme mesure de dispersion l&#8217;&#233;cart de proportions (Deviation of 
Proportions [DP]) de Gries (2008). Cette mesure, qui allie simplicit&#233; et flexibilit&#233;, se calcule en prenant la 
somme des diff&#233;rences entre les proportions esp&#233;r&#233;e et observ&#233;e d&#8217;un ph&#233;nom&#232;ne dans chacune des 
sources par rapport &#224; l&#8217;ensemble du corpus. 
</p>
<p>&#201;quation 3 : &#233;cart de proportions (DP) 
</p>
<p>! 
</p>
<p>DP =
</p>
<p>f i
</p>
<p>f i
i
</p>
<p>&quot;
#
</p>
<p>ti
</p>
<p>ti
i
</p>
<p>&quot;i
&quot;
</p>
<p>2
  
</p>
<p>! 
</p>
<p>dispersion =1&quot;DP  
</p>
<p>o&#249; fi est la fr&#233;quence d&#8217;une combinaison dans la source i et ti est la taille de la source i. Gries (2008) 
mentionne que la plupart des mesures de dispersion sont appliqu&#233;es directement sur la fr&#233;quence pour 
obtenir des fr&#233;quences dites ajust&#233;es; cependant, il n&#8217;est pas clair pour lui ce que repr&#233;sentent r&#233;ellement 
ces pseudofr&#233;quences. Plut&#244;t que de modifier les fr&#233;quences, nous avons donc choisi de multiplier 
directement la valeur de la force (calcul&#233;e par le rapport de vraisemblance avec les fr&#233;quences brutes) par 
la valeur de dispersion. Cette fa&#231;on de faire, plus simple, nous apparait tout aussi valable. 
</p>
<p>Enfin, mentionnons que le score calcul&#233; correspond &#224; la relation entre la t&#234;te d&#8217;un l&#8217;arbre syntaxique et 
l&#8217;une de ses filles. Lorsque la t&#234;te a plus d&#8217;une fille, chaque relation g&#233;n&#232;re un score. Par exemple, dans 
prendre ses jambes &#224; son cou, on a deux filles branch&#233;es sur la m&#234;me m&#232;re. Ces deux branchements 
correspondent aux interpr&#233;tations [prendre ses jambes] + [&#224; son cou] et [prendre &#224; son cou] + [ses jambes]. 
Pour le besoin du filtrage statistique des cooccurrences, un seul score suffit. Nous choisissons alors le plus 
&#233;lev&#233; comme score de la cooccurrence. 
</p>
<p>3.4 &#201;lagage 
Certaines cooccurrences de plus de deux mots ne sont pas naturellement d&#233;composables. Par exemple, 
m&#234;me si les fr&#233;quences de prendre le taureau et prendre par les cornes sont n&#233;cessairement au moins 
aussi &#233;lev&#233;es que la fr&#233;quence de prendre le taureau par les cornes, seule cette derni&#232;re est int&#233;ressante. 
</p>
<p>Pour &#233;laguer ces combinaisons moins int&#233;ressantes, Silva et coll. (1999) ont introduit un algorithme 
nomm&#233; LocalMaxs, qui s&#233;lectionne les combinaisons de n mots dont le score est &#224; la fois sup&#233;rieur ou &#233;gal 
&#224; celui de toutes les combinaisons de n - 1 mots incluses dans celles-ci et sup&#233;rieur &#224; celui de toutes les 
combinaisons de n + 1 mots d&#233;riv&#233;es de celles-ci. Cet algorithme leur &#233;vite aussi de devoir fixer un seuil 
empiriquement : toutes les combinaisons satisfaisant &#224; ces crit&#232;res sont conserv&#233;es, peu importe leur score. </p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>AU-DELA DE LA PAIRE DE MOTS : EXTRACTION DE COOCCURRENCES SYNTAXIQUES MULTILEXEMIQUES 
 
</p>
<p>De notre c&#244;t&#233;, nous avons plut&#244;t choisi de r&#233;duire la force des cooccurrences incompl&#232;tes en modifiant 
leur fr&#233;quence brute. Lorsqu&#8217;une cooccurrence complexe a une fr&#233;quence relativement &#233;lev&#233;e par rapport &#224; 
une cooccurrence plus simple incluse dans celle-ci, on recalculera le score de la cooccurrence plus simple 
en prenant une fr&#233;quence modifi&#233;e qui ne tient pas compte des occurrences de la cooccurrence complexe. 
Ainsi, pour prendre le taureau, on comptera seulement les occurrences o&#249; cette combinaison n&#8217;est pas 
accompagn&#233;e du compl&#233;ment par les cornes. Par contre, si aucune cooccurrence plus complexe n&#8217;a une 
fr&#233;quence assez &#233;lev&#233;e par rapport &#224; une cooccurrence simple, sa fr&#233;quence n&#8217;est pas r&#233;duite. Par exemple, 
la fr&#233;quence de jouer un r&#244;le ne sera pas affect&#233;e par jouer un r&#244;le important, jouer un grand r&#244;le, jouer 
un r&#244;le de premier plan, etc. 
</p>
<p>3.5 D&#233;termination des attributs de forme 
Outre les mots formant la cooccurrence, nous notons certaines donn&#233;es morphosyntaxiques qui d&#233;finissent 
la distribution de ses emplois. Nous retenons ainsi, pour chaque cooccurrence, les types des d&#233;terminants, 
le genre, le nombre et la casse de chaque mot, la position relative de ceux-ci, la pr&#233;sence de clitiques (en, 
y), de pronoms r&#233;fl&#233;chis (se, s&#8217;) ou de particules n&#233;gatives autour des verbes, etc. Ces donn&#233;es d&#233;terminent 
la formulation la plus fr&#233;quente de la cooccurrence, qui sera utilis&#233;e pour g&#233;n&#233;rer la forme finale de la 
cooccurrence au moment de son affichage. 
</p>
<p>3.6 R&#233;vision manuelle 
Les cooccurrences qui ne faisaient pas partie de la premi&#232;re &#233;dition du dictionnaire ont toutes &#233;t&#233; r&#233;vis&#233;es 
par une &#233;quipe de linguistes. Les cooccurrences incompl&#232;tes, inint&#233;ressantes, d&#233;licates (offensantes, 
vulgaires, etc.), fautives (impropri&#233;t&#233;s, calques, pl&#233;onasmes, etc.) ou r&#233;sultant d&#8217;une mauvaise analyse 
syntaxique ont &#233;t&#233; retranch&#233;es. Comme on pouvait s&#8217;y attendre, les mauvaises analyses sont plus 
fr&#233;quentes pour les cooccurrences de plus de deux mots. M&#234;me si une forme peut sembler correcte en 
surface, lorsque l&#8217;on examine en d&#233;tail la structure syntaxique, on d&#233;couvre parfois un mauvais 
branchement ou la substitution d&#8217;un mot par son homographe. Dans ce dernier cas, un m&#233;canisme permet 
au linguiste de corriger la situation en transf&#233;rant la cooccurrence sur le bon mot. Le linguiste a aussi la 
possibilit&#233; de modifier les attributs des mots de la cooccurrence qui ont &#233;t&#233; choisis automatiquement (les 
d&#233;terminants, la flexion, la casse, etc.). Enfin, le linguiste a aussi pour t&#226;che de classer les cooccurrences 
des mots polys&#233;miques selon leurs divers sens.  
</p>
<p>4 R&#233;sultats 
Du 1,1 milliard de combinaisons d&#8217;arbres identifi&#233;es par le syst&#232;me, 4,2 millions sont &#224; la fois assez 
fr&#233;quentes et assez fortes pour &#234;tre conserv&#233;es dans notre base de donn&#233;es. Ces cooccurrences se 
r&#233;partissent comme suit : 49,0 % sont de 2 mots, 42,1 % de 3 mots, 7,5 % de 4 mots et 1,4 % de 5 mots. &#192; 
ce jour, 952 000 cooccurrences ont &#233;t&#233; revues par un linguiste, en proc&#233;dant par ordre d&#233;croissant de force. 
Parmi celles-ci, 55 000 (5,8 %) ont &#233;t&#233; rejet&#233;es, car inint&#233;ressantes; 25 000 (2,6 %) ont &#233;t&#233; identifi&#233;es 
comme &#233;tant de mauvaises analyses; 10 000 (1,1 %) ont &#233;t&#233; jug&#233;es incompl&#232;tes. Au total, 862 000 
cooccurrences, dont 783 000 de deux mots (90,8 %)4, ont &#233;t&#233; retenues et font partie de la pr&#233;sente &#233;dition. 
</p>
<p>                                                
4  La proportion de cooccurrences de deux mots est plus &#233;lev&#233;e pour les cooccurrences retenues (90,8 %) que pour les 
</p>
<p>cooccurrences brutes (49,0 %), car la majorit&#233; d&#8217;entre elles faisait partie de la premi&#232;re &#233;dition et a &#233;t&#233; conserv&#233;e. </p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Simon CHAREST, &#201;ric BRUNELLE, Jean FONTAINE 
 
</p>
<p>5 Pr&#233;sentation des cooccurrences 
Nous avons maintenu les principes de base que nous avions d&#233;termin&#233;s pour la pr&#233;sentation des 
cooccurrences de la premi&#232;re &#233;dition et qui ont fait leurs preuves : affichage des cooccurrences compl&#232;tes 
avec leurs attributs morphosyntaxiques r&#233;els, histogramme illustrant la force, regroupement par sens puis 
par contexte syntaxique, affichage initial des 5 premi&#232;res cooccurrences de chaque groupe avec possibilit&#233; 
d&#8217;afficher la suite en un clic, exemples r&#233;els tir&#233;s du corpus pr&#233;sent&#233;s dans un panneau secondaire. 
</p>
<p> 
</p>
<p>Figure 3 : interface du dictionnaire de cooccurrences d&#8217;Antidote HD 
La seule adaptation que nous avons effectu&#233;e concerne l&#8217;affichage des cooccurrences de plus de deux 
mots. Nous avons choisi de regrouper celles-ci sous une cooccurrence plus simple s&#8217;il en existe une dont la 
force n&#8217;est pas trop faible par rapport &#224; la force des cooccurrences plus complexes. Par exemple, &#224; la 
Figure 3, nous avons, sous le deuxi&#232;me sens de l&#8217;entr&#233;e &#233;mission, &#224; la relation &#171; avec compl&#233;ment 
nominal &#187;, la cooccurrence de 4 mots &#233;missions de gaz &#224; effet de serre, dont la force est tr&#232;s &#233;lev&#233;e. Sous 
elle, diverses cooccurrences de 5 mots comme r&#233;duire les &#233;missions de gaz &#224; effet de serre. Cette derni&#232;re 
</p>
<p>&#169; 2010 Druide informatique inc.  </p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>AU-DELA DE LA PAIRE DE MOTS : EXTRACTION DE COOCCURRENCES SYNTAXIQUES MULTILEXEMIQUES 
 
</p>
<p>cooccurrence se retrouve aussi &#224; la relation &#171; compl&#233;ment direct &#187;, sous la cooccurrence de 3 mots r&#233;duire 
les &#233;missions. En effet, le mot-vedette &#233;mission est &#224; la fois compl&#233;ment direct de r&#233;duire et m&#232;re du 
compl&#233;ment nominal de gaz &#224; effet de serre. Notons que la cooccurrence de 4 mots &#233;missions de gaz &#224; 
effet de serre n&#8217;est pas class&#233;e sous la cooccurrence plus simple &#233;mission de gaz, car sa force est 
significativement plus &#233;lev&#233;e que celle de cette derni&#232;re. 
</p>
<p>6 Conclusion et perspectives 
Nous venons de pr&#233;senter une technique originale pour l&#8217;extraction de cooccurrences de plus de deux mots 
conservant une structure syntaxique compl&#232;te. Notre approche se situe dans la m&#234;me veine que celle de 
Seretan et coll. (2003), en ce sens qu&#8217;elle allie la sortie d&#8217;une analyse syntaxique profonde &#224; un test 
statistique appliqu&#233; sur deux groupes de mots, pour d&#233;gager des cooccurrences saillantes de plusieurs mots 
pour lesquelles aucune restriction de structure syntaxique n&#8217;est impos&#233;e. Comme la leur, notre approche se 
d&#233;marque donc des m&#233;thodes d&#8217;extraction multimot bas&#233;es sur du texte brut ou simplement annot&#233; 
grammaticalement, qui par nature sont beaucoup plus restreintes syntaxiquement (Seretan, 2008).  
</p>
<p>Notre approche se distingue par contre &#224; plusieurs points de vue. D&#8217;abord, la m&#233;thode que nous proposons 
n&#8217;est pas r&#233;cursive ni it&#233;rative : les cooccurrences plus complexes ne sont pas construites &#224; partir de 
cooccurrences plus simples. Nous avons plut&#244;t choisi l&#8217;approche combinatoire : pour une relation 
syntaxique entre deux mots dans l&#8217;analyse d&#8217;une phrase, toutes les combinaisons de tous les sous-arbres 
ayant pour t&#234;te chacun des deux mots sont &#233;valu&#233;es. Le calcul des fr&#233;quences pour la mesure d&#8217;association 
est aussi diff&#233;rent. L&#8217;approche r&#233;cursive de Seretan et coll. (2003) consid&#232;re une cooccurrence complexe 
comme &#233;tant form&#233;e de deux cooccurrences plus simples en intersection, et ce sont les fr&#233;quences de ces 
deux cooccurrences qui sont utilis&#233;es dans la table de contingence. De notre c&#244;t&#233;, les fr&#233;quences utilis&#233;es 
sont celles des deux arbres formant la cooccurrence, arbres qui sont n&#233;cessairement disjoints. 
</p>
<p>Parmi les am&#233;liorations envisag&#233;es, nous aimerions &#233;tendre le concept de cooccurrences &#224; des 
g&#233;n&#233;ralisations des arguments des fonctions syntaxiques. Par exemple, la cooccurrence avoir besoin pour 
vivre, telle qu&#8217;extraite par notre syst&#232;me, semble incompl&#232;te. Elle a d&#251; &#234;tre augment&#233;e manuellement par 
un linguiste pour devenir avoir besoin de qqch. pour vivre. En g&#233;n&#233;ralisant les compl&#233;ments que prennent 
les verbes &#224; l&#8217;aide de pronoms g&#233;n&#233;riques comme qqch. et qqn, nous esp&#233;rons pouvoir inf&#233;rer ces ajouts 
automatiquement. 
</p>
<p>Remerciements 
Nous tenons &#224; remercier Mala Bergevin, Jean Saint-Germain, Maud Pironneau et toute l&#8217;&#233;quipe des 
druides sans qui Antidote HD n&#8217;aurait pu voir le jour.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Simon CHAREST, &#201;ric BRUNELLE, Jean FONTAINE 
 
</p>
<p>R&#233;f&#233;rences 
BANERJEE  S., PEDERSEN T. (2003). The Design, Implementation and Use of the Ngram Statistics Package. 
Proceedings of the Fourth International Conference on Intelligent Text Processing and Computational 
Linguistics, 370&#8211;381. 
BLAHETA D., JOHNSON M. (2001). Unsupervised learning of multi-word verbs. Proceedings of the ACL 
Workshop on Collocations, 54&#8211;60. 
CHAREST S., BRUNELLE &#201;., FONTAINE J., PELLETIER, B. (2007). &#201;laboration automatique d&#8217;un dictionnaire 
de cooccurrences grand public. Actes de la 14e conf&#233;rence sur le Traitement Automatique des Langues 
Naturelles (TALN 2007), 283&#8211;292. 
DIAS G., GUILLORE S., PEREIRA LOPES, J. G. (2000). Normalization of Association Measures for 
Multiword Lexical Unit Extraction. International Conference on Artificial and Computational Intelligence 
for Decision Control and Automation in Engineering and Industrial Applications (ACIDCA 2000), 207-
216. 
DUNNING, T. (1993). Accurate methods for the statistics of surprise and coincidence. Computational 
Linguistics 19, 61-74. 
EVERT S. (2005). The Statistics of Word Cooccurrences : Word Pairs and Collocations. Th&#232;se de doctorat, 
Universit&#233; de Stuttgart. 
GRIES, S. (2008). Dispersions and adjusted frequencies in corpora. International Journal of Corpus 
Linguistics, vol. 13, no 4, 403&#8211;437. 
MANNING C., SCH&#220;TZE H. (1999). Foundations of Statistical Natural Language Processing. Cambridge : 
The MIT Press. 
MOORE R. C. (2004). On log-likelihood-ratios and the significance of rare events. Proceedings of the 2004 
Conference on EMNLP, 333-340. 
PECINA P., SCHLESINGER P. (2006). Combining association measures for collocation extraction. 
Proceedings of the 21th International Conference on Computational Linguistics and 44th Annual Meeting 
of the Association for Computational Linguistics (COLING/ACL 2006), 651-658. 
SERETAN V. (2008). Collocation extraction based on syntactic parsing. Th&#232;se de doctorat, Universit&#233; de 
Gen&#232;ve. 
SERETAN V., NERIMA L., WEHRLI E. (2003). Extraction of Multi-Word Collocations Using Syntactic 
Bigram Composition. Proceedings of the Fourth International Conference on Recent Advances in NLP 
(RANLP-2003), 424&#8211;431. 
SILVA J., DIAS G., GUILLORE S., PEREIRA LOPES, J. G. (1999). Using LocalMaxs Algorithm for the 
Extraction of Contiguous and Non-contiguous Multiword Lexical Units. Proceedings of 9th Portuguese 
Conference in Artificial Intelligence, 21-24. 
TUTIN A. (2008). For an extended definition of lexical collocations. Proceedings of the XIII Euralex 
International Congress (Barcelona, 15-19 July 2008), 1453-1460. 
VILLADA MOIRON B. (2005). Data-driven Identification of fixed expressions and  their modifiability. Th&#232;se 
de doctorat, Universit&#233; de Groningue. </p>

</div></div>
</body></html>