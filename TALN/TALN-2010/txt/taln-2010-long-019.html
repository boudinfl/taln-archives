<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
"http://www.w3.org/TR/html4/loose.dtd">
<html><head><title>Classification du genre vid&#233;o reposant sur des transcriptions automatiques</title>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
</head>
<body>
<div style="page-break-before:always; page-break-after:always"><div><p>TALN 2010, Montr&#233;al, 19&#8211;23 juillet 2010
</p>
<p>Classification du genre vid&#233;o reposant sur des transcriptions
automatiques
</p>
<p>Stanislas Oger, Mickael Rouvier, Georges Linar&#232;s &#8727;
</p>
<p>LIA, Universit&#233; d&#8217;Avignon, France
{stanislas.oger, mickael.rouvier, georges.linares}@univ-avignon.fr
</p>
<p>R&#233;sum&#233;. Dans cet article nous proposons une nouvelle m&#233;thode pour l&#8217;identification du genre vid&#233;o
qui repose sur une analyse de leur contenu linguistique. Cette approche consiste en l&#8217;analyse des mots
apparaissant dans les transcriptions des pistes audio des vid&#233;os, obtenues &#224; l&#8217;aide d&#8217;un syst&#232;me de re-
connaissance automatique de la parole. Les exp&#233;riences sont r&#233;alis&#233;es sur un corpus compos&#233; de dessins
anim&#233;s, de films, de journaux t&#233;l&#233;vis&#233;s, de publicit&#233;s, de documentaires, d&#8217;&#233;missions de sport et de clips
de musique. L&#8217;approche propos&#233;e permet d&#8217;obtenir un taux de bonne classification de 74% sur cette t&#226;che.
En combinant cette approche avec des m&#233;thodes reposant sur des param&#232;tres acoustiques bas-niveau, nous
obtenons un taux de bonne classification de 95%.
</p>
<p>Abstract. In this paper, we present a new method for video genre identification based on the linguis-
tic content analysis. This approach relies on the analysis of the words in the video transcriptions provided
by an automatic speech recognition system. Experiments are conducted on a corpus composed of cartoons,
movies, news, commercials, documentary, sport and music. On this 7-genre identification task, the propo-
sed transcription-based method obtains up to 74% of correct identification. Finally, this rate is increased
to 95% by combining the proposed linguistic-level features with low-level acoustic features.
</p>
<p>Mots-cl&#233;s : classification de genre vid&#233;o, traitement audio de la vid&#233;o, extraction de param&#232;tres
linguistiques.
</p>
<p>Keywords: video genre classification, audio-based video processing, linguistic feature extraction.
</p>
<p>&#8727;. Ces travaux ont &#233;t&#233; en partie financ&#233;s par l&#8217;Agence Nationale de la Recherche (ANR), par l&#8217;interm&#233;diaire du projet RPM2
(ANR-07-AM-008).</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>STANISLAS OGER, MICKAEL ROUVIER, GEORGES LINAR&#200;S
</p>
<p>1 Introduction
</p>
<p>L&#8217;indexation vid&#233;o est un domaine porteur dans le contexte actuel o&#249; l&#8217;on voit se d&#233;velopper les cha&#238;nes
de t&#233;l&#233;visions num&#233;riques et les collections de vid&#233;os sur internet. L&#8217;un des crit&#232;res les plus utiles pour
la recherche de vid&#233;os dans une base de donn&#233;es est sans doute le genre (dessin anim&#233;, documentaire,
etc.). Son identification automatique a suscit&#233; r&#233;cemment beaucoup d&#8217;int&#233;r&#234;t de la part de la communaut&#233;
scientifique. La grande majorit&#233; des travaux publi&#233;s repose sur l&#8217;extraction de param&#232;tres vid&#233;os, tel que la
couleur, la luminosit&#233;, ou des informations sur les changements de prise de vue (Brezeale &amp; Cook, 2008).
Ces descripteurs bas-niveaux sont en g&#233;n&#233;ral combin&#233;s en utilisant des classifieurs statistiques, tel que les
machines &#224; vecteurs de support (MVS) ou les mod&#232;les de m&#233;langes gaussiens (MMG). L&#8217;autre approche
classique consiste &#224; extraire des param&#232;tres cepstraux de la piste audio des vid&#233;os (Roach &amp; Mason, 2001).
C&#8217;est ce que nous avons propos&#233; dans Rouvier et al. (2009), en plus d&#8217;autres param&#232;tres acoustiques haut
niveau.
</p>
<p>L&#8217;identification automatique du genre est une t&#226;che de cat&#233;gorisation de texte &#233;tudi&#233;e depuis longtemps
(Karlgren &amp; Cutting, 1994). Ce qui caract&#233;rise le genre dans un texte est principalement le style &#233;ditorial.
Pour les vid&#233;os, le contenu linguistique est de nature parl&#233;e mais contient la m&#234;me information pour
caract&#233;riser le genre que le texte (Biber, 1988). Malgr&#233; cela, tr&#232;s peu de travaux proposent d&#8217;utiliser cette
modalit&#233; pour la classification en genre vid&#233;o. La raison principale est que le contenu linguistique des
vid&#233;os est rarement disponible et que l&#8217;obtenir revient souvent &#224; faire de la transcription manuelle, qui est
tr&#232;s on&#233;reuse. On trouve cependant quelques &#233;tudes qui utilisent les sous-titres associ&#233;s aux vid&#233;os (Lin &amp;
Hauptmann, 2002; Brezeale &amp; Cook, 2006), mais ces approches sont inutilisables lorsque ceux-ci ne sont
pas disponibles, par exemple sur les plateformes internet d&#8217;&#233;change de contenus vid&#233;os.
</p>
<p>Une mani&#232;re peu co&#251;teuse d&#8217;obtenir la transcription des vid&#233;os est d&#8217;utiliser un syst&#232;me de reconnaissance
automatique de la parole (RAP). La plupart des travaux proposant cette approche concernent la classifi-
cation th&#233;matique et g&#233;n&#233;ralement dans un domaine o&#249; le syst&#232;me de RAP est performant, ce qui permet
d&#8217;aborder le probl&#232;me comme s&#8217;ils s&#8217;agissait de donn&#233;es textuelles non erron&#233;es. Par exemple, Wang et al.
(2003) proposent d&#8217;effectuer la classification automatique de nouvelles issues de journaux radiodiffus&#233;s &#224;
partir de transcriptions automatiques avec un taux d&#8217;erreur mot de l&#8217;ordre de 10%. L&#8217;investissement n&#233;ces-
saire pour obtenir de telles performances de RAP sur des genres vid&#233;os vari&#233;s, tel que les dessins anim&#233;s,
documentaires, etc., serait tr&#232;s co&#251;teux.
</p>
<p>Dans cet article nous proposons une m&#233;thode de classification automatique du genre vid&#233;o reposant sur
la caract&#233;risation du style &#233;ditorial dans des transcriptions issues de RAP avec un taux d&#8217;erreur variable.
Nous proposons plusieurs m&#233;triques pour s&#233;lectionner les termes qui seront fournis au classifieur. Le
mod&#232;le que nous proposons s&#8217;inscrit dans l&#8217;architecture de classification multimodale &#224; deux niveaux
propos&#233;e dans Rouvier et al. (2009) : (I) des descripteurs de contenu homog&#232;nes (acoustique, linguistique,
etc.) qui peuvent &#234;tre eux-m&#234;mes des classifieurs bas-niveau qui pr&#233;-classifient le genre des vid&#233;os et (II)
un classifieur global qui prend la d&#233;cision finale en fonction des descripteurs et des sorties des classifieurs
du niveau inf&#233;rieur.
</p>
<p>Dans la premi&#232;re section, nous pr&#233;sentons en d&#233;tails notre approche pour extraire des param&#232;tres linguis-
tiques des transcriptions automatiques. Ensuite, nous analysons les r&#233;sultats exp&#233;rimentaux obtenus avec
l&#8217;approche linguistique propos&#233;e. Finalement, nous pr&#233;sentons et discutons de la compl&#233;mentarit&#233; de ces
param&#232;tres avec ceux que l&#8217;on trouve dans l&#8217;&#233;tat de l&#8217;art.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>CLASSIFICATION DU GENRE VID&#201;O BAS&#201;E SUR DES TRANSCRIPTIONS
</p>
<p>2 Identification du genre bas&#233;e sur la linguistique
</p>
<p>L&#8217;analyse du contenu linguistique des vid&#233;os que nous proposons repose sur l&#8217;utilisation d&#8217;un syst&#232;me
de RAP pour obtenir les transcriptions des vid&#233;os. Ce syst&#232;me utilise un lexique ferm&#233; et un mod&#232;le de
langage qui est estim&#233; sur un corpus textuel de grande taille. Entra&#238;ner un tel mod&#232;le pour chaque genre
vid&#233;o n&#8217;est pas r&#233;alisable car nous ne disposons pas du volume de donn&#233;es textuelles n&#233;cessaires. Nous
proposons donc d&#8217;utiliser un mod&#232;le de langage standard avec un lexique peu adapt&#233; &#224; certains genres, ce
qui causera la plupart du temps un fort taux d&#8217;erreur dans les transcriptions.
</p>
<p>Dans la description des m&#233;thodes et formules qui suivront, le mot terme est employ&#233; dans un sens large,
proche du sens math&#233;matique, et peut d&#233;signer un n-gramme de mots ou d&#8217;&#233;tiquettes morphosyntaxiques,
avec n variant de 1 a 3.
</p>
<p>Le principe du sac de termes est utilis&#233; pour la mod&#233;lisation des documents. Selon ce mod&#232;le, chaque
dimension de l&#8217;espace des param&#232;tres repr&#233;sente un terme et chaque document est repr&#233;sent&#233; par un vec-
teur de fr&#233;quences de termes dans cet espace. Dans le cas o&#249; le terme est un unigramme de mots, alors ce
mod&#232;le est un sac de mots. La taille du n-gramme utilis&#233; comme terme est appel&#233; l&#8217;ordre du mod&#232;le.
</p>
<p>2.1 Approche classique pour le genre vid&#233;o
</p>
<p>Pour les probl&#232;mes de cat&#233;gorisation automatique de texte, les approches g&#233;n&#233;ralement propos&#233;es reposent
sur l&#8217;extraction de mots porteurs de sens des documents &#224; classer. Pour la classification du genre vid&#233;o,
les &#233;tudes qui s&#8217;appuyent sur la modalit&#233; textuelle utilisent en g&#233;n&#233;ral cette approche. Soit les mots-outils
de la langue sont filtr&#233;s, soit une m&#233;trique de type Term Frequency-Inverse Document Frequency (TF.IDF)
est utilis&#233;e pour ne s&#233;lectionner que les mots porteurs de sens des documents (Brezeale &amp; Cook, 2006;
Lin &amp; Hauptmann, 2002). Cette approche sera notre mod&#232;le de r&#233;f&#233;rence dans cet article.
</p>
<p>Pour un terme t et un document d, TF.IDF est d&#233;fini comme suit :
</p>
<p>TF.IDF(t, d) = TF(t, d)&#215; IDF(t) (1)
avec TF(t, d) la fr&#233;quence normalis&#233;e du terme t dans le document d et IDF(t) une m&#233;trique repr&#233;sentant
le pouvoir discriminant du terme t.
</p>
<p>TF(t, d) est d&#233;fini comme suit :
TF(t, d) =
</p>
<p>nt,d&#8721;
k&#8712;d nk,d
</p>
<p>(2)
</p>
<p>avec nt,d la fr&#233;quence du terme t dans le document d.
</p>
<p>IDF(t) est d&#233;fini comme suit :
</p>
<p>IDF(t) = log
(
</p>
<p>Nd
</p>
<p>DF(t)
</p>
<p>)
(3)
</p>
<p>avec Nd le nombre total de documents dans le corpus et DF(t) le nombre de documents du corpus qui
contiennent le terme t. Plus la valeur de TF.IDF d&#8217;un mot est &#233;lev&#233;e, plus le mot consid&#233;r&#233; est repr&#233;sentatif
du document et porteur de la th&#233;matique qu&#8217;il aborde.
</p>
<p>Pour chaque genre, le vecteur de param&#232;tres contient la liste de termes r&#233;sultant de la fusion des n termes
ayant les meilleurs TF.IDF de chaque document du genre. Un seul exemplaire de chaque terme est conserv&#233;
lors de la fusion. Ces vecteurs sont ensuite regroup&#233;s dans un supervecteur qui est fourni au classifieur bas-
niveau.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>STANISLAS OGER, MICKAEL ROUVIER, GEORGES LINAR&#200;S
</p>
<p>2.2 Termes les plus discriminants pour le genre
</p>
<p>L&#8217;approche classique bas&#233;e sur TF.IDF permet de donner plus de poids aux termes discriminants pour les
documents, qui sont souvent porteurs des th&#233;matiques des vid&#233;os. Pourtant, le genre vid&#233;o fait r&#233;f&#233;rence
au style &#233;ditorial et n&#8217;est pas syst&#233;matiquement corr&#233;l&#233; avec les sujets abord&#233;s et, plus g&#233;n&#233;ralement, avec
le contenu s&#233;mantique.
</p>
<p>Nous proposons ici de concevoir une m&#233;trique permettant d&#8217;identifier les termes discriminants pour le
genre et non pour le document. Pour cela, nous proposons d&#8217;adapter le calcul de TF.IDF en Genre Term
Frequency-Inverse Genre Frequency (GTF.IGF), d&#233;fini comme suit pour un terme t et un genre g :
</p>
<p>GTF.IGF(t, g) = GTF(t, g)&#215; IGF(t) (4)
avec GTF(t, g) la somme normalis&#233;e des fr&#233;quences normalis&#233;es du terme t dans les documents du genre
g et IGF(t) une m&#233;trique repr&#233;sentant le pouvoir discriminant du terme t.
</p>
<p>GTF(t, g) est d&#233;finit comme suit :
</p>
<p>GTF(t, g) =
</p>
<p>&#8721;
d&#8712;g TF(t, g)
|g| (5)
</p>
<p>avec |g| le nombre de documents dans le genre g et TF(t, g) la fr&#233;quence normalis&#233;e du terme t dans le
genre g, d&#233;fini &#224; l&#8217;&#233;quation 2.
</p>
<p>IGF(t) est d&#233;fini comme suit :
</p>
<p>IGF(t) = log
(
</p>
<p>N g
</p>
<p>GF(t)
</p>
<p>)
(6)
</p>
<p>avec N g le nombre total de genres dans le corpus et GF(t) le nombre de genres du corpus qui contiennent
le terme t. Plus la valeur de GTF.IGF d&#8217;un mot est &#233;lev&#233;e, plus le mot consid&#233;r&#233; est discriminant pour
l&#8217;identification du genre.
</p>
<p>Pour chaque genre, un vecteur est construit avec les n termes ayant le meilleurs GTF.IGF et ces vecteurs
sont ensuite concat&#233;n&#233;s dans un supervecteur, de taille n&#215;N g, qui est fourni au classifieur bas-niveau.
</p>
<p>2.3 Termes les plus fr&#233;quentes
</p>
<p>Les m&#233;thodes pr&#233;c&#233;dentes permettent d&#8217;identifier des termes discriminants pour un document ou un genre.
Ces termes sont souvent porteurs de sens et plut&#244;t rares en g&#233;n&#233;ral, ils auront donc une probabilit&#233; &#233;lev&#233;e
d&#8217;&#234;tre victimes du d&#233;calage entre le lexique du syst&#232;me de RAP et celui du document. A l&#8217;oppos&#233; de
ces approches, Stamatatos et al. (2000) ont d&#233;montr&#233; l&#8217;efficacit&#233; de l&#8217;utilisation des fr&#233;quences des mots-
outils pour identifier le genre &#233;crit. Nous pensons que ces param&#232;tres peuvent tout aussi bien &#234;tre porteurs
d&#8217;information pour d&#233;tecter le genre vid&#233;o. Contrairement &#224; l&#8217;approche TF-IDF, celle-ci est ind&#233;pendante
des th&#233;matiques des documents, et est donc plus robuste pour classifier des genres comme les news, les
documentaires ou les cartoons, qui abordent des th&#233;matiques tr&#232;s vari&#233;es. De plus, les mots outils sont
caract&#233;ris&#233;s par leurs fr&#233;quences tr&#232;s &#233;lev&#233;es et sont donc plus robustes aux erreurs lexicales du syst&#232;me
de RAP.
</p>
<p>Les n termes les plus fr&#233;quents de l&#8217;ensemble des transcriptions automatiques des documents du corpus
d&#8217;entra&#238;nement servent ainsi de param&#232;tres au classifieur bas-niveau.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>CLASSIFICATION DU GENRE VID&#201;O BAS&#201;E SUR DES TRANSCRIPTIONS
</p>
<p>2.4 G&#233;n&#233;ralisation des constructions caract&#233;ristiques du genre
</p>
<p>Le but des pr&#233;c&#233;dentes approches est de capturer des s&#233;quences de un ou plusieurs mots qui soient ca-
ract&#233;ristiques du genre vid&#233;o. Nous proposons de les g&#233;n&#233;raliser en capturant des patrons de construction
de phrases. Pour cela, nous pouvons utiliser des s&#233;quences d&#8217;&#233;tiquettes morphosyntaxiques au lieu des
s&#233;quences de mots comme termes dans les m&#233;thodes pr&#233;c&#233;dentes.
</p>
<p>Une limite &#224; l&#8217;utilisation d&#8217;&#233;tiquettes morphosyntaxiques de cette mani&#232;re est que les mots sont syst&#233;ma-
tiquement g&#233;n&#233;ralis&#233;s. Il y a peut-&#234;tre certains mots au sein d&#8217;une classe morphosyntaxique pour lesquels
il faudrait conserver la distinction. Nous proposons donc de ne pas &#233;tiqueter certains mots et d&#8217;utiliser les
techniques pr&#233;c&#233;dentes pour identifier les s&#233;quences int&#233;ressantes pour la classification. Ainsi nous obtien-
drons des s&#233;quences hybrides, contenant des mots et des &#233;tiquettes morphosyntaxiques. Nous proposons
que les n mots les plus fr&#233;quents du corpus d&#8217;apprentissage ne soient pas &#233;tiquet&#233;s. De cette mani&#232;re, nous
capturerons des patrons d&#8217;utilisation des mots-outils plus robustes que des s&#233;quences de mots. La valeur
de n est d&#233;finie dans la Section 3.3.
</p>
<p>3 Dispositif exp&#233;rimental
</p>
<p>Nous allons &#233;valuer notre approche sur deux corpora : un corpus de vid&#233;os appartenant &#224; sept genres pour
lesquelles le contenu linguistique est obtenu avec un syst&#232;me de RAP, et un corpus de sous-titres de vid&#233;os
appartenant &#224; quatre genres.
</p>
<p>3.1 Corpus de vid&#233;os issu de RAP
</p>
<p>Ce corpus est compos&#233; de 1150 vid&#233;os appartenant &#224; sept genres : clip de musique, publicit&#233;, dessin anim&#233;,
documentaire, journal t&#233;l&#233;vis&#233;, sport et film. Elles ont &#233;t&#233; extraites d&#8217;une plateforme d&#8217;&#233;change de vid&#233;os
sur internet et elle durent de 2 &#224; 5 minutes. Le genre de ces vid&#233;os a &#233;t&#233; manuellement renseign&#233;. 870 de
ces vid&#233;os sont utilis&#233;es pour l&#8217;entra&#238;nement et 280 pour le test. Les sept genres sont &#233;galement repr&#233;sent&#233;s
dans le corpus (environ 125 vid&#233;os de chaque genre pour l&#8217;entra&#238;nement et 40 pour le test). Les vid&#233;os
sont en fran&#231;ais et nous ne disposons ni des transcriptions de r&#233;f&#233;rence ni des sous-titres.
</p>
<p>L&#8217;approche classique de classification en genre, reposant sur des param&#232;tres cepstraux et des classifieurs
de type MMG, obtient 52% de bonne classification sur ce corpus, ce qui correspond aux r&#233;sultats obtenus
par Roach &amp; Mason (2001) sur une t&#226;che similaire.
</p>
<p>3.2 Corpus de sous-titres
</p>
<p>Ce corpus n&#8217;est pas la cible principale de nos travaux, mais il va nous servir a de r&#233;f&#233;rence pour identifier
les sp&#233;cificit&#233;s de l&#8217;identification de genre sur les sorties de la RAP. Il est compos&#233; de sous-titres issus de
vid&#233;os appartenant &#224; quatre genres : dessin anim&#233;, documentaire, journal t&#233;l&#233;vis&#233; et film. Il contient 1960
documents dont 1400 servent pour l&#8217;entra&#238;nement et 560 pour le test. Les quatre genres sont &#233;galement
repr&#233;sent&#233;s dans le corpus. Les vid&#233;os auxquelles sont associ&#233;s ces sous-titres durent de 25 minutes &#224;
2h00.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>STANISLAS OGER, MICKAEL ROUVIER, GEORGES LINAR&#200;S
</p>
<p>3.3 Syst&#232;mes utilis&#233;s
</p>
<p>La transcription des vid&#233;os est r&#233;alis&#233;e avec le syst&#232;me de RAP grand vocabulaire du LIA, SPEERAL (No-
c&#233;ra et al., 2004). Ce syst&#232;me utilise un algorithme A* pour le d&#233;codage et des mod&#232;les de Markov cach&#233;s
pour la mod&#233;lisation acoustique. Le lexique contient 65k mots et le mod&#232;le de langage est un 3-gramme
estim&#233; sur 200M de mots du journal Le Monde et sur environ 1M de mots du corpus d&#8217;entra&#238;nement de
la campagne d&#8217;&#233;valuation ESTER. Ce syst&#232;me est destin&#233; &#224; transcrire des journaux radiodiffus&#233;s franco-
phones pour lesquels il obtient un taux d&#8217;erreur mot d&#8217;environ 20%. Il est fort probable que ce taux soit
tr&#232;s &#233;lev&#233; pour les autres genres (probablement entre 40% et 80%).
</p>
<p>L&#8217;&#233;tiquetage morphosyntaxique des corpus est obtenu automatiquement avec l&#8217;outil LIA_TAGG 1, qui
fournit un jeu d&#8217;&#233;tiquettes tr&#232;s d&#233;taill&#233;. Il permet notamment de distinguer le genres et le nombre sur les
pronoms et les verbes, ce qui va permettre de capturer facilement le point de vue narratif, qui est tr&#232;s
diff&#233;rent suivant le genre consid&#233;r&#233;.
</p>
<p>Concernant les mod&#232;les hybrides, nous avons fix&#233; &#224; 90 le nombre de mots les plus fr&#233;quents qui ne seront
pas remplac&#233;s par leurs &#233;tiquettes morphosyntaxiques.
</p>
<p>4 R&#233;sultats et discussion
</p>
<p>Tous les r&#233;sultats pr&#233;sent&#233;s dans cette section sont obtenus en faisant de la validation crois&#233;e sur la r&#233;union
du corpus d&#8217;entra&#238;nement et de test. Le corpus est d&#233;coup&#233; al&#233;atoirement en 50 parties. A chaque tour de
validation une partie est utilis&#233;e pour le test, 5 pour le d&#233;veloppement et 44 pour l&#8217;entra&#238;nement. Il faut
donc faire 50 cycles pour faire une &#233;valuation. Le classifieur bas-niveau utilis&#233; est de type Multi-Layer
Perceptron &#224; trois couches (Cybenko, 1989).
</p>
<p>Les m&#233;thodes que nous avons propos&#233;es poss&#232;dent quatre axes de variabilit&#233; : le type de termes (mots,
&#233;tiquettes ou hybride), la taille des n-grammes utilis&#233;s (l&#8217;ordre du mod&#232;le), la mani&#232;re dont les termes
sont s&#233;lectionn&#233;s (TF.IDF, GTF.IGF ou les plus fr&#233;quents, TF) et enfin le nombre de param&#232;tres utilis&#233;s.
Les combinaisons offrant les meilleurs r&#233;sultats sur le corpus de vid&#233;os issu de la RAP sont pr&#233;sent&#233;s dans
le tableau 1. Le syst&#232;me de r&#233;f&#233;rence sur cette t&#226;che est le mod&#232;le TF.IDF d&#8217;ordre 1(premi&#232;re colonne de
la derni&#232;re ligne du tableau 1). Le tableau 2 contient les r&#233;sultats des quatre meilleures combinaisons des
configurations pr&#233;c&#233;dentes.
</p>
<p>Les r&#233;sultats pr&#233;sent&#233;s dans le tableau 1 montrent que dans tous les cas, augmenter l&#8217;ordre des mod&#232;les
&#224; tendance &#224; d&#233;grader les r&#233;sultats, les mod&#232;les d&#8217;ordre 1 &#233;tant les meilleurs. De plus, les r&#233;sultats du
tableau 2 ne montrent quasiment pas de gain &#224; combiner des mod&#232;les d&#8217;ordre diff&#233;rent, ce qui indique que
l&#8217;information que contiennent les fr&#233;quences de s&#233;quences de mots ou d&#8217;&#233;tiquettes est moins importante
et surtout redondante avec celle contenue dans les fr&#233;quences de ces mots ou &#233;tiquettes.
</p>
<p>En regardant le nombre de param&#232;tres optimal pour chaque configuration, on observe que l&#8217;approche
TF.IDF en n&#233;cessite un nombre consid&#233;rable, ce qui indique que le classifieur a besoin des termes les moins
bien not&#233;s par cette m&#233;trique pour fonctionner. Il semble que TF.IDF ne soit pas adapt&#233; pour identifier les
termes pertinents pour la classification en genre &#224; partir de transcriptions issues de RAP.
</p>
<p>Concernant la nature des termes, les r&#233;sultats report&#233;s dans le tableau 1 montrent que l&#8217;utilisation de classes
morphosyntaxiques seules ou de s&#233;quences hybrides n&#8217;am&#233;liore globalement pas les performances de
classification par rapport &#224; l&#8217;utilisation des mots pour les approches TF et GTF.IGF, alors que les mod&#232;les
TF.IDF d&#8217;ordre sup&#233;rieur &#224; 1 profitent d&#8217;une am&#233;lioration (pour n = 2, 39.5% avec les mots et 59.8% avec
</p>
<p>1. http ://pageperso.lif.univ-mrs.fr/&#8764;frederic.bechet</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>CLASSIFICATION DU GENRE VID&#201;O BAS&#201;E SUR DES TRANSCRIPTIONS
</p>
<p>les s&#233;quences hybrides). Etant donn&#233; le grand nombre de param&#232;tres n&#233;cessaires pour l&#8217;approche TF.IDF,
la g&#233;n&#233;ralisation introduite par les classes morphosyntaxiques &#233;limine certainement une variabilit&#233; lexical
li&#233; &#224; la th&#233;matique des documents, au profit d&#8217;une meilleure mod&#233;lisation des expressions caract&#233;ristiques
du genre vid&#233;o, sous la forme de s&#233;quences d&#8217;&#233;tiquettes morphosyntaxiques r&#233;currentes. De plus, les
r&#233;sultats du tableau 2 ne montrent qu&#8217;une am&#233;lioration de 0.6% &#224; combiner des mod&#232;les qui utilisent des
m&#233;triques diff&#233;rentes pour s&#233;lectionner les termes. On peut en conclure encore une fois que l&#8217;information
captur&#233;e par les diff&#233;rentes m&#233;triques est globalement redondante.
</p>
<p>On remarque que la m&#233;thode GTF.IGF repr&#233;sente une alternative &#224; la m&#233;thode classique TF.IDF puis-
qu&#8217;elle offre des performances toujours meilleurs, sauf avec des unigrames de mots o&#249; l&#8217;on constate une
l&#233;g&#232;re perte (71.7% contre 71.9%), mais permet une r&#233;duction consid&#233;rable de l&#8217;espace de repr&#233;sentation.
</p>
<p>Les r&#233;sultats pr&#233;sent&#233;s dans le tableau 1 montrent que dans le cadre de notre t&#226;che, le meilleur moyen de
s&#233;lectionner les param&#232;tres pertinents reste de prendre les mots les plus fr&#233;quents du corpus. On constate
une am&#233;lioration d&#8217;environ 2% absolus des performances par rapport au syst&#232;me de r&#233;f&#233;rence TF.IDF avec
uniquement les 90 mots les plus fr&#233;quents, tout en r&#233;duisant l&#8217;espace de repr&#233;sentation de 99.7% (de 34k
&#224; 90 mots).
</p>
<p>TABLE 1 &#8211; Taux de bonne classification (%c) et nombre de param&#232;tres optimaux (#p) obtenus sur le corpus
de vid&#233;os issu de la RAP en fonction de la m&#233;trique utilis&#233;e (TF, GTF.IGF ou TF.IDF), du type de terme
(mots, &#233;tiquettes morphosyntaxiques ou hybride) et de l&#8217;ordre du mod&#232;le (n).
</p>
<p>mots &#233;tiquettes morpho. hybride
n = 1 n = 2 n = 3 n = 1 n = 2 n = 3 n = 1 n = 2 n = 3
</p>
<p>TF
%c 73.9 63.6 55.0 65.6 61.1 53.8 70.8 61.5 52.9
#p 90 300 700 50 200 150 100 400 400
</p>
<p>GTF.IGF
%c 71.7 60.2 48.9 64.3 62.8 54.3 70.4 60.1 43.3
#p 200 60 500 40 500 150 60 700 200
</p>
<p>TF.IDF
%c 71.9 39.5 29.1 61.2 55.9 52.0 66.1 59.8 42.7
#p 34k 50k 50k 70 10k 50k 200 50k 50k
</p>
<p>TABLE 2 &#8211; Taux de bonne classification (%c) obtenus sur le corpus de vid&#233;os issu de la RAP pour les
quatre combinaisons de mod&#232;les qui offrent les meilleures performances.
</p>
<p>combinaison %c
mots TF n=1 + &#233;tiquettes morpho. GTF.IGF n=2 74.5
mots TF n=1 et n=3 + &#233;tiquettes morpho. TF n=1 + etiquettes morpho. GTF.IGF n=2 74.1
mots TF n=1 + &#233;tiquettes morpho. TF n=1 73.8
mots TF n=1 + &#233;tiquettes morpho. TF n=1 + etiquettes morpho. GTF.IGF n=2,3 73.4
</p>
<p>Afin de d&#233;terminer si ces r&#233;sultats sont li&#233;s &#224; la nature du corpus issu de la RAP, nous avons men&#233; les
m&#234;mes exp&#233;riences sur le corpus de sous-titres qui contient les transcriptions exactes de vid&#233;os de quatre
des sept genres. Les r&#233;sultats sont pr&#233;sent&#233;s dans le tableau 3 (colonnes intitul&#233;es ST), aux c&#244;t&#233;s de ceux
obtenus sur le corpus issu de la RAP (colonnes intitul&#233;es RAP) en ne prenant en compte que les quatre
genres consid&#233;r&#233;s. Ces r&#233;sultats ne peuvent pas &#234;tre directement compar&#233;s &#233;tant donn&#233; que les corpus uti-
lis&#233;s sont diff&#233;rents, mais peuvent fournir une information indicative. On constate que la m&#233;thode TF.IDF
est la meilleure approche sur les sous-titres suivie de pr&#232;s par TF. Sur le corpus issu de RAP ces r&#233;sultats
sont invers&#233;s. On pourrait penser que TF est plus robuste aux erreurs de RAP que TF.IDF. Le r&#233;sultat le</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>STANISLAS OGER, MICKAEL ROUVIER, GEORGES LINAR&#200;S
</p>
<p>plus important est que la m&#233;thode TF fonctionne tr&#232;s bien sur le corpus de sous-titres, ce qui indique que
les fr&#233;quences d&#8217;utilisation des mots-outils de la langue contiennent une information caract&#233;ristique du
genre vid&#233;o et qui n&#8217;est pas li&#233;e au syst&#232;me de RAP, mais bien un ph&#233;nom&#232;ne linguistique.
</p>
<p>TABLE 3 &#8211; Taux de bonne classification (%c) et nombre de param&#232;tres optimaux (#p) obtenus sur le
corpus de vid&#233;os issu de la RAP (RAP) et sur le corpus de sous-titres (ST) en fonction de la m&#233;trique
utilis&#233;e (TF, GTF.IGF ou TF.IDF) et du type de terme (mots, &#233;tiquettes morphosyntaxiques ou hybride)
avec des mod&#232;les d&#8217;ordre 1.
</p>
<p>mots &#233;tiquettes morpho. hybride
ST RAP ST RAP ST RAP
</p>
<p>TF
%c 79.0 89.7 70.0 82.0 73.0 85.5
#p 700 80 80 40 200 70
</p>
<p>GTF.IGF
%c 80.8 87.0 69.7 82.1 73.1 87.1
#p 800 80 20 40 90 80
</p>
<p>TF.IDF
%c 83.1 88.9 77.7 81.7 79.1 83.0
#p 50k 34k 100 100 200 200
</p>
<p>Ces r&#233;sultats valident notre hypoth&#232;se initiale : les fr&#233;quences des mots-outils contiennent une information
permettant de caract&#233;riser le genre vid&#233;o. De plus, le mod&#232;le TF propos&#233; d&#8217;ordre 1 permet un gain de bonne
classification absolu d&#8217;environ 2% en comparaison du mod&#232;le de r&#233;f&#233;rence TF.IDF sur le corpus de RAP,
alors que l&#8217;espace de repr&#233;sentation est r&#233;duit de 99.7%. La section suivante pr&#233;sente les r&#233;sultats obtenus
en combinant le mod&#232;le de classification que nous proposons avec d&#8217;autres utilisant des param&#232;tres audio
de plus bas niveau.
</p>
<p>5 Compl&#233;mentarit&#233; avec les param&#232;tres acoustiques
</p>
<p>Les param&#232;tres linguistiques propos&#233;s dans cet article sont bas&#233;s sur le contenu parl&#233; des vid&#233;os. Dans un
pr&#233;c&#233;dent article nous avons propos&#233; l&#8217;utilisation de param&#232;tres qui ne sont pas directement d&#233;pendants
du contenu linguistique (Rouvier et al., 2009) et qui permettent d&#8217;attendre un taux de bonne classification
de 94% sur ce corpus. Dans cette section nous allons &#233;valuer &#224; quel point ces jeux de param&#232;tres sont
compl&#233;mentaires.
</p>
<p>La combinaison de descripteurs est r&#233;alis&#233;e en utilisant un classifieur de type MVS avec pour chaque
document un vecteur d&#8217;entr&#233;e contenant les sorties des classifieurs bas-niveaux de chaque descripteur et
les performances sont mesur&#233;es sur le corpus de test issu de la RAP. Le mod&#232;le de contenu linguistique
est TF d&#8217;ordre 1, donc les 90 mots les plus fr&#233;quents du corpus d&#8217;entra&#238;nement. Les r&#233;sultats de ce mod&#232;le
seul sont pr&#233;sent&#233;s dans la colonne not&#233;s L du tableau 4.
</p>
<p>5.1 Param&#232;tres cepstraux &#224; court-terme
</p>
<p>L&#8217;espace acoustique repr&#233;sent&#233; par des param&#232;tres cepstraux est le descripteur le plus couremment utilis&#233;
dans la classification de genre vid&#233;. Dans Rouvier et al. (2009), nous avons montr&#233; que 12 coefficients
de pr&#233;diction lin&#233;aire perceptuelle (PLP) et l&#8217;&#233;nergie du signal avec leurs d&#233;riv&#233;es premi&#232;re et seconde
permettent de bien repr&#233;senter cet espace et offrent de bonnes performances pour la classification en
genre. La variabilit&#233; intra-genre est r&#233;duite gr&#226;ce &#224; l&#8217;utilisation de l&#8217;analyse factorielle. Les performances
de ce descripteur de l&#8217;espace acoustique sont pr&#233;sent&#233;es dans la colonne intitul&#233;e EA du tableau 4.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>CLASSIFICATION DU GENRE VID&#201;O BAS&#201;E SUR DES TRANSCRIPTIONS
</p>
<p>5.2 Interactivit&#233;
</p>
<p>Le nombre d&#8217;intervenants et la mani&#232;re dont ils interagissent sont souvent d&#233;pendants du genre vid&#233;o
consid&#233;r&#233;. Par exemple dans les journaux t&#233;l&#233;vis&#233;s il y a g&#233;n&#233;ralement un pr&#233;sentateur qui occupe la
majorit&#233; du temps de parole, contrairement &#224; la plupart des dessins anim&#233;s et des films. Dans Rouvier
et al. (2009), nous avons propos&#233; d&#8217;extraire des vid&#233;os un descripteur de la mani&#232;re dont les intervenants
communiquent. Pour chaque document, un syst&#232;me de suivi de locuteur 2 est utilis&#233; pour estimer le nombre
d&#8217;intervenants ainsi que les tours de parole. Le vecteur de param&#232;tres contient trois &#233;l&#233;ments : la densit&#233; des
tours de parole, le nombre de locuteurs et le temps de parole de l&#8217;intervenant principal. Les performances
de ce descripteur d&#8217;interactivit&#233; sont pr&#233;sent&#233;es dans la colonne not&#233;e I du tableau 4.
</p>
<p>5.3 Qualit&#233; de la parole
</p>
<p>Cette m&#233;trique consiste &#224; mesurer l&#8217;ad&#233;quation du syst&#232;me de RAP avec les documents &#224; transcrire. Par
exemple, les journaux t&#233;l&#233;vis&#233;s sont en g&#233;n&#233;ral bien reconnus par le syst&#232;me de RAP que nous utilisons
puisqu&#8217;il est destin&#233; &#224; cet usage. Nous proposons d&#8217;extraire du syst&#232;me plusieurs informations qui forment
le vecteur de param&#232;tres fourni au classifieur : la probabilit&#233; a posteriori des mots de l&#8217;hypoth&#232;se retenue,
la probabilit&#233; a posteriori globale de l&#8217;hypoth&#232;se retenue et l&#8217;entropie phon&#233;tique. Les performances de
ce descripteur sont pr&#233;sent&#233;es dans la colonne not&#233;e Q du tableau 4.
</p>
<p>5.4 Combinaisons
</p>
<p>Les r&#233;sultats de la combinaison des descripteurs pr&#233;c&#233;dents avec le descripteur linguistique que nous avons
propos&#233; sont pr&#233;sent&#233;s dans le tableau 4 (colonnes EA+L, I+L et Q+L). On constate que dans tous les cas,
les performances de la combinaison sont meilleures que celles du meilleur des descripteurs individuel-
lement. On peut en d&#233;duire que le descripteur linguistique contient une information compl&#233;mentaire par
rapport &#224; l&#8217;information apport&#233;e par les autres descripteurs.
</p>
<p>La colonne not&#233;e EA+I+Q du tableau 4 contient les r&#233;sultats de la combinaison de tous ces descripteurs
sans le descripteur linguistique propos&#233;. On observe un gain de performances de 3% absolu par rapport au
meilleur descripteur et un gain de 2% absolu par rapport &#224; la meilleure des combinaisons pr&#233;c&#233;dentes. La
colonne intitul&#233;e EA+I+Q+L contient les r&#233;sultats de la combinaison pr&#233;c&#233;dente en ajoutant le descripteur
linguistique. Ce descripteur apporte un gain absolu de 1%, soit une diminution relative du taux d&#8217;erreur
de classification de 16% (de 94% &#224; 95%). Ces r&#233;sultats montrent que le nouveau descripteur linguistique
propos&#233; et les descripteurs acoustiques sont pertinents et surtout compl&#233;mentaires pour la classification en
genre vid&#233;o.
</p>
<p>TABLE 4 &#8211; Taux de bonne classification [%] obtenus avec les descripteurs propos&#233;s et leurs combinaisons.
</p>
<p>L EA I Q EA+L I+L Q+L EA+I+Q EA+I+Q+L
%c 74 91 56 53 93 81 81 94 95
</p>
<p>2. Nous avons utilis&#233; le syst&#232;me de suivi de locuteur du Laboratoire Informatique de l&#8217;Universit&#233; du Maine, disponible &#224;
l&#8217;adresse http ://liumtools.univ-lemans.fr</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>STANISLAS OGER, MICKAEL ROUVIER, GEORGES LINAR&#200;S
</p>
<p>6 Conclusion
</p>
<p>Dans cet article nous avons propos&#233; une nouvelle approche pour tirer parti du contenu linguistique de
vid&#233;os dans le contexte de leur classification en genre lorsqu&#8217;aucune m&#233;ta-donn&#233;e n&#8217;est disponible. Le
contenu linguistique est obtenu par transcription automatique du canal audio. Diff&#233;rentes m&#233;thodes d&#8217;ana-
lyse des transcriptions ainsi obtenues sont compar&#233;es.
</p>
<p>Les r&#233;sultats montrent que, contrairement &#224; l&#8217;approche classique en cat&#233;gorisation de texte qui consiste
&#224; se concentrer sur les mots porteurs de sens, l&#8217;analyse des fr&#233;quences des mots outils permet une ca-
ract&#233;risation du style &#233;ditorial qui est robuste aux erreurs de RAP. La m&#234;me approche, appliqu&#233;e &#224; un
corpus de sous-titres montre les bonnes performances de ces descripteurs qui restent, sur du texte correct,
l&#233;g&#232;rement moins bonnes que la m&#233;thode classique s&#8217;appuyant sur TF.IDF, mais permettent de r&#233;duire
consid&#233;rablement l&#8217;espace de repr&#233;sentation des documents.
</p>
<p>Enfin, les param&#232;tres linguistiques propos&#233;s sont compl&#233;mentaires avec les param&#232;tres acoustiques d&#233;j&#224;
utilis&#233;s dans des syst&#232;mes ant&#233;rieurs. Ils apportent une information que ces derniers ne peuvent probable-
ment pas capturer : en combinant les niveaux acoustiques et linguistiques, on obtient un diminution du
taux d&#8217;erreur de classification de 16% relatif par rapport &#224; l&#8217;acoustique seul. Finalement, cette combinai-
son permet d&#8217;atteindre un taux de bonne classification de 95%, ce qui correspond aux meilleurs r&#233;sultats
publi&#233;s sur ce type de t&#226;ches avec des syst&#232;mes m&#233;langeant descripteurs audio et vid&#233;o.
</p>
<p>R&#233;f&#233;rences
BIBER D. (1988). Variation across speech and writing. Cambridge University Press.
BREZEALE D. &amp; COOK D. (2006). Using closed captions and visual features to classify movies by
genre. In Proceedings of Multimedia Data Mining / Knowledge Discovery and Data Mining.
BREZEALE D. &amp; COOK D. J. (2008). Automatic video classification : A survey of the literature. IEEE
Transactions on Systems, Man, and Cybernetics, 38, 416&#8211;430.
CYBENKO G. (1989). Approximation by superpositions of a sigmoidal function. Mathematics of Control,
Signals, and Systems (MCSS), 2(4), 303&#8211;314.
KARLGREN J. &amp; CUTTING D. (1994). Recognizing text genres with simple metrics using discriminant
analysis. In Proceedings of the international conference on computational linguistics, p. 1071&#8211;1075.
LIN W. &amp; HAUPTMANN A. (2002). News video classification using svm-based multimodal classifiers
and combination strategies. In Proceedings of the International Conference on Multimedia, p. 323&#8211;326.
NOC&#201;RA P., FREDOUILLE C., LINAR&#200;S G., MATROUF D., MEIGNIER S., BONASTRE J., MASSONI&#201;
D. &amp; B&#201;CHET F. (2004). The LIA&#8217;s french broadcast news transcription system. In SWIM : Lectures by
Masters in Speech Processing.
ROACH M. &amp; MASON J. (2001). Classification of video genre using audio. In Proceedings of EUROS-
PEECH, p. 2693&#8211;2696.
ROUVIER M., LINAR&#200;S G. &amp; MATROUF D. (2009). Robust audio-based classification of video genre.
In Proceedings of INTERSPEECH, p. 1159&#8211;1162.
STAMATATOS E., FAKOTAKIS N. &amp; KOKKINAKIS G. (2000). Text genre detection using common word
frequencies. In Proceedings of the international conference on computational linguistics, p. 808&#8211;814.
WANG P., CAI R. &amp; YANG S. (2003). A hybrid approach to news video classification multimodal
features. In Proceedings of the International Conference on Information, Communications and Signal
Processing (ICICS), volume 2, p. 787&#8211;791.</p>

</div></div>
</body></html>