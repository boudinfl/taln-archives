TALN 2010, Montreal, 19-23 juillet 2010

Classiﬁcation du genre vidéo reposant sur des transcriptions
automatiques

Stanislas Oger, Mickael Rouvier, Georges Linares *
LIA, Université d’Avignon, France
{stanislas.oger, mickael.rouVier, georges.linares} @uniV-aVignon.fr

Résumé. Dans cet article nous proposons une nouvelle méthode pour l’identiﬁcation du genre vidéo
qui repose sur une analyse de leur contenu linguistique. Cette approche consiste en l’analyse des mots
apparaissant dans les transcriptions des pistes audio des vidéos, obtenues a l’aide d’un systeme de re-
connaissance automatique de la parole. Les experiences sont réalisées sur un corpus compose de dessins
animés, de ﬁlms, de journaux télévisés, de publicités, de documentaires, d’éInissions de sport et de clips
de musique. L’ approche proposée permet d’obtenir un taux de bonne classiﬁcation de 74% sur cette tache.
En combinant cette approche avec des méthodes reposant sur des parametres acoustiques bas-niveau, nous
obtenons un taux de bonne classiﬁcation de 95%.

Abstract. In this paper, we present a new method for video genre identiﬁcation based on the linguis-
tic content analysis. This approach relies on the analysis of the words in the video transcriptions provided
by an automatic speech recognition system. Experiments are conducted on a corpus composed of cartoons,
movies, news, commercials, documentary, sport and music. On this 7-genre identiﬁcation task, the propo-
sed transcription-based method obtains up to 74% of correct identiﬁcation. Finally, this rate is increased
to 95% by combining the proposed linguistic-level features with low-level acoustic features.

M0tS-CléS I classiﬁcation de genre vidéo, traitement audio de la vidéo, extraction de parametres
linguistiques.

Keywords: video genre classiﬁcation, audio-based video processing, linguistic feature extraction.

*. Ces travaux ont été en partie ﬁnancés par l’Agence Nationale de la Recherche (ANR), par Pintennédiaire du proj et RPM2
(ANR—07—AM—008).

STANISLAS OGER, MICKAEL ROUVIER, GEORGES LINARES

1 Introduction

L’indexation video est un domaine porteur dans le contexte actuel o1‘1 l’on voit se developper les chaines
de televisions numeriques et les collections de videos sur intemet. L’un des criteres les plus utiles pour
la recherche de videos dans une base de donnees est sans doute le genre (dessin anime, documentaire,
etc.). Son identiﬁcation automatique a suscite recemment beaucoup d’interet de la part de la communaute
scientiﬁque. La grande maj orite des travaux publies repose sur l’eXtraction de parametres videos, tel que la
couleur, la luminosite, ou des informations sur les changements de prise de vue (Brezeale & Cook, 2008).
Ces descripteurs bas-niveaux sont en general combines en utilisant des classiﬁeurs statistiques, tel que les
machines a vecteurs de support (MVS) ou les modeles de melanges gaussiens (MMG). L’ autre approche
classique consiste a extraire des parametres cepstraux de la piste audio des videos (Roach & Mason, 2001).
C’est ce que nous avons propose dans Rouvier et al. (2009), en plus d’autres parametres acoustiques haut
niveau.

L’identiﬁcation automatique du genre est une tache de categorisation de texte etudiee depuis longtemps
(Karlgren & Cutting, 1994). Ce qui caracterise le genre dans un texte est principalement le style editorial.
Pour les videos, le contenu linguistique est de nature parlee mais contient la meme information pour
caracteriser le genre que le texte (Biber, 1988). Malgre cela, tres peu de travaux proposent d’utiliser cette
modalite pour la classiﬁcation en genre video. La raison principale est que le contenu linguistique des
videos est rarement disponible et que l’obtenir revient souvent a faire de la transcription manuelle, qui est
tres onereuse. On trouve cependant quelques etudes qui utilisent les sous-titres associes aux videos (Lin &
Hauptmann, 2002; Brezeale & Cook, 2006), mais ces approches sont inutilisables lorsque ceux-ci ne sont
pas disponibles, par exemple sur les plateformes intemet d’echange de contenus videos.

Une maniere peu coﬁteuse d’obtenir la transcription des videos est d’uti1iser un systeme de reconnaissance
automatique de la parole (RAP). La plupart des travaux proposant cette approche concernent la classiﬁ-
cation thematique et generalement dans un domaine ou le systeme de RAP est performant, ce qui permet
d’aborder le probleme comme s’ils s’agissait de donnees textuelles non erronees. Par exemple, Wang et al.
(2003) proposent d’effectuer la classiﬁcation automatique de nouvelles issues de joumaux radiodiffuses a
partir de transcriptions automatiques avec un taux d’erreur mot de l’ordre de 10%. L’investissement neces-
saire pour obtenir de telles performances de RAP sur des genres videos varies, tel que les dessins animes,
documentaires, etc., serait tres coﬁteux.

Dans cet article nous proposons une methode de classiﬁcation automatique du genre video reposant sur
la caracterisation du style editorial dans des transcriptions issues de RAP avec un taux d’erreur variable.
Nous proposons plusieurs metriques pour selectionner les termes qui seront foumis au classiﬁeur. Le
modele que nous proposons s’inscrit dans l’architecture de classiﬁcation multimodale a deux niveaux
proposee dans Rouvier et al. (2009) : (I) des descripteurs de contenu homogenes (acoustique, linguistique,
etc.) qui peuvent etre eux-memes des classiﬁeurs bas-niveau qui pre-classiﬁent le genre des videos et (II)
un classiﬁeur global qui prend la decision ﬁnale en fonction des descripteurs et des sorties des classiﬁeurs
du niveau inferieur.

Dans la premiere section, nous presentons en details notre approche pour extraire des parametres linguis-
tiques des transcriptions automatiques. Ensuite, nous analysons les resultats experimentaux obtenus avec
l’approche linguistique proposee. Finalement, nous presentons et discutons de la complementarite de ces
parametres avec ceux que l’on trouve dans l’etat de l’art.

CLASSIFICATION DU GENRE VIDEO BASEE SUR DES TRANSCRIPTIONS

2 Identiﬁcation du genre basée sur la linguistique

L’analyse du contenu linguistique des vidéos que nous proposons repose sur l’utilisation d’un systeme
de RAP pour obtenir les transcriptions des vidéos. Ce systeme utilise un lexique fermé et un modele de
langage qui est estimé sur un corpus textuel de grande taille. Entrainer un tel modele pour chaque genre
vidéo n’est pas réalisable car nous ne disposons pas du volume de données textuelles nécessaires. Nous
proposons donc d’utiliser un modele de langage standard avec un lexique peu adapté a certains genres, ce
qui causera la plupart du temps un fort taux d’erreur dans les transcriptions.

Dans la description des méthodes et formules qui suivront, le mot terme est employé dans un sens large,
proche du sens mathématique, et peut désigner un n-gramme de mots ou d’étiquettes morphosyntaxiques,
avec n variant de 1 a 3.

Le principe du sac de termes est utilisé pour la modélisation des documents. Selon ce modele, chaque
dimension de l’espace des parametres représente un terme et chaque document est représenté par un vec-
teur de fréquences de termes dans cet espace. Dans le cas ou le terme est un unigramme de mots, alors ce
modele est un sac de mots. La taille du n-gramme utilisé comme terme est appelé l’ordre du modele.

2.1 Approche classique pour le genre vidéo

Pour les problemes de categorisation automatique de texte, les approches généralement proposées reposent
sur l’eXtraction de mots porteurs de sens des documents a classer. Pour la classiﬁcation du genre vidéo,
les études qui s’appuyent sur la modalité textuelle utilisent en général cette approche. Soit les mots-outils
de la langue sont ﬁltrés, soit une métrique de type Term Frequency-Inverse Document Frequency (TF.IDF)
est utilisée pour ne sélectionner que les mots porteurs de sens des documents (Brezeale & Cook, 2006;
Lin & Hauptmann, 2002). Cette approche sera notre modele de référence dans cet article.

Pour un terme t et un document d, TF.IDF est déﬁni comme suit :
TF.IDF(t, d) = TF(t, d) X IDF(t) (1)

avec TF(t, d) la fréquence normalisée du terme t dans le document d et IDF(t) une métrique représentant
le pouvoir discriminant du terme t.

TF(t, d) est déﬁni comme suit : n d
TF(t, d) = A (2)
Zked "led

avec nhd la fréquence du terme t dans le document d.
IDF(t) est déﬁni comme suit :

IDF(t) — lo Nd (3)
_ g DF(t)

avec N d le nombre total de documents dans le corpus et DF(t) le nombre de documents du corpus qui
contiennent le terme 75. Plus la valeur de TF.IDF d’un mot est élevée, plus le mot considéré est représentatif
du document et porteur de la thématique qu’il aborde.

Pour chaque genre, le vecteur de parametres contient la liste de termes résultant de la fusion des n termes
ayant les meilleurs TF.IDF de chaque document du genre. Un seul exemplaire de chaque terme est conservé
lors de la fusion. Ces vecteurs sont ensuite regroupés dans un supervecteur qui est fourni au classiﬁeur bas-
niveau.

STANISLAS OGER, MICKAEL ROUVIER, GEORGES LINARES
2.2 Termes les plus discriminants pour le genre

L’ approche classique basée sur TF.IDF permet de donner plus de poids aux termes discriminants pour les
documents, qui sont souvent porteurs des thématiques des vidéos. Pourtant, le genre vidéo fait référence
au style éditorial et n’est pas systématiquement corrélé avec les sujets abordés et, plus généralement, avec
le contenu sémantique.

Nous proposons ici de concevoir une métrique permettant d’identiﬁer les termes discriminants pour le
genre et non pour le document. Pour cela, nous proposons d’adapter le calcul de TF.IDF en Genre Term
Frequency-Inverse Genre Frequency (GTF.IGF), déﬁni comme suit pour un terme t et un genre g :

GTF.IGF(t, g) = GTF (t, g) X IGF(t) (4)
avec GTF(t, g) la somme normalisée des fréquences normalisées du terme t dans les documents du genre

g et IGF(t) une métrique représentant le pouvoir discriminant du terme t.
GTF(t, g) est déﬁnit comme suit :

GTF(t, 9) :  w’w (5)

|9|

avec |g| le nombre de documents dans le genre g et TF(t, g) la fréquence normalisée du terme t dans le
genre g, déﬁni a l’équation 2.

IGF(t) est déﬁni comme suit :

IGF(t) = log ( G]1\:[(gt)> (6)

avec N 9 le nombre total de genres dans le corpus et GF(t) le nombre de genres du corpus qui contiennent
le terme t. Plus la valeur de GTF.IGF d’un mot est élevée, plus le mot considéré est discriminant pour
l’identiﬁcation du genre.

Pour chaque genre, un vecteur est construit avec les n termes ayant le meilleurs GTF.IGF et ces vecteurs
sont ensuite concaténés dans un supervecteur, de taille n X N 9 , qui est foumi au classiﬁeur bas-niveau.

2.3 Termes les plus fréquentes

Les méthodes précédentes permettent d’identiﬁer des termes discriminants pour un document ou un genre.
Ces termes sont souvent porteurs de sens et plutot rares en général, ils auront donc une probabilité élevée
d’étre victimes du décalage entre le lexique du systeme de RAP et celui du document. A l’opposé de
ces approches, Stamatatos et al. (2000) ont démontré l’efﬁcacité de l’utilisation des fréquences des mots-
outils pour identiﬁer le genre écrit. Nous pensons que ces parametres peuvent tout aussi bien étre porteurs
d’information pour détecter le genre vidéo. Contrairement a l’approche TF-IDF, celle-ci est indépendante
des thématiques des documents, et est donc plus robuste pour classiﬁer des genres comme les news, les
documentaires ou les cartoons, qui abordent des thématiques tres variées. De plus, les mots outils sont
caractérisés par leurs fréquences tres élevées et sont donc plus robustes aux erreurs lexicales du systeme
de RAP.

Les n termes les plus fréquents de l’ensemble des transcriptions automatiques des documents du corpus
d’entrainement servent ainsi de parametres au classiﬁeur bas-niveau.

CLASSIFICATION DU GENRE VIDEO BASEE SUR DES TRANSCRIPTIONS

2.4 Généralisation des constructions caractéristiques du genre

Le but des précédentes approches est de capturer des séquences de un ou plusieurs mots qui soient ca-
ractéristiques du genre vidéo. Nous proposons de les généraliser en capturant des patrons de construction
de phrases. Pour cela, nous pouvons utiliser des séquences d’étiquettes morphosyntaxiques au lieu des
séquences de mots comme termes dans les méthodes précédentes.

Une limite a l’utilisation d’étiquettes morphosyntaxiques de cette maniere est que les mots sont systema-
tiquement généralisés. 11 y a peut-étre certains mots au sein d’une classe morphosyntaxique pour lesquels
il faudrait conserver la distinction. Nous proposons donc de ne pas étiqueter certains mots et d’utiliser les
techniques précédentes pour identiﬁer les séquences intéressantes pour la classiﬁcation. Ainsi nous obtien-
drons des séquences hybrides, contenant des mots et des étiquettes morphosyntaxiques. Nous proposons
que les 12 mots les plus fréquents du corpus d’apprentissage ne soient pas étiquetés. De cette maniere, nous
capturerons des patrons d’utilisation des mots-outils plus robustes que des séquences de mots. La valeur
de n est déﬁnie dans la Section 3.3.

3 Dispositif expérimental

Nous allons évaluer notre approche sur deux corpora : un corpus de vidéos appartenant a sept genres pour
lesquelles le contenu linguistique est obtenu avec un systeme de RAP, et un corpus de sous-titres de vidéos
appartenant a quatre genres.

3.1 Corpus de vidéos issu de RAP

Ce corpus est composé de 1150 videos appartenant a sept genres : clip de musique, publicite’, dessin anime’,
documentaire, joumal télévisé, sport et ﬁlm. Elles ont été extraites d’une plateforme d’échange de vidéos
sur internet et elle durent de 2 a 5 minutes. Le genre de ces videos a été manuellement renseigné. 870 de
ces vidéos sont utilisées pour l’entrainement et 280 pour le test. Les sept genres sont également représentés
dans le corpus (environ 125 videos de chaque genre pour l’entrainement et 40 pour le test). Les vidéos
sont en francais et nous ne disposons ni des transcriptions de référence ni des sous-titres.

L’approche classique de classiﬁcation en genre, reposant sur des parametres cepstraux et des classiﬁeurs
de type MMG, obtient 52% de bonne classiﬁcation sur ce corpus, ce qui correspond aux résultats obtenus
par Roach & Mason (2001) sur une tache similaire.

3.2 Corpus de sous-titres

Ce corpus n’est pas la cible principale de nos travaux, mais il Va nous servir a de référence pour identiﬁer
les spéciﬁcités de l’identiﬁcation de genre sur les sorties de la RAP. Il est composé de sous-titres issus de
vidéos appartenant a quatre genres : dessin anime’, documentaire, journal télévisé et ﬁlm. Il contient 1960
documents dont 1400 servent pour l’entrainement et 560 pour le test. Les quatre genres sont également
représentés dans le corpus. Les vidéos auxquelles sont associés ces sous-titres durent de 25 minutes a
2h00.

STANISLAS OGER, MICKAEL ROUVIER, GEORGES LINARES

3.3 Systémes utilisés

La transcription des vidéos est réalisée avec le systeme de RAP grand vocabulaire du LIA, SPEERAL (No-
céra et al. , 2004). Ce systeme utilise un algorithme A* pour le décodage et des modeles de Markov cachés
pour la modélisation acoustique. Le lexique contient 65k mots et le modele de langage est un 3-gramme
estimé sur 200M de mots du journal Le Monde et sur environ 1M de mots du corpus d’entrainement de
la campagne d’évaluation ESTER. Ce systeme est destiné a transcrire des journaux radiodiffusés franco-
phones pour lesquels il obtient un taux d’erreur mot d’environ 20%. Il est fort probable que ce taux soit
tres élevé pour les autres genres (probablement entre 40% et 80%).

L’étiquetage morphosyntaxique des corpus est obtenu automatiquement avec l’outil LIA_TAGG1, qui
fournit un jeu d’étiquettes tres détaillé. I1 permet notamment de distinguer le genres et le nombre sur les
pronoms et les verbes, ce qui va permettre de capturer facilement le point de vue narratif, qui est tres
différent suivant le genre considéré.

Concernant les modeles hybrides, nous avons ﬁxé a 90 1e nombre de mots les plus fréquents qui ne seront
pas remplacés par leurs étiquettes morphosyntaxiques.

4 Résultats et discussion

Tous les résultats présentés dans cette section sont obtenus en faisant de la validation croisée sur la réunion
du corpus d’entrainement et de test. Le corpus est découpé aléatoirement en 50 parties. A chaque tour de
validation une partie est utilisée pour le test, 5 pour le développement et 44 pour l’entrainement. I1 faut
donc faire 50 cycles pour faire une évaluation. Le classiﬁeur bas-niveau utilisé est de type Multi-Layer
Perceptron a trois couches (Cybenko, 1989).

Les méthodes que nous avons proposées possedent quatre axes de variabilité : le type de termes (mots,
étiquettes ou hybride), la taille des n-grammes utilisés (l’ordre du modele), la maniere dont les termes
sont sélectionnés (TF.IDF, GTF.IGF ou les plus fréquents, TF) et enﬁn le nombre de parametres utilisés.
Les combinaisons offrant les meilleurs résultats sur le corpus de vidéos issu de la RAP sont présentés dans
le tableau 1. Le systeme de référence sur cette tache est le modele TF.IDF d’ordre 1(premiere colonne de
la derniere ligne du tableau 1). Le tableau 2 contient les résultats des quatre meilleures combinaisons des
conﬁgurations précédentes.

Les résultats présentés dans le tableau 1 montrent que dans tous les cas, augmenter l’ordre des modeles
a tendance a dégrader les résultats, les modeles d’ordre 1 étant les meilleurs. De plus, les résultats du
tableau 2 ne montrent quasiment pas de gain a combiner des modeles d’ordre différent, ce qui indique que
l’information que contiennent les fréquences de séquences de mots ou d’étiquettes est moins importante
et surtout redondante avec celle contenue dans les fréquences de ces mots ou étiquettes.

En regardant le nombre de parametres optimal pour chaque conﬁguration, on observe que l’approche
TF.IDF en nécessite un nombre considérable, ce qui indique que le classiﬁeur a besoin des termes les moins
bien notés par cette métrique pour fonctionner. I1 semble que TF.IDF ne soit pas adapté pour identiﬁer les
termes pertinents pour la classiﬁcation en genre a partir de transcriptions issues de RAP.

Concernant la nature des termes, les résultats reportés dans le tableau 1 montrent que l’utilisation de classes
morphosyntaxiques seules ou de séquences hybrides n’améliore globalement pas les performances de
classiﬁcation par rapport a l’utilisation des mots pour les approches TF et GTF.IGF, alors que les modeles
TF.IDF d’ordre supérieur a 1 proﬁtent d’une amélioration (pour n = 2, 39.5% avec les mots et 59.8% avec

1. http ://pageperso.1if.univ—mrs.fr/~frederic.bechet

CLASSIFICATION DU GENRE VIDEO BASEE SUR DES TRANSCRIPTIONS

les séquences hybrides). Etant donné le grand nombre de parametres nécessaires pour l’approche TF.IDF,
la généralisation introduite par les classes morphosyntaxiques élimine certainement une variabilité lexical
lié a la thématique des documents, au proﬁt d’une meilleure modélisation des expressions caractéristiques
du genre vidéo, sous la forme de séquences d’étiquettes morphosyntaxiques récurrentes. De plus, les
résultats du tableau 2 ne montrent qu’une amélioration de 0.6% a combiner des modeles qui utilisent des
métriques différentes pour sélectionner les termes. On peut en conclure encore une fois que l’information
capturée par les différentes métriques est globalement redondante.

On remarque que la méthode GTF.IGF représente une alternative a la méthode classique TF.IDF puis-
qu’elle offre des performances toujours meilleurs, sauf avec des unigrames de mots ou l’on constate une
légere perte (71.7% contre 71.9%), mais permet une réduction considérable de l’espace de représentation.

Les résultats présentés dans le tableau 1 montrent que dans le cadre de notre tache, le meilleur moyen de
sélectionner les parametres pertinents reste de prendre les mots les plus fréquents du corpus. On constate
une amélioration d’environ 2% absolus des performances par rapport au systeme de référence TF.IDF avec
uniquement les 90 mots les plus fréquents, tout en réduisant l’espace de représentation de 99.7% (de 34k
a 90 mots).

TABLE 1 — Taux de bonne classiﬁcation (%c) et nombre de parametres optimaux (#p) obtenus sur le corpus
de vidéos issu de la RAP en fonction de la métrique utilisée (TF, GTF.IGF ou TF.IDF), du type de terme
(mots, étiquettes morphosyntaxiques ou hybride) et de l’ordre du modele (n).

mots étiquettes morpho. hybride
n=1n=2 n=3 n=1n=2n=3 n=1n=2n=3
TF %c 73.9 63.6 55.0 65.6 61.1 53.8 70.8 61.5 52.9
#p 90 300 700 50 200 150 100 400 400
GTF.IGF %c 71.7 60.2 48.9 64.3 62.8 54.3 70.4 60.1 43.3
#p 200 60 500 40 500 150 60 700 200
TFJDF %c 71.9 39.5 29.1 61.2 55.9 52.0 66.1 59.8 42.7
#p 34k 50k 50k 70 10k 50k 200 50k 50k

TABLE 2 — Taux de bonne classiﬁcation (%c) obtenus sur le corpus de vidéos issu de la RAP pour les
quatre combinaisons de modeles qui offrent les meilleures performances.

| combinaison | %c |
mots TF n=1 + étiquettes morpho. GTF.IGF n=2 74.5
mots TF n=1 et n=3 + étiquettes morpho. TF n=1 + etiquettes morpho. GTF.IGF n=2 74.1
mots TF n=1 + étiquettes morpho. TF n=1 73.8
mots TF n=1 + étiquettes morpho. TF n=1 + etiquettes morpho. GTF.IGF n=2,3 73.4

Aﬁn de déterminer si ces résultats sont lies a la nature du corpus issu de la RAP, nous avons mené les
memes expériences sur le corpus de sous-titres qui contient les transcriptions exactes de videos de quatre
des sept genres. Les résultats sont présentés dans le tableau 3 (colonnes intitulées ST), aux cotés de ceux
obtenus sur le corpus issu de la RAP (colonnes intitulées RAP) en ne prenant en compte que les quatre
genres considérés. Ces résultats ne peuvent pas étre directement comparés étant donné que les corpus uti-
lisés sont différents, mais peuvent foumir une information indicative. On constate que la méthode TF.IDF
est la meilleure approche sur les sous-titres suivie de pres par TF. Sur le corpus issu de RAP ces résultats
sont inversés. On pourrait penser que TF est plus robuste aux erreurs de RAP que TF.IDF. Le résultat le

STANISLAS OGER, MICKAEL ROUVIER, GEORGES LINARES

plus important est que la méthode TF fonctionne tres bien sur le corpus de sous-titres, ce qui indique que
les fréquences d’utilisation des mots-outils de la langue contiennent une information caractéristique du
genre vidéo et qui n’est pas liée au systeme de RAP, mais bien un phénomene linguistique.

TABLE 3 — Taux de bonne classiﬁcation (%c) et nombre de parametres optimaux (#p) obtenus sur le
corpus de vidéos issu de la RAP (RAP) et sur le corpus de sous-titres (ST) en fonction de la métrique
utilisée (TF, GTF.IGF ou TF.IDF) et du type de terme (mots, étiquettes morphosyntaxiques ou hybride)
avec des modeles d’ordre 1.

mots étiquettes morpho. hybride
ST RAP ST RAP ST RAP
TF %c 79.0 89.7 70.0 82.0 73.0 85.5
#p 700 80 80 40 200 70
%c 80.8 87.0 69.7 82.1 73.1 87.1
GTEIGF #p 800 80 20 40 90 80
%c 83.1 88.9 77.7 81.7 79.1 83.0
TEIDF #p 50k 34k 100 100 200 200

Ces résultats valident notre hypothese initiale : les fréquences des mots-outils contiennent une information
permettant de caractériser le genre vidéo. De plus, le modele TF proposé d’ordre 1 permet un gain de bonne
classiﬁcation absolu d’environ 2% en comparaison du modele de référence TF.IDF sur le corpus de RAP,
alors que l’espace de représentation est réduit de 99.7%. La section suivante présente les résultats obtenus
en combinant le modele de classiﬁcation que nous proposons avec d’autres utilisant des parametres audio
de plus bas niveau.

5 Complémentarité avec les paramétres acoustiques

Les parametres linguistiques proposés dans cet article sont basés sur le contenu parlé des vidéos. Dans un
précédent article nous avons proposé l’utilisation de parametres qui ne sont pas directement dépendants
du contenu linguistique (Rouvier et al., 2009) et qui permettent d’attendre un taux de bonne classiﬁcation
de 94% sur ce corpus. Dans cette section nous allons évaluer a quel point ces jeux de parametres sont
complémentaires.

La combinaison de descripteurs est réalisée en utilisant un classiﬁeur de type MVS avec pour chaque
document un vecteur d’entrée contenant les sorties des classiﬁeurs bas-niveaux de chaque descripteur et
les performances sont mesurées sur le corpus de test issu de la RAP. Le modele de contenu linguistique
est TF d’ordre 1, donc les 90 mots les plus fréquents du corpus d’entrainement. Les résultats de ce modele
seul sont présentés dans la colonne notés L du tableau 4.

5.1 Paramétres cepstraux £1 court-terme

L’espace acoustique représenté par des parametres cepstraux est le descripteur le plus couremment utilisé
dans la classiﬁcation de genre vidé. Dans Rouvier et al. (2009), nous avons montré que 12 coefﬁcients
de prédiction linéaire perceptuelle (PLP) et l’énergie du signal avec leurs dérivées premiere et seconde
permettent de bien représenter cet espace et offrent de bonnes performances pour la classiﬁcation en
genre. La variabilité intra-genre est réduite grace a l’utilisation de l’analyse factorielle. Les performances
de ce descripteur de l’espace acoustique sont présentées dans la colonne intitulée EA du tableau 4.

CLASSIFICATION DU GENRE VIDEO BASEE SUR DES TRANSCRIPTIONS

5.2 Interactivité

Le nombre d’intervenants et la maniere dont ils interagissent sont souvent dépendants du genre vidéo
considéré. Par exemple dans les journaux télévisés il y a généralement un présentateur qui occupe la
majorité du temps de parole, contrairement a la plupart des dessins animés et des ﬁlms. Dans Rouvier
et al. (2009), nous avons proposé d’eXtraire des vidéos un descripteur de la maniere dont les intervenants
communiquent. Pour chaque document, un systeme de suivi de locuteur 2 est utilisé pour estimer le nombre
d’intervenants ainsi que les tours de parole. Le vecteur de parametres contient trois éléments : la densité des
tours de parole, le nombre de locuteurs et le temps de parole de l’intervenant principal. Les performances
de ce descripteur d’interactivité sont présentées dans la colonne notée I du tableau 4.

5.3 Qualité de la parole

Cette métrique consiste a mesurer l’adéquation du systeme de RAP avec les documents a transcrire. Par
exemple, les journaux télévisés sont en général bien reconnus par le systeme de RAP que nous utilisons
puisqu’il est destiné a cet usage. Nous proposons d’extraire du systeme plusieurs informations qui forment
le vecteur de parametres fourni au classiﬁeur : la probabilité a posteriori des mots de l’hypothese retenue,
la probabilité a posteriori globale de l’hypothese retenue et l’entropie phonétique. Les performances de
ce descripteur sont présentées dans la colonne notée Q du tableau 4.

5.4 Combinaisons

Les résultats de la combinaison des descripteurs précédents avec le descripteur linguistique que nous avons
proposé sont présentés dans le tableau 4 (colonnes EA+L, I+L et Q+L). On constate que dans tous les cas,
les performances de la combinaison sont meilleures que celles du meilleur des descripteurs individuel-
lement. On peut en déduire que le descripteur linguistique contient une information complémentaire par
rapport a l’information apportée par les autres descripteurs.

La colonne notée EA+I+Q du tableau 4 contient les résultats de la combinaison de tous ces descripteurs
sans le descripteur linguistique proposé. On observe un gain de performances de 3% absolu par rapport au
meilleur descripteur et un gain de 2% absolu par rapport a la meilleure des combinaisons précédentes. La
colonne intitulée EA+I+Q+L contient les résultats de la combinaison précédente en ajoutant le descripteur
linguistique. Ce descripteur apporte un gain absolu de 1%, soit une diminution relative du taux d’erreur
de classiﬁcation de 16% (de 94% a 95%). Ces résultats montrent que le nouveau descripteur linguistique
proposé et les descripteurs acoustiques sont pertinents et surtout complémentaires pour la classiﬁcation en
genre vidéo.

TABLE 4 — Taux de bonne classiﬁcation [%] obtenus avec les descripteurs proposés et leurs combinaisons.

L EA I Q EA+L I+L Q+L EA+I+Q EA+I+Q+L

| %c | 74 91 56 53 93 81 81 94 95

2. Nous avons utilisé le systéme de suivi de locuteur du Laboratoire Infonnatique de l’UniVersité du Maine, disponible a
l’adresse http ://liumtools.uniV—lemans.fr

STANISLAS OGER, MICKAEL ROUVIER, GEORGES LINARES
6 Conclusion

Dans cet article nous avons propose une nouvelle approche pour tirer parti du contenu linguistique de
videos dans le contexte de leur classiﬁcation en genre lorsqu’aucune meta-donnee n’est disponible. Le
contenu linguistique est obtenu par transcription automatique du canal audio. Differentes methodes d’ana-
lyse des transcriptions ainsi obtenues sont comparees.

Les resultats montrent que, contrairement a l’approche classique en categorisation de texte qui consiste
a se concentrer sur les mots porteurs de sens, l’analyse des frequences des mots outils permet une ca-
racterisation du style editorial qui est robuste aux erreurs de RAP. La meme approche, appliquee a un
corpus de sous-titres montre les bonnes performances de ces descripteurs qui restent, sur du texte correct,
legerement moins bonnes que la methode classique s’appuyant sur TF.IDF, mais permettent de reduire
considerablement l’espace de representation des documents.

Enﬁn, les parametres linguistiques proposes sont complementaires avec les parametres acoustiques deja
utilises dans des systemes anterieurs. Ils apportent une information que ces demiers ne peuvent probable-
ment pas capturer : en combinant les niveaux acoustiques et linguistiques, on obtient un diminution du
taux d’erreur de classiﬁcation de 16% relatif par rapport a l’acoustique seul. Finalement, cette combinai-
son permet d’atteindre un taux de bonne classiﬁcation de 95%, ce qui correspond aux meilleurs resultats
publies sur ce type de taches avec des systemes melangeant descripteurs audio et video.

Références

BIBER D. (1988). Variation across speech and writing. Cambridge University Press.

BREZEALE D. & COOK D. (2006). Using closed captions and visual features to classify movies by
genre. In Proceedings of Multimedia Data Mining / Knowledge Discovery and Data Mining.

BREZEALE D. & COOK D. J. (2008). Automatic video classiﬁcation : A survey of the literature. IEEE
Transactions on Systems, Man, and Cybernetics, 38, 416-430.

CYBENKO G. (1989). Approximation by superpositions of a sigmoidal function. Mathematics of Control,
Signals, and Systems (MCSS), 2(4), 303-314.

KARLGREN J. & CUTTING D. (1994). Recognizing text genres with simple metrics using discriminant
analysis. In Proceedings of the international conference on computational linguistics, p. 1071-1075.

LIN W. & HAUPTMANN A. (2002). News video classiﬁcation using svm-based multimodal classiﬁers
and combination strategies. In Proceedings of the International Conference on Multimedia, p. 323-326.

NOCERA P., FREDOUILLE C., LINARES G., MATROUF D., MEIGNIER S., BONASTRE J ., MASSONIE
D. & BECHET F. (2004). The LIA’s french broadcast news transcription system. In SWIM .' Lectures by
Masters in Speech Processing.

ROACH M. & MASON J. (2001). Classiﬁcation of video genre using audio. In Proceedings of EUROS-
PEECH, p. 2693-2696.

ROUVIER M., LINARES G. & MATROUF D. (2009). Robust audio-based classiﬁcation of video genre.
In Proceedings of INTERSPEECH, p. 1159-1162.

STAMATATOS E., FAKOTAKIS N. & KOKKINAKIS G. (2000). Text genre detection using common word
frequencies. In Proceedings of the international conference on computational linguistics, p. 808-814.

WANG P., CAI R. & YANG S. (2003). A hybrid approach to news video classiﬁcation multimodal

features. In Proceedings of the International Conference on Information, Communications and Signal
Processing (ICICS), volume 2, p. 787-791.

