TALN 2010, Montreal, 19-23 juillet 2010

Evaluation automatique de résumés avec et sans référence

Juan-Manuel Torres-Moreno1’2 Horacio Saggion3 Iria da Cunha1’4

Patricia Velazquez-Mora1es5 Eric SanJuan1
(1) LIA, Université d’Avignon et des Pays de Vaucluse, Avignon, France
(2) Ecole Polytechnique de Montreal, (Québec) Canada
(3) DTIC, Universtitat Pompeu Fabra, Barcelona, Espagne
(4) IULA, Universitat Pompeu Fabra, Barcelona, Espagne
(5) VM Labs, Avignon, France

juan-manuel.torres@univ-avignon.fr, {horacio.saggion, iria.dacunha} @upf.edu

Résumé. Nous étudions différentes méthodes d’évaluation de résumé de documents basées sur le
contenu. Nous nous intéressons en particulier a la corrélation entre les mesures d’évaluation avec et sans
référence humaine. Nous avons développé FRESA, un nouveau systeme d’évaluation fondé sur le contenu
qui calcule les divergences entre les distributions de probabilité. Nous appliquons notre systeme de com-
paraison aux diverses mesures d’évaluation bien connues en résumé de texte telles que la Couverture, Res-
ponsiveness, Pyramids et Rouge en étudiant leurs associations dans les taches du résumé multi-document
générique (francais/anglais), focalisé (anglais) et résumé mono-document générique (francais/espagnol).

Abstract. We study document-summary content-based evaluation methods in text summarization
and we investigate the correlation among evaluation measures with and without human models. We apply
our comparison framework to various well-established content-based evaluation measures in text summa-
rization such as Coverage, Responsiveness, Pyramids and Rouge studying their associations in various
text summarization tasks including generic (English/French) and focus-based (English) multi-document
summarization and generic multi and single-document summarization (French/Spanish). The research is
carried out using the new content-based evaluation framework FRESA to compute the divergences among
probability distributions.

M0tS-CléS I Mesures d’évaluation, Résumé automatique de textes.

Keywords: Evaluation measures, Text Automatic Summarization.

1 Introduction

L’évaluation des résumés produits de maniere automatique a touj ours été une question complexe et contro-
versée du Traitement Automatique de la Langue (TAL). Au cours des dernieres années, des évaluations
a grande échelle, indépendantes des concepteurs des systémes, ont vu le jour et plusieurs mesures d’éva-
luation ont été proposées. En ce qui concerne l’évaluation des systémes de résumé automatique, deux
campagnes d’évaluation ont déja été menées par l’agence américaine DARPA (Defense Advanced Re-
search Projects Agency). La premiere, intitulée SUMMAC, s’est déroulée de 1996 a 1998 sous l’égide
du programme TIPSTER (Mani et al., 2002), et la deuxieme, intitulée DUC (Document Understanding

Conferences) (Over et al., 2007) a suivi de 2000 2'1 2007. Depuis 2008 c’est la Text Analysis Conference

TORRES-MORENO ET AL.

(TAC) (TAC, 2008) qui a pris la suite et est le forum pour l’evaluation des differentes technologies d’acces
a l’information textuelle, y compris le resume de texte. Les evaluations du resuIne de texte sont de deux
types : extrinseque et intrinseque (Sparck Jones & Galliers, 1996). Dans une evaluation extrinseque, les
resumes sont evalues dans le contexte d’une teche speciﬁque realisee par un humain ou une machine. Dans
l’evaluation intrinseque, les resumes sont evalues par rapport a une reference ou modele ideal. SUMMAC
a suivi un paradigme d’evaluation extrinseque et DUCfI‘AC ont suivi celui de l’evaluation intrinseque.
Aﬁn d’evaluer intrinsequement les resumes, un resume candidat (peer) est compare a un ou plusieurs re-
sumes de reference (models). DUC a utilise l’interface SEE qui permet aux juges humains de comparer
un peer aux models. Les juges attribuent ainsi une note de couverture au resume candidat et la note ﬁnale
(score) est la moyenne des notes obtenues. Le score ﬁnal peut alors etre utilise pour etablir un classement
(ranking) des systemes de resume automatique (resumeurs). Dans le cas du resume oriente par une requete
(par exemple, lorsque le resume doit repondre a une ou a un ensemble de questions) un score d’adequation
de la reponse Responsiveness est assigne au resume. Ce score evalue la maniere dont le resume repond
aux questions. Puisque la comparaison manuelle des resumes candidats avec ceux de reference est un pro-
cessus ardu et coﬁteux, des recherches ont ete menees ces demieres annees sur les procedures d’evaluation
automatique fondees sur le contenu. Les premieres etudes utilisaient des mesures de similarite telles que
le cosinus (avec ou sans ponderation) pour comparer les resumes candidats et ceux de reference (Donaway
et al., 2000). Diverses mesures de Couverture du vocabulaire telles que les n-grammes ou la plus longue
sous-sequence commune entre le candidat et le modele ont ete proposees (Radev et al., 2003). La mesure
d’evaluation des systemes de traduction automatique BLEU (Papineni et al., 2002) a ete egalement testee
en resume de texte par Pastra & Saggion (2003). Les conferences DUC ont adopte ROUGE (Lin, 2004) pour
l’evaluation du resume fondee sur le contenu. II a ete montre que les classements des systemes generes par
certaines mesures ROUGE (par exemple, ROUGE-2 qui utilise 2-grammes) ont une bonne correlation avec
les classements produits par couverture. Ces dernieres annees, la methode d’evaluation PYRAMIDS a ete
introduite par Nenkova & Passonneau (2004). PYRAMIDS est basee sur la distribution de l’informativite
(content) d’un ensemble de resumes de reference. Les unites informatives (Summary Content Units, SCU)
sont d’abord identiﬁees dans les resumes de reference, puis chacune recoit un poids egal au nombre de
references contenant la meme unite. Les SCU des resumes candidats identiﬁees sont alignees contre celles
des references, puis ponderees. Le score PYRAMIDS attribue aux candidats est le rapport entre la somme
des poids de ces unites et la somme des poids du meilleur resume ideale possible avec le meme nombre
d’unites SCUs des candidats. Les scores PYRAMIDS sont aussi utilises pour le classement des systemes.
Nenkova & Passonneau (2004) ont montre que les scores PYRAMIDS produisent de classements ﬁables
lorsque plusieurs (4 ou plus) references sont utilisees et que les classements PYRAMIDS sont en corre-
lation avec ceux produits par ROUGE-2 et ROUGE-SU4 (ROUGE avec skip 2-grammes). Cependant cette
methode necessite la creation de references et l’identiﬁcation, l’alignement et la ponderation des SCU dans
les references et dans les candidats. Nenkova & Passonneau (2004) ont propose d’utiliser directement le
document complet a des ﬁns de comparaison et ont argumente que les mesures basees sur le contenu,
qui comparent le document entier au resume, pourraient etre des substituts acceptables a celles utilisant
les resumes de reference. Une nouvelle methode d’evaluation de systemes de resume sans reference a ete
recemment proposee (Louis & Nenkova, 2009). Elle est basee sur la comparaison directe de l’informa-
tion contenue entre les resumes et leurs documents. Louis & Nenkova (2009) ont evalue l’efﬁcacite de la
mesure theorique de Jensen-Shannon (J8) (Lin, 1991) pour le classement des systemes dans les taches
de resume multi-document focalise sur une requete. Elles ont montre que les classements produits par
PYRAMIDS et ceux produits par la mesure J8 sont correles, meme si on ne tenait pas compte de la re-
quete dans l’evaluation, c’est a dire que l’on ramene en premiere approximation la tache de resume guide
a celle de resume generique. On peut cependant etudier l’effet de la mesure J8 dans une reelle teche de

EVALUATION AUTOMATIQUE DE RESUMES AVEC ET SANS REFERENCE

FERNANDEZ S., SANJUAN E. & TORRES-MORENO J .-M. (2007). Textual Energy of Associative Me-

mories : performants applications of Enertex algorithm in text summarization and topic segmentation. In
MICAI’07, p. 861-871.

KULLBACK S. & LEIBLER R. (1951). On information and sufﬁciency. Ann. of Math. Stat., 22(1), 79-86.
LIN C.-Y. (2004). ROUGE : A Package for Automatic Evaluation of Summaries. In M.-F. MOENS &
S. SZPAKOWICZ, Eds., Text Summarization Branches Out .' ACL-04 Workshop, p. 74-81, Barcelona.
LIN C.-Y., CAO G., GAO J . & NIE J .-Y. (2006). An information-theoretic approach to automatic
evaluation of summaries. In HLT-NAACL, p. 463-470, Morristown, USA.

LIN J . (1991). Divergence Measures based on the Shannon Entropy. IEEE Tr. on Inf Th., 37(145-151).
LOUIS A. & NENKOVA A. (2009). Automatically Evaluating Content Selection in Summarization wi-
thout Human Models. In Empirical Methods in Natural Language Processing, p. 306-314, Singapore.
MANI 1., KLEIN G., HOUSE D., HIRSCHMAN L., FIRMIN T. & SUNDHEIM B. (2002). Summac : a
text summarization evaluation. Natural Language Engineering, 8(1), 43-68.

MANNING C. D. & SCHUTZE H. (1999). Foundations of Statistical Natural Language Processing.
Cambridge, Massachusetts : The MIT Press.

NENKOVA A. & PASSONNEAU R. J . (2004). Evaluating Content Selection in Summarization : The
Pyramid Method. In HLT-NAACL, p. 145-152.

OVER P., DANG H. & HARMAN D. (2007). DUC in context. IPM, 43(6), 1506-1520.

OWKZARZAK K. & DANG H. T. (2009). Evaluation of automatic summaries : Metrics under varying
data conditions. In UCNLG+Sum’09, p. 23-30, Suntec, Singapore.

PAPINENI K., ROUKOS S., WARD T., & ZHU W. J . (2002). BLEU : a method for automatic evaluation
of machine translation. In ACL’02, p. 311-318.

PASTRA K. & SAGGION H. (2003). Colouring summaries BLEU. In Evaluation Initiatives in Natural
Language Processing, Budapest, Hungary : EACL.

RADEV D. R., TEUFEL S., SAGGION H., LAM W., BLITZER J., QI H., CELEBI A., LIU D. & DRABEK
E. (2003). Evaluation challenges in large-scale document summarization. In ACL’03, p. 375-382.
SAGGION H., RADEV D., TEUFEL S. & LAM W. (2002). Meta-evaluation of Summaries in a Cross-
lingual Environment using Content-based Metrics. In COLING 2002, p. 849-855, Taipei, Taiwan.
SIEGEL S. & CASTELLAN N. (1998). Nonparametric Statistics for the Behavioral Sciences. McGraw-
Hill.

SPARCK JONES K. (2007). Automatic summarising : The state of the art. IPM, 43(6), 1449-1481.
SPARCK JONES K. & GALLIERS J . (1996). Evaluating Natural Language Processing Systems, An
Analysis and Review, volume 1083 of Lecture Notes in Computer Science. Springer.

TAC (2008). Proceedings of the Text Analysis Conference, Gaithesburg, Maryland, USA. NIST.

T ORRES-MORENO J .-M. & RAMIREZ J . (2010). REG : un algorithme glouton appliqué au résumé
automatique de texte. In JADT’I0 : Rome.

ToRRES-MoRENo J .—M., VELAZQUEZ-MoRALES P. & MEUNIER J .-G. (2002). Condensés de textes
par des méthodes numériques. In JADT’02, volume 2, p. 723-734, St Malo, France.

VIVALDI J ., DA CUNHA 1., ToRRES-MoRENo J .-M. & VELAZQUEZ-MORALES P. (2010). Automatic
summarization using terminological and semantic resources. In LREC’I0, Malta.

YATSKO V. & VISHNYAKOV T. (2007). A method for evaluating modern systems of automatic text
summarization. Automatic Documentation and Mathematical Linguistics, 41(3), 93-103.

EVALUATION AUTOMATIQUE DE REsUMEs AVEC ET SANS REFERENCE

résumé générique multi-document en se référent a la tache 2 de DUC’04, ce que nous faisons ici. Nous
nous intéressons aussi a d’autres types de résumés tels que le résumé biographique (DUC’04 tache 5), le
résumé d’opinions (TAC’08 OS) et le résumé dans de langues autres que l’anglais. Dans cet article, nous
présentons une série d’expériences visant une meilleure compréhension de la pertinence de la divergence
J8 pour le classement des systemes de résumé. Nous avons effectué des expériences avec la mesure J8
et nous avons vériﬁé que, pour certaines taches (telles que celles étudiées par Louis & Nenkova (2009)
i1 existe une forte corrélation entre PYRAMIDS, Responsiveness et la divergence J8, mais comme nous
allons le montrer plus loin, il existe aussi des jeux de données de référence pour lesquelles la corrélation
n’est pas si forte. Nous présentons aussi des expériences sur des jeux de données en espagnol et en francais
qui montrent aussi une corrélation positive entre les mesures J8 et ROUGE. Cet article est organisé de
la facon suivante : dans la section 2 nous présentons les travaux existants dans le domaine de l’évalua-
tion du résumé basée sur le contenu, ce qui nous permet de préciser le point de départ de nos recherches.
Dans la section 3, nous décrivons la méthodologie suivie, les outils ainsi que les ressources utilisés lors
des expériences. Dans la section 4, nous présentons les expériences menées et les résultats obtenus sur les
différents corpora et taches. En section 5 comprend une discussion sur ces résultats, avant de conclure et
de présenter quelques perspectives et travaux futurs.

2 Etat de l’art

Donaway et al. (2000) est un des premiers travaux qui mentionne l’utilisation de mesures fondées sur le
contenu. II a présenté une méthode d’évaluation des systemes de résumé automatique au moyen de calculs
de rappel et de variantes de la distance du cosinus entre le texte et le résumé produit. Ce dernier calcul est
clairement une mesure fondée sur le contenu. Ils ont montré qu’il y avait une corrélation faible entre les
classements produits par le calcul du rappel, mais que les mesures basées sur le contenu produisaient des
classements fortement corrélés. Ceci a ouvert la voie aux mesures fondées sur le contenu, en comparant
le contenu du résumé automatique a ceux des résumés de référence. Saggion et al. (2002) ont présenté un
ensemble de mesures d’évaluation basées sur la notion de chevauchement du vocabulaire qui inclut les n-
grammes, la similarité cosinus et la plus longue sous-séquence commune. Ils les ont appliquées au résumé
automatique multi-document en anglais et en chinois. Toutefois, ils n’ont pas évalué les performances
de ces mesures sur différentes taches de résumé. Radev et al. (2003) ont également comparé différentes
mesures d’évaluation basées sur le chevauchement du vocabulaire. Bien que ces mesures permettaient
de séparer les systemes aléatoires de ceux non-aléatoires, aucune conclusion claire ne s’est dégagée sur
la pertinence des mesures étudiées. Un systeme d’évaluation de résumé bien répandu est ROUGE, qui
offre un ensemble de statistiques pour comparer les résumés candidats avec un ensemble de références
produites par des experts. Diverses statistiques existent selon le n-gramme utilisé et le type de traitement
appliqué aux textes d’entrée (par exemple lemmatisation, suppression de mots fonctionnels). Lin et al.
(2006) ont proposé une méthode d’évaluation basée sur l’utilisation de mesures de divergence entre deux
distributions de probabilité (la distribution d’unités dans le résumé automatique et celle d’unités dans le
résumé de référence). Ils ont étudié deux mesures théoriques d’information : la divergence de Kullback-
Leibler (ICE) (Kullback & Leibler, 1951) et celle de Jensen-Shannon (J8) (Lin, 1991). La divergence
J8 est déﬁnie par :

2Qw

°g2H.T2.. ‘”

1 213”,
D.7s(P||Q) = 5 ZP111 10g2 W + Q1111

TORRES-MORENO ET AL.

Ces mesures peuvent étre appliquées a la distribution d’unités dans les résumés des systemes P et dans
ceux de référence Q. La valeur obtenue est utilisée pour attribuer un score au résumé produit. La méthode a
été évaluée par Lin et al. (2006) sur le corpus DUC’02, pour les taches de résumé mono et multi-document.
Une bonne corrélation a été trouvée entre les mesures de divergence et les deux classements obtenus avec
ROUGE et la couverture. Louis & Nenkova (2009) sont allées encore plus loin et, comme Donaway et al.
(2000), ont proposé de comparer directement la distribution de mots dans les documents complets avec
celle des mots dans les résumés automatiques aﬁn d’inférer une mesure d’évaluation basée sur le contenu.
Elles ont constaté une forte corrélation entre les classements produits avec références et ceux obtenus sans
référence. Ce travail est le point de départ de nos recherches sur la pertinence des mesures qui ne reposent
pas sur des références humaines.

3 Protocole d’étude

La méthodologie suivie dans cet article reﬂete celle adoptée dans les travaux passés (Donaway et al.,
2000; Radev et al. , 2003; Louis & Nenkova, 2009). I-/3tant donné une tache de résumé spéciﬁque T, une jeu
de p spéciﬁcations textuelles :{I,-}§:3 (par exemple, document(s), question(s), topics) devant guider les
résumés produits, s résumés candidats {SUMZ-,k};:0 par entrée 1', et m résumés de référence {REFM };.”=0
par entrée i, nous allons comparer les classements produits par différentes mesures d’évaluation basées
sur le contenu, de 3 systemes de résumé automatique. Certaines mesures sont utilisées pour comparer les
résumés automatiques avec n des m références humaines :

MESUREM(SUMi,k,  };-‘:0) (2)
tandis que d’autres mesures comparent les résumés candidats avec l’entrée ou une partie de l’entrée :
MESUREM(SUM,-J9,  (3)

o1‘1 I est un sous-ensemble des entrées I,-. On obtient la moyenne des valeurs produites par les mesures
pour chaque résumé SUM,-J9 et pour chaque systeme k = 0, . . . , s. Ces moyennes induisent un classe-
ment. Ensuite, les classements sont comparés avec le taux p de corrélation de Spearman (Siegel & Cas-
tellan, 1998), utilisé pour mesurer le degré d’association entre deux variables dont les valeurs servent a
classer des objets. Nous avons choisi d’utiliser cette corrélation pour comparer directement les résultats
a ceux présentés par Louis & Nenkova (2009). Les calculs des corrélations ont été effectués avec le logi-
ciel Statistics-RankC0rrelati0n-0.I21, qui calcule la corrélation du classement entre deux vecteurs. Nous
avons par ailleurs vériﬁé la bonne conforrnité de ces résultats avec le test de corrélation du 7' de Kendall
calculé avec le logiciel de statistique R. Les deux tests non paramétriques de Spearman et de Kendall ne
se distinguent réellement que sur le traitement des ex-zequo. La bonne correspondance entre les deux tests
montre que ces derniers n’introduisent pas de biais dans nos analyses. Par la suite nous ne mentionneront
que le p de Sperrnan, plus largement utilisé dans ce domaine.

3.1 Outils

Les expériences ont été réalisées en utilisant un nouveau systeme d’évaluation de résumés : FRESA —
FRamew0rk for Evaluating Summaries Aut0matically— qui inclut des mesures d’évaluation fondées sur

1CPAN, http : //search . cpan . org/~gene/Statistics—RankCorrelation—0 . 12/

EVALUATION AUTOMATIQUE DE REsUMEs AVEC ET SANS REFERENCE

la distribution de probabilités. De facon similaire a ROUGE, FRESA utilise des n-grammes et des skip
n-grammes dans le calcul des distributions de probabilité. L’ environnement FRESA peut étre utilisé pour
l’évaluation de résumés en anglais, francais, espagnol et catalan. I1 integre ﬁltrage et lemmatisation dans
le pre-traitement des documents. 11 a été développé en Perl et mis a disposition de la communautéz. Nous
utilisons aussi ROUGE pour calculer diverses statistiques des corpora detest.

3.2 Téiches de résumé et corpora

Nous avons mené nos expériences sur les corpus et les taches de résumé suivants :
1. Resume générique multi-document en anglais (génération d’un petit resume a partir d’un groupe de
documents pertinents d’une thématique donnée) de DUC’043, tache 2 : 50 groupes, 10 documents
par groupe, 294.636 mots;

2. Resume guidé (Focused-based) en anglais (par exemple, génération d’un résumé multi-document
guidé par la question "qui est X ?", ou X est le nom d’une personne) a partir des données DUC’04,
tache 5 : 50 groupes, 10 documents dans chaque plus le nom d’une personne-cible, 284.440 mots;

3. Tache mise a jour, qui consiste a créer un résumé d’un groupe de documents et une thématique.
Deux sous-taches sont considérées : a) un premier résumé doit étre produit a partir d’un ensemble
de documents et d’une thématique; b) une mise a jour du résumé doit étre réalisée a partir d’un
groupe différent (mais lié) en supposant que les documents utilisés en a) ont été lus. Le corpus de
résumés mis a jour TAC’08 en anglais est utilisé : 48 themes, 20 documents chacun, 36.911 mots.

4. Resume d’opinions o1‘1 les systemes résument les opinions sur une entité cible dans un ensemble
d’articles de blogs. Le corpus TAC’08 OS en anglais4 (tiré de la collection de textes du Blogs06) a
été utilisé : 25 groupes et cibles (par exemple, la cible entité et questions), 1,167.735 mots.

5. Resume générique mono-document en espagnol, corpus Medicina C11’nica5 composé de 50 articles
médicaux, chacun avec le résumé de l’auteur correspondant, 124.929 mots ;

6. Resume générique mono-document en francais, revue Perspectives interdisciplinaires sur 1e travail
et la santé (PISTES)6 ; 50 articles et leurs résumés des auteurs, 381.039 mots ;

7. Resume générique multi-document en francais, corpus RPM27 (joumalistique) ; 20 thématiques dif-
férentes composées de 10 articles et 4 résumés de référence par thématique, 185.223 mots.

Pour les expériences avec les corpora TAC et DUC nous avons utilisé directement les résumés candi-

dats produits par les systemes participant aux évaluations (données ofﬁcielles). Pour les expériences en

espagnol et en francais (résumé mono et multi-document), nous avons créé des résumés a un taux de

compression similaire a ceux de référence en utilisant les systemes suivants :

— ENERTEX (Fernandez et al., 2007), un systeme de résumé automatique fondée sur l’énergie textuelle;

— CORTEX (Torres-Moreno et al., 2002), un systeme d’extraction de phrases mono-document multi-
langue qui combine différentes mesures statistiques de pertinence (angle entre les phrases et la the-
matique, le poids de Hamming des phrases, etc.) et applique un algorithme de décision optimale pour la
sélection des phrases pertinentes;

2RéaméaMeaPamB%e:http://lia.univ—avignon.fr/fileadmin/axes/TALNE/Ressources.html
3http://www—nlpir.nist.gov/projects/duc/guidelines/2004.html
4http://www.nist.gov/tac/data/index.html
Shttp://www.elsevier.es/revistas/ctl_servlet?_f=7032&revistaid=2
Ghttp://www.pistes.uqam.ca/

7http://labs.sinequa.com/rpm2/

TORRES-MORENO ET AL.

— SUMMTERM (Vivaldi et al., 2010), systeme de résumé des articles médicaux basé sur la terminologie
spécialisée aﬁn de donner un score et un classement aux phrases;

— REG (Torres-Moreno & Ramirez, 2010), systeme de résumé basé sur un algorithme glouton;

— Résumeur J8, systeme qui donne un score et un classement aux phrases en considérant leur divergence
J ensen-Shannon par rapport au document source ;

— Baseline-premieres phrases, qui sélectionne les premieres phrases du document pour construire les
résumés;

— Baseline-aléatoire, sélectionne les phrases au hasard pour construire les résumés ;

— Open Text Summarizer (Yatsko & Vishnyakov, 2007), résumeur multi-langue basé sur la fréquence, et

— les systemes de résumé commerciaux multi-langues Word, SSSummarizer’8, Pertinenceg et Copemiclo.

3.3 Mesures d’évaluation

Les mesures suivantes, qui proviennent de l’évaluation humaine du contenu des résumés, ont été utilisées

dans nos expériences :

— Couverture : la quantité d’information partagée entre un résumé candidat et un résumé de référence
(Over et al., 2007). Elle a été utilisée dans les campagnes d’évaluation DUC.

— Responsiveness : elle classe les résumés sur une échelle de 5, en indiquant dans quelle mesure le résumé
répond a un besoin d’information précis (Over et al., 2007). Elle est utilisée dans les taches de résumé
guidé, comme cela a été le cas de certaines taches des campagnes DUC et TAC.

— PYRAMIDS : elle vériﬁe que les unités d’information essentielles (telles qu’on les trouve dans des
résumés de référence générés par les humains) soient présents dans les résumés candidats. PYRAMIDS
est la mesure d’évaluation basée sur le contenu des campagnes TAC.

Pour les corpus DUC et TAC, les valeurs de ces mesures sont disponibles et nous les avons utilisées

directement. Dans nos experiences nous avons utilisé les mesures d’évaluation automatiques suivantes :

— ROUGE : métrique de rappel qui emploie des n-grammes comme unités de contenu pour comparer les
résumés candidats vs. ceux de référence. L’ équation ROUGE spéciﬁée dans (Lin, 2004) est la suivante :

Em E M Zn—grammeeP Countmatchln — gramme)
Em E M Zcount(n-gramme)

ROUGE-n(R, M) = (4)

ou R est le resume a évaluer, M est l’ensemble des résumés modeles (humains), countmatch le nombre
de n-grammes communs en m et P, et count est le nombre de n-grammes dans les résumés modeles.
Pour nos expériences, nous avons utilisé uni-grammes, 2-grammes et skip 2-grammes avec une distance
maximale de 4 (ROUGE-1, ROUGE-2 et ROUGE-SU4). ROUGE est utilisée pour comparer un résumé
candidat a l’ensemble des résumés de référence disponibles.

— L’ equation 1 de la divergence de J8 a été implémentée dans notre systeme FRESA avec la spéciﬁcation

suivante pour la distribution de probabilités de mots w :

CS -
P _C_5. Q _ ﬁg s1w€S (5)
'w— N7 ’LU— C'T+5 tr t
—'*%N+6*B au emen

ou P est la distribution de probabilités des mots w dans le texte T et Q la distribution de probabilités
des mots w dans le résumé S ; N est le nombre de mots dans le texte et le résumé N = NT + N3,

8http://www.kryltech.com/summarizer.htm
9http://www.pertinence.net
whttp://www.copernic.com/en/products/summarizer

EVALUATION AUTOMATIQUE DE REsUMEs AVEC ET SANS REFERENCE

B = 1.5|V|, 03; est le nombre de mots dans le texte et 05 est le nombre de mots dans le résumé. Pour
le lissage des probabilités du résumé, nous avons utilisé 6 = 0.005. Nous avons également implémenté
d’autres méthodes de lissage (par exemple, Good-Turing Manning & Schiitze (1999) qui utilise le pa-
ckage statistique de Perl Statistics-Smoothing-SGT-2.1.2“) dans FRESA, mais nous ne les utilisons pas
dans les expériences rapportées ici. A l’instar de l’approche ROUGE, en plus des uni-grammes de mots
nous avons utilisé des 2-grammes et skip 2-grammes pour le calcul des divergences telles que J8 (a
l’aide des uni-graInInes), J 82 (a l’aide des 2-graInInes), J 84 (a l’aide des skip 2-grammes de ROUGE-
SU4) et J 8 M qui est la moyenne des J 8,-. Les mesures J8 sont utilisées pour comparer les résumés
candidats a son document(s) source, dans notre cadre.

4 Expériences et résultats

Dan premier temps, nous avons reproduit les expériences présentées dans Louis & Nenkova (2009) pour
vériﬁer que notre implémentation J8 obtient des résultats de corrélation cohérents avec ce travail. Nous
avons utilisé les corpus de résumés mis a jour de TAC’08 pour calculer les mesures J8 et ROUGE pour
chaque résumé candidat. Nous avons réalisé deux classements des systemes (un pour chaque mesure),
qui ont été comparés aux classements produits selon les scores de PYRAMIDS et de Responsiveness. Les
corrélations de Spearman ont été calculées entre les différents classements. Les résultats sont présentés
au tableau 1 avec leur p-value”. Ils conﬁrment une forte corrélation entre PYRAMIDS, Responsiveness et
J8. Nous avons également vériﬁé la corrélation élevée entre J8 et ROUGE-2 (0, 83 Spearman, non afﬁ-
chée dans le tableau) pour cette tache et ce corpus. Puis, nous avons mené des expériences sur les corpus

Mesure PYRAMIDS p-value Responsiveness p-value
ROUGE-2 0,96 p < 0, 005 0,92 p < 0,005
J8 0,85 p < 0,005 0,74 p < 0,005

TAB. 1 — Corrélation de Spearman, mesures d’informativité, TAC’08 Update Summarization

DUC’04 et TAC’08 pour la tache pilote de résumés d’opinion. Nous avons aussi mené des expériences
de résumé mono et multi-document en francais et espagnol. En dépit du fait que les expériences pour les
corpus francais et espagnol utilisent moins de systemes ou de documents (par exemple, un nombre infe-
rieur de résumés par tache) que pour l’anglais, les résultats restent signiﬁcatifs. Pour DUC’04, nous avons
calculé la mesure J8 pour chaque résumé candidat des taches 2 et 5 et nous avons calculer les différents
classements des systemes induits par J 8 , ROUGE, les score de Couverture et Responsiveness. Les dif-
férentes valeurs de la corrélation de classement Spearman pour DUC’04 sont présentées aux tableaux 2
(pour la tache 2) et 3 (pour la tache 5). Pour la tache 2, nous avons vériﬁé une forte corrélation entre J8
et la Couverture. Pour la tache 5, la corrélation entre J8 et la Couverture est faible, et la corrélation entre
J8 et Responsiveness est faible voire négative. Bien que la tache de résumé d’opinion soit récente et
son évaluation un probleme compliqué, nous avons décidé de comparer les classements J8 avec ceux de
PYRAMIDS et Responsiveness sur le corpus de TAC’08. Les valeurs de la corrélation de Spearman sont
afﬁchées au tableau 4. Come on peut le constater, il existe une corrélation faible voire négative entre
J8 et PYRAMIDS ou Responsiveness. La corrélation entre les classement PYRAMIDS et Responsiveness

HCPAN, http : //search . cpan . org/~bjoernw/Statistics—Smoothing—SGT—2 . 1 . 2/
12En statistique, la p—value est le plus petit niveau auquel on rejette l’hypothése nulle.

TORRES-MORENO ET AL.

Mesure Couverture p-value
ROUGE-2 0,79 p < 0, 0050
J8 0,68 p < 0, 0025

TAB. 2 — Corrélation de Spearman, mesures J8 et ROUGE vs. Couverture, DUC’04 tache 2

Mesure Couverture p-value Responsiveness p-value
ROUGE-2 0,78 p < 0, 001 0,44 p < 0, 05
J8 0,40 p < 0, 050 -0,18 p < 0, 25

TAB. 3 — Corrélation de Spearman, J8 et ROUGE vs. Responsiveness et Couverture, DUC’04 tache 5

est élevée pour cette tache (0,71 valeur de corrélation de Spearman). Pour les expériences en espagnol et

PYRAMIDS
-0,13

Mesure

.78

TAB. 4 — Corrélation de Spearman, mesure J8 vs. Responsiveness et PYRAMIDS, TAC’08 OS

p-value
p < 0, 25

p-value
p < 0, 25

Responsiveness
-0, 14

en francais mono-document, nous avons lancé 11 systemes de résumé multi-langue sur les corpus. Pour
1’expé1ience en francais multi-document nous avons utilisé 12 systemes. Dans tous les cas, nous avons
produit des résumés aux taux de compression proches de ceux des résumés des auteurs (abstracts). Puis
nous avons calculé les mesures J8 et ROUGE pour chaque résumé et nous avons obtenu la moyenne des
valeurs pour chaque systeme. Ces moyennes ont été utilisées pour produire les classements pour chaque
mesure. Nous avons calculé les corrélations de Spearman pour toutes les paires de classement. Les ré-
sultats sont présentés dans les tablaux 5, 6 et 7. Ils montrent une corrélation moyenne a forte entre les
mesures J8 et celles ROUGE. Toutefois, la mesure J8 basée sur les uni-grammes obtient une corrélation
inférieure a celle qui utilise des n-grammes d’ordre supérieur.

5 Discussion

Nos recherches s’inspirent des récents travaux sur 1’uti1isation des métriques d’éva1uation basées sur le
contenu qui ne reposent pas sur des références humaines, mais qui comparent le contenu des résumés
directement aux entrées (Louis & Nenkova, 2009). Nous avons obtenu des résultats positifs et négatifs en
ce qui concerne 1’uti1isation directe des documents a résumer pour établir des mesures d’éva1uation par
contenu. Nous avons vériﬁé que dans les deux variétés de résumé multi-document en anglais, générique et
basé sur une thématique, la corrélation entre les mesures qui utilisent de références humaines (PYRAMIDS,
Responsiveness et ROUGE) et une mesure qui n’uti1ise pas de référence (la divergence de J8) est forte.
Nous avons trouvé que la corrélation entre les mémes mesures est faible pour le résumé d’informations
biographiques et le résumé d’opinions des blogs. Nous pensons que dans ces cas, les mesures fondées sur
le contenu devraient prendre en compte en plus du document d’entrée, la tache du résumé (par exemple
la représentation texte de la tache-question, description, etc.) pour Inieux évaluer le contenu des candi-
dats (Sparck Jones, 2007), puisque la tache est un facteur déterminant dans la sélection du contenu pour

EVALUATION AUTOMATIQUE DE REsUMEs AVEC ET SANS REFERENCE

TAB. 5 — Correlation de Spearman, J8 vs. ROUGE, corpus mono-document Medicina C11’nica (espagnol)

Mesure ROUGE- 1 p-value ROUGE-2 p-value ROUGE- SU4 p-value
J8 0,56 p < 0,100 0,46 p < 0,100 0,45 p < 0,200
J82 0,88 p < 0,001 0,80 p < 0,002 0,81 p < 0,005
J84 0,88 p < 0,001 0,80 p < 0,002 0,81 p < 0,005
J8M 0,82 p < 0,005 0,71 p < 0,020 0,71 p < 0,010

TAB. 6 — Correlation de Spearman, mesures J8 vs. ROUGE, corpus mono-document PISTES (francais)

le resume. Les experiences multi-langue realisees en resume generique mono-document conﬁrment une

Mesure ROUGE- 1 p-value ROUGE-2 p-value ROUGE- SU4 p-value
J8 0,70 p < 0,050 0,73 p < 0,05 0,73 p < 0,500
J82 0,93 p < 0,002 0,86 p < 0,01 0,86 p < 0,005
J84 0,83 p < 0,020 0,76 p < 0,05 0,76 p < 0,050
J8M 0,88 p < 0,010 0,83 p < 0,02 0,83 p < 0,010

Mesure ROUGE- 1 p-value ROUGE-2 p-value ROUGE- SU4 p-value
J8 0,830 p < 0,002 0,660 p < 0,05 0,741 p < 0,01
J82 0,800 p < 0,005 0,590 p < 0,05 0,680 p < 0,02
J84 0,750 p < 0,010 0,520 p < 0,10 0,620 p < 0,05
J8M 0,850 p < 0,002 0,640 p < 0,05 0,740 p < 0,01

TAB. 7 — Correlation de Spearman, mesures J8 vs. ROUGE, corpus multi-document RPM2 (francais)

forte correlation entre les mesures de divergence J8 et ROUGE. Il faut noter que ROUGE est en general
le logiciel choisi pour presenter les resultats des evaluations basees sur le contenu des resumes en d’autres
langues que l’anglais. Pour les experiences en espagnol, nous sommes conscients que nous n’avons qu’un
seul resume de reference a comparer avec les resumes candidats. Neanmoins, ces references sont les resu-
mes d’auteurs. Comme le montrent les experiences conduites par da Cunha et al. (2007), les professionnels
d’un domaine specialise (par exemple le domaine medical) adoptent des strategies similaires pour resumer
leurs textes : ils ont tendance a choisir des passages a peu pres du meme contenu pour leurs resumes.
Des etudes anterieures ont montre qu’a partir des resumes d’auteur, il est possible de reformuler ﬁdele-
ment le contenu du texte (Chuah, 2001). De ce fait, le resume de l’auteur d’un article medical peut étre
pris comme reference pour l’evaluation des resumes. Dans le corpus en francais PISTES, on suppose une
situation semblable au cas en espagnol.

6 Conclusions et travail futur

Dans cet article, nous avons etudie la validite des mesures d’evaluation du contenu des resumes sans
utilisation de resumes de reference. Il est a noter qu’il y a un debat sur le nombre de references a utiliser
pour evaluer les resumes (Owkzarzak & Dang, 2009). Nous avons mene de multiples experimentations sur

TORRES-MORENO ET AL.

un large spectre de taches du résumé mono-document générique au résumé des opinions. Les principales

contributions de cet article sont les suivantes :

- Nous avons montré que, si l’on s’intéresse uniquement au classement des résumeurs par informativité,
il y a des taches ou les références pourraient étre substituées par le document complet, tout en obtenant
des classements ﬁables. Cependant, nous avons aussi constaté que la substitution des références par le
document complet n’est pas toujours souhaitable. Nous n’avons ainsi trouvé qu’une faible corrélation
entre les différents classements dans des taches de résumé complexes telles que le résumé biographique
et le résumé d’opinion.

- Nous avons également effectué des experiences a grande échelle en espagnol et en francais qui montrent
une corrélation positive de moyenne a forte entre les classements des systemes obtenus par ROUGE et
les mesures de divergence qui n’utilisent pas des résumés de référence.

- Nous avons présenté les résultats du systeme FRESA, pour le calcul des mesures basées sur la divergence
J8. De meme que dans ROUGE, FRESA utilise des uni-grammes, 2-grammes et des skip 2-grammes
de mots pour le calcul des divergences.

Bien d’autres développements et expérimentations sont envisageables. Ainsi, aﬁn de vériﬁer la corrélation
entre ROUGE et J8, a court terme, nous avons l’intention d’étendre nos recherches a d’autres langues et
corpora telles que le portugais et le chinois pour lesquels nous avons acces aux données et a la techno-
logie pour produire des résumés dans plusieurs langues. Nous envisageons également d’appliquer FRESA
aux autres taches de résumé de DUC et TAC. A plus long terme, nous prévoyons d’intégrer une repre-
sentation de la tache/thématique dans le calcul des mesures. Pour mener a bien toutes ces comparaisons
nous sommes cependant dépendants de l’existence de références, sans lesquelles on ne peut mesurer la
proximité entre résumés automatiques et performances humaines. Enﬁn, FRESA sera utilisé dans la nou-
velle tache de question-réponse (QA) de la campagne INEX (http : / /www . inex . ot ago . ac . nz /
tracks/ qa/ qa . asp) pour l’évaluation des réponses longues. Cette tache consiste a répondre a une
question de type encyclopédique par extraction et agglomération de phrases de Wikipédia. Ce type de
texte et de question correspond bien a ceux des taches pour lesquelles nous avons constaté des taux de cor-
relation élevés entre les mesures J8 et les méthodes d’évaluation avec intervention humaine. Par ailleurs,
le calcul de la divergence se fera entre les résumés produits et un ensemble représentatif des passages perti-
nents du Wikipédia. FRESA sera ainsi utilisé pour comparer trois types de systemes appliqués a une meme
tache : les résumeurs multi-documents guidés par une requéte, les systemes de recherche d’information
ciblées (focused IR) et les systemes de QA.

Remerciements. Ce travail a été ﬁnancé partiellement par la bourse post-doctorale d’Iria da Cunha (Mi-
nisterio Espaﬁol de Ciencia e Innovacion, MICINN). H. Saggion remercie les supports des programmes
Ramon y Cajal (MICINN) et Comeca 2010 de l’Universitat Pompeu Fabra, Barcelone.

Références

CHUAH C.-K. (2001). Types of lexical substitution in abstracting. In ACL Student Research Workshop,
p. 49-54, Toulouse, France : Association for Computational Linguistics.

DA CUNHA I., WANNER L. & CABRE M. T. (2007). Summarization of specialized discourse : The case
of medical articles in spanish. Terminology, 13(2), 249-286.

DONAWAY R. L., DRUMMEY K. W. & MATHER L. A. (2000). A comparison of rankings produced by
summarization evaluation measures. In NAACL Workshop on Automatic Summarization, p. 69-78.

