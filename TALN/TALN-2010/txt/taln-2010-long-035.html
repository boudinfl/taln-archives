<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
"http://www.w3.org/TR/html4/loose.dtd">
<html><head><title>&#201;valuer des annotations manuelles dispers&#233;es : les coefficients sont-ils suffisants pour estimer l&#8217;accord inter-annotateurs ?</title>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
</head>
<body>
<div style="page-break-before:always; page-break-after:always"><div><p>TALN 2010, Montr&#233;al, 19&#8211;23 juillet 2010
</p>
<p>&#201;valuer des annotations manuelles dispers&#233;es : les coefficients
sont-ils suffisants pour estimer l&#8217;accord inter-annotateurs ?
</p>
<p>Kar&#235;n Fort1,2 Claire Fran&#231;ois1 Maha Ghribi1
</p>
<p>(1) INIST / CNRS, 2 all&#233;e de Brabois, 54500 Vandoeuvre-l&#232;s-Nancy
(2) LIPN, Universit&#233; Paris 13 &amp; CNRS, 99 av. J.B. Cl&#233;ment, 93430 Villetaneuse
</p>
<p>{karen.fort,claire.francois,maha.ghribi}@inist.fr
</p>
<p>R&#233;sum&#233;. L&#8217;objectif des travaux pr&#233;sent&#233;s dans cet article est l&#8217;&#233;valuation de la qualit&#233; d&#8217;annotations
manuelles de relations de renommage de g&#232;nes dans des r&#233;sum&#233;s scientifiques, annotations qui pr&#233;sentent
la caract&#233;ristique d&#8217;&#234;tre tr&#232;s dispers&#233;es. Pour cela, nous avons calcul&#233; et compar&#233; les coefficients les plus
commun&#233;ment utilis&#233;s, entre autres &#954; (Cohen, 1960) et pi (Scott, 1955), et avons analys&#233; dans quelle me-
sure ils sont adapt&#233;s &#224; nos donn&#233;es. Nous avons &#233;galement &#233;tudi&#233; les diff&#233;rentes pond&#233;rations applicables
&#224; ces coefficients permettant de calculer le &#954; pond&#233;r&#233; (Cohen, 1968) et l&#8217;&#945; (Krippendorff, 1980, 2004).
Nous avons ainsi &#233;tudi&#233; le biais induit par la grande pr&#233;valence d&#8217;une cat&#233;gorie et d&#233;fini un mode de calcul
des distances entre cat&#233;gories reposant sur les annotations r&#233;alis&#233;es.
</p>
<p>Abstract. This article details work aiming at evaluating the quality of the manual annotation of gene
renaming relations in scientific abstracts, which generates sparse annotations. To evaluate these anno-
tations, we computed and compared the results obtained using the commonly advocated inter-annotator
agreement coefficients such as &#954; (Cohen, 1960) or pi (Scott, 1955) and analyzed to which extent they are
relevant for our data. We also studied the different weighting computations applicable to &#954;&#969; (Cohen, 1968)
and &#945; (Krippendorff, 1980, 2004) and estimated the bias introduced by prevalence. We then define a way
to compute distances between categories based on the produced annotations.
</p>
<p>Mots-cl&#233;s : Annotation manuelle, &#233;valuation, accord inter-annotateurs.
</p>
<p>Keywords: Manual annotation, evaluation, inter-annotator agreement.
</p>
<p>1 Introduction
</p>
<p>De nombreuses t&#226;ches de traitement automatique des langues (TAL) n&#233;cessitent une annotation manuelle
en amont, afin, non seulement d&#8217;entra&#238;ner des outils automatiques, mais &#233;galement de cr&#233;er une r&#233;f&#233;rence
pour l&#8217;&#233;valuation. Or, s&#8217;il a &#233;t&#233; d&#233;montr&#233; qu&#8217;une annotation incoh&#233;rente limite les capacit&#233;s des moteurs
entra&#238;n&#233;s &#224; partir de celle-ci (Alex et al., 2006; Reidsma &amp; Carletta, 2008), la qualit&#233; de cette r&#233;f&#233;rence est
rarement justifi&#233;e. En effet, peu de campagnes d&#233;taillent la mani&#232;re dont celle-ci a &#233;t&#233; constitu&#233;e. Lorsque
des mesures d&#8217;accord inter-annotateurs sont donn&#233;es, elles le sont sous forme d&#8217;un coefficient qui est
devenu un standard de fait : le &#8220;kappa&#8221; de Cohen (1960) ou de Carletta (1996), sans plus de pr&#233;cision.1
</p>
<p>1Pour plus de d&#233;tails sur les probl&#232;mes de terminologie li&#233;s aux &#8220;kappa&#8220;, voir l&#8217;introduction de (Artstein &amp; Poesio, 2008).</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>KAR&#203;N FORT, CLAIRE FRAN&#199;OIS, MAHA GHRIBI
</p>
<p>Di Eugenio &amp; Glass (2004) ont montr&#233; la sensibilit&#233; de ces coefficients au biais entre annotateurs et au
probl&#232;me de pr&#233;valence. La discussion reste tr&#232;s ouverte concernant la repr&#233;sentativit&#233; de ces diff&#233;rents
coefficients et la n&#233;cessit&#233; d&#8217;en pr&#233;senter plusieurs. Artstein &amp; Poesio (2008) ont r&#233;alis&#233; un inventaire
tr&#232;s int&#233;ressant des diff&#233;rents modes de calcul de l&#8217;accord inter-annotateurs et ont discut&#233; l&#8217;utilisation de
ces mesures dans les t&#226;ches d&#8217;annotation en TAL. Cependant, il reste difficile de savoir quel coefficient
utiliser en fonction des caract&#233;ristiques des donn&#233;es. Nous pr&#233;sentons dans cet article l&#8217;&#233;valuation que
nous avons r&#233;alis&#233; d&#8217;une campagne d&#8217;annotations manuelles en appliquant et comparant les diff&#233;rentes
m&#233;thodes propos&#233;es par ces auteurs.
</p>
<p>Nous d&#233;crivons bri&#232;vement la campagne d&#8217;annotation que nous avons men&#233;e, puis nous d&#233;taillons et ana-
lysons les r&#233;sultats des accords inter-annotateurs obtenus en utilisant les coefficients simples S, pi et &#954;.
Nous appliquons ensuite les coefficients pond&#233;r&#233;s &#945; et &#954;&#969;, pour lesquels nous &#233;tudions le calcul des dis-
tances entre cat&#233;gories. Enfin, nous discutons des r&#233;sultats qui nous semblent n&#233;cessaires &#224; pr&#233;senter, en
particulier dans des cas comme le n&#244;tre de r&#233;partition non homog&#232;ne des ph&#233;nom&#232;nes langagiers.
</p>
<p>2 Pr&#233;sentation de la campagne d&#8217;annotation
</p>
<p>L&#8217;INIST a &#233;t&#233; charg&#233;, dans le cadre du programme Quaero2, de faire annoter par ses experts les relations de
renommages de g&#232;nes de Bacillus Subtilis pr&#233;sentes dans un corpus de 1 843 textes courts (r&#233;sum&#233;s), soit
plus de 400 000 tokens (ici, cha&#238;nes de caract&#232;res s&#233;par&#233;es par des blancs), s&#233;lectionn&#233;s dans Medline par
l&#8217;&#233;quipe MIG de l&#8217;INRA de Jouy en Josas3, &#224; l&#8217;aide de nomenclatures de noms de g&#232;nes et d&#8217;un ensemble
de mots-clefs d&#233;notant des renommages.
</p>
<p>Cette annotation avait pour but, d&#8217;une part, de construire une base de donn&#233;es de couples de renommage de
g&#232;nes de Bacillus Subtilis, et d&#8217;autre part d&#8217;entra&#238;ner et d&#8217;&#233;valuer les outils d&#8217;extraction automatique des
partenaires du programme. Au final, cette campagne aura permis de mettre au jour manuellement environ
200 couples de renommage, tel que : &#8220;Inactivation of a previously unknown gene, yqzB (renamed ccpN
for control catabolite protein of gluconeogenic genes [..]&#8221;.
</p>
<p>Nous avons appliqu&#233; pour cela la m&#233;thodologie propos&#233;e par Fort et al. (2009), et avons calcul&#233; l&#8217;accord
inter-annotateurs d&#232;s le d&#233;but de la campagne, afin de mettre au jour les d&#233;saccords et de modifier le
guide d&#8217;annotation en cons&#233;quence. Nous avons donc fait annoter par deux annotateurs experts (que nous
noterons ici, A1 et A2) un m&#234;me &#233;chantillon de 93 fichiers, correspondant &#224; plus de 15 000 tokens, &#224;
partir duquel nous avons ensuite calcul&#233; l&#8217;accord inter et intra-annotateurs, tel que recommand&#233; par Gut
&amp; Bayerl (2004). Les relations de renommage sont annot&#233;es ici tr&#232;s simplement, gr&#226;ce &#224; l&#8217;outil Cadixe4,
en s&#233;lectionnant le nom d&#8217;origine du g&#232;ne (annot&#233; Former), puis son nouveau nom (annot&#233; New). Le reste
du texte n&#8217;est pas annot&#233;, mais doit &#234;tre pris en compte dans le calcul de l&#8217;accord inter-annotateurs. Nous
avons d&#233;cid&#233; de nommer cette &#8220;pseudo&#8221; cat&#233;gorie Rien, en fran&#231;ais, pour la diff&#233;rencier des cat&#233;gories
signifiantes Former et New.
</p>
<p>Certains fichiers (plus d&#8217;un tiers d&#8217;entre eux) ne comportent pas de renommage du tout. Nous obtenons
ainsi, en moyenne sur l&#8217;&#233;chantillon, 1 renommage par fichier. Les accords et d&#233;saccords ont &#233;t&#233; analys&#233;s
qualitativement, ce qui nous a permis d&#8217;ajouter les cas non trait&#233;s dans le guide d&#8217;annotation. Les r&#233;sultats
</p>
<p>2http://quaero.org
3Merci &#224; cette &#233;quipe : http://genome.jouy.inra.fr/bibliome/renommage/
4http://caderige.imag.fr/Cadixe/</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>&#201;VALUER DES ANNOTATIONS MANUELLES DISPERS&#201;ES
</p>
<p>quantitatifs de cette annotation sont pr&#233;sent&#233;s sous forme de matrice de confusion dans le tableau 1. Les
r&#233;sultats en diagonale pr&#233;sentent le nombre d&#8217;&#233;l&#233;ments sur lesquels les deux annotateurs sont d&#8217;accord,
pour chaque cat&#233;gorie. Les autres cellules repr&#233;sentent les &#233;l&#233;ments pour lesquels les annotateurs ont
choisi des cat&#233;gories diff&#233;rentes (par exemple, 13 &#233;l&#233;ments ont &#233;t&#233; annot&#233;s New par A1 et Former par A2).
Cette matrice r&#233;v&#232;le la pr&#233;dominance de la cat&#233;gorie Rien (plus de 99% du corpus) et montre ainsi que les
&#233;l&#233;ments annot&#233;s sont tr&#232;s dispers&#233;s.
</p>
<p>A1
Former New Rien Total
</p>
<p>A2
</p>
<p>Former 71 13 23 107
New 8 69 15 92
Rien 7 8 18 840 18 855
Total 86 90 18 878 19 054
</p>
<p>TAB. 1 &#8211; Matrice de confusion calcul&#233;e &#224; partir de l&#8217;ensemble des tokens
</p>
<p>Pour construire cette matrice de confusion, nous avons choisi de prendre en compte le nombre total de
tokens, soit 19 054. Partant du principe que les noms de g&#232;nes correspondent &#224; un sous-ensemble bien
sp&#233;cifique de tokens dans les textes, nous pourrions &#233;galement consid&#233;rer l&#8217;ensemble total des occurrences
de noms de g&#232;nes, soit 1 165, selon les r&#233;sultats obtenus par MIG apr&#232;s application d&#8217;un dictionnaire de
noms de g&#232;nes de l&#8217;INRA. N&#233;anmoins, ce choix nous semble discutable, d&#8217;une part parce que la fiabilit&#233;
des r&#233;sultats d&#233;pend de la compl&#233;tude du dictionnaire, qui, &#233;tant donn&#233; la forte &#233;volutivit&#233; du domaine, ne
peut &#234;tre totale, et d&#8217;autre part, parce que cela revient &#224; n&#233;gliger le fait que les annotateurs doivent souvent
lire tout le texte pour prendre des d&#233;cisions, le renommage n&#8217;&#233;tant parfois attest&#233; qu&#8217;&#224; la fin du texte.
</p>
<p>Nous verrons dans la section 3.2 que cette d&#233;cision a un impact non nul sur les r&#233;sultats. Il est donc
fondamental de justifier ce type de choix lorsque l&#8217;on donne des r&#233;sultats d&#8217;accord inter-annotateurs.
</p>
<p>3 &#201;valuation &#224; l&#8217;aide de coefficients simples
</p>
<p>Dans la suite de l&#8217;article, nous utiliserons les notations et les formules de Artstein &amp; Poesio (2008) concer-
nant les mesures d&#8217;accord inter-annotateurs. Les calculs seront r&#233;alis&#233;s par d&#233;faut &#224; partir du tableau 1.
</p>
<p>3.1 Calcul des coefficients simples
</p>
<p>La mesure la plus &#233;vidente d&#8217;accord inter-annotateurs est l&#8217;accord observ&#233; (Ao), qui correspond &#224; la pro-
portion d&#8217;&#233;l&#233;ments sur lesquels les annotateurs sont d&#8217;accord, autrement dit, le nombre total d&#8217;&#233;l&#233;ments
pour lesquels il y a accord, divis&#233; par le nombre total d&#8217;&#233;l&#233;ments, ici :
</p>
<p>Ao =
71 + 69 + 18840
</p>
<p>19054
= 0, 996116
</p>
<p>Le r&#233;sultat est extr&#234;mement &#233;lev&#233;, mais il ne prend pas en compte l&#8217;accord attendu (expected agreement,
Ae), c&#8217;est-&#224;-dire la possibilit&#233; que les annotateurs classent un &#233;l&#233;ment quelconque dans une m&#234;me cat&#233;go-
rie par hasard.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>KAR&#203;N FORT, CLAIRE FRAN&#199;OIS, MAHA GHRIBI
</p>
<p>Pour analyser nos r&#233;sultats nous utilisons donc ici les coefficients permettant de prendre en compte le
hasard, d&#233;crits par Artstein &amp; Poesio (2008) : S (Bennett et al., 1954), &#954; (Cohen, 1960) et pi (Scott, 1955),
qui sont tous les trois obtenus &#224; partir de la formule suivante, dans laquelle seul l&#8217;accord attendu (Ae)
diff&#232;re selon le coefficient :
</p>
<p>S, &#954;, pi =
Ao &#8722; Ae
1&#8722; Ae
</p>
<p>La diff&#233;rence entre ces coefficients r&#233;side dans la mani&#232;re de calculer l&#8217;accord attendu en fonction des hy-
poth&#232;ses concernant le comportement des annotateurs dans le cas d&#8217;une annotation des &#233;l&#233;ments au hasard.
S suppose que les annotations r&#233;alis&#233;es au hasard suivent une distribution uniforme dans les diff&#233;rentes
cat&#233;gories (ici, trois), l&#8217;accord attendu est donc calcul&#233; de la fa&#231;on suivante :
</p>
<p>ASe =
1
</p>
<p>3
= 0, 333333
</p>
<p>S = 0, 99417
</p>
<p>Le biais le plus important de cette mesure est qu&#8217;elle est directement corr&#233;l&#233;e au nombre de cat&#233;gories. Par
cons&#233;quent, plus le nombre de cat&#233;gories est &#233;lev&#233;, plus l&#8217;accord attendu est faible, ce qu&#8217;il est en g&#233;n&#233;ral,
sa valeur maximale &#233;tant de 0, 5 (1
</p>
<p>2
) pour deux cat&#233;gories.
</p>
<p>Le coefficient pi (Scott, 1955), appel&#233; &#233;galement K dans (Siegel &amp; Castellan, 1988) ou kappa dans (Car-
letta, 1996), consid&#232;re lui aussi que les distributions r&#233;alis&#233;es par les annotateurs par hasard sont &#233;quiva-
lentes, mais il suppose que la r&#233;partition des &#233;l&#233;ments entre cat&#233;gories n&#8217;est pas homog&#232;ne et qu&#8217;elle peut
&#234;tre estim&#233;e par la r&#233;partition moyenne r&#233;alis&#233;e par les annotateurs. L&#8217;accord attendu est donc calcul&#233; de
la fa&#231;on suivante :
</p>
<p>Apie =
((86+107
</p>
<p>2
)2 + (90+92
</p>
<p>2
)2 + (18878+18855
</p>
<p>2
)2)
</p>
<p>190542
= 0, 980464
</p>
<p>pi = 0, 8012
</p>
<p>Le coefficient &#954; (Cohen, 1960) suppose lui dans sa mod&#233;lisation du hasard que la r&#233;partition des &#233;l&#233;ments
entre cat&#233;gories peut &#234;tre diff&#233;rente pour chaque annotateur. Dans ce cas, la probabilit&#233; pour qu&#8217;un &#233;l&#233;ment
soit assign&#233; dans une cat&#233;gorie est le produit de la probabilit&#233; que chaque annotateur l&#8217;assigne dans cette
cat&#233;gorie. L&#8217;accord attendu est donc calcul&#233; de la fa&#231;on suivante :
</p>
<p>A&#954;e =
(86&#215; 107) + (90&#215; 92) + (18878&#215; 18855)
</p>
<p>190542
= 0, 980463
</p>
<p>&#954; = 0, 80121
</p>
<p>3.2 Analyse des r&#233;sultats
</p>
<p>En comparant les 3 coefficients obtenus, nous observons que S (0,99417) est &#224; peine plus faible que l&#8217;ac-
cord observ&#233; (0,996116), tandis que pi (0,8012) et &#954; (0,80121) sont tr&#232;s proches, tout en &#233;tant plus faibles
queAo et S. La valeur relative de ces coefficients est conforme &#224; l&#8217;ordre S &gt; pi et pi 6 &#954; d&#233;crit par Artstein
&amp; Poesio (2008). La valeur &#233;lev&#233;e de S montre que les &#233;l&#233;ments sont annot&#233;s selon une certaine logique.
Pour un accord observ&#233; constant, le coefficient S ne d&#233;pend que du nombre de cat&#233;gories, il n&#8217;est donc
pas sensible &#224; la r&#233;partition des &#233;l&#233;ments dans les cat&#233;gories, au contraire de pi et &#954; (Di Eugenio &amp; Glass,</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>&#201;VALUER DES ANNOTATIONS MANUELLES DISPERS&#201;ES
</p>
<p>2004). Ces auteurs montrent que lorsque les cat&#233;gories sont disproportionn&#233;es, en d&#233;pit d&#8217;un fort accord
sur la cat&#233;gorie pr&#233;dominante, les coefficients pi et &#954; sont tr&#232;s sensibles aux d&#233;saccords sur les cat&#233;gories
minoritaires. Les coefficients de type &#954; sont interpr&#233;t&#233;s comme &#233;tant corrects &#224; partir de 0,67 (Krippen-
dorff, 1980), &#954; et pi sont donc ici tr&#232;s satisfaisants, ce qui nous rassure quant &#224; l&#8217;accord obtenu dans les
deux cat&#233;gories minoritaires. Par ailleurs, &#954; et pi sont tr&#232;s proches, ce qui, selon Di Eugenio &amp; Glass (2004)
est tr&#232;s courant, et signifie que nos donn&#233;es montrent peu de biais d&#251; aux annotateurs, puisque, dans le cas
de deux annotateurs, cela refl&#232;te des distributions marginales similaires (Artstein &amp; Poesio, 2008).
</p>
<p>Nos r&#233;sultats sont donc &#233;lev&#233;s et montrent peu de biais. Ils nous semblent pourtant peu s&#251;rs, puisqu&#8217;ils
mettent sur le m&#234;me plan des cat&#233;gories tr&#232;s h&#233;t&#233;rog&#232;nes, deux minoritaires mais signifiantes (Former et
New), et une non signifiante (Rien) tr&#232;s majoritaire. Notre probl&#232;me est donc de nous assurer que ces coef-
ficients calcul&#233;s sur les trois cat&#233;gories refl&#232;tent un accord significatif sur les deux cat&#233;gories signifiantes
Former et New.
</p>
<p>Une premi&#232;re preuve de l&#8217;influence de ce d&#233;s&#233;quilibre appara&#238;t en examinant l&#8217;&#233;volution de ces coefficients
en fonction de la r&#233;f&#233;rence choisie pour d&#233;finir la cat&#233;gorie Rien. En effet, si nous choisissons non plus le
nombre total de tokens, mais le nombre d&#8217;occurrences de noms de g&#232;nes (1 165), &#224; partir de la matrice de
confusion du tableau 2, nous obtenons S = 0, 90472, pi = 0, 77557 et &#954; = 0, 77571. Ces trois coefficients
ont des valeurs inf&#233;rieures aux pr&#233;c&#233;dentes et pr&#233;sentent un &#233;cart constant entre eux. M&#234;me si la r&#233;partition
des &#233;l&#233;ments et le comportement des annotateurs semblent constants, la taille de la cat&#233;gorie Rien influe
donc sur la valeur d&#233;finitive des accord inter-annotateurs.
</p>
<p>A1
Former New Rien Total Noms g&#232;nes
</p>
<p>A2
</p>
<p>Former 71 13 23 107
New 8 69 15 92
Rien 7 8 951 966
</p>
<p>Total Noms G&#232;nes 86 90 989 1 165
</p>
<p>TAB. 2 &#8211; Matrice de confusion calcul&#233;e &#224; partir des noms de g&#232;nes
</p>
<p>Une seconde m&#233;thode pour estimer dans quelle mesure sa tr&#232;s forte pr&#233;valence entra&#238;ne un biais dans le
calcul des accords inter-annotateurs est de consid&#233;rer uniquement les cat&#233;gories Former et New et calculer
la matrice de confusion correspondante (tableau 3).
</p>
<p>A1
Former New Total
</p>
<p>A2
Former 71 13 84
</p>
<p>New 8 69 77
Total 79 82 161
</p>
<p>TAB. 3 &#8211; Matrice de confusion sans la cat&#233;gorie Rien
</p>
<p>A partir de ce tableau et des formules pr&#233;sent&#233;es en section 3.1, nous obtenons pi = 0, 7390 et &#954; = 0, 73934.
Ces valeurs sont inf&#233;rieures aux coefficients pi et &#954; obtenus &#224; partir de la matrice de confusion compl&#232;te
(tableau 1), ou de la matrice de confusion obtenue &#224; partir de l&#8217;ensemble des occurrences de noms de</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>KAR&#203;N FORT, CLAIRE FRAN&#199;OIS, MAHA GHRIBI
</p>
<p>g&#232;nes (tableau 2). La cat&#233;gorie Rien fait l&#8217;objet d&#8217;un accord tr&#232;s important et pr&#233;sente une tr&#232;s forte pr&#233;-
valence, ce qui semble avoir induit une surestimation de ces coefficients, surestimation dont l&#8217;importance
d&#233;pend de la taille de cette cat&#233;gorie.
</p>
<p>Laignelet &amp; Rioult (2009), confront&#233;s &#224; la m&#234;me disproportion entre cat&#233;gories dans leur campagne d&#8217;an-
notation, se sont appuy&#233;s sur une suggestion de Hripcsak &amp; Heitjan (2002) et ont utilis&#233; le coefficient
R (Finn, 1970) propos&#233; dans le logiciel R5. Le coefficient R est calcul&#233; selon la formule suivante :
</p>
<p>R = 1&#8722; V ariance observee
V ariance attendue
</p>
<p>la variance observ&#233;e &#233;tant la moyenne des variances sur les &#233;l&#233;ments annot&#233;s et la variance attendue &#233;tant
la variance de la distribution uniforme discr&#232;te &#224; n cat&#233;gories (ci-dessous nb categories), soit6 :
</p>
<p>V ariance attendue =
(nb categories)2 &#8722; 1
</p>
<p>12
</p>
<p>Dans notre cas, nous obtenons R = 0, 994. Cette valeur proche de S (0,99417) peut &#234;tre expliqu&#233;e par le
fait que ce coefficient mod&#233;lise le hasard comme S, en consid&#233;rant une distribution uniforme des cat&#233;gories
et n&#8217;est donc pas plus sensible que S &#224; la r&#233;partition des &#233;l&#233;ments dans les cat&#233;gories. Notre conclusion
rejoint de ce point de vue l&#8217;opinion de Ron Artstein, lorsqu&#8217;il dit : &#8220;R is similar to Krippendorff&#8217;s alpha
except that it assumes a uniform distribution as its model of chance annotation ; R is to alpha like S is to
Scott&#8217;s pi, and the same criticisms apply.&#8221; (R. Artstein, communication personnelle, 4 d&#233;cembre 2009).
Le coefficient R de Finn n&#8217;apporte pas plus que S dans des cas de dispersion des annotations et donc de
dissym&#233;trie des cat&#233;gories.
</p>
<p>4 &#201;valuation utilisant des coefficients pond&#233;r&#233;s
</p>
<p>Selon Artstein &amp; Poesio (2008), pi et &#954; ont pour d&#233;faut de traiter tous les d&#233;saccords de la m&#234;me mani&#232;re
et seuls des coefficients pond&#233;r&#233;s permettent de donner plus d&#8217;importance &#224; certains d&#233;saccords.
</p>
<p>4.1 Calcul des coefficients &#954;&#969; et &#945;
</p>
<p>Artstein &amp; Poesio (2008) d&#233;taillent deux coefficients pond&#233;r&#233;s : la version pond&#233;r&#233;e de &#954;, &#954;&#969; (Cohen,
1968) et l&#8217;&#945; (Krippendorff, 1980, 2004). Ces deux coefficients prennent pour base le d&#233;saccord entre an-
notateurs et utilisent une distance entre les cat&#233;gories d&#233;crivant &#224; quel point deux cat&#233;gories sont distinctes
l&#8217;une de l&#8217;autre. On trouve dans (Artstein &amp; Poesio, 2008) une discussion sur la d&#233;finition de cette distance
en fonction du type d&#8217;annotation. Cette distance permet entre autres de traiter des annotations de struc-
tures complexes en introduisant plusieurs valeurs de distance entre annotations. Cette m&#233;thode pr&#233;sente
l&#8217;inconv&#233;nient de complexifier l&#8217;interpr&#233;tation des r&#233;sultats.
</p>
<p>5http ://www.r-project.org/
6Finn (1970) ne d&#233;taille pas le calcul de cette variance attendue, mais on le trouve dans les sources de la li-
</p>
<p>brairie irr du logiciel R. Pour une explication plus approfondie, voir : http://mathworld.wolfram.com/
DiscreteUniformDistribution.html.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>&#201;VALUER DES ANNOTATIONS MANUELLES DISPERS&#201;ES
</p>
<p>Dans notre cas, nous avons 2 cat&#233;gories signifiantes Former et New et une non signifiante, Rien. Nous
consid&#233;rons donc qu&#8217;il est plus important d&#8217;identifier les couples de noms de g&#232;nes que de d&#233;terminer
l&#8217;ant&#233;riorit&#233; d&#8217;un nom par rapport &#224; l&#8217;autre. Par cons&#233;quent, la distance entre Former et New devrait &#234;tre
moindre que celle entre ceux-ci et Rien. Si nous faisons l&#8217;hypoth&#232;se qu&#8217;elle est deux fois moindre, nous
obtenons le tableau de distances entre cat&#233;gories suivant (dans l&#8217;intervalle [0,1]) :
</p>
<p>Former New Rien
Former 0 0,5 1
New 0,5 0 1
Rien 1 1 0
</p>
<p>TAB. 4 &#8211; Tableau de distances estim&#233;es entre cat&#233;gories
</p>
<p>Les coefficients pond&#233;r&#233;s &#954;&#969; et &#945; sont calcul&#233;s &#224; partir de la formule suivante :
</p>
<p>&#954;&#969;, &#945; = 1&#8722; D0
De
</p>
<p>o&#249; D0 repr&#233;sente le d&#233;saccord observ&#233; entre les annotateurs et De repr&#233;sente le d&#233;saccord attendu (expec-
ted), autrement dit, si l&#8217;affectation est r&#233;alis&#233;e au hasard. Le d&#233;saccord attendu de &#954;&#969; et d&#8217;&#945; suit la m&#234;me
logique que &#954; et pi respectivement, et inclut la notion de distance entre cat&#233;gories. Il est &#224; noter que si
toutes les cat&#233;gories sont parfaitement distinctes, nous obtenons &#945; = pi et &#954;&#969; = &#954;. Nous obtenons, &#224; partir
des distances du tableau 4, &#945; = 0, 8292 et &#954;&#969; = 0, 8291. Ces valeurs plus &#233;lev&#233;es que pi et &#954; montrent que
la pond&#233;ration a fait diminuer le d&#233;saccord et augmenter l&#233;g&#232;rement l&#8217;accord inter-annotateurs.
</p>
<p>4.2 Calcul des distances entre cat&#233;gories &#224; partir de la matrice de confusion
</p>
<p>Pour pond&#233;rer l&#8217;accord inter-annotateurs, les distances entre cat&#233;gories sont d&#233;finies &#224; partir de connais-
sances pr&#233;alables sur la t&#226;che d&#8217;annotation. En parall&#232;le, il nous semble utile de les &#233;valuer &#233;galement en
fonction de la difficult&#233; qu&#8217;ont les annotateurs &#224; r&#233;partir les &#233;l&#233;ments entre les cat&#233;gories et de confronter
ces deux approches. Pour ce calcul, nous utilisons la matrice de confusion du tableau 1.
</p>
<p>Nous consid&#233;rons que deux cat&#233;gories sont distinctes s&#8217;il y a peu de chance d&#8217;erreur de classement entre
elles. Plus pr&#233;cis&#233;ment, soient deux cat&#233;gories C1 et C2 appartenant &#224; l&#8217;ensemble des cat&#233;gories consid&#233;-
r&#233;es, P (C2|C1) repr&#233;sente la probabilit&#233; qu&#8217;un annotateur affecte un &#233;l&#233;ment &#224; la cat&#233;gorie C2 sachant que
le deuxi&#232;me annotateur l&#8217;affecte &#224; la cat&#233;gorie C1 et elle se calcule de la fa&#231;on suivante :
</p>
<p>P (C2|C1) = n1C1,2C2 + n2C1,1C2
nC1
</p>
<p>avec n1C1,2C2 repr&#233;sentant le nombre d&#8217;&#233;l&#233;ments class&#233;s par l&#8217;annotateur 1 dans la cat&#233;gorie C1 alors que
l&#8217;annotateur 2 les a class&#233;s dans la cat&#233;gorie C2 ; nC1 repr&#233;sente la somme des &#233;l&#233;ments class&#233;s dans la
cat&#233;gorie C1 par les deux annotateurs.
</p>
<p>Quand cette probabilit&#233; est faible, la cat&#233;gorie C2 est peu similaire &#224; la cat&#233;gorie C1 et le risque d&#8217;obtenir
une classification diff&#233;rente est faible. Nous avons ainsi, selon les donn&#233;es du tableau 1 :
</p>
<p>P (New|Former) = 13 + 8
107 + 86
</p>
<p>= 0, 108808</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>KAR&#203;N FORT, CLAIRE FRAN&#199;OIS, MAHA GHRIBI
</p>
<p>&#8601; Former New Rien
Former 0,735751 0,108808 0,155440
New 0,115385 0,758242 0,126374
Rien 0,000795 0,000609 0,998595
</p>
<p>TAB. 5 &#8211; Tableau de probabilit&#233;s
</p>
<p>Dans le tableau 5, qui pr&#233;sente les valeurs de probabilit&#233; calcul&#233;es pour notre cas d&#8217;application, la dia-
gonale permet d&#8217;estimer l&#8217;accord entre annotateurs pour chaque cat&#233;gorie. Il est tr&#232;s important pour la
cat&#233;gorie Rien (99 % d&#8217;accord) et plus faible pour les cat&#233;gories Former et New (73 et 75 % d&#8217;accord res-
pectivement). Les autres cellules du tableau permettent d&#8217;estimer le d&#233;saccord entre annotateurs, cat&#233;gorie
par cat&#233;gorie. Ces probabilit&#233;s sont tr&#232;s faibles, les cat&#233;gories sont donc peu similaires. Nous observons
&#233;galement que ces probabilit&#233;s sont asym&#233;triques. Les valeurs P (Former|Rien) et P (New|Rien) sont
tr&#232;s faibles (&lt;1&#8240;), le risque d&#8217;affecter un &#233;l&#233;ment &#224; la cat&#233;gorie Former ou &#224; la cat&#233;gorie New sachant
qu&#8217;il est d&#233;j&#224; affect&#233; &#224; Rien est donc quasiment nul. Inversement, le risque d&#8217;affecter un &#233;l&#233;ment &#224; la cat&#233;-
gorie Rien alors qu&#8217;il est d&#233;j&#224; dans la cat&#233;gorie Former ou dans la cat&#233;gorie New est plus &#233;lev&#233; (15 % et
12 % respectivement).
</p>
<p>Nous utilisons ces probabilit&#233;s dans le calcul des distances entre cat&#233;gories selon le principe suivant :
d(C1, C1) = 0, et pour tout C1 diff&#233;rent de C2, d(C1, C2) = 1&#8722; P (C1|C2).
Les probabilit&#233;s n&#8217;&#233;tant pas sym&#233;triques, cette formule ne peut pas &#234;tre utilis&#233;e telle quelle. En utilisant
les m&#234;mes hypoth&#232;ses sur les distributions des annotations que pour la d&#233;finition des coefficients, nous
proposons deux transformations. Premi&#232;rement, le coefficient &#945; suppose que, dans le cas d&#8217;une annotation
au hasard, les annotateurs r&#233;alisent des distributions &#233;quivalentes. Nous d&#233;finissons donc la distance as-
soci&#233;e comme &#233;tant la moyenne des distances orient&#233;es (calcul&#233;es &#224; partir du tableau 5) selon la formule
suivante :
</p>
<p>d&#945;(C1, C2) =
(1&#8722; P (C2|C1)) + (1&#8722; P (C1|C2))
</p>
<p>2
Ce qui donne, dans notre cas :
</p>
<p>d&#945;(Former,New) =
(1&#8722; 13+8
</p>
<p>107+86
) + (1&#8722; 13+8
</p>
<p>90+92
)
</p>
<p>2
= 0, 887904
</p>
<p>Le &#954;&#969;, quant &#224; lui, suppose que les annotateurs proc&#232;dent de mani&#232;re diff&#233;rente, aussi nous calculons la
distance associ&#233;e comme &#233;tant le produit des distances orient&#233;es (calcul&#233;es &#224; partir du tableau 5) selon la
formule suivante :
</p>
<p>d&#954;&#969;(C1, C2) = (1&#8722; P (C2|C1))&#215; (1&#8722; P (C1|C2))
</p>
<p>Dans notre cas, nous obtenons donc :
</p>
<p>d&#954;&#969;(Former,New) = (1&#8722;
13 + 8
</p>
<p>107 + 86
)&#215; (1&#8722; 13 + 8
</p>
<p>90 + 92
) = 0, 788362
</p>
<p>Dans le tableau 6, nous observons que d(Former,New) est inf&#233;rieure dans les deux cas &#224; d(Former,Rien)
et d(New,Rien), ce qui semblerait confirmer que Former et New sont plus proches entre elles que de Rien.
Cependant, ces valeurs sont nettement sup&#233;rieures aux valeurs estim&#233;es dans le tableau 4, la distinction
entre ces deux cat&#233;gories s&#8217;av&#232;re donc dans les faits moins probl&#233;matique que ce que nous avions craint.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>&#201;VALUER DES ANNOTATIONS MANUELLES DISPERS&#201;ES
</p>
<p>d&#945; d&#954;&#969;
d(Former,New) 0,887904 0,788362
d(Former,Rien) 0,921882 0,843888
d(New,Rien) 0,936508 0,873094
</p>
<p>TAB. 6 &#8211; Tableau des distances entre cat&#233;gories calcul&#233;es &#224; partir des donn&#233;es
</p>
<p>5 Conclusion
</p>
<p>A partir d&#8217;une campagne d&#8217;annotation, nous avons analys&#233; diff&#233;rents modes de calcul pour estimer l&#8217;ac-
cord inter-annotateurs. La particularit&#233; de cette campagne est le caract&#232;re tr&#232;s dispers&#233; des annotations
dans les textes qui induit un biais li&#233; &#224; la grande pr&#233;valence des tokens non annot&#233;s. La matrice de confu-
sion synth&#233;tise parfaitement cette information. Le tableau des distances calcul&#233;es entre cat&#233;gories montre
que toutes les cat&#233;gories sont bien distinctes, m&#234;me les deux cat&#233;gories minoritaires mais signifiantes For-
mer et New. Bien que les coefficients pi, &#954;, &#945; et &#954;w soient tr&#232;s sensibles &#224; ce biais de pr&#233;valence, ils restent
satisfaisants dans notre cas, indiquant ainsi un bon accord pour ces deux cat&#233;gories. En premi&#232;re approxi-
mation, nous pouvons estimer ce biais en comparant les r&#233;sultats obtenus avec les matrices de confusion
suivantes : compl&#232;te, r&#233;duite aux noms de g&#232;nes et r&#233;duite aux deux cat&#233;gories signifiantes.
</p>
<p>En compl&#233;ment des coefficients, et quand le mode op&#233;ratoire de l&#8217;annotation le permet, le premier r&#233;sultat
&#224; pr&#233;senter est &#224; notre avis la matrice de confusion accompagn&#233;e d&#8217;explications pr&#233;cises sur les choix
effectu&#233;s. Nous rejoignons Hripcsak &amp; Heitjan (2002) lorsqu&#8217;ils &#233;crivent &#8220;showing the two-by-two contin-
gency table with its marginal totals is probably as informative as any measure&#8220;. En effet, cette matrice
r&#233;sume les informations quantitatives obtenues dans une campagne d&#8217;annotation et permet entre autres
d&#8217;avoir rapidement une id&#233;e des probl&#232;mes de pr&#233;valence et de biais entre annotateurs.
</p>
<p>Le tableau des distances entre cat&#233;gories calcul&#233;es &#224; partir des r&#233;sultats d&#8217;annotation est &#233;galement tr&#232;s
riche en information car il permet d&#8217;analyser le risque r&#233;el d&#8217;erreur entre certaines cat&#233;gories et de le
confronter aux distances d&#233;finies &#224; priori en fonction des connaissances du domaine. Ces diff&#233;rents ta-
bleaux permettent de comprendre au mieux les caract&#233;ristiques de la campagne d&#8217;annotation et d&#8217;interpr&#233;-
ter les diff&#233;rents coefficients obtenus selon leur mode de calcul.
</p>
<p>De nouvelles campagnes d&#8217;annotation sont en cours et devraient nous permettre de tester les diff&#233;rents
coefficients ainsi que la reproductibilit&#233; de nos propositions dans des cas aussi vari&#233;s que l&#8217;annotation de
brevets en pharmacologie (entit&#233;s nomm&#233;es, termes) ou des commentaires de matchs de football (entit&#233;s
nomm&#233;es, relations diverses). Ces campagnes devraient &#233;galement nous permettre d&#8217;&#233;largir la r&#233;flexion
&#224; des annotations r&#233;alis&#233;es par plus de deux annotateurs. Dans ce dernier cas, le tableau des distances
entre cat&#233;gories calcul&#233;es &#224; partir des r&#233;sultats d&#8217;annotation permettra de r&#233;aliser une bonne synth&#232;se des
probl&#232;mes existants pour distinguer les cat&#233;gories.
</p>
<p>Remerciements
</p>
<p>Ce travail a &#233;t&#233; r&#233;alis&#233; en partie dans le cadre du programme Quaero 7, financ&#233; par OSEO, agence natio-
nale de valorisation de la recherche. Nous en remercions les participants, en particulier l&#8217;&#233;quipe MIG de
</p>
<p>7http://www.quaero.org</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>KAR&#203;N FORT, CLAIRE FRAN&#199;OIS, MAHA GHRIBI
</p>
<p>l&#8217;INRA. Nous remercions &#233;galement F. Tisserand et B. Taliercio, les annotateurs experts de l&#8217;INIST, ainsi
que Ron Artstein, pour son int&#233;r&#234;t et ses r&#233;ponses d&#233;taill&#233;es.
</p>
<p>R&#233;f&#233;rences
</p>
<p>ALEX B., NISSIM M. &amp; GROVER C. (2006). The impact of annotation on the performance of protein
tagging in biomedical text. In Proceedings of The Fifth International Conference on Language Resources
and Evaluation (LREC), p. 595&#8211;600, G&#232;ne, Italie.
ARTSTEIN R. &amp; POESIO M. (2008). Inter-coder agreement for computational linguistics. Computational
Linguistics, 34(4), 555&#8211;596.
BENNETT E. M., ALPERT R. &amp; C.GOLDSTEIN A. (1954). Communications through limited questio-
ning. Public Opinion Quarterly, 18(3), 303&#8211;308.
CARLETTA J. (1996). Assessing agreement on classification tasks : The kappa statistic. Computational
Linguistics, 22, 249&#8211;254.
COHEN J. (1960). A coefficient of agreement for nominal scales. Educational and Psychological Mea-
surement, 20(1), 37&#8211;46.
COHEN J. (1968). Weighted kappa : Nominal scale agreement with provision for scaled disagreement or
partial credit. Psychological Bulletin, 70(4), 213&#8211;220.
DI EUGENIO B. &amp; GLASS M. (2004). The kappa statistic : a second look. Computational Linguistics,
30(1), 95&#8211;101.
FINN R. H. (1970). A note on estimating the reliability of categorical data. Educational and Psycholo-
gical Measurement, 30, 71&#8211;76.
FORT K., EHRMANN M. &amp; NAZARENKO A. (2009). Vers une m&#233;thodologie d&#8217;annotation des entit&#233;s
nomm&#233;es en corpus ? In Actes de la 16&#232;me Conf&#233;rence sur le Traitement Automatique des Langues
Naturelles 2009 Traitement Automatique des Langues Naturelles 2009, Senlis, France.
GUT U. &amp; BAYERL P. S. (2004). Measuring the reliability of manual annotations of speech corpora. In
Proceedings of Speech Prosody, p. 565&#8211;568, Nara, Japon.
HRIPCSAK G. &amp; HEITJAN D. F. (2002). Measuring agreement in medical informatics reliability studies.
Journal of Biomedical Informatics, 35(2), 99&#8211;110.
KRIPPENDORFF K. (1980). Content Analysis : An Introduction to Its Methodology, chapter 12. Sage :
Beverly Hills, CA.
KRIPPENDORFF K. (2004). Content Analysis : An Introduction to Its Methodology, second edition,
chapter 11. Sage : Thousand Oaks, CA.
LAIGNELET M. &amp; RIOULT F. (2009). Rep&#233;rer automatiquement les segments obsolescents &#224; l&#8217;aide
d&#8217;indices s&#233;mantiques et discursifs. In Actes de Traitement Automatique des Langues Naturelles (TALN
2009), Senlis, France.
REIDSMA D. &amp; CARLETTA J. (2008). Reliability measurement without limits. Computational Linguis-
tics, 34(3), 319&#8211;326.
SCOTT W. A. (1955). Reliability of content analysis : The case of nominal scale coding. Public Opinion
Quaterly, 19(3), 321&#8211;325.
SIEGEL S. &amp; CASTELLAN N. J. (1988). Nonparametric Statistics for the Behavioral Sciences. New
York : McGraw-Hill, 2nd edition.</p>

</div></div>
</body></html>