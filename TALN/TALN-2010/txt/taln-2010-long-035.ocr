TALN 2010, Montréal, 19-23 juillet 2010

Evaluer des annotations manuelles dispersées : les coefﬁcients
sont-ils sufﬁsants pour estimer l’accord inter-annotateurs ?

Karen Fort” Claire Francoisl Maha Ghribil
(1) INIST / CNRS, 2 allée de Brabois, 54500 Vandoeuvre-les-Nancy
(2) LIPN, Université Paris 13 & CNRS, 99 av. J .B. Clément, 93430 Villetaneuse
{karen.fort,claire.francois,maha.ghribi} @inist.fr

Résumé. L’ objectif des travaux présentés dans cet article est l’évaluation de la qualité d’annotations
manuelles de relations de renommage de genes dans des résumés scientiﬁques, annotations qui présentent
la caractéristique d’étre tres dispersées. Pour cela, nous avons calculé et comparé les coefﬁcients les plus
communément utilisés, entre autres H) (Cohen, 1960) et 7r (Scott, 1955), et avons analysé dans quelle me-
sure ils sont adaptés a nos données. Nous avons également étudié les différentes pondérations applicables
a ces coefﬁcients permettant de calculer le K2 pondéré (Cohen, 1968) et l’a (Krippendorff, 1980, 2004).
Nous avons ainsi étudié le biais induit par la grande prévalence d’une catégorie et déﬁni un mode de calcul
des distances entre catégories reposant sur les annotations réalisées.

Abstract. This article details work aiming at evaluating the quality of the manual annotation of gene
renaming relations in scientiﬁc abstracts, which generates sparse annotations. To evaluate these anno-
tations, we computed and compared the results obtained using the commonly advocated inter-annotator
agreement coefﬁcients such as H) (Cohen, 1960) or 7r (Scott, 1955) and analyzed to which extent they are
relevant for our data. We also studied the different weighting computations applicable to H“, (Cohen, 1968)
and 04 (Krippendorff, 1980, 2004) and estimated the bias introduced by prevalence. We then deﬁne a way
to compute distances between categories based on the produced annotations.

M0tS-CléS I Annotation manuelle, évaluation, accord inter-annotateurs.

Keywords: Manual annotation, evaluation, inter-annotator agreement.

1 Introduction

De nombreuses taches de traitement automatique des langues (TAL) nécessitent une annotation manuelle
en amont, aﬁn, non seulement d’entrainer des outils automatiques, mais également de créer une référence
pour l’évaluation. Or, s’il a été démontré qu’une annotation incohérente limite les capacités des moteurs
entrainés a partir de celle-ci (Alex et al., 2006; Reidsma & Carletta, 2008), la qualité de cette référence est
rarement justiﬁée. En effet, peu de campagnes détaillent la maniere dont celle-ci a été constituée. Lorsque
des mesures d’accord inter-annotateurs sont données, elles le sont sous forme d’un coefﬁcient qui est
devenu un standard de fait : le “kappa” de Cohen (1960) ou de Carletta (1996), sans plus de précision.1

1Pour plus de details sur les problemes de terrninologie liés aux “kappa“, voir l’introduction de (Artstein & Poesio, 2008).

KAREN FORT, CLAIRE FRANCOIS, MAHA GHRIBI

Di Eugenio & Glass (2004) ont montré la sensibilité de ces coefﬁcients au biais entre annotateurs et au
probleme de prévalence. La discussion reste tres ouverte concemant la représentativité de ces différents
coefﬁcients et la nécessité d’en présenter plusieurs. Artstein & Poesio (2008) ont réalisé un inventaire
tres intéressant des différents modes de calcul de l’accord inter-annotateurs et ont discuté l’utilisation de
ces mesures dans les taches d’annotation en TAL. Cependant, il reste difﬁcile de savoir quel coefﬁcient
utiliser en fonction des caractéristiques des données. Nous présentons dans cet article l’évaluation que
nous avons réalisé d’une campagne d’annotations manuelles en appliquant et comparant les différentes
méthodes proposées par ces auteurs.

Nous décrivons brievement la campagne d’annotation que nous avons menée, puis nous détaillons et ana-
lysons les résultats des accords inter-annotateurs obtenus en utilisant les coefﬁcients simples S, 7r et n.
Nous appliquons ensuite les coefﬁcients pondérés oz et nw, pour lesquels nous étudions le calcul des dis-
tances entre catégories. Enﬁn, nous discutons des résultats qui nous semblent nécessaires a presenter, en
particulier dans des cas comme le notre de répartition non homogene des phénomenes langagiers.

2 Présentation de la campagne d’annotation

L’ INIST a été chargé, dans le cadre du programme Quaeroz, de faire annoter par ses experts les relations de
renommages de genes de Bacillus Subtilis présentes dans un corpus de 1 843 textes courts (résumés), soit
plus de 400 000 tokens (ici, chaines de caracteres séparées par des blancs), sélectionnés dans Medline par
l’équipe MIG de l’INRA de J ouy en J osas3, a l’aide de nomenclatures de noms de genes et d’un ensemble
de mots-clefs dénotant des renommages.

Cette annotation avait pour but, d’une part, de construire une base de données de couples de renommage de
genes de Bacillus Subtilis, et d’autre part d’entrainer et d’évaluer les outils d’extraction automatique des
partenaires du programme. Au ﬁnal, cette campagne aura permis de mettre au jour manuellement environ
200 couples de renommage, tel que : “Inactivation of a previously unknown gene, yqzB (renamed ccpN
for control catabolite protein of gluconeogenic genes [..]”.

Nous avons appliqué pour cela la méthodologie proposée par Fort et al. (2009), et avons calculé l’accord
inter-annotateurs des le début de la campagne, aﬁn de mettre au jour les désaccords et de modiﬁer le
guide d’annotation en conséquence. Nous avons donc fait annoter par deux annotateurs experts (que nous
noterons ici, A1 et A2) un meme échantillon de 93 ﬁchiers, correspondant a plus de 15 000 tokens, a
partir duquel nous avons ensuite calculé l’accord inter et intra-annotateurs, tel que recommandé par Gut
& Bayerl (2004). Les relations de renommage sont annotées ici tres simplement, grace a l’outil Cadixe4,
en sélectionnant le nom d’origine du gene (annoté Former), puis son nouveau nom (annoté New). Le reste
du texte n’est pas annoté, mais doit étre pris en compte dans le calcul de l’accord inter-annotateurs. Nous
avons décidé de nommer cette “pseudo” catégorie Rien, en francais, pour la différencier des catégories
signiﬁantes Former et New.

Certains ﬁchiers (plus d’un tiers d’entre eux) ne comportent pas de renommage du tout. Nous obtenons
ainsi, en moyenne sur l’échantillon, 1 renommage par ﬁchier. Les accords et désaccords ont été analysés
qualitativement, ce qui nous a perIr1is d’aj outer les cas non traités dans le guide d’annotation. Les résultats

zhttp://quaero.org
3NkKﬁaceﬁeéqmpe:http://genome.jouy.inra.fr/bibliome/renommage/
4http://caderige.imag.fr/Cadixe/

EVALUER DES ANNOTATIONS MANUELLES DISPERSEES

quantitatifs de cette annotation sont présentés sous forme de matrice de confusion dans le tableau 1. Les
résultats en diagonale présentent le nombre d’éléments sur lesquels les deux annotateurs sont d’accord,
pour chaque catégorie. Les autres cellules représentent les éléments pour lesquels les annotateurs ont
choisi des catégories différentes (par exemple, 13 elements ont été annotés New par A1 et Former par A2).
Cette matrice révele la prédominance de la catégorie Rien (plus de 99% du corpus) et montre ainsi que les
éléments annotés sont tres dispersés.

A1
Former New Rien Total
Former 71 13 23 107
New 8 69 15 92
Rien 7 8 18 840 18 855
Total 86 90 18 878 19 054

A2

TAB. 1 — Matrice de confusion calculée a partir de l’ensemble des tokens

Pour construire cette matrice de confusion, nous avons choisi de prendre en compte le nombre total de
tokens, soit 19 054. Partant du principe que les noms de genes correspondent a un sous-ensemble bien
spéciﬁque de tokens dans les textes, nous pourrions également considérer l’ensemble total des occurrences
de noms de genes, soit 1 165, selon les résultats obtenus par MIG apres application d’un dictionnaire de
noms de genes de l’INRA. Néanmoins, ce choix nous semble discutable, d’une part parce que la ﬁabilité
des résultats dépend de la complétude du dictionnaire, qui, étant donné la forte évolutivité du domaine, ne
peut étre totale, et d’autre part, parce que cela revient a négliger le fait que les annotateurs doivent souvent
lire tout le texte pour prendre des décisions, le renommage n’étant parfois attesté qu’a la ﬁn du texte.

Nous verrons dans la section 3.2 que cette decision a un impact non nul sur les résultats. Il est donc
fondamental de justiﬁer ce type de choix lorsque l’on donne des résultats d’accord inter-annotateurs.

3 Evaluation 51 l’aide de coefﬁcients simples

Dans la suite de l’article, nous utiliserons les notations et les formules de Artstein & Poesio (2008) concer-
nant les mesures d’accord inter-annotateurs. Les calculs seront réalisés par défaut a partir du tableau 1.

3.1 Calcul des coefﬁcients simples

La mesure la plus évidente d’accord inter-annotateurs est l’accord observé (A0), qui correspond a la pro-
portion d’éléments sur lesquels les annotateurs sont d’accord, autrement dit, le nombre total d’éléments
pour lesquels il y a accord, divisé par le nombre total d’éléments, ici :

71 + 69 + 18840

A, = T
19054

Le résultat est extrémement élevé, mais il ne prend pas en compte l’accord attendu (expected agreement,

A6), c’est-a-dire la possibilité que les annotateurs classent un élément quelconque dans une méme catego-
rie par hasard.

= 0, 996116

KAREN FORT, CLAIRE FRANCOIS, MAHA GHRIBI

Pour analyser nos resultats nous utilisons donc ici les coefﬁcients permettant de prendre en compte le
hasard, decrits par Artstein & Poesio (2008) : S (Bennett et al., 1954), H) (Cohen, 1960) et 7r (Scott, 1955),
qui sont tous les trois obtenus a partir de la formule suivante, dans laquelle seul l’accord attendu (A6)
differe selon le coefﬁcient : A A

5v’“'=T4.

La difference entre ces coefﬁcients reside dans la maniere de calculer l’accord attendu en fonction des hy-
potheses concemant le comportement des annotateurs dans le cas d’une annotation des elements au hasard.
S suppose que les annotations realisees au hasard suivent une distribution uniforme dans les differentes
categories (ici, trois), l’accord attendu est donc calcule de la facon suivante :

1
A3 = 3 = 0, 333333

6

S = 0, 99417

Le biais le plus important de cette mesure est qu’elle est directement correlee au nombre de categories. Par
consequent, plus le nombre de categories est eleve, plus l’accord attendu est faible, ce qu’il est en general,
sa valeur maximale etant de 0, 5 (%) pour deux categories.

Le coefﬁcient 7r (Scott, 1955), appele egalement K dans (Siegel & Castellan, 1988) ou kappa dans (Car-
letta, 1996), considere lui aussi que les distributions realisees par les annotateurs par hasard sont equiva-
lentes, mais il suppose que la repartition des elements entre categories n’est pas homogene et qu’elle peut
etre estimee par la repartition moyenne realisee par les annotateurs. L’ accord attendu est donc calcule de
la facon suivante :

((%i)2 + (WV + (1)2)

190542
7r = 0,8012

Ag:

= 0, 980464

Le coefﬁcient H) (Cohen, 1960) suppose lui dans sa modelisation du hasard que la repartition des elements
entre categories peut etre differente pour chaque annotateur. Dans ce cas, la probabilite pour qu’un element
soit assigne dans une categorie est le produit de la probabilite que chaque annotateur l’assigne dans cette
categorie. L’ accord attendu est donc calcule de la facon suivante :

_ (86 X 107) + (90 X 92) + (13373 X 13355)
‘ 190542

K) = 0, 30121

A:

=Q%M%

3.2 Analyse des résultats

En comparant les 3 coefﬁcients obtenus, nous observons que S (0,99417) est a peine plus faible que l’ac-
cord observe (0,996116), tandis que 7r (0,8012) et K2 (0,80121) sont tres proches, tout en etant plus faibles
que A, et S. La valeur relative de ces coefﬁcients est conforme a l’ordre S 2 7r et 7r < K) decrit par Artstein
& Poesio (2008). La valeur elevee de S montre que les elements sont annotes selon une certaine logique.
Pour un accord observe constant, le coefﬁcient S ne depend que du nombre de categories, il n’est donc
pas sensible a la repartition des elements dans les categories, au contraire de 7r et K2 (Di Eugenio & Glass,

EVALUER DES ANNOTATIONS MANUELLES DISPERSEES

2004). Ces auteurs montrent que lorsque les categories sont disproportionnees, en depit d’un fort accord
sur la categorie predominante, les coefﬁcients 7r et K2 sont tres sensibles aux desaccords sur les categories
minoritaires. Les coefﬁcients de type K) sont interpretes comme etant corrects a partir de 0,67 (Krippen-
dorff, 1980), K) et 7r sont donc ici tres satisfaisants, ce qui nous rassure quant a l’accord obtenu dans les
deux categories minoritaires. Par ailleurs, K) et 7r sont tres proches, ce qui, selon Di Eugenio & Glass (2004)
est tres courant, et signiﬁe que nos donnees montrent peu de biais dﬁ aux annotateurs, puisque, dans le cas
de deux annotateurs, cela reﬂete des distributions marginales similaires (Artstein & Poesio, 2008).

Nos resultats sont donc eleves et montrent peu de biais. Ils nous semblent pourtant peu sﬁrs, puisqu’ils
mettent sur le meme plan des categories tres heterogenes, deux minoritaires mais signiﬁantes (Former et
New), et une non signiﬁante (Rien) tres majoritaire. Notre probleme est donc de nous assurer que ces coef-
ﬁcients calcules sur les trois categories reﬂetent un accord signiﬁcatif sur les deux categories signiﬁantes
Former et New.

Une premiere preuve de l’inﬂuence de ce desequilibre apparait en examinant l’evolution de ces coefﬁcients
en fonction de la reference choisie pour deﬁnir la categorie Rien. En effet, si nous choisissons non plus le
nombre total de tokens, mais le nombre d’occurrences de noms de genes (1 165), a partir de la matrice de
confusion du tableau 2, nous obtenons S = 0, 90472, 7r = 0, 77557 et K) = 0, 77571. Ces trois coefﬁcients
ont des valeurs inferieures aux precedentes et presentent un ecart constant entre eux. Meme si la repartition
des elements et le comportement des annotateurs semblent constants, la taille de la categorie Rien inﬂue
donc sur la valeur deﬁnitive des accord inter-annotateurs.

A1
Former New Rien Total Noms genes
Former 71 1 3 23 107
A2 New 8 69 15 92
Rien 7 8 951 966
Total Noms Genes 86 90 989 1 165

TAB. 2 — Matrice de confusion calculee a partir des noms de genes

Une seconde methode pour estimer dans quelle mesure sa tres forte prevalence entraine un biais dans le
calcul des accords inter-annotateurs est de considerer uniquement les categories Former et New et calculer
la matrice de confusion correspondante (tableau 3).

A 1
Former New Total
Former 7 1 1 3 84
A2 New 8 69 77
Total 79 82 161

TAB. 3 — Matrice de confusion sans la categorie Rien

A partir de ce tableau et des formules presentees en section 3.1, nous obtenons 7r = 0, 7390 et K) = 0, 73934.
Ces valeurs sont inferieures aux coefﬁcients 7r et K2 obtenus a partir de la matrice de confusion complete
(tableau 1), ou de la matrice de confusion obtenue a partir de l’ensemble des occurrences de noms de

KAREN FORT, CLAIRE FRANCOIS, MAHA GHRIBI

genes (tableau 2). La catégorie Rien fait l’objet d’un accord tres important et présente une tres forte pré-
valence, ce qui semble avoir induit une surestimation de ces coefﬁcients, surestimation dont l’importance
dépend de la taille de cette catégorie.

Laignelet & Rioult (2009), confrontés a la meme disproportion entre catégories dans leur campagne d’an-
notation, se sont appuyés sur une suggestion de Hripcsak & Heitjan (2002) et ont utilisé le coefﬁcient
R (Finn, 1970) proposé dans le logiciel R5. Le coefﬁcient R est calculé selon la formule suivante :

R 1 Variance observee
Variance attendue
la variance observée étant la moyenne des variances sur les éléments annotés et la variance attendue étant

la variance de la distribution uniforme discrete a n catégories (ci-dessous nb categories), soit6 :

Variance attendue =  
Dans notre cas, nous obtenons R = 0,994. Cette valeur proche de S (0,99417) peut étre expliquée par le
fait que ce coefﬁcient modélise le hasard comme S, en considérant une distribution uniforme des catégories
et n’est donc pas plus sensible que S a la répartition des éléments dans les catégories. Notre conclusion
rejoint de ce point de vue l’opinion de Ron Artstein, lorsqu’il dit : “R is similar to Krippendorﬁ°’s alpha
except that it assumes a uniform distribution as its model of chance annotation ; R is to alpha like S is to
Scott’s pi, and the same criticisms apply.” (R. Artstein, communication personnelle, 4 décembre 2009).
Le coefﬁcient R de Finn n’apporte pas plus que S dans des cas de dispersion des annotations et donc de
dissymétrie des categories.

4 Evaluation utilisant des coefﬁcients pondérés

Selon Artstein & Poesio (2008), 7r et K3 ont pour défaut de traiter tous les désaccords de la meme maniere
et seuls des coefﬁcients pondérés permettent de donner plus d’importance a certains désaccords.

4.1 Calcul des coefﬁcients no, et a

Artstein & Poesio (2008) détaillent deux coefﬁcients pondérés : la version pondérée de K2, nu, (Cohen,
1968) et l’oi (Krippendorff, 1980, 2004). Ces deux coefﬁcients prennent pour base le désaccord entre an-
notateurs et utilisent une distance entre les catégories décrivant a quel point deux catégories sont distinctes
l’une de l’autre. On trouve dans (Artstein & Poesio, 2008) une discussion sur la déﬁnition de cette distance
en fonction du type d’annotation. Cette distance permet entre autres de traiter des annotations de struc-
tures complexes en introduisant plusieurs valeurs de distance entre annotations. Cette méthode présente
l’inconvénient de complexiﬁer l’interprétation des résultats.

5http ://www.r—proj ect.org/

5Finn (1970) ne détaille pas le calcul de cette Variance attendue, mais on le trouve dans les sources de la li-
brairie irr du logiciel R. Pour une explication plus approfondie, Voir : http://mathworld.wolfram.com/
DiscreteUniformDistribution.html.

EVALUER DES ANNOTATIONS MANUELLES DISPERSEES

Dans notre cas, nous avons 2 categories signiﬁantes Former et New et une non signiﬁante, Rien. Nous
considerons donc qu’il est plus important d’identiﬁer les couples de noms de genes que de determiner
l’anteriorite d’un nom par rapport a l’autre. Par consequent, la distance entre Former et New devrait étre
moindre que celle entre ceux-ci et Rien. Si nous faisons l’hypothese qu’elle est deux fois moindre, nous
obtenons le tableau de distances entre categories suivant (dans l’intervalle [0,1]) :

Former New Rien
Former 0 0,5 1
New 0,5 0 1
Rien 1 1 0

TAB. 4 — Tableau de distances estimees entre categories

Les coefﬁcients ponderes /<5“, et 04 sont calcules a partir de la formule suivante :

Do
/cw, oz = 1 — E
ou D0 represente le desaccord observe entre les annotateurs et De represente le desaccord attendu (expec-
ted), autrement dit, si l’affectation est realisee au hasard. Le desaccord attendu de /cu, et d’a suit la meme
logique que K) et 7r respectivement, et inclut la notion de distance entre categories. 11 est a noter que si
toutes les categories sont parfaitement distinctes, nous obtenons 04 = 7r et H“, = K). Nous obtenons, a partir
des distances du tableau 4, 04 = 0, 8292 et H“, = 0,8291. Ces valeurs plus elevees que 7r et K2 montrent que
la ponderation a fait diminuer le desaccord et augmenter legerement l’accord inter-annotateurs.

4.2 Calcul des distances entre categories £1 partir de la matrice de confusion

Pour ponderer l’accord inter-annotateurs, les distances entre categories sont deﬁnies a partir de connais-
sances prealables sur la tache d’annotation. En parallele, il nous semble utile de les evaluer egalement en
fonction de la difﬁculte qu’ont les annotateurs a repartir les elements entre les categories et de confronter
ces deux approches. Pour ce calcul, nous utilisons la matrice de confusion du tableau 1.

Nous considerons que deux categories sont distinctes s’il y a peu de chance d’erreur de classement entre
elles. Plus precisement, soient deux categories 01 et C2 appartenant a l’ensemble des categories conside-
rees, P(C'2|C1) represente la probabilite qu’un annotateur affecte un element a la categorie C2 sachant que
le deuxieme annotateur l’affecte a la categorie 01 et elle se calcule de la facon suivante :

P(C2|C1) : 71,101,202”: 71201402
1

avec n1g1,2g2 representant le nombre d’elements classes par l’annotateur 1 dans la categorie C1 alors que
l’annotateur 2 les a classes dans la categorie C2 ; ngl represente la somme des elements classes dans la
categorie 01 par les deux annotateurs.

Quand cette probabilite est faible, la categorie C2 est peu similaire a la categorie 01 et le risque d’obtenir
une classiﬁcation differente est faible. Nous avons ainsi, selon les donnees du tableau 1 :

13 8
P(New|F0rmer) = % = 0,108808

KAREN FORT, CLAIRE FRANCOIS, MAHA GHRIBI

/ Former New Rien

Former 0,735751 0,108808 0,155440
New 0,115385 0,758242 0,126374
Rien 0,000795 0,000609 0,998595

TAB. 5 — Tableau de probabilites

Dans le tableau 5, qui presente les valeurs de probabilite calculees pour notre cas d’application, la dia-
gonale permet d’estimer l’accord entre annotateurs pour chaque categorie. Il est tres important pour la
categorie Rien (99 % d’accord) et plus faible pour les categories Former et New (73 et 75 % d’accord res-
pectivement). Les autres cellules du tableau permettent d’estimer le desaccord entre annotateurs, categorie
par categorie. Ces probabilites sont tres faibles, les categories sont donc peu siIr1ilaires. Nous observons
egalement que ces probabilites sont asymetriques. Les valeurs P(F0rmer|Rz'en) et P(New|Rz'en) sont
tres faibles (<1%o), le risque d’affecter un element a la categorie Former ou a la categorie New sachant
qu’il est deja affecte a Rien est donc quasiment nul. Inversement, le risque d’affecter un element a la cate-
gorie Rien alors qu’il est deja dans la categorie Former ou dans la categorie New est plus eleve (15 % et
12 % respectivement).

Nous utilisons ces probabilites dans le calcul des distances entre categories selon le principe suivant :
d(C1, C1) = 0, et pour tout 01 different de 02, d(C1, C2) = 1 — P(C1|C2).

Les probabilites n’etant pas symetriques, cette formule ne peut pas etre utilisee telle quelle. En utilisant
les memes hypotheses sur les distributions des annotations que pour la deﬁnition des coefﬁcients, nous
proposons deux transformations. Premierement, le coefﬁcient oz suppose que, dans le cas d’une annotation
au hasard, les annotateurs realisent des distributions equivalentes. Nous deﬁnissons donc la distance as-
sociee comme etant la moyenne des distances orientees (calculees a partir du tableau 5) selon la formule
suivante :

_ (1 - P(02|01)) + (1 - P(O1|C2))

doz(C17 C2) 2

Ce qui donne, dans notre cas :

1 _ 13+8 + 1 _ 13+s
da(F0rmer, New) = ( 107+86) 2 ( 90+92) = 0, 887904

Le /cw, quant a lui, suppose que les annotateurs procedent de maniere differente, aussi nous calculons la
distance associee comme etant le produit des distances orientees (calculees a partir du tableau 5) selon la
formule suivante :

drew (C1: C2) = (1 — P(C2|C1)) X (1 — P(C1|C2))

Dans notre cas, nous obtenons donc :

13+8 13+8

g F ,N = r—————— 1-
“( °T"“” 6”” ( 107-+86)><( 90+-92

)=dm&m

Dans le tableau 6, nous observons que d (Former, New) est inferieure dans les deux cas a d (Former, Rien)
et d(New, Rien), ce qui semblerait conﬁrmer que Former et New sont plus proches entre elles que de Rien.
Cependant, ces valeurs sont nettement superieures aux valeurs estimees dans le tableau 4, la distinction
entre ces deux categories s’avere donc dans les faits moins problematique que ce que nous avions craint.

EVALUER DES ANNOTATIONS MANUELLES DISPERSEES

do dm,

d(Former,New) 0,887904 0,788362
d(Former,Rien) 0,92l882 0,843888
d(New,Rien) 0,936508 0,873094

TAB. 6 — Tableau des distances entre categories calculees a partir des donnees
5 Conclusion

A partir d’une campagne d’annotation, nous avons analyse differents modes de calcul pour estimer l’ac-
cord inter-annotateurs. La particularite de cette campagne est le caractere tres disperse des annotations
dans les textes qui induit un biais lie a la grande prevalence des tokens non annotes. La matrice de confu-
sion synthetise parfaitement cette information. Le tableau des distances calculees entre categories montre
que toutes les categories sont bien distinctes, meme les deux categories Ininoritaires mais signiﬁantes For-
mer et New. Bien que les coefﬁcients 7r, rt, 04 et nu, soient tres sensibles a ce biais de prevalence, ils restent
satisfaisants dans notre cas, indiquant ainsi un bon accord pour ces deux categories. En premiere approxi-
mation, nous pouvons estimer ce biais en comparant les resultats obtenus avec les matrices de confusion
suivantes : complete, reduite aux noms de genes et reduite aux deux categories signiﬁantes.

En complement des coefﬁcients, et quand le mode operatoire de l’annotation le permet, le premier resultat
a presenter est a notre avis la matrice de confusion accompagnee d’explications precises sur les choix
effectues. Nous rejoignons Hripcsak & Heitjan (2002) lorsqu’ils ecrivent “showing the two-by-two contin-
gency table with its marginal totals is probably as informative as any measure“. En effet, cette matrice
resume les informations quantitatives obtenues dans une campagne d’annotation et permet entre autres
d’avoir rapidement une idee des problemes de prevalence et de biais entre annotateurs.

Le tableau des distances entre categories calculees a partir des resultats d’annotation est egalement tres
riche en information car il permet d’analyser le risque reel d’erreur entre certaines categories et de le
confronter aux distances deﬁnies a priori en fonction des connaissances du domaine. Ces differents ta-
bleaux permettent de comprendre au Inieux les caracteristiques de la campagne d’annotation et d’interpre-
ter les differents coefﬁcients obtenus selon leur mode de calcul.

De nouvelles campagnes d’annotation sont en cours et devraient nous permettre de tester les differents
coefﬁcients ainsi que la reproductibilite de nos propositions dans des cas aussi varies que l’annotation de
brevets en pharmacologie (entites nommees, termes) ou des commentaires de matchs de football (entites
nommees, relations diverses). Ces campagnes devraient egalement nous permettre d’elargir la reﬂexion
a des annotations realisees par plus de deux annotateurs. Dans ce demier cas, le tableau des distances
entre categories calculees a partir des resultats d’annotation permettra de realiser une bonne synthese des
problemes existants pour distinguer les categories.

Remerciements

Ce travail a ete realise en partie dans le cadre du programme Quaero 7, ﬁnance par OSEO, agence natio-
nale de valorisation de la recherche. Nous en remercions les participants, en particulier l’equipe MIG de

7http://www.quaero.org

KAREN FORT, CLAIRE FRANCOIS, MAHA GHRIBI

l’INRA. Nous remercions également F. Tisserand et B. Taliercio, les annotateurs experts de l’INIST, ainsi
que Ron Artstein, pour son intérét et ses réponses détaillées.

Références

ALEX B., NISSIM M. & GROVER C. (2006). The impact of annotation on the performance of protein
tagging in biomedical text. In Proceedings of The Fifth International Conference on Language Resources
and Evaluation (LREC), p. 595-600, Gene, Italie.

ARTSTEIN R. & POESIO M. (2008). Inter-coder agreement for computational linguistics. Computational
Linguistics, 34(4), 555-596.

BENNETT E. M., ALPERT R. & C.GOLDSTEIN A. (1954). Communications through limited questio-
ning. Public Opinion Quarterly, 18(3), 303-308.

CARLETTA J. (1996). Assessing agreement on classiﬁcation tasks : The kappa statistic. Computational
Linguistics, 22, 249-254.

COHEN J. (1960). A coefﬁcient of agreement for nominal scales. Educational and Psychological Mea-
surement, 20(1), 37-46.

COHEN J. (1968). Weighted kappa : Nominal scale agreement with provision for scaled disagreement or
partial credit. Psychological Bulletin, 70(4), 213-220.

DI EUGENIO B. & GLASS M. (2004). The kappa statistic : a second look. Computational Linguistics,
30(1), 95-101.

FINN R. H. (1970). A note on estimating the reliability of categorical data. Educational and Psycholo-
gical Measurement, 30, 71-76.

FORT K., EHRMANN M. & NAZARENKO A. (2009). Vers une méthodologie d’annotation des entités
nommées en corpus ? In Actes de la I 6eme Conférence sur le Traitement Automatique des Langues
Naturelles 2009 Traitement Automatique des Langues Naturelles 2009, Senlis, France.

GUT U. & BAYERL P. S. (2004). Measuring the reliability of manual annotations of speech corpora. In
Proceedings of Speech Prosody, p. 565-568, Nara, J apon.

HRIPCSAK G. & HEITJAN D. F. (2002). Measuring agreement in medical informatics reliability studies.
Journal of Biomedical Informatics, 35(2), 99-110.

KRIPPENDORFF K. (1980). Content Analysis .' An Introduction to Its Methodology, chapter 12. Sage :
Beverly Hills, CA.

KRIPPENDORFF K. (2004). Content Analysis .' An Introduction to Its Methodology, second edition,
chapter 11. Sage : Thousand Oaks, CA.

LAIGNELET M. & RIOULT F. (2009). Repérer automatiquement les segments obsolescents a l’aide
d’indices sémantiques et discursifs. In Actes de Traitement Automatique des Langues Naturelles (TALN
2009), Senlis, France.

REIDSMA D. & CARLETTA J . (2008). Reliability measurement without liIr1its. Computational Linguis-
tics, 34(3), 319-326.

SCOTT W. A. (1955). Reliability of content analysis : The case of nominal scale coding. Public Opinion
Quaterly, 19(3), 321-325.

SIEGEL S. & CASTELLAN N. J . (1988). Nonparametric Statistics for the Behavioral Sciences. New
York : McGraw-Hill, 2nd edition.

