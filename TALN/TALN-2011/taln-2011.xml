<?xml version="1.0" encoding="UTF-8"?>
<conference>
	<edition>
		<acronyme>TALN'2011</acronyme>
		<titre>18e conférence sur le Traitement Automatique des Langues Naturelles</titre>
		<ville>Montpellier</ville>
		<pays>France</pays>
		<dateDebut>2011-06-27</dateDebut>
		<dateFin>2011-07-01</dateFin>
		<presidents>
			<president>
				<prenom>Mathieu</prenom>
				<nom>Lafourcade</nom>
			</president>
			<president>
				<prenom>Violaine</prenom>
				<nom>Prince</nom>
			</president>
		</presidents>
		<typeArticles>
			<type id="invite">Invités</type>
			<type id="long">Papiers longs</type>
			<type id="court">Papiers courts</type>
			<type id="démonstration">Démonstrations</type>
		</typeArticles>
		<siteWeb>http://www.lirmm.fr/~lopez/TALN2011/</siteWeb>
	</edition>
	<articles>
		<article id="taln-2011-invite-001" session="Conférenciers invités">
			<auteurs>
				<auteur>
					<prenom>Vladimir A.</prenom>
					<nom>Fomichov</nom>
					<email>vfomichov@hse.ru</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Department of Innovations and Business in the Sphere of Informational Technologies, Faculty of Business Informatics, National Research University “Higher School of Economics”, Kirpichnaya str. 33, 105679 Moscow, Russia</affiliation>
			</affiliations>
			<titre></titre>
			<type>invite</type>
			<pages></pages>
			<resume>L’article décrit la structure et les applications possibles de la théorie des K-représentations (représentation des connaissances) dans la bioinformatique afin de développer un Réseau Sémantique d’une génération nouvelle. La théorie des K-répresentations est une théorie originale du développement des analyseurs sémantico–syntactiques avec l’utilisation large des moyens formels pour décrire les données d’entrée, intermédiaires et de sortie. Cette théorie est décrit dans la monographie de V. Fomichov (Springer, 2010). La première partie de la théorie est un modèle formel d’un système qui est composé de dix opérations sur les structures conceptuelles. Ce modèle définit une classe nouvelle des langages formels – la classe des SK-langages. Les possibilités larges de construire des répresentations sémantiques des discours compliqués en rapport à la biologie sont manifestes. Une approche formelle nouvelle de l’élaboration des analysateurs multilinguistiques sémantico-syntactiques est décrite. Cet approche a été implémentée sous la forme d'un programme en langage PYTHON.</resume>
			<mots_cles>dialogue homme-machine en langage naturel, algorithme de l‟analyse sémantico-syntactique, sémantique intégrale formelle, théorie des K-représentations, SK-langues, représentation sémantique, bases de données linguistiques, réseau sémantique d’une génération nouvelle, réseau sémantique multilingue, bioinformatique</mots_cles>
			<title>The prospects revealed by the theory of K-representations for bioinformatics and Semantic Web</title>
			<abstract>The paper describes the structure and possible applications of the theory of K-representations (knowledge representations) in bioinformatics and in the development of a Semantic Web of a new generation. It is an original theory of designing semantic-syntactic analyzers of natural language (NL) texts with the broad use of formal means for representing input, intermediary, and output data. The current version of the theory is set forth in a monograph by V. Fomichov (Springer, 2010). The first part of the theory is a formal model describing a system consisting of ten operations on conceptual structures. This model defines a new class of formal languages – the class of SK-languages. The broad possibilities of constructing semantic representations of complex discourses pertaining to biology are shown. A new formal approach to developing multilingual algorithms of semantic-syntactic analysis of NL-texts is outlined. This approach is realized by means of a program in the language PYTHON.</abstract>
			<keywords>man-machine natural language dialogue, algorithm of semantic-syntactic analysis, integral formal semantics, theory of K-representations, SK-languages, semantic representation, text meaning representation, linguistic database, Semantic Web of a new generation, multilingual Semantic Web, bioinformatics</keywords>
		</article>
		<article id="taln-2011-invite-002" session="Conférenciers invités">
			<auteurs>
				<auteur>
					<prenom>Nicholas</prenom>
					<nom>Asher</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LILac, IRIT, Université Paul Sabatier</affiliation>
			</affiliations>
			<titre>Theorie et Praxis Une optique sur les travaux en TAL sur le discours et le dialogue</titre>
			<type>invite</type>
			<pages></pages>
			<resume></resume>
			<mots_cles></mots_cles>
			<title></title>
			<abstract>Discourse parsing is a relatively new field and it differs from parsing in syntax in its pedegree. Parsing and computational models of syntax have the benefit of 50 years of research in generative syntax and reactions to it. Discourse parsing has on the other hand little conceptual help from linguistics or philosophy. Though impressive gains have been registered in discourse parsing with superficial features, theoretical not really come to grips with the theoretical underpinnings of text interpretation, and its interaction especially with lexical semantics, a rather neglected branch of formal semantics. In my talk I will assess the interaction between theoretical linguistics, formal methods, and experimental work on discourse structure and interpretation. Sounding a note of optimism, I will then turn to assessing the situation for the computational analysis of dialogue. I will argue that the view that we are saddled with from Grice and the philosophy of the seventies is inadequate and is great need of revision from work on communication from economics and theoretical computer science</abstract>
			<keywords></keywords>
		</article>
		<article id="taln-2011-invite-003" session="Conférenciers invités">
			<auteurs>
				<auteur>
					<prenom>Claire</prenom>
					<nom>Gardent</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">CNRS/LORIA, Nancy (France)</affiliation>
				<affiliation affiliationId="2"></affiliation>
				<affiliation affiliationId="3"></affiliation>
			</affiliations>
			<titre></titre>
			<type>invite</type>
			<pages></pages>
			<resume></resume>
			<mots_cles></mots_cles>
			<title>Sentence Generation: Input, Algorithms and Applications</title>
			<abstract>Sentence Generation maps abstract linguistic representations into sentences. A necessary part of any natural language generation system, sentence generation has also recently received increasing attention in applications such as transfer based machine translation (cf. the LOGON project) and natural language interfaces to knowledge bases (e.g., to verbalise, to author and/or to query ontologies). One outstanding issue in Sentence Generation is what it starts from. What is the abstract linguistic representation it generates from? In my talk, I will explore sentence generation from two main input formats (flat semantic formulae and dependency structures) and discuss their impact on efficiency, algorithms and applications. I will start by describing an algorithm that generates from flat semantic formulae, explain why it is computationally intractable and presenting ways of optimising it to make it usable in practice. I will then show how this algorithm can be used to generate paraphrases; to support error mining and to generate teaching material for language learners from an ontology. In the second part of the talk, I will focus on generation from dependency structures. Based on the input data recently made available by the Generation Challenges Surface Realisation Shared Task, I will show how the algorithm previously used to generate from flat semantic formulae can be adapted to generate from dependency structures. I will moreover discuss various issues raised by the GenChal data such as, missing lexical entries and mismatches between dependency and grammar structures.</abstract>
			<keywords></keywords>
		</article>
		<article id="taln-2011-long-001" session="Fouille de textes et applications">
			<auteurs>
				<auteur>
					<prenom>Michael</prenom>
					<nom>Zock</nom>
					<email>michael.zock@lif.univ-mrs.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Guy</prenom>
					<nom>Lapalme</nom>
					<email>2</email>
					<affiliationId>lapalme@iro.umontreal.ca</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">CNRS – LIF (Aix-Marseille II), Laboratoire d’Informatique Fondamentale, Case 901, 163 avenue de Luminy,, F-13288 Marseille Cedex 9</affiliation>
				<affiliation affiliationId="2">RALI-DIRO, Université de Montréal, CP 6128, Succ. Centre-Ville, Montréal, QC Canada H3C 3J7</affiliation>
			</affiliations>
			<titre>Patrons de phrase, raccourcis pour apprendre rapidement à parler une nouvelle langue</titre>
			<type>long</type>
			<pages></pages>
			<resume>Nous décrivons la création d'un environnement web pour aider des apprenants (adolescents ou adultes) à acquérir les automatismes nécessaires pour produire à un débit “normal” les structures fondamentales d’une langue. Notre point de départ est une base de données de phrases, glanées sur le web ou issues de livres scolaires ou de livres de phrases. Ces phrases ont été généralisées (remplacement de mots par des variables) et indexées en termes de buts pour former une arborescence de patrons. Ces deux astuces permettent de motiver l'usage des patrons et de crééer des phrases structurellement identiques à celles rencontrées, tout en étant sémantiquement différentes. Si les notions de 'patrons' ou de 'phrases à trou implicitement typées' ne sont pas nouvelles, le fait de les avoir portées sur ordinateur pour apprendre des langues l'est. Le système étant conçu pour être ouvert, il permet aux utilisateurs, concepteurs ou apprenants, des changements sur de nombreux points importants : le nom des variables, leurs valeurs, le laps de temps entre une question et sa réponse, etc. La version initiale a été développée pour l’anglais et le japonais. Pour tester la généricité de notre approche nous y avons ajouté relativement facilement le français et le chinois.</resume>
			<mots_cles>apprentissage de langues, production de langage, livres de phrases, patrons, schéma de phrase, structures fondamentales</mots_cles>
			<title></title>
			<abstract>We describe a web application to assist language learners (teenagers or adults) to acquire the needed skills to produce at a ‘normal’ rate the fundamental structures of a new language, the scope being the survival level. The starting point is a database of sentences gleaned in textbooks, phrasebooks, or the web. We propose to extend the applicability of these structures by generalizing them: concrete sentences becoming productive sentence patterns. In order to produce such generic structures (schemata), we index the sentences in terms of goals, replacing specific elements (words) of the chain by more general terms (variables). This allows the user not only to acquire these structures, but also to express his/her own thoughts. Starting from a communicative goal, he instantiates the variables of the associated schema with words of his choice. We have developed a prototype for English and Japanese, adding Chinese and French without too many problems.</abstract>
			<keywords>foreign language learning, language production, phrasebook, sentence patterns, basic structure</keywords>
		</article>
		<article id="taln-2011-long-002" session="Fouille de textes et applications">
			<auteurs>
				<auteur>
					<prenom>Eric</prenom>
					<nom>Charton</nom>
					<email>eric.charton@polymtl.ca</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Michel</prenom>
					<nom>Gagnon</nom>
					<email>michel.gagnon@polymtl.ca</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Benoit</prenom>
					<nom>Ozell</nom>
					<email>benoit.ozell@polymtl.ca</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">École Polytechnique, 2900 boul. Edouard Montpetit, Montréal, Canada</affiliation>
			</affiliations>
			<titre>Génération automatique de motifs de détection d’entités nommées en utilisant des contenus encyclopédiques</titre>
			<type>long</type>
			<pages></pages>
			<resume>Les encyclopédies numériques contiennent aujourd’hui de vastes inventaires de formes d’écritures pour des noms de personnes, de lieux, de produits ou d’organisation. Nous présentons un système hybride de détection d’entités nommées qui combine un classifieur à base de Champs Conditionnel Aléatoires avec un ensemble de motifs de détection extraits automatiquement d’un contenu encyclopédique. Nous proposons d’extraire depuis des éditions en plusieurs langues de l’encyclopédie Wikipédia de grandes quantités de formes d’écriture que nous utilisons en tant que motifs de détection des entités nommées. Nous décrivons une méthode qui nous assure de ne conserver dans cette ressources que des formes non ambiguës susceptibles de venir renforcer un système de détection d’entités nommées automatique. Nous procédons à un ensemble d’expériences qui nous permettent de comparer un système d’étiquetage à base de CRF avec un système utilisant exclusivement des motifs de détection. Puis nous fusionnons les résultats des deux systèmes et montrons qu’un gain de performances est obtenu grâce à cette proposition.</resume>
			<mots_cles>Étiqueteur, Entités nommées, Lexiques</mots_cles>
			<title></title>
			<abstract>Encyclopedic content can provide numerous samples of surface writing forms for persons, places, products or organisations names. In this paper we present an hybrid named entities recognition system based on a gazetteer automatically extracted. We propose to extract it from various language editions ofWikipedia encyclopedia. The wide amount of surface forms extracted from this encyclopedic content is then used as detection pattern of named entities.We build a labelling tool using those patterns. This labelling tool is used as simple pattern detection component, to combine with a Conditional Random Field tagger.We compare the performances of each component of our system with the results previously obtained by various systems in the French NER campaign ESTER 2. Finally, we show that the fusion of a CRF label tool with a pattern based ones, can improve the global performances of a named entity recognition system.</abstract>
			<keywords>Tagger, Named entities, Gazetteer</keywords>
		</article>
		<article id="taln-2011-long-003" session="Fouille de textes et applications">
			<auteurs>
				<auteur>
					<prenom>Cédric</prenom>
					<nom>Lopez</nom>
					<email>lopez@lirmm.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Mathieu</prenom>
					<nom>Roche</nom>
					<email>mroche@lirmm.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIRMM, 161, rue ADA 34392 Montpellier Cedex 5</affiliation>
			</affiliations>
			<titre>Approche de construction automatique de titres courts par des méthodes de Fouille du Web</titre>
			<type>long</type>
			<pages></pages>
			<resume>Le titrage automatique de documents textuels est une tâche essentielle pour plusieurs applications (titrage de mails, génération automatique de sommaires, synthèse de documents, etc.). Cette étude présente une méthode de construction de titres courts appliquée à un corpus d’articles journalistiques via des méthodes de Fouille du Web. Il s’agit d’une première étape cruciale dans le but de proposer une méthode de construction de titres plus complexes. Dans cet article, nous présentons une méthode proposant des titres tenant compte de leur cohérence par rapport au texte, par rapport au Web, ainsi que de leur contexte dynamique. L’évaluation de notre approche indique que nos titres construits automatiquement sont informatifs et/ou accrocheurs.</resume>
			<mots_cles>Traitement Automatique du Langage Naturel, Fouille du Web, Titrage automatique</mots_cles>
			<title></title>
			<abstract>The automatic titling of text documents is an essential task for several applications (automatic titling of e-mails, summarization, and so forth). This study presents a method of generation of short titles applied to a corpus of journalistic articles using methods ofWeb Mining. It is a first crucial stage with the aim of proposing a method of generation of more complex titles. In this article, we present a method that proposes titles taking into account their coherence in connection with the text and the Web, as well as their dynamic context. The evaluation of our approach indicates that our titles generated automatically are informative and/or catchy.</abstract>
			<keywords>Natural Language Processing, Web Mining, Automatic Titling</keywords>
		</article>
		<article id="taln-2011-long-004" session="Fouille de textes et applications">
			<auteurs>
				<auteur>
					<prenom>Ludovic</prenom>
					<nom>Jean-Louis</nom>
					<email>ludovic.jean-louis@cea.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Romaric</prenom>
					<nom>Besançon</nom>
					<email>romaric.besancon@cea.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Olivier</prenom>
					<nom>Ferret</nom>
					<email>olivier.ferret@cea.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Adrien</prenom>
					<nom>Durand</nom>
					<email>adrien.durand@cea.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">CEA, LIST, Laboratoire Vision et Ingénierie des Contenus, Fontenay-aux-Roses, F-92265, France</affiliation>
			</affiliations>
			<titre>Une approche faiblement supervisée pour l’extraction de relations à large échelle</titre>
			<type>long</type>
			<pages></pages>
			<resume>Les systèmes d’extraction d’information traditionnels se focalisent sur un domaine spécifique et un nombre limité de relations. Les travaux récents dans ce domaine ont cependant vu émerger la problématique des systèmes d’extraction d’information à large échelle. À l’instar des systèmes de question-réponse en domaine ouvert, ces systèmes se caractérisent à la fois par le traitement d’un grand nombre de relations et par une absence de restriction quant aux domaines abordés. Dans cet article, nous présentons un système d’extraction d’information à large échelle fondé sur un apprentissage faiblement supervisé de patrons d’extraction de relations. Cet apprentissage repose sur la donnée de couples d’entités en relation dont la projection dans un corpus de référence permet de constituer la base d’exemples de relations support de l’induction des patrons d’extraction. Nous présentons également les résultats de l’application de cette approche dans le cadre d’évaluation défini par la tâche KBP de l’évaluation TAC 2010.</resume>
			<mots_cles>extraction d’information, extraction de relations</mots_cles>
			<title></title>
			<abstract>Standard Information Extraction (IE) systems are designed for a specific domain and a limited number of relations. Recent work has been undertaken to deal with large-scale IE systems. Such systems are characterized by a large number of relations and no restriction on the domain, which makes difficult the definition of manual resources or the use of supervised techniques. In this paper, we present a large-scale IE system based on a weakly supervised method of pattern learning. This method uses pairs of entities known to be in relation to automatically extract example sentences from which the patterns are learned. We present the results of this system on the data from the KBP task of the TAC 2010 evaluation campaign.</abstract>
			<keywords>information extraction, relation extraction</keywords>
		</article>
		<article id="taln-2011-long-005" session="Fouille de textes et applications">
			<auteurs>
				<auteur>
					<prenom>Stéphane</prenom>
					<nom>Huet</nom>
					<email>stephane.huet,@univ-avignon.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Florian</prenom>
					<nom>Boudin</nom>
					<email>florian.boudin@univ-avignon.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Juan-Manuel</prenom>
					<nom>Torres-Moreno</nom>
					<email>juan-manuel.torres@univ-avignon.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
					<affiliationId>3</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIA, Université d’Avignon, France</affiliation>
				<affiliation affiliationId="2">École Polytechnique de Montréal, Canada</affiliation>
				<affiliation affiliationId="3">GIL-IINGEN, Universidad Nacional Autónoma de México, Mexique</affiliation>
			</affiliations>
			<titre>Utilisation d’un score de qualité de traduction pour le résumé multi-document cross-lingue</titre>
			<type>long</type>
			<pages></pages>
			<resume>Le résumé automatique cross-lingue consiste à générer un résumé rédigé dans une langue différente de celle utilisée dans les documents sources. Dans cet article, nous proposons une approche de résumé automatique multi-document, basée sur une représentation par graphe, qui prend en compte des scores de qualité de traduction lors du processus de sélection des phrases. Nous évaluons notre méthode sur un sous-ensemble manuellement traduit des données utilisées lors de la campagne d’évaluation internationale DUC 2004. Les résultats expérimentaux indiquent que notre approche permet d’améliorer la lisibilité des résumés générés, sans pour autant dégrader leur informativité.</resume>
			<mots_cles>Résumé cross-lingue, qualité de traduction, graphe</mots_cles>
			<title></title>
			<abstract>Cross-language summarization is the task of generating a summary in a language different from the language of the source documents. In this paper, we propose a graph-based approach to multi-document summarization that integrates machine translation quality scores in the sentence selection process. We evaluate our method on a manually translated subset of the DUC 2004 evaluation campaign. Results indicate that our approach improves the readability of the generated summaries without degrading their informativity.</abstract>
			<keywords>Cross-lingual summary, translation quality, graph</keywords>
		</article>
		<article id="taln-2011-long-006" session="Fouille de textes et applications">
			<auteurs>
				<auteur>
					<prenom>Cyril</prenom>
					<nom>Grouin</nom>
					<email>cyril.grouin@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Louise</prenom>
					<nom>Deléger</nom>
					<email>louise.delegeR@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Bruno</prenom>
					<nom>Cartoni</nom>
					<email>bruno.cartoni@unige.ch</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Sophie</prenom>
					<nom>Rosset</nom>
					<email>sophie.rosset@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Pierre</prenom>
					<nom>Zweigenbaum</nom>
					<email>pierre.zweigenbaum@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIMSI-CNRS, BP133, 91403 Orsay Cedex, France</affiliation>
				<affiliation affiliationId="2">Département de Linguistique, Université de Genève, Suisse</affiliation>
			</affiliations>
			<titre>Accès au contenu sémantique en langue de spécialité : extraction des prescriptions et concepts médicaux</titre>
			<type>long</type>
			<pages></pages>
			<resume>Pourtant essentiel pour appréhender rapidement et globalement l’état de santé des patients, l’accès aux informations médicales liées aux prescriptions médicamenteuses et aux concepts médicaux par les outils informatiques se révèle particulièrement difficile. Ces informations sont en effet généralement rédigées en texte libre dans les comptes rendus hospitaliers et nécessitent le développement de techniques dédiées. Cet article présente les stratégies mises en oeuvre pour extraire les prescriptions médicales et les concepts médicaux dans des comptes rendus hospitaliers rédigés en anglais. Nos systèmes, fondés sur des approches à base de règles et d’apprentissage automatique, obtiennent une F1-mesure globale de 0,773 dans l’extraction des prescriptions médicales et dans le repérage et le typage des concepts médicaux.</resume>
			<mots_cles>Extraction d’information, Indexation contrôlée, Informatique médicale, Concepts médicaux, Prescriptions</mots_cles>
			<title></title>
			<abstract>While essential for rapid access to patient health status, computer-based access to medical information related to prescriptions key medical expressed and concepts proves to be difficult. This information is indeed generally in free text in the clinical records and requires the development of dedicated techniques. This paper presents the strategies implemented to extract medical prescriptions and concepts in clinical records written in English language. Our systems, based upon linguistic patterns and machine-learning approaches, achieved a global F1-measure of 0.773 for extraction of medical prescriptions, and of clinical concepts.</abstract>
			<keywords>Information extraction, Controled indexing, Medical informatics, Clinical concepts, Prescriptions</keywords>
		</article>
		<article id="taln-2011-long-007" session="Parole">
			<auteurs>
				<auteur>
					<prenom>Bassam</prenom>
					<nom>Jabaian</nom>
					<email>bassam.jabaian@imag.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Laurent</prenom>
					<nom>Besacier</nom>
					<email>laurent.besacier@imag.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Fabrice</prenom>
					<nom>Lefèvre</nom>
					<email>fabrice.lefevre@univ-avignon.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIG, University Joseph Fourier, Grenoble - France</affiliation>
				<affiliation affiliationId="2">LIA, University of Avignon, Avignon - France</affiliation>
			</affiliations>
			<titre>Comparaison et combinaison d’approches pour la portabilité vers une nouvelle langue d’un système de compréhension de l’oral</titre>
			<type>long</type>
			<pages></pages>
			<resume>Dans cet article, nous proposons plusieurs approches pour la portabilité du module de compréhension de la parole (SLU) d’un système de dialogue d’une langue vers une autre. On montre que l’utilisation des traductions automatiques statistiques (SMT) aide à réduire le temps et le cout de la portabilité d’un tel système d’une langue source vers une langue cible. Pour la tache d’étiquetage sémantique on propose d’utiliser soit les champs aléatoires conditionnels (CRF), soit l’approche à base de séquences (PH-SMT). Les résultats expérimentaux montrent l’efficacité des méthodes proposées pour une portabilité rapide du SLU vers une nouvelle langue. On propose aussi deux méthodes pour accroître la robustesse du SLU aux erreurs de traduction. Enfin on montre que la combinaison de ces approches réduit les erreurs du système. Ces travaux sont motivés par la disponibilité du corpus MEDIA français et de la traduction manuelle vers l’italien d’une sous partie de ce corpus.</resume>
			<mots_cles>Système de dialogue, compréhension de la parole, portabilité à travers les langues, traduction automatique statistique</mots_cles>
			<title></title>
			<abstract>In this paper we investigate several approaches for language portability of the spoken language understanding (SLU) module of a dialogue system. We show that the use of statistical machine translation (SMT) can reduce the time and the cost of porting a system from a source to a target language. For conceptual decoding we propose to use even conditional random fields (CRF) or phrase based statistical machine translation PB-SMT). The experimental results show the efficiency of the proposed methods for a fast and low cost SLU language portability. Also we proposed two methods to increase SLU robustness to translation errors. Overall we show that the combination of all these approaches reduce the concept error rate. This work was motivated by the availability of the MEDIA French corpus and the manual translation of a subset of this corpus into Italian.</abstract>
			<keywords>Spoken Dialogue Systems, Spoken Language Understanding, Language Portability, Statistical Machine Translation</keywords>
		</article>
		<article id="taln-2011-long-008" session="Parole">
			<auteurs>
				<auteur>
					<prenom>Thierry</prenom>
					<nom>Bazillon</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Benjamin</prenom>
					<nom>Maza</nom>
					<email></email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Mickael</prenom>
					<nom>Rouvier</nom>
					<email></email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Frédéric</prenom>
					<nom>Béchet</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Alexis</prenom>
					<nom>Nasr</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Aix Marseille Université, LIF-CNRS, Marseille, France</affiliation>
				<affiliation affiliationId="2">Université d’Avignon, LIA-CERI, Avignon, France</affiliation>
			</affiliations>
			<titre>Qui êtes-vous ? Catégoriser les questions pour déterminer le rôle des locuteurs dans des conversations orales</titre>
			<type>long</type>
			<pages></pages>
			<resume>La fouille de données orales est un domaine de recherche visant à caractériser un flux audio contenant de la parole d’un ou plusieurs locuteurs, à l’aide de descripteurs liés à la forme et au contenu du signal. Outre la transcription automatique en mots des paroles prononcées, des informations sur le type de flux audio traité ainsi que sur le rôle et l’identité des locuteurs sont également cruciales pour permettre des requêtes complexes telles que : « chercher des débats sur le thème X », « trouver toutes les interviews de Y », etc. Dans ce cadre, et en traitant des conversations enregistrées lors d’émissions de radio ou de télévision, nous étudions la manière dont les locuteurs expriment des questions dans les conversations, en partant de l’intuition initiale que la forme des questions posées est une signature du rôle du locuteur dans la conversation (présentateur, invité, auditeur, etc.). En proposant une classification du type des questions et en utilisant ces informations en complément des descripteurs généralement utilisés dans la littérature pour classer les locuteurs par rôle, nous espérons améliorer l’étape de classification, et valider par la même occasion notre intuition initiale.</resume>
			<mots_cles>Fouille de données orales, Traitement Automatique de la Parole, Annotation de corpus oraux, Classification en rôles de locuteurs</mots_cles>
			<title></title>
			<abstract>Speech Data Mining is an area of research dedicated to characterize audio streams containing speech of one or more speakers, using descriptors related to the form and the content of the speech signal. Besides the automatic word transcription process, information about the type of audio stream and the role and identity of speakers is also crucial to allow complex queries such as : “ seek debates on X ,”“ find all the interviews of Y”, etc. In this framework we present a study done on broadcast conversations on how speakers express questions in conversations, starting with the initial intuition that the form of the questions uttered is a signature of the role of the speakers in the conversation (anchor, guest, expert, etc.). By classifying these questions thanks to a set of labels and using this information in addition to the commonly used descriptors to classify users’ role in broadcast conversations, we want to improve the role classification accuracy and validate our initial intuition.</abstract>
			<keywords>Speech data mining, Automatic Speech Processing, Speech Corpus Annotation, Speaker role classification</keywords>
		</article>
		<article id="taln-2011-long-009" session="Sémantique">
			<auteurs>
				<auteur>
					<prenom>Charles</prenom>
					<nom>Teissèdre</nom>
					<email>charles.teissedre@ u-paris10.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Delphine</prenom>
					<nom>Battistelli</nom>
					<email>delphine.battistelli@paris-sorbonne.fr</email>
					<affiliationId>3</affiliationId>
				</auteur>
				<auteur>
					<prenom>Jean-Luc</prenom>
					<nom>Minel</nom>
					<email>jean-luc.minel@u-paris10.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">MoDyCo - UMR 7114 CNRS, Paris Ouest Nanterre La Défense, 200, av. de la République, 92001 Nanterre</affiliation>
				<affiliation affiliationId="2">>Mondeca, 3, cité Nollez, 75018 Paris</affiliation>
				<affiliation affiliationId="3">STIH, Université Paris Sorbonne, 28, rue Serpente, 75006 Paris</affiliation>
			</affiliations>
			<titre>Recherche d’information et temps linguistique : une heuristique pour calculer la pertinence des expressions calendaires</titre>
			<type>long</type>
			<pages></pages>
			<resume>A rebours de bon nombre d’applications actuelles offrant des services de recherche d’information selon des critères temporels - applications qui reposent, à y regarder de près, sur une approche consistant à filtrer les résultats en fonction de leur inclusion dans une fenêtre de temps, nous souhaitons illustrer dans cet article l’intérêt d’un service s’appuyant sur un calcul de similarité entre des expressions adverbiales calendaires. Nous décrivons une heuristique pour mesurer la pertinence d’un fragment de texte en prenant en compte la sémantique des expressions calendaires qui y sont présentes. A travers la mise en oeuvre d’un système de recherche d’information, nous montrons comment il est possible de tirer profit de l’indexation d’expressions calendaires présentes dans les textes en définissant des scores de pertinence par rapport à une requête. L’objectif est de faciliter la recherche d’information en offrant la possibilité de croiser des critères de recherche thématique avec des critères temporels.</resume>
			<mots_cles>Indexation d’informations calendaires, Recherche d’information, Annotation et extraction d’expressions calendaires</mots_cles>
			<title></title>
			<abstract>Unlike many nowadays applications providing Information Retrieval services able to handle temporal criteria - applications which usually filter results after testing their inclusion in a time span, this paper illustrates the interest of a service based on a calculation of similarity between calendar adverbial phrases. We describe a heuristic to measure the relevance of a fragment of text by taking into account the semantics of calendar expressions. Through the implementation of an Information Retrieval system, we show how it is possible to take advantage of the indexing of calendar expressions found in texts by setting scores of relevance with respect to a query. The objective is to ease Information Retrieval by offering the possibility of crossing thematic research criteria with temporal criteria.</abstract>
			<keywords>Calendar information indexing, Information Retrieval, Annotation and extraction of calendar expressions</keywords>
		</article>
		<article id="taln-2011-long-010" session="Sémantique">
			<auteurs>
				<auteur>
					<prenom>Ismaïl</prenom>
					<nom>El Maarouf</nom>
					<email>ismail.el-maarouf@univ-ubs.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Jeanne</prenom>
					<nom>Villaneau</nom>
					<email>jeanne.villaneau@univ-ubs.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Sophie</prenom>
					<nom>Rosset</nom>
					<email>sophie.rosset@limsi.fr</email>
					<affiliationId>3</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">HCTI UBS-UEB, Centre de Recherche Christiaan Huygens, 56321 Lorient</affiliation>
				<affiliation affiliationId="2">Valoria UBS-UEB, Rue Yves Mainguy, Campus de Tohannic 56017 Vannes cedex</affiliation>
				<affiliation affiliationId="3">LIMSI-CNRS, F-91403 Orsay Cedex</affiliation>
			</affiliations>
			<titre>Extraction de patrons sémantiques appliquée à la classification d'Entités Nommées</titre>
			<type>long</type>
			<pages></pages>
			<resume>La variabilité des corpus constitue un problème majeur pour les systèmes de reconnaissance d'entités nommées. L'une des pistes possibles pour y remédier est l'utilisation d'approches linguistiques pour les adapter à de nouveaux contextes : la construction de patrons sémantiques peut permettre de désambiguïser les entités nommées en structurant leur environnement syntaxico-sémantique. Cet article présente une première réalisation sur un corpus de presse d'un système de correction. Après une étape de segmentation sur des critères discursifs de surface, le système extrait et pondère les patrons liés à une classe d'entité nommée fournie par un analyseur. Malgré des modèles encore relativement élémentaires, les résultats obtenus sont encourageants et montrent la nécessité d'un traitement plus approfondi de la classe Organisation.</resume>
			<mots_cles>entités nommées, patrons sémantiques, segmentation discursive de surface</mots_cles>
			<title></title>
			<abstract>Corpus variation is a major problem for named entity recognition systems. One possible direction to tackle this problem involves using linguistic approaches to adapt them to unseen contexts : building semantic patterns may help for their disambiguation by structuring their syntactic and semantic environment. This article presents a preliminary implementation on a press corpus of a correction system. After a segmentation step based on surface discourse clues, the system extracts and weights the patterns linked to a named entity class provided by an analyzer. Despite relatively elementary models, the results obtained are promising and point on the necessary treatment of the Organisation class.</abstract>
			<keywords>named entities, semantic patterns, surface discourse segmentation</keywords>
		</article>
		<article id="taln-2011-long-011" session="Sémantique">
			<auteurs>
				<auteur>
					<prenom>Didier</prenom>
					<nom>Schwab</nom>
					<email>didier.schwab@imag.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Jérôme</prenom>
					<nom>Goulian</nom>
					<email>jerome.goulian@imag.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Nathan</prenom>
					<nom>Guillaume</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIG-GETALP (Laboratoire d’Informatique de Grenoble, Groupe d’Étude pour la Traduction/le Traitement Automatique des Langues et de la Parole), Université Pierre Mendès France, Grenoble 2</affiliation>
			</affiliations>
			<titre>Désambiguïsation lexicale par propagation de mesures sémantiques locales par algorithmes à colonies de fourmis</titre>
			<type>long</type>
			<pages></pages>
			<resume>Effectuer une tâche de désambiguïsation lexicale peut permettre d’améliorer de nombreuses applications du traitement automatique des langues comme l’extraction d’informations multilingues, ou la traduction automatique. Schématiquement, il s’agit de choisir quel est le sens le plus approprié pour chaque mot d’un texte. Une des approches classiques consiste à estimer la proximité sémantique qui existe entre deux sens de mots puis de l’étendre à l’ensemble du texte. La méthode la plus directe donne un score à toutes les paires de sens de mots puis choisit la chaîne de sens qui a le meilleur score. La complexité de cet algorithme est exponentielle et le contexte qu’il est calculatoirement possible d’utiliser s’en trouve réduit. Il ne s’agit donc pas d’une solution viable. Dans cet article, nous nous intéressons à une autre méthode, l’adaptation d’un algorithme à colonies de fourmis. Nous présentons ses caractéristiques et montrons qu’il permet de propager à un niveau global les résultats des algorithmes locaux et de tenir compte d’un contexte plus long et plus approprié en un temps raisonnable.</resume>
			<mots_cles>Désambiguïsation lexicale, Algorithmes à colonies de fourmis, Mesures sémantiques</mots_cles>
			<title></title>
			<abstract>Word sense disambiguation can lead to significant improvement in many Natural Language Processing applications as Machine Translation or Multilingual Information Retrieval. Basically, the aim is to choose for each word in a text its best sense. One of the most popular method estimates local semantic relatedness between two word senses and then extends it to the whole text. The most direct method computes a rough score for every pair of word senses and chooses the lexical chain that has the best score. The complexity of this algorithm is exponential and the context that it is computationally possible to use is reduced. Brute force is therefore not a viable solution. In this paper, we focus on another method : the adaptation of an ant colony algorithm. We present its features and show that it can spread at a global level the results of local algorithms and consider a longer and more appropriate context in a reasonable time.</abstract>
			<keywords>Lexical Disambiguation, Ant colony algorithms, Semantic relatedness</keywords>
		</article>
		<article id="taln-2011-long-012" session="Lexique et Corpus">
			<auteurs>
				<auteur>
					<prenom>Benoît</prenom>
					<nom>Sagot</nom>
					<email>benoit.sagot@inria.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Karën</prenom>
					<nom>Fort</nom>
					<email>karen.fort@inist.fr</email>
					<affiliationId>2</affiliationId>
					<affiliationId>3</affiliationId>
				</auteur>
				<auteur>
					<prenom>Gilles</prenom>
					<nom>Adda</nom>
					<email>gilles.adda@limsi.fr</email>
					<affiliationId>4</affiliationId>
				</auteur>
				<auteur>
					<prenom>Joseph</prenom>
					<nom>Mariani</nom>
					<email>joseph.mariani@limsi.fr</email>
					<affiliationId>4</affiliationId>
					<affiliationId>5</affiliationId>
				</auteur>
				<auteur>
					<prenom>Bernard</prenom>
					<nom>Lang</nom>
					<email>bernard.lang@inria.fr</email>
					<affiliationId>6</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Alpage, INRIA Paris–Rocquencourt &amp; Université Paris 7, Rocquencourt, BP 105, 78153 Le Chesnay Cedex, France</affiliation>
				<affiliation affiliationId="2">INIST-CNRS, 2 allée de Brabois, 54500 Vandoeuvre-lès-Nancy, France</affiliation>
				<affiliation affiliationId="3">LIPN, Université Paris Nord, 99 av J-B Clément, 93430 Villetaneuse, France</affiliation>
				<affiliation affiliationId="4">LIMSI-CNRS, Bât. 508, rue John von Neumann, Université Paris-Sud BP 133, 91403 Orsay Cedex, France</affiliation>
				<affiliation affiliationId="5">IMMI-CNRS, Bât. 508, rue John von Neumann, Université Paris-Sud BP 133, 91403 Orsay Cedex, France</affiliation>
				<affiliation affiliationId="6">INRIA Paris–Rocquencourt, Rocquencourt, BP 105, 78153 Le Chesnay Cedex, France</affiliation>
			</affiliations>
			<titre>Un turc mécanique pour les ressources linguistiques : critique de la myriadisation du travail parcellisé</titre>
			<type>long</type>
			<pages></pages>
			<resume>Cet article est une prise de position concernant les plate-formes de type Amazon Mechanical Turk, dont l’utilisation est en plein essor depuis quelques années dans le traitement automatique des langues. Ces plateformes de travail en ligne permettent, selon le discours qui prévaut dans les articles du domaine, de faire développer toutes sortes de ressources linguistiques de qualité, pour un prix imbattable et en un temps très réduit, par des gens pour qui il s’agit d’un passe-temps. Nous allons ici démontrer que la situation est loin d’être aussi idéale, que ce soit sur le plan de la qualité, du prix, du statut des travailleurs ou de l’éthique. Nous rappellerons ensuite les solutions alternatives déjà existantes ou proposées. Notre but est ici double : informer les chercheurs, afin qu’ils fassent leur choix en toute connaissance de cause, et proposer des solutions pratiques et organisationnelles pour améliorer le développement de nouvelles ressources linguistiques en limitant les risques de dérives éthiques et légales, sans que cela se fasse au prix de leur coût ou de leur qualité.</resume>
			<mots_cles>Amazon Mechanical Turk, ressources linguistiques</mots_cles>
			<title></title>
			<abstract>This article is a position paper concerning Amazon Mechanical Turk-like systems, the use of which has been steadily growing in natural language processing in the past few years. According to the mainstream opinion expressed in the articles of the domain, these online working platforms allow to develop very quickly all sorts of quality language resources, for a very low price, by people doing that as a hobby. We shall demonstrate here that the situation is far from being that ideal, be it from the point of view of quality, price, workers’ status or ethics. We shall then bring back to mind already existing or proposed alternatives. Our goal here is twofold : to inform researchers, so that they can make their own choices with all the elements of the reflection in mind, and propose practical and organizational solutions in order to improve new language resources development, while limiting the risks of ethical and legal issues without letting go price or quality.</abstract>
			<keywords>Amazon Mechanical Turk, language resources</keywords>
		</article>
		<article id="taln-2011-long-013" session="Lexique et Corpus">
			<auteurs>
				<auteur>
					<prenom>Bo</prenom>
					<nom>Li</nom>
					<email>bo.li@imag.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Eric</prenom>
					<nom>Gaussier</nom>
					<email>eric.gaussier@imag.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Emmanuel</prenom>
					<nom>Morin</nom>
					<email>emmanuel.morin@univ-nantes.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Amir</prenom>
					<nom>Hazem</nom>
					<email>amir.hazem@univ-nantes.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université Grenoble I, LIG UMR 5217</affiliation>
				<affiliation affiliationId="2">LINA, UMR 6241, Université de Nantes</affiliation>
			</affiliations>
			<titre>Degré de comparabilité, extraction lexicale bilingue et recherche d’information interlingue</titre>
			<type>long</type>
			<pages></pages>
			<resume>Nous étudions dans cet article le problème de la comparabilité des documents composant un corpus comparable afin d’améliorer la qualité des lexiques bilingues extraits et les performances des systèmes de recherche d’information interlingue. Nous proposons une nouvelle approche qui permet de garantir un certain degré de comparabilité et d’homogénéité du corpus tout en préservant une grande part du vocabulaire du corpus d’origine. Nos expériences montrent que les lexiques bilingues que nous obtenons sont d’une meilleure qualité que ceux obtenus avec les approches précédentes, et qu’ils peuvent être utilisés pour améliorer significativement les systèmes de recherche d’information interlingue.</resume>
			<mots_cles>Corpus comparables, comparabilité, lexiques bilingues, recherche d’information interlingue</mots_cles>
			<title></title>
			<abstract>We study in this paper the problem of enhancing the comparability of bilingual corpora in order to improve the quality of bilingual lexicons extracted from comparable corpora and the performance of crosslanguage information retrieval (CLIR) systems. We introduce a new method for enhancing corpus comparability which guarantees a certain degree of comparability and homogeneity, and still preserves most of the vocabulary of the original corpus. Our experiments illustrate the well-foundedness of this method and show that the bilingual lexicons obtained are of better quality than the lexicons obtained with previous approaches, and that they can be used to significantly improve CLIR systems.</abstract>
			<keywords>Comparable corpora, comparability, bilingual lexicon, cross-language information retrieval</keywords>
		</article>
		<article id="taln-2011-long-014" session="Lexique et Corpus">
			<auteurs>
				<auteur>
					<prenom>Nadja</prenom>
					<nom>Vincze</nom>
					<email>nadja.vincze@uclouvain.be</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Yves</prenom>
					<nom>Bestgen</nom>
					<email>yves.bestgen@uclouvain.be</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">UCLouvain, Cental, Place Blaise Pascal, 1, B-1348 Louvain-la-Neuve, Belgique</affiliation>
				<affiliation affiliationId="2">UCLouvain, CECL, B-1348 Louvain-la-Neuve, Belgique</affiliation>
			</affiliations>
			<titre>Identification de mots germes pour la construction d'un lexique de valence au moyen d'une procédure supervisée</titre>
			<type>long</type>
			<pages></pages>
			<resume>De nombreuses méthodes automatiques de classification de textes selon les sentiments qui y sont exprimés s'appuient sur un lexique dans lequel à chaque entrée est associée une valence. Le plus souvent, ce lexique est construit à partir d'un petit nombre de mots, choisis arbitrairement, qui servent de germes pour déterminer automatiquement la valence d'autres mots. La question de l'optimalité de ces mots germes a bien peu retenu l'attention. Sur la base de la comparaison de cinq méthodes automatiques de construction de lexiques de valence, dont une qui, à notre connaissance, n'a jamais été adaptée au français et une autre développée spécifiquement pour la présente étude, nous montrons l'importance du choix de ces mots germes et l'intérêt de les identifier au moyen d'une procédure d'apprentissage supervisée.</resume>
			<mots_cles>Analyse de sentiments, lexique de valence, apprentissage supervisé, analyse sémantique latente</mots_cles>
			<title></title>
			<abstract>Many methods of automatic sentiment classification of texts are based on a lexicon in which each entry is associated with a semantic orientation. These entries serve as seeds for automatically determining the semantic orientation of other words. Most often, this lexicon is built from a small number of words, chosen arbitrarily. The optimality of these seed words has received little attention. In this study, we compare five automatic methods to build a semantic orientation lexicon. One among them, to our knowledge, has never been adapted to French and another was developed specifically for this study. Based on them, we show that choosing good seed words is very important and identifying them with a supervised learning procedure brings a benefit.</abstract>
			<keywords>Sentiment analysis, semantic orientation lexicon, supervised learning, latent semantic analysis</keywords>
		</article>
		<article id="taln-2011-long-015" session="Lexique et Corpus">
			<auteurs>
				<auteur>
					<prenom>Philippe</prenom>
					<nom>Muller</nom>
					<email>muller@irit.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Philippe</prenom>
					<nom>Langlais</nom>
					<email>felipe@iro.umontreal.ca</email>
					<affiliationId>3</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">IRIT, Université Paul Sabatier</affiliation>
				<affiliation affiliationId="2">Alpage, INRIA Paris-Rocquencourt</affiliation>
				<affiliation affiliationId="3">RALI / DIRO / Université de Montréal</affiliation>
			</affiliations>
			<titre>Comparaison d’une approche miroir et d’une approche distributionnelle pour l’extraction de mots sémantiquement reliés</titre>
			<type>long</type>
			<pages></pages>
			<resume>Dans (Muller &amp; Langlais, 2010), nous avons comparé une approche distributionnelle et une variante de l’approche miroir proposée par Dyvik (2002) sur une tâche d’extraction de synonymes à partir d’un corpus en français. Nous présentons ici une analyse plus fine des relations extraites automatiquement en nous intéressant cette fois-ci à la langue anglaise pour laquelle de plus amples ressources sont disponibles. Différentes façons d’évaluer notre approche corroborent le fait que l’approche miroir se comporte globalement mieux que l’approche distributionnelle décrite dans (Lin, 1998), une approche de référence dans le domaine.</resume>
			<mots_cles>Sémantique lexicale, similarité distributionnelle, similarité traductionnelle</mots_cles>
			<title></title>
			<abstract>In (Muller &amp; Langlais, 2010), we compared a distributional approach to a variant of the mirror approach described by Dyvik (2002) on a task of synonym extraction. This was conducted on a corpus of the French language. In the present work, we propose a more precise and systematic evaluation of the relations extracted by a mirror and a distributional approaches. This evaluation is conducted on the English language for which widespread resources are available. All the evaluations we conducted in this study concur to the observation that our mirror approach globally outperforms the distributional one described by Lin (1998), which we believe to be a fair reference in the domain.</abstract>
			<keywords>Lexical Semantics, distributional similarity, mirror approach</keywords>
		</article>
		<article id="taln-2011-long-016" session="Lexique et Corpus">
			<auteurs>
				<auteur>
					<prenom>Yann</prenom>
					<nom>Mathet</nom>
					<email>Yann.Mathet@unicaen.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Antoine</prenom>
					<nom>Widlöcher</nom>
					<email>Antoine.Widlocher@unicaen.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">GREYC, UMR CNRS 6072, Université de Caen, 14032 Caen Cedex</affiliation>
			</affiliations>
			<titre>Une approche holiste et unifiée de l’alignement et de la mesure d’accord inter-annotateurs</titre>
			<type>long</type>
			<pages></pages>
			<resume>L’alignement et la mesure d’accord sur des textes multi-annotés sont des enjeux majeurs pour la constitution de corpus de référence. Nous défendons dans cet article l’idée que ces deux tâches sont par essence interdépendantes, la mesure d’accord nécessitant de s’appuyer sur des annotations alignées, tandis que les choix d’alignements ne peuvent se faire qu’à l’aune de la mesure qu’ils induisent. Nous proposons des principes formels relevant cette gageure, qui s’appuient notamment sur la notion de désordre du système constitué par l’ensemble des jeux d’annotations d’un texte. Nous posons que le meilleur alignement est celui qui minimise ce désordre, et que la valeur de désordre obtenue rend compte simultanément du taux d’accord. Cette approche, qualifiée d’holiste car prenant en compte l’intégralité du système pour opérer, est algorithmiquement lourde, mais nous sommes parvenus à produire une implémentation d’une version légèrement dégradée de cette dernière, et l’avons intégrée à la plate-forme d’annotation Glozz.</resume>
			<mots_cles>Alignement d’annotations, mesure d’accord inter-annotateurs, linguistique de corpus</mots_cles>
			<title></title>
			<abstract>Building reference corpora makes it necessary to align annotations and to measure agreement among annotators, in order to test the reliability of the annotated ressources. In this paper, we argue that alignment and agreement measure are interrelated : agreement measure applies to pre-aligned data and alignment assumes a prior agreement measure. We describe here a formal and computational framework which takes this interrelation into account, and relies on the notion of disorder of annotation sets available for a text. In this framework, the best alignment is the one which has the minimal disorder, and this disorder reflects an agreement measure of these data. This approach is said to be holistic insofar as alignment and measure depend on the system as a whole and cannot be locally determined. This holism introduces a computational cost which has been reduced by a heuristic strategy, implemented within the Glozz annotation platform.</abstract>
			<keywords>Alignment, inter-coder agreement measure, corpus linguistics</keywords>
		</article>
		<article id="taln-2011-long-017" session="Lexique et Corpus">
			<auteurs>
				<auteur>
					<prenom>André</prenom>
					<nom>Bittar</nom>
					<email>andre.bittar@xrce.xerox.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Pascal</prenom>
					<nom>Amsili</nom>
					<email>pascal.amsili@linguist.jussieu.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Pascal</prenom>
					<nom>Denis</nom>
					<email>pascal.denis@inria.fr</email>
					<affiliationId>3</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Xerox Research Centre Europe</affiliation>
				<affiliation affiliationId="2">LLF, Université Paris Diderot, UMR CNRS 7110</affiliation>
				<affiliation affiliationId="3">EPI Alpage, INRIA Rocquencourt et Université Paris Diderot</affiliation>
			</affiliations>
			<titre>French TimeBank : un corpus de référence sur la temporalité en français</titre>
			<type>long</type>
			<pages></pages>
			<resume>Cet article a un double objectif : d’une part, il s’agit de présenter à la communauté un corpus récemment rendu public, le French Time Bank (FTiB), qui consiste en une collection de textes journalistiques annotés pour les temps et les événements selon la norme ISO-TimeML ; d’autre part, nous souhaitons livrer les résultats et réflexions méthodologiques que nous avons pu tirer de la réalisation de ce corpus de référence, avec l’idée que notre expérience pourra s’avérer profitable au-delà de la communauté intéressée par le traitement de la temporalité.</resume>
			<mots_cles>Annotation temporelle, corpus, ISO-TimeML</mots_cles>
			<title></title>
			<abstract>This article has two objectives. Firstly, it presents the French TimeBank (FTiB) corpus, which has recently been made public. The corpus consists of a collection of news texts annotated for times and events according to the ISO-TimeML standard. Secondly, we wish to present the results and methodological conclusions that we have drawn from the creation of this reference corpus, with the hope that our experience may also prove useful to others outside the community of those interested in temporal processing.</abstract>
			<keywords>Temporal annotation, corpus, ISO-TimeML</keywords>
		</article>
		<article id="taln-2011-long-018" session="Lexique et Corpus">
			<auteurs>
				<auteur>
					<prenom>Edmond</prenom>
					<nom>Lassalle</nom>
					<email>edmond.lassalle@orange-ftgroup.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Orange Labs, 2 avenue Pierre Marzin, 22 307 Lannion - France</affiliation>
			</affiliations>
			<titre>Acquisition automatique de terminologie à partir de corpus de texte</titre>
			<type>long</type>
			<pages></pages>
			<resume>Les applications de recherche d'informations chez Orange sont confrontées à des flux importants de données textuelles, recouvrant des domaines larges et évoluant très rapidement. Un des problèmes à résoudre est de pouvoir analyser très rapidement ces flux, à un niveau élevé de qualité. Le recours à un modèle d'analyse sémantique, comme solution, n'est viable qu'en s'appuyant sur l'apprentissage automatique pour construire des grandes bases de connaissances dédiées à chaque application. L'extraction terminologique décrite dans cet article est un composant amont de ce dispositif d'apprentissage. Des nouvelles méthodes d'acquisition, basée sur un modèle hybride (analyse par grammaires de chunking et analyse statistique à deux niveaux), ont été développées pour répondre aux contraintes de performance et de qualité.</resume>
			<mots_cles>Apprentissage automatique, acquisition terminologique, entropie, grammaires de chunking</mots_cles>
			<title></title>
			<abstract>Information retrieval applications by Orange must process tremendous textual dataflows which cover large domains and evolve rapidly. One problem to solve is to analyze these dataflows very quickly, with a high quality level. Having a semantic analysis model as a solution is reliable only if unsupervised learning is used to build large knowledge databases dedicated to each application. The terminology extraction described in this paper is a prior component of the learning architecture. New acquisition methods, based on hybrid model (chunking analysis coupled with two-level statistical analysis) have been developed to meet the constraints of both performance and quality.</abstract>
			<keywords>Unsupervised learning, terminology acquisition, entropy, chunking analysis</keywords>
		</article>
		<article id="taln-2011-long-019" session="Lexique et Corpus">
			<auteurs>
				<auteur>
					<prenom>Amir</prenom>
					<nom>Hazem</nom>
					<email>amir.hazem@univ-nantes.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Emmanuel</prenom>
					<nom>Morin</nom>
					<email>emmanuel.morin@univ-nantes.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Sebastián</prenom>
					<nom>Peña Saldarriaga</nom>
					<email>spena@synchromedia.ca</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université de Nantes, LINA - UMR CNRS 6241, 2 rue de la Houssinière, BP 92208, 44322 Nantes Cedex 03</affiliation>
				<affiliation affiliationId="2">Synchromedia, École de technologie supérieure, 1100 rue Notre-Dame Ouest, Montréal, Québec, Canada H3C 1K3</affiliation>
			</affiliations>
			<titre>Métarecherche pour l’extraction lexicale bilingue à partir de corpus comparables</titre>
			<type>long</type>
			<pages></pages>
			<resume>Nous présentons dans cet article une nouvelle manière d’aborder le problème de l’acquisition automatique de paires de mots en relation de traduction à partir de corpus comparables. Nous décrivons tout d’abord les approches standard et par similarité interlangue traditionnellement dédiées à cette tâche. Nous réinterprétons ensuite la méthode par similarité interlangue et motivons un nouveau modèle pour reformuler cette approche inspirée par les métamoteurs de recherche d’information. Les résultats empiriques que nous obtenons montrent que les performances de notre modèle sont toujours supérieures à celles obtenues avec l’approche par similarité interlangue, mais aussi comme étant compétitives par rapport à l’approche standard.</resume>
			<mots_cles>Corpus comparables, lexiques bilingues, métarecherche</mots_cles>
			<title></title>
			<abstract>In this article we present a novel way of looking at the problem of automatic acquisition of pairs of translationally equivalent words from comparable corpora.We first describe the standard and extended approaches traditionally dedicated to this task. We then re-interpret the extended method, and motivate a novel model to reformulate this approach inspired by the metasearch engines in information retrieval. The empirical results show that performances of our model are always better than the baseline obtained with the extended approach and also competitive with the standard approach.</abstract>
			<keywords>Comparable corpora, bilingual lexicon, metasearch</keywords>
		</article>
		<article id="taln-2011-long-020" session="Lexique et Corpus">
			<auteurs>
				<auteur>
					<prenom>Alain</prenom>
					<nom>Joubert</nom>
					<email>alain.joubert@lirmm.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Mathieu</prenom>
					<nom>Lafourcade</nom>
					<email>mathieu.lafourcade@lirmm.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Didier</prenom>
					<nom>Schwab</nom>
					<email>didier.schwab@imag.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Michael</prenom>
					<nom>Zock</nom>
					<email>michael.zock@lif.univ-mrs.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIRMM, Université Montpellier II</affiliation>
				<affiliation affiliationId="2">LIG, Université Grenoble II</affiliation>
				<affiliation affiliationId="3">LIF-CNRS, Marseille</affiliation>
			</affiliations>
			<titre>Évaluation et consolidation d’un réseau lexical via un outil pour retrouver le mot sur le bout de la langue</titre>
			<type>long</type>
			<pages></pages>
			<resume>Depuis septembre 2007, un réseau lexical de grande taille pour le Français est en cours de construction à l'aide de méthodes fondées sur des formes de consensus populaire obtenu via des jeux (projet JeuxDeMots). L’intervention d’experts humains est marginale en ce qu'elle représente moins de 0,5% des relations du réseau et se limite à des corrections, à des ajustements ainsi qu’à la validation des sens de termes. Pour évaluer la qualité de cette ressource construite par des participants de jeu (utilisateurs non experts) nous adoptons une démarche similaire à celle de sa construction, à savoir, la ressource doit être validée sur un vocabulaire de classe ouverte, par des non-experts, de façon stable (persistante dans le temps). Pour ce faire, nous proposons de vérifier si notre ressource est capable de servir de support à la résolution du problème nommé 'Mot sur le Bout de la Langue' (MBL). A l'instar de JeuxdeMots, l'outil développé peut être vu comme un jeu en ligne. Tout comme ce dernier, il permet d’acquérir de nouvelles relations, constituant ainsi un enrichissement de notre réseau lexical.</resume>
			<mots_cles>Réseau lexical, JeuxDeMots, évaluation, outil de MBL, mot sur le bout de la langue</mots_cles>
			<title></title>
			<abstract>Since September 2007, a large scale lexical network for French is under construction through methods based on some kind of popular consensus by means of games (JeuxDeMots project). Human intervention can be considered as marginal. It is limited to corrections, adjustments and validation of the senses of terms, which amounts to less than 0,5 % of the relations in the network. To appreciate the quality of this resource built by non-expert users (players of the game), we use a similar approach to its construction. The resource must be validated by laymen, persistent in time, on open class vocabulary. We suggest to check whether our tool is able to solve the Tip of the Tongue (TOT) problem. Just like JeuxDeMots, our tool can be considered as an on-line game. Like the former, it allows the acquisition of new relations, enriching thus the (existing) network.</abstract>
			<keywords>Lexical network, JeuxDeMots, evaluation, TOT software, tip of the tongue</keywords>
		</article>
		<article id="taln-2011-long-021" session="Morphologie et Segmentation">
			<auteurs>
				<auteur>
					<prenom>Matthieu</prenom>
					<nom>Vernier</nom>
					<email>Matthieu.Vernier@univ-nantes.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Laura</prenom>
					<nom>Monceaux</nom>
					<email>Laura.Monceaux@univ-nantes.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Béatrice</prenom>
					<nom>Daille</nom>
					<email>Beatrice.Daille@univ-nantes.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université de Nantes, LINA, 2, rue de la Houssinière 44322 Nantes</affiliation>
			</affiliations>
			<titre>Identifier la cible d’un passage d’opinion dans un corpus multithématique</titre>
			<type>long</type>
			<pages></pages>
			<resume>L’identification de la cible d’une d’opinion fait l’objet d’une attention récente en fouille d’opinion. Les méthodes existantes ont été testées sur des corpus monothématiques en anglais. Elles permettent principalement de traiter les cas où la cible se situe dans la même phrase que l’opinion. Dans cet article, nous abordons cette problématique pour le français dans un corpus multithématique et nous présentons une nouvelle méthode pour identifier la cible d’une opinion apparaissant hors du contexte phrastique. L’évaluation de la méthode montre une amélioration des résultats par rapport à l’existant.</resume>
			<mots_cles>Fouille d’opinions, Identification des cibles, Méthode RankSVM</mots_cles>
			<title></title>
			<abstract>Recent works on opinion mining deal with the problem of finding the semantic relation between sentiment expressions and their target. Existing methods have been evaluated on monothematic english corpora. These methods are only able to solve intrasentential relationships. In this article, we focus on this task apply to french and we present a new method for solving intrasentential and intersentential relationships in a multithematic corpus. We show that our method is able to improve results on the intra- and intersential relationships.</abstract>
			<keywords>Opinion mining, Targeting sentiment expressions, RankSVM</keywords>
		</article>
		<article id="taln-2011-long-022" session="Morphologie et Segmentation">
			<auteurs>
				<auteur>
					<prenom>Matthieu</prenom>
					<nom>Constant</nom>
					<email>mconstan@univ-mlv.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Isabelle</prenom>
					<nom>Tellier</nom>
					<email>isabelle.tellier@univ-orleans.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Denys</prenom>
					<nom>Duchier</nom>
					<email>denys.duchier@univ-orleans.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Yoann</prenom>
					<nom>Dupont</nom>
					<email>yoann.dupont@etu.univ-orleans.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Anthony</prenom>
					<nom>Sigogne</nom>
					<email>sigogne@univ-mlv.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Sylvie</prenom>
					<nom>Billot</nom>
					<email>sylvie.billot@univ-orleans.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université Paris-Est, LIGM, CNRS, 5 bd Descartes, Champs-sur-Marne 77454, Marne-la-Vallée cedex 2</affiliation>
				<affiliation affiliationId="2">LIFO, université d’Orléans, 6 rue Léonard de Vinci, BP 6759, 45067 Orléans cedex 2</affiliation>
			</affiliations>
			<titre>Intégrer des connaissances linguistiques dans un CRF : application à l’apprentissage d’un segmenteur-étiqueteur du français</titre>
			<type>long</type>
			<pages></pages>
			<resume>Dans cet article, nous synthétisons les résultats de plusieurs séries d’expériences réalisées à l’aide de CRF (Conditional Random Fields ou “champs markoviens conditionnels”) linéaires pour apprendre à annoter des textes français à partir d’exemples, en exploitant diverses ressources linguistiques externes. Ces expériences ont porté sur l’étiquetage morphosyntaxique intégrant l’identification des unités polylexicales. Nous montrons que le modèle des CRF est capable d’intégrer des ressources lexicales riches en unités multi-mots de différentes manières et permet d’atteindre ainsi le meilleur taux de correction d’étiquetage actuel pour le français.</resume>
			<mots_cles>Etiquetagemorphosyntaxique,Modèle CRF, Ressources lexicales, Segmentation, Unités polylexicales</mots_cles>
			<title></title>
			<abstract>In this paper, we synthesize different experiments using a linear CRF (Conditional Random Fields) to annotate French texts from examples, by exploiting external linguistic resources. These experiments especially dealt with part-of-speech tagging including multiword units identification. We show that CRF models allow to integrate, in different ways, large-coverage lexical resources including multiword units and reach stateof- the-art tagging results for French.</abstract>
			<keywords>Part-of-speech tagging, CRF model, Lexical resources, Segmentation, Multiword units</keywords>
		</article>
		<article id="taln-2011-long-023" session="Morphologie et Segmentation">
			<auteurs>
				<auteur>
					<prenom>Pierre</prenom>
					<nom>Magistry</nom>
					<email>pierre.magistry@inria.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Benoît</prenom>
					<nom>Sagot</nom>
					<email>benoit.sagot@inria.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Alpage, INRIA Paris–Rocquencourt &amp; Université Paris 7, Rocquencourt, BP 105, 78153 Le Chesnay Cedex, France</affiliation>
			</affiliations>
			<titre>Segmentation et induction de lexique non-supervisées du mandarin</titre>
			<type>long</type>
			<pages></pages>
			<resume>Pour la plupart des langues utilisant l'alphabet latin, le découpage d'un texte selon les espaces et les symboles de ponctuation est une bonne approximation d'un découpage en unités lexicales. Bien que cette approximation cache de nombreuses difficultés, elles sont sans comparaison avec celles que l'on rencontre lorsque l'on veut traiter des langues qui, comme le chinois mandarin, n'utilisent pas l'espace. Un grand nombre de systèmes de segmentation ont été proposés parmi lesquels certains adoptent une approche non-supervisée motivée linguistiquement. Cependant les méthodes d'évaluation communément utilisées ne rendent pas compte de toutes les propriétés de tels systèmes. Dans cet article, nous montrons qu'un modèle simple qui repose sur une reformulation en termes d'entropie d'une hypothèse indépendante de la langue énoncée par Harris (1955), permet de segmenter un corpus et d'en extraire un lexique. Testé sur le corpus de l'Academia Sinica, notre système permet l'induction d'une segmentation et d'un lexique qui ont de bonnes propriétés intrinsèques et dont les caractéristiques sont similaires à celles du lexique sous-jacent au corpus segmenté manuellement. De plus, on constate une certaine corrélation entre les résultats du modèle de segmentation et les structures syntaxiques fournies par une sous-partie arborée corpus.</resume>
			<mots_cles>Segmentation non-supervisée, entropie, induction de lexique, unité lexicale, chinois mandarin</mots_cles>
			<title></title>
			<abstract>For most languages using the Latin alphabet, tokenizing a text on spaces and punctuation marks is a good approximation of a segmentation into lexical units. Although this approximation hides many difficulties, they do not compare with those arising when dealing with languages that do not use spaces, such as Mandarin Chinese. Many segmentation systems have been proposed, some of them use linguistitically motivated unsupervized algorithms. However, standard evaluation practices fail to account for some properties of such systems. In this paper, we show that a simple model, based on an entropy-based reformulation of a language-independent hypothesis put forward by Harris (1955), allows for segmenting a corpus and extracting a lexicon from the results. Tested on the Academia Sinica Corpus, our system allows for inducing a segmentation and a lexicon with good intrinsic properties and whose characteristics are similar to those of the lexicon underlying the manually-segmented corpus. Moreover, the results of the segmentation model correlate with the syntactic structures provided by the syntactically annotated subpart of the corpus.</abstract>
			<keywords>Non-supervized segmentation, entropy, lexicon induction, Mandarin Chinese</keywords>
		</article>
		<article id="taln-2011-long-024" session="Morphologie et Segmentation">
			<auteurs>
				<auteur>
					<prenom>Delphine</prenom>
					<nom>Bernhard</nom>
					<email>bernhard@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Bruno</prenom>
					<nom>Cartoni</nom>
					<email>bruno.cartoni@unige.ch</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Delphine</prenom>
					<nom>Tribout</nom>
					<email>tribout@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIMSI-CNRS, 91403 Orsay, France</affiliation>
				<affiliation affiliationId="2">Département de linguistique, Université de Genève, Suisse</affiliation>
			</affiliations>
			<titre>Évaluer la pertinence de la morphologie constructionnelle dans les systèmes de Question-Réponse</titre>
			<type>long</type>
			<pages></pages>
			<resume>Les connaissances morphologiques sont fréquemment utilisées en Question-Réponse afin de faciliter l’appariement entre mots de la question et mots du passage contenant la réponse. Il n’existe toutefois pas d’étude qualitative et quantitative sur les phénomènes morphologiques les plus pertinents pour ce cadre applicatif. Dans cet article, nous présentons une analyse détaillée des phénomènes de morphologie constructionnelle permettant de faire le lien entre question et réponse. Pour ce faire, nous avons constitué et annoté un corpus de paires de questions-réponses, qui nous a permis de construire une ressource de référence, utile pour l’évaluation de la couverture de ressources et d’outils d’analyse morphologique. Nous détaillons en particulier les phénomènes de dérivation et de composition et montrons qu’il reste un nombre important de relations morphologiques dérivationnelles pour lesquelles il n’existe pas encore de ressource exploitable pour le français.</resume>
			<mots_cles>Évaluation, Morphologie, Ressources, Système de Question-Réponse</mots_cles>
			<title></title>
			<abstract>Morphological knowledge is often used in Question Answering systems to facilitate the matching between question words and words in the passage containing the answer. However, there is no qualitative and quantitative study about morphological phenomena which are most relevant to this application. In this paper, we present a detailed analysis of the constructional morphology phenomena found in question and answer pairs. To this aim, we gathered and annotated a corpus of question and answer pairs. We relied on this corpus to build a gold standard for evaluating the coverage of morphological analysis tools and resources. We detail in particular the phenomena of derivation and composition and show that a significant number of derivational morphological relations are still not covered by any existing resource for the French language.</abstract>
			<keywords>Evaluation, Morphology, Resources, Question-answering system</keywords>
		</article>
		<article id="taln-2011-long-025" session="Morphologie et Segmentation">
			<auteurs>
				<auteur>
					<prenom>Julien</prenom>
					<nom>Gosme</nom>
					<email>Julien.Gosme@unicaen.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Yves</prenom>
					<nom>Lepage</nom>
					<email>Yves.Lepage@aoni.waseda.jp</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">GREYC, université de Caen Basse-Normandie, France</affiliation>
				<affiliation affiliationId="2">IPS, université Waseda, Japon</affiliation>
			</affiliations>
			<titre>Structure des trigrammes inconnus et lissage par analogie</titre>
			<type>long</type>
			<pages></pages>
			<resume>Nous montrons dans une série d’expériences sur quatre langues, sur des échantillons du corpus Europarl, que, dans leur grande majorité, les trigrammes inconnus d’un jeu de test peuvent être reconstruits par analogie avec des trigrammes hapax du corpus d’entraînement. De ce résultat, nous dérivons une méthode de lissage simple pour les modèles de langue par trigrammes et obtenons de meilleurs résultats que les lissages de Witten-Bell, Good-Turing et Kneser-Ney dans des expériences menées en onze langues sur la partie commune d’Europarl, sauf pour le finnois et, dans une moindre mesure, le français.</resume>
			<mots_cles>analogie, trigrammes inconnus, trigrammes hapax, modèle de langue trigrammes, Europarl</mots_cles>
			<title></title>
			<abstract>In a series of experiments in four languages on subparts of the Europarl corpus, we show that a large number of unseen trigrams can be reconstructed by proportional analogy using only hapax trigrams. We derive a simple smoothing scheme from this empirical result and show that it outperforms Witten-Bell, Good-Turing and Kneser-Ney smoothing schemes on trigram models built on the common part of the Europarl corpus, in all 11 languages except Finnish and French.</abstract>
			<keywords>proportional analogy, unseen trigrams, hapax trigrams, trigram language models, Europarl</keywords>
		</article>
		<article id="taln-2011-long-026" session="Syntaxe">
			<auteurs>
				<auteur>
					<prenom>Joseph</prenom>
					<nom>Le Roux</nom>
					<email>joseph.le-roux@lif.univ-mrs.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Benoît</prenom>
					<nom>Favre</nom>
					<email>benoit.favre@lif.univ-mrs.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Seyed Abolghasem</prenom>
					<nom>Mirroshandel</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Alexis</prenom>
					<nom>Nasr</nom>
					<email>alexis.nasr@lif.univ-mrs.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIF - CNRS UMR 6166 - Université Aix Marseille</affiliation>
			</affiliations>
			<titre>Modèles génératif et discriminant en analyse syntaxique : expériences sur le corpus arboré de Paris 7</titre>
			<type>long</type>
			<pages></pages>
			<resume>Nous présentons une architecture pour l’analyse syntaxique en deux étapes. Dans un premier temps un analyseur syntagmatique construit, pour chaque phrase, une liste d’analyses qui sont converties en arbres de dépendances. Ces arbres sont ensuite réévalués par un réordonnanceur discriminant. Cette méthode permet de prendre en compte des informations auxquelles l’analyseur n’a pas accès, en particulier des annotations fonctionnelles. Nous validons notre approche par une évaluation sur le corpus arboré de Paris 7. La seconde étape permet d’améliorer significativement la qualité des analyses retournées, quelle que soit la métrique utilisée.</resume>
			<mots_cles>analyse syntaxique, corpus arboré, apprentissage automatique, réordonnancement discriminant</mots_cles>
			<title></title>
			<abstract>We present an architecture for parsing in two steps. First, a phrase-structure parser builds for each sentence an n-best list of analyses which are converted to dependency trees. Then these trees are rescored by a discriminative reranker. This method enables the incorporation of additional linguistic information, more precisely functional annotations. We test our approach on the French Treebank. The evaluation shows a significative improvement on different parse metrics.</abstract>
			<keywords>parsing, treebank, machine learning, discriminative reranking</keywords>
		</article>
		<article id="taln-2011-long-027" session="Syntaxe">
			<auteurs>
				<auteur>
					<prenom>Anne-Lyse</prenom>
					<nom>Minard</nom>
					<email>Anne-Lyse.Minard@limsi.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Anne-Laure</prenom>
					<nom>Ligozat</nom>
					<email>Anne-Laure.Ligozat@limsi.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>3</affiliationId>
				</auteur>
				<auteur>
					<prenom>Brigitte</prenom>
					<nom>Grau</nom>
					<email>Brigitte.Grau@limsi.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>3</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIMSI-CNRS, BP 133, 91403 Orsay cedex</affiliation>
				<affiliation affiliationId="2">Université Paris-Sud, 91400 Orsay</affiliation>
				<affiliation affiliationId="3">ENSIIE, 1 square de la résistance, 91000 Évry</affiliation>
			</affiliations>
			<titre>Apport de la syntaxe pour l’extraction de relations en domaine médical</titre>
			<type>long</type>
			<pages></pages>
			<resume>Dans cet article, nous nous intéressons à l’identification de relations entre entités en domaine de spécialité, et étudions l’apport d’informations syntaxiques. Nous nous plaçons dans le domaine médical, et analysons des relations entre concepts dans des comptes-rendus médicaux, tâche évaluée dans la campagne i2b2 en 2010. Les relations étant exprimées par des formulations très variées en langue, nous avons procédé à l’analyse des phrases en extrayant des traits qui concourent à la reconnaissance de la présence d’une relation et nous avons considéré l’identification des relations comme une tâche de classification multi-classes, chaque catégorie de relation étant considérée comme une classe. Notre système de référence est celui qui a participé à la campagne i2b2, dont la F-mesure est d’environ 0,70. Nous avons évalué l’apport de la syntaxe pour cette tâche, tout d’abord en ajoutant des attributs syntaxiques à notre classifieur, puis en utilisant un apprentissage fondé sur la structure syntaxique des phrases (apprentissage à base de tree kernels) ; cette dernière méthode améliore les résultats de la classification de 3%.</resume>
			<mots_cles>extraction de relation, domaine médical, apprentissage multi-classes, tree kernel</mots_cles>
			<title></title>
			<abstract>In this paper, we study relation identification between concepts in medical reports, a task that was evaluated in the i2b2 campaign in 2010, and evaluate the usefulness of syntactic information. As relations are expressed in natural language with a great variety of forms, we proceeded to sentence analysis by extracting features that enable to identify a relation and we modeled this task as a multiclass classification task based on SVM, each category of relation representing a class. This method obtained an F-measure of 0.70 at i2b2 evaluation. We then evaluated the introduction of syntactic information in the classification process, by adding syntactic features, and by using tree kernels. This last method improves the classification up to 3%.</abstract>
			<keywords>relation identification, medical domain, multiclass learning, tree kernel</keywords>
		</article>
		<article id="taln-2011-long-028" session="Syntaxe">
			<auteurs>
				<auteur>
					<prenom>Guillaume</prenom>
					<nom>Bonfante</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Bruno</prenom>
					<nom>Guillaume</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Mathieu</prenom>
					<nom>Morey</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Guy</prenom>
					<nom>Perrier</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">INRIA Nancy-Grand Est - LORIA - Nancy-Université</affiliation>
			</affiliations>
			<titre>Enrichissement de structures en dépendances par réécriture de graphes</titre>
			<type>long</type>
			<pages></pages>
			<resume>Nous montrons comment enrichir une annotation en dépendances syntaxiques au format du French Treebank de Paris 7 en utilisant la réécriture de graphes, en vue du calcul de sa représentation sémantique. Le système de réécriture est composé de règles grammaticales et lexicales structurées en modules. Les règles lexicales utilisent une information de contrôle extraite du lexique des verbes français Dicovalence.</resume>
			<mots_cles>dépendance, French Treebank, réécriture de graphes, Dicovalence</mots_cles>
			<title></title>
			<abstract>We show how to enrich a syntactic dependency annotation of the French Paris 7 Treebank format, using graph rewriting, in order to compute its semantic representation. The rewriting system is composed of grammatical and lexical rules structured in modules. The lexical rules use a control information extracted from Dicovalence, a lexicon of French verbs.</abstract>
			<keywords>dependency, French Treebank, graph rewriting, Dicovalence</keywords>
		</article>
		<article id="taln-2011-long-029" session="Syntaxe">
			<auteurs>
				<auteur>
					<prenom>Alexander</prenom>
					<nom>Pak</nom>
					<email>alexpak@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Patrick</prenom>
					<nom>Paroubek</nom>
					<email>pap@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université de Paris-Sud, Laboratoire LIMSI-CNRS, Bâtiment 508, F-91405 Orsay Cedex, France</affiliation>
			</affiliations>
			<titre>Classification en polarité de sentiments avec une représentation textuelle à base de sous-graphes d’arbres de dépendances</titre>
			<type>long</type>
			<pages></pages>
			<resume>Les approches classiques à base de n-grammes en analyse supervisée de sentiments ne peuvent pas correctement identifier les expressions complexes de sentiments à cause de la perte d’information induite par l’approche « sac de mots » utilisée pour représenter les textes. Dans notre approche, nous avons recours à des sous-graphes extraits des graphes de dépendances syntaxiques comme traits pour la classification de sentiments. Nous représentons un texte par un vecteur composé de ces sous-graphes syntaxiques et nous employons un classifieurs SVM état-de-l’art pour identifier la polarité d’un texte. Nos évaluations expérimentales sur des critiques de jeux vidéo montrent que notre approche à base de sous-graphes est meilleure que les approches standard à modèles « sac de mots » et n-grammes. Dans cet article nous avons travaillé sur le français, mais notre approche peut facilement être adaptée à d’autres langues.</resume>
			<mots_cles>analyse de sentiments, analyse syntaxique, arbre de dépendances, SVM</mots_cles>
			<title></title>
			<abstract>A standard approach for supervised sentiment analysis with n-grams features cannot correctly identify complex sentiment expressions due to the loss of information incurred when representing texts with bagof- words models. In our research, we propose to use subgraphs from sentence dependency parse trees as features for sentiment classification.We represent a text by a feature vector made from extracted subgraphs and use a state of the art SVM classifier to identify the polarity of a text. Our experimental evaluations on video game reviews show that using our dependency subgraph features outperforms standard bag-of-words and n-gram models. In this paper, we worked with French, however our approach can be easily adapted to other languages.</abstract>
			<keywords>sentiment analysis, parsing, dependency tree, SVM</keywords>
		</article>
		<article id="taln-2011-long-030" session="Syntaxe">
			<auteurs>
				<auteur>
					<prenom>Sylvain</prenom>
					<nom>Kahane</nom>
					<email>sylvain@kahane.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Modyco, Université Paris Ouest Nanterre &amp; CNRS / Alpage, INRIA</affiliation>
			</affiliations>
			<titre>Une modélisation des dites alternances de portée des quantifieurs par des opérations de combinaison des groupes nominaux</titre>
			<type>long</type>
			<pages></pages>
			<resume>Nous montrons que les différentes interprétations d’une combinaison de plusieurs GN peuvent être modélisées par deux opérations de combinaison sur les référents de ces GN, appelées combinaison cumulative et combinaison distributive. Nous étudions aussi bien les GN définis et indéfinis que les GN quantifiés ou pluriels et nous montrons comment la combinaison d’un GN avec d’autres éléments peut induire des interprétations collective ou individualisante. Selon la façon dont un GN se combine avec d’autres GN, le calcul de son référent peut être fonction de ces derniers ; ceci définit une relation d’ancrage de chaque GN, qui induit un ordre partiel sur les GN. Considérer cette relation plutôt que la relation converse de portée simplifie le calcul de l’interprétation des GN et des énoncés. Des représentations sémantiques graphiques et algébriques sans considération de la portée sont proposées pour les dites alternances de portée.</resume>
			<mots_cles>portée des quantifieurs, cumulatif, collectif, distributif, référent de discours, ancrage</mots_cles>
			<title></title>
			<abstract>We show that the various interpretations of a combination of several Noun Phrases can be modeled by two operations of combination on the referent of these NPs, called cumulative and distributive combinations. We study definite and indefinite NPs as well as quantified and plural NPs and we show how the combination of an NP with other NPs can induce collective or individualizing interpretations. According to the way a NP combine with another NP, the calculation of its referent can be a function of the latter; this defines an anchoring relation for each NP, which induces a partial order on NPs. Considering this relation rather than the converse scope relation simplifies the calculation of the interpretation of NPs and utterances. Graphic and algebraic semantic representations without considering scope are proposed for the so-called scope alternations.</abstract>
			<keywords>quantifier scope alternation, cumulative, collective, distributive, discourse referent, anchoring</keywords>
		</article>
		<article id="taln-2011-long-031" session="Discours">
			<auteurs>
				<auteur>
					<prenom>Delphine</prenom>
					<nom>Bernhard</nom>
					<email>bernhard@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Anne-Laure</prenom>
					<nom>Ligozat</nom>
					<email>annlor@limsi.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIMSI-CNRS, B.P. 133, 91403 Orsay Cedex</affiliation>
				<affiliation affiliationId="2">ENSIIE, 1 square de la résistance, 91000 Évry</affiliation>
			</affiliations>
			<titre>Analyse automatique de la modalité et du niveau de certitude : application au domaine médical</titre>
			<type>long</type>
			<pages></pages>
			<resume>De nombreux phénomènes linguistiques visent à exprimer le doute ou l’incertitude de l’énonciateur, ainsi que la subjectivité potentielle du point de vue. La prise en compte de ces informations sur le niveau de certitude est primordiale pour de nombreuses applications du traitement automatique des langues, en particulier l’extraction d’information dans le domaine médical. Dans cet article, nous présentons deux systèmes qui analysent automatiquement les niveaux de certitude associés à des problèmes médicaux mentionnés dans des compte-rendus cliniques en anglais. Le premier système procède par apprentissage supervisé et obtient une f-mesure de 0,93. Le second système utilise des règles décrivant des déclencheurs linguistiques spécifiques et obtient une f-mesure de 0,90.</resume>
			<mots_cles>Modalité épistémique, Niveau de certitude, Domaine médical</mots_cles>
			<title></title>
			<abstract>Many linguistic phenomena aim at expressing the speaker’s doubt or incertainty, as well as the potential subjectivity of the point of view. Most natural language processing applications, and in particular knowledge extraction in the medical domain, need to take this type of information into account. In this article, we describe two systems which automatically analyse the levels of certainty associated with medical problems mentioned in English clinical reports. The first system uses supervised machine learning and obtains an f-measure of 0.93. The second system relies on a set of rules decribing specific linguistic triggers and reaches an f-measure of 0.90.</abstract>
			<keywords>Epistemic modality, Certainty level, Medical domain</keywords>
		</article>
		<article id="taln-2011-long-032" session="Discours">
			<auteurs>
				<auteur>
					<prenom>Laurence</prenom>
					<nom>Danlos</nom>
					<email>Laurence.Danlos@linguist.jussieu.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">ALPAGE, Université Paris Diderot (Paris 7), 175 rue du Chevaleret, 750013 Paris</affiliation>
			</affiliations>
			<titre>Analyse discursive et informations de factivité</titre>
			<type>long</type>
			<pages></pages>
			<resume>Les annotations discursives proposées dans le cadre de théories discursives comme RST (Rhetorical Structure Theory) ou SDRT (Segmented Dicourse Representation Theory) ont comme point fort de construire une structure discursive globale liant toutes les informations données dans un texte. Les annotations discursives proposées dans le PDTB (Penn Discourse Tree Bank) ont comme point fort d’identifier la “source” de chaque information du texte—répondant ainsi à la question qui a dit ou pense quoi ? Nous proposons une approche unifiée pour les annotations discursives alliant les points forts de ces deux courants de recherche. Cette approche unifiée repose crucialement sur des information de factivité, telles que celles qui sont annotées dans le corpus (anglais) FactBank.</resume>
			<mots_cles>Discours, Analyse discursive, Factivité (véracité), Interface syntaxe-sémantique, RST, SDRT, PDTB, FactBank</mots_cles>
			<title></title>
			<abstract>Discursive annotations proposed in theories of discourse such as RST (Rhetorical Structure Theory) or SDRT (Segmented Representation Theory Dicourse) have the advatange of building a global discourse structure linking all the information in a text. Discursive annotations proposed in PDTB (Penn Discourse Tree Bank) have the advatange of identifying the “source” of each information—thereby answering to questions such as who says or thinks what ? We propose a unified approach for discursive annotations combining the strengths of these two streams of research. This unified approach relies crucially on factivity information, as encoded in the English corpus FactBank.</abstract>
			<keywords>Discourse, Discursive analysis, Factuality (vericity), Syntax-semantic interface, RST, SDRT, PDTB, FactBank</keywords>
		</article>
		<article id="taln-2011-long-033" session="Discours">
			<auteurs>
				<auteur>
					<prenom>Camille</prenom>
					<nom>Dutrey</nom>
					<email>camille@dutrey.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Houda</prenom>
					<nom>Bouamor</nom>
					<email>Houda.Bouamor@limsi.fr</email>
					<affiliationId>2</affiliationId>
					<affiliationId>3</affiliationId>
				</auteur>
				<auteur>
					<prenom>Delphine</prenom>
					<nom>Bernhard</nom>
					<email>Delphine.Bernhard@limsi.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Aurélien</prenom>
					<nom>Max</nom>
					<email>Aurelien.Max@limsi.fr</email>
					<affiliationId>2</affiliationId>
					<affiliationId>3</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">INALCO, Paris, France</affiliation>
				<affiliation affiliationId="2">LIMSI-CNRS, Orsay, France</affiliation>
				<affiliation affiliationId="3">Univ. Paris-Sud, Orsay, France</affiliation>
			</affiliations>
			<titre>Paraphrases et modifications locales dans l’historique des révisions deWikipédia</titre>
			<type>long</type>
			<pages></pages>
			<resume>Dans cet article, nous analysons les modifications locales disponibles dans l’historique des révisions de la version française de Wikipédia. Nous définissons tout d’abord une typologie des modifications fondée sur une étude détaillée d’un large corpus de modifications. Puis, nous détaillons l’annotation manuelle d’une partie de ce corpus afin d’évaluer le degré de complexité de la tâche d’identification automatique de paraphrases dans ce genre de corpus. Enfin, nous évaluons un outil d’identification de paraphrases à base de règles sur un sous-ensemble de notre corpus.</resume>
			<mots_cles>Wikipédia, révisions, identification de paraphrases</mots_cles>
			<title></title>
			<abstract>In this article, we analyse the modifications available in the French Wikipédia revision history. We first define a typology of modifications based on a detailed study of a large corpus of modifications. Moreover, we detail a manual annotation study of a subpart of the corpus aimed at assessing the difficulty of automatic paraphrase identification in such a corpus. Finally, we assess a rule-based paraphrase identification tool on a subset of our corpus.</abstract>
			<keywords>Wikipedia, revisions, paraphrase identification</keywords>
		</article>
		<article id="taln-2011-long-034" session="Discours">
			<auteurs>
				<auteur>
					<prenom>Patrick</prenom>
					<nom>Saint-Dizier</nom>
					<email>stdizier@irit.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">IRIT-CNRS, Toulouse</affiliation>
			</affiliations>
			<titre>&lt;TextCoop&gt;: un analyseur de discours basé sur les grammaires logiques</titre>
			<type>long</type>
			<pages></pages>
			<resume>Dans ce document, nous présentons les principales caractéristiques de &lt;TextCoop&gt;, un environnement basé sur les grammaires logiques dédié à l’analyse de structures discursives. Nous étudions en particulier le langage DisLog qui fixe la structure des règles et des spécifications qui les accompagnent. Nous présentons la structure du moteur de &lt;TextCoop&gt; en indiquant au fur et à mesure du texte l’état du travail, les performances et les orientations en particulier en matière d’environnement, d’aide à l’écriture de règles et de développement applicatif.</resume>
			<mots_cles>grammaire du discours, programmation en logique, grammaires logiques</mots_cles>
			<title></title>
			<abstract>In this paper, we introduce the main features of &lt;TextCoop&gt;, an environment dedicated to discourse analysis within a logic-based grammar framework.We focus on the structure of discourse rules (DisLog language) and on the features of the engine, while outlining the results, the performances and the orientations for future work.</abstract>
			<keywords>discourse structure, logic programming, logic-based grammars</keywords>
		</article>
		<article id="taln-2011-long-035" session="Discours">
			<auteurs>
				<auteur>
					<prenom>Katya</prenom>
					<nom>Alahverdzhieva</nom>
					<email>K.Alahverdzhieva@sms.ed.ac.uk</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Alex</prenom>
					<nom>Lascarides</nom>
					<email>alex@inf.ed.ac.uk</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">School of Informatics, University of Edinburgh</affiliation>
			</affiliations>
			<titre>Integration of Speech and Deictic Gesture in a Multimodal Grammar</titre>
			<type>long</type>
			<pages></pages>
			<resume>Dans cet article, nous présentons une analyse à base de contraintes de la relation forme-sens des gestes déictiques et de leur signal de parole synchrone. En nous basant sur une étude empirique de corpus multimodaux, nous définissons quels énoncés multimodaux sont bien formés, et lesquels ne pourraient jamais produire le sens voulu dans la situation communicative. Plus précisément, nous formulons une grammaire multimodale dont les règles de construction utilisent la prosodie, la syntaxe et la sémantique de la parole, la forme et le sens du signal déictique, ainsi que la performance temporelle de la parole et la deixis afin de contraindre la production d’un arbre de syntaxe combinant parole et gesture déictique ainsi que la représentation unifiée du sens pour l’action multimodale correspondant à cet arbre. La contribution de notre projet est double : nous ajoutons aux ressources existantes pour le TAL un corpus annoté de parole et de gestes, et nous créons un cadre théorique pour la grammaire au sein duquel la composition sémantique d’un énoncé découle de la synchronie entre geste et parole.</resume>
			<mots_cles>Deixis, parole et geste, grammaires multimodales</mots_cles>
			<title></title>
			<abstract>In this paper we present a constraint-based analysis of the form-meaning relation of deictic gesture and its synchronous speech signal. Based on an empirical study of multimodal corpora, we capture generalisations about which multimodal utterances are well-formed, and which would never produce the intended meaning in the communicative situation. More precisely, we articulate a multimodal grammar whose construction rules use the prosody, syntax and semantics of speech, the form and meaning of the deictic signal, as well as the relative temporal performance of the speech and deixis to constrain the production of a single syntactic tree of speech and deictic gesture and its corresponding meaning representation for the multimodal action. In so doing, the contribution of our project is two-fold: it augments the existing NLP resources with annotated speech and gesture corpora, and it also provides the theoretical grammar framework where the semantic composition of an utterance results from its gestural and speech synchrony.</abstract>
			<keywords>Deixis, speech and gesture, multimodal grammars</keywords>
		</article>
		<article id="taln-2011-long-036" session="Traduction et Alignement">
			<auteurs>
				<auteur>
					<prenom>Adrien</prenom>
					<nom>Lardilleux</nom>
					<email>Adrien.Lardilleux@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>François</prenom>
					<nom>Yvon</nom>
					<email>Francois.Yvon@limsi.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Yves</prenom>
					<nom>Lepage</nom>
					<email>Yves.Lepage@aoni.waseda.jp</email>
					<affiliationId>3</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIMSI-CNRS, BP 133, 91403 Orsay Cedex</affiliation>
				<affiliation affiliationId="2">Université Paris-Sud</affiliation>
				<affiliation affiliationId="3">IPS, université Waseda, Japon</affiliation>
			</affiliations>
			<titre>Généralisation de l’alignement sous-phrastique par échantillonnage</titre>
			<type>long</type>
			<pages></pages>
			<resume>L’alignement sous-phrastique consiste à extraire des traductions d’unités textuelles de grain inférieur à la phrase à partir de textes multilingues parallèles alignés au niveau de la phrase. Un tel alignement est nécessaire, par exemple, pour entraîner des systèmes de traduction statistique. L’approche standard pour réaliser cette tâche implique l’estimation successive de plusieurs modèles probabilistes de complexité croissante et l’utilisation d’heuristiques qui permettent d’aligner des mots isolés, puis, par extension, des groupes de mots. Dans cet article, nous considérons une approche alternative, initialement proposée dans (Lardilleux &amp; Lepage, 2008), qui repose sur un principe beaucoup plus simple, à savoir la comparaison des profils d’occurrences dans des souscorpus obtenus par échantillonnage. Après avoir analysé les forces et faiblesses de cette approche, nous montrons comment améliorer la détection d’unités de traduction longues, et évaluons ces améliorations sur des tâches de traduction automatique.</resume>
			<mots_cles>alignement sous-phrastique, traduction automatique par fragments</mots_cles>
			<title></title>
			<abstract>Sub-sentential alignment is the process by which multi-word translation units are extracted from sentence-aligned multilingual parallel texts. Such alignment is necessary, for instance, to train statistical machine translation systems. Standard approaches typically rely on the estimation of several probabilistic models of increasing complexity and on the use of various heuristics that make it possible to align, first isolated words, then, by extension, groups of words. In this paper, we explore an alternative approach, originally proposed in (Lardilleux &amp; Lepage, 2008), that relies on a much simpler principle, which is the comparison of occurrence profiles in subcorpora obtained by sampling. After analyzing the strengths and weaknesses of this approach, we show how to improve the detection of long translation units, and evaluate these improvements on machine translation tasks.</abstract>
			<keywords>sub-sentential alignment, phrase-based machine translation</keywords>
		</article>
		<article id="taln-2011-long-037" session="Traduction et Alignement">
			<auteurs>
				<auteur>
					<prenom>Nadi</prenom>
					<nom>Tomeh</nom>
					<email>nadi@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Alexandre</prenom>
					<nom>Allauzen</nom>
					<email>allauzen@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>François</prenom>
					<nom>Yvon</nom>
					<email>yvon@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université Paris Sud et LIMSI/CNRS</affiliation>
			</affiliations>
			<titre>Estimation d’un modèle de traduction à partir d’alignements mot-à-mot non-déterministes</titre>
			<type>long</type>
			<pages></pages>
			<resume>Dans les systèmes de traduction statistique à base de segments, le modèle de traduction est estimé à partir d’alignements mot-à-mot grâce à des heuristiques d’extraction et de valuation. Bien que ces alignements mot-à-mot soient construits par des modèles probabilistes, les processus d’extraction et de valuation utilisent ces modèles en faisant l’hypothèse que ces alignements sont déterministes. Dans cet article, nous proposons de lever cette hypothèse en considérant l’ensemble de la matrice d’alignement, d’une paire de phrases, chaque association étant valuée par sa probabilité. En comparaison avec les travaux antérieurs, nous montrons qu’en utilisant un modèle exponentiel pour estimer de manière discriminante ces probabilités, il est possible d’obtenir des améliorations significatives des performances de traduction. Ces améliorations sont mesurées à l’aide de la métrique BLEU sur la tâche de traduction de l’arabe vers l’anglais de l’évaluation NIST MT’09, en considérant deux types de conditions selon la taille du corpus de données parallèles utilisées.</resume>
			<mots_cles>traduction statistique, modèles de traduction à base de segments, modèles d’alignement mot-à-mot</mots_cles>
			<title></title>
			<abstract>In extant phrase-based statistical translation systems, the translation model relies on word-to-word alignments, which serve as constraints for further heuristic extraction and scoring processes. These word alignments are infered in a probabilistic framework ; yet, only one single best word alignment is used as if alignments were deterministically produced. In this paper, we propose to take the full probabilistic alignment matrix into account, where each alignment link is scored by its probability score. By comparison with previous attempts, we show that using an exponential model to compute these probabilities is an effective way to achieve significant improvements in translation accuracy on the NIST MT’09 Arabic to English translation task, where the accuracy is measured in terms of BLEU scores.</abstract>
			<keywords>statistical machine translation, phrase based translation models, word alignment models</keywords>
		</article>
		<article id="taln-2011-long-038" session="Traduction et Alignement">
			<auteurs>
				<auteur>
					<prenom>Houda</prenom>
					<nom>Bouamor</nom>
					<email>Houda.Bouamor@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Aurélien</prenom>
					<nom>Max</nom>
					<email>Aurelien.Max@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Anne</prenom>
					<nom>Vilnat</nom>
					<email>Anne.Vilnat@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIMSI-CNRS, Univ. Paris-Sud, Orsay, F-91403, France</affiliation>
			</affiliations>
			<titre>Combinaison d’informations pour l’alignement monolingue</titre>
			<type>long</type>
			<pages></pages>
			<resume>Dans cet article, nous décrivons une nouvelle méthode d’alignement automatique de paraphrases d’énoncés. Nous utilisons des méthodes développées précédemment afin de produire différentes approches hybrides (hybridations). Ces différentes méthodes permettent d’acquérir des équivalences textuelles à partir d’un corpus monolingue parallèle. L’hybridation combine des informations obtenues par diverses techniques : alignements statistiques, approche symbolique, fusion d’arbres syntaxiques et alignement basé sur des distances d’édition. Nous avons évalué l’ensemble de ces résultats et nous constatons une amélioration sur l’acquisition de paraphrases sous-phrastiques.</resume>
			<mots_cles>Paraphrase sous-phrastique, corpus parallèle monolingue, hybridation</mots_cles>
			<title></title>
			<abstract>In this paper, we detail a new method to automatic alignment of paraphrase of statements.We also use previously developed methods to produce different hybrid approaches. These methods allow the acquisition of textual equivalence from a parallel monolingual corpus. Hybridization combines information obtained by using advanced statistical alignments, symbolic approach, syntax tree based alignment and edit distances technique. We evaluated all these results and we see an improvement on the acquisition of sub-sentential paraphrases.</abstract>
			<keywords>Phrasal paraphrase, monolingual parallel corpora, hybridization</keywords>
		</article>
		<article id="taln-2011-court-001" session="Boosters">
			<auteurs>
				<auteur>
					<prenom>Jean-Yves</prenom>
					<nom>Antoine</nom>
					<email>Jean-Yves.Antoine@univ-tours.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Marc</prenom>
					<nom>Le Tallec</nom>
					<email>Marc.Le-Tallec@univ-tours.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Jeanne</prenom>
					<nom>Villaneau</nom>
					<email>Jeanne.Villaneau@univ-ubs.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université François Rabelais de Tours, LI, 37000 Blois</affiliation>
				<affiliation affiliationId="2">Université Européenne de Bretagne, VALORIA, 56100 Lorient</affiliation>
			</affiliations>
			<titre>Evaluation de la détection des émotions, des opinions ou des sentiments : dictature de la majorité ou respect de la diversité d’opinions ?</titre>
			<type>court</type>
			<pages></pages>
			<resume>Détection d’émotion, fouille d’opinion et analyse des sentiments sont généralement évalués par comparaison des réponses du système concerné par rapport à celles contenues dans un corpus de référence. Les questions posées dans cet article concernent à la fois la définition de la référence et la fiabilité des métriques les plus fréquemment utilisées pour cette comparaison. Les expérimentations menées pour évaluer le système de détection d’émotions EmoLogus servent de base de réflexion pour ces deux problèmes. L’analyse des résultats d’EmoLogus et la comparaison entre les différentes métriques remettent en cause le choix du vote majoritaire comme référence. Par ailleurs elles montrent également la nécessité de recourir à des outils statistiques plus évolués que ceux généralement utilisés pour obtenir des évaluations fiables de systèmes qui travaillent sur des données intrinsèquement subjectives et incertaines.</resume>
			<mots_cles>Détection d’émotion, analyse de sentiments, fouille d’opinion ; Evaluation : métrique d’évaluation, constitution de référence, analyse statistique des résultats</mots_cles>
			<title></title>
			<abstract>Emotion detection, opinion identification and sentiment analysis are generally assessed by means of the comparison of a reference corpus with the answers of the system. This paper addresses the problem of the definition of the reference and the reliability of the metrics which are commonly used for this comparison. We present some experiments led with EmoLogus, a system of emotion detection, to investigate these two problems. A detailed analysis of the quantitative results obtained by EmoLogus on various metrics questions the choice of a majority vote among several human judgments to build a reference. Besides, it shows the necessity of using more sophisticated statistical tools to obtain a reliable evaluation of such systems which are working on intrinsically subjective and uncertain data.</abstract>
			<keywords>Detection of emotion, sentiment analysis, opinion mining, Evaluation: objective measures, test reference, statistical analysis of the results</keywords>
		</article>
		<article id="taln-2011-court-002" session="Boosters">
			<auteurs>
				<auteur>
					<prenom>Violeta</prenom>
					<nom>Seretan</nom>
					<email>violeta.seretan@gmail.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Institute for Language, Cognition and Computation, Human Communication Research Centre, University of Edinburgh, 10 Crichton Street, Edinburgh EH8 9AB, United Kingdom</affiliation>
			</affiliations>
			<auteurs></auteurs>
			<titre>A Collocation-Driven Approach to Text Summarization</titre>
			<type>court</type>
			<pages></pages>
			<resume>Dans cet article, nous décrivons une nouvelle approche pour la création de résumés extractifs – tâche qui consiste à créer automatiquement un résumé pour un document en sélectionnant un sous-ensemble de ses phrases – qui exploite des informations collocationnelles spécifiques à un domaine, acquises préalablement à partir d’un corpus de développement. Un extracteur de collocations fondé sur l’analyse syntaxique est utilisé afin d’inférer un modèle de contenu qui est ensuite appliqué au document à résumer. Cette approche a été utilisée pour la création des versions simples pour les articles de Wikipedia en anglais, dans le cadre d’un projet visant la création automatique d’articles simplifiées, similaires aux articles recensées dans Simple English Wikipedia. Une évaluation du système développé reste encore à faire. Toutefois, les résultats préalables obtenus pour les articles sur des villes montrent le potentiel de cette approche guidée par collocations pour la sélection des phrases pertinentes.</resume>
			<mots_cles>résumé de texte automatique, résumé extractif, statistiques de co-occurrence, collocations, analyse syntaxique, Wikipedia</mots_cles>
			<title></title>
			<abstract>We present a novel approach to extractive summarization – the task of producing an abstract for an input document by selecting a subset of the original sentences – which relies on domain-specific collocation information automatically acquired from a development corpus. A syntax-based collocation extractor is used to infer a content template and then to match this template against the document to summarize. The approach has been applied to generate simplified versions of Wikipedia articles in English, as part of a larger project on automatically generating Simple EnglishWikipedia articles starting from their standard counterpart. An evaluation of the developed system has yet to be performed; nonetheless, the preliminary results obtained in summarizing Wikipedia articles on cities already indicated the potential of our collocation-driven method to select relevant sentences.</abstract>
			<keywords>text summarization, extractive summarization, co-occurrence statistics, collocations, syntactic parsing, Wikipedia</keywords>
		</article>
		<article id="taln-2011-court-003" session="Boosters">
			<auteurs>
				<auteur>
					<prenom>Thomas</prenom>
					<nom>François</nom>
					<email>thomas.francois@uclouvain.be</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
					<affiliationId>3</affiliationId>
				</auteur>
				<auteur>
					<prenom>Patrick</prenom>
					<nom>Watrin</nom>
					<email>patrick.watrin@uclouvain.be</email>
					<affiliationId>2</affiliationId>
					<affiliationId>3</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Aspirant FNRS</affiliation>
				<affiliation affiliationId="2">Centre de traitement automatique du langage (CENTAL), UCLouvain</affiliation>
				<affiliation affiliationId="3">Institut Langage et Communication (IL&amp;C), UCLouvain</affiliation>
			</affiliations>
			<auteurs> and </auteurs>
			<titre>Quel apport des unités polylexicales dans une formule de lisibilité pour le français langue étrangère</titre>
			<type>court</type>
			<pages></pages>
			<resume>Cette étude envisage l’emploi des unités polylexicales (UPs) comme prédicteurs dans une formule de lisibilité pour le français langue étrangère. À l’aide d’un extracteur d’UPs combinant une approche statistique à un filtre linguistique, nous définissons six variables qui prennent en compte la densité et la probabilité des UPs nominales, mais aussi leur structure interne. Nos expérimentations concluent à un faible pouvoir prédictif de ces six variables et révèlent qu’une simple approche basée sur la probabilité moyenne des n-grammes des textes est plus efficace.</resume>
			<mots_cles>Lisibilité du FLE, unités polylexicales nominales, modèles N-grammes</mots_cles>
			<title></title>
			<abstract>This study considers the use of multi-words expressions (MWEs) as predictors for a readability formula for French as a foreign language. Using a MWEs extractor combining a statistical approach with a linguistic filter, we define six variables. These take into account the density and the probability of MWEs, but also their internal structure. Our experiments conclude that the predictive power of these six variables is low. Moreover, we show that a simple approach based on the average probability of n-grams is a more effective predictor.</abstract>
			<keywords>Readability of FFL, nominal MWEs, N-grams models</keywords>
		</article>
		<article id="taln-2011-court-004" session="Boosters">
			<auteurs>
				<auteur>
					<prenom>Frédéric</prenom>
					<nom>Béchet</nom>
					<email>frederic.bechet@lif.univ-mrs.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Benoît</prenom>
					<nom>Sagot</nom>
					<email>benoit.sagot@inria.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Rosa</prenom>
					<nom>Stern</nom>
					<email>rosa.stern@afp.com</email>
					<affiliationId>2</affiliationId>
					<affiliationId>3</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Aix Marseille Université, LIF-CNRS, route de Luminy, Marseille</affiliation>
				<affiliation affiliationId="2">Alpage, INRIA &amp; Univ. Paris 7, Rocquencourt, BP 105, 78153 Le Chesnay Cedex, France</affiliation>
				<affiliation affiliationId="3">Agence France-Presse – Medialab, 2 place de la Bourse, 75002 Paris, France</affiliation>
			</affiliations>
			<titre>Coopération de méthodes statistiques et symboliques pour l’adaptation non-supervisée d’un système d’étiquetage en entités nommées</titre>
			<type>court</type>
			<pages></pages>
			<resume>La détection et le typage des entités nommées sont des tâches pour lesquelles ont été développés à la fois des systèmes symboliques et probabilistes. Nous présentons les résultats d’une expérience visant à faire interagir le système à base de règles NP, développé sur des corpus provenant de l’AFP, intégrant la base d’entités Aleda et qui a une bonne précision, et le système LIANE, entraîné sur des transcriptions de l’oral provenant du corpus ESTER et qui a un bon rappel. Nous montrons qu’on peut adapter à un nouveau type de corpus, de manière non supervisée, un système probabiliste tel que LIANE grâce à des corpus volumineux annotés automatiquement par NP. Cette adaptation ne nécessite aucune annotation manuelle supplémentaire et illustre la complémentarité des méthodes numériques et symboliques pour la résolution de tâches linguistiques.</resume>
			<mots_cles>Détection d’entités nommées, adaptation à un nouveau domaine, coopération entre approches probabilistes et symboliques</mots_cles>
			<title></title>
			<abstract>Named entity recognition and typing is achieved both by symbolic and probabilistic systems. We report on an experiment for making the rule-based system NP, a high-precision system developed on AFP news corpora and relies on the Aleda named entity database, interact with LIANE, a high-recall probabilistic system trained on oral transcriptions from the ESTER corpus.We show that a probabilistic system such as LIANE can be adapted to a new type of corpus in a non-supervized way thanks to large-scale corpora automatically annotated by NP. This adaptation does not require any additional manual anotation and illustrates the complementarity between numeric and symbolic techniques for tackling linguistic tasks.</abstract>
			<keywords>Named entity recognition, domain adaptation, cooperation between probabilistic and symbolic approaches</keywords>
		</article>
		<article id="taln-2011-court-005" session="Boosters">
			<auteurs>
				<auteur>
					<prenom>Nuria</prenom>
					<nom>Gala</nom>
					<email>nuria.gala@lif.univ-mrs.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Nabil</prenom>
					<nom>Hathout</nom>
					<email>nabil.hathout@univ-tlse2.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Alexis</prenom>
					<nom>Nasr</nom>
					<email>alexis.nasr@lif.univ-mrs.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Véronique</prenom>
					<nom>Rey</nom>
					<email>veronique.rey-lafay@univmed.fr</email>
					<affiliationId>3</affiliationId>
				</auteur>
				<auteur>
					<prenom>Selja</prenom>
					<nom>Seppälä</nom>
					<email>selja.seppala@lif.univ-mrs.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIF-TALEP, 163, Av. de Luminy case 901, 13288 Marseille Cedex 9</affiliation>
				<affiliation affiliationId="2">CLLE-ERSS, 5, allées Antonio Machado, 31058 Toulouse Cedex 9</affiliation>
				<affiliation affiliationId="3">EHESS, 2, rue de la Charité, 13002 Marseille</affiliation>
			</affiliations>
			<titre>Création de clusters sémantiques dans des familles morphologiques à partir du TLFi</titre>
			<type>court</type>
			<pages></pages>
			<resume>La constitution de ressources linguistiques est une tâche longue et coûteuse. C’est notamment le cas pour les ressources morphologiques. Ces ressources décrivent de façon approfondie et explicite l’organisation morphologique du lexique complétée d’informations sémantiques exploitables dans le domaine du TAL. Le travail que nous présentons dans cet article s’inscrit dans cette perspective et, plus particulièrement, dans l’optique d’affiner une ressource existante en s’appuyant sur des informations sémantiques obtenues automatiquement. Notre objectif est de caractériser sémantiquement des familles morpho-phonologiques (des mots partageant une même racine et une continuité de sens). Pour ce faire, nous avons utilisé des informations extraites du TLFi annoté morpho-syntaxiquement. Les premiers résultats de ce travail seront analysés et discutés.</resume>
			<mots_cles>Ressources lexicales, familles morphologiques, clusters sémantiques, mesure de Lesk</mots_cles>
			<title></title>
			<abstract>Building lexical resources is a time-consuming and expensive task, mainly when it comes to morphological lexicons. Such resources describe in depth and explicitly the morphological organization of the lexicon, completed with semantic information to be used in NLP applications. The work we present here goes on such direction, and especially, on refining an existing resource with automatically acquired semantic information. Our goal is to semantically characterize morpho-phonological families (words sharing a same base form and semantic continuity). To this end, we have used data from the TLFi which has been morpho-syntactically annotated. The first results of such a task will be analyzed and discussed.</abstract>
			<keywords>Lexical resources, morphological families, semantic clusters, Lesk measure</keywords>
		</article>
		<article id="taln-2011-court-006" session="Boosters">
			<auteurs>
				<auteur>
					<prenom>Louis</prenom>
					<nom>de Viron</nom>
					<email>louis.deviron@student.uclouvain.be</email>
					<affiliationId>1</affiliationId>
					<affiliationId>3</affiliationId>
				</auteur>
				<auteur>
					<prenom>Delphine</prenom>
					<nom>Bernhard</nom>
					<email>delphine.bernhard@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Véronique</prenom>
					<nom>Moriceau</nom>
					<email>moriceau@limsi.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Xavier</prenom>
					<nom>Tannier</nom>
					<email>xtannier@limsi.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIMSI-CNRS, 91403 Orsay, France</affiliation>
				<affiliation affiliationId="2">Université Paris Sud, 91405 Orsay, France</affiliation>
				<affiliation affiliationId="3">Université Catholique de Louvain, Belgique</affiliation>
			</affiliations>
			<titre>Génération automatique de questions à partir de textes en français</titre>
			<type>court</type>
			<pages></pages>
			<resume>Nous présentons dans cet article un générateur automatique de questions pour le français. Le système de génération procède par transformation de phrases déclaratives en interrogatives et se base sur une analyse syntaxique préalable de la phrase de base. Nous détaillons les différents types de questions générées. Nous présentons également une évaluation de l’outil, qui démontre que 41 % des questions générées par le système sont parfaitement bien formées.</resume>
			<mots_cles>génération de questions, analyse syntaxique, transformation syntaxique</mots_cles>
			<title></title>
			<abstract>In this article, we present an automatic question generation system for French. The system proceeds by transforming declarative sentences into interrogative sentences, based on a preliminary syntactic analysis of the base sentence. We detail the different types of questions generated. We also present an evaluation of the tool, which shows that 41 % of the questions generated by the system are perfectly well-formed.</abstract>
			<keywords>question generation, syntactic analysis, syntactic transformation</keywords>
		</article>
		<article id="taln-2011-court-007" session="Boosters">
			<auteurs>
				<auteur>
					<prenom>Arnaud</prenom>
					<nom>Grappy</nom>
					<email>Arnaud.Grappy@limsi.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Brigitte</prenom>
					<nom>Grau</nom>
					<email>Brigitte.Grau@limsi.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>3</affiliationId>
				</auteur>
				<auteur>
					<prenom>Mathieu-Henri</prenom>
					<nom>Falco</nom>
					<email>Mathieu-Henri.Falco@limsi.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Anne-Laure</prenom>
					<nom>Ligozat</nom>
					<email>Anne-Laure.Ligozat@limsi.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>3</affiliationId>
				</auteur>
				<auteur>
					<prenom>Isabelle</prenom>
					<nom>Robba</nom>
					<email>Isabelle.Robba@limsi.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>4</affiliationId>
				</auteur>
				<auteur>
					<prenom>Anne</prenom>
					<nom>Vilnat</nom>
					<email>Anne.Vilnat@limsi.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIMSI-CNRS</affiliation>
				<affiliation affiliationId="2">Université Paris 11</affiliation>
				<affiliation affiliationId="3">ENSIIE</affiliation>
				<affiliation affiliationId="3">UVSQ</affiliation>
			</affiliations>
			<titre>Sélection de réponses à des questions dans un corpus Web par validation</titre>
			<type>court</type>
			<pages></pages>
			<resume>Les systèmes de questions réponses recherchent la réponse à une question posée en langue naturelle dans un ensemble de documents. Les collectionsWeb diffèrent des articles de journaux de par leurs structures et leur style. Pour tenir compte de ces spécificités nous avons développé un système fondé sur une approche robuste de validation où des réponses candidates sont extraites à partir de courts passages textuels puis ordonnées par apprentissage. Les résultats montrent une amélioration du MRR (Mean Reciprocal Rank) de 48% par rapport à la baseline.</resume>
			<mots_cles>systèmes de questions réponses, validation de réponses, analyse de documents Web</mots_cles>
			<title></title>
			<abstract>Question answering systems look for the answer of a question given in natural language in a large collection of documents. Web documents have a structure and a style different from those of newspaper articles. We developed a QA system based on an answer validation process able to handle Web specificity. Large number of candidate answers are extracted from short passages in order to be validated according to question and passage characteristics. The validation module is based on a machine learning approach.We show that our system outperforms a baseline by up to 48% in MRR (Mean Reciprocal Rank).</abstract>
			<keywords>question-answering system, answer validation, Web document analysis</keywords>
		</article>
		<article id="taln-2011-court-008" session="Boosters">
			<auteurs>
				<auteur>
					<prenom>Wei</prenom>
					<nom>Wang</nom>
					<email>wei.wang@cea.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Romaric</prenom>
					<nom>Besançon</nom>
					<email>romaric.besancon@cea.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Olivier</prenom>
					<nom>Ferret</nom>
					<email>olivier.ferret@cea.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Brigitte</prenom>
					<nom>Grau</nom>
					<email>brigitte.grau@limsi.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">CEA, LIST, 18 route du Panorama, BP 6, 92265 Fontenay-aux-Roses</affiliation>
				<affiliation affiliationId="2">LIMSI, UPR-3251 CNRS-DR4, Bat. 508, BP 133, 91403 Orsay Cedex</affiliation>
			</affiliations>
			<titre>Filtrage de relations pour l’extraction d’information non supervisée</titre>
			<type>court</type>
			<pages></pages>
			<resume>Le domaine de l’extraction d’information s’est récemment développé en limitant les contraintes sur la définition des informations à extraire, ouvrant la voie à des applications de veille plus ouvertes. Dans ce contexte de l’extraction d’information non supervisée, nous nous intéressons à l’identification et la caractérisation de nouvelles relations entre des types d’entités fixés. Un des défis de cette tâche est de faire face à la masse importante de candidats pour ces relations lorsque l’on considère des corpus de grande taille. Nous présentons dans cet article une approche pour le filtrage des relations combinant méthode heuristique et méthode par apprentissage. Nous évaluons ce filtrage de manière intrinsèque et par son impact sur un regroupement sémantique des relations.</resume>
			<mots_cles>Extraction d’information non supervisée, filtrage, apprentissage automatique, clustering</mots_cles>
			<title></title>
			<abstract>Information Extraction have recently been extended to new areas, by loosening the constraints on the strict definition of the information extracted, thus allowing to design more open information extraction systems. In this new domain of unsupervised information extraction, we focus on the task of extracting and characterizing new relations between a given set of entity types. One of the challenges of this task is to deal with the large amount of candidate relations when extracting them from a large corpus. We propose in this paper an approach for filtering such candidate relations, based on heuristic and machine learning methods. We present an evaluation of this filtering phase and an evaluation of the impact of the filtering on the semantic clustering of relations.</abstract>
			<keywords>Unsupervised information extraction, filtering, machine learning, clustering</keywords>
		</article>
		<article id="taln-2011-court-009" session="Boosters">
			<auteurs>
				<auteur>
					<prenom>Béatrice</prenom>
					<nom>Arnulphy</nom>
					<email>Beatrice.Arnulphy@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Xavier</prenom>
					<nom>Tannier</nom>
					<email>Xavier.Tannier@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Anne</prenom>
					<nom>Vilnat</nom>
					<email>Anne.Vilnat@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Univ. Paris-Sud 11, 91405 Orsay</affiliation>
				<affiliation affiliationId="2">LIMSI-CNRS, 91403 Orsay</affiliation>
			</affiliations>
			<titre>Un lexique pondéré des noms d’événements en français</titre>
			<type>court</type>
			<pages></pages>
			<resume>Cet article décrit une étude sur l’annotation automatique des noms d’événements dans les textes en français. Plusieurs lexiques existants sont utilisés, ainsi que des règles syntaxiques d’extraction, et un lexique composé de façon automatique, permettant de fournir une valeur sur le niveau d’ambiguïté du mot en tant qu’événement. Cette nouvelle information permettrait d’aider à la désambiguïsation des noms d’événements en contexte.</resume>
			<mots_cles>extraction d’information, événements nominaux, lexiques</mots_cles>
			<title></title>
			<abstract>This article describes a study on automatic extraction of event nominals in French texts. Some existing lexicons are used, as well as some syntactic extraction rules, and a new, automatically built lexicon is presented. This lexicon gives a value concerning the level of ambiguity of each word as an event.</abstract>
			<keywords>information extraction, nominal events, lexicons</keywords>
		</article>
		<article id="taln-2011-court-010" session="Boosters">
			<auteurs>
				<auteur>
					<prenom>Stéphane</prenom>
					<nom>Huet</nom>
					<email>stephane.huet@univ-avignon.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Fabrice</prenom>
					<nom>Lefèvre</nom>
					<email>fabrice.lefevre@univ-avignon.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université d’Avignon, LIA-CERI, France</affiliation>
			</affiliations>
			<titre>Alignement automatique pour la compréhension littérale de l’oral par approche segmentale</titre>
			<type>court</type>
			<pages></pages>
			<resume>Les approches statistiques les plus performantes actuellement pour la compréhension automatique du langage naturel nécessitent une annotation segmentale des données d’entraînement. Nous étudions dans cet article une alternative permettant d’obtenir de façon non-supervisée un alignement segmental d’unités conceptuelles sur les mots. L’impact de l’alignement automatique sur les performances du système de compréhension est évalué sur une tâche de dialogue oral.</resume>
			<mots_cles>Alignement non-supervisé, compréhension de la parole</mots_cles>
			<title></title>
			<abstract>Most recent efficient statistical approaches for language understanding require a segmental annotation of the training data. In this paper we study an alternative that obtains a segmental alignment of conceptual units with words in an unsupervised way. The impact of the automatic alignment on the understanding system performance is evaluated on a spoken dialogue task.</abstract>
			<keywords>Unsupervised alignment, spoken language understanding</keywords>
		</article>
		<article id="taln-2011-court-011" session="Boosters">
			<auteurs>
				<auteur>
					<prenom>Romain</prenom>
					<nom>Deveaud</nom>
					<email>romain.deveaud@univ-avignon.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Eric</prenom>
					<nom>Sanjuan</nom>
					<email>eric.sanjuan@univ-avignon.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Patrice</prenom>
					<nom>Bellot</nom>
					<email>patrice.bellot@univ-avignon.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIA - Université d’Avignon, 339, chemin des Meinajariès Agroparc BP 91228, 84 911 Avignon Cedex 9</affiliation>
			</affiliations>
			<titre>Ajout d’informations contextuelles pour la recherche de passages au sein de Wikipédia</titre>
			<type>court</type>
			<pages></pages>
			<resume>La recherche de passages consiste à extraire uniquement des passages pertinents par rapport à une requête utilisateur plutôt qu’un ensemble de documents entiers. Cette récupération de passages est souvent handicapée par le manque d’informations complémentaires concernant le contexte de la recherche initiée par l’utilisateur. Des études montrent que l’ajout d’informations contextuelles par l’utilisateur peut améliorer les performances des systèmes de recherche de passages. Nous confirmons ces observations dans cet article, et nous introduisons également une méthode d’enrichissement de la requête à partir d’informations contextuelles issues de documents encyclopédiques. Nous menons des expérimentations en utilisant la collection et les méthodes d’évaluation proposées par la campagne INEX. Les résultats obtenus montrent que l’ajout d’informations contextuelles permet d’améliorer significativement les performances de notre système de recherche de passages. Nous observons également que notre approche automatique obtient les meilleurs résultats parmi les différentes approches que nous évaluons.</resume>
			<mots_cles>Recherche de passages, enrichissement de requêtes, contexte, Wikipedia, INEX, entropie</mots_cles>
			<title></title>
			<abstract>Traditional Information Retrieval aims to present whole documents that are relevant to a user request. However, there is sometimes only one sentence that is relevant in the document. The purpose of Focused Information Retrieval is to find and extract relevant passages instead of entire documents. This retrieval task often lacks of complement concerning the context of the information need of the user. Studies show that the performance of focused retrieval systems are improved when user manually add contextual information. In this paper we confirm these observation, and we also introduce a query expansion approach using contextual information taken from encyclopedic documents. We use the INEX workshop collection and evaluation framework in our experiments. Results show that adding contextual information significantly improves the performance of our focused retrieval system. We also see that our automatic approach obtains the best results among the different approach we evaluate.</abstract>
			<keywords>Focused retrieval, query expansion, context, Wikipedia, INEX, entropy</keywords>
		</article>
		<article id="taln-2011-court-012" session="Boosters">
			<auteurs>
				<auteur>
					<prenom>Jana</prenom>
					<nom>Strnadová</nom>
					<email>strnadjana13@gmail.com</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Benoît</prenom>
					<nom>Sagot</nom>
					<email>benoit.sagot@inria.fr</email>
					<affiliationId>3</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LLF, CNRS &amp; Univ. Paris 7, 5 rue Thomas Mann, 75205 Paris Cedex 13, France</affiliation>
				<affiliation affiliationId="2">Univerzita Karlova, Filozofická Fakulta, nám. J. Palacha 2, 116 38 Prague, Rép. Tchèque</affiliation>
				<affiliation affiliationId="3">Alpage, INRIA &amp; Univ. Paris 7, Rocquencourt, BP 105, 78153 Le Chesnay Cedex, France</affiliation>
			</affiliations>
			<titre>Construction d’un lexique des adjectifs dénominaux</titre>
			<type>court</type>
			<pages></pages>
			<resume>Après une brève analyse linguistique des adjectifs dénominaux en français, nous décrivons le processus automatique que nous avons mis en place à partir de lexiques et de corpus volumineux pour construire un lexique d’adjectifs dénominaux dérivés de manière régulière. Nous estimons à la fois la précision et la couverture du lexique dérivationnel obtenu. À terme, ce lexique librement disponible aura été validé manuellement et contiendra également les adjectifs dénominaux à base supplétive.</resume>
			<mots_cles>Adjectifs dénominaux, dérivation morphologique, lexique dérivationnel</mots_cles>
			<title></title>
			<abstract>After a brief linguistic analysis of French denominal adjectives, we describe the automatic technique based on large-scale lexicons and corpora that we developed for building a lexicon of regular denominal adjectives. We evaluate both the precision and coverage of the resulting derivational lexicon. This freely available lexicon should eventually be fully manually validated and contain denominal adjectives with a suppletive base.</abstract>
			<keywords>Denominal adjectives, morphological derivation, derivational lexicon</keywords>
		</article>
		<article id="taln-2011-court-013" session="Boosters">
			<auteurs>
				<auteur>
					<prenom>Benoît</prenom>
					<nom>Sagot</nom>
					<email>benoit.sagot@inria.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Géraldine</prenom>
					<nom>Walther</nom>
					<email>geraldine.walther@linguist.jussieu.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Pegah</prenom>
					<nom>Faghiri</nom>
					<email>pegah.faghiri@etud.sorbonne-nouvelle.fr</email>
					<affiliationId>3</affiliationId>
				</auteur>
				<auteur>
					<prenom>Pollet</prenom>
					<nom>Samvelian</nom>
					<email>pollet.samvelian@univ-paris3.fr</email>
					<affiliationId>3</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Alpage, INRIA &amp; Univ. Paris 7, Rocquencourt, BP 105, 78153 Le Chesnay Cedex, France</affiliation>
				<affiliation affiliationId="2">LLF, CNRS &amp; Univ. Paris 7, 5 rue Thomas Mann, 75205 Paris Cedex 13, France</affiliation>
				<affiliation affiliationId="3">MII, CNRS &amp; Univ. Paris 3, 27 rue Paul Bert, 94204 Ivry-sur-Seine, France</affiliation>
			</affiliations>
			<titre>Développement de ressources pour le persan : PerLex 2, nouveau lexique morphologique et MEltfa, étiqueteur morphosyntaxique</titre>
			<type>court</type>
			<pages></pages>
			<resume>Nous présentons une nouvelle version de PerLex, lexique morphologique du persan, une version corrigée et partiellement réannotée du corpus étiqueté BijanKhan (BijanKhan, 2004) et MEltfa, un nouvel étiqueteur morphosyntaxique librement disponible pour le persan. Après avoir développé une première version de PerLex (Sagot &amp; Walther, 2010), nous en proposons donc ici une version améliorée. Outre une validation manuelle partielle, PerLex 2 repose désormais sur un inventaire de catégories linguistiquement motivé. Nous avons également développé une nouvelle version du corpus BijanKhan : elle contient des corrections significatives de la tokenisation ainsi qu'un réétiquetage à l'aide des nouvelles catégories. Cette nouvelle version du corpus a enfin été utilisée pour l'entraînement de MEltfa, notre étiqueteur morphosyntaxique pour le persan librement disponible, s'appuyant à la fois sur ce nouvel inventaire de catégories, sur PerLex 2 et sur le système d'étiquetage MElt (Denis &amp; Sagot, 2009).</resume>
			<mots_cles>Ressource lexicale, validation, étiqueteur morphosyntaxique, persan, catégories, PerLex, MElt</mots_cles>
			<title></title>
			<abstract>We present a new version of PerLex, the morphological lexicon for the Persian language, a corrected and partially re-annotated version of the BijanKhan corpus (BijanKhan, 2004) and MEltfa, a new freely available POS-tagger for the Persian language. After PerLex's first version (Sagot &amp; Walther, 2010), we propose an improved version of our morphological lexicon. Apart from a partial manual validation, PerLex 2 now relies on a set of linguistically motivated POS. Based on these POS, we also developped a new version of the BijanKhan corpus with significant corrections of the tokenisation. It has been re-tagged according to the new set of POS. The new version of the BijanKhan corpus has been used to develop MEltfa, our new freely-available POS-tagger for the Persian language, based on the new POS set, PerLex 2 and the MElt tagging system (Denis &amp; Sagot, 2009).</abstract>
			<keywords>Lexical resource, validation, tagger, Persian, POS, PerLex, MElt</keywords>
		</article>
		<article id="taln-2011-court-014" session="Boosters">
			<auteurs>
				<auteur>
					<prenom>Mirabela</prenom>
					<nom>Navlea</nom>
					<email>navlea@unistra.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Amalia</prenom>
					<nom>Todiraşcu</nom>
					<email>todiras@unistra.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université de Strasbourg, 22 rue René Descartes, BP, 80010, 67084 Strasbourg, cedex</affiliation>
			</affiliations>
			<titre>Identification de cognats à partir de corpus parallèles français-roumain</titre>
			<type>court</type>
			<pages></pages>
			<resume>Cet article présente une méthode hybride d’identification de cognats français - roumain. Cette méthode exploite des corpus parallèles alignés au niveau propositionnel, lemmatisés et étiquetés (avec des propriétés morphosyntaxiques). Notre méthode combine des techniques statistiques et des informations linguistiques pour améliorer les résultats obtenus. Nous évaluons le module d’identification de cognats et nous faisons une comparaison avec des méthodes statistiques pures, afin d’étudier l’impact des informations linguistiques utilisées sur la qualité des résultats obtenus. Nous montrons que l’utilisation des informations linguistiques augmente significativement la performance de la méthode.</resume>
			<mots_cles>cognat, identification de cognats, corpus parallèles alignés au niveau propositionnel</mots_cles>
			<title></title>
			<abstract>This paper describes a hybrid French - Romanian cognate identification method. This method uses lemmatized, tagged (POS tags) and sentence-aligned parallel corpora. Our method combines statistical techniques and linguistic information in order to improve the results. We evaluate the cognate identification method and we compare it to other methods using pure statistical techniques to study the impact of the used linguistic information on the quality of the results. We show that the use of linguistic information in the cognate identification method significantly improves the results.</abstract>
			<keywords>cognate, cognate identification, sentence-aligned parallel corpora</keywords>
		</article>
		<article id="taln-2011-court-015" session="Boosters">
			<auteurs>
				<auteur>
					<prenom>Richard</prenom>
					<nom>Beaufort</nom>
					<email>richard.beaufort@uclouvain.be</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Sophie</prenom>
					<nom>Roekhaut</nom>
					<email>sophie.roekhaut@uclouvain.be</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">CENTAL, UCLouvain, Place Blaise Pascal 1, B-1348 Louvain-la-Neuve</affiliation>
			</affiliations>
			<titre>Le TAL au service de l’ALAO/ELAO L’exemple des exercices de dictée automatisés</titre>
			<type>court</type>
			<pages></pages>
			<resume>Ce papier s’inscrit dans le cadre général de l’Apprentissage et de l’Enseignement des Langues Assistés par Ordinateur, et concerne plus particulièrement l’automatisation des exercices de dictée. Il présente une méthode de correction des copies d’apprenants qui se veut originale en deux points. Premièrement, la méthode exploite la composition d’automates à états finis pour détecter et pour analyser les erreurs. Deuxièmement, elle repose sur une analyse morphosyntaxique automatique de l’original de la dictée, ce qui facilite la production de diagnostics.</resume>
			<mots_cles>ALAO/ELAO, exercices de dictée, alignement, diagnostic, machines à états finis</mots_cles>
			<title></title>
			<abstract>This paper comes within the scope of the Computer Assisted Language Learning framework, and addresses more especially the automation of dictation exercises. It presents a correction method of learners’ copies that is original in two ways. First, the method exploits the composition of finite-state automata, to both detect and analyze the errors. Second, it relies on an automatic morphosyntactic analysis of the original dictation, which makes it easier to produce diagnoses.</abstract>
			<keywords>CALL, dictation exercises, alignment, diagnosis, finite-state machines</keywords>
		</article>
		<article id="taln-2011-court-016" session="Boosters">
			<auteurs>
				<auteur>
					<prenom>Maxime</prenom>
					<nom>Amblard</nom>
					<email>maxime.amblard@univ-nancy2.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>4</affiliationId>
				</auteur>
				<auteur>
					<prenom>Michel</prenom>
					<nom>Musiol</nom>
					<email>michel.musiol@univ-nancy2.fr</email>
					<affiliationId>2</affiliationId>
					<affiliationId>4</affiliationId>
				</auteur>
				<auteur>
					<prenom>Manuel</prenom>
					<nom>Rebuschi</nom>
					<email>manuel.rebuschi@univ-nancy2.fr</email>
					<affiliationId>3</affiliationId>
					<affiliationId>4</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LORIA - UMR 7503</affiliation>
				<affiliation affiliationId="2">InterPSY - EA 4432 / MSH Lorraine USR 3261</affiliation>
				<affiliation affiliationId="3">Archives Poincaré - UMR 7117 / MSH Lorraine USR 3261</affiliation>
				<affiliation affiliationId="4">Université Nancy 2 – 54000 Nancy</affiliation>
			</affiliations>
			<titre>Une analyse basée sur la S-DRT pour la modélisation de dialogues pathologiques</titre>
			<type>court</type>
			<pages></pages>
			<resume>Dans cet article, nous présentons la définition et l’étude d’un corpus de dialogues entre un schizophrène et un interlocuteur ayant pour objectif la conduite et le maintien de l’échange. Nous avons identifié des discontinuités significatives chez les schizophrènes paranoïdes. Une représentation issue de la S-DRT (sa partie pragmatique) permet de rendre compte des ces usages non standards.</resume>
			<mots_cles>S-DRT, interaction verbale, schizophrénie, dialogue pathologique, incohérence pragmatique</mots_cles>
			<title></title>
			<abstract>In this article, we present a corpus of dialogues between a schizophrenic speaker and an interlocutor who drives the dialogue. We had identified specific discontinuities for paranoid schizophrenics. We propose a modeling of these discontinuities with S-DRT (its pragmatic part).</abstract>
			<keywords>S-DRT, verbal interaction, schizophrenia, pathological dialogue, pragmatical incoherence</keywords>
		</article>
		<article id="taln-2011-court-017" session="Boosters">
			<auteurs>
				<auteur>
					<prenom>Anne</prenom>
					<nom>Göhring</nom>
					<email>Gohring@cl.uzh.ch</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Martin</prenom>
					<nom>Volk</nom>
					<email>Volk@cl.uzh.ch</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">UZH, Institute of Computational Linguistics, University of Zurich, Switzerland</affiliation>
			</affiliations>
			<titre>The Text+Berg Corpus An Alpine French-German Parallel Resource</titre>
			<type>court</type>
			<pages></pages>
			<resume>Cet article présente un corpus parallèle français-allemand de plus de 4 millions de mots issu de la numérisation d’un corpus alpin multilingue. Ce corpus est une précieuse ressource pour de nombreuses études de linguistique comparée et du patrimoine culturel ainsi que pour le développement d’un système statistique de traduction automatique dans un domaine spécifique. Nous avons annoté un échantillon de ce corpus parallèle et aligné les structures arborées au niveau des mots, des constituants et des phrases. Cet “alpine treebank” est le premier corpus arboré parallèle français-allemand de haute qualité (manuellement contrôlé), de libre accès et dans un domaine et un genre nouveau : le récit d’alpinisme.</resume>
			<mots_cles>corpus alpin français-allemand, structures arborées parallèles, annotation morphosyntaxique du français</mots_cles>
			<title></title>
			<abstract>This article presents a French-German parallel corpus of more than 4 million tokens which we have compiled as part of the digitization of a large multilingual heritage corpus of alpine texts. This corpus is a valuable resource for cultural heritage and cross-linguistic studies as well as for the development of domain-specific machine translation systems. We have turned a small fraction of the parallel corpus into a high-quality parallel treebank with manually checked syntactic annotations and cross-language word and phrase alignments. This alpine treebank is the first freely available French-German parallel treebank. It complements other treebanks with texts in a new domain and genre : mountaineering reports.</abstract>
			<keywords>French-German alpine corpus, parallel treebank, French morphosyntactic annotation, Text+Berg, e-Humanities</keywords>
		</article>
		<article id="taln-2011-court-018" session="Boosters">
			<auteurs>
				<auteur>
					<prenom>Aurélien</prenom>
					<nom>Bossard</nom>
					<email>Aurelien.Bossard@orange-ftgroup.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Émilie</prenom>
					<nom>Guimier De Neef</nom>
					<email>emilie;GuimierDeNeef@orange-ftgroup.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Orange Labs, 2 av. Pierre Marzin, 22300 Lannion, France</affiliation>
			</affiliations>
			<titre>Ordonner un résumé automatique multi-documents fondé sur une classification des phrases en classes lexicales</titre>
			<type>court</type>
			<pages></pages>
			<resume>Nous présentons différentes méthodes de réordonnancement de phrases pour le résumé automatique fondé sur une classification des phrases à résumer en classes thématiques. Nous comparons ces méthodes à deux baselines : ordonnancement des phrases selon leur pertinence et ordonnancement selon la date et la position dans le document d’origine. Nous avons fait évaluer les résumés obtenus sur le corpus RPM2 par 4 annotateurs et présentons les résultats.</resume>
			<mots_cles>Résumé automatique, ordonnancement de phrases</mots_cles>
			<title></title>
			<abstract>We present several sentence ordering methods for automatic summarization which are specific to multi-document summarizers, based on sentences subtopic clustering. These methods are compared to two baselines : sentence ordering according to pertinence and according to publication date and inner document position. The resulting summaries on RPM2 corpus have been evaluated by four judges.</abstract>
			<keywords>Automatic summarization, sentence ordering</keywords>
		</article>
		<article id="taln-2011-court-019" session="Boosters">
			<auteurs>
				<auteur>
					<prenom>Fériel</prenom>
					<nom>Ben Fraj</nom>
					<email>Feriel.BenFraj@riadi.rnu.tn</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Laboratoire RIADI, École Nationale des Sciences de l'Informatique, 2010 Manouba, Tunisie</affiliation>
			</affiliations>
			<titre>Construction d’une grammaire d’arbres adjoints pour la langue arabe</titre>
			<type>court</type>
			<pages></pages>
			<resume>La langue arabe présente des spécificités qui la rendent plus ambigüe que d’autres langues naturelles. Sa morphologie, sa syntaxe ainsi que sa sémantique sont en corrélation et se complètent l’une l’autre. Dans le but de construire une grammaire qui soit adaptée à ces spécificités, nous avons conçu et développé une application d’aide à la création des règles syntaxiques licites suivant le formalisme d’arbres adjoints. Cette application est modulaire et enrichie par des astuces de contrôle de la création et aussi d’une interface conviviale pour assister l’utilisateur final dans la gestion des créations prévues.</resume>
			<mots_cles>Outil semi-automatique, grammaire d’arbres adjoints, langue arabe, traits d’unification</mots_cles>
			<title></title>
			<abstract>The Arabic language consists of a set of specificities. Thus, it is more ambiguous than other natural languages. Its morphology, syntax and semantic are correlated to each other. We have constructed an application for the construction of a tree adjoining grammar which respects the characteristics of the Arabic language. This tool allows constructing the grammatical rules as elementary trees enriched by different feature structures. It helps the user by its interface and control system to manage correct and uniform rules.</abstract>
			<keywords>semi-automatic tool, tree adjoining grammar, Arabic language, feature structures</keywords>
		</article>
		<article id="taln-2011-court-020" session="Boosters">
			<auteurs>
				<auteur>
					<prenom>Enrique</prenom>
					<nom>Henestroza Anguiano</nom>
					<email>henestro@inria.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Pascal</prenom>
					<nom>Denis</nom>
					<email>pascal.denis@inria.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Alpage, INRIA Paris-Rocquencourt &amp; Université Paris 7, Rocquencourt, BP 105, 78153 Le Chesnay Cedex, France</affiliation>
			</affiliations>
			<titre>FreDist : Automatic construction of distributional thesauri for French</titre>
			<type>court</type>
			<pages></pages>
			<resume>Dans cet article, nous présentons FreDist, un logiciel libre pour la construction automatique de thésaurus distributionnels à partir de corpus de texte, ainsi qu’une évaluation des différents ressources ainsi produites. Suivant les travaux de (Lin, 1998) et (Curran, 2004), nous utilisons un corpus journalistique de grande taille et implémentons différentes options pour : le type de relation contexte lexical, la fonction de poids, et la fonction de mesure de similarité. Prenant l’EuroWordNet français et le WOLF comme références, notre évaluation révèle, de manière originale, que c’est l’approche qui combine contextes linéaires (ici, de type bigrammes) et contextes syntaxiques qui semble fournir le meilleur thésaurus. Enfin, nous espérons que notre logiciel, distribué avec nos meilleurs thésaurus pour le français, seront utiles à la communauté TAL.</resume>
			<mots_cles>thésaurus distributionnel, similarité sémantique, méthodes non supervisées, lexique</mots_cles>
			<title></title>
			<abstract>In this article we present FreDist, a freely available software package for the automatic construction of distributional thesauri from text corpora, as well as an evaluation of various distributional similarity metrics for French. Following from the work of (Lin, 1998) and (Curran, 2004), we use a large corpus of journalistic text and implement different choices for the type of lexical context relation, the weight function, and the measure function needed to build a distributional thesaurus. Using the EuroWordNet and WOLF wordnet resources for French as gold-standard references for our evaluation, we obtain the novel result that combining bigram and syntactic dependency context relations results in higher quality distributional thesauri. In addition, we hope that our software package and a joint release of our best thesauri for French will be useful to the NLP community.</abstract>
			<keywords>distributional thesaurus, semantic similarity, unsupervised methods, lexicon</keywords>
		</article>
		<article id="taln-2011-court-021" session="Boosters">
			<auteurs>
				<auteur>
					<prenom>Ali</prenom>
					<nom>Reza Ebadat</nom>
					<email>ali_reza.ebadat@inria.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Vincent</prenom>
					<nom>Claveau</nom>
					<email>vincent.claveau@irisa.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Pascale</prenom>
					<nom>Sébillot</nom>
					<email>pascale.sebillot@irisa.fr</email>
					<affiliationId>3</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">INRIA-INSA</affiliation>
				<affiliation affiliationId="2">IRISA-CNRS</affiliation>
				<affiliation affiliationId="3">IRISA-INSA</affiliation>
			</affiliations>
			<titre>Using shallow linguistic features for relation extraction in bio-medical texts</titre>
			<type>court</type>
			<pages></pages>
			<resume>Dans cet article, nous proposons de modéliser la tâche d’extraction de relations à partir de corpus textuels comme un problème de classification. Nous montrons que, dans ce cadre, des représentations fondées sur des informations linguistiques de surface sont suffisantes pour que des algorithmes d’apprentissage artificiel standards les exploitant rivalisent avec les meilleurs systèmes d’extraction de relations reposant sur des connaissances issues d’analyses profondes (analyses syntaxiques ou sémantiques). Nous montrons également qu’en prenant davantage en compte les spécificités de la tâche d’extraction à réaliser et des données disponibles, il est possible d’obtenir des méthodes encore plus efficaces tout en exploitant ces informations simples. La technique originale à base d’apprentissage « paresseux » et de modèles de langue que nous évaluons en extraction d’interactions géniques sur les données du challenge LLL2005 dépasse les résultats de l’état de l’art.</resume>
			<mots_cles>Extraction de relations, classification, apprentissage paresseux, modèle de langue, analyse linguistique de surface</mots_cles>
			<title></title>
			<abstract>In this paper, we model the corpus-based relation extraction task as a classification problem. We show that, in this framework, standard machine learning systems exploiting representations simply based on shallow linguistic information can rival state-of-the-art systems that rely on deep linguistic analysis. Even more effective systems can be obtained, still using these easy and reliable pieces of information, if the specifics of the extraction task and the data are taken into account. Our original method combining lazy learning and language modeling out-performs the existing systems when evaluated on the LLL2005 protein-protein interaction extraction task data.</abstract>
			<keywords>Relation extraction, classification, lazy learning, langage model, shallow linguistic analysis</keywords>
		</article>
		<article id="taln-2011-court-022" session="Boosters">
			<auteurs>
				<auteur>
					<prenom>Julien</prenom>
					<nom>Lebranchu</nom>
					<email>Julien.Lebranchu@unicaen.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Yann</prenom>
					<nom>Mathet</nom>
					<email>Yann.Mathet@unicaen.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université de Caen Basse-Normandie, UMR 6072 GREYC, F-14032 Caen, France</affiliation>
			</affiliations>
			<titre>Vers une prise en charge approfondie des phénomènes itératifs par TimeML</titre>
			<type>court</type>
			<pages></pages>
			<resume>Les travaux menés ces dernières années autour de l’itération en langue, tant par la communauté linguistique que par celle du TAL, ont mis au jour des phénomènes particuliers, non réductibles aux représentations temporelles classiques. En particulier, une itération ne saurait structurellement être réduite à une simple énumération de procès, et du point de vue de l’aspect, met en jeu simultanément deux visées aspectuelles indépendantes. Le formalisme TimeML, qui a vocation à annoter les informations temporelles portées par un texte, intègre déjà des éléments relatifs aux itérations, mais ne prend pas en compte ces dernières avancées. C’est ce que nous entreprenons de faire dans cet article, en proposant une extension à ce formalisme.</resume>
			<mots_cles>TimeML, discours, sémantique, phénomènes itératifs</mots_cles>
			<title></title>
			<abstract>The work that has recently been done concerning the iterative phenomena in language, which was performed by the linguistic and TAL communities, has illuminated specific phenomena, not reducible to classical time representations. In particular, an iteration can not structurally be reduced to a simple listing of process, and involves simultaneously two independent referred aspectual. The TimeML formalism, which aims to annotate temporal information of a given text, includes already relative elements to iterations but does not take into account recent advances. That is the reason why in this paper, we propose to extend this formalism.</abstract>
			<keywords>TimeML, discourse, semantics, iterative phenomena</keywords>
		</article>
		<article id="taln-2011-court-023" session="Boosters">
			<auteurs>
				<auteur>
					<prenom>Noémi</prenom>
					<nom>Boubel</nom>
					<email>noemi.boubel@uclouvain.be</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Yves</prenom>
					<nom>Bestgen</nom>
					<email>yves.bestgen@uclouvain.be</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">UCLouvain, Cental, Place Blaise Pascal, 1, B-1348 Louvain-la-Neuve, Belgique</affiliation>
				<affiliation affiliationId="2">UCLouvain, CECL, B-1348 Louvain-la-Neuve, Belgique</affiliation>
			</affiliations>
			<titre>Une procédure pour identifier les modifieurs de la valence affective d'un mot dans des textes</titre>
			<type>court</type>
			<pages></pages>
			<resume>Cette recherche s'inscrit dans le champ de la fouille d'opinion et, plus particulièrement, dans celui de l'analyse de la polarité d’une phrase ou d'un syntagme. Dans ce cadre, la prise en compte du contexte linguistique dans lequel apparaissent les mots porteurs de valence est particulièrement importante. Nous proposons une méthodologie pour extraire automatiquement de corpus de textes de telles expressions linguistiques. Cette approche s'appuie sur un corpus de textes, ou d'extraits de textes, dont la valence est connue, sur un lexique de valence construit à partir de ce corpus au moyen d'une procédure automatique et sur un analyseur syntaxique. Une étude exploratoire, limitée à la seule relation syntaxique associant un adverbe à un adjectif, laisse entrevoir les potentialités de l'approche.</resume>
			<mots_cles>modifieurs de valence, fouille d’opinion, lexique de valence</mots_cles>
			<title></title>
			<abstract>This research is situated within the field of opinion mining and focuses more particularly on the analysis of the opinion expressed in a sentence or a syntagm. Within this frame of research, taking into account the linguistic context in which words which carry valence appear is particularly important. We propose a methodology to automatically extract such linguistic expressions from text corpora. This approach is based on (a) a corpus of texts, or text excerpts, the valence of which is known, (b) on a valence lexicon built from this corpus using an automatic procedure and (c) on a parser. An exploratory study, focusing on the syntactic relation associating an adverb to an adjective, shows the potential of the approach.</abstract>
			<keywords>contextual valence shifter, opinion mining, semantic orientation lexicon</keywords>
		</article>
		<article id="taln-2011-court-024" session="Boosters">
			<auteurs>
				<auteur>
					<prenom>Yann</prenom>
					<nom>Mathet</nom>
					<email>Yann.Mathet@unicaen.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Antoine</prenom>
					<nom>Widlöcher</nom>
					<email>Antoine.Widlocher@unicaen.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">GREYC, UMR CNRS 6072, Université de Caen, 14032 Caen Cedex</affiliation>
			</affiliations>
			<titre>Stratégie d’exploration de corpus multi-annotés avec GlozzQL</titre>
			<type>court</type>
			<pages></pages>
			<resume>La multiplication des travaux sur corpus, en linguistique computationnelle et en TAL, conduit à la multiplication des campagnes d’annotation et des corpus multi-annotés, porteurs d’informations relatives à des phénomènes variés, envisagés par des annotateurs multiples, parfois automatiques. Pour mieux comprendre les phénomènes que ces campagnes prennent pour objets, ou pour contrôler les données en vue de l’établissement d’un corpus de référence, il est nécessaire de disposer d’outils permettant d’explorer les annotations. Nous présentons une stratégie possible et son opérationalisation dans la plate-forme Glozz par le langage GlozzQL.</resume>
			<mots_cles>Corpus, Annotation, Exploration, GlozzQL</mots_cles>
			<title></title>
			<abstract>More and more works in compuational linguistics and NLP rely on corpora. They lead to an increasing number of annotation campaigns and multi-annotated corpora, providing informations on various linguistic phenomena, annotated by several annotators or computational processes. In order to understand these linguistic phenomena, or to control annotated data, tools dedicated to annotated data mining are needed. We present here an exploration strategy and its implementation within the Glozz platform, GlozzQL.</abstract>
			<keywords>Corpus, Annotation, Exploration, GlozzQL</keywords>
		</article>
		<article id="taln-2011-court-025" session="Boosters">
			<auteurs>
				<auteur>
					<prenom>Fadila</prenom>
					<nom>Hadouche</nom>
					<email>hadouchf@iro.umontreal.ca</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Guy</prenom>
					<nom>Lapalme</nom>
					<email>lapalme@iro.umontreal.ca</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Marie-Claude</prenom>
					<nom>L’Homme</nom>
					<email>mc.lhomme@umontreal.ca</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">RALI, Université de Montréal, C.P 6128 Succursale Centre-ville, Montréal, Québec, Canada H3C 3J7</affiliation>
				<affiliation affiliationId="2">OLST, Université de Montréal, C.P 6128 Succursale Centre-ville, Montréal, Québec, Canada H3C 3J7</affiliation>
			</affiliations>
			<titre>Attribution de rôles sémantiques aux actants des lexies verbales</titre>
			<type>court</type>
			<pages></pages>
			<resume>Dans cet article, nous traitons de l’attribution des rôles sémantiques aux actants de lexies verbales en corpus spécialisé en français. Nous proposons une classification de rôles sémantiques par apprentissage machine basée sur un corpus de lexies verbales annotées manuellement du domaine de l’informatique et d’Internet. Nous proposons également une méthode de partitionnement semi-supervisé pour prendre en compte l’annotation de nouvelles lexies ou de nouveaux rôles sémantiques et de les intégrés dans le système. Cette méthode de partitionnement permet de regrouper les instances d’actants selon les valeurs communes correspondantes aux traits de description des actants dans des groupes d’instances d’actants similaires. La classification de rôles sémantique a obtenu une F-mesure de 93% pour Patient, de 90% pour Agent, de 85% pour Destination et de 76% pour les autres rôles pris ensemble. Quand au partitionnement en regroupant les instances selon leur similarité donne une F-mesure de 88% pour Patient, de 81% pour Agent, de 58% pour Destination et de 46% pour les autres rôles.</resume>
			<mots_cles>Rôles sémantiques, traits syntaxiques, classification, partitionnement semi-supervisé</mots_cles>
			<title></title>
			<abstract>In this paper, we discuss assigning semantic roles to actants of verbal lexical units in French specialized corpus. We propose a machine learning classification of semantic roles based on a corpus of verbal lexical units, which are annotated manually in the Informatics and Internet domain. We also propose a semi supervised clustering method to consider the annotation of new verbal lexical units or new semantic roles and integrated them in the system. Clustering is used to group instances of actants according to their common values corresponding to the features describing these actants into groups of similar instances of actants. The classification model give an F-measure of 93% for Patient, 90% for Agent, 85% for Destination and 76% for other roles. When partitioning by grouping instances according to their similarity gives an F-measure of 88% for Patient, 81% for Agent, 58% for Destination and 46% for other roles.</abstract>
			<keywords>Semantic roles, syntactic features, classification, semi supervised partitioning</keywords>
		</article>
		<article id="taln-2011-court-026" session="Boosters">
			<auteurs>
				<auteur>
					<prenom>Olivier</prenom>
					<nom>Ferret</nom>
					<email>olivier.ferret@cea.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">CEA, LIST, Laboratoire Vision et Ingénierie des Contenus, Fontenay-aux-Roses, F-92265 France</affiliation>
			</affiliations>
			<auteurs></auteurs>
			<titre>Utiliser l’amorçage pour améliorer une mesure de similarité sémantique</titre>
			<type>court</type>
			<pages></pages>
			<resume>Les travaux sur les mesures de similarité sémantique de nature distributionnelle ont abouti à un certain consensus quant à leurs performances et ont montré notamment que leurs résultats sont surtout intéressants pour des mots de forte fréquence et une similarité sémantique étendue, non restreinte aux seuls synonymes. Dans cet article, nous proposons une méthode d’amélioration d’une mesure de similarité classique permettant de rééquilibrer ses résultats pour les mots de plus faible fréquence. Cette méthode est fondée sur un mécanisme d’amorçage : un ensemble d’exemples et de contre-exemples de mots sémantiquement liés sont sélectionnés de façon non supervisée à partir des résultats de la mesure initiale et servent à l’entraînement d’un classifieur supervisé. Celui-ci est ensuite utilisé pour réordonner les voisins sémantiques initiaux. Nous évaluons l’intérêt de ce réordonnancement pour un large ensemble de noms anglais couvrant différents domaines fréquentiels.</resume>
			<mots_cles>Extraction de voisins sémantiques, similarité sémantique, méthodes distributionnelles</mots_cles>
			<title></title>
			<abstract>Work about distributional semantic similarity measures has now widely shown that such measures are mainly reliable for high frequency words and for capturing semantic relatedness rather than strict semantic similarity. In this article, we propose a method for improving such a measure for middle and low frequency words. This method is based on a bootstrapping mechanism : a set of examples and counter-examples of semantically related words are selected in an unsupervised way from the results of the initial measure and used for training a supervised classifier. This classifier is then applied for reranking the initial semantic neighbors. We evaluate the interest of this reranking for a large set of english nouns with various frequencies.</abstract>
			<keywords>Semantic neighbor extraction, semantic similarity, distributional methods</keywords>
		</article>
		<article id="taln-2011-court-027" session="Boosters">
			<auteurs>
				<auteur>
					<prenom>Richard</prenom>
					<nom>Moot</nom>
					<email>richard.moot@labri.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Laurent</prenom>
					<nom>Prévot</nom>
					<email>laurent.prevot@lpl-aix.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Christian</prenom>
					<nom>Retoré</nom>
					<email>christian.retore@labri.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université de Bordeaux, LaBRI &amp; INRIA</affiliation>
				<affiliation affiliationId="2">Université de Provence, LPL</affiliation>
			</affiliations>
			<titre>Un calcul de termes typés pour la pragmatique lexicale: chemins et voyageurs fictifs dans un corpus de récits de voyage</titre>
			<type>court</type>
			<pages></pages>
			<resume>Ce travail s’inscrit dans l’analyse automatique d’un corpus de récits de voyage. À cette fin, nous raffinons la sémantique de Montague pour rendre compte des phénomènes d’adaptation du sens des mots au contexte dans lequel ils apparaissent. Ici, nous modélisons les constructions de type ’le chemin descend pendant une demi-heure’ où ledit chemin introduit un voyageur fictif qui le parcourt, en étendant des idées que le dernier auteur a développé avec Bassac et Mery. Cette introduction du voyageur utilise la montée de type afin que le quantificateur introduisant le voyageur porte sur toute la phrase et que les propriétés du chemin ne deviennent pas des propriétés du voyageur, fût-il fictif. Cette analyse sémantique (ou plutôt sa traduction en lambda-DRT) est d’ores et déjà implantée pour une partie du lexique de Grail.</resume>
			<mots_cles>Sémantique lexicale, pragmatique, sémantique compositionnelle</mots_cles>
			<title></title>
			<abstract>This work is part of the automated analysis of travel stories corpus. To do so, we refine Montague semantics, to model the adaptation of word meaning to the context in which they appear. Here we study construction like ’the path goes down for half an hour’ in which the path introduces a virtual traveller following it, extending ideas of the last author with Bassac, Mery. The introduction of a traveller relies on type raising satisfies the following requirements : the quantification binding the traveller has the widest scope, and properties of the path do not apply to the traveller, be it virtual. This semantical analysis (actually its translation in lambda-DRT) is already implemented for a part of the Grail lexicon.</abstract>
			<keywords>Lexical semantics, pragmatics, compositional semantics</keywords>
		</article>
		<article id="taln-2011-court-028" session="Boosters">
			<auteurs>
				<auteur>
					<prenom>Brigitte</prenom>
					<nom>Bigi</nom>
					<email>brigitte.bigi@lpl-aix.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Cristel</prenom>
					<nom>Portes</nom>
					<email>cristel.portes@lpl-aix.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Agnès</prenom>
					<nom>Steuckardt</nom>
					<email>Agnes.Steuckardt@univ-provence.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Marion</prenom>
					<nom>Tellier</nom>
					<email>marion.tellier@lpl-aix.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Laboratoire Parole &amp; Langage, CNRS &amp; Aix-Marseille Universités, 5, avenue Pasteur, BP 80975, 13604 Aix en Provence, France</affiliation>
			</affiliations>
			<titre>Catégoriser les réponses aux interruptions dans les débats politiques</titre>
			<type>court</type>
			<pages></pages>
			<resume>Cet article traite de l’analyse de débats politiques selon une orientation multimodale. Nous étudions plus particulièrement les réponses aux interruptions lors d’un débat à l’Assemblée nationale. Nous proposons de procéder à l’analyse via des annotations systématiques de différentes modalités. L’analyse argumentative nous a amenée à proposer une typologie de ces réponses. Celle-ci a été mise à l’épreuve d’une classification automatique. La difficulté dans la construction d’un tel système réside dans la nature même des données : multimodales, parfois manquantes et incertaines.</resume>
			<mots_cles>corpus, annotations, multimodalité, classification supervisée</mots_cles>
			<title></title>
			<abstract>This work was conducted to analyze political debates, with a multimodal point of view. Particularly, we focus on the answers produced by a main speakers after he was disrupted. Our approach relies on the annotations of each modality and on their review. We propose a manual categorization of the observed disruptions. A categorization method was applied to validate the manual one. The difficulty is to deal with multimodality, missing values and uncertainty in the automatic classification system.</abstract>
			<keywords>corpus, annotations, multimodality, classification</keywords>
		</article>
		<article id="taln-2011-court-029" session="Boosters">
			<auteurs>
				<auteur>
					<prenom>Ludovic</prenom>
					<nom>Bonnefoy</nom>
					<email>ludovic.bonnefoy@ismart.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Patrice</prenom>
					<nom>Bellot</nom>
					<email>patrice.bellot@univ-avignon.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Michel</prenom>
					<nom>Benoit</nom>
					<email>michel.benoit@ismart.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université d’Avignon - CERI/LIA, Agroparc – B.P. 1228, 84911 Avignon Cedex 9</affiliation>
				<affiliation affiliationId="2">iSmart, Le Mercure A, 13851 Aix-en-Provence Cedex 3</affiliation>
			</affiliations>
			<titre>Mesure non-supervisée du degré d’appartenance d’une entité à un type</titre>
			<type>court</type>
			<pages></pages>
			<resume>La recherche d’entités nommées a été le sujet de nombreux travaux. Cependant, la construction des ressources nécessaires à de tels systèmes reste un problème majeur. Dans ce papier, nous proposons une méthode complémentaire aux outils capables de reconnaître des entités de types larges, dont l’objectif est de déterminer si une entité est d’un type donné, et ce de manière non-supervisée et quel que soit le type. Nous proposons pour cela une approche basée sur la comparaison de modèles de langage estimés à partir du Web. L’intérêt de notre approche est validé par une évaluation sur 100 entités et 273 types différents.</resume>
			<mots_cles>typage d’entités nommées, comparaison de distribution de mots, divergence de Kullback-Leibler</mots_cles>
			<title></title>
			<abstract>Searching for named entities has been the subject of many researches. In this paper, we seek to determine whether a named entity is of a given type and in what extent it is. We propose to address this issue by an unsupervised Web oriented language modeling approach. The interest of it is demonstrated by our evaluation on 100 entities and 273 different types.</abstract>
			<keywords>named entity identification, language modeling approach, Kullback-Leibler divergence</keywords>
		</article>
		<article id="taln-2011-court-030" session="Boosters">
			<auteurs>
				<auteur>
					<prenom>Laurence</prenom>
					<nom>Danlos</nom>
					<email>Laurence.Danlos@linguist.jussieu.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Charlotte</prenom>
					<nom>Roze</nom>
					<email>Charlotte.Roze@linguist.jussieu.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">ALPAGE, Université Paris Diderot (Paris 7), 175 rue du Chevaleret, F-750013 Paris</affiliation>
			</affiliations>
			<titre>Traduction (automatique) des connecteurs de discours</titre>
			<type>court</type>
			<pages></pages>
			<resume>En nous appuyant sur des données fournies par le concordancier bilingue TransSearch qui intègre un alignement statistique au niveau des mots, nous avons effectué une annotation semi-manuelle de la traduction anglaise de deux connecteurs du français. Les résultats de cette annotation montrent que les traductions de ces connecteurs ne correspondent pas aux « transpots » identifiés par TransSearch et encore moins à ce qui est proposé dans les dictionnaires bilingues.</resume>
			<mots_cles>Traduction (automatique), TransSearch, Discours</mots_cles>
			<title></title>
			<abstract>On the basis of data provided by the bilingual concordancer TransSearch which propose a statistical word alignment, we made a semi-manual annotation of the English translation of two French connectives. The results of this annotation show that the translations of these connectives do not correspond to the “transpots” identified by TransSearch and even less to the translations proposed in bilingual dictionaries.</abstract>
			<keywords>(Machine) Translation, TransSearch, Discourse</keywords>
		</article>
		<article id="taln-2011-court-031" session="Boosters">
			<auteurs>
				<auteur>
					<prenom>Bruno</prenom>
					<nom>Cartoni</nom>
					<email>bruno.cartoni@unige.ch</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Louise</prenom>
					<nom>Deléger</nom>
					<email>louise.deleger@cchmc.org</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Département de Linguistique, Université de Genève</affiliation>
				<affiliation affiliationId="2">Division of Biomedical Informatics, Cincinnati Children’s Hospital Medical Center</affiliation>
			</affiliations>
			<titre>Découverte de patrons paraphrastiques en corpus comparable: une approche basée sur les n-grammes</titre>
			<type>court</type>
			<pages></pages>
			<resume>Cet article présente l’utilisation d’un corpus comparable pour l’extraction de patrons de paraphrases. Nous présentons une méthode empirique basée sur l’appariement de n-grammes, permettant d’extraire des patrons de paraphrases dans des corpus comparables d’une même langue (le français), du même domaine (la médecine) mais de registres de langues différents (spécialisé ou grand public). Cette méthode confirme les résultats précédents basés sur des méthodes à base de patrons, et permet d’identifier de nouveaux patrons, apportant également un regard nouveau sur les différences entre les discours de langue générale et spécialisée.</resume>
			<mots_cles>Identification de paraphrases, extraction de patrons, type de discours, domaine médical, corpus comparable monolingue</mots_cles>
			<title></title>
			<abstract>This paper presents the use of a comparable corpus for extracting paraphrase patterns.We present an empirical method based on n-gram matching and ordering, to extract paraphrase pattern in comparable corpora of the same language (French) and the same domaine, but of two different registers (lay and specialised). This method confirms previous results from pattern-based methods, and identify new patterns, giving fresh look on the difference between specialised and lay discourse.</abstract>
			<keywords>paraphrase identification, lexico-syntactic pattern discovery, discourse type, medical domain, monolingual comparable corpora</keywords>
		</article>
		<article id="taln-2011-court-032" session="Boosters">
			<auteurs>
				<auteur>
					<prenom>Alexis</prenom>
					<nom>Kauffmann</nom>
					<email>alexis.kauffmann@unige.ch</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LATL, Université de Genève, 2, Rue de Candolle, 1211 Genève, Suisse</affiliation>
			</affiliations>
			<titre>Prise en compte de la sous-catégorisation verbale dans un lexique bilingue anglais-japonais</titre>
			<type>court</type>
			<pages></pages>
			<resume>Dans cet article, nous présentons une méthode de détection des correspondances bilingues de sous-catégorisation verbale à partir de données lexicales monolingues. Nous évoquons également la structure de ces lexiques et leur utilisation en traduction automatique (TA) à base linguistique anglais-japonais. Les lexiques sont utilisés par un programme de TA fonctionnant selon une architecture classique dite "à transfert", et leur structure permet une classification précise des sous-catégorisations verbales. Nos travaux ont permis une amélioration des données de sous-catégorisation des lexiques pour les verbes japonais et leurs équivalents anglais, en utilisant des données linguistiques compilées à partir d'un corpus de textes extrait du web. De plus, le fonctionnement du programme de TA a pu ^etre amélioré en utilisant ces données.</resume>
			<mots_cles>bases de données lexicales, sous-catégorisation verbale, traduction automatique à base linguistique, japonais</mots_cles>
			<title></title>
			<abstract>In this paper, we present a method for the detection of bilingual correspondences of verb subcategorization from monolingual lexical data. We also mention the structure of the lexicons and examples making use of such data in linguistics-based English-Japanese machine translation (MT). The lexicons are used by a MT system with a classical transfer-based architecture, and their structure allow an accurate classification of verb subcategorization. Our work has improved the lexical data about subcategorization of Japanese verbs and their English equivalents, using linguistic data compiled from a corpus of web extracted texts. Furthermore, the MT system could also be improved by the use of this data.</abstract>
			<keywords>lexical databases, verb subcategorisation, linguistics-based machine translation, Japanese</keywords>
		</article>
		<article id="taln-2011-court-033" session="Boosters">
			<auteurs>
				<auteur>
					<prenom>Yayoi</prenom>
					<nom>Nakamura-Delloye</nom>
					<email>yayoi@yayoi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">ALPAGE, INRIA-Rocquencourt, Domaine de Voluceau Rocquencourt B.P.105 78153 Le Chesnay</affiliation>
			</affiliations>
			<titre>Extraction non-supervisée de relations basée sur la dualité de la représentation</titre>
			<type>court</type>
			<pages></pages>
			<resume>Nous proposons dans cet article une méthode non-supervisée d’extraction des relations entre entités nommées. La méthode proposée se caractérise par l’utilisation de résultats d’analyses syntaxiques, notamment les chemins syntaxiques reliant deux entités nommées dans des arbres de dépendance. Nous avons également exploité la dualité de la représentation des relations sémantiques et le résultat de notre expérience comparative a montré que cette approche améliorait les rappels.</resume>
			<mots_cles>Extraction des connaissances, relations entre entités nommées, dualité relationnelle</mots_cles>
			<title></title>
			<abstract>We propose in this paper an unsupervised method for relation and pattern extraction. The proposed method is characterized by using parsed corpora, especially by leveraging syntactic paths that connect two named entities in dependency trees. We also use the dual representation of semantic relations and the result of our comparative experiment showed that this approach improves recall.</abstract>
			<keywords>Knowledge extraction, named entity relationships, relational duality</keywords>
		</article>
		<article id="taln-2011-court-034" session="Boosters">
			<auteurs>
				<auteur>
					<prenom>Corinna</prenom>
					<nom>Anderson</nom>
					<email>andersoc@loria.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Christophe</prenom>
					<nom>Cerisara</nom>
					<email>cerisara@loria.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Claire</prenom>
					<nom>Gardent</nom>
					<email>gardent@loria.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LORIA-CNRS UMR 7503, Campus Scientifique, Vandoeuvre-les-Nancy</affiliation>
			</affiliations>
			<titre>Vers la détection des dislocations à gauche dans les transcriptions automatiques du Français parlé</titre>
			<type>court</type>
			<pages></pages>
			<resume>Ce travail prend place dans le cadre plus général du développement d’une plate-forme d’analyse syntaxique du français parlé. Nous décrivons la conception d’un modèle automatique pour résoudre le lien anaphorique présent dans les dislocations à gauche dans un corpus de français parlé radiophonique. La détection de ces structures devrait permettre à terme d’améliorer notre analyseur syntaxique en enrichissant les informations prises en compte dans nos modèles automatiques. La résolution du lien anaphorique est réalisée en deux étapes : un premier niveau à base de règles filtre les configurations candidates, et un second niveau s’appuie sur un modèle appris selon le critère du maximum d’entropie. Une évaluation expérimentale réalisée par validation croisée sur un corpus annoté manuellement donne une F-mesure de l’ordre de 40%.</resume>
			<mots_cles>Détection des dislocations à gauche, Maximum Entropy, français parlé</mots_cles>
			<title>Towards automatic recognition of left dislocation in transcriptions of Spoken French</title>
			<abstract>Left dislocations are an important distinguishing feature of spoken French. In this paper, we present a hybrid approach for detecting the coreferential link that holds between left-dislocated elements and the coreferential pronoun occurring further on in the sentence. The approach combines a symbolic graph rewrite step with a maximum entropy classifier and achieves around 40% F-score. We conjecture that developing such approaches could contribute to the general anaphora resolution task and help improve parsers trained on corpora enriched with left dislocation anaphoric links.</abstract>
			<keywords>Left dislocation detection, Maximum Entropy, spoken French</keywords>
		</article>
		<article id="taln-2011-court-035" session="Boosters">
			<auteurs>
				<auteur>
					<prenom>Nabil</prenom>
					<nom>Hathout</nom>
					<email>nabil.hathout@univ-tlse2.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Fiammetta</prenom>
					<nom>Namer</nom>
					<email>fiammetta.namer@univ-nancy2.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">UMR CLLE-ERSS, Toulouse</affiliation>
				<affiliation affiliationId="2">UMR ATILF-Université Nancy2, Nancy</affiliation>
			</affiliations>
			<titre>Règles et paradigmes en morphologie informatique lexématique</titre>
			<type>court</type>
			<pages></pages>
			<resume>Les familles de mots produites par deux analyseurs morphologiques, DériF (basé sur des règles) et Morphonette (basé sur l'analogie), appliqués à un même corpus lexical, sont comparées. Cette comparaison conduit à l'examen de trois sous-ensembles : 
			- un sous-ensemble commun aux deux systèmes dont la taille montre que, malgré leurs différences, les approches expérimentées par chaque système sont valides et décrivent en partie la même réalité morphologique.
			- un sous-ensemble propre à DériF et un autre à Morphonette. Ces ensembles (a) nous renseignent sur les caractéristiques propres à chaque système, et notamment sur ce que l'autre ne peut pas produire, (b) ils mettent en évidence les erreurs d’un système, en ce qu’elles n’apparaissent pas dans l’autre, (c) ils font apparaître certaines limites de la description, notamment celles qui sont liées aux objets et aux notions théoriques comme les familles morphologiques, les bases, l'existence de RCL « transversales » entre les lexèmes qui n'ont pas de relation d'ascendance ou de descendance.</resume>
			<mots_cles>morphologie constructionnelle, analyse automatique, règles, analogie, familles morphologiques, comparaison, synergie</mots_cles>
			<title></title>
			<abstract>The word families produced by two morphological analyzers of French, DériF (rule-based) and Morphonette (analogybased), applied on the same lexical corpus have been compared. The comparison led us to examine three classes of relations:
			- one subset of relations that are shared by both systems. It shows that, despite their differences, the approaches implemented in these systems are valid and describe, to some extent, one and the same morphological reality.
			- one subset of relations specific to DériF and another one to Morphonette. These sets (a) give us informations on the characteristics proper to each system, and especially on what the other system is unable to produce; (b) they highlight the errors of one system, in so that they are absent from the results of the other; (c) they reveal some of the limits of the description, especially the ones related to theoretical objects and concepts such as morphological family, base or the existence of transverse LCR (lexeme construction rules) between lexemes that are not ascendant nor descendant of each other.</abstract>
			<keywords>Word formation, automatic analysis, rules, analogy, morphological families, comparison, synergy</keywords>
		</article>
		<article id="taln-2011-court-036" session="Boosters">
			<auteurs>
				<auteur>
					<prenom>Andrea</prenom>
					<nom>Gesmundo</nom>
					<email>andrea.gesmundo@unige.ch</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université de Genève, route de Drize 7, 1227 Genève</affiliation>
			</affiliations>
			<titre></titre>
			<type>court</type>
			<pages></pages>
			<resume>Dans cet article nous présentons une série d’adaptations de l’algorithme du "cadre d’apprenstissage guidé" pour résoudre différentes tâches d’étiquetage. La spécificité du système proposé réside dans sa capacité à apprendre l’ordre de l’inférence avec les paramètres du classifieur local au lieu de la forcer dans un ordre pré-défini (de gauche à droite). L’algorithme d’entraînement est basé sur l’algorithme du "perceptron". Nous appliquons le système à différents types de tâches d’étiquetage pour atteindre des résultats au niveau de l’état de l’art en un court temps d’exécution.</resume>
			<mots_cles>Bidirectionnel, Classification de Séquence, Apprentissage Guidé</mots_cles>
			<title>Bidirectional Sequence Classification for Tagging Tasks with Guided Learning</title>
			<abstract>In this paper we present a series of adaptations of the Guided Learning framework to solve different tagging tasks. The specificity of the proposed system lies in its ability to learn the order of inference together with the parameters of the local classifier instead of forcing it into a pre-defined order (left-to-right). The training algorithm is based on the Perceptron Algorithm. We apply the system to different kinds of tagging tasks reaching state of the art results with short execution time.</abstract>
			<keywords>Bidirectional, Sequence Classification, Guided Learning</keywords>
		</article>
		<article id="taln-2011-court-037" session="Boosters">
			<auteurs>
				<auteur>
					<prenom>Dominique</prenom>
					<nom>Legallois</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Peggy</prenom>
					<nom>Cellier</nom>
					<email></email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Thierry</prenom>
					<nom>Charnois</nom>
					<email></email>
					<affiliationId>3</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">CRISCO Université de Caen Basse-Normandie, Campus 1, 14000 Caen</affiliation>
				<affiliation affiliationId="2">IRISA-INSA de Rennes, Campus Beaulieu 35042 Rennes cedex</affiliation>
				<affiliation affiliationId="3">GREYC Université de Caen Basse-Normandie, Campus 2, 14000 Caen</affiliation>
			</affiliations>
			<titre>Calcul de réseaux phrastiques pour l’analyse et la navigation textuelle</titre>
			<type>court</type>
			<pages></pages>
			<resume>Le travail présente une méthode de navigation dans les textes, fondée sur la répétition lexicale. La méthode choisie est celle développée par le linguiste Hoey. Son application manuelle à des textes de grandeur conséquente est problématique. Nous proposons dans cet article un processus automatique qui permet d’analyser selon cette méthode des textes de grande taille ; des expériences ont été menées appliquant le processus à différents types de textes (narratif, expositif) et montrant l’intérêt de l’approche.</resume>
			<mots_cles>Réseau phrastique, Appariement de phrases, Analyse textuelle, Navigation textuelle</mots_cles>
			<title></title>
			<abstract>In this paper, we present an automatic process based on lexical repetition introduced by Hoey. The application of that kind of approaches on large texts is difficult to do by hand. In the paper, we propose an automatic process to treat large texts. We have conducted some experiments on different kinds of texts (narrative, expositive) to show the benefits of the approach.</abstract>
			<keywords>Sentence network, Bonds between sentences, Textual analysis, Textual navigation</keywords>
		</article>
		<article id="taln-2011-court-038" session="Boosters">
			<auteurs>
				<auteur>
					<prenom>Achille</prenom>
					<nom>Falaise</nom>
					<email>achille.falaise@imag.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Agnès</prenom>
					<nom>Tutin</nom>
					<email>agnes.tutin@u-grenoble3.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Olivier</prenom>
					<nom>Kraif</nom>
					<email>olivier.kraif@u-grenoble3.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">GETALP-LIG</affiliation>
				<affiliation affiliationId="2">LIDILEM</affiliation>
			</affiliations>
			<titre>Exploitation d'un corpus arboré pour non spécialistes par des requêtes guidées et des requêtes sémantiques</titre>
			<type>court</type>
			<pages></pages>
			<resume>L'exploitation de corpus analysés syntaxiquement (ou corpus arborés) pour le public non spécialiste n'est pas un problème trivial. Si la communauté du TAL souhaite mettre à la disposition des chercheurs non-informaticiens des corpus comportant des annotations linguistiques complexes, elle doit impérativement développer des interfaces simples à manipuler mais permettant des recherches fines. Dans cette communication, nous présentons les modes de recherche « grand public » développé(e)s dans le cadre du projet Scientext, qui met à disposition un corpus d'écrits scientifiques interrogeable par partie textuelle, par partie du discours et par fonction syntaxique. Les modes simples sont décrits : un mode libre et guidé, où l'utilisateur sélectionne lui-même les éléments de la requête, et un mode sémantique, qui comporte des grammaires locales préétablies à l'aide des fonctions syntaxiques.</resume>
			<mots_cles>environnement d'étude de corpus, corpus étiquetés et arborés, création de grammaires assistée, visualisation d'information linguistique</mots_cles>
			<title></title>
			<abstract>The exploitation of syntactically analysed corpora (or treebanks) by non-specialist is not a trivial problem. If the NLP community wants to make publicly available corpora with complex annotations, it is imperative to develop simple interfaces able to handle advanced queries. In this paper, we present queries methods for the general public developed during the Scientext project, which provides a searchable corpus of scientific texts searchable from textual part, part of speech and syntactic relation. The simple query modes are described: a guided query mode, where the user easily selects the elements of the query, and a semantic mode which includes local pre-established grammars using syntactic functions.</abstract>
			<keywords>corpus study environment, treebanks, assisted grammars creation, visualization of linguistic information</keywords>
		</article>
		<article id="taln-2011-court-039" session="Boosters">
			<auteurs>
				<auteur>
					<prenom>Mohammad</prenom>
					<nom>Daoud</nom>
					<email>Mohammad.Daoud@imag.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Christian</prenom>
					<nom>Boitet</nom>
					<email>Christian.Boitet@imag.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Laboratoire LIG — Université Joseph Fourier — 385, rue de la Bibliothèque, 38041 Grenoble, France</affiliation>
			</affiliations>
			<titre>Communautés Internet comme sources de préterminologie</titre>
			<type>court</type>
			<pages></pages>
			<resume>Cet article décrit deux expériences sur la construction de ressources terminologiques multilingues (preterminologies) préliminaires, mais grandes, grâce à des communautés Internet, et s'appuie sur ces expériences pour cibler des données terminologiques plus raffinées venant de communautés Internet et d'applications Web 2.0. La première expérience est une passerelle de contribution pour le site Web de la Route de la Soie numérique (DSR). Les visiteurs contribuent en effet à un référentiel lexical multilingue dédié, pendant qu'ils visitent et lisent les livres archivés, parce qu'ils sont intéressés par le domaine et ont tendance à être polygottes. Nous avons recueilli 1400 contributions lexicales en 4 mois. La seconde expérience est basée sur le JeuxDeMots arabe, où les joueurs en ligne contribuent à un réseau lexical arabe. L'expérience a entraîné une croissance régulière du nombre de joueurs et de contributions, ces dernières contenant des termes absents et des mots de dialectes oraux.</resume>
			<mots_cles>terminologie, préterminologie, approches collaboratives, réseaux lexicaux, DSR, jeux sérieux</mots_cles>
			<title></title>
			<abstract>This paper describes two experiments on building preliminary but large multilingual terminological resources (preterminologies) through Internet communities, and draws on these experiments to target more refined terminological data from Internet communities and Web 2.0 applications. The first experiment is a contribution gateway for the Digital Silk Road (DSR) website. Visitors indeed contribute to a dedicated multilingual lexical repository while they visit and read the archived books, because they are interested in the domain and tend to be multilingual. We collected 1400 lexical contributions in 4 months. The second experiment is based on the Arabic JeuxDeMots, where online players contribute to an Arabic lexical network. The experiment resulted in a steady growth of number of players and contributions, the latter containing absent terms and spoken dialectic words.</abstract>
			<keywords>terminology, preterminology, collaborative approaches, lexical networks, DSR, serious games</keywords>
		</article>
		<article id="taln-2011-court-040" session="Boosters">
			<auteurs>
				<auteur>
					<prenom>Wigdan</prenom>
					<nom>Mekki</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Julien</prenom>
					<nom>Gosme</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Fathi</prenom>
					<nom>Debili</nom>
					<email></email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Yves</prenom>
					<nom>Lepage</nom>
					<email></email>
					<affiliationId>3</affiliationId>
				</auteur>
				<auteur>
					<prenom>Nadine</prenom>
					<nom>Lucas</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">GREYC, UMR 6072, CNRS, Université de Caen Basse-Normandie, Caen, France</affiliation>
				<affiliation affiliationId="2">LLACAN, UMR 8135, CNRS, Villejuif, France</affiliation>
				<affiliation affiliationId="3">IPS, Université Waseda, Japon</affiliation>
			</affiliations>
			<titre>Évaluation de G-LexAr pour la traduction automatique statistique</titre>
			<type>court</type>
			<pages></pages>
			<resume>G-LexAr est un analyseur morphologique de l’arabe qui a récemment reçu des améliorations substantielles. Cet article propose une évaluation de cet analyseur en tant qu’outil de pré-traitement pour la traduction automatique statistique, ce dont il n’a encore jamais fait l’objet. Nous étudions l’impact des différentes formes proposées par son analyse (voyellation, lemmatisation et segmentation) sur un système de traduction arabe-anglais, ainsi que l’impact de la combinaison de ces formes. Nos expériences montrent que l’utilisation séparée de chacune de ces formes n’a que peu d’influence sur la qualité des traductions obtenues, tandis que leur combinaison y contribue de façon très bénéfique.</resume>
			<mots_cles>traduction automatique statistique, analyse morphologique, pré-traitement de l’arabe</mots_cles>
			<title></title>
			<abstract>G-LexAr is an Arabic morphological analyzer that has recently been improved for speed. This paper gives an assessment of this analyzer as a preprocessing tool for statistical machine translation. We study the impact of the use of its possible outputs (vocalized, lemmatized and segmented) through an Arabic-English machine translation system, as well as the impact of the combination of these outputs. Our experiments show that using these outputs separately does not influence much translation quality. However, their combination leads to major improvements.</abstract>
			<keywords>statistical machine translation, morphological analysis, arabic preprocessing</keywords>
		</article>
		<article id="taln-2011-court-041" session="Boosters">
			<auteurs>
				<auteur>
					<prenom>Marion</prenom>
					<nom>Laignelet</nom>
					<email>marion.laignelet@univ-tlse2.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Mouna</prenom>
					<nom>Kamel</nom>
					<email>kamel@irit.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Nathalie</prenom>
					<nom>Aussenac-Gilles</nom>
					<email>aussenac@irit.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">CLLE-ERSS, Université de Toulouse 2, 5 allée A. Machado, 31058 Toulouse Cedex 9</affiliation>
				<affiliation affiliationId="2">IRIT, Université Paul Sabatier, 118 Route de Narbonne, 31062 Toulouse Cedex 9</affiliation>
			</affiliations>
			<titre>Enrichir la notion de patron par la prise en compte de la structure textuelle - Application à la construction d’ontologie</titre>
			<type>court</type>
			<pages></pages>
			<resume>La projection de patrons lexico-syntaxiques sur corpus est une des manières privilégiées pour identifier des relations sémantiques précises entre éléments lexicaux. Dans cet article, nous proposons d’étendre la notion de patron en prenant en compte la sémantique que véhiculent les éléments de structure d’un document (définitions, titres, énumérations) dans l’identification de relations. Nous avons testé cette hypothèse dans le cadre de la construction d’ontologies à partir de textes fortement structurés du domaine de la cartographie.</resume>
			<mots_cles>Construction d’ontologie, patron lexico-syntaxique, structure textuelle</mots_cles>
			<title></title>
			<abstract>Matching lexico-syntactic patterns on text corpora is one of the favorite ways to identify precise semantic relations between lexical items. In this paper, we propose to rely on text structure to extend the notion of pattern and to take into account the semantics that the structure (definitions, titles, item lists) may bear when identifying semantic relations between concepts. We have checked this hypothesis by building an ontology via highly structured texts describing spatial, i.e. geographical information.</abstract>
			<keywords>Ontology engineering, lexico-syntactic patterns, textual structure</keywords>
		</article>
		<article id="taln-2011-court-042" session="Boosters">
			<auteurs>
				<auteur>
					<prenom>Lorenza</prenom>
					<nom>Russo</nom>
					<email>Lorenza.Russo@unige.ch</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Éric</prenom>
					<nom>Wehrli</nom>
					<email>Eric.Wehrli@unige.ch</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Laboratoire d’Analyse et de Technologie du Langage (LATL), Département de linguistique – Université de Genève, 2, rue de Candolle – CH-1211 Genève 4</affiliation>
			</affiliations>
			<titre>La traduction automatique des séquences clitiques dans un traducteur à base de règles</titre>
			<type>court</type>
			<pages></pages>
			<resume>Dans cet article, nous discutons la méthodologie utilisée par Its-2, un système de traduction à base de règles, pour la traduction des pronoms clitiques. En particulier, nous nous focalisons sur les séquences clitiques, pour la traduction automatique entre le français et l’anglais. Une évaluation basée sur un corpus de phrases construites montre le potentiel de notre approche pour des traductions de bonne qualité.</resume>
			<mots_cles>Analyseur syntaxique, traduction automatique, pronom clitique, séquences clitiques</mots_cles>
			<title></title>
			<abstract>In this paper we discuss the methodology applied by Its-2, a rule-based MT system, in order to translate clitic pronouns. In particular, we focus on French clitic clusters, for automatic translation between French and English. An evaluation based on a corpus of constructed sentences shows the potential of this approach for high-quality translation.</abstract>
			<keywords>Syntactic parser, automatic translation, clitic pronoun, clitic clusters</keywords>
		</article>
		<article id="taln-2011-court-043" session="Boosters">
			<auteurs>
				<auteur>
					<prenom>Lorenza</prenom>
					<nom>Russo</nom>
					<email>lorenza.russo@unige.ch</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Yves</prenom>
					<nom>Scherrer</nom>
					<email>yves.scherrer@unige.ch</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Jean-Philippe</prenom>
					<nom>Goldman</nom>
					<email>jean-philippe.goldman@unige.ch</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Sharid</prenom>
					<nom>Loáiciga</nom>
					<email>sharid.loaiciga@unige.ch</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Luka</prenom>
					<nom>Nerima</nom>
					<email>luka.nerima@unige.ch</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Éric</prenom>
					<nom>Wehrli</nom>
					<email>eric.wehrli@unige.ch</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Laboratoire d’Analyse et de Technologie du Langage, Département de Linguistique – Université de Genève, 2, rue de Candolle – CH-1211 Genève 4</affiliation>
			</affiliations>
			<titre>Étude inter-langues de la distribution et des ambiguïtés syntaxiques des pronoms</titre>
			<type>court</type>
			<pages></pages>
			<resume>Ce travail décrit la distribution des pronoms selon le style de texte (littéraire ou journalistique) et selon la langue (français, anglais, allemand et italien). Sur la base d’un étiquetage morpho-syntaxique effectué automatiquement puis vérifié manuellement, nous pouvons constater que la proportion des différents types de pronoms varie selon le type de texte et selon la langue. Nous discutons les catégories les plus ambiguës de manière détaillée. Comme nous avons utilisé l’analyseur syntaxique Fips pour l’étiquetage des pronoms, nous l’avons également évalué et obtenu une précision moyenne de plus de 95%.</resume>
			<mots_cles>Pronoms, ambiguïté pronominale, étiquetage morpho-syntaxique</mots_cles>
			<title></title>
			<abstract>This paper compares the distribution of pronouns according to the text genre (literary or news) and to the language (French, English, German and Italian). On the basis of manually verified part-of-speech tags, we find that the proportion of different pronoun types depends on the text and on the language. We discuss the most ambiguous cases in detail. As we used the Fips parser for the tagging of pronouns, we have evaluated it and obtained an overall precision of over 95%.</abstract>
			<keywords>Pronouns, pronominal ambiguity, part-of-speech tagging</keywords>
		</article>
		<article id="taln-2011-court-044" session="Boosters">
			<auteurs>
				<auteur>
					<prenom>Yves</prenom>
					<nom>Scherrer</nom>
					<email>yves.scherrer@unige.ch</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Lorenza</prenom>
					<nom>Russo</nom>
					<email>lorenza.russo@unige.ch</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Jean-Philippe</prenom>
					<nom>Goldman</nom>
					<email>jean-philippe.goldman@unige.ch</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Sharid</prenom>
					<nom>Loáiciga</nom>
					<email>sharid.loaiciga@unige.ch</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Luka</prenom>
					<nom>Nerima</nom>
					<email>luka.nerima@unige.ch</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Éric</prenom>
					<nom>Wehrli</nom>
					<email>eric.wehrli@unige.ch</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Laboratoire d’Analyse et de Technologie du Langage, Département de Linguistique – Université de Genève, 2, rue de Candolle – CH-1211 Genève 4</affiliation>
			</affiliations>
			<titre>La traduction automatique des pronoms. Problèmes et perspectives</titre>
			<type>court</type>
			<pages></pages>
			<resume>Dans cette étude, notre système de traduction automatique, Its-2, a fait l’objet d’une évaluation manuelle de la traduction des pronoms pour cinq paires de langues et sur deux corpus : un corpus littéraire et un corpus de communiqués de presse. Les résultats montrent que les pourcentages d’erreurs peuvent atteindre 60% selon la paire de langues et le corpus. Nous discutons ainsi deux pistes de recherche pour l’amélioration des performances de Its-2 : la résolution des ambiguïtés d’analyse et la résolution des anaphores pronominales.</resume>
			<mots_cles>Pronoms, traduction automatique, analyse syntaxique, anaphores pronominales</mots_cles>
			<title></title>
			<abstract>In this work, we present the results of a manual evaluation of our machine translation system, Its-2, on the task of pronoun translation for five language pairs and in two corpora : a litterary corpus and a corpus of press releases. The results show that the error rates reach 60% depending on the language pair and the corpus. Then we discuss two proposals for improving the performances of Its-2 : resolution of source language ambiguities and resolution of pronominal anaphora.</abstract>
			<keywords>Pronouns, machine translation, parsing, pronominal anaphora</keywords>
		</article>
		<article id="taln-2011-court-045" session="Boosters">
			<auteurs>
				<auteur>
					<prenom>Daniel</prenom>
					<nom>Kayser</nom>
					<email>Daniel.Kayser@lipn.univ-paris13.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIPN – UMR 7030 du CNRS, Institut Galilée - Université Paris-Nord, 93430 Villetaneuse</affiliation>
			</affiliations>
			<titre>Ressources lexicales pour une sémantique inférentielle : un exemple, le mot « quitter »</titre>
			<type>court</type>
			<pages></pages>
			<resume>On étudie environ 500 occurrences du verbe « quitter » en les classant selon les inférences qu’elles suggèrent au lecteur. On obtient ainsi 43 « schémas inférentiels ». Ils ne s’excluent pas l’un l’autre : si plusieurs d’entre eux s’appliquent, les inférences produites se cumulent ; cependant, comme l’auteur sait que le lecteur dispose de tels schémas, s’il veut l’orienter vers une seule interprétation, il fournit des indices permettant d’éliminer les autres. On conjecture que ces schémas présentent des régularités observables sur des familles de mots, que ces régularités proviennent du fonctionnement d’opérations génériques, et qu’il est donc sans gravité de ne pas être exhaustif, dans la mesure où ces opérations permettent d’engendrer les schémas manquants en cas de besoin.</resume>
			<mots_cles>Sémantique lexicale, Inférence, Glissements de sens</mots_cles>
			<title></title>
			<abstract>Around 500 occurrences of the French verb “quitter” are scrutinized and sorted according to the inferences they trigger in the reader’s mind. This yields 43 so-called inferential schemata. They are not exclusive from one another: when several of them are applicable, their conclusions add together; however, as the author knows that the reader possesses this kind of schema, if s/he wants to direct the reader towards a given interpretation, s/he provides some clues to block the other ones. The schemata reveal regularities across families of similar words, and these regularities are conjectured to be due to the operation of generic procedures: omitting some schemata is thus harmless, insofar as these procedures have the ability to generate the missing ones in case of need.</abstract>
			<keywords>Lexical Semantics, Inference, Shifts in Meaning</keywords>
		</article>
		<article id="taln-2011-court-046" session="Boosters">
			<auteurs>
				<auteur>
					<prenom>Caroline</prenom>
					<nom>Brun</nom>
					<email>Caroline.Brun@xrce.xerox.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Xerox Research Centre Europe, 6 chemin de Maupertuis, 38240 Meylan, France</affiliation>
			</affiliations>
			<titre>Un système de détection d’opinions fondé sur l’analyse syntaxique profonde</titre>
			<type>court</type>
			<pages></pages>
			<resume>Dans cet article, nous présentons un système de détection d’opinions construit à partir des sorties d’un analyseur syntaxique robuste produisant des analyses profondes. L’objectif de ce système est l’extraction d’opinions associées à des produits (les concepts principaux) ainsi qu’aux concepts qui leurs sont associés (en anglais «features-based opinion extraction»). Suite à une étude d’un corpus cible, notre analyseur syntaxique est enrichi par l’ajout de polarité aux éléments pertinents du lexique et par le développement de règles génériques et spécialisées permettant l’extraction de relations sémantiques d’opinions, qui visent à alimenter un modèle de représentation des opinions. Une première évaluation montre des résultats très encourageants, mais de nombreuses perspectives restent à explorer.</resume>
			<mots_cles>détection d’opinions, analyse de sentiments, analyse syntaxique robuste, extraction d’information</mots_cles>
			<title></title>
			<abstract>In this paper, we present an opinion detection system built on top of a deep robust syntactic parser. The goal of this system is to extract opinions associated to products but also to characteristics of these products, i.e. to perform feature-based opinion extraction. To carry out this task, and following the results of a target corpus study, the robust syntactic analyzer is enriched by the association of polarity to pertinent lexical elements and by the development of generic rules extracting semantic relations of opinions, in order to feed an opinion representation model. A first evaluation gave very encouraging results, but many perspectives remain to be explored.</abstract>
			<keywords>opinion detection, sentiment analysis, robust parsing, information extraction</keywords>
		</article>
		<article id="taln-2011-court-047" session="Boosters">
			<auteurs>
				<auteur>
					<prenom>Caroline</prenom>
					<nom>Hagège</nom>
					<email>Caroline.Hagege@xrce.xerox.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Denys</prenom>
					<nom>Proux</nom>
					<email>Denys.Proux@xrce.xerox.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Quentin</prenom>
					<nom>Gicquel</nom>
					<email>Quentin.Gicquel@chu-lyon.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Stéfan</prenom>
					<nom>Darmoni</nom>
					<email>Stefan.Darmoni@cismef.fr</email>
					<affiliationId>3</affiliationId>
				</auteur>
				<auteur>
					<prenom>Suzanne</prenom>
					<nom>Pereira</nom>
					<email>Suzanne.Pereira@vidal.fr</email>
					<affiliationId>4</affiliationId>
				</auteur>
				<auteur>
					<prenom>Frédérique</prenom>
					<nom>Segond</nom>
					<email>Frederique.Segond@xrce.xerox.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Marie-Helène</prenom>
					<nom>Metzger</nom>
					<email>Marie-Helene.Metzger@chu-lyon.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">XRCE, 6 Chemin de Maupertuis, 38240 Meylan, France</affiliation>
				<affiliation affiliationId="2">UCBL-CNRS, UMR 5558 Lyon, France</affiliation>
				<affiliation affiliationId="3">CISMEF, Rouen, France</affiliation>
				<affiliation affiliationId="4">VIDAL, Issy les Moulineaux, France</affiliation>
			</affiliations>
			<titre>Développement d’un système de détection des infections associées aux soins à partir de l’analyse de comptes-rendus d’hospitalisation</titre>
			<type>court</type>
			<pages></pages>
			<resume>Cet article décrit la première version et les résultats de l’évaluation d’un système de détection des épisodes d’infections associées aux soins. Cette détection est basée sur l’analyse automatique de comptes-rendus d’hospitalisation provenant de différents hôpitaux et différents services. Ces comptes-rendus sont sous forme de texte libre. Le système de détection a été développé à partir d’un analyseur linguistique que nous avons adapté au domaine médical et extrait à partir des documents des indices pouvant conduire à une suspicion d’infection. Un traitement de la négation et un traitement temporel des textes sont effectués permettant de restreindre et de raffiner l’extraction d’indices. Nous décrivons dans cet article le système que nous avons développé et donnons les résultats d’une évaluation préliminaire.</resume>
			<mots_cles>Extraction d’information médicale, compte-rendus d’hospitalisation, infection nosocomiale, analyse syntaxique</mots_cles>
			<title></title>
			<abstract>This paper describes the first version and the results obtained by a system which detects occurrences of healthcare-associated infections. The system automatically analyzes hospital discharge summaries coming from different hospitals and from different care units. The output of the system consists in stating for each document, if there is a case of healthcare-associated infection. The linguistic processor which analyzes hospital discharge summaries is a general purpose tool which has been adapted for the medical domain. It extracts textual elements that may lead to an infection suspicion. Jointly with the extraction of suspicious terms, the system performs a negation and temporal processing of texts in order to refine the extraction. We first describe the system that has been developed and give then the results of a preliminary evaluation.</abstract>
			<keywords>Information extraction in medical domain, hospital discharge summaries, hospital acquired infections, parsing</keywords>
		</article>
		<article id="taln-2011-demo-001" session="Démonstrations">
			<auteurs>
				<auteur>
					<prenom>Richard</prenom>
					<nom>Beaufort</nom>
					<email>richard.beaufort@uclouvain.be</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Sophie</prenom>
					<nom>Roekhaut</nom>
					<email>sophie.roekhaut@uclouvain.be</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">CENTAL, UCLouvain, Place Blaise Pascal 1, B-1348 Louvain-la-Neuve</affiliation>
			</affiliations>
			<titre>PLATON, Plateforme d’apprentissage et d’enseignement de l’orthographe sur le Net</titre>
			<type>démonstration</type>
			<pages></pages>
			<resume></resume>
			<mots_cles></mots_cles>
			<title></title>
			<abstract></abstract>
			<keywords></keywords>
		</article>
		<article id="taln-2011-demo-002" session="Démonstrations">
			<auteurs>
				<auteur>
					<prenom>Annelies</prenom>
					<nom>Braffort</nom>
					<email>annelies.braffort@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Laurence</prenom>
					<nom>Bolot</nom>
					<email>laurence.bolot@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIMSI-CNRS, Campus d'Orsay Bt. 508, BP 133, F-91403 Orsay cx, France</affiliation>
			</affiliations>
			<titre>SpatiAnn, un outil pour annoter l’utilisation de l’espace dans les corpus vidéo</titre>
			<type>démonstration</type>
			<pages></pages>
			<resume></resume>
			<mots_cles></mots_cles>
			<title></title>
			<abstract></abstract>
			<keywords></keywords>
		</article>
		<article id="taln-2011-demo-003" session="Démonstrations">
			<auteurs>
				<auteur>
					<prenom>François</prenom>
					<nom>Brown de Colstoun</nom>
					<email>fbc@lingua-et-machina.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Estelle</prenom>
					<nom>Delpech</nom>
					<email>ed@lingua-et-machina.com</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Etienne</prenom>
					<nom>Monneret</nom>
					<email>em@lingua-et-machina.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LINGUA ET MACHINA, Laval Technopole, 6 rue Léonard de Vinci, 53001 Laval Cedex et c/o Inria, Rocquencourt BP 105, 78 153 Le Chesnay Cedex</affiliation>
				<affiliation affiliationId="2">LINA FRE CNRS 2729, 2 rue de la Houssinière BP 92208, 44322 Nantes Cedex 3</affiliation>
			</affiliations>
			<titre>Libellex : une plateforme multiservices pour la gestion des contenus multilingues</titre>
			<type>démonstration</type>
			<pages></pages>
			<resume></resume>
			<mots_cles></mots_cles>
			<title></title>
			<abstract></abstract>
			<keywords></keywords>
		</article>
		<article id="taln-2011-demo-004" session="Démonstrations">
			<auteurs>
				<auteur>
					<prenom>Jacques</prenom>
					<nom>Chauché</nom>
					<email>jacques.chauche@lirmm.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIRMM, 161, rue ADA 34392 Montpellier Cedex 5</affiliation>
			</affiliations>
			<titre>Une application de la grammaire structurelle: L’analyseur syntaxique du français SYGFRAN</titre>
			<type>démonstration</type>
			<pages></pages>
			<resume>La démonstration présentée produit une analyse syntaxique du français. Elle est écrite en SYGMART, fournie avec les actes, exécutable à l’adresse : http ://www.lirmm.fr/ chauche/ExempleAnl.html et téléchargeable à l’adresse : http ://www.sygtext.fr.</resume>
			<mots_cles>Analyse syntaxique</mots_cles>
			<title></title>
			<abstract>The software produces a syntactic analysis of french. It is written in SYGMART, including acts, runable at http ://www.lirmm.fr/ chauche/ExempleAnl.html and downlodable at : http ://www.sygtext.fr.</abstract>
			<keywords>syntactic analysis</keywords>
		</article>
		<article id="taln-2011-demo-005" session="Démonstrations">
			<auteurs>
				<auteur>
					<prenom>François-Régis</prenom>
					<nom>Chaumartin</nom>
					<email>frc@proxem.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Proxem, 19 bd de Magenta, 75010 Paris</affiliation>
			</affiliations>
			<titre>Proxem Ubiq : une solution d’e-réputation par analyse de feedbacks clients</titre>
			<type>démonstration</type>
			<pages></pages>
			<resume></resume>
			<mots_cles>e-réputation, reconnaissance d’entités nommées, classification, clustering, analyse syntaxique, apprentissage</mots_cles>
			<title></title>
			<abstract></abstract>
			<keywords></keywords>
		</article>
		<article id="taln-2011-demo-006" session="Démonstrations">
			<auteurs>
				<auteur>
					<prenom>Béatrice</prenom>
					<nom>Daille</nom>
					<email>beatrice.daille@univ-nantes.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Christine</prenom>
					<nom>Jacquin</nom>
					<email>christine.jacquin@univ-nantes.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Laura</prenom>
					<nom>Monceaux</nom>
					<email>laura.monceaux@univ-nantes.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Emmanuel</prenom>
					<nom>Morin</nom>
					<email>emmanuel.morin@univ-nantes.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Jérome</prenom>
					<nom>Rocheteau</nom>
					<email>jerome.rocheteau@univ-nantes.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université de Nantes - LINA – 2 rue de la Houssinière – BP 92208 – 44322 Nantes cedex 3, France</affiliation>
			</affiliations>
			<titre>TTC TermSuite : une chaîne de traitement pour la fouille terminologique multilingue</titre>
			<type>démonstration</type>
			<pages></pages>
			<resume></resume>
			<mots_cles></mots_cles>
			<title></title>
			<abstract></abstract>
			<keywords></keywords>
		</article>
		<article id="taln-2011-demo-007" session="Démonstrations">
			<auteurs>
				<auteur>
					<prenom>Rodolfo</prenom>
					<nom>Delmonte</nom>
					<email>delmonte@unive.it</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Vincenzo</prenom>
					<nom>Pallotta</nom>
					<email>Vincenzo.Pallotta@internalytics.ch</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Violeta</prenom>
					<nom>Seretan</nom>
					<email>violeta.seretan@gmail.com</email>
					<affiliationId>3</affiliationId>
				</auteur>
				<auteur>
					<prenom>Lammert</prenom>
					<nom>Vrieling</nom>
					<email>Lammert.Vrieling@internalytics.ch</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>David</prenom>
					<nom>Walker</nom>
					<email>David.Walker@internalytics.ch</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Interanalytics, Geneva, Switzerland</affiliation>
				<affiliation affiliationId="2">Department of Language Science, University of Venice, Italy</affiliation>
				<affiliation affiliationId="3">School of Informatics, University of Edinburgh, United Kingdom</affiliation>
			</affiliations>
			<titre></titre>
			<type>démonstration</type>
			<pages></pages>
			<resume></resume>
			<mots_cles></mots_cles>
			<title>An Interaction Mining Suite Based On Natural Language Understanding</title>
			<abstract></abstract>
			<keywords></keywords>
		</article>
		<article id="taln-2011-demo-008" session="Démonstrations">
			<auteurs>
				<auteur>
					<prenom>François-Xavier</prenom>
					<nom>Desmarais</nom>
					<email>francois-xavier.desmarais@polymtl.ca</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Éric</prenom>
					<nom>Charton</nom>
					<email>eric.charton@polymtl.ca</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">École Polytechnique de Montréal, 2900 boul. Edouard Montpetit, Montréal, Canada H3T 1J4</affiliation>
			</affiliations>
			<titre>Démonstration de l'API de NLGbAse</titre>
			<type>démonstration</type>
			<pages></pages>
			<resume></resume>
			<mots_cles></mots_cles>
			<title></title>
			<abstract></abstract>
			<keywords></keywords>
		</article>
		<article id="taln-2011-demo-009" session="Démonstrations">
			<auteurs>
				<auteur>
					<prenom>Michel</prenom>
					<nom>Généreux</nom>
					<email>genereux@clul.ul.pt</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Centro de Linguística da Universidade de Lisboa, Av. Prof. Gama Pinto, 2, 1649-003 Lisboa - Portugal</affiliation>
			</affiliations>
			<titre>Système d’analyse de la polarité de dépêches financières</titre>
			<type>démonstration</type>
			<pages></pages>
			<resume>Nous présentons un système pour la classification en continu de dépêches financières selon une polarité positive ou négative. La démonstration permettra ainsi d’observer quelles sont les dépêches les plus à même de faire varier la valeur d’actions cotées en bourse, au moment même de la démonstration. Le système traitera de dépêches écrites en anglais et en français.</resume>
			<mots_cles>Analyse de Sentiments, Linguistique de Corpus, Dépêches Financières</mots_cles>
			<title></title>
			<abstract>We present a system for classifying on-line financial news items into a positive or negative polarity. The demonstration will therefore allow us to observe which news are most likely to influence the price of shares traded at the time of the demonstration. The system will cover news items written in English and French.</abstract>
			<keywords>Sentiment Analysis, Corpus Linguistics, Financial News Items</keywords>
		</article>
		<article id="taln-2011-demo-010" session="Démonstrations">
			<auteurs>
				<auteur>
					<prenom>Laurence</prenom>
					<nom>Longo</nom>
					<email>longo@unistra.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Amalia</prenom>
					<nom>Todirascu</nom>
					<email>todiras@unistra.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université de Strasbourg, 22 avenue René Descartes, 67084 Strasbourg Cedex, France</affiliation>
			</affiliations>
			<titre>RefGen, outil d’identification automatique des chaînes de référence en français</titre>
			<type>démonstration</type>
			<pages></pages>
			<resume></resume>
			<mots_cles></mots_cles>
			<title></title>
			<abstract></abstract>
			<keywords></keywords>
		</article>
		<article id="taln-2011-demo-011" session="Démonstrations">
			<auteurs>
				<auteur>
					<prenom>Clément</prenom>
					<nom>de Groc</nom>
					<email>cdegroc@syllabs.com</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Javier</prenom>
					<nom>Couto</nom>
					<email>jcouto@syllabs.com</email>
					<affiliationId>1</affiliationId>
					<affiliationId>3</affiliationId>
				</auteur>
				<auteur>
					<prenom>Helena</prenom>
					<nom>Blancafort</nom>
					<email>blancafort@syllabs.com</email>
					<affiliationId>1</affiliationId>
					<affiliationId>4</affiliationId>
				</auteur>
				<auteur>
					<prenom>Claude</prenom>
					<nom>de Loupy</nom>
					<email>loupy@syllabs.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Syllabs, 15 rue Jean-Baptiste Berlier, 75013 Paris</affiliation>
				<affiliation affiliationId="2">Univ. Paris Sud et LIMSI-CNRS, F-91405 Orsay</affiliation>
				<affiliation affiliationId="3">MoDyCo, UMR 7114, CNRS-Université Paris Ouest Nanterre, La Défense</affiliation>
				<affiliation affiliationId="4">Universitat Pompeu Fabra Roc Boronat,138, 08018 Barcelona, Spain</affiliation>
			</affiliations>
			<titre>Babouk – exploration orientée du web pour la constitution de corpus et de terminologies</titre>
			<type>démonstration</type>
			<pages></pages>
			<resume></resume>
			<mots_cles></mots_cles>
			<title></title>
			<abstract></abstract>
			<keywords></keywords>
		</article>
		<article id="taln-2011-demo-012" session="Démonstrations">
			<auteurs>
				<auteur>
					<prenom>Cyril</prenom>
					<nom>Grouin</nom>
					<email>Cyril.Grouin@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Louise</prenom>
					<nom>Deléger</nom>
					<email>louise.deleger@cchmc.org</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Anne-Lyse</prenom>
					<nom>Minard</nom>
					<email>Anne-Lyse.Minard@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Anne-Laure</prenom>
					<nom>Ligozat</nom>
					<email>Anne-Laure.Ligozat@limsi.fr</email>
					<affiliationId>.</affiliationId>
				</auteur>
				<auteur>
					<prenom>Asma</prenom>
					<nom>Ben Abacha</nom>
					<email>Asma.BenAbacha@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Delphine</prenom>
					<nom>Bernhard</nom>
					<email>Delphine.Bernhard@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Bruno</prenom>
					<nom>Cartoni</nom>
					<email>bruno.cartoni@unige.ch</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Brigitte</prenom>
					<nom>Grau</nom>
					<email>Brigitte.Grau@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Sophie</prenom>
					<nom>Rosset</nom>
					<email>Sophie.Rosset@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Pierre</prenom>
					<nom>Zweigenbaum</nom>
					<email>Pierre.Zweigenbaum@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIMSI-CNRS, BP133, 91403 Orsay Cedex, France</affiliation>
				<affiliation affiliationId="2">Département de Linguistique, Université de Genève, Suisse</affiliation>
			</affiliations>
			<titre>Extraction d’informations médicales au LIMSI</titre>
			<type>démonstration</type>
			<pages></pages>
			<resume></resume>
			<mots_cles></mots_cles>
			<title></title>
			<abstract></abstract>
			<keywords></keywords>
		</article>
		<article id="taln-2011-demo-013" session="Démonstrations">
			<auteurs>
				<auteur>
					<prenom>Juyeon</prenom>
					<nom>Kang</nom>
					<email>kjuyeon79@yahoo.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Jean-Pierre</prenom>
					<nom>Desclés</nom>
					<email>jean-pierre.desclés@paris-sorbonne.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LaLIC, 28, Rue Serpente, 75006 Paris, France</affiliation>
			</affiliations>
			<titre>Système d’analyse catégorielle ACCG : adéquation au traitement de problèmes syntaxiques complexes</titre>
			<type>démonstration</type>
			<pages></pages>
			<resume></resume>
			<mots_cles></mots_cles>
			<title></title>
			<abstract></abstract>
			<keywords></keywords>
		</article>
		<article id="taln-2011-demo-014" session="Démonstrations">
			<auteurs>
				<auteur>
					<prenom>Jimmy</prenom>
					<nom>Ma</nom>
					<email>ma@syllabs.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Mickaël</prenom>
					<nom>Mounier</nom>
					<email>mounier@syllabs.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Helena</prenom>
					<nom>Blancafort</nom>
					<email>blancafort@syllabs.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Javier</prenom>
					<nom>Couto</nom>
					<email>couto@syllabs.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Claude</prenom>
					<nom>de Loupy</nom>
					<email>loupy@syllabs.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Syllabs, 15 rue Jean-Baptiste Berlier, 75013 Paris, France</affiliation>
			</affiliations>
			<titre>LOL : Langage objet dédié à la programmation linguistique</titre>
			<type>démonstration</type>
			<pages></pages>
			<resume></resume>
			<mots_cles></mots_cles>
			<title></title>
			<abstract></abstract>
			<keywords></keywords>
		</article>
		<article id="taln-2011-demo-015" session="Démonstrations">
			<auteurs>
				<auteur>
					<prenom>Yann</prenom>
					<nom>Mathet</nom>
					<email>Yann.Mathet@unicaen.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Antoine</prenom>
					<nom>Widlöcher</nom>
					<email>Antoine.Widlocher@unicaen.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">GREYC, UMR CNRS 6072, Université de Caen, 14032 Caen Cedex</affiliation>
			</affiliations>
			<titre>Aligner : un outil d’alignement et de mesure d’accord inter-annotateurs</titre>
			<type>démonstration</type>
			<pages></pages>
			<resume></resume>
			<mots_cles></mots_cles>
			<title></title>
			<abstract></abstract>
			<keywords></keywords>
		</article>
		<article id="taln-2011-demo-016" session="Démonstrations">
			<auteurs>
				<auteur>
					<prenom>Yann</prenom>
					<nom>Mathet</nom>
					<email>Yann.Mathet@unicaen.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Antoine</prenom>
					<nom>Widlöcher</nom>
					<email>Antoine.Widlocher@unicaen.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">GREYC, UMR CNRS 6072, Université de Caen, 14032 Caen Cedex</affiliation>
			</affiliations>
			<titre>GlozzQL : un langage de requêtes incrémental pour les textes annotés</titre>
			<type>démonstration</type>
			<pages></pages>
			<resume></resume>
			<mots_cles></mots_cles>
			<title></title>
			<abstract></abstract>
			<keywords></keywords>
		</article>
		<article id="taln-2011-demo-017" session="Démonstrations">
			<auteurs>
				<auteur>
					<prenom>Frédéric</prenom>
					<nom>Meunier</nom>
					<email>frederic.meunier@watchsystance.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Laurence</prenom>
					<nom>Danlos</nom>
					<email>laurence.danlos@linguist.jussieu.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<prenom>Vanessa</prenom>
					<nom>Combet</nom>
					<email>vanessa.combet@watchsystance.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Watch System Assistance</affiliation>
				<affiliation affiliationId="2">Université Paris Diderot, ALPAGE</affiliation>
			</affiliations>
			<titre>EASYTEXT : un système opérationnel de génération de textes</titre>
			<type>démonstration</type>
			<pages></pages>
			<resume></resume>
			<mots_cles></mots_cles>
			<title></title>
			<abstract></abstract>
			<keywords></keywords>
		</article>
		<article id="taln-2011-demo-018" session="Démonstrations">
			<auteurs>
				<auteur>
					<prenom>Yoann</prenom>
					<nom>Moreau</nom>
					<email>yoann.moreau@univ-avignon.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Eric</prenom>
					<nom>SanJuan</nom>
					<email>eric.sanjuan@univ-avignon.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<prenom>Patrice</prenom>
					<nom>Bellot</nom>
					<email>patrice.bellot@univ-avignon.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIA, 339, chemin des Meinajaries 84911 AVIGNON Cedex 9</affiliation>
			</affiliations>
			<titre>Restad : un logiciel d’indexation et de stockage relationnel de contenus XML</titre>
			<type>démonstration</type>
			<pages></pages>
			<resume></resume>
			<mots_cles></mots_cles>
			<title></title>
			<abstract></abstract>
			<keywords></keywords>
		</article>
		<article id="taln-2011-demo-019" session="Démonstrations">
			<auteurs>
				<auteur>
					<prenom>Gaëlle</prenom>
					<nom>Recourcé</nom>
					<email>recource@kwaga.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Kwaga SAS, 15 rue J-B. Berlier, 75013 Paris, France</affiliation>
			</affiliations>
			<titre>Une chaîne d’analyse des e-mails pour l’aide à la gestion de sa messagerie</titre>
			<type>démonstration</type>
			<pages></pages>
			<resume></resume>
			<mots_cles></mots_cles>
			<title></title>
			<abstract></abstract>
			<keywords></keywords>
		</article>
		<article id="taln-2011-demo-020" session="Démonstrations">
			<auteurs>
				<auteur>
					<prenom>Jean</prenom>
					<nom>Rohmer</nom>
					<email>jean.rohmer@devinci.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Ecole Supérieure d'Ingénieurs Léonard de Vinci 92916 Paris La Défense Cedex</affiliation>
			</affiliations>
			<titre>Démonstration d’un outil de « Calcul Littéraire »</titre>
			<type>démonstration</type>
			<pages></pages>
			<resume></resume>
			<mots_cles></mots_cles>
			<title></title>
			<abstract></abstract>
			<keywords></keywords>
		</article>
	</articles>
</conference>
