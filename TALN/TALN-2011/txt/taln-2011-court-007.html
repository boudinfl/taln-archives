<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
"http://www.w3.org/TR/html4/loose.dtd">
<html><head><title>S&#233;lection de r&#233;ponses &#224; des questions dans un corpus Web par validation</title>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
</head>
<body>
<div style="page-break-before:always; page-break-after:always"><div><p>TALN 2011, Montpellier, 27 juin &#8211; 1er juillet 2011
</p>
<p>S&#233;lection de r&#233;ponses &#224; des questions dans un corpus Web par validation
</p>
<p>A. Grappy1, 2, B. Grau1, 3, M.-H. Falco1, 2, A.-L. Ligozat1, 3, I. Robba1, 4, A. Vilnat1, 2
</p>
<p>(1) LIMSI-CNRS
(2) Universit&#233; Paris 11
</p>
<p>(3) ENSIIE
(4) UVSQ
</p>
<p>prenom.nom@limsi.fr
</p>
<p>R&#233;sum&#233;. Les syst&#232;mes de questions r&#233;ponses recherchent la r&#233;ponse &#224; une question pos&#233;e en langue natu-
relle dans un ensemble de documents. Les collections Web diff&#232;rent des articles de journaux de par leurs structures
et leur style. Pour tenir compte de ces sp&#233;cificit&#233;s nous avons d&#233;velopp&#233; un syst&#232;me fond&#233; sur une approche ro-
buste de validation o&#249; des r&#233;ponses candidates sont extraites &#224; partir de courts passages textuels puis ordonn&#233;es
par apprentissage. Les r&#233;sultats montrent une am&#233;lioration du MRR (Mean Reciprocal Rank) de 48% par rapport
&#224; la baseline.
</p>
<p>Abstract. Question answering systems look for the answer of a question given in natural language in a
large collection of documents. Web documents have a structure and a style different from those of newspaper
articles. We developed a QA system based on an answer validation process able to handle Web specificity. Large
number of candidate answers are extracted from short passages in order to be validated according to question and
passage characteristics. The validation module is based on a machine learning approach. We show that our system
outperforms a baseline by up to 48% in MRR (Mean Reciprocal Rank).
</p>
<p>Mots-cl&#233;s : syst&#232;mes de questions r&#233;ponses ; validation de r&#233;ponses ; analyse de documents Web.
</p>
<p>Keywords: question-answering system ; answer validation ; Web document analysis .
</p>
<p>1 Introduction
</p>
<p>La recherche d&#8217;informations pr&#233;cises dans des textes, en r&#233;ponse &#224; des questions pos&#233;es en langue naturelle,
constitue un domaine largement &#233;tudi&#233; depuis la premi&#232;re &#233;valuation de syst&#232;mes de r&#233;ponses &#224; des questions
(SQR dans la suite) lanc&#233;e &#224; TREC en 1998 (Q&amp;A track). Les meilleurs syst&#232;mes (Hickl et al., 2006; Bouma
et al., 2005; Laurent et al., 2010) utilisent des connaissances et des processus avanc&#233;s de TAL notamment des
analyseurs syntaxiques. Ces connaissances et processus interviennent notamment lors de la phase de s&#233;lection de
passages pertinents et d&#8217;extraction de r&#233;ponses, qui ont fait l&#8217;objet d&#8217;&#233;tudes sp&#233;cifiques.
</p>
<p>L&#8217;ordonnancement de r&#233;ponses ou de passages consiste &#224; ordonner les diff&#233;rentes r&#233;ponses extraites afin d&#8217;obtenir
la meilleure r&#233;ponse en premi&#232;re position. L&#224; aussi, les meilleures approches se fondent sur des correspondances
syntaxiques ou s&#233;mantiques entre les passages (souvent constitu&#233;s d&#8217;une phrase) et la question, correspondances
obtenues par calcul de similarit&#233; entre arbres syntaxiques (Kouylekov et al., 2006) ou en tenant compte de chemins
de d&#233;pendances communs (Cui et al., 2005).
</p>
<p>Ces diff&#233;rents syst&#232;mes obtiennent de bons r&#233;sultats sur des documents issus d&#8217;articles de journaux, mais ne
peuvent &#234;tre appliqu&#233;s en l&#8217;&#233;tat sur des collections provenant du Web, comme celle constitu&#233;e dans le cadre du
projet Qu&#230;ro1 pour &#233;valuer les SQR. Pour le fran&#231;ais, les SQR participants ont trouv&#233; entre 27 % et 50 % des
r&#233;ponses en 2009 (Quintard et al., 2010) apr&#232;s adaptation alors que le meilleur syst&#232;me en obtenait 69% lors de
l&#8217;&#233;valuation CLEF 2006 sur des articles de journaux (Laurent et al., 2010). Ces difficult&#233;s sont dues entre autres
aux sp&#233;cificit&#233;s des documents Web tr&#232;s souvent compos&#233;s de tableaux, de listes ou de menus qui mettent en
d&#233;faut les analyses syntaxiques une fois le texte extrait des pages.
</p>
<p>1http ://www.quaero.org - Qu&#230;ro est un programme financ&#233; par OSEO</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>A. GRAPPY, B. GRAU, M.-H. FALCO, A.-L. LIGOZAT, I. ROBBA, A. VILNAT
</p>
<p>Afin de tenir compte des probl&#232;mes dus au style des documents, nous avons con&#231;u un syst&#232;me sur le fran&#231;ais,
QAVAL (Question Answering by VALidation) pouvant s&#8217;appliquer sur tout type de documents. Alors que nos
pr&#233;c&#233;dentes approches appliquaient des filtres successifs visant &#224; s&#233;lectionner des passages, puis des phrases, puis
extraire des r&#233;ponses, QAVAL extrait directement de nombreuses r&#233;ponses &#224; partir de passages de 300 caract&#232;res
extraits des documents. Ces r&#233;ponses candidates sont ensuite ordonn&#233;es par un module de validation de r&#233;ponses.
</p>
<p>La validation de r&#233;ponses vise &#224; valider des r&#233;ponses extraites par des SQR en v&#233;rifiant qu&#8217;elles sont correctes
et justifi&#233;es par le passage de texte extrait. La plupart des approches (Herrera et al., 2006) utilisent des crit&#232;res
lexicaux et syntaxiques, tels que la pr&#233;sence des termes de la question dans le passage et leur proximit&#233;, pour
mesurer la similarit&#233; entre la question et le passage et &#233;valuer la pertinence de la r&#233;ponse. Quelques SQR ont
int&#233;gr&#233; la validation de r&#233;ponses : pour ordonner les passages et les r&#233;ponses (Harabagiu &amp; Hickl, 2006) ou pour
choisir la r&#233;ponse &#224; partir de plusieurs ensembles propos&#233;s (T&#233;llez-Valero et al., 2010). Le syst&#232;me d&#8217;IBM (Martin
et al., 2001) s&#8217;am&#233;liore ainsi (MRR 0,496 vs 0,458) en utilisant une approche par apprentissage.
</p>
<p>Dans QAVAL, nous avons mis en &#339;uvre une approche par apprentissage afin de pouvoir se fonder sur des crit&#232;res
locaux et robustes pour caract&#233;riser les r&#233;ponses &#224; valider. Cette approche permet de s&#8217;affranchir de l&#8217;absence
de phrases compl&#232;tes bien form&#233;es et de g&#233;rer la dispersion &#233;ventuelle des informations utiles &#224; la validation.
Nous appliquons QAVAL sur des questions factuelles, qui attendent la pr&#233;cision d&#8217;un fait en r&#233;ponse comme par
exemple sa date dans &#171; Quand le pont de Normandie a-t-il &#233;t&#233; inaugur&#233; ? &#187;, et dont les informations sont souvent
r&#233;parties sur plus d&#8217;une phrase.
</p>
<p>L&#8217;article pr&#233;sente d&#8217;abord les pr&#233;traitements effectu&#233;s sur les documents Web. Puis il s&#8217;int&#233;resse aux diff&#233;rents
modules du syst&#232;me : l&#8217;analyse de la question qui extrait de celle-ci les informations utiles &#224; la recherche de la
r&#233;ponse, la recherche et le traitement des passages, l&#8217;extraction des r&#233;ponses depuis ces passages et l&#8217;ordonnance-
ment des r&#233;ponses. Il se termine par une partie exp&#233;rimentation qui pr&#233;sente le corpus et l&#8217;&#233;valuation de QAVAL
et montre que notre approche obtient de bons r&#233;sultats avec un MRR2 sup&#233;rieur de 48% &#224; celui de la baseline.
</p>
<p>2 Le syst&#232;me QAVAL
</p>
<p>Le syst&#232;me QAVAL est constitu&#233; de modules s&#233;quentiels, qui peuvent &#234;tre regroup&#233;s selon quatre grandes &#233;tapes :
1) L&#8217;analyse des questions ; 2) La recherche de passages et leur annotation &#224; partir des documents Web pr&#233;trait&#233;s ;
3) L&#8217;extraction de r&#233;ponses candidates ; 4) La validation de r&#233;ponses. Les deux premi&#232;res &#233;tapes font l&#8217;objet de
cette section.
</p>
<p>2.1 Pr&#233;traitement des documents
</p>
<p>Nous avons pr&#233;trait&#233; l&#8217;ensemble des documents HTML de la collection Qu&#230;ro avec notre logiciel Kitten3 afin de
les rendre homog&#232;nes et utilisables (notamment pour le traitement syntaxique). Les documents HTML sont tout
d&#8217;abord format&#233;s et convertis au format XHTML en appliquant HTMLCleaner4 et jTIDY5. Leur contenu textuel est
ensuite extrait selon un filtre sur les types de balises (script, paragraphe) puis selon des expressions r&#233;guli&#232;res afin
de d&#233;limiter les phrases par ajout de ponctuation. En effet, de par la disposition visuelle des pages HTML (titres,
sections, menus), les phrases sont visuellement s&#233;par&#233;es (une fois le HTML interpr&#233;t&#233;) bien que ne se terminant
pas par un point. Enfin, une extraction non-lin&#233;aire est effectu&#233;e pour les structures visuelles sp&#233;cifiques telles
que les tableaux en r&#233;p&#233;tant les en-t&#234;tes pour chaque valeur afin de faciliter l&#8217;extraction des r&#233;ponses puisqu&#8217;une
extraction lin&#233;aire &#233;loignerait de l&#8217;en-t&#234;te la valeur d&#8217;une case d&#8217;un tableau.
</p>
<p>2.2 Analyse des questions
</p>
<p>L&#8217;analyse des questions vise &#224; extraire les informations utiles &#224; la recherche de passages et &#224; l&#8217;extraction de
r&#233;ponses. Outre le type de r&#233;ponse attendu, qui correspond &#224; un type d&#8217;entit&#233; nomm&#233;e que l&#8217;on sait reconna&#238;tre
</p>
<p>2Mean Reciprocal Rank : moyenne sur l&#8217;inverse du rang de la bonne r&#233;ponse
3Kitten Is A Textual Treatment for Extraction and Normalization
4http://htmlcleaner.sourceforge.net/
5http://jtidy.sourceforge.net/</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>S&#201;LECTION DE R&#201;PONSES &#192; DES QUESTIONS DANS UN CORPUS WEB PAR VALIDATION
</p>
<p>dans les textes, et les termes de la question, nous reconnaissons le type sp&#233;cifique de la r&#233;ponse, s&#8217;il est explicite,
le focus et la cat&#233;gorie de la question. Le focus d&#233;signe l&#8217;&#233;l&#233;ment &#224; propos duquel on demande une information
(une entit&#233; ou un &#233;v&#233;nement) et peut donc &#234;tre repr&#233;sent&#233; par un nom ou un verbe. Par exemple, la question &#171; Quel
pr&#233;sident succ&#233;da &#224; Jacques Chirac ? &#187; a &#171; succ&#233;der &#187; comme focus, &#171; personne &#187; comme type d&#8217;entit&#233; nomm&#233;e
et &#171; pr&#233;sident &#187; comme type sp&#233;cifique. Selon l&#8217;existence du focus, son type, entit&#233; ou &#233;v&#233;nement, et le type de
r&#233;ponse attendue, nous assignons une cat&#233;gorie &#224; la question qui repr&#233;sente le type de relation qui devra exister
entre la r&#233;ponse et le focus ou le type dans les documents : modifieur du nom, sujet ou compl&#233;ment d&#8217;objet du
verbe, compl&#233;ment circonstanciel ... A la question pr&#233;c&#233;dente, on attend une r&#233;ponse sujet du verbe en focus.
</p>
<p>2.3 Recherche, s&#233;lection et annotation des passages
</p>
<p>L&#8217;approche g&#233;n&#233;ralement utilis&#233;e dans les SQR consiste &#224; retenir des passages de tailles variables (de 1 &#224; 3
phrases). Nous avons choisi d&#8217;utiliser le moteur de recherche Lucene6 pour proc&#233;der &#224; l&#8217;indexation des docu-
ments et &#224; la recherche de passages. Lucene peut renvoyer des extraits de documents et permet de param&#233;trer leur
taille. Comme les passages renvoy&#233;s ne contiennent pas toujours des phrases compl&#232;tes, la premi&#232;re et la derni&#232;re
phrase sont donc compl&#233;t&#233;es afin de faciliter l&#8217;analyse syntaxique des passages. Apr&#232;s exp&#233;rimentations, nous
avons d&#233;cid&#233; d&#8217;extraire un passage par document, d&#8217;environ 300 caract&#232;res, soit environ 3 phrases. Les passages
retourn&#233;s par Lucene sont ensuite analys&#233;s par Fastr (Jacquemin, 1996), qui rep&#232;re les termes simples ou com-
plexes de la question et leurs variantes. Ces variations peuvent &#234;tre morphologiques, syntaxiques ou s&#233;mantiques,
et &#224; chacune d&#8217;elle est associ&#233; un poids, d&#8217;autant plus fort que la variation est fiable. Les passages sont ordonn&#233;s
gr&#226;ce &#224; ces scores et les 50 meilleurs sont gard&#233;s.
</p>
<p>Les passages sont ensuite annot&#233;s afin de faciliter l&#8217;extraction des r&#233;ponses candidates. L&#8217;analyseur XIP (A&#239;t-
Mokhtar et al., 2002) identifie pour chaque phrase du passage, ses syntagmes, calcule l&#8217;ensemble des relations de
d&#233;pendances, et les entit&#233;s nomm&#233;es. Comme l&#8217;analyse syntaxique est moins fiable sur les documents Web, nous
ne retenons que les syntagmes et les entit&#233;s nomm&#233;es. Les informations donn&#233;es par l&#8217;analyse de la question sont
aussi annot&#233;es dans les passages : le focus, le type sp&#233;cifique, le verbe principal et les noms propres.
</p>
<p>3 Validation de r&#233;ponses
</p>
<p>3.1 Extraction des r&#233;ponses candidates
</p>
<p>Des r&#233;ponses candidates sont extraites des passages retenus afin d&#8217;&#234;tre ordonn&#233;es. La s&#233;lection des candidats
est volontairement peu contrainte, dans le but de ne pas omettre la r&#233;ponse correcte quitte &#224; avoir davantage
de r&#233;ponses candidates &#224; valider. Potentiellement tous les groupes nominaux pourraient constituer des candidats
puisque les questions consid&#233;r&#233;es sont d&#8217;ordre factuel et attendent en r&#233;ponse un modifieur du nom ou un compl&#233;-
ment du verbe. Toutefois, afin de limiter le nombre de r&#233;ponses extraites, un filtre est appliqu&#233;, pour ne conserver
que les entit&#233;s nomm&#233;es correspondant au type de l&#8217;entit&#233; attendue par la question lorsqu&#8217;il existe. Ainsi la ques-
tion &#171; Qui est le pr&#233;sident des &#201;tats Unis ? &#187; attend une personne en r&#233;ponse et, dans ce cas, seuls les syntagmes
marqu&#233;s personne ou nom propre sont extraits.
</p>
<p>Comme de tr&#232;s nombreuses r&#233;ponses sont extraites des documents, une heuristique permettant de d&#233;classer les
r&#233;ponses qui ont tr&#232;s peu de chances d&#8217;&#234;tre correctes a &#233;t&#233; appliqu&#233;e. Cela correspond aux cas o&#249; la r&#233;ponse est
constitu&#233;e uniquement de mots contenus dans la question et &#224; ceux o&#249; les entit&#233;s nomm&#233;es de la question ne se
trouvent pas dans le passage justificatif. Les r&#233;ponses restantes sont ensuite ordonn&#233;es par le module de validation
de r&#233;ponses suivant une approche par apprentissage : &#233;tant donn&#233;s des couples de r&#233;ponses et passages dont elles
sont extraites, annot&#233;s par le syst&#232;me, il s&#8217;agit de d&#233;cider si les r&#233;ponses sont valides et de leur degr&#233; de validit&#233;.
</p>
<p>3.2 Ordonnancement de r&#233;ponses
</p>
<p>Pour l&#8217;ordonnancement de r&#233;ponses,nous avons adapt&#233; l&#8217;approche d&#233;velopp&#233;e pour la validation de r&#233;ponses four-
nies par des SQR (Grappy et al., 2008) en ajoutant des crit&#232;res d&#8217;apprentissage, notamment pour tenir compte du
</p>
<p>6http ://lucene.apache.org/</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>A. GRAPPY, B. GRAU, M.-H. FALCO, A.-L. LIGOZAT, I. ROBBA, A. VILNAT
</p>
<p>fait que la validit&#233; de la r&#233;ponse ne soit pas la seule v&#233;rification &#224; effectuer, et que les r&#233;ponses n&#8217;ont pas &#233;t&#233; filtr&#233;es
auparavant. Ces crit&#232;res permettent d&#8217;&#233;valuer la pertinence des passages (crit&#232;re 2 ci-dessous) et des r&#233;ponses par
rapport au passage (crit&#232;res 3b et 5). Nous avons aussi am&#233;lior&#233; la v&#233;rification du type de la r&#233;ponse. Globale-
ment, les crit&#232;res 1 et 2 traitent du passage afin d&#8217;&#233;valuer une proximit&#233; avec la question. Les suivants portent sur
la r&#233;ponse et permettent notamment de distinguer plusieurs r&#233;ponses issues d&#8217;un m&#234;me passage.
</p>
<p>1. la proportion des termes de la question pr&#233;sents dans le passage. Quatre calculs sont effectu&#233;s :
&#8211; les mots de la question pris &#224; l&#8217;identique ou sous forme de variantes,
&#8211; les mots r&#233;partis par cat&#233;gorie morphosyntaxique (noms propres, noms communs, verbes, adjectifs),
&#8211; les &#233;l&#233;ments remarqu&#233;s lors de l&#8217;analyse de la question (focus, type sp&#233;cifique et verbe principal),
&#8211; les multi-termes : ensemble de mots cons&#233;cutifs reconnus comme li&#233;s, comme &#171; Prix Nobel &#187; ;
</p>
<p>2. rang du passage obtenu lors de la s&#233;lection des passages ;
</p>
<p>3. proximit&#233; des termes. Si la r&#233;ponse est proche en surface des mots de la question, elle a plus de chance d&#8217;&#234;tre
li&#233;e &#224; ceux-ci, et donc valide. Pour &#233;valuer cette proximit&#233;, deux crit&#232;res sont utilis&#233;s :
a) la longueur de la plus longue cha&#238;ne de mots cons&#233;cutifs pr&#233;sents dans le passage et constitu&#233;e des mots de la
</p>
<p>question et de la r&#233;ponse. Deux mots sont dits cons&#233;cutifs s&#8217;ils sont adjacents, s&#233;par&#233;s par des &#233;l&#233;ments autoris&#233;s
(virgule, d&#233;terminant...) ou s&#233;par&#233;s par un unique mot,
</p>
<p>b) la distance entre la r&#233;ponse et les mots de la question. La moyenne des distances s&#233;parant la r&#233;ponse de chacun
des mots de la question est calcul&#233;e ;
</p>
<p>4. redondance Plus la m&#234;me r&#233;ponse est extraite de diff&#233;rents documents, plus elle a de chances d&#8217;&#234;tre correcte ;
</p>
<p>5. la cat&#233;gorie de la question, crit&#232;re caract&#233;risant la relation de d&#233;pendance avec la r&#233;ponse ;
</p>
<p>6. la v&#233;rification du type de la r&#233;ponse. Certaines questions pr&#233;cisent le type de la r&#233;ponse attendue, comme
&#171; Quel pr&#233;sident succ&#233;da &#224; Jacques Chirac ? &#187; qui attend un nom de pr&#233;sident en r&#233;ponse. Ce crit&#232;re v&#233;rifie que
la r&#233;ponse est du type sp&#233;cifique attendu par la question. Cette v&#233;rification se fait aussi par apprentissage sur
diff&#233;rents crit&#232;res :
&#8211; la fr&#233;quence d&#8217;apparition commune de la r&#233;ponse et du type dans les documents,
&#8211; l&#8217;utilisation des entit&#233;s nomm&#233;es du syst&#232;me de questions r&#233;ponses RITEL (Rosset et al., 2006) qui permet de
</p>
<p>reconna&#238;tre 70 types diff&#233;rents,
&#8211; la recherche du type dans les pages Wikip&#233;dia associ&#233;es &#224; la r&#233;ponse,
&#8211; la recherche de structures de phrases indiquant une correspondance entre la r&#233;ponse et le type (&#171; Albert Einstein
</p>
<p>est un physicien &#187;) dans les pages Wikip&#233;dia ;
La v&#233;rification du type de la r&#233;ponse est pr&#233;sent&#233;e plus en d&#233;tail dans (Grappy &amp; Grau, 2010). Une &#233;valuation
consistant &#224; d&#233;tecter les cas o&#249; une r&#233;ponse correspond au type a &#233;t&#233; men&#233;e et a obtenu 80 % de bons r&#233;sul-
tats. Notons cependant que ce crit&#232;re ne peut &#234;tre appliqu&#233; qu&#8217;&#224; certaines questions, toutes n&#8217;ayant pas un type
sp&#233;cifique.
</p>
<p>L&#8217;apprentissage de la validation de r&#233;ponse, utilisant l&#8217;ensemble des crit&#232;res ci-dessus, est effectu&#233; par une com-
binaison d&#8217;arbres de d&#233;cision gr&#226;ce &#224; la m&#233;thode bagging fournie par WEKA7. Ce classifieur fournit, pour chaque
r&#233;ponse, un score compris entre -1 et 1 indiquant sa confiance dans le fait que la r&#233;ponse soit valide. La valeur 1
indique que la r&#233;ponse est correcte et -1 qu&#8217;elle est non valide. Ce score nous permet d&#8217;ordonner les r&#233;ponses.
</p>
<p>Afin de constituer la base d&#8217;apprentissage, nous avons s&#233;lectionn&#233; les questions factuelles de QA@CLEF05 et
QA@CLEF06 et en avons cherch&#233; des r&#233;ponses dans la collection avec QAVAL. Les r&#233;ponses correspondant &#224; un
patron de r&#233;ponse connu sont ensuite valid&#233;es de mani&#232;re manuelle. La base d&#8217;apprentissage contient 349 r&#233;ponses
valides et 698 non valides.
</p>
<p>4 Exp&#233;rimentation
</p>
<p>La collection sur laquelle nous avons &#233;valu&#233; notre syst&#232;me correspond &#224; un corpus Web constitu&#233; par la soci&#233;t&#233;
Exalead 8 &#224; partir des requ&#234;tes d&#8217;utilisateurs sur leur moteur de recherche : deux millions de documents ont ainsi
</p>
<p>7WEKA : http ://sourceforge.net/projects/weka/
8http://www.exalead.com/software/</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>S&#201;LECTION DE R&#201;PONSES &#192; DES QUESTIONS DANS UN CORPUS WEB PAR VALIDATION
</p>
<p>&#233;t&#233; collect&#233;s. Un sous ensemble de 500 000 documents a &#233;t&#233; s&#233;lectionn&#233; pour les &#233;valuations.
</p>
<p>Pour tester notre syst&#232;me, nous avons utilis&#233; un ensemble de 147 questions factuelles venant de la campagne
Qu&#230;ro 2010. Afin d&#8217;&#233;valuer automatiquement notre SQR, nous avons recueilli un ensemble de r&#233;ponses correctes
pour chacune des questions. L&#8217;&#233;valuation consiste donc &#224; comparer chaque r&#233;ponse extraite aux r&#233;ponses atten-
dues. Nous avons utilis&#233; la mesure MRR, sur les cinq premi&#232;res r&#233;ponses, ainsi que la proportion de questions
ayant une bonne r&#233;ponse en premi&#232;re position ou dans les cinq premi&#232;res positions pour &#233;valuer les r&#233;sultats.
Nous avons compar&#233; notre m&#233;thode &#224; une baseline portant sur l&#8217;extraction et la validation de r&#233;ponses. Elle extrait
les candidats les plus proches des mots de la question dans les cinq premiers passages ordonn&#233;s suivant leur rang
apr&#232;s leur extraction. Le tableau 1 pr&#233;sente les r&#233;sultats obtenus avec 50 passages retenus sur 150 ramen&#233;s par Lu-
cene. 11 405 r&#233;ponses candidates sont extraites, soit 77 en moyenne par question. Les passages de 300 caract&#232;res
servant &#224; extraire les r&#233;ponses contiennent 6 phrases en moyenne, alors que sur un corpus d&#8217;articles de journaux
ils en contiennent 3, et poss&#232;dent moiti&#233; moins de verbes, ce qui rend compte du fait qu&#8217;ils sont souvent form&#233;s
de suites de syntagmes.
</p>
<p>MRR premier rang %(#) cinq premiers rangs %(#)
Qu&#230;ro 0,43 34% (50) 55% (81)
baseline 0,29 21% (32) 43% (64)
</p>
<p>TAB. 1 &#8211; R&#233;sultats QAVAL
</p>
<p>QAVAL surpasse les r&#233;sultats de la baseline de 48 % ce qui montre l&#8217;apport de notre m&#233;thode. Afin d&#8217;&#233;valuer
uniquement le module de validation de r&#233;ponses, nous avons &#233;valu&#233; notre syst&#232;me sur les 125 questions pour
lesquelles la bonne r&#233;ponse se trouve dans un passage s&#233;lectionn&#233;. 65% des questions ont une r&#233;ponse correcte
parmi les cinq meilleures et 40% en premi&#232;re position ce qui est meilleur que les r&#233;sultats obtenus par (Cui et al.,
2005) qui trouvait 39% de r&#233;ponses correctes en premi&#232;re position sur des passages extraits de journaux en anglais.
</p>
<p>L&#8217;article (Quintard et al., 2010) montre que les autres syst&#232;mes cherchant une r&#233;ponse pour les questions factuelles
sur le corpus Qu&#230;ro obtiennent un MRR entre 0,284 et 0,54. Nos r&#233;sultats sont donc de m&#234;me ordre de grandeur
que ceux obtenus par les meilleurs syst&#232;mes.
</p>
<p>Afin de tester la robustesse de notre m&#233;thode, nous avons appliqu&#233; QAVAL sur une collection constitu&#233;e d&#8217;articles
du journal Le Monde et de d&#233;p&#234;ches ATS. Pour cela, nous avons pris 128 questions factuelles provenant de la
campagne EQUER. Une nouvelle base d&#8217;apprentissage a &#233;t&#233; utilis&#233;e, dans laquelle les r&#233;ponses sont extraites de
cet ensemble de documents. Les r&#233;sultats sont comparables &#224; ceux obtenus sur les documents Web (MRR de 0,47)
ce qui t&#233;moigne de la robustesse de la m&#233;thode et portent notre syst&#232;me aux r&#233;sultats de l&#8217;&#233;tat de l&#8217;art avec ce type
d&#8217;approche. L&#8217;apport de la validation de r&#233;ponses est encore important puisqu&#8217;il montre une am&#233;lioration de 38 %
par rapport &#224; la baseline.
</p>
<p>Une analyse des arbres de d&#233;cision a montr&#233; que tous les crit&#232;res sont utilis&#233;s et que les trois crit&#232;res les plus
importants sont la fr&#233;quence de la r&#233;ponse, le rang du passage et la proximit&#233; des termes. L&#8217;apport de la v&#233;rification
du type de la r&#233;ponse a &#233;galement &#233;t&#233; mesur&#233; en retirant ce crit&#232;re de l&#8217;ensemble de d&#233;part. Le MRR passe alors
de 0,43 &#224; 0,41 ce qui montre bien que ce crit&#232;re est important d&#8217;autant plus qu&#8217;il ne s&#8217;applique pas &#224; toutes les
questions. Les m&#234;mes r&#233;sultats sont obtenus avec la cat&#233;gorie de la question.
</p>
<p>Afin d&#8217;&#233;valuer le pr&#233;traitement des documents, nous avons appliqu&#233; QAVAL avec trois pr&#233;traitements diff&#233;rents :
le premier est Kitten, le second, la baseline, est une extraction compl&#232;te du contenu textuel et le troisi&#232;me applique
le logiciel BoilerPipe (Kohlsch&#252;tter et al., 2010) qui utilise des traits textuels de surface. En plus du MRR, nous
avons calcul&#233; la proportion de questions ayant au moins un document contenant la r&#233;ponse peu importe son rang
(cf. tableau 2). Les r&#233;sultats montrent que notre traitement am&#233;liore les r&#233;sultats de la baseline puisque le MRR
lui est sup&#233;rieur de 30%. Nous pouvons aussi voir que la m&#233;thode est sup&#233;rieure aux r&#233;sultats de BoilerPipe qui
semble moins adapt&#233; &#224; l&#8217;utilisation d&#8217;une collection Web dans le cadre des SQR.
</p>
<p>BoilerPipe baseline Kitten
% bons doc. (#) 77 %(114) 82 %(121) 88% (130)
MRR 0,28 0,33 0,43
</p>
<p>TAB. 2 &#8211; R&#244;le du pr&#233;traitement des documents dans QAVAL</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>A. GRAPPY, B. GRAU, M.-H. FALCO, A.-L. LIGOZAT, I. ROBBA, A. VILNAT
</p>
<p>5 Conclusion
</p>
<p>Alors que les SQR d&#233;velopp&#233;s sur des collections d&#8217;articles de journaux rencontrent des difficult&#233;s sur les docu-
ments Web, le syst&#232;me QAVAL9 applique une m&#233;thode robuste de validation de r&#233;ponses permettant d&#8217;ordonner
les nombreuses r&#233;ponses extraites depuis des passages de 300 caract&#232;res environ &#224; partir d&#8217;un apprentissage sur
des crit&#232;res locaux. Ce type de crit&#232;res permet de tenir compte de la dispersion des informations sur plus d&#8217;une
phrase dans les passages. QAVAL permet de surpasser la baseline de 48% et obtient des r&#233;sultats analogues &#224; ceux
obtenus sur des documents issus d&#8217;articles de journaux.
</p>
<p>Pour am&#233;liorer les r&#233;sultats, nous envisageons d&#8217;introduire de nouveaux crit&#232;res. Une possibilit&#233; est d&#8217;utiliser un
module de paraphrases sous phrastique permettant de rapprocher les expressions contenues dans la question et
les passages. QAVAL pourrait aussi &#234;tre utilis&#233; pour traiter des questions bool&#233;ennes. Dans celles-ci la valeur &#224;
donner est OUI si un des passages trouv&#233;s justifie la forme affirmative de la question.
</p>
<p>R&#233;f&#233;rences
</p>
<p>A&#207;T-MOKHTAR S., CHANOD J.-P. &amp; ROUX C. (2002). Robustness beyond shallowness : incremental deep
parsing. Nat. Lang. Eng., 8.
BOUMA G., FAHMI I., MUR J., VAN NOORD G., VAN DER PLAS L. &amp; TIEDEMANN J. (2005). Linguistic
Knowledge and question answering. Traitement automatique des langues sp&#233;cial R&#233;pondre &#224; des questions,
46(3).
CUI H., SUN R., LI K., YEN KAN M. &amp; SENG CHUA T. (2005). Question answering passage retrieval using
dependency relations. In SIGIR 2005.
GRAPPY A. &amp; GRAU B. (2010). Answer type validation in question answering systems. In Recherche d&#8217;Infor-
mations Assist&#233; par Ordinateur.
GRAPPY A., LIGOZAT A.-L. &amp; GRAU B. (2008). Evaluation de la r&#233;ponse d&#8217;un syst&#232;me de question-r&#233;ponse
et de sa justification. In COnf&#233;rence en Recherche d&#8217;Infomations et Applications.
HARABAGIU S. &amp; HICKL A. (2006). Methods for using textual entailment in open-domain question answering.
In Proceedings of the 44th annual meeting of the Association for Computational Linguistics.
HERRERA J., RODRIGO A., PENAS A. &amp; VERDEJO F. (2006). UNED submission to AVE 2006. In Working
Notes for the CLEF 2006 Workshop (AVE).
HICKL A., WILLIAMS J., BENSLEY J., ROBERTS K., SHI Y. &amp; RINK B. (2006). Question answering with
LCC&#8217;s CHAUCER at TREC 2006. In Proceedings of the Fifteenth Text REtrieval Conference.
JACQUEMIN C. (1996). A Symbolic and Surgical Acquisition of Terms Through Variation. In Connectionist,
Statistical and Symbolic Approaches to Learning for Natural Language Processing, p. 425&#8211;438.
KOHLSCH&#220;TTER C., FANKHAUSER P. &amp; NEJDL W. (2010). Boilerplate detection using shallow text features.
In WSDM.
KOUYLEKOV M., NEGRI M., MAGNINI B. &amp; COPPOLA B. (2006). Towards Entailment-based Question Ans-
wering : ITC-irst at CLEF 2006. In 7th Workshop of the Cross-Language Evaluation Forum.
LAURENT D., S&#201;GU&#201;LA P. &amp; N&#200;GRE S. (2010). Cross lingual question answering using qristal for clef 2006.
Evaluation of Multilingual and Multi-modal Information Retrieval, p. 339&#8211;350.
MARTIN A. I., FRANZ M. &amp; ROUKOS S. (2001). Ibm&#8217;s statistical question answering system-trec-10. In In
Proceedings of TREC10.
QUINTARD L., GALIBERT O., ADDA G., GRAU B., LAURENT D., MORICEAU V., ROSSET S., TANNIER X.
&amp; VILNAT A. (2010). Question Answering on web data : the QA evaluation in Qu&#230;ro. In Proceedings of the
Seventh conference on International Language Resources and Evaluation.
ROSSET S., GALIBERT O., ILLOUZ G. &amp; MAX A. (2006). Interaction et recherche d&#8217;information : le projet
ritel. Traitement Automatique des Langues (TAL), num&#233;ro sp&#233;cial R&#233;pondre &#224; des questions, volume 46 :3, 46(3).
T&#201;LLEZ-VALERO A., MONTES-Y G&#211;MEZ M., VILLASE&#209;OR-PINEDA L., DEL LENGUAJE L. &amp; PE&#209;AS-
PADILLA A. (2010). Towards Multi-Stream Question Answering Using Answer Validation. Informatica, 34.
</p>
<p>9Ces travaux ont &#233;t&#233; en partie r&#233;alis&#233;s dans le cadre du programme QUAERO, financ&#233; par OSEO, agence fran&#231;aise pour l&#8217;innovation.</p>

</div></div>
</body></html>