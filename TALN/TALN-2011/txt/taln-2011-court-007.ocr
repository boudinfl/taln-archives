TALN 2011, Montpellier, 27juin — l"'ju1'11et 2011

Selection de réponses £1 des questions dans un corpus Web par validation

A. Grappyl’ 2, B. Grau1’3, M.-H. Fa1co1’2, A.-L. Ligozat1’3, I. Robba1’4, A. Vilnatl’ 2
(1) LHVISI-CNRS
(2) Universite Paris 11
(3) ENSIIE
(4) UVSQ
prenom.nom@1imsi.fr

Résﬂmé. Les systemes de questions reponses recherchent la reponse a une question posee en langue natu-
relle dans un ensemble de documents. Les collections Web different des articles de joumaux de par leurs structures
et leur style. Pour tenir compte de ces speciﬁcites nous avons developpe un systeme fonde sur une approche ro-
buste de validation ou des reponses candidates sont extraites a partir de courts passages textuels puis ordonnees
par apprentissage. Les resultats montrent une amelioration du MRR (Mean Reciprocal Rank) de 48% par rapport
a la baseline.

Abstract. Question answering systems look for the answer of a question given in natural language in a
large collection of documents. Web documents have a structure and a style different from those of newspaper
articles. We developed a QA system based on an answer validation process able to handle Web speciﬁcity. Large
number of candidate answers are extracted from short passages in order to be validated according to question and
passage characteristics. The validation module is based on a machine learning approach. We show that our system
outperforms a baseline by up to 48% in MR (Mean Reciprocal Rank).

M0tS-CléS 3 systemes de questions reponses ; validation de reponses; analyse de documents Web.

Keywords: question—answering system; answer validation ; Web document analysis .

1 Introduction

La recherche d’informations precises dans des textes, en reponse a des questions posees en langue naturelle,
constitue un domaine largement etudie depuis la premiere evaluation de systemes de reponses a des questions
(SQR dans la suite) lancee a TREC en 1998 (Q&A track). Les meilleurs systemes (Hickl et al., 2006; Bouma
et al., 2005; Laurent et al., 2010) utilisent des connaissances et des processus avances de TAL notamment des
analyseurs syntaxiques. Ces connaissances et processus interviennent notarmnent lors de la phase de selection de
passages pertinents et d’ extraction de reponses, qui ont fait l’obj et d’etudes speciﬁques.

L’ ordonnancement de reponses ou de passages consiste a ordonner les differentes reponses extraites aﬁn d’obtenir
la meilleure reponse en premiere position. La aussi, les meilleures approches se fondent sur des correspondances
syntaxiques ou semantiques entre les passages (souvent constitues d’une phrase) et la question, correspondances
obtenues par calcul de similarite entre arbres syntaxiques (Kouylekov et al., 2006) ou en tenant compte de chemins
de dependances communs (Cui et al., 2005).

Ces differents systemes obtiennent de bons resultats sur des documents issus d’articles de joumaux, mais ne
peuvent etre appliques en l’etat sur des collections provenant du Web, comme celle constituee dans le cadre du
projet Quaerol pour evaluer les SQR. Pour le frangais, les SQR participants ont trouve entre 27 % et 50 % des
reponses en 2009 (Quintard et al., 2010) apres adaptation alors que le meilleur systeme en obtenait 69% lors de
l’evaluation CLEF 2006 sur des articles de joumaux (Laurent et al., 2010). Ces difﬁcultes sont dues entre autres
aux speciﬁcites des documents Web tres souvent composes de tableaux, de listes ou de menus qui mettent en
defaut les analyses syntaxiques une fois le texte extrait des pages.

lhttp ://www.quaero.org - Quaaro est un programme ﬁnance par OSEO

A. GRAPPY, B. GRAU, M.—H. FALCO, A.—L. LIGOZAT, I. ROBBA, A. VILNAT

Aﬁn de tenir compte des problemes dus au style des documents, nous avons congu un systeme sur le frangais,
QAVAL (Question Answering by VALidation) pouvant s’appliquer sur tout type de documents. Alors que nos
precedentes approches appliquaient des ﬁltres successifs Visant a sélectionner des passages, puis des phrases, puis
extraire des réponses, QAVAL extrait directement de nombreuses réponses a partir de passages de 300 caracteres
extraits des documents. Ces reponses candidates sont ensuite ordonnées par un module de Validation de reponses.

La Validation de réponses Vise a Valider des reponses extraites par des SQR en Vériﬁant qu’elles sont correctes
et justiﬁées par le passage de texte extrait. La plupart des approches (Herrera et al., 2006) utilisent des criteres
lexicaux et syntaxiques, tels que la presence des termes de la question dans le passage et leur proximite, pour
mesurer la similarite entre la question et le passage et évaluer la pertinence de la reponse. Quelques SQR ont
integré la Validation de réponses : pour ordonner les passages et les reponses (Harabagiu & Hickl, 2006) ou pour
choisir la réponse a partir de plusieurs ensembles proposes (Tellez—Valero et al., 2010). Le systeme d’IBM (Martin
et al., 2001) s’améliore ainsi (MRR 0,496 Vs 0,458) en utilisant une approche par apprentissage.

Dans QAVAL, nous avons mis en oeuvre une approche par apprentissage aﬁn de pouvoir se fonder sur des criteres
locaux et robustes pour caractériser les réponses a Valider. Cette approche permet de s’affranchir de l’absence
de phrases completes bien formées et de gerer la dispersion éventuelle des informations utiles a la Validation.
Nous appliquons QAVAL sur des questions factuelles, qui attendent la precision d’un fait en réponse comme par
exemple sa date dans << Quand le pont de Normandie a—t—il éte inaugure ‘.7 », et dont les informations sont souvent
réparties sur plus d’ une phrase.

L’ article presente d’abord les prétraitements effectués sur les documents Web. Puis il s’interesse aux différents
modules du systeme : l’analyse de la question qui extrait de celle-ci les informations utiles a la recherche de la
réponse, la recherche et le traitement des passages, l’extraction des réponses depuis ces passages et l’ordonnance—
ment des réponses. 11 se termine par une partie experimentation qui présente le corpus et l’éValuation de QAVAL
et montre que notre approche obtient de bons resultats avec un MRR2 superieur de 48% a celui de la baseline.

2 Le systeme QAVAL

Le systeme QAVAL est constitué de modules sequentiels, qui peuvent etre regroupes selon quatre grandes etapes :
1) L’ analyse des questions ; 2) La recherche de passages et leur annotation a partir des documents Web prétraites ;
3) L’extraction de reponses candidates; 4) La Validation de réponses. Les deux premieres etapes font l’objet de
cette section.

2.1 Prétraitement des documents

Nous avons prétraite l’ensemble des documents HTML de la collection Quaero avec notre logiciel Kitten3 aﬁn de
les rendre homogenes et utilisables (notamment pour le traitement syntaxique). Les documents HTML sont tout
d’ abord formates et convertis au format XHTML en appliquant H TMLCleane r4 etjTIDY5. Leur contenu textuel est
ensuite extrait selon un ﬁltre sur les types de balises (script, paragraphe) puis selon des expressions regulieres aﬁn
de delimiter les phrases par ajout de ponctuation. En effet, de par la disposition Visuelle des pages HTML (titres,
sections, menus), les phrases sont Visuellement séparées (une fois le HTML interprete) bien que ne se terminant
pas par un point. Enﬁn, une extraction non-linéaire est effectuée pour les structures Visuelles speciﬁques telles
que les tableaux en répetant les en—tetes pour chaque Valeur aﬁn de faciliter l’extraction des reponses puisqu’une
extraction lineaire eloignerait de l’en—tete la Valeur d’une case d’un tableau.

2.2 Analyse des questions

L’analyse des questions Vise a extraire les informations utiles a la recherche de passages et a l’extraction de
réponses. Outre le type de réponse attendu, qui correspond a un type d’entité nommee que l’on sait reconnaitre

2Mean Reciprocal Rank : moyenne sur l’inverse du rang de la bonne réponse
3Kitten Is A Textual Treatment for Extraction and Normalization
4http://htmlcleaner.sourceforge.net/
Shttp://jtidy.sourceforge.net/

SELECTION DE REPONSES A DES QUESTIONS DANS UN CORPUS WEB PAR VALIDATION

dans les textes, et les termes de la question, nous reconnaissons le type speciﬁque de la reponse, s’il est explicite,
le focus et la categorie de la question. Le focus designe l’element a propos duquel on demande une information
(une entite ou un evenement) et peut donc etre represente par un nom ou un Verbe. Par exemple, la question << Quel
president succeda a Jacques Chirac ‘.7 » a << succeder » comme focus, << personne » comme type d’entite nommee
et « president» comme type speciﬁque. Selon l’existence du focus, son type, entite ou evenement, et le type de
reponse attendue, nous assignons une categorie a la question qui represente le type de relation qui devra exister
entre la reponse et le focus ou le type dans les documents : modiﬁeur du nom, sujet ou complement d’ objet du
Verbe, complement circonstanciel  A la question precedente, on attend une reponse sujet du Verbe en focus.

2.3 Recherche, sélection et annotation des passages

L’approche generalement utilisee dans les SQR consiste a retenir des passages de tailles Variables (de 1 a 3
phrases). Nous avons choisi d’utiliser le moteur de recherche Lucene5 pour proceder a l’indexation des docu-
ments et a la recherche de passages. Lucene peut renvoyer des extraits de documents et permet de pararnetrer leur
taille. Comme les passages renvoyes ne contiennent pas toujours des phrases completes, la premiere et la derniere
phrase sont donc completees aﬁn de faciliter l’analyse syntaxique des passages. Apres experimentations, nous
avons decide d’extraire un passage par document, d’enViron 300 caracteres, soit environ 3 phrases. Les passages
retoumes par Lucene sont ensuite analyses par Fastr (Jacquemin, 1996), qui repere les termes simples ou com-
plexes de la question et leurs Variantes. Ces Variations peuvent etre morphologiques, syntaxiques ou semantiques,
et a chacune d’elle est associe un poids, d’autant plus fort que la Variation est ﬁable. Les passages sont ordonnes
grace a ces scores et les 50 meilleurs sont gardes.

Les passages sont ensuite annotes aﬁn de faciliter l’extraction des reponses candidates. L’ analyseur XIP (A'1't-
Mokhtar et al., 2002) identiﬁe pour chaque phrase du passage, ses syntagmes, calcule l’ensemble des relations de
dependances, et les entites nommees. Comme l’analyse syntaxique est moins ﬁable sur les documents Web, nous
ne retenons que les syntagmes et les entites nommees. Les informations donnees par l’analyse de la question sont
aussi annotees dans les passages : le focus, le type speciﬁque, le Verbe principal et les noms propres.

3 Validation de réponses

3.1 Extraction des réponses candidates

Des reponses candidates sont extraites des passages retenus aﬁn d’etre ordonnees. La selection des candidats
est Volontairement peu contrainte, dans le but de ne pas omettre la reponse correcte quitte a avoir davantage
de reponses candidates a Valider. Potentiellement tous les groupes nominaux pourraient constituer des candidats
puisque les questions considerees sont d’ordre factuel et attendent en reponse un modiﬁeur du nom ou un comple-
ment du Verbe. Toutefois, aﬁn de limiter le nombre de reponses extraites, un ﬁltre est applique, pour ne conserver
que les entites nommees correspondant au type de l’entite attendue par la question lorsqu’il existe. Ainsi la ques-
tion << Qui est le president des Etats Unis ‘.7 » attend une personne en reponse et, dans ce cas, seuls les syntagmes
marques personne ou nom propre sont extraits.

Comme de tres nombreuses reponses sont extraites des documents, une heuristique permettant de declasser les
reponses qui ont tres peu de chances d’etre correctes a ete appliquee. Cela correspond aux cas ou la reponse est
constituee uniquement de mots contenus dans la question et a ceux ou les entites nommees de la question ne se
trouvent pas dans le passage justiﬁcatif. Les reponses restantes sont ensuite ordonnees par le module de Validation
de reponses suivant une approche par apprentissage : etant donnes des couples de reponses et passages dont elles
sont extraites, annotes par le systeme, il s’agit de decider si les reponses sont Valides et de leur degre de Validite.

3.2 Ordonnancement de réponses

Pour l’ordonnancement de reponses,nous avons adapte l’ approche developpee pour la Validation de reponses four-
nies par des SQR (Grappy et al., 2008) en ajoutant des criteres d’ apprentissage, notarmnent pour tenir compte du

ﬁhttp ://1ucene.apache.org/

A. GRAPPY, B. GRAU, M.—H. FALCO, A.—L. LIGOZAT, I. ROBBA, A. VILNAT

fait que la Validite de la réponse ne soit pas la seule Veriﬁcation a effectuer, et que les reponses n’ont pas eté ﬁltrées
auparavant. Ces criteres permettent d’eValuer la pertinence des passages (critere 2 ci-dessous) et des réponses par
rapport au passage (criteres 3b et 5). Nous avons aussi ameliore la Veriﬁcation du type de la réponse. Globale—
ment, les criteres 1 et 2 traitent du passage aﬁn d’ evaluer une proximité avec la question. Les suivants portent sur
la réponse et permettent notamment de distinguer plusieurs réponses issues d’un meme passage.

1. la proportion des termes de la question présents dans le passage. Quatre calculs sont effectues :
— les mots de la question pris a l’identique ou sous forme de Variantes,

— les mots répartis par catégorie morphosyntaxique (noms propres, noms communs, Verbes, adj ectifs),
— les elements remarqués lors de l’analyse de la question (focus, type speciﬁque et Verbe principal),

— les multi—ter1nes : ensemble de mots consecutifs reconnus comme lies, comme << Prix Nobel» ;

2. rang du passage obtenu lors de la selection des passages ;

3. proximité des termes. Si la réponse est proche en surface des mots de la question, elle a plus de chance d’etre

liee a ceux-ci, et donc Valide. Pour evaluer cette proximité, deux criteres sont utilises :

a) la longueur de la plus longue chaine de mots consécutifs presents dans le passage et constituée des mots de la
question et de la réponse. Deux mots sont dits consecutifs s’ils sont adj acents, séparés par des elements autorisés
(Virgule, determinant...) ou separés par un unique mot,

b) la distance entre la reponse et les mots de la question. La moyenne des distances separant la réponse de chacun
des mots de la question est calculée;

4. redondance Plus la meme réponse est extraite de différents documents, plus elle a de chances d’etre correcte;
5. la catégorie de la question, critere caracterisant la relation de dépendance avec la réponse;

6. la vériﬁcation du type de la réponse. Certaines questions precisent le type de la réponse attendue, comme
<< Quel president succéda a Jacques Chirac ? » qui attend un nom de président en réponse. Ce critere Veriﬁe que
la réponse est du type spéciﬁque attendu par la question. Cette Veriﬁcation se fait aussi par apprentissage sur
differents criteres :

— la frequence d’apparition commune de la réponse et du type dans les documents,

— l’utilisation des entités nommées du systeme de questions reponses RITEL (Rosset et al., 2006) qui permet de

reconnaitre 70 types différents,
— la recherche du type dans les pages Wikipédia associées a la reponse,
— la recherche de structures de phrases indiquant une correspondance e11tre la réponse et le type (<< Albert Einstein
est un physicien ») dans les pages Wikipédia;

La Veriﬁcation du type de la réponse est présentée plus en detail dans (Grappy & Grau, 2010). Une evaluation
consistant a détecter les cas ou une réponse correspond au type a été menée et a obtenu 80 % de bons resul-
tats. Notons cependant que ce critere ne peut etre applique qu’a certaines questions, toutes n’ayant pas un type
spéciﬁque.

L’ apprentissage de la Validation de reponse, utilisant l’ensemble des criteres ci—dessus, est effectué par une com-
binaison d’arbres de decision grace a la methode bagging foumie par WEKA7. Ce classiﬁeur foumit, pour chaque
reponse, un score compris e11tre -1 et 1 indiquant sa conﬁance dans le fait que la reponse soit Valide. La Valeur l
indique que la reponse est correcte et -1 qu’elle est non Valide. Ce score nous permet d’ordonner les réponses.

Aﬁn de constituer la base d’apprentissage, nous avons selectionne les questions factuelles de QA@CLEF05 et
QA@CLEF06 et en avons cherché des réponses dans la collection avec QAVAL. Les reponses correspondant a un
patron de réponse connu sont ensuite Validées de maniere manuelle. La base d’ apprentissage contient 349 réponses
Valides et 698 non Valides.

4 Expérimentation

La collection sur laquelle nous avons evalué notre systeme correspond a un corpus Web constitué par la sociéte
Exalead 8 a partir des requetes d’utilisateurs sur leur moteur de recherche : deux millions de documents ont ainsi

7WEKA : http ://sourceforge.net/projects/weka/
shttpz//www.exalead.com/software/

SELECTION DE REPONSES A DES QUESTIONS DANS UN CORPUS WEB PAR VALIDATION

eté collectes. Un sous ensemble de 500 000 documents a ete sélectionné pour les evaluations.

Pour tester notre systeme, nous avons utilise un ensemble de 147 questions factuelles Venant de la campagne
Quaaro 2010. Aﬁn d’éValuer automatiquement notre SQR, nous avons recueilli un ensemble de reponses correctes
pour chacune des questions. L’ evaluation consiste donc a comparer chaque reponse extraite aux reponses atten-
dues. Nous avons utilise la mesure MRR, sur les cinq premieres réponses, ainsi que la proportion de questions
ayant une bonne réponse en premiere position ou dans les cinq premieres positions pour évaluer les résultats.
Nous avons compare notre methode a une baseline portant sur l’extraction et la Validation de réponses. Elle extrait
les candidats les plus proches des mots de la question dans les cinq premiers passages ordonnés suivant leur rang
apres leur extraction. Le tableau 1 presente les résultats obtenus avec 50 passages retenus sur 150 ramenés par Lu-
cene. 11 405 réponses candidates sont extraites, soit 77 en moyenne par question. Les passages de 300 caracteres
servant a extraire les reponses contiennent 6 phrases en moyenne, alors que sur un corpus d’articles de joumaux
ils en contiennent 3, et possedent moitié moins de Verbes, ce qui rend compte du fait qu’ils sont souvent formés
de suites de syntagmes.

MRR premier rang %(#) cinq premiers rangs %(#)
Quaero 0,43 34% (50) 55% (81)
baseline 0,29 21% (32) 43% (64)

TAB. 1 — Resultats QAVAL

QAVAL surpasse les resultats de la baseline de 48 % ce qui montre l’apport de notre méthode. Aﬁn d’éValuer
uniquement le module de Validation de réponses, nous avons evalué notre systeme sur les 125 questions pour
lesquelles la bonne réponse se trouve dans un passage sélectionné. 65% des questions ont une réponse correcte
parmi les cinq meilleures et 40% en premiere position ce qui est meilleur que les résultats obtenus par (Cui et al.,
2005) qui trouvait 39% de réponses correctes en premiere position sur des passages extraits de joumaux en anglais.

L’ article (Quintard et al., 2010) montre que les autres systemes cherchant une réponse pour les questions factuelles
sur le corpus Quaaro obtiennent un MRR entre 0,284 et 0,54. Nos résultats sont donc de meme ordre de grandeur
que ceux obtenus par les meilleurs systemes.

Aﬁn de tester la robustesse de notre méthode, nous avons applique QAVAL sur une collection constituée d’articles
du journal Le Monde et de dépeches ATS. Pour cela, nous avons pris 128 questions factuelles provenant de la
campagne EQUER. Une nouvelle base d’apprentissage a eté utilisee, dans laquelle les réponses sont extraites de
cet ensemble de documents. Les résultats sont comparables a ceux obtenus sur les documents Web (MRR de 0,47)
ce qui témoigne de la robustesse de la methode et portent notre systeme aux résultats de l’etat de l’art avec ce type
d’ approche. L’ apport de la Validation de réponses est encore important puisqu’il montre une amelioration de 38 %
par rapport a la baseline.

Une analyse des arbres de decision a montre que tous les criteres sont utilises et que les trois criteres les plus
importants sont la fréquence de la reponse, le rang du passage et la proximite des termes. L’ apport de la Veriﬁcation
du type de la réponse a egalement ete mesure en retirant ce critere de l’ensemble de depart. Le MRR passe alors
de 0,43 :21 0,41 ce qui montre bien que ce critere est important d’autant plus qu’il ne s’applique pas a toutes les
questions. Les memes résultats sont obtenus avec la catégorie de la question.

Aﬁn d’eValuer le prétraitement des documents, nous avons applique QAVAL avec trois pretraitements differents :
le premier est Kitten, le second, la baseline, est une extraction complete du contenu textuel et le troisieme applique
le logiciel BoilerPipe (Kohlschiitter et al., 2010) qui utilise des traits textuels de surface. En plus du MRR, nous
avons calculé la proportion de questions ayant au moins un document contenant la réponse peu importe son rang
(cf. tableau 2). Les resultats montrent que notre traitement ameliore les résultats de la baseline puisque le MRR
lui est supérieur de 30%. Nous pouvons aussi Voir que la methode est supérieure aux resultats de BoilerPipe qui
semble moins adapte a l’utilisation d’une collection Web dans le cadre des SQR.

BoilerPipe baseline Kitten
% bons doc. (#) 77 %(114) 82 %(121) 88% (130)
MR 0,28 0,33 0,43

TAB. 2 — Role du prétraitement des documents dans QAVAL

A. GRAPPY, B. GRAU, M.—H. FALCO, A.—L. LIGOZAT, I. ROBBA, A. VILNAT
5 Conclusion

Alors que les SQR développés sur des collections d’ articles de joumaux rencontrent des difﬁcultés sur les docu-
ments Web, le systeme QAVAL9 applique une méthode robuste de validation de réponses permettant d’ordonner
les nombreuses reponses extraites depuis des passages de 300 caracteres environ a partir d’un apprentissage sur
des criteres locaux. Ce type de criteres permet de tenir compte de la dispersion des informations sur plus d’une
phrase dans les passages. QAVAL permet de surpasser la baseline de 48% et obtient des résultats analogues a ceux
obtenus sur des documents issus d’articles de joumaux.

Pour améliorer les résultats, nous envisageons d’ introduire de nouveaux criteres. Une possibilité est d’utiliser un
module de paraphrases sous phrastique permettant de rapprocher les expressions contenues dans la question et
les passages. QAVAL pourrait aussi etre utilise pour traiter des questions booléennes. Dans celles—ci la valeur a
donner est OUI si un des passages trouvés justiﬁe la forme afﬁrmative de la question.

Références

AIT—MOKHTAR S., CHANOD J .—P. & ROUX C. (2002). Robustness beyond shallowness : incremental deep
parsing. Nat. Lang. Eng., 8.

BOUMA G., FAHM1 1., MUR J ., VAN NOORD G., VAN DER PLAs L. & TIEDEMANN J . (2005). Linguistic
Knowledge and question answering. Traitement automatique des langues spe’cial Re’pondre a des questions,
46(3).

CUI H., SUN R., LI K., YEN KAN M. & SENG CHUA T. (2005). Question answering passage retrieval using
dependency relations. In SIGIR 2005.

GRAPPY A. & GRAU B. (2010). Answer type validation in question answering systems. In Recherche d ’Infor—
mations Assiste’ par Ordinateur.

GRAPPY A., LIGOZAT A.—L. & GRAU B. (2008). Evaluation de la reponse d’un systeme de question—réponse
et de sa justiﬁcation. In C0nfe’rence en Recherche d ’Infomations et Applications.

HARABAGIU S. & HICKL A. (2006). Methods for using textual entaihnent in open—domain question answering.
In Proceedings of the 44th annual meeting of the Association for Computational Linguistics.

HERRERA J ., RODRIGO A., PENAS A. & VERDEJO F. (2006). UNED submission to AVE 2006. In Working
Notes for the CLEF 2006 Workshop (AVE).

HICKL A., WILLIAMS J ., BENSLEY J ., ROBERTS K., SH1 Y. & RINK B. (2006). Question answering with
LCC’s CHAUCER at TREC 2006. In Proceedings of the Fifteenth Text REtrieval Conference.

JACQUEMIN C. (1996). A Symbolic and Surgical Acquisition of Terms Through Variation. In Connectionist,
Statistical and Symbolic Approaches to Learning for Natural language Processing, p. 425-438.
KOHLSCHUTTER C., FANKHAUSER P. & NEJDL W. (2010). Boilerplate detection using shallow text features.
In WSDM.

KOUYLEKOV M., NEGRI M., MAGNINI B. & COPPOLA B. (2006). Towards Entaihnent—based Question Ans-
wering : ITC—irst at CLEF 2006. In 7th Workshop of the Cross—Language Evaluation Forum.

LAURENT D., SEGUELA P. & NEGRE S. (2010). Cross lingual question answering using qristal for clef 2006.
Evaluation of Multilingual and Multi—modal Information Retrieval, p. 339-350.

MARTIN A. I., FRANZ M. & ROUKOS S. (2001). Ibm’s statistical question answering system—trec—10. In In
Proceedings of TRECIO.

QUINTARD L., GALIBERT 0., ADDA G., GRAU B., LAURENT D., MORICEAU V., ROSSET S., TANNIER X.
& VILNAT A. (2010). Question Answering on web data : the QA evaluation in Quaero. In Proceedings of the
Seventh conference on International Language Resources and Evaluation.

ROSSET S., GALIBERT 0., ILLOUZ G. & MAX A. (2006). Interaction et recherche d’information : le projet
ritel. Traitement Automatique des Langues (TAL), nume’ro spe’cial Re’pondre a des questions, volume 46 :3, 46(3).
TELLEz—VALERO A., MONTES-Y GOMEZ M., VILLAsENOR—PINEDA L., DEL LENGUAJE L. & PENAS-
PADILLA A. (2010). Towards Multi—Stream Question Answering Using Answer Validation. Informatica, 34.

9Ces travaux ont été en partic réalisés dans lc cadre du programme QUAERO, ﬁnancé par OSEO, agcncc francaisc pour l’innovation.

