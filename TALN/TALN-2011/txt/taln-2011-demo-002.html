<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
"http://www.w3.org/TR/html4/loose.dtd">
<html><head><title>SpatiAnn, un outil pour annoter l&#8217;utilisation de l&#8217;espace dans les corpus vid&#233;o</title>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
</head>
<body>
<div style="page-break-before:always; page-break-after:always"><div><p>TALN 2011, Montpellier, 27 juin - 1er juillet 2011 &#8211; Session d&#233;monstration 
</p>
<p>SpatiAnn, un outil pour annoter l&#8217;utilisation de l&#8217;espace dans les corpus vid&#233;o 
</p>
<p>Annelies Braffort, Laurence Bolot 
</p>
<p>LIMSI-CNRS, Campus d'Orsay Bt. 508, BP 133, F-91403 Orsay cx, France 
annelies.braffort@limsi.fr, laurence.bolot@limsi.fr 
</p>
<p>SpatiAnn (Spatial Annotator) est un logiciel d&#233;velopp&#233; au LIMSI pour l&#8217;annotation de l&#8217;utilisation de l&#8217;espace par les gestes 
dans les corpus vid&#233;o de langue des signes ou multimodaux. Les gestes s&#8217;expriment dans le temps, mais aussi dans l&#8217;espace, 
nomm&#233; &#171; espace de signation &#187; pour la langue des signes et &#171; espace de gestualisation &#187; pour le multimodal. Des &#233;tudes de plus 
en plus nombreuses portent sur l&#8217;utilisation linguistique de cet espace et n&#233;cessitent des annotations. Les logiciels d&#8217;annotation 
actuels (Elan, Anvil, iLex&#8230;) ne permettent pas d&#8217;annoter de mani&#232;re directe des informations de nature tridimensionnelles. 
Actuellement, les annotations sont bas&#233;es sur une segmentation arbitraire de l&#8217;espace, par exemple dans un plan vertical tel que 
le &#171; gesture space &#187; de McNeill (1992), ou sous forme de cubes (Lenseigne, Dalle 2005), ce qui peut limiter les analyses qui 
s&#8217;appuient sur ces annotations. C&#8217;est pourquoi nous d&#233;veloppons actuellement un logiciel qui permet d&#8217;annoter directement en 
3d. Il se pr&#233;sente sous la forme d&#8217;un cube, o&#249; la vid&#233;o est projet&#233;e sur l&#8217;une des faces. On peut aussi projeter plusieurs vid&#233;os, en 
fonction des diff&#233;rentes vues dont on dispose, ce qui permet d&#8217;annoter dans un contexte d&#8217;interaction. La figure 1 montre un 
exemple avec le corpus DEGELS1 (Boutora, Braffort 2011) pour lequel on dispose de trois vues. L&#8217;utilisateur peut manipuler le 
cube afin d&#8217;annoter &#171; devant &#187; la vid&#233;o de son choix. Cette annotation peut prendre n&#8217;importe quelle forme, le vocabulaire 
contr&#244;l&#233; et sa forme graphique sont libres. Pour nos &#233;tudes, nous utilisons un vocabulaire contr&#244;l&#233; constitu&#233; d&#8217;un ensemble fini 
de formes g&#233;om&#233;triques simples (point, segment, plan, tore, volume&#8230;), qui cat&#233;gorise la trace du geste dans l&#8217;espace. Par 
exemple la trace d&#8217;un pointage associ&#233; &#224; un mouvement circulaire (description d&#8217;un rond-point) est cat&#233;goris&#233;e par un tore plac&#233; 
&#224; l&#8217;endroit point&#233; (figure 1). Ces traces sont, comme toutes les autres annotations, synchronis&#233;es avec la vid&#233;o. Elles peuvent &#234;tre 
plus ou moins actives selon leur utilisation dans le discours. Une trace r&#233;alis&#233;e pr&#233;c&#233;demment dans la vid&#233;o s&#8217;att&#233;nue au cours 
du temps mais peut &#234;tre r&#233;activ&#233;e par un pointage anaphorique. Cela se visualise en faisant varier sa transparence. 
</p>
<p> 
</p>
<p>Figure 1 : Utilisation de SpatiAnn avec un corpus constitu&#233; de trois vues 
</p>
<p>Le logiciel est d&#233;velopp&#233; actuellement sous la forme d&#8217;un prototype autonome en vue d&#8217;&#233;valuer et d&#8217;am&#233;liorer son ergonomie. 
Dans un deuxi&#232;me temps, ce prototype sera int&#233;gr&#233; dans un syst&#232;me distribu&#233; permettant son emploi avec le logiciel 
d&#8217;annotation AnCoLin d&#233;velopp&#233; dans le cadre du projet europ&#233;en Dicta-Sign (Collet, Gonzalez, Milachon 2010). Dans un 
troisi&#232;me temps, on envisage d&#8217;en d&#233;velopper une version sous forme d&#8217;un plugin utilisable avec certains logiciels courants, tels 
qu&#8217;Anvil et Elan, afin de le rendre accessible &#224; une plus grande partie de la communaut&#233; scientifique concern&#233;e. 
</p>
<p>MCNEILL, D. (1992). Hand and Mind: What gestures reveal about thought. Chicago: Univ. of Chicago Press. 
</p>
<p>LENSEIGNE, B., DALLE, P. (2005). A Signing space model for the interpretation of sign language interactions. Actes de Sign 
Language Linguistics and the Application of Information Technology to Sign Languages. 
</p>
<p>BOUTORA L., BRAFFORT A. (2011). DEGELS1. oai:crdo.fr:crdo000767. 
</p>
<p>COLLET, C., Gonzalez M., Milachon F. (2010). Distributed system architecture for assisted annotation of video corpora. Actes de 
International workshop on the Representation and Processing of Sign Languages : Corpora and Sign Language Technologies 
(LREC 2010). </p>

</div></div>
</body></html>