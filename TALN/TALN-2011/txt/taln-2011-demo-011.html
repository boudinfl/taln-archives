<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
"http://www.w3.org/TR/html4/loose.dtd">
<html><head><title>Babouk &#8211; exploration orient&#233;e du web pour la constitution de corpus et de terminologies</title>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
</head>
<body>
<div style="page-break-before:always; page-break-after:always"><div><p>BABOUK &#8211; EXPLORATION ORIENTEE DU WEB POUR LA CONSTITUTION DE CORPUS ET DE TERMINOLOGIES 
</p>
<p>Babouk &#8211; exploration orient&#233;e du web pour la constitution  
de corpus et de terminologies 
</p>
<p>Cl&#233;ment de Groc
1,2
</p>
<p>   Javier Couto
1,3
</p>
<p>   Helena Blancafort
1,4
</p>
<p>  Claude de Loupy
1
 
</p>
<p>(1) Syllabs, 15 rue Jean-Baptiste Berlier, 75013 Paris 
(2) Univ. Paris Sud et LIMSI-CNRS, F-91405 Orsay 
</p>
<p>(3) MoDyCo, UMR 7114, CNRS-Universit&#233; Paris Ouest Nanterre, La D&#233;fense 
(4) Universitat Pompeu Fabra&#10;Roc Boronat,138, 08018 Barcelona, Spain 
</p>
<p>{cdegroc, jcouto, blancafort, loupy}@syllabs.com 
</p>
<p>Babouk est un crawler orient&#233; (Chakrabarti et al., 1999) : son objectif est le rapatriement efficace de 
documents pertinents pour un domaine d&#233;fini. Comparativement au crawling traditionnel, le crawling 
orient&#233; permet un acc&#232;s rapide &#224; des donn&#233;es sp&#233;cialis&#233;es tout en &#233;vitant le co&#251;t prohibitif d&#8217;un parcours en 
largeur du web. L&#8217;exploitation du web comme source de donn&#233;es linguistiques a permis de cr&#233;er de 
nombreux corpus g&#233;n&#233;ralistes et sp&#233;cialis&#233;s par le biais de requ&#234;tes &#224; un moteur de recherche 
(Baroni, Bernardini, 2004) ou d&#8217;un crawl du web (Baroni &amp; Ueyama, 2006). Babouk ne requiert qu&#8217;un petit 
ensemble de termes ou URLs amorces en entr&#233;e. Le reste de la proc&#233;dure est automatique. L&#8217;utilisateur peut 
r&#233;gler le crawler par un ensemble de param&#232;tres et reprendre la main sur la proc&#233;dure &#224; tout moment. 
</p>
<p>Babouk doit trouver un maximum de documents pertinents en t&#233;l&#233;chargeant le minimum de pages. Le 
crawler s&#8217;appuie sur un cat&#233;goriseur qui filtre les documents non pertinents et ordonne par pertinence les 
pages &#224; t&#233;l&#233;charger. Le cat&#233;goriseur est bas&#233; sur un lexique pond&#233;r&#233; construit durant la premi&#232;re it&#233;ration du 
crawling : une extension de l&#8217;entr&#233;e utilisateur est effectu&#233;e en utilisant la proc&#233;dure BootCaT 
(Baroni, Bernardini, 2004). Le lexique est ensuite pond&#233;r&#233; &#224; l&#8217;aide d&#8217;une mesure de &#171; repr&#233;sentativit&#233; &#187; 
s&#8217;appuyant sur le web. Une phase de calibration automatique permet de d&#233;terminer un seuil pour la 
cat&#233;gorisation. Pour guider le crawler en priorit&#233; vers les pages les plus pertinentes, le score fourni par le 
cat&#233;goriseur est utilis&#233; de mani&#232;re analogue au crit&#232;re OPIC (Abiteboul et al., 2003). 
</p>
<p>Plusieurs crit&#232;res d&#8217;arr&#234;t on &#233;t&#233; impl&#233;ment&#233;s tels qu&#8217;un nombre maximal de tokens ou de documents &#224; 
t&#233;l&#233;charger, une profondeur ou une dur&#233;e de crawl maximale. Plusieurs filtres sont appliqu&#233;s dans le but 
d&#8217;am&#233;liorer la qualit&#233; des corpus constitu&#233;s. L&#8217;utilisateur peut ainsi choisir de ne conserver que des pages 
d&#8217;une certaine taille  ou appartenant &#224; un certain format de fichier (parmi Microsoft Office, Adobe PDF, ou 
HTML). Il peut &#233;galement limiter le crawl &#224; certains domaines/sites ou, au contraire, les filtrer. 
</p>
<p>Babouk est bas&#233; sur Nutch  et distribu&#233; sur une grappe de machines (optionnellement sur le &#171; cloud &#187;), ce 
qui assure un passage &#224; l&#8217;&#233;chelle en termes de puissance de calcul n&#233;cessaire pour la r&#233;alisation de 
nombreux crawls simultan&#233;ment. Enfin, les documents et m&#233;ta-informations r&#233;sultants du crawling peuvent 
&#234;tre stock&#233;s dans une base de donn&#233;es distribu&#233;e assurant, encore une fois, la scalabilit&#233; du syst&#232;me. Les 
utilisateurs peuvent configurer et lancer leurs crawls &#224; partir d&#8217;une interface web dynamique. Cette derni&#232;re 
offre &#233;galement un suivi (logs) du crawl en temps r&#233;el. 
</p>
<p>ABITEBOUL M., PREDA M., COBENA G. (2003). Adaptive on-line page importance computation. Actes de 
12th international conference on the World Wide Web &#8211; WWW. 280-290. 
</p>
<p>BARONI M., BERNARDINI S. (2004). BootCaT : Bootstrapping Corpora and Terms from the Web. Actes de 
4th international conference on language resources and evaluation &#8211; LREC. 1313-1316. 
</p>
<p>BARONI M., UEYAMA M. (2006). Building general- and special-purpose corpora by Web crawling. Actes de 
13th NIJL International Symposium, Language Corpora: Their Compilation and Application. 31-40. 
</p>
<p>CHAKRABARTI S., DEN BERG M.V., DOM B. (1999). Focused crawling : a new approach to topic-specific 
Web resource discovery. Actes de Computer Networks, vol. 31. 1623-1640. </p>

</div></div>
</body></html>