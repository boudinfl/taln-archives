BABOUK — EXPLORATION ORIENTEE DU WEB POUR LA CONSTITUTION DE CORPUS ET DE TERMINOLOGIES

Babouk — exploration orientee du web pour la constitution
de corpus et de terminologies

Clement de Groc1’2 Javier Couto1’3 Helena B1ancafort1’4 Claude de Loupyl

(1) Syllabs, 15 rue Jean—Baptiste Berlier, 75013 Paris
(2) Univ. Paris Sud et LIMSI-CNRS, F-91405 Orsay
(3) MoDyCo, UMR 7114, CNRS—UniVersité Paris Ouest Nanterre, La Defense
(4) Universitat Pompeu Fabra Roc Boronat,138, 08018 Barcelona, Spain
{cdegroc, jcouto, blancafort, loupy} @ sy11abs.com

Babouk est un crawler oriente (Chakrabarti et al., 1999) : son objectif est le rapatriement efﬁcace de
documents pertinents pour un domaine defini. Comparativement au crawling traditionnel, le crawling
oriente perrnet un acces rapide a des donnees specialisees tout en evitant le coﬁt prohibitif d’un parcours en
largeur du web. L’exploitation du web comme source de donnees linguistiques a perrnis de creer de
nombreux corpus generalistes et specialises par le biais de requetes a un moteur de recherche
(Baroni, Bemardini, 2004) ou d’un crawl du web (Baroni & Ueyama, 2006). Babouk ne requiert qu’un petit
ensemble de terrnes ou URLs amorces en entree. Le reste de la procedure est automatique. L’uIilisateur peut
regler le crawler par un ensemble de parametres et reprendre la main sur la procedure a tout moment.

Babouk doit trouver un maximum de documents pertinents en telechargeant le minimum de pages. Le
crawler s’appuie sur un categoriseur qui filtre les documents non pertinents et ordonne par pertinence les
pages a telecharger. Le categoriseur est base sur un lexique pondere construit durant la premiere iteration du
crawling : une extension de l’entree utilisateur est effectuee en utilisant la procedure BootCaT
(Baroni, Bemardini, 2004). Le lexique est ensuite pondere a l’aide d’une mesure de << representativite »
s’appuyant sur le web. Une phase de calibration automatique perrnet de determiner un seuil pour la
categorisation. Pour guider le crawler en priorite vers les pages les plus perlinentes, le score fourni par le
categoriseur est utilise de maniere analogue au critere OPIC (Abiteboul et al., 2003).

Plusieurs criteres d’arret on ete implementes tels qu’un nombre maximal de tokens ou de documents a
telecharger, une profondeur ou une duree de crawl maximale. Plusieurs ﬁltres sont appliques dans le but
d’ameliorer la qualite des corpus constitues. L’uIilisateur peut ainsi choisir de ne conserver que des pages
d’une certaine taille ou appartenant a un certain fomiat de fichier (parmi Microsoft Office, Adobe PDF, ou
HTML). 11 peut egalement limiter le crawl a certains domaines/sites ou, au contraire, les filtrer.

Babouk est base sur Nutch et distribue sur une grappe de machines (optionnellement sur le << cloud »), ce
qui assure un passage a l’echelle en terrnes de puissance de calcul necessaire pour la realisation de
nombreux crawls simultanement. Enfin, les documents et meta-infomiations resultants du crawling peuvent
etre stockes dans une base de donnees distribuee assurant, encore une fois, la scalabilité du systeme. Les
utilisateurs peuvent configurer et lancer leurs crawls a parlir d’une interface web dynamique. Cette derniere
offre egalement un suivi (logs) du crawl en temps reel.

ABITEBOUL M., PREDA M., COBENA G. (2003). Adaptive on-line page importance computation. Actes de
12th international conference on the World Wide Web — WWW. 280-290.

BARONI M., BERNARDINI S. (2004). BootCaT : Bootstrapping Corpora and Terms from the Web. Actes de
4th international conference on language resources and evaluation — LREC. 1313-1316.

BARONI M., UEYAMA M. (2006). Building general- and special-purpose corpora by Web crawling. Actes de
13th NIJL International Symposium, Language Corpora: Their Compilation and Application. 31-40.

CHAKRABARTI S., DEN BERG M.V., DOM B. (1999). Focused crawling : a new approach to topic-specific
Web resource discovery. Actes de Computer Networks, vol. 31. 1623-1640.

