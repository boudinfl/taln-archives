<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
"http://www.w3.org/TR/html4/loose.dtd">
<html><head><title>D&#233;sambigu&#239;sation lexicale par propagation de mesures s&#233;mantiques locales par algorithmes &#224; colonies de fourmis</title>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
</head>
<body>
<div style="page-break-before:always; page-break-after:always"><div><p>TALN 2011, Montpellier, 27 juin &#8211;1er juillet 2011
</p>
<p>D&#233;sambigu&#239;sation lexicale par propagation de mesures s&#233;mantiques
locales par algorithmes &#224; colonies de fourmis
</p>
<p>Didier Schwab, J&#233;r&#244;me Goulian, Nathan Guillaume
LIG-GETALP (Laboratoire d&#8217;Informatique de Grenoble, Groupe d&#8217;&#201;tude pour la
</p>
<p>Traduction/le Traitement Automatique des Langues et de la Parole)
Universit&#233; Pierre Mend&#232;s France, Grenoble 2
</p>
<p>{didier.schwab, jerome.goulian}@imag.fr
</p>
<p>R&#233;sum&#233;. Effectuer une t&#226;che de d&#233;sambigu&#239;sation lexicale peut permettre d&#8217;am&#233;liorer de nombreuses ap-
plications du traitement automatique des langues comme l&#8217;extraction d&#8217;informations multilingues, ou la traduction
automatique. Sch&#233;matiquement, il s&#8217;agit de choisir quel est le sens le plus appropri&#233; pour chaque mot d&#8217;un texte.
Une des approches classiques consiste &#224; estimer la proximit&#233; s&#233;mantique qui existe entre deux sens de mots puis
de l&#8217;&#233;tendre &#224; l&#8217;ensemble du texte. La m&#233;thode la plus directe donne un score &#224; toutes les paires de sens de
mots puis choisit la cha&#238;ne de sens qui a le meilleur score. La complexit&#233; de cet algorithme est exponentielle et
le contexte qu&#8217;il est calculatoirement possible d&#8217;utiliser s&#8217;en trouve r&#233;duit. Il ne s&#8217;agit donc pas d&#8217;une solution
viable. Dans cet article, nous nous int&#233;ressons &#224; une autre m&#233;thode, l&#8217;adaptation d&#8217;un algorithme &#224; colonies de
fourmis. Nous pr&#233;sentons ses caract&#233;ristiques et montrons qu&#8217;il permet de propager &#224; un niveau global les r&#233;sultats
des algorithmes locaux et de tenir compte d&#8217;un contexte plus long et plus appropri&#233; en un temps raisonnable.
</p>
<p>Abstract. Word sense disambiguation can lead to significant improvement in many Natural Language Pro-
cessing applications as Machine Translation or Multilingual Information Retrieval. Basically, the aim is to choose
for each word in a text its best sense. One of the most popular method estimates local semantic relatedness bet-
ween two word senses and then extends it to the whole text. The most direct method computes a rough score for
every pair of word senses and chooses the lexical chain that has the best score. The complexity of this algorithm
is exponential and the context that it is computationally possible to use is reduced. Brute force is therefore not a
viable solution. In this paper, we focus on another method : the adaptation of an ant colony algorithm. We present
its features and show that it can spread at a global level the results of local algorithms and consider a longer and
more appropriate context in a reasonable time.
</p>
<p>Mots-cl&#233;s : D&#233;sambigu&#239;sation lexicale, Algorithmes &#224; colonies de fourmis, Mesures s&#233;mantiques.
</p>
<p>Keywords: Lexical Disambiguation, Ant colony algorithms, Semantic relatedness.
</p>
<p>1 Introduction
</p>
<p>Effectuer une t&#226;che de d&#233;sambigu&#239;sation lexicale peut permettre d&#8217;am&#233;liorer de nombreuses applications du traite-
ment automatique des langues comme l&#8217;extraction d&#8217;informations multilingues, le r&#233;sum&#233; automatique ou encore
la traduction automatique. Sch&#233;matiquement, il s&#8217;agit de choisir quel est le sens le plus appropri&#233; pour chaque mot
d&#8217;un texte dans un inventaire pr&#233;-d&#233;fini. Par exemple, dans &quot;La souris mange le fromage.&quot;, l&#8217;animal devrait &#234;tre</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>DIDIER SCHWAB, J&#201;R&#212;ME GOULIAN, NATHAN GUILLAUME
</p>
<p>pr&#233;f&#233;r&#233; au dispositif &#233;lectronique. De nombreux travaux existent sur le sujet, que l&#8217;on s&#233;pare habituellement en
approches supervis&#233;es et non-supervis&#233;es. Les premi&#232;res utilisent des apprentissages r&#233;alis&#233;s gr&#226;ce &#224; des corpus
manuellement annot&#233;s, les secondes n&#8217;utilisent pas de telles donn&#233;es. Une cat&#233;gorie interm&#233;diaire, constitu&#233;e des
approches semi-supervis&#233;es, utilise quelques donn&#233;es annot&#233;es comme, par exemple, un sens par d&#233;faut issu d&#8217;un
corpus annot&#233; lorsque l&#8217;algorithme principal &#233;choue (Navigli &amp; Lapata, 2010). Le lecteur pourra consulter (Ide &amp;
V&#233;ronis, 1998) pour les travaux ant&#233;rieurs &#224; 1998 et (Agirre &amp; Edmonds, 2006) ou (Navigli, 2009) pour un &#233;tat
de l&#8217;art complet.
</p>
<p>La cr&#233;ation de donn&#233;es annot&#233;es est une op&#233;ration compliqu&#233;e puisqu&#8217;elle n&#233;cessite une importante main d&#8217;&#339;uvre
et qu&#8217;elle doit &#234;tre r&#233;alis&#233;e pour chaque inventaire de sens, pour chaque langue et m&#234;me pour chaque domaine sp&#233;-
cifique (sport, finance, . . .). Cette constatation, que nous partageons avec (Navigli &amp; Lapata, 2010), nous conduit
&#224; nous int&#233;resser plus particuli&#232;rement &#224; des approches non-supervis&#233;es. Une de ces approches classiques consiste
&#224; estimer la proximit&#233; s&#233;mantique qui existe entre deux sens de mots puis de l&#8217;&#233;tendre &#224; l&#8217;ensemble du texte. En
d&#8217;autres termes, il s&#8217;agit de donner des scores locaux et de les propager au niveau global (phrase, paragraphe,
texte, . . .). La m&#233;thode la plus directe, utilis&#233;e par exemple par (Pedersen et al., 2005) utilise un algorithme brutal
qui donne un score &#224; toutes les paires de sens de mots puis choisit la cha&#238;ne de sens qui a le meilleur score. La
complexit&#233; de cet algorithme est exponentielle et le contexte qu&#8217;il est calculatoirement possible d&#8217;utiliser s&#8217;en
trouve r&#233;duit. Ainsi, alors qu&#8217;une analyse au niveau de la phrase n&#8217;est d&#233;j&#224; pas toujours possible, un contexte
linguistiquement plus pertinent comme, par exemple, le paragraphe l&#8217;est encore moins.
</p>
<p>Les applications que nous visons doivent pouvoir &#234;tre utilis&#233;es en temps r&#233;el. Lorsque l&#8217;on recherche une image
et encore plus lorsque l&#8217;on appelle quelqu&#8217;un qui parle une autre langue au t&#233;l&#233;phone, les r&#233;ponses doivent &#234;tre
imm&#233;diates. Il ne s&#8217;agit donc pas une solution viable et nous &#233;tudions d&#8217;autres m&#233;thodes.
</p>
<p>Dans cet article, nous nous int&#233;ressons &#224; la propagation de mesures de proximit&#233; s&#233;mantique locales gr&#226;ce &#224; une
adaptation d&#8217;un algorithme &#224; colonies de fourmis. Nous pr&#233;sentons dans un premier temps les mesures locales que
nous utilisons puis quelques unes des caract&#233;ristiques de notre algorithme de propagation. Enfin, &#224; titre d&#8217;exemple,
nous &#233;valuons notre approche sur la t&#226;che gros grain de la campagne d&#8217;&#233;valuation Semeval 2007 (Navigli et al.,
2007). Nous comparons en particulier notre algorithme de propagation &#224; l&#8217;algorithme exhaustif classique et mon-
trons qu&#8217;il permet d&#8217;obtenir efficacement une meilleure F-mesure.
</p>
<p>2 Algorithmes locaux
</p>
<p>2.1 Mesures de proximit&#233; s&#233;mantique
</p>
<p>Ces m&#233;thodes consistent &#224; donner un score cens&#233; refl&#233;ter la proximit&#233; des objets linguistiques (g&#233;n&#233;ralement des
mots ou des sens de mots) compar&#233;s. Ces scores peuvent &#234;tre des similarit&#233;s, donc avoir une valeur entre 0 et
1, des distances, et donc respecter leurs trois propri&#233;t&#233;s (s&#233;paration, sym&#233;trie et in&#233;galit&#233; triangulaire) ou plus
g&#233;n&#233;ralement, &#234;tre une valeur positive non born&#233;e.
</p>
<p>Parmi elles, on peut citer Hirst &amp; Saint-Honge bas&#233;e sur la distance en terme de graphe entre deux sens dans un
r&#233;seau lexical ; Rada et al. ainsi que Leacock and Chodorow similaires &#224; la pr&#233;c&#233;dente mais ne consid&#233;rant que
les liens de type hyperonymie ; les mesures ou distances entre vecteurs (LSA (Deerwester et al., 1990), vecteurs
conceptuels (Schwab, 2005)). On pourra consulter (Pedersen et al., 2005), (Cramer et al., 2010) ou (Navigli, 2009)
pour un panorama plus complet.
</p>
<p>En d&#233;sambigu&#239;sation lexicale, ces m&#233;thodes sont utilis&#233;es de fa&#231;on locale entre deux sens de mots, et sont ensuite</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>PROPAGATION DE MESURES S&#201;MANTIQUES LOCALES PAR ALGORITHMES &#192; COLONIES DE FOURMIS
</p>
<p>appliqu&#233;es &#224; un niveau global. Dans cet article, nous nous concentrons sur l&#8217;algorithme global et, &#224; des fins de
comparaison, nous pr&#233;sentons deux algorithmes locaux bas&#233;s sur l&#8217;algorithme de Lesk.
</p>
<p>2.2 Algorithmes locaux de cette exp&#233;rience
</p>
<p>2.2.1 Des algorithmes inspir&#233;s par Lesk
</p>
<p>Nous utilisons dans cet article deux variantes de l&#8217;algorithme de Lesk (Lesk, 1986). Propos&#233;es il y a plus de 25
ans, il se caract&#233;rise par sa simplicit&#233;. Il ne n&#233;cessite qu&#8217;un dictionnaire et aucun apprentissage. Le score donn&#233;
&#224; une paire de sens est le nombre de mots &#8211; ici simplement les suites de caract&#232;res s&#233;par&#233;es par des espaces &#8211; en
commun dans leur d&#233;finition, sans tenir compte ni de leur ordre, ni de sous-s&#233;quences communes (approche sac de
mots), ni d&#8217;informations morphologiques ou syntaxiques. Les variantes de cet algorithme sont encore aujourd&#8217;hui
parmi les meilleures sur l&#8217;anglais (Ponzetto &amp; Navigli, 2010). Ce premier algorithme local est nomm&#233; dans la
suite Lesk.
</p>
<p>Nous utilisons WordNet (Fellbaum, 1998), une base lexicale pour l&#8217;anglais, dans laquelle les sens de mots (les
synsets) sont reli&#233;s par des relations (hyperonymie, hyponymie, antonymie, etc.). Notre second algorithme local
exploite ces liens. Au lieu d&#8217;utiliser uniquement la d&#233;finition d&#8217;un sens, elle utilise &#233;galement la d&#233;finition des
diff&#233;rents sens qui lui sont li&#233;s. Cette id&#233;e est similaire &#224; celle de (Banerjee &amp; Pedersen, 2002) 1. Ce second
algorithme local est nomm&#233; dans la suite Lesk &#233;tendu.
</p>
<p>2.2.2 Efficacit&#233; algorithmique
</p>
<p>L&#8217;algorithme de base pour comparer le nombre de mots communs &#224; deux d&#233;finitions a une complexit&#233; en O(n &#215;
m) avec n et m, les longueurs en mots des d&#233;finitions. De plus, la comparaison de cha&#238;nes de caract&#232;res est
une op&#233;ration relativement ch&#232;re. On pourrait penser qu&#8217;il suffirait de pr&#233;calculer la matrice de similarit&#233;s avec
l&#8217;ensemble des d&#233;finitions. Cette id&#233;e est utopique vu la taille que peuvent atteindre les dictionnaires (jusqu&#8217;&#224;
plusieurs millions de d&#233;finitions) 2 mais aussi parce qu&#8217;on a toujours besoin de faire des calculs sur de nouvelles
donn&#233;es puisque (1) les donn&#233;es et les sens peuvent &#233;voluer au cours du temps comme dans (Schwab, 2005), (2)
notre algorithme de propagation utilise des pseudo-d&#233;finitions cr&#233;es &#224; la vol&#233;e (voir partie 4.2.2).
</p>
<p>Nous avons am&#233;lior&#233; ce calcul en utilisant un pr&#233;traitement qui se d&#233;roule en deux &#233;tapes. Dans la premi&#232;re,
nous affectons &#224; chacun des mots trouv&#233;s dans le dictionnaire un nombre entier tandis que, dans la seconde, nous
convertissons chacune des d&#233;finitions en un vecteur de nombres correspondant aux mots qu&#8217;elle contient, tri&#233;s du
plus petit au plus grand. Nous appelons ces vecteurs, vecteurs de d&#233;finitions.
</p>
<p>Par exemple, si notre premi&#232;re &#233;tape a donn&#233; &#8618;&#8618;kind&#8617;&#8617;= 1 ; &#8618;&#8618;of&#8617;&#8617;= 2 ; &#8618;&#8618;evergreen&#8617;&#8617;= 3 ; &#8618;&#8618;tree&#8617;&#8617;= 4 ; &#8618;&#8618;with&#8617;&#8617;= 5
&#8618;&#8618;needle-shaped&#8617;&#8617;= 6 ; &#8618;&#8618;leaves&#8617;&#8617;= 7 ; &#8618;&#8618;fruit&#8617;&#8617;= 8 ; &#8618;&#8618;certain&#8617;&#8617;= 9 avec la d&#233;finition A, &quot;kind of evergreen tree with
needle-shaped leaves&quot;, nous obtenons le vecteur [1, 2, 3, 4, 5, 6, 7] et avec B, &quot;fruit of certain evergreen tree&quot;,
nous obtenons [2, 3, 4, 8, 9].
</p>
<p>Cette conversion a deux avantages : (1) la comparaison de nombres est bien plus efficace que la comparaison
de cha&#238;nes de caract&#232;res, (2) ordonner ces nombres permet d&#8217;&#233;viter des comparaisons inutiles et de gagner en
</p>
<p>1. (Banerjee &amp; Pedersen, 2002) introduit &#233;galement une notion de sous-s&#233;quence identique dans les d&#233;finitions. Nous n&#8217;avons pas encore
test&#233; cette variante dont la complexit&#233; algorithmique est nettement sup&#233;rieure &#224; celle de notre algorithme.
</p>
<p>2. Une forme de cache pourrait en partie r&#233;gler ce probl&#232;me.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>DIDIER SCHWAB, J&#201;R&#212;ME GOULIAN, NATHAN GUILLAUME
</p>
<p>efficacit&#233;. Ainsi, avec ce pr&#233;traitement, la complexit&#233; passe de O(n &#215; m) &#224; O(n) o&#249; n et m (n &#8805; m) sont les
longueurs (en nombre de mots) des d&#233;finitions.
</p>
<p>Pour les d&#233;finitions A et B, calculer cette m&#234;me proximit&#233; s&#233;mantique avec l&#8217;algorithme sur les d&#233;finitions brutes
se fait en 7 &#215; 5 = 35 op&#233;rations (qui plus est sur des cha&#238;nes de caract&#232;res) tandis que si les d&#233;finitions sont
converties en vecteurs, nous n&#8217;avons plus que 7 op&#233;rations.
</p>
<p>3 Algorithmes globaux
</p>
<p>L&#8217;algorithme global est l&#8217;algorithme qui va permettre de propager les r&#233;sultats d&#8217;un ou plusieurs algorithmes
locaux &#224; l&#8217;ensemble du texte afin de pouvoir en d&#233;duire un sens pour chaque mot. La m&#233;thode la plus directe
est la recherche exhaustive utilis&#233;e par exemple dans (Banerjee &amp; Pedersen, 2002). Il s&#8217;agit de consid&#233;rer les
combinaisons de l&#8217;ensemble des sens des mots dans le m&#234;me contexte (fen&#234;tre de mots, phrase, texte, etc.), de
donner un score &#224; chacune de ces combinaisons et de choisir celle qui a le meilleur score. Le principal probl&#232;me
de cette m&#233;thode est la rapide explosion combinatoire qu&#8217;elle engendre. Consid&#233;rons la phrase suivante tir&#233;e du
corpus d&#8217;&#233;valuation que nous utilisons dans la partie 5, &quot;The pictures they painted were flat, not round as a figure
should be, and very often the feet did not look as if they were standing on the ground at all, but pointed downwards
as if they were hanging in the air.&quot;, &#8618;picture&#8617; a 9 sens, &#8618;paint&#8617; 4, &#8618;be&#8617; 13, &#8618;flat&#8617; 17, &#8618;figure&#8617; 13, &#8618;very&#8617; 2, &#8618;often&#8617; 2, &#8618;foot&#8617;
11, &#8618;look&#8617; 10, &#8618;stand&#8617; 12, &#8618;ground&#8617; 11, &#8618;at all&#8617; 1, &#8618;point&#8617; 13, &#8618;downwards&#8617; 1, &#8618;hang&#8617; 15 et &#8618;air&#8617; 9 sens, il y a alors 137 051
946 345 600 combinaisons de sens possibles &#224; analyser. Ce nombre est comparable &#224; la quantit&#233; d&#8217;op&#233;rations (et
le calcul d&#8217;une combinaison n&#233;cessite des dizaines voire des centaines d&#8217;op&#233;rations) que peuvent th&#233;oriquement
effectuer 3300 processeurs Core i7-990X (2,43GHz, 6 c&#339;urs, 12 fils d&#8217;ex&#233;cutions) sortis par Intel au premier
trimestre 2011 en une seconde. Le calcul exhaustif est donc tr&#232;s compliqu&#233; &#224; r&#233;aliser dans des conditions r&#233;elles
et, surtout, rend impossible l&#8217;utilisation d&#8217;un contexte d&#8217;analyse plus important.
</p>
<p>Pour contourner ce probl&#232;me, plusieurs solutions ont &#233;t&#233; propos&#233;es. Par exemple, des approches utilisant un corpus
pour diminuer le nombre de combinaisons &#224; examiner comme la recherche des cha&#238;nes lexicales compatibles (Gale
et al., 1992; Vasilescu et al., 2004) ou encore des approches issues de l&#8217;intelligence artificielle comme le recuit
simul&#233; (Cowie et al., 1992) ou les algorithmes g&#233;n&#233;tiques (Gelbukh et al., 2003).
</p>
<p>Ces m&#233;thodes ont en commun de ne pas permettre l&#8217;exploitation de fa&#231;on directe et simple d&#8217;une structure lin-
guistique sous forme de graphe que ce soit une analyse morphologique ou une analyse syntaxique. Nous utilisons,
au contraire, une m&#233;thode &#224; colonies de fourmis pour l&#8217;analyse s&#233;mantique inspir&#233;e de (Schwab &amp; Lafourcade,
2007) afin de pouvoir &#224; terme utiliser de telles structures 3.
</p>
<p>4 Notre algorithme global : un algorithme &#224; colonies de fourmis
</p>
<p>4.1 Les algorithmes &#224; colonies de fourmis
</p>
<p>Les algorithmes &#224; fourmis ont pour origine la biologie et les observations r&#233;alis&#233;es sur le comportement social des
fourmis. En effet, ces insectes ont collectivement la capacit&#233; de trouver le plus court chemin entre leur fourmili&#232;re
et une source d&#8217;&#233;nergie. Il a pu &#234;tre d&#233;montr&#233; que la coop&#233;ration au sein de la colonie est auto-organis&#233;e et
r&#233;sulte d&#8217;interactions entre individus autonomes. Ces interactions, souvent tr&#232;s simples, permettent &#224; la colonie
</p>
<p>3. Dans un premier temps, nous utiliserons ici une structure linguistique extr&#234;mement simpl(ist)e.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>PROPAGATION DE MESURES S&#201;MANTIQUES LOCALES PAR ALGORITHMES &#192; COLONIES DE FOURMIS
</p>
<p>de r&#233;soudre des probl&#232;mes compliqu&#233;s. Ce ph&#233;nom&#232;ne est appel&#233; intelligence en essaim (Bonabeau &amp; Th&#233;raulaz,
2000). Il est de plus en plus utilis&#233; en informatique o&#249; des syst&#232;mes de contr&#244;le centralis&#233;s gagnent souvent &#224; &#234;tre
remplac&#233;s par d&#8217;autres, fond&#233;s sur les interactions d&#8217;&#233;l&#233;ments simples.
</p>
<p>En 1989, Jean-Louis Deneubourg &#233;tudie le comportement des fourmis biologiques dans le but de comprendre la
m&#233;thode avec laquelle elles choisissent le plus court chemin et le retrouvent en cas d&#8217;obstacle. Il &#233;labore ainsi le
mod&#232;le stochastique dit de Deneubourg (Deneubourg et al., 1989), conforme &#224; ce qui est observ&#233; statistiquement
sur les fourmis r&#233;elles quant &#224; leur partage entre les chemins. Ce mod&#232;le stochastique est &#224; l&#8217;origine des travaux
sur les algorithmes &#224; fourmis.
</p>
<p>Le concept principal de l&#8217;intelligence en essaim est la stygmergie, c.-&#224;-d. l&#8217;interaction entre agents par modifi-
cation de l&#8217;environnement. Une des premi&#232;res m&#233;thodes que l&#8217;on peut apparenter aux algorithmes &#224; fourmis est
l&#8217;&#233;cor&#233;solution qui a montr&#233; la puissance d&#8217;une heuristique de r&#233;solution collective bas&#233;e sur la perception locale,
&#233;vitant tout parcours explicite de graphe d&#8217;&#233;tats (Drogoul, 1993).
</p>
<p>En 1992, Marco Dorigo et Luca Maria Gambardella con&#231;oivent le premier algorithme bas&#233; sur ce paradigme pour
le c&#233;l&#232;bre probl&#232;me combinatoire du voyageur de commerce (Dorigo &amp; Gambardella, 1997). Dans les algorithmes
&#224; base de fourmis artificielles, l&#8217;environnement est g&#233;n&#233;ralement repr&#233;sent&#233; par un graphe et les fourmis virtuelles
utilisent l&#8217;information accumul&#233;e sous la forme de chemins de ph&#233;romone d&#233;pos&#233;e sur les arcs du graphe. De
fa&#231;on simple, une fourmi se contente de suivre les traces de ph&#233;romones d&#233;pos&#233;es pr&#233;c&#233;demment ou explore au
hasard dans le but de trouver un chemin optimal, fonction du probl&#232;me pos&#233;, dans le graphe.
</p>
<p>Ces algorithmes offrent une bonne alternative &#224; tout type de r&#233;solution de probl&#232;mes mod&#233;lisables sous forme
d&#8217;un graphe. Ils permettent un parcours rapide et efficace et offrent des r&#233;sultats comparables &#224; ceux obtenus par
les diff&#233;rentes m&#233;thodes de r&#233;solution. Leur grand int&#233;r&#234;t r&#233;side dans leur capacit&#233; &#224; s&#8217;adapter &#224; un changement
de l&#8217;environnement. Le lecteur trouvera dans (Dorigo &amp; St&#252;tzle, 2004) ou (Monmarche et al., 2009) de bons &#233;tats
de l&#8217;art sur la question.
</p>
<p>4.2 Algorithme &#224; colonies de fourmis et d&#233;sambigu&#239;sation lexicale
</p>
<p>4.2.1 Vue d&#8217;ensemble
</p>
<p>L&#8217;environnement des fourmis est un graphe. Il peut &#234;tre linguistique &#8211; morphologique comme dans (Rouquet et al.,
2010) ou morpho-syntaxique comme dans (Schwab &amp; Lafourcade, 2007; Guinand &amp; Lafourcade, 2009) &#8211; ou &#234;tre
simplement organis&#233; en fonction des &#233;l&#233;ments du texte. En fonction de l&#8217;environnement choisi, les r&#233;sultats de
l&#8217;algorithme ne sont &#233;videmment pas les m&#234;mes. Des recherches sont actuellement men&#233;es &#224; ce sujet mais, dans
cet article, nous ne nous int&#233;ressons qu&#8217;&#224; un cas de base c.-&#224;-d. un graphe simple (voir fig.1), sans information
linguistique externe, afin de mieux comprendre la m&#233;canique de nos algorithmes.
</p>
<p>Dans ce graphe, nous distinguons deux types de n&#339;uds : les fourmili&#232;res et les n&#339;uds normaux. Suivant les id&#233;es
d&#233;velopp&#233;es dans (Schwab, 2005) et (Guinand &amp; Lafourcade, 2009), chaque sens possible d&#8217;un mot est associ&#233; &#224;
une fourmili&#232;re. Les fourmili&#232;res produisent des fourmis. Ces fourmis se d&#233;placent dans le graphe &#224; la recherche
d&#8217;&#233;nergie puis la rapportent &#224; leur fourmili&#232;re m&#232;re qui pourra alors cr&#233;er de nouvelles fourmis. Pour une fourmi,
un n&#339;ud peut &#234;tre : (1) la fourmili&#232;re maison o&#249; elle est n&#233;e ; (2) une fourmili&#232;re ennemie qui correspond &#224; un
autre sens du m&#234;me mot ; (3) une fourmili&#232;re potentiellement amie, toutes celles qui ne sont pas ennemies ; (4) un
n&#339;ud qui n&#8217;est pas une fourmili&#232;re, les n&#339;uds normaux.
</p>
<p>Par exemple, dans la figure 1, pour une fourmi n&#233;e dans la fourmili&#232;re 19, le n&#339;ud 18 est un ennemi comme il a</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>DIDIER SCHWAB, J&#201;R&#212;ME GOULIAN, NATHAN GUILLAUME
</p>
<p>Texte
</p>
<p>Sens
</p>
<p>Phrase Phrase Phrase
</p>
<p>Mot MotMot Mot Mot Mot
</p>
<p>Sens Sens Sens Sens SensSens Sens Sens
</p>
<p>1
</p>
<p>2 3 4
</p>
<p>10
9
</p>
<p>876
</p>
<p>5
</p>
<p>11
12 13 14 15 16 17
</p>
<p>18 19
</p>
<p>FIGURE 1 &#8211; Environnement utilis&#233; dans cette exp&#233;rience : texte, phrases et mots correspondent aux n&#339;uds dits
normaux 1 &#224; 10, un sens de mot correspond &#224; &#224; une fourmili&#232;re (n&#339;uds 11 &#224; 19).
</p>
<p>le m&#234;me p&#232;re (10), les fourmili&#232;res potentiellement amies sont les n&#339;uds 11 &#224; 17 et les n&#339;uds normaux sont les
n&#339;uds 1 &#224; 10.
</p>
<p>Les d&#233;placements des fourmis se d&#233;roulent en fonction des scores locaux (cf. section 2.2), de la pr&#233;sence d&#8217;&#233;nergie,
et du passage des autres fourmis (Les fourmis laissent des traces sur les arcs o&#249; elles passent sous la forme de
ph&#233;romone). Une fois arriv&#233;e sur la fourmili&#232;re d&#8217;un autre terme, une fourmi peut choisir de revenir directement
&#224; sa fourmili&#232;re m&#232;re. Elle &#233;tablit alors, entre les deux fourmili&#232;res, un pont que les autres fourmis sont, &#224; leur
tour, susceptibles d&#8217;emprunter et de renforcer gr&#226;ce &#224; leur ph&#233;romone. Ce renforcement a lieu si les informations
lexicales conduisent les autres fourmis &#224; emprunter le pont et dispara&#238;t dans le cas inverse. Ainsi, les fourmis
&#233;tablissent de nombreux liens entre fourmili&#232;res de sens compatibles.
</p>
<p>Les ponts correspondent ainsi &#224; des interpr&#233;tations de la phrase. L&#8217;&#233;mergence de tels circuits dans le graphe
contribue &#224; la monopolisation des ressources de la colonie (fourmis et &#233;nergie) et &#224; l&#8217;&#233;puisement des ressources
associ&#233;es aux autres fourmili&#232;res (ces cas correspondent donc aux sens incompatibles dans le contexte et avec les
ressources consid&#233;r&#233;s).
</p>
<p>4.2.2 D&#233;tails de l&#8217;algorithme
</p>
<p>&#201;nergie Au d&#233;but de la simulation, le syst&#232;me poss&#232;de une certaine &#233;nergie qui est r&#233;partie &#233;quitablement sur
chacun des n&#339;uds. Les fourmili&#232;res utilisent celle qu&#8217;elles poss&#232;dent pour fabriquer des fourmis avec une probabi-
lit&#233; fonction de cette m&#234;me &#233;nergie et suivant une courbe sigmo&#239;de (cf. fig. 2). On peut remarquer que l&#8217;utilisation
de cette fonction permet aux fourmili&#232;res qui n&#8217;ont plus d&#8217;&#233;nergie de fabriquer quelques fourmis suppl&#233;men-
taires (et ainsi d&#8217;avoir une quantit&#233; d&#8217;&#233;nergie n&#233;gative). L&#8217;id&#233;e est de leur donner une derni&#232;re chance au cas o&#249;
ces fourmis, trouvant des informations lexicales pertinentes, rapportent de l&#8217;&#233;nergie et relancent la production de
fourmis.
</p>
<p>Les fourmis ont une dur&#233;e de vie (nombre de cycles identique pour toutes et param&#233;tr&#233; (cf. tableau 4.2.2)). Lors-
qu&#8217;une fourmi meurt, l&#8217;&#233;nergie qu&#8217;elle porte ainsi que l&#8217;&#233;nergie utilis&#233;e par la fourmili&#232;re pour la produire est
d&#233;pos&#233;e sur le n&#339;ud o&#249; elle se trouve. Il n&#8217;y a donc ni perte ni apport d&#8217;&#233;nergie &#224; aucun moment que ce soit. Si
on excepte l&#8217;emprunt &#224; la nature que peuvent faire de fa&#231;on tr&#232;s limit&#233;e les fourmili&#232;res, le syst&#232;me fonctionne
compl&#232;tement en vase clos. La quantit&#233; d&#8217;&#233;nergie est un &#233;l&#233;ment fondamental de la convergence du syst&#232;me vers
une solution. En effet, puisque l&#8217;&#233;nergie globale est limit&#233;e, les fourmili&#232;res sont en concurrence les unes avec les</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>PROPAGATION DE MESURES S&#201;MANTIQUES LOCALES PAR ALGORITHMES &#192; COLONIES DE FOURMIS
</p>
<p>FIGURE 2 &#8211; Courbe de la fonction sigmo&#239;de arctan(x)pi +
1
2 qui permet de calculer la probabilit&#233; de la naissance
</p>
<p>d&#8217;une fourmi &#224; partir de la quantit&#233; d&#8217;&#233;nergie pr&#233;sente sur le n&#339;ud.
</p>
<p>autres et seules des alliances peuvent permettre de faire &#233;merger des solutions.
</p>
<p>Ph&#233;romone de passage Les fourmis ont deux types de comportement. Elles peuvent soit chercher de l&#8217;&#233;nergie,
soit chercher &#224; revenir &#224; leur fourmili&#232;re m&#232;re. Lorsqu&#8217;elles se d&#233;placent dans le graphe, elles laissent des traces sur
les arcs o&#249; elles passent sous la forme de ph&#233;romone. La ph&#233;romone influe sur les d&#233;placements des fourmis qui
pr&#233;f&#232;rent l&#8217;&#233;viter lorsqu&#8217;elles cherchent de l&#8217;&#233;nergie et pr&#233;f&#232;rent la suivre lorsqu&#8217;elles tentent de revenir d&#233;poser
cette &#233;nergie &#224; leur fourmili&#232;re m&#232;re.
</p>
<p>Lors d&#8217;un d&#233;placement, une fourmi laisse une trace en d&#233;posant sur l&#8217;arc A travers&#233; une quantit&#233; de ph&#233;romone
&#952; &#8712; IR+. On a alors &#981;t+1(A) = &#981;t(A) + &#952;.
&#192; chaque cycle, il y a une l&#233;g&#232;re &#233;vaporation des ph&#233;romones. Cette baisse se fait de fa&#231;on lin&#233;aire jusqu&#8217;&#224; la
disparition totale de la ph&#233;romone. Nous avons ainsi, &#981;c+1(A) = &#981;c(A) &#215; (1 &#8722; &#948;) o&#249; &#948; est la proportion de
ph&#233;romone qui s&#8217;&#233;vapore &#224; chaque cycle.
</p>
<p>Cr&#233;ation, suppression et type de ponts Un pont peut &#234;tre cr&#233;&#233; lorsqu&#8217;une fourmi atteint une fourmili&#232;re po-
tentiellement amie, c.-&#224;-d. lorsqu&#8217;elle arrive sur un n&#339;ud qui correspond &#224; un sens d&#8217;un autre mot que celui de
la fourmili&#232;re m&#232;re. Dans ce cas, la fourmi &#233;value non seulement les n&#339;uds li&#233;s &#224; cette fourmili&#232;re mais aussi le
n&#339;ud correspondant &#224; sa fourmili&#232;re m&#232;re. Si ce dernier est s&#233;lectionn&#233;, il y a cr&#233;ation d&#8217;un pont entre les deux
fourmili&#232;res. Ce pont est ensuite consid&#233;r&#233; comme un arc standard par les fourmis, c.-&#224;-d. que les n&#339;uds qu&#8217;il lie
sont consid&#233;r&#233;s comme voisins. Si le pont ne porte plus de ph&#233;romone, il dispara&#238;t.
</p>
<p>Odeur L&#8217;odeur d&#8217;une fourmili&#232;re est la repr&#233;sentation vectorielle que nous avons introduite dans la partie 2.2.2.
Elle correspond donc &#224; la d&#233;finition du sens sous forme de vecteur de nombres entiers. Chaque fourmi n&#233;e dans
cette fourmili&#232;re porte la m&#234;me odeur, le m&#234;me vecteur. Lors de son d&#233;placement sur les n&#339;uds normaux du
graphe, une fourmi propage son vecteur. Le vecteur V(N) port&#233; par un n&#339;ud normal N est modifi&#233; lors du passage
d&#8217;une fourmi. La fourmi d&#233;pose une partie de son vecteur, un pourcentage des composantes prises au hasard qui
remplace la m&#234;me quantit&#233; d&#8217;anciennes valeurs elles aussi choisies au hasard.
</p>
<p>Cette propagation intervient dans le d&#233;placement des fourmis. Laisser une partie de son vecteur, c&#8217;est laisser une
trace de passage. Ainsi plus un n&#339;ud est proche d&#8217;une fourmili&#232;re plus il y a de chance que les fourmis de cette
fourmili&#232;re y soient pass&#233;es. Ce ph&#233;nom&#232;ne permet aux fourmis de revenir &#224; leur fourmili&#232;re, ou &#233;ventuellement de
se tromper et de se diriger vers des fourmili&#232;res amies. Cette erreur est ainsi potentiellement b&#233;n&#233;fique puisqu&#8217;elle</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>DIDIER SCHWAB, J&#201;R&#212;ME GOULIAN, NATHAN GUILLAUME
</p>
<p>peut permettre de cr&#233;er un pont entre les deux fourmili&#232;res (cf. 4.2.2). En revanche, lorsqu&#8217;une fourmi se trouve
sur une fourmili&#232;re, le vecteur n&#8217;est pas modifi&#233;. Ces n&#339;uds conservent ainsi un vecteur constant tout au long de
la simulation.
</p>
<p>La table suivante pr&#233;sente les param&#232;tres, les notations et les valeurs utilis&#233;es dans l&#8217;algorithme pr&#233;sent&#233; et exp&#233;-
riment&#233; ici. Cet article ne pr&#233;sente pas les exp&#233;riences r&#233;alis&#233;es pour trouver ces valeurs.
</p>
<p>Notation Description Valeurs
FA Fourmili&#232;re correspondant au sensA na
V (X) Vecteur odeur associ&#233; &#224; X. X est un n&#339;ud ou une fourmi na
fA Fourmi n&#233;e dans la fourmili&#232;re FA na
Ef &#201;nergie utilis&#233;e par une fourmili&#232;re pour produire une fourmi na
E(X) &#201;nergie poss&#233;d&#233;e par X. X est un n&#339;ud ou une fourmi na
Emax &#201;nergie maximale que peut porter une fourmi 5
&#981;(A) Quantit&#233; de ph&#233;romone sur l&#8217;arcA na
&#952; Ph&#233;romone d&#233;pos&#233;e par une fourmi lors de la travers&#233;e d&#8217;un arc 1
&#948; &#201;vaporation de la ph&#233;romone entre chaque cycle 20%
</p>
<p>Evalf (X) &#201;valuation de X selon la fourmi f. X est un arc ou un n&#339;ud na
Evalf (N,A) &#201;valuation du n&#339;udN en passant par l&#8217;arcA selon la fourmi f na
</p>
<p>Nombre de cycles de la simulation 100
Quantit&#233; initiale d&#8217;&#233;nergie sur chaque n&#339;ud 20
</p>
<p>Dur&#233;e de vie d&#8217;une fourmi 10
&#201;nergie prise par une fourmi lorsqu&#8217;elle arrive sur un n&#339;ud 1
</p>
<p>Longueur du vecteur odeur 50
Quantit&#233; du vecteur odeur modifi&#233; par une fourmi lorsqu&#8217;elle arrive sur un n&#339;ud 10%
</p>
<p>4.2.3 D&#233;roulement de l&#8217;algorithme
</p>
<p>L&#8217;algorithme consiste en une it&#233;ration potentiellement infinie de cycles. &#192; tout moment, la simulation peut &#234;tre
interrompue et l&#8217;&#233;tat courant observ&#233;. Durant un cycle, on effectue les t&#226;ches suivantes : (1) &#233;liminer les fourmis
trop vieilles (la dur&#233;e de vie est un param&#232;tre) ; (2) pour chaque fourmili&#232;re, solliciter la production d&#8217;une fourmi
(une fourmi peut ou non voir le jour, de fa&#231;on probabiliste) ; (3) pour chaque arc, diminuer le taux de ph&#233;romone
(&#233;vaporation des traces) ; (4) pour chaque fourmi : d&#233;terminer son mode (recherche d&#8217;&#233;nergie, retour &#224; la fourmi-
li&#232;re, le changement est fait de mani&#232;re probabiliste) et la d&#233;placer. Cr&#233;er un pont interpr&#233;tatif le cas &#233;ch&#233;ant ; (5)
calculer les cons&#233;quences du d&#233;placement des fourmis (sur l&#8217;activation des arcs et l&#8217;&#233;nergie des n&#339;uds).
</p>
<p>Les d&#233;placements d&#8217;une fourmi sont al&#233;atoires mais influenc&#233;s par son environnement. Lorsqu&#8217;une fourmi est sur
un n&#339;ud, elle estime tous les n&#339;uds voisins et tous les arcs qui les lient. La probabilit&#233; d&#8217;emprunter un arcAj pour
aller &#224; un n&#339;ud Ni est P (Ni, Aj) = max(
</p>
<p>Evalf (Ni,Aj)Pk=n,l=m
k=1,l=1 Evalf (Nk,Al)
</p>
<p>, &#15;) o&#249; Evalf (N,A) est l&#8217;&#233;valuation du n&#339;ud
</p>
<p>N en prenant l&#8217;arc A, c.-&#224;-d. la somme de Evalf (N) et de Evalf (A). &#15; permet &#224; certaines fourmis de choisir
des destinations &#233;valu&#233;es comme improbables mais qui permettraient d&#8217;atteindre des informations lexicales et des
ressources qui s&#8217;av&#232;reraient int&#233;ressantes ensuite.
</p>
<p>Une fourmi qui vient de na&#238;tre (c.-&#224;-d. &#234;tre produite par sa fourmili&#232;re) part &#224; la recherche d&#8217;&#233;nergie. Elle est attir&#233;e
par les n&#339;uds qui portent beaucoup d&#8217;&#233;nergie (Evalf (N) =
</p>
<p>E(N)Pm
0 E(Ni)
</p>
<p>) et &#233;vite les arcs qui portent beaucoup de
ph&#233;romone (Evalf (A) = 1&#8722;&#981;(A)) afin de permettre l&#8217;exploration de plus de solutions. Elle continue &#224; collecter
de l&#8217;&#233;nergie jusqu&#8217;au cycle o&#249; un tirage al&#233;atoire avec la probabilit&#233; P (retour) = E(f)Emax la fera passer en mode
retour. Dans ce mode, elle va (statistiquement) suivre les arcs avec beaucoup de ph&#233;romone (Evalf (A) = &#981;(A))
et vers les n&#339;uds dont l&#8217;odeur est proche de la leur (Evalf (N) =
</p>
<p>Lesk(V (N),V (fA))Pi=k
i=1 Lesk(V (Ni),V (fA))
</p>
<p>).</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>PROPAGATION DE MESURES S&#201;MANTIQUES LOCALES PAR ALGORITHMES &#192; COLONIES DE FOURMIS
</p>
<p>5 &#201;valuation
</p>
<p>Nous avons test&#233; notre m&#233;thode sur le corpus de la t&#226;che gros grain de la campagne d&#8217;&#233;valuation Semeval
2007 (Navigli et al., 2007) dans laquelle les organisateurs fournissent un inventaire de sens plus grossiers que
ceux de WordNet. Pour chaque terme, les sens consid&#233;r&#233;s comme proches (par exemple, &quot;neige/pr&#233;cipitation&quot; et
&quot;neige/couverture&quot; ou &quot;porc/animal&quot; et &quot;porc/viande&quot;) sont group&#233;s. Le corpus est compos&#233; de 5 textes de genres
divers (journalisme, critique litt&#233;raire, voyage, informatique, biographies) dont il faut annoter les 2269 mots. Le
nombre moyen de sens par mot est de 6,19 ; ramen&#233; &#224; 3,1 pour l&#8217;inventaire de sens grossiers. Les comp&#233;titeurs
&#233;taient libres de se servir de cet inventaire (sens grossiers connus a priori) ou non (sens grossiers connus a poste-
riori). Dans le premier cas, le nombre de choix &#224; faire pour chaque mot est r&#233;duit et la t&#226;che moins compliqu&#233;e.
Dans le second cas, les sens annot&#233;s sont jug&#233;s corrects s&#8217;ils sont dans le bon groupement, une sorte d&#8217;erreur
acceptable. Notre objectif est de tester un syst&#232;me en vue d&#8217;une utilisation dans un cadre applicatif r&#233;el or l&#8217;in-
ventaire de sens grossiers n&#8217;est disponible que pour les 2269 mots utilis&#233;s dans le corpus d&#8217;&#233;valuation, nous ne
l&#8217;utilisons donc pas. Dans les exp&#233;riences pr&#233;sent&#233;es ici, nous nous situons ainsi dans un cas de sens connus a
posteriori. Les r&#233;sultats sont analys&#233;s par les formules classiques :
</p>
<p>Pr&#233;cisionP = sens correctement annote&#769;ssens annote&#769;s RappelR =
sens correctement annote&#769;s
</p>
<p>sens a&#768; annoter F-mesureF =
2&#215;P&#215;R
P+R
</p>
<p>Dans le corpus, les mots sont annot&#233;s avec leur partie du discours (verbe, nom, adverbe, adjectif). &#192; partir de ces
informations, nous construisons l&#8217;environnement des fourmis : un n&#339;ud au niveau du texte, un n&#339;ud pour chaque
phrase, un n&#339;ud pour chaque mot et une fourmili&#232;re pour chaque sens (voir fig. 1). &#192; la fin d&#8217;un cycle, le sens
s&#233;lectionn&#233; pour chaque mot correspond &#224; la fourmili&#232;re qui a la plus grande quantit&#233; d&#8217;&#233;nergie.
</p>
<p>5.1 Ex&#233;cution de l&#8217;algorithme
</p>
<p>L&#8217;algorithme &#224; colonies de fourmis garantit la r&#233;alisation d&#8217;un choix entre les diff&#233;rentes possibilit&#233;s pour chaque
terme. Ainsi, 100% du corpus est annot&#233; et P=R=F puisque les sens annot&#233;s sont &#233;gaux aux sens &#224; annoter (P=R)
et dans ce cas F = 2&#215;P&#215;PP+P =
</p>
<p>2&#215;P 2
2P = P . De plus, un algorithme &#224; colonies de fourmis est un algorithme
</p>
<p>stochastique, il ne s&#233;lectionne donc pas exactement les m&#234;mes sens &#224; chaque ex&#233;cution ni m&#234;me &#224; chaque cycle.
Nous avons ex&#233;cut&#233; cet algorithme des centaines de fois et avons not&#233; qu&#8217;apr&#232;s 70-80 cycles, les r&#233;sultats restaient
globalement constants comme l&#8217;illustre la figure suivante.
</p>
<p>1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 10
0
</p>
<p>59
</p>
<p>61
</p>
<p>63
</p>
<p>65
</p>
<p>67
</p>
<p>69
</p>
<p>71
</p>
<p>73
</p>
<p>75
</p>
<p>Lesk
Lesk &#233;tendu
</p>
<p>Cycles
</p>
<p>P
r&#233;
</p>
<p>ci
si
</p>
<p>on
/R
</p>
<p>ap
pe
</p>
<p>l/F
-m
</p>
<p>es
ur
</p>
<p>e 
(%
</p>
<p>)
</p>
<p>FIGURE 3 &#8211; &#201;volution de la pr&#233;cision/rappel/F-mesure dans les 100 cycles d&#8217;une ex&#233;cution de l&#8217;algorithme &#224;
colonies de fourmis utilis&#233; avec l&#8217;algorithme local Lesk et avec l&#8217;algorithme local Lesk &#233;tendu</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>DIDIER SCHWAB, J&#201;R&#212;ME GOULIAN, NATHAN GUILLAUME
</p>
<p>5.2 Comparaison d&#8217;ex&#233;cutions
</p>
<p>De la m&#234;me mani&#232;re que les r&#233;sultats &#233;voluent entre deux cycles, les r&#233;sultats peuvent &#234;tre diff&#233;rents entre deux
ex&#233;cutions. Pour donner une id&#233;e de cette diff&#233;rence, nous avons r&#233;p&#233;t&#233; notre exp&#233;rience, arr&#234;t&#233;e au bout de 100
cycles, sur chacun des algorithmes locaux, 100 fois. La table suivante en pr&#233;sente les r&#233;sultats. Nous obtenons
seulement 2, 95% d&#8217;&#233;cart entre le meilleur et le moins bon r&#233;sultat (soit 67 termes mal annot&#233;s sur 2269) pour
Lesk &#233;tendu et 3,39% (soit 77 termes mal annot&#233;s sur 2269) pour Lesk.
</p>
<p>Algorithme local Minimum Maximum Moyenne M&#233;diane &#201;tendue &#201;cart-type
Lesk 64,43 67,83 66,34 66,35 3,39 0,66
</p>
<p>Lesk &#233;tendu 72,54 75,5 74,01 74,04 2,95 0,58
</p>
<p>5.3 Comparaisons avec l&#8217;algorithme exhaustif
</p>
<p>&#192; titre de comparaison avec notre approche, nous pr&#233;sentons les r&#233;sultats obtenus par l&#8217;algorithme global exhaustif
(Banerjee &amp; Pedersen, 2002). Nous avons choisi comme contexte la phrase, excluant de facto les phrases d&#8217;un mot
(au nombre de quatre, soit moins de 0, 002% du corpus). Pour des raisons calculatoires, nous avons &#233;galement
exclu les phrases de plus de 10 milliards de combinaisons. Nous pouvons voir que seulement 77, 3% du corpus a
</p>
<p>Algorithme global Algorithme local &#201;tiquet&#233;s Pr&#233;cision Rappel F-mesure Temps
</p>
<p>Calcul exhaustif Lesk 77,30 69,21 53,50 60,35 &#8776; 40hLesk &#233;tendu 77,30 77,82 60,16 67,86 &#8776; 300h
Fourmis Lesk 100,0 64,43 - 67,83 64,43 - 67,83 64,43 - 67,83 &#8776; 3mLesk &#233;tendu 100,0 72,54 - 75,5 72,54 - 75,5 72,54 - 75,5 &#8776; 8m
</p>
<p>&#233;t&#233; annot&#233; au prix d&#8217;une dur&#233;e de plusieurs heures incompatible avec des applications en temps r&#233;el 4.
</p>
<p>Pour les deux algorithmes locaux, la F-mesure est clairement sup&#233;rieure &#224; celle du calcul brut pour un temps
nettement moins long (800 fois plus court pour Lesk et 2250 fois pour Lesk &#233;tendu). Le tableau suivant pr&#233;sente
pour les m&#234;mes ex&#233;cutions les r&#233;sultats sur les diff&#233;rentes sous-parties du corpus : A, la partie annot&#233; par les 2
algorithmes globaux et B celle qui n&#8217;est annot&#233;e que par l&#8217;algorithme fourmis. Sur la partie A, les fourmis sont,
comme on pouvait s&#8217;en douter, l&#233;g&#232;rement en dessous de l&#8217;algorithme exhaustif et leur meilleur r&#233;sultat s&#8217;explique
par la possibilit&#233; d&#8217;annoter la sous-partie B.
</p>
<p>Pour conclure cette &#233;valuation, nous avons compar&#233; nos r&#233;sultats avec les r&#233;sultats obtenus par les diff&#233;rents
syst&#232;mes qui participaient &#224; la campagne Semeval 2007. Avec Lesk &#233;tendu, nous serions arriv&#233;s 8e&#768;me/15 en tenant
compte de tous les participants, 5e&#768;me/8 sur ceux qui ne connaissent pas a priori les sens grossiers, 1er/7 sur
les approches non supervis&#233;es. Ces r&#233;sultats sont tr&#232;s encourageants vu les temps de calcul (aucun article des
participants n&#8217;aborde ce point), les possibilit&#233;s d&#8217;extension qu&#8217;offrent les algorithmes &#224; fourmis et la simplicit&#233;
des algorithmes locaux envisag&#233;s ici.
</p>
<p>4. Exp&#233;riences r&#233;alis&#233;es sur des processeurs Intel Xeon X5550, 4 c&#339;urs &#224; 2.66Ghz (dur&#233;es converties en temps monoprocesseurs).</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>PROPAGATION DE MESURES S&#201;MANTIQUES LOCALES PAR ALGORITHMES &#192; COLONIES DE FOURMIS
</p>
<p>Algorithme local Sous-corpus Algorithme global &#201;tiquet&#233;s Rappel Diff&#233;rentiel
</p>
<p>Lesk
</p>
<p>A + B Exhaustif 77,30 53,50Fourmis 100,0 64,43 - 67,83 + 10,93 &#224; + 14,33
</p>
<p>A Exhaustif 100,0 69,21Fourmis 100,0 65,45 - 68,99 - 3,76 &#224; - 0,22
</p>
<p>B Exhaustif 00,00 00,00Fourmis 100,0 60,97 - 63,88 + 60,97 &#224; + 63,88
</p>
<p>Lesk &#233;tendu
</p>
<p>A + B Exhaustif 77,30 60,16Fourmis 100,0 72,54 - 75,5 + 12,38 &#224; + 15,34
</p>
<p>A Exhaustif 100,0 77,82Fourmis 100,0 74,69 - 77,25 - 3,13 &#224; - 0,57
</p>
<p>B Exhaustif 00,00 00,00Fourmis 100,0 65,24 - 69,52 + 65,24 &#224; + 69,52
</p>
<p>6 Conclusions et Perspectives
</p>
<p>Dans cet article, nous avons pr&#233;sent&#233; un algorithme &#224; colonies de fourmis destin&#233; &#224; la d&#233;sambigu&#239;sation lexicale et
bas&#233; sur des mesures de proximit&#233; s&#233;mantique. Cet algorithme, non supervis&#233;, est volontairement simple puisqu&#8217;il
n&#8217;utilise qu&#8217;une seule ressource lexicale (WordNet) et aucune analyse morphologique ou morpho-syntaxique. Il
permet pourtant de choisir un sens, pour chaque mot d&#8217;un texte, d&#8217;une mani&#232;re plus rapide que l&#8217;algorithme
exhaustif et en atteignant une bonne F-mesure pour un syst&#232;me non supervis&#233;. Nous consid&#233;rons ces r&#233;sultats
comme une ligne de base (baseline) &#224; partir de laquelle nous allons poursuivre nos recherches. Outre l&#8217;ajout
d&#8217;informations morphologiques et/ou syntaxiques, nous travaillons actuellement sur la combinaison de mesures
locales et l&#8217;utilisation de WordNet dans l&#8217;environnement des fourmis. Nos travaux portent &#233;galement sur d&#8217;autres
algorithmes locaux et leur impact sur l&#8217;utilisation dans d&#8217;autres langues notamment flexionnelles. Enfin, nous
travaillons &#224; la comparaison des algorithmes &#224; colonies de fourmis avec d&#8217;autres algorithmes globaux comme les
algorithmes g&#233;n&#233;tiques ou le recuit simul&#233;.
</p>
<p>R&#233;f&#233;rences
</p>
<p>AGIRRE E. &amp; EDMONDS P. (2006). Word Sense Disambiguation : Algorithms and Applications (Text, Speech
and Language Technology). Secaucus, NJ, USA : Springer-Verlag New York, Inc.
</p>
<p>BANERJEE S. &amp; PEDERSEN T. (2002). An adapted lesk algorithm for word sense disambiguation using wordnet.
In the Third International Conference on Intelligent Text Processing and Computational Linguistics, CICLing
2002, Mexico City.
</p>
<p>BONABEAU &#201;. &amp; TH&#201;RAULAZ G. (2000). L&#8217;intelligence en essaim. Pour la science, (271), 66&#8211;73.
</p>
<p>COWIE J., GUTHRIE J. &amp; GUTHRIE L. (1992). Lexical disambiguation using simulated annealing. In COLING
1992, International Conference on Computational Linguistics, volume 1, p. 359&#8211;365, Nantes, France.
</p>
<p>CRAMER I., WANDMACHER T. &amp; WALTINGER U. (2010). WordNet : An electronic lexical database, chapter
Modeling, Learning and Processing of Text Technological Data Structures. Springer.
</p>
<p>DEERWESTER S. C., DUMAIS S. T., LANDAUER T. K., FURNAS G. W. &amp; HARSHMAN R. A. (1990). Indexing
by latent semantic analysis. Journal of the American Society of Information Science, 41(6).
DENEUBOURG J.-L., GROSS S., FRANKS N. &amp; PASTEELS J.-M. (1989). The blind leading the blind : Mode-
ling chemically mediated army ant raid patterns. Journal of Insect Behavior, 2, 719&#8211;725.
DORIGO &amp; ST&#220;TZLE (2004). Ant Colony Optimization. MIT-Press.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>DIDIER SCHWAB, J&#201;R&#212;ME GOULIAN, NATHAN GUILLAUME
</p>
<p>DORIGO M. &amp; GAMBARDELLA L. (1997). Ant colony system : A cooperative learning approach to the traveling
salesman problem. IEEE Transactions on Evolutionary Computation, 1, 53&#8211;66.
DROGOUL A. (1993). When ants play chess (or can strategies emerge from tactical behaviors). In Maa-
maw&#8217;1993.
FELLBAUM C. (1998). WordNet : An Electronic Lexical Database (Language, Speech, and Communication).
The MIT Press.
GALE W., CHURCH K. &amp; YAROWSKY D. (1992). One sense per discourse. In Fifth DARPA Speech and Natural
Language Workshop, p. 233&#8211;237, Harriman, New-York, &#201;tats-Unis.
GELBUKH A., SIDOROV G. &amp; HAN S. Y. (2003). Evolutionary approach to natural language word sense
disambiguation through global coherence optimization. WSEAS Transactions on Communications, 2(1), 11&#8211;19.
GUINAND F. &amp; LAFOURCADE M. (2009). Fourmis Artificielles 2. Nouvelles Directions pour une Intelligence
Collective, chapter Fourmis Artificielles et Traitement de la Langue Naturelle, p. 225&#8211;267. Lavoisier.
IDE N. &amp; V&#201;RONIS J. (1998). Word sense disambiguation : the state of the art. Computational Linguistics,
28(1), 1&#8211;41.
LESK M. (1986). Automatic sense disambiguation using machine readable dictionaries : how to tell a pine cone
from an ice cream cone. In Proceedings of the 5th annual international conference on Systems documentation,
SIGDOC &#8217;86, p. 24&#8211;26, New York, NY, USA : ACM.
N. MONMARCHE, F. GUINAND &amp; P. SIARRY, Eds. (2009). Fourmis Artificielles et Traitement de la Langue
Naturelle. Prague, Czech Republic : Lavoisier.
NAVIGLI R. (2009). Word sense disambiguation : a survey. ACM Computing Surveys, 41(2), 1&#8211;69.
NAVIGLI R. &amp; LAPATA M. (2010). An experimental study of graph connectivity for unsupervised word sense
disambiguation. IEEE Trans. Pattern Anal. Mach. Intell., p. 678&#8211;692.
NAVIGLI R., LITKOWSKI K. C. &amp; HARGRAVES O. (2007). Semeval-2007 task 07 : Coarse-grained english
all-words task. In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007),
p. 30&#8211;35, Prague, Czech Republic : Association for Computational Linguistics.
PEDERSEN T., BANERJEE S. &amp; PATWARDHAN S. (2005). Maximizing Semantic Relatedness to Perform Word
Sense Disambiguation. Research Report UMSI 2005/25, University of Minnesota Supercomputing Institute.
PONZETTO S. P. &amp; NAVIGLI R. (2010). Knowledge-rich word sense disambiguation rivaling supervised sys-
tems. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL &#8217;10, p.
1522&#8211;1531, Stroudsburg, PA, USA : Association for Computational Linguistics.
ROUQUET D., FALAISE A., SCHWAB D., BOITET C., BELLYNCK V., NGUYEN H.-T., MANGEOT M. &amp;
GUILBAUD J.-P. (2010). Rapport final de synth&#232;se, passage &#224; l&#8217;&#233;chelle et impl&#233;mentation : Extraction de
contenu s&#233;mantique dans des masses de donn&#233;es textuelles multilingues. Rapport interne, Agence Nationale
de la Recherche.
SCHWAB D. (2005). Approche hybride - lexicale et th&#233;matique - pour la mod&#233;lisation, la d&#233;tection et l&#8217;ex-
ploitation des fonctions lexicales en vue de l&#8217;analyse s&#233;mantique de texte. PhD thesis, Universit&#233; Montpellier
2.
SCHWAB D. &amp; LAFOURCADE M. (2007). Lexical functions for ants based semantic analysis. In ICAI&#8217;07- The
2007 International Conference on Artificial Intelligence, Las Vegas, Nevada, USA.
VASILESCU F., LANGLAIS P. &amp; LAPALME G. (2004). Evaluating variants of the lesk approach for disambi-
guating words. In Proceedings of LREC 2004, the 4th International Conference On Language Resources And
Evaluation, p. 633&#8211;636, Lisbon, Portugal.</p>

</div></div>
</body></html>