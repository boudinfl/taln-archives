TALN 2011, Montpellier, 27 juin ‚Äì1er juillet 2011
D√©sambigu√Øsation lexicale par propagation de mesures s√©mantiques
locales par algorithmes √† colonies de fourmis
Didier Schwab, J√©r√¥me Goulian, Nathan Guillaume
LIG-GETALP (Laboratoire d‚ÄôInformatique de Grenoble, Groupe d‚Äô√âtude pour la
Traduction/le Traitement Automatique des Langues et de la Parole)
Universit√© Pierre Mend√®s France, Grenoble 2
{didier.schwab, jerome.goulian}@imag.fr
R√©sum√©. Effectuer une t√¢che de d√©sambigu√Øsation lexicale peut permettre d‚Äôam√©liorer de nombreuses ap-
plications du traitement automatique des langues comme l‚Äôextraction d‚Äôinformations multilingues, ou la traduction
automatique. Sch√©matiquement, il s‚Äôagit de choisir quel est le sens le plus appropri√© pour chaque mot d‚Äôun texte.
Une des approches classiques consiste √† estimer la proximit√© s√©mantique qui existe entre deux sens de mots puis
de l‚Äô√©tendre √† l‚Äôensemble du texte. La m√©thode la plus directe donne un score √† toutes les paires de sens de
mots puis choisit la cha√Æne de sens qui a le meilleur score. La complexit√© de cet algorithme est exponentielle et
le contexte qu‚Äôil est calculatoirement possible d‚Äôutiliser s‚Äôen trouve r√©duit. Il ne s‚Äôagit donc pas d‚Äôune solution
viable. Dans cet article, nous nous int√©ressons √† une autre m√©thode, l‚Äôadaptation d‚Äôun algorithme √† colonies de
fourmis. Nous pr√©sentons ses caract√©ristiques et montrons qu‚Äôil permet de propager √† un niveau global les r√©sultats
des algorithmes locaux et de tenir compte d‚Äôun contexte plus long et plus appropri√© en un temps raisonnable.
Abstract. Word sense disambiguation can lead to significant improvement in many Natural Language Pro-
cessing applications as Machine Translation or Multilingual Information Retrieval. Basically, the aim is to choose
for each word in a text its best sense. One of the most popular method estimates local semantic relatedness bet-
ween two word senses and then extends it to the whole text. The most direct method computes a rough score for
every pair of word senses and chooses the lexical chain that has the best score. The complexity of this algorithm
is exponential and the context that it is computationally possible to use is reduced. Brute force is therefore not a
viable solution. In this paper, we focus on another method : the adaptation of an ant colony algorithm. We present
its features and show that it can spread at a global level the results of local algorithms and consider a longer and
more appropriate context in a reasonable time.
Mots-cl√©s : D√©sambigu√Øsation lexicale, Algorithmes √† colonies de fourmis, Mesures s√©mantiques.
Keywords: Lexical Disambiguation, Ant colony algorithms, Semantic relatedness.
1 Introduction
Effectuer une t√¢che de d√©sambigu√Øsation lexicale peut permettre d‚Äôam√©liorer de nombreuses applications du traite-
ment automatique des langues comme l‚Äôextraction d‚Äôinformations multilingues, le r√©sum√© automatique ou encore
la traduction automatique. Sch√©matiquement, il s‚Äôagit de choisir quel est le sens le plus appropri√© pour chaque mot
d‚Äôun texte dans un inventaire pr√©-d√©fini. Par exemple, dans "La souris mange le fromage.", l‚Äôanimal devrait √™tre
DIDIER SCHWAB, J√âR√îME GOULIAN, NATHAN GUILLAUME
pr√©f√©r√© au dispositif √©lectronique. De nombreux travaux existent sur le sujet, que l‚Äôon s√©pare habituellement en
approches supervis√©es et non-supervis√©es. Les premi√®res utilisent des apprentissages r√©alis√©s gr√¢ce √† des corpus
manuellement annot√©s, les secondes n‚Äôutilisent pas de telles donn√©es. Une cat√©gorie interm√©diaire, constitu√©e des
approches semi-supervis√©es, utilise quelques donn√©es annot√©es comme, par exemple, un sens par d√©faut issu d‚Äôun
corpus annot√© lorsque l‚Äôalgorithme principal √©choue (Navigli & Lapata, 2010). Le lecteur pourra consulter (Ide &
V√©ronis, 1998) pour les travaux ant√©rieurs √† 1998 et (Agirre & Edmonds, 2006) ou (Navigli, 2009) pour un √©tat
de l‚Äôart complet.
La cr√©ation de donn√©es annot√©es est une op√©ration compliqu√©e puisqu‚Äôelle n√©cessite une importante main d‚Äô≈ìuvre
et qu‚Äôelle doit √™tre r√©alis√©e pour chaque inventaire de sens, pour chaque langue et m√™me pour chaque domaine sp√©-
cifique (sport, finance, . . .). Cette constatation, que nous partageons avec (Navigli & Lapata, 2010), nous conduit
√† nous int√©resser plus particuli√®rement √† des approches non-supervis√©es. Une de ces approches classiques consiste
√† estimer la proximit√© s√©mantique qui existe entre deux sens de mots puis de l‚Äô√©tendre √† l‚Äôensemble du texte. En
d‚Äôautres termes, il s‚Äôagit de donner des scores locaux et de les propager au niveau global (phrase, paragraphe,
texte, . . .). La m√©thode la plus directe, utilis√©e par exemple par (Pedersen et al., 2005) utilise un algorithme brutal
qui donne un score √† toutes les paires de sens de mots puis choisit la cha√Æne de sens qui a le meilleur score. La
complexit√© de cet algorithme est exponentielle et le contexte qu‚Äôil est calculatoirement possible d‚Äôutiliser s‚Äôen
trouve r√©duit. Ainsi, alors qu‚Äôune analyse au niveau de la phrase n‚Äôest d√©j√† pas toujours possible, un contexte
linguistiquement plus pertinent comme, par exemple, le paragraphe l‚Äôest encore moins.
Les applications que nous visons doivent pouvoir √™tre utilis√©es en temps r√©el. Lorsque l‚Äôon recherche une image
et encore plus lorsque l‚Äôon appelle quelqu‚Äôun qui parle une autre langue au t√©l√©phone, les r√©ponses doivent √™tre
imm√©diates. Il ne s‚Äôagit donc pas une solution viable et nous √©tudions d‚Äôautres m√©thodes.
Dans cet article, nous nous int√©ressons √† la propagation de mesures de proximit√© s√©mantique locales gr√¢ce √† une
adaptation d‚Äôun algorithme √† colonies de fourmis. Nous pr√©sentons dans un premier temps les mesures locales que
nous utilisons puis quelques unes des caract√©ristiques de notre algorithme de propagation. Enfin, √† titre d‚Äôexemple,
nous √©valuons notre approche sur la t√¢che gros grain de la campagne d‚Äô√©valuation Semeval 2007 (Navigli et al.,
2007). Nous comparons en particulier notre algorithme de propagation √† l‚Äôalgorithme exhaustif classique et mon-
trons qu‚Äôil permet d‚Äôobtenir efficacement une meilleure F-mesure.
2 Algorithmes locaux
2.1 Mesures de proximit√© s√©mantique
Ces m√©thodes consistent √† donner un score cens√© refl√©ter la proximit√© des objets linguistiques (g√©n√©ralement des
mots ou des sens de mots) compar√©s. Ces scores peuvent √™tre des similarit√©s, donc avoir une valeur entre 0 et
1, des distances, et donc respecter leurs trois propri√©t√©s (s√©paration, sym√©trie et in√©galit√© triangulaire) ou plus
g√©n√©ralement, √™tre une valeur positive non born√©e.
Parmi elles, on peut citer Hirst & Saint-Honge bas√©e sur la distance en terme de graphe entre deux sens dans un
r√©seau lexical ; Rada et al. ainsi que Leacock and Chodorow similaires √† la pr√©c√©dente mais ne consid√©rant que
les liens de type hyperonymie ; les mesures ou distances entre vecteurs (LSA (Deerwester et al., 1990), vecteurs
conceptuels (Schwab, 2005)). On pourra consulter (Pedersen et al., 2005), (Cramer et al., 2010) ou (Navigli, 2009)
pour un panorama plus complet.
En d√©sambigu√Øsation lexicale, ces m√©thodes sont utilis√©es de fa√ßon locale entre deux sens de mots, et sont ensuite
PROPAGATION DE MESURES S√âMANTIQUES LOCALES PAR ALGORITHMES √Ä COLONIES DE FOURMIS
appliqu√©es √† un niveau global. Dans cet article, nous nous concentrons sur l‚Äôalgorithme global et, √† des fins de
comparaison, nous pr√©sentons deux algorithmes locaux bas√©s sur l‚Äôalgorithme de Lesk.
2.2 Algorithmes locaux de cette exp√©rience
2.2.1 Des algorithmes inspir√©s par Lesk
Nous utilisons dans cet article deux variantes de l‚Äôalgorithme de Lesk (Lesk, 1986). Propos√©es il y a plus de 25
ans, il se caract√©rise par sa simplicit√©. Il ne n√©cessite qu‚Äôun dictionnaire et aucun apprentissage. Le score donn√©
√† une paire de sens est le nombre de mots ‚Äì ici simplement les suites de caract√®res s√©par√©es par des espaces ‚Äì en
commun dans leur d√©finition, sans tenir compte ni de leur ordre, ni de sous-s√©quences communes (approche sac de
mots), ni d‚Äôinformations morphologiques ou syntaxiques. Les variantes de cet algorithme sont encore aujourd‚Äôhui
parmi les meilleures sur l‚Äôanglais (Ponzetto & Navigli, 2010). Ce premier algorithme local est nomm√© dans la
suite Lesk.
Nous utilisons WordNet (Fellbaum, 1998), une base lexicale pour l‚Äôanglais, dans laquelle les sens de mots (les
synsets) sont reli√©s par des relations (hyperonymie, hyponymie, antonymie, etc.). Notre second algorithme local
exploite ces liens. Au lieu d‚Äôutiliser uniquement la d√©finition d‚Äôun sens, elle utilise √©galement la d√©finition des
diff√©rents sens qui lui sont li√©s. Cette id√©e est similaire √† celle de (Banerjee & Pedersen, 2002) 1. Ce second
algorithme local est nomm√© dans la suite Lesk √©tendu.
2.2.2 Efficacit√© algorithmique
L‚Äôalgorithme de base pour comparer le nombre de mots communs √† deux d√©finitions a une complexit√© en O(n √ó
m) avec n et m, les longueurs en mots des d√©finitions. De plus, la comparaison de cha√Ænes de caract√®res est
une op√©ration relativement ch√®re. On pourrait penser qu‚Äôil suffirait de pr√©calculer la matrice de similarit√©s avec
l‚Äôensemble des d√©finitions. Cette id√©e est utopique vu la taille que peuvent atteindre les dictionnaires (jusqu‚Äô√†
plusieurs millions de d√©finitions) 2 mais aussi parce qu‚Äôon a toujours besoin de faire des calculs sur de nouvelles
donn√©es puisque (1) les donn√©es et les sens peuvent √©voluer au cours du temps comme dans (Schwab, 2005), (2)
notre algorithme de propagation utilise des pseudo-d√©finitions cr√©es √† la vol√©e (voir partie 4.2.2).
Nous avons am√©lior√© ce calcul en utilisant un pr√©traitement qui se d√©roule en deux √©tapes. Dans la premi√®re,
nous affectons √† chacun des mots trouv√©s dans le dictionnaire un nombre entier tandis que, dans la seconde, nous
convertissons chacune des d√©finitions en un vecteur de nombres correspondant aux mots qu‚Äôelle contient, tri√©s du
plus petit au plus grand. Nous appelons ces vecteurs, vecteurs de d√©finitions.
Par exemple, si notre premi√®re √©tape a donn√© ‚Ü™‚Ü™kind‚Ü©‚Ü©= 1 ; ‚Ü™‚Ü™of‚Ü©‚Ü©= 2 ; ‚Ü™‚Ü™evergreen‚Ü©‚Ü©= 3 ; ‚Ü™‚Ü™tree‚Ü©‚Ü©= 4 ; ‚Ü™‚Ü™with‚Ü©‚Ü©= 5
‚Ü™‚Ü™needle-shaped‚Ü©‚Ü©= 6 ; ‚Ü™‚Ü™leaves‚Ü©‚Ü©= 7 ; ‚Ü™‚Ü™fruit‚Ü©‚Ü©= 8 ; ‚Ü™‚Ü™certain‚Ü©‚Ü©= 9 avec la d√©finition A, "kind of evergreen tree with
needle-shaped leaves", nous obtenons le vecteur [1, 2, 3, 4, 5, 6, 7] et avec B, "fruit of certain evergreen tree",
nous obtenons [2, 3, 4, 8, 9].
Cette conversion a deux avantages : (1) la comparaison de nombres est bien plus efficace que la comparaison
de cha√Ænes de caract√®res, (2) ordonner ces nombres permet d‚Äô√©viter des comparaisons inutiles et de gagner en
1. (Banerjee & Pedersen, 2002) introduit √©galement une notion de sous-s√©quence identique dans les d√©finitions. Nous n‚Äôavons pas encore
test√© cette variante dont la complexit√© algorithmique est nettement sup√©rieure √† celle de notre algorithme.
2. Une forme de cache pourrait en partie r√©gler ce probl√®me.
DIDIER SCHWAB, J√âR√îME GOULIAN, NATHAN GUILLAUME
efficacit√©. Ainsi, avec ce pr√©traitement, la complexit√© passe de O(n √ó m) √† O(n) o√π n et m (n ‚â• m) sont les
longueurs (en nombre de mots) des d√©finitions.
Pour les d√©finitions A et B, calculer cette m√™me proximit√© s√©mantique avec l‚Äôalgorithme sur les d√©finitions brutes
se fait en 7 √ó 5 = 35 op√©rations (qui plus est sur des cha√Ænes de caract√®res) tandis que si les d√©finitions sont
converties en vecteurs, nous n‚Äôavons plus que 7 op√©rations.
3 Algorithmes globaux
L‚Äôalgorithme global est l‚Äôalgorithme qui va permettre de propager les r√©sultats d‚Äôun ou plusieurs algorithmes
locaux √† l‚Äôensemble du texte afin de pouvoir en d√©duire un sens pour chaque mot. La m√©thode la plus directe
est la recherche exhaustive utilis√©e par exemple dans (Banerjee & Pedersen, 2002). Il s‚Äôagit de consid√©rer les
combinaisons de l‚Äôensemble des sens des mots dans le m√™me contexte (fen√™tre de mots, phrase, texte, etc.), de
donner un score √† chacune de ces combinaisons et de choisir celle qui a le meilleur score. Le principal probl√®me
de cette m√©thode est la rapide explosion combinatoire qu‚Äôelle engendre. Consid√©rons la phrase suivante tir√©e du
corpus d‚Äô√©valuation que nous utilisons dans la partie 5, "The pictures they painted were flat, not round as a figure
should be, and very often the feet did not look as if they were standing on the ground at all, but pointed downwards
as if they were hanging in the air.", ‚Ü™picture‚Ü© a 9 sens, ‚Ü™paint‚Ü© 4, ‚Ü™be‚Ü© 13, ‚Ü™flat‚Ü© 17, ‚Ü™figure‚Ü© 13, ‚Ü™very‚Ü© 2, ‚Ü™often‚Ü© 2, ‚Ü™foot‚Ü©
11, ‚Ü™look‚Ü© 10, ‚Ü™stand‚Ü© 12, ‚Ü™ground‚Ü© 11, ‚Ü™at all‚Ü© 1, ‚Ü™point‚Ü© 13, ‚Ü™downwards‚Ü© 1, ‚Ü™hang‚Ü© 15 et ‚Ü™air‚Ü© 9 sens, il y a alors 137 051
946 345 600 combinaisons de sens possibles √† analyser. Ce nombre est comparable √† la quantit√© d‚Äôop√©rations (et
le calcul d‚Äôune combinaison n√©cessite des dizaines voire des centaines d‚Äôop√©rations) que peuvent th√©oriquement
effectuer 3300 processeurs Core i7-990X (2,43GHz, 6 c≈ìurs, 12 fils d‚Äôex√©cutions) sortis par Intel au premier
trimestre 2011 en une seconde. Le calcul exhaustif est donc tr√®s compliqu√© √† r√©aliser dans des conditions r√©elles
et, surtout, rend impossible l‚Äôutilisation d‚Äôun contexte d‚Äôanalyse plus important.
Pour contourner ce probl√®me, plusieurs solutions ont √©t√© propos√©es. Par exemple, des approches utilisant un corpus
pour diminuer le nombre de combinaisons √† examiner comme la recherche des cha√Ænes lexicales compatibles (Gale
et al., 1992; Vasilescu et al., 2004) ou encore des approches issues de l‚Äôintelligence artificielle comme le recuit
simul√© (Cowie et al., 1992) ou les algorithmes g√©n√©tiques (Gelbukh et al., 2003).
Ces m√©thodes ont en commun de ne pas permettre l‚Äôexploitation de fa√ßon directe et simple d‚Äôune structure lin-
guistique sous forme de graphe que ce soit une analyse morphologique ou une analyse syntaxique. Nous utilisons,
au contraire, une m√©thode √† colonies de fourmis pour l‚Äôanalyse s√©mantique inspir√©e de (Schwab & Lafourcade,
2007) afin de pouvoir √† terme utiliser de telles structures 3.
4 Notre algorithme global : un algorithme √† colonies de fourmis
4.1 Les algorithmes √† colonies de fourmis
Les algorithmes √† fourmis ont pour origine la biologie et les observations r√©alis√©es sur le comportement social des
fourmis. En effet, ces insectes ont collectivement la capacit√© de trouver le plus court chemin entre leur fourmili√®re
et une source d‚Äô√©nergie. Il a pu √™tre d√©montr√© que la coop√©ration au sein de la colonie est auto-organis√©e et
r√©sulte d‚Äôinteractions entre individus autonomes. Ces interactions, souvent tr√®s simples, permettent √† la colonie
3. Dans un premier temps, nous utiliserons ici une structure linguistique extr√™mement simpl(ist)e.
PROPAGATION DE MESURES S√âMANTIQUES LOCALES PAR ALGORITHMES √Ä COLONIES DE FOURMIS
de r√©soudre des probl√®mes compliqu√©s. Ce ph√©nom√®ne est appel√© intelligence en essaim (Bonabeau & Th√©raulaz,
2000). Il est de plus en plus utilis√© en informatique o√π des syst√®mes de contr√¥le centralis√©s gagnent souvent √† √™tre
remplac√©s par d‚Äôautres, fond√©s sur les interactions d‚Äô√©l√©ments simples.
En 1989, Jean-Louis Deneubourg √©tudie le comportement des fourmis biologiques dans le but de comprendre la
m√©thode avec laquelle elles choisissent le plus court chemin et le retrouvent en cas d‚Äôobstacle. Il √©labore ainsi le
mod√®le stochastique dit de Deneubourg (Deneubourg et al., 1989), conforme √† ce qui est observ√© statistiquement
sur les fourmis r√©elles quant √† leur partage entre les chemins. Ce mod√®le stochastique est √† l‚Äôorigine des travaux
sur les algorithmes √† fourmis.
Le concept principal de l‚Äôintelligence en essaim est la stygmergie, c.-√†-d. l‚Äôinteraction entre agents par modifi-
cation de l‚Äôenvironnement. Une des premi√®res m√©thodes que l‚Äôon peut apparenter aux algorithmes √† fourmis est
l‚Äô√©cor√©solution qui a montr√© la puissance d‚Äôune heuristique de r√©solution collective bas√©e sur la perception locale,
√©vitant tout parcours explicite de graphe d‚Äô√©tats (Drogoul, 1993).
En 1992, Marco Dorigo et Luca Maria Gambardella con√ßoivent le premier algorithme bas√© sur ce paradigme pour
le c√©l√®bre probl√®me combinatoire du voyageur de commerce (Dorigo & Gambardella, 1997). Dans les algorithmes
√† base de fourmis artificielles, l‚Äôenvironnement est g√©n√©ralement repr√©sent√© par un graphe et les fourmis virtuelles
utilisent l‚Äôinformation accumul√©e sous la forme de chemins de ph√©romone d√©pos√©e sur les arcs du graphe. De
fa√ßon simple, une fourmi se contente de suivre les traces de ph√©romones d√©pos√©es pr√©c√©demment ou explore au
hasard dans le but de trouver un chemin optimal, fonction du probl√®me pos√©, dans le graphe.
Ces algorithmes offrent une bonne alternative √† tout type de r√©solution de probl√®mes mod√©lisables sous forme
d‚Äôun graphe. Ils permettent un parcours rapide et efficace et offrent des r√©sultats comparables √† ceux obtenus par
les diff√©rentes m√©thodes de r√©solution. Leur grand int√©r√™t r√©side dans leur capacit√© √† s‚Äôadapter √† un changement
de l‚Äôenvironnement. Le lecteur trouvera dans (Dorigo & St√ºtzle, 2004) ou (Monmarche et al., 2009) de bons √©tats
de l‚Äôart sur la question.
4.2 Algorithme √† colonies de fourmis et d√©sambigu√Øsation lexicale
4.2.1 Vue d‚Äôensemble
L‚Äôenvironnement des fourmis est un graphe. Il peut √™tre linguistique ‚Äì morphologique comme dans (Rouquet et al.,
2010) ou morpho-syntaxique comme dans (Schwab & Lafourcade, 2007; Guinand & Lafourcade, 2009) ‚Äì ou √™tre
simplement organis√© en fonction des √©l√©ments du texte. En fonction de l‚Äôenvironnement choisi, les r√©sultats de
l‚Äôalgorithme ne sont √©videmment pas les m√™mes. Des recherches sont actuellement men√©es √† ce sujet mais, dans
cet article, nous ne nous int√©ressons qu‚Äô√† un cas de base c.-√†-d. un graphe simple (voir fig.1), sans information
linguistique externe, afin de mieux comprendre la m√©canique de nos algorithmes.
Dans ce graphe, nous distinguons deux types de n≈ìuds : les fourmili√®res et les n≈ìuds normaux. Suivant les id√©es
d√©velopp√©es dans (Schwab, 2005) et (Guinand & Lafourcade, 2009), chaque sens possible d‚Äôun mot est associ√© √†
une fourmili√®re. Les fourmili√®res produisent des fourmis. Ces fourmis se d√©placent dans le graphe √† la recherche
d‚Äô√©nergie puis la rapportent √† leur fourmili√®re m√®re qui pourra alors cr√©er de nouvelles fourmis. Pour une fourmi,
un n≈ìud peut √™tre : (1) la fourmili√®re maison o√π elle est n√©e ; (2) une fourmili√®re ennemie qui correspond √† un
autre sens du m√™me mot ; (3) une fourmili√®re potentiellement amie, toutes celles qui ne sont pas ennemies ; (4) un
n≈ìud qui n‚Äôest pas une fourmili√®re, les n≈ìuds normaux.
Par exemple, dans la figure 1, pour une fourmi n√©e dans la fourmili√®re 19, le n≈ìud 18 est un ennemi comme il a
DIDIER SCHWAB, J√âR√îME GOULIAN, NATHAN GUILLAUME
1 Texte
2 Phrase 3 Phrase 4 Phrase
5
Mot 6 Mot 7 Mot 8 Mot Mot 10 Mot
9
11
12 13 14 15 16 17
18 19
Sens Sens Sens Sens Sens Sens Sens Sens Sens
FIGURE 1 ‚Äì Environnement utilis√© dans cette exp√©rience : texte, phrases et mots correspondent aux n≈ìuds dits
normaux 1 √† 10, un sens de mot correspond √† √† une fourmili√®re (n≈ìuds 11 √† 19).
le m√™me p√®re (10), les fourmili√®res potentiellement amies sont les n≈ìuds 11 √† 17 et les n≈ìuds normaux sont les
n≈ìuds 1 √† 10.
Les d√©placements des fourmis se d√©roulent en fonction des scores locaux (cf. section 2.2), de la pr√©sence d‚Äô√©nergie,
et du passage des autres fourmis (Les fourmis laissent des traces sur les arcs o√π elles passent sous la forme de
ph√©romone). Une fois arriv√©e sur la fourmili√®re d‚Äôun autre terme, une fourmi peut choisir de revenir directement
√† sa fourmili√®re m√®re. Elle √©tablit alors, entre les deux fourmili√®res, un pont que les autres fourmis sont, √† leur
tour, susceptibles d‚Äôemprunter et de renforcer gr√¢ce √† leur ph√©romone. Ce renforcement a lieu si les informations
lexicales conduisent les autres fourmis √† emprunter le pont et dispara√Æt dans le cas inverse. Ainsi, les fourmis
√©tablissent de nombreux liens entre fourmili√®res de sens compatibles.
Les ponts correspondent ainsi √† des interpr√©tations de la phrase. L‚Äô√©mergence de tels circuits dans le graphe
contribue √† la monopolisation des ressources de la colonie (fourmis et √©nergie) et √† l‚Äô√©puisement des ressources
associ√©es aux autres fourmili√®res (ces cas correspondent donc aux sens incompatibles dans le contexte et avec les
ressources consid√©r√©s).
4.2.2 D√©tails de l‚Äôalgorithme
√ânergie Au d√©but de la simulation, le syst√®me poss√®de une certaine √©nergie qui est r√©partie √©quitablement sur
chacun des n≈ìuds. Les fourmili√®res utilisent celle qu‚Äôelles poss√®dent pour fabriquer des fourmis avec une probabi-
lit√© fonction de cette m√™me √©nergie et suivant une courbe sigmo√Øde (cf. fig. 2). On peut remarquer que l‚Äôutilisation
de cette fonction permet aux fourmili√®res qui n‚Äôont plus d‚Äô√©nergie de fabriquer quelques fourmis suppl√©men-
taires (et ainsi d‚Äôavoir une quantit√© d‚Äô√©nergie n√©gative). L‚Äôid√©e est de leur donner une derni√®re chance au cas o√π
ces fourmis, trouvant des informations lexicales pertinentes, rapportent de l‚Äô√©nergie et relancent la production de
fourmis.
Les fourmis ont une dur√©e de vie (nombre de cycles identique pour toutes et param√©tr√© (cf. tableau 4.2.2)). Lors-
qu‚Äôune fourmi meurt, l‚Äô√©nergie qu‚Äôelle porte ainsi que l‚Äô√©nergie utilis√©e par la fourmili√®re pour la produire est
d√©pos√©e sur le n≈ìud o√π elle se trouve. Il n‚Äôy a donc ni perte ni apport d‚Äô√©nergie √† aucun moment que ce soit. Si
on excepte l‚Äôemprunt √† la nature que peuvent faire de fa√ßon tr√®s limit√©e les fourmili√®res, le syst√®me fonctionne
compl√®tement en vase clos. La quantit√© d‚Äô√©nergie est un √©l√©ment fondamental de la convergence du syst√®me vers
une solution. En effet, puisque l‚Äô√©nergie globale est limit√©e, les fourmili√®res sont en concurrence les unes avec les
PROPAGATION DE MESURES S√âMANTIQUES LOCALES PAR ALGORITHMES √Ä COLONIES DE FOURMIS
FIGURE 2 ‚Äì Courbe de la fonction sigmo√Øde arctan(x)pi +
1
2 qui permet de calculer la probabilit√© de la naissance
d‚Äôune fourmi √† partir de la quantit√© d‚Äô√©nergie pr√©sente sur le n≈ìud.
autres et seules des alliances peuvent permettre de faire √©merger des solutions.
Ph√©romone de passage Les fourmis ont deux types de comportement. Elles peuvent soit chercher de l‚Äô√©nergie,
soit chercher √† revenir √† leur fourmili√®re m√®re. Lorsqu‚Äôelles se d√©placent dans le graphe, elles laissent des traces sur
les arcs o√π elles passent sous la forme de ph√©romone. La ph√©romone influe sur les d√©placements des fourmis qui
pr√©f√®rent l‚Äô√©viter lorsqu‚Äôelles cherchent de l‚Äô√©nergie et pr√©f√®rent la suivre lorsqu‚Äôelles tentent de revenir d√©poser
cette √©nergie √† leur fourmili√®re m√®re.
Lors d‚Äôun d√©placement, une fourmi laisse une trace en d√©posant sur l‚Äôarc A travers√© une quantit√© de ph√©romone
Œ∏ ‚àà IR+. On a alors œÜt+1(A) = œÜt(A) + Œ∏.
√Ä chaque cycle, il y a une l√©g√®re √©vaporation des ph√©romones. Cette baisse se fait de fa√ßon lin√©aire jusqu‚Äô√† la
disparition totale de la ph√©romone. Nous avons ainsi, œÜc+1(A) = œÜc(A) √ó (1 ‚àí Œ¥) o√π Œ¥ est la proportion de
ph√©romone qui s‚Äô√©vapore √† chaque cycle.
Cr√©ation, suppression et type de ponts Un pont peut √™tre cr√©√© lorsqu‚Äôune fourmi atteint une fourmili√®re po-
tentiellement amie, c.-√†-d. lorsqu‚Äôelle arrive sur un n≈ìud qui correspond √† un sens d‚Äôun autre mot que celui de
la fourmili√®re m√®re. Dans ce cas, la fourmi √©value non seulement les n≈ìuds li√©s √† cette fourmili√®re mais aussi le
n≈ìud correspondant √† sa fourmili√®re m√®re. Si ce dernier est s√©lectionn√©, il y a cr√©ation d‚Äôun pont entre les deux
fourmili√®res. Ce pont est ensuite consid√©r√© comme un arc standard par les fourmis, c.-√†-d. que les n≈ìuds qu‚Äôil lie
sont consid√©r√©s comme voisins. Si le pont ne porte plus de ph√©romone, il dispara√Æt.
Odeur L‚Äôodeur d‚Äôune fourmili√®re est la repr√©sentation vectorielle que nous avons introduite dans la partie 2.2.2.
Elle correspond donc √† la d√©finition du sens sous forme de vecteur de nombres entiers. Chaque fourmi n√©e dans
cette fourmili√®re porte la m√™me odeur, le m√™me vecteur. Lors de son d√©placement sur les n≈ìuds normaux du
graphe, une fourmi propage son vecteur. Le vecteur V(N) port√© par un n≈ìud normal N est modifi√© lors du passage
d‚Äôune fourmi. La fourmi d√©pose une partie de son vecteur, un pourcentage des composantes prises au hasard qui
remplace la m√™me quantit√© d‚Äôanciennes valeurs elles aussi choisies au hasard.
Cette propagation intervient dans le d√©placement des fourmis. Laisser une partie de son vecteur, c‚Äôest laisser une
trace de passage. Ainsi plus un n≈ìud est proche d‚Äôune fourmili√®re plus il y a de chance que les fourmis de cette
fourmili√®re y soient pass√©es. Ce ph√©nom√®ne permet aux fourmis de revenir √† leur fourmili√®re, ou √©ventuellement de
se tromper et de se diriger vers des fourmili√®res amies. Cette erreur est ainsi potentiellement b√©n√©fique puisqu‚Äôelle
DIDIER SCHWAB, J√âR√îME GOULIAN, NATHAN GUILLAUME
peut permettre de cr√©er un pont entre les deux fourmili√®res (cf. 4.2.2). En revanche, lorsqu‚Äôune fourmi se trouve
sur une fourmili√®re, le vecteur n‚Äôest pas modifi√©. Ces n≈ìuds conservent ainsi un vecteur constant tout au long de
la simulation.
La table suivante pr√©sente les param√®tres, les notations et les valeurs utilis√©es dans l‚Äôalgorithme pr√©sent√© et exp√©-
riment√© ici. Cet article ne pr√©sente pas les exp√©riences r√©alis√©es pour trouver ces valeurs.
Notation Description Valeurs
FA Fourmili√®re correspondant au sensA na
V (X) Vecteur odeur associ√© √† X. X est un n≈ìud ou une fourmi na
fA Fourmi n√©e dans la fourmili√®re FA na
Ef √ânergie utilis√©e par une fourmili√®re pour produire une fourmi na
E(X) √ânergie poss√©d√©e par X. X est un n≈ìud ou une fourmi na
Emax √ânergie maximale que peut porter une fourmi 5
œÜ(A) Quantit√© de ph√©romone sur l‚ÄôarcA na
Œ∏ Ph√©romone d√©pos√©e par une fourmi lors de la travers√©e d‚Äôun arc 1
Œ¥ √âvaporation de la ph√©romone entre chaque cycle 20%
Evalf (X) √âvaluation de X selon la fourmi f. X est un arc ou un n≈ìud na
Evalf (N,A) √âvaluation du n≈ìudN en passant par l‚ÄôarcA selon la fourmi f na
Nombre de cycles de la simulation 100
Quantit√© initiale d‚Äô√©nergie sur chaque n≈ìud 20
Dur√©e de vie d‚Äôune fourmi 10
√ânergie prise par une fourmi lorsqu‚Äôelle arrive sur un n≈ìud 1
Longueur du vecteur odeur 50
Quantit√© du vecteur odeur modifi√© par une fourmi lorsqu‚Äôelle arrive sur un n≈ìud 10%
4.2.3 D√©roulement de l‚Äôalgorithme
L‚Äôalgorithme consiste en une it√©ration potentiellement infinie de cycles. √Ä tout moment, la simulation peut √™tre
interrompue et l‚Äô√©tat courant observ√©. Durant un cycle, on effectue les t√¢ches suivantes : (1) √©liminer les fourmis
trop vieilles (la dur√©e de vie est un param√®tre) ; (2) pour chaque fourmili√®re, solliciter la production d‚Äôune fourmi
(une fourmi peut ou non voir le jour, de fa√ßon probabiliste) ; (3) pour chaque arc, diminuer le taux de ph√©romone
(√©vaporation des traces) ; (4) pour chaque fourmi : d√©terminer son mode (recherche d‚Äô√©nergie, retour √† la fourmi-
li√®re, le changement est fait de mani√®re probabiliste) et la d√©placer. Cr√©er un pont interpr√©tatif le cas √©ch√©ant ; (5)
calculer les cons√©quences du d√©placement des fourmis (sur l‚Äôactivation des arcs et l‚Äô√©nergie des n≈ìuds).
Les d√©placements d‚Äôune fourmi sont al√©atoires mais influenc√©s par son environnement. Lorsqu‚Äôune fourmi est sur
un n≈ìud, elle estime tous les n≈ìuds voisins et tous les arcs qui les lient. La probabilit√© d‚Äôemprunter un arcAj pour
aller √† un n≈ìud Ni est P (Ni, Aj) = max(P Evalf (Ni,Aj)k=n,l=m , ) o√π Eval ‚Äô√©valuation du n≈ìud
k=1,l=1 Evalf (Nk,A
f (N,A) est l
l)
N en prenant l‚Äôarc A, c.-√†-d. la somme de Evalf (N) et de Evalf (A).  permet √† certaines fourmis de choisir
des destinations √©valu√©es comme improbables mais qui permettraient d‚Äôatteindre des informations lexicales et des
ressources qui s‚Äôav√®reraient int√©ressantes ensuite.
Une fourmi qui vient de na√Ætre (c.-√†-d. √™tre produite par sa fourmili√®re) part √† la recherche d‚Äô√©nergie. Elle est attir√©e
par les n≈ìuds qui portent beaucoup d‚Äô√©nergie (Evalf (N) = PE(N)m
0 E(Ni)
) et √©vite les arcs qui portent beaucoup de
ph√©romone (Evalf (A) = 1‚àíœÜ(A)) afin de permettre l‚Äôexploration de plus de solutions. Elle continue √† collecter
de l‚Äô√©nergie jusqu‚Äôau cycle o√π un tirage al√©atoire avec la probabilit√© P (retour) = E(f)E la fera passer en modemax
retour. Dans ce mode, elle va (statistiquement) suivre les arcs avec beaucoup de ph√©romone (Evalf (A) = œÜ(A))
et vers les n≈ìuds dont l‚Äôodeur est proche de la leur (Evalf (N) = P Lesk(V (N),V (fA))i=k ).
i=1 Lesk(V (Ni),V (fA))
PROPAGATION DE MESURES S√âMANTIQUES LOCALES PAR ALGORITHMES √Ä COLONIES DE FOURMIS
5 √âvaluation
Nous avons test√© notre m√©thode sur le corpus de la t√¢che gros grain de la campagne d‚Äô√©valuation Semeval
2007 (Navigli et al., 2007) dans laquelle les organisateurs fournissent un inventaire de sens plus grossiers que
ceux de WordNet. Pour chaque terme, les sens consid√©r√©s comme proches (par exemple, "neige/pr√©cipitation" et
"neige/couverture" ou "porc/animal" et "porc/viande") sont group√©s. Le corpus est compos√© de 5 textes de genres
divers (journalisme, critique litt√©raire, voyage, informatique, biographies) dont il faut annoter les 2269 mots. Le
nombre moyen de sens par mot est de 6,19 ; ramen√© √† 3,1 pour l‚Äôinventaire de sens grossiers. Les comp√©titeurs
√©taient libres de se servir de cet inventaire (sens grossiers connus a priori) ou non (sens grossiers connus a poste-
riori). Dans le premier cas, le nombre de choix √† faire pour chaque mot est r√©duit et la t√¢che moins compliqu√©e.
Dans le second cas, les sens annot√©s sont jug√©s corrects s‚Äôils sont dans le bon groupement, une sorte d‚Äôerreur
acceptable. Notre objectif est de tester un syst√®me en vue d‚Äôune utilisation dans un cadre applicatif r√©el or l‚Äôin-
ventaire de sens grossiers n‚Äôest disponible que pour les 2269 mots utilis√©s dans le corpus d‚Äô√©valuation, nous ne
l‚Äôutilisons donc pas. Dans les exp√©riences pr√©sent√©es ici, nous nous situons ainsi dans un cas de sens connus a
posteriori. Les r√©sultats sont analys√©s par les formules classiques :
Pr√©cisionP = sens correctement annot√©ssens annot√©s RappelR =
sens correctement annot√©s
sens √† annoter F-mesureF =
2√óP√óR
P+R
Dans le corpus, les mots sont annot√©s avec leur partie du discours (verbe, nom, adverbe, adjectif). √Ä partir de ces
informations, nous construisons l‚Äôenvironnement des fourmis : un n≈ìud au niveau du texte, un n≈ìud pour chaque
phrase, un n≈ìud pour chaque mot et une fourmili√®re pour chaque sens (voir fig. 1). √Ä la fin d‚Äôun cycle, le sens
s√©lectionn√© pour chaque mot correspond √† la fourmili√®re qui a la plus grande quantit√© d‚Äô√©nergie.
5.1 Ex√©cution de l‚Äôalgorithme
L‚Äôalgorithme √† colonies de fourmis garantit la r√©alisation d‚Äôun choix entre les diff√©rentes possibilit√©s pour chaque
terme. Ainsi, 100% du corpus est annot√© et P=R=F puisque les sens annot√©s sont √©gaux aux sens √† annoter (P=R)
et dans ce cas F = 2√óP√óPP+P =
2√óP 2
2P = P . De plus, un algorithme √† colonies de fourmis est un algorithme
stochastique, il ne s√©lectionne donc pas exactement les m√™mes sens √† chaque ex√©cution ni m√™me √† chaque cycle.
Nous avons ex√©cut√© cet algorithme des centaines de fois et avons not√© qu‚Äôapr√®s 70-80 cycles, les r√©sultats restaient
globalement constants comme l‚Äôillustre la figure suivante.
75
73
71
69
67
Lesk
65 Lesk √©tendu
63
61
59
Cycles
FIGURE 3 ‚Äì √âvolution de la pr√©cision/rappel/F-mesure dans les 100 cycles d‚Äôune ex√©cution de l‚Äôalgorithme √†
colonies de fourmis utilis√© avec l‚Äôalgorithme local Lesk et avec l‚Äôalgorithme local Lesk √©tendu
Pr√©cision/Rappel/F-mesure (%)
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
DIDIER SCHWAB, J√âR√îME GOULIAN, NATHAN GUILLAUME
5.2 Comparaison d‚Äôex√©cutions
De la m√™me mani√®re que les r√©sultats √©voluent entre deux cycles, les r√©sultats peuvent √™tre diff√©rents entre deux
ex√©cutions. Pour donner une id√©e de cette diff√©rence, nous avons r√©p√©t√© notre exp√©rience, arr√™t√©e au bout de 100
cycles, sur chacun des algorithmes locaux, 100 fois. La table suivante en pr√©sente les r√©sultats. Nous obtenons
seulement 2, 95% d‚Äô√©cart entre le meilleur et le moins bon r√©sultat (soit 67 termes mal annot√©s sur 2269) pour
Lesk √©tendu et 3,39% (soit 77 termes mal annot√©s sur 2269) pour Lesk.
Algorithme local Minimum Maximum Moyenne M√©diane √âtendue √âcart-type
Lesk 64,43 67,83 66,34 66,35 3,39 0,66
Lesk √©tendu 72,54 75,5 74,01 74,04 2,95 0,58
5.3 Comparaisons avec l‚Äôalgorithme exhaustif
√Ä titre de comparaison avec notre approche, nous pr√©sentons les r√©sultats obtenus par l‚Äôalgorithme global exhaustif
(Banerjee & Pedersen, 2002). Nous avons choisi comme contexte la phrase, excluant de facto les phrases d‚Äôun mot
(au nombre de quatre, soit moins de 0, 002% du corpus). Pour des raisons calculatoires, nous avons √©galement
exclu les phrases de plus de 10 milliards de combinaisons. Nous pouvons voir que seulement 77, 3% du corpus a
Algorithme global Algorithme local √âtiquet√©s Pr√©cision Rappel F-mesure Temps
Calcul exhaustif Lesk 77,30 69,21 53,50 60,35 ‚âà 40hLesk √©tendu 77,30 77,82 60,16 67,86 ‚âà 300h
Fourmis Lesk 100,0 64,43 - 67,83 64,43 - 67,83 64,43 - 67,83 ‚âà 3mLesk √©tendu 100,0 72,54 - 75,5 72,54 - 75,5 72,54 - 75,5 ‚âà 8m
√©t√© annot√© au prix d‚Äôune dur√©e de plusieurs heures incompatible avec des applications en temps r√©el 4.
Pour les deux algorithmes locaux, la F-mesure est clairement sup√©rieure √† celle du calcul brut pour un temps
nettement moins long (800 fois plus court pour Lesk et 2250 fois pour Lesk √©tendu). Le tableau suivant pr√©sente
pour les m√™mes ex√©cutions les r√©sultats sur les diff√©rentes sous-parties du corpus : A, la partie annot√© par les 2
algorithmes globaux et B celle qui n‚Äôest annot√©e que par l‚Äôalgorithme fourmis. Sur la partie A, les fourmis sont,
comme on pouvait s‚Äôen douter, l√©g√®rement en dessous de l‚Äôalgorithme exhaustif et leur meilleur r√©sultat s‚Äôexplique
par la possibilit√© d‚Äôannoter la sous-partie B.
Pour conclure cette √©valuation, nous avons compar√© nos r√©sultats avec les r√©sultats obtenus par les diff√©rents
syst√®mes qui participaient √† la campagne Semeval 2007. Avec Lesk √©tendu, nous serions arriv√©s 8√®me/15 en tenant
compte de tous les participants, 5√®me/8 sur ceux qui ne connaissent pas a priori les sens grossiers, 1er/7 sur
les approches non supervis√©es. Ces r√©sultats sont tr√®s encourageants vu les temps de calcul (aucun article des
participants n‚Äôaborde ce point), les possibilit√©s d‚Äôextension qu‚Äôoffrent les algorithmes √† fourmis et la simplicit√©
des algorithmes locaux envisag√©s ici.
4. Exp√©riences r√©alis√©es sur des processeurs Intel Xeon X5550, 4 c≈ìurs √† 2.66Ghz (dur√©es converties en temps monoprocesseurs).
PROPAGATION DE MESURES S√âMANTIQUES LOCALES PAR ALGORITHMES √Ä COLONIES DE FOURMIS
Algorithme local Sous-corpus Algorithme global √âtiquet√©s Rappel Diff√©rentiel
A + B Exhaustif 77,30 53,50Fourmis 100,0 64,43 - 67,83 + 10,93 √† + 14,33
Lesk A Exhaustif 100,0 69,21Fourmis 100,0 65,45 - 68,99 - 3,76 √† - 0,22
B Exhaustif 00,00 00,00Fourmis 100,0 60,97 - 63,88 + 60,97 √† + 63,88
A + B Exhaustif 77,30 60,16Fourmis 100,0 72,54 - 75,5 + 12,38 √† + 15,34
Lesk √©tendu A Exhaustif 100,0 77,82Fourmis 100,0 74,69 - 77,25 - 3,13 √† - 0,57
B Exhaustif 00,00 00,00Fourmis 100,0 65,24 - 69,52 + 65,24 √† + 69,52
6 Conclusions et Perspectives
Dans cet article, nous avons pr√©sent√© un algorithme √† colonies de fourmis destin√© √† la d√©sambigu√Øsation lexicale et
bas√© sur des mesures de proximit√© s√©mantique. Cet algorithme, non supervis√©, est volontairement simple puisqu‚Äôil
n‚Äôutilise qu‚Äôune seule ressource lexicale (WordNet) et aucune analyse morphologique ou morpho-syntaxique. Il
permet pourtant de choisir un sens, pour chaque mot d‚Äôun texte, d‚Äôune mani√®re plus rapide que l‚Äôalgorithme
exhaustif et en atteignant une bonne F-mesure pour un syst√®me non supervis√©. Nous consid√©rons ces r√©sultats
comme une ligne de base (baseline) √† partir de laquelle nous allons poursuivre nos recherches. Outre l‚Äôajout
d‚Äôinformations morphologiques et/ou syntaxiques, nous travaillons actuellement sur la combinaison de mesures
locales et l‚Äôutilisation de WordNet dans l‚Äôenvironnement des fourmis. Nos travaux portent √©galement sur d‚Äôautres
algorithmes locaux et leur impact sur l‚Äôutilisation dans d‚Äôautres langues notamment flexionnelles. Enfin, nous
travaillons √† la comparaison des algorithmes √† colonies de fourmis avec d‚Äôautres algorithmes globaux comme les
algorithmes g√©n√©tiques ou le recuit simul√©.
R√©f√©rences
AGIRRE E. & EDMONDS P. (2006). Word Sense Disambiguation : Algorithms and Applications (Text, Speech
and Language Technology). Secaucus, NJ, USA : Springer-Verlag New York, Inc.
BANERJEE S. & PEDERSEN T. (2002). An adapted lesk algorithm for word sense disambiguation using wordnet.
In the Third International Conference on Intelligent Text Processing and Computational Linguistics, CICLing
2002, Mexico City.
BONABEAU √â. & TH√âRAULAZ G. (2000). L‚Äôintelligence en essaim. Pour la science, (271), 66‚Äì73.
COWIE J., GUTHRIE J. & GUTHRIE L. (1992). Lexical disambiguation using simulated annealing. In COLING
1992, International Conference on Computational Linguistics, volume 1, p. 359‚Äì365, Nantes, France.
CRAMER I., WANDMACHER T. & WALTINGER U. (2010). WordNet : An electronic lexical database, chapter
Modeling, Learning and Processing of Text Technological Data Structures. Springer.
DEERWESTER S. C., DUMAIS S. T., LANDAUER T. K., FURNAS G. W. & HARSHMAN R. A. (1990). Indexing
by latent semantic analysis. Journal of the American Society of Information Science, 41(6).
DENEUBOURG J.-L., GROSS S., FRANKS N. & PASTEELS J.-M. (1989). The blind leading the blind : Mode-
ling chemically mediated army ant raid patterns. Journal of Insect Behavior, 2, 719‚Äì725.
DORIGO & ST√úTZLE (2004). Ant Colony Optimization. MIT-Press.
DIDIER SCHWAB, J√âR√îME GOULIAN, NATHAN GUILLAUME
DORIGO M. & GAMBARDELLA L. (1997). Ant colony system : A cooperative learning approach to the traveling
salesman problem. IEEE Transactions on Evolutionary Computation, 1, 53‚Äì66.
DROGOUL A. (1993). When ants play chess (or can strategies emerge from tactical behaviors). In Maa-
maw‚Äô1993.
FELLBAUM C. (1998). WordNet : An Electronic Lexical Database (Language, Speech, and Communication).
The MIT Press.
GALE W., CHURCH K. & YAROWSKY D. (1992). One sense per discourse. In Fifth DARPA Speech and Natural
Language Workshop, p. 233‚Äì237, Harriman, New-York, √âtats-Unis.
GELBUKH A., SIDOROV G. & HAN S. Y. (2003). Evolutionary approach to natural language word sense
disambiguation through global coherence optimization. WSEAS Transactions on Communications, 2(1), 11‚Äì19.
GUINAND F. & LAFOURCADE M. (2009). Fourmis Artificielles 2. Nouvelles Directions pour une Intelligence
Collective, chapter Fourmis Artificielles et Traitement de la Langue Naturelle, p. 225‚Äì267. Lavoisier.
IDE N. & V√âRONIS J. (1998). Word sense disambiguation : the state of the art. Computational Linguistics,
28(1), 1‚Äì41.
LESK M. (1986). Automatic sense disambiguation using machine readable dictionaries : how to tell a pine cone
from an ice cream cone. In Proceedings of the 5th annual international conference on Systems documentation,
SIGDOC ‚Äô86, p. 24‚Äì26, New York, NY, USA : ACM.
N. MONMARCHE, F. GUINAND & P. SIARRY, Eds. (2009). Fourmis Artificielles et Traitement de la Langue
Naturelle. Prague, Czech Republic : Lavoisier.
NAVIGLI R. (2009). Word sense disambiguation : a survey. ACM Computing Surveys, 41(2), 1‚Äì69.
NAVIGLI R. & LAPATA M. (2010). An experimental study of graph connectivity for unsupervised word sense
disambiguation. IEEE Trans. Pattern Anal. Mach. Intell., p. 678‚Äì692.
NAVIGLI R., LITKOWSKI K. C. & HARGRAVES O. (2007). Semeval-2007 task 07 : Coarse-grained english
all-words task. In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007),
p. 30‚Äì35, Prague, Czech Republic : Association for Computational Linguistics.
PEDERSEN T., BANERJEE S. & PATWARDHAN S. (2005). Maximizing Semantic Relatedness to Perform Word
Sense Disambiguation. Research Report UMSI 2005/25, University of Minnesota Supercomputing Institute.
PONZETTO S. P. & NAVIGLI R. (2010). Knowledge-rich word sense disambiguation rivaling supervised sys-
tems. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ‚Äô10, p.
1522‚Äì1531, Stroudsburg, PA, USA : Association for Computational Linguistics.
ROUQUET D., FALAISE A., SCHWAB D., BOITET C., BELLYNCK V., NGUYEN H.-T., MANGEOT M. &
GUILBAUD J.-P. (2010). Rapport final de synth√®se, passage √† l‚Äô√©chelle et impl√©mentation : Extraction de
contenu s√©mantique dans des masses de donn√©es textuelles multilingues. Rapport interne, Agence Nationale
de la Recherche.
SCHWAB D. (2005). Approche hybride - lexicale et th√©matique - pour la mod√©lisation, la d√©tection et l‚Äôex-
ploitation des fonctions lexicales en vue de l‚Äôanalyse s√©mantique de texte. PhD thesis, Universit√© Montpellier
2.
SCHWAB D. & LAFOURCADE M. (2007). Lexical functions for ants based semantic analysis. In ICAI‚Äô07- The
2007 International Conference on Artificial Intelligence, Las Vegas, Nevada, USA.
VASILESCU F., LANGLAIS P. & LAPALME G. (2004). Evaluating variants of the lesk approach for disambi-
guating words. In Proceedings of LREC 2004, the 4th International Conference On Language Resources And
Evaluation, p. 633‚Äì636, Lisbon, Portugal.
