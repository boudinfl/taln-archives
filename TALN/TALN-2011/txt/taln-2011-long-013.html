<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
"http://www.w3.org/TR/html4/loose.dtd">
<html><head><title>Degr&#233; de comparabilit&#233;, extraction lexicale bilingue et recherche d&#8217;information interlingue</title>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
</head>
<body>
<div style="page-break-before:always; page-break-after:always"><div><p>TALN 2011, Montpellier, 27 juin &#8211;1er juillet 2011
</p>
<p>Degr&#233; de comparabilit&#233;, extraction lexicale bilingue et recherche
d&#8217;information interlingue
</p>
<p>Bo Li1 Eric Gaussier1 Emmanuel Morin2 Amir Hazem2
(1) Universit&#233; Grenoble I, LIG UMR 5217
(2) LINA, UMR 6241,Universit&#233; de Nantes
</p>
<p>{bo.li,eric.gaussier}@imag.fr, {emmanuel.morin,amir.hazem}@univ-nantes.fr
</p>
<p>R&#233;sum&#233;. Nous &#233;tudions dans cet article le probl&#232;me de la comparabilit&#233; des documents composant un cor-
pus comparable afin d&#8217;am&#233;liorer la qualit&#233; des lexiques bilingues extraits et les performances des syst&#232;mes de
recherche d&#8217;information interlingue. Nous proposons une nouvelle approche qui permet de garantir un certain
degr&#233; de comparabilit&#233; et d&#8217;homog&#233;n&#233;it&#233; du corpus tout en pr&#233;servant une grande part du vocabulaire du corpus
d&#8217;origine. Nos exp&#233;riences montrent que les lexiques bilingues que nous obtenons sont d&#8217;une meilleure qualit&#233;
que ceux obtenus avec les approches pr&#233;c&#233;dentes, et qu&#8217;ils peuvent &#234;tre utilis&#233;s pour am&#233;liorer significativement
les syst&#232;mes de recherche d&#8217;information interlingue.
</p>
<p>Abstract. We study in this paper the problem of enhancing the comparability of bilingual corpora in order
to improve the quality of bilingual lexicons extracted from comparable corpora and the performance of cross-
language information retrieval (CLIR) systems. We introduce a new method for enhancing corpus comparability
which guarantees a certain degree of comparability and homogeneity, and still preserves most of the vocabulary
of the original corpus. Our experiments illustrate the well-foundedness of this method and show that the bilingual
lexicons obtained are of better quality than the lexicons obtained with previous approaches, and that they can be
used to significantly improve CLIR systems
</p>
<p>Mots-cl&#233;s : Corpus comparables, comparabilit&#233;, lexiques bilingues, recherche d&#8217;information interlingue.
</p>
<p>Keywords: Comparable corpora, comparability, bilingual lexicon, cross-language information retrieval.
</p>
<p>1 Introduction
</p>
<p>Les lexiques bilingues sont une ressource incontournable dans diff&#233;rentes applications multilingues du traitement
automatique des langues comme la traduction automatique (Och &amp; Ney, 2003) ou la recherche d&#8217;information in-
terlingue (Ballesteros &amp; Croft, 1997). Dans la mesure o&#249; la constitution manuelle de lexiques bilingues est une
t&#226;che co&#251;teuse et qu&#8217;il est difficilement envisageable de d&#233;velopper un lexique pour chaque domaine d&#8217;&#233;tude, les
recherches se sont int&#233;ress&#233;es &#224; l&#8217;extraction automatique de ces lexiques &#224; partir de corpus. Dans la mesure o&#249; la
plupart des corpus bilingues existants sont par essence comparables, c&#8217;est-&#224;-dire qu&#8217;ils regroupent des documents
dans des langues diff&#233;rentes traitant du m&#234;me domaine sur la m&#234;me p&#233;riode sans &#234;tre en relation de traduc-
tion, diff&#233;rents travaux s&#8217;int&#233;ressent &#224; l&#8217;extraction de lexiques bilingues &#224; partir de corpus comparables (Fung &amp;
McKeown, 1997; Fung &amp; Yee, 1998; Rapp, 1999; D&#233;jean et al., 2002; Gaussier et al., 2004; Robitaille et al.,
2006; Morin et al., 2007; Garera et al., 2009; Yu &amp; Tsujii, 2009; Shezaf &amp; Rappoport, 2010, entre autres). Le</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>socle commun &#224; ces travaux est de reposer sur une hypoth&#232;se de distribution qui postule que les mots qui sont en
correspondance de traduction sont susceptibles d&#8217;appara&#238;tre dans des contextes identiques pour des langues dif-
f&#233;rentes. En s&#8217;appuyant sur cette hypoth&#232;se fondatrice, les chercheurs ont aussi cherch&#233; &#224; identifier de meilleures
repr&#233;sentations pour le contexte des mots de m&#234;me qu&#8217;&#224; utiliser diff&#233;rentes m&#233;thodes pour mettre en correspon-
dance les mots entre diff&#233;rentes langues toujours en s&#8217;appuyant sur cette repr&#233;sentation du contexte. Ces m&#233;thodes
semblent avoir atteint leur limite en termes de performance et les am&#233;liorations les plus r&#233;centes concernent plus
le cadre d&#8217;&#233;valuation des ces approches, plus contraint et limit&#233; (Yu &amp; Tsujii, 2009), ou encore le traitement
de langues sp&#233;cifiques (Shezaf &amp; Rappoport, 2010). Plus r&#233;cemment, et en s&#8217;&#233;loignant des approches tradition-
nelles, Li &amp; Gaussier (2010) ont propos&#233; une approche bas&#233;e sur l&#8217;am&#233;lioration de la comparabilit&#233; des corpus
comme pr&#233;alable &#224; l&#8217;extraction lexicale bilingue. Cette approche postule qu&#8217;il ne sert &#224; rien d&#8217;essayer d&#8217;extraire
des lexiques bilingues &#224; partir d&#8217;un corpus avec un faible degr&#233; de comparabilit&#233; puisque la probabilit&#233; de trouver
des traductions d&#8217;un mot donn&#233; sera faible dans une telle situation. Notre &#233;tude se situe dans la m&#234;me veine que
cette pr&#233;c&#233;dente approche et vise dans un premier temps &#224; am&#233;liorer la comparabilit&#233; d&#8217;un corpus donn&#233;, tout
en pr&#233;servant une large part de son vocabulaire. N&#233;anmoins, nous nous diff&#233;rencions de ce pr&#233;c&#233;dent travail en
montrant qu&#8217;il est possible de garantir un certain degr&#233; d&#8217;homog&#233;n&#233;it&#233; du corpus am&#233;lior&#233;, et que celle-ci induit
une am&#233;lioration significative de la qualit&#233; du corpus r&#233;sultant et des lexiques bilingues extraits. En outre, nous
montrons que les lexiques extraits avec notre approche am&#233;liorent de mani&#232;re manifeste les r&#233;sultats d&#8217;un syst&#232;me
de recherche d&#8217;information interlingue, m&#234;me lorsque ces lexiques sont issus d&#8217;un corpus diff&#233;rent de la collection
interrog&#233;e.
</p>
<p>2 Am&#233;liorer le degr&#233; de comparabilit&#233; d&#8217;un corpus
</p>
<p>Nous commen&#231;ons par donner dans cette partie la mesure de comparabilit&#233; que nous utilisons, avant de d&#233;crire un
algorithme permettant d&#8217;am&#233;liorer la comparabilit&#233; d&#8217;un corpus donn&#233;. Nous fournissons &#233;galement une preuve
du bien-fond&#233; de notre algorithme, ainsi qu&#8217;une approximation conduisant &#224; une implantation efficace. Pour des
raisons pratiques, notre discussion se fera sur la base du couple de langues anglais-fran&#231;ais.
</p>
<p>2.1 Mesure de comparabilit&#233;
</p>
<p>Afin de mesurer le degr&#233; de comparabilit&#233; d&#8217;un corpus bilingue, nous utilisons la mesure d&#233;velopp&#233;e dans (Li &amp;
Gaussier, 2010) : &#233;tant donn&#233; un corpus comparable P constitu&#233; d&#8217;une partie anglaise Pe et d&#8217;une partie fran&#231;aise
Pf , le degr&#233; de comparabilit&#233; de P est d&#233;fini comme l&#8217;esp&#233;rance de trouver la traduction d&#8217;un mot du vocabulaire
source (respectivement cible) dans le vocabulaire cible (respectivement source). Soit &#963; une fonction indiquant si
une traduction de l&#8217;ensemble des traductions possibles Tw du mot w se trouve dans le vocabulaire Pv du corpus
P , c&#8217;est-&#224;-dire :
</p>
<p>&#963;(w,P) =
{
</p>
<p>1 si Tw &#8745; Pv 6= &#8709;
0 sinon
</p>
<p>et soitD un dictionnaire bilingue dont le vocabulaire anglais (respectivement fran&#231;ais) est not&#233;De (respectivement
Df ). La mesure du degr&#233; de comparabilit&#233; M est d&#233;finie par :
</p>
<p>M(Pe,Pf ) =
P
</p>
<p>w&#8712;Pe&#8745;De &#963;(w,Pf ) +
P
</p>
<p>w&#8712;Pf&#8745;Df &#963;(w,Pe)
#w(Pe &#8745; De) + #w(Pf &#8745; Df )</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>o&#249; #w(P) repr&#233;sente le nombre de mots diff&#233;rents pr&#233;sents dans P . Comme on peut le voir d&#8217;apr&#232;s cette d&#233;fini-
tion, M mesure la proportion de mots source et cible dont une traduction est pr&#233;sente dans le vocabulaire cible et
source de P . Pour des raisons qui deviendront claires plus tard, nous utiliserons aussi des mesures partielles o&#249;
seuls les vocabulaires fran&#231;ais ou anglais sont consid&#233;r&#233;s. Ainsi, la proportion de mots anglais traduits sera not&#233;e
Mef , d&#233;finie par :
</p>
<p>P
w&#8712;Pe&#8745;De &#963;(w,Pf )
#w(Pe&#8745;De) . La mesure Mfe est d&#233;finie de la m&#234;me fa&#231;on.
</p>
<p>2.2 Classer les documents pour une meilleure comparabilit&#233;
</p>
<p>L&#8217;hypoth&#232;se distributionnelle sous-tendant l&#8217;extraction de lexiques bilingues est d&#8217;autant plus valide que les do-
cuments dans les diff&#233;rentes langues couvrent des th&#233;matiques proches, car les auteurs ont alors tendance &#224; puiser
dans le m&#234;me vocabulaire (voir (Morin et al., 2007) pour une analyse reli&#233;e). En d&#8217;autres termes, si un corpus
couvre un nombre limit&#233; de th&#233;matiques, il est plus &#224; m&#234;me de contenir une information r&#233;p&#233;t&#233;e et coh&#233;rente
qui pourra &#234;tre exploit&#233;e au mieux pour l&#8217;extraction de lexiques bilingues. Le terme homog&#233;n&#233;it&#233; rend compte de
ce ph&#233;nom&#232;ne et nous dirons, de fa&#231;on informelle, qu&#8217;un corpus est homog&#232;ne s&#8217;il couvre un nombre limit&#233; de
th&#233;matiques. Nous conjecturons ici que si l&#8217;on peut garantir un certain degr&#233; d&#8217;homog&#233;n&#233;it&#233;, en plus d&#8217;un certain
degr&#233; de comparabilit&#233;, alors les lexiques bilingues extraits seront de meilleure qualit&#233;. Comme nous le verrons,
cette conjecture sera valid&#233;e par les exp&#233;riences men&#233;es. De fa&#231;on &#224; garantir un certain degr&#233; d&#8217;homog&#233;n&#233;it&#233;, nous
nous appuyons sur des techniques de classification non supervis&#233;e (clustering). Nous utilisons ici des techniques
de classification agglom&#233;rative ascendante, mais toute autre technique, pour peu qu&#8217;elle dispose d&#8217;une proc&#233;dure
de filtrage adapt&#233;e, peut &#234;tre utilis&#233;e.
</p>
<p>2.2.1 Algorithme de classification bilingue
</p>
<p>L&#8217;ensemble du processus permettant de construire, &#224; partir d&#8217;un corpus donn&#233;, un corpus plus homog&#232;ne et de
plus fort degr&#233; de comparabilit&#233; peut &#234;tre r&#233;sum&#233; par les &#233;tapes suivantes :
</p>
<p>1. &#192; partir de la mesure de similarit&#233;, d&#233;finie en 2.2.2 et fond&#233;e sur la mesure de comparabilit&#233; pr&#233;sent&#233;e
ci-dessus, et de l&#8217;ensemble des documents anglais et fran&#231;ais du corpus originel P , construire les dendro-
grammes en suivant les &#233;tapes classiques de la classification agglom&#233;rative ascendante ;
</p>
<p>2. Filtrer les dendrogrammes en ne retenant que les classes les plus profondes (voir ci-dessous) ;
</p>
<p>3. Fusionner les classes retenues pour former un nouveau corpus PH , qui contient une sous-partie homog&#232;ne
et fortement comparable de P ;
</p>
<p>4. R&#233;p&#233;ter les &#233;tapes ci-dessus pour enrichir la partie restante de P (partie qui sera not&#233;e PL, PL = P \ PH )
avec des documents extraits d&#8217;autres corpus.
</p>
<p>Les trois premi&#232;res &#233;tapes sont d&#233;taill&#233;es dans l&#8217;algorithme 1, o&#249; CAA signifie Classification Agglom&#233;rative As-
cendante. Comme on peut le remarquer, seulP est utilis&#233; pour construirePH , &#224; travers des &#233;tapes de classification
et de filtrage. Ainsi, l&#8217;algorithme 1 vise &#224; extraire de P une sous-partie fortement comparable et homog&#232;ne. Une
fois cela r&#233;alis&#233;, c&#8217;est-&#224;-dire une fois que P a &#233;t&#233; exploit&#233;, il est n&#233;cessaire de recourir &#224; des ressources externes
si l&#8217;on veut construire un corpus fortement comparable &#224; partir de PL (qui est la partie restante de P). Pour cela,
deux nouveaux corpus comparables sont consid&#233;r&#233;s dans l&#8217;&#233;tape 4 du processus global : le premier consiste en la
partie anglaise de PL et la partie fran&#231;aise d&#8217;un autre corpus PT ; le second consiste en la partie fran&#231;aise de PL et
la partie anglaise de PT . Les deux sous-parties fortement comparables et homog&#232;nes obtenues &#224; partir de ces deux
corpus sont alors ajout&#233;es &#224; PH pour constituer le corpus final. L&#8217;utilisation de la classification agglom&#233;rative as-
cendante et du filtrage associ&#233; garantit que le corpus final est homog&#232;ne. La propri&#233;t&#233; 1 que nous pr&#233;sentons plus</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Algorithm 1: Algorithme de classification bilingue
Entr&#233;e :
</p>
<p>Ensemble U de tous les documents anglais et fran&#231;ais de P
R&#233;el positif &#952; (seuil de profondeur)
</p>
<p>Sortie :
PH , sous-partie fortement comparable et homog&#232;ne de P
</p>
<p>1: Initialiser PH = &#8709; ;
2: CAA(U)&#8594; ensemble S de dendrogrammes
3: for chaque dendrogramme T de S do
4: m&#8592; profondeur maximale de T ;
5: for tous les n&#339;uds n de T do
6: if profondeur(n) &#8805; m &#183; &#952; then
7: Ajouter tous les documents sous le n&#339;ud n &#224; PH ;
8: end if
9: end for
</p>
<p>10: end for
11: Supprimer les doublons de PH ;
12: return PH ;
</p>
<p>loin &#233;tablit que ce corpus est fortement comparable. Mais avant de voir en d&#233;tail cette propri&#233;t&#233;, nous introduisons
la mesure de similarit&#233; utilis&#233;e.
</p>
<p>2.2.2 Mesure de similarit&#233;
</p>
<p>Imaginons deux classes de documents bilingues C1 et C2. Pour la t&#226;che d&#8217;extraction de lexiques bilingues, ces
deux classes sont similaires et devraient &#234;tre regroup&#233;es si leur combinaison permet de compl&#233;ter le contenu de
chacune des classes prise isol&#233;ment, ou, en d&#8217;autres termes, si la partie anglaise Ce1 de C1 et la partie fran&#231;aise Cf1
de C1 sont comparables &#224; leur contrepartie dans l&#8217;autre classe (respectivement la partie fran&#231;aise Cf2 de C2 et la
partie anglaise Ce2 de C2) 1. Ceci conduit &#224; la mesure de similarit&#233; suivante pour C1 et C2 :
</p>
<p>sim(C1, C2) = &#946;M(Ce1 , Cf2 ) + (1&#8722; &#946;)M(Ce2 , Cf1 ) (1)
</p>
<p>o&#249; &#946; (0 &#8804; &#946; &#8804; 1) est un poids qui permet de contr&#244;ler l&#8217;importance de chacune des deux parties (Ce1 , Cf2 ) et (Ce2 ,
Cf1 ). De fa&#231;on intuitive, on aimerait donner plus de poids dans cette combinaison &#224; la partie la plus importante,
car elle contient plus d&#8217;information. Si nous utilisons le nombre de paires de documents anglais-fran&#231;ais pour
quantifier cette information, le poids &#946; peut &#234;tre d&#233;fini comme la proportion de paires de documents dans (Ce1 , Cf2 )
sur l&#8217;ensemble des paires de documents dans le corpus fusionn&#233; :
</p>
<p>&#946; =
#d(Ce1) &#183;#d(Cf2 )
</p>
<p>#d(Ce1) &#183;#d(Cf2 ) + #d(Ce2) &#183;#d(Cf1 )
o&#249; #d(C) repr&#233;sente le nombre de documents dans C. Dans la mesure o&#249; les classes sont tout d&#8217;abord form&#233;es
</p>
<p>de documents anglais et fran&#231;ais isol&#233;s, la mesure de similarit&#233; correspond &#224; un score de comparabilit&#233; normalis&#233;
entre les corpus anglais et fran&#231;ais qui forment la nouvelle classe. Cependant, cette mesure ne tient pas compte des
</p>
<p>1. Dans la mesure o&#249; C1 et C2 sont des classes, leurs parties anglaise et fran&#231;aise sont comparables par construction.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>longueurs relatives des corpus anglais et fran&#231;ais, qui ont pourtant un impact sur la qualit&#233; des corpus bilingues
extraits. Si une contrainte de type 1-1 (c&#8217;est-&#224;-dire imposant &#224; chaque classe de contenir le m&#234;me nombre de
documents anglais et fran&#231;ais) est trop forte, se reposer sur des classes par trop d&#233;s&#233;quilibr&#233;es n&#8217;est pas non plus
souhaitable. Nous introduisons donc une nouvelle fonction &#966; qui a pour but de p&#233;naliser les classes pour lesquelles
les nombres de documents anglais et fran&#231;ais sont trop diff&#233;rents :
</p>
<p>&#966;(C) = 1
(1 + log(1 + &#947; |#d(C
</p>
<p>e)&#8722;#d(Cf )|
min(#d(Ce)),#d(Cf ))
</p>
<p>)
(2)
</p>
<p>avec &#947; &#8712; R+. Cette fonction de p&#233;nalit&#233; fournit une nouvelle mesure de similarit&#233; siml qui est celle utilis&#233;e dans
l&#8217;algorithme 1 :
</p>
<p>siml(C1, C2) = sim(C1, C2) &#183; &#966;(C1 &#8746; C2) (3)
Dans la suite de cette &#233;tude, &#947; est fix&#233; &#224; 1 dans &#966;.
</p>
<p>2.2.3 Analyse th&#233;orique
</p>
<p>Le processus de classification utilis&#233; dans l&#8217;algorithme 1 garantit que les documents qui portent sur la m&#234;me
th&#233;matique seront regroup&#233;s avant les documents portant sur des th&#233;matiques diff&#233;rentes. Le corpus obtenu (PH )
sera ainsi homog&#232;ne, c&#8217;est-&#224;-dire qu&#8217;il ne couvrira qu&#8217;un nombre restreint de th&#233;matiques. De plus, le fait que le
corpus comparable (que nous noteronsPF ) obtenu au travers des &#233;tapes 1 &#224; 4 d&#233;coule du corpus originelP indique
que la plus grande partie du vocabulaire de P sera pr&#233;serv&#233;e dans PF . Nous verrons dans la partie exp&#233;rimentale
que c&#8217;est bien le cas. Ce qui semble moins &#233;vident, c&#8217;est le fait que le processus que nous avons d&#233;fini garantisse
un fort degr&#233; de comparabilit&#233;. La propri&#233;t&#233; suivante &#233;tablit que c&#8217;est bien le cas.
</p>
<p>Propri&#233;t&#233; 1 Soit C1 et C2 deux classes de documents qui doivent &#234;tre regroup&#233;es dans le processus de classifi-
cation. Nous faisons l&#8217;hypoth&#232;se que le dictionnaire bilingue D a &#233;t&#233; construit ind&#233;pendamment des documents
trait&#233;s, ce qui implique que le degr&#233; de comparabilit&#233; Mef (respectivement de m&#234;me pour Mfe) est &#224; peu pr&#232;s le
m&#234;me pour diff&#233;rentes parties du corpus 2. Nous faisons de plus l&#8217;hypoth&#232;se que :
</p>
<p>(I) |C
e
1&#8746;Ce2 |
|Ce2 | =
</p>
<p>|Cf1&#8746;Cf2 |
|Cf2 |
</p>
<p>Alors :
M(Ce1 &#8746; Ce2 , Cf1 &#8746; Cf2 ) &#8805; min(M(Ce1 , Cf1 ),M(Ce2 , Cf2 ))
</p>
<p>D&#233;monstration (esquisse) : Soit V = Ce1 &#8745; Ce2 . En utilisant le fait que Mef (Cei , Cfi ) &#8804;Mef (Cei , Cfi
&#8242;
) pour tout Cfi
</p>
<p>&#8242;
</p>
<p>tel que Cfi &#8838; Cfi
&#8242;
</p>
<p>(et de m&#234;me pour la direction fran&#231;ais vers anglais), nous avons, pour i = 1, 2 :X
w&#8712;Cei \V
</p>
<p>&#963;(w, Cf1 &#8746; Cf2 )) &#8805; |Cei \V |Mef (Cei , Cfi )
</p>
<p>et, pour les mots de V : X
w&#8712;V
</p>
<p>&#963;(w, Cf1 &#8746; Cf2 )) &#8805; |V |max(Mef (Ce1 , Cf1 ),Mef (Ce2 , Cf2 ))
</p>
<p>2. En d&#8217;autres termes, la proportion de mots anglais (respectivement fran&#231;ais) traduits dans le corpus fran&#231;ais (respectivement anglais) est
homog&#232;ne sur l&#8217;ensemble du corpus.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Alors, d&#8217;apr&#232;s l&#8217;hypoth&#232;se d&#8217;ind&#233;pendance entre corpus et dictionnaire faite en &#233;non&#231;ant la propri&#233;t&#233; 1 :X
w&#8712;(Ce1&#8746;Ce2)&#8745;De
</p>
<p>&#963;(w, Cf1 &#8746; Cf2 ))
</p>
<p>&#8805; |(Ce1 &#8746; Ce2) &#8745;De|min(Mef (Ce1 , Cf1 ),Mef (Ce2 , Cf2 ))
</p>
<p>Un d&#233;veloppement similaire sur Mfe et l&#8217;utilisation de la condition (I) compl&#232;tent la d&#233;monstration.
</p>
<p>La propri&#233;t&#233; pr&#233;c&#233;dente garantit que la classe obtenue en fusionnant deux classes existantes a un degr&#233; de compa-
rabilit&#233; au moins &#233;gal &#224; celui de la classe la moins comparable. Le degr&#233; de comparabilit&#233; ne peut donc d&#233;cro&#238;tre
dans le processus de classification agglom&#233;rative. Comme l&#8217;on commence par fusionner les documents les plus
comparables, on ne construit que des classes avec un bon degr&#233; de comparabilit&#233;. Enfin, la condition (I) a de
grandes chances d&#8217;&#234;tre r&#233;alis&#233;e car tous les corpus sont pr&#233;trait&#233;s de fa&#231;on &#224; &#233;liminer les documents trop courts ou
trop longs, souvent source de bruit, et la p&#233;nalit&#233; utilis&#233;e dans la mesure de similarit&#233; fournit des classes compre-
nant des nombres comparables de documents dans les deux langues. Le processus global que nous avons d&#233;fini
permet donc d&#8217;obtenir des corpus homog&#232;nes et fortement comparables.
</p>
<p>2.3 Consid&#233;rations informatiques
</p>
<p>Dans la mesure o&#249; les corpus comparables disponibles &#224; l&#8217;heure actuelle comprennent en g&#233;n&#233;ral un nombre
important de documents, la classification agglom&#233;rative peut s&#8217;av&#233;rer trop co&#251;teuse. Nous proposons ici une borne
inf&#233;rieure de la mesure de comparabilit&#233; qui peut &#234;tre calcul&#233;e efficacement ainsi qu&#8217;une mise &#224; jour efficace de
la matrice de similarit&#233; pendant le processus de classification. Le fait de se reposer sur une borne inf&#233;rieure de la
mesure de similarit&#233; garantit que les classes obtenues auront un bon degr&#233; de comparabilit&#233;, car seules les classes
les plus similaires sont regroup&#233;es &#224; chaque it&#233;ration de l&#8217;algorithme de classification. La propri&#233;t&#233; suivante &#233;tablit
une telle borne inf&#233;rieure, sur la base du degr&#233; de comparabilit&#233; moyen des paires de documents.
</p>
<p>Propri&#233;t&#233; 2 Soit P un corpus comparable comprenant une partie anglaise Pe et une partie fran&#231;aise Pf , et soit
D un dictionnaire bilingue, De d&#233;notant le vocabulaire anglais et Df le vocabulaire fran&#231;ais. Supposons que le
dictionnaire est distribu&#233; de fa&#231;on uniforme sur le corpus, c&#8217;est-&#224;-dire que :
</p>
<p>&#8704;de &#8712; Pe, #w(de &#8745; De)
#w(de)
</p>
<p>=
#w(Pe &#8745; De)
</p>
<p>#w(Pe)
et de m&#234;me pour la partie fran&#231;aise. Supposons de plus que tous les documents, ainsi que les parties anglaise et
</p>
<p>fran&#231;aise du corpus, ont &#224; peu pr&#232;s la m&#234;me longueur :
</p>
<p>&#8704;de &#8712; Pe and df &#8712; Pf , #w(de)
#w(Pe) '
</p>
<p>#w(df )
</p>
<p>#w(Pf ) (= &#955;)
</p>
<p>Alors :
M(Pe,Pf ) &#8805; 1
</p>
<p>#d(Pe) &#183;#d(Pf )
X
</p>
<p>de&#8712;Pe,df&#8712;Pf
M(de, df )
</p>
<p>Nous ne d&#233;taillons pas ici la d&#233;monstration de cette propri&#233;t&#233;, purement technique. La premi&#232;re hypoth&#232;se faite
semble raisonnable (et rejoint celle faite dans la propri&#233;t&#233; pr&#233;c&#233;dente) en l&#8217;absence de toute connaissance a priori
sur les th&#233;matiques couvertes par le corpus et leur lien avec le dictionnaire. La seconde hypoth&#232;se est en partie
garantie dans notre cas par le processus de construction que nous avons d&#233;fini et la fonction de p&#233;nalit&#233; associ&#233;e.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>RemplacerM par la borne ci-dessus dans l&#8217;&#233;quation 1 conduit &#224; une mesure de similarit&#233; qui peut &#234;tre vue comme
la valeur accumul&#233;e de toutes les connexions entre deux classes. Il est alors possible de mettre &#224; jour la matrice
de similarit&#233; de fa&#231;on it&#233;rative. Supposons en effet que le processus de classification doive, &#224; un instant donn&#233;,
fusionner les classes C1 et C2 en une seule classe Cnew. Un nouveau score de similarit&#233; entre Cnew et toutes les
autres classes doit alors &#234;tre calcul&#233;. La similarit&#233; entre Cnew et une autre classe C3 peut s&#8217;&#233;crire, &#224; partir de
l&#8217;&#233;quation 3 et de la formule de similarit&#233; :
</p>
<p>siml(Cnew, C3) =
(NC1 +NC2 )&#966;(C1 &#8746; C2)
</p>
<p>#d(Cenew) &#183;#d(Cf3 ) + #d(Ce3) &#183;#d(Cfnew)
</p>
<p>o&#249; (j = 1, 2) et :
</p>
<p>NCj =
(#d(Cej ) &#183;#d(Cf3 ) + #d(Ce3) &#183;#d(Cfj ))siml(Cj , C3)
</p>
<p>&#966;(Cj &#8746; C3)
</p>
<p>Dans le processus de classification, dans la mesure o&#249; siml(C1, C3) et siml(C2, C3) sont d&#233;j&#224; connus avant le
calcul de siml(Cnew, C3), la matrice de similarit&#233; peut directement &#234;tre mise &#224; jour &#224; chaque it&#233;ration. En notantNc
le nombre de classes avant fusion, la complexit&#233; de cette mise &#224; jour est de l&#8217;ordre de O(Nc), alors qu&#8217;elle atteint
O(Nc&#215; C&#772;2) si l&#8217;on applique directement les &#233;quations 1 et 3 (C&#772; repr&#233;sentant le nombre moyen de documents par
classe).
</p>
<p>3 Exp&#233;riences et r&#233;sultats
</p>
<p>Les diff&#233;rentes exp&#233;riences que nous avons r&#233;alis&#233;es ont pour objectif d&#8217;&#233;valuer : (i) si l&#8217;algorithme que nous
avons propos&#233; induit des corpus d&#8217;une meilleure qualit&#233; en ce qui concerne la comparabilit&#233;, (ii) si les lexiques
bilingues extraits de ces corpus sont eux aussi d&#8217;une qualit&#233; plus importante, et (iii) si ces lexiques peuvent &#234;tre
utilis&#233;s pour am&#233;liorer les performances des syst&#232;mes de recherche d&#8217;information interlingue.
</p>
<p>Dans nos exp&#233;riences, diff&#233;rents corpus sont utilis&#233;s : le corpus anglais TREC 3 de l&#8217;Associated Press (not&#233; AP)
et les corpus fournis dans les t&#226;ches multilingues des campagnes CLEF 4 dont pour l&#8217;anglais le Los Angeles Times
(LAT94) et le Glasgow Herald (GH95) et pour le fran&#231;ais Le Monde (MON94), le SDA 94 (SDA94) et 95 (SDA95).
Outre ces corpus existants, deux corpus monolingues ont &#233;t&#233; extraits &#224; partir de Wikip&#233;dia : le corpus anglais Wiki-
En construit en retenant l&#8217;ensemble des articles appartenant &#224; la cat&#233;gorie Society pour une profondeur inf&#233;rieure
&#224; 4 (soit 33 000 mots anglais distincts) et le corpus fran&#231;ais Wiki-Fr toujours pour la cat&#233;gorie Soci&#233;t&#233; pour une
profondeur inf&#233;rieure &#224; 7 (soit 28 000 mots fran&#231;ais distincts). Le dictionnaire bilingue bd0 n&#233;cessaire pour la t&#226;che
d&#8217;extraction de lexiques est quant &#224; lui construit &#224; partir de dictionnaires en ligne. Dans toutes nos exp&#233;riences,
nous utilisons la m&#233;thode d&#233;crite dans le pr&#233;sent article compl&#233;t&#233;e par celle pr&#233;sent&#233;e dans (Li &amp; Gaussier, 2010).
Cette derni&#232;re m&#233;thode est &#224; notre connaissance la seule approche alternative pour am&#233;liorer la comparabilit&#233; des
corpus, d&#8217;o&#249; son importance dans l&#8217;&#233;valuation.
</p>
<p>3.1 Comparabilit&#233; de corpus
</p>
<p>L&#8217;algorithme de classification d&#233;crit en section 2.2.1 est utilis&#233; pour am&#233;liorer le degr&#233; de comparabilit&#233; d&#8217;un
corpus comparable. Les corpus GH95 et SDA95 sont utilis&#233;s pour construire le corpus comparable P0 (56 000
</p>
<p>3. http://trec.nist.gov/
4. http://www.clef-campaign.org</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>mots pour l&#8217;anglais et 42 000 le fran&#231;ais). En outre, nous exploitons deux corpus comparables suppl&#233;mentaires
pour nous assurer que l&#8217;efficacit&#233; de notre algorithme n&#8217;est pas li&#233;e &#224; une ressource externe sp&#233;cifique : i) P1T
compos&#233; &#224; partir des corpus LAT94, MON94 et SDA94 (109 000 mots pour l&#8217;anglais et 87 000 pour le fran&#231;ais) et
ii) P2T compos&#233; &#224; partir des corpus Wiki-En et Wiki-Fr (368 000 mots pour l&#8217;anglais et 378 000 pour le fran&#231;ais).
Apr&#232;s le processus de classification, nous obtenons les corpus P1 (pour le corpus externe P1T ) et P2 (pour le
corpus externe P2T ). Comme nous l&#8217;avons indiqu&#233; pr&#233;c&#233;demment, nous utilisons aussi la m&#233;thode d&#233;crite dans (Li
&amp; Gaussier, 2010) sur les m&#234;mes donn&#233;es pour comparer nos r&#233;sultats et obtenons ainsi le corpus P1&#8242; (pour P1T ) et
P2&#8242; (pour P2T ) &#224; partir de P0. Au niveau de la couverture lexicale, P1 couvre 97,9% du vocabulaire de P0, tandis
que P2 couvre 99,0% de celui de P0. Nous pouvons ainsi constater qu&#8217;une tr&#232;s grande partie du vocabulaire du
corpus d&#8217;origine a &#233;t&#233; conserv&#233;, ce qui est l&#8217;une des exigences de notre approche. En ce qui concerne les scores de
comparabilit&#233;,P1 atteint 0,924 etP2 0,939. Les deux corpus comparables ont donc bien un degr&#233; de comparabilit&#233;
sup&#233;rieur au corpus d&#8217;origine qui &#233;tait de l&#8217;ordre de 0,881 comme cela est sugg&#233;r&#233;e par la propri&#233;t&#233; 1. En outre,
les corpus P1 et P2 sont plus comparables que le corpus P1&#8242; (comparabilit&#233; de 0,912) et P2&#8242; (comparabilit&#233; de
0,915) ce qui montre bien que l&#8217;homog&#233;n&#233;it&#233; est un &#233;l&#233;ment crucial pour &#233;valuer la comparabilit&#233;.
</p>
<p>3.2 Extraction de lexiques bilingues
</p>
<p>TABLE 1 &#8211; &#201;valuation des lexiques bilingues extraits pour diff&#233;rents corpus comparables
P0 P1&#8242; P2&#8242; P1 P2 P1 &gt; P0 P2 &gt; P0
</p>
<p>Pr&#233;cision 0,226 0,277 0,325 0,295 0,461 0,069 30,5 % 0,235 104,0 %
Rappel 0,103 0,122 0,145 0,133 0,212 0,030 29,1 % 0,109 105,8 %
</p>
<p>TABLE 2 &#8211; Comparaison de la pr&#233;cision pour diff&#233;rents intervalles de fr&#233;quences des mots de la liste d&#8217;&#233;valuation
P0 P2&#8242; P2 P2&#8242; &gt; P0 P2 &gt; P0 P2 &gt; P2&#8242;
</p>
<p>Wl 0,135 0,206 0,304 0,071 52,6 % 0,169 125,2 % 0,098 47,6 %
Wm 0,256 0,390 0,564 0,134 52,3 % 0,308 120,3 % 0,174 44,6 %
Wh 0,434 0,632 0,667 0,198 45,6 % 0,233 53,7 % 0,035 5,5
,All 0,226 0,325 0,461 0,099 43,8 % 0,235 104,0 % 0,136 41,8 %
</p>
<p>Comme les travaux ant&#233;rieurs en extraction de lexiques bilingues &#224; partir de corpus comparables exploitent des
ressources diff&#233;rentes et op&#232;rent des choix distincts des n&#244;tres, il est relativement difficile de se comparer &#224; ceux-
ci (Laroche &amp; Langlais, 2010). En outre, puisque notre approche vise &#224; am&#233;liorer la comparabilit&#233; de corpus,
elle peut &#234;tre ensuite coupl&#233;e &#224; une m&#233;thode existante d&#8217;extraction de lexiques bilingues. Il est donc tout aussi
int&#233;ressant de directement &#233;valuer si un tel couplage peut conduire &#224; des performances accrues en termes de
qualit&#233; des lexiques extraits.
</p>
<p>L&#8217;extraction de lexiques bilingues &#224; partir de corpus comparables repose sur la m&#233;thode propos&#233;e par Fung &amp; Yee
(1998) plus connue maintenant sous le nom d&#8217;approche standard notamment dans les travaux de (D&#233;jean et al.,
2002; Gaussier et al., 2004; Yu &amp; Tsujii, 2009). Dans cette approche, chaque mot est repr&#233;sent&#233; sous la forme
d&#8217;un vecteur de contexte compos&#233; des mots qui co-occurrent avec lui dans une fen&#234;tre donn&#233;. Les vecteurs de
contexte de la langue source sont ensuite traduits vers la langue cible en s&#8217;appuyant sur un dictionnaire bilingue.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Enfin, la traduction d&#8217;un mot est obtenue en comparant son vecteur de contexte traduit &#224; l&#8217;ensemble des vecteurs
de la langue cible &#224; travers une mesure de distance ou similarit&#233; vectorielle telle que le cosinus.
</p>
<p>3.2.1 Param&#232;tres exp&#233;rimentaux
</p>
<p>Afin d&#8217;&#233;valuer la qualit&#233; des lexiques bilingues extraits, nous divisons notre dictionnaire bilingue bd0 en deux
parties : 10 % des mots anglais accompagn&#233;s de leurs traductions sont choisis al&#233;atoirement et uniquement utilis&#233;s
comme liste d&#8217;&#233;valuation, les 90 % restant sont utilis&#233;s pour assurer la traduction des vecteurs de contexte dans
l&#8217;approche standard. Les mots anglais absents de Pe ou pour lesquels aucune traduction n&#8217;a &#233;t&#233; trouv&#233;e dans Pf
sont retir&#233;s de la liste d&#8217;&#233;valuation. Pour chaque mot anglais de la liste d&#8217;&#233;valuation, tous les mots fran&#231;ais de
Pf sont ordonn&#233;s suivant leur similarit&#233; avec les mots anglais. Les mesures de pr&#233;cision et rappel sont ensuite
calcul&#233;es sur les N premiers candidats. Les valeurs de la pr&#233;cision dans ce cas correspondent &#224; la proportion de
listes contenant la traduction correcte (en cas de traductions multiples, une liste est r&#233;put&#233;e contenir la traduction
correcte d&#232;s lors que l&#8217;une des traductions possibles est pr&#233;sente). Le rappel est quant &#224; lui la proportion de
traductions correctes trouv&#233;e dans les listes sur toutes les traductions fournies dans le corpus. Cette mani&#232;re de
proc&#233;der a &#233;t&#233; utilis&#233;e dans diff&#233;rents travaux ant&#233;rieurs et peut &#234;tre maintenant consid&#233;r&#233;e comme un m&#233;thode
d&#8217;&#233;valuation attest&#233;e. En outre, plusieurs &#233;tudes ont montr&#233; qu&#8217;il est plus facile de trouver les traductions correctes
pour les mots fr&#233;quents que pour les mots rares (Pekar et al., 2006). Afin de prendre en compte ce ph&#233;nom&#232;ne,
nous distinguons diff&#233;rents intervalles d&#8217;effectifs pour &#233;valuer la validit&#233; de notre approche. Ainsi, les mots avec
un effectif inf&#233;rieur &#224; 100 sont d&#233;finis comme &#233;tant des mots de faibles fr&#233;quence (Wl), ceux avec un effectif
sup&#233;rieur &#224; 400 sont d&#233;finis comme &#233;tant des mots tr&#232;s fr&#233;quents (Wh), et enfin les mots dont l&#8217;effectif est compris
entre ces deux seuils sont consid&#233;r&#233;s comme des mots de fr&#233;quence interm&#233;diaire (Wm).
</p>
<p>3.2.2 Analyse des r&#233;sultats
</p>
<p>Dans une premi&#232;re s&#233;rie d&#8217;exp&#233;riences, les lexiques bilingues sont extraits &#224; partir des corpus obtenus ii) par notre
approche (P1 et P2), ii) par la m&#233;thode d&#233;crite dans (Li &amp; Gaussier, 2010) (P1&#8242; and P2&#8242;) et iii) enfin avec le
corpus d&#8217;origine P0, avec N fix&#233; &#224; 20. La table 1 pr&#233;sente les r&#233;sultats obtenus. Les deux derni&#232;res colonnes
&#8220;P1 &gt; P0&#8221; et &#8220;P2 &gt; P0&#8221; indique les diff&#233;rences absolue et relative, exprim&#233;es en pourcentage, par rapport &#224;
P0. Comme nous pouvons le constater, les meilleurs r&#233;sultats sont obtenus &#224; partir des corpus construits avec
la m&#233;thode que nous avons propos&#233;e. Les lexiques extraits &#224; partir du corpus o&#249; le degr&#233; de comparabilit&#233; a &#233;t&#233;
renforc&#233; sont d&#8217;une bien meilleure qualit&#233; que ceux obtenus &#224; partir du corpus d&#8217;origine ou encore du corpus
construit avec l&#8217;approche de (Li &amp; Gaussier, 2010). La diff&#233;rence de qualit&#233; est encore plus notable avec P2
qui est obtenu &#224; partir d&#8217;un corpus externe volumineux P2T . Ces r&#233;sultats semblent confirmer l&#8217;intuition qu&#8217;il est
possible de trouver plus ais&#233;ment dans des corpus volumineux des documents en relation avec un corpus donn&#233;.
</p>
<p>Afin d&#8217;&#233;valuer la relation entre la qualit&#233; de ces m&#233;thodes et la fr&#233;quence des mots &#224; traduire, nous nous concen-
trons sur les meilleurs r&#233;sultats sur P2&#8242; pour l&#8217;approche pr&#233;c&#233;dente et sur ceux de P2 pour notre approche. La
table 2 r&#233;sume les r&#233;sultats obtenus. On remarquera, sans v&#233;ritablement de surprise, que les r&#233;sultats obtenus
pour les mots ayant une haute fr&#233;quence sont meilleurs que ceux obtenus pour les mots de faible fr&#233;quence. En
outre, notre approche est la meilleure quel que soit l&#8217;intervalle de fr&#233;quence pris en compte. La pr&#233;cision globale
peut &#234;tre augment&#233;e en relatif de 41,8 % (de 0,325 &#224; 0,461). En comparant P2 avec le corpus d&#8217;origine P0, nous
pouvons noter pour la pr&#233;cision globale, une augmentation relative de 104,0 % (de 0,226 &#224; 0,461), ce qui est tr&#232;s
satisfaisant dans ce contexte d&#8217;&#233;valuation. Enfin, l&#8217;am&#233;lioration pour les mots de faible et moyenne fr&#233;quence est
plus importante pour P2, ce qui d&#233;montre que notre approche se comporte bien mieux sur ce qui est g&#233;n&#233;ralement</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>consid&#233;r&#233; comme un probl&#232;me difficile (Pekar et al., 2006).
</p>
<p>3.3 Exp&#233;riences en recherche d&#8217;information interlingue
</p>
<p>TABLE 3 &#8211; Score MAP pour la t&#226;che de recherche d&#8217;information interlingue suivant diff&#233;rents dictionnaires bi-
lingues
</p>
<p>mon bd1 bd1+cc0 bd1+cc1 bd1+cc2 bd2 bd2+cc0 bd2+cc1 bd2+cc2
MAP 0, 422 0, 313 0, 327&#8226; 0, 328&#8226; 0, 338&#8226; 0, 375 0, 382 0, 377 0, 391&#8226;
</p>
<p>Dans la derni&#232;re s&#233;rie d&#8217;exp&#233;riences, nous cherchons &#224; &#233;valuer l&#8217;apport des diff&#233;rents lexiques extraits &#224; partir de
corpus comparables pour une t&#226;che de recherche d&#8217;information interlingue. Pour ce faire, nous exploitons les sujets
des campagnes CLEF de 2001 et 2002, rassemblant environ 100 sujets distincts, comme requ&#234;tes sur une collection
de 113 000 documents issus du Los Angeles Times. Les sujets anglais correspondants sont utilis&#233;s pour interroger la
m&#234;me collection (r&#233;f&#233;rence mon). Seul le titre et la partie description des sujets CLEF sont utilis&#233;s pour construire
des requ&#234;tes. En outre, les mots outils et les phrases non pertinentes telles que find documents which report about
sont supprim&#233;s des requ&#234;tes. La recherche est r&#233;alis&#233;e avec le mod&#232;le Indri du syst&#232;me de recherche d&#8217;information
Lemur (http ://www.lemurproject.org). Une variante de l&#8217;approche introduite dans (Pirkola, 1998) et (Talvensaari
et al., 2007) est aussi utilis&#233;e pour transformer les sujets fran&#231;ais en requ&#234;tes en anglais. L&#8217;id&#233;e est de borner toutes
les possibilit&#233;s de traduction d&#8217;un mot fran&#231;ais dans le sujet du texte avec un op&#233;rateur WSYN. Ensuite, toutes les
traductions candidates dans l&#8217;op&#233;rateur WSYN sont trait&#233;es comme des synonymes avec des poids diff&#233;rents.
</p>
<p>Dans nos exp&#233;riences, nous combinons deux dictionnaires bilingues de langue g&#233;n&#233;rale bd1 (68 0000 traductions)
et bd2 (116 000 traductions) avec les lexiques bilingues obtenus automatiquement dans la pr&#233;c&#233;dente section. Nous
utilisons ici les lexiques cc0 (extrait de P0), cc1 (extrait de P2&#8242;) et cc2 (extrait de P2). Diff&#233;rentes combinaisons
de ces ressources sont r&#233;alis&#233;es, y compris bd1/2, bd1/2+cc0, bd1/2+cc1, bd1/2+cc2. Lorsque qu&#8217;un dictionnaire
de langue g&#233;n&#233;rale et un lexique extrait sont combin&#233;s, plus de poids est attribu&#233; aux traductions candidat du
dictionnaire de langue g&#233;n&#233;rale. Le poids des diff&#233;rents mots traduits &#224; partir de cc0/1/2 est quant &#224; lui le cosinus
entre les vecteurs de contexte de chaque mot (c&#8217;est-&#224;-dire le score donn&#233; par l&#8217;approche standard pr&#233;c&#233;demment
&#233;voqu&#233;e). Le poids pour les traductions trouv&#233;es dans le dictionnaire bilingue est fix&#233; empiriquement &#224; 25. Comme
il est d&#8217;usage en recherche d&#8217;information, nous utilisons la mesure MAP (Mean Average Precision) afin d&#8217;&#233;valuer
les performances des diff&#233;rents syst&#232;mes. L&#8217;importance des diff&#233;rences entre les diff&#233;rents syst&#232;mes est estim&#233;e
par un t-test appari&#233; de Student (p-value fix&#233;e &#224; 0,1). Les r&#233;sultats obtenus sont indiqu&#233;s dans la table 3. Pour le
dictionnaire de langue g&#233;n&#233;rale bd1, on note toujours une am&#233;lioration significative des r&#233;sultats (identifi&#233;e par la
marque &#8226;) du score MAP lorsque l&#8217;un des lexiques bilingues extraits du corpus comparables est utilis&#233;. Lorsque
bd2, qui est beaucoup plus riche que bd1, est utilis&#233;, seulement le lexique bilingue cc2 extrait avec notre m&#233;thode &#224;
partirP2 conduit &#224; une am&#233;lioration significative des r&#233;sultats. Cela montre que cc2 est sup&#233;rieure &#224; cc1 et cc0 dans
la t&#226;che de recherche d&#8217;information interlingue, en particulier lorsque le dictionnaire de langue g&#233;n&#233;rale utilis&#233;
est d&#8217;une taille importante. Ces r&#233;sultats semblent confirmer que notre approche bas&#233;e sur de la classification est
plus adapt&#233;e que l&#8217;approche gloutonne des travaux pr&#233;c&#233;dents de (Li &amp; Gaussier, 2010). Enfin, la combinaison
actuelle des lexiques extraits avec le syst&#232;me de recherche d&#8217;information interlingue est relativement simple et
pourrait &#234;tre certainement am&#233;lior&#233;e en exploitant d&#8217;autres mod&#232;les de combinaison.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>4 Conclusion
</p>
<p>Dans cet article, nous avons propos&#233; une nouvelle approche pour augmenter le degr&#233; de comparabilit&#233; des do-
cuments constituant un corpus comparable afin d&#8217;am&#233;liorer la qualit&#233; des lexiques bilingues extraits de corpus
comparables et les performances des syst&#232;mes de recherche d&#8217;information interlingue. Nous avons d&#233;montr&#233;
th&#233;oriquement puis empiriquement que notre approche permet de garantir un certain degr&#233; de comparabilit&#233; et
l&#8217;homog&#233;n&#233;it&#233; du corpus tout en pr&#233;servant une large part du vocabulaire du corpus d&#8217;origine. Enfin, nos exp&#233;-
riences montrent que les lexiques bilingues que nous obtenons sont d&#8217;une meilleure qualit&#233; que ceux obtenus avec
les approches pr&#233;c&#233;dentes, et que ces lexiques peuvent &#234;tre utilis&#233;s pour am&#233;liorer significativement les r&#233;sultats
des syst&#232;mes de recherche d&#8217;information interlingue.
</p>
<p>Les deux &#233;tapes cruciales de notre approche sont d&#8217;une part l&#8217;extraction d&#8217;un noyau fortement comparable du
corpus original, et, d&#8217;autre part, l&#8217;alignement des parties du corpus orignal, non pr&#233;sentes dans ce noyau, avec un
corpus externe. Le seuil introduit au niveau du degr&#233; de comparabilit&#233; permet de contr&#244;ler la taille et la qualit&#233;
du noyau extrait dans la premi&#232;re &#233;tape. Si le corpus original n&#8217;est que tr&#232;s faiblement comparable, il est alors
possible que ce noyau soit vide (ce qui est un r&#233;sultat souhaitable dans ce cas). Dans tous les cas, except&#233; celui
o&#249; le noyau correspond au corpus original, le corpus final d&#233;pend de la proximit&#233; du corpus original (en fait de
la partie restante apr&#232;s extraction du noyau) et du corpus externe utilis&#233;. Bien &#233;videmment, si le corpus externe
est trop diff&#233;rent du corpus original, l&#8217;on ne pourra pas compl&#233;ter correctement le noyau. Consid&#233;rer des corpus
externes les plus larges possibles permet ici d&#8217;augmenter les chances de trouver des documents comparables 5
</p>
<p>L&#8217;id&#233;al serait bien s&#251;r d&#8217;avoir acc&#232;s &#224; la collection la plus large possible, et le web constitue ici un excellent can-
didat. Il est cependant n&#233;cessaire de pouvoir, &#224; partir d&#8217;un document donn&#233; dans une langue source, extraire du
web un ensemble de documents comparables en langue cible (on peut ensuite directement utiliser notre m&#233;thode
sur l&#8217;union de ces ensembles). Or nous n&#8217;avons pas r&#233;ussi jusqu&#8217;&#224; pr&#233;sent &#224; r&#233;aliser correctement cette extraction.
La constitution enti&#232;rement automatique de collections comparables &#224; partir du web nous semble &#234;tre un pro-
bl&#232;me difficile, qui requiert d&#8217;autres attributs que ceux utilis&#233;s pour les corpus parall&#232;les. C&#8217;est un point que nous
comptons d&#233;velopper dans le futur.
</p>
<p>Remerciements
</p>
<p>Ce travail qui s&#8217;inscrit dans le cadre du projet METRICC (www.metricc.com) a b&#233;n&#233;fici&#233; d&#8217;une aide de
l&#8217;Agence Nationale de la Recherche portant la r&#233;f&#233;rence ANR-08-CORD-009. Enfin, nous tenons &#224; remercier
les relecteurs pour leurs commentaires pr&#233;cieux.
</p>
<p>R&#233;f&#233;rences
</p>
<p>BALLESTEROS L. &amp; CROFT W. B. (1997). Phrasal translation and query expansion techniques for cross-
language information retrieval. In Proceedings of the 20th ACM SIGIR, p. 84&#8211;91, Philadelphia, Pennsylvania,
USA.
</p>
<p>5. C&#8217;est ce qui distingue les corpus P1 et P1 dans nos exp&#233;riences, le deuxi&#232;me &#233;tant obtenu &#224; partir d&#8217;un corpus externe &#224; plus large
couverture.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>D&#201;JEAN H., GAUSSIER E. &amp; SADAT F. (2002). An approach based on multilingual thesauri and model combi-
nation for bilingual lexicon extraction. In Proceedings of the 19th International Conference on Computational
Linguistics, p. 1&#8211;7, Taipei, Taiwan.
</p>
<p>FUNG P. &amp; MCKEOWN K. (1997). Finding terminology translations from non-parallel corpora. In Proceedings
of the 5th Annual Workshop on Very Large Corpora, p. 192&#8211;202, Hong Kong.
</p>
<p>FUNG P. &amp; YEE L. Y. (1998). An IR approach for translating new words from nonparallel, comparable texts. In
Proceedings of the 17th international conference on Computational linguistics, p. 414&#8211;420, Montreal, Quebec,
Canada.
</p>
<p>GARERA N., CALLISON-BURCH C. &amp; YAROWSKY D. (2009). Improving translation lexicon induction from
monolingual corpora via dependency contexts and part-of-speech equivalences. In CoNLL 09 : Proceedings of
the Thirteenth Conference on Computational Natural Language Learning, p. 129&#8211;137, Boulder, Colorado.
</p>
<p>GAUSSIER E., RENDERS J.-M., MATVEEVA I., GOUTTE C. &amp; D&#201;JEAN H. (2004). A geometric view on bilin-
gual lexicon extraction from comparable corpora. In Proceedings of the 42nd Annual Meeting of the Association
for Computational Linguistics, p. 526&#8211;533, Barcelona, Spain.
</p>
<p>LAROCHE A. &amp; LANGLAIS P. (2010). Revisiting context-based projection methods for term-translation spotting
in comparable corpora. In Proceedings of the 23rd International Conference on Computational Linguistics
(Coling 2010), p. 617&#8211;625, Beijing, China.
</p>
<p>LI B. &amp; GAUSSIER E. (2010). Improving corpus comparability for bilingual lexicon extraction from comparable
corpora. In Proceedings of the 23rd International Conference on Computational Linguistics, p. 644&#8211;652, Beijing,
China.
</p>
<p>MORIN E., DAILLE B., TAKEUCHI K. &amp; KAGEURA K. (2007). Bilingual terminology mining - using brain,
not brawn comparable corpora. In Proceedings of the 45th Annual Meeting of the Association for Computational
Linguistics, p. 664&#8211;671, Prague, Czech Republic.
</p>
<p>OCH F. J. &amp; NEY H. (2003). A systematic comparison of various statistical alignment models. Computational
Linguistics, 29(1), 19&#8211;51.
PEKAR V., MITKOV R., BLAGOEV D. &amp; MULLONI A. (2006). Finding translations for low-frequency words
in comparable corpora. Machine Translation, 20(4), 247&#8211;266.
PIRKOLA A. (1998). The effects of query structure and dictionary setups in dictionary-based cross-language
information retrieval. In Proceedings of the 21st annual international ACM SIGIR conference on Research and
development in information retrieval, p. 55&#8211;63, Melbourne, Australia.
</p>
<p>RAPP R. (1999). Automatic identification of word translations from unrelated English and German corpora. In
Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics, p. 519&#8211;526, College
Park, Maryland, USA.
</p>
<p>ROBITAILLE X., SASAKI Y., TONOIKE M., SATO S. &amp; UTSURO T. (2006). Compiling French-Japanese ter-
minologies from the web. In Proceedings of the 11st Conference of the European Chapter of the Association for
Computational Linguistics, p. 225&#8211;232, Trento, Italy.
</p>
<p>SHEZAF D. &amp; RAPPOPORT A. (2010). Bilingual lexicon generation using non-aligned signatures. In Procee-
dings of the 48th Annual Meeting of the Association for Computational Linguistics, p. 98&#8211;107, Uppsala, Sweden.
</p>
<p>TALVENSAARI T., LAURIKKALA J., J&#196;RVELIN K., JUHOLA M. &amp; KESKUSTALO H. (2007). Creating and
exploiting a comparable corpus in cross-language information retrieval. ACM Trans. Inf. Syst., 25(1), 4.
YU K. &amp; TSUJII J. (2009). Extracting bilingual dictionary from comparable corpora with dependency hetero-
geneity. In Proceedings of HLT-NAACL 2009, p. 121&#8211;124, Boulder, Colorado, USA.</p>

</div></div>
</body></html>