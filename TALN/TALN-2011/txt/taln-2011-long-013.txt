TALN 2011, Montpellier, 27 juin -1” jui11et2011

Degré de comparabilité, extraction lexicale bilingue et recherche
d’information interlingue

Bo Lil Eric Gaussierl Emmanuel Morinz Amir Hazemz
(1) Université Grenoble I, LIG UMR 5217
(2) LINA, UMR 6241,Université de Nantes
{bo.li,eric. gaussier} @imag.fr, {emmanuel.morin,aInir.hazem} @univ-nantes.fr

Résumé. Nous étudions dans cet article le probleme de la comparabilité des documents composant un cor-
pus comparable aﬁn d’améliorer la qualité des lexiques bilingues extraits et les performances des systemes de
recherche d’information interlingue. Nous proposons une nouvelle approche qui permet de garantir un certain
degré de comparabilité et d’homogénéité du corpus tout en préservant une grande part du vocabulaire du corpus
d’ origine. Nos experiences montrent que les lexiques bilingues que nous obtenons sont d’une meilleure qualité
que ceux obtenus avec les approches précédentes, et qu’ils peuvent étre utilises pour améliorer signiﬁcativement
les systemes de recherche d’ information interlingue.

Abstract. We study in this paper the problem of enhancing the comparability of bilingual corpora in order
to improve the quality of bilingual lexicons extracted from comparable corpora and the performance of cross-
language information retrieval (CLIR) systems. We introduce a new method for enhancing corpus comparability
which guarantees a certain degree of comparability and homogeneity, and still preserves most of the vocabulary
of the original corpus. Our experiments illustrate the well—foundedness of this method and show that the bilingual
lexicons obtained are of better quality than the lexicons obtained with previous approaches, and that they can be
used to signiﬁcantly improve CLIR systems

M0tS-CléS 3 Corpus comparables, comparabilité, lexiques bilingues, recherche d’information interlingue.

Keywords: Comparable corpora, comparability, bilingual lexicon, cross—language information retrieval.

1 Introduction

Les lexiques bilingues sont une ressource incontoumable dans différentes applications multilingues du traitement
automatique des langues comme la traduction automatique (Och & Ney, 2003) ou la recherche d’information in-
terlingue (Ballesteros & Croft, 1997). Dans la mesure ou la constitution manuelle de lexiques bilingues est une
tache coﬁteuse et qu’il est difﬁcilement envisageable de développer un lexique pour chaque domaine d’ etude, les
recherches se sont intéressées a l’extraction automatique de ces lexiques a partir de corpus. Dans la mesure ou la
plupart des corpus bilingues existants sont par essence comparables, c’est-a-dire qu’ils regroupent des documents
dans des langues différentes traitant du meme domaine sur la meme période sans étre en relation de traduc-
tion, différents travaux s’intéressent a l’extraction de lexiques bilingues a partir de corpus comparables (Fung &
McKeown, 1997; Fung & Yee, 1998; Rapp, 1999; Déjean et al., 2002; Gaussier et al., 2004; Robitaille et al.,
2006; Morin et al., 2007; Garera et al., 2009; Yu & Tsujii, 2009; Shezaf & Rappoport, 2010, entre autres). Le

socle commun a ces travaux est de reposer sur une hypothese de distribution qui postule que les mots qui sont en
correspondance de traduction sont susceptibles d’apparaitre dans des contextes identiques pour des langues dif-
ferentes. En s’appuyant sur cette hypothese fondatrice, les chercheurs ont aussi cherche a identiﬁer de meilleures
representations pour le contexte des mots de meme qu’a utiliser differentes methodes pour mettre en correspon-
dance les mots entre differentes langues touj ours en s’appuyant sur cette representation du contexte. Ces methodes
semblent avoir atteint leur limite en termes de performance et les ameliorations les plus recentes concement plus
le cadre d’ evaluation des ces approches, plus contraint et limite (Yu & Tsujii, 2009), ou encore le traitement
de langues speciﬁques (Shezaf & Rappoport, 2010). Plus recemment, et en s’eloignant des approches tradition-
nelles, Li & Gaussier (2010) ont propose une approche basee sur l’amelioration de la comparabilité des corpus
comme prealable a l’extraction lexicale bilingue. Cette approche postule qu’il ne sert a rien d’essayer d’extraire
des lexiques bilingues a partir d’un corpus avec un faible degré de comparabilité puisque la probabilite de trouver
des traductions d’un mot donne sera faible dans une telle situation. Notre etude se situe dans la meme Veine que
cette precedente approche et Vise dans un premier temps a ameliorer la comparabilité d’un corpus donne, tout
en preservant une large part de son Vocabulaire. Neanmoins, nous nous differencions de ce precedent travail en
montrant qu’il est possible de garantir un certain degré d’homoge’ne’ite’ du corpus ameliore, et que celle—ci induit
une amelioration signiﬁcative de la qualite du corpus resultant et des lexiques bilingues extraits. En outre, nous
montrons que les lexiques extraits avec notre approche ameliorent de maniere manifeste les resultats d’un systeme
de recherche d’ information interlingue, meme lorsque ces lexiques sont issus d’un corpus different de la collection
interrogee.

2 Améliorer le degré de comparabilité d’un corpus

Nous commencons par donner dans cette partie la mesure de comparabilité que nous utilisons, avant de decrire un
algorithme permettant d’ameliorer la comparabilité d’un corpus donne. Nous foumissons egalement une preuve
du bien—fonde de notre algorithme, ainsi qu’une approximation conduisant a une implantation efﬁcace. Pour des
raisons pratiques, notre discussion se fera sur la base du couple de langues anglais—francais.

2.1 Mesure de comparabilité

Aﬁn de mesurer le degré de comparabilité d’un corpus bilingue, nous utilisons la mesure developpee dans (Li &
Gaussier, 2010) : etant donne un corpus comparable 73 constitue d’une partie anglaise 739 et d’une partie francaise
73;, 1e degré de comparabilité de 73 est deﬁni comme l’esperance de trouver la traduction d’un mot du Vocabulaire
source (respectivement cible) dans le Vocabulaire cible (respectivement source). Soit 0 une fonction indiquant si
une traduction de l’ensemble des traductions possibles Tu, du mot w se trouve dans le Vocabulaire 73” du corpus
73, c’est—a—dire :
«<w»»>={; 

et soit D un dictionnaire bilingue dont le Vocabulaire anglais (respectivement frangais) est note De (respectivement
1);). La mesure du degré de comparabilité M est deﬁnie par :

ZwEPgI')De “(W2 Pf) + ZwE’PfI'YDf °'(wa73e)
#..,(P., nDe)+#w(Pf nvf)

M(73ea73f) =

4 Conclusion

Dans cet article, nous avons propose une nouvelle approche pour augmenter le degre de comparabilité des do-
cuments constituant un corpus comparable aﬁn d’ ameliorer la qualité des lexiques bilingues extraits de corpus
comparables et les performances des systemes de recherche d’information interlingue. Nous avons démontre
théoriquement puis empiriquement que notre approche permet de garantir un certain degré de comparabilité et
l’homogenéité du corpus tout en préservant une large part du vocabulaire du corpus d’ origine. Enﬁn, nos expe-
riences montrent que les lexiques bilingues que nous obtenons sont d’une meilleure qualité que ceux obtenus avec
les approches precédentes, et que ces lexiques peuvent etre utilises pour ameliorer signiﬁcativement les resultats
des systemes de recherche d’information interlingue.

Les deux étapes cruciales de notre approche sont d’une part l’extraction d’un noyau fortement comparable du
corpus original, et, d’autre part, l’alignement des parties du corpus orignal, non presentes dans ce noyau, avec un
corpus exteme. Le seuil introduit au niveau du degré de comparabilité permet de contréler la taille et la qualité
du noyau extrait dans la premiere étape. Si le corpus original n’est que tres faiblement comparable, il est alors
possible que ce noyau soit vide (ce qui est un résultat souhaitable dans ce cas). Dans tous les cas, excepté celui
ou le noyau correspond au corpus original, le corpus ﬁnal depend de la proximité du corpus original (en fait de
la partie restante apres extraction du noyau) et du corpus exteme utilise. Bien evidemment, si le corpus exteme
est trop different du corpus original, l’on ne pourra pas completer correctement le noyau. Considerer des corpus
extemes les plus larges possibles permet ici d’augmenter les chances de trouver des documents comparables5
L’idéal serait bien sﬁr d’avoir acces a la collection la plus large possible, et le web constitue ici un excellent can-
didat. Il est cependant nécessaire de pouvoir, a partir d’un document donné dans une langue source, extraire du
web un ensemble de documents comparables en langue cible (on peut ensuite directement utiliser notre méthode
sur l’union de ces ensembles). Or nous n’avons pas réussi jusqu’a present a realiser correctement cette extraction.
La constitution entierement automatique de collections comparables a partir du web nous semble etre un pro-
bleme difﬁcile, qui requiert d’autres attributs que ceux utilises pour les corpus paralleles. C’est un point que nous
comptons developper dans le futur.

Remerciements

Ce travail qui s’inscrit dans le cadre du projet METRICC (www.met ri cc . com) a béneﬁcie d’une aide de
l’Agence Nationale de la Recherche portant la reference ANR—08—CORD—009. Enﬁn, nous tenons a remercier
les relecteurs pour leurs commentaires précieux.

Références

BALLESTEROS L. & CROFT W. B. (1997). Phrasal translation and query expansion techniques for cross-
language information retrieval. In Proceedings of the 20th ACM SIGIR, p. 84-91, Philadelphia, Pennsylvania,
USA.

5. C’est ce qui distingue les corpus ’P1 et 731 dans nos expériences, le deuxiéme étant obtenu a partir d’un corpus exteme a plus large
couverture.

DEJEAN H., GAUSSIER E. & SADAT F. (2002). An approach based on multilingual thesauri and model combi-
nation for bilingual lexicon extraction. In Proceedings of the I 9th International Conference on Computational
Linguistics, p. 1-7, Taipei, Taiwan.

FUNG P. & MCKEOWN K. (1997). Finding terminology translations from non—parallel corpora. In Proceedings
of the 5th Annual Workshop on Very Large Corpora, p. 192-202, Hong Kong.

FUNG P. & YEE L. Y. (1998). An IR approach for translating new words from nonparallel, comparable texts. In
Proceedings of the I 7th international conference on Computational linguistics, p. 414-420, Montreal, Quebec,
Canada.

GARERA N., CALLISON-BURCH C. & YAROWSKY D. (2009). Improving translation lexicon induction from
monolingual corpora via dependency contexts and part—of—speech equivalences. In CoNLL 09 : Proceedings of
the Thirteenth Conference on Computational Natural Language Learning, p. 129-137, Boulder, Colorado.

GAUSSIER E., RENDERS J MATVEEVA 1., GOUTTE C. & DEJEAN H. (2004). A geometric view on bilin-
gual lexicon extraction from comparable corpora. In Proceedings of the 42nd Annual Meeting of the Association
for Computational Linguistics, p. 526-533, Barcelona, Spain.

LAROCHE A. & LANGLAIS P. (2010). Revisiting context—based projection methods for term—translation spotting
in comparable corpora. In Proceedings of the 23rd International Conference on Computational Linguistics
(Coling 2010), p. 617-625, Beijing, China.

LI B. & GAUSSIER E. (2010). Improving corpus comparability for bilingual lexicon extraction from comparable
corpora. In Proceedings of the 23rd International Conference on Computational Linguistics, p. 644-652, Beijing,
China.

MORIN E., DAILLE B., TAKEUCHI K. & KAGEURA K. (2007). Bilingual terminology mining - using brain,
not brawn comparable corpora. In Proceedings of the 45th Annual Meeting of the Association for Computational
Linguistics, p. 664-671, Prague, Czech Republic.

OCH F. J. & NEY H. (2003). A systematic comparison of various statistical alignment models. Computational
Linguistics, 29(1), 19-51.

PEKAR V., MITKOV R., BLAGOEV D. & MULLONI A. (2006). Finding translations for low—frequency words
in comparable corpora. Machine Translation, 20(4), 247-266.

PIRKOLA A. (1998). The effects of query structure and dictionary setups in dictionary—based cross—language
information retrieval. In Proceedings of the 21 st annual international ACM SIGIR conference on Research and
development in information retrieval, p. 55-63, Melbourne, Australia.

RAPP R. (1999). Automatic identiﬁcation of word translations from unrelated English and German corpora. In
Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics, p. 519-526, College
Park, Maryland, USA.

ROBITAILLE X., SASAKI Y., TONOIKE M., SATO S. & UTSURO T. (2006). Compiling French—Japanese ter-
minologies from the web. In Proceedings of the I I st Conference of the European Chapter of the Association for
Computational Linguistics, p. 225-232, Trento, Italy.

SHEZAF D. & RAPPOPORT A. (2010). Bilingual lexicon generation using non—aligned signatures. In Procee-
dings of the 48th Annual Meeting of the Association for Computational Linguistics, p. 98-107, Uppsala, Sweden.
TALVENSAARI T., LAURIKKALA J ., JARVELIN K., JUHOLA M. & KESKUSTALO H. (2007). Creating and
exploiting a comparable corpus in cross—language information retrieval. ACM Trans. Inf Syst., 25(1), 4.

YU K. & TSUJII J . (2009). Extracting bilingual dictionary from comparable corpora with dependency hetero-
geneity. In Proceedings of HLT—NAACL 2009, p. 121-124, Boulder, Colorado, USA.

ou #1., (’P) represente le nombre de mots differents presents dans 73. Come on peut le Voir d’apres cette deﬁni-
tion, M mesure la proportion de mots source et cible dont une traduction est presente dans le Vocabulaire cible et
source de 7?. Pour des raisons qui deviendront claires plus tard, nous utiliserons aussi des mesures partielles ou

seuls les Vocabulaires francais ou anglais sont consideres. Ainsi, la proportion de mots anglais traduits sera notee
Ewe? my 0(w=73f)
#1.; (Pa 0733)

 

M e f, deﬁnie par : . La mesure M fe est deﬁnie de la meme fagon.

2.2 Classer les documents pour une meilleure comparabilité

L’hypothese distributionnelle sous—tendant l’extraction de lexiques bilingues est d’autant plus Valide que les do-
cuments dans les differentes langues couvrent des thematiques proches, car les auteurs ont alors tendance a puiser
dans le meme Vocabulaire (Voir (Morin et al., 2007) pour une analyse reliee). En d’autres termes, si un corpus
couvre un nombre limite de thematiques, il est plus a meme de contenir une information repetee et coherente
qui po11rra etre exploitee au mieux pour l’extraction de lexiques bilingues. Le terme homogénéité rend compte de
ce phenomene et nous dirons, de fagon informelle, qu’un corpus est homogene s’il couvre un nombre limite de
thematiques. Nous conj ecturons ici que si l’on peut garantir un certain degre d’homogeneite, en plus d’un certain
degre de comparabilité, alors les lexiques bilingues extraits seront de meilleure qualite. Comme nous le Verrons,
cette conjecture sera Validee par les experiences menees. De fagon a garantir un certain degre d’homogeneite, nous
nous appuyons sur des techniques de classiﬁcation non supervisee (clustering). Nous utilisons ici des techniques
de classiﬁcation agglomerative ascendante, mais toute autre technique, pour peu qu’elle dispose d’une procedure
de ﬁltrage adaptee, peut etre utilisee.

2.2.1 Algorithme de classiﬁcation bilingue

L’ ensemble du processus permettant de constr11ire, a partir d’un corpus donne, un corpus plus homogene et de
plus fort degre de comparabilité peut etre resume par les etapes suivantes :

1. A partir de la mesure de similarite, deﬁnie en 2.2.2 et fondee sur la mesure de comparabilité presentee
ci—dessus, et de l’ensemble des documents anglais et frangais du corpus originel 73, constr11ire les dendro—
grammes en suivant les etapes classiques de la classiﬁcation agglomerative ascendante;

2. Filtrer les dendrogrammes en ne retenant que les classes les plus profondes (Voir ci—dessous) ;

3. Fusionner les classes retenues pour former un nouveau corpus PH, qui contient une sous—partie homogene
et fortement comparable de 73 ;

4. Repeter les etapes ci—dessus pour enrichir la partie restante de 73 (partie qui sera notee ’PL, 73;, = 73 \ ’PH)
avec des documents extraits d’autres corpus.

Les trois premieres etapes sont detaillees dans l’algorith1ne 1, ou CAA signiﬁe Classiﬁcation Agglomerative As-
cendante. Come on peut le remarquer, seul 73 est utilise pour constr11ire PH, a travers des etapes de classiﬁcation
et de ﬁltrage. Ainsi, l’algorithme 1 Vise a extraire de 73 une sous—partie fortement comparable et homogene. Une
fois cela realise, c’est—a—dire une fois que 73 a ete exploite, il est necessaire de recourir a des ressources extemes
si l’on Veut constr11ire un corpus fortement comparable a partir de 73;, (qui est la partie restante de 7?). Pour cela,
deux nouveaux corpus comparables sont consideres dans l’etape 4 du processus global : le premier consiste en la
partie anglaise de 73;, et la partie frangaise d’un autre corpus PT ; le second consiste en la partie francaise de 73;, et
la partie anglaise de PT. Les deux sous—parties fortement comparables et homogenes obtenues a partir de ces deux
corpus sont alors ajoutees a PH pour constituer le corpus ﬁnal. L’utilisation de la classiﬁcation agglomerative as-
cendante et du ﬁltrage associe garantit que le corpus ﬁnal est homogene. La propriete 1 que nous presentons plus

Algorithm 1: Algorithme de classiﬁcation bilingue
Entrée :
Ensemble M de tous les documents anglais et frangais de P
Reel positif 0 (seuil de profondeur)
Sortie :
PH, sous—partie fortement comparable et homogene de P

: Initialiser PH = 0;

: CAA(?/I) —> ensemble 8 de dendrogrammes

: for chaque dendrogramme T de 8 do

m <— profondeur maximale de T;

for tous les noeuds n de T do

if profondeur(n) 2 m - 0 then
Ajouter tous les documents sous le noeud n a PH ;
end if

end for

: end for

: Supprimer les doublons de PH ;

: return PH ;

5.~99s=:I.cz~.~:-.4.>w-

,_t
,_t

>—A
[0

loin établit que ce corpus est fortement comparable. Mais avant de Voir en detail cette propriete, nous introduisons
la mesure de similarité utilisée.

2.2.2 Mesure de similarité

Imaginons deux classes de documents bilingues C1 et C2. Pour la tache d’extraction de lexiques bilingues, ces
deux classes sont similaires et devraient etre regroupées si leur combinaison permet de compléter le contenu de

chacune des classes prise isolement, ou, en d’ autres termes, si la partie anglaise Cf de C1 et la partie frangaise Cfr
de C1 sont comparables a leur contrepartie dans l’autre classe (respectivement la partie frangaise C; de C2 et la
partie anglaise C§ de C2) 1. Ceci conduit a la mesure de similarité suivante pour C1 et C2 :

s2'm(C1,C2) = BM(Cf,c§) + (1 — ﬁ)M(c§,c{) <1)

ou C (0 3 C 3 1) est un poids qui permet de controler l’importance de chacune des deux parties (Cf, cg”) et (C§,
Cf). De fagon intuitive, on aimerait donner plus de poids dans cette combinaison a la partie la plus importante,
car elle contient plus d’information. Si nous utilisons le nombre de paires de documents anglais—frangais pour

quantiﬁer cette information, le poids ﬂ peut etre déﬁni comme la proportion de paires de documents dans (Cf, C; )
sur l’ensemble des paires de documents dans le corpus fusionne :

B : #d(Cf) ' #d(C;)
#a(Cr) - #405) + #465) - #a(C{)
ou #01 (C) représente le nombre de documents dans C. Dans la mesure ou les classes sont tout d’abord formées

de documents anglais et frangais isolés, la mesure de similarité correspond a un score de comparabilité normalise
e11tre les corpus anglais et frangais qui forment la nouvelle classe. Cependant, cette mesure ne tient pas compte des

1. Dans la mesure o1‘1 C1 ct C2 sont des classes, leurs parties anglaise ct francaise sont comparables par construction.

longueurs relatives des corpus anglais et frangais, qui ont pourtant un impact sur la qualité des corpus bilingues
extraits. Si une contrainte de type 1-1 (c’est—a—dire imposant a chaque classe de contenir le meme nombre de
documents anglais et frangais) est trop forte, se reposer sur des classes par trop déséquilibrées n’est pas non plus
souhaitable. Nous introduisons donc une nouvelle fonction ¢ qui a pour but de pénaliser les classes pour lesquelles
les nombres de documents anglais et francais sont trop djfférents :

 2 :; Ce _# cf 

(1 + l°9(1 + 7mm<#.t<ce>>,#d<cf>>)

avec *y E ]R+. Cette fonction de pénalité foumit une nouvelle mesure de similarité sim; qui est celle utilisée dans
l’algorithme 1 :
sim;(C1,C2) = sim(C1,C2) - ¢(C1 U C2) (3)

Dans la suite de cette etude, *y est ﬁxé a 1 dans <;$.

2.2.3 Analyse théorique

Le processus de classiﬁcation utilise dans l’algorithme 1 garantit que les documents qui portent sur la meme
thématique seront regroupés avant les documents portant sur des thématiques diﬁ’e’rentes. Le corpus obtenu (PH)
sera ainsi homogene, c’est-a-dire qu’il ne couvrira qu’un nombre restreint de thématiques. De plus, le fait que le
corpus comparable (que nous noterons Pp) obtenu au travers des étapes 1 a 4 découle du corpus originel 79 indique
que la plus grande partie du Vocabulaire de 73 sera préservée dans Pp. Nous Verrons dans la partie expérimentale
que c’est bien le cas. Ce qui semble moins evident, c’est le fait que le processus que nous avons déﬁni garantisse
un fort degre de comparabilité. La propriété suivante etablit que c’est bien le cas.

Propriété 1 Soit C1 et C2 deux classes de documents qui doivent etre regroupe’es dans le processus de classiﬁ-
cation. Nous faisons I ’hypothese que le dictionnaire bilingue D a été construit indépendamment des documents
traite’s, ce qui implique que le degre’ de comparabilite’ Mef (respectivement de meme pour M fe ) est :2 peu pres le
meme pour diﬂérentes parties du corpus 2. Nous faisons de plus I ’hypothese que :

(I) Ic:uc;I_Ic{uc§I
|C§| _ |c,7|

Alors :
M(Cf U 65,61‘ U 05) 2 min(M(cf, cf), M65, 6%))

Demonstration (esquisse) : Soit V = Cf ﬂ C§. En utilisant le fait que Mef(Cf, Cg ) 3 Mef (Cf, C19”) pour tout CZ-fl
I
tel que C; Q C; (et de meme pour la direction frangais Vers anglais), nous avons, pour z’ = 1, 2 :
Z a(w,c{ wt» 2 Ic:\VIMef(c:,c{)
wECf\V

et, pour les mots de V :
2 «(met uc§)) 2 |V|maX(Mef(CiaCi)aMef(C;=C§))
1uEV

2. En d’ autres termes, la proportion de mots anglais (respectivement francais) traduits dans le corpus francais (respectivement anglais) est
homogéne sur1’ensemble du corpus.

Alors, d’apres l’hypothese d’independance entre corpus et dictionnaire faite en énongant la propriété 1 :

Z a(w,C{ UC§))

we(Cfuc§)nD,,

2 KC? u cs) n De|mjn(Mef(Cf:C{): Me.«(c3,c§)>
Un développement similaire sur M fe et l’utilisation de la condition (I) completent la demonstration.

La propriété précédente garantit que la classe obtenue en fusionnant deux classes existantes a un degre de compa-
rabilite au moins egal a celui de la classe la moins comparable. Le degré de comparabilité ne peut donc decroitre
dans le processus de classiﬁcation agglomérative. Comme l’on commence par fusionner les documents les plus
comparables, on ne constr11it que des classes avec un bon degre de comparabilite. Enﬁn, la condition (I) a de
grandes chances d’etre realisée car tous les corpus sont pretraites de fagon a eliminer les documents trop courts ou
trop longs, souvent source de bruit, et la penalité utilisee dans la mesure de similarite foumit des classes compre-
nant des nombres comparables de documents dans les deux langues. Le processus global que nous avons deﬁni
permet donc d’obtenir des corpus homogenes et fortement comparables.

2.3 Considérations informatiques

Dans la mesure ou les corpus comparables disponibles a l’heure actuelle comprennent en general un nombre
important de documents, la classiﬁcation agglomerative peut s’aVerer trop coﬁteuse. Nous proposons ici une borne
inferieure de la mesure de comparabilité qui peut etre calculee efﬁcacement ainsi qu’une mise a jour efﬁcace de
la matrice de similarité pendant le processus de classiﬁcation. Le fait de se reposer sur une borne inferieure de la
mesure de similarite garantit que les classes obtenues auront un bon degré de comparabilite, car seules les classes
les plus similaires sont regroupées a chaque iteration de l’algorith1ne de classiﬁcation. La propriété suivante établit
une telle borne inférieure, sur la base du degré de comparabilité moyen des paires de documents.

Propriété 2 Soit 73 un corpus comparable compremmt une partie anglaise 739 et une partie francaise ’Pf, et soit
D un dictiormaire bilingue, De dénotant le vocabulaire anglais et 1); le vocabulaire francais. Supposons que le
dictionnaire est distribue’ de facon umforme sur le corpus, c’est—d—dire que :

#w<de n D.) : #w<v>e we)
#w<de> #w<7>e>

et de meme pour la partie francaise. Supposons de plus que tous les documents, ainsi que les parties anglaise et
francaise du corpus, ont ti peu pres la meme longueur :

Vde 6 775,

we 6 79, anddf e 79,, %$ 2  (= A)
Alors :

M(7>,,7>,) > 1

 Z M‘d“df>

eepevdfepf

Nous ne détaillons pas ici la demonstration de cette propriété, purement technique. La premiere hypothese faite
semble raisonnable (et rejoint celle faite dans la propriéte precedente) en l’absence de toute connaissance a priori
sur les thématiques couvertes par le corpus et leur lien avec le dictionnaire. La seconde hypothese est en partie
garantie dans notre cas par le processus de construction que nous avons deﬁni et la fonction de pénalité associée.

Remplacer M par la borne ci—dessus dans l’equation 1 conduit £1 une mesure de similarité qui peut etre Vue comme
la Valeur accumulee de toutes les connexions er1tre deux classes. 11 est alors possible de mettre £1 jour la matrice
de similarité de fagon iterative. Supposons en effet que le processus de classiﬁcation doive, £1 un instant donné,
fusionner les classes C1 et C2 en une seule classe Cnew. Un nouveau score de similarité entre Cmw et toutes les
autres classes doit alors etre calculé. La similarité entre Cm”, et une autre classe C3 peut s’écrire, £1 partir de
l’équation 3 et de la formule de similarité :

(N61 + N62 )¢(C1 U C2)
#a<c:...) - #a<c.£) + #..<c;) - #d<c£w)

siml (Guam 5 C3) =

o1‘1(j=1,2)et:

N _ <#a<c;) - #a<c.Z.) + #a<c;) - #d<c,f))sim1<cj,c.».)
“J” ‘ ¢<c,- UC3)

Dans le processus de classiﬁcation, dans la mesure ou sz'm;(C1,C3) et sim;(C2, C3) sont dej£1 connus avant le
calcul de sim; (Cum, C3), la matrice de similarité peut directement etre mise £1jour £1 chaque iteration. En notant Ne
le nombre de classes avant fusion, la complexite de cette mise £1 jour est de l’ordre de O(Nc), alors qu’elle atteint
(9(Nc >< C72) si l’on applique directement les equations 1 et 3 (C' représentant le nombre moyen de documents par
classe).

3 Expériences et résultats

Les differentes experiences que nous avons realisées ont pour objectif d’éValuer : (i) si l’algorithme que nous
avons propose induit des corpus d’une meilleure qualité en ce qui concerne la comparabilite, (ii) si les lexiques
bilingues extraits de ces corpus sont eux aussi d’une qualité plus importante, et (iii) si ces lexiques peuvent etre
utilises pour améliorer les performances des systemes de recherche d’information interlingue.

Dans nos experiences, djfférents corpus sont utilises : le corpus anglais TREC 3 de l’Associated Press (note AP)
et les corpus fournis dans les taches multilingues des campagnes CLEF 4 dont pour l’anglais le Los Angeles Times
(LAT94) et le Glasgow Herald (GH95) et pour le frangais Le Monde (M ON94), le SDA 94 (SDA94) et 95 (SDA95).
Outre ces corpus existants, deux corpus monolingues ont ete extraits £1 partir de Wikipe’dia : le corpus anglais Wiki—
En construit en retenant l’ensemble des articles appartenant £1 la catégorie Society pour une profondeur inferieure
£1 4 (soit 33 000 mots anglais distincts) et le corpus frangais Wiki—Fr toujours pour la catégorie Sociéte’ pour une
profondeur inferieure £1 7 (soit 28 000 mots francais distincts). Le dictionnaire bilingue bdo necessaire pour la tache
d’ extraction de lexiques est quant £1 lui constr11it £1 partir de dictionnaires en ligne. Dans toutes nos experiences,
nous utilisons la méthode décrite dans le present article completee par celle presentee dans (Li & Gaussier, 2010).
Cette demiere methode est £1 notre connaissance la seule approche alternative pour ameliorer la comparabilité des
corpus, d’o1‘1 son importance dans l’éValuation.

3.1 Comparabilité de corpus

L’ algorithme de classiﬁcation décrit en section 2.2.1 est utilise pour améliorer le degré de comparabilité d’un
corpus comparable. Les corpus GH95 et SDA95 sont utilises pour construire le corpus comparable 73° (56 000

3. http: //trec .nist .gov/
4. http: //www. clef—campaign . org

mots pour l’anglais et 42 000 le francais). En outre, nous exploitons deux corpus comparables supplementaires
pour nous assurer que l’efﬁcacité de notre algorithme n’est pas liee a une ressource externe spéciﬁque : i) P711
compose a partir des corpus LAT94, MON94 et SDA94 (109 000 mots pour l’anglais et 87 000 pour le francais) et
ii) P% compose a partir des corpus Wiki—En et Wiki-Fr (368 000 mots pour l’anglais et 378 000 pour le francais).

Apres le processus de classiﬁcation, nous obtenons les corpus P1 (pour le corpus externe P711) et P2 (pour le
corpus externe P%). Comme nous l’aVons indiqué precedemment, nous utilisons aussi la méthode décrite dans (Li
& Gaussier, 2010) sur les memes donnees pour comparer nos resultats et obtenons ainsi le corpus P1, (pour P71~) et
P2, (pour P%) a partir de PO. Au niveau de la couverture lexicale, P1 couvre 97,9% du Vocabulaire de PO, tandis
que P2 couvre 99,0% de celui de PO. Nous pouvons ainsi constater qu’une tres grande partie du Vocabulaire du
corpus d’origine a eté conserve, ce qui est l’une des exigences de notre approche. En ce qui concerne les scores de
comparabilite, P1 atteint 0,924 et P2 0,939. Les deux corpus comparables ont donc bien un degré de comparabilité
supérieur au corpus d’origine qui etait de l’ordre de 0,881 comme cela est suggérée par la propriété 1. En outre,
les corpus P1 et P2 sont plus comparables que le corpus P1’ (comparabilité de 0,912) et P2, (comparabilite de
0,915) ce qui montre bien que l’homogénéité est un element crucial pour évaluer la comparabilite.

3.2 Extraction de lexiques bilingues

TABLE 1 — Evaluation des lexiques bilingues extraits pour différents corpus comparables
|79° || 791’ | 792’ || 791 | 792 || 791>79° | 792>79°

Precision 0,226 0,277 0,325 0,295 0,461 0,069 30,5 % 0,235 104,0%

Rappel 0,103 0,122 0,145 0,133 0,212 0,030 29,1 % 0,109 105,8%

TABLE 2 — Comparaison de la precision pour djfférents intervalles de frequences des mots de la liste d’eValuation
| 790 || 792’ | 792 || 79?’ >790 | 792 >790 || 792 >792’
I/V; 0,135 0,206 0,304 0,071 52,6 % 0,169 125,2 % 0,098 47,6 %
Wm 0,256 0,390 0,564 0,134 52,3 % 0,308 120,3 % 0,174 44,6 %
W}, 0,434 0,632 0,667 0,198 45,6 % 0,233 53,7 % 0,035 5,5
,All 0,226 0,325 0,461 0,099 43,8 % 0,235 104,0 % 0,136 41,8 %

Comme les travaux antérieurs en extraction de lexiques bilingues a partir de corpus comparables exploitent des
ressources djfferentes et operent des choix distincts des notres, il est relativement difﬁcile de se comparer a ceux—
ci (Laroche & Langlais, 2010). En outre, puisque notre approche Vise a améliorer la comparabilite de corpus,
elle peut etre ensuite couplee a une méthode existante d’ extraction de lexiques bilingues. Il est donc tout aussi
intéressant de directement évaluer si un tel couplage peut conduire a des performances accrues en tennes de
qualité des lexiques extraits.

L’ extraction de lexiques bilingues a partir de corpus comparables repose sur la methode proposee par Fung & Yee
(1998) plus connue maintenant sous le nom d’approche standard notamment dans les travaux de (Dejean et al.,
2002; Gaussier et al., 2004; Yu & Tsujii, 2009). Dans cette approche, chaque mot est représenté sous la fonne
d’un Vecteur de contexte compose des mots qui co—occurrent avec lui dans une fenetre donné. Les Vecteurs de
contexte de la langue source sont ensuite traduits Vers la langue cible en s’appuyant sur un dictionnaire bilingue.

Enﬁn, la traduction d’un mot est obtenue en comparant son Vecteur de contexte traduit £1l’ensemble des Vecteurs
de la langue cible £1 travers une mesure de distance ou similarité Vectorielle telle que le cosinus.

3.2.1 Paramétres expérimentaux

Aﬁn d’eValuer la qualité des lexiques bilingues extraits, nous divisons notre dictionnaire bilingue bdg en deux
parties : 10 % des mots anglais accompagnés de leurs traductions sont choisis aleatoirement et uniquement utilises
comme liste d’éValuation, les 90 % restant sont utilises pour assurer la traduction des Vecteurs de contexte dans
l’approche standard. Les mots anglais absents de P9 ou pour lesquels aucune traduction n’a éte trouvée dans Pf
sont retires de la liste d’éValuation. Pour chaque mot anglais de la liste d’eValuation, tous les mots frangais de
Pf sont ordonnés suivant leur similarite avec les mots anglais. Les mesures de precision et rappel sont ensuite
calculées sur les N premiers candidats. Les Valeurs de la precision dans ce cas correspondent £1 la proportion de
listes contenant la traduction correcte (en cas de traductions multiples, une liste est réputée contenir la traduction
correcte des lors que l’une des traductions possibles est présente). Le rappel est quant £1 lui la proportion de
traductions correctes trouvée dans les listes sur toutes les traductions foumies dans le corpus. Cette maniere de
proceder a ete utilisee dans différents travaux anterieurs et peut etre maintenant considéree comme un méthode
d’ evaluation attestee. En outre, plusieurs études ont montré qu’il est plus facile de trouver les traductions correctes
pour les mots frequents que pour les mots rares (Pekar et al., 2006). Aﬁn de prendre en compte ce phenomene,
nous distinguons differents intervalles d’effectifs pour evaluer la Validite de notre approche. Ainsi, les mots avec
un effectif inferieur £1 100 sont deﬁnis comme etant des mots de faibles frequence (I/V5), ceux avec un effectif
supérieur £1 400 sont déﬁnis comme étant des mots tres frequents (Wh), et enﬁn les mots dont l’effectif est compris
er1tre ces deux seuils sont consideres comme des mots de frequence intermediaire (Wm).

3.2.2 Analyse des résultats

Dans une premiere serie d’expériences, les lexiques bilingues sont extraits £1 partir des corpus obtenus ii) par notre
approche (P1 et P2), ii) par la méthode décrite dans (Li & Gaussier, 2010) (P1, and P2’) et iii) enﬁn avec le
corpus d’origine PO, avec N ﬁxe £1 20. La table 1 presente les résultats obtenus. Les deux demieres colonnes
“P1 > PO” et “P2 > PO” indique les differences absolue et relative, exprimées en pourcentage, par rapport £1
P0. Comme nous pouvons le constater, les meilleurs résultats sont obtenus £1 partir des corpus constr11its avec
la methode que nous avons proposée. Les lexiques extraits £1 partir du corpus ou le degré de comparabilité a eté
renforce sont d’une bien meilleure qualite que ceux obtenus £1 partir du corpus d’origine ou encore du corpus
construit avec l’approche de (Li & Gaussier, 2010). La difference de qualite est encore plus notable avec P2
qui est obtenu £1 partir d’un corpus exteme Volumineux P%. Ces résultats semblent conﬁrmer l’intuition qu’il est
possible de trouver plus aisément dans des corpus Volumineux des documents en relation avec un corpus donné.

Aﬁn d’éValuer la relation entre la qualité de ces méthodes et la frequence des mots £1 traduire, nous nous concen-
trons sur les meilleurs résultats sur P2, pour l’approche precédente et sur ceux de P2 pour notre approche. La
table 2 resume les résultats obtenus. On remarquera, sans Veritablement de surprise, que les résultats obtenus
pour les mots ayant une haute frequence sont meilleurs que ceux obtenus pour les mots de faible fréquence. En
outre, notre approche est la meilleure quel que soit l’interValle de fréquence pris en compte. La precision globale
peut etre augmentée en relatif de 41,8 % (de 0,325 £1 0,461). En comparant P2 avec le corpus d’origine PO, nous
pouvons noter pour la precision globale, une augmentation relative de 104,0 % (de 0,226 £1 0,461), ce qui est tres
satisfaisant dans ce contexte d’éValuation. Enﬁn, l’amelioration pour les mots de faible et moyenne fréquence est
plus importante pour P2, ce qui demontre que notre approche se comporte bien mieux sur ce qui est généralement

considere comme un probleme difﬁcile (Pekar et al., 2006).

3.3 Experiences en recherche d’information interlingue

TABLE 3 — Score MAP pour la tache de recherche d’information interlingue suivant differents dictionnaires bi-
lingues

| man  M1 | bd1+cc0 | bd1+cc1 | bd1+cc2  bdg | bd2+cc0 | bd2+cc1 | bd2+cc2
MAP | 0,422 || 0,313 | 0,327° | 0,323° | 0,333° || 0,375 | 0,332 | 0,377 | 0,391°

Dans la demiere serie d’experiences, nous cherchons a evaluer l’apport des differents lexiques extraits a partir de
corpus comparables pour une tache de recherche d’ information interlingue. Pour ce faire, nous exploitons les suj ets
des campagnes CLEF de 2001 et 2002, rassemblant environ 100 suj ets distincts, comme requetes sur une collection
de 113 000 documents issus du Los Angeles Times. Les sujets anglais correspondants sont utilises pour interroger la
meme collection (reference mon). Seul le titre et la partie description des suj ets CLEF sont utilises pour construire
des requetes. En outre, les mots outils et les phrases non pertinentes telles que ﬁnd documents which report about
sont supprimes des requetes. La recherche est realisee avec le modele Indri du systeme de recherche d’information
Lemur (http ://www.lemurproj ect.org). Une Variante de l’approche introduite dans (Pirkola, 1998) et (Talvensaari
et al., 2007) est aussi utilisee pour transformer les suj ets frangais en requetes en anglais. L’ idee est de bomer toutes
les possibilites de traduction d’un mot frangais dans le suj et du texte avec un operateur WSYN. Ensuite, toutes les
traductions candidates dans l’operateur WSYN sont traitees comme des synonymes avec des poids differents.

Dans nos experiences, nous combinons deux dictionnaires bilingues de langue generale bdl (68 0000 traductions)
et bdg (116 000 traductions) avec les lexiques bilingues obtenus automatiquement dans la precedente section. Nous
utilisons ici les lexiques ccg (extrait de 730), cc1 (extrait de 732/) et CC2 (extrait de 732). Differentes combinaisons
de ces ressources sont realisees, y compris bdl/2, bdl/2+cc0, bdl/2+cc1, bdl/2+cc2. Lorsque qu’un dictionnaire
de langue generale et un lexique extrait sont combines, plus de poids est attribue aux traductions candidat du
dictionnaire de langue generale. Le poids des differents mots traduits a partir de cco/1/2 est quant a lui le cosinus
entre les Vecteurs de contexte de chaque mot (c’est-a-dire le score donne par l’approche standard precedemment
evoquee). Le poids pour les traductions trouvees dans le dictionnaire bilingue est ﬁxe empiriquement a 25. Come
il est d’usage en recherche d’information, nous utilisons la mesure MAP (Mean Average Precision) aﬁn d’eValuer
les performances des differents systemes. L’importance des differences entre les differents systemes est estimee
par un t—test apparie de Student (p—Value ﬁxee a 0,1). Les resultats obtenus sont indiques dans la table 3. Pour le
dictionnaire de langue generale bdl, on note touj ours une amelioration signiﬁcative des resultats (identiﬁee par la
marque 0) du score MAP lorsque l’un des lexiques bilingues extraits du corpus comparables est utilise. Lorsque
bdg, qui est beaucoup plus riche que bdl, est utilise, seulement le lexique bilingue CC2 extrait avec notre methode a
partir ’P2 conduit a une amelioration signiﬁcative des resultats. Cela montre que CC2 est superieure a ccl et cco dans
la tache de recherche d’information interlingue, en particulier lorsque le dictionnaire de langue generale utilise
est d’une taille importante. Ces resultats semblent conﬁrmer que notre approche basee sur de la classiﬁcation est
plus adaptee que l’approche gloutonne des travaux precedents de (Li & Gaussier, 2010). Enﬁn, la combinaison
actuelle des lexiques extraits avec le systeme de recherche d’information interlingue est relativement simple et
pourrait etre certainement amelioree en exploitant d’autres modeles de combinaison.

