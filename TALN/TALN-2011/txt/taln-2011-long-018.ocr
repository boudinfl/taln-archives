TALN 2011, Montpellier, 27j1/tin - Ierjuillet 2011

Acquisition automatique de terminologie a partir de corpus de texte

Edmond Lassalle

(1) Orange Labs, 2 avenue Pierre Marzin
22 307 Lannion — France
edmond.Iassa||e@orange—ftgroup.com

Résumé :

Les applications de recherche d'informations chez Orange sont confrontées a des ﬂux importants de
données textuelles, recouvrant des domaines larges et évoluant tres rapidement. Un des problemes a
résoudre est de pouvoir analyser tres rapidement ces ﬂux, a un niveau élevé de qualité. Le recours a un
modele d'analyse sémantique, comme solution, n'est viable qu'en s'appuyant sur l'apprentissage
automatique pour construire des grandes bases de connaissances dédiées a chaque application.
L'extraction terminologique décrite dans cet article est un composant amont de ce dispositif
d'apprentissage. Des nouvelles méthodes d'acquisition, basée sur un modele hybride (analyse par
grammaires de chunking et analyse statistique a deux niveaux), ont été développées pour répondre aux
contraintes de performance et de qualité.

Abstract :

Information retrieval applications by Orange must process tremendous textual dataﬂows which cover
large domains and evolve rapidly. One problem to solve is to analyze these dataﬂows very quickly, with
a high quality level. Having a semantic analysis model as a solution is reliable only if unsupervised
learning is used to build large knowledge databases dedicated to each application. The terminology
extraction described in this paper is a prior component of the learning architecture. New acquisition
methods, based on hybrid model (chunking analysis coupled with two-level statistical analysis) have
been developed to meet the constraints of both performance and quality.

Mots-clés : Apprentissage automatique, acquisition terminologique, entropie, grammaires de chunking
Keywords: Unsupervised learning, terminology acquisition, entropy, chunking analysis

EDMOND LASSALLE

1 Introduction

Une amelioration significative de la qualite des moteurs de recherche concerne l'identification des locutions
en tant qu'unites de sens. C'est aussi une difficulte dans le cas de certaines applications d'Orange. Le
probleme est en effet de pouvoir prendre en compte une terminologie en constante evolution dans des
domaines lies a l'actualite (presse, journaux televises...). Il s'agit en plus de traiter en continu des ﬂux
importants de donnees pour indexer les nouveaux documents entrants mais aussi pour acquerir une
terminologie evanescente 0‘uite de pe’trole, nuage de cendres, Jean Paul II, Sidi Bouzid, Antoine de
Le’ocour ...). Les méthodes d'acquisition automatique de terminologie a partir de corpus trouvent ici leur
entiere justiﬁcation.

Un examen de differents modeles d'apprentissage, de leur adequation aux corpus dans nos applications Va
motiver une architecture hybride differente de celles connues et etudiees a ce jour. Ce choix oblige a innover
dans les methodes d'analyse linguistique et statistique pour repondre aux contraintes operationnelles de
qualite. L'objet de nos travaux est alors, d'avoir un systeme «homogene» pour limiter le biais statistique
inherent aux interactions dans tout modele hybride. La loi binomiale regissant le comportement des mots
constitue donc la seule hypothese de depart. Des observations experimentales, une modelisation formalisee
permettent ensuite de deriver par calcul les autres lois. Les resultats obtenus vont conﬁrmer la pertinence de
cette demarche. Dans la suite de l'article, une description du modele d'apprentissage, des methodes d'analyse
statistique Va donner un eclairage sur le fonctionnement de notre composant linguistique.

2 Motivation d'un modéle hybride d'acquisition terminologique

Le choix d'une architecture est dicte par le type de corpus d'apprentissage. Le notre est constitue de textes
decrivant des videos sur un mois d'actualites (http2//www.2424actu.fr/actualite-du-"our/). A chaque instant,
on dispose de 100 000 textes pour un total de 5 millions de mots. Chaque texte comprend un titre suivi d'un
resume court comme 2 «Tunisie : aﬂrontements a Sidi Bouzid. De nouveaux aﬂrontements violents ont eu
lieu dans la nuit dans la région de Sidi Bouzid, dans le centre-ouest de la Tunisie, faisant un blessé par
balle et des dégats matériels importants, a—t-on appris dimanche de sources syndicales Des centaines de
Tunisiens ont participé a une manfestation.»

Dans ce type de corpus, certaines locutions — etant communes (dégats mate’riels, sources syndicales) —
peuvent etre obtenues hors methodes d'apprentissage, mais d'autres (Sidi Bouzid ou Camp Nou) risquent de
ne pas figurer dans un referentiel lexical qui serait etabli a priori. Le probleme a traiter est donc d'avoir un
referentiel de mots simples exhaustif, incluant des mots inconnus. Une analyse visant a extraire des
locutions devra ensuite identifier des constructions bien formees de groupes de mots, puis reconnaitre la
nature compositionnelle ou figee du sens porte par ces constructions, y compris celles comportant des mots
inconnus. Les solutions a cette problematique peuvent étre d'ordre statistique ou mixte, mais excluent une
approche symbolique confrontee au probleme d'exhaustivite.

2.1 Modéles statistiques

L'apport des methodes statistiques concerne la quantification de la compositionalite. L'occurrence d'un mot
mi dans un corpus est modelise par une loi de Bernoulli de parametre pi. Le comportement d'un mot dans le
corpus est ensuite explique par sa frequence d'occurrences et donc par une v.a.r de loi binomiale B(n,p,).
Estimer le degre de compositionalite de deux mots contigus revient alors a determiner le degre de
dependance des v.a.r associees a ces mots. Deux methodes experimentales permettent de realiser ce calcul 2

0 La premiere necessite une fenétre d'observation (par exemple la phrase) pour estimer les
probabilites d'occurrences et de cooccurrences a partir d'un comptage frequentiel. Elle conduit au
calcul de l'information mutuelle (Church et al., 1990) ou a la mesure de Dice (Smadja, 1993).
Citons aussi pour cette methode, le calcul de la log-perplexite (Kit, 2002) qui a l'avantage de
prendre en compte des sequences de N mots mais necessite en contre partie un modele de langue
pour viabiliser l'estimation de la probabilite de telles sequences.

0 La seconde realise un comptage frequentiel direct de la cooccurrence, de la non—cooccurrence et
des non-occurrences de deux mots contigus pour determiner la log-vraisemblance des 2 v.a.r

. , . . , . . 2

assoc1ees (Dunning, 1993) ou aussi leur correlation v1a le calcul du x .

ACQUISITION AUTOMATIQUE DE TERMINOLOGIE A PARTIR DE CORPUS DE TEXTE

Le resultat pour ces 2 methodes est un classement suivant une «Vraisemblance d'étre une locution». La
difficulte restant est de determiner la Valeur de seuillage, mais aussi de mesurer l'importance des termes par
rapport au corpus applicatif.

Pour traiter ce dernier point, les modeles les plus avances (Kit, 2002) (Vu et al., 2008) (Kageura et al., 1996)
caractérisent les sequences extraites par le critere d'unithood, Validant statistiquement la coherence de la
sequence, et par le critere de termhood, caractérisant la specificité de la sequence par rapport au corpus
applicatif. En l'absence d'analyse linguistique, le premier critere permet de Valider la construction
syntaxique de la sequence tandis que le second critere Valide a la fois la non—compositionalite et
l'importance de cette sequence. Cette approche est adaptée pour les domaines techniques ou le Vocabulaire
est limite, ou les expressions figées peuvent étre longues comme Alteration des facteurs de coagulation
sanguine, ou le critere de spécificité est assez proche du critere de non—compositionalite. Une Variante
intéressante (Frantzi et al., 1999) est d'introduire le filtrage de categories grammaticales et de palier
l'absence d'analyse syntaxique par des mesures statistiques (AC/NC—Value).

2.2 Modéles hybrides

Le modele le plus usite est base sur un fonctionnement en tandem du composant linguistique et du
composant statistique. L'aVantage d'une telle architecture concerne la modularité. L'analyse linguistique est
chargée d'annoter le corpus initial (étiquetage grammatical, parenthésage et étiquetage des syntagmes).
L'analyse statistique reprend les inforrnations annotees pour produire une liste de termes classes suivant un
ordre de Vraisemblance. Cette approche permet en plus de reprendre pour le deuxieme composant (Daille,
1996) les mesures utilisees par les modeles statistiques. L'inconVénient du modele en tandem concerne le
biais statistique. Les evaluations que nous aVons menées (Lassalle et al., 2011) sur Acabit ont indique un
différentiel de 30% du taux de precision suivant que nous utilisons en amont, comme composant
linguistique, l'analyseur de Brill (Brill, 1992) couple au lemmatiseur Flem (Namer, 2000) ou l'analyseur Tilt
(Heinecke et al., 2008).

Seul un couplage fin entre analyse linguistique et analyse statistique permettrait de minimiser ce biais. Ce
qui exclut une reutilisation des mesures de classement des modeles statistiques car ces dernieres necessitent,
dans le calcul, des donnees globales et non partielles comme c'est le cas dans un couplage ﬁn. Cela nous
conduit a spécialiser nos méthodes d'analyse statistique dans deux directions :

0 la premiere pour detecter les elements saillants (analyse de régularite)
0 la seconde pour estimer la non—compositionalite des constructions syntaxiques.

Le role de l'analyse linguistique dans cette approche hybride est de proposer successivement des ensembles
«statistiquement coherents» de constructions syntaxiques. Ce que nous preciserons apres aVoir decrit dans
un premier temps les analyses statistiques.

3 Analyse statistique de la régularité

Les finalités de l'analyse statistique décrite dans cette section sont triples. La meme observation
expérimentale permet en effet de deduire les caractéristiques des mots dans le corpus, suivant 2
0 une loi de distribution decrivant leur occurrence,
0 des propriétés macroscopiques autorisant leur regroupement au sein de categories grossieres,
0 et le degré de saillance permettant d'identiﬁer les mots importants dans le corpus.

Seul, le calcul de saillance est preeminent dans l'acquisition de terminologie. La loi de distribution permet
de deduire la loi conjulguee a priori et elle est plutot utilisee dans nos modelisations bayésiennes, comme
dans la categorisation ou dans l'indexation. Le regroupement des mots en categories grossieres, bien
qu'utile dans le processus d'acquisition terminologique, nécessite une extension (restant a faire) du calcul de
saillance.

3.1 Loi de distribution des mots

Si l'on accepte que l'occurrence d'un mot dans un corpus suit une loi de Bernoulli de parametre p, alors sa
frequence d'apparition dans une fenétre de n mots d'un corpus suit la loi binomiale B(n,p). La Valeur de p est
en general tres faible, a l'exception des mots grammaticaux et des termes de domaine (dans le cas de corpus

1Blei D.M., (2003). Latent Dirichlet Allocation, Journal of Machine Learning Research

EDMOND LASSALLE

specialises comme ceux de la medecine, des finances,...). Il est donc possible pour les grandes valeurs de n
d'approximer la loi binomiale B(n,p) par une loi de Poisson ou par une gaussienne discretisee (Saporta
2006).

L'interét d'une loi de Poisson PO») par rapport a une gaussienne est d'avoir l'esperance et la variance egales a
X. Pour les grandes valeurs de 7» (?\«>18), PO») peut étre confondue a une loi de Gauss (Saporta, 2006), avec
l'avantage d'étre caracterisee par un seul parametre. L'estimation d'un seul parametre (esperance = variance)
plutot que 2 presente un gain important en qualite dans l'apprentissage a condition que la loi de Poisson soit
justiﬁée.

Le probleme est donc de savoir, a partir d'observations experimentales, quand representer les frequences
d'occurrence par une loi de Poisson, c'est a dire, pour les grandes valeurs de frequence quand representer par
une gaussienne a un seul parametre ou par une gaussienne a 2 parametres. Nous nous appuierons sur le
theoreme suivant (Saporta, 2006) pour affecter experimentalement les mots observes dans l'une de ces 2
categories.

Théoréme :
Si Xi, est une suite de variables binomiales B(n,p) telles que quand n—><>o et p—>0, np tend vers une limite
finie 7». Alors Xi, converge en loi vers une variable de Poisson P0»)

3.2 Méthode expérimentale

Une loi empirique comme celle de Zipf permet d'estimer si le contenu d'un texte est porteur de sens ou s'il
releve d'une ecriture aleatoire. Par contre, cette loi n'est pas adaptee a une analyse plus fine, car
approximative et non discriminante pour les faibles valeurs de frequence de mot (i.e classe en rang eleve
dans la loi de Zipf). Nous proposons donc une nouvelle methode d'analyse dynamique de corpus pour
caracteriser les probabilites d'occurrence des mots, et simultanement pour classer ces derniers en mots

grammaticaux (mots " vides "), mots specialises de domaine ou mots courants 2

0 Le corpus est analyse en ﬂux continu. L'observation est realisee periodiquement c'est—a—dire qu'on
fige le comptage frequentiel de tous les mots tous les k mots observes dans le corpus. Si, apres
avoir parcouru n premiers mots, on a decompte fi occurrences d'un mot mi alors fi ~ npi, ou pi est la
probabilite d'apparition du mot mi. D'apres le precedent theoreme, il suffit d'observer l'evolution de
fi en fonction de n quand n varie de 0 a taille maximale du corpus (que l'on considere comme tres
grand # oo). En fonction de l'allure de la courbe fi(n) observee, on peut ensuite opter pour la loi
decrivant le mieux la frequence d'apparition du mot mi.

0 Si fi(x) tend vers une droite asymptote d'equation y=cIe alors le theoreme precedent s'applique. La
distribution du mot mi peut étre alors modelisee par une loi de Poisson (et donc, pour les grandes
valeurs de n, par une gaussienne a un seul parametre 7»). Dans une etude experimentale, une courbe
faiblement croissante, par exemple en log(x) peut aussi étre acceptee comme une approximation
acceptable de la droite asymptote y=c1e (log-linearite).

Experimentalement, l'analyse de corpus «relativement» homogenes, comme le notre, montre que les
frequences des mots croissent plutot lineairement. Nous retiendrons donc pour les grandes valeurs de n, un
distribution gaussienne a 2 parametres. De plus, l'analyse de la courbe d'evolution de chaque mot permet de
classer ce dernier dans l'une des categories precedemment evoquees. S'agissant d'un choix empirique des
criteres discriminants pour le classement, ce choix est justifie surtout par des observations dont l'exemple
suivant est decrite en illustration.

3.3 Résultat expérimental et calcul de saillance

Les mots de, monsieur, cheval et chien ont ete choisis pour representer des classes de mots grammaticaux,
de mots specialises et de mots d'emploi general. Leur courbe de frequence cumulee est analysee sur notre
corpus d'actualites. L'accroissement en frequence du mot de est logiquement la plus rapide comme l'indique
la figure ci-dessous. Comparativement, les courbes d'evolution des mots chien et monsieur paraissent plates.
Ce n'est pas le cas comme l'indique la figure suivante lorsqu'on change le facteur d'echelle sur l'axe y. On
constate aussi que la courbe de croissance du mot de est plus reguliere autour de la droite qui la sous—tend
tandis que les courbes de croissance des mots monsieur et chien sont plus dispersees.

ACQUISITION AUTOMATIQUE DE TERMINOLOGIE A PARTIR DE CORPUS DE TEXTE

   

fréquence de "de" "m0nsieur“ er "chien“ fréquenre de "m0nsieur‘ et "chien“
120000 . . ‘ . 1000 . . _ . r
"monsleu ‘ +

900 "chien“+J'P;m -
100000 800 _
'.v'00 -

30000
600 —
60000 500 J“ -

+
4-00 + -
+

40000 300 + 7
200 -

20000
19;] M‘)mﬂl  _

0 0 ' I I I
0 2e+06 4e-I-06 6e-I-06 Be+06 1e-I-07‘ 1.284 0 2e-I-06 4e+06 6e+06 Be-I-06 1e-H)? 1.2e+C

On cherche donc a quantiﬁer cette dispersion pour servir de critere de discrimination des mots a des fins de
classement ou d'ordonnancement. La dispersion peut étre traduite par la variance ou mieux, pour disposer
d'une échelle de valeur uniformisee, par la forme normalisee qu'est le coefficient de variation.

Le calcul du coefficient de variation se fait comme suit : si f1, f2, ..., fk designent la suite de fréquences

cumulees suivant le comptage decrit plus haut, et si n1,n2,...,nk designent les nombres cumules de mots
k—1 2

, f . v i _fi
parcourus pour decompter les fi, alors la moyenne H=n—k et la variance 6 telle que a2:(Z H—H)
k i:l i+l_ i

permettent de calculer le coefficient de variation, egal a 9
H

3.4 Utilisation du coefﬁcient de variation

L'utilisation du coefficient de variation sur une échelle de valeur scalaire permet d'ordonner les mots (et les
locutions une fois apprises) suivant un indice de notoriete. Intuitivement, ce ne sont pas les mots les plus
frequents qui presentent un interét mais plutot ceux utilises le plus regulierement dans de nombreux
contextes. En plus, en associant a chaque locution apprise sa catégorie grammaticale, et en se focalisant sur
les categories les plus porteuses d'information comme les groupes nominaux ou les patronymes, on arrive
ainsi a extraire des elements saillants mais evanescents comme nuage de cendres, fuite de pétrole...

3.5 Regroupement en catégories grossiéres

Le coefficient de variation permet d'estimer l'importance de chaque mot pris isolement par rapport au
corpus. Expérimentalement, il permet une separation effective des mots grammaticaux des autres mots.
Mais pour regrouper les mots restants en categories grossieres, on a besoin de plus d'informations, et
notamment de quantiﬁer les interactions entre mots.

Pour pouvoir réutiliser les memes calculs experimentaux que précedemment sur la frequence des mots, et
pour conserver une coherence dans le formalisme de calcul, on remarquera qu'il existe un parallele entre le
coefﬁcient de variation et la notion de tfxidf en recherche d'information (cette derniere correspond dans les
modeles probabilistes a la probabilite d'avoir un document pertinent contenant un terme t). L'extension de
cette mesure locale, liee a un document, vers une mesure globale sur le corpus se fait naturellement par la
notion d'entropie Em Zdzllg _pr1°g(pr) . Plus un terme est uniformement distribué, plus sa valeur
E
d'entropie est élevée. La notion d'entropie sur un terme isole s'étend ensuite a celle sur des couples de termes

. . ,. . _ Pltrtzl
t1 et t2 v1a la notion d1nformat1on mutuelle I(t1_t2)—Z p(t1_t2)log(e

dED Pltrlpltzl
l'information mutuelle comme critere de regroupement des mots en categories, on utilisera la notion de
coefﬁcient de correlation lineaire entre couple de termes t1 et t2, qui est l'extension de la notion de
coefﬁcient de variation 2

) . Plutot que d'utiliser

0-rl,12 \ . . .
pm 011 0'11 12 est la COVEl1'lElI1Ce de I1 et I2, et 0'12 0'12 leur VaI‘1ElIlCe respective
0-11 0-12 )

. r 1 7
Il s'agit a posteriori d'un calcul equivalent pu1sque I ( t1_t2)= — 2—1og ( 1 — 02)

La realisation de cette partie est prévue pour la prochaine version du composant d'acquisition
terminologique.

EDMOND LASSALLE

4 Analyse statistique de la compositionalitéz

La compositionalité des mots est evaluee en linguistique par leur potentiel combinatoire. C'est un comptage
frequentiel, pour un mot donne, de l'appariement d'autres mots dans les constructions observées dans un
corpus. Le potentiel combinatoire sert d'indicateur pour faciliter le travail d'analyse d'un lexicologue. Ce
critere n'est cependant pas adapté a un apprentissage non supervise’, ou l'analyse doit étre réalisée
automatiquement. Une notion plus appropriée conceme l'entropie, ce qui Va étre precise ci-apres.

Supposons que, dans un corpus, nous ayons observe 4 fois, le mot bdton dont deux avec le qualiﬁcatif
rouge, une avec bleu et une avec vert. Si l'on souhaite ne garder qu'un seul indicateur qui resume la

 .,\»11 .., , ,.,A ..
distribution, estimee a K 5.1,; ) , la somme des probabilites presente peu dinteret comme indicateur. Par
contre en etudiant la quantité d'information (Shannon, 1948) que chacun des precedents qualificatifs peut

apporter au mot bdton, soit (‘10g(:—)r10g(%)r10g(i)) la moyenne attendue (espérance) est une bonne

 ..., .iiiiii ,
indication du degre de compositionalite du mot, soit —2—1og(§)—Z1og‘Z)—Z 1og(Z)=1 .5 dans le cas dune

échelle logarithmique en base 2.

Cette valeur d'entropie indique la quantite d'information que peut recevoir en moyenne chaque mot. Si un
mot ml a été observe n fois dans un corpus, son entropie a une valeur entre 0 et log(n). Une valeur nulle
traduit l'existence d'un mot m2 dont la probabilite d'observer en cooccurrence avec ml, vaut 1. Le mot ml est
dans ce cas non compositionnel puisque fortement lie a ml. C'est le cas des mots comme cochere, aujourd ,
lurette ou escampette. A l'opposé, une valeur maximale de l'entropie, log(n), correspond a la distribution
equiprobable c'est-a-dire a un fort degré de compositionalite. Normalement, c'est vers cette valeur maximale
que tendent les mots grammaticaux.

4.1 Champ de compositionalité

Intuitivement, si un mot est employe dans son sens compositionnel, il est fort possible de trouver, dans le
corpus, ce mot associé a d'autres mots a des degré divers. Par exemple bdton peut étre associé a rouge, vert,
jaune... et peut-étre moins a joyeux, espiegle, content. Le champ de compositionalite d'un mot m correspond
a une distribution probabiliste sur l'ensemble des mots ml et traduit la probabilité d'observer ml sachant
qu'on a observe le mot m. Cette distribution peut étre resumée par sa moyenne (entropie), sa variance et sa
loi de distribution. Le champ ainsi defini permet d'introduire la notion d'intervalle de confiance et de
determiner de quelle maniere une construction est jugée compositionnelle. Dans le cas d'une locution
comme retour de bdton, il n'existe pas de forme alterée ou modiﬁée ne comportant qu'une partie de mots de
ce groupe. Si la locution est souvent employee dans le corpus, la fréquence d'association est plus élevée que
le cas des constructions compositionnelles, ce qui doit permettre a une analyse statistique de conclure que
l'un des mots retour ou bdton n'appartient pas au champ compositionnel de l'autre mot.

4.2 Méthode expérimentale

Pour chaque mot mll, l'entropie et la variance sont deduites expéiimentalement a partir d'un comptage
frequentiel de cooccurrences :

0 Pour chaque mot mll, on procede au comptage de cooccurrence flll (resp. flll) des mots ml contigus
au mot ml, a droite (resp. a gauche). Le positionnement gauche/droite reﬂete la nature séquentielle
du corpus de texte.

0 Pour évaluer le degre de compositionalité, le comptage ne devrait porter que sur les mots ml ayant
un sens compositionnel avec le mot ml, et exclure les mots ml lorsque mllml constitue une locution.
Au stade de l'apprentissage, on ne dispose pas d'une telle information. L'hypothese est que les mots
ml constituant une locution sont en plus faible nombre que les mots ml portant un sens
compositionnel. Cela justifie l'approximation dans l'estimation de la moyenne et de la variance.

0 Pour tenir compte de la masse absente (due au manque d'exhaustivité de tout corpus), on procede a
un lissage de Laplace. La valeur de lissage est plus petite que 1, en raison des faibles frequences de
cooccurrence.

Les notions de compositionalité, de champ de compositionalité... sont revues ici dans une logique calculatoire

ACQUISITION AUTOMATIQUE DE TERMINOLOGIE A PARTIR DE CORPUS DE TEXTE

_ f0,i

0 La robabilite i,i d'obserVer le mot mi est estimee ar P07 A . L'entro ie 0 et la Variance (Si,
P P , P f 0,] P H
i

sontestimés par “o:"ZPo.j1°gPo,j et ‘7§:ZP0.jl“0‘1°gP0,j)2.
J J

4.3 Modélisation de la loi de compositionalité

Il reste a determiner la loi de distribution des log piii pour pouvoir ﬁxer l'interValle de conﬁance a l'interieur
duquel une association de mots est consideree comme compositionnelle.

0 Pour chaque mot mo, la cooccurrence d'un mot mi peut étre consideree comme une épreuve de
Bernoulli et la frequence de cooccurrence comme une V.a.r Xi de loi binomiale de parametre pi.

0 Pour les grandes Valeurs de frequence, la loi de Xi peut étre approximee par une loi gaussienne.

Nous nous interessons pour la suite a la V.a.r n—’ ou ni, est le nombre total de cooccurrences
0

X - . . . .
observees pour le mot mi,. n—' su1t egalement une lo1 gaussienne que nous designerons par X.
0

X. ,X. .
Pour la suite, pi, peut étre considerée comme un resultat d'obserVation d'une V.a.r Y =2 n—'10g(n—') . On est
i 0 0

donc amene a etudier en premier la loi de X log(X) connaissant la loi de X.

4.4 Approximation de Y par une gaussienne.

X etant une distribution connue, on cherche, pour ce faire, a determiner la fonction de distribution g de la
V.a.r Y=X log(X) a partir de la fonction de distribution f de X.

La demarche classique consiste a evaluer a partir de F, fonction de repartition de X, la fonction de

repartition G de Y alors: Gl«V):PlY<«V:‘PlX)) avec ‘PlXl:‘X1°8lX).

La fonction (p n'est pas bijective. Elle est définie, s'agissant de Valeurs de probabilite, sur l'interValle [Q1] .
Elle est croissante sur [0’1_] et decroissante sur [1,1] .
9 9

La fonction inverse (pl est determinee graphiquement a partir de (p par la symetrie axiale par rapport a la

droite d'équation y=x. (pl est bivaluee et elle est composée d'une branche strictement croissante
— 1 1 . , . — 1 1
<Pi15l0,e-J'*l0,;J et d'une branche strictement decroissante @011 l0,e-lale-,1] .
Plus precisement, si Wi, et W_i sont les branches definies sur P111] de la fonction W de Lambert3, partie
9

reelle,alors *1 = ‘Y et *1 = *5’ .Par suite:
([70 (y) WOW wi (y) Wlm

_ ‘ _ . ,, . i _ i —y _ I -y ,. ,

P(Y<y)=P(X<(pi1(x))+14P(X><p01(x)) Ce qu1peutsecr1re : G(,V)~F(W—l(y) )+1 FIWOW) ) .La der1Vee
, ,1 Wtx) ,. , -y -y . -1

de W etant Ky) X(1+W(X)) , les der1Vees de Wiliy) et de WOW) sont respect1Vement1+Wil(y) et

-1 _ . ... —f(’yiC)f(7VU). 
W dou la fonction de d1str1but1on . ( —)_ W_1l,V) + WOW) ou g et f sont les denvees

'1+W-1ny) 1+Wo<y)
respectives de G et F.

La fonction de Lambert est difficile a mettre en oeuvre dans un calcul numerique du fait des phenomenes

d'oscillation lorsqu'on doit utiliser son developpement en série limitee. Nous nous contenterons donc de
rechercher l'allure generale de la courbe g(y) afin de l'approximer par une fonction plus simple.

3 la fonction de Lambert peut étre Visualisée ici 2 http://math.asu.edu/~kawski/MAPLE/274/images/Lambert8.gif

EDMOND LASSALLE

Domaine de variation de g

_ V _y n V _y .
La fonction L l ﬂwillyl) f (L4/Olly) ) est definie sur [0,1/e] et de domaine de Variation [0,1].

~"“”’=%H+wl,Lyl

—1 —1
T. —>0 Me
1+W—1ly) 1+W0ly)

*1 —1

1
P y—>—, as —>oo et , 5 —>oc
°‘“ e 1+W—1(y) 1W0ny)

Si maintenant f est une partie gaussienne definie sur [0,1], f est associee a W,l(y) sur [0,1/e] et a Wll(y) sur
[1/e,l], 3 cas de ﬁgures se presentent suivant que l'esperance LL et la variance 6 de la fonction f conduisent a
un recouvrement important de la Valeur critique 1/e par la gaussienne definie par f.

->1

Pour Y *0,

f L ) . .
0 Pour LL << 1/e, c'est la composante W_1l.V) dans g qui est predominante. Par suite g peut
1 + W4 (Y)
étre approximee par une gaussienne avec une asymetrie (skew negatit) d'autant moins marquee que

LL est proche de 0.

L -y
0 De maniere similaire pour LL proche de 1, c'est la composantedans g HWOLV) ) qui est
1+Woly)
predominante. Et par suite g peut étre approximée par une gaussienne avec une asymetrie (skew
positit) d'autant moins marquee que LL est proche de 1.

0 Dans le cas d'un recouvrement consequent de la Valeur critique 1/e par la gaussienne, l'allure de la
distribution g nécessite une analyse approfondie, autour de 1/e, du comportement joint de

L -y , V L -y
flwrlﬁly) ) module par l+W,l(y) dune part, et de flwow)

cas ne sera pas traite ici.

lmodule par 1+Wll(y) d'autre part. Ce

En pratique, nous ne nous interesserons qu'au premier cas, ou LL << 1/e. En effet, la taille d'un Vocabulaire
type est de 50000 a 300000 mots (sans distinction des categories grammaticales). Ce qui fait, dans nos
estimations de LL a partir d'un comptage frequentiel, et en effectuant un lissage de Laplace pour prendre en
compte la masse absente, que la Valeur de LL est tres eloignée de 1/e et plutot proche de 0. La representation
de la distribution g par une gaussienne est dans ce cas justifiee.

Si, maintenant, Xl et X2 sont 2 V.a.r de loi fl et fl, alors la loi de Xl+X2 est le produit de convolution fl*f2. Et
dans le cas ou Xl et X2 sont des gaussiennes, Xl+X2 est aussi une gaussienne. En fonction des calculs
estimatifs precedents et dans les conditions de nos experimentations, nous admettrons que

X . ,X. .
Y =2 If 10g( '14) peut étre approximee par une loi gaussienne.
i 0 0

4.5 Mise en oeuvre de l'identification de non-compositionalité

Le comptage frequentiel decrit dans §4.2 permet d'associer a chaque mot, pris individuellement, des
caracteristiques de compositionalite a droite (resp. a gauche) Via la moyenne et la Variance. L'hypothese
d'une distribution gaussienne permet ensuite de definir un intervalle de conﬁance fixe experimentalement a
95% (ce qui correspond a une Valeur de 1.96 d'ecart pour une gaussienne).

Pour tout mot ml de moyenne «a droite» LLlll et de Variance «a droite>> olll, si ml est suivi de m2, de moyenne
«a gauche» LL“ et de Variance «a gauche» 6&2 , mlmz est non compositionnel:

° si —log(plll3,ll2) < LLd,l - 1.96x (Slll ou pdgqz est la probabilite d'aVoir le mot H12 qui suit le mot ml

° ou si —log(pga,21) < LLll,2 - l.96x(5g,2 ou pgd,21 est la probabilite d'aVoir le mot H11 qui precede le mot m2

ACQUISITION AUTOMATIQUE DE TERMINOLOGIE A PARTIR DE CORPUS DE TEXTE

5 Couplage du modéle linguistique

Le composant linguistique dispose au depart 2

0 d'un lexique du francais comportant 300 000 formes ﬂechies, décrites par la partie du discours et
des traits d'accord

0 de regles de grammaires de chunking (Abney, 1994) de type hors contexte, decrites sous forme
normale de Chomsky et regroupées par paquets homogenes

0 de meta—regles régissant les paquets de regles aﬁn de rendre, autant que possible, l'analyse
déterministe.

De plus, la profondeur d'analyse est limitee pour couvrir des syntagmes de moins de 6 mots, ce qui est
suffisant dans nos applications. Cette hypothese permet de traduire les regles initiales en regles de
grammaires regulieres au sein de chaque paquet de regles.

Une premiere analyse lexicale du corpus permet de recenser le Vocabulaire utilise et de completer le
réferentiel lexical initial par les nouveaux mots simples inconnus. L'ajout de ces mots inconnus dans le
referentiel lexical est realise seulement apres seuillage suivant leur fréquence d'occurrence et leur coefficient
de Variation.

5.1 Analyse lexicale et syntaxique du corpus

S'agissant de grammaires de chunking, l'absence du non-terminal initial S impose une analyse <<bottom—up».
Il s'agit donc d'une analyse LR classique (Aho et al., 1977) avec une utilisation particuliere du chart parsing.

En effet, plutot que de creer un espace de chart pour l'analyse de chaque phrase du corpus, on construit
successivement des niveaux de chart couvrant tout le corpus et de la maniere suivante 2

0 on dispose d'un réferentiel lexical de mots simples et de locutions, et d'un referentiel des syntagmes
en cours de construction

0 le réferentiel des syntagmes est Vide au depart (eventuellement celui des locutions aussi)
0 le referentiel lexical et le referentiel des syntagmes sont utilises pour indexer tout le corpus
0 le résultat de chaque indexation correspond alors a un niveau du chart

0 une analyse du coefficient de Variation des syntagmes du réferentiel permet d'éliminer les elements
les moins pertinents

0 une analyse de la compositionalité des syntagmes ﬁgurant dans le réferentiel des syntagmes permet
d'identiﬁer les locutions et de les reverser dans le réferentiel des locutions

0 on applique ensuite un nouveau paquet de regles de grammaires pour identifier de nouveaux
syntagmes et pour les reverser dans le referentiel des syntagmes

Le processus se termine apres epuisement des paquets de regles.

5.2 Mise en oeuvre du systéme

Les resultats qui suivent sont issus du corpus d'actualités décrit precédemment dans §2. Les donnees, en
constante evolution, correspondent aux actualites de janvier 2011. Les listes ci—apres correspondent a des
extraits de patronymes et de groupes nominaux classes par ordre de pertinence decroissante. Un réferentiel
terminologique unique est dans un premier temps appris sur le corpus global d'actualités puis «projeté» sur
des plus petits corpus thematises, par analyse du coefficient de Variation intra corpus.

Patronymes GN Patronymes GN sport Patronymes GN international
culturel culturel sport international

-nicolas sarkozy -golden globes -andy murray -autres sports -ben ali -premier ministre
-johnny hallyday -homicide -Caroline wozniacki -quarts de finale -laurent gbagbo -affaires étrangeres
-frédéric mitterrand involontaire -paris sg -championnats -nicolas sarkozy -service frangais
-Conrad murray -premier mjnistre -jean pierre dick étrangers -sidi bouzid -ancien president
-dany boon -discours d un roi -claude onesta -téte de série -zine ben -depart du president
-ben ali -grand palais -kim clijsters -finale de la coupe -saad hariri -president déchu
-john barry -los angeles -justine henin -championnat du -Vincent delory -president tunisien
-brice taton -premiere fois -wilfried tsonga monde -jean claude -conference de
-robert de niro -poivre d arvor -cyril despres -fin de la sajson duvalier presse

- luc chatel -sol majeur -Stanislas wawrinka -milieu de terrain -Silvio berlusconi -forces de l ordre

EDMOND LASSALLE

-Xavier beauvois
-beverly hills
-marc olivier fogiel
-sofia coppola
-justin bieber
-quentin tarantino
-laurent gerra
-caroline lachowsky
-claude monet
-frangois fillon
-ernest hemingway
-alexandre jardin
-jean dutourd

-biographie d
hemingway
-priorite sante
-haute couture
-bande dessinee
-mise en scene
-accuse de plagiat
-téte de bois
-meilleur film
-jeu video
-nouvelles
technologies

-tele realite
-pluies diluviennes
-premier album
-bande dessinee d
angouleme

-jose mourinho
-michel desjoyeaux
-stephane sessegnon
-paris fc

-frangois gabart
-jean tigana

-saint etienne
-loick peyron
-carlos sainz
-dimitri payet
-brian joubert
-tomas berdych
-lionel messi

-ballon d or
-journal du mercato
-finale du tournoi
-champion du
monde

-frangais jean
-coupe de la ligue
-ski alpin

-match en retard
-nuit des frangais
-rumeurs du
mercato

-tenant du titre
-quart de finale
-nuit derniere
-premiere fois
-conference de
presse

-benoit xvi
-frangois fillon
-mohamed elbaradei
-alain juppe
-nelson mandela
-gilles trequesser
-mohamed
ghannouchi
-antoine de leocour
-eric zemmour
-jean stephane
-johan vande
-tarek amara

-henri pierre

-eric faye

-regime du president
-ministre des
affaires

-nouveau
gouvernement
-journaliste de l afp
-droits de l homme
-jeunes frangais
-president americain
-ministere de l
interieur

-demission du
gouvernement
-president sortant
-union europeenne
-ministre de la
defense
-communaute
internationale
-frangais enleves

6 Conclusion

L'approche que nous venons de décrire conﬁrme qu'il est possible de concevoir un systeme d'ac uisition de
terminologie performant en temps d'exécution et aussi de tres bonne qualité. Le taux de precision obtenu est
de 1'0rdre de 90% (Lassalle et a1., 2011). Les principales raisons de ces performances sont liées a 2

0 une architecture de chart parsing couvrant tout le corpus, évitant ainsi des redondances d'analyse
des memes syntagmes

0 1e regroupement des syntagmes analysees dans un meme réferentiel, permettant ainsi un couplage
avec 1'ana1yse statistique tout en minimisant le biais

0 une specialisation des analyses statistiques entre la detection des locutions et le classement de ces
dernieres en fonction du corpus applicatif

7 Annexe :

Extrait de la grammaire de chunking permettant d'identifier1es patronymes 2
#Cat prenoms.prenoms

0 (CatLocl prenoms.prenoms) —>(CatMot prenoms) (CatMot prenoms)

0 (CatLocl prenoms.prenoms) —>(CatMot particulepreﬁxe) (CatMot prenoms)
#Cat PRENOMS.particule

0 (CatLocl PRENOMS.particule) —>(CatMot prenoms) (CatMot particule)
0 (CatLocl PRENOMS.particule) —>(CatMot prenoms) (CatLoc1 particule.particule)
0 (CatLocl PRENOMS.particule) —>(CatLocl prenoms.prenoms) (CatMot particule)

0 (CatLocl PRENOMS.particule) —>(CatLocl prenoms.prenoms) (CatLoc1 particule.particule)
#syntagme PATRO
avec détection de non-compositionalité

(CatLocl PATRO) —>(CatLocl PRENOMS.particule) (CatMot patronyme) + (SeuilleOr $LOCBINl)
(CatLocl PATRO) —>(CatLocl PRENOMS.particule) (CatMot prenoms) + (SeuilleOr $LOCBINl)
(CatLocl PATRO) —>(CatLocl PRENOMS.particule) (CatMot V.stat) + (SeuilleOr $LOCBINl)

o
o

o

0 (CatLocl PATRO) —>(CatLocl prenoms.prenoms) (CatMot patronyme) + (SeuilleOr $LOCBIN1)
Le taux de rappel n'est pas pertinent pour un modele d'apprentissage statistique. En effet, un nombre minimal
d'occurrences (environ 4) d'une méme locution est nécessajre pour que cette derniere puisse étre identiﬁée, Ce qui
exclut des locutions dont la fréquence d'apparition est trop faible. Enfin, l'estimation de ce taux nécessite un
recensement manuel des locutions dans le corpus de test, Ce pour un coﬁt en général prohibitif. Une solution (que
nous n'avons pas mise en oeuvre) consisterait 21 échantillonner le corpus pour estimer le nombre moyen de locutions
observées tous les n mots analysés et de le comparer avec le nombre total des locutions extraites divisé par la taille
(en nombre de mots) du corpus d'apprentissage.

ACQUISITION AUTOMATIQUE DE TERMINOLOGIE A PARTIR DE CORPUS DE TEXTE

0 (CatL0cl PATRO) —>(CatL0cl pren0ms.pren0ms) (CatM0t v.stat) + (SeuilleOr $LOCBINl)

Références

ABNEY S.T.,(1994). PARSING BY CHUNI<s. BELL COMMUNICATION RESEARCH.

AHO A.,SETHI R., ULLMAN J .D.( 1977). Compilers: Principles, Techniques, and Tools. Dragon Book.
BRILL E.,(1992). A Simple Rule Based Part of Speech Tagger. ACL.

CHURCH K., HANI<s P.,(1996). WORD ASSOCIATION NORMS, MUTUAL INFORMATION, AND LEXICOGRAPHY.
COMPUTATIONAL LINGUIsTIcs. 16, 22-29.

CORLESS ET AL.,(l996). On the Lambert W function. Adv. Computational Maths. 5, 329-359.

DAILLE B.,(1996). Study and Implementation of Combined Techniques for Automatic Extraction of
Terminology. MIT Press., 49-66.

DUNNING T.D.,( 1993). Accurate Methods for the Statistics. Computational Linguistics. 19(1), 61-74.

FRANTZI K.T., ANANIADOU S., TsUIII J .,(1998). The C-value/NC—value Method of Automatic Recognition of
Multi-word Terms. E CDL’98, 585-604.

HEINECKE J ., SMITS G., CHARDENON C., GUIMIER DE NEEF E.,MAILLEBUAU E., BOUALEM M., (2008). TILT 2 plate-
forme pour le traitement des langues naturelles. TAL Vol. 49.

KIT C.,(2002). Corpus Tools for Retrieving and Deriving Termhood Evidence. The 5* East Asia Forum of
Terminology, 69-80.

LASSALLE E., CASIMIR P.K., GUIMIER DE NEEF E.,(2011). Evaluation des outils d'extraction terminologique
Quezao et Acabit. EGC 2011, 131, 136.

NAMER F.,(2000). Flemm 2 Un analyseur ﬂexionnel de francais a base de regles. Traitement Automatique des
Langues pour la Recherche d ’Information. Hermes, 523-547.

NAZARENKO A., ZARGAYOUNA H., HAMON 0., VAN PUYMBROUCK J.,(2009). Evaluation des outils
terminologiques 2 enjeux, difficultes et propositions. TA Vol. 50, 257-281.

PAPOULIS A.,(2002). Probability, Random Variables ans Stochastic Processes. Mac Graw Hill.
SAPORTA G., (2006). Probabilité, analyse des donnees et statistique. Ed. Technip.

SHANNON C.E.,( 1948). A Mathematical Theory of Communication. The Bell System Technical Journal. 27,
623-656.

SMADJA F.,(1993). XTRACT 2 An Overview. Computer and the Humanities Kluwer Academic Publishers.
TSURUOKA Y.,(2005). Chunk Parsing Revisited. 9m IWPT.

VU T., Aw A.T., ZHANO M.,(2008). Term Extraction Through Unithood And Termhood Unification. IJNLP.

