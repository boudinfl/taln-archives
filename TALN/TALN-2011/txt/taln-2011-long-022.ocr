TALN 2011, Montpellier, 27 juin -1” jui11et2011

Intégrer des connaissances linguistiques dans un CRF :
application £1 l’apprentissage d’un segmenteur-étiqueteur du francais

Matthieu Constantl Isabelle Tellierz Denys Duchierz

Yoann Dupontz Anthony Sigognel Sylvie Billotz
(1) Universite Paris-Est, LIGM, CNRS, 5 bd Descartes, Champs-sur-Marne 77454
Marne-la-Vallee cedex 2
(2) LIFO, universite d’Orleans, 6 rue Leonard de Vinci
BP 6759, 45067 Orleans cedex 2
mconstan@univ-mlv.fr, isabelle.tellier@univ-orleans.fr,
denys.duchier@univ-orleans.fr, yoann.dupont@etu.univ-orleans.fr,

sigogne@univ-mlv.fr, sylvie.billot@univ-orleans.fr

Résumé. Dans cet article, nous synthetisons les resultats de plusieurs series d’experiences realisees a l’aide
de CRF (Conditional Random Fields ou “champs markoviens conditionnels”) lineaires pour apprendre a annoter
des textes frangais a partir d’exemples, en exploitant diverses ressources linguistiques extemes. Ces experiences
ont porte sur l’etiquetage morphosyntaxique integrant l’identiﬁcation des unites polylexicales. Nous montrons
que le modele des CRF est capable d’integrer des ressources lexicales riches en unites multi—mots de differentes
manieres et permet d’atteindre ainsi le meilleur taux de correction d’etiquetage actuel pour le frangais.

Abstract. In this paper, we synthesize different experiments using a linear CRF (Conditional Random
Fields) to annotate French texts from examples, by exploiting extemal linguistic resources. These experiments
especially dealt with part—of—speech tagging including multiword units identiﬁcation. We show that CRF models
allow to integrate, in different ways, large-coverage lexical resources including multiword units and reach state-
of—the-art tagging results for French.

M0tS-CléS 3 Etiquetage morphosyntaxique, Modele CRF, Ressources lexicales, Segmentation, Unites polylex-
icales.

Keywords: Part-of-speech tagging, CRF model, Lexical resources, Segmentation, Multiword units.

M. CONSTANT, I. TELLIER, D. DUCHIER, Y. DUPONT, A. SIGOGNE, S. B1LLoT

1 Introduction

Dans cet article, nous synthétisons les résultats de plusieurs series d’expériences réalisées a l’aide de CRF (Con-
ditional Random Fields ou “champs markoviens conditionnels” (Lafferty et al., 2001; Tellier & Tommasi, 2011))
linéaires pour apprendre a annoter des textes frangais a partir d’exemples, en exploitant diverses ressources lin-
guistiques extemes. La tache a laquelle nous nous sommes attaches est celle de la segmentation en unites lexicales
des phrases d’un texte, couplée a celle de leur étiquetage en categories morphosyntaxiques (ou “part of speech”
en anglais).

Ces demieres annees, l’etiquetage morphosyntaxique a atteint d’excellents niveaux de performance grace a l’u—
tilisation de modeles probabilistes discriminants comme les modeles de maximum d’entropie [MaxEnt] (Ratna—
parkhi, 1996; Toutanova et al., 2003), les séparateurs a vaste marge [SVM] (Giménez & Marquez., 2004) ou, déj a,
les champs markoviens conditionnels [CRF] (Tsuruoka et al., 2009). Il a par ailleurs été montre que le couplage
de ces modeles avec des lexiques extemes augmente encore la qualité de l’annotation, comme l’illustre (Denis
& Sagot, 2009, 2010) pour MaxEnt. Néanmoins, les evaluations réalisées considerent toujours en entree un texte
avec une segmentation lexicale parfaite, c’est—a—dire que les unites lexicales multi—mots, qui forment par deﬁnition
des unites linguistiques, ont été parfaitement reconnues au préalable. Or cette tache de segmentation est diﬂicile
car elle nécessite des ressources lexicales importantes. On notera que les systemes tels que Macaon (Nasr et al.,
2010) et Unitex (Paumier, 2011) integrent une analyse lexicale avec segmentation multi—mots ambigue avant levee
d’ambiguité par l’utilisation d’un modele de Markov cache [HMM]. Dans cet article, nous proposons d’intégrer
les deux taches de segmentation et d’étiquetage dans un seul modele CRF couple a des ressources lexicales riches.

Le corpus d’apprentissage dont nous sommes partis provient du French Treebank (Abeillé et al., 2003). Les
ressources linguistiques extemes utilisées sont de différentes natures. Nous avons ainsi exploité plusieurs dictio-
nnaires : Leﬂ‘f (Sagot, 2010) mais aussi DELA (Courtois, 2009; Courtois et al., 1997), ainsi que des lexiques
spéciﬁques comme Prolex (Piton et al., 1999) et quelques autres incluant des noms d’organisation et des prénoms
(Martineau et al., 2009). Cet ensemble de dictionnaires est complete par une bibliotheque de grammaires locales
qui reconnaissent différents types d’unités multi—mots (Constant & Watrin, 2008). Nous montrons que le modele
des CRF est capable d’intégrer de telles ressources de différentes manieres et permet d’atteindre ainsi le meilleur
taux actuel de correction pour la segmentation et l’étiquetage du frangais.

Dans la suite de cet article, nous commengons par presenter le modele des CRF et le fonctionnement des bib-
liotheques logicielles que nous avons utilisées pour mener nos experiences. Nous décrivons ensuite le corpus
d’apprentissage ainsi que la tache que nous traitons, en détaillant les diﬂicultés spéciﬁques que posent les unites
multi—mots. Puis nous passons en revue les ressources a notre disposition et menons une réﬂexion méthodologique
sur les différents moyens de les prendre en compte dans une chaine de traitements qui fait appel a un CRF. La
demiere partie est consacrée a la presentation des résultats de nos experiences. Ces travaux ont pennis la Inise au
point de plusieurs segmenteurs—étiqueteurs qui sont librement disponibles.

2 Les CRF

2.1 Le modéle théorique

Les champs markoviens conditionnels ou CRF (Tellier & Tommasi, 2011) sont des modeles probabilistes discrim-
inants introduits par (Lafferty et al., 2001) pour l’annotation séquentielle. Ils ont été utilises dans de nombreuses
taches de Traitement des Langues, ou ils donnent d’excellents résultats (McCallum & Li, 2003; Sha & Pereira,
2003; Tsuruoka et al., 2009; Tellier et al., 2010).

Les CRF permettent d’associer a une observation x une annotation y en se basant sur un ensemble d’exemples
étiquetés, c’est—a—dire un ensemble de couples (x, y). La plupart du temps (et ce sera le cas dans la suite de cet
article), x est une séquence d ’um'te’s (ici, une suite d’unités lexicales) et y la séquence des étiquettes correspondante
(ici, la suite de leurs categories morphosyntaxiques, éventuellement enrichie pour coder la segmentation). Les CRF
sont des modeles discriminants qui appartiennent a la famille des modéles graphiques non dirigés. Ils sont déﬁnis
par X et Y, deux champs aléatoires décrivant respectivement chaque unite de l’observation x et son annotation y,
et par un graphe Q = (V, E) dont V = X U Y est l’ensemble des noeuds (vertices) et E 9 V X V l’ensemble des

INTEGRER DES CONNAISSANCES LINGUISTIQUES DANS UN CRF

arcs (edges). Deux Variables sont reliées dans le graphe si elles dependent l’une de l’autre. Le graphe sur le champ
Y des CRF linéaires, dessiné en Fig 1., traduit le fait que chaque etiquette est supposée dépendre de l’étiquette
précedente et de la suivante et, implicitement, de la donnee x complete. Un dessin complet du graphe devrait ainsi
egalement relier chaque Variable Y,- a chaque variable du champ X, ce qu’on omet sur la ﬁgure pour la lisibilité.

@   @

FIGURE 1 — graphe associé a un CRF linéaire

Dans un CRF, on a la relation suivante (Lafferty et al., 2001) :

p(y|x) = % EEC exp ( Zk: /1kfk(yc, x, c)) avec

— C est l’ensemble des cliques (sous—graphes completement connectes) de Q sur Y : dans le cas du graphe de la
Fig. 1, ces cliques sont constituées soit d’un noeud isole, soit d’un couple de noeuds successifs.

— yc l’ensemble des Valeurs prises par les Variables de Y sur la clique c pour un étiquetage y donne : ici, c’est donc
soit la Valeur d’une etiquette soit celles d’un couple d’étiquettes successives

— Z(x) est un coefﬁcient de normalisation, deﬁni de telle sorte que la somme sur y de toutes les probabilités p(y|x)
pour une donnee x ﬁxée soit egale a 1.

— Les fonctions fk sont appelees fonctions caractéristiques (features) : elles sont deﬁnies a l’interieur de chaque
clique c et sont a Valeurs reelles, mais souvent choisies pour donner un résultat binaire (0 ou 1). Elles doivent
etre founies au systeme par l’utilisateur. Par deﬁnition, la Valeur de ces fonctions peut dépendre des étiquettes
presentes dans une certaine clique c ainsi que de la Valeur de x n ’importe 012 dans la dormée (et pas uniquement
aux indices correspondants a la clique c, ce qui donne beaucoup d’expressiVité aux CRF).

— Les poids /lk, a Valeurs reelles, permettent d’accorder plus ou moins d’importance a chaque fonction fk dont ils
caractérisent le pouvoir discriminant. Ce sont les parametres du modele : l’enjeu de la phase d’apprentissage
est de ﬁxer leur Valeur en cherchant a maxirniser la log—Vraisemblance sur un ensemble d’ exemples deja annotés
(constituant le corpus d’apprentissage).

L’ interet et l’eﬂicacité des CRF proviennent de ce qu’ils prennent en compte des dependances entre etiquettes re-
liees les unes aux autres dans le graphe. En cherchant le meilleur y, c’est—a-dire la meilleure séquence d ’e’tiquettes
associer a une donnee complete x, ils se comportent en general mieux qu’une série de classiﬁcations d’unites
isolées. Mais cette prise en compte a un prix : la phase d’apprentissage d’un CRF peut etre longue. Une fois cette
phase réalisée, annoter une nouvelle sequence x de n mots en entree revient alors a trouver le y qui maximise
p(y|x). L’espace théorique de recherche de ce meilleur étiquetage y est |Y|", ou |Y| est le nombre d’étiquettes dis-
tinctes possibles pour chaque noeud. Mais, grace a des techniques de programmation dynamique, ce calcul peut
etre factorise a l’intérieur des cliques et ramené a K * n * |Y|‘ ou c est la taille de la plus grande clique (c = 2 pour
les CRF lineaires) et K le nombre de fonctions caractéristiques. Une fois appris, l’etiqueteur est donc performant.

2.2 Les bibliothéques CRF++ et Wapiti

Notre objectif étant d’inserer des connaissances linguistiques dans un apprentissage realise it l’aide de CRF
linéaires, il nous semble important de bien comprendre le fonctionnement concret des bibliotheques qui les irn—
plementent. Plusieurs sont disponibles pour mettre en oeuvre les CRF linéaires, notamment crfsourcanetl de
Sarawagi ou Malletz de McCallum. Celles que nous avons utilisées sont CRF++ 3 de Taku Kado et Wapiti4 de
Thomas Lavergne (Lavergne et al., 2010), qui utilisent des moyens similaires pour instancier les fonctions carac—
téristiques qui entrent dans leur deﬁnition.

1. crf.sourceforge.net
2. http://ma11et.cs.umass.edu/

3. http ://crfpp.souIceforge.net/
4. http ://wapiti.limsi.fr

M. CONSTANT, I. TELLIER, D. DUCHIER, Y. DUPONT, A. SIGOGNE, S. B1LLoT

Corpus tabulaires. Les exemples d’apprentissage que requierent ces bibliotheques sont des couples (x, y), ou
x est une sequence d’unités et y la sequence d’étiquettes correspondantes, de memes longueurs. Pour nous, une
unite de x correspond a un “mot”, mais elle peut etre enrichie par d’autres propriétés, representées par p attributs,
du moment que ces demiers sont disponibles ou calculables aussi pour tout nouvel exemple x non etiquete. Les
attributs peuvent etre des booléens (l’unité contient un chiffre, commence par une majuscule, est presente dans un
lexique, etc.), des Valeurs numeriques (nombre de lettres, etc.) ou textuelles (Valeur de l’unité ou de son preﬁxe
ou suﬂixe de telle longueur, etc.). Une donnee étiquetée (x, y) de taille n se presente donc comme un tableau de n
lignes et p + 1 colonnes, ou les p premieres colonnes contiennent toutes les informations disponibles sur la donnee
x et la demiere colonne les étiquettes y :

x% x?  x1; y,
3 6  xii 

Les exemples distincts sont séparés e11tre eux dans un meme ﬁchier par une ligne Vide. Un corpus d’apprentissage
est donc une suite de tels tableaux, tous de largeur p + 1, mais de hauteurs qui peuvent Varier.

Patrons tabulaires. L’ utilisateur des bibliotheques ne déﬁnit pas directement les fonctions caractéristiques du
modele; il doit foumir des patrons. Il existe deux types de patrons correspondant aux deux tailles de clique
possibles : les unigrammes pour les cliques de taille 1, et les bigrammes pour les cliques de taille 2.

Un patron unigramme est une sorte de carte perforee de meme largeur p + 1 que nos tableaux, de hauteur quel-
conque sur les p premieres colonnes mais ne pouvant capturer qu’une seule etiquette sur la colonne p + 1. Chaque
position possible de cette carte sur un exemple déﬁnit une fonction caractéristique : celle qui renvoie la Valeur 1 si
la conﬁguration de Valeurs observée dans les perforations est satisfaite, 0 sinon. Les ronds dans le tableau prece-
dent représentent les Valeurs capturees par une telle carte, positionnée sur la ligne i d’une donnée. Chaque fonction
caractéristique prend donc la forme d’une conjonction de criteres booléens observée au moins une fois parmi les
exemples et un patron en “génere” autant qu’il y a de positions ou il peut s’appliquer dans le ﬁchier d’exemples.
Un patron permet de deﬁnir ainsi succinctement des milliers, Voire des millions de fonctions caracteristiques. Un
patron bigramme est similaire a un patron unigramme, mais on l’applique successivement a une position i, puis a
la position suivante i + 1 et la fonction caracteristique obtenue est la conj onction de tous les criteres rencontrés.

3 Corpus d’apprentissage pour la segmentation et l’étiquetage

3.1 Corpus FTB

Tout systeme d’annotation probabiliste supervise requiert un corpus annoté de reference pour entrainer le modele
et ensuite l’ evaluer. Pour notre tache d’ étiquetage morphosyntaxique intégrant la reconnaissance des unites multi-
mots, il est donc necessaire d’utiliser un corpus annoté en categories grammaticales incluant l’annotation des
unites polylexicales. Le corpus le plus complet en francais est le corpus arboré de Paris 7 (Abeillé et al., 2003),
forme d’articles du journal Le Monde allant de 1989 a 1993. Il decrit la structure syntaxique des différentes
phrases sous la forme d’arbres. Une unite de ce corpus peut etre une ponctuation, un nombre, un mot simple ou
une unite multi—mots. Au niveau morphosyntaxique, il existait initialement un jeu d’étiquettes de 14 categories
principales et de 34 sous—categories. Pour notre tache, nous utilisons un jeu d’étiquettes optimise en 29 categories
pour l’analyse syntaxique (Crabbe & Candito, 2008) et réutilisé come standard dans une experience d’étiquetage
morpho—syntaxique (Denis & Sagot, 2009). Les unites multi—mots codees sont de différents types : mots composes
et entités nommées. Les mots composes comprennent des noms (acquis sociaux), des Verbes (faire face ti), des
adverbes (dans l’imme’diat), des prepositions (en dehors de). Il contient quelques types d’entités nommées : des
noms d’organisation (Société suisse de microélectronique et d ’horlogerie), des noms de famille (Strauss—Kahn),
des noms de lieu (Afrique du Sud, New York).

INTEGRER DES CONNAISSANCES LINGUISTIQUES DANS UN CRF

Dans nos series d’expériences, nous avons utilise deux Versions différentes du corpus : une Version de 569 039
unites (au LIGM), une autre de 350 931 (au LIFO). Dans ces deux Versions, nous n’aVons repris que le niveau des
feuilles, i.e. le niveau lexical. Nous en donnons un extrait ci-dessous :

Quant_a/P la/DET technique/NC ,/PONCT son/DET verdict/NC est/V implacable/ADJ ./PONCT

L’ unite Quant_a est la fusion de deux mots simples (Quant eta), formant la preposition composée quant_a.

3.2 Unités lexicales multi-mots

Expressions multi-mots. Dans le consensus actuel du Traitement Automatique des Langues (TAL), les expres-
sions multi—mots forment des unites linguistiques aux comportements lexicaux, syntaxiques et/ou sémantiques
particuliers. Elles regroupent les expressions ﬁgées et semi—ﬁgées, les collocations, les entités nommees, les Verbes
a particule, les constructions a Verbe support, les termes, etc. (Sag et al., 2002). Leur identiﬁcation est donc cru-
ciale avant toute analyse semantique. Elles apparaissent a différents niveaux de l’analyse linguistique : certaines
forment des unites lexicales contigues a part entiere (ex. cordon bleu, San Francisco, par rapport a), d’autres com-
posent des constituants syntaxiques comme les phrases ﬁgées (N0 prendre le taureau par les comes ; N0 prendre
NI en compte) ou les constructions a Verbe support (N0 donner un avertissement a N1 ; N0faire du bruit).

Phénoménes traités. Dans cet article, nous ne traitons que les expressions multi-mots du niveau lexical, que
nous appellerons dorenavant unites multi-mots ou polylexicales. Elles comportent les mots composes (noms, pre-
positions, adverbes, etc.), les entités nommées, les termes, les collocations nominales. Il existe une grande Varieté
de phenomenes linguistiques rentrant dans cette catégorie et donc de nombreux criteres d’identiﬁcation. Les mots
composes sont des sequences non compositionnelles de mots : ils présentent une opacite semantique totale (cor-
don bleu, tout a fait) ou partielle (vin blanc), des contraintes syntaxiques et lexicales, etc. Il existe un continuum
entre expressions ﬁgees et libres, ce qui rend leur identiﬁcation encore plus diﬂicile. Les collocations sont deﬁnies
a partir de criteres statistiques. Les entités nommées ont souvent une certaine compositionalité semantique mais
ont une syntaxe particuliere : ex. le 5 mars 2010 pour les dates, Jacques Chirac pour les noms de personnes.

Ressources. Les unites polylexicales peuvent etre recensées dans des dictionnaires electroniques ou des gram-
maires locales. Les dictionnaires electroniques sont des listes qui associent des formes lexicales a des informations
linguistiques comme les categories grammaticales ou certains traits sémantiques (ex. humain, concret, etc.). Les
grammaires locales (Gross, 1997; Silberztein, 2000) sont des reseaux recursifs de transitions decrits sous la forme
de graphes d’automates ﬁnis. Chaque transition est etiquetée par un element lexical (ex. mange), un masque lex-
ical correspondant a un ensemble de formes lexicales encodées dans un dictionnaire (ex. <manger> symbolisant
toutes les formes ﬂéchies dont le lemme est manger) ou un element non—terminal reférant a un autre automate.
Elles sont tres utiles pour décrire de maniere compacte des unites multi-mots acceptant des Variations lexicales. Un
systeme de transduction permet d’annoter les expressions décrites, comme la catégorie grammaticale ou l’analyse
des composants intemes pour les entités nommées par exemple (Martineau et al., 2009).

Reconnaissance. La reconnaissance automatique des unites multi-mots est, la plupart du temps, realisée a l’ aide
de ressources lexicales construites manuellement (ex. pour les expressions ﬁgées) ou apprises automatiquement
(ex. collocations nominales). Par ailleurs, une grande partie des entites nommees, du fait de leur syntaxe partic-
uliere sont facilement décrites et reconnues a l’aide de grammaires locales (Friburger & Maurel, 2009; Martineau
et al., 2009), bien qu’il existe d’autres types d’approches telles que les systemes statistiques (McCa]lum & Li,
2003) ou hybrides (Poibeau, 2009). L’identiﬁcation de telles expressions est une tache tres diﬂicile car les unites
non decrites dans les ressources sont diﬂicilement reconnaissables. Elle est d’autant plus diﬂicile qu’elle depend
du contexte d’occurrence. En effet, une expression reconnue est souvent ambigue avec l’analyse en mots simples :
par exemple, il en fait une priorite’ (mots simples) Vs j’ai en fait beaucoup travaille’ (mot compose). On observe
parfois des chevauchements avec d’autres unites polylexicales comme dans la sequence une pomme de terre cuite
ou pomme de terre et terre cuite sont des mots composes. C’est pourquoi les outils existants de segmentation en
unites multi-mots comme dans INTEX (Silberztein, 2000) ou SxPipe (Sagot & Boullier, 2008) produisent une
segmentation ambigue sous la forme d’automates ﬁnis acycliques pour éviter de prendre une decision deﬁnitive

M. CONSTANT, I. TELLIER, D. DUCHIER, Y. DUPONT, A. SIGOGNE, S. B1LLoT

trop hative. Cette analyse ambigue peut alors etre intégrée dans des traitements linguistiques tels que l’étiquetage
morphosyntaxique (Nasr et al., 2010; Paumier, 2011)) ou l’analyse syntaxique superﬁcielle (Blanc et al., 2007;
Nasr et al., 2010) et profonde (Sagot & Boullier, 2006).

3.3 Intégration d’un segmenteur et d’un étiqueteur

L’identiﬁcation des unites multi—mots est similaire a une tache de segmentation comme le chunking ou a la re-
connaissance des entités nommées, qui identiﬁent les limites de segments (chunks ou entités nommees) et les
annotent. En effet, grace a la representation IOB 5 (Ramshaw & Marcus, 1995), segmenter un texte revient a an-
noter ses unites minimales. Pour combiner étiquetage morphosyntaxique et reconnaissance d’unites multi—mots,
il suﬂit de concatener les deux étiquetages en associant a chaque unite minimale une etiquette de la forme X+B
ou X+I, ou X est sa catégorie grammaticale et le suﬂixe indique si elle se trouve au debut d’une unite multi—mots
(B) ou dans une position “inteme” (I). Le suﬂixe 0 est inutile car la ﬁn d’un segment lexical correspond au debut
d’un autre (suﬂixe B) ou a une ﬁn de phrase. Une telle procedure d’annotation determine non seulement les limites
des unites lexicales, mais aussi leur catégorie morphosyntaxique. Pour entrainer nos CRF, nous avons donc trans-
formé le corpus d’apprentissage initial en isolant les unites composant les segments multi—mots et en les étiquetant
conformement a cette nouvelle norme. L’ exemple precedent est alors transformé en :

Quant/P+B a/1>+1 la/DET+B technique/NC+B ,/PONCT+B son/DET+B verdict/NC+B est/V+B
implacable/ADJ+B ./PONCT+B

Le jeu d’etiquettes initial est ainsi double, chaque etiquette se dédoublant en une Variante B et une Variante I.
La reconnaissance des unites polylexicales dependant fortement de la richesse de ressources lexicales utilisees, il
s’agit maintenant de trouver les meilleures facons d’integrer ce type d’informations dans nos CRF.

4 Exploitation d’une ressource externe

Dans cette section, nous commencons par presenter les différentes ressources que nous avons a notre disposition,
et nous cherchons tous les moyens possibles de les prendre en compte dans un apprentissage avec des CRF.

4.1 Ressources

Meme s’il existe de plus en plus d’études sur l’extraction automatique d’unites multi—mots, en particulier les
collocations ou les termes (Daille, 1995; Dias, 2003; Seretan et al., 2003), les ressources les plus riches et les
plus precises ont ete aquises manuellement. Pour notre etude, nous avons compile diverses ressources lexicales
sous la forme de dictionnaires morphosyntaxiques et de grammaires locales fortement lexicalisees. Nous avons
utilise notarmnent deux dictionnaires disponibles de mots simples et composes de la langue genérale : DELA
(Courtois, 2009; Courtois et al., 1997) et Leﬂ‘f (Sagot, 2010). Le DELA a ete construit par une équipe de linguistes.
Le Leﬂ‘f a été automatiquement acquis et manuellement Validé. Il résulte également de la fusion de differentes
sources lexicales. En complement, nous disposons aussi de lexiques spéciﬁques comme Prolex (Piton et al., 1999)
compose de toponymes et d’autres incluant des noms d’organisation et des prenoms (Martineau et al., 2009). Les
nombres d’entrees de ces divers dictionnaires sont donnés dans le tableau 1.

Dictionnaire #mots simples #mots composés

DELA 690,619 272,226
Lefff 553,140 26,311
Prolex 25,190 97,925
Organisations 772 587
Prénoms 22,074 2,220

TABLE 1 — Dictionnaires morphosyntaxiques

5. I : Inside (intérieur du segment); 0 : Outside (hors du segment) ; B : Beginning (début du segment)

INTEGRER DES CONNAISSANCES LINGUISTIQUES DANS UN CRF

Cet ensemble de dictionnaires est complété par une bibliotheque de grammaires locales qui reconnaissent dif-
ferents types d’unités multi—mots comme les entités nommées (dates, noms d’ organisation, de personne et de lieu),
prepositions locatives, determinants numeriques et nominaux. En pratique, nous avons utilise une bibliotheque de
211 automates developpée a partir de la bibliotheque en—ligne GraalWeb (Constant & Watrin, 2008).

4.2 Quelques statistiques préliminaires

Pour les experiences menées avec la Variante du FTB la plus volumineuse, le corpus initial a ete decoupe en trois
parties : 80% pour la phase d’entrainement (TRAIN), 10% pour le developpement (DEV) et 10% pour le test.
Cela nous a permis de faire quelques observations préalables.

Ainsi, dans le corpus FTB-DEV (avec étiquetage initial non transformé), nous avons observe qu’environ 97,4%
des unites lexicalesé sont presentes dans nos ressources lexicales (en particulier, 97% sont presentes dans les
dictionnaires). Alors que 5% des unites sont inconnues (i.e. absentes du corpus d’apprentissage), 1,5% sont a fois
inconnues et absentes des ressources lexicales, ce qui montre que 70% des unites inconnues sont couvertes par nos
ressources. On observe egalement qu’environ 6% des unites sont multi—mots. En décomposant toutes les unites
multi—mots du texte en unites minimales, on s’apercoit qu’a peu pres 15% d’entre elles sont incluses dans une
unite multi—mots. Parmi les unites multi—mots codées dans le corpus FTB—DEV, 75 ,5 % d’entre elles sont presentes
dans nos ressources (87,5% en incluant le lexique du corpus d’entrainement). Ceci montre que l2,5% des unites
multi—mots sont totalement inconnues et, par consequent, seront sans doute tres diﬂicilement reconnaissables.

On observe, par ailleurs, que le corpus FTB ne couvre pas la reconnaissance de toutes les unites multi—mots.
Tout d’abord, certains determinants ou certaines entités nommées ne sont pas identiﬁes comme les determinants
nominaux, les dates, les noms de personne, les adresses postales. Par ailleurs, de nombreux noms composes sont
manquants. Par exemple, apres avoir applique nos ressources lexicales de maniere non contextuelle (en excluant
les grammaires locales reconnaissants des types d’entités nommées ou des determinants nominaux non codes
dans le FTB), nous avons manuellement observe sur le FTB—DEV qu’environ 30% des unites polylexicales de nos
ressources ”adaptées” ne sont pas prises en compte dans le corpus.

4.3 Méthodologie de prise en compte des ressources

Comment prendre en compte une ou plusieurs ressources lors d’une chaine de traitements faisant appel a un
apprentissage realise avec un CRF? Dans le cadre de l’apprentissage de la ressource MEltf, (Denis & Sagot,
2009, 2010), les auteurs ont testé deux approches possibles :

— intégrer les propriétés des mots du lexique dans les fonctions caractéristiques du modele d’apprentissage;

— ﬁltrer les étiquetages incompatibles avec les informations presentes dans la ressource.

Nous avons cherche toutes les facons possibles d’envisager cette integration, ce qui nous a amene :21 en caracteriser
plus ﬁnement le mode opératoire, et :21 en trouver de nouvelles variantes. Nous les présentons ci-dessous, en dis-
cutant leurs interets et leurs limites. Elles peuvent s’organiser en deux familles principales, suivant que la ou les
ressources disponibles sont Inises a contribution comme des ﬁltres avant ou aprés l’appel au CRF ou qu’elles
sont utilisées pendant la phase d ’apprentissage. L’ approche “ﬁltrage” requiert que les etiquettes qui ﬁgurent dans
la ressource soient identiques a celles qui sont la cible de l’apprentissage, alors que ce n’est pas necessairement
le cas pour l’autre approche. Au cas ou les conventions d’étiquetage ne sont pas les memes, une fonction de
correspondance doit etre préalablement appliquée.

Les ressources comme ﬁltrage a priori on a posteriori Les ressources peuvent etre vues comme un moyen de
contraindre, ou encore de ﬁltrer les étiquetages possibles. Concretement, ce ﬁltrage peut opérer avant ou aprés
l’appel au CRF. Le ﬁltrage a priori consiste a deﬁnir l’espace de recherche des etiquetages possibles y d’une nou-
velle chaine x via un pretraitement fondé sur une ressource. Les analyseurs lexicaux actuels auxquels on soumet
une phrase produisent en effet genéralement un dag (graphe orienté acyclique) dont chaque chemin correspond
a une sequence possible d’etiquettes. Les unites multi—mots peuvent etre reconnues lors de cette étape, et ﬁgurer
aussi dans le dag, comme cela a eté évoqué section 3.2. Pour une phrase constituée de n unites minimales, il est
évidemment plus facile et rapide de chercher le y qui maximise p(y|x) (calculé suivant la formule des CRF) parmi

6. Les unités lexicales sont les unités autres que les nombres et les ponctuations.

M. CONSTANT, I. TELLIER, D. DUCHIER, Y. DUPONT, A. SIGOGNE, S. B1LLoT

l’ensemble des etiquetages du dag plutot que sur l’espace de tous les |Y|" étiquetages possibles. Le ﬁltrage est
ainsi a priori mais l’apprentissage du CRF est neanmoins un pre-requis de la chaine de traitements. Le ﬁltrage a
posteriori, lui, cherche non pas le meilleur étiquetage possible y d’une chaine quelconque x mais les m meilleurs
possibles (c’est une option généralement disponibles des bibliotheques CRF) et choisit le premier d’entre eux
compatible avec la ressource. Les deux techniques donnent la meme solution; privilégier l’une ou l’autre depend
de la forme de la ressource. Leur interet est de garantir que dans la solution retenue, chaque mot regoit une eti-
quette compatible avec ce que decrivent la ou les ressources consultees. Un ﬁltrage peut d’ailleurs tres bien se
combiner avec une approche prenant en compte les ressources pendant la phase d’apprentissage.

Les ressources comme aide aT1l’apprentissage. D’apres la section 2.2, quand nous faisons appel a une biblio-
theque qui implemente les CRF linéaires, nous avons a notre disposition trois “leviers” d’action possibles :

— le choix des etiquettes et des propriétés des unites (les colonnes des données tabulaires)

— le choix des exemples (les lignes)

— le choix des fonctions caracteristiques (Via les patrons), choix qui depend fortement des precedents

Nous avons déja Vu qu’un choix pertinent d’etiquettes permettait de “coder” en quelque sorte les deux problemes
de la segmentation et de l’étiquetage simultanément. D’autres expériences ont montré l’intéret de décomposer le
jeu d’étiquettes en sous—étiquettes, notarmnent quand celles—ci sont trop nombreuses (Tellier et al., 2010). Mais le
probleme auquel nous nous confrontons ici ne requiert pas un tel traitement, nous ne l’aVons pas mis en oeuvre.

11 est en revanche “nature ” d’insérer les informations des ressources en tant que propriétés des unites d’un exemple
x, donc en jouant sur les colonnes xf, ..., . Plusieurs choix sont encore possibles pour cela, suivant qu’on se
contente de concaténer les différentes etiquettes possibles d’une meme unite pour en faire une seule colonne de
nature textuelle, ou bien qu’on déﬁnisse autant de colonnes a Valeur booléenne que d’étiquettes possibles dans
l’ensemble de la ressource. Cela aura bien sur des consequences sur la deﬁnition des patrons qui generent les
fonctions caracteristiques. Dans le cas des colonnes booléennes, la combinatoire des conjonctions possibles de
plusieurs criteres est explosive. Dans les deux cas, on peut soit garder les etiquettes des ressources telles quelles,
soit les transformer pour qu’elles s’identiﬁent a celles Visées.

Enﬁn, il est aussi possible de considérer que chaque instance de couple (unite lexicale, etiquette) present dans

la ressource constitue a elle toute seule une “phrase” qu’on insere parmi les exemples étiquetés, en ajoutant de

nouvelles lignes isolées dans le corpus d’apprentissage. Cela suppose bien sur que les étiquettes qui ﬁgurent dans

la ressource sont identiques a celles de l’étiquetage cible. L’idée sous-jacente de cette technique, tres simple a

appliquer, est que la presence dans une ressource equivaut a une occurrence attestée dans la langue, que 1’ on simule

en l’insérant artiﬁciellement dans le corpus d’apprentissage. Elle presente aussi toutefois quelques inconvénients :

— on introduit ainsi un biais sur les comptes d’occurrences puisque les différentes étiquettes possibles d’une unite
donnent chacune lieu a un exemple, comme si elles étaient equiprobables. Il faut donc esperer que le reste de
l’ensemble d’apprentissage soit suﬂisant pour compenser cette distorsion possible.

— en introduisant des “phrases” reduites a un mot, on Va rendre inoperantes sur ces “phrases” particulieres toutes
les fonctions caracteristiques qui testent la Valeur des unites ou des etiquettes Voisins (et donc en particulier tous
les bigrammes). Le poids de ces fonctions ne pourra etre calcule que sur le reste des exemples.

5 Résultats des expériences

Les résultats presentés ici sont issus d’expériences menées en parallele au LIFO (Orleans) et au LIGM (Paris—Est
Mame—la—Vallee). Notons au préalable que les expériences ont eté realisees dans des environnements differents,
sans coordination a priori, ce qui explique la diﬂiculte a comparer précisément les résultats. Les experiences du
LIFO ont eté menées avec Wapiti7 et évaluées par Validation croisée en 10 parties : 9/10 pour l’apprentissage,
1/10 pour le test. Celles du LIGM ont utilisé CRF++ 8 et porté sur une Variante du FTB plus Volumineuse rendant
plus coﬁteuse, mais aussi moins indispensable, une Validation croisée : le corpus initial a alors eté decoupé en trois
parties : 80% pour la phase d’entrainement (TRAIN), 10% pour le developpement (DEV) et 10% pour le test.

Pour l’éValuation globale, nous avons precision = rappel = f-mesure. En effet, tous les mots ayant une unique eti-
quette, une erreur de precision sur une classe C1 correspond a une erreur de rappel sur une classe C2 et Vice Versa.

7. Ce programme a l’avantage d’opérer une sélection des fonctions caractéristiques en cours d ’apprentissage grace 5 une pénalisation L1.
8. L’algorithme de regularisation utilisé est L2 et le seuil de fréquence des traits a été ﬁxé a 2.

INTEGRER DES CONNAISSANCES LINGUISTIQUES DANS UN CRF

Pour l’etiquetage avec segmentation, nous avons deux types d’eValuation : la f—mesure sur les unites minimales
(LIFO) et sur les segments lexicaux (LIGM). Ceci explique les scores plus élevés pour LIFO sur cette tache.

5.1 Evaluation de l’étiquetage avec segmentation parfaite

LIGM. Nous avons tout d’abord evalué l’étiquetage morphosyntaxique sur une segmentation multi—mots par-
faite, au moyen d’un modele CRF appris en utilisant des propriétés classiques des unites (forme lexicale, preﬁxes,
suﬂixes, commence par une majuscule, etc.). Les experiences du LIGM ont porte sur deux methodes d’integra—
tion de la ressource lexicale exteme décrite dans la section 4.1. La premiere methode consiste a introduire, dans
le ﬁchier d’entrainement, une colonne supplementaire (AC) représentant la concatenation des étiquettes trouvées
dans la ressource pour l’unité courante. Nous obtenons alors un modele LEX en utilisant tous les traits décrits dans
la table 1(a). Nous notons STD le modele incorporant les memes traits a l’exception de ceux issus de la ressource.
La deuxieme méthode consiste a proceder a un ﬁltrage a priori de toutes les etiquettes absentes de la ressource
pour chaque unite. Si l’unité est absente, toutes les étiquettes sont gardées. Les etiquettes des ressources ont eté
ajustées a celles du corpus pour le ﬁltrage. Nous avons compare les resultats avec d’autres outils d’étiquetage que
nous avons tous entraines sur le corpus FTB—TRAIN. Nous avons evalué TreeTagger (Sch1nid, 1994) base sur
des arbres de decision probabilistiques, SVMTool (Gimenez & Marquez., 2004) base sur les Separateurs a Vastes
Marges utilisant des traits indépendants de la langue, MElt (Denis & Sagot, 2009) base sur un modele MaxEnt in-
corporant en plus des traits dependants de la langue issus de lexiques extemes. Les lexiques utilises pour entrainer
et tester MElt integrent toutes les ressources de la section 4.1 9. Les précisions obtenues sur le corpus FTB-TEST
pour les différents systemes sont données en pourcentage dans la table 1(b) avec un intervalle de conﬁance a 95%
de +/-0,1.

(b) Comparaison de systemes d’étiquetage pour le

(a) Types de traits francais

Traits intemes lmigrammes sans ﬁltrage avec ﬁltrage
we = X &te = T T T 96 4 _
forme en minuscule de we = L &te = T fee agger '

Préﬁxe de we = P avec |P| < 5 me = T SVl\/I'Tool 97.2 —
Sufﬁxe dewe = S avec |S| < 5 &te = T MElt 97.6 _
we conﬁent un tiret &te =

we conﬁent un chiffre &te = CRF'STD 97'4" 97'6
we commence par une majuscule &te = T CRF-LEX 97.7 97.7
we est tout en majuscule &te = T

we commence par une majuscule et est en début de phrase &te = T

classe d‘a.mbiguité de we, ACe = A &te = T

Traits contextuels unigrammes

w,-=X,ie{—2,—l,l,2] &te=T

Wiwj = XY, (j,k) e {(—1,o), (0, 1), (-1, 1)} &re = T

AC,- =A, i e {—2,—1, 1,2} we = T

Traits bigrammes

L1 = T’ &to = T

TABLE 2 — Résultats du LIGM avec segmentation parfaite

LIFO. Les experiences du LIFO ont porté sur une Version du FI‘B moins Volumineuse, en utilisant les traits un-
igrammes decrits dans la Table 3(a) sur une fenetre [—2..2] et les simples Valeurs d’etiquettes pour les bigrammes.
Les patrons bigrammes produisent en effet un tres grand nombre de fonctions caracteristiques : cette restriction
est destinée a limiter les calculs. La seule ressource a notre disposition était le Leﬂ‘f. La premiere methode utilisee
pour le prendre en compte en cours d’apprentissage est de l’intégrer en tant que pourvoyeur de nouveaux exem-
ples dans chaque ﬁchier d’entrainement. Cette methode augmente le temps d’apprentissage du simple au double
Voire triple selon les parties. La seconde methode consiste a introduire des booleens en tant qu’attributs dans les
colonnes des ﬁchiers d’entrainement, chaque colonne représentant une etiquette possible dans le Leﬂ‘f. Il a fallu
alors generer par programme tous les patrons possibles qui combinent certains attributs entre eux. Cette methode
produit un grand nombre de fonctions caracteristiques mais Wapiti est capable de les gérer puisqu’il opere une
selection des fonctions caractéristiques les plus discriminantes en cours d ’apprentissage (Lavergne et al., 2010).

9. Nous avons regroupé ensemble tous les dictionnaires, ainsi que les unités reconnues lors de 1’application des grammaires locales sur le
corpus.

M. CONSTANT, I. TELLIER, D. DUCHIER, Y. DUPONT, A. SIGOGNE, S. BILLOT

(a) Types de traits

unigrammes (b) Résultats

V“1°“‘ d°1'“"i“5 Sans leﬂ‘f 96.5
Commence par une majuscule

Est uniquement en majuscules Avec lefff (GXGIDPICS) 96-6
ES‘ “*1 chm“ _ Avec leﬂ‘f (attributs booleens) 97 .3
Est une ponctuatron

3 demieres lettres

TABLE 3 — Resultats du LIFO avec segmentation parfaite

5.2 Evaluation de l’étiquetage avec identiﬁcation des unités multi-mots

LIGM. Pour evaluer la tache d’etiquetage integrant la reconnaissance des unites multi-mots, nous avons en-
traine trois modeles CRF sur le corpus FTB-TRAIN apres avoir decompose les unites multi-mots en sequences
d’unites minimales etiquetees dans la representation de type IOB (cf. section 3.3) : STD, LEX et MWE. Les deux
premiers ont les memes types de traits que dans l’experience precedente. Le modele MWE est complete de traits
issus de l’application non-contextuelle de nos ressources multi-mots sur le texte : une unite est associee a la cate-
gorie grammaticale, la structure inteme ou/et le trait semantique de l’unite polylexicale reconnue a laquelle elle
appartient, ainsi qu’a sa position relative dans l’unite (I, O ou B). Par exemple, le mot de dans le contexte du mot
compose eau de vie present dans nos ressources, sera associe a la categorie grammaticale NC (nom), a la structure
inteme NPN (nom+preposition+nom) et a la position relative I (car il est en 2eme position). Ces trois systemes
ont ete compares avec SVMTool, entraine sur le meme corpus. Pour chaque segmenteur-etiqueteur applique sur
le corpus TEST decompose en unites minimales, nous avons calcule la f-mesure 10. La precision et le rappel sont
calcules par rapport aux segments lexicaux trouves et non aux unites minimales simples. Les resultats sont syn-
thetises dans le tableau 3(a). La colonne SEG indique la f-mesure de la segmentation qui ne prend en compte que
les limites des segments. La colonne TAG prend aussi en compte la categorie grammaticale.

LIFO. Pour cette tache, nous comparons les resultats obtenus (1) sans le Leﬂ‘f, (2) avec le Leﬂ‘f comme source
d’exemples, (3) avec le Leﬂ‘f comme source d’attributs booleens. Nos resultats evaluent la qualite de l’etiquetage
des unites minimales avec les categories integrant B et 1, et non celle de l’identiﬁcation des unites multimots.

(a) LIGM (f-mesure sur les (b) LIFO : méthodes d’intégration
segments lexicaux) (f-mesure sur les unités minimales)
TAG SEG Sans leﬂ‘f 94.5%
SVMT001 92-1 94-7 Avec leﬂ‘f (exemples) 94.7%
CRF'STD 93:7 95-8 Avec leﬂ‘f (attributs) 95.2%

CRF-LEX 93.9 95.9
CRF-MWE 94.4 96.4

TABLE 4 — Apprentissage simultane etiquetage/ segmentation

5.3 Description des segmenteurs—etiqueteurs proposes

Les diverses experiences decrites ci—dessus ont mene a la mise au point de segmenteurs—etiqueteurs qui sont li-
brement disponibles. La chaine de traitements de SEM 11 produite au LIFO a ete ecrite en Python. Le programme
offre la possibilite d’exploiter ou non Leﬂ‘f (sous forme d’attributs uniquement), en utilisant soit une segmenta-
tion rudimentaire ecrite a la main (sans prise en compte de ressources extemes), soit la segmentation acquise par
le CRF. Le segmenteur-etiqueteur LGTagger12 produit au LIGM est implante en Java et comprend deux phases
distinctes : (1) une analyse lexicale basee sur des ressources lexicales extemes qui sert a ﬁltrer les analyses (si1n—
ples ou multi-mots) non decrites dans les ressources et qui produit un dag 13 ; (2) un decodeur qui determine le

10. La formule de la f -mesure est la suivante : f = %, 011 p est la precision et r le rappel.

11. http ://www.univ-orleans.fr/lifo/Members/Isabelle.Tel1ier/SEM.htrn1

12. http ://igm.univ-m1v.fr/‘ mconstan/research/software

13. Pour les mots simples inconnus de nos ressources, toutes les étiquettes possibles sont gardées comme candidates. Si l’analyseur n’a
aucune ressource lexicale en entree, il produit un dag représentant toutes les analyses possibles dans le jeu d’étiquettes.

INTEGRER DES CONNAISSANCES LINGUISTIQUES DANS UN CRF

chemin du dag le plus probable en fonction du modele CRF appris. Il peut etre execute avec ou sans segmentation
multi—mots. Les ressources lexicales (pour le calcul des propriétés des fonctions caracteristiques et pour l’analyse
lexicale) lui sont passées en parametres. Les programmes d’Unitex (Paumier, 2011) sont utilises pour l’application
des ressources : consultation des dictionnaires et application des grammaires locales.

6 Conclusion

Dans cet article, nous avons montre que les taches de segmentation et d’étiquetage sont intimement liees et qu’il
est naturel de les traiter simultanément. L’ écart entre la performance de l’étiquetage avec ou sans segmentation
est de 2 a 4 points suivant la mesure utilisee : cela mesure le “coﬁt” d’une bonne segmentation. Par ailleurs, nous
avons montre l’interet certain d’intégrer des ressources lexicales dans un CRF, en particulier les ressources d’unités
polylexicales utiles pour la segmentation. Nous Voyons aussi qu’a ce niveau de performance, il est extremement
diﬂicile de gagner quelques djxiemes de points, meme en mettant en jeu des ressources riches et Variées.

Cet article a aussi eté l’occasion d’une réﬂexion méthodologique poussée sur les differents moyens d’integrer une
ressource linguistique exteme dans une chaine de traitements faisant appel a un CRF. Une bonne partie de cette
réﬂexion est d’ailleurs transposable a l’utilisation d’autres techniques d’apprentissage automatique. Les CRF, en
integrant fonctions caracteristiques locales et combinaison statistique globale, apparaissent comme un modele
particulierement bien adapté a l’hybridation entre ressources symboliques et modeles statistiques. Grace a cette
integration, il a eté possible de produire en peu de temps des segmenteurs-étiqueteurs tres performants.

Références

ABEILLE A., CLEMENT L. & TOUSSENEL F. (2003). Building a treebank for french. In A. ABEILLE, Ed., Treebanks.
Dordrecht : Kluwer.

BLANC 0., CONSTANT M. & WAIRIN P. (2007). Segmentation in super-chunks with a ﬁnite—state approach. In
Proceedings of the 6th Workshop on Finite—State Methods and Natural language Processing (FSMNLP’07), p.
62 - 73.

CONSTANT M. & WAIRIN P. (2008). Networking multiword units. In Proceedings of the 6th International Confer-
ence on Natural language Processing (GoTAL’08), number 5221 in Lecture Notes in Artiﬁcial Intelligence, p.
120 — 125 : Springer—Verlag.
CoURTo1s B. (2009). Un systeme de dictionnaires électroniques pour les mots simples du francais. langue
Francaise, 87, 1941 - 1947.

CoURTo1s B., GARRIGUES M., GROSS G., GROSS M., JUNG R., IVIATHIEU-COLAS M., MONCEAUX A., PoNcET—MoNTANGE
A., SILBERZTEIN M. & Vrvﬁs R. (1997). Dictionnaire e’lectronique DEIAC : les mots compose’s binaires. Rapport
inteme 56, University Paris 7, LADL.

CRABBE B. & CANDITO M. H. (2008). Experiences d’analyse syntaxique statistique du francais. In Actes de TALN
2008 (Traitement automatique des langues naturelles), Avignon.

DAILLE B. (1995). Reperage et extraction de terminologie par une approche mixte statistique et linguistique.
traitement Automatique des langues (TAL), 36(1—2), 101-118.

DENIS P. & SAGor B. (2009). Coupling an annotated corpus and a morphosyntactic lexicon for state—of—the—art pos
tagging with less human effort. In Proceedings of the 23rd Pacific Asia Conference on Language, Information
and Computation (PACLIC 2009 ).

DENIS P & SAGor B. (2010). Exploitation d’une ressource lexicale pour la construction d’un étiqueteur mor-
phosyntaxique etat—de—l’art du francais. In actes de TALN 2010.

DIAS G. (2003). Multiword unit hybrid extraction. In Proceedings of the Workshop on Multiword Expressions of
the 41 st Annual Meeting of the Association of Computational Linguistics (ACL 2003 ), p. 41-49.

FRIBURGER N. & MAUREL D. (2009). Finite—state transducer cascade to extract named entities in texts. Theoretical
Computer Science, 313, 94-104.

GIMENEZ J. & MARQUEZ. L. (2004). Svmtool : A general pos tagger generator based on support Vector machines.
In Proceedings of the 4th International Conference on language Resources and Evaluation (LREC’04).

M. CoNsTANT, I. TELLIER, D. DUCHIER, Y. DUPONT, A. S1GoGNE, S. BILLoT

GRoss M. (1997). The construction of local grammars. In D. J. LIPCOLL, D. H. LAWRIE & A. H. SAMEH, Eds.,
Finite—State Language Processing, p. 329-352. Cambridge, Mass. : The MIT Press.

LAFFERTY J ., MCCALLUM A. & PEREIRA F. (2001). Conditional random ﬁelds : Probabilistic models for segmenting
and labeling sequence data. In Proceedings of the Eighteenth International Conference on Machine Learning
(ICML 2001 ), p. 282-289.

LAVERGNE T., CAPPE O. & YVoN F. (2010). Practical Very large scale CRFs. In Proceedings the 48th Annual
Meeting of the Association for Computational Linguistics (ACL), p. 504-513 : Association for Computational
Linguistics.

MARTINEAU C., NAKAMURA T., VARGA L. & VoYATz1 S. (2009). Annotation et normalisation des entites nommées.
Arena Romanistica, 4, 234-243.

MCCALLUM A. & L1 W. (2003). Early results for named entity recognition with conditional random ﬁelds, feature
induction and web-enhanced lexicons. In Proceedings of CoNLL.

NASR A., BECHET F. & REY J. F. (2010). Macaon : Une chaine linguistique pour le traitement de graphes de mots.
In Traitement Automatique des Langues Naturelles — session de de’monstrations, Montreal.

PAUMIER S. (2011). Unitex 2.1 — user manual. http ://igm.uniV—mlV.fr/"unitex.

PrToN 0., MAUREL D. & BELLEIL C. (1999). The prolex data base : Toponyms and gentiles for nlp. In Proceedings
of the Third International Workshop on Applications of Natural Language to Data Bases (NLDB ’99), p. 233-237.

POIBEAU T. (2009). Boosting Robustness of a Named Entity Recognizer. International Journal of Semantic
Computing, 3(1), 1-14.

RAMsHAw L. A. & MARCUS M. P. (1995). Text chunking using transformation-based learning. In Proceedings of
the 3rd Workshop on Very Large Corpora, p. 88 - 94.

RATNAPARKHI A. (1996). A maximum entropy model for part—of—speech tagging. In Proceedings of the Conference
on Empirical Methods in Natural Language Processing (EMNLP I 996 ), p. 133 - 142.

SAG I. A., BALDWIN T., BoND F., COPESTAKE A. A. & FLICKINGER D. (2002). Multiword expressions : A pain in the
neck for nlp. In Proceedings of the Third International Conference on Computational Linguistics and Intelligent
Text Processing (CICLing ’02), p. 1-15, London, UK : Springer—Verlag.

SAGoT B. (2010). The leﬂ‘f, a freely available, accurate and large-coverage lexicon for french. In Proceedings of
the 7th International Conference on Language Resources and Evaluation (LREC’I 0).

SAGoT B. & BOULLIER P. (2006). Deep non—probabilistic parsing of large corpora. In Proceedings of the 5th
International Conference on Language Resources and Evaluation (LREC’06).

SAGoT B. & BOULLIER P. (2008). Sxpipe 2 : architecture pour le traitement pré—syntaxique de corpus bruts.
Traitement Automatique des Langues, 49(2), 155-188.

SCHMID H. (1994). Probabilistic part—of—speech tagging using decision trees. In Proceedings of International
Conference on New Methods in Language Processing, p. 252 - 259.

SERETAN V., NERIMA L. & WEHRII E. (2003). Extraction of multi—word collocations using syntactic bigram com-
position. In Proceedings of the Fourth International Conference on Recent Advances in NLP (RANLP—2003), p.
424-431, Borovets, Bulgaria.

SHA F. & PEREIRA F. (2003). Shallow parsing with conditional random ﬁelds. In Proceedings of HLT—NAACL, p.
213 - 220.

SILBERZTEIN M. (2000). Intex : an fst toolbox. Theoretical Computer Science, 231(1), 33-46.

TELLIER I., ESHKOL I., TAALAB S. & PRosT J. P. (2010). Pos-tagging for oral texts with crf and category de-
composition. Research in Computing Science, 46, 79-90. Special issue "Natural Language Processing and its
Applications".

TELLIER I. & ToMMAsr M. (2011). Champs Markoviens Conditionnels pour l’extraction d’information. In ERIC
GAUSSIER & FRANgoIs YVoN, Eds., Modeles probabilistes pour l’acces a l’information textuelle. Hermes.
ToUTANoVA K., KLEIN D., MANNING C. D. & SINGER Y. Y. (2003). Feature—rich part—of—speech tagging with a cyclic
dependency network. In Proceedings of HLT—NAACL 2003, p. 252 — 259.

TSURUOKA Y., TsUJI1 J. & ANANIADoU S. (2009). Fast full parsing by linear-chain conditional random ﬁelds. In

Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics
(EACL 2009), p. 790-798.

