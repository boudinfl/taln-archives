<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Abeillé</author>
<author>L Clément</author>
<author>F Toussenel</author>
</authors>
<title>Building a treebank for french.</title>
<date>2003</date>
<booktitle>In A. Abeillé, Ed.,</booktitle>
<publisher>Kluwer.</publisher>
<location>Treebanks. Dordrecht :</location>
<contexts>
<context position="4079" citStr="Abeillé et al., 2003" startWordPosition="542" endWordPosition="545">nt reconnues au préalable. Or cette tâche de segmentation est difficile car elle nécessite des ressources lexicales importantes. On notera que les systèmes tels que Macaon (Nasr et al., 2010) et Unitex (Paumier, 2011) intègrent une analyse lexicale avec segmentation multi-mots ambiguë avant levée d’ambiguité par l’utilisation d’un modèle de Markov caché [HMM]. Dans cet article, nous proposons d’intégrer les deux tâches de segmentation et d’étiquetage dans un seul modèle CRF couplé à des ressources lexicales riches. Le corpus d’apprentissage dont nous sommes partis provient du French Treebank (Abeillé et al., 2003). Les ressources linguistiques externes utilisées sont de différentes natures. Nous avons ainsi exploité plusieurs dictionnaires : Lefff (Sagot, 2010) mais aussi DELA (Courtois, 2009; Courtois et al., 1997), ainsi que des lexiques spécifiques comme Prolex (Piton et al., 1999) et quelques autres incluant des noms d’organisation et des prénoms (Martineau et al., 2009). Cet ensemble de dictionnaires est complété par une bibliothèque de grammaires locales qui reconnaissent différents types d’unités multi-mots (Constant &amp; Watrin, 2008). Nous montrons que le modèle des CRF est capable d’intégrer de </context>
<context position="14088" citStr="Abeillé et al., 2003" startWordPosition="2171" endWordPosition="2174">1 et la fonction caractéristique obtenue est la conjonction de tous les critères rencontrés. 3 Corpus d’apprentissage pour la segmentation et l’étiquetage 3.1 Corpus FTB Tout système d’annotation probabiliste supervisé requiert un corpus annoté de référence pour entraîner le modèle et ensuite l’évaluer. Pour notre tâche d’étiquetage morphosyntaxique intégrant la reconnaissance des unités multimots, il est donc nécessaire d’utiliser un corpus annoté en catégories grammaticales incluant l’annotation des unités polylexicales. Le corpus le plus complet en français est le corpus arboré de Paris 7 (Abeillé et al., 2003), formé d’articles du journal Le Monde allant de 1989 à 1993. Il décrit la structure syntaxique des différentes phrases sous la forme d’arbres. Une unité de ce corpus peut être une ponctuation, un nombre, un mot simple ou une unité multi-mots. Au niveau morphosyntaxique, il existait initialement un jeu d’étiquettes de 14 catégories principales et de 34 sous-catégories. Pour notre tâche, nous utilisons un jeu d’étiquettes optimisé en 29 catégories pour l’analyse syntaxique (Crabbé &amp; Candito, 2008) et réutilisé comme standard dans une expérience d’étiquetage morpho-syntaxique (Denis &amp; Sagot, 200</context>
</contexts>
<marker>Abeillé, Clément, Toussenel, 2003</marker>
<rawString>Abeillé A., Clément L. &amp; Toussenel F. (2003). Building a treebank for french. In A. Abeillé, Ed., Treebanks. Dordrecht : Kluwer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Blanc</author>
<author>M Constant</author>
<author>P Watrin</author>
</authors>
<title>Segmentation in super-chunks with a finite-state approach.</title>
<date>2007</date>
<booktitle>In Proceedings of the 6th Workshop on Finite-State Methods and Natural Language Processing (FSMNLP’07),</booktitle>
<pages>62--73</pages>
<contexts>
<context position="20493" citStr="Blanc et al., 2007" startWordPosition="3102" endWordPosition="3105">e de terre et terre cuite sont des mots composés. C’est pourquoi les outils existants de segmentation en unités multi-mots comme dans INTEX (Silberztein, 2000) ou SxPipe (Sagot &amp; Boullier, 2008) produisent une segmentation ambiguë sous la forme d’automates finis acycliques pour éviter de prendre une décision définitive M. Constant, I. Tellier, D. Duchier, Y. Dupont, A. Sigogne, S. Billot trop hâtive. Cette analyse ambiguë peut alors être intégrée dans des traitements linguistiques tels que l’étiquetage morphosyntaxique (Nasr et al., 2010; Paumier, 2011)) ou l’analyse syntaxique superficielle (Blanc et al., 2007; Nasr et al., 2010) et profonde (Sagot &amp; Boullier, 2006). 3.3 Intégration d’un segmenteur et d’un étiqueteur L’identification des unités multi-mots est similaire à une tâche de segmentation comme le chunking ou à la reconnaissance des entités nommées, qui identifient les limites de segments (chunks ou entités nommées) et les annotent. En effet, grâce à la représentation IOB 5 (Ramshaw &amp; Marcus, 1995), segmenter un texte revient à annoter ses unités minimales. Pour combiner étiquetage morphosyntaxique et reconnaissance d’unités multi-mots, il suffit de concaténer les deux étiquetages en associ</context>
</contexts>
<marker>Blanc, Constant, Watrin, 2007</marker>
<rawString>Blanc O., Constant M. &amp; Watrin P. (2007). Segmentation in super-chunks with a finite-state approach. In Proceedings of the 6th Workshop on Finite-State Methods and Natural Language Processing (FSMNLP’07), p. 62 – 73.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Watrin</author>
</authors>
<title>Networking multiword units.</title>
<date>2008</date>
<booktitle>In Proceedings of the 6th International Conference on Natural Language Processing (GoTAL’08), number 5221 in Lecture Notes in Artificial Intelligence, p. 120 – 125 :</booktitle>
<publisher>Springer-Verlag.</publisher>
<contexts>
<context position="4615" citStr="Watrin, 2008" startWordPosition="620" endWordPosition="621">ge dont nous sommes partis provient du French Treebank (Abeillé et al., 2003). Les ressources linguistiques externes utilisées sont de différentes natures. Nous avons ainsi exploité plusieurs dictionnaires : Lefff (Sagot, 2010) mais aussi DELA (Courtois, 2009; Courtois et al., 1997), ainsi que des lexiques spécifiques comme Prolex (Piton et al., 1999) et quelques autres incluant des noms d’organisation et des prénoms (Martineau et al., 2009). Cet ensemble de dictionnaires est complété par une bibliothèque de grammaires locales qui reconnaissent différents types d’unités multi-mots (Constant &amp; Watrin, 2008). Nous montrons que le modèle des CRF est capable d’intégrer de telles ressources de différentes manières et permet d’atteindre ainsi le meilleur taux actuel de correction pour la segmentation et l’étiquetage du français. Dans la suite de cet article, nous commençons par présenter le modèle des CRF et le fonctionnement des bibliothèques logicielles que nous avons utilisées pour mener nos expériences. Nous décrivons ensuite le corpus d’apprentissage ainsi que la tâche que nous traitons, en détaillant les difficultés spécifiques que posent les unités multi-mots. Puis nous passons en revue les re</context>
<context position="24378" citStr="Watrin, 2008" startWordPosition="3684" endWordPosition="3685">nnaires morphosyntaxiques 5. I : Inside (intérieur du segment) ; O : Outside (hors du segment) ; B : Beginning (début du segment) Intégrer des connaissances linguistiques dans un CRF Cet ensemble de dictionnaires est complété par une bibliothèque de grammaires locales qui reconnaissent différents types d’unités multi-mots comme les entités nommées (dates, noms d’organisation, de personne et de lieu), prépositions locatives, déterminants numériques et nominaux. En pratique, nous avons utilisé une bibliothèque de 211 automates développée à partir de la bibliothèque en-ligne GraalWeb (Constant &amp; Watrin, 2008). 4.2 Quelques statistiques préliminaires Pour les expériences menées avec la variante du FTB la plus volumineuse, le corpus initial a été découpé en trois parties : 80% pour la phase d’entraînement (TRAIN), 10% pour le développement (DEV) et 10% pour le test. Cela nous a permis de faire quelques observations préalables. Ainsi, dans le corpus FTB-DEV (avec étiquetage initial non transformé), nous avons observé qu’environ 97,4% des unités lexicales 6 sont présentes dans nos ressources lexicales (en particulier, 97% sont présentes dans les dictionnaires). Alors que 5% des unités sont inconnues (</context>
</contexts>
<marker>Watrin, 2008</marker>
<rawString>ConstantM. &amp; Watrin P. (2008). Networking multiword units. In Proceedings of the 6th International Conference on Natural Language Processing (GoTAL’08), number 5221 in Lecture Notes in Artificial Intelligence, p. 120 – 125 : Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Courtois</author>
</authors>
<title>Un système de dictionnaires électroniques pour les mots simples du français.</title>
<date>2009</date>
<journal>Langue Française,</journal>
<volume>87</volume>
<contexts>
<context position="4261" citStr="Courtois, 2009" startWordPosition="569" endWordPosition="570">0) et Unitex (Paumier, 2011) intègrent une analyse lexicale avec segmentation multi-mots ambiguë avant levée d’ambiguité par l’utilisation d’un modèle de Markov caché [HMM]. Dans cet article, nous proposons d’intégrer les deux tâches de segmentation et d’étiquetage dans un seul modèle CRF couplé à des ressources lexicales riches. Le corpus d’apprentissage dont nous sommes partis provient du French Treebank (Abeillé et al., 2003). Les ressources linguistiques externes utilisées sont de différentes natures. Nous avons ainsi exploité plusieurs dictionnaires : Lefff (Sagot, 2010) mais aussi DELA (Courtois, 2009; Courtois et al., 1997), ainsi que des lexiques spécifiques comme Prolex (Piton et al., 1999) et quelques autres incluant des noms d’organisation et des prénoms (Martineau et al., 2009). Cet ensemble de dictionnaires est complété par une bibliothèque de grammaires locales qui reconnaissent différents types d’unités multi-mots (Constant &amp; Watrin, 2008). Nous montrons que le modèle des CRF est capable d’intégrer de telles ressources de différentes manières et permet d’atteindre ainsi le meilleur taux actuel de correction pour la segmentation et l’étiquetage du français. Dans la suite de cet art</context>
<context position="23086" citStr="Courtois, 2009" startWordPosition="3492" endWordPosition="3493"> un apprentissage avec des CRF. 4.1 Ressources Même s’il existe de plus en plus d’études sur l’extraction automatique d’unités multi-mots, en particulier les collocations ou les termes (Daille, 1995; Dias, 2003; Seretan et al., 2003), les ressources les plus riches et les plus précises ont été aquises manuellement. Pour notre étude, nous avons compilé diverses ressources lexicales sous la forme de dictionnaires morphosyntaxiques et de grammaires locales fortement lexicalisées. Nous avons utilisé notamment deux dictionnaires disponibles de mots simples et composés de la langue générale : DELA (Courtois, 2009; Courtois et al., 1997) et Lefff (Sagot, 2010). Le DELA a été construit par une équipe de linguistes. Le Lefff a été automatiquement acquis et manuellement validé. Il résulte également de la fusion de différentes sources lexicales. En complément, nous disposons aussi de lexiques spécifiques comme Prolex (Piton et al., 1999) composé de toponymes et d’autres incluant des noms d’organisation et des prénoms (Martineau et al., 2009). Les nombres d’entrées de ces divers dictionnaires sont donnés dans le tableau 1. Dictionnaire #mots simples #mots composés DELA 690,619 272,226 Lefff 553,140 26,311 P</context>
</contexts>
<marker>Courtois, 2009</marker>
<rawString>Courtois B. (2009). Un système de dictionnaires électroniques pour les mots simples du français. Langue Française, 87, 1941 – 1947.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Vivés</author>
</authors>
<title>Dictionnaire électronique DELAC : les mots composés binaires. Rapport interne 56,</title>
<date>1997</date>
<institution>University Paris 7, LADL.</institution>
<marker>Vivés, 1997</marker>
<rawString>Courtois B., GarriguesM., Gross G., GrossM., Jung R., Mathieu-ColasM., Monceaux A., Poncet-Montange A., SilberzteinM. &amp; Vivés R. (1997). Dictionnaire électronique DELAC : les mots composés binaires. Rapport interne 56, University Paris 7, LADL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H</author>
</authors>
<title>Expériences d’analyse syntaxique statistique du français.</title>
<date>2008</date>
<booktitle>In Actes de TALN</booktitle>
<location>Avignon.</location>
<marker>H, 2008</marker>
<rawString>Crabbé B. &amp; CanditoM. H. (2008). Expériences d’analyse syntaxique statistique du français. In Actes de TALN 2008 (Traitement automatique des langues naturelles), Avignon.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Daille</author>
</authors>
<title>Repérage et extraction de terminologie par une approche mixte statistique et linguistique. traitement</title>
<date>1995</date>
<booktitle>Automatique des Langues (TAL),</booktitle>
<pages>36--1</pages>
<contexts>
<context position="22670" citStr="Daille, 1995" startWordPosition="3432" endWordPosition="3433">és polylexicales dépendant fortement de la richesse de ressources lexicales utilisées, il s’agit maintenant de trouver les meilleures façons d’intégrer ce type d’informations dans nos CRF. 4 Exploitation d’une ressource externe Dans cette section, nous commençons par présenter les différentes ressources que nous avons à notre disposition, et nous cherchons tous les moyens possibles de les prendre en compte dans un apprentissage avec des CRF. 4.1 Ressources Même s’il existe de plus en plus d’études sur l’extraction automatique d’unités multi-mots, en particulier les collocations ou les termes (Daille, 1995; Dias, 2003; Seretan et al., 2003), les ressources les plus riches et les plus précises ont été aquises manuellement. Pour notre étude, nous avons compilé diverses ressources lexicales sous la forme de dictionnaires morphosyntaxiques et de grammaires locales fortement lexicalisées. Nous avons utilisé notamment deux dictionnaires disponibles de mots simples et composés de la langue générale : DELA (Courtois, 2009; Courtois et al., 1997) et Lefff (Sagot, 2010). Le DELA a été construit par une équipe de linguistes. Le Lefff a été automatiquement acquis et manuellement validé. Il résulte égalemen</context>
</contexts>
<marker>Daille, 1995</marker>
<rawString>Daille B. (1995). Repérage et extraction de terminologie par une approche mixte statistique et linguistique. traitement Automatique des Langues (TAL), 36(1-2), 101–118.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Denis</author>
<author>B Sagot</author>
</authors>
<title>Coupling an annotated corpus and a morphosyntactic lexicon for state-of-the-art pos tagging with less human effort.</title>
<date>2009</date>
<booktitle>In Proceedings of the 23rd Pacific Asia Conference on Language, Information and Computation (PACLIC</booktitle>
<contexts>
<context position="3198" citStr="Denis &amp; Sagot, 2009" startWordPosition="417" endWordPosition="420">ories morphosyntaxiques (ou “part of speech” en anglais). Ces dernières années, l’étiquetage morphosyntaxique a atteint d’excellents niveaux de performance grâce à l’utilisation de modèles probabilistes discriminants comme les modèles de maximum d’entropie [MaxEnt] (Ratnaparkhi, 1996; Toutanova et al., 2003), les séparateurs à vaste marge [SVM] (Giménez &amp; Márquez., 2004) ou, déjà, les champs markoviens conditionnels [CRF] (Tsuruoka et al., 2009). Il a par ailleurs été montré que le couplage de ces modèles avec des lexiques externes augmente encore la qualité de l’annotation, comme l’illustre (Denis &amp; Sagot, 2009, 2010) pour MaxEnt. Néanmoins, les évaluations réalisées considèrent toujours en entrée un texte avec une segmentation lexicale parfaite, c’est-à-dire que les unités lexicales multi-mots, qui forment par définition des unités linguistiques, ont été parfaitement reconnues au préalable. Or cette tâche de segmentation est difficile car elle nécessite des ressources lexicales importantes. On notera que les systèmes tels que Macaon (Nasr et al., 2010) et Unitex (Paumier, 2011) intègrent une analyse lexicale avec segmentation multi-mots ambiguë avant levée d’ambiguité par l’utilisation d’un modèle </context>
<context position="14690" citStr="Denis &amp; Sagot, 2009" startWordPosition="2261" endWordPosition="2264">illé et al., 2003), formé d’articles du journal Le Monde allant de 1989 à 1993. Il décrit la structure syntaxique des différentes phrases sous la forme d’arbres. Une unité de ce corpus peut être une ponctuation, un nombre, un mot simple ou une unité multi-mots. Au niveau morphosyntaxique, il existait initialement un jeu d’étiquettes de 14 catégories principales et de 34 sous-catégories. Pour notre tâche, nous utilisons un jeu d’étiquettes optimisé en 29 catégories pour l’analyse syntaxique (Crabbé &amp; Candito, 2008) et réutilisé comme standard dans une expérience d’étiquetage morpho-syntaxique (Denis &amp; Sagot, 2009). Les unités multi-mots codées sont de différents types : mots composés et entités nommées. Les mots composés comprennent des noms (acquis sociaux), des verbes (faire face à), des adverbes (dans l’immédiat), des prépositions (en dehors de). Il contient quelques types d’entités nommées : des noms d’organisation (Société suisse de microélectronique et d’horlogerie), des noms de famille (Strauss-Kahn), des noms de lieu (Afrique du Sud, New York). Intégrer des connaissances linguistiques dans un CRF Dans nos séries d’expériences, nous avons utilisé deux versions différentes du corpus : une version</context>
<context position="26695" citStr="Denis &amp; Sagot, 2009" startWordPosition="4032" endWordPosition="4035">é nos ressources lexicales de manière non contextuelle (en excluant les grammaires locales reconnaissants des types d’entités nommées ou des déterminants nominaux non codés dans le FTB), nous avons manuellement observé sur le FTB-DEV qu’environ 30% des unités polylexicales de nos ressources ”adaptées” ne sont pas prises en compte dans le corpus. 4.3 Méthodologie de prise en compte des ressources Comment prendre en compte une ou plusieurs ressources lors d’une chaîne de traitements faisant appel à un apprentissage réalisé avec un CRF ? Dans le cadre de l’apprentissage de la ressource MElt f r (Denis &amp; Sagot, 2009, 2010), les auteurs ont testé deux approches possibles : – intégrer les propriétés des mots du lexique dans les fonctions caractéristiques du modèle d’apprentissage ; – filtrer les étiquetages incompatibles avec les informations présentes dans la ressource. Nous avons cherché toutes les façons possibles d’envisager cette intégration, ce qui nous a amené à en caractériser plus finement le mode opératoire, et à en trouver de nouvelles variantes. Nous les présentons ci-dessous, en discutant leurs intérêts et leurs limites. Elles peuvent s’organiser en deux familles principales, suivant que la ou</context>
<context position="35878" citStr="Denis &amp; Sagot, 2009" startWordPosition="5447" endWordPosition="5450">nsiste à procéder à un filtrage a priori de toutes les étiquettes absentes de la ressource pour chaque unité. Si l’unité est absente, toutes les étiquettes sont gardées. Les étiquettes des ressources ont été ajustées à celles du corpus pour le filtrage. Nous avons comparé les résultats avec d’autres outils d’étiquetage que nous avons tous entrainés sur le corpus FTB-TRAIN. Nous avons évalué TreeTagger (Schmid, 1994) basé sur des arbres de décision probabilistiques, SVMTool (Giménez &amp; Márquez., 2004) basé sur les Séparateurs à Vastes Marges utilisant des traits indépendants de la langue, MElt (Denis &amp; Sagot, 2009) basé sur un modèle MaxEnt incorporant en plus des traits dépendants de la langue issus de lexiques externes. Les lexiques utilisés pour entraîner et tester MElt intègrent toutes les ressources de la section 4.1 9. Les précisions obtenues sur le corpus FTB-TEST pour les différents systèmes sont données en pourcentage dans la table 1(b) avec un intervalle de confiance à 95% de +/-0,1. (b) Comparaison de systèmes d’étiquetage pour le (a) Types de traits français Traits internes unigrammes sans filtrage avec filtrage w0 = X &amp;t0 = T fo TreeTagger 96.4 -rme en minuscule de w0 = L &amp;t0 = T Préfixe de</context>
</contexts>
<marker>Denis, Sagot, 2009</marker>
<rawString>Denis P. &amp; Sagot B. (2009). Coupling an annotated corpus and a morphosyntactic lexicon for state-of-the-art pos tagging with less human effort. In Proceedings of the 23rd Pacific Asia Conference on Language, Information and Computation (PACLIC 2009).</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Denis</author>
<author>B Sagot</author>
</authors>
<title>Exploitation d’une ressource lexicale pour la construction d’un étiqueteur morphosyntaxique état-de-l’art du francais.</title>
<date>2010</date>
<booktitle>In actes de TALN</booktitle>
<marker>Denis, Sagot, 2010</marker>
<rawString>Denis P. &amp; Sagot B. (2010). Exploitation d’une ressource lexicale pour la construction d’un étiqueteur morphosyntaxique état-de-l’art du francais. In actes de TALN 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Dias</author>
</authors>
<title>Multiword unit hybrid extraction.</title>
<date>2003</date>
<booktitle>In Proceedings of the Workshop on Multiword Expressions of the 41st Annual Meeting of the Association of Computational Linguistics (ACL</booktitle>
<pages>41--49</pages>
<contexts>
<context position="22682" citStr="Dias, 2003" startWordPosition="3434" endWordPosition="3435">es dépendant fortement de la richesse de ressources lexicales utilisées, il s’agit maintenant de trouver les meilleures façons d’intégrer ce type d’informations dans nos CRF. 4 Exploitation d’une ressource externe Dans cette section, nous commençons par présenter les différentes ressources que nous avons à notre disposition, et nous cherchons tous les moyens possibles de les prendre en compte dans un apprentissage avec des CRF. 4.1 Ressources Même s’il existe de plus en plus d’études sur l’extraction automatique d’unités multi-mots, en particulier les collocations ou les termes (Daille, 1995; Dias, 2003; Seretan et al., 2003), les ressources les plus riches et les plus précises ont été aquises manuellement. Pour notre étude, nous avons compilé diverses ressources lexicales sous la forme de dictionnaires morphosyntaxiques et de grammaires locales fortement lexicalisées. Nous avons utilisé notamment deux dictionnaires disponibles de mots simples et composés de la langue générale : DELA (Courtois, 2009; Courtois et al., 1997) et Lefff (Sagot, 2010). Le DELA a été construit par une équipe de linguistes. Le Lefff a été automatiquement acquis et manuellement validé. Il résulte également de la fusi</context>
</contexts>
<marker>Dias, 2003</marker>
<rawString>Dias G. (2003). Multiword unit hybrid extraction. In Proceedings of the Workshop on Multiword Expressions of the 41st Annual Meeting of the Association of Computational Linguistics (ACL 2003), p. 41–49.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Friburger</author>
<author>D Maurel</author>
</authors>
<title>Finite-state transducer cascade to extract named entities in texts.</title>
<date>2009</date>
<journal>Theoretical Computer Science,</journal>
<volume>313</volume>
<pages>94--104</pages>
<contexts>
<context position="19172" citStr="Friburger &amp; Maurel, 2009" startWordPosition="2908" endWordPosition="2911">ransduction permet d’annoter les expressions décrites, comme la catégorie grammaticale ou l’analyse des composants internes pour les entités nommées par exemple (Martineau et al., 2009). Reconnaissance. La reconnaissance automatique des unités multi-mots est, la plupart du temps, réalisée à l’aide de ressources lexicales construites manuellement (ex. pour les expressions figées) ou apprises automatiquement (ex. collocations nominales). Par ailleurs, une grande partie des entités nommées, du fait de leur syntaxe particulière sont facilement décrites et reconnues à l’aide de grammaires locales (Friburger &amp; Maurel, 2009; Martineau et al., 2009), bien qu’il existe d’autres types d’approches telles que les systèmes statistiques (McCallum &amp; Li, 2003) ou hybrides (Poibeau, 2009). L’identification de telles expressions est une tâche très difficile car les unités non décrites dans les ressources sont difficilement reconnaissables. Elle est d’autant plus difficile qu’elle dépend du contexte d’occurrence. En effet, une expression reconnue est souvent ambigue avec l’analyse en mots simples : par exemple, il en fait une priorité (mots simples) vs j’ai en fait beaucoup travaillé (mot composé). On observe parfois des ch</context>
</contexts>
<marker>Friburger, Maurel, 2009</marker>
<rawString>Friburger N. &amp; Maurel D. (2009). Finite-state transducer cascade to extract named entities in texts. Theoretical Computer Science, 313, 94–104.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L</author>
</authors>
<title>Svmtool : A general pos tagger generator based on support vector machines.</title>
<date>2004</date>
<booktitle>In Proceedings of the 4th International Conference on Language Resources and Evaluation (LREC’04).</booktitle>
<marker>L, 2004</marker>
<rawString>Giménez J. &amp; Márquez. L. (2004). Svmtool : A general pos tagger generator based on support vector machines. In Proceedings of the 4th International Conference on Language Resources and Evaluation (LREC’04).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Constant</author>
<author>I Tellier</author>
<author>D Duchier</author>
<author>Y Dupont</author>
<author>A Sigogne</author>
<author>S Billot Gross M</author>
</authors>
<title>The construction of local grammars. In</title>
<date>1997</date>
<booktitle>Eds., Finite-State Language Processing,</booktitle>
<pages>329--352</pages>
<publisher>The MIT Press.</publisher>
<location>Cambridge, Mass. :</location>
<marker>Constant, Tellier, Duchier, Dupont, Sigogne, M, 1997</marker>
<rawString>M. Constant, I. Tellier, D. Duchier, Y. Dupont, A. Sigogne, S. Billot Gross M. (1997). The construction of local grammars. In D. J. Lipcoll, D. H. Lawrie &amp; A. H. Sameh, Eds., Finite-State Language Processing, p. 329–352. Cambridge, Mass. : The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lafferty</author>
<author>A McCallum</author>
<author>F Pereira</author>
</authors>
<title>Conditional random fields : Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proceedings of the Eighteenth International Conference on Machine Learning (ICML</booktitle>
<pages>282--289</pages>
<contexts>
<context position="2255" citStr="Lafferty et al., 2001" startWordPosition="278" endWordPosition="281">, in different ways, large-coverage lexical resources including multiword units and reach stateof-the-art tagging results for French. Mots-clés : Etiquetage morphosyntaxique, Modèle CRF, Ressources lexicales, Segmentation, Unités polylexicales. Keywords: Part-of-speech tagging, CRF model, Lexical resources, Segmentation, Multiword units. M. Constant, I. Tellier, D. Duchier, Y. Dupont, A. Sigogne, S. Billot 1 Introduction Dans cet article, nous synthétisons les résultats de plusieurs séries d’expériences réalisées à l’aide de CRF (Conditional Random Fields ou “champs markoviens conditionnels” (Lafferty et al., 2001; Tellier &amp; Tommasi, 2011)) linéaires pour apprendre à annoter des textes français à partir d’exemples, en exploitant diverses ressources linguistiques externes. La tâche à laquelle nous nous sommes attachés est celle de la segmentation en unités lexicales des phrases d’un texte, couplée à celle de leur étiquetage en catégories morphosyntaxiques (ou “part of speech” en anglais). Ces dernières années, l’étiquetage morphosyntaxique a atteint d’excellents niveaux de performance grâce à l’utilisation de modèles probabilistes discriminants comme les modèles de maximum d’entropie [MaxEnt] (Ratnapark</context>
<context position="5770" citStr="Lafferty et al., 2001" startWordPosition="795" endWordPosition="798">que posent les unités multi-mots. Puis nous passons en revue les ressources à notre disposition et menons une réflexion méthodologique sur les différents moyens de les prendre en compte dans une chaîne de traitements qui fait appel à un CRF. La dernière partie est consacrée à la présentation des résultats de nos expériences. Ces travaux ont permis la mise au point de plusieurs segmenteurs-étiqueteurs qui sont librement disponibles. 2 Les CRF 2.1 Le modèle théorique Les champs markoviens conditionnels ou CRF (Tellier &amp; Tommasi, 2011) sont des modèles probabilistes discriminants introduits par (Lafferty et al., 2001) pour l’annotation séquentielle. Ils ont été utilisés dans de nombreuses tâches de Traitement des Langues, où ils donnent d’excellents résultats (McCallum &amp; Li, 2003; Sha &amp; Pereira, 2003; Tsuruoka et al., 2009; Tellier et al., 2010). Les CRF permettent d’associer à une observation x une annotation y en se basant sur un ensemble d’exemples étiquetés, c’est-à-dire un ensemble de couples (x, y). La plupart du temps (et ce sera le cas dans la suite de cet article), x est une séquence d’unités (ici, une suite d’unités lexicales) et y la séquence des étiquettes correspondante (ici, la suite de leurs</context>
<context position="7443" citStr="Lafferty et al., 2001" startWordPosition="1079" endWordPosition="1082">es linguistiques dans un CRF arcs (edges). Deux variables sont reliées dans le graphe si elles dépendent l’une de l’autre. Le graphe sur le champ Y des CRF linéaires, dessiné en Fig 1., traduit le fait que chaque étiquette est supposée dépendre de l’étiquette précédente et de la suivante et, implicitement, de la donnée x complète. Un dessin complet du graphe devrait ainsi également relier chaque variable Yi à chaque variable du champ X, ce qu’on omet sur la figure pour la lisibilité. Y1 ... Yi−1 Yi Yi+1 ... Yn Figure 1 – graphe associé à un CRF linéaire Dans un CRF, on a la relation suivante (Lafferty et al., 2001) : ∏ (∑ ) p(y|x) 1= exp λ avec Z( k fk(yc, x, c)x) c∈C k – C est l’ensemble des cliques (sous-graphes complètement connectés) de G sur Y : dans le cas du graphe de la Fig. 1, ces cliques sont constituées soit d’un nœud isolé, soit d’un couple de nœuds successifs. – yc l’ensemble des valeurs prises par les variables de Y sur la clique c pour un étiquetage y donné : ici, c’est donc soit la valeur d’une étiquette soit celles d’un couple d’étiquettes successives – Z(x) est un coefficient de normalisation, défini de telle sorte que la somme sur y de toutes les probabilités p(y|x) pour une donnée x </context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>Lafferty J., McCallum A. &amp; Pereira F. (2001). Conditional random fields : Probabilistic models for segmenting and labeling sequence data. In Proceedings of the Eighteenth International Conference on Machine Learning (ICML 2001), p. 282–289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Lavergne</author>
<author>O Cappé</author>
<author>F Yvon</author>
</authors>
<title>Practical very large scale CRFs.</title>
<date>2010</date>
<booktitle>In Proceedings the 48th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>504--513</pages>
<contexts>
<context position="10528" citStr="Lavergne et al., 2010" startWordPosition="1596" endWordPosition="1599">RF linéaires) et K le nombre de fonctions caractéristiques. Une fois appris, l’étiqueteur est donc performant. 2.2 Les bibliothèques CRF++ et Wapiti Notre objectif étant d’insérer des connaissances linguistiques dans un apprentissage réalisé à l’aide de CRF linéaires, il nous semble important de bien comprendre le fonctionnement concret des bibliothèques qui les implémentent. Plusieurs sont disponibles pour mettre en œuvre les CRF linéaires, notamment crf.source.net 1 de Sarawagi ou Mallet 2 de McCallum. Celles que nous avons utilisées sont CRF 3++ de Taku Kado et Wapiti 4 de Thomas Lavergne (Lavergne et al., 2010), qui utilisent des moyens similaires pour instancier les fonctions caractéristiques qui entrent dans leur définition. 1. crf.sourceforge.net 2. http ://mallet.cs.umass.edu/ 3. http ://crfpp.sourceforge.net/ 4. http ://wapiti.limsi.fr M. Constant, I. Tellier, D. Duchier, Y. Dupont, A. Sigogne, S. Billot Corpus tabulaires. Les exemples d’apprentissage que requièrent ces bibliothèques sont des couples (x, y), où x est une séquence d’unités et y la séquence d’étiquettes correspondantes, de mêmes longueurs. Pour nous, une unité de x correspond à un “mot”, mais elle peut être enrichie par d’autres </context>
<context position="38334" citStr="Lavergne et al., 2010" startWordPosition="5885" endWordPosition="5888">augmente le temps d’apprentissage du simple au double voire triple selon les parties. La seconde méthode consiste à introduire des booléens en tant qu’attributs dans les colonnes des fichiers d’entraînement, chaque colonne représentant une étiquette possible dans le Lefff. Il a fallu alors générer par programme tous les patrons possibles qui combinent certains attributs entre eux. Cette méthode produit un grand nombre de fonctions caractéristiques mais Wapiti est capable de les gérer puisqu’il opère une sélection des fonctions caractéristiques les plus discriminantes en cours d’apprentissage (Lavergne et al., 2010). 9. Nous avons regroupé ensemble tous les dictionnaires, ainsi que les unités reconnues lors de l’application des grammaires locales sur le corpus. M. Constant, I. Tellier, D. Duchier, Y. Dupont, A. Sigogne, S. Billot (a) Types de traits unigrammes (b) Résultats Valeur de l’unité Sans lefff 96.5 Commence par une majuscule Est uniquement en majuscules Avec lefff (exemples) 96.6 Est un chiffre Avec lefff (attributs booléens) 97.3 Est une ponctuation 3 dernières lettres Table 3 – Résultats du LIFO avec segmentation parfaite 5.2 Evaluation de l’étiquetage avec identification des unités multi-mots</context>
</contexts>
<marker>Lavergne, Cappé, Yvon, 2010</marker>
<rawString>Lavergne T., Cappé O. &amp; Yvon F. (2010). Practical very large scale CRFs. In Proceedings the 48th Annual Meeting of the Association for Computational Linguistics (ACL), p. 504–513 : Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Martineau</author>
<author>T Nakamura</author>
<author>L Varga</author>
<author>S Voyatzi</author>
</authors>
<title>Annotation et normalisation des entités nommées.</title>
<date>2009</date>
<journal>Arena Romanistica,</journal>
<volume>4</volume>
<pages>234--243</pages>
<contexts>
<context position="4447" citStr="Martineau et al., 2009" startWordPosition="596" endWordPosition="599">s cet article, nous proposons d’intégrer les deux tâches de segmentation et d’étiquetage dans un seul modèle CRF couplé à des ressources lexicales riches. Le corpus d’apprentissage dont nous sommes partis provient du French Treebank (Abeillé et al., 2003). Les ressources linguistiques externes utilisées sont de différentes natures. Nous avons ainsi exploité plusieurs dictionnaires : Lefff (Sagot, 2010) mais aussi DELA (Courtois, 2009; Courtois et al., 1997), ainsi que des lexiques spécifiques comme Prolex (Piton et al., 1999) et quelques autres incluant des noms d’organisation et des prénoms (Martineau et al., 2009). Cet ensemble de dictionnaires est complété par une bibliothèque de grammaires locales qui reconnaissent différents types d’unités multi-mots (Constant &amp; Watrin, 2008). Nous montrons que le modèle des CRF est capable d’intégrer de telles ressources de différentes manières et permet d’atteindre ainsi le meilleur taux actuel de correction pour la segmentation et l’étiquetage du français. Dans la suite de cet article, nous commençons par présenter le modèle des CRF et le fonctionnement des bibliothèques logicielles que nous avons utilisées pour mener nos expériences. Nous décrivons ensuite le co</context>
<context position="18733" citStr="Martineau et al., 2009" startWordPosition="2848" endWordPosition="2851">s finis. Chaque transition est étiquetée par un élément lexical (ex. mange), un masque lexical correspondant à un ensemble de formes lexicales encodées dans un dictionnaire (ex. &lt;manger&gt; symbolisant toutes les formes fléchies dont le lemme est manger) ou un élément non-terminal référant à un autre automate. Elles sont très utiles pour décrire de manière compacte des unités multi-mots acceptant des variations lexicales. Un système de transduction permet d’annoter les expressions décrites, comme la catégorie grammaticale ou l’analyse des composants internes pour les entités nommées par exemple (Martineau et al., 2009). Reconnaissance. La reconnaissance automatique des unités multi-mots est, la plupart du temps, réalisée à l’aide de ressources lexicales construites manuellement (ex. pour les expressions figées) ou apprises automatiquement (ex. collocations nominales). Par ailleurs, une grande partie des entités nommées, du fait de leur syntaxe particulière sont facilement décrites et reconnues à l’aide de grammaires locales (Friburger &amp; Maurel, 2009; Martineau et al., 2009), bien qu’il existe d’autres types d’approches telles que les systèmes statistiques (McCallum &amp; Li, 2003) ou hybrides (Poibeau, 2009). L</context>
<context position="23518" citStr="Martineau et al., 2009" startWordPosition="3557" endWordPosition="3560">ntaxiques et de grammaires locales fortement lexicalisées. Nous avons utilisé notamment deux dictionnaires disponibles de mots simples et composés de la langue générale : DELA (Courtois, 2009; Courtois et al., 1997) et Lefff (Sagot, 2010). Le DELA a été construit par une équipe de linguistes. Le Lefff a été automatiquement acquis et manuellement validé. Il résulte également de la fusion de différentes sources lexicales. En complément, nous disposons aussi de lexiques spécifiques comme Prolex (Piton et al., 1999) composé de toponymes et d’autres incluant des noms d’organisation et des prénoms (Martineau et al., 2009). Les nombres d’entrées de ces divers dictionnaires sont donnés dans le tableau 1. Dictionnaire #mots simples #mots composés DELA 690,619 272,226 Lefff 553,140 26,311 Prolex 25,190 97,925 Organisations 772 587 Prénoms 22,074 2,220 Table 1 – Dictionnaires morphosyntaxiques 5. I : Inside (intérieur du segment) ; O : Outside (hors du segment) ; B : Beginning (début du segment) Intégrer des connaissances linguistiques dans un CRF Cet ensemble de dictionnaires est complété par une bibliothèque de grammaires locales qui reconnaissent différents types d’unités multi-mots comme les entités nommées (da</context>
</contexts>
<marker>Martineau, Nakamura, Varga, Voyatzi, 2009</marker>
<rawString>Martineau C., Nakamura T., Varga L. &amp; Voyatzi S. (2009). Annotation et normalisation des entités nommées. Arena Romanistica, 4, 234–243.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A McCallum</author>
<author>LiW</author>
</authors>
<title>Early results for named entity recognition with conditional random fields, feature induction and web-enhanced lexicons.</title>
<date>2003</date>
<booktitle>In Proceedings of CoNLL.</booktitle>
<marker>McCallum, LiW, 2003</marker>
<rawString>McCallum A. &amp; LiW. (2003). Early results for named entity recognition with conditional random fields, feature induction and web-enhanced lexicons. In Proceedings of CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Nasr</author>
<author>F Béchet</author>
<author>J F Rey</author>
</authors>
<title>Macaon : Une chaîne linguistique pour le traitement de graphes de mots. In Traitement Automatique des Langues Naturelles - session de démonstrations,</title>
<date>2010</date>
<location>Montréal.</location>
<contexts>
<context position="3649" citStr="Nasr et al., 2010" startWordPosition="480" endWordPosition="483">l a par ailleurs été montré que le couplage de ces modèles avec des lexiques externes augmente encore la qualité de l’annotation, comme l’illustre (Denis &amp; Sagot, 2009, 2010) pour MaxEnt. Néanmoins, les évaluations réalisées considèrent toujours en entrée un texte avec une segmentation lexicale parfaite, c’est-à-dire que les unités lexicales multi-mots, qui forment par définition des unités linguistiques, ont été parfaitement reconnues au préalable. Or cette tâche de segmentation est difficile car elle nécessite des ressources lexicales importantes. On notera que les systèmes tels que Macaon (Nasr et al., 2010) et Unitex (Paumier, 2011) intègrent une analyse lexicale avec segmentation multi-mots ambiguë avant levée d’ambiguité par l’utilisation d’un modèle de Markov caché [HMM]. Dans cet article, nous proposons d’intégrer les deux tâches de segmentation et d’étiquetage dans un seul modèle CRF couplé à des ressources lexicales riches. Le corpus d’apprentissage dont nous sommes partis provient du French Treebank (Abeillé et al., 2003). Les ressources linguistiques externes utilisées sont de différentes natures. Nous avons ainsi exploité plusieurs dictionnaires : Lefff (Sagot, 2010) mais aussi DELA (Co</context>
<context position="20418" citStr="Nasr et al., 2010" startWordPosition="3092" endWordPosition="3095">ités polylexicales comme dans la séquence une pomme de terre cuite où pomme de terre et terre cuite sont des mots composés. C’est pourquoi les outils existants de segmentation en unités multi-mots comme dans INTEX (Silberztein, 2000) ou SxPipe (Sagot &amp; Boullier, 2008) produisent une segmentation ambiguë sous la forme d’automates finis acycliques pour éviter de prendre une décision définitive M. Constant, I. Tellier, D. Duchier, Y. Dupont, A. Sigogne, S. Billot trop hâtive. Cette analyse ambiguë peut alors être intégrée dans des traitements linguistiques tels que l’étiquetage morphosyntaxique (Nasr et al., 2010; Paumier, 2011)) ou l’analyse syntaxique superficielle (Blanc et al., 2007; Nasr et al., 2010) et profonde (Sagot &amp; Boullier, 2006). 3.3 Intégration d’un segmenteur et d’un étiqueteur L’identification des unités multi-mots est similaire à une tâche de segmentation comme le chunking ou à la reconnaissance des entités nommées, qui identifient les limites de segments (chunks ou entités nommées) et les annotent. En effet, grâce à la représentation IOB 5 (Ramshaw &amp; Marcus, 1995), segmenter un texte revient à annoter ses unités minimales. Pour combiner étiquetage morphosyntaxique et reconnaissance </context>
</contexts>
<marker>Nasr, Béchet, Rey, 2010</marker>
<rawString>Nasr A., Béchet F. &amp; Rey J. F. (2010). Macaon : Une chaîne linguistique pour le traitement de graphes de mots. In Traitement Automatique des Langues Naturelles - session de démonstrations, Montréal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Paumier</author>
</authors>
<date>2011</date>
<note>Unitex 2.1 - user manual. http ://igm.univ-mlv.fr/̃unitex.</note>
<contexts>
<context position="3675" citStr="Paumier, 2011" startWordPosition="486" endWordPosition="487">e le couplage de ces modèles avec des lexiques externes augmente encore la qualité de l’annotation, comme l’illustre (Denis &amp; Sagot, 2009, 2010) pour MaxEnt. Néanmoins, les évaluations réalisées considèrent toujours en entrée un texte avec une segmentation lexicale parfaite, c’est-à-dire que les unités lexicales multi-mots, qui forment par définition des unités linguistiques, ont été parfaitement reconnues au préalable. Or cette tâche de segmentation est difficile car elle nécessite des ressources lexicales importantes. On notera que les systèmes tels que Macaon (Nasr et al., 2010) et Unitex (Paumier, 2011) intègrent une analyse lexicale avec segmentation multi-mots ambiguë avant levée d’ambiguité par l’utilisation d’un modèle de Markov caché [HMM]. Dans cet article, nous proposons d’intégrer les deux tâches de segmentation et d’étiquetage dans un seul modèle CRF couplé à des ressources lexicales riches. Le corpus d’apprentissage dont nous sommes partis provient du French Treebank (Abeillé et al., 2003). Les ressources linguistiques externes utilisées sont de différentes natures. Nous avons ainsi exploité plusieurs dictionnaires : Lefff (Sagot, 2010) mais aussi DELA (Courtois, 2009; Courtois et </context>
<context position="20434" citStr="Paumier, 2011" startWordPosition="3096" endWordPosition="3097">comme dans la séquence une pomme de terre cuite où pomme de terre et terre cuite sont des mots composés. C’est pourquoi les outils existants de segmentation en unités multi-mots comme dans INTEX (Silberztein, 2000) ou SxPipe (Sagot &amp; Boullier, 2008) produisent une segmentation ambiguë sous la forme d’automates finis acycliques pour éviter de prendre une décision définitive M. Constant, I. Tellier, D. Duchier, Y. Dupont, A. Sigogne, S. Billot trop hâtive. Cette analyse ambiguë peut alors être intégrée dans des traitements linguistiques tels que l’étiquetage morphosyntaxique (Nasr et al., 2010; Paumier, 2011)) ou l’analyse syntaxique superficielle (Blanc et al., 2007; Nasr et al., 2010) et profonde (Sagot &amp; Boullier, 2006). 3.3 Intégration d’un segmenteur et d’un étiqueteur L’identification des unités multi-mots est similaire à une tâche de segmentation comme le chunking ou à la reconnaissance des entités nommées, qui identifient les limites de segments (chunks ou entités nommées) et les annotent. En effet, grâce à la représentation IOB 5 (Ramshaw &amp; Marcus, 1995), segmenter un texte revient à annoter ses unités minimales. Pour combiner étiquetage morphosyntaxique et reconnaissance d’unités multi-m</context>
<context position="42831" citStr="Paumier, 2011" startWordPosition="6580" endWordPosition="6581">les mots simples inconnus de nos ressources, toutes les étiquettes possibles sont gardées comme candidates. Si l’analyseur n’a aucune ressource lexicale en entrée, il produit un dag représentant toutes les analyses possibles dans le jeu d’étiquettes. Intégrer des connaissances linguistiques dans un CRF chemin du dag le plus probable en fonction du modèle CRF appris. Il peut être exécuté avec ou sans segmentation multi-mots. Les ressources lexicales (pour le calcul des propriétés des fonctions caractéristiques et pour l’analyse lexicale) lui sont passées en paramètres. Les programmes d’Unitex (Paumier, 2011) sont utilisés pour l’application des ressources : consultation des dictionnaires et application des grammaires locales. 6 Conclusion Dans cet article, nous avons montré que les tâches de segmentation et d’étiquetage sont intimement liées et qu’il est naturel de les traiter simultanément. L’écart entre la performance de l’étiquetage avec ou sans segmentation est de 2 à 4 points suivant la mesure utilisée : cela mesure le “coût” d’une bonne segmentation. Par ailleurs, nous avons montré l’intérêt certain d’intégrer des ressources lexicales dans un CRF, en particulier les ressources d’unités poly</context>
</contexts>
<marker>Paumier, 2011</marker>
<rawString>Paumier S. (2011). Unitex 2.1 - user manual. http ://igm.univ-mlv.fr/̃unitex.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Piton</author>
<author>D Maurel</author>
<author>C Belleil</author>
</authors>
<title>The prolex data base : Toponyms and gentiles for nlp.</title>
<date>1999</date>
<booktitle>In Proceedings of the Third International Workshop on Applications of Natural Language to Data Bases (NLDB’99),</booktitle>
<pages>233--237</pages>
<contexts>
<context position="4355" citStr="Piton et al., 1999" startWordPosition="582" endWordPosition="585">ambiguë avant levée d’ambiguité par l’utilisation d’un modèle de Markov caché [HMM]. Dans cet article, nous proposons d’intégrer les deux tâches de segmentation et d’étiquetage dans un seul modèle CRF couplé à des ressources lexicales riches. Le corpus d’apprentissage dont nous sommes partis provient du French Treebank (Abeillé et al., 2003). Les ressources linguistiques externes utilisées sont de différentes natures. Nous avons ainsi exploité plusieurs dictionnaires : Lefff (Sagot, 2010) mais aussi DELA (Courtois, 2009; Courtois et al., 1997), ainsi que des lexiques spécifiques comme Prolex (Piton et al., 1999) et quelques autres incluant des noms d’organisation et des prénoms (Martineau et al., 2009). Cet ensemble de dictionnaires est complété par une bibliothèque de grammaires locales qui reconnaissent différents types d’unités multi-mots (Constant &amp; Watrin, 2008). Nous montrons que le modèle des CRF est capable d’intégrer de telles ressources de différentes manières et permet d’atteindre ainsi le meilleur taux actuel de correction pour la segmentation et l’étiquetage du français. Dans la suite de cet article, nous commençons par présenter le modèle des CRF et le fonctionnement des bibliothèques l</context>
<context position="23412" citStr="Piton et al., 1999" startWordPosition="3541" endWordPosition="3544"> notre étude, nous avons compilé diverses ressources lexicales sous la forme de dictionnaires morphosyntaxiques et de grammaires locales fortement lexicalisées. Nous avons utilisé notamment deux dictionnaires disponibles de mots simples et composés de la langue générale : DELA (Courtois, 2009; Courtois et al., 1997) et Lefff (Sagot, 2010). Le DELA a été construit par une équipe de linguistes. Le Lefff a été automatiquement acquis et manuellement validé. Il résulte également de la fusion de différentes sources lexicales. En complément, nous disposons aussi de lexiques spécifiques comme Prolex (Piton et al., 1999) composé de toponymes et d’autres incluant des noms d’organisation et des prénoms (Martineau et al., 2009). Les nombres d’entrées de ces divers dictionnaires sont donnés dans le tableau 1. Dictionnaire #mots simples #mots composés DELA 690,619 272,226 Lefff 553,140 26,311 Prolex 25,190 97,925 Organisations 772 587 Prénoms 22,074 2,220 Table 1 – Dictionnaires morphosyntaxiques 5. I : Inside (intérieur du segment) ; O : Outside (hors du segment) ; B : Beginning (début du segment) Intégrer des connaissances linguistiques dans un CRF Cet ensemble de dictionnaires est complété par une bibliothèque </context>
</contexts>
<marker>Piton, Maurel, Belleil, 1999</marker>
<rawString>Piton O., Maurel D. &amp; Belleil C. (1999). The prolex data base : Toponyms and gentiles for nlp. In Proceedings of the Third International Workshop on Applications of Natural Language to Data Bases (NLDB’99), p. 233–237.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Poibeau</author>
</authors>
<title>Boosting Robustness of a Named Entity Recognizer.</title>
<date>2009</date>
<journal>International Journal of Semantic Computing,</journal>
<volume>3</volume>
<issue>1</issue>
<pages>1--14</pages>
<contexts>
<context position="19330" citStr="Poibeau, 2009" startWordPosition="2933" endWordPosition="2934">eau et al., 2009). Reconnaissance. La reconnaissance automatique des unités multi-mots est, la plupart du temps, réalisée à l’aide de ressources lexicales construites manuellement (ex. pour les expressions figées) ou apprises automatiquement (ex. collocations nominales). Par ailleurs, une grande partie des entités nommées, du fait de leur syntaxe particulière sont facilement décrites et reconnues à l’aide de grammaires locales (Friburger &amp; Maurel, 2009; Martineau et al., 2009), bien qu’il existe d’autres types d’approches telles que les systèmes statistiques (McCallum &amp; Li, 2003) ou hybrides (Poibeau, 2009). L’identification de telles expressions est une tâche très difficile car les unités non décrites dans les ressources sont difficilement reconnaissables. Elle est d’autant plus difficile qu’elle dépend du contexte d’occurrence. En effet, une expression reconnue est souvent ambigue avec l’analyse en mots simples : par exemple, il en fait une priorité (mots simples) vs j’ai en fait beaucoup travaillé (mot composé). On observe parfois des chevauchements avec d’autres unités polylexicales comme dans la séquence une pomme de terre cuite où pomme de terre et terre cuite sont des mots composés. C’est</context>
</contexts>
<marker>Poibeau, 2009</marker>
<rawString>Poibeau T. (2009). Boosting Robustness of a Named Entity Recognizer. International Journal of Semantic Computing, 3(1), 1–14.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L A Ramshaw</author>
<author>M P Marcus</author>
</authors>
<title>Text chunking using transformation-based learning.</title>
<date>1995</date>
<booktitle>In Proceedings of the 3rd Workshop on Very Large Corpora,</booktitle>
<pages>88--94</pages>
<contexts>
<context position="20897" citStr="Ramshaw &amp; Marcus, 1995" startWordPosition="3166" endWordPosition="3169">ive. Cette analyse ambiguë peut alors être intégrée dans des traitements linguistiques tels que l’étiquetage morphosyntaxique (Nasr et al., 2010; Paumier, 2011)) ou l’analyse syntaxique superficielle (Blanc et al., 2007; Nasr et al., 2010) et profonde (Sagot &amp; Boullier, 2006). 3.3 Intégration d’un segmenteur et d’un étiqueteur L’identification des unités multi-mots est similaire à une tâche de segmentation comme le chunking ou à la reconnaissance des entités nommées, qui identifient les limites de segments (chunks ou entités nommées) et les annotent. En effet, grâce à la représentation IOB 5 (Ramshaw &amp; Marcus, 1995), segmenter un texte revient à annoter ses unités minimales. Pour combiner étiquetage morphosyntaxique et reconnaissance d’unités multi-mots, il suffit de concaténer les deux étiquetages en associant à chaque unité minimale une étiquette de la forme X+B ou X+I, où X est sa catégorie grammaticale et le suffixe indique si elle se trouve au début d’une unité multi-mots (B) ou dans une position “interne” (I). Le suffixe O est inutile car la fin d’un segment lexical correspond au début d’un autre (suffixe B) ou à une fin de phrase. Une telle procédure d’annotation détermine non seulement les limite</context>
</contexts>
<marker>Ramshaw, Marcus, 1995</marker>
<rawString>Ramshaw L. A. &amp; Marcus M. P. (1995). Text chunking using transformation-based learning. In Proceedings of the 3rd Workshop on Very Large Corpora, p. 88 – 94.</rawString>
</citation>
<citation valid="true">
<authors>
<author>RatnaparkhiA</author>
</authors>
<title>A maximum entropy model for part-of-speech tagging.</title>
<date>1996</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP</booktitle>
<pages>133--142</pages>
<marker>RatnaparkhiA, 1996</marker>
<rawString>RatnaparkhiA. (1996). A maximum entropy model for part-of-speech tagging. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP 1996), p. 133 – 142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I A Sag</author>
<author>T Baldwin</author>
<author>F Bond</author>
<author>A A Copestake</author>
<author>D Flickinger</author>
</authors>
<title>Multiword expressions : A pain in the neck for nlp.</title>
<date>2002</date>
<booktitle>In Proceedings of the Third International Conference on Computational Linguistics and Intelligent Text Processing (CICLing ’02),</booktitle>
<pages>1--15</pages>
<publisher>Springer-Verlag.</publisher>
<location>London, UK :</location>
<contexts>
<context position="16127" citStr="Sag et al., 2002" startWordPosition="2470" endWordPosition="2473">ique/NC ,/PONCT son/DET verdict/NC est/V implacable/ADJ ./PONCT L’unité Quant_à est la fusion de deux mots simples (Quant et à), formant la préposition composée quant_à. 3.2 Unités lexicales multi-mots Expressions multi-mots. Dans le consensus actuel du Traitement Automatique des Langues (TAL), les expressions multi-mots forment des unités linguistiques aux comportements lexicaux, syntaxiques et/ou sémantiques particuliers. Elles regroupent les expressions figées et semi-figées, les collocations, les entités nommées, les verbes à particule, les constructions à verbe support, les termes, etc. (Sag et al., 2002). Leur identification est donc cruciale avant toute analyse sémantique. Elles apparaissent à différents niveaux de l’analyse linguistique : certaines forment des unités lexicales contigues à part entière (ex. cordon bleu, San Francisco, par rapport à), d’autres composent des constituants syntaxiques comme les phrases figées (N0 prendre le taureau par les cornes ; N0 prendre N1 en compte) ou les constructions à verbe support (N0 donner un avertissement à N1 ; N0 faire du bruit). Phénomènes traités. Dans cet article, nous ne traitons que les expressions multi-mots du niveau lexical, que nous app</context>
</contexts>
<marker>Sag, Baldwin, Bond, Copestake, Flickinger, 2002</marker>
<rawString>Sag I. A., Baldwin T., Bond F., Copestake A. A. &amp; Flickinger D. (2002). Multiword expressions : A pain in the neck for nlp. In Proceedings of the Third International Conference on Computational Linguistics and Intelligent Text Processing (CICLing ’02), p. 1–15, London, UK : Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Sagot</author>
</authors>
<title>The lefff, a freely available, accurate and large-coverage lexicon for french.</title>
<date>2010</date>
<booktitle>In Proceedings of the 7th International Conference on Language Resources and Evaluation (LREC’10).</booktitle>
<contexts>
<context position="4229" citStr="Sagot, 2010" startWordPosition="564" endWordPosition="565">s que Macaon (Nasr et al., 2010) et Unitex (Paumier, 2011) intègrent une analyse lexicale avec segmentation multi-mots ambiguë avant levée d’ambiguité par l’utilisation d’un modèle de Markov caché [HMM]. Dans cet article, nous proposons d’intégrer les deux tâches de segmentation et d’étiquetage dans un seul modèle CRF couplé à des ressources lexicales riches. Le corpus d’apprentissage dont nous sommes partis provient du French Treebank (Abeillé et al., 2003). Les ressources linguistiques externes utilisées sont de différentes natures. Nous avons ainsi exploité plusieurs dictionnaires : Lefff (Sagot, 2010) mais aussi DELA (Courtois, 2009; Courtois et al., 1997), ainsi que des lexiques spécifiques comme Prolex (Piton et al., 1999) et quelques autres incluant des noms d’organisation et des prénoms (Martineau et al., 2009). Cet ensemble de dictionnaires est complété par une bibliothèque de grammaires locales qui reconnaissent différents types d’unités multi-mots (Constant &amp; Watrin, 2008). Nous montrons que le modèle des CRF est capable d’intégrer de telles ressources de différentes manières et permet d’atteindre ainsi le meilleur taux actuel de correction pour la segmentation et l’étiquetage du fr</context>
<context position="23133" citStr="Sagot, 2010" startWordPosition="3500" endWordPosition="3501">me s’il existe de plus en plus d’études sur l’extraction automatique d’unités multi-mots, en particulier les collocations ou les termes (Daille, 1995; Dias, 2003; Seretan et al., 2003), les ressources les plus riches et les plus précises ont été aquises manuellement. Pour notre étude, nous avons compilé diverses ressources lexicales sous la forme de dictionnaires morphosyntaxiques et de grammaires locales fortement lexicalisées. Nous avons utilisé notamment deux dictionnaires disponibles de mots simples et composés de la langue générale : DELA (Courtois, 2009; Courtois et al., 1997) et Lefff (Sagot, 2010). Le DELA a été construit par une équipe de linguistes. Le Lefff a été automatiquement acquis et manuellement validé. Il résulte également de la fusion de différentes sources lexicales. En complément, nous disposons aussi de lexiques spécifiques comme Prolex (Piton et al., 1999) composé de toponymes et d’autres incluant des noms d’organisation et des prénoms (Martineau et al., 2009). Les nombres d’entrées de ces divers dictionnaires sont donnés dans le tableau 1. Dictionnaire #mots simples #mots composés DELA 690,619 272,226 Lefff 553,140 26,311 Prolex 25,190 97,925 Organisations 772 587 Préno</context>
</contexts>
<marker>Sagot, 2010</marker>
<rawString>Sagot B. (2010). The lefff, a freely available, accurate and large-coverage lexicon for french. In Proceedings of the 7th International Conference on Language Resources and Evaluation (LREC’10).</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Sagot</author>
<author>P Boullier</author>
</authors>
<title>Deep non-probabilistic parsing of large corpora.</title>
<date>2006</date>
<booktitle>In Proceedings of the 5th International Conference on Language Resources and Evaluation (LREC’06).</booktitle>
<contexts>
<context position="20550" citStr="Sagot &amp; Boullier, 2006" startWordPosition="3112" endWordPosition="3115">est pourquoi les outils existants de segmentation en unités multi-mots comme dans INTEX (Silberztein, 2000) ou SxPipe (Sagot &amp; Boullier, 2008) produisent une segmentation ambiguë sous la forme d’automates finis acycliques pour éviter de prendre une décision définitive M. Constant, I. Tellier, D. Duchier, Y. Dupont, A. Sigogne, S. Billot trop hâtive. Cette analyse ambiguë peut alors être intégrée dans des traitements linguistiques tels que l’étiquetage morphosyntaxique (Nasr et al., 2010; Paumier, 2011)) ou l’analyse syntaxique superficielle (Blanc et al., 2007; Nasr et al., 2010) et profonde (Sagot &amp; Boullier, 2006). 3.3 Intégration d’un segmenteur et d’un étiqueteur L’identification des unités multi-mots est similaire à une tâche de segmentation comme le chunking ou à la reconnaissance des entités nommées, qui identifient les limites de segments (chunks ou entités nommées) et les annotent. En effet, grâce à la représentation IOB 5 (Ramshaw &amp; Marcus, 1995), segmenter un texte revient à annoter ses unités minimales. Pour combiner étiquetage morphosyntaxique et reconnaissance d’unités multi-mots, il suffit de concaténer les deux étiquetages en associant à chaque unité minimale une étiquette de la forme X+B</context>
</contexts>
<marker>Sagot, Boullier, 2006</marker>
<rawString>Sagot B. &amp; Boullier P. (2006). Deep non-probabilistic parsing of large corpora. In Proceedings of the 5th International Conference on Language Resources and Evaluation (LREC’06).</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Sagot</author>
<author>P Boullier</author>
</authors>
<title>Sxpipe 2 : architecture pour le traitement pré-syntaxique de corpus bruts. Traitement Automatique des Langues,</title>
<date>2008</date>
<volume>49</volume>
<issue>2</issue>
<pages>155--188</pages>
<contexts>
<context position="20069" citStr="Sagot &amp; Boullier, 2008" startWordPosition="3042" endWordPosition="3045">s sont difficilement reconnaissables. Elle est d’autant plus difficile qu’elle dépend du contexte d’occurrence. En effet, une expression reconnue est souvent ambigue avec l’analyse en mots simples : par exemple, il en fait une priorité (mots simples) vs j’ai en fait beaucoup travaillé (mot composé). On observe parfois des chevauchements avec d’autres unités polylexicales comme dans la séquence une pomme de terre cuite où pomme de terre et terre cuite sont des mots composés. C’est pourquoi les outils existants de segmentation en unités multi-mots comme dans INTEX (Silberztein, 2000) ou SxPipe (Sagot &amp; Boullier, 2008) produisent une segmentation ambiguë sous la forme d’automates finis acycliques pour éviter de prendre une décision définitive M. Constant, I. Tellier, D. Duchier, Y. Dupont, A. Sigogne, S. Billot trop hâtive. Cette analyse ambiguë peut alors être intégrée dans des traitements linguistiques tels que l’étiquetage morphosyntaxique (Nasr et al., 2010; Paumier, 2011)) ou l’analyse syntaxique superficielle (Blanc et al., 2007; Nasr et al., 2010) et profonde (Sagot &amp; Boullier, 2006). 3.3 Intégration d’un segmenteur et d’un étiqueteur L’identification des unités multi-mots est similaire à une tâche d</context>
</contexts>
<marker>Sagot, Boullier, 2008</marker>
<rawString>Sagot B. &amp; Boullier P. (2008). Sxpipe 2 : architecture pour le traitement pré-syntaxique de corpus bruts. Traitement Automatique des Langues, 49(2), 155–188.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Schmid</author>
</authors>
<title>Probabilistic part-of-speech tagging using decision trees.</title>
<date>1994</date>
<booktitle>In Proceedings of International Conference on New Methods in Language Processing,</booktitle>
<pages>252--259</pages>
<contexts>
<context position="35677" citStr="Schmid, 1994" startWordPosition="5418" endWordPosition="5419"> un modèle LEX en utilisant tous les traits décrits dans la table 1(a). Nous notons STD le modèle incorporant les mêmes traits à l’exception de ceux issus de la ressource. La deuxième méthode consiste à procéder à un filtrage a priori de toutes les étiquettes absentes de la ressource pour chaque unité. Si l’unité est absente, toutes les étiquettes sont gardées. Les étiquettes des ressources ont été ajustées à celles du corpus pour le filtrage. Nous avons comparé les résultats avec d’autres outils d’étiquetage que nous avons tous entrainés sur le corpus FTB-TRAIN. Nous avons évalué TreeTagger (Schmid, 1994) basé sur des arbres de décision probabilistiques, SVMTool (Giménez &amp; Márquez., 2004) basé sur les Séparateurs à Vastes Marges utilisant des traits indépendants de la langue, MElt (Denis &amp; Sagot, 2009) basé sur un modèle MaxEnt incorporant en plus des traits dépendants de la langue issus de lexiques externes. Les lexiques utilisés pour entraîner et tester MElt intègrent toutes les ressources de la section 4.1 9. Les précisions obtenues sur le corpus FTB-TEST pour les différents systèmes sont données en pourcentage dans la table 1(b) avec un intervalle de confiance à 95% de +/-0,1. (b) Comparai</context>
</contexts>
<marker>Schmid, 1994</marker>
<rawString>Schmid H. (1994). Probabilistic part-of-speech tagging using decision trees. In Proceedings of International Conference on New Methods in Language Processing, p. 252 – 259.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Seretan</author>
<author>L Nerima</author>
<author>E Wehrli</author>
</authors>
<title>Extraction of multi-word collocations using syntactic bigram composition.</title>
<date>2003</date>
<booktitle>In Proceedings of the Fourth International Conference on Recent Advances in NLP (RANLP-2003),</booktitle>
<pages>424--431</pages>
<location>Borovets, Bulgaria.</location>
<contexts>
<context position="22705" citStr="Seretan et al., 2003" startWordPosition="3436" endWordPosition="3439"> fortement de la richesse de ressources lexicales utilisées, il s’agit maintenant de trouver les meilleures façons d’intégrer ce type d’informations dans nos CRF. 4 Exploitation d’une ressource externe Dans cette section, nous commençons par présenter les différentes ressources que nous avons à notre disposition, et nous cherchons tous les moyens possibles de les prendre en compte dans un apprentissage avec des CRF. 4.1 Ressources Même s’il existe de plus en plus d’études sur l’extraction automatique d’unités multi-mots, en particulier les collocations ou les termes (Daille, 1995; Dias, 2003; Seretan et al., 2003), les ressources les plus riches et les plus précises ont été aquises manuellement. Pour notre étude, nous avons compilé diverses ressources lexicales sous la forme de dictionnaires morphosyntaxiques et de grammaires locales fortement lexicalisées. Nous avons utilisé notamment deux dictionnaires disponibles de mots simples et composés de la langue générale : DELA (Courtois, 2009; Courtois et al., 1997) et Lefff (Sagot, 2010). Le DELA a été construit par une équipe de linguistes. Le Lefff a été automatiquement acquis et manuellement validé. Il résulte également de la fusion de différentes sourc</context>
</contexts>
<marker>Seretan, Nerima, Wehrli, 2003</marker>
<rawString>Seretan V., Nerima L. &amp; Wehrli E. (2003). Extraction of multi-word collocations using syntactic bigram composition. In Proceedings of the Fourth International Conference on Recent Advances in NLP (RANLP-2003), p. 424–431, Borovets, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Sha</author>
<author>F Pereira</author>
</authors>
<title>Shallow parsing with conditional random fields.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT-NAACL,</booktitle>
<pages>213--220</pages>
<contexts>
<context position="5956" citStr="Sha &amp; Pereira, 2003" startWordPosition="823" endWordPosition="826">s une chaîne de traitements qui fait appel à un CRF. La dernière partie est consacrée à la présentation des résultats de nos expériences. Ces travaux ont permis la mise au point de plusieurs segmenteurs-étiqueteurs qui sont librement disponibles. 2 Les CRF 2.1 Le modèle théorique Les champs markoviens conditionnels ou CRF (Tellier &amp; Tommasi, 2011) sont des modèles probabilistes discriminants introduits par (Lafferty et al., 2001) pour l’annotation séquentielle. Ils ont été utilisés dans de nombreuses tâches de Traitement des Langues, où ils donnent d’excellents résultats (McCallum &amp; Li, 2003; Sha &amp; Pereira, 2003; Tsuruoka et al., 2009; Tellier et al., 2010). Les CRF permettent d’associer à une observation x une annotation y en se basant sur un ensemble d’exemples étiquetés, c’est-à-dire un ensemble de couples (x, y). La plupart du temps (et ce sera le cas dans la suite de cet article), x est une séquence d’unités (ici, une suite d’unités lexicales) et y la séquence des étiquettes correspondante (ici, la suite de leurs catégories morphosyntaxiques, éventuellement enrichie pour coder la segmentation). Les CRF sont des modèles discriminants qui appartiennent à la famille des modèles graphiques non dirig</context>
</contexts>
<marker>Sha, Pereira, 2003</marker>
<rawString>Sha F. &amp; Pereira F. (2003). Shallow parsing with conditional random fields. In Proceedings of HLT-NAACL, p. 213 – 220.</rawString>
</citation>
<citation valid="true">
<authors>
<author>SilberzteinM</author>
</authors>
<title>Intex : an fst toolbox.</title>
<date>2000</date>
<journal>Theoretical Computer Science,</journal>
<volume>231</volume>
<issue>1</issue>
<pages>33--46</pages>
<marker>SilberzteinM, 2000</marker>
<rawString>SilberzteinM. (2000). Intex : an fst toolbox. Theoretical Computer Science, 231(1), 33–46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Tellier</author>
<author>I Eshkol</author>
<author>S Taalab</author>
<author>J P Prost</author>
</authors>
<title>Pos-tagging for oral texts with crf and category decomposition.</title>
<date>2010</date>
<journal>Research in Computing Science,</journal>
<volume>46</volume>
<pages>79--90</pages>
<contexts>
<context position="6002" citStr="Tellier et al., 2010" startWordPosition="831" endWordPosition="834"> un CRF. La dernière partie est consacrée à la présentation des résultats de nos expériences. Ces travaux ont permis la mise au point de plusieurs segmenteurs-étiqueteurs qui sont librement disponibles. 2 Les CRF 2.1 Le modèle théorique Les champs markoviens conditionnels ou CRF (Tellier &amp; Tommasi, 2011) sont des modèles probabilistes discriminants introduits par (Lafferty et al., 2001) pour l’annotation séquentielle. Ils ont été utilisés dans de nombreuses tâches de Traitement des Langues, où ils donnent d’excellents résultats (McCallum &amp; Li, 2003; Sha &amp; Pereira, 2003; Tsuruoka et al., 2009; Tellier et al., 2010). Les CRF permettent d’associer à une observation x une annotation y en se basant sur un ensemble d’exemples étiquetés, c’est-à-dire un ensemble de couples (x, y). La plupart du temps (et ce sera le cas dans la suite de cet article), x est une séquence d’unités (ici, une suite d’unités lexicales) et y la séquence des étiquettes correspondante (ici, la suite de leurs catégories morphosyntaxiques, éventuellement enrichie pour coder la segmentation). Les CRF sont des modèles discriminants qui appartiennent à la famille des modèles graphiques non dirigés. Ils sont définis par X et Y, deux champs a</context>
<context position="30515" citStr="Tellier et al., 2010" startWordPosition="4621" endWordPosition="4624"> à notre disposition trois “leviers” d’action possibles : – le choix des étiquettes et des propriétés des unités (les colonnes des données tabulaires) – le choix des exemples (les lignes) – le choix des fonctions caractéristiques (via les patrons), choix qui dépend fortement des précédents Nous avons déjà vu qu’un choix pertinent d’étiquettes permettait de “coder” en quelque sorte les deux problèmes de la segmentation et de l’étiquetage simultanément. D’autres expériences ont montré l’intérêt de décomposer le jeu d’étiquettes en sous-étiquettes, notamment quand celles-ci sont trop nombreuses (Tellier et al., 2010). Mais le problème auquel nous nous confrontons ici ne requiert pas un tel traitement, nous ne l’avons pas mis en œuvre. Il est en revanche “naturel” d’insérer les informations des ressources en tant que propriétés des unités d’un exemple p x, donc en jouant sur les colonnes x2i , ..., xi . Plusieurs choix sont encore possibles pour cela, suivant qu’on se contente de concaténer les différentes étiquettes possibles d’une même unité pour en faire une seule colonne de nature textuelle, ou bien qu’on définisse autant de colonnes à valeur booléenne que d’étiquettes possibles dans l’ensemble de la r</context>
</contexts>
<marker>Tellier, Eshkol, Taalab, Prost, 2010</marker>
<rawString>Tellier I., Eshkol I., Taalab S. &amp; Prost J. P. (2010). Pos-tagging for oral texts with crf and category decomposition. Research in Computing Science, 46, 79–90. Special issue &amp;quot;Natural Language Processing and its Applications&amp;quot;.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Tellier</author>
<author>M Tommasi</author>
</authors>
<title>Champs Markoviens Conditionnels pour l’extraction d’information. In Eric Gaussier &amp; François Yvon, Eds., Modèles probabilistes pour l’accès à l’information textuelle.</title>
<date>2011</date>
<publisher>Hermès.</publisher>
<contexts>
<context position="2281" citStr="Tellier &amp; Tommasi, 2011" startWordPosition="282" endWordPosition="285">rge-coverage lexical resources including multiword units and reach stateof-the-art tagging results for French. Mots-clés : Etiquetage morphosyntaxique, Modèle CRF, Ressources lexicales, Segmentation, Unités polylexicales. Keywords: Part-of-speech tagging, CRF model, Lexical resources, Segmentation, Multiword units. M. Constant, I. Tellier, D. Duchier, Y. Dupont, A. Sigogne, S. Billot 1 Introduction Dans cet article, nous synthétisons les résultats de plusieurs séries d’expériences réalisées à l’aide de CRF (Conditional Random Fields ou “champs markoviens conditionnels” (Lafferty et al., 2001; Tellier &amp; Tommasi, 2011)) linéaires pour apprendre à annoter des textes français à partir d’exemples, en exploitant diverses ressources linguistiques externes. La tâche à laquelle nous nous sommes attachés est celle de la segmentation en unités lexicales des phrases d’un texte, couplée à celle de leur étiquetage en catégories morphosyntaxiques (ou “part of speech” en anglais). Ces dernières années, l’étiquetage morphosyntaxique a atteint d’excellents niveaux de performance grâce à l’utilisation de modèles probabilistes discriminants comme les modèles de maximum d’entropie [MaxEnt] (Ratnaparkhi, 1996; Toutanova et al.</context>
<context position="5686" citStr="Tellier &amp; Tommasi, 2011" startWordPosition="783" endWordPosition="786">ssage ainsi que la tâche que nous traitons, en détaillant les difficultés spécifiques que posent les unités multi-mots. Puis nous passons en revue les ressources à notre disposition et menons une réflexion méthodologique sur les différents moyens de les prendre en compte dans une chaîne de traitements qui fait appel à un CRF. La dernière partie est consacrée à la présentation des résultats de nos expériences. Ces travaux ont permis la mise au point de plusieurs segmenteurs-étiqueteurs qui sont librement disponibles. 2 Les CRF 2.1 Le modèle théorique Les champs markoviens conditionnels ou CRF (Tellier &amp; Tommasi, 2011) sont des modèles probabilistes discriminants introduits par (Lafferty et al., 2001) pour l’annotation séquentielle. Ils ont été utilisés dans de nombreuses tâches de Traitement des Langues, où ils donnent d’excellents résultats (McCallum &amp; Li, 2003; Sha &amp; Pereira, 2003; Tsuruoka et al., 2009; Tellier et al., 2010). Les CRF permettent d’associer à une observation x une annotation y en se basant sur un ensemble d’exemples étiquetés, c’est-à-dire un ensemble de couples (x, y). La plupart du temps (et ce sera le cas dans la suite de cet article), x est une séquence d’unités (ici, une suite d’unit</context>
</contexts>
<marker>Tellier, Tommasi, 2011</marker>
<rawString>Tellier I. &amp; Tommasi M. (2011). Champs Markoviens Conditionnels pour l’extraction d’information. In Eric Gaussier &amp; François Yvon, Eds., Modèles probabilistes pour l’accès à l’information textuelle. Hermès.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Klein D ToutanovaK</author>
<author>C D Manning</author>
<author>Y Y Singer</author>
</authors>
<title>Feature-rich part-of-speech tagging with a cyclic dependency network.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT-NAACL 2003,</booktitle>
<pages>252--259</pages>
<marker>ToutanovaK, Manning, Singer, 2003</marker>
<rawString>ToutanovaK., Klein D., Manning C. D. &amp; Singer Y. Y. (2003). Feature-rich part-of-speech tagging with a cyclic dependency network. In Proceedings of HLT-NAACL 2003, p. 252 – 259.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Tsuruoka</author>
<author>J Tsujii</author>
<author>S Ananiadou</author>
</authors>
<title>Fast full parsing by linear-chain conditional random fields.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics (EACL</booktitle>
<pages>790--798</pages>
<contexts>
<context position="3028" citStr="Tsuruoka et al., 2009" startWordPosition="389" endWordPosition="392"> externes. La tâche à laquelle nous nous sommes attachés est celle de la segmentation en unités lexicales des phrases d’un texte, couplée à celle de leur étiquetage en catégories morphosyntaxiques (ou “part of speech” en anglais). Ces dernières années, l’étiquetage morphosyntaxique a atteint d’excellents niveaux de performance grâce à l’utilisation de modèles probabilistes discriminants comme les modèles de maximum d’entropie [MaxEnt] (Ratnaparkhi, 1996; Toutanova et al., 2003), les séparateurs à vaste marge [SVM] (Giménez &amp; Márquez., 2004) ou, déjà, les champs markoviens conditionnels [CRF] (Tsuruoka et al., 2009). Il a par ailleurs été montré que le couplage de ces modèles avec des lexiques externes augmente encore la qualité de l’annotation, comme l’illustre (Denis &amp; Sagot, 2009, 2010) pour MaxEnt. Néanmoins, les évaluations réalisées considèrent toujours en entrée un texte avec une segmentation lexicale parfaite, c’est-à-dire que les unités lexicales multi-mots, qui forment par définition des unités linguistiques, ont été parfaitement reconnues au préalable. Or cette tâche de segmentation est difficile car elle nécessite des ressources lexicales importantes. On notera que les systèmes tels que Macao</context>
<context position="5979" citStr="Tsuruoka et al., 2009" startWordPosition="827" endWordPosition="830">ements qui fait appel à un CRF. La dernière partie est consacrée à la présentation des résultats de nos expériences. Ces travaux ont permis la mise au point de plusieurs segmenteurs-étiqueteurs qui sont librement disponibles. 2 Les CRF 2.1 Le modèle théorique Les champs markoviens conditionnels ou CRF (Tellier &amp; Tommasi, 2011) sont des modèles probabilistes discriminants introduits par (Lafferty et al., 2001) pour l’annotation séquentielle. Ils ont été utilisés dans de nombreuses tâches de Traitement des Langues, où ils donnent d’excellents résultats (McCallum &amp; Li, 2003; Sha &amp; Pereira, 2003; Tsuruoka et al., 2009; Tellier et al., 2010). Les CRF permettent d’associer à une observation x une annotation y en se basant sur un ensemble d’exemples étiquetés, c’est-à-dire un ensemble de couples (x, y). La plupart du temps (et ce sera le cas dans la suite de cet article), x est une séquence d’unités (ici, une suite d’unités lexicales) et y la séquence des étiquettes correspondante (ici, la suite de leurs catégories morphosyntaxiques, éventuellement enrichie pour coder la segmentation). Les CRF sont des modèles discriminants qui appartiennent à la famille des modèles graphiques non dirigés. Ils sont définis pa</context>
</contexts>
<marker>Tsuruoka, Tsujii, Ananiadou, 2009</marker>
<rawString>Tsuruoka Y., Tsujii J. &amp; Ananiadou S. (2009). Fast full parsing by linear-chain conditional random fields. In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics (EACL 2009), p. 790–798.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>