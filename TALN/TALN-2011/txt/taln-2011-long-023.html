<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
"http://www.w3.org/TR/html4/loose.dtd">
<html><head><title>Segmentation et induction de lexique non-supervis&#233;es du mandarin</title>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
</head>
<body>
<div style="page-break-before:always; page-break-after:always"><div><p>TALN 2011, Montpellier, 27 juin &#8211; 1er juillet 2011
</p>
<p>Segmentation et induction de lexique non-supervis&#233;es
du mandarin
</p>
<p>Pierre Magistry Beno&#238;t Sagot
Alpage, INRIA Paris&#8211;Rocquencourt &amp; Universit&#233; Paris 7,
Rocquencourt, BP 105, 78153 Le Chesnay Cedex, France
</p>
<p>{pierre.magistry,benoit.sagot}@inria.fr
</p>
<p>R&#233;sum&#233;. Pour la plupart des langues utilisant l'alphabet latin, le d&#233;coupage d'un texte selon les espaces et les
symboles de ponctuation est une bonne approximation d'un d&#233;coupage en unit&#233;s lexicales. Bien que cette approxi-
mation cache de nombreuses difficult&#233;s, elles sont sans comparaison avec celles que l'on rencontre lorsque l'on
veut traiter des langues qui, comme le chinois mandarin, n'utilisent pas l'espace. Un grand nombre de syst&#232;mes de
segmentation ont &#233;t&#233; propos&#233;s parmi lesquels certains adoptent une approche non-supervis&#233;e motiv&#233;e linguistique-
ment. Cependant les m&#233;thodes d'&#233;valuation commun&#233;ment utilis&#233;es ne rendent pas compte de toutes les propri&#233;t&#233;s
de tels syst&#232;mes. Dans cet article, nous montrons qu'un mod&#232;le simple qui repose sur une reformulation en termes
d'entropie d'une hypoth&#232;se ind&#233;pendante de la langue &#233;nonc&#233;e par Harris (1955), permet de segmenter un corpus et
d'en extraire un lexique. Test&#233; sur le corpus de l'Academia Sinica, notre syst&#232;me permet l'induction d'une segmen-
tation et d'un lexique qui ont de bonnes propri&#233;t&#233;s intrins&#232;ques et dont les caract&#233;ristiques sont similaires &#224; celles
du lexique sous-jacent au corpus segment&#233; manuellement. De plus, on constate une certaine corr&#233;lation entre les
r&#233;sultats du mod&#232;le de segmentation et les structures syntaxiques fournies par une sous-partie arbor&#233;e corpus.
Abstract. For most languages using the Latin alphabet, tokenizing a text on spaces and punctuation marks
is a good approximation of a segmentation into lexical units. Although this approximation hides many difficulties,
they do not compare with those arising when dealing with languages that do not use spaces, such as Mandarin
Chinese. Many segmentation systems have been proposed, some of them use linguistitically motivated unsuper-
vized algorithms. However, standard evaluation practices fail to account for some properties of such systems. In
this paper, we show that a simple model, based on an entropy-based reformulation of a language-independent hy-
pothesis put forward by Harris (1955), allows for segmenting a corpus and extracting a lexicon from the results.
Tested on the Academia Sinica Corpus, our system allows for inducing a segmentation and a lexicon with good in-
trinsic properties and whose characteristics are similar to those of the lexicon underlying the manually-segmented
corpus. Moreover, the results of the segmentation model correlate with the syntactic structures provided by the
syntactically annotated subpart of the corpus.
Mots-cl&#233;s : Segmentation non-supervis&#233;e, entropie, induction de lexique, unit&#233; lexicale, chinois mandarin.
Keywords: Non-supervized segmentation, entropy, lexicon induction, Mandarin Chinese.
</p>
<p>1 Introduction
La segmentation d'un texte en formes 1 est la premi&#232;re &#233;tape de presque tout traitement automatique de donn&#233;es
textuelles. Pour la plupart des langues utilisant l'alphabet latin, dont le fran&#231;ais ou l'anglais, un d&#233;coupage selon
les espaces et les symboles de ponctuation est une bonne approximation d'une segmentation en unit&#233;s lexicales.
&#192; l'inverse, dans le cas des syst&#232;mes d'&#233;criture utilis&#233;s par exemple pour &#233;crire le chinois, le japonais, le thai,
le khmer ou le vietnamien, la typographie n'est pas utilis&#233;e pour indiquer des fronti&#232;res entre les m&#234;mes unit&#233;s
linguistiques : en vietnamien, qui utilise une variante de l'alphabet latin, l'espace s&#233;pare des unit&#233;s sous-lexicales.
En chinois ou japonais, seuls les signes de ponctuation indiquent des fronti&#232;res entre unit&#233;s lexicales ; ailleurs, les
caract&#232;res, qui repr&#233;sentent aussi des unit&#233;s sous-lexicales, sont directement juxtapos&#233;s. L'&#233;tape de segmentation en
unit&#233;s lexicales est donc un probl&#232;me d&#233;licat pour ces langues dites non-segment&#233;es, et donne lieu &#224; une litt&#233;rature
</p>
<p>1. Dans cet article, une forme est un segment continu de texte venant occuper de fa&#231;on autonome une position syntaxique. Travaillant sur
le mandarin, nous pouvons faire l'approximation qu'il y a identit&#233; entre la notion de forme et celle d'unit&#233; lexicale, Pour une discussion plus
d&#233;taill&#233;es de l'unit&#233; lexicale en mandarin, se reporter &#224; Packard (2000) ou en fran&#231;ais &#224; Nguyen (2006).</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>P&#63337;&#63333;&#63346;&#63346;&#63333; M&#63329;&#63335;&#63337;&#63347;&#63348;&#63346;&#63353; B&#63333;&#63342;&#63343;&#63470;&#63348; S&#63329;&#63335;&#63343;&#63348;
</p>
<p>abondante (Zhao &amp; Liu, 2010), y compris dans la communaut&#233; francophone (Seng et al., 2009; Wu, 2010). Mais
de tels travaux peuvent aussi &#234;tre utiles pour les langues segment&#233;es, en raison des cas de non-correspondance
entre s&#233;parateurs et fronti&#232;res d'unit&#233;s lexicales, lesquelles restent difficiles &#224; d&#233;finir et &#224; rep&#233;rer quelle que soit la
langue (Zhikov et al., 2010).
Parmi les m&#233;thodes de segmentation, nous nous int&#233;ressons en particulier aux m&#233;thodes non supervis&#233;es qui
cherchent une d&#233;finition implicite du mot en faisant &#233;merger la segmentation &#224; partir des propri&#233;t&#233;s non-al&#233;atoires
de la distribution des formes en corpus. Ces m&#233;thodes sont difficiles &#224; &#233;valuer car elles ne s'adaptent pas &#224; un
standard donn&#233;. En contrepartie, elles pr&#233;sentent un plus grand potentiel d'adaptation &#224; la dynamique d'une langue
(changement de domaine, variantes g&#233;ographiques, &#233;volution diachronique, traitement des n&#233;ologismes), et peuvent
&#234;tre utilis&#233;es pour la segmentation de langues peu ou pas dot&#233;es.
Nous d&#233;crivons ici une s&#233;rie d'exp&#233;riences de segmentation en unit&#233;s lexicales r&#233;alis&#233;es sur le chinois mandarin au
moyen d'un syst&#232;me non-supervis&#233; qui repose sur une hypoth&#232;se motiv&#233;e linguistiquement formul&#233;e par (Harris,
1955), en adaptant sa mod&#233;lisation pr&#233;sent&#233;e par (Tanaka-Ishii, 2005) dans le m&#234;me but (Jin &amp; Tanaka-Ishii,
2006). Nous insistons en particulier sur l'&#233;valuation des r&#233;sultats obtenus, t&#226;che rendue d&#233;licate par la nature non-
supervis&#233;e de l'approche et par la vari&#233;t&#233; des conventions de segmentation qui existent pour le chinois mandarin.
Dans la section suivante nous pr&#233;sentons la t&#226;che de segmentation et les probl&#232;mes que posent la m&#233;thode tradi-
tionnellement utilis&#233;e pour son &#233;valuation. Les sections 3 et 4 pr&#233;sentent les syst&#232;mes dont nous nous inspirons et
celui que nous avons d&#233;velopp&#233;. Nous utilisons ensuite notre syst&#232;me de segmentation pour extraire un lexique, dont
nous proposons une &#233;valuation (section 5). Enfin nous cherchons &#224; corr&#233;ler la sortie du syst&#232;me de segmentation
&#233;tudi&#233; avec des informations syntaxiques extraites d'un corpus arbor&#233;.
</p>
<p>2 La segmentation du chinois
</p>
<p>La segmentation est la premi&#232;re &#233;tape de tout syst&#232;me d'analyse automatique du chinois &#233;crit. En fran&#231;ais et dans la
majorit&#233; des langues utilisant l'alphabet latin, un d&#233;coupage sur les espaces (et autour des signes de ponctuation),
souvent appel&#233; tokenisation et dont la sortie est un flux de tokens, constitue une premi&#232;re &#233;tape raisonnable, que
l'on peut ensuite affiner pour identifier les cas de non-alignement entre tokens et formes (qui peuvent par exemple
&#234;tre des formes compos&#233;es). &#192; l'inverse, l'&#233;criture chinoise ne comporte pas de s&#233;parateur typographique comme
l'espace. Un d&#233;coupage effectu&#233; uniquement autour des caract&#232;res de ponctuation produirait des tokens bien plus
longs que des formes. &#192; l'inverse, un d&#233;coupage isolant chaque caract&#232;re chinois (ci-apr&#232;s sinogramme) ressem-
blerait plut&#244;t &#224; une segmentation en morph(&#232;m)es qu'en formes. Il faut donc consid&#233;rer un texte en chinois comme
un flux de sinogrammes, la t&#226;che de segmentation consistant &#224; identifier entre quels sinogrammes il faut segmenter
le texte afin de d&#233;limiter les formes, que l'on peut, en mandarin, assimiler &#224; des unit&#233;s lexicales.
</p>
<p>2.1 &#201;tat de l'art, enjeux actuels
</p>
<p>Un grand nombre de m&#233;thodes ont &#233;t&#233; propos&#233;es pour effectuer une segmentation automatique. Certaines re-
posent sur des r&#232;gles et des lexiques, d'autres utilisent des m&#233;thodes d'apprentissage automatique supervis&#233; ou
non-supervis&#233;. Cinq campagnes du &#171; Chinese Word Segmentation Bakeoff &#187; ont &#233;t&#233; organis&#233;s par l'ACL, dont la
derni&#232;re s'est tenue &#224; l'&#233;t&#233; 2010. Zhao &amp; Liu (2010) donnent un r&#233;sum&#233; des performances obtenues par les syst&#232;mes
en comp&#233;tition. Ils soulignent que si la pr&#233;cision peut sembler satisfaisante, la tol&#233;rance au changement de domaine
et la reconnaissance des mots inconnus restent les limitations majeures.
Notons que lors de cette campagne, le syst&#232;me de base (baseline) et le meilleur syst&#232;me (topline) sont obtenus avec
le m&#234;me algorithme, un simple maximum-matching (minimisation du nombre de mots) reposant sur un inventaire
d'unit&#233;s lexicales, et ne se distinguent que par le lexique utilis&#233; : la baseline utilise un lexique extrait &#224; partir du
corpus d'entra&#238;nement, tandis que la topline utilise un lexique extrait &#224; partir de la totalit&#233; du corpus et conna&#238;t
donc tous les formes attendues. Xue (2003), qui pr&#233;sentait un syst&#232;me d'apprentissage supervis&#233; reposant sur une
classification IOB (Inside, Outside, Begin) des sinogrammes, commente les r&#233;sultats d'une autre heuristique simple
qui repose sur un lexique, celle dite du longest-match gauche-droite (plus longue cha&#238;ne d'abord) : cette heuristique
fournit de tr&#232;s bons r&#233;sultats (f-mesure 0,952) si le lexique est exhaustif mais se d&#233;grade tr&#232;s rapidement lorsque
le corpus de test contient des mots inconnus (f-mesure de 0,898). Le maximum-matching utilis&#233; lors du bakeoff
obtient quant &#224; lui des scores (f-mesure) sup&#233;rieurs &#224; 0,98 sur diff&#233;rents corpus (la topline) avec un lexique exhaustif,</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>S&#63333;&#63335;&#63341;&#63333;&#63342;&#63348;&#63329;&#63348;&#63337;&#63343;&#63342; &#63333;&#63348; &#63337;&#63342;&#63332;&#63349;&#63331;&#63348;&#63337;&#63343;&#63342; &#63332;&#63333; &#63340;&#63333;&#63352;&#63337;&#63345;&#63349;&#63333; &#63342;&#63343;&#63342;-&#63347;&#63349;&#63344;&#63333;&#63346;&#63350;&#63337;&#63347;&#63465;&#63333;&#63347; &#63344;&#63343;&#63349;&#63346; &#63340;&#63333; &#63331;&#63336;&#63337;&#63342;&#63343;&#63337;&#63347; &#63341;&#63329;&#63342;&#63332;&#63329;&#63346;&#63337;&#63342;
</p>
<p>et des scores de 0,72 &#224; 0,88 selon les domaines dans la configuration baseline. Les 18 syst&#232;mes pr&#233;sent&#233;s lors du
segmentation bakeoff ont tous obtenu des r&#233;sultats interm&#233;diaires entre ces deux niveaux. Il faut donc souligner
l'importance pour cette t&#226;che des ressources lexicales.
Parmi ces syst&#232;mes, et en g&#233;n&#233;ral parmi les syst&#232;mes de segmentation du chinois mandarin, les deux principaux
paradigmes d'apprentissage automatique ont &#233;t&#233; utilis&#233;s. Chacun pr&#233;sente des avantages et des inconv&#233;nients.
Les m&#233;thodes supervis&#233;es n&#233;cessitent un corpus d'entra&#238;nement constitu&#233; d'un ensemble de textes d&#233;j&#224; segment&#233;s
(la r&#233;ponse attendue, consid&#233;r&#233;e comme &#171;bonne &#187;). &#192; partir de ce jeu d'exemples, l'algorithme effectue une g&#233;n&#233;ra-
lisation qui lui permet ensuite d'imiter la prise de d&#233;cision effectu&#233;e par l'humain lors de la segmentation manuelle
du corpus. De nombreuses m&#233;thodes d'apprentissage supervis&#233; existent et les syst&#232;mes de segmentation actuels
tendent &#224; les combiner (cf. par exemple celui d&#233;crit par (Wu et al., 2010), tr&#232;s bien class&#233; au dernier segmentation
bakeoff, qui repose sur un &#171; conditional support Markov model &#187;). Les m&#233;thodes supervis&#233;es sont celles qui ob-
tiennent les meilleurs r&#233;sultats, mais elles n&#233;cessitent l'utilisation d'un corpus d'entra&#238;nement dont la construction
est longue et co&#251;teuse. Ce corpus influe sur le comportement des syst&#232;mes qui d&#233;pendent de choix linguistiques
particuliers, ainsi que de la nature du corpus (l'&#233;tat de la langue &#224; une &#233;poque donn&#233;e et pour un domaine donn&#233;).
L'adaptation &#224; d'autres domaines est un enjeu de recherche pour ce type de syst&#232;me.
Lesm&#233;thodes non-supervis&#233;es n'utilisent pas de corpus pr&#233;-annot&#233;mais se contentent d'une grande quantit&#233; de don-
n&#233;es brutes non-segment&#233;es. L'hypoth&#232;se sous-jacente est que les donn&#233;es ne sont pas distribu&#233;es al&#233;atoirement
mais poss&#232;dent une certaine structure que l'on cherche &#224; faire &#233;merger par l'analyse de leur distribution. Parmi les
m&#233;thodes utilis&#233;es pour la segmentation du chinois, on peut citer des approches utilisant l'information mutuelle,
comme dans les travaux pionniers de Sproat et al. (1996), puis plus r&#233;cemment des m&#233;thodes reposant sur l'al-
gorithme Expectation Maximization (Peng &amp; Schuurmans, 2001), ou sur la Minimum Description Length (Hua,
2000). La complexit&#233; contextuelle, inspir&#233;e des hypoth&#232;ses de Harris (Harris, 1955) et utilis&#233;e dans les travaux de
(Tanaka-Ishii &amp; Jin, 2006; Jin &amp; Tanaka-Ishii, 2006) est pr&#233;sent&#233;e plus en d&#233;tail &#224; la section 3. Les m&#233;thodes non
supervis&#233;es pr&#233;sentent l'avantage de s'adapter &#224; un corpus brut peu co&#251;teux et plus facile &#224; obtenir que des corpus
segment&#233;s manuellement. Mais elles sont difficiles &#224; &#233;valuer : il n'existe pas &#224; priori de raison pour que la sortie
d'un tel syst&#232;me corresponde &#224; un guide de segmentation plut&#244;t qu'&#224; un autre.
</p>
<p>2.2 M&#233;thodes d'&#233;valuation des syst&#232;mes de segmentation
</p>
<p>Les diff&#233;rents syst&#232;mes de segmentation sont entra&#238;n&#233;s et &#233;valu&#233;s sur des parties de corpus dits &#171; de r&#233;f&#233;rence &#187;
en utilisant les mesures classiques en apprentissage automatique (rappel, pr&#233;cision, f-mesure sur les formes ou
sur les fronti&#232;res), mais ce mode d'&#233;valuation sous-estime une r&#233;alit&#233; linguistique complexe. Les diff&#233;rents corpus
disponibles segment&#233;s manuellement ne suivent pas les m&#234;mes guides d'annotation. Ainsi le corpus de l'Universit&#233;
de P&#233;kin suit le guide de Yu (1999) tandis que le corpus &#233;quilibr&#233; de l'Academia Sinica suit Huang et al. (1996) et
que le Chinese Treebank respecte les conventions de Xia (2000).
Il a &#233;t&#233; plusieurs fois observ&#233; que le taux d'accord entre locuteurs natifs non linguistes &#224; qui il &#233;tait demand&#233; de
segmenter un texte est assez faible ((Sproat et al., 1996) rapportent 76% de moyenne entre rappel et pr&#233;cision
sur les mots, Jin (2007) rapporte une f-mesure de 0,839) . Ceci peut s'expliquer en partie par le fait que la t&#226;che
de segmentation recouvre diff&#233;rents probl&#232;mes qui ne sont sp&#233;cifiques ni au mandarin ni &#224; l'&#233;criture chinoise : la
d&#233;finition des unit&#233;s lexicales n'est triviale pour aucune langue et Packard (2000) propose 8 d&#233;finitions diff&#233;rentes
du &#171; mot &#187; : il fait remarquer que les crit&#232;res phonologiques, syntaxiques, s&#233;mantiques, sociologiques, et autres
ne co&#239;ncident pas toujours. La question de la segmentation soul&#232;ve en effet des probl&#232;mes relatifs aux expressions
multi-mots, au traitement des entit&#233;s nomm&#233;es et aux ph&#233;nom&#232;nes de figement et de collocation (des exemples
sont donn&#233;s &#224; la section 5.3). Certains d&#233;saccords sur la segmentation rel&#232;vent d'une diff&#233;rence d'analyse morpho-
syntaxique syst&#233;matique et sont explicables, motiv&#233;s et le plus souvent homog&#233;n&#233;isables (Xia, 2000). C'est le cas
par exemple du traitement de la marque du pluriel sur les nom humains ( &#20497; men ) analys&#233;e en tant que suffixe
([N &#20497; ] = une unit&#233;) ou en tant que postposition (N + &#20497; = deux unit&#233;s). Il en va de m&#234;me pour les marques
aspectuelles et r&#233;sultatives sur les verbes. Les d&#233;saccords autour de figements lexicaux sont eux bien plus difficile
&#224; trancher (exemple : &#20840;&#29699;&#26262;&#21270; qu&#225;nqi&#250;nu&#462;nhu&#224; terre-enti&#232;re-chaleur-devenir, r&#233;chauffement plan&#233;taire)
Enfin, dans un contexte applicatif, Wu (2003) note que diff&#233;rentes applications de TAL n&#233;cessitent diff&#233;rents cri-
t&#232;res de segmentation en amont. Dans notre cas, notre objectif premier est la construction de ressources lexicales
&#224; des fins de linguistique exp&#233;rimentale sur corpus. Les contraintes et besoins que cela implique diff&#232;rent donc
l&#233;g&#232;rement des besoins pos&#233;s par la conception d'applications TAL &#224; vis&#233;e plus industrielle.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>P&#63337;&#63333;&#63346;&#63346;&#63333; M&#63329;&#63335;&#63337;&#63347;&#63348;&#63346;&#63353; B&#63333;&#63342;&#63343;&#63470;&#63348; S&#63329;&#63335;&#63343;&#63348;
</p>
<p>2.3 Contexte et motivations
</p>
<p>L'objectif de notre travail est notamment l'induction de lexiques et le pr&#233;-traitement de corpus &#224; des fins d'&#233;tudes
linguistiques. Ces corpus sont susceptibles de manifester une importante variation li&#233;e &#224; trois facteurs au moins :
&#8211; l'espace : au travers des diff&#233;rentes variantes du mandarin pratiqu&#233;es &#224; P&#233;kin, Hong Kong, Singapour ou Ta&#239;wan ;
&#8211; le temps : la publication r&#233;cente des n-grammes de GoogleBooks ouvre de nouvelles possibilit&#233;s pour une
&#233;tude du lexique &#224; la fois diachronique et quantitative, mais pose le probl&#232;me de la segmentation sous un angle
diff&#233;rent ; il est en effet exclu d'utiliser un syst&#232;me entra&#238;n&#233; sur un corpus de la fin du XXe si&#232;cle pour segmenter
des textes bien plus anciens 2 ;
</p>
<p>&#8211; le domaine : des corpus de natures diff&#233;rentes, voire des corpus de sp&#233;cialit&#233;, utiliseront des lexiques diff&#233;rents
et en partie sp&#233;cifiques, significativement diff&#233;rents de ce que l'on peut trouver dans les corpus d'apprentissage
utilis&#233;s par les syst&#232;mes supervis&#233;s.
</p>
<p>Les m&#233;thodes classiques de segmentation et d'&#233;valuation exploitant des lexiques pr&#233;-existants ou des corpus seg-
ment&#233;s manuellement semblent donc peu appropri&#233;es pour nos recherches o&#249; les mots inconnus et la tol&#233;rance &#224;
la variation nous int&#233;ressent particuli&#232;rement. D'un autre c&#244;t&#233;, les analyses propos&#233;es dans des travaux de linguis-
tiques portant sur la d&#233;finition de l'unit&#233; lexicale (Nguyen, 2006; Magistry, 2008) sont difficiles &#224; automatiser.
Cette motivation &#224; la fois linguistique et quantitative a nourri notre int&#233;r&#234;t pour les m&#233;thodes non supervis&#233;es et
particuli&#232;rement celles qui reposent sur l'hypoth&#232;se, motiv&#233;e linguistiquement, de Harris (1955). Un exemple en
est notamment les exp&#233;rimentations men&#233;es sur corpus par Jin &amp; Tanaka-Ishii (2006).
</p>
<p>3 L'hypoth&#232;se harrissienne et sa reformulation entropique
</p>
<p>Dans son article &#171; From phoneme to morpheme &#187;, Harris (1955) formule l'hypoth&#232;se de l'existence d'un lien entre
les fronti&#232;res de morph&#232;mes ou de mots et le nombre de successeurs possibles &#224; une suite de phon&#232;mes dans la
cha&#238;ne parl&#233;e. Il effectue ensuite diff&#233;rentes exp&#233;riences visant &#224; confirmer cette hypoth&#232;se et &#224; pr&#233;ciser la proc&#233;dure
de segmentation.
&#192; l'&#233;poque, ces exp&#233;riences ne pouvaient pas tirer parti de grands volumes de textes ou d'enregistrements et furent
r&#233;alis&#233;es sous forme d'enqu&#234;tes. Plus r&#233;cemment, cette id&#233;e a &#233;t&#233; d&#233;clin&#233;e pour r&#233;aliser diff&#233;rentes t&#226;ches telles que
la d&#233;tection de collocations (Frantzi &amp; Ananiadou, 1996) ou la segmentation du chinois (Jin &amp; Tanaka-Ishii, 2006).
Les exp&#233;riences sur le chinois reposent sur la reformulation de l'hypoth&#232;se de Harris dans le cadre de la th&#233;orie de
l'information propos&#233;e par Tanaka-Ishii (2005), qui repose sur la notion d'entropie (not&#233;e H) : la distribution des
successeurs possibles d'une suite de tokens, mod&#233;lis&#233;e ici par un n-gramme xn (de phon&#232;mes ou de sinogrammes),
permet de d&#233;finir et de calculer une entropie h(xn), dite entropie de branchement, comme suit :
</p>
<p>h(xn) = H(&#967;|xn) = &#8722;
&#8721;
x&#8712;&#967;
</p>
<p>P (x|xn). logP (x|xn),
</p>
<p>o&#249; &#967; est l'ensemble de tous les sinogrammes connus, mais aussi des lettres latines (en raison des expressions ou
noms &#233;trangers) et des chiffres arabes, et P (x|xn) est la probabilit&#233; conditionnelle de trouver le caract&#232;re x &#224; la
suite du n-gramme xn. Cette reformulation a ainsi servi &#224; tester l'hypoth&#232;se de Harris en corpus (Tanaka-Ishii &amp;
Jin, 2006).
Le mod&#232;le de Jin &amp; Tanaka-Ishii (2006), qui est plus proche de l'article de Harris que notre syst&#232;me (d&#233;crit ci-
dessous), est aussi beaucoup plus complexe. Il repose sur cinq mod&#232;les de langue (reposant sur des 1 &#224; 5-grammes
de sinogrammes) utilis&#233;s conjointement. Ceci permet de calculer une entropie de branchement apr&#232;s une s&#233;quence
de longueur variable. Cependant &#224; chaque intervalle entre deux sinogrammes, son syst&#232;me applique une s&#233;rie
de crit&#232;res qui lui permettent de d&#233;cider de fa&#231;on binaire si il faut segmenter ou non. La condition principale
correspondant &#224; l'hypoth&#232;se de Harris est qu'une fronti&#232;re d'unit&#233; linguistique correspond &#224; un point o&#249; l'entropie
branchante atteint un maximum local. L'&#233;criture chinoise produisant de nombreuses occurrences de mot d'un seul
sinogramme, Jin et Tanaka-Ishii consid&#232;rent qu'il existe une fronti&#232;re &#224; chaque point o&#249; l'entropie est croissante.
Leur syst&#232;me est ensuite &#233;valu&#233; de fa&#231;on classique par rappel/pr&#233;cision/f-mesure sur un extrait du corpus segment&#233;
manuellement (celui de l'Universit&#233; de P&#233;kin suivant Yu (1999)).
</p>
<p>2. Les n-grammes de Google ont toutefois &#233;t&#233; extraits apr&#232;s segmentation des textes, mais aucune information n'est donn&#233;e sur la m&#233;thode
utilis&#233;e, ce qui pose probl&#232;me pour l'exploitation de ces donn&#233;es.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>S&#63333;&#63335;&#63341;&#63333;&#63342;&#63348;&#63329;&#63348;&#63337;&#63343;&#63342; &#63333;&#63348; &#63337;&#63342;&#63332;&#63349;&#63331;&#63348;&#63337;&#63343;&#63342; &#63332;&#63333; &#63340;&#63333;&#63352;&#63337;&#63345;&#63349;&#63333; &#63342;&#63343;&#63342;-&#63347;&#63349;&#63344;&#63333;&#63346;&#63350;&#63337;&#63347;&#63465;&#63333;&#63347; &#63344;&#63343;&#63349;&#63346; &#63340;&#63333; &#63331;&#63336;&#63337;&#63342;&#63343;&#63337;&#63347; &#63341;&#63329;&#63342;&#63332;&#63329;&#63346;&#63337;&#63342;
</p>
<p>Notre travail est motiv&#233; par l'id&#233;e que ce type de r&#233;sultat binaire et ce mode d'&#233;valuation ne r&#233;v&#232;le pas tout le
potentiel de l'hypoth&#232;se et des mod&#232;les sous-jacents. Le syst&#232;me est &#233;valu&#233; &#224; chaque point du corpus alors que
des g&#233;n&#233;ralisations pertinentes peuvent &#234;tre obtenues &#224; partir de l'ensemble des mesures de variation d'entropie
effectu&#233;es, m&#234;me bruit&#233;s. Par ailleurs, les mesures de variations d'entropie ont des valeurs continues qui semblent
&#224; m&#234;me de rendre compte plus finement du probl&#232;me linguistique que les modes d'&#233;valuation standard r&#233;duisent &#224;
une t&#226;che de classification binaire.
Dans la section suivante nous pr&#233;sentons un syst&#232;me d'analyse qui conserve l'information sur les variations d'en-
tropie afin de pouvoir utiliser celle-ci pour induire un lexique (section 5) ou corr&#233;ler la variation d'entropie &#224; la
syntaxe sans la discr&#233;tiser (section 6).
</p>
<p>4 Architecture de notre syst&#232;me de segmentation
Le mod&#232;le pr&#233;sent&#233; ci-dessus peut &#234;tre d&#233;clin&#233; en divers syst&#232;mes ayant en commun l'utilisation d'une mesure de
&#171; surprise &#187; pour d&#233;tecter les fronti&#232;res. Contrairement aux travaux de Jin et Tanaka-Ishii, notre objectif n'est pas la
segmentation en elle m&#234;me mais l'induction de lexiques. Il nous est donc possible de ne pas prendre une d&#233;cision
binaire sur la segmentation &#224; chaque intervalle entre deux sinogrammes mais de propager la mesure de &#171; surprise &#187;
&#224; un syst&#232;me qui prend une d&#233;cision sur l'int&#233;gration ou non une suite de sinogrammes donn&#233;e &#224; notre lexique.
Afin d'obtenir des r&#233;sultats plus lisibles, nous avons choisi dans un premier temps d'utiliser un syst&#232;me simplifi&#233;
ne reposant que sur un seul mod&#232;le de langue (4-grammes) qui calcule l'entropie branchante h(x3) &#224; chaque inter-
sinogramme et propage celle-ci pour en observer la variation.
Pour la s&#233;quence &#21488;&#21271;&#24066;&#25919;&#24220;&#26152;&#26085;&#38283;&#26371;&#27770;&#35696;&#8230; T&#225;ib&#283;ish&#236;zh&#232;ngf&#468; zu&#243;r&#236; k&#257;ihu&#236; ju&#233;y&#236;&#8230; (La municipalit&#233; de Taipei
&#224; d&#233;cid&#233; hier en r&#233;union&#8230;), notre cha&#238;ne de traitement produit la sortie suivante :
</p>
<p>&#21488; -4,98 &#21271; 1,11 &#24066; 1,53 &#25919; -4,51 &#24220; 4,77 &#26152; -4,26 &#26085; 1,55 &#38283; -0,06 &#26371; -0,77 &#27770; -0,92 &#35696;
Segmenter lorsque l'entropie est croissante produit donc le d&#233;coupage : &#21488;&#21271; (Taipei) &#24066; (ville) &#25919;&#24220; (gouver-
nement) &#26152;&#26085; (hier) &#38283;&#26371;&#27770;&#35696; (au lieu de &#38283;&#26371;/&#27770;&#35696; tenir une r&#233;union / d&#233;cider).
</p>
<p>5 Induction de lexique : m&#233;thodologie et &#233;valuation comparative
Dans cette section nous cherchons &#224; induire un lexique &#224; partir des informations sur la variations d'entropie et &#224;
d&#233;finir une mesure de confiance dans les unit&#233;s lexicales induites.
Une cha&#238;ne w = c1c2...cn est une unit&#233; lexicale candidate s'il en existe au moins une occurrence wi dont les va-
riations d'entropie inter-sinogrammes sont not&#233;es ewi,0, . . . , ewin avec ewi,k la variation d'entropie apr&#232;s le k-&#232;me
sinogramme de la i-&#232;me occurrence de w qui v&#233;rifie ewi,0 &gt; 0, ewi,n &gt; 0 (l'entropie est croissante avant et apr&#232;s
la cha&#238;ne) et &#8704;k &#8712; [1, n&#8722; 1] , ewik &#8804; 0 (l'entropie est d&#233;croissante ou constante &#224; l'int&#233;rieur de la cha&#238;ne).
Nous avons appliqu&#233; notre segmenteur au corpus de l'Academia Sinica (Chen et al., 1996), qui contient environ
7, 7 millions d'occurrences de 198 236 unit&#233;s lexicales (segment&#233;es manuellement) pour environ 12 millions de
sinogrammes. Nous en avons ainsi extrait 193 714 unit&#233;s lexicales candidates. Pour chaque unit&#233; lexicale candidate,
nous disposons donc d'un ensemble d'occurrences auxquelles sont associ&#233;es des variations d'entropie aux fronti&#232;res
et internes. Le corpus utilis&#233;, qui compte 12 millions de sinogrammes, produit 193 714 unit&#233;s lexicales candidates.
</p>
<p>5.1 M&#233;triques de confiance
</p>
<p>Nous avons d&#233;fini puis compar&#233; diff&#233;rentes m&#233;triques pour filtrer le bruit parmi les unit&#233;s lexicales candidates, qui
combinent de fa&#231;ons diff&#233;rentes leur fr&#233;quence et une mesure de confiance, d&#233;finie ci-dessous. La fr&#233;quence d'une
unit&#233; lexicale candidate w est directement estim&#233;e &#224; partir du nombre d'occurrence de celle-ci dans le corpus, not&#233;
Nocc(w). Pour tirer parti de l'information sur la variation d'entropie, nous d&#233;finissons pour chaque unit&#233; lexicale
candidate une mesure de confiance d&#233;finie &#224; partir des variations d'entropie comme suit :
&#8211; pour chaque occurrencewi dew, on d&#233;finit une confiance locale cwi = min(ewi,o, ewi,n, &#8722;ewi,1, . . . , &#8722;ewi,n&#8722;1) ;
&#8211; on associe &#224; la cha&#238;ne candidate w la confiance (globale) cw = maxNocc(w)i=1 (cwi).</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>P&#63337;&#63333;&#63346;&#63346;&#63333; M&#63329;&#63335;&#63337;&#63347;&#63348;&#63346;&#63353; B&#63333;&#63342;&#63343;&#63470;&#63348; S&#63329;&#63335;&#63343;&#63348;
</p>
<p>En d'autres termes, la confiance accord&#233;e &#224; une occurrence est d&#233;finie par valeur de variation d'entropie la moins
fiable parmi celles ayant abouti &#224; cette segmentation, et la confiance accord&#233;e &#224; une unit&#233; lexicale est &#233;gale &#224; la
confiance de l'occurrence &#224; laquelle on fait le plus confiance.
Diff&#233;rentes combinaisons de l'indice de confiance et du nombre d'occurrence sont alors possibles. Nous avons
retenu les quatre combinaisons suivantes :
fr&#233;quence seule : Nocc(w). Cette m&#233;trique simple pr&#233;sente l'avantage de cibler les unit&#233;s lexicales couvrant le
</p>
<p>plus grand nombre d'occurrences en corpus, mais n'utilise pas l'information sur la variation d'entropie que
nous conservons et se comporte ainsi comme les syst&#232;mes de segmentation binaires.
</p>
<p>produit : cw &#215;Nocc(w). Cette m&#233;trique introduit la mesure de confiance, en lui conf&#233;rant une importance iden-
tique &#224; celle de la fr&#233;quence.
</p>
<p>log : cw &#215; log(Nocc(w)). Cette m&#233;trique diminue l'impact des hautes fr&#233;quences.
confiance seule : cw. Cette m&#233;trique ignore la mesure de fr&#233;quence et n'utilise que les informations extraites du
</p>
<p>mod&#232;le d'entropie.
Ces m&#233;triques sont choisies arbitrairement mais de mani&#232;re &#224; donner une importance croissante &#224; la mesure de
confiance afin d'&#233;valuer sa pertinence.
</p>
<p>5.2 Filtrage et &#233;valuation du lexique de fa&#231;on semi-supervis&#233;e.
</p>
<p>Chacune des m&#233;triques pr&#233;sent&#233;es &#224; la section pr&#233;c&#233;dente permet de d&#233;finir un ordre de confiance sur les entr&#233;es
lexicales, et donc de trier le lexique induit. On peut alors choisir par exemple de ne conserver que les n premi&#232;res
entr&#233;es. Pour un m&#234;me n, chaque m&#233;trique conduit donc &#224; un lexique filtr&#233; distinct. Il reste &#224; choisir la meilleure
m&#233;trique, puis un seuil sur cette m&#233;trique en dessous duquel filtrer le lexique, afin d'obtenir le meilleur compromis
entre couverture et bruit.
Ces choix sont d&#233;licats &#224; effectuer a priori, de m&#234;me que l'&#233;valuation de la qualit&#233; du lexique obtenu. Nous avons
donc commenc&#233; par utiliser notre syst&#232;me dans une configuration semi-supervis&#233;e afin de pouvoir le comparer &#224;
une r&#233;f&#233;rence &#233;tablie manuellement, et comprendre notamment quelle m&#233;trique semble se comporte le mieux.
Afin d'avoir une id&#233;e de la qualit&#233; d'un lexique Li induit (filtr&#233; ou non), nous le comparons au lexique Lm extrait &#224;
partir de la segmentation effectu&#233;e manuellement &#224; l'Academia Sinica. Rappelons qu'il ne s'agit pas d'un standard
mais d'une analyse possible des donn&#233;es, motiv&#233;e linguistiquement, mais qui n'est pas la seule. Pour comparer
les deux lexiques nous avons utilis&#233; l'indice de Jaccard et la f-mesure (qui donnent des r&#233;sultats similaires). En
l'absence de &#171; bonne &#187; ou de &#171;mauvaise &#187; r&#233;ponse, ces indicateurs donnent tout de m&#234;me une id&#233;e de la coh&#233;rence
entre le r&#233;sultat de notre syst&#232;me non-supervis&#233; et une analyse effectu&#233;e manuellement. Ces deux mesures sont
d&#233;finies classiquement comme suit.
</p>
<p>f(Li, Lr) =
2pr
</p>
<p>p+ r
, avec p(Li, Lm) =
</p>
<p>|Li &#8745; Lm|
|Li| et r(Li, Lr) =
</p>
<p>|Li &#8745; Lm|
|Lm| ; jaccard(Li, Lr) =
</p>
<p>|Li &#8745; Lm|
|Li &#8746; Lm|
</p>
<p>Le lexique Lm obtenu &#224; partir de la segmentation manuelle est tri&#233; par nombre d'occurrences (on consid&#232;re que
l'on a une confiance &#233;gale en toutes les entr&#233;es de ce lexique et l'on cherche &#224; d&#233;tecter en priorit&#233; les formes
les plus fr&#233;quentes). Les diff&#233;rentes m&#233;triques d&#233;crites ci-dessus, qui combinent de diverses fa&#231;ons le nombre
d'occurrencesNocc et l'indice de confiance c permettent de moduler l'importance respective de ces deux quantit&#233;s,
depuis l'utilisation du nombre d'occurrence seul jusqu'&#224; l'utilisation du seul indice de confiance.
Pour chacune des quatre m&#233;triques retenues, nous avons calcul&#233; le Jaccard et la f-mesure entre lexique induit filtr&#233;
&#224; diff&#233;rentes valeurs de la m&#233;trique et lexique manuel filtr&#233; &#224; diff&#233;rents niveaux de fr&#233;quence. La figure 1 montre
les r&#233;sultats obtenus avec la f-mesure, en en indiquant les lignes de niveaux. Ceci nous permet de choisir un seuil
en fonction d'un taux d'accord avec la r&#233;f&#233;rence. On observe que la qualit&#233; du tri par la fr&#233;quence se d&#233;grade plus
rapidement que les autres. &#192; l'inverse, l'utilisation du seul indice de confiance ne semble pas accorder suffisamment
d'importance aux formes fr&#233;quentes (le sommet est plus &#233;loign&#233; de l'origine du graphique).
Afin de nous faire une id&#233;e plus pr&#233;cise du contenu des lexiques induits par rapport &#224; celui sous-jacent au corpus
de l'Academia Sinica, nous avons choisi de les filtrer de la fa&#231;on suivante : nous avons choisi le seuil de fa&#231;on &#224;
maximiser la taille du lexique filtr&#233; tout en pr&#233;servant une f-mesure d'au moins 0.6 par comparaison avec le lexique</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>S&#63333;&#63335;&#63341;&#63333;&#63342;&#63348;&#63329;&#63348;&#63337;&#63343;&#63342; &#63333;&#63348; &#63337;&#63342;&#63332;&#63349;&#63331;&#63348;&#63337;&#63343;&#63342; &#63332;&#63333; &#63340;&#63333;&#63352;&#63337;&#63345;&#63349;&#63333; &#63342;&#63343;&#63342;-&#63347;&#63349;&#63344;&#63333;&#63346;&#63350;&#63337;&#63347;&#63465;&#63333;&#63347; &#63344;&#63343;&#63349;&#63346; &#63340;&#63333; &#63331;&#63336;&#63337;&#63342;&#63343;&#63337;&#63347; &#63341;&#63329;&#63342;&#63332;&#63329;&#63346;&#63337;&#63342;
</p>
<p>2e+04 4e+04 6e+04 8e+04 1e+05
</p>
<p>2e
+0
</p>
<p>4
4e
</p>
<p>+0
4
</p>
<p>6e
+0
</p>
<p>4
8e
</p>
<p>+0
4
</p>
<p>1e
+0
</p>
<p>5
</p>
<p>Taille du lexique induit
</p>
<p>Ta
ille
</p>
<p> d
u 
</p>
<p>le
xi
</p>
<p>qu
e 
</p>
<p>de
 r&#233;
</p>
<p>f&#233;
re
</p>
<p>nc
e
</p>
<p>Nocc(w)
</p>
<p>2e+04 4e+04 6e+04 8e+04 1e+05
</p>
<p>2e
+0
</p>
<p>4
4e
</p>
<p>+0
4
</p>
<p>6e
+0
</p>
<p>4
8e
</p>
<p>+0
4
</p>
<p>1e
+0
</p>
<p>5
</p>
<p>Taille du lexique induit
</p>
<p>Ta
ille
</p>
<p> d
u 
</p>
<p>le
xi
</p>
<p>qu
e 
</p>
<p>de
 r&#233;
</p>
<p>f&#233;
re
</p>
<p>nc
e
</p>
<p>c &#215; Nocc(w)
</p>
<p>2e+04 4e+04 6e+04 8e+04 1e+05
</p>
<p>2e
+0
</p>
<p>4
4e
</p>
<p>+0
4
</p>
<p>6e
+0
</p>
<p>4
8e
</p>
<p>+0
4
</p>
<p>1e
+0
</p>
<p>5
</p>
<p>Taille du lexique induit
</p>
<p>Ta
ille
</p>
<p> d
u 
</p>
<p>le
xi
</p>
<p>qu
e 
</p>
<p>de
 r&#233;
</p>
<p>f&#233;
re
</p>
<p>nc
e
</p>
<p>c &#215; log(Nocc(w))
</p>
<p>2e+04 4e+04 6e+04 8e+04 1e+05
</p>
<p>2e
+0
</p>
<p>4
4e
</p>
<p>+0
4
</p>
<p>6e
+0
</p>
<p>4
8e
</p>
<p>+0
4
</p>
<p>1e
+0
</p>
<p>5
</p>
<p>Taille du lexique induit
</p>
<p>Ta
ille
</p>
<p> d
u 
</p>
<p>le
xi
</p>
<p>qu
e 
</p>
<p>de
 r&#233;
</p>
<p>f&#233;
re
</p>
<p>nc
e
</p>
<p>c
</p>
<p>F&#63337;&#63335;&#63349;&#63346;&#63333; 1 &#8211; comparaison des lexiques induits avec diff&#233;rentes mesures de confiance avec le lexique obtenu apr&#232;s
segmentation manuelle (f-mesure)
</p>
<p>mesure de confiance taille formes valid&#233;es occurrences couverture nombres
|L| |L &#8745; Lm| couvertes de caract&#232;res
</p>
<p>lexique manuel Lm 116 844 116 844 7 584 040 100 % 5 861
Nocc(w) 27 500 16 929 6 719 083 89 % 3 421
</p>
<p>c&#215;Nocc(w) 38 000 24 045 6 901 992 91 % 4 016
c&#215; log(Nocc(w)) 38 000 24 571 6 892 594 91 % 4 070
</p>
<p>c 31 000 23 620 6 695 416 88 % 4 097
</p>
<p>T&#63329;&#63330;&#63340;&#63333; 1 &#8211; Comparaison des lexiques induitsLi. Pour chaque mesure de confiance, le lexique est de taille maximale
parmi ceux ayant une f-mesure de 0.6 par rapport au lexique Lm extractible du corpus de l'Academia Sinica.
</p>
<p>manuel Lm (sur les graphiques de la figure 1, le seuil correspond donc &#224; l'abscisse du point le plus &#224; droite de la
ligne &#224; f-mesure de 0.6). Pour chacun des quatre lexiques ainsi extraits, nous avons effectu&#233; les mesures suivantes :
&#8211; nombre d'unit&#233;s lexicales (|Li|) ;
&#8211; nombre d'unit&#233;s lexicales pr&#233;sentes dans la r&#233;f&#233;rence manuelle (|Li &#8745; Lm|)
&#8211; nombre d'occurrences dans le corpus des unit&#233;s lexicales communes (celles de Li &#8745; Lm) ;
&#8211; couverture de Li &#8745; Lm, c'est-&#224;-dire proportion du corpus couverte par les unit&#233;s lexicales communes ;
&#8211; nombre de sinogrammes distincts utilis&#233;s dans le lexique.
Les r&#233;sultats sont donn&#233;es dans le tableau 1, o&#249; nous donnons &#233;galement les valeurs correspondantes pour le lexique
manuelLm. On constate que l'utilisation de notre indice de confiance am&#233;liore bien la proportion de formes valides
tandis que la fr&#233;quence reste une valeur int&#233;ressante pour optimiser la couverture du corpus.
Remarquons que les formes valides captur&#233;es par nos lexiques diff&#232;rent sensiblement. Le tableau 2 donne les indices
de Jaccard entre nos lexiques calcul&#233;es 2 &#224; 2 et confirme l'int&#233;r&#234;t de combiner les deux informations de confiance
et de fr&#233;quence.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>P&#63337;&#63333;&#63346;&#63346;&#63333; M&#63329;&#63335;&#63337;&#63347;&#63348;&#63346;&#63353; B&#63333;&#63342;&#63343;&#63470;&#63348; S&#63329;&#63335;&#63343;&#63348;
</p>
<p>Nocc(w)
</p>
<p>c&#215;Nocc(w) 0,64 c&#215;Nocc(w)
c&#215; log(Nocc(w)) 0,59 0,92 c&#215; log(Nocc(w))
</p>
<p>c 0,46 0,71 0,76
</p>
<p>T&#63329;&#63330;&#63340;&#63333; 2 &#8211; Indices de Jaccard entre les quatre lexiques induits filtr&#233;s, en se restreignant aux unit&#233;s lexicales &#233;gale-
ment pr&#233;sentes dans le lexique manuel.
</p>
<p>type d'erreur quantit&#233; type d'erreur quantit&#233;
suffixation 37 &#233;criture non-chinoise 14
</p>
<p>dates et nombres 35 conjonction 14
verbes 27 adverbes 10
</p>
<p>expressions fig&#233;e 21 entit&#233; nomm&#233;e 4
translitt&#233;ration 2 autre 36
</p>
<p>T&#63329;&#63330;&#63340;&#63333; 3 &#8211; R&#233;partition des faux n&#233;gatifs
</p>
<p>5.3 Analyse d'erreur
</p>
<p>Dans cette section, nous pr&#233;sentons les r&#233;sultats d'un analyse d'erreur, ou de divergence, entre le lexique de r&#233;-
f&#233;rence et le lexique construit &#224; la section pr&#233;c&#233;dente au moyen de la mesure cw &#215; log(Nocc(w)). Nous avons
concentr&#233; notre analyse sur deux axes : les unit&#233;s lexicales de haute fr&#233;quence absentes de notre lexique induit
mais pr&#233;sentes dans le lexique de r&#233;f&#233;rence (faux n&#233;gatifs) et les cha&#238;nes consid&#233;r&#233;es comme des unit&#233;s lexicales
avec un haut niveau de confiance mais absentes du lexique de r&#233;f&#233;rence (faux positifs). Pour chaque groupe, nous
avons consid&#233;r&#233;s les 200 premiers cas.
Le syst&#232;me mis en &#339;uvre pour cet article est volontairement simpliste pour &#233;tablir un syst&#232;me de base auquel se
comparer. Les erreurs observ&#233;es dans cette section sugg&#232;rent diff&#233;rentes pistes d'am&#233;lioration en amont (modifi-
cation sur le mod&#232;le de langue utilis&#233;) et en aval (ajout de r&#232;gles linguistiques bas&#233;es sur les cat&#233;gories ferm&#233;es).
</p>
<p>5.3.1 Analyse des faux n&#233;gatifs
</p>
<p>Nous avons class&#233; les faux n&#233;gatifs suivant leur morphologie lorsque leur construction interne &#233;tait transparente,
dans le cas contraire nous avons les avons class&#233; en parties du discours, mais le mandarin &#233;tant tr&#232;s ambigu sur ce
point (le ph&#233;nom&#232;ne de conversion est fr&#233;quent), de nombreux cas sont rest&#233;s non class&#233;s. Les r&#233;sultats de cette
analyse sont donn&#233;s dans le tableau 3 dont nous d&#233;taillons la moiti&#233; gauche ci-dessous.
Le type d'erreur le plus repr&#233;sent&#233; concerne des unit&#233;s lexicales, essentiellement nominales, construite sur le mo-
d&#232;le base+suffixe. C'est l&#224; un ph&#233;nom&#232;ne de morphologie constructionnelle tr&#232;s productif en mandarin moderne
dont on peut donner l'exemple &#27861;&#21209;&#37096;&#38263; f&#462;w&#249;b&#249;zh&#462;ng ministre de la justice construit sur la base &#27861;&#21209;&#37096; f&#462;w&#249;b&#249;
minist&#232;re de la justice &#224; laquelle on ajoute le suffixe &#38263; zh&#462;ng pour t&#234;te/chef ( &#37096; b&#249; &#233;tant lui m&#234;me un suffixe
indiquant un minist&#232;re) . Dans (Magistry, 2008), des m&#233;thodes quantitatives ont permis d'estimer la productivit&#233;
de ce proc&#233;d&#233; et ont ainsi montr&#233; que les r&#232;gles tr&#232;s productives correspondent aux cas o&#249; le statut morphologique
ou syntaxique de la composition est le plus discutable. C'est aussi un des points sur lesquels les guides de segmen-
tation manuelle peuvent diverger. La grande proportion de ce type d'erreur n'est donc pas &#233;tonnante mais un soin
particulier devra &#234;tre apport&#233; au traitement de ce ph&#233;nom&#232;ne dans des travaux futurs.
Les dates et nombres sont aussi une erreur attendue, la distribution des tokens qui les composent &#233;tant particuli&#232;re.
Le cas des verbes est moins clair. cependant la pr&#233;sence de marques d'aspect (formant une petite classe ferm&#233;e)
directement &#224; la suite du verbe peuvent induire notre syst&#232;me en erreur. Il coupera apr&#232;s le marqueur d'aspect. Il en
va de m&#234;me (mais dans une moindre mesure) des constructions verbe+r&#233;sultatif (ex : &#21507;&#23436; ch&#299;w&#225;n manger-finir,
avoir fini de manger) qui bien que s&#233;cables (ex : &#21507;&#19981;&#23436; ch&#299;buw&#225;n manger-n&#233;gation-finir, ne pas pouvoir finir de
manger) n'ont pas toujours un sens compositionnel. Certaines combinaisons sont lexicalis&#233;es et peuvent donner
lieu &#224; d&#233;bat concernant leur bonne segmentation.
Parmi les 200 premiers nous avons compt&#233; 21 cas qui nous semblent &#234;tre des figements &#224; diff&#233;rents degr&#233;s, comme
&#39640;&#29246;&#22827;-&#29699;&#22580; g&#257;o'&#283;rf&#363;-qi&#250;ch&#462;ng golf-terrain, terrain de golf, l'unit&#233; pour golf &#233;tant autonome et celle pour terrain
pouvant concerner tout type de terrain sport utilisant une balle. Mais ceci inclut aussi des &#171; expressions en quatre</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>S&#63333;&#63335;&#63341;&#63333;&#63342;&#63348;&#63329;&#63348;&#63337;&#63343;&#63342; &#63333;&#63348; &#63337;&#63342;&#63332;&#63349;&#63331;&#63348;&#63337;&#63343;&#63342; &#63332;&#63333; &#63340;&#63333;&#63352;&#63337;&#63345;&#63349;&#63333; &#63342;&#63343;&#63342;-&#63347;&#63349;&#63344;&#63333;&#63346;&#63350;&#63337;&#63347;&#63465;&#63333;&#63347; &#63344;&#63343;&#63349;&#63346; &#63340;&#63333; &#63331;&#63336;&#63337;&#63342;&#63343;&#63337;&#63347; &#63341;&#63329;&#63342;&#63332;&#63329;&#63346;&#63337;&#63342;
</p>
<p>Type d'erreur Qnt Type d'erreur Qnt
nom 17 verbe+DE 8
</p>
<p>adverbe+verbe 15 adverbe+copule 8
suffixe+DE 14 adverbe+adverbe 8
</p>
<p>nombre+classificateur 14 adverbe+avoir 7
verbe+aspect 13 adverbe+auxiliaire 7
</p>
<p>d&#233;monstratif+classificateur 9 pronom+DE 6
</p>
<p>T&#63329;&#63330;&#63340;&#63333; 4 &#8211; R&#233;partition des faux positifs
</p>
<p>caract&#232;res &#187; dont la concision et la structure interne rel&#232;vent d'un &#233;tat ant&#233;rieur de la langue (ex : &#21069;&#25152;&#26410;&#26377; avant/
ce que/pas encore/avoir, sans pr&#233;c&#233;dent). Ce type d'erreurs regroupe ainsi des expressions fig&#233;es dont certaines
sont idiomatiques et d'autres compositionnelles (en quantit&#233;s &#233;quivalentes).
</p>
<p>5.3.2 Analyse des faux positifs
</p>
<p>Nous avons ensuite analys&#233; les faux positifs en observant leur composition interne. Une premi&#232;re remarque est que
sur les 200 premiers, 198 sont des bigrammes et 2 sont des unigrammes (cette pr&#233;f&#233;rence dispara&#238;t &#224; mesure que
la confiance diminue). Nous avons donc classifi&#233; les erreurs en fonction des deux sinogrammes qui les composent.
La dispersion est plus grande, nous ne donnons donc dans le tableau 4 que les types les plus importants.
Les 17 noms sont des unit&#233;s lexicales dont le statut est discutable. 13 comportent en seconde position un &#233;l&#233;ment
appartenant &#224; une classe tr&#232;s ferm&#233;e de &#171;mots &#187; indiquant un lieu ou une direction ( &#19978;,&#19979;,&#20869;,&#35041;,&#20013; sh&#224;ng, xi&#224;, n&#232;i,
l&#464;, zh&#333;ng sur, sous, dans, &#224;, au milieu) et en premi&#232;re position un nom monosyllabique.
Les adverbe+verbe sont des combinaisons d'adverbe monosyllabique (tr&#232;s, le plus, trop, aussi, relativement) et de
verbes d'&#233;tat (rapide, grand, petit, suffisant, nombreux, difficile).
Suffixe+DE d&#233;signe un groupe compos&#233; en premi&#232;re position d'un suffixe nominal tr&#232;s productif (voir plus haut)
et en seconde de &#30340; DE qui marque une relative ou la possession. Il s'agit ici d'une double erreur de segmentation
d&#251;e au fait que de nombreuses bases peuvent commuter devant ces suffixes et qu'il viennent terminer une large
classe de nom et peuvent donc fr&#233;quemment se trouver devant le DE.
Les s&#233;quences nombre+classificateur et d&#233;monstratif+classificateur sont li&#233;es &#224; la structure des groupes nominaux
(non nus) en mandarin dans lesquels un classificateur est requis et doit obligatoirement &#234;tre pr&#233;c&#233;d&#233; d'un &#233;l&#233;ment
d&#233;notant une quantit&#233; ou d'un d&#233;monstratif. Il est donc peu &#233;tonnant que notre syst&#232;me tende &#224; les regrouper.
Remarquons aussi les 6 s&#233;quences pronom+DE qui se traduiraient par des possessifsmon/le mien, ton/le tien, son/
le sien. . . auxquels s'ajoutent deux variantes avec un possesseur non-humain. On ne compte que 6 erreurs de ce
type, mais c'est l&#224; une liste exhaustive des pronoms singuliers + DE.
Une analyse de ces erreurs r&#233;alis&#233;e sur les caract&#232;res et non sur couples de caract&#232;res erron&#233;s montre que toutes
ces erreurs incluent une unit&#233; lexicale qui est un unigramme tr&#232;s fr&#233;quent ou appartenant &#224; une classe tr&#232;s ferm&#233;e.
</p>
<p>5.4 Caract&#233;ristiques g&#233;n&#233;rales du lexique
Chen et al. (1993) fournissent des informations sur la distribution globale d'un lexique du chinois mandarin et des
occurrences en corpus. On peut ainsi v&#233;rifier si notre lexique induit poss&#232;de bien les m&#234;me propri&#233;t&#233;s que le lexique
obtenu manuellement.
Tout d'abord, en observant le nombre d'occurrence des unit&#233;s lexicale ordonn&#233;es par fr&#233;quence d&#233;croissante, on
obtient une courbe zipfienne qui se superpose bien avec celle obtenue en utilisant le lexique manuel.
Les observations plus sp&#233;cifiques au mandarin concernent la r&#233;partition des unit&#233;s lexicales et de leurs occurrences
en fonction de leur longueur en nombre de sinogrammes. Sur les 38 000 unit&#233;s lexicales &#171; de confiance &#187; (ou les
plus fr&#233;quentes pour le lexique manuel), le lexique induit est bien constitu&#233; principalement de bigrammes et de
trigrammes (respectivement 65,8% et 27,3%) tandis que les unigrames constituent 6,5% du lexique. Le lexique
manuel compte lui 7% d'unigrammes, 67,7% de bigrammes et 19% de trigrammes. Concernant le nombre d'oc-
currence observ&#233;es dans le corpus, on obtient 37% d'unigramme, 54% de bigrammes et 7% de trigrammes (pour
le corpus segment&#233; manuellement, on obtient respectivement 45%, 49% et 4%)</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>P&#63337;&#63333;&#63346;&#63346;&#63333; M&#63329;&#63335;&#63337;&#63347;&#63348;&#63346;&#63353; B&#63333;&#63342;&#63343;&#63470;&#63348; S&#63329;&#63335;&#63343;&#63348;
</p>
<p> 0.0001
</p>
<p> 0.001
</p>
<p> 0.01
</p>
<p> 0.1
</p>
<p> 1
</p>
<p>Adj Adv Classif Conj DE Det Interj Nbre Nom ParticPostp Prep Pro Verbe YOU
</p>
<p>Pr
op
</p>
<p>or
tio
</p>
<p>n 
(&#233;c
</p>
<p>he
lle
</p>
<p> lo
ga
</p>
<p>rith
mi
</p>
<p>qu
e)
</p>
<p>Cat&#233;gorie principale dans le corpus
</p>
<p>R&#233;partition globale des occurrences
R&#233;partition des occurrences couvertes par le lexique induit
</p>
<p> 0.0001
</p>
<p> 0.001
</p>
<p> 0.01
</p>
<p> 0.1
</p>
<p> 1
</p>
<p>Adj Adv Classif Conj DE Det Interj Nbre Nom ParticPostp Prep Pro Verbe YOU
</p>
<p>Pr
op
</p>
<p>or
tio
</p>
<p>n 
(&#233;c
</p>
<p>he
lle
</p>
<p> lo
ga
</p>
<p>rith
mi
</p>
<p>qu
e)
</p>
<p>Cat&#233;gorie principale dans le corpus
</p>
<p>R&#233;partition globale des unit&#233;s lexicales
R&#233;partition des unit&#233;s lexicales dans le lexique induit
</p>
<p>(a) (b)
</p>
<p>F&#63337;&#63335;&#63349;&#63346;&#63333; 2 &#8211; Comparaison des r&#233;partitions en cat&#233;gories entre les occurrences de formes segment&#233;es &#224; l'identique
par notre syst&#232;me et par le corpus lui-m&#234;me (a), et entre les unit&#233;s lexicales correspondantes (b).
</p>
<p>6 &#201;valuation de la segmentation par comparaison avec les informations
morphosyntaxiques et syntaxiques
</p>
<p>Apr&#232;s avoir &#233;valu&#233; notre mod&#232;le de segmentation au travers du lexique qu'elle permet d'extraire du corpus de
l'Academia Sinica, nous avons poursuivi nos exp&#233;riences d'&#233;valuation en cherchant &#224; tirer parti des annotations
morphosyntaxiques et syntaxiques que fournit le Sinica Treebank (Chen et al., 1996), qui en couvre une partie.
</p>
<p>6.1 R&#233;partition en cat&#233;gories
</p>
<p>L'&#233;tude de la distribution du lexique induitLi en fonction de sa fr&#233;quence, bien qu'importante, ne suffit pas &#224;montrer
que ses propri&#233;t&#233;s sont coh&#233;rentes avec celles du lexique motiv&#233; linguistiquement Lm qui est sous-jacent au corpus
de l'Academia Sinica. Nous avons donc cherch&#233; &#224; comparer la r&#233;partition en cat&#233;gories de ces deux lexiques.
Nous avons donc identifi&#233; dans le treebank de l'Academia Sinica les occurrences de formes communes entre la
segmentation du corpus et celle de notre syst&#232;me. Nous avons alors compar&#233; la r&#233;partition de ces occurrences en
parties du discours par rapport &#224; celle de l'ensemble des occurrences de formes dans le corpus de l'Academia Sinica
(figure 2a). Nous avons effectu&#233; la m&#234;me chose avec unit&#233;s lexicales correspondantes (figure 2b). Les cat&#233;gories
que nous avons utilis&#233;es sont les suivantes : Adj (adjectif), Adv (adverbe), Classif (classifieur), Conj (conjonction),
DE (particules &#30340;,&#20043;,&#24471; et &#22320; ), Det (d&#233;terminant), Interj (interjection), Nbre, Nom, Partic (particule), Postp
(postposition), Prep (pr&#233;position), Pro (pronom), Verbe et YOU (verbe &#26377; , avoir).
Nous discuterons de fa&#231;on plus d&#233;taill&#233;e ces figures &#224; la section suivante, mais on constate globalement une bonne
corr&#233;lation entre les deux r&#233;partitions, tant pour les cat&#233;gories les plus fr&#233;quentes que pour celles qui le sont moins.
</p>
<p>6.2 Corr&#233;lation entre variation d'entropie et structure syntaxique
</p>
<p>Notre mod&#232;le de segmentation reposant sur les variations d'entropie, il ne produit pas simplement pour chaque
paire de sinogrammes adjacents une d&#233;cision binaire (segmenter ou non), mais bien une mesure quantitative de
la s&#233;parabilit&#233; des deux sinogrammes concern&#233;s. Nous avons cherch&#233; &#224; confronter ce degr&#233; de s&#233;paration lin&#233;aire
Sl avec les informations syntaxiques (arbres en constituants) fournies par le Sinica Treebank. L'intuition sous-
jacente est que l'on pourrait constater une corr&#233;lation entre la s&#233;parabilit&#233; lin&#233;aire produite par notre mod&#232;le de
segmentation et un degr&#233; de s&#233;paration syntaxique, mesure qui serait d'autant plus &#233;lev&#233;e que les deux sinogrammes
&#233;tudi&#233;s appartiennent &#224; des unit&#233;s lexicales &#233;loign&#233;es l'une de l'autre au sein de la structure en constituants.
Pour effectuer cette exp&#233;rience, nous avons d&#233;fini le degr&#233; de s&#233;paration syntaxique Ss entre deux unit&#233;s lexicales
adjacentes comme &#233;tant la longueur (en arcs) du plus court chemin permettant de les relier entre elles dans l'arbre
de constituance. Il r&#233;sulte de cette d&#233;finition que Ss est n&#233;cessairement au moins &#233;gal &#224; 2, ce qui est le cas lorsque</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>S&#63333;&#63335;&#63341;&#63333;&#63342;&#63348;&#63329;&#63348;&#63337;&#63343;&#63342; &#63333;&#63348; &#63337;&#63342;&#63332;&#63349;&#63331;&#63348;&#63337;&#63343;&#63342; &#63332;&#63333; &#63340;&#63333;&#63352;&#63337;&#63345;&#63349;&#63333; &#63342;&#63343;&#63342;-&#63347;&#63349;&#63344;&#63333;&#63346;&#63350;&#63337;&#63347;&#63465;&#63333;&#63347; &#63344;&#63343;&#63349;&#63346; &#63340;&#63333; &#63331;&#63336;&#63337;&#63342;&#63343;&#63337;&#63347; &#63341;&#63329;&#63342;&#63332;&#63329;&#63346;&#63337;&#63342;
</p>
<p> 0
</p>
<p> 0.5
</p>
<p> 1
</p>
<p> 1.5
</p>
<p> 2
</p>
<p> 2.5
</p>
<p> 3
</p>
<p> 3.5
</p>
<p> 4
</p>
<p> 2  3  4  5  6  7  8
</p>
<p>D
eg
</p>
<p>r&#233;
 d
</p>
<p>e 
s&#233;
</p>
<p>pa
ra
</p>
<p>tio
n 
</p>
<p>lin
&#233;a
</p>
<p>ire
 (v
</p>
<p>ari
ati
</p>
<p>on
 d&#8217;
</p>
<p>en
tro
</p>
<p>pie
)
</p>
<p>Degr&#233; de separation syntaxique
</p>
<p>S&#233;p&#233;ration lin&#233;aire m&#233;diane
S&#233;paration lin&#233;aire (70&#232;me quantile)
</p>
<p>F&#63337;&#63335;&#63349;&#63346;&#63333; 3 &#8211; &#201;volution de la s&#233;paration lin&#233;aire Sl (variation d'entropie) en fonction de la s&#233;paration syntaxique Ss
sur les fronti&#232;res communes au corpus de l'Academia Sinica et &#224; la segmentation induite par notre syst&#232;me.
</p>
<p>les unit&#233;s lexicales ont un m&#234;me n&#339;ud p&#232;re.
La figure 3 montre pour chaque valeur de la s&#233;paration syntaxique Ss quelle est la valeur m&#233;diane de la s&#233;paration
lin&#233;aire Sl, ainsi que son soixante-dixi&#232;me quantile. On constate deux choses. Tout d'abord, lorsque le degr&#233; de
s&#233;paration syntaxique est de 4 ou moins, il y a une nette corr&#233;lation entre Sl et Ss. Autrement dit, le mod&#232;le
de segmentation utilis&#233; r&#233;ussit &#224; capturer une partie des informations syntaxiques locales, de niveau terme voire
chunk. En revanche, au-del&#224; d'une s&#233;paration syntaxique de 4, la s&#233;paration lin&#233;aire m&#233;diane n'&#233;volue quasiment
plus. Deux hypoth&#232;ses, non-exclusives l'une de l'autre, viennent &#224; l'esprit. Tout d'abord, le mod&#232;le utilis&#233; est 4-
gramme, et il est difficile de capturer des fronti&#232;res entre longs constituants avec un mod&#232;le local de ce type. Par
ailleurs, le mod&#232;le simple que nous utilisons, inspir&#233; de Harris, est un mod&#232;le tr&#232;s surfacique qui n'a aucune raison
de pouvoir capturer des informations sur la macro-structure de l'arbre syntaxique d'une phrase.
Ce r&#233;sultat, bien qu'obtenu &#224; l'&#233;chelle de tout le corpus aumoyen d'un calcul dem&#233;diane, est n&#233;anmoins prometteur :
il ne semble pas exclu de pouvoir utiliser notre mod&#232;le de segmentation non-supervis&#233;, tel quel ou sous une forme
raffin&#233;e, non seulement pour induire une segmentation en unit&#233;s lexicales et un lexique associ&#233; mais &#233;galement
pour identifier des collocations, termes, locutions et autres unit&#233;s lexicales complexes, et de tenter de leur associer
une structure interne. On peut ainsi esp&#233;rer avoir acc&#232;s &#224; un moyen objectif, qui n'utilise pas de connaissance a
priori et qui est donc ind&#233;pendant de la langue, pour mettre en &#233;vidence le continuum qui relie les unit&#233;s lexicales
les plus classiques aux expressions semi-compositionnelles ou collocationnelles.
</p>
<p>7 Conclusion et perspectives
</p>
<p>Dans cet article, nous montrons sur le chinois mandarin qu'un mod&#232;le simple utilisant une hypoth&#232;se linguisti-
quement motiv&#233;e mais ind&#233;pendante de toute connaissance a priori sur une langue particuli&#232;re donne des r&#233;sultats
prometteurs pour la segmentation non supervis&#233;e de textes et l'induction d'unit&#233;s lexicales coh&#233;rentes avec des an-
notations de niveau syntaxique. De plus, certains r&#233;sultats pouvant appara&#238;tre comme des erreurs de segmentation
sont susceptible de questionner de fa&#231;on constructive des analyses linguistiques traditionnelles parfois influenc&#233;es
par les &#233;tats ant&#233;rieurs de la langue.
Certaines erreurs r&#233;sultent toutefois des limites de notre syst&#232;me dans son &#233;tat actuel. En particulier, un traitement
plus fin des cat&#233;gories ferm&#233;es (d&#233;monstratifs, DE,. . .) pourrait nettement am&#233;liorer les r&#233;sultats tout en demandant
une quantit&#233; d'analyse born&#233;e par la taille de ces cat&#233;gories.Mais d'autres am&#233;liorations destin&#233;es &#224; rendre lemod&#232;le
plus proche de consid&#233;rations linguistiques pourront &#233;galement &#234;tre test&#233;es. Le mod&#232;le lui-m&#234;me peut &#233;galement
faire l'objet de raffinements, par exemple pour comprendre si la prise en compte du mot situ&#233; &#224; droite de l'inter-
sinogramme consid&#233;r&#233; est de nature a am&#233;liorer les r&#233;sultats.
Nous pr&#233;voyons de tester notre syst&#232;me sur des corpus relevant de diff&#233;rentes vari&#233;t&#233;s du chinois mandarin, pour
en &#233;tudier notamment les variations des distributions lexicales, mais &#233;galement de le tester sur d'autres langues
non-segment&#233;es, pour valider l'approche sur un &#233;chantillon plus large de langues. Nous souhaitons &#233;galement me-</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>P&#63337;&#63333;&#63346;&#63346;&#63333; M&#63329;&#63335;&#63337;&#63347;&#63348;&#63346;&#63353; B&#63333;&#63342;&#63343;&#63470;&#63348; S&#63329;&#63335;&#63343;&#63348;
</p>
<p>ner des exp&#233;rimentations sur diverses langues, y compris le fran&#231;ais, pour segmenter non seulement des flux de
sinogrammes mais ainsi, par exemple, des flux de phon&#232;mes (en vue d'une segmentation en morph&#232;mes) ou de
tokens (en vue de l'identification d'unit&#233;s lexicales multi-mots et de termes).
</p>
<p>R&#233;f&#233;rences
C&#63336;&#63333;&#63342; C. Y., T&#63347;&#63333;&#63342;&#63335; S. F., H&#63349;&#63329;&#63342;&#63335; C. R. &amp; C&#63336;&#63333;&#63342; K. J. (1993). Some distributional properties of Mandarin Chinese
&#8212; A study based on the academia sinica corpus. In Proceedings of Pacific Asia Conference on Formal and
Computational Linguistics I, p. 81&#8211;95.
C&#63336;&#63333;&#63342; K. J., H&#63349;&#63329;&#63342;&#63335; C. R., C&#63336;&#63329;&#63342;&#63335; L. P. &amp; H&#63347;&#63349; H. L. (1996). Sinica corpus : Design methodology for balanced
corpora. In Proceedings of PACLIC 11th Conference, p. 167--176.
F&#63346;&#63329;&#63342;&#63348;&#63354;&#63337; K. T. &amp; A&#63342;&#63329;&#63342;&#63337;&#63329;&#63332;&#63343;&#63349; S. (1996). Extracting nested collocations. In Proceedings of the 16th conference
on Computational linguistics-Volume 1, p. 41&#8211;46.
H&#63329;&#63346;&#63346;&#63337;&#63347; Z. S. (1955). From phoneme to morpheme. Language, 31(2), 190&#8211;222.
H&#63349;&#63329; Y. (2000). Unsupervised word induction using MDL criterion. In Proceedings of ISCSL.
H&#63349;&#63329;&#63342;&#63335; C. R., C&#63336;&#63333;&#63342; K. J. &amp; C&#63336;&#63329;&#63342;&#63335; L. L. (1996). Segmentation standard for chinese natural language processing.
In Proceedings of the 16th conference on Computational linguistics-Volume 2, p. 1045&#8211;1048.
J&#63337;&#63342; Z. (2007). A Study on Unsupervised Segmentation of Text Using Contextual Complexity. PhD thesis, Uni-
versity of Tokyo, Graduate School of Information Science and Technology, Tokyo, Japon.
J&#63337;&#63342; Z. &amp; T&#63329;&#63342;&#63329;&#63339;&#63329;-I&#63347;&#63336;&#63337;&#63337; K. (2006). Unsupervised segmentation of chinese text by use of branching entropy. In
Proceedings of the COLING/ACL on Main conference poster sessions, p. 428&#8211;435.
M&#63329;&#63335;&#63337;&#63347;&#63348;&#63346;&#63353; P. (2008). Productivit&#233; morphologique : &#201;tude sur le chinois mandarin. Master's thesis, Universit&#233;
Paris Diderot, UFR de Linguistique, Paris, France.
N&#63335;&#63349;&#63353;&#63333;&#63342; &#65535;. (2006). Unit&#233; lexicale et morphologie en chinois mandarin. PhD thesis, Universit&#233; de Montr&#233;al,
Montr&#233;al.
P&#63329;&#63331;&#63339;&#63329;&#63346;&#63332; J. L. (2000). The morphology of Chinese : A linguistic and cognitive approach. Cambridge Univ Pr.
P&#63333;&#63342;&#63335; F. &amp; S&#63331;&#63336;&#63349;&#63349;&#63346;&#63341;&#63329;&#63342;&#63347; D. (2001). Self-supervised chinese word segmentation. Advances in Intelligent Data
Analysis, p. 238&#8211;247.
S&#63333;&#63342;&#63335; S., B&#63337;&#63335;&#63337; B., B&#63333;&#63347;&#63329;&#63331;&#63337;&#63333;&#63346; L. &amp; C&#63329;&#63347;&#63348;&#63333;&#63340;&#63340;&#63337; E. (2009). Segmentation multiple d'un flux de donn&#233;es textuelles pour
la mod&#233;lisation statistique du langage. In Actes de la conf&#233;rence TALN 2009, Senlis, France.
S&#63344;&#63346;&#63343;&#63329;&#63348; R., G&#63329;&#63340;&#63333; W., S&#63336;&#63337;&#63336; C. &amp; C&#63336;&#63329;&#63342;&#63335; N. (1996). A stochastic finite-state word-segmentation algorithm for
chinese. Computational linguistics, 22(3), 377&#8211;404.
T&#63329;&#63342;&#63329;&#63339;&#63329;-I&#63347;&#63336;&#63337;&#63337; K. (2005). Entropy as an indicator of context boundaries : An experiment using a web search
engine. Natural Language Processing&#8211;IJCNLP 2005, p. 93&#8211;105.
T&#63329;&#63342;&#63329;&#63339;&#63329;-I&#63347;&#63336;&#63337;&#63337; K. &amp; J&#63337;&#63342; Z. (2006). From phoneme to morpheme : Another verification using a corpus. Computer
Processing of Oriental Languages. Beyond the Orient : The Research Challenges Ahead, p. 234&#8211;244.
W&#63349; A. (2003). Customizable segmentation of morphologically derived words in chinese. International Journal
of Computational Linguistics and Chinese Language Processing, 8(1), 1&#8211;27.
W&#63349; L.-C. (2010). Outils de segmentation du chinois et textome&#769;trie. In Actes de la conf&#233;rence TALN 2010,
Montr&#233;al, Canada.
W&#63349; Y. C., Y&#63329;&#63342;&#63335; J. C. &amp; L&#63333;&#63333; Y. S. (2010). Chinese word segmentation with conditional support vector inspired
markov models. In Proceedings of the Joint Conference on Chinese Language Processing.
X&#63337;&#63329; F. (2000). The segmentation guidelines for the penn chinese treebank (3.0). IRCS Technical Reports Series.
X&#63349;&#63333; N. (2003). Chinese word segmentation as character tagging. International Jourrnal of Computational Lin-
guistics and Chinese Language Processing.
Y&#63349; S. (1999). Guidelines for the annotation of contemporary chinese texts : word segmentation and POS-tagging.
Institute of Computational Linguistics, Beijing University, Beijing.
H. Z&#63336;&#63329;&#63343; &amp; Q. L&#63337;&#63349;, Eds. (2010). The CIPS-SIGHAN CLP 2010 Chinese Word Segmentation Bakeoff.
Z&#63336;&#63337;&#63339;&#63343;&#63350; V., T&#63329;&#63339;&#63329;&#63341;&#63349;&#63346;&#63329; H. &amp; O&#63339;&#63349;&#63341;&#63349;&#63346;&#63329; M. (2010). An efficient algorithm for unsupervised word segmentation
with branching entropy and MDL. In Proceedings of the 2010 Conference on Empirical Methods in Natural
Language Processing, p. 832&#8211;842.</p>

</div></div>
</body></html>