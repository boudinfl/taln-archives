<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
"http://www.w3.org/TR/html4/loose.dtd">
<html><head><title>Structure des trigrammes inconnus et lissage par analogie</title>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
</head>
<body>
<div style="page-break-before:always; page-break-after:always"><div><p>TALN 2011, Montpellier, 27 juin &#8211;1er juillet 2011
</p>
<p>Structure des trigrammes inconnus et lissage par analogie
</p>
<p>Julien Gosme1 Yves Lepage2
(1) GREYC, universit&#233; de Caen Basse-Normandie, France
</p>
<p>Julien.Gosme@unicaen.fr
(2) IPS, universit&#233; Waseda, Japon
</p>
<p>Yves.Lepage@aoni.waseda.jp
</p>
<p>R&#233;sum&#233;. Nous montrons dans une s&#233;rie d&#8217;exp&#233;riences sur quatre langues, sur des &#233;chantillons du corpus
Europarl, que, dans leur grande majorit&#233;, les trigrammes inconnus d&#8217;un jeu de test peuvent &#234;tre reconstruits par
analogie avec des trigrammes hapax du corpus d&#8217;entra&#238;nement. De ce r&#233;sultat, nous d&#233;rivons une m&#233;thode de
lissage simple pour les mod&#232;les de langue par trigrammes et obtenons de meilleurs r&#233;sultats que les lissages de
Witten-Bell, Good-Turing et Kneser-Ney dans des exp&#233;riences men&#233;es en onze langues sur la partie commune
d&#8217;Europarl, sauf pour le finnois et, dans une moindre mesure, le fran&#231;ais.
</p>
<p>Abstract. In a series of experiments in four languages on subparts of the Europarl corpus, we show that
a large number of unseen trigrams can be reconstructed by proportional analogy using only hapax trigrams. We
derive a simple smoothing scheme from this empirical result and show that it outperforms Witten-Bell, Good-
Turing and Kneser-Ney smoothing schemes on trigram models built on the common part of the Europarl corpus,
in all 11 languages except Finnish and French.
</p>
<p>Mots-cl&#233;s : analogie, trigrammes inconnus, trigrammes hapax, mod&#232;le de langue trigrammes, Europarl.
</p>
<p>Keywords: proportional analogy, unseen trigrams, hapax trigrams, trigram language models, Europarl.
</p>
<p>1 Introduction
</p>
<p>Les techniques de lissage de mod&#232;les de langue reposent habituellement sur des hypoth&#232;ses purement statistiques
pour estimer la probabilit&#233; des &#233;v&#232;nements inconnus. Il y a dix ans, (Rosenfeld, 2000) constatait que :
</p>
<p>Ironically, the most successful SLM techniques use very little knowledge of what language really
is. The most popular language models (n-grams) take no advantage of the fact that what is being
modeled is language.
</p>
<p>Nous pr&#233;sentons ici une technique de lissage pour les mod&#232;les de langue trigrammes qui repose sur la structure
des &#233;v&#232;nements inconnus, c&#8217;est-&#224;-dire la mani&#232;re dont les trigrammes inconnus peuvent &#234;tre construits &#224; partir des
trigrammes connus en utilisant une op&#233;ration structurelle linguistiquement justifi&#233;e, l&#8217;analogie.
</p>
<p>Le but du lissage des mod&#232;les de langue est d&#8217;attribuer des probabilit&#233;s non-nulles aux &#233;v&#232;nements inconnus.
Habituellement, les probabilit&#233;s attribu&#233;es d&#233;pendent d&#8217;une caract&#233;risation th&#233;orique des &#233;v&#233;nements inconnus.
L&#8217;hypoth&#232;se &#224; l&#8217;origine de ce travail est que les trigrammes inconnus peuvent &#234;tre caract&#233;ris&#233;s, dans une large
mesure, par la similitude de leurs structures avec des trigrammes rares. Plus pr&#233;cis&#233;ment nous montrons ci-dessous
que, dans une large mesure, les trigrammes inconnus sont analogues aux trigrammes hapax.
</p>
<p>En guise d&#8217;illustration, dans une de nos exp&#233;riences pr&#233;liminaires, le trigramme de mots opportunit&#233; de servir
&#233;tait un trigramme de notre jeu de test absent du corpus d&#8217;entra&#238;nement. Il se trouvait que ce trigramme pouvait
&#234;tre reconstruit par analogie &#224; l&#8217;aide de trois trigrammes du corpus d&#8217;entra&#238;nement de la mani&#232;re suivante :
</p>
<p>opportunit&#233; de servir : opportunit&#233; de modifier :: qui pourrait servir : qui pourrait modifier
</p>
<p>La ligne pr&#233;c&#233;dente se lit ainsi : le trigramme inconnu opportunit&#233; de servir est au trigramme connu opportunit&#233;
de modifier ce qu&#8217;un autre trigramme connu, qui pourrait servir, est &#224; un dernier trigramme connu, qui pourrait</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>JULIEN GOSME ET YVES LEPAGE
</p>
<p>modifier. Les diff&#233;rents &#233;l&#233;ments du trigramme inconnu sont obtenus par similarit&#233; avec le second et le troisi&#232;me
trigrammes (opportunit&#233; de et servir) et peuvent &#234;tre assembl&#233;s par diff&#233;rence avec le quatri&#232;me trigramme (mots
barr&#233;s). En plus de permettre la reconstruction, les trois trigrammes ci-dessus &#233;taient tous hapax dans le corpus
d&#8217;entra&#238;nement.
</p>
<p>La relation, telle celle donn&#233;e ci-dessus entre trigrammes de mots, qui &#233;nonce qu&#8217;A est &#224; B ce que C est &#224; D
est appel&#233;e analogie. Un certain nombre de travaux en traitement automatique des langues exploitent l&#8217;analogie.
Nous n&#8217;en citons que quelques-uns ici. Par exemple, sur des t&#226;ches de segmentation morphologique, (Lavall&#233;e
&amp; Langlais, 2010) ont r&#233;cemment obtenu d&#8217;excellents r&#233;sultats dans la d&#233;coupe des mots par analogie. (Claveau
&amp; L&#8217;Homme, 2005), entre autres auteurs, avaient auparavant &#233;tudi&#233;, en faisant usage de l&#8217;analogie, dans quelle
mesure la similarit&#233; liait la forme et le sens des mots : connector : to connect :: editor : to edit. En plus des
analogies entre mots eux-m&#234;mes, (Hathout, 2009) a r&#233;cemment exploit&#233; les analogies entre d&#233;finitions extraites
du TLFi pour construire automatiquement des familles de mots li&#233;s par la forme et le sens. Dans le m&#234;me ordre
d&#8217;id&#233;e, (Langlais et al., 2008) avaient propos&#233; d&#8217;utiliser l&#8217;analogie pour forger de nouvelles &#233;quivalences termino-
logiques dans le domaine m&#233;dical &#224; cheval sur deux langues. Sur le seul plan s&#233;mantique, (Turney, 2008) a quant
&#224; lui pr&#233;sent&#233; une approche g&#233;n&#233;rale au probl&#232;me de l&#8217;association entre mots utilisant l&#8217;analogie entre vecteurs
contextuels : mason : stone :: carpenter : wood, approche qu&#8217;il pr&#233;tend g&#233;n&#233;ralisable aux relations de synonymie
et d&#8217;antonymie. (Lepage &amp; Denoual, 2005) quant &#224; eux ont con&#231;u un syst&#232;me de traduction automatique enti&#232;re-
ment fond&#233; sur l&#8217;analogie. Dans le cadre de la traduction automatique aussi, (Denoual, 2007) et (Langlais &amp; Patry,
2007) ont montr&#233; la possibilit&#233; de traduire certains mots inconnus par analogie.
</p>
<p>La d&#233;finition de l&#8217;analogie que nous utilisons dans ce travail est d&#233;taill&#233;e et justifi&#233;e dans (Lepage, 2004). Nous
l&#8217;appliquons aux trigrammes de mots. Un quadruplet de trigrammes de motsA,B,C etD est une analogie lorsque
les contraintes suivantes sont v&#233;rifi&#233;es :&#63729;&#63730;&#63731; d(A,B) = d(C,D)d(A,C) = d(B,D)|A|m &#8722; |B|m = |C|m &#8722; |D|m,&#8704;m
Ici, d est la distance d&#8217;&#233;dition qui compte le nombre minimum d&#8217;insertions et de suppressions de mots n&#233;cessaires
&#224; la transformation d&#8217;un trigramme en un autre. 1 |A|m est le nombre d&#8217;occurrences du mot m dans le trigramme
A. En reprenant l&#8217;exemple pr&#233;c&#233;dent :
</p>
<p>A = opportunit&#233; de servir
B = opportunit&#233; de modifier
C = qui pourrait servir
D = qui pourrait modifier
</p>
<p>on peut v&#233;rifier que d(A,B) = d(C,D) = 2 et d(A,C) = d(B,D) = 4. La relation entre nombres d&#8217;occurrences
est v&#233;rifi&#233;e pour chaque mot :
</p>
<p>mot m |A|m&#8722;|B|m= |C|m&#8722;|D|m
opportunit&#233; 1 &#8722; 1 = 0 &#8722; 0
de 1 &#8722; 1 = 0 &#8722; 0
servir 1 &#8722; 0 = 1 &#8722; 0
modifier 0 &#8722; 1 = 0 &#8722; 1
qui 0 &#8722; 0 = 1 &#8722; 1
pourrait 0 &#8722; 0 = 1 &#8722; 1
</p>
<p>Le bon sens veut que les trigrammes inconnus apparaissant dans un jeu de test qui peuvent &#234;tre reconstruits par
analogie avec des trigrammes d&#8217;un corpus d&#8217;entra&#238;nement soient consid&#233;r&#233;s plus s&#251;rs que ceux qui ne peuvent pas
l&#8217;&#234;tre. Une technique de lissage bas&#233;e sur de simples d&#233;comptes devrait donc donner une plus forte r&#233;-estimation
aux trigrammes pouvant &#234;tre reconstruits par analogie et une plus faible r&#233;-estimation aux autres. Si, en plus, les
trigrammes reconstruits peuvent l&#8217;&#234;tre &#224; l&#8217;aide de trigrammes hapax, la r&#233;-estimation de leur effectif devrait &#234;tre
proche de 1 puisqu&#8217;ils sont alors proches structurellement des trigrammes hapax.
</p>
<p>1. La distance d&#8217;&#233;dition de Levenshtein (Levenshtein, 1966) prend en compte la substitution comme op&#233;ration d&#8217;&#233;dition suppl&#233;mentaire.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>STRUCTURE DES TRIGRAMMES INCONNUS ET LISSAGE PAR ANALOGIE
</p>
<p>La suite de l&#8217;article est divis&#233;e en deux parties : la section 2 est consacr&#233;e &#224; v&#233;rifier l&#8217;hypoth&#232;se que les trigrammes
inconnus sont structurellement analogues aux trigrammes hapax. Des exp&#233;riences successives men&#233;es sur quatre
langues europ&#233;ennes confirment, les unes apr&#232;s les autres, cette hypoth&#232;se. La section 3 pr&#233;sente alors une tech-
nique de lissage reposant sur cette propri&#233;t&#233;, et directement inspir&#233;e des techniques &#233;l&#233;mentaires de Lidstone et
de Laplace. Des comparaisons effectu&#233;es avec quatre autres techniques de lissage classiques sur onze langues
europ&#233;ennes montrent son efficacit&#233;, voire sa sup&#233;riorit&#233;.
</p>
<p>2 La structure des trigrammes inconnus d&#8217;un jeu de test
</p>
<p>Nous menons des exp&#233;riences sur les quatre langues suivantes : l&#8217;anglais, le fran&#231;ais, l&#8217;allemand et le finnois. Ces
langues ont &#233;t&#233; choisies pour leurs diff&#233;rentes richesses morphologiques. Sur une &#233;chelle croissante, on peut en
effet placer successivement l&#8217;anglais, le fran&#231;ais, l&#8217;allemand puis le finnois qui a la morphologie la plus riche.
</p>
<p>Le corpus Europarl (Koehn, 2005) offre des textes align&#233;s dans ces quatre langues, 2 ce qui permet de mener
des exp&#233;riences v&#233;ritablement comparables. Pour les exp&#233;riences de cette section, de l&#8217;ensemble de toutes les
phrases correspondantes dans toutes les langues, nous avons extrait al&#233;atoirement 100 000 phrases. Parmi elles,
90 000 phrases ont &#233;t&#233; s&#233;lectionn&#233;es al&#233;atoirement, les m&#234;mes dans toutes les langues, pour servir de corpus
d&#8217;entra&#238;nement. Le jeu de test est constitu&#233; des 10 000 phrases restantes.
</p>
<p>2.1 Proportion de trigrammes inconnus reconstruits
</p>
<p>Pour v&#233;rifier dans quelle mesure les trigrammes inconnus d&#8217;un jeu de test peuvent &#234;tre reconstruits par analogie,
nous effectuons une premi&#232;re s&#233;rie d&#8217;exp&#233;riences par validation crois&#233;e. Nous comptons simplement le nombre
total de trigrammes inconnus du jeu de test reconstructibles par analogie &#224; l&#8217;aide de trois trigrammes du corpus
d&#8217;entra&#238;nement. D&#232;s lors que la reconstruction est possible, le processus est interrompu pour ce trigramme.
</p>
<p>Les r&#233;sultats obtenus, report&#233;s dans la table 1, montrent que la proportion de trigrammes inconnus dans le jeu de
test reconstructibles par analogie avec trois autres trigrammes du corpus d&#8217;entra&#238;nement est sup&#233;rieure &#224; 80 % en
anglais et en fran&#231;ais et sup&#233;rieure &#224; 70 % en allemand. Cette proportion, appel&#233;e &#181; ici, est donc importante. Elle
est calcul&#233;e sur le nombre total de trigrammes inconnus diff&#233;rents (sans r&#233;p&#233;tition) dont la proportion relativement
au nombre total de trigrammes du jeu de test est appel&#233;e &#955;. Des exp&#233;riences non rapport&#233;es ici montrent qu&#8217;en
augmentant la taille des donn&#233;es, les valeurs de &#955; baissent tandis que les valeurs de &#181; augmentent. Les param&#232;tres
&#955; et &#181; seront exploit&#233;s dans la section 3.1.
</p>
<p>TABLE 1 &#8211; Nombre de trigrammes inconnus diff&#233;rents dans le jeu de test et proportion de trigrammes inconnus
diff&#233;rents reconstructibles par analogie &#224; l&#8217;aide de trois trigrammes du corpus d&#8217;entra&#238;nement.
</p>
<p>Trigrammes inconnus
du jeu de test reconstruits
</p>
<p>(&#955;) (&#181;)
anglais 114,566 (60,04 %) 83,67 %
fran&#231;ais 116,922 (57,81 %) 81,87 %
allemand 140,226 (68,97 %) 72,14 %
finnois 132,931 (83,33 %) 44,93 %
</p>
<p>En finnois, la proportion de trigrammes inconnus diff&#233;rents reconstruits est faible avec seulement 45 %. Cette
faible valeur est certainement explicable par la richesse morphologique de cette langue et donc l&#8217;absence relative
de mots-fonctions permettant plus de commutations de mots dans les trigrammes. Nous pouvons d&#232;s &#224; pr&#233;sent
nous attendre &#224; des r&#233;sultats diff&#233;rents en finnois dans toute la suite de notre &#233;tude.
</p>
<p>2. http://www.statmt.org/europarl</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>JULIEN GOSME ET YVES LEPAGE
</p>
<p>2.2 Patrons d&#8217;analogie les plus fr&#233;quents
</p>
<p>Un trigramme de mots donn&#233; peut &#234;tre obtenu par analogie &#224; l&#8217;aide d&#8217;autres trigrammes de plusieurs fa&#231;ons. Par
exemple, pour le trigramme opportunit&#233; de servir, on a, entre autres, les deux analogies suivantes qui utilisent des
trigrammes diff&#233;rents du corpus d&#8217;entra&#238;nement et respectent bien la d&#233;finition de l&#8217;analogie vue en introduction :
</p>
<p>opportunit&#233; de servir : opportunit&#233; de modifier :: qui pourrait servir : qui pourrait modifier
</p>
<p>opportunit&#233; de servir : opportunit&#233; pour dire :: de servir le : pour dire le
</p>
<p>Ces deux analogies exemplifient deux patrons d&#8217;analogie diff&#233;rents donn&#233;s dans la table 2 et num&#233;rot&#233;s 1 et 2. Le
patron 1 correspond au remplacement du bigramme correspondant &#224; la partie gauche du premier trigramme par un
autre bigramme et le remplacement de l&#8217;unigramme restant &#224; droite par un autre unigramme : opportunit&#233; de est
remplac&#233; par qui pourrait, et servir est remplac&#233; par modifier. Le patron 2 revient &#224; trouver deux bigrammes dans
les m&#234;mes contextes droit et gauche : de servir et pour dire existent dans les m&#234;mes contextes opportunit&#233; &#8764; et &#8764;
le.
</p>
<p>Dans le double but d&#8217;&#233;num&#233;rer les patrons existant r&#233;ellement en corpus et d&#8217;en d&#233;terminer les fr&#233;quences d&#8217;ap-
parition respectives, nous &#233;num&#233;rons simplement toutes les analogies existantes entre trigrammes &#224; partir d&#8217;un
&#233;chantillon al&#233;atoire de 10 000 phrases dans chaque langue. Pour cela, nous avons contraint la m&#233;thode d&#8217;&#233;nu-
m&#233;ration de toutes les analogies d&#8217;un texte propos&#233;e dans (Gosme &amp; Lepage, 2009) pour n&#8217;&#233;num&#233;rer que les
analogies entre trigrammes de mots. Ensuite, nous regroupons les instances d&#8217;analogies obtenues par patron et les
comptons.
</p>
<p>La table 2 donne les patrons d&#8217;analogie list&#233;s par ordre d&#233;croissant de fr&#233;quences pour l&#8217;anglais. Un r&#233;sultat
remarquable est que les cinq patrons d&#8217;analogie les plus fr&#233;quents dans les quatre langues apparaissent dans le
m&#234;me ordre avec des proportions semblables.
</p>
<p>TABLE 2 &#8211; Patrons d&#8217;analogie entre trigrammes de mots dans un &#233;chantillon anglais de 10 000 phrases du corpus
Europarl tri&#233;s par proportions relatives sur l&#8217;ensemble des analogies entre trigrammes. Les symboles utilis&#233;s dans
l&#8217;&#233;criture des patrons d&#8217;analogie sont distincts deux &#224; deux. Ces patrons respectent la d&#233;finition de l&#8217;analogie
donn&#233;e en introduction.
</p>
<p>No A : B : : C : D Proportion
1 a b c : a b d : : e f c : e f d 12,6 %
2 a b c : a d e : : b c f : d e f 9,1 %
3 a b c : d b c : : e f a : e f d 3,1 %
4 a b c : a e c : : b c d : e c d 2,7 %
5 a b c : a b d : : b c e : b d e 2,6 %
6 a b c : a d e : : f b c : f d e 2,4 %
7 a b c : a d c : : b e f : d e f 1,3 %
8 a b c : a b d : : a e c : a e d 0,9 %
...
</p>
<p>...
...
</p>
<p>2.3 Patrons d&#8217;analogie les plus rentables
</p>
<p>Jusqu&#8217;&#224; pr&#233;sent, nous avons montr&#233;, d&#8217;une part, qu&#8217;une grande majorit&#233; des trigrammes inconnus peuvent &#234;tre
reconstruits par analogie &#224; l&#8217;aide de trigrammes issus du corpus d&#8217;entra&#238;nement (section 2.1) ; et nous avons
identifi&#233;, d&#8217;autre part, les patrons d&#8217;analogie de trigrammes de mots les plus fr&#233;quents dans un m&#234;me corpus
(Section 2.2). L&#8217;&#233;tape suivante est d&#8217;identifier les patrons d&#8217;analogie qui permettent de reconstruire le plus de
trigrammes inconnus d&#8217;un jeu de test &#224; l&#8217;aide de trigrammes du corpus d&#8217;entra&#238;nement, autrement dit, les patrons
les plus rentables. &#192; cette fin, nous conduisons une nouvelle s&#233;rie d&#8217;exp&#233;riences. En raison de la lourdeur en temps
de calcul, nous limitons notre exp&#233;rience aux cinq patrons d&#8217;analogie les plus fr&#233;quents list&#233;s dans la table 2, et
nous proc&#233;dons de la sorte : pour chaque trigramme inconnu du jeu de test, chaque patron d&#8217;analogie est essay&#233;
successivement dans l&#8217;ordre de la table 2. D&#232;s lors qu&#8217;un patron d&#8217;analogie permet de reconstruire le trigramme
inconnu en question, nous notons son rang et passons au trigramme inconnu suivant.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>STRUCTURE DES TRIGRAMMES INCONNUS ET LISSAGE PAR ANALOGIE
</p>
<p>Les r&#233;sultats sont pr&#233;sent&#233;s dans les tables 3((a))&#8211;(d)). Ils montrent les contributions cumul&#233;es des patrons d&#8217;ana-
logie &#224; la reconstruction des trigrammes inconnus. Le patron 1 contribue seul &#224; la majorit&#233; de la reconstruction
des trigrammes inconnus : plus de 70 % en anglais, fran&#231;ais et allemand, mais seulement 61,5 % pour le finnois.
Les patrons 1 et 2 suffisent &#224; reconstruire environ 95 % des trigrammes inconnus en anglais, fran&#231;ais et allemand,
et presque 90 % en finnois.
</p>
<p>TABLE 3 &#8211; Cumul des contributions des cinq patrons d&#8217;analogie les plus fr&#233;quents &#224; la reconstruction des tri-
grammes inconnus dans les quatre langues &#233;tudi&#233;es. Les pourcentages pr&#233;sent&#233;s sont relatifs au nombre total de
trigrammes inconnus.
</p>
<p>(a) Anglais
</p>
<p>No de Trigrammes Proportion
patron reconstruits cumul&#233;e
</p>
<p>(&#181;)
1 72 426 (63,22 %) 75,55 %
2 19 952 (17,42 %) 96,37 %
3 3 411 (2,98 %) 99,93 %
4 46 (0,04 %) 99,97 %
5 25 (0,02 %) 100,00 %
</p>
<p>Total 95 860 (83,67 %) 100,00 %
</p>
<p>(b) Fran&#231;ais
</p>
<p>No de Trigrammes Proportion
patron reconstruits cumul&#233;e
</p>
<p>(&#181;)
1 71,466 (61,98 %) 74,66 %
2 20,475 (17,51 %) 96,05 %
3 3 655 (3,13%) 99,87 %
4 92 (0,08 %) 99,97 %
5 35 (0,03 %) 100,00 %
</p>
<p>Total 95 723 (81,87%) 100,00 %
</p>
<p>(c) Allemand
</p>
<p>No de Trigrammes Proportion
patron reconstruits cumul&#233;e
</p>
<p>(&#181;)
1 71 150 (50,74 %) 70,34 %
2 23 810 (16,98 %) 93,87 %
3 6 003 (4,28 %) 99,81 %
4 156 (0,11 %) 99,96 %
5 37 (0,03 %) 100,00 %
</p>
<p>Total 101 156 (72,14 %) 100,00 %
</p>
<p>(d) Finnois
</p>
<p>No de Trigrammes Proportion
patron reconstruits cumul&#233;e
</p>
<p>(&#181;)
1 36 717 (27,62 %) 61,48 %
2 16 064 (12,08 %) 88,37 %
3 6 227 (4,68 %) 98,80 %
4 548 (0,41 %) 99,72 %
5 169 (0,13 %) 100,00 %
</p>
<p>Total 59 725 (44,93 %) 100,00 %
</p>
<p>Pour les quatre langues, les cinq patrons suffisent &#224; reconstruire l&#8217;int&#233;gralit&#233; des trigrammes ; notre restriction se
justifie donc a posteriori. Quelques exemples de reconstructions de trigrammes sont donn&#233;s dans les figures 1 et 2.
</p>
<p>en justice et : en est , :: justice et de : est , de
d&#233;bat en tant : d&#233;bat de ce :: en tant qu&#8217; : de ce qu&#8217;
</p>
<p>co&#251;ts de la : co&#251;ts et les :: de la plus : et les plus
</p>
<p>FIGURE 1 &#8211; Exemples de trigrammes de mots du corpus fran&#231;ais d&#8217;Europarl respectant le patron 2, c&#8217;est-&#224;-dire
a b c : a d e :: b c f : d e f.
</p>
<p>debate and we : debate and far :: but as we : but as far
Union have set : Union have that :: a committee set : a committee that
</p>
<p>but they do : but they must :: so we do : so we must
</p>
<p>FIGURE 2 &#8211; Exemples de trigrammes de mots du corpus anglais d&#8217;Europarl respectant le patron 1, c&#8217;est-&#224;-dire
a b c : a b d :: e f c : e f d.
</p>
<p>2.4 Effectif suffisant pour la reconstruction d&#8217;un trigramme inconnu
</p>
<p>Puisque l&#8217;hypoth&#232;se de la reconstruction massive des trigrammes inconnus par analogie est confirm&#233;e par les
exp&#233;riences pr&#233;c&#233;dentes, nous passons maintenant &#224; l&#8217;&#233;tude des effectifs des trigrammes impliqu&#233;s dans les re-
constructions. Nous cherchons &#224; savoir quels effectifs ont les trigrammes qui permettent la reconstruction des</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>JULIEN GOSME ET YVES LEPAGE
</p>
<p>trigrammes inconnus. Une supposition naturelle serait que les trigrammes d&#8217;effectifs semblables aient tendance
&#224; appara&#238;tre dans les m&#234;mes analogies. Suivant cette supposition, on peut faire l&#8217;hypoth&#232;se que les trigrammes
inconnus, c&#8217;est-&#224;-dire apparaissant z&#233;ro fois dans le corpus d&#8217;entra&#238;nement, pourraient &#234;tre reconstruits &#224; l&#8217;aide
de trigrammes apparaissant une fois dans le corpus, c&#8217;est-&#224;-dire les trigrammes hapax. Nous confirmons ici cette
hypoth&#232;se.
</p>
<p>Nous effectuons une nouvelle s&#233;rie d&#8217;exp&#233;riences afin d&#8217;obtenir les effectifs des trigrammes en relation d&#8217;analogie
avec les trigrammes inconnus. En raison de la lourdeur des calculs, nous nous limitons &#224; l&#8217;analyse du patron 1 :
a b c : a b d :: e f c : e f d. Pour chaque instance de ce patron, nous d&#233;finissons son effectif maximum comme
l&#8217;effectif du trigramme le plus fr&#233;quent parmi les quatre trigrammes de l&#8217;analogie (comme le premier trigramme
est inconnu, son effectif dans le corpus d&#8217;entra&#238;nement est &#233;videmment z&#233;ro). Pour chaque trigramme inconnu
reconstruit, nous m&#233;morisons le minimum sur les effectifs maximums de toutes les analogies permettant de le
reconstruire (effectif min-max). De cette m&#233;morisation, et par inversion, pour chaque effectif min-max, nous
pouvons compter le nombre de trigrammes inconnus reconstruits. Chaque effectif min-max est donc l&#8217;effectif
suffisant &#224; consid&#233;rer pour trouver &#224; coup s&#251;r des trigrammes permettant la reconstruction de tant de trigrammes
inconnus.
</p>
<p>Ces d&#233;comptes sont donn&#233;s dans la table 4. Pour chaque effectif min-max, la table pr&#233;sente la quantit&#233; de tri-
grammes reconstruits et un pourcentage cumul&#233;. Selon ces r&#233;sultats, les instances d&#8217;analogie du patron 1 impli-
quant trois trigrammes hapax (effectif min-max = 1) permettent la reconstruction de plus de 95 % des trigrammes
inconnus en anglais, 94 % en fran&#231;ais ou en allemand et 91 % en finnois.
</p>
<p>TABLE 4 &#8211; Pourcentages cumul&#233;s des trigrammes reconstruits, class&#233;s par effectif suffisant des trigrammes for-
mant analogie pour leur reconstruction (colonne effectif min-max).
</p>
<p>(a) Anglais
</p>
<p>Effectif Trigrammes Pourcentage
min-max reconstruits cumul&#233;
</p>
<p>1 54 227 (96,24 %) 96,24 %
2 1 288 (2,29 %) 98,53 %
3 345 (0,61 %) 99,14 %
4 127 (0,23 %) 99,36 %
5 99 (0,18 %) 99,54 %
...
</p>
<p>...
...
</p>
<p>523 1 (0,00 %) 100,00 %
TOTAL 56 345 (100,00 %) &#8212;
</p>
<p>(b) Fran&#231;ais
</p>
<p>Effectif Trigrammes Pourcentage
min-max reconstruits cumul&#233;
</p>
<p>1 59 050 (94,07 %) 94,07 %
2 2 167 (3,45 %) 97,52 %
3 608 (0,97 %) 98,49 %
4 302 (0,48 %) 98,97 %
5 167 (0,26 %) 99,24 %
...
</p>
<p>...
...
</p>
<p>576 1 (0,00 %) 100,00 %
TOTAL 62 771 (100,00 %) &#8212;
</p>
<p>(c) Allemand
</p>
<p>Effectif Trigrammes Pourcentage
min-max reconstruits cumul&#233;
</p>
<p>1 41 272 (94,01 %) 94,01 %
2 1 475 (3,36 %) 97,36 %
3 465 (1,06 %) 98,42 %
4 219 (0,50 %) 98,92 %
5 124 (0,28 %) 99,21 %
...
</p>
<p>...
...
</p>
<p>412 1 (0,00 %) 100,00 %
TOTAL 43 904 (100,00 %) &#8212;
</p>
<p>(d) Finnois
</p>
<p>Effectif Trigrammes Pourcentage
min-max reconstruits cumul&#233;
</p>
<p>1 13 382 (91,02 %) 91,02 %
2 760 (5,217 %) 96,18 %
3 238 (1,62 %) 97,80 %
4 101 (0,68 %) 98,49 %
5 56 (0,38 %) 98,87 %
...
</p>
<p>...
...
</p>
<p>458 1 (0,01 %) 100,00 %
TOTAL 56 542 (100,00 %) &#8212;
</p>
<p>L&#8217;ensemble des r&#233;sultats exp&#233;rimentaux pr&#233;c&#233;dents conduit &#224; la conclusion que non seulement les analogies entre
trigrammes structurent les trigrammes inconnus, mais qu&#8217;en plus, la reconstruction des trigrammes inconnus est
massivement possible avec des trigrammes d&#8217;effectif semblable, c&#8217;est-&#224;-dire d&#8217;effectif 1.
</p>
<p>Pour r&#233;sumer l&#8217;&#233;tude empirique pr&#233;sent&#233;e ci-dessus, on peut donc dire que : dans leur grande majorit&#233; les tri-
grammes inconnus sont analogues aux trigrammes hapax ; leurs structures et leurs effectifs sont semblables.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>STRUCTURE DES TRIGRAMMES INCONNUS ET LISSAGE PAR ANALOGIE
</p>
<p>3 Lissage de mod&#232;les trigrammes par analogie
</p>
<p>Dans cette deuxi&#232;me partie, nous allons exploiter les r&#233;sultats de l&#8217;&#233;tude empirique pr&#233;c&#233;dente pour proposer une
technique de lissage de mod&#232;les de langue. Notre proposition est volontairement simple et s&#8217;inspire de m&#233;thodes
de lissage &#233;l&#233;mentaires : les lissages de Lidstone et de Laplace.
</p>
<p>Habituellement, lorsqu&#8217;on utilise directement des outils tels que SRILM (Stolcke, 2002), on a l&#8217;habitude d&#8217;utiliser
les techniques de lissage classiques connues pour donner des r&#233;sultats acceptables. Des techniques de lissage
plus &#233;labor&#233;es ont &#233;t&#233; propos&#233;es afin de r&#233;duire la taille des mod&#232;les de langue, nous pensons en particulier au
clustering (Brown et al., 1992). Cependant, de telles techniques requi&#232;rent une phase de pr&#233;-traitement complexe,
ce qui accro&#238;t le co&#251;t de calcul (Matsuzaki et al., 2003). En comparaison, la m&#233;thode que nous proposons dans cet
article n&#8217;extrait pas de connaissances suppl&#233;mentaires des donn&#233;es d&#8217;entra&#238;nement. La structure des trigrammes
inconnus est v&#233;rifi&#233;e au fil du calcul. Les principaux avantages de cette m&#233;thode sont sa simplicit&#233; et sa facilit&#233;
d&#8217;utilisation.
</p>
<p>3.1 R&#233;-estimation des effectifs
</p>
<p>Redisons une v&#233;rit&#233; &#233;l&#233;mentaire : tout &#233;v&#232;nement inconnu apparaissant dans le jeu de test a un effectif nul dans
le corpus d&#8217;entra&#238;nement. Imm&#233;diatement au-dessus de la classe des &#233;v&#232;nements d&#8217;effectif nul, vient la classe des
&#233;v&#232;nements observ&#233;s une seule fois dans le corpus d&#8217;entra&#238;nement : ce sont les hapax. Or, il est classique pour
une technique de lissage d&#8217;essayer d&#8217;estimer la probabilit&#233; liss&#233;e des &#233;v&#232;nements inconnus en se basant sur les
propri&#233;t&#233;s des &#233;v&#233;nements class&#233;s selon leurs fr&#233;quences d&#8217;apparition : c&#8217;est la base du lissage de Good-Turing
(Gale, 1994). Nous exploitons la m&#234;me id&#233;e mais dans une mise en application plus simple.
</p>
<p>Dans le lissage de Laplace, tout &#233;v&#233;nement voit son effectif augment&#233; de 1. Dans notre technique de lissage, nous
gardons cet incr&#233;ment de 1 pour les &#233;v&#233;nements connus. L&#8217;essence de notre technique de lissage tient dans la
distinction faite entre &#233;v&#232;nements inconnus selon qu&#8217;ils peuvent &#234;tre reconstruits par analogie ou non.
</p>
<p>Nous donnons un fort avantage aux &#233;v&#232;nements inconnus qui peuvent &#234;tre reconstruits par analogie au d&#233;triment
de ceux qui ne peuvent pas l&#8217;&#234;tre. Les r&#233;sultats des exp&#233;riences pr&#233;sent&#233;es en section 2.4 conduisent &#224; proposer
un effectif tr&#232;s proche de 1 pour les trigrammes reconstructibles puisqu&#8217;ils sont analogues aux trigrammes hapax
Nous fixons leur effectif &#224; 1&#8722; &#945; avec &#945; proche de 0. Ils deviennent donc de nouveaux quasi-hapax, alors que les
anciens hapax sont r&#233;-estim&#233;s avec un effectif de 1 + 1 = 2.
</p>
<p>En d&#233;sespoir de cause, nous affectons comme estimation des effectifs des trigrammes inconnus qui ne peuvent
pas &#234;tre reconstruits une valeur tr&#232;s proche de 0. Pour simplifier, nous utilisons la valeur &#945;. On peut dire que cette
partie du lissage est en fait un lissage de Lidstone.
</p>
<p>Au total donc, la probabilit&#233; liss&#233;e d&#8217;un trigramme hi.mi (hi repr&#233;sente les deux mots pr&#233;c&#233;dant mi) est r&#233;-
estim&#233;e selon chacun des trois cas suivants, avec N la longueur du texte, |V | la taille du vocabulaire et &#948; restant &#224;
d&#233;terminer :
</p>
<p>&#8211; trigrammes connus :
C(hi.mi) + 1
</p>
<p>N + &#948; &#215; |V |
&#8211; trigrammes inconnus pouvant &#234;tre reconstruits par analogie :
</p>
<p>1&#8722; &#945;
N + &#948; &#215; |V |
</p>
<p>&#8211; trigrammes inconnus ne pouvant &#234;tre reconstruits par analogie 3 :
&#945;
</p>
<p>N + &#948; &#215; |V |
</p>
<p>En reprenant les notations de la section 2.1 et de la table 1, nous notons &#955; la proportion de trigrammes inconnus
diff&#233;rents et &#181; la proportion relative de trigrammes inconnus diff&#233;rents reconstruits par analogie. Les valeurs de &#955;
et &#181; sont comprises entre 0 et 1. Avec ces notations :
</p>
<p>&#8211; (1&#8722;&#955;) est la proportion de trigrammes connus dans le jeu de test, &#955; &#233;tant la proportion de trigrammes inconnus
dans le jeu de test ;
</p>
<p>&#8211; &#181;&#955; est la proportion, sur l&#8217;ensemble du jeu de test, de trigrammes inconnus qui peuvent &#234;tre reconstruits, &#181; &#233;tant
la proportion de trigrammes inconnus reconstructibles ;
</p>
<p>3. C&#8217;est en particulier le cas de tout trigramme contenant un mot inconnu. Un tel trigramme ne peut en effet &#234;tre reconstruit par analogie
de par la d&#233;finition donn&#233;e en introduction (test sur le nombre d&#8217;occurrences des mots).</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>JULIEN GOSME ET YVES LEPAGE
</p>
<p>&#8211; et (1 &#8722; &#181;)&#955; est le reste sur l&#8217;ensemble des trigrammes du jeu de test, c&#8217;est-&#224;-dire la proportion de trigrammes
inconnus ne pouvant &#234;tre reconstruits par analogie.
</p>
<p>La somme des probabilit&#233;s de tous les trigrammes devant faire 1, la valeur de &#948; peut &#234;tre d&#233;termin&#233;e :
</p>
<p>&#948; = (1&#8722; &#955;)&#215; 1 + &#181;&#955;&#215; (1&#8722; &#945;) + (1&#8722; &#181;)&#955;&#215; &#945;
= 1&#8722; (2&#945;&#181;&#8722; &#945;&#8722; &#181;+ 1)&#955;
</p>
<p>3.2 Estimation des param&#232;tres
</p>
<p>Dans la pratique, les param&#232;tres &#955; et &#181; sont estim&#233;s dans une phase de pr&#233;-traitement. Le corpus d&#8217;entra&#238;nement
est divis&#233; en deux parties, l&#8217;une comprenant les neuf dixi&#232;mes du corpus, l&#8217;autre comprenant le dixi&#232;me restant. La
proportion de trigrammes inconnus dans la plus petite partie ainsi que la part de trigrammes inconnus reconstruits
par analogie sont estim&#233;es par &#233;chantillonnage. Ces estimations deviennent les valeurs des param&#232;tres &#955; et &#181;.
</p>
<p>Concernant le param&#232;tre &#945;, des r&#233;sultats d&#8217;exp&#233;riences non pr&#233;sent&#233;s dans cet article nous ont conduits &#224; le fixer
&#224; 10&#8722;6 pour toutes les langues.
</p>
<p>3.3 Temps d&#8217;ex&#233;cution
</p>
<p>Afin de d&#233;terminer si un trigramme inconnu peut &#234;tre reconstruit ou non par analogie, le corpus d&#8217;entra&#238;nement est
m&#233;moris&#233; sous forme de deux tableaux de suffixes (sens de lecture normal et miroir). Lorsque la reconstruction
d&#8217;un trigramme doit &#234;tre test&#233;e, pour chaque patron d&#8217;analogie, une recherche appropri&#233;e &#224; ce patron est effectu&#233;e
dans ces tableaux de suffixes. Par exemple, pour le patron 1, le trigramme candidat a b c est d&#233;compos&#233; en une
partie gauche a b et une partie droite c. La recherche de ces s&#233;quences dans les tableaux de suffixes r&#233;duit aux
trigrammes hapax est tr&#232;s rapide. Il suffit alors de prendre l&#8217;intersection en termes de positions de l&#8217;ensemble des
unigrammes d qui suivent a b (sens de lecture normal) et de l&#8217;ensemble des bigrammes e f qui pr&#233;c&#232;dent c (miroir).
D&#232;s qu&#8217;une position est trouv&#233;e dans l&#8217;intersection, nous en d&#233;duisons qu&#8217;il existe au moins un trigramme e f d
dans le corpus d&#8217;entra&#238;nement et nous pouvons conclure en l&#8217;existence d&#8217;une analogie a b c : a b d :: e f c : e f d.
Cela signifie que le trigramme a b c peut &#234;tre reconstruit par analogie &#224; l&#8217;aide de trigrammes du corpus d&#8217;entra&#238;-
nement. Une proc&#233;dure similaire a &#233;t&#233; implant&#233;e pour le patron 2.
</p>
<p>L&#8217;implantation des lissages classiques de SRILM permet de lisser environ 1 000 phrases par seconde en fran&#231;ais
quelle que soit la m&#233;thode de lissage et quelle que soit la taille du corpus d&#8217;entra&#238;nement sur une machine &#233;quip&#233;e
d&#8217;un processeur 16 bits cadenc&#233; &#224; 2 GHz et ayant 4 Go de m&#233;moire.
</p>
<p>Notre m&#233;thode de lissage effectue des recherches dans des tableaux de suffixes et nous nous attendons &#224; ce que la
vitesse de lissage d&#233;pende de la taille du corpus d&#8217;entra&#238;nement. Sur le m&#234;me type de machine, nous mesurons la
vitesse de notre m&#233;thode de lissage en fonction de la taille du corpus d&#8217;entra&#238;nement pour deux variantes : patron 1
seul et patrons 1 et 2. Nous utilisons des &#233;chantillons de la partie fran&#231;aise d&#8217;Europarl avec des tailles variant de
900 &#224; 320 000 phrases. Dans la seconde variante, le patron 2 est utilis&#233; en deuxi&#232;me instance dans la cas o&#249; le
patron 1 n&#8217;a pas permis de reconstruire le trigramme.
</p>
<p>Les courbes de la figure 3 donnent le nombre de phrases du jeu de test trait&#233;es par seconde en fonction de la
taille du corpus d&#8217;entra&#238;nement. La vitesse de lissage de notre m&#233;thode d&#233;pend nettement de la taille du corpus
d&#8217;entra&#238;nement. Pour de petits corpus, notre implantation traite 300 phrases par seconde. Cette vitesse chute &#224; 100
phrases par seconde pour les corpus de plus grande taille et n&#8217;&#233;volue plus vraiment &#224; partir de 180 000 phrases.
Les deux variantes sont similaires, ce qui signifie que la variante patrons 1 et 2 n&#8217;engendre qu&#8217;un faible surco&#251;t
de temps de traitement.
</p>
<p>L&#8217;implantation actuelle de notre m&#233;thode de lissage par analogie, en Python, est donc dix fois plus lente que les
implantations de SRILM en C++ des m&#233;thodes classiques de lissage. On peut raisonnablement esp&#233;rer des temps
comparables avec une implantation en C++ si l&#8217;on se fie aux r&#232;gles tr&#232;s grossi&#232;res donnant des acc&#233;l&#233;rations par
dix lors de r&#233;&#233;critures de Python en C++. 4
</p>
<p>4. http://shootout.alioth.debian.org/u32q/benchmark.php?test=all&amp;lang=gpp&amp;lang2=python</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>STRUCTURE DES TRIGRAMMES INCONNUS ET LISSAGE PAR ANALOGIE
</p>
<p>N
om
</p>
<p>br
e
</p>
<p>de
ph
</p>
<p>ra
se
</p>
<p>s
tr
</p>
<p>ai
t&#233;
</p>
<p>es
pa
</p>
<p>rs
ec
</p>
<p>on
de
</p>
<p>0
</p>
<p>50
</p>
<p>100
</p>
<p>150
</p>
<p>200
</p>
<p>250
</p>
<p>300
</p>
<p>350
</p>
<p>0 50000 100000 150000 200000 250000 300000 350000
Taille du corpus d&#8217;entra&#238;nement
</p>
<p>patron 1
patrons 1 et 2
</p>
<p>FIGURE 3 &#8211; Vitesse de la m&#233;thode de lissage par analogie pour diff&#233;rentes tailles du corpus d&#8217;entra&#238;nement, pour
deux variantes de la m&#233;thode : patron 1 et patrons 1 et 2.
</p>
<p>3.4 &#201;valuation des performances
</p>
<p>Nous comparons notre m&#233;thode de lissage par analogie avec quatre m&#233;thodes de lissage classiques : Lidstone
(Chen &amp; Goodman, 1999), Witten &amp; Bell (1991), Good-Turing (Gale, 1994) et Kneser &amp; Ney (1995). Cette der-
ni&#232;re m&#233;thode est souvent consid&#233;r&#233;e comme la meilleure en pratique. 5 Pour ces quatre m&#233;thodes, nous utilisons
les implantations de SRILM (Stolcke, 2002). Dans le cas du lissage de Lidstone, apr&#232;s optimisation, nous utilisons
la valeur 10&#8722;3 pour le param&#232;tre &#945; pour chaque langue.
</p>
<p>Les crit&#232;res d&#8217;&#233;valuation utilis&#233;s sont la divergence de Kullback-Leibler et la perplexit&#233; en mots. La divergence de
Kullback-Leibler est d&#233;finie pour chaque phrase par :
</p>
<p>&#8211; entropie du jeu de test : H(p) = &#8722;
l&#8721;
</p>
<p>i=1
</p>
<p>p(mi|hi)&#215; log2p(mi|hi) ;
</p>
<p>&#8211; entropie d&#8217;un mod&#232;le de langue : H(p, q) = &#8722;
l&#8721;
</p>
<p>i=1
</p>
<p>p(mi|hi)&#215; log2q(mi|hi) ;
&#8211; divergence de Kullback-Leibler : DKL = H(p, q)&#8722;H(p).
La perplexit&#233; en mots est d&#233;finie comme la moyenne g&#233;om&#233;trique des inverses des probabilit&#233;s r&#233;estim&#233;es. En
</p>
<p>notant n le nombre de mots du jeu de test : PPL = 2
&#8722;&#8721;ni=1 log2p(mi|hi)
</p>
<p>n .
</p>
<p>Dans ces formules, p(mi|hi) est la probabilit&#233; conditionnelle obtenue sur le jeu de test, avec mi le mot &#224; la
position i et hi son histoire, c&#8217;est-&#224;-dire les deux mots pr&#233;c&#233;dant mi ; q(mi|hi) est la probabilit&#233; conditionnelle
liss&#233;e utilisant le corpus d&#8217;entra&#238;nement et l est la longueur de la phrase du jeu de test.
</p>
<p>La comparaison est effectu&#233;e sur des donn&#233;es extraites d&#8217;Europarl en onze langues. Pour chaque langue, les
phrases ayant une traduction en anglais sont retenues. Nous obtenons de cette mani&#232;re onze corpus align&#233;s de
383 237 phrases repr&#233;sentant 10 millions de mots ou plus dans chaque langue, sauf en finnois (seulement 8 mil-
lions). Chaque corpus est ensuite divis&#233; en deux parties : 90 % du corpus pour l&#8217;entra&#238;nement, les 10 % restants
servant de jeu de test. De cette mani&#232;re, nos exp&#233;riences sont v&#233;ritablement comparables entre langues. Les sta-
tistiques concernant le corpus d&#8217;entra&#238;nement et le jeu de test pour chaque langue sont pr&#233;sent&#233;es dans la table 5.
</p>
<p>Les estimations des param&#232;tres &#955; et &#181; n&#233;cessaires &#224; notre m&#233;thode de lissage sont d&#233;taill&#233;es dans la table 6.
</p>
<p>5. &#171; Kneser &amp; Ney (1995) smoothing and its variants are generally recognized as having the best perplexity of any known method for esti-
mating N-gram language models. &#187; (Moore &amp; Quirk, 2009). (Chen &amp; Goodman, 1998) ont montr&#233; qu&#8217;une premi&#232;re version modifi&#233;e du lissage
de Kneser-Ney &#171; consistently had the best performance &#187; sur l&#8217;ensemble de leurs tests et qu&#8217;une seconde version modifi&#233;e &#171; [p]erform[ed] just
slightly worse &#187;.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>JULIEN GOSME ET YVES LEPAGE
</p>
<p>TABLE 5 &#8211; Statistiques des corpus d&#8217;entra&#238;nement et des jeux de tests utilis&#233;s pour la comparaison.
Corpus d&#8217;entra&#238;nement : 347 613 phrases Jeux de test : 38 624 phrases
</p>
<p>Langue Nbr total demots (&#215;106)
Taille du
</p>
<p>vocabulaire
Mots/phrase Nbr total demots (&#215;106)
</p>
<p>Taille du
vocabulaire
</p>
<p>Mots/phrase
</p>
<p>da 9,46 153 425 27,21 1,06 46 117 27,36
de 9,51 167 942 27,36 1,06 51 398 27,48
el 10,00 149 247 28,76 1,12 52 671 28,89
en 9,94 67 819 28,60 1,11 25 854 28,76
es 10,47 100 410 30,12 1,17 37 128 30,27
fi 7,18 299 116 20,65 0,80 84 964 20,74
fr 10,95 86 567 31,51 1,22 33 403 31,65
it 9,88 99 252 28,42 1,10 36 624 28,54
nl 10,01 125 565 28,80 1,12 39 728 29,00
pt 10,29 102 800 29,59 1,15 38 041 29,73
sv 8,99 157 116 25,86 1,00 48 327 25,98
</p>
<p>TABLE 6 &#8211; Proportion de trigrammes inconnus diff&#233;rents (&#955;) et proportion de trigrammes inconnus diff&#233;rents
reconstructibles par analogie (&#181;) estim&#233;es &#224; partir d&#8217;un &#233;chantillon de 10 % du corpus d&#8217;entra&#238;nement pour chaque
langue, et valeurs correspondantes de &#948; (&#945; = 10&#8722;6). Lors du calcul de &#948;, les valeurs de &#955; et &#181; sont ramen&#233;es entre
0 et 1.
</p>
<p>Trigrammes inconnus diff&#233;rents
Reconstruits
</p>
<p>(&#955;) (&#181;) (&#948;)
Patron 1 Patrons 1 et 2 Patron 1 Patrons 1 et 2
</p>
<p>da 55,03 % 45,84 % 70,22 % 0,702 0,836
de 61,41 % 43,24 % 69,43 % 0,651 0,812
el 59,57 % 42,98 % 69,29 % 0,660 0,817
en 51,97 % 55,72 % 79,72 % 0,770 0,895
es 48,96 % 50,34 % 73,45 % 0,757 0,870
fi 78,24 % 26,68 % 49,25 % 0,426 0,603
fr 49,13 % 53,40 % 79,19 % 0,771 0,898
it 58,88 % 49,82 % 75,27 % 0,705 0,854
nl 54,56 % 52,00 % 75,94 % 0,738 0,869
pt 54,72 % 47,46 % 72,84 % 0,713 0,851
sv 60,18 % 47,25 % 71,28 % 0,683 0,827
</p>
<p>TABLE 7 &#8211; Comparaison de la technique de lissage par analogie (patron 1 et patrons 1 et 2) avec quatre techniques
de lissage classiques en onze langues.
</p>
<p>Perplexit&#233;s en mots
da de el en es fi fr it nl pt sv
</p>
<p>Patron 1 197,5 401,5 226,9 125,6 144,5 10099,8 106,0 149,0 181,0 141,6 334,6
Lidstone 171,0 247,1 179,3 107,4 107,6 1135,9 84,5 141,0 162,1 125,6 235,3
Witten-Bell 130,1 192,0 139,5 93,2 91,9 828,3 73,7 119,9 132,3 106,2 180,0
Good-Turing 128,9 189,2 138,1 92,6 91,0 784,6 73,3 119,1 131,0 105,3 177,7
Kneser-Ney 134,7 196,3 158,3 95,6 92,0 824,3 74,6 120,1 137,3 106,9 186,4
Patron 1 et 2 107,8 182,4 116,4 90,9 85,8 2876,6 73,7 81,0 99,2 79,7 152,6
</p>
<p>Divergences de Kullback-Leibler
Patron 1 61,2 73,1 73,5 52,2 56,7 121,9 55,8 68,8 62,9 62,3 70,3
Lidstone 54,2 66,3 63,2 44,2 47,9 95,7 45,0 56,7 54,8 53,0 60,6
Witten-Bell 47,0 60,0 56,2 40,6 43,4 89,0 41,0 52,5 49,4 48,4 54,1
Good-Turing 46,5 59,3 55,7 40,0 42,9 87,6 40,5 52,0 48,8 47,8 53,5
Kneser-Ney 46,4 58,8 55,9 40,2 43,0 87,2 40,4 51,9 48,8 47,8 53,5
Patron 1 et 2 43,5 50,2 51,8 38,7 41,5 105,7 41,2 48,5 44,8 44,3 48,4</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>STRUCTURE DES TRIGRAMMES INCONNUS ET LISSAGE PAR ANALOGIE
</p>
<p>Nous rappelons que &#955; est la proportion de trigrammes inconnus diff&#233;rents et que &#181; est la proportion relative de
trigrammes inconnus diff&#233;rents qui peuvent &#234;tre reconstruits par analogie. Nous consid&#233;rons deux variantes de
notre m&#233;thode : la premi&#232;re n&#8217;utilise que le patron 1, la seconde utilise les patrons 1 et 2. Afin de rendre la tech-
nique de lissage ind&#233;pendante du jeu de test, pour chaque langue les estimations des param&#232;tres ont &#233;t&#233; obtenues
automatiquement &#224; partir d&#8217;un &#233;chantillon al&#233;atoire form&#233; d&#8217;un dixi&#232;me du corpus d&#8217;entra&#238;nement comme d&#233;crit
dans la section 3.2. Comme le montrent les chiffres de la table 6, l&#8217;utilisation du patron 2 en plus du patron 1
augmente sensiblement la valeur du param&#232;tre &#181; : plus d&#8217;un quart en valeurs absolues. &#192; l&#8217;exception du finnois,
l&#8217;utilisation conjointe des patrons 1 et 2 permet la reconstruction de 70 % &#224; 80 % des trigrammes inconnus. Les
valeurs pour le finnois, en gras dans la table, sont nettement diff&#233;rentes des valeurs pour les autres langues.
</p>
<p>Les r&#233;sultats de l&#8217;&#233;valuation des deux variantes de la m&#233;thode propos&#233;e sont pr&#233;sent&#233;s dans la table 7 :
&#8211; le patron 1 seul est insuffisant pour atteindre m&#234;me le niveau du lissage de Lidstone. On obtient syst&#233;matique-
</p>
<p>ment les plus mauvais r&#233;sultats dans les onze langues ;
&#8211; &#224; l&#8217;exception du finnois, et dans une moindre mesure du fran&#231;ais, l&#8217;ajout du patron 2 est suffisant pour obtenir
</p>
<p>des r&#233;sultats bien meilleurs que ceux des quatre m&#233;thodes de lissage classiques.
La contre-performance sur le finnois n&#8217;est pas surprenante si l&#8217;on consid&#232;re le nombre important de trigrammes
inconnus et la faible proportion de ces trigrammes qui peuvent &#234;tre reconstruits par analogie (voir table 6). Afin
de rem&#233;dier &#224; ce probl&#232;me, plut&#244;t que d&#8217;accro&#238;tre la quantit&#233; de donn&#233;es d&#8217;entra&#238;nement, il serait sans doute plus
judicieux de segmenter les mots en morph&#232;mes. Quant aux r&#233;sultats en fran&#231;ais, ils sont comparables &#224; ceux des
m&#233;thodes classiques.
</p>
<p>4 Conclusion et perspectives
</p>
<p>Dans cet article, &#224; l&#8217;aide d&#8217;une s&#233;rie d&#8217;exp&#233;riences sur quatre langues, nous avons montr&#233; qu&#8217;en majorit&#233; les
trigrammes inconnus dans un jeu de test sont structurellement analogues aux trigrammes hapax d&#8217;un corpus d&#8217;en-
tra&#238;nement.
</p>
<p>De cette propri&#233;t&#233;, nous avons d&#233;riv&#233; une m&#233;thode de lissage pour mod&#232;les de langue trigrammes. L&#8217;effectif
des trigrammes connus est r&#233;-estim&#233; en appliquant un incr&#233;ment de 1 comme dans le lissage de Laplace. Les
trigrammes inconnus qui peuvent &#234;tre reconstruits par analogie sont consid&#233;r&#233;s comme quasi-hapax : leurs effectifs
sont r&#233;-estim&#233;s &#224; une valeur proche de 1. Les trigrammes inconnus qui ne peuvent &#234;tre reconstruits par analogie
sont presque ignor&#233;s, leurs effectifs &#233;tant fix&#233;s &#224; une valeur proche de 0 comme dans le lissage de Lidstone. En
comparaison de techniques de lissage utilisant des techniques de clustering, cette m&#233;thode est simple ; elle ne
construit que deux classes de trigrammes inconnus : ceux qui peuvent &#234;tre reconstruits et les autres.
</p>
<p>Des mesures sur onze langues ont montr&#233; que cette m&#233;thode de lissage donne de bons r&#233;sultats en comparaison
des techniques de lissage classiques, sauf dans le cas du finnois.
</p>
<p>L&#8217;&#233;tude pr&#233;sent&#233;e ici laisse un certain nombre de points &#224; examiner. Tout d&#8217;abord, cette &#233;tude a &#233;t&#233; consacr&#233;e
aux trigrammes. Or, aujourd&#8217;hui, dans de nombreux domaines du traitement automatique des langues, comme
par exemple la traduction automatique par approche statistique, on utilise des mod&#232;les de langue 5-grammes.
Des exp&#233;riences restent donc &#224; effectuer avec des n-grammes d&#8217;ordres sup&#233;rieurs pour savoir si de bons r&#233;sultats
peuvent aussi &#234;tre obtenus. L&#8217;influence du nombre de patrons d&#8217;analogie sur l&#8217;entropie des mod&#232;les de langue
obtenus reste elle aussi &#224; &#233;tudier. Un autre point porte sur la taille des corpus utilis&#233;s. Les exp&#233;riences rapport&#233;es
ici visant &#224; une comparaison sur plusieurs langues et les tr&#232;s grands corpus multilingues &#233;tant rares, la taille du
corpus utilis&#233; ici est relativement faible en regard de corpus monolingues d&#233;passant le milliard de mots. Des
exp&#233;riences sur des corpus de tailles plus importantes restent donc &#224; effectuer. Enfin, dans la perspective d&#8217;une
int&#233;gration &#224; la traduction automatique par approche statistique, les pouvoirs discriminants de la technique de
lissage propos&#233;e ici restent &#224; examiner.
</p>
<p>5 Remerciements
</p>
<p>Cet article d&#233;crit des r&#233;sultats de recherche obtenus en partie gr&#226;ce une subvention de l&#8217;universit&#233; Waseda pour
projets de recherche sp&#233;cifiques (projet num&#233;ro : 2010A-906).</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>JULIEN GOSME ET YVES LEPAGE
</p>
<p>R&#233;f&#233;rences
</p>
<p>BROWN P., PIETRA V., DESOUZA P., LAI J. &amp; MERCER R. (1992). Class-based n-gram models of natural
language. Computational linguistics, 18(4), 467&#8211;479.
CHEN S. F. &amp; GOODMAN J. (1998). An empirical study of smoothing techniques for language modeling.
Rapport interne, Harvard university, Cambridge, Massachussetts.
CHEN S. F. &amp; GOODMAN J. (1999). An empirical study of smoothing techniques for language modeling.
Computer Speech and Language, 13(4), 359&#8211;394.
CLAVEAU V. &amp; L&#8217;HOMME M.-C. (2005). Terminology by analogy-based machine learning. In Proceedings of
the 7th International Conference on Terminology and Knowledge Engineering, TKE 2005, Copenhagen.
DENOUAL E. (2007). Analogical translation of unknown words in a statistical machine translation framework.
In Proceedings of Machine Translation Summit XI, Copenhagen.
GALE W. (1994). Good-turing smoothing without tears. Journal of Quantitative Linguistics, 2.
GOSME J. &amp; LEPAGE Y. (2009). A first study of the complete enumeration of all analogies contained in a text.
In 4th Language and Technology Conference (LTC 2009), p. 401&#8211;405, Poznan&#769;, Poland.
HATHOUT N. (2009). Acquisition morphologique &#224; partir d&#8217;un dictionnaire informatis&#233;. In T. NAZARENKO, D.
ET POIBEAU, Ed., Actes de la 16e Conf&#233;rence Annuelle sur le Traitement Automatique des Langues Naturelles
(TALN-2009), p. 10 p. : ATALA.
KNESER R. &amp; NEY H. (1995). Improved backing-off for m-gram language modeling. In Acoustics, Speech, and
Signal Processing, 1995. ICASSP-95., 1995 International Conference on, volume 1.
KOEHN P. (2005). Europarl : A Parallel Corpus for Statistical Machine Translation. In Proceedings of the tenth
Machine Translation Summit (MT Summit X), p. 79&#8211;86, Phuket.
LANGLAIS P. &amp; PATRY A. (2007). Translating unknown words by analogical learning. In Proceedings of
the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning (EMNLP-CoNLL), p. 877&#8211;886.
LANGLAIS P., YVON F. &amp; ZWEIGENBAUM P. (2008). Analogical Translation of Medical Words in Different
Languages, volume 5221/2008 of Lecture Notes in Computer Science, p. 284&#8211;295. Springer Berlin / Heidelberg :
Springer Berlin / Heidelberg.
LAVALL&#201;E J. F. &amp; LANGLAIS P. (2010). Analyse morphologique non supervis&#233;e par analogie formelle. In
TALN 2010, p. 10 pages, Montr&#233;al, Qu&#233;bec, Canada.
LEPAGE Y. (2004). Analogy and formal languages. Electronic notes in theoretical computer science, 53, 180&#8211;
191.
LEPAGE Y. &amp; DENOUAL E. (2005). Purest ever example-based machine translation : detailed presentation and
assessment. Machine Translation, 19, 251&#8211;282.
LEVENSHTEIN V. (1966). Binary codes capable of correcting deletions, insertions, and reversals. Soviet Physics
Doklady, 10(8), 707&#8211;710.
MATSUZAKI T., MIYAO Y. &amp; TSUJII J. (2003). An efficient clustering algorithm for class-based language
models. In Proceedings of the seventh conference on Natural language learning at HLT-NAACL, volume 4, p.
119&#8211;126 : Association for Computational Linguistics.
MOORE R. &amp; QUIRK C. (2009). Improved smoothing for N-gram language models based on ordinary counts.
In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, p. 349&#8211;352 : Association for Computational
Linguistics.
ROSENFELD R. (2000). Two decades of statistical language modelling : where do we go from here ? Proceedings
of the IEEE, 88(8), 1270&#8211;1278.
STOLCKE A. (2002). SRILM-an extensible language modeling toolkit. In Seventh International Conference on
Spoken Language Processing, volume 3, p. 901&#8211;904.
TURNEY P. (2008). A uniform approach to analogies, synonyms, antonyms, and associations. In Proceedings of
the 22nd International Conference on Computational Linguistics (Coling 2008), p. 905&#8211;912, Manchester, UK :
Coling 2008 Organizing Committee.
WITTEN I. &amp; BELL T. (1991). The zero-frequency problem : Estimating the probabilities of novel events in
adaptive text compression. Information Theory, IEEE Transactions on, 37(4), 1085&#8211;1094.</p>

</div></div>
</body></html>