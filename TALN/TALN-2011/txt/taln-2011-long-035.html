<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
"http://www.w3.org/TR/html4/loose.dtd">
<html><head><title>Integration of Speech and Deictic Gesture in a Multimodal Grammar</title>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
</head>
<body>
<div style="page-break-before:always; page-break-after:always"><div><p>TALN 2011, Montpellier, 27 juin &#8211; 1er juillet 2011
</p>
<p>Integration of Speech and Deictic Gesture
in a Multimodal Grammar
</p>
<p>Katya Alahverdzhieva &amp; Alex Lascarides
School of Informatics, University of Edinburgh
</p>
<p>K.Alahverdzhieva@sms.ed.ac.uk, alex@inf.ed.ac.uk
</p>
<p>R&#233;sum&#233;. Dans cet article, nous pr&#233;sentons une analyse &#224; base de contraintes de la relation forme-sens
des gestes d&#233;ictiques et de leur signal de parole synchrone. En nous basant sur une &#233;tude empirique de corpus
multimodaux, nous d&#233;finissons quels &#233;nonc&#233;s multimodaux sont bien form&#233;s, et lesquels ne pourraient jamais
produire le sens voulu dans la situation communicative. Plus pr&#233;cis&#233;ment, nous formulons une grammaire mul-
timodale dont les r&#232;gles de construction utilisent la prosodie, la syntaxe et la s&#233;mantique de la parole, la forme
et le sens du signal d&#233;ictique, ainsi que la performance temporelle de la parole et la deixis afin de contraindre la
production d&#8217;un arbre de syntaxe combinant parole et gesture d&#233;ictique ainsi que la repr&#233;sentation unifi&#233;e du sens
pour l&#8217;action multimodale correspondant &#224; cet arbre. La contribution de notre projet est double : nous ajoutons
aux ressources existantes pour le TAL un corpus annot&#233; de parole et de gestes, et nous cr&#233;ons un cadre th&#233;orique
pour la grammaire au sein duquel la composition s&#233;mantique d&#8217;un &#233;nonc&#233; d&#233;coule de la synchronie entre geste et
parole.
</p>
<p>Abstract. In this paper we present a constraint-based analysis of the form-meaning relation of deictic
gesture and its synchronous speech signal. Based on an empirical study of multimodal corpora, we capture
generalisations about which multimodal utterances are well-formed, and which would never produce the intended
meaning in the communicative situation. More precisely, we articulate a multimodal grammar whose construction
rules use the prosody, syntax and semantics of speech, the form and meaning of the deictic signal, as well as the
relative temporal performance of the speech and deixis to constrain the production of a single syntactic tree of
speech and deictic gesture and its corresponding meaning representation for the multimodal action. In so doing,
the contribution of our project is two-fold: it augments the existing NLP resources with annotated speech and
gesture corpora, and it also provides the theoretical grammar framework where the semantic composition of an
utterance results from its gestural and speech synchrony.
</p>
<p>Mots-cl&#233;s : Deixis, parole et geste, grammaires multimodales
.
</p>
<p>Keywords: Deixis, speech and gesture, multimodal grammars.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>KATYA ALAHVERDZHIEVA, ALEX LASCARIDES
</p>
<p>1 Introduction
</p>
<p>Through the physical co-location of people known as co-presence (Goffman, 1963), individuals convey informa-
tion to each other using various meaningful and visibly accessible channels such as the arrangements of the bodies
in the shared space, the bodily orientations, the pointing signals of their hands, etc. In recent years, it has become
commonplace to integrate input from different modalities of interaction, such as natural language and deictic ges-
ture, in multimodal systems for the purposes of human-robot interaction (Giuliani &amp; Knoll, 2007), or pen-based
applications (Oviatt et al., 1997), (Johnston, 1998).
In this paper, we show that co-speech deictic gesture can be integrated into a constraint-based grammar using
purely linguistic information such as the prosody, syntax, semantics of speech, the form and meaning of the
deictic signal, and their relative temporal performance. Our overall aim is to articulate the mapping from the
form of multimodal signals to their (underspecified) meaning, using established methods from linguistics such as
constraint-based syntactic derivation and semantic composition. To specify this mapping, we develop a grammar
for speech and co-speech deictic gesture (referred to as deixis) which captures generalisations about well-formed
multimodal actions and about multimodal actions that cannot convey the intended meaning in the specific context.
We have already captured constraints on depicting dimensions via a constraint-based grammar (Alahverdzhieva
&amp; Lascarides, 2010). Here were are going to demonstrate that constraint-based grammars are expressive enough
to represent the form-meaning mapping for deictic dimensions too.
</p>
<p>2 Data
</p>
<p>We start with an overview of deictic gesture and its relation to other co-speech gestures, and we then present the
major challenges arising from the range of ambiguities and distinct performances of the pointing hand.
</p>
<p>2.1 Deixis Background
</p>
<p>Our focus of study are spontaneously performed co-speech deictic gestures. Compared to, say, depicting gestures
where the hand literally or metaphorically depicts its denotation, deictic gestures designate spatial reference in
Euclidean space marked by the projection of the pointing medium (finger, hand, arm, head, etc.) to a region that
is proximal or distant in relation to the speaker&#8217;s origo. Deictic gestures are thus anchored to the space and time
of the communicative act, and so their propositional content is understood as a function that maps from a world in
its contextually-specific time and space to truth values. The same is not necessarily valid for depicting gestures:
uttering &#8220;What a big cake&#8221; while performing a circular motion with both hands in the frontal centre is not related to
the spatial and temporal context in which the utterance occurs. We therefore argue that whereas depicting gestures
provide qualitative characteristics of the referent, deictic gestures are at heart quantitative. This is the diametrical
distinction that sets apart depicting and deictic gestures, and that prevails in how their semantics is defined.
</p>
<p>Note that by &#8220;gesture&#8221; we mean the expressive part of the whole movement, the kinetic peak of the excursion
that carries the gesture&#8217;s meaning&#8212;the so called stroke. What is intuitively recognised as a gesture, is known as
a gesture phrase. It contains the following phases: a non-obligatory preparation (the hands are lifted from the
rest position to the frontal space to perform the semantically intended motion), a non-obligatory pre-stroke hold
(the hands are sustained in a position before reaching the kinetic peak), an obligatory stroke, and a non-obligatory
post-stroke hold (the hands sustain their expressing position). The deictic stroke might be static (the pointing
forelimbs are stationary in the expressive position) or dynamic (gesture&#8217;s meaning is derived from a movement of
the pointing forelimbs).
</p>
<p>2.2 Range of Deictic Use
</p>
<p>The deictic signal on its own is ambiguous with respect to the region pointed out and the syntactic and semantic
relation between speech and deixis. To clarify the region&#8217;s ambiguity, let&#8217;s consider the following example: when
pointing in the direction of a book, does the space demarcated by the deictic gesture identify with the physical
object book, the location of the book&#8212;e.g., the table&#8212;or with the cover of the book? Often there is not an exact</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>INTEGRATION OF SPEECH AND DEIXIS IN A MULTIMODAL GRAMMAR
</p>
<p>correspondence between the region identified by the pointing hand, the so called &#8216;pointing cone&#8217; (Kranstedt et al.,
2006) and the reference. Our formal model does not intend to solve this ambiguity since it has no effects on
multimodal perception, and certain ambiguities remain unresolved in context similarly to unimodal input. Based
on Lascarides &amp; Stone (2009), we formally regiment the location of the pointing hand with the constant ~c, that
marks the physical location of the tip of the index finger. This combines with the hand&#8217;s shape, orientation and
movement to determine the region ~p actually marked the gesture&#8212;e.g., a stationary stroke with hand shape 1-
index will make ~p a line (or even a cone) that starts at ~c and continues in the direction of the index finger. We will
also be using a function v to map the physical space ~p designated by the gesture to the actual space it denotes.
</p>
<p>We further stated that there is a range of distinct relations between the speech signal and the pointing signal. An
example from Clark (1996) illustrates this: George points at a copy of Wallace Stegner&#8217;s novel Angle of Repose
and says: 1. &#8220;That book is mine&#8221;; 2. &#8220;That man was a friend of mine&#8221;; 3. &#8220;I find that period of American
history fascinating&#8221;. In 1., there is one-to-one correspondence between the gesture space and the physical space
(so v is identity), and the speech referent for &#8220;man&#8221; and the deictic referent are also bound by identity. In 2., the
denotation of the deictic gesture and that of the synchronous speech are not identical since the individual pointed
at is not present at the exact coordinates projected by the pointing fingers, and so the relation would be rather
virtual counterpart. Finally in 3., the deictic gesture&#8217;s denotation is again not equal to that in speech, and they
are connected through depiction. Further ambiguity arises even in the context of the co-occurring speech: does
the pointing gesture while uttering &#8220;We turn right&#8221; identify the event e of turning or the direction x? Our formal
model fully supports ambiguity and partial meaning since we map deictic form to an underspecified meaning
representation whose main variable can resolve to either e or x in context, and we also connect speech and deictic
referents in the grammar through an underspecified relation deictic_rel that is resolvable in context to several
possible values, among them identity, virtual counterpart, depiction, and even paraphrase.
</p>
<p>We have also observed that depending on how the hand is used in the pointing act, deictic gestures can designate
regions of the visible space in two distinct ways: first, the form of the hand, including the location ~c of the tip of
the index finger, identifies the region ~p in visible space that is designated by the gesture as exactly that region that
is taken up by the hand itself. This use of deixis is common in living space descriptions and in direction giving
dialogues; e.g., (1).1
</p>
<p>(1) There&#8217;s like a [NN little] [Nhallway]
Hands are open, vertical, parallel to each other. The speaker places them between the centre and the left
periphery.
</p>
<p>Second, the hand marks a distant region in the visible space to establish a real or virtual identity between the
individual pointed at and the individual referred to in speech as in (2), or to perform a meta-narrative function
such as offering up an instance of an object or acknowledging the addressee&#8217;s statement. In this case, the form of
the hand, including the physical coordinate ~c, establish a region ~p in visible space that does not overlap with the
hand.
</p>
<p>(2) . . . [PN You] guys come from tropical [Ncountries]
Speaker C turns slightly to the right towards speaker A pointing at him using Right Hand (RH) with palm
open up.
</p>
<p>&#167; 3.2 details how these two meanings are reflected in the formal semantic representation of deictic gesture.
It is generally assumed in the literature that deictic gesture combines with the temporally co-occurring speech
signal without considering synchrony outside the temporal alignment (McNeill, 2005). For depicting gestures,
we have shown elsewhere that synchrony is also possible beyond the strict temporal alignment of both signals
(Alahverdzhieva &amp; Lascarides, 2010). For deictic gestures, we have observed that the synchronous semantically
related speech phrase can be a few milliseconds before or after the deictic stroke. In (3), for instance, the gesture
is produced while uttering &#8220;Thank you&#8221; when obviously the denotation of the hand is identical to that of the
computer mouse.
</p>
<p>(3) [NThank] you. [NN I&#8217;ll] take the [Nmouse]
RH is loosely closed, index finger is loosely extended, pointing at the computer mouse
</p>
<p>1For the utterance transcription, we have adopted the following convention: the speech signal aligned with the stroke is underlined, and
the signal aligned with a post-stroke hold is underlined with a curved line. Here we have also included those words that start/end at midpoint
in relation to the gesture phase boundaries. The pitch accented words are shown in square brackets with the accent type in the left corner: PN
(pre-nuclear), NN (non-nuclear) and N (nuclear).</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>KATYA ALAHVERDZHIEVA, ALEX LASCARIDES
</p>
<p>Upon our empirical study of the temporally misaligned occurrences, we learnt that the temporal relaxation is
applicable in cases where the visible space ~p that is designated by the gesture is identical to the space v(~p) that it
denotes (in other words, v is identity). Otherwise, any synchronicity between a deictic gesture and an individual
not present at the exact coordinates of the gesture space would fail to produce the intended logical form in the
specific context. We shall therefore equip our grammar with rules that apply only when there is an identity
function mapping the visible space to space in denotation. This will support an analysis of (3) where the deixis
does not denote the same individual as the pronoun &#8220;I&#8221;. An alternative interpretation would be where the gesture
is synchronous with the temporally co-occurring speech &#8220;Thank you&#8221; in which case the gesture complements on
the speech by introducing a causal relationship of the sort &#8220;Thank you for handing me the mouse&#8221;.
</p>
<p>Having introduced the main challenges that we are dealing with, we now turn to the problem of how deixis and
speech interact at the level of linguistic form (prosody) and meaning.
</p>
<p>3 Speech-Deixis Interaction
</p>
<p>Our motivation for unifying speech and gesture into a grammar stems from the descriptive accounts that gesture
takes an integral part in language production and language comprehension (McNeill, 2005). We thus analyse
deixis in synchrony with speech, as a mapping from form to some (underspecified) meaning in the final logical
form of the utterance. Due to the controversial findings concerning the temporal alignment of speech and ges-
ture, Alahverdzhieva &amp; Lascarides (2010) proposed the following definition of synchrony, which considers only
qualitative factors coming from form and meaning:
</p>
<p>Definition 1 Synchrony. The choice of which linguistic phrase a gesture stroke is synchronous with is guided
by: i. the final interpretation of the gesture in specific context-of-use; ii. the speech phrase whose content is
semantically related to that of the gesture given the value of (i); and iii. the syntactic structure that, with standard
semantic composition rules, would yield an underspecified logical formula supporting (ii) and hence also (i).
The gestural signal and the spoken signal are closely related on both the level of form and of meaning. We
view form as a matter of temporal co-occurrence between the two modalities: there is increasing evidence in the
literature that gesture performance is constrained by the prosody of speech, both speech and gesture are integrated
into a common rhythmical system, and the perception of one mode is dependent on the performance of the other&#8212;
e.g., Loehr (2004), Giorgolo &amp; Verstraten (2008). We shall perform some experiments to validate these claims,
and hence equip our grammar with the constraints on the mapping between form and meaning of co-speech
deictic actions that stem from the relative temporal performance of gesture and speech, and prosody (among other
factors), where these constraints model our empirical findings in multimodal corpora.
</p>
<p>3.1 Prosody
</p>
<p>In this project, we adopt the Autosegmental-Metrical (AM) theory (term coined by Ladd (1996)) for the analysis
of speech prosody. This theory views prosodic prominence as a relational property between two juxtaposed units
where the prominence of unit A is determined by its (strong or weak) relation to unit B.
Based on the findings of a previous prosody study (Calhoun, 2006), we argue that it is not the lower or higher
tune but rather the nuclear accenting that constrains the alignment between gesture and speech. We view nuclear
accenting as the perception of phrase-level prominence which is relative to the metrical structure, and not to the
acoustic properties of the syllables. In the AM model, nuclear prominence results from the following operations:
(a). mapping a syntactic structure to a binary metrical tree; (b). assigning strong (s) or weak (w) prosodic weight
to the nodes in the metrical tree according to the metrical formulation of the Nuclear Stress Rule (Liberman &amp;
Prince, 1977, p.257) as shown in Definition 2; and (c). tracing the path dominated by s nodes.
Definition 2 Nuclear Stress Rule. In a configuration [CAB], if C is a phrasal category, B is strong.
In the default case of broad focus, the metrical structure is right-branching&#8212;that is, the nuclear accent is associated
with the right-most word. For instance, Figure 12 illustrates the metrical tree for &#8220;fasten a cloak&#8221; in its broad
</p>
<p>2The example is taken from Klein (2000)</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>INTEGRATION OF SPEECH AND DEIXIS IN A MULTIMODAL GRAMMAR
</p>
<p>focused reading with the nuclear accent being on the word entirely dominated by s nodes&#8212;&#8220;cloak&#8221;. Liberman &amp;
Prince (1977) call the most prominent element of a given constituent Designated Terminal Element (DTE).
</p>
<p>VP
</p>
<p>V
</p>
<p>fasten
</p>
<p>NP
</p>
<p>Det
</p>
<p>a
</p>
<p>N
</p>
<p>cloak
</p>
<p>&#8658; &#8226;
</p>
<p>w
</p>
<p>fasten
</p>
<p>s
</p>
<p>w
</p>
<p>a
</p>
<p>s
</p>
<p>cloak
</p>
<p>Figure 1: Syntactic Tree and Metrical tree
</p>
<p>Strong nodes on the left of the nuclear accent can also appear, and these are known as pre-nuclear accents. Unlike
the nuclear accents, pre-nuclear accents are signalled by their acoustic properties rather than their relative position
in the metrical tree.
</p>
<p>3.1.1 Empirical Study
</p>
<p>We used empirical data to determine constraints on the interaction between deictic gestures and speech signals.
</p>
<p>Hypothesis 1 Deictic gestures align with the nuclear pitch accents in speech both in the default case of broad
focus, and in case of narrow focus. In case of early pre-nuclear rise, deictic gestures align with the pre-nuclear
pitch accents.
</p>
<p>To test the validity of our hypothesis, we used two multimodal corpora: a 5.53 min recording from the Talkbank
Data,3 and observation IS1008c, speaker C from the AMI corpus.4 The domain of the former is living-space
descriptions and navigation giving, and the latter is a multi-party face-to-face conversation among four people dis-
cussing the design of a remote control. Annotation on both corpora proceeded in two separate stages: annotation
of speech which included word transcription, pitch accents pointing to words and prosodic phrases; and gesture
annotation which included marking of gesture phrases, gesture phases, and also formless moves that beat along
the speech rhythm known as beats. Both annotations were performed independently from each other.
</p>
<p>Prosody Annotation As an annotation tool, we used Praat (Boersma &amp; Weenink, 2003). Our annotation schema
is largely based on the guidelines of the prosody annotation of the Switchboard corpus (Brenier &amp; Calhoun, 2006).
We marked the following layers:
</p>
<p>1. Orthographic Transcription.
2. Pitch Accents. Words were unambiguously associated with at least one accent of the following type: nu-
</p>
<p>clear: the accent of the whole prosodic phrase that is structurally, and not phonetically perceived as the most
important one; pre-nuclear: an early emphatic high rise characterised by a high pitch contour; non-nuclear:
unlike nuclear accents, non-nuclear accents are perceived on the basis of their phonetic properties, and the
rhythm of the sentence (they correspond to &#8216;plain&#8217; or &#8216;regular&#8217; accents in Brenier &amp; Calhoun (2006) and
Calhoun (2006)); none: a non-discernible accent in a phrase (it corresponds to a &#8216;Z&#8217; accent in Brenier &amp;
Calhoun (2006)); ?: uncertainty concerning the presence of an accent.
</p>
<p>3. Prosodic Phrases. A group of words form a prosodic phrase whose type is determined by the break type
after the last word in the phrase. We annotated the following phrases: disfluent: phrase where the break
after the last word would be marked in ToBI with the p diacritic, that is 1p, 2p, 3p correspond to disfluent
phrases; minor: phrase where the break after the last word corresponds to ToBI break 3; major: phrase
where the break after the last word corresponds to ToBI break 4; backchannel: short phrases containing
only fillers such as &#8220;er&#8221;, &#8220;um&#8221;, &#8220;you know&#8221;, etc.
</p>
<p>3The video clip can be found here http://www.talkbank.org/media/Gesture/Cassell/kimiko.mov
4http://corpus.amiproject.org</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>KATYA ALAHVERDZHIEVA, ALEX LASCARIDES
</p>
<p>Gesture Annotation We used the Anvil labelling tool (Kipp, 2001) to annotate the gesture phrases, gesture phases
and beats. Along the lines of Loehr (2004), we annotated gestures for the dominant H1 hand, and for the non-
dominant H2 hand. Bi-handed gestures where the movement of H1 was symmetrical to H2 were coded in H1.
</p>
<p>1. Hand Movement. The annotation of the hand movement proceeded in two main passes. The first pass aimed
at marking the temporal boundaries of all hand movements, and performing a binary classification on them
in terms of communicative&#8211;non-communicative signals. The second pass determined what dimensions the
communicative signals belong to, they being literally depicting, metaphorically depicting or deictic. To stay
consistent with the findings in the literature that a single gesture can have dimensions of, say, depicting and
deictic gestures (McNeill, 2005), our annotation schema permitted for marking gestures belonging to more
than one dimension.
</p>
<p>2. Gesture Phases. This step involved annotating the phases comprising each hand movement: preparation,
pre-stroke hold, stroke, post-stroke hold and retraction. The distinction between pre-stroke holds and post-
stroke holds was often not clear, that is, the form of the hand itself was ambiguous as to whether the signal
belonged to the new gesture phrase and it was thus a pre-stroke hold, or it belonged to the previous gesture
phrase, and it was thus a post-stroke hold. We observed that pre-stroke holds tend to appear with hesitation
pauses while the speaker is looking for some stable verbal form, and so recovery of the temporal cohesion
is anticipated; contrarily, post-stroke holds are more likely to occur with fluent speech when the speaker
elaborates on the content reached during the stroke.
</p>
<p>3. Beat. Beat movements were marked in a separate layer so as to study whether they always superimpose
other gestural dimensions, or pure beats also occur.
</p>
<p>Past annotation tasks of the Switchboard corpus (Calhoun, 2006) and of the multimodal corpus of Loehr (2004)
have shown that the annotation of accents and boundaries is reliable (see Table 1), and also the annotation of
gesture dimensions (see Table 2).
</p>
<p>All Types +/-
Accents 0.800 0.800
Boundaries 0.889 0.910
Words (752)
</p>
<p>Table 1: Inter-coder reliability for
accents and phrase boundaries &amp;
for the presence/absence (=/-) of an
accent/boundary in kappa (Calhoun,
2006)
</p>
<p>Coding Segmentation
Hand movement 0.8536/0.8994 0.8502/0.8659
Deictic gesture 0.8605/0.8994 0.8502/0.8659
Literally depicting 0.8663/0.8916 0.8502/0.8659
Metaphorically depicting 0.8221/0.8623 0.8502/0.8659
Gesture phase 0.662/0.7 0.8864/0.8971
Beat 0.6599/0.8203
</p>
<p>Table 2: Inter-coder reliability for gesture coding agreement &amp;
segmentation agreement in Cohen&#8217;s kappa/corrected kappa
</p>
<p>Multimodal Corpora in NXT The annotated corpora were converted into Nite XML Technology (NXT) format
(Carletta et al., 2005), (Calhoun et al., 2010) which allows for querying a corpus as a coherent set and extract-
ing information from it by exploring the relations between the annotation layers. A corpus in NXT consists of
&#8216;observations&#8217;&#8212;our two video recordings&#8212;and annotations associated with it&#8212;orthographic transcriptions, pitch
accents, prosodic phrases, gesture phrases, gesture phases and beats. Each data object is necessarily equipped
with timestamps which are synchronised with the video and/or audio signal.
</p>
<p>Data objects can be bound either by structural or by temporal relations which is specified in a meta-data file
containing the annotation schema of the corpus. The type of relation also determines the query that can be
executed onto these objects. The annotation of each data object is stored in a separate XML file; any relations
between the annotation objects are defined in terms of stand-off links between the elements. Figure 2 illustrates
the relation between the &#8216;accents&#8217; and &#8216;words&#8217; layers: the accent&#8217;s attribute nite:pointer serves as a pointer to the
unique nite:id of the relevant word. In this way, we can elegantly capture accents not overlapping a word, accents
associated with two words, and also words associated with two accents.
</p>
<p>We further specified the relationships between gestures and gesture phases, and between prosodic phrases and
words as parent-child relations. This choice of representation is consistent with the essence of prosodic phrases
and gesture phrases: prosodic phrases are made up by a certain number of words, and so the beginning of the
first word aligns with the beginning of the prosodic phrase, and the end of the last word aligns with the end of the
prosodic phrase. The same mechanism applies to gestures which are made up by at least one gesture phase. We</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>INTEGRATION OF SPEECH AND DEIXIS IN A MULTIMODAL GRAMMAR
</p>
<p>T01.accents.xml
. . .
</p>
<p>&lt;accent nite:id=&quot;T01.accent.15&quot; starttime=&quot;14.79498&quot; endtime=&quot;14.85108&quot; type=&quot;pre-nuclear&quot;&gt;
&lt;nite:pointer href=&quot;T01.words.xml#id(T01.words.29)&quot; role=&quot;at&quot; /&gt;
</p>
<p>&lt;/accent&gt;
&lt;accent nite:id=&quot;T01.accent.16&quot; starttime=&quot;15.78731&quot; endtime=&quot;15.86445&quot; type=&quot;nuclear&quot;&gt;
</p>
<p>&lt;nite:pointer href=&quot;T01.words.xml#id(T01.words.31)&quot; role=&quot;at&quot; /&gt;
&lt;/accent&gt;
. . .
</p>
<p>T01.words.xml
. . .
</p>
<p>&lt;word nite:id=&quot;T01.words.29&quot; starttime=&quot;14.74&quot; endtime=&quot;15.14&quot; orth=&quot;enter&quot; /&gt;
&lt;word nite:id=&quot;T01.words.30&quot; starttime=&quot;15.14&quot; endtime=&quot;15.515&quot; orth=&quot;my&quot; /&gt;
&lt;word nite:id=&quot;T01.words.31&quot; starttime=&quot;15.515&quot; endtime=&quot;16.315&quot; orth=&quot;apartment&quot; /&gt;
. . .
</p>
<p>Figure 2: NXT Coding of Accents Associated with Words
</p>
<p>forego any details about the specification of beats since they are not represented in a structural relationship with
other layers.
</p>
<p>3.1.2 Results and Discussion
</p>
<p>In relation to our hypothesis, we searched for the types of accents overlapping a deictic gesture stroke. The cor-
pora contained 87 deictic strokes (65 for the Talkbank, and 22 for AMI). 86 of them&#8212;that is, 98.85%&#8212;overlapped
a nuclear and/or a pre-nuclear accented word. Strokes overlapping a combination of non-nuclear and nuclear ac-
cented words were also common. Essentially, the empirical analysis confirmed the expected alignment between
the nuclear prominent word (not simply the nuclear accent) and the gesture stroke both in case of broad focus, and
in case of narrow-focused utterances. The following two utterances illustrate our findings: (4) is a broad-focused
utterance with the nuclear accent being on the right-most word. Utterance (5), which is a continuation of (4),
displays narrow focus with the nuclear accent pointing to the first word of the prosodic phrase&#8211;&#8220;left&#8221;. The interac-
tion between prosodic prominence and gesture stroke appears to be on the level of information structure: nuclear
prominence, along with gesture stroke aligns with the focused (kontrastive)5 elements that push the communica-
tion forward, and not with those available from the background. This prediction has its grounds in the descriptive
literature of gesture where &#8220;a break in the continuity&#8221; (Giv&#243;n, 1985) of the narrative implies &#8220;highest degree of
gesture materialisation&#8221; (McNeill, 2005, p.55).
</p>
<p>(4) I keep [Ngoing] until I [NN hit] Mass [NAve], I think
Right arm is bent in the elbow at a 90-degree angle, RH is loosely closed and relaxed, fingers point
forward. Left arm is bent at the elbow, held almost parallel to the torso, palm is open vertical facing
forward, finger tips point to the left
</p>
<p>(5) And then I [N turn] [pause] [N left] on
::::::::::
</p>
<p>[NN Mass]
::::
</p>
<p>Ave
Hands are held in the same position as in (4), then along with &#8220;left&#8221; RH moves to the left periphery over
LH, RH is vertically open
</p>
<p>The single counterexample concerns the first gesture in (6): at this stage we remain agnostic as to why this
misalignment occurred. As long as it is not a recurrent feature found over a larger amount of data, we would
rather attribute it to impreciseness of annotation than to a general phenomenon to be considered in a model of
multimodal actions.
</p>
<p>(6) [NN Between] the living [N room] and [pause] the [N study] and the [pause] [Nbedroom]
Hands are in the front centre, bent in elbows, palms are open, vertical, facing each other; along with
&#8220;between&#8221;, they perform a loose sweeping movement to the right periphery, then LH moves away to the
</p>
<p>5In the Information Structure literature kontrast designates &#8220;parts of the utterance&#8212;actually, words&#8212;which contribute to distinguishing its
actual content from alternatives the context makes available.&#8221; (Kruijff-Korbayov&#225; &amp; Steedman, 2003)</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>KATYA ALAHVERDZHIEVA, ALEX LASCARIDES
</p>
<p>left upper centre with palm vertical, finger tips oriented forward; along with &#8220;the study&#8221;, RH is moved in
parallel to LH, as if both hands place a rectangular object in space
</p>
<p>Further to this, we looked at all-new information utterances with an initial strong acoustic pitch and then a nuclear
accent on the right-most element. In these utterances, the stroke was performed along with the initial pre-nuclear
accent, and there might have been a post-stroke hold on the other components of the utterance. This is exemplified
in (7) where an initial meaningful speech segment aligns with the stroke, and then the content is elaborated while
holding the hands in an expressive position.
</p>
<p>(7) I [PN enter]
:::
</p>
<p>my
::::::::::::
</p>
<p>[N apartment]
Hands are in centre, palms are open vertically, finger tips point forward; along with &#8220;enter&#8221; they move
briskly downwards.
</p>
<p>We use the results of this statistical analysis to define constraints on the temporal overlap between deictic gesture
and speech. Also, we need to explore whether any semantic relation can be established between the temporally
aligned signals.
</p>
<p>3.2 Mapping Deixis Form to Deixis Meaning
</p>
<p>Following previous research (Johnston, 1998), (Kopp et al., 2004), the form of the pointing hand is represented
using typed feature structures, where each feature value pair corresponds to an aspect of form. We use fine-grained
an analysis as possible: we consider that the shape of the hand, the orientation of the palm and the fingers, the
hand movement, and also the location of the hand in the spatio-temporal coordinates ~c are the distinct classes
of form that potentially have semantic effects, e.g., the shape of the hand influences the mapping from ~c to ~p.
Moreover, this form representation captures the fact that the different attributes composing deictic gesture&#8217;s form
are not hierarchically ordered, but are rather a flat list. Figure 3 gives the form representation of the gesture
in (1)&#8212;the value ~c, which identifies the spatio-temporal coordinates of the hand, together with the other values,
serve to identify the region ~p designated by the gesture&#8217;s content (Lascarides &amp; Stone, 2009); as explained in
&#167; 2.2, a pointing gesture (with hand shape 1-index) will make ~p denote a cone or line that starts at hand-location
~c and whose direction is the same value as finger-direction (Kranstedt et al., 2006). Note also that the gesture is
typed as communicative_gesture_deictic to distinguish between form features contributed by depicting gestures,
and those contributed by pointing gestures.&#63726;
</p>
<p>&#63727;&#63727;&#63727;&#63728;
communicative_gesture_deictic
HAND-SHAPE: open-flat
PALM-ORIENTATION: vertical
FINGER-ORIENTATION: forward
HAND-MOVEMENT: away-centre-left
HAND-LOCATION: ~c
</p>
<p>&#63737;
&#63738;&#63738;&#63738;&#63739;
</p>
<p>Figure 3: TFS Representation of Form of Deictic Gesture
</p>
<p>As a semantics description language we use Robust Minimal Recursion Semantics (RMRS) since it is highly flex-
ible about the semantic underspecification it supports: in RMRS, one can leave the main predicate underspecified
until resolved by further context. In this way, we can elegantly capture the fact that the form of a deictic gesture
alone does not fully determine its content &#8212; e.g., it does not determine whether the gesture denotes an individual
or an event, but rather contextual information is needed as well to infer this aspect of the gesture&#8217;s (pragmatic)
interpretation. Defining flat semantics in RMRS involves defining a set of Elementary Predications (EPs). Each EP
is associated with a label li that ultimately identifies the scopal position of the predicate in the context-resolved
logical form. Shared labels are also possible, and they mark implicit conjunction as in intersective modifiers. Each
EP is also associated with a unique anchor ai, which serves as its locus for specifying arguments to the EP &#8212; e.g.,
ARG2(a, x) means that the second argument to the EP whose anchor is a is the individual x. The absence of such
ARG relations in the RMRS thus serves to underspecify the arguments to predicates and even the predicate&#8217;s arity.
Holes (hi) are used to represent scopal arguments whose value is not fully determined by syntax. The admissible
pluggings are constrained by equality conditions (=q) between holes and labels (hi =q li means that only 0 or
more quantifiers intervene between the scopal positions). Finally, a top label h0 is added for the whole formula.
&#167; 2.2 detailed the two distinct functions of deictic gestures. We will now present their compositional semantics as
follows:</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>INTEGRATION OF SPEECH AND DEIXIS IN A MULTIMODAL GRAMMAR
</p>
<p>1. Hand as reference. The speaker here points to an individual/event represented by the hand which is located
at the spatial coordinate ~p designated by the finger tips often, but not necessarily, in relation to another
individual available from the discourse. The form features of the pointing hand further constrain the set
of possible relations between gesture and speech, e.g., an open hand supine used for turn coordination can
resolve to a metatalk relation (Lascarides &amp; Stone, 2009) &#8212; roughly put, the gesture can have a meaning
that can be paraphrased by the parenthetical phrase &#8220;I am telling you&#8221;.
The RMRS representation of the gesture in (1) is shown in Figure 4. Following Lascarides &amp; Stone (2009),
this RMRS semantics says that the pointing hand provides the spatial reference of an underspecified referent
i (an individual or an event) at some position in the physical space v(~p). In context, the underspecified
variable i may resolve to an individual x as in (1), or to an event e as in (7). To stay consistent with the
findings in the descriptive literature, namely that the shape of the pointing hand is associated with a specific
meaning (Kendon, 2004), we map each form feature-value pair to a two-place predicate. Their formal
treatment is similar to the treatment of intersective modifiers in the English Resource Grammar (ERG) in
that they share labels with the main predicate sp_ref . Again for consistency with ERG where individuals
are bound by quantifiers, there is a quantifier outscoping the referent introduced by the deictic gesture.
Following Lascarides &amp; Stone (2009), we use the G operator so as to guarantee that individuals referred
in speech cannot be co-referred to individuals introduced in gesture. To obtain this, G must outscope all
gesture predications (formalised in terms of =q equality conditions).
</p>
<p>2. Reference is the region marked by the hand. The hand here also points to an underspecified reference
i located at v(~p) but unlike the previous function, the hand shape denotes not the reference itself but the
region marked by it. The semantics of the gesture in (2) is shown in Figure 5, and it is similar to the one
displayed in Figure 4 with the only difference being that it is the region that is modified by the various
gesture form-features. Since the rest of the predications remain the same, we forego any details about them.
</p>
<p>l0 : a0 : [G](h1)
l1 : a1 : deictic_q(i) RSTR(a1, h2) BODY (a1, h3)
l2 : a2 : sp_ref(i) ARG1(a2, v(~p))
l2 : a3 : hand_shape_open_flat(e0) ARG1(a3, i)
l2 : a4 : palm_orient_vertical(e1) ARG1(a4, i)
l2 : a5 : finger_orient_forward(e3) ARG1(a5 , i)
l2 : a6 : hand_move_away_centre_left(e5) ARG1(a6, i)
h1 =q l1; h1 =q l2; h2 =q l2
</p>
<p>Figure 4: RMRS for Hand as Reference
</p>
<p>l0 : a0 : [G](h1)
l1 : a1 : deictic_q(i) RSTR(a1, h2) BODY (a1, h3)
l2 : a2 : sp_ref(i) ARG1(a2, v(~p))
l2 : a3 : RH_palm_orient_vertical(e1) ARG1(a3 , ~p)
l2 : a4 : RH_finger_orient_forward(e2) ARG1(a4, ~p)
l2 : a5 : RH_hand_move_away_body_left(e3) ARG1(a5, ~p)
h1 =q l1; h1 =q l2; h2 =q l2
</p>
<p>Figure 5: RMRS for the Region Marked by the Hand
</p>
<p>4 Rules for Combining Deixis and Speech in the Grammar
</p>
<p>We intend to augment the existing wide-coverage grammar for English&#8212;the English Resource Grammar ERG&#8212;
with construction rules for combining speech and gesture. This task involves specifying the prosodic component
in the grammar (we shall be using the AM theory), and also interfacing it with the syntax-semantics component.
We formally regiment our findings about the deixis-prosody interaction (&#167; 3.1) into the following basic construc-
tion rules:
</p>
<p>Definition 2.1 Deictic gesture attaches to the temporally overlapping nuclear/pre-nuclear head word.
</p>
<p>Definition 2.2 Deictic gesture attaches to the temporally overlapping nuclear/pre-nuclear head word after it had
been combined with the arguments and/or modifiers to the head.
The motivation to include the latter stems from the fact that semantically the deictic signal is not strictly con-
strained to its temporally co-occurring word but rather it can be linked to a larger phrase. For instance, in (7),
there is no information coming from the form of the hand, nor from its relative timing that it should be attached
to &#8220;enter&#8221; only, and not to &#8220;enter my apartment&#8221;, in which case the form of the hand would be related to the
rectangular shape of, say, an entrance door to an apartment. Intuitively in this case, the gesture directs not only to
the point of entering the house, but also to the entrance door which by the hand shape is rectangular.
</p>
<p>The syntactic structure is derived in parallel with the prosodic one, and so the syntactic component would consist
either of a single head word without further constraints on its syntactic category, or a larger phrase it being a
head-argument, a head-modifier phrase, or an entire utterance. Generally speaking, a deictic gesture cannot be</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>KATYA ALAHVERDZHIEVA, ALEX LASCARIDES
</p>
<p>combined with a non-prosodic word. We will come back to this point a bit later, when we will see that some
exceptions to this rule can also arise. Finally, the semantic component uses the RMRS representation in &#167; 3.2.
Semantic composition with RMRS (Copestake et al., 2001), (Copestake et al., 2005) is monotonic, ensuring that
the semantic representations of the daughters are always subsumed by that of the mother. For each phrase, one
specifies semantic entities (sements) of the following parts: (1). Top: the global label containing the whole
formula. During composition, the top labels of the daughters are equated with the top label of the mother to
demonstrate the derivation of a single logical form; (2). Hook: placeholder that records the semantic value of
the formula. It contains (a). local top: the label containing an EP. For instance, in Figure 4 the local top of
l2 : a2 : sp_ref(i) ARG1(a2, v(~p)) is l2; (b). semantic index (i1, i2 . . . in): it indicates what the phrase is about
and has two subtypes: events (e1, e2 . . . en) and individuals (x1, x2 . . . xn). It is obtained by co-indexation with
the topmost EP. For instance, in Figure 4, the semantic index of the phrase is i obtained by co-indexing it with the
main variable of sp_ref(i); (3). Slots: resources that need to be consumed so that a functor becomes semantically
saturated; (4). Rels: a bag of EPs; (5). Equality constraints (=q): scopal constraints indicating the admissible
plugging of a subformula into a hole.
</p>
<p>To summarise, an RMRS sement is: &#12296;Top [ltop, i]{slots}{rels} [=q]&#12297;. Semantic composition of a sementM =
op(sementD1, sementD2) involves the following operations: 1. making Top of sementM = sementD1 =
sementD2; 2. making the hook of sementM the hook of sementD1; 3. making the remaining slots of sementD1
and sementD2 the slots of sementM ; 4. making the rels and hcons of sementM the union of those of the
daughters.
</p>
<p>As argued in &#167; 2.2, deictic gesture always relates with the synchronous speech signal through some sort of relation;
e.g., identity, virtual counterpart or a paraphrase relation. Based on Lascarides &amp; Stone (2009), the construction
rule therefore introduces an underspecified relation deictic_rel(i1, i2) between the semantic index i1 of the deictic
gesture and the semantic index i2 of speech. How this relation resolves is a matter of discourse context. Similarly
to the treatment of intersective modification in language, deictic_rel shares the same label as the speech head
daughter since it further restricts the individual/event introduced in speech. In so doing, any quantifier outscoping
the head would also outscope this relation. This is similar to the treatment of appositives in ERG.
</p>
<p>With this machinery at hand, let us now turn to the derivation of utterance (1): the deictic gesture overlaps the
nuclear-accented prosodic word &#8220;hallway&#8221;, and hence we could build a single situated noun out of &#8220;hallway&#8221; and
deixis as demonstrated in Figure 6. In semantics, we need to extend the RMRS representation in Figure 4 with a
top label, hook and slots as follows (for the sake of readability, we gloss the semantic predications contributed by
the deictic form features as l2 : a3 : deictic_eps(e0) ARG1(a3, i)):
&lt; h0, [l0, a0, i] , {}
{l0 : a0 : [G](h1)
l1 : a1 : deictic_q(i) RSTR(a1, h2) BODY (a1, h3)
l2 : a2 : sp_ref(i) ARG1(a2, v(~p))
l2 : a3 : deictic_eps(e0) ARG1(a3, i)}
[h1 =q l1; h1 =q l1; h2 =q l2] &gt;
</p>
<p>The top label of the whole formula is h0 and in derivation it is made identical with that of the mother. Further, the
local top is identified with the label of the G operator which contains all other predications. The semantic index of
the deictic gesture is the underspecified variable i introduced by the sp_ref predicate that in composition resolves
to x1. Finally, we assign no slots to the formula.
</p>
<p>As argued above, the form of the deictic signal is not sufficient to decide whether the hand refers to &#8220;hallway&#8221;,
to &#8220;little hallway&#8221; or even to &#8220;a little hallway&#8221;, and so our grammar does not impose constraints on the syntactic
phrase, and is thus able to generate all these combinations. Prosodically, we integrate deixis into a metrical
tree where the prosodically prominent element, the DTE, is &#8220;hallway&#8221; , and syntactically into a head-modifier
construction with &#8220;hallway&#8221; being the head daughter. Since deictic_rel shares the same label as the head, when
combining the NP &#8220;a little hallway&#8221; and deixis, both the head noun and the deictic relation would appear within
the restrictor of the quantifier. The semantic composition remains the same as above.
</p>
<p>We shall now look at some exceptions that are not covered by the temporal alignment constraint and the nuclear
prominence constraint. We saw that in (3) there exists an obvious misalignment between the semantically related
speech and deixis signals. Similarly in (8), the deixis denotation is identical to the denotation of &#8220;she&#8221; despite it
not being prosodically prominent.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>INTEGRATION OF SPEECH AND DEIXIS IN A MULTIMODAL GRAMMAR
</p>
<p>N
</p>
<p>&#9001;
PHON 1
</p>
<p>SEM
</p>
<p>&#63729;&#63732;&#63732;&#63732;&#63732;&#63732;&#63732;&#63730;
&#63732;&#63732;&#63732;&#63732;&#63732;&#63732;&#63732;&#63731;
</p>
<p>h0
</p>
<p>l4 : a11 : deictic_rel(e) ARG1(a11, x2) ARG2(a11, x1)
</p>
<p>N rel
</p>
<p>Dxrel
</p>
<p>&#63729;&#63730;
&#63731;
l0 : a0[G](h1)
</p>
<p>l1 : a1 : deictic_q(x1) RSTR(a1, h2) BODY (a1, h3)
</p>
<p>l2 : a2 : sp_ref(x1) ARG1(a2, v(~p))
</p>
<p>l2 : a3 : deictic_eps(e0) ARG1(a3, x1)
</p>
<p>&#63740;&#63741;
&#63742;
</p>
<p>Dx=q
</p>
<p>&#63740;&#63732;&#63732;&#63732;&#63732;&#63732;&#63732;&#63741;
&#63732;&#63732;&#63732;&#63732;&#63732;&#63732;&#63732;&#63742;
</p>
<p>&#9002;
</p>
<p>N
</p>
<p>hallway &#9001;PHON 1 prosodic-word
SEM
</p>
<p>{
h0
</p>
<p>Nrel
</p>
<p>{
l4 : a10 : hallway(x2)
</p>
<p>}}
&#9002;
</p>
<p>Dx
</p>
<p>&#9001;
SEM
</p>
<p>&#63729;&#63732;&#63732;&#63732;&#63732;&#63732;&#63732;&#63732;&#63732;&#63730;
&#63732;&#63732;&#63732;&#63732;&#63732;&#63732;&#63732;&#63731;
</p>
<p>h0
</p>
<p>Dxrel
</p>
<p>&#63729;&#63730;
&#63731;
l0 : a0[G](h1)
</p>
<p>l1 : a1 : deictic_q(i) RSTR(a1, h2) BODY (a1, h3)
</p>
<p>l2 : a2 : sp_ref(i) ARG1(a2, v(~p))
</p>
<p>l2 : a3 : deictic_eps(e0) ARG1(a3, i)
</p>
<p>&#63740;&#63741;
&#63742;
</p>
<p>Dx=q
</p>
<p>{
h1 =q l1
</p>
<p>h1 =q l2
</p>
<p>h2 =q l2
</p>
<p>}
</p>
<p>&#63740;&#63732;&#63732;&#63732;&#63732;&#63732;&#63732;&#63732;&#63732;&#63741;
&#63732;&#63732;&#63732;&#63732;&#63732;&#63732;&#63732;&#63742;
</p>
<p>&#9002;
</p>
<p>Figure 6: Derivation Tree for Deictic Gesture and the N &#8220;hallway&#8221;
</p>
<p>(8) And a as she [N said], it&#8217;s an environmentally friendly uh material
Speaker C extends right hand palm supine towards the speaker B
</p>
<p>To cope with these exceptions, we studied all utterances where the semantically preferred attachment of the de-
ictic gesture is an element beyond its temporal performance and/or a non-prosodically prominent element. This
temporal/prosodic relaxation is a matter of making individuals in the surrounding space salient and it is thus nec-
essary only in utterances where the gesture&#8217;s denotation is physically present in the visible space, that is, there is
an identity between the physical space that the hand points at and the actual denotation of the gesture&#8217;s referent.
Of course, this does not mean that deictic_rel would always in this case resolve to identity. Let us illustrate this
by reusing the example from Clark (1996) in &#167; 2.2: when pointing to the novel while uttering &#8220;This man was a
friend of mine&#8221; there is an identity between the visible space that the hand points at and the denotation of the
gesture since the novel is salient in the physical space gesture points at. However, the denotation of the gesture is
not identical to the one of speech, and we therefore claimed that the relation between speech and deixis is rather
depiction. In our grammar, we therefore spell out the following rule:
Definition 2.3 Deictic gesture attaches to an item (prosodically prominent or non-prominent) whose temporal
performance is adjacent to that of the gesture if the mapping v resolves to identity.
Importantly, this relaxation is not applicable in cases where the hand serves as an abstract reference pointing to an
object not present in the communicative act. If the gesture in (4) was related to the speech head daughter &#8220;I&#8221;, the
logical form would fail to resolve.
</p>
<p>5 Conclusions
</p>
<p>In this paper, we demonstrated that well-established methods from linguistics are sufficient to provide the form-
meaning mapping of multimodal communicative actions consisting of speech and deictic gesture. This goal was
achieved by integrating them into a multimodal grammar thereby using constraints coming from the form of the
speech signal, the form of the gesture signal and their relative temporal performance so as to map them to a single
meaning representation in the final logical form of the utterance. This paper contributed to the existing resources
by setting the theoretical framework for a multimodal grammar, and also by extending the existing corpora with
prosody and gesture annotation in the NXT format which can further be used for various studies of multimodal
communication.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>KATYA ALAHVERDZHIEVA, ALEX LASCARIDES
</p>
<p>ALAHVERDZHIEVA K. &amp; LASCARIDES A. (2010). Analysing speech and co-speech gesture in constraint-based grammars.
In S. M&#220;LLER, Ed., The Proceedings of the 17th International Conference on Head-Driven Phrase Structure Grammar, p.
6&#8211;26, Stanford: CSLI Publications.
BOERSMA P. &amp; WEENINK D. (2003). &#8216;Praat:doing phonetics by computer&#8217;. http://www.praat.org.
BRENIER J. &amp; CALHOUN S. (2006). Switchboard prosody annotation scheme. Department of Linguistics, Stanford Univer-
sity and ICCS, University of Edinburgh. Internal publication.
CALHOUN S. (2006). Information Structure and the Prosodic Structure of English: a Probabilistic Relationship. University
of Edinburgh. PhD Thesis.
CALHOUN S., CARLETTA J., BRENIER J., MAYO N., JURAFSKY D., STEEDMAN M. &amp; BEAVER D. (2010). The nxt-
format switchboard corpus: a rich resource for investigating the syntax, semantics, pragmatics and prosody of dialogue.
Language Resources and Evaluation, 44, 387&#8211;419.
CARLETTA J., EVERT S., HEID U. &amp; KILGOUR J. (2005). The nite xml toolkit: Data model and query language. Language
Resources and Evaluation, 39, 313&#8211;334.
CLARK H. H. (1996). Using Language. Cambridge: Cambridge University Press.
COPESTAKE A., FLICKINGER D., SAG I. &amp; POLLARD C. (2005). Minimal recursion semantics: An introduction. Journal
of Research on Language and Computation, 3(2&#8211;3), 281&#8211;332.
COPESTAKE A., LASCARIDES A. &amp; FLICKINGER D. (2001). An algebra for semantic construction in constraint-based
grammars. In Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics (ACL/EACL 2001),
p. 132&#8211;139, Toulouse.
GIORGOLO G. &amp; VERSTRATEN F. (2008). Perception of speech-and-gesture integration. In Proceedings of the International
Conference on Auditory-Visual Speech Processing 2008, p. 31&#8211;36.
GIULIANI M. &amp; KNOLL A. (2007). Integrating multimodal cues using grammar based models. In HCI (6), p. 858&#8211;867.
GIV&#211;N T. (1985). Iconicity, Isomorphism and Non-arbitrary Coding in Syntax. In J. HAIMAN, Ed., Iconicity in Syntax, p.
187&#8211;219. Amsterdam: John Benjamins.
GOFFMAN E. (1963). Behavior in Public Places: Notes on the Social Organization of Gatherings. The Free Press.
JOHNSTON M. (1998). Multimodal language processing. In Proceedings of the International Conference on Spoken Lan-
guage Processing (ICSLP).
KENDON A. (2004). Gesture. Visible Action as Utterance. Cambridge: Cambridge University Press.
KIPP M. (2001). Anvil &#8212; a generic annotation tool for multimodal dialogue. In Proceedings of the 7th European Conference
on Speech Communication and Technology (Eurospeech), Aalborg: Georgetown University.
KLEIN E. (2000). Prosodic constituency in hpsg. In Grammatical Interfaces in HPSG, Studies in Constraint-Based Lexical-
ism, p. 169&#8211;200: CSLI Publications.
KOPP S., TEPPER P. &amp; CASSELL J. (2004). Towards integrated microplanning of language and iconic gesture for multimodal
output. In ICMI &#8217;04: Proceedings of the 6th international conference on Multimodal interfaces, p. 97&#8211;104, New York, NY,
USA: State College, PA, USA ACM.
KRANSTEDT A., L&#220;CKING A., PFEIFFER T., RIESER H. &amp; WACHSMUTH I. (2006). Deixis: How to determine demon-
strated objects using a pointing cone. In S. GIBET, N. COURTY &amp; J.-F. KAMP, Eds., Gesture in Human-Computer Interac-
tion and Simulation, volume 3881 of Lecture Notes in Computer Science, p. 300&#8211;311. Springer Berlin / Heidelberg.
KRUIJFF-KORBAYOV&#193; I. &amp; STEEDMAN M. (2003). Discourse and information structure. Journal of Logic, Language and
Information, 12, 249&#8211;259.
LADD R. D. (1996). Intonational Phonology (first edition). Cambridge University Press.
LASCARIDES A. &amp; STONE M. (2009). A formal semantic analysis of gesture. Journal of Semantics.
LIBERMAN M. &amp; PRINCE A. (1977). On stress and linguistic rhythm. Linguistic Inquiry, 8(2), 249&#8211;336.
LOEHR D. (2004). Gesture and Intonation. Washington DC: Georgetown University. Doctoral Dissertation.
MCNEILL D. (2005). Gesture and Thought. Chicago: University of Chicago Press.
OVIATT S. L., DEANGELI A. &amp; KUHN K. (1997). Integration and synchronization of input modes during multimodal
human-computer interaction. CHI, p. 415&#8211;422.</p>

</div></div>
</body></html>