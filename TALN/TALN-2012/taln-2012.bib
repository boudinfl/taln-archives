@proceedings{TALN:2012,
  editor    = {Antoniadis, Georges and Blanchon, Hervé},
  title     = {Actes de la 19e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2012},
  address   = {Grenoble, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2012}
}

@inproceedings{minard-ligozat-grau:2012:TALN,
  author    = {Minard, Anne-Lyse and Ligozat, Anne-Laure and Grau, Brigitte},
  title     = {Simplification de phrases pour l'extraction de relations},
  booktitle = {Actes de la 19e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2012},
  address   = {Grenoble, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {1--14},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2012/taln-2012-long-001},
  language  = {french},
  note      = {Automatic Information Extraction in the Medical Domain by Cross-Lingual Projection},
  resume    = {L’extraction de relations par apprentissage nécessite un corpus annoté de très grande taille pour couvrir toutes les variations d’expressions des relations. Pour contrer ce problème, nous proposons une méthode de simplification de phrases qui permet de réduire la variabilité syntaxique des relations. Elle nécessite l’annotation d’un petit corpus qui sera par la suite augmenté automatiquement. La première étape est l’annotation des simplifications grâce à un classifieur à base de CRF, puis l’extraction des relations, et ensuite une complétion automatique du corpus d’entraînement des simplifications grâce aux résultats de l’extraction des relations. Les premiers résultats que nous avons obtenus pour la tâche d’extraction de relations d’i2b2 2010 sont très encourageants.},
  abstract  = {Machine learning based relation extraction requires large annotated corpora to take into account the variability in the expression of relations. To deal with this problem, we propose a method for simplifying sentences, i.e. for reducing the syntactic variability of the relations. Simplification requires the annotation of a small corpus, which will be automatically augmented. The process starts with the annotation of the simplification thanks to a CRF classifier, then the relation extraction, and lastly the automatic completion of the training corpus for the simplification through the results of the relation extraction. The first results we obtained for the task of relation extraction of the i2b2 2010 challenge are encouraging.},
  motscles  = {Extraction de relations, simplification de phrases, apprentissage automatique},
  keywords  = {Relation extraction, sentence simplification, machine learning},
}

@inproceedings{benabacha-zweigenbaum-max:2012:TALN,
  author    = {Ben Abacha, Asma and Zweigenbaum, Pierre and Max, Aurélien},
  title     = {Extraction d'information automatique en domaine médical par projection inter-langue : vers un passage à l'échelle},
  booktitle = {Actes de la 19e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2012},
  address   = {Grenoble, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {15--28},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2012/taln-2012-long-002},
  language  = {french},
  note      = {Automatic Information Extraction in the Medical Domain by Cross-Lingual Projection},
  resume    = {Cette recherche est issue de notre volonté de tester de nouvelles méthodes automatiques d’annotation ou d’extraction d’information à partir d’une langue L1 en exploitant des ressources et des outils disponibles pour une autre langue L2. Cette approche repose sur le passage par un corpus parallèle (L1-L2) aligné au niveau des phrases et des mots. Pour faire face au manque de corpus médicaux français annotés, nous nous intéressons au couple de langues (françaisanglais) dans le but d’annoter automatiquement des textes médicaux en français. En particulier, nous nous intéressons dans cet article à la reconnaissance des entités médicales. Nous évaluons dans un premier temps notre méthode de reconnaissance d’entités médicales sur le corpus anglais. Dans un second temps, nous évaluons la reconnaissance des entités médicales du corpus français par projection des annotations du corpus anglais. Nous abordons également le problème de l’hétérogénéité des données en exploitant un corpus extrait du Web et nous proposons une méthode statistique pour y pallier.},
  abstract  = {This research stems from our willingness to test new methods for automatic annotation or information extraction from one language L1 by exploiting resources and tools available to another language L2. This approach involves the use of a parallel corpus (L1-L2) aligned at the level of sentences and words. To address the lack of annotated medical French corpus, we focus on the French-English language pair to annotate automatically medical French texts. In particular, we focus in this article on medical entity recognition. We evaluate our medical entity recognition method on the English corpus and the projection of the annotations on the French corpus. We also discuss the problem of scalability since we use a parallel corpus extracted from the Web and propose a statistical method to handle heterogeneous corpora.},
  motscles  = {Extraction d’information, projection d’annotation, reconnaissance des entités médicales, apprentissage},
  keywords  = {Automatic Information Extraction, Annotation Projection, Medical Entity Recognition, Machine Learning},
}

@inproceedings{jeanlouis-besancon-ferret:2012:TALN,
  author    = {Jean-Louis, Ludovic and Besançon, Romaric and Ferret, Olivier},
  title     = {Une méthode d'extraction d'information fondée sur les graphes pour le remplissage de formulaires},
  booktitle = {Actes de la 19e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2012},
  address   = {Grenoble, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {29--42},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2012/taln-2012-long-003},
  language  = {french},
  note      = {A Graph-Based Method for Template Filling in Information Extraction},
  resume    = {Dans les systèmes d’extraction d’information sur des événements, une tâche importante est le remplissage automatique de formulaires regroupant les informations sur un événement donné à partir d’un texte non structuré. Ce remplissage de formulaire peut s’avérer difficile lorsque l’information est dispersée dans tout le texte et mélangée à des éléments d’information liés à un autre événement similaire. Nous proposons dans cet article une approche en deux étapes pour ce problème : d’abord une segmentation du texte en événements pour sélectionner les phrases relatives au même événement ; puis une méthode de sélection dans les phrases sélectionnées des entités liées à l’événement. Une évaluation de cette approche sur un corpus annoté de dépêches dans le domaine des événements sismiques montre un F-score de 72% pour la tâche de remplissage de formulaires.},
  abstract  = {In event-based Information Extraction systems, a major task is the automated filling from unstructured texts of a template gathering information related to a particular event. Such template filling may be a hard task when the information is scattered throughout the text and mixed with similar pieces of information relative to a different event. We propose in this paper a two-step approach for template filling : first, an event-based segmentation is performed to select the parts of the text related to the target event ; then, a graph-based method is applied to choose the most relevant entities in these parts for characterizing the event. Using an evaluation of this model based on an annotated corpus for earthquake events, we achieve a 72% F-measure for the template-filling task.},
  motscles  = {Extraction d’information, segmentation de texte, remplissage de formulaires},
  keywords  = {Information Extraction, Text Segmentation, Template Filling},
}

@inproceedings{lefeuvre-EtAl:2012:TALN,
  author    = {Lefeuvre, Anaïs and Moot, Richard and Retoré, Christian and Sandillon-Rezer, Noémie-Fleur},
  title     = {Traitement automatique sur corpus de récits de voyages pyrénéens : Une analyse syntaxique, sémantique et temporelle},
  booktitle = {Actes de la 19e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2012},
  address   = {Grenoble, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {43--56},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2012/taln-2012-long-004},
  language  = {french},
  note      = {Processing of a Pyrenees travel novels corpus : a syntactical, semantical and temporal analysis},
  resume    = {Cet article présente notre utilisation de la théorie des types dans laquelle nous nous situons pour l’analyse syntaxique, sémantique et pour la construction du lexique. Notre outil, Grail permet de traiter le discours automatiquement à partir du texte brut et nous le testons sur un corpus de récit de voyages pyrénéens, Ititpy. Nous expliquons donc notre usage des grammaires catégorielles et plus particulièrement du calcul de Lambek et la correspondance entre ces catégories et le lambda-calcul simplement typé dans le cadre de la DRT. Une flexibilité du typage doit être autorisée dans certains cas et bloquée dans d’autres. Quelques phénomènes linguistiques participant à une forme de glissement de sens provocant des conflits de types sont présentés. Nous expliquons ensuite nos motivations d’ordre pragmatique à utiliser un système à sortes et types variables en sémantique lexicale puis notre traitement compositionnel du temps des évènements inspiré du Binary Tense de (Verkuyl, 2008).},
  abstract  = {In this article, we present a type theoretical framework which we apply to the syntactic analysis and the computation of DRS semantics. Our tool, Grail, is used for the automatic treatment of French text and we use a Pyrenées travel novels corpus, Itipy, as a test case. We explain our use of categorial grammars and specifically the Lambek calculus and its connection to the simply typed lambda-calculus in connection with DRT. Flexible typing has to be allowed in some cases and forbidden in others. Some linguistic phenomena presenting some kind of meaning shifts inducing typing conflicts will be introduced. We then present our motivations in the pragmatic field to use a system with sorts and variable types in lexical semantics and then we present how we process events temporality, in the light of Verkuyl’s Binary Tense (Verkuyl, 2008).},
  motscles  = {compositionalité, interface syntaxe-sémantique, interface sémantique-pragmatique, grammaire catégorielle, théorie des types, récit de voyage},
  keywords  = {compositionality, syntax-semantics interface, semantics-pragmatics interface, categorial grammar, type theory, travel novel},
}

@inproceedings{constant-sigogne-watrin:2012:TALN,
  author    = {Constant, Matthieu and Sigogne, Anthony and Watrin, Patrick},
  title     = {La reconnaissance des mots composés à l'épreuve de l'analyse syntaxique et vice-versa : évaluation de deux stratégies discriminantes},
  booktitle = {Actes de la 19e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2012},
  address   = {Grenoble, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {57--70},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2012/taln-2012-long-005},
  language  = {french},
  note      = {Recognition of compound words tested against parsing and vice-versa : evaluation of two discriminative approaches},
  resume    = {Nous proposons deux stratégies discriminantes d’intégration des mots composés dans un processus réel d’analyse syntaxique : (i) pré-segmentation lexicale avant analyse, (ii) post-segmentation lexicale après analyse au moyen d’un réordonnanceur. Le segmenteur de l’approche (i) se fonde sur un modèle CRF et permet d’obtenir un reconnaisseur de mots composés état-de-l’art. Le réordonnanceur de l’approche (ii) repose sur un modèle MaxEnt intégrant des traits dédiés aux mots composés. Nous montrons que les deux approches permettent de combler jusqu’à 18% de l’écart entre un analyseur baseline et un analyseur avec segmentation parfaite et jusqu’à 25% pour la reconnaissance des mots composés.},
  abstract  = {We propose two discriminative strategies to integrate compound word recognition in a real parsing context : (i) state-of-the-art compound pregrouping with Conditional Random Fields before parsing, (ii) reranking parses with features dedicated to compounds after parsing. We show that these two approaches help reduce up to 18% of the gap between a baseline parser and parser with golden segmentation and up to 25% for compound recognition.},
  motscles  = {Mots composés, analyse syntaxique, champs markoviens aléatoires, réordonnanceur},
  keywords  = {Multiword expressions, parsing, Conditional random Fields, reranker},
}

@inproceedings{alfared-bechet-dikovsky:2012:TALN,
  author    = {Alfared, Ramadan and Béchet, Denis and Dikovsky, Alexander},
  title     = {Calcul des cadres de sous catégorisation des noms déverbaux français (le cas du génitif)},
  booktitle = {Actes de la 19e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2012},
  address   = {Grenoble, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {71--84},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2012/taln-2012-long-006},
  language  = {french},
  note      = {On Computing Subcategorization Frames of French Deverbal Nouns (Case of Genitive)},
  resume    = {L’analyse syntaxique fine en dépendances nécessite la connaissance des cadres de souscatégorisation des unités lexicales. Le cas des verbes étant bien étudié, nous nous intéressons dans cet article au cas des noms communs dérivés de verbes. Notre intérêt principal est de calculer le cadre de sous-catégorisation des noms déverbaux à partir de celui du verbe d’origine pour le français. Or, pour ce faire il faut disposer d’une liste représentative de noms déverbaux français. Pour calculer cette liste nous utilisons un algorithme simplifié de repérage des noms déverbaux, l’appliquons à un corpus et comparons la liste obtenue avec la liste Verbaction des déverbaux exprimant l’action ou l’activité du verbe. Pour les noms déverbaux ainsi obtenus et attestés ensuite par une expertise linguistique, nous analysons la provenance des groupes prépositionnels subordonnés des déverbaux dans des contextes différents en tenant compte du verbe d’origine. L’analyse est effectuée sur le corpus Paris 7 et est limitée au cas le plus fréquent du génitif, c’est-à-dire des groupes prépositionnels introduits par de, des, etc.},
  abstract  = {Fine dependency analysis needs exact information on the subcategoriziation frames of lexical units. These frames being well studied for the verbs, we are interested in this paper by the case of the noun deverbals. Our main goal is to calculate the subcategoriziation frame of deverbals in French from that of the source verb. However, this task needs a representative list of French deverbal nouns. To obtain such a list, we use a simplified algorithm detecting deverbal nouns in texts. The obtained list attested by linguists is compared with the existing list Verbaction of deverbals expressing the action/activity of French verbs. For these deverbal nouns, we analyse the origin of their subordinate prepositional phrases in different contexts relative to their source verbs. This analysis is carried out over the corpus Paris 7 and is limited to the most frequent cases of the genitive, i.e. to the prepositional phrases headed by the prepositions de, des, etc.},
  motscles  = {nom déverbal, cadre de sous-catégorisation, groupe prépositionnel, analyse en dépendances},
  keywords  = {Deverbal Noun, Subcategorization Frame, Prepositional Phrase, Dependency Tree},
}

@inproceedings{claveau:2012:TALN,
  author    = {Claveau, Vincent},
  title     = {Vectorisation, Okapi et calcul de similarité pour le TAL : pour oublier enfin le TF-IDF},
  booktitle = {Actes de la 19e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2012},
  address   = {Grenoble, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {85--98},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2012/taln-2012-long-007},
  language  = {french},
  note      = {Vectorization, Okapi and computing similarity for NLP : say goodbye to TF-IDF},
  resume    = {Dans cette prise de position, nous nous intéressons au calcul de similarité (ou distances) entre textes, problématique présente dans de nombreuses tâches de TAL. Nous nous efforçons de montrer que ce qui n’est souvent qu’un composant dans des systèmes plus complexes est parfois négligé et des solutions sous-optimales sont employées. Ainsi, le calcul de similarité par TF-IDF/cosinus est souvent présenté comme « état-de-l’art », alors que des alternatives souvent plus performantes sont employées couramment dans le domaine de la Recherche d’Information (RI). Au travers de quelques expériences concernant plusieurs tâches, nous montrons combien ce simple calcul de similarité peut influencer les performances d’un système. Nous considérons plus particulièrement deux alternatives. La première est le schéma de pondération Okapi-BM25, bien connu en RI et directement interchangeable avec le TF-IDF. L’autre, la vectorisation, est une technique de calcul de similarité que nous avons développée et qui offrent d’intéressantes propriétés.},
  abstract  = {In this position paper, we review a problem very common for many NLP tasks: computing similarity (or distances) between texts. We aim at showing that what is often considered as a small component in a broader complex system is very often overlooked, leading to the use of sub-optimal solutions. Indeed, computing similarity with TF-IDF weighting and cosine is often presented as “state-of-theart”, while more effective alternatives are in the Information Retrieval (IR) community. Through some experiments on several tasks, we show how this simple calculation of similarity can influence system performance. We consider two particular alternatives. The first is the weighting scheme Okapi-BM25, well known in IR and directly interchangeable with TF-IDF. The other, called vectorization, is a technique for calculating text similarities that we have developed which offers some interesting properties.},
  motscles  = {Calcul de similarité, modèle vectoriel, TF-IDF, Okapi BM-25, vectorisation},
  keywords  = {Calculating similarities, vector space model, TF-IDF, Okapi BM-25, vectorization},
}

@inproceedings{benzitoun-fort-sagot:2012:TALN,
  author    = {Benzitoun, Christophe and Fort, Karën and Sagot, Benoît},
  title     = {TCOF-POS : un corpus libre de français parlé annoté en morphosyntaxe},
  booktitle = {Actes de la 19e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2012},
  address   = {Grenoble, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {99--112},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2012/taln-2012-long-008},
  language  = {french},
  note      = {TCOF-POS : A Freely Available POS-Tagged Corpus of Spoken French},
  resume    = {Nous présentons dans cet article un travail portant sur la création d’un corpus de français parlé spontané annoté en morphosyntaxe. Nous détaillons la méthodologie suivie afin d’assurer le contrôle de la qualité de la ressource finale. Ce corpus est d’ores et déjà librement diffusé pour la recherche et peut servir aussi bien de corpus d’apprentissage pour des logiciels que de base pour des descriptions linguistiques. Nous présentons également les résultats obtenus par deux étiqueteurs morphosyntaxiques entrainés sur ce corpus.},
  abstract  = {This article details the creation of TCOF-POS, the first freely available corpus of spontaneous spoken French. We present here the methodology that was followed in order to obtain the best possible quality in the final resource. This corpus already is freely available and can be used as a training/validation corpus for NLP tools, as well as a study corpus for linguistic research. We also present the results obtained by two POS-taggers trained on the corpus.},
  motscles  = {Etiquetage morpho-syntaxique, français parlé, ressources langagières},
  keywords  = {Etiquetage morpho-syntaxique, français parlé, ressources langagières},
}

@inproceedings{lardilleux-yvon-lepage:2012:TALN,
  author    = {Lardilleux, Adrien and Yvon, François and Lepage, Yves},
  title     = {Alignement sous-phrastique hiérarchique avec Anymalign},
  booktitle = {Actes de la 19e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2012},
  address   = {Grenoble, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {113--126},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2012/taln-2012-long-009},
  language  = {french},
  note      = {Hierarchical sub-sentential alignment with Anymalign},
  resume    = {Nous présentons un algorithme d’alignement sous-phrastique permettant d’aligner très facilement un couple de phrases à partir d’une matrice d’alignement pré-remplie. Cet algorithme s’inspire de travaux antérieurs sur l’alignement par segmentation binaire récursive ainsi que de travaux sur le clustering de documents. Nous évaluons les alignements produits sur des tâches de traduction automatique et montrons qu’il est possible d’atteindre des résultats du niveau de l’état de l’art, affichant des gains très conséquents allant jusqu’à plus de 4 points BLEU par rapport à nos travaux antérieurs, à l’aide une méthode très simple, indépendante de la taille du corpus à traiter, et produisant directement des alignements symétriques. En utilisant cette méthode en tant qu’extension à l’outil d’extraction de traductions Anymalign, nos expériences nous permettent de cerner certaines limitations de ce dernier et de définir des pistes pour son amélioration.},
  abstract  = {We present a sub-sentential alignment algorithm that aligns sentence pairs from an existing alignment matrix in a very easy way. This algorithm is inspired by previous work on alignment by recursive binary segmentation and on document clustering. We evaluate the alignments produced on machine translation tasks and show that we can obtain state-of-the-art results, with gains up to more than 4 BLEU points compared to our previous work, with a method that is very simple, independent of the size of the corpus to be aligned, and can directly produce symmetric alignments. When using this method as an extension of the translation extraction tool Anymalign, our experiments allow us to determine some of its limitations and to define possible leads for further improvements.},
  motscles  = {corpus parallèle, alignement sous-phrastique, traduction automatique statistique},
  keywords  = {parallel corpus, sub-sentential alignment, statistical machine translation},
}

@inproceedings{saadane-semmar:2012:TALN,
  author    = {Saadane, Houda and Semmar, Nasredine},
  title     = {Utilisation de la translittération arabe pour l’amélioration de l’alignement de mots à partir de corpus parallèles français-arabe},
  booktitle = {Actes de la 19e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2012},
  address   = {Grenoble, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {127--140},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2012/taln-2012-long-010},
  language  = {french},
  note      = {Using Arabic transliteration to improve word alignment from French-Arabic parallel corpora},
  resume    = {Dans cet article, nous nous intéressons à l’utilisation de la translittération arabe pour l’amélioration des résultats d’une approche linguistique d’alignement de mots simples et composés à partir de corpus de textes parallèles français-arabe. Cette approche utilise, d’une part, un lexique bilingue et les caractéristiques linguistiques des entités nommées et des cognats pour l’alignement de mots simples, et d’autre part, les relations de dépendance syntaxique pour aligner les mots composés. Nous avons évalué l’aligneur de mots simples et composés intégrant la translittération arabe en utilisant deux procédés : une évaluation de la qualité d’alignement à l’aide d’un alignement de référence construit manuellement et une évaluation de l’impact de cet alignement sur la qualité de la traduction en faisant appel au système de traduction automatique statistique Moses. Les résultats obtenus montrent que la translittération améliore aussi bien la qualité de l’alignement que celle de la traduction.},
  abstract  = {In this paper, we focus on the use of Arabic transliteration to improve the results of a linguistic word alignment approach from parallel text corpora. This approach uses, on the one hand, a bilingual lexicon, named entity and cognates linguistic properties to align single words, and on the other hand, syntactic dependency relations to align compound words. We have evaluated the word aligner integrating Arabic transliteration using two methods: A manual evaluation of the alignment quality and an evaluation of the impact of this alignment on the translation quality by using the statistical machine translation system Moses. The obtained results show that Arabic transliteration improves the quality of both alignment and translation.},
  motscles  = {Translittération, alignement de mots, construction de dictionnaires multilingues, traduction automatique, recherche d’information interlingue},
  keywords  = {Transliteration, word alignment, multilingual lexicons construction, machine translation, cross-language information retrieval},
}

@inproceedings{morin-daille:2012:TALN,
  author    = {Morin, Emmanuel and Daille, Béatrice},
  title     = {Compositionnalité et contextes issus de corpus comparables pour la traduction terminologique},
  booktitle = {Actes de la 19e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2012},
  address   = {Grenoble, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {141--154},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2012/taln-2012-long-011},
  language  = {french},
  note      = {Compositionality and Context for Bilingual Lexicon Extraction from Comparable Corpora},
  resume    = {Dans cet article, nous cherchons à mettre en correspondance de traduction des termes extraits de chaque partie monolingue d’un corpus comparable. Notre objectif concerne l’identification et la traduction de termes spécialisés. Pour ce faire, nous mettons en oeuvre une approche compositionnelle dopée avec des informations contextuelles issues du corpus comparable. Notre évaluation montre que cette approche améliore significativement l’approche compositionnelle de base pour la traduction de termes complexes extraits de corpus comparables.},
  abstract  = {In this article, we study the possibilities of improving the alignment of equivalent terms monolingually acquired from bilingual comparable corpora. Our overall objective is to identify and to translate highly specialised terminology. We applied a compositional approach enhanced with pre-processed context information. Our evaluation demonstrates that our alignment method outperforms the compositional approach for translationally equivalent term discovery from comparable corpora.},
  motscles  = {Corpus comparable, compositionnalité, information contextuelle, lexique bilingue},
  keywords  = {Comparable Corpora, compositionality, context information, bilingual lexicon},
}

@inproceedings{bedaride:2012:TALN,
  author    = {Bédaride, Paul},
  title     = {Raffinement du Lexique des Verbes Français},
  booktitle = {Actes de la 19e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2012},
  address   = {Grenoble, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {155--168},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2012/taln-2012-long-012},
  language  = {french},
  note      = {Resource Refining : « Les Verbes Français »},
  resume    = {Nous présentons dans cet article les améliorations apportées à la ressource « Les Verbes Français » afin de la rendre plus formelle et utilisable pour le traitement automatique des langues naturelles. Les informations syntaxiques et sémantiques ont été corrigées, restructurées, unifiées puis intégrées à la version XML de cette ressource, afin de pouvoir être utilisée par un système d’étiquetage de rôles sémantiques.},
  abstract  = {This paper introduce the impovements we made to the resource « Les Verbes Français » in order to make it more usable in the field of natural language processing. Syntactic and semantic information is corrected, restructured, unified and then integrated to the XML version of this resource, in order to be used by a semantic role labelling system.},
  motscles  = {ressource, lexique, verbes, raffinement, étiquetage de rôles sémantiques},
  keywords  = {resource, lexicon, verbs, refinement, semantic roles labeling},
}

@inproceedings{morlanehondere-fabre:2012:TALN,
  author    = {Morlane-Hondère, François and Fabre, Cécile},
  title     = {Étude des manifestations de la relation de méronymie dans une ressource distributionnelle},
  booktitle = {Actes de la 19e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2012},
  address   = {Grenoble, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {169--182},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2012/taln-2012-long-013},
  language  = {french},
  note      = {Study of meronymy in a distribution-based lexical resource},
  resume    = {Cette étude vise à étudier les manifestations de la relation de méronymie dans une ressource lexicale générée automatiquement à partir d’un corpus de langue générale. La démarche que nous adoptons consiste à recueillir un jeu de couples de méronymes issus d’une ressource externe que nous croisons avec une base distributionnelle calculée à partir d’un corpus de textes encyclopédiques. Une annotation sémantique des mots qui entrent dans ces couples de méronymes montre que la prise en compte de la nature sémantique des mots composant les couples de méronymes permet de mettre au jour des inégalités au niveau du repérage de la relation par la méthode d’analyse distributionnelle.},
  abstract  = {In this paper, we study the way meronymy behaves in a distribution-based lexical resource. We address the question of the evaluation of such resources through a semantic-based approach. Our method consists in collecting meronyms from a resource which we cross with a distributionbased lexical resource made from an encyclopedic corpus. Meronyms are then sub-categorized manually : firstly following the sub-relation they bear (STUFF/OBJECT, MEMBER/COLLECTION, etc.), then following the semantic class of their members. Results show that distributional analysis identifies meronymic relations in different proportions according to the semantic classes of the words involved in the meronymic pairs.},
  motscles  = {analyse distributionnelle, sémantique lexicale, méronymie, évaluation},
  keywords  = {distributional analysis, lexical semantics, meronymy, evaluation},
}

@inproceedings{degroc-tannier-deloupy:2012:TALN,
  author    = {De Groc, Clément and Tannier, Xavier and De Loupy, Claude},
  title     = {Un critère de cohésion thématique fondé sur un graphe de cooccurrences},
  booktitle = {Actes de la 19e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2012},
  address   = {Grenoble, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {183--195},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2012/taln-2012-long-014},
  language  = {french},
  note      = {Topical Cohesion using Graph Random Walks},
  resume    = {Dans cet article, nous définissons un nouveau critère de cohésion thématique permettant de pondérer les termes d’un lexique thématique en fonction de leur pertinence. Le critère s’inspire des approches Web as corpus pour accumuler des connaissances exogènes sur un lexique. Ces connaissances sont ensuite modélisées sous forme de graphe et un algorithme de marche aléatoire est appliqué pour attribuer un score à chaque terme. Après avoir étudié les performances et la stabilité du critère proposé, nous l’évaluons sur une tâche d’aide à la création de lexiques bilingues.},
  abstract  = {In this article, we propose a novel metric to weight specialized lexicons terms according to their relevance to the underlying thematic. Our method is inspired by Web as corpus approaches and accumulates exogenous knowledge about a specialized lexicon from the web. Terms cooccurrences are modelled as a graph and a random walk algorithm is applied to compute terms relevance. Finally, we study the performance and stability of the metric and evaluate it in a bilingual lexicon creation context.},
  motscles  = {Cohésion thématique, graphe de cooccurrences, marche aléatoire},
  keywords  = {Thematic relevance, cooccurrence graph, random walk},
}

@inproceedings{bouamor-EtAl:2012:TALN,
  author    = {Bouamor, Houda and Max, Aurélien and Illouz, Gabriel and Vilnat, Anne},
  title     = {Validation sur le Web de reformulations locales: application à la Wikipédia},
  booktitle = {Actes de la 19e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2012},
  address   = {Grenoble, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {197--210},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2012/taln-2012-long-015},
  language  = {french},
  note      = {Assisted rephrasing for Wikipedia contributors through Web-based validation},
  resume    = {Ce travail présente des expériences initiales en validation de paraphrases en contexte. Les révisions de Wikipédia nous servent de domaine d’évaluation : pour un énoncé ayant connu une courte révision dans l’encyclopédie, nous disposons d’un ensemble de réécritures possibles, parmi lesquelles nous cherchons à identifier celles qui correspondent à des paraphrases valides. Nous abordons ce problème comme une tâche de classification fondée sur des informations issues du Web, et parvenons à améliorer la performance de plusieurs techniques simples de référence.},
  abstract  = {This works describes initial experiments on the validation of paraphrases in context. Wikipedia’s revisions are used : we assume that a set of possible rewritings are available for a given phrase that has been rewritten in the encyclopedia’s revision history, and we attempt to find the subset of those rewritings that can be considered as valid paraphrases. We tackle this problem as a classication task which we provide with features obtained from Web data. Our experiments show that our system improves performance over a set of simple baselines.},
  motscles  = {paraphrase, Wikipédia, aide à la rédaction},
  keywords  = {paraphrasing, Wikipedia, authoring aids},
}

@inproceedings{brouwers-EtAl:2012:TALN,
  author    = {Brouwers, Laetitia and Bernhard, Delphine and Ligozat, Anne-Laure and François, Thomas},
  title     = {Simplification syntaxique de phrases pour le français},
  booktitle = {Actes de la 19e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2012},
  address   = {Grenoble, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {211--224},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2012/taln-2012-long-016},
  language  = {french},
  note      = {Syntactic Simplification for French Sentences},
  resume    = {Cet article présente une méthode de simplification syntaxique de textes français. La simplification syntaxique a pour but de rendre des textes plus abordables en simplifiant les éléments qui posent problème à la lecture. La méthode mise en place à cette fin s’appuie tout d’abord sur une étude de corpus visant à étudier les phénomènes linguistiques impliqués dans la simplification de textes en français. Nous avons ainsi constitué un corpus parallèle à partir d’articles de Wikipédia et Vikidia, ce qui a permis d’établir une typologie de simplifications. Dans un second temps, nous avons implémenté un système qui opère des simplifications syntaxiques à partir de ces observations. Des règles de simplification ont été décrites afin de générer des phrases simplifiées. Un module sélectionne ensuite le meilleur ensemble de phrases. Enfin, nous avons mené une évaluation de notre système montrant qu’environ 80% des phrases générées sont correctes.},
  abstract  = {This paper presents a method for the syntactic simplification of French texts. Syntactic simplification aims at making texts easier to understand by simplifying the elements that hinder reading. It is based on a corpus study that aimed at investigating the linguistic phenomena involved in the manual simplification of French texts. We have first gathered a parallel corpus of articles from Wikipedia and Vikidia, that we used to establish a typology of simplifications. In a second step, we implemented a system that carries out syntactic simplifications based on these corpus observations. We described simplification rules in order to generate simplified sentences. A module subsequently selects the best subset of sentences. The evaluation of our system shows that about 80% of the sentences produced by our system are accurate.},
  motscles  = {simplification automatique, lisibilité, analyse syntaxique},
  keywords  = {automatic simplification, readability, syntactic analysis},
}

@inproceedings{keskes-EtAl:2012:TALN,
  author    = {Keskes, Iskandar and Mahdi Boudabous, Mohamed and Hédi Maâloul, Mohamed and Hadrich Belguith, Lamia},
  title     = {Étude comparative entre trois approches de résumé automatique de documents arabes},
  booktitle = {Actes de la 19e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2012},
  address   = {Grenoble, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {225--238},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2012/taln-2012-long-017},
  language  = {french},
  note      = {Comparative study of three approaches to automatic summarization of Arabic documents},
  resume    = {Dans cet article, nous proposons une étude comparative entre trois approches pour le résumé automatique de documents arabes. Ainsi, nous avons proposé trois méthodes pour l’extraction des phrases les plus représentatives d'un document. La première méthode se base sur une approche symbolique, la deuxième repose sur une approche numérique et la troisième se base sur une approche hybride. Ces méthodes sont implémentées respectivement par le système ARSTResume, le système R.I.A et le système HybridResume. Nous présentons, par la suite, les résultats obtenus par les trois systèmes et nous procédons à une étude comparative entre les résultats obtenus afin de souligner les avantages et les limites de chaque méthode. Les résultats de l’évaluation ont montré que l‘approche numérique est plus performante que l’approche symbolique au niveau des textes longs. Mais, l’intégration de ces deux approches en une approche hybride aboutit aux résultats les plus performants dans notre corpus de textes.},
  abstract  = {In this paper, we propose a comparative study between three approaches for automatic summarization of Arabic documents. Thus, we proposed three methods for extracting most representative sentences of a document. The first method is based on a symbolic approach, the second is relied on a numerical approach and the third is based on a hybrid approach. These methods are implemented respectively by the ARSTResume, R.I.A and HybridResume systems. Then, we present the results obtained by the three systems and we conduct a comparative study between the obtained results in order to highlight the advantages and limitations of each method. The evaluation results showed that the numerical approach has better performances than the symbolic approach. But, combining into a hybrid approach achieved the best results for our text corpus.},
  motscles  = {Résumé automatique, approche symbolique, approche numérique, approche hybride, document arabe},
  keywords  = {Automatic summarization, symbolic approach, numerical approach, hybrid approach, Arabic document},
}

@inproceedings{bertels-dehertog-heylen:2012:TALN,
  author    = {Bertels, Ann and De Hertog, Dirk and Heylen, Kris},
  title     = {Etude sémantique des mots-clés et des marqueurs lexicaux stables dans un corpus technique},
  booktitle = {Actes de la 19e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2012},
  address   = {Grenoble, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {239--252},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2012/taln-2012-long-018},
  language  = {french},
  note      = {Semantic analysis of keywords and stable lexical markers in a technical corpus},
  resume    = {Cet article présente les résultats d’une analyse sémantique quantitative des unités lexicales spécifiques dans un corpus technique, relevant du domaine des machines-outils pour l’usinage des métaux. L’étude vise à vérifier si et dans quelle mesure les mots-clés du corpus technique sont monosémiques. A cet effet, nous procédons à une analyse statistique de régression simple, qui permet d’étudier la corrélation entre le rang de spécificité des mots-clés et leur rang de monosémie, mais qui soulève des problèmes statistiques et méthodologiques, notamment un biais de fréquence. Pour y remédier, nous adoptons une approche alternative pour le repérage des unités lexicales spécifiques, à savoir l’analyse des marqueurs lexicaux stables ou Stable Lexical Marker Analysis (SLMA). Nous discutons les résultats quantitatifs et statistiques de cette approche dans la perspective de la corrélation entre le rang de spécificité et le rang de monosémie.},
  abstract  = {This article presents the results of a quantitative semantic analysis of typical lexical units in a specialised technical corpus of metalworking machinery in French. The study aims to find out whether and to what extent the keywords of the technical corpus are monosemous. A simple regression analysis, used to examine the correlation between typicality rank and monosemy rank of the keywords, points out some statistical and methodological problems, notably a frequency bias. In order to overcome these problems, we adopt an alternative approach for the identification of typical lexical units, called Stable Lexical Marker Analysis (SLMA). We discuss the quantitative and statistical results of this approach with respect to the correlation between typicality rank and monosemy rank.},
  motscles  = {unités lexicales spécifiques, analyse des mots-clés, analyse des marqueurs lexicaux stables, sémantique quantitative, analyse de régression},
  keywords  = {typical lexical units, Keyword Analysis, Stable Lexical Marker Analysis (SLMA), quantitative semantics, regression analysis},
}

@inproceedings{quiniou-EtAl:2012:TALN,
  author    = {Quiniou, Solen and Cellier, Peggy and Charnois, Thierry and Legallois, Dominique},
  title     = {Fouille de graphes sous contraintes linguistiques pour l'exploration de grands textes},
  booktitle = {Actes de la 19e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2012},
  address   = {Grenoble, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {253--266},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2012/taln-2012-long-019},
  language  = {french},
  note      = {Graph Mining Under Linguistic Constraints to Explore Large Texts},
  resume    = {Dans cet article, nous proposons une approche pour explorer des textes de taille importante en mettant en évidence des sous-parties cohérentes. Cette méthode d’exploration s’appuie sur une représentation en graphe du texte, en utilisant le modèle linguistique de Hoey pour sélectionner et apparier les phrases dans le graphe. Notre contribution porte sur l’utilisation de techniques de fouille de graphes sous contraintes pour extraire des sous-parties pertinentes du texte (c’est-à-dire des collections de sous-réseaux phrastiques homogènes). Nous avons réalisé des expérimentations sur deux textes anglais de taille conséquente pour montrer l’intérêt de l’approche que nous proposons.},
  abstract  = {In this paper, we propose an approach to explore large texts by highlighting coherent sub-parts. The exploration method relies on a graph representation of the text according to the Hoey linguistic model which allows the selection and the binding of sentences in the graph. Our contribution relates to using graph mining techniques under constraints to extract relevant subparts of the text (i.e., collections of homogeneous sentence sub-networks). We have conducted some experiments on two large English texts to show the interest of the proposed approach.},
  motscles  = {Fouille de graphes, réseaux phrastiques, analyse textuelle, navigation textuelle},
  keywords  = {Graph Mining, sentence networks, textual analysis, textual navigation},
}

@inproceedings{bouamor-max-vilnat:2012:TALN,
  author    = {Bouamor, Houda and Max, Aurélien and Vilnat, Anne},
  title     = {Une étude en 3D de la paraphrase: types de corpus, langues et techniques},
  booktitle = {Actes de la 19e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2012},
  address   = {Grenoble, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {267--280},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2012/taln-2012-long-020},
  language  = {french},
  note      = {A study of paraphrase along 3 dimensions : corpus types, languages and techniques},
  resume    = {Cet article présente une étude détaillée de l’impact du type du corpus sur la tâche d’acquisition de paraphrases sous-phrastiques. Nos expériences sont menées sur deux langues et quatre types de corpus, et incluent une combinaison efficace de quatre systèmes d’acquisition de paraphrases. Nous obtenons une amélioration relative de plus de 27% en F-mesure par rapport au meilleur système, en anglais et en français, ainsi qu’une amélioration relative à notre combinaison de systèmes de 22% pour l’anglais et de 5% pour le français quand tous les types de corpus sont utilisés pour l’acquisition depuis le type de corpus le plus couramment disponible.},
  abstract  = {In this paper, we report a detailed study of the impact of corpus type on the task of sub-sentential paraphrase acquisition. Our experiments are for 2 languages and 4 corpus types, and involve an efficient machine learning-based combination of 4 paraphrase acquisition systems. We obtain relative improvements of more than 27% in F-measure over the best individual system on English and French, and obtain a relative improvement over the combination system of 22% for English and 5% for French when using all other corpus types as additional training data for our most readily available corpus type.},
  motscles  = {acquisition de paraphrases, constitution de corpus},
  keywords  = {paraphrase acquisition, corpus collection},
}

@inproceedings{boudin-hernandez:2012:TALN,
  author    = {Boudin, Florian and Hernandez, Nicolas},
  title     = {Détection et correction automatique d'erreurs d'annotation morpho-syntaxique du French TreeBank},
  booktitle = {Actes de la 19e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2012},
  address   = {Grenoble, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {281--291},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2012/taln-2012-long-021},
  language  = {french},
  note      = {Detecting and correcting POS annotation in the French TreeBank},
  resume    = {La qualité de l’annotation morpho-syntaxique d’un corpus est déterminante pour l’entraînement et l’évaluation de méthodes d’étiquetage. Cet article présente une série d’expériences que nous avons menée sur la détection et la correction automatique des erreurs du French Treebank. Deux méthodes sont utilisées. La première consiste à identifier les mots sans étiquette et leur attribuer celle d’une forme correspondante observée dans le corpus. La seconde méthode utilise les variations de n-gramme pour détecter et corriger les anomalies d’annotation. L’évaluation des corrections apportées au corpus est réalisée de manière extrinsèque en comparant les scores de performance de différentes méthodes d’étiquetage morpho-syntaxique en fonction du niveau de correction. Les résultats montrent une amélioration significative de la précision et indiquent que la qualité du corpus peut être sensiblement améliorée par l’application de méthodes de correction automatique des erreurs d’annotation.},
  abstract  = {The quality of the Part-Of-Speech (POS) annotation in a corpus has a large impact on training and evaluating POS taggers. In this paper, we present a series of experiments that we have conducted on automatically detecting and correcting annotation errors in the French TreeBank. Two methods are used. The first simply relies on identifying tokens with missing tags and correct them by assigning the tag the same token observed in the corpus. The second method uses n-gram variations to detect and correct conflicting annotations. The evaluation of the automatic correction is performed extrinsically by comparing the performance of different POS taggers in relation to the level of correction. Results show a statistically significant improvement in precision and indicate that the POS annotation quality can be noticeably enhanced by using automatic correction methods.},
  motscles  = {Étiquetage morpho-syntaxique, correction automatique, qualité d’annotation},
  keywords  = {Part-Of-Speech tagging, automatic correction, annotation quality},
}

@inproceedings{guillaume-perrier:2012:TALN,
  author    = {Guillaume, Bruno and Perrier, Guy},
  title     = {Annotation sémantique du French Treebank à l’aide de la réécriture modulaire de graphes},
  booktitle = {Actes de la 19e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2012},
  address   = {Grenoble, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {293--306},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2012/taln-2012-long-022},
  language  = {french},
  note      = {Semantic Annotation of the French Treebank using Modular Graph Rewriting},
  resume    = {Nous proposons d’annoter le French Treebank à l’aide de dépendances sémantiques dans le cadre de la DMRS en partant d’une annotation en dépendances syntaxiques de surface et en utilisant la réécriture modulaire de graphes. L’article présente un certain nombre d’avancées concernant le calcul de réécriture utilisé : l’utilisation de règles pour faire le lien avec des lexiques, en particulier le lexique des verbes de Dicovalence, et l’introduction de filtres pour écarter à certaines étapes les annotations incohérentes. Il présente aussi des avancées dans le système de réécriture lui-même, qui a une plus large couverture (constructions causatives, verbes à montée, . . .) et dont l’ordre des modules a été étudié de façon plus systématique. Ce système a été expérimenté sur l’ensemble du French Treebank à l’aide du prototype GREW, qui implémente le calcul de réécriture utilisé.},
  abstract  = {We propose to annotate the French Treebank with semantic dependencies in the framework of DMRS starting from an annotation with surface syntactic dependencies and using modular graph rewriting. The article presents some new results related to the rewriting calculus: the use of rules to make a link with lexicons, especially with the lexicon of verbs Dicovalence, and the introduction of filters to discard inconsistent annotations at some computation steps. It also presents new results related to the rewriting system itself: the system has a larger coverage (causative constructions, rising verbs, . . .) and the order between modules has been studied in a more systematic way. This system has been experimented on the whole French Treebank with the prototype GREW, which implements the used rewriting calculus.},
  motscles  = {réécriture de graphes, interface syntaxe-sémantique, dépendances, DMRS},
  keywords  = {graph rewriting, syntax-semantics interface, dependencies, DMRS},
}

@inproceedings{blache-rauzy:2012:TALN,
  author    = {Blache, Philippe and Rauzy, Stéphane},
  title     = {Enrichissement du FTB : un treebank hybride constituants/propriétés},
  booktitle = {Actes de la 19e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2012},
  address   = {Grenoble, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {307--320},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2012/taln-2012-long-023},
  language  = {french},
  note      = {Enriching the French Treebank with Properties},
  resume    = {Cet article présente les mécanismes de création d’un treebank hybride enrichissant le FTB à l’aide d’annotations dans le formalisme des Grammaires de Propriétés. Ce processus consiste à acquérir une grammaire GP à partir du treebank source et générer automatiquement les structures syntaxiques dans le formalisme cible en s’appuyant sur la spécification d’un schéma d’encodage adapté. Le résultat produit, en partant d’une version du FTB corrigée et modifiée en fonction de nos besoins, constitue une ressource ouvrant de nouvelles perspectives pour le traitement et la description du français.},
  abstract  = {We present in this paper the hybridation of the French Treebank with Property Grammars annotations. This process consists in acquiring a PG grammar from the source treebank and generating the new syntactic encoding on top of the original one. The result is a new resource for French, opening the way to new tools and descriptions.},
  motscles  = {Treebank hybride, French Treebank, Grammaires de Propriétés},
  keywords  = {Hybrid treebank, French Treebank, Property Grammars},
}

@inproceedings{candito-seddah:2012:TALN,
  author    = {Candito, Marie and Seddah, Djamé},
  title     = {Le corpus Sequoia : annotation syntaxique et exploitation pour l'adaptation d'analyseur par pont lexical},
  booktitle = {Actes de la 19e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2012},
  address   = {Grenoble, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {321--334},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2012/taln-2012-long-024},
  language  = {french},
  note      = {The Sequoia corpus : syntactic annotation and use for a parser lexical domain adaptation method},
  resume    = {Nous présentons dans cet article la méthodologie de constitution et les caractéristiques du corpus Sequoia, un corpus en français, syntaxiquement annoté d’après un schéma d’annotation très proche de celui du French Treebank (Abeillé et Barrier, 2004), et librement disponible, en constituants et en dépendances. Le corpus comporte des phrases de quatre origines : Europarl français, le journal l’Est Républicain, Wikipédia Fr et des documents de l’Agence Européenne du Médicament, pour un total de 3204 phrases et 69246 tokens. En outre, nous présentons une application de ce corpus : l’évaluation d’une technique d’adaptation d’analyseurs syntaxiques probabilistes à des domaines et/ou genres autres que ceux du corpus sur lequel ces analyseurs sont entraînés. Cette technique utilise des clusters de mots obtenus d’abord par regroupement morphologique à l’aide d’un lexique, puis par regroupement non supervisé, et permet une nette amélioration de l’analyse des domaines cibles (le corpus Sequoia), tout en préservant le même niveau de performance sur le domaine source (le FTB), ce qui fournit un analyseur multi-domaines, à la différence d’autres techniques d’adaptation comme le self-training.},
  abstract  = {We present the building methodology and the properties of the Sequoia treebank, a freely available French corpus annotated following the French Treebank guidelines (Abeillé et Barrier, 2004). The Sequoia treebank comprises 3204 sentences (69246 tokens), from the French Europarl, the regional newspaper L’Est Républicain, the French Wikipedia and documents from the European Medicines Agency. We then provide a method for parser domain adaptation, that makes use of unsupervised word clusters. The method improves parsing performance on target domains (the domains of the Sequoia corpus), without degrading performance on source domain (the French treenbank test set), contrary to other domain adaptation techniques such as self-training.},
  motscles  = {Corpus arboré, analyse syntaxique statistique, adaptation de domaine},
  keywords  = {Treebank, statistical parsing, parser domain adaptation},
}

@inproceedings{brunetmanquat-goulian:2012:TALN,
  author    = {Brunet-Manquat, Francis and Goulian, Jérôme},
  title     = {ACOLAD Plateforme pour l’édition collaborative dépendancielle},
  booktitle = {Actes de la 19e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2012},
  address   = {Grenoble, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {335--342},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2012/taln-2012-court-001},
  language  = {french},
  note      = {ACOLAD: platform for collaborative dependency annotation},
  resume    = {Cet article présente une plateforme open-source pour l’édition collaborative de corpus de dépendances. Cette plateforme, nommée ACOLAD (Annotation de COrpus Linguistique pour l’Analyse de Dépendances), propose des services manuels de segmentation et d’annotation multi-niveaux (segmentation en mots et en syntagmes minimaux (chunks), annotation morphosyntaxique des mots, annotation syntaxique des chunks et annotation syntaxique des dépendances entre mots ou entre chunks). Dans cet article, nous présentons la plateforme ACOLAD, puis nous détaillons la représentation pivot utilisée pour gérer les annotations concurrentes, enfin décrivons le mécanisme d’importation de ressources linguistiques externes.},
  abstract  = {This paper presents an open-source platform for collaborative editing dependency corpora. ACOLAD platform (Annotation of corpus linguistics for the analysis of dependencies) offers manual annotation services such as segmentation and multi-level annotation (segmentation into words and phrases minimum (chunks), morphosyntactic annotation of words, syntactic annotation chunks and annotating syntactic dependencies between words or chunks). In this paper, we present ACOLAD platform, then we detail the representation used to manage concurrent annotations, then we describe the mechanism for importing external linguistic resources.},
  motscles  = {annotation collaborative de corpus, annotations concurrentes, dépendances},
  keywords  = {corpus collaborative annotation, concurrent annotations, dependencies},
}

@inproceedings{cadilhac-EtAl:2012:TALN,
  author    = {Cadilhac, Anaïs and Benamara, Farah and Popescu, Vladimir and Asher, Nicholas and Seck, Mohamadou},
  title     = {Extraction de préférences à partir de dialogues de négociation},
  booktitle = {Actes de la 19e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2012},
  address   = {Grenoble, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {343--350},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2012/taln-2012-court-002},
  language  = {french},
  note      = {Towards Preference Extraction From Negotiation Dialogues},
  resume    = {Cet article présente une approche linguistique pour l’extraction d’expressions de préférence à partir de dialogues de négociation. Nous proposons un nouveau schéma d’annotation pour encoder les préférences et les dépendances exprimées linguistiquement dans deux genres de corpus différents. Ensuite, nous proposons une méthode d’apprentissage qui extrait les expressions de préférence en utilisant une combinaison de traits locaux et discursifs. Finalement, nous évaluons la fiabilité de notre approche sur chaque genre de corpus.},
  abstract  = {This paper presents an NLP based approach for preference expression extraction from negotiation dialogues. We propose a new annotation schema for preferences and dependencies among them and illustrate on two different corpus genres. We then suggest a learning approach that efficiently extracts preference expressions using a combination of local and discursive features and assess the reliability of our approach on each corpus genre.},
  motscles  = {Préférence, dialogue, apprentissage automatique},
  keywords  = {Preference, dialogue, machine learning},
}

@inproceedings{denis-EtAl:2012:TALN,
  author    = {Denis, Alexandre and Quignard, Matthieu and Freard, Dominique and Detienne, Francoise and Baker, Michael and Barcellini, Flore},
  title     = {Détection de conflits dans les communautés épistémiques en ligne},
  booktitle = {Actes de la 19e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2012},
  address   = {Grenoble, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {351--358},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2012/taln-2012-court-003},
  language  = {french},
  note      = {Conflicts detection in online epistemic communities},
  resume    = {La présence de conflits dans les communautés épistémiques en ligne peut s’avérer bloquante pour l’activité de conception. Nous présentons une étude sur la détection automatique de conflit dans les discussions entre contributeurs Wikipedia qui s’appuie sur des traits de surface tels que la subjectivité ou la connotation des énoncés et évaluons deux règles de décision : l’une découle d’un modèle dialectique en exploitant localement la structure linéaire de la discussion, la subjectivité et la connotation ; l’autre, plus globale, ne s’appuie que sur la taille des fils et les marques de subjectivité au détriment des marques de connotation. Nous montrons que ces deux règles produisent des résultats similaires mais que la simplicité de la règle globale en fait une approche préférée dans la détection des conflits.},
  abstract  = {Conflicts in online epistemic communities can be a blocking factor when producing knowledge. We present a way to automatically detect conflict in Wikipedia discussions, based on subjectivity and connotation marks. Two rules are evaluated : a local rule that uses the structure of the discussion threads, connotation and subjectivity marks and a global rule that takes the whole thread into account and only subjectivity. We show that the two rules produce similar results but that the simplicity of the global rule makes it a preferred approach to detect conflicts.},
  motscles  = {wikipedia, conflit, syntaxe, sémantique, interaction},
  keywords  = {wikipedia, conflict, syntax, semantics, interaction},
}

@inproceedings{dutrey-EtAl:2012:TALN,
  author    = {Dutrey, Camille and Clavel, Chloé and Rosset, Sophie and Vasilescu, Ioana and Adda-Decker, Martine},
  title     = {Quel est l'apport de la détection d'entités nommées pour l'extraction d'information en domaine restreint ?},
  booktitle = {Actes de la 19e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2012},
  address   = {Grenoble, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {359--366},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2012/taln-2012-court-004},
  language  = {french},
  note      = {What is the contribution of named entities detection for information extraction in restricted domain ?},
  resume    = {Les travaux liés à la définition et à la reconnaissance des entités nommées sont généralement envisagés en domaine ouvert, à travers la conception de catégories génériques (noms de personnes, de lieux, etc.) et leur application à des données textuelles issues de la presse (orale comme écrite). Par ailleurs, la fouille des données issues de centres d’appel est stratégique pour une entreprise comme EDF, compte tenu du rôle crucial joué par l’opinion pour les applications marketing, ce qui passe par la définition d’entités d’intérêt propres au domaine. Nous comparons les deux types de modèles d’entités - génériques et spécifiques à un domaine précis - afin d’observer leurs points de recouvrement, via l’annotation manuelle d’un corpus de conversations en centres d’appel. Nous souhaitons ainsi étudier l’apport d’une détection en entités nommées génériques pour l’extraction d’information métier en domaine restreint.},
  abstract  = {In the framework of general domain dialog corpora a particular focus is dedicated to Named Entities definition and recognition, which are mostly very generic (personal names, locations, etc.). Moreover, call-centre data mining is strategic for a company like EDF, the public opinion analysis playing a significant role in EDF services quality evaluation and for marketing applications. In this purpose a domain dependant definition of entities of interest is essential. In this primary work we compare two types of entities models (generic and specific to the domain) in order to observe their respective coverage. We annotated manually a sub-corpus extracted from a large corpus of oral dialogs recorded in an EDF call-centre. The respective proportion of generic vs domain-specific Named Entities is then estimated. Impact for future work on building EDF domain-specific entities models is discussed.},
  motscles  = {entités nommées, concepts métier, extraction d’information, données conversationnelles, annotation},
  keywords  = {named entities, business concept, information extraction, conversational data, annotation},
}

@inproceedings{eensoo-valette:2012:TALN,
  author    = {Eensoo, Egle and Valette, Mathieu},
  title     = {Sur l'application de méthodes textométriques à la construction de critères de classification en analyse des sentiments},
  booktitle = {Actes de la 19e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2012},
  address   = {Grenoble, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {367--374},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2012/taln-2012-court-005},
  language  = {french},
  note      = {About the application of textometric methods for developing classification criteria in Sentiment analysis},
  resume    = {Depuis une dizaine d'années, le TAL s'intéresse à la subjectivité, notamment dans la perspective d'applications telles que la fouille d'opinion et l'analyse des sentiments. Or, la linguistique de corpus outillée par des méthodes textométriques a souvent abordé la question de la subjectivité dans les textes. Notre objectif est de montrer d'une part, ce que pourrait apporter à l'analyse des sentiments l'analyse textométrique et d'autre part, comment mutualiser les avantages d'une association entre celle-ci et une méthode de classification automatique basée sur l'apprentissage supervisé. En nous appuyant sur un corpus de témoignages issus de forums de discussion, nous montrerons que la prise en compte de critères sélectionnés suivant une analyse textométrique permet d'obtenir des résultats de classification satisfaisants par rapport à une vision purement lexicale.},
  abstract  = {Over the last ten years, NLP has contributed to applied research on subjectivity, especially in applications such as Opinion mining and Sentiment analysis. However, corpus linguistics and textometry have often addressed the issue of subjectivity in text. Our purpose is to show, !rst, what textometric analysis could bring to sentiment analysis, and second, the bene!ts of pooling linguistic/textometric analysis and automatic classification methods based on supervised learning. By processing a corpus of posts from fora, we will show that the building of criteria from a textometric analysis could improve classification results, compared to a purely lexical approach.},
  motscles  = {linguistique de corpus, textométrie, analyse de sentiments, classification automatique supervisée},
  keywords  = {corpus linguistics, textometry, sentiment analysis, supervised learning},
}

@inproceedings{filhol-braffort:2012:TALN,
  author    = {Filhol, Michael and Braffort, Annelies},
  title     = {Méthodologie d'exploration de corpus et de formalisation de règles grammaticales pour les langues des signes},
  booktitle = {Actes de la 19e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2012},
  address   = {Grenoble, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {375--382},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2012/taln-2012-court-006},
  language  = {french},
  note      = {Methodology for corpus exploration and grammatical rule building in Sign Language},
  resume    = {Cet article présente une méthodologie visant, à partir d'une observation de corpus vidéo de langue des signes, à repérer puis formaliser les régularités de structure dans les constructions linguistiques. Cette méthodologie est applicable à tous les niveaux du langage, du sub-lexical à l'énoncé complet. En s'appuyant sur deux exemples, il présente une application de cette méthodologie ainsi que le modèle AZee qui, intégrant la souplesse nécessaire en termes de synchronisation des articulateurs, permet une formalisation des règles repérées.},
  abstract  = {This paper presents a methodology for Sign Language video observation to extract and then formalise observed linguistic structure. This methodology is relevant to all linguistic layers from sub-lexical to discourse as a whole. Relying on two examples, we apply this methodology and describe the AZee model, which integrates the required flexibility for synchronising articulators, hence enables a specification of any new systematic rule observed.},
  motscles  = {Langue des signes, analyse de corpus, modèle grammatical, synchronisation},
  keywords  = {Sign Language, corpus analysis, grammatical models, synchronisation},
}

@inproceedings{fort-claveau:2012:TALN,
  author    = {Fort, Karën and Claveau, Vincent},
  title     = {Annotation manuelle de matchs de foot : Oh la la la ! l'accord inter-annotateurs ! et c'est le but !},
  booktitle = {Actes de la 19e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2012},
  address   = {Grenoble, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {383--390},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2012/taln-2012-court-007},
  language  = {french},
  note      = {Manual Annotation of Football Matches : Inter-annotator Agreement ! Gooooal !},
  resume    = {Cet article présente une campagne d’annotation de commentaires de matchs de football en français. L’annotation a été réalisée à partir d’un corpus très hétérogène, contenant à la fois des comptes-rendus minute par minute et des transcriptions des commentaires vidéo. Nous montrons ici comment les accords intra- et inter-annotateurs peuvent être utilisés efficacement, en en proposant une définition adaptée à notre type de tâche et en mettant en exergue l’importance de certaines bonnes pratiques concernant leur utilisation. Nous montrons également comment certains indices collectés à l’aide d’outils statistiques simples peuvent être utilisés pour indiquer des pistes de corrections des annotations. Ces différentes propositions nous permettent par ailleurs d’évaluer l’impact des modalités sources de nos textes (oral ou écrit) sur le coût et la qualité des annotations.},
  abstract  = {We present here an annotation campaign of commentaries of football matches in French. The annotation was done from a very heterogeneous text corpus of both match minutes and video commentary transcripts. We show how the intra- and inter-annotator agreement can be used efficiently during the whole campaign by proposing a definition of the markables suited to our type of task, as well as emphasizing the importance of using it appropriately. We also show how some clues, collected through statistical analyses, could be used to help correcting the annotations. These statistical analyses are then used to assess the impact of the source modality (written or spoken) on the cost and quality of the annotation process.},
  motscles  = {annotation manuelle, accords inter-annotateurs},
  keywords  = {manual annotation, inter-annotator agreement},
}

@inproceedings{garciafernandez-ferret:2012:TALN,
  author    = {Garcia-Fernandez, Anne and Ferret, Olivier},
  title     = {Etude de différentes stratégies d'adaptation à un nouveau domaine en fouille d'opinion},
  booktitle = {Actes de la 19e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2012},
  address   = {Grenoble, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {391--398},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2012/taln-2012-court-008},
  language  = {french},
  note      = {Study of various strategies for adapting an opinion classifier to a new domain},
  resume    = {Le travail présenté dans cet article se situe dans le contexte de la fouille d’opinion et se focalise sur la détermination de la polarité d’un texte en adoptant une approche par apprentissage. Dans ce cadre, son objet est d’étudier différentes stratégies d’adaptation à un nouveau domaine dans le cas de figure fréquent où des données d’entraînement n’existent que pour un ou plusieurs domaines différents du domaine cible. Cette étude montre en particulier que l’utilisation d’une forme d’auto-apprentissage par laquelle un classifieur annote un corpus du domaine cible et modifie son corpus d’entraînement en y incorporant les textes classés avec la plus grande confiance se révèle comme la stratégie la plus performante et la plus stable pour les différents domaines testés. Cette stratégie s’avère même supérieure dans un nombre significatif de cas à la méthode proposée par (Blitzer et al., 2007) sur les mêmes jeux de test tout en étant plus simple.},
  abstract  = {The work presented in this article takes place in the field of opinion mining and aims more particularly at finding the polarity of a text by relying on machine learning methods. In this context, it focuses on studying various strategies for adapting a statistical classifier to a new domain when training data only exist for one or several other domains. This study shows more precisely that a self-training procedure consisting in enlarging the initial training corpus with texts from the target domain that were reliably classified by the classifier is the most successful and stable strategy for the tested domains. Moreover, this strategy gets better results in most cases than (Blitzer et al., 2007)’s method on the same evaluation corpus while it is more simple.},
  motscles  = {fouille d’opinion, adaptation à un nouveau domaine, auto-apprentissage},
  keywords  = {opinion mining, domain adaptation, self-training},
}

@inproceedings{kraif-diwersy:2012:TALN,
  author    = {Kraif, Olivier and Diwersy, Sascha},
  title     = {Le Lexicoscope : un outil pour l'étude de profils combinatoires et l'extraction de constructions lexico-syntaxiques},
  booktitle = {Actes de la 19e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2012},
  address   = {Grenoble, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {399--406},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2012/taln-2012-court-009},
  language  = {french},
  note      = {The Lexicoscope : an integrated tool for combinatoric profles observation and lexico-syntactic constructs extraction},
  resume    = {Dans le cadre du projet franco-allemand Emolex, dédié à l'étude contrastive de la combinatoire du lexique des émotions en 5 langues, nous avons développé des outils et des méthodes permettant l'extraction, la visualisation et la comparaison de profls combinatoires pour des expressions simples et complexes. Nous présentons ici l'architecture d'ensemble de la plate-forme, conçue pour efectuer des extractions sur des corpus de grandes dimensions (de l'ordre de la centaine de millions de mots) avec des temps de réponse réduits (le corpus étant interrogeable en ligne1). Nous décrivons comment nous avons introduit la notion de pivots complexes, afn de permettre aux utilisateurs de rafner progressivement leurs requêtes pour caractériser des constructions lexico-syntaxiques élaborées. Enfn, nous donnons les premiers résultats d'un module d'extraction automatique d'expressions polylexicales récurrentes.},
  abstract  = {The German-French research project Emolex whose aim is the contrastive study of the combinatorial behaviour of emotion lexemes in 5 languages has led to the development of methods and tools to extract, display and compare the combinatorial profles of simple and complex expressions. In this paper, we present the overall architecture of the query platform which has been conceived to ensure efcient processing of huge annotated text corpora (consisting of several hundred millions of word tokens) accessible through a web-based interface. We put forward the concept of “complex query nodes” introduced to enable users to carry out progressively elaborated extractions of lexical-syntactic patterns. We fnally give primary results of an automated method for the retrieval of recurrent multi-word expressions, which takes advantage of the complex query nodes implementation.},
  motscles  = {collocations, cooccurrences, profl combinatoire, expressions polylexicales, lexique des émotions},
  keywords  = {collocations, combinatorial profles, multi-word expressions},
}

@inproceedings{laroche:2012:TALN,
  author    = {Laroche, Audrey},
  title     = {Analyse des contextes et des candidats dans l'identification des équivalents terminologiques en corpus comparables},
  booktitle = {Actes de la 19e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2012},
  address   = {Grenoble, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {407--414},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2012/taln-2012-court-010},
  language  = {french},
  note      = {Analysis of contexts and candidates in term-translation spotting in comparable corpora},
  resume    = {L’approche standard d’identification d’équivalents terminologiques à partir de corpus comparables repose sur la comparaison de mots contextuels en langues source et cible et sur l’utilisation d’un lexique bilingue. Nous analysons manuellement, selon des critères linguistiques (parties du discours, spécificité et relations sémantiques), les propriétés des mots contextuels et des erreurs commises par l’approche standard appliquée à la terminologie médicale pour suggérer des améliorations basées sur la sélection de mots contextuels.},
  abstract  = {The standard approach for identifying terminological equivalents from comparable corpora is based on the comparison of source and target language context words using a bilingual lexicon. We cary a manual analysis of the linguistic properties (parts of speech, specificity and semantic relations) of the context words and the inacurrate equivalents given by the standard approach applied to medical terminology, in order to suggest improvements based on the selection of context words.},
  motscles  = {équivalents terminologiques, vecteurs contextuels, corpus comparables, terminologie médicale, étude qualitative},
  keywords  = {terminological equivalents, contextual vectors, comparable corpora, medical terminology, qualitative study.},
}

@inproceedings{planas:2012:TALN,
  author    = {Planas, Emmanuel},
  title     = {BiTermEx Un prototype d'extraction de mots composés à partir de documents comparables via la méthode compositionnelle},
  booktitle = {Actes de la 19e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2012},
  address   = {Grenoble, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {415--422},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2012/taln-2012-court-011},
  language  = {french},
  note      = {BiTermEx , A prototype for the extraction of multiword terms from comparable documents through the compositional approach},
  resume    = {Nous décrivons BiTermEx, un prototype d'expérimentation de l'extraction de terminologie bilingue de mots composés, à partir de documents comparables, via la méthode compositionnelle. Nous expliquons la variation morphologique et la combinaison des constituants lexicaux des termes composés. Cette permet une précision TOP1 de 92% et 97,5% en français anglais, et de 94% en français japonais pour l'alignement de termes composés (textes scientifiques et de vulgarisation scientifique).},
  abstract  = {We describe BiTermEx, a prototype for extracting multiword terms from comparable corpora, using the compositional method. We focus on morphology-based variations of multiword constituents and their recombinaison. We experimented our approach on scientific and popular science corpora. We record TOP1 precisions of 92% and 97,5% on French to English alignments and 94% on French to Japanese.},
  motscles  = {extraction terminologique, prototype, terminologie bilingue, documents comparables, méthode compositionnelle, mots composés, corpus},
  keywords  = {term extraction, prototype, bilingual terminology, comparable documents, compositional method, multiword terms, corpus},
}

@inproceedings{serrano-EtAl:2012:TALN,
  author    = {Serrano, Laurie and Charnois, Thierry and Brunessau, Stephan and Grilheres, Bruno and Bouzid, Maroua},
  title     = {Combinaison d'approches pour l'extraction automatique d'événements},
  booktitle = {Actes de la 19e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2012},
  address   = {Grenoble, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {423--430},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2012/taln-2012-court-012},
  language  = {french},
  note      = {Automatic events extraction by combining multiple approaches},
  resume    = {Dans cet article, nous présentons un système d’extraction automatique d’événements fondé sur deux approches actuelles en extraction d’information : la première s’appuie sur des règles linguistiques construites manuellement et la seconde se fonde sur un apprentissage automatique de patrons linguistiques. Les expérimentations réalisées montrent que combiner ces deux méthodes d’extraction permet d’améliorer significativement la qualité des événements extraits (amélioration de près de 10 points de F-mesure).},
  abstract  = {In this paper, we present an automatic system for extracting events based on the combination of two existing information extraction approaches : the first one is made of hand-crafted linguistic rules and the second one is based on an automatic learning of linguistic patterns. We have shown that this mixed approach leads to a significant improvement of extraction performances.},
  motscles  = {Extraction d’information, événements, approche symbolique, apprentissage de patrons linguistiques},
  keywords  = {Text mining, events, symbolic extraction, linguistic pattern learning},
}

@inproceedings{tellier-EtAl:2012:TALN,
  author    = {Tellier, Isabelle and Duchier, Denys and Eshkol, Iris and Courmet, Arnaud and Martinet, Mathieu},
  title     = {Apprentissage automatique d'un chunker pour le français},
  booktitle = {Actes de la 19e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2012},
  address   = {Grenoble, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {431--438},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2012/taln-2012-court-013},
  language  = {french},
  note      = {Machine Learning of a chunker for French},
  resume    = {Nous décrivons dans cet article comment nous avons procédé pour apprendre automatiquement un chunker à partir du French Tree Bank, en utilisant les CRF (Conditional Random Fields). Nous avons réalisé diverses expériences, pour reconnaître soit l’ensemble de tous les chunks possibles, soit les seuls groupes nominaux simples. Nous évaluons le chunker obtenu aussi bien de manière interne (sur le French Tree Bank lui-même) qu’externe (sur un corpus distinct transcrit de l’oral), afin de mesurer sa robustesse.},
  abstract  = {We describe in this paper how to automatically learn a chunker for French, from the French Tree Bank and CRFs (Conditional Random Fields). We did several experiments, either to recognize every possible kind of chunks, or to focus on simple nominal phrases only. We evaluate the obtained chunker on internal data (i.e. also extracted from the French Tree Bank) as well as on external (i.e from a distinct corpus) ones, to measure its robustness.},
  motscles  = {chunking, apprentissage automatique, French Tree Bank, CRF},
  keywords  = {chunking, Machine Learning, French Tree Bank, CRF},
}

@inproceedings{tulechki-tanguy:2012:TALN,
  author    = {Tulechki, Nikola and Tanguy, Ludovic},
  title     = {Effacement de dimensions de similarité textuelle pour l’exploration de collections de rapports d’incidents aéronautiques},
  booktitle = {Actes de la 19e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2012},
  address   = {Grenoble, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {439--446},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2012/taln-2012-court-014},
  language  = {french},
  note      = {Deletion of dimensions of textual similarity for the exploration of collections of accident reports in aviation},
  resume    = {Cet article étudie le lien entre la similarité textuelle et une classification extrinsèque dans des collections de rapports d’incidents aéronautiques. Nous cherchons à compléter les stratégies d’analyse de ces collections en établissant automatiquement des liens de similarité entre les documents de façon à ce qu’ils ne reflètent pas l’organisation des schémas de codification utilisés pour leur classement. Afin de mettre en évidence les dimensions de variation transversales à la classification, nous calculons un score de dépendance entre les termes et les classes et excluons du calcul de similarité les termes les plus corrélés à une classe donnée. Nous montrons par une application sur 500 documents que cette méthode permet effectivement de dégager des thématiques qui seraient passées inaperçues au vu de la trop grande saillance des similarités de haut niveau.},
  abstract  = {In this paper we study the relationship between external classification and textual similarity in collections of incident reports. Our goal is to complement the existing classification-based analysis strategies by automatically establishing similarity links between documents in such a way that they do not reflect the dominant organisation of the classification schemas. In order to discover such transversal dimensions of similarity, we compute association scores between terms and classes and exlude the most correlated terms from the similarity calculation. We demonstrate on a 500 document corpus that by using this method, we can isolate topics that would otherwise have been masked by the dominant dimensions of similarity in the collection.},
  motscles  = {similarité textuelle, classification de documents, corpus spécialisé},
  keywords  = {textual simliarity, document classification, specialised corpora},
}

@inproceedings{afli-barrault-schwenk:2012:TALN,
  author    = {Afli, Haithem and Barrault, Loïc and Schwenk, Holger},
  title     = {Traduction automatique à partir de corpus comparables: extraction de phrases parallèles à partir de données comparables multimodales},
  booktitle = {Actes de la 19e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2012},
  address   = {Grenoble, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {447--454},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2012/taln-2012-court-015},
  language  = {french},
  note      = {Automatic Translation from Comparable corpora : extracting parallel sentences from multimodal comparable corpora},
  resume    = {Les performances des systèmes de traduction automatique statistique dépendent de la disponibilité de textes parallèles bilingues, appelés aussi bitextes. Cependant, les corpus parallèles sont des ressources limitées et parfois indisponibles pour certains couples de langues ou domaines. Nous présentons une technique pour l’extraction de phrases parallèles à partir d’un corpus comparable multimodal (audio et texte). Ces enregistrements sont transcrits avec un système de reconnaissance automatique de la parole et traduits avec un système de traduction automatique. Ces traductions sont ensuite utilisées comme requêtes d’un système de recherche d’information pour sélectionner des phrases parallèles sans erreur et générer un bitexte. Plusieurs expériences ont été menées sur les données de la campagne IWSLT’11 (TED) qui montrent la faisabilité de notre approche.},
  abstract  = {Statistical Machine Translation (SMT) systems depend on the availability of bilingual parallel text, also called bitext. However parallel corpora are a limited resource and are often not available for some domains or language pairs. We present an alternative method for extracting parallel sentences from multimodal comparable corpora. This work extends the use of comparable corpora, in using audio instead of text on the source side. The audio is transcribed by an automatic speech recognition system and translated with a base-line SMT system. We then use information retrieval in a large text corpus of the target language to extract parallel sentences. We have performed a series of experiments on data of the IWSLT’11 speech translation task (TED) that shows the feasibility of our approach.},
  motscles  = {Reconnaissance de la parole, traduction automatique statistique, corpus comparables multimodaux, extraction de phrases parallèles},
  keywords  = {Automatic speech recognition, statistical machine translation, multimodal comparable corpora, extraction of parallel sentences},
}

@inproceedings{benyahia-mezghanihammami-hadrichbelguith:2012:TALN,
  author    = {Ben Yahia, Yacine and Mezghani Hammami, Souha and Hadrich Belguith, Lamia},
  title     = {La reconnaissance automatique de la fonction des pronoms démonstratifs en langue arabe},
  booktitle = {Actes de la 19e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2012},
  address   = {Grenoble, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {455--462},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2012/taln-2012-court-016},
  language  = {french},
  note      = {Automatic recognition of demonstrative pronouns function in Arabic},
  resume    = {La résolution d'anaphores est l'une des tâches les plus difficiles du Traitement Automatique du Langage Naturel (TALN). La capacité de classifier les pronoms avant de tenter une tâche de résolution d'anaphores serait importante, puisque pour traiter un pronom cataphorique le système doit chercher l’antécédent dans le segment qui suit le pronom. Alors que, pour le pronom anaphorique, le système doit chercher l’antécédent dans le segment qui précède le pronom. En outre, le nombre des pronoms a été jugée non-trivial dans la langue arabe. C’est dans ce cadre que se situe notre travail qui consiste à proposer une méthode pour la classification automatique des pronoms démonstratifs arabes, basée sur l’apprentissage. Nous avons évalué notre approche sur un corpus composé de 365585 mots contenant 14318 pronoms démonstratifs et nous avons obtenu des résultats encourageants : 99.3% comme F-Mesure.},
  abstract  = {Anaphora resolution is one of the most difficult tasks in NLP. Classifying pronouns before attempting a task of anaphora resolution is important because to handle the cataphoric pronoun, the system should determine the antecedent into the segment following the pronoun. Although, for the anaphoric pronoun, the system should look for the antecedent into the segment before the pronoun. In addition, the number of demonstrative pronouns is very important in Arabic. In this paper, we describe a machine learning method for classifying demonstrative pronouns in Arabic. We have evaluated our approach on a corpus of 365585 words which contain 14318 demonstrative pronouns and we have obtained encouraging results: 99.3% as F-Measure.},
  motscles  = {Pronoms démonstratifs, résolution des anaphores, traitement de la langue arabe},
  keywords  = {Demonstrative pronouns, anaphora resolution, ANLP},
}

@inproceedings{bittar-hagege:2012:TALN,
  author    = {Bittar, André and Hagège, Caroline},
  title     = {Un annotateur automatique d'expressions temporelles du français et son évaluation sur le TimeBank du français},
  booktitle = {Actes de la 19e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2012},
  address   = {Grenoble, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {463--470},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2012/taln-2012-court-017},
  language  = {french},
  note      = {An Automatic Temporal Expression Annotator and its Evaluation on the French TimeBank},
  resume    = {Dans cet article, nous présentons un outil d’extraction et de normalisation d’un sous-ensemble d’expressions temporelles développé pour le français. Cet outil est mis au point et utilisé dans le cadre du projet ANR Chronolines1 et il est appliqué sur un corpus fourni par l’AFP. Notre but final dans le cadre du projet est de construire semi-automatiquement des chronologies événementielles à partir de la base de depêches de l’AFP. L’une des étapes du traitement est l’analyse de l’information temporelle véhiculée dans les textes. Nous avons donc développé un annotateur d’expressions temporelles pour le français que nous décrivons dans cet article. Nous présenterons également les résultats de son évaluation.},
  abstract  = {In this article, we present a tool that extracts and normalises a subset of temporal expressions in French. This tool is being developed and used in the ANR (French National Research Agency) project Chronolines, applied to a corpus of provided by the Agence France Presse. The aim of the project is to semi-automatically construct event chronologies from this corpus. To do this, a detailed analysis of the temporal information conveyed by texts, is required. The system we present here is the first version of a temporal annotator that we have developed for French. We describe it in this article and present the results of an evaluation.},
  motscles  = {Analyse temporelle, évaluation},
  keywords  = {Temporal processing, evaluation},
}

@inproceedings{danlos-EtAl:2012:TALN,
  author    = {Danlos, Laurence and Antolinos-Basso, Diégo and Braud, Chloé and Roze, Charlotte},
  title     = {Vers le FDTB : French Discourse Tree Bank},
  booktitle = {Actes de la 19e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2012},
  address   = {Grenoble, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {471--478},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2012/taln-2012-court-018},
  language  = {french},
  note      = {Towards the FDTB : French Discourse Tree Bank},
  resume    = {Nous présentons les premiers pas vers la création d’un corpus annoté en discours pour le français : le French Discourse TreeBank enrichissant le FTB. La méthodologie adoptée s’inspire du Penn Discourse TreeBank (PDTB) mais elle s’en distingue sur au moins deux points à caractère théorique. D’abord, notre objectif est de fournir une couverture totale d’un texte du corpus, tandis que le PDTB ne fournit qu’une couverture partielle, qui ne peut donc pas être qualifiée d’analyse discursive comme celle faite en RST ou SDRT, deux théories majeures sur le discours. Ensuite, nous avons été amenés à définir une nouvelle hiérarchie des relations de discours qui s’inspire de RST, de SDRT et du PDTB.},
  abstract  = {We present the first steps towards creating an annotated corpus for discourse in French : the French Discourse Treebank enriching the FTB. Our methodology is based on the Penn Discourse Treebank (PDTB), but it differs in at least two points of a theoretical nature. First, our goal is to provide full coverage of a text, while the PDTB provides only partial coverage, which can not be described as discourse analysis such as the one made in RST or SDRT, two major theories on discourse. Second, we were led to define a new hierarchy of discourse relations which is based on RST, SDRT and PDTB.},
  motscles  = {Discours, corpus annoté manuellement, analyse discursive, PDTB, RST, SDRT},
  keywords  = {Discourse, manually annotated corpus, discourse analysis, PDTB, RST, SDRT},
}

@inproceedings{deveaud-bellot:2012:TALN,
  author    = {Deveaud, Romain and Bellot, Patrice},
  title     = {Combinaison de ressources générales pour une contextualisation implicite de requêtes},
  booktitle = {Actes de la 19e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2012},
  address   = {Grenoble, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {479--486},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2012/taln-2012-court-019},
  language  = {french},
  note      = {Query Contextualization and Reformulation by Combining External Corpora},
  resume    = {L’utilisation de sources externes d’informations pour la recherche documentaire a été considérablement étudiée dans le passé. Des améliorations de performances ont été mises en lumière avec des corpus larges ou structurés. Néanmoins, dans ces études les ressources sont souvent utilisées séparément mais rarement combinées. Nous présentons une évaluation de la combinaison de quatre différentes ressources générales, standards et accessibles. Nous utilisons une mesure de distance informative pour extraire les caractéristiques contextuelles des différentes ressources et améliorer la représentation de la requête. Cette évaluation est menée sur une tâche de recherche d’information sur le Web en utilisant le corpus ClueWeb09 et les topics de la piste Web de TREC. Les meilleurs résultats sont obtenus en combinant les quatre ressources, et sont statistiquement significativement supérieurs aux autres approches.},
  abstract  = {Improving document retrieval using external sources of information has been extensively studied throughout the past. Improvements with either structured or large corpora have been reported. However, in these studies resources are often used separately and rarely combined together. We present an evaluation of the combination of four different scalable corpora over a web search task. An informative divergence measure is used to extract contextual features from the corpora and improve query representation. We use the ClueWeb09 collection along with TREC’s Web Track topics for the purpose of our evaluation. Best results are achieved when combining all four corpora, and are significantly better than the results of other approaches.},
  motscles  = {Combinaison de ressources, RI contextuelle, recherche web},
  keywords  = {Resources combination, contextual IR, web search.},
}

@inproceedings{gahbichebraham-EtAl:2012:TALN,
  author    = {Gahbiche-Braham, Souhir and Bonneau-Maynard, Hélène and Lavergne, Thomas and Yvon, François},
  title     = {Repérage des entités nommées pour l'arabe : adaptation non-supervisée et combinaison de systèmes},
  booktitle = {Actes de la 19e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2012},
  address   = {Grenoble, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {487--494},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2012/taln-2012-court-020},
  language  = {french},
  note      = {Named Entity Recognition for Arabic : Unsupervised adaptation and Systems combination},
  resume    = {La détection des Entités Nommées (EN) en langue arabe est un prétraitement potentiellement utile pour de nombreuses applications du traitement des langues, en particulier pour la traduction automatique. Cette tâche représente toutefois un sérieux défi, compte tenu des spécificités de l’arabe. Dans cet article, nous présentons un compte-rendu de nos efforts pour développer un système de repérage des EN s’appuyant sur des méthodes statistiques, en détaillant les aspects liés à la sélection des caractéristiques les plus utiles pour la tâche ; puis diverses tentatives pour adapter ce système d’une manière entièrement non supervisée.},
  abstract  = {The recognition of Arabic Named Entities (NE) is a potentially useful preprocessing step for many Natural Language Processing Applications, such as Machine Translation. This task is however made very complex by some peculiarities of the Arabic language. In this paper, we present a summary of our recent efforts aimed at developing a statistical NE recognition system, with a specific focus on feature engineering aspects. We also report several approaches for adapting this system in an entirely unsupervised manner to a new domain.},
  motscles  = {Adaptation non supervisée, Repérage des entités nommées},
  keywords  = {Unsupervised domain adaptation, named entity recognition},
}

@inproceedings{gala-brun:2012:TALN,
  author    = {Gala, Nuria and Brun, Caroline},
  title     = {Propagation de polarités dans des familles de mots : impact de la morphologie dans la construction d'un lexique pour l'analyse de sentiments},
  booktitle = {Actes de la 19e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2012},
  address   = {Grenoble, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {495--502},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2012/taln-2012-court-021},
  language  = {french},
  note      = {Spreading Polarities among Word Families: Impact of Morphology on Building a Lexicon for Sentiment Analysis},
  resume    = {Les ressources lexicales sont cruciales pour de nombreuses applications de traitement automatique de la langue (par exemple, l'extraction d'opinions à partir de corpus). Cependant, leur construction pose des problèmes à différents niveaux (coût, couverture, etc.). Dans cet article, nous avons voulu vérifier si les informations morphologiques liées à la dérivation pouvaient être exploitées pour l'annotation automatique d'informations sémantiques. En partant d'une ressource regroupant les mots en familles morphologiques en français, nous avons construit un lexique de polarités pour 4 065 mots, à partir d'une liste initiale d'adjectifs annotés manuellement. Les résultats obtenus montrent que la propagation des polarités est correcte pour 78,89% des familles avec un seul adjectif. Le lexique ainsi obtenu améliore aussi les résultats du système d'extraction d'opinions.},
  abstract  = {Lexical resources are essential for many natural language applications (for example, opinion mining from corpora). However, building them entails different problems (cost, coverage, etc.). In this paper, we wanted to verify whether morphological information about derivation could be used to automatically annotate semantic information. Starting from a resource that groups words into morphological families in French, we have built a lexicon with polarities for 4 065 words from an initial seed set of manual annotated adjectives. The results obtained show that spreading polarities is accurate for 78.89% of the families with a unique adjective. The lexicon obtained also improves the results of the opinion mining system on different corpora.},
  motscles  = {ressources lexicales, morphologie dérivationnelle, analyse de sentiments},
  keywords  = {lexical resources, derivational morphology, opinion mining},
}

@inproceedings{labadie-enjalbert-ferrari:2012:TALN,
  author    = {Labadié, Alexandre and Enjalbert, Patrice and Ferrari, Stéphane},
  title     = {Transitions thématiques : Annotation d'un corpus journalistique et premières analyses},
  booktitle = {Actes de la 19e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2012},
  address   = {Grenoble, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {503--510},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2012/taln-2012-court-022},
  language  = {french},
  note      = {Manual thematic annotation of a journalistic corpus : first observations and evaluation},
  resume    = {Le travail présenté dans cet article est centré sur la constitution d’un corpus de textes journalistiques annotés au niveau discursif d’un point de vue thématique. Le modèle d’annotation est une segmentation classique, à laquelle nous ajoutons un repérage de zones de transition entre unités thématiques. Nous faisons l’hypothèse que dans un texte bien construit, le scripteur fournit des indications aidant le lecteur à passer d’un sujet à un autre, l’identification de ces indices étant susceptible d’améliorer les procédures de segmentation automatique. Les annotations produites ont fait l’objet d’analyses quantitatives mettant en évidence un ensemble de propriétés des transitions entre thèmes.},
  abstract  = {The work presented in this paper focuses on the creation of a corpus of journalistic texts annotated at dicourse level, more precisely on a topic level. The annotation model is a classic segmentation one, to which we add transition zones between topical units. We assume that in a well-structured text, the author provides information helping the reader to move from one topic to another, where an identification of these clues is likely to improve automatic segmentation. The produced annotations have been subject of several quantitative analyses showing a set of linguistic properties of topical transitions.},
  motscles  = {Structure du discours, segments thématiques, transitions thématiques, annotation},
  keywords  = {Discourse structure, topical segments, topical transitions, annotation},
}

@inproceedings{plaisantinalecu-thomas-renahy:2012:TALN,
  author    = {Plaisantin Alecu, Blandine and Thomas, Izabella and Renahy, Julie},
  title     = {La "multi-extraction" comme stratégie d'acquisition optimisée de ressources terminologiques et non terminologiques},
  booktitle = {Actes de la 19e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2012},
  address   = {Grenoble, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {511--518},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2012/taln-2012-court-023},
  language  = {french},
  note      = {Multi-extraction as a strategy of optimized extraction of terminological and lexical resources},
  resume    = {A partir de l'évaluation d'extracteurs de termes menée initialement pour détecter le meilleur outil d'acquisition du lexique d'une langue contrôlée, nous proposons dans cet article une stratégie d'optimisation du processus d'extraction terminologique. Nos travaux, menés dans le cadre du projet ANR Sensunique, prouvent que la « multiextraction », c'est-à-dire la coopération de plusieurs extracteurs de termes, donne des résultats significativement meilleurs que l’extraction via un seul outil. Elle permet à la fois de réduire le silence et de filtrer automatiquement le bruit grâce à la variation d'un indice relatif au potentiel terminologique.},
  abstract  = {Based on the evaluation of terminological extractors, initially to find the best tool for building a controlled language lexicon, we propose a strategy of optimized extraction of terminological resources. Our work highlights that the cooperation of several extraction tools gives better results than the use of a single one. It both reduces silence and automatically filters noise thanks to a variable related to termhood.},
  motscles  = {terminologie, extraction, langue contrôlée, potentiel terminologique, filtrage de termes},
  keywords  = {terminology, extraction, controlled language, termhood, term filtering},
}

@inproceedings{renard-calabretto-rumpler:2012:TALN,
  author    = {Renard, Arnaud and Calabretto, Sylvie and Rumpler, Béatrice},
  title     = {Une Approche de Recherche d’Information Structurée fondée sur la Correction d’Erreurs à l’Indexation des Documents},
  booktitle = {Actes de la 19e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2012},
  address   = {Grenoble, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {519--526},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2012/taln-2012-court-024},
  language  = {french},
  note      = {Structured Information Retrieval Approach based on Indexing Time Error Correction},
  resume    = {Dans cet article, nous nous sommes intéressés à la prise en compte des erreurs dans les contenus textuels des documents XML. Nous proposons une approche visant à diminuer l’impact de ces erreurs sur les systèmes de Recherche d’Information (RI). En effet, ces systèmes produisent des index associant chaque document aux termes qu’il contient. Les erreurs affectent donc la qualité des index ce qui conduit par exemple à considérer à tort des documents mal indexés comme non pertinents (resp. pertinents) vis-à-vis de certaines requêtes. Afin de faire face à ce problème, nous proposons d’inclure un mécanisme de correction d’erreurs lors de la phase d’indexation des documents. Nous avons implémenté cette approche au sein d’un prototype que nous avons évalué dans le cadre de la campagne d’évaluation INEX.},
  abstract  = {In this paper, we focused on errors in the textual content of XML documents. We propose an approach to reduce the impact of these errors on Information Retrieval (IR) systems. Indeed, these systems rely on indexes associating each document to corresponding terms. Indexes quality is negatively affected by those misspellings. These errors makes it difficult to later retrieve documents (or parts of them) in an effective way during the querying phase. In order to deal with this problem we propose to include an error correction mechanism during the indexing phase of documents. We achieved an implementation of this spelling aware information retrieval system which is currently evaluated over INEX evaluation campaign documents collection.},
  motscles  = {Recherche d’information, dysorthographie, correction d’erreurs, xml},
  keywords  = {Information retrieval, misspellings, error correction, xml},
}

@inproceedings{rubino-EtAl:2012:TALN,
  author    = {Rubino, Raphaël and Huet, Stéphane and Lefèvre, Fabrice and Linarès, Georges},
  title     = {Post-édition statistique pour l'adaptation aux domaines de spécialité en traduction automatique},
  booktitle = {Actes de la 19e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2012},
  address   = {Grenoble, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {527--534},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2012/taln-2012-court-025},
  language  = {french},
  note      = {Statistical Post-Editing of Machine Translation for Domain Adaptation},
  resume    = {Cet article présente une approche de post-édition statistique pour adapter aux domaines de spécialité des systèmes de traduction automatique génériques. En utilisant les traductions produites par ces systèmes, alignées avec leur traduction de référence, un modèle de post-édition basé sur un alignement sous-phrastique est construit. Les expériences menées entre le français et l’anglais pour le domaine médical montrent qu’une telle adaptation a posteriori est possible. Deux systèmes de traduction statistiques sont étudiés : une implémentation locale état-de-l’art et un outil libre en ligne. Nous proposons aussi une méthode de sélection de phrases à post-éditer permettant d’emblée d’accroître la qualité des traductions et pour laquelle les scores oracles indiquent des gains encore possibles.},
  abstract  = {This paper presents a statistical approach to adapt generic machine translation systems to the medical domain through an unsupervised post-edition step. A statistical post-edition model is built on statistical machine translation outputs aligned with their translation references. Evaluations carried out to translate medical texts from French to English show that a generic machine translation system can be adapted a posteriori to a specific domain. Two systems are studied : a state-of-the-art phrase-based implementation and an online publicly available software. Our experiments also indicate that selecting sentences for post-edition leads to significant improvements of translation quality and that more gains are still possible with respect to an oracle measure.},
  motscles  = {Traduction automatique statistique, post-édition, adaptation aux domaines de spécialité},
  keywords  = {Statistical Machine Translation, Post-editing, Domain Adaptation},
}

@inproceedings{sagot-richard-stern:2012:TALN,
  author    = {Sagot, Benoît and Richard, Marion and Stern, Rosa},
  title     = {Annotation référentielle du Corpus Arboré de Paris 7 en entités nommées},
  booktitle = {Actes de la 19e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2012},
  address   = {Grenoble, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {535--542},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2012/taln-2012-court-026},
  language  = {french},
  note      = {Referential named entity annotation of the Paris 7 French TreeBank},
  resume    = {Le Corpus Arboré de Paris 7 (ou French TreeBank) est le corpus de référence pour le français aux niveaux morphosyntaxique et syntaxique. Toutefois, il ne contient pas d’annotations explicites en entités nommées. Ces dernières sont pourtant parmi les informations les plus utiles pour de nombreuses tâches en traitement automatique des langues et de nombreuses applications. De plus, aucun corpus du français annoté en entités nommées et de taille importante ne contient d’annotation référentielle, qui complète les informations de typage et d’empan sur chaque mention par l’indication de l’entité à laquelle elle réfère. Nous avons annoté manuellement avec ce type d’informations, après pré-annotation automatique, le Corpus Arboré de Paris 7. Nous décrivons les grandes lignes du guide d’annotation sous-jacent et nous donnons quelques informations quantitatives sur les annotations obtenues.},
  abstract  = {The French TreeBank developed at the University Paris 7 is the main source of morphosyntactic and syntactic annotations for French. However, it does not include explicit information related to named entities, which are among the most useful information for several natural language processing tasks and applications. Moreover, no large-scale French corpus with named entity annotations contain referential information, which complement the type and the span of each mention with an indication of the entity it refers to. We have manually annotated the French TreeBank with such information, after an automatic pre-annotation step. We sketch the underlying annotation guidelines and we provide a few figures about the resulting annotations.},
  motscles  = {Résolution d’entités nommées, Corpus annoté, Corpus arboré de Paris 7},
  keywords  = {Named entity resolution, Annotated corpus, French TreeBank},
}

@inproceedings{servan-petitrenaud:2012:TALN,
  author    = {Servan, Christophe and Petitrenaud, Simon},
  title     = {Utilisation des fonctions de croyance pour l'estimation de paramètres en traduction automatique},
  booktitle = {Actes de la 19e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2012},
  address   = {Grenoble, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {543--550},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2012/taln-2012-court-027},
  language  = {french},
  note      = {Feature calculation for Statistical Machine Translation by using belief functions},
  resume    = {Cet article concerne des travaux effectués dans le cadre du 7ème atelier de traduction automatique statistique et du projet ANR COSMAT. Ces travaux se focalisent sur l’estimation de paramètres contenus dans une table de traduction. L’approche classique consiste à estimer ces paramètres à partir de fréquences relatives d’éléments de traduction. Dans notre approche, nous proposons d’utiliser le concept de masses de croyance afin d’estimer ces paramètres. La théorie des fonctions de croyances est une théorie très adaptée à la gestion des incertitudes dans de nombreux domaines. Les expériences basées sur notre approche s’appliquent sur la traduction de la paire de langue français-anglais dans les deux sens de traduction.},
  abstract  = {In this paper, we consider the translation of texts within the framework of the 7th Workshop of Machine Translation evaluation task and the COSMAT corpus using a statistical machine translation approach. This work is focused on the translation features calculation of the phrase contained in a phrase table. The classical way to estimate these features are based on the direct computation counts or frequencies. In our approach, we propose to use the concept of belief masses to estimate the phrase probabilities. The Belief Function theory has proven to be suitable and adapted for the management of uncertainties in many domains. The experiments based on our approach are focused on the language pair English-French.},
  motscles  = {Traduction automatique statistique, fonctions de croyance, apprentissage automatique, estimation de paramètres},
  keywords  = {Statistical machine Translation, belief function, machine learning, feature estimation},
}

@inproceedings{suignard-cailliau-cavet:2012:TALN,
  author    = {Suignard, Philippe and Cailliau, Frederik and Cavet, Ariane},
  title     = {La longueur des tours de parole comme critère de sélection de conversations dans un centre d’appels},
  booktitle = {Actes de la 19e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2012},
  address   = {Grenoble, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {551--558},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2012/taln-2012-court-028},
  language  = {french},
  note      = {Turn-taking length as criterion to select call center conversations},
  resume    = {Cet article s’intéresse aux conversations téléphoniques d’un Centre d’Appels EDF, automatiquement découpées en « tours de parole » et automatiquement transcrites. Il fait apparaître une relation entre la longueur des tours de parole et leur contenu, en ce qui concerne le vocabulaire qui les compose et les sentiments qui y sont véhiculés. Après avoir montré qu’il y a un intérêt à étudier ces longs tours, l’article analyse leur contenu et liste quelques exemples autour des notions d’argumentation et de réclamation. Il montre ainsi que la longueur des tours de parole peut être un critère utile de sélection de conversations.},
  abstract  = {This article focuses on telephone conversations collected in an EDF Call Center, automatically segmented in “turn-taking” and automatically transcribed. It shows a relationship between the length of the turns and their content regarding the vocabulary and the feelings that are conveyed. After showing that there is an interest in studying these long turns, the article analyzes their content and lists some examples around the notions of argumentation and claim. It shows that the length of turns can be a useful criterion for selecting conversations.},
  motscles  = {Centre d’appels, Conversation, Tour de parole, Reconnaissance de Parole},
  keywords  = {Call Center, Conversation, Turn Taking, Automatic Speech Recognition},
}

@inproceedings{vanrullen-boutora-dagron:2012:TALN,
  author    = {Vanrullen, Tristan and Boutora, Leïla and Dagron, Jean},
  title     = {Enjeux méthodologiques, linguistiques et informatiques pour le traitement du français écrit des sourds},
  booktitle = {Actes de la 19e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2012},
  address   = {Grenoble, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {559--566},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2012/taln-2012-court-029},
  language  = {french},
  note      = {Methodological, linguistic and computational challenges for processing written French of deaf people},
  resume    = {L’ouverture du Centre National de Réception des Appels d’Urgence (CNRAU) accessible aux sourds et malentendants fait émerger des questions linguistiques qui portent sur le français écrit des sourds, et des questions informatiques dans le domaine du traitement automatique du langage naturel. Le français écrit des sourds, pratiqué par une population hétérogène, comporte des spécificités morpho-syntaxiques et morpholexicales qui peuvent rendre problématique la communication écrite entre les personnes sourdes appelantes et les agents du CNRAU. Un premier corpus de français écrit sourd élicité avec mise en situation d’urgence (FAX-ESSU) a été recueilli dans la perspective de proposer des solutions TAL et linguistiques aux agents du CNRAU dans le cadre de ces échanges écrits. Nous présentons une première étude lexicale, morphosyntaxique et syntaxique de ce corpus reposant en partie sur une chaîne de traitement automatique, afin de valider les phénomènes linguistiques décrits dans la littérature et d'enrichir la connaissance du français écrit des sourds.},
  abstract  = {With the setup of a national emergency call-center for deaf people in France (CNRAU), some questions arise in linguistics and natural language processing about the written expression of deaf people. It is practiced by an heterogeneous population and shows morpho-syntactic, lexical and syntactic specificities which increase the difficulty, over the emergency situation, to successfully communicate between the deaf callers and the call-center operators. A first corpus (FAX-ESSU) of written French of deaf people was built with emergency conditions in order to provide linguistic and NLP solutions to the call center operators. On this corpus, we present a first study realized with the help of a natural language processing toolbox, in order to validation linguistic phenomenons described in the scientific literature and to enrich the knowledge of written French of deaf people.},
  motscles  = {Français écrit des sourds, TAL, Français Langue Etrangère, linguistique de corpus, lexique, syntaxe, méthodologie},
  keywords  = {Written French of deaf people, NLP, French as a foreign language, corpus linguistics, lexicon, syntax, methodology},
}

@inproceedings{guillaume-EtAl:2012:TALN,
  author    = {Guillaume, Bruno and Bonfante, Guillaume and Masson, Paul and Morey, Mathieu and Perrier, Guy},
  title     = {Grew : un outil de réécriture de graphes pour le TAL},
  booktitle = {Actes de la 19e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2012},
  address   = {Grenoble, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {1--2},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2012/taln-2012-demo-001},
  language  = {french},
  note      = {Grew: a Graph Rewriting Tool for NLP},
  resume    = {Nous présentons un outil de réécriture de graphes qui a été conçu spécifiquement pour des applications au TAL. Il permet de décrire des graphes dont les noeuds contiennent des structures de traits et dont les arcs décrivent des relations entre ces noeuds. Nous présentons ici la réécriture de graphes que l’on considère, l’implantation existante et quelques expérimentations.},
  abstract  = {We present a Graph Rewriting Tool dedicated to NLP applications. Graph nodes contain feature structures and edges describe relations between nodes. We explain the Graph Rewriting framework we use, the implemented system and some experiments.},
  motscles  = {réécriture de graphes, interface syntaxe-sémantique},
  keywords  = {graph rewriting, syntax-semantics interface},
}

@inproceedings{damnati:2012:TALN,
  author    = {Damnati, Géraldine},
  title     = {Interfaces de navigation dans des contenus audio et vidéo},
  booktitle = {Actes de la 19e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2012},
  address   = {Grenoble, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {3--4},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2012/taln-2012-demo-002},
  language  = {french},
  note      = {Navigation interfaces through audio and video contents},
  resume    = {Deux types de démonstrateurs sont présentés. Une première interface à visée didactique permet d'observer des traitements automatiques sur des documents vidéo. Plusieurs niveaux de représentation peuvent être montrés simultanément, ce qui facilite l'analyse d'approches multi-vues. La seconde interface est une interface opérationnelle de "consommation" de documents audio. Elle offre une expérience de navigation enrichie dans des documents audio grâce à une visualisation de métadonnées extraites automatiquement.},
  abstract  = {Two types of demonstrators are shown. A first interface, with didactic purposes, allows automatic processing of video documents to be observed. Several representation levels can be viewed simultaneously, which is particularly helpful to analyse the behaviour of multi-view approaches. The second interface is an operational audio document "consumption" interface. It offers an enriched navigation experience through the visualisation of automatically extracted metadata.},
  motscles  = {Traitements multi-vues, navigation enrichie},
  keywords  = {Multi-view processing, enriched navigation},
}

@inproceedings{clement:2012:TALN,
  author    = {Clément, Lionel},
  title     = {Synthèse de texte avec le logiciel Syntox},
  booktitle = {Actes de la 19e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2012},
  address   = {Grenoble, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {5--6},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2012/taln-2012-demo-003},
  language  = {french},
  note      = {Automated generation of text with Syntox},
  resume    = {Le logiciel Syntox, dont une interface utilisateur en ligne ce trouve à cette URL : http://www.syntox.net, est une mise en application d’un modèle basé sur les grammaires attribuées, dans le cadre de la synthèse de texte. L’outil est une plateforme d’expérimentation dont l’ergonomie est simple. Syntox est capable de traiter des lexiques et des grammaires volumineux sur des textes ambigus à partir de la description explicite de phénomènes linguistiques.},
  abstract  = {Syntox, which includes an online user interface at URL http://www.syntox.net, is an implementation of a model based on attribute grammars, in the context of automated generation of text. The sofware is intended as a platform for experimentation with an ergonomic interface. Syntox is usable with large vocabularies and grammars to produce ambiguous texts from an explicit description of linguistic phenomena.},
  motscles  = {Synthèse de texte, Grammaire attribuée, Syntaxe},
  keywords  = {Text generation, Attribute Grammars, Syntax},
}

@inproceedings{tellier-dupont-courmet:2012:TALN,
  author    = {Tellier, Isabelle and Dupont, Yoann and Courmet, Arnaud},
  title     = {Un segmenteur-étiqueteur et un chunker pour le français},
  booktitle = {Actes de la 19e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2012},
  address   = {Grenoble, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {7--8},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2012/taln-2012-demo-004},
  language  = {french},
  note      = {A Segmenter-POS Labeller and a Chunker for French},
  resume    = {Nous proposons une démonstration de deux programmes : un segmenteur-étiqueteur POS pour le français et un programme de parenthésage en “chunks” de textes préalablement traités par le programme précédent. Tous deux ont été appris à partir du French Tree Bank.},
  abstract  = {We propose a demo of two softwares : a Segmenter-POS Labeller for French and a Chunker for texts treated by the first program. Both have been learned from the French Tree Bank.},
  motscles  = {étiquetage POS, chunking, apprentissage automatique, French Tree Bank, CRF},
  keywords  = {POS tagging, chunking, Machine Learning, French Tree Bank, CRF},
}

@inproceedings{bigi:2012:TALN,
  author    = {Bigi, Brigitte},
  title     = {SPPAS : segmentation, phonétisation, alignement, syllabation},
  booktitle = {Actes de la 19e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2012},
  address   = {Grenoble, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {9--10},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2012/taln-2012-demo-005},
  language  = {french},
  note      = {SPPAS : a tool to perform text/speech alignment},
  resume    = {SPPAS est le nouvel outil du LPL pour l’alignement texte/son. La segmentation s’opère en 4 étapes successives dans un processus entièrement automatique ou semi-automatique, à partir d’un fichier audio et d’une transcription. Le résultat comprend la segmentation en unités inter-pausales, en mots, en syllabes et en phonèmes. La version actuelle propose un ensemble de ressources qui permettent le traitement du français, de l’anglais, de l’italien et du chinois. L’ajout de nouvelles langues est facilitée par la simplicité de l’architecture de l’outil et le respect des formats de fichiers les plus usuels. L’outil bénéficie en outre d’une documentation en ligne et d’une interface graphique afin d’en faciliter l’accessibilité aux non-informaticiens. Enfin, SPPAS n’utilise et ne contient que des ressources et programmes sous licence libre GPL.},
  abstract  = {SPPAS is a new tool dedicated to phonetic alignments, from the LPL laboratory. SPPAS produces automatically or semi-automatically annotations which include utterance, word, syllabic and phonemic segmentations from a recorded speech sound and its transcription. SPPAS is currently implemented for French, English, Italian and Chinese There is a very simple procedure to add other languages in SPPAS : it is just needed to add related resources in the appropriate directories. SPPAS can be used by a large community of users : accessibility and portability are importants aspects in its development. The tools and resources will all be distributed with a GPL license.},
  motscles  = {segmentation, phonétisation, alignement, syllabation},
  keywords  = {segmentation, phonetization, alignement, syllabification},
}

@inproceedings{chaumartin:2012:TALN,
  author    = {Chaumartin, François-Régis},
  title     = {Solution Proxem d’analyse sémantique verticale : adaptation au domaine des Ressources Humaines},
  booktitle = {Actes de la 19e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2012},
  address   = {Grenoble, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {11--12},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2012/taln-2012-demo-006},
  language  = {french},
  note      = {How to adapt the Proxem semantic analysis engine to the Human Resources field},
  resume    = {Proxem développe depuis 2007 une plate-forme de traitement du langage, Antelope, qui permet de construire rapidement des applications sémantiques verticales (par exemple, pour l’e-réputation, la veille économique ou l’analyse d’avis de consommateurs). Antelope a servi à créer une solution pour les Ressources Humaines, utilisée notamment par l’APEC, permettant (1) d’extraire de l’information à partir d’offres et de CVs et (2) de trouver les offres d’emploi correspondant le mieux à un CV (ou réciproquement). Nous présentons ici l’adaptation d’Antelope à un domaine particulier, en l’occurrence les RH.},
  abstract  = {Proxem develops since 2007 the NLP platform, Antelope, with which one can quickly build vertical semantic applications (for e-reputation, business intelligence or consumer reviews analysis, for instance). Antelope was used to create a Human Resources solution, notably used by APEC, making it possible (1) to extract information from resumes and offers and (2) to find the most relevant jobs matching a given resume (or vice versa). We present here how to adapt Antelope to a particular area, namely HR.},
  motscles  = {entités nommées, extraction de relations, création d’ontologies, similarité},
  keywords  = {named entities, information extraction, ontologies development, matching},
}

@inproceedings{delpech-candillier:2012:TALN,
  author    = {Delpech, Estelle and Candillier, Laurent},
  title     = {Nomao : un moteur de recherche géolocalisé spécialisé dans la recommandation de lieux et l’e-réputation},
  booktitle = {Actes de la 19e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2012},
  address   = {Grenoble, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {13--14},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2012/taln-2012-demo-007},
  language  = {french},
  note      = {Nomao : a geolocalized search engine dedicated to place recommendation and ereputation},
  resume    = {Cette démonstration présente NOMAO, un moteur de recherche géolocalisé qui permet à ses utilisateurs de trouver des lieux (bars, magasins...) qui correspondent à leurs goûts, à ceux de leurs amis et aux recommandations des internautes.},
  abstract  = {This demonstration showcases NOMAO, a geolocalized search engine which recommends places (bars, shops...) based on the user’s and its friend’s tastes and on the web surfers’ recommendations},
  motscles  = {recherche d’information, analyse d’opinion, génération de texte, fouille du web},
  keywords  = {information retrieval, opinion mining, text generation, web mining},
}

@inproceedings{moukrim:2012:TALN,
  author    = {Moukrim, Samira},
  title     = {Le DictAm : Dictionnaire électronique des verbes amazighs-français},
  booktitle = {Actes de la 19e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2012},
  address   = {Grenoble, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {15--16},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2012/taln-2012-demo-008},
  language  = {french},
  note      = {The DictAm : An electronic dictionary of Amazigh-French verbs},
  resume    = {Le DictAm est un dictionnaire électronique des verbes amazighs-français. Il vise à rendre compte de l’ensemble des verbes dans le domaine berbère : conjugaison, diathèse et sens. Le DictAm comporte actuellement près de 3000 verbes dans une soixantaine de parlers berbères. C’est un travail qui est en cours de réalisation et qui a pour ambition de répertorier tous les verbes berbères ainsi que leurs équivalents en français.},
  abstract  = {In the frame of promoting the linguistic diversity among knowledge society, we suggest to elaborate an electronic dictionary of Amazigh-French verbs (DictAm). This dictionary aims at accounting for the verbs in the Amazigh sphere as a whole: conjugation, diathesis and meaning. The DictAm also adopts a comparative perspective in the sense that it collects the lexical materials of different dialectal varieties and makes them reachable. Now, the DictAm comprises around 3000 verbs deriving from about sixty Amazigh speeches. This work is currently in progress as well as it aspires to set up a repertoire of all Amazigh verbs and their French equivalents.},
  motscles  = {Dictionnaire électronique, dimension bilingue, diversité linguistique, verbes},
  keywords  = {Electronic dictionary, bilingual dimension, linguistic diversity, verbs},
}

@inproceedings{hueber-EtAl:2012:TALN,
  author    = {Hueber, Thomas and Ben-Youssef, Atef and Badin, Pierre and Bailly, Gérard and Eliséi, Frédéric},
  title     = {Vizart3D : Retour Articulatoire Visuel pour l’Aide à la Prononciation},
  booktitle = {Actes de la 19e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2012},
  address   = {Grenoble, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {17--18},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2012/taln-2012-demo-009},
  language  = {french},
  note      = {Vizart3D: Visual Articulatory Feedack for Computer-Assisted Pronunciation Training},
  resume    = {L’objectif du système Vizart3D est de fournir à un locuteur, en temps réel, et de façon automatique, un retour visuel sur ses propres mouvements articulatoires. Les applications principales de ce système sont l’aide à l’apprentissage des langues étrangères et la rééducation orthophonique (correction phonétique). Le système Vizart3D est basé sur la tête parlante 3D développée au GIPSA-lab, qui laisse apparaître, en plus des lèvres, les articulateurs de la parole normalement cachés (comme la langue). Cette tête parlante est animée automatiquement à partir du signal audio de parole, à l’aide de techniques de conversion de voix et de régression acoustico-articulatoire par GMM.},
  abstract  = {We describe a system of visual articulatory feedback, which aims to provide any speaker with a real feedback on his/her own articulation. Application areas are computerassisted pronunciation training (phonetic correction) for second-language learning and speech rehabilitation. This system, named Vizartd3D, is based on the 3D augmented talking head developed at GIPSA-lab, which is able to display all speech articulators including usually hidden ones like the tongue. In our approach, the talking head is animated automatically from the audio speech signal, using GMM-based voice conversion and acoustic-to-articulatory regression.},
  motscles  = {retour visuel, aide à la prononciation, GMM, temps réel, tête parlante},
  keywords  = {visual feedback, pronunciation training, GMM, real-time, talking head},
}

@inproceedings{ferragne-flavier-fressard:2012:TALN,
  author    = {Ferragne, Emmanuel and Flavier, Sébastien and Fressard, Christian},
  title     = {ROCme! : logiciel pour l’enregistrement et la gestion de corpus oraux},
  booktitle = {Actes de la 19e conférence sur le Traitement Automatique des Langues Naturelles},
  month     = {June},
  year      = {2012},
  address   = {Grenoble, France},
  publisher = {Association pour le Traitement Automatique des Langues},
  pages     = {19--20},
  url       = {http://www.atala.org/taln_archives/TALN/TALN-2012/taln-2012-demo-010},
  language  = {french},
  note      = {ROCme!: software for the recording and management of oral corpora},
  resume    = {ROCme! permet une gestion rationalisée, autonome et dématérialisée de l’enregistrement de corpus oraux. Il dispose notamment d’une interface pour le recueil de métadonnées sur les locuteurs totalement paramétrable via des balises XML. Les locuteurs peuvent gérer les réponses au questionnaire, l’enregistrement audio, la lecture, la sauvegarde et le défilement des phrases (ou autres types de corpus) en toute autonomie. ROCme! affiche du texte, avec ou sans mise en forme HTML, des images, du son et des vidéos.},
  abstract  = {management of speech recordings. Users can create interfaces for metadata collection thanks to XML tags. Speakers autonomously fill in questionnaires, record, play, and save audio; and browse sentences (or other types of corpora). ROCme! can display text, optionally with HTML formatting, images, sounds, and video.},
  motscles  = {corpus, oral, linguistique, logiciel},
  keywords  = {corpus, oral, linguistics, software},
}