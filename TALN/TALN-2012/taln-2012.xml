<?xml version="1.0" encoding="UTF-8"?>
<conference>
	<edition>
		<acronyme>TALN'2012</acronyme>
		<titre>19e conférence sur le Traitement Automatique des Langues Naturelles</titre>
		<ville>Grenoble</ville>
		<pays>France</pays>
		<dateDebut>2012-06-04</dateDebut>
		<dateFin>2012-06-08</dateFin>
		<presidents>
			<nom>Georges Antoniadis</nom>
			<nom>Hervé Blanchon</nom>
		</presidents>
		<typeArticles>
			<type id="long">Papiers longs</type>
			<type id="court">Papiers courts</type>
			<type id="démonstration">Démonstrations</type>
		</typeArticles>
		<statistiques>
			<acceptations id="long" soumissions="62">24</acceptations>
			<acceptations id="court" soumissions="61">29</acceptations>
		</statistiques>
		<siteWeb>http://www.jeptaln2012.org/</siteWeb>
		<meilleurArticle>
			<articleId>taln-2012-long-005</articleId>
		</meilleurArticle>
	</edition>
	<articles>
		<article id="taln-2012-long-001" session="Extraction d’informations/de relations">
			<auteurs>
				<auteur>
					<nom>Anne-Lyse Minard</nom>
					<email>anne-lyse.minard@limsi.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Anne-Laure Ligozat</nom>
					<email>anne-laure.ligozat@limsi.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>3</affiliationId>
				</auteur>
				<auteur>
					<nom>Brigitte Grau</nom>
					<email>brigitte.grau@limsi.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>3</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIMSI-CNRS, BP 133, 91403 Orsay Cedex</affiliation>
				<affiliation affiliationId="2">Université Paris Sud, 91400 Orsay</affiliation>
				<affiliation affiliationId="3">ENSIIE, square de la résistance, 91000 Évry</affiliation>
			</affiliations>
			<titre>Simplification de phrases pour l'extraction de relations</titre>
			<type>long</type>
			<pages>1-14</pages>
			<resume>L’extraction de relations par apprentissage nécessite un corpus annoté de très grande taille pour couvrir toutes les variations d’expressions des relations. Pour contrer ce problème, nous proposons une méthode de simplification de phrases qui permet de réduire la variabilité syntaxique des relations. Elle nécessite l’annotation d’un petit corpus qui sera par la suite augmenté automatiquement. La première étape est l’annotation des simplifications grâce à un classifieur à base de CRF, puis l’extraction des relations, et ensuite une complétion automatique du corpus d’entraînement des simplifications grâce aux résultats de l’extraction des relations. Les premiers résultats que nous avons obtenus pour la tâche d’extraction de relations d’i2b2 2010 sont très encourageants.</resume>
			<mots_cles>Extraction de relations, simplification de phrases, apprentissage automatique</mots_cles>
			<title>Automatic Information Extraction in the Medical Domain by Cross-Lingual Projection</title>
			<abstract>Machine learning based relation extraction requires large annotated corpora to take into account the variability in the expression of relations. To deal with this problem, we propose a method for simplifying sentences, i.e. for reducing the syntactic variability of the relations. Simplification requires the annotation of a small corpus, which will be automatically augmented. The process starts with the annotation of the simplification thanks to a CRF classifier, then the relation extraction, and lastly the automatic completion of the training corpus for the simplification through the results of the relation extraction. The first results we obtained for the task of relation extraction of the i2b2 2010 challenge are encouraging.</abstract>
			<keywords>Relation extraction, sentence simplification, machine learning</keywords>
		</article>
		<article id="taln-2012-long-002" session="Extraction d’informations/de relations">
			<auteurs>
				<auteur>
					<nom>Asma Ben Abacha</nom>
					<email>abacha@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Pierre Zweigenbaum</nom>
					<email>pz@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Aurélien Max</nom>
					<email>aurelien.max@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIMSI-CNRS, BP 133 91403 Orsay cedex</affiliation>
			</affiliations>
			<titre>Extraction d'information automatique en domaine médical par projection inter-langue : vers un passage à l'échelle</titre>
			<type>long</type>
			<pages>15-28</pages>
			<resume>Cette recherche est issue de notre volonté de tester de nouvelles méthodes automatiques d’annotation ou d’extraction d’information à partir d’une langue L1 en exploitant des ressources et des outils disponibles pour une autre langue L2. Cette approche repose sur le passage par un corpus parallèle (L1-L2) aligné au niveau des phrases et des mots. Pour faire face au manque de corpus médicaux français annotés, nous nous intéressons au couple de langues (françaisanglais) dans le but d’annoter automatiquement des textes médicaux en français. En particulier, nous nous intéressons dans cet article à la reconnaissance des entités médicales. Nous évaluons dans un premier temps notre méthode de reconnaissance d’entités médicales sur le corpus anglais. Dans un second temps, nous évaluons la reconnaissance des entités médicales du corpus français par projection des annotations du corpus anglais. Nous abordons également le problème de l’hétérogénéité des données en exploitant un corpus extrait du Web et nous proposons une méthode statistique pour y pallier.</resume>
			<mots_cles>Extraction d’information, projection d’annotation, reconnaissance des entités médicales, apprentissage</mots_cles>
			<title>Automatic Information Extraction in the Medical Domain by Cross-Lingual Projection</title>
			<abstract>This research stems from our willingness to test new methods for automatic annotation or information extraction from one language L1 by exploiting resources and tools available to another language L2. This approach involves the use of a parallel corpus (L1-L2) aligned at the level of sentences and words. To address the lack of annotated medical French corpus, we focus on the French-English language pair to annotate automatically medical French texts. In particular, we focus in this article on medical entity recognition. We evaluate our medical entity recognition method on the English corpus and the projection of the annotations on the French corpus. We also discuss the problem of scalability since we use a parallel corpus extracted from the Web and propose a statistical method to handle heterogeneous corpora.</abstract>
			<keywords>Automatic Information Extraction, Annotation Projection, Medical Entity Recognition, Machine Learning</keywords>
		</article>
		<article id="taln-2012-long-003" session="Extraction d’informations/de relations">
			<auteurs>
				<auteur>
					<nom>Ludovic Jean-Louis</nom>
					<email>ludovic.jean-louis@cea.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Romaric Besançon</nom>
					<email>romaric.besancon@cea.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Olivier Ferret</nom>
					<email>olivier.ferret@cea.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">CEA, LIST, Laboratoire Vision et Ingénierie des Contenus, F-91191 Gif-sur-Yvette, France</affiliation>
			</affiliations>
			<titre>Une méthode d'extraction d'information fondée sur les graphes pour le remplissage de formulaires</titre>
			<type>long</type>
			<pages>29-42</pages>
			<resume>Dans les systèmes d’extraction d’information sur des événements, une tâche importante est le remplissage automatique de formulaires regroupant les informations sur un événement donné à partir d’un texte non structuré. Ce remplissage de formulaire peut s’avérer difficile lorsque l’information est dispersée dans tout le texte et mélangée à des éléments d’information liés à un autre événement similaire. Nous proposons dans cet article une approche en deux étapes pour ce problème : d’abord une segmentation du texte en événements pour sélectionner les phrases relatives au même événement ; puis une méthode de sélection dans les phrases sélectionnées des entités liées à l’événement. Une évaluation de cette approche sur un corpus annoté de dépêches dans le domaine des événements sismiques montre un F-score de 72% pour la tâche de remplissage de formulaires.</resume>
			<mots_cles>Extraction d’information, segmentation de texte, remplissage de formulaires</mots_cles>
			<title>A Graph-Based Method for Template Filling in Information Extraction</title>
			<abstract>In event-based Information Extraction systems, a major task is the automated filling from unstructured texts of a template gathering information related to a particular event. Such template filling may be a hard task when the information is scattered throughout the text and mixed with similar pieces of information relative to a different event. We propose in this paper a two-step approach for template filling : first, an event-based segmentation is performed to select the parts of the text related to the target event ; then, a graph-based method is applied to choose the most relevant entities in these parts for characterizing the event. Using an evaluation of this model based on an annotated corpus for earthquake events, we achieve a 72% F-measure for the template-filling task.</abstract>
			<keywords>Information Extraction, Text Segmentation, Template Filling</keywords>
		</article>
		<article id="taln-2012-long-004" session="Analyse">
			<auteurs>
				<auteur>
					<nom>Anaïs Lefeuvre</nom>
					<email>anais.lefeuvre@labri.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
					<affiliationId>3</affiliationId>
				</auteur>
				<auteur>
					<nom>Richard Moot</nom>
					<email>moot@labri.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Christian Retoré</nom>
					<email>retore@labri.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Noémie-Fleur Sandillon-Rezer</nom>
					<email>nfsr@labri.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université de Bordeaux</affiliation>
				<affiliation affiliationId="2">LaBRI-CNRS</affiliation>
				<affiliation affiliationId="3">INRIA</affiliation>
			</affiliations>
			<titre>Traitement automatique sur corpus de récits de voyages pyrénéens : Une analyse syntaxique, sémantique et temporelle</titre>
			<type>long</type>
			<pages>43-56</pages>
			<resume>Cet article présente notre utilisation de la théorie des types dans laquelle nous nous situons pour l’analyse syntaxique, sémantique et pour la construction du lexique. Notre outil, Grail permet de traiter le discours automatiquement à partir du texte brut et nous le testons sur un corpus de récit de voyages pyrénéens, Ititpy. Nous expliquons donc notre usage des grammaires catégorielles et plus particulièrement du calcul de Lambek et la correspondance entre ces catégories et le lambda-calcul simplement typé dans le cadre de la DRT. Une flexibilité du typage doit être autorisée dans certains cas et bloquée dans d’autres. Quelques phénomènes linguistiques participant à une forme de glissement de sens provocant des conflits de types sont présentés. Nous expliquons ensuite nos motivations d’ordre pragmatique à utiliser un système à sortes et types variables en sémantique lexicale puis notre traitement compositionnel du temps des évènements inspiré du Binary Tense de (Verkuyl, 2008).</resume>
			<mots_cles>compositionalité, interface syntaxe-sémantique, interface sémantique-pragmatique, grammaire catégorielle, théorie des types, récit de voyage</mots_cles>
			<title>Processing of a Pyrenees travel novels corpus : a syntactical, semantical and temporal analysis</title>
			<abstract>In this article, we present a type theoretical framework which we apply to the syntactic analysis and the computation of DRS semantics. Our tool, Grail, is used for the automatic treatment of French text and we use a Pyrenées travel novels corpus, Itipy, as a test case. We explain our use of categorial grammars and specifically the Lambek calculus and its connection to the simply typed lambda-calculus in connection with DRT. Flexible typing has to be allowed in some cases and forbidden in others. Some linguistic phenomena presenting some kind of meaning shifts inducing typing conflicts will be introduced. We then present our motivations in the pragmatic field to use a system with sorts and variable types in lexical semantics and then we present how we process events temporality, in the light of Verkuyl’s Binary Tense (Verkuyl, 2008).</abstract>
			<keywords>compositionality, syntax-semantics interface, semantics-pragmatics interface, categorial grammar, type theory, travel novel</keywords>
		</article>
		<article id="taln-2012-long-005" session="Analyse">
			<auteurs>
				<auteur>
					<nom>Matthieu Constant</nom>
					<email>mconstan@univ-mlv.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Anthony Sigogne</nom>
					<email>sigogne@univ-mlv.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Patrick Watrin</nom>
					<email>patrick.watrin@uclouvain.be</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">université Paris-Est, LIGM, CNRS, 5, bd Descartes 774545 Marne-la-Vallée</affiliation>
				<affiliation affiliationId="2">Université de Louvain, CENTAL, Louvain-la-Neuve</affiliation>
			</affiliations>
			<titre>La reconnaissance des mots composés à l'épreuve de l'analyse syntaxique et vice-versa : évaluation de deux stratégies discriminantes</titre>
			<type>long</type>
			<pages>57-70</pages>
			<resume>Nous proposons deux stratégies discriminantes d’intégration des mots composés dans un processus réel d’analyse syntaxique : (i) pré-segmentation lexicale avant analyse, (ii) post-segmentation lexicale après analyse au moyen d’un réordonnanceur. Le segmenteur de l’approche (i) se fonde sur un modèle CRF et permet d’obtenir un reconnaisseur de mots composés état-de-l’art. Le réordonnanceur de l’approche (ii) repose sur un modèle MaxEnt intégrant des traits dédiés aux mots composés. Nous montrons que les deux approches permettent de combler jusqu’à 18% de l’écart entre un analyseur baseline et un analyseur avec segmentation parfaite et jusqu’à 25% pour la reconnaissance des mots composés.</resume>
			<mots_cles>Mots composés, analyse syntaxique, champs markoviens aléatoires, réordonnanceur</mots_cles>
			<title>Recognition of compound words tested against parsing and vice-versa : evaluation of two discriminative approaches</title>
			<abstract>We propose two discriminative strategies to integrate compound word recognition in a real parsing context : (i) state-of-the-art compound pregrouping with Conditional Random Fields before parsing, (ii) reranking parses with features dedicated to compounds after parsing. We show that these two approaches help reduce up to 18% of the gap between a baseline parser and parser with golden segmentation and up to 25% for compound recognition.</abstract>
			<keywords>Multiword expressions, parsing, Conditional random Fields, reranker</keywords>
		</article>
		<article id="taln-2012-long-006" session="Analyse">
			<auteurs>
				<auteur>
					<nom>Ramadan Alfared</nom>
					<email>Ramadan.Alfared@etu.univ-nantes.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Denis Béchet</nom>
					<email>Denis.Bechet@univ-nantes.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Alexander Dikovsky</nom>
					<email>Alexandre.Dikovsky@univ-nantes.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LINA, Université de Nantes, 2, rue de la Houssinière, 44000 Nantes</affiliation>
			</affiliations>
			<titre>Calcul des cadres de sous catégorisation des noms déverbaux français (le cas du génitif)</titre>
			<type>long</type>
			<pages>71-84</pages>
			<resume>L’analyse syntaxique fine en dépendances nécessite la connaissance des cadres de souscatégorisation des unités lexicales. Le cas des verbes étant bien étudié, nous nous intéressons dans cet article au cas des noms communs dérivés de verbes. Notre intérêt principal est de calculer le cadre de sous-catégorisation des noms déverbaux à partir de celui du verbe d’origine pour le français. Or, pour ce faire il faut disposer d’une liste représentative de noms déverbaux français. Pour calculer cette liste nous utilisons un algorithme simplifié de repérage des noms déverbaux, l’appliquons à un corpus et comparons la liste obtenue avec la liste Verbaction des déverbaux exprimant l’action ou l’activité du verbe. Pour les noms déverbaux ainsi obtenus et attestés ensuite par une expertise linguistique, nous analysons la provenance des groupes prépositionnels subordonnés des déverbaux dans des contextes différents en tenant compte du verbe d’origine. L’analyse est effectuée sur le corpus Paris 7 et est limitée au cas le plus fréquent du génitif, c’est-à-dire des groupes prépositionnels introduits par de, des, etc.</resume>
			<mots_cles>nom déverbal, cadre de sous-catégorisation, groupe prépositionnel, analyse en dépendances</mots_cles>
			<title>On Computing Subcategorization Frames of French Deverbal Nouns (Case of Genitive)</title>
			<abstract>Fine dependency analysis needs exact information on the subcategoriziation frames of lexical units. These frames being well studied for the verbs, we are interested in this paper by the case of the noun deverbals. Our main goal is to calculate the subcategoriziation frame of deverbals in French from that of the source verb. However, this task needs a representative list of French deverbal nouns. To obtain such a list, we use a simplified algorithm detecting deverbal nouns in texts. The obtained list attested by linguists is compared with the existing list Verbaction of deverbals expressing the action/activity of French verbs. For these deverbal nouns, we analyse the origin of their subordinate prepositional phrases in different contexts relative to their source verbs. This analysis is carried out over the corpus Paris 7 and is limited to the most frequent cases of the genitive, i.e. to the prepositional phrases headed by the prepositions de, des, etc.</abstract>
			<keywords>Deverbal Noun, Subcategorization Frame, Prepositional Phrase, Dependency Tree</keywords>
		</article>
		<article id="taln-2012-long-007" session="Session Commune JEP-TALN">
			<auteurs>
				<auteur>
					<nom>Vincent Claveau</nom>
					<email>vincent.claveau@irisa.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">IRISA-CNRS, Campus de Beaulieu, 35042 Rennes, France</affiliation>
			</affiliations>
			<titre>Vectorisation, Okapi et calcul de similarité pour le TAL : pour oublier enfin le TF-IDF</titre>
			<type>long</type>
			<pages>85-98</pages>
			<resume>Dans cette prise de position, nous nous intéressons au calcul de similarité (ou distances) entre textes, problématique présente dans de nombreuses tâches de TAL. Nous nous efforçons de montrer que ce qui n’est souvent qu’un composant dans des systèmes plus complexes est parfois négligé et des solutions sous-optimales sont employées. Ainsi, le calcul de similarité par TF-IDF/cosinus est souvent présenté comme « état-de-l’art », alors que des alternatives souvent plus performantes sont employées couramment dans le domaine de la Recherche d’Information (RI). Au travers de quelques expériences concernant plusieurs tâches, nous montrons combien ce simple calcul de similarité peut influencer les performances d’un système. Nous considérons plus particulièrement deux alternatives. La première est le schéma de pondération Okapi-BM25, bien connu en RI et directement interchangeable avec le TF-IDF. L’autre, la vectorisation, est une technique de calcul de similarité que nous avons développée et qui offrent d’intéressantes propriétés.</resume>
			<mots_cles>Calcul de similarité, modèle vectoriel, TF-IDF, Okapi BM-25, vectorisation</mots_cles>
			<title>Vectorization, Okapi and computing similarity for NLP : say goodbye to TF-IDF</title>
			<abstract>In this position paper, we review a problem very common for many NLP tasks: computing similarity (or distances) between texts. We aim at showing that what is often considered as a small component in a broader complex system is very often overlooked, leading to the use of sub-optimal solutions. Indeed, computing similarity with TF-IDF weighting and cosine is often presented as “state-of-theart”, while more effective alternatives are in the Information Retrieval (IR) community. Through some experiments on several tasks, we show how this simple calculation of similarity can influence system performance. We consider two particular alternatives. The first is the weighting scheme Okapi-BM25, well known in IR and directly interchangeable with TF-IDF. The other, called vectorization, is a technique for calculating text similarities that we have developed which offers some interesting properties.</abstract>
			<keywords>Calculating similarities, vector space model, TF-IDF, Okapi BM-25, vectorization</keywords>
		</article>
		<article id="taln-2012-long-008" session="Session Commune JEP-TALN">
			<auteurs>
				<auteur>
					<nom>Christophe Benzitoun</nom>
					<email>christophe.benzitoun@atilf.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Karën Fort</nom>
					<email>karen.fort@inist.fr</email>
					<affiliationId>2</affiliationId>
					<affiliationId>3</affiliationId>
				</auteur>
				<auteur>
					<nom>Benoît Sagot</nom>
					<email>benoit.sagot@inria.fr</email>
					<affiliationId>4</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">ATILF, Nancy Université &amp; CNRS, 44, avenue de la Libération, BP 30687, 54063 Nancy cedex</affiliation>
				<affiliation affiliationId="2">INIST-CNRS, 2 allée de Brabois, 54500 Vandoeuvre-lès-Nancy</affiliation>
				<affiliation affiliationId="3">LIPN, Université Paris 13 &amp; CNRS, 99 av. J.B. Clément, 93430 Villetaneuse</affiliation>
				<affiliation affiliationId="4">Alpage, INRIA Paris-Rocquencourt &amp; Université Paris 7, Rocquencourt, France</affiliation>
			</affiliations>
			<titre>TCOF-POS : un corpus libre de français parlé annoté en morphosyntaxe</titre>
			<type>long</type>
			<pages>99-112</pages>
			<resume>Nous présentons dans cet article un travail portant sur la création d’un corpus de français parlé spontané annoté en morphosyntaxe. Nous détaillons la méthodologie suivie afin d’assurer le contrôle de la qualité de la ressource finale. Ce corpus est d’ores et déjà librement diffusé pour la recherche et peut servir aussi bien de corpus d’apprentissage pour des logiciels que de base pour des descriptions linguistiques. Nous présentons également les résultats obtenus par deux étiqueteurs morphosyntaxiques entrainés sur ce corpus.</resume>
			<mots_cles>Etiquetage morpho-syntaxique, français parlé, ressources langagières</mots_cles>
			<title>TCOF-POS : A Freely Available POS-Tagged Corpus of Spoken French</title>
			<abstract>This article details the creation of TCOF-POS, the first freely available corpus of spontaneous spoken French. We present here the methodology that was followed in order to obtain the best possible quality in the final resource. This corpus already is freely available and can be used as a training/validation corpus for NLP tools, as well as a study corpus for linguistic research. We also present the results obtained by two POS-taggers trained on the corpus.</abstract>
			<keywords>Etiquetage morpho-syntaxique, français parlé, ressources langagières</keywords>
		</article>
		<article id="taln-2012-long-009" session="Alignement">
			<auteurs>
				<auteur>
					<nom>Adrien Lardilleux</nom>
					<email>adrien.lardilleux@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>François Yvon</nom>
					<email>francois.yvon@limsi.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Yves Lepage</nom>
					<email></email>
					<affiliationId>3</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIMSI-CNRS</affiliation>
				<affiliation affiliationId="2">Université Paris-Sud</affiliation>
				<affiliation affiliationId="3">Université Waseda, Japon</affiliation>
			</affiliations>
			<titre>Alignement sous-phrastique hiérarchique avec Anymalign</titre>
			<type>long</type>
			<pages>113-126</pages>
			<resume>Nous présentons un algorithme d’alignement sous-phrastique permettant d’aligner très facilement un couple de phrases à partir d’une matrice d’alignement pré-remplie. Cet algorithme s’inspire de travaux antérieurs sur l’alignement par segmentation binaire récursive ainsi que de travaux sur le clustering de documents. Nous évaluons les alignements produits sur des tâches de traduction automatique et montrons qu’il est possible d’atteindre des résultats du niveau de l’état de l’art, affichant des gains très conséquents allant jusqu’à plus de 4 points BLEU par rapport à nos travaux antérieurs, à l’aide une méthode très simple, indépendante de la taille du corpus à traiter, et produisant directement des alignements symétriques. En utilisant cette méthode en tant qu’extension à l’outil d’extraction de traductions Anymalign, nos expériences nous permettent de cerner certaines limitations de ce dernier et de définir des pistes pour son amélioration.</resume>
			<mots_cles>corpus parallèle, alignement sous-phrastique, traduction automatique statistique</mots_cles>
			<title>Hierarchical sub-sentential alignment with Anymalign</title>
			<abstract>We present a sub-sentential alignment algorithm that aligns sentence pairs from an existing alignment matrix in a very easy way. This algorithm is inspired by previous work on alignment by recursive binary segmentation and on document clustering. We evaluate the alignments produced on machine translation tasks and show that we can obtain state-of-the-art results, with gains up to more than 4 BLEU points compared to our previous work, with a method that is very simple, independent of the size of the corpus to be aligned, and can directly produce symmetric alignments. When using this method as an extension of the translation extraction tool Anymalign, our experiments allow us to determine some of its limitations and to define possible leads for further improvements.</abstract>
			<keywords>parallel corpus, sub-sentential alignment, statistical machine translation</keywords>
		</article>
		<article id="taln-2012-long-010" session="Alignement">
			<auteurs>
				<auteur>
					<nom>Houda Saadane</nom>
					<email>houda.saadane@e.u-grenoble3.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Nasredine Semmar</nom>
					<email>nasredine.semmar@cea.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIDILEM, Université de Grenoble, 38400 Grenoble Cedex 9</affiliation>
				<affiliation affiliationId="2">CEA, LIST, Laboratoire Vision et Ingénierie des Contenus, 91191 Gif-sur-Yvette Cedex</affiliation>
			</affiliations>
			<titre>Utilisation de la translittération arabe pour l’amélioration de l’alignement de mots à partir de corpus parallèles français-arabe</titre>
			<type>long</type>
			<pages>127-140</pages>
			<resume>Dans cet article, nous nous intéressons à l’utilisation de la translittération arabe pour l’amélioration des résultats d’une approche linguistique d’alignement de mots simples et composés à partir de corpus de textes parallèles français-arabe. Cette approche utilise, d’une part, un lexique bilingue et les caractéristiques linguistiques des entités nommées et des cognats pour l’alignement de mots simples, et d’autre part, les relations de dépendance syntaxique pour aligner les mots composés. Nous avons évalué l’aligneur de mots simples et composés intégrant la translittération arabe en utilisant deux procédés : une évaluation de la qualité d’alignement à l’aide d’un alignement de référence construit manuellement et une évaluation de l’impact de cet alignement sur la qualité de la traduction en faisant appel au système de traduction automatique statistique Moses. Les résultats obtenus montrent que la translittération améliore aussi bien la qualité de l’alignement que celle de la traduction.</resume>
			<mots_cles>Translittération, alignement de mots, construction de dictionnaires multilingues, traduction automatique, recherche d’information interlingue</mots_cles>
			<title>Using Arabic transliteration to improve word alignment from French-Arabic parallel corpora</title>
			<abstract>In this paper, we focus on the use of Arabic transliteration to improve the results of a linguistic word alignment approach from parallel text corpora. This approach uses, on the one hand, a bilingual lexicon, named entity and cognates linguistic properties to align single words, and on the other hand, syntactic dependency relations to align compound words. We have evaluated the word aligner integrating Arabic transliteration using two methods: A manual evaluation of the alignment quality and an evaluation of the impact of this alignment on the translation quality by using the statistical machine translation system Moses. The obtained results show that Arabic transliteration improves the quality of both alignment and translation.</abstract>
			<keywords>Transliteration, word alignment, multilingual lexicons construction, machine translation, cross-language information retrieval</keywords>
		</article>
		<article id="taln-2012-long-011" session="Alignement">
			<auteurs>
				<auteur>
					<nom>Emmanuel Morin</nom>
					<email>emmanuel.morin@univ-nantes.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Béatrice Daille</nom>
					<email>beatrie.daille@univ-nantes.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université de Nantes, LINA UMR CNRS 6241, 2, rue de la Houssinière, BP 92208, F-44322 Nantes cedex 03</affiliation>
			</affiliations>
			<titre>Compositionnalité et contextes issus de corpus comparables pour la traduction terminologique</titre>
			<type>long</type>
			<pages>141-154</pages>
			<resume>Dans cet article, nous cherchons à mettre en correspondance de traduction des termes extraits de chaque partie monolingue d’un corpus comparable. Notre objectif concerne l’identification et la traduction de termes spécialisés. Pour ce faire, nous mettons en oeuvre une approche compositionnelle dopée avec des informations contextuelles issues du corpus comparable. Notre évaluation montre que cette approche améliore significativement l’approche compositionnelle de base pour la traduction de termes complexes extraits de corpus comparables.</resume>
			<mots_cles>Corpus comparable, compositionnalité, information contextuelle, lexique bilingue</mots_cles>
			<title>Compositionality and Context for Bilingual Lexicon Extraction from Comparable Corpora</title>
			<abstract>In this article, we study the possibilities of improving the alignment of equivalent terms monolingually acquired from bilingual comparable corpora. Our overall objective is to identify and to translate highly specialised terminology. We applied a compositional approach enhanced with pre-processed context information. Our evaluation demonstrates that our alignment method outperforms the compositional approach for translationally equivalent term discovery from comparable corpora.</abstract>
			<keywords>Comparable Corpora, compositionality, context information, bilingual lexicon</keywords>
		</article>
		<article id="taln-2012-long-012" session="Lexique">
			<auteurs>
				<auteur>
					<nom>Paul Bédaride</nom>
					<email>paul.bedaride@gmail.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université de Stuttgart</affiliation>
			</affiliations>
			<titre>Raffinement du Lexique des Verbes Français</titre>
			<type>long</type>
			<pages>155-168</pages>
			<resume>Nous présentons dans cet article les améliorations apportées à la ressource « Les Verbes Français » afin de la rendre plus formelle et utilisable pour le traitement automatique des langues naturelles. Les informations syntaxiques et sémantiques ont été corrigées, restructurées, unifiées puis intégrées à la version XML de cette ressource, afin de pouvoir être utilisée par un système d’étiquetage de rôles sémantiques.</resume>
			<mots_cles>ressource, lexique, verbes, raffinement, étiquetage de rôles sémantiques</mots_cles>
			<title>Resource Refining : « Les Verbes Français »</title>
			<abstract>This paper introduce the impovements we made to the resource « Les Verbes Français » in order to make it more usable in the field of natural language processing. Syntactic and semantic information is corrected, restructured, unified and then integrated to the XML version of this resource, in order to be used by a semantic role labelling system.</abstract>
			<keywords>resource, lexicon, verbs, refinement, semantic roles labeling</keywords>
		</article>
		<article id="taln-2012-long-013" session="Lexique">
			<auteurs>
				<auteur>
					<nom>François Morlane-Hondère</nom>
					<email>francois.morlane@univ-tlse2.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Cécile Fabre</nom>
					<email>cecile.fabre@univ-tlse2.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">CLLE-ERSS, Université de Toulouse - Le Mirail, 5, allées Antonio Machado - Toulouse Cedex 9</affiliation>
			</affiliations>
			<titre>Étude des manifestations de la relation de méronymie dans une ressource distributionnelle</titre>
			<type>long</type>
			<pages>169-182</pages>
			<resume>Cette étude vise à étudier les manifestations de la relation de méronymie dans une ressource lexicale générée automatiquement à partir d’un corpus de langue générale. La démarche que nous adoptons consiste à recueillir un jeu de couples de méronymes issus d’une ressource externe que nous croisons avec une base distributionnelle calculée à partir d’un corpus de textes encyclopédiques. Une annotation sémantique des mots qui entrent dans ces couples de méronymes montre que la prise en compte de la nature sémantique des mots composant les couples de méronymes permet de mettre au jour des inégalités au niveau du repérage de la relation par la méthode d’analyse distributionnelle.</resume>
			<mots_cles>analyse distributionnelle, sémantique lexicale, méronymie, évaluation</mots_cles>
			<title>Study of meronymy in a distribution-based lexical resource</title>
			<abstract>In this paper, we study the way meronymy behaves in a distribution-based lexical resource. We address the question of the evaluation of such resources through a semantic-based approach. Our method consists in collecting meronyms from a resource which we cross with a distributionbased lexical resource made from an encyclopedic corpus. Meronyms are then sub-categorized manually : firstly following the sub-relation they bear (STUFF/OBJECT, MEMBER/COLLECTION, etc.), then following the semantic class of their members. Results show that distributional analysis identifies meronymic relations in different proportions according to the semantic classes of the words involved in the meronymic pairs.</abstract>
			<keywords>distributional analysis, lexical semantics, meronymy, evaluation</keywords>
		</article>
		<article id="taln-2012-long-014" session="Lexique">
			<auteurs>
				<auteur>
					<nom>Clément De Groc</nom>
					<email>cdegroc@limsi.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Xavier Tannier</nom>
					<email>xtannier@limsi.fr</email>
					<affiliationId>2</affiliationId>
					<affiliationId>3</affiliationId>
				</auteur>
				<auteur>
					<nom>Claude De Loupy</nom>
					<email>loupy@syllabs.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Syllabs, 15 rue Jean-Baptiste Berlier, 75013 Paris</affiliation>
				<affiliation affiliationId="2">Univ. Paris-Sud, 91403 Orsay Cedex</affiliation>
				<affiliation affiliationId="3">LIMSI-CNRS, B.P. 133, 91403 Orsay Cedex</affiliation>
			</affiliations>
			<titre>Un critère de cohésion thématique fondé sur un graphe de cooccurrences</titre>
			<type>long</type>
			<pages>183-195</pages>
			<resume>Dans cet article, nous définissons un nouveau critère de cohésion thématique permettant de pondérer les termes d’un lexique thématique en fonction de leur pertinence. Le critère s’inspire des approches Web as corpus pour accumuler des connaissances exogènes sur un lexique. Ces connaissances sont ensuite modélisées sous forme de graphe et un algorithme de marche aléatoire est appliqué pour attribuer un score à chaque terme. Après avoir étudié les performances et la stabilité du critère proposé, nous l’évaluons sur une tâche d’aide à la création de lexiques bilingues.</resume>
			<mots_cles>Cohésion thématique, graphe de cooccurrences, marche aléatoire</mots_cles>
			<title>Topical Cohesion using Graph Random Walks</title>
			<abstract>In this article, we propose a novel metric to weight specialized lexicons terms according to their relevance to the underlying thematic. Our method is inspired by Web as corpus approaches and accumulates exogenous knowledge about a specialized lexicon from the web. Terms cooccurrences are modelled as a graph and a random walk algorithm is applied to compute terms relevance. Finally, we study the performance and stability of the metric and evaluate it in a bilingual lexicon creation context.</abstract>
			<keywords>Thematic relevance, cooccurrence graph, random walk</keywords>
		</article>
		<article id="taln-2012-long-015" session="Réécriture">
			<auteurs>
				<auteur>
					<nom>Houda Bouamor</nom>
					<email>houda.bouamor@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Aurélien Max</nom>
					<email>aurelien.max@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Gabriel Illouz</nom>
					<email>gabriel.illouz@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Anne Vilnat</nom>
					<email>anne.vilnat@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIMSI-CNRS, Univ. Paris Sud 11, Orsay, France</affiliation>
			</affiliations>
			<titre>Validation sur le Web de reformulations locales: application à la Wikipédia</titre>
			<type>long</type>
			<pages>197-210</pages>
			<resume>Ce travail présente des expériences initiales en validation de paraphrases en contexte. Les révisions de Wikipédia nous servent de domaine d’évaluation : pour un énoncé ayant connu une courte révision dans l’encyclopédie, nous disposons d’un ensemble de réécritures possibles, parmi lesquelles nous cherchons à identifier celles qui correspondent à des paraphrases valides. Nous abordons ce problème comme une tâche de classification fondée sur des informations issues du Web, et parvenons à améliorer la performance de plusieurs techniques simples de référence.</resume>
			<mots_cles>paraphrase, Wikipédia, aide à la rédaction</mots_cles>
			<title>Assisted rephrasing for Wikipedia contributors through Web-based validation</title>
			<abstract>This works describes initial experiments on the validation of paraphrases in context. Wikipedia’s revisions are used : we assume that a set of possible rewritings are available for a given phrase that has been rewritten in the encyclopedia’s revision history, and we attempt to find the subset of those rewritings that can be considered as valid paraphrases. We tackle this problem as a classication task which we provide with features obtained from Web data. Our experiments show that our system improves performance over a set of simple baselines.</abstract>
			<keywords>paraphrasing, Wikipedia, authoring aids</keywords>
		</article>
		<article id="taln-2012-long-016" session="Réécriture">
			<auteurs>
				<auteur>
					<nom>Laetitia Brouwers</nom>
					<email></email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Delphine Bernhard</nom>
					<email></email>
					<affiliationId>1</affiliationId>
					<affiliationId>3</affiliationId>
				</auteur>
				<auteur>
					<nom>Anne-Laure Ligozat</nom>
					<email></email>
					<affiliationId>1</affiliationId>
					<affiliationId>4</affiliationId>
				</auteur>
				<auteur>
					<nom>Thomas François</nom>
					<email></email>
					<affiliationId>2</affiliationId>
					<affiliationId>5</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIMSI-CNRS, 91403 Orsay, France</affiliation>
				<affiliation affiliationId="2">Université catholique de Louvain, Belgique</affiliation>
				<affiliation affiliationId="3">LiLPa, Université de Strasbourg, France</affiliation>
				<affiliation affiliationId="4">ENSIIE, Evry, France</affiliation>
				<affiliation affiliationId="5">University of Pennsylvania, USA</affiliation>
			</affiliations>
			<titre>Simplification syntaxique de phrases pour le français</titre>
			<type>long</type>
			<pages>211-224</pages>
			<resume>Cet article présente une méthode de simplification syntaxique de textes français. La simplification syntaxique a pour but de rendre des textes plus abordables en simplifiant les éléments qui posent problème à la lecture. La méthode mise en place à cette fin s’appuie tout d’abord sur une étude de corpus visant à étudier les phénomènes linguistiques impliqués dans la simplification de textes en français. Nous avons ainsi constitué un corpus parallèle à partir d’articles de Wikipédia et Vikidia, ce qui a permis d’établir une typologie de simplifications. Dans un second temps, nous avons implémenté un système qui opère des simplifications syntaxiques à partir de ces observations. Des règles de simplification ont été décrites afin de générer des phrases simplifiées. Un module sélectionne ensuite le meilleur ensemble de phrases. Enfin, nous avons mené une évaluation de notre système montrant qu’environ 80% des phrases générées sont correctes.</resume>
			<mots_cles>simplification automatique, lisibilité, analyse syntaxique</mots_cles>
			<title>Syntactic Simplification for French Sentences</title>
			<abstract>This paper presents a method for the syntactic simplification of French texts. Syntactic simplification aims at making texts easier to understand by simplifying the elements that hinder reading. It is based on a corpus study that aimed at investigating the linguistic phenomena involved in the manual simplification of French texts. We have first gathered a parallel corpus of articles from Wikipedia and Vikidia, that we used to establish a typology of simplifications. In a second step, we implemented a system that carries out syntactic simplifications based on these corpus observations. We described simplification rules in order to generate simplified sentences. A module subsequently selects the best subset of sentences. The evaluation of our system shows that about 80% of the sentences produced by our system are accurate.</abstract>
			<keywords>automatic simplification, readability, syntactic analysis</keywords>
		</article>
		<article id="taln-2012-long-017" session="Réécriture">
			<auteurs>
				<auteur>
					<nom>Iskandar Keskes</nom>
					<email>Keskes@irit.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Mohamed Mahdi Boudabous</nom>
					<email>mehdiboudabous@gmail.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Mohamed Hédi Maâloul</nom>
					<email>mohamed.maaloul@lpl-aix.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>3</affiliationId>
				</auteur>
				<auteur>
					<nom>Lamia Hadrich Belguith</nom>
					<email>l.belguith@fsegs.rnu.tn</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">ANLP Research Group, Laboratoire MIRACL, Route de Tunis Km 10, BP 242, Sfax, Tunisie</affiliation>
				<affiliation affiliationId="2">Laboratoire IRIT, 118 Route de Narbonne, F-31062 Toulouse Cedex 9, France</affiliation>
				<affiliation affiliationId="3">Laboratoire LPL, 5 avenue Pasteur, BP 80975, 13604 Aix-en-Provence, France</affiliation>
			</affiliations>
			<titre>Étude comparative entre trois approches de résumé automatique de documents arabes</titre>
			<type>long</type>
			<pages>225-238</pages>
			<resume>Dans cet article, nous proposons une étude comparative entre trois approches pour le résumé automatique de documents arabes. Ainsi, nous avons proposé trois méthodes pour l’extraction des phrases les plus représentatives d'un document. La première méthode se base sur une approche symbolique, la deuxième repose sur une approche numérique et la troisième se base sur une approche hybride. Ces méthodes sont implémentées respectivement par le système ARSTResume, le système R.I.A et le système HybridResume. Nous présentons, par la suite, les résultats obtenus par les trois systèmes et nous procédons à une étude comparative entre les résultats obtenus afin de souligner les avantages et les limites de chaque méthode. Les résultats de l’évaluation ont montré que l‘approche numérique est plus performante que l’approche symbolique au niveau des textes longs. Mais, l’intégration de ces deux approches en une approche hybride aboutit aux résultats les plus performants dans notre corpus de textes.</resume>
			<mots_cles>Résumé automatique, approche symbolique, approche numérique, approche hybride, document arabe</mots_cles>
			<title>Comparative study of three approaches to automatic summarization of Arabic documents</title>
			<abstract>In this paper, we propose a comparative study between three approaches for automatic summarization of Arabic documents. Thus, we proposed three methods for extracting most representative sentences of a document. The first method is based on a symbolic approach, the second is relied on a numerical approach and the third is based on a hybrid approach. These methods are implemented respectively by the ARSTResume, R.I.A and HybridResume systems. Then, we present the results obtained by the three systems and we conduct a comparative study between the obtained results in order to highlight the advantages and limitations of each method. The evaluation results showed that the numerical approach has better performances than the symbolic approach. But, combining into a hybrid approach achieved the best results for our text corpus.</abstract>
			<keywords>Automatic summarization, symbolic approach, numerical approach, hybrid approach, Arabic document</keywords>
		</article>
		<article id="taln-2012-long-018" session="Exploitation de corpus">
			<auteurs>
				<auteur>
					<nom>Ann Bertels</nom>
					<email>ann.bertels@ilt.kuleuven.be</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Dirk De Hertog</nom>
					<email>dirk.dehertog@arts.kuleuven.be</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Kris Heylen</nom>
					<email>kris.heylen@arts.kuleuven.be</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">ILT, KU Leuven, Dekenstraat 6, B-3000 Leuven (Belgique)</affiliation>
				<affiliation affiliationId="2">QLVL, KU Leuven, Faculty of Arts, Blijde-Inkomststraat 21, B-3000 Leuven (Belgique)</affiliation>
			</affiliations>
			<titre>Etude sémantique des mots-clés et des marqueurs lexicaux stables dans un corpus technique</titre>
			<type>long</type>
			<pages>239-252</pages>
			<resume>Cet article présente les résultats d’une analyse sémantique quantitative des unités lexicales spécifiques dans un corpus technique, relevant du domaine des machines-outils pour l’usinage des métaux. L’étude vise à vérifier si et dans quelle mesure les mots-clés du corpus technique sont monosémiques. A cet effet, nous procédons à une analyse statistique de régression simple, qui permet d’étudier la corrélation entre le rang de spécificité des mots-clés et leur rang de monosémie, mais qui soulève des problèmes statistiques et méthodologiques, notamment un biais de fréquence. Pour y remédier, nous adoptons une approche alternative pour le repérage des unités lexicales spécifiques, à savoir l’analyse des marqueurs lexicaux stables ou Stable Lexical Marker Analysis (SLMA). Nous discutons les résultats quantitatifs et statistiques de cette approche dans la perspective de la corrélation entre le rang de spécificité et le rang de monosémie.</resume>
			<mots_cles>unités lexicales spécifiques, analyse des mots-clés, analyse des marqueurs lexicaux stables, sémantique quantitative, analyse de régression</mots_cles>
			<title>Semantic analysis of keywords and stable lexical markers in a technical corpus</title>
			<abstract>This article presents the results of a quantitative semantic analysis of typical lexical units in a specialised technical corpus of metalworking machinery in French. The study aims to find out whether and to what extent the keywords of the technical corpus are monosemous. A simple regression analysis, used to examine the correlation between typicality rank and monosemy rank of the keywords, points out some statistical and methodological problems, notably a frequency bias. In order to overcome these problems, we adopt an alternative approach for the identification of typical lexical units, called Stable Lexical Marker Analysis (SLMA). We discuss the quantitative and statistical results of this approach with respect to the correlation between typicality rank and monosemy rank.</abstract>
			<keywords>typical lexical units, Keyword Analysis, Stable Lexical Marker Analysis (SLMA), quantitative semantics, regression analysis</keywords>
		</article>
		<article id="taln-2012-long-019" session="Exploitation de corpus">
			<auteurs>
				<auteur>
					<nom>Solen Quiniou</nom>
					<email>solen.quiniou@unicaen.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Peggy Cellier</nom>
					<email>peggy.cellier@irisa.fr</email>
					<affiliationId>3</affiliationId>
				</auteur>
				<auteur>
					<nom>Thierry Charnois</nom>
					<email>thierry.charnois@unicaen.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Dominique Legallois</nom>
					<email>dominique.legallois@unicaen.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">GREYC Université de Caen Basse-Normandie, Campus 2, 14000 Caen</affiliation>
				<affiliation affiliationId="2">CRISCO Université de Caen Basse-Normandie, Campus 1, 14000 Caen</affiliation>
				<affiliation affiliationId="3">IRISA-INSA de Rennes, Campus de Beaulieu, 35042 Rennes Cedex</affiliation>
			</affiliations>
			<titre>Fouille de graphes sous contraintes linguistiques pour l'exploration de grands textes</titre>
			<type>long</type>
			<pages>253-266</pages>
			<resume>Dans cet article, nous proposons une approche pour explorer des textes de taille importante en mettant en évidence des sous-parties cohérentes. Cette méthode d’exploration s’appuie sur une représentation en graphe du texte, en utilisant le modèle linguistique de Hoey pour sélectionner et apparier les phrases dans le graphe. Notre contribution porte sur l’utilisation de techniques de fouille de graphes sous contraintes pour extraire des sous-parties pertinentes du texte (c’est-à-dire des collections de sous-réseaux phrastiques homogènes). Nous avons réalisé des expérimentations sur deux textes anglais de taille conséquente pour montrer l’intérêt de l’approche que nous proposons.</resume>
			<mots_cles>Fouille de graphes, réseaux phrastiques, analyse textuelle, navigation textuelle</mots_cles>
			<title>Graph Mining Under Linguistic Constraints to Explore Large Texts</title>
			<abstract>In this paper, we propose an approach to explore large texts by highlighting coherent sub-parts. The exploration method relies on a graph representation of the text according to the Hoey linguistic model which allows the selection and the binding of sentences in the graph. Our contribution relates to using graph mining techniques under constraints to extract relevant subparts of the text (i.e., collections of homogeneous sentence sub-networks). We have conducted some experiments on two large English texts to show the interest of the proposed approach.</abstract>
			<keywords>Graph Mining, sentence networks, textual analysis, textual navigation</keywords>
		</article>
		<article id="taln-2012-long-020" session="Exploitation de corpus">
			<auteurs>
				<auteur>
					<nom>Houda Bouamor</nom>
					<email>houda.bouamor@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Aurélien Max</nom>
					<email>aurelien.max@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Anne Vilnat</nom>
					<email>anne.vilnat@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIMSI-CNRS, Univ. Paris-Sud, Orsay, France</affiliation>
			</affiliations>
			<titre>Une étude en 3D de la paraphrase: types de corpus, langues et techniques</titre>
			<type>long</type>
			<pages>267-280</pages>
			<resume>Cet article présente une étude détaillée de l’impact du type du corpus sur la tâche d’acquisition de paraphrases sous-phrastiques. Nos expériences sont menées sur deux langues et quatre types de corpus, et incluent une combinaison efficace de quatre systèmes d’acquisition de paraphrases. Nous obtenons une amélioration relative de plus de 27% en F-mesure par rapport au meilleur système, en anglais et en français, ainsi qu’une amélioration relative à notre combinaison de systèmes de 22% pour l’anglais et de 5% pour le français quand tous les types de corpus sont utilisés pour l’acquisition depuis le type de corpus le plus couramment disponible.</resume>
			<mots_cles>acquisition de paraphrases, constitution de corpus</mots_cles>
			<title>A study of paraphrase along 3 dimensions : corpus types, languages and techniques</title>
			<abstract>In this paper, we report a detailed study of the impact of corpus type on the task of sub-sentential paraphrase acquisition. Our experiments are for 2 languages and 4 corpus types, and involve an efficient machine learning-based combination of 4 paraphrase acquisition systems. We obtain relative improvements of more than 27% in F-measure over the best individual system on English and French, and obtain a relative improvement over the combination system of 22% for English and 5% for French when using all other corpus types as additional training data for our most readily available corpus type.</abstract>
			<keywords>paraphrase acquisition, corpus collection</keywords>
		</article>
		<article id="taln-2012-long-021" session="Banques d’arbres">
			<auteurs>
				<auteur>
					<nom>Florian Boudin</nom>
					<email>florian.boudin@univ-nantes.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Nicolas Hernandez</nom>
					<email>nicolas.hernandez@univ-nantes.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université de Nantes</affiliation>
			</affiliations>
			<titre>Détection et correction automatique d'erreurs d'annotation morpho-syntaxique du French TreeBank</titre>
			<type>long</type>
			<pages>281-291</pages>
			<resume>La qualité de l’annotation morpho-syntaxique d’un corpus est déterminante pour l’entraînement et l’évaluation de méthodes d’étiquetage. Cet article présente une série d’expériences que nous avons menée sur la détection et la correction automatique des erreurs du French Treebank. Deux méthodes sont utilisées. La première consiste à identifier les mots sans étiquette et leur attribuer celle d’une forme correspondante observée dans le corpus. La seconde méthode utilise les variations de n-gramme pour détecter et corriger les anomalies d’annotation. L’évaluation des corrections apportées au corpus est réalisée de manière extrinsèque en comparant les scores de performance de différentes méthodes d’étiquetage morpho-syntaxique en fonction du niveau de correction. Les résultats montrent une amélioration significative de la précision et indiquent que la qualité du corpus peut être sensiblement améliorée par l’application de méthodes de correction automatique des erreurs d’annotation.</resume>
			<mots_cles>Étiquetage morpho-syntaxique, correction automatique, qualité d’annotation</mots_cles>
			<title>Detecting and correcting POS annotation in the French TreeBank</title>
			<abstract>The quality of the Part-Of-Speech (POS) annotation in a corpus has a large impact on training and evaluating POS taggers. In this paper, we present a series of experiments that we have conducted on automatically detecting and correcting annotation errors in the French TreeBank. Two methods are used. The first simply relies on identifying tokens with missing tags and correct them by assigning the tag the same token observed in the corpus. The second method uses n-gram variations to detect and correct conflicting annotations. The evaluation of the automatic correction is performed extrinsically by comparing the performance of different POS taggers in relation to the level of correction. Results show a statistically significant improvement in precision and indicate that the POS annotation quality can be noticeably enhanced by using automatic correction methods.</abstract>
			<keywords>Part-Of-Speech tagging, automatic correction, annotation quality</keywords>
		</article>
		<article id="taln-2012-long-022" session="Banques d’arbres">
			<auteurs>
				<auteur>
					<nom>Bruno Guillaume</nom>
					<email>bruno.guillaume@loria.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Guy Perrier</nom>
					<email>guy.perrier@loria.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>3</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LORIA - Campus Scientifique - BP 239 - 54506 Vandoeuvre-lès-Nancy cedex</affiliation>
				<affiliation affiliationId="2">INRIA Grand Est - 615, rue du Jardin Botanique - 54600 Villers-lès-Nancy</affiliation>
				<affiliation affiliationId="3">Université de Lorraine - 34, cours Léopold - CS 25233 - 54502 Nancy cedex</affiliation>
			</affiliations>
			<titre>Annotation sémantique du French Treebank à l’aide de la réécriture modulaire de graphes</titre>
			<type>long</type>
			<pages>293-306</pages>
			<resume>Nous proposons d’annoter le French Treebank à l’aide de dépendances sémantiques dans le cadre de la DMRS en partant d’une annotation en dépendances syntaxiques de surface et en utilisant la réécriture modulaire de graphes. L’article présente un certain nombre d’avancées concernant le calcul de réécriture utilisé : l’utilisation de règles pour faire le lien avec des lexiques, en particulier le lexique des verbes de Dicovalence, et l’introduction de filtres pour écarter à certaines étapes les annotations incohérentes. Il présente aussi des avancées dans le système de réécriture lui-même, qui a une plus large couverture (constructions causatives, verbes à montée, . . .) et dont l’ordre des modules a été étudié de façon plus systématique. Ce système a été expérimenté sur l’ensemble du French Treebank à l’aide du prototype GREW, qui implémente le calcul de réécriture utilisé.</resume>
			<mots_cles>réécriture de graphes, interface syntaxe-sémantique, dépendances, DMRS</mots_cles>
			<title>Semantic Annotation of the French Treebank using Modular Graph Rewriting</title>
			<abstract>We propose to annotate the French Treebank with semantic dependencies in the framework of DMRS starting from an annotation with surface syntactic dependencies and using modular graph rewriting. The article presents some new results related to the rewriting calculus: the use of rules to make a link with lexicons, especially with the lexicon of verbs Dicovalence, and the introduction of filters to discard inconsistent annotations at some computation steps. It also presents new results related to the rewriting system itself: the system has a larger coverage (causative constructions, rising verbs, . . .) and the order between modules has been studied in a more systematic way. This system has been experimented on the whole French Treebank with the prototype GREW, which implements the used rewriting calculus.</abstract>
			<keywords>graph rewriting, syntax-semantics interface, dependencies, DMRS</keywords>
		</article>
		<article id="taln-2012-long-023" session="Banques d’arbres">
			<auteurs>
				<auteur>
					<nom>Philippe Blache</nom>
					<email>blache@lpl-aix.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Stéphane Rauzy</nom>
					<email>rauzy@lpl-aix.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LPL, 5 Avenue Pasteur, 13100 Aix-en-Provence</affiliation>
			</affiliations>
			<titre>Enrichissement du FTB : un treebank hybride constituants/propriétés</titre>
			<type>long</type>
			<pages>307-320</pages>
			<resume>Cet article présente les mécanismes de création d’un treebank hybride enrichissant le FTB à l’aide d’annotations dans le formalisme des Grammaires de Propriétés. Ce processus consiste à acquérir une grammaire GP à partir du treebank source et générer automatiquement les structures syntaxiques dans le formalisme cible en s’appuyant sur la spécification d’un schéma d’encodage adapté. Le résultat produit, en partant d’une version du FTB corrigée et modifiée en fonction de nos besoins, constitue une ressource ouvrant de nouvelles perspectives pour le traitement et la description du français.</resume>
			<mots_cles>Treebank hybride, French Treebank, Grammaires de Propriétés</mots_cles>
			<title>Enriching the French Treebank with Properties</title>
			<abstract>We present in this paper the hybridation of the French Treebank with Property Grammars annotations. This process consists in acquiring a PG grammar from the source treebank and generating the new syntactic encoding on top of the original one. The result is a new resource for French, opening the way to new tools and descriptions.</abstract>
			<keywords>Hybrid treebank, French Treebank, Property Grammars</keywords>
		</article>
		<article id="taln-2012-long-024" session="Banques d'arbres">
			<auteurs>
				<auteur>
					<nom>Marie Candito</nom>
					<email>marie.candito@linguist.jussieu.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Djamé Seddah</nom>
					<email>djame.seddah@paris-sorbonne.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Alpage (Univ. Paris Diderot &amp; INRIA), 175 rue du Chevaleret, 75013 Paris, France</affiliation>
				<affiliation affiliationId="2">Univ. Paris Sorbonne, 28, rue Serpente, 75006 Paris, France</affiliation>
			</affiliations>
			<titre>Le corpus Sequoia : annotation syntaxique et exploitation pour l'adaptation d'analyseur par pont lexical</titre>
			<type>long</type>
			<pages>321-334</pages>
			<resume>Nous présentons dans cet article la méthodologie de constitution et les caractéristiques du corpus Sequoia, un corpus en français, syntaxiquement annoté d’après un schéma d’annotation très proche de celui du French Treebank (Abeillé et Barrier, 2004), et librement disponible, en constituants et en dépendances. Le corpus comporte des phrases de quatre origines : Europarl français, le journal l’Est Républicain, Wikipédia Fr et des documents de l’Agence Européenne du Médicament, pour un total de 3204 phrases et 69246 tokens. En outre, nous présentons une application de ce corpus : l’évaluation d’une technique d’adaptation d’analyseurs syntaxiques probabilistes à des domaines et/ou genres autres que ceux du corpus sur lequel ces analyseurs sont entraînés. Cette technique utilise des clusters de mots obtenus d’abord par regroupement morphologique à l’aide d’un lexique, puis par regroupement non supervisé, et permet une nette amélioration de l’analyse des domaines cibles (le corpus Sequoia), tout en préservant le même niveau de performance sur le domaine source (le FTB), ce qui fournit un analyseur multi-domaines, à la différence d’autres techniques d’adaptation comme le self-training.</resume>
			<mots_cles>Corpus arboré, analyse syntaxique statistique, adaptation de domaine</mots_cles>
			<title>The Sequoia corpus : syntactic annotation and use for a parser lexical domain adaptation method</title>
			<abstract>We present the building methodology and the properties of the Sequoia treebank, a freely available French corpus annotated following the French Treebank guidelines (Abeillé et Barrier, 2004). The Sequoia treebank comprises 3204 sentences (69246 tokens), from the French Europarl, the regional newspaper L’Est Républicain, the French Wikipedia and documents from the European Medicines Agency. We then provide a method for parser domain adaptation, that makes use of unsupervised word clusters. The method improves parsing performance on target domains (the domains of the Sequoia corpus), without degrading performance on source domain (the French treenbank test set), contrary to other domain adaptation techniques such as self-training.</abstract>
			<keywords>Treebank, statistical parsing, parser domain adaptation</keywords>
		</article>
		<article id="taln-2012-court-001" session="Poster 1">
			<auteurs>
				<auteur>
					<nom>Francis Brunet-Manquat</nom>
					<email>Francis.Brunet-Manquat@imag.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Jérôme Goulian</nom>
					<email>Jerome.Goulian@imag.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIG-GETALP, Université Pierre Mendès France Grenoble 2</affiliation>
			</affiliations>
			<titre>ACOLAD Plateforme pour l’édition collaborative dépendancielle</titre>
			<type>court</type>
			<pages>335-342</pages>
			<resume>Cet article présente une plateforme open-source pour l’édition collaborative de corpus de dépendances. Cette plateforme, nommée ACOLAD (Annotation de COrpus Linguistique pour l’Analyse de Dépendances), propose des services manuels de segmentation et d’annotation multi-niveaux (segmentation en mots et en syntagmes minimaux (chunks), annotation morphosyntaxique des mots, annotation syntaxique des chunks et annotation syntaxique des dépendances entre mots ou entre chunks). Dans cet article, nous présentons la plateforme ACOLAD, puis nous détaillons la représentation pivot utilisée pour gérer les annotations concurrentes, enfin décrivons le mécanisme d’importation de ressources linguistiques externes.</resume>
			<mots_cles>annotation collaborative de corpus, annotations concurrentes, dépendances</mots_cles>
			<title>ACOLAD: platform for collaborative dependency annotation</title>
			<abstract>This paper presents an open-source platform for collaborative editing dependency corpora. ACOLAD platform (Annotation of corpus linguistics for the analysis of dependencies) offers manual annotation services such as segmentation and multi-level annotation (segmentation into words and phrases minimum (chunks), morphosyntactic annotation of words, syntactic annotation chunks and annotating syntactic dependencies between words or chunks). In this paper, we present ACOLAD platform, then we detail the representation used to manage concurrent annotations, then we describe the mechanism for importing external linguistic resources.</abstract>
			<keywords>corpus collaborative annotation, concurrent annotations, dependencies</keywords>
		</article>
		<article id="taln-2012-court-002" session="Poster 1">
			<auteurs>
				<auteur>
					<nom>Anaïs Cadilhac</nom>
					<email>adilha@irit.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Farah Benamara</nom>
					<email>benamara@irit.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Vladimir Popescu</nom>
					<email>popescu@irit.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Nicholas Asher</nom>
					<email>asher@irit.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Mohamadou Seck</nom>
					<email>seck@irit.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">IRIT, 118, Route de Narbonne, 31062 Toulouse Cedex 9</affiliation>
			</affiliations>
			<titre>Extraction de préférences à partir de dialogues de négociation</titre>
			<type>court</type>
			<pages>343-350</pages>
			<resume>Cet article présente une approche linguistique pour l’extraction d’expressions de préférence à partir de dialogues de négociation. Nous proposons un nouveau schéma d’annotation pour encoder les préférences et les dépendances exprimées linguistiquement dans deux genres de corpus différents. Ensuite, nous proposons une méthode d’apprentissage qui extrait les expressions de préférence en utilisant une combinaison de traits locaux et discursifs. Finalement, nous évaluons la fiabilité de notre approche sur chaque genre de corpus.</resume>
			<mots_cles>Préférence, dialogue, apprentissage automatique</mots_cles>
			<title>Towards Preference Extraction From Negotiation Dialogues</title>
			<abstract>This paper presents an NLP based approach for preference expression extraction from negotiation dialogues. We propose a new annotation schema for preferences and dependencies among them and illustrate on two different corpus genres. We then suggest a learning approach that efficiently extracts preference expressions using a combination of local and discursive features and assess the reliability of our approach on each corpus genre.</abstract>
			<keywords>Preference, dialogue, machine learning</keywords>
		</article>
		<article id="taln-2012-court-003" session="Poster 1">
			<auteurs>
				<auteur>
					<nom>Alexandre Denis</nom>
					<email>denis@loria.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Matthieu Quignard</nom>
					<email>matthieu.quignard@univ-lyon2.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Dominique Freard</nom>
					<email>dominique.freard@telecom-paristech.fr</email>
					<affiliationId>3</affiliationId>
				</auteur>
				<auteur>
					<nom>Francoise Detienne</nom>
					<email>francois.detienne@telecom-paristech.fr</email>
					<affiliationId>3</affiliationId>
				</auteur>
				<auteur>
					<nom>Michael Baker</nom>
					<email>michael.baker@telecom-paristech.fr</email>
					<affiliationId>3</affiliationId>
				</auteur>
				<auteur>
					<nom>Flore Barcellini</nom>
					<email>flore.barcellini@cnam.fr</email>
					<affiliationId>4</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">UMR 7503 LORIA, CNRS Campus scientifique 54 506 Vandoeuvre-lès-Nancy</affiliation>
				<affiliation affiliationId="2">UMR 5191 ICAR, CNRS 5 parvis René Descartes 69342 Lyon Cedex 07</affiliation>
				<affiliation affiliationId="3">UMR 5141 LTCI, CNRS 46 rue Barrault 75 634 Paris Cedex 13</affiliation>
				<affiliation affiliationId="4">CNAM-CRTD, 41 rue Gay-Lussac 75 005 Paris</affiliation>
			</affiliations>
			<titre>Détection de conflits dans les communautés épistémiques en ligne</titre>
			<type>court</type>
			<pages>351-358</pages>
			<resume>La présence de conflits dans les communautés épistémiques en ligne peut s’avérer bloquante pour l’activité de conception. Nous présentons une étude sur la détection automatique de conflit dans les discussions entre contributeurs Wikipedia qui s’appuie sur des traits de surface tels que la subjectivité ou la connotation des énoncés et évaluons deux règles de décision : l’une découle d’un modèle dialectique en exploitant localement la structure linéaire de la discussion, la subjectivité et la connotation ; l’autre, plus globale, ne s’appuie que sur la taille des fils et les marques de subjectivité au détriment des marques de connotation. Nous montrons que ces deux règles produisent des résultats similaires mais que la simplicité de la règle globale en fait une approche préférée dans la détection des conflits.</resume>
			<mots_cles>wikipedia, conflit, syntaxe, sémantique, interaction</mots_cles>
			<title>Conflicts detection in online epistemic communities</title>
			<abstract>Conflicts in online epistemic communities can be a blocking factor when producing knowledge. We present a way to automatically detect conflict in Wikipedia discussions, based on subjectivity and connotation marks. Two rules are evaluated : a local rule that uses the structure of the discussion threads, connotation and subjectivity marks and a global rule that takes the whole thread into account and only subjectivity. We show that the two rules produce similar results but that the simplicity of the global rule makes it a preferred approach to detect conflicts.</abstract>
			<keywords>wikipedia, conflict, syntax, semantics, interaction</keywords>
		</article>
		<article id="taln-2012-court-004" session="Poster 1">
			<auteurs>
				<auteur>
					<nom>Camille Dutrey</nom>
					<email>camille.dutrey@edf.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
					<affiliationId>3</affiliationId>
				</auteur>
				<auteur>
					<nom>Chloé Clavel</nom>
					<email>chloe.clavel@edf.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Sophie Rosset</nom>
					<email>sophie.rosset@limsi.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Ioana Vasilescu</nom>
					<email>vasilescu@limsi.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Martine Adda-Decker</nom>
					<email>madda@univ-paris3.fr</email>
					<affiliationId>2</affiliationId>
					<affiliationId>3</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">EDF R&amp;D, 1 avenue du Général de Gaulle 92141 Clamart</affiliation>
				<affiliation affiliationId="2">LIMSI-CNRS, rue John von Neumann 91403 Orsay</affiliation>
				<affiliation affiliationId="3">LPP, 19 rue des Bernardins 75005 Paris</affiliation>
			</affiliations>
			<titre>Quel est l'apport de la détection d'entités nommées pour l'extraction d'information en domaine restreint ?</titre>
			<type>court</type>
			<pages>359-366</pages>
			<resume>Les travaux liés à la définition et à la reconnaissance des entités nommées sont généralement envisagés en domaine ouvert, à travers la conception de catégories génériques (noms de personnes, de lieux, etc.) et leur application à des données textuelles issues de la presse (orale comme écrite). Par ailleurs, la fouille des données issues de centres d’appel est stratégique pour une entreprise comme EDF, compte tenu du rôle crucial joué par l’opinion pour les applications marketing, ce qui passe par la définition d’entités d’intérêt propres au domaine. Nous comparons les deux types de modèles d’entités - génériques et spécifiques à un domaine précis - afin d’observer leurs points de recouvrement, via l’annotation manuelle d’un corpus de conversations en centres d’appel. Nous souhaitons ainsi étudier l’apport d’une détection en entités nommées génériques pour l’extraction d’information métier en domaine restreint.</resume>
			<mots_cles>entités nommées, concepts métier, extraction d’information, données conversationnelles, annotation</mots_cles>
			<title>What is the contribution of named entities detection for information extraction in restricted domain ?</title>
			<abstract>In the framework of general domain dialog corpora a particular focus is dedicated to Named Entities definition and recognition, which are mostly very generic (personal names, locations, etc.). Moreover, call-centre data mining is strategic for a company like EDF, the public opinion analysis playing a significant role in EDF services quality evaluation and for marketing applications. In this purpose a domain dependant definition of entities of interest is essential. In this primary work we compare two types of entities models (generic and specific to the domain) in order to observe their respective coverage. We annotated manually a sub-corpus extracted from a large corpus of oral dialogs recorded in an EDF call-centre. The respective proportion of generic vs domain-specific Named Entities is then estimated. Impact for future work on building EDF domain-specific entities models is discussed.</abstract>
			<keywords>named entities, business concept, information extraction, conversational data, annotation</keywords>
		</article>
		<article id="taln-2012-court-005" session="Poster 1">
			<auteurs>
				<auteur>
					<nom>Egle Eensoo</nom>
					<email>egle.eensoo@inalco.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Mathieu Valette</nom>
					<email>mathieu.valette@inalco.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">INALCO, ERTIM, 2 rue de Lille, 75343 Paris Cedex 07</affiliation>
			</affiliations>
			<titre>Sur l'application de méthodes textométriques à la construction de critères de classification en analyse des sentiments</titre>
			<type>court</type>
			<pages>367-374</pages>
			<resume>Depuis une dizaine d'années, le TAL s'intéresse à la subjectivité, notamment dans la perspective d'applications telles que la fouille d'opinion et l'analyse des sentiments. Or, la linguistique de corpus outillée par des méthodes textométriques a souvent abordé la question de la subjectivité dans les textes. Notre objectif est de montrer d'une part, ce que pourrait apporter à l'analyse des sentiments l'analyse textométrique et d'autre part, comment mutualiser les avantages d'une association entre celle-ci et une méthode de classification automatique basée sur l'apprentissage supervisé. En nous appuyant sur un corpus de témoignages issus de forums de discussion, nous montrerons que la prise en compte de critères sélectionnés suivant une analyse textométrique permet d'obtenir des résultats de classification satisfaisants par rapport à une vision purement lexicale.</resume>
			<mots_cles>linguistique de corpus, textométrie, analyse de sentiments, classification automatique supervisée</mots_cles>
			<title>About the application of textometric methods for developing classification criteria in Sentiment analysis</title>
			<abstract>Over the last ten years, NLP has contributed to applied research on subjectivity, especially in applications such as Opinion mining and Sentiment analysis. However, corpus linguistics and textometry have often addressed the issue of subjectivity in text. Our purpose is to show, !rst, what textometric analysis could bring to sentiment analysis, and second, the bene!ts of pooling linguistic/textometric analysis and automatic classification methods based on supervised learning. By processing a corpus of posts from fora, we will show that the building of criteria from a textometric analysis could improve classification results, compared to a purely lexical approach.</abstract>
			<keywords>corpus linguistics, textometry, sentiment analysis, supervised learning</keywords>
		</article>
		<article id="taln-2012-court-006" session="Poster 1">
			<auteurs>
				<auteur>
					<nom>Michael Filhol</nom>
					<email>michael.filhol@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Annelies Braffort</nom>
					<email>annelies.braffort@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIMSI-CNRS, Campus d'Orsay bat 508, BP133, 91403 Orsay cx</affiliation>
			</affiliations>
			<titre>Méthodologie d'exploration de corpus et de formalisation de règles grammaticales pour les langues des signes</titre>
			<type>court</type>
			<pages>375-382</pages>
			<resume>Cet article présente une méthodologie visant, à partir d'une observation de corpus vidéo de langue des signes, à repérer puis formaliser les régularités de structure dans les constructions linguistiques. Cette méthodologie est applicable à tous les niveaux du langage, du sub-lexical à l'énoncé complet. En s'appuyant sur deux exemples, il présente une application de cette méthodologie ainsi que le modèle AZee qui, intégrant la souplesse nécessaire en termes de synchronisation des articulateurs, permet une formalisation des règles repérées.</resume>
			<mots_cles>Langue des signes, analyse de corpus, modèle grammatical, synchronisation</mots_cles>
			<title>Methodology for corpus exploration and grammatical rule building in Sign Language</title>
			<abstract>This paper presents a methodology for Sign Language video observation to extract and then formalise observed linguistic structure. This methodology is relevant to all linguistic layers from sub-lexical to discourse as a whole. Relying on two examples, we apply this methodology and describe the AZee model, which integrates the required flexibility for synchronising articulators, hence enables a specification of any new systematic rule observed.</abstract>
			<keywords>Sign Language, corpus analysis, grammatical models, synchronisation</keywords>
		</article>
		<article id="taln-2012-court-007" session="Poster 1">
			<auteurs>
				<auteur>
					<nom>Karën Fort</nom>
					<email>karen.fort@inist.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Vincent Claveau</nom>
					<email>vincent.claveau@irisa.fr</email>
					<affiliationId>3</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">INIST-CNRS, 2 allée de Brabois, 54500 Vandoeuvre-lès-Nancy</affiliation>
				<affiliation affiliationId="2">LIPN, Université Paris 13 &amp; CNRS, 99 av. J.B. Clément, 93430 Villetaneuse</affiliation>
				<affiliation affiliationId="3">IRISA - CNRS, Campus de Beaulieu, 35200 Rennes</affiliation>
			</affiliations>
			<titre>Annotation manuelle de matchs de foot : Oh la la la ! l'accord inter-annotateurs ! et c'est le but !</titre>
			<appartenance>
				<affiliation>INIST-CNRS</affiliation>
				<affiliation>LIPN, Université Paris 13</affiliation>
				<affiliation>IRISA-CNRS</affiliation>
			</appartenance>
			<type>court</type>
			<pages>383-390</pages>
			<resume>Cet article présente une campagne d’annotation de commentaires de matchs de football en français. L’annotation a été réalisée à partir d’un corpus très hétérogène, contenant à la fois des comptes-rendus minute par minute et des transcriptions des commentaires vidéo. Nous montrons ici comment les accords intra- et inter-annotateurs peuvent être utilisés efficacement, en en proposant une définition adaptée à notre type de tâche et en mettant en exergue l’importance de certaines bonnes pratiques concernant leur utilisation. Nous montrons également comment certains indices collectés à l’aide d’outils statistiques simples peuvent être utilisés pour indiquer des pistes de corrections des annotations. Ces différentes propositions nous permettent par ailleurs d’évaluer l’impact des modalités sources de nos textes (oral ou écrit) sur le coût et la qualité des annotations.</resume>
			<mots_cles>annotation manuelle, accords inter-annotateurs</mots_cles>
			<title>Manual Annotation of Football Matches : Inter-annotator Agreement ! Gooooal !</title>
			<abstract>We present here an annotation campaign of commentaries of football matches in French. The annotation was done from a very heterogeneous text corpus of both match minutes and video commentary transcripts. We show how the intra- and inter-annotator agreement can be used efficiently during the whole campaign by proposing a definition of the markables suited to our type of task, as well as emphasizing the importance of using it appropriately. We also show how some clues, collected through statistical analyses, could be used to help correcting the annotations. These statistical analyses are then used to assess the impact of the source modality (written or spoken) on the cost and quality of the annotation process.</abstract>
			<keywords>manual annotation, inter-annotator agreement</keywords>
		</article>
		<article id="taln-2012-court-008" session="Poster 1">
			<auteurs>
				<auteur>
					<nom>Anne Garcia-Fernandez</nom>
					<email>Anne.Garcia-Fernandez@cea.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Olivier Ferret</nom>
					<email>Olivier.Ferret@cea.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">CEA, LIST, Laboratoire Vision et Ingénierie des Contenus, Gif-sur-Yvette, F-91191 France</affiliation>
			</affiliations>
			<titre>Etude de différentes stratégies d'adaptation à un nouveau domaine en fouille d'opinion</titre>
			<type>court</type>
			<pages>391-398</pages>
			<resume>Le travail présenté dans cet article se situe dans le contexte de la fouille d’opinion et se focalise sur la détermination de la polarité d’un texte en adoptant une approche par apprentissage. Dans ce cadre, son objet est d’étudier différentes stratégies d’adaptation à un nouveau domaine dans le cas de figure fréquent où des données d’entraînement n’existent que pour un ou plusieurs domaines différents du domaine cible. Cette étude montre en particulier que l’utilisation d’une forme d’auto-apprentissage par laquelle un classifieur annote un corpus du domaine cible et modifie son corpus d’entraînement en y incorporant les textes classés avec la plus grande confiance se révèle comme la stratégie la plus performante et la plus stable pour les différents domaines testés. Cette stratégie s’avère même supérieure dans un nombre significatif de cas à la méthode proposée par (Blitzer et al., 2007) sur les mêmes jeux de test tout en étant plus simple.</resume>
			<mots_cles>fouille d’opinion, adaptation à un nouveau domaine, auto-apprentissage</mots_cles>
			<title>Study of various strategies for adapting an opinion classifier to a new domain</title>
			<abstract>The work presented in this article takes place in the field of opinion mining and aims more particularly at finding the polarity of a text by relying on machine learning methods. In this context, it focuses on studying various strategies for adapting a statistical classifier to a new domain when training data only exist for one or several other domains. This study shows more precisely that a self-training procedure consisting in enlarging the initial training corpus with texts from the target domain that were reliably classified by the classifier is the most successful and stable strategy for the tested domains. Moreover, this strategy gets better results in most cases than (Blitzer et al., 2007)’s method on the same evaluation corpus while it is more simple.</abstract>
			<keywords>opinion mining, domain adaptation, self-training</keywords>
		</article>
		<article id="taln-2012-court-009" session="Poster 1">
			<auteurs>
				<auteur>
					<nom>Olivier Kraif</nom>
					<email>olivier.kraif@u-grenoble3.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Sascha Diwersy</nom>
					<email>sascha.diwersy@uni-koeln.de</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIDILEM, Université Stendhal Grenoble 3, BP 25, 38040 Grenoble Cedex</affiliation>
				<affiliation affiliationId="2">Université de Cologne</affiliation>
			</affiliations>
			<titre>Le Lexicoscope : un outil pour l'étude de profils combinatoires et l'extraction de constructions lexico-syntaxiques</titre>
			<type>court</type>
			<pages>399-406</pages>
			<resume>Dans le cadre du projet franco-allemand Emolex, dédié à l'étude contrastive de la combinatoire du lexique des émotions en 5 langues, nous avons développé des outils et des méthodes permettant l'extraction, la visualisation et la comparaison de profls combinatoires pour des expressions simples et complexes. Nous présentons ici l'architecture d'ensemble de la plate-forme, conçue pour efectuer des extractions sur des corpus de grandes dimensions (de l'ordre de la centaine de millions de mots) avec des temps de réponse réduits (le corpus étant interrogeable en ligne1). Nous décrivons comment nous avons introduit la notion de pivots complexes, afn de permettre aux utilisateurs de rafner progressivement leurs requêtes pour caractériser des constructions lexico-syntaxiques élaborées. Enfn, nous donnons les premiers résultats d'un module d'extraction automatique d'expressions polylexicales récurrentes.</resume>
			<mots_cles>collocations, cooccurrences, profl combinatoire, expressions polylexicales, lexique des émotions</mots_cles>
			<title>The Lexicoscope : an integrated tool for combinatoric profles observation and lexico-syntactic constructs extraction</title>
			<abstract>The German-French research project Emolex whose aim is the contrastive study of the combinatorial behaviour of emotion lexemes in 5 languages has led to the development of methods and tools to extract, display and compare the combinatorial profles of simple and complex expressions. In this paper, we present the overall architecture of the query platform which has been conceived to ensure efcient processing of huge annotated text corpora (consisting of several hundred millions of word tokens) accessible through a web-based interface. We put forward the concept of “complex query nodes” introduced to enable users to carry out progressively elaborated extractions of lexical-syntactic patterns. We fnally give primary results of an automated method for the retrieval of recurrent multi-word expressions, which takes advantage of the complex query nodes implementation.</abstract>
			<keywords>collocations, combinatorial profles, multi-word expressions</keywords>
		</article>
		<article id="taln-2012-court-010" session="Poster 1">
			<auteurs>
				<auteur>
					<nom>Audrey Laroche</nom>
					<email>audrey.laroche@umontreal.ca</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">RALI-DIRO, Université de Montréal, C.P. 6128, Succ. Centre-Ville Montréal (Québec) H3C 3J7, Canada</affiliation>
			</affiliations>
			<titre>Analyse des contextes et des candidats dans l'identification des équivalents terminologiques en corpus comparables</titre>
			<type>court</type>
			<pages>407-414</pages>
			<resume>L’approche standard d’identification d’équivalents terminologiques à partir de corpus comparables repose sur la comparaison de mots contextuels en langues source et cible et sur l’utilisation d’un lexique bilingue. Nous analysons manuellement, selon des critères linguistiques (parties du discours, spécificité et relations sémantiques), les propriétés des mots contextuels et des erreurs commises par l’approche standard appliquée à la terminologie médicale pour suggérer des améliorations basées sur la sélection de mots contextuels.</resume>
			<mots_cles>équivalents terminologiques, vecteurs contextuels, corpus comparables, terminologie médicale, étude qualitative</mots_cles>
			<title>Analysis of contexts and candidates in term-translation spotting in comparable corpora</title>
			<abstract>The standard approach for identifying terminological equivalents from comparable corpora is based on the comparison of source and target language context words using a bilingual lexicon. We cary a manual analysis of the linguistic properties (parts of speech, specificity and semantic relations) of the context words and the inacurrate equivalents given by the standard approach applied to medical terminology, in order to suggest improvements based on the selection of context words.</abstract>
			<keywords>terminological equivalents, contextual vectors, comparable corpora, medical terminology, qualitative study.</keywords>
		</article>
		<article id="taln-2012-court-011" session="Poster 1">
			<auteurs>
				<auteur>
					<nom>Emmanuel Planas</nom>
					<email>emmanuel.planas@univ-nantes.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">UNAM, LINA, 2 rue de la Houssinière, BP 92208, 44322 Nantes</affiliation>
				<affiliation affiliationId="2">UNAM, UCO, ST, 3, place André Leroy, 49008 Angers</affiliation>
			</affiliations>
			<titre>BiTermEx Un prototype d'extraction de mots composés à partir de documents comparables via la méthode compositionnelle</titre>
			<type>court</type>
			<pages>415-422</pages>
			<resume>Nous décrivons BiTermEx, un prototype d'expérimentation de l'extraction de terminologie bilingue de mots composés, à partir de documents comparables, via la méthode compositionnelle. Nous expliquons la variation morphologique et la combinaison des constituants lexicaux des termes composés. Cette permet une précision TOP1 de 92% et 97,5% en français anglais, et de 94% en français japonais pour l'alignement de termes composés (textes scientifiques et de vulgarisation scientifique).</resume>
			<mots_cles>extraction terminologique, prototype, terminologie bilingue, documents comparables, méthode compositionnelle, mots composés, corpus</mots_cles>
			<title>BiTermEx , A prototype for the extraction of multiword terms from comparable documents through the compositional approach</title>
			<abstract>We describe BiTermEx, a prototype for extracting multiword terms from comparable corpora, using the compositional method. We focus on morphology-based variations of multiword constituents and their recombinaison. We experimented our approach on scientific and popular science corpora. We record TOP1 precisions of 92% and 97,5% on French to English alignments and 94% on French to Japanese.</abstract>
			<keywords>term extraction, prototype, bilingual terminology, comparable documents, compositional method, multiword terms, corpus</keywords>
		</article>
		<article id="taln-2012-court-012" session="Poster 1">
			<auteurs>
				<auteur>
					<nom>Laurie Serrano</nom>
					<email>Laurie.Serrano@unicaen.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Thierry Charnois</nom>
					<email>Thierry.Charnois@unicaen.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Stephan Brunessau</nom>
					<email>Stephan.Brunessau@cassidian.com</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Bruno Grilheres</nom>
					<email>Bruno.Grilheres@cassidian.com</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Maroua Bouzid</nom>
					<email>Maroua.Bouzid@unicaen.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Laboratoire GREYC, Université de Caen Basse-Normandie, Campus Côte de Nacre, Boulevard du Maréchal Juin, BP 5186 - 14032 Caen</affiliation>
				<affiliation affiliationId="2">Département IPCC, Cassidian, Parc d’Affaires des Portes - 27600 Val de Reuil</affiliation>
			</affiliations>
			<titre>Combinaison d'approches pour l'extraction automatique d'événements</titre>
			<type>court</type>
			<pages>423-430</pages>
			<resume>Dans cet article, nous présentons un système d’extraction automatique d’événements fondé sur deux approches actuelles en extraction d’information : la première s’appuie sur des règles linguistiques construites manuellement et la seconde se fonde sur un apprentissage automatique de patrons linguistiques. Les expérimentations réalisées montrent que combiner ces deux méthodes d’extraction permet d’améliorer significativement la qualité des événements extraits (amélioration de près de 10 points de F-mesure).</resume>
			<mots_cles>Extraction d’information, événements, approche symbolique, apprentissage de patrons linguistiques</mots_cles>
			<title>Automatic events extraction by combining multiple approaches</title>
			<abstract>In this paper, we present an automatic system for extracting events based on the combination of two existing information extraction approaches : the first one is made of hand-crafted linguistic rules and the second one is based on an automatic learning of linguistic patterns. We have shown that this mixed approach leads to a significant improvement of extraction performances.</abstract>
			<keywords>Text mining, events, symbolic extraction, linguistic pattern learning</keywords>
		</article>
		<article id="taln-2012-court-013" session="Poster 1">
			<auteurs>
				<auteur>
					<nom>Isabelle Tellier</nom>
					<email>isabelle.tellier@univ-paris3.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Denys Duchier</nom>
					<email>denys.duchier@univ-orleans.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Iris Eshkol</nom>
					<email>iris.eshkol@univ-orleans.fr</email>
					<affiliationId>3</affiliationId>
				</auteur>
				<auteur>
					<nom>Arnaud Courmet</nom>
					<email>arnaud.coumet@gmail.com</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Mathieu Martinet</nom>
					<email>mathieu_martinet@hotmail.fr</email>
					<affiliationId>3</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LaTTiCe, université Paris 3 - Sorbonne Nouvelle</affiliation>
				<affiliation affiliationId="2">LIFO, université d’Orléans</affiliation>
				<affiliation affiliationId="3">LLL, université d’Orléans</affiliation>
			</affiliations>
			<titre>Apprentissage automatique d'un chunker pour le français</titre>
			<type>court</type>
			<pages>431-438</pages>
			<resume>Nous décrivons dans cet article comment nous avons procédé pour apprendre automatiquement un chunker à partir du French Tree Bank, en utilisant les CRF (Conditional Random Fields). Nous avons réalisé diverses expériences, pour reconnaître soit l’ensemble de tous les chunks possibles, soit les seuls groupes nominaux simples. Nous évaluons le chunker obtenu aussi bien de manière interne (sur le French Tree Bank lui-même) qu’externe (sur un corpus distinct transcrit de l’oral), afin de mesurer sa robustesse.</resume>
			<mots_cles>chunking, apprentissage automatique, French Tree Bank, CRF</mots_cles>
			<title>Machine Learning of a chunker for French</title>
			<abstract>We describe in this paper how to automatically learn a chunker for French, from the French Tree Bank and CRFs (Conditional Random Fields). We did several experiments, either to recognize every possible kind of chunks, or to focus on simple nominal phrases only. We evaluate the obtained chunker on internal data (i.e. also extracted from the French Tree Bank) as well as on external (i.e from a distinct corpus) ones, to measure its robustness.</abstract>
			<keywords>chunking, Machine Learning, French Tree Bank, CRF</keywords>
		</article>
		<article id="taln-2012-court-014" session="Poster 1">
			<auteurs>
				<auteur>
					<nom>Nikola Tulechki</nom>
					<email>tanguy@univ-tlse2.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Ludovic Tanguy</nom>
					<email>tulechki@univ-tlse2.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">CLLE-ERSS : CNRS et Université de Toulouse 2, 5 allées Antonio Machado, 31058 Toulouse CEDEX 9</affiliation>
				<affiliation affiliationId="2">Conseil en Facteurs Humains, 4 impasse Montcabrier. 31500 Toulouse</affiliation>
			</affiliations>
			<titre>Effacement de dimensions de similarité textuelle pour l’exploration de collections de rapports d’incidents aéronautiques</titre>
			<type>court</type>
			<pages>439-446</pages>
			<resume>Cet article étudie le lien entre la similarité textuelle et une classification extrinsèque dans des collections de rapports d’incidents aéronautiques. Nous cherchons à compléter les stratégies d’analyse de ces collections en établissant automatiquement des liens de similarité entre les documents de façon à ce qu’ils ne reflètent pas l’organisation des schémas de codification utilisés pour leur classement. Afin de mettre en évidence les dimensions de variation transversales à la classification, nous calculons un score de dépendance entre les termes et les classes et excluons du calcul de similarité les termes les plus corrélés à une classe donnée. Nous montrons par une application sur 500 documents que cette méthode permet effectivement de dégager des thématiques qui seraient passées inaperçues au vu de la trop grande saillance des similarités de haut niveau.</resume>
			<mots_cles>similarité textuelle, classification de documents, corpus spécialisé</mots_cles>
			<title>Deletion of dimensions of textual similarity for the exploration of collections of accident reports in aviation</title>
			<abstract>In this paper we study the relationship between external classification and textual similarity in collections of incident reports. Our goal is to complement the existing classification-based analysis strategies by automatically establishing similarity links between documents in such a way that they do not reflect the dominant organisation of the classification schemas. In order to discover such transversal dimensions of similarity, we compute association scores between terms and classes and exlude the most correlated terms from the similarity calculation. We demonstrate on a 500 document corpus that by using this method, we can isolate topics that would otherwise have been masked by the dominant dimensions of similarity in the collection.</abstract>
			<keywords>textual simliarity, document classification, specialised corpora</keywords>
		</article>
		<article id="taln-2012-court-015" session="Poster 2">
			<auteurs>
				<auteur>
					<nom>Haithem Afli</nom>
					<email>Haithem.Afli@lium.univ-lemans.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Loïc Barrault</nom>
					<email>Loic.Barrault@lium.univ-lemans.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Holger Schwenk</nom>
					<email>Holger.Schwenk@lium.univ-lemans.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Laboratoire d’Informatique de l’Université du Maine</affiliation>
			</affiliations>
			<titre>Traduction automatique à partir de corpus comparables: extraction de phrases parallèles à partir de données comparables multimodales</titre>
			<type>court</type>
			<pages>447-454</pages>
			<resume>Les performances des systèmes de traduction automatique statistique dépendent de la disponibilité de textes parallèles bilingues, appelés aussi bitextes. Cependant, les corpus parallèles sont des ressources limitées et parfois indisponibles pour certains couples de langues ou domaines. Nous présentons une technique pour l’extraction de phrases parallèles à partir d’un corpus comparable multimodal (audio et texte). Ces enregistrements sont transcrits avec un système de reconnaissance automatique de la parole et traduits avec un système de traduction automatique. Ces traductions sont ensuite utilisées comme requêtes d’un système de recherche d’information pour sélectionner des phrases parallèles sans erreur et générer un bitexte. Plusieurs expériences ont été menées sur les données de la campagne IWSLT’11 (TED) qui montrent la faisabilité de notre approche.</resume>
			<mots_cles>Reconnaissance de la parole, traduction automatique statistique, corpus comparables multimodaux, extraction de phrases parallèles</mots_cles>
			<title>Automatic Translation from Comparable corpora : extracting parallel sentences from multimodal comparable corpora</title>
			<abstract>Statistical Machine Translation (SMT) systems depend on the availability of bilingual parallel text, also called bitext. However parallel corpora are a limited resource and are often not available for some domains or language pairs. We present an alternative method for extracting parallel sentences from multimodal comparable corpora. This work extends the use of comparable corpora, in using audio instead of text on the source side. The audio is transcribed by an automatic speech recognition system and translated with a base-line SMT system. We then use information retrieval in a large text corpus of the target language to extract parallel sentences. We have performed a series of experiments on data of the IWSLT’11 speech translation task (TED) that shows the feasibility of our approach.</abstract>
			<keywords>Automatic speech recognition, statistical machine translation, multimodal comparable corpora, extraction of parallel sentences</keywords>
		</article>
		<article id="taln-2012-court-016" session="Poster 2">
			<auteurs>
				<auteur>
					<nom>Yacine Ben Yahia</nom>
					<email>benyacine.sint@gmail.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Souha Mezghani Hammami</nom>
					<email>souha.mezghani@fsegs.rnu.tn</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Lamia Hadrich Belguith</nom>
					<email>l.belguith@fsegs.rnu.tn</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">ANLP Research Group - Laboratoire MIRACL/ FSEGS Sfax, Tunisie</affiliation>
			</affiliations>
			<titre>La reconnaissance automatique de la fonction des pronoms démonstratifs en langue arabe</titre>
			<type>court</type>
			<pages>455-462</pages>
			<resume>La résolution d'anaphores est l'une des tâches les plus difficiles du Traitement Automatique du Langage Naturel (TALN). La capacité de classifier les pronoms avant de tenter une tâche de résolution d'anaphores serait importante, puisque pour traiter un pronom cataphorique le système doit chercher l’antécédent dans le segment qui suit le pronom. Alors que, pour le pronom anaphorique, le système doit chercher l’antécédent dans le segment qui précède le pronom. En outre, le nombre des pronoms a été jugée non-trivial dans la langue arabe. C’est dans ce cadre que se situe notre travail qui consiste à proposer une méthode pour la classification automatique des pronoms démonstratifs arabes, basée sur l’apprentissage. Nous avons évalué notre approche sur un corpus composé de 365585 mots contenant 14318 pronoms démonstratifs et nous avons obtenu des résultats encourageants : 99.3% comme F-Mesure.</resume>
			<mots_cles>Pronoms démonstratifs, résolution des anaphores, traitement de la langue arabe</mots_cles>
			<title>Automatic recognition of demonstrative pronouns function in Arabic</title>
			<abstract>Anaphora resolution is one of the most difficult tasks in NLP. Classifying pronouns before attempting a task of anaphora resolution is important because to handle the cataphoric pronoun, the system should determine the antecedent into the segment following the pronoun. Although, for the anaphoric pronoun, the system should look for the antecedent into the segment before the pronoun. In addition, the number of demonstrative pronouns is very important in Arabic. In this paper, we describe a machine learning method for classifying demonstrative pronouns in Arabic. We have evaluated our approach on a corpus of 365585 words which contain 14318 demonstrative pronouns and we have obtained encouraging results: 99.3% as F-Measure.</abstract>
			<keywords>Demonstrative pronouns, anaphora resolution, ANLP</keywords>
		</article>
		<article id="taln-2012-court-017" session="Poster 2">
			<auteurs>
				<auteur>
					<nom>Andre Bittar</nom>
					<email>Andre.Bittar@xrce.xerox.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Caroline Hagège</nom>
					<email>Caroline.Hagege@xrce.xerox.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">XRCE, 6 Chemin de Maupertuis, 38240 Meylan, FRANCE</affiliation>
			</affiliations>
			<titre>Un annotateur automatique d'expressions temporelles du français et son évaluation sur le TimeBank du français</titre>
			<type>court</type>
			<pages>463-470</pages>
			<resume>Dans cet article, nous présentons un outil d’extraction et de normalisation d’un sous-ensemble d’expressions temporelles développé pour le français. Cet outil est mis au point et utilisé dans le cadre du projet ANR Chronolines1 et il est appliqué sur un corpus fourni par l’AFP. Notre but final dans le cadre du projet est de construire semi-automatiquement des chronologies événementielles à partir de la base de depêches de l’AFP. L’une des étapes du traitement est l’analyse de l’information temporelle véhiculée dans les textes. Nous avons donc développé un annotateur d’expressions temporelles pour le français que nous décrivons dans cet article. Nous présenterons également les résultats de son évaluation.</resume>
			<mots_cles>Analyse temporelle, évaluation</mots_cles>
			<title>An Automatic Temporal Expression Annotator and its Evaluation on the French TimeBank</title>
			<abstract>In this article, we present a tool that extracts and normalises a subset of temporal expressions in French. This tool is being developed and used in the ANR (French National Research Agency) project Chronolines, applied to a corpus of provided by the Agence France Presse. The aim of the project is to semi-automatically construct event chronologies from this corpus. To do this, a detailed analysis of the temporal information conveyed by texts, is required. The system we present here is the first version of a temporal annotator that we have developed for French. We describe it in this article and present the results of an evaluation.</abstract>
			<keywords>Temporal processing, evaluation</keywords>
		</article>
		<article id="taln-2012-court-018" session="Poster 2">
			<auteurs>
				<auteur>
					<nom>Laurence Danlos</nom>
					<email>Laurence.Danlos@linguist.jussieu.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Diégo Antolinos-Basso</nom>
					<email>Diego.Antolinos-Basso@linguist.jussieu.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Chloé Braud</nom>
					<email>Chloe.Braud@linguist.jussieu.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Charlotte Roze</nom>
					<email>Charlotte.Roze@linguist.jussieu.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">ALPAGE, Université Paris Diderot, 175 rue du Chevaleret, 75013 Paris</affiliation>
			</affiliations>
			<titre>Vers le FDTB : French Discourse Tree Bank</titre>
			<type>court</type>
			<pages>471-478</pages>
			<resume>Nous présentons les premiers pas vers la création d’un corpus annoté en discours pour le français : le French Discourse TreeBank enrichissant le FTB. La méthodologie adoptée s’inspire du Penn Discourse TreeBank (PDTB) mais elle s’en distingue sur au moins deux points à caractère théorique. D’abord, notre objectif est de fournir une couverture totale d’un texte du corpus, tandis que le PDTB ne fournit qu’une couverture partielle, qui ne peut donc pas être qualifiée d’analyse discursive comme celle faite en RST ou SDRT, deux théories majeures sur le discours. Ensuite, nous avons été amenés à définir une nouvelle hiérarchie des relations de discours qui s’inspire de RST, de SDRT et du PDTB.</resume>
			<mots_cles>Discours, corpus annoté manuellement, analyse discursive, PDTB, RST, SDRT</mots_cles>
			<title>Towards the FDTB : French Discourse Tree Bank</title>
			<abstract>We present the first steps towards creating an annotated corpus for discourse in French : the French Discourse Treebank enriching the FTB. Our methodology is based on the Penn Discourse Treebank (PDTB), but it differs in at least two points of a theoretical nature. First, our goal is to provide full coverage of a text, while the PDTB provides only partial coverage, which can not be described as discourse analysis such as the one made in RST or SDRT, two major theories on discourse. Second, we were led to define a new hierarchy of discourse relations which is based on RST, SDRT and PDTB.</abstract>
			<keywords>Discourse, manually annotated corpus, discourse analysis, PDTB, RST, SDRT</keywords>
		</article>
		<article id="taln-2012-court-019" session="Poster 2">
			<auteurs>
				<auteur>
					<nom>Romain Deveaud</nom>
					<email>romain.deveaud@univ-avignon.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Patrice Bellot</nom>
					<email>patrice.bellot@lsis.org</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIA - Université d’Avignon</affiliation>
				<affiliation affiliationId="2">LSIS - Université Aix-Marseille</affiliation>
			</affiliations>
			<titre>Combinaison de ressources générales pour une contextualisation implicite de requêtes</titre>
			<type>court</type>
			<pages>479-486</pages>
			<resume>L’utilisation de sources externes d’informations pour la recherche documentaire a été considérablement étudiée dans le passé. Des améliorations de performances ont été mises en lumière avec des corpus larges ou structurés. Néanmoins, dans ces études les ressources sont souvent utilisées séparément mais rarement combinées. Nous présentons une évaluation de la combinaison de quatre différentes ressources générales, standards et accessibles. Nous utilisons une mesure de distance informative pour extraire les caractéristiques contextuelles des différentes ressources et améliorer la représentation de la requête. Cette évaluation est menée sur une tâche de recherche d’information sur le Web en utilisant le corpus ClueWeb09 et les topics de la piste Web de TREC. Les meilleurs résultats sont obtenus en combinant les quatre ressources, et sont statistiquement significativement supérieurs aux autres approches.</resume>
			<mots_cles>Combinaison de ressources, RI contextuelle, recherche web</mots_cles>
			<title>Query Contextualization and Reformulation by Combining External Corpora</title>
			<abstract>Improving document retrieval using external sources of information has been extensively studied throughout the past. Improvements with either structured or large corpora have been reported. However, in these studies resources are often used separately and rarely combined together. We present an evaluation of the combination of four different scalable corpora over a web search task. An informative divergence measure is used to extract contextual features from the corpora and improve query representation. We use the ClueWeb09 collection along with TREC’s Web Track topics for the purpose of our evaluation. Best results are achieved when combining all four corpora, and are significantly better than the results of other approaches.</abstract>
			<keywords>Resources combination, contextual IR, web search.</keywords>
		</article>
		<article id="taln-2012-court-020" session="Poster 2">
			<auteurs>
				<auteur>
					<nom>Souhir Gahbiche-Braham</nom>
					<email>souhir.gahbiche@limsi.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Hélène Bonneau-Maynard</nom>
					<email>helene.maynard@limsi.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Thomas Lavergne</nom>
					<email>thomas.lavergne@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>François Yvon</nom>
					<email>francois.yvon@limsi.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIMSI-CNRS</affiliation>
				<affiliation affiliationId="2">Université Paris-Sud</affiliation>
			</affiliations>
			<titre>Repérage des entités nommées pour l'arabe : adaptation non-supervisée et combinaison de systèmes</titre>
			<type>court</type>
			<pages>487-494</pages>
			<resume>La détection des Entités Nommées (EN) en langue arabe est un prétraitement potentiellement utile pour de nombreuses applications du traitement des langues, en particulier pour la traduction automatique. Cette tâche représente toutefois un sérieux défi, compte tenu des spécificités de l’arabe. Dans cet article, nous présentons un compte-rendu de nos efforts pour développer un système de repérage des EN s’appuyant sur des méthodes statistiques, en détaillant les aspects liés à la sélection des caractéristiques les plus utiles pour la tâche ; puis diverses tentatives pour adapter ce système d’une manière entièrement non supervisée.</resume>
			<mots_cles>Adaptation non supervisée, Repérage des entités nommées</mots_cles>
			<title>Named Entity Recognition for Arabic : Unsupervised adaptation and Systems combination</title>
			<abstract>The recognition of Arabic Named Entities (NE) is a potentially useful preprocessing step for many Natural Language Processing Applications, such as Machine Translation. This task is however made very complex by some peculiarities of the Arabic language. In this paper, we present a summary of our recent efforts aimed at developing a statistical NE recognition system, with a specific focus on feature engineering aspects. We also report several approaches for adapting this system in an entirely unsupervised manner to a new domain.</abstract>
			<keywords>Unsupervised domain adaptation, named entity recognition</keywords>
		</article>
		<article id="taln-2012-court-021" session="Poster 2">
			<auteurs>
				<auteur>
					<nom>Nuria Gala</nom>
					<email>nuria.gala@lif.univ-mrs.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Caroline Brun</nom>
					<email>caroline.brun@xrce.xerox.com</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIF-CNRS UMR 7279, 163 av. de Luminy case 901, 13288 Marseille Cedex 9, France</affiliation>
				<affiliation affiliationId="2">Xerox Research Centre Europe, 6 chemin de Maupertuis 38240 Meylan, France</affiliation>
			</affiliations>
			<titre>Propagation de polarités dans des familles de mots : impact de la morphologie dans la construction d'un lexique pour l'analyse de sentiments</titre>
			<type>court</type>
			<pages>495-502</pages>
			<resume>Les ressources lexicales sont cruciales pour de nombreuses applications de traitement automatique de la langue (par exemple, l'extraction d'opinions à partir de corpus). Cependant, leur construction pose des problèmes à différents niveaux (coût, couverture, etc.). Dans cet article, nous avons voulu vérifier si les informations morphologiques liées à la dérivation pouvaient être exploitées pour l'annotation automatique d'informations sémantiques. En partant d'une ressource regroupant les mots en familles morphologiques en français, nous avons construit un lexique de polarités pour 4 065 mots, à partir d'une liste initiale d'adjectifs annotés manuellement. Les résultats obtenus montrent que la propagation des polarités est correcte pour 78,89% des familles avec un seul adjectif. Le lexique ainsi obtenu améliore aussi les résultats du système d'extraction d'opinions.</resume>
			<mots_cles>ressources lexicales, morphologie dérivationnelle, analyse de sentiments</mots_cles>
			<title>Spreading Polarities among Word Families: Impact of Morphology on Building a Lexicon for Sentiment Analysis</title>
			<abstract>Lexical resources are essential for many natural language applications (for example, opinion mining from corpora). However, building them entails different problems (cost, coverage, etc.). In this paper, we wanted to verify whether morphological information about derivation could be used to automatically annotate semantic information. Starting from a resource that groups words into morphological families in French, we have built a lexicon with polarities for 4 065 words from an initial seed set of manual annotated adjectives. The results obtained show that spreading polarities is accurate for 78.89% of the families with a unique adjective. The lexicon obtained also improves the results of the opinion mining system on different corpora.</abstract>
			<keywords>lexical resources, derivational morphology, opinion mining</keywords>
		</article>
		<article id="taln-2012-court-022" session="Poster 2">
			<auteurs>
				<auteur>
					<nom>Alexandre Labadié</nom>
					<email>Alexandre.Labadie@imag.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Patrice Enjalbert</nom>
					<email>Patrice.Enjalbert@unicaen.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Stephane Ferrari</nom>
					<email>Stephane.Ferrari@unicaen.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">GETALP, LIG, BP 53, 38041 Grenoble Cedex 9, France</affiliation>
				<affiliation affiliationId="2">Laboratoire GREYC, Université de Caen &amp; CNRS, Bd Maréchal Juin, BP 5186 F-14032 Caen Cedex, France</affiliation>
			</affiliations>
			<titre>Transitions thématiques : Annotation d'un corpus journalistique et premières analyses</titre>
			<type>court</type>
			<pages>503-510</pages>
			<resume>Le travail présenté dans cet article est centré sur la constitution d’un corpus de textes journalistiques annotés au niveau discursif d’un point de vue thématique. Le modèle d’annotation est une segmentation classique, à laquelle nous ajoutons un repérage de zones de transition entre unités thématiques. Nous faisons l’hypothèse que dans un texte bien construit, le scripteur fournit des indications aidant le lecteur à passer d’un sujet à un autre, l’identification de ces indices étant susceptible d’améliorer les procédures de segmentation automatique. Les annotations produites ont fait l’objet d’analyses quantitatives mettant en évidence un ensemble de propriétés des transitions entre thèmes.</resume>
			<mots_cles>Structure du discours, segments thématiques, transitions thématiques, annotation</mots_cles>
			<title>Manual thematic annotation of a journalistic corpus : first observations and evaluation</title>
			<abstract>The work presented in this paper focuses on the creation of a corpus of journalistic texts annotated at dicourse level, more precisely on a topic level. The annotation model is a classic segmentation one, to which we add transition zones between topical units. We assume that in a well-structured text, the author provides information helping the reader to move from one topic to another, where an identification of these clues is likely to improve automatic segmentation. The produced annotations have been subject of several quantitative analyses showing a set of linguistic properties of topical transitions.</abstract>
			<keywords>Discourse structure, topical segments, topical transitions, annotation</keywords>
		</article>
		<article id="taln-2012-court-023" session="Poster 2">
			<auteurs>
				<auteur>
					<nom>Blandine Plaisantin Alecu</nom>
					<email>blandine.alecu@prolipsia.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Izabella Thomas</nom>
					<email>izabella.thomas@univ-fcomte.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Julie Renahy</nom>
					<email>julie.renahy@prolipsia.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Prolipsia SAS, TEMIS Innovation, 18 r Alain Savary, 25000 Besançon</affiliation>
				<affiliation affiliationId="2">Centre Tesnière, Université de Franche-Comté, UFR SLHS, 30 r Mégevand, 25030 Besançon</affiliation>
			</affiliations>
			<titre>La "multi-extraction" comme stratégie d'acquisition optimisée de ressources terminologiques et non terminologiques</titre>
			<type>court</type>
			<pages>511-518</pages>
			<resume>A partir de l'évaluation d'extracteurs de termes menée initialement pour détecter le meilleur outil d'acquisition du lexique d'une langue contrôlée, nous proposons dans cet article une stratégie d'optimisation du processus d'extraction terminologique. Nos travaux, menés dans le cadre du projet ANR Sensunique, prouvent que la « multiextraction », c'est-à-dire la coopération de plusieurs extracteurs de termes, donne des résultats significativement meilleurs que l’extraction via un seul outil. Elle permet à la fois de réduire le silence et de filtrer automatiquement le bruit grâce à la variation d'un indice relatif au potentiel terminologique.</resume>
			<mots_cles>terminologie, extraction, langue contrôlée, potentiel terminologique, filtrage de termes</mots_cles>
			<title>Multi-extraction as a strategy of optimized extraction of terminological and lexical resources</title>
			<abstract>Based on the evaluation of terminological extractors, initially to find the best tool for building a controlled language lexicon, we propose a strategy of optimized extraction of terminological resources. Our work highlights that the cooperation of several extraction tools gives better results than the use of a single one. It both reduces silence and automatically filters noise thanks to a variable related to termhood.</abstract>
			<keywords>terminology, extraction, controlled language, termhood, term filtering</keywords>
		</article>
		<article id="taln-2012-court-024" session="Poster 2">
			<auteurs>
				<auteur>
					<nom>Arnaud Renard</nom>
					<email>arnaud.renard@insa-lyon.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Sylvie Calabretto</nom>
					<email>sylvie.calabretto@insa-lyon.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Béatrice Rumpler</nom>
					<email>beatrice.rumpler@insa-lyon.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université de Lyon, CNRS</affiliation>
				<affiliation affiliationId="2">INSA-Lyon, LIRIS, UMR 5205, F-69621 Villeurbanne Cedex</affiliation>
			</affiliations>
			<titre>Une Approche de Recherche d’Information Structurée fondée sur la Correction d’Erreurs à l’Indexation des Documents</titre>
			<type>court</type>
			<pages>519-526</pages>
			<resume>Dans cet article, nous nous sommes intéressés à la prise en compte des erreurs dans les contenus textuels des documents XML. Nous proposons une approche visant à diminuer l’impact de ces erreurs sur les systèmes de Recherche d’Information (RI). En effet, ces systèmes produisent des index associant chaque document aux termes qu’il contient. Les erreurs affectent donc la qualité des index ce qui conduit par exemple à considérer à tort des documents mal indexés comme non pertinents (resp. pertinents) vis-à-vis de certaines requêtes. Afin de faire face à ce problème, nous proposons d’inclure un mécanisme de correction d’erreurs lors de la phase d’indexation des documents. Nous avons implémenté cette approche au sein d’un prototype que nous avons évalué dans le cadre de la campagne d’évaluation INEX.</resume>
			<mots_cles>Recherche d’information, dysorthographie, correction d’erreurs, xml</mots_cles>
			<title>Structured Information Retrieval Approach based on Indexing Time Error Correction</title>
			<abstract>In this paper, we focused on errors in the textual content of XML documents. We propose an approach to reduce the impact of these errors on Information Retrieval (IR) systems. Indeed, these systems rely on indexes associating each document to corresponding terms. Indexes quality is negatively affected by those misspellings. These errors makes it difficult to later retrieve documents (or parts of them) in an effective way during the querying phase. In order to deal with this problem we propose to include an error correction mechanism during the indexing phase of documents. We achieved an implementation of this spelling aware information retrieval system which is currently evaluated over INEX evaluation campaign documents collection.</abstract>
			<keywords>Information retrieval, misspellings, error correction, xml</keywords>
		</article>
		<article id="taln-2012-court-025" session="Poster 2">
			<auteurs>
				<auteur>
					<nom>Raphaël Rubino</nom>
					<email>Raphael.Rubino@univ-avignon.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Stéphane Huet</nom>
					<email>Stephane.Huet@univ-avignon.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Fabrice Lefèvre</nom>
					<email>Fabrice.Lefevre@univ-avignon.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Georges Linarès</nom>
					<email>Georges.Linares@univ-avignon.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIA-CERI, Université d’Avignon et des Pays de Vaucluse, Avignon, France</affiliation>
			</affiliations>
			<titre>Post-édition statistique pour l'adaptation aux domaines de spécialité en traduction automatique</titre>
			<type>court</type>
			<pages>527-534</pages>
			<resume>Cet article présente une approche de post-édition statistique pour adapter aux domaines de spécialité des systèmes de traduction automatique génériques. En utilisant les traductions produites par ces systèmes, alignées avec leur traduction de référence, un modèle de post-édition basé sur un alignement sous-phrastique est construit. Les expériences menées entre le français et l’anglais pour le domaine médical montrent qu’une telle adaptation a posteriori est possible. Deux systèmes de traduction statistiques sont étudiés : une implémentation locale état-de-l’art et un outil libre en ligne. Nous proposons aussi une méthode de sélection de phrases à post-éditer permettant d’emblée d’accroître la qualité des traductions et pour laquelle les scores oracles indiquent des gains encore possibles.</resume>
			<mots_cles>Traduction automatique statistique, post-édition, adaptation aux domaines de spécialité</mots_cles>
			<title>Statistical Post-Editing of Machine Translation for Domain Adaptation</title>
			<abstract>This paper presents a statistical approach to adapt generic machine translation systems to the medical domain through an unsupervised post-edition step. A statistical post-edition model is built on statistical machine translation outputs aligned with their translation references. Evaluations carried out to translate medical texts from French to English show that a generic machine translation system can be adapted a posteriori to a specific domain. Two systems are studied : a state-of-the-art phrase-based implementation and an online publicly available software. Our experiments also indicate that selecting sentences for post-edition leads to significant improvements of translation quality and that more gains are still possible with respect to an oracle measure.</abstract>
			<keywords>Statistical Machine Translation, Post-editing, Domain Adaptation</keywords>
		</article>
		<article id="taln-2012-court-026" session="Poster 2">
			<auteurs>
				<auteur>
					<nom>Benoît Sagot</nom>
					<email>benoit.sagot@inria.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Marion Richard</nom>
					<email>ma.rih.on75@gmail.com</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Rosa Stern</nom>
					<email>rosa.stern@afp.com</email>
					<affiliationId>1</affiliationId>
					<affiliationId>3</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Alpage, INRIA Paris-Rocquencourt &amp; Université Paris Diderot, 175 rue du Chevaleret, 75013 Paris</affiliation>
				<affiliation affiliationId="2">ISHA, Université Paris Sorbonne, 7 rue Victor Cousin, 75006 Paris</affiliation>
				<affiliation affiliationId="3">AFP MediaLab, 2 place de la Bourse, 75002 Paris</affiliation>
			</affiliations>
			<titre>Annotation référentielle du Corpus Arboré de Paris 7 en entités nommées</titre>
			<type>court</type>
			<pages>535-542</pages>
			<resume>Le Corpus Arboré de Paris 7 (ou French TreeBank) est le corpus de référence pour le français aux niveaux morphosyntaxique et syntaxique. Toutefois, il ne contient pas d’annotations explicites en entités nommées. Ces dernières sont pourtant parmi les informations les plus utiles pour de nombreuses tâches en traitement automatique des langues et de nombreuses applications. De plus, aucun corpus du français annoté en entités nommées et de taille importante ne contient d’annotation référentielle, qui complète les informations de typage et d’empan sur chaque mention par l’indication de l’entité à laquelle elle réfère. Nous avons annoté manuellement avec ce type d’informations, après pré-annotation automatique, le Corpus Arboré de Paris 7. Nous décrivons les grandes lignes du guide d’annotation sous-jacent et nous donnons quelques informations quantitatives sur les annotations obtenues.</resume>
			<mots_cles>Résolution d’entités nommées, Corpus annoté, Corpus arboré de Paris 7</mots_cles>
			<title>Referential named entity annotation of the Paris 7 French TreeBank</title>
			<abstract>The French TreeBank developed at the University Paris 7 is the main source of morphosyntactic and syntactic annotations for French. However, it does not include explicit information related to named entities, which are among the most useful information for several natural language processing tasks and applications. Moreover, no large-scale French corpus with named entity annotations contain referential information, which complement the type and the span of each mention with an indication of the entity it refers to. We have manually annotated the French TreeBank with such information, after an automatic pre-annotation step. We sketch the underlying annotation guidelines and we provide a few figures about the resulting annotations.</abstract>
			<keywords>Named entity resolution, Annotated corpus, French TreeBank</keywords>
		</article>
		<article id="taln-2012-court-027" session="Poster 2">
			<auteurs>
				<auteur>
					<nom>Christophe Servan</nom>
					<email>christophe.servan@lium.univ-lemans.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Simon Petitrenaud</nom>
					<email>simon.petit-renaud@lium.univ-lemans.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIUM, Le Mans</affiliation>
			</affiliations>
			<titre>Utilisation des fonctions de croyance pour l'estimation de paramètres en traduction automatique</titre>
			<type>court</type>
			<pages>543-550</pages>
			<resume>Cet article concerne des travaux effectués dans le cadre du 7ème atelier de traduction automatique statistique et du projet ANR COSMAT. Ces travaux se focalisent sur l’estimation de paramètres contenus dans une table de traduction. L’approche classique consiste à estimer ces paramètres à partir de fréquences relatives d’éléments de traduction. Dans notre approche, nous proposons d’utiliser le concept de masses de croyance afin d’estimer ces paramètres. La théorie des fonctions de croyances est une théorie très adaptée à la gestion des incertitudes dans de nombreux domaines. Les expériences basées sur notre approche s’appliquent sur la traduction de la paire de langue français-anglais dans les deux sens de traduction.</resume>
			<mots_cles>Traduction automatique statistique, fonctions de croyance, apprentissage automatique, estimation de paramètres</mots_cles>
			<title>Feature calculation for Statistical Machine Translation by using belief functions</title>
			<abstract>In this paper, we consider the translation of texts within the framework of the 7th Workshop of Machine Translation evaluation task and the COSMAT corpus using a statistical machine translation approach. This work is focused on the translation features calculation of the phrase contained in a phrase table. The classical way to estimate these features are based on the direct computation counts or frequencies. In our approach, we propose to use the concept of belief masses to estimate the phrase probabilities. The Belief Function theory has proven to be suitable and adapted for the management of uncertainties in many domains. The experiments based on our approach are focused on the language pair English-French.</abstract>
			<keywords>Statistical machine Translation, belief function, machine learning, feature estimation</keywords>
		</article>
		<article id="taln-2012-court-028" session="Poster 2">
			<auteurs>
				<auteur>
					<nom>Philippe Suignard</nom>
					<email>Philippe.Suignard@edf.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Frederik Cailliau</nom>
					<email>cailliau@sinequa.com</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Ariane Cavet</nom>
					<email>cavet@sinequa.com</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">EDF R&amp;D, 1, avenue du Général de Gaulle, 92141, Clamart</affiliation>
				<affiliation affiliationId="2">Sinequa, 12 rue d’Athènes, 75009 Paris</affiliation>
			</affiliations>
			<titre>La longueur des tours de parole comme critère de sélection de conversations dans un centre d’appels</titre>
			<type>court</type>
			<pages>551-558</pages>
			<resume>Cet article s’intéresse aux conversations téléphoniques d’un Centre d’Appels EDF, automatiquement découpées en « tours de parole » et automatiquement transcrites. Il fait apparaître une relation entre la longueur des tours de parole et leur contenu, en ce qui concerne le vocabulaire qui les compose et les sentiments qui y sont véhiculés. Après avoir montré qu’il y a un intérêt à étudier ces longs tours, l’article analyse leur contenu et liste quelques exemples autour des notions d’argumentation et de réclamation. Il montre ainsi que la longueur des tours de parole peut être un critère utile de sélection de conversations.</resume>
			<mots_cles>Centre d’appels, Conversation, Tour de parole, Reconnaissance de Parole</mots_cles>
			<title>Turn-taking length as criterion to select call center conversations</title>
			<abstract>This article focuses on telephone conversations collected in an EDF Call Center, automatically segmented in “turn-taking” and automatically transcribed. It shows a relationship between the length of the turns and their content regarding the vocabulary and the feelings that are conveyed. After showing that there is an interest in studying these long turns, the article analyzes their content and lists some examples around the notions of argumentation and claim. It shows that the length of turns can be a useful criterion for selecting conversations.</abstract>
			<keywords>Call Center, Conversation, Turn Taking, Automatic Speech Recognition</keywords>
		</article>
		<article id="taln-2012-court-029" session="Poster 2">
			<auteurs>
				<auteur>
					<nom>Tristan Vanrullen</nom>
					<email>tristan.vanrullen@gmail.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Leïla Boutora</nom>
					<email>leila.boutora@lpl-aix.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Jean Dagron</nom>
					<email>jean.dagron@ap-hm.fr</email>
					<affiliationId>3</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">TVSI, 13009 Marseille</affiliation>
				<affiliation affiliationId="2">LPL, UMR 7309 CNRS/Univ. d’Aix-Marseille, 13100 Aix-en-Provence</affiliation>
				<affiliation affiliationId="3">Assistance publique - Hôpitaux de Marseille, 13005 Marseille</affiliation>
			</affiliations>
			<titre>Enjeux méthodologiques, linguistiques et informatiques pour le traitement du français écrit des sourds</titre>
			<type>court</type>
			<pages>559-566</pages>
			<resume>L’ouverture du Centre National de Réception des Appels d’Urgence (CNRAU) accessible aux sourds et malentendants fait émerger des questions linguistiques qui portent sur le français écrit des sourds, et des questions informatiques dans le domaine du traitement automatique du langage naturel. Le français écrit des sourds, pratiqué par une population hétérogène, comporte des spécificités morpho-syntaxiques et morpholexicales qui peuvent rendre problématique la communication écrite entre les personnes sourdes appelantes et les agents du CNRAU. Un premier corpus de français écrit sourd élicité avec mise en situation d’urgence (FAX-ESSU) a été recueilli dans la perspective de proposer des solutions TAL et linguistiques aux agents du CNRAU dans le cadre de ces échanges écrits. Nous présentons une première étude lexicale, morphosyntaxique et syntaxique de ce corpus reposant en partie sur une chaîne de traitement automatique, afin de valider les phénomènes linguistiques décrits dans la littérature et d'enrichir la connaissance du français écrit des sourds.</resume>
			<mots_cles>Français écrit des sourds, TAL, Français Langue Etrangère, linguistique de corpus, lexique, syntaxe, méthodologie</mots_cles>
			<title>Methodological, linguistic and computational challenges for processing written French of deaf people</title>
			<abstract>With the setup of a national emergency call-center for deaf people in France (CNRAU), some questions arise in linguistics and natural language processing about the written expression of deaf people. It is practiced by an heterogeneous population and shows morpho-syntactic, lexical and syntactic specificities which increase the difficulty, over the emergency situation, to successfully communicate between the deaf callers and the call-center operators. A first corpus (FAX-ESSU) of written French of deaf people was built with emergency conditions in order to provide linguistic and NLP solutions to the call center operators. On this corpus, we present a first study realized with the help of a natural language processing toolbox, in order to validation linguistic phenomenons described in the scientific literature and to enrich the knowledge of written French of deaf people.</abstract>
			<keywords>Written French of deaf people, NLP, French as a foreign language, corpus linguistics, lexicon, syntax, methodology</keywords>
		</article>
		<article id="taln-2012-demo-001" session="Démonstration">
			<auteurs>
				<auteur>
					<nom>Bruno Guillaume</nom>
					<email></email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Guillame Bonfante</nom>
					<email></email>
					<affiliationId>1</affiliationId>
					<affiliationId>3</affiliationId>
				</auteur>
				<auteur>
					<nom>Paul Masson</nom>
					<email></email>
					<affiliationId></affiliationId>
				</auteur>
				<auteur>
					<nom>Mathieu Morey</nom>
					<email></email>
					<affiliationId>4</affiliationId>
					<affiliationId>5</affiliationId>
				</auteur>
				<auteur>
					<nom>Guy Perrier</nom>
					<email></email>
					<affiliationId>1</affiliationId>
					<affiliationId>3</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LORIA - Campus Scientifique - BP 239 - 54506 Vandoeuvre-lès-Nancy cedex</affiliation>
				<affiliation affiliationId="2">INRIA Grand Est - 615, rue du Jardin Botanique - 54600 Villers-lès-Nancy</affiliation>
				<affiliation affiliationId="3">Université de Lorraine - 34, cours Léopold - CS 25233 - 54502 Nancy cedex</affiliation>
				<affiliation affiliationId="4">Laboratoire Parole et Langage, Aix-Marseille Université</affiliation>
				<affiliation affiliationId="5">Linguistics and Multilingual Studies, Nanyang Technological University</affiliation>
			</affiliations>
			<titre>Grew : un outil de réécriture de graphes pour le TAL</titre>
			<type>démonstration</type>
			<pages>1-2</pages>
			<resume>Nous présentons un outil de réécriture de graphes qui a été conçu spécifiquement pour des applications au TAL. Il permet de décrire des graphes dont les noeuds contiennent des structures de traits et dont les arcs décrivent des relations entre ces noeuds. Nous présentons ici la réécriture de graphes que l’on considère, l’implantation existante et quelques expérimentations.</resume>
			<mots_cles>réécriture de graphes, interface syntaxe-sémantique</mots_cles>
			<title>Grew: a Graph Rewriting Tool for NLP</title>
			<abstract>We present a Graph Rewriting Tool dedicated to NLP applications. Graph nodes contain feature structures and edges describe relations between nodes. We explain the Graph Rewriting framework we use, the implemented system and some experiments.</abstract>
			<keywords>graph rewriting, syntax-semantics interface</keywords>
		</article>
		<article id="taln-2012-demo-002" session="Démonstration">
			<auteurs>
				<auteur>
					<nom>Géraldine Damnati</nom>
					<email>geraldine.damnati@orange.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">France Telecom, Orange Labs, Lannion</affiliation>
			</affiliations>
			<titre>Interfaces de navigation dans des contenus audio et vidéo</titre>
			<type>démonstration</type>
			<pages>3-4</pages>
			<resume>Deux types de démonstrateurs sont présentés. Une première interface à visée didactique permet d'observer des traitements automatiques sur des documents vidéo. Plusieurs niveaux de représentation peuvent être montrés simultanément, ce qui facilite l'analyse d'approches multi-vues. La seconde interface est une interface opérationnelle de "consommation" de documents audio. Elle offre une expérience de navigation enrichie dans des documents audio grâce à une visualisation de métadonnées extraites automatiquement.</resume>
			<mots_cles>Traitements multi-vues, navigation enrichie</mots_cles>
			<title>Navigation interfaces through audio and video contents</title>
			<abstract>Two types of demonstrators are shown. A first interface, with didactic purposes, allows automatic processing of video documents to be observed. Several representation levels can be viewed simultaneously, which is particularly helpful to analyse the behaviour of multi-view approaches. The second interface is an operational audio document "consumption" interface. It offers an enriched navigation experience through the visualisation of automatically extracted metadata.</abstract>
			<keywords>Multi-view processing, enriched navigation</keywords>
		</article>
		<article id="taln-2012-demo-003" session="Démonstration">
			<auteurs>
				<auteur>
					<nom>Lionel Clément</nom>
					<email>lionel.clement@labri.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université Bordeaux 1 - LaBRI (UMR 5800) - CLEE-ERSS (UMR 5263)</affiliation>
			</affiliations>
			<titre>Synthèse de texte avec le logiciel Syntox</titre>
			<type>démonstration</type>
			<pages>5-6</pages>
			<resume>Le logiciel Syntox, dont une interface utilisateur en ligne ce trouve à cette URL : http://www.syntox.net, est une mise en application d’un modèle basé sur les grammaires attribuées, dans le cadre de la synthèse de texte. L’outil est une plateforme d’expérimentation dont l’ergonomie est simple. Syntox est capable de traiter des lexiques et des grammaires volumineux sur des textes ambigus à partir de la description explicite de phénomènes linguistiques.</resume>
			<mots_cles>Synthèse de texte, Grammaire attribuée, Syntaxe</mots_cles>
			<title>Automated generation of text with Syntox</title>
			<abstract>Syntox, which includes an online user interface at URL http://www.syntox.net, is an implementation of a model based on attribute grammars, in the context of automated generation of text. The sofware is intended as a platform for experimentation with an ergonomic interface. Syntox is usable with large vocabularies and grammars to produce ambiguous texts from an explicit description of linguistic phenomena.</abstract>
			<keywords>Text generation, Attribute Grammars, Syntax</keywords>
		</article>
		<article id="taln-2012-demo-004" session="Démonstration">
			<auteurs>
				<auteur>
					<nom>Isabelle Tellier</nom>
					<email>isabelle.tellier@univ-paris3.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Yoann Dupont</nom>
					<email>yoann.dupont@etu.univ-orleans.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Arnaud Courmet</nom>
					<email>arnaud.coumet@gmail.com</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LaTTiCe, université Paris 3 - Sorbonne Nouvelle</affiliation>
				<affiliation affiliationId="2">LIFO, université d’Orléans</affiliation>
			</affiliations>
			<titre>Un segmenteur-étiqueteur et un chunker pour le français</titre>
			<type>démonstration</type>
			<pages>7-8</pages>
			<resume>Nous proposons une démonstration de deux programmes : un segmenteur-étiqueteur POS pour le français et un programme de parenthésage en “chunks” de textes préalablement traités par le programme précédent. Tous deux ont été appris à partir du French Tree Bank.</resume>
			<mots_cles>étiquetage POS, chunking, apprentissage automatique, French Tree Bank, CRF</mots_cles>
			<title>A Segmenter-POS Labeller and a Chunker for French</title>
			<abstract>We propose a demo of two softwares : a Segmenter-POS Labeller for French and a Chunker for texts treated by the first program. Both have been learned from the French Tree Bank.</abstract>
			<keywords>POS tagging, chunking, Machine Learning, French Tree Bank, CRF</keywords>
		</article>
		<article id="taln-2012-demo-005" session="Démonstration">
			<auteurs>
				<auteur>
					<nom>Brigitte Bigi</nom>
					<email>brigitte.bigi@lpl-aix.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Laboratoire Parole et Langage, CNRS &amp; Aix-Marseille Université, 5 avenue Pasteur, BP80975, 13604 Aix-en-Provence France</affiliation>
			</affiliations>
			<titre>SPPAS : segmentation, phonétisation, alignement, syllabation</titre>
			<type>démonstration</type>
			<pages>9-10</pages>
			<resume>SPPAS est le nouvel outil du LPL pour l’alignement texte/son. La segmentation s’opère en 4 étapes successives dans un processus entièrement automatique ou semi-automatique, à partir d’un fichier audio et d’une transcription. Le résultat comprend la segmentation en unités inter-pausales, en mots, en syllabes et en phonèmes. La version actuelle propose un ensemble de ressources qui permettent le traitement du français, de l’anglais, de l’italien et du chinois. L’ajout de nouvelles langues est facilitée par la simplicité de l’architecture de l’outil et le respect des formats de fichiers les plus usuels. L’outil bénéficie en outre d’une documentation en ligne et d’une interface graphique afin d’en faciliter l’accessibilité aux non-informaticiens. Enfin, SPPAS n’utilise et ne contient que des ressources et programmes sous licence libre GPL.</resume>
			<mots_cles>segmentation, phonétisation, alignement, syllabation</mots_cles>
			<title>SPPAS : a tool to perform text/speech alignment</title>
			<abstract>SPPAS is a new tool dedicated to phonetic alignments, from the LPL laboratory. SPPAS produces automatically or semi-automatically annotations which include utterance, word, syllabic and phonemic segmentations from a recorded speech sound and its transcription. SPPAS is currently implemented for French, English, Italian and Chinese There is a very simple procedure to add other languages in SPPAS : it is just needed to add related resources in the appropriate directories. SPPAS can be used by a large community of users : accessibility and portability are importants aspects in its development. The tools and resources will all be distributed with a GPL license.</abstract>
			<keywords>segmentation, phonetization, alignement, syllabification</keywords>
		</article>
		<article id="taln-2012-demo-006" session="Démonstration">
			<auteurs>
				<auteur>
					<nom>François-Régis Chaumartin</nom>
					<email>frc@proxem.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Proxem, 19 bd de Magenta, 75010 Paris</affiliation>
			</affiliations>
			<titre>Solution Proxem d’analyse sémantique verticale : adaptation au domaine des Ressources Humaines</titre>
			<type>démonstration</type>
			<pages>11-12</pages>
			<resume>Proxem développe depuis 2007 une plate-forme de traitement du langage, Antelope, qui permet de construire rapidement des applications sémantiques verticales (par exemple, pour l’e-réputation, la veille économique ou l’analyse d’avis de consommateurs). Antelope a servi à créer une solution pour les Ressources Humaines, utilisée notamment par l’APEC, permettant (1) d’extraire de l’information à partir d’offres et de CVs et (2) de trouver les offres d’emploi correspondant le mieux à un CV (ou réciproquement). Nous présentons ici l’adaptation d’Antelope à un domaine particulier, en l’occurrence les RH.</resume>
			<mots_cles>entités nommées, extraction de relations, création d’ontologies, similarité</mots_cles>
			<title>How to adapt the Proxem semantic analysis engine to the Human Resources field</title>
			<abstract>Proxem develops since 2007 the NLP platform, Antelope, with which one can quickly build vertical semantic applications (for e-reputation, business intelligence or consumer reviews analysis, for instance). Antelope was used to create a Human Resources solution, notably used by APEC, making it possible (1) to extract information from resumes and offers and (2) to find the most relevant jobs matching a given resume (or vice versa). We present here how to adapt Antelope to a particular area, namely HR.</abstract>
			<keywords>named entities, information extraction, ontologies development, matching</keywords>
		</article>
		<article id="taln-2012-demo-007" session="Démonstration">
			<auteurs>
				<auteur>
					<nom>Estelle Delpech</nom>
					<email>Estelle@nomao.com</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Laurent Candillier</nom>
					<email>Laurent@nomao.com</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">NOMAO, 1 avenue Jean Rieux 31500 Toulouse</affiliation>
				<affiliation affiliationId="2">EBUZZING GROUP, 97 rue du cherche-midi 75006 Paris</affiliation>
			</affiliations>
			<titre>Nomao : un moteur de recherche géolocalisé spécialisé dans la recommandation de lieux et l’e-réputation</titre>
			<type>démonstration</type>
			<pages>13-14</pages>
			<resume>Cette démonstration présente NOMAO, un moteur de recherche géolocalisé qui permet à ses utilisateurs de trouver des lieux (bars, magasins...) qui correspondent à leurs goûts, à ceux de leurs amis et aux recommandations des internautes.</resume>
			<mots_cles>recherche d’information, analyse d’opinion, génération de texte, fouille du web</mots_cles>
			<title>Nomao : a geolocalized search engine dedicated to place recommendation and ereputation</title>
			<abstract>This demonstration showcases NOMAO, a geolocalized search engine which recommends places (bars, shops...) based on the user’s and its friend’s tastes and on the web surfers’ recommendations</abstract>
			<keywords>information retrieval, opinion mining, text generation, web mining</keywords>
		</article>
		<article id="taln-2012-demo-008" session="Démonstration">
			<auteurs>
				<auteur>
					<nom>Samira Moukrim</nom>
					<email>samiramoukrim@yahoo.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université Sidi Mohamed Ben Abdellah</affiliation>
			</affiliations>
			<titre>Le DictAm : Dictionnaire électronique des verbes amazighs-français</titre>
			<type>démonstration</type>
			<pages>15-16</pages>
			<resume>Le DictAm est un dictionnaire électronique des verbes amazighs-français. Il vise à rendre compte de l’ensemble des verbes dans le domaine berbère : conjugaison, diathèse et sens. Le DictAm comporte actuellement près de 3000 verbes dans une soixantaine de parlers berbères. C’est un travail qui est en cours de réalisation et qui a pour ambition de répertorier tous les verbes berbères ainsi que leurs équivalents en français.</resume>
			<mots_cles>Dictionnaire électronique, dimension bilingue, diversité linguistique, verbes</mots_cles>
			<title>The DictAm : An electronic dictionary of Amazigh-French verbs</title>
			<abstract>In the frame of promoting the linguistic diversity among knowledge society, we suggest to elaborate an electronic dictionary of Amazigh-French verbs (DictAm). This dictionary aims at accounting for the verbs in the Amazigh sphere as a whole: conjugation, diathesis and meaning. The DictAm also adopts a comparative perspective in the sense that it collects the lexical materials of different dialectal varieties and makes them reachable. Now, the DictAm comprises around 3000 verbs deriving from about sixty Amazigh speeches. This work is currently in progress as well as it aspires to set up a repertoire of all Amazigh verbs and their French equivalents.</abstract>
			<keywords>Electronic dictionary, bilingual dimension, linguistic diversity, verbs</keywords>
		</article>
		<article id="taln-2012-demo-009" session="Démonstration">
			<auteurs>
				<auteur>
					<nom>Thomas Hueber</nom>
					<email>Thomas.Hueber@gipsa-lab.grenoble-inp.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Atef Ben-Youssef</nom>
					<email>Atef.Ben-Youssef@gipsa-lab.grenoble-inp.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Pierre Badin</nom>
					<email>Pierre.Badin@gipsa-lab.grenoble-inp.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Gérard Bailly</nom>
					<email>Gerard.Bailly@gipsa-lab.grenoble-inp.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Frédéric Eliséi</nom>
					<email>Frederic.Elisei@gipsa-lab.grenoble-inp.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">GIPSA-lab, UMR 5216/CNRS/INP/UJF/U.Stendhal, Grenoble, France</affiliation>
			</affiliations>
			<titre>Vizart3D : Retour Articulatoire Visuel pour l’Aide à la Prononciation</titre>
			<type>démonstration</type>
			<pages>17-18</pages>
			<resume>L’objectif du système Vizart3D est de fournir à un locuteur, en temps réel, et de façon automatique, un retour visuel sur ses propres mouvements articulatoires. Les applications principales de ce système sont l’aide à l’apprentissage des langues étrangères et la rééducation orthophonique (correction phonétique). Le système Vizart3D est basé sur la tête parlante 3D développée au GIPSA-lab, qui laisse apparaître, en plus des lèvres, les articulateurs de la parole normalement cachés (comme la langue). Cette tête parlante est animée automatiquement à partir du signal audio de parole, à l’aide de techniques de conversion de voix et de régression acoustico-articulatoire par GMM.</resume>
			<mots_cles>retour visuel, aide à la prononciation, GMM, temps réel, tête parlante</mots_cles>
			<title>Vizart3D: Visual Articulatory Feedack for Computer-Assisted Pronunciation Training</title>
			<abstract>We describe a system of visual articulatory feedback, which aims to provide any speaker with a real feedback on his/her own articulation. Application areas are computerassisted pronunciation training (phonetic correction) for second-language learning and speech rehabilitation. This system, named Vizartd3D, is based on the 3D augmented talking head developed at GIPSA-lab, which is able to display all speech articulators including usually hidden ones like the tongue. In our approach, the talking head is animated automatically from the audio speech signal, using GMM-based voice conversion and acoustic-to-articulatory regression.</abstract>
			<keywords>visual feedback, pronunciation training, GMM, real-time, talking head</keywords>
		</article>
		<article id="taln-2012-demo-010" session="Démonstration">
			<auteurs>
				<auteur>
					<nom>Emmanuel Ferragne</nom>
					<email>rocme@ish-lyon.cnrs.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Sébastien Flavier</nom>
					<email>rocme@ish-lyon.cnrs.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Christian Fressard</nom>
					<email>rocme@ish-lyon.cnrs.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">CLILLAC-ARP, Université Paris 7</affiliation>
				<affiliation affiliationId="2">Dynamique Du Langage, UMR 5596, CNRS-Université de Lyon</affiliation>
			</affiliations>
			<titre>ROCme! : logiciel pour l’enregistrement et la gestion de corpus oraux</titre>
			<type>démonstration</type>
			<pages>19-20</pages>
			<resume>ROCme! permet une gestion rationalisée, autonome et dématérialisée de l’enregistrement de corpus oraux. Il dispose notamment d’une interface pour le recueil de métadonnées sur les locuteurs totalement paramétrable via des balises XML. Les locuteurs peuvent gérer les réponses au questionnaire, l’enregistrement audio, la lecture, la sauvegarde et le défilement des phrases (ou autres types de corpus) en toute autonomie. ROCme! affiche du texte, avec ou sans mise en forme HTML, des images, du son et des vidéos.</resume>
			<mots_cles>corpus, oral, linguistique, logiciel</mots_cles>
			<title>ROCme!: software for the recording and management of oral corpora</title>
			<abstract>management of speech recordings. Users can create interfaces for metadata collection thanks to XML tags. Speakers autonomously fill in questionnaires, record, play, and save audio; and browse sentences (or other types of corpora). ROCme! can display text, optionally with HTML formatting, images, sounds, and video.</abstract>
			<keywords>corpus, oral, linguistics, software</keywords>
		</article>
	</articles>
</conference>