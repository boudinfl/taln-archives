<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
"http://www.w3.org/TR/html4/loose.dtd">
<html><head><title>Annotation manuelle de matchs de foot : Oh la la la ! l'accord inter-annotateurs ! et c'est le but !</title>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
</head>
<body>
<div style="page-break-before:always; page-break-after:always"><div><p>Actes de la conf&#233;rence conjointe JEP-TALN-RECITAL 2012, volume 2: TALN, pages 383&#8211;390,
Grenoble, 4 au 8 juin 2012. c&#169;2012 ATALA &amp; AFCP
</p>
<p>Annotation manuelle de matchs de foot :
Oh la la la ! l&#8217;accord inter-annotateurs ! et c&#8217;est le but !
</p>
<p>Kar&#235;n Fort1, 2 Vincent Claveau3
(1) INIST-CNRS, 2 all&#233;e de Brabois, 54500 Vandoeuvre-l&#232;s-Nancy
</p>
<p>(2) LIPN, Universit&#233; Paris 13 &amp; CNRS, 99 av. J.B. Cl&#233;ment, 93430 Villetaneuse
(3) IRISA - CNRS, Campus de Beaulieu, 35200 Rennes
</p>
<p>karen.fort@inist.fr, vincent.claveau@irisa.fr
</p>
<p>R&#201;SUM&#201;
Cet article pr&#233;sente une campagne d&#8217;annotation de commentaires de matchs de football en
fran&#231;ais. L&#8217;annotation a &#233;t&#233; r&#233;alis&#233;e &#224; partir d&#8217;un corpus tr&#232;s h&#233;t&#233;rog&#232;ne, contenant &#224; la fois des
comptes-rendus minute par minute et des transcriptions des commentaires vid&#233;o. Nous montrons
ici comment les accords intra- et inter-annotateurs peuvent &#234;tre utilis&#233;s efficacement, en en
proposant une d&#233;finition adapt&#233;e &#224; notre type de t&#226;che et en mettant en exergue l&#8217;importance
de certaines bonnes pratiques concernant leur utilisation. Nous montrons &#233;galement comment
certains indices collect&#233;s &#224; l&#8217;aide d&#8217;outils statistiques simples peuvent &#234;tre utilis&#233;s pour indiquer
des pistes de corrections des annotations. Ces diff&#233;rentes propositions nous permettent par
ailleurs d&#8217;&#233;valuer l&#8217;impact des modalit&#233;s sources de nos textes (oral ou &#233;crit) sur le co&#251;t et la
qualit&#233; des annotations.
</p>
<p>ABSTRACT
Manual Annotation of Football Matches : Inter-annotator Agreement ! Gooooal !
</p>
<p>We present here an annotation campaign of commentaries of football matches in French. The
annotation was done from a very heterogeneous text corpus of both match minutes and video
commentary transcripts. We show how the intra- and inter-annotator agreement can be used
efficiently during the whole campaign by proposing a definition of the markables suited to our
type of task, as well as emphasizing the importance of using it appropriately. We also show
how some clues, collected through statistical analyses, could be used to help correcting the
annotations. These statistical analyses are then used to assess the impact of the source modality
(written or spoken) on the cost and quality of the annotation process.
</p>
<p>MOTS-CL&#201;S : annotation manuelle, accords inter-annotateurs.
</p>
<p>KEYWORDS: manual annotation, inter-annotator agreement.
</p>
<p>383</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>1 Introduction
Nous &#233;tudions dans cet article la cr&#233;ation d&#8217;un corpus textuel annot&#233; construit &#224; partir de
transcriptions de commentaires vid&#233;os et de sites Web sp&#233;cialis&#233;s. Ce corpus annot&#233; est d&#233;velopp&#233;
dans le but de mettre au point des techniques automatiques d&#8217;analyse, tels que le r&#233;sum&#233; vid&#233;o,
le repurposing (transformation du contenu et du format pour un autre support de diffusion) ou
l&#8217;extraction d&#8217;information pour les vid&#233;os d&#8217;&#233;v&#233;nements sportifs. Cette application, d&#233;velopp&#233;e
dans le cadre d&#8217;un partenariat industriel, n&#8217;est pas d&#233;taill&#233;e plus avant dans cet article, mais il est
important de noter qu&#8217;elle guide la d&#233;finition des &#233;l&#233;ments &#224; annoter (cf. section 2).
</p>
<p>Outre la pr&#233;sentation d&#8217;une nouvelle ressource annot&#233;e, cet article a pour objectif de montrer
l&#8217;int&#233;r&#234;t d&#8217;analyses fines pour &#233;valuer la qualit&#233; d&#8217;une telle ressource h&#233;t&#233;rog&#232;ne. En particulier,
nous proposons une d&#233;finition des mesures d&#8217;accord inter- et intra-annotateur adapt&#233;e &#224; ce type
d&#8217;annotation o&#249; seuls certains &#233;l&#233;ments des corpus sont annot&#233;s. Nous montrons &#233;galement
comment certains indices collect&#233;s &#224; l&#8217;aide d&#8217;outils statistiques simples peuvent &#234;tre utilis&#233;s
pour souligner les difficult&#233;s de la t&#226;che d&#8217;annotation et indiquer des pistes de corrections des
annotations. Ces diff&#233;rentes propositions nous permettent par ailleurs d&#8217;&#233;valuer l&#8217;impact des
modalit&#233;s sources de nos textes (oral ou &#233;crit) sur le co&#251;t et la qualit&#233; des annotations.
</p>
<p>D&#8217;un point de vue applicatif, quelques travaux (Nemrava et al., 2007, par exemple) font r&#233;f&#233;rence
&#224; un corpus annot&#233; du domaine du football, mais &#224; notre connaissance, aucun ne d&#233;taille
l&#8217;annotation du corpus utilis&#233;. D&#8217;autres &#233;tudes ont fait usage de corpus de football pour cr&#233;er
des lexiques monolingues (Gasiglia, 2003) or multilingues (Schmidt, 2008) plus ou moins
d&#233;taill&#233;s. Dans ces cas, si les publications associ&#233;es d&#233;taillent l&#8217;annotation du corpus utilis&#233;, les
annotations elles-m&#234;me sont de nature linguistique plut&#244;t que du domaine et soul&#232;vent des
questions diff&#233;rentes. D&#8217;un point de vue m&#233;thodologique, l&#8217;analyse statistique des annotations
repose principalement sur les calculs d&#8217;accord inter-annotateurs (Artstein et Poesio, 2008, pour
une revue d&#233;taill&#233;e). Ces derniers sont g&#233;n&#233;ralement fournis sur les corpus annot&#233;s comme
mesure d&#8217;&#233;valuation de la qualit&#233; de la ressource produite (Dandapat et al., 2009, inter alia). Les
m&#233;thodes d&#8217;annotation agiles (Voormann et Gut, 2008) proposent d&#8217;utiliser ces mesures pendant
toute l&#8217;annotation du corpus, pour assurer la coh&#233;rence des annotations et limiter les divergences
dans les cas, majoritaires, o&#249; l&#8217;on ne peut pas tout annoter en double avec adjudication. Notre
travail se situe dans ce cadre mais aborde plusieurs probl&#232;mes pos&#233;s par les particularit&#233;s de nos
annotations. Apr&#232;s une pr&#233;sentation des donn&#233;es et des annotations en section 2, nous d&#233;taillons
les diff&#233;rentes analyses men&#233;es en section 3 et nous concluons en donnant quelques perspectives
&#224; ce travail.
</p>
<p>2 Campagne d&#8217;annotation
</p>
<p>2.1 Donn&#233;es, annotations et m&#233;thodologie
</p>
<p>Le corpus annot&#233; couvre 16 matchs de football. Il est compos&#233; de 24 transcriptions de commen-
taires tir&#233;s de vid&#233;os (1 par mi-temps, 12 matchs) et de 16 fichiers contenant une description
minute-par-minute du match (dont les 12 de la transcription et 4 matchs additionnels) tir&#233;s
de sites Web sp&#233;cialis&#233;s. La parole contenue dans les vid&#233;os a &#233;t&#233; transcrite manuellement en
utilisant TRANSCRIBER (Barras et al., 1998) et son guide de transcription par d&#233;faut. L&#8217;ensemble du
corpus a une taille d&#8217;environ 250 000 mots. Sa principale caract&#233;ristique est d&#8217;&#234;tre tr&#232;s h&#233;t&#233;rog&#232;ne
</p>
<p>384</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>(Fort et al., 2011), que ce soit d&#8217;un point de vue des types de match (ligues, championnats...),
de la taille des fichiers (de 1 116 tokens par match pour les minutes &#224; 21 000 tokens pour les
transcriptions), ou de la source (cha&#238;nes de diffusion des vid&#233;os, commentateurs, sites Web...).
</p>
<p>Le jeu d&#8217;&#233;tiquettes a &#233;t&#233; construit en d&#233;finissant les &#233;l&#233;ments int&#233;ressants pour l&#8217;application finale
et ensuite affin&#233; durant les phases d&#8217;entra&#238;nement et de pr&#233;-campagne. L&#8217;ensemble des &#233;tiquettes
retenues a &#233;t&#233; divis&#233; en trois couches, Unit&#233;s, Actions et Relations (cf. tableau 1 1), chacune corres-
pondant &#224; un niveau d&#8217;analyse de complexit&#233; croissante &#224; aborder successivement par les annota-
teurs. Par coh&#233;rence avec les besoins applicatifs et pour prendre en compte le style elliptique de
l&#8217;oral (&#171; Makoun. Et c&#8217;est r&#233;cup&#233;r&#233;. Clerc, avec Cris. Boumsong, Makoun. &#187;),
nous avons d&#233;cid&#233; de ne pas faire porter les annotations sur les pr&#233;dicats d&#233;notant les actions ou
les relations, souvent absents, mais sur les acteurs impliqu&#233;s.
</p>
<p>Unit&#233;s
acteurs Joueur, Equipe, Arbitre, Entraineur, ArbitreAssistant, Pr&#233;sident
circonstants EspaceSurTerrain, LieuDuMatch, TempsDansMatch
</p>
<p>Actions
arbitrales TirerCoupFrancDirect, TirerCoupFrancIndirect, TirerCorner, TirerPenalty, Faire-
</p>
<p>FauteDeJeu, HorsJeu, MarquerBut, PrendreCartonJaune, PrendreCartonRouge,
PrendreRappelALOrdre
</p>
<p>autres Centrer, FaireTentative2Centre, Dribbler, RaterBut, ArreterBut, IntercepterBallon,
PossederBallon, ActionDuPublic
</p>
<p>Relations
arbitrales FaireFauteSurJoueur, TaclerFaute, RemplacerJoueur
autres FaireCombinaison, FairePasse, FaireTentative2Passe
</p>
<p>TABLE 1 &#8211; Couches d&#8217;annotations retenues et &#233;tiquettes correspondantes
</p>
<p>La m&#233;thodologie employ&#233;e pour l&#8217;annotation de ce corpus suit les recommandations de Bonneau-
Maynard et al. (2005) et Gut et Bayerl (2004) ; elle est d&#233;crite en d&#233;tail dans (Fort et Claveau,
2012). L&#8217;annotation a &#233;t&#233; r&#233;alis&#233;e par deux annotateurs experts du domaine avec l&#8217;outil d&#8217;anno-
tation GLOZZ (Widl&#246;cher et Mathet, 2009), choisi en raison de sa facilit&#233; d&#8217;utilisation et de la
possibilit&#233; qu&#8217;il offre d&#8217;annoter des relations. Les temps d&#8217;annotation par couche ont &#233;t&#233; mesur&#233;es
&#224; l&#8217;aide de l&#8217;outil TIMETRACKER 2. Nous avons &#233;galement invit&#233; les annotateurs &#224; ajouter des
commentaires sur leurs annotations, et un attribut Incertitude a &#233;t&#233; mis &#224; leur disposition dans
GLOZZ.
</p>
<p>2.2 Donn&#233;es g&#233;n&#233;rales sur le processus d&#8217;annotation
</p>
<p>Le nombre total d&#8217;annotations produites s&#8217;&#233;l&#232;ve &#224; 37 784 dont 27 736 (soit plus de 73 %) pour
les transcriptions. Toutes les cat&#233;gories ont &#233;t&#233; utilis&#233;es, mais avec une grande disparit&#233; : par
exemple, TirerCoupFrancIndirect et TirerPenalty n&#8217;ont servi que 2 fois (et uniquement dans les
minutes), PrendreCartonRouge 6 fois et Pr&#233;sident 9 fois.
</p>
<p>Le tableau 2 pr&#233;sente le temps d&#8217;annotation moyen (pour 1 000 tokens) par annotateur et par
source. Un t-test de Welsh &#224; deux &#233;chantillons (avec p = 0,05) montre que les diff&#233;rences
entre annotateurs ne sont pas significatives, que ce soit pour les transcriptions ou pour les
</p>
<p>1. Le regroupement des &#233;tiquettes &#224; l&#8217;int&#233;rieur de ces couches (circonstants, acteurs, etc) est propos&#233; ici pour faciliter
la lecture et l&#8217;analyse, mais n&#8217;existait pas dans le mod&#232;le de donn&#233;es utilis&#233; pour l&#8217;annotation.
</p>
<p>2. http://www.formassembly.com/time-tracker/#
</p>
<p>385</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Minutes Transcriptions
</p>
<p>Annotateur 1 36,92 20,03
</p>
<p>Annotateur 2 41,30 16,06
</p>
<p>TABLE 2 &#8211; Temps moyen d&#8217;annotation par source et par annotateur, en minute/1 000 tokens
</p>
<p>minutes. En revanche, les diff&#233;rences entre modalit&#233;s sont jug&#233;es statistiquement significatives,
pour les deux annotateurs. Cela s&#8217;explique par la diff&#233;rence (statistiquement significative) de
densit&#233; d&#8217;annotations (nombre d&#8217;annotations par token) : 0,16 pour les minutes et 0,08 pour les
transcriptions. En effet, les commentateurs sportifs ne parlent pas uniquement des &#233;v&#233;nements
du matchs et ont tendance &#224; digresser. En revanche, si l&#8217;on rapporte le temps d&#8217;annotation au
nombre d&#8217;annotations produites, aucune diff&#233;rence n&#8217;est constat&#233;e entre minutes et transcriptions.
Les diff&#233;rences de temps entre les deux modalit&#233;s s&#8217;expliquent donc uniquement par le nombre
plus important d&#8217;annotations &#224; produire &#224; volume de texte constant.
</p>
<p>3 Analyse statistique des annotations
</p>
<p>3.1 Mesures d&#8217;accord et estimation des &#8220;annotables&#8221;
</p>
<p>Les calculs d&#8217;accords inter- et intra-annotateur servent &#224; quantifier la fiabilit&#233;, et donc la qualit&#233;,
des annotations produites, mais aussi &#224; fixer une limite sup&#233;rieure aux performances que l&#8217;on
peut attendre d&#8217;un syst&#232;me automatique, et enfin, dans notre cas, &#224; mesurer la difficult&#233; de
la t&#226;che selon la modalit&#233; d&#8217;origine. Pour ce faire, les Kappa (&#954;) de Cohen (Cohen, 1960) et
de Carletta (Carletta, 1996) sont pr&#233;f&#233;r&#233;s aux mesures plus simples telles que la F-mesure car
ils normalisent l&#8217;accord observ&#233; en fonction de l&#8217;accord attendu (ou d&#251; au hasard). Carletta
consid&#232;re que l&#8217;annotation par hasard se traduit par une unique distribution valable pour les deux
annotateurs, alors que Cohen consid&#232;re que ces distributions d&#233;pendent de chaque annotateur
(Artstein et Poesio, 2008, pour une description compl&#232;te et des comparaisons).
</p>
<p>Cependant, ces d&#233;finitions posent probl&#232;me d&#232;s lors que ce ne sont pas seulement les &#233;tiquettes
qui peuvent varier, mais aussi les &#233;l&#233;ments &#224; annoter (les marquables ou annotables), puisqu&#8217;elles
ne pr&#233;cisent en rien comment le d&#233;saccord sur les annotables doit &#234;tre trait&#233;. Nous proposons
donc d&#8217;&#233;tendre les &#954; en d&#233;composant l&#8217;accord en un accord sur l&#8217;annotable et un accord sur
l&#8217;&#233;tiquette. De telles mesures n&#233;cessitent donc de conna&#238;tre le nombre d&#8217;annotables M . Ce
nombre d&#8217;annotables est &#233;vident ou connu a priori pour certaines t&#226;ches (comme l&#8217;&#233;tiquetage
morphosyntaxique : tous les tokens sont annotables), mais ne peut &#234;tre qu&#8217;estim&#233; a posteriori pour
des t&#226;ches comme la n&#244;tre (Grouin et al., 2011). Nous proposons pour ce faire une estimation
originale bas&#233;e sur une proc&#233;dure EM (Expectation-Maximization) d&#233;crite dans l&#8217;algorithme 1.
Celui-ci &#233;num&#232;re it&#233;rativement le nombre d&#8217;annotables &#948; (&#233;tape de Maximization) en utilisant la
probabilit&#233; &#947; (estim&#233;e it&#233;rativement) que tous les annotateurs aient manqu&#233; le m&#234;me annotable,
elle-m&#234;me calcul&#233;e gr&#226;ce &#224; l&#8217;estimation du nombre d&#8217;annotables &#948; de l&#8217;it&#233;ration pr&#233;c&#233;dente
(expectation).
</p>
<p>Avoir une estimation la plus exacte possible du nombre d&#8217;annotables est un enjeu d&#8217;importance
pour obtenir des accords inter-annotateurs r&#233;alistes. Par exemple, si l&#8217;on consid&#232;re que tous
</p>
<p>386</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Algorithme 1 Estimation EM des annotables
</p>
<p>Entr&#233;es : {M j} (ensembles des &#233;l&#233;ments annot&#233;s par les annotateurs A j) ; &#948;0 =
&#12;&#12;&#12;&#12;&#12;&#12;&#8899;j M j
</p>
<p>&#12;&#12;&#12;&#12;&#12;&#12;
for (i=1 ; &#948;i 6= &#948;i&#8722;1 ; i++) do
</p>
<p>expectation : &#947;i =
&#8719;
</p>
<p>j P(A j manque un marquable) =
&#8719;
</p>
<p>j
&#948;i&#8722;1&#8722;|M j |
</p>
<p>&#948;i&#8722;1
maximization : &#948;i =
</p>
<p>&#948;0
</p>
<p>1&#8722; &#947;i
end for
return &#948;
</p>
<p>les mots (tokens) des textes sont des annotables (et donc ceux non annot&#233;s sont consid&#233;r&#233;s
annot&#233;s par d&#233;faut par une &#233;tiquette sans-annotation), le Kappa de Cohen pour les accords
intra- et inter-annotateurs atteindrait respectivement 0,9456 et 0,9404, principalement par
l&#8217;abondance des accords sur les tr&#232;s nombreux mots sans-annotation. De telles valeurs masquent
des diff&#233;rences qui sont r&#233;v&#233;l&#233;es avec l&#8217;estimation plus r&#233;aliste des annotables que nous proposons
(voir sous-section 3.2).
</p>
<p>Les deux &#954;, tels que nous les avons impl&#233;ment&#233;s, sont aussi tr&#232;s stricts, puisque la moindre
diff&#233;rence dans les annotations (&#233;tiquette bien s&#251;r, mais aussi d&#233;limitation des entit&#233;s) est
consid&#233;r&#233;e comme un d&#233;saccord. Quand cela est possible, nous fournissons donc &#233;galement
la mesure d&#8217;accord entropique impl&#233;ment&#233;e dans GLOZZ (Mathet et Widl&#246;cher, 2011) ; celle-ci
autorise en effet les correspondances partielles d&#8217;annotation et fournit donc des valeurs d&#8217;accord
prenant en compte ces accords partiels. Elle ne s&#8217;applique cependant pas encore aux relations.
</p>
<p>3.2 Accords inter-annotateurs
</p>
<p>Le tableau 3 pr&#233;sente l&#8217;accord inter- et intra-annotateur, selon la modalit&#233;, calcul&#233;s avec le &#954; de
Cohen, et, &#224; des fins de comparaison, la mesure d&#8217;entropie de GLOZZ. Le &#954; de Carletta a &#233;galement
&#233;t&#233; calcul&#233; et est tr&#232;s proche dans la quasi-totalit&#233; des cas au &#954; de Cohen ; nous ne reportons donc
pas ses valeurs par manque de place. Cette proximit&#233; signifie qu&#8217;il n&#8217;y a pas de biais d&#8217;annotateur :
les distributions des annotations produites par chacun des annotateurs sont tr&#232;s similaires
(Artstein et Poesio, 2008). On constate sans surprise que l&#8217;accord (aussi bien inter- qu&#8217;intra-
annotateur) a tendance &#224; &#234;tre plus faible dans les transcriptions que dans les minutes, &#224; l&#8217;exception
d&#8217;une transcription pour laquelle les unit&#233;s/actions ont produit un accord bien sup&#233;rieur (pr&#232;s de
0,65). Cette tendance g&#233;n&#233;rale se manifeste sp&#233;cialement dans les cas d&#8217;annotations complexes
comme les relations. Les sp&#233;cificit&#233;s de l&#8217;oral mentionn&#233;es pr&#233;c&#233;demment, et en particulier le
style elliptique propre aux commentaires, expliquent facilement cette diff&#233;rence.
</p>
<p>Si le calcul d&#8217;accord inter-annotateurs est devenu une bonne pratique standard du d&#233;veloppement
de ressources annot&#233;es, nous souhaitons promouvoir dans cet article l&#8217;int&#233;r&#234;t d&#8217;une analyse plus
d&#233;taill&#233;e. Cela est d&#8217;autant plus important quand les &#233;l&#233;ments annot&#233;s rel&#232;vent de cat&#233;gories
diff&#233;rentes et que ces cat&#233;gories elles-m&#234;mes ont des populations tr&#232;s diff&#233;rentes, comme c&#8217;est
le cas ici. En effet, les valeurs pr&#233;sent&#233;es pr&#233;c&#233;demment masquent des disparit&#233;s importantes
entre cat&#233;gories d&#8217;annotation. Dans le tableau 4, colonnes 2 et 5, nous d&#233;veloppons les r&#233;sul-
tats d&#8217;accord inter-annotateurs par regroupements de cat&#233;gories. Les difficult&#233;s accrues sur les
transcriptions se v&#233;rifient &#224; cette &#233;chelle, mais l&#8217;on constate en outre de tr&#232;s faibles accords pour
</p>
<p>387</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>inter-annotateurs intra-annotateur A1 intra-annotateur A2
&#954; de Cohen Glozz &#954; de Cohen Glozz &#954; de Cohen Glozz
</p>
<p>Minutes unit&#233;s/actions 0,5992 0,7627 0,7531 0,8753 0,7109 0,8519
Minutes relations 0,5707 - 0,6377 - 0,5983 -
</p>
<p>Transcriptions unit&#233;s/actions 0,6234 0,7498 0,7558 0,8327 0,6812 0,8179
Transcriptions relations 0,4345 - 0,4010 - 0,4701 -
</p>
<p>TABLE 3 &#8211; Accords inter-annotateurs et intra-annotateur par modalit&#233;
</p>
<p>Minutes Transcriptions
&#954; Incertitude Gain d&#8217;entropie &#954; Incertitude Gain d&#8217;entropie
</p>
<p>Acteurs 0,9228 0,5 - 0,8974 1,0 -1
Circonstants 0,4827 1,9 49 0,4441 10,0 15
Actions arbitrales 0,5999 4,3 - 0,5082 19,7 7
Actions autres 0,3240 1,3 92 0,1407 9,8 26
Relations arbitrales 0,6355 10,7 - 0,4520 18,4 8
Relations autres 0,5540 10,2 8 0,3793 69,9 23
</p>
<p>TABLE 4 &#8211; Accords inter-annotateurs par modalit&#233; et par famille d&#8217;annotations
</p>
<p>certaines cat&#233;gories. Les accords sur les entit&#233;s offrent un grand contraste entre les annotations
des acteurs et des circonstants, davantage sujets &#224; interpr&#233;tations. De la m&#234;me mani&#232;re, les &#233;v&#233;-
nements (actions ou relations) sanctionn&#233;s par une action de l&#8217;arbitre obtiennent des accords bien
sup&#233;rieurs aux autres &#233;v&#233;nements. Un examen d&#233;taill&#233; des r&#233;sultats montre que les annotateurs
sont rarement en d&#233;saccord sur les types des &#233;l&#233;ments annot&#233;s, mais qu&#8217;ils annotent des &#233;l&#233;ments
diff&#233;rents. Ce dernier point justifie d&#8217;autant plus l&#8217;emploi de notre technique d&#8217;estimation des
annotables et explique pourquoi la d&#233;finition standard des &#954; sur-estime tant l&#8217;accord.
</p>
<p>3.3 Incertitudes
</p>
<p>Les annotateurs avaient la possibilit&#233; d&#8217;indiquer les annotations leur posant probl&#232;me, pour
quelque raison que ce soit, &#224; l&#8217;aide d&#8217;un champ Incertitude. Ces incertitudes permettent, lors de
la campagne, de pr&#233;ciser les instructions d&#8217;annotations, de comprendre certaines annotations
lors de l&#8217;utilisation du corpus, mais aussi d&#8217;aider &#224; l&#8217;analyse automatique des r&#233;sultats, comme
indicateur de la difficult&#233; d&#8217;annotation. Il est &#224; noter qu&#8217;un seul des annotateurs de la campagne
a v&#233;ritablement utilis&#233; les incertitudes, mais de mani&#232;re syst&#233;matique.
</p>
<p>Dans les colonnes 3 et 6 du tableau 4, nous pr&#233;sentons les taux d&#8217;incertitude par cat&#233;gorie
d&#8217;annotations et par modalit&#233;. On y constate encore une fois que proportionnellement plus
d&#8217;incertitude concerne l&#8217;oral retranscrit (diff&#233;rence statistiquement significative, test de Student
pour deux ensembles, avec p = 0,05).
</p>
<p>Nous nous sommes int&#233;ress&#233;s au lien &#233;ventuel entre incertitude et d&#233;saccord. Nous avons cherch&#233;
&#224; savoir si la pr&#233;sence d&#8217;une incertitude est li&#233;e au d&#233;saccord. Par contre, nous consid&#233;rons
non interpr&#233;table l&#8217;absence d&#8217;incertitude. Pour ce faire, nous avons calcul&#233; la diff&#233;rence entre
l&#8217;entropie de l&#8217;accord H(Acc) (eqn 2) de la variable al&#233;atoire Acc indiquant s&#8217;il y a accord ou
non (DAcc = {vrai ; faux}) et l&#8217;entropie conditionnelle de l&#8217;accord sachant qu&#8217;une incertitude est
</p>
<p>388</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>pr&#233;sente (H(Acc|Inc = pr&#233;sent), eqn 2). Un gain positif signifie que l&#8217;incertitude aide &#224; discerner
les accords des d&#233;saccords. Autrement dit, pour une cat&#233;gorie donn&#233;e, un gain positif indique
que l&#8217;incertitude peut aider &#224; pr&#233;dire les cat&#233;gories susceptibles de d&#233;saccord.
</p>
<p>H(Acc) =&#8722;&#8721;v&#8712;DAcc P(Acc = v) log P(Acc = v) (1)
H(Acc|Inc = vrai) =&#8722;&#8721;v&#8712;DAcc P(Acc = v|Inc = vrai) log P(Acc = v|Inc = vrai) (2)
</p>
<p>Ces gains sont indiqu&#233;s en colonnes 4 et 7 du tableau 4 pour les familles d&#8217;annotation (dans
trois cas, il n&#8217;y a pas assez d&#8217;incertitudes pour les calculer). &#192; une exception pr&#232;s, ils sont tous
positifs, ce qui signifie que ces incertitudes sont des bons indicateurs d&#8217;erreurs, m&#234;me si elles
n&#8217;ont &#233;t&#233; pos&#233;es que par un seul annotateur. Que ce soit pour les minutes ou les transcriptions, il
faut remarquer que le gain est d&#8217;autant plus fort que le taux de d&#233;saccord est important. L&#8217;&#233;tude
des causes de ces incertitudes est donc une piste privil&#233;gi&#233;e pour la correction syst&#233;matis&#233;e des
d&#233;saccords et donc des &#233;ventuelles erreurs d&#8217;annotation.
</p>
<p>4 Conclusion et perspectives
</p>
<p>L&#8217;analyse de la campagne d&#8217;annotation pr&#233;sent&#233;e dans cet article 3 a mis en exergue diff&#233;rents
&#233;l&#233;ments. D&#8217;un point de vue m&#233;thodologique, notre technique d&#8217;estimation des annotables doit
permettre un calcul d&#8217;accord inter-annotateurs plus r&#233;aliste dans les cas o&#249; leur nombre peut
varier selon l&#8217;annotateur. Nous avons aussi montr&#233; que les bonnes pratiques ne sauraient se
limiter &#224; un calcul d&#8217;accord inter-annotateurs unique pour l&#8217;ensemble des annotations quand
celles-ci rel&#232;vent de cat&#233;gories diff&#233;rentes et d&#8217;effectifs non &#233;quilibr&#233;s. Enfin, nous avons montr&#233;
que l&#8217;&#233;tude statistique des incertitudes met au jour une possibilit&#233; de d&#233;tecter syst&#233;matiquement
les d&#233;saccords ou erreurs potentiels. Ces diff&#233;rentes analyses nous ont aussi permis de montrer
que le co&#251;t d&#8217;annotation des textes issus de l&#8217;oral est moindre que pour ceux issus de l&#8217;&#233;crit,
du fait de la diff&#233;rence de densit&#233; des annotations. En revanche, les indicateurs de qualit&#233;
(d&#233;saccord, incertitudes) indiquent sans ambigu&#239;t&#233; la difficult&#233; accrue de traiter de l&#8217;oral. Les
annotations seront librement disponibles sous licence LGPL-LR &#224; http://www.irisa.fr/
texmex/people/claveau/corpora/FootQuaero/ d&#232;s que les corrections identifi&#233;es auront
&#233;t&#233; effectu&#233;es. Le guide d&#8217;annotation mis &#224; jour sera lui-aussi fourni.
</p>
<p>En suite de ce travail, et aussi bien d&#8217;un point de vue th&#233;orique que pratique, nous souhaitons
d&#233;velopper des approches permettant de propager automatiquement des corrections d&#8217;annota-
tions &#224; partir de quelques corrections apport&#233;es &#224; une petite quantit&#233; de donn&#233;es. Ces approches
s&#8217;appuieraient d&#8217;une part sur les analyses pr&#233;c&#233;dentes pour d&#233;tecter les cat&#233;gories les plus pro-
bl&#233;matiques, et &#233;ventuellement sur des approches d&#8217;apprentissage artificiel pour proposer des
corrections.
</p>
<p>3. Nous remercions chaleureusement les annotateurs de la campagne, C. Ris et A. Z&#233;rouki, de l&#8217;INIST-CNRS, pour leur
travail minutieux et leurs pr&#233;cieux retours. Nous remercions &#233;galement V. Lux et A.-R. Ebadat pour leur participation &#224; la
pr&#233;paration de la campagne, et Technicolor pour la mise &#224; disposition d&#8217;une partie des donn&#233;es. Ce travail a &#233;t&#233; r&#233;alis&#233;
dans le cadre du programme Qu&#230;ro (http://www.quaero.org), financ&#233; par OSEO, agence nationale de valorisation
de la recherche.
</p>
<p>389</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>R&#233;f&#233;rences
</p>
<p>ARTSTEIN, R. et POESIO, M. (2008). Inter-Coder Agreement for Computational Linguistics.
Computational Linguistics, 34(4):555&#8211;596.
BARRAS, C., GEOFFROIS, E., WU, Z. et LIBERMAN, M. (1998). Transcriber: a free tool for segmenting,
labeling and transcribing speech. In Actes de First International Conference on Language Resources
and Evaluation (LREC 1998), Grenade, Espagne.
BONNEAU-MAYNARD, H., ROSSET, S., AYACHE, C., KUHN, A. et MOSTEFA, D. (2005). Semantic
annotation of the french media dialog corpus. In Actes de InterSpeech, Lisbonne, Portugal.
CARLETTA, J. (1996). Assessing Agreement on Classification Tasks: the Kappa Statistic. Compu-
tational Linguistics, 22:249&#8211;254.
COHEN, J. (1960). A coefficient of agreement for nominal scales. Educational and Psychological
Measurement, 20(1):37&#8211;46.
DANDAPAT, S., BISWAS, P., CHOUDHURY, M. et BALI, K. (2009). Complex Linguistic Annotation -
No Easy Way Out ! A Case from Bangla and Hindi POS Labeling Tasks. In Proceedings of the
third ACL Linguistic Annotation Workshop, Singapour.
FORT, K. et CLAVEAU, V. (2012). Annotating football matches: : Influence of the source medium
on manual annotation. In Actes de Eighth International Conference on Language Resources and
Evaluation (LREC 2012), Istanbul, Turquie.
FORT, K., NAZARENKO, A. et RIS, C. (2011). Corpus linguistics for the annotation manager. In
Actes de Corpus Linguistics, Birmingham, Angleterre.
GASIGLIA, N. (2003). Pistes m&#233;thodologiques pour l&#8217;exploration d&#8217;un corpus &#224; haut rendement
relatif au parler du football, une langue de sp&#233;cialit&#233; de grande diffusion. In 3es journ&#233;es de
linguistique de corpus. Centre de Recherche en Litt&#233;rature, Linguistique et Civilisation (CRELLIC),
Universit&#233; de Bretagne-Sud, Lorient.
</p>
<p>GROUIN, C., ROSSET, S., ZWEIGENBAUM, P., FORT, K., GALIBERT, O. et QUINTARD, L. (2011). Proposal
for an extension of traditional named entities: from guidelines to evaluation, an overview. In
Actes de 5th Linguistic Annotation Workshop, pages 92&#8211;100, Portland, Oregon, USA. Association
for Computational Linguistics.
</p>
<p>GUT, U. et BAYERL, P. S. (2004). Measuring the reliability of manual annotations of speech
corpora. In Actes de Speech Prosody, pages 565&#8211;568, Nara, Japon.
MATHET, Y. et WIDL&#214;CHER, A. (2011). Une approche holiste et unifi&#233;e de l&#8217;alignement et de la
mesure d&#8217;accord inter-annotateurs. In Actes de Traitement Automatique des Langues Naturelles
2011 (TALN 2011), Montpellier, France.
NEMRAVA, J., SVATEK, V., SIMUNEK, M. et BUITELAAR, P. (2007). Mining over: football match data:
seeking associations among explicit and implicit events. In Proc. of Znalosti 2007.
SCHMIDT, T. (2008). The Linguistics of Football (Language in Performance 38), volume 38, chapitre
The Kicktionary: Combining corpus linguistics and lexical semantics for a multilingual football
dictionary, pages 11&#8211;23. Gunter Narr, T&#252;bingen, Allemagne.
</p>
<p>VOORMANN, H. et GUT, U. (2008). Agile corpus creation. Corpus Linguistics and Linguistic Theory,
4(2):235&#8211;251.
</p>
<p>WIDL&#214;CHER, A. et MATHET, Y. (2009). La plate-forme Glozz : environnement d&#8217;annotation et
d&#8217;exploration de corpus. In Actes de Traitement Automatique des Langues 2009 (TALN 2009),
Senlis, France.
</p>
<p>390</p>

</div></div>
</body></html>