Actes de la conférence conjointe JEP-TALN-RECITAL 2012, volume 2: TALN, pages 399–406,
Grenoble, 4 au 8 juin 2012. c©2012 ATALA & AFCP
Le Lexicoscope : un outil pour l'étude de profls 
combinatoires et l'extraction de constructions 
lexico-syntaxiques
Olivier Kraif 1, Sascha Diwersy 2
(1) LIDILEM, Université Stendhal Grenoble 3, BP 25, 38040 Grenoble Cedex
(2) Université de Cologne
olivier.kraif@u-grenoble3.fr, sascha.diwersy@uni-koeln.de
RÉSUMÉ_________________________________________________________________________
Dans  le  cadre  du  projet  franco-allemand  Emolex,  dédié  à  l'étude  contrastive  de  la 
combinatoire du lexique des émotions en 5 langues, nous avons développé des outils et 
des  méthodes  permettant  l'extraction,  la  visualisation  et  la  comparaison  de  profls 
combinatoires  pour  des  expressions  simples  et  complexes.  Nous  présentons  ici 
l'architecture d'ensemble de la plate-forme, conçue pour efectuer des extractions sur des 
corpus de grandes dimensions (de l'ordre de la centaine de millions de mots) avec des 
temps  de  réponse  réduits  (le  corpus  étant  interrogeable  en  ligne1).  Nous  décrivons 
comment nous avons introduit  la notion de pivots  complexes,  afn de permettre aux 
utilisateurs de rafner progressivement leurs requêtes pour caractériser des constructions 
lexico-syntaxiques  élaborées.  Enfn,  nous donnons les  premiers  résultats  d'un module 
d'extraction automatique d'expressions polylexicales récurrentes.
ABSTRACT_______________________________________________________________________
The Lexicoscope  :  an integrated tool  for combinatoric  profles  observation and 
lexico-syntactic constructs extraction.
The German-French research project Emolex whose aim is the contrastive study of the 
combinatorial behaviour of emotion lexemes in 5 languages has led to the development 
of  methods  and  tools  to  extract,  display  and  compare  the  combinatorial  profles  of 
simple and complex expressions. In this paper, we present the overall architecture of the 
query  platform  which  has  been  conceived  to  ensure  efcient  processing  of  huge 
annotated text corpora (consisting of several hundred millions of word tokens) accessible 
through a web-based interface. We put forward the concept of “complex query nodes” 
introduced  to  enable  users  to  carry  out  progressively  elaborated  extractions  of 
lexical-syntactic patterns. We fnally give primary results of an automated method for 
the retrieval of recurrent multi-word expressions, which takes advantage of the complex 
query nodes implementation.
MOTS-CLÉS : collocations,  cooccurrences,  profl  combinatoire,  expressions  polylexicales, 
lexique des émotions.
KEYWORDS : collocations, combinatorial profles, multi-word expressions.
1 L'accès au corpus sera rendu public, moyennant authentifcation, d'ici quelques mois.
399
1 Introduction
Cette  communication  présente  des  travaux  réalisés  dans  le  cadre  du  projet  Emolex,  
projet franco-allemand cofnancé par l'ANR et la DFG. Dans le cadre de cette recherche, 
nous avons rassemblé des corpus massifs comportant plusieurs centaines de millions de 
mots pour 5 langues diférentes (l'allemand, le français, l'anglais, l'espagnol et le russe). 
L'objectif du projet est d'analyser, dans une perspective formulée par Sinclair (2004) ou 
encore Hoey (2005) et d'un point de vue contrastif, les valeurs sémantiques et les rôles 
discursifs  à  partir  de  la  combinatoire  du  lexique  des  émotions,  afn  d'élaborer  une 
cartographie permettant de mieux structurer ce champ lexical, avec des applications en 
lexicographie mais aussi en didactique des langues et traductologie. Cette étude porte 
plus précisément sur le développement d'une approche automatisée permettant de guider 
l'observation linguistique par l'extraction de cooccurrences autour d'un pivot.
2 Un modèle de cooccurrence fexible
Pour  caractériser  le  profl  combinatoire  d'une  entrée,  nous  reprenons  le  concept  de 
lexicogramme, introduit par Maurice Tournier et repris dans le logiciel WebLex (Heiden, 
Tournier 1998) : il s'agit d'établir, pour un pivot donné, la liste de ses cooccurrents les 
plus  fréquents,  à  gauche  et  à  droite,  en  faisant  l'extraction  des  fréquences  de 
cooccurrence et en calculant des mesures d'association statistiques (telles que rapport de 
vraisemblance  ou  t-score).  Pour  construire  ces  lexicogrammes,  nous  proposons  un 
modèle de cooccurrence fexible permettant à l'utilisateur de défnir lui-même les unités  
de cooccurrences  :   formes,  lemmes, catégories  morphosyntaxiques,  traits  additionnels 
(p.ex.  sémantiques),  relations  syntaxiques  (dans  le  cas  des  colligations)  ou  des 
combinaisons de ces informations. La possibilité de faire intervenir des combinaisons de 
ses traits nous semble importante pour permettre à l'utilisateur d'ajuster la focale de ses  
observations  en  allant  du  général  au  particulier  (ou  vice-versa),  de  préciser  des 
contraintes pour désambiguïser certains contextes, et de combiner les aspects lexicaux et 
syntaxiques  dans  ses  observations.  Par  ailleurs  nous  proposons  également  une 
caractérisation  fexible  de  l'espace  de  cooccurrence,  qui  conditionne  les  points  de 
rencontre entre pivot et collocatifs, ainsi que la manière de les dénombrer. On peut par 
exemple défnir la cooccurrence à l'intérieur d'un empan de largeur fxe, éventuellement 
diférente à droite et à gauche du pivot. Mais on peut aussi rechercher la  cooccurrence  
syntaxique, à l'instar de Kilgarif et Tugwell (2001) ou Charest et al. (2010), mise en jeu 
lorsqu'une relation fonctionnelle (du type sujet, complément d'objet, modifeur, etc.) a 
été identifée entre deux unités. Evert (2007), signale l'intérêt de ce type de cooccurrence 
en terme de bruit et de silence : "(...)  unlike surface cooccurrence, it does not set an 
arbitrary  distance  limit,  but  at  the  same  time  introduces  less  “noise”  than  textual 
cooccurrence".  Pour  la  cooccurrence  syntaxique,  nous  exploitons  les  relations  de 
dépendances obtenues grâce à diférents analyseurs : XIP pour l'anglais (Aït-Mokhtar et 
al. 2001), Connexor pour l'allemand, le français et l'espagnol (Tapanainen & Järvinen 
1997), DeSR pour le russe (Attardi et al. 2007), basé sur un modèle stochastique créé à 
partir  du  corpus arboré SyntagRus (Nivre  et  al., 2008).  Un post-traitement a  permis 
d'harmoniser  et  de  standardiser  l'annotation  des  relations  de  dépendance  entre  les 
400
langues (l'annotation de Connexor ayant servi de référence). Nous avons par la suite 
complété  ces  relations  pour  obtenir  des  dépendances  plus  pertinentes  sur  le  plan 
sémantique (p. ex. sujet profond dans les constructions passives, etc.).
Avec le modèle de cooccurrence ainsi défni, on peut viser des aspects très génériques de 
la combinatoire (par exemple : quels sont les principaux collocatifs de la forme surprise 
toutes relations confondues) ou beaucoup plus spécifques et circonscrits (par exemple : 
quels sont les principaux collocatifs verbaux à l'imparfait du nom lemmatisé surprise en 
tant qu'objet direct). Le tableau 1 montre un tel lexicogramme :
l1 l2 f f1 f2 loglike
surprise_N créer_V 614 2098 21658 4548,43
surprise_N réserver_V 230 2098 2869 2143,50
surprise_N avoir_V 484 2098 423602 627,50
surprise_N constituer_V 94 2098 13778 406,80
surprise_N éviter_V 43 2098 16296 109,30
surprise_N manifester_V 22 2098 2424 106,62
surprise_N causer_V 19 2098 2210 90,06
surprise_N ménager_V 15 2098 1495 75,58
surprise_N exprimer_V 23 2098 6186 72,54
surprise_N provoquer_V 23 2098 10551 50,61
surprise_N feindre_V 9 2098 676 50,31
TABLEAU 1 : extrait du lexicogramme pour le nom lemmatisé surprise pris en tant qu'objet 
direct (f=fréquence de cooccurrence, f1=fréquence de l1, f2=fréquence de l2)
3 Visualisations comparatives
A partir de ces lexicogrammes, nous ofrons diférentes modalités d'exploration : 
– pour l'analyse linguistique, le "retour au texte" est indispensable : un simple clic sur 
un collocatif permet de retrouver, sous forme de concordance, tous les contextes de 
cooccurrence avec le pivot.
– pour comparer de manière synthétique divers profls combinatoires, nous proposons 
d'identifer les lexicogrammes à des points dans un espace vectoriel, en ne retenant 
que la mesure jugée la plus pertinente (fréquence, loglike, t-score, etc.). Il est dès 
lors  possible  d'utiliser  des  méthodes  d'analyse  de  données  pour  visualiser  les 
similarités  entre  pivots  :  analyse  factorielle  des  correspondances  (AFC), 
échelonnement multidimensionnel (MDS) ou classifcation hiérarchique ascendante 
(hClust). La fgure 1 montre ces sorties pour des unités du domaine sémantique de la 
'colère' (obtenues grâce aux modules du projet 'GNU R').  La classifcation, réalisée 
pour la relation "objet", indique une hiérarchisation assez bien corrélée à l'intensité 
du  sentiment.  Quant  à  la  'factor  map',  réalisée  pour  des  relations  quelconques 
401
concernant  des  collocatifs  adjectivaux,  elle  permet  de  distinguer  trois  groupes  : 
révolte, indignation - souvent lié à la sphère publique et politique ; fureur, rage, colère - 
lié à l'expression ponctuelle et plus ou moins intense de l'afect ; enfn énervement, 
irritation,  exaspération - qui concernent plutôt des états émotionnels précurseurs de 
cette  manifestation.  Ces  cas  montrent  de  façon assez  éclairante  le  lien  entre  les 
valeurs sémantiques et la combinatoire lexico-syntaxique.
FIGURE 1 : Classifcation hiérarchique et AFC (domaine sémantique de la 'colère')
4 Architecture logicielle
Comment répondre rapidement à une requête d'utilisateur lorsqu'on interroge des corpus 
contenant des centaines de millions d'occurrences ? La réponse est simple,  a priori  : 
grâce à une indexation préalable des unités et des cooccurrences. Mais la difculté de 
notre système tient au fait que ni les unités, ni l'espace de cooccurrence ne sont défnis à 
l'avance  :  on  peut  interroger  des  lemmes,  des  formes,  des  combinaisons 
lemmes-catégories,  et  toute  combinaison  de  forme,  lemme,  catégorie  et  traits  (ces 
derniers pouvant être caractérisés par des expressions régulières). En outre, l'espace de 
cooccurrence est établi dynamiquement, au moment de la requête, par des expressions 
régulières défnissant l'ensemble des relations à prendre en compte.
Pour répondre à la double exigence de fexibilité et d'efcacité, nous avons élaboré une 
indexation  multi-niveaux, sous la forme de hachages  de hachages  sérialisés  :  chaque 
forme pointe vers l'ensemble des lemmes correspondants ; chaque lemme pointe vers 
l'ensemble de ses catégories possibles (dans le corpus) ; chaque lemme-catégorie pointe 
vers l'ensemble des traits associés (dans le corpus) ; chaque lemme-catégorie-traits pointe 
vers l'ensemble des  relations associées ;  chaque lemme-catégorie-traits-relation pointe 
vers  un  ensemble  de  paires  (collocatif,fréquence).  Les  expressions  régulières  liées  au 
contraintes portant sur les catégories, traits et relations sont appliquées lors du parcours  
de l'index. Les ensembles de catégories, traits et relations étant réduits (et fermés) et le 
temps de recherche dans le hachage étant en O(1), la succession de ces recherches n'est 
402
pas très couteuse.
En ce qui concerne l'implémentation, nous avons opté pour le langage Perl, pour son 
traitement très efcace des expressions régulières. Pour les index, nous avons testé deux 
systèmes  de  bases  de  données  réputés  pour  leur  efcacité  :  BerkeleyDB  5.1.252 et 
KyotoCabinet 1.2.483. Les résultats du tableau 3 montrent que le système qui est apparu 
le plus efcace pour nos requêtes était celui de KyotoCabinet::BTree.
Corpus presse (fr) 2007-
2008 (87 807 463 tokens)
Taille 
des 
index
test 2
1 pivot
test 2
5 pivots
test 3
24 pivots
BerkeleyDB:Hash 1800 Mo 125 s. / 1,3 s. 275 s. / 206 s. 892 s. / 766 s.
KyotoCabinet::Hash 1200 Mo 50 s. / 1 s. 376 s. / 180 s. 749 s. / 702 s.
KyotoCabinet::Btree 955 Mo 76 s. / 1.5 s. 247 s./ 231 s. 416 s. / 315 s.
TABLEAU 2 : comparaison des tailles et des temps de réponse pour diférents types de DBM 
(le 2ème temps est obtenu lorsqu'une requête est immédiatement réitérée).
Ces temps sont donnés à titre de comparaison : ils ont été obtenus sur un PC ancien et 
assez lent. Sur notre matériel actuel (Intel Core2 Quad CPU Q9550  2.83GHz, avec 4Go 
de RAM) nous obtenons des temps environ 4 fois supérieurs. La diférence importante 
entre le 1er et le 2ème temps indique que ce sont les accès disques qui pénalisent les  
traitements, car lorsque la DBM est en cache, la réponse est presque instantanée. En 
utilisant un disque SSD ultra-rapide, nous prévoyons d'améliorer les temps de réponse de 
manière drastique.
5 Prise en compte des pivots multimots
L'aspect exclusivement binaire des relations de dépendance directe peut aboutir à un 
rétrécissement  du  contexte  des  observations  et  faire  manquer  des  phénomènes 
intéressants  sur  le  plan  phraséologique.  Ces  limitations  empêchent  notamment 
l’extraction automatique de séquences polylexicales à valeur d’unité minimale de sens 
(les  «  meaning  units  »  selon  Sinclair  2004),  qui  peuvent  présenter  une  variabilité 
considérable sur le plan de l’expression.
Cependant, en ce qui concerne les « collocations lexicales », Tutin (2008) afrme que la 
plupart d'entre elles ont une structure binaire, même pour celles qui s'étendent à plus de 
deux  éléments,  car  elles  correspondent  sémantiquement  à  une  structure 
prédicat-argument : "Collocations can be considered as predicate-argument structures, and as  
such,  are  prototypically  binary  associations,  where  the  predicate  is  the  collocate  and  the  
argument is the base. Most ternary (and over) collocations are merged collocations (collocational  
clusters) or recursive collocations."
Et en efet, de nombreux travaux dédiés à l'extraction de collocations étendues à plus de 
deux  mots  se  basent  en  fait  sur  des  modèles  binaires,  appliqués  à  deux  éléments 
composés : collocation d'arbres syntaxiques (Charest et al., 2010), construction itérative 
2http://www.oracle.com/technetwork/database/berkeleydb/overview/index.html
3http://fallabs.com/kyotocabinet/
403
de cooccurrence multimots à partir de cooccurrences binaires (Seretan et al., 2003), ou 
encore  calcul  de  mesure  d'association  multimots  en  combinant  des  mesures  à  deux 
termes.
De la  même manière,  il  est  possible  d'étendre  notre  architecture  pour  le  calcul  des 
lexicogrammes d'un pivot donné, en la généralisant à des confgurations plus complexes : 
la solution consiste  à  défnir  le  pivot  non plus  seulement à  partir  d'une forme prise 
isolément, mais comme une forme associée à un certain contexte lexico-syntaxique. Une fois 
déterminé  ce  contexte,  il  est  possible  de  calculer  le  tableau  de  contingence  comme 
précédemment, le pivot et son contexte formant en quelque sorte une nouvelle unité 
pour laquelle il est possible de calculer à la fois les fréquences de cooccurrence (en se 
basant sur les relations du pivot) et la fréquence marginale dans le corpus.
Pour l'écriture des contextes, nous utilisons le formalisme de méta-expressions régulières 
proposé par Kraif (2008).  Par exemple, pour rechercher le pattern V+ DET(poss.) + 
admiration_N + POUR, nous défnissons le contexte suivant : 
pivot : #1= admiration#N
contexte : <l=son> <#1> && <#2> && <pour,#3>::(.*,#1,#2)(.*,#2,#3)
Le calcul est seulement un peu plus long à mettre en œuvre, car les pivots multimots  
n'étant pas connus a priori, il n'est pas possible de les indexer tels quels. Seuls les tokens  
(formes ou lemmes) composant le contexte, ainsi que les relations de dépendances entre 
deux tokens défnis, sont indexés, ce qui permet de réduire signifcativement l'ensemble 
des phrases à analyser. Pour des expressions comportant plusieurs relations, comme c'est 
l'intersection des phrases indexées pour chaque relation qui est retenue, la recherche est  
plus rapide : en d'autres termes, plus un pivot complexe est long, plus sa recherche est 
rapide. Dans le tableau 3 ci-dessous, on constate que pour le contexte donné en exemple, 
la mesure du log-likelihood fait clairement ressortir les verbes  cacher et  dissimuler, qui 
correspondent tous deux à la même construction stéréotypée : X ne pas cacher/dissimuler 
son admiration pour Y.
l1 l2 f f1 f2 N loglike
admiration_N cacher_V 4 14 527 544994 38,83
admiration_N dissimuler_V 2 14 107 544994 22,70
admiration_N proclamer_V 2 14 176 544994 20,70
admiration_N exprimer_V 2 14 642 544994 15,53
admiration_N redire_V 1 14 76 544994 10,57
admiration_N manifester_V 1 14 193 544994 8,70
admiration_N confier_V 1 14 1319 544994 4,91
TABLEAU 3 - extrait de lexicogramme pour le pivot multimot son admiration pour pris en 
tant qu'objet direct
Ainsi conçue, l'extraction des lexicogrammes pour les pivots multimots se veut surtout 
être un outil d'observation permettant aux utilisateurs, par complexifcation progressive, 
de  mieux  préciser  le  contexte  des  phénomènes  qui  les  intéressent  (comme  ici  en 
précisant la détermination ou la structure prépositionnelle).
Cette  approche  qui  va  du  simple  vers  le  complexe  peut  néanmoins,  d'une  certaine 
404
manière, s'automatiser. Partant d'un pivot simple, on peut retenir ses collocatifs les plus 
saillants pour former de nouveaux pivots multimots. Et l'on peut réitérer l'opération de 
manière récursive sur les nouveaux pivots, jusqu'à une taille limite fxée arbitrairement. 
Nous  avons  implémenté  ce  processus  jusqu'à  une taille  maximale  de  5  mots,  en  ne 
retenant, à chaque itération, que les candidats à l'extension qui cooccurrent au moins 3 
fois et pour lesquelles la valeur de loglike sont supérieure à 10. Ne sont retenus que les 
pivots multimots maximaux (de 5 mots) ou qui ne peuvent être étendus par un pivot 
multimot plus long.
Dans l'exemple ci-dessous, pour mieux cibler l'extraction autour du nom admiration, nous 
avons imposé que le premier collocatif soit issu de la relation d'objet direct (on trouve 
donc, pour commencer, un verbe). Voici les résultats obtenus, sans fltrage, pour les 3 
verbes les plus saillants.
1 : précision_N qui_PRON forcer_V la_DET admiration_N 
2 : précision_N forcer_V la_DET admiration_N 
3 : vouer_V une_DET admiration_N sans_PREP borne_N 
4 : vouer_V une_DET profond_A  admiration_N
5 : vouer_V une_DET grand_A admiration_N 
6 : il_PRON vouer_V une_DET grand_A admiration_N 
7 : qui_PRON vouer_V une_DET admiration_N 
8 : qui_PRON pas_ADV cacher_V son_PRON admiration_N 
9 : ne_ADV pas_ADV cacher_V son_PRON admiration_N 
10 : avoir_V cacher_V son_PRON  admiration_N
11 : il_PRON pas_ADV cacher_V son_PRON admiration_N 
12 : qui_PRON ne_ADV cacher_V pas_ADV admiration_N
Comme souvent dans les extractions d'expressions  multimots,  on trouve un ensemble 
d'expressions de natures diverses (collocations simples, collocations récursives, locutions, 
etc.), avec notamment des fragments incomplets d'expressions plus larges (cf. exemple. 
2) ou des expressions qui agrègent des éléments de contexte non pertinent (cf. exemple 
10, avec avoir). On obtient cependant, et ceci de façon assez précise, des constructions 
récurrentes et stéréotypées caractéristiques de la combinatoire du nom admiration pris en 
tant qu'objet.
6 Conclusion
Nous avons présenté un nouvel outil d'exploration de la combinatoire lexico-syntaxique, 
que nous avons baptisé le lexicoscope. Cet outil s'appuie sur un modèle de cooccurrence 
fexible permettant  à  l'utilisateur de défnir  lui  même les  unités  qui  l'intéressent  (en 
combinant forme, lemme, catégorie et traits) ainsi que l'espace de cooccurrence visé (en 
précisant  les  relations  de  dépendance  concernées).  Le  lexicoscope  permet  en  outre 
d'efectuer  des  comparaisons  des  profls  combinatoires,  synthétisés  sous  la  forme  de 
lexicogrammes, et propose en sortie des visualisations du type AFC, MDS ou hClust.
Enfn, pour permettre à l'utilisateur de ne pas se limiter aux seules dépendances directes 
autour d'un pivot, nous avons ajouté la possibilité de défnir des pivots multimots avec 
leurs contextes syntaxiques. Ce nouvel outil est actuellement à l'essai, dans le cadre des 
405
observations  contrastives  efectuées  pour le  projet  Emolex.  L'interface sera accessible 
pour  le  grand  public  d'ici  quelque  mois  (mais  les  corpus,  qui  sont  soumis  à  des 
restrictions de droits d'auteur, ne pourront être difusés dans leur intégralité). D'ici là,  
nous  pourrons  efectuer  une  analyse  plus  précise  des  possibilités  ofertes  par  le 
lexicoscope pour la comparaison des  profls combinatoires de diférents  pivots,  et  en 
dégager une méthodologie d'observation adaptée. 
7 Références
AÏT-MOKHTAR,  S.,  CHANOD,  J.-P.,  ROUX C. (2002)  “Robustness  beyond  Shallowness: 
Incremental Deep Parsing”, Natural Language Engineering, 8 :121-144.
ATTARDI,  G.,  DELL'ORLETTA,  F.,  SIMI,  M.,  CHANEV,  A.,  CIARAMITA,  M. (2007)  “Multilingual 
Dependency Parsing and Domain Adaptation using DeSR”, In Proc. of the CoNLL Shared  
Task Session of EMNLP-CoNLL 2007, Prague.
CHAREST,S.  BRUNELLE E.,  FONTAINE J. (2010)  Au-delà de la paire de mots  :  extraction de 
cooccurrences syntaxiques multilexémiques, Actes de TALN 2010, Montréal, juillet 2010
EVERT, STEFAN (2007). Corpora and collocations. in A. Lüdeling and M. Kytö (eds.), Corpus  
Linguistics. An International Handbook, article 58. Mouton de Gruyter, Berlin.
Heiden S., Tournier M. (1998) Lexicométrie textuelle, sens et stratégie discursive, actes I  
Simposio Internacional de Análisis del Discurso, Madrid.
HOEY,  M.  (2005) :  Lexical  Priming:  A  New  Theory  of  Words  and  Language,  London, 
Routledge.
KILGARIFF A.,TUGWELL D. (2001)  WORD SKETCH:  Extraction  and  Display  of  Signifcant 
Collocations  for  Lexicography,  Proc  ACL  workshop  on  COLLOCATION  Computational  
Extraction Analysis and Exploitation, Toulouse July 2001.
KRAIF,  O.  (2008)  Comment  allier  la  puissance  du  TAL  et  la  simplicité  d'utilisation  ? 
l'exemple du concordancier bilingue ConcQuest, JADT 2008, PUL, 625-634, vol. 2.
NIVRE, J., BOGUSLAVSKY, I. M., IOMDIN, L. L. (2008) “Parsing the SYNTAGRUS Treebank of 
Russian”,  Proceedings  of  the  22nd  International  Conference  on  Computational  Linguistics 
(Coling 2008), Manchester, August 2008, p. 641–648.
SERETAN V.,  NERIMA L.,  WEHRLI E.  (2003).  Extraction  of  Multi-Word  Collocations  Using 
Syntactic  Bigram  Composition.  Proceedings  of  the  Fourth  International  Conference  on  
Recent Advances in NLP, (RANLP-2003), 424–431.
SINCLAIR,  JOHN MCH.  (2004)  Trust  the  text  :  language,  corpus  and  discourse,  London, 
Routledge.
TAPANAINEN, P., JÄRVINEN, T. (1997) “A non-projective dependency parser”, In Proceedings of  
the 5th Conference on Applied Natural Language Processing, Washington, DC, p. 64-74.
TUTIN A. (2008), For an extended defnition of lexical collocations, Proceedings of Euralex, 
Barcelone 15-19 juillet 2008, Université Pompeu Fabra.
406
