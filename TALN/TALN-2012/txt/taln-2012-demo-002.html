<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
"http://www.w3.org/TR/html4/loose.dtd">
<html><head><title>Interfaces de navigation dans des contenus audio et vid&#233;o</title>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
</head>
<body>
<div style="page-break-before:always; page-break-after:always"><div><p>Actes de la conf&#233;rence conjointe JEP-TALN-RECITAL 2012, volume 5: D&#233;monstrations, pages 3&#8211;4,
Grenoble, 4 au 8 juin 2012. c&#169;2012 ATALA &amp; AFCP
</p>
<p>Interfaces de navigation dans des contenus  
audio et vid&#233;o 
</p>
<p>G&#233;raldine Damnati 
(1)France Telecom, Orange Labs, Lannion 
geraldine.damnati@orange.com 
</p>
<p>RESUME____________________________________________________________________________________________________________  
</p>
<p>Deux types de d&#233;monstrateurs sont pr&#233;sent&#233;s. Une premi&#232;re interface &#224; vis&#233;e didactique 
permet d'observer des traitements automatiques sur des documents vid&#233;o. Plusieurs 
niveaux de repr&#233;sentation peuvent &#234;tre montr&#233;s simultan&#233;ment, ce qui facilite l'analyse 
d'approches multi-vues. La seconde interface est une interface op&#233;rationnelle de 
&quot;consommation&quot; de documents audio. Elle offre une exp&#233;rience de navigation enrichie 
dans des documents audio gr&#226;ce &#224; une visualisation de m&#233;tadonn&#233;es extraites 
automatiquement. 
</p>
<p>ABSTRACT _________________________________________________________________________________________________________  
</p>
<p>Navigation interfaces through audio and video contents 
</p>
<p>Two types of demonstrators are shown. A first interface, with didactic purposes, allows 
automatic processing of video documents to be observed. Several representation levels 
can be viewed simultaneously, which is particularly helpful to analyse the behaviour of 
multi-view approaches. The second interface is an operational audio document 
&quot;consumption&quot; interface. It offers an enriched navigation experience through the 
visualisation of automatically extracted metadata. 
</p>
<p>MOTS-CLES : Traitements multi-vues, navigation enrichie. 
KEYWORDS : Multi-view processing, enriched navigation. 
</p>
<p>1 Interface didactique  
</p>
<p>Il s'agit d'un d&#233;monstrateur qui permet d'illustrer les traitements automatiques r&#233;alis&#233;s 
sur des contenus vid&#233;o. Le principe est de visualiser sous forme de timeline des 
informations de structuration extraites automatiquement. Pour chaque segment, un 
onglet permet de visualiser des informations issues du canal audio (typiquement la 
transcription automatique synchronis&#233;e avec le player) et un onglet permet de visualiser 
des informations li&#233;es au canal vid&#233;o (typiquement des images cl&#233; ou key frames). 
L'interface offre des fonctionnalit&#233;s de navigation d'un segment &#224; l'autre. Au-del&#224; de ces 
fonctionalit&#233;s de base, l'int&#233;r&#234;t de l'outil est de pouvoir cumuler plusieurs timeline et 
observer ainsi l'apport de traitement multi-niveaux. Plusieurs r&#233;sultats de travaux de 
recherche seront montr&#233;s via cette interface. 
</p>
<p>Reconnaissance du r&#244;le du locuteur 
</p>
<p>La capture d'&#233;cran ci-contre repr&#233;sente une analyse en r&#244;le 
des tours de parole dans des Journaux T&#233;l&#233;vis&#233;s. Elle 
illustre une approche multi-vue qui consiste &#224; fusionner 
une analyse purement acoustique mod&#233;lisant l'intonation 
</p>
<p>3</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>des locuteurs en fonction de leur r&#244;le et une analyse purement linguistique bas&#233;e sur une 
analyse de la transcription automatique du contenu parl&#233; (Damnati et Charlet, 2011). 
L'interface permet de visualiser les r&#233;sultats de chacune des analyses ainsi que de leur 
fusion, afin de mieux analyser leur compl&#233;mentarit&#233;.  
</p>
<p>D&#233;tection de personnes dans des documents vid&#233;o 
</p>
<p>Les travaux r&#233;alis&#233;s dans le cadre du d&#233;fi REPERE (B&#233;chet et al., 2012) seront &#233;galement 
montr&#233;s via cette interface. Ce projet a pour but d'identifier les personnes dans des 
contenus t&#233;l&#233;vis&#233;s en exploitant conjointement le canal audio (contenu parl&#233; et analyse 
en locuteurs) et le canal vid&#233;o (texte incrust&#233; et analyse de visages). L'interface permet 
de visualiser les informations extraites dans les diff&#233;rentes modalit&#233;s ainsi que le r&#233;sultat 
de la fusion. 
</p>
<p>2 Interface de navigation enrichie 
</p>
<p>Cette interface &#224; pour vocation de proposer aux utilisateurs une exp&#233;rience de navigation 
enrichie dans des contenus purement audio, en s'appuyant sur des m&#233;tadonn&#233;es 
produites automatiquement. Elle propose en quelque sorte de &quot;visualiser&quot; des contenus 
audio. Elle est d&#233;clin&#233;e &#224; Orange Labs dans diff&#233;rents domaines, allant de la 
consommation de podcast de radio &#224; l'&#233;coute de conversations issues des centres 
d'appels. 
</p>
<p>La capture d'&#233;cran ci-contre illustre une 
interface de visualisation conversations 
client/t&#233;leconseiller, et s'inscrit dans le 
domaine plus large du Speech Analytics. Elle 
permet d'avoir une vue synth&#233;tique du 
d&#233;roul&#233; de la conversation, structur&#233;e en 
locuteurs, une visualisation d'expressions 
cl&#233;s extraites des transcriptions 
automatiques, un filtrage des conversations 
par motif d'appel, etc&#8230; 
</p>
<p> 
</p>
<p>R&#233;f&#233;rences 
</p>
<p>DAMNATI, G., CHARLET, D. (2011). Multi-view approach for speaker turn role labeling in 
TV Broadcast News shows, Proc. Interspeech'11, Florence, 2011.  
</p>
<p>BECHET, F., AUGUSTE, R., AYACHE, S., CHARLET, D., DAMNATI, G., FAVRE, B., FREDOUILLE, C., 
LEVY, C. (2012). Percol0 - un syst&#232;me multimodal de d&#233;tection de personnes dans des 
documents vid&#233;o. Proc. JEP'12, Grenoble, 2012. 
</p>
<p>4</p>

</div></div>
</body></html>