<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
"http://www.w3.org/TR/html4/loose.dtd">
<html><head><title>Vizart3D : Retour Articulatoire Visuel pour l&#8217;Aide &#224; la Prononciation</title>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
</head>
<body>
<div style="page-break-before:always; page-break-after:always"><div><p>Actes de la conf&#233;rence conjointe JEP-TALN-RECITAL 2012, volume 5: D&#233;monstrations, pages 17&#8211;18,
Grenoble, 4 au 8 juin 2012. c&#169;2012 ATALA &amp; AFCP
</p>
<p>Vizart3D : Retour Articulatoire Visuel  pour l&#8217;Aide &#224; la Prononciation  
Thomas Hueber1 Atef Ben-Youssef1  
</p>
<p>Pierre Badin1 G&#233;rard Bailly1 Fr&#233;d&#233;ric Elis&#233;i1  
(1) GIPSA-lab, UMR 5216/CNRS/INP/UJF/U.Stendhal, Grenoble, France  
</p>
<p>(pr&#233;nom.nom)@gipsa-lab.grenoble-inp.fr 
RESUME ____________________________________________________________________________________________________________  
&#8232;L&#8217;objectif du syst&#232;me Vizart3D est de fournir &#224; un locuteur, en temps r&#233;el, et de fa&#231;on 
automatique, un retour visuel sur ses propres mouvements articulatoires. Les 
applications principales de ce syst&#232;me sont l&#8217;aide &#224; l&#8217;apprentissage des langues 
&#233;trang&#232;res et la r&#233;&#233;ducation orthophonique (correction phon&#233;tique). Le syst&#232;me Vizart3D 
est bas&#233; sur la t&#234;te parlante 3D d&#233;velopp&#233;e au GIPSA-lab, qui laisse appara&#238;tre, en plus 
des l&#232;vres, les articulateurs de la parole normalement cach&#233;s (comme la langue). Cette 
t&#234;te parlante est anim&#233;e automatiquement &#224; partir du signal audio de parole, &#224; l&#8217;aide de 
techniques de conversion de voix et  de r&#233;gression acoustico-articulatoire  par GMM. 
ABSTRACT__________________________________________________________________________________________________________ 
Vizart3D: Visual Articulatory Feedack for Computer-Assisted Pronunciation 
Training 
We describe a system of visual articulatory feedback, which aims to provide any speaker 
with a real feedback on his/her own articulation. Application areas are computer-
assisted pronunciation training (phonetic correction) for second-language learning and 
speech rehabilitation. This system, named Vizartd3D, is based on the 3D augmented 
talking head developed at GIPSA-lab, which is able to display all speech articulators 
including usually hidden ones like the tongue. In our approach, the talking head is 
animated automatically from the audio speech signal, using GMM-based voice 
conversion and acoustic-to-articulatory regression.  
MOTS-CLES : retour visuel, aide &#224; la prononciation, GMM, temps r&#233;el, t&#234;te parlante  
KEYWORDS : visual feedback, pronunciation training, GMM, real-time, talking head  
Plusieurs &#233;tudes semblent montrer que fournir &#224; un locuteur un retour visuel sur ses 
propres mouvements articulatoires pouvait s&#8217;av&#233;rer utile pour la r&#233;&#233;ducation 
orthophonique et l&#8217;apprentissage des langues (Badin, 2010). Ce retour visuel peut 
notamment s&#8217;effectuer via une t&#234;te parlante augment&#233;e, c&#8217;est-&#224;-dire un clone orofacial 
virtuel qui laisse appara&#238;tre l&#8217;ensemble des articulateurs, externes (l&#232;vres, m&#226;choire) 
comme internes (langue, voile du palais). Dans  
(Engwall, 2008), Engwall propose un paradigme exp&#233;rimental du type &#171; magicien d&#8217;Oz &#187; 
pour montrer l&#8217;efficacit&#233; d&#8217;une telle approche (syst&#232;me ARTUR): un phon&#233;ticien expert 
&#233;value la nature du d&#233;faut de prononciation du sujet, et lui fait visualiser le geste 
articulatoire cible en s&#233;lectionnant l&#8217;animation ad&#233;quate parmi un ensemble 
d&#8217;animations pr&#233; calcul&#233;es. Dans (Ben Youssef, 2011), nous avons propos&#233; un syst&#232;me de 
retour articulatoire visuel &#233;galement bas&#233; sur l&#8217;utilisation d&#8217;une t&#234;te parlante augment&#233;e. 
Dans notre approche, la t&#234;te parlante augment&#233;e est anim&#233;e automatiquement &#224; partir du 
</p>
<p>17</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>signal audio, par inversion acoustico-articulatoire. Cependant, dans ce syst&#232;me, 
l&#8217;animation de la t&#234;te parlante ne peut d&#233;buter qu&#8217;une fois la phrase enti&#232;rement 
produite (approche par HMM, d&#233;codage acoustico-phon&#233;tique bas&#233; sur l&#8217;algorithme de 
Viterbi). C&#8217;est cette limitation que le syst&#232;me Vizart3D tente de lever, en proposant une 
version temps-r&#233;el de notre syst&#232;me de retour articulatoire visuel. Un sch&#233;ma g&#233;n&#233;ral du 
syst&#232;me Vizart3D est pr&#233;sent&#233; &#224; la Figure 1.  
</p>
<p> 
Figure 1 : Sch&#233;ma g&#233;n&#233;ral du syst&#232;me Vizart3D   
</p>
<p>Le syst&#232;me Vizart3D est bas&#233; sur la t&#234;te parlante augment&#233;e, d&#233;velopp&#233;e au GIPSA-lab &#224; 
partir de donn&#233;es IRM, CT et vid&#233;o, acquises sur un locuteur de r&#233;f&#233;rence. L&#8217;animation 
de cette t&#234;te parlante &#224; partir de la voix d&#8217;un locuteur &#955; s&#8217;effectue en 3 &#233;tapes (ex&#233;cut&#233;es 
toutes les 10 ms) : (1) Conversion de voix : l&#8217;enveloppe spectrale du locuteur &#955;, extraite 
par analyse mel-cepstrale, est transform&#233;e en une enveloppe spectrale dite &#171; cible &#187;, qui 
peut &#234;tre vue comme l&#8217;enveloppe qui aurait &#233;t&#233; obtenue si la m&#234;me phrase avait &#233;t&#233; 
prononc&#233;e par le locuteur de r&#233;f&#233;rence ; dans notre impl&#233;mentation, nous utilisons une 
approche bas&#233;e sur une r&#233;gression par GMM (Gaussian Mixture Model) &#8211; (2) Inversion 
acoustico-articulatoire : une cible articulatoire est estim&#233;e &#224; partir de l&#8217;enveloppe 
spectrale cible (position de la langue (3 points), des l&#232;vres (2 points), et de la m&#226;choire 
(1 point)); cette &#233;tape d&#8217;inversion est &#233;galement bas&#233;e sur une mod&#233;lisation par GMM, &#224; 
partir d&#8217;un corpus de donn&#233;es audio et articulatoires, acquises sur le locuteur de 
r&#233;f&#233;rence par articulographie &#233;lectromagn&#233;tique 2D) &#8211; (3), les param&#232;tres de contr&#244;le de 
la t&#234;te parlante sont inf&#233;r&#233;s par r&#233;gression lin&#233;aire, &#224; partir de la cible articulatoire 
estim&#233;e &#224; l&#8217;&#233;tape 2.    
</p>
<p>R&#233;f&#233;rences 
BADIN, P., BEN YOUSSEF, A., BAILLY, G., ELISEI, F., HUEBER, T. (2010) Visual articulatory 
feedback for phonetic correction in second language learning, Actes de SLATE, P1-10. 
ENGWALL, O. (2008) Can audio-visual instructions help learners improve their 
articulation? - An ultrasound study of short term changes, Actes d&#8217;Interspeech, Brisbane, 
Australie, pp. 2631-2634. 
BEN YOUSSEF A., HUEBER T., BADIN P., BAILLY G. (2011) Toward a multi-speaker visual 
articulatory feedback system, Actes d&#8217;Interspeech, Florence, Italie, pp. 489-492. 
</p>
<p>18</p>

</div></div>
</body></html>