Vizart3D : R_etour Articulato_ire_Visuel
pour l’A1de a la P1‘0I10I1C1at10I1

Thomas Hueber’ Atef Ben-Youssef’
Pierre Badin1 Gérard Bailly1 Frédéric Eliséi1
(1) GIPSA-lab, UMR 5216/CNRS/INP/UJF/U.Stendhal, Grenoble, France
(prénom. nom) @gipsa—lab . grenoble—inp . fr

RESUME

L’objectif du systeme Vizart3D est de foumir 5 un locuteur, en temps réel, et de facon
automatique, un retour visuel sur ses propres mouvements articulatoires. Les
applications principales de ce systeme sont l’aide 5 l’apprentissage des langues
étrangeres et la rééducation orthophonique (correction phonétique). Le systéme Vizart3D
est basé sur la téte parlante 3D développée au GIPSA-lab, qui laisse apparaitre, en plus
des lévres, les articulateurs de la parole normalement cachés (comme la langue). Cette
téte parlante est animée automatiquement a partir du signal audio de parole, 5 l’aide de
techniques de conversion de voix et de régression acoustico-articulatoire par GMM.

ABSTRACT

Vizart3D: Visual Articulatory Feedack for Computer-Assisted Pronunciation
Training

We describe a system of visual articulatory feedback, which aims to provide any speaker
with a real feedback on his/her own articulation. Application areas are computer-
assisted pronunciation training (phonetic correction) for second-language learning and
speech rehabilitation. This system, named Vizartd3D, is based on the 3D augmented
talking head developed at GIPSA-lab, which is able to display all speech articulators
including usually hidden ones like the tongue. In our approach, the talking head is
animated automatically from the audio speech signal, using GMM-based voice
conversion and acoustic-to-articulatory regression.

MOTS-CLES : retour visuel, aide 5 la prononciation, GMM, temps réel, téte parlante
KEYWORDS : visual feedback, pronunciation training, GMM, real-time, talking head

Plusieurs études semblent montrer que foumir 5 un locuteur un retour visuel sur ses
propres mouvements articulatoires pouvait s’avérer utile pour la rééducation
orthophonique et l’apprentissage des langues (Badin, 2010). Ce retour visuel peut
notamment s’effectuer via une téte parlante augmentée, c’est-a-dire un clone orofacial
virtuel qui laisse apparaitre l’ensemble des articulateurs, extemes (lévres, machoire)
comme intetnes (langue, voile du palais). Dans
(Engwall, 2008), Engwall propose un paradigme expérimental du type « magicien d’Oz »
pour montrer l’efﬁcacité d’une telle approche (systéme ARTUR): un phonéticien expert
évalue la nature du défaut de prononciation du sujet, et lui fait visualiser le geste
articulatoire cible en sélectionnant l’animation adéquate parmi un ensemble
d’animations pré calculées. Dans (Ben Youssef, 2011), nous avons proposé un systéme de
retour articulatoire visuel également basé sur l’utilisation d’une téte parlante augmentée.
Dans notre approche, la téte parlante augmentée est animée automatiquement a partir du

Actes de la conférence conjointe JEP-TALN-RECITAL 2012, volume 5: Dénwnstrations, pages 17-18,
Grenoble, 4 an 8 juin 2012. ©2012 ATAI.A 8: AFCP

17

signal audio, par inversion acoustico-articulatoire. Cependant, dans ce systeme,
l’animation de la tete parlante ne peut debuter qu’une fois la phrase entierement
produite (approche par HMM, decodage acoustico-phonetique base sur l’algorithme de
Viterbi). C’est cette limitation que le systeme Vizart3D tente de lever, en proposant une
version temps-reel de notre systeme de retour articulatoire visuel. Un schema general du
systeme Vizart3D est presente a la Figure 1.

Clone orofacial 3D

   
 

Retour articulatoire visuel

“ Conversion Inversion Animation
de voix acoustico-articulatoire 3D

Figure 1 : Schema general du systeme Vizart3D

  

Le systeme Vizart3D est base sur la tete parlante augmentee, developpee au GIPSA-lab a
partir de donnees IRM, CT et video, acquises sur un locuteur de reference. L’animation
de cette tete parlante a partir de la voix d’un locuteur 7» s’effectue en 3 etapes (executees
toutes les 10 ms) : (1) Conversion de voix: l’enveloppe spectrale du locuteur 7», extraite
par analyse mel-cepstrale, est transformee en une enveloppe spectrale dite << cible >>, qui
peut etre vue comme l’enveloppe qui aurait ete obtenue si la meme phrase avait ete
prononcee par le locuteur de reference ; dans notre implementation, nous utilisons une
approche basee sur une regression par GMM (Gaussian Mixture Model) — (2) Inversion
acoustico-articulatoire : une cible articulatoire est estimee a partir de l’enveloppe
spectrale cible (position de la langue (3 points), des levres (2 points), et de la machoire
(1 point)); cette etape d’inversion est egalement basee sur une modelisation par GMM, a
partir d’un corpus de donnees audio et articulatoires, acquises sur le locuteur de
reference par articulographie electromagnetique 2D) — (3), les parametres de controle de
la tete parlante sont inferes par regression lineaire, a partir de la cible articulatoire
estimee a l’etape 2.

References
BADIN, P., BEN YOUSSEF, A., BAILLY, G., ELISEI, F., HUEBER, T. (2010) Visual articulatory

feedback for phonetic correction in second language learning, Actes de SLATE, P1-10.

ENGWALL, O. (2008) Can audio-visual instructions help learners improve their
articulation? - An ultrasound study of short term changes, Actes d’Interspeech, Brisbane,
Australie, pp. 2631-2634.

BEN YOUSSEF A., HUEBER T., BADIN P., BAILLY G. (2011) Toward a multi-speaker visual
articulatory feedback system, Actes d’Interspeech, Florence, Italie, pp. 489-492.

18

