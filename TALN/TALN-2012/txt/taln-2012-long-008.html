<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
"http://www.w3.org/TR/html4/loose.dtd">
<html><head><title>TCOF-POS : un corpus libre de fran&#231;ais parl&#233; annot&#233; en morphosyntaxe</title>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
</head>
<body>
<div style="page-break-before:always; page-break-after:always"><div><p>Actes de la conf&#233;rence conjointe JEP-TALN-RECITAL 2012, volume 2: TALN, pages 99&#8211;112,
Grenoble, 4 au 8 juin 2012. c&#169;2012 ATALA &amp; AFCP
</p>
<p>TCOF-POS : un corpus libre de fran&#231;ais parl&#233;
annot&#233; en morphosyntaxe
</p>
<p>Christophe Benzitoun1 Kar&#235;n Fort2,3 Beno&#238;t Sagot4
(1) ATILF, Nancy Universit&#233; &amp; CNRS, 44, avenue de la Lib&#233;ration, BP 30687, 54063 Nancy cedex
</p>
<p>(2) INIST-CNRS, 2 all&#233;e de Brabois, 54500 Vandoeuvre-l&#232;s-Nancy
(3) LIPN, Universit&#233; Paris 13 &amp; CNRS, 99 av. J.B. Cl&#233;ment, 93430 Villetaneuse
</p>
<p>(4) Alpage, INRIA Paris&#8211;Rocquencourt &amp; Universit&#233; Paris 7, Rocquencourt, France
christophe.benzitoun@atilf.fr, karen.fort@inist.fr, benoit.sagot@inria.fr
</p>
<p>R&#201;SUM&#201;
Nous pr&#233;sentons dans cet article un travail portant sur la cr&#233;ation d&#8217;un corpus de fran&#231;ais parl&#233;
spontan&#233; annot&#233; en morphosyntaxe. Nous d&#233;taillons la m&#233;thodologie suivie afin d&#8217;assurer le
contr&#244;le de la qualit&#233; de la ressource finale. Ce corpus est d&#8217;ores et d&#233;j&#224; librement diffus&#233; pour
la recherche et peut servir aussi bien de corpus d&#8217;apprentissage pour des logiciels que de base
pour des descriptions linguistiques. Nous pr&#233;sentons &#233;galement les r&#233;sultats obtenus par deux
&#233;tiqueteurs morphosyntaxiques entrain&#233;s sur ce corpus.
</p>
<p>ABSTRACT
TCOF-POS : A Freely Available POS-Tagged Corpus of Spoken French
</p>
<p>This article details the creation of TCOF-POS, the first freely available corpus of spontaneous
spoken French. We present here the methodology that was followed in order to obtain the best
possible quality in the final resource. This corpus already is freely available and can be used as a
training/validation corpus for NLP tools, as well as a study corpus for linguistic research. We also
present the results obtained by two POS-taggers trained on the corpus.
</p>
<p>MOTS-CL&#201;S : Etiquetage morpho-syntaxique, fran&#231;ais parl&#233;, ressources langagi&#232;res.
</p>
<p>KEYWORDS: POS tagging, French, speech, language resources.
</p>
<p>1 Introduction
</p>
<p>L&#8217;annotation automatique du fran&#231;ais parl&#233; est g&#233;n&#233;ralement r&#233;alis&#233;e par le biais de pr&#233;-
traitements de corpus ou d&#8217;adaptation d&#8217;outils existant pour le texte (Dister, 2007; Blanc et al.,
2008). Une autre solution peut consister &#224; masquer certains ph&#233;nom&#232;nes tels que les &quot;disfluences&quot;
(r&#233;p&#233;titions, amorces de mots, etc.) (Valli et V&#233;ronis, 1999). Pourtant, l&#8217;utilisation d&#8217;&#233;tiqueteurs
automatiques &#233;labor&#233;s pour et &#224; partir de donn&#233;es &#233;crites n&#8217;est pas une solution optimale &#233;tant
donn&#233;es les particularit&#233;s des corpus oraux par rapport &#224; l&#8217;&#233;crit. M&#234;me si l&#8217;&#233;tiquetage de corpus
oraux ne repr&#233;sente pas un probl&#232;me sp&#233;cifique (Benzitoun, 2004), l&#8217;utilisation de mod&#232;les
entra&#238;n&#233;s sur des donn&#233;es &#233;crites donne des r&#233;sultats m&#233;diocres. Ainsi, nous avons test&#233; Tree-
Tagger (Schmid, 1997), avec son mod&#232;le standard pour le fran&#231;ais, sur un &#233;chantillon de 3 007
tokens extraits du corpus de r&#233;f&#233;rence d&#233;crit dans cet article et nous avons obtenu une pr&#233;cision
de seulement 83,1 %.
</p>
<p>99</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Un corpus du fran&#231;ais parl&#233; annot&#233; en morphosyntaxe librement disponible serait donc utile, non
seulement pour les logiciels d&#8217;annotation en morphosyntaxe, mais &#233;galement pour am&#233;liorer les
syst&#232;mes de transcription automatique (Huet et al., 2006) ou d&#8217;autres outils. Cependant, il n&#8217;existe
pas encore, &#224; notre connaissance, de corpus de fran&#231;ais parl&#233; spontan&#233; annot&#233; en morphosyntaxe
(parties du discours et/ou lemmes) qui soit diffus&#233; librement. Parmi les corpus annot&#233;s mais non
diffus&#233;s librement, on peut citer les projets elicop (Mertens, 2002), C-ORAL-ROM (Campione
et al., 2005), Valibel (Dister, 2007), Corpus de Fran&#231;ais Parl&#233; Parisien (Branca-Rosoff et al., 2010)
ou bien encore ESLO (Eshkol et al., 2010).
</p>
<p>Notre objectif est donc de d&#233;velopper et diffuser librement &#224; l&#8217;ensemble de la communaut&#233;
scientifique un corpus pr&#233;-annot&#233; automatiquement puis corrig&#233; manuellement, dont la qualit&#233;
aura &#233;t&#233; pr&#233;cis&#233;ment &#233;valu&#233;e. Il pourra servir notamment de corpus d&#8217;apprentissage sp&#233;cifique
au fran&#231;ais parl&#233; et plus largement de corpus exploitable pour des recherches en linguistique ou
en Traitement Automatique des Langues (TAL).
</p>
<p>Nous pr&#233;sentons tout d&#8217;abord le corpus de fran&#231;ais parl&#233; TCOF (Traitement des Corpus Oraux du
Fran&#231;ais), puis la m&#233;thodologie utilis&#233;e pour l&#8217;annotation manuelle, les diff&#233;rentes &#233;valuations
r&#233;alis&#233;es pendant la campagne et enfin les r&#233;sultats obtenus par les &#233;tiqueteurs morphosyn-
taxiques entrain&#233;s sur une partie du corpus annot&#233; TCOF-POS.
</p>
<p>2 Pr&#233;sentation du corpus
</p>
<p>Le corpus d&#8217;origine que nous avons annot&#233; est celui du projet TCOF (Andr&#233; et Canut, 2010),
librement disponible sur le site du CNRTL 1. Ce corpus est constitu&#233; de transcriptions de donn&#233;es
orales recueillies dans des contextes aussi naturels que possible. Il comporte une partie d&#8217;interac-
tions entre adultes et une autre entre adultes et enfants. En ce qui concerne la partie adulte (la
seule que nous ayons exploit&#233;e jusqu&#8217;&#224; pr&#233;sent), elle est compos&#233;e :
</p>
<p>&#8211; d&#8217;interactions sollicit&#233;es, dans lesquelles au moins deux locuteurs sont engag&#233;s dans des
r&#233;cits de vie, d&#8217;&#233;v&#233;nements ou d&#8217;exp&#233;riences, ou dans des explications sur un savoir-faire
professionnel ou technique ;
</p>
<p>&#8211; de conversations &#224; b&#226;tons rompus ou portant sur des th&#233;matiques sp&#233;cifiques ;
&#8211; de donn&#233;es non sollicit&#233;es dans des situations publiques ou professionnelles : r&#233;unions pu-
</p>
<p>bliques, activit&#233;s professionnelles diverses.
</p>
<p>De ce corpus, nous avons extrait un &#233;chantillon de 22 240 tokens 2, soit 11 transcriptions
diff&#233;rentes. Cet &#233;chantillon contient des conversations, des r&#233;unions professionnelles, ainsi
que des extraits d&#8217;une Assembl&#233;e G&#233;n&#233;rale &#224; l&#8217;Universit&#233;. L&#8217;int&#233;gralit&#233; des paroles prononc&#233;es
a &#233;t&#233; scrupuleusement retranscrite en orthographe standard, sans artifice ou am&#233;nagement
orthographique (donc sans ponctuation), suivant en cela les recommandations de (Blanche-
Benveniste et Jeanjean, 1987) 3 largement diffus&#233;es et utilis&#233;es. Elles sont au format g&#233;n&#233;r&#233; par
le logiciel Transcriber (trs - XML).
</p>
<p>1. http ://cnrtl.fr/corpus/tcof/
2. Notre conception de la notion de token est assez &#233;l&#233;mentaire (aucune insertion possible).
3. Les conventions de transcription sont disponibles sur le site suivant :
</p>
<p>http ://cnrtl.fr/corpus/tcof/TCOFConventions.pdf
</p>
<p>100</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Ces transcriptions sont automatiquement converties en texte brut &#224; l&#8217;aide d&#8217;une feuille de style
XSLT (qui &#233;limine l&#8217;int&#233;gralit&#233; des balises XML), puis d&#8217;une s&#233;rie d&#8217;expressions r&#233;guli&#232;res qui
supprime les informations non d&#233;sir&#233;es, telles que les pauses. Le texte final contient les mentions
des locuteurs (L1, L2, etc.), l&#8217;int&#233;gralit&#233; des paroles prononc&#233;es, ainsi que les multi-transcriptions.
Il s&#8217;agit donc de transcriptions brutes non retouch&#233;es, dont voici un exemple :
</p>
<p>L1 et puis je crois que c&#8217;est en je crois je crois m&#234;me que c&#8217;est en zone industrielle
L2 ouais ouais je pense aussi &#231;a doit pas &#234;tre en ville
L1 oui mais
L2 en Belgique aussi il y a des trucs euh un genre de grand tr- enfin un genre de grande
galerie en Belgique et puis c&#8217;est que des magasins de fringues aussi
</p>
<p>Les transcriptions ont &#233;t&#233; faites dans le cadre d&#8217;un cours de deuxi&#232;me ann&#233;e de Sciences
du Langage &#224; l&#8217;Universit&#233; Nancy 2, puis revues par des enseignants de Sciences du Langage.
L&#8217;anonymisation, quant &#224; elle, a &#233;t&#233; r&#233;alis&#233;e manuellement par des &#233;tudiants-vacataires. A la
lecture, ils devaient rep&#233;rer les toponymes, anthroponymes, etc. puis les remplacer par un
symbole et ins&#233;rer un son dans la portion de signal sonore correspondante.
</p>
<p>3 M&#233;thodologie
</p>
<p>L&#8217;annotation totalement manuelle de corpus &#233;tant tr&#232;s co&#251;teuse, nous avons proc&#233;d&#233;, comme
d&#233;crit dans (Marcus et al., 1993), &#224; une correction manuelle de corpus pr&#233;-annot&#233;s automatique-
ment. La nature du pr&#233;-annotateur, ainsi que les modalit&#233;s de la correction manuelle diff&#232;rent
selon les &#233;tapes du processus, comme nous allons le voir dans cette section. Toutefois, toutes
les pr&#233;-annotations ont &#233;t&#233; produites par diff&#233;rentes instances du syst&#232;me TreeTagger (Schmid,
1997), qui fournit pour chaque token d&#8217;entr&#233;e une &#233;tiquette morphosyntaxique et un lemme.
</p>
<p>Comme indiqu&#233; en introduction, l&#8217;utilisation comme pr&#233;-annotateur pour un corpus de parole
spontan&#233;e transcrite, d&#8217;un &#233;tiqueteur morphosyntaxique entra&#238;n&#233; sur un corpus &#233;crit n&#8217;est pas
adapt&#233;e. Parmi les ph&#233;nom&#232;nes qui posent probl&#232;me, lesquels ne sont pas totalement absents
des corpus &#233;crits mais y sont bien plus rares (Benzitoun, 2004), on peut citer :
&#8211; les r&#233;p&#233;titions de mots ou de groupes de mots (&#231;a &#231;a redevient &#231;a redevient le bordel comme &#231;a),
&#8211; les reformulations (peut-&#234;tre s&#233;parer compl&#232;tement euh junior euh homme enfin euh adulte),
&#8211; les ruptures de construction (ouais ouais que de la gueule que de la),
&#8211; les amorces de mots (moi j&#8217;aurais p- j&#8217;aurais pas mis de pantalon),
&#8211; les incises (euh on consid&#233;rait que former les hommes et c&#8217;est toujours euh en en en vigueur &#231;a
</p>
<p>hein former les les en- les les enfants d&#8217;aujourd&#8217;hui c&#8217;est aussi former les hommes de demain),
&#8211; les formes non conventionnelles (tu sais genre trop v&#233;n&#232;re ; avoir du matos en entr&#233;e de mag),
&#8211; les particules discursives (hein, eh ben, etc.). . .
Pour ne prendre que deux exemples, la version standard de TreeTagger pour le fran&#231;ais consid&#232;re
que bon est syst&#233;matiquement un adjectif et quoi un pronom, alors qu&#8217;ils sont majoritairement des
particules discursives. De plus, nos transcriptions ne sont pas segment&#233;es en &#171; phrases &#187; (Blanche-
Benveniste et Jeanjean, 1987), ce qui peut &#233;galement poser des probl&#232;mes aux outils. Par
exemple, l&#8217;&#233;tiquette SENT (pour sentence) indiquant une fronti&#232;re de phrase doit obligatoirement
&#234;tre pr&#233;sente dans le lexique servant pour l&#8217;apprentissage de TreeTagger (m&#234;me s&#8217;il ne s&#8217;en sert
pas par la suite). En cons&#233;quence, nous avons proc&#233;d&#233;, d&#232;s que possible, &#224; l&#8217;entra&#238;nement de
versions de TreeTagger &#224; partir des annotations d&#233;j&#224; obtenues sur notre corpus.
</p>
<p>101</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>La m&#233;thodologie retenue, d&#233;crite en d&#233;tail dans cette partie, peut &#234;tre r&#233;sum&#233;e comme suit :
</p>
<p>1. D&#233;finition de crit&#232;res de tokenisation et d&#8217;identification des compos&#233;s, puis tokenisation
automatique ;
</p>
<p>2. D&#233;finition d&#8217;un jeu d&#8217;&#233;tiquettes adapt&#233; &#224; la parole spontan&#233;e transcrite ;
</p>
<p>3. Cr&#233;ation d&#8217;un corpus de r&#233;f&#233;rence Cref de 22 240 tokens par correction d&#8217;une pr&#233;-annotation
automatique, effectu&#233;e par deux experts linguistes :
&#8211; Les 10 000 premiers tokens de Cref ont &#233;t&#233; pr&#233;-annot&#233;s avec la version standard de
</p>
<p>TreeTagger ;
&#8211; Les 12 240 tokens suivants de Cref ont &#233;t&#233; pr&#233;-annot&#233;s avec une version de TreeTagger
</p>
<p>entra&#238;n&#233;e sur les 10 000 premiers ;
</p>
<p>4. R&#233;-annotation par deux &#233;tudiantes d&#8217;environ 7 500 tokens du corpus de r&#233;f&#233;rence Cref
(suivie d&#8217;une phase d&#8217;adjudication), afin d&#8217;&#233;valuer la qualit&#233; des annotations dans deux
configurations distinctes :
&#8211; environ 6 000 tokens ont &#233;t&#233; pr&#233;-annot&#233;s par la version standard de TreeTagger ;
&#8211; environ 1 500 tokens ont &#233;t&#233; pr&#233;-annot&#233;s avec une version de TreeTagger entra&#238;n&#233;e sur
</p>
<p>les 16 312 premiers tokens de Cref ;
L&#8217;objectif &#233;tait ici de mesurer l&#8217;impact de la diff&#233;rence de qualit&#233; entre pr&#233;-annotateurs en
termes de vitesse d&#8217;annotation et de pr&#233;cision du r&#233;sultat de l&#8217;&#233;tape manuelle ;
</p>
<p>5. Application de cette m&#233;thodologie &#224; un plus grand nombre d&#8217;&#233;tudiants pour en valider la
robustesse ;
</p>
<p>6. Annotation par deux &#233;tudiantes d&#8217;un corpus additionnel Cadd de 80 000 nouveaux tokens,
pr&#233;-annot&#233;s avec la version de TreeTagger entra&#238;n&#233;e sur la totalit&#233; de Cref.
</p>
<p>Nous avons appliqu&#233; pour cette campagne les bonnes pratiques actuelles en annotation manuelle
de corpus, qui consistent &#224; &#233;valuer le plus t&#244;t possible l&#8217;accord inter-annotateurs et de mettre
&#224; jour le guide d&#8217;annotation (Bonneau-Maynard et al., 2005). La r&#233;p&#233;tition r&#233;guli&#232;re de ce
processus conduit &#224; ce qu&#8217;on appelle maintenant l&#8217;annotation agile (Voormann et Gut, 2008).
</p>
<p>3.1 Tokenisation et gestion des compos&#233;s
</p>
<p>Le corpus ayant fait l&#8217;objet d&#8217;une pr&#233;-annotation (voir section 3.3), nous avons pris comme base
la tokenisation par d&#233;faut de TreeTagger, qui repose notamment sur un fichier de compos&#233;s. Mais
ce dernier s&#8217;est av&#233;r&#233; insuffisant (par exemple, parce que reste d&#233;coup&#233; en deux tokens distincts
mais puisqu&#8217;ils en un seul token). Nous l&#8217;avons donc compl&#233;t&#233; au fur et &#224; mesure, en respectant
le crit&#232;re suivant : toute s&#233;quence dans laquelle il est possible d&#8217;ins&#233;rer un &#233;l&#233;ment est d&#233;coup&#233;e
en plusieurs tokens, afin d&#8217;exclure les unit&#233;s discontinues. Ainsi, un peu est d&#233;coup&#233; en deux
tokens (car on peut trouver un tout petit peu).
</p>
<p>3.2 Un jeu d&#8217;&#233;tiquettes adapt&#233; &#224; la parole spontan&#233;e transcrite
</p>
<p>Afin de b&#233;n&#233;ficier au mieux des ressources d&#233;velopp&#233;es pour l&#8217;&#233;crit et de limiter le travail de
correction, tout en prenant en consid&#233;ration les ph&#233;nom&#232;nes sp&#233;cifiques &#224; la parole spontan&#233;e
cit&#233;s ci-dessus, nous avons d&#233;cid&#233; d&#8217;utiliser un jeu d&#8217;&#233;tiquettes bas&#233; au d&#233;part sur les &#233;tiquettes
par d&#233;faut fournies par TreeTagger. Nous l&#8217;avons compl&#233;t&#233; &#224; l&#8217;aide de (Abeill&#233; et Cl&#233;ment, 2006).
</p>
<p>102</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Les r&#233;p&#233;titions, reformulations, etc. n&#8217;ont pas fait l&#8217;objet de traitements sp&#233;cifiques, chacun des
tokens a la cat&#233;gorie qu&#8217;il a habituellement (ex : le[DET] le[DET] le[DET] chat). Au final, m&#234;me
si les identifiants des &#233;tiquettes sont diff&#233;rents, les cat&#233;gories retenues sont quasiment identiques
&#224; (Abeill&#233; et Cl&#233;ment, 2006), avec toutefois un peu moins de sous-cat&#233;gories (notamment aucune
pour les adverbes et les adjectifs) et l&#8217;ajout de la cat&#233;gorie &#171; auxiliaire &#187; ainsi que de trois
&#233;tiquettes sp&#233;cifiques &#224; l&#8217;oral : MLT (multi-transcription), TRC (amorce de mot) et LOC (locuteur)
(cf. tableau 1). Afin de nous aider dans la r&#233;daction du manuel d&#8217;annotation, nous nous sommes
d&#8217;ailleurs inspir&#233;s de (Abeill&#233; et Cl&#233;ment, 2006). Notre jeu d&#8217;&#233;tiquettes comprend 62 &#233;tiquettes.
</p>
<p>En outre, il a &#233;t&#233; affin&#233; durant la phase de constitution du corpus servant de r&#233;f&#233;rence. En effet,
nous voulions que les &#233;tiquettes soient appos&#233;es de mani&#232;re aussi syst&#233;matique que possible pour
que nos choix soient r&#233;versibles et que les modifications soient automatisables, autant que faire
se peut. De ce fait, m&#234;me si cela peut para&#238;tre discutable d&#8217;un point de vue th&#233;orique, nous avons
privil&#233;gi&#233; les choix qui potentiellement g&#233;n&#232;rent le moins de fluctuations entre annotateurs. Par
exemple, la distinction entre participe pass&#233; et adjectif n&#8217;est pas ais&#233;e et plut&#244;t que d&#8217;obtenir une
annotation de qualit&#233; moindre, nous avons pr&#233;f&#233;r&#233; neutraliser celle-ci. Ainsi, chaque fois que
la forme verbale existe (sauf cas de changement notoire de sens), nous avons annot&#233; &#171; verbe &#187;.
Dans le cas contraire, nous avons annot&#233; &#171; adjectif &#187;.
</p>
<p>Nous avons &#233;galement d&#233;cid&#233; d&#8217;essayer de limiter les cas de transferts d&#8217;une cat&#233;gorie vers
une autre (trans-cat&#233;gorisation). En effet, ceux-ci auraient artificiellement &#233;t&#233; limit&#233;s aux cas
rencontr&#233;s dans le corpus &#224; annoter, sans possibilit&#233; d&#8217;avoir une vision globale du ph&#233;nom&#232;ne.
De plus, cela aurait complexifi&#233; la t&#226;che de correction. Ainsi, dans rouler tranquille, tranquille est
consid&#233;r&#233; comme un adjectif et non comme un adverbe (ce qui, de toute fa&#231;on, est discutable
d&#8217;un point de vue th&#233;orique). Enfin, il n&#8217;a pas &#233;t&#233; possible d&#8217;exclure totalement les cas d&#8217;&#233;tiquettes
limit&#233;es &#224; un mot unique. Ainsi, l&#8217;&#233;tiquette &#171; particule interrogative &#187; ne s&#8217;utilise que pour est-ce
qu-e/i et &#171; pr&#233;d&#233;terminant &#187; uniquement pour tous.
</p>
<p>3.3 Cr&#233;ation du sous-corpus de r&#233;f&#233;rence
</p>
<p>Comme indiqu&#233; ci-dessus, la cr&#233;ation de la premi&#232;re tranche de 10 000 tokens du corpus de
r&#233;f&#233;rence Cref de 22 240 tokens a &#233;t&#233; r&#233;alis&#233;e en utilisant comme pr&#233;-annotateur la version
standard de TreeTagger, entra&#238;n&#233;e sur un corpus &#233;crit. Nous (L. B&#233;rard et C. Benzitoun) avons
ensuite corrig&#233; ces pr&#233;-annotations en plusieurs passes. Nous avons tout d&#8217;abord effectu&#233; des
remplacements automatiques, lorsque les modifications &#233;taient syst&#233;matiques ou que l&#8217;&#233;tiquette
majoritaire n&#8217;&#233;tait pas celle appos&#233;e par d&#233;faut par le logiciel (ce qui est le cas pour bon (ADJ/INT)
et quoi (PRO :int/INT), par exemple). Ensuite, nous nous sommes r&#233;partis les donn&#233;es &#224; corriger
et, apr&#232;s les avoir int&#233;gralement trait&#233;es, nous nous les sommes &#233;chang&#233;es pour r&#233;vision. Nous
avons ensuite discut&#233; des cas o&#249; nous n&#8217;&#233;tions pas en accord jusqu&#8217;&#224; trouver des solutions.
Nous avons effectu&#233; ces &#233;tapes plusieurs fois, jusqu&#8217;&#224; obtenir des annotations fiables. Le guide
d&#8217;annotation &#233;tait mis &#224; jour &#224; chaque &#233;tape.
</p>
<p>Nous avons par ailleurs g&#233;n&#233;r&#233; automatiquement des fichiers de fr&#233;quences, afin de faciliter
le rep&#233;rage des erreurs. A ainsi &#233;t&#233; calcul&#233;e la fr&#233;quence de chaque &#233;tiquette pour un m&#234;me
lemme ou un m&#234;me token, ce qui nous a permis d&#8217;identifier et de corriger quelques erreurs
suppl&#233;mentaires. Par exemple, C.E. ayant &#233;t&#233; annot&#233; 1 fois NAM et 3 fois NOM :sg (pour un
m&#234;me lemme C.E.), cette derni&#232;re &#233;tiquette a &#233;t&#233; attribu&#233;e aux 4 occurrences. De m&#234;me, cela
nous a permis de corriger deux occurrences de du, ind&#251;ment annot&#233;es DET :ind.
</p>
<p>103</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>ADJ adjectif NUM num&#233;ral
ADV adverbe PRO :clo clitique objet
AUX :cond auxiliaire au conditionnel PRO :cls clitique sujet
AUX :futu auxiliaire au futur PRO :clsi clitique sujet impersonnel
AUX :impe auxiliaire &#224; l&#8217;imp&#233;ratif PRO :dem pronom d&#233;monstratif
AUX :impf auxiliaire &#224; l&#8217;imparfait PRO :ind pronom ind&#233;fini
AUX :infi auxiliaire &#224; l&#8217;infinitif PRO :int pronom interrogatif
AUX :pper auxiliaire au participe pass&#233; PRO :pos pronom possessif
AUX :ppre auxiliaire au participe pr&#233;-
</p>
<p>sent
PRO :rel pronom relatif
</p>
<p>AUX :pres auxiliaire au pr&#233;sent PRO :ton pronom tonique
AUX :simp auxiliaire au pass&#233; simple PRP pr&#233;position
AUX :subi auxiliaire au subjonctif im-
</p>
<p>parfait
PRP :det pr&#233;position/d&#233;terminant
</p>
<p>AUX :subp auxiliaire au subjonctif pr&#233;-
sent
</p>
<p>PRT :int particule interrogative (est-
ce que)
</p>
<p>DET :def d&#233;terminant d&#233;fini SYM symbole
DET :dem d&#233;terminant d&#233;monstratif TRC amorces de mots
DET :ind d&#233;terminant ind&#233;fini VER verbe sans flexion (voil&#224;)
DET :int d&#233;terminant interrogatif VER :cond verbe au conditionnel
DET :par d&#233;terminant partitif (du) VER :futu verbe au futur
DET :pos d&#233;terminant possessif VER :impe verbe &#224; l&#8217;imp&#233;ratif
DET :pre pr&#233;-d&#233;terminant (tout (le)) VER :impf verbe &#224; l&#8217;imparfait
EPE &#233;penth&#233;tique VER :infi verbe &#224; l&#8217;infinitif
ETR mots &#233;trangers VER :pper verbe au participe pass&#233;
FNO forme noyau (oui, non, d&#8217;ac-
</p>
<p>cord, etc.)
VER :ppre verbe au participe pr&#233;sent
</p>
<p>INT interjection et particules dis-
cursives
</p>
<p>VER :pres verbe au pr&#233;sent
</p>
<p>KON conjonction VER :simp verbe au pass&#233; simple
LOC locuteur VER :subi verbe au subjonctif impar-
</p>
<p>fait
MLT multi-transcription VER :subp verbe au subjonctif pr&#233;sent
NAM nom propre NOM :trc nom commun tronqu&#233;
NOM nom commun NAM :trc nom propre tronqu&#233;
NOM :sig sigle VER :trc verbe tronqu&#233;
NAM :sig sigle ADJ :trc adjectif tronqu&#233;
</p>
<p>TABLE 1 &#8211; Jeu d&#8217;&#233;tiquettes du corpus TCOF-POS
</p>
<p>104</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Nous avons ensuite appliqu&#233; les r&#233;sultats obtenus par Fort et Sagot (2010) sur l&#8217;int&#233;r&#234;t d&#8217;une
pr&#233;-annotation avec un outil de qualit&#233; moyenne. Ainsi, une fois la premi&#232;re tranche de 10 000
tokens annot&#233;s, nous avons r&#233;-entra&#238;n&#233; TreeTagger sur ce sous-corpus (mais sans utiliser de
lexique externe) et avons pr&#233;-annot&#233; les transcriptions suivantes de Cref (12 240 tokens) avec ce
nouvel outil. La m&#234;me m&#233;thodologie que celle utilis&#233;e pour corriger les 10 000 premiers tokens
nous a permis de finaliser le corpus de r&#233;f&#233;rence Cref de 22 240 tokens.
</p>
<p>3.4 Cr&#233;ation du sous-corpus additionnel
</p>
<p>Le corpus diffus&#233; est compos&#233; pour une part du sous-corpus de r&#233;f&#233;rence Cref et pour une autre
part d&#8217;un autre sous-corpus additionnel Cadd corrig&#233; par deux &#233;tudiantes (de L3 et M2 de Sciences
du Langage) recrut&#233;es sp&#233;cifiquement pour cette t&#226;che. Dans un premier temps, afin d&#8217;&#233;valuer a
priori la m&#233;thodologie pr&#233;vue pour l&#8217;annotation de Cadd, nous avons men&#233; une campagne de tests
en nous servant de Cref comme r&#233;f&#233;rence. Pour ce faire, les deux &#233;tudiantes ont eu 15 fichiers
extraits de Cref d&#8217;environ 500 tokens chacun
</p>
<p>4 &#224; corriger dans un ordre contraint. Les 12 premiers
fichiers avaient &#233;t&#233; pr&#233;-annot&#233;s par la version standard de TreeTagger. Afin de mesurer l&#8217;impact
de la qualit&#233; du pr&#233;-annotateur, les 3 derniers fichiers avaient &#233;t&#233; pr&#233;-annot&#233;s par une version
de TreeTagger r&#233;-entra&#238;n&#233;e &#224; partir d&#8217;un extrait de 16 312 tokens de la r&#233;f&#233;rence Cref (et sans
lexique externe). Naturellement, ces tokens forment un sous-ensemble de Cref disjoint des 15
fichiers &#224; r&#233;-annoter. Les &#233;tudiantes avaient l&#8217;interdiction d&#8217;&#233;changer des informations durant la
phase d&#8217;annotation.
</p>
<p>La correction a &#233;t&#233; effectu&#233;e dans un tableur, les cellules contenant les &#233;tiquettes &#233;tant mu-
nies d&#8217;une liste d&#233;roulante se limitant au jeu d&#8217;&#233;tiquettes d&#233;fini ci-dessus. La saisie &#233;tait donc
contrainte. Une fois la correction termin&#233;e, les fichiers annot&#233;s en parall&#232;le ont &#233;t&#233; compa-
r&#233;s automatiquement. Les cas de divergence entre les deux annotateurs ont ainsi &#233;t&#233; rep&#233;r&#233;s
automatiquement et corrig&#233;s par un expert 5.
</p>
<p>Dans le cadre de ce travail, les mesures suivantes ont &#233;t&#233; effectu&#233;es :
&#8211; le temps mis par les &#233;tudiantes pour annoter chaque fichier ;
&#8211; la pr&#233;cision de chaque fichier par rapport &#224; la r&#233;f&#233;rence ;
&#8211; l&#8217;accord inter-annotateurs des &#233;tudiantes (Kappa de Cohen (Cohen, 1960)) ;
&#8211; la pr&#233;cision apr&#232;s fusion et adjudication.
L&#8217;&#233;valuation de leurs annotations sur ces 15 fichiers est reproduite ci-dessous (figures 1 et 2 et
tableau 2). Elle tient compte de la lemmatisation et des parties du discours.
</p>
<p>1e 2e 3e 4e 5e 6e 7e 8e 9e 10e 11e 12e 13e 14e 15e
107 71 80 67 60 60 57 65 50 50 52 47 32 32 31
</p>
<p>TABLE 2 &#8211; Temps d&#8217;annotation (en minutes)
</p>
<p>Entre le 12e et le 13e fichier, la diff&#233;rence de temps est vraisemblablement imputable au change-
ment de pr&#233;-annotation par TreeTagger.
</p>
<p>4. Cette taille a &#233;t&#233; retenue car nous avons observ&#233; qu&#8217;elle permet une correction rapide et une attention soutenue
sans &#234;tre oblig&#233; de s&#8217;interrompre en cours d&#8217;annotation.
</p>
<p>5. Pour des raisons pratiques, il n&#8217;a pas &#233;t&#233; possible de confier cette phase &#224; un expert externe. La personne qui
l&#8217;a r&#233;alis&#233;e a &#233;galement collabor&#233; &#224; la r&#233;alisation du corpus servant de r&#233;f&#233;rence, ce qui peut repr&#233;senter un biais
m&#233;thodologique.
</p>
<p>105</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>FIGURE 1 &#8211; &#201;volution de la pr&#233;cision des deux &#233;tudiantes
</p>
<p>FIGURE 2 &#8211; &#201;volution du Kappa (&#224; gauche) des 2 &#233;tudiantes et de la pr&#233;cision apr&#232;s adjudication
(&#224; droite)
</p>
<p>La qualit&#233; des corrections apr&#232;s r&#233;-entra&#238;nement et pr&#233;-&#233;tiquetage ainsi que le faible temps
de correction (pour les 3 derniers fichiers, donc) nous ont paru suffisants pour valider notre
m&#233;thodologie et ainsi poursuivre l&#8217;&#233;laboration du corpus. Sur les 3 derniers fichiers, la pr&#233;cision
moyenne est de 98,03 % en ne tenant compte que des &#233;tiquettes. Nous sommes donc pass&#233;s
&#224; l&#8217;annotation par les deux &#233;tudiantes du corpus Cadd. Elles ont ainsi re&#231;u le m&#234;me jeu de 160
nouveaux fichiers de 500 tokens chacun, pr&#233;-annot&#233;s par la version r&#233;-entra&#238;n&#233;e de TreeTagger.
En 60 heures, elles ont corrig&#233; 80 000 tokens chacune, ce qui fait une moyenne d&#8217;un peu plus
de 21 minutes par fichier de 500 tokens. Sur l&#8217;ensemble, l&#8217;accord inter-annotateurs (Kappa de
Cohen (Cohen, 1960)) est en moyenne de 96,5 % et le temps moyen consacr&#233; &#224; l&#8217;adjudication de
2 min. 45 sec par fichier.
</p>
<p>4 &#201;largissement de l&#8217;&#233;valuation
</p>
<p>Afin d&#8217;&#233;valuer le caract&#232;re robuste de notre m&#233;thodologie, nous avons &#233;largi l&#8217;&#233;valuation &#224;
plus d&#8217;&#233;tudiants. En effet, nous comptons augmenter de mani&#232;re importante la quantit&#233; de
fichiers corrig&#233;s dans les ann&#233;es &#224; venir et nous voulons v&#233;rifier si notre m&#233;thodologie donne
des r&#233;sultats comparables quels que soient les correcteurs. Pour ce faire, nous avons adopt&#233;
</p>
<p>106</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>la m&#234;me m&#233;thodologie que celle d&#233;crite ci-dessus, &#224; savoir une double-annotation de chaque
fichier puis une adjudication pour les cas de divergence uniquement. Cette &#233;valuation a port&#233; sur
les corrections fournies par 10 &#233;tudiants en Sciences du Langage &#224; l&#8217;Universit&#233; Nancy 2 (L3 et
M2) dans le cadre de deux enseignements. A chaque bin&#244;me, nous avons donn&#233; 6 fichiers (4
fichiers pr&#233;-annot&#233;s avec le TreeTagger de base et 2 fichiers avec le TreeTagger r&#233;-entra&#238;n&#233;) &#224;
corriger dans un ordre contraint. Dans cette exp&#233;rience, comme dans la pr&#233;c&#233;dente, les &#233;tudiants
devaient corriger les lemmes en plus des &#233;tiquettes. Dans la suite de ce travail, les mesures que
nous pr&#233;sentons tiennent compte &#224; la fois des lemmes et &#233;tiquettes (sauf pr&#233;cision contraire).
</p>
<p>4.1 Temps d&#8217;annotation et accord inter-annotateurs
</p>
<p>En ce qui concerne le temps d&#8217;annotation, nous avons observ&#233; une diminution syst&#233;matique avec
une nette diff&#233;rence entre les 4 premiers fichiers et les deux derniers (voir tableau 3).
</p>
<p>1e annot. 2e annot. 3e annot. 4e annot. 5e annot. 6e annot.
110,1 101,8 79,2 72,5 41,3 39,7
</p>
<p>TABLE 3 &#8211; Temps d&#8217;annotation (en minutes)
</p>
<p>Au-del&#224; de la diminution du temps de correction inh&#233;rente &#224; une meilleure ma&#238;trise des &#233;tudiants,
il para&#238;t difficile d&#8217;expliquer la diminution du temps entre le quatri&#232;me et le cinqui&#232;me fichier par
un autre facteur que le basculement entre le TreeTagger standard et la version r&#233;-entra&#238;n&#233;e. Le
m&#234;me ph&#233;nom&#232;ne peut &#234;tre observ&#233; concernant l&#8217;accord inter-annotateurs (cf. figure 3). Le coef-
ficient d&#8217;accord inter-annotateurs pr&#233;sent&#233; ici est, comme pr&#233;c&#233;demment, le &#954; de Cohen (Cohen,
1960).
</p>
<p>FIGURE 3 &#8211; &#201;volution de l&#8217;accord inter-annotateurs (kappa) des &#233;tudiants
</p>
<p>Dans la figure 3, la courbe noire repr&#233;sente l&#8217;&#233;volution de la moyenne des accords inter-
annotateurs, de m&#234;me que dans les graphiques suivants.
</p>
<p>107</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>4.2 Pr&#233;cision
</p>
<p>Outre une diminution significative du temps d&#8217;annotation et une augmentation de l&#8217;accord inter-
annotateurs, nous avons &#233;galement constat&#233; une importante augmentation de la pr&#233;cision en
moyenne pour chaque &#233;tudiant (cf. figure 4). On observe encore une fois une nette augmentation
entre le quatri&#232;me et le cinqui&#232;me fichier, et ce chez tous les &#233;tudiants. La figure 5 indique la
pr&#233;cision de chaque fichier apr&#232;s fusion et adjudication.
</p>
<p>FIGURE 4 &#8211; Evolution de la pr&#233;cision de chaque &#233;tudiant
</p>
<p>FIGURE 5 &#8211; &#201;volution de la pr&#233;cision de chaque fichier corrig&#233; apr&#232;s fusion et adjudication
</p>
<p>Finalement, sur les deux derniers fichiers annot&#233;s, le taux de pr&#233;cision moyen est respectivement
de 97,28 % et 97,12 % en &#233;valuant les erreurs portant &#224; la fois sur les lemmes et les &#233;tiquettes. Si
l&#8217;on prend en compte seulement les &#233;tiquettes, le taux de pr&#233;cision moyen est alors respectivement
de 97,42 % et de 97,8 % pour ces m&#234;mes fichiers, ce qui est l&#233;g&#232;rement inf&#233;rieur &#224; ce qui a
</p>
<p>108</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>&#233;t&#233; relev&#233; pr&#233;c&#233;demment pour les deux &#233;tudiantes. Cependant, nous estimons que cela permet
d&#8217;affirmer que les principes adopt&#233;s permettent d&#8217;obtenir des corpus annot&#233;s de qualit&#233; proche,
et ce quelle que soit la personne qui corrige.
</p>
<p>5 Premiers r&#233;sultats de l&#8217;annotation automatique
</p>
<p>Pour effectuer les premiers tests concernant l&#8217;apprentissage automatique, notre choix s&#8217;est port&#233;
sur deux &#233;tiqueteurs morphosyntaxiques : MElt (Denis et Sagot, 2009, 2012), &#233;tiqueteur &#233;tat
de l&#8217;art pour le fran&#231;ais, et TreeTagger, utilis&#233; comme pr&#233;-annotateur pour constituer le corpus,
et largement utilis&#233; bien qu&#8217;il ne soit pas celui qui donne les meilleurs r&#233;sultats &#224; l&#8217;heure
actuelle (Denis et Sagot, 2009; Eshkol et al., 2010). Tous deux sont librement disponibles et
multi-plateformes.
</p>
<p>Notre objectif &#233;tait d&#8217;&#233;tudier la courbe d&#8217;apprentissage, et ce sous plusieurs angles : la pr&#233;cision
de l&#8217;&#233;tiqueteur entra&#238;n&#233; augmente-t-elle avec la taille du corpus d&#8217;entra&#238;nement ? L&#8217;utilisation
d&#8217;un lexique externe augmente-t-elle de fa&#231;on significative la pr&#233;cision de l&#8217;&#233;tiqueteur ? Avec
quelle taille de corpus d&#8217;entra&#238;nement obtient-on le meilleur &#233;tiqueteur ? Quelle est sa pr&#233;cision ?
&#192; partir de quelle taille de corpus d&#8217;entra&#238;nement l&#8217;&#233;tiqueteur obtenu peut-il &#234;tre utilis&#233; comme
pr&#233;-annotateur dans une campagne d&#8217;annotation manuelle qui consiste en la correction manuelle
de l&#8217;annotation automatique ? Les deux syst&#232;mes d&#8217;&#233;tiquetage, MElt et TreeTagger, conduisent-ils
&#224; des r&#233;sultats similaires concernant les questions pr&#233;c&#233;dentes ?
</p>
<p>Nous avons donc proc&#233;d&#233; &#224; l&#8217;entra&#238;nement de MElt et de TreeTagger sur 10 sous-corpus suc-
cessifs du corpus de r&#233;f&#233;rence Cref, dont la taille cro&#238;t de 2 000 &#224; 20 000 tokens. Nous avions
pr&#233;alablement mis de c&#244;t&#233; trois tranches de 500 tokens afin de servir d&#8217;&#233;chantillons de test. Pour
rendre nos r&#233;sultats comparables avec ceux pr&#233;sent&#233;s dans d&#8217;autres campagnes, l&#8217;&#233;valuation de
la pr&#233;cision s&#8217;est limit&#233;e aux seules &#233;tiquettes.
</p>
<p>Pour l&#8217;entra&#238;nement de TreeTagger, nous avons utilis&#233; comme lexique externe le lexique Mor-
phalou 2.0 (Romary et al., 2004). Nous avons d&#251; convertir Morphalou au format attendu par
TreeTagger puis le fusionner, pour chacun des 10 sous-corpus d&#8217;apprentissage, avec le lexique
qui en est extrait. Nous avons &#233;galement effectu&#233; des tests sans Morphalou, uniquement avec
un lexique endog&#232;ne. Pour l&#8217;entra&#238;nement de MElt, nous avons utilis&#233; la version du lexique Lefff
(Sagot, 2010) utilis&#233;e pour l&#8217;entra&#238;nement de la version standard de l&#8217;&#233;tiqueteur MElt pour le
fran&#231;ais. Le lexique externe &#233;tant utilis&#233; par MElt comme une source de traits pour le mod&#232;le
d&#8217;&#233;tiquetage, nous avons pu conserver le jeu de cat&#233;gories du lexique externe bien qu&#8217;il soit
diff&#233;rent des cat&#233;gories du corpus d&#8217;entra&#238;nement. Pour MElt, le lexique externe reste distinct du
lexique extrait du corpus d&#8217;entra&#238;nement.
</p>
<p>Premier constat : &#224; l&#8217;exception du premier &#233;tiqueteur entra&#238;n&#233; sur 2 000 tokens, les &#233;tiqueteurs
obtenus avec MElt sont syst&#233;matiquement meilleurs que ceux obtenus avec TreeTagger. Les
meilleurs scores, obtenus &#224; partir du corpus de 20 000 tokens, sont respectivement 96,9 % avec
MElt et 94,9 % avec TreeTagger. Compar&#233;s aux 85&#8211;90 % annonc&#233;s par (Eshkol et al., 2010) et
aux 80 % obtenus par A. Dister (c.p. du 24 janvier 2008 6), nos r&#233;sultats constituent donc une
am&#233;lioration significative. Mais cela masque des variations d&#8217;un &#233;chantillon &#224; un autre, ainsi
qu&#8217;au niveau de la moyenne (voir figure 6), mais surtout des diff&#233;rences entre jeux d&#8217;&#233;tiquettes.
</p>
<p>6. Diaporama disponible &#224; l&#8217;adresse : http ://rhapsodie.risc.cnrs.fr/docs/Dister_Syntaxe_240108.pdf
</p>
<p>109</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Deuxi&#232;me constat, sans surprise : l&#8217;utilisation du lexique externe am&#233;liore la pr&#233;cision de l&#8217;&#233;tique-
teur. Par exemple, sur le corpus de 2 000 tokens, TreeTagger n&#8217;atteint que 78,4 % de pr&#233;cision
sans lexique externe contre 90,9 % avec Morphalou. Pour ce m&#234;me corpus, la diff&#233;rence est
moins importante avec MElt, mais elle est significative : la pr&#233;cision passe de 84,9 % &#224; 88,1 %.
Avec 20 000 tokens, l&#8217;utilisation du lexique externe permet &#224; la pr&#233;cision de l&#8217;&#233;tiqueteur entra&#238;n&#233;
par MElt de passer de 95,5 % &#224; 96,9 %. On note que MElt, sans lexique externe, donne des
r&#233;sultats sup&#233;rieurs &#224; TreeTagger avec lexique externe d&#232;s que le corpus d&#8217;entra&#238;nement fait plus
de 12 000 tokens.
</p>
<p>FIGURE 6 &#8211; &#201;volution de la pr&#233;cision de l&#8217;annotation automatique par tranche de 2 000 tokens
</p>
<p>La figure 6 montre qu&#8217;&#224; partir de 6 000 tokens, les r&#233;sultats commencent &#224; progresser de mani&#232;re
moins marqu&#233;e, que ce soit avec MElt ou TreeTagger. Les pr&#233;cisions obtenues avec cette taille
de corpus (94,3% avec MElt) sont suffisantes pour lancer une campagne de correction telle
que nous la d&#233;crivons ci-dessus, apr&#232;s r&#233;-entra&#238;nement. Il n&#8217;est pas indispensable que le corpus
d&#8217;apprentissage soit plus volumineux. En tout cas, au vu de nos r&#233;sultats, il n&#8217;est pas utile
d&#8217;aller au-del&#224; de 10 000 tokens si l&#8217;on utilise TreeTagger. L&#8217;utilisation de MElt semble toutefois
pr&#233;f&#233;rable, avec des r&#233;sultats qui continuent &#224; cro&#238;tre jusqu&#8217;&#224; 20 000 tokens.
</p>
<p>6 Conclusion et perspectives
</p>
<p>Le corpus TCOF-POS (Cref + Cadd) est disponible sur le site du CNRTL 7 sous licence Creative
Commons BY-NC-SA 2.0 8, h&#233;rit&#233;e du corpus TCOF. Il contient un peu plus de 100 000 tokens,
dont un peu plus de 20 000 tokens de r&#233;f&#233;rence et 80 000 tokens obtenus gr&#226;ce &#224; la double-
annotation (par les deux &#233;tudiantes recrut&#233;es) puis adjudication par un expert linguiste (C.
</p>
<p>7. http ://cnrtl.fr/corpus/perceo/
8. http ://creativecommons.org/licenses/by-nc-sa/2.0/fr/
</p>
<p>110</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Benzitoun). Les meilleurs mod&#232;les d&#8217;&#233;tiquetage pour TreeTagger et pour MElt, qui ont une
pr&#233;cision respectivement de 94,9 % et 96,9 % &#224; ce stade de d&#233;veloppement du corpus, seront
&#233;galement mis &#224; disposition sous peu sur ce site (pour l&#8217;instant, seul le fichier param&#232;tre pour
TreeTagger est t&#233;l&#233;chargeable). Le lexique fusionn&#233; avec Morphalou est &#233;galement disponible &#224;
cette m&#234;me adresse. La version r&#233;-entra&#238;n&#233;e de TreeTagger a d&#233;j&#224; &#233;t&#233; utilis&#233;e par les concepteurs
du Corpus de Fran&#231;ais Parl&#233; Parisien 9 pour annoter leurs donn&#233;es.
</p>
<p>Dans le cadre d&#8217;une campagne de correction d&#8217;une pr&#233;-annotation automatique, nous avons mis
en &#233;vidence le seuil de 6 000 tokens comme base de d&#233;part minimale pour r&#233;-entra&#238;ner le logiciel.
A ce stade, on obtient de bons r&#233;sultats (94,3 % pour MElt et 93,6 % pour TreeTagger) et la
pr&#233;cision progresse de mani&#232;re moins marqu&#233;e. Mais cette recommandation est valable lorsque
le logiciel d&#8217;&#233;tiquetage est coupl&#233; &#224; un lexique externe. Or, dans le cadre de notre campagne
d&#8217;&#233;valuation des corrections manuelles, nous n&#8217;avons pas utilis&#233; de lexique externe pour r&#233;-
entra&#238;ner TreeTagger. Il faudra donc tester si les r&#233;sultats sont de meilleure qualit&#233; lorsque l&#8217;on
ajoute ce param&#232;tre, travail que nous effectuons &#224; l&#8217;heure actuelle.
</p>
<p>Remerciements
</p>
<p>Nous tenons &#224; remercier les 12 &#233;tudiants ayant collabor&#233; &#224; ce projet et plus particuli&#232;rement
M. Salcedo et M. Paquot, recrut&#233;es sp&#233;cifiquement pour faire l&#8217;annotation. De m&#234;me, L. B&#233;rard,
E. Jacquey, V. Meslard, S. Ollinger et E. Petitjean ont apport&#233; une contribution significative &#224; ce
projet. Nous souhaitons &#233;galement remercier l&#8217;ATILF pour son soutien financier dans le cadre d&#8217;un
projet interne. La participation de K. Fort a &#233;t&#233; financ&#233;e dans le cadre du programme Qu&#230;ro 10,
financ&#233; par OSEO, agence nationale de valorisation de la recherche. Celle de B. Sagot entre dans
le cadre du projet ANR EDyLex (ANR-09-CORD-008).
</p>
<p>R&#233;f&#233;rences
</p>
<p>ABEILL&#201;, A. et CL&#201;MENT, L. (2006). Annotation morpho-syntaxique. Les mots simples - Les mots
compos&#233;s Corpus Le Monde.
</p>
<p>ANDR&#201;, V. et CANUT, E. (2010). Mise &#224; disposition de corpus oraux interactifs : le projet TCOF
(traitement de corpus oraux en francais). Pratiques, 147/148:35&#8211;51.
</p>
<p>BENZITOUN, C. (2004). L&#8217;annotation syntaxique de corpus oraux constitue-t-elle un probl&#232;me
sp&#233;cifique ? In Actes de la conf&#233;rence RECITAL, pages 13&#8211;22, F&#232;s, Maroc.
</p>
<p>BLANC, O., CONSTANT, M., DISTER, A. et WATRIN, P. (2008). Corpus oraux et chunking. In Journ&#233;es
d&#8217;&#233;tude sur la parole (JEP), Avignon, France.
</p>
<p>BLANCHE-BENVENISTE, C. et JEANJEAN, C. (1987). Le Francais parl&#233;. Transcription et &#233;dition. Didier
&#201;rudition, Paris, France.
</p>
<p>BONNEAU-MAYNARD, H., ROSSET, S., AYACHE, C., KUHN, A. et MOSTEFA, D. (2005). Semantic
Annotation of the French Media Dialog Corpus. In InterSpeech, Lisbonne, Portugal.
</p>
<p>9. http ://cfpp2000.univ-paris3.fr/search-transcription-tt/
10. http://www.quaero.org
</p>
<p>111</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>BRANCA-ROSOFF, S., FLEURY, S., LEFEUVRE, F. et PIRES, M. (2010). Discours sur la ville. corpus de
francais parl&#233; parisien des ann&#233;es 2000 (CFPP2000). Rapport technique.
</p>
<p>CAMPIONE, E., V&#201;RONIS, J. et DEULOFEU, J. (2005). C-ORAL-ROM, Integrated Reference Corpora for
Spoken Romance Languages, &#233;dit&#233; par E. Cresti et M. Moneglia, chapitre 3. The French corpus,
pages 111&#8211;133. John Benjamins, Amsterdam, Hollande.
</p>
<p>COHEN, J. (1960). A Coefficient of Agreement for Nominal Scales. Educational and Psychological
Measurement, 20(1):37&#8211;46.
</p>
<p>DENIS, P. et SAGOT, B. (2009). Coupling an annotated corpus and a morphosyntactic lexicon for
state-of-the-art pos tagging with less human effort. In Proceedings of PACLIC 2009, Hong Kong,
Chine.
</p>
<p>DENIS, P. et SAGOT, B. (2012). Coupling an annotated corpus and a lexicon for state-of-the-art
POS tagging. Language Resources and Evaluation. &#192; para&#238;tre.
</p>
<p>DISTER, A. (2007). De la transcription &#224; l&#8217;&#233;tiquetage morphosyntaxique. Le cas de la banque de
donn&#233;es textuelle orale VALIBEL. Th&#232;se de doctorat, Universit&#233; de Louvain, Belgique.
</p>
<p>ESHKOL, I., TELLIER, I., TAALAB, S. et BILLOT, S. (2010). &#233;tiqueter un corpus oral par apprentissage
automatique &#224; l&#8217;aide de connaissances linguistiques. In 10th International Conference on
statistical analysis of textual data (JADT 2010), Rome, Italie.
</p>
<p>FORT, K. et SAGOT, B. (2010). Influence of Pre-annotation on POS-tagged Corpus Development.
In Proc. of the Fourth ACL Linguistic Annotation Workshop, Uppsala, Su&#232;de.
</p>
<p>HUET, S., GRAVIER, G. et S&#201;BILLOT, P. (2006). Peut-on utiliser les &#233;tiqueteurs morphosyntaxiques
pour am&#233;liorer la transcription automatique. In Actes des 26&#232;mes Journ&#233;es d&#8217;&#201;tudes sur la Parole
(JEP), Dinard, France.
</p>
<p>MARCUS, M., SANTORINI, B. et MARCINKIEWICZ, M. A. (1993). Building a large annotated corpus
of english : The penn treebank. Computational Linguistics, 19(2):313&#8211;330.
</p>
<p>MERTENS, P. (2002). Les corpus de francais parl&#233; ELICOP : consultation et exploitation. In
BINON, J., DESMET, P., ELEN, J., MERTENS, P. et SERCU, L., &#233;diteurs : Tableaux Vivants. Opstellen
over taal-en-onderwijs aangeboden aan Mark Debrock, pages 101&#8211;116. Universitaire Pers, Leuven,
Belgique.
</p>
<p>ROMARY, L., SALMON-ALT, S. et FRANCOPOULO, G. (2004). Standards going concrete : from LMF
to Morphalou. In Workshop on Electronic DictionariesWorkshop on Electronic Dictionaries, Coling
2004, Gen&#232;ve, Suisse.
</p>
<p>SAGOT, B. (2010). The Lefff, a freely available and large-coverage morphological and syntactic
lexicon for french. In 7th international conference on Language Resources and Evaluation (LREC
2010), La Vallette, Malte.
</p>
<p>SCHMID, H. (1997). New Methods in Language Processing, Studies in Computational Linguistics,
&#233;dit&#233; par D. Jones et H. Somers, chapitre Probabilistic part-of-speech tagging using decision
trees, pages 154&#8211;164. UCL Press, Londres.
</p>
<p>VALLI, A. et V&#201;RONIS, J. (1999). &#233;tiquetage grammatical de corpus oraux : probl&#232;mes et
perspectives. Revue Francaise de Linguistique Appliqu&#233;e, IV(2):113&#8211;133.
</p>
<p>VOORMANN, H. et GUT, U. (2008). Agile corpus creation. Corpus Linguistics and Linguistic Theory,
4(2):235&#8211;251.
</p>
<p>112</p>

</div></div>
</body></html>