TCOF-POS : un corpus libre de francais parlé
annoté en morphosyntaxe

Christophe Benzitounl Karen Fort2»3 Benoit Sagot4
(1) ATILE Nancy Université 8: CNRS, 44, avenue de la Libération, BP 30687, 54063 Nancy cedex
(2) INIST—CNRS, 2 allée de Brabois, 54500 Vandoeuvre—les—Nancy
(3) LIPN, Université Paris 13 8: CNRS, 99 av. J.B. Clément, 93430 Vrlletaneuse
(4) Alpage, INRIA Paris—Rocquencourt 8: Université Paris 7, Rocquencourt, France
christophe .benzi1:ou.natilf .fr, karen. fortinist .fr, benoit . sagotinria.fr

RESUME
Nous présentons dans cet article un travail portant sur la création d’un corpus de francais parlé
spontané annoté en morphosyntaxe. Nous détaillons la méthodologie suivie afin d’assurer le
controle de la qualité de la ressource ﬁnale. Ce corpus est d’ores et déja librement diffusé pour
la recherche et peut servir aussi bien de corpus d’apprentissage pour des logiciels que de base
pour des descriptions linguistiques. Nous présentons également les résultats obtenus par deux
étiqueteurs morphosyntaxiques entrainés sur ce corpus.

AB STRACT
TCOF-POS : A Freely Available POS-'I'agged Corpus of Spoken French

This article details the creation of TCOF-POS, the first freely available corpus of spontaneous
spoken French. We present here the methodology that was followed in order to obtain the best
possible quality in the ﬁnal resource. This corpus already is freely available and can be used as a
training/validation corpus for NLP tools, as well as a study corpus for linguistic research. We also
present the results obtained by two PO S-taggers trained on the corpus.

MOTS-CLES : Etiquetage morpho-syntaxique, francais parlé, ressources langagieres.
KEYWORDS: POS tagging, French, speech, language resources.

1 Introduction

L’annotation automatique du frangais parlé est généralement réalisée par le biais de pré-
traitements de corpus ou d’adaptation d’outils existant pour le texte (Dister, 2007; Blanc et aL,
2008). Une autre solution peut consister 5 masquer certains phénoménes tels que les “disﬂuences“
(répétitions, amorces de mots, etc.) (Valli et Véronis, 1999). Pourtant, l’utilisation d’étiqueteurs
automatiques élaborés pour et a partir de données écrites n’est pas une solution optimale étant
données les particularités des corpus oraux par rapport 21 l’écrit. Méme si l’étiquetage de corpus
oraux ne représente pas un probleme spécifique (Benzitoun, 2004), l’utilisation de modeles
entrainés sur des données écrites donne des résultats médiocres. Ainsi, nous avons testé Tree-
Tagger (Schmid, 1997), avec son modele standard pour le francais, sur un échantillon de 3 007
tokens extraits du corpus de référence décrit dans cet article et nous avons obtenu une précision
de seulement 83,1 %.

Actes de la conférence conjointe JEP-TALN-RECITAL 2012, volume 2: TALN, pages 99—112,
Grenoble, 4 au 8 juin 2012. ©2012 ATAI.A 8: AFCP

99

Un corpus du francais parlé annoté en morphosyntaxe librement disponible serait donc utile, non
seulement pour les logiciels d’annotau'on en morphosyntaxe, mais également pour améliorer les
systémes de transcription automatique (Huet et al., 2006) ou d’autres outils. Cependant, il n’existe
pas encore, a notre connaissance, de corpus de francais parlé spontané annoté en morphosyntaxe
(parties du discours et/ou lemmes) qui soit diffusé librement. Parmi les corpus annotés mais non
diffusés librement, on peut citer les projets elicop (Mertens, 2002), C-ORAL-ROM (Campione
et al., 2005), Valibel (Dister, 2007), Corpus de Francais Parlé Parisien (Branca-Rosoff et aL, 2010)
ou bien encore ESLO (Eshkol et aL, 2010).

Notre objectif est donc de développer et diffuser librement ‘a l’ensemble de la communauté
scientiﬁque un corpus pré-annoté automatiquement puis corrigé manuellement, dont la qualité
aura été précisément évaluée. I1 pourra servir notamment de corpus d’apprenu'ssage spéciﬁque
au frangais parlé et plus largement de corpus exploitable pour des recherches en linguistique ou
en Traitement Automatique des Langues ('I'AL).

Nous présentons tout d’abord le corpus de francais parlé TCOF (Traitement des Corpus Oraux du
Francais), puis la méthodologie utilisée pour l’annotau'on manuelle, les différentes évaluaﬁons
réalisées pendant la campagne et enﬁn les résultats obtenus par les étiqueteurs morphosyn-
taxiques entrainés sur une partie du corpus annoté TCOF-POS.

2 Présentation du corpus

Le corpus d’origine que nous avons annoté est celui du projet TCOF (André et Canut, 2010),

librement disponible sur le site du CNRTL 1. Ce corpus est constitué de transcriptions de données

orales recueillies dans des contextes aussi naturels que possible. Il comporte une partie d’interac-
tions entre adultes et une autre entre adultes et enfants. En ce qui concerne la partie adulte (la
seule que nous ayons exploitée jusqu’a présent), elle est composée :

— d’interactions sollicitées, dans lesquelles au moins deux locuteurs sont engagés dans des
récits de vie, d’événements ou d’expériences, ou dans des explications sur un savoir-faire
professionnel ou technique ;

— de conversations 21 batons rompus ou portant sur des thématiques spéciﬁques ;

— de données non sollicitées dans des situations publiques ou professionnelles : réunions pu-
bliques, activités professionnelles diverses.

De ce corpus, nous avons extrait un échantillon de 22 240 tokens 2, soit 11 transcriptions
différentes. Cet échantillon contient des conversations, des réunions professionnelles, ainsi
que des extraits d’une Assemblée Générale ‘a l’Université. L’intégralité des paroles prononcées
a été scrupuleusement retranscrite en orthographe standard, sans artifice ou aménagement
orthographique (donc sans ponctuation), suivant en cela les recommandations de (Blanche-
Benveniste et Jeanjean, 1987) 3 largement diffusées et utilisées. Elles sont au format généré par
le logiciel Transcriber (trs - XML).

1. http ://cnrt1.fr/corpus/tcof/

2. Notre conception de la notion de token est assez élémentaire (aucune insertion possible).

3. Les conventions de transcription sont disponibles sur le site suivant
http : / / cn1t1.fr/ corpusl tcof/ TC0FConvem1'ons.pdf

100

Ces transcriptions sont automatiquement converties en texte brut ‘a l’aide d’une feuille de style
XSLT (qui élimine l’intégralité des balises XML), puis d’une série d’expressions régulieres qui
supprime les informations non désirées, telles que les pauses. Le texte ﬁnal contient les mentions
des locuteurs (L1, L2, etc.), l’intégraJité des paroles prononcées, ainsi que les multi-transcriptions.
Il s’agit donc de transcriptions brutes non retouchées, dont voici un exemple :

L1 et puis je crois que c’est en je crois je crois méme que c’est en zone industrielle

L2 ouais ouais je pense aussi g:a doit pas étre en ville

L1 oui mais

L2 en Belgique aussi il y a des trucs euh un genre de grand tr- enﬁn un genre de grande
galerie en Belgique et puis c’est que des magasins de fringues aussi

Les transcriptions ont été faites dans le cadre d’un cours de deuxieme année de Sciences
du Langage a l’Université Nancy 2, puis revues par des enseignants de Sciences du Langage.
L’anonymisation, quant ‘a elle, a été réalisée manuellement par des étudiants-vacataires. A la
lecture, ils devaient repérer les toponymes, anthroponymes, etc. puis les remplacer par un
symbole et insérer un son dans la portion de signal sonore correspondante.

3 Méthodologie

L’annotau'on totalement manuelle de corpus étant tres coﬁteuse, nous avons procédé, comme
décrit dans (Marcus et aL, 1993), a une correction manuelle de corpus pré-annotés automatique-
ment. La nature du pré-annotateur, ainsi que les modalités de la correction manuelle different
selon les étapes du processus, comme nous allons le voir dans cette section. Toutefois, toutes
les pré-annotations ont été produites par différentes instances du systeme 'I'reeTagger (Schmid,
1997), qui fournit pour chaque token d’entrée une étiquette morphosyntaxique et un lemme.

Comme indiqué en introduction, l’utilisation comme pré-annotateur pour un corpus de parole
spontanée transcrite, d’un étiqueteur morphosyntaxique entrainé sur un corpus écrit n’est pas
adaptée. Parmi les phénomenes qui posent probleme, lesquels ne sont pas totalement absents
des corpus écrits mais y sont bien plus rares (Benzitoun, 2004), on peut citer :

— les répétitions de mots ou de groupes de mots (ga ga redevient ga redevient Ze bordel comme pa),
— les reformulations (peut-étre se’parer completement euh junior euh homme enﬁn euh adulte),

— les ruptures de construction (ouais ouais que de la gueule que de la),

— les amorces de mots (moi j’aurais p- j’aurais pas mis de pantalon),

— les incises (euh on considérait que former les hommes et c’est toujours euh en en en vigueur ca

hein former les les en- les les enfants d’aujourd’hui c’est aussi former les hommes de demain),

— les formes non conventionnelles (tu sais genre trop ve’nere ; avoir du matos en entre’e de mag),
— les particules discursives (hein, eh ben, etc.). ..

Pour ne prendre que deux exemples, la version standard de TreeTagger pour le frangais considére
que bon est systématiquement un adjectif et quoi un pronom, alors qu’ils sont majoritairement des
particules discursives. De plus, nos transcriptions ne sont pas segmentées en « phrases » (Blanche-
Benveniste et Jeanjean, 1987), ce qui peut également poser des problemes aux outils. Par
exemple, l’étiquette SENT (pour sentence) indiquant une frontiére de phrase doit obligatoirement
étre présente dans le lexique servant pour l’apprent1'ssage de TreeTagger (méme s’il ne s’en sert
pas par la suite). En consequence, nous avons procédé, des que possible, ‘a l’entrainement de
versions de TreeTagger a partir des annotations déja obtenues sur notre corpus.

101

La méthodologie retenue, décrite en détail dans cette partie, peut étre résumée comme suit :

1. Déﬁnition de critéres de tokenisation et d’identification des composés, puis tokenisation
automatique ;

2. Déﬁnition d’un jeu d’étiquettes adapté a la parole spontanée transcrite;

3. Création d’un corpus de référence CI-ef de 22 240 tokens par correction d’une pré-annotation
automatique, effectuée par deux experts linguistes :
— Les 10 000 premiers tokens de Cmf ont été pré-annotés avec la version standard de
TreeTagger;
— Les 12 240 tokens suivants de CI-ef ont été pré-annotés avec une version de TreeTagger
entrainée sur les 10 000 premiers;

4. Ré-annotation par deux étudiantes d’environ 7 500 tokens du corpus de référence Cm,
(suivie d’une phase d’adjudication), afin d’évaluer la qualité des annotations dans deux
conﬁgurations distinctes :

— environ 6 000 tokens ont été pré-annotés par la version standard de TreeTagger;
— environ 1 500 tokens ont été pré-annotés avec une version de TreeTagger entrainée sur
les 16 312 premiers tokens de CI-ef;
L’objectif était ici de mesurer l’impact de la différence de qualité entre pré-annotateurs en
termes de vitesse d’annotation et de précision du résultat de l’étape manuelle;

5. Application de cette méthodologie a un plus grand nombre d’étudiants pour en valider la
robustesse;

6. Annotation par deux étudiantes d’un corpus additionnel Cadd de 80 000 nouveaux tokens,
pré-annotés avec la version de TreeTagger entrainée sur la totalité de Cmf.

Nous avons appliqué pour cette campagne les bonnes pratiques actuelles en annotation manuelle
de corpus, qui consistent ‘a évaluer le plus tot possible l’accord inter-annotateurs et de mettre
‘a jour le guide d’annotation (Bonneau-Maynard et al., 2005). La répétition réguliére de ce
processus conduit a ce qu’on appelle maintenant l’annotation agile (Voormann et Gut, 2008).

3.1 Tokenisation et gestion des composés

Le corpus ayant fait l’objet d’une pré-annotation (voir section 3.3), nous avons pris comme base
la tokenisation par défaut de TreeTagger, qui repose notamment sur un ﬁchier de composés. Mais
ce dernier s’est avéré insufﬁsant (par exemple, paroe que reste découpé en deux tokens distincts
mais puisqu’ils en un seul token). Nous l’avons donc complété au fur et a mesure, en respectant
le critére suivant : toute séquence dans laquelle il est possible d’insérer un élément est découpée
en plusieurs tokens, aﬁn d’exclure les unités discontinues. Ainsi, un peu est découpé en deux
tokens (car on peut trouver un tout petit peu).

3.2 Un jeu d’étiquettes adapté 5 la parole spontanée transcrite

Aﬁn de bénéﬁcier au mieux des ressources développées pour l’écrit et de limiter le travail de
correction, tout en prenant en considération les phénoménes spéciﬁques ‘a la parole spontanée
cités ci-dessus, nous avons décidé d’utiliser un jeu d’étiquettes basé au départ sur les étiquettes
par défaut fournies par TreeTagger. Nous l’avons complété a‘1l’aide de (Abeillé et Clément, 2006).

102

Les répétitions, reformulations, etc. n’ont pas fait l’objet de traitements spéciﬁques, chacun des
tokens a la catégorie qu’il a habituellement (ex : le[DET] le[DET] le[DET] chat). Au ﬁnal, méme
si les identiﬁants des étiquettes sont différents, les catégories retenues sont quasiment identiques
a (Abeillé et Clément, 2006), avec toutefois un peu moins de sous-catégories (notamment aucune
pour les adverbes et les adjectifs) et l’ajout de la catégorie << auxiliaire >> ainsi que de trois
étiquettes spéciﬁques a l’oral : MLT (multi-transcription), TRC (amorce de mot) et LOC (locuteur)
(cf. tableau 1). Aﬁn de nous aider dans la rédaction du manuel d’annotation, nous nous sommes
d’ailleurs inspirés de (Abeillé et Clément, 2006). Notre jeu d’étiquettes comprend 62 étiquettes.

En outre, il a été afﬁné durant la phase de constitution du corpus servant de référence. En effet,
nous voulions que les étiquettes soient apposées de maniére aussi systématique que possible pour
que nos choix soient réversibles et que les modiﬁcations soient automatisables, autant que faire
se peut. De ce fait, méme si cela peut paraitre discutable d’un point de vue théorique, nous avons
privilégié les choix qui potentiellement générent le moins de ﬂuctuations entre annotateurs. Par
exemple, la distinction entre participe passé et adjectif n’est pas aisée et plutot que d’obtenir une
annotation de qualité moindre, nous avons préféré neutraliser celle-ci. Ainsi, chaque fois que
la forme verbale existe (sauf cas de changement notoire de sens), nous avons annoté « verbe ».
Dans le cas contraire, nous avons annoté « adjectif ».

Nous avons également décidé d’essayer de limiter les cas de transferts d’une catégorie vers
une autre (trans-catégorisation). En effet, ceux-ci auraient artiﬁciellement été limités aux cas
rencontrés dans le corpus ‘a annoter, sans possibilité d’avoir une vision globale du phénomene.
De plus, cela aurait complexiﬁé la tache de correction. Ainsi, dans rouler tranquille, tranquille est
considéré comme un adjectif et non comme un adverbe (ce qui, de toute facon, est discutable
d’un point de vue théorique). Enﬁn, il n’a pas été possible d’exc1ure totalement les cas d’étiquettes
limitées a un mot unique. Ainsi, l’étiquette « particule interrogative » ne s’ut1'lise que pour est-oe
qu-e/i et « prédéterminant » uniquement pour tous.

3.3 Création du sous-corpus de référence

Comme indiqué ci-dessus, la création de la premiere tranche de 10 000 tokens du corpus de
référence Cm; de 22 240 tokens a été réalisée en utilisant comme pré-annotateur la version
standard de TreeTagger, entrainée sur un corpus écrit. Nous (L. Bérard et C. Benzitoun) avons
ensuite corrigé ces pré-annotations en plusieurs passes. Nous avons tout d’abord effectué des
remplacements automatiques, lorsque les modiﬁcations étaient systématiques ou que l’étiquette
majoritaire n’était pas celle apposée par défaut par le logiciel (ce qui est le cas pour bon (ADJ / INT)
et quoi (PRO :int/ INT), par exemple). Ensuite, nous nous sommes répartis les données a corriger
et, apres les avoir intégralement traitées, nous nous les sommes échangées pour révision. Nous
avons ensuite discuté des cas o1‘1 nous n’étions pas en accord jusqu"a trouver des solutions.
Nous avons effectué ces étapes plusieurs fois, jusqu"a obtenir des annotations ﬁables. Le guide
d’annotation était mis a jour a chaque étape.

Nous avons par ailleurs généré automatiquement des ﬁchiers de fréquences, afin de faciliter
le repérage des erreurs. A ainsi été calculée la fréquence de chaque étiquette pour un méme
lemme ou un méme token, ce qui nous a permis d’identiﬁer et de corriger quelques erreurs
supplémentaires. Par exemple, C.E. ayant été annoté 1 fois NAM et 3 fois NOM :sg (pour un
méme lemme C.E.), cette derniere étiquette a été attribuée aux 4 occurrences. De méme, cela
nous a permis de corriger deux occurrences de du, indﬁment annotées DET :ind.

103

AUX
AUX

AUX

DET -
DET -
DET
DET :int
DET :par
DET .
DET .
EPE

ETR
FNO

INT

au
au participe

au
au

au

au

(tout ))

mots

noyau (oui, non, ac-
cord, etc.)
et

cursives

NOM :trc
NAM :trc
VER :trc
ADJ :trc

nom propre
nom commun

pronom

pronom
pronom

pronom

ce

amorces 1'1'10tS
sans
au
au

au
au parﬁcipe

au

au
au
fait
au
nom commun
1'l01'1'l

TABLE 1 — Jeu d’ét1'quettes du corpus TCOF-POS

104

Nous avons ensuite appliqué les résultats obtenus par Fort et Sagot (2010) sur l’intérét d’une
pré-annotation avec un outil de qualité moyenne. Ainsi, une fois 1a premiere tranche de 10 000
tokens annotés, nous avons ré-entrainé TreeTagger sur ce sous-corpus (mais sans utiliser de
lexique externe) et avons pré-annoté les transcriptions suivantes de CE, (12 240 tokens) avec ce
nouvel outil. La méme méthodologie que celle utilisée pour corriger les 10 000 premiers tokens
nous a permis de ﬁnaliser le corpus de référence Cref de 22 240 tokens.

3.4 Création du sous-corpus additionnel

Le corpus diffusé est composé pour une part du sous-corpus de référence Cm, et pour une autre
part d’un autre sous-corpus additionnel Cadd corrigé par deux étudiantes (de L3 et M2 de Sciences
du Langage) recrutées spéciﬁquement pour cette tache. Dans un premier temps, aﬁn d’évaluer a
priori la méthodologie prévue pour l’annotation de Cadd, nous avons mené une campagne de tests
en nous servant de Cm, comme référence. Pour ce faire, les deux étudiantes ont eu 15 ﬁchiers
extraits de CI-ef d’environ 500 tokens chacun4 a corriger dans un ordre contraint. Les 12 premiers
ﬁchiers avaient été pré-annotés par la version standard de TreeTagger. Aﬁn de mesurer l’impact
de la qualité du pré-annotateur, les 3 demiers ﬁchiers avaient été pré-annotés par une version
de TreeTagger ré-entrainée ‘a partir d’un extrait de 16 312 tokens de la référence Cref (et sans
lexique externe). Naturellement, ces tokens forment un sous-ensemble de CI-ef disjoint des 15
ﬁchiers a ré-annoter. Les étudiantes avaient l’interdiction d’échanger des informations durant la
phase d’annotation.

La correction a été effectuée dans un tableur, les cellules contenant les étiquettes étant mu-
nies d’une liste déroulante se limitant au jeu d’étiquettes déﬁni ci-dessus. La saisie était donc
contrainte. Une fois la correction terminée, les ﬁchiers annotés en paralléle ont été compa-
rés automatiquement. Les cas de divergence entre les deux annotateurs ont ainsi été repérés
automatiquement et corrigés par un expert5.

Dans le cadre de ce travail, les mesures suivantes ont été effectuées :

— le temps mis par les étudiantes pour annoter chaque ﬁchier;

— la précision de chaque ﬁchier par rapport a la référence;

— l’accord inter-annotateurs des étudiantes (Kappa de Cohen (Cohen, 1960)) ;

— la précision aprés fusion et adjudication.

L’évaluation de leurs annotations sur ces 15 ﬁchiers est reproduite ci-dessous (ﬁgures 1 et 2 et
tableau 2). Elle tient compte de la lemmatisation et des parties du discours.

1e 2e 3e 4e 5e 6e 7e 8e 9e 10e 11e 12e 13e 14e 15e
107 71 80 67 60 60 57 65 50 50 52 47 32 32 31

TABLE 2 — Temps d’annotation (en minutes)

Entre le 12‘ et le 13‘ ﬁchier, la différence de temps est vraisemblablement imputable au change-
ment de pré-annotation par TreeTagger.

4. Cette taille a été retenue car nous avons observé qu’e11e permet une correction rapide et une attention soutenue
sans étre obligé de s’interrompre en cours d’annotation.

5. Pour des raisons pratiques, i1 n’a pas été possible de conﬁer cette phase a un expert externe. La personne qui
1’a réalisée a également collaboré a la réalisation du corpus servant de référence, ce qui peut représenter un biais
méthodologique.

105

——L

Fl'eC|5lOﬂ (en %)
.-. AD In
0 N

I
I
\
\
~
I
z /
I
x

 

123A5a7s91n1112131413

Numéro du ﬁchier

FIGURE 1 — Evolution de la précision des deux étudiantes

91 \ _ I
W / \

12:ass7»:—ai-31112131415 avzamuniamis
Numéru du ﬁchier Numém du ﬁchier

Kappaten '96]

33
:
.2.
:
9
us
5
‘E
n.

 

FIGURE 2 — Evolution du Kappa (a gauche) des 2 étudiantes et de la précision apres adjudication
(a droite)

La qualité des corrections apres ré-entrainement et pré-étiquetage ainsi que le faible temps
de correction (pour les 3 derniers ﬁchiers, donc) nous ont paru sufﬁsants pour valider notre
méthodologie et ainsi poursuivre l’élaboration du corpus. Sur les 3 derniers ﬁchiers, la précision
moyenne est de 98,03 % en ne tenant compte que des étiquettes. Nous sommes donc passes
a l’annotation par les deux étudiantes du corpus Cadd. Elles ont ainsi recu le méme jeu de 160
nouveaux ﬁchiers de 500 tokens chacun, pré-annotés par la version ré-entrainée de TreeTagger.
En 60 heures, elles ont corrigé 80 000 tokens chacune, ce qui fait une moyenne d’un peu plus
de 21 minutes par ﬁchier de 500 tokens. Sur l’ensemble, l’accord inter-annotateurs (Kappa de
Cohen (Cohen, 1960)) est en moyenne de 96,5 % et le temps moyen consacré a l’adjudication de
2 min. 45 sec par ﬁchier.

4 Elargissement de l’évaluation

Aﬁn d’évaluer le caractere robuste de notre méthodologie, nous avons élargi l’évaluation a
plus d’étudiants. En effet, nous comptons augmenter de maniere importante la quantité de
ﬁchiers corrigés dans les années a venir et nous voulons vériﬁer si notre méthodologie donne
des résultats comparables quels que soient les correcteurs. Pour ce faire, nous avons adopté

106

la meme méthodologie que celle décrite ci-dessus, a savoir une double-annotation de chaque
ﬁchier puis une adjudication pour les cas de divergence uniquement. Cette evaluation a porté sur
les corrections fournies par 10 étudiants en Sciences du Langage a l’Université Nancy 2 (L3 et
M2) dans le cadre de deux enseignements. A chaque binéme, nous avons donné 6 ﬁchiers (4
ﬁchiers pré-annotés avec le TreeTagger de base et 2 ﬁchiers avec le TreeTagger ré-entrainé) a
corriger dans un ordre contraint. Dans cette experience, comme dans la précédente, les étudiants
devaient corriger les lemmes en plus des étiquettes. Dans la suite de ce travail, les mesures que
nous présentons tiennent compte 21 la fois des lemmes et étiquettes (sauf précision contraire).

4.1 Temps d’annotau'on et accord inter—annotateurs

En ce qui concerne le temps d’annotation, nous avons observé une diminution systématique avec
une nette différence entre les 4 premiers ﬁchiers et les deux derniers (voir tableau 3).

1e annot. 2e annot. 3e annot. 4e annot. Se annot. 6e annot.
110,1 101,8 79,2 72,5 41,3 39,7

TABLE 3 — Temps d’annotation (en minutes)

Au-dela de la diminution du temps de correction inhérente a une meilleure maitrise des étudiants,
il parait difﬁcile d’expliquer la diminution du temps entre le quatrieme et le cinquieme ﬁchier par
un autre facteur que le basculement entre le TreeTagger standard et la version ré-entrainée. Le
méme phénomene peut étre observé concernant l’accord inter—annotateurs (cf ﬁgure 3). Le coef-
ﬁcient d’accord inter—annotateurs présenté ici est, comme précédemment, 1e K de Cohen (Cohen,
1960).

3 4
Numém du ﬁchier

 

FIGURE 3 — Evolution de l’accord inter—annotateurs (kappa) des étudiants

Dans la ﬁgure 3, la courbe noire représente l’évolution de la moyenne des accords inter-
annotateurs, de méme que dans les graphiques suivants.

107

4.2 Précision

Outre une diminution signiﬁcative du temps d’annotation et une augmentation de 1’accord inter-
annotateurs, nous avons également constaté une importante augmentation de la precision en
moyenne pour chaque étudiant (cﬁ ﬁgure 4). On observe encore une fois une nette augmentation
entre le quatrieme et le cinquieme ﬁchier, et ce chez tous les étudiants. La ﬁgure 5 indique 1a
précision de chaque ﬁchier apres fusion et adjudication.

53
E
.1’.
E
.9
.2
u
an
._
D.

3 4
Numéro du ﬁchier

 

FIGURE 4 — Evolution de la précision de chaque étudiant

Précision (en Wm)

3 4
Numéro du ﬁchier

 

FIGURE 5 — Evolution de la précision de chaque ﬁchier corrigé apres fusion et adjudication

Finalement, sur les deux derniers ﬁchiers annotés, le taux de precision moyen est respectivement
de 97,28 % et 97,12 % en évaluant les erreurs portant a la fois sur les lemmes et les étiquettes. Si
l’on prend en compte seulement les étiquettes, le taux de precision moyen est alors respectivement
de 97,42 % et de 97,8 % pour ces memes ﬁchiers, ce qui est légerement inférieur a ce qui a

108

été relevé précédemment pour les deux étudiantes. Cependant, nous estimons que cela permet
d’afF1rmer que les principes adoptés permettent d’obtenir des corpus annotés de qualité proche,
et ce quelle que soit 1a personne qui corrige.

5 Premiers résultats de l’annotation automatique

Pour effectuer les premiers tests concernant1’apprentissage automatique, notre choix s’est porté
sur deux étiqueteurs morphosyntaxiques : ME1t (Denis et Sagot, 2009, 2012), étiqueteur état
de 1’art pour le francais, et TreeTagger, utilisé comme pré-annotateur pour constituer 1e corpus,
et largement utilisé bien qu’i1 ne soit pas celui qui donne les meilleurs résultats ‘a l’heure
actuelle (Denis et Sagot, 2009; Eshkol et al., 2010). Tous deux sont librement disponibles et
multi-plateformes.

Notre objecﬁf était d’étudier la courbe d’apprenu'ssage, et ce sous plusieurs angles : la précision
de 1’étiqueteur entrainé augmente-t-elle avec la taille du corpus d’entrainement? L’uti1isation
d’un1exique externe augmente-t-elle de facon signiﬁcative 1a précision de 1’étiqueteur? Avec
quelle taille de corpus d’entrainement obtient-on 1e meilleur étiqueteur? Quelle est sa précision?
A partir de quelle taille de corpus d’entrainement 1’ét1'queteur obtenu peut-i1 étre utilisé comme
pré-annotateur dans une campagne d’annotation manuelle qui consiste en la correction manuelle
de 1’annotation automatique ? Les deux systémes d’étiquetage, ME1t et TreeTagger, conduisent-ils
a des résultats similaires concernant les questions précédentes ?

Nous avons donc procédé ‘a 1’entrainement de ME1t et de TreeTagger sur 10 sous-corpus suc-
cessifs du corpus de référence Cref, dont 1a taille croit de 2 000 a 20 000 tokens. Nous avions
préalablement mis de cété trois tranches de 500 tokens aﬁn de servir d’échantiJlons de test. Pour
rendre nos résultats comparables avec ceux présentés dans d’autres campagnes, 1’éva1uation de
la précision s’est1irnitée aux seules étiquettes.

Pour 1’entrainement de TreeTagger, nous avons utilisé comme lexique exteme 1e lexique Mor-
phalou 2.0 (Romary et al., 2004). Nous avons dﬁ convertir Morphalou au format attendu par
TreeTagger puis 1e fusionner, pour chacun des 10 sous-corpus d’apprentissage, avec le lexique
qui en est extrait. Nous avons également effectué des tests sans Morphalou, uniquement avec
un lexique endogéne. Pour 1’entrainement de ME1t, nous avons utilisé la version du lexique Lejﬁ‘
(Sagot, 2010) utilisée pour Pentrainement de la version standard de 1’étiqueteur ME1t pour le
francais. Le lexique externe étant utilisé par ME1t comme une source de traits pour le modéle
d’étiquetage, nous avons pu conserver 1e jeu de catégories du lexique exteme bien qu’i1 soit
différent des catégories du corpus d’entrainement. Pour ME1t, 1e lexique externe reste distinct du
lexique extrait du corpus d’entrainement.

Premier constat : a1’excepu'on du premier étiqueteur entrainé sur 2 000 tokens, les étiqueteurs
obtenus avec ME1t sont systématiquement meilleurs que ceux obtenus avec TreeTagger. Les
meilleurs scores, obtenus a partir du corpus de 20 000 tokens, sont respectivement 96,9 % avec
ME1t et 94,9 % avec TreeTagger. Comparés aux 85-90 % annoncés par (Eshkol et al., 2010) et
aux 80 % obtenus par A. Dister (c.p. du 24 janvier 2008 5), nos résultats constituent donc une
amélioration signiﬁcative. Mais cela masque des variations d’un échantillon ‘a un autre, ainsi
qu’au niveau de la moyenne (voir ﬁgure 6), mais surtout des différences entre jeux d’étiquettes.

6. Diaporama disponible a1’adresse : http ://rhapsodierisc.cnrs.fr/docs/Dister_Syntaxe_240108.pdf

109

Deuxieme constat, sans surprise : l’utilisation du lexique externe améliore la précision de l’étique-
teur. Par exemple, sur le corpus de 2 000 tokens, TreeTagger n’atteint que 78,4 % de précision
sans lexique externe contre 90,9 % avec Morphalou. Pour ce méme corpus, la différence est
moins importante avec MElt, mais elle est signiﬁcative : la précision passe de 84,9 % a 88,1 %.
Avec 20 000 tokens, l’uti1isation du lexique externe permet a la précision de 1’étiqueteur entrainé
par MElt de passer de 95,5 % a 96,9 %. On note que MElt, sans lexique externe, donne des
résultats supérieurs a TreeTagger avec lexique externe des que le corpus d’entrainement fait plus
de 12 000 tokens.

98

U)
G»

KO
4:-

Lo
M

Lo
0

----TreeTagger +Morph

so
so

----- -- TreeTagger —Morph
— - MELT +lei‘ff

Precision (en %)
U0
U1

: MELT Jefff

oo
4:

an
M

 

an
O

‘b

'\§v§<<§§§&§@@&@&$§@@

N N N N

Taille corpus entrafnement (en nb tokens)

FIGURE 6 — Evolution de la précision de l’annotation automatique par tranche de 2 000 tokens

La ﬁgure 6 montre qu’a partir de 6 000 tokens, les résultats commencent a progresser de maniere
moins marquée, que ce soit avec MElt ou TreeTagger. Les précisions obtenues avec cette taille
de corpus (94,3% avec MElt) sont sufﬁsantes pour lancer une campagne de correction telle
que nous la décrivons ci-dessus, apres ré-entrainement. Il n’est pas indispensable que le corpus
d’apprentissage soit plus volumineux. En tout cas, au vu de nos résultats, il n’est pas utile
d’aller au-dela de 10 000 tokens si l’on utilise TreeTagger. L’utilisation de MElt semble toutefois
préférable, avec des résultats qui continuent a croitre jusqu’a 20 000 tokens.

6 Conclusion et perspectives

Le corpus TCOF-POS (Cref + Cadd) est disponible sur le site du CNRTL7 sous licence Creative
Commons BY-NC-SA 2.0 8, héritée du corpus TCOE Il contient un peu plus de 100 000 tokens,
dont un peu plus de 20 000 tokens de référence et 80 000 tokens obtenus grace a la double-
annotation (par les deux étudiantes recrutées) puis adjudication par un expert linguiste (C.

7. http ://cnrtl.fr/corpus/perceo/
8. http ://creativecommons.org/licenses/by-nc-sa/2.0/fr/

110

Benzitoun). Les meilleurs modeles d’étiquetage pour TreeTagger et pour MElt, qui ont une
précision respectivement de 94,9 % et 96,9 % ‘a ce stade de développement du corpus, seront
également mis ‘a disposition sous peu sur ce site (pour l’instant, seul le ﬁchier parametre pour
TreeTagger est téléchargeable). Le lexique fusionné avec Morphalou est également disponible a

cette méme adresse. La version ré-entrainée de TreeTagger a deja eté utilisée par les concepteurs
du Corpus de Francais Parlé Parisien9 pour annoter leurs données.

Dans le cadre d’une campagne de correction d’une pré-annotation automatique, nous avons mis
en évidence le seuil de 6 000 tokens comme base de départ minimale pour ré-entrainer le logiciel.
A ce stade, on obtient de bons résultats (94,3 % pour MElt et 93,6 % pour TreeTagger) et la
précision progresse de maniére moins marquée. Mais cette recommandation est valable lorsque
le logiciel d’étiquetage est couplé a un lexique externe. Or, dans le cadre de notre campagne
d’évaluation des corrections manuelles, nous n’avons pas utilisé de lexique externe pour ré-
entrainer 'I'reeTagger. Il faudra donc tester si les résultats sont de meilleure qualité lorsque l’on
ajoute ce paramétre, travail que nous effectuons a l’heure actuelle.

Remerciements

Nous tenons ‘a remercier les 12 étudiants ayant collaboré ‘a ce projet et plus particulierement
M. Salcedo et M. Paquot, recrutées spéciﬁquement pour faire l’annotation. De méme, L. Bérard,
E. Jacquey, V Meslard, S. Ollinger et E. Petitjean ont apporté une contribution signiﬁcative a ce
projet. Nous souhaitons également remercier l’ATILF pour son soutien ﬁnancier dans le cadre d’un
projet interne. Ia participation de K. Fort a été ﬁnancée dans le cadre du programme Quaero 1°,
ﬁnancé par OSEO, agence nationale de valorisation de la recherche. Celle de B. Sagot entre dans

le cadre du projet ANR EDyLex (ANR-09-CORD-008).

Références

ABEILLE, A. et CLEMENT, L. (2006). Annotation morpho-syntaxique. Les mots simples - Les mots
composés Corpus Le Monde.

ANDRE’, V et CANUT, E. (2010). Mise ‘a disposition de corpus oraux interactifs : le projet TCOF
(traitement de corpus oraux en francais). Pratiques, 147/ 148:35—51.

BENZITOUN, C. (2004). 1.’annotation syntaxique de corpus oraux constitue-t-elle un probleme
spéciﬁque ? In Actes de la conférenoe RECITAL, pages 13-22, Fés, Maroc.

BLANC, 0., CONSTANT, M., DISTER, A. et WATRIN, P. (2008). Corpus oraux et chunking. In Journées
d’e’tude sur la parole (JEP), Avignon, France.

BLANCHE-BENVENISTE, C. et JEANJEAN, C. (1987). Le Francais parle’. Transcription et édition. Didier
Erudition, Paris, France.

BONNEAU-MAYNARD, H., ROSSET, S., AYACHE, C., KUHN, A. et MOSTEFA, D. (2005). Semantic
Annotation of the French Media Dialog Corpus. In InterSpeech, Lisbonne, Portugal.

9. http ://cfpp2000.univ-pa1is3.fr/search-transcription-rd
10. http://mnnquaero .013

111

BRANCA-ROSOFF, S., FLEURY, S., LEFEUVRE, F. et PIRES, M. (2010). Discours sur la ville. corpus de
francais parlé parisien des années 2000 (CFPP2000). Rapport technique.

CAMPIONE, E., VERONIS, J. et DEULOFEU, J. (2005). C-ORAL-ROIVI, Integrated Reference Corpora for
Spoken Romance Languages, édité par E. Cresti et M. Moneglia, chapitre 3. The French corpus,
pages 111-133. John Benjamins, Amsterdam, Hollande.

COHEN, J. (1960). A Coefﬁcient of Agreement for Nominal Scales. Educational and Psychological
Measurement, 20(1) :37—46.

DENIS, R et SAGOT, B. (2009). Coupling an annotated corpus and a morphosyntactic lexicon for
state-of-the-art pos tagging with less human effort. In Proceedings of PACLIC 2009, Hong Kong,
Chine.

DENIS, P. et SAGOT, B. (2012). Coupling an annotated corpus and a lexicon for state-of-the-art
POS tagging. Language Resources and Evaluation. A paraitre.

DISTER, A. (2007). De la transcription 61 l’e’tiquetage morphosyntaxique. Le cas de la banque de
données textuelle orale VALIBEL. These de doctorat, Université de 1.ouvain, Belgique.

EsH1<oL, I., TELLIER, I., TAALAB, S. et BILLOT, S. (2010). étiqueter un corpus oral par apprentissage
automatique ‘a l’aide de connaissances linguistiques. In 10th International Conference on
statistical analysis of textual data (JADT 2010), Rome, Italie.

FORT, K. et SAGOT, B. (2010). Inﬂuence of Pre-annotation on POS-tagged Corpus Development.
In Proc. of the Fourth ACL Linguistic Annotation Workshop, Uppsala, Suede.

HUET, S., GRAVIER, G. et SEBILLOT, R (2006). Peut-on utiliser les étiqueteurs morphosyntaxiques
pour améliorer la transcription automatique. In Actes des 26emes Journe’es d’Etudes sur la Parole
(JEP), Dinard, France.

MARCUS, M., SANTORINI, B. et MARCINKIEWICZ, M. A. (1993). Building a large annotated corpus
of english : The penn treebank. Computational Linguistics, 19(2):313—330.

MERTENS, P. (2002). Les corpus de francais parlé ELICOP : consultation et exploitation. In
B1NoN, J., DESMET, R, ELEN, J., MERTENS, P. et SERCU, L., éditeurs : Tableaux Vivants. Opstellen
over taal-en-onderwijs aangeboden aan Mark Debrock, pages 101-116. Universitaire Pers, Leuven,
Belgique.

ROMARY, L., SALMON-ALT, S. et FRANCOPOULO, G. (2004). Standards going concrete : from LMF
to Morphalou. In Workshop on Electronic Dictionariesworkshop on Electronic Dictionaries, Coling
2004, Genéve, Suisse.

SAGOT, B. (2010). The Leﬂf, a freely available and large-coverage morphological and syntactic
lexicon for french. In 7th international conference on Language Resources and Evaluation (LREC
2010), 1a Vallette, Malte.

SCHMID, H. (1997). New Methods in Language Processing, Studies in Computational Linguistics,
édité par D. Jones et H. Somers, chapitre Probabilistic part-of-speech tagging using decision
trees, pages 154-164. UCL Press, Londres.

VALLI, A. et VERONIS, J. (1999). étiquetage grammatical de corpus oraux : problémes et
perspectives. Revue Francaise de Linguistique Applique’e, IV(2):113—133.

VOORMANN, H. et GUT, U. (2008). Agile corpus creation. Corpus Linguistics and Linguistic 'Iheory,
4(2):235—251.

112

