<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
"http://www.w3.org/TR/html4/loose.dtd">
<html><head><title>Un crit&#232;re de coh&#233;sion th&#233;matique fond&#233; sur un graphe de cooccurrences</title>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
</head>
<body>
<div style="page-break-before:always; page-break-after:always"><div><p>Actes de la conf&#233;rence conjointe JEP-TALN-RECITAL 2012, volume 2: TALN, pages 183&#8211;195,
Grenoble, 4 au 8 juin 2012. c&#169;2012 ATALA &amp; AFCP
</p>
<p>Un crit&#232;re de coh&#233;sion th&#233;matique fond&#233; sur
un graphe de cooccurrences
</p>
<p>Cl&#233;ment de Groc1, 2 Xavier Tannier2, 3 Claude de Loupy1
(1) Syllabs, 15 rue Jean-Baptiste Berlier, 75013 Paris
</p>
<p>(2) Univ. Paris-Sud, 91403 Orsay Cedex
(3) LIMSI-CNRS, B.P. 133, 91403 Orsay Cedex
</p>
<p>cdegroc@limsi.fr, xtannier@limsi.fr, loupy@syllabs.com
</p>
<p>R&#201;SUM&#201;
Dans cet article, nous d&#233;finissons un nouveau crit&#232;re de coh&#233;sion th&#233;matique permettant de
pond&#233;rer les termes d&#8217;un lexique th&#233;matique en fonction de leur pertinence. Le crit&#232;re s&#8217;inspire
des approches Web as corpus pour accumuler des connaissances exog&#232;nes sur un lexique. Ces
connaissances sont ensuite mod&#233;lis&#233;es sous forme de graphe et un algorithme de marche al&#233;atoire
est appliqu&#233; pour attribuer un score &#224; chaque terme. Apr&#232;s avoir &#233;tudi&#233; les performances et
la stabilit&#233; du crit&#232;re propos&#233;, nous l&#8217;&#233;valuons sur une t&#226;che d&#8217;aide &#224; la cr&#233;ation de lexiques
bilingues.
</p>
<p>ABSTRACT
Topical Cohesion using Graph Random Walks
</p>
<p>In this article, we propose a novel metric to weight specialized lexicons terms according to their
relevance to the underlying thematic. Our method is inspired by Web as corpus approaches
and accumulates exogenous knowledge about a specialized lexicon from the web. Terms
cooccurrences are modelled as a graph and a random walk algorithm is applied to compute
terms relevance. Finally, we study the performance and stability of the metric and evaluate it in a
bilingual lexicon creation context.
</p>
<p>MOTS-CL&#201;S : Coh&#233;sion th&#233;matique, graphe de cooccurrences, marche al&#233;atoire.
</p>
<p>KEYWORDS: Thematic relevance, cooccurrence graph, random walk.
</p>
<p>183</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>1 Introduction
</p>
<p>Les lexiques et les terminologies sont des &#233;l&#233;ments essentiels du traitement automatique des
langues. Ils sont utilis&#233;s dans une grande vari&#233;t&#233; de t&#226;ches, allant de la cat&#233;gorisation de textes &#224;
l&#8217;analyse d&#8217;opinions. Dans cet article, nous nous int&#233;ressons plus particuli&#232;rement aux lexiques
dits th&#233;matiques ou sp&#233;cialis&#233;s, c&#8217;est-&#224;-dire compos&#233;s de termes pertinents pour un domaine
particulier. La Table 1 pr&#233;sente un extrait de lexique th&#233;matique sur le domaine de l&#8217;astronomie.
</p>
<p>soleil &#233;toile rayon gamma &#233;toile &#224; neutron masse solaire . . .
plan&#232;te disque d&#8217;accr&#233;tion naine blanche proto-&#233;toile pulsar . . .
</p>
<p>astronome quasar astronomie trou noir neutron . . .
</p>
<p>TABLE 1 &#8211; Extrait de lexique th&#233;matique sur l&#8217;astronomie
</p>
<p>La construction manuelle de tels lexiques est une t&#226;che laborieuse et co&#251;teuse. C&#8217;est pourquoi
l&#8217;utilisation du Web ou de traducteurs automatiques comme appui pour la cr&#233;ation de lexiques
et de corpus sp&#233;cialis&#233;s est une id&#233;e maintenant largement r&#233;pandue (Baroni et Bernardini,
2004; Groc et al., 2011; Kilgarriff et Grefenstette, 2003; Wan, 2009). Bien que l&#8217;utilit&#233; de telles
approches ne soit plus &#224; d&#233;montrer, une &#233;tape de validation manuelle reste requise.
</p>
<p>Dans cet article, nous proposons un nouveau crit&#232;re de coh&#233;sion th&#233;matique permettant de
pond&#233;rer les termes d&#8217;un lexique th&#233;matique en fonction de leur pertinence pour le th&#232;me. Nous
utilisons le Web comme source de corpus sp&#233;cialis&#233;s sur les termes d&#8217;un lexique th&#233;matique.
Nous mod&#233;lisons ensuite les cooccurrences entre les termes du lexique sous la forme d&#8217;un graphe
orient&#233; o&#249; les sommets sont les termes du lexique et les arcs d&#233;notent la cooccurrence de ces
termes. Ce graphe peut &#234;tre per&#231;u comme un graphe de recommandation o&#249; l&#8217;apparition de deux
termes dans un m&#234;me document signifie qu&#8217;ils se recommandent l&#8217;un l&#8217;autre. Cette observation
nous am&#232;ne naturellement &#224; utiliser un algorithme de marche al&#233;atoire (random walk (Cohen,
2010; Page et al., 1999)) attribuant une pertinence globale &#224; chaque sommet du graphe.
</p>
<p>Ce crit&#232;re de coh&#233;sion th&#233;matique peut avoir de multiples applications. Dans le cadre de lexiques
sp&#233;cialis&#233;s construits automatiquement (Baroni et Bernardini, 2004; Groc et al., 2011), ordonner
les &#233;l&#233;ments du lexique par leur score de coh&#233;sion peut r&#233;duire la charge de validation manuelle
ou m&#234;me limiter la dispersion au fil des it&#233;rations. Dans le cadre de la traduction assist&#233;e, une
valeur de coh&#233;sion peut repr&#233;senter un score de confiance utile pour le traducteur. C&#8217;est d&#8217;ailleurs
par cette derni&#232;re application que nous choisissons d&#8217;&#233;valuer notre crit&#232;re dans cet article.
</p>
<p>L&#8217;article pr&#233;sente tout d&#8217;abord bri&#232;vement les travaux en Web as corpus dont notre approche
d&#233;coule, ainsi que ceux centr&#233;s sur les graphes de cooccurences et les algorithmes de marche
al&#233;atoire (section 2). Dans une 3&#232;me section, nous pr&#233;sentons le mod&#232;le de graphe et l&#8217;algorithme
de marche al&#233;atoire utilis&#233;s pour le calcul du crit&#232;re de coh&#233;sion th&#233;matique. Nous &#233;valuons
ensuite ce dernier sur une t&#226;che de filtrage de lexiques th&#233;matiques traduits automatiquement
(section 4). Nous concluons enfin (section 5) en sugg&#233;rant plusieurs pistes envisag&#233;es.
</p>
<p>184</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>2 Travaux li&#233;s
</p>
<p>L&#8217;utilisation du Web comme source de documents (Kilgarriff et Grefenstette, 2003) est une id&#233;e
maintenant largement r&#233;pandue. Pour acc&#233;der aux documents Web, deux approches sont cou-
ramment mises en &#339;uvre : soumettre un ensemble de requ&#234;tes &#224; un moteur de recherche (Baroni
et Bernardini, 2004; Ghani et al., 2005) ou parcourir directement le Web (crawling) (Baroni et
Ueyama, 2006). Le parcours du Web permet une meilleure sp&#233;cification du besoin mais n&#233;cessite
un investissement important. De plus, les efforts des moteurs de recherche pour garantir des
r&#233;sultats de qualit&#233; doivent &#234;tre reproduits (filtrage des pages de spam). Au contraire, l&#8217;utilisation
d&#8217;un moteur de recherche grand public comme point d&#8217;entr&#233;e au Web offre un acc&#232;s simple et peu
co&#251;teux pour la communaut&#233; de Traitement Automatique des Langues. Nous adoptons ici cette
approche afin de constituer un ensemble de connaissances exog&#232;nes sur les lexiques th&#233;matiques
fournis en entr&#233;e.
</p>
<p>Dans cet article, nous d&#233;finissons un crit&#232;re bas&#233; sur les cooccurrences des termes d&#8217;une m&#234;me
th&#233;matique pour d&#233;terminer leur lien avec le th&#232;me. Ces travaux partagent donc certaines
hypoth&#232;ses avec les travaux en similarit&#233; s&#233;mantique et notamment les analyses distribution-
nelles (Pereira et al., 1993; Baker et McCallum, 1998; Rajman et al., 2000) ou la d&#233;sambigu&#239;sation
s&#233;mantique via des r&#233;seaux de cooccurrences (Dorow et Widdows, 2003; Ferret, 2004). En effet,
notre graphe de cooccurrences mod&#233;lise explicitement les cooccurrences de premier ordre mais
l&#8217;application d&#8217;un algorithme de propagation d&#8217;importance de type PageRank permet la prise en
compte de cooccurrences d&#8217;ordres sup&#233;rieurs.
</p>
<p>Enfin, l&#8217;algorithme TextRank (Mihalcea et Tarau, 2004) est &#233;troitement li&#233; &#224; nos travaux. Les
auteurs mod&#233;lisent la cooccurrence des mots dans une fen&#234;tre de taille N sous forme de graphe
non-orient&#233; et appliquent un algorithme de marche al&#233;atoire afin de d&#233;tecter les mots-cl&#233;s
saillants. Nous nous d&#233;marquons cependant de ces travaux en au moins deux points : nous consi-
d&#233;rons les cooccurrences au niveau du document (snippet dans nos &#233;valuations) et mod&#233;lisons
ces derni&#232;res par un graphe orient&#233; (plus de d&#233;tails en Section 3).
</p>
<p>3 Un crit&#232;re de coh&#233;sion th&#233;matique
</p>
<p>&#201;tant donn&#233; un lexique th&#233;matiqueLT compos&#233; de N termes,LT = (t1, t2, . . . , tN ), nous voulons
calculer un vecteur de poids wLT = (w1, w2, . . . , wN ) o&#249; chaque poids wi mesure la pertinence
du terme t i pour la th&#233;matique T .
</p>
<p>3.1 Recueil de connaissances exog&#232;nes
</p>
<p>Dans ces travaux, nous adoptons une approche Web as corpus, qui nous permet de cr&#233;er ra-
pidement des corpus sp&#233;cialis&#233;s en nous appuyant sur un moteur de recherche g&#233;n&#233;raliste.
Nous proposons d&#232;s lors de constituer, pour chaque terme t i , un corpus Ci correspondant au M
meilleurs r&#233;sultats renvoy&#233;s par un moteur de recherche pour la requ&#234;te &quot;&lt;t_i&gt;&quot;.
</p>
<p>L&#8217;unit&#233; d&#8217;information que nous consid&#233;rons dans le cadre de cet article est le snippet, le court
extrait de page Web renvoy&#233; par le moteur de recherche. En effet, si prendre en compte le
document entier permettrait en th&#233;orie de b&#233;n&#233;ficier d&#8217;un contexte plus large et plus riche, cela
</p>
<p>185</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>pose surtout de nombreux probl&#232;mes. D&#8217;une part, t&#233;l&#233;charger les documents renvoy&#233;s par le
moteur de recherche rallonge consid&#233;rablement le temps de calcul. D&#8217;autre part, il est ensuite
indispensable d&#8217;op&#233;rer un nettoyage des pages Web, et en particulier de supprimer les menus,
les publicit&#233;s ou les balises HTML, dans le but de ne conserver que le minimum de contenu non
informationnel 1. L&#8217;&#233;valuation finale d&#233;pend donc beaucoup de la qualit&#233; de ce nettoyage, ce qui
la rend plus difficilement interpr&#233;table. Enfin, le caract&#232;re local des snippets peut permettre de
r&#233;duire le bruit pouvant appara&#238;tre dans les pages Web.
</p>
<p>Nous avons utilis&#233; le moteur de recherche Bing 2 comme source de snippets. Ces derniers sont
compos&#233;s de portions de textes de 155 caract&#232;res en moyenne issus du corps des pages Web et
contenant les termes de la requ&#234;te.
</p>
<p>3.2 Coh&#233;sion th&#233;matique et graphe de cooccurrences
</p>
<p>&#201;tant donn&#233; un lexique th&#233;matique LT , nous proposons une premi&#232;re d&#233;finition de notre crit&#232;re
comme suit : le poids wi d&#8217;un terme t i est &#233;gal au nombre de termes du lexique (t i exclu)
cooccurrant avec t i dans le corpus Ci . Plus formellement, le poids wi d&#8217;un terme t i est d&#233;fini par :
</p>
<p>wi =
&#8721;
</p>
<p>t j&#8712;L &#8224;T
nt j ,Ci (1)
</p>
<p>o&#249; nt j ,Ci est le nombre d&#8217;occurrences du terme t j dans l&#8217;ensemble des documents du corpus Ci et
</p>
<p>L &#8224;T =LT \ {t i}, c&#8217;est-&#224;-dire l&#8217;ensemble des termes du lexique LT , t i exclu.
Cette m&#234;me d&#233;finition peut &#234;tre mod&#233;lis&#233;e sous la forme d&#8217;un graphe (Figure 1). Soit un graphe
orient&#233; G =&lt; V, E &gt;, o&#249; V est l&#8217;ensemble des sommets (V = LT ) et E l&#8217;ensemble des arcs.
Chaque arc e(t i , t j) symbolise l&#8217;apparition du terme t i dans le corpus C j de t j . Les arcs sont
pond&#233;r&#233;s en fonction du nombre d&#8217;occurrences du terme t i dans C j . Notre approche Web as
corpus nous diff&#233;rencie des pr&#233;c&#233;dents travaux (Mihalcea et Tarau, 2004) visant &#224; mod&#233;liser les
cooccurrences sous forme de graphe non-orient&#233; : en effet, pour deux termes t i et t j et leurs
corpus associ&#233;s Ci et C j , l&#8217;apparition du terme t i dans le corpus C j constitue un indice du &#8220;vote&#8221;
de t i pour t j . Cependant, cette relation n&#8217;est pas sym&#233;trique puisque les corpus Ci et C j sont
distincts. En cons&#233;quence, nous optons pour un mod&#232;le de graphe orient&#233;.
</p>
<p>Le poids d&#8217;un terme tel que d&#233;fini par l&#8217;&#233;quation 1 est alors &#233;quivalent au degr&#233; entrant de ce
terme dans le graphe, c&#8217;est-&#224;-dire la somme des poids des arcs entrants.
</p>
<p>Cette nouvelle mod&#233;lisation graphique nous am&#232;ne &#224; int&#233;grer les poids du voisinage entrant d&#8217;un
sommet dans le calcul du poids de celui-ci. Nous rectifions alors la premi&#232;re d&#233;finition de notre
crit&#232;re et proposons la d&#233;finition suivante : le poids wi d&#8217;un terme t i est &#233;gal &#224; la somme des
poids des termes du lexique cooccurrant avec t i dans le corpus Ci . De plus, nous normalisons
cette somme afin que le poids d&#8217;un terme soit r&#233;parti entre tous les termes auxquels il est li&#233;.
</p>
<p>1. Ce probl&#232;me est d&#8217;ailleurs un th&#232;me de recherche &#224; part enti&#232;re f&#233;d&#233;r&#233; par la campagne d&#8217;&#233;valuation CLEANE-
VAL (http://cleaneval.sigwac.org.uk).
</p>
<p>2. http://www.bing.com
</p>
<p>186</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>LT
t1
t2
t3
t4
t5
</p>
<p>-t3-------
----t2----
-------t1-
-----t2---
</p>
<p> t2
</p>
<p> t3
</p>
<p> t1
2
</p>
<p>1
</p>
<p>Corpus C1 du terme t1
</p>
<p>FIGURE 1 &#8211; Mod&#233;lisation des cooccurrences sous forme de graphe orient&#233;
</p>
<p>Formellement, nous d&#233;finissons le poids wi d&#8217;un terme t i comme :
</p>
<p>wi =
</p>
<p>&#8721;
t j&#8712;L &#8224;T nt j ,Ci &#183;w j&#8721;
</p>
<p>t j&#8712;L &#8224;T nt j ,Ci
(2)
</p>
<p>Cette nouvelle d&#233;finition est &#8220;r&#233;cursive&#8221; dans le sens o&#249; la pertinence d&#8217;un terme du lexique est
d&#233;finie en fonction de la pertinence des autres termes du lexique apparaissant dans son corpus. Il
est ainsi possible de voir la pertinence d&#8217;un terme t i d&#233;fini en fonction de la pertinence d&#8217;un terme
t j , elle-m&#234;me d&#233;fini en fonction la pertinence de t i . D&#8217;un point de vue graphique, ce ph&#233;nom&#232;ne
se traduit alors simplement par un cycle dans le graphe de cooccurrences.
</p>
<p>L&#8217;&#233;quation 2 est en fait proche de l&#8217;algorithme de marche al&#233;atoire PageRank (Page et al., 1999)
et peut &#234;tre r&#233;solue par un algorithme it&#233;ratif sous certaines conditions. Cette version na&#239;ve de
l&#8217;algorithme pose cependant deux probl&#232;mes :
</p>
<p>1. L&#8217;algorithme ne g&#232;re pas correctement les sommets sans arcs sortant (appel&#233;s &#8220;dangling
nodes&#8221; ou &#8220;rank sink&#8221; dans la litt&#233;rature) : il n&#8217;est pas souhaitable qu&#8217;un terme ne renvoyant
que des documents ext&#233;rieurs &#224; la th&#233;matique obtienne un poids important. Par exemple,
si un terme de notre lexique LT est un mot outil (&#8220;et&#8221;), il poss&#232;dera de nombreux liens
entrant mais potentiellement aucun lien sortant : il accumulera it&#233;rativement un poids
important.
</p>
<p>2. La convergence vers une unique solution n&#8217;est pas garantie pour notre graphe (Langville et
Meyer, 2005; Farahat et al., 2006).
</p>
<p>Pour r&#233;soudre le premier probl&#232;me, nous ajoutons un lien des sommets sans arcs sortant vers
un sommet virtuel et un lien de ce sommet virtuel vers tous les sommets du graphe. Le poids
des sommets sans arcs sortant est ainsi redistribu&#233; &#224; tous les sommets du graphe. Concernant le
second probl&#232;me, nous appliquons la solution de Page et Brin (Page et al., 1999) et ajoutons une
probabilit&#233; de t&#233;l&#233;portation uniforme &#224; chaque it&#233;ration de l&#8217;algorithme ce qui garantit la forte
connectivit&#233; du graphe et la convergence vers une solution unique (&#233;quation 3).
</p>
<p>187</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>wi,n+1 =
(1&#8722;&#945;)
</p>
<p>N
+&#945; &#183;
</p>
<p>&#8721;
t j&#8712;L &#8224;T nt j ,Ci &#183;w j,n&#8721;
</p>
<p>t j&#8712;L &#8224;T nt j ,Ci
(3)
</p>
<p>o&#249; N est le nombre de sommets du graphe (c&#8217;est &#224; dire le nombre de termes du lexique) et &#945; est
un facteur d&#8217;amortissement traditionnellement fix&#233; &#224; 0.85 (Page et al., 1999; Mihalcea et Tarau,
2004). Nous utilisons &#233;galement cette valeur de &#945; pour nos exp&#233;riences.
</p>
<p>L&#8217;algorithme 1 r&#233;capitule l&#8217;int&#233;gralit&#233; du calcul du crit&#232;re de coh&#233;sion th&#233;matique.
</p>
<p>Algorithme 1 Crit&#232;re de coh&#233;sion th&#233;matique
1: Entr&#233;es : LT : termes t i , i &#8712; [1, N]
</p>
<p>M : nombre de documents t&#233;l&#233;charg&#233;s par requ&#234;te
&#945; : facteur d&#8217;amortissement
</p>
<p>// T&#233;l&#233;chargement du corpus
2: Pour tout terme t i &#8712; LT faire
3: Soumettre t i &#224; un moteur de recherche
4: T&#233;l&#233;charger M documents comme corpus Ci
5: Fin Pour
</p>
<p>// Initialisation
6: Pour tout terme t i &#8712; LT faire
7: wi,1 = 1/N
8: Fin Pour
</p>
<p>// Proc&#233;dure it&#233;rative de calcul des poids
9: n = 1
</p>
<p>10: Tant que (non convergence) faire
11: Pour tout terme t i &#8712; LT faire
12: wi,n+1 =
</p>
<p>(1&#8722;&#945;)
N
</p>
<p>+&#945; &#183;
&#8721;
</p>
<p>t j&#8712;L &#8224;T nt j ,Ci &#183;w j,n&#8721;
t j&#8712;L &#8224;T nt j ,Ci
</p>
<p>13: Fin Pour
14: Normalisation des poids :
</p>
<p>&#8721;
i wi,n+1 = 1
</p>
<p>15: n = n+ 1
16: Fin Tant que
17: Retourner wn
</p>
<p>Pour am&#233;liorer la correspondance entre les lexiques et les documents issus du Web, une s&#233;rie de
normalisations suppl&#233;mentaires est appliqu&#233;e : conversion des termes en minuscules, racinisation
(stemming) et normalisation des caract&#232;res unicode (accents, . . . ).
</p>
<p>188</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Notons que le crit&#232;re propos&#233; traite les ph&#233;nom&#232;nes d&#8217;ambigu&#239;t&#233;s graphiques de la langue
(homographie) de la fa&#231;on souhait&#233;e. Par exemple, si le terme &#8220;jaguar&#8221; est soumis &#224; un moteur de
recherche actuel, il est fort probable que ce dernier renvoie des r&#233;sultats diversifi&#233;s &#224; propos de
l&#8217;animal mais &#233;galement de la marque de voiture, de la console de jeu ou du syst&#232;me d&#8217;exploitation
MacOS. Le nombre de cooccurrences avec les termes du lexique sera donc plus limit&#233;, conduisant
&#224; un score plus faible, soulignant ainsi qu&#8217;un terme ambig&#252;e contribue moins &#224; la coh&#233;sion
th&#233;matique.
</p>
<p>4 &#201;valuation
</p>
<p>4.1 T&#226;che
</p>
<p>Dans cet article, nous &#233;valuons l&#8217;apport de notre crit&#232;re pour l&#8217;aide &#224; la cr&#233;ation de lexiques
th&#233;matiques bilingues &#224; partir de lexiques monolingues. Comme mentionn&#233; dans l&#8217;introduction,
nous envisageons de nombreuses applications pour le crit&#232;re propos&#233; dont notamment le boots-
trapping de lexiques th&#233;matiques monolingues. Cependant, la t&#226;che de cr&#233;ation de lexiques
th&#233;matiques bilingues, claire et facilement reproductible, fournit une &#233;valuation objective de
notre crit&#232;re de coh&#233;sion.
</p>
<p>Partant de lexiques th&#233;matiques monolingues, une approche commune pour la cr&#233;ation de
lexiques th&#233;matiques bilingues est d&#8217;utiliser un outil de traduction automatique en ligne tel que
Google Translate 3. Cependant, ces outils ne permettent pas d&#8217;int&#233;grer une notion de contexte
th&#233;matique dans le processus de traduction simplement. Ainsi, le terme simple &#8220;avocat&#8221; non
int&#233;gr&#233; &#224; une phrase, par exemple, sera traduit par ces outils &#8220;avocado&#8221; ou &#8220;lawyer&#8221; en anglais,
indiff&#233;remment du fait qu&#8217;il appartient &#224; un lexique juridique ou culinaire. Une validation
manuelle laborieuse est donc n&#233;cessaire pour supprimer les traductions erron&#233;es.
</p>
<p>Nous proposons d&#8217;appliquer notre crit&#232;re de coh&#233;sion th&#233;matique aux lexiques traduits, attribuant
ainsi &#224; chaque traduction un score de confiance. Le tri des lexiques en fonction de ce score permet
alors de r&#233;duire le temps n&#233;cessaire &#224; leur validation.
</p>
<p>4.2 Donn&#233;es
</p>
<p>Nous avons utilis&#233; trois lexiques bilingues fran&#231;ais/anglais sp&#233;cialis&#233;s sur trois th&#232;mes diff&#233;rents :
&#8211; Astronomie (The Astronomy Thesaurus 4) ;
&#8211; M&#233;dical (Unified Medical Language System - UMLS 5) ;
&#8211; Statistiques (International Statistical Institute 6).
Un exemple de termes issus de chaque lexique bilingue est donn&#233; Table 2.
</p>
<p>3. http://translate.google.com
4. http://msowww.anu.edu.au/library/thesaurus/
5. http://www.nlm.nih.gov/research/umls/
6. http://isi.cbs.nl/glossary/
</p>
<p>189</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Astronomie Statistiques
Anglais Fran&#231;ais Anglais Fran&#231;ais
</p>
<p>afterglow r&#233;manence Birnbaum&#8217;s inequality in&#233;galit&#233; de Birnbaum
celestial coordinates coordonnee celeste geometric mean moyenne g&#233;om&#233;trique
</p>
<p>asteroids asteroide K-test test K de Mann
dwarf stars etoile naine invariant invariant
bow shocks onde de choc en form d&#8217;arc cross spectrum spectre crois&#233;
</p>
<p>M&#233;dical
Anglais Fran&#231;ais
</p>
<p>wandering spleen rate flottante
dimethoxyphenylethylamine dim&#233;thoxyph&#233;nyl&#233;thylamine
</p>
<p>wolman disease maladie de wolman
antimalarials antipaludiques
</p>
<p>optical illusions illusions optiques
</p>
<p>TABLE 2 &#8211; Extrait des lexiques th&#233;matiques bilingues utilis&#233;s pour l&#8217;&#233;valuation
</p>
<p>Une s&#233;rie de traitements a &#233;t&#233; appliqu&#233;e &#224; chaque lexique dans le but d&#8217;en am&#233;liorer la qualit&#233; ou
l&#8217;utilisation pour notre &#233;valuation. Ainsi, nous avons supprim&#233; les termes apparaissant entre cro-
chets ou parenth&#232;ses dans les lexiques Astronomie et Statistiques. Le lexique M&#233;dical pr&#233;sentant
des termes trop ambigus pour &#234;tre nettoy&#233;s automatiquement (par exemple 3-pyridinecarboxylic
acid, 1,4-dihydro-2,6-dimethyl-5-nitro-4-(2-(trifluoromethyl)phenyl)-, methyl ester), nous avons
simplement supprim&#233; les termes contenant une parenth&#232;se ou une virgule.
</p>
<p>Nous avons ensuite trait&#233; le cas des traductions multiples de la mani&#232;re suivante : lorsqu&#8217;un
terme de la langue source poss&#233;dait plusieurs traductions dans la langue cible, nous n&#8217;avons
conserv&#233; que le terme le plus proche (au sens de la distance de Damerau-Levenshtein (Damerau,
1964; Levenshtein, 1966)) de la traduction automatique 7. Ainsi dans l&#8217;exemple &#8220;Afterglow&#8221;&#8658;
&#8220;Postluminescence ou Remanence&#8221; issu du lexique &#8220;Astronomie&#8221;, nous n&#8217;avons conserv&#233; que le
terme &#8220;Remanence&#8221; car il est le plus proche de la traduction automatique trouv&#233;e : &#8220;r&#233;manence&#8221;.
</p>
<p>Le lexique UMLS comprenant plus de 19 000 termes, nous avons choisi de ne travailler que sur
un &#233;chantillon de ce dernier. Nous avons donc tir&#233; al&#233;atoirement deux s&#233;ries de 2 000 termes
que nous d&#233;signons comme lexiques M&#233;dical-1 et M&#233;dical-2.
</p>
<p>Nous obtenons enfin les lexiques suivants :
&#8211; Astronomie (2 940 termes) ;
&#8211; Statistiques (2 752 termes) ;
&#8211; M&#233;dical-1 (2 000 termes) ;
&#8211; M&#233;dical-2 (2 000 termes).
</p>
<p>4.3 M&#233;thode
</p>
<p>Nous traduisons chaque lexique th&#233;matique d&#8217;une langue source vers une langue cible (par
exemple Astronomie fr&#8594; en ou Astronomie en&#8594; fr) &#224; l&#8217;aide du moteur de traduction Google
</p>
<p>7. voir section 4.3 pour la m&#233;thode de traduction automatique
</p>
<p>190</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Translate. Le lexique r&#233;sultant est alors pond&#233;r&#233; avec le crit&#232;re de coh&#233;sion th&#233;matique puis
ordonn&#233; et compar&#233; avec la r&#233;f&#233;rence.
</p>
<p>Le choix de Google Translate est justifi&#233; par le fait que ce moteur propose une tr&#232;s large
couverture, ce qui en fait un candidat id&#233;al pour traiter nos lexiques sp&#233;cialis&#233;s. De plus, les
r&#233;centes &#233;valuations du NIST ont montr&#233; que l&#8217;outil de Google propose des performances &#233;tat de
l&#8217;art quant &#224; la qualit&#233; des traductions produites (NIST, 2005, 2008).
</p>
<p>La comparaison entre les termes traduits et les termes des lexiques originaux est r&#233;alis&#233;e &#224; l&#8217;aide
d&#8217;une mesure ad hoc incluant la suppression des d&#233;terminants en d&#233;but de terme (&#8220;le bleu de
bromothymol&#8221;&#8658; &#8220;bleu de bromothymol&#8221;) et une distance d&#8217;&#233;dition de Damerau-Levenshtein (Da-
merau, 1964; Levenshtein, 1966). Nous avons consid&#233;r&#233; un terme comme valide s&#8217;il est au plus &#224;
une distance d&#8217;&#233;dition de 1 du terme de r&#233;f&#233;rence, autorisant ainsi une l&#233;g&#232;re marge d&#8217;erreur due
au moteur de traduction ou &#224; la r&#233;f&#233;rence (singuliers transform&#233;s en pluriels, espace remplac&#233;
par un tiret, . . . ).
</p>
<p>La mesure de pr&#233;cision moyenne non-interpol&#233;e (uninterpolated average precision - UAP (Manning
et Sch&#252;tze, 1999)) est employ&#233;e pour &#233;valuer la validit&#233; de l&#8217;ordre des termes traduits.
</p>
<p>4.4 &#201;valuation du crit&#232;re
</p>
<p>Nous &#233;valuons notre crit&#232;re Coh&#233;sion-RW comparativement &#224; une baseline Hasard, obtenue par
le simple tri al&#233;atoire de la liste de traduction, et &#224; la premi&#232;re version du crit&#232;re Coh&#233;sion-DEG
(&#233;quation 1). La baseline Hasard est obtenue en calculant une macro-moyenne sur dix tris
al&#233;atoires successifs. Nous fixons le nombre de snippets t&#233;l&#233;charg&#233;s pour chaque requ&#234;te (la
valeur M de l&#8217;algorithme 1) &#224; 200 documents. Les r&#233;sultats sont pr&#233;sent&#233;s &#224; la Table 3.
</p>
<p>Th&#232;me Hasard Coh&#233;sion-DEG Coh&#233;sion-RW
</p>
<p>Astronomie
en&#8594; fr 0.429 0.494 0.512
fr&#8594; en 0.553 0.664 0.678
</p>
<p>Statistiques
en&#8594; fr 0.382 0.663 0.711
fr&#8594; en 0.488 0.667 0.705
</p>
<p>M&#233;dical-1
en&#8594; fr 0.530 0.683 0.735
fr&#8594; en 0.620 0.707 0.718
</p>
<p>M&#233;dical-2
en&#8594; fr 0.522 0.662 0.699
fr&#8594; en 0.638 0.739 0.750
</p>
<p>TABLE 3 &#8211; Pr&#233;cision moyenne non-interpol&#233;e (UAP) pour le classement des termes des lexiques
traduits.
</p>
<p>Nous constatons que l&#8217;algorithme de marche al&#233;atoire fournit les meilleurs r&#233;sultats avec gain
sur la baseline Hasard allant de 15,8 % (M&#233;dical-1 fr&#8594; en) &#224; 86,1 % (Statistiques en&#8594; fr). La
baseline fournit une id&#233;e de la qualit&#233; des traductions produites par le moteur de traduction.
Ainsi, il semble que les lexiques M&#233;dical-1 et M&#233;dical-2 soient les mieux traduits. Au contraire
le lexique Statistiques semble &#234;tre le plus difficile &#224; traduire. Le coefficient de corr&#233;lation de
Pearson entre la pr&#233;cision du Hasard et le gain obtenu vaut -0,61 ce qui semble signifier qu&#8217;ils
sont fortement corr&#233;l&#233;s n&#233;gativement : plus la traduction est de bonne qualit&#233; et plus le gain est
</p>
<p>191</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Th&#232;me 50 100 150 200
</p>
<p>Astronomie
en&#8594; fr 0.500 0.507 0.507 0.512
fr&#8594; en 0.672 0.676 0.680 0.678
</p>
<p>Statistiques
en&#8594; fr 0.666 0.695 0.704 0.711
fr&#8594; en 0.678 0.691 0.702 0.705
</p>
<p>M&#233;dical-1
en&#8594; fr 0.710 0.719 0.726 0.735
fr&#8594; en 0.677 0.693 0.706 0.718
</p>
<p>M&#233;dical-2
en&#8594; fr 0.672 0.678 0.687 0.699
fr&#8594; en 0.702 0.723 0.735 0.750
</p>
<p>TABLE 4 &#8211; Pr&#233;cision moyenne non-interpol&#233;e (UAP) pour le classement des termes des lexiques
traduits avec le crit&#232;re Coh&#233;sion-RW pour diff&#233;rentes valeurs de NB_DOCS.
</p>
<p>faible. Cependant ce r&#233;sultat n&#8217;est pas statistiquement significatif (la p-value vaut 0,106).
</p>
<p>Nous &#233;valuons ensuite l&#8217;influence du nombre de snippets t&#233;l&#233;charg&#233;s par requ&#234;te sur les r&#233;sultats
du crit&#232;re propos&#233; (Table 4). Nous constatons que la pr&#233;cision moyenne augmente avec le
nombre de documents t&#233;l&#233;charg&#233;s. Cela est probablement d&#251; au fait que la qualit&#233; du graphe
de cooccurrences augmente avec le nombre de documents et que la pr&#233;cision moyenne en est
directement impact&#233;e.
</p>
<p>4.5 Stabilit&#233; du crit&#232;re
</p>
<p>Le poids d&#8217;un terme t i est d&#233;fini en fonction des cooccurrences du terme t i avec les autres termes
du lexique (&#233;quation 3). Il semble donc l&#233;gitime de s&#8217;interroger quant &#224; la stabilit&#233; des poids des
termes en fonction de la taille du lexique. La somme des poids &#233;tant &#233;gale &#224; 1, le poids absolu de
chaque terme est donc li&#233; &#224; la taille du graphe, il va diminuer avec l&#8217;augmentation du nombre
total de termes. La question est donc de savoir ce qu&#8217;il en est du poids relatif, c&#8217;est-&#224;-dire du rang
des termes en fonction de la taille des lexiques.
</p>
<p>Pour &#233;valuer l&#8217;&#233;volution des rangs des termes, nous avons de nouveau utilis&#233; les lexiques traduits.
Pour chaque lexique, nous avons s&#233;lectionn&#233; al&#233;atoirement 20 termes, puis avons augment&#233;
it&#233;rativement la taille du lexique de 20 termes. Chaque lexique (de 20, 40, 60, . . . termes) a
ensuite &#233;t&#233; ordonn&#233; par le crit&#232;re de coh&#233;sion th&#233;matique. Enfin, le coefficient de corr&#233;lation de
Spearman a &#233;t&#233; employ&#233; pour mesurer l&#8217;&#233;volution des rangs des termes entre lexiques successifs
(20-40, 40-60, . . . ).
</p>
<p>Afin de r&#233;duire l&#8217;influence du hasard, nous avons r&#233;p&#233;t&#233; la proc&#233;dure d&#233;crite pr&#233;c&#233;demment
10 fois et avons calcul&#233; une macro-moyenne des coefficients de corr&#233;lation. Dans un soucis de
clart&#233;, nous ne pr&#233;sentons que les r&#233;sultats sur les lexiques anglais (Figure 2). Toutefois, les
r&#233;sultats obtenus sur les lexiques fran&#231;ais sont &#233;quivalents.
</p>
<p>Nous constatons que le coefficient de corr&#233;lation augmente pour tous les lexiques au fur et &#224;
mesure que la taille des lexiques augmente. D&#233;j&#224; forte avec une corr&#233;lation de plus de 0,70, le
coefficient de corr&#233;lation s&#8217;approche du maximum &#224; partir de 300 termes. Autrement dit, pass&#233;e
cette limite, l&#8217;ajout de nouveaux termes ne modifie presque pas l&#8217;ordre d&#233;j&#224; &#233;tabli entre les autres
termes, ce qui nous permet de conclure que la mesure est stable &#224; partir de ce seuil.
</p>
<p>192</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>50 100 150 200 250 300 350
Taille du lexique
</p>
<p>0.70
</p>
<p>0.75
</p>
<p>0.80
</p>
<p>0.85
</p>
<p>0.90
</p>
<p>0.95
</p>
<p>1.00
</p>
<p>C
o
</p>
<p>e
ff
</p>
<p>ic
ie
</p>
<p>n
t 
</p>
<p>d
e
</p>
<p> c
o
</p>
<p>rr
&#233;
</p>
<p>la
ti
</p>
<p>o
n
</p>
<p> d
e
</p>
<p> S
p
</p>
<p>e
a
</p>
<p>rm
a
</p>
<p>n
</p>
<p>Astronomie en
Statistiques en
</p>
<p>M&#233;dical-1 en
</p>
<p>M&#233;dical-2 en
</p>
<p>FIGURE 2 &#8211; &#201;tude de l&#8217;&#233;volution des rangs des termes en fonction de la taille des lexiques
</p>
<p>5 Conclusion
</p>
<p>Dans cet article, nous avons pr&#233;sent&#233; un nouveau crit&#232;re de coh&#233;sion th&#233;matique fond&#233; sur un
graphe de cooccurrences et un algorithme de marche al&#233;atoire.
</p>
<p>Nous avons &#233;valu&#233; ce crit&#232;re par une t&#226;che d&#8217;aide &#224; la cr&#233;ation de lexiques bilingues car il s&#8217;agit
d&#8217;une t&#226;che claire, facilement reproductible et &#233;valuable de fa&#231;on objective. Cependant, comme
nous l&#8217;avons indiqu&#233;, les applications possibles sont diverses, que ce soit pour r&#233;duire la charge de
validation manuelle ou pour mieux s&#233;lectionner les termes automatiquement pour de la recherche
d&#8217;information, de la collecte de corpus ou la mise en &#339;uvre de techniques de bootstrapping. Les
r&#233;sultats obtenus sont encourageants et montrent la pertinence de notre approche.
</p>
<p>Nous pr&#233;voyons d&#8217;analyser plus en d&#233;tail le comportement du crit&#232;re propos&#233; en &#233;valuant, par
exemple, sa robustesse &#224; la pr&#233;sence de termes non-pertinents dans les lexiques th&#233;matiques.
Nous comptons &#233;galement &#233;valuer l&#8217;apport de quelques annotations manuelles en int&#233;grant ces
annotations dans l&#8217;algorithme de marche al&#233;atoire sous forme d&#8217;un vecteur de personnalisa-
tion (Haveliwala, 2003).
</p>
<p>193</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Remerciements
</p>
<p>Nous voudrions remercier Pierre Zweigenbaum de nous avoir fourni les lexiques n&#233;cessaires
&#224; l&#8217;&#233;valuation de notre m&#233;thode et l&#8217;International Statistical Institute de nous avoir autoris&#233;
&#224; utiliser le glossaire de termes statistiques multilingue. Nous remercions &#233;galement Javier
Couto pour ses conseils avis&#233;s sur la premi&#232;re version de ce manuscrit ainsi que les relecteurs
anonymes pour leurs remarques et conseils. Ce travail s&#8217;inscrit dans le cadre des projets METRICC
(ANR-08-CORD-013) et TTC (FP7/2007-2013 GA n&#9702;248005).
</p>
<p>R&#233;f&#233;rences
</p>
<p>BAKER, L. et MCCALLUM, A. (1998). Distributional clustering of words for text classification. In
Proceedings of the 21st annual international ACM SIGIR conference on Research and development
in information retrieval, pages 96&#8211;103.
</p>
<p>BARONI, M. et BERNARDINI, S. (2004). BootCaT : Bootstrapping Corpora and Terms from the
Web. In Proceedings of the LREC 2004 conference, pages 1313&#8211;1316.
</p>
<p>BARONI, M. et UEYAMA, M. (2006). Building general-and special-purpose corpora by web
crawling. In Proceedings of the 13th NIJL international symposium, language corpora : Their
compilation and application, pages 31&#8211;40.
</p>
<p>COHEN, W. W. (2010). Graph Walks and Graphical Models. Carnegie Mellon University, School of
Computer Science, Machine Learning Dept.
</p>
<p>DAMERAU, F. J. (1964). A technique for computer detection and correction of spelling errors.
Communications of the ACM, 7(3):171&#8211;176.
</p>
<p>DOROW, B. et WIDDOWS, D. (2003). Discovering corpus-specific word senses. In Proceedings of
the tenth conference on European chapter of the Association for Computational Linguistics-Volume
2, pages 79&#8211;82.
</p>
<p>FARAHAT, A., LOFARO, T., MILLER, J., RAE, G. et WARD, L. (2006). Authority rankings from
hits, pagerank, and salsa : Existence, uniqueness, and effect of initialization. SIAM Journal on
Scientific Computing, 27(4):1181&#8211;1201.
</p>
<p>FERRET, O. (2004). Discovering word senses from a network of lexical cooccurrences. In
Proceedings of the 20th international conference on Computational Linguistics, pages 1326&#8211;1332.
</p>
<p>GHANI, R., JONES, R. et MLADENIC, D. (2005). Building Minority Language Corpora by Learning
to Generate Web Search Queries. Knowl. Inf. Syst., 7(1):56&#8211;83.
</p>
<p>GROC, C. d., TANNIER, X. et COUTO, J. (2011). GrawlTCQ : Terminology and Corpora Building
by Ranking Simultaneously Terms , Queries and Documents using Graph Random Walks. In
Proceedings of the TextGraphs-6 Workshop, Association for Computational Linguistics, pages 37&#8211;41.
</p>
<p>HAVELIWALA, T. (2003). Topic-sensitive pagerank : A context-sensitive ranking algorithm for web
search. Knowledge and Data Engineering, IEEE Transactions on, 15(4):784&#8211;796.
</p>
<p>KILGARRIFF, A. et GREFENSTETTE, G. (2003). Introduction to the Special Issue on the Web as
Corpus. Computational Linguistics, 29(3):333&#8211;347.
</p>
<p>LANGVILLE, A. et MEYER, C. (2005). A survey of eigenvector methods for web information
retrieval. SIAM review, pages 135&#8211;161.
</p>
<p>194</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>LEVENSHTEIN, V. (1966). Binary Codes Capable of Correcting Deletions, Insertions and Reversals.
Soviet Physics Doklady, 10:707&#8211;710.
</p>
<p>MANNING, C. et SCH&#220;TZE, H. (1999). Foundations of statistical natural language processing. MIT
Press.
</p>
<p>MIHALCEA, R. et TARAU, P. (2004). TextRank bringing order into text. In Proceedings of the
Conference on Empirical Methods on Natural Language Processing, pages 404&#8211;411.
</p>
<p>NIST (2005). Nist 2005 machine translation evaluation official results. http:
//www.itl.nist.gov/iad/mig/tests/mt/2005/doc/mt05eval_official_
results_release_20050801_v3.html. [consult&#233; le 23/01/2012].
NIST (2008). Nist 2008 open machine translation evaluation (mt08) - official evaluation
results. http://www.itl.nist.gov/iad/mig/tests/mt/2008/doc/mt08_official_
results_v0.html. [consult&#233; le 23/01/2012].
PAGE, L., BRIN, S., MOTWANI, R. et WINOGRAD, T. (1999). The PageRank Citation Ranking :
Bringing Order to the Web. Rapport technique, Stanford InfoLab.
</p>
<p>PEREIRA, F., TISHBY, N. et LEE, L. (1993). Distributional clustering of english words. In Proceedings
of the 31st annual meeting on Association for Computational Linguistics, pages 183&#8211;190.
</p>
<p>RAJMAN, M., BESAN&#199;ON, R. et CHAPPELIER, J. (2000). Le mod&#232;le dsir : Une approche &#224; base
de s&#233;mantique distributionnelle pour la recherche documentaire. Traitement automatique des
langues, 41(2):549&#8211;578.
</p>
<p>WAN, X. (2009). Co-training for cross-lingual sentiment classification. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the Association for Computational Linguistics and the
4th International Joint Conference on Natural Language Processing of the AFNLP : Volume 1, pages
235&#8211;243.
</p>
<p>195</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div>
</div></div>
</body></html>