Un critére de cohésion thématique fondé sur
un graphe de cooccurrences

Clément de Groc1»Z Xavier Tannierz» 3 Claude de Loupyl
(1) Syllabs, 15 rue Jean—Baptiste Berlier, 75013 Paris
(2) Univ. Paris—Sud, 91403 Orsay Cedex
(3) LIMSI—CNRS, B.1? 133, 91403 Orsay Cedex
cdegroctﬁlimsi . fr , xtannierﬁlimsi . fr, loupytﬁsyllabs . com

RESUME
Dans cet article, nous déﬁnissons un nouveau critére de cohésion thématique permettant de
pondérer les termes d’un lexique thématique en fonction de leur pertinence. Le critére s’inspire
des approches Web as corpus pour accumuler des connaissances exogénes sur un lexique. Ces
connaissances sont ensuite modélisées sous forme de graphe et un algorithme de marche aléatoire
est appliqué pour attribuer un score ‘a chaque terme. Aprés avoir étudié les performances et
la stabilité du critére proposé, nous l’évaluons sur une téche d’aide a la création de lexiques
bilingues.

AB STRACT
Topical Cohesion using Graph Random Walks

In this article, we propose a novel metric to weight specialized lexicons terms according to their
relevance to the underlying thematic. Our method is inspired by Web as corpus approaches
and accumulates exogenous knowledge about a specialized lexicon from the web. Terms
cooccurrences are modelled as a graph and a random walk algorithm is applied to compute
terms relevance. Finally, we study the performance and stability of the metric and evaluate it in a
bilingual lexicon creation context.

MOTS-CLES : Cohésion thématique, graphe de cooccurrences, marche aléatoire.

KEYWORDS: Thematic relevance, cooccurrence graph, random walk.

Actes de la con_fe'rence onjointe JEP-TALN-RECITAL 2012, volume 2: TALN, pages 183-195,
Grenoble, 4 an 8 juin 2012. ©2012 ATAI.A 8: AFCP

183

1 Introduction

Les lexiques et les terminologies sont des éléments essentiels du traitement automatique des
langues. Ils sont utilisés dans une grande variété de taches, allant de la catégorisation de textes a
l’analyse d’opinions. Dans cet article, nous nous intéressons plus particulierement aux lexiques
dits thématiques ou spécialisés, c’est-a-dire composés de termes pertinents pour un domaine
par11'culier. La Table 1 présente un extrait de lexique thématique sur le domaine de l’astronomie.

soleil étoile rayon gamma étoile a neutron masse solaire
planéte disque d’accréu'on naine blanche proto-étoile pulsar
astronome quasar astronomie trou noir neutron

TABLE 1 — Extrait de lexique thématique sur l’astronornie

La construction manuelle de tels lexiques est une tache laborieuse et coﬁteuse. C’est pourquoi
l’utilisation du Web ou de traducteurs automatiques comme appui pour la création de lexiques
et de corpus spécialisés est une idée maintenant largement répandue (Baroni et Bernardini,
2004; Groc et al., 2011; Kilgarriff et Grefenstette, 2003; Wan, 2009). Bien que l’utilité de telles
approches ne soit plus a démontrer, une étape de validation manuelle reste requise.

Dans cet article, nous proposons un nouveau critere de cohésion thématique permettant de
pondérer les termes d’un lexique thématique en fonction de leur pertinence pour le theme. Nous
utilisons le Web comme source de corpus spécialisés sur les termes d’un lexique thématique.
Nous modélisons ensuite les cooccurrences entre les termes du lexique sous la forme d’un graphe
orienté o1‘1les sommets sont les termes du lexique et les arcs dénotent la cooccurrence de ces
termes. Ce graphe peut étre pergu comme un graphe de recommandation o1‘1l’apparition de deux
termes dans un méme document signiﬁe qu’ils se recommandent l’un l’autre. Cette observation
nous amene naturellement ‘a utiliser un algorithme de marche aléatoire (random walk (Cohen,
2010; Page et al., 1999)) attribuant une pertinence globale a chaque sommet du graphe.

Ce critére de cohésion thématique peut avoir de multiples applications. Dans le cadre de lexiques
spécialisés construits automatiquement (Baroni et Bernardini, 2004; Groc et al., 2011), ordonner
les éléments du lexique par leur score de cohésion peut réduire la charge de validation manuelle
ou méme limiter la dispersion au ﬁl des itérations. Dans le cadre de la traduction assistée, une
valeur de cohésion peut représenter un score de conﬁance utile pour le traducteur. C’est d’ailleurs
par cette derniére application que nous choisissons d’évaluer notre critére dans cet ar11'cle.

L’article présente tout d’abord brievement les travaux en Web as corpus dont notre approche
découle, ainsi que ceux centrés sur les graphes de cooccurences et les algorithmes de marche
aléatoire (section 2). Dans une 3éme section, nous présentons le modéle de graphe et l’a1gorithme
de marche aléatoire utilisés pour le calcul du critere de cohésion thématique. Nous évaluons
ensuite ce demier sur une tache de ﬁltrage de lexiques thématiques traduits automatiquement
(section 4). Nous concluons enﬁn (section 5) en suggérant plusieurs pistes envisagées.

184

.°
LO
Ln

.0

to

o
.

 

.0

on

o
l

+—+ Astronomie en
~—* Statistiques en —
0-0 Médica|—1 en
x-——>< Médica|—2 en

Coefficient de corrélation de Spearman
_O O
\l 8)
Ln Ln
.

0'70 50 100 150 2(IJ0 2_I':0 300 350

Taille du Iexique

FIGURE 2 — Etude de l’évolution des rangs des termes en fonction de la taille des lexiques

5 Conclusion

Dans cet article, nous avons présenté un nouveau critére de cohésion thématique fondé sur un
graphe de cooccurrences et un algorithme de marche aléatoire.

Nous avons évalué ce critére par une tache d’aide a la création de lexiques bilingues car il s’agit
d’une tache claire, facilement reproducﬁble et évaluable de fagon objective. Cependant, comme
nous l’avons indiqué, les applications possibles sont diverses, que ce soit pour réduire la charge de
validation manuelle ou pour mieux sélectionner les termes automatiquement pour de la recherche
d’information, de la collecte de corpus ou la mise en oeuvre de techniques de bootstrapping. Les
résultats obtenus sont encourageants et montrent la pertinence de notre approche.

Nous prévoyons d’analyser plus en détail le comportement du critére proposé en évaluant, par
exemple, sa robustesse ‘a la présence de termes non-pertinents dans les lexiques thématiques.
Nous comptons également évaluer l’apport de quelques annotations manuelles en intégrant ces
annotations dans l’algorithme de marche aléatoire sous forme d’un vecteur de personnalisa-
tion (Haveliwala, 2003).

193

Remerciements

Nous voudrions remercier Pierre Zweigenbaum de nous avoir fourni les lexiques nécessaires
‘a l’évaluation de notre méthode et l’Intemational Statistical Institute de nous avoir autorisé
‘a utiliser le glossaire de terrnes statistiques multilingue. Nous remercions également Javier
Couto pour ses conseils avisés sur la premiere version de ce manuscrit ainsi que les relecteurs
anonymes pour leurs remarques et conseils. Ce travail s’inscrit dans le cadre des projets METRICC
(ANR-08-CORD-013) et TTC (FP7/2007-2013 GA n°248005).

Références

BAKER, L. et MCCALLUM, A. (1998). Distributional clustering of words for text classiﬁcation. In
Proceedings of the 21st annual international ACM SIGIR conference on Research and development
in information retrieval, pages 96-103.

BARoN1, M. et BERNARDINI, S. (2004). BootCaT : Bootstrapping Corpora and Terms from the
Web. In Proceedings of the LREC 2004 conference, pages 1313-1316.

BARoN1, M. et UEYAMA, M. (2006). Building general-and special-purpose corpora by web
crawling. In Proceedings of the 13th NIJL international symposium, language corpora : Their
compilation and application, pages 3140.

CoHEN, W. W. (2010). Graph Walks and Graphical Models. Carnegie Mellon University, School of
Computer Science, Machine Learning Dept.

DAMERAU, F. J. (1964). A technique for computer detection and correction of spelling errors.
Communications of the ACM, 7(3) : 171-176.

DoRow, B. et WIDDOWS, D. (2003). Discovering corpus-speciﬁc word senses. In Proceedings of
the tenth conference on European chapter of the Association for Computational Linguistics-Volume
2, pages 79-82.

FARAHAT, A., LoFARo, T., MILLER, J., RAE, G. et WARD, L. (2006). Authority rankings from
hits, pagerank, and salsa : Existence, uniqueness, and effect of initialization. SIAM Journal on
Scientiﬁc Computing, 27(4):1181—1201.

FERRET, O. (2004). Discovering word senses from a network of lexical cooccurrences. In
Proceedings of the 20th international conference on Computational Linguistics, pages 1326-1332.
GHANI, R., JONES, R. et MLADENIC, D. (2005). Building Minority Language Corpora by Learning
to Generate Web Search Queries. Knowl. Inf Syst., 7(1):56-83.

GRoc, C. d., TANNIER, X. et Couro, J. (2011). GrawlTCQ : Terminology and Corpora Building
by Ranking Simultaneously Terms , Queries and Documents using Graph Random Walks. In
Proceedings of the TextGraphs-6 Workshop, Association for Computational Linguistics, pages 37-41.
HAVELIWALA, T. (2003). Topic-sensitive pagerank : A context-sensitive ranking algorithm for web
search. Knowledge and Data Engineering, IEEE Transactions on, 15(4):784—796.

KILGARRIFF, A. et GREFENSTETTE, G. (2003). Introduction to the Special Issue on the Web as
Corpus. Computational Linguistics, 29(3) :333—347.

LANGVILLE, A. et MEYER, C. (2005). A survey of eigenvector methods for web information
retrieval. SIAM review, pages 135-161.

194

LEVENSHTEIN, V (1966). Binary Codes Capable of Correcting Deletions, Insertions and Reversals.

Soviet Physics Doklady, 10:707-710.

MANNING, C. et ScHI"J'rzE, H. (1999). Foundations of statistical natural language processing. MIT
Press.

MIHALCEA, R. et TARAU, 1? (2004). TextRank bringing order into text. In Proceedings of the
Conference on Empirical Methods on Natural Language Processing, pages 404411.

NIST (2005). Nist 2005 machine translation evaluation official results. http:

//www.it1.nist .gov/iad/mig/tests/mt/2005/doc/mtO5eva1_officia1_
resu1ts_re1ease_20050801_v3 .htm1. [consulté le 23/01/2012].

NIST (2008). Nist 2008 open machine translation evaluation (mt08) - official evaluation
results. http://www.it1.nist.gov/iad/mig/1:ests/mt/2008/doc/mtO8_officia1_
resu1ts_vO .htm1. [consulté le 23/01/2012].

PAGE, L., BRIN, S., MOTWANI, R. et WINOGRAD, T. (1999). The PageRank Citation Ranking :
Bringing Order to the Web. Rapport technique, Stanford InfoLab.

PEREIRA, F., TISHBY, N. et LEE, L. (1993). Distributional clustering of english words. In Proceedings
of the 31st annual meeting on Association for Computational Linguistics, pages 183-190.

RAJMAN, M., BEsANgoN, R. et CHAPPELIER, J. (2000). Le modéle dsir : Une approche ‘a base
de sémantique distributionnelle pour la recherche documentaire. Traitement automatique des
langues, 41(2) :549-578.

WAN, X. (2009). Co-training for cross-lingual sentiment classiﬁcation. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the Association for Computational Linguistics and the
4th International Joint Conference on Natural Language Processing of the APNLP : Volume 1 , pages
235-243.

195

2 'I'ravaux liés

L’utilisation du Web comme source de documents (Kilgarriff et Grefenstette, 2003) est une idée
maintenant largement répandue. Pour accéder aux documents Web, deux approches sont cou-
ramment mises en oeuvre : soumettre un ensemble de requétes a un moteur de recherche (Baroni
et Bernardini, 2004; Ghani et al., 2005) ou parcourir directement le Web (crawling) (Baroni et
Ueyama, 2006). Le parcours du Web permet une meilleure spéciﬁcation du besoin mais nécessite
un investissement important. De plus, les efforts des moteurs de recherche pour garantir des
résultats de qualité doivent étre reproduits (ﬁltrage des pages de spam). Au contraire, l’utilisation
d’un moteur de recherche grand public comme point d’entrée au Web offre un accés simple et peu
coﬁteux pour la communauté de Traitement Automatique des Langues. Nous adoptons ici cette
approche aﬁn de constituer un ensemble de connaissances exogénes sur les lexiques thématiques
fournis en entrée.

Dans cet article, nous déﬁnissons un critére basé sur les cooccurrences des termes d’une méme
thématique pour déterminer leur lien avec le theme. Ces travaux partagent donc certaines
hypotheses avec les travaux en similarité sémantique et notamment les analyses distribution-
nelles (Pereira et aL, 1993; Baker et McCaJlum, 1998; Rajman et aL, 2000) ou la désambiguisation
sémantique via des réseaux de cooccurrences (Dorow et Widdows, 2003; Ferret, 2004). En effet,
notre graphe de cooccurrences modélise explicitement les cooccurrences de premier ordre mais
l’applica11'on d’un algorithme de propagation d’importance de type PageRank permet la prise en
compte de cooccurrences d’ordres supérieurs.

Enfin, l’algorithme TextRank (Mihalcea et Tarau, 2004) est étroitement lié a nos travaux. Les
auteurs modélisent la cooccurrence des mots dans une fenétre de taille N sous forme de graphe
non-orienté et appliquent un algorithme de marche aléatoire aﬁn de détecter les mots-clés
saillants. Nous nous démarquons cependant de ces travaux en au moins deux points : nous consi-
dérons les cooccurrences au niveau du document (snippet dans nos évaluations) et modélisons
ces derniéres par un graphe orienté (plus de détails en Section 3).

3 Un critére de cohésion thématique

Etant donné un lexique thématique ET composé de N termes, $T = (t1, t2, . . ., tN), nous voulons
calculer un vecteur de poids w,.T = (wl, wz, . . ., wN) o1‘1 chaque poids w,- mesure la perﬁnence
du terme ti pour la thématique T.

3.1 Recueil de connaissances exogénes

Dans ces travaux, nous adoptons une approche Web as corpus, qui nous permet de créer ra-
pidement des corpus spécialisés en nous appuyant sur un moteur de recherche généraliste.
Nous proposons des lors de constituer, pour chaque terme ti, un corpus Ci correspondant au M
meilleurs résultats renvoyés par un moteur de recherche pour la requéte " <t_i>".

L’unité d’informau'on que nous considérons dans le cadre de cet article est le snippet, le court
extrait de page Web renvoyé par le moteur de recherche. En effet, si prendre en compte le
document entier permettrait en théorie de bénéﬁcier d’un contexte plus large et plus riche, cela

185

pose surtout de nombreux problemes. D’une part, télécharger les documents renvoyés par le
moteur de recherche rallonge considérablement le temps de calcul. D’autre part, il est ensuite
indispensable d’opérer un nettoyage des pages Web, et en particulier de supprimer les menus,
les publicités ou les balises HTML, dans le but de ne conserver que le minimum de contenu non
informationnel 1. L’évaluation ﬁnale dépend donc beaucoup de la qualité de ce nettoyage, ce qui
la rend plus difﬁcilement interprétable. Enﬁn, le caractere local des snippets peut permettre de
réduire le bruit pouvant apparaitre dans les pages Web.

Nous avons utilisé le moteur de recherche Bingz comme source de snippets. Ces derniers sont
composés de portions de textes de 155 caracteres en moyenne issus du corps des pages Web et
contenant les termes de la requéte.

3.2 Cohésion thématique et graphe de cooccurrences

Etant donné un lexique thématique ET, nous proposons une premiere déﬁnition de notre critére
comme suit : le poids wi d’un terme ti est égal au nombre de termes du lexique (ti exclu)
cooccurrant avec ti dans le corpus Ci. Plus formellement, le poids wi d’un terme ti est déﬁni par :

Wi = Z “:,-,c,- (1)

ti-E2}

o1‘1 nijici est le nombre d’occurrences du terme ti dans l’ensemble des documents du corpus Ci et
55; = ET \ {ti}, c’est-a-dire l’ensemble des termes du lexique ET, ti exclu.

Cette méme déﬁnition peut étre modélisée sous la forme d’un graphe (Figure 1). Soit un graphe
orienté G =< V,E >, o1‘1 V est l’ensemble des sommets (V = fi) et E l’ensemble des arcs.
Chaque arc e(ti, ti) symbolise l’apparition du terme ti dans le corpus Ci de ti. Les arcs sont
pondérés en fonction du nombre d’occurrences du terme ti dans Ci. Notre approche Web as
corpus nous différencie des précédents travaux (Mihalcea et Tarau, 2004) visant a modéliser les
cooccurrences sous forme de graphe non-orienté : en effet, pour deux termes ti et ti et leurs
corpus associés Ci et Ci, l’apparition du terme ti dans le corpus Ci constitue un indice du “vote”
de ti pour ti. Cependant, cette relation n’est pas symétrique puisque les corpus Ci et Ci sont
distincts. En conséquence, nous optons pour un modéle de graphe orienté.

Le poids d’un terme tel que défini par l’équation 1 est alors équivalent au degré entrant de ce
terme dans le graphe, c’est-a-dire la somme des poids des arcs entrants.

Cette nouvelle modélisation graphique nous améne a intégrer les poids du voisinage entrant d’un
sommet dans le calcul du poids de celui-ci. Nous rectiﬁons alors la premiére déﬁnition de notre
critere et proposons la déﬁnition suivante : le poids wi d’un terme ti est égal a la somme des
poids des termes du lexique cooccurrant avec ti dans le corpus Ci. De plus, nous normalisons
cette somme aﬁn que le poids d’un terme soit réparti entre tous les termes auxquels il est lie.

1. Ce probléme est d’ai11eurs un theme de recherche a part entiére fédéré par la campagne d’éva1uation CLEANE-
VAL (http : //cleaneval . sigwac . org . uk) .
2. http: //www. bing. com

186

 

FIGURE 1 — Modélisation des cooccurrences sous forme de graphe orienté

Formellement, nous déﬁnissons le poids w,~ d’un terme t,~ comme :

216$‘ nt,C- WV)’
WI. : L (2)
21162; nt],Ci

Cette nouvelle déﬁnition est “récursive” dans le sens ou la pertinence d’un terme du lexique est
déﬁnie en fonction de la pertinence des autres termes du lexique apparaissant dans son corpus. Il
est ainsi possible de voir la pertinence d’un terme ti déﬁni en fonction de la pertinence d’un terme
tj, elle-méme déﬁni en fonction la pertinence de ti. D’un point de vue graphique, ce phénomene
se traduit alors simplement par un cycle dans le graphe de cooccurrences.

L’équation 2 est en fait proche de l’algorithme de marche aléatoire PageRank (Page et (IL, 1999)
et peut étre résolue par un algorithme itératif sous certaines conditions. Cette version na'ive de
l’algorithme pose cependant deux problemes :

1. L’algorithme ne gere pas correctement les sommets sans arcs sortant (appelés “dangling
nodes” ou “rank sink” dans la littérature) : il n’est pas souhaitable qu’un terme ne renvoyant
que des documents extérieurs a la thématique obtienne un poids important. Par exemple,
si un terme de notre lexique .$T est un mot outil (“et”), il possedera de nombreux liens
entrant mais potentiellement aucun lien sortant : il accumulera itérativement un poids
important.

2. La convergence vers une unique solution n’est pas garantie pour notre graphe (Langville et
Meyer, 2005; Farahat et al., 2006).

Four résoudre le premier probleme, nous ajoutons un lien des sommets sans arcs sortant vers
un sommet virtuel et un lien de ce sommet virtuel vers tous les sommets du graphe. Le poids
des sommets sans arcs sortant est ainsi redistribué a tous les sommets du graphe. Concernant le
second probleme, nous appliquons la solution de Page et Brin (Page et al., 1999) et ajoutons une
probabilité de téléportation uniforme a chaque itération de l’algorithme ce qui garantit la forte
connectivité du graphe et la convergence vers une solution unique (équation 3).

187

(1 — a) Zcjeseg “r;.C.- ""’j.n
wi,"+1 :   '  

(3)
21:,-ex} nE;.Cx-

ou N est le nombre de sommets du graphe (c’est a dire 1e nombre de termes du lexique) et at est
un facteur d’amortissement traditionnellement ﬁxé a 0.85 (Page et al., 1999; Mihalcea et Tarau,
2004). Nous utilisons également cette valeur de or pour nos expériences.

I.’a1gorithme 1 récapitule 1’intégra1ité du calcul du critére de cohésion thématique.

Algorithme 1 Critére de cohésion thématique
1: Entrées : ET :termes tl-,i e [1,N]
M : nombre de documents téléchargés par requéte
or : facteur d’amortissement

// Téléchargement du corpus
: Pour tout terme ti 6 ET faire
Soumettre ti a un moteur de recherche
Télécharger M documents comme corpus C,-
: Fin Pour

<n::_>g._:ro

// Iniﬁalisation
: Pour tout terme ti 6 ET faire
wt-,1 = 1/N
8: Fin Pour

O\

>1

// Procédure itérative de calcul des poids
9: T1 = 1

10: 'I'ant que (non convergence) faire

11: Pour tout terme ti 6 ET faire

12 (1 — a) Z:,.e2'T* “r;.C.- ""’j.n

: w-, +1 = T a- T
I n N 2:562} nt,-.Cr

13: Fin Pour

14: Normalisation des poids : Ziwl-,,l+1 = 1
15: n = n + 1

16: Fin 'I'ant que

17: Retourner wn

Pour améliorer 1a correspondance entre les lexiques et les documents issus du Web, une série de
normalisations supplémentaires est appliquée : conversion des termes en minuscules, racinisation
(stemming) et normalisation des caractéres unicode (accents, ...).

188

Notons que le critére proposé traite les phénoménes d’ambigu’ités graphiques de la langue
(homographie) de la facon souhaitée. Par exemple, si le terme “jaguar” est soumis a un moteur de
recherche actuel, il est fort probable que ce dernier renvoie des résultats diversiﬁés £1 propos de
l’animal mais également de la marque de voiture, de la console de jeu ou du systéme d’exploitation
MacOS. Le nombre de cooccurrences avec les termes du lexique sera donc plus limité, conduisant
‘a un score plus faible, soulignant ainsi qu’un terme ambigiie contribue moins 21 la cohésion
thématique.

4 Evaluation

4.1 Téche

Dans cet article, nous évaluons l’apport de notre critére pour l’aide ‘a la création de lexiques
thématiques bilingues £1 par11'r de lexiques monolingues. Comme mentionné dans l’introduct1'on,
nous envisageons de nombreuses applications pour le critére proposé dont notamment le boots-
trapping de lexiques thématiques monolingues. Cependant, la téche de création de lexiques
thématiques bilingues, claire et facilement reproductible, foumit une évaluation objective de
notre critére de cohésion.

Partant de lexiques thématiques monolingues, une approche commune pour la création de
lexiques thématiques bilingues est d’uti1iser un outil de traduction automatique en ligne tel que
Google Translate 3. Cependant, ces outils ne permettent pas d’intégrer une notion de contexte
thématique dans le processus de traduction simplement. Ainsi, le terme simple “avocat” non
intégré £1 une phrase, par exemple, sera traduit par ces outils “avocado” ou “lawyer” en anglais,
indifféremment du fait qu’il apparﬁent 21 un lexique juridique ou culinaire. Une validation
manuelle laborieuse est donc nécessaire pour supprimer les traductions erronées.

Nous proposons d’appliquer notre critére de cohésion thématique aux lexiques traduits, attribuant
ainsi a chaque traduction un score de conﬁance. Le tri des lexiques en fonction de ce score permet
alors de réduire le temps nécessaire £1 leur validation.

4.2 Données

Nous avons utilisé trois lexiques bilingues francais/anglais spécialisés sur trois themes différents :
— Astronomie (The Astronomy Thesaurus 4) ;

— Médical (Unified Medical Language System - UMLS 5) ;

— Statistiques (International Statistical Institute 5).

Un exemple de termes issus de chaque lexique bilingue est donné Table 2.

3. http: //translate . google . com

4. http: //msowww . anu . edu . au/library/1:hesau:rus/
5. http: //www. nlm.nih.gov/research/umls/

6. http: //isi . obs .nl/glossary/

189

celestial coordinates coordonnee celeste

stars
how shocks onde de choc en d’arc

illusions illusions

 

TABLE 2 — Extrait des lexiques thématiques bilingues utilisés pour l’évaluation

Une série de traitements a été appliquée £1 chaque lexique dans le but d’en améliorer la qualité ou
l’utilisau'on pour notre évaluation. Ainsi, nous avons supprimé les termes apparaissant entre cro-
chets ou parentheses dans les lexiques Astronomie et Statistiques. Le lexique Médical présentant
des termes trop ambigus pour étre nettoyés autornatiquement (par exemple 3-pyridinecarboxylic
acid, 1,4-dihydro-2,6-dimethyl-5-nitro-4-(2-(try"luoromethyl)phenyl)-, methyl ester), nous avons
simplement supprimé les termes contenant une parenthése ou une virgule.

Nous avons ensuite traité le cas des traductions multiples de la maniére suivante : lorsqu’un
terme de la langue source possédait plusieurs traductions dans la langue cible, nous n’avons
conservé que le terme le plus proche (au sens de la distance de Damerau-Levenshtein (Damerau,
1964; Levenshtein, 1966)) de la traduction automatique 7. Ainsi dans l’exemple “Afterglow” =>
“Postluminescence ou Remanence” issu du lexique “Astronomie”, nous n’avons conservé que le
terme “Remanence” car il est le plus proche de la traduction automatique trouvée : “rémanence”.

Le lexique UMLS comprenant plus de 19 000 termes, nous avons choisi de ne travailler que sur
un échantillon de ce demier. Nous avons donc tiré aléatoirement deux séries de 2 000 termes
que nous désignons comme lexiques Médical-1 et Médical-2.

Nous obtenons enﬁn les lexiques suivants :
— Astronomie (2 940 termes) ;
Statistiques (2 752 termes) ;

— Médical-1 (2 000 termes) ;

Médical-2 (2 000 termes).

4.3 Méthode

Nous traduisons chaque lexique thématique d’une langue source vers une langue cible (par
exemple Astronomie fr —> en ou Astronomie en —> fr) ‘a l’aide du rnoteur de traduction Google

7. voir section 4.3 pour la méthode de traduction automatique

190

Translate. Le lexique résultant est alors pondéré avec le critere de cohésion thématique puis
ordonné et comparé avec la référence.

Le choix de Google Translate est just1'fié par le fait que ce moteur propose une tres large
couverture, ce qui en fait un candidat idéal pour traiter nos lexiques spécialisés. De plus, les
récentes évaluations du NIST ont montré que l’outil de Google propose des performances état de
l’art quant a la qualité des traductions produites (NIST, 2005, 2008).

La comparaison entre les termes traduits et les termes des lexiques originaux est réalisée a l’aide
d’une mesure ad hoc incluant la suppression des déterminants en début de terme (“le bleu de
bromothymol” => “bleu de bromothymol”) et une distance d’édition de Damerau-Levenshtein (Da-
merau, 1964; Levenshtein, 1966). Nous avons considéré un terme comme valide s’il est au plus a
une distance d’édition de 1 du terme de référence, autorisant ainsi une légére marge d’erreur due
au moteur de traduction ou ‘a la référence (singuliers transformés en pluriels, espace remplacé
par un tiret, . . . ).

La mesure de précision moyenne non-interpolée (uninterpolated average precision - UAP (Manning
et Schiitze, 1999)) est employée pour évaluer la validité de l’ordre des termes traduits.

4.4 Evaluation du critére

Nous évaluons notre critére Cohesion-RW comparativement a une baseline Hasard, obtenue par
le simple tri aléatoire de la liste de traduction, et a la premiére version du critére Cohésion-DEG
(équation 1). 1a baseline Hasard est obtenue en calculant une macro-moyenne sur dix tris
aléatoires successifs. Nous ﬁxons le nombre de snippets téléchargés pour chaque requéte (la
valeur M de l’algorithme 1) a 200 documents. Les résultats sont présentés a la Table 3.

Theme Hasard Cohésion-DEG Cohésion-RW

Astmnomie en —» fr 0.429 0.494 0.512
fr —» en 0.553 0.664 0.678

Statistiques en —» fr 0.382 0.663 0.711
fr —» en 0.488 0.667 0.705

, . en —» fr 0.530 0.683 0.735
Med‘°a1'1 fr —» en 0.620 0.707 0.713
, . en —» fr 0.522 0.662 0.699
Med‘°a1'2 fr —» en 0.638 0.739 0.750

TABLE 3 — Précision moyenne non-interpolée (UAP) pour le classement des termes des lexiques
traduits.

Nous constatons que l’algorithme de marche aléatoire fournit les meilleurs résultats avec gain
sur la baseline Hasard allant de 15,8 % (Médical-1 fr —> en) a 86,1 % (Statistiques en —> fr). La
baseline fournit une idée de la qualité des traductions produites par le moteur de traduction.
Ainsi, il semble que les lexiques Médical-1 et Médical-2 soient les mieux traduits. Au contraire
le lexique Statistiques semble étre le plus difficile a traduire. Le coefficient de corrélation de
Pearson entre la précision du Hasard et le gain obtenu vaut -0,61 ce qui semble signiﬁer qu’ils
sont fortement corrélés négativement : plus la traduction est de bonne qualité et plus le gain est

191

Théme 50 100 150 200
mmmmmmm
 3:; 3-2:: 3-2:: 3-3:: 3:2,;
Médica1_1 en —> fr 0.710 0.719 0.726 0.735

fr —> en 0.677 0.693 0.706 0.718
Médica1_2 en —> fr 0.672 0.678 0.687 0.699
fr —> en 0.702 0.723 0.735 0.750

TABLE 4 — Précision moyenne non-interpolée (UAP) pour le classement des termes des lexiques
traduits avec le critére Cohésion-RW pour différentes valeurs de NB_DOCS.

faible. Cependant ce résultat n’est pas statistiquement signiﬁcatif (la p-value vaut 0,106).

Nous évaluons ensuite l’inﬂuence du nombre de snippets téléchargés par requéte sur les résultats
du critere proposé (Table 4). Nous constatons que la précision moyenne augmente avec le
nombre de documents téléchargés. Cela est probablement dﬁ au fait que la qualité du graphe
de cooccurrences augmente avec le nombre de documents et que la précision moyenne en est
directement impactée.

4.5 Stabilité du critére

Le poids d’un terme t,- est déﬁni en fonction des cooccurrences du terme t,- avec les autres termes
du lexique (équation 3). I1 semble donc légitime de s’interroger quant a la stabilité des poids des
termes en fonction de la taille du lexique. La somme des poids étant égale a 1, le poids absolu de
chaque terme est donc lié a la taille du graphe, il va diminuer avec l’augmentation du nombre
total de termes. La question est donc de savoir ce qu’il en est du poids relatif, c’est-a-dire du rang
des termes en fonction de la taille des lexiques.

Pour évaluer l’évolution des rangs des termes, nous avons de nouveau utilisé les lexiques traduits.

Pour chaque lexique, nous avons sélectionné aléatoirement 20 termes, puis avons augmenté
itérativement la taille du lexique de 20 termes. Chaque lexique (de 20, 40, 60, ...termes) a
ensuite été ordonné par le critére de cohésion thématique. Enﬁn, le coefﬁcient de corrélation de
Spearman a été employé pour mesurer l’évoluu'on des rangs des termes entre lexiques successifs
(20-40, 40-60, . . . ).

Aﬁn de réduire l’inﬂuence du hasard, nous avons répété la procédure décrite précédemment
10 fois et avons calculé une macro-moyenne des coefficients de corrélation. Dans un soucis de
clarté, nous ne présentons que les résultats sur les lexiques anglais (Figure 2). Toutefois, les
résultats obtenus sur les lexiques frangais sont équivalents.

Nous constatons que le coefficient de corrélation augmente pour tous les lexiques au fur et ‘a
mesure que la taille des lexiques augmente. Déja forte avec une corrélation de plus de 0,70, le
coefﬁcient de corrélation s’approche du maximum a partir de 300 termes. Autrement dit, passée

cette limite, l’ajout de nouveaux termes ne modiﬁe presque pas l’ordre deja etabli entre les autres
termes, ce qui nous permet de conclure que la mesure est stable a parﬁr de ce seuil.

192

