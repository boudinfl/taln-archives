Détection et correction automatique d’erreurs
d’annotation morpho-syntaxique du French 'I'reeBank

Florian Boudin Nicolas Hernandez
Université de Nantes
prénom . nom@u.niv—nantes . fr

RESUME
La qualité de l’annotation morpho-syntaxique d’un corpus est déterminante pour l’entrainement
et l’évaluation de méthodes d’étiquetage. Cet article présente une série d’expériences que nous
avons menée sur la détection et la correction automatique des erreurs du French Treebank.
Deux méthodes sont utilisées. La premiere consiste ‘a identifier les mots sans étiquette et leur
attribuer celle d’une forme correspondante observée dans le corpus. La seconde méthode utilise
les variations de n-gramme pour détecter et corriger les anomalies d’annotation. L’évaluation des
corrections apportées au corpus est réalisée de maniére extrinséque en comparant les scores de
performance de différentes méthodes d’étiquetage morpho-syntaxique en fonction du niveau de
correction. Les résultats montrent une amélioration signiﬁcative de la précision et indiquent
que la qualité du corpus peut étre sensiblement améliorée par l’application de méthodes de
correction automatique des erreurs d’annotation.

AB STRACT
Detecting and correcting POS annotation in the French 'I‘reeBank

The quality of the Part-Of-Speech (POS) annotation in a corpus has a large impact on training
and evaluating POS taggers. In this paper, we present a series of experiments that we have
conducted on automatically detecting and correcting annotation errors in the French TreeBank.
Two methods are used. The ﬁrst simply relies on identifying tokens with missing tags and correct
them by assigning the tag the same token observed in the corpus. The second method uses
n-gram variations to detect and correct conﬂicting annotations. The evaluation of the automatic
correction is performed extrinsically by comparing the performance of different POS taggers
in relation to the level of correction. Results show a statistically signiﬁcant improvement in
precision and indicate that the POS annotation quality can be noticeably enhanced by using
automatic correction methods.

MOTS-CLES : Etiquetage morpho-syntaxique, correction automatique, qualité d’annotation.

KEYWORDS: Part-Of-Speech tagging, automatic correction, annotation quality.

1 Introduction

Le corpus arboré de Paris 7, également appelé French Treebank (FTB), est la plus grande ressource
disponible de textes annotés syntaxiquement et morpho-syntaxiquement pour le frangais (Abeillé

Actes de la con_fe'rence onjointe JEP-TALN-RECITAL 2012, volume 2: TALN, pages 281-291,
Grenoble, 4 au 8 juin 2012. ©2012 ATAI.A 8: AFCP

281

et al., 2003). Il est le résultat d’un projet d’annotation supervisée d’articles du journal Le
Monde mené depuis plus d’une dizaine d’années. La quasi-totalité des méthodes d’étiquetage
morpho-syntaxique du francais utilisent cet ensemble de données que ce soit pour leur phase
d’entrainement ou d’évaluation, e.g. (Crabbé et Candito, 2008; Denis et Sagot, 2010). La qualité
de l’annotation du corpus est donc déterrninante.

De la méme maniere que la plupart des corpus annotés morpho-syntaxiquement, e.g. le Penn
'Ii'eeBank (Marcus et aL, 1993) pour l’ang1ais, le FTB a été construit de maniére semi-automatique.
Un étiqueteur automatique est d’abord appliqué sur l’ensemble des textes. Les sorties sont ensuite
corrigées manuellement des éventuelles erreurs commises par l’outil. Malgré cette derniére étape,
il est presque certain que des erreurs existantes ne sont pas corrigées et que de nouvelles erreurs
sont introduites (les humains n’étant pas infaillibles). Plusieurs études illustrent d’ailleurs cette
problématique en décrivant quelques unes des erreurs d’annotation récurrentes du FTB telles que
l’absence d’étiquette ou la présence d’éléments XML vides 1 (Arun et Keller, 2005; Green et al.,
20 1 1) .

Dans cette étude, nous présentons une série d’expériences que nous avons menée sur la correction
automatique du FTB. Nous détaillons les différentes erreurs que nous avons rencontrées ainsi
que les solutions que nous appliquons. Deux méthodes sont utilisées. La premiere consiste ‘a
identifier les mots sans étiquette et leur attribuer celle d’une forme correspondante observée
dans le corpus. La seconde méthode utilise les variations de n-gramme pour détecter et corriger
les anomalies d’annotation. 1.’évaluation de la correction du corpus est réalisée de maniere
extrinseque en étudiant l’impact du niveau de correction sur les performances de plusieurs
méthodes d’étiquetage morpho-syntaxique.

Le reste de cet article est organisé comme suit. La section 2 présente le corpus French 'Ii‘eebank
que nous utilisons dans cette étude. La section 3 est consacrée a la description de la méthode
que nous proposons. Nous décrivons ensuite en section 4 nos résultats expérimentaux avant de
présenter les travaux connexes aux n6tres. La section 6 conclut cette étude et donne quelques
perspectives de travaux futurs.

2 Description du corpus French Treebank

Notre intérét pour le FTB est motivé par deux objectifs : d’une part réaliser des traitements
automatiques sur le corpus, et d’autre part, construire des modélisations permettant de prédire
l’étiquette grammaticale d’un mot a l’aide d’approches statistiques; ce deuxiéme objectif est un
cas particulier du premier. Nos observations concernent donc a la fois la structure du corpus qui
porte les annotations et la qualité de ses annotations grammaticales.

Le corpus est toujours en développement. La version que nous utilisons dans cette étude est datée
de juillet 2010, elle est composée de 21 562 phrases pour 628 767 mots (tokens). Les ﬁchiers qui
composent le corpus sont au format XML (voir la Figure 1). Les mots sont répartis en 13 catégories
principales (attribut cat), elles mémes réparties en 34 sous-catégories (attribut subcat). De
plus, les traits ﬂexionnels (attribut mph), les lemmes (attribut lemma) et les mots composés

1. Certaines des erreurs recencées ne sont que des choix de représentation qui ne sont pas forcément des choix
les plus adaptés dans une perspective de traitement automatique du corpus. C.f. http:/ /www. llf . curs . :Er/Gens/
Abei11e/ guide—morpho— synt .02.pd:E pour la représentation de « du» en deux balises tokens « de» et « la», la
seconde ayant un contenu textuel vide.

282

CONSTANT, M., TELLIER, I., DUCHIER, D., DUPONT, Y., SIGOGNE, A. et BILLOT, S. (2011). Intégrer
des connaissances linguistiques dans un CRF : application ‘a l’apprentissage d’un segmenteur-
étiqueteur du frangais. In Actes de Traitement automatique des langues naturelles (201 1).

CRABEE, B. et CANDIro, M. (2008). Expériences d’analyse syntaxique statistique du frangais. In
Actes de Traitement automatique des langues naturelles (2008).

DENIs, 1? et SAGor, B. (2010). Exploitation d’une ressource lexicale pour la construction d’un
étiqueteur morphosyntaxique état-de-l’art du francais. In Actes de Traitement automatique des
langues naturelles (2010).

DICKINSON, M. et MEURERs, W. D. (2003). Detecting errors in part-of-speech annotation. In
Proceedings of the 10th Conference of the European Chapter of the Association for Computational
Linguistics (EACL-03), pages 107-114, Budapest, Hungary.

GREEN, 5., de MARNEFFE, M.-C., BAUER, J. et MANNING, C. D. (2011). Multiword expression iden-
tiﬁcation with tree substitution grammars : A parsing tour de force with french. In Proceedings
of the 201 1 Conference on Empirical Methods in Natural Language Processing, pages 725-735,
Edinburgh, Scotland, UK. Association for Computational Linguistics.

KVETON, P. et OLIvA, K. (2002). (semi-)automatic detection of errors in pos-tagged corpora. In
COLING.

LOFTSSON, H. (2009). Correcting a POS-tagged corpus using three complementary methods.
In Proceedings of the 12th Conference of the European Chapter of the ACL (EACL 2009), Pages
523-531, Athens, Greece. Association for Computational linguistics.

MANNING, C. (2011). Part-of-speech tagging from 97linguistics? In GELBUKH, A., éditeur :
Computational Linguistics and Intelligent Text Processing, volume 6608 de Lecture Notes in
Computer Science, pages 171-189. Springer Berlin / Heidelberg.

MARCUS, M., MARCINKIEWICZ, M. et SANroRINI, B. (1993). Building a large annotated corpus of
English : The Penn 'I'reebank. Computational linguistics, 19(2):313—330.

NAKAGAWA, T. et MATSUMOTO, Y. (2002). Detecting errors in corpora using support vector
machines. In COLING.

TOUTANOVA, K., KLEIN, D., MANNING, C. et SINGER, Y. (2003). Feature-rich part-of-speech tagging
with a cyclic dependency network. In Proceedings of the 3rd Conference of the North American
Chapter of the ACL (NAACL 2003), pages 173-180. Association for Computational Linguistics.

291

(e.g. « aujourd’hui », « mettre en garde ») sont explicités. Ces derniers sont trés nombreux dans le
corpus : ~14% des occurrences de tokens entrent dans un mot composé. On peut noter que la
structure originale de la phrase (avec les caractéres espaces) ainsi que l’identiﬁant du document
source ne ﬁgurent pas dans le corpus.

<SENT nb="226">
<NP>

<w ca1:="D" [...] lemma="son" mph="1fss" subca1:="poss">Ma</w>

<w ca1:="N" [...] lemma="posi1:ion" mph=":Es" subca1:="C">posi1:ion</w>
</NP>
<VN>

<w ca1:="V" [...] lemma="é1:re" mph="P3s" subca1:="">es1:</w>
</VN>

<NP>

<w ca1:="D" [...] lemma="le" mph=":Es" subca1:="def">la</w>

<w ca1:="N" [...] lemma="suivan1:" mph=":Es" subcat="C">suivan1:e</w>
</NP>

<w ca1:="PONCT" [...] lemma="." subca1:="S">.</w>
</sENT>

FIGURE 1 — Exemple de phrase annotée extraite du ﬁchier lmf300_13000ep . cat .xml, certains
attributs ont été supprimées pour faciliter la lecture ([. . .] ).

L’encodage natif du corpus est iso-8859-1. Le premier traitement que nous avons opéré est sa
conversion en utf-8 via l’outil GNU iconv. L’encodage utf-8 est utilisé par défaut par l’ensemble des
outils et des applications que nous utilisons. Seul le ﬁchier lmf7ad1co . aa . xml fut récalcitrant
et nous avons été amené ‘a corriger les caracteres accentués ‘a l’aide de quelques regles de
conversion ad hoc. Le FTB nécessite ensuite de nombreux pré-traitements avant de pouvoir étre
utilisé pour l’entrainement et l’évaluation d’étiqueteurs morpho-syntaxiques. Le format XML
d’origine doit tout d’abord étre converti au format d’entrée standard 2. Cette premiére conversion
du corpus nous a permis d’identiﬁer et de corriger quelques problémes liés a sa structure : absence
d’attribut, étiquette de catégorie morpho-syntaxique non valide, etc.

2.1 Choix du jeu d’étiquettes et du découpage en unités lexicales

Plusieurs possibilités s’offrent ‘a nous quant au choix du jeu d’étiquettes morpho-syntaxiques.
(Crabbé et Candito, 2008) ont proposé un jeu d’étiquettes optimisé en 29 catégories (utilisant
l’information supplémentaire du mode des verbes et de certaines sous-catégories). Les résultats
obtenus avec leur méthode indiquent une amélioration de la précision par rapport a l’utilisation
du jeu de 13 étiquettes du FTB. Cependant, les tokens présents dans les mots composés ne
contiennent que l’information de la catégorie principale (attribut catint). Il n’est donc pas
toujours possible de leur attribuer une étiquette optimisée automatiquement. La solution retenue
par (Arun et Keller, 2005) et les travaux suivants consiste a fusionner les tokens et de leur affecter
l’étiquette du mot composé. Par exemple, les tokens du mot composé illustré dans la Figure 2
seront fusionnés en << levée_de_boucliers >> avec l’étiquette NC (ca1:="N"+subca1:="C").
Cette méthodologie simpliﬁe artiﬁciellement la tache d’ét1'quetage mais facilite la comparaison
avec les approches précédentes.

2. Une phrase par ligne dans laquelle chaque mot est suivi d’un séparateur et de son étiquette. Par example, la phrase
de la Figure 1 doit étre converti en : Ma/D position/N est/V la/D suivante/N . /PONCT

283

<w ca1:="N" [...] lemma="levée de boucliers" mph=":Es" subca1:="C">
<w ca1:in1:="N">levée</w>
<w ca1:in1:="P">de</w>
<w ca1:in1:="N">boucliers</w>
</w>

FIGURE 2 — Exemple de mot composé extrait du ﬁchier lmf300_13000ep . cat .xml.

Néanmoins, entrainer des méthodes d’étiquetage avec un ensemble de données dans lequel
les mots composés sont fusionnés suppose par la suite l’utilisau'on d’un tokenizer capable de
détecter les mots composés. Or, les méthodes existantes ne sont pas encore arrivées a un niveau
de maturité satisfaisant. De plus, la notion de mot composé reste encore ambigué et spéciﬁque
aux choix faits par les annotateurs du FTB. En effet, la déﬁnition du mot composé dans le FTB est
assez large, avec par exemple << 61 tout prix » ou << seconde guerre mondiale ». Nous avons fait ici
un choix restrictif en ne fusionnant que les mots composés lexicaux dont le lemme ne contient
pas le caractére espace (e.g. « aujourd’ » + « hui » —> « aujourd’hui », « celles » + « - » + « ci » —>
« celles-ci ») ainsi que les nombres décimaux (e.g. « 16 » + « , » + « 7 » —> « 16,7 ») et découpés
(e.g. « 500 >> + « 000 >> —> « 500000 >>). Un total de 8 967 mots-composés sont fusionnés de cette
mamere.

Pour l’ensemble des raisons que nous avons évoquées précédemment, nous utilisons dans cette
étude le jeu de 13 étiquettes dérivées des catégories principales du FTB. Ce choix est également
appuyé par le fait que nous souhaitons proposer un ensemble de regles de corrections automa-
tiques ne nécessitant pas de ressources externes ou d’intervention manuelle. Le corpus généré a
partir de cette conversion directe du FTB contient 7 747 tokens pour lesquels aucune étiquette n’a
pu étre affectée, i.e. soit l’étiquette est manquante, soit l’étiquette présente n’est pas valide. Au
total, le corpus généré contient 2 090 phrases dans lesquelles au moins un token sans étiquette
est présent.

D’un point de vue pratique, la plupart des systemes d’étiquetage nécessitent des données d’en-
trainement completement étiquetées (i.e. sans étiquette manquante). Nous attribuons donc
l’étiquette U (pour Unknown) aux tokens pour lesquels aucune étiquette n’a pu étre affectée.

3 Correction automatique des erreurs d’annotation

La méthodologie de correction automatique des erreurs d’annotation peut étre décomposée
en deux étapes : i. identiﬁer les occurrences des mots incorrectement étiquetés (ou ayant
une étiquette manquante) dans le corpus; ii. assigner les bonnes étiquettes correspondant ‘a
ces occurrences. Concemant la seconde étape, nous avons avant tout cherché ‘a privilégier la
précision des corrections. Ainsi, notre choix s’est porté sur des méthodes cherchant ‘a assigner
une étiquette corrective avec la plus grande conﬁance possible au détriment du nombre d’erreurs
candidates corrigées.

Nous proposons deux méthodes pour corriger les erreurs : la premiere vise la correction des
étiquettes manquantes de certains mots ‘a l’aide de la fréquence d’occurrence des étiquettes
associées a d’autres occurrences du mot (Section 3.1), que nous désignerons par FTB+COI'I'. 1. La
seconde vise la correction des erreurs d’annotation par la détection des variations d’étiquetage
pour des n-grammes de mots (Section 3.2), que nous désignerons par FTB+COI'I'. 2.

284

3.1 Correction des étiquettes manquantes

Une solution simple au probléme des étiquettes manquantes consiste a attribuer l’étiquette de la
forme correspondante dans le corpus. Dans le cas ou plusieurs étiquettes ont été attribuées ‘a
un méme token, la plus fréquente sera choisie. Cette stratégie peut s’avérer étre problématique
dans le cas o1‘1les fréquences des différentes étiquettes d’un token sont égales ou tres proches.
Par exemple, « quelque » apparait 47 fois en tant qu’adverbe, 46 fois en tant que déterminant et
34 fois en tant qu’adjectif. Le choix de l’étiquette serait dans ce cas ambigu. Pour minimiser le
risque d’in1Ioduire des erreurs d’annotations, nous n’attribuons l’étiquette la plus fréquente que
si sa fréquence dans le corpus est supérieure ‘a la somme des fréquences des autres étiquettes
candidates. Seules les étiquettes avec une fréquence supérieure a 1 sont utilisées.

Le nombre de tokens sans étiquette est ainsi ramené ‘a 901, tandis que le nombre de phrases
contenant au moins une étiquette manquante est réduit de 2 090 a 582. Malgré les contraintes
que nous avons mises en place, il est probable que cette méthode de correction introduit des
étiquettes erronées. La seconde méthode que nous décrivons dans la section suivante permet de
détecter et de corriger les éventuelles séquences d’étiquettes erronées.

3.2 Détection et correction des variations d’annotation

Aﬁn de détecter les erreurs d’annotation nous mettons en oeuvre l’approche proposée par
(Dickinson et Meurers, 2003) puis reprise par (Loftsson, 2009) pour évaluer les corpus Wall
Street Journal (wsa) et Icelandic Frequency Dictionary (IFD). L’approche repose sur la détection de
variations d’étiquetage pour un méme n-gramme de mots. On utilisera le terme de variation de
n-gramme (variation n-gram) pour désigner un n-gramme de mots dans un corpus qui contient un
mot annoté différemment dans une autre occurrence du méme n-gramme dans le corpus. I.e(s)
mot(s) sujet(s) a la variation (qui ont une étiquette différente dans les différentes occurrences)
est(sont) appelé(s) noyau de variation (variation nucleus).

La présence au sein d’un corpus d’une variation d’annotations pour un méme n-gramme de mots
peut s’expliquer soit par l’ambigu’ité des mots noyaux de la variation (une méme forme peut
admettre des étiquettes distinctes dans un contexte d’occurrence différent) soit par une erreur.
L’hypothése que l’on pose est : plus des contextes d’une variation sont similaires, plus grande est
la probabilité qu’il s’agisse d’une erreur. La notion de contexte se traduit ici par le nombre de
mots, n, que l’on considére dans les n-grammes observés. La table 1 rapporte une comparaison
en chiffres des observations menées sur les différents corpus traités par cette méthode.

Comme l’explique (Loftsson, 2009), une méme erreur candidate peut étre détectée plusieurs
fois du fait du fait qu’un n-gramme de mots peut se retrouver contenu dans un autre pour une
valeur de n supérieure. De plus, une variation de n-gramme contient a minima deux annotations
possibles pour le méme n-gramme de mots. I1 n’est ainsi pas facile de calculer la précision de
cette méthode (i.e. le ratio d’erreurs correctement détectées sur toutes les erreurs candidates).

(Dickinson et Meurers, 2003; Loftsson, 2009) avaient pour objectif d’évaluer manuellement
le nombre de variations distinctes correctes. Pour cette raison, ils ont choisi un n minimal
sufﬁsamment grand pour que le contexte soit discriminant. Ils ont par ailleurs considéré la plus
longue variation de n-grammes pour chaque occurrence de mot présentant une variation aﬁn
d’avoir le plus de matériel sous les yeux pour permettre la levée de l’ambigu’ité. Notre objectif

285

Corpus WSJ mp FTB+C01'I'. 2 FTB+C01'I'. 1&2

# tokens 1,3 M 590 297 628 767

# étiquettes 36 700 13**

+ longue variation 224 20 87

Valeur de n observé n 2 6 n 2 5 n 2 5
Variations distinctes 2495 752 293 147
Vraies erreurs 2436 (97,6%) 254 — —
# tokens corrigés 4417 (0,34%) 236* (0,04%) 741 169

TABLE 1 — Comparatifs des corpus en chiffres sur lesquels des variations de n-gramme ont été
calculées. * Nous constatons que le nombre réel de tokens corrigés, calculé a partir du pourcentage
fourni par (Loftsson, 2009), est inférieur au nombre de variations étant de vraies erreurs; nous
supposons que cela est peut étre dﬁ a une erreur dans le recensement des variations distinctes
observées. ** A ce nombre i1 faut rajouter une étiquette supplémentaire que l’on utilise pour tous
les mots qui n’ont pas nativement une des 13 étiquettes retenues.

différe puisque nous souhaitons détecter et corriger des erreurs automatiquement. Nous sommes
néanmoins sensibles au fait de poser un n suffisamment grand pour discriminer mais aussi
suffisamment petit pour que la différence du nombre d’occurrences entre les variations puisse
étre utilisée pour ﬁltrer les variations les moins probables. Du fait que 1’1D1> et le FTB ont une taille
proche, nous suivons 1e choix de (Loftsson, 2009) et optons pour n 2 5.

Nous proposons une heuristique pour corriger certaines variations. Celle-ci est la suivante : nous
considérons les n-grammes par taille décroissante, puis par nombre d’occurrences décroissant.
Nous sélectionnons les candidats pour une correction selon deux contraintes : i. la présence
d’au moins deux unités lexicales et ii. 1a présence d’une variation, sans étiquette manquante,
dont 1e nombre d’occurrence est strictement supérieur ‘a la somme des occurrences des autres
variations. De fait seuls les n-grammes apparaissant au moins trois fois sont considérés. Cette
derniere contrainte nous sert aussi de base pour proposer une correction. En effet, 1a variation
qui valide la contrainte est considérée comme 1a séquence d’étiquettes correcte.

Les exemples 1, 2 et 3 illustrent des corrections opérées avec cette heuristique. Les mots corrigés
sont soulignés.

(1) ,/PONCT 1’/D une/N des/P plus/ADV—> ,/PONCT 1’/D une/PRO des/P plus/ADV

(2) produit/N intérieur/N brut/A (/PONCT FIB/N )/PONCT —> produit/N intérieur/A
brut/A (/PONCT FIB/N )/PONCT

(3) d’/P état/N chargé/N de/P la/D—>d’/P état/N chargé/V de/P la/D

Pour n 2 5, lorsque 1’on applique cette méthode directement sur le FTB, nous comptons 293
variations distinctes (vériﬁant les contraintes citées ci-dessus) et le nombre de tokens corrigé
est 741 (dont 593 étaient sans étiquette). Le nombre de tokens corrigés augrnente1orsque1’on
diminue la taille minimale des n-grammes traités. Nous avons néanmoins préféré garder un n
sufﬁsamment haut pour maintenir une certaine conﬁance dans le choix de considérer certaines
des variations détectées comme erreurs.

Intrinsequement la méthode par détection de variations de n-gramme repose sur le nombre
d’occurrences des n-grammes. La méthode est donc sensible ‘a la taille du corpus et1’on peut

286

s’attendre a ce qu’e]le fournisse de meilleurs résultats sur des corpus homogenes (i.e. d’un genre
spéciﬁque) utilisant un jeu d’étiquette ‘a gros grain; caractéristiques que nous retrouvons dans
le FTB.

4 Résultats

L’évaluation des corrections apportées au corpus est réalisée de maniére extrinséque. L’idée
derriére cette méthodologie est simple, il s’agit de comparer les scores de performance de
différentes méthodes d’ét1'quetage morpho-syntaxique en fonction du niveau de correction du
FTB. Une amélioration de la précision de l’étiquetage est une indication indirecte de la bonne
correction du corpus.

Les méthodes d’étiquetage morpho-syntaxique fondées sur des modéles probabilistes discrimi-
nants atteignent des niveaux de performance trés élevés. Dans cette étude, nous avons choisi
deux systémes utilisant des modéles par maximum d’entropie (MaxEnt) : la version 3.0.4 du
Stanford POS Tagger (Toutanova et aL, 2003) et l’étiqueteur morpho-syntaxique de la suite Apache
OpenNLP3. Le Stanford POS Tagger a été entrainé avec un ensemble standard 4 de traits bidirec-
tionnels sur les mots et les étiquettes. L’étiqueteur d’Apache OpenNLP a, quant a lui, été entrainé
avec l’implémentation par défaut qui caractérise chaque mot a l’aide de traits caractéristiques des
trois mots précédents et suivants. Ces traits sont les préﬁxes et les sufﬁxes de quatre caractéres,
la classe rudimentaire d’information de ces caractéres (e.g. débute avec une majuscule, est un
nombre, est un symbole), l’ét1'quette grammaticale et la forme de surface des mots. On note
que les ensembles de traits que nous utilisons n’ont pas été optimisés pour le francais, cette
tache sortant du cadre de notre étude. Les résultats que nous présentons ici ne correspondent
donc pas a la performance maximale des systemes. De plus, nous souhaitons préciser que nous
n’entendons pas comparer ici les deux systémes. Il faudrait utiliser les mémes ensembles de traits
pour discuter a minima de leur implémentation de l’algorithme MaxEnt. Les résultats sont donc
donnés a titre informatif principalement parce qu’ils sont tous deux utilisés dans la communauté.

Les deux systémes sont entrainées et évaluées ‘a partir des différents niveaux de correction du
FTB. I.’ensemble de données 1='rB+corr. 1 correspond a la correction des étiquettes manquantes
par la fréquence (Section 3.1), FTB+COI'I'. 2 correspond a la correction des erreurs d’annotat1'on
par la méthode des variations de n-grammes (Section 3.2). FTB+COIT. 1&2 et 1='rB+corr. 2&1
correspondent a l’utilisation successive des deux méthodes de correction : corr. 1 puis +corr. 2 et
inversement.

Dans la littérature, les méthodes d’étiquetage morpho-syntaxique pour le francais ont presque
toujours été évaluées a partir d’un découpage du FTB en trois sous-ensembles : 80% pour
l’entrainement, 10% pour le développement et 10% pour le test, e.g. (Denis et Sagot, 2010;
Constant et aL, 2011). Intuitivement, une évaluation fondée uniquement sur 10% des données
ne peut pas étre représentative du niveau de performance réel d’une méthode. Une premiere
série d’expériences nous a conforté dans cette idée puisque nous avons observé une variation
de plus de 2% (en absolu) de la précision en fonction du découpage effectué. Ia construction
incrémentale du FTB ainsi que la nature des documents annotés joue un role prépondérant dans
ce phénomene. Pour palier ce probleme, les résultats que nous présentons dans cette étude ont

3. http: / / incubator . apache . org/open.nlp/
4. Nous avons utilisé la macro naacl2003un.knowns décrite dans (Toutanova et aL, 2003).

287

tous été obtenus en validation croisée en 10 strates, ils ne sont donc pas directement comparables
5 ceux présentés dans les travaux précédents.

Trois mesures d’évaluation sont considérées comme pertinentes pour nos expériences : 1a précision
sur les tokens, 1a précision sur les phrases (nombre de phrases dans 1esque]les tous les tokens ont
été correctement étiquetés par rapport au nombre de phrases total) et la précision sur les mots
inconnus. L’écart type (0) des scores sur les 10 strates est également calculé.

Les résultats sont présentés dans les tables 2 et 3. Les corrections apportées au corpus permettent
d’amé1iorer les scores de précision des méthodes d’étiquetage de maniére signiﬁcat1've.Ainsi, la
précision sur les tokens passe de 96,39 5 97,53 pour le Stanford POS tagger et de 95,70 5 97,05
pour Apache OpenNLP. De plus, on peut observer que 1’écart type des scores calculé sur les 10
strates diminue fortement. Cette mesure est un indicateur de 1’amé1ioration de la stabilité du
niveau de performance des systémes. Une amélioration encore plus importante est observée sur
la précision au niveau des phrases, elle passe de 53,05% a 57,05% pour le Stanford POS tagger et
de 47,56% 5 51,67% pour Apache OpenNLP. Cette augmentation s’exp1ique par la réduction du
nombre de phrases contenant au moins un token auquel aucune étiquette n’a pu étre affectée.
Concernant la précision au niveau des mots inconnus, la stabilité des scores est normale puisque
nous n’introduisons pas de nouveaux tokens dans les données.

Correction Stanford POS 'I'agger

Prec. tokens

Prec. phrases

Prec. inconnus

FTB non corrigé
FTB + corr. 1
FTB + corr. 2

FTB + corr. 1&2

FTB + corr. 2&1

96, 39 (o = 0, 96)
97, 52T(o = 0,26)
96, 51T(o = 0, 89)
97, 53*(o = 0, 26)
97, 53T(o = 0, 27)

53,05 (0 = 3,71)
56, 76T(o = 2,15)
53,57*(a = 3,61)
57, 05T(o = 1,15)
57, 02T(o = 2,25)

83,36 (0 = 3,43)
83, 52T(a = 3,40)
83,37 (0 = 3,42)
83, 50 (o = 3, 38)
83,51 (0 = 3,40)

TABLE 2 — Scores de précision obtenus avec le Stanford POS tagger en fonction du niveau de
correction du FTB. 0 correspond 5 1’écart type des scores calculé sur les 10 strates. Les scores
indiqués par les caractéres T sont statistiquement significatifs par rapport au FTB non corrigé

(p < 0, 01 avec un t-test de Student).

Correction

Apache OpenN1.P

Prec. tokens

Prec. phrases

Prec. inconnus

FTB non corrigé
FTB + corr. 1
FTB + corr. 2

FTB + corr. 1&2

FTB + corr. 2&1

95, 82 (a = 0, 95)
97, 03T(o = 0,26)
95, 94*(o = 0, 88)
97, 05T(o = 0,26)
97, 04T(o = 0,26)

47,56 (0 = 3,25)
51,40T(a = 2,04)
48, 08T(o = 3,21)
51,67*(o = 2,13)
51,67*(o = 2,11)

85,50 (0 = 1,57)
85,68 (0 = 1,57)
85,50 (0 = 1,60)
85, 70 (0 = 1,55)
85,68 (0 = 1,57)

TABLE 3 — Scores de précision obtenus avec Apache OpenNLP en fonction du niveau de correction
du FTB. 0 correspond a1’écart type des scores calculé sur les 10 strates. Les scores indiqués par
les caractéres T sont statistiquement signiﬁcatifs par rapport au FTB non corrigé (p < 0.01 avec
un t-test de Student).

288

La méthode de correction par détection de variations de n-grammes permet de ramener davantage
d’erreurs candidates lorsque l’on traite des n-grammes de taille inférieure a 5. De plus nous avons
constaté que pour une taille strictement supérieure a 1, les résultats des étiqueteurs morpho-
syntaxiques étaient améliorés. Nous n’avons pas gardé ces résultats car un premier retour au
corpus nous conduisait a nous interroger sur la qualité des corrections opérées et par conséquent
sur l’amélioration qui pourrait bien étre due a un phénoméne de lissage des annotations du
corpus. Ce dernier point nécessitera une évaluation plus approfondie dans le futur.

5 'I'ravaux connexes

Les méthodes d’étiquetage morpho-syntaxique actuelles, basées sur des modéles probabilistes,
offrent un niveau de performance élevé. L’analyse des erreurs restantes suggérent néanmoins que
le gain de précision potentiel venant de meilleurs traits ou d’une méthode d’apprentissage plus
performante reste trés limité. Les problémes relevés montrent que les inconsistances et les erreurs
d’annotations présentes dans les données d’entrainement et de test sont en partie responsables
du palier auquel les méthodes sont confrontées. Partant de ce constat, (Manning, 2011) propose
un ensemble de régles manuelles visant a corriger les erreurs d’annotation présentes dans le
Penn Treebank. Une évaluation comparative de la précision d’un systéme d’étiquetage morpho-
syntaxique sur les données ainsi corrigées a permis de montrer l’efﬁcacité des régles de correction
proposées.

Concernant la problématique de détection et de correction automatique d’erreurs d’annotation,
la majorité des travaux s’est penchée sur le premier probléme avec une attention particuliére
sur l’étiquetage grammatical (Loftsson, 2009). Outre la question d’annotation d’unité lexicale,
le projet DECCA5 aborde aussi les problemes de détection d’erreurs d’annotations 5 continues
(concernant une séquence de mots), discontinues et de type dépendance.

Quelles que soient les approches et le type d’annotations observé, le principe de détection
d’une erreur repose sur la recherche d’annotations inconsistantes au sein du corpus; c’est-a-dire
d’étiquetages différents pour des occurrences comparables du phénoméne observé.

Concernant la détection d’erreur d’étiquetage grammatical, cinq approches ont été proposées.
(Loftsson, 2009) compare trois méthodes de détection : la premiere fondée sur la détection de
variation de n-gramme, la seconde fondée sur l’utilisation de plusieurs étiqueteurs automatiques
et la troisiéme fondée sur de l’analyse syntaxique en constituants. Ia seconde méthode consiste
‘a utiliser plusieurs étiqueteurs (l’auteur en a utilisé cinq) et ‘a les combiner en utilisant un
simple mécanisme de vote (chaque étiqueteur vote pour une éﬁquette etl’é11'quette avec le plus
grand nombre de votes est sélectionné). La troisiéme méthode consiste a exploiter l’étiquette
syntaxique produite par l’analyse en constituants pour corriger certaines erreurs éventuelles
d’étiquetage grammatical interne. Cette méthode requiert d’idenu'ﬁer dans un premier temps les
types d’erreurs d’étiquetage grammatical possibles sous chaque constituant puis d’écrire les régles
de correction correspondante. Les résultats de (Loftsson, 2009) montrent que ces méthodes
permettent toutes de détecter effectivement des erreurs et qu’elles agissent en complémentarité.

(Loftsson, 2009) rapporte aussi les travaux de (Nakagawa et Matsumoto, 2002) et de (Kveton

5. http: //decca. osu. edu
6. La nature syntaxique ou sérnantique de Pannotation est discutée mais le problerne est secondaire.

289

et Oliva, 2002). (Nakagawa et Matsumoto, 2002) ont utilisé le poids de conﬁance que leur
algorithme de classiﬁcation (machines ‘a vecteurs de support) utilise pour décider de la classe
d’un mot afin de déterminer si la classe assignée était une erreur candidate. Cette méthode
est intéressante méme si l’entrainement des modeles peut étre coﬁteuse pour de larges jeux
d’étiquettes.

(Kveton et Oliva, 2002) décrivent, quant a eux, une méthode semi-automatique pour détecter des
n-grammes d’étiquettes grammaticales invalides en partant d’un ensemble construit a la main
de paires d’étiquettes adjacentes invalides (e.g. un déterminant suivi d’un verbe). La méthode
consiste pour chaque bigramme invalide a construire par collecte successive dans les phrases du
corpus l’ensemble d’étiquettes pouvant apparaitre entre deux étiquettes du bigramme invalide.
Tout n-gramme d’étiquettes débutant et finissant par les étiquettes d’un bigramme invalide et
ayant une étiquette n’appartenant pas a l’ensemble d’étiquettes avérées est considéré comme une
erreur potentielle dans un nouveau corpus. Cette méthode requiert d’une part une construction
manuelle (par un linguiste) de bigrammes invalides et d’autre part ne permet pas de détecter des
n-grammes valides d’étiquettes utilisés incorrectement dans certains contextes.

6 Conclusion et perspectives

Nous avons présenté une étude menée sur la détection et la correction automatique des er-
reurs d’annotation morpho-syntaxique du French TreeBank. Deux méthodes ont été utilisées. La
premiere consiste ‘a identiﬁer les mots sans étiquette et leur attribuer celle d’une forme corres-
pondante observée dans le corpus. La seconde méthode utilise les variations de n-gramme pour
détecter et corriger les anomalies d’annotation. les résultats que nous avons obtenus montrent
que les corrections apportées au corpus permettent d’améliorer de maniére signiﬁcative les scores
de précision de deux différentes méthodes d’étiquetage morpho-syntaxique.

Les perspectives de cette étude sont nombreuses. Dans un premier temps, nous souhaitons
poursuivre nos travaux en utilisant un jeu d’étiquettes plus étendu. Nous envisageons également
d’améliorer la détection des erreurs en utilisant conjointement les variations de n-gramme et
la combinaison des sorties de plusieurs étiqueteurs (Loftsson, 2009). A plus long terme, nous
voulons étudier la possibilité d’utiliser la correction automatique des erreurs d’annotation soit
comme une étape préliminaire ‘a la vérification manuelle des annotations, soit comme une
alternative. Pour ce dernier point, l’objectif serait d’étendre le FTB par l’ajout de données annotées
et corrigées automatiquement.

Les modéles construits a partir des données corrigées pour les étiqueteurs Stanford POS tagger et
Apache OPCTINLP sont disponibles ‘a l’adresse : http://www.lina.u.niv—nantes. fr/?—TALN— .h1:ml

Références

ABEILLE, A., CLEMENT, L. et TOUSSENEL, E (2003). Building a treebank for French. Treebanks :
building and using parsed corpora, pages 165-188.

ARUN, A. et KELLER, F. (2005). Lexicalization in crosslinguistic probabilistic parsing : The case of
French. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics
(ACL’05), pages 306-313, Ann Arbor, Michigan. Association for Computational Linguistics.

290

