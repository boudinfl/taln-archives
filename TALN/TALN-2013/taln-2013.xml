<?xml version="1.0" encoding="UTF-8"?>
<conference>
	<edition>
		<acronyme>TALN'2013</acronyme>
		<titre>20e conférence sur le Traitement Automatique des Langues Naturelles</titre>
		<ville>Les Sables d'Olonne</ville>
		<pays>France</pays>
		<dateDebut>2013-06-17</dateDebut>
		<dateFin>2012-06-21</dateFin>
		<presidents>
			<nom>Emmanuel Morin</nom>
			<nom>Yannick Estève</nom>
		</presidents>
		<typeArticles>
			<type id="invite">Conférenciers invités</type>
			<type id="charte">Charte Ethique et Big Data</type>
			<type id="long">Articles longs</type>
			<type id="court">Articles courts</type>
			<type id="démonstration">Démonstrations</type>
		</typeArticles>
		<statistiques>
			<acceptations id="long" soumissions="70">36</acceptations>
			<acceptations id="court" soumissions="57">35</acceptations>
			<acceptations id="démonstration" soumissions="13">13</acceptations>
		</statistiques>
		<siteWeb>http://www.taln2013.org/</siteWeb>
		<meilleurArticle>
			<articleId></articleId>
		</meilleurArticle>
	</edition>
	<articles>
		<article id="taln-2013-invite-001" session="Conférence invité">
			<auteurs>
				<auteur>
					<nom>Alexander Fraser</nom>
					<email>fraser@cis.uni-muenchen.de</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Center for Information and Language Processing, LMU, Munich</affiliation>
			</affiliations>
			<titre>Améliorer la traduction des langages morphologiquement riches</titre>
			<type>invite</type>
			<pages>1-1</pages>
			<resume>Si les techniques statistiques pour la traduction automatique ont fait des progrès significatifs au cours des 20 dernières années, les résultats pour la traduction de langues morphologiquement riches sont toujours mitigés par rapport aux précédentes générations de systèmes à base de règles. Les recherches actuelles en traduction statistique de langues morphologiquement riches varient grandement en fonction de la quantité de connaissances linguistiques utilisées et de la nature de ces connaissances. Cette variation est plus importante en langue cible (par exemple, les ressources utilisées en traduction automatique statistique respectueuse de linguistique en arabe, en français et en allemand sont très différentes). La conférence portera sur les techniques état de l’art dédiées à la tâche de traduction statistique pour une langue cible qui est morphologiquement plus riche que la langue source.</resume>
			<mots_cles>traduction statistique, langages morphologiquement riches, connaissances linguistiques</mots_cles>
			<title>Improving Translation to Morphologically Rich Languages</title>
			<abstract>While statistical techniques for machine translation have made significant progress in the last 20 years, results for translating to morphologically rich languages are still mixed versus previous generation rule-based systems. Current research in statistical techniques for translating to morphologically rich languages varies greatly in the amount of linguistic knowledge used and the form of this linguistic knowledge. This varies most strongly by target language (e.g., the resources used for linguistically-aware statistical machine translation to Arabic, French, German are very different). The talk will discuss state-of-the-art techniques for statistical translation tasks involving translating to a target language which is morphologically richer than the source language.</abstract>
			<keywords>statistical translation, morphologically rich languages, linguistic knowledge</keywords>
		</article>
		<article id="taln-2013-invite-002" session="Conférence invité">
			<auteurs>
				<auteur>
					<nom>Josiane Mothe</nom>
					<email>Josiane.mothe@irit.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">IRIT, UMR 5505, Université de Toulouse, 118 Route de Narbonne, 31062 Toulouse Cedex</affiliation>
				<affiliation affiliationId="2">IUFM, Ecole interne, Université de Toulouse, 56 av. de l’URSS, 31079 Toulouse</affiliation>
			</affiliations>
			<titre>Recherche d’Information et Traitement Automatique des Langues Naturelles</titre>
			<type>invite</type>
			<pages>2-2</pages>
			<resume>La recherche d’information s’intéresse à l’accès aux documents et une majorité de travaux dans le domaine s’appuie sur les éléments textuels de ces documents écrits en langage naturel. Les requêtes soumisses par les utilisateurs de moteurs de recherche sont également textuelles, même si elles sont très pauvres d’un point de vue linguistique. Il parait donc naturel que les travaux en recherche d’information cherchent à s’alimenter par les avancées et les résultats en traitement automatique des langues naturelles. Malgré les espoirs déçus des années 80, l’engouement pour l’utilisation du traitement du langage naturel en recherche d’information reste intact, poussé par les nouvelles perspectives offertes. Dans cette conférence, nous balayerons les aspects de la recherche d’information qui se sont le plus appuyés sur des éléments du traitement automatique des langues naturelles. Nous présenterons en particulier quelques résultats relatifs à la reformulation automatique de requêtes, à la prédiction de la difficulté des requêtes, au résumé automatique et à la contextualisation de textes courts ainsi que les perspectives actuelles offertes en particulier par les travaux en linguistique computationnelle.</resume>
			<mots_cles>Recherche d’information, traitement automatique des langues, reformulation de requêtes, difficulté des requêtes, résumé automatique</mots_cles>
			<title>Information Retrieval and Natural Language Processing</title>
			<abstract>Information retrieval aims at providing means to access documents. Most of current work in the domain relies on the textual elements of these documents which are written in natural language. Users’ queries are also generally textual, even if the queries are very poor from a linguistic point of view. As a results information retrieval field aimed at feeding on advances and results from natural language processing field. In spite of the disappointed hopes of the 80s, the enthusiasm for using natural language processing in information retrieval remains high, pushed by the new perspectives. In this talk, we will mention the various aspects of information retrieval which rely, at various levels, on natural language processing components. We will present in particular some results regardless automatic query reformulation, query difficulty prediction, automatic summarization and short text contextualization as well as some perspectives offered in particular considering computational linguistics.</abstract>
			<keywords>Information retrieval, natural language processing, query reformulation, query difficulty, automatic summarization</keywords>
		</article>
		<article id="taln-2013-charte-001" session="Charte Ethique &amp; Big Data">
			<auteurs>
				<auteur>
					<nom>Karën Fort</nom>
					<email>karen.fort@loria.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
					<affiliationId>3</affiliationId>
				</auteur>
				<auteur>
					<nom>Alain Couillault</nom>
					<email>alain.couillault@univ-lr.fr</email>
					<affiliationId>4</affiliationId>
					<affiliationId>5</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université de Lorraine</affiliation>
				<affiliation affiliationId="2">LORIA 54500 Vandoeuvre-lès-Nancy</affiliation>
				<affiliation affiliationId="3">ATALA</affiliation>
				<affiliation affiliationId="4">Université de La Rochelle</affiliation>
				<affiliation affiliationId="5">APROGED</affiliation>
			</affiliations>
			<titre>La Charte Éthique et Big Data : pour des ressources pour le TAL (enfin !) traçables et pérennes</titre>
			<type>charte</type>
			<pages>3-4</pages>
			<resume>La charte Ethique &amp; Big Data a été conçue à l’initiative de l’ATALA, de l’AFCP, de l’APROGED et de CAP DIGITAL, au sein d’un groupe de travail mixte réunissant d’autres partenaires académiques et industriels (tels que le CERSA-CNRS, Digital Ethics, Eptica-Lingway, le cabinet Itéanu ou ELRA/ELDA). Elle se donne comme objectif de fournir des garanties concernant la traçabilité des données (notamment des ressources langagières), leur qualité et leur impact sur l’emploi. Cette charte a été adoptée par Cap Digital (co-rédacteur). Nous avons également proposé à la DGLFLF et à l’ANR de l’utiliser. Elle est aujourd’hui disponible sous forme de wiki, de fichier pdf et il en existe une version en anglais. La charte est décrite en détails dans (Couillault et Fort, 2013).</resume>
			<mots_cles>éthique, big data, ressources langagières</mots_cles>
			<title>The Ethics &amp; Big Data Charter : for tractable and lasting NLP resources</title>
			<abstract>The Ethics &amp; Big Data Charter was designed by ATALA, AFCP, APROGED and CAP DIGITAL, in a working group including other academic and industrial partners (such as CERSA-CNRS, Digital Ethics, Eptica-Lingway, Itéanu office or ELRA/ELDA). Its aims at ensuring the traceability and quality of the data (including language resources), how they are produced and their impact on working conditions. This charter has been adopted by Cap Digital (co-writer). We also proposed it to DGLFLF and ANR. As of today, it is available as a wiki, a pdf file and an English version 6. The charter is detailled in (Couillault et Fort, 2013).</abstract>
			<keywords>ethics, big data, language resources</keywords>
		</article>
		<article id="taln-2013-long-001" session="Morphologie et Segmentation">
			<auteurs>
				<auteur>
					<nom>Fatima Zahra Nejme</nom>
					<email>fatimazahra.nejme@gmail.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Siham Boulaknadel</nom>
					<email>Boulaknadel@ircam.ma</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Driss Aboutajdine</nom>
					<email>aboutaj@fsr.ac.ma</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LRIT, Unité Associée au CNRST (URAC 29), Faculté des Sciences, Mohammed V-Agdal, Rabat, Maroc.</affiliation>
				<affiliation affiliationId="2">IRCAM, Avenue Allal El Fassi, Madinat Al Irfane, Rabat-Instituts, Maroc.</affiliation>
			</affiliations>
			<titre>Analyse Automatique de la Morphologie Nominale Amazighe</titre>
			<type>long</type>
			<pages>5-18</pages>
			<resume>Dans le but de préserver le patrimoine amazighe et éviter qu’il soit menacé de disparition, il semble opportun de doter cette langue de moyens nécessaires pour faire face aux enjeux de l'accès au domaine de l'Information et de la Communication (TIC). Dans ce contexte, et dans la perspective de construire des outils et des ressources linguistiques pour le traitement automatique de cette langue, nous avons entrepris de construire un système d’analyse morphologique pour l’amazighe standard du Maroc. Ce système profite des apports des modèles { états finis au sein de l’environnement linguistique de développement NooJ en faisant appel à des règles grammaticales à large couverture.</resume>
			<mots_cles>La langue amazighe, TALN, NooJ, analyse morphologique, morphologie flexionnelle, morphologie dérivationnelle</mots_cles>
			<title>Morphological analysis of the standard Amazigh language using NooJ platform</title>
			<abstract>In the aim of safeguarding the Amazigh heritage from being threathned of disappearance, it seems opportune to equip this language of necessary means to confront the stakes of access to the domain of New Information and Communication Technologies (ICT). In this context, and in the perspective to build tools and linguistic resources for the automatic processing of Amazigh language, we have undertaken to develop a system of a morphological description for standard Amazigh of Morocco. This system uses finite state technology, within the linguistic developmental environment NooJ by using a large-coverage of morphological grammars covering all grammatical rules.</abstract>
			<keywords>Amazigh language, NLP, NooJ, morphological analysis, inflectional morphology, derivational morphology</keywords>
		</article>
		<article id="taln-2013-long-002" session="Apprentissage">
			<auteurs>
				<auteur>
					<nom>Isabelle Tellier</nom>
					<email>isabelle.tellier@univ-paris3.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Yoann Dupont</nom>
					<email>yoa.dupont@gmail.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Laboratoire Lattice, 1 rue Maurice Arnoux, 92320 Montrouge</affiliation>
			</affiliations>
			<titre>Apprentissage symbolique et statistique pour le chunking:comparaison et combinaisons</titre>
			<type>long</type>
			<pages>19-32</pages>
			<resume>Nous décrivons dans cet article l’utilisation d’algorithmes d’inférence grammaticale pour la tâche de chunking, pour ensuite les comparer et les combiner avec des CRF (Conditional Random Fields), à l’efficacité éprouvée pour cette tâche. Notre corpus est extrait du French TreeBank. Nous proposons et évaluons deux manières différentes de combiner modèle symbolique et modèle statistique appris par un CRF et montrons qu’ils bénéficient dans les deux cas l’un de l’autre.</resume>
			<mots_cles>apprentissage automatique, chunking, CRF, inférence grammaticale, k-RI, FrenchTreeBank</mots_cles>
			<title>Symbolic and statistical learning for chunking : comparison and combinations</title>
			<abstract>We describe in this paper how to use grammatical inference algorithms for chunking, then compare and combine them to CRFs (Conditional Random Fields) which are known efficient for this task. Our corpus is extracted from the FrenchTreebank. We propose and evaluate two ways of combining a symbolic model and a statistical model learnt by a CRF, and show that in both cases they benefit from one another.</abstract>
			<keywords>machine learning, chunking, CRF, grammatical inference, k-RI, French TreeBank</keywords>
		</article>
		<article id="taln-2013-long-003" session="">
			<auteurs>
				<auteur>
					<nom>Yllias Chali</nom>
					<email>yllias.chali@uleth.ca</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Sadid A.Hasan</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Mustapha Mojahid</nom>
					<email>mustapha.mojahid@irit.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">University of Lethbridge, AB, Canada</affiliation>
				<affiliation affiliationId="2">Université Paul Sabatier – IRIT, 118 Rte de Narbonne 31062 Toulouse Cedex</affiliation>
			</affiliations>
			<titre>L’utilisation des POMDP pour les résumés multi-documents orientés par une thématique</titre>
			<type>long</type>
			<pages>33-47</pages>
			<resume>L’objectif principal du résumé multi-documents orienté par une thématique est de générer un résumé à partir de documents sources en réponse à une requête formulée par l’utilisateur. Cette tâche est difficile car il n’existe pas de méthode efficace pour mesurer la satisfaction de l’utilisateur. Cela introduit ainsi une incertitude dans le processus de génération de résumé. Dans cet article, nous proposons une modélisation de l’incertitude en formulant notre système de résumé comme un processus de décision markovien partiellement observables (POMDP) car dans de nombreux domaines on a montré que les POMDP permettent de gérer efficacement les incertitudes. Des expériences approfondies sur les jeux de données du banc d’essai DUC ont démontré l’efficacité de notre approche.</resume>
			<mots_cles>Résumé multi-document, résumé orienté requête, POMDP</mots_cles>
			<title>Using POMDPs for Topic-Focused Multi‐Document Summarization</title>
			<abstract>The main goal of topic-focused multidocument summarization is to generate a summary from the source documents in response to a given query or particular information requested by the user. This task is difficult in large part because there is no significant way of measuring whether the user is satisfied with the information provided. This introduces uncertainty in the current state of the summary generation procedure. In this paper, we model the uncertainty explicitly by formulating our summarization system as a Partially Observable Markov Decision Process (POMDP) since researchers in many areas have shown that POMDPs can deal with uncertainty successfully. Extensive experiments on the DUC benchmark datasets demonstrate the effectiveness of our approach.</abstract>
			<keywords>Topic-focused multi-document summarization, POMDP</keywords>
		</article>
		<article id="taln-2013-long-004" session="Plénière">
			<auteurs>
				<auteur>
					<nom>Olivier Ferret</nom>
					<email>olivier.ferret@cea.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">CEA, LIST, Laboratoire Vision et Ingénierie des Contenus, Gif-sur-Yvette, F-91191 France.</affiliation>
			</affiliations>
			<titre>Sélection non supervisée de relations sémantiques pour améliorer un thésaurus distributionnel</titre>
			<type>long</type>
			<pages>48-61</pages>
			<resume>Les travaux se focalisant sur la construction de thésaurus distributionnels ont montré que les relations sémantiques qu’ils recèlent sont principalement fiables pour les mots de forte fréquence. Dans cet article, nous proposons une méthode pour rééquilibrer de tels thésaurus en faveur des mots de fréquence faible sur la base d’un mécanisme d’amorçage : un ensemble d’exemples et de contre-exemples de mots sémantiquement similaires sont sélectionnés de façon non supervisée et utilisés pour entraîner un classifieur supervisé. Celui-ci est ensuite appliqué pour réordonner les voisins sémantiques du thésaurus utilisé pour sélectionner les exemples et contre-exemples. Nous montrons comment les relations entre les constituants de noms composés similaires peuvent être utilisées pour réaliser une telle sélection et comment conjuguer ce critère à un critère déjà expérimenté sur la symétrie des relations sémantiques. Nous évaluons l’intérêt de cette procédure sur un large ensemble de noms en anglais couvrant un vaste spectre de fréquence.</resume>
			<mots_cles>Sémantique lexicale, similarité sémantique, thésaurus</mots_cles>
			<title>Unsupervised selection of semantic relations for improving a distributional thesaurus</title>
			<abstract>Work about distributional thesauri has shown that the relations in these thesauri are mainly reliable for high frequency words. In this article, we propose a method for improving such a thesaurus through its re-balancing in favor of low frequency words. This method is based on a bootstrapping mechanism : a set of positive and negative examples of semantically similar words are selected in an unsupervised way and used for training a supervised classifier. This classifier is then applied for reranking the semantic neighbors of the thesaurus used for example selection. We show how the relations between the mono-terms of similar nominal compounds can be used for performing this selection and how to associate this criterion with an already tested criterion based on the symmetry of semantic relations. We evaluate the interest of the global procedure for a large set of English nouns with various frequencies.</abstract>
			<keywords>Lexical semantics, semantic similarity, distributional thesauri</keywords>
		</article>		
		<article id="taln-2013-long-005" session="Plénière">
			<auteurs>
				<auteur>
					<nom>Marie Dupuch</nom>
					<email>mdupuch@objetdirect.com</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Thierry Hamon</nom>
					<email>thierry.hamon@univ-paris13.fr</email>
					<affiliationId>3</affiliationId>
				</auteur>
				<auteur>
					<nom> Natalia Grabar</nom>
					<email>natalia.grabar@univ-lille3.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">CNRS UMR 8163 STL, Université Lille 3, 59653 Villeneuve d’Ascq, France</affiliation>
				<affiliation affiliationId="2">Viseo-Objet Direct, 4, avenue Doyen Louis Weil, 38000 Grenoble</affiliation>
				<affiliation affiliationId="3">LIM&amp;BIO (EA3969), Université Paris 13, Sorbonne Paris Cité, 74, rue Marcel Cachin, 93017 Bobigny Cedex France</affiliation>
			</affiliations>
			<titre>Groupement de termes basé sur des régularités linguistiques et sémantiques dans un contexte cross-langue</titre>
			<type>long</type>
			<pages>62-75</pages>
			<resume>Nous proposons d’exploiter des méthodes du Traitement Automatique de Langues dédiées à la structuration de terminologie indépendamment dans deux langues (anglais et français) et de fusionner ensuite les résultats obtenus dans chaque langue. Les termes sont groupés en clusters grâce aux relations générées. L’évaluation de ces relations est effectuée au travers de la comparaison des clusters avec des données de référence et la baseline, tandis que la complémentarité des relations est analysée au travers de leur implication dans la création de clusters de termes. Les résultats obtenus indiquent que : chaque langue contribue de manière équilibrée aux résultats, le nombre de relations hiérarchiques communes est plus grand que le nombre de relations synonymiques communes. Globalement, les résultats montrent que, dans un contexte cross-langue, chaque langue permet de détecter des régularités linguistiques et sémantiques complémentaires. L’union des résultats obtenus dans les deux langues améliore la qualité globale des clusters.</resume>
			<mots_cles>Relations sémantiques, termes, domaine de spécialité, médecine, contexte crosslangue</mots_cles>
			<title>Grouping of terms based on linguistic and semantic regularities in a cross-lingual context</title>
			<abstract>We propose to exploit the Natural Language Processing methods dedicated to terminology structuring independently in two languages (English and French) and then to merge the results obtained in each language. The terms are grouped into clusters thanks to the generated relations. The evaluation of the relations is done via the comparison of the clusters with the reference data and the baseline, while the complementarity of the relations is analyzed through their involvement in the clusters of terms. Our results indicate that : each language contributes almost equally to the generated results ; the number of common hierarchical relations is greater than the number of common synonym relations. On the whole, the obtained results point out that in a cross-language context, each language brings additional linguistic and semantic regularities. The union of the results obtained in each language improves the overall quality of the clusters.</abstract>
			<keywords>Semantic relations, terms, specialized areas, medicine, cross-lingual context</keywords>
		</article>
		<article id="taln-2013-long-006" session="Traduction et Alignement">
			<auteurs>
				<auteur>
					<nom>Quentin Pradet</nom>
					<email>quentin.pradet@cea.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Jeanne Baguenier-Desormeaux</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Gaël de Chalendar</nom>
					<email>gael.de-chalendar@cea.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Laurence Danlos</nom>
					<email>laurence.danlos@linguist.univ-paris-diderot.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">CEA, LIST, Laboratoire Vision et Ingénierie des Contenus,Gif-sur-Yvette, F-91191, France.</affiliation>
				<affiliation affiliationId="2">Univ Paris Diderot, Sorbonne Paris Cité, ALPAGE, UMR-I 001 INRIA</affiliation>
			</affiliations>
			<titre>WoNeF : amélioration, extension et évaluation d’une traduction française automatique de WordNet</titre>
			<type>long</type>
			<pages>76-89</pages>
			<resume>Identifier les sens possibles des mots du vocabulaire est un problème difficile demandant un travail manuel très conséquent. Ce travail a été entrepris pour l’anglais : le résultat est la base de données lexicale WordNet, pour laquelle il n’existe encore que peu d’équivalents dans d’autres langues. Néanmoins, des traductions automatiques de WordNet vers de nombreuses langues cibles existent, notamment pour le français. JAWS est une telle traduction automatique utilisant des dictionnaires et un modèle de langage syntaxique. Nous améliorons cette traduction, la complétons avec les verbes et adjectifs de WordNet, et démontrons la validité de notre approche via une nouvelle évaluation manuelle. En plus de la version principale nommée WoNeF, nous produisons deux versions supplémentaires : une version à haute précision (93% de précision, jusqu’à 97% pour les noms), et une version à haute couverture contenant 109 447 paires (littéral, synset).</resume>
			<mots_cles>WordNet, désambiguïsation lexicale, traduction, ressource</mots_cles>
			<title>WoNeF, an improved, extended and evaluated automatic French translation of WordNet</title>
			<abstract>Identifying the various possible meanings of each word of the vocabulary is a difficult problem that requires a lot of manual work. It has been tackled by the WordNet lexical semantics database in English, but there are still few resources available for other languages. Automatic translations of WordNet have been tried to many target languages such as French. JAWS is such an automatic translation of WordNet nouns to French using bilingual dictionaries and a syntactic langage model. We improve the existing translation precision and coverage, complete it with translations of verbs and adjectives and enhance its evaluation method, demonstrating the validity of the approach. In addition to the main result called WoNeF, we produce two additional versions : a high-precision version with 93% precision (up to 97% on nouns) and a high-coverage version with 109,447 (literal, synset) pairs.</abstract>
			<keywords>WordNet, Word Sense Disambiguation, translation, resource</keywords>
		</article>
		<article id="taln-2013-long-007" session="Sémantique">
			<auteurs>
				<auteur>
					<nom>Bassam Jabaian</nom>
					<email>{bassam.jabaian@univ-avignon.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Fabrice Lefèvre</nom>
					<email>fabrice.lefevre@univ-avignon.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Laurent Besacier</nom>
					<email>laurent.besacier@imag.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIA, Université d’Avignon et des Pays de Vaucluse, Avignon, France</affiliation>
				<affiliation affiliationId="2">LIG, Université Joseph Fourrier, Grenoble, France</affiliation>
			</affiliations>
			<titre>Approches statistiques discriminantes pour l’interprétation sémantique multilingue de la parole</titre>
			<type>long</type>
			<pages>90-103</pages>
			<resume>Les approches statistiques sont maintenant très répandues dans les différentes applications du traitement automatique de la langue et le choix d’une approche particulière dépend généralement de la tâche visée. Dans le cadre de l’interprétation sémantique multilingue, cet article présente une comparaison entre les méthodes utilisées pour la traduction automatique et celles utilisées pour la compréhension de la parole. Cette comparaison permet de proposer une approche unifiée afin de réaliser un décodage conjoint qui à la fois traduit une phrase et lui attribue ses étiquettes sémantiques. Ce décodage est obtenu par une approche à base de transducteurs à états finis qui permet de composer un graphe de traduction avec un graphe de compréhension. Cette représentation peut être généralisée pour permettre des transmissions d’informations riches entre les composants d’un système d’interaction vocale homme-machine.</resume>
			<mots_cles>compréhension multilingue, système de dialogue, CRF, graphes d’hypothèses</mots_cles>
			<title>Discriminative statistical approaches for multilingual speech understanding</title>
			<abstract>Statistical approaches are now widespread in the various applications of natural language processing and the elicitation of an approach usually depends on the targeted task. This paper presents a comparison between the methods used for machine translation and speech understanding. This comparison allows to propose a unified approach to perform a joint decoding which translates a sentence and assign semantic tags to the translation at the same time. This decoding is achieved through a finite-state transducer approach which allows to compose a translation graph with an understanding graph. This representation can be generalized to allow the rich transmission of information between the components of a human-machine vocal interface.</abstract>
			<keywords>multilingual understanding, dialogue system, CRF, hypothesis graphs</keywords>
		</article>
		<article id="taln-2013-long-008" session="Connaissances et Discours">
			<auteurs>
				<auteur>
					<nom>Chloé Braud</nom>
					<email>chloe.braud@inria.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Pascal Denis</nom>
					<email>pascal.denis@inria.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">ALPAGE, INRIA Paris-Rocquencourt &amp; Université Paris Diderot</affiliation>
				<affiliation affiliationId="2">MAGNET, INRIA Lille Nord-Europe</affiliation>
			</affiliations>
			<titre>Identification automatique des relations discursives « implicites » à partir de données annotées et de corpus bruts</titre>
			<type>long</type>
			<pages>104-117</pages>
			<resume>Cet article présente un système d’identification des relations discursives dites « implicites » (à savoir, non explicitement marquées par un connecteur) pour le français. Etant donné le faible volume de données annotées disponibles, notre système s’appuie sur des données étiquetées automatiquement en supprimant les connecteurs non ambigus pris comme annotation d’une relation, une méthode introduite par (Marcu et Echihabi, 2002). Comme l’ont montré (Sporleder et Lascarides, 2008) pour l’anglais, cette approche ne généralise pas très bien aux exemples de relations implicites tels qu’annotés par des humains. Nous arrivons au même constat pour le français et, partant du principe que le problème vient d’une différence de distribution entre les deux types de données, nous proposons une série de méthodes assez simples, inspirées par l’adaptation de domaine, qui visent à combiner efficacement données annotées et données artificielles. Nous évaluons empiriquement les différentes approches sur le corpus ANNODIS : nos meilleurs résultats sont de l’ordre de 45.6% d’exactitude, avec un gain significatif de 5.9% par rapport à un système n’utilisant que les données annotées manuellement.</resume>
			<mots_cles>analyse du discours, relations implicites, apprentissage automatique</mots_cles>
			<title>Automatically identifying implicit discourse relations using annotated data and raw corpora</title>
			<abstract>This paper presents a system for identifying « implicit » discourse relations (that is, relations that are not marked by a discourse connective). Given the little amount of available annotated data for this task, our system also resorts to additional automatically labeled data wherein unambiguous connectives have been suppressed and used as relation labels, a method introduced by (Marcu et Echihabi, 2002). As shown by (Sporleder et Lascarides, 2008) for English, this approach doesn’t generalize well to implicit relations as annotated by humans. We show that the same conclusion applies to French due to important distribution differences between the two types of data. In consequence, we propose various simple methods, all inspired from work on domain adaptation, with the aim of better combining annotated data and artificial data. We evaluate these methods through various experiments carried out on the ANNODIS corpus : our best system reaches a labeling accuracy of 45.6%, corresponding to a 5.9% significant gain over a system solely trained on manually labeled data.</abstract>
			<keywords>discourse analysis, implicit relations, machine learning</keywords>
		</article>
		<article id="taln-2013-long-009" session="Apprentissage">
			<auteurs>
				<auteur>
					<nom>Emmanuel Lassalle</nom>
					<email>emmanuel.lassalle@ens-lyon.org</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Pascal Denis</nom>
					<email>pascal.denis@inria.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Alpage : INRIA - Université Paris Diderot, Sorbonne Paris Cité</affiliation>
				<affiliation affiliationId="2">Magnet : INRIA Nord Lille Europe - Université de Lille LIFL</affiliation>
			</affiliations>
			<titre>Apprentissage d’une hiérarchie de modèles à paires spécialisés pour la résolution de la coréférence</titre>
			<type>long</type>
			<pages>118-131</pages>
			<resume>Nous proposons une nouvelle méthode pour améliorer significativement la performance des modèles à paires de mentions pour la résolution de la coréférence. Étant donné un ensemble d’indicateurs, notre méthode apprend à séparer au mieux des types de paires de mentions en classes d’équivalence, chacune de celles-ci donnant lieu à un modèle de classification spécifique. La procédure algorithmique proposée trouve le meilleur espace de traits (créé à partir de combinaisons de traits élémentaires et d’indicateurs) pour discriminer les paires de mentions coréférentielles. Bien que notre approche explore un très vaste ensemble d’espaces de trait, elle reste efficace en exploitant la structure des hiérarchies construites à partir des indicateurs. Nos expériences sur les données anglaises de la CoNLL-2012 Shared Task indiquent que notre méthode donne des gains de performance par rapport au modèle initial utilisant seulement les traits élémentaires, et ce, quelque soit la méthode de formation des chaînes ou la métrique d’évaluation choisie. Notre meilleur système obtient une moyenne de 67.2 en F1-mesure MUC, B3 et CEAF ce qui, malgré sa simplicité, le situe parmi les meilleurs systèmes testés sur ces données.</resume>
			<mots_cles>résolution de la coréférence, apprentissage automatique</mots_cles>
			<title>Learning a hierarchy of specialized pairwise models for coreference resolution</title>
			<abstract>This paper proposes a new method for significantly improving the performance of pairwise coreference models. Given a set of indicators, our method learns how to best separate types of mention pairs into equivalence classes for which we construct distinct classification models. In effect, our approach finds the best feature space (derived from a base feature set and indicator set) for discriminating coreferential mention pairs. Although our approach explores a very large space of possible features spaces, it remains tractable by exploiting the structure of the hierarchies built from the indicators. Our experiments on the CoNLL-2012 shared task English datasets indicate that our method is robust to different clustering strategies and evaluation metrics, showing large and consistent improvements over a single pairwise model using the same base features. Our best system obtains 67.2 of average F1 over MUC, B3, and CEAF which, despite its simplicity, places it among the best performing systems on these datasets.</abstract>
			<keywords>coreference resolution, machine learning</keywords>
		</article>
		<article id="taln-2013-long-010" session="Connaissances &amp; Discours">
			<auteurs>
				<auteur>
					<nom>Jean-Philippe Fauconnier</nom>
					<email>Jean-Philippe.Fauconnier@irit.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Mouna Kamel</nom>
					<email>Mouna.Kamel@irit.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Bernard Rothenburger</nom>
					<email>Bernard.Rothenburger@irit.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Nathalie Aussenac-Gilles</nom>
					<email>Nathalie.Aussenac-Gilles@irit.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
                        </auteurs>
			<affiliations>
				<affiliation affiliationId="1">IRIT, 118 route de Narbonne 31060 Toulouse Cedex 5</affiliation>
			</affiliations>
			<titre>Apprentissage supervisé pour l’identification de relations sémantiques au sein de structures énumératives parallèles</titre>
			<type>long</type>
			<pages>132-145</pages>
			<resume>Ce travail s’inscrit dans le cadre de la construction et l’enrichissement d’ontologies à partir de textes de type encyclopédique ou scientifique. L’originalité de notre travail réside dans l’extraction de relations sémantiques exprimées au-delà de la linéarité du texte. Pour cela, nous nous appuyons sur la sémantique véhiculée par les caractères typo-dispositionels qui ont pour fonction de suppléer des formulations strictement linguistiques qui seraient plus difficilement exploitables. L’étude que nous proposons concerne les relations sémantiques portées par les structures énumératives parallèles qui, bien qu’affichant des discontinuités entre ses différents composants, présentent un tout sur le plan sémantique. Ce sont des structures textuelles qui sont propices aux relations hiérarchiques. Après avoir défini une typologie des relations portées par ce type de structure, nous proposons une approche par apprentissage visant à leur identification. Sur la base de traits incorporant informations lexico-syntaxiques et typo-dispositionnelles, les premiers résultats aboutissent à une exactitude de 61,1%.</resume>
			<mots_cles>extraction de relations, structures énumératives parallèles, mise en forme matérielle, apprentissage supervisé, construction d’ontologies</mots_cles>
			<title>A Supervised learning for the identification of semantic relations in parallel enumerative structures</title>
			<abstract>This work falls within the framework of ontology engineering and learning from encyclopedic or scientific texts. Our original contribution lies within the extraction of semantic relations expressed beyond the text linearity. To this end, we relied on the semantics behind the typo-dispositional characters whose function is to supplement the strictly linguistic formulations that could be more difficult to exploit. The work reported here is dealing with the semantic relations carried by the parallel enumerative structures. Although they display discontinuities between their various components, these enumerative structures form a whole at the semantic level. They are textual structures that are prone to hierarchic relations. After defining a typology of the relationships carried by this type of structure, we are proposing a learning approach aimed at their identification. Based on features including lexico-syntactic and typo-dispositional informations, the first results led an accuracy of 61.1%.</abstract>
			<keywords>relationship extraction, parallel enumerative structures, material shaping, supervised learning, ontology learning</keywords>
		</article>
		<article id="taln-2013-long-011" session="Lexique et Corpus">
			<auteurs>
				<auteur>
					<nom>Marie-Paule Jacques</nom>
					<email>marie-paule.jacques@ujf-grenoble.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Laura Hartwell</nom>
					<email>laura.hartwell@ujf-grenoble.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Achille Falaise</nom>
					<email>achille.falaise@imag.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Univ. Grenoble Alpes, UJF &amp; LIDILEM, F-38040 Grenoble</affiliation>
				<affiliation affiliationId="2">Univ. Grenoble Alpes, UPMF &amp; LIG-GETALP, F-38040 Grenoble</affiliation>
			</affiliations>
			<titre>Techniques de TAL et corpus pour faciliter les formulations en anglais scientifique écrit</titre>
			<type>long</type>
			<pages>146-159</pages>
			<resume>Nous présentons l'adaptation de la base d'écrits scientifiques en ligne Scientext pour un « nouveau » public : chercheurs et autres auteurs français d'écrits scientifiques, ayant besoin de rédiger en anglais. Cette adaptation a consisté à ajouter dans la base des requêtes précodées qui permettent d'afficher les contextes dans lesquels les auteurs d'articles scientifiques en anglais expriment leur objectif de recherche et à enrichir l'interface ScienQuest de nouvelles fonctionnalités pour mémoriser et réafficher les contextes pertinents, pour faciliter la consultation par un public plus large. Les nombreuses descriptions linguistiques de la rhétorique des articles scientifiques insistent sur l'importance de la création et de l'occupation d'une « niche » de recherche. Chercheurs et doctorants ont ici un moyen d'en visualiser des exemples sans connaître sa formulation a priori, via nos requêtes. Notre évaluation sur le corpus de test en donne une précision globale de 86,5 %.</resume>
			<mots_cles>anglais, patrons lexico-syntaxiques, ScienQuest, Scientext</mots_cles>
			<title>NLP and corpus techniques for finding formulations that facilitate scientific writing in English</title>
			<abstract>This paper presents adaptations of the query options integrated into the online corpus Scientext so as to better serve a new audience: French scientists writing in English. We added pre-coded queries that display the contexts in which authors of scientific articles in English state their research objective. Furthermore, new functional options enrich the ScienQuest interface allowing results to be filtered for noise and then saved for consultation by a larger public. Previous studies on the scientific discourse and rhetoric of scientific articles have highlighted the importance of establishing and occupying a research niche. Here, francophone researchers and doctoral students without prior discursive knowledge, can access authentic and multiple ways of formulating a research objective. Our evaluation of a test corpus showed an overall accuracy of 86.5 %.</abstract>
			<keywords>ESP, lexico-syntactic patterns, ScienQuest, Scientext</keywords>
		</article>
		<article id="taln-2013-long-012" session="Lexique et Corpus">
			<auteurs>
				<auteur>
					<nom>Nicolas Hernandez</nom>
					<email>nicolas.hernandez@univ-nantes.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Florian Boudin</nom>
					<email>florian.boudin@univ-nantes.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université de Nantes</affiliation>
			</affiliations>
			<titre>Construction d’un large corpus écrit libre annoté morpho-syntaxiquement en français</titre>
			<type>long</type>
			<pages>160-173</pages>
			<resume>Cet article étudie la possibilité de créer un nouveau corpus écrit en français annoté morphosyntaxiquement à partir d’un corpus annoté existant. Nos objectifs sont de se libérer de la licence d’exploitation contraignante du corpus d’origine et d’obtenir une modernisation perpétuelle des textes. Nous montrons qu’un corpus pré-annoté automatiquement peut permettre d’entraîner un étiqueteur produisant des performances état-de-l’art, si ce corpus est suffisamment grand.</resume>
			<mots_cles>corpus arboré, construction de corpus, étiquetage morpho-syntaxique</mots_cles>
			<title>Construction of a Free Large Part-of-Speech Annotated Corpus in French</title>
			<abstract>This paper studies the possibility of creating a new part-of-speech annotated corpus in French from an existing one. The objectives are to propose an exit from the restrictive licence of the source corpus and to obtain a perpetual modernisation of texts. Results show that it is possible to train a state-of-the-art POS-tagger from an automatically tagged corpus if this one is large enough.</abstract>
			<keywords>French treebank, Building a corpus, Part-of-Speech Tagging</keywords>
		</article>
		<article id="taln-2013-long-013" session="Lexique et Corpus">
			<auteurs>
				<auteur>
					<nom>Anne Abeillé</nom>
					<email>abeille@univ-paris-diderot.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Benoit Crabbé</nom>
					<email>bcrabbe@univ-paris-diderot.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>3</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LLF, CNRS-Université Paris Diderot, 75013 Paris</affiliation>
				<affiliation affiliationId="2">IUF Alpage, INRIA, Université Paris Diderot, 75013 Paris</affiliation>
				<affiliation affiliationId="3">PRES Sorbonne Paris Cité</affiliation>
			</affiliations>
			<titre>Vers un treebank du français parlé</titre>
			<type>long</type>
			<pages>174-187</pages>
			<resume>Nous présentons les premiers résultats d’un corpus arboré pour le français parlé. Il a été réalisé dans le cadre du projet ANR Etape (resp. G. Gravier) en 2011 et 2012. Contrairement à d’autres langues comme l’anglais (voir le Switchboard treebank de (Meteer, 1995)), il n’existe pas de grand corpus oral du francais annoté et validé pour les constituants et les fonctions syntaxiques. Nous souhaitons construire une ressource comparable, qui serait une extension naturelle du Corpus arboré de Paris 7 (FTB : (Abeillé et al., 2003))) basé sur des textes du journal Le Monde. Nous serons ainsi en mesure de comparer, avec des annotations comparables, l’écrit et l’oral. Les premiers résultats, qui consistent à réutiliser l’analyseur de (Petrov et al., 2006) entraîné sur l’écrit, avec une phase de correction manuelle, sont encourageants.</resume>
			<mots_cles>Corpus arboré, français parlé, corpus oral, analyse syntaxique automatique</mots_cles>
			<title>Towards a treebank of spoken French</title>
			<abstract>We present the first results of an attempt to build a spoken treebank for French. It has been conducted as part of the ANR project Etape (resp. G. Gravier). Contrary to other languages such as English (see the Switchboard treebank (Meteer, 1995)), there is no sizable spoken corpus for French annotated for syntactic constituents and grammatical functions. Our project is to build such a resource which will be a natural extension of the Paris 7 treebank (FTB : (Abeillé et al., 2003))) for written French, in order to be able to compare with similar annotations written and spoken French. We have reused and adapted the parser (Petrov et al., 2006) which has been trained on the written treebank, with manual correction and validation. The first results are promising.</abstract>
			<keywords>Treebank, spoken French, spoken corpus, parsing</keywords>
		</article>
		<article id="taln-2013-long-014" session="Syntaxe">
			<auteurs>
				<auteur>
					<nom>Assaf Urieli</nom>
					<email>assaf.urieli@univ-tlse2.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Ludovic Tanguy</nom>
					<email>ludovic.tanguy@univ-tlse2.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">CLLE-ERSS : CNRS &amp; Université de Toulouse 2</affiliation>
			</affiliations>
			<titre>L’apport du faisceau dans l’analyse syntaxique en dépendances par transitions : études de cas avec l’analyseur Talismane</titre>
			<type>long</type>
			<pages>188-201</pages>
			<resume>L’analyse syntaxique (ou parsing) en dépendances par transitions se fait souvent de façon déterministe, où chaque étape du parsing propose une seule solution comme entrée de l’étape suivante. Il en va de même pour la chaîne complète d’analyse qui transforme un texte brut en graphe de dépendances, généralement décomposé en quatre modules (segmentation en phrases, en mots, étiquetage et parsing) : chaque module ne fournit qu’une seule solution au module suivant. On sait cependant que certaines ambiguïtés ne peuvent pas être levées sans prendre en considération le niveau supérieur. Dans cet article, nous présentons l’analyseur Talismane, outil libre et complet d’analyse syntaxique probabiliste du français, et nous étudions plus précisément l’apport d’une recherche par faisceau (beam search) à l’analyse syntaxique. Les résultats nous permettent à la fois de dégager la taille de faisceau la plus adaptée (qui permet d’atteindre un score de 88,5 % d’exactitude, légèrement supérieur aux outils comparables), ainsi que les meilleures stratégies concernant sa propagation.</resume>
			<mots_cles>Analyse syntaxique en dépendances, ambiguïtés, évaluation, beam search</mots_cles>
			<title>APPLYING A BEAM SEARCH TO TRANSITION-BASED DEPENDENCY PARSING: A CASE STUDY FOR FRENCH WITH THE TALISMANE SUITE</title>
			<abstract>Transition-based dependency parsing often uses deterministic techniques, where each parse step provides a single solution as the input to the next step. The same is true for the entire analysis chain which transforms raw text into a dependency graph, generally composed of four modules (sentence detection, tokenising, pos-tagging and parsing): each module provides only a single solution to the following module. However, some ambiguities cannot be resolved without taking the next level into consideration. In this article, we present Talismane, an open-source suite of tools providing a complete statistical parser of French. More specifically, we study the contribution of a beam search to syntax parsing. Our analysis allows us to conclude on the most appropriate beam width (enabling us to attain an accuracy of 88.5%, slightly higher than comparable tools), and on the best strategies concerning beam propagation from one level of analysis to the next.</abstract>
			<keywords>Dependency parsing, ambiguities, evaluation, beam search</keywords>
		</article>
		<article id="taln-2013-long-015" session="Morphologie et Segmentaion">
			<auteurs>
				<auteur>
					<nom>Anca Simon</nom>
					<email>anca-roxana.simon@irisa.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Guillaume Gravier</nom>
					<email>guillaume.gravier@irisa.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Pascale Sébillot</nom>
					<email>pascale.sebillot@irisa.fr</email>
					<affiliationId>3</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université de Rennes 1</affiliation>
				<affiliation affiliationId="2">CNRS</affiliation>
				<affiliation affiliationId="3">INSA de Rennes IRISA &amp; INRIA Rennes</affiliation>
			</affiliations>
			<titre>Un modèle segmental probabiliste combinant cohésion lexicale et rupture lexicale pour la segmentation thématique</titre>
			<type>long</type>
			<pages>202-214</pages>
			<resume>L’identification d’une structure thématique dans des données textuelles quelconques est une tâche difficile. La plupart des techniques existantes reposent soit sur la maximisation d’une mesure de cohésion lexicale au sein d’un segment, soit sur la détection de ruptures lexicales. Nous proposons une nouvelle technique combinant ces deux critères de manière à obtenir le meilleur compromis entre cohésion et rupture. Nous définissons un nouveau modèle probabiliste, fondé sur l’approche proposée par Utiyama et Isahara (2001), en préservant les propriétés d’indépendance au domaine et de faible a priori de cette dernière. Des évaluations sont menées sur des textes écrits et sur des transcriptions automatiques de la parole à la télévision, transcriptions qui ne respectent pas les normes des textes écrits, ce qui accroît la difficulté. Les résultats expérimentaux obtenus démontrent la pertinence de la combinaison des critères de cohésion et de rupture.</resume>
			<mots_cles>segmentation thématique, cohésion lexicale, rupture de cohésion, journaux télévisés</mots_cles>
			<title>A probabilistic segment model combining lexical cohesion and disruption for topic segmentation</title>
			<abstract>Identifying topical structure in any text-like data is a challenging task. Most existing techniques rely either on maximizing a measure of the lexical cohesion or on detecting lexical disruptions. A novel method combining the two criteria so as to obtain the best trade-off between cohesion and disruption is proposed in this paper. A new statistical model is defined, based on the work of Isahara and Utiyama (2001), maintaining the properties of domain independence and limited a priori of the latter. Evaluations are performed both on written texts and on automatic transcripts of TV shows, the latter not respecting the norms of written texts, thus increasing the difficulty of the task. Experimental results demonstrate the relevance of combining lexical cohesion and disrupture.</abstract>
			<keywords>topic segmentation, lexical cohesion, lexical disrupture, TV broadcast news</keywords>
		</article>
		<article id="taln-2013-long-016" session="Syntaxe">
			<auteurs>
				<auteur>
					<nom>Pierre Bourreau</nom>
					<email>bourreau@hhu.de</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">SFB 991 Institut für Sprache und Information Université Heinrich-Heine, 40225 Düsseldorf</affiliation>
			</affiliations>
			<titre>Traitements d’ellipses : deux approches par les grammaires catégorielles abstraites</titre>
			<type>long</type>
			<pages>215-228</pages>
			<resume>L’étude de phénomènes d’ellipses dans les modèles de l’interface syntaxe-sémantique pose certains problèmes du fait que le matériel linguistique effacé au niveau phonologique est néanmoins présent au niveau sémantique. Tel est le cas d’une ellipse verbale ou d’une élision du sujet, par exemple, phénomènes qui interviennent lorsque deux phrases reliées par une conjonction partagent le même verbe, ou le même sujet. Nous proposons un traitement de ces phénomènes dans le formalisme des grammaires catégorielles abstraites selon un patron que nous intitulons extraction/instanciation et que nous implémentons de deux manières différentes dans les ACGs.</resume>
			<mots_cles>ellipse, coordination, interface syntaxe-sémantique, grammaires catégorielles abstraites, grammaires d’arbres adjoints, grammaires IO d’arbres</mots_cles>
			<title>Treating ellipsis : two abstract categorial grammar perspectives</title>
			<abstract>The treatment of ellipsis in models of the syntax-semantics interface is troublesome as the linguistic material removed in the phonologic interpretation is still necessary in the semantics. Examples are particular cases of coordination, especially the ones involving verbal phrase ellipsis or subject elision. We show a way to use abstract categorial grammars so as to implement a pattern we call extraction/instantiation in order to deal with some of these phenomena ; we exhibit two different constructions of this principle into ACGs.</abstract>
			<keywords>ellipsis, coordination, syntax-semantics interface, abstract categorial grammars, tree-adjoining grammars, IO tree-grammars</keywords>
		</article>
		<article id="taln-2013-long-017" session="Plénière">
			<auteurs>
				<auteur>
					<nom>Philippe Blache</nom>
					<email>blache@lpl-aix.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Aix-Marseille Université, CNRS, LPL 5 Avenue Pasteur, 13100 Aix-en-Provence</affiliation>
			</affiliations>
			<titre>Chunks et activation : un modèle de facilitation du traitement linguistique</titre>
			<type>long</type>
			<pages>229-242</pages>
			<resume>Nous proposons dans cet article d’intégrer la notion de chunk au sein d’une architecture globale de traitement de la phrase. Les chunks jouent un rôle important dans les théories cognitives comme ACT-R (Anderson et al., 2004) : il s’agit d’unités de traitement globales auxquelles il est possible d’accéder directement via des buffers en mémoire à court ou long terme. Ces chunks sont construits par une fonction d’activation (processus cognitif pouvant être quantifié) s’appuyant sur l’évaluation de leur relation au contexte. Nous proposons une interprétation de cette théorie appliquée à l’analyse syntaxique. Un mécanisme de construction des chunks est proposé. Nous développons pour cela une fonction d’activation tirant parti de la représentation de l’information linguistique sous forme de contraintes. Cette fonction permet de montrer en quoi les chunks sont faciles à construire et comment leur existence facilite le traitement de la phrase. Plusieurs exemples sont proposés, illustrant cette hypothèse de facilitation.</resume>
			<mots_cles>Chunks, ACT-R, activation, mémoire, parsing, traitement de la phrase, expérimentation</mots_cles>
			<title>Chunks and the notion of activation : a facilitation model for sentence processing</title>
			<abstract>We propose in this paper to integrate the notion of chunk within a global architecture for sentence processing. Chunks play an important role in cognitive theories such as ACT-R cite Anderson04 : they constitute global processing units which can be accessed directly via short or long term memory buffers. Chunks are built on the basis of an activation function evaluating their relationship to the context. We propose an interpretation of this theory applied to parsing. A construction mechanism is proposed, based on an adapted version of the activation function which takes advantage of the representation of linguistic information in terms of constraints. This feature allows to show how chunks are easy to build and how they can facilitate treatment. Several examples are given, illustrating this hypothesis of facilitation.</abstract>
			<keywords>Chunks, ACT-R, activation, memory, parsing, sentence processing, experimentation</keywords>
		</article>
		<article id="taln-2013-long-018" session="Traduction et Alignement">
			<auteurs>
				<auteur>
					<nom>Amir Hazem</nom>
					<email>amir.hazem@univ-nantes.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Emmanuel Morin</nom>
					<email>emmanuel.morin@univ-nantes.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LINA - UMR CNRS 6241, 2 rue de la houssinière, BP 92208, 44322 Nantes Cedex 03</affiliation>
			</affiliations>
			<titre>Extraction de lexiques bilingues à partir de corpus comparables par combinaison de représentations contextuelles</titre>
			<type>long</type>
			<pages>243-256</pages>
			<resume>La caractérisation du contexte des mots constitue le coeur de la plupart des méthodes d’extraction de lexiques bilingues à partir de corpus comparables. Dans cet article, nous revisitons dans un premier temps les deux principales stratégies de représentation contextuelle, à savoir celle par fenêtre ou sac de mots et celle par relations de dépendances syntaxiques. Dans un second temps, nous proposons deux nouvelles approches qui exploitent ces deux représentations de manière conjointe. Nos expériences montrent une amélioration significative des résultats sur deux corpus de langue de spécialité.</resume>
			<mots_cles>Multilingualisme, corpus comparables, lexique bilingue, vecteurs de contexte, dépendances syntaxiques</mots_cles>
			<title>Bilingual Lexicon Extraction from Comparable Corpora by Combining Contextual Representations</title>
			<abstract>Words context characterisation constitute the heart of most methods of bilingual lexicon extraction from comparable corpora. In this article, we first revisit the two main strategies of context representation, that is : the window-based and the syntactic based context representation. Secondly, we propose two new methods that exploit jointly these different representations . Our experiments show a significant improvement of the results obtained on two different domain specific comparable corpora.</abstract>
			<keywords>Multilingualism, comparable corpora, bilingual lexicon, context vectors, syntactic dependencies</keywords>
		</article>
		<article id="taln-2013-long-019" session="Entités Nommées">
			<auteurs>
				<auteur>
					<nom>Vincent Claveau</nom>
					<email>vincent.claveau@irisa.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Abir Ncibi</nom>
					<email>abir.ncibi@inria.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">IRISA-CNRS Campus de Beaulieu, 35042 Rennes, France</affiliation>
				<affiliation affiliationId="2">INRIA-IRISA Campus de Beaulieu, 35042 Rennes, France</affiliation>
			</affiliations>
			<titre>Découverte de connaissances dans les séquences par CRF non-supervisés</titre>
			<type>long</type>
			<pages>257-270</pages>
			<resume>Les tâches de découverte de connaissances ont pour but de faire émerger des groupes d’entités cohérents. Ils reposent le plus souvent sur du clustering, tout l’enjeu étant de définir une notion de similarité pertinentes entre ces entités. Dans cet article, nous proposons de détourner les champs aléatoires conditionnels (CRF), qui ont montré leur intérêt pour des tâches d’étiquetage supervisées, pour calculer indirectement ces similarités sur des séquences de textes. Pour cela, nous générons des problèmes d’étiquetage factices sur les données à traiter pour faire apparaître des régularités dans les étiquetages des entités. Nous décrivons comment ce cadre peut être mis en oeuvre et l’expérimentons sur deux tâches d’extraction d’informations. Les résultats obtenus démontrent l’intérêt de cette approche non-supervisée, qui ouvre de nombreuses pistes pour le calcul de similarités dans des espaces de représentations complexes de séquences.</resume>
			<mots_cles>Découverte de connaissances, CRF, clustering, apprentissage non-supervisé, extraction d’informations</mots_cles>
			<title>Unsupervised CRF for knowledge discovery</title>
			<abstract>Knowledge discovery aims at bringing out coherent groups of entities. They are usually based on clustering ; the challenge is then to define a notion of similarity between the relevant entities. In this paper, we propose to divert Conditional Random Fields (CRF), which have shown their interest in supervised labeling tasks, in order tocalculate indirectly the similarities among text sequences. Our approach consists in generate artificial labeling problems on the data to be processed to reveal regularities in the labeling of the entities. We describe how this framework can be implemented and experiment it on two information retrieval tasks. The results demonstrate the usefulness of this unsupervised approach, which opens many avenues for defining similarities for complex representations of sequential data.</abstract>
			<keywords>Knowledge discovery, CRF, clustering, unsupervised machine learning, information extraction</keywords>
		</article>
		<article id="taln-2013-long-020" session="Lexique et Corpus">
			<auteurs>
				<auteur>
					<nom>Thomas Gaillat</nom>
					<email>thomas.gaillat@univ-rennes1.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université ParisDiderot – CLILLACARP (3967) &amp; Université de Rennes 1</affiliation>
			</affiliations>
			<titre>Annotation automatique d'un corpus d'apprenants d'anglais avec un jeu d'étiquettes modifié du Penn Treebank</titre>
			<type>long</type>
			<pages>271-284</pages>
			<resume>Cet article aborde la problématique de l'annotation automatique d'un corpus d'apprenants d'anglais. L'objectif est de montrer qu'il est possible d'utiliser un étiqueteur PoS pour annoter un corpus d'apprenants afin d'analyser les erreurs faites par les apprenants. Cependant, pour permettre une analyse suffisamment fine, des étiquettes fonctionnelles spécifiques aux phénomènes linguistiques à étudier sont insérées parmi celles de l'étiqueteur. Celuici est entraîné avec ce jeu d'étiquettes étendu sur un corpus de natifs avant d'être appliqué sur le corpus d'apprenants. Dans cette expérience, on s'intéresse aux usages erronés de this et that par les apprenants. On montre comment l'ajout d'une couche fonctionnelle sous forme de nouvelles étiquettes pour ces deux formes, permet de discriminer des usages variables chez les natifs et nonnatifs et, partant, d’identifier des schémas incorrects d'utilisation. Les étiquettes fonctionnelles éclairent sur le fonctionnement discursif.</resume>
			<mots_cles>Apprentissage L2, corpus d'apprenants, analyse linguistique d'erreurs, étiquetage automatique, this, that</mots_cles>
			<title>Automatic tagging of a learner corpus of English with a modified version of the Penn Treebank tagset</title>
			<abstract>This article covers the issue of automatic annotation of a learner corpus of English. The objective is to show that it is possible to PoStag the corpus with a tagger to prepare the ground for learner error analysis. However, in order to have a finegrain analysis, some functional tags for the study of specific linguistic points are inserted within the tagger's tagset. This tagger is trained on a native-English corpus with an extended tagset and the tagging is done on the learner corpus. This experiment focuses on the incorrect use of this and that by learners. We show how the insertion of a functional layer by way of new tags for the forms allows us to discriminate varying uses among natives and nonnatives. This opens the path to the identification of incorrect patterns of use. The functional tags cast a light on the way the discourse functions.</abstract>
			<keywords>Second Language Acquisition, learner corpus, linguistic error analysis, automated tagging, this, that</keywords>
		</article>
		<article id="taln-2013-long-021" session="Lexique et Corpus">
			<auteurs>
				<auteur>
					<nom>Franck Sajous</nom>
					<email>franck.sajous@univ-tlse2.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Nabil Hathout</nom>
					<email>nabil.hathout@univ-tlse2.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Basilio Calderone</nom>
					<email>basilio.calderone@univ-tlse2.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">CLLE-ERSS – CNRS et Université de Toulouse 2 Le Mirail</affiliation>
			</affiliations>
			<titre>GLÀFF, un Gros Lexique À tout Faire du Français</titre>
			<type>long</type>
			<pages>285-298</pages>
			<resume>Cet article présente GLÀFF, un lexique du français à large couverture extrait du Wiktionnaire, le dictionnaire collaboratif en ligne. GLÀFF contient pour chaque entrée une description morphosyntaxique et une transcription phonémique. Il se distingue des autres lexiques existants principalement par sa taille, sa licence libre et la possibilité de le faire évoluer de façon constante. Nous décrivons ici comment nous l’avons construit, puis caractérisé en le comparant à différentes ressources connues. Cette comparaison montre que sa taille et sa qualité font de GLÀFF un candidat sérieux comme nouvelle ressource standard pour le TAL, la linguistique et la psycholinguistique.</resume>
			<mots_cles>Lexique morpho-phonologique, ressources lexicales libres, Wiktionnaire</mots_cles>
			<title>GLÀFF, a Large Versatile French Lexicon</title>
			<abstract>This paper introduces GLÀFF, a large-scale versatile French lexicon extracted from Wiktionary, the collaborative online dictionary. GLÀFF contains, for each entry, a morphosyntactic description and a phonetic transcription. It distinguishes itself from the other available lexicons mainly by its size, its potential for constant updating and its copylefted license that makes it available for use, modification and redistribution. We explain how we have built GLÀFF and compare it to other known resources. We show that its size and quality are strong assets that could allow GLÀFF to become a reference lexicon for NLP, linguistics and psycholinguistics.</abstract>
			<keywords>Morpho-phonological lexicon, free lexical resources, French Wiktionary</keywords>
		</article>
		<article id="taln-2013-long-022" session="Lexique et Corpus">
			<auteurs>
				<auteur>
					<nom>Authoul Abdul Hay</nom>
					<email>authoul@voila.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Olivier Kraif</nom>
					<email>olivier.Kraif@u-grenoble3.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Alzaytoonah University of Jordan - 11733 Jordan</affiliation>
				<affiliation affiliationId="2">Univ. Grenoble Alpes, LIDILEM, F-38040 Grenoble</affiliation>
			</affiliations>
			<titre>Constitution d’une ressource sémantique arabe à partir de corpus multilingue aligné</titre>
			<type>long</type>
			<pages>299-312</pages>
			<resume>Cet article porte sur la mise en oeuvre etsur l'étudede techniques d'extraction de relations sémantiques à partir d'un corpus multilingue aligné, en vue de construire une ressource lexicale pour l’arabe. Ces relations sontextraites par transitivité de l'équivalence traductionnelle, deux lexèmes qui possèdent les mêmes équivalents dans une langue cible étant susceptibles de partager un même sens. A partir d’équivalences extraites d’un corpus multilingue aligné, nous tâchons d'extraire des "cliques", ou sous-graphes maximaux complets connexes, dont toutes les unités sont en interrelation, du fait d'une probable intersection sémantique. Ces cliques présentent l'intérêt de renseigner à la fois sur la synonymie et la polysémie des unités, et d'apporter une forme de désambiguïsation sémantique. Ensuite nous tâchons de relier ces cliques avec un lexique sémantique (de type Wordnet) afin d'évaluer la possibilité de récupérer pour les unités arabes des relations sémantiques définies pour des unités en d’autres langues (français, anglais ou espagnol). Les résultats sont encourageants, et montrent qu’avec des corpus adaptés ces relations pourraient permettrede construire automatiquement un réseau utile pour certaines applications de traitement de la langue arabe.</resume>
			<mots_cles>Corpus multilingues alignés, désambigüisation sémantique, cliques, lexiques multilingues, réseaux sémantiques, traitement de l’arabe</mots_cles>
			<title>The constitution of an Arabic semantic resource from a multilingual aligned corpus</title>
			<abstract>This paper aims at the implementation and evaluation of techniques for extracting semantic relations from a multilingual alignedcorpus, in order to build a lexical resource for Arabic language. We first extract translational equivalents froma multilingual aligned corpus. From these equivalences, we try to extract "cliques", which are maximum complete related sub-graphs, where all units are interrelated because of a probable semantic intersection. These cliques have the advantage of giving information on both the synonymy and polysemy of units, providing a kindof semantic disambiguation. Secondly, we attempt to link these cliques with a semantic lexicon (like WordNet) in order to assess the possibility of recovering, for the Arabicunits, a semantic relationships already defined for English, French or Spanish units. These relations would automatically build a semantic resource which would be useful for different applications of NLP, such as Question Answering systems, Machine Translation, alignment systems, Information Retrieval…etc.</abstract>
			<keywords>Multilingual aligned corpus, semantic disambiguation, cliques, multilingual lexicons, word net, Arabic Language Processing</keywords>
		</article>
		<article id="taln-2013-long-023" session="Traduction et Alignement">
			<auteurs>
				<auteur>
					<nom>Rima Harastani</nom>
					<email>rima.harastani@univ-nantes.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Béatrice Daille</nom>
					<email>beatrice.daille@univ-nantes.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Emmanuel Morin</nom>
					<email>emmanuel.morin@univ-nantes.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LINA UMR CNRS 6241, 2 Chemin de la Houssinière 44300 Nantes</affiliation>
			</affiliations>
			<titre>Identification, alignement, et traductions des adjectifs relationnels en corpus comparables</titre>
			<type>long</type>
			<pages>313-326</pages>
			<resume>Dans cet article, nous extrayons des adjectifs relationnels français et nous les alignons automatiquement avec les noms dont ils sont dérivés en utilisant un corpus monolingue. Les alignements adjectif-nom seront ensuite utilisés dans la traduction compositionelle des termes complexes de la forme [N AdjR] à partir d’un corpus comparable français-anglais. Un nouveau terme [N N0] (ex. cancer du poumon) sera obtenu en remplaçant l’adjectif relationnel Ad jR (ex. pulmonaire) dans [N AdjR] (ex. cancer pulmonaire) par le nom N0 (ex. poumon) avec lequel il est aligné. Si aucune traduction n’est proposée pour [N AdjR], nous considérons que ses traduction(s) sont équivalentes à celle(s) de sa paraphrase [N N0]. Nous expérimentons avec un corpus comparable dans le domaine de cancer du sein, et nous obtenons des alignements adjectif-nom qui aident à traduire des termes complexes de la forme [N AdjR] vers l’anglais avec une précision de 86 %.</resume>
			<mots_cles>Adjectifs relationnels, Corpus comparables, Méthode compositionnelle, Termes complexes</mots_cles>
			<title>Identification, Alignment, and Tranlsation of Relational Adjectives from Comparable Corpora</title>
			<abstract>In this paper, we extract French relational adjectives and automatically align them with the nouns they are derived from by using a monolingual corpus. The obtained adjective-noun alignments are then used in the compositional translation of compound nouns of the form [N ADJR] with a French-English comparable corpora. A new term [N N0] (eg, cancer du poumon) is obtained by replacing the relational adjective Ad jR (eg, pulmonaire) in [N AdjR] (eg, cancer pulmonaire) by its corresponding N0 (eg, poumon). If no translation(s) are obtained for [N AdjR], we consider the one(s) obtained for its paraphrase [N N0]. We experiment with a comparable corpora in the field of breast cancer, and we get adjective-noun alignments that help in translating French compound nouns of the form [N AdjR] to English with a precision of 86%.</abstract>
			<keywords>Relational adjectives, Comparable corpora, Compositional method, Complex terms</keywords>
		</article>
		<article id="taln-2013-long-024" session="Traduction et Alignement">
			<auteurs>
				<auteur>
					<nom>Dhouha Bouamor</nom>
					<email>dhouha.bouamor@cea.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
					<affiliationId>3</affiliationId>
				</auteur>
				<auteur>
					<nom>Nasredine Semmar</nom>
					<email>nasredine.semmar@cea.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Pierre Zweigenbaum</nom>
					<email>pz@limsi.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">CEA-LIST, LVIC, F91191 Gif sur Yvette Cedex, France</affiliation>
				<affiliation affiliationId="2">LIMSI-CNRS, F-91403 Orsay, France</affiliation>
				<affiliation affiliationId="3">Univ. Paris Sud, Orsay, France</affiliation>
			</affiliations>
			<titre>Utilisation de la similarité sémantique pour l’extraction de lexiques bilingues à partir de corpus comparables</titre>
			<type>long</type>
			<pages>327-338</pages>
			<resume>Cet article présente une nouvelle méthode visant à améliorer les résultats de l’approche standard utilisée pour l’extraction de lexiques bilingues à partir de corpus comparables spécialisés. Nous tentons de résoudre le problème de la polysémie des mots dans les vecteurs de contexte par l’introduction d’un processus de désambiguïsation sémantique basé sur WordNet. Pour traduire les vecteurs de contexte, au lieu de considérer toutes les traductions proposées par le dictionnaire bilingue, nous n’utilisons que les mots caractérisant au mieux les contextes en langue cible. Les expériences menées sur deux corpus comparables spécialisés français-anglais (financier et médical) montrent que notre méthode améliore les résultats de l’approche standard plus particulièrement lorsque plusieurs mots du contexte sont ambigus.</resume>
			<mots_cles>lexique bilingue, corpus comparable spécialisé, désambiguïsation sémantique, WordNet</mots_cles>
			<title></title>
			<abstract>This paper presents a new method that aims to improve the results of the standard approach used for bilingual lexicon extraction from specialized comparable corpora. We attempt to solve the problem of context vector word polysemy. Instead of using all the entries of the dictionary to translate a context vector, we only use the words of the lexicon that are more likely to give the best characterization of context vectors in the target language. On two specialised French-English comparable corpora, empirical experimental results show that our method improves the results obtained by the standard approach especially when many words are ambiguous.</abstract>
			<keywords>bilingual lexicon, specialized comparable corpora, semantic disambiguation, Word-Net</keywords>
		</article>
		<article id="taln-2013-long-025" session="Sémantique">
			<auteurs>
				<auteur>
					<nom>Manel Zarrouk</nom>
					<email>manel.zarrouk@lirmm.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Mathieu Lafourcade</nom>
					<email>mathieu.lafourcade@lirmm.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Alain Joubert</nom>
					<email>Alain.Joubert@lirmm.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIRMM, 161, rue ADA 34095 Montpellier Cedex 5</affiliation>
			</affiliations>
			<titre>Inférences déductives et réconciliation dans un réseau lexico-sémantique</titre>
			<type>long</type>
			<pages>339-352</pages>
			<resume>La construction et la validation des réseaux lexico-sémantiques est un enjeu majeur en TAL. Indépendamment des stratégies de construction utilisées, inférer automatiquement de nouvelles relations à partir de celles déjà existantes est une approche possible pour améliorer la couverture et la qualité globale de la ressource. Dans ce contexte, le moteur d’inférences a pour but de formuler de nouvelles conclusions (c’est-à-dire des relations entre les termes) à partir de prémisses (des relations préexistantes). L’approche que nous proposons est basée sur une méthode de triangulation impliquant la transitivité sémantique avec un mécanisme de blocage pour éviter de proposer des relations douteuses. Les relations inférées sont proposées aux contributeurs pour être validées. Dans le cas d’invalidation, une stratégie de réconciliation est engagée pour identifier la cause de l’inférence erronée : une exception, une erreur dans les prémisses, ou une confusion d’usage causée par la polysémie.</resume>
			<mots_cles>inférence de relations, réconciliation, enrichissement, réseau lexical, peuplonomie</mots_cles>
			<title>Inductive and deductive inferences in a Crowdsourced Lexical-Semantic Network</title>
			<abstract>In Computational Linguistics, validated lexical-semantic networks are crucial resources. Regardless the construction strategies used, automatically inferring new relations from already existing ones may improve coverage and global quality of the resource. In this context, an inference engine aims at producing new conclusions (i.e. potential relations) from premises (pre-existing relations). The approach we propose is based on a triangulation method involving the semantic transitivity with a blocking mechanism to avoid proposing dubious relations. Inferred relations are then proposed to contributors to be validated or rejected. In cas of invalidation, a reconciliation strategy is implemented to identify the cause of the erroneous inference : an exception, an error in the premises, or a confusion caused by polysemy.</abstract>
			<keywords>relation inferences, reconcialiation, enrichment, lexical network, crowdsourcing</keywords>
		</article>
		<article id="taln-2013-long-026" session="Fouille de textes et applications">
			<auteurs>
				<auteur>
					<nom>Wei Wang</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Romaric Besançon</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Olivier Ferret</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Brigitte Grau</nom>
					<email></email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">CEA, LIST, Laboratoire Vision et Ingénierie des Contenus, Gif-sur-Yvette, F-91191 France.</affiliation>
				<affiliation affiliationId="2">LIMSI, UPR-3251 CNRS-DR4, Bât. 508, BP 133, 91403 Orsay Cedex.</affiliation>
			</affiliations>
			<titre>Regroupement sémantique de relations pour l’extraction d’information non supervisée</titre>
			<type>long</type>
			<pages>353-366</pages>
			<resume>Beaucoup des recherches menées en extraction d’information non supervisée se concentrent sur l’extraction des relations et peu de travaux proposent des méthodes pour organiser les relations extraites. Nous présentons dans cet article une méthode de clustering en deux étapes pour regrouper des relations sémantiquement équivalentes : la première étape regroupe des relations proches par leur expression tandis que la seconde fusionne les premiers clusters obtenus sur la base d’une mesure de similarité sémantique. Nos expériences montrent en particulier que les mesures distributionnelles permettent d’obtenir pour cette tâche de meilleurs résultats que les mesures utilisant WordNet. Nous montrons également qu’un clustering à deux niveaux permet non seulement de limiter le nombre de similarités sémantiques à calculer mais aussi d’améliorer la qualité des résultats du clustering.</resume>
			<mots_cles>Extraction d’Information Non Supervisée, Similarité Sémantique, Clustering</mots_cles>
			<title>Semantic relation clustering for unsupervised information extraction</title>
			<abstract>Most studies in unsupervised information extraction concentrate on the relation extraction and few work has been proposed on the organization of the extracted relations. We present in this paper a two-step clustering procedure to group semantically equivalent relations : a first step clusters relations with similar expressions while a second step groups these first clusters into larger semantic clusters, using different semantic similarities. Our experiments show the stability of distributional similarities over WordNet-based similarities for semantic clustering. We also demonstrate that the use of a multi-level clustering not only reduces the calculations from all relation pairs to basic clusters pairs, but it also improves the clustering results.</abstract>
			<keywords>Unsupervised Information Extraction, Semantic Similarity, Relation Clustering</keywords>
		</article>
		<article id="taln-2013-long-027" session="Sémantique">
			<auteurs>
				<auteur>
					<nom>Christian Retoré</nom>
					<email>Christian.Retore@irit.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">IRIT (CNRS, Toulouse) &amp; Université de Bordeaux (LaBRI)</affiliation>
			</affiliations>
			<titre>Sémantique des déterminants dans un cadre richement typé</titre>
			<type>long</type>
			<pages>367-380</pages>
			<resume>La variation du sens des mots en contexte nous a conduit à enrichir le système de types utilisés dans notre analyse syntaxico-sémantique du français basé sur les grammaires catégorielles et la sémantique de Montague (ou la lambda-DRT). L’avantage majeur d’une telle sémantique profonde est de représenter le sens par des formules logiques aisément exploitables, par exemple par un moteur d’inférence. Déterminants et quantificateurs jouent un rôle fondamental dans la construction de ces formules, et il nous a fallu leur trouver des termes sémantiques adaptés à ce nouveau cadre. Nous proposons une solution inspirée des opérateurs epsilon et tau de Hilbert, éléments génériques qui s’apparentent à des fonctions de choix. Cette modélisation unifie le traitement des différents types de déterminants et de quantificateurs et autorise le liage dynamique des pronoms. Surtout, cette description calculable des déterminants s’intègre parfaitement à l’analyseur à large échelle du français Grail, tant en théorie qu’en pratique.</resume>
			<mots_cles>Analyse sémantique automatique, Sémantique formelle, Compositionnalité</mots_cles>
			<title>On the semantics of determiners in a rich type-theoretical framework</title>
			<abstract>The variation of word meaning according to the context led us to enrich the type system of our syntactical and semantic analyser of French based on categorial grammars and Montague semantics (or lambda-DRT). The main advantage of a deep semantic analyse is too represent meaning by logical formulae that can be easily used e.g. for inferences. Determiners and quantifiers play a fundamental role in the construction of those formulae and we needed to provide them with semantic terms adapted to this new framework. We propose a solution inspired by the tau and epsilon operators of Hilbert, generic elements that resemble choice functions. This approach unifies the treatment of the different determiners and quantifiers and allows a dynamic binding of pronouns. Above all, this fully computational view of determiners fits in well within the wide coverage parser Grail, both from a theoretical and a practical viewpoint.</abstract>
			<keywords>Automated semantic analysis, Formal Semantics, Compositional Semantics</keywords>
		</article>
		<article id="taln-2013-long-028" session="Traduction et Alignement">
			<auteurs>
				<auteur>
					<nom>Charlotte Lecluze</nom>
					<email>Charlotte.Lecluze@unicaen.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Romain Brixtel</nom>
					<email>Romain.Brixtel@unicaen.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Loïs Rigouste</nom>
					<email>Loïs.Rigouste</email>
					<affiliationId></affiliationId>
				</auteur>
				<auteur>
					<nom>Emmanuel Giguet</nom>
					<email>Emmanuel.Giguet@unicaen.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Régis Clouard</nom>
					<email>Régis.Clouard@unicaen.fr</email>
					<email>Régis.Clouard@ensicaen.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>3</affiliationId>
				</auteur>
				<auteur>
					<nom>Gaël Lejeune</nom>
					<email>Gaël.Lejeune@unicaen.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Patrick Constant</nom>
					<email>Patrick.Constant@pertimm.com</email>
					<affiliationId>2</affiliationId>
				</auteur>	
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">GREYC - CNRS UMR 6072 - Université de Caen Basse-Normandie, Caen, France</affiliation>
				<affiliation affiliationId="2">Pertimm, Asnières-sur-Seine, France</affiliation>
				<affiliation affiliationId="3">EnsiCaen, Ecole Nationale Supérieure d’Ingénieurs de Caen, France</affiliation>
			</affiliations>
			<titre>Détection de zones parallèles à l’intérieur de multi-documents pour l’alignement multilingue</titre>
			<type>long</type>
			<pages>381-394</pages>
			<resume>Cet article aborde une question centrale de l’alignement automatique, celle du diagnostic de parallélisme des documents à aligner. Les recherches en la matière se sont jusqu’alors concentrées sur l’analyse de documents parallèles par nature : corpus de textes réglementaires, documents techniques ou phrases isolées. Les phénomènes d’inversions et de suppressions/ajouts pouvant exister entre les différentes versions d’un document sont ainsi souvent ignorées. Nous proposons donc une méthode pour diagnostiquer en contexte des zones parallèles à l’intérieur des documents. Cette méthode permet la détection d’inversions ou de suppressions entre les documents à aligner. Elle repose sur l’affranchissement de la notion de mot et de phrase, ainsi que sur la prise en compte de la Mise en Forme Matérielle du texte (MFM). Sa mise en oeuvre est basée sur des similitudes de répartition de chaînes de caractères répétées dans les différents documents. Ces répartitions sont représentées sous forme de matrices et l’identification des zones parallèles est effectuée à l’aide de méthodes de traitement d’image.</resume>
			<mots_cles>détection et alignement de zones, appariement de N-grammes de caractères, corpus de multidocuments</mots_cles>
			<title>Parallel areas detection in multi-documents for multilingual alignment</title>
			<abstract>This article broaches a central issue of the automatic alignment : diagnosing the parallelism of documents. Previous research was concentrated on the analysis of documents which are parallel by nature such as corpus of regulations, technical documents or simple sentences. Inversions and deletions/additions phenomena that may exist between different versions of a document has often been overlooked. To the contrary, we propose a method to diagnose in context the parallel areas allowing the detection of deletions or inversions between documents to align. This original method is based on the freeing from word and sentence as well as the consideration of the text formatting. The implementation is based on the detection of repeated character strings and the identification of parallel segments by image processing.</abstract>
			<keywords>area detection and alignment, character N-grams matching, multidocuments corpora</keywords>
		</article>
		<article id="taln-2013-long-029" session="Traduction et Alignement">
			<auteurs>
				<auteur>
					<nom>Ahmed Hamdi</nom>
					<email>ahmed.hamdi@lif.univ-mrs.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Rahma Boujelbane</nom>
					<email>rahma.boujelbane@lif.univ-mrs.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Nizar Habash</nom>
					<email>habash@ccls.columbia.edu</email>
					<affiliationId>3</affiliationId>
				</auteur>
				<auteur>
					<nom>Alexis Nasr</nom>
					<email>alexis.nasr@lif.univ-mrs.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Laboratoire d’Informatique Fondamentale de Marseille- CNRS - UMR 7279 Université Aix-Marseille</affiliation>
				<affiliation affiliationId="2">Multimedia, InfoRmation Systems and Advanced Computing Laboratory, Sfax 3021, TUNISIE.</affiliation>
				<affiliation affiliationId="3">Center for Computational Learning Systems Columbia University New York, NY 10115, USA</affiliation>
			</affiliations>
			<titre>Un système de traduction de verbes entre arabe standard et arabe dialectal par analyse morphologique profonde</titre>
			<type>long</type>
			<pages>395-406</pages>
			<resume>Le développement d’outils de TAL pour les dialectes de l’arabe se heurte à l’absence de ressources pour ces derniers. Comme conséquence d’une situation de diglossie, il existe une variante de l’arabe, l’arabe moderne standard, pour laquelle de nombreuses ressources ont été développées et ont permis de construire des outils de traitement automatique de la langue. Etant donné la proximité des dialectes de l’arabe, le tunisien dans notre cas, avec l’arabe moderne standard, une voie consiste à réaliser une traduction surfacique du dialecte vers l’arabe moderne standard afin de pouvoir utiliser les outils existants pour l’arabe standard. Nous décrivons dans cet article une architecture pour une telle traduction et nous l’évaluons sur les verbes.</resume>
			<mots_cles>dialectes, langues peu dotées, analyse morphologique, traitement automatique de l’arabe</mots_cles>
			<title>Translating verbs between MSA and arabic dialects through deep morphological analysis</title>
			<abstract>The developpment of NLP tools for dialects faces the severe problem of lack of resources for such dialects. In the case of diglossia, as in arabic, a variant of arabic, Modern Standard Arabic, exists, for which many resources have been developped which can be used to build NLP tools. Taking advantage of the closeness of MSA and dialects, one way to solve the problem consist in performing a surfacic translation of the dialect into MSA in order to use the tools developped for MSA. We describe in this paper an achitecture for such a translation and we evaluate it on arabic verbs.</abstract>
			<keywords>dialects, Arabic NLP, morphological analysis</keywords>
		</article>
		<article id="taln-2013-long-030" session="Morphologie et Segmentation">
			<auteurs>
				<auteur>
					<nom>Benoît Sagot</nom>
					<email>Benoît.Sagot@inria.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Damien Nouvel</nom>
					<email>Damien.Nouvel@inria.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Virginie Mouilleron</nom>
					<email>Virginie.Mouilleron@inria.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Marion Baranes</nom>
					<email>Marion.Baranes@inria.fr</email>
					<affiliationId>2</affiliationId>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Alpage, INRIA &amp; Université Paris-Diderot, 75013 Paris</affiliation>
				<affiliation affiliationId="2">viavoo, 92100 Boulogne Billancourt</affiliation>
			</affiliations>
			<titre>Extension dynamique de lexiques morphologiques pour le français à partir d’un flux textuel</titre>
			<type>long</type>
			<pages>407-420</pages>
			<resume>L’incomplétude lexicale est un problème récurrent lorsque l’on cherche à traiter le langage naturel dans sa variabilité. Effectivement, il semble aujourd’hui nécessaire de vérifier et compléter régulièrement les lexiques utilisés par les applications qui analysent d’importants volumes de textes. Ceci est plus particulièrement vrai pour les flux textuels en temps réel. Dans ce contexte, notre article présente des solutions dédiées au traitement des mots inconnus d’un lexique. Nous faisons une étude des néologismes (linguistique et sur corpus) et détaillons la mise en oeuvre de modules d’analyse dédiés à leur détection et à l’inférence d’informations (forme de citation, catégorie et classe flexionnelle) à leur sujet. Nous y montrons que nous sommes en mesure, grâce notamment à des modules d’analyse des dérivés et des composés, de proposer en temps réel des entrées pour ajout aux lexiques avec une bonne précision.</resume>
			<mots_cles>Néologismes, analyse morphologique, lexiques dynamiques</mots_cles>
			<title>Dynamic extension of a French morphological lexicon based a text stream</title>
			<abstract>Lexical incompleteness is a recurring problem when dealing with natural language and its variability. It seems indeed necessary today to regularly validate and extend lexica used by tools processing large amounts of textual data. This is even more true when processing real-time text flows. In this context, our paper introduces techniques aimed at addressing words unknown to a lexicon. We first study neology (from a theoretic and corpus-based point of view) and describe the modules we have developed for detecting them and inferring information about them (lemma, category, inflectional class). We show that we are able, using among others modules for analyzing derived and compound neologisms, to generate lexical entries candidates in real-time and with a good precision.</abstract>
			<keywords>Neologisms, Morphological Analysis, Dynamic Lexica</keywords>
		</article>
		<article id="taln-2013-long-031" session="Entités Nommées">
			<auteurs>
				<auteur>
					<nom>Damien Nouvel</nom>
					<email>Damien.Nouvel@univ-tours.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Jean-Yves Antoine</nom>
					<email>Jean-Yves.Antoine@univ-tours.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Nathalie Friburger</nom>
					<email>Nathalie.Friburger@univ-tours.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Arnaud Soulet</nom>
					<email>Arnaud.Soulet@univ-tours.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>					
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LI, 3 place Jean Jaurès, 41000 Blois</affiliation>
				<affiliation affiliationId="2">Alpage, INRIA &amp; Université Paris-Diderot, 75013 Paris</affiliation>
			</affiliations>
			<titre>Fouille de règles d’annotation partielles pour la reconnaissance des entités nommées</titre>
			<type>long</type>
			<pages>421-434</pages>
			<resume>Ces dernières décennies, l’accroissement des volumes de données a rendu disponible une diversité toujours plus importante de types de contenus échangés (texte, image, audio, vidéo, SMS, tweet, données statistiques, spatiales, etc.). En conséquence, de nouvelles problématiques ont vu le jour, dont la recherche d’information au sein de données potentiellement bruitées. Dans cet article, nous nous penchons sur la reconnaissance d’entités nommées au sein de transcriptions (manuelles ou automatiques) d’émissions radiodiffusées et télévisuelles. À cet effet, nous mettons en oeuvre une approche originale par fouille de données afin d’extraire des motifs, que nous nommons règles d’annotation. Au sein d’un modèle, ces règles réalisent l’annotation automatique de transcriptions. Dans le cadre de la campagne d’évaluation Etape, nous mettons à l’épreuve le système implémenté, mXS, étudions les règles extraites et rapportons les performances du système. Il obtient de bonnes performances, en particulier lorsque les transcriptions sont bruitées.</resume>
			<mots_cles>Entités nommées, Fouille de données, Règles d’annotation</mots_cles>
			<title>Mining Partial Annotation Rules for Named Entity Recognition</title>
			<abstract>During the last decades, the unremitting increase of numeric data available has led to a more and more urgent need for efficient solution of information retrieval (IR). This paper concerns a problematic of first importance for the IR on linguistic data : the recognition of named entities (NE) on speech transcripts issued from radio or TV broadcasts.We present an original approach for named entity recognition which is based on data mining techniques. More precisely, we propose to adapt hierarchical sequence mining techniques to extract automatically from annotated corpora intelligible rules of NE detection. This research was carried out in the framework of the Etape NER evaluation campaign, where mXS, our text-mining based system has shown good performances challenging the best symbolic or data-driven systems</abstract>
			<keywords>Named Entities, Data Mining, Annotation Rules</keywords>
		</article>
		<article id="taln-2013-long-032" session="Morphologie et Segmentation">
			<auteurs>
				<auteur>
					<nom>Iskandar Keskes</nom>
					<email>keskes@irit.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Farah Beanamara</nom>
					<email>benamara@irit.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Lamia Hadrich Belguith</nom>
					<email>l.belguith@fsegs.rnu.tn</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">ANLP Research Group, MIRACL, Route de Tunis km 10, 3021, Sfax, Tunisie</affiliation>
				<affiliation affiliationId="2">IRIT, 118, route de Narbonne F-31062 Toulouse Cedex 9</affiliation>
			</affiliations>
			<titre>Segmentation de textes arabes en unités discursives minimales</titre>
			<type>long</type>
			<pages>435-449</pages>
			<resume>La segmentation d’un texte en Unités Discursives Minimales (UDM) a pour but de découper le texte en segments qui ne se chevauchent pas. Ces segments sont ensuite reliés entre eux afin de construire la structure discursive d’un texte. La plupart des approches existantes utilisent une analyse syntaxique extensive. Malheureusement, certaines langues ne disposent pas d'analyseur syntaxique robuste. Dans cet article, nous étudions la faisabilité de la segmentation discursive de textes arabes en nous basant sur une approche d'apprentissage supervisée qui prédit les UDM et les UDM imbriqués. La performance de notre segmentation a été évaluée sur deux genres de corpus : des textes de livres de l’enseignement secondaire et des textes du corpus Arabic Treebank. Nous montrons que la combinaison de traits typographiques, morphologiques et lexicaux permet une bonne reconnaissance des bornes de segments. De plus, nous montrons que l'ajout de traits syntaxiques n’améliore pas les performances de notre segmentation.</resume>
			<mots_cles>Segmentation discursive, unité discursive minimale, langue arabe</mots_cles>
			<title>Segmenting Arabic Texts into Elementary Discourse Units</title>
			<abstract>Discourse segmentation aims at splitting texts into Elementary Discourse Units (EDUs) which are non-overlapping units that serve to build a discourse structure of a document. Current state of the art approaches in discourse segmentation make an extensive use of syntactic information. Unfortunately, some languages do not have any robust parser. In this paper, we investigate the feasibility of Arabic discourse segmentation using a supervised learning approach that predicts nested EDUs. The performance of our segmenter was assessed on two genres of corpora: elementary school textbooks that we build ourselves and documents extracted from the Arabic Treebank. We show that a combination of typographical, morphological and lexical features is sufficient to achieve good results in segment boundaries detection. In addition, we show that adding low-level syntactic features that are manually encoded in ATB does not enhance the performance of our segmenter.</abstract>
			<keywords>Discourse segmentation, Elementary discourse units, Arabic language</keywords>
		</article>
		<article id="taln-2013-long-033" session="Traduction et Alignement">
			<auteurs>
				<auteur>
					<nom>Thomas Lavergne</nom>
					<email>lavergne@limsi.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Alexandre Allauzen</nom>
					<email>allauzen@limsi.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>François Yvon</nom>
					<email>yvon@limsi.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université Paris Sud 91 405 Orsay</affiliation>
				<affiliation affiliationId="2">LIMSI/CNRS rue John von Neuman 91 405 Orsay</affiliation>
			</affiliations>
			<titre>Un cadre d’apprentissage intégralement discriminant pour la traduction statistique</titre>
			<type>long</type>
			<pages>450-463</pages>
			<resume>Une faiblesse des systèmes de traduction statistiques est le caractère ad hoc du processus d’apprentissage, qui repose sur un empilement d’heuristiques et conduit à apprendre des paramètres dont la valeur est sous-optimale. Dans ce travail, nous reformulons la traduction automatique sous la forme familière de l’apprentissage d’un modèle probabiliste structuré utilisant une paramétrisation log-linéaire. Cette entreprise est rendue possible par le développement d’une implantation efficace qui permet en particulier de prendre en compte la présence de variables latentes dans le modèle. Notre approche est comparée, avec succès, avec une approche de l’état de l’art sur la tâche de traduction de données du BTEC pour le couple Français-Anglais.</resume>
			<mots_cles>Traduction Automatique, Apprentissage Discriminant</mots_cles>
			<title>A fully discriminative training framework for Statistical Machine Translation</title>
			<abstract>A major pitfall of existing statistical machine translation systems is their lack of a proper training procedure. In fact, the phrase extraction and scoring processes that underlie the construction of the translation model typically rely on a chain of crude heuristics, a situation deemed problematic by many. In this paper, we recast machine translation in the familiar terms of a probabilistic structure learning problem, using a standard log-linear parameterization. The tractability of this enterprise is achieved through an efficient implementation that can take into account all the aspects of the underlying translation process through latent variables. We also address the reference reachability issue by using oracle decoding techniques. This approach is experimentally contrasted with a state-of-the-art system on the French-English BTEC translation task.</abstract>
			<keywords>Machine Translation, Discriminative Learning</keywords>
		</article>
		<article id="taln-2013-long-034" session="Sémantique">
			<auteurs>
				<auteur>
					<nom>Yue Ma</nom>
					<email>mayue@tcs.inf.tu-dresden.de</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>François Lévy</nom>
					<email>francois.levy@lipn.univ-paris13.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Adeline Nazarenko</nom>
					<email>adeline.nazarenko@lipn.univ-paris13.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">TU-Dresden, Germany</affiliation>
				<affiliation affiliationId="2">LIPN, Université Paris 13-CNRS, France</affiliation>
			</affiliations>
			<titre>Annotation sémantique pour des domaines spécialisés et des ontologies riches</titre>
			<type>long</type>
			<pages>464-478</pages>
			<resume>Explorer et maintenir une documentation technique est une tâche difficile pour laquelle on pourrait bénéficier d’un outillage efficace, à condition que les documents soient annotés sémantiquement. Les annotations doivent être riches, cohérentes, suffisamment spécialisées et s’appuyer sur un modèle sémantique explicite – habituellement une ontologie – qui modélise la sémantique du domaine cible. Il s’avère que les approches d’annotation traditionnelles donnent pour cette tâche des résultats limités. Nous proposons donc une nouvelle approche, l’annotation sémantique statistique basée sur les syntagmes, qui prédit les annotations sémantiques à partir d’un ensemble d’apprentissage réduit. Cette modélisation facilite l’annotation sémantique spécialisée au regard de modèles sémantiques de domaine arbitrairement riches. Nous l’évaluons à l’aide de plusieurs métriques et sur deux textes décrivant des réglementations métier. Notre approche obtient de bons résultats. En particulier, la F-mesure est de l’ordre de 91, 9% et 97, 6% pour la prédiction de l’étiquette et de la position avec différents paramètres. Cela suggère que les annotateurs humains peuvent être fortement aidés pour l’annotation sémantique dans des domaines spécifiques.</resume>
			<mots_cles>Annotation sémantique, Ontologie de domaine, Annotation automatique, Analyse sémantique des textes, Méthodes statistiques</mots_cles>
			<title>Semantic Annotation in Specific Domains with rich Ontologies</title>
			<abstract>Technical documentations are generally difficult to explore and maintain. Powerful tools can help, but they require that the documents have been semantically annotated. The annotations must be sufficiently specialized, rich and consistent. They must rely on some explicit semantic model – usually an ontology – that represents the semantics of the target domain. We observed that traditional approaches have limited success on this task and we propose a novel approach, phrase-based statistical semantic annotation, for predicting semantic annotations from a limited training data set. Such a modeling makes the challenging problem, domain specific semantic annotation regarding arbitrarily rich semantic models, easily handled. Our approach achieved a good performance, with several evaluation metrics and on two different business regulatory texts. In particular, it obtained 91.9% and 97.65% F-measure in the label and position predictions with different settings. This suggests that human annotators can be highly supported in domain specific semantic annotation tasks.</abstract>
			<keywords>Semantic Annotation, Domain Ontology, Automatic annotation, Semantic Text Analysis, Statistical methods</keywords>
		</article>
		<article id="taln-2013-long-035" session="Fouille de textes et applications">
			<auteurs>
				<auteur>
					<nom>Nicolas Foucault</nom>
					<email>Nicolas.Foucault@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Sophie Rosset</nom>
					<email>Sophie.Rosset@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Gilles Adda</nom>
					<email>Gilles.Adda@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIMSI-CNRS - 508 rue John von Neumann - Plateau du Moulon Université de Paris-Sud - B.P. 133 - 91403 Orsay Cedex - France</affiliation>
			</affiliations>
			<titre>Pré-segmentation de pages web et sélection de documents pertinents en Questions-Réponses</titre>
			<type>long</type>
			<pages>479-492</pages>
			<resume>Dans cet article, nous présentons une méthode de segmentation de pages web en blocs de texte pour la sélection de documents pertinents en questions-réponses. La segmentation des documents se fait préalablement à leur indexation en plus du découpage des segments obtenus en passages au moment de l’extraction des réponses. L’extraction du contenu textuel des pages est faite à l’aide d’un extracteur maison. Nous avons testé deux méthodes de segmentation. L’une segmente les textes extraits des pages web uniformément en blocs de taille fixe, l’autre les segmente par TextTiling (Hearst, 1997) en blocs thématiques de taille variable. Les expériences menées sur un corpus de 500K pages web et un jeu de 309 questions factuelles en français, issus du projet Quaero (Quintard et al., 2010), montrent que la méthode employée tend à améliorer la précision globale (top-10) du système RITEL–QR (Rosset et al., 2008) dans sa tâche.</resume>
			<mots_cles>pages web, TextTiling, sélection de documents, questions-réponses, Quaero, Ritel, segmentation textuelle, segmentation thématique</mots_cles>
			<title>Web pages segmentation for document selection in Question Answering</title>
			<abstract>In this paper, we study two different kinds of web pages segmentation for document selection in question answering. The segmentation is applied prior to indexation in addition to the traditionnal passage retrieval step in question answering. In both cases, the segmentation is textual and processed once the web pages textual content has been extracted using our own extraction system. In the first case, a document is tilled homogeneously in text blocs of fixed size while in the second case the segmentation is based on the TextTiling algorithm (Hearst, 1997). Evaluation on 309 factoid questions and a collection of 500K French web pages, coming from the Quaero project (Quintard et al., 2010), showed that such approaches tend to support properly the RITEL–QR system (Rosset et al., 2008) in this task.</abstract>
			<keywords>web pages, TextTiling, document selection, question answering, Quaero, Ritel, textual segmentation, topic segmentation</keywords>
		</article>
		<article id="taln-2013-long-036" session="Fouille de textes et applications">
			<auteurs>
				<auteur>
					<nom>Anne-Laure Ligozat</nom>
					<email></email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Cyril Grouin</nom>
					<email></email>
					<affiliationId>1</affiliationId>
					<affiliationId>3</affiliationId>
				</auteur>
				<auteur>
					<nom>Anne Garcia-Fernandez</nom>
					<email></email>
					<affiliationId>4</affiliationId>
				</auteur>
				<auteur>
					<nom>Delphine Bernhard</nom>
					<email></email>
					<affiliationId>5</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIMSI–CNRS, Orsay</affiliation>
				<affiliation affiliationId="2">ENSIIE, Évry</affiliation>
				<affiliation affiliationId="3">INSERM U872 Eq 20 &amp; UPMC, Paris</affiliation>
				<affiliation affiliationId="4">LAS, CNRS/EHESS/Collège de France, Paris</affiliation>
				<affiliation affiliationId="5">LiLPa, Université de Strasbourg, Strasbourg</affiliation>
			</affiliations>
			<titre>Approches à base de fréquences pour la simplification lexicale</titre>
			<type>long</type>
			<pages>493-506</pages>
			<resume>La simplification lexicale consiste à remplacer des mots ou des phrases par leur équivalent plus simple. Dans cet article, nous présentons trois modèles de simplification lexicale, fondés sur différents critères qui font qu’un mot est plus simple à lire et à comprendre qu’un autre. Nous avons testé différentes tailles de contextes autour du mot étudié : absence de contexte avec un modèle fondé sur des fréquences de termes dans un corpus d’anglais simplifié ; quelques mots de contexte au moyen de probabilités à base de n-grammes issus de données du web ; et le contexte étendu avec un modèle fondé sur les fréquences de cooccurrences.</resume>
			<mots_cles>simplification lexicale, fréquence lexicale, modèle de langue</mots_cles>
			<title>Studying frequency-based approaches to process lexical simplification</title>
			<abstract>Lexical simplification aims at replacing words or phrases by simpler equivalents. In this paper, we present three models for lexical simplification, focusing on the criteria that make one word simpler to read and understand than another. We tested different contexts of the considered word : no context, with a model based on word frequencies in a simplified English corpus ; a few words context, with n-grams probabilites on Web data, and an extended context, with a model based on co-occurrence frequencies.</abstract>
			<keywords>lexical simplification, lexical frequency, language model</keywords>
		</article>
		<article id="taln-2013-court-001" session="Poster">
			<auteurs>
				<auteur>
					<nom>Florian Boudin</nom>
					<email>florian.boudin@univ-nantes.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LINA - UMR CNRS 6241, Université de Nantes, France</affiliation>
			</affiliations>
			<titre>TALN Archives : une archive numérique francophone des articles de recherche en Traitement Automatique de la Langue</titre>
			<type>court</type>
			<pages>507-514</pages>
			<resume>La recherche scientifique est un processus incrémental. La première étape à effectuer avant de débuter des travaux consiste à réaliser un état de l’art des méthodes existantes. La communauté francophone du Traitement Automatique de la Langue (TAL) produit de nombreuses publications scientifiques qui sont malheureusement dispersées sur différents sites et pour lesquelles aucune méta-donnée n’est disponible. Cet article présente la construction de TALN Archives, une archive numérique francophone des articles de recherche en TAL dont le but est d’offrir un accès simplifié aux différents travaux effectués dans notre domaine. Nous présentons également une analyse du réseau de collaboration construit à partir des méta-données que nous avons extraites et dévoilons l’identité du Kevin Bacon de TALN Archives, i.e. l’auteur le plus central dans le réseau de collaboration.</resume>
			<mots_cles>TALN Archives, archive numérique, articles scientifiques</mots_cles>
			<title>TALN Archives : a digital archive of French research articles in Natural Language Processing</title>
			<abstract>Scientific research is an incremental process. Reviewing the literature is the first step to do before starting a new research project. The French Natural Language Processing (NLP) community produces numerous scientific publications which are scattered across different sources and for which no metadata is available. This paper presents the construction of TALN Archives, a digital archive of French research articles whose aim is to provide efficient access to articles in the NLP field. We also present an analysis of the collaboration network constructed from the metadata and disclose the identity of the Kevin Bacon of the TALN Archives, i.e. the most central author in the collaboration network.</abstract>
			<keywords>TALN Archives, digital archive, scientific articles</keywords>
		</article>
		<article id="taln-2013-court-002" session="Poster">
			<auteurs>
				<auteur>
					<nom>Pierre-Francois Marteau</nom>
					<email>pierre-francois.marteau@univ-ubs.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Gildas Ménier</nom>
					<email>gildas.menier@univ-ubs.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">IRISA, UMR 6074</affiliation>
				<affiliation affiliationId="2">Université de Bretagne Sud, 56000 Vannes</affiliation>
			</affiliations>
			<titre>Similarités induites par mesure de comparabilité : signification et utilité pour le clustering et l’alignement de textes comparables</titre>
			<type>court</type>
			<pages>515-522</pages>
			<resume>En présence de corpus comparables bilingues, nous sommes confrontés à des données qu’il est naturel de plonger dans deux espaces de représentation linguistique distincts, chacun éventuellement muni d’une mesure quantifiable de similarité (ou d’une distance). Dès lors que ces données bilingues sont comparables au sens d’une mesure de comparabilité également calculable (Li et Gaussier, 2010), nous pouvons établir une connexion entre ces deux espaces de représentation linguistique en exploitant une carte d’association pondérée ("mapping") appréhendée sous la forme d’un graphe bi-directionnel dit de comparabilité. Nous abordons dans cet article les conséquences conceptuelles et pratique d’une telle connexion similarité-comparabilité en développant un algorithme (Hit-ComSim) basé sur sur le principe de similarité induite par la topologie du graphe de comparabilité. Nous essayons de qualifier qualitativement l’intérêt de cet algorithme en considérant quelques expériences préliminaires de clustering de documents comparables bilingues (Français/Anglais) collectés sur des flux RSS.</resume>
			<mots_cles>Graphe de comparabilité, Similarités induites, Documents comparables, Clustering</mots_cles>
			<title>Similarities induced by a comparability mapping : meaning and utility in the context of the clustering of comparable texts</title>
			<abstract>In the presence of bilingual comparable corpora it is natural to embed the data in two distinct linguistic representation spaces in which a "computational" notion of similarity is potentially defined. As far as these bilingual data are comparable in the sense of a measure of comparability also computable (Li et Gaussier, 2010), we can establish a connection between these two areas of linguistic representation by exploiting a weighted mapping that can be represented in the form of a weighted bidirectional graph of comparability. We study in this paper the conceptual and practical consequences of such a similarity-comparability connection, while developing an algorithm (Hit-ComSim) based on the concept of similarities induced by the topology of the graph of comparability. We try to evaluate the benefit of this algorithm considering some preliminary categorization or clustering tasks of bilingual (English/French) documents collected from RSS feeds.</abstract>
			<keywords>Comparability graph, Induced similarities, Comparable documents, Clustering</keywords>
		</article>
		<article id="taln-2013-court-003" session="Poster">
			<auteurs>
				<auteur>
					<nom>Denis Maurel</nom>
					<email>denis.maurel@univ-tours.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Béatrice Bouchou Markhoff</nom>
					<email>beatrice.bouchou@univ-tours.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université François Rabelais Tours</affiliation>
			</affiliations>
			<titre>ProLMF version 1.2. Une ressource libre de noms propres avec des expansions contextuelles</titre>
			<type>court</type>
			<pages>523-530</pages>
			<resume>ProLMF est la version LMF de la base lexicale multilingue de noms propres Prolexbase. Disponible librement sur le site du CNRTL, la version 1.2 a été largement améliorée et augmentée par de nouvelles entrées en français, complétées par des expansions contextuelles, et par de petits lexiques en une huitaine de langues.</resume>
			<mots_cles>ressource libre, base lexicale multilingue, noms propres, expansions contextuelles, schémas de contextualisation, relations sémantiques, alias, point de vue, Prolexbase</mots_cles>
			<title>ProLMF 1.2, Proper Names with their Expansions</title>
			<abstract>ProLMF is the LMF version of Prolexbase, a multilingual lexical database of Proper Names. It can be freely downloaded on the CNRTL Website. Version 1.2 had been widely improved and increased, with new French entries whose description includes contextual expansions, and eight small lexica for other languages.</abstract>
			<keywords>free resource, multilingual lexical database, Proper Names, context, semantic relations, alias, point of view, Prolexbase</keywords>
		</article>
		<article id="taln-2013-court-004" session="Poster">
			<auteurs>
				<auteur>
					<nom>Benjamin Lecouteux</nom>
					<email>benjamin.lecouteux@imag.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Laurent Besacier</nom>
					<email>laurent.besacier@imag.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Laboratoire d’Informatique de Grenoble (LIG), Université de Grenoble</affiliation>
			</affiliations>
			<titre>Vers un décodage guidé pour la traduction automatique</titre>
			<type>court</type>
			<pages>531-538</pages>
			<resume>Récemment, le paradigme du décodage guidé a montré un fort potentiel dans le cadre de la reconnaissance automatique de la parole. Le principe est de guider le processus de décodage via l’utilisation de transcriptions auxiliaires. Ce paradigme appliqué à la traduction automatique permet d’envisager de nombreuses applications telles que la combinaison de systèmes, la traduction multi-sources etc. Cet article présente une approche préliminaire de l’application de ce paradigme à la traduction automatique (TA). Nous proposons d’enrichir le modèle log-linéaire d’un système primaire de TA avec des mesures de distance relatives à des systèmes de TA auxiliaires. Les premiers résultats obtenus sur la tâche de traduction Français/Anglais issue de la campagne d’évaluation WMT 2011 montrent le potentiel du décodage guidé.</resume>
			<mots_cles>Décodage guidé, traduction automatique, combinaison de systèmes</mots_cles>
			<title>Driven Decoding for machine translation</title>
			<abstract>Recently, the concept of driven decoding (DD), has been sucessfully applied to the automatic speech recognition (speech-to-text) task : an auxiliary transcription guide the decoding process. There is a strong interest in applying this concept to statistical machine translation (SMT). This paper presents our approach on this topic. Our first attempt in driven decoding consists in adding several feature functions corresponding to the distance between the current hypothesis decoded and the auxiliary translations available. Experimental results done for a french-to-english machine translation task, in the framework of the WMT 2011 evaluation, show the potential of the DD approach proposed.</abstract>
			<keywords>Driven Decoding, machine translation, system combination</keywords>
		</article>
		<article id="taln-2013-court-005" session="Poster">
			<auteurs>
				<auteur>
					<nom>Johanna Gerlach</nom>
					<email>Johanna.Gerlach@unige.ch</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Victoria Porro</nom>
					<email>Victoria.Porro@unige.ch</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Pierrette Bouillon</nom>
					<email>Pierrette.Bouillon@unige.ch</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Sabine Lehmann</nom>
					<email>Sabine.Lehmann@acrolinx.com</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université de Genève fti/tim - 40, bvd Du Pont-d’Arve, CH-1211 Genève 4, Suisse</affiliation>
				<affiliation affiliationId="2">Acrolinx GmbH, Friedrichstr. 100, 10117 Berlin, Allemagne</affiliation>
			</affiliations>
			<titre>La La préédition avec des règles peu coûteuses, utile pour la TA statistique des forums ?</titre>
			<type>court</type>
			<pages>539-546</pages>
			<resume>Cet article s’intéresse à la traduction automatique statistique des forums, dans le cadre du projet européen ACCEPT (« Automated Community Content Editing Portal »). Nous montrons qu’il est possible d’écrire des règles de préédition peu coûteuses sur le plan des ressources linguistiques et applicables sans trop d’effort avec un impact très significatif sur la traduction automatique (TA) statistique, sans avoir à modifier le système de TA. Nous décrivons la méthodologie proposée pour écrire les règles de préédition et les évaluer, ainsi que les résultats obtenus par type de règles.</resume>
			<mots_cles>préédition, langage contrôlé, traduction statistique, forums</mots_cles>
			<title>Can lightweight pre-editing rules improve statistical MT of forum content?</title>
			<abstract>This paper focuses on the statistical machine translation (SMT) of forums within the context of the European Framework ACCEPT («Automated Community Content Editing Portal») project. We demonstrate that it is possible to write lightweight pre-editing rules that require few linguistic resources, are relatively easy to apply and have significant impact on SMT without any changes to the machine translation system. We describe methodologies for rule development and evaluation, and provide results obtained for different rule types.</abstract>
			<keywords>pre-edition, controlled language, statistical machine translation, forums</keywords>
		</article>
		<article id="taln-2013-court-006" session="Poster">
			<auteurs>
				<auteur>
					<nom>Ludovic Hamon</nom>
					<email>ludovic.hamon@univ-ubs.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Sylvie Gibet</nom>
					<email>Sylvie.Gibet@univ-ubs.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Sabah Boustila</nom>
					<email>boustila@unistra.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">IRISA, Université Bretagne Sud, Campus de Tohannic, Rue Yves Mainguy, 56000 Vannes</affiliation>
				<affiliation affiliationId="2">ICUBE, Université de Strasbourg, 300 bd Sébastien Brant, BP 10413, 67412 IIIkirch Cedex, France</affiliation>
			</affiliations>
			<titre>Édition interactive d’énoncés en langue des signes française dédiée aux avatars signeurs</titre>
			<type>court</type>
			<pages>547-554</pages>
			<resume>Les avatars signeurs en Langue des Signes Française (LSF) sont de plus en plus utilisés en tant qu’interface de communication à destination de la communauté sourde. L’un des critères d’acceptation de ces avatars est l’aspect naturel et réaliste des gestes produits. Par conséquent, des méthodes de synthèse de gestes ont été élaborées à l’aide de corpus de mouvements capturés et annotés provenant d’un signeur réel. Néanmoins, l’enrichissement d’un tel corpus, en faisant fi des séances de captures supplémentaires, demeure une problématique certaine. De plus, l’application automatique d’opérations sur ces mouvements (e.g. concaténation, mélange, etc.) ne garantit pas la consistance sémantique du geste résultant. Une alternative est d’insérer l’opérateur humain dans la boucle de construction des énoncés en LSF. Dans cette optique, cet article propose un premier système interactif d’édition de gestes en LSF, basé "données capturées" et dédié aux avatars signeurs.</resume>
			<mots_cles>Langue des Signes Française, édition, geste, base de données sémantiques, signeur virtuel, interaction</mots_cles>
			<title>Interactive editing of utterances in French sign language dedicated to signing avatars</title>
			<abstract>Signing avatars dedicated to French Sign Language (LSF) are more and more used as a communication interface for the deaf community. One of the acceptation criteria of these avatars is the natural and realistic aspect of the constructed gestures. Consequently, gestures synthesis methods have been designed thanks to some corpus of captured and annotated motions, performed by a real signer. However, the enlarging of such a corpus, without requiring of some additional capture sessions, is a major issue. Furthermore, the automatic application of motion transformations (e.g. concatenation, blending, etc.) does not guarantee the semantic consistency of the resulting gesture. Another option is to insert the human operator in the utterance building loop. In this context, this paper provides a first interactive editing system of FSL gestures, based on captured motions and dedicated to signing avatars.</abstract>
			<keywords>French sign language, editing, gesture, semantic data base, virtual signer, interaction</keywords>
		</article>
		<article id="taln-2013-court-007" session="Poster">
			<auteurs>
				<auteur>
					<nom>Judith Muzerelle</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Anaïs Lefeuvre</nom>
					<email></email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Jean-Yves Antoine</nom>
					<email>Jean-Yves.Antoine@univ-tour.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Emmanuel Schang</nom>
					<email>Emmanuel.Schang@univ-orleans.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Denis Maurel</nom>
					<email></email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Jeanne Villaneau</nom>
					<email>Jeanne.Villaneau@univ-ubs.fr</email>
					<affiliationId>3</affiliationId>
				</auteur>
				<auteur>
					<nom>Iris Eshkol</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LLL Orléans, Université d’Orléans</affiliation>
				<affiliation affiliationId="2">Université François Rabelais Tours, LI, 3 place Jean Jaurès, 41000 Blois</affiliation>
				<affiliation affiliationId="3">IRISA, Université Européenne de Bretagne, 56100 Lorient</affiliation>
			</affiliations>
			<titre>ANCOR, premier corpus de français parlé d’envergure annoté en coréférence et distribué librement</titre>
			<type>court</type>
			<pages>555-563</pages>
			<resume>Cet article présente la réalisation d’ANCOR, qui constitue par son envergure (453 000 mots) le premier corpus francophone annoté en anaphores et coréférences permettant le développement d’approches centrées sur les données pour la résolution des anaphores et autres traitements de la coréférence. L’annotation a été réalisée sur trois corpus de parole conversationnelle (Accueil_UBS, OTG et ESLO) qui le destinent plus particulièrement au traitement du langage parlé. En l’absence d’équivalent pour le langage écrit, il est toutefois susceptible d’intéresser l’ensemble de la communauté TAL. Par ailleurs, le schéma d’annotation retenu est suffisamment riche pour permettre des études en linguistique de corpus. Le corpus sera diffusé librement à la mi-2013 sous licence Creative Commons BY-NC-SA. Cet article se concentre sur sa mise en oeuvre et décrit brièvement quelques résultats obtenus sur la partie déjà annotée de la ressource.</resume>
			<mots_cles>Corpus, annotation, coréférence, anaphore, parole conversationnelle</mots_cles>
			<title>ANCOR, the first large French speaking corpus of conversational speech annotated in coreference to be freely available</title>
			<abstract>This paper presents the first French spoken corpus annotated in coreference whose size (453,000 words) is sufficient to investigate the achievement of data oriented systems of coreference resolution. The annotation was conducted on three different corpora of conversational speech (Accueil_UBS, OTG, ESLO) but this resource can also be interesting for NLP researchers working on written language, considering the lack of a large written French corpus annotated in coreference. We followed a rich annotation scheme which enables also research motivated by linguistic considerations. This corpus will be freely available (Creative Commons BY-NC-SA) around mid-2013. The paper details the achievement of the resource as well as preliminary experiments conducted on the part of the corpus already annotated.</abstract>
			<keywords>Corpus, annotation, coreference, anaphora, conversational speech</keywords>
		</article>
		<article id="taln-2013-court-008" session="Poster">
			<auteurs>
				<auteur>
					<nom>Elizaveta Loginova-Clouet</nom>
					<email>elizaveta.loginova@univ-nantes.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Béatrice Daille</nom>
					<email>beatrice.daille@univ-nantes.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LINA, 2, rue de la Houssinière 44322 Nantes Cedex 03</affiliation>
			</affiliations>
			<titre>Segmentation Multilingue des Mots Composés</titre>
			<type>court</type>
			<pages>564-571</pages>
			<resume>La composition est un phénomène fréquent dans plusieurs langues, surtout dans des langues ayant une morphologie riche. Le traitement des mots composés est un défi pour les systèmes de TAL car pour la plupart, ils ne sont pas présents dans les lexiques. Dans cet article, nous présentons une méthode de segmentation des composés qui combine des caractéristiques indépendantes de la langue (mesure de similarité, données du corpus) avec des règles de transformation sur les frontières des composants spécifiques à une langue. Nos expériences de segmentation de termes composés allemands et russes montrent une exactitude jusqu’à 95 % pour l’allemand et jusqu’à 91 % pour le russe. Nous constatons que l’utilisation de corpus spécialisés relevant du même domaine que les composés améliore la qualité de segmentation.</resume>
			<mots_cles>segmentation des mots composés, outil multilingue, mesure de similarité, règles de transformation des composants, corpus spécialisés</mots_cles>
			<title>Multilingual Compound Splitting</title>
			<abstract>Compounding is a common phenomenon for many languages, especially those with a rich morphology. Dealing with compounds is a challenge for natural language processing systems since all compounds can not be included in lexicons. In this paper, we present a compound splitting method combining language independent features (similarity measure, corpus data) and language dependent features (component transformation rules). We report on our experiments in splitting of German and Russian compound terms giving accuracy up to 95% for German and up to 91% for Russian language. We observe that the usage of a corpus of the same domain as compounds improves splitting quality.</abstract>
			<keywords>compound splitting, multilingual tool, similarity measure, component transformation rules, specialized corpora</keywords>
		</article>
		<article id="taln-2013-court-009" session="Poster">
			<auteurs>
				<auteur>
					<nom>Ying Zhang</nom>
					<email>ying.zhang@imag.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Mathieu Mangeot</nom>
					<email>mathieu.mangeot@imag.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">GETALP-LIG, 41, rue des Mathématiques BP53 38041 Grenoble Cedex 9</affiliation>
			</affiliations>
			<titre>Gestion des terminologies riches : l'exemple des acronymes</titre>
			<type>court</type>
			<pages>572-579</pages>
			<resume>La gestion des terminologies pose encore des problèmes, en particulier pour des constructions complexes comme les acronymes. Dans cet article, nous proposons une solution en reliant plusieurs termes différents à un seul référent via les notions de pivot et de prolexème. Ces notions permettent par exemple de faire le lien entre plusieurs termes qui désignent un même et unique référent : Nations Unies, ONU, Organisation des Nations Unies et onusien. Il existe Jibiki, une plate-forme générique de gestion de bases lexicales permettant de gérer n'importe quel type de structure (macro et microstructure). Nous avons implémenté une nouvelle macrostructure de ProAxie dans la plate-forme Jibiki pour réaliser la gestion des acronymes.</resume>
			<mots_cles>base lexicale multilingue, macrostructure, Jibiki, Common Dictionary Markup, Proaxie, Prolèxeme</mots_cles>
			<title>Complex terminologies management - the case of acronyms</title>
			<abstract>Terminology management is still problematic, especially for complex constructions such as acronyms. In this paper, we propose a solution to connect several different terms with a single referent through using the concepts of pivot and prolexeme. These concepts allow for example to link several terms for the same referent: Nations Unies, ONU, Organisation des Nations Unies and onusien. Jibiki is a generic platform for lexical database management, allowing the representation of any type of structure (macro and microstructure). We have implemented a new macrostructure ProAxie in the Jibiki platform to achieve acronym management.</abstract>
			<keywords>multilingual lexical database, macrostructure, Jibiki, Common Dictionary Markup, Proaxie, Prolexeme</keywords>
		</article>
		<article id="taln-2013-court-010" session="Poster">
			<auteurs>
				<auteur>
					<nom>Marcos Zampieri</nom>
					<email>mzampier@uni-koeln.de</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Binyam Gebrekidan Gebre</nom>
					<email>bingeb@mpi.nl</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Sascha Diwersy</nom>
					<email>sascha.diwersy@uni-koeln.de</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">University of Cologne, Germany</affiliation>
				<affiliation affiliationId="2">Max Planck Institute for Psycholinguistics, Nijmegen, Holland</affiliation>
			</affiliations>
			<titre>Ngrammes et Traits Morphosyntaxiques pour la Identification de Variétés de l’Espagnol</titre>
			<type>court</type>
			<pages>580-587</pages>
			<resume>Notre article présente expérimentations portant sur la classification supervisée de variétés nationales de l’espagnol. Outre les approches classiques, basées sur l’utilisation de ngrammes de caractères ou de mots, nous avons testé des modèles calculés selon des traits morphosyntaxiques, l’objectif étant de vérifier dans quelle mesure il est possible de parvenir à une classification automatique des variétés d’une langue en s’appuyant uniquement sur des descripteurs grammaticaux. Les calculs ont été effectués sur la base d’un corpus de textes journalistiques de quatre pays hispanophones (Espagne, Argentine, Mexique et Pérou).</resume>
			<mots_cles>classification automatique, ngrammes, espagnol, variétés nationales</mots_cles>
			<title>N-gram Language Models and POS Distribution for the Identification of Spanish Varieties</title>
			<abstract>This article presents supervised computational methods for the identification of Spanish varieties. The features used for this task were the classical character and word n-gram language models as well as POS and morphological information. The use of these features is to our knowledge new and we aim to explore the extent to which it is possible to identify language varieties solely based on grammatical differences. Four journalistic corpora from different countries were used in these experiments : Spain, Argentina, Mexico and Peru.</abstract>
			<keywords>automatic classification, n-grams, Spanish, language varieties</keywords>
		</article>
		<article id="taln-2013-court-011" session="Poster">
			<auteurs>
				<auteur>
					<nom>Amel Fraisse</nom>
					<email>fraisse@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Patrick Paroubek</nom>
					<email>pap@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Gil Francopoulo</nom>
					<email>gil.francopoulo@tagmatica.com</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIMSI-CNRS, Bât. 508 Université Paris-Sud, 91403 Orsay Cedex, France</affiliation>
				<affiliation affiliationId="2">TAGMATICA, 126 rue de Picpus, 75012 Paris France</affiliation>
			</affiliations>
			<titre>L’apport des Entités Nommées pour la classification des opinions minoritaires</titre>
			<type>court</type>
			<pages>588-595</pages>
			<resume>La majeure partie des travaux en fouille d’opinion et en analyse de sentiment concerne le classement des opinions majoritaires. Les méthodes d’apprentissage supervisé à base de ngrammes sont souvent employées. Elles ont l’inconvénient d’avoir un biais en faveur des opinions majoritaires si on les utilise de manière classique. En fait la présence d’un terme particulier, fortement associé à la cible de l’opinion dans un document peut parfois suffire à faire basculer le classement de ce document dans la classe de ceux qui expriment une opinion majoritaire sur la cible. C’est un phénomène positif pour l’exactitude globale du classifieur, mais les documents exprimant des opinions minoritaires sont souvent mal classés. Ce point est un problème dans le cas où l’on s’intéresse à la détection des signaux faibles (détection de rumeur) ou pour l’anticipation de renversement de tendance. Nous proposons dans cet article d’améliorer la classification des opinions minoritaires en prenant en compte les Entités Nommées dans le calcul de pondération destiné à corriger le biais en faveur des opinions majoritaires.</resume>
			<mots_cles>Fouille d’opinions, Opinion minoritaires, Entités Nommées, Apprentissage, N-grammes, Pondération</mots_cles>
			<title>Improving Minor Opinion Polarity Classification with Named Entity Analysis</title>
			<abstract>The main part of the work on opinion mining and sentiment analysis concerns polarity classification of majority opinions. Supervised machine learning with n-gram features is a common approach to polarity classification, which is often biased towards the majority of opinions about a given opinion target, when using this kind of approach with traditional settings. The presence of a specific term, strongly associated to the opinion target in a document, is often enough to tip the classifier decision toward the majority opinion class. This is actually a good thing for overall accuracy. Howeverm documents about the opinion taget, but expressing a polarity different from the majority one, get misclassified. It is a problem if we want to detect weak signals (rumor detection) or for anticipating opinion reversal trends. We propose in this paper to improve minor reviews polarity classification by taking into account Named Entity information in the computation of specific weighting scheme used for correcting the bias toward majority opinions.</abstract>
			<keywords>Opinion Mining, Minor Opinion, Named Entities, Machine Learning, N-grams, Weighting Scheme</keywords>
		</article>
		<article id="taln-2013-court-012" session="Poster">
			<auteurs>
				<auteur>
					<nom>Gemma Bel-Enguix</nom>
					<email>gemma.belenguix@gmail.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Michael Zock</nom>
					<email>michael.zock@lif.univ-mrs.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">CNRS-LIF, UMR 7279, Aix Marseille Université, Marseille</affiliation>
			</affiliations>
			<titre>Trouver les mots dans un simple réseau de co-occurrences</titre>
			<type>court</type>
			<pages>596-603</pages>
			<resume>Au cours des deux dernières décennies des psychologues et des linguistes informaticiens ont essayé de modéliser l'accès lexical en construisant des simulations ou des ressources. Cependant, parmi ces chercheurs, pratiquement personne n'a vraiment cherché à améliorer la navigation dans des 'dictionnaires électroniques destinés aux producteurs de langue'. Pourtant, beaucoup de travaux ont été consacrés à l'étude du phénomène du mot sur le bout de la langue et à la construction de réseaux lexicaux. Par ailleurs, vu les progrès réalisés en neurosciences et dans le domaine des réseaux complexes, on pourrait être tenté de construire un simulacre du dictionnaire mental, ou, à défaut une ressource destinée aux producteurs de langue (écrivains, conférenciers). Nous sommes restreints en construisant un réseau de co-occurrences à partir des résumés de Wikipedia, le but étant de vérifier jusqu'où l'on pouvait pousser une telle ressource pour trouver un mot, sachant que la ressource ne contient pas de liens sémantiques, car le réseau est construit de manière automatique et à partir de textes non-annotés.</resume>
			<mots_cles>accès lexical, anomie, mot sur le bout de la langue, réseaux lexicaux</mots_cles>
			<title>Lexical access via a simple co-occurrence network</title>
			<abstract>During the last two decades psychologists and computational linguists have attempted to tackle the problem of word access via computational resources, yet hardly none of them has seriously tried to support 'interactive' word finding. Yet, a lot of work has been done to understand the causes of the tip-of-the-tongue problem (TOT). Given the progress made in neuroscience, corpus linguistics, and graph theory (complex graphs), one may be tempted to emulate the mental lexicon, or to build a resource likely to help authors (speakers, writers) to overcome word-finding problems. Our goal here is much more limited. We try to identify good hints for finding a target word. To this end we have built a co-occurrence network on the basis of Wikipedia abstracts. Since the network is built automatically and from raw data, i.e. non-annotated text, it does not reveal the kind of relationship holding between the nodes. Despite this shortcoming we tried to see whether we can find a given word, or, to identify what is a good clue word.</abstract>
			<keywords>lexical access, anomia, tip of the tongue (TOT), lexical networks</keywords>
		</article>
		<article id="taln-2013-court-013" session="Poster">
			<auteurs>
				<auteur>
					<nom>Guy Perrier</nom>
					<email>guy.perrier@loria.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LORIA, Université de Lorraine, équipe Sémagramme, bât. C, Campus Scientifique BP 239 54506 Vandøeuvre-lès-Nancy, cedex, France</affiliation>
			</affiliations>
			<titre>Analyse statique des interactions entre structures élémentaires d’une grammaire</titre>
			<type>court</type>
			<pages>604-611</pages>
			<resume>Nous nous intéressons ici à la construction semi-automatique de grammaires computationnelles et à leur utilisation pour l’analyse syntaxique. Nous considérons des grammaires lexicalisées dont les structures élémentaires sont des arbres, sous-spécifiés ou pas. Nous présentons un algorithme qui vise à prévoir l’ensemble des arbres élémentaires attachés aux mots qui peuvent s’intercaler entre deux mots donnés d’une phrase, dont on sait que les arbres élémentaires associées sont des compagnons, c’est-à-dire qu’ils interagiront nécessairement dans la composition syntaxique de la phrase.</resume>
			<mots_cles>grammaire lexicalisée, grammaire d’interaction, construction de grammaires</mots_cles>
			<title>Static Analysis of Interactions between Elementary Structures of a Grammar</title>
			<abstract>We are interested in the semi-automatic construction of computational grammars and in their use for parsing. We consider lexicalized grammars with elementary structures which are trees, underspecified or not. We present an algorithm that aims at foreseeing all elementary trees attached at words which can come between two given words of a sentence, whose associated elementary trees are companions, that is, they will necessarily interact in the syntactic composition of the sentence.</abstract>
			<keywords>Lexicalized Grammar, Interaction Grammar, Grammar Construction</keywords>
		</article>
		<article id="taln-2013-court-014" session="Poster">
			<auteurs>
				<auteur>
					<nom>Eric Charton</nom>
					<email>eric.charton@polymtl.ca</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Michel Gagnon</nom>
					<email>michel.gagnon@polymtl.ca</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Ludovic Jean-Louis</nom>
					<email>ludovic.jean-louis@polymtl.ca</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">École Polytechnique de Montréal, Montréal, QC, Canada</affiliation>
			</affiliations>
			<titre>Influence des annotations sémantiques sur un système de détection de coréférence à base de perceptron multi-couches</titre>
			<type>court</type>
			<pages>612-619</pages>
			<resume>La série de campagnes d’évaluation CoNLL-2011/2012 a permis de comparer diverses propositions d’architectures de systèmes de détection de co-références. Cet article décrit le système de résolution de coréférence Poly-co développé dans le cadre de la campagne d’évaluation CoNLL-2011 et évalue son potentiel d’amélioration en introduisant des propriétés sémantiques dans son modèle de détection. Notre système s’appuie sur un classifieur perceptron multi-couches. Nous décrivons les heuristiques utilisées pour la sélection des paires de mentions candidates, ainsi que l’approche de sélection des traits caractéristiques que nous avons utilisée lors de la campagne CoNLL-2011. Nous introduisons ensuite un trait sémantique complémentaire et évaluons son influence sur les performances du système.</resume>
			<mots_cles>Coréférence, Perceptron multi-couches</mots_cles>
			<title>Semantic annotation influence on coreference detection using perceptron approach</title>
			<abstract>The ConLL-2011/2012 evaluation campaign was dedicated to coreference detection systems. This paper presents the coreference resolution system Poly-co submitted to the closed track of the CoNLL-2011 Shared Task and evaluate is potential of evolution when it includes a semantic feature. Our system integrates a multilayer perceptron classifier in a pipeline approach. We describe the heuristic used to select the candidate coreference pairs that are fed to the network for training, and our feature selection method. We introduce a complementary semantic feature and evaluate the performances improvement.</abstract>
			<keywords>Coreference, Multilayer perceptron</keywords>
		</article>
		<article id="taln-2013-court-015" session="Poster">
			<auteurs>
				<auteur>
					<nom>Fatiha Sadat</nom>
					<email>Sadat.fatiha@uqam.ca</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Emad Mohamed</nom>
					<email>emohamed@umail.iu.edu</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université du Québec à Montréal, 201 Président Kennedy, Montréal, H2X 3Y7, QC, Canada</affiliation>
			</affiliations>
			<titre>Traduction automatique statistique pour l’arabe-français améliorée par le prétraitement et l’analyse de la langue</titre>
			<type>court</type>
			<pages>620-627</pages>
			<resume>Dans cet article, nous nous intéressons au prétraitement de la langue arabe comme langue source à des fins de traduction automatique statistique. Nous présentons une étude sur la traduction automatique statistique basée sur les syntagmes, pour la paire de langues arabe-français utilisant le décodeur Moses ainsi que d’autres outils de base. Les propriétés morphologiques et syntaxiques de la langue arabe sont complexes, ce qui rend cette langue difficile à maîtriser dans le domaine du TALN. Aussi, les performances d’un système de traduction statistique dépendent considérablement de la quantité et de la qualité des corpus d’apprentissage. Dans cette étude, nous montrons qu’un prétraitement basé sur les mots de la langue source (arabe) et l’introduction de quelques règles linguistiques par rapport à la syntaxe de la langue cible (français), permet d’obtenir des améliorations du score BLEU. Cette amélioration est réalisée sans augmenter la quantité des corpus d’apprentissage.</resume>
			<mots_cles>Traduction automatique statistique, traduction arabe-français, pré-traitement de corpus, morphologie de l’Arabe</mots_cles>
			<title>Pre-processing and Language Analysis for Arabic to French Statistical Machine Translation</title>
			<abstract>Arabic is a morphologically rich and complex language, which presents significant challenges for natural language processing and machine translation. In this paper, we describe an ongoing effort to build a competitive Arabic-French phrase–based machine translation system using the Moses decoder and other tools. The results show an increase in terms of BLEU score after introducing some pre-processing schemes for Arabic and applying additional language analysis rules in relation to the target language. The proposed approach is completed using pre-processing and language analysis rules without increasing the amount of training data.</abstract>
			<keywords>Statistical machine translation, Arabic-French translation, Corpus pre-processing, Arabic morphology</keywords>
		</article>
		<article id="taln-2013-court-016" session="Poster">
			<auteurs>
				<auteur>
					<nom>Bruno Guillaume</nom>
					<email>bruno.guillaume@loria.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Karën Fort</nom>
					<email>karen.fort@loria.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>3</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LORIA 54500 Vandoeuvre-lès-Nancy</affiliation>
				<affiliation affiliationId="2">Inria Nancy Grand-Est</affiliation>
				<affiliation affiliationId="3">Université de Lorraine</affiliation>
			</affiliations>
			<titre>Expériences de formalisation d’un guide d’annotation : vers l’annotation agile assistée</titre>
			<type>court</type>
			<pages>628-635</pages>
			<resume>Nous proposons dans cet article une méthodologie, qui s’inspire du développement agile et qui permettrait d’assister la préparation d’une campagne d’annotation. Le principe consiste à formaliser au maximum les instructions contenues dans le guide d’annotation afin de vérifier automatiquement si le corpus en construction est cohérent avec le guide en cours d’écriture. Pour exprimer la partie formelle du guide, nous utilisons la réécriture de graphes, qui permet de décrire par des motifs les constructions définies. Cette formalisation permet de repérer les constructions prévues par le guide et, par contraste, celles qui ne sont pas cohérentes avec le guide. En cas d’incohérence, un expert peut soit corriger l’annotation, soit mettre à jour le guide et relancer le processus.</resume>
			<mots_cles>annotation, guide d’annotation, annotation agile, réécriture de graphes</mots_cles>
			<title>Formalizing an annotation guide : some experiments towards assisted agile annotation</title>
			<abstract>This article presents a methodology, inspired from the agile development paradigm, that helps preparing an annotation campaign. The idea behind the methodology is to formalize as much as possible the instructions given in the guidelines, in order to automatically check the consistency of the corpus being annotated with the guidelines, as they are being written. To formalize the guidelines, we use a graph rewriting tool, that allows us to use a rich language to describe the instructions. This formalization allows us to spot the rightfully annotated constructions and, by contrast, those that are not consistent with the guidelines. In case of inconsistency, an expert can either correct the annotation or update the guidelines and rerun the process.</abstract>
			<keywords>annotation, annotation guide, agile annotation, graph rewriting</keywords>
		</article>
		<article id="taln-2013-court-017" session="Poster">
			<auteurs>
				<auteur>
					<nom>Catherine Dominguès</nom>
					<email>catherine.domingues@ign.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Iris Eshkol-Taravella</nom>
					<email>iris.eshkol@univ-orleans.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">IGN, laboratoire COGIT 73 avenue de Paris 94160 Saint-Mandé</affiliation>
				<affiliation affiliationId="2">LLL, UMR 7270, 10 Rue de Tours, BP 46527, 45065 ORLEANS cedex 2</affiliation>
			</affiliations>
			<titre>Repérer des toponymes dans des titres de cartes topographiques</titre>
			<type>court</type>
			<pages>636-642</pages>
			<resume>Les titres de cartes topographiques personnalisées composent un corpus spécifique caractérisé par des variations orthographiques et un nombre élevé de désignations de lieux. L'article présente le repérage des toponymes dans ces titres. Ce repérage est fondé sur l'utilisation de BDNyme, la base de données de toponymes géoréférencés de l'IGN, et sur une analyse de surface à l'aide de patrons. La méthode proposée élargit la définition du toponyme pour tenir compte de la nature du corpus et des données qu'il contient. Elle se décompose en trois étapes successives qui tirent parti du contexte extralinguistique de géoréférencement des toponymes et du contexte linguistique. Une quatrième étape qui ne retient pas le géoréférencement est aussi étudiée. Le balisage et le typage des toponymes permettent de mettre en avant d'une part la diversité des désignations de lieux et d'autre part leurs variations d'écriture. La méthode est évaluée (rappel, précision, F-mesure) et les erreurs analysées.</resume>
			<mots_cles>toponyme, information spatiale, écriture des toponymes, BDNyme, ressource lexicale</mots_cles>
			<title>Localizing toponyms in topographic map titles</title>
			<abstract>The titles of customized topographic maps constitute a specific corpus which is characterized by spelling variations and a very significant number of place names. This paper is about identifying toponyms in these titles. The toponym tracking is based on IGN's toponym data base as well as light parsing according to patterns. The method used broadens the definition of the toponym to include the nature of the corpus and the data in it. It consists of three successive stages where both the extralinguistic context - in this case georeferencing toponyms - and the linguistic context are taken into account. The fourth stage which is without georeferencing is examined too. Toponym tagging and typing allow to highlight toponym naming and spelling variations. The method has been assessed (recall, precision, F-measure) and the results analysed.</abstract>
			<keywords>toponyme, spatial information, toponyme writing, BDNyme, lexical resource</keywords>
		</article>
		<article id="taln-2013-court-018" session="Poster">
			<auteurs>
				<auteur>
					<nom>Pierre Zweigenbaum</nom>
					<email></email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Xavier Tannier</nom>
					<email></email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIMSI–CNRS, Orsay</affiliation>
				<affiliation affiliationId="2">Univ. Paris-Sud, Orsay</affiliation>
			</affiliations>
			<titre>Extraction des relations temporelles entre événements médicaux dans des comptes rendus hospitaliers</titre>
			<type>court</type>
			<pages>643-650</pages>
			<resume>Le défi i2b2/VA 2012 était dédié à la détection de relations temporelles entre événements et expressions temporelles dans des comptes rendus hospitaliers en anglais. Les situations considérées étaient beaucoup plus variées que dans les défis TempEval. Nous avons donc axé notre travail sur un examen systématique de 57 situations différentes et de leur importance dans le corpus d’apprentissage en utilisant un oracle, et avons déterminé empiriquement le classifieur qui se comportait le mieux dans chaque situation, atteignant ainsi une F-mesure globale de 0,623.</resume>
			<mots_cles>extraction d’information, événements médicaux, relations temporelles, médecine</mots_cles>
			<title>Extraction of temporal relations between clinical events in clinical documents</title>
			<abstract>The 2012 i2b2/VA challenge focused on the detection of temporal relations between events and temporal expressions in English clinical texts. The addressed situations were much more diverse than in the TempEval challenges. We thus focused on the systematic study of 57 distinct situations and their importance in the training corpus by using an oracle, and empirically determined the best performing classifier for each situation, thereby achieving a 0.623 F-measure.</abstract>
			<keywords>Information Extraction, Clinical Events, Temporal Relations, Medicine</keywords>
		</article>
		<article id="taln-2013-court-019" session="Poster">
			<auteurs>
				<auteur>
					<nom>Nikola Tulechki</nom>
					<email>tulechki@univ-tlse2.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Ludovic Tanguy</nom>
					<email>tanguy@univ-tlse2.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">CLLE-ERSS : CNRS et Université de Toulouse 2, 5 allées Antonio Machado, 31058 Toulouse CEDEX 9</affiliation>
				<affiliation affiliationId="2">Conseil en Facteurs Humains, 4 impasse Montcabrier, 31500 Toulouse</affiliation>
			</affiliations>
			<titre>Similarité de second ordre pour l’exploration de bases textuelles multilingues</titre>
			<type>court</type>
			<pages>651-658</pages>
			<resume>Cet article décrit l’utilisation de la technique de similarité de second ordre pour l’identification de textes semblables au sein d’une base de rapports d’incidents aéronautiques mélangeant les langues française et anglaise. L’objectif du système est, pour un document donné, de retrouver des documents au contenu similaire quelle que soit leur langue. Nous utilisons un corpus bilingue aligné de rapports d’accidents aéronautiques pour construire des paires de pivots et indexons les documents avec des vecteurs de similarités, tels que chaque coordonnée correspond au score de similarité entre un document dans une langue donnée et la partie du pivot de la même langue. Nous évaluons les performances du système sur un volumineux corpus de rapports d’incidents aéronautiques pour lesquels nous disposons de traductions. Les résultats sont prometteurs et valident la technique.</resume>
			<mots_cles>similarité de second ordre, multilingue, ESA</mots_cles>
			<title>Second order similarity for exploring multilingual textual databases</title>
			<abstract>This paper describes the use of second order similarities for identifying similar texts inside a corpus of aviation incident reports written in both French and English. We use a second bilingual corpus to construct pairs of reference documents and map each target document to a vector so each coordinate represents a similarity score between this document and the part of the reference corpus written in the same language. We evaluate the system using a large corpus of translated incident reports. The results are promising and validate the approach.</abstract>
			<keywords>second order similarity, multilingual, ESA</keywords>
		</article>
		<article id="taln-2013-court-020" session="Poster">
			<auteurs>
				<auteur>
					<nom>François-Régis Chaumartin</nom>
					<email>frc@proxem.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Proxem, 19 boulevard de Magenta, 75010 Paris</affiliation>
			</affiliations>
			<titre>Apprentissage d’une classification thématique générique et cross-langue à partir des catégories de la Wikipédia</titre>
			<type>court</type>
			<pages>659-666</pages>
			<resume>La catégorisation de textes nécessite généralement un investissement important en amont, avec une adaptation de domaine. L’approche que nous proposons ici permet d’associer finement à un texte tout-venant écrit dans une langue donnée, un graphe de catégories de la Wikipédia dans cette langue. L’utilisation de l’index inter-langues de l’encyclopédie en ligne permet de plus d’obtenir un sous-ensemble de ce graphe dans la plupart des autres langues.</resume>
			<mots_cles>catégorisation, apprentissage, recherche d’information, Wikipédia, graphes</mots_cles>
			<title>Cross-lingual and generic text categorization</title>
			<abstract>Text categorization usually requires a significant investment, which must often be associated to a field adaptation. The approach we propose here allows to finely associate a graph of Wikipedia categories to any text written in a given language. Moreover, the inter-lingual index of the online encyclopedia allows to get a subset of this graph in most other languages.</abstract>
			<keywords>categorization, machine learning, information retrieval, Wikipedia, graphs</keywords>
		</article>
		<article id="taln-2013-court-021" session="Poster">
			<auteurs>
				<auteur>
					<nom>Nadia Okinina</nom>
					<email>Nadia.Okinina@univ-tours.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Damien Nouvel</nom>
					<email>Damien.Nouvel@univ-tours.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Nathalie Friburger</nom>
					<email>Nathalie.Friburger@univ-tours.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Jean-Yves Antoine</nom>
					<email>Jean-Yves.Antoine@univ-tours.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université François Rabelais Tours, LI, 3 place Jean Jaurès, 41 000 Blois</affiliation>
				<affiliation affiliationId="2">ALPAGE, INRIA Roquencourt</affiliation>
			</affiliations>
			<titre>Apprentissage supervisé sur ressources encyclopédiques pour l’enrichissement d’un lexique de noms propres destiné à la reconnaissance des entités nommées</titre>
			<type>court</type>
			<pages>667-674</pages>
			<resume>Cet article présente une méthode hybride d’enrichissement d’un lexique de noms propres à partir de la base encyclopédique en ligne Wikipedia. Une des particularités de cette recherche est de viser l’enrichissement d’une ressource existante (Prolexbase) très contrôlée décrivant finement les noms propres. A la différence d’autres travaux destinés à la reconnaissance des entités nommées, notre objectif est donc de réaliser un enrichissement automatique de qualité. Notre approche repose sur l’utilisation en pipe-line de règles déterministes basées sur certaines informations DBpedia et d’une catégorisation supervisée à base de classifieur SVM. Nos résultats montrent qu’il est ainsi possible d’enrichir un lexique de noms propres avec une très bonne précision.</resume>
			<mots_cles>reconnaissance des entités nommées, lexique de nom propre, enrichissement automatique de lexique, Wikipedia, règles, classification supervisée, machine à vecteurs de support, SVM</mots_cles>
			<title>Supervised learning on encyclopaedic resources for the extension of a lexicon of proper names dedicated to the recognition of named entities</title>
			<abstract>This paper concerns the automatic extension of a lexicon of proper names by means of a hybrid mining of Wikipedia. The specificity of this research is to focus on the quality of the added lexical entries, since the mining process is supposed to extend a controlled existing resource (Prolexbase). Our approach consists in the successive application of deterministic rules based on some specific information of the DBpedia and of a supervised classification with a SVM classifier. Our experiments show that it is possible to extend automatically such a lexicon without adding a perceptible noise to the resource.</abstract>
			<keywords>named entities recognition, proper names lexicon, automatic extension of lexicon, Wikipedia, rules, supervised classification, support vector machines, SVM</keywords>
		</article>
		<article id="taln-2013-court-022" session="Poster">
			<auteurs>
				<auteur>
					<nom>Patrick Paroubek</nom>
					<email>pap@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Munshi Asadullah</nom>
					<email>munshi@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Anne Vilnat</nom>
					<email>anne@limsi.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIMSI-CNRS Bât. 508 Université Paris-Sud, 91403 Orsay Cedex France</affiliation>
			</affiliations>
			<titre>Convertir des analyses syntaxiques en dépendances vers les relations fonctionnelles PASSAGE</titre>
			<type>court</type>
			<pages>675-682</pages>
			<resume>Nous présentons ici les premiers travaux concernant l’établissement d’une passerelle bidirectionnelle entre d’une, part les schémas d’annotation syntaxique en dépendances qui ont été définis pour convertir les annotations du French Treebank en arbres de dépendances de surface pour l’analyseur syntaxique Bonsai, et d’autre part le formalisme d’annotation PASSAGE développé initialement pour servir de support à des campagnes d’évaluation ouvertes en mode objectif quantitatif boîte-noire pour l’analyse syntaxique du français.</resume>
			<mots_cles>Analyse Syntaxique, Corpus arboré, Dependances, DepFTB, ConLL, PASSAGE</mots_cles>
			<title>Converting dependencies for syntactic analysis of French into PASSAGE functional relations</title>
			<abstract>We present here a first attempt at building a bidrictionnal converter between, on the one hand the dependency based syntaxtic formalism which has been defined to map the French Treebank annotation onto surface dependency trees used by the Bonsai parser, on the other hand the PASSAGE formalism developped intially for French parsing quantitative black-box objective open evaluation campaigns.</abstract>
			<keywords>Parsing, Treebank, Dependencies, DepFTB, ConLL, PASSAGE</keywords>
		</article>
		<article id="taln-2013-court-023" session="Poster">
			<auteurs>
				<auteur>
					<nom>Sharid Loáiciga</nom>
					<email>sharid.loaiciga@unige.ch</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Laboratoire d’Analyse et de Technologie du Langage, CUI - Université de Genève, Battelle - bâtiment A, 7 route de Drize, CH-1227 Carouge</affiliation>
			</affiliations>
			<titre>Résolution d’anaphores et traitement des pronoms en traduction automatique à base de règles</titre>
			<type>court</type>
			<pages>683-690</pages>
			<resume>La traduction des pronoms est l’un des problèmes actuels majeurs en traduction automatique. Étant donné que les pronoms ne transmettent pas assez de contenu sémantique en euxmêmes, leur traitement automatique implique la résolution des anaphores. La recherche en résolution des anaphores s’intéresse à établir le lien entre les entités sans contenu lexical (potentiellement des syntagmes nominaux et pronoms) et leurs référents dans le texte. Dans cet article, nous mettons en oeuvre un premier prototype d’une méthode inspirée de la théorie du liage chomskyenne pour l’interprétation des pronoms dans le but d’améliorer la traduction des pronoms personnels entre l’espagnol et le français.</resume>
			<mots_cles>Résolution d’anaphores, traduction automatique à base de règles, sujets nuls</mots_cles>
			<title>Anaphora Resolution for Machine Translation</title>
			<abstract>Pronoun translation is one of the current problems within Machine Translation. Since pronouns do not convey enough semantic content by themselves, pronoun processing requires anaphora resolution. Research in anaphora resolution is interested in establishing the link between entities (NPs and pronouns) and their antecedents in the text. In this article, we implement a prototype of a linguistic anaphora resolution method inspired from the Chomskyan Binding Theory in order to improve the translation of personal pronouns between Spanish and French.</abstract>
			<keywords>Anaphora Resolution, Rule-based Machine Translation, nul subjects</keywords>
		</article>
		<article id="taln-2013-court-024" session="Poster">
			<auteurs>
				<auteur>
					<nom>Frederik Cailliau</nom>
					<email>cailliau@sinequa.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Ariane Cavet</nom>
					<email>cavet@sinequa.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Clément De Groc</nom>
					<email>cdegroc@syllabs.com</email>
					<affiliationId>2</affiliationId>
					<affiliationId>3</affiliationId>
				</auteur>
				<auteur>
					<nom>Claude De Loupy</nom>
					<email>loupy@syllabs.com</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Sinequa, 12 rue d’Athènes, 75009 Paris</affiliation>
				<affiliation affiliationId="2">Syllabs, 53 bis rue Sedaine, 75011 Paris</affiliation>
				<affiliation affiliationId="3">LIMSI-CNRS, BP 133, 91403 Orsay CEDEX</affiliation>
			</affiliations>
			<titre>Lexiques de corpus comparables et recherche d’information multilingue</titre>
			<type>court</type>
			<pages>691-698</pages>
			<resume>Nous évaluons l’utilité de trois lexiques bilingues dans un cadre de recherche interlingue français vers anglais sur le corpus CLEF. Le premier correspond à un dictionnaire qui couvre le corpus, alors que les deux autres ont été construits automatiquement à partir des sous-ensembles français et anglais de CLEF, en les considérant comme des corpus comparables. L’un contient des mots simples, alors que le deuxième ne contient que des termes complexes. Les lexiques sont intégrés dans des interfaces différentes dont les performances de recherche interlingue sont évaluées par 5 utilisateurs sur 15 thèmes de recherche CLEF. Les meilleurs résultats sont obtenus en intégrant le lexique de mots simples généré à partir des corpus comparables dans une interface proposant les cinq « meilleures » traductions pour chaque mot de la requête.</resume>
			<mots_cles>recherche d’information multilingue, corpus comparables, lexiques multilingues</mots_cles>
			<title>Lexicons from Comparable Corpora for Multilingual Information Retrieval</title>
			<abstract>We evaluate the utility of three bilingual lexicons for English-to-French crosslingual search on the CLEF corpus. The first one is a kind of dictionary whose content covers the corpus. The other two have been automatically built on the French and English subparts of the CLEF corpus, by considering them as comparable corpora. One is made of simple words, the other one of complex words. The lexicons are integrated in different interfaces whose crosslingual search performances are evaluated by 5 users on 15 topics of CLEF. The best results are given with the interface having the simple-words lexicon generated on comparable corpora and proposing 5 translations for each query term.</abstract>
			<keywords>multilingual information retrieval, comparable corpora, multilingual lexicons</keywords>
		</article>
		<article id="taln-2013-court-025" session="Poster">
			<auteurs>
				<auteur>
					<nom>Philippe Suignard</nom>
					<email>philippe.suignard@edf.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Sofiane Kerroua</nom>
					<email>skerroua@aid.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Electricité de France R&amp;D, 1 avenue du Général de Gaulle, 92141 Clamart</affiliation>
				<affiliation affiliationId="2">A.I.D., 4 rue Henri Le Sidaner, 78000 Versailles</affiliation>
			</affiliations>
			<titre>Utilisation de contextes pour la correction automatique ou semi-automatique de réclamations clients</titre>
			<type>court</type>
			<pages>699-706</pages>
			<resume>Cet article présente deux méthodes permettant de corriger des réclamations contenant des erreurs rédactionnelles, en s’appuyant sur le graphe des voisins orthographiques et contextuels. Ce graphe est constitué des formes ou mots trouvés dans un corpus d’apprentissage. Un lien entre deux formes traduit le fait que les deux formes se « ressemblent » et partagent des contextes similaires. La première méthode est semi-automatique et consiste à produire un dictionnaire de substitution à partir de ce graphe. La seconde méthode, plus ambitieuse, est entièrement automatisée. Elle s’appuie sur les contextes pour déterminer à quel mot correspond telle forme abrégée ou erronée. Les résultats ainsi obtenus permettent d’améliorer le processus déjà existant de constitution d’un dictionnaire de substitution mis en place au sein d’EDF.</resume>
			<mots_cles>Correction automatique, analyse distributionnelle, graphe, contexte</mots_cles>
			<title>Using contexts for automatic or semi-automatic correction of customer complaints</title>
			<abstract>This article presents two methods allowing correcting complaints containing spelling errors, by using the spelling and contextual neighbors' graph. This graph is made of forms or words found in a learning corpus. A link between two forms conveys the fact that the two forms ''look alike'' and share similar contexts. The first method is semi-automatic and consists in producing a substitutional dictionary from this graph. The second method, more ambitious, is fully automatic. It is based on contexts to determine to which word corresponds such abbreviated or erroneous form. The results thus obtained allow us to improve the existing process regarding the creation of a substitutional dictionary at EDF.</abstract>
			<keywords>Spelling correction, distributional analysis, graph, context</keywords>
		</article>
		<article id="taln-2013-court-026" session="Poster">
			<auteurs>
				<auteur>
					<nom>Luis Adrián Cabrera-Diego</nom>
					<email>adrian.cabrera@flejay.com</email>
					<affiliationId>1</affiliationId>
					<affiliationId>4</affiliationId>
				</auteur>
				<auteur>
					<nom>Juan-Manuel Torres-Moreno</nom>
					<email>juan-manuel.torres@univ-avignon.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
					<affiliationId>3</affiliationId>
				</auteur>
				<auteur>
					<nom>Marc El-Bèze</nom>
					<email>marc.elbeze@univ-avignon.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>3</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIA, Université d’Avignon et des Pays de Vaucluse, France</affiliation>
				<affiliation affiliationId="2">École Polytechnique de Montréal, Canada</affiliation>
				<affiliation affiliationId="3">SFR Agorantic UAPV, France</affiliation>
				<affiliation affiliationId="4">Flejay Group, France</affiliation>
			</affiliations>
			<titre>SegCV : traitement efficace de CV avec analyse et correction d’erreurs</titre>
			<type>court</type>
			<pages>707-714</pages>
			<resume>Le marché d’offres d’emploi et des candidatures sur Internet a connu, ces derniers temps, une croissance exponentielle. Ceci implique des volumes d’information (majoritairement sous la forme de textes libres) intraitables manuellement. Les CV sont dans des formats très divers : .pdf, .doc, .dvi, .ps, etc., ce qui peut provoquer des erreurs lors de la conversion en texte plein. Nous proposons SegCV, un système qui a pour but l’analyse automatique des CV des candidats. Dans cet article, nous présentons des algorithmes reposant sur une analyse de surface, afin de segmenter les CV de manière précise. Nous avons évalué la segmentation automatique selon des corpus de référence que nous avons constitués. Les expériences préliminaires réalisées sur une grande collection de CV en français avec correction du bruit montrent de bons résultats en précision, rappel et F-Score.</resume>
			<mots_cles>RI, Ressources humaines, traitement de CV, Modèle à base de règles</mots_cles>
			<title>SegCV : Eficient parsing of résumés with analysis and correction of errors</title>
			<abstract>Over the last years, the online market of jobs and candidatures offers has reached an exponential growth. This has implied great amounts of information (mainly in a text free style) which cannot be processed manually. The résumés are in several formats : .pdf, .doc, .dvi, .ps, etc., that can provoque errors or noise during the conversion to plain text. We propose SegCV, a system that has as goal the automatic parsing of candidates’ résumés. In this article we present the algoritms, which are based over a surface analysis, to segment the résumés in an accurate way. We evaluated the automatic segmentation using a reference corpus that we have created. The preliminary experiments, done over a large collection of résumés in French with noise correction, show good results in precision, recall and F-score.</abstract>
			<keywords>Information Retrieval, Human Resources, CV Parsing, Rules Model</keywords>
		</article>
		<article id="taln-2013-court-027" session="Poster">
			<auteurs>
				<auteur>
					<nom>Jean-Valère Cossu</nom>
					<email>jean-valere.cossu@univ-avignon.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Juan-Manuel Torres-Moreno</nom>
					<email>juan-manuel.torres@univ-avignon.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
					<affiliationId>3</affiliationId>
					<affiliationId>4</affiliationId>
				</auteur>
				<auteur>
					<nom>Marc El-Bèze</nom>
					<email>marc.elbeze@univ-avignon.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
					<affiliationId>4</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Laboratoire Informatique d'Avignon - Université d'Avignon et des Pays de Vaucluse 339 chemin des Meinajaries, BP91228 84911 Avignon Cedex 9, France</affiliation>
				<affiliation affiliationId="2">SFR Agorantic Université d’Avignon et des Pays de Vaucluse, 84000 Avignon Cedex</affiliation>
				<affiliation affiliationId="3">École Polytechnique de Montréal, 2900 Bd Edouard-Montpetit Montréal, QC H3T1J4</affiliation>
				<affiliation affiliationId="4">Brain &amp; Language Research Institute, 5 avenue Pasteur, 13604 Aix-en-Provence Cedex 1</affiliation>
			</affiliations>
			<titre>Recherche et utilisation d'entités nommées conceptuelles dans une tâche de catégorisation</titre>
			<type>court</type>
			<pages>715-722</pages>
			<resume>Les recherches présentées sont directement liées aux travaux menés pour résoudre les problèmes de catégorisation automatique de texte. Les mots porteurs d’opinions jouent un rôle important pour déterminer l’orientation du message. Mais il est essentiel de pouvoir identifier les cibles auxquelles ils se rapportent pour en contextualiser la portée. L’analyse peut également être menée dans l’autre sens, on cherchant dans le contexte d’une cible détectée les termes polarisés. Une première étape d’apprentissage depuis des données permet d'obtenir automatiquement les marqueurs de polarité les plus importants. A partir de cette base, nous cherchons les cibles qui apparaissent le plus fréquemment à proximité de ces marqueurs d'opinions. Ensuite, nous construisons un ensemble de couples (marqueur de polarité, cible) pour montrer qu’en s’appuyant sur ces couples, on arrive à expliquer plus finement les prises de positions tout en maintenant (voire améliorant) le niveau de performance du classifieur.</resume>
			<mots_cles>Fouille d’opinion, Marqueurs de polarité, Reconnaissance d’entités nommées</mots_cles>
			<title>Search and usage of named conceptual entities in a categorisazion task</title>
			<abstract>The researchs presented are part of a text automatic categorization task. Words bearing opinions play an important role in determining the overall direction of the message. But it is essential to identify the elements (targets) which they are intended to relativize the scope. The analysis can also be conducted in the reverse direction. When a target is detected we need to search polarized terms in the context. A first step in an automatic learning from data will allow us to obtain the most important polarity markers. From this basis, we look for targets that appear most frequently in the vicinity of these opinions markers. Then, we construct a set of pairs (polarity marker, target) to show that relying on these couples we can maintain (or improve) the performance of the classifier.</abstract>
			<keywords>Opinion Mining, Named Entity Recognition</keywords>
		</article>
		<article id="taln-2013-court-028" session="Poster">
			<auteurs>
				<auteur>
					<nom>Guillaume Wisniewski</nom>
					<email>wisniews@limsi.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Anil Kumar Singh</nom>
					<email>anil@limsi.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Natalia Segal</nom>
					<email>nsegal@softissimo.com</email>
					<affiliationId>3</affiliationId>
				</auteur>
				<auteur>
					<nom>François Yvon</nom>
					<email>yvon@limsi.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université Paris Sud 91 403 ORSAY CEDEX</affiliation>
				<affiliation affiliationId="2">LIMSI–CNRS 91 403 ORSAY CEDEX</affiliation>
				<affiliation affiliationId="3">Reverso–Softissimo, 5 rue Soyer, 92 500 NEUILLY</affiliation>
			</affiliations>
			<titre>Un corpus d’erreurs de traduction</titre>
			<type>court</type>
			<pages>723-730</pages>
			<resume>Avec le développement de la post-édition, de plus en plus de corpus contenant des corrections de traductions sont disponibles. Ce travail présente un corpus de corrections d’erreurs de traduction collecté dans le cadre du projet ANR/TRACE et illustre les différents types d’analyses auxquels il peut servir. Nous nous intéresserons notamment à la détection des erreurs fréquentes et à l’analyse de la variabilité des post-éditions.</resume>
			<mots_cles>Traduction automatique, Analyse d’erreur, Post-édiition</mots_cles>
			<title>A corpus of post-edited translations</title>
			<abstract>More and more datasets of post-edited translations are being collected. These corpora have many applications, such as failure analysis of SMT systems and the development of quality estimation systems for SMT. This work presents a large corpus of post-edited translations that has been gathered during the ANR/TRACE project. Applications to the detection of frequent errors and to the analysis of the inter-rater agreement of hTER are also reported.</abstract>
			<keywords>Machine Translation, Failure Analysis, Post-edition</keywords>
		</article>
		<article id="taln-2013-court-029" session="Poster">
			<auteurs>
				<auteur>
					<nom>Samira Walha Ellouze</nom>
					<email>ellouze.samira@gmail.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Maher Jaoua</nom>
					<email>Maher.Jaoua@fsegs.rnu.tn</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Lamia Hadrich Belguith</nom>
					<email>l.belguith@fsegs.rnu.tn</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">ANLP Research Group, Laboratoire MIRACL, Route de l’aéroport Km 4, 3018, Sfax, Tunisie</affiliation>
			</affiliations>
			<titre>Une méthode d’évaluation des résumés basée sur la combinaison de métriques automatiques et de complexité textuelle</titre>
			<type>court</type>
			<pages>731-738</pages>
			<resume>Cet article présente une méthode automatique d’évaluation du contenu des résumés automatiques. La méthode proposée est basée sur une combinaison de caractéristiques englobant des scores de contenu et d’autres de complexité textuelle et ce en s’appuyant sur une technique d’apprentissage, à savoir la régression linéaire. L’objectif de cette combinaison consiste à prédire le score manuel PYRAMID à partir des caractéristiques utilisées. Afin d’évaluer la méthode présentée, nous nous sommes intéressés à deux niveaux de granularité d’évaluation : la première est qualifiée de Micro-évaluation et propose l’évaluation de chaque résumé, alors que la deuxième est une Macro-évaluation et s’applique au niveau de chaque système.</resume>
			<mots_cles>Evaluation intrinsèque, évaluation du contenu, résumé automatique, complexité textuelle, régression linéaire</mots_cles>
			<title>An evaluation summary method based on combination of automatic and textual complexity metrics</title>
			<abstract>This article presents an automatic method for evaluating content summaries. The proposed method is based on a combination of features encompassing scores of content and others of textual complexity. This method relies on a learning technique namely the linear regression. The objective of this combination is to predict the PYRAMID score from used features. In order to evaluate the presented method, we are interested in two levels of granularity evaluation: the first is named Micro-evaluation and proposes an evaluation of each summary, while the second is called Macro-evaluation and it applies at the level of each system.</abstract>
			<keywords>Intrinsic evaluation, content evaluation, automatic summary, textual complexity, linear regression</keywords>
		</article>
		<article id="taln-2013-court-030" session="Poster">
			<auteurs>
				<auteur>
					<nom>Abdessalam Bouchekif</nom>
					<email>abdessalam.bouchekif@orange.com</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Géraldine Damnati</nom>
					<email>geraldine.damnati@orange.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Delphine Charlet</nom>
					<email>delphine.charlet@orange.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Orange Labs , 2, Avenue Pierre Marzin 22307 Lannion Cedex</affiliation>
				<affiliation affiliationId="2">Laboratoire d’Informatique de l’Universite du Maine, LIUM - France</affiliation>
			</affiliations>
			<titre>Segmentation thématique : processus itératif de pondération intra-contenu</titre>
			<type>court</type>
			<pages>739-746</pages>
			<resume>Dans cet article, nous nous intéressons à la segmentation thématique d’émissions télévisées exploitant la cohésion lexicale. Le but est d’étudier une approche générique, reposant uniquement sur la transcription automatique sans aucune information externe ni aucune information structurelle sur le contenu traité. L’étude porte plus particulièrement sur le mécanisme de pondération des mots utilisés lors du calcul de la cohésion lexicale. Les poids TF-IDF sont estimés à partir du contenu lui-même, qui est considéré comme une collection de documents mono-thème. Nous proposons une approche itérative, intégrée à un algorithme de segmentation, visant à raffiner la partition du contenu en documents pour l’estimation de la pondération. La segmentation obtenue à une itération donnée fournit un ensemble de documents à partir desquels les poids TF-IDF sont ré-estimés pour la prochaine itération. Des expériences menées sur un corpus couvrant différents formats des journaux télévisés issus de 8 chaînes françaises montrent une amélioration du processus global de segmentation.</resume>
			<mots_cles>Segmentation thématique, pondération TF-IDF, cohésion lexicale, TextTiling</mots_cles>
			<title>An iterative topic segmentation algorithm with intra-content term weighting</title>
			<abstract>This paper deals with topic segmentation of TV Broadcasts using lexical cohesion. The aim is to propose a generic approach, only relying on the automatic speech transcription with no external nor a priori information on the TV content. The study focuses on a new weighting scheme for lexical cohesion computation. TF-IDF weights are estimated from the content itself which is considered as a collection of mono-thematic documents. We propose an iterative process, integrated to a segmentation algorithm, aiming to refine the partition of a content into documents in order to estimate the weights. Topic segmentation obtained at a given iteration provides a set of documents from which TF-IDF weights are re-estimated for the next iteration. An experiment on a rich corpus covering various formats of Broadcast News shows from 8 French TV channels improves the overall topic segmentation process.</abstract>
			<keywords>Topic segmentation, TF-IDF weighting, lexical cohesion, TextTiling</keywords>
		</article>
		<article id="taln-2013-court-031" session="Poster">
			<auteurs>
				<auteur>
					<nom>Alexander Panchenko</nom>
					<email>Alexander.Panchenko@uclouvain.be</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Hubert Naets</nom>
					<email>Hubert.Naets@uclouvain.be</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Laetitia Brouwers</nom>
					<email>Laetitia.Brouwers@uclouvain.be</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Pavel Romanov</nom>
					<email>aromanov@it-claim.ru</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Cédrick Fairon</nom>
					<email>Cedrick.Fairon@uclouvain.be</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">CENTAL, Université catholique de Louvain, Belgique</affiliation>
				<affiliation affiliationId="2">Bauman Moscow State Technical University, Russie</affiliation>
			</affiliations>
			<titre>Recherche et visualisation de mots sémantiquement liés</titre>
			<type>court</type>
			<pages>747-754</pages>
			<resume>Nous présentons PatternSim, une nouvelle mesure de similarité sémantique qui repose d’une part sur des patrons lexico-syntaxiques appliqués à de très vastes corpus et d’autre part sur une formule de réordonnancement des candidats extraits. Le système, initialement développé pour l’anglais, a été adapté au français. Nous rendons compte de cette adaptation, nous en proposons une évaluation et décrivons l’usage de ce nouveau modèle dans la plateforme de consultation en ligne Serelex.</resume>
			<mots_cles>Mesure de similarité sémantique, relations sémantiques</mots_cles>
			<title>Search and Visualization of Semantically Related Words</title>
			<abstract>We present PatternSim, a new semantic similarity measure that relies on morpho-syntactic patterns applied to very large corpora and on a re-ranking formula that reorder extracted candidates. The system, originally developed for English, was adapted to French. We explain this adaptation, propose a first evaluation of it and we describe how this new model was used to build the Serelex online search platform.</abstract>
			<keywords>Semantic similarity measure, semantic relations</keywords>
		</article>
		<article id="taln-2013-court-032" session="Poster">
			<auteurs>
				<auteur>
					<nom>Jean-Philippe Guilbaud</nom>
					<email>Jean-Philippe.Guilbaud@imag.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Christian Boitet</nom>
					<email>Christian.Boitet@imag.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Vincent Berment</nom>
					<email>Vincent.Berment@imag.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">CNRS, LIG-campus, 38041 Grenoble Cedex 09</affiliation>
				<affiliation affiliationId="2">UJF, Université de Grenoble, LIG-campus, 38041 Grenoble Cedex 09</affiliation>
			</affiliations>
			<titre>Un analyseur morphologique étendu de l'allemand traitant les formes verbales à particule séparée</titre>
			<type>court</type>
			<pages>755-763</pages>
			<resume>Nous décrivons l’organisation et l'état courant de l’analyseur morphologique de l’allemand AMALD de grande taille couvrant (près de 103000 lemmes et 500000 formes fléchies simples, en croissance) développé dans le cadre du projet ANR-Émergence Traouiero. C’est le premier lemmatiseur de l’allemand capable de traiter non seulement les mots simples et les mots composés, mais aussi les verbes à particules séparables quand elles sont séparées, même par un grand nombre de mots (ex : Hier schlagen wir eine neue Methode für die morphologische Analyse vor).</resume>
			<mots_cles>analyse morphologique, lemmatisation, allemand, verbes à particule séparable</mots_cles>
			<title>An extended morphological analyzer of German handling verbal forms with separated separable particles</title>
			<abstract>We describe the organisation and the current state of the large-scale (nearly 103000 lemmas and 500000 simple inflected forms, growing) morphological analyzer AMALD developed in the framework of the ANR-Émergence Traouiero project. It is the first lemmatizer of German able to handle not only simple and compound words, but also verbs with separable particles when they are separated, even by many words (e.g. Hier schlagen wir eine neue Methode für die morphologische Analyse vor.).</abstract>
			<keywords>morphological analysis, lemmatization, German, verbs with separable particles</keywords>
		</article>
		<article id="taln-2013-court-033" session="Poster">
			<auteurs>
				<auteur>
					<nom>Marc Vincent</nom>
					<email>marc.r.vincent@gmail.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Grégoire Winterstein</nom>
					<email>gregoire.winterstein@linguist.jussieu.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">UMR S-775, Université Paris Descartes</affiliation>
				<affiliation affiliationId="2">LLF, UMR 7110, Université Sorbonne Nouvelle</affiliation>
			</affiliations>
			<titre>Construction et exploitation d’un corpus français pour l’analyse de sentiment</titre>
			<type>court</type>
			<pages>764-771</pages>
			<resume>Ce travail présente un corpus en français dédié à l’analyse de sentiment. Nous y décrivons la construction et l’organisation du corpus. Nous présentons ensuite les résultats de l’application de techniques d’apprentissage automatique pour la tâche de classification d’opinion (positive ou négative) véhiculée par un texte. Deux techniques sont utilisées : la régression logistique et la classification basée sur des Support Vector Machines (SVM). Nous mentionnons également l’intérêt d’appliquer une sélection de variables avant la classification (par régularisation par elastic net).</resume>
			<mots_cles>Analyse de sentiments, Corpus, Classification, Apprentissage automatique, Sélection de variable</mots_cles>
			<title>Building and exploiting a French corpus for sentiment analysis</title>
			<abstract>This work introduces a French corpus for sentiment analysis. We describe the construction and organization of the corpus. We then apply machine learning techniques to automatically predict whether a text is positive or negative (the opinion classification task). Two techniques are used : logistic regression and classification based on Support Vector Machines (SVM). Finally, we briefly evaluate the merits of applying feature selection algorithms to our models (via elastic net regularization).</abstract>
			<keywords>Sentiment Analysis, Corpus, Opinion Mining, Classification, Machine Learning, Variable Selection</keywords>
		</article>
		<article id="taln-2013-court-034" session="Poster">
			<auteurs>
				<auteur>
					<nom>Luka Nerima</nom>
					<email>luka.nerima@unige.ch</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Éric Wehrli</nom>
					<email>eric.wehrli@unige.ch</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LATL, Université de Genève, 2 rue de Candolle, 1211 Genève 4</affiliation>
			</affiliations>
			<titre>Résolution d'anaphores appliquée aux collocations: une évaluation préliminaire</titre>
			<type>court</type>
			<pages>772-778</pages>
			<resume>Le traitement des collocations en analyse et en traduction est depuis de nombreuses années au centre de nos intérêts de recherche. L’analyseur Fips a été récemment enrichi d’un module de résolution d’anaphores. Dans cet article nous décrivons comment la résolution d’anaphores a été appliquée à l’identification des collocations et comment cela permet à l’analyseur de repérer une collocation même si un de ses termes a été pronominalisé. Nous décrivons aussi la méthodologie de l’évaluation, notamment la préparation des données pour le calcul du rappel. Dans la tâche d’identification des collocations pronominalisées, Fips montre des résultats très encourageants : la précision mesurée est de 98% alors que le rappel est proche de 50%. Dans cette évaluation nous nous intéressons aux collocations de type verbe-objet direct en conjonction avec les pronoms anaphoriques à la 3e personne. Le corpus utilisé est un corpus anglais d’environ dix millions de mots.</resume>
			<mots_cles>Analyse, résolution d’anaphores, pronoms personnels, collocations, corpus</mots_cles>
			<title>Anaphora Resolution Applied to Collocation Identification: A Preliminary Evaluation</title>
			<abstract>Collocation identification and collocation translation have been at the center of our research interests for several years. Recently, the Fips parser has been enriched by an anaphora resolution mechanism. This article discusses how anaphora resolution has been applied to the collocation identification task, and how it enables the parser to identify a collocation when one of its terms is pronominalized. We also describe the evaluation methodology, in particular the preparation of data for the calculation of the recall. In the task of pronominalized collocation identification, Fips shows encouraging results: the measured precision is 98% while recall approaches 50%. In this paper we focus on collocations of the type verb-direct object and on a widespread type of anaphora: the third personal pronouns. The corpus used is a corpus of approximately ten million English words.</abstract>
			<keywords>Parsing, anaphora resolution, personal pronoun, collocations, corpus</keywords>
		</article>
		<article id="taln-2013-court-035" session="Poster">
			<auteurs>
				<auteur>
					<nom>Thibault Mondary</nom>
					<email>Thibault.Mondary@lipn.univ-paris13.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Adeline Nazarenko</nom>
					<email>Adeline.Nazarenko@lipn.univ-paris13.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Haïfa Zargayouna</nom>
					<email>Haifa.Zargayouna@lipn.univ-paris13.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Sabine Barreaux</nom>
					<email>Sabine.Barreaux@inist.fr</email>
					<affiliationId>2</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université Paris 13, Sorbonne Paris Cité, LIPN (UMR 7030), F-93430 Villetaneuse, France</affiliation>
				<affiliation affiliationId="2">INIST-CNRS, Vandoeuvre-lès-Nancy, France</affiliation>
			</affiliations>
			<titre>Aide à l’enrichissement d’un référentiel terminologique : propositions et expérimentations</titre>
			<type>court</type>
			<pages>779-786</pages>
			<resume>En s’appuyant sur une expérience d’enrichissement terminologique, cet article montre comment assister le travail d’acquisition terminologique et surmonter concrètement les deux difficultés qu’il présente : la masse de candidats-termes à considérer et la subjectivité des jugements terminologiques qui varient notamment en fonction du type de terminologie à produire. Nous proposons des stratégies simples pour filtrer a priori une partie du bruit des résultats des extracteurs et rendre ainsi la validation praticable pour des terminologues et nous démontrons leur efficacité sur un échantillon de candidats-termes proposés à la validation de deux spécialistes du domaine. Nous montrons également qu’en appliquant à une campagne de validation terminologique les mêmes principes méthodologiques que pour une campagne d’annotation, on peut contrôler la qualité des jugements de validation posés et de la terminologie qui en résulte.</resume>
			<mots_cles>Acquisition terminologique, validation de candidats-termes, filtrage de termes, distance terminologique, vote, accord inter-juges</mots_cles>
			<title>Help enrich a terminological repository : proposals and experiments</title>
			<abstract>Based on an experience of terminological enrichment, this paper shows how to support the work of terminological acquisition and overcome practical difficulties it presents, i.e. the mass of candidate terms to consider and the subjectivity of terminological judgments which depends on the type of terminology to produce. We propose simple strategies to filter a priori part of the noise from the results of term extractors so as to make the validation practicable for terminologists. We demonstrate their effectiveness on a sample of candidate terms proposed for the validation of two experts. We also show that by applying to term validation campaigns the methodological principles that have been proposed for corpus annotation campaigns, we can control the quality of validation judgments and of the resulting terminologies.</abstract>
			<keywords>Terminology acquisition, term candidate validation, term filtering, terminological distance, vote, inter-judge agreement</keywords>
		</article>
		<article id="taln-2013-demo-001" session="Démonstrations">
			<auteurs>
				<auteur>
					<nom>Gaël Lejeune</nom>
					<email>Gael.Lejeune@unicaen.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Romain Brixtel</nom>
					<email>Romain.Brixtel@unicaen.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Charlotte Lecluze</nom>
					<email>Charlotte.Lecluze@unicaen.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Antoine Doucet</nom>
					<email>Antoine.Doucet@unicaen.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Nadine Lucas</nom>
					<email>Nadine.Lucas@unicaen.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Normandie Université; UNICAEN, GREYC, CNRS UMR 6072, F-14032 Caen</affiliation>
			</affiliations>
			<titre>DAnIEL : Veille épidémiologique multilingue parcimonieuse</titre>
			<type>démonstration</type>
			<pages>787-788</pages>
			<resume>DAnIEL est un système multilingue de veille épidémiologique. DAnIEL permet de traiter un grand nombre de langues à faible coût grâce à une approche parcimonieuse en ressources.</resume>
			<mots_cles>extraction d’information, recherche d’information, veille, multilinguisme, genre journalistique, grain caractère</mots_cles>
			<title>DAnIEL, parsimonious yet high-coverage multilingual epidemic surveillance</title>
			<abstract>DAnIEL is a multilingual epidemic surveillance system. DAnIEL relies on a parsimonious scheme making it possible to process new languages at small cost.</abstract>
			<keywords>information extraction, information retrieval, news surveillance, multilingualism, news genre, character-level analysis</keywords>
		</article>
		<article id="taln-2013-demo-002" session="Démonstrations">
			<auteurs>
				<auteur>
					<nom>Elena Kozlova</nom>
					<email>Helen_Koz@abbyy.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Maria Gontcharova</nom>
					<email>maria_go@abbyy.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Tatiana Popova</nom>
					<email>Tatiana_P@abbyy.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">ABBYY, 2B rue Otradnaya, Moscou, Russie</affiliation>
			</affiliations>
			<titre>Lexique multilingue dans le cadre du modèle Compreno développé ABBYY</titre>
			<type>démonstration</type>
			<pages>789-790</pages>
			<resume>Le lexique multilingue basé sur une hiérarchie sémantique universelle fait partie du modèle linguistique Compreno destiné à plusieurs applications du TALN, y compris la traduction automatique et l’analyse sémantique et syntaxique. La ressource est propriétaire et n’est pas librement disponible.</resume>
			<mots_cles>Lexique multilingue, hiérarchie sémantique universelle, traduction automatique</mots_cles>
			<title>Multilingual lexical database in the framework of COMPRENO linguistic model developed by ABBYY</title>
			<abstract>The multilingual lexical database based on the universal semantic hierarchy is part of Compreno linguistic model. This model is meant for various NLP applications dealing with machine translation, semantic and syntactic analysis. The resource is private and is not freely available.</abstract>
			<keywords>Multilingual lexical database, universal semantic hierarchy, machine translation</keywords>
		</article>
		<article id="taln-2013-demo-003" session="Démonstrations">
			<auteurs>
				<auteur>
					<nom>Manon Quintana</nom>
					<email>mquintana@inbenta.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">INBENTA FR 164 route de Revel 31400 Toulouse</affiliation>
			</affiliations>
			<titre>Inbenta Semantic Search Engine : un moteur de recherche sémantique inspiré de la Théorie Sens-Texte</titre>
			<type>démonstration</type>
			<pages>791-792</pages>
			<resume>Avec la digitalisation massive de documents apparaît la nécessité de disposer de systèmes de recherche capables de s’adapter aux habitudes de recherche des utilisateurs et de leur permettre d’accéder à l’information rapidement et efficacement. INBENTA a ainsi créé un moteur de recherche intelligent appellé Inbenta Semantic Search Engine (ISSE). Les deux tâches principales de l’ISSE sont d’analyser les questions des utilisateurs et de trouver la réponse appropriée à la requête en effectuant une recherche dans une base de connaissances. Pour cela, la solution logicielle d’INBENTA se base sur la Théorie Sens-Texte qui se concentre sur le lexique et la sémantique.</resume>
			<mots_cles>Moteur de Recherche Sémantique, Théorie Sens-Texte, fonction lexicale</mots_cles>
			<title>Inbenta Semantic Search Engine: a semantic search engine inspired by the Meaning-Text Theory</title>
			<abstract>The need to have search systems able to adapt themselves to the particular way users pose their questions so that they can get a quick and efficient access to information is increasingly relevant due to the huge digitalization of documents. To cope with this reality, INBENTA has developed an intelligent search engine, called Inbenta Semantic Search Engine (ISSE). ISSE's main two tasks are analysing users' queries and finding the most appropriate answer to those questions in a knowledge-base. To carry out these tasks, INBENTA's software solution relies upon the Meaning-Text Theory, which focusses on the lexicon and semantics.</abstract>
			<keywords>Semantic Search Engine, Meaning-Text Theory, lexical function</keywords>
		</article>
		<article id="taln-2013-demo-004" session="Démonstrations">
			<auteurs>
				<auteur>
					<nom>Jean-Leon Bouraoui</nom>
					<email>jl.bouraoui@prometil.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Marc Canitrot</nom>
					<email>m.canitrot@prometil.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Prometil, 42 Avenue du Général de Croutte, 31100 Toulouse</affiliation>
			</affiliations>
			<titre>FMO : un outil d’analyse automatique de l’opinion</titre>
			<type>démonstration</type>
			<pages>793-794</pages>
			<resume>Nous décrivons notre prototype d’analyse automatique d’opinion. Celui-ci est basé sur un moteur d’analyse linguistique. Il permet de détecter finement les segments de texte porteurs d’opinions, de les extraire, et de leur attribuer une note selon la polarité qu’ils expriment. Nous présentons enfin les différentes perspectives que nous envisageons pour ce prototype.</resume>
			<mots_cles>Analyse d’opinion, e-reputation, extraction d’information</mots_cles>
			<title>FMO: a tool for automated opinion mining</title>
			<abstract>We describe our prototype of automatic opinion mining. It is based on a linguistic analysis engine. It allows to subtly identifying the text phrases which bear some opinion, to extract them, and to give them a note according to the polarity that they express. Finally, we present the perspectives that we plan to carry out.</abstract>
			<keywords>Opinion mining, e-reputation, information extraction</keywords>
		</article>
		<article id="taln-2013-demo-005" session="Démonstrations">
			<auteurs>
				<auteur>
					<nom>Patrick Séguéla</nom>
					<email>patrick.seguela@synapse-f.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Dominique Laurent</nom>
					<email>dlaurent@synapse-fr.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Synapse Développement, 33 rue Maynard, 31000 Toulouse</affiliation>
			</affiliations>
			<titre>Corriger, analyser et représenter le texte Synapse Développement</titre>
			<type>démonstration</type>
			<pages>795-796</pages>
			<resume>Synapse Développement souhaite échanger avec les conférenciers autour des technologies qu'elle commercialise : correction de textes et analyse sémantique. Plusieurs produits et démonstrateurs seront présentés, notre but étant d'instaurer un dialogue et de confronter notre approche du TAL, à base de méthodes symboliques et statistiques influencées par des contraintes de production, et celles utilisées par les chercheurs, industriels ou passionnés qui viendront à notre rencontre.</resume>
			<mots_cles>Correction grammaticale, analyse syntaxique, analyse sémantique, analyse d'opinions</mots_cles>
			<title>Checking, analysing and representing texts</title>
			<abstract>Synapse Développement would like to demonstrate its grammar checker and semantic analysis technologies to open exciting discussions with natural language specialists. We are particularly interested in discussing the scientific issues we have to face and solve according to our industrial needs.</abstract>
			<keywords>Grammar checker, POS tagging, semantic analysis, opinion mining</keywords>
		</article>
		<article id="taln-2013-demo-006" session="Démonstrations">
			<auteurs>
				<auteur>
					<nom>Xavier Tannier</nom>
					<email>Xavier.Tannier@limsi.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Véronique Moriceau</nom>
					<email>Veronique.Moriceau@limsi.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>2</affiliationId>
				</auteur>
				<auteur>
					<nom>Erwan Le Flem</nom>
					<email>Erwan.LeFlem@limsi.fr</email>
					<affiliationId>1</affiliationId>
					<affiliationId>3</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIMSI-CNRS</affiliation>
				<affiliation affiliationId="2">Université Paris-Sud, 91403 Orsay, France</affiliation>
				<affiliation affiliationId="3">IUT de Vannes</affiliation>
			</affiliations>
			<titre>Une interface pour la validation et l’évaluation de chronologies thématiques</titre>
			<type>démonstration</type>
			<pages>797-798</pages>
			<resume>Cet article décrit une interface graphique de visualisation de chronologies événementielles construites automatiquement à partir de requêtes thématiques en utilisant un corpus de dépêches fourni par l’Agence France Pressse (AFP). Cette interface permet également la validation des chronologies par des journalistes qui peuvent ainsi les éditer et les modifier.</resume>
			<mots_cles>chronologie événementielle, évaluation, validation</mots_cles>
			<title>An Interface for Validating and Evaluating Thematic Timelines</title>
			<abstract>This demo paper presents a graphical interface for the visualization and evaluation of event timelines built automatically from a search query on a newswire article corpus provided by the Agence France Pressse (AFP). This interface also enables journalists to validate chronologies by editing and modifying them.</abstract>
			<keywords>event timeline, evaluation, validation</keywords>
		</article>
		<article id="taln-2013-demo-007" session="Démonstrations">
			<auteurs>
				<auteur>
					<nom>Denis Maurel</nom>
					<email>denis.maurel@univ-tours.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Nathalie Friburger</nom>
					<email>nathalie.friburger@univ-tours.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université François Rabelais Tours</affiliation>
			</affiliations>
			<titre>CasSys Un système libre de cascades de transducteurs</titre>
			<type>démonstration</type>
			<pages>799-800</pages>
			<resume>CasSys est un système de création et de mise en oeuvre de cascades de transducteurs intégré à la plateforme Unitex. Nous présentons dans cette démonstration la nouvelle version implantée fin 2012. En particulier ont été ajoutées une interface plus conviviale et la possibilité d’itérer un même transducteur jusqu’à ce qu’il n’ait plus d’influence sur le texte. Un premier exemple concernera le traitement de texte avec une gestion complexe de balises XML et un deuxième présentera la cascade CasEN de reconnaissance des entités nommées.</resume>
			<mots_cles>cascade de transducteurs, graphes Unitex, texte avec balises XML, reconnaissance d'entités nommées</mots_cles>
			<title>CasSys, a free transducer cascade system</title>
			<abstract>CasSys is a free toolkit integrated in the Unitex platform to create and use transducer cascades. We are presenting the new version implemented at the end of 2012. The system interface has been improved and the Kleen star operation has been added: this operation allows applying the same transducer until it no longer produces changes in the text. The first example deals with complex XML text parsing and the second with CasEN, a free cascade for French Named Entity Recognition.</abstract>
			<keywords>transducer cascade, Unitex graphs, XML text, French Named Entity Recognition</keywords>
		</article>
		<article id="taln-2013-demo-008" session="Démonstrations">
			<auteurs>
				<auteur>
					<nom>Lingxiao Wang</nom>
					<email>Lingxiao.Wang@imag.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Ying Zhang</nom>
					<email>Ying.Zhang@imag.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">GETALP - LIG, 41 rue des Mathématiques, BP 53, 38041 Grenoble Cedex 9</affiliation>
			</affiliations>
			<titre>iMAG : post-édition, évaluation de qualité de TA et production d'un corpus parallèle</titre>
			<type>démonstration</type>
			<pages>801-802</pages>
			<resume>Une passerelle interactive d’accès multilingue (iMAG) dédiée à un site Web S (iMAG-­‐S) est un bon outil pour rendre S accessible dans beaucoup de langues, immédiatement et sans responsabilité éditoriale. Les visiteurs de S ainsi que des post-­‐éditeurs et des modérateurs payés ou non contribuent à l’amélioration continue et incrémentale des segments textuels les plus importants, et éventuellement de tous. Dans cette approche, les pré-­‐traductions sont produites par un ou plusieurs systèmes de Traduction Automatique (TA) gratuits. Il y a deux effets de bord intéressants, obtenables sans coût additionnel : les iMAGs peuvent être utilisées pour produire des corpus parallèles de haute qualité, et pour mettre en place une évaluation permanente et finalisée de multiples systèmes de TA.</resume>
			<mots_cles>post-édition, évaluation de systèmes de TA, production d’un corpus parallèle</mots_cles>
			<title>iMAG : MT-postediting, translation quality evaluation and parallel corpus production</title>
			<abstract>An interactive Multilingual Access Gateway (iMAG) dedicated to a web site S (iMAG-­‐S) is a good tool to make S accessible in many languages immediately and without editorial responsibility. Visitors of S as well as paid or unpaid post-­‐editors and moderators contribute to the continuous and incremental improvement of the most important textual segments, and eventually of all. In this approach, pre-­‐translations are produced by one or more free machine translation systems. There are two interesting side effects obtainable without any added cost: iMAGs can be used to produce high-­‐quality parallel corpora and to set up a permanent task-­‐based evaluation of multiple MT systems.</abstract>
			<keywords>post-edition, evaluation of MT systems, production of parallel corpora</keywords>
		</article>
		<article id="taln-2013-demo-009" session="Démonstrations">
			<auteurs>
				<auteur>
					<nom>David Rouquet</nom>
					<email>david.rouquet@imag.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LIG-GETALP</affiliation>
			</affiliations>
			<titre>Technologies du Web Sémantique pour l'exploitation de données lexicales en réseau (Lexical Linked Data)</titre>
			<type>démonstration</type>
			<pages>803-804</pages>
			<resume>Nous présentons un système basé sur les technologies du Web Sémantique pour la gestion, le développement et l'exploitation de données lexicales en réseau (Lexical Linked Data, LLD).</resume>
			<mots_cles>Lexical Linked Data, Lexique Multilingue, Pivot, Axies, Sparql, Spin</mots_cles>
			<title>Semantic Web technologies for Lexical Linked Data management</title>
			<abstract>We present a system based on Semantic Web technologies for Lexical Linked Data management.</abstract>
			<keywords>Lexical Linked Data, Multilingual Lexicon, Pivot, Axies, Sparql, Spin</keywords>
		</article>
		<article id="taln-2013-demo-010" session="Démonstrations">
			<auteurs>
				<auteur>
					<nom>Achille Falaise</nom>
					<email>achille.falaise@imag.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Université Grenoble Alpes, LIGGETALP, F38040 Grenoble</affiliation>
			</affiliations>
			<titre>Adaptation de la plateforme corporale ScienQuest pour l'aide à la rédaction en langue seconde</titre>
			<type>démonstration</type>
			<pages>805-806</pages>
			<resume>La plateforme ScienQuest fut initialement créée pour l'étude linguistique du positionnement et du raisonnement dans le corpus Scientext. Cette démonstration présente les modifications apportées à cette plateforme, pour en faire une base phraséologique adaptée à l'aide à la rédaction en langue seconde. Cette adaptation est utilisée dans le cadre de deux expérimentations en cours : l'aide à la rédaction en anglais pour les scientifiques, et l'aide à la rédaction académique en français pour les apprenants.</resume>
			<mots_cles>Aide à la rédaction, langue seconde, ScienQuest, Scientext</mots_cles>
			<title>Adaptation of the corpus platform ScienQuest for assistance to writing in a second language</title>
			<abstract>The ScienQuest platform was initially created for the linguistic study of positioning and reasoning in the Scientext corpus. This demonstration introduces modifications to this platform, transforming it into a phraseological database adapted for assistance to writing in a second language. This adaptation is used as part of two ongoing experiments: an assistance to writing in English for scientists, and an assistance to academic writing in French for learners.</abstract>
			<keywords>Writing assistance, second language, ScienQuest, Scientext</keywords>
		</article>
		<article id="taln-2013-demo-011" session="Démonstrations">
			<auteurs>
				<auteur>
					<nom>Sebastián Peña Saldarriaga</nom>
					<email>sebastian.pena-saldarriaga@univ-nantes.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Damien Vintache</nom>
					<email>damien.vintache@univ-nantes.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Béatrice Daille</nom>
					<email>beatrice.daille @univ-nantes.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LINA, 44322 Nantes Cedex 03</affiliation>
			</affiliations>
			<titre>Démonstrateur Apopsis pour l’analyse des tweets</titre>
			<type>démonstration</type>
			<pages>807-808</pages>
			<resume>Le démonstrateur Apopsis permet de délimiter et de catégoriser les opinions émises sur les tweets en temps réel pour un sujet choisi par l’utilisateur au travers d’une interface web.</resume>
			<mots_cles>fouille d’opinion, polarité, twitter</mots_cles>
			<title>Apopsis Demonstrator for Tweet Analysis</title>
			<abstract>Apopsis web demonstrator detects and categorizes opinion expressions appearing in Twitter in real time thorigh a web interface.</abstract>
			<keywords>opinion mining, polarity, twitter</keywords>
		</article>
		<article id="taln-2013-demo-012" session="Démonstrations">
			<auteurs>
				<auteur>
					<nom>Frederik Cailliau</nom>
					<email>cailliau@sinequa.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Ariane Cavet</nom>
					<email>cavet@sinequa.com</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">Sinequa, 12 rue d’Athènes 75009 Paris</affiliation>
			</affiliations>
			<titre>L’analyse des sentiments au service des centres d’appels</titre>
			<type>démonstration</type>
			<pages>809-811</pages>
			<resume>Les conversations téléphoniques qui contiennent du sentiment négatif sont particulièrement intéressantes pour les centres d’appels, aussi bien pour évaluer la perception d’un produit par les clients que pour améliorer la formation des télé-conseillers. Néanmoins, ces conversations sont peu nombreuses et difficiles à trouver dans la masse d’enregistrements. Nous présentons un module d’analyse des sentiments qui permet de visualiser le déroulement émotionnel des conversations. Il se greffe sur un moteur de recherche, ce qui permet de trouver rapidement les conversations problématiques grâce à l’ordonnancement par score de négativité.</resume>
			<mots_cles>analyse des sentiments, conversations téléphoniques, recherche d’information, parole spontanée, parole conversationnelle</mots_cles>
			<title>Sentiment Analysis for Call-centers</title>
			<abstract>Phone conversations in which negative sentiment is expressed are particularly interesting for call centers, both to evaluate the clients’ perception of a product and for the training of the agents. However, these conversations are scarce and hard to find in the mass of the recorded calls. We present a module for sentiment analysis that allows the user to visualize the emotional course of each conversation. In combination with a search engine, a user can rapidly find the problematic calls using the ranking by negativity score.</abstract>
			<keywords>sentiment analysis, information retrieval, spontaneous speech, conversational speech</keywords>
		</article>
		<article id="taln-2013-demo-013" session="Démonstrations">
			<auteurs>
				<auteur>
					<nom>Béatrice Daille</nom>
					<email>beatrice.daille@univ-nantes.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
				<auteur>
					<nom>Rima Harastani</nom>
					<email>rima.harastani@univ-nantes.fr</email>
					<affiliationId>1</affiliationId>
				</auteur>
			</auteurs>
			<affiliations>
				<affiliation affiliationId="1">LINA, 44322 Nantes Cedex 03</affiliation>
			</affiliations>
			<titre>TTC TermSuite alignement terminologique à partir de corpus comparables</titre>
			<type>démonstration</type>
			<pages>812-813</pages>
			<resume>TermSuite est outil libre multilingue réalisant une extraction terminologique monolingue et une extraction terminologique bilingue à partir de corpus comparables.</resume>
			<mots_cles>corpus comparable, extraction terminologique, alignement, UIMA</mots_cles>
			<title>TTC TermSuite - Terminological Alignment from Comparable Corpora</title>
			<abstract>TermSuite is based on a UIMA framework and performs monolingual and bilingual term extraction from comparable corpora for a range of languages.</abstract>
			<keywords>comparable corpora, terminology extraction, terminology alignment, UIMA</keywords>
		</article>
	</articles>
</conference>
