
La préédition avec des régles eu coﬁteuses, utile pour la TA
statistique es forums ?

Iohanna Gerlachl, Victoria Porrol, Pierrette Bouillonl, Sabine Lehmannz
(1) UN1vERs1'rE DE GENEVE FTI/TIM - 40, bvd Du Pont-d'Arve, CH-1211 Genéve 4, Suisse
(2) ACROLINX GmbH, Friedrichstr. 100, 10117 Berlin, Allemagne
Johanna.Gerlach@unige.ch, Victoria.Porro@unige.ch,
Pierrette . Bouillon@unige . ch, Sabine . Lehmann@acrolinx . com

RESUME

Cet article s'intéresse 5 la traduction automatique statistique des forums, dans le cadre du
projet européen ACCEPT (« Automated Community Content Editing Portal »). Nous
montrons qu'il est possible d’écrire des régles de préédition peu coﬁteuses sur le plan des
ressources linguistiques et applicables sans trop d'effort avec un impact trés signiﬁcatif sur
la traduction automatique (TA) statistique, sans avoir 5 modiﬁer le systéme de TA. Nous
décrivons la méthodologie proposée pour écrire les régles de préédition et les évaluer, ainsi
que les résultats obtenus par type de régles.

ABSTRACT

Can lightweight pre-editing rules improve statistical MT of forum content?

This paper focuses on the statistical machine translation (SMT) of forums within the context
of the European Framework ACCEPT («Automated Community Content Editing Portal»)
project We demonstrate that it is possible to write lightweight pre-editing rules that require
few linguistic resources, are relatively easy to apply and have signiﬁcant impact on SMT
without any changes to the machine translation system. We describe methodologies for rule
development and evaluation, and provide results obtained for different rule types.

MOTS-CLES : préédition, langage contrélé, traduction statistique, forums
KEYWORDS : pre-edition, controlled language, statistical machine translation, forums

1 Introduction

Aujourd'hui, les textes communautaires (forums) jouent un r6le de plus en plus important

sur le Web. Ils restent cependant difficiles a traduire, en raison de spéciﬁcités, qui les
rendent plus proches de l’oral que de l'écrit (voir Figure 1).

La sa ne pose pas de probléme (du moins on ne recoit pas |'a|er1e).La dessus on est sur de
rien et c'est valable pour n'impor1e qu'e||e antivirus que j'ai put voir, contrairement aux dires.

FIGURE 1 — Exemples tirés de forums informatiques.

Le projet européen ACCEPT («Automated Community Content Editing PorTal »,
www.accept.unige.ch) tente de lever ce paradoxe et s'intéresse 5 trois méthodes pour
améliorer la traduction des forums, dont il cherche 5 mesurer l'impact respectif: la
préédition, la post-édition et des techniques issues de la TA statistique elle-méme (par
exemple, pour l'adaptation au domaine). Les forums utilisés dans le projet sont ceux des
partenaires: Symantec (forums informatiques, fr.communi1:y.norton.com) et Traducteurs
sans Frontiéres (textes médicaux). La traduction automatique se fait avec Moses (Koehn et
al., 2007) et la préédition/post-édition avec la plateforme linguistique d'Acrolinx
( ), l'un des logiciels les plus utilisés aujourd'hui pour le controle-qualité
de la documentation technique (Bredenkamp et al., 2000). Acrolinx est un logiciel de
validation semi-automatique. Pour Accept, il est accessible via un plug-1'11 aux membres de la
communauté, qui se chargent d'appliquer les regles de préédition et de post-édition, en vue
d’améliorer la qualité de la source et de la traduction.

Le travail décrit ici s'inscrit directement dans ce projet et se focalise sur la préédition en
francais pour les forums de Symantec : est-il possible d'écrire avec la plate-forme d'Acrolinx
des regles de préédition utiles pour la TA statistigue francais-anglais et avec guel impact?
Notre but ﬁnal est double: décrire les regles utiles ; mesurer ensuite s'il est possible
d'obtenir le méme gain avec d'autres méthodes (Cf. Rayner et al., 2012). Dans cet article,
nous décrivons d'abord la méthodologie utilisée pour déﬁnir les regles (Section 2). Nous
présentons ensuite les regles développées (Section 3). La derniere partie sera consacrée a
l'évaluation de l'impact des différentes regles sur la traduction (Sections 4 et 5). Un
probleme est d’évaluer les regles de maniere rapide et fiable, sans référence : nous
comparons les résultats obtenus avec des traducteurs et des juges recrutés avec Amazon
Mechanical Turk (AMT).

2 La préédition

La préédition revét des réalités tres différentes en traduction automatique (TA) : correction
orthographique et grammaticale ; normalisation lexicale du texte source (par exemple, Han
et Baldwin, 2011, Banerjee et al., 2012) ; langage controlé (O'Brien, 2003) ; regles de
réordonnancement (par ex., Wang et al., 2007, Genzel, 2010). De maniere générale, peu
d'outils de préédition s'intéressent a ces différents types de préédition en méme temps. Pour
des raisons en partie historiques, le langage controlé a été d’avantage associé a la TA
linguistique, par regles (Pym, 1988, Bernth et Gdaniec, 2002, O'Brien et Roturier, 2007,
etc.) (a l’exception de Aikawa et al. 2007) ; en revanche, la correction orthographique, la
normalisation lexicale et les regles de réordonnancement ont toujours fait partie intégrante
de la TA statistique. Dans ce travail, dans une optique plus éclectique, nous avons développé
des regles des quatre types vus plus haut, qui répondent aux criteres suivants :

o Elles se focalisent sur quatre phénomenes qui ont un impact clair sur la TA statistique
des forums, a savoir les problemes de confusion entre mots (liés aux homophones), la
langue informelle et familiere (généralement absente des données d'entrainement), la
ponctuation et les différences syntaxiques entre le francais et l’anglais.

a Les fautes doivent pouvoir étre détectées avec les regles d'Acrolinx. Celles-ci sont
décrites avec un langage de patrons, qui repose sur un étiquetage syntaxique des textes
(Bredenkamp, 2000). Ceci a évidemment des conséquences sur le type de regles
développées: il est difﬁcile de détecter/corriger avec précision les fautes non-locales.
Par contre, les regles sont facilement portables dans d'autres outils puisqu'elles
nécessitent tres peu de ressources linguistiques.

a Les premiers tests a Symantec ont montré que la communauté des utilisateurs des
forums Symantec ne semble pas disposée a passer beaucoup de temps sur la préédition.
La précision est donc plus importante que le rappel et il est important que l'outil de
préédition produise des suggestions de corrections, si possible uniques.
A défaut de données post-éditées qui permettraient d'identiﬁer automatiquement les
phénomenes mal traduits, les regles ont toutes été définies manuellement avec Acrolinx, qui
offre une plate-forme complete pour développer, déboguer et tester les regles sur des corpus
(Bredenkamp et al., 2000). Développer une regle implique de passer par les étapes suivantes
(Figure 2) : 1) identiﬁer une regle a priori utile pour la TA statistique, par exemple ‘éviter
« si... et que... »' ; 2) définir un ou plusieurs patrons (« Trigger ») correspondants sous forme
d'eXpression réguliere pour identiﬁer le phénomene dans les textes; 3) proposer une
transformation (« Suggestion ») plus traduisible, ici remplacer « que » par « si » pour obtenir
« si... et si... »; 4) appliquer la regle sur le corpus de test et traduire les phrases prééditées et
non prééditées de maniere a produire le ﬁchier-résultat de la Figure 3 ; 5) vérifier les
résultats et ajouter si nécessaire des exceptions pour bloquer la regle dans certains cas
(« Negative evidence»). La Figure 2 résume la regle « si... et  que ».

Conjonction « si » suivie de 2 £1 15 mots puis de la oonjonction « et » et

Patron (Trigger) : @oonj |]{2-15} ‘et’ ‘que’ de « que »

Suggestion : ‘que’ -> @oonj « que » est rerrplaoé par la oonjonction « si »

Exception (Negative evidence) :que |]+
@<>0ni ll’ que

Bloquer la regle si la oonjonction « si » est préoedée par « que » + un ou plusieurs
mots.

FIGURE 2 — Regle Acrolinx « si et  que  »

Source1 Source2 Trans|ation1 Trans|alion2

Si ton probleme est résolu et
que tout marche bien, pense
é supprimer tes points de

Si ton probleme est résolu et
si tout marche bien, pense é
supprimer tes points de

If your problem is solved and
that everything is working
well, think to remove your

If your problem is solved and
if all goes well, think to
remove your restore points.

restauration. restauration.

restore points.

FIGURE 3 — Extrait du ﬁchier résultat

Deux ressources se sont révélées particulierement utiles pour alimenter les regles : les mots
inconnus du systeme statistique (OOV), extraits des données-test avec Moses sur la base des
corpus-test, qui sont des bons indicateurs de ce qui n’est pas couvert par les données
d'entrainement (voir aussi Banerjee et al., 2012) et des listes des bigrammes/trigrammes
fréquents dans le corpus de test, mais absents de celui d’entrainement, avec la traduction des
phrases correspondantes, qui sont souvent des segments mal traduits. Dans la suite, nous
décrivons l'ensemble de regles développé suivant cette méthodologie.

3 Les régles développées

Notre but est donc de développer des regles utiles pour la TA statistique qui suivent les
criteres déﬁnis dans la Section 2. Celles-ci peuvent étre classées en fonction de différentes
dimensions, dont nous mesurerons l'impact dans la suite : regles pour les humains qui
améliorent le texte source ou regles pour la TA uniquement; regles automatiques ou
manuelles ; catégorie de regles ; regles avec une ou plusieurs suggestions ou sans suggestion.
Acrolinx permet de déﬁnir aisément des ensembles de regles et des ordres d’application
différents. Pour Symantec, les regles ont été regroupées en trois ensembles, destinés a étre
utilisés en séquence dans leur plug-1'11 de préédition (www.accept-portal.eu, Roturier et al.,
2012) et a réduire le plus possible le role des utilisateurs :
o Le premier ensemble (Ensemble 1) comprend les regles automatiques, qui doivent étre
appliquées en premier lieu, pour limiter le bruit dans les autres ensembles (2 et 3). Il
inclut la plupart des regles pour la confusion de mots de différentes catégories
syntaxiques, liée aux homophones, et gere également la ponctuation et l’élision, par
exemple : Merci beaucoupje fais sa de suite 9 Merci beaucoup, je fais pa de suite.

o Le deuxieme (Ensemble 2) comprend les regles avec plusieurs suggestions ou sans
suggestion ou l’utilisateur doit nécessairement intervenir pour le controle. Cet ensemble
inclut les regles de grammaire pour l’accord et la confusion des temps /modes, ainsi que
les regles de style, en particulier pour éviter le langage familier et informel (questions
directes, phrases clivées, mots familiers, troncations, etc.), par exemple : Tu aslu 1e tuto
5ur1e forum? 9 As-tu Iu 1e tut'on'eI5ur1e forum .7

o Finalement, le troisieme (Ensemble 3) regroupe les regles automatiques pour la TA, qui
n'améliorent pas nécessairement la qualité du texte source. Celles-ci modiﬁent l'ordre des
mots pour les rendre plus proches de l'anglais ou pour éviter des ambigu'1'tés (/e te 1e
domze en piece jointe 9 je te domze ca en piécejoi11te,'j’ai tvutpris 9 j’ai pris tout) ou
encore transforment des mots ou expressions mal traduits dans un équivalent plus
traduisible («ne »  « que » 9 uniquement; « soit»  « soit» 9 ou, etc.). Une des regles
automatiques convertit la deuxieme personne du singulier informelle dans le
correspondant formel, beaucoup plus fréquent dans les données d'entrainement (Rayner
et al., 2012) : As-tu 11: 1e tutoriel 5ur1e forum? 9 Avez-Vous Iu 1e tutoriel 5ur1e forum .7

La suite se focalise sur l'évaluation de ces différentes regles sur des textes extraits des
forums de Symantec. Nous décrivons d’abord la méthodologie de l'évaluation, puis discutons
les résultats.

4- Méthodologie de l’évaluation

4.1 Sélecﬁon des données

Aﬁn de constituer un corpus représentatif, nous avons sélectionné 10 000 phrases des
données fournies par Symantec, sur la base des mots et des bigrammes fréquents dans
l'ensemble des données, en conservant les mémes proportions de phrases de chaque
longueur. Pour simuler l'utilisation des regles décrite en 3, les trois ensembles de regles ont
été appliqués en séquence, chacun prenant en entrée le corpus entier avec les corrections de
l'étape précédente.

Pour chaque ensemble, les regles ont donc été appliquées une par une et les phrases
corrigées, en suivant les suggestions de correction proposées par nos regles. Toujours aﬁn de
simuler l'utilisation prévue, cette étape differe selon les ensembles de regles: pour les
premier et troisieme ensembles, les corrections ont été appliquées automatiquement, sans
vériﬁcation de la suggestion; pour le deuxieme, manuellement. Par conséquent, pour les
ensembles un et trois, la correction a pu produire des erreurs, vu que la précision des regles
n’est pas parfaite.

Finalement, les deux sources, brutes et prééditées, ont été traduites en anglais avec le
systeme de TA statistique développé avec Moses dans le cadre du projet ACCEPT (Accept
Deliverable D4.1, 2012). Celui-ci a été entrainé avec les données d'Europarl
(http://www.statmt.org/wmt12/), ainsi que des manuels techniques de Symantec. A titre
indicatif, le score Bleu est de 42.41 sur un extrait de 500 phrases. Pour l'évaluation, 50
phrases maximum par regle ont été retenues (soit un total de 1 733 phrases), d’ou ont étés
extraites toutes les phrases ou les cibles étaient différentes (soit un total de 1 364 phrases).

4.2 Annotation

Pour évaluer l'impact des regles sur la traduction, nous avons opté pour une évaluation
humaine comparative de la traduction des phrases brutes et prééditées. Nous présentons
donc aux évaluateurs des groupes de phrases du type {source, traduct1'o11_1 brute,
traduct7'o11_2 de la phrase prééditée}, ou traduct1'o11_1 et traduct1'o11_2 apparaissent dans un
ordre aléatoire, avec les différences marquées en couleur (Figure 4). Pour les premier et
deuxieme ensembles de regles, la phrase source correspond a la phrase prééditée. Pour le
troisieme, il s'agit de la source non prééditée, puisque le résultat de la modiﬁcation pourrait
ne pas étre du francais correct, par exemple suite a un réordonnancement. Les évaluateurs
doivent ensuite choisir l'un des cinq jugements comparatifs : « first clearly better», «first
slightly better », « about tbe same », « second slightly better », « second clearly better ».

Nous avons eu recours a deux types d’évaluateurs, aﬁn de comparer les résultats : d’une part,
des travailleurs recrutés sur Amazon Mechanical Turk (AMT), d'autre part des traducteurs
de langue maternelle anglaise en ﬁn de formation a la Faculté de Traduction et
d'Interprétation (FTI) de l’Université de Geneve. Comme dans une précédente étude (Rayner
et al., 2012), nous avons imposé les restrictions suivantes aux travailleurs AMT: 1) qu'ils
soient de langue maternelle anglaise et qu'ils résident au Canada, aﬁn d'augmenter la
probabilité qu'ils soient bilingues anglais-francais, et 2) qu'ils aient un historique de travail
ﬁable sur AMT. Pour les évaluateurs ‘traducteurs’, qui n'ont pas acces a la plate-forme AMT,
actuellement limitée aux résidents des Etats-Unis/Canada, nous avons développé une
application Windows aﬁn qu'ils puissent effectuer l'évaluation dans des conditions similaires
que sur AMT. Tous les évaluateurs ont été payés pour la tache.

OWN‘ yen ouruclus que eela vlenl de oerlams sues, car méme quamd J9 le crée il ne veun
pas rempllr les champs, Je suppose que l'on me peul new lave ?

H'3“'3”5‘3“”” l conclude Ihal II has some slles, because even when l cr-eales II dues mcl llllfll

rm 1 when
the ﬁelds. l assume lhal we can do anylllmmg'7 I W3 H

cm illglvtly better
album the same
second slightly better

5?W"d"E”5'3“=‘” l conclude lhal II has some slles. because even when l creales II dues mcl lulﬁl
the ﬁelds, l assurnelhalyuw cannuldn anylhvng?

secund clearly beiler

FIGURE 4 — Interface d’évaluation dans notre application
Nous avons ainsi récolté 6 jugements pour chaque groupe de phrases, 3 par des travailleurs
AMT, 3 par des traducteurs. Les résultats seront discutés dans la section suivante.

5 Résultats

5.1. Résultats par ensemble et par type de juges

Le Tableau 1 présente les résultats globaux pour les trois ensembles de regles et les deux
types de juges (AMT, traducteurs). Pour chaque ensemble, nous avons calculé le pourcentage
d'application des regles sur 10000 phrases et, pour les données d’évaluation (1733 phrases),
la précision (nombre de suggestions correctes sur l'ensemble des suggestions) et l'impact
des régles sur la traduction. Ce dernier a été mesuré en regroupant les catégories
first/second «sligbtly better» et «clearly better» pour obtenir les deux catégories «Raw
better/Pre-edited better» et en gardant le jugement majoritaire pour chacun des groupes de
juges. Le caractere signiﬁcatif des résultats a été calculé avec le test de McNemar, en
comparant les résultats des catégories « Rawbetter» et « Pre-edited better».

as .5 § 5 § as E x g E as '§ -"5 s <% E as § ~§ 8
E 3 as B E O 3 “I, g E 0; E C v
8 if E 2 g 9 E 2 3 ‘ % “
Ensemble 1 (precision = 91%)
AMT 42% 61 1 183 30% 59 10% 50 8% 297 49% 22 4% pos OUI
trad. 42% 611 183 30% 56 9% 85 14% 258 42% 29 5% pos oui

Ensemble 2 (precision = 88%)
AMT 20% 674 103 15% 114 17% 37 6% 393 58% 27 4% pos oui
trad. 20% 674 103 15% 109 16% 73 11% 360 53% 29 4% pos oui
Ensemble 3 (precision = 98%)
AMT 36% 448 83 19% 77 17% 28 6% 239 53% 21 5% pos oui
trad. 36% 448 83 19% 69 15% 53 12% 224 50% 19 4% pos oui

TABLEAU 1 — Résultats par ensemble et par type de juges

Les résultats montrent qu'il est possible d'arriver a une précision élevée (entre 98% pour
l’Ensemble 3 et 88% pour l’Ensemble 2) et que les trois ensembles de régles ont un impact
statistiquement signiﬁcatif sur la traduction (p<0,05), méme s'ils ont été appliqués en
séquence et partiellement automatiquement. Ces résultats sont trés proches pour les trois
ensembles de régles et les deux types de juges (AMT/traducteurs). Une comparaison plus
approfondie des résultats avec les juges AMT et traducteurs montre cependant que ces
derniers ont été plus rapides et arrivent plus souvent a un jugement majoritaire ou unanime
(Tableau 2). Les deux groupes convergent vers le méme jugement majoritaire dans 75% des
cas. Le coefﬁcient de concordance kappa (calculé sur la base des jugements majoritaires
pour chacun des deux groupes de juges) est de 0,53.

Evaluateurs Temps moyen pour |’éva|uation de 20 Aooord observé
phrases (minutes)

AMT 06 :13 82%

traducteurs O4 :09 89%

TABLEAU 2 — Temps et accord entre types de juge

Notre conclusion est donc qu'il est tout a fait possible de recourir a des juges AMT pour ce
type de tache assez simple, mais que les traducteurs restent plus efﬁcaces.

5.2. Résultats par catégorie d'erreurs

Nous avons ensuite mesuré l'impact sur la traduction par type de regles, en prenant cette
fois-ci le jugement majoritaire pour les 6 juges ensemble. Les catégories retenues sont, pour
les Ens_emhlg_s_1e_tZ, 1) ponctuation (y compris les régles pour l’élision), 2) grammaire
(accord), 3) grammaire (autres), avec les regles qui régissent l'utilisation des temps /modes,
4) homophones (avec toutes les regles qui traitent les confusions au niveau des catégories
syntaxiques), 5) style informel et pour  , 1) clitiques (avec toutes les régles de
réordonnancement/reformulation spéciﬁques aux clitiques), 2) reformulation
(remplacement d’une expression par une autre), 3) ordre des mots (pour les phénoménes
autres que les clitiques) et 4) tu-vous (regle qui remplace la deuxieme personne informelle
par la deuxieme personne formelle) (cf. Section 3).

Le Tableau 3 présente les résultats sur 10 000 phrases, regroupés par catégorie, triés d’apres
le nombre de cas marqués. Nous voyons que nous obtenons une amélioration significative de
la traduction pour toutes les catégories, sauf pour grammaire (autres). Cette derniere a un
impact négatif car elle tend a générer des données non couvertes par le systéme (avec un
subjonctif ou conditionnel, par exemple, a la place de l'indicatif). Parmi les catégories a
impact positif, les plus signiﬁcatives concernent la ponctuation, les confusions de mots dues
aux homophones et a la langue informelle; la moins signiﬁcative est la catégorie ordre,
comme nous l'avions déja constaté sur des données antérieures (Accept Deliverable D2.1,
2012) : les regles de réordonnancement s’averent en effet assez fragiles, si elles ne sont pas
également appliquées aux données d'entrainement

w 3 ea 3 E *5 E 2 3 -~E‘‘E *5

C 0 ._=, .3 ‘i n.

E’ E g 25 g as 3 $ § E as E % $  as § «E
as :3 E 7:» ‘§ 3 5 § ‘” g =' 3 §’ <1 a’

,_ _ -_ ._.

ponctuation 3796 416 147 35% 33 8% 32 8% 184 44% 20 5% 2.4E-24 oui
tu 1968 50 20 40% 3 6% 4 8% 21 42% 2 4% 5.2E-O4 oui
clitiques 1206 150 32 21% 21 18% 14 9% 69 46% 8 5% 2.9E-O5 oui
infonnel 971 367 42 1 1% 67 18% 19 5% 216 59% 23 6% 1 .4E-18 oui
homophones 659 323 55 17% 38 12% 33 10% 185 51% 12 4% 1.4E-22 oui
grammaire
(aooord) 591 150 32 21 % 22 15% 1 1 7% 82 55% 3 2% 7.2E-09 oui
refonnulation 177 177 19 1 1% 26 15% 8 5% 115 65% 9 5% 1.3E-13 oui
ordre 71 71 12 17% 17 24% 3 4% 34 48% 5 7% 2.5E-O2 oui
grammaire no
(autres) 36 28 9 32% 9 32% 2 7% 7 25% 1 4% 8.0E-O1 n

TABLEAU 3 — Résultats par catégorie
6 Conclusion

Dans cet article, nous avons montré qu'il est possible d’écrire, pour un domaine, des regles
automatiques de préédition peu coﬁteuses sur le plan des ressources linguistigues et
applicables sans trop d'effort avec un impact tres sig1_1iﬁcatif sur la TA statistique, sans avoir
a modifier le systeme statistique. L’évaluation peut se faire avec des juges AMT, qui
conviennent bien pour la tache d'évaluation proposée ici.

L'étape suivante sera de montrer l'impact de ces regles appliquées ensemble et de voir
comment les regles sont utilisées par la communauté Symantec. Nous voulons également
vériﬁer si l’effort de post-édition diminue avec les regles de préédition (Aikawa et al. 2007).
Comme mentionné dans l'introduction, l'un des objectifs du projet ACCEPT est de voir s'il est
possible d'obtenir le méme gain avec d'autres méthodes. Une étude faite en parallele (Rayner
et al., 2012) a montré qu'il est plus efﬁcace d’appliquer la regle « tu-vous » qui transforme la
deuxieme personne du singulier informelle en deuxieme personne du singulier formelle lors
de la préédition que de générer, avec la méme regle inversée, des données d'entrainement
avec la deuxieme personne informelle. Nous comptons tester de la méme maniere les autres
types de regles.
Références

Accept Deliverable D4.1 (2 0 12), http: / /www.accept.unige.ch / Products /
Accept Deliverable D2.1 (2 01 2), http: / /www.accept.unige.ch / Products /

AIKAWA, T., ScHwAR'I'z, L., KING, R., CORSTON-OLIVER, M., & LOZANO, C. (2007). Impact of
controlled language on translation quality and post-editing in a statistical machine
translation environment. In Proceedings ofMT.S'ummitXI, Copenhagen, Denmark.

BANERJEE, P., NASKAR, ROTURIER, ]., WAY, A. &vAN GENABITH ]. (2012). Domain Adaptation in SMT
of User-Generated Forum Content Guided by OOV Word Reduction: Normalization and/or
Supplementary Data? In Proceedings ofEAM7I Trento.

BERNTH, A., & GDANIEC, C. (2 002). MTranslatability. In Macliine Translation 1 6, pages 1 75-2 18.

BREDENKAMP, A., CRYSMANN, B., & PETREA, M. (2000). Looking for Errors : A Declarative
Formalism for Resource-Adaptive Language Checking. In Proceedings of LREC 2000. Athens,
Greece.

GENZEL, D. (2010). Automatically learning source-side reordering rules for large scale
machine translation. In Proceedings of the 23rd International Conference on Computational
Linguistics, Beijing, China.

HAN, B. & BALDWIN, T (2011). Lexical Normalisation of Short Text Messages: Makn Sens a
#twitter. In Proceedings of the 49tl1 Annual Meeting of the Association for Computational
Linguistics: Human Language Teclznologies.

KOEHN, P. ETAL. (2007). Moses: open source toolkit for statistical machine translation. In ACL-
2007: Proceedings of demo and poster sessions, Prague, Czech Republic, pp. 177-180.

O'BRIEN, SH. (2003). Controlling controlled English: An Analysis of Several Controlled
Language Rule Sets. In EAMT-CLA W-03, Dublin City University, pages 105-114.

O'BRIEN, SH. & ROTURIER, ]. (2007). How Portable are Controlled Languages Rules? A
Comparison of Two Empirical MT Studies. In MT.S'ummitXI, Copenhagen, pages 105-114.

PYM, P. ]. (1988). Pre-editing and the use of simpliﬁed writing for MT: an engineer's
experience of operating an MT system. In Translatingand the Computer 10.

RAYNER, M., BOUILLON, P. & HADDOW, B. (2012). Using Source-Language Transformations to
Address Register Mismatches in SMT . In Proceedings of the Conference of the Association
forMacl1ine Translation in the Americas (AMTA), San Diego, USA.

ROTURIER, ]., MITCHELL, L., GRABOWSKI, R., SIEGEL, M. (2012). Using Automatic Machine
Translation Metrics to Analyze the Impact of Source Reformulations. In Proceedings of the
Conference of the Association for Macliine Translation in the Americas (AMTA), October
2012, San Diego, USA.

WANG, CH., COLLINS, M. & KOEHN, PH., Chinese Syntactic Reordering for Statistical Machine
Translation. In Proceedings of the 2007 joint Conference on Empirical Metliods in Natural
Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pp.
737-745.

