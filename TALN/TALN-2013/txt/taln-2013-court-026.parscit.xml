<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Sumeet Agarwal</author>
<author>Shantanu Godbole</author>
<author>Diwakar Punjani</author>
<author>Shourya Roy</author>
</authors>
<title>How much noise is too much : A study in automatic text classification.</title>
<date>2007</date>
<booktitle>In Data Mining,</booktitle>
<pages>3--12</pages>
<publisher>IEEE,</publisher>
<contexts>
<context position="9960" citStr="[2, 9, 1, 4]" startWordPosition="1532" endWordPosition="1535">0). Pour les CV académiques, nous utiliserons le corpus CVac comme référence afin de tester le correcteur. Ainsi nous avons ajouté du bruit à l’ensemble CVac suivant la même procédure qu’auparavant. Les correcteurs utilisent tous les ngrammes générés avec les CV commerciaux plus les documents de l’ensemble T afin de debruiter les CV de CVac. La correction d’erreurs est une tâche généralement abordée dans la reconnaissance optique de caractères (OCR) ou dans le traitement d’information informelle, comme les blogs, les forums, les SMS ou les tchats. Les travaux concernant la correction de bruit [2, 9, 1, 4] traitent la correction 1. Grâce à la codification homogène des éditeurs (Word, Libre/OpenOffice) 2. Nous considérons un mot comme l’ensemble de caractères entre deux espaces 709 c ATALA TALN-RÉCITAL 2013, 17-21 Juin, Les Sables d’Olonne de fautes d’orthographe et grammaticales, la mauvais ponctuation ou l’utilisation d’abréviations. Mais le problème spécifique des blancs a été peu traité à notre connaissance. Pour résoudre ce problème, nous proposons deux stratégies à base de n-grammes de caractères : un correcteur binaire et un autre probabiliste. 4.1 Correcteur binaire L’algorithme utilise</context>
</contexts>
<marker>[1]</marker>
<rawString>Sumeet Agarwal, Shantanu Godbole, Diwakar Punjani, and Shourya Roy. How much noise is too much : A study in automatic text classification. In Data Mining, 2007. ICDM 2007. Seventh IEEE International Conference on, pages 3–12. IEEE, 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Clark</author>
</authors>
<title>Pre-processing very noisy text.</title>
<date>2003</date>
<booktitle>In Proc. of Workshop on Shallow Processing of Large Corpora,</booktitle>
<pages>12--22</pages>
<contexts>
<context position="9960" citStr="[2, 9, 1, 4]" startWordPosition="1532" endWordPosition="1535">0). Pour les CV académiques, nous utiliserons le corpus CVac comme référence afin de tester le correcteur. Ainsi nous avons ajouté du bruit à l’ensemble CVac suivant la même procédure qu’auparavant. Les correcteurs utilisent tous les ngrammes générés avec les CV commerciaux plus les documents de l’ensemble T afin de debruiter les CV de CVac. La correction d’erreurs est une tâche généralement abordée dans la reconnaissance optique de caractères (OCR) ou dans le traitement d’information informelle, comme les blogs, les forums, les SMS ou les tchats. Les travaux concernant la correction de bruit [2, 9, 1, 4] traitent la correction 1. Grâce à la codification homogène des éditeurs (Word, Libre/OpenOffice) 2. Nous considérons un mot comme l’ensemble de caractères entre deux espaces 709 c ATALA TALN-RÉCITAL 2013, 17-21 Juin, Les Sables d’Olonne de fautes d’orthographe et grammaticales, la mauvais ponctuation ou l’utilisation d’abréviations. Mais le problème spécifique des blancs a été peu traité à notre connaissance. Pour résoudre ce problème, nous proposons deux stratégies à base de n-grammes de caractères : un correcteur binaire et un autre probabiliste. 4.1 Correcteur binaire L’algorithme utilise</context>
</contexts>
<marker>[2]</marker>
<rawString>Alexander Clark. Pre-processing very noisy text. In Proc. of Workshop on Shallow Processing of Large Corpora, pages 12–22, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jérémy Clech</author>
<author>Djamel A Zighed</author>
</authors>
<title>Data mining et analyse des cv : une expérience et des perspectives.</title>
<date>2003</date>
<booktitle>In Extraction et la Gestion des Connaissances, EGC’03,</booktitle>
<pages>189--200</pages>
<contexts>
<context position="3907" citStr="[6, 7, 3]" startWordPosition="581" endWordPosition="583">propre à chaque individu et difficile à modéliser. Nous nous situons dans la double perspective d’emplois académiques et commerciaux. L’employeur est ici une institution (université, grande école, centre de recherche) ou une entreprise, et les candidats présentant des dossiers adaptés pour correspondre au mieux aux profils recherchés. Donc, nous projetons de concevoir un système intégral d’analyse des candidatures académiques ou commerciales, dont la première étape consiste dans le découpage des CV des candidats. La problématique qui aborde SegCV est plus générale que celle étudiée auparavant [6, 7, 3], car ces travaux analysent seulement des CV commerciaux. SegCV est composé des modules suivants : Extraction d’information à partir des CV en formats PDF, Word, Open Office, PS, DVI et RTF ; analyse des CV pour extraire les sections importantes. Cet article présente un système de découpage automatique des CV ainsi qu’une étude portant sur la correction d’erreurs lors de la transformation en format texte. Nous présentons en section 2 la stratégie mise en œuvre. En Section 3, sont décrits les corpus utilisés. Nous présentons, en Section 4, la méthode pour détecter et corriger les erreurs avec d</context>
</contexts>
<marker>[3]</marker>
<rawString>Jérémy Clech and Djamel A. Zighed. Data mining et analyse des cv : une expérience et des perspectives. In Extraction et la Gestion des Connaissances, EGC’03, pages 189–200, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lipika Dey</author>
<author>SK Mirajul Haque</author>
</authors>
<title>Opinion mining from noisy text data.</title>
<date>2009</date>
<journal>International Journal on Document Analysis and Recognition (IJDAR),</journal>
<volume>12</volume>
<issue>3</issue>
<pages>205--226</pages>
<contexts>
<context position="9960" citStr="[2, 9, 1, 4]" startWordPosition="1532" endWordPosition="1535">0). Pour les CV académiques, nous utiliserons le corpus CVac comme référence afin de tester le correcteur. Ainsi nous avons ajouté du bruit à l’ensemble CVac suivant la même procédure qu’auparavant. Les correcteurs utilisent tous les ngrammes générés avec les CV commerciaux plus les documents de l’ensemble T afin de debruiter les CV de CVac. La correction d’erreurs est une tâche généralement abordée dans la reconnaissance optique de caractères (OCR) ou dans le traitement d’information informelle, comme les blogs, les forums, les SMS ou les tchats. Les travaux concernant la correction de bruit [2, 9, 1, 4] traitent la correction 1. Grâce à la codification homogène des éditeurs (Word, Libre/OpenOffice) 2. Nous considérons un mot comme l’ensemble de caractères entre deux espaces 709 c ATALA TALN-RÉCITAL 2013, 17-21 Juin, Les Sables d’Olonne de fautes d’orthographe et grammaticales, la mauvais ponctuation ou l’utilisation d’abréviations. Mais le problème spécifique des blancs a été peu traité à notre connaissance. Pour résoudre ce problème, nous proposons deux stratégies à base de n-grammes de caractères : un correcteur binaire et un autre probabiliste. 4.1 Correcteur binaire L’algorithme utilise</context>
</contexts>
<marker>[4]</marker>
<rawString>Lipika Dey and SK Mirajul Haque. Opinion mining from noisy text data. International Journal on Document Analysis and Recognition (IJDAR), 12(3) :205–226, 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rémy Kessler</author>
<author>Nicolas Béchet</author>
<author>Mathieu Roche</author>
<author>Marc El-Bèze</author>
<author>Juan-Manuel Torres-Moreno</author>
</authors>
<title>Automatic profiling system for ranking candidates answers in human resources.</title>
<date>2008</date>
<booktitle>In OTM ’08</booktitle>
<pages>625--634</pages>
<location>Monterrey, Mexico,</location>
<contexts>
<context position="14187" citStr="[5]" startWordPosition="2258" endWordPosition="2258">âche principale de SegCV consiste à repérer, découper et regrouper les sections pertinentes des CV. À cette fin, on peut être tenté d’utiliser des méthodes d’apprentissage automatique, car on sait qu’elles donnent de très bons résultats sur les tâches de TALN. Mais l’apprentissage automatique nécessite une grande quantité de documents préalablement étiquetés. Or, nous ne disposons pas d’un grand corpus annoté manuellement. En conséquence, nous avons deux possibilités pour faire face à ce problème. La première consiste à faire un découpage à de tailles fixes (1/3, 2/3, etc.), comme proposé par [5], mais cette approche nous semble trop grossière. L’autre possibilité consiste à établir des règles de découpage. Notre objectif étant de découper les CV de la manière la plus fine possible, nous avons décidé d’utiliser des règles. À cette fin, nous avons suivi deux approches. La première est basée sur la structure du CV : les titres, les sous-titres ou les débuts des lignes avec un symbole délimitant une section. 94 expressions régulières composent ces règles. La deuxième approche essaie d’améliorer le découpage au moyen de mots-clés qui seront recherchés à l’intérieur des sections. Le découp</context>
</contexts>
<marker>[5]</marker>
<rawString>Rémy Kessler, Nicolas Béchet, Mathieu Roche, Marc El-Bèze, and Juan-Manuel Torres-Moreno. Automatic profiling system for ranking candidates answers in human resources. In OTM ’08 Monterrey, Mexico, pages 625–634, 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rémy Kessler</author>
</authors>
<title>Juan-Manuel Torres-Moreno, and Marc El-Bèze. E-Gen : Automatic Job Offer Processing system for Human Ressources. In</title>
<date>2007</date>
<booktitle>MICAI,</booktitle>
<pages>985--995</pages>
<contexts>
<context position="3907" citStr="[6, 7, 3]" startWordPosition="581" endWordPosition="583">propre à chaque individu et difficile à modéliser. Nous nous situons dans la double perspective d’emplois académiques et commerciaux. L’employeur est ici une institution (université, grande école, centre de recherche) ou une entreprise, et les candidats présentant des dossiers adaptés pour correspondre au mieux aux profils recherchés. Donc, nous projetons de concevoir un système intégral d’analyse des candidatures académiques ou commerciales, dont la première étape consiste dans le découpage des CV des candidats. La problématique qui aborde SegCV est plus générale que celle étudiée auparavant [6, 7, 3], car ces travaux analysent seulement des CV commerciaux. SegCV est composé des modules suivants : Extraction d’information à partir des CV en formats PDF, Word, Open Office, PS, DVI et RTF ; analyse des CV pour extraire les sections importantes. Cet article présente un système de découpage automatique des CV ainsi qu’une étude portant sur la correction d’erreurs lors de la transformation en format texte. Nous présentons en section 2 la stratégie mise en œuvre. En Section 3, sont décrits les corpus utilisés. Nous présentons, en Section 4, la méthode pour détecter et corriger les erreurs avec d</context>
</contexts>
<marker>[6]</marker>
<rawString>Rémy Kessler, Juan-Manuel Torres-Moreno, and Marc El-Bèze. E-Gen : Automatic Job Offer Processing system for Human Ressources. In MICAI, pages 985–995, 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rémy Kessler</author>
</authors>
<title>Juan-Manuel Torres-Moreno, and Marc El-Bèze. E-Gen : Profilage automatique de candidatures.</title>
<date>2008</date>
<booktitle>In TALN’08</booktitle>
<location>Avignon,</location>
<contexts>
<context position="3907" citStr="[6, 7, 3]" startWordPosition="581" endWordPosition="583">propre à chaque individu et difficile à modéliser. Nous nous situons dans la double perspective d’emplois académiques et commerciaux. L’employeur est ici une institution (université, grande école, centre de recherche) ou une entreprise, et les candidats présentant des dossiers adaptés pour correspondre au mieux aux profils recherchés. Donc, nous projetons de concevoir un système intégral d’analyse des candidatures académiques ou commerciales, dont la première étape consiste dans le découpage des CV des candidats. La problématique qui aborde SegCV est plus générale que celle étudiée auparavant [6, 7, 3], car ces travaux analysent seulement des CV commerciaux. SegCV est composé des modules suivants : Extraction d’information à partir des CV en formats PDF, Word, Open Office, PS, DVI et RTF ; analyse des CV pour extraire les sections importantes. Cet article présente un système de découpage automatique des CV ainsi qu’une étude portant sur la correction d’erreurs lors de la transformation en format texte. Nous présentons en section 2 la stratégie mise en œuvre. En Section 3, sont décrits les corpus utilisés. Nous présentons, en Section 4, la méthode pour détecter et corriger les erreurs avec d</context>
</contexts>
<marker>[7]</marker>
<rawString>Rémy Kessler, Juan-Manuel Torres-Moreno, and Marc El-Bèze. E-Gen : Profilage automatique de candidatures. In TALN’08 Avignon, 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Craig Knoblock</author>
<author>Daniel Lopresti</author>
<author>Shourya Roy</author>
<author>L Venkata Subramaniam</author>
</authors>
<title>Special issue on noisy text analytics.</title>
<date>2007</date>
<journal>IJDAR,</journal>
<pages>10--3</pages>
<contexts>
<context position="5662" citStr="[8]" startWordPosition="855" endWordPosition="855">aque langue. Il transforme l’information des CV en blocs d’information selon des modèles définis par l’utilisateur, faciles à comprendre par les humains et exploitables par les machines. Les CV originaux sont déclinés en formats divers : .doc, .odt, .pdf, .ps, .txt, etc. Afin de pouvoir les traiter convenablement, les CV sont transformés en texte utf-8. Cependant, cette transformation n’est pas libre d’erreurs, surtout dans les fichiers issus de PDF. Nous considérons le bruit comme la différence entre la forme superficielle d’une représentation textuelle et le texte prévu, correct ou originel [8]. Si la source est PostScript ou PDF du LATEX, le texte extrait peut comporter un certain nombre d’erreurs. Les caractères accentués, la police utilisée et les petites majuscules sont des sources d’erreurs récurrentes et difficiles à modéliser. Or, les fichiers générés par LATEX risquent d’être très fréquents dans les CV issus du milieu académique. Cette étape du pré-traitement est souvent négligée alors qu’elle a un fort impact dans des étapes ultérieures. En effet, le découpage des CV (tâche déjà difficile du fait de la variabilité évoquée) peut être un vrai casse-tête si l’on tient compte d</context>
</contexts>
<marker>[8]</marker>
<rawString>Craig Knoblock, Daniel Lopresti, Shourya Roy, and L.Venkata Subramaniam. Special issue on noisy text analytics. IJDAR, 10(3-4) :127–128, 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benoît Sagot</author>
<author>Pierre Boullier</author>
</authors>
<title>Sxpipe 2 : architecture pour le traitement pré-syntaxique de corpus bruts.</title>
<date>2008</date>
<booktitle>Traitement Automatique des Langues,</booktitle>
<volume>49</volume>
<issue>2</issue>
<pages>155--188</pages>
<note>714 c ATALA</note>
<contexts>
<context position="9960" citStr="[2, 9, 1, 4]" startWordPosition="1532" endWordPosition="1535">0). Pour les CV académiques, nous utiliserons le corpus CVac comme référence afin de tester le correcteur. Ainsi nous avons ajouté du bruit à l’ensemble CVac suivant la même procédure qu’auparavant. Les correcteurs utilisent tous les ngrammes générés avec les CV commerciaux plus les documents de l’ensemble T afin de debruiter les CV de CVac. La correction d’erreurs est une tâche généralement abordée dans la reconnaissance optique de caractères (OCR) ou dans le traitement d’information informelle, comme les blogs, les forums, les SMS ou les tchats. Les travaux concernant la correction de bruit [2, 9, 1, 4] traitent la correction 1. Grâce à la codification homogène des éditeurs (Word, Libre/OpenOffice) 2. Nous considérons un mot comme l’ensemble de caractères entre deux espaces 709 c ATALA TALN-RÉCITAL 2013, 17-21 Juin, Les Sables d’Olonne de fautes d’orthographe et grammaticales, la mauvais ponctuation ou l’utilisation d’abréviations. Mais le problème spécifique des blancs a été peu traité à notre connaissance. Pour résoudre ce problème, nous proposons deux stratégies à base de n-grammes de caractères : un correcteur binaire et un autre probabiliste. 4.1 Correcteur binaire L’algorithme utilise</context>
</contexts>
<marker>[9]</marker>
<rawString>Benoît Sagot, Pierre Boullier, et al. Sxpipe 2 : architecture pour le traitement pré-syntaxique de corpus bruts. Traitement Automatique des Langues, 49(2) :155–188, 2008. 714 c ATALA</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>