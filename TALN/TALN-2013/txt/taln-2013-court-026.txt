TALN-RECITAL 2013, 17-21 Iuin, Les Sables d’Olonne

SegCV : traitement efﬁcace de CV
avec analyse et correction d’erreurs

Luis Adrian Cabrera-Diego1:4 Juan-Manuel Torres-Moreno1’2’3 Marc El-Béze1:3
(1) LIA, Université d’Avignon et des Pays de Vaucluse, France
(2) Ecole Polytechnique de Montréal, Canada
(3) SFR Agorantic UAPV, France
(4) Flejay Group, France
adrian.cabrera@ﬂejay.com; {juan-manuel.torres, marc.elbeze}@univ-avignon.fr

RESUME
Le marché d’offres d’emploi et des candidatures sur Internet a connu, ces derniers temps, une
croissance exponentielle. Ceci implique des volumes d’information (majoritairement sous la
forme de textes libres) intraitables manuellement. Les CV sont dans des formats trés divers :
.pdf, .doc, .dvi, .ps, etc., ce qui peut provoquer des erreurs lors de la conversion en texte plein.
Nous proposons SegCV, un systéme qui a pour but l’analyse automatique des CV des candidats.
Dans cet article, nous présentons des algorithmes reposant sur une analyse de surface, aﬁn de
segmenter les CV de maniére précise. Nous avons évalué la segmentation automatique selon
des corpus de référence que nous avons constitués. Les expériences préliminaires réalisées sur
une grande collection de CV en frangais avec correction du bruit montrent de bons résultats en
précision, rappel et F—Score.

ABSTRACT
SegCV : Eficient parsing of résumés with analysis and correction of errors

Over the last years, the online market of jobs and candidatures offers has reached an exponential
growth. This has implied great amounts of information (mainly in a text free style) which cannot
be processed manually. The résumés are in several formats : .pdf, .doc, .dvi, .ps, etc., that can
provoque errors or noise during the conversion to plain text. We propose SegCV, a system that
has as goal the automatic parsing of candidates’ résumés. In this article we present the algoritms,
which are based over a surface analysis, to segment the résumés in an accurate way. We evaluated
the automatic segmentation using a reference corpus that we have created. The preliminary
experiments, done over a large collection of résumés in French with noise correction, show good
results in precision, recall and F-score.

MOTS-CLES : RI, Ressources humaines, traitement de CV Modéle a base de régles.

KEYWORDS: Information Retrieval, Human Resources, CV Parsing, Rules Model.

1 Introduction

L’accés massif d’internet par les personnes, les institutions et les entreprises a changé radicalement
la facon dont fonctionne le marché de l’emploi. De nos jours, des milliers de candidats mettent
en ligne leur Curriculum Vita (CV), et les entreprises ou les institutions publient des proﬁls de
postes recherchés. Analyser automatiquement cette quantité d’informations est une tache difficile

707 © ATALA

TALN-RECITAL 2013, 17-21 Iuin, Les Sables d’Olonne

a accomplir. Ceci est dﬁ, d’un cété a la masse grandissante de CV recus par les départements de
ressources humaines, et d’un autre a l’énorme diversité de la présentation des CV En particulier,
dans certains sections (identité, formation, expérience et compétences) et leur organisation.
Si on ne peut pas parler vraiment de documents << non-structurés >>, on peut les qualiﬁer de
« trop librement structurés >>, répondant a une structure conceptuelle propre a chaque individu
et difﬁcile a modéliser. Nous nous situons dans la double perspective d’emplois académiques et
commerciaux. L’employeur est ici une institution (université, grande école, centre de recherche)
ou une entreprise, et les candidats présentant des dossiers adaptés pour correspondre au mieux
aux proﬁls recherchés. Donc, nous projetons de concevoir un systeme intégral d’analyse des
candidatures académiques ou commerciales, dont la premiere étape consiste dans le découpage
des CV des candidats.

La problématique qui aborde SegCV est plus générale que celle étudiée auparavant [6, 7, 3],
car ces travaux analysent seulement des CV commerciaux. SegCV est composé des modules
suivants : Extraction d’information a partir des CV en formats PDE Word, Open Office, PS, DVI
et RTF ; analyse des CV pour extraire les sections importantes. Cet article présente un systéme
de découpage automatique des CV ainsi qu’une étude portant sur la correction d’erreurs lors
de la transformation en format texte. Nous présentons en section 2 la stratégie mise en oeuvre.
En Section 3, sont décrits les corpus utilisés. Nous présentons, en Section 4, la méthode pour
détecter et corriger les erreurs avec deux modéles basés sur des n—grammes. En section 6, sont
détaillés les différents résultats obtenus avant de conclure.

2 Méthodologie

Nous présentons la premiere étape d’un analyseur automatique d’offres et de demandes d’em—
ploi : un analyseur des CV basé sur le contexte. En fonction des sections déﬁnies comme étant
importantes par le recruteur, le systéme extrait l’information pertinente du CV, puis génére un
ﬁchier avec le contexte et la granularité voulue. L’analyseur est essentiellement basé sur un
nombre restreint de regles dépendantes de chaque langue. I1 transforme l’information des CV
en blocs d’information selon des modeles déﬁnis par l’utilisateur, faciles a comprendre par les
humains et exploitables par les machines.

Les CV originaux sont déclinés en formats divers : .doc, .odt, .pdf, .ps, .txt, etc. Aﬁn de pouvoir les
traiter convenablement, les CV sont transformés en texte utf—8. Cependant, cette transformation
n’est pas libre d’erreurs, surtout dans les ﬁchiers issus de PDE Nous considérons le bruit comme
la différence entre la forme superﬁcielle d’une représentation textuelle et le texte prévu, correct
ou originel [8]. Si la source est PostScript ou PDF du I5I‘EX, le texte extrait peut comporter un
certain nombre d’erreurs. Les caractéres accentués, la police utilisée et les petites majuscules sont
des sources d’erreurs récurrentes et difficiles a modéliser. Or, les ﬁchiers générés par ETEX risquent
d’étre tres frequents dans les CV issus du milieu acade’mique. Cette étape du pré—traitement est
souvent négligée alors qu’elle a un fort impact dans des étapes ultérieures. En effet, le découpage
des CV (tache déja difﬁcile du fait de la variabilité évoquée) peut étre un vrai casse—téte si l’on
tient compte du bruit introduit par les convertisseurs PDF a texte.

3 Corpus

Nous avons constitué un corpus de 100 CV en francais issus du domaine commercial. Ce corpus
a été découpé a la main par 2 annotateurs. Les annotateurs ont recu des consignes strictes quant

708 © ATALA

TALN-RECITAL 2013, 17-21 Iuin, Les Sables d’Olonne

au découpage des sections, selon un manuel fourni :

— Identité (coordonnées du candidat) ; Résumé ; Poste demandé (information qui décrit le poste
demandé) ; Situation actuelle du candidat ; Autres (loisirs, les références, etc.).

— Formation (formation universitaire) ; formation additionnelle (diplémes ou certiﬁcations).

— Experience (experience professionnelle).

— Compétences (compétences ou aptitudes personnelles, les langues étrangéres, les outils
maitrisés, etc).

Nous appelons ce corpus étalon CD. Pour les tests de découpage, nous avons constitué le corpus

CN, composé des mémes 100 CV mais sans le découpage manuel.

Pour étudier le bruit, nous disposons d’un corpus de 750 CV commerciaux, provenant de ﬁchiers
.doc, .odt et .rtf, pour lesquels la conversion, en théorie, n’a généré aucune erreurl. Ce corpus
sera nommé CVcomm. En ce qui concerne les CV académiques, la question est plus délicate :
La plupart de CV sont bruités, et les dé—bruiter manuellement serait une tache pénible et pas
exempte d’erreurs. Cependant, nous avons détecté 8 CV sans bruit, qui seront utilisés lors de
tests. Ce corpus sera nommé CVac.

4 Détection et correction du bruit

La transformation des CV en texte peut générer plusieurs erreurs : l’introduction de caractéres
composés, de caractéres superposés, la séparation des caractéres ou l’ajout des espaces entre
caractéres. En général, tous les cas, a exception du dernier, peuvent étre corrigés en utilisant
des expressions régulieres car ces erreurs suivent des patrons réguliers. Cependant le probleme
d’ajout d’espaces entre les caracteres semble étre de nature aléatoire. Parfois, ce type d’erreur est
occasionné par l’utilisation de caracteres accentués, de majuscules ou par l’utilisation d’un format
particulier des documents. Les blancs peuvent étre présents plusieurs fois dans le méme mot
ou dans la méme ligne. Ces espaces placés au milieu de mots peuvent empécher le découpage
correct des sections.

Pour bien mener nos tests, a partir du corpus CVcomm, nous construisons a tour de r6le 5
sous-corpus qui seront utilisés comme suit : 4/5 sous-corpus seront employés pour le calcul des
n—grammes et 1/5 pour la phase de tests. 11 faut dire que la génération des n—grammes est enrichie
d’un ensemble T de textes sans bruit : romans, livres scientiﬁques et discours composé de 784k
mots. Pour les tests, nous avons bruité le 1/ 5 du corpus avec des espaces en blanc introduits
de facon aléatoire. Aﬁn d’injecter chaque espace, nous avons généré 3 numéros aléatoires :
le premier ﬁxe la ligne du ﬁchier a bruiter, le deuxiéme le mot et le troisiéme la position a
l’intérieur du mot (en évitant les extrémes). Le bruit injecté est donc a pourcentage variable 2.
Nous appellerons ces corpus CB(,=0,5,10,15M100). Pour les CV académiques, nous utiliserons le
corpus CVac comme référence aﬁn de tester le correcteur. Ainsi nous avons ajouté du bruit a
l’ensemble CVac suivant la méme procédure qu’auparavant. Les correcteurs utilisent tous les n-
grammes générés avec les CV commerciaux plus les documents de l’ensemble T aﬁn de debruiter
les CV de CVac.

La correction d’erreurs est une tache généralement abordée dans la reconnaissance optique de
caractéres (OCR) ou dans le traitement d’information informelle, comme les blogs, les forums, les
SMS ou les tchats. Les travaux concernant la correction de bruit [2, 9, 1, 4] traitent la correction

1. Grace 31 la codiﬁcation homogéne des éditeurs (Word, Libre/Openoffice)
2. Nous considérons un mot comme l’ensemble de caractéres entre deux espaces

709 © ATALA

TALN—RECITAL 2013, 17-21 Iuin, Les Sables d’Olonne

de fautes d’orthographe et grammaticales, la mauvais ponctuation ou 1’utilisation d’abréviations.
Mais le probleme spéciﬁque des blancs a été peu traité a notre connaissance. Pour résoudre ce
probleme, nous proposons deux stratégies a base de n—grammes de caracteres : un correcteur
binaire et un autre probabiliste.

4. 1 Correcteur binaire

L’algorithme utilise des n—grammes de caracteres avec n = 4, .., 7. Ces n—grammes ont comme
caractéristique principale la présence, d’au moins, un espace entre deux caracteres ([a-zA—Z],
caracteres accentués ou l’apostrophe). Pour chaque ligne avec au moins un espace, on génere
le n—gramme le plus grand possible avec un espace en son centre. Le n—gramme original et ses
voisins a gauche et a droite du centre, sont recherchés.

Le n—gramme pere est considéré comme correct (l’espace central doit étre conservé), si lui ou
ses ﬁls, remplissent au moins une des conditions suivantes : i/ le 7—gramme existe; ii/ deux
6—grammes existent; iii/ au moins deux 5—grammes existent; iv/ Deux 4-grammes existent (zone
encadrée en pontillé de la ﬁgure 1). Ces conditions se basent sur l’idée qu’un n—gramme pére avec
un espace central engendre deux 6—grammes, trois 5—grammes et deux 4-grammes. Si la majorité
de ses ﬁls existent, il est probable que le n—gramme pere soit correct. Si le pere est considéré
comme incorrect, il faut analyser la classe a laquelle il appartient. Les cas et les corrections
dépendent du nombre d’espaces apres ou avant l’espace central, du nombre de caractéres a droite
et a gauche ou si le n—gramme est au début ou a la ﬁn d’une ligne. Les corrections sont des régles
perrnettant1’é1imination de blancs génants. L’algorithme de correction peut étre exécuté itératif
aﬁn de corriger des erreurs non trouvées lors des corrections précédentes.

     

   
 

9’9’=3m”7e5 luln ilvl elr slil
8i‘7’a’"’"e5 univ ers ln ilv erlslil
-_g ____ __\-
/ \
7—grammes lu nlilv lelrl// n Ilvl elrls \\nli Me rls
/’  ,/    ‘x
 nu nxifv  s\ vl new in
»,.. _ .,
5—grammes / nlilv le ‘ilv lelrl ‘V e rs
< \ ~ ~ >

\ \ // /
\ ,
4—arammes  i V 
\ , , , , , , , , , , , "4

FIGURE 1: Exemple de n—grammes pour les correcteurs.

4.2 Correcteur probabiliste

Aﬁn d’obtenir des performances plus robustes et de meilleurs résultats, nous avons développé
un correcteur probabiliste. Le principe étant proche de celui binaire, sauf que le parcours
des branches sera conditionné par la probabilité des n—grammes. L’algorithme construit le n-
gramme le plus grand possible (n = 4, ...,9) ayant un espace central entre deux caracteres.
Nous énumérons toutes les combinaisons de n—grammes en éliminant ou en maintenant les
espaces qu’ils contenant. Des caractéres a droite peuvent étre ajoutés pour maintenir la taille et
le contexte du n—gramme le plus grand possible. Puis on calcule les probabilités conditionnelles

710 © ATALA

TALN-RECITAL 2013, 17-21 Iuin, Les Sables d’Olonne

de chaque combinaison en utilisant l’est1'mation du maximum de vraisemblance :

C ("F101") = C (Hi)
C (W-1) C (W-1)

ou c,- est le dernier caractére du n—gramme de taille i, C (n,-) et C (n,-_1) sont leurs fréquences.
Si la probabilité de toutes les combinaisons du n—gramme sont nulles, la taille du n-gramme
est diminuée en 1 caractere (ﬁgure 1) et le processus itere a nouveau. Autrement on considere
comme une correction acceptable la combinaison ayant la probabilité conditionnelle la plus
grande.

P (Cilni—1) = (1)

5 Découpage en sections

La tache principale de SegCV consiste a repérer, découper et regrouper les sections pertinentes
des CV A cette ﬁn, on peut étre tenté d’utiliser des méthodes d’apprentissage automatique, car
on sait qu’elles donnent de trés bons résultats sur les taches de TALN. Mais l’apprentissage
automatique nécessite une grande quantité de documents préalablement étiquetés. Or, nous
ne disposons pas d’un grand corpus annoté manuellement. En conséquence, nous avons deux
possibilités pour faire face a ce probléme. La premiere consiste a faire un découpage a de tailles
ﬁxes (1/3, 2/3, etc.), comme proposé par [5], mais cette approche nous semble trop grossiere.
L’autre possibilité consiste a établir des régles de découpage. Notre objectif étant de découper les
CV de la maniére la plus ﬁne possible, nous avons décidé d’utiliser des régles.

A cette ﬁn, nous avons suivi deux approches. La premiere est basée sur la structure du CV : les
titres, les sous—titres ou les débuts des lignes avec un symbole délimitant une section. 94 expres-
sions réguliéres composent ces régles. La deuxiéme approche essaie d’améliorer le découpage au
moyen de mots—clés qui seront recherchés a 1’intérieur des sections. Le découpage fait appel a un
prétraitement (élimination ou normalisation de symboles et la normalisation d’espaces), puis,
les regles de structure sont appliquées. Apres ce premier découpage, nous vériﬁons la taille des
sections trouvées. Si elle est anormalment grande (ou petite) par rapport a la taille du CV nous
faisons appel aux mots—clés pour déclancher une procédure de déplacement de l’information. Par
exemple, si un fragment de texte dans la section << Compétences >> contient les mots célibataire
ou situation de famille ce fragment sera déplacé a la section << Identité ».

6 Résultats

Nous avons effectué trois expériences pour évaluer le découpage automatique et la correction du
bruit. Nous avons décidé d’utiliser des mesures de similarité et non pas des mesures basées sur les
frontieres du découpage car l’information dans les sections peut étre éparpillée. Puisque les CV
sont des ﬁchiers trop librement structurés, les limites exactes des sections sont difﬁciles a repérer.
Si l’on ajoute du bruit, ces frontiéres sont souvent perdues. Essayer de trouver les frontiéres
exactes est alors un exercice trés délicat et imprécis. Nous avons décidé donc de mesurer la
pertinence du découpage par le contenu des sections, plutét que par les frontiéres. A ce ﬁn, nous
avons utilisé deux mesures de similarité entre le découpage manuel et celui automatique : la
similarité cosinus et une mesure de divergence de Kullback—Leiber modiﬁée (issue du domaine du
résumé automatique). Une section sera considérée comme correctement découpée si sa similarité
dépasse un certain seuil. Le seuil peut étre strict (similarité = 1) ou relaxé (0, 95 < similarité
< 0, 5). Ensuite nous avons calculé la précision, le rappel et le F—score.

711 © ATALA

TALN-RECITAL 2013, 17-21 Iuin, Les Sables d’Olonne

Pour évaluer la correction du bruit, nous avons comparé le nombre de mots corrects dans le
ﬁchier corrigé par rapport au nombre de mots dans le ﬁchier d’origine. Forrnellement, la précision
et le rappel ont été déﬁnis de facon classique comme suit :

, . . _ Cc _ Cc
Precision — — Rappel — — (2)
TC To

ou, CC est le nombre de mots corrects dans le ﬁchier corrigé, TC le nombre de mots dans le ﬁchier
corrigé et To le nombre de mots dans le ﬁchier d’origine.

Découpage automatique. La premiere expérience a consisté a découper automatiquement les
ﬁchiers du corpus CN. La ﬁgure 2 montre le F—score pour les deux mesures de similarité.

1,0

0'9 _ —o KL

0.8 — 0/‘/4./3:3
0,7 — 9/. O 0/0 _
0,s _ /O//O/O’ _
0 5 - 0 /O -

F-SCOIE

 /0/° -

0,3 — _
0,2 — _

0,1 _ _

0,0

: u u u u u u u u u :
1,00 0,95 0,90 0,05 0,00 0,75 0,70 0,65 0,60 0,55 0,50
Seuil de relaxation

FIGURE 2: Découpage de CV : F—score

Le systéme ne découpe pas les sections avec une grande précision. Les raisons de ce probléme sont
variées. D’abord, les annotateurs ont évité les informations inutiles (numéros de page, en—tétes
ou les pieds de page), ce qui le systéme ne fait pas encore. Ensuite, la perte de la structure du CV
(comme les tables ou les colonnes) produit une mélange erronée de l’information. Et ﬁnalement,
les régles de mots—clés peuvent déplacer incorrectement 1’information d’une section.

Correction du bruit. Nous avons simulé le bruit par ajout aléatoire de blancs au milieu des mots.
La quantité d’espaces a été déterminée par la taille du ﬁchier d’origine et par un pourcentage
variable (0%, 5 %, 10 %...100 %). Les correcteurs binaire et probabiliste ont été appliqués itératif
trois fois. Au dela de la troisiéme application, les résultats n’ont guére changé. Pour l’évaluation
des corpus bruités CB,-, nous nous sommes servis des corpus de référence. La ﬁgure 3 montre
le F—score pour cette expérience mesuré sur des CV commerciaux a gauche et académiques a
droite. Les résultats montrent que le correcteur binaire fonctionne assez mal, méme pour des
quantités minimales de bruit. A 50 % de bruit, le correcteur probabiliste obtient un F—score
de 0,82 (CV commerciaux) et de 0,75 (CV académiques). Pour un taux de bruit de 100% le
correcteur probabiliste obtient un F—score de 0,80 (commerciaux) et de 0,71 (académiques). I1
faut dire que la quantité de bruit dans les cas réels n’est pas si élevée, mais nous voulions tester
nos correcteurs dans les cas extrémes.

Découpage automatique plus correction de bruit La derniere expérience a consisté a seg-
menter automatiquement le corpus CD. Mais cette fois, nous y avons ajouté du bruit de maniére

712 © ATALA

TALN-RECITAL 2013, 17-21 Iuin, Les Sables d’Olonne

ah Prg[)a[)i|i5(e _ ah —O—Probabi|is(e _
’ -\.$\_\ o Binaire ’ %Binaire
o
N

$.__.§.‘ _

.%"“§; 0+‘.
n,7- - n,7- ‘o——‘9—*I
n s — — n,s — —
§ 0 5 - - § 0 5 - -
u. n,4— — u. n,4— —
n 3 - - n,s - -
n 2 _:ITo——o-——o——n——o}:—~_g ,; _.,_E n 2 _¢>~—o———o:o——o——o;r,_O ,,,\__A__5

0, u u u v u u v u u v u
a 1n an an to so so 70 an nu ma
Bnit (96) Bnit (96)

FIGURE 3: Correction de l’injection de bruit : a gauche CV commerciaux, a droite CV académiques

aléatoire (de la méme facon que pour le CB), en utilisant le correcteur probabiliste, appliqué 3
fois, pour le diminuer. Nous avons évalué la qualité du découpage avec la mesure de cosinus.
La ﬁgure 4 montre la surface de F—score en fonction du pourcentage du bruit et du seuil de
relaxation. Les résultats obtenus indiquent que l’utilisation du correcteur impacte la qualité

0,5

11,5 — —

P

4

1%
1:
on

seuil do lalanlion
9
we on
S,
63
4
I

0 5
0, 70,54 E/\/\z\J:
£ 
20

Bmit (%)

FIGURE 4: Découpage automatique avec correction de bruit : F—score

de découpage. Pour un pourcentage de zéro bruit ajouté et un seuil de relaxation égal a 1,00,
c’est—a—dire une similarité exacte, le découpage automatique avec correction probabiliste obtient
un F—score = 0,13, contre un F—score = 0,37 du découpage sans correctif. Nonobstant, pour un
niveau de bruit nul et un seuil de relaxation égal 2‘: 0,50, le F—score pour le premier est de 0,84 et
de 0,79 sans correctif. Pour un niveau de bruit égal a 50 % et un seuil de relaxation de 50 %, le
découpage automatique avec correction obtient un F—score de 0,796.

7 Conclusions et perspectives

L’analyse automatique de CV est une tache extrémement difﬁcile. Ceci s’explique par plusieurs
raisons, dont la principale est la structure des CV : malgré une structure conventionnelle,
l’information présente dans les CV est en format libre. En outre, ils sont produits en plusieurs
formats électroniques. Leur transformation peut occasionner des erreurs ou perte d’information.
Le Vocabulaire utilisé peut Varier énormément au niveau des CV ou des proﬁls. Dans ce travail,

713 © ATALA

TALN-RECITAL 2013, 17-21 Iuin, Les Sables d’Olonne

nous avons présenté la premiere étape d’un systeme d’analyse automatique des CV Nous avons
présenté un module pour découper des CV en francais et un module pour corriger les erreurs
générées a cause de la transformation du ﬁchier d’origine. Les expériences réalisées montrent que
le découpage automatique doit étre amélioré pour se rapprocher plus du découpage manuel. Par
contre, la correction de bruit a montré de tres bons résultats. Nous avons vériﬁé que la méthode
probabiliste corrective donne les meilleurs résultats. Cependant, il faut éviter la correction de
ﬁchiers non bruités, car, en effet, il semble que la correction de faux positifs génere une diminution
de la qualité du découpage. A l’avenir, nous voulons augmenter la qualité de nos modules et les
appliquer dans de corpus académiques de taille plus conséquente. Pour le découpage automatique,
nous pensons ajouter un module de nettoyage aﬁn d’éliminer les numéros de pages, les en—tétes
et les pieds de page. De la méme facon, il sera intéressant d’effectuer des expériences avec des
CV dans des langues autres que le francais.

Remerciements

Ce travail a été ﬁnancé par la convention ANRT—CIFRE n° 2012/0293 entre Flejay et l’UAP\l

Références

[1] Sumeet Agarwal, Shantanu Godbole, Diwakar Punjani, and Shourya Roy. How much noise
is too much : A study in automatic text classiﬁcation. In Data Mining, 2007. ICDM 2007.
Seventh IEEE International Conference on, pages 3-12. IEEE, 2007.

[2] Alexander Clark. Pre—processing very noisy text. In Proc. of Workshop on Shallow Processing
of Large Corpora, pages 12-22, 2003.

[3] Jérémy Clech and Djamel A. Zighed. Data mining et analyse des cv : une expérience et des
perspectives. In Extraction et la Gestion des Connaissances, EGC’03, pages 189-200, 2003.

[4] Lipika Dey and SK Mirajul Haque. Opinion mining from noisy text data. International
Journal on Document Analysis and Recognition (IJDAR), 12(3) :205-226, 2009.

[5] Rémy Kessler, Nicolas Béchet, Mathieu Roche, Marc El-Beze, and Juan—Manuel Torres—Moreno.
Automatic proﬁling system for ranking candidates answers in human resources. In OTM ’08
Monterrey, Mexico, pages 625-634, 2008.

[6] Rémy Kessler, Juan—Manuel Torres—Moreno, and Marc El—Béze. E-Gen : Automatic Job Offer
Processing system for Human Ressources. In MICAI, pages 985-995, 2007.

[7] Rémy Kessler, Juan-Manuel Torres—Moreno, and Marc El-Beze. E-Gen : Proﬁlage automatique
de candidatures. In TALN’08 Avignon, 2008.

[8] Craig Knoblock, Daniel Lopresti, Shourya Roy, and L.Venkata Subramaniam. Special issue
on noisy text analytics. IJDAR, 10(3-4) :127-128, 2007.

[9] Benoit Sagot, Pierre Boullier, et al. Sxpipe 2 : architecture pour le traitement pré—syntaxique
de corpus bruts. Traitement Automatique des Langues, 49 (2) 2155-188, 2008.

714 © ATALA

