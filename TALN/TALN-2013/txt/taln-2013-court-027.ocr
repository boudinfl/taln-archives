TALN-RECITAL 2013, 17-21 Iuin, Les Sables d’Olonne

Recherche et utilisation d'entités nqmniées conceptuelles
dans une téche de catégorisation

]ean—Valere Cossul ]uan—Manue1 Torres—Moreno 13-3-4 Marc E1—Beze1-2-4
(1) Laboratoire Informatique d'Avignon - Université d'Avignon et des Pays de Vaucluse
339 chemin des Meinajaries, BP91228 84911 Avignon Cedex 9, France
(2) SFR Agorantic Université d'Avignon et des Pays de Vaucluse, 84000 Avignon Cedex
(3) Ecole Polytechnique de Montréal, 2900 Bd Edouard-Montpetit Montréal, QC H3T1]4
(4) Brain & Language Research Institute, 5 avenue Pasteur, 13604 Aix-en-Provence Cedex 1

{jean—valere.cossu,juan—manuel.torres,marc.el—beze}
@univ—avignon.fr

RESUME

Les recherches présentées sont directement liées aux travaux menés pour résoudre les
problémes de catégorisation automatique de texte. Les mots porteurs d’opinions jouent un
r6le important pour déterminer l’orientation du message. Mais il est essentiel de pouvoir
identiﬁer les cibles auxquelles ils se rapportent pour en contextualiser la portée. L'analyse
peut également étre menée dans l'autre sens, on cherchant dans le contexte d'une cible
détectée les termes polarisés. Une premiere étape d'apprentissage depuis des données
permet d'obtenir automatiquement les marqueurs de polarité les plus importants. A partir
de cette base, nous cherchons les cibles qui apparaissent le plus fréquemment :31 proximité
de ces marqueurs d’opinions. Ensuite, nous construisons un ensemble de couples (marqueur
de polarité, cible) pour montrer qu'en s’appuyant sur ces couples, on arrive :31 expliquer plus
ﬁnement les prises de positions tout en maintenant (voire améliorant) le niveau de
performance du classiﬁeur.

ABSTRACT

Search and usage of named conceptual entities in a categorisazion task

The researchs presented are part of a text automatic categorization task. Words bearing
opinions play an important role in determining the overall direction of the message. But it is
essential to identify the elements (targets) which they are intended to relativize the scope.
The analysis can also be conducted in the reverse direction. When a target is detected we
need to search polarized terms in the context. A ﬁrst step in an automatic learning from data
will allow us to obtain the most important polarity markers. From this basis, we look for
targets that appear most frequently in the vicinity of these opinions markers. Then, we
construct a set of pairs (polarity marker, target) to show that relying on these couples we
can maintain (or improve) the performance of the classiﬁer.

MOTS-CLES : Fouille d'opinion, Marqueurs de polarité, Reconnaissance d'entités nommées.
KEYWORDS : Opinion Mining, Named Entity Recognition.

1 Introduction

Depuis fort longtemps, la prise de décision se fait toujours aprés consultation des points de
vue d’autres personnes. On prend trés souvent connaissance des critiques émises par
d’autres consommateurs avant de consommer un produit ou service. Cette interaction entre

715 © ATALA

TALN-RECITAL 2013, 17-21 Iuin, Les Sables d’Olonne

individus peut étre élargie a ce qui se produit durant les campagnes précédant des élections.
Depuis le développement d'Internet, de plus en plus de personnes donnent leurs avis et ces
derniers étant de plus en plus disponibles, il est facile d'avoir acces a de larges corpus
d’opinion. Les applications possibles de la fouille d'opinion sont multiples (Pang et Lee,
2008), systemes de recommandation, outils de marketing, suivi de tendances etc. . Certains
moteurs de recherche proposent d'ailleurs déja des applications pour résumer les opinions
des consommateurs dans des interfaces dédiées (Blair-Goldensohn et al, 2008).

L'analyse d'opinion peut se décomposer en trois sous-taches :

1. Détection de la présence ou non de l’opinion ;

2. Classiﬁcation et intensité : (tres) positif, (tres) négatif ou neutre ;

3. Identiﬁcation des cibles et sources de l’opinion (sur quoi porte l’opinion et qui l'exprime).

Pour autant, alors qu'il est bon d'avoir un avis « général », extraire ce qui est exprimé sur un
point précis est tout aussi voire plus utile. La tache 3 pouvant étre répétée en changeant de
granularité, en se situant au niveau du texte entier, du paragraphe, de la phrase ou bien du
fragment selon les applications envisagées. L'exemple le plus frappant peut étre pris en
politique, ou l'enjeu n'est pas tant de convaincre ses opposants a l’ensemble a adhérer a ses
propositions mais plut6t de pousser ceux qui hésitent encore a basculer de son coté en
prenant appui sur un sujet donné. Dans ce cas-la, ce n'est pas sur l’ensemble de l'entité qu'il
faut agir mais plut6t sur des points précis. Points qu'il reste a déterminer et que nous
appellerons par la suite cibles ou Entités Nommées Conceptuelles (ENC).

2 Entités nommées

La reconnaissance des entités nommées (REN) fait partie de l'extraction d'information. Elle
consiste a délimiter et catégoriser certaines expressions linguistiques. Ces dernieres
correspondent a des ensembles de noms (entités, expressions temporelles, géographiques,
etc.). Toutefois les entités nommées peuvent étre plus spécifiques a un domaine et on parle
alors d'entités nommées d’intérét spécifique. Ici, il s'agira plus véritablement de sous-entités
dans la mesure ou elles correspondront a des éléments constitutifs d'entités (personnes,
entreprises, produits ou services) et représenteront donc des concepts qui seront
déterminés en fonction des données d’apprentissage sans connaissance a priori de la langue
et du domaine. La délimitation d'EN se fait habituellement sous la forme d'annotations en
utilisant des listes de connaissances ou avec l'aide d'experts (Dutrey et al, 2012). Les besoins
en connaissances linguistiques (et en connaissances du domaine) deviennent vite tres
importants. Nous proposons de les détecter de facon semi-automatique, puis de les utiliser a
des ﬁns de classification tout en gardant a l’esprit que ces dernieres peuvent faire un
excellent support permettant de produire automatiquement des résumés « polarisés ».

2.1 Proposiﬁons

L'hypothese de travail est la suivante: lorsqu'une opinion est exprimée, cette derniere
l'est forcément sur un élément de l'entité critiquée ou sur l'entité dans sa globalité. Cet
élément sera appelé cible ou sous-cible selon son niveau de granularité. A rebours, si dans un
message une cible est citée par une personne, nous supposons que c'est parce qu’elle
souhaite en dire ce qu’elle en pense ou a la limite dire qu’elle n'en pense pas grand-chose et
le fait de l'exprimer ainsi n'est probablement pas a négliger. A contrario, nous pouvons

716 © ATALA

TALN-RECITAL 2013, 17-21 Iuin, Les Sables d’Olonne

également considérer qu'en l’absence de cible dans la critique (critiques tres courtes) le
marqueur de polarité doit probablement porter sur l'entité par exemple : « super ﬁlm ».

Un des objectifs visés est l’eXtraction de couples (cible, marqueur de polarité) permettant a
la fois de catégoriser le message mais également de constituer un résumé de la
représentation de l'entité (ou du produit dans le cas d’un systeme de recommandation). Ces
couples ne sont pas limités aux seuls concepts identiﬁés par des experts du domaine ou par
ce qui est communément admis (Pupier, 1998), mais sont censés émerger des avis analysés
conformément a la facon dont ils ont été exprimés. Cette facon de procéder tient
implicitement compte de la restriction des différents sens d’un mot a ceux qui ont cours dans
le domaine abordé par les auteurs des critiques (Riloff et Wiebe, 2003). C'est le cas du terme
« navet » qui est un légume plus ou moins apprécié par les gastronomes mais aussi et surtout
pour ce qui nous concerne un mauvais ﬁlm dans le domaine du cinéma. On pourrait se baser
sur des listes de marqueurs d'opinion comme le propose (Navigli, 2009) mais s'il nous fallait
préétablir leur polarité cela impliquerait une coﬁteuse désambiguisation lexicale.

La méthode consiste a extraire dans le corpus les éléments les plus porteurs d’opinions
(marqueurs de polarité). Une fois ceux-ci extraits, nous cherchons, a proximité de ces
derniers, s'il existe des éléments a pouvoir discriminant modéré, non présents dans un anti-
dictionnaire (SL composé principalement de mots-outils). Si la fréquence de ces éléments
dépasse un plancher déterminé empiriquement, nous pouvons les considérer comme des «
cibles ». Nous pouvons considérer l'ensemble de ces cibles de méme que les métadonnées
ﬁlm ou pseudo comme des ENC.

3 Données

Des expériences de classiﬁcation automatique ont été menées sur un corpus de micro-
critiques (uc) de cinéma provenant du portail communautaire Vodkasterl.

Chaque [LC étant un tuple2: utilisateur, ﬁlm, note3, critique correspondant a la définition
d’une opinion donnée par (Liu, 2012).

- L'échelle des notes comporte dix barreaux espacés de 0,5 point entre 5 et 0,5 ;
- La critique est dite [1-critique car d’une longueur maximale de 140 caracteres.

Le corpus contient 77 000 [LG les 20 000 plus récentes constituent les corpus de
développement et test (10 000 chacun), le reste étant considéré comme apprentissage4.
L'échelle des notes est dans le cadre de nos expériences ramenée de facon volontaire a deux
barreaux. Nous avons tablé sur le fait que les positions les plus tranchées feraient ressortir
plus de cibles associées a des qualiﬁcatifs. Les seuils des deux barreaux ont été déterminés
de facon empirique : Positif (note>4) et Négatif (note<2). Les critiques dites neutres (dont
la note vaut entre 2 et 4 non inclus) sont pour l’instant exclues des corpus d’apprentissage,
développement et test.

Malgré les tailles restreintes des critiques et la liste de cible, les utilisateurs arrivent a
exprimer plusieurs opinions (parfois opposées) sur les différents éléments des ﬁlms.

1 http://www.vodkaster.com

2 Nous envisageons d’utiliser par la suite d’autres métadonnées comme : acbeurs, réalisateurs, genre.
3 Note mise par l’utilisabeur lorsqu’il a déposé sa critique sur le portail.

4 Bien évidemment, les 3 intersections de ces corpus pris 2 E1 2 sont vides.

717 © ATALA

TALN-RECITAL 2013, 17-21 Iuin, Les Sables d’Olonne

Les critiques nuancées ou équilibrées ([LC contenant un des « pivots » prédéterminés) sont
retirées des différents corpus. Nous avons a cet effet sélectionné uniquement les deux «
pivots » les plus fréquents5 dans le corpus d'apprentissage : « mais » et « malgré ». Ne seront
donc présentes dans le corpus de test que les [LC a priori fortement polarisées contenant au
moins une cible et ne contenant aucun de ces « pivots » de langage ce qui réduit a 5 010

critiques sur l'ensemble des 10 000 présentes a l'origine dans le corpus.

Deux systemes concurrents ont été mis en place: l’un prenant en compte le couple
(cible-marqueur de polarité), l’autre se basant sur l'ensemble des termes présents dans
la [LC. Toutes les expériences présentées tiennent compte de la polarité du pseudo de
l’utilisateur ainsi que celle du titre du film, qui ont été intégrés comme des termes
a l’intérieur de la [LC et deviennent de fait porteurs d'opinions.

3.1 Classiﬁeurs

Le premier classiﬁeur utilisé est un CosinusGini (M1) (Torres et al, 2011). Il est basé sur
l'ensemble des termes présents dans la critique. Le classifieur Cosinus a été préféré a
d'autres méthodes plus classiques et parfois plus performantes comme les SVM (Collobert et
al, 2002) du fait que ces méthodes ne permettent pas d'avoir facilement acces aux éléments
ayant contribué a la classiﬁcation.

Le second (M2) est une variante du premier, ne prenant cette fois en compte que les couples
(cible, marqueur de polarité) comme mentionné en 2.1 et repris en 3.2 ; les marqueurs de
polarité seront recherchés avec un rayon de R (variable entre 1 et 9) termes de part et
d'autre de la cible. Nous avons fait varier le rayon aﬁn d’évaluer l'impact du contexte sur la
catégorisation de la cible.

Les performances sont mesurées en termes de rappel et de précision. Il arrive parfois pour
des petits rayons qu'il n'y ait aucun couple présent dans une [LC pour cette raison nous
comparons M1 et M2 sur la précision a un méme niveau de rappel, celui déterminé par M2.

3.2 Liste de cibles

Les cibles sont déterminées de maniere semi-automatique selon le protocole suivant :

A partir d’un apprentissage, le systeme détermine les termes ayant la plus forte contribution
dans chacune des catégories. Puis il cherche a proximité de ces derniers s'il existe des
éléments, non présents dans l'anti-dictionnaire et n'étant potentiellement pas de forts
marqueurs de polarité. Le travail de relecture se trouve étre ici assez limité, il consiste a
contr6ler les sorties du systeme pour valider ce qui est conservé comme cible ou non.
Ceci est nettement moins coﬁteux qu’un travail de brainstorming avec des experts du
domaine. Cette facon présente un autre avantage de taille : celui de coller a la langue dont on
percoit l’évolution rapide notamment dans les réseaux sociaux. Pour illustrer notre propos
nous donnons quelques exemples (extraits a partir d’une premiere liste d’environ 550
cibles) avec leur nombre d'apparitions sur l'ensemble du corpus : « acteurs» (3 000), « mise
en scéne» (2 000), « re’a11'5at1'011» (931), « e5t11e’t7'que» (630).

5 On aurait pu en rajouber d'autres comme « bien que » et « et pourtant » (130 et 150 occurrences).

718 © ATALA

TALN-RECITAL 2013, 17-21 Iuin, Les Sables d’Olonne

Listons aussi quelques termes porteurs de polarité purement positiveé auxquels on n’aurait
pas forcément pensé en premier lieu7 : « coup de poing » (42:14), « norme » (33;9), «
exaltant» (32:10). Ce dernier apparait d'ailleurs une fois en négatif dans le test a propos du
ﬁlm ]uno. On en comprend la raison au vu de l’ironie de son contexte : « aussi exaltant qu'un
foetus mort». En téte des termes a polarité négative, on trouve: « regardable » (17;1), «
bidon » (16;5), « beurk » (16;5) 

Puis, l'enrichissement de la LC se fait selon les deux procédures suivantes :

- Procédure P1 : trouver des cibles permettant de couvrir des [LC ou aucun terme
n’appartient a la liste de cibles. Ne sont alors retenus que les termes présents dans le plus
grand nombre de [LC résiduelles mais qui permettraient également d’améliorer la couverture
des [LC déja sélectionnées. Les termes ayant un pouvoir discriminant proche de celui des
mots outils sont filtrés.

- Procédure P2 : dans le cas de [LC correctement étiquetées par M1 mais pas par M2.
L’objectif est de chercher dans le voisinage du terme de polarité P, qui ale plus contribué a la
bonne décision de M1, un terme T répondant au critere : T E (LC n SL). Seront alors
proposés les termes se trouvant dans le plus grand nombre de [LC résiduelles, avec fréquence
élevée et pouvoir discriminant supérieur a celui des mots outils.

Itérer ces deux procédures a permis d’augmenter facilement la couverture de la LC en
refrénant l'accroissement de sa taille (550 puis 982 cibles). Parmi les 5 010 critiques restant
dans le corpus apres retrait de celles contenant un pivot. 4 580 contiennent au moins une
des cibles présentes dans la liste. La couverture est d’environ 2,9 cibles par [LC traitée.

En s’appuyant sur les marqueurs de polarité se trouvant a proximité des cibles, et donc en
ﬁltrant ce que l’on peut considérer comme du bruit, on cherche a éliminer une partie de ce
qui pourrait amener a prendre une mauvaise décision.

4 Expérimentations
4.1 Résultats

Les recherches présentées ici mettent en avant l'utilisation des couples (cible-marqueur de
polarité). L'extraction d'un couple peut sufﬁre a catégoriser un tweet Au lieu d’opter pour
un protocole lourd d'évaluation de la pertinence des cibles détectées nous avons choisi d'en
faire une estimation certes grossiere mais peu coﬁteuse. Leur extraction peut étre
considérée comme valide des que la prise en compte des seuls couples présents permet de
faire aussi bien qu'un classiﬁeur utilisant l’intégralité des termes de la [LC (Table 1 pour
R=7).

Une premiere série d’eXpériences a été menée (pour une LC comprenant 550 cibles). Avec un
rayon égal a 7, on trouvait 4 449 critiques du développement contenant au moins une cible,
M1 en classait correctement 3 957 (soit 88.94%) contre 3 975 (89,35%) pour M2.
En ramenant le rayon a 1, il ne restait que 3286 [LC, M1 retrouve correctement la classe de
2977 [LC (90,59%) et 2 827 (86,03%) pour M2.

5 La polarité de ces termes dans le corpus prend parfois le contrepied des usages courants.
7 Avec leurs fréquences d’apparitions (apprentissage et développement).

719 © ATALA

TALN-RECITAL 2013, 17-21 Iuin, Les Sables d’Olonne

Aﬁn de pouvoir intégrer dans le test des critiques ne contenant pas de cibles (c’est le cas des
critiques tres courtes ne contenant qu'un seul terme tres souvent porteur de polarité) nous
avons considéré que l’entité (ici le ﬁlm) pouvait étre une cible. LC a donc été enrichie et on
arrive :31 982 « cibles » potentielles. La couverture passe a environ 3,4 cibles par [LC traitée
contre 2,9 avec la premiere liste.

Dev (M1) Dev (M2) Corpus Test (M1) Test (M2) Corpus
3540 (91.09) 3399 (87.47) 3886 3453 (90.00) 3311 (86.36) 3834
4137 (90.00) 4031 (87.77) 4593 4025 (89.54) 3892 (86.59) 4495
4288 (89.70) 4242 (88.84) 4780 4179 (89.39) 4083 (87.34) 4675
4318 (89.60) 4278 (88.77) 4819 4216 (89.38) 4153 (88.04) 4717
4327 (89.51) 4316 (89.28) 4834 4228 (89.31) 4182 (88.34) 4734
4337 (89.51) 4349 (89.76) 4845 4234 (89.25) 4204 (88.62) 4744
4340 (89.52) 4366 (90.06) 4848 4237 (89.22) 4227 (89.01) 4749
4344 (89.51) 4365 (89.94) 4853 4239 (89.20) 4231 (89.04) 4752
4346 (89.51) 4363 (89.87) 4855 4239 (89.20) 4235 (89.12) 4752

TABLE 1 — Résultats des 2 méthodes en fonction du rayon Ren termes de précision

En effets les résultats obtenus sur le test conﬁrment la robustesse de la méthode car, compte
tenu de l'intervalle de confiance, ils sont du méme niveau que ceux obtenus sur le corpus de
Développement L'intervalle de conﬁance est de 0,8% pour le corpus Dev et 0,9% pour le
corpus Test

4.2 Analyse des résultats et exemples

En réduisant le rayon de la fenétre dans laquelle, autour d'une cible, sont pris en compte des
marqueurs de polarité, les résultats de M2 et notamment le rappel chutent logiquement,
comme le montre la derniere colonne de la Table 1. Par contre pour la premiere méthode
(M1) le rappel reste stable car M1 prend en compte l'ensemble du contenu de chaque uC.
Toutefois, la méthode M2 permet d'identiﬁer les critiques pour lesquelles M1 est bien plus
performant que sur l'ensemble du corpus (89,50 sur le Dev et 88,92 sur le Test). Cette
mesure permet donc de faire ainsi un premier ﬁltrage des données a tester. Nous constatons
également que le passage de 550 a 982cibles a permis d’améliorer les résultats, il ne serait
pas improbable que les résultats s’améliorent encore avec une liste de cibles plus grande.

Une des retombées essentielles de la méthode que nous proposons ici réside dans sa capacité
a étre utilisée pour savoir ce qui a été dit sur une entité particuliere. Il sufﬁt pour cela de

720 © ATALA

TALN-RECITAL 2013, 17-21 Iuin, Les Sables d’Olonne

retenir avec leur orientation les couples (cible, marque de polarité) de fréquence et pouvoir
discriminant élevée. Par exemple pour le ﬁlm « Slglfall » nous avons extrait les couples
suivants : « ﬁlm, d ‘actions rate’ », « beaute’, st11pe’ﬁante», « mise-en-scene, classique ». 11 en va
de méme si l’on souhaite savoir précisément quelles sont les expressions les plus employées
et les plus marquantes utilisées par un membre donné du réseau. Par exemple, parmi les
expressions employées de facon marquante par « IMTHEROOKIE » un des plus
gros contributeurs du site Vodkaster, on trouve : « c11ef d ’as-uvre, ultime », «mise-en-scene,
radicale ». Quelques exemples avec les genres de ﬁlm: « drame, sentimental », « come’die,
jubila toire ».

9‘ ' I I I I I I M2 Delv T '
M‘1‘De\r T

90.5 ' lM2l|'est T ’
1M1 Test T

90 ' ‘

88.5 ' ‘

Précision en %

87.5

E37

86.5

 

1 2 3. 4 5 6 7 s 9
Rayon en nombre de termes

FIGURE 1 — Evolution des résultats en fonction du rayon.

Le corpus pourrait alors devenir une base de données interrogeable en fonction des besoins
de chacun. On se donne ainsi la possibilité de répondre aux questions que pourrait se poser
un producteur de ﬁlm ; « comment est appre’cie’e la mise en scene des /ames Bond ?», « tous
les Volets de la saga sont-i1s critiques de la méme maniére .7 », « que11e opinion ceux qui ont
appre’cie’ Casino-Royal ontpu avoir sur Sig/15211? » ou encore « alors que la saga T wi111g11t est
ma] cote’e, quels sontles points mis en aVant par les gens qui ontaime’ ces films .7».

4.3 Perspectives

Les couples extraits pourront servir pour des taches d'analyse plus ﬁne ou de « reporting »,
par exemple dans l'analyse d’un service a besoin relationnel ou il apparait important de
connaitre les points a améliorer tout autant que les points appréciés. Dans le but de produire
des résumés de ce que pensent les consommateurs d’un produit, la procédure présentée en
4.2 est réutilisable pour dresser un tableau de bord résumant l’ensemble des avis émis sur
un produit et, a partir de la liste des cibles, il ne reste plus qu'a extraire tous les marqueurs
de polarité qui leurs ont été associés.

721 © ATALA

TALN-RECITAL 2013, 17-21 Iuin, Les Sables d’Olonne

La méthode proposée permet d'eXtraire des cibles en fonction d’une liste constituée de
maniere semi-automatique. La principale perspective d'évolution vise a automatiser
totalement le processus d’élaboration et d'enrichissement de la liste aﬁn de faciliter le
portage du systeme a un autre domaine ou a une autre langue.

Il serait possible en appliquant des méthodes de généralisation de remonter jusqu'au
concept des cibles extraites. Dans le cadre du projet ImagiWeb, nous disposons a l’inverse de
concepts de cibles avec des exemples et il nous faudrait, par annotations manuelles, en
rechercher l’ensemble des marqueurs. La méthode présentée deviendrait encore plus
intéressante dans la mesure ou elle permettrait de pré annoter certains passages et limiter
ainsi le travail des annotateurs.

Remerciements

Ce travail a été subventionné par l'ANR, Projet IMAGIWEB contrat n° 2012-CORD-002-05 et
par le Pole de Compétitivité SCS. Le corpus sur lequel ont porté les experiences a été mis a
notre disposition par les fondateurs du Site Vodkaster. Nous tenons ales en remercier.

Références

BLAIR-GOLDENSOHN, S, HANNAN, K, MCDONALD, R, NEYLON, T, REIS, G, REYNAR, ]. (2008)
Building a sentiment summarizer for local service reviews. In WWWWork5hop on NLP.

COLLOBERT R, BENGIO S. et MARIETHOZ ]. (2002). Torch: a modular machine
Iearningsoftware Iibraij/. In Technical Report IDIAP-RR02-46, IDIA

DUTREY, C, CLAVEL, C, ROSSET, S, VASILESCU, I, ADDA-DECKER, M, (2012). Quel est l’apport
de la détection d’entités nommées pour l'eXtraction d’information en domaine restreint ? In
Actes de TALN12.

LIU, B. 2012). Sentiment Analysis and 0 inion Mining A Comprehensive Introduction and
Survey organ & Claypool, May 2012, 16 pages.

NAVIGLI, R. (2009). Word sense disambiguation : A survey. ACM Computingsurveys.

BANG, B. et_LEE L. (2008). Oiainion mining and sentiment analysis. Foundations and Yrends
In Information REITIVHI, 2:1— .

PUP_IER, _P. _(1998 . Une premiere systématique des évaluatifs en francais. Revue que’be’coi5e
de hnguistique, 2 (1).

 E, WIEBE, ]. (2003). Learning extraction patterns for subjective expressions. In

TORRES-MORENO, ]-M, EL-BEZE, M, BEL_LOT, _P, BE(;HET_F. (20_11) Peut-on voir la_ détection
d'op1n1ons_comme _un probleme de classiﬁcation thematuiue ? 1n Modeles 5tat15t1 ues pour
I’acce5 a I’1nformat1on eXtueIIe sous la d1rect1on de GAUSS ER, E, YVON, F, Hermes, 011

722 © ATALA

