TALN-RECITAL 2013, 17-21 Iuin, Les Sables d’Olonne

Un corpus d’erreurs de traduction

Guillaume Wisniewski1:2 Anil Kumar Singh2 Natalia Segal3 Francois
Yvon”
(1) Université Paris Sud 91 403 ORSAY CEDEX
(2) LIMSI—CNRS 91 403 ORSAY CEDEX
(3) Reverso—Softissimo, 5 rue Soyer, 92 500 NEUILLY

{wisniews , a.ni1 , yvon}<01imsi . fr , nsegalcsoftissimo . com

RESUME
Avec le développement de la post—édition, de plus en plus de corpus contenant des corrections de
traductions sont disponibles. Ce travail présente un corpus de corrections d’erreurs de traduction
collecté dans le cadre du projet ANR/TRACE et illustre les différents types d’analyses auxquels
il peut servir. Nous nous intéresserons notamment a la détection des erreurs fréquentes et a
l’analyse de la variabilité des post—éditions.

ABSTRACT
A corpus of post-edited translations

More and more datasets of post-edited translations are being collected. These corpora have many
applications, such as failure analysis of SMT systems and the development of quality estimation
systems for SM'I‘. This work presents a large corpus of post-edited translations that has been
gathered during the ANR/ TRACE project. Applications to the detection of frequent errors and to
the analysis of the inter—rater agreement of hTER are also reported.

MOTS-CLES : Traduction automatique,Analyse d’erreur,Post—édiiu'on.

KEYWORDS: Machine Translation, Failure Analysis,Post-edition.

1 Introduction

La post—édition consiste a corriger les sorties d’un systéme de traduction automatique (TA) aﬁn
de produire une traduction de qualité. Cette pratique se développe de plus en plus, aussi bien
dans le cadre de traduction professionnelle (Garcia, 2011), que pour l’évaluation des systemes
de TA : quantiﬁer le nombre d’éditions nécessaires pour la post—édition, comme le fait le score
hTER (Snover et al., 2006), fournit une indication pertinente de la qualité d’un systéme de TA.

Le développement de la post—édition suscite le développent et la diffusion de corpus contenant
des corrections de traductions (Potet et al., 2012; Callison—Burch et al., 2012). Le travail présenté
dans cet article s’inscrit dans cette lignée et décrit la constitution et l’exploitation d’un nouveau
corpus de corrections de traductions collecté dans le cadre du projet ANR—TRACE 1. Le recueil de
ce corpus permet de répondre a un des principaux objectifs de TRACE, a savoir le développement
de mesures de conﬁance pour la TA (Zhuang et al., 2012) et la détection de zones difﬁciles a

1. anr—trace. 1imsi.fr

723 © ATALA

TALN-RECITAL 2013, 17-21 Iuin, Les Sables d’Olonne

traduire. D’autres usages sont également envisageables : il peut, par exemple, étre utilisé pour
identiﬁer les limites des systémes de traduction, ou encore pour étudier la cohérence des scores
hTER et, de maniére plus qualitative, la variabilité des post—éditions.

C’est sur ces derniers points que porte le travail présenté dans cet article : aprés avoir détaillé les
caractéristiques du corpus et la maniére dont les données ont été collectées (Section 2), nous
discutons a la Section 3 de deux manieres de mettre en évidence certaines limites des systemes
de TA. Nous présentons ﬁnalement, a la Section 4, une premiere analyse de la variabilité des
post—éditions.

2 Description du corpus

Le corpus TRACE de corrections de traductions comprend 6693 phrases (soit 109 689 mots)
pour la direction francais—anglais; et 5 929 phrases (soit 120378 mots) pour la direction anglais—
francais. Ces phrases ont été traduites par deux systémes de TA : un systéme commercial 2‘: base de
régles, SYSRULE, et un systéme statistique, NCode (Crego et al., 2011; Le et al., 2012), désigné par
SYSSTAT dans la suite du texte. Pour chaque direction de traduction, un traducteur professionnel
conﬁrmé 2 a ensuite corrigé une des deux traductions automatiques (choisie aléatoirement) pour
produire la référence post—éditée. Conformément 2 l’usage, les traducteurs traduisaient vers leur
langue maternelle. Le corpus TRACE contient, en outre, pour chaque direction de traduction,
1 000 phrases qui ont été corrigées par deux traducteurs différents. Ces corpus sont librement
téléchargeables sur le site du projet TRACE. 3

Ces données proviennent, pour moitié, de demandes de traduction d’uti1isateurs << grand public »
collectées sur le portail de traduction en ligne de Softissimo (3 434 phrases en francais et 2 541
en anglais) ; l’autre moitié est issue d’extrait d’un site journalistique en ligne (2 268 phrases
en francais), de différents corpus utilisés dans les compagnes d’évaluation de traduction WMT
(Callison—Burch et al., 2012) (991 phrases en francais et 864 en anglais) et IWLST (Cettolo
et al., 2012) (1 524 phrases en anglais) ainsi que d’une campagne d’évaluation de modules
de désambiguisation sémantique (Lefever et Hoste, 2010) (1000 phrases en anglais). Les
exemples de ce dernier sous-corpus sont accompagnés d’informations complémentaires, telles
que des traductions de référence ou des annotations sémantiques, qui ont été collectées par les
organisateurs de ces différentes campagnes d’évaluation.

Des consignes de correction précises (diffusées avec le corpus) ont été fournies aux traducteurs
aﬁn d’assurer que celles-ci soient minimales : l’objectif est d’obtenir des traductions jugées
correctes (aussi bien au niveau du sens que de la langue) tout en restant le plus proche possible
de la traduction automatique. Aﬁn de garantir leur qualité, des échantillons des corrections ont
été Validées par un expert et, au besoin, des modiﬁcations ont été demandées aux traducteurs
pour assurer le respect des consignes. Par ailleurs, les traductions corrigées ont été utilisées
pour évaluer automatiquement la qualité des systémes de TA. Comme le montre le Tableau 1,
les principales métriques ont des valeurs bien plus élevées que celles généralement observées,
montrant clairement que les références produites sont effectivement plus proches des sorties des
systémes que les références utilisées dans les campagnes d’évaluation. Ainsi, lorsque SYSSTAT est
évalué par rapport aux références fournies pour la campagne d’évaluation WMT 2012, son score

2. Au total, 10 traducteurs différents (5 pour chaque direction de traduction) ont été sollicités
3. anr—trace. 1imsi.fr

724 © ATALA

TALN—RECITAL 2013, 17-21 Iuin, Les Sables d’Olonne

SYSSTAT SYSRULE
BLEUT 57,0 47,6

hTERJ, 29, 1 36,8
MétéorT 40, 6 33,8

TABLE 1 — Evaluation des systemes de TA quand les hypotheses post—éditées sont prises comme
références. Les scores suivis de T (resp. 1,) sont d’autant meilleurs qu’ils sont grands (resp. petits).

TER est de 56,3 (contre 36,8 ici). Notons également que, comme cela a déja observé par ailleurs,
les métriques automatiques défavorisent fortement le systeme a base de regles.

3 Analyse des limites des systemes de TA

Nous montrons dans cette section comment la comparaison des hypotheses de traduction avec
leur post-édition permet d’identiﬁer certaines limites des systemes de TA. Pour des raisons de
place, seuls les résultats obtenus pour les traductions de l’anglais vers le francais sont présentés.

3. 1 Erreurs fréquentes

Le calcul de la distance d’édition entre les hypotheses de traduction et leur post-édition permet
de déterminer automatiquement les corrections a effectuer pour rendre << acceptables » les
traductions automatiques. L’étude des éditions les plus fréquentes permet de caractériser certaines
limites des systemes de TA actuels.

Une premiere observation porte sur le type des éditions fréquentes : il s’agit essentiellement
de substitutions (Tableau 2), meme si le systeme a base de regles a tendance a produire des
traductions trop longues. Une part non négligeable des substitutions (pres de 9 %) correspond 2
la modiﬁcation de la terminaison d’un mot (par exemple, « penserai » est corrigé en « penserais »,
« spéciales » en « spécial >>, ...). Il est toutefois difﬁcile d’évaluer si ces modiﬁcations sont des
corrections isolées (par exemple, pour corriger une erreur d’accord) ou si bien elles découlent
d’autres corrections (accord d’un adjectif suite a la substitution du mot avec lequel il s’accorde).

Une e’tude statistique des éditions montre que la plupart des modiﬁcations (pres de 70 %) sont
uniques, ce qui rend difﬁcile l’identiﬁcation de motifs d’erreurs . Les erreurs les plus fréquentes
portent presqu’exclusivement sur des mots outils (Table 3) et, comme précédemment, il est
difﬁcile de savoir si ces révisions sont dues a des erreurs de la TA, ou bien découlent d’autres
corrections. Le ﬁltrage des mots outils permet de faire apparaitre certains motifs d’erreurs
récurrents. Ainsi, sur les 5 929 traductions du corpus, la traduction de « order » par « ordre » a
été corrigée 23 fois en << commande » et « maison » 10 fois en « chez  >>. Une centaine de motifs
de ce type ont été extraits, meme si tous ne sont pas aussi facilement interprétables.

725 © ATALA

TALN-RECITAL 2013, 17-21 Iuin, Les Sables d’Olonne

Substitution Inserﬁon Suppression

148 les—>des 380 de 799 de
93 des—>les 233 la 335 e

opération SYSSTAT SYSRULE

déplacement 2 861 3 473 60 la —> le 204 le 329 la

substitution 10 065 10 991 57 du —> le 204 a 278 le

suppression 3 572 7 371 55 des —> d6 184 5 277 que
insertion 2 502 2 253 53 du —> de 141 dans 256 les

51 de —> des 131 que 242 en
46 de —> pour 99 en 215 et
43 cela —> il 97 un 212 des
42 une —> un 96 des 167 pour

TABLE 2 — Nombre d’opérations nécessaires
pour corriger les sorties des deux systemes
de TA

TABLE 3 — Corrections les plus fréquentes

3.2 Différences entre les traductions automatiques et leur post-édition

Une autre analyse, inspirée des travaux en estimation de conﬁance pour la traduction (Kulesza
et Shieber, 2004), permet d’avoir une vision plus globale des différences entre hypotheses de
traduction et traductions post—éditées. Cette analyse repose sur l’apprentissage d’un classiﬁeur
capable de distinguer ces deux types de traductions et l’étude des caractéristiques utiles pour
faire cette distinction. Le meme principe peut étre utilisé pour caractériser les différences entre
les références obtenues en post—éditant des hypotheses de traduction et les références << libres >>
utilisées dans les campagnes d’évaluation de la traduction.

Dans les expériences de cette section, chaque traduction est représentée par un ensemble

de 336 caractéristiques utilisées dans un systeme d’esu'mation de conﬁance pour la TA (Wisniewski

et al., 2013). Ces caractéristiques se répartissent en quatre grandes catégories :

— des mesures de la qualité de 1’ << association >> entre la source et l’hypothese de traduction, telles
des caractéristiques dérivées des modeles d’alignement;

— des mesures de la ﬂuidité et de la grammaticalité de 1’hypothese de traduction ainsi que de la
phrase source, telles des caractéristiques dérivées des modeles de langue;

— des caractéristiques de surfaces telles le nombre de mots hors vocabulaire, de signes de
ponctuation,  ;

— des caractéristiques syntaxiques simples comme le nombre de noms, de mots outils, 

Une liste complete des caractéristiques utilisées est donnée dans (Wisniewski et al., 2013).

Pour mener cette analyse, nous avons utilisé comme classiﬁeur une forét aléatoire (Breiman,
2001), une méthode d’apprentissage ensembliste qui repose sur la combinaison des prédictions de
plusieurs arbres de décision. Les foréts aléatoires ont montré leur efﬁcacité dans de nombreuses
taches; elles sont connues pour etre particulierement robustes au sur—apprentissage et pour
permettre la modélisation d’interactions complexes entre les caractéristiques. En plus de la
construction d’un classiﬁeur, l’algorithme d’apprentissage permet d’estimer l’importance de chaque
caractéristique (Breiman, 2001) qui quantifie directement son pouvoir discriminant : plus cette
importance est élevée, plus la caractéristique est utile a la prédiction de l’étiquette.

Nous avons utilisé, dans nos expériences, l’implémentation des forets aléatoires fournies par
la bibliotheque scikit—1earn (Pedregosa et al., 2011). Les parametres de la foret aléatoire
sont appris sur 2/3 des données; le dernier tiers des données étant utilisé pour évaluer les

726 © ATALA

TALN-RECITAL 2013, 17-21 Iuin, Les Sables d’Olonne
performances du classiﬁeur. L’ensemble des hyper—parametres sont choisis par validation croisée.

La premiere téiche considérée a pour objectif de distinguer les traductions produites par un
systeme de TA de leur post—édition : elle nécessite donc de distinguer automatiquement une
bonne traduction d’une mauvaise. C’est une tﬁche difﬁcile, ces deux traductions étant par
construction proches l’une de l’autre. I1 n’est donc pas surprenant que la précision du classiﬁeur
ne soit que de 63 % en apprentissage et de 59 % en test. Les performances de la seconde
tache, visant a distinguer les références obtenues par post—édition des références << libres >> sont
sensiblement meilleures : la précision en apprentissage est de 71 % et de 67 % en test.

Les 8 caractéristiques les plus discriminantes et leur importance sont représentées Figure 1. Pour
les deux téiches, seules quelques caractéristiques sont discriminantes et celles—ci sont presque
uniquement dérivées des scores de modeles de langue. Les modeles de langue neuronaux (Le
et al., 2011) (caractéristiques comportant SOUL dans leur nom), appliqués aussi bien a la source
qu’a la traduction, jouent un role prédominant, surtout pour la distinction entre les hypotheses
de traduction et leur post—édition. Ces caractéristiques sont complétées par des modeles de
langue « classiques >> appris aussi bien sur les étiquettes morpho-syntaxiques (POSLMLOGPROB
correspond a la log—probabilité d’une séquences d’étiquettes morpho-syntaxiques) que sur les
mots (BIGRAMSFREQQUARTILE1 décrit le pourcentage de bi—grams dont la fréquence est dans le
premier quartile). Dans tous les cas, les valeurs des caractéristiques sont plus faibles pour les
traductions automatiques que pour les hypotheses post-éditées qui ont elles—mémes des valeur
plus faibles que celles observées dans les références libres. Cette observation indique soit que
l’espace de recherche des systemes de TA n’est pas assez riche puisque le systeme de TA n’est pas
capable de générer des hypotheses sufﬁsamment « ﬂuides >>, soit le modele de langue n’a pas un
poids sufﬁsant dans la fonction de score qui permet au systeme de TA d’évaluer la qualité des
hypotheses. Des expériences supplémentaires sont toutefois nécessaires pour déterminer laquelle
de ces deux hypotheses est correcte.

Parmi les autres caractéristiques importantes, on peut noter la présence de descripteurs de surface
simples décrivant les longueurs des phrases (SENLENGTH), le nombre de signes de ponctuation
(NUMPUNC) ou la longueur moyenne des tokens (AVGTOKENLENGTH). Finalement, la caractéristique
la plus importante pour distinguer les références post-éditées des références libres est fondée sur
la probabilité d’alignement de la traduction avec la source, telle qu’esu'mée par un modele IBM 1,
et quantiﬁe le nombre moyen de mots dont la probabilité d’alignement est plus grande que 0,02.

4 Evaluation de la variabilité des post-éditions

Une autre application du corpus TRACE est l’étude de l’accord inter—annotateur de la post—édition,
puisque, pour chaque direction de traduction, 1 000 traductions ont été corrigées deux fois
indépendamment. A notre connaissance, c’est la premiere fois que deux annotateurs différents
corrigent les memes phrases, permettant une comparaison des post-éditions et une estimation de
l’accord inter—annoteur du score hT'ER. Pour des raisons de place, nous décrirons uniquement les
résultats obtenus sur le corpus de traductions de l’anglais vers le francais. Les résultats pour la
direction francais vers anglais sont similaires.

De maniere uantitative il est ossible de mesurer la similarité entre les ost—éditions effectuées
7
par les différents correcteurs en mesurant la corrélation entre les scores hTER obtenus lorsque

727 © ATALA

TALN-RECITAL 2013, 17-21 Iuin, Les Sables d’Olonne

Distinction hypotheses/post-édition Distinction 1-éférence/post-édition
|  1'lTstSoul  l I25-‘\V8N1l1'ﬂTf31'lS02
|:| FPOSLMT-0817F0b  | fAV8T°k91'l1-91'l8Ih

B :1 rSoul |  FPOSLIW-°8PF°b
 |:l rNumPunc |:| rsoul
‘Q |:l t8tBi8T31T|SF1‘9¢IQ1-13115191 |:| nSrcSoul
5 El nSrcSoul :| rNu1'nPu11c
|:| nTgtBigramsFreqQuam'1e1 |:| rSenLength
[1 tgtAvgTokenLength |:| nTgtNumCaps
0 0,1 0,2 0,3 0,4 0,5 0 0,02 0,04 0,06 0,08 0,1 0,12
Importance de la caractéristique Importance de la caractéristique

FIGURE 1 — Importance des caractéristiques les plus discriminantes pour les deux taches considé—
rées. Les caractéristiques dont le nom commence par un N sont normalisées par la longueur de la
phrase; celles dont le nom commence par un R sont constituées par le rapport entre les valeurs
de la caractéristique calculée sur la phrase source et sur la traduction.

ces corrections sont utilisées comme référence. Cette corrélation est faible : le coefﬁcient de
Pearson entre les deux notes n’est que de 0,642 et le ‘L’ de Kendall de 0,476. L’interprétation est
que si les traductions étaient ordonnées suivant leur score hTER, deux traductions quelconques
ne seraient dans le méme ordre pour les deux références qu’une fois sur deux. De maniére
globale, les post—éditions produites ne sont identiques que dans 12 % des cas4. La distance
d’édition normalisée moyenne entre les deux post—éditions est de 24 % : il faut donc, pour passer
d’une post—édition a l’autre, changer en moyenne un mot sur quatre. Bien qu’elles ne soient
pas directement comparables, puisque dans l’un des cas le score (h)TER n’est pas calculé par
rapport a une référence « adaptée >>, cette valeur est a peine plus petite que celle observée lors de
l’évaluation des sorties de SYSSTAT. Ce résultat illustre les limites de l’évaluation de la TA par des
score (h)TER. Les opérations les plus fréquentes dans cette transformation sont les substitutions
de mots (57 % des modiﬁcations) suivi des suppressions et des insertions de mots (16 % dans les
deux cas) ; les déplacements de mots n’interviennent que dans 11 % des cas.

Plus qualitaﬁvement, le Tableau 4 reprend des exemples des corrections les plus différentes ainsi
que des phrases sources et des traductions automatiques. Ces exemples illustrent la variété des
différences entre les post—éditions qui peuvent étre dues a :

— une sensibilité différente aux traductions littérales : dans de nombreux cas, un correcteur
accepte une traduction parfaitement comprehensible et juste d’un point de Vue grammatical,
meme si elle n’aurait jamais été << produite » par un locuteur natif, alors que le second préfere
la reformuler (49 exemple) ;

— une reformulation non nécessaire de la traduction automatique (le second correcteur qui
corrige << cette réglementation » en << le présent reglement » dans le 1°’ exemple)

— une utilisation de paraphrases ou de synonymes sans raisons apparentes (<< ultramodernes »

4. La comparaison entre les deux corrections ne tient compte ni de la ponctuation, ni de la casse.

728 © ATALA

TALN-RECITAL 2013, 17-21 Iuin, Les Sables d’Olonne

1. source Each year, the Member States shall send the Commission a report on the evaluation of the
execution and effectiveness of this regulation.
trad. autom. Chaque année, les Etats membres transmettent a la Commission un rapport sur l’évaluation

de l’exécution et l’efﬁcacité de cette réglementation.

correction n° 1 Chaque année, les Etats membres transmettent a la Commission un rapport sur l’évaluation
de l’exécution et l’efficacité de cette 1-églementation.

correction n° 2 Chaque année, les Etats membres communiquent a la Commission un rapport d’évaluation
concernant l’exécution et l’efficacité du présent 1-églement.

2. source I’m thinking this must be an ancient print date, right.
trad. autom. Je retiens ce doit étre une date imprimée antique.
correction n° 1 Je pense qu’il s’agit une ancienne édition, c’est évident.
correction n° 2 Je pense que ca doit etre une ancienne date d’impression, n’est-ce pas.

3. source So let’s take a tour of this state-of-the-art clean coal facility.
trad. autom. Donc prenons un tour de cet état de l’art nettoient la facilité de charbon.
correction n° 1 Alors allons voir ces installations ultramodernes de charbon propre.
correction n° 2 Donc faisons une visite de cette installation de charbon propre ‘a la pointe de la technolo-

gie.
4. source Dear Valued Customer, please follow the steps below to have a troubleshooting.
trad. autom. Cher valorisées a la clientele, veuillez suivre les étapes ci-dessous pour avoir un dépannage.

correction n° 1 Cher client estimé, veuillez suivre les étapes ci-dessous pour avoir un dépannage.
correction n° 2 Tres cher client, veuillez suivre les étapes ci-dessous pour étre dépanné.

TABLE 4 — Exemple de différences de post—éditions.

versus << a la pointe de la technologie » dans le 3*’ exemple) ;
— une ambiguité liée au manque de contexte en source (<< cette installation » versus « ces

installations » dans le 39 exemple).
Remarquons que les corrections sont différentes aussi bien quand la traduction automatique est
plut6t bonne (ler exemple) que quand elle est completement fausse (29 et 3*’ exemples).

Ces observations mettent en évidence les limites inhérentes a l’évaluation des systémes de TA
par un score comme hTER : dans la mesure ou la post—édition semble aussi subjective que la
traduction e1le—méme, les scores hTER seront aussi variables et difficiles a interpréter que les
autres métriques automatique utilisées pour évaluer la TA.

5 Conclusion

Nous avons présenté, dans ce travail, un grand corpus de corrections de traductions et illustré
différents types d’analyse que celui—ci rend possible. Bien qu’ils ne soient que préliminaires, les
résultats présentés sont déja riches en enseignements : ils montrent notamment les limites de la
métrique hTER et illustrent une maniere d’idenu'ﬁer les erreurs fréquentes en traduction. D’autres
exploitations sont possibles, notamment en exploitant les annotations complémentaires qui sont
disponibles pour diverses sous—parties du corpus TRACE. Nos travaux futurs ont pour objectif
d’approfondir ces observations et d’arriver a les intégrer dans les systemes de TA aﬁn d’amé1iorer
la qualité des hypotheses produites. Une autre piste de recherche consiste a comparer les erreurs
faites par les systémes de TA aux erreurs faites par les humains en utilisant, par exemple, des
corpus contenant des corrections de traduction (Abekawa et al., 2010).

729 © ATALA

TALN-RECITAL 2013, 17-21 Iuin, Les Sables d’Olonne
Remerciements

Ce travail a été partiellement ﬁnancé par l’Agence Nationale de la Recherche au travers du projet
ANR/ CONTINT-20 10 / TRACE.

Références

ABEKAWA, 'I‘., UTIYAMA, M., SUMITA, E. et KAGEURA, K. (2010). Community—based construction of
draft and ﬁnal translation corpus through a translation hosting site minna no hon’yaku (mnh).
In Proc. of LREC. ELRA.

BREIMAN, L. (2001). Random forests. Mach. Leam., 45(1):5—32.

CALLISON-BURCH, C., KOEHN, P., MONZ, C., POST, M., SORICUT, R. et SPECIA, L. (2012). Findings of
the 2012 workshop on statistical machine translation. In Proc. of WMT, pages 10-51, Montréal,
Canada. ACL.

CETTOLO, M., GIRARDI, C. et FEDERICO, M. (2012). Wit3 : Web inventory of transcribed and
translated talks. In Proc. of EAMT , pages 261-268, Trento, Italy.

CREGO, J. M., YvoN, E et NO, J. B. M. (2011). N—code : an open—source Bilingual N—gram SMT
Toolkit. Prague Bulletin of Mathematical Linguistics, 96:49-58.

GARCIA, I. (2011). Translating by post-editing : is it the way forward? Machine Translation,
25:217-237.

KULESZA, A. et SHIEBER, S. M. (2004). A learning approach to improving sentence-level mt
evaluation. In Proc. of TMI.

LE, H.—S., LAVERGNE, 'I‘., ALLAUZEN, A., APIDIANAKI, M., GONG, L., MAX, A., SOKOLOV, A., WISNIEWSKI,
G. et YvoN, F. (2012). LIMSI @ WMT12. In Proc. of WMT, pages 330-337, Montréal, Canada.
ACL.

LE, H. S., OPARIN, I., ALLAUZEN, A., GAUVAIN, J.-L. et YvoN, E (2011). Structured Output Layer
Neural Network Language Model. In Proceedings of IEEE International Conference on Acoustic,
Speech and Signal Processing, pages 5524-5527, Prague, Czech Republic.

LEFEVER, E. et HOSTE, V (2010). Semeval—2010 task 3 : Cross—lingual word sense disambiguation.
In Proceedings of the 5th International Workshop on Semantic Evaluation, pages 15-20, Uppsala,
Sweden. ACL.

PEDREGOSA, E, VAROQUAUX, G., GRAMFORT, A., MICHEL, V, THIRION, B., GRISEL, 0., BLONDEL, M.,
PRETTENHOFER, P., WEISS, R., DUBOURG, V, VANDERPLAS, J., PAssos, A., COURNAPEAU, D., BRUCHER,
M., PERROT, M. et DUCHESNAY, E. (2011). Scikit—learn : Machine Learning in Python . JMLR,
12:2825—2830.

POTET, M., ESPERANCA-RODIER, E., BESACIER, L. et BLANCHON, H. (2012). Collection of a large
database of French-English SMT output corrections. In Proc. of LREC, Istanbul, Turkey ELRA.
SNOVER, M., DORR, B., SCHWARTZ, R., MICCIULLA, L. et MAKHOUL, J. (2006). A study of translation
edit rate with targeted human annotation. In Proc. of AMTA, pages 223-231.

WISNIEWSKI, G., SINGH, A. K. et YVoN, E (2013). Quality estimation for machine translation :
Some lessons learned. Machine Translation, page accepté pour publication.

ZHUANG, Y., WISNIEWSKI, G. et YvoN, E (2012). Non-linear models for confidence estimation. In
Proc. of WMT, pages 157-162, Montréal, Canada. ACL.

730 © ATALA

