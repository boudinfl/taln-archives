TALN-RECITAL 2013, 17-21 Iuin, Les Sables d’Olonne

Segmentation thématique : processus itératif de pondération
intra-contenu

Abdessalam Bouchekif(1-2)- Géraldine Damnati(1), Delphine Charlet(1)
(1) Orange Labs , 2, Avenue Pierre Marzin 22307 Lannion Cedex
(2) Laboratoire d'Informatique de l'Universite du Maine, LIUM - France

{abdessalam.bouchekif,geraldine.damnati,delphine.char1et}@orange.com

RESUME

Dans cet article, nous nous intéressons 51 la segmentation thématique d'émissions télévisées
exploitant la cohésion lexicale. Le but est d'étudier une approche générique, reposant
uniquement sur la transcription automatique sans aucune information externe ni aucune
information structurelle sur le contenu traité. L'étude porte plus particuliérement sur le
mécanisme de pondération des mots utilisés lors du calcul de la cohésion lexicale. Les poids
TF-IDF sont estimés £1 partir du contenu lui-méme, qui est considéré comme une collection
de documents mono-theme. Nous proposons une approche itérative, intégrée £1 un
algorithme de segmentation, visant £1 raffiner la partition du contenu en documents pour
l'estimation de la pondération. La segmentation obtenue £1 une itération donnée fournit un
ensemble de documents 51 partir desquels les poids TF-IDF sont ré-estimés pour la prochaine
itération. Des expériences menées sur un corpus couvrant différents formats des journaux
télévisés issus de 8 chaines francaises montrent une amélioration du processus global de
segmentation.

ABSTRACI‘

An iterative topic segmentation algorithm with intra-content term weighting

This paper deals with topic segmentation of TV Broadcasts using lexical cohesion. The aim is
to propose a generic approach, only relying on the automatic speech transcription with no
external nor a priori information on the TV content. The study focuses on a new weighting
scheme for lexical cohesion computation. TF-IDF weights are estimated from the content
itself which is considered as a collection of mono-thematic documents. We propose an
iterative process, integrated to a segmentation algorithm, aiming to reﬁne the partition of a
content into documents in order to estimate the weights. Topic segmentation obtained at a
given iteration provides a set of documents from which TF-IDF weights are re-estimated for
the next iteration. An experiment on a rich corpus covering various formats of Broadcast
News shows from 8 French TV channels improves the overall topic segmentation process.

MOTS-CLES : Segmentation thématique, pondération TF-IDF, cohésion lexicale, Text'I‘iling
KEYWORDS : Topic segmentation, TF-IDF weighting, lexical cohesion, Text'I‘iling

1 Introduction

La segmentation thématique consiste £1 effectuer un pavage d'un document (texte classique,
audio ou vidéo) en segments thématiquement homogénes. Plusieurs programmes de
recherche se sont attachés £1 traiter la segmentation thématique de journaux télévisés (]'I‘)
mais le probléme demeure d'actualité et doit étre considéré avec de nouvelles perspectives

739 © ATALA

TALN-RECITAL 2013, 17-21 Iuin, Les Sables d’Olonne

pour pouvoir traiter des contenus a la ligne éditoriale de plus en plus variée. En particulier,
la structuration traditionnelle d'un IT ou le présentateur principal, en plateau, introduit un
nouveau sujet suivi d'un reportage ou d'une interview, tend a étre substituée ou complétée
par des mises en scenes plus modernes. Dans certains IT sont intercalées des breves lues par
le présentateur principal ou par un autre journaliste, sans qu'un reportage ne vienne
illustrer le propos (c'est le cas du journal d'Arte par exemple). Au contraire, certains IT
contiennent une succession de reportages, sans retours plateaux et sans introduction par le
présentateur principal (c'est le cas du journal du soir de France 3 qui inclut en fin de
programme une succession de reportages issus des éditions régionales, ainsi que certains IT
de M6 ou d'Euronews qui n'ont pas du tout de présentateur principal). La plupart des études
dans la littérature ont porté sur des corpus de IT de format traditionnel. Une des
particularités du présent travail est d'étre mené sur un corpus varié de IT issus de 8 chaines
différentes, de durée et de format divers.

Dans la littérature, trois catégories d'indices ont été exploitées: des indices lexicaux,
acoustiques et visuels. La combinaison de ces indices est en regle générale proﬁtable a la
tache de segmentation (Wang et al., 2012). Cependant, les deux derniers sont fortement liés
aux regles éditoriales de chaque chaine télévisée (Xie et al. 2010): présence ou non d'un
présentateur principal, présence ou non de titres incrustés ou de logos.

Notre objectif étant de développer un systeme de segmentation thématique générique, nous
avons fait le choix de privilégier les indices lexicaux qui révelent des frontieres a partir de
variations sémantiques dans un contenu, indépendamment de toute sorte d'information
structurelle sur l'émission traitée. L'exploitation spéciﬁque d'informations structurelles peut
améliorer les performances comme dans (Bouchekif et al., 2013), mais nous cherchons ici a
améliorer en amont l'approche générique basée sur la cohésion lexicale.

Plusieurs algorithmes de segmentation thématique basés sur la cohésion lexicale ont été
proposés dans la littérature (voir par exemple (Eisenstein et Barzilay, 2008) pour une revue
des approches). Les algorithmes varient tant du point de vue de la méthode de détection des
frontieres que du point de vue de la mesure de similarité (y compris des approches en
recrudescence a base de Latent Semantic Analysis). Méme si l'approche de TEXTTILING
(Hearst, 1997) initialement concue pour segmenter du texte s'est avérée peu performante
sur des contenus audiovisuels (Claveau et Lefevre, 2011) (Guinaudeau et al., 2010), nous
avons néanmoins choisi d'adopter ce schéma de facon a en explorer deux dimensions. La
premiere est la méthode de sélection des frontieres a partir de la courbe de similarité et la
seconde est le mécanisme de pondération des mots utilisé pour calculer une valeur
pertinente de cohésion lexicale. Ce choix n'est néanmoins pas restrictif et les propositions
développées dans cet article peuvent s'appliquer a des algorithmes plus sophistiqués.
L'article est structuré de la facon suivante: la section 2 présente notre algorithme de
segmentation thématique, la section 3 présente une évolution vers une approche intégrée
itérative qui permet de raffiner la pondération TF-IDF des mots. Les expériences sont
présentées dans la section 4.

2 Algorithme de segmentation thématique

Comme pour l'algorithme TEXTTILING, la similarité est calculée entre chaque paire de blocs
adjacents. Les segments unitaires considérés sont des groupes de soufﬂe (GS), c'est-a-dire
des séquences de mots entre deux pauses dans un tour de parole. Les pauses et les
changements de locuteur sont détectés automatiquement par le systeme de transcription

740 © ATALA

TALN-RECITAL 2013, 17-21 Iuin, Les Sables d’Olonne

automatique. La similarité est donc calculée tout au long de l'émission a l'aide d'une fenétre
glissante de taille KC entre des blocs adjacents de K GS de part et d'autre de la frontiere
potentielle. 11 en résulte une courbe de cohésion lexicale a partir de laquelle sont extraites les
hypotheses de frontieres. Dans les deux premieres sous-sections, nous décrivons comment
sont réalisés la pondération des termes et le calcul de similarité lexicale. Nous proposons
ensuite un algorithme de sélection des frontieres a partir de la courbe de cohésion obtenue.

2.1 La pondération TF-IDF intra-document

La pondération TF-IDF est largement utilisée en recherche d'information (RI) pour évaluer
le pouvoir discriminant d'un terme t dans un documentd (via TF : fréquence locale du
terme), relativement a une collection de documents (via IDF : fréquence globale inverse du
terme). Dans le cadre de la segmentation thématique, la pondération des mots permet
d'augmenter la pertinence des mesures de similarité lexicale, en renforcant la contribution
de certains mots dans l'estimation de ces mesures. Dans le domaine de la segmentation de
contenus du type information (journaux télévisés, journaux radiophoniques, émissions de
reportages), les poids sont généralement estimés par un large corpus. Par exemple,
(Guinaudeau et Hirschberg, 2011) utilisent l'outil kiwi (Lecorvé et al., 2008) qui produit des
poids estimés a partir d'une collection de 800000 articles du journal Le Monde. Aﬁn de nous
affranchir de la contrainte de disposer d'une base d'apprentissage, nous nous proposons de
suivre l'approche donnée dans (Malioutov et al., 06) ou les auteurs introduisent une
pondération intra-document pour le domaine de la segmentation thématique de conférences.
Sans aucune information externe, les poids TF-IDF sont estimés uniquement a partir du
contenu en question. Le principe est de découper uniformément l'émission en N morceaux
(ou chunk). Chaque chunk est une succession de groupes de soufﬂes et correspond a
l'équivalent d'un document en RI. Le terme t dans le groupe de soufﬂe x est associé au poids
w(c(x), t) qui dépend du chunkc(x) dans lequel se trouve x.

w(c(x), t) = TFCW. x IDFt,o1‘1 IDFt = log  (1)

oil TFc(,,),t est la fréquence du terme t dans le morceau c(x) et 11,; est le nombre de chunks
dans lequel le terme t apparait.

Cette approche permet de faire ressortir les mots discriminants dans un passage de
l'émission relativement aux autres passages. Des expériences utilisant d'autres pondérations
comme Okapi n'ont pas permis d'améliorer les performances de la segmentation.

2.2 Calcul de similarité

La mesure cosinus permet de mesurer la proximité entre la représentation vectorielle de
deux blocs adjacents 17,- et b,-+1. Le coefficient associé au terme t dans la représentation
vectorielle d'un bloc b est une valeur pondérée v(b,t). Dans notre approche, il n'y a pas
unicité de la pondération TF-IDF dans un bloc donné car les groupes de soufﬂes du bloc
peuvent ne pas appartenir tous au méme chunk Ainsi le coefficient associé a t dans le bloc b
est obtenu en sommant les fréquences pondérées du terme tdans chaque GS du bloc :

120:. r) = Z (fx; >< w(c(x). r)) (2)

xeb
ou f,,,t est la fréquence du terme tdans le GS X

Pour une frontiere potentielle j entre deux blocs bj et bl-+1, la similarité est donnée par

741 © ATALA

TALN-RECITAL 2013, 17-21 Iuin, Les Sables d’Olonne
Xt(”(1’1'vt) X ”(17j+1»t))

T ,—Z- <3)
2; (‘l7(l7]',t)) X 2; (‘l7(l7]'+1,t))

Le nombre de chunks N est calculé automatiquement pour chaque émission en fonction de
sa durée et de la durée moyenne des themes de l'ensemble d'émissions.

cohesion(i) =

2.3 Algorithme de division récursive (Splitting)

Plusieurs stratégies ont été introduites pour sélectionner les frontieres a partir de la courbe
de similarité. L'approche classique (Hearst, 1997) consiste a détecter les vallées (un point
entouré par deux pics) et a calculer leur profondeur en faisant la somme des deux
différences (entre le point et le pic a gauche d'une part et le point et le pic a droite d'autre
part). Les vallées dont la profondeur dépasse un certain seuil (approche dite par seuillage)
sont considérées comme des points de transition thématique. Il faut noter que les points qui
ne correspondent pas a des vallées valent 0. Il peut se produire que plusieurs vallées
profondes apparaissent dans un court intervalle de temps, ou bien qu'un changement
thématique se traduise par une succession de vallées de profondeur limitée. (Lu et al., 2011)
proposent une approche basée sur la programmation dynamique pour optimiser
globalement la recherche des frontieres dans la courbe. (Claveau et Lefevre, 2011) ont
proposé d'appliquer en plus d'une métrique alternative basée sur la vectorisation,
l'algorithme dit Ligne de Partage des Eaux (LPE) issu de la morphologie mathématique pour
réaliser un partitionnement de l'émission a partir de la courbe.

Pour améliorer la robustesse de l'extraction des frontieres, nous proposons un nouvel
algorithme avec deux particularités : premierement nous proposons d'exploiter
conjointement la similarité lexicale et la profondeur des vallées, et deuxiemement nous
avons implémenté un algorithme itératif de partitionnement d'une émission a partir de la
courbe. Il résulte de la premiere observation que la recherche directe sur les valeurs de
similarité n'est pas optimale (certains changements de themes entre deux sujets proches, sur
un méme pays par exemple, peuvent se traduire par une similarité relativement importante).
De facon similaire, travailler uniquement sur la profondeur des vallées n'est pas optimal:
(les pics de part et d'autre peuvent ne pas étre tres hauts si un sujet ne contient que peu de
répétitions de termes). Nous proposons ainsi de combiner ces deux mesures
complémentaires a l'aide d'une interpolation linéaire. Pour une frontiere potentielle j, le
score suivant doit étre maximisé :

score(i) = /1 (1 — cohesion(i)) + (1 — /1) depth (i)) . (4)
La deuxieme proposition est un algorithme de division récursive lors duquel est déﬁnie une

zone d'exclusion autour des frontieres trouvées a chaque itération. Le partitionnement
consiste a construire un ensemble Sde segments :

1. Initialement, Scontient un seul segment constitué de l'émission entiere.
2. Chaque segment de 5 est coupé en deux, le point de coupure correspond a la valeur
maximale du score, si cette valeur dépasse un seuil donné.
3. Les GS situés autour du point de coupure ne seront pas pris en considération lors de
la prochaine itération.
4. Les segments obtenus sont présentés a l'étape 2.
L'étape 3 permet de limiter les phénomenes de maxima locaux et garantit que l'on
n'obtiendra pas plusieurs frontieres consécutives. La zone de neutralisation est ﬁxée a 3 GS

742 © ATALA

TALN-RECITAL 2013, 17-21 Iuin, Les Sables d’Olonne

de part et d'autres d'une frontiere. L'algorithme s'arréte lorsqu'aucun point de coupure
candidat ne dépasse le seuil. Cette approche par zone d'exclusion s'est avérée plus efficace
qu'un lissage de la courbe pour limiter l'effet des maxima locaux. La granularité des groupes
de soufﬂes est trop grande pour envisager un lissage efficace sans perte d'information.

3 Pondération itérative intra-document

Dans cette section, nous introduisons une variation de la pondération TF-IDF intra-
document. Le principe initial présenté dans la section 2.1 consistait a découper
uniformément le contenu en N chunks simulant la notion de document Au-dela de ce
découpage uniforme, nous proposons une approche itérative, utilisant les résultats de notre
algorithme de segmentation thématique pour déterminer les chunks. La segmentation
obtenue a une itération donnée fournit un ensemble de documents a partir desquels les

poids TF-IDF sont ré-estimés pour l'itération suivante.

Initialement, le document est coupé en N morceaux uniformes. Le nombre de chunks N est
obtenu automatiquement pour chaque émission en divisant la durée de l'émission par une
durée moyenne de segments thématiques estimée sur un corpus de développement. L'indice
du premier GS de chaque chunk uniforme est considéré comme l'ensemble initial de
frontieres et est placé dans le vecteur hypo. A l'itération i, les hypotheses de l'itération i — 1
(hyp,-_1) sont utilisées pour estimer les pondérations TF-IDF (i). La combinaison linéaire
entre la cohésion lexicale et la profondeur des vallées est recalculée. Ensuite, l'algorithme de
division récursive est appliqué pour déterminer les hypotheses hypi .

L'algorithme s'arréte lorsque la segmentation se stabilise (pas de changement signiﬁcatif
entre les hypotheses de deux itérations successives hyp,- et hyp,-+1). Afin de mesurer
objectivement cette stabilisation, nous utilisons la mesure pk (Beeferman et al., 1999).

La mesure pk compare une segmentation de référence R et une hypothese de segmentation
H Elle est basée sur le principe d'une fenétre glissante de taille k parcourant la totalité de
l'émission. Dans cette métrique, la tache de segmentation est vue comme un probleme de
classification binaire répondant a la question : le groupe de souffle jet le groupe de soufﬂe
/'+1(appartiennent-ils au meme segment '.7 pk mesure la probabilité que deux GS distants de A’
soient classés de la meme facon par Ret par H

n—k
pk(RrH) = n:kZf(f(1l'77+k)r(h-jrh-j+k)) (5)
j=1

La fonction f vaut 1 si ses deux arguments sont identiques, sinon elle vaut 0. Plusieurs
études ont mis en exergue qu'une faible valeur de A’ favorise la précision de cette mesure.
Dans notre implémentation, la valeur de k est ﬁxée a 6. L'algorithme s'arréte lorsque la
valeur de pk entre hyp,-_1 et hypi est proche de 1 (1 — pk(hyp,-_1, hyp,-)) S 6.

Il faut noter que nous n'avons pas encore étudié la preuve de la convergence de l'algorithme.
L'algorithme s'arréte au bout de 6 itérations si le critere d'arrét n'a pas atteint la valeur du
seuil 5. En pratique trois a quatre itérations sont suffisantes.

4 Expériences et résultats

Les expériences sont menées sur deux corpus d'émissions. Le premier pour le
développement (Dev) est constitué de 33 IT de 7 chaines francaises (TF1, France2, France3,

743 © ATALA

TALN-RECITAL 2013, 17-21 Iuin, Les Sables d’Olonne

LCI, France24, Arte, M6). Le deuxieme pour le test (Test) est composé de 6 IT d'une autre
chaine : Euronews. La particularité de ce corpus est qu'il n'y a ni présentateur principal ni
plateau, il s'agit uniquement d'une succession de reportages, chacun associé a un reporter
différent et de longueur variable. Le tableau 1 résume les caractéristiques de ces deux corpus.

Dev Test
Nombre d’émissions 33 6
Durée moyenne ~ 22 min ~ 26 min
Nombre de frontiéres (par IT) 397 (11,5) 156 (2 6,0)
Durée moyenne des thémes 115 s 79 s

TABLE 1 — Description des corpus

Ces émissions ont été transcrites a l'aide du moteur de reconnaissance automatique de la
parole de Vocapia Research basé sur le systeme du LIMSI (Gauvain et al., 2002). Le taux
d'erreurs mots sur le corpus de Devest de 16,1%. Nous ne pouvons donner les performances
du corpus Test qui n'a pas été transcrit manuellement. Les mots qui ont un score de
confiance inférieur a0.5 sont écartés. Les pré-traitements classiques ont été appliqués:
lemmatisation (Lia_tagg : http://pageperso.lif.univ-mrs.fr/~frederic.bechet/download.html),
suppression de certains mots non porteurs de sens a partir d'une stop-liste. Par ailleurs, de
facon similaire a (Guinaudeau et al., 2010) nous avons écarté la premiere partie d'un journal
lorsqu'elle ne contient que des titres et la derniere partie lorsqu'il s'agit du rappel des titres.
La définition d'un segment est plus large que la simple notion de reportage, il s'agit d'un
segment thématiquement cohérent L'annotation manuelle a été validée par deux
annotateurs. Pour les cas ambigus (comme un long passage dans un journal relatant diverses
conséquences des chutes de neiges ou dela crise économique), nous avons fait le choix de
segmenter en sujet pour chaque cons équence.

La taille de la fenétre pour la détermination de la courbe de cohésion lexicale a été optimisée
sur le Dev et a été fixée a 16 groupes de soufﬂes. Le coefficient A d'interpolation pour le
calcul du score a été fixé a 0,75. Le seuil 5 sur la mesure pk pour le critere d'arrét de
l'algorithme itératif vaut 0,09. Les performances sont mesurées en termes de rappel et de
précision, en comparant la segmentation de référence avec celle d'hypothese. De facon
similaire a plusieurs travaux dans la littérature, une tolérance de 10 s a été autorisée entre
les frontieres d'hypotheses et de références.

La ﬁgure 1 illustre l'importance de l'algorithme de sélection et du score de similarité. Les
courbes rappel/précision ont été obtenues en faisant varier le seuil présent dans chacune
des approches. Les 6 courbes de la ﬁgure 1 représentent la combinaison des trois scores
(cohésion, profondeur de vallée et l'interpolation des deux) et des deux algorithmes de
sélection (seuillage et splitting). La pondération TF-IDF est calculée selon une partition
uniforme de l'émission en N chunks. Seuillage (vallee) correspond a l'algorithme de
TEXTTILING de base. Splitting (cohesion + vallee) correspond a notre proposition. La
combinaison linéaire des deux scores associée a l'algorithme de splittingaugrnente la F-max
de 12,5 points. On observe plus de fausses alarmes (faible précision) avec l'approche
seuillage (toutes les hypotheses dépassant le seuil sont prises en compte), contrairement a
l'approche splitting avec une zone d'exclusion qui permet d'avoir une meilleure précision
dans les zones de plus fort rappel. Les trois courbes correspondant a l'approche splitting
montrent clairement que l'interpolation de la cohésion et la profondeur des vallées est une
bonne stratégie, avec une F-max finale de 55,3% sur le Dev.

744 © ATALA

TALN-RECITAL 2013, 17-21 Iuin, Les Sables d’Olonne

100 100

spnmng (cohesion + vallee) Pmax : 55 3 —n— Sang mar pmax : 35 9 _._

Seulllage (cohesion + vallee): 53 00 + Algorﬂhrne de base pmax : 55 3 _.._

SD|‘lW7Q(C071e5‘0”) Pmax 1 52 5 —-— Algorllhme iteratif Pmax : so 0 —o—

80 spnmng (vallee) Pmax : 43.5 —u— oracle pmax : 68 3 _._
seumage (vallee) Pmax : 42 8 —a— 80

Seulllage (cohesion) Pmax : 31 5 —o—

‘\
4“ EA 4° ‘\
20 \ 20

0 20 40 so 80 100 0 20 40 60 80 100

Rappel Rappel
FIGURE 1 — Impact de la stratégie de FIGURE 2 — Impact de la stratégie
sélection et de calcul du score itérative de pondération.

Pour le reste des expériences, Splitting (cohesion + vallee) est systématiquement utilisé.
Lorsque nous utilisons la pondération TF-IDF avec le découpage uniforme pour les chunks,
cette technique sera dénommée baseline. Aﬁn de montrer l'intérét de la pondération
itérative, nous comparons notre approche a une version dans laquelle aucune pondération
ne serait utilisée (similarité calculée uniquement a partir de la fréquence) et a une version
ou la pondération TF-IDF serait calculée a partir de la segmentation thématique réelle,
obtenue a partir des frontieres de référence (condition Oracle). Les résultats de la ﬁgure 2
montrent que les meilleures performances sont obtenues dans les conditions Oracle, avec
une importante marge de progression par rapport a la baseline (de 553% a 68,3% de F-
max) confortant ainsi le potentiel de notre proposition. Les performances sont sérieusement
dégradées lorsqu'aucune pondération n'est appliquée.

Le systeme itératif améliore la baseline, permettant de passer d'une F-max de 553% a une F-
max de 60.0%. Pres de 30% de l’écart entre la condition baseline et la condition Oracle a pu
étre comblé. Enfin, nous avons validé l'approche sur un nouvel ensemble de ]Ts issus
d'Euronews. Le tableau 2 illustre les résultats obtenus sur ce corpus en choisissant le seuil
optimal établi de facon a atteindre la F-max sur le corpus de Dev.

Condition de pondération Rappel Précision F — mesure
Algorithme de base 46,8 59,3 52,3
Itératif 53,8 62,7 57,9
Oracle 57,7 69,8 63,2

TABLE 2 — Résultats sur le Test (Euronews)

Les mémes tendances peuvent étre observées, avec une meilleure couverture de l’écart entre
la baseline et l'Oracle. En analysant les résultats, il s'avere que l'algorithme itératif permet de
retrouver des frontieres entre des sujets proches (deux sujets sportifs, deux reportages
consécutifs sur un méme pays). Une évaluation plus fine de la pondération TF-IDF permet de
remonter ce type de frontieres difficilement accessibles pour les approches lexicales.

5 Conclusion
Cet article propose une approche de segmentation thématique s'appuyant uniquement sur le
contenu lexical d'un ]ournal Télévisé. L'objet de notre approche est de développer une

méthode générique pouvant s'appliquer a tout type de ]T, indépendamment de sa structure.

745 © ATALA

TALN-RBCITAL 2013, 17-21 Iuin, Les Sables d’Olonne

A partir d'un algorithme classique de segmentation thématique basé sur la cohésion lexicale
locale (TEX'I"I‘ILING), deux modiﬁcations sont proposées : la premiere portant sur le processus
d'extraction des frontieres thématiques a partir de la courbe de cohésion et la seconde
portant sur l'estimation des poids associés aux mots pour l'estimation de cette cohésion. Les
deux propositions se traduisent par une amélioration signiﬁcative des performances de
segmentation sur un corpus diversiﬁé de ]T issus de 8 chaines. L'approche itérative de calcul
de la pondération TF-IDF a partir du contenu lui-méme n'est pas limitée a notre algorithme

mais peut avoir une portée beaucoup plus large dans de nombreux contextes d'utilisation.
Références

BEEFERMAN, D., BERGER, A., et LAFFERTY, ]. D. (1999). Statistical models for text segmentation,
Machine Learning pages 177-2 10.

BOUCHEKIF, A., DAMNATI, G., et CHARLET, D. (2013). Complementarity of Lexical Cohesion and
Speaker Role Information for Story Segmentation of French TV Broadcast News. In Proc. of
SLSP.

CLAVEAU, V., et LEFEVRE, S. (2011). Segmentation thématique : apport de la vectorisation,
Actes de la conférence CORIA.

EISENSTEIN, ]. et BARZILAY, R. (2008). Bayesian Unsupervised Topic Segmentation, In Proc.
EMNLR

GAUVAIN, ].L., LAMEL. et ADDA, G. (2002) The LIMSI Broadcast News Transcription System.
Speech Communication, pages 89-108.

GUINAUDEAU C., GRAVIER G. et SEBILLOT P. (2010). Utilisation de relations sémantiques pour
améliorer la segmentation thématique de documents télévisuels, In Proc. TALN

GUINAUDEAU, C., et HIRSCHBERG, ]. (2011). Accounting for prosodic information to improve asr-
based topic tracking for TV Broadcast. In Proc. oflnterspeecli.

HEARST, M. (1997). Text'I‘iling: segmenting text into multiparagraph subtopic passages,
Computational Linguistics, pages 33-64.

LECORVE, G., et GRAVIER, G. (2008). An unsupervised web-based topic language model
adaptation method". In Proc. ofICASSR

LU, M., LEUNG, C., XIE, L., MA, B., et LI, H. (2011). Probabilistic Latent Semantic Analysis for
Broadcast News Story Segmentation, In Proc. oflnterspeecb.

MALIOUTOV, I., et BARZILAY, R. (2006). Minimum cut model for spoken lecture segmentation.
In Proc. ACL, pages 25-32.

WANG, X., XIE, L., MA, B., CHNG, E.-S. et Li. H. (2012). Broadcast News Story Segmentation
Using CRF and Multi-modal Features. IEICE Transactions on Information and Systems, pages
1206-1215.

XIE, L., YANG, Y., LIU, Z-Q, FRENG. W. et LIUM, Z. (2010). Integrating Acoustic and Lexical
Features In Topic Segmentation of Chinese Broadcast News Using Maximum Entropy
Approach. In Proc. ofICAI.IR

746 © ATALA

