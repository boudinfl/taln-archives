<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
"http://www.w3.org/TR/html4/loose.dtd">
<html><head><title>Apprentissage symbolique et statistique pour le chunking:comparaison et combinaisons</title>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
</head>
<body>
<div style="page-break-before:always; page-break-after:always"><div><p>Apprentissage symbolique et statistique pour le chunking:
comparaison et combinaisons
</p>
<p>Isabelle Tellier, Yoann Dupont
Laboratoire Lattice, 1 rue Maurice Arnoux, 92320 Montrouge
</p>
<p>R&#201;SUM&#201;
Nous d&#233;crivons dans cet article l&#8217;utilisation d&#8217;algorithmes d&#8217;inf&#233;rence grammaticale pour la t&#226;che
de chunking, pour ensuite les comparer et les combiner avec des CRF (Conditional Random
Fields), &#224; l&#8217;efficacit&#233; &#233;prouv&#233;e pour cette t&#226;che. Notre corpus est extrait du French TreeBank.
Nous proposons et &#233;valuons deux mani&#232;res diff&#233;rentes de combiner mod&#232;le symbolique et mod&#232;le
statistique appris par un CRF et montrons qu&#8217;ils b&#233;n&#233;ficient dans les deux cas l&#8217;un de l&#8217;autre.
</p>
<p>ABSTRACT
Symbolic and statistical learning for chunking : comparison and combinations
</p>
<p>We describe in this paper how to use grammatical inference algorithms for chunking, then
compare and combine them to CRFs (Conditional Random Fields) which are known efficient for
this task. Our corpus is extracted from the FrenchTreebank. We propose and evaluate two ways
of combining a symbolic model and a statistical model learnt by a CRF, and show that in both
cases they benefit from one another.
</p>
<p>MOTS-CL&#201;S : apprentissage automatique, chunking, CRF, inf&#233;rence grammaticale, k-RI, French
TreeBank.
</p>
<p>KEYWORDS: machine learning, chunking, CRF, grammatical inference, k-RI, French TreeBank.
</p>
<p>1 Introduction
</p>
<p>L&#8217;apprentissage automatique supervis&#233;, surtout lorsqu&#8217;une grande quantit&#233; de donn&#233;es annot&#233;es
est disponible, a largement prouv&#233; son efficacit&#233; pour les t&#226;ches de fouille de textes classiques
comme la classification ou l&#8217;annotation. Les bases th&#233;oriques des techniques d&#8217;apprentissage les
plus performantes rel&#232;vent en g&#233;n&#233;ral des statistiques (Naive Bayes), de l&#8217;optimisation (SVM) ou
des deux (HMM, CRF). L&#8217;inconv&#233;nient principal des mod&#232;les produits par ces m&#233;thodes est qu&#8217;ils
sont difficilement lisibles par un humain.
</p>
<p>Il existe pourtant aussi d&#8217;autres branches de l&#8217;apprentissage automatique, qualifi&#233;es de symbolique,
qui ont la particularit&#233; d&#8217;offrir une sortie g&#233;n&#233;ralement plus lisible par un &#234;tre humain. Les plus
illustres membres de cette famille sont les arbres de d&#233;cision, la Programmation Logique Inductive
(PLI) ou l&#8217;Inf&#233;rence Grammaticale (IG par la suite) (de la Higuera, 2010). C&#8217;est cette derni&#232;re qui
nous int&#233;resse ici. On peut la d&#233;finir comme l&#8217;&#233;tude des techniques permettant d&#8217;apprendre une
grammaire formelle ou tout autre mod&#232;le capable de repr&#233;senter un langage (comme un automate,
une expression r&#233;guli&#232;re, etc...) &#224; partir d&#8217;exemples de s&#233;quences (&#233;ventuellement enrichies)
</p>
<p>TALN-R&#201;CITAL 2013, 17-21 Juin, Les Sables d&#8217;Olonne
</p>
<p>19 c&#65535; ATALA</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>appartenant (ou non) &#224; ce langage. Ce domaine, qui a son origine dans l&#8217;informatique th&#233;orique
et la th&#233;orie des langages formels, est souvent m&#233;connu. Les algorithmes d&#8217;IG sont en effet
r&#233;put&#233;s ne pas tr&#232;s bien se comporter sur des donn&#233;es r&#233;elles : ils sont souvent algorithmiquement
complexes, sensibles aux erreurs et peu adapt&#233;s aux langages fond&#233;s sur de grands alphabets (ce
qui est le cas quand l&#8217;alphabet est l&#8217;ensemble des mots d&#8217;une langue naturelle).
</p>
<p>Dans cet article, nous voulons donner leur chance &#224; des algorithmes classiques d&#8217;IG pour les
comparer aux m&#233;thodes d&#8217;apprentissage automatique statistique &#233;tat de l&#8217;art, en l&#8217;occurrence
les CRF (Lafferty et al., 2001). La t&#226;che consid&#233;r&#233;e est le chunking (Abney, 1991) du fran&#231;ais,
qui peut en effet tr&#232;s bien &#234;tre r&#233;alis&#233;e &#224; l&#8217;aide d&#8217;automates construits manuellement (Antoine
et al., 2008; Blanc et al., 2010). &#192; notre connaissance, essayer d&#8217;apprendre automatiquement ces
automates au lieu de les &#233;crire &#224; la main n&#8217;a encore pas jamais &#233;t&#233; test&#233;, pour quelque langue que
ce soit. Par ailleurs, le chunking peut &#233;galement &#234;tre vu comme une t&#226;che d&#8217;annotation (objet de
la Shared Task CoNLL&#8217;2000) et de ce fait abord&#233; via des m&#233;thodes d&#8217;apprentissage statistique. Ce
contexte nous semblait par cons&#233;quent id&#233;al pour comparer les deux approches.
</p>
<p>Cette comparaison n&#8217;est cependant pas notre seul but. Notre intuition est que les deux tech-
niques sont compl&#233;mentaires car elles se concentrent sur des propri&#233;t&#233;s distinctes des donn&#233;es
d&#8217;apprentissage. Nous proposons donc &#233;galement dans cet article deux mani&#232;res diff&#233;rentes de
les combiner, en fonction du but vis&#233;. La premi&#232;re mani&#232;re est orient&#233;e vers l&#8217;efficacit&#233; : elle vise
&#224; enrichir un mod&#232;le CRF &#224; l&#8217;aide d&#8217;informations extraites des automates. La seconde privil&#233;gie
la lisibilit&#233; : elle propose d&#8217;analyser les automates appris par IG &#224; l&#8217;aide de poids calcul&#233;s par un
CRF, poids qui seront tous interpr&#233;tables relativement &#224; cet automate.
</p>
<p>L&#8217;article suit le plan suivant. Dans la premi&#232;re section, nous introduisons la t&#226;che de chunking et
d&#233;crivons les donn&#233;es utilis&#233;es pour nos exp&#233;riences. La deuxi&#232;me section est d&#233;di&#233;e &#224; l&#8217;inf&#233;rence
grammaticale. Apr&#232;s un bref &#233;tat de l&#8217;art, nous d&#233;taillons la famille des algorithmes k-RI (Angluin,
1982) et donnons les meilleurs r&#233;sultats exp&#233;rimentaux qu&#8217;ils permettent d&#8217;atteindre pour le
chunking. Dans la section qui suit, nous appliquons les CRF &#224; la m&#234;me t&#226;che. Comme on pouvait
s&#8217;y attendre, les CRF donnent de bien meilleurs r&#233;sultats que ceux obtenus par IG. Dans la
derni&#232;re section, nous d&#233;crivons et &#233;valuons deux mani&#232;res de combiner automates et CRF. Les
r&#233;sultats obtenus pour chacune de ces combinaisons sont prometteurs et sugg&#232;rent des pistes
originales pour associer mod&#232;les symboliques et apprentissage statistique.
</p>
<p>2 Chunking: la t&#226;che et les donn&#233;es
</p>
<p>Nous d&#233;crivons ici la t&#226;che de chunking par annotation et nous pr&#233;sentons les donn&#233;es
d&#8217;apprentissage que nous avons utilis&#233;es pour nos exp&#233;riences. Ces derni&#232;res reprennent et
prolongent celles pr&#233;sent&#233;es dans (Tellier et al., 2012). Notre but &#233;tant de construire un chunker
pour le fran&#231;ais, nous sommes partis du French Tree Bank (Abeill&#233; et al., 2003).
</p>
<p>2.1 La t&#226;che
</p>
<p>La t&#226;che de chunking, &#233;galement appel&#233;e analyse syntaxique de surface, a pour but d&#8217;identifier les
groupes syntaxiques &#233;l&#233;mentaires des phrases. Les chunks sont en effet des s&#233;quences contig&#252;es et
non-r&#233;cursives d&#8217;unit&#233;s lexicales li&#233;es &#224; une unique t&#234;te forte (Abney, 1991). Chacun est caract&#233;ris&#233;
</p>
<p>TALN-R&#201;CITAL 2013, 17-21 Juin, Les Sables d&#8217;Olonne
</p>
<p>20 c&#65535; ATALA</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>par le type (ou &#233;tiquette Part-Of-Speech (POS)) de sa t&#234;te. Il y a ainsi autant de types de chunks
que de types de t&#234;tes fortes possibles.
</p>
<p>La t&#226;che de chunking a fait l&#8217;objet de de la comp&#233;tition CoNLL&#8217;20001, dont le corpus
d&#8217;apprentissage &#233;tait constitu&#233; d&#8217;environ 9 000 phrases issues du Penn Treebank, associ&#233;es
&#224; deux niveaux d&#8217;annotion : un niveau POS donn&#233; par l&#8217;&#233;tiqueteur Brill et un de chunking. Les
vainqueurs avaient utilis&#233; des SVM et des &#8220;Weighted Probability Distribution Voting&#8221;. Ce m&#234;me
corpus a aussi servi plus tard &#224; montrer l&#8217;efficacit&#233; des CRF (Sha and Pereira, 2003).
</p>
<p>2.2 Les donn&#233;es
</p>
<p>Le French TreeBank (FTB) est un recueil de phrases extraites d&#8217;articles du journal &#8220;Le Monde&#8221;
publi&#233;s entre 1989 et 1993 (Abeill&#233; et al., 2003). Les phrases ont &#233;t&#233; tokenis&#233;es (en conservant
certaines unit&#233;s multi-mots), lemmatis&#233;es, &#233;tiquet&#233;es et analys&#233;es syntaxiquement. Il existe
plusieurs variantes du FTB, celle que nous avons utilis&#233;e contenait environ 8 600 arbres XML
enrichis de fonctions syntaxiques (parfois n&#233;cessaires pour identifier certains chunks). Pour le
POS, nous avons repris les 30 &#233;tiquettes morpho-syntaxiques d&#233;finies dans (Crabb&#233; and Candito,
2008), assurant ainsi la continuit&#233; avec nos pr&#233;c&#233;dents travaux (Constant et al., 2011).
</p>
<p>Nous consid&#233;rons 7 types de chunks distincts : AP (Adjectival Phrase), AdP (Adverbial Phrase),
CONJ (Conjonctions), NP (Noun Phrase), PP (Prepositional Phrase), VP (verbal Phrase) et
UNKONWN (coquilles ou certains mots &#233;trangers, eux-m&#234;mes &#233;tiquet&#233;s UNKNOWN). Les marques
de ponctuations, sauf exceptions (certains guillemets par exemple) sont hors chunks (&#233;tiquette
O comme Out). Nous avons d&#233;cid&#233; de modifier certains choix que nous avions faits dans
(Tellier et al., 2012). Par exemple, le chunk CONJ contient seulement la conjonction. Le PP, en
revanche, int&#232;gre toujours le chunk introduit par la pr&#233;position. Et, &#224; l&#8217;inverse de (Paroubek
et al., 2006), les adjectifs &#233;pith&#232;tes appartiennent toujours au chunk NP contenant le nom qu&#8217;ils
qualifient, qu&#8217;ils soient situ&#233;s avant ou apr&#232;s lui. Les chunks AP sont donc assez rares car ils ne
correspondent qu&#8217;aux adjectifs s&#233;par&#233;s d&#8217;un groupe nominal, comme les attributs du sujet ou de
l&#8217;objet (les fonctions syntaxiques disponibles dans les arbres XML sont n&#233;cessaires pour identifier
ces derniers). La phrase suivante illustre notre notion de parenth&#233;sage en chunks2 :
(la/DET d&#233;pr&#233;ciation/NC)NP (par_rapport_au/P dollar/NC)PP (a/V &#233;t&#233;/VPP limit&#233;e/VPP)V P
(&#224;/P 2,5/DET %/NC)PP
</p>
<p>Nous avons extrait du FTB deux corpus distincts, chacun repr&#233;sentant un chunking diff&#233;rent :
</p>
<p>&#8226; un corpus o&#249; tous les chunks sont extraits et &#233;tiquet&#233;s selon le mod&#232;le BIO (Begin/In/Out).
Les proportions de chaque type de chunk trouv&#233;es dans le corpus sont les suivantes : PP : 33,86%,
AdP : 7,23%, VP : 17,11%, AP : 2,21%, NP : 32,95%, CONJ : 6,61%, UNKNOWN : 0,03%.
</p>
<p>&#8226; un corpus o&#249; seuls les NP sont &#233;tiquet&#233;s, tout autre groupe &#233;tant alors consid&#233;r&#233; O. Ce
corpus n&#8217;est pas un sous-ensemble du pr&#233;c&#233;dent : par exemple, de nombreux PP incluent un NP
qui ne devient visible que dans ce deuxi&#232;me corpus. L&#8217;exemple pr&#233;c&#233;dent devient ainsi :
(la/DET d&#233;pr&#233;ciation/NC)NP par_rapport_au/P (dollar/NC)NP a/V &#233;t&#233;/VPP limit&#233;e/VPP &#224;/P
(2,5/DET %/NC)NP
</p>
<p>1http://www.cnts.ua.be/conll2000/chunking
2guide complet disponible sur :
</p>
<p>TALN-R&#201;CITAL 2013, 17-21 Juin, Les Sables d&#8217;Olonne
</p>
<p>21 c&#65535; ATALA</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>3 L&#8217;inf&#233;rence grammaticale
</p>
<p>L&#8217;inf&#233;rence grammaticale (IG) est un domaine de recherche tr&#232;s riche apparu dans les ann&#233;es 60
dont il n&#8217;est, par cons&#233;quent, pas ais&#233; de faire un r&#233;sum&#233;. Nous nous pla&#231;ons ici dans le cadre
de l&#8217;IG d&#8217;automates par exemples positifs seuls. Apr&#232;s un bref &#233;tat de l&#8217;art, nous d&#233;crivons les
algorithmes k-RI (Angluin, 1982) utilis&#233;s dans nos exp&#233;riences et les r&#233;sultats obtenus avec eux.
</p>
<p>3.1 Bref &#233;tat de l&#8217;art
</p>
<p>L&#8217;IG &#233;tudie les diff&#233;rentes mani&#232;res d&#8217;apprendre automatiquement un dispositif symbolique
capable de repr&#233;senter un langage (comme une grammaire formelle, un automate, etc...) &#224; partir
d&#8217;un ensemble de s&#233;quences (parfois enrichies) regroup&#233;es selon leur (non-)appartenance &#224; ce
langage (de la Higuera, 2010). Lorsque seules des s&#233;quences appartenant au langage cible sont
disponibles, le probl&#232;me est appel&#233; IG par pr&#233;sentation positive. Nous nous situons dans ce cadre
car les s&#233;quences &#224; notre disposition ne comportent aucun contre-exemple. L&#8217;IG est, dans ce cas,
notoirement plus difficile car, sans contre-exemple, on risque la surg&#233;n&#233;ralisation. Par exemple, si
un programme d&#8217;IG fait l&#8217;hypoth&#232;se, lors de sa phase d&#8217;apprentissage, que le langage &#224; apprendre
est le langage universel (&#931;&#8727;, o&#249; &#931; est l&#8217;alphabet du langage), aucun exemple positif n&#8217;est en
mesure de le contredire, alors m&#234;me qu&#8217;il a peut-&#234;tre surg&#233;n&#233;ralis&#233;.
</p>
<p>La premi&#232;re chose que l&#8217;IG se doit de fournir est une d&#233;finition pr&#233;cise de ce qu&#8217; &#8220;apprendre
une langue par exemples positifs&#8221; signifie pour un programme. Le crit&#232;re d&#8217;apprenabilit&#233; est
th&#233;orique et formel et non pas empirique. Faisons le parall&#232;le avec l&#8217;acquisition du langage
chez les enfants. Un enfant n&#8217;est pas &#8220;programm&#233;&#8221; pour une langue pr&#233;cise, il est capable
d&#8217;acqu&#233;rir n&#8217;importe laquelle parl&#233;e dans son environnement. De m&#234;me, un programme d&#8217;IG par
pr&#233;sentation positive doit &#234;tre en mesure d&#8217;apprendre une classe de langages formels, c&#8217;est-&#224;-dire
d&#8217;identifier n&#8217;importe lequel de ses membres &#224; l&#8217;aide d&#8217;exemples de s&#233;quences (phrases) lui
appartenant. Les principaux crit&#232;res possibles qui caract&#233;risent la notion d&#8221;&#8217;apprenabilit&#233; d&#8217;une
classe de langages&#8221; en IG (aussi appel&#233;s mod&#232;les d&#8217;apprentissage) sont &#8220;l&#8217;identification &#224; la limite&#8221;
(Gold, 1967) et &#8220;l&#8217;apprentissage PAC&#8221; (Valiant, 1984), que nous ne pouvons d&#233;tailler ici.
</p>
<p>Malheureusement, m&#234;me pour la classe des langages r&#233;guliers, la plus simple dans la hi&#233;rarchie
de Chomsky, ces crit&#232;res sont impossibles &#224; satisfaire : il n&#8217;existe aucun algorithme capable
d&#8217;apprendre par pr&#233;sentation positive la classe compl&#232;te des langages r&#233;guliers dans ces mod&#232;les
(Gold, 1967; Kearns and Vazirani, 1994). Les recherches se sont donc orient&#233;es vers des classes
plus petites, ou transverses &#224; la hi&#233;rarchie de Chomsky, et apprenables, caract&#233;ris&#233;es notamment
dans (Angluin, 1980). Les classes de langages k-r&#233;versibles (Angluin, 1982) entrent dans ce
cadre, elles constituent le point de d&#233;part de nos exp&#233;riences. Depuis, bien d&#8217;autres classes
apprenables par pr&#233;sentation positive ont &#233;t&#233; d&#233;crites et &#233;tudi&#233;es (Garcia and Vidal, 1990; Denis
et al., 2002; Kanazawa, 1998; Koshiba et al., 2000; Yokomori, 2003). Des avanc&#233;es r&#233;centes
dans le domaine concernent aussi l&#8217;apprenabilit&#233; de dispositifs int&#233;grant des probabilit&#233;s, comme
les automates probabilistes et leurs liens avec les HHM (Thollard et al., 2000; Dupont et al.,
2005). Parall&#232;lement, des comp&#233;titions 3 ont permis de tester l&#8217;efficacit&#233; des algorithmes propos&#233;s
lorsqu&#8217;ils sont confront&#233;s &#224; des donn&#233;es r&#233;elles.
</p>
<p>3les plus r&#233;cents &#233;tant Stamina ( ) et Zulu (
)
</p>
<p>TALN-R&#201;CITAL 2013, 17-21 Juin, Les Sables d&#8217;Olonne
</p>
<p>22 c&#65535; ATALA</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>3.2 L&#8217;algorithme k-RI
</p>
<p>Dans cette section, nous d&#233;crivons les algorithmes d&#8217;IG par pr&#233;sentation positive utilis&#233;s dans
nos exp&#233;riences. Ils sont destin&#233;s &#224; apprendre un automate pour un type sp&#233;cifique de chunk, &#224;
partir uniquement des diff&#233;rentes s&#233;quences de POS aparaissant dans ce type de chunks dans
les donn&#233;es d&#8217;apprentissage. Les algorithmes d&#8217;IG par exemples positifs semblent adapt&#233;s &#224; ce
probl&#232;me en raison du vocabulaire restreint mis en jeu (au maximum les 30 &#233;tiquettes POS) et
de la relativement faible variabilit&#233; des s&#233;quences de POS pouvant d&#233;crire un m&#234;me chunk.
</p>
<p>L&#8217;algorithme k-Reversible Inference (k-RI) (Angluin, 1982) a la propri&#233;t&#233; d&#8217;identifier &#224; la limite
tout langage k-r&#233;versible, pour tout k &#8712; &#65535; fix&#233;. Les langages k-r&#233;versibles sont r&#233;guliers, ils sont
donc repr&#233;sentables par des automates finis. Un automate fini d&#233;finit un langage k-r&#233;versible
s&#8217;il est d&#233;terministe et si son miroir 4 est d&#233;terministe avec anticipation k. Pour k = 0, les
langages 0-r&#233;versibles peuvent &#234;tre repr&#233;sent&#233;s par un automate d&#233;terministe dont le miroir l&#8217;est
&#233;galement, l&#8217;algorithme correspondant &#233;tant appel&#233; Z&#233;ro R&#233;versible (ZR). Si k1 &lt; k2, la classe
des langages k1-r&#233;versibles est strictement incluse dans celle des langages k2-r&#233;versibles.
</p>
<p>Soit un ensemble de s&#233;quences positives S, la premi&#232;re &#233;tape de k-RI est de construire PTA(S),
le Prefix Tree Acceptor de S. PTA(S) est le plus petit (en nombre d&#8217;&#233;tats) AFD (Automate Fini
D&#233;terministe) en forme d&#8217;arbre reconnaissant exactement le langage S. La racine de PTA(S)
est son &#233;tat initial. L&#8217;espace de recherche de tout algorithme d&#8217;IG partant de S est un trelli
dont la borne inf&#233;rieure est PTA(S) et la borne sup&#233;rieure l&#8217;automate universel construit sur
l&#8217;alphabet observ&#233; dans S (Dupont et al., 1994). La plupart des algorithmes d&#8217;IG suivent le
m&#234;me sch&#233;ma : ils partent de PTA(S) pour ensuite g&#233;n&#233;raliser le langage d&#233;fini par fusions
d&#8217;&#233;tats, la connaissance de k permettant d&#8217;&#233;viter la surg&#233;n&#233;ralisation. k-RI, d&#233;taill&#233; ci-dessous,
fonctionne selon ce principe. La fusion qu&#8217;il emploie est appel&#233;e d&#233;terministe car elle se propage
r&#233;cursivement &#224; travers l&#8217;automate pour pr&#233;server son d&#233;terminisme.
</p>
<p>Algorithme k-RI
Entr&#233;e : S : un ensemble de s&#233;quences (positives), k : un entier naturel ;
Sortie : A : un automate k-r&#233;versible ;
d&#233;but
</p>
<p>A := PTA(S);
tant que non(A k-r&#233;versible) faire
</p>
<p>// soient N1 et N2 deux n&#339;uds emp&#234;chant la k-r&#233;versibilit&#233; de A.
Fusion_D&#233;terministe(A, N1, N2);
</p>
<p>fin tant que;
renvoyer A;
</p>
<p>fin k-RI;
</p>
<p>Dans la figure 1, nous illustrons le comportement de ZR (k-RI pour k = 0) sur les s&#233;quences de
POS suivantes : S = {DET NC , DET ADJ NC}. Sur cet exemple tr&#232;s simple, nous voyons que
ZR g&#233;n&#233;ralise PTA(S) pour obtenir un automate reconnaissant le langage d&#233;fini par l&#8217;expression
r&#233;guli&#232;re : DET ADJ&#8727; NC . Cette g&#233;n&#233;ralisation est sens&#233;e d&#8217;un point de vue linguistique. Mais
si on ajoute aux exemples pr&#233;c&#233;dents la simple s&#233;quence NC , alors ZR m&#232;ne &#224; un automate
reconnaissant le langage {DET |ADJ}&#8727;NC , ce qui est une g&#233;n&#233;ralisation plus discutable.
</p>
<p>4l&#8217;automate miroir est obtenu en transformant les &#233;tats initiaux de l&#8217;automate de d&#233;part en &#233;tats finaux, ses &#233;tats
finaux en initiaux, et en retournant le sens de ses transitions
</p>
<p>TALN-R&#201;CITAL 2013, 17-21 Juin, Les Sables d&#8217;Olonne
</p>
<p>23 c&#65535; ATALA</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>&#233;tape 1 : PTA(S) &#233;tape 1 : miroir de PTA(S)
</p>
<p>&#233;tape 2 : les &#233;tats finaux de PTA(S) sont fusionn&#233;s &#233;tape 3 : q1 et q2 sont fusionn&#233;s
(sinon le miroir n&#8217;est pas d&#233;terministe) (sinon le miroir n&#8217;est pas d&#233;terministe)
</p>
<p>Figure 1: D&#233;monstration pas-&#224;-pas de ZR
</p>
<p>3.3 R&#233;sultats des exp&#233;riences d&#8217;IG sur le chunking NP
</p>
<p>Nous avons appliqu&#233; k-RI pour diff&#233;rentes valeurs de k (k = 0, k = 1, k = 2) sur les s&#233;quences
d&#8217;&#233;tiquettes POS correspondant &#224; un m&#234;me chunk. Les annotations BIO sont obtenues en utilisant
les automates appris comme des expressions r&#233;guli&#232;res selon un parcours s&#233;quentiel de la phrase.
Nous avons cherch&#233; &#224; recona&#238;tre soit les s&#233;quences les plus longues (&quot;Longest Match&quot;, LM) soit
les s&#233;quences les plus courtes (&quot;Shortest Match&quot;, SM). L&#8217;&#233;tiquetage en NP seuls est la t&#226;che pour
laquelle l&#8217;IG est la plus appropri&#233;e. Il est aussi &#233;videmment possible d&#8217;apprendre un automate
distinct pour chaque type de chunk. Mais l&#8217;application de plusieurs automates distincts sur une
nouvelle donn&#233;e pose des probl&#232;mes de recouvrements de fronti&#232;res. Nous n&#8217;utiliserons donc ces
automates que dans le cadre d&#8217;une combinaison avec un mod&#232;le statistique, en section 5.
</p>
<p>k-RI est connu pour &#234;tre tr&#232;s sensible aux donn&#233;es d&#8217;entr&#233;e : une seule s&#233;quence incorrecte peut
mener &#224; de multiples fusions d&#8217;&#233;tats, et donc &#224; une surg&#233;n&#233;ralisation. C&#8217;est le cas pour notre jeu
de donn&#233;es issues du FTB, d&#8217;o&#249; les cas aberrants et les erreurs d&#8217;&#233;tiquetage ne sont pas absents.
Quelques mauvais exemples &#233;taient cependant faciles &#224; d&#233;tecter : par exemple, des s&#233;quences
d&#8217;&#233;tiquettes POS ne contenant aucune t&#234;te nominale possible peuvent &#234;tre retir&#233;es sans risque.
Nous avons envisag&#233; diverses autres fa&#231;ons de nettoyer les donn&#233;es. Un nettoyage retirant toutes
les s&#233;quences apparaissant moins d&#8217;une certaine proportion fix&#233;e s&#8217;est r&#233;v&#233;l&#233; le plus efficace.
Cette strat&#233;gie entra&#238;ne n&#233;anmoins la suppression de s&#233;quences utiles, en raison de la faible
proportion de certaines t&#234;tes (clitiques notamment).
</p>
<p>Nos exp&#233;riences ont &#233;t&#233; r&#233;alis&#233;es selon un protocole de validation crois&#233;e partitionant les donn&#233;es
en cinq (4/5 pour l&#8217;apprentissage d&#8217;un automate, 1/5 pour le test). L&#8217;&#233;galit&#233; requise sur les
chunks est stricte, c&#8217;est-&#224;-dire que pour &#234;tre &#233;gaux, ils doivent partager exactement les m&#234;me
fronti&#232;res. Les pr&#233;cision, rappel et F-mesure des chunks NP sont calcul&#233;s sans prendre en compte
les &#233;tiquettes O. Le tableau 1 contient diverses F1-mesures obtenues par inf&#233;rence grammaticale
sur les chunks NP seuls, en appliquant sur les donn&#233;es de test une strat&#233;gie de correspondance LM.
Les versions d&#238;tes nettoy&#233;es ont &#233;t&#233; obtenues en supprimant toute s&#233;quence de POS apparaissant
strictement moins de 0.01%. Les valeurs entre parenth&#232;ses sont les nombres moyens d&#8217;&#233;tats des
</p>
<p>TALN-R&#201;CITAL 2013, 17-21 Juin, Les Sables d&#8217;Olonne
</p>
<p>24 c&#65535; ATALA</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>xp PTA pur PTA net. 0-RI net. (1) 1-RI net. (19) 2-RI net. (68.6)
F-mes. 51.92 88.05 26.95 72.74 88.25
</p>
<p>Table 1: R&#233;sultats de l&#8217;IG pour le chunking NP
</p>
<p>5 automates calcul&#233;s pendant la phase d&#8217;apprentissage. Les versions PTA, dont les performances
ne sont pas n&#233;gligeables, peuvent &#234;tre vues comme un apprentissage &#8220;par c&#339;ur&#8221;, puisqu&#8217;ils n&#8217;ont
donn&#233; lieu &#224; aucune g&#233;n&#233;ralisation. Les automates de taille 1 correspondent &#224; ceux reconnaissant
le langage universel des &#233;tiquettes POS pr&#233;sentes au moins une fois dans un chunk NP. Il faut
atteindre k = 2 pour obtenir un automate meilleur que le PTA sur des donn&#233;es nettoy&#233;es.
</p>
<p>4 Apprentissage statistique pour l&#8217;annotation
</p>
<p>Dans cette section, nous nous concentrons sur la meilleure approche statistique actuelle pour
une t&#226;che d&#8217;annotation : les Conditional Random Fields (CRF), qui se comportent tr&#232;s bien
sur notre probl&#232;me (Tellier et al., 2012). Nous rappelons aussi comment un HMM peut &#234;tre
&#8220;transform&#233;&#8221; en un CRF, parce que cette transformation sera une source d&#8217;inspiration pour une
des combinaisons pr&#233;sent&#233;es par la suite.
</p>
<p>4.1 Conditional Random Fields et HMMs
</p>
<p>Les CRF, introduits par (Lafferty et al., 2001) sont de la famille des mod&#232;les graphiques. Lorsque
que le graphe exprimant les d&#233;pendances entre &#233;tiquettes est lin&#233;aire (ce qui est g&#233;n&#233;ralement le
cas pour &#233;tiqueter des s&#233;quences), la distribution de probabilit&#233; d&#8217;une s&#233;quence d&#8217;annotations y
connaissant une s&#233;quence observable x est d&#233;finie par :
</p>
<p>p(y|x) = 1
Z(x)
</p>
<p>&#65535;
t
exp
&#65535; K&#65535;
</p>
<p>k=1
</p>
<p>&#955;k fk(t, yt , yt&#8722;1, x)
&#65535;
</p>
<p>O&#249; Z(x) est un facteur de normalisation d&#233;pendant de x et les K features (ou fonctions
caract&#233;ristiques) fk des fonctions fournies par l&#8217;utilisateur. Une feature fk est v&#233;rifi&#233;e (i.e.
fk(t, yt , yt&#8722;1, x) = 1) si, &#224; la position courante t, une configuration entre x , yt et yt&#8722;1 est
observ&#233;e (elle vaut 0 sinon). &#192; chaque feature fk est associ&#233; un poids &#955;k. Ces poids constituent
les param&#232;tres du mod&#232;le devant &#234;tre estim&#233;s au cours de l&#8217;apprentissage. Pour d&#233;finir un grand
nombre de features, les programmes impl&#233;mentant les CRF permettent d&#8217;avoir recours &#224; des
patrons (ou templates) qui seront instanci&#233;s en autant de features qu&#8217;il y a de positions sur les
donn&#233;es d&#8217;entra&#238;nement o&#249; ils peuvent s&#8217;appliquer. L&#8217;impl&#233;mentation la plus efficace &#224; l&#8217;heure
actuelle des CRF lin&#233;aires est fournie par Wapiti5, qui utilise des p&#233;nalisations pour s&#233;lectionner
les features les plus pertinentes (Lavergne et al., 2010). C&#8217;est le logiciel que nous avons utilis&#233;.
</p>
<p>Les CRF se sont montr&#233;s efficaces sur de nombreuses t&#226;ches d&#8217;annotation, notamment l&#8217;&#233;tiquetage
POS (Lafferty et al., 2001), la reconnaissance d&#8217;entit&#233;s nomm&#233;es (McCallum and Li, 2003), le
chunking (Sha and Pereira, 2003) et m&#234;me le parsing complet (Finkel et al., 2008; Tsuruoka
</p>
<p>5
</p>
<p>TALN-R&#201;CITAL 2013, 17-21 Juin, Les Sables d&#8217;Olonne
</p>
<p>25 c&#65535; ATALA</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Feature Type Fen&#234;tre
Mot Unigram [-2..1]
POS Bigram [-2..1]
</p>
<p>chunking Complet NP seuls
micro 97.53 N/A
macro 90.49 N/A
F1-mesure N/A 96.43
</p>
<p>Table 2: Le patron de template et les r&#233;sultats obtenus avec les CRF seuls pour chaque t&#226;che
</p>
<p>et al., 2009). Leur principal inconv&#233;nient est qu&#8217;ils apparaissent comme des &#8220;bo&#238;tes noires&#8221;. Un
mod&#232;le issu d&#8217;un apprentissage par CRF est simplement une liste de features pond&#233;r&#233;es pouvant
avoir plusieurs millions d&#8217;&#233;l&#233;ments, ce qui le rend difficile &#224; interpr&#233;ter.
</p>
<p>Les HMM, qui &#233;taient parmi les meilleures m&#233;thodes d&#8217;annotation statistique avant que les CRF
n&#8217;apparaissent, pr&#233;sentent quant &#224; eux l&#8217;avantage d&#8217;&#234;tre plus interpr&#233;tables. Cependant, tout
HMM peut &#234;tre &#8220;transform&#233;&#8221; en un CRF d&#233;finissant la m&#234;me distribution de probabilit&#233; (Sutton
and McCallum, 2006; Tellier and Tommasi, 2011). Pour ce faire, pour un HMM donn&#233;, nous
devons d&#233;finir deux familles de features :
</p>
<p>&#8226; les features de la forme f (yt , xt) associant une seule &#233;tiquette yt avec une seule entr&#233;e de
m&#234;me position xt : elles valent 1 quand l&#8217;&#233;tats yt du HMM &#233;met xt ;
</p>
<p>&#8226; les features de la forme f (yt&#8722;1, yt) qui associent deux &#233;tats yt&#8722;1 et yt du HMM ; elles valent
1 quand la transition entre ces deux &#233;tats est utilis&#233;e.
</p>
<p>Si &#952; est une probabilit&#233; d&#8217;&#233;mission ou de transition du HMM, alors on choisit &#955; = log(&#952; ) comme
poids pour la feature correspondant dans le CRF. Le calcul de p(y |x) s&#8217;&#233;crira alors exactement de
la m&#234;me fa&#231;on dans les deux cas. Un HMM peut ainsi &#234;tre vu comme un cas particulier de CRF.
Mais les CRF sont plus g&#233;n&#233;raux car ils permettent d&#8217;avoir recours &#224; d&#8217;autres features que celles
utilis&#233;es dans la transformation. Cette correspondance nous a inspir&#233;s pour exploiter les CRF
afin de diagnostiquer les automates appris par IG. Cette id&#233;e sera &#233;tudi&#233;e dans la section 5. Mais
auparavant, nous pr&#233;sentons les r&#233;sultats obtenus avec les CRF seuls sur nos donn&#233;es.
</p>
<p>4.2 R&#233;sultats des exp&#233;riences
</p>
<p>Les tableaux 2 montrent les patrons de features utilis&#233;s ainsi que les r&#233;sultats obtenus avec
les CRF seuls sur les deux t&#226;ches de chunking. Pour ces exp&#233;riences, comme en section 3.3,
nous avons suivi un protocole de validation crois&#233;e &#224; 5 plis et un crit&#232;re d&#8217;&#233;galit&#233; stricte des
chunks. Pour la t&#226;che de chunking complet, nous avons calcul&#233; les micro et macro-average,
qui correspondent aux moyennes des F1-mesures des diff&#233;rents types de chunks, pond&#233;r&#233;es
(micro) ou pas (macro) par leur proportion. Comme attendu, les CRF seuls sont tr&#232;s performants.
Remarquons toutefois qu&#8217;ils exploitent dans leurs features &#224; la fois des mots et des &#233;tiquettes
POS pr&#233;sents dans les donn&#233;es, alors que les algorithmes d&#8217;IG n&#8217;ont acc&#232;s qu&#8217;aux seuls POS.
</p>
<p>On peut comparer ces r&#233;sultats avec ceux obtenus lors de la campagne PASSAGE (Paroubek
et al., 2006), m&#234;me si les notions de chunks adopt&#233;es de part et d&#8217;autre diff&#232;rent (dans PASSAGE,
les adjectifs &#233;pith&#232;tes situ&#233;s apr&#232;s un nom ne font pas partie du chunk nomimal, par exemple)
et si les corpus ne sont pas les m&#234;mes. Les meilleurs participants de la campagne PASSAGE
atteignaient une micro-average de 92,7, ce qui situe tout de m&#234;me la performance de nos CRF.
</p>
<p>TALN-R&#201;CITAL 2013, 17-21 Juin, Les Sables d&#8217;Olonne
</p>
<p>26 c&#65535; ATALA</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>mot POS auto. NP auto. VP auto. PP ... label correct auto. NP NP-label correct
la DET B O O ... B-NP B B
d&#233;pr&#233;ciation NC I O O ... I-NP I I
par_rapport_au P O O B ... B-PP O O
dollar NC B O I ... I-PP B B
a V O B O ... B-VP O O
&#233;t&#233; VPP O I O ... I-VP O O
limit&#233;e VPP O I O ... I-VP O O
&#224; P O O B ... B-PP O O
2,5 DET B O I ... I-PP B B
% NC I O I ... I-PP I I
</p>
<p>Table 3: Donn&#233;es enrichies par des sorties d&#8217;automates sp&#233;cifiques pour chaque chunk
</p>
<p>5 Combinaisons
</p>
<p>Dans les sections pr&#233;c&#233;dentes, nous avons appliqu&#233; &#224; la t&#226;che de chunking une approche soit
purement symbolique soit purement statistique. Dans cette section, nous allons combiner les
deux approches, cette combinaison pouvant s&#8217;envisager selon deux axes distincts :
</p>
<p>&#8226; Soit le but est la seule performance, auquel cas il faut privil&#233;gier l&#8217;apprentissage statistique.
Cependant, les automates obtenus par IG offrent une vision globale (et non locale, comme c&#8217;est
le cas dans les features) des relations entre les &#233;tiquettes POS d&#8217;un m&#234;me chunk qui pourrait
s&#8217;av&#233;rer utile dans un CRF. Nous pouvons donc chercher &#224; int&#233;grer les r&#233;sultats de l&#8217;apprentissage
symbolique en tant que ressource externe de l&#8217;apprentissage statistique.
</p>
<p>&#8226; Soit nos fins sont plus en rapport avec la lisibilit&#233;, auquel cas nous favoriserons les automates
produits par IG. Or, comme &#233;voqu&#233; en 4.1, il est tout &#224; fait possible de simuler la structure d&#8217;un
HMM (et, similairement, d&#8217;un automate) avec les features d&#8217;un CRF. On pourrait donc &#233;valuer la
qualit&#233; des &#233;tats et des transitions d&#8217;un automate en fonction des poids associ&#233;s aux features qui
les repr&#233;sentent dans un CRF, offrant ainsi par la m&#234;me occasion un moyen de l&#8217;am&#233;liorer.
</p>
<p>5.1 Les automates en tant que ressource externe
</p>
<p>Nous nous attaquons ici aux deux types de chunking. Le premier mode de combinaison envisag&#233;
consiste &#224; enrichir les donn&#233;es du CRF avec des attributs provenant de la ressource externe, &#224;
la fa&#231;on de (Constant and Tellier, 2012). Dans le cas du chunking complet, nous appliquons
l&#8217;IG &#224; chaque type de chunk distinct, produisant ainsi autant d&#8217;automates qu&#8217;il y a de types de
chunks selon un protocole de validation crois&#233;e &#224; 5 plis (les PTA dans ces exp&#233;riences sont donc
uniquement extraits des corpus d&#8217;apprentissage). Chacun des automates de chunk fournit un
&#233;tiquetage BIO ind&#233;pendant, comme dans le tableau 3 (les automates sont ici suppos&#233;s fournir
un &#233;tiquetage parfait). Il y a donc dans nos donn&#233;es autant d&#8217;attributs nouveaux que de chunks.
</p>
<p>Les tableaux de gauche dans les tables 4 donnent les patrons aboutissant aux meilleurs r&#233;sultats
(micro resp. macro-average resp. F-mesure) pour le chunking complet ou le chunking NP.
La ligne &quot;Automate&quot; prend en compte la sortie de chaque automate ind&#233;pendamment alors
que &quot;POS+Automates&quot; repr&#233;sente la concat&#233;nation des colonnes POS et des sorties de tous les
automates. Les r&#233;sultats correspondants sont donn&#233;s dans les tableaux de droite. Ils montrent
que les attributs provenant des automates permettent d&#8217;am&#233;liorer significativement les r&#233;sultats
des CRF. C&#8217;est particuli&#232;rement vrai pour la macro-average, qui donne un poids &#233;quivalent &#224; la
</p>
<p>TALN-R&#201;CITAL 2013, 17-21 Juin, Les Sables d&#8217;Olonne
</p>
<p>27 c&#65535; ATALA</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>F1-mesure de chaque type de chunk. Les informations issues des automates am&#233;liorent donc
surtout les performances de reconnaissance des chunks rares. Dans l&#8217;exp&#233;rience permettant
d&#8217;obtenir la meilleure macro, les trois am&#233;liorations les plus significatives en terme de F-mesure
sont : UNKNOWN (de 41.67 &#224; 61.22), AP (de 96.78 &#224; 97.44) et AdP (de 98.72 &#224; 98.92).
</p>
<p>Feature Type Fen&#234;tre
Mot Unigram [-2..1]
Automate Bigram [-2..1]
POS Bigram [-2..1]
</p>
<p>F-mesure pur 1-RI LM
micro 97.66
macro 92.22
</p>
<p>Feature Type Fen&#234;tre
Mot Unigram [-2..1]
Automate Unigram [-1..1]
POS Bigram [-2..1]
POS+Automates Bigram [-1..1]
</p>
<p>F-mesure pur 1-RI SM
micro 97.62
macro 93.52
</p>
<p>Feature Type Fen&#234;tre
Mot Unigram [-2..1]
POS Bigram [-2..1]
Automate Bigram [-1..1]
POS+Automate Bigram [-1..1]
</p>
<p>pur 2-RI LM
F-measure 96.75
</p>
<p>Table 4: Patrons et meilleure micro-average (resp. macro-average) pour le chunking complet,
idem pour la F-mesure du chunking NP seul
</p>
<p>5.2 Diagnostiquer un automate &#224; l&#8217;aide d&#8217;un CRF
</p>
<p>Nous voulons ici obtenir des informations sur l&#8217;automate produit par IG &#224; l&#8217;aide des CRF, en
faisant un apprentissage n&#8217;utilisant que des features interpr&#233;tables relativement &#224; lui. Les poids
associ&#233;s par le CRF &#224; ces features fourniront un diagnostic fin de l&#8217;automate. Cette id&#233;e se
rapproche de (Roark and Saraclar, 2004), o&#249; un CRF &#233;tait appris selon la structure d&#8217;un automate
pond&#233;r&#233; pour le &#8220;corriger&#8221; gr&#226;ce &#224; l&#8217;estimation des poids. Elle en diff&#232;re toutefois car nous
ne cherchons pas &#224; obtenir un automate pond&#233;r&#233; mais &#224; trouver d&#8217;&#233;ventuelles modifications
&#224; effectuer sur l&#8217;automate selon le diagnostic fourni par le CRF, tout en pr&#233;servant sa nature
purement symbolique. Pour illustrer cette approche, nous nous concentrons sur la t&#226;che de
chunking NP seul car elle ne n&#233;cessite la prise en compte que d&#8217;un seul automate. Il peut &#234;tre plus
facile pour comprendre la suite de se repr&#233;senter les automates finis d&#233;terministes (AFD) &#8220;&#224; la
Unitex&#8221; (http://www-igm.univ-mlv.fr/ unitex/). Ainsi, le r&#233;sultat de l&#8217;algorithme ZR sur la figure
1 (l&#8217;automate final, en bas &#224; droite) est identique &#224; celui de la figure 2. Cette repr&#233;sentation a
l&#8217;avantage de montrer les &#233;tiquettes POS et les transitions entre deux &#233;tiquettes POS comme deux
objets diff&#233;rents. Pour construire un CRF &#224; partir d&#8217;un tel automate, nous consid&#233;rons surtout les
sorties en termes d&#8217;&#233;tiquetage BIO que cet automate produit (partie droite de la Table 3).
</p>
<p>Nous inspirant de la relation entre les HMM et les CRF &#233;voqu&#233;e en section 4.1, nous d&#233;finissons
des patrons de features qui peuvent s&#8217;interpr&#233;ter relativement &#224; l&#8217;automate :
</p>
<p>TALN-R&#201;CITAL 2013, 17-21 Juin, Les Sables d&#8217;Olonne
</p>
<p>28 c&#65535; ATALA</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Figure 2: Automate repr&#233;sent&#233; &#8220;&#224; la Unitex&#8221;
</p>
<p>&#8226; Un patron unigramme qui observe une &#233;tiquette POS et l&#8217;&#233;tiquette BIO pr&#233;dite par l&#8217;automate
&#224; la m&#234;me position, conjointement avec le label BIO correct. L&#8217;&#233;tiquette POS correspond &#224; un
(ou plusieurs) &#233;tat(s) de l&#8217;automate. Si les &#233;tiquettes BIO co&#239;ncident pour un POS donn&#233;, cela
signifie que l&#8217;automate a en quelque sorte raison d&#8217;&#234;tre dans cet &#233;tat en analysant la donn&#233;e.
</p>
<p>&#8226; Un patron bigramme qui observe un couple d&#8217;&#233;tiquettes POS successives et le couple
d&#8217;&#233;tiquettes BIO pr&#233;dites par l&#8217;automate correspondant, associ&#233; au couple de labels BIO correct.
Le couple de POS repr&#233;sente une (ou plusieurs) transition(s) de l&#8217;automate. Si les deux couples
d&#8217;&#233;tiquettes BIO co&#239;ncident, cela signifie que la transition est correctement utilis&#233;e.
</p>
<p>Il est &#224; noter que les mots eux-m&#234;mes ne sont pas pris en compte dans ces patrons, afin de
pr&#233;server l&#8217;interpr&#233;tation des features relativement &#224; l&#8217;automate, d&#8217;o&#249; les mots sont absents.
</p>
<p>La Table 5 est une matrice de confusion qui met en relation les &#233;tiquettes BIO pr&#233;dites par un
automate (EP) et les &#233;tiquettes BIO correctes (EC), pour une &#233;tiquette POS donn&#233;e (ici, l&#8217;&#233;tiquette
DET d&#8217;un automate appris). On peut construire autant de tables que d&#8217;&#233;tiquettes POS pr&#233;sentes
dans un chunk NP, chaque case de chaque table correspondant &#224; une feature unigramme. Les
cases de la Table 5 sont remplies par les poids appris par le CRF pour les features en question, les
couleurs montrent comment elles s&#8217;interpr&#232;tent relativement &#224; l&#8217;automate de d&#233;part. Comme
esp&#233;r&#233;, les poids sur la diagonale, qui signalent un &#233;tiquetage correct, sont plus grands que
ceux en dehors, qui d&#233;signent une erreur d&#8217;&#233;tiquetage. Les features bigrammes sont un peu plus
compliqu&#233;es mais il est &#233;galement possible d&#8217;en tirer des matrices de confusion interpr&#233;tables.
</p>
<p>EP \EC B I O
B 1.66 -4.05 -0.84
I -0.44 0.46 -2.51
O N/A N/A N/A
</p>
<p>vert : les deux sorties sont identiques.
rouge : d&#233;but pr&#233;matur&#233; de chunk.
</p>
<p>jaune : d&#233;but de chunk manqu&#233;.
</p>
<p>bleu : continuation intempestive de chunk.
cyan : arr&#234;t pr&#233;matur&#233; de chunk.
</p>
<p>Table 5: Une matrice de confusion color&#233;e pour l&#8217;&#233;tiquette DET (2-RI, tableau 1)
</p>
<p>De mani&#232;re g&#233;n&#233;rale, le poids associ&#233; &#224; une feature d&#8217;un CRF repr&#233;sente son pouvoir discriminant.
Ces poids sont donc bien plus pertinents que de simples comptes d&#8217;occurences sur le nombre de
fois qu&#8217;une feature a &#233;t&#233; satisfaite ou pas dans les donn&#233;es d&#8217;apprentissage. Les poids sur les
diagonales peuvent ainsi &#234;tre vus comme &#233;valuant la qualit&#233; des &#233;tats / transitions de l&#8217;automate,
alors que les poids dans les autres cases correspondent aux gains obtenus en prenant une d&#233;cision
d&#8217;&#233;tiquetage non pr&#233;conis&#233;e par l&#8217;automate. L&#8217;ensemble des matrices de confusion offre donc une
mesure extr&#234;mement fine et pr&#233;cise de la qualit&#233; de l&#8217;automate.
</p>
<p>Le tableau 6 rappelle le meilleur r&#233;sultat obtenu par IG &#8220;pure&#8221; sur le chunking NP de la section
3.3 et donne les r&#233;sultats des CRF construits comme pr&#233;c&#233;demment sur le meilleur automate
</p>
<p>TALN-R&#201;CITAL 2013, 17-21 Juin, Les Sables d&#8217;Olonne
</p>
<p>29 c&#65535; ATALA</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Exp&#233;rience baseline (IG seule) 0-RI 1-RI 2-RI
chunk 88.25 93.00 93.07 93.08
</p>
<p>Table 6: R&#233;sultats du chunking NP avec les CRF construits sur les automates
</p>
<p>produit par k-RI pour chaque valeur de k. Comme on pouvait s&#8217;y attendre, les CRF construits
sur les automates NP sont meilleurs que les automates NP seuls, mais moins bons qu&#8217;un CRF
exploitant plus d&#8217;attributs et de features. Les r&#233;sultats des matrices de confusion doivent encore
&#234;tre examin&#233;s en d&#233;tail. Nous esp&#233;rons en tirer un diagnostic pr&#233;cis pour analyser o&#249; et pourquoi
les automates prennent de bonnes ou de mauvaises d&#233;cisions, et les modifier en cons&#233;quence. Les
am&#233;liorations observ&#233;es dans la Table 6 laissent en effet supposer qu&#8217;&#224; de nombreuses occasions
le CRF a eu raison de prendre une d&#233;cision diff&#233;rente de celle pr&#233;conis&#233;e par l&#8217;automate.
</p>
<p>Conclusion et perspectives
</p>
<p>Dans cet article, nous avons appliqu&#233; deux m&#233;thodes d&#8217;apprentissage automatique sur le m&#234;me
jeu de donn&#233;es et avons propos&#233; deux fa&#231;ons diff&#233;rentes de les combiner.
</p>
<p>Pour ce qui est de l&#8217;apprentissage symbolique seul, il est possible que d&#8217;autres algorithmes d&#8217;IG
par pr&#233;sentation positive pourraient donner de meilleurs r&#233;sultats que les n&#244;tres, comme ceux de
(Garcia and Vidal, 1990; Denis et al., 2002). Le choix d&#8217;une grande valeur de k dans certains cas
peut &#234;tre important, mais il s&#8217;accompagne d&#8217;une plus grande complexit&#233; de calculs6.
</p>
<p>Mais la partie la plus originale de notre travail concerne les combinaisons automates/CRF.
Notons que ces combinaisons peuvent tout autant s&#8217;appliquer &#224; des automates &#233;crits &#224; la main,
g&#233;n&#233;ralement plus pertinents d&#8217;un point de vue linguistique que ceux obtenus par IG. Nous nous
sommes concentr&#233;s ici sur des automates appris automatiquement pour montrer que, m&#234;me sans
ressource ni expertise linguistique, il est possible de combiner mod&#232;les symboliques et statistiques.
L&#8217;intuition derri&#232;re ce travail est que ces deux types de mod&#232;les sont compl&#233;mentaires, et qu&#8217;ils
peuvent chacun b&#233;n&#233;ficier de l&#8217;autre. Les CRF sont bas&#233;s sur un grand nombre de configurations
locales pond&#233;r&#233;es. Il est th&#233;oriquement possible d&#8217;utiliser dans un CRF des features portant sur
l&#8217;int&#233;gralit&#233; de la s&#233;quence x mais dans la pratique, cela est rarement fait. L&#8217;IG au contraire
s&#8217;applique &#224; un ensemble de s&#233;quences globales qu&#8217;elle est capable de g&#233;n&#233;raliser. Il a d&#233;j&#224; &#233;t&#233;
observ&#233; que les CRF gagnent &#224; recourir &#224; des features exprimant des propri&#233;t&#233;s plus g&#233;n&#233;rales
que de simples configurations locales (Pu et al., 2010). Notre pari &#233;tait que l&#8217;IG pouvait fournir
ce type de g&#233;n&#233;ralisation, via le premier mode de combinaison. Les r&#233;sultats obtenus vont dans
ce sens. Il est aussi int&#233;ressant de constater que les mod&#232;les symboliques permettent d&#8217;am&#233;liorer
le traitement des cas rares, mal pris en compte par les mod&#232;les statistiques.
</p>
<p>Les CRF construits sur des automates restent encore &#224; &#233;tudier, notamment pour interpr&#233;ter et
exploiter au mieux les matrices de confusion qu&#8217;ils produisent. Certaines cases de ces matrices
sont vides car Wapiti &#233;limine les features non pertinentes de l&#8217;ensemble de d&#233;part selon un crit&#232;re
de p&#233;nalit&#233;. Il devrait &#234;tre possible, &#224; l&#8217;aide de ces informations, de modifier l&#8217;automate sur
lequel se base le CRF en supprimant ou ajoutant des &#233;tats ou des transitions pour se conformer
au diagnostic fourni par une table. Une IG dirig&#233;e par des CRF reste encore &#224; d&#233;finir ! Un autre
</p>
<p>6la complexit&#233; algorithmique de k-RI est |&#931;|k|Q|k+3 o&#249; |Q| est le nombre d&#8217;&#233;tats du PTA
</p>
<p>TALN-R&#201;CITAL 2013, 17-21 Juin, Les Sables d&#8217;Olonne
</p>
<p>30 c&#65535; ATALA</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>d&#233;fi serait l&#8217;&#233;tude du lien entre les automates associ&#233;s aux poids calcul&#233;s par CRF que nous
d&#233;finissons et les plus classiques HMM ou automates probabilistes pour lesquels des algorithmes
d&#8217;apprentissage existent d&#233;j&#224; (Thollard et al., 2000).
</p>
<p>6 R&#233;f&#233;rences
</p>
<p>Abeill&#233;, A., Cl&#233;ment, L., and Toussenel, F. (2003). Building a treebank for french. In Abeill&#233;, A.,
editor, Treebanks. Kluwer, Dordrecht.
</p>
<p>Abney, S. (1991). Parsing by chunks. In Berwick, R., Abney, R., and Tenny, C., editors,
Principle-based Parsing. Kluwer Academic Publisher.
</p>
<p>Angluin, D. (1980). Inductive inference of formal languages from positive data. Information
and Control, 45(2):117&#8211;135.
</p>
<p>Angluin, D. (1982). Inference of reversible languages. Journal of the ACM, 29(3):741&#8211;765.
</p>
<p>Antoine, J.-Y., Mokrane, A., and Friburger, N. (2008). Automatic rich annotation of large corpus
of conversational transcribed speech: the chunking task of the epac project. In Proceedings of
LREC&#8217;2008.
</p>
<p>Blanc, O., Constant, M., Dister, A., and Watrin, P. (2010). Partial parsing of spontaneous spoken
french. In Proceedings of LREC&#8217;2010.
</p>
<p>Constant, M. and Tellier, I. (2012). Evaluating the impact of external lexical resources unto a
crf-based multiword segmenter and part-of-speech tagger. In Proceedings of LREC 2012.
</p>
<p>Constant, M., Tellier, I., Duchier, D., Dupont, Y., Sigogne, A., and Billot, S. (2011). Int&#233;grer
des connaissances linguistiques dans un CRF : application &#224; l&#8217;apprentissage d&#8217;un segmenteur-
&#233;tiqueteur du fran&#231;ais. In Actes de TALN&#8217;11.
</p>
<p>Crabb&#233;, B. and Candito, M. H. (2008). Exp&#233;riences d&#8217;analyse syntaxique statistique du fran&#231;ais.
In Actes de TALN&#8217;08.
</p>
<p>de la Higuera, C. (2010). Grammatical Inference: Learning Automata and Grammars. CU Press.
</p>
<p>Denis, F., Lemay, A., and Terlutte, A. (2002). Some language classes identifiable in the limit
from positive data. In ICGI 2002, number 2484 in LNAI, pages 63&#8211;76. Springer Verlag.
</p>
<p>Dupont, P., Denis, F., and Esposito, Y. (2005). Links between probabilistic automata and hidden
markov models: probability distributions, learning models and induction algorithms. Pattern
Recognition, 38(9):1349&#8211;1371.
</p>
<p>Dupont, P., Miclet, L., and Vidal, E. (1994). What is the search space of the regular inference. In
ICGI&#8217;94 - LNCS, volume 862 - Grammatical Inference and Applications, pages 25&#8211;37, Heidelberg.
</p>
<p>Finkel, J. R., Kleeman, A., and Manning, C. D. (2008). Efficient, feature-based, conditional
random field parsing. In Proceedings of ACL&#8217;2008, pages 959&#8211;967.
</p>
<p>TALN-R&#201;CITAL 2013, 17-21 Juin, Les Sables d&#8217;Olonne
</p>
<p>31 c&#65535; ATALA</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Garcia, P. and Vidal, E. (1990). Inference of k-testable languages in the strict sense and
application to syntactic pattern recognition. IEEE TPAMI, 12(9):920&#8211;925.
</p>
<p>Gold, E. (1967). Language identification in the limit. Information and Control, 10:447&#8211;474.
</p>
<p>Kanazawa, M. (1998). Learnable Classes of Categorial Grammars. FoLLI. CLSI Publications.
</p>
<p>Kearns, M. J. and Vazirani, U. V. (1994). An Introduction to Computational Learning Theory. MIT
Press.
</p>
<p>Koshiba, T., M&#228;kinen, E., and Takada, Y. (2000). Inferring pure context-free languages from
positive data. Acta Cybernetica, 14(3):469&#8211;477.
</p>
<p>Lafferty, J., McCallum, A., and Pereira, F. (2001). Conditional random fields: Probabilistic
models for segmenting and labeling sequence data. In Proceedings of ICML 2001, pages 282&#8211;289.
</p>
<p>Lavergne, T., Capp&#233;, O., and Yvon, F. (2010). Practical very large scale CRFs. In Proceedings of
ACL&#8217;2010, pages 504&#8211;513. Association for Computational Linguistics.
</p>
<p>McCallum, A. and Li, W. (2003). Early results for named entity recognition with conditional
random fields. In Proceedings of CoNLL&#8217;2003.
</p>
<p>Paroubek, P., Robba, I., Vilnat, A., and C., A. (2006). Data annotations and measures in easy,
the evaluation campain for parsers of french. In Proceedings of LREC&#8217;2006, pages 315&#8211;320.
</p>
<p>Pu, X., Mao, Q., Wu, G., and Yuan, C. (2010). Chinese named entity recognition with the
improved smoothed conditional random fields. Research in Computing Science, 46:90&#8211;103.
</p>
<p>Roark, B. and Saraclar, M. (2004). Discriminative language modeling with conditional random
fields and the perceptron algorithm. In Proceedings of ACL&#8217;2004, pages 47&#8211;54.
</p>
<p>Sha, F. and Pereira, F. (2003). Shallow parsing with conditional random fields. In Proceedings of
HLT-NAACL 2003, pages 213 &#8211; 220.
</p>
<p>Sutton, C. and McCallum, A. (2006). Introduction to Statistical Relational Learning, chapter An
Introduction to Conditional Random Fields for Relational Learning. MIT Press.
</p>
<p>Tellier, I., Duchier, D., Eshkol, I., Courmet, A., and Martinet, M. (2012). Apprentissage
automatique d&#8217;un chunker pour le fran&#231;ais. In Actes de TALN&#8217;12, papier court (poster).
</p>
<p>Tellier, I. and Tommasi, M. (2011). Champs Markoviens Conditionnels pour l&#8217;extraction
d&#8217;information. In Mod&#232;les probabilistes pour l&#8217;acc&#232;s &#224; l&#8217;information textuelle. Herm&#232;s.
</p>
<p>Thollard, F., Dupont, P., and de la Higuera, C. (2000). Probabilistic DFA inference using
Kullback-Leibler divergence and minimality. In Proc. of ICML&#8217;2000, pages 975&#8211;982.
</p>
<p>Tsuruoka, Y., Tsujii, J., and Ananiadou, S. (2009). Fast full parsing by linear-chain conditional
random fields. In Proceedings of EACL 2009, pages 790&#8211;798.
</p>
<p>Valiant, L. G. (1984). A theory of the learnable. Communications of the ACM, 27(11):1134&#8211;1142.
</p>
<p>Yokomori, T. (2003). Polynomial-time identification of very simple grammars from positive data.
Theoretical Computer Science, 1.
</p>
<p>TALN-R&#201;CITAL 2013, 17-21 Juin, Les Sables d&#8217;Olonne
</p>
<p>32 c&#65535; ATALA</p>

</div></div>
</body></html>