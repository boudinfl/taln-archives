<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Abeillé</author>
<author>L Clément</author>
<author>F Toussenel</author>
</authors>
<title>Building a treebank for french.</title>
<date>2003</date>
<editor>In Abeillé, A., editor, Treebanks.</editor>
<publisher>Kluwer,</publisher>
<location>Dordrecht.</location>
<contexts>
<context position="5858" citStr="Abeillé et al., 2003" startWordPosition="864" endWordPosition="867">ns et évaluons deux manières de combiner automates et CRF. Les résultats obtenus pour chacune de ces combinaisons sont prometteurs et suggèrent des pistes originales pour associer modèles symboliques et apprentissage statistique. 2 Chunking: la tâche et les données Nous décrivons ici la tâche de chunking par annotation et nous présentons les données d’apprentissage que nous avons utilisées pour nos expériences. Ces dernières reprennent et prolongent celles présentées dans (Tellier et al., 2012). Notre but étant de construire un chunker pour le français, nous sommes partis du French Tree Bank (Abeillé et al., 2003). 2.1 La tâche La tâche de chunking, également appelée analyse syntaxique de surface, a pour but d’identifier les groupes syntaxiques élémentaires des phrases. Les chunks sont en effet des séquences contigües et non-récursives d’unités lexicales liées à une unique tête forte (Abney, 1991). Chacun est caractérisé 20 c ATALA TALN-RÉCITAL 2013, 17-21 Juin, Les Sables d’Olonne par le type (ou étiquette Part-Of-Speech (POS)) de sa tête. Il y a ainsi autant de types de chunks que de types de têtes fortes possibles. La tâche de chunking a fait l’objet de de la compétition CoNLL’20001, dont le corpus</context>
</contexts>
<marker>Abeillé, Clément, Toussenel, 2003</marker>
<rawString>Abeillé, A., Clément, L., and Toussenel, F. (2003). Building a treebank for french. In Abeillé, A., editor, Treebanks. Kluwer, Dordrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Abney</author>
</authors>
<title>Parsing by chunks.</title>
<date>1991</date>
<editor>In Berwick, R., Abney, R., and Tenny, C., editors, Principle-based Parsing.</editor>
<publisher>Kluwer Academic Publisher.</publisher>
<contexts>
<context position="3401" citStr="Abney, 1991" startWordPosition="489" endWordPosition="490">es formels, est souvent méconnu. Les algorithmes d’IG sont en effet réputés ne pas très bien se comporter sur des données réelles : ils sont souvent algorithmiquement complexes, sensibles aux erreurs et peu adaptés aux langages fondés sur de grands alphabets (ce qui est le cas quand l’alphabet est l’ensemble des mots d’une langue naturelle). Dans cet article, nous voulons donner leur chance à des algorithmes classiques d’IG pour les comparer aux méthodes d’apprentissage automatique statistique état de l’art, en l’occurrence les CRF (Lafferty et al., 2001). La tâche considérée est le chunking (Abney, 1991) du français, qui peut en effet très bien être réalisée à l’aide d’automates construits manuellement (Antoine et al., 2008; Blanc et al., 2010). À notre connaissance, essayer d’apprendre automatiquement ces automates au lieu de les écrire à la main n’a encore pas jamais été testé, pour quelque langue que ce soit. Par ailleurs, le chunking peut également être vu comme une tâche d’annotation (objet de la Shared Task CoNLL’2000) et de ce fait abordé via des méthodes d’apprentissage statistique. Ce contexte nous semblait par conséquent idéal pour comparer les deux approches. Cette comparaison n’es</context>
<context position="6147" citStr="Abney, 1991" startWordPosition="909" endWordPosition="910">hunking par annotation et nous présentons les données d’apprentissage que nous avons utilisées pour nos expériences. Ces dernières reprennent et prolongent celles présentées dans (Tellier et al., 2012). Notre but étant de construire un chunker pour le français, nous sommes partis du French Tree Bank (Abeillé et al., 2003). 2.1 La tâche La tâche de chunking, également appelée analyse syntaxique de surface, a pour but d’identifier les groupes syntaxiques élémentaires des phrases. Les chunks sont en effet des séquences contigües et non-récursives d’unités lexicales liées à une unique tête forte (Abney, 1991). Chacun est caractérisé 20 c ATALA TALN-RÉCITAL 2013, 17-21 Juin, Les Sables d’Olonne par le type (ou étiquette Part-Of-Speech (POS)) de sa tête. Il y a ainsi autant de types de chunks que de types de têtes fortes possibles. La tâche de chunking a fait l’objet de de la compétition CoNLL’20001, dont le corpus d’apprentissage était constitué d’environ 9 000 phrases issues du Penn Treebank, associées à deux niveaux d’annotion : un niveau POS donné par l’étiqueteur Brill et un de chunking. Les vainqueurs avaient utilisé des SVM et des “Weighted Probability Distribution Voting”. Ce même corpus a </context>
</contexts>
<marker>Abney, 1991</marker>
<rawString>Abney, S. (1991). Parsing by chunks. In Berwick, R., Abney, R., and Tenny, C., editors, Principle-based Parsing. Kluwer Academic Publisher.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Angluin</author>
</authors>
<title>Inductive inference of formal languages from positive data.</title>
<date>1980</date>
<journal>Information and Control,</journal>
<volume>45</volume>
<issue>2</issue>
<contexts>
<context position="12412" citStr="Angluin, 1980" startWordPosition="1847" endWordPosition="1848">ntification à la limite” (Gold, 1967) et “l’apprentissage PAC” (Valiant, 1984), que nous ne pouvons détailler ici. Malheureusement, même pour la classe des langages réguliers, la plus simple dans la hiérarchie de Chomsky, ces critères sont impossibles à satisfaire : il n’existe aucun algorithme capable d’apprendre par présentation positive la classe complète des langages réguliers dans ces modèles (Gold, 1967; Kearns and Vazirani, 1994). Les recherches se sont donc orientées vers des classes plus petites, ou transverses à la hiérarchie de Chomsky, et apprenables, caractérisées notamment dans (Angluin, 1980). Les classes de langages k-réversibles (Angluin, 1982) entrent dans ce cadre, elles constituent le point de départ de nos expériences. Depuis, bien d’autres classes apprenables par présentation positive ont été décrites et étudiées (Garcia and Vidal, 1990; Denis et al., 2002; Kanazawa, 1998; Koshiba et al., 2000; Yokomori, 2003). Des avancées récentes dans le domaine concernent aussi l’apprenabilité de dispositifs intégrant des probabilités, comme les automates probabilistes et leurs liens avec les HHM (Thollard et al., 2000; Dupont et al., 2005). Parallèlement, des compétitions 3 ont permis </context>
</contexts>
<marker>Angluin, 1980</marker>
<rawString>Angluin, D. (1980). Inductive inference of formal languages from positive data. Information and Control, 45(2):117–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Angluin</author>
</authors>
<title>Inference of reversible languages.</title>
<date>1982</date>
<journal>Journal of the ACM,</journal>
<volume>29</volume>
<issue>3</issue>
<contexts>
<context position="4934" citStr="Angluin, 1982" startWordPosition="725" endWordPosition="726"> orientée vers l’efficacité : elle vise à enrichir un modèle CRF à l’aide d’informations extraites des automates. La seconde privilégie la lisibilité : elle propose d’analyser les automates appris par IG à l’aide de poids calculés par un CRF, poids qui seront tous interprétables relativement à cet automate. L’article suit le plan suivant. Dans la première section, nous introduisons la tâche de chunking et décrivons les données utilisées pour nos expériences. La deuxième section est dédiée à l’inférence grammaticale. Après un bref état de l’art, nous détaillons la famille des algorithmes k-RI (Angluin, 1982) et donnons les meilleurs résultats expérimentaux qu’ils permettent d’atteindre pour le chunking. Dans la section qui suit, nous appliquons les CRF à la même tâche. Comme on pouvait s’y attendre, les CRF donnent de bien meilleurs résultats que ceux obtenus par IG. Dans la dernière section, nous décrivons et évaluons deux manières de combiner automates et CRF. Les résultats obtenus pour chacune de ces combinaisons sont prometteurs et suggèrent des pistes originales pour associer modèles symboliques et apprentissage statistique. 2 Chunking: la tâche et les données Nous décrivons ici la tâche de </context>
<context position="9953" citStr="Angluin, 1982" startWordPosition="1490" endWordPosition="1491"> précédent devient ainsi : (la/DET dépréciation/NC)NP par_rapport_au/P (dollar/NC)NP a/V été/VPP limitée/VPP à/P (2,5/DET %/NC)NP 1http://www.cnts.ua.be/conll2000/chunking 2guide complet disponible sur : 21 c ATALA TALN-RÉCITAL 2013, 17-21 Juin, Les Sables d’Olonne 3 L’inférence grammaticale L’inférence grammaticale (IG) est un domaine de recherche très riche apparu dans les années 60 dont il n’est, par conséquent, pas aisé de faire un résumé. Nous nous plaçons ici dans le cadre de l’IG d’automates par exemples positifs seuls. Après un bref état de l’art, nous décrivons les algorithmes k-RI (Angluin, 1982) utilisés dans nos expériences et les résultats obtenus avec eux. 3.1 Bref état de l’art L’IG étudie les différentes manières d’apprendre automatiquement un dispositif symbolique capable de représenter un langage (comme une grammaire formelle, un automate, etc...) à partir d’un ensemble de séquences (parfois enrichies) regroupées selon leur (non-)appartenance à ce langage (de la Higuera, 2010). Lorsque seules des séquences appartenant au langage cible sont disponibles, le problème est appelé IG par présentation positive. Nous nous situons dans ce cadre car les séquences à notre disposition ne </context>
<context position="12467" citStr="Angluin, 1982" startWordPosition="1854" endWordPosition="1855">ge PAC” (Valiant, 1984), que nous ne pouvons détailler ici. Malheureusement, même pour la classe des langages réguliers, la plus simple dans la hiérarchie de Chomsky, ces critères sont impossibles à satisfaire : il n’existe aucun algorithme capable d’apprendre par présentation positive la classe complète des langages réguliers dans ces modèles (Gold, 1967; Kearns and Vazirani, 1994). Les recherches se sont donc orientées vers des classes plus petites, ou transverses à la hiérarchie de Chomsky, et apprenables, caractérisées notamment dans (Angluin, 1980). Les classes de langages k-réversibles (Angluin, 1982) entrent dans ce cadre, elles constituent le point de départ de nos expériences. Depuis, bien d’autres classes apprenables par présentation positive ont été décrites et étudiées (Garcia and Vidal, 1990; Denis et al., 2002; Kanazawa, 1998; Koshiba et al., 2000; Yokomori, 2003). Des avancées récentes dans le domaine concernent aussi l’apprenabilité de dispositifs intégrant des probabilités, comme les automates probabilistes et leurs liens avec les HHM (Thollard et al., 2000; Dupont et al., 2005). Parallèlement, des compétitions 3 ont permis de tester l’efficacité des algorithmes proposés lorsqu’</context>
<context position="13860" citStr="Angluin, 1982" startWordPosition="2063" endWordPosition="2064">s cette section, nous décrivons les algorithmes d’IG par présentation positive utilisés dans nos expériences. Ils sont destinés à apprendre un automate pour un type spécifique de chunk, à partir uniquement des différentes séquences de POS aparaissant dans ce type de chunks dans les données d’apprentissage. Les algorithmes d’IG par exemples positifs semblent adaptés à ce problème en raison du vocabulaire restreint mis en jeu (au maximum les 30 étiquettes POS) et de la relativement faible variabilité des séquences de POS pouvant décrire un même chunk. L’algorithme k-Reversible Inference (k-RI) (Angluin, 1982) a la propriété d’identifier à la limite tout langage k-réversible, pour tout k ∈  fixé. Les langages k-réversibles sont réguliers, ils sont donc représentables par des automates finis. Un automate fini définit un langage k-réversible s’il est déterministe et si son miroir 4 est déterministe avec anticipation k. Pour k = 0, les langages 0-réversibles peuvent être représentés par un automate déterministe dont le miroir l’est également, l’algorithme correspondant étant appelé Zéro Réversible (ZR). Si k1 &lt; k2, la classe des langages k1-réversibles est strictement incluse dans celle des langages </context>
</contexts>
<marker>Angluin, 1982</marker>
<rawString>Angluin, D. (1982). Inference of reversible languages. Journal of the ACM, 29(3):741–765.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J-Y Antoine</author>
<author>A Mokrane</author>
<author>N Friburger</author>
</authors>
<title>Automatic rich annotation of large corpus of conversational transcribed speech: the chunking task of the epac project.</title>
<date>2008</date>
<booktitle>In Proceedings of LREC’2008.</booktitle>
<contexts>
<context position="3523" citStr="Antoine et al., 2008" startWordPosition="506" endWordPosition="509">données réelles : ils sont souvent algorithmiquement complexes, sensibles aux erreurs et peu adaptés aux langages fondés sur de grands alphabets (ce qui est le cas quand l’alphabet est l’ensemble des mots d’une langue naturelle). Dans cet article, nous voulons donner leur chance à des algorithmes classiques d’IG pour les comparer aux méthodes d’apprentissage automatique statistique état de l’art, en l’occurrence les CRF (Lafferty et al., 2001). La tâche considérée est le chunking (Abney, 1991) du français, qui peut en effet très bien être réalisée à l’aide d’automates construits manuellement (Antoine et al., 2008; Blanc et al., 2010). À notre connaissance, essayer d’apprendre automatiquement ces automates au lieu de les écrire à la main n’a encore pas jamais été testé, pour quelque langue que ce soit. Par ailleurs, le chunking peut également être vu comme une tâche d’annotation (objet de la Shared Task CoNLL’2000) et de ce fait abordé via des méthodes d’apprentissage statistique. Ce contexte nous semblait par conséquent idéal pour comparer les deux approches. Cette comparaison n’est cependant pas notre seul but. Notre intuition est que les deux techniques sont complémentaires car elles se concentrent </context>
</contexts>
<marker>Antoine, Mokrane, Friburger, 2008</marker>
<rawString>Antoine, J.-Y., Mokrane, A., and Friburger, N. (2008). Automatic rich annotation of large corpus of conversational transcribed speech: the chunking task of the epac project. In Proceedings of LREC’2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Blanc</author>
<author>M Constant</author>
<author>A Dister</author>
<author>P Watrin</author>
</authors>
<title>Partial parsing of spontaneous spoken french.</title>
<date>2010</date>
<booktitle>In Proceedings of LREC’2010.</booktitle>
<contexts>
<context position="3544" citStr="Blanc et al., 2010" startWordPosition="510" endWordPosition="513">sont souvent algorithmiquement complexes, sensibles aux erreurs et peu adaptés aux langages fondés sur de grands alphabets (ce qui est le cas quand l’alphabet est l’ensemble des mots d’une langue naturelle). Dans cet article, nous voulons donner leur chance à des algorithmes classiques d’IG pour les comparer aux méthodes d’apprentissage automatique statistique état de l’art, en l’occurrence les CRF (Lafferty et al., 2001). La tâche considérée est le chunking (Abney, 1991) du français, qui peut en effet très bien être réalisée à l’aide d’automates construits manuellement (Antoine et al., 2008; Blanc et al., 2010). À notre connaissance, essayer d’apprendre automatiquement ces automates au lieu de les écrire à la main n’a encore pas jamais été testé, pour quelque langue que ce soit. Par ailleurs, le chunking peut également être vu comme une tâche d’annotation (objet de la Shared Task CoNLL’2000) et de ce fait abordé via des méthodes d’apprentissage statistique. Ce contexte nous semblait par conséquent idéal pour comparer les deux approches. Cette comparaison n’est cependant pas notre seul but. Notre intuition est que les deux techniques sont complémentaires car elles se concentrent sur des propriétés di</context>
</contexts>
<marker>Blanc, Constant, Dister, Watrin, 2010</marker>
<rawString>Blanc, O., Constant, M., Dister, A., and Watrin, P. (2010). Partial parsing of spontaneous spoken french. In Proceedings of LREC’2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Constant</author>
<author>I Tellier</author>
</authors>
<title>Evaluating the impact of external lexical resources unto a crf-based multiword segmenter and part-of-speech tagger.</title>
<date>2012</date>
<booktitle>In Proceedings of LREC</booktitle>
<contexts>
<context position="27280" citStr="Constant and Tellier, 2012" startWordPosition="4266" endWordPosition="4269">ué en 4.1, il est tout à fait possible de simuler la structure d’un HMM (et, similairement, d’un automate) avec les features d’un CRF. On pourrait donc évaluer la qualité des états et des transitions d’un automate en fonction des poids associés aux features qui les représentent dans un CRF, offrant ainsi par la même occasion un moyen de l’améliorer. 5.1 Les automates en tant que ressource externe Nous nous attaquons ici aux deux types de chunking. Le premier mode de combinaison envisagé consiste à enrichir les données du CRF avec des attributs provenant de la ressource externe, à la façon de (Constant and Tellier, 2012). Dans le cas du chunking complet, nous appliquons l’IG à chaque type de chunk distinct, produisant ainsi autant d’automates qu’il y a de types de chunks selon un protocole de validation croisée à 5 plis (les PTA dans ces expériences sont donc uniquement extraits des corpus d’apprentissage). Chacun des automates de chunk fournit un étiquetage BIO indépendant, comme dans le tableau 3 (les automates sont ici supposés fournir un étiquetage parfait). Il y a donc dans nos données autant d’attributs nouveaux que de chunks. Les tableaux de gauche dans les tables 4 donnent les patrons aboutissant aux </context>
</contexts>
<marker>Constant, Tellier, 2012</marker>
<rawString>Constant, M. and Tellier, I. (2012). Evaluating the impact of external lexical resources unto a crf-based multiword segmenter and part-of-speech tagger. In Proceedings of LREC 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Constant</author>
<author>I Tellier</author>
<author>D Duchier</author>
<author>Y Dupont</author>
<author>A Sigogne</author>
<author>S Billot</author>
</authors>
<title>Intégrer des connaissances linguistiques dans un CRF : application à l’apprentissage d’un segmenteurétiqueteur du français.</title>
<date>2011</date>
<booktitle>In Actes de TALN’11.</booktitle>
<contexts>
<context position="7497" citStr="Constant et al., 2011" startWordPosition="1115" endWordPosition="1118">cueil de phrases extraites d’articles du journal “Le Monde” publiés entre 1989 et 1993 (Abeillé et al., 2003). Les phrases ont été tokenisées (en conservant certaines unités multi-mots), lemmatisées, étiquetées et analysées syntaxiquement. Il existe plusieurs variantes du FTB, celle que nous avons utilisée contenait environ 8 600 arbres XML enrichis de fonctions syntaxiques (parfois nécessaires pour identifier certains chunks). Pour le POS, nous avons repris les 30 étiquettes morpho-syntaxiques définies dans (Crabbé and Candito, 2008), assurant ainsi la continuité avec nos précédents travaux (Constant et al., 2011). Nous considérons 7 types de chunks distincts : AP (Adjectival Phrase), AdP (Adverbial Phrase), CONJ (Conjonctions), NP (Noun Phrase), PP (Prepositional Phrase), VP (verbal Phrase) et UNKONWN (coquilles ou certains mots étrangers, eux-mêmes étiquetés UNKNOWN). Les marques de ponctuations, sauf exceptions (certains guillemets par exemple) sont hors chunks (étiquette O comme Out). Nous avons décidé de modifier certains choix que nous avions faits dans (Tellier et al., 2012). Par exemple, le chunk CONJ contient seulement la conjonction. Le PP, en revanche, intègre toujours le chunk introduit par</context>
</contexts>
<marker>Constant, Tellier, Duchier, Dupont, Sigogne, Billot, 2011</marker>
<rawString>Constant, M., Tellier, I., Duchier, D., Dupont, Y., Sigogne, A., and Billot, S. (2011). Intégrer des connaissances linguistiques dans un CRF : application à l’apprentissage d’un segmenteurétiqueteur du français. In Actes de TALN’11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Crabbé</author>
<author>M H Candito</author>
</authors>
<title>Expériences d’analyse syntaxique statistique du français.</title>
<date>2008</date>
<booktitle>In Actes de TALN’08.</booktitle>
<contexts>
<context position="7415" citStr="Crabbé and Candito, 2008" startWordPosition="1103" endWordPosition="1106">é des CRF (Sha and Pereira, 2003). 2.2 Les données Le French TreeBank (FTB) est un recueil de phrases extraites d’articles du journal “Le Monde” publiés entre 1989 et 1993 (Abeillé et al., 2003). Les phrases ont été tokenisées (en conservant certaines unités multi-mots), lemmatisées, étiquetées et analysées syntaxiquement. Il existe plusieurs variantes du FTB, celle que nous avons utilisée contenait environ 8 600 arbres XML enrichis de fonctions syntaxiques (parfois nécessaires pour identifier certains chunks). Pour le POS, nous avons repris les 30 étiquettes morpho-syntaxiques définies dans (Crabbé and Candito, 2008), assurant ainsi la continuité avec nos précédents travaux (Constant et al., 2011). Nous considérons 7 types de chunks distincts : AP (Adjectival Phrase), AdP (Adverbial Phrase), CONJ (Conjonctions), NP (Noun Phrase), PP (Prepositional Phrase), VP (verbal Phrase) et UNKONWN (coquilles ou certains mots étrangers, eux-mêmes étiquetés UNKNOWN). Les marques de ponctuations, sauf exceptions (certains guillemets par exemple) sont hors chunks (étiquette O comme Out). Nous avons décidé de modifier certains choix que nous avions faits dans (Tellier et al., 2012). Par exemple, le chunk CONJ contient seu</context>
</contexts>
<marker>Crabbé, Candito, 2008</marker>
<rawString>Crabbé, B. and Candito, M. H. (2008). Expériences d’analyse syntaxique statistique du français. In Actes de TALN’08.</rawString>
</citation>
<citation valid="true">
<authors>
<author>de la Higuera</author>
<author>C</author>
</authors>
<title>Grammatical Inference: Learning Automata and Grammars.</title>
<date>2010</date>
<publisher>CU Press.</publisher>
<marker>Higuera, C, 2010</marker>
<rawString>de la Higuera, C. (2010). Grammatical Inference: Learning Automata and Grammars. CU Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Denis</author>
<author>A Lemay</author>
<author>A Terlutte</author>
</authors>
<title>Some language classes identifiable in the limit from positive data.</title>
<date>2002</date>
<booktitle>In ICGI 2002, number 2484 in LNAI,</booktitle>
<pages>63--76</pages>
<publisher>Springer Verlag.</publisher>
<contexts>
<context position="12688" citStr="Denis et al., 2002" startWordPosition="1886" endWordPosition="1889">existe aucun algorithme capable d’apprendre par présentation positive la classe complète des langages réguliers dans ces modèles (Gold, 1967; Kearns and Vazirani, 1994). Les recherches se sont donc orientées vers des classes plus petites, ou transverses à la hiérarchie de Chomsky, et apprenables, caractérisées notamment dans (Angluin, 1980). Les classes de langages k-réversibles (Angluin, 1982) entrent dans ce cadre, elles constituent le point de départ de nos expériences. Depuis, bien d’autres classes apprenables par présentation positive ont été décrites et étudiées (Garcia and Vidal, 1990; Denis et al., 2002; Kanazawa, 1998; Koshiba et al., 2000; Yokomori, 2003). Des avancées récentes dans le domaine concernent aussi l’apprenabilité de dispositifs intégrant des probabilités, comme les automates probabilistes et leurs liens avec les HHM (Thollard et al., 2000; Dupont et al., 2005). Parallèlement, des compétitions 3 ont permis de tester l’efficacité des algorithmes proposés lorsqu’ils sont confrontés à des données réelles. 3les plus récents étant Stamina ( ) et Zulu ( ) 22 c ATALA TALN-RÉCITAL 2013, 17-21 Juin, Les Sables d’Olonne 3.2 L’algorithme k-RI Dans cette section, nous décrivons les algori</context>
<context position="35449" citStr="Denis et al., 2002" startWordPosition="5554" endWordPosition="5557">s améliorations observées dans la Table 6 laissent en effet supposer qu’à de nombreuses occasions le CRF a eu raison de prendre une décision différente de celle préconisée par l’automate. Conclusion et perspectives Dans cet article, nous avons appliqué deux méthodes d’apprentissage automatique sur le même jeu de données et avons proposé deux façons différentes de les combiner. Pour ce qui est de l’apprentissage symbolique seul, il est possible que d’autres algorithmes d’IG par présentation positive pourraient donner de meilleurs résultats que les nôtres, comme ceux de (Garcia and Vidal, 1990; Denis et al., 2002). Le choix d’une grande valeur de k dans certains cas peut être important, mais il s’accompagne d’une plus grande complexité de calculs6. Mais la partie la plus originale de notre travail concerne les combinaisons automates/CRF. Notons que ces combinaisons peuvent tout autant s’appliquer à des automates écrits à la main, généralement plus pertinents d’un point de vue linguistique que ceux obtenus par IG. Nous nous sommes concentrés ici sur des automates appris automatiquement pour montrer que, même sans ressource ni expertise linguistique, il est possible de combiner modèles symboliques et sta</context>
</contexts>
<marker>Denis, Lemay, Terlutte, 2002</marker>
<rawString>Denis, F., Lemay, A., and Terlutte, A. (2002). Some language classes identifiable in the limit from positive data. In ICGI 2002, number 2484 in LNAI, pages 63–76. Springer Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Dupont</author>
<author>F Denis</author>
<author>Y Esposito</author>
</authors>
<title>Links between probabilistic automata and hidden markov models: probability distributions, learning models and induction algorithms.</title>
<date>2005</date>
<journal>Pattern Recognition,</journal>
<volume>38</volume>
<issue>9</issue>
<contexts>
<context position="12965" citStr="Dupont et al., 2005" startWordPosition="1926" endWordPosition="1929">omsky, et apprenables, caractérisées notamment dans (Angluin, 1980). Les classes de langages k-réversibles (Angluin, 1982) entrent dans ce cadre, elles constituent le point de départ de nos expériences. Depuis, bien d’autres classes apprenables par présentation positive ont été décrites et étudiées (Garcia and Vidal, 1990; Denis et al., 2002; Kanazawa, 1998; Koshiba et al., 2000; Yokomori, 2003). Des avancées récentes dans le domaine concernent aussi l’apprenabilité de dispositifs intégrant des probabilités, comme les automates probabilistes et leurs liens avec les HHM (Thollard et al., 2000; Dupont et al., 2005). Parallèlement, des compétitions 3 ont permis de tester l’efficacité des algorithmes proposés lorsqu’ils sont confrontés à des données réelles. 3les plus récents étant Stamina ( ) et Zulu ( ) 22 c ATALA TALN-RÉCITAL 2013, 17-21 Juin, Les Sables d’Olonne 3.2 L’algorithme k-RI Dans cette section, nous décrivons les algorithmes d’IG par présentation positive utilisés dans nos expériences. Ils sont destinés à apprendre un automate pour un type spécifique de chunk, à partir uniquement des différentes séquences de POS aparaissant dans ce type de chunks dans les données d’apprentissage. Les algorit</context>
</contexts>
<marker>Dupont, Denis, Esposito, 2005</marker>
<rawString>Dupont, P., Denis, F., and Esposito, Y. (2005). Links between probabilistic automata and hidden markov models: probability distributions, learning models and induction algorithms. Pattern Recognition, 38(9):1349–1371.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Dupont</author>
<author>L Miclet</author>
<author>E Vidal</author>
</authors>
<title>What is the search space of the regular inference.</title>
<date>1994</date>
<booktitle>In ICGI’94 - LNCS, volume 862 - Grammatical Inference and Applications,</booktitle>
<pages>25--37</pages>
<location>Heidelberg.</location>
<contexts>
<context position="14991" citStr="Dupont et al., 1994" startWordPosition="2238" endWordPosition="2241">2, la classe des langages k1-réversibles est strictement incluse dans celle des langages k2-réversibles. Soit un ensemble de séquences positives S, la première étape de k-RI est de construire PTA(S), le Prefix Tree Acceptor de S. PTA(S) est le plus petit (en nombre d’états) AFD (Automate Fini Déterministe) en forme d’arbre reconnaissant exactement le langage S. La racine de PTA(S) est son état initial. L’espace de recherche de tout algorithme d’IG partant de S est un trelli dont la borne inférieure est PTA(S) et la borne supérieure l’automate universel construit sur l’alphabet observé dans S (Dupont et al., 1994). La plupart des algorithmes d’IG suivent le même schéma : ils partent de PTA(S) pour ensuite généraliser le langage défini par fusions d’états, la connaissance de k permettant d’éviter la surgénéralisation. k-RI, détaillé ci-dessous, fonctionne selon ce principe. La fusion qu’il emploie est appelée déterministe car elle se propage récursivement à travers l’automate pour préserver son déterminisme. Algorithme k-RI Entrée : S : un ensemble de séquences (positives), k : un entier naturel ; Sortie : A : un automate k-réversible ; début A := PTA(S); tant que non(A k-réversible) faire // soient N1 </context>
</contexts>
<marker>Dupont, Miclet, Vidal, 1994</marker>
<rawString>Dupont, P., Miclet, L., and Vidal, E. (1994). What is the search space of the regular inference. In ICGI’94 - LNCS, volume 862 - Grammatical Inference and Applications, pages 25–37, Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Finkel</author>
<author>A Kleeman</author>
<author>C D Manning</author>
</authors>
<title>Efficient, feature-based, conditional random field parsing.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL’2008,</booktitle>
<pages>959--967</pages>
<contexts>
<context position="22066" citStr="Finkel et al., 2008" startWordPosition="3368" endWordPosition="3371"> en autant de features qu’il y a de positions sur les données d’entraînement où ils peuvent s’appliquer. L’implémentation la plus efficace à l’heure actuelle des CRF linéaires est fournie par Wapiti5, qui utilise des pénalisations pour sélectionner les features les plus pertinentes (Lavergne et al., 2010). C’est le logiciel que nous avons utilisé. Les CRF se sont montrés efficaces sur de nombreuses tâches d’annotation, notamment l’étiquetage POS (Lafferty et al., 2001), la reconnaissance d’entités nommées (McCallum and Li, 2003), le chunking (Sha and Pereira, 2003) et même le parsing complet (Finkel et al., 2008; Tsuruoka 5 25 c ATALA TALN-RÉCITAL 2013, 17-21 Juin, Les Sables d’Olonne chunking Complet NP seuls Feature Type Fenêtre micro 97.53 N/A Mot Unigram [-2..1] macro 90.49 N/A POS Bigram [-2..1] F1-mesure N/A 96.43 Table 2: Le patron de template et les résultats obtenus avec les CRF seuls pour chaque tâche et al., 2009). Leur principal inconvénient est qu’ils apparaissent comme des “boîtes noires”. Un modèle issu d’un apprentissage par CRF est simplement une liste de features pondérées pouvant avoir plusieurs millions d’éléments, ce qui le rend difficile à interpréter. Les HMM, qui étaient parm</context>
</contexts>
<marker>Finkel, Kleeman, Manning, 2008</marker>
<rawString>Finkel, J. R., Kleeman, A., and Manning, C. D. (2008). Efficient, feature-based, conditional random field parsing. In Proceedings of ACL’2008, pages 959–967.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Garcia</author>
<author>E Vidal</author>
</authors>
<title>31 c ATALA TALN-RÉCITAL 2013, 17-21 Juin, Les Sables d’Olonne</title>
<date>1990</date>
<journal>IEEE TPAMI,</journal>
<volume>12</volume>
<issue>9</issue>
<contexts>
<context position="12668" citStr="Garcia and Vidal, 1990" startWordPosition="1882" endWordPosition="1885">les à satisfaire : il n’existe aucun algorithme capable d’apprendre par présentation positive la classe complète des langages réguliers dans ces modèles (Gold, 1967; Kearns and Vazirani, 1994). Les recherches se sont donc orientées vers des classes plus petites, ou transverses à la hiérarchie de Chomsky, et apprenables, caractérisées notamment dans (Angluin, 1980). Les classes de langages k-réversibles (Angluin, 1982) entrent dans ce cadre, elles constituent le point de départ de nos expériences. Depuis, bien d’autres classes apprenables par présentation positive ont été décrites et étudiées (Garcia and Vidal, 1990; Denis et al., 2002; Kanazawa, 1998; Koshiba et al., 2000; Yokomori, 2003). Des avancées récentes dans le domaine concernent aussi l’apprenabilité de dispositifs intégrant des probabilités, comme les automates probabilistes et leurs liens avec les HHM (Thollard et al., 2000; Dupont et al., 2005). Parallèlement, des compétitions 3 ont permis de tester l’efficacité des algorithmes proposés lorsqu’ils sont confrontés à des données réelles. 3les plus récents étant Stamina ( ) et Zulu ( ) 22 c ATALA TALN-RÉCITAL 2013, 17-21 Juin, Les Sables d’Olonne 3.2 L’algorithme k-RI Dans cette section, nous </context>
<context position="35428" citStr="Garcia and Vidal, 1990" startWordPosition="5550" endWordPosition="5553">ifier en conséquence. Les améliorations observées dans la Table 6 laissent en effet supposer qu’à de nombreuses occasions le CRF a eu raison de prendre une décision différente de celle préconisée par l’automate. Conclusion et perspectives Dans cet article, nous avons appliqué deux méthodes d’apprentissage automatique sur le même jeu de données et avons proposé deux façons différentes de les combiner. Pour ce qui est de l’apprentissage symbolique seul, il est possible que d’autres algorithmes d’IG par présentation positive pourraient donner de meilleurs résultats que les nôtres, comme ceux de (Garcia and Vidal, 1990; Denis et al., 2002). Le choix d’une grande valeur de k dans certains cas peut être important, mais il s’accompagne d’une plus grande complexité de calculs6. Mais la partie la plus originale de notre travail concerne les combinaisons automates/CRF. Notons que ces combinaisons peuvent tout autant s’appliquer à des automates écrits à la main, généralement plus pertinents d’un point de vue linguistique que ceux obtenus par IG. Nous nous sommes concentrés ici sur des automates appris automatiquement pour montrer que, même sans ressource ni expertise linguistique, il est possible de combiner modèl</context>
</contexts>
<marker>Garcia, Vidal, 1990</marker>
<rawString>31 c ATALA TALN-RÉCITAL 2013, 17-21 Juin, Les Sables d’Olonne Garcia, P. and Vidal, E. (1990). Inference of k-testable languages in the strict sense and application to syntactic pattern recognition. IEEE TPAMI, 12(9):920–925.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Gold</author>
</authors>
<title>Language identification in the limit.</title>
<date>1967</date>
<journal>Information and Control,</journal>
<pages>10--447</pages>
<contexts>
<context position="11835" citStr="Gold, 1967" startWordPosition="1764" endWordPosition="1765"> l’acquisition du langage chez les enfants. Un enfant n’est pas “programmé” pour une langue précise, il est capable d’acquérir n’importe laquelle parlée dans son environnement. De même, un programme d’IG par présentation positive doit être en mesure d’apprendre une classe de langages formels, c’est-à-dire d’identifier n’importe lequel de ses membres à l’aide d’exemples de séquences (phrases) lui appartenant. Les principaux critères possibles qui caractérisent la notion d”’apprenabilité d’une classe de langages” en IG (aussi appelés modèles d’apprentissage) sont “l’identification à la limite” (Gold, 1967) et “l’apprentissage PAC” (Valiant, 1984), que nous ne pouvons détailler ici. Malheureusement, même pour la classe des langages réguliers, la plus simple dans la hiérarchie de Chomsky, ces critères sont impossibles à satisfaire : il n’existe aucun algorithme capable d’apprendre par présentation positive la classe complète des langages réguliers dans ces modèles (Gold, 1967; Kearns and Vazirani, 1994). Les recherches se sont donc orientées vers des classes plus petites, ou transverses à la hiérarchie de Chomsky, et apprenables, caractérisées notamment dans (Angluin, 1980). Les classes de langag</context>
</contexts>
<marker>Gold, 1967</marker>
<rawString>Gold, E. (1967). Language identification in the limit. Information and Control, 10:447–474.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Kanazawa</author>
</authors>
<title>Learnable Classes of Categorial Grammars.</title>
<date>1998</date>
<publisher>FoLLI. CLSI Publications.</publisher>
<contexts>
<context position="12704" citStr="Kanazawa, 1998" startWordPosition="1890" endWordPosition="1891">hme capable d’apprendre par présentation positive la classe complète des langages réguliers dans ces modèles (Gold, 1967; Kearns and Vazirani, 1994). Les recherches se sont donc orientées vers des classes plus petites, ou transverses à la hiérarchie de Chomsky, et apprenables, caractérisées notamment dans (Angluin, 1980). Les classes de langages k-réversibles (Angluin, 1982) entrent dans ce cadre, elles constituent le point de départ de nos expériences. Depuis, bien d’autres classes apprenables par présentation positive ont été décrites et étudiées (Garcia and Vidal, 1990; Denis et al., 2002; Kanazawa, 1998; Koshiba et al., 2000; Yokomori, 2003). Des avancées récentes dans le domaine concernent aussi l’apprenabilité de dispositifs intégrant des probabilités, comme les automates probabilistes et leurs liens avec les HHM (Thollard et al., 2000; Dupont et al., 2005). Parallèlement, des compétitions 3 ont permis de tester l’efficacité des algorithmes proposés lorsqu’ils sont confrontés à des données réelles. 3les plus récents étant Stamina ( ) et Zulu ( ) 22 c ATALA TALN-RÉCITAL 2013, 17-21 Juin, Les Sables d’Olonne 3.2 L’algorithme k-RI Dans cette section, nous décrivons les algorithmes d’IG par p</context>
</contexts>
<marker>Kanazawa, 1998</marker>
<rawString>Kanazawa, M. (1998). Learnable Classes of Categorial Grammars. FoLLI. CLSI Publications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M J Kearns</author>
<author>U V Vazirani</author>
</authors>
<title>An Introduction to Computational Learning Theory.</title>
<date>1994</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="12238" citStr="Kearns and Vazirani, 1994" startWordPosition="1820" endWordPosition="1823">rases) lui appartenant. Les principaux critères possibles qui caractérisent la notion d”’apprenabilité d’une classe de langages” en IG (aussi appelés modèles d’apprentissage) sont “l’identification à la limite” (Gold, 1967) et “l’apprentissage PAC” (Valiant, 1984), que nous ne pouvons détailler ici. Malheureusement, même pour la classe des langages réguliers, la plus simple dans la hiérarchie de Chomsky, ces critères sont impossibles à satisfaire : il n’existe aucun algorithme capable d’apprendre par présentation positive la classe complète des langages réguliers dans ces modèles (Gold, 1967; Kearns and Vazirani, 1994). Les recherches se sont donc orientées vers des classes plus petites, ou transverses à la hiérarchie de Chomsky, et apprenables, caractérisées notamment dans (Angluin, 1980). Les classes de langages k-réversibles (Angluin, 1982) entrent dans ce cadre, elles constituent le point de départ de nos expériences. Depuis, bien d’autres classes apprenables par présentation positive ont été décrites et étudiées (Garcia and Vidal, 1990; Denis et al., 2002; Kanazawa, 1998; Koshiba et al., 2000; Yokomori, 2003). Des avancées récentes dans le domaine concernent aussi l’apprenabilité de dispositifs intégra</context>
</contexts>
<marker>Kearns, Vazirani, 1994</marker>
<rawString>Kearns, M. J. and Vazirani, U. V. (1994). An Introduction to Computational Learning Theory. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Koshiba</author>
<author>E Mäkinen</author>
<author>Y Takada</author>
</authors>
<title>Inferring pure context-free languages from positive data.</title>
<date>2000</date>
<journal>Acta Cybernetica,</journal>
<volume>14</volume>
<issue>3</issue>
<contexts>
<context position="12726" citStr="Koshiba et al., 2000" startWordPosition="1892" endWordPosition="1895">prendre par présentation positive la classe complète des langages réguliers dans ces modèles (Gold, 1967; Kearns and Vazirani, 1994). Les recherches se sont donc orientées vers des classes plus petites, ou transverses à la hiérarchie de Chomsky, et apprenables, caractérisées notamment dans (Angluin, 1980). Les classes de langages k-réversibles (Angluin, 1982) entrent dans ce cadre, elles constituent le point de départ de nos expériences. Depuis, bien d’autres classes apprenables par présentation positive ont été décrites et étudiées (Garcia and Vidal, 1990; Denis et al., 2002; Kanazawa, 1998; Koshiba et al., 2000; Yokomori, 2003). Des avancées récentes dans le domaine concernent aussi l’apprenabilité de dispositifs intégrant des probabilités, comme les automates probabilistes et leurs liens avec les HHM (Thollard et al., 2000; Dupont et al., 2005). Parallèlement, des compétitions 3 ont permis de tester l’efficacité des algorithmes proposés lorsqu’ils sont confrontés à des données réelles. 3les plus récents étant Stamina ( ) et Zulu ( ) 22 c ATALA TALN-RÉCITAL 2013, 17-21 Juin, Les Sables d’Olonne 3.2 L’algorithme k-RI Dans cette section, nous décrivons les algorithmes d’IG par présentation positive u</context>
</contexts>
<marker>Koshiba, Mäkinen, Takada, 2000</marker>
<rawString>Koshiba, T., Mäkinen, E., and Takada, Y. (2000). Inferring pure context-free languages from positive data. Acta Cybernetica, 14(3):469–477.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lafferty</author>
<author>A McCallum</author>
<author>F Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proceedings of ICML</booktitle>
<pages>282--289</pages>
<contexts>
<context position="3350" citStr="Lafferty et al., 2001" startWordPosition="479" endWordPosition="482">rigine dans l’informatique théorique et la théorie des langages formels, est souvent méconnu. Les algorithmes d’IG sont en effet réputés ne pas très bien se comporter sur des données réelles : ils sont souvent algorithmiquement complexes, sensibles aux erreurs et peu adaptés aux langages fondés sur de grands alphabets (ce qui est le cas quand l’alphabet est l’ensemble des mots d’une langue naturelle). Dans cet article, nous voulons donner leur chance à des algorithmes classiques d’IG pour les comparer aux méthodes d’apprentissage automatique statistique état de l’art, en l’occurrence les CRF (Lafferty et al., 2001). La tâche considérée est le chunking (Abney, 1991) du français, qui peut en effet très bien être réalisée à l’aide d’automates construits manuellement (Antoine et al., 2008; Blanc et al., 2010). À notre connaissance, essayer d’apprendre automatiquement ces automates au lieu de les écrire à la main n’a encore pas jamais été testé, pour quelque langue que ce soit. Par ailleurs, le chunking peut également être vu comme une tâche d’annotation (objet de la Shared Task CoNLL’2000) et de ce fait abordé via des méthodes d’apprentissage statistique. Ce contexte nous semblait par conséquent idéal pour </context>
<context position="20478" citStr="Lafferty et al., 2001" startWordPosition="3110" endWordPosition="3113">dre k = 2 pour obtenir un automate meilleur que le PTA sur des données nettoyées. 4 Apprentissage statistique pour l’annotation Dans cette section, nous nous concentrons sur la meilleure approche statistique actuelle pour une tâche d’annotation : les Conditional Random Fields (CRF), qui se comportent très bien sur notre problème (Tellier et al., 2012). Nous rappelons aussi comment un HMM peut être “transformé” en un CRF, parce que cette transformation sera une source d’inspiration pour une des combinaisons présentées par la suite. 4.1 Conditional Random Fields et HMMs Les CRF, introduits par (Lafferty et al., 2001) sont de la famille des modèles graphiques. Lorsque que le graphe exprimant les dépendances entre étiquettes est linéaire (ce qui est généralement le cas pour étiqueter des séquences), la distribution de probabilité d’une séquence d’annotations y connaissant une séquence observable x est définie par :  p(y |1  Kx) = exp λ Z(x) k fk(t, yt , yt−1, x) t k=1 Où Z(x) est un facteur de normalisation dépendant de x et les K features (ou fonctions caractéristiques) fk des fonctions fournies par l’utilisateur. Une feature fk est vérifiée (i.e. fk(t, yt , yt−1, x) = 1) si, à la position courante t, </context>
<context position="21920" citStr="Lafferty et al., 2001" startWordPosition="3345" endWordPosition="3348">éfinir un grand nombre de features, les programmes implémentant les CRF permettent d’avoir recours à des patrons (ou templates) qui seront instanciés en autant de features qu’il y a de positions sur les données d’entraînement où ils peuvent s’appliquer. L’implémentation la plus efficace à l’heure actuelle des CRF linéaires est fournie par Wapiti5, qui utilise des pénalisations pour sélectionner les features les plus pertinentes (Lavergne et al., 2010). C’est le logiciel que nous avons utilisé. Les CRF se sont montrés efficaces sur de nombreuses tâches d’annotation, notamment l’étiquetage POS (Lafferty et al., 2001), la reconnaissance d’entités nommées (McCallum and Li, 2003), le chunking (Sha and Pereira, 2003) et même le parsing complet (Finkel et al., 2008; Tsuruoka 5 25 c ATALA TALN-RÉCITAL 2013, 17-21 Juin, Les Sables d’Olonne chunking Complet NP seuls Feature Type Fenêtre micro 97.53 N/A Mot Unigram [-2..1] macro 90.49 N/A POS Bigram [-2..1] F1-mesure N/A 96.43 Table 2: Le patron de template et les résultats obtenus avec les CRF seuls pour chaque tâche et al., 2009). Leur principal inconvénient est qu’ils apparaissent comme des “boîtes noires”. Un modèle issu d’un apprentissage par CRF est simplem</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>Lafferty, J., McCallum, A., and Pereira, F. (2001). Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of ICML 2001, pages 282–289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Lavergne</author>
<author>O Cappé</author>
<author>F Yvon</author>
</authors>
<title>Practical very large scale CRFs.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL’2010,</booktitle>
<pages>504--513</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="21753" citStr="Lavergne et al., 2010" startWordPosition="3320" endWordPosition="3323">elle vaut 0 sinon). À chaque feature fk est associé un poids λk. Ces poids constituent les paramètres du modèle devant être estimés au cours de l’apprentissage. Pour définir un grand nombre de features, les programmes implémentant les CRF permettent d’avoir recours à des patrons (ou templates) qui seront instanciés en autant de features qu’il y a de positions sur les données d’entraînement où ils peuvent s’appliquer. L’implémentation la plus efficace à l’heure actuelle des CRF linéaires est fournie par Wapiti5, qui utilise des pénalisations pour sélectionner les features les plus pertinentes (Lavergne et al., 2010). C’est le logiciel que nous avons utilisé. Les CRF se sont montrés efficaces sur de nombreuses tâches d’annotation, notamment l’étiquetage POS (Lafferty et al., 2001), la reconnaissance d’entités nommées (McCallum and Li, 2003), le chunking (Sha and Pereira, 2003) et même le parsing complet (Finkel et al., 2008; Tsuruoka 5 25 c ATALA TALN-RÉCITAL 2013, 17-21 Juin, Les Sables d’Olonne chunking Complet NP seuls Feature Type Fenêtre micro 97.53 N/A Mot Unigram [-2..1] macro 90.49 N/A POS Bigram [-2..1] F1-mesure N/A 96.43 Table 2: Le patron de template et les résultats obtenus avec les CRF seul</context>
</contexts>
<marker>Lavergne, Cappé, Yvon, 2010</marker>
<rawString>Lavergne, T., Cappé, O., and Yvon, F. (2010). Practical very large scale CRFs. In Proceedings of ACL’2010, pages 504–513. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A McCallum</author>
<author>W Li</author>
</authors>
<title>Early results for named entity recognition with conditional random fields.</title>
<date>2003</date>
<booktitle>In Proceedings of CoNLL’2003.</booktitle>
<contexts>
<context position="21981" citStr="McCallum and Li, 2003" startWordPosition="3353" endWordPosition="3356">nt les CRF permettent d’avoir recours à des patrons (ou templates) qui seront instanciés en autant de features qu’il y a de positions sur les données d’entraînement où ils peuvent s’appliquer. L’implémentation la plus efficace à l’heure actuelle des CRF linéaires est fournie par Wapiti5, qui utilise des pénalisations pour sélectionner les features les plus pertinentes (Lavergne et al., 2010). C’est le logiciel que nous avons utilisé. Les CRF se sont montrés efficaces sur de nombreuses tâches d’annotation, notamment l’étiquetage POS (Lafferty et al., 2001), la reconnaissance d’entités nommées (McCallum and Li, 2003), le chunking (Sha and Pereira, 2003) et même le parsing complet (Finkel et al., 2008; Tsuruoka 5 25 c ATALA TALN-RÉCITAL 2013, 17-21 Juin, Les Sables d’Olonne chunking Complet NP seuls Feature Type Fenêtre micro 97.53 N/A Mot Unigram [-2..1] macro 90.49 N/A POS Bigram [-2..1] F1-mesure N/A 96.43 Table 2: Le patron de template et les résultats obtenus avec les CRF seuls pour chaque tâche et al., 2009). Leur principal inconvénient est qu’ils apparaissent comme des “boîtes noires”. Un modèle issu d’un apprentissage par CRF est simplement une liste de features pondérées pouvant avoir plusieurs m</context>
</contexts>
<marker>McCallum, Li, 2003</marker>
<rawString>McCallum, A. and Li, W. (2003). Early results for named entity recognition with conditional random fields. In Proceedings of CoNLL’2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Paroubek</author>
<author>I Robba</author>
<author>A Vilnat</author>
<author>A C</author>
</authors>
<title>Data annotations and measures in easy, the evaluation campain for parsers of french.</title>
<date>2006</date>
<booktitle>In Proceedings of LREC’2006,</booktitle>
<pages>315--320</pages>
<contexts>
<context position="8156" citStr="Paroubek et al., 2006" startWordPosition="1212" endWordPosition="1215">distincts : AP (Adjectival Phrase), AdP (Adverbial Phrase), CONJ (Conjonctions), NP (Noun Phrase), PP (Prepositional Phrase), VP (verbal Phrase) et UNKONWN (coquilles ou certains mots étrangers, eux-mêmes étiquetés UNKNOWN). Les marques de ponctuations, sauf exceptions (certains guillemets par exemple) sont hors chunks (étiquette O comme Out). Nous avons décidé de modifier certains choix que nous avions faits dans (Tellier et al., 2012). Par exemple, le chunk CONJ contient seulement la conjonction. Le PP, en revanche, intègre toujours le chunk introduit par la préposition. Et, à l’inverse de (Paroubek et al., 2006), les adjectifs épithètes appartiennent toujours au chunk NP contenant le nom qu’ils qualifient, qu’ils soient situés avant ou après lui. Les chunks AP sont donc assez rares car ils ne correspondent qu’aux adjectifs séparés d’un groupe nominal, comme les attributs du sujet ou de l’objet (les fonctions syntaxiques disponibles dans les arbres XML sont nécessaires pour identifier ces derniers). La phrase suivante illustre notre notion de parenthésage en chunks2 : (la/DET dépréciation/NC)NP (par_rapport_au/P dollar/NC)PP (a/V été/VPP limitée/VPP)V P (à/P 2,5/DET %/NC)PP Nous avons extrait du FTB d</context>
<context position="24910" citStr="Paroubek et al., 2006" startWordPosition="3839" endWordPosition="3842">dation croisée à 5 plis et un critère d’égalité stricte des chunks. Pour la tâche de chunking complet, nous avons calculé les micro et macro-average, qui correspondent aux moyennes des F1-mesures des différents types de chunks, pondérées (micro) ou pas (macro) par leur proportion. Comme attendu, les CRF seuls sont très performants. Remarquons toutefois qu’ils exploitent dans leurs features à la fois des mots et des étiquettes POS présents dans les données, alors que les algorithmes d’IG n’ont accès qu’aux seuls POS. On peut comparer ces résultats avec ceux obtenus lors de la campagne PASSAGE (Paroubek et al., 2006), même si les notions de chunks adoptées de part et d’autre diffèrent (dans PASSAGE, les adjectifs épithètes situés après un nom ne font pas partie du chunk nomimal, par exemple) et si les corpus ne sont pas les mêmes. Les meilleurs participants de la campagne PASSAGE atteignaient une micro-average de 92,7, ce qui situe tout de même la performance de nos CRF. 26 c ATALA TALN-RÉCITAL 2013, 17-21 Juin, Les Sables d’Olonne mot POS auto. NP auto. VP auto. PP ... label correct auto. NP NP-label correct la DET B O O ... B-NP B B dépréciation NC I O O ... I-NP I I par_rapport_au P O O B ... B-PP O O</context>
</contexts>
<marker>Paroubek, Robba, Vilnat, C, 2006</marker>
<rawString>Paroubek, P., Robba, I., Vilnat, A., and C., A. (2006). Data annotations and measures in easy, the evaluation campain for parsers of french. In Proceedings of LREC’2006, pages 315–320.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Pu</author>
<author>Q Mao</author>
<author>G Wu</author>
<author>C Yuan</author>
</authors>
<title>Chinese named entity recognition with the improved smoothed conditional random fields.</title>
<date>2010</date>
<journal>Research in Computing Science,</journal>
<pages>46--90</pages>
<contexts>
<context position="36692" citStr="Pu et al., 2010" startWordPosition="5745" endWordPosition="5748">ière ce travail est que ces deux types de modèles sont complémentaires, et qu’ils peuvent chacun bénéficier de l’autre. Les CRF sont basés sur un grand nombre de configurations locales pondérées. Il est théoriquement possible d’utiliser dans un CRF des features portant sur l’intégralité de la séquence x mais dans la pratique, cela est rarement fait. L’IG au contraire s’applique à un ensemble de séquences globales qu’elle est capable de généraliser. Il a déjà été observé que les CRF gagnent à recourir à des features exprimant des propriétés plus générales que de simples configurations locales (Pu et al., 2010). Notre pari était que l’IG pouvait fournir ce type de généralisation, via le premier mode de combinaison. Les résultats obtenus vont dans ce sens. Il est aussi intéressant de constater que les modèles symboliques permettent d’améliorer le traitement des cas rares, mal pris en compte par les modèles statistiques. Les CRF construits sur des automates restent encore à étudier, notamment pour interpréter et exploiter au mieux les matrices de confusion qu’ils produisent. Certaines cases de ces matrices sont vides car Wapiti élimine les features non pertinentes de l’ensemble de départ selon un crit</context>
</contexts>
<marker>Pu, Mao, Wu, Yuan, 2010</marker>
<rawString>Pu, X., Mao, Q., Wu, G., and Yuan, C. (2010). Chinese named entity recognition with the improved smoothed conditional random fields. Research in Computing Science, 46:90–103.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Roark</author>
<author>M Saraclar</author>
</authors>
<title>Discriminative language modeling with conditional random fields and the perceptron algorithm.</title>
<date>2004</date>
<booktitle>In Proceedings of ACL’2004,</booktitle>
<pages>47--54</pages>
<contexts>
<context position="29810" citStr="Roark and Saraclar, 2004" startWordPosition="4652" endWordPosition="4655">Mot Unigram [-2..1] pur 2-RI LM POS Bigram [-2..1] Automate Bigram [-1..1] F-measure 96.75 POS+Automate Bigram [-1..1] Table 4: Patrons et meilleure micro-average (resp. macro-average) pour le chunking complet, idem pour la F-mesure du chunking NP seul 5.2 Diagnostiquer un automate à l’aide d’un CRF Nous voulons ici obtenir des informations sur l’automate produit par IG à l’aide des CRF, en faisant un apprentissage n’utilisant que des features interprétables relativement à lui. Les poids associés par le CRF à ces features fourniront un diagnostic fin de l’automate. Cette idée se rapproche de (Roark and Saraclar, 2004), où un CRF était appris selon la structure d’un automate pondéré pour le “corriger” grâce à l’estimation des poids. Elle en diffère toutefois car nous ne cherchons pas à obtenir un automate pondéré mais à trouver d’éventuelles modifications à effectuer sur l’automate selon le diagnostic fourni par le CRF, tout en préservant sa nature purement symbolique. Pour illustrer cette approche, nous nous concentrons sur la tâche de chunking NP seul car elle ne nécessite la prise en compte que d’un seul automate. Il peut être plus facile pour comprendre la suite de se représenter les automates finis dét</context>
</contexts>
<marker>Roark, Saraclar, 2004</marker>
<rawString>Roark, B. and Saraclar, M. (2004). Discriminative language modeling with conditional random fields and the perceptron algorithm. In Proceedings of ACL’2004, pages 47–54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Sha</author>
<author>F Pereira</author>
</authors>
<title>Shallow parsing with conditional random fields.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT-NAACL 2003,</booktitle>
<pages>213--220</pages>
<contexts>
<context position="6823" citStr="Sha and Pereira, 2003" startWordPosition="1018" endWordPosition="1021">, 17-21 Juin, Les Sables d’Olonne par le type (ou étiquette Part-Of-Speech (POS)) de sa tête. Il y a ainsi autant de types de chunks que de types de têtes fortes possibles. La tâche de chunking a fait l’objet de de la compétition CoNLL’20001, dont le corpus d’apprentissage était constitué d’environ 9 000 phrases issues du Penn Treebank, associées à deux niveaux d’annotion : un niveau POS donné par l’étiqueteur Brill et un de chunking. Les vainqueurs avaient utilisé des SVM et des “Weighted Probability Distribution Voting”. Ce même corpus a aussi servi plus tard à montrer l’efficacité des CRF (Sha and Pereira, 2003). 2.2 Les données Le French TreeBank (FTB) est un recueil de phrases extraites d’articles du journal “Le Monde” publiés entre 1989 et 1993 (Abeillé et al., 2003). Les phrases ont été tokenisées (en conservant certaines unités multi-mots), lemmatisées, étiquetées et analysées syntaxiquement. Il existe plusieurs variantes du FTB, celle que nous avons utilisée contenait environ 8 600 arbres XML enrichis de fonctions syntaxiques (parfois nécessaires pour identifier certains chunks). Pour le POS, nous avons repris les 30 étiquettes morpho-syntaxiques définies dans (Crabbé and Candito, 2008), assura</context>
<context position="22018" citStr="Sha and Pereira, 2003" startWordPosition="3359" endWordPosition="3362"> à des patrons (ou templates) qui seront instanciés en autant de features qu’il y a de positions sur les données d’entraînement où ils peuvent s’appliquer. L’implémentation la plus efficace à l’heure actuelle des CRF linéaires est fournie par Wapiti5, qui utilise des pénalisations pour sélectionner les features les plus pertinentes (Lavergne et al., 2010). C’est le logiciel que nous avons utilisé. Les CRF se sont montrés efficaces sur de nombreuses tâches d’annotation, notamment l’étiquetage POS (Lafferty et al., 2001), la reconnaissance d’entités nommées (McCallum and Li, 2003), le chunking (Sha and Pereira, 2003) et même le parsing complet (Finkel et al., 2008; Tsuruoka 5 25 c ATALA TALN-RÉCITAL 2013, 17-21 Juin, Les Sables d’Olonne chunking Complet NP seuls Feature Type Fenêtre micro 97.53 N/A Mot Unigram [-2..1] macro 90.49 N/A POS Bigram [-2..1] F1-mesure N/A 96.43 Table 2: Le patron de template et les résultats obtenus avec les CRF seuls pour chaque tâche et al., 2009). Leur principal inconvénient est qu’ils apparaissent comme des “boîtes noires”. Un modèle issu d’un apprentissage par CRF est simplement une liste de features pondérées pouvant avoir plusieurs millions d’éléments, ce qui le rend di</context>
</contexts>
<marker>Sha, Pereira, 2003</marker>
<rawString>Sha, F. and Pereira, F. (2003). Shallow parsing with conditional random fields. In Proceedings of HLT-NAACL 2003, pages 213 – 220.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Sutton</author>
<author>A McCallum</author>
</authors>
<title>Introduction to Statistical Relational Learning, chapter An Introduction to Conditional Random Fields for Relational Learning.</title>
<date>2006</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="22940" citStr="Sutton and McCallum, 2006" startWordPosition="3500" endWordPosition="3503">es résultats obtenus avec les CRF seuls pour chaque tâche et al., 2009). Leur principal inconvénient est qu’ils apparaissent comme des “boîtes noires”. Un modèle issu d’un apprentissage par CRF est simplement une liste de features pondérées pouvant avoir plusieurs millions d’éléments, ce qui le rend difficile à interpréter. Les HMM, qui étaient parmi les meilleures méthodes d’annotation statistique avant que les CRF n’apparaissent, présentent quant à eux l’avantage d’être plus interprétables. Cependant, tout HMM peut être “transformé” en un CRF définissant la même distribution de probabilité (Sutton and McCallum, 2006; Tellier and Tommasi, 2011). Pour ce faire, pour un HMM donné, nous devons définir deux familles de features : • les features de la forme f (yt , xt) associant une seule étiquette yt avec une seule entrée de même position xt : elles valent 1 quand l’états yt du HMM émet xt ; • les features de la forme f (yt−1, yt) qui associent deux états yt−1 et yt du HMM ; elles valent 1 quand la transition entre ces deux états est utilisée. Si θ est une probabilité d’émission ou de transition du HMM, alors on choisit λ = log(θ ) comme poids pour la feature correspondant dans le CRF. Le calcul de p(y |x) s’</context>
</contexts>
<marker>Sutton, McCallum, 2006</marker>
<rawString>Sutton, C. and McCallum, A. (2006). Introduction to Statistical Relational Learning, chapter An Introduction to Conditional Random Fields for Relational Learning. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Tellier</author>
<author>D Duchier</author>
<author>I Eshkol</author>
<author>A Courmet</author>
<author>M Martinet</author>
</authors>
<title>Apprentissage automatique d’un chunker pour le français.</title>
<date>2012</date>
<booktitle>In Actes de TALN’12, papier</booktitle>
<contexts>
<context position="5736" citStr="Tellier et al., 2012" startWordPosition="843" endWordPosition="846"> s’y attendre, les CRF donnent de bien meilleurs résultats que ceux obtenus par IG. Dans la dernière section, nous décrivons et évaluons deux manières de combiner automates et CRF. Les résultats obtenus pour chacune de ces combinaisons sont prometteurs et suggèrent des pistes originales pour associer modèles symboliques et apprentissage statistique. 2 Chunking: la tâche et les données Nous décrivons ici la tâche de chunking par annotation et nous présentons les données d’apprentissage que nous avons utilisées pour nos expériences. Ces dernières reprennent et prolongent celles présentées dans (Tellier et al., 2012). Notre but étant de construire un chunker pour le français, nous sommes partis du French Tree Bank (Abeillé et al., 2003). 2.1 La tâche La tâche de chunking, également appelée analyse syntaxique de surface, a pour but d’identifier les groupes syntaxiques élémentaires des phrases. Les chunks sont en effet des séquences contigües et non-récursives d’unités lexicales liées à une unique tête forte (Abney, 1991). Chacun est caractérisé 20 c ATALA TALN-RÉCITAL 2013, 17-21 Juin, Les Sables d’Olonne par le type (ou étiquette Part-Of-Speech (POS)) de sa tête. Il y a ainsi autant de types de chunks qu</context>
<context position="7974" citStr="Tellier et al., 2012" startWordPosition="1183" endWordPosition="1186">es morpho-syntaxiques définies dans (Crabbé and Candito, 2008), assurant ainsi la continuité avec nos précédents travaux (Constant et al., 2011). Nous considérons 7 types de chunks distincts : AP (Adjectival Phrase), AdP (Adverbial Phrase), CONJ (Conjonctions), NP (Noun Phrase), PP (Prepositional Phrase), VP (verbal Phrase) et UNKONWN (coquilles ou certains mots étrangers, eux-mêmes étiquetés UNKNOWN). Les marques de ponctuations, sauf exceptions (certains guillemets par exemple) sont hors chunks (étiquette O comme Out). Nous avons décidé de modifier certains choix que nous avions faits dans (Tellier et al., 2012). Par exemple, le chunk CONJ contient seulement la conjonction. Le PP, en revanche, intègre toujours le chunk introduit par la préposition. Et, à l’inverse de (Paroubek et al., 2006), les adjectifs épithètes appartiennent toujours au chunk NP contenant le nom qu’ils qualifient, qu’ils soient situés avant ou après lui. Les chunks AP sont donc assez rares car ils ne correspondent qu’aux adjectifs séparés d’un groupe nominal, comme les attributs du sujet ou de l’objet (les fonctions syntaxiques disponibles dans les arbres XML sont nécessaires pour identifier ces derniers). La phrase suivante illu</context>
<context position="20209" citStr="Tellier et al., 2012" startWordPosition="3068" endWordPosition="3071"> peuvent être vues comme un apprentissage “par cœur”, puisqu’ils n’ont donné lieu à aucune généralisation. Les automates de taille 1 correspondent à ceux reconnaissant le langage universel des étiquettes POS présentes au moins une fois dans un chunk NP. Il faut atteindre k = 2 pour obtenir un automate meilleur que le PTA sur des données nettoyées. 4 Apprentissage statistique pour l’annotation Dans cette section, nous nous concentrons sur la meilleure approche statistique actuelle pour une tâche d’annotation : les Conditional Random Fields (CRF), qui se comportent très bien sur notre problème (Tellier et al., 2012). Nous rappelons aussi comment un HMM peut être “transformé” en un CRF, parce que cette transformation sera une source d’inspiration pour une des combinaisons présentées par la suite. 4.1 Conditional Random Fields et HMMs Les CRF, introduits par (Lafferty et al., 2001) sont de la famille des modèles graphiques. Lorsque que le graphe exprimant les dépendances entre étiquettes est linéaire (ce qui est généralement le cas pour étiqueter des séquences), la distribution de probabilité d’une séquence d’annotations y connaissant une séquence observable x est définie par :  p(y |1  Kx) = exp λ Z(x</context>
</contexts>
<marker>Tellier, Duchier, Eshkol, Courmet, Martinet, 2012</marker>
<rawString>Tellier, I., Duchier, D., Eshkol, I., Courmet, A., and Martinet, M. (2012). Apprentissage automatique d’un chunker pour le français. In Actes de TALN’12, papier court (poster).</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Tellier</author>
<author>M Tommasi</author>
</authors>
<title>Champs Markoviens Conditionnels pour l’extraction d’information. In Modèles probabilistes pour l’accès à l’information textuelle.</title>
<date>2011</date>
<publisher>Hermès.</publisher>
<contexts>
<context position="22968" citStr="Tellier and Tommasi, 2011" startWordPosition="3504" endWordPosition="3507">es CRF seuls pour chaque tâche et al., 2009). Leur principal inconvénient est qu’ils apparaissent comme des “boîtes noires”. Un modèle issu d’un apprentissage par CRF est simplement une liste de features pondérées pouvant avoir plusieurs millions d’éléments, ce qui le rend difficile à interpréter. Les HMM, qui étaient parmi les meilleures méthodes d’annotation statistique avant que les CRF n’apparaissent, présentent quant à eux l’avantage d’être plus interprétables. Cependant, tout HMM peut être “transformé” en un CRF définissant la même distribution de probabilité (Sutton and McCallum, 2006; Tellier and Tommasi, 2011). Pour ce faire, pour un HMM donné, nous devons définir deux familles de features : • les features de la forme f (yt , xt) associant une seule étiquette yt avec une seule entrée de même position xt : elles valent 1 quand l’états yt du HMM émet xt ; • les features de la forme f (yt−1, yt) qui associent deux états yt−1 et yt du HMM ; elles valent 1 quand la transition entre ces deux états est utilisée. Si θ est une probabilité d’émission ou de transition du HMM, alors on choisit λ = log(θ ) comme poids pour la feature correspondant dans le CRF. Le calcul de p(y |x) s’écrira alors exactement de l</context>
</contexts>
<marker>Tellier, Tommasi, 2011</marker>
<rawString>Tellier, I. and Tommasi, M. (2011). Champs Markoviens Conditionnels pour l’extraction d’information. In Modèles probabilistes pour l’accès à l’information textuelle. Hermès.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Thollard</author>
<author>P Dupont</author>
<author>de la Higuera</author>
<author>C</author>
</authors>
<title>Probabilistic DFA inference using Kullback-Leibler divergence and minimality.</title>
<date>2000</date>
<booktitle>In Proc. of ICML’2000,</booktitle>
<pages>975--982</pages>
<contexts>
<context position="12943" citStr="Thollard et al., 2000" startWordPosition="1922" endWordPosition="1925">s à la hiérarchie de Chomsky, et apprenables, caractérisées notamment dans (Angluin, 1980). Les classes de langages k-réversibles (Angluin, 1982) entrent dans ce cadre, elles constituent le point de départ de nos expériences. Depuis, bien d’autres classes apprenables par présentation positive ont été décrites et étudiées (Garcia and Vidal, 1990; Denis et al., 2002; Kanazawa, 1998; Koshiba et al., 2000; Yokomori, 2003). Des avancées récentes dans le domaine concernent aussi l’apprenabilité de dispositifs intégrant des probabilités, comme les automates probabilistes et leurs liens avec les HHM (Thollard et al., 2000; Dupont et al., 2005). Parallèlement, des compétitions 3 ont permis de tester l’efficacité des algorithmes proposés lorsqu’ils sont confrontés à des données réelles. 3les plus récents étant Stamina ( ) et Zulu ( ) 22 c ATALA TALN-RÉCITAL 2013, 17-21 Juin, Les Sables d’Olonne 3.2 L’algorithme k-RI Dans cette section, nous décrivons les algorithmes d’IG par présentation positive utilisés dans nos expériences. Ils sont destinés à apprendre un automate pour un type spécifique de chunk, à partir uniquement des différentes séquences de POS aparaissant dans ce type de chunks dans les données d’appr</context>
</contexts>
<marker>Thollard, Dupont, Higuera, C, 2000</marker>
<rawString>Thollard, F., Dupont, P., and de la Higuera, C. (2000). Probabilistic DFA inference using Kullback-Leibler divergence and minimality. In Proc. of ICML’2000, pages 975–982.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Tsuruoka</author>
<author>J Tsujii</author>
<author>S Ananiadou</author>
</authors>
<title>Fast full parsing by linear-chain conditional random fields.</title>
<date>2009</date>
<booktitle>In Proceedings of EACL</booktitle>
<pages>790--798</pages>
<marker>Tsuruoka, Tsujii, Ananiadou, 2009</marker>
<rawString>Tsuruoka, Y., Tsujii, J., and Ananiadou, S. (2009). Fast full parsing by linear-chain conditional random fields. In Proceedings of EACL 2009, pages 790–798.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L G Valiant</author>
</authors>
<title>A theory of the learnable.</title>
<date>1984</date>
<journal>Communications of the ACM,</journal>
<volume>27</volume>
<issue>11</issue>
<contexts>
<context position="11876" citStr="Valiant, 1984" startWordPosition="1769" endWordPosition="1770">ants. Un enfant n’est pas “programmé” pour une langue précise, il est capable d’acquérir n’importe laquelle parlée dans son environnement. De même, un programme d’IG par présentation positive doit être en mesure d’apprendre une classe de langages formels, c’est-à-dire d’identifier n’importe lequel de ses membres à l’aide d’exemples de séquences (phrases) lui appartenant. Les principaux critères possibles qui caractérisent la notion d”’apprenabilité d’une classe de langages” en IG (aussi appelés modèles d’apprentissage) sont “l’identification à la limite” (Gold, 1967) et “l’apprentissage PAC” (Valiant, 1984), que nous ne pouvons détailler ici. Malheureusement, même pour la classe des langages réguliers, la plus simple dans la hiérarchie de Chomsky, ces critères sont impossibles à satisfaire : il n’existe aucun algorithme capable d’apprendre par présentation positive la classe complète des langages réguliers dans ces modèles (Gold, 1967; Kearns and Vazirani, 1994). Les recherches se sont donc orientées vers des classes plus petites, ou transverses à la hiérarchie de Chomsky, et apprenables, caractérisées notamment dans (Angluin, 1980). Les classes de langages k-réversibles (Angluin, 1982) entrent </context>
</contexts>
<marker>Valiant, 1984</marker>
<rawString>Valiant, L. G. (1984). A theory of the learnable. Communications of the ACM, 27(11):1134–1142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Yokomori</author>
</authors>
<title>Polynomial-time identification of very simple grammars from positive data.</title>
<date>2003</date>
<journal>Theoretical Computer Science,</journal>
<volume>1</volume>
<contexts>
<context position="12743" citStr="Yokomori, 2003" startWordPosition="1896" endWordPosition="1897">on positive la classe complète des langages réguliers dans ces modèles (Gold, 1967; Kearns and Vazirani, 1994). Les recherches se sont donc orientées vers des classes plus petites, ou transverses à la hiérarchie de Chomsky, et apprenables, caractérisées notamment dans (Angluin, 1980). Les classes de langages k-réversibles (Angluin, 1982) entrent dans ce cadre, elles constituent le point de départ de nos expériences. Depuis, bien d’autres classes apprenables par présentation positive ont été décrites et étudiées (Garcia and Vidal, 1990; Denis et al., 2002; Kanazawa, 1998; Koshiba et al., 2000; Yokomori, 2003). Des avancées récentes dans le domaine concernent aussi l’apprenabilité de dispositifs intégrant des probabilités, comme les automates probabilistes et leurs liens avec les HHM (Thollard et al., 2000; Dupont et al., 2005). Parallèlement, des compétitions 3 ont permis de tester l’efficacité des algorithmes proposés lorsqu’ils sont confrontés à des données réelles. 3les plus récents étant Stamina ( ) et Zulu ( ) 22 c ATALA TALN-RÉCITAL 2013, 17-21 Juin, Les Sables d’Olonne 3.2 L’algorithme k-RI Dans cette section, nous décrivons les algorithmes d’IG par présentation positive utilisés dans nos </context>
</contexts>
<marker>Yokomori, 2003</marker>
<rawString>Yokomori, T. (2003). Polynomial-time identification of very simple grammars from positive data. Theoretical Computer Science, 1.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>