TALN-RECITAL 2013, 17-21 Iuin, Les Sables d’Olonne

L’utilisation des P_OMQP pour les résumés multi-
documents orlentes par une thematlque

Yllias Chalil, Sadid A. Hasanl, Mustapha Mojahidz
(1) University of Lethbridge, AB, Canada
(2) Université Paul Sabatier — IRIT, 118 Rte de Narbonne 31062 Toulouse Cedex

(1) yllias.chali@uleth.ca (2) mustapha.mojahid@irit.fr

RESUME

L'objectif principal du résumé multi-documents orienté par une thématique est de générer
un résumé a partir de documents sources en réponse a une requéte formulée par
l’utilisateur. Cette tache est difﬁcile car il n'existe pas de méthode efficace pour mesurer la
satisfaction de l’utilisateur. Cela introduit ainsi une incertitude dans le processus de
génération de résumé. Dans cet article, nous proposons une modélisation de l’incertitude en
formulant notre systeme de résumé comme un processus de décision markovien
partiellement observables (POMDP) car dans de nombreux domaines on a montré que les
POMDP permettent de gérer efﬁcacement les incertitudes. Des expériences approfondies
sur les jeux de données du banc d'essai DUC ont démontré l'efﬁcacité de notre approche.

ABSTRACT

Using POMDPs for Topic-Focused Multi-Document Summarization

The main goal of topic-focused multidocument summarization is to generate a summary
from the source documents in response to a given query or particular information
requested by the user. This task is difﬁcult in large part because there is no signiﬁcant way
of measuring whether the user is satisﬁed with the information provided. This introduces
uncertainty in the current state of the summary generation procedure. In this paper, we
model the uncertainty explicitly by formulating our summarization system as a Partially
Observable Markov Decision Process (POMDP) since researchers in many areas have shown
that POMDPs can deal with uncertainty successfully. Extensive experiments on the DUC
benchmark datasets demonstrate the effectiveness of our approach.

MOTS-CLES : Résumé multi-document, résumé orienté requéte, POMDP.
KEYWORDS: Topic-focused multi-document summarization, POMDP.

1 Introduction

Un résumé multi-documents orienté par la thématique (i.e. par une requéte) est utile dans
la gestion des documents et les systemes de recherche. ll peut fournir par exemple des
services d’information personnalisés selon les besoins des utilisateurs. Dans cet article,
nous considérons que le probleme de produire des résumés multi-documents orientés par
la thématique reviendrait a extraire un sous-ensemble de phrases choisies a partir des
documents originaux (Mani et Maybury, 1999). ]uger de l'importance d'une phrase est
l'aspect le plus essentiel de la génération de résumé. Cette tache est difﬁcile, en grande
partie parce qu'il n'est pas certain que la phrase choisie soit suffisamment importante pour
étre considérée comme une phrase du résumé. Il est également difficile de garantir que les
phrases choisies vont satisfaire totalement l’utilisateur. Ce probleme peut étre résolu en
reformulant la tache dans une problématique de prise de décisions séquentielles.

33 © ATALA

TALN-RECITAL 2013, 17-21 Iuin, Les Sables d’Olonne

Les modeles de processus de décision markovien (MDP) se sont avérés utiles dans une
variété de problemes de décision (Puterman, 1994). La spécification d'un probleme de
décisions séquentielles en environnement totalement observable avec un modele de
transition markovien est appelé MDP (Russel et Norvig, 2003). Les MDP sont utiles pour
modéliser la prise de décision dans des situations ou les résultats sont en partie aléatoires
et en partie dépendent du choix d'un décideur. A tout instant, un MDP est dans un certain
état 5, et une action disponible a est ensuite choisie déplacant le MDP aléatoirement dans un
nouvel état 5’; et une récompense correspondante R..{s, 5’) est attribuée. Ainsi, le nouvel état
s’ dépend de l'état actuel 5 et l’action a en étant conditionnellement indépendant de tous les
états et actions antérieurs. La tache principale des MDP est de trouver une fonction de
décision zrqui spéciﬁe une action particuliere qui sera choisie dans un état s ; le but étant de
choisir une politique qui maximise la récompense. Cependant, les MDP ne peuvent traiter
l’incertitude ou se trouvent les états de l’utilisateur, l'historique des transitions, et les
actions. Le modele MDP partiellement observables (POMDP) généralise le modele MDP en
permettant de prendre en compte également des formes d'incertitude. Ainsi, dans différents
domaines, plusieurs applications ont fait appel au POMDP (Young, 2006 ; Lison, 2010 ; Bui
et al., 2007). Dans cet article, nous proposons d’utiliser le POMDP pour modéliser
l’incertitude inhérente a la tache de résumé multi-documents.

Dans notre tache de résumé, l'environnement est partiellement observable, car il est
incertain de savoir si une phrase choisie conduit le systeme a un état de résumé ou non.
Ainsi, nous définissons un modele d'observation 0(s, 0] qui déﬁnit la probabilité de
percevoir l’observation o dans l'état s. Ceci construit un ensemble d'états de croyance qui
est une représentation de probabilités des états réels possibles. Puisque nous ne pouvons
déterminer de maniere sﬁre si nous avons atteint l'état résumé ou non, nous supposons le
fait que l'agent a connaissance de tous les états de croyance et ainsi, la politique optimale
peut étre apprise par la transformation des états de croyance par les actions.

Pour formuler notre tache en termes de POMDP, nous supposons qu’un ensemble de
documents sources et leurs résumés de référence (RR) créés par un expert humain sont
donnés garantissant la satisfaction des utilisateurs. Lorsqu'une phrase est sélectionnée
comme candidate pour un résumé, sa probabilité d'observation est calculée en mesurant sa
parenté avec le RR. Une valeur de récompense est attribuée lorsque le systeme atteint l'état
de résumé. Nous représentons chaque phrase d'un document comme un vecteur de traits-
valeurs (Voir section 4). Notre approche tente de produire des résumés automatiques qui
soient le plus proche des RR. Dans la phase d'apprentissage, le systeme apprend les poids
appropriés des traits en modélisant la relation entre le RR et le résumé candidat extrait. Une
fois que le modele d'observation est appris, l'agent atteint l'état final de résumé et la phase
d’apprentissage se termine. Les poids finaux appris pour chaque attribut sont utilisés pour
produire des résumés a partir de nouvelles données dans la phase de test. Nous avons
utilisé une version de l'algorithme QM) de la descente du gradient de Watkins (Sutton et
Barto, 1998) pour résoudre notre modele POMDP proposé. Des tests ont été menés sur les
jeux de données de référence DUC1 et les résultats des évaluations ont montré l'efficacité de
notre approche. Dans la suite de l'article nous décrirons successivement la terminologie des
POMDP, les bases formelles de notre travail déﬁnissant la tache de résumé multi-documents
orienté par une thématique comme un probleme d'un POMDP, l’espace de traits pour

1 http://duc.nist.gov/
34 © ATALA

TALN-RECITAL 2013, 17-21 Iuin, Les Sables d’Olonne
représenter les phrases, les parametres expérimentaux et les résultats de l'évaluation.

2 Terminologie des POMDP

Contrairement aux MDP qui offrent un bon cadre statistique pour permettre la planification
dans un environnement totalement observable, un POMDP fournit un modele
mathématique pour les problemes de décisions séquentielles dans les environnements
partiellement observables. Le principal avantage d’un POMDP est d'avoir une architecture
complete pour modéliser l’incertitude inhérente au probleme étudié (Young, 2006).

2.1 Définition formelle

Un POMDP est un tuple <S, A, Z, T, 0, R> (Lison, 2010) oil:

1. S est I’espace d'états déﬁni comme un ensemble d'états mutuellement exclusifs.

2. A représente I’espace d’actions possibles qu'un agent peut effectuer dans un état.

3. Z est I’espace des observations que l’agent peut percevoir.

4. T(s, a, 5’) = Pr{s’|s, a) est Ie modéle de transition qui dénote la probabilité d’atteindre l'état
s’ si l'action a est effectuée dans l'état s.

5. 0{s, 0) est le modéle d’observation qui spécifie la probabilité de percevoir l’observation o
dans l'état s.

6. R(s, a) est lafonction de récompense qui définit l'utilité de l’agent a effectuer une action a
étant dans l'état s.

2.2 L'état de croyance

Un POMDP suppose que l'état du monde n'est pas directement observable. Par conséquent,
un POMDP peut seulement déduire des informations utiles a partir d’observations
disponibles. Cette incertitude peut étre codée par l'état de croyance b, qui est une
distribution de probabilité sur tous les états possibles (Lison, 2010). On note b(s) la
probabilité attribuée a l'état réel par l'état de croyance b. Nous pouvons calculer un état de
croyance courant comme une distribution de probabilité conditionnelle sur les états réels
étant données la séquence des observations et des actions effectuées jusqu'ici. Dans l'état de
croyance courant b{s), si l’action est effectuée et une observation 0 est percue, le nouvel état

de croyance sera donné par (Russel et Norvig, 2003) :
b’(s’) = aO(s’,o)2T(s,a,s’)b(s) (1)

oil a est une constante de normalisation qui met la somme des états de croyance a 1. Dans
un POMDP, l’action optimale d’un agent s'appuie uniquement sur l'état de croyance courant
de l’agent puisque l’agent n'est pas conscient de son état actuel. Par conséquent, le cycle de
décisions d’un agent POMDP consiste a exécuter l’action a en tenant compte de l'état actuel
de croyance b, de l’observation 0, et de la transition au nouvel état de croyance (équation 1).
La tache suivante consiste a effectuer une recherche de la politique optimale dans I’espace
continu des états de croyances (Russel et Norvig, 2003). Un état de croyance initial b0
(ayant une distribution uniforme) est spéciﬁée a l'exécution lors de l'initialisation de notre
systeme proposé.

35 © ATALA

TALN-RECITAL 2013, 17-21 Iuin, Les Sables d’Olonne
2.3 Les politiques

La solution au probleme d’un POMDP doit préciser l'action possible de l'agent dans un état
qu'il pourrait atteindre dans un certain intervalle de temps. Ce type de solution est appelé
une politique et noté 11' (Russel et Norvig, 2003). L’idée générale d’un POMDP est que
l'action optimale ne dépend que de l’état de croyance courant de l'agent, car il n'a acces qu'a
l'état réel actuel. Par conséquent, la politique optimale (qui donne la plus grande utilité
espérée), notée 11' *(b], est une application des états de croyance a l'action. Nous définissons
la valeur de l'action a dans l’état de croyance b selon la politique 11' notée Q" (b, a) :

Q"(b,a) = E1r{Rt I 121 =b,at = L1}

={Eyk't+k+1|bt=b,at=a} (2)

Ici, E, représente la valeur espérée étant donné que l'agent suit la politique 11', Rt est le
rendement espéré qui est définie en fonction de la séquence de récompenses, rt+1, rt+2,  ou
rt est la récompense numérique que l'agent recoit a l'instant t. Nous appelons Q" la fonction
action-valeur pour la politique 11'. y représente le facteur d’actualisation qui détermine
l'importance des futures récompenses. Le systeme essaie de trouver la politique optimale
par l'itération de la politique. Une fois la politique optimale 11'* est obtenue, l'agent choisit
les actions en utilisant le principe d'utilité maximale espérée (Russel et Norvig, 2003).

3 Modéle POMDP pour le résumé

3.1 Environnement, état et action

La tache de résumé multi-documents orientée par une thématique considere une requéte q
(une thématique) et une collection de documents voisins D = {d1, d2, ..., d,,} pour générer un
résumé. L’espace d'états qui décrit l’environnement, inclue les états de résumé et non
résumé. L’action optimale de l'agent, ne sachant pas dans quel état il est, dépend de son état
de croyance courant. L’espace d'états des croyances est continu, représenté par une
distribution de probabilités, comprend l'ensemble des états que l'agent pourrait prendre.
Dans le probleme d'extraction de résumé, l'objectif est de sélectionner un ensemble de
phrases importantes de la collection de documents donnée pour constituer le résumé
candidat. Dans chaque état de croyance, on dispose d’un ensemble d’actions possibles qui
pourraient étre opérées sur l’environnement ou chaque action indique la sélection d’une
phrase particuliere (en utilisant la fonction politique de l'équation 2) a partir des phrases
des documents restantes non encore incluses dans le résumé candidat

3.2 Observation et récompense

Lorsque nous choisissons une phrase considérée importante, nous calculons sa probabilité
d'observation en mesurant sa parenté avec les résumés de références fournis. Nous
mesurons cette similarité en utilisant l'outil automatique ROUGE (Recall Oriented
Understudy pour Gisting Evaluation) (Lin, 2004). Les mesures concernent le nombre
d'unités qui se chevauchent tels que les n-grammes, les mot-séquences, et les paires de
mots entre les deux résumés (de référence et extrait). Les mesures de ROUGE considérées

36 © ATALA

TALN-RECITAL 2013, 17-21 Iuin, Les Sables d’Olonne

sont ROUGE-N (N = 1, 2, 3, 4), ROUGE-L, ROUGE-W et ROUGES. Des que la probabilité? la
plus élevée de la croyance est atteinte, on arrive :31 l'état de résumé, et une récompense de 1
est attribuée, sinon 0 est retourné. Le POMDP pour un résumé fonctionne de la maniere
suivante: a chaque instant, l’environnement est dans un certain état non observé s. Cela
signifie qu'une phrase est sélectionnée et déplacée dans le résumé candidat sans savoir si le
systeme a atteint l'état de résumé ou non. Puisque s n'est pas connu « exactement», une
distribution de probabilité (espace des états de croyance) sur tous les états possibles
(résumé ou non résumé) est maintenue ou b(s) indique la probabilité d'étre dans un état s
particulier. Sur la base de l'état actuel de croyance, une action optimale a est choisi et recoit
une récompense r basée sur la probabilité d'observation, et l’environnement se déplace a un
nouvel état non observée s’. L’environnement génere alors une observation 0’ pour
actualiser l'état de croyance. L’ensemble du processus tente de produire un résumé
similaire au résumé de référence en assurant également que le résumé candidat ne contient
pas d’informations redondantes.

3.3 Résolution du POMDP

Dans notre formulation, le nombre d'états réels est égal a deux (état résumé, état non
résumé). Puisque l’environnement est partiellement accessible, nous avons modélisé le
probleme comme un POMDP en introduisant un nombre inﬁni d'états de croyance, qui sont
observables par l'agent. Pour résoudre ce probleme, nous considérons une approche
d'approximation fonctionnelle au probleme. Nous représentons la fonction approximée
action-valeur comme une fonction linéaire paramétrée avec le vecteur de parametres Ht. A
chaque paire état de croyance-action (b, a], il existe un vecteur colqnne de traits,
(fab =((pb(1),<pb(2),...,(pb(n)) avec le méme nombre de composants que Ht. La fonction
approximée action-valeur est donnée par :

Q1 (b,a) = 63¢}, = (i)q9»(i) (3)

Nous considérons notre probleme comme un probligme de prise décision séquentielle a
horizon infini qui trouve un vecteur de parametres 0 pour maximiser Q{b, a) a partir de
l'équation (3). Nous utilisons un algorithme du gradient de la politique pour résoudre notre
probleme de POMDP. Les Algorithmes du gradient de la politique ont tendance a estimer les
parametres 6 en effectuant une montée du gradient stochastique. Le gradient approximé
par l'interaction avec l’environnement, et la récompense qui en résulte est utilisé pour
mettre a jour l'estimation de 0. Les algorithmes du gradient de la politique optimise un
objectif non convexe et ne sont garantis que pour trouver un optimum local (Branavan et
al., 2009). Nous utilisons une version de l'algorithme modiﬁée Q (A) de Watkins avec
descente de gradient linéaire (Sutton et Barto, 1998) en appliquant la politique 5-gloutonne
afin de déterminer la meilleure action possible pour sélectionner les phrases les plus
importantes. Nous nous servons de la politique 5-gloutonne (ce qui signifie que la plupart
du temps cette politique choisit une action avec la valeur maximale estimée, mais une action
est sélectionnée au hasard avec une probabilité (5) pour trouver l’équilibre entre
l'exploration et l'exploitation pendant la phase d'apprentissage. Nous avons posé 5 = 0.1.

2 Dans chaque état, la probabilité d'observation est mise E1 jour lorsque que le sysbéme essaie de générer un
résumé qui est un pas de plus vers le résumé de référence. La probabilité la plus élevée de la croyance est
observée lorsque le résumé du sysbéme a la correspondance la plus proche avec le résumé de référence.

37 © ATALA

TALN-RECITAL 2013, 17-21 Iuin, Les Sables d’Olonne

Ainsi, notre algorithme choisit une action avec la meilleure action-valeur a 90 % et il choisit
une action au hasard a 10 %.

Les étapes de notre solution POMDP sont présentées dans l'algorithme 1 oil cp est un
vecteur de atraits-valeurs (Voir section 4) utilisé pour représenter chaque phrase de
document ; Ht est le vecteur de poids pour le vecteur de traits que le systeme va apprendre
et y est le coefficient d'actualisation utilisé pour calculer la récompense d’une paire état-
action. Le facteur d'actualisation détermine l'importance des futures récompenses. Nous
avons gardé la valeur initiale de y a 0.1 qu'on fait diminuer d'un facteur correspondant au
nombre d'itération. Tant que la politique initiale sélectionne les actions gloutonnes,
l'algorithme continue l’apprentissage de la fonction action-valeur avec une la politique
gloutonne. Cependant, quand une action exploratoire est sélectionnée par la politique de
comportement, les traces d'éligibilité3, E, sont initialisées pour tous les couples état-action
et elles sont mises a jour en deux étapes. Dans la premiere étape, si une action exploratoire
est prise, elles sont mises a 0 pour toutes les paires état-action. Dans le cas contraire, les
traces d'éligibilité pour toutes les paires état-action sont décomposées par yl. Dans la
deuxieme étape, la valeur de la trace d'éligibilité de la paire actuelle état-action est
incrémentée de 1 tout en accumulant les traces. La version originale de l'algorithme Q(7L] de
Watkins utilise une approximation linéaire de la fonction de la descente de gradient avec
des traits binaires. Toutefois, étant donné que nous traitons un mélange de traits a valeur
réelle et booléenne (voir section 4), nous avons modifié l'algorithme pour déclencher une
mise a jour différente pour les traces d'éligibilité. Dans la deuxieme étape de la mise a jour
des traces d'éligibilité, on incrémente la valeur avec le score du trait correspondant. L'ajout
d’une étape de saut aléatoire évite les maximums locaux dans notre algorithme. Le
parametre A définit quel crédit nous pouvons accorder aux états antérieurs. or est le
parametre de la taille de pas pour le procédé de la descente de gradient qui est réduit par
un facteur de 0,99 de sorte que l’apprentissage converge vers le but. 6 est le taux d'erreur de
prédiction état-valeur. Un «épisode» est lancé a la phase d’apprentissage pour chaque
theme et se termine quand l'état de résumé ﬁnal est atteint. La condition «b n'est pas
terminale » est satisfaite lorsque b se réfere a l'état ﬁnal de résumé. L'état de croyance
initial a une distribution de probabilité uniforme sur les états réels. L’action initiale est
choisie en fonction de la probabilité d'observation des phrases. A notre connaissance, la
formulation proposée par la version modiﬁée de l'algorithme Q(7L] de Watkins est originale
dans la facon dont il représente la tache de résumé orientée sur une thématique.

4 Espace des traits

Nous représentons chaque phrase d'un document comme un vecteur de trait-valeur (zp dans
l'algorithme 1). Notre ensemble de traits comprend deux types, le premier caractérise
l'importance d’une phrase dans un document et le second mesure la similarité entre chaque
phrase et la requéte de l'utilisateur. Ces traits ont été adoptés par plusieurs travaux
connexes a cette problématique (Edmundson, 1969 ; Litvak et al., 2010 ; Schilder et
Kondadadi, 2008).

3 Une trace d'éligibilité est un enregistrement bemporaire de 1'occurrence d'un événement, comme la visibe d'un
état ou le choix d’une action [Sutton et Barbo, 1998).

38 © ATALA

TALN-RECITAL 2013, 17-21 Iuin, Les Sables d’Olonne
4.1 Mesure de l’importance d’une phrase

Position des phrases : Les phrases qui se trouvent au debut et a la fin d’un document ont
souvent tendance a inclure les informations les plus precieuses. Nous avons analyse
manuellement la collection de documents donnes et nous avons constate que la premiere et
les 3 dernieres phrases d’un document repondent bien a ce trait. Nous leurs attribuons ainsi
le score de 1 et 0 aux autres phrases.

Longueur des phrases : Les plus longues phrases contiennent plus de mots et ont une plus
grande probabilite de contenir des informations importantes. Par consequent, une longue
phrase a de meilleure chance de figurer dans un resume. Nous donnons le score de 1 a une
longue phrase et 0 aux autres. L’analyse manuelle de la collection de documents nous a
amene a ﬁxer le seuil de 11 mots pour considerer qu’une phrase est longue.

Correspondance avec Ie titre: Si nous trouvons des chevauchements de mots exacts, de
synonymes, ou d’hyponymes entre le titre et une phrase, nous lui attribuons le score de 1,
sinon 0. Nous utilisons la base de donnees WordNet4 (Fellbaum, 1998) pour l’acces aux
synonymes et aux hyponymes.

Algorithme 1 Q0») de Watkins modifie :

Entrés: oc,§,A,y,cp,s, nombre d’iterations T
Sorties : Vecteur 5 des poids appris

Initialisation: (5 :31 6, E :31 5,0: :31 0.01,)» :31 0.9,}! :31 0.1
b, a 6 etat de croyance et action de l’episode initiaux
(p 6 ensemble des traits presents dans s, a

Pour i = 1 .. TFaire

Si b est non terminal Alors

Pour i Eq) Faire

Le(i) 6 e(i)+ ¢p(i)

faire l’action a, observer la recompense et l’etat suivant b
5 61‘— Ei<P(i)0(i)

Pour a EA(b] Faire

(p 6 ensemble de traits presents dans b, a
Q, 6 Ei<P(i)9(i)

6 6 6 + 1/max, Q,

9 6 0 + (265

6 6 0.99 x on

Si probabilite s 1- s Alors

Pour 11 E A(b) Faire
LQ, 6 21' W i)0( i)
a 6 aagmaxa Q“
e 6 y e
sinon a 6 action aléatoire E A(b)
| E e 6

retourner 9

4 Nous utilisons dans cette recherche la version 3.0 de WordNet.

39 © ATALA

TALN-RECITAL 2013, 17-21 Iuin, Les Sables d’Olonne

Entités nommées (EN) : Le score de 1 est attribué a une phrase appartenant a une classe
d’EN parmi : Personne, Localisation, Organisation, Entité géopolitique, Installation, Date,
Monnaie, Nombre, Horaire. Nous pensons qu'une EN accroit l'importance d’une phrase et
nous utilisons le systeme OAK (Sekine, 2002) pour la reconnaissance d'entités nommées.

Lexique spéciﬁque : La pertinence probable d’une phrase est affectée par la présence de
marqueurs lexicaux (« important», «impossible», «en conclusion», «enﬁn », etc.). Nous
utilisons une liste5 de 228 expressions et nous donnons le score de 1 a une phrase
contenant une expression dans cette liste et 0 sinon.

4-.2 Mesure de similarité avec la requéte de l'utilisateur

chevauchement de n-gramme : Il s’agit du rappel entre la requéte et la phrase candidate
dans laquelle n représente la longueur du n-gramme (n = 1, 2, 3, 4). Cette valeur est obtenue
en divisant le total des co-occurrences de n-grammes dans la requéte et dans la phrase
candidate, par le nombre de n-grammes dans la phrase (Lin, 2004).

LSC : Etant donné deux séquences S1 et S2, la plus longue séquence commune (LSC) de S1 et
S2 est une sous-séquence commune avec la longueur maximale.

LSCP : La plus Longue Séquence Commune Pondérée (LSCP) améliore la méthode de base
LSC en prenant en compte la longueur des correspondances consécutifs rencontrés (Lin,
2004). La LSCP permet de conserver la durée des correspondances consécutives dans une
table a deux dimensions de programmation dynamique. Le calcul de LCSP est basé sur la F-
mesure entre une requéte et une phrase.

Saut-bigramme : Ce trait mesure le chevauchement de bigrammes entre une phrase
candidate et une phrase requéte. Le Saut-bigramme compte scrupuleusement toutes les
paires de mots en correspondance, alors que LSC s’intéresse seulement a la plus longue
séquence commune.

chevauchement de mots exacts : Il s’agit de calculer le nombre de mots équivalents dans la
phrase candidate et la phrase de la requéte.

chevauchement de synonymes : C’est le chevauchement entre la liste des synonymes des
mots lexicaux5 extraite de la phrase candidate et les mots de la requéte associée. Les mots
liés a la requéte sont obtenus en les remplacant par leur sens premier synonyme utilisant
WordNet (Fellbaum, 1998). Nous considérons que les synsets de chaque mot.

chevauchement des hyperonymes et des hyponymes : C’est le chevauchement entre la
liste des hyperonymes et hyponymes (jusqu'au niveau 2 dans WordNet) des noms extraits
de la phrase et les mots de la requéte associée.

chevauchement du glossaire : Notre systeme extrait les glossaires pour les noms propres
de WordNet. Le chevauchement des glossaires est le chevauchement entre la liste des mots
lexicaux qui sont extraits de la déﬁnition du glossaire des noms dans la phrase candidate, et
les mots de la requéte associée.

5 Nous avons construit notre lexique en se référant aux expressions de transition disponibles dans
http://www.smart-words.org/transitionwords.html
5 Les mots lexicaux sont les noms, les verbes, les adverbes et les adjectifs.

40 © ATALA

TALN-RECITAL 2013, 17-21 Iuin, Les Sables d’Olonne

Trait syntaxique : La premiere étape pour calculer la similarité syntaxique entre la requéte
et la phrase consiste a analyser leurs arbres syntaxiques en utilisant un analyseur
syntaxique ; nous utilisons celui de (Charniak, 1999)7. La similarité entre deux arbres
syntaxiques est calculée en utilisant lafonction noyau sur Ies arbres (Collins et Duffy, 2001).

chevauchement des Eléments de Base (EB) : Nous extrayons les EB des phrases dans la
collection de documents en utilisant le package des EB d’ISI3. Nous filtrons ensuite les EB en
vériﬁant s'ils contiennent un mot de la requéte ou un mot connexe. Nous obtenons ainsi le
meilleur score de chevauchement des EB (Hovy et al., 2005).

5 Expérimentation et évaluation

5.1 Définition de la téiche et corpus

Cet article s'intéresse a la tache de résumé multi-documents orientée par une thématique
(i.e. orientée par une requéte) comme cela est définit dans la Document Understanding
Conference, DUC-2007 : «Etant donnée une question complexe {description du sujet) et une
collection de documents pertinents, Ia tdche consiste d produire un résumé bien construit
n’excédant pas 250 mots». L’ensemble des documents-9 DUC-2006 et 2007 provenaient du
corpus AQUAINT, qui est composé d'articles provenant de fils de presse de l’Associated
Press et New York Times (1998 - 2000) et de la Xinhua News Agency (1996-2000). Nous
utilisons les groupes de documents (contenant chacune 25 articles de presse) de DUC-2006
pour apprendre les poids respectifs de chaque trait considéré qui seront exploités pour
produire des résumés pour les groupes de documents de DUC-2 007.

5.2 Paramétres du systéme

Le jeu de données produit un nombre de phrases volumineux. Pour simpliﬁer notre tache
nous filtrons les 100 premieres phrases d’un groupe de documents donnés en employant
une approche supervisée basée sur l’entropie maximale (MaxEnt). Nous choisissons MaxEnt
car il a montré de bonnes performances en résumé automatique de texte (Ferrier, 2001).
Les systemes supervisés nécessitent une grande quantité de données pendant
l’apprentissage. Nous appliquons les noyaux de sous-séquences de chaine étendue
(Extended String Subsequence Kernel - ESSK) pour étiqueter automatiquement toutes les
phrases de DUC-2006 et produire sufﬁsamment de données pour l’apprentissage. L'ESSK a
été appliqué avec succes pour l'annotation automatique (Hirao et al., 2004) ; nous
l'exploitons dans le calcul de similarité entre chaque phrase candidate et le résumé de
référence. Les N premieres phrases sont ensuite choisies en fonction du score donné par
ESSK et étiquetées « +1 » (phrase résumé), et les autres « -1 » (phrase non résumé). Nous
construisons le systeme MaxEnt utilisant le package 10 MaxEnt de Lin. Pour définir
l'exponentiel avant les valeurs A dans les modeles MaxEnt, un parametre supplémentaire
est utilisé dans le package lors de l’apprentissage. Nous conservons la valeur a comme
valeur par défaut. Le modele MaxEnt appris est utilisé pour prédire les étiquettes des

7 Disponible sur ftp ://ftp.cs.brown.edu/pub/nlparser/

9 http://www.isi.edu/"cyl/BE

9 DUC-2006 et DUC-2007 ont fourni respectivement 50 et 45 groupes de documents.
1° http ://www.cs.ualberta.ca/"lindek/downloads.htm

41 © ATALA

TALN-RECITAL 2013, 17-21 Iuin, Les Sables d’Olonne

phrases non observées des données DUC-2007. Les valeurs de probabilité correspondant
aux étiquettes prédites sont utilisées pour classer les phrases.

Pour résoudre notre modele POMDP, nous appliquons l'algorithme QM) modiﬁé de Watkins
sur les données de DUC-2006 et nous utilisons les résultats des poids finaux pour obtenir
les 250 mots du résumé a partir des 100 premieres phrases (déja sélectionnées par le
systeme MaxEnt de base) des goupes de documents de DUC-2007. Nous avons également
généré des résumés de 250 mots en utilisant le systeme MaxEnt seul. Pour comparer les
performances du systeme POMDP, nous avons construit trois autres systemes supervisés en
utilisant les techniques bien connues : Machines :31 vecteurs de support, Modeles de Markov
cachés et Champs conditionnels aléatoires (respectivement SVM, HMM, CRF).

Nous utilisons les mémes (ESSK étiquetés) données dans la phase d’apprentissage de ces
systemes. Nous utilisons également un modele de résumé non supervisé pour évaluer le
modele POMDP; les approches supervisées sont souvent plus difficiles a appliquer a des
ensembles de données arbitraires ou des modeles de résumés générés par l’humain ne sont
pas disponibles. L'algorithme de regroupement K-means est utilisé pour construire le
systeme non supervisé. Dans tous ces systemes, le méme ensemble de traits (Section 4) est
utilisé pour représenter les phrases du document comme vecteurs de traits.

5.3 Evaluation automatique : ROUGE

En DUC-2007, chaque theme et son groupe de documents ont été remis aux 4 différents
évaluateurs du NIST. L’évaluateur a créé ensuite un résumé de 250 mots qui répond au
besoin d’informations exprimées dans le sujet. Ces multiples « résumés de référence » sont
utilisés dans l’évaluation de notre contenu de résumé. Nous avons considéré les mesures
d'évaluation largement pratiquées, la précision (P), le rappel (R) et la F-mesure pour notre

tache d’évaluation des résumés générés par le systeme en utilisant la boite a outils
d'évaluation automatique ROUGE (Lin, 2004), qui a été adoptée par DUC.

5.3.1 Comparaison avec différents systémes

Les tables 1 et 2 donnent les scores ROUGE-2 et ROUGE-SU de tous les systemes, et la
table 3 indique les intervalles de confiance a 95 % de tous les systemes aﬁn de pouvoir faire
des comparaisons signiﬁcatives. Ces résultats montrent que le systeme POMDP est le plus
performant, la plupart du temps, prouvant ainsi l'efficacité de notre modele POMDP, a
l'exception d’un petit nombre cas11. La raison de cette performance est due au fait que dans
le cas des approches supervisées et non supervisées, le comportement appris est basé sur
un corpus d’apprentissage ﬁxe alors que le modele POMDP met a jour en permanence son
état de croyance en se basant sur la probabilité d'observation et utilise la récompense
obtenue pour réajuster les poids des traits dans chaque itération aﬁn de fournir un
environnement d’apprentissage plus efficace. En particulier, l'avantage de l’approche
POMDP est obtenu en prenant des décisions séquentielles, la ou les autres systemes vont
traiter indépendamment chaque phrase (SVM), ou considérer seulement les dépendances
entre les phrases directement adjacentes (CRF, HMM).

11 Les chevauchements des intervalles de conﬁance ROUGE-SU pour SVM, MaxEnt et POMDP indiquent qu'ils ne
sont pas significativement différents les uns des autres.

42 © ATALA

TALN-RECITAL 2013, 17-21 Iuin, Les Sables d’Olonne

Systemes Rappel Précision F-mesure
SVM 0.0801 0.0878 0.0838
HMM 0.0865 0.0951 0.0906
CRF 0.0767 0.0844 0.0804
MaxEnt 0.0815 0.0897 0.0854
K-mens 0.0779 0.1072 0.0902
POMDP 0.1286 0.1065 0.1164

Table 1 : Les mesures ROUGE-2

Systemes Rappel

Précision F-mesure

SVM 0.1324 0.1592 0.1445
HMM 0.1349 0.1636 0.1478
CRF 0.1238 0.1487 0.1461
MaXEnt 0.1339 0.1611 0.1461
K-mens 0.1348 0.1742 0.1520
POMDP 0.2089 0.1432 0.1694

Table 2 : Les mesures ROUGE-SU

Systemes

ROUGE-2

ROUGE-SU

SVM

0.0678 — 0.1027

0.1243 — 0.1703

HMM

0.0838 — 0.0967

0.1306 — 0.1593

CRF

0.0629 — 0.0964

0.1161 — 0.1546

MaxEnt

0.0681 — 0.1036

0.1267 — 0.1636

K-mens

0.0662 — 0.0953

0.1241 — 0.1594

POMDP

0.1078 —0.1263

0.1635 — 0.1762

Table 3 : Les intervalles de conﬁance a 95 % pour les différents systemes

5.3.2 Comparaison avec des systémes de base

Dans la table 4, notre modele POMDP proposé est comparé aux systemes de base (SB:
officiels de DUC-2 007. SB-1 retourne toutes les meilleures phrases (a hauteur de 250 mots:
dans le champ <TEXT> du plus récent document L’idée principale de SB-2 est d’ignorer lc
theme narratif tout en générant automatiquement des résumés basés sur une formulatior
HMM12. Selon ces résultats, le modele POMDP proposé dépasse tres largement tous les
systemes de base. La table 5 indique les intervalles de confiance a 95% des F-mesure:
ROUGE permettant de vériﬁer la significativité des résultats.

Systemes ROUGE-2 ROUGE-SU
SB-1 0.060039 0.10507
SB-2 0.09382 0.14641

POMDP 0.1164 0.1694

Table 4 : Comparaison avec
les systemes de base (F-scores)

12 http: / / duc.nist.gov/ pubs / 2 004p ap ers/ida.conroy.ps

43

© ATALA

TALN-RECITAL 2013, 17-21 Iuin, Les Sables d’Olonne

Systemes ROUGE-2 ROUGE-SU
SB-1 0.0563 — 0.0643 0.1007 — 0.1091
SB-2 0.0892 — 0.0980 0.1422 — 0.1506

POMDP 0.1078 — 0.1263 0.1635 — 0.1762

Table 5 : Intervalles de confiance a 95 %

5.3.3 Comparison avec l’état de l’art

Dans la table 6, notre systeme POMDP est comparé aux systemes qui ont participé au DUC‘
2007. La moyenne-DUC représente la moyenne des scores ROUGE de tous les systeme:
participant a DUC-2007. Les scores du meilleur systeme au DUC-2007 (Pingali et al., 2007:
sont également indiqués dans la table. Nous constatons que notre systeme a atteint des
scores plus élevés que les scores moyens ROUGE de tous les systemes participants a DUC‘
2007 tout en rivalisant de tres pres avec le meilleur systeme. La table 7 donne les intervalle:
de confiance a 95% des F-mesures ROUGE.

Systemes ROUGE-1 ROUGE-2
Moyenne-DUC 0.4006 0.0955

Meﬂ.le“r 0.4388 0.1228
systeme
POMDP 0.4370 0.1164

Table 6 : Comparaison avec
les systemes de l'état de l’art (F-scores)

Systemes ROUGE-1 ROUGE-2
Meilleur 0.4316 — 0.1180 —
systeme 0.4459 0.1276
0.4273 — 0.1078 —
POMDP 0.4486 0.1263

Table 7 : Intervalles de conﬁance a 95%

Systemes Qualité ling Rg:::::S
SB-1 4.24 1.86
SB-2 4.48 2.71

Meilleur 4.11 3.40
SVM 3.30 3.50
HMM 3.20 3.10
CRF 3.00 2.70

MaxEnt 3.20 2.90

K-mens 3.60 3.40

POMDP 4.00 3.80

Table 8 : Scores moyens de la qualité linguistique et des réponses

44 © ATALA

TALN-RECITAL 2013, 17-21 Iuin, Les Sables d’Olonne

5.4- Evaluation Manuelle

Nous avons mené une évaluation manuelle intensive aﬁn d'analyser l'efﬁcacité de notre
approche, en demandant :31 deux universitaires dipl6més de langue maternelle anglaise de
juger13 la qualité linguistique des résumés et des réponses globales conformément aux
directives d’évaluation14 DUC-2007. La table 8 présente la qualité moyenne linguistique et
les scores des réponses globales de tous les systemes. Nous pouvons constater que le
systeme POMDP dépasse de maniere signiﬁcative15 tous les systemes dans la plupart des
cas, tout en se rapprochant de tres pres des systémes de base et du meilleur systéme en
termes de qualité linguistique. Ce résultat s'explique par le fait que notre modele POMDP ne
considere aucun algorithme de post-traitement ou d'ordonnancement de phrases pour
améliorer les résumés générés par le systeme. Cependant, en termes de réponse générale
du contenu, notre systeme POMDP a emporté le meilleur score en apportant une meilleure
précision pour répondre au besoin d'information demandé par l'utilisateur.

6 Conclusion

La contribution principale de cet article est une formulation en termes POMDP du probleme
du résumé multi-documents orienté par une thématique. Comme il n'est pas certain de
savoir si un résumé centré sur un theme répond au besoin d'information de l'utilisateur ou
non, nous avons proposé que cette incertitude soit modélisée en considérant la téche de
résumé comme étant un probleme POMDP. Nous avons comparé le systéme POMDP
proposé avec quatre méthodes d’apprentissage supervisées bien connues : MaxEnt, CRF,
SVM et HMM. Nous avons également évalué notre approche sur un modele de clustering K-
Means non supervisé. Notre systeme POMDP a dépassé les deux systemes de bases officiels
et les scores moyens de DUC tout en s’approchant des performances du meilleur systéme de
DUC-2007. Les intervalles de conﬁance :31 95% ont montré qu'il n'existe pas de différence
significative entre notre systeme et ce lauréat. Nous avons aussi procédé :31 une évaluation
approfondie manuelle des résumés générés par le systeme. Les résultats montrent
l'efﬁcacité de notre systeme POMDP proposée dans la modélisation de l’incertitude de la
tﬁche de résumé multi-documents orientée par une thématique.

Parmi les perspectives :31 ce travail, nous envisageons étudier l'apport des indices fournis
par la mise en forme matérielle et plus particulierement dans les citations et les structures
énumératives.

Remerciements

Les recherches présentées dans cet article ont été prises en charge par le Natural Sciences
and Engineering Research Council (NSERC) du Canada — Discovery Grant and l'Université
de Lethbridge.

13 L'accord inter-annotabeur de Cohen est calculé pour les deux juges [K = 0.61), ce qui traduit un degré important
d'accord entre eux [Landis et Koch, 1977).
14 http://www-nlpir.nistgov/projects/duc/duc2007/qualityquestionslxt
15 Les différences sont statistiquement signiﬁcatives 51 p <.05 en utilisant le best du t de Student.

45 © ATALA

TALN-RECITAL 2013, 17-21 Iuin, Les Sables d’Olonne
Références

S.R.K. Branavan, H. Chen, L. S. Zettlemoyer, and R. Barzilay. 2009. Reinforcement Learning for
Mapping Instructions to Actions. In Proceedings of the Joint conference of the 47th Annual
Meeting of the Association for Computational Linguistics (ACLIJCNLP 2009), pages 329-332,
Suntec, Singapore.

T. H. Bui, B. van Schooten, and D. Hofs. 2007. Practical Dialogue Manager Development using
POMDPs. In Proceedings of the 8th SIGdial Workshop on Discourse and Dialogue, Antwerp,
Belgium, pages 215- 218.

Y. Chali, S. A. Hasan, K. Imam: A reinforcement learning framework for answering complex
questions. I U] 2011, pages 307-310

Y. Chali, S. A. Hasan, K. Imam: Improving the performance of the reinforcement learning model
for answering complex questions. CIKM 2012, pages 2499-2502

E. Charniak. 1999. A Maximum-Entropy-Inspired Parser. In Technical Report CS-99-12, Brown
University, Computer Science Department.

M. Collins and N. Duffy. 2001. Convolution Kernels for Natural Language. In Proceedings of
Neural Information Processing Systems, pages 625-632, Vancouver, Canada.

H. P. Edmundson. 1969. New Methods in Automatic Extracting. Journal of the Association for
Computing Machinery (ACA/I), 16(2):264—285. C. Fellbaum. 1998. WordNet - An Electronic
Lexical Database. Cambridge, MA. MIT Press.

L. Ferrier, 2001. A Maximum Entropy Approach to Text Summarization. M. Sc. thesis, School of
Artiﬁcial Intelligence, Division of Informatics, University of Edinburgh

T. Hirao, I. Suzuki, H. Isozaki, and E. Maeda. 2004. Dependency-based Sentence Alignment for
Multiple Document Summarization In Proceedings of the 20th International Conference on
Computational Linguistics, pages 446-452, Geneva, Switzerland.

E. Hovy, C. Y. Lin, and L. Zhou. 2005. A BE-based Multi-document Summarizer with Query
Interpretation In Proceedings of the Document Understanding Conference, Canada.

T. Joachims. 1999. Making large-Scale SVM Learning Practical. In Advances in Kernel Methods
- Support Vector Learning.

J. R. Landis and G. G. Koch 1977. The Measurement of Observer Agreement for Categorical
Data. Biometrics, 33(1):159—174.

C. Lin 2004. ROUGE: A Package for Automatic Evaluation of Summaries. In Proceedings of
Workshop on Text Summarization Branches Out, Post-Conference Workshop of Association for
Computational Linguistics, pages 74-81, Barcelona, Spain

P. Lison 2010. Towards relational POMDPs for adaptive dialogue management. In Proceedings
of the ACL 2010 Student Research Workshop, pages 7-12.

M. Litvak, M. Last, and M. Friedman 2010. A New Approach to Improving Multilingual
Summarization using a Genetic Algorithm. In Proceedings of the 48th Annual Meeting of the
Association for Computational Linguistics, pages 927-936. ACL.

I. Mani and M. T. Maybury, 1999. Advances in Automatic Text Summarization. MIT Press. A. K.
McCallum. 2002. MALLET: A Machine Learning for Language Toolkit.

46 © ATALA

TALN-RECITAL 2013, 17-21 Iuin, Les Sables d’Olonne

D. Pelleg and A. Moore. 1999. Accelerating exact k-means algorithms with geometric reasoning.
In Proceedings of the fiﬁh ACM SIGKDD international conference on Knowledge discovery and
data mining, pages 277-281. ACM.

P. Pingali, R. K, and V. Varma. 2007. IIIT Hyderabad at DUC 2007. In Proceedings of the
Document Understanding Conference, Rochester, USA. NIST.

M. L. Puterman 1994. Markov Decision Processes— Discrete Stochastic Dynamic Programming.
John Wiley & Sons, Inc, New York.

S. Russel and P. Norvig, 2003. Artificial IntelligenceA Modern Approach, 2nd Edition. Prentice
Hall.

S. Ryang and T. Abekawa: Framework of Automatic Text Summarization Using Reinforcement
Learning. Empirical Methods. EMNLP-CoNLL 2012, pages 256-265, Jeju Island, Korea.

F. Schilder and R. Kondadadi. 2008. FastSum: Fast and Accurate Query-based Multi-document
Summarization In Proceedings of the 46th Annual Meeting of the Association for Comput-
ational Linguistics on Human Language Technologies, pages 205-208. ACL.

S. Sekine. 2002. Proteus Project OAK System, http://nlp.nyu.edu/oak.

R. S. Sutton and A. G. Barto. 1998. Reinforcement Learning: An Introduction. The MIT Press,
Cambridge, Massachusetts, London, England.

S. Young. 2006. Using POMDPs for dialog management. In Proceedings of the 1st IEEE/ACL
Workshop on Spoken Language Technologies (SLT 06).

47 © ATALA

