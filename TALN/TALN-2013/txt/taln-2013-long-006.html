<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
"http://www.w3.org/TR/html4/loose.dtd">
<html><head><title>WoNeF : am&#233;lioration, extension et &#233;valuation d&#8217;une traduction fran&#231;aise automatique de WordNet</title>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
</head>
<body>
<div style="page-break-before:always; page-break-after:always"><div><p>WoNeF : am&#233;lioration, extension et &#233;valuation
d&#8217;une traduction fran&#231;aise automatique de WordNet
</p>
<p>Quentin Pradet1 Jeanne Baguenier-Desormeaux1
Ga&#235;l de Chalendar1 Laurence Danlos2
</p>
<p>(1) CEA, LIST, Laboratoire Vision et Ing&#233;nierie des Contenus,
Gif-sur-Yvette, F-91191, France;
</p>
<p>(2) Univ Paris Diderot, Sorbonne Paris Cit&#233;, ALPAGE, UMR-I 001 INRIA
{quentin.pradet,gael.de-chalendar}@cea.fr
</p>
<p>laurence.danlos@linguist.univ-paris-diderot.fr
</p>
<p>R&#201;SUM&#201;
Identifier les sens possibles des mots du vocabulaire est un probl&#232;me difficile demandant un
travail manuel tr&#232;s cons&#233;quent. Ce travail a &#233;t&#233; entrepris pour l&#8217;anglais : le r&#233;sultat est la base de
donn&#233;es lexicale WordNet, pour laquelle il n&#8217;existe encore que peu d&#8217;&#233;quivalents dans d&#8217;autres
langues. N&#233;anmoins, des traductions automatiques de WordNet vers de nombreuses langues
cibles existent, notamment pour le fran&#231;ais. JAWS est une telle traduction automatique utilisant
des dictionnaires et un mod&#232;le de langage syntaxique. Nous am&#233;liorons cette traduction, la
compl&#233;tons avec les verbes et adjectifs de WordNet, et d&#233;montrons la validit&#233; de notre approche
via une nouvelle &#233;valuation manuelle. En plus de la version principale nomm&#233;e WoNeF, nous
produisons deux versions suppl&#233;mentaires : une version &#224; haute pr&#233;cision (93% de pr&#233;cision,
jusqu&#8217;&#224; 97% pour les noms), et une version &#224; haute couverture contenant 109 447 paires (litt&#233;ral,
synset).
</p>
<p>ABSTRACT
WoNeF, an improved, extended and evaluated automatic French translation of WordNet
</p>
<p>Identifying the various possible meanings of each word of the vocabulary is a difficult problem
that requires a lot of manual work. It has been tackled by the WordNet lexical semantics database
in English, but there are still few resources available for other languages. Automatic translations
of WordNet have been tried to many target languages such as French. JAWS is such an automatic
translation of WordNet nouns to French using bilingual dictionaries and a syntactic langage
model. We improve the existing translation precision and coverage, complete it with translations
of verbs and adjectives and enhance its evaluation method, demonstrating the validity of the
approach. In addition to the main result called WoNeF, we produce two additional versions : a
high-precision version with 93% precision (up to 97% on nouns) and a high-coverage version
with 109,447 (literal, synset) pairs.
</p>
<p>MOTS-CL&#201;S : WordNet, d&#233;sambigu&#239;sation lexicale, traduction, ressource.
KEYWORDS: WordNet, Word Sense Disambiguation, translation, resource.
</p>
<p>TALN-R&#201;CITAL 2013, 17-21 Juin, Les Sables d&#8217;Olonne
</p>
<p>76 c&#65535; ATALA</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>1 Introduction
</p>
<p>WordNet est une base de donn&#233;es lexicale en d&#233;veloppement depuis les ann&#233;es 80 (Fellbaum,
1998). Cette base est organis&#233;e autour du concept de synset (ensemble de synonymes), chaque
synset repr&#233;sentant un sens tr&#232;s pr&#233;cis &#224; l&#8217;aide d&#8217;une d&#233;finition et d&#8217;un certain nombre de mots
que nous nommons litt&#233;raux. Ces synsets sont li&#233;s par diff&#233;rentes relations s&#233;mantiques telles
que la m&#233;ronymie et l&#8217;hyponymie. Malgr&#233; des d&#233;fauts reconnus (Boyd-Graber et al., 2006)
principalement li&#233;s &#224; la granularit&#233; trop fine des sens, WordNet reste une ressource extr&#234;mement
utile et reproduire ce travail pour d&#8217;autres langues serait co&#251;teux et difficile &#224; maintenir. Et
malgr&#233; quelques probl&#232;mes th&#233;oriques, (Fellbaum et Vossen, 2007; de Melo et Weikum, 2008)
montrent que traduire WordNet en gardant sa structure et ses synsets m&#232;ne &#224; des ressources
linguistiques utiles.
</p>
<p>Les traductions automatiques de WordNet emploient une approche dite d&#8217;extension (extend
approach) : la structure de WordNet est pr&#233;serv&#233;e et seuls les litt&#233;raux sont traduits. Trois
techniques principales repr&#233;sentent cette approche dans la litt&#233;rature. La plus simple utilise des
dictionnaires bilingues pour faciliter le travail des lexicographes qui filtrent ensuite manuellement
les entr&#233;es propos&#233;es (Vossen, 1998; Pianta et al., 2002; Tufis et al., 2004). Une deuxi&#232;me
m&#233;thode de traduction utilise des corpus parall&#232;les, ce qui &#233;vite l&#8217;utilisation de dictionnaires
qui peuvent entra&#238;ner un biais lexicographique. (Dyvik, 2004) repr&#233;sente cette m&#233;thode en
s&#8217;appuyant sur des back-translations entre le norv&#233;gien et l&#8217;anglais, alors que (Sagot et Fi&#353;er,
2008) combinent un lexique multilingue et les diff&#233;rents WordNets de BalkaNet comme autant de
sources aidant &#224; la d&#233;sambigu&#239;sation. Enfin, plus r&#233;cemment, des ressources telles que Wikip&#233;dia
ou le Wiktionnaire ont &#233;t&#233; explor&#233;es. Gr&#226;ce aux nombreux liens entre les diff&#233;rentes langues de
ces ressources, il est possible de cr&#233;er de nouveaux wordnets (de Melo et Weikum, 2009; Navigli
et Ponzetto, 2010) ou d&#8217;am&#233;liorer des wordnets existants (Hanoka et Sagot, 2012).
</p>
<p>Concernant le fran&#231;ais, l&#8217;EuroWordNet (Vossen, 1998) est la premi&#232;re traduction fran&#231;aise
de WordNet. C&#8217;est une ressource d&#8217;une couverture limit&#233;e qui demande des am&#233;liorations
significatives avant de pouvoir &#234;tre utilis&#233;e (Jacquin et al., 2007), et qui n&#8217;est ni libre ni librement
accessible. WOLF est une seconde traduction initialement construite &#224; l&#8217;aide de corpus parall&#232;les
(Sagot et Fi&#353;er, 2008) et &#233;tendue depuis avec diff&#233;rentes techniques (Apidianaki et Sagot, 2012).
WOLF est distribu&#233; sous une licence libre compatible avec la LGPL et c&#8217;est aujourd&#8217;hui le WordNet
fran&#231;ais standard. Enfin, JAWS (Mouton et de Chalendar, 2010) est une traduction des noms de
WordNet d&#233;velopp&#233;e &#224; l&#8217;aide de dictionnaires bilingues et d&#8217;un mod&#232;le de langue syntaxique.
</p>
<p>Nos travaux &#233;tendent et am&#233;liorent les techniques utilis&#233;es dans JAWS et l&#8217;&#233;valuent &#224; l&#8217;aide d&#8217;une
adjudication de deux annotateurs. Le r&#233;sultat de ce travail est WoNeF 1. Il se d&#233;cline en trois
versions pour r&#233;pondre &#224; diff&#233;rents besoins. Le WoNeF principal a un F-score de 70.9%, une autre
version a une pr&#233;cision de 93.3%, et une derni&#232;re contient 109 447 paires (litt&#233;ral, synset).
</p>
<p>L&#8217;approche de JAWS consiste &#224; combiner des s&#233;lecteurs vari&#233;s permettant de choisir les tra-
ductions adapt&#233;es &#224; chaque synset (section 2). Les contributions principales de cet article sont
l&#8217;am&#233;lioration de JAWS et sa compl&#233;tion en ajoutant les verbes et les adjectifs (section 3) et son
&#233;valuation (sections 4 et 5). Cette &#233;valuation se fait &#224; travers une adjudication elle-m&#234;me valid&#233;e
par la mesure de l&#8217;accord inter-annotateur, ce qui montre la validit&#233; de l&#8217;approche par extension
pour traduire WordNet.
</p>
<p>1. Ce travail a &#233;t&#233; en partie financ&#233; par le projet ANR ASFALDA ANR-12-CORD-0023.
</p>
<p>TALN-R&#201;CITAL 2013, 17-21 Juin, Les Sables d&#8217;Olonne
</p>
<p>77 c&#65535; ATALA</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>2 JAWS
</p>
<p>2.1 Processus de traduction
</p>
<p>(Mouton et de Chalendar, 2010) ont con&#231;u JAWS comme un algorithme faiblement supervis&#233; qui
ne demande aucune donn&#233;e annot&#233;e manuellement. Pour traduire un wordnet source, JAWS
s&#8217;appuie sur un dictionnaire bilingue et un mod&#232;le de langue syntaxique pour le langage cible.
</p>
<p>Le dictionnaire bilingue est une concat&#233;nation du dictionnaire bilingue SCI-FRAN-EurADic 2
</p>
<p>et des liens entre les Wiktionnaires fran&#231;ais et anglais 3. Le mod&#232;le de langue syntaxique a
&#233;t&#233; entra&#238;n&#233; sur un grand corpus extrait du web (Grefenstette, 2007). Le corpus a &#233;t&#233; analys&#233;
par LIMA (Besan&#231;on et al., 2010), une cha&#238;ne d&#8217;analyse linguistique ici utilis&#233;e comme un
analyseur syntaxique &#224; base de r&#232;gles produisant des d&#233;pendances syntaxiques fines. Pour une
relation donn&#233;e r et un mot x , le mod&#232;le de langue indique quels sont les 100 premiers mots
co-occurrant le plus fr&#233;quemment avec x dans la relation r. Avec le mot avion et la relation
de compl&#233;ment du nom, le mot billet modifie le plus avion : billet d&#8217;avion est fr&#233;quent dans le
corpus. Le mod&#232;le de langue ici pr&#233;sent&#233; peut-&#234;tre visualis&#233; sur http://www.kalisteo.
fr/demo/semanticmap/index.php.
</p>
<p>Gr&#226;ce aux dictionnaires, JAWS n&#8217;a pas besoin de s&#233;lectionner les litt&#233;raux de chaque synset parmi
l&#8217;ensemble du vocabulaire mais seulement parmi un petit nombre de candidats (9 en moyenne).
Le processus de traduction se fait en trois &#233;tapes :
</p>
<p>1. Cr&#233;er un wordnet vide : la structure de WordNet est pr&#233;serv&#233;e, mais les synsets eux-m&#234;mes
n&#8217;ont pas de litt&#233;raux associ&#233;s.
</p>
<p>2. Choisir les traductions les plus faciles parmi les candidats des dictionnaires pour commencer
&#224; remplir JAWS.
</p>
<p>3. &#201;tendre JAWS de mani&#232;re incr&#233;mentale en utilisant le mod&#232;le de langue, les relations entre
synsets et le JAWS d&#233;j&#224; existant.
</p>
<p>S&#233;lecteurs initiaux Quatre algorithmes que nous nommons s&#233;lecteurs initiaux choisissent des
traductions correctes parmi celles qui sont propos&#233;es par les dictionnaires. Premi&#232;rement, les
mots qui apparaissent dans un seul synset ne sont pas ambig&#252;s et il suffit d&#8217;ajouter toutes leurs
traductions au WordNet fran&#231;ais : c&#8217;est le s&#233;lecteur par monos&#233;mie. C&#8217;est le cas de grumpy :
toutes ses traductions sont valid&#233;es dans le synset o&#249; il appara&#238;t. Deuxi&#232;mement, le s&#233;lecteur
par unicit&#233; identifie les mots n&#8217;ayant qu&#8217;une seule traduction et la valident dans tous les synsets
o&#249; elle est pr&#233;sente. Les cinq synsets contenant pill en anglais sont ainsi compl&#233;t&#233;s avec pilule.
Un troisi&#232;me s&#233;lecteur vise &#224; traduire les mots qui ne sont pas dans le dictionnaire en utilisant
directement la traduction anglaise : c&#8217;est le s&#233;lecteur des transfuges. Un quatri&#232;me s&#233;lecteur
utilise la distance d&#8217;&#233;dition de Levenshtein : si la distance entre un mot anglais et sa traduction
est petite, on peut consid&#233;rer que c&#8217;est le m&#234;me sens (c&#8217;est le cas par exemple pour portion ou
encore university), malgr&#233; l&#8217;existence de certains faux amis. Ces quatre s&#233;lecteurs produisent une
premi&#232;re version du WordNet fran&#231;ais qui contient assez de traductions pour pouvoir ensuite
utiliser le mod&#232;le de langue et continuer de compl&#233;ter les synsets.
</p>
<p>2. http://catalog.elra.info/product_info.php?products_id=666
3. http://www.wiktionary.org/
</p>
<p>TALN-R&#201;CITAL 2013, 17-21 Juin, Les Sables d&#8217;Olonne
</p>
<p>78 c&#65535; ATALA</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Expansion de JAWS JAWS &#233;tant partiellement rempli, une nouvelle &#233;tape d&#8217;expansion tire
parti des relations entre les synsets de WordNet pour valider de nouvelles traductions. Par
exemple, si :
</p>
<p>&#8211; un synset S1 est m&#233;ronyme d&#8217;un synset S2 dans WordNet,
&#8211; il existe un contexte o&#249; un litt&#233;ral dans S1 est m&#233;ronyme d&#8217;un litt&#233;ral candidat C dans S2,
</p>
<p>alors ce litt&#233;ral est consid&#233;r&#233; comme correct. La t&#226;che de traduction est ainsi r&#233;duite &#224; une t&#226;che
de comparaison entre d&#8217;une part les relations lexicales entre les synsets de WordNet et d&#8217;autre
part les relations lexicales entre les lex&#232;mes du fran&#231;ais.
</p>
<p>Prenoms l&#8217;exemple de quill qui peut se traduire par piquant ou plume (Figure 1). Dans WordNet,
quill est m&#233;ronyme de porcupine qui a d&#233;j&#224; &#233;t&#233; traduit par porc-&#233;pic par un s&#233;lecteur initial. Dans
le mod&#232;le de langue, piquant fait partie des compl&#233;ments du noms de porc-&#233;pic mais ce n&#8217;est
pas le cas de plume. Ici, la relation de compl&#233;ment du nom implique la m&#233;ronymie et c&#8217;est donc
piquant qu&#8217;il faut choisir comme la traduction correcte de quill. Le mod&#232;le de langue a permis la
d&#233;sambigu&#239;sation parmi les deux traductions possibles.
</p>
<p>Synset S1
- Anglais : quill
- Fran&#231;ais : piquant ? plume ?
(a stiff hollow protective spine
on a porcupine or hedgehog)
</p>
<p>Synset S2
- Anglais : porcupine, hedgehog
- Fran&#231;ais : porc-&#233;pic
(relatively large rodents with
sharp erectile bristles mingled
with the fur)
</p>
<p>m&#233;ronyme de
</p>
<p>(relation WordNet)
</p>
<p>porc-&#233;pic
m&#233;moire, piquant, poil,
&#233;pine, y&#233;ti, rago&#251;t, grotte,
tactique, pelage, dextre, aiguille, ...
</p>
<p>compl&#233;ment du nom de
</p>
<p>(mod&#232;le de langue)
</p>
<p>FIGURE 1 &#8211; Traduction via la relation de m&#233;ronymie de partie.
</p>
<p>Un probl&#232;me potentiel avec cette approche est que la relation de compl&#233;ment du nom n&#8217;est pas
limit&#233;e &#224; la m&#233;ronymie. Par exemple, le mot m&#233;moire qui appara&#238;t dans le mod&#232;le de langue
(Figure 1) vient d&#8217;un livre intitul&#233; M&#233;moires d&#8217;un porc-&#233;pic. Heureusement, m&#233;moire n&#8217;est pas
dans les candidats de quill et ne peut pas &#234;tre choisi comme une traduction. Paradoxalement, le
mod&#232;le de langue ne peut pas choisir entre deux mots tr&#232;s diff&#233;rents, mais est capable de choisir
la traduction correcte d&#8217;un mot polys&#233;mique. Alors que traduire WordNet automatiquement avec
un dictionnaire ou un mod&#232;le de langue syntaxique est impossible, combiner les deux ressources
permet de r&#233;soudre le probl&#232;me.
</p>
<p>Chaque s&#233;lecteur suit le m&#234;me principe que le s&#233;lecteur par m&#233;ronymie de partie et traduit de
nouveaux synsets en identifiant les relations entre lex&#232;mes via le mod&#232;le de langue syntaxique. La
correspondance entre la relation de compl&#233;ment du nom et la relation de m&#233;ronymie est directe,
mais ce n&#8217;est pas le cas pour les autres relations : il n&#8217;y a par exemple pas de relation syntaxique
qui exprime directement la synonymie entre deux lex&#232;mes. Pour ces relations, il est n&#233;cessaire
d&#8217;employer soit des motifs lexicaux (Hearst, 1992) soit des relations syntaxiques de deuxi&#232;me
ordre (Lenci et Benotto, 2012). Ce sont ces derni&#232;res, aussi nomm&#233;es relations paradigmatiques,
</p>
<p>TALN-R&#201;CITAL 2013, 17-21 Juin, Les Sables d&#8217;Olonne
</p>
<p>79 c&#65535; ATALA</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>que JAWS utilise. Pour la synonymie, si deux mots partagent les m&#234;mes co-occurents dans une
relation syntaxique donn&#233;e, alors ils peuvent &#234;tre synonymes dans ce contexte. Pour les noms,
les relations syntaxiques qui donnent les meilleurs r&#233;sultats sont les relations de compl&#233;ment du
nom, d&#8217;objet du verbe et d&#8217;apposition. Concr&#232;tement, si deux noms qui modifient les m&#234;mes noms
sont les objets des m&#234;mes verbes ou sont appos&#233;s aux m&#234;mes noms, alors il est probable qu&#8217;ils
soient synonymes et si l&#8217;un des deux est d&#233;j&#224; dans un synset, alors on peut y ajouter le second.
Par exemple, avant-propos et pr&#233;face partagent les m&#234;mes compl&#233;ments du noms : livre, &#233;dition,
ouvrage. Le s&#233;lecteur par synonymie peut ajouter avant-propos une fois que le litt&#233;ral pr&#233;face
est dans JAWS. (Mouton et de Chalendar, 2010; Mouton, 2011) d&#233;crivent d&#8217;autres s&#233;lecteurs
exploitant notamment les relations d&#8217;hyperonymie et d&#8217;hyponymie.
</p>
<p>2.2 Limites de JAWS
</p>
<p>JAWS souffre d&#8217;un certain nombre de limites. Avant tout, il ne contient que des noms, ce qui
emp&#234;che de l&#8217;utiliser dans de nombreuses applications. Ensuite, la fa&#231;on dont il a &#233;t&#233; &#233;valu&#233; rend
difficile tout jugement sur sa qualit&#233;. En effet, JAWS a &#233;t&#233; &#233;valu&#233; en le comparant &#224; l&#8217;EuroWordNet
du fran&#231;ais et &#224; WOLF 0.1.4 (qui date de 2008). Ces deux WordNets du fran&#231;ais ne sont pas des
annotations de r&#233;f&#233;rences : ils souffrent soit d&#8217;une pr&#233;cision limit&#233;e soit d&#8217;une couverture limit&#233;e.
</p>
<p>M&amp;C ont d&#233;cid&#233; de compl&#233;ter cette &#233;valuation limit&#233;e par une &#233;valuation manuelle des litt&#233;raux
n&#8217;existant pas dans WOLF, mais elle n&#8217;a &#233;t&#233; faite que sur 120 paires (litt&#233;ral, synset). La pr&#233;cision
de JAWS est &#233;valu&#233;e &#224; 67,1% (Mouton, 2011), ce qui est plus bas que celle de WOLF 0.1.4
et consid&#233;rablement plus bas que la pr&#233;cision de WOLF 1.0b 4. Ce score, m&#234;me bas, est &#224;
prendre avec pr&#233;caution &#233;tant donn&#233; la taille de l&#8217;&#233;chantillon de test : l&#8217;intervalle de confiance est
d&#8217;environ 25%. Une autre limite de JAWS est qu&#8217;il ne contient qu&#8217;une seule et unique ressource
qui ne correspond pas &#224; tous les besoins.
</p>
<p>&#192; notre connaissance, les traductions automatiques de WordNet actuelles n&#8217;existent qu&#8217;en une
seule version o&#249; les auteurs d&#233;cident eux-m&#234;mes quelle m&#233;trique optimiser. Nous fournissons
aussi une telle version, mais ajoutons aussi deux ressources qui peuvent servir des besoins
diff&#233;rents. M&#234;me si notre WoNeF &#224; haute pr&#233;cision est petit, il peut &#234;tre utilis&#233; comme une
annotation de r&#233;f&#233;rence et servir pour entra&#238;ner un syst&#232;me d&#8217;apprentissage. Une ressource &#224;
haute couverture peut servir de base &#224; une correction manuelle ou servir pour une intersection &#224;
d&#8217;autres ressources, ce qui est la raison pour laquelle nous en fournissons une aussi.
</p>
<p>3 WoNeF : un JAWS nominal am&#233;lior&#233;
</p>
<p>Cette section pr&#233;sente les trois am&#233;liorations essentielles qui ont &#233;t&#233;s apport&#233;es &#224; JAWS. Un
changement non d&#233;taill&#233; est celui qui a men&#233; &#224; une meilleure rapidit&#233; d&#8217;ex&#233;cution : JAWS
se construit en plusieurs heures contre moins d&#8217;une minute pour WoNeF, ce qui a facilit&#233; les
exp&#233;rimentations.
</p>
<p>4. Nous remercions Beno&#238;t Sagot pour nous avoir fourni cette version pr&#233;liminaire de WOLF 1.0.
</p>
<p>TALN-R&#201;CITAL 2013, 17-21 Juin, Les Sables d&#8217;Olonne
</p>
<p>80 c&#65535; ATALA</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>3.1 S&#233;lecteurs initiaux
</p>
<p>Les s&#233;lecteurs initiaux de JAWS ne sont pas optimaux. Alors que les s&#233;lecteurs par monos&#233;mie et
par unicit&#233; sont conserv&#233;s, nous avons chang&#233; les autres s&#233;lecteurs. Premi&#232;rement, le s&#233;lecteur
des transfuges est supprim&#233; : sa pr&#233;cision &#233;tait tr&#232;s basse, m&#234;me pour les noms.
</p>
<p>Deuxi&#232;mement, un nouveau s&#233;lecteur consid&#232;re les traductions candidates provenant de plusieurs
mots anglais diff&#233;rents dans un synset donn&#233; : c&#8217;est le s&#233;lecteur par sources multiples. Par exemple,
dans le synset line, railway line, rail line (the road consisting of railroad track and roadbed), les
litt&#233;raux fran&#231;ais ligne de chemin de fer et voie sont des traductions &#224; la fois de line et railway line,
et sont donc choisis comme traductions.
</p>
<p>Troisi&#232;mement, le s&#233;lecteur de la distance de Levenshtein a &#233;t&#233; am&#233;lior&#233;. 28% du vocabulaire
anglais est d&#8217;origine fran&#231;aise (Finkenstaedt et al., 1973), et l&#8217;anglicisation a produit des transfor-
mations pr&#233;visibles. Il est possible d&#8217;appliquer ces m&#234;mes transformations aux litt&#233;raux candidats
fran&#231;ais, et seulement alors d&#8217;appliquer la distance de Levenshtein. Nous commen&#231;ons par sup-
primer les accents, puis appliquons diff&#233;rentes op&#233;rations. Par exemple, l&#8217;inversion des lettres &quot;r&quot;
et &quot;e&quot; prend en compte (order/ordre) et (tiger/tigre) 5. Toutes les transformations ne s&#8217;appliquent
qu&#8217;&#224; la fin des mots : -que est transform&#233; en -k ou -c (marque devient mark), -t&#233; vers -ty (extremit&#233;
devient extremity), etc. Les faux-amis ne sont toujours pas explicitement pris en compte.
</p>
<p>3.2 Apprentissage de seuils
</p>
<p>Dans JAWS, chaque litt&#233;ral anglais ne peut avoir qu&#8217;une traduction fran&#231;aise correspondante. La
traduction choisie est celle qui a le meilleur score, ind&#233;pendamment des scores des traductions
moins bien not&#233;es. Cela a pour effet de rejeter des candidats valides et d&#8217;accepter des candidats
erron&#233;s. Par exemple, JAWS n&#8217;inclut pas particulier au synset (a human being) &#8220;there was too
much for one person to do&#8221; parce que personne est d&#233;j&#224; inclus avec un score sup&#233;rieur.
</p>
<p>Dans WoNeF, nous avons donc appris un seuil pour chaque partie du discours et s&#233;lecteur. Nous
avons d&#8217;abord g&#233;n&#233;r&#233; les scores pour toutes les paires (litt&#233;ral, synset) candidates, puis tri&#233; ces
paires par score. Les 12 399 paires pr&#233;sentes dans l&#8217;&#233;valuation manuelle associ&#233;e &#224; WOLF 1.0b
(notre ensemble d&#8217;apprentissage) ont &#233;t&#233; jug&#233;es correctes tandis que les paires n&#8217;y &#233;tant pas ont
&#233;t&#233; jug&#233;es erron&#233;es. Nous avons ensuite calcul&#233; les seuils maximisant la pr&#233;cision et le F-score. Le
seuil qui maximise le F-score est utilis&#233; dans les ressources &#224; haut F-score et &#224; haute couverture,
tandis que le seuil maximisant la pr&#233;cision est utilis&#233; dans la ressource &#224; haute pr&#233;cision.
</p>
<p>Une fois que ces seuils sont d&#233;finis, les s&#233;lecteurs choisissent tous les candidats au-dessus du
nouveau seuil, ce qui a deux effets positifs :
</p>
<p>&#8211; des candidats valides ne sont plus rejet&#233;s simplement parce qu&#8217;un meilleur candidat est aussi
s&#233;lectionn&#233;, ce qui am&#233;liore &#224; la fois le rappel et la couverture.
</p>
<p>&#8211; les candidats invalides qui &#233;taient jusque-l&#224; accept&#233;s sont maintenant rejet&#233;s gr&#226;ce au seuil
plus strict : la pr&#233;cision s&#8217;en retrouve augment&#233;e.
</p>
<p>5. La distance de Damerau-Levenshtein qui prend en compte les inversions n&#8217;importe-o&#249; dans un mot (Damerau,
1964) a donn&#233; de moins bons r&#233;sultats.
</p>
<p>TALN-R&#201;CITAL 2013, 17-21 Juin, Les Sables d&#8217;Olonne
</p>
<p>81 c&#65535; ATALA</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>3.3 Vote
</p>
<p>Apr&#232;s l&#8217;application des diff&#233;rents s&#233;lecteurs, notre WordNet est large mais contient des synsets
bruit&#233;s. Comme toutes les traductions automatiques de WordNet, WoNeF doit alors &#234;tre nettoy&#233;
(Sagot et Fi&#353;er, 2012). Dans WoNeF, le bruit provient de diff&#233;rents facteurs :
</p>
<p>&#8211; les s&#233;lecteurs essaient d&#8217;inf&#233;rer des informations s&#233;mantiques &#224; partir d&#8217;une analyse syntaxique
sans prendre en compte toute la complexit&#233; de l&#8217;interface syntaxe-s&#233;mantique,
</p>
<p>&#8211; l&#8217;analyseur syntaxique produit lui-m&#234;me des r&#233;sultats bruit&#233;s,
&#8211; le mod&#232;le de langue syntaxique est produit &#224; partir d&#8217;un corpus extrait du web lui-m&#234;me bruit&#233;
(texte mal &#233;crit, contenu non textuel, phrases non fran&#231;aises) et n&#8217;est pas une &#171; distribution
id&#233;ale &#187; (Copestake et Herbelot, 2012),
</p>
<p>&#8211; les traductions d&#233;j&#224; choisies sont consid&#233;r&#233;es comme valides dans les &#233;tapes suivantes alors
que ce n&#8217;est pas toujours le cas.
</p>
<p>Pour la ressource haute-pr&#233;cision, il fallait donc un moyen de ne garder que les litt&#233;raux pour
lesquels les s&#233;lecteurs &#233;taient les plus confiants. &#201;tant donn&#233; que, contrairement &#224; JAWS, plusieurs
s&#233;lecteurs peuvent choisir une m&#234;me traduction (sous-section 3.2), notre solution est simple et
efficace : les traductions valid&#233;es par un bon s&#233;lecteur ou par plusieurs s&#233;lecteurs moyens sont
conserv&#233;es tandis que les autres sont supprim&#233;es. Ce principe de vote est aussi appel&#233; m&#233;thode
d&#8217;ensemble en apprentissage automatique. Les s&#233;lecteurs performants varient d&#8217;une partie du
discours &#224; une autre : le choix est fait sur un ensemble de d&#233;veloppement contenant 10% de
notre r&#233;f&#233;rence.
</p>
<p>Cette op&#233;ration de nettoyage ne conserve que 18% des traductions (de 87 757 paires (litt&#233;ral,
synset) &#224; 15 625) mais la pr&#233;cision grimpe de 68,4% &#224; 93,3%. Cette ressource &#224; haute pr&#233;cision
peut &#234;tre utilis&#233;e comme donn&#233;e d&#8217;entra&#238;nement. Un d&#233;faut classique des m&#233;thodes de vote
est de ne choisir que des exemples faciles et peu int&#233;ressants, mais la ressource obtenue ici
est &#233;quilibr&#233;e entre les synsets ne contenant que des mots monos&#233;miques et d&#8217;autres synsets
contenant des mots polys&#233;miques et plus difficiles &#224; d&#233;sambigu&#239;ser (section 5.2).
</p>
<p>3.4 Extension aux verbes, adjectifs et adverbes
</p>
<p>Les travaux sur JAWS ont commenc&#233; par les noms parce qu&#8217;ils repr&#233;sentent 70% des synsets
dans WordNet. Nous avons continu&#233; ce travail sur les autres parties du discours qui sont aussi
importantes pour examiner le sens d&#8217;un texte donn&#233; : verbes, adjectifs et adverbes. Les s&#233;lecteurs
g&#233;n&#233;riques ont ici &#233;t&#233; modifi&#233;s, mais il s&#8217;agira dans le futur d&#8217;impl&#233;menter des s&#233;lecteurs prenant
en compte les sp&#233;cificit&#233;s des diff&#233;rentes parties du discours dans WordNet.
</p>
<p>Verbes Les s&#233;lecteurs choisis pour les verbes sont le s&#233;lecteur par unicit&#233; et par monos&#233;mie.
En effet, la distance de Levenshtein a donn&#233; des r&#233;sultats m&#233;diocres pour les verbes : seuls 25%
des verbes choisis par ce s&#233;lecteur &#233;taient des traductions correctes. Concernant les s&#233;lecteurs
syntaxiques, seul le s&#233;lecteur par synonymie a donn&#233; de bons r&#233;sultats, alors que le s&#233;lecteur par
hyponymie avait les performances d&#8217;un classifieur al&#233;atoire.
</p>
<p>TALN-R&#201;CITAL 2013, 17-21 Juin, Les Sables d&#8217;Olonne
</p>
<p>82 c&#65535; ATALA</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Adjectifs Les adjectifs sont traduits de la m&#234;me mani&#232;re que les noms : tout d&#8217;abord un nombre
limit&#233; de s&#233;lecteurs initiaux remplit un WordNet vide, puis les s&#233;lecteurs syntaxiques compl&#232;tent
cette traduction avec le mod&#232;le de langue syntaxique. Tous les s&#233;lecteurs initiaux sont ici choisis,
et le s&#233;lecteur syntaxique choisi est le s&#233;lecteur par synonymie. Ils ont donn&#233; de bons r&#233;sultats
qui sont pr&#233;sent&#233;s dans la section 5.3.
</p>
<p>Adverbes Nous n&#8217;avons pas d&#8217;annotation de r&#233;f&#233;rence pour les adverbes, ce qui explique
qu&#8217;ils ne sont pas inclus dans WoNeF : nous ne pouvons &#233;valuer leur pr&#233;cision. Cependant, la
comparaison avec WOLF (section 5.4) montre que les adverbes ont de meilleurs r&#233;sultats que
les autres parties du discours, ce qui laisse penser que c&#8217;est une ressource de qualit&#233;. C&#8217;est une
ressource aussi tr&#232;s compl&#233;mentaire : 87% des adverbes propos&#233;s ne sont pas dans WOLF. Une
fusion entre WoNeF et WOLF aurait trois fois plus d&#8217;adverbes que WOLF seul.
</p>
<p>4 WoNeF : un JAWS &#233;valu&#233;
</p>
<p>4.1 D&#233;veloppement d&#8217;une annotation de r&#233;f&#233;rence
</p>
<p>L&#8217;&#233;valuation de JAWS souffre d&#8217;un certain nombre de limites (section 2.2). Pour &#233;valuer rigoureu-
sement notre propre traduction de WordNet, nous avons produit une annotation de r&#233;f&#233;rence.
Pour chaque partie du discours, 300 synsets ont &#233;t&#233; annot&#233;s par deux annotateurs locuteurs natifs
du fran&#231;ais. Pour chaque traduction candidate fournie par nos dictionnaires, il fallait d&#233;cider si
oui ou non elle appartenait au synset. Puisque les dictionnaires ne proposent pas de candidats
pour tous les synsets et que certains synsets n&#8217;ont pas de candidat valable, le nombre r&#233;el de
synsets non vides est inf&#233;rieur &#224; 300 (section 4.2).
</p>
<p>Durant l&#8217;annotation manuelle, nous avons rencontr&#233; une difficult&#233; importante d&#233;coulant de la
tentative de traduire WordNet dans une autre langue. Dans le cas de l&#8217;anglais vers le fran&#231;ais, la
plupart des difficult&#233;s proviennent des verbes et adjectifs figurant dans une collocation. Dans
WordNet, ils peuvent &#234;tre regroup&#233;s d&#8217;une mani&#232;re qui fait sens en anglais, mais qui ne se retrouve
pas directement dans une autre langue. Par exemple, l&#8217;adjectif pointed est le seul &#233;l&#233;ment d&#8217;un
synset d&#233;fini comme direct and obvious in meaning or reference ; often unpleasant ; &#8220;a pointed
critique&#8221; ; &#8220;a pointed allusion to what was going on&#8221; ; &#8220;another pointed look in their direction&#8221;. Ces
exemples se traduiraient par trois adjectifs diff&#233;rents en fran&#231;ais : une critique dure, une allusion
claire et un regard appuy&#233;. Il n&#8217;existe pas de solution satisfaisante lors de la traduction d&#8217;un tel
synset : le synset r&#233;sultant contiendra soit trop soit trop peu de traductions. Nous avons d&#233;cid&#233;
de ne pas traduire ces synsets dans notre annotation manuelle. Ces probl&#232;mes de granularit&#233;
concernent 3% des synsets nominaux, 8% des synsets verbaux et 6% des synsets adjectivaux.
Actuellement, WoNeF ne d&#233;tecte pas de tels synsets.
</p>
<p>L&#8217;autre difficult&#233; principale d&#233;coule de traductions manquantes, ce qui peut &#234;tre consid&#233;r&#233; comme
un d&#233;faut de nos ressources. Les sens rares d&#8217;un mot sont parfois absents. Par exemple, le sens
to catch du jeu du chat (ou du loup) et le sens coat with beaten egg du verbe to egg ne sont pas
pr&#233;sents. Aucun de ces sens ne sont dans les synsets les polys&#233;miques (d&#233;finis &#224; la section 5.2),
ce qui confirme que cela ne se produit que pour les sens rares. Pourtant, WoNeF pourrait &#234;tre
am&#233;lior&#233; en utilisant des dictionnaires sp&#233;cifiques pour, par exemple, les esp&#232;ces (comme dans
(Sagot et Fi&#353;er, 2008)), les termes m&#233;dicaux, les entit&#233;s nomm&#233;es (en utilisant Wikipedia) et ainsi
</p>
<p>TALN-R&#201;CITAL 2013, 17-21 Juin, Les Sables d&#8217;Olonne
</p>
<p>83 c&#65535; ATALA</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>de suite. Un autre exemple est celui des adjectifs de jugement : il n&#8217;y a pas de bonne traduction
de weird en fran&#231;ais. M&#234;me si la plupart des dictionnaires fournissent bizarre comme traduction,
on ne retrouve pas dans bizarre l&#8217;aspect stupide du mot weird : les deux adjectifs ne sont pas
substituables dans tous les contextes, ce qui est un probl&#232;me si l&#8217;on consid&#232;re que le sens d&#8217;un
synset doit &#234;tre conserv&#233; par la traduction.
</p>
<p>4.2 Accord inter-annotateurs
</p>
<p>Malgr&#233; les difficult&#233;s mentionn&#233;es ci-dessus, l&#8217;annotation r&#233;sultante a &#233;t&#233; valid&#233;e par la mesure de
l&#8217;accord inter-annotateurs, qui montre que l&#8217;approche par extension pour la cr&#233;ation de nouveaux
wordnets est valide et peut produire des ressources utiles. Deux annotateurs humains, auteurs
de cet article, respectivement linguiste informaticien et informaticien linguiste, ont annot&#233; de
fa&#231;on ind&#233;pendante les m&#234;mes synsets choisis au hasard pour chaque partie du discours. Ils
ont utilis&#233; WordNet pour examiner les synsets voisins, le dictionnaire Merriam-Webster, le TLFi
(Pierrel, 2003) et des moteurs de recherche pour attester l&#8217;utilisation des divers sens des mots
consid&#233;r&#233;s. Apr&#232;s adjudication faite par ces deux annotateurs en confrontant leurs opinions en
cas de d&#233;saccord, l&#8217;annotation de r&#233;f&#233;rence a &#233;t&#233; form&#233;e.
</p>
<p>Noms Verbes Adjectifs
</p>
<p>Kappa de Fleiss 0.715 0.711 0.663
Synsets non-vides 270 222 267
</p>
<p>Candidats par synset 6.22 14.50 7.27
</p>
<p>TABLE 1 &#8211; Accord inter-annotateurs sur l&#8217;annotation de r&#233;f&#233;rence
</p>
<p>La table 1 montre l&#8217;accord inter-annotateur &#233;valu&#233; par le kappa de Fleiss pour les trois parties
du discours annot&#233;es. M&#234;me s&#8217;il s&#8217;agit d&#8217;une m&#233;trique discut&#233;e (Powers, 2012), toutes les tables
d&#8217;&#233;valuation existantes consid&#232;rent ces scores comme &#233;tant suffisamment &#233;lev&#233;s pour d&#233;crire cet
accord inter-annotateurs comme &#171; bon &#187; (Gwet, 2001), ce qui nous permet de dire que notre
annotation de r&#233;f&#233;rence est de bonne qualit&#233;. L&#8217;approche par extension pour la traduction de
WordNet est elle aussi valid&#233;e.
</p>
<p>5 R&#233;sultats
</p>
<p>Nous pr&#233;sentons dans cette section les r&#233;sultats de WoNeF. Nous commen&#231;ons par d&#233;crire
les r&#233;sultats apr&#232;s l&#8217;application de l&#8217;&#233;tape des s&#233;lecteurs initiaux seulement puis ceux de la
ressource compl&#232;te. Notre annotation de r&#233;f&#233;rence est d&#233;coup&#233;e en deux parties : 10% des
litt&#233;raux forment l&#8217;ensemble de d&#233;veloppement utilis&#233; pour choisir les s&#233;lecteurs s&#8217;appliquant aux
diff&#233;rentes versions de WoNeF, tandis que les 90% restant forment l&#8217;ensemble de test servant &#224;
l&#8217;&#233;valuation. Pr&#233;cision et rappel sont calcul&#233;s sur l&#8217;intersection des synsets pr&#233;sents dans WoNeF et
l&#8217;annotation de r&#233;f&#233;rence consid&#233;r&#233;e, que ce soit l&#8217;ensemble de test de notre propre adjudication
(sections 5.1 &#224; 5.3) ou WOLF (section 5.4). Par exemple, la pr&#233;cision est la fraction des paires
(litt&#233;ral, synset) correctes au sein de l&#8217;intersection en question.
</p>
<p>TALN-R&#201;CITAL 2013, 17-21 Juin, Les Sables d&#8217;Olonne
</p>
<p>84 c&#65535; ATALA</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>5.1 S&#233;lecteurs initiaux
</p>
<p>Pour les noms, les verbes et les adjectifs, nous avons calcul&#233; l&#8217;efficacit&#233; de chaque s&#233;lecteur
initial sur notre ensemble de d&#233;veloppement, et utilis&#233; ces donn&#233;es pour d&#233;terminer ceux qui
doivent &#234;tre inclus dans la version ayant une haute pr&#233;cision, celle ayant un F-score &#233;lev&#233; et celle
pr&#233;sentant une grande couverture. Les scores ci-dessous sont calcul&#233;s sur l&#8217;ensemble de test, plus
grand et plus repr&#233;sentatif.
</p>
<p>P R F1 C
monos&#233;mie 71.5 76.6 74.0 54 499
</p>
<p>unicit&#233; 91.7 63.0 75.3 9 533
sources multiples 64.5 45.0 53.0 27 316
</p>
<p>Levenshtein 61.9 29.0 39.3 20 034
</p>
<p>haute pr&#233;cision 93.8 50.1 65.3 13 867
haut F-score 71.1 72.7 71.9 82 730
</p>
<p>haute couverture 69.0 69.8 69.4 90 248
</p>
<p>TABLE 2 &#8211; S&#233;lecteurs initiaux sur l&#8217;ensemble des traductions (noms, verbes et adjectifs). La
couverture C est le nombre total de paires (litt&#233;ral, synset).
</p>
<p>La table 2 montre les r&#233;sultats de cette op&#233;ration. La couverture donne une id&#233;e de la taille
des ressources. En fonction des objectifs de chaque ressource, les s&#233;lecteurs initiaux choisis
seront diff&#233;rents. Diff&#233;rents s&#233;lecteurs peuvent choisir plusieurs fois une m&#234;me traduction, ce qui
explique que la somme des couvertures soit sup&#233;rieure &#224; la couverture de la ressource &#224; haute
couverture. Fait int&#233;ressant non visible dans la table, le s&#233;lecteur le moins efficace pour les verbes
est la distance de Levenshtein avec une pr&#233;cision de l&#8217;ordre de 25% : les faux amis semblent &#234;tre
plus nombreux pour les verbes.
</p>
<p>5.2 R&#233;sultats globaux
</p>
<p>Nous nous int&#233;ressons maintenant aux r&#233;sultats globaux (Table 3). Ils comprennent l&#8217;application
des s&#233;lecteurs initiaux et des s&#233;lecteurs syntaxiques. Le mode de haute pr&#233;cision applique
&#233;galement un vote (section 3.3). Comme pour la table pr&#233;c&#233;dente, la couverture C indique le
nombre de paires (litt&#233;ral, synset).
</p>
<p>Tous synsets Synsets BCS
P R F1 C P R F1 C
</p>
<p>haute pr&#233;cision 93.3 51.5 66.4 15 625 90.4 36.5 52.0 1 877
haut F-score 68.9 73.0 70.9 88 736 56.5 62.8 59.1 14 405
</p>
<p>haute couverture 60.5 74.3 66.7 109 447 44.5 66.9 53.5 23 166
</p>
<p>TABLE 3 &#8211; R&#233;sultats globaux : tous les synsets et synsets BCS.
</p>
<p>Dans WordNet, les mots sont majoritairement monos&#233;miques, mais c&#8217;est une petite minorit&#233; de
mots polys&#233;miques qui est la plus repr&#233;sent&#233;e dans les textes. C&#8217;est justement sur cette minorit&#233;
que nous souhaitons produire une ressource de qualit&#233;. Pour l&#8217;&#233;valuer, nous utilisons la liste des
</p>
<p>TALN-R&#201;CITAL 2013, 17-21 Juin, Les Sables d&#8217;Olonne
</p>
<p>85 c&#65535; ATALA</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>synsets BCS (Basic Concept Set) fournie par le projet BalkaNet (Tufis et al., 2004). Cette liste
contient les 8 516 synsets lexicalis&#233;s dans six traductions diff&#233;rentes de WordNet, et repr&#233;sente
les synsets les plus fr&#233;quents et ceux qui comportent le plus de mots polys&#233;miques. Les r&#233;sultats
montrent le nombre de synsets BCS pour les ressources &#224; haut F-score et haute couverture. Alors
que les ressources &#224; haut F-score et &#224; haute couverture perdent en pr&#233;cision pour les synsets BCS,
ce n&#8217;est pas le cas pour la ressource &#224; haute pr&#233;cision. En effet, le m&#233;canisme de vote rend la
ressource haute-pr&#233;cision tr&#232;s robuste, et ce m&#234;me pour les synsets BCS.
</p>
<p>5.3 R&#233;sultats par partie du discours
</p>
<p>P R F1 C
</p>
<p>haute pr&#233;cision
noms 96.8 56.6 71.4 11 294
verbes 68.4 41.9 52.0 1 110
adjectifs 90.0 36.7 52.2 3 221
</p>
<p>haut F-score
</p>
<p>noms 71.7 73.2 72.4 59 213
JAWS 70.7 68.5 69.6 55 416
verbes 48.9 76.6 59.6 9 138
adjectifs 69.8 71.0 70.4 20 385
</p>
<p>haute couverture
noms 61.8 78.4 69.1 70 218
verbes 45.4 61.5 52.2 18 844
adjectifs 69.8 71.9 70.8 20 385
</p>
<p>TABLE 4 &#8211; R&#233;sultats par partie du discours. JAWS ne contient que des noms : il est compar&#233; &#224; la
ressource nominale &#224; haut F-score.
</p>
<p>La table 4 montre les r&#233;sultats d&#233;taill&#233;s pour chaque partie du discours. Concernant les noms,
le mode de haute pr&#233;cision utilise deux s&#233;lecteurs, tous deux fond&#233;s sur la relation syntaxique
de compl&#233;ment du nom : le s&#233;lecteur par m&#233;ronymie d&#233;crit &#224; la section 2.1, et le s&#233;lecteur par
hyponymie. La ressource de haute pr&#233;cision pour les noms est notre meilleure ressource. La
version avec le F-score optimis&#233; a un F-score de 72,4%, ce qui garantit que peu de paires (litt&#233;ral,
synset) sont absentes tout en ayant une pr&#233;cision l&#233;g&#232;rement sup&#233;rieure &#224; celle de JAWS.
</p>
<p>Les r&#233;sultats des verbes sont moins &#233;lev&#233;s. L&#8217;explication principale est que les verbes sont en
moyenne plus polys&#233;miques dans WordNet et nos dictionnaires que les autres parties du discours :
les synsets verbaux ont deux fois plus de candidats que les noms et les adjectifs (Table 1). Cela
montre l&#8217;importance du dictionnaire pour limiter le nombre initial de litt&#233;raux parmi lesquels les
algorithmes doivent choisir.
</p>
<p>Le s&#233;lecteur par synonymie est le seul s&#233;lecteur syntaxique appliqu&#233; aux verbes. Il utilise les
relations syntaxiques de second ordre pour trois types de d&#233;pendances syntaxiques verbales :
si deux verbes partagent les m&#234;mes objets, ils sont susceptibles d&#8217;&#234;tre synonymes ou quasi-
synonymes. C&#8217;est le cas des verbes d&#233;vorer et manger qui acceptent tous deux l&#8217;objet pain. Les
autres s&#233;lecteurs syntaxiques n&#8217;ont pas &#233;t&#233; retenus pour les verbes en raison de leurs faibles
r&#233;sultats. En effet, alors que la d&#233;tection de l&#8217;hyponymie en utilisant seulement l&#8217;inclusion de
contextes a &#233;t&#233; efficace sur les noms, elle a les performances d&#8217;un classifieur al&#233;atoire pour les
verbes. Cela met en &#233;vidence la complexit&#233; de la polys&#233;mie des verbes.
</p>
<p>TALN-R&#201;CITAL 2013, 17-21 Juin, Les Sables d&#8217;Olonne
</p>
<p>86 c&#65535; ATALA</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Pour les adjectifs, comme pour les verbes, seul le s&#233;lecteur de synonymie a &#233;t&#233; appliqu&#233;. Pour
les ressources &#224; haut F-score et haute couverture, ce sont les m&#234;mes s&#233;lecteurs (initiaux et
syntaxiques) qui sont appliqu&#233;s, ce qui explique que les r&#233;sultats sont les m&#234;mes. Alors que
l&#8217;accord inter-annotateurs &#233;tait plus bas sur les adjectifs que sur les verbes, les r&#233;sultats eux sont
bien meilleurs pour les adjectifs. Cela s&#8217;explique principalement par le nombre de candidats
parmi lesquels s&#233;lectionner : il y en a deux fois moins pour les adjectifs. Cela met en avant
l&#8217;importance des dictionnaires.
</p>
<p>5.4 &#201;valuation par rapport &#224; WOLF
</p>
<p>WOLF 0.1.4 WOLF 1.0b
pP pR Ajouts pP pR Ajouts
</p>
<p>Noms 50.7 40.0 9 646 73.6 46.4 6 842
Verbes 33.0 23.9 1 064 41.7 17.5 1 084
</p>
<p>Adjectifs 41.7 46.1 3 009 64.4 53.8 3 172
Adverbes 56.2 44.4 3 061 76.5 41.9 2 835
</p>
<p>TABLE 5 &#8211; &#201;valuation de la ressource &#224; haute pr&#233;cision en consid&#233;rant WOLF 0.1.4 et 1.0b comme
des r&#233;f&#233;rences.
</p>
<p>Il n&#8217;est pas possible de comparer WOLF et WoNeF en utilisant notre annotation de r&#233;f&#233;rence :
tout mot correct de WOLF non pr&#233;sent dans les dictionnaires p&#233;nalisera WOLF injustement. Nous
avons d&#233;cid&#233; d&#8217;&#233;valuer WoNeF en consid&#233;rant WOLF 0.1.4 et WOLF 1.0b comme des r&#233;f&#233;rences
(Table 5). Les mesures ne sont pas de v&#233;ritables pr&#233;cision et rappel puisque WOLF lui-m&#234;me n&#8217;est
pas enti&#232;rement valid&#233;. Le dernier article pF donnant des chiffres globaux (Sagot et Fi&#353;er, 2012) :
iindique un nombre de paires autour de 77 000 pour une pr&#233;cision de 86% 6. Nous appelons donc
pseudo-pr&#233;cision (pP) le pourcentage des &#233;l&#233;ments pr&#233;sents dans WoNeF qui sont &#233;galement
pr&#233;sents dans WOLF, et pseudo-rappel le pourcentage d&#8217;&#233;l&#233;ments de WOLF qui sont pr&#233;sents
dans WoNeF. Ces chiffres montrent que m&#234;me si WoNeF est encore plus petit que WOLF, il s&#8217;agit
d&#8217;une ressource compl&#233;mentaire, surtout quand on se souvient que le WoNeF utilis&#233; pour cette
comparaison est celui pr&#233;sentant une pr&#233;cision &#233;lev&#233;e, avec une pr&#233;cision globale de 93,3%. Il
convient &#233;galement de noter que la comparaison de la diff&#233;rence entre WOLF 0.1.4 et WOLF
1.0b est instructive puisque elle montre l&#8217;&#233;tendue des am&#233;liorations apport&#233;es &#224; WOLF.
</p>
<p>La colonne &#171; Ajouts &#187; donne le nombre de traductions qui sont pr&#233;sentes dans WoNeF mais pas
dans WOLF. Pour les noms, les verbes et les adjectifs, cela signifie que nous pouvons contribuer
11 098 nouvelles paires (litt&#233;ral, synset) de haute pr&#233;cision en cas de fusion de WOLF et WoNeF,
soit 94% des paires du WoNeF haute pr&#233;cision ce qui montre la compl&#233;mentarit&#233; des approches :
ce sont des litt&#233;raux diff&#233;rents qui sont ici choisis. Cela produira un wordnet fran&#231;ais 13% plus
grand que WOLF avec une pr&#233;cision am&#233;lior&#233;e. Une fusion avec la ressource de F-score &#233;lev&#233;e
aurait une pr&#233;cision l&#233;g&#232;rement inf&#233;rieure, mais fournirait 57 032 nouvelles paires (litt&#233;ral,
synset) par rapport &#224; WOLF 1.0b, r&#233;sultant en une fusion contenant 73 712 synsets non vides et
159 705 paires (litt&#233;ral, synset), augmentant la couverture de WOLF de 56% et celle de WoNeF
de 83%.
</p>
<p>6. Les r&#233;sultats d&#233;taill&#233;s pour WOLF 1.0b ne sont pas actuellement disponibles.
</p>
<p>TALN-R&#201;CITAL 2013, 17-21 Juin, Les Sables d&#8217;Olonne
</p>
<p>87 c&#65535; ATALA</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Conclusion
</p>
<p>Dans ce travail, nous avons montr&#233; que l&#8217;utilisation d&#8217;un mod&#232;le de langue syntaxique pour
identifier des relations lexicales entre des lex&#232;mes est possible dans un environnement contraint et
conduit &#224; des r&#233;sultats ayant une pr&#233;cision au niveau de l&#8217;&#233;tat de l&#8217;art pour la t&#226;che de traduction
de WordNet. Nous offrons trois ressources diff&#233;rentes, chacune d&#8217;elles ayant un objectif diff&#233;rent.
Enfin, nous fournissons une annotation de r&#233;f&#233;rence valid&#233;e de haute qualit&#233; qui nous a permis
de montrer &#224; la fois la validit&#233; de l&#8217;approche de traduction de WordNet par extension et la
validit&#233; de notre approche sp&#233;cifique. Cette annotation de r&#233;f&#233;rence peut &#233;galement &#234;tre utilis&#233;e
pour &#233;valuer et d&#233;velopper d&#8217;autres traductions fran&#231;aises de WordNet. WoNeF est disponible
librement au format XML DEBVisDic 7 sur http://wonef.fr/ sous la licence CC-BY-SA.
</p>
<p>Les travaux futurs sur WoNeF mettront l&#8217;accent sur les verbes, les adjectifs et les adverbes, pour
lesquels de nouveaux s&#233;lecteurs efficaces peuvent &#234;tre envisag&#233;s pour am&#233;liorer la couverture.
Par exemple, le s&#233;lecteur de similarit&#233; peut &#234;tre &#233;tendu &#224; la relation de quasi-synonymit&#233; que
partagent certains adjectifs dans WordNet. En effet, la synonymie entre les adjectifs est limit&#233;e
par rapport &#224; la quasi-synonymie : alors que fast est le seul mot dans son synset, c&#8217;est le quasi-
synonyme de 20 synsets. Puisque les techniques de s&#233;mantique distributionnelle ont plut&#244;t
tendance &#224; identifier des quasi-synonymes plut&#244;t que des synonymes, utiliser cette relation de
WordNet pour identifier de nouveaux adjectifs fait partie de nos objectifs.
</p>
<p>Une autre source importante d&#8217;am&#233;lioration sera l&#8217;enrichissement de notre mod&#232;le de langue
syntaxique qui pourra prendre en compte les verbes pronominaux et les expressions multi-mots.
Nous aimerions aussi nous orienter vers un mod&#232;le de langue continu (Le et al., 2012) plus
performant. Cela sera coupl&#233; avec la collecte d&#8217;un corpus issu du Web plus r&#233;cent et plus grand
analys&#233; avec une version r&#233;cente de notre analyseur linguistique LIMA. Cela nous permettra de
mesurer l&#8217;impact de la qualit&#233; du mod&#232;le de langue sur la traduction de WordNet.
</p>
<p>Le wordnet fran&#231;ais WOLF a &#233;t&#233; construit en utilisant plusieurs techniques. La fusion de WOLF
et de WoNeF permettra de bient&#244;t am&#233;liorer &#224; nouveau le statut de la traduction fran&#231;aise de
WordNet : nous travaillons avec les auteurs de WOLF afin de fusionner WOLF et WoNeF.
</p>
<p>R&#233;f&#233;rences
</p>
<p>APIDIANAKI, M. et SAGOT, B. (2012). Applying cross-lingual WSD to wordnet development. In
LREC 2012.
</p>
<p>BESAN&#199;ON, R., de CHALENDAR, G., FERRET, O., GARA, F., LAIB, M., MESNARD, O. et SEMMAR,
N. (2010). LIMA : A multilingual framework for linguistic analysis and linguistic resources
development and evaluation. In LREC 2010.
</p>
<p>BOYD-GRABER, J., FELLBAUM, C., OSHERSON, D. et SCHAPIRE, R. (2006). Adding dense, weighted
connections to wordnet. In GWC 2006.
</p>
<p>COPESTAKE, A. et HERBELOT, A. (2012). Lexicalised compositionality. Unpublished draft.
</p>
<p>DAMERAU, F. J. (1964). A technique for computer detection and correction of spelling errors.
Commun. ACM, 7(3):171&#8211;176.
</p>
<p>7. http://nlp.fi.muni.cz/trac/deb2/wiki/WordNetFormat
</p>
<p>TALN-R&#201;CITAL 2013, 17-21 Juin, Les Sables d&#8217;Olonne
</p>
<p>88 c&#65535; ATALA</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>de MELO, G. et WEIKUM, G. (2008). On the Utility of Automatically Generated Wordnets. In
GWC 2008.
DE MELO, G. et WEIKUM, G. (2009). Towards a universal wordnet by learning from combined
evidence. In CIKM 2009, pages 513&#8211;522. ACM.
DYVIK, H. (2004). Translations as semantic mirrors : from parallel corpus to wordnet. Language
and computers, 49(1):311&#8211;326.
FELLBAUM, C., &#233;diteur (1998). WordNet : an Electronic Lexical Database. The MIT Press.
FELLBAUM, C. et VOSSEN, P. (2007). Connecting the universal to the specific : Towards the global
grid. Intercultural Collaboration, pages 1&#8211;16.
FINKENSTAEDT, T., WOLFF, D., NEUHAUS, H. et HERGET, W. (1973). Ordered profusion : Studies in
dictionaries and the English lexicon, volume 13. C. Winter.
GREFENSTETTE, G. (2007). Conquering language : Using NLP on a massive scale to build high
dimensional language models from the web. In CICLing 2007, pages 35&#8211;49.
GWET, K. (2001). Handbook of inter-rater reliability. Advanced Analytics, LLC.
HANOKA, V. et SAGOT, B. (2012). Wordnet extension made simple : A multilingual lexicon-based
approach using wiki resources. In LREC 2012.
HEARST, M. (1992). Automatic acquisition of hyponyms from large text corpora. In Proceedings
of the 14th conference on Computational linguistics - Volume 2, pages 539&#8211;545. ACL.
JACQUIN, C., DESMONTILS, E. et MONCEAUX, L. (2007). French EuroWordNet Lexical Database
Improvements. In CICLing 2007, volume 4394 de LNCS, pages 12&#8211;22.
LE, H.-S., ALLAUZEN, A. et YVON, F. (2012). Continuous Space Translation Models with Neural
Networks. In NAACL-HLT 2012, pages 39&#8211;48. ACL.
LENCI, A. et BENOTTO, G. (2012). Identifying hypernyms in distributional semantic spaces. In
*SEM 2012, pages 75&#8211;79. ACL.
MOUTON, C. (2011). Ressources et m&#233;thodes semi-supervis&#233;es pour l&#8217;analyse s&#233;mantique de texte en
fran&#231;ais. Th&#232;se de doctorat.
MOUTON, C. et de CHALENDAR, G. (2010). JAWS : Just Another WordNet Subset. In TALN 2010.
NAVIGLI, R. et PONZETTO, S. (2010). BabelNet : Building a very large multilingual semantic
network. In ACL 2010, pages 216&#8211;225.
PIANTA, E., BENTIVOGLI, L. et GIRARDI, C. (2002). MultiWordNet : developing an aligned
multilingual database.
PIERREL, J. (2003). Un ensemble de ressources de r&#233;f&#233;rence pour l&#8217;&#233;tude du fran&#231;ais : TLFi,
Frantext et le logiciel Stella. Revue qu&#233;b&#233;coise de linguistique, 32(1):155&#8211;176.
POWERS, D. (2012). The Problem with Kappa. In EACL 2012, page 345.
SAGOT, B. et FI&#352;ER, D. (2012). Cleaning noisy wordnets. In LREC 2012.
SAGOT, B. et FI&#352;ER, D. (2012). Automatic Extension of WOLF. In GWC 2012.
SAGOT, B. et FI&#352;ER, D. (2008). Building a free French wordnet from multilingual resources. In
Ontolex 2008.
TUFIS, D., CRISTEA, D. et STAMOU, S. (2004). BalkaNet : Aims, methods, results and perspectives.
a general overview. Romanian Journal of Information Science and Technology, 7(1-2):9&#8211;43.
VOSSEN, P. (1998). EuroWordNet : a multilingual database with lexical semantic networks. Kluwer
Academic.
</p>
<p>TALN-R&#201;CITAL 2013, 17-21 Juin, Les Sables d&#8217;Olonne
</p>
<p>89 c&#65535; ATALA</p>

</div></div>
</body></html>