TALN-RECITAL 2013, 17-21 Iuin, Les Sables d’Olonne

Identiﬁcation automatique des relations discursives
«imp1icites>> a partir de données annotées et de corpus bruts

Chloé Braudl Pascal Denisz
(1) ALPAGE, INRIA Paris-Rocquencourt 8: Université Paris Diderot
(2) MAGNET, INRIA Lille Nord-Europe
Chloe . braud@inria . fr , pascal . denis@inria. fr

RESUME
Cet article présente un systéme d’identiﬁcation des relations discursives dites << implicites » (£1
savoir, non explicitement marquées par un connecteur) pour le francais. Etant donné le faible
volume de données annotées disponibles, notre systeme s’appuie sur des données étiquetées
automatiquement en supprimant les connecteurs non ambigus pris comme annotation d’une
relation, une méthode introduite par (Marcu et Echihabi, 2002). Comme l’ont montré (Sporleder
et Lascarides, 2008) pour l’anglais, cette approche ne généralise pas trés bien aux exemples
de relations implicites tels qu’annotés par des humains. Nous arrivons au méme constat pour
le frangais et, partant du principe que le probléme vient d’une différence de distribution entre
les deux types de données, nous proposons une série de méthodes assez simples, inspirées
par l’adaptation de domaine, qui visent a combiner efficacement données annotées et données
artiﬁcielles. Nous évaluons empiriquement les différentes approches sur le corpus ANNODIS : nos
meilleurs résultats sont de l’ordre de 45.6% d’exactitude, avec un gain signiﬁcatif de 5.9% par
rapport a un systeme n’utilisant que les données annotées manuellement.

ABSTRACT
Automatically identifying implicit discourse relations using annotated data and raw cor-
pora

This paper presents a system for identifying << implicit » discourse relations (that is, relations that
are not marked by a discourse connective). Given the little amount of available annotated data for
this task, our system also resorts to additional automatically labeled data wherein unambiguous
connectives have been suppressed and used as relation labels, a method introduced by (Marcu et
Echihabi, 2002). As shown by (Sporleder et Lascarides, 2008) for English, this approach doesn’t
generalize well to implicit relations as annotated by humans. We show that the same conclusion
applies to French due to important distribution differences between the two types of data. In
consequence, we propose various simple methods, all inspired from work on domain adaptation,
with the aim of better combining annotated data and artiﬁcial data. We evaluate these methods
through various experiments carried out on the ANNODIS corpus : our best system reaches a
labeling accuracy of 45.6%, corresponding to a 5.9% signiﬁcant gain over a system solely trained
on manually labeled data.

MOTS-CLES : analyse du discours, relations implicites, apprentissage automatique.

KEYWORDS: discourse analysis, implicit relations, machine learning.

104 © ATALA

TALN-RECITAL 2013, 17-21 Iuin, Les Sables d’Olonne
1 Introduction

L’analyse discursive rend compte de la cohérence d’un texte en liant, par des relations discursives,
les propositions qui le constituent. En dépit de différences, les principales théories du discours,
telles que la Rhetorical Structure Theory (RST) (Mann et Thompson, 1988) et la Segmented
Discourse Representation Theory (SDRT) (Asher et Lascarides, 2003), s’accordent sur les étapes
d’analyse : segmentation en unités élémentaires de discours (EDU), attachement des EDU,
identiﬁcation des relations entre EDU, puis récursivement les paires attachées sont liées a des
segments simples ou complexes pour aboutir a une structure couvrant le document. Ainsi on
peut associer au discours 1.1 1 segmenté en trois EDU la structure entre accolades. Les deux
premiers segments sont liés par un contrast et le segment complexe ainsi constitué est argument
d’une relation de continuation. Un systeme dérivant automatiquement cette structure permettrait
d’améliorer d’autres systémes de TAL ou de RI car la structure du discours contraint les référents
des anaphores, révéle la structure thématique d’un texte et l’ordonnancement temporel des
événements : dans 1.2, les phrases a et b sont liées par une relation de type explanation, b
explique a, qui implique (loi de cause a effet) l’ordre des événements, b avant a.

Exemple 1.1 {{[La hulotte est un rapace nocturne] [mais elle peut vivre le jour.]}w,,m,st [La
hulotte mesure une quarantaine de centimetres] }w,,t,-,,,,,m-,,,,

Exemple 1.2 {[Juliette est tombe'e.]a [Marion l’a pousse’e.],,},,,,,,,,,,,,,t,-,,,,

Grace aux corpus annotés comme le PDTB 2 ou le RST DT 3 des systémes automatiques ont été
développés pour l’anglais sur la tache complete ou seulement les sous—té‘1ches (notamment la
phase d’identiﬁcation des relations). A partir du corpus RST D'I} (Sagae, 2009) et (Hernault
et al., 2010) ont développé des systémes complets avec des scores de f—mesure respecﬁfs de 44.5
et 47.3 donc des performances encore modestes. Sur le PDTB, (Lin et al., 2010) construit un
systéme complet obtenant 46.8 de f-mesure.

Le PDTB permet de séparer 1’étude des exemples avec ou sans connecteur discursif déclenchant
la relation. Lorsqu’un tel marqueur est présent, la relation est dite explicite (ou marquée ou
lexicalisée) : ainsi, mais lexicalise la relation de contrast dans 1.1. Sinon, elle est implicite,
comme la relation causale dans 1.2. Les différentes études menées sur le PDTB montrent que
l’identiﬁcau'on des relations implicites est considérablement plus difﬁcile que celle des relations
explicites. Ainsi, (Lin et al., 2010) obtiennent une f—mesure qui dépasse les 80 pour les explicites,
mais de seulement 39.63 pour les implicites. Sur un jeu de relations plus petit, (Pitler et Nenkova,
2009) rapportent une exactitude de 94% sur les explicites alors que (Pitler et al., 2009) de 60
sur les implicites. Sur des données tirées du RST D'I‘, avec 5 relations, (Sporleder et Lascarides,
2008) obtiennent des scores de l’ordre de 40% d’exactitude. Pour le francais, il n’existe pas de
corpus annoté en connecteur, donc aucune étude séparant le cas des implicites du cas général :
(Muller et al., 2012) ont développé un systéme complet qui obtient une exactitude de 44.8 pour
17 relations et de 65.5 pour ces relations regroupées en 4 classes (ANNODIS, 3143 exemples).
On peut supposer que, comme pour l’anglais, ce sont les relations implicites qui dégradent les
performances du systéme.

1. Tiré du corpus francais ANNODIS, (Péry-Woodley et al., 2009) document WK_-_hulotte.
2. Penn Discourse Treebank, (Prasad et al., 2008)
3. RST Discourse Treebank, (Carlson et aL, 2001)

105 © ATALA

TALN—RECITAL 2013, 17-21 Iuin, Les Sables d’Olonne

Avec MANW, l’exactitude augmente avec la croissance du coefﬁcient et dépasse 39.7 a partir du
coefﬁcient 400 équivalent a un corpus manuel d’environ 24000 exemples par relation soit du
meme ordre que le nombre d’exemples artiﬁciels.

Les experiences ou la prise en compte des données artiﬁcielles passe par l’ajout de traits donnent
les meilleurs résultats avec une exactitude de 42.9 pour le modéle qui intégre les prédictions
du modéle artiﬁciel comme descripteur (ADDPRED). Le second modéle, qui exploite en plus les
probabilités (ADDPROB) mene quant a lui a une légere diminution ce qui suggere que les traits de
probabilité dégradent les performances.

Quant aux expériences de combinaison des modéles, l’initialisation du modéle manuel par
l’artiﬁciel (AUTOINIT) conduit a un systéme d’exactitude 39.3, et l’interpolau'on linéaire (LININT)
correspond a une décroissance de l’exactitude suivant l’augmentation du coefﬁcient a sur le
modéle artiﬁciel (voir table 4), avec cependant un saut important entre a = 0.8 et a = 0.9
(exactitude de 28.2) en lien avec une forte dégradation de l’identiﬁcation de explanation.

Au niveau des scores par relation, ces systemes ont des effets diffe’rents. Une inﬂuence forte du
modéle artiﬁciel permet une amélioration importante pour contrast et une dégradation forte
pour result et explanation par rapport a MANONLY. Ces phénoménes sont visibles avec AUTOONLY
mais aussi avec LININT : la f—mesure de contrast augmente avec a tandis que celle de result et
de explanation diminue (voir table 4). Avec une inﬂuence similaire des deux types de données
(LININT a = 0.5 cu MANW coefﬁcient 400), la chute pour result est moins importante et on
améliore l’identiﬁcation de explanation (voir table 4). Pour continuation, il faut une inﬂuence des
données manuelles inférieure a celle des données artiﬁcielles pour observer une amélioration
(voir table 4, LININT a = 0.8). Le systéme ADDPRED permet notamment une amélioration forte de
la f—mesure de explanation. On n’obtient pas d’amélioration pour result.

Les méthodes de combinaison aboutissent a des systemes d’exacu'tude similaire voire supérieure a
MANONLY et a des améliorations pour l’identiﬁcation des relations sauf result. La relation contrast
proﬁte peut—étre de données artiﬁcielles moins bruitées : la majorité des exemples (plus de
75%) sont extraits a partir de mais, forme toujours en emploi discursif dont les arguments sont
dans l’ordre canonique, argumentl+connecteur+argument2. Pour explanation, la majorité des
données (77.5%) est extraite a partir de formes déclenchant la méta—relau'on explanation* qui ne
correspond a aucun exemple dans ANNODIS expliquant peut—étre le manque de généralisation
entre les deux types de données. Les prédictions du modele artiﬁciel construit surtout sur cette
méta-relation pourraient étre cohérentes expliquant l’amélioration observée. Les différences de
performance au niveau des labels peuvent venir de distribution plus ou moins proche entre les
deux types de donnée. Si on regarde la distribution en terme de traits (850 traits en tout), on
constate un écart de plus de 30% pour 2 et 5 traits pour result et explanation mais aucun pour
contrast et continuation pour lesquelles l’apport direct des données artiﬁcielles est positif.

5.3 Modéles avec sélection automatique d’exemples

Les expériences précédentes ont montré que l’ajout de données artiﬁcielles donnaient le plus
souvent lieu a des gains de performance, mais ces gains restent relativement modestes, voire non
signiﬁcatifs. Notre hypothése est que de nombreux exemples artiﬁciels amenent du bruit dans le
modele. Idéalement, nous souhaiterions étre capables de sélectionner les exemples artificiels les
plus informatifs et qui complémentent le mieux les données manuelles.

114 © ATALA

TALN-RECITAL 2013, 17-21 Iuin, Les Sables d’Olonne

La méthode de sélection d’exemples que nous proposons a pour objectif d’éliminer les exemples
potentiellement plus bruités. Pour cela, le modele artiﬁciel est utilisé sur les données d’en-
trainement et on conserve les exemples prédits avec une probabilité supérieure a un seuil
5 E [30, 40, 50, 55, 60, 65, 70, 75]. Si ce modéle est assez sﬁr de sa prédiction, on peut espérer que
l’exemple ne correspond pas a du bruit, a une forme en emploi non discursif et/ou une erreur
de segmentation. On vériﬁe en quelque sorte aussi l’hypothese de redondance du connecteur.
Pour chaque seuil, on rééquilibre les données en se basant sur la relation la moins représentée
(systeme+sELEc). A partir du seuil 80, ces expériences ne sont plus pertinentes, on conserve
moins de 10 exemples par relation. Les seuils les plus intéressants sont les seuils 60, 65, 70 et 75
qui représentent respectivement un ajout de 553, 205, 72 et 16 exemples par relation. Les scores
des systémes présentant les résultats les plus pertinents sont repris dans la table 5.

+SELEC MANONLY AUTOONLY UNION MANW ADDPRED ADDPROB AUTOINIT LININT
Seuil - 60 70 60 75 30 65 40 65 65 65 60 75
Paramétre - - - - - 250 0.5 900 - - - - 0.7 0.7
Exactitude 39.7 27.0 23.8 26.2 41.7 35.3 30.6 45.6 42.5 44.4 44.0 43.3 36.5 34.9
contrast 13.3 32.0 29.5 26.7 11.6 16.4 37.2 32.0 14.5 31.6 24.7 24.0 34.1 24.6
result 49.0 20.0 8.2 25.4 50.0 29.5 27.8 53.2 47.4 52.6 53.2 47.8 33.6 29.7
continuation 39.7 8.6 16.5 19.4 43.3 49.1 20.6 38.5 36.8 40.6 43.4 43.4 28.8 27.0
explanation 43.8 31.8 32.1 30.9 45.6 35.3 34.3 51.1 55.9 45.9 44.4 48.9 46.1 52.9

TABLE 5 — Modéles avec sélection d’exemples, exactitude du systéme et f—mesure par relation

La sélection automatique d’exemples permet d’améliorer les résultats précédents, qu’il s’agisse
du modéle AUTOONLY ou des modéles avec combinaison des données. De 23.0 d’exacu'tude avec
AUTOONLY, on passe a 27.0 avec AUTOONLY +SELEC au seuil 60. De méme on passe de 24.2 avec
UNION a 41.7 avec UNION -I-SELEC au seuil 75, l’exactitude augmentant avec la croissance du seuil.

Il semble que les meilleurs systemes soient obtenus entre les seuils 60 et 70. Au seuil 65, les
systémes AUTOINIT +SELEC, ADDPRED +SELEC et ADDPROB +SELEC atteignent leur meilleur score
(voir table 5), ce dernier améliorant significaﬁvement MANONLY (p-valeur= 0.046). L’exacu'tude
de ces systémes ne suit pas une évolution claire suivant le seuil. De méme, si on retrouve avec
LININT +SELEC une baisse de l’exactitude suivant a a chaque seuil, on n’a pas d’inﬂuence des
seuils sur l’exactitude aux valeurs extrémes de a.

Avec AUTOSUB -I-SELEC et MANW +SELEC on a la méme tendance qu’avant, l’exactitude respecti-
vement décroit et croit avec la croissance du coefﬁcient pour chaque seuil, mais pour AUTOSUB
-I-SELEC on n’a rapidement plus assez d’exemples artiﬁciels pour extraire des sous—ensembles. Pour
MANW -I-SELEC, l’exactitude avec le coefficient le plus bas augmente avec le seuil, de 22.6 (seuil
30) a 37.7 (seuil 75). C’est avec ce systeme et une inﬂuence tres faible des données artiﬁcielles
qu’on obtient le meilleur score d’exactitude, 45.6 améliorant signiﬁcativement les performances
de MANONLY (p-valeur= 0.021).

Au niveau des scores par relation, de nouveau une inﬂuence forte des données artiﬁcielles
améliore l’identiﬁcation de contrast avec en plus une inﬂuence positive d’un seuil haut mais
inférieur 2 70, au—del2‘1 le nombre d’exemples artificiels étant probablement trop bas pour inﬂuer
sur l’identiﬁcation (voir table 5). Parallélement, a part avec AUTOONLY -I-SELEC, l’identiﬁcation des
autres relations s’améliore avec la croissance du seuil donc une baisse de l’inﬂuence du modele
artiﬁciel. Pour continuation on observe toujours une amélioration pour une inﬂuence similaire

115 © ATALA

TALN-RECITAL 2013, 17-21 Iuin, Les Sables d’Olonne

des deux types de données et pour explanation, c’est toujours l’ajout de traits de prédictions qui
permet les meilleures performances. I1 semble qu’en plus on améliore ici l’identiﬁcation de result
(MANW -l-SELEC et ADDPROB -l-SELEC, table 5).

La sélection des exemples améliore l’identiﬁcation des relations et conduit a deux systemes
améliorant signiﬁcativement l’exactitude de MANONLY montrant que les données artiﬁcielles
lorsqu’intégrées de facon adéquate peuvent améliorer l’identiﬁcation des relations implicites,
notamment lorsque leur inﬂuence est faible, le modéle étant guidé vers la bonne distribution.

Ala constitution des corpus avec sélection on observe qu’avec la croissance du seuil on conserve
toujours plus d’exemples pour result, des le seuil 40 environ 3900 de plus, alors que contrast
devient sous-re résenté. Cette observation montre ue le bruit n’est robablement as la seule
P q P P
facon d’expliquer les résultats puisque la relation améliorée par les données artiﬁcielles est celle
pour laquelle le modéle artiﬁciel est le moins conﬁant alors que celle dont les résultats sont les
plus dégradés est celle pour laquelle il est le plus conﬁant.

6 Conclusion

Nous avons développé la premiere série de systemes d’identiﬁcation des relations discursives
implicites pour le francais. Ces relations sont difﬁciles a identiﬁer en raison du manque d’indices
forts. Dans les études sur l’anglais, les performances sont basses malgré les indices complexes
utilisés, probablement par manque de données. Pour pallier ce probleme, plus crucial encore
en francais, nous avons utilisé des données annotées automatiquement en relation a partir
d’exemples explicites. Mais ces nouvelles données ne généralisent pas bien aux données impli-
cites car elles sont de distribution différente. Nous avons donc testé des méthodes inspirées
de l’adaptation de domaine pour combiner ces données en ajoutant une étape de sélection
automatique des exemples artiﬁciels pour gérer le bruit induit par leur création. Elles nous
permettent des améliorations signiﬁcatives par rapport au modele n’utilisant que les données
manuelles. Les meilleurs systémes utilisent la sélection d’exemples et la pondération des données
manuelles ou l’ajout de traits de prédictions du modéle artiﬁciel.

Si les méthodes de combinaison et de sélection simples utilisées ici parviennent a des résultats
encourageants, on peut espérer que des méthodes plus sophistiquées pourraient conduire a des
améliorations plus importantes. De plus, une étude des données explicites pourrait permettre
d’augmenter la taille du corpus artiﬁciel et d’améliorer sa qualité en sélectionnant des connecteurs
et en identiﬁant des relations pour lesquelles cette méthode est plus ou moins efﬁcace et des
traits plus informatifs dans une optique de combinaison des données. Il faudra enﬁn porter ces
méthodes sur les données anglaises pour une comparaison avec d’autres études.

Références

ASHER, N. et LASCARIDES, A. (2003). Logics of conversation. Cambridge University Press.

BERGER, A. L., PIETRA, V J. D. et PIETRA, S. A. D. (1996). A maximum entropy approach to
natural language processing. Computational linguistics, 22(1):39—71.
116 © ATALA

TALN—RECITAL 2013, 17-21 Iuin, Les Sables d’Olonne
BLAIR—GoLDENsoHN, S., McKEowN, K. R. et RAMBow, O. C. (2007). Building and reﬁning
rhetorical—semantic relation models. In Proceedings of NAACL HLT, page 428-435.

CANDITO, M., NIVRE, J., DENIs, P. et ANGUIANO, E. H. (2010). Benchmarking of statistical
dependency parsers for french. In Proceedings of the 23rd ICCL posters, page 108-116.

CARLsoN, L., MARcU, D. et OKUROWSKI, M. E. (2001). Building a discourse—tagged corpus in
the framework of rhetorical structure theory. In Proceedings of the Second SIGdial Workshop on
Discourse and Dialogue—Volume 16, page 1-10.

DAUME III, H. (2007). Frustratingly easy domain adaptation. In Proceedings of ACL, page 256.

DAUME III, H. et MARCU, D. (2006). Domain adaptation for statistical classiﬁers. Journal of
Artificial Intelligence Research, 26(1):101—126.

DENIs, R et SAGOT, B. (2009). Coupling an annotated corpus and a morphosyntactic lexicon for
state-of—the—art POS tagging with less human effort. In Proceedings of PACLIC.

HERNAULT, H., PRENDINGER, H. et IsHIzUKA, M. (2010). HILDA : a discourse parser using support
vector machine classiﬁcation. Dialogue & Discourse, 1 (3).

LIN, Z., NG, H. T. et KAN, M. Y. (2010). A PDTB—styled end—to—end discourse parser. Rapport
technique, National University of Singapore.

MANN, W. C. et THOMPSON, S. A. (1988). Rhetorical structure theory : Toward a functional
theory of text organization. Text, 8(3):243—281.

MARCU, D. et ECHIHABI, A. (2002). An unsupervised approach to recognizing discourse relations.
In Proceedings of ACL, page 368-375.

MULLER, R, AEANTENos, S., DENIs, P. et AsHER, N. (2012). Constrained decoding for text—level
discourse parsing. In Proceedings of COLING, pages 1883-1900.

PITLER, E., LoUIs, A. et NENKovA, A. (2009). Automatic sense prediction for implicit discourse
relations in text. In Proceedings of ACL-IJCNLP, page 683-691.

PITLER, E. et NENKOVA, A. (2009). Using syntax to disambiguate explicit discourse connectives
in text. In Proceedings of ACL—IJCNLP short papers, page 13-16.

PRAsAD, R., DINEsH, N., LEE, A., MILTSAKAKI, E., ROBALDO, L., JosHI, A. et WEBBER, B. (2008).
The penn discourse treebank 2.0. In Proceedings of LREC, page 2961.

PERY—WooDLEY, M. R, AsHER, N., ENJALBERT, R, BENAMARA, F., BRAs, M., FABRE, C., FERRARI, S.,
Ho—DAc, L. M., LE DRAoULEc, A. et MATHET, Y. (2009). ANNODIS : une approche outillée de
l’annotation de structures discursives. Actes de TALN 2009.

RozE, C. (2009). Base lexicale des connecteurs discursifs du francais. Mémoire de D.E.A.,
Université Paris Diderot.

SAGAE, K. (2009). Analysis of discourse structure with syntactic dependencies and data—driven
shift—reduce parsing. In Proceedings of IWPT, page 81-84.

SoRIA, C. et FERRARI, G. (1998). Lexical marking of discourse relations—some experimental
ﬁndings. In Proceedings of the ACL—98 Workshop on Discourse Relations and Discourse Markers.

SPORLEDER, C. et LASCARIDES, A. (2007). Exploiting linguistic cues to classify rhetorical relations.
Amsterdam Studies In The Theory And History Of Linguistic Science Series 4, 2922157.

SPORLEDER, C. et LASCARIDES, A. (2008). Using automatically labelled examples to classify
rhetorical relations : An assessment. Natural Language Engineering, 14(3):369—416.

117 © ATALA

TALN—RECITAL 2013, 17-21 Iuin, Les Sables d’Olonne

Malheureusement, les corpus discursifs disponibles sont encore tres petits (surtout pour le fran-
cais). En vue de pallier le manque d’annotations humaines, (Marcu et Echihabi, 2002) proposent
d’uti1iser des exemples annotés automatiquement grace aux connecteurs comme données impli-
cites supplémentaires. Cette étude et celles qui l’ont suivie, notamment (Sporleder et Lascarides,
2008), utilisaient ces nouvelles données artiﬁcielles comme seules données d’entrainement et
obtenaient de basses performances. Le probleme repose sur une différence de distribution entre
les deux types de données, qu’il est possible de prendre en compte aﬁn d’améliorer l’identiﬁcation
des relations implicites. A cette ﬁn, nous proposons et évaluons différentes méthodes visant
a créer un nouveau modéle enrichi par les nouvelles données mais guidé vers la distribution
des données manuelles. Nous nous inspirons des méthodes utilisées en adaptation de domaine
décrites dans (Daumé III, 2007). Notre contribution se situe au niveau du développement d’un
systéme d’identiﬁcation des relations discursives implicites pour le francais et de l’étude de
stratégies d’uti1isau'on de données de distributions différentes en TAL.

Nous présentons dans la partie suivante un rapide état de l’art sur les expériences déja menées sur
l’identiﬁcation des relations de discours avec données artiﬁcielles, aﬁn d’en montrer les limites et
de proposer une nouvelle stratégie. La section 3 est consacrée aux données et la section 4 au
modéle utilisé. La section 5 regroupe les expériences menées et l’analyse des résultats. Enﬁn,
nous ﬁnirons par les perspectives ouvertes par ces expériences dans la section 6.

2 Utilisation des données générées automatiquement

Les obstacles associés a l’identiﬁcation des relations implicites résident, d’une part, dans l’absence
d’indicateur ﬁable (comme le connecteur pour les relations explicites) et, d’autre part, dans
le manque de données pour entrainer des classiﬁeurs performants. Néanmoins, on dispose de
données quasiment annotées en grande quantité : celles contenant un connecteur discursif non
ambigu, c’est—a—dire ne déclenchant qu’une seule relation (p.ex., parce que déclenche nécessai—
rement une relation de type explanation). Ce constat a amené (Marcu et Echihabi, 2002) a
proposer d’uti1iser ces exemples pour l’identiﬁcation des implicites. Plus précisément, on génere
de nouvelles données annotées a partir d’un corpus brut : des exemples sont extraits sur la
présence d’une forme de connecteur discursif non ambigu, ﬁltrés pour éliminer les cas d’emploi
non discursif de la forme, puis le connecteur est supprimé pour empécher le modéle de se
baser sur cet indice non ambigu. On crée ainsi des données implicites annotées en relation de
discours mais des données qui n’ont jamais été produites, non naturelles d’o1‘1 le terme de données
artiﬁcielles. A titre d’illustration, considérons la paire de phrases suivante tirée du corpus Est
Re’pub1icain (2.1) : dans ce cas, le connecteur cela dit est supprimé et on génere un exemple de
relation de contrast entre les deux syntagmes arguments a et b.

Exemple 2.1 [Elle e’tait tres comique, trés dr6le.]a Cela_dit [,le drame n’ étaitjamais loin.],,

L’idée est ﬁnalement de s’appuyer sur ces données artiﬁcielles pour construire un modele d’iden-
tiﬁcation des relations pour des données naturelles implicites, on a donc des données de type
différent : implicites versus explicites et naturelles versus artiﬁcielles.

Dans les études précédentes basées sur ce principe les données artiﬁcielles sont utilisées comme
seules données d’entrainement ce qui conduit a des performances basses, juste au—dessus de la

106 © ATALA

TALN-RECITAL 2013, 17-21 Iuin, Les Sables d’Olonne

chance pour (Sporleder et Lascarides, 2008) avec 25.8% d’exactitude contre 40.3 en utilisant
les seules données manuelles (1051 exemples manuels, 72000 artiﬁciels, 5 relations). (Blair-
Goldensohn et al., 2007) cherchent a tester l’impact de la qualité du corpus artiﬁciel en améliorant
l’extraction des données grace a une segmentation en topics ou des informations syntaxiques. Ils
semblent améliorer légérement les performances mais une comparaison est difﬁcile puisqu’ils ne
testent que des classiﬁeurs binaires et 2 relations. L’idée de base de (Marcu et Echihabi, 2002)
résidait dans l’extraction de paires de mots de type antonymes ou hypéronymes pouvant révéler
une relation mais dont le lien n’est pas forcément recensé dans des ressources comme WordNet.
(Pitler et al., 2009) montrent que l’utilisation des paires de mots extraites d’un corpus artiﬁciel
comme trait supplémentaire n’améliore pas les performances d’un systéme d’identiﬁcation des
relations implicites. Mais l’étude de (Sporleder et Lascarides, 2008) utilisant d’autres types de
traits indique que le probleme ne réside pas ou pas uniquement dans le choix des traits.

Ces résultats montrent qu’un modéle entrainé sur les données artiﬁcielles ne généralise pas bien
aux données manuelles. Pourtant en regardant des exemples de type artiﬁciel, il semble que
dans certains cas on aurait pu produire les arguments sans le connecteur. De plus, les résultats
de (Sporleder et Lascarides, 2008) demeurent supérieurs a la chance (en considerant la chance
a 20%), donc ces données ne sont pas complétement différentes des données de test. Nous
cherchons ici a prendre en compte cette différence de distribution qui rapproche le probléme de
ceux traités en adaptation de domaine.

2.1 Probléme : différence de distribution entre les données

Pour que cette stratégie fonctionne, il faut nécessairement faire l’hypothése d’une certaine
redondance du connecteur par rapport a son contexte : il doit rester sufﬁsamment d’information
apres sa suppression pour que la relation reste identiﬁable. Une étude psycho—linguistique menée
sur l’italien (Soria et Ferrari, 1998) et les conclusions de (Sporleder et Lascarides, 2008) semblent
montrer que c’est le cas dans une partie des données. Cette étude reste a faire pour le francais, et
l’approfondir pourrait permettre d’améliorer la qualité du corpus artiﬁciel en déterminant par
exemple si cette redondance est différente selon les relations et les connecteurs.

Plus généralement, en apprentissage on fair l’hypothése que données d’entrainement et de
test sont identiquement et indépendamment distribuées (données i.i.d.). Or il nous semble
que justement la stratégie proposée par (Marcu et Echihabi, 2002) pose le probléme d’un
apprentissage avec des données non identiquement distribuées. On a deux ensembles de données
qui se ressemblent (méme ensemble d’étiquettes, les exemples sont des segments de texte) mais
qui sont néanmoins distribués différemment, et ce, pour deux raisons au moins. D’une part, les
données artiﬁcielles sont par déﬁnition obtenues a partir d’exemples de relations explicites : il n’y
a aucune garantie que ces données soient distribuées comme les “vrais” exemples implicites. La
différence porte tant sur la distribution des labels (des relations) que sur l’assocation entre labels
(relations) et inputs (paires des segments) a classer. En outre, la suppression du connecteur
ajoute probablement une forme de bruit en cas d’erreur d’étiquetage contrairement aux données
manuelles correctement étiquetées.

D’autre part, les données artiﬁcielles se distinguent aussi des données manuelles en termes des
segments. Ainsi, la segmentation des premieres est basée sur des heuristiques (p.ex., les argu-
ments ne peuvent étre que deux phrases adjacentes ou deux propositions couvrant une phrase).
Dans les données manuelles, en revanche, on a des arguments contigus ou non, propositionnels,

107 © ATALA

TALN-RECITAL 2013, 17-21 Iuin, Les Sables d’Olonne

phrastiques ou multi-phrastiques dont les frontiéres ont été déterminées par des annotateurs
humains. Ceci induit une différence de distribution au niveau des objets a classer et une forme
de bruit en cas d’erreur de segmentation due a ces hypotheses simpliﬁcatrices ou a une erreur
d’heurisu'que.

On peut se rendre compte de cette différence de distribution sur l’association entre labels et
inputs en considérant certaines caractéristiques des données. La répartition entre exemples
inter-phrastiques et intra—phrastiques (la relation s’établit entre deux phrases ou deux segments a
l’intérieur d’une phrase) est ainsi similaire pour contrast (57.1% d’inter—phrastiques dans les deux
types de données), proche pour result (45.7% d’inter—phrastiques dans les données manuelles,
39.8% dans les artiﬁcielles) mais trés différente pour continuation (70.0% d’inter—phrastiques
dans les manuelles, 96.5% dans les artiﬁcielles), et pour explanation (21.4% dans les manuelles,
53.0% dans les artiﬁcielles).

Ne pas prendre en compte ces différences de distribution conduit 2‘: de basses performances, nous
avons donc cherché a les gérer en testant différentes stratégies avec un point commun : chercher
a guider le modéle vers la distribution des données manuelles.

2.2 Modéles testés

Dans des études précédentes, l’entrainement sur les seules données artiﬁcielles aboutit a des
résultats inférieurs a un entrainement sur des données manuelles (pourtant bien moins nom-
breuses). Ceci s’exp1ique par les différences de distribution entre les deux ensembles de données.
Dans cette section, nous décrivons différentes méthodes visant a exploiter les nouvelles données
artiﬁcielles, non plus seules, mais en combinaison avec les données manuelles existantes.

De nombreux travaux s’attachant au probléme de données non identiquement distribuées
concernent l’adaptation de domaine. Nous nous sommes donc inspirés des méthodes utilisées
dans ce cadre, méme si notre probleme differe au sens ou nous n’aVons qu’un seul domaine et
des données bruitées. Ainsi, nous avons testé une série de systémes utilisés pour 1’adapation de
domaine par (Daumé III, 2007), qui sont tres simples a mettre en oeuvre et obtiennent néanmoins
de bonnes performances sur différentes taches, ainsi que quelques solutions dérivées. Dans un
second temps, nous avons ajouté une étape de sélection d’exemples, aﬁn de choisir parmi les
exemples artiﬁciels ceux qui seraient susceptibles d’amé1iorer les performances.

Les différentes méthodes de combinaison que nous proposons different selon que la combinaison
s’opére directement au niveau des jeux de données ou au niveau des modéles entrainés sur
ceux—ci. La premiere stratégie de combinaison de données que nous étudions (UNION) reléve du
premier type : elle consiste a créer un corpus d’entrainement qui contient la réunion des deux
ensembles de données. Une stratégie dérivée (AUTOSUB) consiste a prendre, non pas 1’intégra1ité
des données artiﬁcielles, mais des sous-ensembles aléatoires de ces données, en addition des
données manuelles. Cette méthode est un peu plus subtile dans la mesure ou on peut faire varier
la proportion des exemples artiﬁciels par rapport aux exemples manuels. Enﬁn, la troisieme
méthode du premier type (MANW) garde cette fois la totalité des données artiﬁcielles mais
pondére (ou duplique) les exemples manuels de maniére a éviter un déséquilibre trop grand au
proﬁt des données artiﬁcielles.

Dans le second type de méthodes, on trouve tout d’abord une méthode (ADDPRED) qui consiste a
utiliser les prédictions d’un modéle entrainé sur les données artiﬁcielles (a savoir les données

108 © ATALA

TALN-RECITAL 2013, 17-21 Iuin, Les Sables d’Olonne

“source”) comme descripteur dans le modéle entrainé sur les données manuelles (a savoir les
données “cibles”). Le paramétre associé a ce descripteur mesure donc l’importance a accorder
aux prédictions du modele entrainé sur les données artiﬁcielles. Cette méthode est la meilleure
baseline et le troisiéme meilleur modele dans (Daumé III et Marcu, 2006). Une variation de
cette méthode (ADDPROB) utilise en plus le score de conﬁance (p.ex., la probabilité) du modele
artiﬁciel comme descripteur supplémentaire dans le modéle manuel. Une troisiéme méthode
(AUTOINIT) vise a initialiser les paramétres du modéle entrainé sur les données manuelles avec
ceux du modele utilisant les données artiﬁcielles. Enﬁn, la derniére méthode (LININT) se base sur
une interpolation linéaire de deux modéles préalablement entrainés sur chacun des ensembles
de données.

Nous avons aussi testé toutes ces stratégies en ajoutant une étape de sélection automatique
d’exemples artiﬁciels. La méthode utilisée est na'ive puisqu’elle se base simplement sur la proba-
bilité du label prédit : on teste différents seuils sur ces probabilités en ajoutant a chaque fois les
seuls exemples prédits avec une probabilité supérieure au seuil. Cette sélection vise a écarter
des données bruitées, en explorant ﬁnalement l’une des voies proposées par (Marcu et Echihabi,
2002) et développée d’une autre maniére par (Blair-Goldensohn et al., 2007), a savoir améliorer
la qualité du corpus artiﬁciel.

Les performances de tous ces systemes seront comparées a celles des systemes entrainés séparé—
ment sur les deux ensembles de données dans la section 5.

3 Données

Nous avons choisi de nous restreindre a 4 relations : contrast, result, continuation et explanation.
Ces relations sont annotées dans le corpus francais utilisé et correspondent a des exemples
implicites et explicites. De plus ce sont 4 des 5 relations (summary n’est pas annotée dans
ANNoDIs) utilisées dans (Sporleder et Lascarides, 2008), ce qui nous permet une comparaison
mais non directe puisque la langue et le corpus sont différents. Dans nos données manuelles, nous
avons fusionné les méta—relations avec les relations correspondantes avec l’hypothése qu’elles
mettaient en jeu le méme genre d’indices et de constructions. Les données manuelles permettent
d’obtenir des exemples de relations implicites manuellement annotés. Les données générées
automatiquement sont des exemples explicites extraits par heuristique de données brutes dans
lesquels on supprime le connecteur : des données implicites artiﬁcielles.

3.1 Le corpus ANNODIS

Le projet ANNODIS (Péry—Woodley et al., 2009) vise la construction d’un corpus annoté en discours
pour le francais suivant le cadre SDR'I‘. La version du corpus utilisée (en date du 15/11/2012)
comporte 86 documents provenant de l’Est Républicain et de Wikipedia. 3339 exemples sont
annotés avec 17 relations rhétoriques. Les documents sont segmentés en EDU : propositions,
syntagmes prépositionnels, adverbiaux détachés a gauche et incises, si le segment contient la
description d’une éventualité. Les relations sont annotées entre EDU ou segments complexes,
contigués ou non. Les connecteurs discursifs ne sont pas annotés.

Le corpus a subi une série de pré-traitements. Le MElt tagger (Denis et Sagot, 2009) fournjt un
109 © ATALA

TALN-RECITAL 2013, 17-21 Iuin, Les Sables d’Olonne

étiquetage en catégorie morpho—syntaxique, une lemmatisation, des indications morphologiques
(temps, personne, genre, nombre). Le MSTParser (Candito et al., 2010) fournit une analyse en
dépendances. Aﬁn de restreindre notre étude aux relations implicites, nous utilisons le LexConn,
lexique des connecteurs discursifs du francais développé par (Roze, 2009) et étendu en 2012
aux connecteurs introduisant des syntagmes nominaux. Nous utilisons une méthode simple :
nous projetons le lexique (sauf la forme c‘1 jugée trop ambigue) sur les données, ce qui nous
permet d’identiﬁer tout token correspondant a un connecteur. Nous ne contraignons pas cette
identiﬁcation sur des critéres de position. Cette méthode nous assure d’identiﬁer tout connecteur
donc de ne récupérer que des exemples implicites mais comporte le risque d’en perdre certains.
Sur les 1108 exemples disponibles pour les 4 relations nous disposons de 494 exemples implicites ;
la distribution des exemples par relation est résumée dans le tableau 1.

Relation Exemples explicites Exemples implicites Total
contrast 100 42 142
result 52 1 10 162
cont inuat ion 404 272 676
explanati on 58 70 128
all 614 494 1 108

TABLE 1 — Co us ANNODIS : nombre d’exem les ex licites et im licites ar relation
1’P P P P P

3.2 Le corpus généré automatiquement

Nous avons utilisé 100 connecteurs du LexConn de (Roze, 2009) pour identiﬁer des formes de
connecteur ne pouvant déclencher qu’une relation parmi les 4 choisies dans le corpus compose’
d’articles de l’Est Républicain (9M de phrases), avec les mémes traitements que pour ANNODIS.
Les exemples sont ensuite ﬁltrés pour éliminer les emplois non discursifs en tenant compte de la
position du connecteur et de la ponctuation et en s’aidant des indications de LexConn. L’identiﬁ-
cation des arguments d’un connecteur est une simpliﬁcation du probléme de segmentation. Nous
faisons les mémes hypotheses simpliﬁcatrices que dans les études précédentes : les arguments
sont adjacents et couvrent au plus une phrase, au plus 2 EDU par phrase.

Cette méthode simple permet de générer rapidement de gros volumes de données : au total, nous
avons pu extraire 392260 exemples (voir tableau 2). Lorsque deux connecteurs sont présents
dans un segment, il peut arriver que l’un modiﬁe l’autre (par exemple << mais parce qu’il est... >>).
Dans ce cas, nous risquons de récupérer les mémes arguments pour deux formes déclenchant des
relations différentes ce qui est problématique pour un systéme de classiﬁcation. Nous ne générons
donc deux exemples quand deux connecteurs sont présents qu’a condition que les arguments
soient différents, l’un inter—phrastique, l’autre intra—phrastique. Nous avons équilibré le corpus en
relation en conservant le maximum d’exemples disponibles en un corpus d’entrainement (80%
des données), un de développement (10%) et un de test (10%).

Notons quelques différences importantes de distribution entre les données manuelles et artiﬁ-
cielles : continuation la plus représentée dans les manuelles devient la moins représentée dans
les artiﬁcielles. Ceci est dﬁ a la forte ambiguité des connecteurs de cette relation qui nous ont
forcé a déﬁnir des motifs stricts pour l’extraction des exemples. Notons ﬁnalement que cette
méthode génére du bruit : sur 250 exemples choisis aléatoirement, on trouve 37 erreurs de
frontiére d’arguments et 18 d’emplois non discursifs.

110 © ATALA

TALN-RECITAL 2013, 17-21 Iuin, Les Sables d’Olonne

Relation Disponible Entrainement Développement Test Total
contrast 252 793 23 409 2 926 2 926 29 261
result 50 297 23 409 2 926 2 926 29 261
continuation 29 261 23 409 2 926 2 926 29 261
explanation 59 909 23 409 2 926 2 926 29 261
all 392 260 93 636 11 704 11 704 117 044

TABLE 2 — Corpus artiﬁciel : nombre d’exemples par relation

4 Modéle et jeu de traits

Pour cette étude, nous avons employé un modéle de classiﬁcation discriminant par régression
logistique (on maximum d’entropie). Ce choix est basé sur le fait que ce type de modéles donne de
bonnes performances pour différents problémes de TAL et a été implanté dans différentes librairies
librement disponibles. Le principe de cet algorithme est d’apprendre un jeu de paramétres qui
maximise la log—vraisemblance des données fournies a l’apprentissage (voir (Berger et al., 1996)).
Un attrait important de ces modéles, par rapport a des modéles génératifs, est de permettre l’ajout
de nombreux descripteurs potentiellement redondants sans faire d’hypotheses d’indépendance.

Notre jeu de traits se base sur les travaux existants avec quelques adaptations notables pour le
francais. Ces traits exploitent des informations de surface, ainsi que d’autres issues d’un traitement
linguistique plus profond. Par comparaison, (Marcu et Echihabi, 2002) ne se base que sur la
co—occurrence de lemmes dans les segments. (Sporleder et Lascarides, 2007) montrent que la
prise en compte de différents types de traits linguistiquement motivés améliore les performances.
(Sporleder et Lascarides, 2008) utilisent des traits variés dont des bi—grammes de lemmes mais
sans traits syntaxiques. Nous avons testé des traits lexico—syntaxiques utilisés dans les précédentes
études sur cette tache. Nous n’aVons pas pu reprendre les traits sémantiques comme les classes
sémantiques des tétes des arguments car les ressources nécessaires n’existent pas pour le francais.
On utilise une version binaire des traits a valeur nominale.

Certains traits sont calculés pour chaque argument :

1. Indice de complexité syntaxique : nombre de syntagmes nominaux, verbaux, préposition—
nels, adjectivaux, adverbiaux (valeur continue)

2. Information sur la téte d’un argument :

— Lemme d’é1éments négatifs sur la téte comme “pas” (nominale)

— Information temporelle/aspectuelle : nombre de fois o1‘1 un lemme de fonction auxiliaire
dépendant de la téte apparait (continue), temps, personne, nombre de l’auxiliaire
(nominale)

— Informations sur les dépendants de la téte : présence d’un objet, par—objet (syntagme
prépositionnel introduit par “par”), modiﬁeur ou dépendant prépositionnel de la téte,
du sujet ou de l’objet (booléen) ; catégorie morpho—syntaxique des modiﬁeurs et des
dépendants prépositionnels de la téte, du sujet ou de l’objet (nominale)

— Informations morphologiques : temps et personne de la téte verbale, genre de la téte non
verbale, nombre de la téte, catégorie morpho—syntaxique précise (par exemple “VPP”) et
simpliﬁée (respectivement “V”) (nominale)

D’autres traits portent sur la paire d’arguments :
111 © ATALA

TALN-RECITAL 2013, 17-21 Iuin, Les Sables d’Olonne

1. Trait de position : si l’exemple est inter ou intra-phrastique (booléen)

2. Indice de continuité thématique : chevauchement en lemmes et en lemmes de catégorie
ouverte, comme nom, verbe etc... (continue)

3. Information sur les tétes des arguments :
— Paire des temps des tétes verbales (booléen)
— Paire des nombres des tétes (booléen)

On notera ﬁnalement que notre but portant avant tout sur la combinaison de données, nous
n’avons pas cherché a optimiser ce jeu de traits, ce qui aurait introduit un paramétre supplémen—
taire dans notre modele.

5 Expériences

Pour rappel, l’objecu'f central de ces expériences est de déterminer dans quelle mesure l’ajout de
données artiﬁcielles, via les différentes méthodes présentées en Section 2, peut nous permettre
de dépasser les performances obtenues en s’entrainant sur des données manuelles présentes
seulement en faible quantité.

Les expériences sont réalisées avec l’implémentation de l’algorithme par maximum d’entropie
fourni dans la librairie MegaM4 en version multi-classe avec au maximum 100 itérations. On
effectue une validation croisée en 10 sous-ensembles sur un corpus des données manuelles
équilibré a 70 exemples maximum par relation. Il faudra envisager des expériences conservant la
distribution naturelle des données, trés déséquilibrée, mais pour l’instant nous nous focalisons
sur l’aspect combinaison des données. Comme dans les études précédentes, les performances
sont données en termes d’exactitude globale sur l’ensemble des relations, des scores ventilés de
F1 par relation sont également fournis. La signiﬁcativité statistique des écarts de performance est
évaluée avec un Wilcoxon signed—rank test (avec une p—valeur < 0.05).

5.1 Modéles de base

Dans un premier temps, nous construisons deux modeles distincts, l’un a partir des seules données
manuelles (MANONLY, 252 exemples), l’autre des seules données artiﬁcielles (AUTOONLY, 93636
exemples d’entrainement). Notre modéle MANONLY obtient une exactitude de 39.7, avec des
scores de f-mesure par relation compris entre 13.3 pour contrast et 49.0 pour result (voir table 3).
La relation contrast est donc tres mal identiﬁée peut—étre parce que sous—représentée, seulement
42 exemples contre 70 pour les autres relations, le manque de données joue probablement ici un
role important.

Le modele AUTOONLY obtient une exactitude de 47.8 lorsqu’évalué sur le méme type de données
(11704 exemples de test), mais de 23.0 lorsqu’évalué sur les données manuelles (voir table 3).
Cette baisse importante est comparable a celle observée dans les études précédentes sur l’anglais.
Elle s’explique par les différences de distribution étudiées en Section 2. De maniére générale,
on observe des dégradations par rapport a MANONLY pour l’identiﬁcation de result, explanation
et continuation (voir table 3). Par contre l’identiﬁcation de contrast présente une amélioration,
obtenant 23.2 de f-mesure avec 11 exemples correctement identifiés contre 5 précédemment.

4. http : //www . umiacs . umd . edu/"hal/megam/version0_3/

112 © ATALA

TALN-RECITAL 2013, 17-21 Iuin, Les Sables d’Olonne

MANONLY AUTOONLY
Données de test Manuelles Manuelles Artiﬂcielles
Exactitude 39.7 23.0 47.8
contrast 13.3 23.2 38.3
result 49.0 15.7 57.4
continuation 39.7 32.1 54.3
explanation 43.8 22.4 37.5

TABLE 3 — Modeles de base, exactitude du systeme et f-mesure par relation

5.2 Modéles avec combinaisons de données

Dans cette section, nous présentons les résultats des systemes qui exploitent a la fois les don-
nées manuelles et les données artiﬁcielles. Ces ensembles de données sont ou bien combinés
directement ou bien donnent lieu a des modeles séparés qui sont combinés plus tard.

Certains de ces modéles utilisent des hyper—paramétres. Ainsi, pour la pondération des exemples
manuels nous testons différents coefﬁcients de pondération c avec c E [0.5;2000] avec un
incrément de 10 jusqu’a 100, de 50 jusqu’a 1000 et de 500 jusqu’a 2000. Pour l’ajout de sous-
ensembles des données artiﬁcielles, on ajoute a chaque fois k exemples parmi ces données
avec k E [0.1; 600] avec un incrément de 10 jusqu’a 100 et de 50 jusqu’a 600. Enﬁn, pour
l’interpolation linéaire des modeles, on construit un nouveau modele en pondérant le modele
artiﬁciel avec a E [0.1; 0.9] avec des incréments de 0.1.

De maniere générale, l’ensemble de ces systemes avec les bons hyper-parametres conduit a des
résultats au moins équivalents et parfois supérieurs en exactitude par rapport a MANONLY. Si
la tendance générale est donc plut6t d’une hausse des performances, aucune des différences
observées a ce stade ne semble cependant étre statistiquement signiﬁcative. Les scores des
systemes présentant les résultats les plus pertinents sont repris dans la table 4.

MANONLY AUTOONLY UNION MANW AUTOSUE ADDPRED ADDPROB AUTOINIT LININT

Paramétre - - - 100 400 0.2 - - - 0.2 0.5 0.8

Exactitude 39.7 23.0 24.2 34.9 41.7 39.7 42.9 39.3 39.3 39.7 38.5 35.3
contrast 13.3 23.2 21.1 32.4 19.7 16.7 22.9 24.0 11.9 13.2 16.2 29.6
result 49.0 15.7 16.4 28.0 39.2 44.9 47.4 45.8 46.3 47.0 39.5 24.8
continuation 39.7 32.1 38.5 45.8 48.7 39.4 37.3 35.4 38.9 40.6 39.2 44.2
explanation 43.8 22.4 21.7 31.7 47.7 46.1 52.8 43.8 45.4 45 .4 48.6 40.3

TABLE 4 — Modeles sans sélection d’exemples, exactitude du systeme et f—mesure par relation

La seule conﬁguration qui méne a des résultats négatifs est l’union simple des corpus d’entraine—
ment (UNION). Ce systeme obtient 24.2 d’exactitude donc de l’ordre d’un entrainement sur les
seules données artiﬁcielles. Ces résultats ne sont pas surprenants, les données manuelles environ
372 fois moins nombreuses que les artiﬁcielles se retrouvent noyées dans les données artiﬁcielles.

Les expériences de combinaison des données, ajout de sous—ensembles aléatoires des données
artiﬁcielles (AUTOSUB) et pondération des exemples manuels (MANW), ont des tendances inverses.
Avec AUTOSUB, l’exactitude diminue lorsque le coefﬁcient augmente et atteint ou dépasse le
modéle manuel avec les coefﬁcients 0.1 et 0.2, donc une inﬂuence trés faible du modele artiﬁciel.

113 © ATALA

