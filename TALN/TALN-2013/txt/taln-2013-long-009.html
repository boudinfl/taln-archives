<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
"http://www.w3.org/TR/html4/loose.dtd">
<html><head><title>Apprentissage d&#8217;une hi&#233;rarchie de mod&#232;les &#224; paires sp&#233;cialis&#233;s pour la r&#233;solution de la cor&#233;f&#233;rence</title>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
</head>
<body>
<div style="page-break-before:always; page-break-after:always"><div><p>Apprentissage d&#8217;une hi&#233;rarchie de mod&#232;les &#224; paires
sp&#233;cialis&#233;s pour la r&#233;solution de la cor&#233;f&#233;rence
</p>
<p>Emmanuel Lassalle1 Pascal Denis2
(1) Alpage : INRIA - Universit&#233; Paris Diderot, Sorbonne Paris Cit&#233;
(2) Magnet : INRIA Nord Lille Europe - Universit&#233; de Lille LIFL
</p>
<p>emmanuel.lassalle@ens-lyon.org, pascal.denis@inria.fr
</p>
<p>R&#201;SUM&#201;
Nous proposons une nouvelle m&#233;thode pour am&#233;liorer significativement la performance des
mod&#232;les &#224; paires de mentions pour la r&#233;solution de la cor&#233;f&#233;rence. &#201;tant donn&#233; un ensemble
d&#8217;indicateurs, notre m&#233;thode apprend &#224; s&#233;parer au mieux des types de paires de mentions en
classes d&#8217;&#233;quivalence, chacune de celles-ci donnant lieu &#224; un mod&#232;le de classification sp&#233;cifique.
La proc&#233;dure algorithmique propos&#233;e trouve le meilleur espace de traits (cr&#233;&#233; &#224; partir de
combinaisons de traits &#233;l&#233;mentaires et d&#8217;indicateurs) pour discriminer les paires de mentions
cor&#233;f&#233;rentielles. Bien que notre approche explore un tr&#232;s vaste ensemble d&#8217;espaces de trait,
elle reste efficace en exploitant la structure des hi&#233;rarchies construites &#224; partir des indicateurs.
Nos exp&#233;riences sur les donn&#233;es anglaises de la CoNLL-2012 Shared Task indiquent que notre
m&#233;thode donne des gains de performance par rapport au mod&#232;le initial utilisant seulement
les traits &#233;l&#233;mentaires, et ce, quelque soit la m&#233;thode de formation des cha&#238;nes ou la m&#233;trique
d&#8217;&#233;valuation choisie. Notre meilleur syst&#232;me obtient une moyenne de 67.2 en F1-mesure MUC, B3
</p>
<p>et CEAF ce qui, malgr&#233; sa simplicit&#233;, le situe parmi les meilleurs syst&#232;mes test&#233;s sur ces donn&#233;es.
</p>
<p>ABSTRACT
Learning a hierarchy of specialized pairwise models for coreference resolution
</p>
<p>This paper proposes a new method for significantly improving the performance of pairwise
coreference models. Given a set of indicators, our method learns how to best separate types of
mention pairs into equivalence classes for which we construct distinct classification models. In
effect, our approach finds the best feature space (derived from a base feature set and indicator set)
for discriminating coreferential mention pairs. Although our approach explores a very large space
of possible features spaces, it remains tractable by exploiting the structure of the hierarchies built
from the indicators. Our experiments on the CoNLL-2012 shared task English datasets indicate
that our method is robust to different clustering strategies and evaluation metrics, showing large
and consistent improvements over a single pairwise model using the same base features. Our
best system obtains 67.2 of average F1 over MUC, B3, and CEAF which, despite its simplicity,
places it among the best performing systems on these datasets.
</p>
<p>MOTS-CL&#201;S : r&#233;solution de la cor&#233;f&#233;rence, apprentissage automatique.
KEYWORDS: coreference resolution, machine learning.
</p>
<p>TALN-R&#201;CITAL 2013, 17-21 Juin, Les Sables d&#8217;Olonne
</p>
<p>118 c&#65535; ATALA</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>1 Introduction
</p>
<p>La r&#233;solution de la cor&#233;f&#233;rence consiste &#224; partitionner une s&#233;quence de syntagmes nominaux (ou
mentions) apparaissant dans un texte en un ensemble d&#8217;entit&#233;s qui partagent chacune le m&#234;me
r&#233;f&#233;rent. Une approche d&#233;sormais classique pour r&#233;soudre cette t&#226;che consiste &#224; la diviser en deux
&#233;tapes : d&#8217;abord, on d&#233;finit un mod&#232;le pour traiter les relations de cor&#233;f&#233;rence ind&#233;pendamment
les unes des autres, en g&#233;n&#233;ral via un classifieur binaire d&#233;tectant les mentions cor&#233;f&#233;rentielles.
Ensuite, les liens d&#233;tect&#233;s sont regroup&#233;s en clusters par un d&#233;codeur pour former une sortie
coh&#233;rente. Typiquement, cette &#233;tape est r&#233;alis&#233;e par des m&#233;thodes heuristiques gloutonnes
(McCarthy et Lehnert, 1995; Soon et al., 2001; Ng et Cardie, 2002; Bengston et Roth, 2008), bien
qu&#8217;il existe des approches plus sophistiqu&#233;es telles que les m&#233;thodes de graph cutting (Nicolae et
Nicolae, 2006; Cai et Strube, 2010) ou l&#8217;ILP (Integer Linear Programming) (Klenner, 2007; Denis
et Baldridge, 2009). Malgr&#233; sa simplicit&#233; apparente, cette approche en deux &#233;tapes demeure
comp&#233;titive m&#234;me lorsqu&#8217;on la compare &#224; des mod&#232;les plus complexes utilisant des mesures de
perte globale (Bengston et Roth, 2008).
</p>
<p>Avec ce type d&#8217;architecture, la performance du syst&#232;me complet d&#233;pend fortement de la qualit&#233;
du classifieur local de paires. 1 Par cons&#233;quent, beaucoup de travaux de recherche ont consist&#233;
&#224; essayer d&#8217;am&#233;liorer la performance de ce classifieur. Nombre d&#8217;entre eux se concentrent
sur l&#8217;extraction de traits, typiquement en essayant d&#8217;enrichir le classifieur avec davantage de
connaissances linguistiques et/ou de connaissances du monde (Ng et Cardie, 2002; Kehler et al.,
2004; Ponzetto et Strube, 2006; Bengston et Roth, 2008; Versley et al., 2008; Uryupina et al.,
2011). D&#8217;autres travaux cherchent &#224; utiliser des mod&#232;les locaux distincts pour diff&#233;rents types de
mentions, en particulier pour diff&#233;rents types de mentions anaphoriques en se basant sur leur
cat&#233;gories grammaticales (telles que pronoms, noms propres, descriptions d&#233;finies). On entra&#238;ne
par exemple un mod&#232;le pour les pronoms, un autre pour les SN d&#233;finis, etc (Morton, 2000; Ng,
2005; Denis et Baldridge, 2008) 2. L&#8217;utilisation de mod&#232;les sp&#233;cialis&#233;s trouve une justification
importante en psycho-linguistique, dans des travaux th&#233;oriques sur la saillance ou l&#8217;accessibilit&#233;
(Ariel, 1988). Du point de vue de l&#8217;apprentissage statistique, ces seconds travaux se rapprochent
de ceux sur l&#8217;extraction de traits dans la mesure o&#249; les deux approches reviennent &#224; poser le
probl&#232;me de la classification de paires dans un espace de plus grande dimension.
</p>
<p>Dans ce travail, nous soutenons que les paires de mentions ne devraient pas &#234;tre trait&#233;es
par un seul classifieur, mais au contraire par des mod&#232;les sp&#233;cifiques. En somme, nous nous
int&#233;ressons &#224; apprendre comment construire et s&#233;lectionner de tel mod&#232;les. Notre argumentation
se fonde sur des consid&#233;rations statistiques plut&#244;t que purement linguistiques (l&#8217;approche est
donc compl&#233;mentaire aux &#233;tudes th&#233;oriques). La question que nous posons est, &#233;tant donn&#233;
un ensemble d&#8217;indicateurs (tels que les types grammaticaux, la distance entre deux mentions
ou le type d&#8217;entit&#233; nomm&#233;e), comment s&#233;parer les paires de mentions afin de discriminer au
mieux les paires cor&#233;f&#233;rentielles par rapport &#224; celles qui ne le sont pas. Ainsi, nous cherchons &#224;
apprendre les &#8220;meilleurs&#8221; espaces de repr&#233;sentation pour nos diff&#233;rents mod&#232;les : c&#8217;est-&#224;-dire des
espaces ni trop grossiers (c.-&#224;-d. peu aptes &#224; bien s&#233;parer les donn&#233;es), ni trop sp&#233;cifiques (c.-&#224;-d.
pouvant souffrir du manque de donn&#233;es ou de bruit). Nous verrons que cette d&#233;marche est aussi
&#233;quivalente &#224; construire un seul tr&#232;s grand espace de traits pour repr&#233;senter toutes les donn&#233;es.
</p>
<p>1. Il n&#8217;y a toutefois aucune garantie th&#233;orique pour que l&#8217;am&#233;lioration de la classification locale ait toujours un impact
positif sur la performance globale lorsque les deux modules sont optimis&#233;s s&#233;par&#233;ment.
</p>
<p>2. Parfois, des &#233;chantillonnages diff&#233;rents sont choisis lors de la phase d&#8217;apprentissage des mod&#232;les locaux distincts
(Ng et Cardie, 2002; Uryupina, 2004).
</p>
<p>TALN-R&#201;CITAL 2013, 17-21 Juin, Les Sables d&#8217;Olonne
</p>
<p>119 c&#65535; ATALA</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Notre approche g&#233;n&#233;ralise les approches pr&#233;c&#233;dentes de plusieurs mani&#232;res. D&#8217;une part, la
d&#233;finition des diff&#233;rents mod&#232;les n&#8217;est plus restreinte au simple typage grammatical (notre mod&#232;le
permet d&#8217;utiliser n&#8217;importe quel type d&#8217;indicateurs) ni au seul typage de la mention anaphorique
(nos mod&#232;les peuvent aussi &#234;tre associ&#233;s au typage de l&#8217;ant&#233;c&#233;dent ou bien au types des deux
&#233;l&#233;ments de la paire). D&#8217;autre part, nous proposons une m&#233;thode originale pour apprendre les
meilleurs ensembles de mod&#232;les que l&#8217;on peut construire &#224; partir d&#8217;un ensemble d&#8217;indicateurs
donn&#233;s et des donn&#233;es d&#8217;apprentissage. Ces mod&#232;les sont organis&#233;s dans une hi&#233;rarchie o&#249;
chaque feuille correspond &#224; un ensemble de paires de mentions disjoint des autres et sur lequel
un classifieur est entra&#238;n&#233;. Nos diff&#233;rents mod&#232;les sont entra&#238;n&#233;s en utilisant l&#8217;algorithme Online
Passive-Aggressive, ou PA (Crammer et al., 2006), qui est une version &#224; large marge du perceptron.
Notre m&#233;thode peut &#234;tre qualifi&#233;e d&#8217;exacte dans le sens o&#249; elle explore compl&#232;tement l&#8217;espace des
hi&#233;rarchies d&#233;finissables &#224; partir d&#8217;un ensemble d&#8217;indicateurs donn&#233; (on en d&#233;nombre au moins
22
</p>
<p>n
pour n indicateurs), tout en ma&#238;trisant la complexit&#233; algorithmique par une technique de
</p>
<p>programmation dynamique qui exploite la structure particuli&#232;re des hi&#233;rarchies. Cette approche
obtient de tr&#232;s bonnes performances, et d&#233;passe largement le mod&#232;le de d&#233;part qui utilise
seulement les traits &#233;l&#233;mentaires. Comme le montreront diverses exp&#233;riences sur les donn&#233;es
anglaises de la CoNLL-2012 Shared Task, des am&#233;liorations importantes sont observables sur
diff&#233;rentes m&#233;triques d&#8217;&#233;valuation ; par ailleurs, celles-ci ne d&#233;pendent pas de la m&#233;thode de
clustering choisie pour le d&#233;codeur.
</p>
<p>La suite de cet article est organis&#233;e comme suit : dans la section 2, nous discutons les hypoth&#232;ses
statistiques sur lesquelles repose le mod&#232;le standard &#224; paires de mentions, et nous d&#233;finissons un
mod&#232;le alternatif qui utilise une simple s&#233;paration des paires de mentions en fonction de leur
type grammatical. Ensuite, dans la section 3, nous g&#233;n&#233;ralisons ce mod&#232;le en introduisant les
hi&#233;rarchies d&#8217;indicateurs en expliquant comment apprendre le meilleur mod&#232;le possible &#224; partir
de celles-ci. La section 4 donne une br&#232;ve description du syst&#232;me complet et la section 5 donne
les r&#233;sultats d&#8217;&#233;valuation des diff&#233;rents mod&#232;les sur les donn&#233;es anglaises de CoNLL-2012.
</p>
<p>2 Mod&#233;lisation des paires
</p>
<p>En principe, les mod&#232;les &#224; paires emploient un seul classifieur local pour d&#233;cider si deux mentions
sont cor&#233;f&#233;rentes ou non. Lorsque l&#8217;on utilise des techniques d&#8217;apprentissage automatique, cela
entra&#238;ne quelques hypoth&#232;ses sur le comportement statistique des paires de mentions.
</p>
<p>2.1 Hypoth&#232;ses statistiques
</p>
<p>Pour commencer, adoptons un point de vue probabiliste pour d&#233;crire le prototype du mod&#232;le &#224;
paires. &#201;tant donn&#233; un document, le nombre de mentions est fix&#233; et chaque paire de mentions
suit une certaine distribution (que l&#8217;on observe en partie en projetant les paires dans un espace de
traits). L&#8217;id&#233;e fondamentale du mod&#232;le &#224; paires est de consid&#233;rer que les paires de mentions sont
ind&#233;pendantes les unes des autres (du coup, la propri&#233;t&#233; de transitivit&#233; n&#8217;est pas n&#233;cessairement
v&#233;rifi&#233;e en sortie, c&#8217;est pourquoi il faut un d&#233;codeur la transformer en partition coh&#233;rente).
</p>
<p>Utiliser un seul classifieur pour traiter toutes les paires de mentions revient &#224; supposer qu&#8217;elles
sont identiquement distribu&#233;es. Nous pensons que les paires ne sont pas identiquement dis-
</p>
<p>TALN-R&#201;CITAL 2013, 17-21 Juin, Les Sables d&#8217;Olonne
</p>
<p>120 c&#65535; ATALA</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>tribu&#233;es, mais qu&#8217;il faut au contraire s&#233;parer diff&#233;rents &#8220;types&#8221; de paires et cr&#233;er des mod&#232;les
sp&#233;cifiques pour ces types.
</p>
<p>S&#233;parer diff&#233;rents types de paires et les traiter avec des mod&#232;les sp&#233;cifiques peut amener &#224;
des mod&#232;les globaux plus pr&#233;cis. Certains syst&#232;mes de r&#233;solution traitent d&#233;j&#224; diff&#233;rents types
d&#8217;anaphores s&#233;par&#233;ment, ce qui revient &#224; supposer que par exemple, les paires qui contiennent
un pronom se comportent diff&#233;remment des autres (Morton, 2000; Ng, 2005; Denis et Baldridge,
2008). Nous pourrions essayer de capturer ces diff&#233;rents comportements avec un ensemble
tr&#232;s riche de traits, mais en r&#233;alit&#233; nous ne disposons que d&#8217;un nombre assez restreint de traits
&#233;l&#233;mentaires (voir la section 4) et cr&#233;er de nouveaux traits en les combinant doit &#234;tre fait avec
prudence pour &#233;viter d&#8217;introduire du bruit dans le mod&#232;le. Au lieu de cela, nous montrerons
qu&#8217;une s&#233;paration habile des instances apporte de bonnes am&#233;liorations au mod&#232;le &#224; paires.
</p>
<p>2.2 Espaces de traits
</p>
<p>2.2.1 D&#233;finitions
</p>
<p>Commen&#231;ons par donner une vision plus formelle de la mod&#233;lisation. Chaque paire de mentions
mi et mj est repr&#233;sent&#233;e par une variable al&#233;atoire :
</p>
<p>Pi j : &#8486; &#8594; &#65535; &#215;&#65535;
&#969; &#65535;&#8594; (xi j(&#969;), yi j(&#969;))
</p>
<p>o&#249; &#8486; d&#233;note classiquement l&#8217;al&#233;atoire, &#65535; est l&#8217;espace des objets &quot;paires de mentions&quot; qui n&#8217;est
pas directement observable et yi j(&#969;) &#8712; &#65535; = {+1,&#8722;1} sont les &#233;tiquettes indiquant si mi et mj
sont cor&#233;f&#233;rentes ou non. Pour all&#233;ger un peu ces notations, nous n&#8217;&#233;crirons pas toujours l&#8217;indice
i j. Maintenant nous d&#233;finissons une fonction :
</p>
<p>&#966;&#65535; :&#65535; &#8594; &#65535;
x &#65535;&#8594; &#966;&#65535; (x)
</p>
<p>qui projette les paires dans un espace de traits &#65535; &#224; travers lequel elles sont observ&#233;es. Pour nous,
&#65535; est simplement un espace vectoriel sur &#65535; (dans notre cas, la plupart des traits sont bool&#233;ens ;
ils sont projet&#233;s sur &#65535; avec les valeurs 0 et 1). Pour des raisons de coh&#233;rence technique, nous
supposons que &#966;&#65535;1(x(&#969;)) et &#966;&#65535;2(x(&#969;)) conservent les m&#234;mes valeurs lorsqu&#8217;on les projette sur
l&#8217;espace de traits &#65535;1 &#8745;&#65535;2 : cela signifie simplement que les traits communs &#224; deux espaces ont
toujours les m&#234;mes valeurs.
</p>
<p>De ce point de vue formel, la t&#226;che de r&#233;solution de la cor&#233;f&#233;rence consiste &#224; fixer un es-
pace de traits &#65535; , observer des &#233;chantillons &#233;tiquet&#233;s {(&#966;&#65535; (x), y)t}t&#8712;TrainSet et, &#233;tant donn&#233; de
nouvelles variables partiellement observ&#233;es {(&#966;&#65535; (x))t}t&#8712;TestSet , tenter de retrouver la valeur
correspondante de y .
</p>
<p>2.2.2 Un autre point de vue sur les hypoth&#232;ses statistiques
</p>
<p>Nous avons &#233;crit plus haut que les paires de mentions n&#8217;apparaissent pas identiquement distri-
bu&#233;es puisque, par exemple, les pronoms ne se comportent pas de la m&#234;me fa&#231;on que les noms.
</p>
<p>TALN-R&#201;CITAL 2013, 17-21 Juin, Les Sables d&#8217;Olonne
</p>
<p>121 c&#65535; ATALA</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Nous pouvons maintenant formuler cela de fa&#231;on plus rigoureuse : puisque nous ne pouvons
pas observer directement l&#8217;espace des objets &#65535; , nous en ignorons la complexit&#233;. En particulier,
lorsque nous utilisons une projection vers un espace de traits trop petit, le classifieur ne parvient
pas &#224; capturer la distribution correctement : les donn&#233;es semblent trop bruit&#233;es.
</p>
<p>Maintenant en remarquant que les anaphores pronominales ne se comportent pas de la m&#234;me
mani&#232;re que les autres anaphores, nous distinguons deux types de paires, c&#8217;est-&#224;-dire que nous
voyons la distribution des paires dans &#65535; comme un m&#233;lange de deux distributions. De ce fait,
nous pourrons peut-&#234;tre s&#233;parer les paires positives et n&#233;gatives plus facilement si nous projetons
chaque type de paires dans un espace de traits sp&#233;cifique. Appelons ces espaces de traits &#65535;1
et &#65535;2. Nous pouvons ou bien d&#233;finir deux classifieurs ind&#233;pendants sur &#65535;1 et &#65535;2 pour traiter
chaque type de paires ou d&#233;finir un seul mod&#232;le sur un espace plus grand &#65535; =&#65535;1 &#8853;&#65535;2. Si le
mod&#232;le est lin&#233;aire, et &#231;a sera notre cas, il se trouve que cela est &#233;quivalent.
</p>
<p>En cons&#233;quence, nous pouvons de fait supposer que les variables Pi j sont identiquement distri-
bu&#233;es. Et le nouveau probl&#232;me &#224; r&#233;soudre est de trouver une projection &#966;&#65535; qui repr&#233;sente au
mieux la distribution des donn&#233;es (qui les rend facilement s&#233;parables).
</p>
<p>D&#8217;un point de vue th&#233;orique, plus la dimension de l&#8217;espace des traits est grande (par exemple
la somme directe de tous les espaces de traits dont nous disposons), plus nous avons de d&#233;tails
sur la distribution des paires de mentions et plus nous pouvons esp&#233;rer s&#233;parer les positifs des
n&#233;gatifs avec pr&#233;cision. En pratique, nous sommes confront&#233;s au probl&#232;me de raret&#233; des donn&#233;es :
il n&#8217;y a pas assez de donn&#233;es pour entra&#238;ner correctement un mod&#232;le lin&#233;aire sur un tel espace.
Au final, nous cherchons un espace de traits qui se situe entre les deux extr&#234;mes que constituent
un espace trop grand (donn&#233;es rares) ou trop petit (donn&#233;es bruit&#233;es). L&#8217;objectif principal de ce
travail est de d&#233;finir une m&#233;thode g&#233;n&#233;rale pour choisir l&#8217;espace &#65535; le plus ad&#233;quat parmi un tr&#232;s
grand nombre de possibilit&#233;s et lorsque nous ne savons pas a priori lequel peut &#234;tre le meilleur.
</p>
<p>2.2.3 Mod&#232;les lin&#233;aires et espaces ind&#233;pendants
</p>
<p>Dans ce travail, nous essayons de s&#233;parer lin&#233;airement les instances positives des n&#233;gatives dans
&#65535; : le mod&#232;le apprend un vecteur param&#232;tre w qui d&#233;finit un hyperplan coupant l&#8217;espace en
deux parties. La classe pr&#233;dite pour la paire x avec vecteur de traits &#966;&#65535; (x) est donn&#233;e par :
</p>
<p>C&#65535; (x) := si gn(wT &#183;&#966;&#65535; (x))
La propri&#233;t&#233; de lin&#233;arit&#233; rend &#233;quivalentes les s&#233;parations des instances de deux types t1 et t2,
dans deux mod&#232;les ind&#233;pendants avec pour espace de traits respectif&#65535;1 et&#65535;2 et pour param&#232;tres
w1 et w2, et un mod&#232;le simple sur &#65535;1 &#8853;&#65535;2. Pour voir pourquoi, d&#233;finissons la projection :
</p>
<p>&#966;&#65535;1&#8853;&#65535;2(x) :=
&#65535;&#65535;
</p>
<p>&#966;&#65535;1(x)T 0
&#65535;T
</p>
<p>si x est de type t1&#65535;
0 &#966;&#65535;2(x)T
</p>
<p>&#65535;T
si x est de type t2
</p>
<p>et le vecteur param&#232;tre w=
&#65535;
</p>
<p>w1
</p>
<p>w2
</p>
<p>&#65535;
&#8712; &#65535;1 &#8853;&#65535;2. Nous avons alors :
</p>
<p>C&#65535;1&#8853;&#65535;2(x) =
&#65535;
C&#65535;1(x) si x est de type t1
C&#65535;2(x) si x est de type t2
</p>
<p>TALN-R&#201;CITAL 2013, 17-21 Juin, Les Sables d&#8217;Olonne
</p>
<p>122 c&#65535; ATALA</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Il faut maintenant s&#8217;assurer que cette propri&#233;t&#233; est v&#233;rifi&#233;e lors de l&#8217;apprentissage du param&#232;tre
w. Dans ce travail nous avons utilis&#233; l&#8217;algorithme en ligne Passive-Aggressive pour la classification
binaire (Crammer et al., 2006). Ce mod&#232;le est une extension du perceptron, dont l&#8217;obectif &#224;
chaque it&#233;ration est, d&#8217;une part de minimiser les changements apport&#233;s au mod&#232;le existant
(d&#8217;o&#249; la caract&#233;ristique &#8220;passive&#8221;) et, d&#8217;autre part, de faire en sorte que l&#8217;exemple courant
soit correctement classifi&#233; avec une large marge (d&#8217;o&#249; la caract&#233;ristique &#8220;aggressive&#8221;). Plus
pr&#233;cis&#233;ment, la mise &#224; jour du vecteur de poids &#224; chaque it&#233;ration prend la forme suivante :
</p>
<p>wt+1 = argmin
w&#8712;&#65535;
</p>
<p>1
2
</p>
<p>&#65535;&#65535;w&#8722;wt&#65535;&#65535;2 tq l(w; (xt , yt)) = 0
o&#249; l(w; (xt , yt)) =min(0,1&#8722; yt(w &#183;&#966;&#65535; (xt))), de sorte que lorsque &#65535; =&#65535;1 &#8853;&#65535;2, le minimum
si x est de type t1 est wt+1 =
</p>
<p>&#65535;
w1t+1
w2t
</p>
<p>&#65535;
et si x est de type t2 is wt+1 =
</p>
<p>&#65535;
w1t
w2t+1
</p>
<p>&#65535;
o&#249; wit+1
</p>
<p>correspond aux mises &#224; jour dans l&#8217;espace &#65535;i ind&#233;pendamment du reste. Ce r&#233;sultat peut &#234;tre
facilement &#233;tendu au cas de n espaces de traits. Par cons&#233;quent, avec une s&#233;paration d&#233;terministe
des donn&#233;es, un mod&#232;le sur un grand espace peut &#234;tre appris en le d&#233;composant en mod&#232;les
ind&#233;pendants sur des espaces plus petits.
</p>
<p>2.3 Un exemple : la s&#233;paration par gramtype
</p>
<p>Pour motiver notre approche, nous commen&#231;ons par introduire une s&#233;paration relativement
simple des paires de mentions qui s&#8217;appuie sur les 9 mod&#232;les obtenus en consid&#233;rant toutes
les combinaisons possibles des types grammaticaux {nominal, name, pronoun} pour les deux
mentions de la paire (une s&#233;paration fine similaire peut &#234;tre trouv&#233;e dans (Chen et al., 2011)).
</p>
<p>Cela revient &#224; utiliser 9 espaces de traits diff&#233;rents &#65535;1, . . . ,&#65535;9 pour capturer la distribution
globale des paires. Avec des classifieur lin&#233;aires, nous obtenons un seul mod&#232;le sur l&#8217;espace de
traits &#65535; =&#65535;1 &#8853; &#183; &#183; &#183;&#8853;&#65535;9. Nous appellerons cela le mod&#232;le gramtype.
Comme nous le verrons dans la section 5, ces mod&#232;les s&#233;par&#233;s obtiennent des performances qui
d&#233;passent significativement celles d&#8217;un unique mod&#232;le qui utilise les m&#234;mes traits &#233;l&#233;mentaires.
Mais nous voudrions d&#233;finir une m&#233;thode qui adapte l&#8217;espace de traits aux donn&#233;es en choisissant
elle-m&#234;me la s&#233;paration des paires la plus appropri&#233;e.
</p>
<p>3 Hi&#233;rarchisation des espaces de traits
</p>
<p>Dans cette section, nous pr&#233;sentons notre m&#233;thode pour trouver automatiquement une s&#233;paration
optimale des paires de mentions. On gardera &#224; l&#8217;esprit que s&#233;parer les paires dans diff&#233;rents
mod&#232;les est la m&#234;me chose que construire un grand espace de traits dans lequel le param&#232;tre w
peut &#234;tre appris par parties dans des sous-espaces ind&#233;pendants.
</p>
<p>3.1 Indicateurs sur les paires
</p>
<p>Pour d&#233;finir des espaces de traits suppl&#233;mentaires, nous utilisons des indicateurs, qui sont des
fonctions d&#233;terministes sur les paires de mentions avec un nombre restreint de valeurs possibles.
</p>
<p>TALN-R&#201;CITAL 2013, 17-21 Juin, Les Sables d&#8217;Olonne
</p>
<p>123 c&#65535; ATALA</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Les indicateurs sont utilis&#233;s pour classer les paires dans des cat&#233;gories pr&#233;d&#233;finies et en bijection
avec un ensemble d&#8217;espaces de traits &#233;l&#233;mentaires ind&#233;pendants. Nous pouvons r&#233;utiliser les
traits du syst&#232;me comme indicateurs, par exemple, le type grammatical ou celui des entit&#233;s
nomm&#233;es. Nous pouvons &#233;galement utiliser des fonctions qui ne sont pas des traits, par exemple
la position approximative d&#8217;une des deux mentions dans le texte.
</p>
<p>Le petit nombre de valeurs possibles pour un indicateur est requis pour des raisons pratiques : si
une cat&#233;gories de paires est trop fine, l&#8217;espace de traits associ&#233; souffrira de la raret&#233; des donn&#233;es.
Les indicateurs utilisant des distances doivent donc les approximer par des histogrammes assez
grossiers. Dans nos exp&#233;riences, le nombre de valeurs possibles ne d&#233;passera jamais une douzaine
(ce qui sera amplement suffisant pour g&#233;n&#233;rer assez de combinatoire). Une fa&#231;on de r&#233;duire la
taille de l&#8217;ensemble des valeurs d&#8217;un indicateur est de le binariser, de la m&#234;me fa&#231;on que l&#8217;on
binarise un arbre (il y a plusieurs binarisations possibles). Cette op&#233;ration produit une hi&#233;rarchie
d&#8217;indicateurs imbriqu&#233;s, qui est exactement la structure que nous exploitons dans la suite.
</p>
<p>3.2 Des hi&#233;rarchies pour s&#233;parer les paires
</p>
<p>Nous d&#233;finissons les hi&#233;rarchies comme des combinaisons d&#8217;indicateurs cr&#233;ant des cat&#233;gories
de plus en plus fines de paires de mentions : &#233;tant donn&#233;e une suite d&#8217;indicateurs, une paire
de mentions est class&#233;e en appliquant les indicateurs successivement, chaque fois en raffinant
une cat&#233;gorie en sous-cat&#233;gories, de la m&#234;me mani&#232;re que dans un arbre de d&#233;cision (chaque
n&#339;ud ayant le m&#234;me nombre d&#8217;enfants que le nombre de valeurs prises par son indicateur).
Nous autorisons la classification &#224; s&#8217;arr&#234;ter avant d&#8217;appliquer le dernier indicateur, mais le
comportement doit &#234;tre le m&#234;me pour toutes les instances. Ainsi une hi&#233;rarchie est en principe
un sous-arbre de l&#8217;arbre de d&#233;cision complet qui contient des copies d&#8217;un m&#234;me indicateur &#224;
chaque niveau.
</p>
<p>Si toutes les feuilles de l&#8217;arbre de d&#233;cision ont la m&#234;me profondeur, cela correspond &#224; prendre le
produit cart&#233;sien des valeurs de tous les indicateurs pour indexer les cat&#233;gories. Dans ce cas, nous
parlerons de hi&#233;rarchies-produit. Le mod&#232;le gramtype peut &#234;tre vu comme une hi&#233;rarchie-produit
&#224; deux niveaux (figure 1).
</p>
<p>FIGURE 1 &#8211; Le mod&#232;le gramtype vu comme une hi&#233;rarchie-produit.
</p>
<p>Les hi&#233;rarchies-produit seront le point de d&#233;part de notre m&#233;thode pour trouver un espace de
</p>
<p>TALN-R&#201;CITAL 2013, 17-21 Juin, Les Sables d&#8217;Olonne
</p>
<p>124 c&#65535; ATALA</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>traits qui repr&#233;sente bien les donn&#233;es. Maintenant, pour choisir une suite d&#8217;indicateurs appropri&#233;s,
il faut faire appel aux intuitions linguistiques et aux travaux th&#233;oriques sur le sujet. Le syst&#232;me
trouvera lui-m&#234;me la meilleure fa&#231;on d&#8217;utiliser ces indicateurs lorsqu&#8217;il optimisera la hi&#233;rarchie.
La suite d&#8217;indicateurs est donc un param&#232;tre du mod&#232;le.
</p>
<p>3.3 Lien entre les hi&#233;rarchies et les espaces de traits
</p>
<p>Comme nous l&#8217;avons fait pour le mod&#232;le gramtype, nous associons un espace de traits &#65535;i &#224;
chacune des feuilles de la hi&#233;rarchie. De la m&#234;me mani&#232;re, la somme &#65535; =&#65535;i&#65535;i d&#233;finit un
grand espace de traits, et le param&#232;tre correspondant w d&#8217;un mod&#232;le lin&#233;aire peut &#234;tre appris en
apprenant les wi dans les &#65535;i .
&#201;tant donn&#233;e une s&#233;quence d&#8217;indicateurs, le nombre de hi&#233;rarchies diff&#233;rentes que nous pouvons
d&#233;finir est &#233;gal au nombre de sous-arbres entiers (chaque n&#339;ud a tous ses enfants possibles
ou aucun) de l&#8217;arbre complet de d&#233;cision (chaque n&#339;ud interne ayant tous ses enfants). Le cas
minimal est celui d&#8217;indicateurs bool&#233;ens. Le nombre d&#8217;arbre binaires entiers de taille au plus n
peut &#234;tre calcul&#233; par la r&#233;currence suivante : T (1) = 1 et T (n+1) = 1+ T (n)2. Donc T (n)&#8805; 22n :
m&#234;me avec des petites valeurs de n, le nombre de hi&#233;rarchies diff&#233;rentes (ou de grand espaces
de traits) d&#233;finissables par une s&#233;quence d&#8217;indicateurs est gigantesque (p.ex. T (10)&#8776; 3.8.1090).
Parmi toutes les possibilit&#233;s pour un grand espace de traits, beaucoup ne sont pas appropri&#233;s
parce qu&#8217;avec eux les donn&#233;es sont trop rares ou trop bruit&#233;es dans certains sous-espaces. Nous
avons besoin d&#8217;une m&#233;thode g&#233;n&#233;rale pour trouver le meilleur espace sans avoir &#224; &#233;num&#233;rer et
tester chacun d&#8217;eux.
</p>
<p>3.4 Optimisation des hi&#233;rarchies
</p>
<p>Consid&#233;rons que la s&#233;quence d&#8217;indicateurs est fix&#233;e, soit n sa longueur. Pour trouver le meilleur
espace de traits parmi un tr&#232;s grand nombre de possibilit&#233;s, nous avons besoin d&#8217;un crit&#232;re de
s&#233;lection applicable sans trop de calculs suppl&#233;mentaires. Pour cela, nous n&#8217;&#233;valuons l&#8217;espace
de traits que localement sur les paires, c&#8217;est-&#224;-dire sans appliquer un d&#233;codeur &#224; la sortie. Nous
employons trois mesures sur les r&#233;sultats de la classification des paires : la pr&#233;cision, le rappel et
le F1-score. S&#233;lectionner le meilleur espace pour une de ces mesures peut &#234;tre r&#233;alis&#233; en utilisant
des techniques de programmation dynamique. Dans nos exp&#233;riences, nous cherchons &#224; optimiser
le F1-score.
</p>
<p>Entra&#238;nement de la hi&#233;rarchie : Partant de la hi&#233;rarchie-produit, nous associons un classifieur
et son propre espace de traits &#224; chacun des n&#339;uds de l&#8217;arbre 3. Les classifieurs sont alors entra&#238;n&#233;s
comme suit : pour chaque instance, il existe un unique chemin de la racine vers une feuille de
l&#8217;arbre complet. Chaque classifieur situ&#233; sur ce chemin est mis &#224; jour avec cette instance. Le
nombre d&#8217;it&#233;rations pour le Passive-Aggressive est fix&#233; (nous n&#8217;avons pas cherch&#233; &#224; optimiser ce
param&#232;tre).
</p>
<p>Calcul des scores : Apr&#232;s la phase d&#8217;apprentissage, nous testons tous les classifieurs sur un autre
</p>
<p>3. Dans les exp&#233;riences, les classifieurs utilisent une copie d&#8217;un m&#234;me espace de traits, mais pas les m&#234;mes donn&#233;es,
ce qui correspond &#224; croiser les traits avec les cat&#233;gories de l&#8217;arbre de d&#233;cision.
</p>
<p>TALN-R&#201;CITAL 2013, 17-21 Juin, Les Sables d&#8217;Olonne
</p>
<p>125 c&#65535; ATALA</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>ensemble de paires de d&#233;veloppement 4. Une fois encore, un classifieur est test&#233; sur une instance
seulement s&#8217;il est situ&#233; sur le chemin de la racine vers une feuille associ&#233; &#224; l&#8217;instance. Nous
obtenons des nombres TP/FP/FN 5 sur les classifications des paires, qui suffisent pour calculer
le F1-score. Comme pour l&#8217;apprentissage, les donn&#233;es sur lesquelles un classifieur &#224; un n&#339;ud
donn&#233; est &#233;valu&#233; sont les m&#234;mes que la r&#233;union de toutes les donn&#233;es utilis&#233;es pour &#233;valuer les
classifieurs correspondant aux enfant de ce n&#339;ud. C&#8217;est ainsi que nous sommes en mesure de
comparer les scores obtenus au niveau d&#8217;un n&#339;ud &#224; la &quot;r&#233;union des scores&quot; obtenus au niveau de
ses enfants.
</p>
<p>D&#233;coupage de la hi&#233;rarchie : Pour le moment, nous avons un arbre complet avec un classifieur
&#224; chaque n&#339;ud. Nous utilisons une technique de programmation dynamique pour calculer la
meilleure hi&#233;rarchie en coupant cet arbre et en ne gardant que les classifieurs situ&#233;s au niveau
des feuilles. L&#8217;algorithme assemble les meilleurs mod&#232;les locaux (ou espaces de traits) pour cr&#233;er
des mod&#232;les plus grands. Il part des feuilles pour remonter jusqu&#8217;&#224; la racine et coupe le sous-arbre
qui commence &#224; un n&#339;ud &#224; chaque fois qu&#8217;il ne fournit pas de meilleur score que le score du
n&#339;ud seul, ou au contraire il propage le score du sous-arbre lorsqu&#8217;il y a une am&#233;lioration. Les
d&#233;tails sont donn&#233;s dans l&#8217;algorithme 1.
</p>
<p>1 list&#8592; list of nodes given by breadth-first search for node in reversed list do
2 if node.children &#65535;= &#65535; then
3 if sum-score(node.children) &gt; node.score then
4 node.TP/FP/FN&#8592; sum-num(node.children)
5 else
6 node.children&#8592; &#65535;
7 end
8 end
9 end
</p>
<p>ALGORITHME 1 &#8211; D&#233;coupage de la hi&#233;rarchie
</p>
<p>Discutons bri&#232;vement la validit&#233; et la complexit&#233; de l&#8217;algorithme. Chaque n&#339;ud n&#8217;est vu que deux
fois donc la complexit&#233; est lin&#233;aire en le nombre de n&#339;uds qui est au moins &#65535; (2n). Toutefois,
seulement les n&#339;uds qui ont rencontr&#233; au moins une instance d&#8217;apprentissage sont utiles et il y
en a &#65535; (n&#215; k) (o&#249; k est la taille de l&#8217;ensemble d&#8217;apprentissage). Donc nous pouvons optimiser
l&#8217;algorithme pour tourner en temps &#65535; (n&#215; k) (qui est &#233;galement le temps d&#8217;entra&#238;nement de la
hi&#233;rarchie). En parcourant &#224; l&#8217;envers la liste obtenue par le parcours en largeur de la hi&#233;rarchie,
nous sommes assur&#233;s que chaque n&#339;ud sera trait&#233; apr&#232;s ses enfants donc que le mod&#232;le optimal
sera construit de proche en proche jusqu&#8217;&#224; la racine. (node.children) est l&#8217;ensemble des enfants de
node, et (node.score) est son score. sum-num fournit les TP/FP/FN en sommant simplement les
nombres correspondants des enfants et sum-score calcule le score bas&#233; sur ces nouveaux nombres
TP/FP/FN. La (ligne 6) coupe les enfants d&#8217;un n&#339;ud quand ils ne sont pas utilis&#233;s pour d&#233;finir le
meilleur score. L&#8217;algorithme propage alors les meilleurs scores depuis les feuilles vers la racine,
ce qui donne au final un seul score qui correspond &#224; celui de la meilleure hi&#233;rarchie. Seulement
les feuilles utilis&#233;es pour calculer le meilleur score sont gard&#233;es et elles d&#233;finissent la meilleure
hi&#233;rarchie.
</p>
<p>Relation entre le d&#233;coupage et l&#8217;espace de traits global : Nous pouvons voir l&#8217;op&#233;ration de
</p>
<p>4. Les donn&#233;es d&#8217;apprentissages sont coup&#233;es en deux parties, pour l&#8217;apprentissage et pour tester la hi&#233;rarchie.
5. &quot;True positives&quot;, &quot;false positives&quot; et &quot;false negatives&quot;.
</p>
<p>TALN-R&#201;CITAL 2013, 17-21 Juin, Les Sables d&#8217;Olonne
</p>
<p>126 c&#65535; ATALA</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>d&#233;coupage comme le remplacement d&#8217;un groupe de sous-espaces par un seul sous-espace dans
la somme (voir figure 2). D&#233;couper la hi&#233;rarchie-produit revient donc &#224; r&#233;duire l&#8217;espace de
traits global (l&#8217;espace somme) de mani&#232;re optimale. Nous voyons ici le lien entre la meilleure
hi&#233;rarchie et l&#8217;espace de traits qui permet de s&#233;parer au mieux les paires.
</p>
<p>FIGURE 2 &#8211; D&#233;couper la hi&#233;rarchie r&#233;duit l&#8217;espace de traits
</p>
<p>4 Description du syst&#232;me
</p>
<p>Notre syst&#232;me se compose du mod&#232;le &#224; paires s&#233;par&#233;es obtenu en d&#233;coupant la hi&#233;rarchie (c&#8217;est
donc un PA sur l&#8217;espace de traits somme) et un d&#233;codeur glouton pour cr&#233;er des clusters &#224; partir
de sa sortie. Il est param&#233;tr&#233; par le choix de la s&#233;quence initiale d&#8217;indicateurs.
</p>
<p>Les traits &#233;l&#233;mentaires Nous avons utilis&#233; un ensemble de traits classiques qui sont d&#233;taill&#233;s
dans (Bengston et Roth, 2008) et (Rahman et Ng, 2011). Nous ne listons ici que les groupes de
traits : types et sous-types grammaticaux des mentions, m&#234;me cha&#238;ne/sous-cha&#238;ne de caract&#232;res,
apposition, copule, distance (en nombre de mentions/phrases/mots), &#233;galit&#233; en genre/nombre,
synonymie/hyperonymie et caract&#232;re anim&#233; (en utilisant WordNet), nom de famille (&#224; partir de
liste), types d&#8217;entit&#233; nomm&#233;e, traits syntaxiques (gold parse tree) et d&#233;tection d&#8217;anaphoricit&#233;.
</p>
<p>Indicateurs Comme indicateurs nous avons utilis&#233; : types et sous-types grammaticaux pour les
mentions gauche (ant&#233;c&#233;dent) et droite (anaphore) selon l&#8217;ordre du texte, types d&#8217;entit&#233;s nom-
m&#233;es, un bool&#233;en indiquant si les mentions se trouvent dans la m&#234;me phrase et un histogramme
tr&#232;s grossier de la distance en nombre de phrase. Nous avons syst&#233;matiquement commenc&#233; les
s&#233;quences (de diff&#233;rentes longueurs) par les types grammaticaux droit et gauche, en ajoutant
ensuite d&#8217;autres indicateurs. Le param&#232;tre a &#233;t&#233; optimis&#233; par cat&#233;gorie de document en utilisant
les donn&#233;es de d&#233;veloppement, apr&#232;s avoir d&#233;cod&#233; la sortie du mod&#232;le &#224; paires.
</p>
<p>D&#233;codeurs Nous avons test&#233; trois strat&#233;gies gloutonnes classiques pour s&#233;lectionner les liens
et former les clusters &#224; partir des d&#233;cisions du classifieur : Closest-First (fusionne les mentions
avec la mention cor&#233;f&#233;rente &#224; gauche la plus proche, si elle existe) (Soon et al., 2001), Best-first
(fusionne les mentions avec la mention &#224; gauche qui obtient le meilleur score positif) (Ng et
Cardie, 2002; Bengston et Roth, 2008), et Aggressive-Merge (fermeture transitive sur les paires
positives) (McCarthy et Lehnert, 1995). Chacun de ces d&#233;codeurs va typiquement de paire
avec un &#233;chantillonnage particulier lors de l&#8217;apprentissage (m&#234;me si ce n&#8217;est pas obligatoire).
</p>
<p>TALN-R&#201;CITAL 2013, 17-21 Juin, Les Sables d&#8217;Olonne
</p>
<p>127 c&#65535; ATALA</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Par exemple, Closest-First est combin&#233; avec un &#233;chantillonnage o&#249; sont utilis&#233;es seulement les
instances dans lesquelles la mention de gauche appara&#238;t entre le l&#8217;anaphore et l&#8217;ant&#233;c&#233;dent le
plus proche (Soon et al., 2001).
</p>
<p>5 Exp&#233;riences
</p>
<p>5.1 Donn&#233;es
</p>
<p>Nous avons &#233;valu&#233; le syst&#232;me sur la partie anglaise du corpus fourni dans la CoNLL-2012 Shared
Task (Pradhan et al., 2012). Le corpus contient 7 cat&#233;gories de documents (plus de 2k documents,
1.3M de mots). Nous avons utilis&#233; les donn&#233;es d&#8217;entra&#238;nement/d&#233;veloppement/test officielles.
</p>
<p>5.2 Param&#232;tres
</p>
<p>Les hi&#233;rarchies ont &#233;t&#233; entra&#238;n&#233;es par validation crois&#233;e (10-fold) sur les donn&#233;es d&#8217;entra&#238;nement
(d&#233;couper les hi&#233;rarchies se fait apr&#232;s avoir cumul&#233; les scores obtenus par la validation crois&#233;e) et
les param&#232;tres ont &#233;t&#233; optimis&#233;s par cat&#233;gorie de documents sur les donn&#233;es de d&#233;veloppement :
la s&#233;quence d&#8217;indicateurs obtenant le meilleur score moyen (entre MUC, B3 et CEAF) apr&#232;s
d&#233;codage a &#233;t&#233; s&#233;lectionn&#233; comme param&#232;tre optimal pour la cat&#233;gorie. Dans les r&#233;sultats, nous
appellerons best hierarchy la hi&#233;rarchie obtenue. Nous avons fix&#233; le nombre d&#8217;it&#233;rations du
Passive-Aggressive pour tous les mod&#232;les.
</p>
<p>Nos baselines sont le mod&#232;le initial avec les traits &#233;l&#233;mentaires (single model) et le mod&#232;le
gramtype (section 2) associ&#233;s &#224; chacun des d&#233;codeurs gloutons, et &#233;galement les versions o&#249; l&#8217;on
utilise ces d&#233;codeurs avec un &#233;chantillonnage particulier.
</p>
<p>Dans nos exp&#233;riences, nous ne prenons en compte que les mentions gold (pas de singletons ni
de non-r&#233;f&#233;rentiels). Cela n&#8217;est pas tout &#224; fait r&#233;aliste, mais notre but est de comparer les divers
mod&#232;les &#224; paires locaux plut&#244;t que de mettre en place un syst&#232;me complet de r&#233;solution. De plus,
nous voulons &#233;viter d&#8217;avoir &#224; consid&#233;rer trop de param&#232;tres dans nos exp&#233;riences.
</p>
<p>5.3 M&#233;triques d&#8217;&#233;valuation
</p>
<p>Nous utilisons les trois m&#233;triques les plus communes, &#224; savoir :
&#8211; MUC (Vilain et al., 1995) calcule pour chaque vrai cluster-entit&#233; le nombre de clusters syst&#232;me
</p>
<p>n&#233;cessaires pour le recouvrir. La pr&#233;cision est cette quantit&#233; divis&#233;e par la taille du vrai cluster
moins un. Le rappel est obtenu en inversant les clusters vrai et pr&#233;dits. Le F1 est la moyenne
harmonique du rappel et de la pr&#233;cision.
</p>
<p>&#8211; B3 (Bagga et Baldwin, 1998) calcule les scores de rappel et de pr&#233;cision pour chaque mention,
&#224; partir de l&#8217;intersection entre le cluster syst&#232;me et le vrai cluster pour cette mention. La
pr&#233;cision est le rapport des tailles de l&#8217;intersection et du cluster syst&#232;me, alors que le rappel est
le rapport des tailles de l&#8217;intersection et du vrai cluster. Les rappel et pr&#233;cision globaux et le F1
sont obtenus en prenant la moyenne sur les scores des mentions.
</p>
<p>TALN-R&#201;CITAL 2013, 17-21 Juin, Les Sables d&#8217;Olonne
</p>
<p>128 c&#65535; ATALA</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>&#8211; CEAF (Luo, 2005) : scores obtenus en calculant la meilleure bijection entre la vraie partition
et la partition syst&#232;me, ce qui est &#233;quivalent &#224; trouver l&#8217;alignement optimal dans le graphe
bipartite form&#233; par ces partitions. Nous utilisons la fonction de similarit&#233; &#966;4 de (Luo, 2005).
</p>
<p>Ces m&#233;triques ont &#233;t&#233; r&#233;cemment utilis&#233;es dans les Shared Task CoNLL-2011 et 2012. Par ailleurs,
ces campagnes utilisent une moyenne non pond&#233;r&#233;e sur les F1 scores donn&#233;s par ces trois
m&#233;triques. Comme cela est fait normalement, nous utilisons le mode micro-averaging (moyennes
sur le nombre de mention) lorsque nous donnons nos scores sur l&#8217;ensemble des donn&#233;es.
</p>
<p>5.4 R&#233;sultats
</p>
<p>Les r&#233;sultats obtenus par le syst&#232;me sont repris dans les tableaux 1, 2 et 3. Les &#233;chantillonnages
originaux associ&#233;s aux d&#233;codeurs Closest-First et Best-First sont d&#233;sign&#233;s par Soon et NgCardie.
single model correspond &#224; un mod&#232;le simple entra&#238;n&#233; sans &#233;chantillonnage sp&#233;cifique. Malgr&#233;
l&#8217;utilisation de d&#233;codeurs gloutons, nous pouvons observer sur la sortie un effet positif tr&#232;s
significatif sur la s&#233;paration des paires dans les mod&#232;les locaux. L&#8217;utilisation de mod&#232;les distincts
plut&#244;t qu&#8217;un seul mod&#232;le a un effet positif sur le score moyen, avec un incr&#233;ment de 6.4 &#224; 15.5
en fonction du d&#233;codeur. Il est int&#233;ressant de constater qu&#8217;ind&#233;pendamment du d&#233;codeur utilis&#233;,
le mod&#232;le gramtype surpasse toujours le single model, et est lui-m&#234;me d&#233;pass&#233; par le mod&#232;le best
hierarchy. Nous avons observ&#233; des variations dans le param&#232;tre optimal des hi&#233;rarchies, toutefois
un param&#232;tre fr&#233;quemment bien class&#233; &#233;tait : gramtype droite &#8594; gramtype gauche &#8594; m&#234;me
phrase&#8594; type d&#8217;entit&#233; nomm&#233;e droite.
</p>
<p>MUC B3 CEAF
P R F1 P R F1 P R F1 Mean
</p>
<p>Soon 79.49 93.72 86.02 26.23 89.43 40.56 49.74 19.92 28.44 51.67
single model 78.95 75.15 77.0 51.88 68.42 59.01 37.79 43.89 40.61 58.87
gramtype 80.5 71.12 75.52 66.39 61.04 63.6 43.11 59.93 50.15 63.09
</p>
<p>best hierarchy 83.23 73.72 78.19 73.5 67.09 70.15 47.3 60.89 53.24 67.19
</p>
<p>TABLE 1 &#8211; Scores sur CoNLL-2012 avec mentions gold, d&#233;codeur Closest-First.
</p>
<p>En regardant les trois diff&#233;rentes m&#233;triques, nous constatons que globalement, la s&#233;paration des
paires am&#233;liore B3 et CEAF (mais pas toujours MUC, &#224; cause du tr&#232;s gros rappel du single model)
apr&#232;s le d&#233;codage de la sortie : gramtype donne un meilleur score que le mod&#232;le simple, et best
hierarchy donne les plus hauts B3, CEAF et scores moyens.
</p>
<p>La meilleure combinaison de classifieur-d&#233;codeur r&#233;alise un score de 67.19, ce qui la placerait au
niveau des meilleurs syst&#232;mes qui ont pris part &#224; la CoNLL-2012 Shared Task sur la configuration
gold mentions (moyenne &#224; 66.41, le premier isol&#233; &#224; 77, les meilleurs suivants &#224; 68-69).
</p>
<p>MUC B3 CEAF
P R F1 P R F1 P R F1 Mean
</p>
<p>NgCardie 81.02 93.82 86.95 23.33 93.92 37.37 40.31 18.97 25.8 50.04
single model 79.22 73.75 76.39 40.93 75.48 53.08 30.52 37.59 33.69 54.39
gramtype 77.21 65.89 71.1 49.77 67.19 57.18 32.08 47.83 38.41 55.56
</p>
<p>best hierarchy 78.11 69.82 73.73 53.62 70.86 61.05 35.04 46.67 40.03 58.27
</p>
<p>TABLE 2 &#8211; Score sur CoNLL-2012 avec mentions gold, d&#233;codeur Best-First.
</p>
<p>TALN-R&#201;CITAL 2013, 17-21 Juin, Les Sables d&#8217;Olonne
</p>
<p>129 c&#65535; ATALA</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>MUC B3 CEAF
P R F1 P R F1 P R F1 Mean
</p>
<p>single model 83.15 88.65 85.81 35.67 88.18 50.79 36.3 28.27 31.78 56.13
gramtype 83.12 84.27 83.69 44.73 81.58 57.78 45.02 42.94 43.95 61.81
</p>
<p>best hierarchy 83.26 85.2 84.22 45.65 82.48 58.77 46.28 43.13 44.65 62.55
</p>
<p>TABLE 3 &#8211; Scores sur CoNLL-2012 avec mentions gold, d&#233;codeur Aggressive-Merge.
</p>
<p>6 Conclusion et perspectives
</p>
<p>Dans cet article, nous avons d&#233;crit une m&#233;thode pour construire un espace de traits s&#233;parant les
paires, en exploitant la lin&#233;arit&#233; et en combinant des indicateurs pour s&#233;parer les instances. Nous
avons mis en &#339;uvre une technique de programmation dynamique pour calculer efficacement
l&#8217;espace de traits fournissant la meilleure classification des paires parmi un tr&#232;s grand nombre
de possibilit&#233;s. Nous avons appliqu&#233; cette m&#233;thode pour optimiser le mod&#232;le &#224; paires dans un
syst&#232;me de r&#233;solution de la cor&#233;f&#233;rence. En testant diff&#233;rents d&#233;codeurs gloutons, nous avons
montr&#233; que cela apporte un gain significatif au syst&#232;me.
</p>
<p>Pour ce travail, nous n&#8217;avons consid&#233;r&#233; que des strat&#233;gies heuristiques standards pour cr&#233;er les
clusters telles que Closest-First et Best-First. Donc une extension naturelle de ce travail serait de
combiner notre m&#233;thode pour apprendre des mod&#232;les &#224; paires avec des strat&#233;gies de d&#233;codage
plus sophistiqu&#233;es (comme Mincut ou Integer Linear Programming). Nous pourrons alors &#233;valuer
l&#8217;impact des hi&#233;rarchies dans des conditions plus r&#233;alistes.
</p>
<p>Notre approche est adaptable dans le sens o&#249; elle peut s&#8217;appliquer avec des indicateurs tr&#232;s vari&#233;s.
Dans le futur, nous appliquerons les hi&#233;rarchies sur des espaces de traits plus fins pour pouvoir
obtenir des optimisations plus pr&#233;cises. Par ailleurs, &#233;tant donn&#233; que la m&#233;thode g&#233;n&#233;rale
de d&#233;coupage des hi&#233;rarchies n&#8217;est pas sp&#233;cifique &#224; la mod&#233;lisation des paires, mais peut
&#234;tre appliquer &#224; d&#8217;autres probl&#232;me ayant des aspects bool&#233;ens, nous projetons d&#8217;employer
les hi&#233;rarchies pour traiter d&#8217;autres t&#226;ches TAL (p.ex. d&#233;tection d&#8217;anaphoricit&#233;, classification de
relations de discours ou de relations temporelles).
</p>
<p>La s&#233;lection d&#8217;espaces avec les hi&#233;rarchies, si les indicateurs sont tous des traits du mod&#232;le,
s&#8217;apparente aux m&#233;thodes de noyaux polynomiaux. Il serait int&#233;ressant de les comparer. Par
ailleurs, nous pourrons d&#233;velopper cette m&#233;thode en utilisant des crit&#232;res statistiques pour choisir
les indicateurs et construire des hi&#233;rarchies de d&#233;part plus complexes que les hi&#233;rarchies-produits,
&#224; la mani&#232;re des arbres de d&#233;cision. Le param&#233;trage du syst&#232;me sera alors facilit&#233;.
</p>
<p>R&#233;f&#233;rences
</p>
<p>ARIEL, M. (1988). Referring and accessibility. Journal of Linguistics, pages 65&#8211;87.
BAGGA, A. et BALDWIN, B. (1998). Algorithms for scoring coreference chains. In Proceedings of
LREC 1998, pages 563&#8211;566.
BENGSTON, E. et ROTH, D. (2008). Understanding the value of features for coreference resolution.
In Proceedings of EMNLP 2008, pages 294&#8211;303, Honolulu, Hawaii.
CAI, J. et STRUBE, M. (2010). End-to-end coreference resolution via hypergraph partitioning. In
COLING, pages 143&#8211;151.
</p>
<p>TALN-R&#201;CITAL 2013, 17-21 Juin, Les Sables d&#8217;Olonne
</p>
<p>130 c&#65535; ATALA</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>CHEN, B., SU, J., PAN, S. J. et TAN, C. L. (2011). A unified event coreference resolution by
integrating multiple resolvers. In Proceedings of 5th IJCNLP, pages 102&#8211;110. Asian Federation of
Natural Language Processing.
</p>
<p>CRAMMER, K., DEKEL, O., KESHET, J., SHALEV-SHWARTZ, S. et SINGER, Y. (2006). Online passive-
aggressive algorithms. Journal of Machine Learning Research, 7:551&#8211;585.
</p>
<p>DENIS, P. et BALDRIDGE, J. (2008). Specialized models and ranking for coreference resolution. In
Proceedings of EMNLP 2008, pages 660&#8211;669, Honolulu, Hawaii.
</p>
<p>DENIS, P. et BALDRIDGE, J. (2009). Global joint models for coreference resolution and named
entity classification. Procesamiento del Lenguaje Natural, 43.
</p>
<p>KEHLER, A., APPELT, D., TAYLOR, L. et SIMMA, A. (2004). The (non)utility of predicate-argument
frequencies for pronoun interpretation. In Proceedings of HLT-NAACL 2004.
</p>
<p>KLENNER, M. (2007). Enforcing coherence on coreference sets. In Proceedings of RANLP 2007.
</p>
<p>LUO, X. (2005). On coreference resolution performance metrics. In Proceedings of HLT-
NAACL 2005, pages 25&#8211;32.
</p>
<p>MCCARTHY, J. F. et LEHNERT, W. G. (1995). Using decision trees for coreference resolution. In
IJCAI, pages 1050&#8211;1055.
</p>
<p>MORTON, T. (2000). Coreference for NLP applications. In Proceedings of ACL 2000, Hong Kong.
</p>
<p>NG, V. (2005). Supervised ranking for pronoun resolution : Some recent improvements. In
Proceedings of AAAI 2005.
</p>
<p>NG, V. et CARDIE, C. (2002). Improving machine learning approaches to coreference resolution.
In Proceedings of ACL 2002, pages 104&#8211;111.
</p>
<p>NICOLAE, C. et NICOLAE, G. (2006). Bestcut : A graph algorithm for coreference resolution. In
EMNLP, pages 275&#8211;283.
</p>
<p>PONZETTO, S. et STRUBE, M. (2006). Exploiting semantic role labeling, WordNet and Wikipedia
for coreference resolution. In Proceedings of the HLT 2006, pages 192&#8211;199, New York City, N.Y.
</p>
<p>PRADHAN, S., MOSCHITTI, A., XUE, N., URYUPINA, O. et ZHANG, Y. (2012). Conll-2012 shared task :
Modeling multilingual unrestricted coreference in ontonotes. In Joint Conference on EMNLP and
CoNLL - Shared Task, pages 1&#8211;40, Jeju Island, Korea. Association for Computational Linguistics.
</p>
<p>RAHMAN, A. et NG, V. (2011). Narrowing the modeling gap : a cluster-ranking approach to
coreference resolution. J. Artif. Int. Res., 40(1):469&#8211;521.
</p>
<p>SOON, W. M., NG, H. T. et LIM, D. (2001). A machine learning approach to coreference resolution
of noun phrases. Computational Linguistics, 27(4):521&#8211;544.
</p>
<p>URYUPINA, O. (2004). Linguistically motivated sample selection for coreference resolution. In
Proceedings of DAARC 2004, Furnas.
</p>
<p>URYUPINA, O., POESIO, M., GIULIANO, C. et TYMOSHENKO, K. (2011). Disambiguation and filtering
methods in using web knowledge for coreference resolution. In FLAIRS Conference.
</p>
<p>VERSLEY, Y., MOSCHITTI, A., POESIO, M. et YANG, X. (2008). Coreference systems based on kernels
methods. In COLING, pages 961&#8211;968.
</p>
<p>VILAIN, M., BURGER, J., ABERDEEN, J., CONNOLLY, D. et HIRSCHMAN, L. (1995). A model-theoretic
coreference scoring scheme. In Proceedings fo the 6th Message Understanding Conference (MUC-6),
pages 45&#8211;52, San Mateo, CA. Morgan Kaufmann.
</p>
<p>TALN-R&#201;CITAL 2013, 17-21 Juin, Les Sables d&#8217;Olonne
</p>
<p>131 c&#65535; ATALA</p>

</div></div>
</body></html>