TALN-RECITAL 2013, 17-21 Iuin, Les Sables d’Olonne

Construction d’un large corpus écrit libre annoté
morpho-syntaxiquement en francais

Nicolas Hernandez Florian Boudin
Université de Nantes
nicolas . herna.ndez@univ—na.ntes . fr , florian . boudin@u.niv—na.ntes . fr

RESUME
Cet article étudie la possibilité de créer un nouveau corpus écrit en francais annoté morpho-
syntaxiquement a partir d’un corpus annoté existant. Nos objectifs sont de se libérer de la licence
d’exploitau'on contraignante du corpus d’origine et d’obtenir une modernisation perpétuelle des
textes. Nous montrons qu’un corpus pré—annoté automatiquement peut permettre d’entrainer un
étiqueteur produisant des performances état—de—l’art, si ce corpus est sufﬁsamment grand.

ABSTRACT
Construction of a Free Large Part-of-Speech Annotated Corpus in French

This paper studies the possibility of creating a new part—of—speech annotated corpus in French
from an existing one. The objectives are to propose an exit from the restrictive licence of the
source corpus and to obtain a perpetual modernisation of texts. Results show that it is possible
to train a state—of—the—art POS—tagger from an automatically tagged corpus if this one is large
enough.

MOTS-CLES : corpus arboré, construction de corpus, étiquetage morpho—syntaxique.

KEYWORDS: French treebank, Building a corpus, Part-of-Speech Tagging.

1 Introduction

L’entrainement et le test de systemes statistjques de Traitement Automatique des Langues (TAL)
requierent la disponibilité de larges corpus annotés (Hajiéové et al., 2010). Force est de constater
que la communauté scientiﬁque est pauvre en corpus écrits en francais librement accessibles,
annotés en quantité et en qualité sufﬁsantes avec des analyses linguistiques structurelles (seg-
mentation des textes en titres, paragraphes, phrases et mots), morpho—syntaxiques (parties du
discours, lemme, genre, nombre, temps...) et syntaxiques (en constituants et en dépendances)
qui constituent les pré—traitements de la plupart des applications du TAL. Nous reprenons ainsi a
notre compte des propos énoncés pres de dix ans plus tot dans (Salmon—Alt LL, 2004). Dans
cet article nous nous intéressons a l’entrainement d’étiqueteurs morpho—syntaxiques pour traiter
des écrits en francais ainsi qu’a la construction des corpus annotés associés.

Parmi les corpus écrits annotés et en francais que nous recensons, nous comptons PAROLE 1 et
MULTEXT JOC2 (Véronis et Khouri, 1995), le French Treebank (P7T) (Abeillé et al., 2003), la base

1. http : //catalog . elra . info/product_info . php?products_id=565
2. http : //catalog . elra . info/product_info . php?products_id=534

160 © ATALA

TALN—RECITAL 2013, 17-21 Iuin, Les Sables d’Olonne

FREEBANK (Sa1mon—A1t e:tja1., 2004) et le récent corpus Sequoia 3 (Candito et Seddah, 2012).
Excepté la FREEBANK, ces corpus sont toujours accessibles aujourd’huiv1'a un guichet sur le Web.
La FREEBANK, dont la motivation était le recueil collaboratif, la construction et le partage de
corpus libres annotés en francais a malheureusement disparu dans les limbes du Web 4 du fait de
la difﬁculté d’acquisition de textes libres et du coﬁt de réalisation d’une telle entreprise.

Le P7T 5 est probablement le corpus annoté le plus utilisé et le plus référencé, et ce essentiellement
pour trois raisons : il est libre d’usage pour des activités de recherche, il bénéﬁcie d’une analyse
mulu'—niveaux (de la structure textuelle a la structure syntaxique en passant par des annotations en
morphologie) et il compte pres du double de mots annotés que tous les autres corpus disponibles
réunis. En pratique ce corpus se compose d’artic1es journalistiques issus du journal Le Monde
écrits dans les années 90, soit plus de 500 000 mots annotés. Ainsi (Candito LL, 2010a) utilisent
la structure en constituants du P7T pour construire une structure en dépendances et permettre
1’entrainement d’analyseurs syntaxiques statistiques en dépendance du francais (Candito LL,
2010b). (Sagot et al., 2012) l’enrichissent avec des annotations référentielles en entités nommées.
Tandis que (Danlos L1,, 2012) projettent de l’uti1iser comme base d’annotations discursives.

11 y a néanmoins quelques problémes associés a l’uti1isation du corpus P7T dans une optique de
développement de systémes statistiques de TAL.

1. Le premier probléme concerne la faible adéquation du modéle théorique linguistique avec
la tache d’entrainement a laquelle on le destine. (Sch1uter et van Genabith, 2007; Crabbé
et Candito, 2008) montrent qu’en remaniant certaines annotations syntaxiques et le jeu
d’étiquettes, il est possible d’améliorer les performances des systémes entrainés avec ce
corpus. Un autre aspect du probléme porte sur la notion de mots composés déﬁnie par les
auteurs. Celle-ci est trés large et a pour conséquence de rendre difﬁcilement reproductible
la segmentation du P7T par un systéme automatique non entrainé sur cette ressource.
Cette conséquence conduit a s’interroger sur la pertinence d’utiliser des modélisations
construites sur ce corpus pour traiter d’autres corpus. (Candito et Seddah, 2012), par
exemple, décident de restreindre cette déﬁnition et d’aborder 1e traitement des formes les
plus ouvertes (composés nominaux et verbaux) qu’au niveau syntaxique. En comparaison,
le Penn T reebank 6, qui constitue la référence pour l’ang1ais—américain (Marcus LL, 1993;
Gabbard L1,, 2006), favorise un découpage en mots simples 7 en privilégiant la rupture
pour les mots joints. Il est néanmoins important de rappeler que le P7T a initialement été
créé avec une motivation différente de la notre aujourd’hui a savoir la construction de

ressources lexicales de type dictjonnaire 8.

2. Le second probléme est plus technique et concerne la relative inadéquation du schéma
XML de représentation des annotations pour des taches automatiques ainsi que le manque
de consistance de la structure d’annotation. La représentation des amalgames en deux
éléments XML distincts qui se retrouvent distribués dans différentes conﬁgurations selon
qu’ils se produisent en partie dans un mot composé est une situation difﬁcile a traiter
automatiquement car elle oblige a énumérer tous les cas possibles. Certains éléments n’ont
pas systématiquement tous leurs attributs, d’autres ont des noms d’attribut erronés. .. Ces

. https://www.rocq.inria.fr/a1page—wiki/tiki—index.php?page=CorpusSequoia
. http://web.archive.org/web/20081215041844/http://freebank.1oria.fr/

. http://www.11f.cnrs.fr/Gens/Abeille/French-Treebank—fr.php

. http://www.ldc.upenn.edu/Catalog/cata1ogEntry.jsp?cata1ogId=LDC99T42

. http://www.cis.upenn.edu/"treebank/tokenization.htm1

. http://www.11f.cnrs.fr/Gens/Abeille/guide—morpho—synt.02.pdf

161 © ATALA

<X!\lO’\U'I-#03

TALN-RECITAL 2013, 17-21 Iuin, Les Sables d’Olonne

Cette phase de post—édition, connue comme étant toujours nécessaire, constitue une entreprise
coﬁteuse en temps et pécuniairement. (Fort et Sagot, 2010) montrent néanmoins qu’il sufﬁt d’un
petit corpus d’entrainement pour construire un systeme produisant une pré—annotau'on de qualité
suffisante pour permettre une annotation par correction plus rapide qu’une annotation manuelle.
Dans ce travail, nous ne nous situons pas dans une perspective d’un post—traitement correctif
manuel.

Différentes techniques ont été proposées pour rendre plus ﬁable l’assignation automatique
d’étiquettes ainsi que pour faciliter le travail des annotateurs en détectant (voire en corrigeant)
les erreurs d’annotation. En ce qui concerne l’assignation automatique, (Clark etTal., 2003)
utilisent deux étiqueteurs morpho—syntaxiques pour annoter de nouvelles données et étendre
leur corpus d’entrainement avec une sélection de celles—ci. Leur idée consiste a sélectionner
les phrases qui maximisent l’accord d’annotation entre les étiqueteurs et d’ajouter celles-ci aux
données d’entrainement, puis de recommencer la procédure. Les auteurs constatent que le co-
entrainement permet d’améliorer la performance des systémes entrainés a partir d’une quantité
de données manuellement annotée trés faible. Cette approche trouve son utilité lorsque l’on
dispose de peu de quantité de données annotés pour entrainer un systeme.

L’idée de combiner plusieurs étiqueteurs se retrouve dans d’autres travaux. (Loftsson LL, 2010),
par exemple, entrainent cinq étiqueteurs sur un méme corpus (le corpus Icelandic Frequency
Dictionary (IFD)), et utilisent leur combinaison pour annoter un second corpus. La combinaison 25
se fait par vote a la majorité et par degré de conﬁance dans les étiqueteurs en cas d’égalité. Le
résultat de cette combinaison est ensuite sujet a la détection d’erreurs en utilisant la détection
d’incohérences entre un étiquetage en constituants fourni par un outil tiers et l’éu'quetage morpho-
syntaxique des mots contenus dans les constituants (Loftsson, 2009). La correction effective
des erreurs est ensuite réalisée manuellement. Les auteurs montrent que la combinaison des
étiqueteurs permet d’augmenter la précision de 1’étiquetage comparativement aux performances
individuelles de chacun des étiqueteurs. La raison invoquée pour expliquer le phénomene est que
les différents étiqueteurs produisent différentes erreurs et que cette différence peut souvent étre
exploitée pour conduire a de meilleurs résultats.

Sur le francais, le travail qui se rapproche le plus de ces efforts est celui de (Dejean LL, 2010)
pour qui le développement d’un corpus annoté morpho-syntaxiquement reste avant tout un
moyen d’atteindre leur objectif : construire un étiqueteur morpho-syntaxique libre du francais.
Les auteurs observent (aprés alignement des jeux d’étiquettes) les divergences d’annotations des
étiqueteurs de (Brill, 1994) (BRILL) et de (Schmid, 1994) (TREETAGGER). Ces observations les
conduisent a émettre des regles correctives sur le résultat de la combinaison de ces étiqueteurs,
qu’ils utilisent pour entrainer un étiqueteur état—de-l’art. Leurs expérimentations sont réalisées
sur un corpus de pres de 500 000 mots construit a partir d’extraits de Wikipédia, Wikiversity
et Wikinews. L’étiqueteur est entrainé sur une partie du corpus et ses résultats sont comparés
sur une autre partie par rapport aux sorties produites par 1’étiqueteur BRILL. Le fait que la mise
au point des étiqueteurs BRILL et TREETAGGER n’aient pas été réalisée sur un méme corpus ainsi
que l’absence de corpus de référence pour évaluer les étiquetages produits, rendent difﬁcile
l’interprétau'on de ces résultats.

Aﬁn d’assister la tache de correction de corpus annotés, (Dickinson et Meurers, 2003) proposent,
dans le cadre du projet DECCA 26, de s’appuyer sur l’observation des variations d’annotations

25. http : //combitagger . sourcef orge . net
26. http : //decca. osu . edu

170 © ATALA

TALN-RECITAL 2013, 17-21 Iuin, Les Sables d’Olonne

associées a un méme n—gramme de mots pour trouver des erreurs d’étiquetage. L’hypothése qu’ils
font est qu’un mot ambigu peut avoir différentes étiquettes dans différents contextes mais plus
ses contextes d’occurrences sont similaires, plus rare devrait étre la variation d’étiquetage; et
par conséquent plus grande devrait étre la probabilité qu’il s’agisse d’une erreur. Appliqué sur le
corpus du Wall Street Journal (WSJ), il observe que 97,6% des variations ramenées pour des
n—grammes de taille supérieure a 6 constituent des erreurs effectives.

Poursuivant le méme objectif, (Loftsson, 2009) s’appuie sur cette technique ainsi que sur deux
autres : le vote de plusieurs étiqueteurs automatiques et la cohérence de l’étiquetage morpho-
syntaxique des mots en regard d’une analyse en constituants des phrases. I1 observe que ces
techniques permettent individuellement de détecter des erreurs et qu’elles agissent en complé—
mentarité; ce qui lui permet de corriger manuellement 0,23% (1 334 tokens mots) du corpus
IFD. Nous notons que les deux premieres techniques ne sont pas dépendantes de la langue mais
que la derniére repose sur 1’écriture de régles ad’hoc issues de l’observation des données.

(Boudin et Hernandez, 2012) appliquent sur le P7T des techniques de détection d’erreurs
fondées sur les travaux de (Dickinson et Meurers, 2003) ainsi que des heuristiques pour assigner
automatiquement des étiquettes morpho—syntaxiques aux mots composants. Ils montrent que ces
corrections améliorent les performances de systémes d’étiquetage état—de—l’art.

5 Conclusion et perspectives

Dans cet article nous montrons qu’a partir d’une certaine quantité de données pré-annotées
automatiquement il est possible d’entrainer des étiqueteurs morpho—syntaxiques qui produisent
des résultats équivalents a des systemes entrainés sur des données validées manuellement. La
conséquence directe de ce résultat découle de la nature des données utilisées pour ces expériences
(a savoir Wikinews et Europarl) : il est possible de construire un corpus libre annoté morpho-
syntaxiquement offrant une modernisation perpétuelle des textes et qui puisse servir de base pour
entrainer des étiqueteurs morpho—syntaxiques statistiques produisant des analyses état—de—l’art.

Les perspectives a ce travail sont triples : d’abord conﬁrmer la qualité de l’étiquetage automatique
des annotations morpho—syntaxiques du corpus ainsi construit, ensuite étendre les annotations
du corpus a d’autres niveaux d’analyse, et enﬁn diffuser librement la ressource par un moyen qui
permette un enrichissement collaboratif. Concernant l’amélioration de la qualité d’étiquetage,
(Schluter et van Genabith, 2007; Loftsson et al., 2010; Boudin et Hernandez, 2012) ont montré
des pistes pour la détection et la correction d’erreurs par des procédures automatiques en utilisant
la détection de variations d’étiquetage ou la combinaison de multiples étiqueteurs. Concernant
1’extension du corpus a d’autres niveaux d’analyses, (Candito et Seddah, 2012) utilisent pour le
projet Sequoia différentes techniques pour pré-annoter automatiquement le niveau syntaxique
avec des analyses en constituants et en dépendances. Les solutions mises en oeuvre dans le
projet DECCA (cf. note 26) permettent d’envisager la détection d’erreurs a ces niveaux. L’une des
difﬁcultés sera de voir s’il est possible d’automatiser certaines corrections comme dans (Boudin et
Hernandez, 2012) ainsi que de voir si la taille des données annotées a une incidence sur la qualité
des systémes entrainés. L’enjeu de la mise au point de telles techniques est énorme puisqu’il s’agit
de pouvoir offrir a la communauté un large corpus annoté croissant continuellement sous une
licence d’exploitau'on offrant a l’utilisateur le droit de copier, modiﬁer et utiliser la ressource pour
la ﬁnalité qu’il souhaite.

171 © ATALA

TALN-RECITAL 2013, 17-21 Iuin, Les Sables d’Olonne
Références

ABEILLE, A., CLEMENT, L. et TOUssENEL, E (2003). Building and using Parsed Corpora, chapitre
Building a treebank for French. Language and Speech series, Kluwer, Dordrecht.

ARUN, A. et KELLER, E (2005). Lexicalization in crosslinguistic probabilistic parsing : The case
of French. I_n Proceedings of the 43rd Annual Meeting of the Association for Computational
Linguistics (ACL’05), pages 306-313, Ann Arbor, Michigan.

BEN0lT, S. et BOULLIER, P (2008). Sxpipe 2 : architecture pour le traitement pré—syntaxique de
corpus bruts. Traitement Automatique des Langues, 49 (2):155—188.

BENZITOUN, C., FORT, K. et SAGOT, B. (2012). TCOF—POS : un corpus libre de francais parlé
annoté en morphosyntaxe. I_n Actes de la conférence conjointe JEP-TALN—RECITAL, pages 99-
112, Grenoble, France. Quaero.

BOUDIN, E et HERNANDEZ, N. (2012). Détection et correction automatique d’erreurs d’an—
notation morpho—syntaxique du french treebank. I_n Proceedings of the Joint Conference
JEP-TALN-RECITAL 2012, volume 2 : TALN, pages 281-291, Grenoble, France. ATALA/AFCP
BRILL, E. (1994). Some advances in rule—based part of speech tagging. I_n Proceedings of the
Twelfth National Conference on Artiﬁcial Intelligence (AAAI), pages 722-727.

CANDITO, M. et SEDDAH, D. (2012). Le corpus Sequoia : annotation syn-
taxique et exploitation pour l’adaptation d’analyseur par pont lexical. I_n
19e conférence sur le Traitement Automatique des Langues Naturelles, Grenoble, France.

CANDITO, M.—H., CRABBE, B. et DENIS, P. (2010a). Statistical french dependency parsing : Treebank
conversion and first results. I_n Proceedings of LREC, Valletta, Malta.

CANDITO, M.—H., NIVRE, J ., DENIs, P. et ANGUIANO, E. H. (2010b). Benchmarking of statistical
dependency parsers for french. I_n COLING’2010 (poster session), Beijing, China.

CLARK, S., CURRAN, J. et OSBORNE, M. (2003). Bootstrapping pos—taggers using unlabelled data.
I_n DAELEMANS, W. et OSBORNE, M., éditeurs : Proceedings of the Seventh Conference on Natural
Language Learning at HLT-NAACL 2003, pages 49-55.

CONsTANT, M., TELLIER, I., DUCHIER, D., DUPONT, Y., SIGOGNE, A. et BILLOT, S. (2011). Intégrer
des connaissances linguistiques dans un CRF : application a l’apprentissage d’un segmenteur—
étiqueteur du francais. I_n Actes de la 18e conférence sur le Traitement Automatique des
Langues Naturelles (TALN’2011), Montpellier, France.

CRABBE, B. et CANDITO, M. (2008). Expériences d’ana1yse syntaxique statistique du francais. I_n
Actes de la 15eme conférence sur le Traitement Automatique des Langues Naturelles (TALN),
Avignon, France.

DANLOs, L., ANTOL1NOs—BAssO, D., BRAUD, C. et ROZE, C. (2012). Vers le FDTB : French Discourse
Tree Bank. I_nActes de la 19e conférence sur le Traitement Automatique des Langues Naturelles
(TALN), pages 471—478, Grenoble, France.

DEJEAN, C., FORTUN, M., MASSOT, C., POTTIER, V, POULARD, F. et VERNIER, M. (2010). Un
étiqueteur de r6les grammaticaux libre pour le francais intégré a Apache UIMA. I_n
Actes de la 17e Conférence sur le Traitement Automatique des Langues Naturelles, Montréal,
Canada.

DENIs, P. et SAGOT, B. (2010). Exploitation d’une ressource lexicale pour la construction d’un
étiqueteur morpho—syntaxique état—de—l’art du francais. I_n Actes de la 17e conférence sur le
Traitement Automatique des Langues Naturelles (TALN’2010), Montréal, Canada.

172 © ATALA

rALN—REcIrAL 2013, 17-21 Iuin, Les Sables d’Olonne

DICKINSON, M. et MEURERS, W. D. (2003). Detecting errors in part—of—speech annotation.
I_n Proceedings of the 10th Conference of the European Chapter of the Association for
Computational Linguistics (EACL—03), pages 107-114, Budapest, Hungary.

FORT, K. et SAGoT, B. (2010). Inﬂuence of pre—annotation on pos—tagged corpus development.
I_n Proceedings of the Fourth Linguistic Annotation Workshop, pages 56-63, Uppsala, Sweden.
Association for Computational Linguistics.

GABBARD, R., MARCUS, M. et KULICK, S. (2006). Fully parsing the penn treebank. I_n Proceedings
of the main conference on Human Language Technology Conference of the North American
Chapter of the Association of Computational Linguistics, HLT—NAACL ’06, pages 184-191,
Stroudsburg, PA, USA. Association for Computational Linguistics.

GREEN, S., de MARNEFFE, M.—C., BAUER, J. et MANNING, C. D. (2011). Multiword expression
identification with tree substitution grammars : A parsing tour de force with french. I_n EMNLP.

HAJICOVA, E., ABEILLE, A., HAJIC, J., MIROVSKY, J. et URESOVA, Z. (2010). Handbook of Natural
Language Processing, chapitre Treebank Annotation. Chapman 8: Hall/CRC.

KOEHN, P. (2005). Europarl : A parallel corpus for statistical machine translation. I_n MT Summit.
LOFTSSON, H. (2009). Correcting a POS-tagged corpus using three complementary methods. I_n

Proceedings of the 12th Conference of the European Chapter of the ACL (EACL 2009), pages
523-531, Athens, Greece. Association for Computational Linguistics.

LOFTSSON, H., YNGvASoN, J. H., HELGADOTTIR, S. et RoGNvALDSSoN, E. (2010). Developing a
pos—tagged corpus using existing tools. I_n Proceedings of LREC.

MARCUS, M. P., MARCINKIEWICZ, M. A. et SANTORINI, B. (1993). Building a large annotated corpus
of english : the penn treebank. Computational Linguistics, 19(2):313—330.

NASR, A., BECHET, F. et REY, J .—F. (2010). Macaon : Une chaine linguistique pour le traite—
ment de graphes de mots. I_n Traitement Automatique des Langues Naturelles — session de
démonstrations, Montréal.

NIELSEN, M. (2011). Reinventing Discovery : The New Era of Networked Science. Princeton,
N.J. Princeton University Press.

SAGoT, B., RICHARD, M. et STERN, R. (2012). Annotation référentielle du Corpus Arboré de
Paris 7 en entités nommées. I_n Actes de la 19e conférence sur le Traitement Automatique des
Langues Naturelles (TALN), pages 535-542, Grenoble, France.

SALMoN—ALT, S., BICK, E., ROMARY, L. et PIERREL, J.—M. (2004). La FReeBank : vers une base libre
de corpus annotés. I_n Traitement Automatique des Langues Naturelles — TALN’04, Fes, Maroc.

SCHLUTER, N. et van GENABITH, J . (2007). Preparing, restructuring, and augmenting a french
treebank : lexicalised parsers or coherent treebanks ? I_n Proceedings of the 10th Conference of
the Paciﬁc Association for Computational Linguistics (PACLING), Melbourne, Australia.

SCHMID, H. (1994). Probabilistic part—of—speech tagging using decision trees. I_n Proceedings of
the Conference on New Methods in Language Processing, Manchester, UK.

ToUrANovA, K., KLEIN, D., MANNING, C. et SINGER, Y. (2003). Feature—rich part—of—speech tagging
with a cyclic dependency network. I_n Proceedings of the 3rd Conference of the North American
Chapter of the ACL (NAACL 2003), pages 173-180. Association for Computational Linguistics.

VERONIS, J. et KHOURI, L. (1995). Etiquetage grammatical multilingue : le projet multext.
Traitement Automatique des Langues, 36(1/2):233—248.

173 © ATALA

TALN-RECITAL 2013, 17-21 Iuin, Les Sables d’Olonne

inconsistances sont relevées dans de nombreux travaux (Arun et Keller, 2005; Schluter et
van Genabith, 2007; Green et al., 2011; Candito et Seddah, 2012; Boudin et Hernandez,
2012) qui militent en faveur d’une amélioraﬁon voire d’une réorganisation de la structure
du P7T avant de pouvoir l’utiliser dans toute étude sérieuse.

3. Généralement les mémes auteurs ont aussi observés des inconsistances au niveau d’anno-
tations. Certains, comme (Schluter et van Genabith, 2007; Boudin et Hernandez, 2012),
mettent en oeuvre des techniques automatiques pour détecter et corriger des erreurs d’éti-
quetage morpho—syntaxique. Le nombre d’erreurs de ce type est souvent minime ramené
au nombre de mots annotés. Sur les 628 767 tokens mots considérés dans (Boudin et
Hernandez, 2012) par exemple, seulement 169 ont été considérés comme ayant une erreur
d’étiquette. Le corpus étant construit semi—automatiquement (d’abord analysé automati-
quement puis validé manuellement), ce type de probleme illustre le fait que la validation
humaine ne garantie pas l’absence d’erreurs sur un large corpus.

4. Le quatriéme probléme que nous relevons résulte d’un parti pris que nous prenons9.

Nous estimons en effet que sa licence d’exploitation n’est pas adaptée pour favoriser son
utilisation dans le monde de la recherche. Bien que la licence permette des utilisations
avec des ouu'ls propriétaires et a des ﬁns commerciales moyennant ﬁnance, elle n’autorise
pas la modiﬁcation et la diffusion libre des modiﬁcations du corpus. Cela a pour principale
conséquence de ralentir voire de décourager les contributions extérieures et l’amélioration
de la ressource (par exemple pour corriger les problemes précédemment cites).

5. Les données annotées sont des textes mono—genres Vieux de pres de vingt ans encodés en
Iso—8859-1. On peut se poser la question de la robustesse et de la précision des systémes
entrainés sur ceux—ci pour traiter des textes plus récents (qui présentent de nouveaux
phénomenes linguistiques et des caracteres encodés en UTF-8, qui est le standard de facto
aujourd’hui pour encoder des textes en francais) et/ou de genre different.

6. Meme s’i1 constitue le plus gros corpus annoté disponible pour le francais, on peut s’inter—
roger sur la représentaﬁvité d’un corpus d’un demi—million de mots pour la construction de
systemes automatiques. A titre de comparaison, le Penn Treebank compte en corpus écrits
pres de 2,4 millions de mots annotés morphologiquement et syntaxiquement et couvrent le
domaine journalistique (Wall Street Journal) et l’ang1ais général (Brown).

Comparativement, PAROLE et MULT EXT JOC ont aussi des licences restrictives, le Sequoia offre
quant a lui le plus de libertés 1° aux utilisateurs. Aucun des corpus n’est de taille comparable a
celle du P7T. Ils comptent respectivement 250 000, 200 000 et 72 311 mots annotés morpho-
syntaxiquement. Excepté en partie pour le Sequoia, les textes datent des années 80 et 90.

Dans cet article, nous ré—ouvrons la question de la construction de corpus annotés libres en
francais. Une conjoncture a la fois sociétale, politique, technique et scientiﬁque nous y conduit.
En effet nous bénéﬁcions aujourd’hui d’au moins deux sources de contenu libres et multilingues,
en croissance perpétuelle et comptant déja plusieurs millions de mots, a savoir les projets de

9. Nous nous situons dans une démarche de recherche scientiﬁque <<ouverte>> (Nielsen, 2011).

10. LGPL-LR (Lesser General Public License For Linguistic Resources). Les auteurs ne précisent pas l’objet désigné
par la licence. Celle-ci doit se restreindre aux annotations produites et ne peut comprendre les textes. Le corpus est
composé de textes de quatre origines. On note que le journal Est Republicain diffusé par le CNRTL est sous licence
CC-BY-NC-SA 2.0 FR qui par sa clause de non-diffusion commerciale s’oppose a la LGPL-LR. La licence de wikipedia
(CC-BY-SA 3.0) ne semble pas contredire cette licence. La licence de Europarl et de EMEA manque de précision sur les
droits d’usage mais autorise la reproduction.

162 © ATALA

TALN-RECITAL 2013, 17-21 Iuin, Les Sables d’Olonne

la Wikimedia Foundation 11 (Wikipedia, Wikinews. . .) et les actes du Parlement Européen 12
(Europarl) tels que remaniés par (Koehn, 2005). Nous nous intéresserons ici aux écrits en francais
de Wikinews et de Europarl. La version en francais de Janvier 2013 de Wikinews compte plus
de 28 000 articles d’actualité (soit plus de 2,5 millions de mots sur pres de 90 000 phrases) et
couvre une période s’étalant de Janvier 2005 2‘: nos jours. La section en francais de la version 7
(mai 2012) du corpus Europarl compte, quant a elle, plus de 61,5 millions de mots (plus de 2
millions de phrases) et couvre une période s’étalant de 1996 a 2011. Les textes du premier sont
disponibles sous licence 13 Creative Commons Attribution 2.5 (CC—BY 2.5) (les versions antérieures
a Septembre 2005 sont dans le domaine public) qui permet a l’utilisateur d’utiliser, de modiﬁer
et de diffuser la ressource et ses modiﬁcations comme il le souhaite moyennant l’obligau'on d’en
citer l’auteur. Les textes du second sont libres de reproduction 14.

Dans les sections suivantes, nous nous interrogeons sur la possibilité d’exploiter des données
pré—annotées automatiquement pour construire un systeme ayant des performances similaires
a des systemes entrainés sur des données validées manuellement. Nous proposons notamment
d’observer comment la taille des données pré—annotées automatiquement peut jouer un r6le dans
la performance d’un étiqueteur morpho—syntaxique entrainé sur celles—ci.

2 Cadre expérimental

Dans cette section, nous présentons les données, le jeu d’étiquettes et l’étiqueteur que nous
utilisons (section 2.1). Nous présentons aussi les pré—traitements opérés sur les données pour les
exploiter (sections 2.2 et 2.3) ainsi que le protocole d’évaluation de nos expériences (section 2.4).

2.1 Données, jeu d’étiquettes et étiqueteur

Pour nos expérimentations nous utilisons tour a tour le corpus P7T comme données d’entraine-
ment et de test. Le corpus Sequoia est aussi utilisé selon les expériences.

Nous utilisons les parties en francais du Wikinews et d’Europarl comme données non étiquetées.
Nous filtrons les phrases courtes (i.e. inférieures a 5 tokens) de chaque document et nettoyons
la syntaxe wiki de Wikinews. L’ensemble de données ainsi généré posséde plusieurs avantages.
Tout d’abord, Wikinews est du méme genre que le P7T (journalistique). La différence de genre
avec Europarl permet de discuter de la portabilité de l’approche a des genres différents. Ensuite
ces corpus possédent une taille bien supérieure au P7T ; environ quatre fois supérieure pour
Wikinews et soixante fois pour Europarl. Enﬁn la licence associée a ces ressources permettent de
les distribuer librement accompagnées des annotations que nous générons.

Le jeu de catégories morpho-syntaxiques que nous utilisons est celui mis ou point par (Crabbé
et Candito, 2008), contenant 28 catégories qui combinent différentes valeurs de traits morpho-
syntaxiques du P7T. Outre le fait que ce jeu soit plus complet que les catégories du P7T, qui

11. http : //wikimediafoundation . org

12. http : //www . statmt . org/europar1/

13. http : //dumps . wikimedia. org/legal . html

14. «Except where otherwise indicated, reproduction is authorised, provided that the source is acknowledged.»
http : //www . europarl . europa . eu/guide/publisher/default _en . htm

163 © ATALA

TALN-RECITAL 2013, 17-21 Iuin, Les Sables d’Olonne

elles sont au nombre de 13, les auteurs montrent que les performances d’un étiqueteur entrainé
sur de telles annotations sont meilleures. Par ailleurs, son utilisation facilite l’acces a d’autres
ressources tels que les analyseurs syntaxiques statistiques en dépendance du francais qui ont
déja été développés a partir de ce jeu d’étiquettes 15 (Ma1tParser, MSTParser, Berkeley Parser)
(Candito et al., 2010b). Par la suite nous ferons référence a ce jeu d’étiquette par le nom P7T+.
Par abus ce nom désignera aussi le corpus P7T avec des étiquettes converties en P7T+.

En ce qui concerne l’étiqueteur morpho—syntaxique que nous avons utilisé pour nos expériences,
il s’agit de la version 3.1.3 du Stanford POS Tagger (Toutanova et al., 2003). Ce systéme utilise un
modele par maximum d’entropie, et peut atteindre des performances au niveau de l’état—de—l’art
en francais (Boudin et Hernandez, 2012). Nous utilisons un ensemble standard 16 de traits
bidirectionnels sur les mots et les étiquettes.

2.2 Segmentation en mots

Le P7T fournit des analyses linguistiques qui reposent sur une segmentation en mots simples et
en mots composés. Les mots composant les composés (nous appelons <<mots composants» les mots
qui composent les mots composés) sont signalés mais seulement un sous—ensemble bénéﬁcie d’une
catégorie grammaticale et aucun d’eux ne bénéﬁcie des autres traits (sous—catégorie, ﬂexions
morphologiques et lemme). Excepté le lemme, ces traits sont requis pour la conversion en P7T+.

La notion de composé dans le P7T est trés large (cf. note 8). La composition se justiﬁe par des
criteres aussi bien graphiques que morphologiques, syntaxiques et sémantiques. La segmentation
en unités lexicales n’est pas un probleme trivial. De nombreuses marques de ponctuation (apos-
trophe, virgule, tiret, point et espace) sont ambigués, et suivant la situation, jouent le r6le de
joint ou de séparateur. Cela conduit la majorité des systémes de segmentation (Benoit et Boullier,
2008; Nasr et al., 2010; Constant et al., 2011) a exploiter, en complément de régles générales,
des listes de formes ﬁnies ou régulieres a considérer comme unités lexicales. La segmentation en
composés du P7T résulte d’un processus d’annotation a la fois manuel et a base de lexiques non
précisément référencés. Outre la difﬁculté a reproduire automatiquement cette segmentation,
il n’y a pas d’enjeu a chercher a le faire car celle—ci est avant tout ad hoc a une période et un
genre de textes. Motivés par la volonté d’entrainer des analyseurs robustes aﬁn de pouvoir traiter
des textes pour lesquels des dictionnaires de mots composés ne seraient pas disponibles, nous
avons souhaité nous abstraire au maximum de la notion de composé du P7T. Nous n’avons ainsi
considéré comme unités lexicales que les composés consistant en des unités graphiques exemptes
d’espace ou ceux consistant en des formes numérales régulieres (e.g. «20 000», «50,12», «deux
cent vingt—et—un>>), lesquelles peuvent admettre des espaces. Certains mots composants sont
donc amenés a étre considérés comme unités lexicales. 11 en découle le besoin de déterminer
les traits morpho—syntaxiques manquants de ceux—ci aﬁn de pouvoir leur affecter une étiquette
P7T+ (cf. section 2.3). Le P7T compte 6 791 lemmes distincts de mots composés qui ne sont pas
des unités graphiques (i.e. ne contenant pas d’espace) soient 26 648 occurrences. 1 892 de ces
lemmes de mots composés ont au moins un de leur composant sans étiquette grammaticale. Cela
représente 7 795 occurrences. 1 106 n’ont aucune étiquette a leurs composants.

Pour les données autres que le P71; nous utilisons dans nos expériences le segmenteur KEA 17.

15. http : //alpage . inria . fr/statgram/frdep/fr_stat_dep_parsing . html
16. Nous avons utilisé la macro generic ,naac12003u.uk11ow11s décrite dans (Toutanova et al., 2003).
1 7. https : //github . com/boudinf 1/kea

164 © ATALA

TALN-RECITAL 2013, 17-21 Iuin, Les Sables d’Olonne
2.3 Révision et extension du P7T

Aﬁn de faciliter le traitement automatique ultérieur du P7T, nous réalisons des opérations de
révision et d’extension de la forme et du contenu. Nous envisageons a terme la construction de
modélisations de toutes les informations disponibles dans le P7T pour des systémes prédictifs
statistiques. Nous avons jugé compliquées les situations ou l’extraction de certaines informations
nécessite des travaux d’analyse dédiées (que cela soit des analyses de valeurs d’attributs ou des
manipulations de la modélisation objet des documents (DOM) pour obtenir différents fragments
d’une meme information).

Concernant les opérations visant la validation 18, l’homogénéisation et la simpliﬁcation de la
structure XML des documents (second type de problémes recensé a la section 1), nous avons par
exemple fusionné les éléments XML composant les amalgames 19 en un seul élément de maniére
similaire a (Candito et Seddah, 2012) (19 489 fusions). Nous avons explicité les caractéristiques
morphologiques de chaque mot par des attributs propres (genre, nombre, temps, personne. . .).
Nous avons fait diverses corrections pour valider les documents comme l’ajout d’attributs man-
quant (e.g. 3166 attribut compound ajoutés) et le renommage d’attribut (e.g. 3 726 attributs cat
corrigés en catint).

Concernant les opérations de modification de contenu, la segmentation en tokens mots originale
et le contenu textuel du P7T ont été épargnés. De méme, les corrections d’erreurs triviales
d’étiquetage ont été considérées a la marge pour cette étude. Les opérations se sont concen-
trées d’une part sur la détermination des traits morpho—syntaxiques (catégorie grammaticale,
sous—catégorie, ﬂexion morphologique et lemme) des mots composant les mots composés, et
d’autre part sur l’attribution a chaque mot de l’étiquette grammaticale du jeux d’étiquettes du
P7T+ correspondant a ses attributs. Ces deux types d’opérations, dont le détail est présenté
respectivement dans les deux paragraphes suivants, visent a traiter le premier type de problémes
recensé a la section 1 ; en particulier la détermination des traits morpho-syntaxiques est une
étape nécessaire a l’affectation d’une étiquette P7T+ aux mots composants (cf. section 2.2).

Le processus de détermination des traits manquants pour les mots composants repose sur
l’observation des séquences de traits associées aux occurrences des composés, aux séquences
de mots simples correspondant aux composants des composés, ainsi que sur l’observation des
traits associés individuellement a chaque mot du corpus. Notre approche tente d’abord une
résolution avec des statistiques globales et s’appuie ensuite sur des traits locaux au composé
en cas d’ambigu'1'té au niveau global. Sur les 1 892 lemmes de composés incomplets que nous
observons, nous proposons une solution a 1 736 (3 009 occurrences).

Le processus d’attribution a chaque mot d’une étiquette du P7T+ exploite les traits catégorie,
sous—catégorie et ﬂexion morphologique des mots. Pour ce faire, nous nous sommes appuyés sur
la table de conversion énoncée par (Crabbé et Candito, 2008) ainsi que sur la documentation
de l’étiqueteur morpho-syntaxique MELT (Denis et Sagot, 2010) pour compléter quelques regles
manquantes 2°. 31 régles réalisent la conversion. Sur les 679 584 mots (simples, composés et
composants) que compte le P7'I‘, la procédure attribue une étiquette P7T+ a 664 240 mots;

18. Seulement 27 des 44 ﬁchiers com osant la section t ed (éti ueté rammaticalement) de la version de Janvier
P 088 ‘l S

2012 sont valides (c’est-5-dire vérifient la spécification déﬁnie par le schéma NG fourni par les auteurs.)

19. Les amalgames sont des unités lexicales décrite par une unité graphique mais composés deux catégories grammati-
cales (e.g. “du“ pour “de+le“, “auxquel“ pour “a+lequel“).

20. Un mot de catégorie “Nom“ et de sous—catégorie “cardinal“ (million, huit, 2001...) est converti en nom commun.
L’étiquette “préfix“ ne chan e as, comme celle des amal ames a res fusion de ses sous-éléments.

S P S P

165 © ATALA

TALN-RECITAL 2013, 17-21 Iuin, Les Sables d’Olonne

15 344 sont donc indéﬁnis. Nos regles de conversion, testées sur les annotations P7T du corpus
Sequoia, produisent les mémes 21 annotations P7T+ que le corpus met aussi a disposition.

En pratique les différentes opérations ont été mises en oeuvre via des régles 22 plus ou moins
générales exprimées sur le DOM des documents. Les opérations de comptage requises par
certaines stratégies ont été réalisés sur tout le corpus et non seulement sur chaque document.

2.4 Protocole d’évaluation

Notre objectif est d’évaluer les performances d’un étiqueteur morpho-syntaxique construit sur des
données pré—annotées automatiquement par rapport a un étiqueteur construit sur des données
validées manuellement. Notre méthodologie est présentée a la ﬁgure 1. La premiere étape
consiste a produire l’ensemb1e de données d’entrainement. Pour cela, nous utilisons le Stanford
POS tagger avec un modéle entrainé sur le P7T+ pour annoter un large corpus de données
non—étiquetées. Cet ensemble de données est noté C0R1>UsP°S apres qu’il ait été étiqueté morpho-
syntaxiquement. Nous 1’utilisons alors dans une deuxiéme étape pour entrainer un nouveau
modéle. La performance du modele créé a partir de C0RPUsP°S est ensuite évaluée sur le P7T+.

A t» ’/J’ 444444444444 “‘~“'"»-.\“\
ppren issage I M déle 1 

‘t\
”"/l Etiquetage
morpho-syntaxique

 
    

   
 
 
 

 Apprentissage

FIGURE 1 — Apprentissage d’un modele a partir de données automatiquement annotées.

Aﬁn d’étudier l’impact de la taille du corpus d’entrainement sur la performance de l’étiquetage
morpho—syntaxique, nous avons entrainé différents modéles en utilisant des portions de C0R1>UsP°S
représentant un facteur x du nombre de phrases de P7T+. Ici, nous utilisons Wikinews et Europarl
en francais comme CORPUS. Pour Wikinews, nous avons testé les facteurs allant de 1 a 4 fois le
nombre de phrases de P7T+ (4 étant la limite que nous pouvions atteindre avec le nombre de
phrases contenu dans Wikinews). Pour Europarl, nous avons exploré jusqu’au facteur 16.

Trois mesures d’évaluation sont considérées comme pertinentes pour nos expériences : la précision
sur les tokens, la précision sur les phrases (nombre de phrases dans lesquelles tous les tokens ont
été correctement étiquetés par rapport au nombre de phrases total) et la précision sur les mots
inconnus (calculée a partir des tokens n’apparaissant pas dans l’ensemble d’entrainement).

21. 108 mots obtiennent une étiquette différente de celle attribuée par les auteurs du Sequoia, a savoir une étiquette
désignant une valeur indéfinie. En y regardant d’un peu plus prés nous avons constaté que cela concemait en fait 22 formes
distinctes et que ces formes étaient ambigués et pouvaient correspondre a des noms communs ou bien a des adverbes
négatifs (e.g. 34 «personnes ?>>, 37 «points ?>>). En creusant davantage, nous avons constaté un probléme d’annotation. Ces
mots étaient annotés en tant que nom (catégorie <<N») mais possédaient une sous-catégorie «NEG» propre aux adverbes.
La description incomplete de certains traits semblent étre aussi la raison de l’attribution d’une étiquette indéﬁnie. C’est
le cas de verbes («aboutisse, «agrandisse», «remplisse») dont le mode n’est pas précisé. Indirectement notre systéme a
permis ainsi de détecter des erreurs d’inconsistances dans le Sequoia.

22. L’outil de révision et d’extension est librement disponible sur https : / / sites . google . com/site/
nico1asherna.udez/ resources

166 © ATALA

TALN-RECITAL 2013, 17-21 Iuin, Les Sables d’Olonne
3 Expériences

Cette section présente les expériences que nous avons menées. Nous rapportons d’abord la perfor-
mance d’un étiqueteur état-de-l’art construit sur des données manuellement validées (section 3.1).
Puis nous rapportons les performances observées pour différentes tailles de données d’entraine-
ment annotées automatiquement et ce pour des corpus d’entrainement de deux genres différents
(sections 3.2 et 3.2). Enﬁn nous rapportons les performances de ces étiqueteurs construits sur des
données non validées sur un corpus sans aucun lien de ﬁliation connu (section 3.3). Le modele
et les traits d’entrainement de ces étiqueteurs sont présentés a la section 2.1.

3.1 Performance d’un étiqueteur état-de-l’art

La premiere expérience que nous avons menée porte sur1’éva1uation du Stanford POS Tagger sur
1’ensemb1e de données P7T+. I1 s’agit de connaitre la performance maximale que peut obtenir 1e
systéme 1orsqu’i1 est entrainé sur des données qui ont été manuellement Validées. Les résultats
que nous présentons ici ont été obtenus en validation croisée en 10 strates. L’écart type (0') des
scores calculés sur les différentes strates est également reporté. Les résultats sont présentés dans
la table 1. Le Stanford POS Tagger obtient une précision moyenne de 96,93% sur les tokens et de
50,03% sur les phrases. Ces résultats sont conformes a 1’état—de—1’art des méthodes n’uti1isant
pas de ressources externes (Crabbé et Candito, 2008). I1 faut cependant noter que les scores
présentés ne sont pas directement comparables aux approches précédentes qui n’uti1isaient pas
une méthodologie d’éva1uation en validation croisée.

Précision Min. - Max. Ecart type

Tokens 96,93 96,55 — 97,28 0,219
Phrases 50,03 47,08 — 52,41 1,888
Mots inconnus 85,44 82,04 — 87,67 1,661

TABLE 1 — Scores de précision sur les tokens, phrases et mots inconnus du Stanford POS tagger
calculés a partir du P7T+ en validation croisée en 10 strates. Le minimum, 1e maximum et1’écart
type des scores calculés sur les 10 strates sont également reportés.

3.2 Entrainement a partir de données automatiquement annotées

Dans une seconde série d’expériences, nous évaluons la performance d’une méthode d’étiquetage
morpho—syntaxique entrainée a partir de données automatiquement annotées. Les résultats sont
présentés dans la table 2. Le modele entrainé sur la totalité de WikinewsP°S obtient les meilleurs
scores avec une précision moyenne de 96,97% sur les tokens et de 49,74% sur les phrases. I1 s’agit
d’un niveau de performance statistiquement comparable 23 a celui obtenu avec le modele entrainé
sur le P7T+ (décrit a la section 3.1). Ce résultat montre qu’i1 est possible, compte tenu de la
taille des données manuellement annotées disponibles en francais a ce jour, de créer un modéle
d’étiquetage morpho—syntaxique tout aussi performant a partir de données automatiquement
annotées.

23. p > 0,1 avec un t-test de Student.
167 © ATALA

TALN-RECITAL 2013, 17-21 Iuin, Les Sables d’Olonne

Entrainement Préc. tokens Préc. phrases Préc. inconnus

WikinewsP°S (1:1 P7T+) 96,46 44,42 80,81
WikinewsP°S (2:1 P7T+) 96,77 47,35 80,08
WikinewsP°S (3:1 P7T+) 96,88 48,52 79,20
WikinewsP°S (4:1 P7T+) 96,97l 49,5 7* 78,20

TABLE 2 — Scores de précision sur les tokens, phrases et mots inconnus du Stanford POS tagger
entrainé a partir de Wikinews (annoté automatiquement) et évalué sur le P7T+. Le ratio entre la
taille de l’ensemble d’entrainement et la taille du P7T+ est indiqué entre parentheses. Les scores
indiqués par le caractere T n’ont pas de différence statistiquement signiﬁcative par rapport aux
scores obtenus par le modéle entrainé sur le P7T+ (p > 0,1 avec un t—test de Student).

11 est intéressant de voir que la précision sur les tokens et les phrases est en constante aug-
mentation par rapport a la taille du corpus d’entrainement et ce, malgré un nombre d’erreurs
d’éu'quetage automatique obligatoirement a la hausse. La précision moyenne sur les mots incon-
nus est quant a elle en diminution. Néanmoins, le nombre total d’erreurs commises sur les mots
inconnus est en nette diminution (7128 mots inconnus mal étiquetés avec le modéle entrainé
a partir d’un facteur 1 du P7T+ contre 5168 avec le modéle entrainé sur 100% WikinewsP°S).
On peut également constater qu’il faut une quantite’ bien plus importante de données automati-
quement annotées que de données manuellement annotées, ici quatre fois plus, pour obtenir le
meme niveau de performance.

Entrainement Préc. tokens Préc. phrases Préc. inconnus

EuroparlP°S (1:1 P7T+) 95,85 40,22 79,45
Europar1P°S (4:1 P7T+) 96,53 45,51 77,46
EuroparlP°S (8:1 P7T+) 96,74 47,38 76,68
EuroparlP°S (16:1 P7T+) 96,93l 49,221 75,81
Sequoia 93,99 28,42 83,49

TABLE 3 — Scores de précision sur les tokens, phrases et mots inconnus du Stanford POS tagger
entrainé a partir de Europarl (annoté automatiquement) et Sequoia (validé manuellement) et
évalué sur le P7T+. Le ratio entre la taille de l’ensemble d’entrainement et la taille du FTB+
est indiqué entre parentheses. Les scores indiqués par le caractére T (p > 0,1 avec un t—test
de Student) et 1 (p > 0,05 avec un t—test de Student) n’ont pas de différence statistiquement
signiﬁcative par rapport aux scores obtenus par le modele entrainé sur le P7T+.

La table 3 rapporte les résultats que nous obtenons avec le corpus EuroparlP°S. De par la différence
de genre, il était attendu que les scores obtenus avec ce corpus soient moins élevés que ceux
obtenus avec WikinewsP°S. On note que, en comparaison avec WikinewsP°S, il faut davantage de
données de EuroparlP°S pour obtenir un niveau de performance acceptable. Plus exactement, il
semble falloir quatre fois plus de données pour obtenir les mémes performances. Ainsi avec 16
fois plus de données que le P7T+, on arrive 31 une performance signiﬁcative similaire a un systeme
état—de-l’art entrainé sur celui—ci. Malgré des scores de précisions moins élevés, on observe les
mémes tendances de progression quels que soient les scores. Bien que la précision sur les mots
inconnus diminue, le nombre de mots inconnus mal étiquetés est également a la baisse.

168 © ATALA

TALN-RECITAL 2013, 17-21 Iuin, Les Sables d’Olonne

Dans la méme table, nous présentons a titre de comparaison les résultats obtenus par un modele
entrainé sur Sequoia, seul corpus librement disponible a ce jour. Les scores de précision de ce
modéle évalué sur le P7T+ sont bien en dessous de ceux obtenus par les modéles entrainés
sur WikinewsP°S et EuroparlP°S, avec une précision de 93,99% sur les tokens et de seulement
28,42% sur les phrases. Ces résultats conﬁrment qu’un ensemble de données automatiquement
annotées représente une alternative pertinente pour l’entrainement de modéles d’étiquetage
morpho-syntaxique.

3.3 Performance sur un corpus sans lien de ﬁliation

La troisieme et demiére expérience que nous avons menée consiste a évaluer la performance des
modeles entrainés a partir de WikinewsP°S et du P7T+ sur un corpus autre que le French TreeBank.
Pour cela nous avons choisi le corpus Sequoia. Ce demier est composé de phrases provenant de
quatre origines : Europarl francais, le journal l’Est Républicain, Wikipedia Fr et des documents
de l’Agence Européenne du Médicament (EMEA). Les résultats sont présentés dans la table 4.

D’une maniére générale, les scores de précisions sont plus faibles que ceux observés sur le
P7T+. La taille trés restreinte de Sequoia (3204 phrases) ne permet cependant pas d’établir des
conclusions. Les meilleurs scores sont obtenus sur les phrases provenant de l’Est Républicain
et les moins bons sur celles provenant de documents de l’EMEA (domaine médical). I1 s’agit
d’un comportement normal puisque les modeles ont été construits a partir de phrases issues de
documents joumalistiques. Encore une fois, les résultats du modéle entrainé sur WikinewsP°S sont
tres proches de ceux obtenus par le modele entrainé sur le P7T+.

Entrainement Europarl Est Rép. Wikipedia EMEA Tout

FTB+ 94,00 95, 10 94,86 92,06 93,85
WikineWsP°5 93,55 94,56 94,61 91,09 93,30

TABLE 4 — Scores de précision sur les tokens du Stanford POS tagger entrainé a partir de WikinewsP°S
et du FTB+ et évalué sur le Sequoia. Les scores de précision en fonction de l’origine des phrases
sont également reportés.

4 Travaux connexes relatifs a la construction de corpus

La procédure d’annotau'on morpho—syntaxique de corpus repose en général sur une procédure en
deux étapes 24 : d’abord une assignation automatique des étiquettes par un étiqueteur existant
(étape aussi appelée <<pré-annotation») et ensuite une révision de celles-ci par des annotateurs
humains (Hajicova et al., 2010). On retrouve cette maniére de précéder dans la construction
des corpus Penn Treebank (Marcus et al., 1993), PAROLE, MULT EXT JOC (Véronis et Khouri,
1995), French T reebank (Abeillé et al., 2003), FREEBANK (Salmon—Alt et al., 2004), T COF-POS
(un corpus libre de francais parlé) (Benzitoun LL, 2012) et Sequoia (Candito et Seddah, 2012).

24. Le processus de construction d’un corpus annoté est plus complexe et comprend notamment les étapes suivantes :
sélection et constitution de la base de textes a annoter, deﬁnition du schéma d’annotation, n1ise en place du protocole de
validation par les experts, entrainement et mesure du taux d’accord entre ceux-ci.

169 © ATALA

