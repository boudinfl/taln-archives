<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
"http://www.w3.org/TR/html4/loose.dtd">
<html><head><title>Un mod&#232;le segmental probabiliste combinant coh&#233;sion lexicale et rupture lexicale pour la segmentation th&#233;matique</title>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
</head>
<body>
<div style="page-break-before:always; page-break-after:always"><div><p>Un mod&#232;le segmental probabiliste combinant coh&#233;sion
lexicale et rupture lexicale pour la segmentation th&#233;matique
</p>
<p>Anca Simon1 Guillaume Gravier2 Pascale S&#233;billot3
(1) Universit&#233; de Rennes 1
</p>
<p>(2) CNRS
(3) INSA de Rennes
</p>
<p>IRISA &amp; INRIA Rennes
</p>
<p>R&#201;SUM&#201;
L&#8217;identification d&#8217;une structure th&#233;matique dans des donn&#233;es textuelles quelconques est une t&#226;che
difficile. La plupart des techniques existantes reposent soit sur la maximisation d&#8217;une mesure de
coh&#233;sion lexicale au sein d&#8217;un segment, soit sur la d&#233;tection de ruptures lexicales. Nous proposons
une nouvelle technique combinant ces deux crit&#232;res de mani&#232;re &#224; obtenir le meilleur compromis
entre coh&#233;sion et rupture. Nous d&#233;finissons un nouveau mod&#232;le probabiliste, fond&#233; sur l&#8217;approche
propos&#233;e par Utiyama et Isahara (2001), en pr&#233;servant les propri&#233;t&#233;s d&#8217;ind&#233;pendance au domaine
et de faible a priori de cette derni&#232;re. Des &#233;valuations sont men&#233;es sur des textes &#233;crits et sur
des transcriptions automatiques de la parole &#224; la t&#233;l&#233;vision, transcriptions qui ne respectent pas
les normes des textes &#233;crits, ce qui accro&#238;t la difficult&#233;. Les r&#233;sultats exp&#233;rimentaux obtenus
d&#233;montrent la pertinence de la combinaison des crit&#232;res de coh&#233;sion et de rupture.
</p>
<p>ABSTRACT
A probabilistic segment model combining lexical cohesion and disruption for topic seg-
mentation
</p>
<p>Identifying topical structure in any text-like data is a challenging task. Most existing techniques
rely either on maximizing a measure of the lexical cohesion or on detecting lexical disruptions.
A novel method combining the two criteria so as to obtain the best trade-off between cohesion
and disruption is proposed in this paper. A new statistical model is defined, based on the work of
Isahara and Utiyama (2001), maintaining the properties of domain independence and limited a
priori of the latter. Evaluations are performed both on written texts and on automatic transcripts
of TV shows, the latter not respecting the norms of written texts, thus increasing the difficulty
of the task. Experimental results demonstrate the relevance of combining lexical cohesion and
disrupture.
</p>
<p>MOTS-CL&#201;S : segmentation th&#233;matique, coh&#233;sion lexicale, rupture de coh&#233;sion, journaux
t&#233;l&#233;vis&#233;s.
</p>
<p>KEYWORDS: topic segmentation, lexical cohesion, lexical disrupture, TV broadcast news.
</p>
<p>TALN-R&#201;CITAL 2013, 17-21 Juin, Les Sables d&#8217;Olonne
</p>
<p>202 c&#65535; ATALA</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>1 Introduction
</p>
<p>La segmentation th&#233;matique consiste &#224; mettre en &#233;vidence la structure s&#233;mantique d&#8217;un do-
cument et les algorithmes d&#233;velopp&#233;s pour cette t&#226;che visent &#224; d&#233;tecter automatiquement les
fronti&#232;res qui d&#233;finissent des segments th&#233;matiquement coh&#233;rents. Cible de nombreux travaux,
la segmentation th&#233;matique a &#233;galement des retomb&#233;es en recherche d&#8217;information, r&#233;sum&#233;
automatique, syst&#232;mes de question-r&#233;ponse...
</p>
<p>Diverses m&#233;thodes de segmentation de donn&#233;es textuelles ont &#233;t&#233; propos&#233;es dans la litt&#233;rature
(Yamron et al., 1998; Georgescul et al., 2006; Galley et al., 2003; Hearst, 1997; Reynar, 1994;
Moens and Busser, 2001; Choi, 2000; Ferret et al., 1998; Utiyama and Isahara, 2001). Comme
indiqu&#233; dans (Purver, 2011), elles peuvent &#234;tre supervis&#233;es ou non, reposer sur des changements
de vocabulaire, des techniques de clustering, sur la d&#233;tection de fronti&#232;res discriminantes
ou sur des mod&#232;les probabilistes. D&#233;terminer les segments th&#233;matiques &#224; l&#8217;aide de mod&#232;les
probabiliste consiste la plupart du temps &#224; inf&#233;rer la s&#233;quence de th&#232;mes la plus probable &#224;
partir des mots observ&#233;s et &#224; d&#233;river les positions des fronti&#232;res (Yamron et al., 1998; Blei and
Moreno, 2001). Ces mod&#232;les utilisent un corpus d&#8217;apprentissage pour estimer les distributions
documents-th&#232;mes et th&#232;mes-mots. Des travaux r&#233;cents ont montr&#233; l&#8217;int&#233;r&#234;t de l&#8217;int&#233;gration
de ces mod&#232;les probabilistes dans les algorithmes de segmentation de textes reposant sur la
similarit&#233; de vocabulaire (Misra and Yvon, 2010; Riedl and Biemann, 2012). Nos travaux portent
sur les m&#233;thodes non supervis&#233;es. La plupart d&#8217;entre elles repose sur la coh&#233;sion du vocabulaire
pour identifier des segments coh&#233;rents dans les textes, exploitant les mots qu&#8217;ils contiennent et les
relations s&#233;mantiques que ces mots entretiennent. Pour mesurer la coh&#233;rence dans les (segments
de) textes, la coh&#233;sion lexicale, fond&#233;e sur la r&#233;p&#233;tition de mots ou sur l&#8217;exploitation de cha&#238;nes
lexicales, est fr&#233;quemment retenue en privil&#233;giant l&#8217;une ou l&#8217;autre des deux strat&#233;gies suivantes :
soit on cherche &#224; maximiser la mesure de coh&#233;sion lexicale des segments, en regroupant les
portions de texte lexicalement coh&#233;rentes, soit on cherche &#224; identifier des ruptures entre les
segments en pla&#231;ant des fronti&#232;res quand survient un changement significatif dans le vocabulaire
utilis&#233; (Hearst, 1997). Dans cet article, notre objectif est de proposer une nouvelle solution pour
la segmentation th&#233;matique de documents qui consiste &#224; m&#234;ler ces deux approches, c&#8217;est-&#224;-dire &#224;
combiner les mesures de coh&#233;sion lexicale et de rupture lexicale afin d&#8217;obtenir une segmentation
en fragments &#224; la fois th&#233;matiquement coh&#233;rents et diff&#233;rents les uns des autres.
</p>
<p>La technique que nous proposons peut s&#8217;appliquer &#224; tout type de donn&#233;es textuelles et est ind&#233;-
pendante d&#8217;un domaine particulier. Notre objectif est cependant de l&#8217;appliquer &#224; la segmentation
de journaux t&#233;l&#233;vis&#233;s afin de permettre &#224; des utilisateurs de naviguer dans ce type de donn&#233;es.
De mani&#232;re &#224; rester g&#233;n&#233;rique et non supervis&#233;e, la segmentation th&#233;matique peut dans ce cas
s&#8217;appuyer sur la transcription automatique de la parole prononc&#233;e dans les &#233;missions. L&#8217;analyse
des mots de la transcription vise alors &#224; trouver un changement significatif de vocabulaire et
donc un changement de th&#232;me (Hearst, 1997). Cependant, les particularit&#233;s des transcriptions
automatiques accroissent la difficult&#233; de la t&#226;che de segmentation. En effet, ces transcriptions ne
contiennent ni casse, ni ponctuation, et ne sont donc pas structur&#233;es en phrases comme des textes
standards mais en groupes de soufle correspondant aux mots prononc&#233;s par une personne entre
deux inspirations. De plus, elles peuvent contenir de nombreux mots mal transcrits. Difficult&#233;
suppl&#233;mentaire, les journaux TV peuvent avoir des segments th&#233;matiques tr&#232;s courts, contenant
peu de mots et donc peu de r&#233;p&#233;titions, en particulier quand le pr&#233;sentateur fait volontairement
usage de synonymes. Cela rend l&#8217;utilisation du crit&#232;re de coh&#233;sion lexicale particuli&#232;rement ardue.
Notre algorithme de segmentation th&#233;matique ayant un fort potentiel pour traiter ces cas, nous
</p>
<p>TALN-R&#201;CITAL 2013, 17-21 Juin, Les Sables d&#8217;Olonne
</p>
<p>203 c&#65535; ATALA</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>avons souhait&#233; le tester sur ces donn&#233;es difficiles.
</p>
<p>La technique pr&#233;sent&#233;e ici repose sur l&#8217;algorithme de segmentation de textes propos&#233; par Utiyama
et Isahara (2001), algorithme dont les capacti&#233;s ont &#233;t&#233; attest&#233;es pour le texte &#233;crit. C&#8217;est un
mod&#232;le probabiliste qui fournit une segmentation non supervis&#233;e. Dans cette approche, il n&#8217;y a
donc pas de tentative d&#8217;apprentissage de l&#8217;ensemble des mod&#232;les th&#233;matiques le plus probable &#224;
partir des donn&#233;es d&#8217;apprentissage, mais au contraire l&#8217;ensemble est g&#233;n&#233;r&#233; par l&#8217;algorithme &#233;tant
donn&#233;s les textes &#224; segmenter. Ce mod&#232;le est ind&#233;pendant du domaine et permet l&#8217;obtention de
segments de longueurs tr&#232;s vari&#233;es. Il consiste en une repr&#233;sentation du document &#224; segmenter
sous forme d&#8217;un graphe, o&#249; les n&#339;uds repr&#233;sentent les fronti&#232;res th&#233;matiques potentielles et les
arcs les segments. La segmentation th&#233;matique est obtenue en trouvant le meilleur chemin dans
le graphe valu&#233;, dans lequel les poids refl&#232;tent la valeur de coh&#233;sion lexicale. Notre contribution
consiste &#224; d&#233;finir un mod&#232;le statistique am&#233;lior&#233; qui permet d&#8217;int&#233;grer la rupture lexicale. Par
cons&#233;quent, notre algorithme se r&#233;sume en un d&#233;codage d&#8217;un treillis afin d&#8217;identifier la meilleure
segmentation. Cette repr&#233;sentation permet de consid&#233;rer la valeur de rupture lexicale en chaque
n&#339;ud. La solution propos&#233;e est test&#233;e pour la segmentation de journaux TV transcrits mais
&#233;galement de textes &#233;crits, et les &#233;valuations montrent une am&#233;lioration en pr&#233;cision et rappel
par rapport &#224; la seule utilisation de la valeur de la coh&#233;sion lexicale.
</p>
<p>L&#8217;article est organis&#233; de la fa&#231;on suivante : des travaux en segmentation th&#233;matique existants
sont pr&#233;sent&#233;s dans la section 2. Dans la section 3, nous d&#233;taillons notre approche, en d&#233;crivant
d&#8217;abord le mod&#232;le g&#233;n&#233;ral d&#8217;Utiyama et Isahara puis le nouveau mod&#232;le statistique propos&#233;. Dans
la section 4, les exp&#233;riences sont pr&#233;sent&#233;es, avec des d&#233;tails sur les corpus utilis&#233;s et une analyse
des r&#233;sultats.
</p>
<p>2 Techniques de segmentation th&#233;matique
</p>
<p>Dans cette section, nous pr&#233;sentons rapidement les notions-cl&#233;s concernant le concept de seg-
mentation th&#233;matique, ainsi que les techniques existantes et les traits qu&#8217;elles exploitent pour
r&#233;aliser cette t&#226;che.
</p>
<p>2.1 Le concept de th&#232;me
</p>
<p>Le concept de th&#232;me est difficile &#224; d&#233;finir pr&#233;cis&#233;ment et les linguistes qui ont tent&#233; de le
caract&#233;riser en offrent de nombreuses d&#233;finitions. Dans (Brown and Yule, 1983), la difficult&#233; de
d&#233;finir un th&#232;me est longuement discut&#233;e et les auteurs soulignent que : &quot;The notion of &#8217;topic&#8217; is
clearly an intuitively satisfactory way of describing the unifying principle which makes one stretch
of discourse &#8217;about&#8217; something and the next stretch &#8217;about&#8217; something else, for it is appealed to very
frequently in the discourse analysis literature. Yet the basis for the identification of &#8217;topic&#8217; is rarely
made explicit.&quot;.
</p>
<p>Souhaitant appliquer la segmentation th&#233;matique &#224; des journaux TV, nous avons cherch&#233; &#224; voir
si la notion de th&#232;me avait &#233;t&#233; d&#233;finie dans le contexte d&#8217;&#233;missions t&#233;l&#233;vis&#233;es. Le projet Topic
Detection and Tracking (Allan, 2002) s&#8217;est par exemple focalis&#233; sur le rep&#233;rage de segments de
journaux TV th&#233;matiquement li&#233;s. Dans ce cadre, les notions d&#8217;&#233;v&#233;nement et de th&#232;me ont &#233;t&#233;
d&#233;finies : un &#233;v&#233;nement est quelque chose qui se produit &#224; un instant et un endroit sp&#233;cifique
</p>
<p>TALN-R&#201;CITAL 2013, 17-21 Juin, Les Sables d&#8217;Olonne
</p>
<p>204 c&#65535; ATALA</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>et qui est associ&#233; &#224; des actions particuli&#232;res ; un th&#232;me est, quant &#224; lui, l&#8217;ensemble form&#233; d&#8217;un
&#233;v&#233;nement et de tous les &#233;v&#233;nements qui lui sont directement li&#233;s. Un &#233;v&#233;nement est donc
relativement court et &#233;volue dans le temps, tandis qu&#8217;un th&#232;me est plus stable et plus long.
</p>
<p>Dans notre cadre de segmentation de journaux TV, un th&#232;me correspond &#224; un reportage qui forme
une unit&#233; s&#233;mantique coh&#233;rente dans la structure d&#8217;un journal. Notre algorithme est &#233;galement
&#233;valu&#233; sur des textes &#233;crits, form&#233;s par concat&#233;nation de parties extraites d&#8217;articles s&#233;lectionn&#233;s
al&#233;atoirement dans le corpus Brown (Choi, 2000) ; un th&#232;me est alors associ&#233; &#224; chaque partie
formant le texte final.
</p>
<p>2.2 M&#233;thodes pour la segmentation th&#233;matique
</p>
<p>Pour r&#233;aliser la segmentation th&#233;matique de textes, diverses caract&#233;ristiques peuvent &#234;tre ex-
ploit&#233;es afin d&#8217;identifier les changements th&#233;matiques. Elles peuvent reposer sur la coh&#233;sion
lexicale (i.e., prendre en compte les informations de distribution du vocabulaire) ou sur des
marqueurs linguistiques tels que des indices prosodiques (Guinaudeau and Hirschberg, 2011)
ou des marqueurs du discours (Grosz and Sidner, 1986; Litman and Passonneau, 1995). Les
techniques g&#233;n&#233;riques, qui sont celles qui nous int&#233;ressent ici, exploitent traditionnellement la
seule coh&#233;sion lexicale, ind&#233;pendante du type de documents consid&#233;r&#233;s et ne n&#233;cessitant pas de
phase d&#8217;apprentissage. L&#8217;id&#233;e-cl&#233; des m&#233;thodes fond&#233;es sur la coh&#233;sion lexicale est de consid&#233;rer
qu&#8217;un changement significatif dans le vocabulaire utilis&#233; est un signe de changement th&#233;matique.
Ces approches peuvent &#234;tre divis&#233;es en deux familles :
&#8226; les m&#233;thodes locales (Hearst, 1997; Hernandez and Grau, 2002; Ferret et al., 1998; Claveau
</p>
<p>and Lef&#232;vre, 2011) qui cherchent &#224; rep&#233;rer localement les ruptures lexicales ;
&#8226; les m&#233;thodes globales (Reynar, 1994; Choi, 2000; Utiyama and Isahara, 2001; Malioutov and
</p>
<p>Barzilay, 2006; Misra and Yvon, 2010) exploitant une mesure de la coh&#233;sion lexicale.
Une m&#233;thode locale repose sur la comparaison locale de r&#233;gions du document et associe un
changement th&#233;matique aux endroits o&#249; il y a une similarit&#233; faible entre deux r&#233;gions cons&#233;cutives
(i.e., elles identifient les zones de fortes ruptures lexicales). Par exemple, TextTiling (Hearst,
1997), qui est consid&#233;r&#233; comme un algorithme de segmentation th&#233;matique fondamental, analyse
le texte &#224; l&#8217;aide d&#8217;une fen&#234;tre glissante qui couvre des blocs adjacents de texte et est centr&#233;e en
un point du texte correspondant &#224; une fronti&#232;re th&#233;matique potentielle. Les contenus avant et
apr&#232;s chaque fronti&#232;re possible sont repr&#233;sent&#233;s par des vecteurs de mots pond&#233;r&#233;s, un poids
fort indiquant qu&#8217;un mot est particuli&#232;rement pertinent pour d&#233;crire un contenu. Une mesure de
similarit&#233;, par exemple cosinus, est calcul&#233;e entre les deux vecteurs. Plus l&#8217;angle entre les deux
vecteurs diminue, plus le cosinus approche de 1, indiquant par l&#224;-m&#234;me la plus grande similarit&#233;
entre les contenus avant et apr&#232;s la fronti&#232;re potentielle. Les valeurs de similarit&#233; sont calcul&#233;es
&#224; chaque fronti&#232;re possible et la s&#233;quence r&#233;sultante de valeurs de similarit&#233; est analys&#233;e. Les
points de scores de similarit&#233; les plus bas (i.e., forte rupture) repr&#233;sentent alors les fronti&#232;res
th&#233;matiques. Ce type de m&#233;thode locale pr&#233;sente certains d&#233;savantages dont une sensibilit&#233; aux
variations de tailles des segments dans les textes puisqu&#8217;un voisinage de taille fixe est consid&#233;r&#233;,
ainsi qu&#8217;une difficult&#233; de choix de la valeur de seuil pour d&#233;cider qu&#8217;une rupture est suffisamment
forte pour placer une fronti&#232;re.
</p>
<p>Une m&#233;thode globale r&#233;alise quant &#224; elle une comparaison globale entre toutes les r&#233;gions du
document, en cherchant &#224; maximiser globalement la valeur de la coh&#233;sion lexicale. Dans Utiyama
et Isahara (2001), la valeur de la coh&#233;sion lexicale d&#8217;un segment Si est vue comme la mesure de
</p>
<p>TALN-R&#201;CITAL 2013, 17-21 Juin, Les Sables d&#8217;Olonne
</p>
<p>205 c&#65535; ATALA</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>la capacit&#233; d&#8217;un mod&#232;le de langue &#8710;i , appris sur le segment Si , &#224; pr&#233;dire les mots du segment. Le
mod&#232;le de langue &#8710;i doit donc d&#8217;abord &#234;tre estim&#233;, puis la probabilit&#233; g&#233;n&#233;ralis&#233;e des mots du
segment Si , &#233;tant donn&#233;&#8710;i , doit &#234;tre d&#233;termin&#233;e. Apr&#232;s le calcul de la valeur de coh&#233;sion lexicale
pour chaque segment, la segmentation maximisant globalement cette valeur est choisie. Cet
algorithme s&#8217;est av&#233;r&#233; performant au regard d&#8217;autres algorithmes de segmentation th&#233;matique de
textes tels que ceux de Choi (2000) ou Reynar (1994). Cependant, la limite principale de ce type
de m&#233;thode globale est un risque de sur-segmentation.
</p>
<p>L&#8217;originalit&#233; de la solution que nous proposons consiste dans la combinaison des deux types de
m&#233;thodes. Une m&#233;thode fond&#233;e sur le m&#234;me principe, visant &#224; capturer dans une vue globale
des dissimilarit&#233;s locales, a &#233;t&#233; pr&#233;sent&#233;e dans (Malioutov and Barzilay, 2006), mais, d&#8217;une part,
le nombre de segments &#224; trouver est fix&#233; a priori et, d&#8217;autre part, la couverture est limit&#233;e car la
dissimilarit&#233; entre segments est calcul&#233;e en utilisant une fen&#234;tre.
</p>
<p>Le point de d&#233;part de notre m&#233;thode est le mod&#232;le statistique propos&#233; dans (Utiyama and
Isahara, 2001), qui est flexible et offre des possibilit&#233;s d&#8217;extension par int&#233;gration de nouvelles
informations. Plusieurs travaux l&#8217;ont d&#233;j&#224; utilis&#233; avec succ&#232;s dans le contexte de la segmentation
de journaux TV (Huet et al., 2008; Guinaudeau et al., 2012), le modifiant pour int&#233;grer des
connaissances sp&#233;cifiques aux &#233;missions TV. Contrairement &#224; ces travaux, nous avons red&#233;fini le
mod&#232;le de (Utiyama and Isahara, 2001) afin qu&#8217;il puisse prendre en compte non seulement la
coh&#233;sion mais aussi la rupture lexicale et, par cons&#233;quent, am&#233;liorer la segmentation de tout
type de donn&#233;es textuelles. Consid&#233;rer la rupture est en particulier int&#233;ressant pour traiter les
cas de textes contenant des changements brutaux de vocabulaire. La fa&#231;on dont nous combinons
les deux crit&#232;res est d&#233;taill&#233;e dans la section 3.
</p>
<p>3 Combinaison de la coh&#233;sion et de la rupture lexicales
</p>
<p>Nous rappelons tout d&#8217;abord l&#8217;algorithme de Utiyama et Isahara, puis expliquons le nouveau
mod&#232;le statistique que nous proposons.
</p>
<p>3.1 Le mod&#232;le statistique
</p>
<p>L&#8217;algorithme propos&#233; par Utiyama et Isahara d&#233;finit un mod&#232;le probabiliste et consiste &#224; d&#233;termi-
ner la segmentation qui produit les segments les plus coh&#233;rents d&#8217;un point de vue lexical tout en
respectant une distribution a priori de la longueur des segments. L&#8217;id&#233;e principale est de trouver
la segmentation la plus probable pour une s&#233;quence de t unit&#233;s &#233;l&#233;mentaires (i.e., phrases ou
&#233;nonc&#233;s compos&#233;s de mots) W = ut1 parmi toutes les segmentations possibles, i.e.,
</p>
<p>S&#770; = argmax
S
</p>
<p>P[W |S]P[S] . (1)
</p>
<p>En admettant que chaque segment est une unit&#233; ind&#233;pendante du reste du texte et que les mots
contenus dans un segment sont eux aussi ind&#233;pendants, la probabilit&#233; du texte W pour une
segmentation S = Sm1 est donn&#233;e par
</p>
<p>P[W |Sm1 ] =
m&#65535;
i=1
</p>
<p>ni&#65535;
j=1
</p>
<p>P[wij |Si] , (2)
</p>
<p>TALN-R&#201;CITAL 2013, 17-21 Juin, Les Sables d&#8217;Olonne
</p>
<p>206 c&#65535; ATALA</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>o&#249; ni est le nombre de mots du segment Si , wij est le j
e mot de Si et m le nombre de segments.
</p>
<p>La probabilit&#233; P[wij |Si] est donn&#233;e par une loi de Laplace dont les param&#232;tres sont estim&#233;s sur
Si , i.e.,
</p>
<p>P[wij |Si] =
fi(wij) + 1
</p>
<p>ni + k
, (3)
</p>
<p>o&#249; fi(wij) est le nombre d&#8217;occurrences de w
i
j dans Si et k est le nombre total de mots diff&#233;rents
</p>
<p>dans le texte W (i.e., la taille du vocabulaire). Cette probabilit&#233; va favoriser les segments
homog&#232;nes car elle cro&#238;t quand les mots sont r&#233;p&#233;t&#233;s et d&#233;cro&#238;t quand ils sont diff&#233;rents. La
distribution a priori des longueurs des segments est donn&#233;e par P[Sm1 ] = n
</p>
<p>&#8722;m, o&#249; n est le nombre
total de mots. Elle a une valeur &#233;lev&#233;e quand le nombre de segments est faible, tandis que
P[W |S] a des valeurs &#233;lev&#233;es quand le nombre de segments est grand.
Cette approche peut &#234;tre vue comme la recherche du meilleur chemin dans un graphe valu&#233;,
graphe repr&#233;sentant toutes les segmentations possibles. Chaque n&#339;ud correspond &#224; une fronti&#232;re
possible et un arc entre les n&#339;uds i et j repr&#233;sente un segment contenant les unit&#233;s comprises
entre ui+1 et uj . Le poids attribu&#233; &#224; chaque arc de ce type est
</p>
<p>v(i, j) =
j&#65535;
</p>
<p>k=i+1
</p>
<p>ln(P[uk|Si&#8594; j])&#8722;&#945;ln(n) , (4)
</p>
<p>o&#249; Si&#8594; j est le segment correspondant &#224; l&#8217;arc allant du n&#339;ud i au n&#339;ud j. Pour les petits segments,
la probabilit&#233; d&#8217;estimer les mots contenus dans le segment est plus faible ; le facteur &#945; fournit
un compromis entre la longueur moyenne des segments retourn&#233;s et la valeur de la coh&#233;sion
lexicale.
</p>
<p>3.2 Introduction de la rupture lexicale
</p>
<p>Le mod&#232;le d&#233;fini ci-dessus suppose que chaque segment Si du texte est ind&#233;pendant des autres, ce
qui ne permet pas de combiner la valeur de la coh&#233;sion lexicale et celle de la rupture lexicale. En
effet, lors du calcul du poids associ&#233; au segment Si , nous devrions ajouter une p&#233;nalit&#233; marquant
&#224; quel point le contenu de Si diff&#232;re de celui du segment pr&#233;c&#233;dent Si&#8722;1. Pour cette raison,
nous proposons une hypoth&#232;se markovienne entre les segments nous permettant, pour chaque
segment, de consid&#233;rer celui qui le pr&#233;c&#232;de. La probabilit&#233; d&#8217;un texte W pour une segmentation
S = Sm1 devient alors
</p>
<p>P[W |Sm1 ] = P[W |S1]
m&#65535;
i=2
</p>
<p>P[W |Si ,Si&#8722;1] . (5)
</p>
<p>Pour d&#233;terminer la segmentation de probabilit&#233; maximum S&#770;, le co&#251;t associ&#233; au segment Si , &#233;tant
donn&#233; Si&#8722;1, est
</p>
<p>ln(P[W |Si ,Si&#8722;1]) = ln(P[Wi |Si])&#8722;&#955;( 1&#8710;(Wi ,Wi&#8722;1) ) , (6)
o&#249; &#8710;(Wi ,Wi&#8722;1) est la valeur de rupture entre le contenu de Si et celui de Si&#8722;1, et &#955; est un
param&#232;tre qui permet de contr&#244;ler l&#8217;influence de la rupture dans le co&#251;t. Wi repr&#233;sente les unit&#233;s
&#233;l&#233;mentaires du segment Si . Choisir 1/&#8710;(Wi ,Wi&#8722;1) conduit &#224; une p&#233;nalit&#233; faible quand il y a une
forte rupture. Dans l&#8217;&#233;quation 6, P[W |Si ,Si&#8722;1] ne repr&#233;sente plus une probabilit&#233; ; cependant,
</p>
<p>TALN-R&#201;CITAL 2013, 17-21 Juin, Les Sables d&#8217;Olonne
</p>
<p>207 c&#65535; ATALA</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>puisque l&#8217;algorithme de segmentation consiste &#224; d&#233;terminer le meilleur chemin dans un graphe
pond&#233;r&#233;, cela n&#8217;a pas d&#8217;impact car aucune pr&#233;supposition de graphe probabiliste n&#8217;est faite pour
segmenter. Par cons&#233;quent, la nouvelle d&#233;finition de la segmentation la plus probable est
</p>
<p>S&#770; = argmax
S
</p>
<p>m&#65535;
i=1
</p>
<p>ln(P[Wi |Si])&#8722;&#955;
m&#65535;
i=2
</p>
<p>(
1
</p>
<p>&#8710;(Wi ,Wi&#8722;1)
)&#8722;&#945;mln(n) . (7)
</p>
<p>De l&#8217;&#233;quation 6, on peut d&#233;duire que, pour un n&#339;ud donn&#233; repr&#233;sentant une fronti&#232;re th&#233;ma-
tique, tous les segments de longueurs diff&#233;rentes arrivant &#224; ce n&#339;ud sont conserv&#233;s. Au niveau
impl&#233;mentation, nous d&#233;finission un treillis dans lequel un arc eip, jl repr&#233;sente une prolongation
d&#8217;un chemin de longueur l du n&#339;ud nip au n&#339;ud njl . Un n&#339;ud nip rassemble donc tous les
segments de longueur p unit&#233;s se terminant apr&#232;s ui . Ceci signifie qu&#8217;en chaque point du texte
o&#249; une fronti&#232;re potentielle est consid&#233;r&#233;e, nous analysons toutes les combinaisons possibles
d&#8217;unit&#233;s cons&#233;cutives pr&#233;c&#233;dant cette fronti&#232;re. Un arc eip, jl repr&#233;sente un segment contenant
toutes les unit&#233;s entre ui+1 et uj , avec j &#8722; i = l. Un co&#251;t est associ&#233; &#224; chaque arc en se fondant
sur l&#8217;&#233;quation 6. D&#8217;une part, ce co&#251;t consiste en la valeur de la coh&#233;sion lexicale du segment
couvert par l&#8217;arc calcul&#233; gr&#226;ce &#224; l&#8217;&#233;quation 3 ; d&#8217;autre part, une p&#233;nalit&#233; est associ&#233;e &#224; chacune
des valeurs de ce type, en fonction de la rupture lexicale entre le segment couvert par l&#8217;arc et le
segment pr&#233;c&#233;dent dans le texte. Selon le n&#339;ud dont il provient, le segment pr&#233;c&#233;dent peut lui
aussi avoir diff&#233;rentes longueurs. Par cons&#233;quent, la rupture est calcul&#233;e entre toutes les paires
possibles de segments. Pour obtenir la rupture, une mesure de similarit&#233; cosinus est utilis&#233;e entre
les vecteurs repr&#233;sentant
</p>
<p>&#8226; le segment qui contient les unit&#233;s couvertes par l&#8217;arc (de score le plus &#233;lev&#233;) arrivant au n&#339;ud
ni, j et
</p>
<p>&#8226; le segment qui contient les unit&#233;s couvertes par l&#8217;arc sortant de ce n&#339;ud vers ni+k,k.
Les vecteurs contiennent les poids associ&#233;s aux mots dans les unit&#233;s. Ces poids sont calcul&#233;s en
utilisant les mesures de TF-IDF et Okapi (Claveau, 2012), transform&#233;es en dissimilarit&#233;s.
</p>
<p>Pour d&#233;terminer la meilleure segmentation, nous utilisons un algorithme de programmation
dynamique. Lors du d&#233;codage, on associe &#224; chaque n&#339;ud le co&#251;t du meilleur chemin en fonction
des arcs entrants. Par exemple dans la figure 1, les calculs au n&#339;ud n3,1 consistent &#224; choisir la
valeur la plus &#233;lev&#233;e entre le poids associ&#233; &#224; l&#8217;arc e21,31 et &#224; l&#8217;arc e22,31.
</p>
<p>&#8226; Pour le premier arc, le score est donn&#233; par la valeur associ&#233;e au n&#339;ud n2,1, la valeur de la
coh&#233;sion lexicale de l&#8217;arc e21,31 et la rupture entre le segment contenant u2 et le segment
contenant u3.
</p>
<p>&#8226; Pour le second, le score est donn&#233; par la valeur associ&#233;e au n&#339;ud n2,2, la coh&#233;sion lexicale de
l&#8217;arc e22,31 et la rupture lexicale entre le segment contenant &#224; la fois u1 et u2 et le segment
contenant seulement u3.
</p>
<p>Si dans l&#8217;exemple donn&#233; (cf. FIGURE 1) le score le plus &#233;lev&#233; est obtenu pour le chemin form&#233;
de e01,11e11,32e32,41, la segmentation de probabilit&#233; maximum est [u1][u2u3][u4]. Utiliser cette
repr&#233;sentation nous permet donc de consid&#233;rer tous les chemins possibles de longueurs variables,
traitant ainsi toutes les combinaisons possibles de segments cons&#233;cutifs pour le calcul de la
coh&#233;sion lexicale et &#233;galement de la rupture lexicale.
</p>
<p>TALN-R&#201;CITAL 2013, 17-21 Juin, Les Sables d&#8217;Olonne
</p>
<p>208 c&#65535; ATALA</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>FIGURE 1 &#8211; Un exemple de treillis de segmentation
</p>
<p>4 Exp&#233;riences
</p>
<p>Nous pr&#233;sentons ici les exp&#233;riences r&#233;alis&#233;es en fournissant tout d&#8217;abord des d&#233;tails sur les
transcriptions de journaux TV et les donn&#233;es textuelles utilis&#233;es, puis en analysant les r&#233;sultats
obtenus.
</p>
<p>4.1 Corpus
</p>
<p>Deux corpora sont consid&#233;r&#233;s dans notre t&#226;che de segmentation th&#233;matique. Le premier est un
corpus de journaux TV contenant 56 journaux (&#8764;1/2 heure chacun), enregistr&#233;s de f&#233;vrier &#224;
mars 2007 sur la cha&#238;ne de TV fran&#231;aise France 2. Les journaux consistent en une succession
de reportages de courte dur&#233;e (2-3 mn), contenant tr&#232;s peu de r&#233;p&#233;titions de mots par rapport
&#224; d&#8217;autres types d&#8217;&#233;missions, des synonymes &#233;tant fr&#233;quemment pr&#233;f&#233;r&#233;s. Les transcriptions
utilis&#233;es dans les exp&#233;riences proviennent de deux syst&#232;mes de transcription : IRENE, le syst&#232;me
de l&#8217;IRISA, et LIMSI, le syst&#232;me du Laboratoire d&#8217;Informatique pour la M&#233;canique et les Sciences
de l&#8217;Ing&#233;nieur. IRENE a un taux d&#8217;erreurs mots plus &#233;lev&#233; d&#8217;environ 7%. La segmentation de
r&#233;f&#233;rence a &#233;t&#233; cr&#233;&#233;e en associant un th&#232;me &#224; chaque reportage. Les fronti&#232;res th&#233;matiques sont
donc plac&#233;es au d&#233;but de l&#8217;introduction du reportage et &#224; la fin de ses remarques conclusives.
</p>
<p>Le second corpus est un jeu de donn&#233;es artificiel propos&#233; par Choi (2000) et utilis&#233; par diff&#233;rents
auteurs pour comparer leurs m&#233;thodes &#224; des approches existantes. Il consiste en 700 documents
cr&#233;&#233;s par concat&#233;nation de 10 parties de textes correspondant chacune aux z premi&#232;res phrases
d&#8217;articles choisis al&#233;atoirement dans le corpus Brown, z &#233;tant lui-m&#234;me choisi al&#233;atoirement dans
un intervalle fix&#233;. Une limite de ce jeu de donn&#233;es est qu&#8217;il comporte donc des changements
th&#233;matiques tr&#232;s brutaux, ce qui est rarement le cas dans des documents classiques. Cependant,
il est int&#233;ressant car il contient des segments de longueurs variables.
</p>
<p>TALN-R&#201;CITAL 2013, 17-21 Juin, Les Sables d&#8217;Olonne
</p>
<p>209 c&#65535; ATALA</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Transcriptions Manuelles IRENE automatiques LIMSI automatiques
Gain de F1-mesure 0.77 0.2 0.5
</p>
<p>TABLE 1 &#8211; Gain en F1-mesure pour les transcriptions manuelles et automatiques de journaux TV
</p>
<p>4.2 R&#233;sultats
</p>
<p>Nous pr&#233;sentons dans cette sous-section l&#8217;impact de notre mod&#232;le statistique sur la t&#226;che de
segmentation th&#233;matique de journaux TV et de donn&#233;es textuelles. Les r&#233;sultats sont compar&#233;s &#224;
ceux d&#8217;un syst&#232;me basique et bien que les am&#233;liorations obtenues soient limit&#233;es, elles montrent
nettement l&#8217;int&#233;r&#234;t de combiner rupture et coh&#233;sion lexicales. Pour les journaux TV, le traitement
de ces donn&#233;es difficiles diminue les capacit&#233;s de notre m&#233;thode et, pour cette raison, des
transcriptions manuelles ont &#233;galement &#233;t&#233; consid&#233;r&#233;es lors des exp&#233;riences.
</p>
<p>Pour l&#8217;&#233;valuation, des mesures de rappel, pr&#233;cision et F1-mesure ont &#233;t&#233; utilis&#233;es apr&#232;s alignement
de la r&#233;f&#233;rence et des fronti&#232;res propos&#233;es. Une tol&#233;rance de 10 secondes dans le positionnement
est autoris&#233;e dans le cas des transcriptions de journaux TV, et de 2 phrases pour les donn&#233;es
textuelles. Le rappel correspond &#224; la part de fronti&#232;res de r&#233;f&#233;rence d&#233;tect&#233;es par la m&#233;thode et
la pr&#233;cision au ratio des fronti&#232;res produites appartenant &#224; la segmentation de r&#233;f&#233;rence. La F1-
mesure combine rappel et pr&#233;cision en une valeur unique. D&#8217;autres mesures ont &#233;t&#233; pr&#233;c&#233;demment
propos&#233;es pour &#233;valuer la segmentation th&#233;matique de textes. Cependant, contrairement &#224; la
mesure Pk (Beeferman et al., 1997), le rappel et la pr&#233;cision ne sont pas sensibles aux variations
de tailles des segments et ces mesures ne favorisent pas les segmentations avec peu de fronti&#232;res
comme la mesure WindowDiff (Pevzner and Hearst, 2002), ce qui justifie notre choix.
</p>
<p>Les tests effectu&#233;s ont consist&#233; &#224; faire varier les param&#232;tres &#945; et &#955; de l&#8217;&#233;quation 7, &#945; permettant
diff&#233;rents compromis entre les valeurs de pr&#233;cision et de rappel, tandis que &#955; donne plus ou
moins d&#8217;importance &#224; la rupture.
</p>
<p>Parmi les diverses configurations test&#233;es dans les exp&#233;riences, seules quelques-unes sont pr&#233;sen-
t&#233;es ici. La figure 2 illustre tout d&#8217;abord les r&#233;sultats obtenus pour la segmentation des journaux
TV transcrits par les deux syst&#232;mes de RAP, en les comparant au syst&#232;me de r&#233;f&#233;rence correspon-
dant &#224; l&#8217;algorithme d&#8217;Utiyama et Isahara (2001) standard. Les valeurs pr&#233;sent&#233;es correspondent
&#224; des pond&#233;rations TF-IDF lors de l&#8217;&#233;valuation de la rupture lexicale, les r&#233;sultats obtenus avec
Okapi &#233;tant similaires. Nous constatons que les pr&#233;cision et rappel pour le corpus LIMSI sont
sup&#233;rieurs &#224; ceux du corpus IRENE, ce qui se justifie par le taux d&#8217;erreur de transcription plus
&#233;lev&#233; de ce dernier. Notre m&#233;thode reposant sur la coh&#233;rence du vocabulaire, l&#8217;am&#233;lioration
assez faible obtenue par rapport au syst&#232;me &#233;talon s&#8217;explique par le fait que les transcriptions
sont des donn&#233;es difficiles, contenant des segments tr&#232;s courts et peu de r&#233;p&#233;titions. Le gain
en F1-mesure lors de la segmentation des transcriptions manuelles et automatiques est donn&#233;
dans le tableau 1. Ces r&#233;sultats ne concernent toutefois que 6 journaux TV, la F1-mesure retenue
correspondant aux segmentations fournissant le nombre de fronti&#232;res le plus proche de celui de
la r&#233;f&#233;rence. Le gain est inf&#233;rieur l&#224; encore pour les transcriptions IRENE dont le taux d&#8217;erreur
est plus &#233;lev&#233;. Avoir &#224; sa disposition moins de mots potentiellement r&#233;p&#233;t&#233;s accro&#238;t la difficult&#233;
de discriminer entre des segments appartenant &#224; des th&#232;mes diff&#233;rents. Cependant notre mod&#232;le
parvient &#224; am&#233;liorer la segmentation m&#234;me pour ces donn&#233;es bruit&#233;es.
</p>
<p>Notre m&#233;thode offrant une am&#233;lioration limit&#233;e sur la segmentation des transcriptions de
</p>
<p>TALN-R&#201;CITAL 2013, 17-21 Juin, Les Sables d&#8217;Olonne
</p>
<p>210 c&#65535; ATALA</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>FIGURE 2 &#8211; Courbe rappel/pr&#233;cision pour les transcriptions obtenues gr&#226;ce aux syst&#232;mes de
reconnaissance de la parole LIMSI et IRENE. UI repr&#233;sente les r&#233;sultats obtenus gr&#226;ce &#224; la
seule coh&#233;sion lexicale ; &#955;&#8722; value indique l&#8217;importance donn&#233;e &#224; la rupture lexicale dans notre
approche
</p>
<p>journaux TV, nous avons &#233;galement utilis&#233; le corpus de Choi afin de v&#233;rifier que notre mod&#232;le
fonctionnait bien sur des donn&#233;es plus classiques. Par ailleurs, le jeu de donn&#233;es artificiel de Choi
nous permet d&#8217;observer le comportement de notre approche lorsque les longueurs des segments
varient. Les r&#233;sultats de notre m&#233;thode sur le corpus de Choi sont pr&#233;sent&#233;s sur la figure 3.
</p>
<p>Les nombres mentionn&#233;s sur chaque figure (par exemple 3-5, 3-11) correspondent &#224; l&#8217;intervalle de
valeurs pour z. Les r&#233;sultats de diff&#233;rents &#233;chantillons du jeu de donn&#233;es sont fournis. On observe
que lorsque notre algorithme traite des textes &#233;crits, il obtient de meilleures performances,
augmentant les valeurs de rappel et de pr&#233;cision. Plus les segments sont longs en moyenne,
plus importante est l&#8217;am&#233;lioration apport&#233;e par la prise en compte de la rupture. Cependant les
param&#232;tres utilis&#233;s doivent encore &#234;tre ajust&#233;s pour que l&#8217;importance donn&#233;e &#224; la rupture, pour
tout type de donn&#233;es, soit fix&#233;e et soit capable d&#8217;assigner la p&#233;nalit&#233; n&#233;cessaire aux poids calcul&#233;s.
Nous avons observ&#233; qu&#8217;il ne semble pas y avoir de valeur pr&#233;cise &#224; donner &#224; l&#8217;importance de la
rupture ; cependant les valeurs plus &#233;lev&#233;es conduisent &#224; un rappel plus bas et une pr&#233;cision plus
&#233;lev&#233;e, conduisant &#224; une sous-segmentation.
</p>
<p>TALN-R&#201;CITAL 2013, 17-21 Juin, Les Sables d&#8217;Olonne
</p>
<p>211 c&#65535; ATALA</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>FIGURE 3 &#8211; Courbes rappel/pr&#233;cision obtenues sur le corpus de Choi
</p>
<p>5 Conclusions
</p>
<p>Nous avons propos&#233; une m&#233;thode originale de segmentation th&#233;matique qui combine la coh&#233;sion
lexicale et la rupture lexicale, identifiant des zones de continuit&#233;s et de ruptures dans l&#8217;organisa-
tion globale des donn&#233;es. Les r&#233;sultats obtenus montrent que la combinaison des deux mesures
produit des segmentations de meilleure qualit&#233; que lors de l&#8217;emploi de la seule coh&#233;sion lexicale.
Il reste toutefois encore des possibilit&#233;s d&#8217;am&#233;liorer notre approche.
</p>
<p>Nous proposons comme perspectives d&#8217;employer d&#8217;autres techniques de calcul de la rupture
lexicale. Parmi elles, la vectorisation (Claveau and Lef&#232;vre, 2011) implique une comparaison
indirecte entre des segments cons&#233;cutifs, en proposant un changement dans l&#8217;espace de repr&#233;sen-
tation des segments et l&#8217;utilisation de documents pivots pour le calcul de la rupture. Les segments
ne partageant pas beaucoup de vocabulaire quoiqu&#8217;abordant le m&#234;me th&#232;me pourraient alors
&#234;tre consid&#233;r&#233;s comme similaires. Cette m&#233;thode pourrait donc permettre de pallier le manque
de r&#233;p&#233;titions de mots qui appara&#238;t particuli&#232;rement dans le cas de transcriptions de journaux
TV. Par ailleurs, une fa&#231;on de r&#233;gler finement les param&#232;tres &#945; and &#955; utilis&#233;s dans notre mod&#232;le
statistiques doit &#234;tre d&#233;termin&#233;e.
</p>
<p>R&#233;f&#233;rences
</p>
<p>Allan, J., editor (2002). Topic Detection and Tracking : event-based information organization.
Kluwer Academic Publishers.
</p>
<p>Beeferman, D., Berger, A., and Lafferty, J. (1997). Text segmentation using exponential models.
In Proceedings of the 2nd Conference on Empirical Methods in Natural Language Processing, pages
35&#8211;46.
</p>
<p>Blei, D. and Moreno, P. (2001). Topic segmentation with an aspect hidden Markov model. In
Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development
in Information Retrieval, pages 343&#8211;348.
</p>
<p>Brown, G. and Yule, G. (1983). Discourse analysis. Cambridge University Press.
</p>
<p>TALN-R&#201;CITAL 2013, 17-21 Juin, Les Sables d&#8217;Olonne
</p>
<p>212 c&#65535; ATALA</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Choi, F. Y. Y. (2000). Advances in domain independent linear text segmentation. In Procee-
dings of the 1st International Conference of the North American Chapter of the Association for
Computational Linguistics, pages 26&#8211;33.
Claveau, V. (2012). Vectorisation, Okapi et calcul de similarit&#233; pour le TAL : pour oublier enfin
le TF-IDF. Actes de la conf&#233;rence conjointe JEP-TALN-RECITAL 2012, pages 85&#8211;98.
Claveau, V. and Lef&#232;vre, S. (2011). Topic segmentation of TV-streams by mathematical morpho-
logy and vectorization. In Proceedings of the 12th International Conference of the International
Speech Communication Association, Interspeech&#8217;11, pages 1105&#8211;1108.
Ferret, O., Grau, B., and Masson, N. (1998). Thematic segmentation of texts : Two methods for
two kinds of texts. In Proceedings of the 36th Annual Meeting of the Association for Computational
Linguistics and 17th International Conference on Computational Linguistics, pages 392&#8211;396.
Galley, M., McKeown, K., Fosler-Lussier, E., and Jing, H. (2003). Discourse segmentation
of multi-party conversation. In Proceedings of the 41st Annual Meeting of the Association for
Computational Linguistics, ACL, pages 562&#8211;569.
Georgescul, M., Clark, A., and Armstrong, S. (2006). Word distributions for thematic seg-
mentation in a support vector machine approach. In Proceedings of the 10th Conference on
Computational Natural Language Learning, CoNLL-X, pages 101&#8211;108.
Grosz, B. J. and Sidner, C. L. (1986). Attention, intentions, and the structure of discourse.
Computational Linguistics, 12(3) :175&#8211;204.
Guinaudeau, C., Gravier, G., and S&#233;billot, P. (2012). Enhancing lexical cohesion measure with
confidence measures, semantic relations and language model interpolation for multimedia
spoken content topic segmentation. Computer Speech and Language, 26(2) :90&#8211;104.
Guinaudeau, C. and Hirschberg, J. (2011). Accounting for prosodic information to improve
ASR-based topic tracking for TV broadcast news. In 12th Annual Conference of the International
Speech Communication Association, Interspeech&#8217;11, pages 1401&#8211;1404.
Hearst, M. A. (1997). TextTiling : Segmenting text into multi-paragraph subtopic passages.
Computational Linguistics, 23(1) :33&#8211;64.
Hernandez, N. and Grau, B. (2002). Analyse th&#233;matique du discours : segmentation, struc-
turation, description et repr&#233;sentation. In Actes du 5e colloque international sur le document
&#233;lectronique, pages 277&#8211;285.
Huet, S., Gravier, G., and S&#233;billot, P. (2008). Un mod&#232;le multi-sources pour la segmentation en
sujets de journaux radiophoniques. In Actes de 15e conf&#233;rence sur le traitement automatique des
langues naturelles, TALN&#8217;08, pages 49&#8211;58.
Litman, D. J. and Passonneau, R. J. (1995). Combining multiple knowledge sources for discourse
segmentation. In Proceedings of the 33rd Annual Meeting of the Association for Computational
Linguistics, pages 108&#8211;115.
Malioutov, I. and Barzilay, R. (2006). Minimum cut model for spoken lecture segmentation.
In Proceedings of the 21st International Conference on Computational Linguistics and the 44th
Annual Meeting of the Association for Computational Linguistics, pages 25&#8211;32.
Misra, H. and Yvon, F. (2010). Mod&#232;les th&#233;matiques pour la segmentation de documents. In
Actes des 10e journ&#233;es internationales d&#8217;analyse statistique des donn&#233;es textuelles, pages 203&#8211;213.
Moens, M.-F. and Busser, R. D. (2001). Generic topic segmentation of document texts. In
Proceedings of the 24th International Conference on Research and Developement in Information
Retrieval, pages 418&#8211;419.
</p>
<p>TALN-R&#201;CITAL 2013, 17-21 Juin, Les Sables d&#8217;Olonne
</p>
<p>213 c&#65535; ATALA</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Pevzner, L. and Hearst, M. A. (2002). A critique and improvement of an evaluation metric for
text segmentation. Computational Linguistics, 28 :19&#8211;36.
</p>
<p>Purver, M. (2011). Topic segmentation. In Tur, G. and de Mori, R., editors, Spoken Language
Understanding : Systems for Extracting Semantic Information from Speech, chapter 11, pages
291&#8211;317. Wiley.
</p>
<p>Reynar, J. C. (1994). An automatic method of finding topic boundaries. In Proceedings of the
32nd Annual Meeting on Association for Computational Linguistics, pages 331&#8211;333.
</p>
<p>Riedl, M. and Biemann, C. (2012). How text segmentation algorithms gain from topic models. In
Proceedings of the Conference of the North American Chapter of the Association for Computational
Linguistics : Human Language Technologies, pages 553&#8211;557.
</p>
<p>Utiyama, M. and Isahara, H. (2001). A statistical model for domain-independent text segmenta-
tion. In Proceedings of the 39th Annual Meeting on the Association for Computational Linguistics,
pages 499&#8211;506.
</p>
<p>Yamron, J., Carp, I., Gillick, L., Lowe, S., and van Mulbregt P. (1998). A hidden Markov model
approach to text segmentation and event tracking. In Proceedings of the IEEE International
Conference on Acoustics, Speech, and Signal Processing, ICASSP, pages 333&#8211;336.
</p>
<p>TALN-R&#201;CITAL 2013, 17-21 Juin, Les Sables d&#8217;Olonne
</p>
<p>214 c&#65535; ATALA</p>

</div></div>
</body></html>