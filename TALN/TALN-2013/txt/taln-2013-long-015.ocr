TALN-RECITAL 2013, 17-21 Iuin, Les Sables d’Olonne

Un modele segmental probabiliste combinant cohésion
lexicale et rupture lexicale pour la segmentation thématique

Anca Simonl Guillaume Gravierz Pascale Sébillot3
(1) Université de Rennes 1
(2) CNRS
(3) INSA de Rennes
IRISA 8: INRIA Rennes
anca—roxana . simon@irisa . fr , guillaume . gravie:r@irisa . fr ,pas<:ale . sebi11ot@i:risa . fr

RESUME
L’identiﬁcau'on d’une structure thématique dans des données textuelles quelconques est une tache
difﬁcile. La plupart des techniques existantes reposent soit sur la maximisation d’une mesure de
cohésion lexicale au sein d’un segment, soit sur la détection de ruptures lexicales. Nous proposons
une nouvelle technique combinant ces deux criteres de maniére a obtenir le meilleur compromis
entre cohésion et rupture. Nous déﬁnissons un nouveau modele probabiliste, fondé sur l’approche
proposée par Utiyama et Isahara (2001), en préservant les propriétés d’indépendance au domaine
et de faible a priori de cette derniére. Des évaluations sont menées sur des textes écrits et sur
des transcriptions automatjques de la parole a la télévision, transcriptions qui ne respectent pas
les normes des textes écrits, ce qui accroit la difﬁculté. Les résultats expérimentaux obtenus
démontrent la pertinence de la combinaison des criteres de cohésion et de rupture.

ABSTRACT
A probabilistic segment model combining lexical cohesion and disruption for topic seg-
mentation

Identifying topical structure in any text-like data is a challenging task. Most existing techniques
rely either on maximizing a measure of the lexical cohesion or on detecting lexical disruptions.
A novel method combining the two criteria so as to obtain the best trade—off between cohesion
and disruption is proposed in this paper. A new statistical model is deﬁned, based on the work of
Isahara and Utiyama (2001), maintaining the properties of domain independence and limited a
priori of the latter. Evaluations are performed both on written texts and on automatic transcripts
of TV shows, the latter not respecting the norms of written texts, thus increasing the difficulty
of the task. Experimental results demonstrate the relevance of combining lexical cohesion and
disrupture.

MOTS-CLES : segmentation thématique, cohésion lexicale, rupture de cohésion, journaux
télévisés.

KEYWORDS: topic segmentation, lexical cohesion, lexical disrupture, TV broadcast news.

202 © ATALA

TALN-RECITAL 2013, 17-21 Iuin, Les Sables d’Olonne
1 Introduction

La segmentation thématique consiste a mettre en évidence la structure sémantique d’un do-
cument et les algorithmes développés pour cette tache visent a détecter automatiquement les
frontieres qui déﬁnissent des segments thématiquement cohérents. Cible de nombreux travaux,
la segmentation thématique a également des retombées en recherche d’information, résumé
automatique, systémes de question—réponse...

Diverses méthodes de segmentation de données textuelles ont été proposées dans la littérature
(Yamron et al., 1998; Georgescul et al., 2006; Galley et al., 2003; Hearst, 1997; Reynar, 1994;
Moens and Busser, 2001; Choi, 2000; Ferret et al., 1998; Utiyama and Isahara, 2001). Comme
indiqué dans (Purver, 2011), elles peuvent étre supervisées ou non, reposer sur des changements
de vocabulaire, des techniques de clustering, sur la détection de frontieres discriminantes
ou sur des modeles probabilistes. Déterminer les segments thématiques a l’aide de modeles
probabiliste consiste la plupart du temps a inférer la séquence de themes la plus probable a
partir des mots observés et a dériver les positions des frontiéres (Yamron et al., 1998; Blei and
Moreno, 2001). Ces modeles utilisent un corpus d’apprentissage pour estimer les distributions
documents—themes et themes—mots. Des travaux récents ont montré l’intérét de l’intégration
de ces modeles probabilistes dans les algorithmes de segmentation de textes reposant sur la
similarité de vocabulaire (Misra and Yvon, 2010; Riedl and Biemann, 2012). Nos travaux portent
sur les méthodes non supervisées. La plupart d’entre elles repose sur la cohésion du vocabulaire
pour identiﬁer des segments cohérents dans les textes, exploitant les mots qu’ils contiennent et les
relations sémantiques que ces mots entretiennent. Pour mesurer la cohérence dans les (segments
de) textes, la cohésion lexicale, fondée sur la répétition de mots ou sur 1’exploitation de chaines
lexicales, est fréquemment retenue en privilégiant l’une ou l’autre des deux stratégies suivantes :
soit on cherche a maximiser la mesure de cohésion lexicale des segments, en regroupant les
portions de texte lexicalement cohérentes, soit on cherche a identiﬁer des ruptures entre les
segments en placant des frontiéres quand survient un changement signiﬁcatif dans le vocabulaire
utilisé (Hearst, 1997). Dans cet article, notre objectif est de proposer une nouvelle solution pour
la segmentation thématique de documents qui consiste a méler ces deux approches, c’est—a—dire a
combiner les mesures de cohésion lexicale et de rupture lexicale aﬁn d’obtenir une segmentation
en fragments 31 la fois thématiquement cohérents et différents les uns des autres.

La technique que nous proposons peut s’appliquer a tout type de données textuelles et est indé—
pendante d’un domaine particulier. Notre objectif est cependant de l’appliquer a la segmentation
de journaux télévisés aﬁn de permettre a des utilisateurs de naviguer dans ce type de données.
De maniere a rester générique et non supervisée, la segmentation thématique peut dans ce cas
s’appuyer sur la transcription automatique de la parole prononcée dans les émissions. L’analyse
des mots de la transcription vise alors a trouver un changement signiﬁcatif de vocabulaire et
donc un changement de theme (Hearst, 1997). Cependant, les particularités des transcriptions
automatiques accroissent la difﬁculté de la tache de segmentation. En effet, ces transcriptions ne
contiennent ni casse, ni ponctuation, et ne sont donc pas structurées en phrases comme des textes
standards mais en groupes de souﬂe correspondant aux mots prononcés par une personne entre
deux inspirations. De plus, elles peuvent contenir de nombreux mots mal transcrits. Difﬁculté
supplémentaire, les journaux TV peuvent avoir des segments thématiques trés courts, contenant
peu de mots et donc peu de répétitions, en particulier quand le présentateur fait volontairement
usage de synonymes. Cela rend l’utilisation du critére de cohésion lexicale particuliérement ardue.
Notre algorithme de segmentation thématique ayant un fort potentiel pour traiter ces cas, nous

203 © ATALA

TALN-RECITAL 2013, 17-21 Iuin, Les Sables d’Olonne
avons souhaité le tester sur ces données difﬁciles.

La technique présentée ici repose sur l’algorithme de segmentation de textes proposé par Utiyama
et Isahara (2001), algorithme dont les capactiés ont été attestées pour le texte écrit. C’est un
modele probabiliste qui fournit une segmentation non supervisée. Dans cette approche, il n’y a
done pas de tentative d’apprenu'ssage de l’ensemble des modeles thématiques le plus probable a
partir des données d’apprenu'ssage, mais au contraire l’ensemble est généré par l’algorithme étant
donnés les textes a segmenter. Ce modele est indépendant du domaine et permet l’obtention de
segments de longueurs tres variées. I1 consiste en une représentation du document a segmenter
sous forme d’un graphe, ou les noeuds représentent les frontieres thématiques potentielles et les
arcs les segments. La segmentation thématique est obtenue en trouvant le meilleur chemin dans
le graphe valué, dans lequel les poids reﬂetent la valeur de cohésion lexicale. Notre contribution
consiste a déﬁnir un modele statistique amélioré qui permet d’intégrer la rupture lexicale. Par
conséquent, notre algorithme se résume en un décodage d’un treillis aﬁn d’identiﬁer la meilleure
segmentation. Cette représentation permet de considérer la valeur de rupture lexicale en chaque
noeud. La solution proposée est testée pour la segmentation de journaux TV transcrits mais
également de textes écrits, et les évaluations montrent une amélioration en précision et rappel
par rapport a la seule utilisation de la valeur de la cohésion lexicale.

L’article est organisé de la facon suivante : des travaux en segmentation thématique existants
sont présentés dans la section 2. Dans la section 3, nous détaillons notre approche, en décrivant
d’abord le modele général d’Uu'yama et Isahara puis le nouveau modele statistique proposé. Dans
la section 4, les expériences sont présentées, avec des détails sur les corpus utilisés et une analyse
des résultats.

2 Techniques de segmentation thématique

Dans cette section, nous présentons rapidement les notions—clés concernant le concept de seg-
mentation thématique, ainsi que les techniques existantes et les traits qu’elles exploitent pour
réaliser cette tache.

2.1 Le concept de theme

Le concept de theme est difﬁcile a déﬁnir précisément et les linguistes qui ont tenté de le
caractériser en offrent de nombreuses déﬁnitions. Dans (Brown and Yule, 1983), la difﬁculté de
déﬁnir un theme est longuement discutée et les auteurs soulignent que : "The notion of ’topic’ is
clearly an intuitively satisfactory way of describing the unifying principle which makes one stretch
of discourse ’about’ something and the next stretch ’about’ something else, for it is appealed to very
frequently in the discourse analysis literature. Yet the basis for the identification of ’topic’ is rarely
made explicit.".

Souhaitant appliquer la segmentation thématique a des journaux T\l nous avons cherché a Voir
si la notion de theme avait été déﬁnie dans le contexte d’émissions télévisées. Le projet Topic
Detection and Hacking (Allan, 2002) s’est par exemple focalisé sur le repérage de segments de

journaux TV thématiquement liés. Dans ce cadre, les notions d’eVenement et de theme ont été
déﬁnies : un événement est quelque chose qui se produit a un instant et un endroit spéciﬁque

204 © ATALA

TALN-RECITAL 2013, 17-21 Iuin, Les Sables d’Olonne

et qui est associé a des actions particuliéres; un theme est, quant a lui, l’ensemble formé d’un
événement et de tous les événements qui lui sont directement liés. Un événement est donc
relativement court et évolue dans le temps, tandis qu’un theme est plus stable et plus long.

Dans notre cadre de segmentation de joumaux T\l un theme correspond a un reportage qui forme
une unité sémantique cohérente dans la structure d’un journal. Notre algorithme est également
évalué sur des textes écrits, formés par concaténation de parties extraites d’articles sélectionnés
aléatoirement dans le corpus Brown (Choi, 2000) ; un theme est alors associé a chaque partie
formant le texte ﬁnal.

2.2 Méthodes pour la segmentation thématique

Pour réaliser la segmentation thématique de textes, diverses caractéristiques peuvent étre ex-
ploitées aﬁn d’identiﬁer les changements thématiques. Elles peuvent reposer sur la cohésion
lexicale (1'.e., prendre en compte les informations de distribution du vocabulaire) ou sur des
marqueurs linguistiques tels que des indices prosodiques (Guinaudeau and Hirschberg, 2011)
ou des marqueurs du discours (Grosz and Sidner, 1986; Litman and Passonneau, 1995). Les
techniques génériques, qui sont celles qui nous intéressent ici, exploitent traditionnellement la
seule cohésion lexicale, indépendante du type de documents considérés et ne nécessitant pas de
phase d’apprenu'ssage. L’idée—clé des méthodes fondées sur la cohésion lexicale est de considérer
qu’un changement signiﬁcatif dans le vocabulaire utilisé est un signe de changement thématique.
Ces approches peuvent étre divisées en deux familles :

0 les méthodes locales (Hearst, 1997; Hernandez and Grau, 2002; Ferret et al., 1998; Claveau
and Lefevre, 2011) qui cherchent a repérer localement les ruptures lexicales;

0 les méthodes globales (Reynar, 1994; Choi, 2000; Utiyama and Isahara, 2001; Malioutov and
Barzilay, 2006; Misra and Yvon, 2010) exploitant une mesure de la cohésion lexicale.

Une méthode locale repose sur la comparaison locale de régions du document et associe un
changement thématique aux endroits o1‘1 il y a une similarité faible entre deux régions consécutives
(1'.e., elles identiﬁent les zones de fortes ruptures lexicales). Par exemple, TextTiling (Hearst,
1997), qui est considéré comme un algorithme de segmentation thématique fondamental, analyse
le texte a l’aide d’une fenétre glissante qui couvre des blocs adjacents de texte et est centrée en
un point du texte correspondant a une frontiére thématique potentielle. Les contenus avant et
apres chaque frontiére possible sont représentés par des vecteurs de mots pondérés, un poids
fort indiquant qu’un mot est particulierement pertinent pour décrire un contenu. Une mesure de
similarité, par exemple cosinus, est calculée entre les deux vecteurs. Plus l’angle entre les deux
vecteurs diminue, plus le cosinus approche de 1, indiquant par la—méme la plus grande similarité
entre les contenus avant et apres la frontiére potentielle. Les valeurs de similarité sont calculées
a chaque frontiere possible et la séquence résultante de valeurs de similarité est analysée. Les
points de scores de similarité les plus bas (1'.e., forte rupture) représentent alors les frontieres
thématiques. Ce type de méthode locale présente certains désavantages dont une sensibilité aux
variations de tailles des segments dans les textes puisqu’un voisinage de taille ﬁxe est considéré,
ainsi qu’une difﬁculté de choix de la valeur de seuil pour décider qu’une rupture est sufﬁsamment
forte pour placer une frontiere.

Une méthode globale réalise quant a elle une comparaison globale entre toutes les régions du
document, en cherchant a maximiser globalement la valeur de la cohésion lexicale. Dans Utiyama
et Isahara (2001), la valeur de la cohésion lexicale d’un segment S,- est vue comme la mesure de

205 © ATALA

TALN-RECITAL 2013, 17-21 Iuin, Les Sables d’Olonne

la capacité d’un modele de langue A,-, appris sur le segment S,-, a prédire les mots du segment. Le
modele de langue Ai doit donc d’abord étre estimé, puis la probabilité généralisée des mots du
segment S,-, étant donné A,-, doit étre déterminée. Apres le calcul de la valeur de cohésion lexicale
pour chaque segment, la segmentation maximisant globalement cette valeur est choisie. Cet
algorithme s’est avéré performant au regard d’autres algorithmes de segmentation thématique de
textes tels que ceux de Choi (2000) ou Reynar (1994). Cependant, la limite principale de ce type
de méthode globale est un risque de sur—segmentation.

L’originalité de la solution que nous proposons consiste dans la combinaison des deux types de
méthodes. Une méthode fondée sur le méme principe, visant a capturer dans une vue globale
des dissimilarités locales, a été présentée dans (Malioutov and Barzilay, 2006), mais, d’une part,
le nombre de segments a trouver est ﬁxé a priori et, d’autre part, la couverture est limitée car la
dissimilarité entre segments est calculée en utilisant une fenétre.

Le point de départ de notre méthode est le modéle statistique proposé dans (Utiyama and
Isahara, 2001), qui est ﬂexible et offre des possibilités d’extension par intégration de nouvelles
informations. Plusieurs travaux 1’ont déja utilisé avec succés dans le contexte de la segmentation
de journaux TV (Huet et al., 2008; Guinaudeau et al., 2012), le modiﬁant pour intégrer des
connaissances spéciﬁques aux émissions TV Contrairement a ces travaux, nous avons redéﬁni le
modéle de (Utiyama and Isahara, 2001) aﬁn qu’il puisse prendre en compte non seulement la
cohésion mais aussi la rupture lexicale et, par conséquent, améliorer la segmentation de tout
type de données textuelles. Considérer la rupture est en particulier intéressant pour traiter les
cas de textes contenant des changements brutaux de vocabulaire. La facon dont nous combinons
les deux critéres est détaillée dans la section 3.

3 Combinaison de la cohésion et de la rupture lexicales

Nous rappelons tout d’abord l’algorithme de Utiyama et Isahara, puis expliquons le nouveau
modéle statistique que nous proposons.

3.1 Le modéle statistique

L’algorithme proposé par Utiyama et Isahara déﬁnit un modele probabiliste et consiste a détermi-
ner la segmentation qui produit les segments les plus cohérents d’un point de vue lexical tout en
respectant une distribution a priori de la longueur des segments. L’idée principale est de trouver
la segmentation la plus probable pour une séquence de t unités élémentaires (1'.e., phrases ou
énoncés composés de mots) W = u; parmi toutes les segmentations possibles, 1'.e.,

§=argm§axP[W|S]P[S] . (1)

En admettant que chaque segment est une unité indépendante du reste du texte et que les mots
contenus dans un segment sont eux aussi indépendants, la probabilité du texte W pour une
segmentation S = Si" est donnée par

m 711‘

P[W|S{"] = ]_[]_[P[w;.Is,-J . (2)
i=1 j=1
206 ©ATALA

TALN-RECITAL 2013, 17-21 Iuin, Les Sables d’Olonne

ou ni est le nombre de mots du segment Si, wJ': est le je mot de Si et m le nombre de segments.

La probabilité P [wjl |Si] est donnée par une loi de Laplace dont les parametres sont estimés sur
Si‘, 1.3.,
.fi(M,l') + 1
,- _ J

P[WjlSi] — T + k , (3)
ou fi(wJl) est le nombre d’occurrences de wl dans Si et k est le nombre total de mots différents
dans le texte W (1'.e., la taille du vocabulaire). Cette probabilité Va favoriser les segments
homogenes car elle croit quand les mots sont répétés et décroit quand ils sont différents. La
distribution a priori des longueurs des segments est donnée par P [Sf] = n‘"‘, ou n est le nombre
total de mots. Elle a une valeur élevée quand le nombre de segments est faible, tandis que
P[W|S] a des valeurs élevées quand le nombre de segments est grand.

Cette approche peut étre vue comme la recherche du meilleur chemin dans un graphe valué,
graphe représentant toutes les segmentations possibles. Chaque noeud correspond a une frontiére
possible et un arc entre les noeuds i et j représente un segment contenant les unités comprises
entre ui+1 et ui. Le poids attribué a chaque arc de ce type est

v(i,j) = 2}: ln(P[uk|Si_,i]) — aln(n) , (4)

k=i+1

ou Si_,i est le segment correspondant a l’arc allant du noeud i au noeud j. Pour les petits segments,
la probabilité d’estimer les mots contenus dans le segment est plus faible; le facteur a fournit
un compromis entre la longueur moyenne des segments retournés et la valeur de la cohésion
lexicale.

3.2 Introduction de la rupture lexicale

Le modéle déﬁni ci—dessus suppose que chaque segment Si du texte est indépendant des autres, ce
qui ne permet pas de combiner la valeur de la cohésion lexicale et celle de la rupture lexicale. En
effet, lors du calcul du poids associé au segment Si, nous devrions ajouter une pénalité marquant
a quel point le contenu de Si differe de celui du segment précédent Si_1. Pour cette raison,
nous proposons une hypothese markovienne entre les segments nous permettant, pour chaque
segment, de considérer celui qui le précede. La probabilité d’un texte W pour une segmentation
S = S{" devient alors

Tfl
P[W|S{"] = P[WIs1] ]_[P[wIs,-,5,--1] . (5)
i=2

Pour déterminer la segmentation de probabilité maximum S, le coﬁt associé au segment Si, étant

donné Si_1, est

1
ln(P[WlSi:Si—1]): lnlP[VVilSi])—l( ) , (6)
cu A(Wi,Wi_1) est la valeur de rupture entre le contenu de Si et celui de Si_1, et A est un
paramétre qui permet de controler l’inﬂuence de la rupture dans le coﬁt. Wi représente les unités
élémentaires du segment Si. Choisir 1/A(Wi, Wi_i) conduit 2‘: une pénalité faible quand il y a une
forte rupture. Dans l’équation 6, P[W|Si, Si_1] ne représente plus une probabilité; cependant,
207 © ATALA

TALN-RECITAL 2013, 17-21 Iuin, Les Sables d’Olonne

puisque l’algorithme de segmentation consiste a déterminer le meilleur chemin dans un graphe
pondéré, cela n’a pas d’impact car aucune présupposition de graphe probabiliste n’est faite pour
segmenter. Par conséquent, la nouvelle déﬁnition de la segmentation la plus probable est

S = argm§xZ1:ln(P[VV,-IS,-D — lZ2:( ) — amln(n) . (7)

De l’équation 6, on peut déduire que, pour un noeud donné représentant une frontiére théma—
tique, tous les segments de longueurs différentes arrivant a ce noeud sont conservés. Au niveau
implémentation, nous déﬁnission un treillis dans lequel un arc e,-W-I représente une prolongation
d’un chemin de longueur l du noeud n,-P au noeud nil. Un noeud n,-1, rassemble donc tous les
segments de longueur p unités se terminant aprés u,-. Ceci signiﬁe qu’en chaque point du texte
ou une frontiére potentielle est considérée, nous analysons toutes les combinaisons possibles
d’unités consécutives précédant cette frontiére. Un arc e,-W-1 représente un segment contenant
toutes les unités entre u,-+1 et uj, avec j — i = 1. Un coﬁt est associé a chaque arc en se fondant
sur l’équation 6. D’une part, ce coﬁt consiste en la valeur de la cohésion lexicale du segment
couvert par l’arc calculé grace a l’équation 3 ; d’autre part, une pénalité est associée a chacune
des valeurs de ce type, en fonction de la rupture lexicale entre le segment couvert par l’arc et le
segment précédent dans le texte. Selon le noeud dont il provient, le segment précédent peut lui
aussi avoir différentes longueurs. Par conséquent, la rupture est calculée entre toutes les paires
possibles de segments. Pour obtenir la rupture, une mesure de similarité cosinus est utilisée entre
les vecteurs représentant

0 le segment qui contient les unités couvertes par l’arc (de score le plus élevé) arrivant au noeud
n,-J et

0 le segment qui contient les unités couvertes par l’arc sortant de ce noeud vers n,-+k,k.

Les vecteurs contiennent les poids associés aux mots dans les unités. Ces poids sont calculés en
utilisant les mesures de TF-IDF et Okapi (Claveau, 2012), transformées en dissimilarités.

Pour déterminer la meilleure segmentation, nous utilisons un algorithme de programmation
dynamique. Lors du décodage, on associe a chaque noeud le coﬁt du meilleur chemin en fonction
des arcs entrants. Par exemple dans la ﬁgure 1, les calculs au noeud n3,1 consistent a choisir la
valeur la plus élevée entre le poids associé a l’arc e21,31 et a l’arc e22,31.

0 Pour le premier arc, le score est donné par la valeur associée au noeud nu, la valeur de la
cohésion lexicale de l’arc e21,31 et la rupture entre le segment contenant uz et le segment
contenant u3.

0 Pour le second, le score est donné par la valeur associée au noeud nu, la cohésion lexicale de
l’arc e22,31 et la rupture lexicale entre le segment contenant a la fois ul et uz et le segment
contenant seulement u3.

Si dans l’exemple donné (cf. FIGURE 1) le score le plus élevé est obtenu pour le chemin formé
de e01,11e11,32e32,41, la segmentation de probabilité maximum est [u1][u2u3][u4]. Utiliser cette
représentation nous permet donc de considérer tous les chemins possibles de longueurs variables,
traitant ainsi toutes les combinaisons possibles de segments consécutifs pour le calcul de la
cohésion lexicale et également de la rupture lexicale.

208 © ATALA

TALN-RECITAL 2013, 17-21 Iuin, Les Sables d’Olonne

euu e1_L21 e21.31 931-:41

    

FIGURE 1 — Un exemple de treillis de segmentation

4 Expériences

Nous présentons ici les expériences réalisées en fournissant tout d’abord des détails sur les
transcriptions de journaux TV et les données textuelles utilisées, puis en analysant les résultats
obtenus.

4. 1 Corpus

Deux corpora sont considérés dans notre tache de segmentation thématique. Le premier est un
corpus de journaux TV contenant 56 journaux (~1/ 2 heure chacun), enregistrés de février 51
mars 2007 sur la chaine de TV francaise France 2. Les journaux consistent en une succession
de reportages de courte durée (2-3 mn), contenant tres peu de répétitions de mots par rapport
a d’autres types d’émissions, des synonymes étant fréquemment préférés. Les transcriptions
utilisées dans les expériences proviennent de deux systemes de transcription : IRENE, 1e systeme
de 1’IRISA, et LIMSI, 1e systéme du Laboratoire d’Informatique pour la Mécanique et les Sciences
de 1’Ingénieur. IRENE a un taux d’erreurs mots plus élevé d’environ 7%. La segmentation de
référence a été créée en associant un theme a chaque reportage. Les frontieres thématiques sont
donc placées au début de 1’introduction du reportage et a la ﬁn de ses remarques conclusives.

Le second corpus est un jeu de données artiﬁciel proposé par Choi (2000) et utilisé par différents
auteurs pour comparer leurs méthodes a des approches existantes. I1 consiste en 700 documents
créés par concaténation de 10 parties de textes correspondant chacune aux z premieres phrases
d’artic1es choisis aléatoirement dans le corpus Brown, z étant lui-meme choisi aléatoirement dans
un intervalle ﬁxé. Une limite de ce jeu de données est qu’i1 comporte donc des changements
thématiques trés brutaux, ce qui est rarement 1e cas dans des documents classiques. Cependant,
il est intéressant car i] contient des segments de longueurs variables.

209 © ATALA

TALN-RECITAL 2013, 17-21 Iuin, Les Sables d’Olonne

| Transcriptions | Manuelles | IRENE automatiques | LIMSI automatiques |
| Gain de F1—mesure | 0.77 | 0.2 | 0.5 |

TABLE 1 — Gain en F1—mesure pour les transcriptions manuelles et automatiques de journaux TV

4.2 Résultats

Nous présentons dans cette sous—section l’impact de notre modéle statistique sur la tache de
segmentation thématique de journaux TV et de données textuelles. Les résultats sont compares a
ceux d’un systéme basique et bien que les améliorations obtenues soient limitées, elles montrent
nettement 1’intérét de combiner rupture et cohésion lexicales. Pour les journaux TV le traitement
de ces données difﬁciles diminue les capacités de notre méthode et, pour cette raison, des
transcriptions manuelles ont également été considérées lors des expériences.

Pour l’évaluation, des mesures de rappel, précision et F1—mesure ont été utilisées apres alignement
de la référence et des frontieres proposées. Une tolérance de 10 secondes dans le positionnement
est autorisée dans le cas des transcriptions de journaux TV et de 2 phrases pour les données
textuelles. Le rappel correspond a la part de frontiéres de référence détectées par la méthode et
la précision au ratio des frontiéres produites appartenant a la segmentation de référence. La F1-
mesure combine rappel et précision en une valeur unique. D’autres mesures ont été précédemment
proposées pour évaluer la segmentation thématique de textes. Cependant, contrairement a la
mesure Pk (Beeferman et al., 1997), le rappel et la précision ne sont pas sensibles aux variations
de tailles des segments et ces mesures ne favorisent pas les segmentations avec peu de frontiéres
comme la mesure WindowDiﬁ° (Pevzner and Hearst, 2002), ce qui justiﬁe notre choix.

Les tests effectués ont consisté a faire varier les paramétres (1 et 1 de l’équation 7, a permettant
différents compromis entre les valeurs de précision et de rappel, tandis que A donne plus ou
moins d’importance a la rupture.

Parmi les diverses conﬁgurations testées dans les expériences, seules quelques—unes sont présen-
tées ici. La ﬁgure 2 illustre tout d’abord les résultats obtenus pour la segmentation des journaux
TV transcrits par les deux systémes de RAB en les comparant au systéme de référence correspon-
dant a l’algorithme d’Utiyama et Isahara (2001) standard. Les valeurs présentées correspondent
a des pondérations TF—IDF lors de l’évaluation de la rupture lexicale, les résultats obtenus avec
Okapi étant similaires. Nous constatons que les précision et rappel pour le corpus LIMSI sont
supérieurs a ceux du corpus IRENE, ce qui se justiﬁe par le taux d’erreur de transcription plus
élevé de ce dernier. Notre méthode reposant sur la cohérence du vocabulaire, l’amélioration
assez faible obtenue par rapport au systéme étalon s’explique par le fait que les transcriptions
sont des données difﬁciles, contenant des segments trés courts et peu de répétitions. Le gain
en F1—mesure lors de la segmentation des transcriptions manuelles et automatiques est donné
dans le tableau 1. Ces résultats ne concernent toutefois que 6 journaux TV la F1—mesure retenue
correspondant aux segmentations fournissant le nombre de frontiéres le plus proche de celui de
la référence. Le gain est inférieur la encore pour les transcriptions IRENE dont le taux d’erreur
est plus élevé. Avoir a sa disposition moins de mots potentiellement répétés accroit la difﬁculte’
de discriminer entre des segments appartenant a des themes différents. Cependant notre modéle
parvient a améliorer la segmentation méme pour ces données bruitées.

Notre méthode offrant une amélioration limitée sur la segmentation des transcriptions de
210 © ATALA

TALN-RECITAL 2013, 17-21 Iuin, Les Sables d’Olonne

'R'E"'E Ll MSI

TFDF TFIDF

52 55

51

so 54

49 —|—UI

43 -0-?» 53 —I-U1

47 “‘3‘

45

45

44 51

43

42 so

57 E75 55 535 59 75 ?ES ?9 795 :30 B05 31 51.5 82 52.5 83
Pf9Cl5lDl\

Recall
Recall

E2

Precls ion

FIGURE 2 — Courbe rappel/précision pour les transcriptions obtenues grace aux systemes de
reconnaissance de la parole LIMSI et IRENE. UI représente les résultats obtenus grace a la
seule cohésion lexicale; A — value indique l’importance donnée a la rupture lexicale dans notre
approche

journaux TV nous avons également utilisé le corpus de Choi aﬁn de vériﬁer que notre modele
fonctionnait bien sur des données plus classiques. Par ailleurs, le jeu de données artiﬁciel de Choi
nous permet d’observer le comportement de notre approche lorsque les longueurs des segments
varient. Les résultats de notre méthode sur le corpus de Choi sont présentés sur la ﬁgure 3.

Les nombres mentionnés sur chaque ﬁgure (par exemple 3-5, 3-11) correspondent a l’intervalle de
valeurs pour z. Les résultats de différents échantillons du jeu de données sont fournis. On observe
que lorsque notre algorithme traite des textes écrits, il obtient de meilleures performances,
augmentant les valeurs de rappel et de précision. Plus les segments sont longs en moyenne,
plus importante est 1’amé1ioration apportée par la prise en compte de la rupture. Cependant les
paramétres utilisés doivent encore étre ajustés pour que l’importance donnée a la rupture, pour
tout type de données, soit ﬁxée et soit capable d’assigner la pénalité nécessaire aux poids calculés.
Nous avons observé qu’il ne semble pas y avoir de valeur précise a donner a l’importance de la
rupture; cependant les valeurs plus élevées conduisent a un rappel plus bas et une précision plus
élevée, conduisant a une sous-segmentation.

CHE, 35 CH-31 311

TFIDF OKAFW

9° 75

55
T0
30 —I—_U| —I—U|
—O—A B5 —o—7\

Recall
Recall

75
60
TD

55 55

45 50 55 60 I35 70 75 47 49 51 53 E5 E7 E9 '3 1 E3

PlE(lSlDH Preclsloll

211 © ATALA

TALN-RECITAL 2013, 17-21 Iuin, Les Sables d’Olonne

CHCI 6-El CHC1 9-ll
Ola-uni TFIDF

33 .713

31 74

'79 72

T7

75 —l—U| 7” _._|_1|
' -’-7‘ . —o—.'«
73 us

Recall
Rs-tall

71 as
53 :34
57 62

B5

_ EU
35 40 45 50 55 DD 65

35 40 45 50 55 60

Pretis mu .
Pl ems Ion

FIGURE 3 — Courbes rappel/précision obtenues sur le corpus de Choi

5 Conclusions

Nous avons proposé une méthode originale de segmentation thématique qui combine la cohésion
lexicale et la rupture lexicale, identiﬁant des zones de continuités et de ruptures dans l’organisa—
tion globale des données. Les résultats obtenus montrent que la combinaison des deux mesures
produit des segmentations de meilleure qualité que lors de l’emploi de la seule cohésion lexicale.
I1 reste toutefois encore des possibilités d’améliorer notre approche.

Nous proposons comme perspectives d’employer d’autres techniques de calcul de la rupture
lexicale. Parmi elles, la vectorisation (Claveau and Lefévre, 2011) implique une comparaison
indirecte entre des segments consécutifs, en proposant un changement dans l’espace de represen-
tation des segments et l’utilisau'on de documents pivots pour le calcul de la rupture. Les segments
ne partageant pas beaucoup de vocabulaire quoiqu’abordant le méme theme pourraient alors
etre considérés comme similaires. Cette méthode pourrait donc permettre de pallier le manque
de répétitions de mots qui apparait particuliérement dans le cas de transcriptions de journaux
TV Par ailleurs, une facon de régler ﬁnement les parametres a and A utilisés dans notre modele
statistiques doit étre déterminée.

Références

Allan, J ., editor (2002). Topic Detection and Tracking : event—based information organization.
Kluwer Academic Publishers.

Beeferman, D., Berger, A., and Lafferty, J. (1997). Text segmentation using exponential models.
In Proceedings of the 2nd Conference on Empirical Methods in Natural Language Processing, pages
35-46.

Blei, D. and Moreno, P. (2001). Topic segmentation with an aspect hidden Markov model. In
Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development
in Information Retrieval, pages 343-348.
Brown, G. and Yule, G. (1983). Discourse analysis. Cambridge University Press.

212 © ATALA

TALN—RECITAL 2013, 17-21 Iuin, Les Sables d’Olonne

Choi, E Y. Y. (2000). Advances in domain independent linear text segmentation. In Procee-
dings of the 1 st International Conference of the North American Chapter of the Association for
Computational Linguistics, pages 26-33.

Claveau, V (2012). Vectorisation, Okapi et calcul de similarité pour le TAL : pour oublier enﬁn
le TF-IDF. Actes de la confe’rence conjointe JEP—TALN—RECITAL 2012, pages 85-98.

Claveau, V and Lefevre, S. (2011). Topic segmentation of TV-streams by mathematical morpho-
logy and vectorization. In Proceedings of the 12th International Conference of the International
Speech Communication Association, Interspeech’1 1, pages 1105-1108.

Ferret, 0., Grau, B., and Masson, N. (1998). Thematic segmentation of texts : Two methods for
two kinds of texts. In Proceedings of the 36th Annual Meeting of the Association for Computational
Linguistics and 17th International Conference on Computational Linguistics, pages 392-396.

Galley, M., McKeown, K., Fosler—Lussier, E., and Jing, H. (2003). Discourse segmentation
of multi—party conversation. In Proceedings of the 41st Annual Meeting of the Association for
Computational Linguistics, ACL, pages 562-569.

Georgescul, M., Clark, A., and Armstrong, S. (2006). Word distributions for thematic seg-
mentation in a support vector machine approach. In Proceedings of the 10th Conference on
Computational Natural Language Learning, CoNLL—X, pages 101-108.

Grosz, B. J . and Sidner, C. L. (1986). Attention, intentions, and the structure of discourse.
Computational Linguistics, 12(3) :175—204.

Guinaudeau, C., Gravier, G., and Sébillot, R (2012). Enhancing lexical cohesion measure with
conﬁdence measures, semantic relations and language model interpolation for multimedia
spoken content topic segmentation. Computer Speech and Language, 26 (2) :90-104.

Guinaudeau, C. and Hirschberg, J . (2011). Accounting for prosodic information to improve
ASR—based topic tracking for TV broadcast news. In 12th Annual Conference of the International
Speech Communication Association, Interspeech’1 1, pages 1401-1404.

Hearst, M. A. (1997). TextTiling : Segmenting text into multi—paragraph subtopic passages.
Computational Linguistics, 23(1) :33—64.

Hernandez, N. and Grau, B. (2002). Analyse thématique du discours : segmentation, struc-
turation, description et représentation. In Actes du 5e colloque international sur le document
e’lectronique, pages 277-285.

Huet, S., Gravier, G., and Sébillot, P (2008). Un modéle multi—sources pour la segmentation en
sujets de journaux radiophoniques. In Actes de 15e conférence sur le traitement automatique des
langues naturelles, TALN’08, pages 49-58.

Litman, D. J. and Passonneau, R. J. (1995). Combining multiple knowledge sources for discourse
segmentation. In Proceedings of the 33rd Annual Meeting of the Association for Computational
Linguistics, pages 108-115.

Malioutov, I. and Barzilay, R. (2006). Minimum cut model for spoken lecture segmentation.
In Proceedings of the 21st International Conference on Computational Linguistics and the 44th
Annual Meeting of the Association for Computational Linguistics, pages 25-32.

Misra, H. and Yvon, F. (2010). Modeles thématiques pour la segmentation de documents. In
Actes des 10e journe'es intemationales d’anal_yse statistique des données textuelles, pages 203-213.

Moens, M.-E and Busser, R. D. (2001). Generic topic segmentation of document texts. In
Proceedings of the 24th International Conference on Research and Developement in Information
Retrieval, pages 418-419.

213 © ATALA

TALN-RECITAL 2013, 17-21 Iuin, Les Sables d’Olonne

Pevzner, L. and Hearst, M. A. (2002). A critique and improvement of an evaluation metric for
text segmentation. Computational Linguistics, 28 :19—36.

Purver, M. (2011). Topic segmentation. In 'Iur, G. and de Mori, R., editors, Spoken Language
Understanding : Systems for Extracting Semantic Information from Speech, chapter 11, pages
291-317. Wiley.

Reynar, J. C. (1994). An automatic method of ﬁnding topic boundaries. In Proceedings of the
32nd Annual Meeting on Association for Computational Linguistics, pages 331-333.

Riedl, M. and Biemann, C. (2012). How text segmentation algorithms gain from topic models. In
Proceedings of the Conference of the North American Chapter of the Association for Computational
Linguistics : Human Language Technologies, pages 553-557.

Utiyama, M. and Isahara, H. (2001). A statistical model for domain-independent text segmenta-
tion. In Proceedings of the 39th Annual Meeting on the Association for Computational Linguistics,
pages 499-506.

Yamron, J., Carp, I., Gillick, L., Lowe, S., and van Mulbregt P. (1998). A hidden Markov model
approach to text segmentation and event tracking. In Proceedings of the IEEE International
Conference on Acoustics, Speech, and Signal Processing, ICASSP, pages 333-336.

214 © ATALA

