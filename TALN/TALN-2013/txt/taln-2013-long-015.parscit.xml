<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>J Allan</author>
</authors>
<title>editor (2002). Topic Detection and Tracking : event-based information organization.</title>
<publisher>Kluwer Academic Publishers.</publisher>
<marker>Allan, </marker>
<rawString>Allan, J., editor (2002). Topic Detection and Tracking : event-based information organization. Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Beeferman</author>
<author>A Berger</author>
<author>J Lafferty</author>
</authors>
<title>Text segmentation using exponential models.</title>
<date>1997</date>
<booktitle>In Proceedings of the 2nd Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>35--46</pages>
<contexts>
<context position="26495" citStr="Beeferman et al., 1997" startWordPosition="4150" endWordPosition="4153">près alignement de la référence et des frontières proposées. Une tolérance de 10 secondes dans le positionnement est autorisée dans le cas des transcriptions de journaux TV, et de 2 phrases pour les données textuelles. Le rappel correspond à la part de frontières de référence détectées par la méthode et la précision au ratio des frontières produites appartenant à la segmentation de référence. La F1- mesure combine rappel et précision en une valeur unique. D’autres mesures ont été précédemment proposées pour évaluer la segmentation thématique de textes. Cependant, contrairement à la mesure Pk (Beeferman et al., 1997), le rappel et la précision ne sont pas sensibles aux variations de tailles des segments et ces mesures ne favorisent pas les segmentations avec peu de frontières comme la mesure WindowDiff (Pevzner and Hearst, 2002), ce qui justifie notre choix. Les tests effectués ont consisté à faire varier les paramètres α et λ de l’équation 7, α permettant différents compromis entre les valeurs de précision et de rappel, tandis que λ donne plus ou moins d’importance à la rupture. Parmi les diverses configurations testées dans les expériences, seules quelques-unes sont présentées ici. La figure 2 illustre </context>
</contexts>
<marker>Beeferman, Berger, Lafferty, 1997</marker>
<rawString>Beeferman, D., Berger, A., and Lafferty, J. (1997). Text segmentation using exponential models. In Proceedings of the 2nd Conference on Empirical Methods in Natural Language Processing, pages 35–46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Blei</author>
<author>P Moreno</author>
</authors>
<title>Topic segmentation with an aspect hidden Markov model.</title>
<date>2001</date>
<booktitle>In Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>343--348</pages>
<contexts>
<context position="3575" citStr="Blei and Moreno, 2001" startWordPosition="517" endWordPosition="520">Galley et al., 2003; Hearst, 1997; Reynar, 1994; Moens and Busser, 2001; Choi, 2000; Ferret et al., 1998; Utiyama and Isahara, 2001). Comme indiqué dans (Purver, 2011), elles peuvent être supervisées ou non, reposer sur des changements de vocabulaire, des techniques de clustering, sur la détection de frontières discriminantes ou sur des modèles probabilistes. Déterminer les segments thématiques à l’aide de modèles probabiliste consiste la plupart du temps à inférer la séquence de thèmes la plus probable à partir des mots observés et à dériver les positions des frontières (Yamron et al., 1998; Blei and Moreno, 2001). Ces modèles utilisent un corpus d’apprentissage pour estimer les distributions documents-thèmes et thèmes-mots. Des travaux récents ont montré l’intérêt de l’intégration de ces modèles probabilistes dans les algorithmes de segmentation de textes reposant sur la similarité de vocabulaire (Misra and Yvon, 2010; Riedl and Biemann, 2012). Nos travaux portent sur les méthodes non supervisées. La plupart d’entre elles repose sur la cohésion du vocabulaire pour identifier des segments cohérents dans les textes, exploitant les mots qu’ils contiennent et les relations sémantiques que ces mots entreti</context>
</contexts>
<marker>Blei, Moreno, 2001</marker>
<rawString>Blei, D. and Moreno, P. (2001). Topic segmentation with an aspect hidden Markov model. In Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 343–348.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Brown</author>
<author>G Yule</author>
</authors>
<title>Discourse analysis.</title>
<date>1983</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="9006" citStr="Brown and Yule, 1983" startWordPosition="1321" endWordPosition="1324">sahara puis le nouveau modèle statistique proposé. Dans la section 4, les expériences sont présentées, avec des détails sur les corpus utilisés et une analyse des résultats. 2 Techniques de segmentation thématique Dans cette section, nous présentons rapidement les notions-clés concernant le concept de segmentation thématique, ainsi que les techniques existantes et les traits qu’elles exploitent pour réaliser cette tâche. 2.1 Le concept de thème Le concept de thème est difficile à définir précisément et les linguistes qui ont tenté de le caractériser en offrent de nombreuses définitions. Dans (Brown and Yule, 1983), la difficulté de définir un thème est longuement discutée et les auteurs soulignent que : &amp;quot;The notion of ’topic’ is clearly an intuitively satisfactory way of describing the unifying principle which makes one stretch of discourse ’about’ something and the next stretch ’about’ something else, for it is appealed to very frequently in the discourse analysis literature. Yet the basis for the identification of ’topic’ is rarely made explicit.&amp;quot;. Souhaitant appliquer la segmentation thématique à des journaux TV, nous avons cherché à voir si la notion de thème avait été définie dans le contexte d’ém</context>
</contexts>
<marker>Brown, Yule, 1983</marker>
<rawString>Brown, G. and Yule, G. (1983). Discourse analysis. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>c ATALA TALN-RÉCITAL</author>
</authors>
<title>Advances in domain independent linear text segmentation.</title>
<date>2013</date>
<booktitle>In Proceedings of the 1st International Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>17--21</pages>
<contexts>
<context position="2357" citStr="TALN-RÉCITAL 2013" startWordPosition="342" endWordPosition="343">work of Isahara and Utiyama (2001), maintaining the properties of domain independence and limited a priori of the latter. Evaluations are performed both on written texts and on automatic transcripts of TV shows, the latter not respecting the norms of written texts, thus increasing the difficulty of the task. Experimental results demonstrate the relevance of combining lexical cohesion and disrupture. MOTS-CLÉS : segmentation thématique, cohésion lexicale, rupture de cohésion, journaux télévisés. KEYWORDS: topic segmentation, lexical cohesion, lexical disrupture, TV broadcast news. 202 c ATALA TALN-RÉCITAL 2013, 17-21 Juin, Les Sables d’Olonne 1 Introduction La segmentation thématique consiste à mettre en évidence la structure sémantique d’un document et les algorithmes développés pour cette tâche visent à détecter automatiquement les frontières qui définissent des segments thématiquement cohérents. Cible de nombreux travaux, la segmentation thématique a également des retombées en recherche d’information, résumé automatique, systèmes de question-réponse... Diverses méthodes de segmentation de données textuelles ont été proposées dans la littérature (Yamron et al., 1998; Georgescul et al., 2006; Gall</context>
<context position="6553" citStr="TALN-RÉCITAL 2013" startWordPosition="955" endWordPosition="956">me des textes standards mais en groupes de soufle correspondant aux mots prononcés par une personne entre deux inspirations. De plus, elles peuvent contenir de nombreux mots mal transcrits. Difficulté supplémentaire, les journaux TV peuvent avoir des segments thématiques très courts, contenant peu de mots et donc peu de répétitions, en particulier quand le présentateur fait volontairement usage de synonymes. Cela rend l’utilisation du critère de cohésion lexicale particulièrement ardue. Notre algorithme de segmentation thématique ayant un fort potentiel pour traiter ces cas, nous 203 c ATALA TALN-RÉCITAL 2013, 17-21 Juin, Les Sables d’Olonne avons souhaité le tester sur ces données difficiles. La technique présentée ici repose sur l’algorithme de segmentation de textes proposé par Utiyama et Isahara (2001), algorithme dont les capactiés ont été attestées pour le texte écrit. C’est un modèle probabiliste qui fournit une segmentation non supervisée. Dans cette approche, il n’y a donc pas de tentative d’apprentissage de l’ensemble des modèles thématiques le plus probable à partir des données d’apprentissage, mais au contraire l’ensemble est généré par l’algorithme étant donnés les textes à segmenter.</context>
<context position="9954" citStr="TALN-RÉCITAL 2013" startWordPosition="1474" endWordPosition="1475">y in the discourse analysis literature. Yet the basis for the identification of ’topic’ is rarely made explicit.&amp;quot;. Souhaitant appliquer la segmentation thématique à des journaux TV, nous avons cherché à voir si la notion de thème avait été définie dans le contexte d’émissions télévisées. Le projet Topic Detection and Tracking (Allan, 2002) s’est par exemple focalisé sur le repérage de segments de journaux TV thématiquement liés. Dans ce cadre, les notions d’événement et de thème ont été définies : un événement est quelque chose qui se produit à un instant et un endroit spécifique 204 c ATALA TALN-RÉCITAL 2013, 17-21 Juin, Les Sables d’Olonne et qui est associé à des actions particulières ; un thème est, quant à lui, l’ensemble formé d’un événement et de tous les événements qui lui sont directement liés. Un événement est donc relativement court et évolue dans le temps, tandis qu’un thème est plus stable et plus long. Dans notre cadre de segmentation de journaux TV, un thème correspond à un reportage qui forme une unité sémantique cohérente dans la structure d’un journal. Notre algorithme est également évalué sur des textes écrits, formés par concaténation de parties extraites d’articles sélectionné</context>
<context position="13869" citStr="TALN-RÉCITAL 2013" startWordPosition="2065" endWordPosition="2066">thode locale présente certains désavantages dont une sensibilité aux variations de tailles des segments dans les textes puisqu’un voisinage de taille fixe est considéré, ainsi qu’une difficulté de choix de la valeur de seuil pour décider qu’une rupture est suffisamment forte pour placer une frontière. Une méthode globale réalise quant à elle une comparaison globale entre toutes les régions du document, en cherchant à maximiser globalement la valeur de la cohésion lexicale. Dans Utiyama et Isahara (2001), la valeur de la cohésion lexicale d’un segment Si est vue comme la mesure de 205 c ATALA TALN-RÉCITAL 2013, 17-21 Juin, Les Sables d’Olonne la capacité d’un modèle de langue ∆i , appris sur le segment Si , à prédire les mots du segment. Le modèle de langue ∆i doit donc d’abord être estimé, puis la probabilité généralisée des mots du segment Si , étant donné∆i , doit être déterminée. Après le calcul de la valeur de cohésion lexicale pour chaque segment, la segmentation maximisant globalement cette valeur est choisie. Cet algorithme s’est avéré performant au regard d’autres algorithmes de segmentation thématique de textes tels que ceux de Choi (2000) ou Reynar (1994). Cependant, la limite principale</context>
<context position="16896" citStr="TALN-RÉCITAL 2013" startWordPosition="2554" endWordPosition="2555"> lexical tout en respectant une distribution a priori de la longueur des segments. L’idée principale est de trouver la segmentation la plus probable pour une séquence de t unités élémentaires (i.e., phrases ou énoncés composés de mots) W = ut1 parmi toutes les segmentations possibles, i.e., Ŝ = argmax P[W |S]P[S] . (1) S En admettant que chaque segment est une unité indépendante du reste du texte et que les mots contenus dans un segment sont eux aussi indépendants, la probabilité du texte W pour une segmentation S = Sm1 est donnée par m ni P[W |Sm1 ] = P[wij |Si] , (2) i=1 j=1 206 c ATALA TALN-RÉCITAL 2013, 17-21 Juin, Les Sables d’Olonne où ni est le nombre de mots du segment Si , wij est le j e mot de Si et m le nombre de segments. La probabilité P[wij |Si] est donnée par une loi de Laplace dont les paramètres sont estimés sur Si , i.e., fi(wi P[wij |Si] = j ) + 1 , (3) ni + k où fi(wij) est le nombre d’occurrences de w i j dans Si et k est le nombre total de mots différents dans le texte W (i.e., la taille du vocabulaire). Cette probabilité va favoriser les segments homogènes car elle croît quand les mots sont répétés et décroît quand ils sont différents. La distribution a priori des longueu</context>
<context position="19685" citStr="TALN-RÉCITAL 2013" startWordPosition="3064" endWordPosition="3065">1] P[W |Si ,Si−1] . (5) i=2 Pour déterminer la segmentation de probabilité maximum Ŝ, le coût associé au segment Si , étant donné Si−1, est ln(P[W |Si ,Si−1]) = ln(P[Wi |Si])− 1λ( ) , (6)∆(Wi ,Wi−1) où ∆(Wi ,Wi−1) est la valeur de rupture entre le contenu de Si et celui de Si−1, et λ est un paramètre qui permet de contrôler l’influence de la rupture dans le coût. Wi représente les unités élémentaires du segment Si . Choisir 1/∆(Wi ,Wi−1) conduit à une pénalité faible quand il y a une forte rupture. Dans l’équation 6, P[W |Si ,Si−1] ne représente plus une probabilité ; cependant, 207 c ATALA TALN-RÉCITAL 2013, 17-21 Juin, Les Sables d’Olonne puisque l’algorithme de segmentation consiste à déterminer le meilleur chemin dans un graphe pondéré, cela n’a pas d’impact car aucune présupposition de graphe probabiliste n’est faite pour segmenter. Par conséquent, la nouvelle définition de la segmentation la plus probable est m m 1 Ŝ = argmax ln(P[W )− λ ( )−αmln(n) . (7) S i |Si] i=1 i=2 ∆(Wi ,Wi−1) De l’équation 6, on peut déduire que, pour un nœud donné représentant une frontière thématique, tous les segments de longueurs différentes arrivant à ce nœud sont conservés. Au niveau implémentation, nous dé</context>
<context position="23079" citStr="TALN-RÉCITAL 2013" startWordPosition="3632" endWordPosition="3633">n2,2, la cohésion lexicale de l’arc e22,31 et la rupture lexicale entre le segment contenant à la fois u1 et u2 et le segment contenant seulement u3. Si dans l’exemple donné (cf. FIGURE 1) le score le plus élevé est obtenu pour le chemin formé de e01,11e11,32e32,41, la segmentation de probabilité maximum est [u1][u2u3][u4]. Utiliser cette représentation nous permet donc de considérer tous les chemins possibles de longueurs variables, traitant ainsi toutes les combinaisons possibles de segments consécutifs pour le calcul de la cohésion lexicale et également de la rupture lexicale. 208 c ATALA TALN-RÉCITAL 2013, 17-21 Juin, Les Sables d’Olonne FIGURE 1 – Un exemple de treillis de segmentation 4 Expériences Nous présentons ici les expériences réalisées en fournissant tout d’abord des détails sur les transcriptions de journaux TV et les données textuelles utilisées, puis en analysant les résultats obtenus. 4.1 Corpus Deux corpora sont considérés dans notre tâche de segmentation thématique. Le premier est un corpus de journaux TV contenant 56 journaux (∼1/2 heure chacun), enregistrés de février à mars 2007 sur la chaîne de TV française France 2. Les journaux consistent en une succession de reportages d</context>
<context position="25000" citStr="TALN-RÉCITAL 2013" startWordPosition="3929" endWordPosition="3930">par Choi (2000) et utilisé par différents auteurs pour comparer leurs méthodes à des approches existantes. Il consiste en 700 documents créés par concaténation de 10 parties de textes correspondant chacune aux z premières phrases d’articles choisis aléatoirement dans le corpus Brown, z étant lui-même choisi aléatoirement dans un intervalle fixé. Une limite de ce jeu de données est qu’il comporte donc des changements thématiques très brutaux, ce qui est rarement le cas dans des documents classiques. Cependant, il est intéressant car il contient des segments de longueurs variables. 209 c ATALA TALN-RÉCITAL 2013, 17-21 Juin, Les Sables d’Olonne Transcriptions Manuelles IRENE automatiques LIMSI automatiques Gain de F1-mesure 0.77 0.2 0.5 TABLE 1 – Gain en F1-mesure pour les transcriptions manuelles et automatiques de journaux TV 4.2 Résultats Nous présentons dans cette sous-section l’impact de notre modèle statistique sur la tâche de segmentation thématique de journaux TV et de données textuelles. Les résultats sont comparés à ceux d’un système basique et bien que les améliorations obtenues soient limitées, elles montrent nettement l’intérêt de combiner rupture et cohésion lexicales. Pour les journaux</context>
<context position="28689" citStr="TALN-RÉCITAL 2013" startWordPosition="4491" endWordPosition="4492">is que 6 journaux TV, la F1-mesure retenue correspondant aux segmentations fournissant le nombre de frontières le plus proche de celui de la référence. Le gain est inférieur là encore pour les transcriptions IRENE dont le taux d’erreur est plus élevé. Avoir à sa disposition moins de mots potentiellement répétés accroît la difficulté de discriminer entre des segments appartenant à des thèmes différents. Cependant notre modèle parvient à améliorer la segmentation même pour ces données bruitées. Notre méthode offrant une amélioration limitée sur la segmentation des transcriptions de 210 c ATALA TALN-RÉCITAL 2013, 17-21 Juin, Les Sables d’Olonne FIGURE 2 – Courbe rappel/précision pour les transcriptions obtenues grâce aux systèmes de reconnaissance de la parole LIMSI et IRENE. UI représente les résultats obtenus grâce à la seule cohésion lexicale ; λ− value indique l’importance donnée à la rupture lexicale dans notre approche journaux TV, nous avons également utilisé le corpus de Choi afin de vérifier que notre modèle fonctionnait bien sur des données plus classiques. Par ailleurs, le jeu de données artificiel de Choi nous permet d’observer le comportement de notre approche lorsque les longueurs des s</context>
<context position="30336" citStr="TALN-RÉCITAL 2013" startWordPosition="4750" endWordPosition="4751">e précision. Plus les segments sont longs en moyenne, plus importante est l’amélioration apportée par la prise en compte de la rupture. Cependant les paramètres utilisés doivent encore être ajustés pour que l’importance donnée à la rupture, pour tout type de données, soit fixée et soit capable d’assigner la pénalité nécessaire aux poids calculés. Nous avons observé qu’il ne semble pas y avoir de valeur précise à donner à l’importance de la rupture ; cependant les valeurs plus élevées conduisent à un rappel plus bas et une précision plus élevée, conduisant à une sous-segmentation. 211 c ATALA TALN-RÉCITAL 2013, 17-21 Juin, Les Sables d’Olonne FIGURE 3 – Courbes rappel/précision obtenues sur le corpus de Choi 5 Conclusions Nous avons proposé une méthode originale de segmentation thématique qui combine la cohésion lexicale et la rupture lexicale, identifiant des zones de continuités et de ruptures dans l’organisation globale des données. Les résultats obtenus montrent que la combinaison des deux mesures produit des segmentations de meilleure qualité que lors de l’emploi de la seule cohésion lexicale. Il reste toutefois encore des possibilités d’améliorer notre approche. Nous proposons comme perspecti</context>
</contexts>
<marker>TALN-RÉCITAL, 2013</marker>
<rawString>212 c ATALA TALN-RÉCITAL 2013, 17-21 Juin, Les Sables d’Olonne Choi, F. Y. Y. (2000). Advances in domain independent linear text segmentation. In Proceedings of the 1st International Conference of the North American Chapter of the Association for Computational Linguistics, pages 26–33.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Claveau</author>
</authors>
<title>Vectorisation, Okapi et calcul de similarité pour le TAL : pour oublier enfin le TF-IDF. Actes de la conférence conjointe JEP-TALN-RECITAL</title>
<date>2012</date>
<pages>85--98</pages>
<marker>Claveau, 2012</marker>
<rawString>Claveau, V. (2012). Vectorisation, Okapi et calcul de similarité pour le TAL : pour oublier enfin le TF-IDF. Actes de la conférence conjointe JEP-TALN-RECITAL 2012, pages 85–98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Claveau</author>
<author>S Lefèvre</author>
</authors>
<title>Topic segmentation of TV-streams by mathematical morphology and vectorization.</title>
<date>2011</date>
<booktitle>In Proceedings of the 12th International Conference of the International Speech Communication Association, Interspeech’11,</booktitle>
<pages>1105--1108</pages>
<contexts>
<context position="11732" citStr="Claveau and Lefèvre, 2011" startWordPosition="1736" endWordPosition="1739"> du discours (Grosz and Sidner, 1986; Litman and Passonneau, 1995). Les techniques génériques, qui sont celles qui nous intéressent ici, exploitent traditionnellement la seule cohésion lexicale, indépendante du type de documents considérés et ne nécessitant pas de phase d’apprentissage. L’idée-clé des méthodes fondées sur la cohésion lexicale est de considérer qu’un changement significatif dans le vocabulaire utilisé est un signe de changement thématique. Ces approches peuvent être divisées en deux familles : • les méthodes locales (Hearst, 1997; Hernandez and Grau, 2002; Ferret et al., 1998; Claveau and Lefèvre, 2011) qui cherchent à repérer localement les ruptures lexicales ; • les méthodes globales (Reynar, 1994; Choi, 2000; Utiyama and Isahara, 2001; Malioutov and Barzilay, 2006; Misra and Yvon, 2010) exploitant une mesure de la cohésion lexicale. Une méthode locale repose sur la comparaison locale de régions du document et associe un changement thématique aux endroits où il y a une similarité faible entre deux régions consécutives (i.e., elles identifient les zones de fortes ruptures lexicales). Par exemple, TextTiling (Hearst, 1997), qui est considéré comme un algorithme de segmentation thématique fon</context>
<context position="31062" citStr="Claveau and Lefèvre, 2011" startWordPosition="4853" endWordPosition="4856"> 5 Conclusions Nous avons proposé une méthode originale de segmentation thématique qui combine la cohésion lexicale et la rupture lexicale, identifiant des zones de continuités et de ruptures dans l’organisation globale des données. Les résultats obtenus montrent que la combinaison des deux mesures produit des segmentations de meilleure qualité que lors de l’emploi de la seule cohésion lexicale. Il reste toutefois encore des possibilités d’améliorer notre approche. Nous proposons comme perspectives d’employer d’autres techniques de calcul de la rupture lexicale. Parmi elles, la vectorisation (Claveau and Lefèvre, 2011) implique une comparaison indirecte entre des segments consécutifs, en proposant un changement dans l’espace de représentation des segments et l’utilisation de documents pivots pour le calcul de la rupture. Les segments ne partageant pas beaucoup de vocabulaire quoiqu’abordant le même thème pourraient alors être considérés comme similaires. Cette méthode pourrait donc permettre de pallier le manque de répétitions de mots qui apparaît particulièrement dans le cas de transcriptions de journaux TV. Par ailleurs, une façon de régler finement les paramètres α and λ utilisés dans notre modèle statis</context>
</contexts>
<marker>Claveau, Lefèvre, 2011</marker>
<rawString>Claveau, V. and Lefèvre, S. (2011). Topic segmentation of TV-streams by mathematical morphology and vectorization. In Proceedings of the 12th International Conference of the International Speech Communication Association, Interspeech’11, pages 1105–1108.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Ferret</author>
<author>B Grau</author>
<author>N Masson</author>
</authors>
<title>Thematic segmentation of texts : Two methods for two kinds of texts.</title>
<date>1998</date>
<booktitle>In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics,</booktitle>
<pages>392--396</pages>
<contexts>
<context position="3057" citStr="Ferret et al., 1998" startWordPosition="438" endWordPosition="441">ste à mettre en évidence la structure sémantique d’un document et les algorithmes développés pour cette tâche visent à détecter automatiquement les frontières qui définissent des segments thématiquement cohérents. Cible de nombreux travaux, la segmentation thématique a également des retombées en recherche d’information, résumé automatique, systèmes de question-réponse... Diverses méthodes de segmentation de données textuelles ont été proposées dans la littérature (Yamron et al., 1998; Georgescul et al., 2006; Galley et al., 2003; Hearst, 1997; Reynar, 1994; Moens and Busser, 2001; Choi, 2000; Ferret et al., 1998; Utiyama and Isahara, 2001). Comme indiqué dans (Purver, 2011), elles peuvent être supervisées ou non, reposer sur des changements de vocabulaire, des techniques de clustering, sur la détection de frontières discriminantes ou sur des modèles probabilistes. Déterminer les segments thématiques à l’aide de modèles probabiliste consiste la plupart du temps à inférer la séquence de thèmes la plus probable à partir des mots observés et à dériver les positions des frontières (Yamron et al., 1998; Blei and Moreno, 2001). Ces modèles utilisent un corpus d’apprentissage pour estimer les distributions d</context>
<context position="11704" citStr="Ferret et al., 1998" startWordPosition="1732" endWordPosition="1735">011) ou des marqueurs du discours (Grosz and Sidner, 1986; Litman and Passonneau, 1995). Les techniques génériques, qui sont celles qui nous intéressent ici, exploitent traditionnellement la seule cohésion lexicale, indépendante du type de documents considérés et ne nécessitant pas de phase d’apprentissage. L’idée-clé des méthodes fondées sur la cohésion lexicale est de considérer qu’un changement significatif dans le vocabulaire utilisé est un signe de changement thématique. Ces approches peuvent être divisées en deux familles : • les méthodes locales (Hearst, 1997; Hernandez and Grau, 2002; Ferret et al., 1998; Claveau and Lefèvre, 2011) qui cherchent à repérer localement les ruptures lexicales ; • les méthodes globales (Reynar, 1994; Choi, 2000; Utiyama and Isahara, 2001; Malioutov and Barzilay, 2006; Misra and Yvon, 2010) exploitant une mesure de la cohésion lexicale. Une méthode locale repose sur la comparaison locale de régions du document et associe un changement thématique aux endroits où il y a une similarité faible entre deux régions consécutives (i.e., elles identifient les zones de fortes ruptures lexicales). Par exemple, TextTiling (Hearst, 1997), qui est considéré comme un algorithme de</context>
</contexts>
<marker>Ferret, Grau, Masson, 1998</marker>
<rawString>Ferret, O., Grau, B., and Masson, N. (1998). Thematic segmentation of texts : Two methods for two kinds of texts. In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, pages 392–396.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Galley</author>
<author>K McKeown</author>
<author>E Fosler-Lussier</author>
<author>H Jing</author>
</authors>
<title>Discourse segmentation of multi-party conversation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, ACL,</booktitle>
<pages>562--569</pages>
<contexts>
<context position="2972" citStr="Galley et al., 2003" startWordPosition="424" endWordPosition="427">2013, 17-21 Juin, Les Sables d’Olonne 1 Introduction La segmentation thématique consiste à mettre en évidence la structure sémantique d’un document et les algorithmes développés pour cette tâche visent à détecter automatiquement les frontières qui définissent des segments thématiquement cohérents. Cible de nombreux travaux, la segmentation thématique a également des retombées en recherche d’information, résumé automatique, systèmes de question-réponse... Diverses méthodes de segmentation de données textuelles ont été proposées dans la littérature (Yamron et al., 1998; Georgescul et al., 2006; Galley et al., 2003; Hearst, 1997; Reynar, 1994; Moens and Busser, 2001; Choi, 2000; Ferret et al., 1998; Utiyama and Isahara, 2001). Comme indiqué dans (Purver, 2011), elles peuvent être supervisées ou non, reposer sur des changements de vocabulaire, des techniques de clustering, sur la détection de frontières discriminantes ou sur des modèles probabilistes. Déterminer les segments thématiques à l’aide de modèles probabiliste consiste la plupart du temps à inférer la séquence de thèmes la plus probable à partir des mots observés et à dériver les positions des frontières (Yamron et al., 1998; Blei and Moreno, 20</context>
</contexts>
<marker>Galley, McKeown, Fosler-Lussier, Jing, 2003</marker>
<rawString>Galley, M., McKeown, K., Fosler-Lussier, E., and Jing, H. (2003). Discourse segmentation of multi-party conversation. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, ACL, pages 562–569.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Georgescul</author>
<author>A Clark</author>
<author>S Armstrong</author>
</authors>
<title>Word distributions for thematic segmentation in a support vector machine approach.</title>
<date>2006</date>
<booktitle>In Proceedings of the 10th Conference on Computational Natural Language Learning, CoNLL-X,</booktitle>
<pages>101--108</pages>
<contexts>
<context position="2951" citStr="Georgescul et al., 2006" startWordPosition="420" endWordPosition="423">02 c ATALA TALN-RÉCITAL 2013, 17-21 Juin, Les Sables d’Olonne 1 Introduction La segmentation thématique consiste à mettre en évidence la structure sémantique d’un document et les algorithmes développés pour cette tâche visent à détecter automatiquement les frontières qui définissent des segments thématiquement cohérents. Cible de nombreux travaux, la segmentation thématique a également des retombées en recherche d’information, résumé automatique, systèmes de question-réponse... Diverses méthodes de segmentation de données textuelles ont été proposées dans la littérature (Yamron et al., 1998; Georgescul et al., 2006; Galley et al., 2003; Hearst, 1997; Reynar, 1994; Moens and Busser, 2001; Choi, 2000; Ferret et al., 1998; Utiyama and Isahara, 2001). Comme indiqué dans (Purver, 2011), elles peuvent être supervisées ou non, reposer sur des changements de vocabulaire, des techniques de clustering, sur la détection de frontières discriminantes ou sur des modèles probabilistes. Déterminer les segments thématiques à l’aide de modèles probabiliste consiste la plupart du temps à inférer la séquence de thèmes la plus probable à partir des mots observés et à dériver les positions des frontières (Yamron et al., 1998</context>
</contexts>
<marker>Georgescul, Clark, Armstrong, 2006</marker>
<rawString>Georgescul, M., Clark, A., and Armstrong, S. (2006). Word distributions for thematic segmentation in a support vector machine approach. In Proceedings of the 10th Conference on Computational Natural Language Learning, CoNLL-X, pages 101–108.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B J Grosz</author>
<author>C L Sidner</author>
</authors>
<title>Attention, intentions, and the structure of discourse.</title>
<date>1986</date>
<journal>Computational Linguistics,</journal>
<volume>12</volume>
<issue>3</issue>
<pages>175--204</pages>
<contexts>
<context position="11142" citStr="Grosz and Sidner, 1986" startWordPosition="1652" endWordPosition="1655">s extraites d’articles sélectionnés aléatoirement dans le corpus Brown (Choi, 2000) ; un thème est alors associé à chaque partie formant le texte final. 2.2 Méthodes pour la segmentation thématique Pour réaliser la segmentation thématique de textes, diverses caractéristiques peuvent être exploitées afin d’identifier les changements thématiques. Elles peuvent reposer sur la cohésion lexicale (i.e., prendre en compte les informations de distribution du vocabulaire) ou sur des marqueurs linguistiques tels que des indices prosodiques (Guinaudeau and Hirschberg, 2011) ou des marqueurs du discours (Grosz and Sidner, 1986; Litman and Passonneau, 1995). Les techniques génériques, qui sont celles qui nous intéressent ici, exploitent traditionnellement la seule cohésion lexicale, indépendante du type de documents considérés et ne nécessitant pas de phase d’apprentissage. L’idée-clé des méthodes fondées sur la cohésion lexicale est de considérer qu’un changement significatif dans le vocabulaire utilisé est un signe de changement thématique. Ces approches peuvent être divisées en deux familles : • les méthodes locales (Hearst, 1997; Hernandez and Grau, 2002; Ferret et al., 1998; Claveau and Lefèvre, 2011) qui cherc</context>
</contexts>
<marker>Grosz, Sidner, 1986</marker>
<rawString>Grosz, B. J. and Sidner, C. L. (1986). Attention, intentions, and the structure of discourse. Computational Linguistics, 12(3) :175–204.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Guinaudeau</author>
<author>G Gravier</author>
<author>P Sébillot</author>
</authors>
<title>Enhancing lexical cohesion measure with confidence measures, semantic relations and language model interpolation for multimedia spoken content topic segmentation.</title>
<date>2012</date>
<journal>Computer Speech and Language,</journal>
<volume>26</volume>
<issue>2</issue>
<pages>90--104</pages>
<contexts>
<context position="15334" citStr="Guinaudeau et al., 2012" startWordPosition="2298" endWordPosition="2301">e globale des dissimilarités locales, a été présentée dans (Malioutov and Barzilay, 2006), mais, d’une part, le nombre de segments à trouver est fixé a priori et, d’autre part, la couverture est limitée car la dissimilarité entre segments est calculée en utilisant une fenêtre. Le point de départ de notre méthode est le modèle statistique proposé dans (Utiyama and Isahara, 2001), qui est flexible et offre des possibilités d’extension par intégration de nouvelles informations. Plusieurs travaux l’ont déjà utilisé avec succès dans le contexte de la segmentation de journaux TV (Huet et al., 2008; Guinaudeau et al., 2012), le modifiant pour intégrer des connaissances spécifiques aux émissions TV. Contrairement à ces travaux, nous avons redéfini le modèle de (Utiyama and Isahara, 2001) afin qu’il puisse prendre en compte non seulement la cohésion mais aussi la rupture lexicale et, par conséquent, améliorer la segmentation de tout type de données textuelles. Considérer la rupture est en particulier intéressant pour traiter les cas de textes contenant des changements brutaux de vocabulaire. La façon dont nous combinons les deux critères est détaillée dans la section 3. 3 Combinaison de la cohésion et de la ruptur</context>
</contexts>
<marker>Guinaudeau, Gravier, Sébillot, 2012</marker>
<rawString>Guinaudeau, C., Gravier, G., and Sébillot, P. (2012). Enhancing lexical cohesion measure with confidence measures, semantic relations and language model interpolation for multimedia spoken content topic segmentation. Computer Speech and Language, 26(2) :90–104.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Guinaudeau</author>
<author>J Hirschberg</author>
</authors>
<title>Accounting for prosodic information to improve ASR-based topic tracking for TV broadcast news.</title>
<date>2011</date>
<booktitle>In 12th Annual Conference of the International Speech Communication Association, Interspeech’11,</booktitle>
<pages>1401--1404</pages>
<contexts>
<context position="11089" citStr="Guinaudeau and Hirschberg, 2011" startWordPosition="1643" endWordPosition="1646">valué sur des textes écrits, formés par concaténation de parties extraites d’articles sélectionnés aléatoirement dans le corpus Brown (Choi, 2000) ; un thème est alors associé à chaque partie formant le texte final. 2.2 Méthodes pour la segmentation thématique Pour réaliser la segmentation thématique de textes, diverses caractéristiques peuvent être exploitées afin d’identifier les changements thématiques. Elles peuvent reposer sur la cohésion lexicale (i.e., prendre en compte les informations de distribution du vocabulaire) ou sur des marqueurs linguistiques tels que des indices prosodiques (Guinaudeau and Hirschberg, 2011) ou des marqueurs du discours (Grosz and Sidner, 1986; Litman and Passonneau, 1995). Les techniques génériques, qui sont celles qui nous intéressent ici, exploitent traditionnellement la seule cohésion lexicale, indépendante du type de documents considérés et ne nécessitant pas de phase d’apprentissage. L’idée-clé des méthodes fondées sur la cohésion lexicale est de considérer qu’un changement significatif dans le vocabulaire utilisé est un signe de changement thématique. Ces approches peuvent être divisées en deux familles : • les méthodes locales (Hearst, 1997; Hernandez and Grau, 2002; Ferr</context>
</contexts>
<marker>Guinaudeau, Hirschberg, 2011</marker>
<rawString>Guinaudeau, C. and Hirschberg, J. (2011). Accounting for prosodic information to improve ASR-based topic tracking for TV broadcast news. In 12th Annual Conference of the International Speech Communication Association, Interspeech’11, pages 1401–1404.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A Hearst</author>
</authors>
<title>TextTiling : Segmenting text into multi-paragraph subtopic passages.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>1</issue>
<pages>33--64</pages>
<contexts>
<context position="2986" citStr="Hearst, 1997" startWordPosition="428" endWordPosition="429"> Sables d’Olonne 1 Introduction La segmentation thématique consiste à mettre en évidence la structure sémantique d’un document et les algorithmes développés pour cette tâche visent à détecter automatiquement les frontières qui définissent des segments thématiquement cohérents. Cible de nombreux travaux, la segmentation thématique a également des retombées en recherche d’information, résumé automatique, systèmes de question-réponse... Diverses méthodes de segmentation de données textuelles ont été proposées dans la littérature (Yamron et al., 1998; Georgescul et al., 2006; Galley et al., 2003; Hearst, 1997; Reynar, 1994; Moens and Busser, 2001; Choi, 2000; Ferret et al., 1998; Utiyama and Isahara, 2001). Comme indiqué dans (Purver, 2011), elles peuvent être supervisées ou non, reposer sur des changements de vocabulaire, des techniques de clustering, sur la détection de frontières discriminantes ou sur des modèles probabilistes. Déterminer les segments thématiques à l’aide de modèles probabiliste consiste la plupart du temps à inférer la séquence de thèmes la plus probable à partir des mots observés et à dériver les positions des frontières (Yamron et al., 1998; Blei and Moreno, 2001). Ces modèl</context>
<context position="4733" citStr="Hearst, 1997" startWordPosition="686" endWordPosition="687">nnent et les relations sémantiques que ces mots entretiennent. Pour mesurer la cohérence dans les (segments de) textes, la cohésion lexicale, fondée sur la répétition de mots ou sur l’exploitation de chaînes lexicales, est fréquemment retenue en privilégiant l’une ou l’autre des deux stratégies suivantes : soit on cherche à maximiser la mesure de cohésion lexicale des segments, en regroupant les portions de texte lexicalement cohérentes, soit on cherche à identifier des ruptures entre les segments en plaçant des frontières quand survient un changement significatif dans le vocabulaire utilisé (Hearst, 1997). Dans cet article, notre objectif est de proposer une nouvelle solution pour la segmentation thématique de documents qui consiste à mêler ces deux approches, c’est-à-dire à combiner les mesures de cohésion lexicale et de rupture lexicale afin d’obtenir une segmentation en fragments à la fois thématiquement cohérents et différents les uns des autres. La technique que nous proposons peut s’appliquer à tout type de données textuelles et est indé- pendante d’un domaine particulier. Notre objectif est cependant de l’appliquer à la segmentation de journaux télévisés afin de permettre à des utilisat</context>
<context position="11657" citStr="Hearst, 1997" startWordPosition="1726" endWordPosition="1727">rosodiques (Guinaudeau and Hirschberg, 2011) ou des marqueurs du discours (Grosz and Sidner, 1986; Litman and Passonneau, 1995). Les techniques génériques, qui sont celles qui nous intéressent ici, exploitent traditionnellement la seule cohésion lexicale, indépendante du type de documents considérés et ne nécessitant pas de phase d’apprentissage. L’idée-clé des méthodes fondées sur la cohésion lexicale est de considérer qu’un changement significatif dans le vocabulaire utilisé est un signe de changement thématique. Ces approches peuvent être divisées en deux familles : • les méthodes locales (Hearst, 1997; Hernandez and Grau, 2002; Ferret et al., 1998; Claveau and Lefèvre, 2011) qui cherchent à repérer localement les ruptures lexicales ; • les méthodes globales (Reynar, 1994; Choi, 2000; Utiyama and Isahara, 2001; Malioutov and Barzilay, 2006; Misra and Yvon, 2010) exploitant une mesure de la cohésion lexicale. Une méthode locale repose sur la comparaison locale de régions du document et associe un changement thématique aux endroits où il y a une similarité faible entre deux régions consécutives (i.e., elles identifient les zones de fortes ruptures lexicales). Par exemple, TextTiling (Hearst, </context>
</contexts>
<marker>Hearst, 1997</marker>
<rawString>Hearst, M. A. (1997). TextTiling : Segmenting text into multi-paragraph subtopic passages. Computational Linguistics, 23(1) :33–64.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Hernandez</author>
<author>B Grau</author>
</authors>
<title>Analyse thématique du discours : segmentation, structuration, description et représentation. In Actes du 5e colloque international sur le document électronique,</title>
<date>2002</date>
<pages>277--285</pages>
<contexts>
<context position="11683" citStr="Hernandez and Grau, 2002" startWordPosition="1728" endWordPosition="1731">inaudeau and Hirschberg, 2011) ou des marqueurs du discours (Grosz and Sidner, 1986; Litman and Passonneau, 1995). Les techniques génériques, qui sont celles qui nous intéressent ici, exploitent traditionnellement la seule cohésion lexicale, indépendante du type de documents considérés et ne nécessitant pas de phase d’apprentissage. L’idée-clé des méthodes fondées sur la cohésion lexicale est de considérer qu’un changement significatif dans le vocabulaire utilisé est un signe de changement thématique. Ces approches peuvent être divisées en deux familles : • les méthodes locales (Hearst, 1997; Hernandez and Grau, 2002; Ferret et al., 1998; Claveau and Lefèvre, 2011) qui cherchent à repérer localement les ruptures lexicales ; • les méthodes globales (Reynar, 1994; Choi, 2000; Utiyama and Isahara, 2001; Malioutov and Barzilay, 2006; Misra and Yvon, 2010) exploitant une mesure de la cohésion lexicale. Une méthode locale repose sur la comparaison locale de régions du document et associe un changement thématique aux endroits où il y a une similarité faible entre deux régions consécutives (i.e., elles identifient les zones de fortes ruptures lexicales). Par exemple, TextTiling (Hearst, 1997), qui est considéré c</context>
</contexts>
<marker>Hernandez, Grau, 2002</marker>
<rawString>Hernandez, N. and Grau, B. (2002). Analyse thématique du discours : segmentation, structuration, description et représentation. In Actes du 5e colloque international sur le document électronique, pages 277–285.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Huet</author>
<author>G Gravier</author>
<author>P Sébillot</author>
</authors>
<title>Un modèle multi-sources pour la segmentation en sujets de journaux radiophoniques. In Actes de 15e conférence sur le traitement automatique des langues naturelles, TALN’08,</title>
<date>2008</date>
<pages>49--58</pages>
<contexts>
<context position="15308" citStr="Huet et al., 2008" startWordPosition="2294" endWordPosition="2297">apturer dans une vue globale des dissimilarités locales, a été présentée dans (Malioutov and Barzilay, 2006), mais, d’une part, le nombre de segments à trouver est fixé a priori et, d’autre part, la couverture est limitée car la dissimilarité entre segments est calculée en utilisant une fenêtre. Le point de départ de notre méthode est le modèle statistique proposé dans (Utiyama and Isahara, 2001), qui est flexible et offre des possibilités d’extension par intégration de nouvelles informations. Plusieurs travaux l’ont déjà utilisé avec succès dans le contexte de la segmentation de journaux TV (Huet et al., 2008; Guinaudeau et al., 2012), le modifiant pour intégrer des connaissances spécifiques aux émissions TV. Contrairement à ces travaux, nous avons redéfini le modèle de (Utiyama and Isahara, 2001) afin qu’il puisse prendre en compte non seulement la cohésion mais aussi la rupture lexicale et, par conséquent, améliorer la segmentation de tout type de données textuelles. Considérer la rupture est en particulier intéressant pour traiter les cas de textes contenant des changements brutaux de vocabulaire. La façon dont nous combinons les deux critères est détaillée dans la section 3. 3 Combinaison de l</context>
</contexts>
<marker>Huet, Gravier, Sébillot, 2008</marker>
<rawString>Huet, S., Gravier, G., and Sébillot, P. (2008). Un modèle multi-sources pour la segmentation en sujets de journaux radiophoniques. In Actes de 15e conférence sur le traitement automatique des langues naturelles, TALN’08, pages 49–58.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D J Litman</author>
<author>R J Passonneau</author>
</authors>
<title>Combining multiple knowledge sources for discourse segmentation.</title>
<date>1995</date>
<booktitle>In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>108--115</pages>
<contexts>
<context position="11172" citStr="Litman and Passonneau, 1995" startWordPosition="1656" endWordPosition="1659">électionnés aléatoirement dans le corpus Brown (Choi, 2000) ; un thème est alors associé à chaque partie formant le texte final. 2.2 Méthodes pour la segmentation thématique Pour réaliser la segmentation thématique de textes, diverses caractéristiques peuvent être exploitées afin d’identifier les changements thématiques. Elles peuvent reposer sur la cohésion lexicale (i.e., prendre en compte les informations de distribution du vocabulaire) ou sur des marqueurs linguistiques tels que des indices prosodiques (Guinaudeau and Hirschberg, 2011) ou des marqueurs du discours (Grosz and Sidner, 1986; Litman and Passonneau, 1995). Les techniques génériques, qui sont celles qui nous intéressent ici, exploitent traditionnellement la seule cohésion lexicale, indépendante du type de documents considérés et ne nécessitant pas de phase d’apprentissage. L’idée-clé des méthodes fondées sur la cohésion lexicale est de considérer qu’un changement significatif dans le vocabulaire utilisé est un signe de changement thématique. Ces approches peuvent être divisées en deux familles : • les méthodes locales (Hearst, 1997; Hernandez and Grau, 2002; Ferret et al., 1998; Claveau and Lefèvre, 2011) qui cherchent à repérer localement les </context>
</contexts>
<marker>Litman, Passonneau, 1995</marker>
<rawString>Litman, D. J. and Passonneau, R. J. (1995). Combining multiple knowledge sources for discourse segmentation. In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics, pages 108–115.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Malioutov</author>
<author>R Barzilay</author>
</authors>
<title>Minimum cut model for spoken lecture segmentation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>25--32</pages>
<contexts>
<context position="11899" citStr="Malioutov and Barzilay, 2006" startWordPosition="1761" endWordPosition="1764">t la seule cohésion lexicale, indépendante du type de documents considérés et ne nécessitant pas de phase d’apprentissage. L’idée-clé des méthodes fondées sur la cohésion lexicale est de considérer qu’un changement significatif dans le vocabulaire utilisé est un signe de changement thématique. Ces approches peuvent être divisées en deux familles : • les méthodes locales (Hearst, 1997; Hernandez and Grau, 2002; Ferret et al., 1998; Claveau and Lefèvre, 2011) qui cherchent à repérer localement les ruptures lexicales ; • les méthodes globales (Reynar, 1994; Choi, 2000; Utiyama and Isahara, 2001; Malioutov and Barzilay, 2006; Misra and Yvon, 2010) exploitant une mesure de la cohésion lexicale. Une méthode locale repose sur la comparaison locale de régions du document et associe un changement thématique aux endroits où il y a une similarité faible entre deux régions consécutives (i.e., elles identifient les zones de fortes ruptures lexicales). Par exemple, TextTiling (Hearst, 1997), qui est considéré comme un algorithme de segmentation thématique fondamental, analyse le texte à l’aide d’une fenêtre glissante qui couvre des blocs adjacents de texte et est centrée en un point du texte correspondant à une frontière t</context>
<context position="14799" citStr="Malioutov and Barzilay, 2006" startWordPosition="2213" endWordPosition="2216"> valeur de cohésion lexicale pour chaque segment, la segmentation maximisant globalement cette valeur est choisie. Cet algorithme s’est avéré performant au regard d’autres algorithmes de segmentation thématique de textes tels que ceux de Choi (2000) ou Reynar (1994). Cependant, la limite principale de ce type de méthode globale est un risque de sur-segmentation. L’originalité de la solution que nous proposons consiste dans la combinaison des deux types de méthodes. Une méthode fondée sur le même principe, visant à capturer dans une vue globale des dissimilarités locales, a été présentée dans (Malioutov and Barzilay, 2006), mais, d’une part, le nombre de segments à trouver est fixé a priori et, d’autre part, la couverture est limitée car la dissimilarité entre segments est calculée en utilisant une fenêtre. Le point de départ de notre méthode est le modèle statistique proposé dans (Utiyama and Isahara, 2001), qui est flexible et offre des possibilités d’extension par intégration de nouvelles informations. Plusieurs travaux l’ont déjà utilisé avec succès dans le contexte de la segmentation de journaux TV (Huet et al., 2008; Guinaudeau et al., 2012), le modifiant pour intégrer des connaissances spécifiques aux ém</context>
</contexts>
<marker>Malioutov, Barzilay, 2006</marker>
<rawString>Malioutov, I. and Barzilay, R. (2006). Minimum cut model for spoken lecture segmentation. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association for Computational Linguistics, pages 25–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Misra</author>
<author>F Yvon</author>
</authors>
<title>Modèles thématiques pour la segmentation de documents. In Actes des 10e journées internationales d’analyse statistique des données textuelles,</title>
<date>2010</date>
<pages>203--213</pages>
<contexts>
<context position="3886" citStr="Misra and Yvon, 2010" startWordPosition="559" endWordPosition="562">discriminantes ou sur des modèles probabilistes. Déterminer les segments thématiques à l’aide de modèles probabiliste consiste la plupart du temps à inférer la séquence de thèmes la plus probable à partir des mots observés et à dériver les positions des frontières (Yamron et al., 1998; Blei and Moreno, 2001). Ces modèles utilisent un corpus d’apprentissage pour estimer les distributions documents-thèmes et thèmes-mots. Des travaux récents ont montré l’intérêt de l’intégration de ces modèles probabilistes dans les algorithmes de segmentation de textes reposant sur la similarité de vocabulaire (Misra and Yvon, 2010; Riedl and Biemann, 2012). Nos travaux portent sur les méthodes non supervisées. La plupart d’entre elles repose sur la cohésion du vocabulaire pour identifier des segments cohérents dans les textes, exploitant les mots qu’ils contiennent et les relations sémantiques que ces mots entretiennent. Pour mesurer la cohérence dans les (segments de) textes, la cohésion lexicale, fondée sur la répétition de mots ou sur l’exploitation de chaînes lexicales, est fréquemment retenue en privilégiant l’une ou l’autre des deux stratégies suivantes : soit on cherche à maximiser la mesure de cohésion lexicale</context>
<context position="11922" citStr="Misra and Yvon, 2010" startWordPosition="1765" endWordPosition="1768">indépendante du type de documents considérés et ne nécessitant pas de phase d’apprentissage. L’idée-clé des méthodes fondées sur la cohésion lexicale est de considérer qu’un changement significatif dans le vocabulaire utilisé est un signe de changement thématique. Ces approches peuvent être divisées en deux familles : • les méthodes locales (Hearst, 1997; Hernandez and Grau, 2002; Ferret et al., 1998; Claveau and Lefèvre, 2011) qui cherchent à repérer localement les ruptures lexicales ; • les méthodes globales (Reynar, 1994; Choi, 2000; Utiyama and Isahara, 2001; Malioutov and Barzilay, 2006; Misra and Yvon, 2010) exploitant une mesure de la cohésion lexicale. Une méthode locale repose sur la comparaison locale de régions du document et associe un changement thématique aux endroits où il y a une similarité faible entre deux régions consécutives (i.e., elles identifient les zones de fortes ruptures lexicales). Par exemple, TextTiling (Hearst, 1997), qui est considéré comme un algorithme de segmentation thématique fondamental, analyse le texte à l’aide d’une fenêtre glissante qui couvre des blocs adjacents de texte et est centrée en un point du texte correspondant à une frontière thématique potentielle. </context>
</contexts>
<marker>Misra, Yvon, 2010</marker>
<rawString>Misra, H. and Yvon, F. (2010). Modèles thématiques pour la segmentation de documents. In Actes des 10e journées internationales d’analyse statistique des données textuelles, pages 203–213.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M-F Moens</author>
<author>R D Busser</author>
</authors>
<title>Generic topic segmentation of document texts.</title>
<date>2001</date>
<booktitle>In Proceedings of the 24th International Conference on Research and Developement in Information Retrieval,</booktitle>
<pages>418--419</pages>
<contexts>
<context position="3024" citStr="Moens and Busser, 2001" startWordPosition="432" endWordPosition="435">ion La segmentation thématique consiste à mettre en évidence la structure sémantique d’un document et les algorithmes développés pour cette tâche visent à détecter automatiquement les frontières qui définissent des segments thématiquement cohérents. Cible de nombreux travaux, la segmentation thématique a également des retombées en recherche d’information, résumé automatique, systèmes de question-réponse... Diverses méthodes de segmentation de données textuelles ont été proposées dans la littérature (Yamron et al., 1998; Georgescul et al., 2006; Galley et al., 2003; Hearst, 1997; Reynar, 1994; Moens and Busser, 2001; Choi, 2000; Ferret et al., 1998; Utiyama and Isahara, 2001). Comme indiqué dans (Purver, 2011), elles peuvent être supervisées ou non, reposer sur des changements de vocabulaire, des techniques de clustering, sur la détection de frontières discriminantes ou sur des modèles probabilistes. Déterminer les segments thématiques à l’aide de modèles probabiliste consiste la plupart du temps à inférer la séquence de thèmes la plus probable à partir des mots observés et à dériver les positions des frontières (Yamron et al., 1998; Blei and Moreno, 2001). Ces modèles utilisent un corpus d’apprentissage</context>
</contexts>
<marker>Moens, Busser, 2001</marker>
<rawString>Moens, M.-F. and Busser, R. D. (2001). Generic topic segmentation of document texts. In Proceedings of the 24th International Conference on Research and Developement in Information Retrieval, pages 418–419.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Pevzner</author>
<author>M A Hearst</author>
</authors>
<title>213 c ATALA TALN-RÉCITAL 2013, 17-21 Juin, Les Sables d’Olonne</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<contexts>
<context position="26711" citStr="Pevzner and Hearst, 2002" startWordPosition="4185" endWordPosition="4188">les. Le rappel correspond à la part de frontières de référence détectées par la méthode et la précision au ratio des frontières produites appartenant à la segmentation de référence. La F1- mesure combine rappel et précision en une valeur unique. D’autres mesures ont été précédemment proposées pour évaluer la segmentation thématique de textes. Cependant, contrairement à la mesure Pk (Beeferman et al., 1997), le rappel et la précision ne sont pas sensibles aux variations de tailles des segments et ces mesures ne favorisent pas les segmentations avec peu de frontières comme la mesure WindowDiff (Pevzner and Hearst, 2002), ce qui justifie notre choix. Les tests effectués ont consisté à faire varier les paramètres α et λ de l’équation 7, α permettant différents compromis entre les valeurs de précision et de rappel, tandis que λ donne plus ou moins d’importance à la rupture. Parmi les diverses configurations testées dans les expériences, seules quelques-unes sont présentées ici. La figure 2 illustre tout d’abord les résultats obtenus pour la segmentation des journaux TV transcrits par les deux systèmes de RAP, en les comparant au système de référence correspondant à l’algorithme d’Utiyama et Isahara (2001) stand</context>
</contexts>
<marker>Pevzner, Hearst, 2002</marker>
<rawString>213 c ATALA TALN-RÉCITAL 2013, 17-21 Juin, Les Sables d’Olonne Pevzner, L. and Hearst, M. A. (2002). A critique and improvement of an evaluation metric for text segmentation. Computational Linguistics, 28 :19–36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Purver</author>
</authors>
<title>Topic segmentation.</title>
<date>2011</date>
<booktitle>Spoken Language Understanding : Systems for Extracting Semantic Information from Speech, chapter 11,</booktitle>
<pages>291--317</pages>
<editor>In Tur, G. and de Mori, R., editors,</editor>
<publisher>Wiley.</publisher>
<contexts>
<context position="3120" citStr="Purver, 2011" startWordPosition="449" endWordPosition="450"> algorithmes développés pour cette tâche visent à détecter automatiquement les frontières qui définissent des segments thématiquement cohérents. Cible de nombreux travaux, la segmentation thématique a également des retombées en recherche d’information, résumé automatique, systèmes de question-réponse... Diverses méthodes de segmentation de données textuelles ont été proposées dans la littérature (Yamron et al., 1998; Georgescul et al., 2006; Galley et al., 2003; Hearst, 1997; Reynar, 1994; Moens and Busser, 2001; Choi, 2000; Ferret et al., 1998; Utiyama and Isahara, 2001). Comme indiqué dans (Purver, 2011), elles peuvent être supervisées ou non, reposer sur des changements de vocabulaire, des techniques de clustering, sur la détection de frontières discriminantes ou sur des modèles probabilistes. Déterminer les segments thématiques à l’aide de modèles probabiliste consiste la plupart du temps à inférer la séquence de thèmes la plus probable à partir des mots observés et à dériver les positions des frontières (Yamron et al., 1998; Blei and Moreno, 2001). Ces modèles utilisent un corpus d’apprentissage pour estimer les distributions documents-thèmes et thèmes-mots. Des travaux récents ont montré </context>
</contexts>
<marker>Purver, 2011</marker>
<rawString>Purver, M. (2011). Topic segmentation. In Tur, G. and de Mori, R., editors, Spoken Language Understanding : Systems for Extracting Semantic Information from Speech, chapter 11, pages 291–317. Wiley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J C Reynar</author>
</authors>
<title>An automatic method of finding topic boundaries.</title>
<date>1994</date>
<booktitle>In Proceedings of the 32nd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>331--333</pages>
<contexts>
<context position="3000" citStr="Reynar, 1994" startWordPosition="430" endWordPosition="431">ne 1 Introduction La segmentation thématique consiste à mettre en évidence la structure sémantique d’un document et les algorithmes développés pour cette tâche visent à détecter automatiquement les frontières qui définissent des segments thématiquement cohérents. Cible de nombreux travaux, la segmentation thématique a également des retombées en recherche d’information, résumé automatique, systèmes de question-réponse... Diverses méthodes de segmentation de données textuelles ont été proposées dans la littérature (Yamron et al., 1998; Georgescul et al., 2006; Galley et al., 2003; Hearst, 1997; Reynar, 1994; Moens and Busser, 2001; Choi, 2000; Ferret et al., 1998; Utiyama and Isahara, 2001). Comme indiqué dans (Purver, 2011), elles peuvent être supervisées ou non, reposer sur des changements de vocabulaire, des techniques de clustering, sur la détection de frontières discriminantes ou sur des modèles probabilistes. Déterminer les segments thématiques à l’aide de modèles probabiliste consiste la plupart du temps à inférer la séquence de thèmes la plus probable à partir des mots observés et à dériver les positions des frontières (Yamron et al., 1998; Blei and Moreno, 2001). Ces modèles utilisent u</context>
<context position="11830" citStr="Reynar, 1994" startWordPosition="1753" endWordPosition="1754">ui nous intéressent ici, exploitent traditionnellement la seule cohésion lexicale, indépendante du type de documents considérés et ne nécessitant pas de phase d’apprentissage. L’idée-clé des méthodes fondées sur la cohésion lexicale est de considérer qu’un changement significatif dans le vocabulaire utilisé est un signe de changement thématique. Ces approches peuvent être divisées en deux familles : • les méthodes locales (Hearst, 1997; Hernandez and Grau, 2002; Ferret et al., 1998; Claveau and Lefèvre, 2011) qui cherchent à repérer localement les ruptures lexicales ; • les méthodes globales (Reynar, 1994; Choi, 2000; Utiyama and Isahara, 2001; Malioutov and Barzilay, 2006; Misra and Yvon, 2010) exploitant une mesure de la cohésion lexicale. Une méthode locale repose sur la comparaison locale de régions du document et associe un changement thématique aux endroits où il y a une similarité faible entre deux régions consécutives (i.e., elles identifient les zones de fortes ruptures lexicales). Par exemple, TextTiling (Hearst, 1997), qui est considéré comme un algorithme de segmentation thématique fondamental, analyse le texte à l’aide d’une fenêtre glissante qui couvre des blocs adjacents de text</context>
<context position="14436" citStr="Reynar (1994)" startWordPosition="2159" endWordPosition="2160">me la mesure de 205 c ATALA TALN-RÉCITAL 2013, 17-21 Juin, Les Sables d’Olonne la capacité d’un modèle de langue ∆i , appris sur le segment Si , à prédire les mots du segment. Le modèle de langue ∆i doit donc d’abord être estimé, puis la probabilité généralisée des mots du segment Si , étant donné∆i , doit être déterminée. Après le calcul de la valeur de cohésion lexicale pour chaque segment, la segmentation maximisant globalement cette valeur est choisie. Cet algorithme s’est avéré performant au regard d’autres algorithmes de segmentation thématique de textes tels que ceux de Choi (2000) ou Reynar (1994). Cependant, la limite principale de ce type de méthode globale est un risque de sur-segmentation. L’originalité de la solution que nous proposons consiste dans la combinaison des deux types de méthodes. Une méthode fondée sur le même principe, visant à capturer dans une vue globale des dissimilarités locales, a été présentée dans (Malioutov and Barzilay, 2006), mais, d’une part, le nombre de segments à trouver est fixé a priori et, d’autre part, la couverture est limitée car la dissimilarité entre segments est calculée en utilisant une fenêtre. Le point de départ de notre méthode est le modèl</context>
</contexts>
<marker>Reynar, 1994</marker>
<rawString>Reynar, J. C. (1994). An automatic method of finding topic boundaries. In Proceedings of the 32nd Annual Meeting on Association for Computational Linguistics, pages 331–333.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Riedl</author>
<author>C Biemann</author>
</authors>
<title>How text segmentation algorithms gain from topic models.</title>
<date>2012</date>
<booktitle>In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies,</booktitle>
<pages>553--557</pages>
<contexts>
<context position="3912" citStr="Riedl and Biemann, 2012" startWordPosition="563" endWordPosition="566">des modèles probabilistes. Déterminer les segments thématiques à l’aide de modèles probabiliste consiste la plupart du temps à inférer la séquence de thèmes la plus probable à partir des mots observés et à dériver les positions des frontières (Yamron et al., 1998; Blei and Moreno, 2001). Ces modèles utilisent un corpus d’apprentissage pour estimer les distributions documents-thèmes et thèmes-mots. Des travaux récents ont montré l’intérêt de l’intégration de ces modèles probabilistes dans les algorithmes de segmentation de textes reposant sur la similarité de vocabulaire (Misra and Yvon, 2010; Riedl and Biemann, 2012). Nos travaux portent sur les méthodes non supervisées. La plupart d’entre elles repose sur la cohésion du vocabulaire pour identifier des segments cohérents dans les textes, exploitant les mots qu’ils contiennent et les relations sémantiques que ces mots entretiennent. Pour mesurer la cohérence dans les (segments de) textes, la cohésion lexicale, fondée sur la répétition de mots ou sur l’exploitation de chaînes lexicales, est fréquemment retenue en privilégiant l’une ou l’autre des deux stratégies suivantes : soit on cherche à maximiser la mesure de cohésion lexicale des segments, en regroupa</context>
</contexts>
<marker>Riedl, Biemann, 2012</marker>
<rawString>Riedl, M. and Biemann, C. (2012). How text segmentation algorithms gain from topic models. In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies, pages 553–557.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Utiyama</author>
<author>H Isahara</author>
</authors>
<title>A statistical model for domain-independent text segmentation.</title>
<date>2001</date>
<booktitle>In Proceedings of the 39th Annual Meeting on the Association for Computational Linguistics,</booktitle>
<pages>499--506</pages>
<contexts>
<context position="3085" citStr="Utiyama and Isahara, 2001" startWordPosition="442" endWordPosition="445">nce la structure sémantique d’un document et les algorithmes développés pour cette tâche visent à détecter automatiquement les frontières qui définissent des segments thématiquement cohérents. Cible de nombreux travaux, la segmentation thématique a également des retombées en recherche d’information, résumé automatique, systèmes de question-réponse... Diverses méthodes de segmentation de données textuelles ont été proposées dans la littérature (Yamron et al., 1998; Georgescul et al., 2006; Galley et al., 2003; Hearst, 1997; Reynar, 1994; Moens and Busser, 2001; Choi, 2000; Ferret et al., 1998; Utiyama and Isahara, 2001). Comme indiqué dans (Purver, 2011), elles peuvent être supervisées ou non, reposer sur des changements de vocabulaire, des techniques de clustering, sur la détection de frontières discriminantes ou sur des modèles probabilistes. Déterminer les segments thématiques à l’aide de modèles probabiliste consiste la plupart du temps à inférer la séquence de thèmes la plus probable à partir des mots observés et à dériver les positions des frontières (Yamron et al., 1998; Blei and Moreno, 2001). Ces modèles utilisent un corpus d’apprentissage pour estimer les distributions documents-thèmes et thèmes-mo</context>
<context position="11869" citStr="Utiyama and Isahara, 2001" startWordPosition="1757" endWordPosition="1760">xploitent traditionnellement la seule cohésion lexicale, indépendante du type de documents considérés et ne nécessitant pas de phase d’apprentissage. L’idée-clé des méthodes fondées sur la cohésion lexicale est de considérer qu’un changement significatif dans le vocabulaire utilisé est un signe de changement thématique. Ces approches peuvent être divisées en deux familles : • les méthodes locales (Hearst, 1997; Hernandez and Grau, 2002; Ferret et al., 1998; Claveau and Lefèvre, 2011) qui cherchent à repérer localement les ruptures lexicales ; • les méthodes globales (Reynar, 1994; Choi, 2000; Utiyama and Isahara, 2001; Malioutov and Barzilay, 2006; Misra and Yvon, 2010) exploitant une mesure de la cohésion lexicale. Une méthode locale repose sur la comparaison locale de régions du document et associe un changement thématique aux endroits où il y a une similarité faible entre deux régions consécutives (i.e., elles identifient les zones de fortes ruptures lexicales). Par exemple, TextTiling (Hearst, 1997), qui est considéré comme un algorithme de segmentation thématique fondamental, analyse le texte à l’aide d’une fenêtre glissante qui couvre des blocs adjacents de texte et est centrée en un point du texte c</context>
<context position="15090" citStr="Utiyama and Isahara, 2001" startWordPosition="2261" endWordPosition="2264">ipale de ce type de méthode globale est un risque de sur-segmentation. L’originalité de la solution que nous proposons consiste dans la combinaison des deux types de méthodes. Une méthode fondée sur le même principe, visant à capturer dans une vue globale des dissimilarités locales, a été présentée dans (Malioutov and Barzilay, 2006), mais, d’une part, le nombre de segments à trouver est fixé a priori et, d’autre part, la couverture est limitée car la dissimilarité entre segments est calculée en utilisant une fenêtre. Le point de départ de notre méthode est le modèle statistique proposé dans (Utiyama and Isahara, 2001), qui est flexible et offre des possibilités d’extension par intégration de nouvelles informations. Plusieurs travaux l’ont déjà utilisé avec succès dans le contexte de la segmentation de journaux TV (Huet et al., 2008; Guinaudeau et al., 2012), le modifiant pour intégrer des connaissances spécifiques aux émissions TV. Contrairement à ces travaux, nous avons redéfini le modèle de (Utiyama and Isahara, 2001) afin qu’il puisse prendre en compte non seulement la cohésion mais aussi la rupture lexicale et, par conséquent, améliorer la segmentation de tout type de données textuelles. Considérer la </context>
</contexts>
<marker>Utiyama, Isahara, 2001</marker>
<rawString>Utiyama, M. and Isahara, H. (2001). A statistical model for domain-independent text segmentation. In Proceedings of the 39th Annual Meeting on the Association for Computational Linguistics, pages 499–506.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Yamron</author>
<author>I Carp</author>
<author>L Gillick</author>
<author>S Lowe</author>
<author>P van Mulbregt</author>
</authors>
<title>A hidden Markov model approach to text segmentation and event tracking.</title>
<date>1998</date>
<booktitle>In Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing, ICASSP,</booktitle>
<pages>333--336</pages>
<note>c ATALA</note>
<marker>Yamron, Carp, Gillick, Lowe, van Mulbregt, 1998</marker>
<rawString>Yamron, J., Carp, I., Gillick, L., Lowe, S., and van Mulbregt P. (1998). A hidden Markov model approach to text segmentation and event tracking. In Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing, ICASSP, pages 333–336. 214 c ATALA</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>