<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
"http://www.w3.org/TR/html4/loose.dtd">
<html><head><title>Chunks et activation : un mod&#232;le de facilitation du traitement linguistique</title>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
</head>
<body>
<div style="page-break-before:always; page-break-after:always"><div><p>Chunks et activation : un mod&#232;le de facilitation du traitement
linguistique
</p>
<p>Philippe Blache
Aix-Marseille Universit&#233;, CNRS, LPL
</p>
<p>5 Avenue Pasteur, 13100 Aix-en-Provence
blache@lpl-aix.fr
</p>
<p>R&#201;SUM&#201;
Nous proposons dans cet article d&#8217;int&#233;grer la notion de chunk au sein d&#8217;une architecture globale
de traitement de la phrase. Les chunks jouent un r&#244;le important dans les th&#233;ories cognitives
comme ACT-R (Anderson et al., 2004) : il s&#8217;agit d&#8217;unit&#233;s de traitement globales auxquelles
il est possible d&#8217;acc&#233;der directement via des buffers en m&#233;moire &#224; court ou long terme. Ces
chunks sont construits par une fonction d&#8217;activation (processus cognitif pouvant &#234;tre quantifi&#233;)
s&#8217;appuyant sur l&#8217;&#233;valuation de leur relation au contexte. Nous proposons une interpr&#233;tation de
cette th&#233;orie appliqu&#233;e &#224; l&#8217;analyse syntaxique. Un m&#233;canisme de construction des chunks est
propos&#233;. Nous d&#233;veloppons pour cela une fonction d&#8217;activation tirant parti de la repr&#233;sentation
de l&#8217;information linguistique sous forme de contraintes. Cette fonction permet de montrer en
quoi les chunks sont faciles &#224; construire et comment leur existence facilite le traitement de la
phrase. Plusieurs exemples sont propos&#233;s, illustrant cette hypoth&#232;se de facilitation.
</p>
<p>ABSTRACT
Chunks and the notion of activation : a facilitation model for sentence processing
</p>
<p>We propose in this paper to integrate the notion of chunk within a global architecture for
sentence processing. Chunks play an important role in cognitive theories such as ACT-R cite
Anderson04 : they constitute global processing units which can be accessed directly via short or
long term memory buffers. Chunks are built on the basis of an activation function evaluating
their relationship to the context. We propose an interpretation of this theory applied to parsing.
A construction mechanism is proposed, based on an adapted version of the activation function
which takes advantage of the representation of linguistic information in terms of constraints.
This feature allows to show how chunks are easy to build and how they can facilitate treatment.
Several examples are given, illustrating this hypothesis of facilitation.
</p>
<p>MOTS-CL&#201;S : Chunks, ACT-R, activation, m&#233;moire, parsing, traitement de la phrase, exp&#233;rimen-
tation.
</p>
<p>KEYWORDS: Chunks, ACT-R, activation, memory, parsing, sentence processing, experimentation.
</p>
<p>TALN-R&#201;CITAL 2013, 17-21 Juin, Les Sables d&#8217;Olonne
</p>
<p>229 c&#65535; ATALA</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>1 Introduction
</p>
<p>L&#8217;interpr&#233;tation d&#8217;un &#233;nonc&#233;, &#224; commencer par son traitement syntaxique, peut &#234;tre plus ou
moins facile pour un sujet humain. Plusieurs travaux proposent des &#233;l&#233;ments d&#8217;explication de
cette variabilit&#233;. Au niveau syntaxique, des travaux proposent par exemple des explications en
termes de distance pour une relation &#224; &#233;tablir entre deux &#233;l&#233;ments, une grande distance &#233;tant
plus complexe &#224; traiter qu&#8217;une plus faible (Gibson, 1998) ; (Grodner et Gibson, 2005). D&#8217;autres
travaux portent sur l&#8217;identification d&#8217;un niveau d&#8217;activation des items en s&#8217;appuyant notamment
sur des relations avec le reste de la structure en cours de construction (Lewis et Vasishth, 2005).
Dans tous les cas, ces mod&#232;les de difficult&#233; abordent la question d&#8217;un point de vue global, en
tentant d&#8217;identifier les param&#232;tres pouvant complexifier le traitement. Nous proposons dans cet
article d&#8217;aborder un point de vue compl&#233;mentaire en tentant d&#8217;identifier des facteurs qui au
contraire peuvent permettre de faciliter le traitement.
</p>
<p>En se situant dans l&#8217;hypoth&#232;se d&#8217;un traitement incr&#233;mental du langage, dans laquelle les mots
sont int&#233;gr&#233;s au fur et &#224; mesure de leur d&#233;codage dans une structure en cours de construction, des
travaux ant&#233;rieurs ont montr&#233; la possibilit&#233; de mesurer la quantit&#233; d&#8217;information linguistique 1
</p>
<p>disponible au moment de l&#8217;int&#233;gration d&#8217;un mot. Dans les cas o&#249; le niveau d&#8217;information est
&#233;lev&#233;, le traitement (la compr&#233;hension) s&#8217;en trouve facilit&#233;. En revanche, un d&#233;ficit d&#8217;information
entra&#238;ne une complexification du traitement. En termes computationnels, la quantit&#233; d&#8217;informa-
tion disponible permet de contr&#244;ler l&#8217;espace de recherche requis pour l&#8217;interpr&#233;tation d&#8217;un &#233;nonc&#233;.
Une construction associ&#233;e &#224; une faible quantit&#233; d&#8217;information est tr&#232;s ambigu&#235; et donc difficile &#224;
traiter car le nombre d&#8217;interpr&#233;tations possibles (donc l&#8217;espace de recherche) est tr&#232;s grand. En
revanche, une construction pour laquelle une grande quantit&#233; d&#8217;information (&#233;ventuellement
redondante) est disponible sera peu ou pas ambigu&#235;, son espace de recherche plus restreint et son
traitement (son interpr&#233;tation) devient plus facile. Dans certains cas, il n&#8217;y a aucune ambigu&#239;t&#233;, le
traitement est alors purement d&#233;terministe. La quantit&#233; d&#8217;information est dans ce cas un facteur
de simplification du traitement et non pas de complexification.
</p>
<p>D&#8217;une fa&#231;on g&#233;n&#233;rale, la quantit&#233; (ou densit&#233;) d&#8217;information disponible est variable selon les
parties de l&#8217;&#233;nonc&#233; ou de la phrase. L&#8217;hypoth&#232;se que nous formulons est que les zones comportant
une densit&#233; d&#8217;information importante sont trait&#233;es plus facilement que les autres. Dans certains
cas, ces zones de haute densit&#233; peuvent &#234;tre trait&#233;es d&#8217;un bloc. Nous nous int&#233;ressons dans cet
article &#224; cette id&#233;e que le processus d&#8217;int&#233;gration syntaxique pourrait se faire au niveau de ces
zones plut&#244;t qu&#8217;au niveau des mots. Une pr&#233;sence plus importante de zones de haute densit&#233;
d&#8217;information dans un &#233;nonc&#233; ou une phrase faciliterait ainsi son traitement. Cette id&#233;e s&#8217;appuie
sur le principe Maximize On-Line Processing (not&#233; MoP) propos&#233; dans (Hawkins, 2003) :
</p>
<p>The human parser prefers to maximize the set of properties that are assignable to each item X as X is
parsed. [...] The maximization difference between competing orders and structures will be a function
of the number of properties that are misassigned or unassigned to X in a structure S, compared with
the number in an alternative.
</p>
<p>Ce principe comporte plusieurs &#233;l&#233;ments. Il int&#232;gre tout d&#8217;abord l&#8217;id&#233;e selon laquelle, dans
un processus incr&#233;mental, l&#8217;int&#233;gration d&#8217;un mot repose sur la v&#233;rification d&#8217;un ensemble de
propri&#233;t&#233;s. Il indique &#233;galement que deux constructions peuvent se distinguer par le nombre
de propri&#233;t&#233;s qu&#8217;elles v&#233;rifient. La notion de densit&#233; d&#8217;information recoupe donc ce principe de
</p>
<p>1. On entend ici par information linguistique toute propri&#233;t&#233; morpho-syntaxique ou syntaxique caract&#233;risant la
structure en cours de construction.
</p>
<p>TALN-R&#201;CITAL 2013, 17-21 Juin, Les Sables d&#8217;Olonne
</p>
<p>230 c&#65535; ATALA</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>maximisation : un mot sera plus ou moins facilement int&#233;gr&#233; &#224; la structure selon que le nombre
de propri&#233;t&#233;s qui lui sont associ&#233;es est important ou pas.
</p>
<p>Notre hypoth&#232;se est que ces unit&#233;s, d&#233;finies par maximisation, correspondent en termes de
traitement &#224; des chunks tels que d&#233;crits dans les th&#233;ories cognitives de type ACT-R (Adaptive
Character of Thought&#8211;Rational (Anderson et al., 2004)) et peuvent &#224; ce titre &#234;tre stock&#233;s en
m&#233;moire &#224; court terme et b&#233;n&#233;ficier d&#8217;un acc&#232;s direct.
</p>
<p>2 Chunks et activation
</p>
<p>La notion de chunk est bien connue en TAL, et g&#233;n&#233;ralement d&#233;finie comme une suite de
cat&#233;gories non r&#233;cursive, form&#233;e d&#8217;une t&#234;te, &#224; laquelle peuvent &#234;tre adjoints mots fonctionnels et
modifieurs adjacents (Abney, 1991) ; (Bird et al., 2009). Nous nous int&#233;ressons dans cet article &#224;
la fa&#231;on dont ces chunks peuvent &#234;tre construits, dans le cadre d&#8217;un processus incr&#233;mental, par
un parseur humain.
</p>
<p>2.1 Les chunks dans les th&#233;ories cognitives
</p>
<p>Le traitement du langage, comme celui des activit&#233;s cognitives de haut niveau, repose sur la
capacit&#233; d&#8217;identifier des unit&#233;s de traitement pouvant &#234;tre de taille et de nature variable. Cette
id&#233;e est plus particuli&#232;rement d&#233;velopp&#233;e par la th&#233;orie ACT-R et son adaptation au langage
(Lewis et Vasishth, 2005), (Reitter et al., 2011) dans laquelle les m&#233;canismes de traitement
s&#8217;organisent autour de buffers (jouant comme en informatique le r&#244;le de m&#233;moire tampon)
pouvant m&#233;moriser des chunks. Un chunk est dans cette approche d&#233;crit comme un ensemble de
propri&#233;t&#233;s caract&#233;risant une cat&#233;gorie (ou une unit&#233; de plus haut niveau), pouvant par exemple
contenir une structure syntaxique partielle (Lewis et Vasishth, 2005). Les chunks sont repr&#233;sent&#233;s
en ACT-R par des structures de traits et peuvent repr&#233;senter des objets atomiques ou complexes,
offrant la possibilit&#233; pour un chunk de faire r&#233;f&#233;rence &#224; un autre chunk et exprimer ainsi des
relations. La d&#233;finition d&#8217;un chunk est donc tr&#232;s g&#233;n&#233;rale et permet de r&#233;f&#233;rencer des structures
incompl&#232;tes ou sous-sp&#233;cifi&#233;es.
</p>
<p>La th&#233;orie ACT-R s&#8217;int&#233;resse d&#8217;une part aux processus de base et d&#8217;autre part aux structures de
m&#233;moire sur lesquelles ils s&#8217;appuient. Elle distingue notamment entre m&#233;moire proc&#233;durale et
d&#233;clarative, cette derni&#232;re permettant de stocker &#224; la fois des informations lexicales (&#224; long terme)
mais &#233;galement les structures nouvelles (&#224; court terme). La m&#233;moire d&#233;clarative repose sur un
petit nombre de buffers, chacun contenant un chunk. L&#8217;&#233;l&#233;ment important de cette organisation
r&#233;side dans le fait que ces chunks forment une unit&#233; et sont utilisables (ou accessibles) directement
en m&#233;moire. Cette accessibilit&#233; est soumise &#224; un niveau d&#8217;activation d&#233;pendant de plusieurs
param&#232;tres : degr&#233; de latence depuis le dernier acc&#232;s, poids des &#233;l&#233;ments associ&#233;s au chunk et qui
peuvent l&#8217;activer (les sources), mais &#233;galement force des relations associant les sources au chunk
consid&#233;r&#233;. Il est ainsi possible de proposer une formule permettant de quantifier l&#8217;activation d&#8217;un
chunk i :
</p>
<p>Ai = Bi +
&#65535;
j
</p>
<p>WjSji (1)
</p>
<p>TALN-R&#201;CITAL 2013, 17-21 Juin, Les Sables d&#8217;Olonne
</p>
<p>231 c&#65535; ATALA</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>FIGURE 1 &#8211; Nombre de fixations par cat&#233;gorie
</p>
<p>Dans cette formule, B repr&#233;sente l&#8217;activation de base (fr&#233;quence et historique de l&#8217;acc&#232;s au
chunk), W correspond aux poids des termes en relation avec le chunk et S la force des relations
reliant ces termes au chunk. Il est donc possible de caract&#233;riser un chunk en fonction de son
niveau d&#8217;activation. Le point important qui nous int&#233;resse ici r&#233;side dans le fait que cette
activation est en partie d&#233;pendante des relations avec le contexte. En d&#8217;autres termes, la force des
relations permettra d&#8217;activer de fa&#231;on plus ou moins importante un chunk (et donc la cat&#233;gorie
correspondante). Or, l&#8217;activation d&#8217;un chunk contr&#244;le &#224; la fois sa probabilit&#233; et la vitesse de son
acc&#232;s : un chunk fortement activ&#233; sera ainsi accessible tr&#232;s rapidement.
</p>
<p>On remarquera que cette approche est compatible avec le principe MoP de Hawkins (cf. section
pr&#233;c&#233;dente) : les relations activant un chunk peuvent &#234;tre vues comme des propri&#233;t&#233;s dont on
recherche la maximisation.
</p>
<p>Dans le cadre du traitement du langage et plus particuli&#232;rement de l&#8217;analyse syntaxique, notre
hypoth&#232;se est que les chunks facilitent l&#8217;analyse d&#8217;un &#233;nonc&#233;. Plus pr&#233;cis&#233;ment, les &#233;nonc&#233;s
comportant des chunks hautement activ&#233;s sont trait&#233;s plus facilement que les autres.
</p>
<p>2.2 Une observation exp&#233;rimentale des chunks dans le traitement de la
phrase
</p>
<p>Dans le cadre d&#8217;une exp&#233;rience r&#233;cente, consistant &#224; acqu&#233;rir des donn&#233;es de mouvement oculaire
de sujets lisant le French Treebank (Rauzy et Blache, 2012), nous avons observ&#233; un ph&#233;nom&#232;ne
int&#233;ressant en relation avec les chunks. Le nombre de fixations du regard par mot diff&#232;re en
effet fortement en fonction de la taille du mot, mais &#233;galement de sa cat&#233;gorie. La figure 1
repr&#233;sente le nombre moyen de fixations par cat&#233;gorie. On observe ainsi que les cat&#233;gories &#224;
contenu lexical (N, V, Adj, Adv) ont un nombre de fixations du regard nettement plus &#233;lev&#233; que
les mots grammaticaux (Det, Prep, Clit, etc.).
</p>
<p>Ce ph&#233;nom&#232;ne peut &#234;tre mis en relation avec l&#8217;&#233;tude de l&#8217;&#233;volution de l&#8217;indice de surprise (Hale,
2001) dans une phrase. Cet indice refl&#232;te une probabilit&#233; d&#8217;int&#233;gration de chaque mot dans la
structures syntaxique en cours de construction (calcul&#233; comme une fonction de la diff&#233;rence de
probabilit&#233; entre les structures pr&#233;c&#233;dant et celle int&#233;grant le mot courant). Plusieurs exp&#233;riences
ont montr&#233; qu&#8217;il &#233;tait un bon pr&#233;dicteur du temps de lecture, pouvant donc &#234;tre utilis&#233; comme
</p>
<p>TALN-R&#201;CITAL 2013, 17-21 Juin, Les Sables d&#8217;Olonne
</p>
<p>232 c&#65535; ATALA</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>FIGURE 2 &#8211; Evolution de l&#8217;indice de surprise dans une phrase
</p>
<p>mesure de difficult&#233; (voir (Demberg et Keller, 2008) pour l&#8217;anglais et (Rauzy et Blache, 2012)
pour le fran&#231;ais). Un indice de surprise peut donc &#234;tre associ&#233; &#224; chaque mot de la phrase. La figure
2 illustre l&#8217;&#233;volution de la valeur de cet indice (calcul&#233; selon la m&#233;thode d&#233;crite dans (Blache
et Rauzy, 2011)) sur une phrase. On remarque l&#224; aussi un ph&#233;nom&#232;ne int&#233;ressant, soulignant
la succession d&#8217;indices &#233;lev&#233;s et faibles en fonction de la cat&#233;gorie : les mots grammaticaux
correspondent syst&#233;matiquement &#224; un indice de surprise plus &#233;lev&#233; que les mots lexicaux auxquels
ils sont associ&#233;s.
</p>
<p>Ces deux observations sont convergentes : la fixation du regard en lecture englobe en un seul
mouvement le token lexicalis&#233; et les mots grammaticaux qui lui sont associ&#233;s, ce qui peut &#234;tre
pr&#233;dit au niveau de l&#8217;&#233;volution de l&#8217;indice de surprise. Elles confortent donc l&#8217;hypoth&#232;se d&#8217;un
traitement non pas au niveau du mot, mais directement par chunk, chaque fois que c&#8217;est possible.
</p>
<p>2.3 Hypoth&#232;se
</p>
<p>La th&#233;orie ACT-R appliqu&#233;e au langage fait l&#8217;hypoth&#232;se que le traitement linguistique d&#8217;int&#233;gration
repose sur des chunks. Ceux-ci sont des structures partielles, pouvant &#234;tre &#224; la fois stock&#233;es dans
la m&#233;moire &#224; long terme, mais &#233;galement construites en temps r&#233;el, en m&#233;moire &#224; court terme.
Ces chunks reposent sur une notion d&#8217;activation, elle-m&#234;me correspondant au principe Maximize
Online Processing : l&#8217;int&#233;gration d&#8217;un mot &#224; une structure (par exemple l&#8217;association de deux
cat&#233;gories pour construire un chunk) repose sur la v&#233;rification d&#8217;un maximum de propri&#233;t&#233;s.
La force des relations unissant un objet avec des &#233;l&#233;ments qui le pr&#233;c&#232;dent permet d&#8217;activer
fortement cet objet.
</p>
<p>Nous &#233;mettons l&#8217;hypoth&#232;se que les chunks facilitent le traitement linguistique. Nous nous ap-
puyons pour cela sur trois aspects :
</p>
<p>1. Les chunks sont construits en m&#233;moire sur la base du processus d&#8217;activation, qui ne
correspond pas &#224; une v&#233;ritable analyse syntaxique. Leur construction peut reposer sur des
m&#233;canismes de bas niveau (comme la fr&#233;quence de cooccurrence) ou sur l&#8217;accumulation de
propri&#233;t&#233;s ou relations entre deux cat&#233;gories. Lorsqu&#8217;une cat&#233;gorie est fortement activ&#233;e
par une ou plusieurs cat&#233;gories pr&#233;c&#233;dentes, elle formera un chunk avec elles. Dans la
plupart des cas, ces chunks sont form&#233;s d&#8217;une suite [mot grammatical + mot lexical].
</p>
<p>TALN-R&#201;CITAL 2013, 17-21 Juin, Les Sables d&#8217;Olonne
</p>
<p>233 c&#65535; ATALA</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>2. Les chunks sont stock&#233;s en m&#233;moire d&#233;clarative et accessibles directement. Certains chunks
peuvent &#234;tre tr&#232;s fr&#233;quents voire correspondre &#224; des suites plus ou moins fig&#233;es (par
exemple dans des collocations). Dans ce cas, ils sont stock&#233;s en m&#233;moire &#224; long terme. Les
chunks construits dynamiquement sur la base d&#8217;une activation sont quant &#224; eux disponibles
dans des buffers de traitement &#224; court terme.
</p>
<p>3. La pr&#233;sence de chunks dans une phrase facilite son traitement : ils sont accessible d&#8217;un bloc
et ne n&#233;cessitent pas d&#8217;analyse. Une phrase contenant des chunks sera plus facile &#224; traiter
qu&#8217;une autre n&#8217;en contenant pas.
</p>
<p>La question qui se pose est celle de la notion d&#8217;activation, son &#233;valuation et sa mise en &#339;uvre
dans le processus de construction des chunks. Nous proposons pour cela d&#8217;utiliser la description
des propri&#233;t&#233;s syntaxiques sous la forme de contraintes. Maximiser les propri&#233;t&#233;s (et donc activer
une cat&#233;gorie) correspond ainsi &#224; la maximisation de l&#8217;ensemble des contraintes &#224; satisfaire.
Nous utilisons pour cela la repr&#233;sentation propos&#233;e dans le cadre des Grammaires de Propri&#233;t&#233;s
(Blache, 2001).
</p>
<p>3 Propri&#233;t&#233;s et activation
</p>
<p>Nous pr&#233;sentons dans cette section les principales caract&#233;ristiques de l&#8217;approche des Grammaires
de Propri&#233;t&#233;s (Blache, 2001) utilis&#233;es pour d&#233;finir la notion d&#8217;activation. Elle repose sur la
repr&#233;sentation des informations syntaxiques sous la forme d&#8217;un ensemble de propri&#233;t&#233;s pouvant
&#234;tre d&#233;crites, suivant la proposition de (Duchier et al., 2009), comme des relations caract&#233;risant
un syntagme (ici not&#233; A) et mettant en relation des constituants (not&#233;s B,C ou S) :
</p>
<p>Obligation A :&#8710;B au moins un B
Unicit&#233; A : B! au plus un B
Lin&#233;arit&#233; A : B &#8826; C B pr&#233;c&#232;de C
Implication A : B&#8658; C si &#8707;B, alors &#8707;C
Exclusion A : B &#65535;&#8660; C pas de B et C simultan&#233;ment
Constituance A : S? les descendants &#8712; S
D&#233;pendance A : B&#65535; C B d&#233;pend de C
</p>
<p>Une Grammaire de Propri&#233;t&#233;s associe &#224; chaque syntagme un ensemble de contraintes. Le tableau
suivant illustre la grammaire du syntagme adjectival (not&#233; SA) (extraite du French Treebank, cf.
(Abeill&#233; et al., 2003)). Soulignons au passage la compacit&#233; de la repr&#233;sentation : 22 contraintes
sont utilis&#233;es pour d&#233;crire les constructions possibles du SA 2.
</p>
<p>Constituance AP : {AdP, A, VPinf, PP, Ssub, AP, NP} ?
</p>
<p>Lin
AP : A &#8826; {VPinf, Ssub, PP, NP, AP}
AP : AdP &#8826; {A, Ssub, PP}
AP : AP &#8826; {A, AdP}
AP : PP &#8826; {Ssub}
</p>
<p>D&#233;pendance AP : {AdP, VPinf, PP, Ssub, NP}&#65535; A
Unicit&#233; AP : {A, VPinf, Ssub} !
Obligation AP : &#8710; A
Exclusion AP : VPinf &#65535;&#8660; {PP, Ssub}
</p>
<p>2. Le jeu d&#8217;&#233;tiquettes utilis&#233; est celui du FTB, notant AP pour syntagme adjectival, AdP pour syntagme adverbial, etc.
</p>
<p>TALN-R&#201;CITAL 2013, 17-21 Juin, Les Sables d&#8217;Olonne
</p>
<p>234 c&#65535; ATALA</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>FIGURE 3 &#8211; Graphe des propri&#233;t&#233;s satisfaites pour &#8220;L&#8217;industrie est tr&#232;s capable.&#8221;
</p>
<p>Une analyse dans le cadre de GP consiste, pour une suite de cat&#233;gories donn&#233;e, &#224; &#233;valuer
l&#8217;ensemble des propri&#233;t&#233;s correspondantes. Une propri&#233;t&#233; correspondant &#224; une relation entre une
ou plusieurs cat&#233;gories, le r&#233;sultat de l&#8217;analyse est donc un graphe comme repr&#233;sent&#233; dans la
figure suivante illustrant l&#8217;analyse de la phrase &#8220;L&#8217;industrie est tr&#232;s capable.&#8221;, extraite du FTB. Ce
graphe indique les propri&#233;t&#233;s satisfaites entre les diff&#233;rentes cat&#233;gories composant la structure
syntaxique. Par exemple, la contrainte de lin&#233;arit&#233; entre le d&#233;terminant et le nom est repr&#233;sent&#233;e
par un arc reliant les deux n&#339;uds correspondants) :
</p>
<p>Construire une analyse syntaxique dans ce type d&#8217;approche consiste donc &#224; chaque &#233;tape &#224; par-
courir le syst&#232;mes de contraintes en &#233;valuant celles qui correspondent aux cat&#233;gories concern&#233;es.
Dans une perspective incr&#233;mentale, il est donc possible &#224; chaque &#233;tape de conna&#238;tre les relations
qui concernent le mot ou la cat&#233;gorie &#224; analyser. Cette caract&#233;ristique constituera la base de la
d&#233;finition de la notion d&#8217;activation utilis&#233;e ici.
</p>
<p>Par ailleurs, il est possible de distinguer deux constructions en fonction du nombre de relations
permettant de les caract&#233;riser. Dans l&#8217;exemple pr&#233;c&#233;dent, le SA est form&#233; d&#8217;un adjectif accompagn&#233;
d&#8217;un modifieur adverbial. L&#8217;exemple suivant illustre une construction l&#233;g&#232;rement diff&#233;rente d&#8217;un
SA, correspondant &#224; la phrase &#8220;L&#8217;industrie est capable d&#8217;investir.&#8221; dans laquelle une infinitive
est compl&#233;ment de l&#8217;adjectif. Dans ce cas, conform&#233;ment &#224; la grammaire du SA d&#233;crite plus
haut, un plus grand nombre de contraintes sera v&#233;rifi&#233;e, la densit&#233; du graphe est donc plus
importante. Le nombre de propri&#233;t&#233;s v&#233;rifi&#233;es joue un r&#244;le important en offrant la possibilit&#233;
de quantifier l&#8217;information syntaxique. Dans la perspective du principe MoP, la maximisation
reposera pr&#233;cis&#233;ment sur cette capacit&#233;.
</p>
<p>Un des avantages de cette approche r&#233;side dans sa souplesse : il est toujours possible d&#8217;&#233;valuer
les relations existant entre deux cat&#233;gories, sans qu&#8217;il ne soit n&#233;cessaire de construire de structure
syntaxique. Cette caract&#233;ristique r&#233;pond au besoin d&#8217;&#233;valuation de la notion d&#8217;activation d&#8217;une
cat&#233;gorie : celle-ci sera d&#233;pendante du nombre et de la force des relations existant entre un mot
et les cat&#233;gories qui la pr&#233;c&#232;dent. Nous disposons ainsi d&#8217;un cadre th&#233;orique d&#8217;implantation des
notions propos&#233;es par ACT-R appliqu&#233;e au langage.
</p>
<p>TALN-R&#201;CITAL 2013, 17-21 Juin, Les Sables d&#8217;Olonne
</p>
<p>235 c&#65535; ATALA</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>FIGURE 4 &#8211; Graphe des propri&#233;t&#233;s satisfaites pour &#8220;L&#8217;industrie est capable d&#8217;investir.&#8221;
</p>
<p>4 Activation et cr&#233;ation de chunks
</p>
<p>Nous proposons de d&#233;finir la notion d&#8217;activation sur la base des caract&#233;risations syntaxiques
construites &#224; l&#8217;aide des contraintes pr&#233;sent&#233;es dans la section pr&#233;c&#233;dente. Nous avons vu qu&#8217;il
&#233;tait possible en Grammaire de Propri&#233;t&#233;s d&#8217;&#233;valuer, pour tout sous-ensemble de cat&#233;gories, les
contraintes qui leur sont attach&#233;es. Il s&#8217;agit pour cela d&#8217;identifier les contraintes pertinentes, &#224;
savoir celles qui permettent de mettre en relation les cat&#233;gories concern&#233;es. Le principe est
simple et consiste &#224; parcourir la grammaire (l&#8217;ensemble des contraintes) et s&#233;lectionner celles qui
concernent les cat&#233;gories. En reprenant l&#8217;exemple de la grammaire du syntagme adjectival d&#233;crite
plus haut, le sous-ensemble de cat&#233;gories {AdP, A} permettra d&#8217;identifier comme pertinentes les
contraintes suivantes :
</p>
<p>AP : {AdP, A} ?
AP : AdP &#8826; A
AP : AdP&#65535; A
AP : A !
AP : &#8710; A
</p>
<p>En g&#233;n&#233;ralisant ce m&#233;canisme, il &#233;galement possible d&#8217;identifier les contraintes qui sont potentiel-
lement pertinentes : soit une contrainte A&#65535;B reliant deux cat&#233;gories A et B, la connaissance de A
permet de dire que A&#65535;B pourra devenir pertinente, &#224; la condition que B soit r&#233;alis&#233;. Dans le cas
de la grammaire du SA, la r&#233;alisation de la cat&#233;gorie AdP permet d&#8217;identifier comme contrainte
potentiellement pertinente l&#8217;ensemble suivant :
</p>
<p>AP : {AdP} ?
AP : AdP &#8826; A
AP : AP &#8826; AdP
AP : AdP&#65535; A
</p>
<p>TALN-R&#201;CITAL 2013, 17-21 Juin, Les Sables d&#8217;Olonne
</p>
<p>236 c&#65535; ATALA</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Nous proposons d&#8217;utiliser cette caract&#233;ristique pour d&#233;crire et &#233;valuer la notion d&#8217;activation. Dans
la perspective d&#8217;un traitement incr&#233;mental de la langue, le principe consiste &#224; associer &#224; chaque
cat&#233;gorie les contraintes potentiellement pertinentes qui peuvent lui &#234;tre associ&#233;es. Remarquons
que du point de vue du traitement automatique, cette information n&#8217;a pas besoin d&#8217;&#234;tre calcul&#233;e
online, mais peut &#234;tre compil&#233;e. L&#8217;ensemble des contraintes ainsi identifi&#233;es permet de d&#233;finir les
cat&#233;gories activ&#233;es : il s&#8217;agit de toutes les cat&#233;gories appartenant &#224; cet ensemble et pouvant &#234;tre
r&#233;alis&#233;es apr&#232;s la cat&#233;gorie en question. Cette derni&#232;re information est obtenue en v&#233;rifiant les
contraintes de lin&#233;arit&#233;. Dans l&#8217;exemple pr&#233;c&#233;dent, seule la cat&#233;gorie A se retrouve activ&#233;e par
AdP (la cat&#233;gorie AP ne pouvant suivre AdP comme stipul&#233; par la contrainte AP : AP &#8826; AdP).
</p>
<p>4.1 Calcul du degr&#233; d&#8217;activation
</p>
<p>Le niveau d&#8217;activation d&#8217;une cat&#233;gorie dans un contexte donn&#233; d&#233;pend de sa densit&#233; ou, en
d&#8217;autres termes, du nombre de contraintes dont elle est la cible (et dont la source la pr&#233;c&#232;de)
et de leur poids. Il s&#8217;agit donc exactement de la notion d&#8217;activation telle que d&#233;crite dans la
th&#233;orie ACT-R. Nous proposons d&#8217;&#233;valuer cette activation en tirant parti de la repr&#233;sentation par
contraintes. Pour chaque cat&#233;gorie c de la grammaire, nous &#233;tablissons une liste de transition
form&#233;e par toutes les cat&#233;gories pr&#233;sentes dans au moins une contrainte contenant c et respectant
les contrainte de lin&#233;arit&#233; (i.e. pouvant suivre c). L&#8217;activation est alors &#233;valu&#233;e comme suit :
</p>
<p>&#8211; Soit la cat&#233;gorie courante ci . Notons Trans(ci) l&#8217;ensemble des cat&#233;gories faisant partie de la
liste de transition de ci . Notons PP(ci) l&#8217;ensemble des propri&#233;t&#233;s potentiellement pertinentes
d&#233;clench&#233;es par la cat&#233;gorie ci . Notons N le nombre de ces propri&#233;t&#233;s (N =| PP(ci) |).
</p>
<p>&#8211; Notons PPcj (ci) le sous ensemble de PP(ci) form&#233; des propri&#233;t&#233;s contenant une cat&#233;gorie c j ,
avec n son cardinal. Chacune des propri&#233;t&#233;s de PP est associ&#233;e dans la grammaire &#224; un poids.
Notons
</p>
<p>&#65535;
W
</p>
<p>cj
ci la somme des poids de ces propri&#233;t&#233;s.
</p>
<p>&#8211; Pour toute cat&#233;gorie de transition de ci tq c j &#8712; Trans(ci), son degr&#233; d&#8217;activation est donn&#233; par
la formule suivante :
</p>
<p>A(c j) =
n
N
&#8727;&#65535;Wcjci (2)
</p>
<p>Le premier terme de l&#8217;activation correspond &#224; une &#233;valuation de la densit&#233; du r&#233;seau de
contraintes en rapportant le nombre de contraintes n qui permet d&#8217;activer la cat&#233;gorie &#233;tu-
di&#233;e par rapport au nombre total de contraintes potentiellement pertinentes pour la cat&#233;gorie
source. Le second terme correspond quant &#224; lui &#224; la force des relations qui unissent la cat&#233;gorie
courante (ou cat&#233;gorie activante) &#224; la cat&#233;gorie activ&#233;e.
</p>
<p>Concr&#232;tement, en cours d&#8217;analyse, cette mesure permettra d&#8217;identifier le type de cat&#233;gorie activ&#233;e
par la cat&#233;gorie courante ainsi que le niveau de son activation. Lorsque qu&#8217;une cat&#233;gorie est
activ&#233;e et r&#233;alis&#233;e, elle formera un chunk avec la cat&#233;gorie qui l&#8217;active. Ce chunk pourra avoir
un niveau d&#8217;activation plus ou moins &#233;lev&#233;, identifi&#233; par cette fonction d&#8217;activation. Notons que
cette d&#233;finition de l&#8217;activation permet &#233;galement de rendre compte des relations lexicales du
type collocationnelles. La s&#233;lection lexicale entre les termes sera dans ce cas repr&#233;sent&#233;e par une
contrainte d&#8217;implication avec un poids &#233;lev&#233;e. Il sera ainsi possible de former un chunk dot&#233; d&#8217;un
niveau d&#8217;activation fort.
</p>
<p>L&#8217;exemple qui suit illustre l&#8217;utilisation de la fonction d&#8217;activation pour la construction d&#8217;un chunk
&#224; l&#8217;int&#233;rieur du SN entre les cat&#233;gories Det et N en nous appuyant sur la grammaire extraite
</p>
<p>TALN-R&#201;CITAL 2013, 17-21 Juin, Les Sables d&#8217;Olonne
</p>
<p>237 c&#65535; ATALA</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>du French Treebank. Les contraintes dont la cat&#233;gorie Det est source sont r&#233;pertori&#233;es dans le
tableau suivant, comportant &#233;galement l&#8217;indication de leurs poids (calcul&#233; en suivant la m&#233;thode
propos&#233;e dans (Blache, 2012)).
</p>
<p>D&#233;pendance
Det&#65535; N 7,080586081
</p>
<p>Exclusion
Pro &#65535;&#8660; Det 4,358766626
Clit &#65535;&#8660; Det 0,003417994
</p>
<p>Unicit&#233;
Det 3,253068199
</p>
<p>Exigence
Det&#8658; N 2,461019161
</p>
<p>Lin&#233;arit&#233;
Det &#8826; N 12,18569885
Det &#8826; Np 0,718659942
Det &#8826; AdP 0,178675795
Det &#8826; AP 0,135447163
Det &#8826; VPpart 0,077399536
Det &#8826; VPinf 0,03891139
Det &#8826; Ssub 0,025216138
Det &#8826; Srel 0,021433718
Det &#8826; PP 0,016570605
Det &#8826; NP 0,016030259
</p>
<p>L&#8217;ensemble de transition de Det extrait de ces contraintes est le suivant :
</p>
<p>Trans(Det) = {N,Np,AdP,AP,VPpart,VPinf ,Ssub,Srel,PP,NP} (3)
L&#8217;&#233;valuation du degr&#233; d&#8217;activation des cat&#233;gories de l&#8217;ensemble de transition est r&#233;capitul&#233;e dans
le tableau suivant :
</p>
<p>Cat&#233;gorie activ&#233;e Contraintes Densit&#233; Poids Activation
N 3 0,2 21,72730409 4,345460818
Np 1 0,066666667 0,718659942 0,047910663
AdP 1 0,066666667 0,178675795 0,01191172
AP 1 0,066666667 0,135447163 0,009029811
VPpart 1 0,066666667 0,077399536 0,005159969
VPinf 1 0,066666667 0,03891139 0,002594093
Ssub 1 0,066666667 0,025216138 0,001681076
Srel 1 0,066666667 0,021433718 0,001428915
PP 1 0,066666667 0,016570605 0,001104707
NP 1 0,066666667 0,016030259 0,001068684
</p>
<p>Cet ensemble de r&#233;sultats indique, comme attendu, une forte activation de la cat&#233;gorie N
provenant d&#8217;une part du nombre de propri&#233;t&#233;s potentielles qui l&#8217;activent et d&#8217;autre part de leur
importance (i.e. un poids &#233;lev&#233;). Cette forte activation conduit &#224; la constitution d&#8217;un chunk [Det,
N] qui sera stock&#233; dans un buffer de la m&#233;moire d&#233;clarative. Ce processus d&#8217;identification de
chunk repose donc sur des m&#233;canismes de bas niveau, effectu&#233;s en temps r&#233;el ce qui se manifeste
concr&#232;tement par un traitement global notamment au niveau du mouvement oculaire dans le cas
de la lecture. L&#8217;exemple de la figure 5 illustre ce m&#233;canisme. La r&#233;alisation de la cat&#233;gorie Det
permet d&#8217;identifier trois propri&#233;t&#233;s activant le N conduisant &#224; la cr&#233;ation du chunk.
</p>
<p>L&#8217;exemple de la figure 6 d&#233;crit le m&#234;me m&#233;canisme, appliqu&#233; ici &#224; la constitution d&#8217;un chunk
form&#233;, dans le cas d&#8217;une relative sujet, par le pronom relatif et le verbe qui suit. Les cat&#233;gories
activ&#233;es les plus importantes (celles correspondant &#224; des contraintes de plus fort poids) sont V et
N , repr&#233;sent&#233;es dans le cadre associ&#233; au pronom relatif. La cat&#233;gorie V dispose cependant d&#8217;un
niveau d&#8217;activation tr&#232;s sup&#233;rieur au N . Le V &#233;tant r&#233;alis&#233; imm&#233;diatement apr&#232;s l&#8217;activation, ceci
conduit &#224; la construction du chunk [ProR, V].
</p>
<p>TALN-R&#201;CITAL 2013, 17-21 Juin, Les Sables d&#8217;Olonne
</p>
<p>238 c&#65535; ATALA</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>(a) Activation du N (b) Construction du chunk [Det, N]
</p>
<p>FIGURE 5 &#8211; Activation et construction de chunk
</p>
<p>FIGURE 6 &#8211; Activation et construction de chunk, suite
</p>
<p>Ce processus appliqu&#233; &#224; la suite des cat&#233;gories de la phrase permet de construire la suite de
chunks illustr&#233;e par la figure 7.
</p>
<p>4.2 Les chunks, m&#233;canisme de facilitation
</p>
<p>L&#8217;hypoth&#232;se que nous d&#233;fendons repose tout d&#8217;abord sur l&#8217;id&#233;e que les chunks sont construits
directement, sur la base de m&#233;canismes tirant parti &#224; la fois de crit&#232;res de fr&#233;quence et de
densit&#233; de relation. Les m&#233;canismes conduisant &#224; la construction de chunks ne sont donc pas
les m&#233;canismes classiques de l&#8217;analyse syntaxique : le probl&#232;me pos&#233; consiste &#224; mesurer les
relations unissant deux cat&#233;gories adjacentes alors que l&#8217;analyse syntaxique consiste &#224; int&#233;grer
une cat&#233;gorie &#224; une structure syntaxique globale. Il s&#8217;agit donc de m&#233;canismes de bas niveau,
effectu&#233;s tr&#232;s rapidement.
</p>
<p>Une fois construits, ces chunks sont stock&#233;s en m&#233;moire et accessibles directement, comme
indiqu&#233; dans la th&#233;orie ACT-R. Notre hypoth&#232;se consiste donc &#224; dire que les chunks facilitent le
traitement. Leur acc&#232;s se faisant en bloc, il revient du point de vue cognitif &#224; un acc&#232;s lexical. De
plus, leur int&#233;gration se fait &#233;galement de fa&#231;on globale. Par cons&#233;quent, la pr&#233;sence de chunks
dans un &#233;nonc&#233; ou une phrase en facilitera le traitement par rapport &#224; d&#8217;autres situation o&#249;
l&#8217;int&#233;gration devra se faire mot par mot. Autrement dit, une phrase contenant un grand nombre
de chunks sera plus facile &#224; traiter qu&#8217;une phrase qui en contiendra moins.
</p>
<p>Illustrons cette hypoth&#232;se en revenant sur le cas des phrases relatives. Les travaux en psycholin-
guistique (Gibson, 2000), confirm&#233;s par plusieurs &#233;tudes exp&#233;rimentales (Fedorenko et al., 2006),
(Demberg et Keller, 2009) ont montr&#233; que les relatives objet sont plus difficiles &#224; traiter que les
</p>
<p>FIGURE 7 &#8211; Construction des chunks pour la phrase compl&#232;te
</p>
<p>TALN-R&#201;CITAL 2013, 17-21 Juin, Les Sables d&#8217;Olonne
</p>
<p>239 c&#65535; ATALA</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>FIGURE 8 &#8211; Cas de la relative objet
</p>
<p>relatives sujet. Ce ph&#233;nom&#232;ne se retrouve au niveau de la construction des chunks. Nous avons vu
en effet dans l&#8217;exemple de la figure 7 que la relative sujet conduisait &#224; la construction d&#8217;un chunk
entre le pronom relatif et le verbe. La phrase correspondante contient ainsi 4 chunks au total.
La figure 6 illustre ce ph&#233;nom&#232;ne par l&#8217;impossibilit&#233; de construire un chunk contenant le relatif.
Celui-ci active bien un certain nombre de cat&#233;gories, mais aucune d&#8217;entre elle ne correspond
directement &#224; la cat&#233;gorie adjacente. Au total, la phrase contenant la relative objet ne contient
que 3 chunks. Cet exemple ne pr&#233;tend bien entendu pas &#233;riger le r&#244;le des chunks en th&#233;orie de la
difficult&#233; syntaxique comme propos&#233; par (Gibson, 2000). Elle illustre cependant des diff&#233;rences
de fonctionnement pouvant accompagner ou compl&#233;ter ces mod&#232;les.
</p>
<p>5 Conclusion
</p>
<p>Nous avons pr&#233;sent&#233; dans cet article une approche proposant de donner une place centrale &#224;
la notion de chunk dans le processus de traitement de la phrase par des sujets humains. Nous
utilisons pour cela l&#8217;architecture de traitement des processus cognitifs &#233;labor&#233;e dans le cadre de
la th&#233;orie ACT-R. Cette approche pr&#233;cise le r&#244;le jou&#233; par les chunks en m&#233;moire. Elle introduit
de plus une notion d&#8217;activation permettant d&#8217;expliquer la rapidit&#233; de traitement de ces objets.
Appliqu&#233;e &#224; la question de l&#8217;analyse syntaxique (ou du traitement de la phrase si l&#8217;on se situe
dans une perspective psycholinguistique), cette th&#233;orie offre un cadre permettant de d&#233;crire la
construction et le r&#244;le jou&#233; par ces chunks.
</p>
<p>En tirant parti d&#8217;une description des informations syntaxiques bas&#233;e sur les contraintes (dans le
cadre des Grammaires de Propri&#233;t&#233;s), nous avons propos&#233; une &#233;valuation de la notion d&#8217;activation
servant de base &#224; la construction des chunks. Il s&#8217;agit d&#8217;un m&#233;canisme de bas niveau, n&#8217;ayant
pas recours &#224; l&#8217;analyse syntaxique &#224; proprement parler et qui permet la construction d&#8217;unit&#233;s de
niveau supra-lexical facilitant le processus car accessibles directement en m&#233;moire. L&#8217;utilisation de
telles unit&#233;s correspond &#224; des observations exp&#233;rimentales, notamment de mouvement oculaire,
montrant que les chunks correspondent &#224; des unit&#233;s de traitement pertinentes.
</p>
<p>Il reste &#224; &#233;valuer la validit&#233; de l&#8217;hypoth&#232;se de facilitation des chunks de fa&#231;on exp&#233;rimentale. Il
s&#8217;agira notamment de v&#233;rifier que la construction des chunks est un processus de bas niveau et que
leur acc&#232;s correspond &#224; un acc&#232;s lexical en compl&#233;tant les observations de mouvement oculaire
par des exp&#233;riences &#224; l&#8217;aide de potentiels &#233;voqu&#233;s et de localisation de source. L&#8217;&#233;tape suivante
consistera &#224; v&#233;rifier la facilitation induite par les chunks en termes de temps de traitement.
</p>
<p>TALN-R&#201;CITAL 2013, 17-21 Juin, Les Sables d&#8217;Olonne
</p>
<p>240 c&#65535; ATALA</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Remerciements
</p>
<p>Ce travail r&#233;alis&#233; dans le cadre du Labex BLRI (http ://www.blri.fr) portant la r&#233;f&#233;rence ANR-11-
LABX-0036 a b&#233;n&#233;fici&#233; d&#8217;une aide de l&#8217;Etat g&#233;r&#233;e par l&#8217;ANR au titre du projet Investissements
d&#8217;Avenir A*MIDEX portant la r&#233;f&#233;rence ANR-11-IDEX-0001-02.
</p>
<p>R&#233;f&#233;rences
</p>
<p>ABEILL&#201;, A., CL&#201;MENT, L. et TOUSSENEL, F. (2003). Building a treebank for french. In ABEILL&#201;, A.,
&#233;diteur : Treebanks, Kluwer, Dordrecht.
</p>
<p>ABNEY, S. (1991). Parsing by chunks. In Principle-Based Parsing. Kluwer Academic Publishers,
pages 257&#8211;278.
</p>
<p>ANDERSON, J. R., BOTHELL, D., BYRNE, M. D., DOUGLASS, S., LEBIERE, C. et QIN, Y. (2004). An
integrated theory of the mind. Psychological Review, 111(4):1036&#8211;1060.
</p>
<p>BIRD, S., KLEIN, E. et LOPER, E. (2009). Natural Language Processing with Python. O&#8217;Reilly Media.
</p>
<p>BLACHE, P. (2001). Les Grammaires de Propri&#233;t&#233;s : Des contraintes pour le traitement automatique
des langues naturelles. Herm&#232;s.
</p>
<p>BLACHE, P. (2012). Estimating constraint weights from treebanks. In Proceedings of CSLP.
</p>
<p>BLACHE, P. et RAUZY, S. (2011). Predicting linguistic difficulty by means of a morpho-syntactic
probabilistic model. In Proceedings of PACLIC 2011, december 2011, Singapour.
</p>
<p>DEMBERG, V. et KELLER, F. (2008). Data from eye-tracking corpora as evidence for theories of
syntactic processing complexity. In Cognition, volume 109, Issue 2, pages 193&#8211;210.
</p>
<p>DEMBERG, V. et KELLER, F. (2009). A computational model of prediction in human parsing :
Unifying locality and surprisal effects. In Proceedings of the 31st Annual Conference of the
Cognitive Science Society, pages 1888&#8211; 1893.
</p>
<p>DUCHIER, D., PROST, J.-P. et DAO, T.-B.-H. (2009). A model-theoretic framework for grammatica-
lity judgements. In Conference on Formal Grammar (FG&#8217;09).
</p>
<p>FEDORENKO, E., GIBSON, E. et ROHDE, D. (2006). The nature of working memory capacity in
sentence comprehension : Evidence against domain-specific working memory resources. Journal
of Memory and Language, 54(4):541&#8211;553.
</p>
<p>GIBSON, E. (1998). Linguistic complexity : locality of syntactic dependencies. Cognition, 68:1&#8211;76.
</p>
<p>GIBSON, E. (2000). The dependency locality theory : A distance-based theory of linguistic
complexity. In Image. A. Marantz, Y. Miyashita, W. O&#8217;Neil (Edts).
</p>
<p>GRODNER, D. J. et GIBSON, E. A. F. (2005). Consequences of the serial nature of linguistic input
for sentenial complexity. Cognitive Science, 29:261&#8211;291.
</p>
<p>HALE, J. (2001). A probabilistic earley parser as a psycholinguistic model. In Proceeding of
2nd Conference of the North American Chapter of the Association for Computational Linguistics,
Pittsburgh, PA.
</p>
<p>HAWKINS, J. (2003). Efficiency and complexity in grammars : Three general principles. In
MOORE, J. et POLINSKY, M., &#233;diteurs : The Nature of Explanation in Linguistic Theory, pages
95&#8211;126. CSLI Publications.
</p>
<p>TALN-R&#201;CITAL 2013, 17-21 Juin, Les Sables d&#8217;Olonne
</p>
<p>241 c&#65535; ATALA</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>LEWIS, R. L. et VASISHTH, S. (2005). An activation-based model of sentence processing as skilled
memory retrieval. Cognitive Science, 29:375&#8211;419.
</p>
<p>RAUZY, S. et BLACHE, P. (2012). Robustness and processing difficulty models. a pilot study
for eye-tracking data on the french treebank. In Proceedings of the 1st Eye-Tracking and NLP
workshop.
</p>
<p>REITTER, D., KELLER, F. et MOORE, J. D. (2011). A computational cognitive model of syntactic
priming. Cognitive Science, 35(4):587&#8211;637.
</p>
<p>TALN-R&#201;CITAL 2013, 17-21 Juin, Les Sables d&#8217;Olonne
</p>
<p>242 c&#65535; ATALA</p>

</div></div>
</body></html>