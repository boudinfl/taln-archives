TALN-RECITAL 2013, 17-21 Iuin, Les Sables d’Olonne

Annotation sémantique pour des domaines spécialisés et des
ontologies riches

Yue Mal Francois Lévyz Adeline Nazarenkoz
(1) TU-Dresden, Germany
(2) LIPN, Université Paris 13-CNRS, France
mayue<0tcs . inf . tu—dresden . de ,
{francois . levy , adeline . naza.renko}<01ipn . univ—paris 13 . fr

RESUME
Ex lorer et maintenir une documentation techni ue est une téiche difﬁcile our la uelle on
P (1

pourrait bénéﬁcier d’un outillage efﬁcace, a condition que les documents soient annotés sémanti-
uement. Les annotations doivent étre riches, cohérentes, suffisamment s écialisées et s’a u er

(1 P

sur un modele sémantique explicite — habituellement une ontologie — qui modélise la sémantique

du domaine cible. I1 s’avére que les approches d’annotation traditionnelles donnent pour cette

tache des résultats limités. Nous ro osons donc une nouvelle a roche, l’annotation sémanti ue

P P PP
statisti ue basée sur les s ta es, ui rédit les annotations sémanti ues a artir d’un ensemble
(1 YT1 8m (1 P ‘l P
d’apprentissage réduit. Cette modélisation facilite l’annotation sémantique spécialisée au regard
de modéles sémanti ues de domaine arbitrairement riches. Nous l’évaluons a l’aide de lusieurs
(1 P

métriques et sur deux textes décrivant des réglementations métier. Notre approche obtient de

bons résultats. En particulier, la F—mesure est de l’ordre de 91, 9% et 97, 6% pour la prédiction de

l’étiquette et de la position avec différents parametres. Cela suggere que les annotateurs humains

peuvent étre fortement aidés pour l’annotation sémantique dans des domaines spéciﬁques.

ABSTRACT
Semantic Annotation in Specific Domains with rich Ontologies

Technical documentations are generally difﬁcult to explore and maintain. Powerful tools can
help, but they require that the documents have been semantically annotated. The annotations
must be sufﬁciently specialized, rich and consistent. They must rely on some explicit semantic
model — usually an ontology — that represents the semantics of the target domain. We observed
that traditional approaches have limited success on this task and we propose a novel approach,
phrase—based statistical semantic annotation, for predicting semantic annotations from a limited
training data set. Such a modeling makes the challenging problem, domain speciﬁc semantic
annotation regarding arbitrarily rich semantic models, easily handled. Our approach achieved a
good performance, with several evaluation metrics and on two different business regulatory texts.
In particular, it obtained 91.9% and 97.65 % F-measure in the label and position predictions
with different settings. This suggests that human annotators can be highly supported in domain
speciﬁc semantic annotation tasks.

MOTS-CLES : Annotation sémantique, Ontologie de domaine, Annotation automatique, Analyse
sémantique des textes, Méthodes statistiques.

KEYWORDS: Semantic Annotation, Domain Ontology, Automatic annotation, Semantic Text
Analysis, Statistical methods.

464 © ATALA

TALN-RECITAL 2013, 17-21 Iuin, Les Sables d’Olonne
1 Introduction

Les documents techniques sont souvent complexes a lire et a maintenir mais ce sont des ressources
critiques pour de nombreuses organisations. Les textes réglementaires décrivent les procédures,
les régles et les politiques auxquels les organisations doivent se conformer; ce sont des sources
importantes, qui guide souvent la prise de décision dans ces organisations. Les instructions
d’utilisation indiquent comment utiliser et maintenir des objets techniques qui sont parfois
extrémement complexes. Les experts ont besoin d’outils pour les aider a maitriser et a valider ces
documents autant que pour les maintenir a jour quand des évolutions techniques se produisent.
Les textes sont de longueur variable (de quelques dizaines a plusieurs centaines de pages), mais
souvent trop longs pour étre faciles a lire, en particulier quand les informations importantes sont
dispersées dans différentes parties. Ils contiennent des descriptions génériques plutot que des
exemples et reposent sur des Vocabulaires spécialisés, qui sont souvent déﬁnis de facon plus ou
moins forinelle et précise dans des thesaurus ou des ontologies.

Il est possible d’aider les experts qui consultent ces textes en leur fournissant des outils. Le bénéﬁce
est plus important si les documents sources sont enrichis par des informations sémantiques
(ontologiques), qui assurent une certaine interopérabilité et qui permettent de faire des recherches
sémantiques plutot que de simples recherches de chaines de caracteres (Welty et Ide, 1999; Uren
et al., 2006; Nazarenko et al., 2011). L’annotation sémantique aide a Visualiser et a rassembler
l’information importante, mais aussi a controler la documentation technique (vériﬁcation de
cohérence, aide a la décision et tracabilité, mise a jour, etc.).

Des outils ont été développés, tels que GATE (Cunningham et al., 2011) ou SemEx (Nazarenko
et al., 2011), pour explorer des textes dont certaines portions sont liées par des annotations a
divers éléments d’un modele sémantique de domaine. L’annotation sémantique automatique de
la documentation technique spécialisée présente cependant deux caractéristiques importantes.

En premier lieu, les annotations sémantiques intéressantes étiquettent souvent des notions
génériques ou des concepts plutot que des mentions d’entités modélisées comme des instances
de concepts. Ceci differe de la Reconnaissance des Entités Nommées (REN) qui vise a repérer
les instances de certains types sémantiquesl. Par exemple, dans le texte de la figure 1, le
fragment “Service conducting approval tests” est annoté par le concept TestConductingSer1/ice
et pas par l’une ses instances. Les notions génériques susceptibles d’étre annotées sont plus
nombreuses que les types canoniques des entités nommées, et les approches d’annotation
sémantique traditionnelles sont handicapées dans ce cas par des caractéristiques moins régulieres
et des ressources plus rares. On observe que les méthodes d’annotation au regard d’une ontologie
se concentrent généralement sur les instances de concepts dans une perspective de peuplement
d’ontologies (Kiryakov et al., 2004; Amardeilh et al., 2005; Uren et al., 2006).

'l!'h»e seats of the vehicle shall be fitted and shall be placed in the position for
driving use chosen by the Technical Service conducting approval tests to give the
most adverse conditions with respect to strength, compatible with installing the
manikin in the vehicle. The positions of the seats shall be stated in the report.

FIGURE 1 — Exemple : texte réglementaire avec annotations sémantiques

1. Typiquement : Personne, Organisation, Lieu, Temps.
465 © ATALA

TALN-RECITAL 2013, 17-21 Iuin, Les Sables d’Olonne

En second lieu, les ontologies génériques (par ex. DBpedia) utilisées par de nombreux services
d’annotation sémantique ouverts sont peu utiles pour les documents techniques. Nous avons testé
plusieurs d’entre elles sur un corpus traitant de la réglementation dans l’industrie automobile.
Quatre produisent trés peu d’annotau'ons : Opencalais 2, Zemanta 3 et DBpedia Spotlight (Mendes
et al., 2011) ont des rappels de 3,3 %, 0,8 % et 0 % ; AlchemyAPI4 reconnait la mention de deux
organisations 5 et d’une ville 5, mais deux de ces annotations sont manifestement erronées dans le
domaine considéré. A l’inverse, la Wiki Machine (LiveMemories, 2010) annote surabondamment
le réglement : dans le fragment “In the case of an assembly incorporating a retractor”, “case” est
annoté par Law, Justice et “assembly” est lié a Parliamentary procedure et Meetings, mais ce n’est
pas le sens qu’ont ces termes dans nos données. Ces annotateurs du Web basés sur des ontologies
publiques renvoient souvent une interprétation trompeuse des textes spécialisés.

Nous en concluons qu’un systeme d’annotation sémantique des documents techniques devrait
avoir les propriétés suivantes : (1) pouvoir noter un concept et pas seulement des instances de
types généraux comme signiﬁcation d’un terme; (2) fournir une interprétation précise et fiable,
en tenant compte des modéles sémantiques du domaine traité; (3) avoir une bonne couverture
du texte, de sorte que les fragments textuels intéressants puissent étre facilement détectés et
reliés. Notre approche repose sur le constat qu’un expert métier peut fournir un petit nombre
d’exemples annotés manuellement, mais ne peut pas annoter des documents volumineux. Nous
avons vu que les approches de l’état de l’art répondent mal a ces spéciﬁcations.

Nous proposons donc une nouvelle approche d’annotation, a la fois simple et naturelle, qui
s’inspire de la traduction automatique basée sur les syntagmes et qui est adaptée a l’annotau'on
spécialisée requise par les textes techniques.

Nous transposons le modele de la traduction automatique statistique (TAS) basée sur les syn-
tagmes a notre probleme et nous montrons expérimentalement, avec différentes métriques
d’évaluation, que l’annotateur ainsi construit obtient des résultats signiﬁcatifs a partir d’un corpus
réduit annoté manuellement. Par effet de bord, il peut intégrer dans un modéle unique les inter-
prétations que différents experts auraient données du méme texte. Les expériences rapportées
ici portent sur un reglement international sur le controle des ceintures de sécurité (par la suite
« Reglement des ceintures de sécurité »), auquel les constructeurs d’automobiles doivent se
conformer.

Le reste de l’article est structuré ainsi : nous discutons l’état de l’art dans la section qui suit
et déﬁnissons la tache dans la section 3. Puis notre méthode est décrite dans la section 4. Les
expériences et leur évaluation sont présentées dans les sections 5 et 6.

2. http : //www . opencalais . com

3. http : //www . zemanta. com

4. http : //www . alchemyapi . com

5. “cabinet” dans “... shall be placed in a refrigerated cabinet at -10 C + 1 C for two hours” et“Technical Service” dans
“One of these axes shall be in the direction chosen by the Technical Service conducting the approval test”.

6. “anchorage” dans la phrase “except in the case of retractors having a pulley or strap guide at the upper belt
anchorage”.

466 © ATALA

TALN-RECITAL 2013, 17-21 Iuin, Les Sables d’Olonne
2 Etat de 1’art

Les deux facettes de notre probleme, prédire les labels sémantiques et les frontieres de ces
étiquettes, se retrouvent dans la REN (Nadeau et Sekine, 2007) et les annotateurs du Web
sémantique (Uren et al., 2006). Dans la REN, les étiquetages sont souvent limités a quelques
grandes catégories génériques comme Personne, Endroit, Organization, Produit, et Date. Quant
aux annotateurs du Web, dont les étiquetages proviennent généralement d’ontologies générales,
comme TAP (Dill et al., 2003), DBpedia Lexicalization Dataset 7), ils ne sont pas efﬁcaces pour
les textes spécialisés et des domaines différents. De plus, ils privilégient souvent la précision au
détriment du rappel, produisant moins de deux annotations par page en moyenne (Dill et al.,
2003; Mihalcea et Csomai, 2007; Cucerzan, 2007).

Un premier type d’approches de l’annotation sémantique consiste a appliquer des régles sur des
segments sélectionnés par des wrappers (Ciravegna, 2003; Etzioni et al., 2004; Cimiano et al.,
2004). S’agissant d’une annotation sémantique précise et spécialisée, il est difﬁcile d’apprendre
des regles pour chaque type d’annotation, a cause du grand nombre de catégories sémantiques.
De plus, les regles sont souvent plus complexes que pour la REN, ou les entités cibles ont
généralement une forme particuliére (par ex. débutant par une majuscule) ou sont associées a
des déclencheurs comme un titre (par ex. « M. », « Le président >>). Dans l’annotation spécialisée,
les fragments de texte a annoter sont trés variés et leurs frontiéres sont difﬁciles a identiﬁer. Par
exemple, dans le réglement des ceintures de sécurité, “tested according to paragraph 7.6.4.2.” a
été étiqueté manuellement par Method (voir section 5).

Une seconde famille d’approches d’annotation sémantique repose sur des modeles statistiques
ou l’apprentissage automatique (par ex. HMM (Zhou et Su, 2002; Ratinov et Roth, 2009), CRF
(Finkel et Manning, 2009), et Perceptron ou Winnow (Collins, 2002)). Ces approches exploitent
la richesse des ressources textuelles du Web (Dill et al., 2003; LiveMemories, 2010; Mendes
et al., 2011) ou de journaux (Nadeau et Sekine, 2007; Ratinov et Roth, 2009) comme données
d’entrainement pour la désambiguisation. Le traitement de l’ambigu'ite' est important quand on
considere différents niveaux de granularité ontologique : selon le contexte, un terme comme
“test" peut faire référence au concept génénral Test, a une catégorie précise de tests ou a une
instance de test particuliere. Cependant, dans les domaines spécialisés, on a rarement de gros
volumes de données. Notre approche repose sur un modele statistique différent, qui prend en
compte la forme brute des textes (sans traitement linguistique préalable) et montre de meilleures
performances que les champs aléatoires conditionnels en chaines linéaires (CRF) sur un petit
volume de données.

Les recherches sur la REN dans des corpus spécialisés (Wang, 2009; Liu et al., 2011) indiquent
qu’il faudrait entrainer des systemes d’annotation spéciﬁques méme dans le cas ou le jeu d’éti—
quettes est le méme que pour la REN classique (Wang, 2009; Liu et al., 2011) quand le corpus
est spécialisé (ex. Tweet, notes cliniques). Le présent travail s’intéresse aux cas ou le corpus
et les jeux d’étiquettes sont spécialisés, comme dans (Aronson et Lang, 2010; Muller et al.,
2004) qui proposent un entrainement spécialisé pour la biomédicine. A la différence de cette
approche qui est difﬁcile a adapter a un autre domaine, notre méthode, fondée sur TAS, peut étre
facilement appliquée sur un autre domaine spécialisé a condition qu’un petit volume de données
d’entrainement annotées soit disponible.

7. http : //dbpedia . org/ Lexi calizations
467 © ATALA

TALN—RECITAL 2013, 17-21 Iuin, Les Sables d’Olonne

Les modeles de TAS ont été appliqués a d’autres questions que la traduction, en particulier
a la normalisation de textes et de SMS (Aw et al., 2006; Beaufort et al., 2010) et a l’analyse
sémantique (Wong et Mooney, 2006). Selon ces auteurs, leurs résultats, mesurés par les métriques
de traduction automatique, sont bons. S’agissant de l’annotation sémantique de documents
spécialisés, nous adoptons nous aussi un modele de TAS basée sur les syntagmes, mais nous
l’éValuons différemment parce que les métriques de traduction automatique s’avérent limitées
pour notre tache.

3 Déﬂnition de la téiche

On dispose d’une ontologie pour un domaine spécialisé dont le volet lexical est utilisé pour
annoter un petit corpus d’entrainement. La tache consiste a identiﬁer a la fois les frontiéres et la
catégorie ontologique des éléments sémantiques majeurs de chaque phrase du corpus a annoter.

En plus des termes spécialisés qu’i1 est utile de détecter et d’annoter, un autre probléme fréquent
pour l’annotation sémantique de documents techniques au regard d’une ontologie riche est qu’un
grand nombre de mots identiques en surface peuvent étre annotés avec plusieurs étiquettes
ontologiques qui ne sont pas logiquement disjointes comme c’est le cas dans l’homonymie, mais
qui reﬂetent simplement une granularité de sens variable en contexte. Par exemple, dans les
quatre phrases ci—dessous, « test >> a été annoté par 1’expert comme BuckleTest a trois reprises

(S1, S2 et S3) et Method une fois (S4). La résolution de l’ambigu'ité est importante pour le

succés de cette tache.

S1. The force required to open the buckle in the Q as prescribed in paragraph 7.8. below shall not
exceed 6 daN.

S2. In the case of harness belt buckles, this Q may be carried out without all the tongues being
introduced.

S3. In the case of buckles which incorporate a component common to two assemblies, the strength
and release E of paragraphs 7.7. and 7.8. shall also be can'ied out with the part of the buckle
pertaining to one assembly being engaged in the mating part pertaining to the other, if it is possible
for the buckle to be so assembled in use.

S4. Retractors shall be subjected to tests and shall fulﬁll the requirements speciﬁed below, including
the tests for strength prescribed in paragraphs 7.5.1. and 7.5.2.

4 Annotation sémantique statistique basée sur les syntagmes

Nous modélisons l’annotation sémantique des documents spécialisés comme une tache de tra-
duction automatique ayant les caractéristiques suivantes : (1) les unités textuelles pertinentes
pour traduire ou annoter sont des syntagmes plut6t que de simples mots; (2) de méme qu’un
mot peut étre traduit de différentes facons, on peut annoter un fragment de texte de plusieurs
maniéres, des éléments ontologiques différents pouvant avoir des lexicalisations communes.

468 © ATALA

TALN-RECITAL 2013, 17-21 Iuin, Les Sables d’Olonne

4.1 L’annotation sémantique en tant que traduction automatique

Dans cette vision d’une annotation sémantique comme traduction, le texte initial non annoté est
considéré comme le texte a << traduire » et le texte annoté comme le texte cible << traduit ».

Formellement, on a deux phrases (s1,s2) dans deux « langages >> L1 and L2 : L1 est ici l’anglais
et L2 = L1 U Voc(O) est l’union de l’anglais et du vocabulaire de l’ontologie, Voc(O), utilisé
comme ensemble d’étiquettes8. Nous disons que 52 est une Version annotée de 51 s’il est obtenu
en remplagant certains groupes de mots anglais de 51 par des éléments de Voc(O) comme illustré
dans la ﬁgure 2.

_/ \.

diSi0i"' ' Natura|_Person

é

If an AAdvantag agreement changes

51 or terminates, you may find 

  

If an AAdvantage XXAir|ine_Participant agreement

5 . .
2 changes or terminates, you may find 

FIGURE 2 — L’annotation sémantique en tant que traduction

D’apres (Tomeh, 2012), la TAS a conceptuellement trois étapes9 : 1) les phrases appariées sont
alignées sur les mots — ou les syntagmes — pour constituer la relation de traduction qui spéciﬁe
quel élément de 52 est la traduction de quel élément de 51 ; 2) des regles de traduction sont
apprises sur ces données, en général en s’appuyant sur une table de traduction ; 3) chaque
phrase a traduire est segmentée en syntagmes qui sont traduits séparément puis réordonnés
pour adapter le résultat au langage cible. Quand il s’agit d’annotation sémantique, la relation
de traduction est monotone (sans rearrangement). C’est méme l’identité pour tous les éléments
qui restent non-annotés. Les données en entrée de l’algorithme d’apprentissage sont donc moins
bruitées que dans le cas d’un alignement bilingue. L’obtention d’annotations correctes quand
l’information lexicale est ambigiie repose sur l’algorithme d’apprentissage et la projection de
ses résultats sur le texte, dans la mesure o1‘1 cet algorithme prend en compte le contexte pour
apprendre les regles. A noter que le modele tient compte dans ses calculs des éléments qui ne
doivent pas étre annotés : il apprend aussi a traduire a l’identique.

8. Pour différentier les éléments de Voc(O) du vocabulaire anglais, les noms de 0 sont préﬁxés par ’XX’ dans L2.
9. Méme si elles peuvent étre entrelacées dans le calcul.

469 © ATALA

TALN—RECITAL 2013, 17-21 Iuin, Les Sables d’Olonne
4.2 Le modéle

Notre approche repose sur le modéle du canal bruité, qui considére que les phrases annotées
constituent l’information Visée (en entre’e du canal) mais qu’e1les ont été brouillées, produisant
ainsi le texte brut (recu en sortie). Il s’agit donc de reconstituer l’entrée. On attribue une étiquette
sémantique a une phrase vue pour la premiere fois 51 6 L1 en recherchant la phrase 52 6 L2 qui a
la plus grande Valeur pour P(s2|s1). Par la régle de Bayes et puisque P(s1) est ﬁxée, il s’agit de
calculer
5* = 3r8H;aXP(52l51) = 31'8H;3X‘lP(52)P(51l52)}-
2 2

Suivant le modéle de traduction basé sur les syntagmes, la phrase d’entrée non annotée 51 est
segmentée pendant le décodage en une suite de m syntagmes, notée {s;}lT’;1. Chaque segment
5'1 est associé a sa version annotée 5; de sorte que.P(s1|s2)  HlT’;1P(s'1Js'2). On suppose que la
distribution de probab111te sur toutes les segmentations possibles est umforme et on a

5* = argmax{P(s2) x HT’; P(si Isl )}.
52 l 1 1 2

11 y a deux paramétres a calculer dans le modéle ci—dessus : le modéle de langage P(s2) et la table
de traduction des syntagmes P(s§|s;). Le modéle de langage sélectionne la phrase annotée la
plus probable parmi toutes celles qui sont possibles et la table de traduction des syntagmes joue
le r61e d’un dictionnaire sophistiqué entre les langages source et cible. Nous ne pouvons entrer
ici dans les détails, mais, pour nos expérimentations, nous utilisons SRILM (Stolcke, 2002), la
boite a outils du SR1 servant a construire et exploiter des modeles de langage, pour apprendre
un modele de trigrammes. Parmi les nombreuses solutions proposées pour l’apprentissage d’une
table de traduction (Marcu et Wong, 2002; Koehn et al., 2003; Och et Ney, 2003; Chiang, 2007),
nous utilisons la méthode relativement simple mais efﬁcace déﬁnie dans (Koehn et al., 2003). A
cause de la proximité des langages source et cible, les données fournies a cet algorithme sont peu
bruitées. Le décodage est réalise’ par une recherche en faisceau telle qu’imp1émentée par Moses
(Koehn et al., 2007).

4.3 Repérage des annotations sémantiques

Pour identiﬁer la position précise des annotations sémantiques prédites par l’annotation séman-
tique statistique basée sur les syntagmes (ASSS), nous utilisons 1’alignement des traductions au
niveau du mot. Par exemple, dans un tel alignement, la suite “15—14 16-14” indique que les 15*“
et 16ém° mots de la phrase originale ont été remplacés par le 14‘*-me mot de la traduction. Si le
14‘“-‘“‘’ mot appartient a Voc(O) (par exemple XX M ethod), c’est que le concept qui la compose
(dans notre exemple, le concept Method) est l’étiquette sémantique associée au 15"“ et au 16*“
mots de la phrase originale.

5 Expérimentation

Cette section décrit les données d’évaluation et les métriques utilisées.
470 © ATALA

TALN—RECITAL 2013, 17-21 Iuin, Les Sables d’Olonne

L’approche ASSS a été testée sur deux textes annotés. L’un est completement annoté, c’est—a—dire
annoté par 1’ensemb1e des étiquettes sémantiques provenant d’une ontologie construite pour le
domaine en question. On trouve dans le texte des mentions de chaque concept, mais en nombre
limité : ce corpus permet de tester la tolérance de notre approche a la dispersion des données
d’annotation sémantique. L’autre texte est plus volumineux mais il n’est annoté que par une partie
de 1’onto1ogie, par les 17 concepts identiﬁés considérés comme ambigus, car étant associés a des
termes ambigus. Ce second corpus permet de tester la capacité de notre approche 2 résoudre les
ambiguités, qui sont fréquentes en domaine de spécialité, ne serait—ce parce qu’on peut choisir de
rattacher un terrne a différents niveaux de l’ontologie.

Deux méthodes de référence ont été déﬁnies et sont utilisées pour les expériences. La premiere
est une approche a base de dictionnaire de fréquence qui est traditionnelle pour les taches de
désambiguisation lexicale et qui peut s’étendre a notre scénario d’annotation sémantique. L’autre
repose sur un modéle basé sur l’étiquetage de séquences, plus particuliérement sur les champs
aléatoires conditionnels en chaines linéaires (CRF) (Lafferty, 2001; Sutton et Mccallum, 2006)) :
l’annotation sémantique est souvent vue comme une tache d’étiquetage de séquences et les
champs aléatoires conditionnels permettent de tenir compte des noeuds voisins dans un graphe.
L’éva1uation expérimentale montre que notre méthode dépasse signiﬁcativement des approches
standards sur les deux corpus utilisés.

5. 1 Données d’évaluation

Matériel 1° Les corpus choisis sont deux textes extraits d’un réglement international décrivant les
tests auxquels les fabricants d’automobi1es doivent se plier dans la fabrication des ceintures de
sécurité. Aprés segmentation par Treetagger (Schmid, 1995), le corpus 1 comporte 133 phrases et
le corpus 2 en a 1821, dont beaucoup sont longues. L’ontologie 11 qui forme le modéle sémantique
contient 154 entités sémantiques (73 concepts, 58 individus, 23 propriétés).

Annotation sémantique le corpus 1 a été complétement annoté par un expert du domaine (un
des auteurs), soit 364 annotations (2,78 annotations par phrase). Pour la validation croisée, 90 %
des données sont utilisées comme données d’entrainement (80 % servent a entrainer le modele,
et 10 % au tunning) et les 10 % restant sont utilisées comme données de test. Les données
d’entrainement de chaque expérience comportent plus de 50 entrées sémantiques distinctes.

Deux facteurs principaux ont été pris en compte dans la constitution du corpus 2 : le degré
d’ambigu'ité (une méme forme lexicale peut étre annotée différemment dans des contextes
différents — voir la section 3 pour un exemple) et la taille du corpus, de facon que notre seconde
méthode de référence puisse étre calculée en un temps raisonnable eu égard a nos ressources de
calcul (Mac OS X 10.6.8, g++ 4.2.1, avec 2Go de mémoire et un CPU Intel Core 2 Duo 2.26GHz).
Nous avons sélectionné 17 entités sémantiques ambigués de l’ontologie, nous nous en sommes
servis pour annoter le document entier et nous avons sélectionné les 313 phrases étiquetées au
moins une fois.

Pour le corpus 2, la table 1 liste les graphies choisies, le nombre de leurs occurrences annotées et
les 17 étiquettes sémantiques qui leur sont associées. 11 y a aussi 14 occurrences supplémentaires

10. Ce matériel vient du projet européen OntoRule.
11. A noter que, une fois que les exemples annotés sont disponibles, notre méthode n’a plus besoin de l’ontologie.

471 © ATALA

TALN-RECITAL 2013, 17-21 Iuin, Les Sables d’Olonne

Graphie | #Occ | Etiquettes possibles dans la référence
“type” 123 T_ypeReactor, T_ypeRetractor, T_ypeBelt
RetractorLockingTest, BreakingstrengthOfStrapTest,

“tested” 29 D_ynamicTest, Method, ColdImpactTest, NULL
“Test(s)” 19 D_ynamicTest,Method

D_ynamicTest, RetractorDurabilit_yTest, Method,
“tests’' 55 BreakingStrengthOfStrapTest, AccelerationTest,
DecelerationTest, RetractorLockingTest, BuckleTest

ColdImpactTest, RetractorLockingTest,
Method, MicroSlipTest, BuckleOpeningTest,
“test” 190 BreakingStrengthOfStrapTest, D_ynamicTest, BuckleTest,
CorrosionTest, RetractorUnlockingTest, FrontalImpactTest

TABLE 1 — Description des ambiguités du corpus2

de « tested >> non annotées (notées NULL en ligne 2, colonne 3), ce qui constitue une forme
particuliere d’ambigu'1'té.

5.2 Annotations de référence

Nous comparons l’approche proposée avec deux méthodes de référence : l’Annotau'on Sémantique
a base de Dictionnaire et de Fréquence (ASDF) et l’Annotation Sémantique par CRF (ASCRF).

L’approche ASDF est une extension de la désambiguisation lexicale classique parce qu’elle intégre
le fait qu’une annotation peut couvrir plusieurs mots. L’ASDF repose essentiellement sur la
construction et la consultation d’un dictionnaire d’annotation. Ce1ui—ci a comme entrées des
mots ou groupes de mots associés a des labels sémantiques. Ces groupes sont extraits des textes
annotés d’entrainement, et pour chaque mot ou groupe de mots, les labels sémantiques qui
annotent ses occurrences sont enregistrés dans le dictionnaire. L’entrée est lemmatisée pour
s’affranchir des variations morphologiques. L’algorithme d’annotation cherche d’abord dans le
texte lemmaﬁsé les entrées du dictionnaire. Une forme de surface reconnue pouvant étre incluse
dans une autre, seules les entités sémantiques attachées 2‘: la plus longue sont conservées. Pour
désambiguiser une entrée donnée, on choisit le label le plus fréquent. ASDF est implémentée en

Python.

ASCRF segmente et annote les séquences de mots grace au modele discriminant suivant :

my I x) = expiz 9kFk(x:.}')l’:
k=1

Z906)
o1‘1 x = (x1,%..xT) et y = (_y1,...,_yT) sont les séquences d’entrée et de sortie; Fk(x, y) est
déﬁni par 2t=1fk(xt_1,_yt), {fk}1SkSK étant un ensemble arbitraire de fonctions de traits; les
{6k}kSkSK sont les valeurs paramétriques associées.

Pour étre comparables avec le modéle ASSS proposé, les patrons extraits par ASCRF sont des
traits orthographiques et lexicaux des unigrammes et des bigrammes ﬁgurant dans une fenétre

472 © ATALA

TALN-RECITAL 2013, 17-21 Iuin, Les Sables d’Olonne

de 3 mots avant et aprés chaque position observée. Comme les annotations s’étendent éventue1—
lement sur plusieurs mots, elles sont représentées selon le scheme D.I.E. (le Début, 1’Intérieur
et 1’Extérieur du segment de texte. Enﬁn, ASCRF est mis en oeuvre grace a l’implémentation
hautement optimisée de la boite a outils Wapiti (Lavergne et al., 2010).

5.3 Métriques d’évaluation

Bien que nous utilisions un modéle de traduction automatique, le systéme est évalué en calculant
la précision, le rappel et la F—mesure, qui sont plus souvent utilisés dans le domaine de l’extraction
d’information. Nous considérons en outre deux critéres différents : la correction des étiquettes
sémantiques (label) et celle de leurs frontiéres (position). Bien que seul le critére d’éu'quette im-
porte dans certaines applications, comme en REN, la position peut étre signiﬁcative dans d’autres
cas, comme par exemple pour 1’extraction de relations sémantiques. On peut former d’autres
mesures par combinaison des précédentes, comme label et position considérés indépendamment
(le score cumule l’évaluation des labels et des positions) et label et position considérés groupés.
Dans ce dernier cas, c’est le couple (label, position) qui est considéré globalement comme correct
ou incorrect.

Pour chaque métrique ll, parmi {Précision, Rappel, F—mesure}, nous écrivons ii-label, u—position,
u—indep, et u—couple pour désigner les quatre critéres d’évaluation ci—dessus 12. Pour u—position,
le critére est l’identité des positions de l’annotau'on dans le candidat et la référence, méme si on
pourrait aussi tenir compte du recouvrement partiels des positions.

6 Evaluation

Dans cette section, nous comparons d’abord la méthode ASSS proposée et le systeme ASDE
Ensuite, nous comparons ASSS et ASCRF sur les méme corpus sous des réglages différents. Pour
ces deux comparaisons, les expériences ont été effectuées sur les deux corpus en utilisant une
validation croisée par 10‘*-’“*’. Pour ASCRE nous avons partiellement réutilisé la mise en oeuvre de
MOSES (Koehn et al., 2007) en inactivant son modéle de distorsion.

6.1 Comparaison de ASSS et ASDF sur le corpus 1

Le tableau 2 compare les performances moyennes de ASDF et ASSS sur le corpus 1 et les confronte
a ceux de 1’approche hybride déﬁnie ci-aprés.

ASSS a été légérement meilleur pour la prédiction des étiquettes que ASDF (0,26 % d’amélioration
de la F—mesure), mais ASDF a fonctionné mieux qu’ASSS dans la prédiction des positions (+5,2 %
sur la F—mesure), Les deux systémes ont réalisé des performances comparables sur le corpus 1.

Cela signiﬁe que si l’on ne considére que les labels d’annotations, la méthode ASSS est un meilleur
choix : contrairement a la consultation de dictionnaires, ASSS permet une correspondance
approchée. Cependant, ASSS manque plus souvent l’emplacement exact de l’étiquette. Par

12. Dans la section Expérimentation, p.-indep et p,-couple ne ﬁgurent qu’a titre d’explication; en fait ces mesures sont
des combinaisons des deux autres.

473 © ATALA

TALN-RECITAL 2013, 17-21 Iuin, Les Sables d’Olonne
| Métrique | ASDF | ASSS | Hybride |
| F-mesure d’étiquette | 0,9885 | 0,9911 | 0,9911 |
| F-mesure deposition | 0,9858 | 0,9369 | 0,9797 |

TABLE 2 — Evaluation de la ASSS et ASDF sur Corpus1

exemple, alors que la phrase “The test has to be performed separately from the tensile test” a été
annotée avec Tensiletest pour la position | 9 — 10 | par l’expert, ASSS n’a associé l’étiquetage
Tensile test qu’a la position I 9 — 9 I 13. Pour remédier a cela, nous faisons une combinaison de
ASSS et ASDF pour avoir un systeme hybride (la 4eme colonne du tableau 2) déﬁni comme suit :

Définition. (Hybride de ASSS et ASDF) Pour une phrase donnée, soit ANNOASSS et ANNOASDF
les annotations sémantiques générées respectivement par ASSS et ASDE Nous disons que deux
annotations provenant de ANNOASSS et ANNOASDF sont uniﬁables si leurs positions se chevauchent.

Dans le tableau 2, nous pouvons Voir que le systeme hybride a la méme F-mesure de label et a
amélioré la F-mesure de position d’ASSS de 4,28 %, méme si cette derniére est encore inférieure
de 0,61 % a celle d’ASDF.

6.2 Comparaison d’ASSS et ASDF sur le corpus 2

Le tableau 3 montre l’intérét de l’approche ASSS en cas d’ambigu'ité. ASSSa” signiﬁe que
l’expérience a été réalisée sur les 313 phrases du Corpus 2 (sélectionnées pour la présence
de syntagrnes ambigus) mais qu’elles sont annotées avec toutes les entrées sémantiques possibles
de l’ontologie. Nous pouvons Voir qu’ASSS est robuste a l’ambigu'ité, comme en témoigne la
F-mesure de label a 92,95 %.

Une autre observation est que, sauf pour la perte de 1,04 % de rappel de position, ASSS a de
meilleures performances qu’ASDE En effet, les différences entre ASSS et ASDF sont importantes
pour la prédiction des étiquettes (par exemple +21,17 % pour la F-mesure de label), mais assez
faibles pour la prédiction des positions (par exemple +2,21 % pour la F-mesure de position).
L’explication est que le choix des annotations appropriées est plus difﬁcile que la localisation de
ces annotations dans le corpus 2, en raison d’une plus grande proportion d’ambigu'ités dans le
corpus 2 que dans le corpus 1.

Enﬁn, le tableau 3 montre que meme si ASSS a obtenu des scores élevés dans la prédiction
de label sur le corpus 2, les scores sont encore inférieurs a ceux de la position (par exemple
une F-mesure de 92,95 % en prédiction de label vs. 97,65 % en prédiction de position), ce qui
contredit le résultat du corpus 1. C’est encore parce que dans le corpus 2, la désambiguisation
d’étiquettes est plus difﬁcile a réaliser que détection de la position.

11 convient enﬁn de noter que l’approche ASSS fonctionnant mieux en prédiction de position
qu’ASDF (97,65 % contre 95,44% pour la F-mesure de position) pour le corpus 2, l’approche
hybride considérée pour le corpus 1 est inutile pour le corpus 2.

13. On rappelle que, dans nos déﬁnitions, seules les positions exactes (mémes début et ﬁn) sont comptées correctes
dans la F-mesure.

474 © ATALA

TALN—RECITAL 2013, 17-21 Iuin, Les Sables d’Olonne

| Métriques | ASDF | Asss,” | ASSSa”—ASDF |
Précision—groupe 0,7288 0,9369 0,2081
Précision-label 0, 7525 0,9369 0, 1844
Précision—indep 0,8613 0,9598 0,0985
Précision-position 0,9293 0,9826 0,0533
Rappel—groupe 0,7699 0,9222 0,1523
Rappel-label 0,6861 0,9222 0,2361
Rappel—indep 0,9029 0,9464 0,0435
Rappel-position 0,9809 0,9705 -0,0104
F—mesure—label 0, 71 78 0,9295 0,2 1 1 7
F—mesure—position 0,9544 0,9765 0,0221

TABLE 3 — Evaluation d’ASSS et ASDF sur le corpus 2

6.3 Comparaison d’ASSS et ASCRF

Le tableau 4 compare les performances moyennes d’ASCRF et de’ASSS a la fois sur le corpus 1 et

sur le corpus 2. A la différence d’ASSSmu, dans le tableau 3, ASSS17 et ASCRF17 correspondent

au cas ou les 313 phrases sélectionnées dans le corpus 2 ne sont annotées que par les 17 entrées
sémantiques ambigiies, ceci pour réduire le temps d’exécution d’ASCRF 14. Les résultats montrent
que :

— Sur le corpus 1, ASSS a supplanté ASCRF pour toutes les mesures. C’est parce que la taille des
données d’entrainement dans le corpus 1 n’est pas sufﬁsante pour qu’ASCRF parvienne a une
prédiction précise. La comparaison avec la tableau 2 montre qu’ASCRF a fonctionné bien plus
mal qu’ASDF sur le corpus 1. Cela signiﬁe qu’ASSS est plus robuste qu’ASCRF lorsque la taille
des données d’entrainement est limitée.

— Sur le corpus 2, ASSS17 a surpassé ASCRF17 de plus de 8 % pour la prédiction des étiquettes, a
la fois en précision et en rappel, mais a été surpassé de 1,71 % en précision dans la prédiction
de position. Cela montre qu’ASSS a une plus forte capacité de désambiguisation qu’ASCRF;
mais est moins bon qu’ASCRF pour placer les annotations parce que le modéle des positions
d’étiquettes est implicite pour ASSS. De plus, il est intéressant de noter que les deux approches
ASSS et ASCRF ont obtenu des scores relativement élevés en prédiction de position pour le
corpus 2 (> 94 % en précision et en rappel).

— ASSSa” a une meilleure performance que ASCRF17 et ASSS17 sur le corpus 2. Cela montre
que le pourcentage plus élevé d’ambigu'ités dans les corpus ASCRF17 et ASSS17 augmente la
difﬁculté de la tache.

7 Conclusion et perspectives

Cet article ro ose une a roche statisti ue basée sur les s ta mes, nouvelle et ﬂexible ui
PP <1 YT1 8 1 <1
ermet d’annoter les entités sémanti ues dans des documents s écialisés en utilisant des onto-
‘l P

14. Pour ASCRE l’exécution de la validation croisée au 105116 a duré 30 heures en se limitant aux 17 entrées séman-
tiques ambigiies. Traiter toutes les entrées comme pour ASSSHW, aurait nécessité beaucoup plus de temps parce que
Yentrainement d’un modéle de CRF est quadratique en le nombre d’étiquettes (Lavergne et aL, 2011).

475 © ATALA

TALN-RECITAL 2013, 17-21 Iuin, Les Sables d’Olonne

1 Corpus 1 Corpus 2

Métrique ASCRF | ASSS ASCRF17 | ASSS17 | Asss,”
Précision-label 0,8239 0,9889 0,8299 0,9142 0,9369
Précision-position 0,8975 0,9389 0,9577 0,9406 0,9826
Rappel-label 0,8239 0,9889 0,8308 0,9235 0,9222
Rappel-position 0,8975 0,9349 0,9588 0,9518 0,9705

TABLE 4 — Evaluation de ASSS et ASCRF

logies de domaine riches. La méthode a été concue pour des documents techniques, tels que
des textes réglementaires, pour lesquels les approches traditionnelles d’étiquetage sémantique
(étiquetage des entités nommées et annotation sémantique générique) présentent des limitations
importantes. En utilisant plusieurs métriques d’évaluation, nous avons montré que la méthode
proposée donne de meilleurs résultats qu’une approche classique a base de dictionnaire de
fréquence ou qu’une approche discriminante, avec un ensemble réduit d’exemples annotés. Elle
obtient des scores élevés sur le corpus ambigu : une F-mesure de 92,95 % (resp. 97,65 %) pour la
prédiction de label (resp. de position) pour ASSSa”, et une F-mesure de 91.88% (resp. 94,62 %)
pour la prédiction de label (resp. de position) pour ASSS17

Nous projetons maintenant d’améliorer notre approche en étendant ASSS pour utiliser des infor-
mations linguistiques rendues accessibles en pré-traitant les documents source. Nous envisageons
aussi de concevoir des campagnes d’annotation ontologique dans des domaines spécialisés, en
exploitant cette méthode qui permet d’entrainer un systeme d’annotation sur un petit ensemble
d’annotations manuelles. En effet, il semble qu’il soit plus facile pour les annotateurs humains de
corriger une annotation initiale, pourvu qu’elle soit sufﬁsamment bonne, que d’annoter a partir
de rien (Fort et Sagot, 2010).

Remerciements

Ce travail a été partiellement financé par OSEO dans le cadre du programme Quaro. I1 s’inscrit
également dans l’axe 5 du labex EFL (ANR/CGI).

Références

AMARDEILH, E, LAUBLET, P. et MINEL, J.—L. (2005). Document annotation and ontology population
from linguistic extractions. In Proceedings of the 3rd international conference on Knowledge
capture (K—CAP '05), pages 161-168, New York, NY, USA. ACM.

ARONSON, A. R. et LANG, F.—M. (2010). An overview of metamap : historical perspective and
recent advances. JAMIA, 17(3):229—236.

Aw, A., ZHANG, M., XIAO, J. et SU, J. (2006). A phrase—based statistical model for sms text
normalization. In Proceedings of COLING—ACL '06 poster sessions, pages 33-40.

BEAUFORT, R., ROEKHAUT, S., COUGNON, L.—A. et FAIRON, C. (2010). A hybrid rule/model—based
ﬁnite—state framework for normalizing sms messages. In ACL, pages 770-779.

476 © ATALA

TALN-RECITAL 2013, 17-21 Iuin, Les Sables d’Olonne

CHIANG, D. (2007). Hierarchical phrase-based translation. Comput. Linguist., 33:201-228.

CIMIANO, P., HANDSCHUH, S. et STAAB, S. (2004). Towards the self—annotating web. In Proceedings
of WWW’04, pages 462-471.

CIRAVEGNA, E (2003). (lp) : Rule induction for information extraction using linguistic constraints.
Rapport technique, Sheffield university.

COLLINS, M. (2002). Discriminative training methods for hidden markov models : theory and
experiments with perceptron algorithms. In Proceedings of EMNLP’02, pages 1-8.

CUcERzAN, S. (2007). Large—scale named entity disambiguation based on wikipedia data. In
Proceedings of EMNLP-CoNLI.’07, pages 708-716.

CUNNINGHAM, H., MAYNARD, D., BONTCHEVA, K., TABLAN, V, ASwANI, N., ROBERTS, I., GORRELL, G.,
FUNK, A., ROBERTS, A., DAMLJANOVIC, D., HEITz, 'I‘., GREENWOOD, M. A., SAGGION, H., PETRAK, J.,
LI, Y. et PETERS, W. (2011). Text Processing with GATE (Version 6).

DILL, S., EIRON, N., GIBSON, D., GRUHL, D., GUHA, R., JHINGRAN, A., KANUNGO, T., RAJAGOPALAN,
S., TOMKINS, A., TOMLIN, J . A. et ZIEN, J. Y. (2003). Semtag and seeker : bootstrapping the
semantic web via automated semantic annotation. In Proceedings of WW '03, pages 178-186.

ETZIONI, 0., CAEARELLA, M., DOWNEY, D., KOK, S., POPESCU, A.—M., SHAKED, 'I‘., SODERLAND, S.,
WELD, D. S. et YATES, A. (2004). Web-scale information extraction in knowitall (preliminary
results). In Proceedings of WWW’04, pages 100-110.

FINKEL, J. R. et MANNING, C. D. (2009). Nested named entity recognition. In EMNLP '09, pages
141-150.

FORT, K. et SAGOT, B. (2010). Inﬂuence of Pre—annotation on POS—tagged Corpus Development.
In ACL 4th Linguistic Annotation Workshop (LAW 2010), pages 56-63, Uppsala, Suede. Quaero
(en partie).

KIRYAKOV, A., POPOV, B., TERzIEv, 1., MANOv, D. et OGNYANOFF, D. (2004). Semantic annotation,
indexing, and retrieval. Journal of Web Semantics, 2:49-79.

KOEHN, R, HOANG, H., BIRCH, A., CALLISON—BURcH, C., FEDERICO, M., BERTOLDI, N., COwAN, B.,
SHEN, W., MORAN, C., ZENS, R., DYER, C., BOJAR, 0., CONSTANTIN, A. et HERBST, E. (2007). Moses :
Open source toolkit for statistical machine translation. In Proceedings ofACI.’07, pages 177-180.
KOEHN, P, OCH, F. J . et MARCU, D. (2003). Statistical phrase-based translation. In HLT—NAACL,
pages 127-133.

LAFFERTY, J . (2001). Conditional random ﬁelds : Probabilistic models for segmenting and
labeling sequence data. In Proceedings of the International Conference on Machine Learning,
pages 282-289. Morgan Kaufmann.

LAVERGNE, 'I‘., ALLAUZEN, A., CREGO, J. M. et YVON, F. (2011). From n—gram—based to crf—based
translation models. In Proceedings of the Sixth Workshop on Statistical Machine Translation,
pages 542-553, Edinburgh, Scotland. Association for Computational Linguistics.

LAVERGNE, 'I‘., CAPPE, O. et YVON, F. (2010). Practical very large scale CRFS. In Proceedings the
48th Annual Meeting of the Association for Computational Linguistics (ACL), pages 504-513.
Association for Computational Linguistics.

LIU, X., ZHANG, S., WEI, E et ZHOU, M. (2011). Recognizing named entities in tweets. In
Proceedings of HLT '1 1, pages 359-367.

LIVEMEMORIES (2010). Livememories : Second year scientiﬁc report. Rapport technique,
LiveMemories.

477 © ATALA

TALN-RECITAL 2013, 17-21 Iuin, Les Sables d’Olonne
MARCU, D. et WONG, W. (2002). A phrase-based, joint probability model for statistical machine
translation. In Proceedings of EMNLP’02, pages 133-139.

MENDES, P. N., JAKOB, M., GARClA-SILVA, A. et BIZER, C. (2011). DBpedia Spotlight : Shedding
light on the web of documents. In Proceedings of I-Semantics’1 1.

MIHALCEA, R. et CsOMA1, A. (2007). Wikify! : linking documents to encyclopedic knowledge. In
Proceedings of CIKM’07, pages 233-242.

MULLER, H., KENNY, E. E. et STERNBERG, P. W. (2004). Textpresso : An ontology-based information
retrieval and extraction system for biological literature. PLoS Biol, 2:309.

NADEAU, D. et SEKINE, S. (2007). A survey of named entity recognition and classiﬁcation.
Linguisticae Investigationes, 30(1):3—26. Publisher : John Benjamins Publishing Company.
NAZARENKO, A., GUISSE, A., LEVY, F., OMRANE, N. et SZULMAN, S. (2011). Integrating written
policies in business rule management systems. In Proceedings of RuleMI.’1 1.

OCH, E J. et NEY, H. (2003). A systematic comparison of various statistical alignment models.
Computational Linguistics, pages 19-51.

RATINOV, L. et ROTH, D. (2009). Design challenges and misconceptions in named entity recogni-
tion. In Proceedings of CoNLI.’09, pages 147-155.

SCHMID, H. (1995). Improvements in part-of-speech tagging with an application to german. In
Proceedings of the ACL SIGDAT’95-Workshop.

STOLCKE, A. (2002). Srilm — an extensible language modeling toolkit. In In Proceedings of
ICSLP’02, pages 901-904.

SUTTON, C. et MCCALLUM, A. (2006). Introduction to Conditional Random Fields for Relational
Learning, chapitre 4, pages 93-128. MIT Press.

TOMEH, N. (2012). Discriminative Alignment Models For Statistical Machine Translation. These
de doctorat, University of Paris 11, Orsay.

UREN, V S., CIMIANO, P, IRIA, J ., HANDSCHUH, S., VARGAS-VERA, M., MOTTA, E. et CIRAVEGNA, F.
(2006). Semantic annotation for knowledge management : Requirements and a survey of the
state of the art. J. Web Sem., 4(1):14—28.

WANG, Y. (2009). Annotating and recognising named entities in clinical notes. In ACL/AFNLP
(Student Workshop), pages 18-26.

WELTY, C. et IDE, N. (1999). Using the right tools : Enhancing retrieval from marked—up
documents. In Journal Computers and the Humanities, pages 33-10.

WONG, Y. W. et MOONEY, R. J. (2006). Learning for semantic parsing with statistical machine
translation. In Proceedings of HLT—NAACL’06, pages 439-446.

ZHOU, G. et SU, J. (2002). Named entity recognition using an hmm—based chunk tagger. In
Proceedings of ACI.’02, pages 473-480.

478 © ATALA

